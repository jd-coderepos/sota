\documentclass{article}

\PassOptionsToPackage{numbers, sort}{natbib}





\usepackage{enumitem,kantlipsum}



\usepackage[final]{neurips_2021_nofoot}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[dvipsnames]{xcolor}
\usepackage{wrapfig}


\newcommand{\define}[1]{{\bf \boldmath{#1}}}



\usepackage{amsmath,amsfonts,bm,amsthm,amssymb}

\newcommand{\bx}{\mathbf{x}}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\spn}{span}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\bO}{\mathcal{O}}
\newcommand{\gph}{\mathcal{G}} \newcommand{\Fcoe}[1]{\widehat{#1}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\eigfm}[1][\ell]{\phi_{#1}}
\newcommand{\ws}[1][j]{\mathcal{S}^{(#1)}} \newcommand{\vj}[1][j]{v^{(#1)}} \newcommand{\wj}[1][j]{w^{(#1)}} \newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{\bm{A}}
\def\mB{\bm{B}}
\def\mC{{\bm{C}}}
\def\mD{\bm{D}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{\bm{X}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\nup}{\gN_{\uparrow}}
\newcommand{\ndown}{\gN_{\downarrow}}

\newcommand{\da}{\downarrow}
\newcommand{\ua}{\uparrow}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \newcommand{\yg}[1]{\textcolor{red!60!black}{\textbf{YG:} #1}}

\usepackage{todonotes}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{mathtools}
\usepackage{scalerel}
\usepackage{amssymb}

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\renewcommand{\eqref}[1]{(\ref{#1})} 


\newcommand*{\ldblbrace}{\{\mskip-5mu\{}
\newcommand*{\rdblbrace}{\}\mskip-5mu\}}

\def\mcirc{\mathbin{\scalerel*{\bigcirc}{t}}}

\newcommand{\FF}{\mathcal{F}}
\newcommand{\Hilb}{\mathrm{Hilb}}
\newcommand{\res}{\mathrm{res}}
\newcommand{\id}{I}
\newcommand{\tte}{\texttt{e}}

\newcommand{\first}[1]{\textbf{\textcolor{red}{#1}}}
\newcommand{\second}[1]{\textbf{\textcolor{violet}{#1}}}
\newcommand{\third}[1]{\textbf{\textcolor{black}{#1}}}


\title{Weisfeiler and Lehman Go Cellular: CW Networks}



\author{Cristian Bodnar\thanks{Authors contributed equally.}  \\
  University of Cambridge\\
  \texttt{cb2015@cam.ac.uk} \\
\And
   Fabrizio Frasca \\
   Imperial College London \& Twitter \\
\texttt{ffrasca@twitter.com} \\
   \AND
   Nina Otter \\
   UCLA \\
\texttt{otter@math.ucla.edu} \\
   \And
   Yu Guang Wang \\
   MPI-MIS, SJTU \& UNSW \\
\texttt{yuguang.wang@unsw.edu.au} \\
   \AND
   Pietro Li\`{o} \\
   University of Cambridge \\
\texttt{pl219@cam.ac.uk} \\
   \And
   Guido Mont\'{u}far \\
   MPI-MIS \& UCLA \\
   \texttt{montufar@math.ucla.edu} \\
   \And
   Michael Bronstein \\
   Imperial College London \& Twitter \\
   \texttt{mbronstein@twitter.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures.
These problems can be attributed to the strong coupling between the computational graph and the input graph structure.
The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). 
In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. 
We show that this generalisation provides a powerful set of graph ``lifting'' transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higher-order signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.
\end{abstract}


\section{Introduction}

The operations performed by message passing Graph Neural Networks (GNNs) emulate the structure of the input graph. While this property has clear computational advantages, it brings with it a series of fundamental limitations. As observed by \citet{GIN} and \citet{morris2019weisfeiler} the local neighbourhood aggregations used by GNNs are at most as powerful as the Weisfeiler-Lehman (WL) test \citep{weisfeiler1968reduction} in distinguishing non-isomorphic graphs. Therefore, GNNs fail to detect certain higer-order meso-scale structures such as cliques or (induced) cycles~\citep{ARVIND202042, chen2020can}, which are particularly important in applications dealing with social and biological networks or molecular graphs. At the same time, many such layers have to be stacked to make long-range interactions in the graph possible. Besides the computational burden this incurs, deep GNNs typically come with additional problems such as over-smoothing \citep{li2018deeper} and over-squashing \citep{alon2020bottleneck} of the node representations.  

To address these problems, we propose a novel message passing procedure based on (regular) cell complexes, also known as CW complexes\footnote{We use these terms interchangeably. For the latter, the C stands for ``closure-finite'', and the W for ``weak'' topology. The term was coined by \citet{bams/1183513543}.}, topological objects that form the building block of algebraic topology \citep{hatcher_book}. When paired with a theoretically-justified ``lifting'' transformation augmenting the graph with higher-dimensional constructs called ``cells'', our method results in a multi-dimensional and hierarchical message passing procedure over the input graph. Our approach generalises and subsumes the recently proposed Message Passing Simplicial Networks (MPSNs) \citep{bodnar2021weisfeiler}, which operate on simplicial complexes (SCs), topological generalisations of graphs. However, SCs have a rigid combinatorial structure that significantly limits the range of lifting transformations one could use to meaningfully modulate the message passing procedure. In contrast, we show that cell complexes, which in turn generalise simplicial complexes and come with additional flexibility, allow one to construct new and better ways of decoupling the input and computational graphs. 

\paragraph{Main Contributions} To summarise, we propose a message passing scheme operating on regular cell complexes. We call this family of models CW Networks (CWNs) and study their expressive power using a cellular version of the WL test. We show that for an entire class of ``lifting'' transformations CWNs are at least as powerful as the WL test. Furthermore, we prove that for some of the maps in this class, CWNs can be strictly more powerful than WL, Simplicial WL (SWL) and also not less powerful than 3-WL. We also express the fundamental symmetries of these models and show how they can be seen as generalised convolutional operators on cell complexes. Experimentally, we focus our attention on a particular ``lifting'' map based on induced cycles. When applied to molecular graphs, it leads to an intuitive hierarchical message passing procedure involving the atoms, the bonds between them and the chemical rings of the molecules. We demonstrate that this provably powerful approach obtains state-of-the-art results on popular large-scale molecular graph datasets and other related tasks. To the best of our knowledge, this is the first work proposing a cell complex representation for molecules. Our code is available at \url{https://github.com/twitter-research/cwn}. 

\section{Background}

\begin{definition}[\citet{HGh19}]
A \define{regular cell complex} (Figure \ref{fig:cell_def}) is a topological space  together with a partition  of subspaces  of  called \define{cells}, and such that
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=-0.5ex]
    \item  For each  there exists an open neighbordhood of  that intersects finitely many cells.
\item  For all  we have that  iff , where  is the closure of a cell. 
\item Every cell is homeomorphic to  for some .
\item  ({\em Regularity}) For every  there is a homeomorphism  of a closed ball in  to  such that the restriction of  to the  interior of the ball is a homeomorphism onto .
\end{enumerate}
\end{definition}

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \vspace{-15pt}
    \begin{subfigure}[b]{1.0\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/definition.jpeg}
\label{fig:y equals x}
     \end{subfigure}\hspace{3mm}
    \vspace{-23pt}
    \caption{A cell complex  and the corresponding homeomorphisms to the closed balls for three cells of different dimensions in the complex.}
    \label{fig:cell_def}
    \vspace{-20pt}
\end{wrapfigure}






We note that by  condition  the indexing set  has a poset structure ,
while condition  guarantees that this poset structure encodes all the topological information about . Thus, we can identify a regular cell complex  with this poset, called \define{face poset} of . We also use  for the strict version of this partial order.  

Intuitively, one constructs a cell complex through a hierarchical gluing procedure. One starts with a set of vertices (0-cells). Then edges (1-cells) are attached to these by gluing the endpoints of closed line segments to them. We have now only described a (multi) graph. However, one can generalise this even further by taking a two-dimensional closed disk and glue its boundary (i.e. a circle) to any simple cycle in the (multi) graph previously built as in Figure \ref{fig:gluing}. While we are generally not concerned with dimensions above two, this can be further generalised by gluing the boundary of -dimensional balls to certain cells in the complex. 

\begin{figure}
    \centering
     \begin{minipage}{.63\textwidth}
         \includegraphics[width=0.48\linewidth]{figures/glue_disks.jpeg}
         \hfill
         \includegraphics[width=0.48\linewidth]{figures/glued_disks.jpeg}
         \caption{Closed two-dimensional disks are glued to the boundary of the rings present in the graph (left). The result is a 2D regular cell complex (right).}
        \label{fig:gluing}
     \end{minipage}
     \hfill
     \begin{minipage}{0.33\textwidth}
         \centering
         \includegraphics[width=0.35\linewidth]{figures/sphere.jpeg}
\hspace{10pt}
         \includegraphics[width=0.35\linewidth]{figures/empty_tetrahderon.jpeg}
         \caption{A sphere and an empty tetrahedron. The latter is also a simplicial complex.}
        \label{fig:1}
     \end{minipage}
     \vspace{-10pt}
\end{figure}

Consider the examples in Figure \ref{fig:1}. The shown sphere is a cell complex obtained from  two 0-cells (i.e.\ vertices), to which two 1-cells (i.e.\ edges), which form the equator, were attached. The boundary of two 2-dimensional disks (i.e.\ the two hemispheres) were glued to the equator to form a sphere. The second example is a tetrahedron with empty interior. It is a particular type of cell complex called a \emph{simplicial complex} (SC). The only 2-cells it allows are triangle-shaped. More generally, the -dimensional cells of SCs are -simplices, which makes them slightly more rigid structures.



\begin{definition}
The \textbf{-skeleton} of a cell complex , denoted , is the subcomplex of  consisting of cells of dimension at most .
\end{definition}

This definition is useful for referring for certain parts of the complex. For instance,  contains the vertices in the complex, while  contains the vertices \emph{and} the edges (i.e. the underlying graph). 

The combinatorial structure of the complex can be more compactly described by an incidence relation we call the \emph{boundary relation}, whose reflexive and transitive closure gives the partial order defined above. The boundary relation describes what cells are on the boundary of other cells. For instance, the edges of the sphere in Figure \ref{fig:1} are on the boundary of the 2-cells forming the two hemispheres. 

\begin{definition}
\label{def:boundary_rel}
We have the \textbf{boundary relation}  iff  and there is no cell  such that .
\end{definition}

We can use this to define the four types of (local) adjacencies present in cell complexes. These adjacencies will be the fundamental building block of our message passing procedure. To explain these in more familiar terms, for each adjacency, we exemplify how it shows up in graphs.   

\begin{definition}[Cell complex adjacencies]
\label{def:adj}
For a cell complex  and a cell , we define:
\begin{enumerate}[leftmargin=*, topsep=0pt,itemsep=-0.5ex]
    \item The boundary adjacent cells . These are the lower-dimensional cells on the boundary of . For instance, the boundary cells of an edge are its vertices.
    \item The co-boundary adjacent cell . These are the higher-dimensional cells with  on their boundary. For instance, the co-boundary cells of a vertex are the edges it is part of.
    \item The lower adjacent cells  such that  and . These are the cells of the same dimension as  that share a lower dimensional cell on their boundary. The line graph adjacencies between the edges are a classic example of this. 
    \item The upper adjacent cells  such that  and . These are the cells of the same dimension as  that are on the boundary of the same higher-dimensional cell as . The typical graph adjacencies between vertices are the canonical example here. 
\end{enumerate}
\end{definition}

\section{Cellular Weisfeiler Lehman}

\paragraph{Overview} The results in this section show how one can transform graphs into higher-dimensional cell complexes in such a way that performing colour refinement on the resulting cell complexes makes it easier to test their isomorphism. The message passing model from Section \ref{sec:CWN_MMP} will take advantage of these theoretical results. All proofs can be found in Appendix \ref{app:proofs_cellular}. 

\begin{definition}
Let  be a colouring of the cells in a complex  with  denoting the colour assigned to cell . Define  and . We define the following multi-sets of colours:
\begin{enumerate}[leftmargin=*, topsep=0pt,itemsep=-0.5ex]
    \item The colours of the boundary cells of : . 
    \item The colours of the co-boundary cells of : .
    \item The lower adjacent colours of :  and .
    \item The upper adjacent colours of :  and .
\end{enumerate}
\end{definition}

Note that unlike in graphs and simplicial complexes, the sets  and  can have more than one element. For instance, two (closed) 2-cells might intersect in more than one edge (e.g. the two hemispheres in Figure \ref{fig:1}), and conversely, two edges might be on the boundary of the same two 2-cells. This illustrates the more flexible combinatorial structure of cell complexes. 

\paragraph{Cellular WL (CWL)} We consider CWL, a colour refinement scheme for cell complexes that generalises the Simplicial WL~\citep{bodnar2021weisfeiler} and WL~\citep{weisfeiler1968reduction} tests. We use  to refer to the colour assigned by CWL to cell  at iteration  of the algorithm. When the input is a simplicial complex, this recovers the SWL algorithm. A step of the algorithm is graphically depicted in Figure \ref{fig:CWL} for a single cell. 
\begin{enumerate}[leftmargin=*]
    \item Given a regular cell complex , all the cells  are initialised with the same colour. 
    \item Given the colour  of cell  at iteration , we compute the colour of cell  at the next iteration   by injectively mapping the multi-sets of colours belonging to the adjacent cells of  using a perfect HASH function:  
    \item The algorithm stops when a stable colouring is reached. Two cell complexes are considered non-isomorphic if their colour histograms are different. Otherwise, the test is inconclusive. 
\end{enumerate}


First, we state the following theorem from \citet{bodnar2021weisfeiler} involving SWL and simplicial complexes. This theorem shows that on simplicial complexes, certain adjacencies can be pruned without affecting the non-isomorphic SCs that can be distinguished. This has important computational implications. 

\begin{theorem}
SWL without coboundary and lower-adjacencies has the same expressive power in distinguishing non-isomorphic simplicial complexes as SWL with the complete set of adjacencies. 
\end{theorem}

It is not immediately clear whether an equivalent theorem would also hold for cell complexes. This is because cells, unlike simplices, can have widely different shapes and, as described above, the adjacencies between them take more complicated forms. Nevertheless, we show that a positive result can be obtained. 

\begin{theorem}
\label{thm:sparse_cwl}
CWL without coboundary and lower-adjacencies has the same expressive power in distinguishing non-isomorphic cell complexes as CWL with the complete set of adjacencies. 
\end{theorem}

We note this does not mean that the removed adjacencies are completely redundant in practice. Even if they are not needed from a (theoretical) colour refinment perspective, they might still include important inductive biases that make them suitable for certain tasks. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/CWL.png}
    \caption{The CWL colouring procedure for the yellow edge of the cell complex. All cells have been assigned unique colours to aid the visualisation of the adjacencies. Note that the yellow edge aggregates long-range information from the light green edge.}
    \label{fig:CWL}
\end{figure}

We are now interested in examining various procedures for mapping, or ``lifting'', graphs into the space of regular cell complexes. Such a procedure can be used to test the isomorphism of two graphs by performing colour refinement on the cell complexes they are mapped to. The hope is that CWL applied to these cell complexes is more powerful than WL applied to the initial graphs. We will later show that for a wide range of transformations, this is indeed the case. We start by rigorously defining what we mean by a  ``lifting''.

\begin{definition}
A \textbf{cellular lifting map} is a function  from the space of graphs  to the space of regular cell complexes  with the property that two graphs  are isomorphic iff the cell complexes  are isomorphic.  
\end{definition}

This property ensures that testing the isomorphism of the two cell complexes is equivalent to testing the isomorphism in the input graphs. This would not be the case if two non-isomoprhic graphs were mapped to the same cell complex.

\begin{example}
\label{ex:clique_complex}
It can be verified that the function mapping each graph to its clique complex (i.e.\ every -clique in the graph becomes a -simplex) is a cellular lifting map. \end{example}

The clique complex lifting map from Example \ref{ex:clique_complex} has been used by \citet{bodnar2021weisfeiler} to show that SWL is strictly more powerful than WL. We restate this result:
\begin{theorem}
SWL with clique complex lifting is strictly more powerful than WL. 
\end{theorem}

A natural question is what other lifting transformations make CWL strictly more powerful than WL? We first describe a space of lifting transformations that make CWL at least as powerful as WL. 

\begin{definition}
A lifting map is \textbf{skeleton-preserving} if for any graph , the -skeleton of  and  are isomorphic as (multi) graphs.
\end{definition}

Intuitively, skeleton-preserving liftings ensure that the additional structure added by the lifting map comes from attaching cells of dimension at least two to the graph. These mappings keep the 0-cells and 1-cells intact and are, therefore, restricted from making modifications to the input graph structure. An important remark is that for simplicial complexes, attaching simplices based on cliques present in the graph is the only possible skeleton preserving transformation. Once again, this illustrates the limitations of simplicial complexes for adding useful higher-dimensional structures to the graph. 

\begin{wrapfigure}[9]{r}{0.35\textwidth}
    \begin{subfigure}[t!]{1.0\linewidth}
        \centering
        \vspace{-.3cm}
        \includegraphics[width=1.0\textwidth]{figures/lifting_map_example.jpeg}
        \vspace{-4mm}
    \end{subfigure}
     \caption{A graph, its clique complex and the graph with duplicated edges. The first map is skeleton-preserving, while the second is not.}
     \label{fig:sk_example}
\end{wrapfigure}

\begin{example}
\label{ex:skeleton-lift}
The function from Example \ref{ex:clique_complex} is also skeleton-preserving because the 1-skeleton of the clique complex of a graph is trivially isomorphic to the graph. A lifting function mapping each graph to a multi-graph where each edge is doubled by a parallel edge is not skeleton-preserving (Figure \ref{fig:sk_example}). 
\end{example}

We now show that all the maps in the skeleton-preserving class have the following desirable property: 

\begin{theorem}
\label{thm:skeleton}
Let  be a skeleton-preserving lifting map. Then CWL() (i.e. CWL using lifting ) is at least as powerful as WL in distinguishing non-isomorphic graphs.  
\end{theorem}

To prove that some of these make CWL strictly more powerful than WL, it is sufficient to find a pair of graphs that cannot be distinguished by WL, but can be distinguished by CWL. The following result gives examples of such maps. 

\begin{definition}
Let -, -, - be the lifting maps attaching cells to all the cliques, induced cycles and simple cycles, respectively, of size at most .  
\end{definition}

\begin{corollary}
\label{cor:WL_lifting_maps}
For all , CWL(-), CWL(-) and CWL- are strictly more powerful than WL.   
\end{corollary}

We note that this is not a complete list. For instance, the result can also be extended to combinations of the above or other transformations. We can also relate CWL to the higher-order 3-WL test.  

\begin{theorem}
\label{thm:lifting3WL}
There exists a pair of graphs indistinguishable by 3-WL but distinguishable by CWL(-) with , CWL(-) with  and CWL- with .
\end{theorem}

Finally, we conclude this section by showing how CWL can achieve a superior expressive power compared to SWL. This result is proven by Corollary \ref{cor:cwl_better_than_swl} in the Appendix. 

\begin{theorem}
Let -  - and -  - denote combined liftings attaching cells to the union of the specified substructures. CWL(--) and CWL(--) are strictly more powerful than SWL(-) for all . 
\end{theorem}

\section{Molecular Message Passing with CW Networks}
\label{sec:CWN_MMP}

We now describe CW Networks with an applied focus on molecular graphs to ground the discussion. Therefore, from now on we assume the use of the skeleton-preserving lifting transformation that attaches 2-cells to all the induced cycles (i.e. chordless cycles) in the graph as in Figure \ref{fig:gluing}. This leads to a message passing procedure involving atoms (vertices / 0-cells), the bonds between atoms (edges / 1-cells) and chemical rings (induced cycles / 2-cells). Additionally, in virtue of Theorem \ref{thm:sparse_cwl}, we consider only the boundary and upper adjacencies between these cells without sacrificing the expressive power. The equations for the other adjacencies, which we do not use, can be found in Appendix \ref{app:proofs_cellular}. We note however, that the theoretical results in this section are general and not particular to these specific choices of adjacencies and lifting transformation. 

\paragraph{Molecular Message Passing} The cells in our CW Network receive two types of messages: 

The first specifies messages from atoms to bonds and from bonds to rings. The second type of message, specifies messages between atoms connected by a bond and messages between bonds that are part of the same ring (Figure \ref{fig:message_passing}). Note that for the second type of adjacency, when two atoms communicate, we include the features of the bond between them. Similarly, when two bonds communicate, we include the features of the ring they communicate through. 
The update operation takes into account these two types of incoming messages and updates the features of the cells:

To obtain a global embedding for a cell complex  from a model with  layers, the readout function takes as input the separate multi-sets of features corresponding to the atoms, bonds and the rings:


\begin{wrapfigure}[]{r}{0.22\textwidth}
    \begin{subfigure}[t!]{1.0\linewidth}
        \centering
        \vspace{-10pt}
        \includegraphics[width=1.0\textwidth]{figures/message_passing.png}
    \end{subfigure}
    \caption{Hierarchical depiction of the message passing procedure. \textcolor{orange}{Orange} arrows indicate boundary messages received by cells  and , while \textcolor{BlueViolet}{blue} ones show upper messages received by cells  and .}
    \label{fig:message_passing}
    \vspace{-40pt}
\end{wrapfigure}

\paragraph{Expressivity} Naturally, the ability of CWNs to distinguish non-isomorphic regular cell complexes is bounded by CWL. Similarly to GNNs and WL, CWNs can also be shown to be as powerful as CWL as long as they are equipped with a sufficient number of layers and the parametric local aggregators they use can learn to be injective. Multiple such multi-set aggregators \citep{GIN, Corso2020_PNA} are known to exist and can be directly employed in our model.   

\begin{theorem}
\label{thm:CWandCWN}
CW Networks are at most as powerful as CWL. Additionally, when using injective neighbourhood aggregators and a sufficient number of layers, CWNs are as powerful as CWL.
\end{theorem}

Corollary~\ref{cor:WL_lifting_maps} states that CWL is strictly more powerful than the standard WL when the lifting procedure attaches 2-cells to induced cycles of size . As a consequence of Theorem~\ref{thm:CWandCWN}, this result also holds for molecular message passing CWNs equipped with injective aggregators. In practice,  is to be considered as a standard hyperparameter, and its choice can either be driven by validation set performance, or by domain knowledge (if available).

\paragraph{Symmetries} Given a graph  with adjacency matrix  and feature matrix , a function  is (node) permutation equivariant if , for any permutation matrix . GNN layers respect this equation, which ensures 
they compute the same functions up to a permutation (i.e. relabeling) of the nodes. Similarly, it can be shown that CW Networks are equivariant with respect to permutations of the cells and corresponding permutations of the boundary relations  between cells. We define this notion of equivariance more formally in Appendix \ref{app:symmetries}.    

\begin{theorem}
\label{thm:CWequivariant}
CW Network layers are cell permutation equivariant. 
\end{theorem}

\paragraph{Long-Range Interactions} 

Several graph-related tasks require the ability to capture long-range interactions between nodes. For instance, certain molecular properties depend on atoms placed on the opposite sides of a ring \citep{gilmer2017neural, ramakrishnan2014quantum}. 
As a consequence of the coupling between the input and computational graphs,  message passing operations are necessary in GNNs to let a node receive information from an -hops distant node. 
In contrast, our hierarchical message passing scheme requires \emph{at most}  layers since -cells create shortcuts. 
For example, a constant number of CWN layers () is enough to capture dependencies between atoms on the opposite sides of a ring, independently of the ring size. In Section~\ref{ss:synth} we verify this in a controlled scenario. Additional experiments on real world graphs in Section~\ref{ss:real} confirm that it can achieve state-of-the-art performance with a limited number of layers.

\paragraph{Anisotropic Filters} 
Due to the lack of a canonical ordering between neighbours, many common GNNs use symmetric convolutional kernels, resulting in isotropic filters treating neighbours equally. Recent works have proposed to address this limitation by employing additional structural information~\citep{beaini2020directional, bouritsas2020improving}. CWNs also implicitly achieve this form of anisotropy by integrating information from the higher-order cells and their associated substructures into the message passing procedure. For instance, bond features can learn to encode their membership to a ring and also communicate directly with other bonds present in the ring. Consequently, the messages between atoms connected through these bonds are modulated by the presence of the ring as well as by the presence of other nodes and bonds part of that ring.

\paragraph{CWNs as Generalised Convolutions}
Our message passing scheme can be seen a (non-linear) generalisation of linear diffusion operators on cell complexes.
Recent works~\citep{ebli2020simplicial, bunch2020simplicial} have introduced convolutional operators on SCs by employing the Hodge Laplacian \citep{schaub2020random},
a generalisation of the graph Laplacian. 
By leveraging on the cellular Sheaf Laplacian \citep{HGh19}, a similar construction can be extended to cell complexes to define cellular convolutional operators. In Appendix~\ref{app:convs} we discuss this approach and show that our cellular message passing scheme subsumes it. This represents a promising avenue for studying CWNs from a spectral perspective, an endeavour we leave for future work. 

\paragraph{Computational Complexity} When considering cells of a constant maximum dimension and boundary size, the computational complexity of the message passing scheme is linear in the size of the input complex. For the molecular applications we are interested in, the average number of rings per molecule is upper bounded by a small constant (e.g. three for MOLHIV), so the size of the complex is approximately the same as the size of the graph. Therefore, in this setting, the computational complexity of the model is similar to that of message passing GNNs. Separately of this, the one-time preprocessing step of computing the lifting of the graphs should also be considered. The  induced cycles in a graph can be listed in  time \citep{ferreira2014amortized}. Again, given that  is upper bounded by a small constant for the molecular datasets of interest in this work, the complexity of the lifting procedure is also almost linear in the size of the graph. A more detailed analysis backed up by wall-clock time experiments is given in Appendix \ref{app:complexity}.




\section{Experiments}
\label{sec:results}

In this section we validate the theoretical and empirical properties of our proposed message passing scheme in controlled scenarios as well as in real-world graph classification problems, with a focus on large scale molecular benchmarks. For simplicity, in all experiments we employ a model which stacks CWN layers with local aggregators as in GIN~\citep{GIN}. We name our architecture ``Cell Isomorphism Network'' (CIN). -cells are always endowed with the original node features; higher-dimensional cells are populated in a benchmark specific manner. See Appendix~\ref{app:results} for details on feature initialisation, message passing and readout operations, hyperparameters, implementation and benchmark statistics.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t!]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ring_transfer.pdf}
        \caption{RingTransfer Results. Accuracy is over  balanced classes. A score of  is equivalent to a random guess. Error bars show the min and max. Our model obtains high-scores in average even for large rings despite using only three layers.}
        \label{fig:rt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t!]{0.53\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sr_exp_rings_sns.pdf}
        \caption{Failure rates on the SR isomorphism task, \emph{the smaller the better} (mean and std-error over  runs). In parantheses, for each model, the maximum size  of rings lifted to -cells.}
        \label{fig:sr_iso}
    \end{subfigure}
    \caption{Results on the RingTransfer and SR synthetic benchmarks.}
    \label{fig:synth}
    \vspace{-12pt}
\end{figure}

\subsection{Synthetic Benchmarks}\label{ss:synth}



\paragraph{CSL} Circular Skip Link dataset was first introduced in~\cite{pmlr-v97-murphy19a} and has been recently adopted as a reference benchmark to test the expressivity of GNNs~\citep{dwivedi2020benchmarkgnns}. It consists of 150 -regular graphs from 10 different isomorphism classes, which we need to predict. 
Unsolvable by the WL test and message passing approaches \citep{pmlr-v97-murphy19a, Chen2019_ring_gnn}, we use it to validate the expressive power of CWNs. 

\begin{wraptable}[7]{l}{0.425\textwidth}
        \centering
        \vspace{-13pt}
        \begin{minipage}[t]{1.0\linewidth}
\caption{Classification accuracy on CSL.}\label{tab:csl}
          \resizebox{\columnwidth}{!}{
          \begin{tabular}{l  ccc}
            \toprule
            Method & 
            Mean &
            Min &
            Max \\
            \midrule
            
            MP-GNNs & 
            10.0000.000 &
            10.000 &
            10.000 \\
            
            RingGNN & 
            10.0000.000 &
            10.000 &
            10.000 \\
            
            3WLGNN &
            97.80010.916 &
            30.000 &
            100.000 \\

            \midrule
            
            CIN (Ours) & 
            100.0000.000 &
            100.000 &
            100.000 \\
            \bottomrule
          \end{tabular}}
    \end{minipage}
\end{wraptable}

We follow the same evaluation setting as ~\citet{dwivedi2020benchmarkgnns}: -fold cross validation procedure and  different random weight initialisations. For our model, we set the maximum ring size . In Table~\ref{tab:csl} we follow the common practice on this dataset and report the mean, minimum and maximum test accuracy obtained by CIN over the  runs, along with the results by the baselines presented in~\citet{dwivedi2020benchmarkgnns}.
MP-GNNs, 
that is classic message passing GNNs (GAT~\citep{velivckovic2017graph}, MoNet~\citep{Monti_etal2017}, GIN~\citep{GIN}, etc.), and RingGNN~\citep{Chen2019_ring_gnn} perform as random guessers. In contrast, our model is able to identify the isomorphism class of each test graph in every run while featuring only a fraction of the computational complexity of 3WLGNN, the best performing reference baseline~\citep{dwivedi2020benchmarkgnns, maron2019provably}.





\paragraph{SR} Similarly to~\citet{bodnar2021weisfeiler} and~\cite{bouritsas2020improving}, we consider Strongly Regular graphs within the same family as hard examples of non-isomorphic graphs we seek to distinguish. Any pair of graphs within the same family cannot provably be distinguished by 3-WL test~\citep{bodnar2021weisfeiler, bouritsas2020improving}. We reproduce the same experimental setting of~\citet{bodnar2021weisfeiler}. In particular, we consider  distinct SR families\footnote{Data available at: \url{http://users.cecs.anu.edu.au/~bdm/data/graphs.html}.} and run our model untrained on the cell complex lifting of each graph, with . -cells (nodes) are initialised with a constant unitary signal, while - and -cells are initialised with the sum of the contained -cells. We additionally run an MLP baseline with sum readout to appreciate the contribution of message passing. We report the percentage of non-distinguished pairs in Figure~\ref{fig:sr_iso}. 
Contrary to 3-WL, both CIN and the MLP baseline are able to distinguish many pairs across all families, with better performance attained for larger . For , we observed CIN to disambiguate all pairs in all families (\% failure rate). Despite the strong results achieved by the baseline, we found CIN to always distinguish a larger number of non-isomorphic pairs for the same values of , this confirming the importance of cellular message passing.


\paragraph{RingTransfer} In order to empirically validate the ability of CIN to capture long-range node dependencies, we additionally design a third synthetic benchmark dubbed as `RingTransfer'. Graphs in this dataset are chordless cycles (rings) of size . In each graph we mark two special nodes as \textbf{target} and \textbf{source}, always placed at distance . The task is for \textbf{target} to output the one-hot encoded label assigned to \textbf{source}. All other nodes in the ring are assigned a unitary constant feature vector. A model has to learn to transfer the information contained in \textbf{source} to the opposite side of the ring, where \textbf{target} resides. We initialise - and -dimensional cells with a null signal. In Figure~\ref{fig:rt} we show the performance of a -layer CIN as a function of the ring size , along with that of GIN~\citep{GIN} baselines equipped with  stacked layers. We observe that our model learns to solve the task with only  computational steps, independent of . As for GIN, we observed degradation in the performance for , up to complete failure. We hypothesise this to be due to the difficulties of training such a deep GNN ( layers). We further verify the (theoretically expected) failure of GIN (not included) when endowed with less than  layers. 

\subsection{Real-World Graph Benchmarks}\label{ss:real}

\begin{table}[!t]
    \centering
\caption{TUDatasets. The first section of the table includes the accuracy of graph kernel methods, while the second includes GNNs. The top three are highlighted by \textbf{\textcolor{red}{First}}, \textbf{\textcolor{violet}{Second}}, \textbf{Third}.}
\label{tab:tud_datasets}
    \resizebox{\linewidth}{!}{\begin{tabular}{l | lllll | lll}
\toprule
        Dataset & 
        MUTAG &
        PTC &
        PROTEINS &
        NCI1 &
        NCI109 &
        IMDB-B & 
        IMDB-M & 
        RDT-B \\
        \midrule       
RWK \citep{gartner2003graph} & 
         79.22.1 & 
         55.90.3 & 
         59.60.1 & 
         3 days & 
         N/A & 
         N/A &
         N/A &
         N/A \\


        GK () \citep{shervashidze2009efficient} &
        81.41.7 & 
        55.70.5 &
        71.40.3 & 
        62.50.3 & 
        62.40.3 &
        N/A &
        N/A &
        N/A \\

         
        PK \citep{neumann2016propagation} & 
         76.02.7& 
         59.52.4 &
         73.70.7 & 
         82.50.5 & 
         N/A & 
         N/A & 
         N/A &
         N/A \\

         
        WL kernel \citep{shervashidze2011weisfeiler} &
          90.45.7 & 
          59.94.3 & 
          75.03.1 & 
          \first{86.0}1.8 & 
          N/A &
          73.83.9 &
          50.93.8 &
          81.03.1 \\

        
        \midrule
         
        DCNN \citep{DCNN_2016} & 
        N/A&  
        N/A &
        61.31.6 &
        56.61.0 &
        N/A &
        49.11.4 &
        33.51.4 &
        N/A \\


        DGCNN \citep{zhang2018end} & 
        85.81.8 & 
        58.62.5 & 
        75.50.9 & 
        74.40.5 & 
        N/A &
        70.00.9 & 
        47.80.9 &
        N/A \\
        
        IGN \citep{maron2018invariant} &
        83.913.0 &
        58.56.9 &
        \third{76.6}5.5 &
        74.32.7 & 
        \third{72.8}1.5 & 
        72.05.5 & 
        48.73.4 &
        N/A \\


        GIN \citep{GIN} & 
        89.45.6 & 
        64.67.0	& 
        76.22.8 &
        82.71.7 &
        N/A & 
        75.15.1 &
        52.32.8 &
        \first{92.4}2.5 \\

        PPGNs \citep{maron2019provably} &
        \third{90.6}8.7 &
        66.26.6 &
        \first{77.2}4.7 & 
        83.21.1 & 
        \second{82.2}1.4 &
        73.05.8 & 
        50.53.6 &
        N/A \\

        Natural GN \citep{de2020natural} &
        89.41.6 &
        \third{66.8}1.7 &
        71.71.0 &
        82.41.3 &
        N/A &
        73.52.0 &
        51.31.5 &
        N/A\\

        GSN \citep{bouritsas2020improving} &
        \second{92.2}  7.5 &
        \first{68.2}  7.2 & 
        \third{76.6}  5.0 & 
        \third{83.5}  2.0 &
        N/A & \first{77.8}  3.3 & 
        \first{54.3}  3.3 &
        N/A \\
        
        SIN \citep{bodnar2021weisfeiler} & 
        N/A  &
        N/A & 
        76.4  3.3 & 
        82.7  2.1 &
        N/A &
        \second{75.6}  3.2 & 
        \third{52.4}  2.9 &
        \third{92.2}  1.0 \\

        \midrule
        
        {{\bf CIN} (Ours)} & 
        \first{92.7}  6.1 &
        \first{68.2}  5.6 &
        \second{77.0}  4.3 &
        \second{83.6}  1.4 &
        \first{84.0}  1.6 &
        \second{75.6}  3.7 & 
        \second{52.7}  3.1 &
        \first{92.4}  2.1 \\
        
        \bottomrule

\end{tabular}
    }
\end{table}


\paragraph{TUD} We test our model on  TUDataset benchmarks \citep{morris2020tudataset} with small and medium sizes from biology (\textbf{PROTEINS}  \citep{dobson2003distinguishing,borgwardt2005protein}), chemistry (i.e. molecules -- \textbf{MUTAG} \citep{kazius2005derivation,riesen2008iam}, \textbf{PTC}, \textbf{NCI1} and \textbf{NCI109} \citep{wale2008comparison}) to social networks (\textbf{IMDB-B}, \textbf{IMDB-M}, \textbf{RDT-B}). 
We consider induced cycle of size up to  for our graph lifting procedure. We initialise node (and -cell) features as described in~\citet{GIN}, and higher dimensional cells by averaging or summing the features of the included -cells. The training setting and evaluation procedure follow those in \citet{GIN}. 
We report the results in Table~\ref{tab:tud_datasets}. CIN compares more than favourably with the baselines, displaying strong empirical performance on all benchmarks. The mean accuracy of CIN ranks top on four out of eight datasets. On the remaining datasets, CIN achieves the second place. We observe that the best results are on datasets from the biological and chemical domains, where rings play a relevant role.


\paragraph{ZINC} 
We study the effectiveness of cellular message passing on larger scale molecular benchmarks from the ZINC database \citep{ZINCdataset}. \textbf{ZINC} (12k graphs) and \textbf{ZINC-FULL} (250k graphs) \citep{dwivedi2020benchmarkgnns,jin2018junction,you2018graph,gomez2018automatic} are two graph regression task datasets for drug constrained solubility prediction. In these experiments, we consider rings up to size . We follow the training and evaluation procedures in~\citep{dwivedi2020benchmarkgnns}. Our experiments encompass different scenarios, examine the impact of ablating edge features and of constraining the parameter budget of the architecture to k. All results are illustrated in Table~\ref{tab:mol_dataset} where we also include the results for \textbf{ZINC-FULL} obtained by the same exact architectures. Our model exhibits particularly strong performance on these benchmarks: it attains state-of-the-art results on both the two dataset variants, outperforming other models by a significant margin. CIN attains strong results even when constrained by the parameter budget. It still achieves state-of-the-art performance on \textbf{ZINC} and is on-par with the best unconstrained baseline under edge-feature ablation. 


\paragraph{Mol-HIV} We additionally test our model on the molecular \textbf{ogbg-molhiv} dataset from the Open Graph Benchmark~\citep{hu2020open} (k graphs). The task is to predict the capacity of compounds to inhibit HIV replication. Rings of size up to  are considered as -cells. We take the architecture in~\cite{Fey2020_himp} as reference and replicate the same hyperparameter setting in our model, including the use of only  message passing layers. We report the mean of test ROC-AUC metrics at the epoch of best validation performance for  random weight initialisations. Similarly to ZINC, we experiment with a ``small'' model whose number of parameters is constrained in the order of k. Table~\ref{tab:mol_dataset} displays the results. CIN significantly outperforms other strong GNN baselines, even when constrained by the parameter budget. Consistently with~\cite{Fey2020_himp}, we observe that only two layers are sufficient when performing hierarchical message passing across meso-scale structures such as rings.

\begin{table}[!t]
    \centering
    \begin{minipage}[t]{0.80\textwidth}
        \centering
         \caption{ZINC (MAE), ZINC-FULL (MAE) and Mol-HIV (ROC-AUC).}
        \label{tab:mol_dataset}
           \resizebox{\columnwidth}{!}{
          \begin{tabular}{l cccc}
            \toprule
            \multirow{2}{*}{Method} & 
            
            \multicolumn{2}{c}{ZINC } &
            ZINC-FULL & 
            MOLHIV \\
            
            &
            No Edge Feat. &
            With Edge Feat. &
            All methods &
            All methods \\
            \midrule
            
            GCN \citep{kipf2017graph} & 
            0.4690.002 &
            N/A &
            N/A &
            76.060.97 \\
            
        
            GAT \citep{velivckovic2017graph} & 
            0.4630.002 &
            N/A &
            N/A &
            N/A \\
            
            GatedGCN \citep{bresson2017residual} &
            0.4220.006 & 
            0.3630.009 &
            N/A &
            N/A \\

            
            GIN \citep{GIN}  & 
            0.4080.008 &
            0.2520.014 &
            0.0880.002 &
            77.071.49 \\
            
            PNA \citep{Corso2020_PNA} & 
            0.3200.032 & 
            0.1880.004 &
            N/A &
            79.051.32 \\
            
            DGN \citep{beaini2020directional} & 
            0.2190.010 &
            0.1680.003 &
            N/A &
            79.700.97 \\
            
            HIMP \citep{Fey2020_himp} &
            N/A &
            0.1510.006 &
            0.0360.002 &
            78.800.82\\

            GSN \citep{bouritsas2020improving} & 
            0.1390.007 &
            0.1080.018 &
            N/A &
            77.991.00 \\
            
            \midrule
            
            \textbf{CIN-small} (Ours) & 
            0.1390.008 &
            0.0940.004 &
            0.0440.003 &
            80.551.04 \\
            
            \textbf{CIN} (Ours) & 
            \textbf{0.115}\textbf{0.003} &
            \textbf{0.079}\textbf{0.006} &
            \textbf{0.022}\textbf{0.002} &
            \textbf{80.94}\textbf{0.57} \\
            \bottomrule
          \end{tabular}}
    \end{minipage}
    \vspace{-11pt}
\end{table}

\section{Related Work, Discussion and Conclusion}
\label{sec:conclusion}



\paragraph{Cell complex models} Recent works have proposed the generalisation of GNNs to simplicial complexes \citep{ebli2020simplicial, bunch2020simplicial, glaze2021principled, hajij2021simplicial}. All these simplicial methods are subsumed by the model in~\citet{bodnar2021weisfeiler}, which CWNs in turn subsume. 
To the best of our knowledge, \citet{hajij2020cell} is the only other example of message passing on cell complexes, but this work does not study the expressive power of the proposed scheme, neither it experimentally validates its performance. In contrast, our work comprehensively characterises the expressiveness of cellular message passing, and introduces a theoretically grounded and empirically effective framework to apply it on graph structured data in a way to address several limitations of standard Graph Neural Networks.


\paragraph{Molecular substructures} A few other works have extended GNNs to account for molecular substructures. Junction Trees (JT), which conveniently represent singletons, bonds and rings as supernodes in a tree, have been used in molecular graph generation~\citep{jin2018junction, jin2019learning}. JTs are also used in the recent work of~\citet{Fey2020_himp}, who employs them to design a hierarchical message passing scheme based on the tree structure. However, this hierarchy has a different configuration than the one cell complexes provide. Information about cycles is also used in GSNs~\citep{bouritsas2020improving} to augment the node features, but the model retains the usual message passing procedure of GNNs. These last two models are of particular relevance to the present work, since they utilise information about chemical rings. It is important to remark that CWNs compare favourably with both of them in all our benchmarks. 

\paragraph{Higher-order GNNs} A related line of work has studied lifting graphs into -dimensional tensor representations that can be processed by provably expressive -GNNs \citep{maron2018invariant, maron2019provably, azizian2021expressive}. With higher values of , these models achieve higher-expressivity, but due to the computational complexity this incurs, values of  are of little use in practice. Therefore, unlike CWNs, these models cannot explicitly represent in practice chemical rings of common sizes (e.g. five or six). Furthermore, by being upper-bounded by 3-WL, the 2-GNN models cannot count the number of induced cycles of size greater than four (see Appendix \ref{app:proofs_cellular} for details). In contrast, CWNs can easily count these important chemical substructures through the readout operation it performs on the 2-cells. 

\paragraph{Limitations} 
The main limitations of the model are of computational nature. While the computational complexity of the message passing procedure and its preprocessing step is suitable for molecular and geometric graphs, the number of rings (and more generally simple cycles) in general graphs can be exponential in the number of nodes. In that case, one has to resort to smaller 2-cells like triangles, which can be found efficiently in general graphs. Moreover, one has to typically use weights specific for each dimension of the cell complex, increasing the number of parameters compared to GNNs. However, we have shown that our model can compensate this increase with a reduced number of layers and still achieve state-of-the-art results on some of the molecular benchmarks. 

From a theoretical point of view, this work is concerned only with \emph{regular} cell complexes. Adopting this restriction is useful from multiple perspectives: regular cell complexes are easier to analyse, their combinatorial structure completely describes their topology and convolutions can be defined on them through the Sheaf Laplacian (see Appendix \ref{app:convs}). Nonetheless, some of our theoretical results could be extended to non-regular complexes, which could be obtained by lifting transformations not studied in this work, such as attaching 2-cells to paths in the graph. We leave addressing non-regular complexes and their trade-offs to future developments of this work.   

\paragraph{Societal Impacts} Most of our paper is theoretical in 
nature and we do not see immediate direct negative societal impacts. Within the scope of social network applications, we do not yet have sufficient evidence of performance improvement on related benchmarks to justify obvious adoption in such a domain. In contrast, the empirical performance on molecular benchmarks suggests it may have a positive impact on applications of immediate interest in pharmaceutics, such as drug discovery~\citep{drug_discovery}.



\paragraph{Conclusion} We have proposed a provably powerful message passing procedure on cell complexes motivated by a novel colour refinement algorithm to test their isomorphism. This allows us to consider flexible lifting operations on graphs to implement more expressive architectures which benefit from decoupling the computational and input graphs. Our methods show excellent performance on diverse synthetic and real-world molecular benchmarks. 

\section*{Funding and Acknowledgements}

YW and GM acknowledge support from the ERC under the EU's Horizon 2020 programme (grant agreement n\textsuperscript{o} 757983). MB is supported in part by ERC Consolidator grant n\textsuperscript{o} 724228 (LEMAN). The authors declare no competing interests. We are also grateful to Ben Day, Gabriele Corso and Nikola Simidjievski for their helpful feedback. We would also like to thank Vijay P. Dwivedi and Chaitanya K. Joshi for clarifying certain aspects of their Benchmarking GNNs \citep{dwivedi2020benchmarkgnns} work, and to Muhammet Balcilar for signalling a numerical precision issue in early SR graphs experiments. 

\bibliographystyle{plainnat}
\bibliography{references}



















\newpage 

\appendix

\section{Proofs}
\label{app:proofs_cellular}
\subsection{Cellular WL Results}

In this section, we assume basic familiarity with the WL test and its higher-order variants. For an introduction to these topics, we refer the reader to the survey of \citet{sato2020survey}. We begin by introducing a few useful concepts. 

\begin{definition}
A \textbf{cellular colouring} is a map  that maps a cell complex  and one of its cells  to a colour from a fixed colour palette. We denote this colour by . 
\end{definition}

\begin{definition}
Let  be two regular cell complexes and  a cellular colouring. We say that  are \textbf{-similar}, denoted by , if the number of cells in  coloured with a given colour equals the number of cells in  with the same colour. Otherwise, we have .  
\end{definition}

We emphasise that in this paper we are interested only in colourings  with the property that any two isomorphic cell complexes are -similar. 

\begin{definition}
A cellular colouring  \textbf{refines} a cellular colouring , denoted by , if for all cell complexes  and 
and all  and ,  implies . Additionally, if , we say the two colourings are equivalent and we represent it by . 
\end{definition}

We state the following result from \citet{bodnar2021weisfeiler} about simplicial colourings, which we translate here directly to cell complexes. The proof is however, identical, and we refer the reader to their work for that. 

\begin{proposition}
\label{prop:refine_multiset}
Let  be any regular cellular complexes with  and . Consider two cellular colourings  such that . If , then .
\end{proposition}

\begin{corollary}
\label{cor:non_iso_colour}
Consider two cellular colourings  such that . For all cell complexes  and , if , then . 
\end{corollary}

This last result implies that if  refines , then  can distinguish all the non-isomorphic cell complexes that  can distinguish. We say that the colouring  is at least as powerful as the colouring . 

In contrast to simplicial complexes, cell complexes have a more flexible structure. The main complication compared to the proofs in \citet{bodnar2021weisfeiler} is that cells can have a variable number of lower-dimensional cells on their boundary. It is therefore useful in many proofs, to separate the cells into buckets containing cells with the same boundary size. The following result helps us do that. 

\begin{proposition}
\label{prop:cwl_face_id}
Let  be the CWL colouring at iteration . For all cells  in any cell complexes  and , if , then for any  we have .  
\end{proposition}

\begin{proof}
If  and  have boundaries of different sizes, then , which immediately implies  for all . 
\end{proof}

Next, we show that one can drop the co-boundary adjacencies without sacrificing expressive power.

\begin{lemma}
\label{lemma:drop_cofaces}
    CWL with  is as powerful as CWL with the generalised update rule .
\end{lemma}

\begin{proof}
Let  denote the colouring produced by CWL using the general version and  the colouring produced using the restricted version at iteration . It can be verified that  because it considers the additional  colours in the refinement rule. We  now prove  by induction. Note that to take advantage of Proposition \ref{prop:cwl_face_id}, we shift the time-step by one (i.e. we use  as opposed to ). 

The base case holds since  assigns the same colour to all the cells. Suppose  for any two cells  and  from any cell complexes  and , respectively. Then we know that  and . The goal is to show that this also implies that .

Given , by definition 
 
By Proposition \ref{prop:cwl_face_id}, cells with different boundary sizes have different colours. Therefore, we can partition these two multi-sets by the size of the cell boundaries, while preserving the equality between these sub-multisets. Therefore, for each :  

Let  be an arbitrary cell. Then for each cell ,  exchanges messages with all the other boundary cells of . Therefore, the colour of each  with  shows up with a multiplicity of  in the tuples of . Eliminating  of these repeated colours for all  and : 

Merging these in a single multi-set gives the colours of the co-boundary cells:

By the induction hypothesis,  and . This implies . 
\end{proof}

The following theorem shows that we can further prune the CWL update rule by removing the colours associated with the lower adjacencies. The structure of the proof is similar to the one in \citet{bodnar2021weisfeiler}, with the main difference being in the proof of Proposition \ref{prop:partition_low_adj}. 


\begin{proof}[\textbf{Proof of Theorem~\ref{thm:sparse_cwl}}]
Let  denote the colouring of CWL using  and  the colouring of CWL using the rule  from  Lemma~\ref{lemma:drop_cofaces}. Trivially  because of the additional argument  in the update rule. We prove  by induction. As before, the addition by one in  is to allow us to apply Proposition \ref{prop:cwl_face_id} in the induction step. The multiplication by  is due to the fact that the information transmitted through the lower adjacencies in one step is propagated in two steps through the boundary adjacencies.  

As before, the base case trivially holds since  assigns the same colour to all cells. Suppose . By unrolling the hash function two steps in time, we obtain , , and . We need to prove that  also holds. For the sake of contradiction, assume . Then there exists a pair of colours  that shows up (without loss of generality) more times in  than in . For simplicity, we also assume  as this special case can be easily treated separately. 

For all cell complexes  and all cells  in , consider the collection of multi-sets  indexed by :

We are interested in the size of these multi-sets for some specific cells . To that end, for each cell , we define the multi-set:

We know that  since the sum of the elements of , which gives the number of tuples  in , is greater than the sum of the elements of , which gives the number of tuples  in . We prove this contradicts our hypothesis that . 

\begin{proposition}
\label{prop:partition_low_adj} 
For all regular cell complexes  and all , if , then . 
\end{proposition}

\begin{proof}
Given a cell complex  and a cell , consider the cellular colouring . The idea of the proof is to show that , which allows us to use Proposition \ref{prop:refine_multiset} for the multi-sets  and . 

Let  be two arbitrary cells from any regular cell complexes  such that . Assume without loss of generality that . Two cases can be distinguished for this inequality. In the first case, , which implies  and, therefore, . Then . 

In the second case, , which implies  and . Then, the difference in the size of the multi-sets is made by the number of times  shows up in  and , respectively. By Proposition \ref{prop:cwl_face_id}, all -cells  with  and  must have a fixed boundary size . Because each cell  is upper adjacent with every other cell in ,  appears  times in the tuples inside . Additionally, note that since the cell complex is regular, self-loops are not allowed and, therefore, . 

Applying this to  and ,  shows up  times in  and  times in . Therefore,  and, similarly to the first case, . The results obtained for the two cases prove .

Applying Proposition \ref{prop:refine_multiset} for the multi-sets  and , we obtain two non-equal multi-sets:

Since these two multi-sets are used in the colour updating rule, . 
\end{proof}

Therefore, . Finally, applying the induction hypothesis, we have that  and  . Then . 
\end{proof}

\begin{proof}[\textbf{Proof of Theorem~\ref{thm:skeleton}}]
Consider the map , a skeleton-preserving lifting transformation from the space of graphs , to the space of regular cell complexes . Let  be the graph isomorphism associated to  between the vertices of  and the 0-cells of  for all . Let  be the WL colouring of graph  at iteration  and  the colouring of  induced by the isomorphism  (i.e ) at the same time step . 

Because  and  are isomorphic as graphs and WL is invariant under isomorphism, . It follows that for all graphs , if  then . Let  be the CWL colouring of the 0-cells at iteration . The goal is to show that for all regular cell complexes , . By transitivity and combined with Corollary \ref{cor:non_iso_colour}, it follows that if , then . 

The base case trivially holds. Let  be two 0-cells in  and , respectively such that . Since 0-cells have only upper adjacencies, the equality implies that  and . The latter multi-set equality further implies

Equivalently, for 0-cells of a cell complex whose 1-skeleton is a graph (i.e. not a multi-graph), this can be rewritten as

By the induction hypothesis we have  and

These equalities imply .
\end{proof}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.44\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/clique_complex_wl.jpeg}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cycle_complex_wl.jpeg}
    \end{subfigure}
    \caption{(Left) A pair of non-isomorphic graphs indistinguishable by WL, but distinguishable by CWL with a clique complex, ring or cycle-based lifting. (Right) A pair of non-isomorphic molecular graphs (Decalin and Bicyclopentyl) indistinguishable by WL but distinguishable by CWL with a ring-based or cycle-based lifting. The node colours show the stable colouring reached by WL. }
    \label{fig:wl_corollary}
\end{figure}

\begin{proof}[\textbf{Proof of Corollary~\ref{cor:WL_lifting_maps}}]
Due to Theorem \ref{thm:skeleton}, it is sufficient to find some examples of non-isomorphic graph pairs that WL cannot distinguish, but CWL can with the given lifting transformations. Figure \ref{fig:wl_corollary} includes such examples. Based on Proposition \ref{prop:cwl_face_id}, CWL can distinguish these graphs since it can count the number of substructures (e.g. triangles, rings, cycles) that the lifting is based on. 
\end{proof}

The next proposition shows that CWL can identify cells that are -simplices. 

\begin{proposition}[Simplex Identification]
\label{prop:cwl_simplex_identification}
Let  be regular cell complexes and  two cells. Denote by  the CWL colouring at iteration . Suppose  is an -simplex and  is not.  Then  for all .
\end{proposition}

\begin{proof}
The base case holds since  if  is a vertex and  is a cell of another dimension. This is because  has no boundary adjacencies, while  does. 

Suppose the statement holds for -simplices. Then, an -simplex can be identified by having  -simplices on its boundary. By Proposition \ref{prop:cwl_face_id}, the colour of  encodes the boundary size. Furthermore, by the induction hypothesis  encodes the fact that the boundary cells are -simplices. 
\end{proof}

\begin{proof}[\textbf{Proof of Theorem~\ref{thm:lifting3WL}}]
The sub-results of the theorem can be proven by finding pairs of graphs from the same family of Strongly Regular Graphs that can be distinguished by CWL with the corresponding lifting transformations. Graphs in this family are provably indistinguishable by the higher-order 3-WL test~\citep{bodnar2021weisfeiler}. 

\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{figures/sr_pair_r-4-10.pdf}
    \caption{The two SR graphs in family SR: Rook's 44 (left) and Shrikhande (right). The -WL test is not able to deem them as non-isomorphic. Contrary to the Shrikhande graph, Rook’s graph possesses -cliques. The Shrikhande graph, however, features -rings, not present in Rook’s. Instances of these substructures are marked in blue. With appropriate lifting procedures, CWL can distinguish between them.} 
    \label{fig:sr_pair}
\end{figure}

\paragraph{Ring-based lifting} We can show that there is a pair of SR graphs in the same family with a different number of induced cycles of a certain size. We include such an example in Figure \ref{fig:sr_pair}. The two graphs differ in the number of -, -, - and -rings (see Table~\ref{tab:sr_ring_counts}), which indirectly proves 3-WL cannot count induced cycles of these sizes. It is also natural to conjecture that 3-WL cannot count induced cycles of size strictly larger than . In contrast, CWL(-) is sufficient to distinguish these two graphs. 

\paragraph{Clique complex lifting} We can leverage on the same example: the graph on the right does not possess -cliques, contrary to the graph on the left (one such example is marked in blue). This proves that 3-WL cannot count cliques of size . As shown by \citet{bodnar2021weisfeiler}, this result immediately implies that SWL (and consequently CWL) with a clique complex lifting is not less powerful than 3-WL.

\begin{table}[h!]
    \centering
    \caption{Number of cycles and induced cycles (rings) on the SR graphs in family SR.}
    \label{tab:sr_ring_counts}\vspace{1mm}
\begin{tabular}{l|cccccc}
    \toprule
    Graph  / Size  & 
         (Tri.) & 
         &
         &
         &
         &
         \\
    \midrule
    Rook's 44 (cycles)&
        32 &
        60 &
        288 &
        1,248 &
        4,032 &
        11,952 \\
    Shrikhande (cycles)&
        32 &
        60 &
        288 &
        1,248 &
        4,032 &
        11,688 \\
    \midrule
    Rook's 44 (rings)&
        32 &
        36 &
        0 &
        96 &
        0 &
        72 \\
    Shrikhande (rings)&
        32 &
        12 &
        96 &
        64 &
        0 &
        36 \\
    \bottomrule
    \end{tabular}
\end{table}


\paragraph{Cycle-based lifting} To prove the result for this lifting transformation we leverage on a result by \citet{ARVIND202042}, who show that 2-Folklore WL (which is equivalent to 3-WL \citep{sato2020survey}) cannot count subgraph cycles of size strictly larger than 7. Table \ref{tab:sr_ring_counts} illustrates this for the same example as above. Since CWL can count the number of 8-cycles when the lifting transformation - with  is used (see Proposition~\ref{prop:cwl_face_id}), this proves the result. 

\end{proof}

We note that while the proof above is purely based on substructure counts, the superior expressive power of CWL is very likely not limited to counting the substructures involved in the lifting transformation. We have seen evidence in favour of this claim in the SR experiment in Section \ref{sec:results}, where message passing layers reduced the failure rate. 

Next, we prove a statement comparing Simplicial WL and CWL. This will later be used to show that CWNs are strictly more powerful than MPSNs when a lifting transformation based on the clique complex and rings is used.  

\begin{definition}
A subset  of a cell complex  is called a \textbf{subcomplex} if it is a union of cells of  containing the closures of these cells.
\end{definition}

\begin{theorem}
Let  be a skeleton-preserving transformation such that for any graph , the clique complex of  is a subcomplex of . Then CWL() is at least as powerful as SWL using the clique complex lifting at distinguishing non-isomorphic graphs. 
\end{theorem}

\begin{proof}

Let  be the simplicial colouring performed by SWL. We can extend it into a cellular colouring  defined as follows:

where  is the maximal simplicial complex that is a subcomplex of  and  is a special colour assigned to the cells that are not simplices. Let  be the clique-complex lifting map. Then, it is easy to see that for all graphs , if , then . Let  be the CWL colouring map at iteration . We aim to show that  by using Proposition \ref{prop:cwl_simplex_identification}. Then, by transitivity and using Corollary \ref{prop:refine_multiset}, if , then . 

Let  be the maximum dimension of the cells used by the lifting transformation . As usual, the base case holds at initialisation since  assigns the same colour to all the cells. Let  be two cells from the regular cell complexes . When  and  are not simplices, then . Suppose  and  are both simplices and . Then we know that  and . Since  and  are simplices, their boundary cells are also lower-dimensional simplices, so by induction hypothesis, .

Let us consider the equality between the colours involving the upper adjacent cells. By expanding the definition we have: 

Generally, not all of these adjacencies involve simplices. For instance, a 2-simplex could incident to a general 3-cell. However, by Proposition \ref{prop:cwl_simplex_identification} this equality must still hold if we restrict the multi-sets to the colour of those cells that are simplices: 

These multi-sets, give exactly the upper adjacencies used by SWL for computing its colouring map . Therefore, by the induction hypothesis, . Finally, this proves . 
\end{proof}

\begin{corollary}
\label{cor:cwl_better_than_swl}
CWL(--) and CWL(--) are strictly more powerful than SWL(-) for all . 
\end{corollary}

\begin{proof}
The second pair of graphs from Figure \ref{fig:wl_corollary} cannot be distinguished by SWL(-) because it has no cliques greater than two, but it can be distinguished by CWL with the liftings above because of the different number of (induced) cycles. 
\end{proof}

\subsection{CW Network Proof} 

\begin{proof}[\textbf{Proof of Theorem~\ref{thm:CWandCWN}}]
Let  denote the colouring of CWL at iteration  and  the colouring (i.e. features) produced by a CW-Network as described in Section \ref{sec:CWN_MMP}. Without loss of generality (Theorem \ref{thm:sparse_cwl}), we use only boundary and upper adjacencies for both methods. 

To show CWNs are at most as powerful as CWL, we must show . Again, we show this by induction. For a CWN with  layers we assume  for all .
Let  be two cells with . Then, ,  and . By the induction hypothesis, ,  and . 

If , then . Otherwise,  is given by Equation \ref{eq:cwn_update} involving the update function , the aggregate function AGG and the message functions . Given that the inputs passed to these functions are equal for  and , . 

We now prove that CWNs can be as powerful as CWL. Suppose the aggregation from Equation \ref{eq:cwn_update} is injective and the model is equipped with a number of layers  sufficient to guarantee the convergence of the  colouring. Then, we show that . 
Let  be two cells with . Then, since the local aggregation is injective ,  and . By the induction hypothesis, ,  and . Finally, .
\end{proof}

The consequence of this result is that CWNs inherit all the properties of CWL. We summarise these in the following Corollary. 

\begin{corollary}
CWNs have the following properties:
\begin{enumerate}[leftmargin=*]
    \item They are at least as powerful as the WL test when using skeleton-preserving lifting transformations. 
    \item They are strictly more powerful than the WL test when using the lifting maps from Corollary \ref{cor:WL_lifting_maps}.
    \item They are not less powerful than 3-WL when using the lifting transformations from Theorem \ref{thm:lifting3WL}. 
    \item They are at least as powerful as MPSNs using the clique complex lifting \citep{bodnar2021weisfeiler} when using a lifting transformation whose output complexes have the clique complex as a subcomplex. 
    \item They are strictly more powerful than MPSNs when using a transformation attaching cells to cliques and rings/cycles. In particular, CWNs using rings are strictly more powerful than MPSNs using a lifting based on triangles (i.e. 2-simplices), since triangles are rings of size 3. 
\end{enumerate}
\end{corollary}

The latter point regarding triangles is important because \citet{bodnar2021weisfeiler} do not use simplices of dimension higher than two in practice.  

\subsection{Equations for Other Adjacencies}

For completeness, we include in this section the equations for the co-boundary and lower adjacent messages.

Together with the adjacencies described in the main text, the update rule takes the form  

As mentioned before, even though these adjacencies are redundant from a colour refinement perspective when the others are used, they might still be employed in other combinations that preserve the expressive power of the test. Additionally, for certain applications, they might still encode important inductive biases. 

\section{Computational Analysis}
\label{app:complexity}

Let  be a -dimensional regular cell complex. For an arbitrary -cell  with boundary size , the  number of -messages between the -cells on its boundary is  and the number of -messages it receives is . Let  be the maximum boundary size of a -cell in  and  the number of -cells. The computational complexity of our message passing scheme is thus . For instance, consider the skeleton-preserving lifting based on induced cycles. There, the dimension of the complex is  and we have , and  equals the size of the maximum induced cycle considered. For all practical purposes, we can consider  and  as fixed constants. Then the complexity can be rewritten as . This is optimal because the complexity is linear in the size of the cell complex and a linear time is required to read the cell complex. 


\begin{table}[h!]
    \centering
    \caption{Wall-clock training and evaluation times on ZINC; mean, std over  runs (seconds).}
    \label{tab:zinc_training_time}
    \vspace{1mm}
\begin{tabular}{l|cccc}
    \toprule
    Model & 
        Training (Epoch) & 
        Eval (Train) &
        Eval (Val) &
        Eval (Test) \\
    \midrule
    GIN &
        4.582  0.012 &
        3.138  0.071 & 
        0.310  0.002 &
        0.309  0.001 \\
    GIN-small &
        3.737  0.012 &
        3.070  0.058 &
        0.304  0.002 &
        0.303  0.003 \\
    \midrule
    CIN &
        10.828  0.059 &
        4.679  0.051 &
        0.470  0.002 &
        0.471  0.003 \\
    CIN-small &
        7.082  0.041 &
        3.682  0.056 &
        0.365  0.002 &
        0.373  0.030 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Wall-clock training and evaluation times on ZINC-FULL; mean, std over  runs (seconds).}
    \label{tab:zincfull_training_time}
    \vspace{1mm}
\begin{tabular}{l|cccc}
    \toprule
    Model & 
        Training (Epoch) & 
        Eval (Train) &
        Eval (Val) &
        Eval (Test) \\
    \midrule
    GIN &
        106.268  1.991 &
        73.051   1.742 &
        7.874   0.174 &
        1.618  0.039 \\
    GIN-small &
        87.581  2.343 &
        71.160  1.865 &
        7.714  0.206 &
        1.583  0.037 \\
    \midrule
    CIN &
        249.334  17.927 &
        107.510  1.637 &
        11.759  0.642 &
        2.398  0.028 \\
    CIN-small &
        163.282  8.016 &
        85.342  2.637 &
        9.251  0.431 &
        1.876  0.044 \\
    \bottomrule
    \end{tabular}
\end{table}

In practice, we observed the empirical training runtimes to be contained, even on the largest benchmarks. We performed timing analyses on ZINC and ZINC-FULL, measuring the time required to complete one training epoch and a full performance evaluation on train, validation and test sets. We report the runtimes in Tables~\ref{tab:zinc_training_time} and~\ref{tab:zincfull_training_time} the runtimes measured for our best performing CIN and CIN-small models and by GIN baselines with, approximately, the same number of parameters. We observe that the evaluation runtimes are relatively comparable to those of GIN models and that the difference decreases significantly at inference time (i.e. no backprop). The training runtimes are significantly reduced on CIN-small architectures, which always perform on-par or even better than state-of-the-art baselines, regardless of the imposed parameter budget (see Table~\ref{tab:mol_dataset}). These experiments where run over an NVIDIA\textsuperscript{\textregistered} Tesla V100 GPU device on an Amazon Web Services (AWS) Elastic Cloud (EC) 2 \texttt{p3.16xlarge} instance.

Other than the computational complexity of message passing we need to consider the (one-off) complexity pertaining the graph lifting procedures. Lifting procedures that are more likely to find immediate practical applications involve clique, cycle and induced cycle listing. For cliques, we refer readers to \citet{bodnar2021weisfeiler}, where the authors report theoretical results regarding clique-listing complexity and the practical impact of employing specialised topological data analysis libraries. 

As for cycle-based liftings, specialised cycle-listing algorithms exist. The algorithm in \citet{birmele2013optimal} is able to list all simple cycles in a graph in , where  is the number of edges,  is the set of simple cycles in graph  and  is the size of the cycle. As for \emph{induced} cycles, the algorithm presented in \citet{ferreira2014amortized} has a listing time of , with  and  being the number of nodes and induced cycles, respectively. In certain types of graphs, a better complexity can be obtained. In the case of planar graphs, \citet{chiba1985arboricity} show linear time complexity to list triangles and quadratic complexity for 4-rings. This is very important because almost all molecules are planar in a graph-theoretic sense \citep{SIMMONS1981287} as a direct consequence of the chemical implications of Kuratowski's theorem \citep{kuratowski1930probleme}. However, we are not aware of any improved bounds for finding general induced cycles in planar graphs. Finally, we remind the reader that molecular rings can also be listed from the junction tree representation~\citep{jin2018junction, Fey2020_himp}, obtained by specialised molecular libraries such as RDKit~\citep{Landrum2016RDKit2016_09_4}.

\begin{table}[h!]
    \centering
    \caption{Wall-clock lifting times, mean and std over  runs (seconds).}
    \label{tab:wallclock_lift}
    \vspace{1mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ l|cccccc}
    \toprule
    Dataset  / Processes  & 
        Seq. & 
         &
         &
         &
         &
         \\
    \midrule
    ZINC (12k) &
        320.27  0.54 &
        169.95  0.32 & 
        84.90  0.21 &
        43.38  0.07 &
        23.17  0.68 &
        18.59  0.68 \\
    Mol-HIV (41k) &
        1178.98  3.90 &
        635.58  0.83 &
        319.01  0.40 &
        164.26  0.52 &
        86.92  0.77 &
        60.62  2.05 \\
    ZINC-FULL (250k) &
        6805.35  16.50 &
        3549.16  7.73 &
        1782.41  3.84 &
        918.38  3.46 &
        492.77  6.13 &
        383.92  3.30 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

In our experiments, we implemented a lifting procedure based on the \emph{generic} substructure matching algorithm exposed by the graph-tool Python library, which internally employs VF2~\citep{cordella2004asub} to perform subgraph isomorphism. Noticing that the lifting procedure is embarrassingly parallel w.r.t. the independent graphs in a dataset, we easily parallelised the procedure via Python's Joblib library. On molecular benchmarks we observed the effective time required by preprocessing routines to always be modest compared to the training times. In Table~\ref{tab:wallclock_lift} we report the wall clock runtimes, averaged over  runs, to lift all the graphs in the largest datasets amongst our benchmarks: ZINC, Mol-HIV and ZINC-FULL. The analysis has been conducted considering rings up to size  and by varying the number of parallel processing jobs on a server with an Intel\textsuperscript{\textregistered} Xeon E5-2686 v4 processor with  vCPUs. It is possible to observe that the empirical lifting runtime scales linearly with the number of jobs in the range , and that such a simple parallelisation scheme dramatically reduces the preprocessing time on all datasets. When employing  parallel jobs, less than  seconds are required to preproceess the whole ZINC dataset, only  minute is required for Mol-HIV, and we needed slightly more than  minutes to lift all the k graphs in ZINC-FULL. We remark once more that these experiments have been conducted with a \emph{generic} subgraph matching algorithm, and that even more parsimonious computation would be possible by using optimised ring-listing routines.

\section{Symmetries}
\label{app:symmetries}

In line with a recent effort in Geometric Deep Learning to understand different models through the lens of symmetry \citep{Bronstein_etal2017}, we aim here to give a description of the underlying equivariance properties of CW Networks.  

First, let us define the following matrix representation of the boundary relation from Definition \ref{def:boundary_rel}.

\begin{definition}
Let  be a regular cell complex with  denoting the number of cells in dimension . The -th unsigned boundary matrix  of  is given by  if  and , otherwise. 
\end{definition}

Let  be a regular cell complex of dimension  with boundary matrices  and feature matrices  for the cells of different dimensions. Additionally, consider a sequence of permutation matrices . Denote by  and .

\begin{definition}
\label{def:equiv}
A function  mapping  with the property that  for any  is called cell permutation equivariant. 
\end{definition}

\begin{proof}[\textbf{Proof of Theorem~\ref{thm:CWequivariant}}]
Definition \ref{def:equiv} is similar to the (simplex) permutation equivariance definition from \citet{bodnar2021weisfeiler}, with the subtle difference that the boundary matrices now have a more flexible structure in the case of cell complexes. The high-level idea is to see that all the adjacency matrices used by CWNs (i.e. ) are permuted accordingly by the permutation matrices in . Therefore, CWNs layers computes the same function up to a permutation of the cells. The proof follows a similar logic to to the one in \citet{bodnar2021weisfeiler} for simplicial networks, and we refer the reader to their work for a detailed proof.   
\end{proof}


It is common in algebraic topology and differential geometry to equip the incidence relation  with additional structure that makes it a signed incidence relation.  

\begin{definition}[\citet{HGh19}]
\label{def:signed_inc}
A signed incidence relation on  is a map  with the properties:
\begin{enumerate}[leftmargin=*]
    \item If , then . 
    \item For any , 
\end{enumerate}
\end{definition}

This signed incidence relation can be be encoded by the signed incidence (boundary) matrices of . We define these below:

\begin{definition}
Let  be a regular cell complex with a signed incidence relation . Let  denote the number of cells in dimension . The -th signed boundary matrix  of  is given by . 
\end{definition}

The difference with respect to the unsigned boundary matrices is that the non-zero values of the matrix can be , not just . This can be used to define a notion of orientation equivariance for CW Networks. This ensures that when changing the orientation of the cell complex  (i.e. changing ) one computes the same function up to that change in orientation. 

Let  be a regular cell complex of dimension  described by the \emph{signed} boundary matrices  and feature matrices  for the cells of different dimensions. Additionally, consider a sequence of diagonal matrices  with values in . Additionally, let . Denote by  and .

\begin{definition}
A function  mapping  with the property that  for any  is called  orientation equivariant. 
\end{definition}

Making CWNs orientation equivariant requires imposing additional constraints on the layers of the model. This proceeds similarly to MPSNs \citep{bodnar2021weisfeiler}. Since applications involving oriented simplicial complexes are out of the scope of this work, we refer the reader to \citet{bodnar2021weisfeiler} for an intuition of how this can be extended to cell complexes. 

\section{Sheaves, Laplacians and Convolutions}\label{app:convs}

It is useful on cell complexes to derive a Laplacian operator based on cellular sheaves \citep{HGh19}, since many interesting Laplacians, such as the (normalised) graph Laplacian \citep{ChGr1997}, the Hodge Laplacian \citep{schaub2020random} and the connection Laplacian \citep{SW12} can be obtained as particular cases. Intuitively, a cellular sheaf is a construction that assigns a vector space to each cell in the complex and a (linear) map for each face relation in the complex . Additionally, these linear maps must satisfy some compositionality constraints imposed by the structure of . 

\subsection{Sheaf Laplacian}\label{A:sheaf laplacian}


\begin{definition}
Let  be a regular cell complex, and denote by  the class of Hilbert spaces over a field . A \define{weighted cellular sheaf}  is given by the assignment 

together with a bounded linear map  for any .

This data satisfies that  for all  and  whenever .
\end{definition}

Given a weighted cellular sheaf , we define a chain complex as follows. For each  we set 

Further, we define coboundary maps  by 

where  is a signed incidence relation (see Definition \ref{def:signed_inc}).

Given Hilbert spaces  and  and  a bounded linear map , the adjoint of  is the unique bounded linear map  satisfying that for all  and all :



\begin{definition}
Let  be a chain complex of Hilbert spaces. The \define{Hodge Laplacian} is the graded linear map defined in degree  as  with . 
When  is the complex of cochains of a weighted cellular sheaf , the Hodge Laplacian is called the \define{sheaf Laplacian} of .
\end{definition}

In particular, the Hodge Laplacian of a cell complex can be obtained by considering the \emph{constant weighted cellular sheaf} with a standard inner product. That is the cellular sheaf where  and the restriction maps . A normalised version of it can also be obtained by carefully adjusting the inner products associated with each . This normalisation is always possible for finite cell complexes (see \citet{HGh19} for details). This is very useful because finding normalised versions of Hodge Laplacians is not trivial and even on simplicial complexes \citep{schaub2020random}, the process of constructing one can be quite involved. 


\subsection{Convolutional Operators}

One can use the general sheaf Laplacian to define linear, local diffusion operators which, in the GNN literature, are broadly addressed as `convolutional'. Diffusion operators built from the standard graph Laplacian have been employed in several graph neural network architectures \citep{defferrard2016convolutional, kipf2017graph}. Recent works~\citep{ebli2020simplicial, bunch2020simplicial} have introduced convolutional operators on SCs by employing the Hodge Laplacian \citep{schaub2020random}, interpreted as a generalisation of the graph Laplacian. As for cell complexes, here we focus, for simplicity, on the case of a constant sheaf with a standard inner product in . Then, the matrix representations of  and  are the signed incidence matrices  and , respectively. Therefore, the Hodge Laplacian can be written in matrix form as  

A convenient way to define a convolutional operator on cochains is by designing a learnable filter parameterised as a polynomial of the Hodge Laplacian. This approach has been already adopted on graphs using the standard graph Laplacian~\citep{defferrard2016convolutional} or more general sheaf Laplacians~\citep{hansen2020sheaf}, and also on simplicial complexes~\citep{ebli2020simplicial}. The advantage of this approach is that of retaining a connection with spectral constructions~\citep{defferrard2016convolutional, ebli2020simplicial} while not requiring any explicit diagonalisation of the operator itself. A polynomial convolutional filter of this kind, when applied to the -cells of a -cell complex, would take the form

\noindent where  is a matrix gathering -cell representations at layer ,  are learnable parameters, and  summarises the application of a bias term and a non-linearity.

\begin{proof}[\textbf{Proof of Theorem \ref{thm:CWequivariant}}] While the structure of the boundary matrices is more flexible in a cell complex than in a simplicial complex, algebraically, the proof is very similar to the proof showing MPSNs generalise simplicial convolutions in \citet{bodnar2021weisfeiler}. We offer here a high-level view of the proof and refer the reader to Appendix C of their paper for a detailed version. 

For a generic -cell , and , the application of the -power of the Hodge Laplacian effectively induces an information flow from a generalised notion of -upper and -lower adjacent -cells, i.e. -cells  such that there exists a sequence of upper- (respectively, lower-) adjacent -cells  such that .

Therefore, the convolution described above is easily interpreted in terms of a cellular message passing scheme which only exchanges - and -messages. Intuitively, the upper- and lower- message functions would share their parameters  and compute messages by linearly projecting the representations of upper- and lower-adjacent cells (ignoring any information in shared (co)boudaries). Such messages would then be aggregated by summation into an overall message, taken as input by an update function parameterised by  and . A formal derivation of how the equation~\eqref{eq:cell_conv} is rewritten in terms of cellular message passing would closely follow the one provided in~\citet{bodnar2021weisfeiler} for SCs, and we therefore refer readers to Section C of such work. \end{proof}

Normalised versions of the aforementioned Hodge Laplacian can be used to design a model in the spirit of the popular Graph Convolutional Network of~\citet{kipf2017graph}. To this aim, one could resort to normalised sheaves as suggested in~\cite{HGh19}. Additionally, one could explicitly make use of the (co)boundary operators defined in Section~\ref{app:symmetries} to let information flow from lower- and higher- dimensional cells contained in cell (co)boundaries, effectively extending the Simplicial Convolutional Networks recently introduced in~\citet{bunch2020simplicial}. We defer these research directions to future developments of this work.


\section{Experimental details and additional results}\label{app:results}

\subsection{Used Code Assets} 

The model has been implemented in PyTorch~\citep{NEURIPS2019_9015} and by building on top of the PyTorch Geometric library~\citep{fey2019fast}. Lifting operations use the graph-tool\footnote{\url{https://graph-tool.skewed.de/}} Python library and are parallelised via Joblib\footnote{\url{https://joblib.readthedocs.io/en/latest/}}.
PyTorch, NumPy, SciPy and Joblib are made available under the BSD license, Matplotlib under the PSF license, graph-tool under the GNU LGPL v3 license. PyTorch Geometric is made available under the MIT license.

\subsection{Used Computer Resources}
All experiments were run on NVIDIA\textsuperscript{\textregistered} GPUs. Experiments on \textbf{SR}, \textbf{Mol-HIV} and molecular \textbf{TUDatasets} were run on Tesla V100 GPUs with 5,120 CUDA cores and 16GB GPU memory on a \texttt{p3.16xlarge} Amazon Web Services (AWS) Elastic Cloud (EC) 2 instance. Experiments on the social \textbf{TUDatasets} were run on the same GPU devices but with 32GB HBM2 memory mounted on an HPC cluster. All remaining experiments, that is \textbf{CSL}, \textbf{RingTransfer} and \textbf{ZINC}, were run on a machine with TITAN Xp GPUs with 12GB GPU memory and an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU E5-2630 v4 @ 2.20GHz CPU.

\subsection{Model}

In all cases, we apply our model to the -dimensional cell complexes obtained by ring-lifting the original graphs, i.e. we consider nodes and edges as - and -cells, and each induced cycle of size up to  as a -cell. -cell are always endowed with the original node features or learnt node embeddings, if the benchmark prescribes so. The way higher dimensional cells are assigned features depend on the specific benchmark.

Throughout all experiments, we employ cellular message passing layers which update the representation of -cell  as follows:


Here,  indicates concatenation,  are 2-Layer Perceptrons and ,  consist of a dense layer followed by a non-linearity. We neglect messages from cofaces and down-adjacent cells consistently with Theorem~\ref{thm:sparse_cwl}. We name an architecture which stacks  layers of this form as `Cell Isomorphism Network' (CIN). Readout operations are performed as follows. First, for , we compute the joint representation  of the cells at dimension  by applying a mean or sum readout operation. Then, for complex , we compute an overall representation , where each  is parameterised as a single dense layer followed by a non-linearity. Complex-wise predictions are obtained by a final dense layer preceded by dropout. All MLP layers internally apply Batch Normalization~\citep{BN} and ReLU activations, unless otherwise specified. All training procedures are performed with the Adam optimiser~\cite{kingma2014adam}.

\subsection{Additional experimental details}

\paragraph{CSL} Each of the  -regular graphs in the CSL dataset comprises  nodes and is characterized by \emph{skip number} parameter . Parameters  and  determine the isomorphism class  of each graph, which we seek to predict. The number of possible classes is . We employ the same stratified dataset folds in~\citet{dwivedi2020benchmarkgnns}. Consistently with the adopted reference procedure, -cells share the same learnt embedding, while - and -cells are endowed with the sum of the embeddings of the included -cells. As for the optimisation procedure, we set the batch size to  and the initial learning rate of 5\tte-4. which is halved whenever the validation performance does not improve after a patience value of . The training is early stopped as soon as it falls below 1\tte-6, at which step we measure the model test accuracy. The size of hidden layers in our model is set to  and we stack  cellular message passing layers. In this benchmark, we replace Batch Normalisation with Layer Normalization~\cite{ba2016layer}, as the former wsa observed to produce instabilities in the optimisation procedure. At each dimension, cell embeddings are readout via averaging.

\paragraph{SR} These experiments are run in double floating point precision and with untrained models. We initialise the cell complexes associated with SR graph by populating -cells with constant, scalar, unitary signal, and - and -dimensional cells with the sum of the contained -cells. Complexes are embedded in a -dimensional space and, coherently with~\citet{bodnar2021weisfeiler} and~\citet{bouritsas2020improving}, if the -distance between the embeddings of two complexes is larger than , we deem the corresponding graphs to be non-isomorphic. 
We confirmed the validity of the chosen threshold  by numerically verifying that, under the described experimental setting, each SR graph in our datasets is deemed isomorphic w.r.t.\ a counterpart obtained by randomly permuting its nodes.
We run a CIN model with  cellular message passing layers, whose hidden layers comprise  units. At each dimension, cell embeddings are readout via summation. As the number of induced cycles of a certain size may be enough to tell apart non-isomorphic SR graphs (see Table~\ref{tab:sr_ring_counts}), an MLP with sum readouts represents a strong baseline, which we additionally run. Such a model applies non-linear dense layers at each cell dimension, and then performs readout operations as in CIN. We set the size of hidden layers to , while the final complex embeddings are embedded in a -dimensional space as in our model. Both approaches are equipped with ELU nonlinearities~\citep{ELU}. 

\paragraph{RingTransfer} This benchmark dataset comprises  training graphs. Each graph is randomly associated with one of the  independent labels, which are also assigned as node features to \textbf{source} nodes. Labels are unifomly represented. On this benchmark we run a CIN model with  stacked message passing layers, independently on the ring size. The hidden size of the layers is set to  and we do not apply Batch Normalisation. Differently than in the other benchmarks, we do not need to perform readout operations to compute complex-wise embeddings; instead, we simply take the representation of the -cell corresponding to node \textbf{target} at the last layer of the architecture and use it to predict the label of \textbf{source}. GIN models have always  standard message passing layers with hidden size . The models are trained with an initial learning rate of , decayed by a factor of  and a patience of  epochs. The training is stopped when the learning rate drops below .

\paragraph{TUD} Amongst the datasets from this benchmarking suite: the task in \textbf{MUTAG} is to recognise mutagenic molecular compounds for potentially marketable drug \citep{kazius2005derivation,riesen2008iam}; the one in \textbf{PTC} is to recognise the chemical compounds according to carcinogenicity on rodents \citep{kriege2012subgraph,helma2001predictive}; \textbf{PROTEINS} is about to categorising proteins into enzyme and non-enzyme structures \citep{dobson2003distinguishing,borgwardt2005protein}; \textbf{NCI1} and \textbf{NCI109} deal with identifying chemical compounds against the activity of non-small lung cancer and ovarian cancer cells, respectively \citep{wale2008comparison}; \textbf{REDDIT-BINARY} or \textbf{RDT-B} is a social network dataset where the task is to predict whether a graph belongs to a question-answer-based community or a discussion-based community.
On these datasets, we followed the approach in \citet{GIN}, which prescribes to run a -fold cross-validation procedure and report the maximum of the average validation accuracy across folds. Consistently with such work, we train our model starting from an initial learning rate which is decayed after a fixed amount of epochs and we apply cell-readout operations on the multiscale representations obtained by a Jumping Knowledge scheme~\citep{JK} by performing averaging or summation depending on the dataset, still in accordance with \citet{GIN}. 
We ran a grid-search to tune batch size, hidden dimension, dropout rate, initial learning rate along with its decay steps and strengths, feature initialisation strategy of higher-dimensional cells (mean vs. sum), inclusion of coboundary features in -messages, number of layers and the dropout position (immediately after readout on cells (``cell read.'') or the final readout on the complex (``comp read.'')). We report the hyperparameter configurations in Table~\ref{tab:tu_hyper}. We finally report that we did not employ Batch Normalization layers in \textbf{RDT-B} since they were observed to produce severe instabilities in the training procedure.

\begin{table}[t]
    \centering
    \caption{Hyperparameter configurations on TUDatasets.}
    \label{tab:tu_hyper}
    \vspace{1mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccccc|ccc}
        \toprule
        Hyperparameter &
            MUTAG &
            PTC &
            PROTEINS &
            NCI1 &
            NCI109 &
            IMDB-B &
            IMDB-M &
            RDT-B \\
        \midrule
        Batch Size &
            32 &
            32 &
            128 &
            32 &
            32 &
            128 &
            128 &
            32 \\
        Initial LR &
            0.01 &
            0.01 &
            0.01 &
            0.001 &
            0.001 &
            0.001 &
            0.0005 &
            0.001 \\
        LR Dec. Steps &
            20 &
            50 &
            20 &
            20 &
            20 &
            50 &
            20 &
            50 \\
        LR Dec. Strength &
            0.5 &
            0.9 &
            0.5 &
            0.5 &
            0.5 &
            0.5 &
            0.5 &
            0.5 \\
        Hidden Dim. &
            64 &
            16 &
            32 &
            16 &
            64 &
            16 &
            64 &
            64 \\
        Drop. Rate &
            0.5 &
            0.0 &
            0.0 &
            0.5 &
            0.0 &
            0.0 &
            0.5 &
            0.0 \\
        Drop. Pos. &
            cell read. &
            comp read. &
            comp read. &
            comp read. &
            comp read. &
            comp read. &
            comp read. &
            comp read. \\
        Initialisation &
            sum &
            mean &
            mean &
            mean &
            mean &
            mean &
            mean &
            mean \\
        Cobound. in -msg &
            N &
            N &
            Y &
            Y &
            Y &
            N &
            N &
            N \\
        Num. Layers &
            4 &
            4 &
            3 &
            4 &
            4 &
            4 &
            4 &
            4 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\paragraph{ZINC} The ZINC benchmarks dataset have been constructed by the ZINC database provided by the Irwin and Shoichet Laboratories in the Department of Pharmaceutical Chemistry at the University of California, San Francisco (UCSF)~\citep{ZINCdataset}. Each graph represents a molecule, with node features indicating the atom type and edge features the type of chemical bond between two atoms. Graph targets correspond to the penalised water-octanol partition coefficient -- logP~\cite{gomez2018automatic}. In these experiments, rings up to size  are mapped to -cells, and are assigned feature values as the sum of the learnable atom embeddings for the included -cells (nodes). -cells are assigned learnable bond embeddings if edge-features are considered, otherwise we apply the same policy employed for -cells. We employ the same predefined training, validation and test splits as in~\citet{dwivedi2020benchmarkgnns}, and train our model by minimising the the Mean Absolute Error (MAE) loss on the train targets. As prescribed by the benchmark, the optimisation procedure employs a batch size of  and a dynamic learning rate which starts from  and is halved whenever the validation loss does not improve after a patience value we set to . The training is early stopped as soon as it falls below . We repeat the training with  different weight initialisations and report the mean of the test MAEs at the time of early stopping. In accordance with the best performing baselines, our CIN model does not use any dropout, and stacks  message passing layers with hidden size . In order to enforce the parameter budget we reduce the size of hidden layers to  and only perform  message passing layers. At each dimension, cell embeddings are readout via summation.

\paragraph{Mol-HIV} This dataset comprises  molecular graphs associated with a binary label representing their capacity to inhibit HIV replication. The benchmark provides predefined train, validation and test sets based on the ``scaffold splitting'' procedure, which separates molecules based on their two-dimensional structural frameworks~\citep{hu2020open}. As in \textbf{ZINC}, graphs are attributed at the level of nodes and edges, and we directly employ the atom and bond embedding layers provided by the benchmarking platform\footnote{\url{https://github.com/snap-stanford/ogb/blob/master/ogb/graphproppred/mol_encoder.py}} to populate - and -dimensional cells. Rings of size up to  are considered as -cells, and are endowed with feature vectors with the same procedure as in \textbf{ZINC}. The value  has been chosen from the pool of values  as it yielded the highest validation performance. The architecture hyperparameters are directly replicated from the HIMP model in~\citet{Fey2020_himp}:  message passing layers, dropout rate of  applied after each layer,  as size of hidden layers, constant learning rate of , batch size of . We train our model for  epochs. The small CIN model is obtained by simply reducing the size of hidden layers to . At each dimension, cell embeddings are readout via averaging.

\subsection{Ablation study on ZINC}

We end this section by reporting the results of an ablation study we conducted on the ZINC dataset to appreciate the contribution of including rings. In Table~\ref{tab:zinc_abl} we show the average test MAE for two additional CIN models: ``CIN No-Rings small'' and ``CIN No-Rings'', which differ from their original counterparts in that they neglect -cells when performing message passing. In these experiments we always make use of edge features and use the same hyperparameters as our original CIN model. 

\begin{wraptable}{l}{0.4\textwidth}
    \centering
    \vspace{-12pt}
    \begin{minipage}[t]{1.0\linewidth}
        \caption{ZINC Ablation with edge features. The ablation shows the benefits of integrating rings into the message passing procedure.}
        \label{tab:zinc_abl}
        \resizebox{\columnwidth}{!}{
        \begin{tabular}{l  c}
            \toprule
            Method & 
            MAE \\
            \midrule
            
            GatedGCN \citep{bresson2017residual} &
            0.3630.009  \\
            
            GIN \citep{GIN}  & 
            0.2520.014  \\
            
            PNA \citep{Corso2020_PNA} & 
            0.1880.004  \\
            
            
            DGN \citep{beaini2020directional} & 
            0.1680.003  \\
            
            HIMP \citep{Fey2020_himp} &
            0.1510.006 \\
            
            GSN \citep{bouritsas2020improving} & 
            0.1080.018  \\
            
            \midrule
            
            GIN-E Custom &
            0.1960.007 \\
            
            CIN No-Rings small & 
            0.1740.006 \\
            
            CIN No-Rings & 
            0.1590.007 \\
            
            CIN-small &
            0.0940.004 \\
            
            CIN  & 
            \textbf{0.079}\textbf{0.006}  \\
            
            \bottomrule
        \end{tabular}}
    \end{minipage}
    \vspace{-12pt}
\end{wraptable}


In line with our expectations, we observe a decrease in the overall performance of both versions. They are outperformed by the GSN~\citep{bouritsas2020improving} and HIMP~\citep{Fey2020_himp} models, which either include structural information from cycle isomorphism counting (GSN) or additionally perform message passing on the Junction Tree representation of molecules (where rings are considered as nodes). At the same time, we observe ``CIN No-Rings'' still outperforms all other ring-agnostic baselines. We attribute such strong performance to the more natural and richer modelling of edge signals (-cells): this model updates edge representations at each layer as a function of the present representations and those of the incident nodes (-cells). As an additional confirmation of this hypothesis, we implemented an architecture which replicates the same structure as ``CIN No-Rings'', but replaces cellular message passing with GIN-E layers~\citep{hu2020pretraining}. These layers extend the message passing scheme in GIN by accounting for edge features. We refer to this model as ``GIN-E Custom''. Contrary to CIN, it does not update edge representations and performs readout only at the node level. As expected, we observed that ``GIN-E Custom'' is outperformed by all our models, including, in particular, ``CIN No-Rings''.

\end{document}
