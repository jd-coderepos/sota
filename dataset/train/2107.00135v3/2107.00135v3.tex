\section{Experiments} 
We apply MBT to the task of video classification. In this section we first describe the datasets used to train and test multimodal fusion and their respective evaluation protocols (Sec. \ref{sec:datasets}), then discuss implementation details (Sec. \ref{sec:implementation-details}). We then ablate the key design choices in our model (Sec. \ref{sec:ablations}), before finally comparing our model to the state of the art (Sec. \ref{sec:sota}). 
\subsection{Datasets and evaluation protocol} \label{sec:datasets}
We experiment with three video classification datasets -- AudioSet~\cite{gemmeke2017audio}, Epic-Kitchens-100~\cite{damen2020rescaling} and VGGSound~\cite{chen2020vggsound}, described in more detail below. Results on two additional datasets Moments in Time~\cite{monfort2019moments} and Kinetics~\cite{kay2017kinetics} are provided in the appendix.   \\
\noindent\textbf{AudioSet~\cite{gemmeke2017audio}} consists of almost 2 million 10-second video clips from YouTube, annotated with 527 classes. Like other YouTube datasets, this is a dynamic dataset (we only use the clips still available online). This gives us 20,361 clips for the balanced train set (henceforth referred to as mini-AudioSet or miniAS) and 18,589 clips for the test set. This test set is exactly the same as recent works we compare to, including Perceiver~\cite{jaegle2021perceiver}. Instead of using the 2M unbalanced training set, we train on a (slightly more) balanced subset consisting of 500K samples (AS-500K). Details are provided in the appendix.
Because each sample has multiple labels, we train with a binary cross-entropy (BCE) loss and report mean average precision (mAP) over all classes, following standard practice. \\
\noindent\textbf{Epic-Kitchens 100~\cite{damen2020rescaling}} consists of egocentric videos capturing daily kitchen activities. The dataset consists of 90,000 variable length clips spanning 100 hours. We report results for action recognition following standard protocol~\cite{damen2020rescaling} - each action label is a combination of a verb and noun, and we predict both using a single network with two `heads', both trained with a cross-entropy loss. The top scoring verb and action pair predicted by the network are used, and Top-1 action accuracy is the primary metric. Actions are mainly short-term (average length is 2.6s
with minimum length 0.25s). \\
\noindent\textbf{VGGSound~\cite{chen2020vggsound}} contains almost 200K video clips of length 10s, annotated with 309 sound classes consisting of human actions, sound-emitting objects and human-object interactions. Unlike AudioSet, the sound source for each clip is `visually present' in the video. This was ensured during dataset creation through the use of image classifiers. After filtering clips that are no longer available on YouTube, we end up with 172,427 training and 14,448 test clips. We train with a standard cross-entropy loss for classification and report Top-1 and Top-5 classification accuracy. 
\vspace{-1em}
\subsection{Implementation details} \label{sec:implementation-details}
Our backbone architecture follows that of ViT~\cite{dosovitskiy2020image} identically, specifically we use ViT-Base (ViT-B, , , )\footnote{ is the number of transformer layers,  is the number of self-attention heads with hidden dimension .} initialised from ImageNet-21K~\cite{deng2009imagenet}, however we note that our method is agnostic to transformer backbone. Unless otherwise specialised, we use  bottleneck tokens for all experiments with bottleneck fusion. Bottleneck tokens are initialized using a Gaussian with mean of 0 and standard deviation of 0.02, similar to the positional embeddings in the public ViT~\cite{dosovitskiy2020image} code. We randomly sample clips of  seconds for training. RGB frames for all datasets are extracted at 25 fps. For AudioSet and VGGSound we sample 8 RGB frames over the sampling window of length  with a uniform stride of length . 
We extract  patches from each frame of size , giving us a total of  patches per video. For Epic-Kitchens (because the segments are shorter), we sample 32 frames with stride 1.
Audio for all datasets is sampled at 16kHz and converted to mono channel. Similar to~\cite{gong2021ast}, we extract log mel spectrograms with a frequency dimension of 128 computed using a 25ms Hamming window with hop length 10ms. This gives us an input of size  for  seconds of audio. Spectrogram patches are extracted with size , giving us  patches for 8 seconds of audio.
For images we apply the standard data augmentations used in~\cite{arnab2021vivit} (random crop, flip, colour jitter), and for spectrograms we use SpecAugment~\cite{park2019specaugment} with a max time mask length of 192 frames and max frequency mask length of 48 bins following AST~\cite{gong2021ast}. We set the base learning rate to  and train for  epochs, using Mixup~\cite{zhang2017mixup} with  and stochastic depth regularisation~\cite{huang2016deep} with probability .
All models (across datasets) are trained with a batch size of , synchronous SGD with momentum of , and a cosine learning rate schedule with warmup of  epochs on TPU accelerators using the Scenic library~\cite{dehghani2021scenic}. \\
\noindent\textbf{Inference:} Following standard practice, we uniformly sample multiple temporal crops from the clip and average per-view logits to obtain the final result. The number of test crops is set to 4.

\subsection{Ablation analysis} \label{sec:ablations}
In this section we investigate the impact of the different architectural choices in MBT. Unless otherwise specified, we use the mini-AudioSet split for training and report results on the AudioSet eval split. More ablations on backbone size and pretraining initalisation can be found in the appendix. 
\subsubsection{Fusion strategies} \label{sec:ablations-fusion}
We implement all the three fusion strategies described in Sec. \ref{sec:fusion-strategies}: \\
(i) \textbf{Vanilla self-attention} -- Unrestricted pairwise attention between all latent units within a layer; \\
(ii) \textbf{Vanilla cross-attention with separate weights:} Same as above, but we now have separate weights for each modality. The latent units are updated via pairwise attention with all other latent units from both modalities; and finally \\
(iii) \textbf{Bottleneck fusion:} Here all cross-modal attention must pass through bottleneck fusion latents. \\
Note that these three fusion strategies only describe attention flow between tokens within a layer. For strategies (ii) and (iii), we also conduct experiments showing the impact of restricting cross-modal attention to layers after a fixed fusion layer . We investigate models with different fusion layers, , and present the results in Fig. \ref{fig:bottlenecks}.\footnote{Note that  refers to late fusion, where logits are only aggregated after the classifiers, and neither fusion strategy (ii) nor (iii) is applied, but we show results on the same plot for convenience.} \\
\noindent\textbf{Sharing weights for both modalities:} We first investigate the impact of sharing the encoder weights for both modalities (strategy (i) vs (ii)). The results can be found in Fig. 
\if\sepappendix1{1} \else{\ref{fig:weights}}
\fi
 in the appendix. When modalities are fused at earlier layers, using separate encoders improves performance. For models with later fusion layers, performance is similar for both models. We hence use separate modality weights for further experiments. \\
\noindent\textbf{Fusion layer:} We then investigate the impact of varying the fusion layer , for the latter two strategies: (ii) Vanilla Cross-Attention and (iii) Bottleneck Fusion. We conduct experiments with . We fix the input span  to 4s and the number of bottleneck tokens  to . We conduct 3 runs for each experiment and report mean and std deviation. As can be seen from Fig. \ref{fig:bottlenecks} (left), `mid fusion' outperforms both early () and late fusion (), with optimal performance obtained by using fusion layer  for vanilla cross-attention and  for bottleneck attention. This suggests that the model benefits from restricting cross-modal connections to later layers, allowing earlier layers to specialise to learning unimodal features, however still benefits from multiple layers of cross-modal information flow. In appendix
\if\sepappendix1{D} \else{\ref{sec:late_fusion_dataset_analysis}}
\fi
, we confirm that mid fusion outperforms late fusion across a number of different datasets.\\
\noindent\textbf{Attention bottlenecks:} In Fig.~\ref{fig:bottlenecks}, we also examine the effect of bottleneck attention vs vanilla cross-attention for multimodal fusion. We find that for all values of  restricting flow to bottlenecks improves or maintains performance, with improvements more prominent at lower values of . At , both perform similarly, note that at this stage we only have 3 fusion layers in the model. Our best performing model uses attention bottlenecks with , and we fix this for all further experiments. We also compare the amount of computation, measured in GFLOPs, for both fusion strategies (Fig. \ref{fig:bottlenecks}, right). Using a small number of bottleneck tokens (in our experiments ) adds negligible extra computation over a late fusion model, with computation remaining largely constant with varying fusion layer . This is in contrast to vanilla cross-fusion, which has a non-negligible computational cost for every layer it is applied to. We note that for early fusion (), bottleneck fusion outperforms vanilla cross-attention by over 2 mAP, with less than half the computational cost.  \\
\noindent\textbf{Number of bottleneck tokens :} We experiment with  and , and find that performance is relatively consistent (all within 0.5 mAP). We hence fix the number of tokens to  for all experiments. It is interesting that with such a small number of cross-modal connections through only 4 hidden units () at each cross-modal layer, we get large performance gains over late fusion (Fig. \ref{fig:bottlenecks}), highlighting the importance of allowing cross-modal information to flow at multiple layers of the model.

\subsubsection{Input sampling and dataset size} 
In this section we investigate the impact of different modality sampling strategies. We also compare to single modality baselines -- the visual-only and audio-only baselines consist of a vanilla transformer model applied to only the RGB or spectrogram patches respectively. \\
\noindent\textbf{Sampling window size :} An advantage of our transformer based model is that we can easily input variable length token sequences. We experiment with varying the sampling window  with the following values  and  seconds (note that all videos in AudioSet are 10s), and show results in Fig. \ref{fig:span}\footnote{Averaged over 3 runs. Because error bars are small in the plot we also provide them in Table \if\sepappendix1{3} \else{\ref{tab:span-ablation}}
\fi in the appendix.}. At inference, we uniformly sample multiple windows covering the entire video. While the number of spectrogram patches  changes with , we keep the number of RGB patches  fixed by changing the stride of frames (to avoid running out of memory). Our results indicate that the performance of both the audio and audio-visual fusion model increases with input span, however the performance of the visual-only model slightly decreases (we hypothesize that this is due to the increased fixed stride, meaning fewer frames are randomly sampled during training). We fix  in all further experiments.  \\
\noindent\textbf{Synchronous vs asynchronous sampling:} Given that auditory and visual events may not always be perfected aligned in videos~\cite{kazakos2019epic}, we also investigate asynchronous sampling of different modalities. Here input windows are sampled independently from the entire video clip for each modality. Results are provided in Fig. \if\sepappendix1{2} \else{\ref{fig:sync-async}}
\fi in the appendix. We find performance to be largely robust to either case, and so for simplicity we use synchronised sampling for all further experiments.  \\
\noindent\textbf{Modality MixUp:} While applying Mixup regularization~\cite{zhang2017mixup} to training, we note that there are two different ways to apply it for multimodal inputs -- the standard approach is to sample one set of mixup weights from a Beta distribution using the parameter , and use it to generate all virtual modality-label pairs~\cite{zhang2017mixup}. We also explore a modified version which we call \textit{modality mixup}, which samples an independent weight for each modality. Modality mixup imposes stronger augmentation than standard mixup, leading to a slight improvement (42.6 mAP to 43.9 mAP) on AudioSet.\\
\noindent\textbf{Impact of dataset size:} We show the impact of varying the number of training samples in Fig. \ref{fig:trainingsize}, and find a monotonic increase with dataset size (more steeply for audio-only than visual-only).



\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/bottlenecks-ablation-combined.pdf}
    \caption{The impact of using attention bottlenecks for fusion on performance (left) and compute (right) at different fusion layers  on AudioSet, using clip span  and  bottleneck tokens. Attention bottlenecks improve performance at lower computational cost.
    }
    \label{fig:bottlenecks}
    \vspace{-\baselineskip}
\end{figure}
\begin{figure}
\begin{floatrow}
\ffigbox{\includegraphics[width=0.45\textwidth]{images/span-ablation.pdf} 
}{\caption{The effect of varying input clip span  on the AudioSet test set.} \label{fig:span}}
\ffigbox{\includegraphics[width=0.45\textwidth]{images/training_data.pdf}
}{\caption{The effect of training data size on the AudioSet test set.} \label{fig:trainingsize}}
\end{floatrow}
\vspace{-\baselineskip}
\end{figure}
\begin{table}[h]
    \begin{tabular}{llccc}
    \toprule 
  Model & Training Set & A only & V only & AV Fusion \\
    \midrule 
    GBlend~\cite{wang2020makes}    & MiniAS &29.1 & 22.1 & 37.8 \\
    
    GBlend~\cite{wang2020makes}    & FullAS-2M & 32.4 & 18.8 & 41.8 \\
    Attn Audio-Visual~\cite{fayek2020large} & FullAS-2M & 38.4 & 25.7 & 46.2 \\
    Perceiver~\cite{jaegle2021perceiver} &  FullAS-2M & 38.4 & 25.8 & 44.2  \\
    \hdashline
    MBT & MiniAS & 31.3 & 27.7 & 43.9 \\
MBT & AS-500K & \textbf{41.5} & \textbf{31.3} & \textbf{49.6} \\
    \bottomrule 
    \end{tabular}
    \caption{\textbf{Comparison to SOTA on AudioSet~\cite{gemmeke2017audio}.} We report mean average precision (mAP). We outperform works that train on the full Audioset (2M samples), while we train on only 500K samples.}
    \label{tab:audioset-sota}
\end{table}

\begin{table}
            \begin{tabular}{llccc}
                \toprule 
                Model & Modalities & Verb & Noun & Action \\
                \midrule 
                Damen et al.~\cite{damen2020rescaling} & A & 42.1 & 21.5 & 14.8 \\
                AudioSlowFast~\cite{kazakos2021slow} & A & 46.5 & 22.78 & 15.4 \\
                TSN~\cite{wang2016temporal}  & V, F & 60.2 & 46.0 & 33.2  \\
                TRN~\cite{zhou2018temporal} & V, F & 65.9 & 45.4 & 35.3 \\
                TBN~\cite{kazakos2019epic} & A, V, F &   66.0 & 47.2 & 36.7 \\
                TSM~\cite{lin2019temporal} & V, F &  \textbf{67.9} & 49.0 & 38.3 \\ 
                SlowFast~\cite{feichtenhofer2019slowfast} & V & 65.6 & 50.0 & 38.5 \\ 
                \hdashline
                MBT  & A &  44.3 & 22.4 & 13.0\\
                MBT & V & 62.0  & 56.4 & 40.7\\
                MBT & A, V & 64.8 & \textbf{58.0} & \textbf{43.4}\\
                \bottomrule 
            \end{tabular}
        \caption{\textbf{Comparison to SOTA on EpicKitchens-100~\cite{damen2020rescaling}}. Modalities are \textbf{A:} Audio, \textbf{V:} Visual, \textbf{F:} Optical flow. Uses pretraining on VGGSound.}
        \label{tab:epickitchens-sota}
    \end{table}

\begin{table}
           \begin{tabular}{llcc}
                \toprule 
                Model & Modalities & Top-1 Acc & Top-5 Acc \\
                \midrule 
                Chen et al~\cite{chen2020vggsound} & A & 48.8 & 76.5\\
                AudioSlowFast~\cite{kazakos2021slow} & A & 50.1 & 77.9 \\
                \hdashline
                MBT & A & 52.3 & 78.1\\
                MBT & V & 51.2 & 72.6 \\
                MBT & A,V & \textbf{64.1} & \textbf{85.6} \\
                \bottomrule 
            \end{tabular}
        \caption{\textbf{Comparison to the state of the art on VGGSound~\cite{chen2020vggsound}}. Modalities are \textbf{A:} Audio, \textbf{V:} Visual, \textbf{F:} Optical flow.  We calculate metrics on our test set for a fair comparison using the scores provided by the authors.}
        \label{tab:vggsound-sota}

    \end{table}
 \iffalse
\begin{table}[h]
    \centering
    \begin{tabular}{llccc}
    \toprule 
  Model & Mods & Verb & Noun & Action \\
  \midrule 
  Damen et al.~\cite{damen2020rescaling} & A & 42.1 & 21.5 & 14.8 \\
  AudioSlowFast~\cite{kazakos2021slow} & A & 46.5 & 22.78 & 15.4 \\
  TSN~\cite{wang2016temporal}  & V, F & 60.2 & 46.0 & 33.2  \\
  TRN~\cite{zhou2018temporal} & V, F & 65.9 & 45.4 & 35.3 \\
  TBN~\cite{kazakos2019epic} & A, V, F &   66.0 & 47.2 & 36.7 \\
  TSM~\cite{lin2019temporal} & V, F &  \textbf{67.9} & 49.0 & 38.3 \\ 
SlowFast~\cite{feichtenhofer2019slowfast} & V & 65.6 & 50.0 & 38.5 \\ 
\hdashline
  MBT  & A &  44.3 & 22.4 & 13.0\\
  MBT & V & 62.0  & 56.4 & 40.7\\
  MBT & A, V & 64.8 & \textbf{58.0} & \textbf{43.4}\\
    \bottomrule 
    \end{tabular}
    \caption{\textbf{Results on Epic Kitchens 100~\cite{damen2020rescaling}.} We report top 1\% classification accuracy.  Uses pretraining on VGGSound. Modalities (Mods) are \textbf{A:} Audio, \textbf{V:} Visual (RGB only), \textbf{F:} Optical Flow.}
    \label{tab:epickitchens-sota}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{llcc}
    \toprule 
  Model & Mods & Top-1 Acc & Top-5 Acc \\
  \midrule 
  Chen et al~\cite{chen2020vggsound} & A & 48.8 & 76.5\\
  AudioSlowFast~\cite{kazakos2021slow} & A & 50.1 & 77.9 \\
  \hdashline
  MBT & A & 52.3 & 78.1\\
  MBT & V & 51.2 & 72.6 \\
  MBT & A,V & \textbf{64.1} & \textbf{85.6} \\
    \bottomrule 
    \end{tabular}
    \caption{\textbf{Comparison to the state of the art on VGGSound~\cite{chen2020vggsound}}.  We calculate metrics on our test set for a fair comparison using the scores provided by the authors. Modalities (Mods) are \textbf{A:} Audio, \textbf{V:} Visual}
    \label{tab:vggsound-sota}
\end{table}
\fi

\subsection{Results} \label{sec:sota}
\noindent\textbf{Comparison to single modality performance:}
We compare MBT to visual-only and audio-only baselines on AudioSet (Table \ref{tab:audioset-sota}), Epic-Kitchens (Table \ref{tab:epickitchens-sota}) and VGGSound (Table \ref{tab:vggsound-sota}). Note we use the best parameters obtained via the ablations above, i.e.\ bottleneck fusion with , ,  and modality mixup. For all datasets, multimodal fusion outperforms the higher-performing single modality baseline, demonstrating the value of complementary information. The relative importance of modalities for the classification labels varies (audio-only has higher relative performance for AudioSet and lower for Epic-Kitchens, while both audio and visual baselines are equally strong for VGGSound). This is (unsurprisingly) largely a function of the dataset annotation procedure and positions VGGSound as a uniquely suitable dataset for fusion. We also show that audio-visual fusion provides slight performance gains for traditionally video only datasets such as Kinetics and Moments in Time (details provided in Appendix \if\sepappendix1{C} \else{\ref{sec:additional-data}}
\fi). 
We also examine per-class performance on the Audioset dataset (Figures
\if\sepappendix1{3} \else{\ref{fig:per-class}}
\fi
and 
\if\sepappendix1{4} \else{\ref{fig:per-class-diff}}
\fi
in the Appendix), and find that for the top 60 classes (ranked by overall performance), audio-visual fusion improves performance over audio only or visual only for almost all (57 out of 60) classes, except for `bagpiping', `emergency vehicle' and `didgeridoo' which have strong audio signatures. For classes such as `bicycle' and `shuffling cards' where audio signals are weaker, fusion improves over the audio-only baseline by over 60\% in absolute AP. \\
\noindent\textbf{Comparison to state of the art:} 
We compare MBT to previous fusion methods on AudioSet in Table \ref{tab:audioset-sota}. We outperform all previous works on fusion (even though we only train on a quarter of the training set -- 500K samples), including the recently introduced Perceiver~\cite{jaegle2021perceiver} which uses early fusion followed by multiple self attention layers, and Attn Audio-Visual~\cite{fayek2020large} which uses self-attention fusion on top of individual modality CNNs. We compare to previous video classification methods on Epic-Kitchens in Table \ref{tab:epickitchens-sota}, and note that our model outperforms all previous works that use vision only, as well as TBN~\cite{kazakos2019epic} which uses three modalities - RGB, audio and optical flow. Given VGGSound is a relatively new dataset, we compare to two existing audio-only works\footnote{To fairly compare to these works, we obtain the scores on the full VGGSound test set from the authors, and compute accuracy metrics on our slightly smaller test set as described in Sec. \ref{sec:datasets}.} (Table \ref{tab:vggsound-sota}), and set the first audiovisual benchmark (that we are aware of) on this dataset. \\
\noindent\textbf{Visualisation of attention maps}
Finally, we compute maps of the attention from the output CLS tokens to the RGB image input space using Attention Rollout~\cite{abnar2020quantifying}. Results on test images for both a vanilla fusion model and MBT trained on Audioset-mini (fusion layer ) are shown in Figure~\ref{fig:attn}. We show the attention maps summed over all the frames in the video clip. We note that first, the model focuses on semantically salient regions in the video for audio classification, particularly regions where there is motion that creates or modifies sound, i.e.\ the mouth of humans making sounds, fingertips on a piano, hands and instruments. This is unlike state of the art sound source localisation techniques trained with images~\cite{chen2021localizing}, which tend to highlight the entire object. We further note that the attention maps for MBT are more localised to these regions, showing that the tight bottlenecks do force the model to focus only on the image patches that are actually relevant for the audio classification task and which benefit from early fusion with audio. \\
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/visualisations.pdf}
    \caption{\textbf{Attention Maps.} We compute maps of the attention from the output CLS tokens to the RGB image input space for a vanilla self-attention model and MBT on the Audioset test set. For each video clip, we show the original middle frame on the left with the ground truth labels overlayed at the bottom. The attention is particularly focused on sound source regions in the video that contain motion, eg.\ the fingertips on the piano, the hands on the string instrument, faces of humans. The bottlenecks in MBT further force the attention to be localised to smaller regions of the images (i.e\ the mouth of the baby on the top left and the mouth of the woman singing on the bottom right). 
    }
    \label{fig:attn}
\end{figure}
