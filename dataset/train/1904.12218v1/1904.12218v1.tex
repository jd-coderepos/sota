\documentclass[twoside,11pt]{article}
\usepackage{jair, theapa, rawfonts}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{units}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfig} 
\usepackage{url}
\usepackage{multirow}
\usepackage{dcolumn}
\usepackage{bbm}

\newcommand{\eg}{e.\,g. }
\newcommand{\ie}{i.\,e. }
\newcommand{\aka}{a.\,k.\,a. }

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator{\sign}{sgn}

\newcolumntype{d}[1]{D{.}{.}{#1} }

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\firstpageno{1}

\begin{document}

\title{Graph Kernels: A Survey}

\author{\name Giannis Nikolentzos \email nikolentzos@lix.polytechnique.fr \\
		\addr LIX, \'Ecole Polytechnique\\
       	Palaiseau, 91120, France\\
       \name Giannis Siglidis \email yiannis.siglidis@lip6.fr \\
       \addr LIP6, UPMC Universit\'e Paris 6, Sorbonne Universit\'es\\
       Paris, 75005, France\\
       \name Michalis Vazirgiannis \email mvazirg@lix.polytechnique.fr \\
       \addr LIX, \'Ecole Polytechnique\\
       Palaiseau, 91120, France}



\maketitle


\begin{abstract}
Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data.
During the past  years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs.
Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics.
The goal of this survey is to provide a unifying view of the literature on graph kernels.
In particular, we present a comprehensive overview of a wide range of graph kernels.
Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study.
Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed. 
\end{abstract}

\section{Introduction}\label{Introduction}
In recent years, data that can be naturally modeled as graphs has augmented significantly.
Such types of data have become ubiquitous in many application domains, ranging from social networks to biology and chemistry.
A large portion of the available graph representations corresponds to data derived from social networks.
These networks represent the interactions between a set of individuals such as friendships in a social website, and collaborations in a network of film actors or scientists.
In chemistry, molecular compounds are traditionally modeled as graphs where vertices represent atoms and edges represent chemical bonds.
Biology constitutes another primary source of graph-structured data. Protein-protein interaction networks, metabolic networks, regulatory networks, and phylogenetic networks are all examples of graphs that arise in this domain.
Graphs are also well-suited to representing technological networks.
For example, the World Wide Web can be modeled as a graph where vertices correspond to webpages and edges to hyperlinks between these webpages.
The use of graph representations is not limited to the above application domains.
In fact, most complex systems are usually represented as compositions of entities along with their interactions, and can thus be modeled as graphs.
Interestingly, graphs are very flexible and rich as a means of data representation.
It is not thus surprising that they can also represent data that do not inherently possess an underlying graph structure.
For instance, sequential data such as text can be mapped to graph structures \cite{filippova2010multi}.
From the above, it becomes clear that graphs emerge in many real-world applications, and hence, they deserve no less attention than feature vectors which is the dominant representation in data mining and machine learning.

The aforementioned abundance of graph-structured data raised requirements for automated methods that can gain useful insights.
This often requires applying machine learning techniques to graphs.
In chemistry and biology, some experimental methods are very expensive and time-consuming, and machine learning methods can serve as cost-effective alternatives.
For example, identifying experimentally the function of a protein with known sequence and structure is a very expensive and tedious process.
Therefore, it is often desirable to be able to use computational approaches in order to predict the function of a protein.
By representing proteins as graphs, the problem can be formulated as a graph classification problem where the function of a newly discovered protein is predicted based on structural similarity to proteins with known function \cite{borgwardt2005protein}.
Besides the need for more efficient methods, there is also a need for automating tasks that were traditionally handled by humans and which involve large amounts of data.
For instance, in cybersecurity, humans used to manually inspect code samples to identify if they contain malicious functionality.
However, due to the rapid increase of the number of malicious applications in the past years, humans are no longer capable of meeting the demands of this task \cite{suarez2014evolution}.
Hence, there is a need for methods that can accumulate the knowledge and experience of humans, and that can successfully detect malicious behavior in code samples.
It turns out that machine learning approaches are particularly suited to this task since most of the newly discovered malware samples are variations of existing malware.
By representing code samples as function call graphs, detecting such variations becomes less problematic.
Hence, the problem of detecting malicious software can be formulated as a graph classification problem where unknown code samples are compared against known malware samples and clean code \cite{anderson2011graph}.
From the above example, it becomes thus clear that performing machine learning tasks on graph-structured data is of critical importance for many applications. 

A central issue for machine learning is modelling and computation of similarity among objects.
In the case of graphs, graph kernels have received a lot of attention in recent years, and have been established as the dominant approach for learning on graph-structured data.
A graph kernel is a symmetric, positive semidefinite function defined on the space of graphs .
This function can be expressed as an inner product in some Hilbert space.
Specifically, given a kernel , there exists a map  into a Hilbert space  such that  for all .
Roughly speaking, a graph kernel is a measure of similarity between graphs.
Comparing graphs is a fundamental problem which has numerous applications in many disciplines \cite{conte2004thirty}.
However, the problem is far from trivial and requires considerable computational time.
Graph kernels tackle the graph comparison problem by trying to both capture as much as possible the semantics inherent in the graph but also to remain computationally efficient.
One of the most important factors behind the success of graph kernels is that they allow the large family of kernel methods to work directly on graphs.
Therefore, graph kernels can bring to bear several machine learning algorithms to real-world problems on graph-structured data.
The field of graph kernels has been intensively developed recently.
Interestingly, dozens of graph kernels have been proposed in the past  years.
Some of these kernels have achieved state-of-the-art results on several datasets.

This paper is a survey of kernels that operate on graph-structured data.
We present a comprehensive study of these approaches.
We begin with well-known kernels that established the foundations of the field, and we proceed with more recent kernels that are considered the state-of-the-art for many graph-related machine learning tasks.
Besides the detailed description of the kernels, we also provide an extensive experimental evaluation of most of them.
As we show in the survey, graph kernels are powerful tools with a wide range of applications.
We expect these methods to gain soon more attention in a wealth of applications due to their attractive properties.
Importantly, this study aims to assist both practitioners and researchers who are interested in applying machine learning tasks on graphs.
Furthermore, it should be of interest to all researchers who deal with the problems of graph similarity and graph comparison.
The abundance of applications related to the above problems stresses the value of the survey.
We should note that a similar survey reviewing work on graph kernels became very recently available \cite{ghosh2018journey}.
One may thus ask the question: why another survey within such a short period of time?
The answer is that in contrast to the existing survey, this survey is much more thorough and covers a larger number of kernels.
Moreover, it presents kernels in a more comprehensive way allowing researchers to identify open problems and areas for further exploration, and practitioners to gain a deeper understanding of kernels so that they can decide which kernel suits best to their needs.
This survey also provides a much more meaningful taxonomy of graph kernels.
Specifically, kernels are grouped into classes based on different criteria such as the type of data on which they operate, and the design paradigm that they follow.
Finally, to the best of our knowledge, we provide the most complete evaluation of a large number of graph kernels.
The need for such a wider, and more extensive experimental comparison of graph kernels is very strong since it can provide useful insights into the strengths and weaknesses of the different kernels. 

The rest of this manuscript is organized as follows.
In Section~\ref{sec:motivation_challenges}, we discuss why the use of graphs as a means of object representation is vital and necessary in many domain areas, and we also present the challenges of applying learning algorithms on graphs.
In Section~\ref{sec:preliminaries}, we introduce notation and background material that we need for the remainder of the paper, including some fundamental concepts from graph theory and from kernel methods.
In Section~\ref{sec:graph_kernels}, we discuss the core concepts of graph kernels, and we give an overview of the literature on graph kernels.
We begin by describing important kernels that were developed in the early days of the field.
We next present kernels that are based on neighborhood aggregation mechanisms.
We then describe more recent kernels that do not employ neighborhood aggregation mechanisms.
Subsequently, we present kernels that are based on assignment, and methods that can handle continuous node attributes.
Finally, we give details about frameworks that work on top of graph kernels and aim to improve their performance.
The grouping of the reported studies is designed to make it easier for the reader to follow the analysis of the literature, and to obtain a complete picture of the different graph kernels that have been proposed throughout the years.
In Section~\ref{sec:applications}, we present applications of graph kernels in many different domain areas.
In Section~\ref{sec:experiments}, we experimentally evaluate the performance of many graph kernels on several widely-used graph classification benchmark datasets.
Furthermore, we measure the running times of the kernels.
Based on the obtained results, we provide guidelines for the successful application of graph kernels in different classification problems.
Finally, Section~\ref{sec:conclusion} contains the summary of the survey, along with a discussion about future research directions in the field of graph kernels.

\section{Motivation and Challenges}\label{sec:motivation_challenges}
In this Section, we present the main reasons that motivate the use of graphs instead of feature vectors as a means of data representation.
Furthermore, we describe the problem of learning on graphs which arises in many application domains.
We focus on the instance of the problem where every graph is an input example, and highlight its relationship to the graph comparison problem.

\subsection{Why Graphs}
Graphs are a powerful and flexible means of representing structured data.
The power of graphs stems from the fact that they represent both entities, and the relationships between them.
Typically, the vertices of a graph correspond to some entities, and the edges model how these entities interact with each other.
It is important to note that several fundamental structures for representing data can be seen as instances of graphs \cite{borgwardt2007graph}.
This highlights the generality of graphs as a form of representation.
For example, a vector can be naturally thought of as a graph where vertices correspond to components of the vector and consecutive components within the vector are joined by an edge.
Associative arrays can be modeled as graphs, with keys and values represented as vertices, and directed edges connecting keys to their corresponding values.
Strings can also be represented as graphs, with one vertex per character and edges between consecutive characters.
Due to the power and the generality of graphs as representational models, in some cases, even data that does not exhibit graph-like structure is mapped to graph representations.
A very common example is that of textual data, where graphs are usually employed to model the relationships between sentences or terms \cite{mihalcea2004textrank}.

In data mining and machine learning, observations traditionally come in the form of vectors.
However, vector representations suffer from a series of limitations.
Specifically, vectors have limited capability to model complex objects since they are unable to capture relationships that may exist between different entities of an object.
Furthermore, all the input objects are usually represented as vectors of the same length, despite their size and complexity.
On the other hand, as discussed above, graphs are characterized by increased flexibility which allows them to adequately model a variety of different objects.
Graphs model both the entities and the relationships between them.
Moreover, they are allowed to vary in the number of vertices and in the number of edges.
Therefore, graphs address several of the limitations inherent to vectors.
It is thus clear that the need for methods that perform learning tasks on graphs is intense.


\subsection{Learning on Graphs and Challenges}
Learning on graphs has gained extensive attention in the past years.
This is mainly due to the representational power of graphs which has established them as a major structure for modeling data from various disciplines.
Hence, it is not surprising that a plethora of learning problems have been defined on graphs. 
Most of these learning problems focus either on the node level or on the graph level.
Node classification belongs to the former set of problems, while graph classification belongs to the latter set of problems.
In this survey, we focus exclusively on tasks performed at the graph level.
Therefore, all the kernels that are presented correspond to functions between graphs.

The representation of data is a key issue in the fields of data mining and machine learning.
Algorithms can only handle data in a specific representation.
Due to the appealing properties of graphs, one would expect that there would be great progress in the development of algorithms that can handle graph-structured data.
However, the combinatorial nature of graphs acts as a ``barrier'' since it is very likely that algorithms that operate directly on graphs will be computationally expensive and will not scale to large datasets.
This is why research in these areas focused mainly on algorithms operating on vectors, as vectors possess many desirable mathematical properties and can be dealt with much more efficiently.
Hence, it is not surprising that the most popular learning algorithms are designed for data represented as vectors.
As a consequence, it has become common practice to represent any type of data as feature vectors.
Even in application domains where data is naturally represented as graphs, attempts were made to transform the graphs into feature vectors instead of designing algorithms that operate directly on graphs.
Ideally, we would like to have a method capable of transforming graphs to feature vectors without losing their representational power.
Unfortunately, such a method does not exist.
Directly representing data as vectors is thus suboptimal since vectors fail to preserve the rich topological information encoded in a graph.
Hence, it would be much more preferable to devise algorithms that operate directly on graphs.

The problem of learning on graphs (at the graph level) is directly related to that of \textit{graph comparison}.
The ability to compute meaningful similarity or distance measures is often a prerequisite to perform machine learning tasks.
Such similarity and distance measures are at the core of many machine learning algorithms.
Examples include the -nearest neighbor classifier, and algorithms that learn decision functions in proximity spaces \cite{graepel1999classification}.
These algorithms are very flexible since they require only a distance or similarity function to be defined as the sole mathematical structure on the set of input objects.
Hence, by defining a meaningful distance function  between graphs, we can immediately use one of the above algorithms to perform tasks such as graph classification and graph clustering.
However, it turns out that graph comparison is a very complex problem.
Specifically, graphs lack the convenient mathematical context of vector spaces, and many operations on them, though conceptually simple, are either not properly defined or computationally expensive.
Perhaps the most striking example of these operations is to determine if two objects are identical.
In the case of vectors, it requires comparing all their corresponding components, and it can thus be accomplished in linear time with respect to the size of the vectors.
For the analogous operation on graphs, known as \textit{graph isomorphism}, no polynomial-time algorithm has been discovered so far \cite{garey1979computers}.
In general, the problem of comparing two objects is much less well-defined on graphs compared to vectors.
For vectors, distance can be computed efficiently using the universally accepted Euclidean distance metric.
Unfortunately, there exists no such metric on graphs.
Several fundamental problems in graph theory related to graph comparison such as the subgraph isomorphism problem and the maximum common subgraph problem are NP-complete \cite{garey1979computers}.
Furthermore, identifying common parts in two graphs is computationally infeasible.
Given a graph consisting of  vertices, there are  possible subsets of nodes.
There are hence exponentially many in the size of the graphs pairs of subsets to consider.
It becomes thus clear that although graphs offer a very intuitive way of modeling data from diverse sources, their power and flexibility do not come without a price.
Due to these limitations, graphs have failed to become the major data structure in computer science.





\section{Preliminaries}\label{sec:preliminaries}
Before we delve into the details of graph kernels, we outline some fundamental aspects of graph theory and kernel methods. 
We first introduce basic concepts from graph theory, and define our notation.
We also give a short introduction to kernel functions and kernel methods in machine learning.

\subsection{Definitions and Notations}
\begin{definition}[Graph]
    A graph is a pair  consisting of a set of vertices (or nodes)  and a set of edges  which connect pairs of vertices. 
\end{definition}
The size of the graph corresponds to its number of vertices denoted by  or .
As regards the number of edges of the graph, we will denote it as  or .
An example of a graph is given in Figure~\ref{fig:example_graphs} (left).
\begin{figure}[t]
    \centering
    \subfloat
    {\includegraphics[width=.25\linewidth]{example_simple.pdf}} \qquad \qquad
    \subfloat
    {\includegraphics[width=.25\linewidth]{example_labeled.pdf}} \qquad \qquad
    \subfloat
    {\includegraphics[width=.25\linewidth]{example_attributed.pdf}} 
    \caption{Examples of different types of graphs. A simple undirected graph (left), a labeled graph (center), and an attributed graph (right).}
    \label{fig:example_graphs}
\end{figure}
A graph may have labels on its nodes and edges.
This is often necessary for capturing the semantics of complex objects.
For instance, most graphs derived from chemistry (\eg molecules) are annotated by categorical labels from a finite set.
\begin{definition}[Labeled Graph]
    A labeled graph is a graph  endowed with a function  that assigns labels to the vertices and edges of the graph from a discrete set of labels .
\end{definition}
A graph with labels on its vertices is called node-labeled.
Similarly, a graph with labels on edges is called edge-labeled. 
A graph with labels on both the vertices and edges is called fully-labeled.
An example of a node-labeled graph is given in Figure~\ref{fig:example_graphs} (center).
In many settings, vertex and edge annotations are in the form of vectors.
For example, vertices and edges may be annotated with multiple categorical or real-valued properties.
These graphs are known as attributed graphs.
\begin{definition}[Attributed Graph]
    An attributed graph is a graph  endowed with a function  that assigns real-valued vectors to the vertices and edges of the graph.
\end{definition}
An example of a node-attributed graph is given in Figure~\ref{fig:example_graphs} (right).
Note that labeled graphs are a special case of attributed graphs.
We can represent labeled graphs as attributed graphs if we map the discrete labels to one-hot vector representations.
A graph  can be represented by its adjacency matrix .
\begin{definition}[Adjacency Matrix]
    Let  be the element in the -th row and -th column of matrix .
    Then, the adjacency matrix  of a graph  can be defined as follows
    
\end{definition}
Matrix  is of dimensionality  (or ).
The neighborhood  of vertex  is the set of all vertices adjacent to .
Hence,  where  is an edge between vertices  and  of .
A concept closely related to the neighborhood of a vertex  is its degree .
\begin{definition}[Degree]
    Given an undirected graph  and a vertex , the degree of  in  is the number of edges incident to , and is defined as
    
\end{definition}
Besides the adjacency matrix , a graph  can also be represented by its Laplacian matrix .
\begin{definition}[Laplacian Matrix]
    Let  be the adjacency matrix of a graph  and  a diagonal matrix with .
    Then, the Laplacian matrix  of a graph  can be defined as follows
    
\end{definition}


Similarly to the adjacency matrix , the dimensionality of the Laplacian matrix is .
A subgraph of a graph  is a graph whose set of vertices and set of edges are both subsets of those of .
Let  denote that  is a subgraph of .
\begin{definition}[Induced Subgraph]
    Given a graph  and a subset of vertices , the subgraph  induced by  consists of the set of vertices  and the set of edges  that have both end-points in  defined as follows
    
\end{definition}
The degree of a vertex , , is equal to the number of vertices that are adjacent to  in .
The \textit{density} of a graph  is , the number of edges  over the total possible edges.
A graph  with density  is called a \textit{complete} graph.
In a complete graph, every pair of distinct vertices are adjacent.
A \textit{clique} is a subset of vertices such that every pair of them are connected by an edge, that is, their induced subgraph is complete.
\begin{definition}[Walk, Path, Cycle]
    A walk in a graph  is a sequence of vertices  where  for all  and  for all .
    The length of the walk is equal to the number of edges in the sequence, \ie  in the above case.
    A walk in which  is called a path.
    A cycle is a path with .
\end{definition}
\begin{definition}[Shortest Path]
    A shortest path from vertex  to vertex  of a graph  is a path from  to  such that there exist no other path between these two vertices with smaller length.
\end{definition}
The \textit{diameter} of a graph  is the length of the longest shortest path between any pair of vertices of .
The \textit{neighborhood of radius}  (or -hop neighborhood) of vertex  is the set of vertices whose shortest path distance from  is less than or equal to  and is denoted
by .
Table~\ref{tab:symbols} gives a list of the most commonly used symbols along with their definition.
\begin{table}[t]
  \centering
  \footnotesize
  \def\arraystretch{1.1}
  \begin{tabular}{llll} \hline
    \multicolumn{4}{c}{\textbf{List of key symbols}} \\ \hline
     & Set of graphs &  & A graph \\
     & Set of vertices &  & Set of edges \\
     & Number of vertices &  & Number of edges \\
     & Neighbors of  &  & Degree of  in  \\
     & Subgraph of  induced by set of vertices  &  & -hop neighborhood of  \\
     & Adjacency matrix of graph &  & Laplacian matrix of graph \\
     & Function that assigns labels to vertices and edges &  & Diameter of graph \\ 
     & Function that assigns attributes to vertices and edges \\ \hline
  \end{tabular}
  \caption{Commonly Used Symbols and Notations}
  \label{tab:symbols}
\end{table}


\subsection{Kernel Functions and Kernel Methods}
We next give an introduction to kernel functions and kernel methods.
\begin{definition}[Gram Matrix]
  Given a set of inputs  and a function , the  matrix  defined as
  
  is called the gram matrix (or kernel matrix) of  with respect to the inputs .
\end{definition} 
In what follows, we will refer to gram matrices as kernel matrices.
\begin{definition}[Positive Semidefinite Matrix]
  A real  symmetric matrix  satisfying
  
  for all  is called positive semidefinite.
\end{definition}
\begin{definition}[Positive Semidefinite Kernel]
  Let  be a nonempty set.
  A function  which for all  and all  gives rise to a positive semidefinite kernel matrix is called a positive semidefinite kernel, or just a kernel.
\end{definition} 
Informally, a kernel function 
measures the similarity between two objects. 
Furthermore, kernel functions can be represented as inner products between the vector representations of these objects. 
Specifically, if we define a kernel  on , then there exists a mapping  into a Hilbert space with inner product , such that:

A Hilbert space is an inner product space which also possesses the completeness property that every Cauchy sequence of points taken from the space converges to a point in the space.
Furthermore, the Hilbert space  has the following property known as the reproducing property:

By virtue of this property,  is called a reproducing kernel Hilbert space (RKHS) associated with kernel .
It is interesting to note that every kernel function on  is associated with an RKHS and vice versa \cite{aronszajn1950theory}.

Kernel methods are a class of machine learning algorithms which operate on input data after they have been mapped into an implicit feature space using a kernel function.
One of the major advantages of kernel methods is that they can operate on very general types of data \cite{scholkopf2002learning}.
The input space  does not have to be a vector space, but it can represent any structured domain, such as the space of strings or graphs \cite{gartner2003survey}.
Kernel methods can still be applied to such types of data, as long as we can find a mapping , where  is an RKHS.
This mapping is not neccasary to be explicitly determined. 
These methods implicitly represent data in a feature space and compute inner products between them in that space using a kernel function.
These inner products can be interpreted as the similarities between the corresponding objects.
Machine learning tasks such as classification and clustering can be carried out by using only the inner products computed in that feature space.
Kernel methods are very popular and have been successfully used in a wide variety of applications.
Here, we need to stress that the optimization problem of several kernel methods such as the Support Vector Machines is convex only if the employed function is positive semidefinite.

To make these ideas clear, consider a binary classification problem with training set  where ,  is an inner product space (\eg ), and .
As mentioned above, given the training set , the goal is to learn a function  such that the generalization error of  is as low as possible.
Hence, we are not only interested in minimizing the training error, but we are also interested in performing 
accurate predictions/classifications on previously unseen instances.

Large margin methods such as the Support Vector Machines seek a hyperplane that separates the instances belonging to class  from those belonging to class .
Therefore,  can take the form  where  takes the value  if its argument is positive,  otherwise.
Then, given an instance , the decision function  outputs a prediction depending on the location of  with respect to the hyperplane .
Let us first assume that the training data is linearly separable.
In such a case, there exists a hyperplane such that points belonging to different classes are on opposing sides of the hyperplane.
In fact, there exist infinitely many of these hyperplanes.
Large margin classifiers select among all these hyperplanes the one that maximizes the margin between the two classes of training data, that is, the one whose distance from the nearest data points of the two classes is maximum.
To find this optimal hyperplane, classifiers solve a convex quadratic optimization problem.
The solution vector  of this problem is a linear combination of the training instances:

where .
Using the above result, which is known as the \textit{representer theorem} \cite{scholkopf2001generalized}, the linear classifier  can be written as

In case that the training data is not linearly separable, we search for the hyperplane that maximizes the margin and at the same time, minimizes a quantity proportional to the number of misclassification errors. 
Computing this hyperplane can again be formulated as a convex quadratic optimization problem, and the solution vector  is still a linear combination of the training inputs.

For complex classification problems, there may be no hyperplanes that can separate the positive from the negative instances and provide good classification performance.
The answer to this problem, which is commonly known as the \textit{kernel trick} \cite{aizerman1964theoretical,boser1992training}, is to map the inputs into a (usually higher-dimensional) feature space , and to find the separating hyperplane in that space.
Let  be a mapping from the input space  to a feature space , equipped with an inner product.
To compute the optimal hyperplane in the feature space we can use our previous formulation, simply by replacing  with .
Let  be a kernel function with the following property:

The decision function  can now be written as

From the above analysis, it is clear that by defining a kernel function , we implicitly map all data points into a feature space .
Kernel methods can then perform machine learning tasks such as classification without ever explicitly performing the mapping.



\section{Graph Kernels}\label{sec:graph_kernels}
In this Section, we give an overview of the graph kernel literature.
Our study is not exhaustive, however, we have tried to cover the most representative approaches that have appeared in the literature of graph kernels., 
We first present some fundamental aspects of graph kernels, and we then proceed by discussing the details of several graph kernel instances.

\subsection{Kernels between Graphs}
Kernels on graphs can be divided into two categories: () those that compare nodes in a graph, and () those that compare graphs.
As mentioned above, in this survey, we focus on the second category, that is, \textit{kernels between graphs} and thus we exclusively use the term \textit{graph kernel }for describing such kernel functions.
As regards the first category, we refer the interested reader to the work of Kondor and Lafferty \citeyear{kondor2002diffusion} which was later extended by Smola and Kondor \citeyear{smola2003kernels}.
Graph kernels have recently emerged as a promising approach for learning on graph-structured data.
These methods exhibit several attractive statistical properties.
They combine the representative power of graphs and the discrimination power of kernel-based methods.
Hence, they constitute powerful tools for tackling the graph similarity and learning tasks at the same time.

From the previous Section, it is clear that the application of kernel methods consists of two steps.
First, a kernel function is designed, and based on this function the kernel matrix is constructed.
Second, a learning algorithm is employed to compute the optimal manifold in the feature space (\eg a hyperplane in binary classification problems).
Since several mature kernel-based classifiers are available in the literature, research on graph kernels has focused on the first step.
Hence, the main effort has been devoted to developing expressive and efficient graph kernels capable of accurately measuring the similarity between input graphs.
These kernels implicitly (or explicitly sometimes) project graphs into a feature space  as illustrated in Figure~\ref{fig:mapping}.
\begin{figure}
  \centering
  \includegraphics[width=.7\linewidth]{mapping.pdf}
    \caption{Feature space and map defined by graph kernels.
    Any kernel on a space of graphs  can be represented as an inner product after graphs are mapped into a Hilbert space .}
    \label{fig:mapping}
\end{figure}
As regards the second step, it is common to employ off-the-shelf algorithms such as the Support Vector Machines classifier \cite{boser1992training}, and thus, we will not enter into more details here.
The interested reader is referred to Sch{\"o}lkopf and Smola \citeyear{scholkopf2002learning} or to Shawe-Taylor and Cristianini \citeyear{shawe2004kernel}.

Concluding, the main challenge in applying kernel methods to graphs is  to \textit{define appropriate positive semidefinte kernel functions on the set of input graphs which are able to reliably assess the similarity among them}.
We next present, for illustration purposes, two very simple kernels that compare node and edge labels of the two involved graphs.

\subsection{Simple Kernels}
The vertex histogram and edge histogram kernels are very simple instances of graph kernels which generate explicit graph representations.

\subsubsection{Vertex Histogram Kernel}
The vertex histogram kernel is a basic linear kernel on vertex label histograms.
The kernel assumes node-labeled graphs.
Let  be a collection of graphs, and assume that each of their vertices comes from an abstract vertex space .
Given a set of node labels ,  is a function that assigns labels to the vertices of the graphs.
Assume that there are  labels in total, that is .
Furthermore, assume that .
Then, the vertex label histogram of a graph  is a vector , such that  for each .
Let  be the vertex label histograms of two graphs , respectively.
The vertex histogram kernel is then defined as the linear kernel between  and , that is

The complexity of the vertex histogram kernel is linear in the number of vertices of the graphs.

\subsubsection{Edge Histogram Kernel}
The edge histogram kernel is a basic linear kernel on edge label histograms.
The kernel assumes edge-labeled graphs.
Let  be a collection of graphs, and assume that each of their edges comes from an abstract edge space .
Given a set of node labels ,  is a function that assigns labels to the edges of the graphs.
Assume that there are  labels in total, that is .
Furthermore, assume that .
Then, the edge label histogram of a graph  is a vector , such that  for each .
Let  be the edge label histograms of two graphs , respectively.
The edge histogram kernel is then defined as the linear kernel between  and , that is

The complexity of the edge histogram kernel is linear in the number of edges of the graphs.

\subsection{Expressiveness vs Efficiency}
The two kernels defined above are indeed postive semidefinite, but they both correspond to rather naive concepts - as a distribution of values is.
A question that may arise at this point is how expressive can graph kernels be in practice.


Let us first define the class of kernels which are capable of distinguishing between all (non-isomorphic) graphs in the feature space.
Such kernels are called \textit{complete}.
\begin{definition}[Complete Graph Kernel]
  A graph kernel  is complete if  is injective.
\end{definition}
G{\"a}rtner, Flach and Wrobel \citeyear{gartner2003graph} showed that computing any complete graph kernel is at least as hard as deciding whether two graphs are isomorphic.
The above result, in effect, prohibits the use of complete graph kernels in practical applications.
Instead, by using kernels that are not complete, it is not further guaranteed that non-isomorphic graphs will not be mapped into the same point in the feature space.
This is a negative result since it implies that to develop expressive kernels, it is necessary to sacrifice some of their efficiency.
More recently, Kriege et al. \citeyear{kriege2018property} showed that several established graph kernels, such as the Weisfeiler-Lehman subtree kernel, cannot distinguish essential graph properties such as connectivity, planarity and bipartiteness.
Considering that the Weisfeiler-Lehman subtree kernel achieves state-of-the-art results on most benchmark datasets, this result blurs even more the already vague issue of choosing a graph kernel a practitioner is faced with when dealing with a particular application.
In fact, devising a good trade-off between efficiency and effectiveness is an issue of vital importance when designing a graph kernel.


\subsection{Taxonomy of Graph Kernels}
There exist many different criteria we can use to divide the various graph kernels into different categories.
For instance, graph kernels are traditionally grouped into some major families, each focusing on a different structural aspect of graphs such as random walks, subtrees, cycles, paths, and small subgraphs.
Alternatively, graph kernels can be divided into groups according to their ability to handle unlabeled graphs, node-labeled or node-attributed graphs.
Furthermore, graph kernels can be divided into approaches that employ explicit computation schemes and approaches that employ implicit computation schemes \cite{kriege2014explicit}.
Graph kernels can also be grouped into categories based on the design paradigm that they follow (\ie if they are -convolution, assignment or intersection kernels).
Note that groups emerging from different criteria may be related to each other.
For instance, graph kernels that can handle node-attributed graphs usually employ implicit computation schemes.
Figure~\ref{fig:taxonomy} illustrates the taxonomy of graph kernels.
\begin{figure}[t]
    \centering
    \includegraphics[width=.8\linewidth]{taxonomy.pdf}
    \caption{Taxonomy of graph kernels.}
    \label{fig:taxonomy}
\end{figure}
The devised taxonomy is based on some of the criteria mentioned above. 
However, in what follows, we do not adopt exclusively any of these criteria.
We begin our treatment with approaches that were proposed in the early days of graph kernels, starting from the well-studied random walk kernel till the very popular Weisfeiler-Lehman subtree kernel.
We next present some approaches that were inspired from the neighborhood aggregation schmeme of the Weisfeiler-Lehman subtree kernel, and then kernels that do not fall into either of the previous two categories.
The subequent subsections are devoted to assignment kernels, and to kernels that can handle continuous node attributes.
The final subsections deals with frameworks and approaches that can be applied on top of existing graph kernels.
An overview of the graph kernels that are presented in this survey and their properties is given in Table~\ref{tab:comparison}.
\begin{table}[t]
  \centering
  \scriptsize
  \begin{sc}
  \def\arraystretch{1.2}
  \begin{tabular}{l|c|c|c|c}
    \multirow{2}{*}{Graph Kernel}& \multirow{2}{*}{Exp. } & Node & Node & \multirow{2}{*}{Type} \\ 
    & & Labels & Attributes & \\ \hline
    Vertex Histogram &  &  &  & -convolution \\ 
    Edge Histogram &  &  &  & -convolution \\ 
    Random Walk &  &  &  & -convolution \\
    Subtree &  &  &  & -convolution \\
    Cyclic Pattern &  &  &  & intersection \\
    Shortest Path &  &  &  & -convolution  \\
    Graphlet &  &  &  & -convolution \\
    Weisfeiler-Lehman Subtree &  &  &  & -convolution \\ 
    Neighborhood Hash &  &  &  & intersection \\
    Neighborhood Subgraph Pairwise Distance &  &  &  & -convolution \\
    Lov\'asz  &  &  &  & -convolution \\ 
    SVM- &  &  &  & -convolution \\ 
    Ordered Decomposition DAGs &  &  &  & -convolution \\
    Pyramid Match &  &  &  & assignment \\
    Weisfeiler-Lehman Optimal Assignment &  &  &  & assignment \\ 
    Subgraph Matching &  &  &  & -convolution \\
    GraphHopper &  &  &  & -convolution \\ 
    Graph Invariant Kernels &  &  &  & -convolution \\ 
    Propagation &  &  &  & -convolution \\ 
    Multiscale Laplacian &  &  &  & -convolution \\ \hline
  \end{tabular}
  \end{sc}
  \caption{Summary of selected graph kernels regarding computation by explicit feature mapping (Exp. ), support for node-labeled and node-attributed graphs, and type.}
  \label{tab:comparison}
\end{table}


\subsection{Early Days of Graph Kernels}
While early studies on kernel functions and kernel methods focused almost exclusively on input data represented as vectors, it soon became clear that these methods could handle more complex structured objects such as strings, trees and graphs.
One of the most popular methods for defining kernels between such objects is to decompose the objects into their ``parts'', and to compare all pairs of these ``parts'' by applying existing kernels on them.
Kernels constructed using the above framework are called \textit{R-convolution} kernels \cite{haussler1999convolution}.
Most graph kernels in the literature are instances of the -convolution framework.
These kernels decompose graphs into their substructures and add up the pairwise similarities between these substructures.

The most intuitive example of an -convolution kernel is probably a kernel that decomposes each graph into the set of all of its subgraphs, and compares them pairwise.
G{\"a}rtner, Flach and Wrobel \citeyear{gartner2003graph} showed that the problem of computing the kernel that compares all the subgraphs of two graphs is NP-hard.
Based on this result, it becomes clear that we need to consider alternative, less powerful graph kernels that are computable in polynomial time.
However, as discussed above, it is necessary that these kernels provide an expressive measure of similarity on graphs.
Over the years,  several graph kernels have been proposed, each focusing on a \textit{different structural aspect} of graphs.
Such aspects involve comparing graphs based on random walks, subtrees, cycles, paths, and small subgraphs, to name a few.
We next look at some kernels that date back to the early days of this field.
Furthermore, we present kernels that were motivated by problems encountered by the above instances, and were proposed as more advanced alternatives. 

\subsubsection{Random Walk Kernel}
The random walk kernels are perhaps of the first successful efforts to design kernels between graphs that can be computed in polynomial time.
The members of this well-studied family of graph kernels quantify the similarity between a pair of graphs based on the number of common walks in the two graphs \cite{kashima2003marginalized,gartner2003graph,mahe2004extensions,borgwardt2005protein,vishwanathan2010graph,sugiyama2015halting,zhang2018retgk}.
Kernels belonging to this family have concentrated mainly on counting matching walks in the two input graphs.
There are several variations of random walk kernels.
The -step random walk kernel compares random walks up to length  in the two graphs.
The most widely-used kernel from this family is the geometric random walk kernel \cite{gartner2003graph} which compares walks up to infinity assigning a weight  () to walks of length  in order to ensure convergence of the corresponding geometric series.
We next give the formal definition of the geometric random walk kernel.
Given two node-labeled graphs  and , their direct product  is a graph with vertex set:

and edge set:

An example of the product graph of two graphs is illustrated in Figure~\ref{fig:product_graph}.
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\linewidth]{product_graph.pdf}
    \caption{Two graphs (top left and right) and their direct product (bottom). Each vertex of the direct product graph is labeled with a pair of vertices; an edge exists in the direct product if and only if the corresponding vertices are adjacent in both original graphs. For instance, nodes  and  are adjacent because there is an edge between vertices  and  in the first, and  and  in the second graph.}
    \label{fig:product_graph}
\end{figure}
Performing a random walk on  is equivalent to performing a simultaneous random walk on  and .
The geometric random walk kernel counts common walks (of potentially infinite length) in two graphs and is defined as follows.  
\begin{definition}[Geometric Random Walk Kernel]
  Let  and  be two graphs, let  denote the adjacency matrix of their product graph , and let  denote the vertex set of the product graph .
  Then, the geometric random walk kernel is defined as
  
  where  is the identity matrix,  is the all-ones vector, and  is a positive, real-valued weight.
  The geometric random walk kernel converges only if  where  is the largest eigenvalue of .
\end{definition}
Direct computation of the geometric random walk kernel requires  time.
The computational complexity of the method severely limits its applicability to real-world applications.
To account for this, Vishwanathan et al. \citeyear{vishwanathan2010graph} proposed four efficient methods to compute random walk graph kernels which generally reduce the computational complexity from  to .
Mah{\'e} et al. \citeyear{mahe2004extensions} proposed some other extensions of random walk kernels.
Specifically, they proposed a label enrichment approach which increases specificity and in most cases also reduces computational complexity.
They also employed a second order Markov random walk to deal with the problem of ``tottering''.
Sugiyama and Borgwardt \citeyear{sugiyama2015halting} focused on a different problem of random walk kernels, a phenomenon referred to as ``halting''.
assicMore recently, Zhang et al. \citeyear{zhang2018retgk} proposed a kernel that capitalizes on the isomorphism-invariance property of the return probabilities of random walks.

\subsubsection{Subtree Kernel}
Due to problems with the expressiveness of the random walk kernels that they identified, Ramon and G{\"a}rtner \citeyear{ramon2003expressivity} worked on designing new kernels.
Their research efforts resulted in the development of the subtree kernel, an algorithm that counts the number of common subtree patterns in two graphs.
The kernel is more expressive, but also more computationally expensive than the random walk kernels.

The subtree patterns that the subtree kernel considers correspond to rooted subgraphs.
Every subtree pattern has a tree-structured signature, and the kernel associates each possible subtree pattern signature to a feature.
Given a graph, the value of each feature is the number of times that a subtree of the signature that corresponds to this feature occurs in the graph.
Let  be a kernel that counts the pairs of subtrees of the same signature of height less than or equal to , where the first subtree is rooted at  and the second one is rooted at .
The kernel  is equal to:

where  and  are positive values smaller than  to cause higher trees to have a smaller weight in the overall sum, and  is the dirac kernel.
Therefore, if  and the two nodes share the same label, then it holds that .
If  and the two nodes have different labels, we have .
For , one can compute  using a recursive scheme.
Specifically, we define the set of all matchings from  to  as follows

Each element  of  is a set of pairs of nodes from the neighborhoods of  and , such that nodes in each pair have identical labels and no node is contained in more than one pair.
The subtree kernel compares all pairs of vertices from two graphs by iteratively comparing their neighborhoods.
\begin{definition}[Subtree Kernel]
  Let  and  be two graphs.
  Then, the subtree kernel is defined as
  
\end{definition}
The computational complexity of the subtree kernel for a pair of graphs is  where  is the maximum degree.
Although in the worst-case scenario, the runtime complexity of the subtree kernel is very high, in practice, it can be quite low if the input graphs are sparse or if there is sufficient diversity in the labels of the vertices.


\subsubsection{Cyclic Pattern Kernel}
The cyclic pattern kernel is also one of the earliest approaches developed in the area of graph kernels.
This kernel decomposes a graph into cyclic and tree patterns, and counts the number of common patterns which occur in two graphs \cite{horvath2004cyclic}.
More specifically, let  be a graph.
Let also  denote the set of cycles of .
Let  be a sequence of vertices that forms a cycle in , \ie .
The canonical representation of a cycle  is the lexicographically smallest string  among the strings obtained by concatenating the labels along the vertices of the cyclic permutations of  and its reverse.
Formally, denoting by  the set of cyclic permutations of a sequence  and its reverse, the canonical representation of  is defined by

where  is a function that assigns labels to the vertices of the graph.
In case of edge-labeled graphs, edgle labels can also be taken into account.
The set of cyclic patterns of  is then defined by


The kernel then extracts from  all the edges that do not belong to any cycle (a.k.a bridges) by removing from  all the edges of all cycles.
The set of bridges of  forms a set of trees (each tree is a connected component composed of bridges).
Then, similarly to cycles, the kernel computes the canonical representation  of each tree .
The set of tree patterns of  is then defined by

Then, given two graphs, the kernel computes the intersection of their sets of cyclic and tree patters.
\begin{definition}[Cyclic Pattern Kernel]
  Let ,  be two graphs, and  and  be the sets of cyclic patterns and tree patters of the two graphs, respectively.
  Then, the cyclic pattern kernel is defined as
  
\end{definition}
Unfortunately, computing the cyclic pattern kernel is an NP-hard problem.
The cardinality of the set of cyclic and tree patterns of a graph can be exponential in the number of vertices of the graph. 
However, the cyclic pattern kernel can prove useful for practical problem classes where the number of cycles in the input graphs is bounded.

\subsubsection{Shortest-Path Kernel}
The high computational complexity of the graph kernels based on walks, subtrees and cycles renders them impractical for most real-world scenarios.
Borgwardt and Kriegel \citeyear{borgwardt2005shortest} worked on developing more efficient kernels based on paths.
However, computing all the paths in a graph and computing the longest paths in a graph are both NP-hard problems.
Instead, shortest paths can be computed in polynomial time, and they gave rise to the shortest-path kernel, one of the most popular kernels at present.

The shortest-path kernel decomposes graphs into shortest paths and compares pairs of shortest paths according to their lengths and the labels of their endpoints.
The first step of the shortest-path kernel is to transform the input graphs into shortest-paths graphs.
Given an input graph , the algorithm creates a new graph  (\ie its shortest-path graph).
The shortest-path graph  contains the same set of vertices as the graph from which it originates.
The edge set of the former is a superset of that of the latter, since in the shortest-path graph , there exists an edge between all vertices which are connected by a walk in the original graph .
To complete the transformation, the algorithm assigns labels to all the edges of the shortest-path graph .
The label of each edge is set equal to the shortest distance between its endpoints in the original graph .

Given the above procedure for transforming a graph into a shortest-path graph, the shortest-path kernel is defined as follows.
\begin{definition}[Shortest-Path Kernel]
  Let ,  be two graphs, and ,  their corresponding shortest-path graphs.
  The shortest-path kernel is then defined as
  
  where  is a positive semidefinite kernel on edge walks of length .
\end{definition}
In labeled graphs, the  kernel is designed to compare both the lengths of the shortest paths corresponding to edges  and , and the labels of their endpoint vertices.
Let  and .
Then,  is usually defined as

where  is a kernel comparing vertex labels, and  a kernel comparing shortest path lengths.
Vertex labels are usually compared via a dirac kernel, while shortest path lengths may also be compared via a dirac kernel or, more rarely, via a brownian bridge kernel \cite{borgwardt2005shortest}.
When  and  both are dirac kernels, an explicit computation scheme can be employed as shown in Figure~\ref{fig:shortest_path}.
In terms of runtime complexity, the shortest-path kernel can be computed in  time.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{sp_explicit.pdf}
    \caption{Example of explicit computation of the shortest path kernel. Each triple is a feature and corresponds to: (label of source vertex; label of sink vertex; shortest path length between the two vertices).}
    \label{fig:shortest_path}
\end{figure}


\subsubsection{Graphlet Kernel}
The graphlet kernel decomposes graphs into graphlets (\ie small subgraphs with  nodes where ) \cite{prvzulj2007biological} and counts matching graphlets in the input graphs.
The set of graphlets of size  is shown in Figure~\ref{fig:graphlets}.
The kernel was originally designed to address scalability issues experienced by earlier approaches.
In fact, the graphlet kernel was one of the first kernels that could cope with very large graphs using a simple sampling scheme.
However, apart from the scalability issue, the graphlet kernel was also motivated by the graph reconstruction conjecture \cite{bondy1977graph}, which states that any graph of size  can be reconstructed from the set of all its subgraphs of size .
This could possibly be interpreted as indicating that kernels that compare graphs based on their subgraphs should reflect graph similarity better than approaches that are defined based on random walks, subtrees, cyclic patterns or shortest paths.
However, even if graphs that have similar distributions of graphlets are very likely to be similar themselves, there is no theoretical justification on why such a substructure (\ie graphlets) is better than the others.

As mentioned above, the graphlet kernel computes the distribution of small subgraphs in a graph.
Let , , ,  be the set of size- graphlets.
Let also  be a vector such that its -th entry is equal to the frequency of occurrence of  in , .
Then, the graphlet kernel is defined as follows.
\begin{definition}[Graphlet of size  Kernel]
  Let ,  be two graphs of size , and  vectors that count the occurrence of each graphlet of size  (not necessarily connected) in the two graphs. 
  Then the graphlet kernel is defined as
  
\end{definition}
As is evident from the above definition, the graphlet kernel is computed by explicit feature maps.
First, the representation of each graph in the feature space is computed.
And then, the kernel value is computed as the dot product of the two feature vectors.
The main problem of graphlet kernel is that an exaustive enumeration of graphlets is very expensive.
Since there are  size- subgraphs in a graph, computing the feature vector for a graph of size  requires  time.
To account for that, Shervashidze et al. \citeyear{shervashidze2009efficient} resorted to sampling. 
Following Weissman et al. \citeyear{weissman2003inequalities}, they showed that by sampling a fixed number of graphlets the empirical distribution of graphlets will be sufficiently close to their actual distribution in the graph.  
An alternative proposed strategy that reduces the expressivity of the kernel is to enumerate only the connected graphlets of  vertices, and not all the possible graphlets.

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\linewidth]{graphlets.pdf}
    \caption{All graphlets of size .}
    \label{fig:graphlets}
\end{figure}

\subsubsection{Weisfeiler-Lehman Subtree Kernel}
The Weisfeiler-Lehman subtree kernel is a very popular algorithm, and is considered the state-of-the-art in graph classification.
It belongs to the family of subtree kernels, and was motivated by the need for a fast subtree kernel that scales up to large, labeled graphs.
The kernel is an instance of the Weisfeiler-Lehman framework.
This framework operates on top of existing graph kernels and is inspired by the Weisfeiler-Lehman test of graph isomorphism \cite{weisfeiler1968reduction}.
The key idea of the Weisfeiler-Lehman algorithm is to replace the label of each vertex with a multiset label consisting of the original label of the vertex and the sorted set of labels of its neighbors.
The resultant multiset is then compressed into a new, short label.
This relabeling procedure is then repeated for  iterations.
Note that this procedure is performed simultaneously on all input graphs.
Therefore, two vertices from different graphs will get identical new labels if and only if they have identical multiset labels.

More formally, given a graph  endowed with a labeling function , the Weisfeiler-Lehman graph of  at height  is a graph  endowed with a labeling function  which has emerged after  iterations of the relabeling procedure described above.
The Weisfeiler-Lehman sequence up to height  of  consists of the Weisfeiler-Lehman graphs of  at heights from  to , . 
\begin{definition}[Weisfeiler-Lehman Framework]
  Let  be any kernel for graphs, that we will call the base kernel.
  Then the Weisfeiler-Lehman kernel with  iterations with the base kernel  between two graphs  and  is defined as
  
  where  is the number of Weisfeiler-Lehman iterations, and  and ,  are the Weisfeiler-Lehman sequences of  and  respectively.
\end{definition}
From the above definition, it is clear that any graph kernel that takes into account discrete node labels can take advantage of the Weisfeiler-Lehman framework and compare graphs based on the whole Weisfeiler-Lehman sequence.

When the base kernel compares subtrees extracted from two graphs, the computation involves counting common original and compressed labels in the two graphs.
The emerging Weisfeiler-Lehman subtree kernel is a byproduct of the Weisfeiler-Lehman test of isomorphism.
\begin{definition}[Weisfeiler-Lehman Subtree Kernel]
  Let ,  be two graphs.
  Define  as the set of letters that occur as node labels at least once in  or  at the end of the  iteration of the Weisfeiler-Lehman algorithm.
  Let  be the set of original node labels of  and .
  Assume all  are pairwise disjoint.
  Without loss of generality, assume that every  is ordered.
  Define a map  such that  is the number of occurrences of the letter  in the graph .

  The Weisfeiler-Lehman subtree kernel on two graphs  and  with  iterations is defined as
  
  where
  
  and
  
\end{definition}
An illustration of the Weisfeiler-Lehman subtree kernel is given in Figure~\ref{fig:wl_example}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{wl_example3.pdf}
    \caption{Illustration of the computation of the Weisfeiler-Lehman subtree kernel with  for two  graphs  and . Here,  are letters that occur as node labels. Compressed labels map to subtree patterns. For example, if a node has label , this means that there is a subtree pattern of height  rooted at this node, where the root has label  and its single neighbor has label .}
    \label{fig:wl_example}
\end{figure}
It can be shown that the above definition is equivalent to comparing the number of shared subtrees between the two input graphs \cite{shervashidze2011weisfeiler}.
In contrast to the subtree kernel that was proposed by Ramon and G{\"a}rtner and was presented above, the Weisfeiler-Lehman subtree kernel considers all subtrees up to height , instead of subtrees of exactly height .
Furthermore, the Weisfeiler-Lehman subtree kernel checks whether the neighborhoods of two vertices match exactly, while the subtree kernel considers all pairs of matching subsets of the neighborhoods of two vertices.
It is interesting to note that the Weisfeiler-Lehman subtree kernel exhibits a very attractive computational complexity since it can be computed in  time.


\subsection{Neighborhood Aggregation Approaches}
The Weisfeiler-Lehman subtree kernel triggered a lot of activity in the field of graph kernels.
The relabeling procedure of the Weisfeiler-Lehman algorithm can be viewed as a \textit{neighborhood aggregation} scheme.
The main idea behind neighborhood aggregation algorithms (a.k.a. message-passing algorithms) is that each vertex receives messages from its neighbors and utilizes these messages to update its representation.
Following the success of this kernel, several variations of it were proposed.
All these variations employ a neighborhood aggregation scheme similar to that of the Weisfeiler-Lehman algorithm.
The goal of most of these works is to speed-up the computation time of the Weisfeiler-Lehman subtree kernel \cite{hido2009linear,kataoka2016hadamard}.
However, other types of variations were also proposed such as a streaming version of the Weisfeiler-Lehman algorithm \cite{li2012nested}, and a kernel that uses the -dimensional Weisfeiler-Lehman test of isomorphism \cite{morris2017global}.
We next present the neighborhood hash kernel, a kernel that was born out of these research efforts.

\subsubsection{Neighborhood Hash Kernel}
Similar to the Weisfeiler-Lehman subtree kernel, the neighborhood hash kernel also assumes node-labeled graphs \cite{hido2009linear}.
It compares graphs by updating their node labels and counting the number of common labels.
The kernel replaces the discrete node labels with binary arrays of fixed length, and it then employs logical operations to update the labels so that they contain information about the neighborhood structure of each vertex.

Let  be a function that maps vertices to an alphabet  which is the set of possible discrete node labels.
Hence, given a vertex ,  is the label of vertex .
The algorithm first transforms each discrete node label to a bit label.
A bit label is a binary array consisting of  bits as

where the constant  satisfies  and .

The most important step of the algorithm involves a procedure that updates the labels of the vertices.
To achieve that, the kernel makes use of two very common bit operations: () the exclusive or () operation, and () the bit rotation () operation.
Let  denote the  operation between two bit labels  and  (\ie the  operation is applied to all their components).
The output of the operation is a new binary array whose components represent the  value between the corresponding components of the  and  arrays.
The  operation takes as input a bit array and shifts its last  bits to the left by  bits and moves the first  bits to the right end as shown below  

Below, we present in detail two procedures for updating the labels of the vertices: () the simple neighborhood hash, and () the count-sensitive neighborhood hash.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.55\textwidth]{simple_nh}
  \caption{Example of computation of the simple neighborhood hash for a vertex (in green). The vertex has two adjacent vertices (in red). The three vertices have different labels from each other. The algorithm uses XOR and ROT operations to compute the neighborhood hash of the vertex ().}
  \label{fig:simple_nh}
\end{figure}

\paragraph{Simple Neighborhood Hash.}
Given a graph  with bit labels, the simple neighborhood hash update procedure computes a neighborhood hash for each vertex using the logical operations  and .
More specifically, given a vertex , let  be the set of neighbors of .
Then, the kernel computes the neighborhood hash as

The resulting hash  is still a bit array of length , and we regard it as the new label of .
This new label represents the distribution of the node labels around .
Hence, if  and  are two vertices that have the same label (\ie ) and the label sets of their neighborhors are also identical, their hash values will be the same (\ie .
Otherwise, they will be different except for accidental hash collisions.
The main idea behind this update procedure is that the hash value is independent of the order of the neighborhood values due to the properties of the  operation.
Hence, one can check whether or not the distributions of neighborhood labels of two vertices are equivalent without sorting or matching these two label sets.
Figure~\ref{fig:simple_nh} illustrates how the simple neighborhood hash is computed for a given vertex.

\paragraph{Count-sensitive Neighborhood Hash.}
The simple neighborhood hash update procedure described above suffers from some problematic hash collisions.
Specifically, the neighborhood hash values for two independent nodes have a small probability of being the same even if there is no accidental hash collision.
Such problematic hash collisions may affect the positive semidefiniteness of the kernel.
To address that problem, the count-sensitive neighborhood hash update procedure counts the number of occurences of each label in the set.
More specifically, it first uses a sorting algorithm (\eg radix sort) to align the bit labels of the neighbors, and then, it extracts the unique labels (set  in the case of  unique labels) and for each label counts its number of occurences.
Then, it updates each unique label based on its number of occurences as follows

where  is the initial and updated label respectively, and  is the number of occurences of that label in the set of neighbors.
The above operation makes the hash values unique by depending on the number of label occurrences.
Then, the count-sensitive neighborhood hash is computed as

Figure~\ref{fig:count_sensitive_nh} illustrates the operations of the count-sensitive neighborhood hash for a given vertex.
Both the simple and the count-sensitive neighborhood hash can be seen as general approaches for enriching the labels of vertices based on the label distribution of their neighborhood vertices.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{count_sensitive_nh}
  \caption{Example of computation of the count-sensitive neighborhood hash for a vertex (in green). The vertex has three adjacent vertices (in red). Two of these three vertices have identical labels. The algorithm uses XOR and ROT operations to compute the count-sensitive neighborhood hash of the vertex ().}
  \label{fig:count_sensitive_nh}
\end{figure}

\paragraph{Kernel Calculation.}
The neighborhood hash update procedures presented above aggregate the information of the neighborhood vertices to each vertex.
Then, given two graphs  and , the updated labels of their vertices are compared using the following function

where  is the number of labels the two graphs have in common.
This function is equivalent to the Tanimoto coefficent which is commonly used as a similarity measure between sets of discrete values and which has been proven to be positive semidefinite \cite{gower1971general}.

The label-update procedures is not necessary to be applied once, but they can be applied iteratively.
By updating the bit labels several times, the new labels can capture high-order relationships between vertices.
For instance, if the procedure is performed  times in total, the updated label  of a vertex  represents the label distribution of its -neighbors.
Hence, two vertices  with identical labels and connections among their -neighbors will be assigned the same label.

\begin{definition}[Neighborhood Hash Kernel]
	Let  and  be two graphs, and let  and  denote their updated graphs where the node labels have been updated  times based on one of the two procedures presented above, respectively.
	Then, the neighborhood hash kernel is defined as
	
\end{definition}
The computational complexity of the neighborhood hash kernel is  where  is the number of vertices of the graphs and  is the average degree of their vertices.


\subsection{Other Approaches}
There are several kernels that were proposed recently which belong to the -convolution framework, but do not perform neighborhood aggregation.
There are, for instance, kernels specially designed for graphs with ordered neighborhoods \cite{draief2018kong}, kernels that compare pairs of rooted subgraphs containing nodes up to a certain distance from the root \cite{costa2010fast}, kernels that extract directed acyclic graphs from the input graphs \cite{da2012tree}, and kernels that use the orthonormal representations of vertices introduced by Lov\'asz \cite{johansson2014global}.
We next present some of these kernels in detail.

\subsubsection{Neighborhood Subgraph Pairwise Distance Kernel}
The neighborhood subgraph pairwise distance kernel extracts pairs of rooted subgraphs from each graph whose roots are located at a certain distance from each other, and which contain vertices up to a certain distance from the root.
It then compares graphs based on these pairs of rooted subgraphs.
To avoid isomorphism checking, graph invariants are employed to encode each rooted subgraph \cite{costa2010fast}.

Let  be a graph.
The distance between two vertices , denoted , is the length of the shortest path between them.
The neighborhood of radius  of a vertex  is the set of vertices at a distance less than or equal to  from , that is .
Given a subset of vertices , let  be the set of edges that have both end-points in .
Then, the subgraph with vertex set  and edge set  is known as the subgraph induced by .
The neighborhood subgraph of radius  of vertex  is the subgraph induced by the neighborhood of radius  of  and is denoted by .
Let also  be a relation between two rooted graphs ,  and a graph  that is true if and only if both  and  are in , where we require  to be isomorphic to some  to verify the set inclusion, and that .
We denote with  the inverse relation that yields all the pairs of rooted graphs ,  satisfying the above constraints.
Hence,  selects all pairs of neighborhood graphs of radius  whose roots are at distance  in a given graph .

\begin{definition}[Neighborhood Subgraph Pairwise Distance Kernel]
	Let  be two graphs.
	The neighborhood subgraph pairwise distance kernel extracts from the two graphs pairs of rooted subgraphs of radius  whose roots are located at distance  from each other.
	It then utilizes the following kernel to compare them
	
	where  is  if its input subgraphs are isomorphic, and  otherwise.
	The above kernel counts the number of identical pairs of neighboring graphs of radius  at distance  between two graphs.
	Then, the neighborhood subgraph pairwise distance kernel is defined as
	
	where  is a normalized version of , that is
	
\end{definition}
The above version ensures that relations of all orders are equally weighted regardless of the size of the induced part sets.
The neighborhood subgraph pairwise distance kernel includes an exact matching kernel over two graphs (\ie the  kernel) which is equivalent to solving the graph isomorphism problem.
Solving the graph isomorphism problem is not feasible.
Therefore, the kernel produces an approximate solution to it instead.
Given a subgraph  induced by the set of vertices , the kernel computes a graph invariant encoding for the subgraph via a label function , where  is the set of rooted graphs and  is the set of strings over a finite alphabet .
The function  makes use of two other label functions: () a function  for vertices, and () a function  for edges.
The  function assigns to vertex  the concatenation of the lexicographically sorted list of distance-distance from root-label triplets  for all , where  is the root of the subgraph and  is a function that maps vertices/edges to their label symbol.
Hence, the above function relabels each vertex with a string that encodes the initial label of the vertex, the vertex distance from all other labeled vertices, and the distance from the root vertex.
The  function assigns to edge  the label , , .
The  function thus annotates each edge based on the new labels of its endpoints, and its initial label, if any.
Finally, the function  assigns to the rooted graph induced by  the concatenation of the lexicographically sorted list of  for all .
The kernel then employs a hashing function from strings to natural numbers  to obtain a unique identifier for each subgraph.
Hence, instead of testing pairs of subgraphs for isomorphism, the kernel just checks if the subgraphs share the same identifier.

The computational complexity of the neighborhood subgraph pairwise distance kernel is  and is dominated by the repeated computation of the graph invariant for each vertex of the graph.
Since this is a constant time procedure, for small values of  and , the complexity of the kernel is in practice linear in the size of the graph.

\subsubsection{Lov\'asz  Kernel}
The Lov\'asz number  of a graph  is a real number that is an upper bound on the Shannon capacity of the graph.
It was introduced by L\'aszl\'o Lov\'asz in  \cite{lovasz1979shannon}.
The Lov\'asz number is intimately connected with the notion of orthonormal representations of graphs.
An orthonormal representation of a graph  consists of a set of unit vectors  where each vertex  is assigned a unit vector  such that .
Specifically, the Lov\'asz number of a graph  is defined as

where  is a unit vector and  is an orthonormal representation of . 
Geometrically,  is defined by the smallest cone enclosing a valid orthonormal representation .
The Lov\'asz number  of a graph  can be computed to arbitrary precision in polynomial time by solving a semidefinite program.

The Lov\'asz  kernel utilizes the orthonormal representations associated with the Lov\'asz number to compare graphs \cite{johansson2014global}.
The kernel is applicable only to unlabeled graphs.
Given a collection of graphs, it first generates orthonormal representations for the vertices of each graph by computing the Lov\'asz  number.
Hence,  is a set that contains the orthonormal representations of .
Let  be a subset of the vertex set of .
Then, the Lov\'asz value of the set of vertices  is defined as

where  is a unit vector and  is the representation of vertex  obtained by computing the Lov\'asz number  of .
The Lov\'asz value of a set of vertices  represents the angle of the smallest cone enclosing the set of orthonormal representations of these vertices (\ie subset of  defined as ).

\begin{definition}[Lov\'asz  Kernel]
	Let  and  be two graphs.
	The Lov\'asz  kernel between the two graphs is defined as follows
	
	where ,  is a delta kernel (equal to  if , and  otherwise), and  is a positive semi-definite kernel between Lov\'asz values (\eg linear kernel, gaussian kernel).
\end{definition}
The Lov\'asz  kernel consists of two main steps: () computing the Lov\'asz number  of each graph and obtaining the associated orthonormal representations, and () computing the Lov\'asz value for all subgraphs (\ie subsets of vertices ) of each graph.
Exact computation of the Lov\'asz  kernel is in most real settings infeasible since it requires computing the minimum enclosing cones of  sets of vertices.

When dealing with large graphs, it is thus necessary to resort to sampling.
Given a graph , instead of evaluating the Lov\'asz value on all  sets of vertices, the algorithm evaluates it in on a smaller number of subgraphs induced by sets of vertices contained in .
Then, the Lov\'asz  kernel is defined as follows

where  and  denotes the subset of  consisting of all sets of cardinality , that is .

The time complexity of computing  is  where  is the complexity of computing the base kernel , ,  and .
The first term represents the cost of solving the semi-definite program that computes the Lov\'asz number .
The second term corresponds to the worst-case complexity of computing the sum of the Lov\'asz values.
And finally, the third term is the cost of computing the Lov\'asz values of the sampled subsets of vertices.


\subsubsection{SVM- Kernel}
The SVM- kernel is very related to the Lov\'asz  kernel \cite{johansson2014global}.
The Lov\'asz  kernel suffers from high computational complexity, and the SVM- kernel was developed as a more efficient alternative. 
Similar to the Lov\'asz  kernel, this kernel also assumes unlabeled graphs.

Given a graph  such that , the Lov\'asz number of  can be defined as

where  is the one-class SVM given by

and  is a set of positive semidefinite matrices defined as

where  is the set of all  positive semidefinite matrices.

The SVM- kernel first computes the matrix  which is equal to

where  is the adjacency matrix of ,  is the  identity matrix, and  with  the minimum eigenvalue of .
The matrix  is positive semidefinite by construction and it has been shown in \cite{jethava2013lovasz} that

where  are the maximizers of Equation~\ref{eq:oneclass_svm}. 
Furthermore, it was shown that on certain families of graphs (\eg Erd{\"o}s R{\'e}nyi random graphs),  is with high probability a constant factor approximation to .

\begin{definition}[SVM- Kernel]
	Let  and  be two graphs.
	Then, the SVM- kernel is defined as follows
	
	where ,  is a delta kernel (equal to  if , and  otherwise), and  is a positive semi-definite kernel between real values (\eg linear kernel, gaussian kernel).
\end{definition}

The SVM- kernel consists of three main steps: () constructing matrix  of  which takes  time () solving the one-class SVM problem in  time to obtain the  values, and () computing the sum of the  values for all subgraphs (\ie subsets of vertices ) of each graph.
Computing the above quantity for all  sets of vertices is not feasible in real-world scenarios.

To address the above issue, the SVM- kernel employs sampling schemes.
Given a graph , the kernel samples a specific number of subgraphs induced by sets of vertices contained in .
Then, the SVM- kernel is defined as follows

where  and  denotes the subset of  consisting of all sets of cardinality , that is .

The time complexity of computing  is  where  is the complexity of computing the base kernel  and .
The first term represents the cost of computing  (dominated by the eigenvalue decomposition).
The second term corresponds to the worst-case complexity of comparing the sums of the  values.
And finally, the third term is the cost of computing the sum of the  values for the sampled subsets of vertices.

\subsubsection{Ordered Decomposition DAGs Kernel}
In contrast to the above two kernels, the ordered decomposition DAGs kernel can handle node-labeled graphs.
The kernel decomposes graphs into multisets of directed acyclic graphs (DAGs), and then uses existing tree kernels to compare these DAGs \cite{da2012tree}.

Given a graph , the kernel generates one unordered rooted DAG, say , for each vertex .
To generate the DAG, the kernel keeps only those edges belonging to the shortest paths between  and any vertex .
Furthermore, a direction is given to each edge, while edges connecting vertices visited at level  to vertices visited at level  are also removed.
Figure~\ref{fig:odd_1} gives an example of the decomposition of a graph into a set of DAGs.
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{odd_sth_1}
  \caption{Example of decomposition of a graph into its four DAGs (one for each vertex).}
  \label{fig:odd_1}
\end{figure}
\begin{definition}[Ordered Decomposition DAGs Kernel]
  Let  and  be two graphs.
  Let also  and  be multisets defined as  and , respectively.
  Then, the ordered decomposition DAGs kernel is defined as
  
  where  is a kernel between DAGs.
\end{definition}
The kernel is thus defined as the sum of the computation of a local kernel for DAGs, over all pairs of DAGs in the multiset. 
Note that these DAGs are unordered.
Moreover, there is a vast literature on kernels for ordered trees, but only a few kernel functions for unordered trees.
Hence, the ordered decomposition DAGs kernel transforms the unordered DAGs to ordered DAGs, and then applies a kernel for ordered trees.
More specifically, the kernel defines a strict partial order among the vertices of each DAG.
This partial order takes into account the labels of the vertices, the outdegrees of the vertices (in case of identical node labels), and the relation between the sequence of successors of each vertex (in case of identical node labels and equal outdegrees).
Let  denote the DAG of  ordered according to the above relation.
Let a tree visit be a function  that, given a vertex  of a , returns the tree resulting from the visit of the DAG starting in .
Figure~\ref{fig:odd_2} gives an example of tree visits.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.55\textwidth]{odd_sth_2}
  \caption{Two DAGs (left) and their associated tree visits  starting from each node  (right).}
  \label{fig:odd_2}
\end{figure}
Then, the ordered decomposition DAGs kernel uses tree visits to project subdags to a tree space and applies tree kernels on the visits

where ,  are the set of vertices of  and , respectivevly, and  is a kernel between ordered trees.
The time complexity of the ordered decomposition DAGs kernel depends on the employed tree kernel .
For instance, using the subtree and subset tree kernel leads to a time complexity of  and , respectively.
To reduce the time complexity, the kernel employs a strategy that allows it to compute  once for each unique pair of subtrees appearing in different DAGs.
Furthermore, in case the subtree kernel is employed, some other strategies can be applied to speed up the computation such as for instance limiting the depth of the visits during the generation of the multiset of DAGs 


\subsection{Assignment Kernels}
The majority of the kernels that have been presented so far belong to the family of -convolution kernels.
Besides this family of kernels, another family that has received a lot of attention recently are the \textit{assignment kernels}.
In general, these kernels compute a matching between substructures of one object and substructures of a second object such that the overall similarity of the two objects is maximized.
Such a matching can reveal structural correspondences between the two objects.
However, defining graph kernels that follow this design paradigm is not trivial.
For example, an optimal assignment kernel that was proposed in the early days of graph kernels to compute a correpondence between the atoms of molecules \cite{frohlich2005optimal} was later proven not to always be positive semidefinite \cite{vert2008optimal}.
Despite these design difficulties, there is a handful of valid assignment graph kernels.
For instance, there is a method that capitalizes on the well-known pyramid match kernel to match the node embeddings of graphs \cite{nikolentzos2017matching}, while another approach uses multi-graph matching techniques to obtain valid assignment kernels \cite{schiavinato2015transitive}.
More importantly, it was recently shown that there exists a class of base kernels used to compare substructures that guarantees positive semidefinite optimal assignment kernels \cite{kriege2016valid}.
We next present some of the above instances of assignment kernels in more detail.

\subsubsection{Pyramid Match Graph Kernel}
The pyramid match kernel is a very popular algorithm in Computer Vision, and has proven useful for many applications including object recognition and image retrieval \cite{grauman2007pyramid,lazebnik2006beyond}.
The pyramid match graph kernel extends its applicability to graph-structured data \cite{nikolentzos2017matching}.
The kernel can handle unlabeled graphs as well as graphs that contain discrete node labels.

The pyramid match graph kernel first embeds the vertices of each graph into a low-dimensional vector space using the eigenvectors of the  largest in magnitude eigenvalues of the graph's adjacency matrix.
Since the signs of these eigenvectors are arbitrary, it replaces all their components by their absolute values.
Each vertex is thus a point in the -dimensional unit hypercube.
To find an approximate correspondence between the sets of vertices of two graphs, the kernel maps these points to multi-resolution histograms, and compares the emerging histograms with a weighted histogram intersection function.

Initially, the kernel partitions the feature space into regions of increasingly larger size and takes a weighted sum of the matches that occur at each level.
Two points match with each other if they fall into the same region.
Matches made within larger regions are weighted less than those found in smaller regions.
The kernel repeatedly fits a grid with cells of increasing size to the -dimensional unit hypercube.
Each cell is related only to a specific dimension and its size along that dimension is doubled at each iteration, while its size along the other dimensions stays constant and equal to .
Given a sequence of levels from  to , then at level , the -dimensional unit hypercube has  cells along each dimension and  cells in total.
Given a pair of graphs , let  and  denote the histograms of  and  at level , and , , the number of vertices of ,  that lie in the  cell.
The number of points in two sets which match at level  is then computed using the histogram intersection function

The matches that occur at level  also occur at levels .
The algorithm takes into account only the new matches found at each level which is given by  for .
Furthermore, the number of new matches found at each level in the pyramid is weighted according to the size of that level's cells.
Matches found within smaller cells are weighted more than those that occur in larger cells.
Specifically, the weight for level  is set equal to .
Hence, the weights are inversely proportional to the length of the side of the cells that varies in size as the levels increase.

\begin{definition}[Pyramid Match Graph Kernel]
  Let  and  be two graphs.
	The pyramid match kernel is defined as follows
	
	where  is the number of different levels.
\end{definition}
The complexity of the pyramid match kernel is  where  is the number of nodes of the graphs under comparison.

In the case of labeled graphs, the kernel restricts matchings to occur only between vertices that share same labels.
It represents each graph as a set of sets of vectors, and matches pairs of sets of two graphs corresponding to the same label using the pyramid match kernel.
The emerging kernel for labeled graphs corresponds to the sum of the separate kernels

where  is the number of distinct labels and  is the pyramid match kernel between the sets of vertices of the two graphs which are assigned the label .

\subsubsection{Weisfeiler-Lehman Optimal Assignment Kernel}
The Weisfeiler-Lehman optimal assignment kernel is currently one of the state-of-the-art algorithms for learning on graphs \cite{kriege2016valid}.
The kernel capitalizes on the theory of valid assignment kernels to improve the performance of the Weisfeiler-Lehman subtree kernel.
Before we delve into the details of the kernel, it is necessary to introduce the theory of valid optimal assignment kernels.

Let  be a set, and  denote the set of all -element subsets of .
Let also  for , and  denote the set of all bijections between  and .
The optimal assignment kernel on  is defined as

where  is a kernel between the elements of  and .
Kriege et al. \citeyear{kriege2016valid} showed that the above function  is a valid kernel only if the base kernel  is strong.
\begin{definition}[Strong Kernel]
  A function  is called strong kernel if  for all .
\end{definition}
Strong kernels are equivalent to kernels obtained from a hierarchy defined on set .
More specifically, let  be a rooted tree such that the leaves of  are the elements of .
Let  be the set of vertices of .
Each inner vertex  in  corresponds to a subset of  comprising all leaves of the subtree rooted at .
Let  be a weight function such that  for all  in  where  is the parent of vertex .
Then, the tuple  defines a hierarchy.
Let  be the lowest common ancestor of vertices  and , that is, the unique vertex with maximum depth that is an ancestor of both  and .
\begin{definition}[Hierarchy-induced Kernel]
  Let  be a hierarchy on , then the function defined as  for all  in  is the kernel on  induced by .
\end{definition}
Interestingly, strong kernels are equivalent to kernels obtained from a hierarchical partition of the domain of the kernel. 
Hence, by constructing a hierarchy on , we can derive a strong kernel  and ensure that the emerging assignment function is a valid kernel.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{optimal_assignment_example.pdf}
    \caption{The matrix of a strong kernel on objects  and  (a) induced by the hierarchy (b) and the derived feature vectors (c). A vertex  in (b) is annotated by its weights .}
    \label{fig:optimal_assignment_example}
\end{figure}

Based on the property that every strong kernel is induced by a hierarchy, we can derive explicit feature maps for strong kernels.
Let  be an additive weight function defined as  and  for the root .
Note that the property of a hierarchy assures that the values of the  function are nonnegative.
For , let  denote the vertices on the path from  to the root .
The strong kernel  induced by the hierarchy  can be defined using the mapping , where  and the components indexed by  are

Figure~\ref{fig:optimal_assignment_example} shows an example of a strong kernel, an associated hierarchy and the derived feature vectors.

Let  be a hierarchy on .
As mentioned above, the hierarchy  induces a strong kernel .
Since  is strong, the function  defined in Equation~\ref{eq:valid_assignment_kernel} is a valid kernel.
The kernel  can be computed in linear time in the number of vertices  of the tree  using the histogram intersection kernel \cite{swain1991color} as follows

which is known to be a valid kernel on  \cite{barla2003histogram}.
Hence, the complexity of the proposed kernel depends on the size of the tree .
Figure~\ref{fig:optimal_assignment_histograms} illustrates the relation between the optimal assignment kernel employing a strong base kernel and the histogram intersection kernel.
\begin{figure}[t]
  \centering
  \includegraphics[width=.8\linewidth]{optimal_assignment_histograms.pdf}
  \caption{An assignment instance (a) for  and the derived histograms (b). The set  contains three distinct vertices labeled  and the set  two distinct vertices labeled  and . Taking the multiplicities into account the histograms are obtained from the hierarchy of the base kernel  depicted in Figure~\ref{fig:optimal_assignment_example}.  The optimal assignment yields a value of .}
  \label{fig:optimal_assignment_histograms}
\end{figure}

We next present the Weisfeiler-Lehman optimal assignment kernel.
\begin{definition}[Weisfeiler-Lehman Optimal Assignment Kernel]
  Let  and  be two graphs.
  The Weisfeiler-Lehman optimal assignment kernel is defined as
  
  where  is the following base kernel
  
  where  is the label of node  at the end of the  iteration of the Weisfeiler-Lehman relabeling procedure.
\end{definition}
The base kernel value reflects to what extent two vertices  and  have a similar neighborhood.
It can be shown that the colour refinement process of the Weisfeiler-Lehman algorithm defines a hierarchy on the set of all vertices of the input graphs.
Specifically, the sequence  gives rise to a family of nested subsets, which can naturally be represented by a hierarchy .
When assuming  for all vertices , the hierarchy induces the kernel defined above.
Such a hierarchy for a graph on six vertices is illustrated in Figure~\ref{fig:wl_optimal_assignment}.
\begin{figure}[t]
  \centering
  \includegraphics[width=.9\linewidth]{wl_optimal_assignment.pdf}
    \caption{A graph  with uniform initial labels  and refined labels  for  (a), and the associated hierarchy (b).}
    \label{fig:wl_optimal_assignment}
\end{figure}


\subsection{Kernels for Graphs with Continuous Attributes}
Most existing graph kernels are designed to operate on both unlabeled and node-labeled graphs.
However, many real-world graphs contain continuous real-valued node attributes.
One example comes from the field of cybersecurity where the function call graphs extracted from the source code of application programs typically contain multi-dimensional node labels. 
Such types of graphs do not appear only in cybersecurity, but also in computer vision \cite{harchaoui2007image} or even in bioinformatics \cite{borgwardt2005protein}, where labels may represent RGB values of colors or physical properties of protein secondary structure elements, respectively.
Research in graph kernels has achieved a remarkable progress in recent years.
However, it has focused mainly on unlabeled graphs and graphs with discrete node labels.
For such kind of graphs, there are several highly scalable graph kernels available which can handle graphs with thousands of vertices (\eg the Weisfeiler-Lehman subtree kernel).
However, the same does not happen in the case of datasets where node labels correspond to vectors.
Some existing graph kernels for node-labeled graphs such as the shortest-path kernel can be extended to handle continuous labels.
However, by taking into account these labels, their computational complexity becomes prohibitive.
Designing graph kernels for graphs with continuous node labels is a much less well studied problem which started to gain some attention recently \cite{kriege2012subgraph,feragen2013scalable,orsini2015graph,neumann2016propagation,morris2016faster,su2016fast,kondor2016multiscale}.
There are mainly two categories of approaches for graphs with continuous node labels: () those that directly handle the continuous node labels, and () those that first discretize the node labels and then employ existing kernels for graphs with discrete node labels.
We will next present some kernels belonging to the first category.
With regards to the second category, worthy of mention is the work of Morris et al. \citeyear{morris2016faster} where the authors propose the hash graph kernel framework which iteratively transforms continuous attributes into discrete labels using randomized hash functions, thus allowing kernels that support discrete node labels to handle node-attributed graphs.


\subsubsection{Subgraph Matching Kernel}
The subgraph matching kernel counts the number of matchings between subgraphs of bounded size in two graphs \cite{kriege2012subgraph}.
The kernel is very general since it can be applied to graphs that contain node labels, edge labels, node attributes or edge attributes.

Let  be a set of graphs.
We assume that the graphs that are contained in the set are labeled or attributed.
Specifically, let  be a labeling function that assigns either discrete labels or continuous attributes to vertices and edges.
A graph isomorphism between two labeled/attributed graphs  and  is a bijection  that preserves adjacencies, \ie , and labels, \ie if  is the mapping of vertex pairs implicated by the bijection  such that , then, the conditions  and  must hold, where  denotes that two labels are considered equivalent.

\begin{definition}[Subgraph Matching Kernel]
  Given two graphs  and , let  denote the set of all bijections between sets  and , and let  be a weight function.
  The subgraph matching kernel is defined as
  
  where  and  are kernel functions defined on vertices and edges, respectively.
\end{definition}

The instance of the subgraph matching kernel that is obtained if we set the  functions as follows

is known as the common subgraph isomorphism kernel.
This kernel counts the number of isomorphic subgraphs contained in two graphs.

To count the number of isomorphisms between subgraphs, the kernel capitalizes on a classical result of Levi \citeyear{levi1973note} which makes a connection between common subgraphs of two graphs and cliques in their product graph.
More specifically, each maximum clique in the product graph is associated with a maximum common subgraph of the factor graphs.
This allows someone to compute the common subgraph isomorphism kernel by enumerating the cliques of the product graph.

The general subgraph matching kernel extends the theory of Levi and builds a weighted product graph to allow a more flexible scoring of bijections.
Given two graphs , , and vertex and edge kernels  and , the weighted product graph  of  and  is defined as

After creating the weighted product graph, the kernel enumerates its cliques.
The kernel starts from an empty clique and extends it stepwise by all vertices preserving the clique property.
Let  be the weight of a clique .
Whenever the clique  is extended by a new vertex , the weight of the clique is updated as follows: first it is multiplied by the weight of the vertex , and then, it is multiplied by all the edges connecting  to a vertex in , that is .
The algorithm effectively avoids duplicates by removing a vertex from the candidate set after all cliques containing it have been exhaustively explored.

The runtime of the subgraph matching kernel depends on the number of cliques in the product graph.
The worst-case runtime complexity of the kernel when considering subgraphs of size up to  is , where  is the sum of the number of vertices of the two graphs.


\subsubsection{GraphHopper Kernel}
The GraphHopper kernel is very related to the shortest path kernel.
In the case of graphs with discrete node labels, the kernels  and  of the shortest-path kernel which compare vertex labels and path lengths correspond typically to dirac kernels.
Hence, nodes and shortest path lengths are considered similar if they are completely identical.
That specific instance of the shortest path kernel allows the use of an explicit computation scheme which is very efficient, even for larger datasets.
However, for attributed graphs, such an explicit mapping is no longer possible.
This has a large impact on the runtime of the algorithm which is generally , and makes the kernel unfeasible for many real-world applications.
GraphHopper is a kernel which also compares shortest paths between node pairs from the two graphs, but with a different path kernel
\cite{feragen2013scalable}.
The kernel takes into account both path lengths and the vertices encountered while ``hopping'' along shortest paths.
The kernel is equivalent to a weighted sum of node kernels.
Moreover, it can handle both labeled and attributed graphs, and is much more efficient than the shortest-path kernel.

Let  be a graph.
The graph contains discrete node labels, continuous node attributes or both.
Let  be a labeling function that assigns either discrete labels or continuous attributes to vertices.
The kernel compares node labels/attributes using a kernel  (\eg delta kernel in the case of node labels, and linear or gaussian kernel in the case of node attributes).
Given two vertices , a path  from  to  in  is defined as a sequence of vertices

where ,  and  for all .
Let  denote the  vertex encountered when ``hopping'' along the path.
Denote by  the weighted length of  and by  its discrete length, defined as the number of vertices in .
The shortest path  from  to  is defined in terms of weighted length.
The diameter  of  is the maximal number of nodes in a shortest path in , with respect to the weighted path length.

\begin{definition}[GraphHopper Kernel]
  The GraphHopper kernel is defined as a sum of path kernels  over the families  of shortest
  paths in 
  
  The path kernel  is a sum of node kernels  on vertices simultaneously encountered while simultaneously hopping along paths  and  of equal discrete length, that is
  
\end{definition}

The  kernel can be decomposed into a weighted sum of node kernels

where  counts the number of times  and  appear at the same hop, or coordinate,  of shortest paths  of equal discrete length .
We can decompose the weight  as

where  is a  matrix whose entry  counts how many times  appears at the  coordinate of a shortest path in  of discrete length , and .
The components of these matrices can be computed efficiently using recursive message-passing algorithms. 
The total complexity of computing the GraphHopper kernel is  where  is the number of vertices,  is the number of edges and  is the dimensionality of the node attributes ( in the case of discrete node labels).


\subsubsection{Graph Invariant Kernels}
Kernels for attributed graphs have received increased attention recently, and research efforts have focused not only on new kernels, but also on frameworks for building kernels that can handle such continuous node attributes.
The graph invariant kernels are instances of such a framework \cite{orsini2015graph}.
These kernels decompose graphs into sets of vertices, and compare them to each other using a kernel that measures their similarity both in terms of their attributes and in terms of their structural roles.

Let  be a graph.
Let  be a decomposition relation that specifies a decomposition of  into its parts.
Then, we denote by  the multiset of all patterns in .
An example of such a decomposition relation is the one that generates neighborhood subgraphs.
Graph invariant kernels compare vertices of graphs based on their attributes, but also based on their  structural role in subgraphs obtained using a decomposition relation.
\begin{definition}[Graph Invariant Kernel]
  Given two attributed graphs  and , the graph invariant kernels compare the attributes of all pairs of vertices of the two graphs using a kernel
  
  where  is a kernel between vertex attributes, and  is a weight function defined as follows
  
  where  is a dirac function that determines whether two patterns match,  are the set of vertices of patterns , and  is an indicator function. 
\end{definition}
If  are subgraphs of ,  can be a dirac function that compares the canonical representations of the subgraphs obtained by applying a labeling function which produces efficient string encodings of the subgraphs along with a hash function from strings to natural numbers.  
The indicator function  from all the subgraphs extracted from the two graphs selects only those in which vertices  and  are involved into.
The kernel function  is used to measure the similarity between the colors produced by a vertex invariant  and encodes the extent to which the vertices play the same structural role in the two subgraphs.
By employing different graph invariants , different instances of graph invariant kernels emerge.
Some common graph invariants include the Weisfeiler-Lehman relabeling procedure and coloring methods that capitalize on diffusion updates.
For kernels that decompose graphs into sets of subgraphs, their complexity is 

\subsubsection{Propagation Kernel}
The propagation kernel is another instance of the neighborhood aggregation framework, and in contrast to most other instances, it can handle continuous node attributes \cite{neumann2016propagation}.
The kernel leverages quantization in order to transform continuous node attributes to discrete labels.
Similarly to the Weisfeiler-Lehman subtree kernel, the propagation kernel applies an iterative procedure which updates the node attributes, places the nodes into bins based on their attributes, and counts nodes that fall into the same bins in two graphs. 

Let  be a node-attributed graph.
Let also  be a matrix whose  row contains the intial attribute of vertex .
The propagation kernel first uses a hash function that maps the node attributes to integer-valued bins, such that vertices with similar attributes end up in the same bin.
Hence, this function maps each row of matrix  to an integer.
Then, the kernel employs a propagation scheme to update the attributes of the vertices.
Different schemes can be employed.
A common scheme updates node attributes as follows

where  is the transition matrix, \ie the row-normalized adjacency matrix , and  is a diagonal matrix with .
The above two steps (hashing and update of node attributes) are performed for  iterations.

\begin{definition}[Propagation Kernel]
  Let ,  be two node-attributed graphs.
  Define  as the number of integer bins occupied by nodes of  and  after applying the hashing function to the node attributes at the  iteration of the algorithm.
  Let also  be the number of nodes of  placed into bin  at the  iteration of the algorithm.
  Then, the propagation kernel on two graphs  and  with  iterations is defined as
  
  where
  
  and
  
\end{definition}
An illustration of the propagation kernel is given in Figure~\ref{fig:propagation_kernel}.
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{propagation_kernel}
  \caption{Propagation kernel computation. Distributions, bins, count features, and kernel contributions for two graphs  and  with binary node labels and one iteration of label propagation. Node-label distributions are decoded by color.}
  \label{fig:propagation_kernel}
\end{figure}
The total runtime complexity of the propagation kernel is .


\subsubsection{Multiscale Laplacian Graph Kernel}
The multiscale Laplacian graph kernel can handle unlabeled graphs, graphs with discrete node labels and graphs with continuous node attributes \cite{kondor2016multiscale}.
It takes into account structure in graphs at a range of different scales by building a hierarchy of nested subgraphs.
These subgraphs are compared to each other using another graph kernel, called the feature space Laplacian graph kernel.
This kernel is capable of lifting a base kernel defined on the vertices of two graphs to a kernel between the graphs themselves.
Since exact computation of the multiscale Laplacian graph kernel is a very expensive operation, the kernel uses a randomized projection procedure  similar to the popular Nystr{\"o}m approximation for kernel matrices \cite{williams2001using}.

Let  be an undirected graph such that  and let  be the Laplacian of .
Given two graphs  and  of  vertices, we can define the kernel between them to be a kernel between the corresponding normal distributions  and  where  is the -dimensional all-zeros vector.
Note that the Laplacian matrices of the two graphs have a zero eigenvalue eigenvector.
Hence, in order to be able to invert them, the algorithm adds a small constant ``regularizer''  to them.
In the following, we denote the regularized Laplacians of  and  by  and , respectively.
More specifically, given two graphs  and  of  vertices with regularized Laplacians  and  respectively, the Laplacian graph kernel with parameter  between the two graphs is

where ,  and  is the  identity matrix.
The Laplacian graph kernel captures similarity between the overall shapes of the two graphs.
However, it assumes that both graphs have the same size, and it is not invariant to permutations of the vertices.

To achieve permutation invariance, the multiscale Laplacian graph kernel represents each vertex as a -dimensional vector whose components correspond to local and permutation invariant vertex features.
Such features may include for instance the degree of the vertex or the number of triangles in which it participates.
Then, it performs a linear transformation and represents each graph as a distribution of the considered features instead of a distribution of its vertices.
Let  be the feature mapping matrices of the two graphs, that is the matrices whose columns contain the vector representations of the vertices of the two graphs. 
Then, the feature space Laplacian graph kernel is defined as

where ,  and  is the  identity matrix.
Since the vertex features are local and invariant to vertex reordering, the feature space Laplacian graph kernel is permutation invariant.
Furthermore, since the distributions now live in the space of features rather than the space of vertices, the feature space Laplacian graph kernel can be applied to graphs of different sizes.

Let  be the representation of vertex  constructed from local vertex features as described above.
The base kernel  between two vertices  and  corresponds to the dot product of their feature vectors

Let  and  be two graphs with vertex sets  and  respectively, and let  be the union of the two vertex sets.
Let also  be the kernel matrix defined as

Let  be a maximal orthonormal set of the non-zero eigenvalue eigenvectors of 
with corresponding eigenvalues .
Then the vectors

where  is the  component of vector  form an orthonormal basis for the subspace .
Moreover, let  and  denote the first  and last  rows of matrix  respectively.
Then, the generalized feature space Laplacian graph kernel induced from the base kernel  is defined as

where  and  where  is the  identity matrix.

The multiscale Laplacian graph kernel builds a hierarchy of nested subgraphs, where each subgraph is centered around a vertex and computes the generalized feature space Laplacian graph kernel between every pair of these subgraphs.
Let  be a graph with vertex set , and  a positive semi-definite kernel on .
Assume that for each , we have a nested sequence of  neighborhoods

and for each , let  be the corresponding induced subgraph of .
The multiscale Laplacian subgraph kernels are defined as  as follows
\begin{enumerate}
    \item  is just the generalized feature space Laplacian graph kernel  induced from the base kernel  between the lowest level subgraphs (\ie the vertices)
    
    \item For ,  is the the generalized feature space Laplacian graph kernel induced from  between  and 
    
\end{enumerate}

\begin{definition}[Multiscale Laplacian Graph Kernel]
	Let  be two graphs.
	The multiscale Laplacian graph kernel between the two graphs is defined as follows
	
\end{definition}
The multiscale Laplacian graph kernel computes  for all pairs of vertices, then computes  for all pairs of vertices, and so on.
Hence, it requires  kernel evaluations.
At the top levels of the hierarchy each subgraph centered around a vertex  may have as many as  vertices.
Therefore, the cost of a single evaluation of the generalized feature space Laplacian graph kernel may take  time.
This means that in the worst case, the overall cost of computing  is .
Given a dataset of  graphs, computing the kernel matrix requires repeating this for all pairs of graphs, which takes  time and is clearly problematic for real-world settings.

The solution to this issue is to compute for each level  a single joint basis for all subgraphs at the given level across all graphs.
Let  be a collection of graphs,  their vertex sets, and assume that  for some general vertex space .
The joint vertex feature space of the whole graph collection is .
Let  be the total number of vertices and  be the concatenation of the vertex sets of all graphs.
Let  be the corresponding joint kernel matrix and  be a maximal orthonormal set of non-zero eigenvalue eigenvectors of  with corresponding eigenvalues  and .
Then the vectors

form an orthonormal basis for .
Moreover, let  and  denote the first  rows of matrix ,  denote the next  rows of matrix  and so on.
For any pair of graphs  of the collection, the generalized feature space Laplacian graph kernel induced from  can be expressed as

where ,  and  is the  identity matrix.

Computing the kernel matrix between all vertices of all graphs ( vertices in total) and storing it is a very costly procedure.
Computing its eigendecomposition is even worse in terms of the required runtime.
Morever,  is also very large.
Hence, managing the  matrices (each of which is of size ) becomes infeasible.
Hence, the multiscale Laplacian graph kernel replaces  with a smaller, approximate joint features space.
Let  be  vertices sampled from the joint vertex set.
Then, the corresponding subsampled vertex feature space is .
Let .
Similarly to before, the kernel constructs an orthonormal basis  for  by forming the (now much smaller) kernel matrix , computing its eigenvalues and eigenvectors, and setting . 
The resulting approximate generalized feature space Laplacian graph kernel is

where ,  are the projections of  and  to  and  is the  identity matrix.
Finally, the kernel introduces a further layer of approximation by restricting  to be the space spanned by the first  basis vectors (ordered by descending eigenvalue), effectively doing kernel PCA on .
The combination of these two factors makes computing the entire stack of kernels feasible, reducing the complexity of computing the kernel matrix for a dataset of  graphs to .


\subsection{Frameworks}
Besides the kernels themselves, research on graph kernels has also focused on \textit{frameworks} and approaches that can be applied to existing graph kernels and increase their performance.
The most popular of all frameworks is perhaps the Weisfeiler-Lehman framework which has been already presented \cite{shervashidze2011weisfeiler}.
Interestingly, any kernel that can handle discrete node labels can be plugged into that framework.
Recently, two other frameworks were presented for deriving variants of popular -convolution graph kernels \cite{yanardag2015deep,yanardag2015structural}.
Inspired by recent advances in NLP, these frameworks offer a way to take into account similarity between substructures.
In addition, a method that combines several kernels using the multiple kernel learning framework was also recently proposed \cite{aiolli2015multiple}.
Another recently proposed framework generates a hierarchy of subgraphs and compares the corresponding according to the hierarchy subgraphs using graph kernels \cite{nikolentzos2018}.
Moreover, a recent approach employs graph kernels and performs a series of successive embeddings in order to derive more expressive kernels \cite{nikolentzos2018enhancing}.
Some of these frameworks are described in more detail below.

\subsubsection{Frameworks Dealing with Diagonal Dominance}
We next present two frameworks that are inspired by recent advances in natural language processing, namely the deep graph kernels framework \cite{yanardag2015deep} and the structural smoothing framework \cite{yanardag2015structural}. 
These two frameworks were developed to address the problem of diagonal dominance which is inherent to -convolution kernels.
The feature space of these kernels is usually large (\ie grows exponentially) and we encounter the sparsity problem: only a few substructures will be common across graphs, and therefore each graph is similar to itself, but not to any other graph in the dataset.
However, the substructures used to define a graph kernel are often related to each other, but commonly-used -convolution kernels respect only exact matchings. 
For example, when the features correspond to large graphlets (\eg ), two graphs may be composed of many similar graphlets, but not any identical.
As a consequence, the kernel value between the two graphs (\ie inner product of their feature representations) will be equal to  even though the two graphs are similar to each other.

Ideally, we would like the kernels to output large values for pairs of graphs that belong to the class, and lower values for pairs of graphs that belong to different classes.
To deal with the aforementioned problem, the deep graph kernels framework computes the kernel between two graphs  and  as follows

where  represents a positive semidefinite matrix that encodes the relationship between substructures and  are the representations of graphs  according to a graph kernel which contains counts of atomic substructures.
Therefore, one can design an  matrix that respects the similarity of the substructure space.
Clearly, the deep graph kernels framework can be applied only to graph kernels whose feature maps  can be computed explicitly.

Matrix  can be generated by manually defining functions to compare substructures or alternatively, it can be learned using techniques inspired from the field of natural language processing.
When substructures exhibit a clear mathematical relationship, one can define a function to measure the similarities between them (\eg edit distance in the case of graphlets).
However, the above approach requires manually designing the similarity functions.
Furthermore, in many cases, it becomes prohibitively expensive to compare all pairs of substructures.
On the other hand, learning the latent representations of substructures is more efficient and does not involve any manual intervention.
Matrix  can then be computed based on the learned representations.
To learn a latent representation for each substructure, the framework utilizes recent approaches for generating word embeddings such as the continuous bag-of-words (CBOW) and Skip-gram models \cite{mikolov2013distributed}.
These models generate semantic representations from word co-occurrence statistics derived from large text corpora.
However, unlike words in a traditional text corpora, substructures of graphs do not have a linear co-occurrence relationship.
Hence, these co-occurrence relationships need to be manually defined. 
Yanardag and Vishwanathan \citeyear{yanardag2015deep} propose a methodology on how to generate corpora where co-occurrence relationship is meaningful on three popular kernels, namely the Weisfeiler-Lehman subtree kernel, the graphlet kernel, and the shortest path kernel.

The structural smoothing framework is inspired by recent smoothing techniques in natural language processing.
Similar to the deep graph kernels framework, this framework also can be applied only to graph kernels whose feature maps  can be computed explicitly.
The framework takes structural similarity into account by constructing a directed acyclic graph (DAG) that encodes the relationships between lower and higher order substructures.
Each vertex of the DAG corresponds to a substructure (and also to a feature in the explicit graph representation).
For each substructure  of size , the framework determines all possible substructures of size  into which  can be reduced.
These substructures are the parents of , and a weighted directed edge is drawn from each parent to its children vertices.
Since all descendants of a given substructure at depth  are at depth , the emerging graph is indeed a DAG.
Yanardag and Vishwanathan \citeyear{yanardag2015deep} present how such a DAG can be constructed for three popular graph kernels, namely the Weisfeiler-Lehman subtree kernel, the graphlet kernel, and the shortest path kernel.
Given the DAG, the structural smoothing for a substructure  at level  is defined as

where  denotes the number of times substructure  appears in the graph,  denotes the total number of substructures present in the graph,  is a discount factor,  is the number of substructures whose counts are larger than ,  denotes the weight of the edge connecting vertex  to vertex ,  denotes the parents of vertex , and  the children of vertex .
The above equation subtracts a fixed discount factor  from every substructure that appears in the graph, and accumulates it to a total mass of .
Each substructure  receives some portion of this accumulated probability mass from its parents.
The proportion of the mass that a parent  at level  transmits to a given child a depends on the weight  between the parent and the child, and the probability mass  that is assigned to the parent.
It is thus clear that, even if a graph does not contain a substructure  (\ie ), its value in the feature vector may become greater than  (\ie ).

\subsubsection{Core Framework}
The core framework is another tool for improving the performance of graph kernels \cite{nikolentzos2018}.
The framework is not restricted to graph kernels, but can be applied to any graph comparison algorithm.
It capitalizes on the -core decomposition which is capable of uncovering topological and hierarchical properties of graphs.
Specifically, the -core decomposition is a powerful tool for network analysis and it is commonly used as a measure of importance and well connectedness for vertices in a broad spectrum of applications.
The notion of -core was first introduced by Seidman to study the cohesion of social networks \cite{seidman1983network}.
In recent years, the -core decomposition has been established as a standard tool in many application domains such as in network visualization \cite{alvarez2006large}.

\paragraph{Core Decomposition.}
Let  be an undirected and unweighted graph.
Given a subset of vertices , let  be the set of edges that have both end-points in .
Then,  is the subgraph induced by .
We use  to denote that  is a subgraph of .
The degree of a vertex , , is equal to the number of vertices that are adjacent to  in .
Let  be a graph and  a subgraph of  induced by a set of vertices .
Then,  is defined to be a -core of , denoted by , if it is a maximal subgraph of  in which all vertices have degree at least .
Hence, if  is a -core of , then , .
Each -core is a unique subgraph of , and it is not necessarily connected.
The core number  of a vertex  is equal to the highest-order core that  belongs to.
In other words,  has core number , if it belongs to the -core but not to the -core.
The degeneracy  of a graph  is defined as the maximum  for which graph  contains a non-empty -core subgraph, .
Furthermore, assuming that  is the set of all -cores, then  forms a nested chain

Therefore, the -core decomposition is a very useful tool for discovering the hierarchical structure of graphs.
The -core decomposition of a graph can be computed in  time \cite{matula1983smallest,batagelj2011fast}. 
The underlying idea is that we can obtain the -core of a graph if we recursively remove all vertices with degree less than  and their incident edges from the graph until no other vertex can be removed.

\paragraph{Core Kernels.}
The -core decomposition builds a hierarchy of nested subgraphs, each having stronger connectedness properties compared to the previous ones.
The core framework measures the similarity between the corresponding according to the hierarchy subgraphs and aggregates the results.
Let  and  be two graphs.
Let also  be any kernel for graphs.
Then, the core variant of the base kernel  is defined as

where  is the minimum of the degeneracies of the two graphs, and  and  are the -core, -core,, -core subgraphs of  and , respectively.
By decomposing graphs into subgraphs of increasing importance, the algorithm is capable of more accurately capturing their underlying structure.

The computational complexity of the core framework depends on the complexity of the base kernel and the degeneracy of the graphs under comparison.
Given a pair of graphs  and an algorithm  for comparing the two graphs, let  be the time complexity of algorithm .
Let also  be the minimum of the degeneracies of the two graphs.
Then, the complexity of computing the core variant of algorithm  is .


\section{Applications of Graph Kernels}\label{sec:applications}
In the past years, graph kernels have been applied successfully to a series of real-world problems.
Most of these problems come from the fields of bioinformatics and chemoinformatics.
However, graph kernels are not limited only to these two fields, but they have been applied to problems arising in other domains as well.
We list below some examples of such fields of application.

\subsection{Chemoinformatics}
Traditionally, chemistry is one of the richest sources of graph-structured data.
A common problem in this field is to find chemical compounds with a specific property or activity.
The experimental characterization of molecules is often an expensive and time-consuming process, and thus people usually resort to computational methods.
Specifically, they model chemical compounds as graphs where vertices correspond to atoms and edges to bonds, and then they apply computational methods to identify a small set of potentially interesting molecules for a given property or activity, which are then tested experimentally.
Graph kernels have been used extensively for predicting the mutagenicity, toxicity and anti-cancer activity of small molecules \cite{swamidass2005kernels,ralaivola2005graph,mahe2005graph,ceroni2007classification,mahe2009graph,smalter2009graph}
Furthermore, graph kernels have been applied to other problems such as the prediction of the atomization energies of organic molecules \cite{ferre2017learning}, the predicition of the boiling points of molecules \cite{gauzere2011two}, the prediction of the activity against HIV \cite{gauzere2011two}, and the prediction of properties of stereoisomers \cite{brown2010compound,grenier2017chemoinformatics}.
A review of the applications of graph kernels in chemoinformatics is provided by Rupp and Schneider \citeyear{rupp2010graph}.

\subsection{Bioinformatics}
Bioinformatics is also one of the major application domains of graph representations and therefore, of graph kernels.
Recent advances in technology have delivered a step change in our ability to sequence genomes, measure gene expression levels, and test large numbers of potential regulatory interactions between genes.
Despite these advancements, some problems of high interest such as the experimental determination of the function of a protein still remain both expensive and time-consuming.
Interestingly, the above-mentioned processes produce large volumes of data which can give rise to various types of graphs, such as protein structures, protein and gene co-expression networks, or protein-protein interaction networks.
These graphs can then be processed by computational approaches such as graph kernels, and provide solutions to some of these challenging problems. 
Among others, in the field of bioinformatics, graph kernels have been applied to the prediction of the function of proteins with known sequence and structure \cite{borgwardt2005protein,schietgat2015predicting}, to the identification of the interactions that are involved in disease outbreak and progression \cite{borgwardt2007graphkernels}, to the analysis of functional non-coding RNA sequences \cite{sato2008directed}, to the identification of temporally localized relationships among genes \cite{antoniotti2010application}, and to the prediction of domain-peptide interactions \cite{kundu2013graph}.

\subsection{Computer Vision}
Graph representations have been investigated a lot in the fields of image processing and computer vision.
There exist many different approaches to represent images as graphs.
For instance, vertices usually correspond to pixels or to segmented regions, while edges join neighboring pixels or neighboring regions with each other.
Graph kernels have served as an effective tool for many computer vision tasks such as for classifying images \cite{harchaoui2007image,mahboubi2010object,antanas2012relational,zhang2013fast}, for detecting objects represented as point clouds \cite{bach2008graph,neumann2013graph}, for achieving place recognition \cite{stumm2016robust}, for achieving action recognition \cite{wang2013directed,wu2014human,li20163d}, for scene  modeling \cite{fisher2011characterizing}, and for matching observations of persons across different cameras \cite{brun2011people}.

Besides the above applications, graphs are also used increasingly often in biomedical imaging.
Different types of graphs such as connectivity graphs are usually extracted from functional magnetic resonance imaging (fMRI) data.
Then, graph kernels capitalize on these graphs to address various tasks such as to distinguish between different brain states \cite{shahnazian2012method,mokhtari2013decoding,vega2013brain,vega2014classification}, to determine whether a subject is cocaine-addicted or not \cite{gkirtzou2016pyramid}, or to predict mild cognitive impairment, a prodromal stage of Alzheimer's disease \cite{jie2014topological,jie2016sub}.
There have also been proposed kernels that can handle inter-subject variability in fMRI data \cite{takerkart2014graph}.

\subsection{Cybersecurity and Software Verification}
The number of malicious applications targeting desktop and mobile devices has exploded in the past few years.
Due to this unprecedented increase in the number of malicious applications, malware detection has recently become a very active area of research.
It has been observed that most newly discovered malware samples are variations of existing malware. 
Furthermore, it has been shown that it is easier to detect these variations if high-level code representations, such as function call graphs or control flow graphs, are employed.
It should be mentioned that these graphs can prove useful not only for detecting malware, but also for retrieving similar application programs.
Therefore, graph kernels can be applied to such graphs, and have served as a common tool for detecting malware \cite{anderson2011graph,gascon2013structural,narayanan2016contextual}, but also for analyzing execution traces obtained from dynamic analysis \cite{wagner2009malware}, for measuring the similarity between programs \cite{li2016detecting}, and for predicting metamorphic relations \cite{kanewala2016predicting}.

\subsection{Natural Language Processing}
Although textual documents do not exhibit an underlying graph structure, in many cases, they are also modeled as graphs.
A vertex corresponds to some meaningful linguistic unit such as a sentence, a word, or even a character, while an edge corresponds to some relationship between two vertices which can be statistical, syntactic, or semantic among others.
A common representation is the word co-occurence network, where vertices correspond to terms and edges represent co-occurrences between the terms within a fixed-size sliding window.
This representation addresses some of the limitations of the bag-of-words representation which treats terms as independent of one another.
Graph kernels have proven useful for several text mining applications such as for recognizing identical real-world events modeled as event graphs \cite{glavavs2013recognizing}, for classifying biomedical text documents represented as concept graphs \cite{bleik2013text}, for extracting protein-protein interactions from scientific literature \cite{airola2008graph,airola2008all}, and for measuring the similarity between documents represented as word co-occurence networks \cite{nikolentzos2017shortest}.


\subsection{Others}
Graph kernels have been applied to many other practical problems involving graph representations such as for classifying Resource Description Framework (RDF) data \cite{losch2012graph,de2015substructure}, for entity disambiguation in anonymized graphs \cite{hermansson2013entity}, for classifying architectural designs into architectural styles \cite{strobbe2016automatic}, and for estimating the similarity of relational states in relational reinforcement learning \cite{driessens2006graph,halbritter2007learning}.


\section{Experimental Comparison}\label{sec:experiments}
In this Section, we experimentally evaluate many of the graph kernels presented above and compare them to each other.
Although there are approaches that measure the expressiveness of graph kernels by exploiting recent results in the field of statistical learning theory \cite{oneto2017measuring}, empirically evaluating the graph kernels can provide insights into their utility in real-world scenarios.
We first present the problem of graph classification.
We then describe the datasets that we used for our experiments, and give details about the experimental settings.
Finally, we report on the performance and running time of the different kernels.


\subsection{Graph Classification}
Classification is perhaps the most frequently encountered machine learning problem.
In classification, the goal is to learn a mapping from inputs to outputs, given a training set.
When the input objects are graphs, the problem is called \textit{graph classification}.
More formally, in this setting, we are given a training set  consisting of  graphs along with their class labels.
The goal is to learn a function , where  is the input space of graphs and  the set of graph labels.
This function can then be used to assign class labels to new previously unseen graphs, such as those contained in the test set.

The problem of graph classification has become a popular area of research in recent years because it finds numerous applications in a wide variety of fields.
Several of these application have already been discussed above.
For example, graph classification arises in applications which range from predicting the mutagenicity of a chemical compound \cite{swamidass2005kernels}, and predicting the function of a protein given its amino acid sequence \cite{borgwardt2005protein}, to detecting if a software object is infected with malware \cite{wagner2009malware}.

\subsection{Datasets}
We next briefly describe the graph datasets used in our experiments.
We have considered data from different domains, including chemoinformatics, bioinformatics and social networks.
All graphs are undirected.
Furtermore, the graphs contained in the chemoinformatics and bioinformatics datasets are node-labeled, node-attributed or both.
All datasets are publicly available \cite{KKMMN2016}.
Table~\ref{tab:dataset_statistics} provides a summary of the employed datasets.
\begin{table}[t]
\centering
\begin{sc}
\def\arraystretch{1.2}
\resizebox{\textwidth}{!} {
\begin{tabular}{|l|c|c|d{3.2}|d{3.2}|d{5.2}|cc|cc|} \hline
\multirow{4}{*}{Dataset} & \multicolumn{5}{c|}{\multirow{2}{*}{Statistics}} & \multicolumn{4}{c|}{Node Labels/} \\ 
& \multicolumn{5}{c|}{}  & \multicolumn{4}{c|}{Attributes} \\ \cline{2-10} 
& \multirow{2}{*}{\#Graphs} & \multirow{2}{*}{\#Classes} & \multicolumn{1}{c|}{Max Class} & \multicolumn{1}{c|}{Avg.} & \multicolumn{1}{c|}{Avg.} & \multicolumn{2}{c|}{Labels} & \multicolumn{2}{c|}{Attributes} \\
& & & \multicolumn{1}{c|}{Imbalance} & \multicolumn{1}{c|}{\#Nodes} & \multicolumn{1}{c|}{\#Edges} & \multicolumn{2}{c|}{(Num.)} & \multicolumn{2}{c|}{(Dim.)} \\ \hline
AIDS                          & 2,000           & 2      & 1:4.0   & 15.69                & 16.20                & + & (38)      & + &(4)                             \\ \hline
BZR                           & 405             & 2      & 1:3.70  & 35.75                & 38.36                & + & (10)      & + &(3)                             \\ \hline
COLLAB                        & 5,000           & 3      & 1:3.35  & 74.49                & 2,457.78             & --&          & --&                                 \\ \hline
D\&D                            & 1,178           & 2      & 1:1.41  & 284.32               & 715.66               & + & (82)      & --&                                 \\ \hline
ENZYMES                       & 600             & 6      & 1:1     & 32.63                & 62.14                & + & (3)       & + &(18)                            \\ \hline
IMDB-BINARY                   & 1,000           & 2      & 1:1     & 19.77                & 96.53                & --&          & --&                                 \\ \hline
IMDB-MULTI                    & 1,500           & 3      & 1:1     & 13.00                & 65.94                & --&          & --&                                 \\ \hline
MUTAG                         & 188             & 2      & 1:1.98  & 17.93                & 19.79                & + & (7)       & --&                                 \\ \hline
NCI1                          & 4,110           & 2      & 1:1     & 29.87                & 32.30                & + & (37)      & --&                                 \\ \hline
PROTEINS                      & 1,113           & 2      & 1:1.47  & 39.06                & 72.82                & + &(3)       & + &(1)                             \\ \hline
PROTEINS\_full                & 1,113           & 2      & 1:1.47  & 39.06                & 72.82                & + &(3)       & + &(29)                            \\ \hline
PTC-MR                        & 344             & 2      & 1:1.26  & 14.29                & 14.69                & + &(19)      & --&                                 \\ \hline
REDDIT-BINARY                 & 2,000           & 2      & 1:1     & 429.63               & 497.75               & --&          & --&                                 \\ \hline
REDDIT-MULTI-5K               & 4,999           & 5      & 1:1     & 508.52               & 594.87               & --&         & -- &                               \\ \hline
REDDIT-MULTI-12K              & 11,929          & 11     & 1:5.05  & 391.41               & 456.89               & --&          & -- &                                \\ \hline
SYNTHETICnew                  & 300             & 2      & 1:1     & 100.00               & 196.25               & --&          & + &(1)                             \\ \hline
Synthie                       & 400             & 4      & 1:1.22  & 95.00                & 172.93               & --&          & + &(15)                            \\ \hline
\end{tabular}
}
\end{sc}
\caption{Summary of the  datasets used in our experiments. The ``Max Class Imbalance'' column indicates the ratio of the size of the smallest class of the dataset to the size of its largest class.}
\label{tab:dataset_statistics}
\end{table}

\paragraph{AIDS.} It consists of molecular compounds represented as graphs.
The compounds were obtained from the AIDS Antiviral Screen Database of Active Compounds.
Vertices correspond to atoms and edges to covalent bonds.
Vertices are labeled with the corresponding chemical symbol and edges with the valence of the linkage.
The task is to predict whether or not each compound is active against HIV \cite{riesen2008iam}.
\paragraph{BZR.} It contains  chemical compounds (ligands for the benzodiazepine receptor) which are modeled as graphs.
The task is to predict whether a compound is active or inactive \cite{sutherland2003spline}.
\paragraph{COLLAB.} This is a scientific collaboration dataset consisting of the ego-networks of several researchers from three subfields of Physics (High Energy Physics, Condensed Matter Physics and Astro Physics).
The task is to determine the subfield of Physics to which the ego-network of each researcher belongs \cite{yanardag2015deep}. 
\paragraph{D\&D.} This dataset contains over a thousand protein structures.
Each protein is a graph whose vertices correspond to amino acids and a pair of amino acids are connected by an edge if they are less than  \AA ngstroms apart.
The task is to predict if a protein is an enzyme or not \cite{dobson2003distinguishing}.
\paragraph{ENZYMES.} It comprises of  protein tertiary structures obtained from the BRENDA enzyme database.
Each enzyme is a member of one of the Enzyme Commission top level enzyme classes (EC classes) and the task is to correctly assign the enzymes to their classes \cite{borgwardt2005protein}.
\paragraph{IMDB-BINARY and IMDB-MULTI.} These datasets were created from IMDb (\url{www.imdb.com}), an online database of information related to movies and television programs. 
The graphs contained in the two datasets correspond to movie collaborations.
The vertices of each graph represent actors/actresses and two vertices are connected by an edge if the corresponding actors/actresses appear in the same movie.
Each graph is the ego-network of an actor/actress, and the task is to predict which genre an ego-network belongs to \cite{yanardag2015deep}.
\paragraph{MUTAG.} This dataset consists of  mutagenic aromatic and heteroaromatic nitro compounds.
The task is to predict whether or not each chemical compound has mutagenic effect on the Gram-negative bacterium {\it Salmonella typhimurium} \cite{debnath1991structure}.
\paragraph{NCI1.} This dataset contains a few thousand chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines \cite{wale2008comparison}.
\paragraph{PROTEINS, PROTEINS\_full.} They contain proteins represented as graphs where vertices are secondary structure elements and there is an edge between two vertices if they are neighbors in the amino-acid sequence or in D space.
The task is to classify proteins into enzymes and non-enzymes \cite{borgwardt2005protein}. 
\paragraph{PTC-MR.} This dataset contains  organic molecules represented as graphs.
The task is to predict their carcinogenic effects on male rats \cite{toivonen2003statistical}. 
\paragraph{REDDIT-BINARY, REDDIT-MULTI-5K, REDDIT-MULTI-12K.} The graphs contained in these three datasets represent social interaction between users of Reddit (\url{www.reddit.com}), one of the most popular social media websites.
Each graph represents an online discussion thread.
Specifically, each vertex corresponds to a user, and two users are connected by an edge if one of them responded to at least one of the other's comments.
The task is to classify graphs into either communities or subreddits \cite{yanardag2015deep}.
\paragraph{SYNTHETICnew.} It comprises of  synthetic graphs divided into two classes of equal size.
Each graph is obtained by adding noise to a random graph with  nodes and  edges, whose vertices are endowed with normally distributed scalar attributes sampled from .
The graphs of the first class were generated by rewiring  edges and permuting  node attributes, while the graphs of the second class were generated by rewiring  edges and permuting  node attributes.
After the generation of all graphs, noise from  was also added to every node attribute in every graph \cite{feragen2013scalable}.
\paragraph{Synthie.} This dataset consists of  synthetic graphs, subdivided into four classes, with  real-valued node attributes.
Two types of graphs and two types of attributes were generated, and each combination of those gave rise to a class (four classes in total).
All graphs were generated by randomly adding edges between  perturbed instances of two Erd{\"o}s R{\'e}nyi graphs.
To generate graphs of the first type, perturbed instances of the first Erd{\"o}s R{\'e}nyi graph were choosen with probability , while perturbed instances of the second Erd{\"o}s R{\'e}nyi graph were choosen with probability .
To generate graphs of the second type, the two probabilities were reversed.
The vertices of each graph were then annotated by attributes drawn either from the first or from the second type of attributes \cite{morris2016faster}.


\subsection{Experimental Setup}
We evaluated the performance of the graph kernels on the datasets presented above. 
Specifically, we made use of the GraKeL library which contains implementations of a large number of graph kernels \cite{siglidis2018grakel}.
We used the following  kernels in our experimental evaluation: () vertex histogram kernel (VH), () random walk kernel (RW), () shortest path kernel (SP), () Weisfeiler-Lehman subtree kernel (WL-VH), () Weisfeiler-Lehman shortest path kernel (WL-SP), () Weisfeiler-Lehman pyramid match kernel (WL-PM), () neighborhood hash kernel (NH), () neighborhood subgraph pairwise distance kernel (NSPDK), () ordered decompositional DAGs with subtree kernel (ODD-STh), () pyramid match kernel (PM), () GraphHopper kernel (GH), () subgraph matching kernel (SM), () propagation kernel (PK), () multiscale Laplacian kernel (ML), () core Weisfeiler-Lehman subtree kernel (CORE-WL), () core shortest path kernel (CORE-SP).
Note that some of the kernels (\eg WL-SP, CORE-SP) correspond to frameworks applied to graph kernels.
Furthermore, since some kernels can handle different types of graphs than others, we conduct three distinct experiments.
The three experiments are characterized by the types of graphs contained in the employed datasets: () datasets with unlabeled graphs, () datasets with node-labeled graphs, and () datasets with node-attributed graphs.
It is important to mention that kernels that are designed for node-labeled graphs can also be applied to unlabaled graphs by initializing the node labels of all vertices of the unlabaled graphs to the same value.
Hence, we evaluate these kernels on datasets that contain node-labeled graphs, but also on datasets that contain unlabeled graphs.
Moreover, kernels that are designed for node-attributed graphs can be applied to unlabeled graphs and to graphs that contain dicrete node labels.
To achieve that, in the case of unlabeled graphs, all the vertices of all graphs are assigned the same attribute, while in the case of node-labeled graphs, node labels are transformed into feature vectors (\eg using a ``one-hot'' encoding scheme).
Hence, we evaluated these kernels on all three experimental scenarios.

In all cases, to perform graph classification, we employed a Support Vector Machine (SVM) classifier and in particular, the LIB-SVM implementation \cite{chang2011libsvm}.
To evaluate the performance of the different kernels, we performed -fold cross-validation.
Furthermore, since the size of most of the above datasets is not large, the whole process was repeated  times in order to exclude random effects of the fold assignments.
Within each fold, the parameter  of the SVM and the hyperparameters of the kernels (see below) were chosen based on a validation experiment on a single  split of the training data.
We chose the value of parameter  from .
Moreover, we normalized all kernel matrices.
All experiments were performed on a cluster of  Intel\textsuperscript{\textcopyright} Xeon\textsuperscript{\textcopyright} CPU E @ GHz with TB RAM.
Note that each kernel was computed on a single thread of the cluster.
We set a time limit of  hours for each kernel to compute the kernel matrix.
Hence, we denote by \texttt{TIMEOUT} kernel computations that did not finish within one day.
We also set a memory limit of GB, and we denote by \texttt{OUT-OF-MEM} computations that exceeded this limit.

\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.2}
\begin{tabular}{|l|c|c|} \hline
\multirow{2}{*}{Kernels} & \multicolumn{2}{c|}{Hyperparameters} \\ \cline{2-3} 
                         & Fixed          & Chosen based on validation set performance          \\ \hline
VH                       &   --                & --                 \\ \hline
RW                      &                 &                 \\ \hline
SP                       &--                   &--                  \\ \hline
GR                       &                   &                  \\ \hline
WL                       &  --                 &                  \\ \hline
NH                       &Count-sensitive neighborhood hash             &                  \\ \hline
NSPDK                    &--                   &                 \\ \hline
Lo-      &                &               \\ \hline
SVM-  &  &                  \\ \hline
ODD-STh                  &--                   &                 \\ \hline
PM                       &--                   &                  \\ \hline
GH                       &--                   &linear kernel/gaussian kernel                 \\ \hline
SM                       &                   &    --              \\ \hline
PK                      &                   &                 \\ \hline
ML                       &                &                  \\ \hline
CORE                     &  --                 & --                \\ \hline
\end{tabular}
\end{sc}
\caption{Values of the hyperparameters of the graphs kernels and frameworks included in our experimental comparison. Note that for some kernels, only a subset of the hyperparameters was optimized, while the rest of the hyperparameters were kept fixed.}
\label{tab:kernel_parametrization}
\end{table}

As mentioned above, to choose the hyperparameters of the kernels, we performed a validation experiment on a single  split of the training set.
Hence, given a kernel, for each combination of hyperparameter values, we generated a seperate kernel matrix.
The hyperparameter values that result into the classifier with the best performance on the validation set are the ones selected for the final model learning.
The values of the different hyperparameters of the kernels are shown in Table~\ref{tab:kernel_parametrization}.
It is interesting to mention that some kernels such as the vertex historgram kernel (VH) lack hyperparameters, while other kernels such as the multiscale Laplacian kernel (ML) contain a large number of hyperparameters.
Hence, for the vertex historgram (VH) kernel, we computed only a single kernel matrix in each experiment, while for the multiscale Laplacian (ML) kernel, we computed  different kernel matrices in each experiment.
Note also that instead of performing cross-validation to identify the best combination of hyperparameter values, we could have applied multiple kernel learning to the generated kernel matrices \cite{massimo2016hyper}.

For each experiment, we report the average accuracy over the  runs of the cross-validation procedure.
Furthermore, we report running times averaged over the  independent runs.
For each run, we compute running times as follows: for each fold of a -fold cross-validation experiment, the running time of the kernel corresponds to the running time for the computation of the kernel matrix that performed best on the validation experiment.


\subsection{Experimental Results}
We next present our experimental results.
As mentioned above, we evaluate the graph kernels by performing graph classification on unlabeled, node-labeled and node-attributed benchmark datasets.

\subsubsection{Node-Labeled Graphs}

\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.1}
\begin{tabular}{lcccc} \hline
\multirow{3}{*}{Kernels} & \multicolumn{4}{c}{DATASETS} \\ \cline{2-5}
& \multirow{2}{*}{MUTAG} & \multirow{2}{*}{ENZYMES} & \multirow{2}{*}{NCI1} & \multirow{2}{*}{PTC-MR} \\
& & & & \\ \hline
VH & 71.87 {\tiny ( 1.83)} & 16.87 {\tiny ( 1.56)} & 56.09 {\tiny ( 0.35)} & 58.09 {\tiny ( 0.62)}  \\
RW & 82.24 {\tiny ( 2.87)} & 12.90 {\tiny ( 1.42)} & \texttt{TIMEOUT} & 51.26 {\tiny ( 2.30)} \\
SP & 82.54 {\tiny ( 1.00)} & 40.13 {\tiny ( 1.34)} & 72.25 {\tiny ( 0.28)} & 59.26 {\tiny ( 2.34)} \\
WL-VH & 84.00 {\tiny ( 1.25)} & 53.15 {\tiny ( 1.22)} & 85.03 {\tiny ( 0.20)} & 63.28 {\tiny ( 1.34)} \\
WL-SP & 82.29 {\tiny ( 1.93)} & 28.23 {\tiny ( 1.00)} & 61.43 {\tiny ( 0.32)} & 55.51 {\tiny ( 1.68)} \\
WL-PM & 88.60 {\tiny ( 0.95)} & 57.72 {\tiny ( 0.84)} & 85.31 {\tiny ( 0.42)} & 64.52 {\tiny ( 1.36)} \\
NH & 87.74 {\tiny ( 1.17)} & 43.43 {\tiny ( 1.45)} & 74.81 {\tiny ( 0.37)} & 60.50 {\tiny ( 2.10)} \\
NSPDK & 82.46 {\tiny ( 1.55)} & 41.97 {\tiny ( 1.66)} & 74.36 {\tiny ( 0.31)} & 60.04 {\tiny ( 1.15)} \\
ODD-STh & 79.01 {\tiny ( 2.04)} & 31.87 {\tiny ( 1.35)} & 75.03 {\tiny ( 0.45)} & 59.08 {\tiny ( 1.85)} \\
PM & 84.72 {\tiny ( 1.67)} & 42.67 {\tiny ( 1.78)} & 73.11 {\tiny ( 0.49)} & 57.99 {\tiny ( 2.45)} \\
GH & 82.11 {\tiny ( 2.13)} & 36.47 {\tiny ( 2.13)} & 71.36 {\tiny ( 0.13)} & 55.64 {\tiny ( 2.03)} \\
SM & 84.04 {\tiny ( 1.55)} & 35.68 {\tiny ( 0.80)} & \texttt{TIMEOUT} & 57.91 {\tiny ( 1.73)} \\
PK & 77.23 {\tiny ( 1.22)} & 44.48 {\tiny ( 1.63)} & 82.12 {\tiny ( 0.22)} & 59.30 {\tiny ( 1.24)} \\
ML & 86.11 {\tiny ( 1.60)} & 53.08 {\tiny ( 1.53)} & 79.40 {\tiny ( 0.47)} & 59.95 {\tiny ( 1.71)} \\
CORE-WL & 85.90 {\tiny ( 1.44)} & 52.37 {\tiny ( 1.29)} & 85.12 {\tiny ( 0.21)} & 63.03 {\tiny ( 1.67)} \\
CORE-SP & 85.13 {\tiny ( 2.46)} & 41.55 {\tiny ( 1.66)} & 73.87 {\tiny ( 0.19)} & 58.21 {\tiny ( 1.87)} \\ \hline
\end{tabular}
\vspace{.2cm}
\\
\begin{tabular}{lccc m{1.7cm}} \hline
\multirow{3}{*}{Kernels} & \multicolumn{3}{c}{DATASETS} & \multicolumn{1}{c}{\multirow{2}{*}{Avg.}} \\ \cline{2-4}
& \multirow{2}{*}{D\&D} & \multirow{2}{*}{PROTEINS} & \multirow{2}{*}{AIDS} & \multicolumn{1}{c}{\multirow{2}{*}{Rank}} \\ 
& & & & \\ \hline
VH & 74.83 {\tiny ( 0.40)} & 70.93 {\tiny ( 0.28)} & 79.78 {\tiny ( 0.13)} & \multicolumn{1}{c}{13.7} \\
RW & \texttt{OUT-OF-MEM} & 69.31 {\tiny ( 0.29)} & 79.52 {\tiny ( 0.58)} & \multicolumn{1}{c}{15.0} \\
SP & 78.93 {\tiny ( 0.53)} & 75.92 {\tiny ( 0.35)} & 99.41 {\tiny ( 0.12)} & \multicolumn{1}{c}{6.7} \\
WL-VH & 78.88 {\tiny ( 0.46)} & 75.45 {\tiny ( 0.33)} & 98.51 {\tiny ( 0.05)} & \multicolumn{1}{c}{4.8} \\
WL-SP & 75.66 {\tiny ( 0.42)} & 71.88 {\tiny ( 0.22)} & 99.36 {\tiny ( 0.02)} & \multicolumn{1}{c}{11.8} \\
WL-PM & \texttt{OUT-OF-MEM} & 75.63 {\tiny ( 0.49)} & 99.37 {\tiny ( 0.04)} & \multicolumn{1}{c}{2.1} \\
NH & 76.02 {\tiny ( 0.94)} & 75.55 {\tiny ( 1.00)} & 99.54 {\tiny ( 0.02)} & \multicolumn{1}{c}{5.0} \\
NSPDK & 78.76 {\tiny ( 0.56)} & 73.17 {\tiny ( 0.76)} & 98.04 {\tiny ( 0.20)} & \multicolumn{1}{c}{8.0} \\
ODD-STh & 75.82 {\tiny ( 0.54)} & 70.49 {\tiny ( 0.64)} & 90.75 {\tiny ( 0.30)} & \multicolumn{1}{c}{11.4} \\
PM & 76.98 {\tiny ( 0.84)} & 71.90 {\tiny ( 0.79)} & 99.56 {\tiny ( 0.08)} & \multicolumn{1}{c}{8.2} \\
GH & \texttt{TIMEOUT} & 74.19 {\tiny ( 0.42)} & 99.57 {\tiny ( 0.02)} & \multicolumn{1}{c}{9.6} \\
SM & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & 91.96 {\tiny ( 0.18)} & \multicolumn{1}{c}{11.2} \\
PK & 78.43 {\tiny ( 0.55)} & 72.71 {\tiny ( 0.62)} & 96.51 {\tiny ( 0.38)} & \multicolumn{1}{c}{8.4} \\
ML & 78.28 {\tiny ( 0.99)} & 73.89 {\tiny ( 0.93)} & 98.48 {\tiny ( 0.12)} & \multicolumn{1}{c}{6.0} \\
CORE-WL & 78.91 {\tiny ( 0.50)} & 75.46 {\tiny ( 0.38)} & 98.70 {\tiny ( 0.09)} & \multicolumn{1}{c}{4.1} \\
CORE-SP & 79.33 {\tiny ( 0.65)} & 76.31 {\tiny ( 0.40)} & 99.47 {\tiny ( 0.05)} & \multicolumn{1}{c}{5.5} \\ \hline
\end{tabular}
\end{sc}
\caption{Average classification accuracy ( standard deviation) on the  classification datasets containing node-labeled graphs. The ``Avg. Rank'' column illustrates the average rank of each kernel. The lower the average rank, the better the overall performance of the kernel.}
\label{tab:results_labeled}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.1}
\begin{tabular}{lrrrr} \hline
\multirow{3}{*}{Kernels} & \multicolumn{4}{c}{DATASETS} \\ \cline{2-5}
& \multicolumn{1}{c}{\multirow{2}{*}{MUTAG}} & \multicolumn{1}{c}{\multirow{2}{*}{ENZYMES}} & \multicolumn{1}{c}{\multirow{2}{*}{NCI1}} & \multicolumn{1}{c}{\multirow{2}{*}{PTC-MR}} \\
& & & & \\ \hline
VH & 0.01s & 0.04s & 0.84s & 0.02s \\
RW & 1m 46.86s & 4h 24m 16.26s & \texttt{TIMEOUT} & 6m 41.20s \\
SP & 0.92s & 11.03s & 1m 9.69s & 1.52s \\
WL-VH & 0.21s & 3.81s & 7m 5.33s & 0.55s \\
WL-SP & 7.02s & 1m 27.07s & 15m 29.50s & 12.55s \\
WL-PM & 3m 42.07s & 1h 5m 37.26s & 13h 31m 34.36s & 11m 8.16s \\
NH & 0.40s & 11.17s & 7m 4.54s & 1.31s \\
NSPDK & 4.05s & 27.02s & 6m 9.81s & 7.66s \\
ODD-STh & 1.54s & 50.05s & 46m 2.13s & 4.03s \\
PM & 2.59s & 31.38s & 37m 37.50s & 11.35s \\
GH & 24.70s & 15m 38.33s & 3h 45m 8.31s & 1m 33.90s \\
SM & 1m 57.25s & 3h 25m 43.59s & \texttt{TIMEOUT} & 4m 19.80s \\
PK & 0.48s & 12.05s & 10m 27.83s & 1.81s \\
ML & 10m 3.15s & 56m 43.76s & 5h 30m 56.29s & 19m 22.43s \\
CORE-WL & 0.55s & 12.52s & 14m 30.56s & 17m 2.27s \\
CORE-SP & 2.69s & 48.02s & 3m 16.54s & 3.97s \\ \hline
\end{tabular}
\vspace{.2cm}
\\
\begin{tabular}{lrrrm{1.48cm}} \hline
\multirow{3}{*}{Kernels} & \multicolumn{3}{c}{DATASETS} & \multicolumn{1}{c}{\multirow{2}{*}{Avg.}} \\ \cline{2-4}
& \multicolumn{1}{c}{\multirow{2}{*}{D\&D}} & \multicolumn{1}{c}{\multirow{2}{*}{PROTEINS}} & \multicolumn{1}{c}{\multirow{2}{*}{AIDS}} & \multicolumn{1}{c}{\multirow{2}{*}{Rank}} \\ 
& & & & \\ \hline
VH & 0.24s & 0.10s & 0.25s & \multicolumn{1}{c}{1.0} \\
RW & \texttt{OUT-OF-MEM} & 51m 10.11s & 1h 51m 56.47s & \multicolumn{1}{c}{13.6} \\
SP & 55m 58.79s & 1m 18.91s & 13.93s & \multicolumn{1}{c}{4.4} \\
WL-VH & 5m 52.96s & 32.48s & 40.49s & \multicolumn{1}{c}{2.8} \\
WL-SP & 7h 27m 21.90s & 8m 3.68s & 1m 33.46s & \multicolumn{1}{c}{10.1} \\
WL-PM & \texttt{OUT-OF-MEM} & 5h 37m 10.33s & 5h 55m 20.37s & \multicolumn{1}{c}{14.6} \\
NH & 6m 17.21s & 41.81s & 33.30s & \multicolumn{1}{c}{3.5} \\
NSPDK & 4h 36m 28.97s & 9m 9.80s & 1m 12.31s & \multicolumn{1}{c}{8.1} \\
ODD-STh & 27m 59.18s & 4m 7.81s & 2m 5.32s & \multicolumn{1}{c}{8.7} \\
PM & 5m 48.51s & 1m 26.82s & 2m 48.04s & \multicolumn{1}{c}{8.0} \\
GH & \texttt{TIMEOUT} & 3h 43m 1.54s & 38m 51.78s & \multicolumn{1}{c}{12.1} \\
SM & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & 4h 26m 46.71s & \multicolumn{1}{c}{14.0} \\
PK & 9m 34.30s & 51.20s & 1m 43.62s & \multicolumn{1}{c}{5.5} \\
ML & 3h 40m 30.72s & 2h 20m 39.57s & 1h 11m 58.23s & \multicolumn{1}{c}{13.2} \\
CORE-WL & 17m 2.27s & 1m 16.74s & 54.79s & \multicolumn{1}{c}{7.2} \\
CORE-SP & 5h 2m 39.71s & 3m 31.97s & 40.11s & \multicolumn{1}{c}{7.2} \\ \hline
\end{tabular}
\end{sc}
\caption{Average CPU running time for kernel matrix computation on the  classification datasets containing node-labeled graphs. The ``Avg. Rank'' column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.}
\label{tab:runtimes_labeled}
\end{table}

Tables~\ref{tab:results_labeled} and~\ref{tab:runtimes_labeled} illustrate average prediction accuracies and average running times of the compared kernels on the datasets that contain node-labeled graphs.
We observe that the kernels that employ some neighborhood aggregation mechanism (\eg the Weisfeiler-Lehman framework) yield very good performance.
Specifically, the WL-PM kernel outperforms all the other kernels on  out of the  datasets (MUTAG, ENZYMES, NCI1, and PTC-MR).
Moreover, the WL-VH, CORE-WL and NH kernels also achive high accuracies on most datasets.
Surprisingly, WL-SP, although equipped with a neighborhood aggregation scheme, performs much worse than the other kernels which employ the same framework and also much worse than the SP and CORE-SP kernels.
The core framework leads to performance improvements on most datasets.
For instance, in the case of the SP kernel, it leads to better accuracies on all but one dataset.
It is worth mentioning that CORE-SP provides the highest accuracies on two datasets (D\&D and PROTEINS).
As regards the kernels for graphs with continuous attributes (GH, SM, PK, and ML), most of them failed to produce results comparable to the best-performing kernels.
The only exception is the ML kernel which yielded good results on most datasets.
Moreover, the GH kernel reached the highest accuracy on the AIDS dataset.
Finally, VH and RW achieved very low accuracy levels.

On most datasets, the variability in the performance of the different kernels is low.
The ENZYMES dataset is an exception to that, since the average accuracy of the best-performing kernel is equal to , while that of the worst-performing kernel is equal to .
Furthermore, the AIDS dataset is almost perfectly classified by several kernels, and this raises some concerns about the value of this dataset for graph kernel comparison.

In terms of running time, as expected, VH is the fastest kernel on all datasets.
This kernel computes the dot product on vertex label histograms, hence, its complexity is linear to the number of vertices.
The running time of WL-VH, NH, SP and PK is also low compared to the other kernels on most datasets.
Hence, the effectiveness of WL-PM comes at a price, as computing the kernel requires a large amount of time.
Note also that while the worst-case complexity of SP is very high, by employing an explicit computation scheme, the running time of the kernel in real scenarios is very attractive.
We also observe that the ML, RW, SM and WL-PM kernels are very expensive in terms of runtime.
Specifically, the SM kernel failed to compute the kernel matrix on NCI1 within one day, while it exceeded the maximum available memory on two other datasets (D\&D and PROTEINS).
It should be mentioned that the size of the graphs (\ie number of nodes) and the size of the dataset (\ie number of graphs) have a different impact on the running time of the kernels.
For instance, the average running time of the PM kernel is relatively high on datasets that contain small graphs.
However, this kernel is much more competitive on datasets which contain large graphs such as the D\&D dataset on which it was the third fastest kernel.

Overall, when dealing with tasks that involve node-labeled graphs, we suggest to use a kernel that utilizes some neighborhood aggregation mechanism.
For instance, the WL-VH and NH kernels achieve high accuracies and are very efficient even when the size of the graphs and/or the dataset is large.


\subsubsection{Unabeled Graphs}

\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.1}
\resizebox{\textwidth}{!} {
\begin{tabular}{lccccccc} \hline
\multirow{3}{*}{Kernels} & \multicolumn{6}{c}{DATASETS} & \multirow{2}{*}{Avg.} \\ \cline{2-7}
& IMDB & IMDB & REDDIT & REDDIT & REDDIT & \multirow{2}{*}{COLLAB} & \multirow{2}{*}{Rank}\\
& BINARY & MULTI & BINARY & MULTI-5K & MULTI-12K & & \\ \hline
VH & 46.54 {\tiny ( 0.80)} & 29.59 {\tiny ( 0.40)} & 47.32 {\tiny ( 0.66)} & 17.92 {\tiny ( 0.42)} & 21.73 {\tiny ( 0.00)} & 52.00 {\tiny ( 0.00)} & 12.4\\ 
RW & 63.87 {\tiny ( 1.06)} & 45.75 {\tiny ( 1.03)} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & 68.00 {\tiny ( 0.07)} & 7.6 \\
SP & 55.18 {\tiny ( 1.23)} & 39.37 {\tiny ( 0.84)} & 81.67 {\tiny ( 0.23)} & 47.90 {\tiny ( 0.13)} & \texttt{TIMEOUT} & 58.80 {\tiny ( 0.08)} & 8.3 \\  
GR & 65.19 {\tiny ( 0.97)} & 39.82 {\tiny ( 0.89)} & 76.80 {\tiny ( 0.27)} & 34.06 {\tiny ( 0.38)} & 23.08 {\tiny ( 0.11)} & 70.63 {\tiny ( 0.25)} & 7.0 \\ 
WL-VH & 72.47 {\tiny ( 0.50)} & 50.76 {\tiny ( 0.30)} & 67.96 {\tiny ( 1.01)} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & 78.12 {\tiny ( 0.17)} & 4.2 \\ 
WL-SP & 55.87 {\tiny ( 1.19)} & 39.63 {\tiny ( 0.68)} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 58.80 {\tiny ( 0.06)} & 10.8 \\
NH & 73.34 {\tiny ( 0.98)} & 50.68 {\tiny ( 0.50)} & 81.65 {\tiny ( 0.28)} & 49.36 {\tiny ( 0.18)} & 39.62 {\tiny ( 0.19)} & 79.99 {\tiny ( 0.39)} & 2.3 \\ 
NSPDK & 68.81 {\tiny ( 0.71)} & 45.10 {\tiny ( 0.63)} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 7.5 \\ 
Lo- & 49.21 {\tiny ( 1.33)} & 39.33 {\tiny ( 0.95)} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 15.0 \\ 
SVM- & 51.35 {\tiny ( 1.54)} & 38.40 {\tiny ( 0.60)} & 74.54 {\tiny ( 0.27)} & 29.65 {\tiny ( 0.53)} & 23.04 {\tiny ( 0.18)} & 55.72 {\tiny ( 0.31)} & 10.1 \\ 
ODD-STh & 64.70 {\tiny ( 0.73)} & 46.80 {\tiny ( 0.51)} & 50.61 {\tiny ( 1.06)} & 42.99 {\tiny ( 0.09)} & 29.83 {\tiny ( 0.08)} & 52.00 {\tiny ( 0.00)} & 7.5 \\ 
PM & 66.67 {\tiny ( 1.45)} & 45.25 {\tiny ( 0.79)} & 86.77 {\tiny ( 0.42)} & 48.22 {\tiny ( 0.29)} & 41.15 {\tiny ( 0.17)} & 74.57 {\tiny ( 0.34)} & 4.1 \\ 
GH & 57.69 {\tiny ( 1.31)} & 40.04 {\tiny ( 0.91)} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 60.21 {\tiny ( 0.10)} & 9.3 \\ 
SM & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & \texttt{TIMEOUT} & -- \\ 
PK & 51.15 {\tiny ( 1.67)} & 33.15 {\tiny ( 1.08)} & 63.41 {\tiny ( 0.77)} & 34.32 {\tiny ( 0.61)} & 24.07 {\tiny ( 0.11)} & 58.67 {\tiny ( 0.15)} & 10.1 \\ 
ML & 70.94 {\tiny ( 0.93)} & 47.92 {\tiny ( 0.87)} & 89.44 {\tiny ( 0.30)} & 35.01 {\tiny ( 0.65)} & \texttt{OUT-OF-MEM} & 75.29 {\tiny ( 0.49)} & 3.8 \\ 
CORE-WL & 73.31 {\tiny ( 1.06)} & 50.79 {\tiny ( 0.54)} & 72.82 {\tiny ( 1.05)} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & 3.8 \\ 
CORE-SP & 69.37 {\tiny ( 0.68)} & 50.79 {\tiny ( 0.57)} & 90.76 {\tiny ( 0.14)} & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & \texttt{TIMEOUT} & 2.5 \\ 
\hline
\end{tabular}
}
\end{sc}
\caption{Average classification accuracy ( standard deviation) on the  classification datasets containing unlabeled graphs. The ``Avg. Rank'' column illustrates the average rank of each kernel. The lower the average rank, the better the overall performance of the kernel.}
\label{tab:results_unlabeled}
\end{table}


\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.1}
\resizebox{\textwidth}{!} {
\begin{tabular}{lrrrrrrc} \hline
\multirow{3}{*}{Kernels} & \multicolumn{6}{c}{DATASETS} & \multirow{2}{*}{Avg.} \\ \cline{2-7}
& IMDB & IMDB & REDDIT & REDDIT & REDDIT & \multirow{2}{*}{COLLAB} & \multirow{2}{*}{Rank}\\
& BINARY & MULTI & BINARY & MULTI-5K & MULTI-12K & & \\ \hline
VH & 0.07s & 0.15s & 0.67s & 2.20s & 6.37s & 1.12s & 1.0\\ 
RW & 7m 20.94s & 13m 40.75s & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 13h 38m 11.49s & 13.6 \\ 
SP & 11.51s & 7.92s & 4h 48m 11.19s & 12h 40m 19.50s & \texttt{TIMEOUT} & 1h 9m 5.50s & 7.0\\ 
GR & 22m 45.89s & 21m 44.30s & 44m 45.42s & 44m 6.52s & 53m 14.22s & 2h 58m 1.14s & 9.5 \\ 
WL-VH & 4.49s & 6.16s & 16m 2.65s & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & 38m 42.24s & 4.2\\ 
WL-SP & 1m 32.66s & 1m 40.46s & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 10h 27m 41.97s & 10.3 \\
NH & 21.83s & 26.07s & 23m 3.42s & 2h 44m 44.66s & 9h 11m 23.67s & 35m 49.96s & 6.3 \\ 
NSPDK & 4m 18.12s & 2m 49.45s & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 12.5 \\ 
Lo- & 5h 19m 27.17s & 6h 33m 6.55s & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 17.0 \\ 
SVM- & 39.40s & 1m 0.57s & 19m 24.73s & 23m 14.31s & 52m 10.36s & 5m 57.31s & 5.3 \\ 
ODD-STh & 4.47s & 4.85s & 1m 53.50s & 4m 48.92s & 8m 20.66s & 2h 1m 9.55s & 3.1 \\ 
PM & 1m 28.02s & 2m 13.01s & 10m 9.24s & 51m 45.10s & 3h 50m 38.60s & 36m 26.14s & 7.0\\ 
GH & 2m 11.15s & 2m 3.71s & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 5h 51m 32.27s & 10.3 \\ 
SM & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & \texttt{TIMEOUT} & -- \\ 
PK & 7.41s & 14.26s & 1m 23.42s & 5m 49.01s & 20m 41.73s & 4m 34.26s & 3.1 \\ 
ML & 1h 22m 6.04s & 1h 41m 13.74s & 8h 21m 18.76s & 47m 51.91s & \texttt{OUT-OF-MEM} & 9h 24m 15.22s & 10.0 \\ 
CORE-WL & 36.74s & 1m 1.82s & 45m 1.09s & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & \texttt{OUT-OF-MEM} & 8.0 \\ 
CORE-SP & 3m 58.29s & 4m 29.55s & 10h 37m 3.94s & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & \texttt{TIMEOUT} & 12.3 \\ 
\hline
\end{tabular}
}
\end{sc}
\caption{Average CPU running time for kernel matrix computation on the  classification datasets containing unlabeled graphs. The ``Avg. Rank'' column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.}
\label{tab:runtimes_unlabeled}
\end{table}

Tables~\ref{tab:results_unlabeled} and~\ref{tab:runtimes_unlabeled} illustrate average prediction accuracies and average running times of the compared kernels on the  datasets that contain unlabeled graphs.
We observe that NH is the best-performing method.
It outperforms all the other kernels on  out of the  datasets (IMDB-BINARY, REDDIT-MULTI-5K and COLLAB).
Furthermore, on the remaining datasets, it reached the second, the fourth and the fifth best accuracy levels among all methods considered.
The CORE-SP, ML, CORE-WL, PM and WL-VH kernels also yielded high performance on most datasets.
We should note that the core framework improved significantly the performance of the SP kernel on several datasets.
On the other hand, the Weisfeiler-Lehman framework did not provide such a large increase in the performance of the SP kernel.
Lo- was the worst-performing kernel, followed by VH, WL-SP, SVM- and PK, in that order.
It is interesting to mention that most of the kernels that reached the highest accuracies can also handle graphs with dicrete node labels.
For those kernels, the label of each vertex was set equal to its degree.
The kernels that can only handle unlabeled graphs (GR, Lo-, SVM-) failed to achieve accuracies competitive to the best-performing kernels.
As regards the kernels that can handle graphs with continuous attributes (GH, SM, PK, and ML), as mentioned above, ML yielded very good results.
GH and PK achieved low accuracy levels, while SM failed to generate even a single kernel matrix due to running time or memory issues.

On most datasets, the variability in the performance of the different kernels is low.
The kernels achieve higher performance on binary classification tasks (IMDB-BINARY and REDDIT-BINARY) than on multi-class classification tasks.
For instance, on IMDB-MULTI, REDDIT-MULTI-5K and REDDIT-MULTI-12K, the highest average accuracies obtained by the considered kernels are ,  and , respectively.
Hence, it is clear that these three datasets are very challenging even for state-of-the-art methods.

In terms of running time, similar to the labeled case, VH is again the fastest kernel on all datasets.
The running time of PK, ODD-STh, and WL-VH is also low compared to the other kernels on most datasets.
The SVM-, NH, PM, SP and CORE-WL kernels were also competitive in terms of running time.
Besides achieving low accuracy levels, the Lo- kernel is also very computationally expensive.
The RW, NSPDK, CORE-SP, WL-SP and GH are also very expensive in terms of running time.
The above  kernels did not manage to calculate any kernel matrix on REDDIT-MULTI-5K and REDDIT-MULTI-12K within one day.
The SM kernel failed to compute the kernel matrix on IMDB-BINARY, IMDB-MULTI and COLLAB within one day, while it exceeded the maximum available memory on the remaining three datasets.
The WL-VH and CORE-WL kernels also exceeded the maximum available memory on REDDIT-MULTI-5K and REDDIT-MULTI-12K.
It should be mentioned that these two datasets contain several thousands of graphs, while the size of the graphs is also large (\ie several hundreds of vertices on average).

When dealing with tasks that involve unlabeled graphs, we suggest to assign discrete node labels to the vertices of the graphs (\eg set the label of each vertex equal to its degree), and then to again employ kernels that utilize some neighborhood aggregation mechanism.
For instance, the NH and WL-VH kernels achieve high accuracies and their computational complexity is low, although WL-VH is not always efficient in terms of memory consumption.

\subsubsection{Node-Attributed Graphs}

\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.1}
\begin{tabular}{lcccccc} \hline
\multirow{3}{*}{Kernels} & \multicolumn{5}{c}{DATASETS} & \multirow{2}{*}{Avg.} \\ \cline{2-6}
& \multirow{2}{*}{ENZYMES} & \multirow{2}{*}{PROTEINS\_full} & \multirow{2}{*}{SYNTHETICnew} & \multirow{2}{*}{Synthie} & \multirow{2}{*}{BZR} & \multirow{2}{*}{Rank} \\ 
& & & & & \\ \hline
SP & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & -- \\
SM & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 80.52 {\tiny ( 0.43)} & 3.0 \\
GH & 66.25 {\tiny ( 1.24)} & 72.49 {\tiny ( 0.34)} & 76.43 {\tiny ( 1.97)} & 71.75 {\tiny ( 1.65)} & 82.58 {\tiny ( 1.05)} & 1.0 \\
PK & 15.42 {\tiny ( 1.00)} & 59.56 {\tiny ( 0.01)} & 47.90 {\tiny ( 3.26)} & 48.90 {\tiny ( 2.05)} & 78.76 {\tiny ( 0.02)} & 3.0 \\
ML & 65.55 {\tiny ( 0.93)} & 70.55 {\tiny ( 0.99)} & 47.90 {\tiny ( 2.13)} & 69.42 {\tiny ( 1.98)} & 82.33 {\tiny ( 1.29)} & 2.0 \\ \hline
\end{tabular}
\end{sc}
\caption{Average classification accuracy ( standard deviation) on the  classification datasets containing node-attributed graphs. The ``Avg. Rank'' column illustrates the average rank of each kernel. The lower the average rank, the better the overall performance of the kernel.}
\label{tab:results_attributed}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\begin{sc}
\def\arraystretch{1.1}
\begin{tabular}{lrrrrrc} \hline
\multirow{3}{*}{Kernels} & \multicolumn{5}{c}{DATASETS} & \multirow{2}{*}{Avg.} \\ \cline{2-6}
& \multicolumn{1}{c}{\multirow{2}{*}{ENZYMES}} & \multicolumn{1}{c}{\multirow{2}{*}{PROTEINS\_full}} & \multicolumn{1}{c}{\multirow{2}{*}{SYNTHETICnew}} & \multicolumn{1}{c}{\multirow{2}{*}{Synthie}} & \multicolumn{1}{c}{\multirow{2}{*}{BZR}} & \multirow{2}{*}{Rank} \\ 
& & & & & \\ \hline
SP & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & -- \\
SM & \texttt{TIMEOUT} & \texttt{OUT-OF-MEM} & \texttt{TIMEOUT} & \texttt{TIMEOUT} & 8h 2m 3.79s & 4.0 \\
GH & 16m 36.12s & 5h 16m 46.48s & 13m 54.36s & 24m 20.00s & 4m 24.79s & 2.6 \\
PK & 15.85s & 1m 43.58s & 13.44s & 34.68s & 10.40s & 1.0 \\
ML & 26.05s & 4h 29m 35.69s & 2h 54m 31.22s & 15m 11.29s & 49m 33.60s & 2.4 \\ \hline
\end{tabular}
\end{sc}
\caption{Average CPU running time for kernel matrix computation on the  classification datasets containing node-attributed graphs. The ``Avg. Rank'' column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.}
\label{tab:runtimes_attributed}
\end{table}

As mentioned above, the majority of graph kernels can handle graphs that are either unlabeled or contain discrete node labels.
On the other hand, the number of graph kernels that can handle graphs that contain continuous vertex attributes is limited.
Moreover, most of these kernels do not scale even to relatively small datasets.
Tables~\ref{tab:results_attributed} and~\ref{tab:runtimes_attributed} illustrate average prediction accuracies and average runtimes of  graph kernels on datasets that contain node-attributed graphs.
Note that although the graphs of some of these datasets contain discrete node labels, we did not take these discrete labels into account since our main aim was to evaluate the ability of the kernels to properly handle continuous node attributes.
GH is the best-performing kernel since it outperforms all the other kernels on all datasets.
Interestingly, on PROTEINS\_full, it outperforms the other kernels by wide margins.
GH is followed by ML and PK in terms of performance in that order.
ML yields average accuracies comparable to those of GH on most datasets, while PK is less competitive.
One of the most striking findings of this set of experiments is that the SP kernel did not manage to compute the kernel matrix even on a single dataset within one day, while the SM kernel finished its computations within one day only on the BZR dataset on which it was outperformed by GH and ML.

In terms of running time, PK is the most efficient kernel since it handled all datasets in less than two minutes.
GH and ML are much slower than PK on all datasets.
For instance, the average computation time of ML and GH was greater than  hours and  hours on PROTEINS\_full, respectively.
The SP and SM kernels, as already discussed, are very expensive in terms of running time, and hence, their usefulness in real-world problems is limited.

To sum up, it is clear that the running time of most kernels for node-attributed graphs is prohibitive, especially considering the relatively small size of the datasets.
Although the running time of PK is attractive, it achieved low accuracies on almost all datasets.
An open challenge in the field of graph kernels is thus to develop scalable kernels for graphs with continuous vertex attributes.



\section{Conclusion}\label{sec:conclusion}
Recent years have witnessed a tremendous increase in the availability of graph-structured data.
Graphs arise in many different contexts where it is necessary to represent relationships between entities.
Specifically, graphs are the commonly employed structure for representing data in various domains including bioinformatics, chemoinformatics, social networks and information networks.
The abundance of graph-structured data and the need to perform machine learning tasks on this kind of data led to the development of several sophisticated approaches such as graph kernels.
In this survey, we provided a detailed overview of graph kernels.
Furthermore, we empirically evaluated the effectiveness of several graph kernels, and measured their running time.
We hope that this survey will provide a better understanding of the current progress on graph kernels and graph classification, and offer some guidelines on how to apply these approaches in order to solve real-world problems.

Although graph kernels have achieved remarkable results in many tasks, there are still some challenges to be addressed, while there is also still some room for improvement.
For example, the majority of the kernels that can handle graphs with continuous attributes are either very expensive in terms of computational complexity or fail to produce competitive results.
Hence, we believe that an important direction of research is the development of scalable graph kernels for graphs annotated with continuous attributes which will also provide improvements over the state-of-the-art approaches.
Another useful direction of research is to capitalize on the framework for designing valid assignment kernels presented above, and to develop new kernels which compute an optimal assignment between substructures extracted from graphs.
In general, the complexity of the assignment kernels is more attractive than that of kernels that belong to the -convolution framework, and hence, it is our belief that this framework can pave the way for the development of more efficient graph kernels.


\vskip 0.2in
\bibliography{sample}
\bibliographystyle{theapa}

\end{document}