

\documentclass[11pt]{article}
 \usepackage{a4wide}
\usepackage{amssymb}
\usepackage{stmaryrd}


\usepackage[Postscript=dvips]{diagrams}
\usepackage{QED}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\even}[1]{\mbox{}}
\newcommand{\E}{{\cal E}}
\newcommand{\G}{{\cal G}}
\newcommand{\M}{{\cal M}}
\newcommand{\D}{{\cal D}}
\newcommand{\Siep}{{\Sigma}}
\newcommand{\Ip}[1]{\lesssim_{#1}}
\newcommand{\Mat}[4]{\left( \begin{array}{cc}
#1 & #2 \\
#3 & #4
\end{array} \right)}
\newcommand{\odd}[1]{\mbox{}}
\newcommand{\Dom}[1]{{\cal D} ( #1 )}
\newcommand{\sqpreord}{\precapprox}
\newcommand{\rat}{\triangledown}
\newcommand{\ident}[1]{{\tt id}_{#1}}
\newcommand{\Fst}{{\tt fst}}
\newcommand{\Snd}{{\tt snd}}
\newcommand{\Mod}{{\cal M}}
\newcommand{\Sierp}{{\Sigma}}
\newcommand{\preord}{\lesssim}
\newcommand{\Bool}{{\bf Bool}}
\newcommand{\Nat}{{\bf Nat}}
\newcommand{\sD}{{\bf sD}}
\newcommand{\plays}[1]{M^{\circledast}_{#1}}
\newcommand{\Dim}[1]{\begin{proof} #1
\end{proof}}
\newcommand{\ITEM}[1]{\begin{itemize} #1 \end{itemize}}
\newcommand{\SET}[1]{\{ #1 \}}
\newcommand{\Fr}{\rightarrow}
\newcommand{\Sqleq}{\sqsubseteq}
\newcommand{\Rest}{{\upharpoonright}}
\newcommand{\Eqdef}{\succeq}
\newcommand{\La}{\lambda}
\newcommand{\Pfr}{\rightharpoonup}
\newcommand{\Deq}{\approx}
\newcommand{\Over}[1]{\overline{ #1 }}
\newcommand{\Ort}{\bot}
\newcommand{\Sum}{\sum}
\newcommand{\Hat}[1]{\widehat{ #1}}
\newcommand{\Incl}{\subseteq}
\newcommand{\B}[1]{{\tt #1 }}
\newcommand{\THEN}{\; \Longrightarrow \;}
\newcommand{\IFF}{\;\mbox{ iff }\;}
\newcommand{\ofcourse}{\mbox{}}
\newcommand{\whynot}{\mbox{}}
\newcommand{\with}{\mbox{}}
\newcommand{\aor}{\mbox{}}
\newcommand{\inl}{\mbox{\bf inl }}
\newcommand{\inr}{\mbox{\bf inr }}
\newcommand{\tensor}{\mbox{}}
\newcommand{\linimpl}{\mbox{}}
\newcommand{\Games}{\mbox{}}
\newcommand{\KG}{K_{\ofcourse }({\cal G})}
\newcommand{\tunit}{I}

\newcommand{\pcfc}{\mbox{}}

\newcommand{\finevalT}[2]{\mbox{{\bf FET}()}}

\newcommand{\twotrans}[2]{\mbox{#1#2}}

\newcommand{\lpo}{\mbox{}}

\newcommand{\evalT}[2]{\mbox{{\bf ET}()}}

\newcommand{\varempty}{\varnothing}
\newcommand{\lang}{\langle}
\newcommand{\rang}{\rangle}
\newcommand{\Sx}{\sqsubseteq}
\newlength{\sqpreordheight}
\newlength{\sqpreorddepth}
\settoheight{\sqpreordheight}{\raisebox{-1.02ex}{}}
\settodepth{\sqpreorddepth}{}
\newcommand{\Subeq}{\mathbin{\raisebox{-1.02ex}[\sqpreordheight][\sqpreorddepth]{}}}



\begin{document}


















\newcommand{\affil}[1]{\footnote{#1}}

\title{Full Abstraction for PCF\footnote{This
research was supported by grants from UK SERC and ESPRIT Basic
Research Action 6811 ``CLICS II''. Radha Jagadeesan was supported
in part by grants from NSF and  ONR. Pasquale Malacaria was
supported in part by the HCM fellowship n. ERBCHBICT940947. }}








\author{Samson.Abramsky\footnote{University of Oxford, email: samson.abramsky@cs.oxac.uk}
\, and Radha
Jagadeesan\footnote{DePaul University
Chicago, email: RJagadeesan@cs.depaul.edu} 
\, and Pasquale
Malacaria\footnote{Queen Mary University of London, email: p.malacaria@qmul.ac.uk}
}















\maketitle

\abstract{An intensional model for the programming language PCF is
described, in which the types of PCF are interpreted by games, and
the terms by certain ``history-free'' strategies. This model is
shown to capture definability in PCF. More precisely, every
compact strategy in the model is definable in a certain simple
extension of PCF. We then introduce an intrinsic preorder on
strategies, and show that it satisfies some striking properties,
such that the intrinsic preorder on function types coincides with
the pointwise preorder. We then obtain an order-extensional fully
abstract model of PCF by quotienting the intensional model by the
intrinsic preorder. This is the first syntax-independent
description of the fully abstract model for PCF. (Hyland and Ong
have obtained very similar results by a somewhat different route,
independently and at the same time).

We then consider the effective version of our model, and prove a
Universality Theorem: every element of the effective extensional
model is definable in PCF. Equivalently, every recursive strategy
is definable up to observational equivalence. }

\paragraph{Keywords}
Game semantics, full abstraction, sequentiality, PCF,
  functional computation, programming language semantics, Linear Logic.












\section{Introduction}


The Full Abstraction Problem for  PCF \cite{PlotkinGD:lcfcpl,MilnerR:fulamt,BerryG:fulasl,CurienPL:seqful} is one
of the longest-standing problems in the semantics of programming languages.
There is quite widespread agreement that it is one of the most difficult;
there is much less agreement as to what exactly the problem is, or more
particularly as to the precise criteria for a solution.
The usual formulation is that one wants a ``semantic characterization''
of the fully abstract model (by which we mean the inequationally
fully abstract order-extensional model, which Milner proved to be uniquely
specified up to isomorphism by these properties \cite{MilnerR:fulamt}).
The problem is to understand what should be meant by a 
``semantic characterization''.

Our view is that the essential content of the problem, what makes it
important, is that it calls for a semantic characterization of
{\em sequential, functional computation at higher types}.
The phrase ``sequential functional computation'' deserves careful
consideration. On the one hand, sequentiality refers to a computational
process extended over time, not a mere function; on the other hand,
we want to capture just those sequential computations in which the
different parts or ``modules'' interact with each other in a purely
functional fashion.

There have, to our knowledge, been just four models of PCF put forward
as embodying some semantic analysis. Three are domain-theoretic:
the ``standard model'' based on Scott-continuous functions \cite{PlotkinGD:lcfcpl};
Berry's bidomains model based on stable functions \cite{BerryG:modcom};
and the Bucciarelli-Ehrhard model based on strongly stable functions \cite{EhrhardT:extemb}.
The fourth is the Berry-Curien model based on sequential algorithms
\cite{BerryG:seqacd}.\footnote{Cartwright and Felleisen's model without error
values turns out to be equivalent to the sequential algorithms model
\cite{FelleisenM:obsseq,CurienPL:obssac}.
The main result in \cite{FelleisenM:obsseq,CurienPL:obssac} is that
the sequential algorithms model with errors is fully abstract for
SPCF, an extension of PCF with a {\tt catch} construct and errors. This
is a fine result, but SPCF has a rather different flavour to PCF, and
arguably is no longer purely functional in character.}
Of these, we can say that the standard model gives a good account
of functional computation at higher types, but fails to capture
sequentiality, while the sequential algorithms model gives a good
analysis of sequential computation, but fails to capture functional
behaviour. In each case, the failure can calibrated in terms of
{\em definability}: the standard model includes parallel functions;
the sequential algorithms model includes algorithms which compute
``functionals'' which are sensitive to non-functional aspects of the
behaviour of their arguments. The bidomains model also contains
non-sequential functions; while the strongly stable model, in the
light of a recent result by Ehrhard \cite{EhrhardT:prosas}, can be seen as the
``extensional collapse'' of the sequential algorithms model.
In short, all these models are unsatisfactory because they contain ``junk''.
On the other side of the coin, we have Milner's result that an
order-extensional
model is fully-abstract iff all its compact elements are definable.

\subsection*{Intensional Full Abstraction}
This suggests that the key step towards solving the Full Abstraction problem
for PCF is to capture PCF definability.
This motivates the following definition. A model 
(not necessarily extensional) is {\em intensionally fully abstract}
if it is algebraic, and all its compact elements are definable in PCF.
In support of this terminology, we have the fact that the fully abstract
model can be obtained from an intensionally fully abstract model 
in the following canonical fashion. Firstly, define a logical relation
on  induced by the ordering on the ground types
(which are assumed {\em standard}, i.e. isomorphic to the usual
flat domains of natural numbers and booleans). Because of the
definability properties of , this relation is a preorder at
all types.
In particular, it is {\em reflexive} at all types. This says that
all elements of the model have extensional (functional) behaviour---there
is no junk.

We can now apply Theorem 7.2.2 of \cite{StoughtonA:fulamp} to conclude
that  
can be collapsed by a continuous homomorphism to the fully abstract model.
In short, the fully abstract model is the extensional collapse of
any intensionally fully abstract model. Moreover, note that the collapsing
map is  a homomorphism, and in particular preserves application. This
contrasts sharply with ``collapses'' of the standard model to obtain the
fully abstract model, as in the work of Mulmuley \cite{MulmuleyK:fulase} and
Stoughton and Jung \cite{JungA:stufam}, which are only homomorphic on the
``inductively reachable'' subalgebra.

Thus we propose that a reasonable factorization of the full abstraction
problem is to look for  a semantic presentation of an intensionally
fully abstract model, which embodies a semantic analysis of sequential
functional computation. 
The construction of such a model is our first main result; it is described
in Sections~2 and~3.

We have explained how the (order-extensional, inequationally) fully abstract
model can be obtained from any intensionally fully abstract model
by means of a general construction, described in \cite{StoughtonA:fulamp}. 
However, this description of the fully
abstract model leaves something to be desired. Firstly, just because the
construction in \cite{StoughtonA:fulamp} is very general, it is
unlikely to yield any useful information
about the fully abstract model. Secondly, it is not entirely syntax-free:
it refers to the {\sl type structure} of PCF.

What would the ideal form of description of the fully abstract model be?
We suggest that it should comprise the specification of a cartesian closed
category whose objects are certain cpo's, given together with certain
additional ``intensional'' structure, to be used to characterize sequentiality;
and whose morphisms are continuous functions between these cpo's---not
{\sl all} continuous functions, of course, but only the sequential ones,
as determined by the intensional structure. The interpretation of PCF
generated from this category should then be the fully abstract model.
Most of the attempts at solving the full abstraction problem of which we
are aware, including Berry's bidomains, Curien's bicds, and Bucciarelli
and Erhard's strongly stable functions, clearly fall within this general
scheme. (Thus for example the intensional structure in bidomains is the
stable ordering; for domains with coherence it is the coherence.)

In Section~4, we will explain how the category of games described in Section~2 
does indeed give rise to a category of sequential domains in exactly this
sense. This yields the first syntax-independent description of the
fully abstract model for PCF.  

A still more stringent requirement on a description of the fully abstract model
is that it should yield effective methods for deciding observation
equivalence on terms. For example, consider ``Finitary PCF'', i.e. PCF
based on the booleans rather than the natural numbers. The interpretation
of each type of Finitary PCF in the fully abstract model is a finite poset.
A natural question is whether these finite posets can be
effectively
presented. Suppose that we have a category of sequential domains as described
in the previous paragraph, yielding a fully abstract model of PCF.
If the ``intensional structure'' part of the interpretation of each type
could itself be specified in a finite, effective fashion, then such a model
would immediately yield a positive solution to this problem.
Because of its intensional character, our model does not meet this requirement:
there are infinitely many strategies at each functional type of Finitary PCF.
The same point occurs in one form or another with all the currently known
descriptions of the fully abstract model for PCF.
A remarkable result by Ralph Loader \cite{Loa96} shows that this is in fact 
inevitable.
Loader proved that observation equivalence for Finitary PCF is undecidable.
This shows that an intensional description of the fully abstract model is
the best that we can hope to do.
\subsection*{Related Work}
The results in the present paper were obtained in June 1993 (the results on
Intensional Full Abstraction in Section 3) and September 1993 
(the results on the intrinsic
preorder and (extensional) Full Abstraction in Section 4).
They were announced on various electronic mailing lists in June and September
1993. An extended  abstract of the present paper appeared in the Proceedings
of the Second Symposium on Theoretical Aspects of Computer Science, which
was held in Sendai in April 1994 \cite{AbramskyS:fulap}.

Independently, and essentially simultaneously, Martin Hyland and Luke Ong
gave a different model construction, also based on games and strategies,
which led to the same model of PCF, and essentially the same results on 
Intensional
Full Abstraction. Following our work on the intrinsic preorder, they
showed that similar results held for their model. What is interesting
is that such similar results have been obtained by somewhat different routes.
Hyland and Ong's approach is based on dialogue games and innocent strategies,
in the tradition of Lorentzen's dialogue interpretations of logical proofs
\cite{LorenzenP:logua,LorenzenP:eindk}, and the work by Kleene and Gandy on the
semantics of higher-type 
recursion theory \cite{GandyRO:diabso}, while our approach is closer
to process semantics  
and the Geometry of Interaction
\cite{AbramskyS:gamfcm,MalacariaP:frogi}. Further work is needed to 
understand more fully the relationship between the two approaches.

Independently, Hanno Nickau obtained essentially the same model and results
as Hyland and Ong \cite{NickauH:hersf}. A very different description
of the fully abstract 
model for PCF was obtained by Peter O'Hearn and Jon Riecke, using Kripke logical
relations \cite{OHearnPW:krilrp}. This construction is very
interesting, and probably 
of quite general applicability, but does not appear to us to embody a
specific semantic analysis of sequentiality.

Since the results described in this paper were obtained, there has
been significant further progress in the use of game semantics to give
fully abstract models for programming languages. 
These results all build on the concepts, methods and results developed
in the present paper, and that of Hyland and Ong. For an expository account of 
some of these results, and some references,
see \cite{AM97}; there is an overview in \cite{Abr97}.
The main results of the present paper are recast in an abstract,
axiomatic form in \cite{Abr00}.
There have also been some
significant applications of game semantics, notably \cite{MH99,GM00}.


\section{The Model}
We shall refer to
\cite{AbramskyS:gamfcm} for general background and motivation on game semantics.

We begin by fixing some notation.  If  is a set, we write
 for the set of finite sequences (words, strings) on .
We shall use , , ,   and primed and subscripted variants
of these to denote sequences, and , , , , ,  and
variants to denote
elements of these sequences. Concatenation of sequences will be indicated
by juxtaposition, and we will not distinguish notationally between an element
and the corresponding unit sequence. Thus {\sl e.g.}  denotes a sequence
with first element   and tail .
If , then  is
the unique monoid homomorphism extending .  We write  for the
length of a finite sequence, and  for the 'th element of ,
.
Given a set  of sequences, we write
\even{S} for the subset of even length sequences and \odd{S} for the
subset of odd length sequences.  If  and , we write  for the result of
deleting all occurrences of symbols not in  from .
We write  if  is a prefix of , {\em i.e.} for some , .  We always consider sequences under this prefix ordering and use
order-theoretic notions~\cite{DaveyBA:Intlor} without further comment.

Given a family of sets  we write 
for their disjoint union (coproduct); we fix

as a canonical concrete representation. In particular, we write
 for .
If  and ,
we define  inductively by:

We use  and  as notation for first and second projection functions.
Note that with  as above,  is a sequence of indices  tracking which components of the disjoint union the
successive elements of  are in.

We will also need some notation for manipulating partial functions.
We write  if  is a partial function from the set  to
the set ; and  for `` is defined and equal to ''.
If  is an injective partial function, we write   for the converse, which is also an injective partial function.
(NB: the reader should beware of confusing  with .
In practice, this should not be a problem.)
If  are partial functions with disjoint domains
of definition, then we write   for the
partial function obtained by taking  the union of (the graphs of)  and .
We write  for the everywhere-undefined partial function on  and
sometimes , sometimes  for the identity function on .
We shall omit subscripts whenever we think
we can get away with it.

\subsection{Games}
The games we consider are between Player and Opponent.  A {\em play} or {\em
run} of the game consists of an alternating sequence of moves, which may be
finite or infinite.  Our plays are always with Opponent to move first.


A game is a structure , where
\begin{itemize}
\item  is the set of moves.
\item  is the
  labelling function.

  The labelling function indicates if a move is by Player (P) or
  Opponent (O), and if a move is a question (Q) or an answer (A). The
  idea is that questions correspond to requests for data, while
  answers correspond to data
  ({\em e.g.} integer or boolean values). In a higher-order
  context, where arguments may be functions which may themselves be
  applied to arguments, all four combinations of Player/Opponent with
  Question/Answer are possible.  can be decomposed into
  two functions  and
  .


  We write
 
etc., and define



\item Let  be the set of all finite sequences  of
  moves satisfying:

Then , the set of valid {\em positions} of the game, is a
non-empty prefix closed subset of .

The conditions {\bf (p1)}--{\bf (p3)} can be thought of as global rules
applying to all games. {\bf (p1)} says that Opponent moves first, and
{\bf (p2)}  that Opponent and Player alternate. {\bf (p3)} is known as
the {\em bracketing condition}, and can be nicely visualised as follows.
Write each
question in a play as a left parenthesis ``('', and each answer as a
right parenthesis ``)''.  Then the string must be well-formed in the
usual sense, so that each answer is associated with a unique previous
question---the most recently asked, as yet unanswered question.  In
particular, note that a question by Player must be answered by
Opponent, and vice versa.

\item
 is an equivalence relation on  satisfying

Note in particular that {\bf (e1)} implies that if , then .
\end{itemize}
For example, the game for  has one possible opening move
 (request for data), with ; and for each , a possible response  with
.
 is the identity relation on .
The game for  is defined similarly.

\subsection{Strategies}
A {\em strategy} for Player in  is a non-empty subset
 such that

is prefix-closed, where

We will be interested in a restricted class of strategies, the history-free
(or history independent, or history insensitive) ones.
A strategy  is {\em history-free} if it satisfies
\begin{itemize}
\item 
\item  (equivalently, ).
\end{itemize}
Henceforth, ``strategy'' will always by default mean ``history-free strategy''.

Given any strategy , we can define  by

Conversely, given  we can define
 inductively by:

We say that  induces the strategy
, if . Note that if
 is a strategy, we have

so there is always a least partial function on moves canonically inducing a
(history-free) strategy.

\begin{proposition}
If  is any partial function, then
.
\end{proposition}

\begin{proof} Certainly any  satisfies ``
moves first'' and the alternation condition. We show that it
satisfies the bracketing condition by induction on . If , then since  and  is odd, the number of
questions in  must exceed the number of answers; hence 
satisfies the bracketing condition.
\end{proof}

The equivalence relation on positions extends to a relation on
strategies, which we shall write as . \\
 iff:


By abuse of notation we write the symmetric closure of this relation
as :


Interpreting the equivalence on positions as factoring out coding conventions,
 expresses the fact that  and 
are the same modulo
coding conventions.   expresses a
``representation independence'' property of strategies.

\begin{proposition} [Properties of ] \hfill

 is a partial preorder relation (i.e. transitive) on
  strategies.
Hence  is a partial equivalence relation (i.e. symmetric and transitive).

\end{proposition}

\begin{proof}  Suppose  and , and , , 
and . By induction on  using the definition of
 and (e3), there is  with
. But then , and since ,  with  and hence
 as required.
\end{proof}

>From now on, we are only interested in those history-free
strategies  such that .We write  for the set of such strategies over . If  is
such a strategy for a game , we shall write . We
write  for the set of partial equivalence classes of
strategies on  , which we think of as the set of ``points'' of
. We write  when
.

\subsection{Multiplicatives}
\paragraph{Tensor}

The game  is defined as follows.   We call the games 
and  the {\sl component} games.

\begin{itemize}
\item , the disjoint union of the two move
sets.
\item , the source tupling.
\item  is the set of all 
such that:
\begin{enumerate}
\item {\em Projection condition:} The restriction to the moves in  (resp. ) is in
 (resp. ).
\item {\em Stack discipline:} Every answer in  must be in the same component game as the
  corresponding question.
\end{enumerate}

\item 
\end{itemize}
We omit the easy proof that  satisfies
{\bf (e1)}--{\bf (e3)}.
Note that, if the equivalence relations  and  are
the identities on  and  respectively, then 
is the identity on .

The tensor unit is given by


\paragraph{Linear Implication}
The game  is defined as follows.   We call the games 
and  the {\sl component} games.

\begin{itemize}
\item , the disjoint union of the two move
sets.
\item .
\item  is the set of all 
such that:
\begin{enumerate}
\item {\em Projection condition:} The restriction to the moves in  (resp. ) is in
 (resp. ).
\item {\em Stack discipline:} Every answer in  must be in the same component game as the
  corresponding question.
\end{enumerate}

\item 
\end{itemize}
Note that, by {\bf (p1)}, the first move in any position in
 must be in .

We refer to the condition requiring answers to be given in the same
components as the corresponding questions as the {\sl stack discipline}.
It ensures that computations must evolve in a properly nested fashion.
This abstracts out a key structural feature of functional computation, and plays
an important r\^{o}le in our results.

\begin{proposition}[Switching Condition]
If a pair of successive moves in a position in  are in different
components, (i.e. one was in  and the other in ),
then the second move was by Opponent (i.e. it was Opponent who
switched components). If two successive moves in  are
in different components, the second move was by Player (i.e. it was Player
who switched components).
\end{proposition}

\begin{proof} Each position in  can be classified as in
one of four ``states'': , i.e. an even number of moves
played in both components, so Opponent to move in both; ,
meaning an odd number of moves played in the first component, so
Player to move there, and an even number of moves played in the
second component, so Opponent to play there; ; and . Initially, we are in state . After Opponent moves, we
are in  or , and Player can only move in the same
component that Opponent has just moved in. After Player's move, we
are back in the state . A simple induction shows that this
analysis holds throughout any valid play, so that we can never in
fact reach a state , and Player must always play in the
same component as the preceding move by Opponent. A similar
analysis applies to ; in this case the initial state
is , after Opponent's move we are in , and after
Player's response we are in  or . 
\end{proof}

Note that, by comparison with \cite{AbramskyS:gamfcm}, the Switching Condition is
a consequence of our definition of the multiplicatives rather than
having to be built into it.
This is because of our global condition {\bf (p1)}, which corresponds to
restricting our attention to ``Intuitionistic'' rather than ``Classical'' games.
Note also that the unreachable state  in  is precisely
the problematic one in  the analysis of Blass' game semantics in \cite{AbramskyS:gamfcm}.

\subsection{The Category of Games}
We build a category \Games:

We shall write  to mean that  is a strategy in
 satisfying .

There are in general two ways of defining a (history-free) strategy or operation
on strategies: in terms of the representation of strategies as sets of
positions, or via the partial function on moves inducing the strategy.
Some notation
will be useful in describing these partial functions.
Note that the type of the function  inducing a strategy in 
is

Such a function can be written as a matrix

where


For example, the twist map

corresponds to the matrix

where  is the everywhere-undefined partial function.
(Compare the interpretation
of axiom links in \cite{GirardJY:geoi1i}.) The strategy induced by this function is
the copy-cat strategy as defined in \cite{AbramskyS:gamfcm}.
As a set of positions, this strategy is defined by:


In process terms, this is a bi-directional one place
buffer~\cite{AbramskyS:prop}.
These copy-cat strategies are the identity morphisms in \Games .

\paragraph{Composition}
The composition of (history-free) strategies can similarly
be defined either
in terms of the set representation, or via the underlying functions on moves
inducing the strategies.  We begin with the set representation.
Given , we define

This definition bears a close resemblance to that of ``parallel composition
plus hiding'' in the trace semantics of CSP \cite{HoareCAR:comsp};
see \cite{AbramskyS:gamfcm} for an extended
discussion of the analogies between game semantics and concurrency
semantics, and \cite{AbramskyS:prop} for other aspects.

We now describe composition
in terms of the functions inducing strategies.  Say we have .  We want to find  such
that .  We shall compute  by
the ``execution formula''~\cite{GirardJY:towgi,GirardJY:geoi1i,GirardJY:geoi2d}.  Before giving the
formal definition, let us explain the idea, which is rather simple.
We want to hook the strategies up so that Player's moves in  under
 get turned into Opponent's moves in  for ,
and vice versa.  Consider the following picture:
\begin{center}
\setlength{\unitlength}{0.0125in}
\begin{picture}(361,195)(135,550)
\thicklines
\put(350,560){\line( 1, 0){ 30}}
\put(220,560){\line( 1, 0){ 40}}
\put(350,740){\vector( 1, 0){ 30}}
\put(260,740){\vector(-1, 0){ 40}}
\put(260,560){\line( 1, 2){ 90}}
\put(260,740){\line( 1,-2){ 90}}
\put(440,620){\vector( 0,-1){ 60}}
\put(440,740){\vector( 0,-1){ 60}}
\put(380,620){\vector( 0,-1){ 60}}
\put(380,740){\vector( 0,-1){ 60}}
\put(220,620){\vector( 0,-1){ 60}}
\put(160,620){\vector( 0,-1){ 60}}
\put(220,740){\vector( 0,-1){ 60}}
\put(160,740){\vector( 0,-1){ 60}}
\put(360,620){\framebox(100,60){}}
\put(140,620){\framebox(100,60){}}
\put (445,565) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (445,720) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (360,565) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{
}}}
\put (360,720) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{
}}}
\put (225,565) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{
}}}
\put (225,720) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{
}}}
\put (135,565) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (135,720) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (405,645) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ g}}}
\put (190,645) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ f}}}
\end{picture}

\end{center}

Assume that the Opponent starts in .  There are two
possible cases:
\begin{itemize}
\item  The move is mapped by  to a response in : In this case,
this is the response of the function .
\item The move is mapped by  to a response in .  In this
  case, this response is interpreted as a move of the Opponent in
   and fed as input to .  In turn, if  responds in
  , this is the response of the function .  Otherwise, if 
  responds in , this is fed back to .  In this way, we
  get an internal dialogue between the strategies  and .
\end{itemize}


It remains to give a formula for computing  according to these
ideas.  This is the execution formula:

The join in the definition of  can be interpreted concretely as union of
graphs.  It is well-defined because it is being applied to a family of
partial functions with pairwise disjoint domains of definition.  The
functions  are defined by

The idea is that  is the function which, when
defined, feeds an input from  or  exactly  times around
the channels of the internal feedback loop and then exits from  or
.  The retraction

is defined by

and the ``message exchange'' function

is defined by

Here,  is the everywhere undefined partial function.

The fact that this definition of composition coincides with that given
previously in terms
of sets of positions is proved in \cite[Proposition~3]{AbramskyS:gamfcm}.


\begin{proposition}
Composition is monotone with respect to :
  
\end{proposition}

\begin{proof}  We follow the analysis of composition given in the
proof of Proposition~1 of \cite{AbramskyS:gamfcm}. Suppose , ,  and
. Then  for uniquely determined  such that , . We must have . Since ,  for unique , and . Now  and  implies that  for unique
, and . Continuing in this
way, we obtain a uniquely determined sequence  such that , , and , as required. This argument is
extended to general strings  by an induction
on . 
\end{proof}

We say that a string  is
{\em well-formed}
if it satisfies the bracketing condition and the stack discipline; and
{\em balanced} if it is well-formed, and the number of questions in 
equals the number of answers.
Note that these properties depend only on the string  obtained from 
by replacing each question in  by
 respectively, and each answer in  by  respectively.

\begin{lemma}
The balanced and well-formed strings in

are generated by the following context-free grammar:


(More precisely,  is well-formed (balanced) iff  is derivable from
{\sc wf} ({\sc bal}) in the above grammar.)
\end{lemma}

\begin{proof} It is easy to see that the terminal strings derivable
from {\sc bal} are exactly the balanced ones, and that strings
derivable from {\sc wf} are well-formed. Now suppose that  is
well-formed. We show by induction on  that  is derivable
from {\sc wf}. If  is non-empty, it must begin with a question,
. If this question is not answered in , then  is
well-formed, and by induction hypothesis  is derivable from
{\sc wf}, hence  is derivable via the production . If this question is answered, so
, then  is balanced, and hence
derivable from {\sc bal}, and  is well-formed, and so by
induction hypothesis derivable from {\sc wf}. Then  is
derivable from {\sc wf} via the production . 
\end{proof}

\begin{lemma}[Projection Lemma]
If  is well-formed (balanced),
then
so is  for any subsequence
 of .
\end{lemma}

\begin{proof} We use the characterization of well-formed and
balanced strings from the previous lemma, and argue by induction
on the size of the derivation of  from {\sc wf} or {\sc bal}.
Suppose  is well-formed. If  is empty, the result is
immediate. If  is derivable via , so  where  is
balanced and  is well-formed, then we can apply the induction
hypothesis to  and . Similarly when  where 
is well-formed, we can apply the induction hypothesis to . The
argument when  is balanced is similar. 
\end{proof}

\begin{lemma}[Parity Lemma]
If  is such that , where
,  are moves in the ``visible'' components  and , then:
\begin{itemize}
\item if ,  are in the {\em same} component, then  is even.
\item if ,  are in {\em different} components, then  is odd.
\end{itemize}
\end{lemma}

\begin{proof} Firstly, we consider the case where all moves in 
are in . Suppose for example that  and  are both in .
Then the first move in  is by , while the last move is
by , since it must have been  which returned to .
Thus  is even. Similarly if  and  are both in . Now
suppose that  is in  while  is in . Then the first and
last moves in  were both by , so  is odd; and
similarly if  is in  and  is in .

Now we consider the general case, and argue by induction on .
Suppose  and  are both in . Let ,
where all moves in  are in . Suppose firstly that 
is in ; then  is even, and by induction hypothesis
 is even, so  is even. If  is
in , then  is odd, and by induction hypothesis  is odd, so  is even. The other cases are
handled similarly. 
\end{proof}

\begin{proposition}
If  and , then  satisfies
the bracketing condition and the stack discipline.
\end{proposition}

\begin{proof} By the Projection Lemma, it suffices to verify that
every  is well-formed. We argue by induction
on . The basis is trivial. Suppose . If  is a
question, it cannot destroy well-formedness. If  is an answer
with no matching question, then by induction hypothesis  is
balanced. Suppose  is in  or ; then by the Projection
Lemma,  is balanced, so  has no matching question
in , contradicting . A similar argument applies when  is in  or .

So we need only consider  where ,  are a matching question-answer pair.
It remains to show  that  and  must be in the same component.
Suppose firstly that  and  both occur in  or . Note that  is
balanced, and then by the Projection Lemma, so is .
So  and  will be paired in , and hence they
must be in the same component. Similarly when  and  are both in  or
.

The final case to be considered is when  and  both occur in
 or . Since  is balanced, by the Projection Lemma so is
. It follows that  is even, so by the
Parity Lemma,  and  must be in the same component. 
\end{proof}


Combining Propositions~2.4.2 and~2.4.6 with Proposition~2 from \cite{AbramskyS:gamfcm}, we obtain:

\begin{proposition}
\Games\ is a category.
\end{proposition}

\subsection{ as an autonomous category}
We have already defined the object part of the tensor product , linear implication , and the tensor unit .
The action of tensor on morphisms is defined as follows.  If , then
  is
induced by the partial function

The natural isomorphisms for associativity, commutativity and unit of the
tensor product:

are induced by the evident bijections on the sets of moves:



The application morphism  is induced by



\setlength{\unitlength}{0.0125in}
\begin{picture}(200,263)(40,540)
\thicklines
\put(300,620){\vector( 0,-1){ 60}}
\put(260,620){\vector( 0,-1){ 60}}
\put(220,620){\vector( 0,-1){ 60}}
\put(180,620){\vector( 0,-1){ 60}}
\put(300,780){\vector( 0,-1){ 60}}
\put(260,780){\vector( 0,-1){ 60}}
\put(220,780){\vector( 0,-1){ 60}}
\put(180,780){\vector( 0,-1){ 60}}
\put(180,780){\framebox(0,0){}}
\put(140,620){\framebox(200,100){}}
\put(300,720){\line(-4,-5){ 80}}
\put(220,720){\line( 4,-5){ 80}}
\put(260,720){\line(-4,-5){ 80}}
\put(180,720){\line( 4,-5){ 80}}
\put (205,550) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (285,785) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (165,550) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (245,785) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (285,550) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (205,785) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (240,550) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\put (165,785) {\makebox(0,0) [lb] {\raisebox{0pt}[0pt][0pt]{ }}}
\end{picture}

This ``message switching'' function can be understood in algorithmic terms
as follows.  A demand for output from the application at  is
switched to the function part of the input, ; a demand by the
function input for information about its input at  is forwarded to
the input port ;  a reply with this information about the input at
 is sent back to the function; an answer from the function to the
original demand for output at  is sent back to the output port .
Thus, this strategy does indeed correspond to a protocol for linear
function application---linear in that the ``state'' of the input changes as
we interact with it, and there are no other copies available
allowing us to backtrack.

As for currying, given
,
 is induced by


For discussion of these definitions, and most of the verification that they
work as claimed, we refer to Section~3.5 of \cite{AbramskyS:gamfcm}.

\begin{proposition}
\begin{enumerate}
\item If  and  then .

\item  satisfies the stack discipline.
\end{enumerate}
\end{proposition}
\begin{proposition}
\Games\ is an autonomous category.
\end{proposition}

\subsection{Products}
The game  is defined as follows.

The projections

are induced by the partial injective maps


which are undefined on  and  respectively.
Pairing {\sl cannot} be defined in general on history-free strategies in
;
however, it can be defined on the co-Kleisli category for the
comonad , as we will see.

\subsection{Exponentials}
Our treatment of the exponentials is based on \cite{AbramskyS:gamexp}.
The game  is defined as the ``infinite symmetric tensor power''
of .
The symmetry is built in via the equivalence relation on positions.

\begin{itemize}
\item , the
  disjoint union of countably many copies of the moves of .  So, moves
  of  have the form , where  is a natural
  number, called the index, and  is a move of .
\item Labelling is by source tupling:

\item We write  to indicate the restriction to moves with
  index .   is the set of all
  
such that:
\begin{enumerate}
\item {\em Projection condition:} .
\item {\em Stack discipline:} Every answer in  is in the same index as the
corresponding question.
\end{enumerate}
\item Let  be the set of permutations on .


\end{itemize}

\paragraph{Dereliction}
For each game  and , we define a strategy

induced by the partial function :

In matrix form


\begin{proposition}
\begin{enumerate}
\item For all , :

\item \,  satisfies the stack discipline.
\end{enumerate}
\end{proposition}

By virtue of this Proposition, we henceforth write , meaning
 for arbitrary choice of .

\paragraph{Promotion}
A {\em pairing function} is an injective map

Given  and a pairing function , we define
 as the strategy induced by the
partial function  defined by:

In matrix form

where


\begin{proposition}
\begin{enumerate}
\item \, If , , and , 
are pairing functions, then .
\item \,  satisfies the stack discipline.
\end{enumerate}
\end{proposition}
By virtue of this Proposition, we shall henceforth write ,
dropping explicit reference to the pairing function.

\begin{proposition}
For all , :

\end{proposition}
As an immediate consequence of this Proposition and standard results
\cite{ManesE:algt}:
\begin{proposition}
 is a comonad in ``Kleisli form''.
If we define, for ,
, and
 by ,
then  is a comonad in the standard sense.
\end{proposition}

\paragraph{Contraction and Weakening}
For each game , we define  by
.

A {\em tagging function} is an injective map

Given such a map, the contraction strategy

is induced by the function
 where , .

Again, it is easily verified that 
for any tagging functions , .

\begin{proposition}
,  are well-defined strategies which give
a cocommutative comonoid structure on , {\em i.e.} the following diagrams
commute:


\end{proposition}


\subsection{The co-Kleisli category}
By Proposition~2.7.4, we can form the co-Kleisli category , with:
\begin{description}
\item[Objects] The objects of .
\item[Morphisms] .
\item[Composition] If  and 
then composition in  is given by:

\item[Identities] The identity on  in  is
.
\end{description}

\paragraph{Exponential laws}
\begin{proposition}
\begin{enumerate}
\item There is a natural isomorphism
.
\item \, .
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item We define  as
  (the strategy induced by) the
map which sends  to
,  to
 and similarly sends  to ,  to  .

We define  as
(the strategy induced by) the map
which sends  to ,   to  and  to  ,  to .

It is straightforward to check that  are strategies.
Let's prove that  define the required isomorphism.

\begin{itemize}
\item For  (we have used different subscripts for different copies of
the same game) we have that  is sent to
 and  is
sent to  . This strategy is
equivalent to the
identity. The automorphism which witnesses the equivalence is the map
which sends  in  to  and  in  to  (and is
the identity elsewhere).

\item For  the same map as above witnesses the
equivalence of  with the identity.
\end{itemize}
\item Immediate by definition.

\end{enumerate}
\end{proof}


\paragraph{Products in }
\begin{proposition}
 is terminal in .
\end{proposition}
\begin{proof} For any game  there is only one strategy in
, namely . This is because  has an
empty set of moves and for any opening move  in  we have
 so that Opponent has no opening move in .
  \end{proof}


\begin{proposition}

is a product diagram in , where


If  then their pairing
 is defined by


\end{proposition}

In fact, we have:

\begin{proposition}
 has countable products.
\end{proposition}


\paragraph{Cartesian closure}
We define .
\begin{proposition}
 is cartesian closed.
\end{proposition}

\begin{proof} We already know that  has finite products. Also,
we have the natural isomorphisms

Thus  is cartesian closed, with ``function spaces'' given by .
\end{proof}

We shall write , since we think of this category as our
intensional model.

\subsection{Order-enrichment}
There is a natural ordering on strategies on a game 
given by set inclusion.
It is easily seen that (history-free) strategies are closed under directed
unions, and that  is the least element in this ordering.
However, morphisms in  are actually partial equivalence classes
of strategies, and we must define an order on these partial equivalence
classes.

We define:



\begin{proposition}
 is a partial order over  . The least element
in this partial order is .
\end{proposition}

We have not been able to determine whether  is
a cpo in general. However, a weaker property than cpo-enrichment suffices
to model PCF, namely {\em rationality}, and this property can be
verified for .

A {\em pointed poset} is a partially ordered set with a least element.
A cartesian closed category  is {\em pointed-poset enriched}
(ppo-enriched) if:
\begin{itemize}
\item Every hom-set  has a ppo structure
.
\item Composition, pairing and currying are monotone.
\item Composition is {\em left-strict}: for all ,

\end{itemize}
 is {\em cpo-enriched} if it is ppo-enriched, and moreover
each poset

is directed-complete,
and composition preserves directed suprema.
 is {\em rational}  if it is ppo-enriched, and moreover
for all :
\begin{itemize}
\item The chain  in 
defined inductively by

has a least upper bound, which we denote by .
\item For all , ,

\end{itemize}
Altough the standard definition of categorical model for PCF is based on
cpo-enriched categories, in fact rational categories suffice to interpret
PCF, as we will see in Section~2.10.

\paragraph{Strong completeness and continuity}
Let  be a game, and  a directed set.
A family  is said to be
{\em strongly directed} if there exist strategies  for each
 such that 
and .

\begin{proposition}
A strongly directed family is -directed.
Every strongly directed family has a -least upper bound.
\end{proposition}

Now consider the constructions in   we have introduced in previous
sections. They have all been given in terms of concrete operations
on strategies,
which have then been shown to be compatible with the partial preorder
relation , and hence to give rise to well-defined operations
on morphisms of .
Say that an -ary concrete operation  on strategies is
{\em strongly continuous} if it is monotone with respect to ,
and monotone
and continuous with respect to subset inclusion and directed unions:

for directed .
(Note that for , these properties reduce to .)

\begin{proposition}
Composition, tensor product, currying and promotion
are strongly continuous.
\end{proposition}

\begin{proposition}\label{229}
 is a rational cartesian closed category.
\end{proposition}

\subsection{The model of PCF}

PCF is an applied simply-typed -calculus; that is, the terms in PCF are
terms of the simply-typed -calculus built from a certain
stock of constants.
As such, they can be interpreted in any cartesian closed category once we have
fixed the interpretation of the ground types and the constants.
The constants of PCF fall into two groups: the ground and first-order constants
concerned with arithmetic manipulation and conditional branching;
and the recursion combinators
  for each type .
These recursion combinators can be canonically interpreted in any rational
cartesian closed category .
Indeed, given any object  in , we can define

by

Now define .
Note that

where

These terms  are the standard ``syntactic approximants''
to .


Thus, given a rational cartesian closed category , a model

of PCF can be defined by stipulating the following additional information:
\begin{itemize}
\item For each ground type of PCF, a corresponding object
of . This suffices to determine the interpretation of each
PCF type  as an object in , using the cartesian closed structure
of .
(For simplicity, we shall work with the version of PCF with a single ground
type .)
\item For each ground constant  and first-order function of PCF,
say of type ,
a morphism  in , where 
is the terminal object in , and  is the object in 
interpreting the type . ( is a ``point'' or ``global element'' of the
type .)
\end{itemize}

We say that  is a {\em standard model} if , the flat cpo of the
natural numbers, and moreover the interpretation of the ground and
first-order arithmetic constants agrees with the standard one. We
cite an important result due to Berry
\cite{BerryG:modcom,BerryG:fulasl}.

\begin{theorem}[Computational Adequacy]
If  is a standard model, then it is computationally
adequate; {\em i.e.} for all programs  and ground constants ,

and hence the model is {\em sound}: for all terms ,

\end{theorem}
(Berry stated his result for models based on cpo-enriched categories,
but only used rational closure.)

Thus to obtain a model  it remains only to specify the
ground types and first-order constants.
The interpretation of  as  has already been given at the
end of Section~2.1. It is readily seen that .

\paragraph{Ground constants}
For each natural number , there is a strategy , given by

Also, .

\paragraph{Arithmetic functions}
For each number-theoretic partial function 
there is a strategy



\paragraph{Conditionals}
The strategy  interpreting  is defined as follows:
in response to the initial question,  interrogates its first
argument;
if the answer is 0, then it interrogates the second argument, and copies the
reply to the output; if the answer is any number greater than 0, it interrogates
the third argument, and copies the reply to the output.

\begin{proposition}
 is a standard model of PCF.
\end{proposition}

\section{Intensional Full Abstraction}
\newcommand{\Intmod}{\cal M(I)}

\subsection{\pcfc}

In order to obtain our intensional full abstraction result, it turns out
that we need to consider an extension of PCF. This extension is quite
``tame'', and does not change the character of the language. It consists
of extending PCF with a family of first order constants
 for each .  The
functions that these constants are intended to denote are defined by:

The interpretation of  as a strategy is immediate:
this strategy responds to the initial question by interrogating its
first input; if the response is , with , it
interrogates the 'th input and copies the answer to the output;
otherwise, it has no response.

To see how harmless this extension, which we call \pcfc, is, note that
each term in \pcfc\  is observationally equivalent to one in PCF.
Specifically,

\begin{tabbing}
case =  lx. \=ly.\= ...\= y. \kill
 \\
\>  \\
\> \> (\\
\>\>\> \vdots \\
\>\>\> (\\
\end{tabbing}
The point is that our intensional model is sufficiently fine-grained
to distinguish between these observationally equivalent terms.
However, note that our results in Section~4 apply directly to PCF.



\subsection{Evaluation Trees}

We shall now describe a suitable analogue of B\"{o}hm
trees~\cite{BarendregtHP:lamcss}
for \pcfc.  These give an (infinitary) notion of normal forms for
\pcfc\ terms, and provide a bridge between syntax and semantics.

We use  to range over type environments .  We define \finevalT{\Gamma}{T}, the finite
evaluation trees of type  in context , inductively as
follows:
\begin{itemize}

\item \twotrans{M \in \finevalT{\Gamma,x:T}{U}}{\lambda x^T. M \in
    \finevalT{\Gamma}{T \Rightarrow U}}

\item \twotrans{}{\Omega, {\tt n} \in \finevalT{\Gamma}{N}}

\item \twotrans{
\begin{array}{l}
\Gamma(x) = T_1 \Rightarrow \ldots T_k \Rightarrow N, \\
 P_i \in \finevalT{\Gamma}{T_i}, 1 \leq i \leq k, \\ Q_n \in
    \finevalT{\Gamma}{N}, n \in \omega, \\ \exists n \in \omega. \;
    \forall m \geq n. \; Q_n = \Omega
\end{array}
}{{\tt case}(xP_1 \ldots P_k, (Q_n
    \mid n \in \omega)) \in \finevalT{\Gamma}{N}}
\end{itemize}

We regard these evaluation trees as defined ``up to
--equivalence'' in the usual sense. Note that if we
identify each  with  for the least  such that  for all , then every finite evaluation tree is a term in \pcfc.

We order \finevalT{\Gamma}{T} by the ``--match ordering'':
 if  can be obtained from  by replacing
occurrences of  by arbitrary finite evaluation trees.

\begin{proposition}\label{A}
 is a pointed poset with non-empty
meets.  Every principal ideal is a finite distributive lattice.
\end{proposition}

Now we define \evalT{\Gamma}{T}, the space of evaluation trees, to be
the ideal completion of \finevalT{\Gamma}{T}.  As an immediate
consequence of proposition~\ref{A}, we have

\begin{proposition}
\evalT{\Gamma}{T} is a dI-domain.  The compact elements are terms of
\pcfc.
\end{proposition}
Strictly speaking, the compact elements of \evalT{\Gamma}{T} are
principal ideals , where  is a finite evaluation
tree, which can be identified with a term in \pcfc\ as explained
above.

\subsection{The Bang Lemma}

We now prove a key technical result. This will require an additional
hypothesis on games. Say that a game  is {\em well-opened} if the
opening moves of  can only appear in opening positions. That is,
for all  if  then

It is easy to see that  and  are well-opened, that if 
and  are well-opened so is  and that if  is
well-opened so is .
Here and henceforth we blur the distinction between the type  and
the game it denotes.
Thus the category of well-opened
games is cartesian closed, and generates the same PCF model
.


Now let  be well-opened and consider .
Using the switching condition, we see that  can be written uniquely
as

where each ``block''  has the form ,
i.e. starts with a move in ; every move in  occurring in
 has the form  for some , i.e. has the same
index as the opening move in ; if  are two adjacent
blocks then ; and  is even (so
each block starts with an O-move). We refer to   as the {\em
  block index} for . For each such block index  we define
 to be the subsequence of  obtained by deleting all blocks
with index .

Some further notation. For , we define

i.e. the set of all indices of moves in  occurring in . Also,
we write  for the projection of  to moves of the
form , i.e. moves in  with index ; and similarly
.

\begin{lemma}\label{disblock}
 For all  with  well-opened, ,
 and block indices  occurring in :
\ITEM{
\item[(i)] ,
\item[(ii)]  implies .
}
\end{lemma}
\begin{proof} By induction on . The basis is trivial. For the
inductive step, write ,
, . Let the index of
 be . We show firstly that . By the induction hypothesis, for all , while obviously .
Also,  is either a move in  with index , or a move in
. In the latter case, by the switching condition the index of 
is in .
Hence the projection conditions are satisfied
by . Moreover  is well-formed by the Projection Lemma
2.4.4.
Thus  as required.

By induction hypothesis, , and since
 is a well-defined history-free strategy, with
 since  we conclude that
. Moreover, for ,
 by induction hypothesis. This establishes
{\it (i)}.

Now note that, if  satisfies {\it (ii)}, so does  by the
switching condition. Suppose for a contradiction that  does
not satisfy {\it (ii)}. This means that , where  for some  and hence that  where , so that  is a non-opening
move in . But we have just shown that  and hence that . By induction hypothesis  and hence . Thus  is both an opening and a non-opening move of
, contradicting our hypothesis that  is well opened.
\end{proof}

With the same notation as in lemma \ref{disblock}:

\begin{corollary}\label{cordisblock}
\begin{itemize}
\item[(i)] .
\item[(ii)] .
\item[(iii)] .
\item[(iv)]  implies .
\end{itemize}
\end{corollary}

\begin{lemma}\label{lemma2}
Let  with  well-opened. If
 then .
\end{lemma}
\begin{proof} We prove the contrapositive. Suppose
. Then w.l.o.g. we can assume that there
exist positions  such that ,
, , and either
 or  and
. Let the block index of  in  be
, and of  in  be . Note that the block index of
 in  must also be .

By Lemma \ref{disblock},  and . We
claim that . Indeed, if , , then by definition of
 we must have  and the permutation
 witnessing  must map the block
index of each  to that of , so that in particular
. Moreover,  must map
 bijectively onto . Using
Corollary \ref{cordisblock} for each , .

Now let  be defined by replacing each  in  by ; and  be defined by replacing each 
in  by . Then  and . We wish to
conclude that  witness the non equivalence
. Suppose for a
contradiction that for some ,
 and . This
would imply that for some ,  and
. Since  and
 is a well-defined history-free strategy, this implies that
. Using Lemma \ref{disblock} and Corollary
\ref{cordisblock} as above, . This yields the
required contradiction with our assumptions on 
\end{proof}

\begin{proposition}
[The Bang Lemma]

For all  with  well opened,

\end{proposition}

\begin{proof} By the right identity law (Prop. 2.11 \textbf{(m3)}),
 .
By Lemma \ref{lemma2}, this implies that

\end{proof}


\subsection{The Decomposition Lemma}

In this section we prove the key lemma for our definability
result. We begin with some notational conventions. We will work
mostly in the cartesian closed category  . We write
arrows in this category as  and
composition e.g. of  and  as . We will continue to write
composition in the Linear Category  in diagram order
denoted by  . We write  for the application in the cartesian closed
category, and ``linear'' application in  as  All games considered in
this section are assumed to be well-opened. If , we write  i.e. the set of all
indices of moves in  occurring in .

Now we define a strategy

corresponding to the {\bf case} construct. It will actually be most
convenient to firstly define the affine version

where we have tagged the occurrences of  for ease of
identification;

i.e.  responds to the initial question by interrogating its
first input; if it gets the response  it interrogates the 'th
component of its second input, and copies the response as its answer
to the initial question.

Now we define


We will now fix some notation for use in the next few lemmas. Let

be a strategy where we have tagged the two occurrences of  for
ease of identification. We assume that 's response to the
initial question  in  is to interrogate its second input,
i.e. to ask the initial question  in . Thus any non-empty
position in  must have the form . Moreover by the
stack discipline any {\em complete} position in , i.e. one
containing an answer to the initial question , must have the form

where  is the answer corresponding to the question  (this is
the sole---albeit crucial---point at which the stack condition is used
in the definability proof). Thus a general description of non-empty
positions in  is that they have the form

where  is the answer corresponding to , or

where  is not answered in .

\begin{lemma}\label{lem1}
For all 
\begin{itemize}
\item[(i)] 
\item[(ii)] .
\end{itemize}
\end{lemma}
\begin{proof} By induction on , which must be odd. (The proof follows very similar
lines to that of Lemma~3.1 in the previous section). The basis is
when , and , where . Then
\textit{(i)} follows because  is a well-defined history-free
strategy,
and \textit{(ii)} holds because otherwise  where  is
both
a starting move, using , and a non-starting
move, using , contradicting
well-openedness.
If , then we firstly show that  By the induction
hypothesis and the switching conditions, for all   so
 satisfies the projection conditions because  does. Also,  is balanced so by the Parity
Lemma 2.4.3  is well formed, and hence  is
well formed. Thus  Now since  is a well-defined
history-free strategy with , and 
by induction hypothesis, we must have ,
establishing {\it (i)}.

For {\it (ii)} suppose for a contradiction that  for . Then , where
. On the other hand, by induction hypothesis
, and by {\it
(i)}, . This contradicts our
assumption that games are well-opened.
\end{proof}


Now we define

and for all 

\begin{lemma}\label{lem2}
 and
 () are valid
strategies.
\end{lemma}
\begin{proof} The fact that each  is a set of valid
positions follows from Lemma \ref{lem1}. That  are
history-free and satisfy the partial equivalence relation follows
directly from their definitions and the fact that  is a
valid strategy.
\end{proof}

\begin{lemma}\label{lem3}
.
\end{lemma}
\begin{proof} Unpacking the definition of the RHS  we see that the second and third moves of 
synchronize and cancel out with the first and last moves of
 respectively, and the fourth and fifth moves of 
cancel out with the first and last moves of the appropriate
. Thus positions in  have the form  where 
are positions in , and  are bijectively reindexed
versions of  and , with the property that . However, by Lemma \ref{lem1} we know
that , and hence  and 
as required.
\end{proof}

\begin{lemma}\label{lem4}

\end{lemma}
\begin{proof}

\end{proof}

We continue with our decomposition, and define

\begin{lemma}\label{lem5}
 is a well-defined
strategy, and

\end{lemma}
\begin{proof}We must firstly explain how moves in  can be
interpreted as being of type . Let the index in  of the response
by  to the initial question  be . Then we regard
all moves in  with index  as moves in the
target  , and all moves with index  as moves in the
source . The projection conditions and
stack discipline are easily seen to hold for  with respect to
this type. The fact that  is history-free and satisfies
the partial equivalence relation follows directly from its
definition and the fact that  is a valid strategy.

Now write  for the RHS of . We diagram , tagging
occurrences of the types for ease of reference.



From the definitions  plays copy-cat strategies
between  and  and  and ;  plays a
copy-cat strategy between  and a single index
 in ;  {\tt con} splits
 into two disjoint address spaces
 and  and  plays copy-cat strategies between  and  and
between  and
. Thus we see that the opening move
in  is copied to  via  and , and
any response in  is copied back to . Similarly,
O's moves  are copied to  via  and
; and P's responses in  following  are
copied back to . Finally, O's moves in  are copied to , and P's responses following  are copied back to
.

As regards sequencing, the initial move  is copied
immediately as . Opponent may now either immediately
reply with , which will be copied back as ,
completing the play; or move in --- the only other
option by the switching condition. Play then proceeds following
 transposed to  until Opponent
replies with some  to . Thus positions in
 have the form  where  is a bijectively
reindexed version of , with . Clearly
, and hence
.

\end{proof}

We now prove a useful general lemma.
\begin{lemma}\label{lem6}
For all strategies


\end{lemma}
\begin{proof}

\end{proof}

Now consider a game

where 
Let , .

We define
 by

and 
by .Thus  is
the completely undefined strategy of type
 while  is the constant
strategy which responds immediately to the initial question in 
with the answer .

Finally, if , and for each 

and for each 

we define

by

\begin{lemma}[The Decomposition Lemma (uncurried version)]\label{lem7}

Let  be any strategy,
where

Then exactly one of the following three cases applies:
\begin{itemize}
\item[(i)] .
\item[(ii)]  for some .
\item[(iii)] 

where , ,  .
\end{itemize}
\end{lemma}

\begin{proof} By cases on 's response to the initial
question. If it has no response, we are in case {\it (i)}. If its
response is an immediate answer  for some , we are
in case {\it (ii)}. Otherwise,  must respond with the
initial question in the 'th argument, for some . In this case, write . We have the natural isomorphism
 so
we can apply Lemma \ref{lem4} to conclude that
 By Lemma \ref{lem5}

where . By the Bang
Lemma,  Moreover
 so by the universal property of the
product,  where .

Thus  and by Lemma \ref{lem6},

Thus

\end{proof}

The Decomposition Lemma in its uncurried version is not sufficiently
general for our purposes. Suppose now that we have a game

where


If for some  and each  we have

and for each 

then we define

by

To relate  and , consider the canonical isomorphisms

Let  so

where  is the
uncurried version of .
Then

In terms of calculus, this just boils down to the familiar
equations


To see the relationship between the combinators  and
 and
the syntax of PCF, we use the combinators to write the semantics of
finite evaluation trees.

\newcommand{\sem}[1]{{\cal S}( #1)}
Given  where , we
will define


\begin{itemize}
\item 
\item 
\item 
\item 
where



\end{itemize}
We can now prove the general form of the Decomposition Lemma:
\begin{proposition}[Decomposition Lemma]\label{lem8}
Let  be any
strategy, where

We write .
(Notation : if , then
.)

Then exactly one of the following three cases applies.
\begin{itemize}
\item[(i)] 
\item[(ii)] 
  for some .
\item[(iii)]
   where , and

\end{itemize}

\end{proposition}
\begin{proof} Let  be the
canonical isomorphism between  and its uncurried version
 for each
.

Let

Note that


We can apply Lemma \ref{lem7} to
. The result now follows from
equations (1)--(3) since 
\end{proof}

With the same notations as in the Decomposition Lemma:

\begin{lemma}[Unicity of Decomposition]\label{lem9}
\begin{itemize}
\item[(i)] If  then
  
\item[(ii)] If  then
  
\item[(iii)] If
 then

\end{itemize}
\end{lemma}
\begin{proof} {\it (i)} and {\it (ii)} are trivial.

For {\it (iii)} write  and
.

Suppose firstly that . Then , so
since , for some ,  and
. This implies that  and
. We conclude that .

Now suppose that . Then  where
 is a reindexed version of  with . Since
, there exists  such that 
and . This implies that there exists
 with . We conclude that

\end{proof}

\subsection{Approximation Lemmas}
The Decomposition Lemma provides for one step of decomposition of an
arbitrary strategy into a form matching that of the semantic clauses
for evaluation trees. However, infinite strategies will not admit a
well-founded inductive decomposition process. Instead, we must appeal
to notions of continuity and approximation, in the spirit of Domain
Theory \cite{AbramskyS:domt}.

We define a PCF {\em type-in-context} (\cite{CroleRL:catt}) to be a type of
the form

where  are PCF types. Given such a type-in-context
, we will write  for the set of strategies on the game
.

The Unicity of Decomposition Lemma says that decompositions are unique up
to partial equivalence. Referring to the Decomposition Lemma,
Prop. \ref{lem8}, note that the proof of the decomposition

involved defining specific strategies
 from the given
. If we also fix specific pairing and tagging functions and
dereliction indices in the definition of promotion,
dereliction, contraction etc.( and hence in the 
operations of composition, pairing, currying etc.), we obtain an
operation  on strategies such that

according to the case of the Decomposition Lemma which applies to
. We shall use  to define a family of
functions

inductively as follows:
\begin{itemize}
\item 
\item 
where

and


\end{itemize}
The principal properties of these functions are collected in the
following Lemma.

\begin{lemma}[Approximation Lemma for Strategies]\label{lemm1}
For all :
\begin{itemize}
\item[(i)]  implies 
\item[(ii)] If  is an increasing
  sequence,

\item[(iii)]  implies 
\item[(iv)] 
\item[(v)] 
\item[(vi)] 
\item[(vii)] 
\item[(viii)] 
\end{itemize}
\end{lemma}
\begin{proof} Firstly, consider the operation . In
case {\it (iii)}, where
  is obtained by firstly defining
 and the  from , then  from
, and finally  Note that  and the  are defined
{\em
  locally}, i.e. by operations on positions applied pointwise to
 and  respectively. Together with the
monotonicity and continuity of Promotion, Dereliction,
Contraction etc. (Proposition 2.9.4) this implies {\it (i)} and
{\it (ii)}. Now note that  is  and
 monotonic by Proposition 2.9.3. A straightforward
induction using monotonicity and monotonicity
of  respectively and the Unicity of Decomposition Lemma
yelds {\it (iii)}. Similarly routine inductions using
monotonicity and monotonicity of 
respectively prove {\it (iv)} and {\it (vi)}.

We prove {\it (v)} by induction on . The basis is trivial as are cases
{\it (i)} and {\it (ii)} of the Decomposition Lemma at the inductive step. Suppose
we are in case {\it (iii)}, with

Consider firstly  where  with  not
answered in . Then  where  is derived
from  and  from  as in the proof of the
Decomposition Lemma. Since
,  can be
decomposed into subsequences  with
, , .

Since , we can apply the induction hypothesis to
conclude that , and hence
that there is  with . The case where  is similar.

To prove {\it (vii)}, note firstly that the union  is well-defined by {\it (vi)}. Now  follows from {\it (iv)}, while  follows from {\it (v)}.

Finally {\it (viii)} can be proved by induction   on  and {\it
  (iii)} using the Unicity of Decomposition Lemma.
\end{proof}

We now turn to evaluation trees. Let
. We define a family of functions

inductively by



where



The following is then standard:
\begin{lemma}[Approximation Lemma for Evaluation Trees]\label{alet}
The  form as increasing sequence of
continuous functions with . Each  is idempotent and has finite
image.
\end{lemma}

\subsection{Main Results}
We are now equipped to address the relationship between strategies
and evaluation trees directly. Let .
We define a map  this map is a concrete version of
the semantic map defined in section 2.4. That is, we fix choices
of pairing functions etc. as in the definition of  in 2.5,
and define  as a specific
representative of the partial equivalence class . Thus we will have  We were sloppy about this
distinction in 2.4; we give the definition of 
explicitly for emphasis:

where


\begin{lemma}\label{lemmm1}
If  then 
\end{lemma}
\begin{proof} By induction on the construction of , using
--monotonicity  of .
\end{proof}

Let , and  be the set of
all contexts .
For each , we define a map

inductively by:




where

and


\begin{lemma}\label{lemmm2}
For all  :
\begin{itemize}
\item[(i)]  implies
   .
\item[(ii)] If  is an
  increasing sequence,

\item[(iii)] 
\item[(iv)] 
\end{itemize}
\end{lemma}
\begin{proof} {\it (i)} is proved similarly to part {\it (iii)} of
the Approximation Lemma for strategies; {\it (ii)} is proved
similarly to part {\it (ii)}; and {\it (iii)} to part {\it (vi)};
{\it (iv)} is proved by a routine induction on .
\end{proof}

\begin{lemma}\label{lemmm3}
For all 
\begin{itemize}
\item[(i)] 
\item[(ii)] 
\end{itemize}
\end{lemma}
\begin{proof} Both parts are proved by induction on . The
induction bases are trivial as are cases {\it (i)} and {\it (ii)}
of the Decomposition Lemma at the inductive step, and the
corresponding cases on the construction of 

{\it (i)}


where




{\it (ii)}

where



\end{proof}

Now we define functions



by:


By Lemma \ref{lemmm1} and the Approximation Lemma for evaluation trees,
 is an
increasing sequence of strategies, so  is
well-defined. Similarly, by Lemma \ref{lemmm2}  is well-defined.

We now prove the key result on definability.
\begin{theorem}[Isomorphism Theorem]\label{isotheo}

\begin{itemize}
\item[(i)] For all 

\item[(ii)] For all ,

\item[(iii)] Let . Then there is an
  order-isomorphism

where  (i.e. the partial
equivalence class of ), and .
\end{itemize}
\end{theorem}
\begin{proof}

{\it (i)}



{\it (ii)}



{\it (iii)} Firstly  is well-defined and monotone by
Lemma \ref{lemmm2}{\it (i)}. Also,  is monotone by
Lemma \ref{lemmm1}. By {\it (i)} and {\it (ii)}, 
\end{proof}

As an immediate corollary of the Isomorphism Theorem and Proposition
3.2.2:
\begin{proposition}
For each PCF type ,  is a dI-domain. Hence 
is an algebraic cpo-based model.
\end{proposition}
Thus although a priori we only knew that  was a rational
model, by virtue of the Isomorphism theorem we know that the carrier
at each PCF type is an algebraic cpo. Hence the notion of {\em
  intensional full abstraction} makes sense for . Recall
from the introduction that a model is intensionally fully abstract for a
language  if every compact element of the model is denoted by
a term of .

We can now prove the culminating result of this section.

\begin{theorem}[Intensional Full Abstraction]

 is intensionally fully abstract for .
\end{theorem}
\begin{proof} Consider any PCF type . By the Isomorphism Theorem,
the compact elements of  are the image under  of the compact elements of 
(where  is the empty context). But the compact elements
of  are just the finite evaluation trees
 and the restriction of  to  is the semantic map  on finite evaluation trees {\sl qua} terms of .
\end{proof}

\section{Extensional Full Abstraction}
\newcommand{\converges}{{\downarrow}}
\newcommand{\Converges}{{\Downarrow}}
\newcommand{\diverges}{{\uparrow}}
\newcommand{\Siepinski}{{\Sigma}}
\subsection{The Intrinsic Preorder}

We define the {\em Sierpinski game}  to be the game

with one initial question , and one possible response . Note
that   is indeed the usual Sierpinski space. i.e. the
two-point lattice  with .

Now for any game  we define the {\em intrinsic preorder}  on
 by:

Note that if we write  and , then:

It is trivially verified that  is a preorder.
\begin{lemma}[Point Decomposition Lemma]\label{PDL}

\begin{itemize}
\item[(i)] 
\item[(ii)] 
\item[(iii)]
\end{itemize}
\end{lemma}
\begin{proof} Firstly we must explain the notation. We think of a
strategy  in  indifferently as having the type . Now since , we can regard
 as in . Similarly, since
, we can regard  as in , where . Finally, using  again we can form
 where
.


Next we note that {\it (i)} is a special case of the Bang Lemma, while {\it (ii)}
follows from the universal property of the product.

Finally, we prove {\it (iii)}. Given , write
, where . By the
switching condition, we can decompose  as , where
, and . Now
define . It is clear that  and  are
well-defined strategies, and

\end{proof}

Now we characterise the intrinsic preorder on the Linear types. The
general theme is that ``intrinsic = pointwise''. This is analogous to
results in Synthetic Domain Theory and PER models, although the proofs
are quite different, and remarkably enough no additional hypotheses
are required.
\begin{lemma}[Extensionality for Tensor]\label{EfT}

For all 

\end{lemma}
\begin{proof} . If  and , then  where


\noindent . This implies that , and hence that . This shows
that ; the proof that  is similar.

. Suppose that  and  where . Then define  by:




Then , so
 since .
This shows that . A similar
argument shows that , and
so 
\end{proof}

\begin{lemma}[Extensionality for Product]\label{EfP}

For all 

\end{lemma}
\begin{proof} By the definition of , any  must factor as

or as

This shows the right-to-left implication. Conversely, given
 we can form

and similarly for .
\end{proof}

\begin{lemma}[Linear Function Extensionality]\label{LFE}

For all 

\end{lemma}
\begin{proof}  Suppose  and
. Then we define  by

For all , so
 since  and
.

 Suppose  where
. From the switching
condition we know that  can respond to the initial move in
 only in  or ; to a move in 
only in  or  and to a move in  only in  or
. Moreover, whenever Player is to move in  the
number of moves played in  is odd, hence there is an unanswered
question in  which must have been asked more recently than the
opening question in . By the stack discipline
 can in fact only respond in  to a move in .  Thus
if  where  we can decompose 
as  where: .
If we now define  then:
\begin{itemize}
\item[(i)] .
\item[(ii)] .
\item[(iii)] .
\end{itemize}
Now

as required.
\end{proof}

This argument can be understood in terms of Classical Linear Logic. If
we think of  as ``approximately '', then

To prove our final extensionality result, we will need an auxiliary
lemma.

\begin{lemma}[Separation of head occurrence]\label{Soho}

For all , for some :

\end{lemma}
\begin{proof}If  or
, the result is trivial. If  responds
to the initial question  with a move  in  we define
 by interpreting the index  as a separate tensorial
factor rather than an index in . The only non-trivial point is
to show that . If  where , then any
permutation  witnessing the equivalence must satisfy
. Let the response of  to  be 
and to   . Since  we must have
, and hence either  or . In either cases, , as required.
\end{proof}

\begin{lemma}[Bang Extensionality]\label{BE}

For all 

\end{lemma}
\begin{proof}  If  and
 then , so
, and hence
 as required.

 If , define  to be
the number of indices in  occurring in . We show
that, for all  such that
, by induction on
.  For the basis, note that  and 
implies that . For the inductive step, let
. By Lemma \ref{Soho}, for some ,
. For all
, so
.

Now define

For all . In
particular, .
Since , there is a first index  in  used by
. By the definition of ,  is
 with all moves at index  deleted. Hence
, and by induction hypothesis
.

Define  by

Then for all . In particular, . By the assumption that . This implies that , as required.
\end{proof}

\begin{lemma}[Intuitionistic Function Extensionality]

\end{lemma}

\begin{proof}

\end{proof}

\begin{lemma}[Congruence Lemma]
\begin{itemize}
\item[(i)] 
\item[(ii)] 
\item[(iii)] 
\end{itemize}
\end{lemma}

\begin{proof} {\it (i)}


{\it (ii)}
For all ;
and similarly,  By {\it (i)}, 
and . The result now follows by Product
Extensionality.

{\it (iii)} Identifying morphisms with points of arrow types,

\end{proof}

Finally we consider the relationship between the intrinsic
and intensional preorders.

\begin{lemma}\label{II}
\begin{itemize}
\item[(i)] If , then .
\item[(ii)] If  is an
  increasing sequence, and for all , , then
  
\end{itemize}
\end{lemma}
\begin{proof} {\it (i)} By monotonicity of composition
(Proposition 2.9.3) if  and
 then 
and hence .

{\it (ii)} By continuity of composition (Proposition
2.9.3), similarly to {\it (i)}.
\end{proof}

By Lemma \ref{II},  implies  where
 is the equivalence induced by the preorder . Thus each
equivalence class is a union of
classes. Henceforth, when we write  we shall mean
the equivalence class of .

We can define the notion of {\em strong chain} of equivalence
classes, just as we did for classes:
a sequence

such that there are  with
 and  for
all .

\begin{lemma}\label{Strong}
Every strong chain has a least upper bound.
\end{lemma}
\begin{proof} Given a strong chain (\dag), take
 where
. For all , so by Lemma
\ref{II}{\it(i)},  is un upper bound for
.

Finally, if  is another upper bound, then for all ,
; so by Lemma \ref{II}{\it (ii)},
.
\end{proof}


\subsection{The Extensional Category }

We begin with some general considerations on quotients of rational cartesian
closed categories. Let  be a rational CCC. A {\em precongruence}
on  is a family
 of
relations  satisfying
the following properties:
\begin{itemize}
\item[(r1)] each  is a preorder
\item[(r2)]  and  implies

\item[(r3)]  and  implies

\item[(r4)]  implies 
\item[(r5)] 
\item[(r6)] for all :

\end{itemize}

Given such a precongruence, we define a new category  as
follows. The objects are the same as those of ;

That is, a morphism in 
is a equivalence class , where  is the
equivalence relation induced by . The partial ordering is then
the induced one:

Note that by (r5),  is the least element with respect to this
partial order. By (r2)--(r4), composition, pairing and currying are well-defined
on equivalence classes by

 It is then immediate by (r5)
and the fact that  is a rational (and hence in particular a
ppo-enriched) CCC that  is a ppo-enriched CCC.
It remains to verify rationality for . By (r2) and (r5),
for any 
the sequence   is a -chain.
By (r5) and (r6),  is the least
upper bound of this chain. In particular~, taking  and
,  is the least upper bound of
.

We record this result, which is a variant of [ADJ76], as

\begin{lemma}[Rational Quotient]\label{5.1}
If  is a precongruence on a rational CCC , then 
is a rational CCC.
\end{lemma}

Now we define a family 

\begin{lemma}\label{5.2}
 is a precongruence on .
\end{lemma}
\begin{proof} The fact that  is a preorder has
already been noted. (r2)--(r4) are the pre-congruence Lemma 4.1.8.
(r5) is Lemma 4.1.9{\it (i)}. Finally, we verify (r6). Let
. As already explained, since , we work directly with classes of
strategies, rather that classes of classes of
strategies. Now  is a chain (using
monotonicity of composition), and we can apply Lemma
4.1.9{\it (ii)} to yeld (r6).
\end{proof}



Now we define .

\begin{proposition}\label{5.3}
 is a rational CCC. Moreover,  is well-pointed in the
order-enriched sense:

\end{proposition}
\begin{proof}  is a rational CCC by Lemma \ref{5.1} and
\ref{5.2}. It is well-pointed by Intuitionistic Function
Extensionality (Lemma 4.1.7).
\end{proof}



Now we define the PCF model  with the same interpretation
of  as in . The ground and first-order
constants of PCF are interpreted by the equivalence
classes of their interpretations in .

\begin{proposition}\label{5.4}
 is an order-extensional standard model of PCF.
\end{proposition}
\begin{proof}  is an order-extensional model of PCF by
Proposition 4.2.3. It is standard because 
is, and  .
\end{proof}

\subsection{An alternative view of }
We now briefly sketch another way of looking
at , which brings out its extensional character more clearly,
although technically it is no more than a presentational variant of the
above description.
Given a game , define

Then  is a pointed poset. Given , define
 as the (monotone) function defined by:

Write  if  is a monotone
function such that  for some . In this
case we say that  is {\em sequentially realised} by , and write
.

Note that there are order-isomorphisms

\begin{itemize}
\item 
\item 
\item 
\end{itemize}

Here  is the cartesian product of the posets ,
with the pointwise order; while  is the set of
all functions , again with the pointwise order.

Now note that, with respect to the representations of  as a
cartesian product and  as a ``function space'', the
interpretations of composition, pairing and projections, and currying and
application in  are the usual set-theoretic operations on functions
{\em in extenso}. That is,

\noindent where the operations on the right hand sides are defined as
in the category
of sets (or any concrete category of domains).

Thus an equivalent definition of  is as follows:


The r\^ole of the intensional structure, that is of the use of the game 
to represent the abstract space , is to cut down the function spaces
to the sequentially realisable functions. Specifically, note the use of 
and  in the definition of .

\subsection{Full Abstraction}

We recall that a model  is fully abstract for a language  if,
for all types  and closed terms 

where

Here a program context  is one for which  is a closed term of type
  for any closed term ; and  is the operational
convergence relation. The left---to---right implication in  is known as
{\em soundness} and the converse as {\em completeness}.
It is standard that soundness is a consequence of computational
adequacy \cite{CurienPL:catcsa};
thus by Proposition 2.10.1, standard models are sound. Also,
full abstraction for closed terms is easily seen to imply the corresponding
statement  for open terms.

\begin{theorem}\label{1}
 is fully abstract for PCF.
\end{theorem}
\begin{proof} Firstly,  is a standard model by
Proposition~\ref{5.4}, and hence sound. We shall prove the
contrapositive of completeness. Suppose  are closed terms of
PCF of type  and
 Let . By
Intuitionistic Function Extensionality, for some   By
monotonicity of composition, this implies that
, and hence that  for some , and . By continuity of
composition and the properties of the projections  given in
the Approximation Lemma 3.5.1, for some , , while by
monotonicity of composition, . By Lemma 3.6.3, there are
finite evaluation trees,and hence PCFc terms  such
that , . This
means that , while
. By computational
adequacy, this implies that  and
. By Lemma 3.1.1, each PCFc term
is observationally congruent to a PCF term. Hence there is a PCF
context , where ,
, such that  and
. This implies that , as required.
\end{proof}

As an instructive consequence of this proof, we have:

\begin{corollary}[Context Lemma]
For all closed ,

\end{corollary}
\begin{proof} The left-to-right implication is obvious, by
considering applicative contexts . The converse
follows from the proof of the Full Abstraction Theorem, since if
, then  by soundness, and then by the argument for
completeness this can be translated back into an applicative
context separating  and .
\end{proof}

The point of reproving this well-known result is that a semantic proof
falls out of the Full Abstraction Theorem. By contrast, Milner had to prove
the Context Lemma directly, as a necessary preliminary to his syntactic
construction of the fully abstract model. Moreover, the direct syntactic
proof, particularly for the calculus formulation of PCF
\cite{CurienPL:catcsa}, is quite subtle. This gives some immediate
evidence of substance in our ``semantic analysis''.




\section{Universality}

The definability result we have achieved so far refers only to compact
strategies. Our aim in this section is to characterize precisely which
strategies are (extensionally) definable in PCF, and in fact to
construct a fully abstract model in which {\em all} strategies are
definable.

\subsection{ Recursive Strategies}

We shall develop effective versions of  and . Our treatment will be
very sketchy, as the details are lengthy and tedious, but quite routine.
We refer to standard texts such as \cite{SoareRI:recesd} for background.

We say that a game  is {\em effectively given} if there is a surjective map
 with respect to which  (with some
coding of ) and the characteristic functions of  and
 (with some coding of finite sequences) are tracked by recursive
functions. A strategy  on  is then said to be {\em recursive}
if  is a recursively enumerable subset of 
(strictly speaking, if the set of codes of positions in  is r.e.).

\begin{lemma}\label{uni.1}
 is recursive iff  is tracked by a partial recursive
function. There are recursive functions taking an index for  to
one for , and vice versa.
\end{lemma}
\begin{proof} The predicate  is clearly r.e. in , hence  has an r.e.
graph and is partial recursive

Conversely, given  define  a predicate  by:


Clearly  is r.e. and hence so is  These constructions are defined via
simple syntactic transformations and yield effective operations on
indices. \end{proof}

If  and  are effectively given, one can verify that the effective
structure lifts to ,  and . Also,
 and  are evidently effectively given. The most interesting point
which arises in verifying these assertions is that  is
recursive. This requires the observation that, in checking ,
it suffices to consider permutations  of {\em bounded}
(finite) support, where the bound is easily computed from  and .

Similarly, one can check that all operations on strategies defined in
Section 2 effectivize. For example, it is easily seen that the definition of
 in terms of sets of positions is r.e. in  and ;
or, we can give an algorithm for computing . This algorithm
simply consists of applying  and  alternately starting from
whichever is applicable to the input, until an ``externally visible'' output
appears. Note that it is {\sl not} the case in general that unions of
chains of recursive strategies are recursive. For example every
strategy of type  is a union of an increasing chain of finite
and hence recursive strategies. However, given a recursive
,  is recursive, since it can be enumerated uniformly effectively
in  (``r.e. unions of r.e. sets are r.e.'').

Thus we can define a category  with objects effectively  given
games, and morphisms (partial equivalence classes of ) recursive strategies.
Also, the interpretations of PCF constants in  are clearly
recursive strategies.

\begin{proposition}
\begin{itemize}
\item[(i)]  is a Linear category
\item[(ii)]  is a rational cartesian closed category
\item[(iii)]  is a standard model of PCF
\end{itemize}
\end{proposition}

We can now consider the extensional quotient  where  is defined just
as for , but of course with respect to recursive tests,
i.e. recursive strategies . All the results
of section 4 go through with respect to recursive tests.

\begin{proposition}
 is a well-pointed rational CCC.  is a
fully abstract model of PCF.
\end{proposition}
\begin{proof} The result does require a little care, since the
Isomorphism Theorem 3.6.4 is not valid for .
However, the Isomorphism Theorem was not used in the proof of the
Full Abstraction Theorem 4.3.1, but rather the finitary version
Lemma 3.6.3, which is valid in .
\end{proof}

It is worth remarking that a nice feature of our definition of model in
terms of rationality rather than cpo-enrichment is that the recursive
version  is again a model in exactly the same sense as . By
contrast, in the cpo-enriched setting one must either modify the definition
of model explicitly (by only requiring completeness with respect to
r.e. chains), or implicitly by working inside some recursive realizability
universe.

\subsection{ Universal Terms }

The fact that  and  are models
shows that all PCF terms denote recursive strategies, as we would expect.
Our aim now is to prove a converse; every recursive strategy is, up to
extensional equivalence, the denotation of a PCF term, and hence every
functional in the extensional model  is definable in PCF.

More precisely our aim is to define, for each PCF type , a
``universal term'' , such that


for each recursive . These universal terms will
work by simulating the evaluation tree corresponding to .

Firstly, we recall some notations from recursion theory. We fix an acceptable
numbering of the partial recursive functions \cite{SoareRI:recesd}
such that  is the 'th
partial recursive function and  is the 'th r.e. set. We also fix
a recursive pairing function

and a recursive coding of finite sequences.

A recursive strategy  is regarded as being given by a code
(natural number) . By virtue of Lemma 5.1.1
we use such a code indifferently as determining  by
 or
 The
following lemma is a recursive refinement of the Decomposition
Lemma, and assumes the notations of Section 3.4.


\newcommand{\casodue}[5] {  }

\newcommand{\casotre}[7] {  }



\begin{lemma}[Decomposition Lemma (Recursive Version)]
For each PCF type  there are partial recursive functions

such that, if  is a recursive strategy on 

\casotre{D_T\lceil\sigma\rceil}{\mbox{undefined,}}{\sigma=\bot_{\tilde{T}}}
{\lang 2,n\rang ,}{\sigma=K_{\tilde{T}}n}{\lang 3,i\rang ,}{R(\sigma)}
\casodue{H_T\lceil\sigma\rceil}{\lang \lceil\sigma_1\rceil,\dots,\lceil\sigma_{l_i}
\rceil\rang ,}{R(\sigma)}{\mbox{undefined},}{\mbox{otherwise}}
\casodue{B_T(\lceil\sigma\rceil,n)}{\lceil\tau_n\rceil,}{R(\sigma)}
{\mbox{undefined},}{\mbox{otherwise}}
where  stands for

\end{lemma}
\begin{proof}  is computed by applying
 to the (code of) the initial
question. The extraction of  from ,
, is obviously r.e. in
, uniformly effectively in . Hence we obtain an r.e.
predicate , and by an application
of the \textsf{S-m-n} theorem we obtain the index for
``''.

Similarly the extraction of  from  is r.e. in
, and that of  for  is r.e. in
; while  are obtained from
 by composition, dereliction and projection, which are
computable operations by Proposition 5.1.2. Hence applying the
\textsf{S-m-n} theorem again we obtain the codes for .
\end{proof}

Given a PCF type , we define the subtypes of  to be the PCF
types occurring as subformulas of , e.g.  and
 are subtypes of . Let
 be a listing of all the (finitely many) subtypes
of , where we write  To aid the presentation, we will use an
abstract datatype  of ``-contexts'', which we
will show later how to implement in PCF. We will make essential
use of the fact that, while contexts can grow to arbritary size in
the recursive unfolding of an evaluation tree of type , the
types occurring in the context can only be subtypes of .

 comes with the following operations:
\begin{itemize}
\item 
\item  for each subtype
 of 
\item  for each subtype  of 
\item .
\end{itemize}

If , then  is the subsequence of all
entries of type ,  and 

The idea is that, if  is an ``abstract context'',
\begin{itemize}
\item 
\item  where 
\item .
\end{itemize}

Now we use the standard fact that every partial recursive function
 can be represented by a closed PCF term
 in the sense that, for all 

This obviously implies that partial recursive functions of two arguments can
be represented by closed terms of type .
Specifically, we fix terms  and
 which represent  and 
respectively.

Now we define a family of functions  for each subtype  of , by the following mutual recursion:



These functions have been defined using some ``syntactic sugar''.
Standard techniques can be used to transform these definitions
into PCF syntax. In particular Beki\u{c}'s rule
\cite{WinskelG:fspl} can be used to transform a finite system of
simultaneous recursion equations into iterated applications of the
 combinator. The universal term  can then be defined
by 

It remains to be shown how  can be implemented in PCF. To do
this, we assume two lower-level data-type abstractions, namely product
types  with pairing and projections, and list types 
for each PCF type , with the usual operations:
\begin{itemize}
\item 
\item 
\item 
\item 
\item 
\end{itemize}

We write  for the 'th component of a list.

We represent an abstract context  by the tuple
 where  and
. The idea is that , while


It is straightforward to implement the operations on contexts in terms of this representation.

\begin{itemize}
\item 
\item 
\item 
\item 

where

and  equals

where  is list concatenation.
\end{itemize}
Finally, we show how to represent lists and products in PCF. We represent
lists by

where e.g.
\begin{itemize}
\item 

\item 
\end{itemize}
A function taking an argument of product type

can be replaced by its curried version

while a function returning a product type can be replaced by the two
component functions.

This completes our description of the universal term .

\newcommand{\R}{{\; \cal R\; }}
For each PCF type , we define a relation  between
closed PCF terms of type  and strategies  by

This is extended to sequences  in
the evident fashion.

We fix a type  with subtypes  as in the previous
discussion.

\begin{lemma}\label{522}
Let  be a PCF type-in-context and
 a compact strategy, where
 are subtypes of . Let  be a closed expression
of type  (which we will regard as a sequence of
closed terms), and  a sequence of strategies.
Then

\end{lemma}

\begin{proof} By induction on the height of the finite evaluation
tree corresponding to  under Theorem \ref{isotheo} , and
by cases on the Decomposition Lemma for . The cases for
 and
 are
clear.

Suppose 
By Intuitionistic Function Extensionality Lemma, it suffices to show
that, for all closed  and
strategies  such that 

Let . Then
,
so by induction hypothesis,

Hence if we define

where , then  Thus if
, then
, while if  then .

In the former case, 
In the latter case,

while , and by induction
hypothesis 
\end{proof}

Now we define a family of relations , where
, inductively as follows:


We can read  as: the stategy coded by  simulates the
strategy coded by  for all behaviours of size .

We write 

\begin{lemma}\label{due}
For all PCF types , 
\begin{itemize}
\item[(i)] .
\item[(ii)] 
\end{itemize}
\end{lemma}

\begin{lemma}\label{tre}
With  as in Lemma \ref{522}, and  any strategy
in :

\end{lemma}

\begin{proof}  By Lemma \ref{due}{\it (i)}.

 By Lemma \ref{due}{\it (ii)}, using continuity,
and hence the fact that only finitely many calls to  and 
are made in evaluating .
(This can be made precise using Berry's Syntactic Approximation
Lemma for PCF \cite{BerryG:fulasl}).
\end{proof}



\begin{theorem}[Universality Theorem]
For all PCF types  and recursive strategies  with
,

Thus every functional in  (equivalently, every functional
in  realised by a recursive strategy) is definable in PCF.
\end{theorem}

\begin{proof} For all closed .


\noindent By the Intuitionistic Function Extensionality Lemma this
shows that 
\end{proof}


In the case of cpo-enriched models, an important result due to Milner
is that the fully-abstract order extensional model is unique up to isomorphism.
For rational models, the situation is not quite so rigid. For example, both
 and  are fully abstract, but  is
properly contained in . To see this, note that all monotonic functions
of type  are sequentially realised and hence live in
, while only the recursive ones live in .
We can, however, give a very satisfactory account of the canonicity of
. We define a category  with objects
the fully abstract (rational) models of PCF.
A homomorphism  is a functor
from the full cartesian closed sub category of  generated by
the interpretation
of  in  to the corresponding subcategory of .
 is additionally required to be a rational CCC functor, and to preserve the
interpretation of  and of the PCF ground and first-order constants.

\begin{theorem}[Extensional Initiality Theorem]


 is initial in .
\end{theorem}
\newcommand{\N}{{\cal N}}
\begin{proof} Let  be any fully abstract model. By the
Universality Theorem, there is only one possible definition of
, given by  for
all closed terms  of PCF. Since  and 
are both fully abstract,

so this map is well-defined, and preserves and reflects order. It
is a homomorphism by the compositional definition of the semantic
function.
\end{proof}






\begin{thebibliography}{Cur92b}

\bibitem[AP90]{AP90}
M. Abadi and G. Plotkin.
\newblock A {\sc PER} model of polymorphism and recursive types.
\newblock  In {\em Proceedings, Fifth Annual IEEE Symposium on Logic in Computer
  Science}, IEEE Computer Society Press, 1990.

\bibitem[Abr94]{AbramskyS:prop}
Samson Abramsky.
\newblock Proofs as processes.
\newblock {\em Theoretical Computer Science}, 135:5--9, 1994.

\bibitem[AJ93]{AbramskyS:gamexp}
S.~Abramsky and R.~Jagadeesan.
\newblock Game semantics for exponentials.
\newblock Announcement on the types mailing list, 1993.

\bibitem[AJ94a]{AbramskyS:gamfcm}
Samson Abramsky and Radha Jagadeesan.
\newblock Games and full completeness for multiplicative linear logic.
\newblock {\em Journal of Symbolic Logic}, 59(2):543 -- 574, June 1994.
\newblock Also appeared as Technical Report 92/24 of the Department of
  Computing, Imperial College of Science, Technology and Medicine.

\bibitem[AJ94b]{AbramskyS:domt}
Samson Abramsky and Achim Jung.
\newblock Domain theory.
\newblock In S.~Abramsky, D.~Gabbay, and T.~S.~E. Maibaum, editors, {\em
  Handbook of Logic in Computer Science Volume 3}, pages 1--168. Oxford
  University Press, 1994.

\bibitem[AJM94]{AbramskyS:fulap}
Samson Abramsky, Radha Jagadeesan, and Pasquale Malacaria.
\newblock Full abstraction for {PCF} (extended abstract).
\newblock In Masami Hagiya and John~C. Mitchell, editors, {\em Theoretical
  Aspects of Computer Software. International Symposium TACS'94}, number 789 in
  Lecture Notes in Computer Science, pages 1--15, Sendai, Japan, April 1994.
  Springer-Verlag.

\bibitem[AM95]{McCuskerGA:gamfal}
Samson Abramsky and Guy McCusker.
\newblock Games and full abstraction for the lazy -calculus.
\newblock In {\em Proceedings, Tenth Annual IEEE Symposium on Logic in Computer
  Science}, pages 234--243. IEEE Computer Society Press, 1995.

\bibitem[AM99]{AM97}
Samson Abramsky and Guy McCusker.
\newblock Game Semantics.
\newblock In Ulrich Berger and Helmut Schwichtenberg, editors,
{\em Computational Logic}, pages 1--56, Springer-Verlag 1999.

\bibitem[Abr97]{Abr97}
Samson Abramsky.
\newblock Games in the Semantics of Programming Languages.
\newblock In {\em Proceedings of the 1997 Amsterdam Colloquium}.

\bibitem[Abr00]{Abr00}
Samson Abramsky.
\newblock Axioms for Definability and Full Completeness.
\newblock In G. Plotkin, C. Stirling and M. Tofte, editors,
{\em Proof, Language and Interaction: Essays in honour of
Robin Milner}, pages 55--75, MIT Press 2000.

\bibitem[Bar84]{BarendregtHP:lamcss}
Henk~P. Barendregt.
\newblock {\em The Lambda Calculus: Its Syntax and Semantics}.
\newblock North-Holland, revised edition, 1984.

\bibitem[Ber79]{BerryG:modcom}
G{\'e}rard Berry.
\newblock {\em Modeles completement adequats et stable de lambda-calculs}.
\newblock PhD thesis, Universite Paris VII, 1979.

\bibitem[BC82]{BerryG:seqacd}
G{\'e}rard Berry and P.-L. Curien.
\newblock Sequential algorithms on concrete data structures.
\newblock {\em Theoretical Computer Science}, 20:265--321, 1982.

\bibitem[BCL85]{BerryG:fulasl}
G{\'e}rard Berry, P.-L. Curien, and Jean-Jacques L\'evy.
\newblock Full abstraction for sequential languages: the state of the art.
\newblock In M.~Nivat and John Reynolds, editors, {\em Algebraic Semantics},
  pages 89--132. Cambridge University Press, 1985.

\bibitem[BE91]{EhrhardT:extemb}
A.~Bucciarelli and T.~Ehrhard.
\newblock Extensional embedding of a strongly stable model of {PCF}.
\newblock In {\em Proc. ICALP}, number 510 in LNCS, pages 35--46. Springer,
  1991.

\bibitem[CF92]{FelleisenM:obsseq}
R.~Cartwright and M.~Felleisen.
\newblock Observable sequentiality and full abstraction.
\newblock In {\em Proc. POPL}, pages 328--342. ACM {P}ress, 1992.

\bibitem[Cro94]{CroleRL:catt}
Roy Crole.
\newblock {\em Categories for Types}.
\newblock Cambridge University Press, 1994.

\bibitem[Cur92a]{CurienPL:obssac}
Pierre-Louis Curien.
\newblock Observable sequential algorithms on concrete data structures.
\newblock In {\em Proceedings, {S}eventh {A}nnual {IEEE} {S}ymposium on {L}ogic
  in {C}omputer {S}cience}, pages 432--443. IEEE {C}omputer {S}cience {P}ress,
  1992.

\bibitem[Cur92b]{CurienPL:seqful}
Pierre-Louis Curien.
\newblock Sequentiality and full abstraction.
\newblock In P.~T.~Johnstone M.~Fourman and A.~M. Pitts, editors, {\em
  Applications of Categories in Computer Science}, pages 66--94. Cambridge
  University Press, 1992.

\bibitem[Cur93]{CurienPL:catcsa}
Pierre-Louis Curien.
\newblock {\em Categorical Combinators, Sequential Algorithms and
  Fun\-ctio\-nal Programming}.
\newblock Progress in Theoretical Computer Science. Birkhauser, 1993.

\bibitem[DP90]{DaveyBA:Intlor}
B.~A. Davey and H.~A. Priestley.
\newblock {\em Introduction to Lattices and Order}.
\newblock Cambridge University Press, 1990.

\bibitem[Ehr]{EhrhardT:prosas}
Thomas Ehrhard.
\newblock Projecting sequential algorithms on the strongly stable functions.
\newblock {\em {A}nnals of {P}ure and {A}pplied {L}ogic}
77(3):201--244, 1996.

\bibitem[Gan93]{GandyRO:diabso}
R.~O. Gandy.
\newblock Dialogues, {B}lass games and sequentiality for objects of finite
  type.
\newblock Unpublished manuscript, 1993.

\bibitem[GM00]{GM00}
Dan R. Ghica and Guy McCusker.
\newblock Reasoning about Idealized Algol Using Regular Languages.
\newblock In {\em Proceedings of ICALP 2000}, Springer Lecture Notes in
Computer Science, 2000.

\bibitem[Gir88]{GirardJY:geoi2d}
Jean-Yves Girard.
\newblock Geometry of interaction 2: Deadlock-free algorithms.
\newblock In P.~Martin-L\"{o}f and G.~Mints, editors, {\em International
  Conference on Computer Logic, {COLOG} 88}, pages 76--93. Springer-Verlag,
  1988.
\newblock Lecture Notes in Computer Science 417.

\bibitem[Gir89a]{GirardJY:geoi1i}
Jean-Yves Girard.
\newblock Geometry of interaction 1: Interpretation of {S}ystem {F}.
\newblock In R.~Ferro et~al., editor, {\em Logic Colloquium 88}, pages
  221--260. North Holland, 1989.

\bibitem[Gir89b]{GirardJY:towgi}
Jean-Yves Girard.
\newblock Towards a geometry of interaction.
\newblock In J.~W. Gray and Andre Scedrov, editors, {\em Categories in Computer
  Science and Logic}, volume~92 of {\em Contemporary Mathematics}, pages
  69--108. American Mathematical Society, 1989.

\bibitem[Hoa85]{HoareCAR:comsp}
C.~A.~R. Hoare.
\newblock {\em Communicating Sequential Processes}.
\newblock Prentice Hall, 1985.

\bibitem[JS93]{JungA:stufam}
Achim Jung and Allen Stoughton.
\newblock Studying the fully abstract model of {PCF} within its continous
  function model.
\newblock In {\em Proc. Int. Conf. Typed Lambda Calculi and Applications},
  pages 230--245, Berlin, 1993. Springer-verlag.
\newblock Lecture Notes in Computer Science Vol. 664.

\bibitem[Loa96]{Loa96}
Ralph Loader.
\newblock Finitary PCF is undecidable.
\newblock {\em Theoretical Computer Science}, to appear, 2000.

\bibitem[Lor60]{LorenzenP:logua}
P.~Lorenzen.
\newblock Logik und agon.
\newblock In {\em Atti del Congresso Internazionale di Filosofia}, pages
  187--194, Firenze, 1960. Sansoni.

\bibitem[Lor61]{LorenzenP:eindk}
P.~Lorenzen.
\newblock Ein dialogisches {K}onstruktivit\"{a}tskriterium.
\newblock In {\em Infinitistic Methods}, pages 193--200, Warszawa, 1961. PWN.
\newblock Proceed. Symp. Foundations of Math.

\bibitem[Mal93]{MalacariaP:frogi}
Pasquale Malacaria.
\newblock Dalle macchine a ambienti alla geometria dell'interazione.
\newblock Unpublished manuscript, 1993.

\bibitem[MH99]{MH99}
Pasquale Malacaria and Chris Hankin.
Non-Deterministic Games and Program Analysis: an application to security.
In {\em Proceedings of LiCS `99}, pages 443--453, 1999.

\bibitem[Man76]{ManesE:algt}
E.~Manes.
\newblock {\em Algebraic Theories}, volume~26 of {\em Graduate Texts in
  Mathematics}.
\newblock Springer-Verlag, 1976.

\bibitem[Mil77]{MilnerR:fulamt}
Robin Milner.
\newblock Fully abstract models of typed lambda-calculi.
\newblock {\em Theoretical Computer Science}, 4:1--22, 1977.

\bibitem[Mul87]{MulmuleyK:fulase}
K.~Mulmuley.
\newblock {\em Full Abstraction and Semantic Equivalence}.
\newblock MIT Press, 1987.

\bibitem[Nic94]{NickauH:hersf}
H.~Nickau.
\newblock Hereditarily sequential functionals.
\newblock In {\em Proceedings of the Symposium on Logical Foundations of
  Computer Science: Logic at St.\ Petersburg}, Lecture notes in Computer
  Science. Springer, 1994.

\bibitem[OR95]{OHearnPW:krilrp}
Peter~W. O'Hearn and Jon~G. Riecke.
\newblock Kripke logical relations and {PCF}.
\newblock {\em Information and Computation}, 120(1):107--116, 1995.

\bibitem[Plo77]{PlotkinGD:lcfcpl}
Gordon Plotkin.
\newblock {LCF} considered as a programming language.
\newblock {\em Theoretical Computer Science}, 5:223--255, 1977.

\bibitem[Soa87]{SoareRI:recesd}
R.~I. Soare.
\newblock {\em Recursively Enumerable Sets and Degrees}.
\newblock Perspectives in Mathematical Logic. Springer-Verlag, Berlin, 1987.

\bibitem[Sto88]{StoughtonA:fulamp}
A.~Stoughton.
\newblock {\em Fully abstract models of programming languages}.
\newblock Pitman, 1988.

\bibitem[Win93]{WinskelG:fspl}
G.~Winskel.
\newblock {\em The Formal Semantics of Programming Languages}.
\newblock Foundations of Computing. The MIT Press, Cambridge, Massachusetts,
  1993.
\end{thebibliography}



\end{document}
