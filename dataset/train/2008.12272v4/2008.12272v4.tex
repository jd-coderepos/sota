\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[accsupp]{axessibility}  \usepackage{packages/iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage[table,dvipsnames]{xcolor}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\iccvfinalcopy 

\def\iccvPaperID{1355} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}
\title{Monocular, One-stage, Regression of Multiple 3D People\vspace{-8mm}}


\author{Yu Sun\thanks{This work was done when Yu Sun was an intern at JD AI Research. }\quad
Qian Bao \quad
Wu Liu\thanks{Corresponding author.} \quad
Yili Fu\quad
Michael J. Black \quad
Tao Mei\\\vspace{-2.5mm}
Harbin Institute of Technology \quad JD AI Research \\
Max Planck Institute for Intelligent Systems, T\"ubingen, Germany\\
{\tt\small \texttt{yusun@stu.hit.edu.cn, baoqian@jd.com, liuwu@live.cn, meylfu@hit.edu.cn}}\\\vspace{-1mm}
{\tt\small\texttt{black@tuebingen.mpg.de, tmei@live.com}}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code\footnote{\url{https://github.com/Arthur151/ROMP}} is the first real-time implementation of monocular multi-person 3D mesh regression.

\end{abstract}
\vspace{-3mm}
\section{Introduction}~\label{sec:intro}
\vspace{-5mm}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{figs/Figure1.png} \caption{
    Given a challenging multi-person image like (a), the state-of-the-art approaches, e.g., VIBE~\cite{kocabas2020vibe} (left), fail to deal with truncation, scene occlusion, and person-person occlusion. 
 The reason lies in the multi-stage design (b), where the bounding-box-level features are often implicit, ambiguous, and inseparable in multi-person cases.
 We propose to regress all meshes in one single stage for multiple 3D people.
Specifically, we develop an explicit pixel-level representation (c) for fine-grained one-stage estimation that increases robustness to truncation and occlusion while significantly reducing computational complexity.
}
	\label{fig:motivation}
\end{figure}

Recently, great progress has been made in monocular 3D human pose and shape estimation, particularly in images with a single person~\cite{hmr,kocabas2020vibe,kolotouros2019spin,surreal,Xu_2019_ICCV}.
However, for more general scenes with multiple people, it is crucial to deal with truncation by the image frame, person-person occlusion, and environmental occlusion. 
Robustness to such occlusions is critical for real-world applications.

Existing approaches~\cite{jiang2020coherent,kocabas2020vibe,zanfir2018monocular,zanfir2018deep} follow a multi-stage design that equips the single-person method with a 2D person detector to handle multi-person scenes.
Generally, they first detect regions with people and then extract the bounding-box-level features, which are used to regress each single 3D human mesh \cite{Guler_2019_CVPR,hmr,kanazawa2019learning,kocabas2020vibe,kolotouros2019spin,gcmr,pavlakos2019texturepose,sun2019dsd-satn,Xu_2019_ICCV,Zhu_2019_CVPR}. 
However, as shown in Fig.~\ref{fig:motivation}, this strategy is prone to fail in cases of multi-person occlusion and truncation. 
Specifically, as shown in Fig.~\ref{fig:motivation}(b), when two people overlap, it is hard for the multi-stage method to estimate diverse body meshes from similar image patches.
The ambiguity of the implicit bounding-box-level representation results in failure for such inseparable multi-person cases. 


For multi-person 2D pose estimation, this problem has been tackled via a subtle and effective bottom-up framework. 
The paradigm first detects all body joints and then assigns them to different people by grouping body joints. 
This pixel-level body-joint representation enables their impressive performance in crowded scenes~\cite{openpose,cheng2020bottom,pishchulin2016deepcut}. 
However, it is non-trivial to extend this bottom-up one-stage process beyond joints~\cite{jiang2020coherent}.
Unlike 2D pose estimation, which predicts dozens of body joints, we need to regress a human body mesh with thousands of vertices, making it hard to follow the paradigm of body joint detection and grouping.


Instead, we introduce ROMP, a one-stage network for regressing multiple 3D people in a per-pixel prediction fashion.
It directly estimates multiple differentiable maps from the whole image, from which we can easily parse out the 3D meshes of all people.
Specifically, as shown in Fig.~\ref{fig:motivation}(c), ROMP predicts a Body Center heatmap and a Mesh Parameter map, representing the 2D position of the body center and the parameter vectors of the corresponding 3D body mesh, respectively.
Via a simple parameter sampling process, we extract 3D body mesh parameter vectors of all people from the Mesh Parameter map at the body center locations described by the heatmap. 
Then we put the sampled mesh parameter vectors into the SMPL body model~\cite{SMPL} to derive multi-person 3D meshes.

Following the guidance of body centers during training, the ambiguity of the regression target is greatly alleviated in crowded multi-person scenes. 
Additionally, in contrast to the local, bounding-box-level, features learned by traditional methods, end-to-end learning from the whole image forces the model to learn appropriate features from the holistic scene to predict bodies with occlusion.
This holistic approach captures the complexity of real-world scenes, enabling the generalization and robustness to complex multi-person cases.

Moreover, since the body centers of severely overlapping people may collide at the same 2D position, we further develop an advanced collision-aware representation (CAR). 
The key idea is to construct a repulsion field of body centers, where close body centers are analogous to positive charges that are pushed apart by mutual repulsion. In this way, the body centers of the overlapping people are more distinguishable. Especially in the case of severe overlap, most part of the human body is invisible. Mutual repulsion will push the center to the visible body area, meaning that the model tends to sample 3D mesh parameters estimated from the position centered on the visible body parts. It improves the robustness under heavy person-person occlusion. 

Compared with previous state-of-the-art methods for multi-person~\cite{jiang2020coherent,zanfir2018monocular,zanfir2018deep} and single-person~\cite{kocabas2020vibe,kolotouros2019spin} 3D mesh regression, ROMP 
achieves superior performance on challenging benchmarks, including 3DPW~\cite{3dpw} and CMU Panoptic~\cite{cmu_panoptic}. 
Experiments on person-person occlusion datasets (Crowdpose~\cite{crowdpose} and 3DPW-PC, a person-occluded subset of 3DPW~\cite{3dpw}) demonstrate the effectiveness of the proposed CAR under person-person occlusion.  
To further evaluate it in general cases, we test ROMP on images from the Internet and web camera videos. 
With the same backbone as the multi-stage counterparts, ROMP runs in real-time (over 30 FPS) on a 1070Ti GPU.

In summary, the contributions are:
(1) To the best of our knowledge, ROMP is the first one-stage method for monocular multi-person 3D mesh regression, along with an open-source real-time implementation. Its simple yet effective framework leads to superior accuracy and efficiency.
(2) The proposed explicit body-center-guided representation facilitates the pixel-level human mesh regression in an end-to-end manner.
(3) We develop a collision-aware representation to deal with cases of severe overlap.

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.00\textwidth]{figs/framework_new.png}
	\caption{An overview of ROMP. Given an input image, ROMP predicts multiple maps: 1) the Body Center heatmap  predicts the probability of each position being a body center, 2) the Camera map and 3) SMPL map  contain the camera and SMPL parameters~\cite{SMPL} of the person at each center, respectively. As the concatenation of the Camera map and SMPL map, the Mesh Parameter map contains the information of the predicted 3D body mesh and its location. Via the designed parameter sampling process, we can obtain the final 3D mesh results by parsing the Body Center heatmap and sampling the Mesh Parameter map.}
	\label{fig:framework}
\end{figure*}

\section{Related Work}

\textbf{Single-person 3D mesh regression.} 
Parametric human body models, e.g., SMPL~\cite{SMPL}, have been widely adopted to encode the complex 3D human mesh into a low-dimensional parameter vector, which can be regressed from images~\cite{liu2021recent}.
Impressive performance has been achieved using various weak supervision signals, such as 2D pose~\cite{choi2020pose2mesh,hmr,kundu2020appearance}, semantic segmentation~\cite{Xu_2019_ICCV}, 
geometric priors and discriminative training~\cite{hmr}, 
motion dynamics~\cite{kanazawa2019learning}, temporal coherence~\cite{kocabas2020vibe,sun2019dsd-satn}, texture consistency~\cite{pavlakos2019texturepose}, optimization~\cite{keep} in the loop~\cite{kolotouros2019spin}, etc.
However, all these methods adopt a bounding-box-level representation, which is implicit and ambiguous for multi-person/occlusion cases.
To handle occluding objects, Zhang et al.~\cite{zhang2020object} use a 2D UV map to represent a 3D human mesh. 
Considering the object-occluded body parts as blank areas in the partial UV map, they in-paint the partial UV map to make up the occluded information.
However, in the case of person-person occlusion where one person's body parts are occluded by those of another, it is hard to generate the partial UV map.

\textbf{Multi-person 3D pose estimation}. 
Mainstream methods can be divided into two categories: the one-stage and the multi-stage.
Many multi-stage methods follow the top-down design of Faster R-CNN~\cite{ren2015fasterrcnn}, such as LCR-Net++~\cite{rogez2019lcr} and 3DMPPE~\cite{moon2019camera}. 
From anchor-based feature proposals, they estimate the target via regression.
Other works explore a one-stage solution that reasons about all people in a single forward pass. They estimate all the body joint positions and then group them into individuals.
Mehta et al.~\cite{mehta2018single} propose  occlusion-robust pose-maps and exploit the body part association to avoid the bounding box prediction.
PandaNet~\cite{PandaNet_Benzine_2020_CVPR} is an anchor-based one-stage model that relies on a huge number of pre-defined anchor predictions and the positive anchor selection. 
To handle person-person occlusion, Zhen et al.~\cite{zhen2020smap} extend part affinity fields \cite{openpose} to be depth-aware.
ROMP extends the end-to-end one-stage process beyond joints through a concise body-center-guided pixel-level representation and does not require the part association or an enormous number of anchor predictions.

\textbf{Multi-person 3D mesh regression.} 
There are only a few approaches for multi-person 3D mesh regression. 
Zanfir et al.~\cite{zanfir2018deep} estimate the 3D mesh of each person from its intermediate 3D pose estimation.
Zanfir et al.~\cite{zanfir2018monocular} further employ multiple scene constraints to optimize the multi-person 3D mesh results. 
Jiang et al.~\cite{jiang2020coherent} propose a network for Coherent Reconstruction of Multiple Humans (CRMH).
Built on Faster-RCNN~\cite{ren2015fasterrcnn}, they use the RoI-aligned feature of each person to predict the SMPL parameters. 
Additionally, they learn the relative positions between multiple people via an interpenetration and depth ordering loss.
All these methods follow a multi-stage design.
The complex multi-step process requires a repeated feature extraction, which is computationally expensive. 
Moreover, since they rely on detected bounding boxes, the ambiguity and the limited local view of the bounding-box-level features make it hard to effectively learn from person-person occlusion and truncation.
Instead, our proposed one-stage method learns an explicit pixel-level representation with a holistic view, which significantly improves both accuracy and efficiency in multi-person in-the-wild scenes. 
    
 \textbf{Pixel-level representations} have proven useful in anchor-free detection, such as CornerNet~\cite{law2018cornernet}, CenterNet~\cite{duan2019centernet,zhou2019objects}, and ExtremeNet~\cite{ExtremeNet}.
They directly estimate the corner or center point of the bounding box in a heatmap manner, which avoids the dense anchor-based proposal.
Inspired by these, we develop a pixel-level fine-grained representation for multi-person 3D meshes.
Different from the bounding box center used in~\cite{zhou2019objects}, our body center is determined by the body joints, as introduced in Sec.~\ref{sec:CAR}. 
A recent work, BMP~\cite{zhang2021bmp}, uses a multi-scale grid-level representation for multi-person 3D mesh recovery, which locates a target person at the center of the grid cell.\footnote{The arXiv version of ROMP, called CenterHMR~\cite{centerhmr}, predates BMP.}
In contrast to these methods, ROMP adopts a concise body-center-based representation and further evolves it to a collision-aware version to deal with the inherent center collision problem. 


\textbf{Disambiguation} is a key goal of ROMP, enabling it to  deal with the crowded multi-person scenes. Related techniques have been studied in many other fields. For instance segmentation, Adaptis~\cite{sofiiuk2019adaptis} separately learns the segmentation mask of each instance selected by the guide point. 
To alleviate the ambiguity between embeddings of similar samples, associate embedding~\cite{newell2017associative}, triplet loss~\cite{tripletloss}, and pose-guided association~\cite{bao2020pose} are developed for pose estimation, face recognition, and tracking respectively. 
In this paper, a robust and distinguishable representation is developed to help the model explicitly learn from the crowded scenes.




\section{Our Approach}

\vspace{-1mm}
\subsection{Overview}

The overall framework is illustrated in Fig.~\ref{fig:framework}.
It adopts a simple multi-head design with a backbone and three head networks.
Given a single RGB image as input, it outputs a Body Center heatmap, Camera map, and SMPL map, describing the detailed information of the estimated 3D human meshes. 
In the Body Center heatmap, we predict the probability of each position being a human body center. 
At each position of the Camera/SMPL map, we predict the camera/SMPL parameters of the person that takes the position as the center. 
For simplicity, we combine the Camera map and SMPL map into the Mesh Parameter map. 
During inference, we sample the 3D body mesh parameter results from the Mesh Parameter map at the 2D body center locations parsed from the Body Center heatmap. 
Finally, we put the sampled parameters into the SMPL model to generate the 3D body meshes. 

\subsection{Basic Representations~\label{sec:representations}}

In this section, we introduce the detailed representation of each map. Each output map is of size , where  is the number of channels. Here, we set . 

\textbf{Body Center heatmap:}  is a heatmap representing the 2D human body center in the image. 
Each body center is represented as a Gaussian distribution in the Body Center heatmap. 
For better representation learning, the Body Center heatmap also integrates the scale information of the body in the 2D image.
Specifically, we calculate the Gaussian kernel size  of each person center in terms of its 2D body scale in the image. 
Given the diagonal length  of the person bounding box and the width  of the Body Center heatmap,  is derived as

where  is the minimum kernel size and  is the variation range of . We set  and   by default.

\textbf{Mesh Parameter map:}  consists of two parts, the Camera map and SMPL map.
Assuming that each location of these maps is the center of a human body, we estimate its corresponding 3D body mesh parameters.
Following~\cite{hmr,sun2019dsd-satn}, we employ a weak-perspective camera model to project  3D body joints  of the estimated 3D mesh back to the 2D joints  on the image plane. 
This facilitates training the model with in-the-wild 2D pose datasets, which helps with robustness and generalization. 
 
\textbf{Camera map:}  contains the 3-dim camera parameters  that describe the 2D scale  and translation  of the person in the image. 
The scale  reflects the body size and the depth to some extent.
 and , ranging in , reflect the normalized translation of the human body relative to the image center on the  and  axis, respectively.
The 2D projection  of  3D body joints  can be derived as  .
The translation parameters allow more accurate position estimates than the Body Center heatmap. 

\textbf{SMPL map:}   contains the 142-dim SMPL parameters, which describe the 3D pose and shape of the body mesh. 
SMPL establishes an efficient mapping from the pose    and shape  parameters to the human 3D body mesh .
The shape parameter  is the top-10 PCA coefficients of the SMPL statistical shape space. 
The pose parameters  contain the 3D rotation of the 22 body joints in a 6D representation~\cite{Zhou_2019_CVPR}.  
Instead of using the full 24 joints of the original SMPL model, we drop the last two hand joints. 
The 3D rotation of the first joint denotes the body 3D orientation in the camera coordinate system, 
while the remainder are the relative 3D orientations of each body part with respect to its parent in a kinematic chain. 
3D joints  are derived via , where  is a sparse weight matrix that describes the linear mapping from 6890 vertices of the body mesh  to the  body joints.

\subsection{CAR: Collision-Aware Representation~\label{sec:CAR}}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{figs/CAR.png} \caption{{\bf Collision-Aware Representation.} The body centers of overlapping people are treated as positive charges, which are pushed apart if they are too close in the repulsion field. }
	\label{fig:CAR}
\end{figure}

The entire framework is based on a concise body-center-guided representation. 
It is crucial to define an explicit and robust body center so that the model can easily estimate the center location in various cases. 
Here we introduce the basic definition of the body center for the general case and its advanced version for severe occlusion.

\textbf{Basic definition of the body center.} Existing center-based methods~\cite{duan2019centernet,zhou2019objects} define the center of the bounding box as the target center. 
This works well for general objects (e.g., a ball or bottle) that lack semantically meaningful keypoints.
However, a bounding box center is not a meaningful point on the human body and often falls outside the body area.
For stable parameter sampling, we need an explicit body center.
Therefore, we calculate each body center from the ground truth 2D pose. 
Considering that any body joint may be occluded in general cases, we define the body center as the center of visible torso joints (neck, left/right shoulders, pelvis, and left/right hips). 
When all torso joints are invisible, the center is simply determined by the average of the visible joints.
In this way, the model is encouraged to predict the body location from the visible parts. 

However, in cases of severely overlapping people, the body center of the people might be very close or even at the same location on .
This center collision problem makes the center ambiguous and hard to identify in crowded cases.
To tackle this, we develop a more robust representation to deal with person-person occlusion.
To alleviate the ambiguity, the center points of overlapping people should be kept at a minimum distance to ensure that they can be well distinguished. 
Additionally, to avoid sampling multiple parameters for the same person, the network should assign a unique and explicit center for each person.

Based on these principles, we develop a novel \textbf{Collision-Aware Representation (CAR)}. 
To ensure that the body centers are far enough from each other, we construct a repulsion field. 
In this field, each body center is treated as a positive charge, whose radius of repulsion is equal to its Gaussian kernel size derived by Eq.~(\ref{eq:center kernel size}).
In this way, the closer the body centers are, the greater the mutual repulsion and the further they will be pushed apart. 
Fig. \ref{fig:CAR} illustrates the principle of CAR.  
Suppose that  are the  body centers of two overlapping people. 
If  their Euclidean distance  and Gaussian kernel sizes  satisfy  , the repulsion is triggered to push the close centers apart  via

where  is the repulsion vector from  to  and  is an intensity coefficient to adjust the strength. When there are  multiple overlapped people, we take Eq.~(\ref{eq:centerpush}) to generate the mutual repulsion vectors  for the -th pair of centers. For the center that is affected by   repulsive forces, we calculate the composition of these forces as the numerical summation .


During training, we use CAR to push apart close body centers and use them to supervise the Body Center heatmap. 
In this way, the model is encouraged to estimate the  centers that maintain a distinguishable distance.
For the Body Center heatmap, it helps the model to effectively locate the occluded person.
For the Mesh Parameter map, sampling the parameters from these shifted locations enables the model to extract diverse and individual features for each person.
The model trained with CAR is more suitable for crowded scenes with significant person-person occlusion such as train stations, canteens, etc.


\subsection{Parameter Sampling~\label{sec:parameter sampling}}


To parse the 3D body meshes from the estimated maps, we need to first parse the 2D body center coordinates  from , where  is the number of the detected people, and then use them to sample  for the SMPL parameters. 
In this section, we introduce the process of the center parsing, matching and sampling. 

 is a probability map whose local maxima are regarded as the body centers. 
The local maxima are derived via  where  is the max pooling operation and  is the logical conjunction operation. 
Let  be the 2D coordinates of a local maximum with confidence score larger than a threshold . 
We rank the confidence score at each  and take the top  as the final centers.
During inference, we directly sample the parameters from  at . 
During training, the estimated  are matched with the nearest ground truth body center according to the  distance. 

Additionally, we approximate the depth order between multiple people by using the center confidence from  and the 2D body scale  of the camera parameters from .
For people of different , we regard the one with the larger  as located in the front.
For people of similar , the person with a higher center confidence is considered to be in the front. Please refer to the SuppMat for the details.


\subsection{Loss Functions}

To supervise ROMP, we develop individual loss functions for different maps.
In total, ROMP is supervised by the weighted sum of the body center loss  and mesh parameter loss .

\textbf{Body Center loss.}  encourages a high confidence value at the body center  of the Body Center heatmap  and low confidence elsewhere.
To deal with the imbalance between the center location and the non-center locations in , we train the Body Center heatmap based on the focal loss~\cite{lin2017focal}. 
Given the predicted Body Center heatmap  and the ground truth ,   is defined as 

where   is a binary matrix with a positive value at the body center location, and  is the loss weight.

\textbf{Mesh Parameter loss.} 

As we introduced in Sec.~\ref{sec:parameter sampling}, the parameter sampling process matches each ground truth body with a predicted parameter result for supervision.
The mesh parameter loss is derived as 

 is the  loss of the pose parameters in the  rotation matrix format. 
 is the  loss of the shape parameters. 
 is the  loss of the 3D joints  regressed from the body mesh .
 is the  loss of the 3D joints  after Procrustes alignment.
 is the  loss of the projected 2D joints . 
 is the Mixture Gaussian prior loss of the SMPL parameters adopted in~\cite{keep,SMPL} for supervising the plausibility of 3D joint rotation and body shape. Lastly,  denotes the corresponding loss weights.


\begin{table*}[t]
\setlength\tabcolsep{2pt}
  \centering
  {
  \rowcolors{2}{gray!10}{white}
    \begin{tabular}{l|cccccc}
    \hline
    \textbf{Method} & \textbf{MPJPE} &  \textbf{PMPJPE} &\textbf{PCK} &	\textbf{AUC} &\textbf{MPJAE} & \textbf{PMPJAE} \\
    \hline
        OpenPose + SPIN~\cite{kolotouros2019spin}&  95.8 &  66.4 &33.3 & 55.0  & 23.9 & 24.4\\
        YOLO + VIBE~\cite{kocabas2020vibe}& 94.7 & 66.1 & 33.9 & 56.6 &25.2 &20.46 \\
        CRMH~\cite{jiang2020coherent}& 105.9 &71.8 & 28.5 & 51.4 & 26.4 & 22.0\\
        BMP~\cite{zhang2021bmp} & 104.1 & 63.8 & 32.1 & 54.5 & - & - \\
        \hline
        ROMP (ResNet-50) & 87.0 & 62.0 & 34.4 & 57.6 & 21.9 & 20.1\\
        ROMP (ResNet-50) & \textbf{80.1} &  \textbf{56.8} &36.4 & \textbf{60.1} & 20.8 &19.1\\
        ROMP (HRNet-32) &82.7 & 60.5 &  \textbf{36.5} & 59.7 & \textbf{20.5} & \textbf{18.9}\\
    \hline
    \end{tabular} }
    \caption{{Comparisons to the state-of-the-art methods  on 3DPW following \textit{Protocol 1} (without using any ground truth during inference).  means using extra datasets for training.}}  \label{tab:3DPW}\end{table*}

\begin{table}
 \setlength{\tabcolsep}{2pt}{
	\begin{center}
        \rowcolors{2}{gray!10}{white}
			\begin{tabular}{l|ccc}
				\hline
			    \textbf{Method}  &  \textbf{MPJPE} & \textbf{PMPJPE}  & \textbf{PVE}\\
				\hline 
                 HMR~\cite{hmr} & 130.0 & 76.7 & - \\
                 Kanazawa et al.~\cite{kanazawa2019learning}  & 116.5 & 72.6 & 139.3 \\
                 Arnab et al.~\cite{arnab2019exploiting}  & - & 72.2 & - \\
                 GCMR~\cite{gcmr} & - & 70.2 & - \\
                 DSD-SATN~\cite{sun2019dsd-satn}  & - & 69.5 & - \\
                 SPIN~\cite{kolotouros2019spin}  & 96.9 & 59.2 & 116.4 \\
                 ROMP (ResNet-50) & \textbf{91.3} & \textbf{54.9} & \textbf{108.3}\\
                 \hline
                  I2L-MeshNet~\cite{moon2020i2l} & 93.2 & 58.6 & - \\
                 EFT~\cite{joo2020eft}  & - & 54.2 & - \\
                 VIBE~\cite{kocabas2020vibe} & 93.5 & 56.5 & 113.4\\
                 ROMP (ResNet-50)& 89.3 & 53.5 & 105.6\\
                 ROMP (HRNet-32) &\textbf{85.5} & \textbf{53.3} & \textbf{103.1}\\
				\hline
		\end{tabular}
	\end{center}}
    \caption{{Comparisons to the state-of-the-art methods on 3DPW following VIBE~\cite{kocabas2020vibe}, using \textit{Protocol 2} (on the test set only).  means using extra datasets (compared with SPIN) for training.}}\label{tab:3DPW_VIBE2}
\end{table}




\section{Experiments}
\vspace{-1mm}
\subsection{Implementation Details}


\textbf{Network Architecture.} 
For a fair comparison with other approaches, we use ResNet-50~\cite{resnet} as the default backbone.
Since our method is not limited to a specific backbone, we also test HRNet-32~\cite{cheng2020bottom} in the experiments.
Through the backbone, a feature vector  is extracted from a single RGB image. 
Also, we adopt the CoordConv~\cite{liu2018coordconv} to enhance the spatial information.
Therefore, the backbone feature  is the combination of a coordinate index map  and .
Next, from , three head networks are developed to estimate the Body Center, Camera, and SMPL maps. 
More details of the architecture are in the SuppMat.

\textbf{Setting Details.} 
The input images are resized to , keeping the same aspect ratio and padding with zeros. 
The size of the backbone feature is . 
The maximum number of detections, , which is set manually. 
The loss weights are set to , and  to ensure that the weighted loss items are of the same magnitude.
The threshold  of the Body Center heatmap is 0.2. The repulsion coefficient  of CAR is 0.2. 

\textbf{Training Datasets.}
For a fair comparison with previous methods~\cite{jiang2020coherent,hmr,kolotouros2019spin,sun2019dsd-satn}, the basic training datasets we used in the experiments include two 3D pose datasets (Human3.6M~\cite{h36m} and MPI-INF-3DHP~\cite{mono-3dhp2017}), one pseudo-label 3D dataset (UP~\cite{unite}) and four in-the-wild 2D pose datasets (MS COCO~\cite{coco}, MPII~\cite{mpii}, LSP~\cite{lsp,lsp_extended} and AICH~\cite{aich}). 
We also use the pseudo 3D annotations from \cite{kolotouros2019spin}. 
To further explore the upper limit of performance, we also use additional training datasets, including two 3D pose datasets (MuCo-3DHP~\cite{mono-3dhp2017} and OH~\cite{zhang2020object}), the pseudo 3D labels of 2D pose datasets provided by~\cite{joo2020eft}, and two 2D pose datasets (PoseTrack~\cite{PoseTrack} and Crowdpose~\cite{crowdpose}), to train an advanced model.

\textbf{Evaluation Benchmarks.} 
3DPW~\cite{3dpw} is employed as the main benchmark for evaluating 3D mesh/joint error since it contains in-the-wild multi-person videos with abundant 2D/3D annotations.
Specially, we divide 3DPW into 3 subsets, including 3DPW-PC for person-person occlusion, 3DPW-OC for object occlusion, and 3DPW-NC for the non-occluded/truncated cases, to evaluate the performance in different scenarios.
Additionally, we also evaluate on a indoor multi-person 3D pose benchmark, CMU Panoptic~\cite{cmu_panoptic}. 
Furthermore, we evaluate the stability under occlusion on  Crowdpose~\cite{crowdpose}, a crowded-person in-the-wild 2D pose benchmark.

\textbf{Evaluation Metrics.}
We adopt per-vertex error (PVE) to evaluate the 3D surface error.
To evaluate the 3D pose accuracy, we employ mean per joint position error (MPJPE), Procrustes-aligned MPJPE (PMPJPE), percentage of correct keypoints (PCK), and area under the PCK-threshold curve (AUC).
We also adopt mean per joint angle error (MPJAE), and Procrustes-aligned MPJAE (PA-MPJAE) to evaluate the 3D joint rotation accuracy. 
Also, to evaluate the pose accuracy in crowded scenes, we calculate the average precision () between the 2D-projection  and the ground truth 2D poses on Crowdpose.

\begin{table}
 \setlength{\tabcolsep}{2pt}{
	\begin{center}
        \rowcolors{2}{gray!10}{white}
			\begin{tabular}{l|ccc}
				\hline
			    \textbf{Method} &  \textbf{MPJPE} & \textbf{PMPJPE}  &\textbf{PVE}\\
				\hline 
                 EFT~\cite{joo2020eft} &   - & 52.2 & - \\
                 VIBE~\cite{kocabas2020vibe} & 82.9 & 51.9 & 99.1\\
                 \hline 
                 ROMP (ResNet-50) &  84.2 & 51.9 & 100.4 \\
                 ROMP(ResNet-50) & 79.7 & 49.7 & 94.7 \\
                 ROMP (HRNet-32)   & 78.8 & 48.3 & 94.3 \\
                 ROMP (HRNet-32) & \textbf{76.7} & \textbf{47.3} & \textbf{93.4}\\
				\hline
		\end{tabular}
	\end{center}}
    \caption{ {Comparisons to the state-of-the-art methods on 3DPW following \textit{Protocol 3} (fine-tuned on the training set).  means using extra datasets (compared with EFT) for training.}}\label{tab:3DPW_VIBE3}
\end{table}


\begin{table}[t]
\setlength\tabcolsep{0.5mm}
  \centering
  {
    \rowcolors{2}{gray!10}{white}
    \begin{tabular}{l|cccc|c}
    \hline
    \textbf{Method} &  \textbf{Haggling} & \textbf{Mafia} &\textbf{Ultim.} & \textbf{Pizza} & \textbf{Mean}\\
    \hline
        Zanfir et. al.~\cite{zanfir2018deep} & 141.4 & 152.3 & 145.0 & 162.5 & 150.3 \\
        MSC~\cite{zanfir2018monocular} & 140.0 & 165.9 & 150.7 & 156.0 & 153.4 \\
        CRMH~\cite{jiang2020coherent} & 129.6 & 133.5 & 153.0 & 156.7 & 143.2 \\
	    ROMP (ResNet-50) & \textbf{111.8} & \textbf{129.0} & \textbf{148.5} & \textbf{149.1} & \textbf{134.6} \\
        \hline
        BMP~\cite{zhang2021bmp} & 120.4 & 132.7 & 140.9 & 147.5 & 135.4\\
        ROMP (ResNet-50) & \textbf{107.8} & 125.3 & \textbf{135.4} & 141.8 & \textbf{127.6} \\
        ROMP (HRNet-32) & 110.8 & \textbf{122.8} & 141.6 & \textbf{137.6} & 128.2 \\
    \hline
    \end{tabular} }
    \caption{ {Comparisons to the state-of-the-art methods on CMU Panoptic~\cite{cmu_panoptic} benchmark. The evaluation metric is MPJPE after centering the root joint. All methods are directly evaluated without any fine-tuning.   means using extra datasets for training.}}  \label{tab:CMU Panoptic}
\end{table}


\begin{table}
	\begin{center}
        \setlength{\tabcolsep}{0.8mm}{
        {
          \rowcolors{2}{gray!10}{white}
			\begin{tabular}{l|ccc}
				\hline
				\textbf{Method} & \textbf{3DPW-PC} &  \textbf{3DPW-NC} & \textbf{3DPW-OC} \\
				\hline 
                CRMH~\cite{jiang2020coherent} & 103.5  & 68.7 & 78.9\\
                VIBE~\cite{kocabas2020vibe}  &103.9  & 57.3 &\textbf{ 65.9}\\
                \hline 
                ROMP w/o CAR & 79.7 & 56.7 &  67.0\\
                - w/ CAR () & 77.6 & \textbf{55.6} & 66.6 \\
                - w/ CAR () & \textbf{75.8}  & 57.1 & 67.1\\
                - w/ CAR () & 77.0 & 56.4 & 66.5 \\
				\hline
		\end{tabular}}}
	\end{center}
	\caption{ {Comparisons to the state-of-the-art methods on the person-occluded (3DPW-PC),  object-occluded (3DPW-OC) and  non-occluded/truncated (3DPW-NC) subsets of 3DPW. We also ablate CAR and vary the  repulsion coeff.~.  The evaluation metric is PMPJPE. }}\label{tab:3DPW-SPLIT}
\end{table}


\subsection{Comparisons to the State-of-the-Art~\label{sec:comparisons}}

\textbf{3DPW.} 
We adopt three evaluation protocols, which reveal different properties.
To validate the performance in actual scenes, we follow \textit{Protocol 1} from the 3DPW Challenge to evaluate on the entire 3DPW dataset without using any ground truth, especially the bounding box. 
With the whole image as input, we equip each single-person method~\cite{kocabas2020vibe,kolotouros2019spin} with a human detector (OpenPose~\cite{openpose} or YOLO~\cite{redmon2018yolov3}).
For a fair comparison, ROMP uses the same backbone (ResNet-50) and training datasets as the competing method~\cite{kolotouros2019spin}.
We obtain the results of  OpenPose + SPIN from~\cite{imry2020challengespin}. 
The results of YOLO + VIBE are obtained using the officially released code, which already contains the YOLO part for human detection.
The results of BMP, which adopts a multi-scale grid-level representation, are obtained from~\cite{zhang2021bmp}.
In Tab.~\ref{tab:3DPW}, ROMP significantly outperforms all these methods, particularly in MPJPE, PMPJPE, and MPJAE.
These results validate that learning a robust pixel-level representation with a holistic view is helpful for improving the robustness and generalization in actual scenes.
Training with extra datasets () shows that the accuracy of ROMP can be further improved.

As a sanity check, we also compare ROMP with the single-person approaches in the evaluation protocols that allow them to use the cropped single-person image as input, while ROMP still takes the whole image as input.
Following VIBE~\cite{kocabas2020vibe}, \textit{Protocol 2} uses the 3DPW test set for evaluation without fine-tuning on the training set, while \textit{Protocol 3} fine-tunes the model on the 3DPW training set and uses the test set for evaluation.
In Tab.~\ref{tab:3DPW_VIBE2}, ROMP outperforms these multi-stage approaches on \textit{Protocol 2}, further demonstrating the advantage of our one-stage design.
In Tab.~\ref{tab:3DPW_VIBE3}, ROMP achieves comparable results with the state-of-the-art methods. 
If we use HRNet-32 as the backbone, the accuracy improves significantly after fine-tuning. 

\begin{table}
	\begin{center}
		
        \setlength{\tabcolsep}{0.6mm}{
			\begin{tabular}{l|c>{\columncolor{gray!10}}ccc}
                 \hline
				\textbf{Split}  & CRMH~\cite{jiang2020coherent}& ROMP &  ROMP+CAR \\
				\hline 
               Test & 33.9 & 54.1 & \textbf{59.7}\\
               Validation  & 32.9& 55.6 & \textbf{58.6}\\
				\hline
		\end{tabular}}
	\end{center}
   \caption{{Comparisons to the state-of-the-art methods on the Crowdpose~\cite{crowdpose} benchmark.  The evaluation metric is .}}\label{tab:Crowdpose}
\end{table}


\begin{table}
	\begin{center}
        \setlength{\tabcolsep}{0.6mm}{
			\begin{tabular}{c|c>{\columncolor{gray!10}}cc>{\columncolor{gray!10}}c}
				\hline
				\textbf{Method} & VIBE~\cite{kocabas2020vibe} & CRMH~\cite{jiang2020coherent}&ROMP& ROMP \\
                \hline
                FPS  & 10.9 & 14.1 & 20.8 & \textbf{30.9}\\
                \hline
                Backbone & ResNet-50 & ResNet-50 & HRNet-32 & ResNet-50\\
				\hline
		\end{tabular}}
	\end{center}
	\caption{{Run-time comparisons on a 1070Ti GPU. }}\label{tab:runtime comparison}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{figs/FPS_nonum.png}
	\caption{ The FPS variations of ROMP and YOLO+VIBE~\cite{kocabas2020vibe} when processing images with different number of people.}
	\label{fig:fps}
\end{figure}


\textbf{CMU Panoptic.} 
Following the evaluation protocol of CRMH~\cite{jiang2020coherent}, we evaluate ROMP on the multi-person benchmark, CMU Panoptic, without any fine-tuning.
For a fair comparison, we use the same backbone and similar training set as CRMH.
As shown in Tab.~\ref{tab:CMU Panoptic} , ROMP outperforms the existing multi-stage  methods~\cite{jiang2020coherent,zanfir2018monocular,zanfir2018deep} in all activities by a large margin.
These results further demonstrate that learning pixel-level representation with a holistic view improves the performance on multi-person scenes.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.96\columnwidth]{figs/demo_comparison_min.png}
	\caption{Qualitative comparisons to CRMH~\cite{jiang2020coherent} on the Crowdpose and the internet images.} 
	\label{fig:demo_comparison}
\end{figure}

\textbf{Occlusion benchmarks.}
To validate the stability under occlusion, we evaluate ROMP on multiple occlusion benchmarks.
Firstly, on the person-occluded \textbf{3DPW-PC} and \textbf{Crowdpose}~\cite{crowdpose}, results in Tab.~\ref{tab:3DPW-SPLIT} and \ref{tab:Crowdpose} show that ROMP significantly outperforms previous state-of-the-art methods~\cite{jiang2020coherent,kocabas2020vibe}.
Additionally, in Fig.~\ref{fig:demo_comparison}, some qualitative comparisons to 
CRMH also demonstrate ROMP's robustness to person-person occlusion.
These results suggest that the pixel-level representation is important for improving the performance under person-person occlusion.
Finally, on the object-occluded \textbf\textbf{3DPW-OC}, ROMP also achieves promising performance.
These results demonstrate that the fine-grained pixel-level representation is beneficial for dealing with various occlusion cases.

\textbf{Runtime comparisons.} 
We compare ROMP with the state-of-the-art methods in processing videos captured by a web camera.
All runtime comparisons are performed on a desktop with a GTX 1070Ti GPU, Intel i7-8700K CPU, and 8 GB RAM.
As shown in Tab.~\ref{tab:runtime comparison}, ROMP achieves real-time performance, significantly faster than the competing methods.
Additionally, as shown in Fig.~\ref{fig:fps}, compared with the multi-stage methods~\cite{jiang2020coherent,kocabas2020vibe}, ROMP's processing time is roughly constant regardless of the number of people. 

\vspace{-1mm}
\subsection{Ablation Study of the CAR}
\vspace{-1mm}
As shown in Tab.~\ref{tab:3DPW-SPLIT} and \ref{tab:Crowdpose}, CAR improve the PMPJPE metric on the 3DPW-PC and the Crowdpose datasets by 4.8\% and 10.3\%, respectively.
Additionally, Fig.~\ref{fig:CAR_qab} shows, qualitatively, the impact of ablating CAR.
Adding CAR improves  performance in  crowded scenes, which demonstrates that CAR effectively alleviates the center collision problem.

\textbf{Intensity coefficient  of the CAR. }
To set , we conduct an ablation study on 3DPW-PC.
In Tab.~\ref{tab:3DPW-SPLIT}, setting  performs much better on the crowded scenes (3DPW-PC) and its performance on the normal cases (3DPW-NC/3DPW-OC) is comparable to the best. 
For general in-the-wild cases of Crowdpose, setting  improves the performance by 10\%  over  in Tab.~\ref{tab:Crowdpose}.
Therefore, we suggest training the model with  for all cases.
The reason performance degrades in the normal cases is probably that pushing apart the body centers affects the consistency of the body-center-guided representation.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.96\columnwidth]{figs/CAR_qab.png}
	\caption{Qualitative ablation study of the CAR on the Crowdpose.}
	\label{fig:CAR_qab}
\end{figure}
\vspace{-1mm}
\subsection{Discussion}
\vspace{-1mm}
To understand the source of our performance gains, we conduct an ablation study on different subsets of 3DPW subsets. 
Results in Tab.~\ref{tab:3DPW-SPLIT} show that our main gains come from the person-occluded and the non-occluded/truncated cases.
It demonstrates the effectiveness of the proposed pixel-level representation in improving the disambiguation in the crowded scenes.
Our experiments suggest that the difference between ROMP and the state-of-the-arts~\cite{jiang2020coherent,kocabas2020vibe,kolotouros2019spin} is the method of representation learning. 
ROMP learns the pixel-level representation from a holistic view, while the multi-stage methods learn a bounding-box-level representation in a local view.
Our one-stage framework enables ROMP to learn more discriminative features that are robust to rich disturbances outside the bounding box, helping generalization. 

\vspace{-1mm}
\section{Conclusion}
\vspace{-1mm}
We introduce a novel one-stage network, ROMP, for monocular multi-person 3D mesh regression. 
For pixel-level estimation, we propose an explicit body-center-guided representation and further develop it as a collision-aware version, CAR, enabling robust prediction under person-person occlusion. 
ROMP is the first open-source one-stage method that achieves state-of-the-art performance on multiple benchmarks as well as real-time inference speed.
ROMP can serve as a simple yet effective foundation for related multi-person 3D tasks, such as depth estimation, tracking, and interaction modeling. 

\noindent \textbf{Acknowledgements:} This work was supported by the National Key R\&D Program of China under Grand No. 2020AAA0103800. 
We thank Peng Cheng for discussing the Center map training.


\noindent 
\noindent\textbf{Disclosure:} MJB has received research funds
from Adobe, Intel, Nvidia, Facebook, and Amazon. While MJB
is a part-time employee of Amazon, his research was performed
solely at Max Planck. MJB has financial interests in Amazon, Datagen Technologies, and Meshcapade
GmbH.

{\small
\bibliographystyle{packages/ieee_fullname}
\bibliography{arxiv}
}

\clearpage
\includepdf[pages=1]{SuppMat.pdf}
\includepdf[pages=2]{SuppMat.pdf}
\includepdf[pages=3]{SuppMat.pdf}
\includepdf[pages=4]{SuppMat.pdf}
\includepdf[pages=5]{SuppMat.pdf}
\includepdf[pages=6]{SuppMat.pdf}
\includepdf[pages=7]{SuppMat.pdf}

\end{document}
