\documentclass{llncs} \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}


\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\iter}{\mathrm{iter}}



\title{Clustering Protein Sequences Given the Approximation Stability of the Min-Sum Objective Function}

\author{Konstantin Voevodski\inst{1} \and Maria-Florina Balcan\inst{2} \and Heiko R{\"o}glin\inst{3} \and Shang-Hua Teng\inst{4} \and Yu Xia\inst{5}}

\institute{Department of Computer Science, Boston University, Boston, MA 02215, USA
\and College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA
\and Department of Computer Science, University of Bonn, Bonn, Germany
\and Computer Science Department, University of Southern California, Los Angeles, CA 90089, USA
\and Bioinformatics Program and Department of Chemistry, Boston University, Boston, MA 02215, USA}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
We study the problem of efficiently clustering protein sequences in a limited information setting.  We assume that we do not know the distances between the sequences in advance, and must query them during the execution of the algorithm.  Our goal is to find an accurate clustering using few queries.  We model the problem as a point set  with an unknown metric  on , and assume that we have access to \emph{one versus all} distance queries that given a point  return the distances between  and all other points.  Our one versus all query represents an efficient sequence database search program such as BLAST, which compares an input sequence to an entire data set.  Given a natural assumption about the approximation stability of the \emph{min-sum} objective function for clustering, we design a provably accurate clustering algorithm that uses few one versus all queries.  In our empirical study we show that our method compares favorably to well-established clustering algorithms when we compare computationally derived clusterings to gold-standard manual classifications.
\end{abstract}

\section{Introduction}

	Biology is an information-driven science, and the size of available data continues to expand at a remarkable rate.  The growth of biological sequence databases has been particularly impressive.  For example, the size of GenBank, a biological sequence repository, has doubled every 18 months from 1982 to 2007. It has become important to develop computational techniques that can handle such large amounts of data.  Clustering is very useful for exploring relationships between protein sequences. However, most clustering algorithms require distances between all pairs of points as input, which is infeasible to obtain for very large protein sequence data sets.  Even with a \emph{one versus all} distance query such as BLAST (Basic Local Alignment Search Tool) \cite{blast}, which efficiently compares a sequence to an entire database of sequences, it may not be possible to use it  times to construct the entire pairwise distance matrix, where  is the size of the data set.  In this work we present a clustering algorithm that gives an accurate clustering using only  queries, where  is the number of clusters.

We analyze the correctness of our algorithm under a natural assumption about the data, namely the  approximation stability property of \cite{bbg}.  Balcan et al. assume that there is some relevant ``target'' clustering , and optimizing a particular objective function for clustering (such as min-sum) gives clusterings that are structurally close to .  More precisely, they assume that any -approximation of the objective is -close to , where the distance between two clusterings is the fraction of misclassified points under the optimum matching between the two sets of clusters.  Our contribution is designing an algorithm that given the -property for the \emph{min-sum} objective produces an accurate clustering using only  \emph{one versus all} distance queries, and has a runtime of .  We conduct an empirical study that compares computationally derived clusterings to those given by gold-standard classifications of protein evolutionary relatedness.  We show that our method compares favorably to well-established clustering algorithms in terms of accuracy.  Moreover, our algorithm easily scales to massive data sets that cannot be handled by traditional algorithms.

The algorithm presented here is related to the one presented in \cite{vbrtx}.  The \emph{Landmark-Clustering} algorithm presented there gives an accurate clustering if the instance satisfies the -property for the -median objective.  However, if the property is satisfied for the \emph{min-sum} objective the structure of the clustering instance is quite different, and the algorithm given in \cite{vbrtx} fails to find an accurate clustering in such cases.  Indeed, the analysis presented here is also quite different.  The min-sum objective is also considerably harder to approximate.  For -median the best approximation guarantee is  given by \cite{kMedApprox}.  For the min-sum objective when the number of clusters is arbitrary there is an -approximation algorithm with running time  due to \cite{bcr}.

There are also several other clustering algorithms that are applicable in our limited information setting \cite{av,ajm,sampleBasedClustering,sampleBasedClustering3}.  However, because all of these methods seek to approximate an objective function they will not produce an accurate clustering in our model if the -property holds for values of  for which finding a -approximation is difficult.  Other than \cite{vbrtx} we are not aware of any results providing both provably accurate algorithms and strong query complexity guarantees in such a model.

\section{Preliminaries}

Given a metric space  with point set , an unknown distance function
 satisfying the triangle inequality, and a set of points , we
would like to find a -clustering  that partitions the points in  into
 sets  by using \emph{one versus all} distance
queries.

The \emph{min-sum} objective function for clustering is to minimize \linebreak .  We reduce the min-sum clustering problem to the related \emph{balanced k-median} problem.  The balanced -median objective function seeks to minimize , where  is the median of cluster , which is the point  that minimizes .  As pointed out in \cite{bcr}, in metric spaces the two objective functions are related to within a factor of 2: .  For any objective function  we use  to denote its optimum value.

In our analysis we assume that  satisfies the -property of \cite{bbg} for the min-sum and balanced -median objective functions.  To formalize the -property we need to define a notion of distance
between two -clusterings  and
. As in \cite{bbg}, we define the
distance between  and  as the fraction of points on which they disagree
under the optimal matching of clusters in  to clusters in :

where  is the set of bijections .  Two clusterings  and  are said to be \emph{-close}
if .

We assume that there exists some unknown relevant ``target'' clustering  and given a proposed clustering  we define the error of  with respect to  as . Our goal is to find a clustering of low error.  The  approximation stability property is defined as follows.
\begin{definition}
We say that the instance  satisfies the -property for objective function  with respect to the target clustering  if any
clustering of  that approximates  within a factor of  is
-close to , that is,

\end{definition}

We note that because any -approximation of the balanced -median objective is a -approximation of the min-sum objective, it follows that if the clustering instance satisfies the -property for the min-sum objective, then it satisfies the -property for balanced -median.

\section{Algorithm Overview}

In this section we present a clustering algorithm that given the -property for the balanced -median objective finds an accurate clustering using few distance queries.   Our algorithm is outlined in Algorithm~\ref{alg-main} (with some implementation details omitted).  We start by uniformly at random choosing  points that we call \emph{landmarks}, where  is an appropriate number.  For each landmark that we choose we use a \emph{one versus all} query to get the distances between this landmark and all other points.  These are the only distances used by our procedure.

Our algorithm then expands a ball  around each landmark  one point at a time.  In each iteration we check whether some ball  passes the test in line 7.  Our test considers the size of the ball and its radius, and checks whether their product is greater than the threshold .  If this is the case, we consider all balls that overlap  on any points, and compute a cluster that contains all the points in these balls.  Points and landmarks in the cluster are then removed from further consideration.

\vspace{-0.5cm}
\begin{algorithm}[H]
\caption{Landmark-Clustering-Min-Sum()}
\begin{algorithmic}[1]
\STATE choose a set of landmarks  of size  uniformly at random from ;
\STATE , ;
\WHILE{}
\FOR{each }
\STATE ;
\ENDFOR
\IF{}
\STATE ;
\STATE  and ;
\STATE ;
\STATE remove clustered points from consideration;
\ENDIF
\STATE increment  to the next relevant distance;
\ENDWHILE
\RETURN ;
\end{algorithmic}
\label{alg-main}
\end{algorithm}
\vspace{-0.5cm}

A complete description of this algorithm can be found in the next section.  We now present our theoretical guarantee for Algorithm~\ref{alg-main}.

\begin{theorem}\label{thm:Main}
Given a metric space , where  is unknown, and a set of points ,
if the instance  satisfies the -property for the balanced--median objective function, we are given the optimum objective value , and each cluster in the target clustering  has size at least
, then \emph{Landmark-Clustering-Min-Sum}() outputs a clustering that is -close to  with probability at least .   The algorithm uses  \emph{one versus all distance queries}, and has a runtime of .
\end{theorem}

We note that  if the sizes of the target clusters are balanced.  In addition, if we do not know the value of , we can still find an accurate clustering by running Algorithm~\ref{alg-main} from line 2 at most  times with increasing estimates of  until enough points are clustered.  It is not necessary to recompute the landmarks, so the number of distance queries that are required remains the same.  We next give some high-level intuition for how our procedures work.

\begin{figure}
\begin{center}
\includegraphics[width=45mm,height=30mm]{minSumStructure.pdf}
\caption{Cluster cores ,  and  are shown with diameters ,  and , respectively.  The diameters of the cluster cores are inversely proportional to their sizes.  \label{fig:minSumStructure}}
\end{center}
\end{figure}

Given our approximation stability assumption, the target clustering must have the structure shown in Figure~\ref{fig:minSumStructure}.  Each target cluster  has a ``core'' of well-separated points, where any two points in the cluster core are closer than a certain distance  to each other, and any point in a different core is farther than , for some constant .  Moreover, the diameters of the cluster cores are inversely proportional to the cluster sizes: there is some constant  such that  for each cluster .  Given this structure, it is possible to classify the points in the cluster cores correctly if we extract the smaller diameter clusters first.  In the example in Figure~\ref{fig:minSumStructure}, we can extract , followed by  and  if we choose the threshold  correctly and we have selected a landmark from each cluster core.  However, if we wait until some ball contains all of ,  and  may be merged.

\section{Algorithm Analysis}

In this section we present a formal analysis of our algorithm, and give the proof of Theorem~\ref{thm:Main}.  We first present a complete description of the algorithm.  We then describe the structure of the clustering instance that is implied by our approximation stability assumption.  We then give a general overview of our argument, which is followed by the complete proof.

\subsection{Algorithm Description}

A full description of our algorithm is given in Algorithm~\ref{alg-full}.  In order to efficiently expand a ball around each landmark, we first sort all landmark-point pairs  by .  We then consider these pairs in order of increasing distance (line 7), skipping pairs where  or  have already been clustered; the clustered points are maintained in the set .

In each iteration we check whether some ball  passes the test in line 19.  Our actual test, which is slightly different than the one presented earlier, considers the size of the ball and the \emph{next largest} landmark-point distance (denoted by ), and checks whether their product is greater than the threshold .  If this is the case, we consider all balls that overlap  on any points, and compute a cluster that contains all the points in these balls.  Points and landmarks in the cluster are then removed from further consideration by adding the clustered points to , and removing the clustered points from any ball.

Our procedure terminates once we find  clusters.  If we reach the final landmark-point pair, we stop and report the remaining unclustered points as part of the same cluster (line 12).  If the algorithm terminates without partitioning all the points, we assign each remaining point to the cluster containing the closest clustered landmark.  In our analysis we show that if the clustering instance satisfies the -property for the balanced -median objective function, our procedure will output exactly  clusters.

The most time-consuming part of our algorithm is sorting all landmark-points pairs, which takes , where  is the size of the data set and  is the set of landmarks.  With a simple implementation that uses a hashed set to store the points in each ball, the total cost of computing the clusters and removing clustered points from active balls is at most  each.  All other operations take asymptotically less time, so the overall runtime of our procedure is .

\begin{algorithm}[H]
\caption{Landmark-Clustering-Min-Sum()}
\begin{algorithmic}[1]
\STATE choose a set of landmarks  of size  uniformly at random from ;
\FOR{each }
\STATE ;
\ENDFOR
\STATE , ;
\WHILE{}
\STATE  = GetNextActivePair();
\STATE ;
\IF{( = PeekNextActivePair())  null}
\STATE ;
\ELSE
\STATE ;
\STATE break;
\ENDIF
\STATE ;
\IF{}
\STATE continue;
\ENDIF
\WHILE{ and }
\STATE ;
\STATE ;
\STATE  and ;
\FOR{each }
\STATE ;
\FOR{each }
\STATE ;
\ENDFOR
\ENDFOR
\STATE ;
\ENDWHILE
\ENDWHILE
\RETURN ;
\end{algorithmic}
\label{alg-full}
\end{algorithm}

\subsection{Structure of the Clustering Instance}

We next describe the structure of the clustering instance that is implied by our approximation stability assumption.  We denote by  the optimal balanced--median clustering with objective value OPT=.  For each cluster , let  be the median point in the cluster.  For , define  and let  = avg.  Define  = min.

It is proved in \cite{bbg} that if the instance satisfies the )-property for the balanced -median objective function and each cluster in  has size at least max, then at most -fraction of points  have .  In addition, by definition of the average weight  at most -fraction of points  have .

We call point  \emph{good} if both  and , else  is called bad.  Let  be the \emph{good} points in the optimal cluster , and let  be the bad points.

Lemma~\ref{lemma:structureBalancedKMedian}, which is similar to Lemma 14 of \cite{bbg}, proves that the optimum balanced -median clustering must have the following structure:
\begin{enumerate}
\item For all  in the same , we have .
\item For  and , .
\item The number of bad points is at most .
\end{enumerate}

\subsection{Proof of Theorem~\ref{thm:Main}}

Our algorithm expands a ball around each landmark, one point at a time, until some ball is large enough.  We use  to refer to the current radius of the balls, and  to refer to the next relevant radius (next largest landmark-point distance).  To pass the test in line 19, a ball  must satisfy .  We choose  such that by the time a ball satisfies the conditional, it must overlap some good set .  Moreover, at this time the radius must be large enough for  to be entirely contained in some ball;  will therefore be part of the cluster computed in line 22.  However, the radius is too small for a single ball to overlap different good sets and for two balls overlapping different good sets to share any points.  Therefore the computed cluster cannot contain points from any other good set.  Points and landmarks in the cluster are then removed from further consideration.  The same argument can then be applied again to show that each cluster output by the algorithm entirely contains a single good set.  Thus the clustering output by the algorithm agrees with  on all the good points, so it must be closer than  to .  A more detailed argument is given below.

\begin{proof}
Since each cluster in the target clustering has more than
 points, and the optimal balanced--median clustering  can differ from the target clustering by fewer than  points, each cluster in  must have more than  points.  Moreover, by Lemma~\ref{lemma:structureBalancedKMedian} we may have at most  bad points, and hence each .  We will use  to refer to the  quantity.

Our argument assumes that we have chosen at least one landmark from each good set . Lemma~\ref{lemma:numberSamples} argues that after selecting  landmarks the probability of this happening is at least .  Moreover, if the target clusters are balanced in size:  for some constant , because the size of each good set is at least half the size of the corresponding target cluster, it must be the case that , so .

Suppose that we order the clusters of  such that , and let .  Define  and recall that .   Note that because there is a landmark in each good set , for radius  there exists some ball containing all of .  We use  to denote a ball of radius  around landmark : .

If we apply Lemma~\ref{lemma:noOverlap} with all the clusters in , we can see that as long as , a ball cannot contain points from more than one good set and balls overlapping different good sets cannot share any points.  We also observe that when both  and  are true, a ball  containing points from  does not satisfy .  For  a ball cannot contain points from different good sets; therefore any ball containing points from  has size at most .  In addition, for  the size bound .  Therefore for these values of  any ball containing points from  is too small to satisfy the conditional.

Finally, we observe that for  some ball  containing all of  does satisfy .  Clearly, for  there is some ball containing all of , which must have size at least .  For  the size bound , so this ball is large enough to satisfy this conditional.  Moreover, for  the size bound .  Therefore a ball containing only bad points cannot pass our test for  because the number of bad points is at most .

Consider the smallest radius  for which some ball  satisfies .  It must be the case that , and  overlaps with some good set  because we cannot have a ball containing only bad points for .  Moreover, by our previous argument because  contains points from , it must be the case that , and therefore some ball contains all the points in .  Consider a cluster  of all the points in balls that overlap :  and , which must include all the points in .  In addition,  cannot share any points with balls that overlap other good sets because , therefore  does not contain points from any other good set.  Therefore the cluster  entirely contains some good set and no points from any other good set.

These facts suggest the following conceptual algorithm for finding a clustering that classifies all the good points correctly: increment  until some ball satisfies , compute the cluster containing all points in balls that overlap , remove these points, and repeat until we find  clusters.  We can argue that each cluster output by the algorithm entirely contains some good set and no points from any other good set.  Each time we can consider the clusters  whose good sets have not yet been output, order them by size, and apply Lemma~\ref{lemma:noOverlap} with  to argue that while  the radius is too small for the computed cluster to overlap any of the remaining good sets.  As before, we can argue that by the time we reach  we must output some cluster.  In addition, when  we cannot output a cluster of only bad points and whenever we output a cluster overlapping some good set , it must be the case that .  Therefore each computed cluster must entirely contain some good set and no points from any other good set.  If there are any unclustered points upon the completion of the algorithm, we can assign the remaining points to any cluster.  Still, we are able to classify all the good points correctly, so the reported clustering must be closer than  to .

It suffices to show that even though our algorithm only considers discrete values of  corresponding to landmark-point distances, the output of our procedure exactly matches the output of the conceptual algorithm described above.  Consider the smallest (continuous) radius  for which some ball  satisfies .  We use  to refer to the largest landmark-point distance such that .  Clearly, by the time our algorithm reaches  it must be the case that  passes the test on line 19: , and this test is not passed by any ball at any prior time.  Moreover,  must be the largest ball passing our test at this point because if there is another ball  that also satisfies our test when  it must be the case that  because  satisfies  for a smaller .  Finally because there are no landmark-point pairs  with ,  for each landmark .  Therefore the cluster that we compute on line 22 for  is equivalent to the cluster the conceptual algorithm computes for .  We can repeat this argument for each cluster output by the conceptual algorithm, showing that Algorithm~\ref{alg-full} finds exactly the same clustering.

We note that when there is only one good set left the test in line 19 may not be satisfied anymore if , where  is the diameter of the remaining good set.  However, in this case if we exhaust all landmark-points pairs we report the remaining points as part of a single cluster (line 12), which must contain the remaining good set, and possibly some additional bad points that we consider misclassified anyway.

With a simple implementation that uses a hashed set to keep track of the points in each ball, the runtime of our procedure is , which is given by the time necessary to sort all landmark-point pairs by distance.  All other operations take asymptotically less time.  In particular, over the entire run of the algorithm, the cost of computing the clusters in lines 21-22 is at most , and the cost of removing clustered points from active balls in lines 23-28 is also at most ). \qed

\end{proof}

\begin{theorem}\label{lemma:unknownW}
If we are not given the optimum objective value , then we can still find a clustering that is -close to  with probability at least  by running Landmark-Clustering-Min-Sum at most  times with the same set of landmarks, where the number of landmarks  as before.
\end{theorem}

\begin{proof}

If we are not given the value of  then we have to estimate the threshold parameter  for deciding when a cluster develops.  Let us use  to refer to its correct value ().  We first note that there are at most  relevant values of  to try, where  is the set of landmarks.  Our test in line 19 checks whether the product of a ball size and a ball radius is larger than , and there are only  possible ball sizes and  possible values of a ball radius.

Suppose that we choose a set of landmarks , , as before. We then compute all  relevant values of  and order them in ascending order:  for .  Then we repeatedly execute Algorithm~\ref{alg-full} starting on line 2 with increasing estimates of .  Note that this is equivalent to trying all continuous values of  in ascending order because the execution of the algorithm does not change for any  such that .  In other words, when , the algorithm will give the same exact answer for  as it would for .

Our procedure stops the first time we cluster at least  points, where  is the maximum number of bad points.  We give an argument that this gives an accurate clustering with an additional error of .

As before, we assume that we have selected at least one landmark from each good set, which happens with probability at least .  Clearly, if we choose the right threshold  the algorithm must cluster at least  points because the clustering will contain all the good points.  Therefore the first time the algorithm clusters at least  points for some estimated threshold , it must be the case that .  Lemma~\ref{lemma:correctEstimate} argues that if  and the number of clustered points is at least , then the reported partition must be a -clustering that contains a distinct good set in each cluster.  This clustering may exclude up to  points, all of which may be good points.  Still, if we arbitrarily assign the remaining points we will get a clustering that is closer than  to . \qed

\end{proof}

\begin{lemma}\label{lemma:structureBalancedKMedian}
If the balanced -median instance satisfies the )-property and each cluster in  has size at least  we have:
\begin{enumerate}
\item For all  in the same , we have .
\item For  and , .
\item The number of bad points is at most .
\end{enumerate}
\end{lemma}

\begin{proof}
For part 1, since  are both good, they are at distance of at most  to , and hence at distance of at most  to each other.

For part 2 assume without loss of generality that .  Both  and  are good; it follows that , and  because .  By the triangle inequality it follows that

where we use that .

Part 3 follows from the maximum number of points that may not satisfy each of the properties of the good points and the union bound. \qed
\end{proof}

\begin{lemma}\label{lemma:numberSamples}
After selecting  points uniformly at random, where  is the size of the smallest good set, the probability that we did not choose a point from every good set is smaller than .

\end{lemma}

\begin{proof}
We denote by  the cardinality of .  Observe that the probability of not selecting a point from some good set  after  samples is .  By the union bound the probability of not selecting a point from every good set after  samples is at most , which is equal to  for . \qed
\end{proof}



\begin{lemma}\label{lemma:noOverlap}
Given a subset of clusters , and the set of the corresponding good sets , let  be the size of the largest cluster in , and .  Then for , a ball cannot overlap a good set  and any other good set, and a ball containing points from a good set  cannot share any points with a ball containing points from any other good set.
\end{lemma}

\begin{proof}

By part 2 of Lemma~\ref{lemma:structureBalancedKMedian}, for  and  we have



It follows that for  and  we must have , where we use the fact that .  So a point in a good set in  and a point in any other good set must be farther than .

To prove the first part, consider a ball  of radius  around landmark .  In other words, .  If  overlaps a good set in  and any other good set, then it must contain a point  and a point .  It follows that , giving a contradiction.

To prove the second part, consider two balls  and  of
radius  around landmarks  and .  Suppose  and  share at least one
point: , and use  to refer to
this point. It follows that the distance between any
point  and  satisfies 

If  overlaps with  and  overlaps with , and the two balls share at least one point, there must be a pair of points  and  such that , giving a contradiction.  Therefore if
 overlaps with some good set  and  overlaps with any other good set, . \qed


\end{proof}

\begin{lemma}\label{lemma:correctEstimate}
If  and the number of clustered points is at least , then the clustering output by Landmark-Clustering-Min-Sum using the threshold  must be a -clustering that contains a distinct good set in each cluster.

\end{lemma}

\begin{proof}
Our argument considers the points that are in each cluster that is output by the algorithm.  Let us call a good set \emph{covered} if any of the clusters  found so far contain points from it.  We will use  to refer to the clusters in  whose good sets are not \emph{covered}. It is critical to observe that if  then if  contains points from an \emph{uncovered} good set,  cannot overlap with any other good set.

To see this, let us order the clusters in  by decreasing size: , and let .  As before, define . Applying Lemma~\ref{lemma:noOverlap} with  we can see that for , a ball of radius  cannot overlap a good set in  and any other good set, and a ball containing points from a good set in  cannot share any points with a ball containing points from any other good set.  Because  we can also argue that by the time we reach  we must output some cluster.

Given this observation, it is clear that the algorithm can cover at most one new good set in each cluster that it outputs.  In addition, if a new good set is covered this cluster may not contain points from any other good set. If the algorithm is able to cluster at least  points, it must cover every good set because the size of each good set is larger than .  So it must report  clusters where each cluster contains points from a distinct good set. \qed

\end{proof}

\section{Experimental Results}

We present some preliminary results of testing our \emph{Landmark-Clustering-Min-Sum} algorithm on protein sequence data.  Instead of requiring all pairwise similarities between the sequences as input, our algorithm is able to find accurate clusterings by using only a few BLAST calls.  For each data set we first build a BLAST database containing all the sequences, and then compare only some of the sequences to the entire database.  To compute the distance between two sequences, we invert the bit score corresponding to their alignment, and set the distance to infinity if no significant alignment is found.  In practice we find that this distance is almost always a metric, which is consistent with our theoretical assumptions.


In our computational experiments we use data sets created from the Pfam \cite{pfam} (version 24.0, October 2009) and SCOP \cite{scop} (version 1.75, June 2009) classification databases.  Both of these sources classify proteins by their evolutionary relatedness, therefore we can use their classifications as a ground truth to evaluate the clusterings produced by our algorithm and other methods.  These are the same data sets that were used in the \cite{vbrtx} study, therefore we also show the results of the original \emph{Landmark-Clustering} algorithm on these data, and use the same amount of distance information for both algorithms ( landmarks/queries for each data set, where  is the number of clusters).  In order to run \emph{Landmark-Clustering-Min-Sum} we need to set the parameter .  Because in practice we do not know its correct value, we use increasing estimates of  until we cluster enough of the points in the data set; this procedure is similar to the algorithm for the case when we don't know the optimum objective value  and hence don't know . In order to compare a computationally derived clustering to the one given by the gold-standard classification, we use the distance measure from the theoretical part of our work.


Because our Pfam data sets are so large, we cannot compute the full distance matrix, so we can only compare with methods that use a limited amount of distance information.  A natural choice is the following algorithm: uniformly at random choose a set of landmarks , ; embed each point in a -dimensional space using distances to ; use -means clustering in this space (with distances given by the Euclidian norm).  This procedure uses exactly  one versus all distance queries, so we can set  equal to the number of queries used by the other algorithms.  For SCOP data sets we are able to compute the full distance matrix, so we can compare with a spectral clustering algorithm that has been shown to work very well on these data \cite{spectralClusteringProteinSeqs}.

From Figure~\ref{fig:figure1} we can see that \emph{Landmark-Clustering-Min-Sum} outperforms -means in the embedded space on all the Pfam data sets.  However, it does not perform better than the original \emph{Landmark-Clustering} algorithm on most of these data sets.  When we investigate the structure of the ground truth clusters in these data sets, we see that the diameters of the clusters are roughly the same.  When this is the case the original algorithm will find accurate clusterings as well \cite{vbrtx}.  Still, \emph{Landmark-Clustering-Min-Sum} tends to give better results when the original algorithm does not work well (data sets 7 and 9).

\begin{figure}
\begin{center}
\includegraphics[width=100mm,height=70mm]{figure1.jpg}
\caption{Comparing the performance of -means in the embedded space (blue), \emph{Landmark-Clustering} (red), and \emph{Landmark-Clustering-Min-Sum} (green) on 10 data sets from Pfam.  Datasets \textbf{1-10} are created by uniformly at random choosing 8 families from Pfam of size , . \label{fig:figure1}}
\end{center}
\end{figure}

Figure~\ref{fig:figure2} shows the results of our computational experiments on the SCOP data sets.  We can see that the three algorithms are comparable in performance here.  These results are encouraging because the spectral clustering algorithm significantly outperforms other clustering algorithms on these data \cite{spectralClusteringProteinSeqs}. Moreover, the spectral algorithm needs the full distance matrix as input and takes much longer to run.  When we examine the structure of the SCOP data sets, we find that the diameters of the ground truth clusters vary considerably, which resembles the structure implied by our approximation stability assumption, assuming that the target clusters vary in size.  Still, most of the time the product of the cluster sizes and their diameters varies, so it does not quite look like what we assume in the theoretical part of this work.

\begin{figure}
\begin{center}
\includegraphics[width=100mm,height=70mm]{figure2.jpg}
\caption{Comparing the performance of spectral clustering (blue), \emph{Landmark-Clustering} (red), and \emph{Landmark-Clustering-Min-Sum} (green) on 10 data sets from SCOP.  Data sets \textbf{A} and \textbf{B} are the two main examples from \cite{spectralClusteringProteinSeqs}, the other data sets (\textbf{1-8}) are created by uniformly at random choosing 8 superfamilies from SCOP of size , .
\label{fig:figure2}}
\end{center}
\end{figure}

We plan to conduct further studies to find data where clusters have different scale and there is an inverse relationship between cluster sizes and their diameters.  This may be the case for data that have many outliers, and the correct clustering groups sets of outliers together rather than assigns them to arbitrary clusters.  The algorithm presented here will consider these sets to be large diameter, small cardinality clusters.  More generally, the algorithm presented here is more robust because it will give an answer no matter what the structure of the data is like, whereas the original \emph{Landmark-Clustering} algorithm often fails to find a clustering if there are no well-defined clusters in the data.

\section{Conclusion}

We present a new algorithm that clusters protein sequences in a limited information setting.  Instead of requiring all pairwise distances between the sequences as input, we can find an accurate clustering using few BLAST calls.  We show that our algorithm produces accurate clusterings when compared to gold-standard classifications, and we expect it to work even better on data who structure more closely resembles our theoretical assumptions.


\bibliographystyle{alpha}
\bibliography{references}
\end{document}
