\documentclass{article}


\PassOptionsToPackage{numbers, compress, sort}{natbib}









\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{todonotes}
\usepackage{amsmath,bm,amsthm,amssymb,mathtools}
\usepackage{thmtools,thm-restate}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mleftright}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{nicematrix}
\usepackage[cmtip,all]{xy}



\newcommand{\bx}{\mathbf{x}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\bO}{\mathcal{O}}
\newcommand{\gph}{\mathcal{G}} \newcommand{\Fcoe}[1]{\widehat{#1}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\eigfm}[1][\ell]{\phi_{#1}}
\newcommand{\ws}[1][j]{\mathcal{S}^{(#1)}} \newcommand{\vj}[1][j]{v^{(#1)}} \newcommand{\wj}[1][j]{w^{(#1)}} 

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bf{a}}}
\def\vb{{\bf{b}}}
\def\vc{{\bf{c}}}
\def\vd{{\bf{d}}}
\def\ve{{\bf{e}}}
\def\vf{{\bf{f}}}
\def\vg{{\bf{g}}}
\def\vh{{\bf{h}}}
\def\vi{{\bf{i}}}
\def\vk{{\bf{k}}}
\def\vl{{\bf{l}}}
\def\vm{{\bf{m}}}
\def\vn{{\bf{n}}}
\def\vo{{\bf{o}}}
\def\vp{{\bf{p}}}
\def\vq{{\bf{q}}}
\def\vr{{\bf{r}}}
\def\vs{{\bf{s}}}
\def\vt{{\bf{t}}}
\def\vu{{\bf{u}}}
\def\vv{{\bf{v}}}
\def\vw{{\bf{w}}}
\def\vx{{\bf{x}}}
\def\vy{{\bf{y}}}
\def\vz{{\bf{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bf{A}}}
\def\mB{{\bf{B}}}
\def\mC{{\bf{C}}}
\def\mD{{\bf{D}}}
\def\mE{{\bf{E}}}
\def\mF{{\bf{F}}}
\def\mG{{\bf{G}}}
\def\mH{{\bf{H}}}
\def\mI{{\bf{I}}}
\def\mJ{{\bf{J}}}
\def\mK{{\bf{K}}}
\def\mL{{\bf{L}}}
\def\mM{{\bf{M}}}
\def\mN{{\bf{N}}}
\def\mO{{\bf{O}}}
\def\mP{{\bf{P}}}
\def\mQ{{\bf{Q}}}
\def\mR{{\bf{R}}}
\def\mS{{\bf{S}}}
\def\mT{{\bf{T}}}
\def\mU{{\bf{U}}}
\def\mV{{\bf{V}}}
\def\mW{{\bf{W}}}
\def\mX{{\bf{X}}}
\def\mY{{\bf{Y}}}
\def\mZ{{\bf{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\nup}{\gN_{\uparrow}}
\newcommand{\ndown}{\gN_{\downarrow}}

\newcommand{\da}{\downarrow}
\newcommand{\ua}{\uparrow}

\newcommand{\parents}{Pa} \let\ab\allowbreak

\newcommand{\tleq}{\trianglelefteq}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert} 

 
\DeclareMathOperator{\Tr}{\mathrm{Tr}}

\newcommand{\first}[1]{\mathbf{\textcolor{red}{#1}}}
\newcommand{\second}[1]{\mathbf{\textcolor{blue}{#1}}}
\newcommand{\third}[1]{\mathbf{\textcolor{violet}{#1}}}

\newtheorem*{remark}{Remark}


\title{Neural Sheaf Diffusion: 
A Topological Perspective on Heterophily and Oversmoothing in GNNs}





\author{Cristian Bodnar\thanks{Work done as a research intern at Twitter.} \\
University of Cambridge\\
\texttt{cristian.bodnar@cl.cam.ac.uk} \\
\And
Francesco Di Giovanni\thanks{Proved the results in Section~\ref{sec:harmonic_space}.} \\
Twitter \\
\texttt{fdigiovanni@twitter.com} \\
\AND
Benjamin P. Chamberlain \\
Twitter \\
\And
Pietro Li\`{o} \\
University of Cambridge \\
\And
Michael Bronstein \\
University of Oxford \& Twitter \\
}


\begin{document}


\maketitle


\begin{abstract}
Cellular sheaves equip graphs with a ``geometrical'' structure by assigning vector spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs) implicitly assume a graph with a trivial underlying sheaf. This choice is reflected in the structure of the graph Laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. In this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of GNNs in heterophilic settings and their oversmoothing behaviour. By considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. At the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than GNNs over their asymptotic behaviour. On the practical side, we study how sheaves can be learned from data. The resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding GNN models) and obtain competitive results in heterophilic settings. Overall, our work provides new connections between GNNs and algebraic topology and would be of interest to both fields.
\end{abstract}

\let\thefootnote\relax\footnotetext{Our code is available at \url{https://github.com/twitter-research/neural-sheaf-diffusion}.}
\begin{minipage}{0.5\textwidth}
\centering
    \includegraphics[width=0.8\textwidth]{fig/cellular_sheaf.pdf}
    \captionof{figure}{A sheaf  shown for a single edge of the graph. The \emph{stalks} are isomorphic to . The \emph{restriction maps} ,  and their adjoints move the vector features between these spaces. In practice, we learn the sheaf (i.e. the restrictions maps) from data via a parametric function .}
    \label{fig:sheaf}
\end{minipage}
\hspace{11pt}
\begin{minipage}{0.44\textwidth}
\centering
    \includegraphics[width=0.62\textwidth]{fig/parallel_transport.pdf}
    \captionof{figure}{Analogy between parallel transport on a sphere and transport on a discrete vector bundle (cellular sheaf). A tangent vector is moved from  and back. Because the vector returns in a different position, the transport is not path-independent.}
    \label{fig:sheaf_transport}
\end{minipage}

\section{Introduction}

Graph Neural Networks (GNNs) 
\cite{sperduti1994encoding,goller1996learning,gori2005new,scarselli2008graph,bruna2013spectral,defferrard2016convolutional,kipf2017graph,gilmer2017neural}
have recently become very popular in the ML community as a model of choice to deal with relational and interaction data due to their multiple successful applications in domains ranging from social science and particle physics to structural biology and drug design.  In this work, we focus on two main problems often observed in GNNs: their poor performance in heterophilic graphs \cite{zhu2020beyond} and their oversmoothing behaviour \cite{os1,os2}. The former arises from the fact that many GNNs are built on the strong assumption of {\em homophily}, i.e., that nodes tend to connect to other similar nodes. The latter refers to a phenomenon of some deeper GNNs producing features that are too smooth to be useful.  

\textbf{Contributions. } We show that these two fundamental problems are linked by a common cause: the underlying ``geometry'' of the graph (used here in a very loose sense). When this geometry is trivial, as is typically the case, the two phenomena described above emerge. We make these statements precise through the lens of (cellular) sheaf theory \citep{curry2014sheaves, shepard1985cellular, bredon2012sheaf, maclane2012sheaves, rosiak2021sheaf, ghrist2014elementary}, a subfield of algebraic topology and geometry. Intuitively, a cellular sheaf associates a vector space to each node and edge of a graph, and a linear map between these spaces for each incident node-edge pair (Figure \ref{fig:sheaf}).

In Section \ref{sec:diffusion_power_big}, we analyse how by considering a hierarchy of increasingly general sheaves, starting from a trivial one, a diffusion equation based on the sheaf Laplacian \citep{hansen2019toward} can solve increasingly more complicated node-classification tasks in the infinite time limit. In this regime, we show that oversmoothing and problems due to heterophily can be avoided by equipping the graph with the right sheaf structure for the task. In Section \ref{sec:oversmoothing}, we study the behaviour of a non-linear, parametric, and discrete version of this process. This results in a {\em Sheaf Convolutional Network}~\citep{hansen2020sheaf} that generalises Graph Convolutional Networks~\citep{kipf2017graph}. We prove that this discrete diffusion process is more flexible and has greater control over its asymptotic behaviour than GCNs~\citep{cai2020note, oono2019graph}. 
All these results are based on the properties of the harmonic space of the sheaf Laplacian, which we study from a spectral perspective in Section \ref{sec:harmonic_space}. We provide a new Cheeger-type inequality for the spectral gap of the sheaf Laplacian and note that these results might be of independent interest for spectral sheaf theory~\citep{hansen2019toward}. Finally, in Section \ref{sec:sheaf_learning}, we apply our theory to designing simple and practical GNN models.  
We describe how to construct Sheaf Neural Networks by learning sheaves from data, thus making these types of models applicable beyond the toy experimental setting where they were originally introduced~\citep{hansen2020sheaf}. The resulting models obtain competitive results both in heterophilic and homophilic graphs. 


\section{Background}

\textbf{Cellular Sheaves. } A \emph{cellular sheaf} \citep{curry2014sheaves, shepard1985cellular} over a graph (Figure \ref{fig:sheaf}) is a mathematical object associating a vector space to each node and edge in the graph and a map between these spaces for each incident node-edge pair. We define this formally below:

\begin{definition}
A cellular sheaf  on an undirected graph  consists of:
\begin{itemize}[leftmargin=10mm, topsep=0pt,itemsep=-0.4ex]
    \item A vector space  for each .
    \item A vector space  for each .
    \item A linear map  for each incident  node-edge pair.
\end{itemize}
\end{definition}
\vspace{-5pt}
The vector spaces of the nodes and edges are called \emph{stalks}, while the linear maps are referred to as \emph{restriction maps}. The space formed by all the spaces associated with the nodes of the graph is called the space of 0-{\em cochains} , where  denotes the direct sum of vector spaces.
For a 0-cochain , we use  to refer to the vector in  of node . \citet{hansen2021opinion} have constructed a convenient mental model for these objects based on opinion dynamics. In this context,  is the `private opinion' of node , while  expresses how that opinion manifests publicly in a `discourse space' formed by . A particularly important subspace of  is the space of \emph{global sections}  containing those private opinions  for which all neighbours  agree with each other in the discourse space. 
Given a cellular sheaf , we can define a {\em sheaf Laplacian} operator \citep{hansen2019toward} measuring the aggregated `disagreement of opinions' at each node:
\begin{definition}\label{def:laplacian}
The sheaf Laplacian of a sheaf  is a linear map  defined node-wise as . 
\end{definition}
\vspace{-5pt}
The sheaf Laplacian is a positive semi-definite block matrix (Figure \ref{fig:sheaf_laplacian}). The diagonal blocks are , while the non-diagonal blocks . Denoting by  the block-diagonal of , the normalised sheaf Laplacian is given by . For simplicity, we assume that all the stalks have a fixed dimension . In that case, the sheaf Laplacian is a  real matrix, where  is the number of nodes of . When the vector spaces are set to  (i.e.,  ) and the linear maps to the identity map over , the underlying sheaf is trivial and one recovers the well-known  graph Laplacian matrix and its normalised version . In general,  is preferred to  for most practical purposes due to its bounded spectrum and, therefore, we focus on the former. A cochain  is called {\em harmonic} if  or, equivalently, if . This means harmonic cochains are characterised by zero disagreements along all the edges of the graph, and it is not difficult to see that, in fact,  and  are isomorphic as vector spaces~\citep{hansen2021opinion}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/sheaf_laplacian.pdf}
    \caption{A graph \textit{(left)}, the Laplacian matrix of a sheaf with -dimensional stalks over the graph \textit{(middle)} , and a -cochain  represented as a block-vector stacking the vectors of all nodes  \textit{(right)}.}
    \label{fig:sheaf_laplacian}
    \vspace{-10pt}
\end{figure}

The sheaves with orthogonal maps (i.e.  the Lie group of  orthogonal matrices) provide a more geometric interpretation of sheaves and play an important role in our analysis. Such sheaves are called \emph{discrete}  \emph{bundles} and can be seen as a discrete version of vector bundles~\citep{zein2009local, scoccola2021approximate, gao2021geometry} from differential geometry~\citep{tu2011manifolds}. Intuitively, these objects describe vector spaces attached to the points of a manifold. In our discrete case, the role of the manifold is played by the graph, and the sheaf Laplacian describes how the elements of a vector space are transported via rotations in another neighbouring vector space similarly to how tangent vectors are moved across a manifold via parallel transport (connection; see Figure \ref{fig:sheaf_transport}). Due to this analogy, the sheaf Laplacian on  \emph{bundles} is also referred to as {\em connection Laplacian} \citep{singer2012vector}.

\textbf{Heat Diffusion and GCNs. } Consider a graph with adjacency matrix , diagonal degree matrix , normalised graph Laplacian , and an  feature matrix . We can define the heat diffusion equation and its Euler discretisation with a unit step as follows: 

Comparing this with the Graph Convolutional Network~\citep{kipf2017graph} model, we observe that GCN is an augmented heat diffusion process with an additional  weight matrix  and a nonlinearity :

From this perspective, it is perhaps not surprising that GCN is particularly affected by heterophily and oversmoothing since heat diffusion makes the features of neighbouring nodes increasingly smooth. In what follows, we consider a much more general and powerful family of (sheaf) diffusion processes leading to more expressive sheaf convolutions. 

\section{The Expressive Power of Sheaf Diffusion}\label{sec:diffusion_power_big}

\paragraph{Preliminaries.} Let us now assume  to be a graph with -dimensional node feature vectors . The features of all nodes are represented as a single vector  stacking all the individual -dimensional vectors (Figure \ref{fig:sheaf_laplacian}). Additionally, if we allow for  feature channels, everything can be represented as a matrix , whose columns are vectors in . We are interested in the spatially discretised {\em sheaf diffusion} process governed by the following PDE:

It can be shown that in the time limit, each feature channel is projected into  \citep{hansen2019toward}. As described above (up to a  normalisation), this space contains the signals that agree with the restriction maps of the sheaf along all the edges. Thus, sheaf diffusion can be seen as a `synchronisation' process over the graph, where all the private opinions converge towards global agreement. 

In this section, we investigate the expressive power of this process within the infinite time limit. Because the asymptotic behaviour of sheaf diffusion is determined by the properties of , in Section \ref{sec:harmonic_space}, we investigate when this subspace is non-trivial (i.e. it contains more than just the zero vector). In Section \ref{sec:diffusion_power}, we use this characterisation of the harmonic space to study what sort of sheaf diffusion processes will asymptotically produce projections into  that can linearly separate the classes for various kinds of graphs and initial conditions. Since diffusion converges exponentially fast, the following results are also relevant for models with finite integration time or layers. 

\subsection{Harmonic Space of Sheaf Laplacians}\label{sec:harmonic_space}

A major role in the analysis below is played by discrete vector bundles, and we concentrate on this case. We note though that our results below generalise to the general linear group , the Lie group of  invertible matrices, provided we can also control the norm of the restriction maps from below. Given a discrete -bundle,  and the block diagonal of  has a diagonal structure since , where  is the degree of node . Accordingly, if a signal , then the signal  and similarly for the inverse transformation.  


Key to our analysis is studying \emph{transport} operators induced by the restriction maps of the sheaf. Given nodes  and a path  from  to , we consider a notion of \emph{transport} from the stalk  to the stalk , 
constructed by composing restriction maps (and their transposes) along the edges: \vspace{-1mm}

For general sheaf structures, the graph transport is \emph{path dependent}, meaning that how the vectors are transported across two nodes depends on the path between them (see Figure \ref{fig:sheaf_transport}). In fact, we show that this property characterises the {\em spectral gap} of a sheaf Laplacian, i.e. the smallest eigenvalue of . 
\begin{restatable}{proposition}{UpperBoundEigenv}\label{prop:upper_bound_eigenv} If  is a discrete  bundle over a connected graph and  then we have 
.
\end{restatable}
A consequence of this result is that there is always a non-trivial harmonic space (i.e. ) if the transport maps generated by an orthogonal sheaf are \emph{path-independent} (i.e. ). Next, we address the opposite direction.\begin{restatable}{proposition}{CycleHarmonic}\label{prop:cycle_harmonic}
If  is a discrete  bundle over a connected graph and , then for any cycle  based at  we have 
.
\end{restatable}


This proposition highlights the interplay between the graph and the sheaf structure. A simple consequence of this result is that for any cycle-free subset , we have that any sheaf (or connection-) Laplacian restricted to  always admits a non-trivial harmonic space. A natural question connected to the previous result is whether a Cheeger-like inequality holds in the other direction. This turns out to be the case:

\begin{restatable}{proposition}{LowerBoundEigen}\label{prop:lower_bound_eigen}
Let  be a discrete  bundle over a connected graph  with  nodes and let  for all cycles . Then 
.
\end{restatable}
While the bound above is of little use in practice, it shows how the spectral gap of a sheaf Laplacian is indeed related to the deviation of the transport maps from being path-independent, as measured by . We note that the Cheeger-like inequality presented here is not unique, and other types of bounds on  have been derived \citep{bandeira2013cheeger}.  
We conclude this section by further analysing the dimensionality of the harmonic space of discrete -bundles:

\begin{restatable}{lemma}{BundleHDim}\label{lemma:bundle_h0_dim}
Let  be a discrete  bundle over a connected graph . Then  and  if and only if the transport is path-independent. 
\end{restatable}





\subsection{The Linear Separation Power of Sheaf Diffusion}\label{sec:diffusion_power}

In what follows, we use the results above to analyse the ability of certain classes of sheaves to linearly separate the features in the limit of the diffusion processes they induce. We utilise this as a proxy for the capacity of certain diffusion processes to avoid oversmoothing.  

\begin{definition}
A hypothesis class of sheaves with -dimensional stalks  has {\em linear separation power} over a family of graphs  if for any labelled graph , there is a sheaf  that can linearly separate the classes of  in the time limit of Equation \ref{eq:diffusion} for 
almost all initial conditions. 
\end{definition}

Note that the restriction to almost all initial conditions is necessary because, in the limit, diffusion behaves like a projection in the harmonic space and there will always be degenerate initial conditions (e.g. the zero matrix) that will yield a zero projection. We will now show how the choice of the sheaf impacts the behaviour of the diffusion process. For this purpose, we will consider a hierarchy of increasingly general classes of sheaves. 
\paragraph*{Symmetric invertible.} 
.
We note that for , the sheaf Laplacians induced by this class of sheaves coincides with the set of the well-known weighted graph Laplacians with strictly positive weights, which also includes the usual graph Laplacian (see proof in Appendix \ref{app:diffusion_power}). Therefore, this hypothesis class is of particular interest since it includes those graph Laplacians typically used by graph convolutional models such as GCN \citep{kipf2017graph} and ChebNet \citep{defferrard2016convolutional}. We first show that this class of sheaf Laplacians can linearly separate the classes in binary classification settings under certain homophily assumptions:

\begin{restatable}{proposition}{HOneSymHomophily}\label{prop:h1_sym_homophily}
Let  be the set of connected graphs  with two classes  such that for each , there exists  and an edge . Then  has linear separation power over .  
\end{restatable}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/static_animation_3C_top.png}
    \caption{Diffusion process on -bundles progressively separates the classes of the graph.}
    \vspace{-15pt} 
    \label{fig:d2_diff}
\end{figure}

In contrast, under certain heterophilic conditions, this hypothesis class is not powerful enough to linearly separate the two classes no matter what the initial conditions are: 

\begin{restatable}{proposition}{HOneSymHeterophily}\label{prop:h1_sym_heterophily}
Let  be the set of connected bipartite graphs , with partitions  forming two classes and . Then   cannot linearly separate the classes of any graph in  for any initial conditions .  
\end{restatable}


\paragraph*{Non-symmetric invertible.}
. This larger hypothesis class addresses the above limitation by allowing non-symmetric relations:

\begin{restatable}{proposition}{HOneLinearSeparation}\label{prop:h1_linear_separation}
Let  contain all the connected graphs  with two classes . Consider a sheaf  with  if  and  if  with  for all . Then the diffusion induced by  can linearly separate the classes of  for almost all initial conditions, and  has linear separation power over . 
\end{restatable}
\vspace{-5pt}
Since , the type of sheaf above can be interpreted as a discrete -bundle over a weighted graph with edge weights  and transport maps  for the inter-class edges and  for the intra-class edges. Intuitively, this type of transport, which is path-independent, polarises the features of the two classes and forces them to take opposite signs in the infinite limit. This provides a sheaf-theoretic explanation for why negatively-weighted edges have been widely adopted in heterophilic settings~\citep{yan2021two, chien2021adaptive, fagcn2021}. 

So far we have only studied the effects of changing the type of sheaves in dimension one. We now consider the effects of adjusting the dimension of the stalks and begin by stating a fundamental limitation of (sheaf) diffusion when . 
\begin{restatable}{proposition}{ImpossibleSeparation}\label{prop:impossible_separation}
Let  be a connected graph with  classes. Then,  cannot linearly separate the classes of  for any initial conditions . 
\end{restatable}
This is essentially a consequence of   in this case, by virtue of Lemma \ref{lemma:bundle_h0_dim}. From a GNN perspective, this means that in the infinite depth setting, sufficient \emph{stalk width} (i.e., dimension ) is needed in order to solve tasks involving more than two classes. Note that  is different from the classical notion of feature channels . As the result above shows, the latter has no effect on the linear separability of the classes in . Next, we will see that the former does. 


\paragraph*{Diagonal invertible.}
. 
The sheaves in this class can be seen as  independent sheaves from  encoded in the -dimensional diagonals of their restriction maps. This perspective allows us to generalise Proposition \ref{prop:h1_linear_separation} to a multi-class setting: 
\begin{restatable}{proposition}{DiagSeparation}\label{prop:diag_separation}
Let  be the set of connected graphs with nodes belonging to  classes. Then for ,  has linear separation power over .  
\end{restatable}
This result illustrates the benefits of using higher-dimensional stalks while maintaining a simple and computationally convenient class of diagonal restriction maps. Next, with more complex restriction maps, we can show that lower-dimensional stalks can be used to achieve linear separation in the presence of even more classes. 

\paragraph*{Orthogonal.}

 is the class of -bundles. 
Orthogonal maps are able to make more efficient use of the space available to them than diagonal restriction maps: 

\begin{restatable}{proposition}{OrthSeparation}\label{theo:orth_separation}
Let  be the class of connected graphs with  classes. Then, for all ,  has linear separation power over .  
\end{restatable}

Figure \ref{fig:d2_diff} includes an example diffusion process over an -bundle. 


\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
\textbf{Summary:} Different sheaf classes give rise to different behaviours of the diffusion process and, consequently, to different separation capabilities. Taken together, these results show that solving any node classification task can be reduced to performing diffusion with the right sheaf.    
\end{tcolorbox}
\vspace{-5pt}

\section{Expressive Power of Sheaf Convolutions}\label{sec:oversmoothing}

Analogously to how GCN augments heat diffusion, we can construct a \textbf{Sheaf Convolutional Network (SCN)} augmenting the sheaf diffusion process. In this section, we analyse the capacity of SCNs to change, \emph{if necessary}, their asymptotic behaviour compared to the base diffusion process. Since the sheaf structure will be ultimately learned from data, this is particularly important for the common setting when the learned sheaf is different from the ``ground truth'' sheaf for the task to be solved.  

The continous diffusion process from Equation \ref{eq:diffusion} has the Euler discretisation with unit step-size .
Assuming , we can equip the right side with weight matrices  and a non-linearity  to arrive at the following model originally proposed by \citet{hansen2020sheaf}:

where  are the number of input and output feature channels, and  denotes the Kronecker product. Here,  multiplies from the left the vector feature of all the nodes in all channels (i.e.  for all  and channels ), while  multiplies the features from the right and can adjust the number of feature channels, just like in GCNs. As one would expect, when using a trivial sheaf, ,  becomes a scalar and one recovers the GCN of \citet{kipf2017graph}. To see how SCNs behave compared to their base diffusion process, we investigate how SCN layers affect the \emph{sheaf Dirichlet energy} , which sheaf diffusion is known to minimise over time. 
\begin{definition}\label{def:energy}

\end{definition}
Similarly, for multiple channels the energy is . This is a measure of how close a signal  is to  and it is easy to see that . We begin by studying the sheaves for which the energy decreases and representations end up asymptotically in . Let  and denote by .  

\begin{restatable}{theorem}{OneDimOversmoothing}\label{theo:1dim_oversmoothing}
For  and  being (Leaky)ReLU, .
\end{restatable}

This generalises existent results for GCNs~\citep{cai2020note, oono2019graph} and proves that SCNs using this family of Laplacians, which includes all weighted graph Laplacians, exponentially converge to  if . In particular, if , then  and the representations remain trapped inside the kernel no matter what the norm of the weights is. Therefore, in settings as those described by Propositions \ref{prop:h1_sym_heterophily} and \ref{prop:impossible_separation}, the linear separation capabilities of this class of models are severely limited (see Corollaries \ref{cor:gcn_one}, \ref{cor:gcn_two} in Appendix \ref{app:diffusion_power}). 

Finally, the Theorem also extends to bundles with symmetric maps, :
\begin{restatable}{theorem}{GraphOversmoothing}\label{theo:graph_oversmoothing}
If  and , . 
\end{restatable}
In some sense, this is not surprising because, for this class,  contains the same information as the kernel of the classical normalised graph Laplacian (see Proposition \ref{prop:harmonicbundle} in Appendix \ref{app:energy_flow}). 

More generally, SCNs with sheaves outside , are much more flexible and can easily increase the Dirichlet energy using an arbitrarily small linear transformation :
\begin{restatable}{proposition}{SCNEnergyIncrease}\label{prop:scn_energy_increase}
For any connected graph  and , there exist a sheaf  ,  with  and feature vector  such that .
\end{restatable}
Importantly, this proves that this family of SCNs can, if necessary, escape the kernel of the Laplacian.

\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
\textbf{Summary:} Not only that sheaf diffusion is more expressive than heat diffusion as shown in Section \ref{sec:diffusion_power}, but SCNs are also more expressive than GCNs in the sense that they are generally not constrained to decrease the Dirichlet energy when using low-norm weights. This provides them with greater control than GCNs over their asymptotic behaviour.  
\end{tcolorbox}
\vspace{-5pt}

\section{Neural Sheaf Diffusion and Sheaf Learning}\label{sec:sheaf_learning}

In the previous sections, we discussed the various advantages provided by sheaf diffusion and sheaf convolutions. However, in general, the ground truth sheaf is unknown or unspecified. Therefore, we aim to learn the underlying sheaf from data end-to-end, thus allowing the model to pick the right geometry for solving the task. 

\textbf{Neural Sheaf Diffusion. } We propose the  diffusion-type model from Equation \ref{eq:cont_model}. We note that by setting  to identity and  with  small enough or simply , we recover (up to a scaling) the sheaf diffusion equation. Therefore, the model is at least as expressive as sheaf diffusion and benefits from all the positive properties outlined in Section \ref{sec:diffusion_power}. 

Crucially, the sheaf Laplacian  is that of a sheaf  that {\em evolves over time}. More specifically, the evolution of the sheaf structure is described by a learnable function of the data . This allows the model to use the latest available features to manipulate the underlying geometry of the graph and, implicitly, the behaviour of the diffusion process. Additionally, We use an MLP followed by a reshaping to map the raw features of the dataset to a matrix  of shape  and a final linear layer to perform the node classification. 

In our experiments, we focus on the time-discretised version of this model from Equation \ref{eq:disc_model}, which allows us to use a new set of weights at each layer  while maintaining the nice theoretical properties of the model above. 

We note that this model is different from the SCN model from Equation \ref{eq:scn} in two major ways. First, \citet{hansen2020sheaf} used a \emph{hand-crafted} sheaf with , constructed in a synthetic setting with full knowledge of the data-generating process. In contrast, we \emph{learn} a sheaf, which makes our model applicable to any real-world graph dataset, even in the absence of a sheaf structure. Additionally, motivated by our theoretical results, we use the full generality of sheaves by using stalks with  and higher-dimensional maps. Second, our model uses a residual parametrisation of the discretised diffusion process, which empirically improves its performance. 

\textbf{Sheaf Learning. } The restriction maps are learned using \emph{locally} available information. Each  matrix  is learned via a parametric matrix-valued function , with . 
This function must be non-symmetric to be able to learn asymmetric transport maps along each edge. 
In practice, we set  followed by a reshaping of the output, where  is a weight matrix. For simplicity, the equations above use a single feature channel, but in practice, all channels are supplied as input. More generally, we can show that if the function  has enough capacity and the features are diverse enough, we can learn any sheaf over a graph. 

\begin{restatable}{proposition}{SheafLearning}\label{prop:sheaf_learning}
Let  be a finite graph with features . Then, if  for any  and  is an MLP with sufficient capacity,  can learn any sheaf . 
\end{restatable}

First, this result formally motivates learning a sheaf at each layer since the model can learn to distinguish more nodes after each aggregation step. Second, this suggests that more expressive models (in the Weisfeiler-Lehman sense \citep{gin, morris2019weisfeiler, pmlr-v139-bodnar21a, bodnar2021b}) could learn a more general family of sheaves. We leave a deeper investigation of these aspects for future work. In what follows, we distinguish between several types of functions  depending on the type of matrix they learn.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig/biartite.png}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig/biartite_test.png}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig/transport_hist.png}
    \end{subfigure}
    \caption{(\textit{Left}) Train and (\textit{Middle}) test accuracy as a function of diffusion time. (\textit{Right}) Histogram of the learned scalar transport maps. The performance of the sheaf diffusion model is superior to that of weighted-graph diffusion and correctly learns to invert the features of the two classes.}
    \label{fig:bipartite_graph}
    \vspace{-12pt}
\end{figure}

\textbf{Diagonal. } 
The main advantage of this parametrisation is that fewer parameters need to be learned per edge, and the sheaf Laplacian ends up being a matrix with diagonal blocks, which also results in fewer operations in sparse matrix multiplications. The main disadvantage is that the  dimensions of the stalks interact only via the left  multiplication. 

\textbf{Orthogonal. } In this case, the model effectively learns a discrete vector bundle. Orthogonal matrices provide several advantages: (1) they can mix the various dimension of the stalks, (2) the orthogonality constraint prevents overfitting while reducing the number of parameters, (3) they have better understood theoretical properties, and (4) the resulting Laplacians are easier to normalise numerically since the diagonal entries correspond to the degrees of the nodes. In our model, we build orthogonal matrices from a composition of Householder reflections~\citep{mhammedi2017efficient}. 

\textbf{General. } Finally, we consider the most general option of learning arbitrary matrices. The maximal flexibility these maps provide can be useful, but it also comes with the danger of overfitting. At the same time, the sheaf Laplacian is more challenging to normalise numerically since one has to compute  for a positive semi-definite matrix . To perform this at scale, one has to rely on SVD, whose gradients can be infinite if  has repeated eigenvalues. Therefore, this model is more challenging to train. 

\textbf{Computational Complexity. } The GCN from Equation \ref{eq:gcn_diffusion} has complexity , where  is the number of channels and  the number of edges. Assume a sheaf diffusion model with stalk dimension  and  channels such that  (i.e. same representation size). Then, when the model uses diagonal maps, the complexity is . When using orthogonal or general matrices, the complexity becomes  (see Appendix \ref{app:complexity} for detailed derivations). 
In practice, we use  
, which effectively results in a constant overhead compared to GCN. 
\vspace{-10pt}

\section{Experiments}
\label{sec:exp}

\paragraph{Synthetic experiments.} We consider a simple setup given by a connected bipartite graph with equally sized partitions. We sample the features from two overlapping isotropic Gaussian distributions to make the classes linearly non-separable at initialisation time. From Proposition \ref{prop:h1_sym_heterophily}, we know that diffusion models using symmetric restriction maps cannot separate the classes in the limit, while a diffusion process using negative transport maps can. Therefore, we use two vanilla sheaf diffusion processes by setting , ,  and  in Equation \ref{eq:cont_model}. In both models, we learn a sheaf at  as a function of , and we keep the sheaf constant over time. For the first model, we learn a sheaf with general maps . For the second model, we use a similar layer but constraint , obtaining a weighted graph Laplacian.   

\begin{table*}[t]
    \centering
    \caption{Results on node classification datasets sorted by their homophily level. Top three models are coloured by \textbf{\textcolor{red}{First}}, \textbf{\textcolor{blue}{Second}}, \textbf{\textcolor{violet}{Third}}. Our models are marked {\bf NSD}.}
    \resizebox{1.0\textwidth}{!}{\begin{tabular}{l ccccccccc}
    \toprule 
         &
         \textbf{Texas} &  
         \textbf{Wisconsin} & 
         \textbf{Film} &
         \textbf{Squirrel} &
         \textbf{Chameleon} &
         \textbf{Cornell} &
         \textbf{Citeseer} & 
         \textbf{Pubmed} & 
         \textbf{Cora} \\
         
         Hom level &
         \textbf{0.11} &
         \textbf{0.21} & 
         \textbf{0.22} & 
         \textbf{0.22} & 
         \textbf{0.23} &
         \textbf{0.30} &
         \textbf{0.74} &
         \textbf{0.80} &
         \textbf{0.81} \\ 
         
         \#Nodes &
         183 &
         251 & 
         7,600 &
         5,201 & 
         2,277 &
         183 &
         3,327 &
         18,717 &
         2,708 \\
         
         \#Edges &
         295 &
         466 & 
         26,752 & 
         198,493 & 
         31,421 &
         280 &
         4,676 &
         44,327 &
         5,278 \\
         
         \#Classes &
         5 &
         5 & 
         5 &
         5 &
         5 & 
         5 &
         7 &
         3 &
         6 \\ \midrule
        
         \textbf{Diag-NSD} &
          &
          &
          & 
          & 
          &
          & 
          &
          &
         \\
         
         \textbf{-NSD} &
          &
          &
          & 
           & 
          &
          & 
          &
          &
          \\
         
         \textbf{Gen-NSD} &
          &
          &
          & 
          & 
          &
          & 
          &
          &
          \\ \midrule

         GGCN &
          &
          &
          &
          & 
          &
          &
          &
          &
          \\
         
         H2GCN &
          &
          &
          &
          & 
          &
          &
          &
          &
          \\

         GPRGNN &
          &
          &
          & 
          &
          &
          &
          &
          &
          \\ 
         
         FAGCN &
          &
          &
          &
          &
          & 
          &
         N/A & 
         N/A & 
         N/A \\
         
         MixHop &
          &
          &
          & 
          &
          &
          &
          &
          &
          \\

         GCNII &
          &
           &
          &
          &
          &
          &
          &
          &
          \\
         
         Geom-GCN &
          &
          &
          & 
          & 
          &
          &
          &
          &  
          \\ 
         
         PairNorm &
          &
          &
          & 
          & 
          &
          &
          &
          &
          \\ 
         
         GraphSAGE &
          &
          &
          & 
          & 
          &
          &
          &
          &
         \\
         
         GCN &
          &
          &
          & 
          &
          &
          &
          &
          &
          \\ 
         
         GAT &
          &
          &
          & 
          &
          &
          &
          &
          & 
          \\ 
         
         MLP &
          &
          &
          & 
          & 
          &
          &
          &
          &
          \\ 














         \bottomrule
         
    \end{tabular}
    }
    \vspace{-12pt}
    \label{tab:main_results}
\end{table*} 

Figure \ref{fig:bipartite_graph} presents the results across five seeds. As expected, for diffusion time zero (i.e. no diffusion), we see that a linear classifier cannot separate the classes. At later times, the diffusion process using symmetric maps cannot perfectly fit the data. In contrast, with the more general sheaf diffusion, as time increases and the signal approaches the harmonic space, the model gets better and the features become linearly separable. In the last subfigure, we take a closer look at the sheaf that the model learns in the time limit by plotting a histogram of all the transport (scalar) maps . In accordance with Proposition \ref{prop:h1_linear_separation}, the model learns a negative transport map for all edges. This shows that the model manages to avoid oversmoothing (see Appendix \ref{app:extra_experiments} for an experiment with ). \vspace{-5pt}

\paragraph{Real-world experiments.} We test our models on multiple real-world datasets~\citep{rozemberczki2021multi, pei2020geom, namata2012query, tang2009social, sen2008collective} with an edge homophily coefficient  ranging from  (very heterophilic) to  (very homophilic). Therefore, they offer a view of how a model performs over this entire spectrum. We evaluate our models on the 10 fixed splits provided by \citet{pei2020geom} and report the mean accuracy and standard deviation. Each split contains  of nodes per class for training, validation and testing, respectively. As baselines, we use an ample set of GNN models that can be placed in three categories: (1) classical: GCN~\citep{kipf2017graph}, GAT~\citep{velivckovic2017graph}, GraphSAGE~\citep{hamilton2017representation}; (2) models specifically designed for heterophilic settings: GGCN~\citep{yan2021two}, Geom-GCN~\citep{pei2020geom}, H2GCN~\citep{zhu2020beyond}, GPRGNN~\citep{chien2021adaptive}, FAGCN~\citep{fagcn2021}, MixHop~\citep{mixhop}; (3) models addressing oversmoothing: GCNII~\citep{pmlr-v119-chen20v}, PairNorm~\citep{Zhao2020PairNorm:}. All the results are taken from \citet{yan2021two}, except for FAGCN and MixHop, which come from \citet{lingam2021simple} and \citet{zhu2020beyond}, respectively. All of these were evaluated on the same set of splits as ours. In Appendix \ref{app:extra_experiments} we also include experiments with continuous GNN models. 
\vspace{-5pt}

\paragraph{Results.} From Table \ref{tab:main_results} we see that our models are first in  benchmarks with high heterophily () and second-ranked on the remaining one (i.e. Chameleon). At the same time, NSD also shows strong performance on the homophilic graphs by being within approximately 1\% of the top model. Overall, NSD models are among the top three models on  datasets. 
The -bundle diffusion model performs best overall confirming the intuition that it can better avoid overfitting, while also transforming the vectors in sufficiently complex ways. We also remark on the strong performance of the model learning diagonals maps, despite the simpler functional form of the Laplacian.

\section{Related Work, Discussion, and Conclusion}

\paragraph{Sheaf Neural Networks \& Sheaf Learning.} Sheaf Neural Networks \citep{hansen2020sheaf} with a \emph{hand-crafted} sheaf Laplacian were originally introduced in a toy experimental setting. Since then, they have remained completely unexplored, and we hope this paper will fill this lacuna. In contrast to \citep{hansen2020sheaf}, we provide an ample theoretical analysis justifying the use of sheaves in Graph ML and study for the first time how sheaves can be {\em learned from data} using neural networks. Furthermore, we present the first successful application of Sheaf Neural Networks on real-world datasets. \citet{hansen2019learning} have also considered learning a sheaf Laplacian by minimising directly in matrix space a regularised Dirichlet energy metric. Different from their approach, we learn the sheaf as part of an end-to-end model and use an efficient parametrisation that is independent of the size of the graph. 

Follow-up works have also experimented with inferring a connection Laplacian directly from data at pre-processing time~\citep{barbero2022sheaf}, combining sheaves with attention~\citep{barbero2022sheaf_att}, and designing models based on the wave equation on sheaves~\citep{suk2022surfing}. Besides the sheaf Laplacians employed in all these works and ours, one can also use higher-order sheaf (connection) Laplacians that operate on higher-order tensors. These were shown to encode important information about the underlying symmetries in the data~\citep{pfau2020disentangling}, which hints at the powerful data properties that Sheaf Neural Networks could potentially extract from these operators. 




\paragraph{Heterophily and Oversmoothing.} While good empirical designs jointly addressing these two problems have been proposed before~\citep{chien2021adaptive, yan2021two}, \citet{yan2021two} is the only other work connecting the two theoretically. Their analysis~\citep{yan2021two} is very different in terms of methods and assumptions and, therefore, their results are completely orthogonal. Concretely, the authors analyse the performance of linear SGCs~\citep{SGC} (i.e. GCN without nonlinearities) on random attributed graphs. In contrast, our analysis is not probabilistic, focuses on diffusion PDEs and also extends to GCNs in the non-linear regime. Furthermore, we employ a new set of mathematical tools from cellular sheaf theory, which brings a new language and new tools to analyse these problems. Perhaps the only commonality is that both works find evidence for the benefits of negatively signed edges in GNNs, although with different mathematical motivations. At the same time, other recent works~\citep{luan2021heterophily, du2022gbk} have shown that GCNs with finite layers (typically one) can perform well in heterophilic graphs (including bipartite). This is in no contradiction with our results, which consider an \emph{infinite time/layer} regime (i.e. not finite) and \emph{perfect} linear separation (i.e. a model that cannot fit the data can still achieve high accuracy).  

\paragraph{Category Theory and GNNs.} From the perspective of category theory~\citep{mac2013categories}, cellular sheaves are a \emph{functor} from a \emph{category} describing the incidence structure of the graph to a \emph{category} describing the data living on top of the graph. Informally, this says that the vertices and edges are mapped to some type of data (e.g. vector spaces) and the incidence relations between vertices and edges are mapped to some type of relation between the assigned data (e.g. linear maps between the vector spaces). The generality provided by this perspective could be used to extend the models described in this work to more exotic types of data such as lattices and their associated sheaf Laplacians~\citep{ghrist2020cellular}. At the same time, our work echoes other recent efforts to place GNNs on a categorical foundation~\citep{de2020natural, dudzik2022graph}. 

\paragraph{Message Passing Neural Networks.} The layer from Equation \ref{eq:disc_model} can be seen as a form of GNN-FiLM layer~\citep{brockschmidt2020gnn, perez2018film}, where each node learns a linear message function conditioned on the features of the neighbours. Such models have been recently shown to perform well empirically in heterophilic settings~\citep{palowitch2022graphworld}. At the same time, the model bares an algorithmic resemblance to GAT~\citep{velivckovic2017graph}. For a central node  and a neighbouring node , GAT learns an attention coefficient , while our model learns a matrix given by the block  of . Finally, a message-passing procedure based on parallel transport has also been proposed by \citet{haan2021gauge} in the context of geometric graphs (meshes). In the absence of a natural geometric structure on arbitrary graphs, in our case, the transport structure is learned from data end-to-end.    

\paragraph{Limitations and societal impact.} One of the main limitations of our theoretical analysis is that it does not address the generalisation properties of sheaves, but this remains a major impediment for the entire field of deep learning. Nonetheless, our setting was sufficient to produce many valuable insights about heterophily and oversmoothing and a basic understanding of what various types of sheaves can and cannot do. Much more work remains to be done in this direction, and we expect to see further cross-fertilization between ML and algebraic topology in the future. Finally, due to the theoretical nature of this work, we do not foresee any immediate negative societal impacts.  


\paragraph{Conclusion.} In this work, we used cellular sheaf theory to provide a novel topological perspective on heterophily and oversmoothing in GNNs. We showed that the underlying sheaf structure of the graph is intimately connected with both of these important factors affecting the performance of GNNs. To mitigate this, we proposed a new paradigm for graph representation learning where models not only evolve the features at each layer but also the underlying geometry of the graph. In practice, we demonstrated that this framework achieves competitive results in heterophilic settings.  
\vspace{-7pt}

\newpage
\begin{ack}

We are grateful to Iulia Duta, Dobrik Georgiev and Jacob Deasy for valuable comments on an earlier version of this manuscript. CB would also like to thank the Twitter Cortex team for making the research internship a fantastic experience. This research was supported in part by ERC Consolidator grant No. 724228 (LEMAN). 

\end{ack}








\bibliography{ref}
\bibliographystyle{plainnat}
































\newpage
\appendix

\section{Harmonic Space Proofs}

\UpperBoundEigenv*
\begin{proof}
We first note that on a discrete  bundle the degree operator  since by orthogonality .
We can use the Rayleigh quotient to characterize  as 

\noindent Fix  and choose a minimal path  for all . For an arbitrary non-zero , consider the signal 
and we set .

\noindent where we have again used that the maps are orthogonal. Since  we find that the right hand side can be bound from above by . Therefore, by using Definition \ref{def:energy} we finally obtain

\noindent Since the transport maps are all orthogonal we get

\noindent We conclude that

\end{proof}

\CycleHarmonic*
\begin{proof}
Assume that  and consider  and any cycle based at  denoted by . According to the Hodge Theorem we have that 

\noindent By composing all the maps we find:

\noindent which completes the proof.
\end{proof}

\LowerBoundEigen*
\begin{proof}
If  there is nothing to prove. Assume that . By Proposition \ref{prop:cycle_harmonic} we derive that the harmonic space is trivial and hence . Consider a \emph{unit} eigenvector  and let  such that  for . There exists a cycle  based at  such that  for otherwise we could extend  to any other node independently of the path choice and hence find a non-trivial harmonic signal. In particular, we can assume this cycle to be non-degenerate, otherwise if there existed a non-trivial degenerate loop contained in  that does not fix  we could consider this loop instead of  for our argument. Let us write this path as  and consider the rescaled signal . By assumption we have 

\noindent By iterating the approach above we find:


\noindent From Definition \ref{def:energy} we derive that the last term can be bounded from above by . Therefore, we conclude:

\noindent By construction we get , meaning that

\end{proof}
 
\BundleHDim*
\begin{proof}
We first note that the argument below extends to weighted -bundles as well. Let . According to Proposition \ref{prop:cycle_harmonic}, given , we see that  for any path . It means that the harmonic space is uniquely determined by the choice of . Explicitly, given any cycle  based at , we know that . If the transport is everywhere path-independent, then the kernel coincides with the whole stalk  and hence we can extend any basis  to a basis in  via the transport maps, i.e. . If instead there exists a transport map over a cycle  with non-trivial fixed points, then  and hence . 
\end{proof} 

\section{Proofs for the Power of Sheaf Diffusion}\label{app:diffusion_power}

\begin{definition}
Let  be a weighted graph, where  is a matrix with  for all ,  for all , and  is an edge if and only if . 
\end{definition}

The graph Laplacian of a weighted graph is , where  is the diagonal matrix of weighted degrees (i.e. ). Its normalised version is . 

\begin{proposition}
Let  be a graph. The set  is isomorphic to the set of all possible weighted graph Laplacians over . 
\end{proposition}

\begin{proof}
We prove only one direction. Let  be a choice of valid weight matrix for the graph . We can construct a sheaf  such that for all edges  we have that . Then,  and . The equality for the normalised version of the Laplacians follows directly.
\end{proof}

We state the following Lemma without proof based on Theorem 3.1 in \citet{hansen2021opinion}. 

\begin{lemma}\label{lemma:harmonic_convergence}
Solutions  to the diffusion in Equation \ref{eq:diffusion} converge as  to the orthogonal projection of  onto . 
\end{lemma}

Due to this Lemma, the proofs below rely entirely on the structure of  that one obtains for certain . 

\HOneSymHomophily*
\begin{proof}
Let  be a graph with two classes  such that for each , there exists  and an edge . Additionally, let  be any channel of the feature matrix . 

We can construct a sheaf  as follows. For all nodes  and edges , . For all  and edge , set . Otherwise, set . 

Denote by  the number of neighbours of node  in the same class as  . Note that based on the assumptions,  if . Then the only harmonic eigenvector of  is:

Denote its unit-normalised version . In the limit of the diffusion process, the features converge to  by Lemma \ref{lemma:harmonic_convergence}. Assuming, , which is nowhere dense in  and, without loss of generality, that , for sufficiently large ,  for all . 
\end{proof}

\HOneSymHeterophily*
\begin{proof}
Let  be a bipartite graph with  and let  be any channel of the feature matrix . 

Consider an arbitrary sheaf . Since the graph is connected, the only harmonic eigenvector of  is  with  (i.e. the square root of the weighted degree). Based on Lemma \ref{lemma:harmonic_convergence}, the diffusion process converges in the limit (up to a scaling) to . For the features to be linearly separable we require that  and, without loss of generality, for all  that . 

Suppose for the sake of contradiction there exists a sheaf in  with such a harmonic eigenvector. Then, because : 

However, because , we have  and the sum above is zero. 
\end{proof}


\HOneLinearSeparation*
\begin{proof}
Let  be a connected graph with two classes . Additionally, let  be any channel of the feature matrix . Any sheaf of the  described type has a single harmonic eigenvector by virtue of Lemma \ref{lemma:bundle_h0_dim}, and it has the form:

Assume , which is nowhere dense in  and, without loss of generality, that . Then,  for all .  
\end{proof}

Next, we showed that using signed relations is necessary in  and simply using positive asymmetric relations is not sufficient in this dimension. 

\begin{definition}
The class of sheaves over  with non-zero maps, one-dimensional stalks, and similarly signed restriction maps 
\end{definition}

\begin{proposition}\label{prop:h1_with_same_sign}
Let  be the connected graph with two nodes belonging to two different classes. Then  cannot linearly separate the two nodes for any initial conditions . 
\end{proposition}

\begin{proof}
Let  be the connected graph with two nodes . Then any sheaf  has restriction maps of the form  and (without loss of generality) . As before, the only (unnormalized) harmonic eigenvector for a sheaf of this form is . Since this is a constant vector, the two nodes are not separable in the diffusion limit. 
\end{proof}

We state the following result without a proof (see Exercise 4.1 in \citet{bishop}).  

\begin{lemma}\label{lemma:convex_hull_separability}
Let  and  be two sets of points in . If their convex hulls intersect, the two sets of points cannot be linearly separable. 
\end{lemma}

\ImpossibleSeparation*
\begin{proof}
If the sheaf has a trivial global section, all features converge to zero in the diffusion limit. Suppose  is non-trivial. Since  is connected and all the restriction maps are invertible, by Lemma \ref{lemma:bundle_h0_dim}, . 

In that case, let  be the unit-normalised harmonic eigenvector of . By Lemma \ref{lemma:harmonic_convergence}, for any node , its scalar feature in channel  is given by . Note that we can always find three nodes  belonging to three different classes such that . Then, there exists a convex combination , with . Therefore:

Since this is true for all channels , it follows that . Because  is in the convex hull of the points belonging to other classes, by Lemma \ref{lemma:convex_hull_separability}, the class of  is not linearly separable from the other classes.  
\end{proof}

\DiagSeparation*
\begin{proof}
Let  be a connected graph with  classes and , an arbitrary sheaf in . Because  has diagonal restriction maps, there is no interaction during diffusion between the different dimensions of the stalks. Therefore, the diffusion process can be written as  independent diffusion processes, where the -th process uses a sheaf  with all stalks isomorphic to  and  for all  and incident edges . Therefore, we can construct  sheaves  with  as in Proposition \ref{prop:h1_linear_separation}, where (in one vs all fashion) the two classes are given by the nodes in class  and the nodes belonging to the other classes. 

It remains to restrict that the projection of  on any of the harmonic eigenvectors of  in the standard basis is non-zero. Formally, we require  for all positive integers . Since  is nowhere dense in ,  belongs to the direct sum of dense subspaces, which is dense. 
\end{proof}

\begin{lemma}\label{lemma:orth_bundle_0eigenspace}
Let  be a graph and  a (weighted) orthogonal vector bundle over  with path-independent parallel transport and edge weights . Consider an arbitrary node  and denote by  the -th standard basis vector of . Then  form an orthogonal eigenbasis for the harmonic space of , where:

\end{lemma}

\begin{proof}
First, we show that  is harmonic. 

For orthogonality, notice that for any  and , it holds that:

\end{proof}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/orth_separation_proof.jpeg}
        \caption{Aligning the features of each class with the axis of coordinates in a 2D space. Dotted lines indicate linear decision boundaries for each class.}
        \label{fig:orth_separation_proof}
    \end{subfigure}
    \hspace{40pt}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/reg_orth_separation_proof.jpeg}
        \caption{Separating an arbitrary number of classes when the graph is regular. Dotted line shows an example decision boundary for one of the classes.}
        \label{fig:reg_orth_separation_proof}
    \end{subfigure}
    \caption{Proof sketch for Lemma \ref{lemma:orth_separation_d2} and Proposition \ref{prop:regular_orth_separation}.}
    \label{fig:orth_separation_all_proofs}
\end{figure}

\begin{lemma}\label{lemma:swap_rotations}
Let  be two 2D rotation matrices and  the two standard basis vectors of . Then .
\end{lemma}

\begin{proof}
The angle between  and  is . Letting  be the positive rotation angles of the two matrices, the first inner product is equal to  while the second is . The result follows from applying the trigonometric identity .  
\end{proof}

We first prove Theorem \ref{theo:orth_separation} in dimension two in the following lemma and then we will look at the general case. 

\begin{lemma}\label{lemma:orth_separation_d2}
Let  be the class of connected graphs with  classes. Then,  has linear separation power over .  
\end{lemma}

\begin{proof} Idea: We can use rotation matrices to align the harmonic features of the classes with the axis of coordinates as in Figure \ref{fig:orth_separation_proof}. Then, for each side of each axis, we can find a separating hyperplane separating each class from all the others. 

Let  be a connected graph with  classes. Denote by  the following set of rotation matrices together with their signed-flipped counterparts:

and by  the set of all class labels. Then, fix a node  and construct an injective map  assigning each class label one of the signed basis vectors such that , where  denotes the class of node . 
 
Then, we can construct a sheaf  in terms of certain parallel transport maps along each edge, that will depend on . For all nodes  and edges , . For each , we set . Then for all , set . It is easy to see that the resulting parallel transport is path-independent because it depends purely on the classes of the endpoints of the path. 

Based on Lemma \ref{lemma:orth_bundle_0eigenspace}, the -th eigenvector of  is  with . Now we will show that the projection of  in this subspace will have a configuration as in Figure \ref{fig:orth_separation_proof} up to a rotation.

Let  be two nodes belonging to two different classes. Denote by . Then the inner product between the features of nodes  in the limit of the diffusion process is: 

It can be checked that by substituting the transport maps  with any  from  such that , the inner product above is zero. Similarly, substituting any , the inner product is , which is equal to the product of the norms of the two vectors. Therefore, the diffused features of different classes are positioned at  from each other, as in Figure \ref{fig:orth_separation_proof}. 
\end{proof}

\OrthSeparation*
\begin{proof}
To generalise the proof in Lemma \ref{lemma:orth_separation_d2}, we need to find a set  of size  containing rotation matrices that make the projected features of different classes be pairwise orthogonal for any projection coefficients . For that, each term in Equation \ref{eq:inner_product} must be zero for any coefficients . 

Therefore,  must satisfy the following requirements:
\begin{enumerate}
    \item , since transport for neighbours in the same class must be the identity. Therefore,  for all .  
    \item Since  for all  and , it follows that the diagonal elements of  are zero. 
    \item From  for all  and point (2) it follows that . Therefore,  for all . 
    \item We have  for all  and . Together with (3), it follows that the diagonal elements of  are zero. 
    \item We have  for all , and , with . Together with point (4) it follows that . Similarly, from point (3) we have that . Therefore, the two matrices are anti-commutative: .
\end{enumerate}

We remark that points (1), (3), (5) coincide with the defining algebraic properties of the algebra of complex numbers, quaternions, octonions, sedenions and their generalisations based on the Cayley-Dickson construction \citep{schafer2017introduction}. Therefore, the matrices in  must be a representation of one of these algebras. Firstly, such algebras exist only for  that are powers of two. Secondly, matrix representations for these algebras exist only in dimensions two and four. This is because the algebra of octonions and their generalisations, unlike matrix multiplication, is non-associative. As a sanity check, note that the matrices  from Lemma \ref{lemma:orth_separation_d2} are a well-known representation of the unit complex numbers.  

We conclude this section by giving out the matrices for , which are the real matrix representations of the four unit quaternions:

It can be checked that these matrices respect the properties outlined above. Thus, in , we can select the transport maps from the set  containing eight matrices, which also form a group. Therefore, following the same procedure as in Lemma \ref{lemma:orth_separation_d2}, we can linearly separate up to eight classes.  
\end{proof}

\begin{proposition}\label{prop:regular_orth_separation}
Let  be the class of connected regular graphs with a finite number of classes. Then,  has linear separation power over . 
\end{proposition}

\begin{proof}
Idea: Since the graph is regular, the harmonic features of the nodes will be uniformly scaled and thus positioned on a circle. The aim is to place different classes at different locations on the circle, which would make the classes linearly separable as shown in Figure \ref{fig:reg_orth_separation_proof}. 

Let  be a regular graph with  classes and define . Denote by  the 2D rotation matrix:

Then let  the set of rotation matrices with an angle multiple of . Then we can define a bijection  and a sheaf  as in the proof above. Checking the inner-products from Equation \ref{eq:inner_product} between the harmonic features of the nodes, we can verify that the angle between any two classes is different from zero. By Lemma \ref{lemma:swap_rotations}, the cross terms of the inner product vanish:

Thus, the angle between classes  is . 
\end{proof}



\section{Energy Flow Proofs}\label{app:energy_flow}

\begin{proposition}\label{prop:harmonicbundle}
If  is an -bundle in , then  if and only if  for all .
\end{proposition}

\begin{proof}[\textbf{Proof of Proposition \ref{prop:harmonicbundle}}] Let . Then we have

\noindent The last term vanishes if and only if  for each .
\end{proof}


\SCNEnergyIncrease*

\begin{proof}
Let  be an -bundle over  and . Assume that  for each  and that  with . Then there exist a linear map  with  and  such that . We sketch the proof. Let . Define then  by

\noindent Then . If we now take  the rescaled orthogonal projection in the orthogonal complement of the kernel of  we verify the given claim.
\end{proof}

We provide below a proof for the equality in Definition \ref{def:energy}. 
\begin{proposition}

\end{proposition}

\begin{proof}We prove the result for the normalised sheaf Laplacian, and other versions can be obtained as particular cases. 

Note that  is symmetric for any node  and so is any . Therefore, the two vectors in the parenthesis are the transpose of each other and the result is their inner product. Thus, we have:

The result follows identically for other types of Laplacian. For the augmented normalized Laplacian, one should simply replace  with  and for the non-normalised Laplacian, one should simply remove  from the equation. \end{proof}

\GraphOversmoothing*
\begin{proof}
We first prove a couple of Lemmas before proving the Theorem. The proof structure follows that of \citet{cai2020note}, which in turn generalises that of \citet{oono2019graph}. The latter proof technique is not directly applicable to our setting because it makes some strong assumptions about the harmonic space of the Laplacian (i.e. that the eigenvectors of the harmonic space have positive entries). 

, where  are the smallest and largest non-zero eigenvalues of .

\begin{lemma}
For , .
\end{lemma}

\begin{proof}
We can write  as a sum of the eigenvectors  of . Then , where  are the eigenvalues of . 

The inequality follows from the fact that the eigenvectors of the normalised sheaf Laplacian are in the range  \citep[Proposition 5.5]{hansen2019toward}. We note that the original proof of \citet{cai2020note} bounds the expression by  instead of , which appears to be an error. 
\end{proof}

\begin{lemma}

\end{lemma}

\begin{proof}
Following the proof of \citet{cai2020note} we have:

\end{proof}

\begin{lemma}\label{lemma:left_weight_energy_decrease}
For conditions as in Theorem \ref{theo:graph_oversmoothing}, . 
\end{lemma}
\begin{proof}
First, we note that for orthogonal matrices,  \citep[Lemma 4.4]{hansen2019toward}


The proof can also be extended easily extended to vector bundles over weighted graphs (i.e. allowing weighted edges as in \citet{hansen2019toward}). For the non-normalised Laplacian, the assumption that  is orthogonal can be relaxed to being non-singular and then the upper bound will also depend on the maximum conditioning number over all . 
\end{proof}

\begin{lemma}\label{lemma:activation_energy_decrease}
For conditions as in Theorem \ref{theo:graph_oversmoothing}, .
\end{lemma}

\begin{proof}
\end{proof}

Combining these three lemmas for an entire diffusion layer proves the Theorem. 
\end{proof}

\OneDimOversmoothing*
\begin{proof}
If , then Lemma \ref{lemma:left_weight_energy_decrease} becomes superfluous as  becomes a scalar that can be absorbed into the right-weights. It remains to verify that a version of Lemma \ref{lemma:activation_energy_decrease} holds in this case.

\begin{lemma}
For conditions as in Theorem \ref{theo:1dim_oversmoothing}, .
\end{lemma}

\begin{proof}

\end{proof}
We note that if  (i.e. the relation is signed), then it is very easy to find counter-examples where ReLU does not work anymore. However, the result still holds in the deep linear case. 
\end{proof}

If the features of an SCN/GCN oversmoothing as in Theorem \ref{theo:1dim_oversmoothing} converge to , then the model will no longer be able to linearly separate the classes. This is shown by the following Corollaries. 


\begin{corollary}\label{cor:gcn_one}
Consider an SCN model  with  layers and a sheaf  over a bipartite graph  as in Proposition \ref{prop:h1_sym_heterophily}. Then for any finite ,  is not linearly separable for any input with .  
\end{corollary}

\begin{proof}
By Theorem \ref{theo:1dim_oversmoothing}, if , then  and, therefore,  for any column . The proof of Proposition \ref{prop:h1_sym_heterophily} showed that the classes of such a bipartite graph cannot be linearly separated for any such feature matrix . 
\end{proof}

\begin{corollary}\label{cor:gcn_two}
Consider an SCN model  with  layers and a sheaf  over any graph  with more than two classes as in Proposition \ref{prop:impossible_separation}. Then for any finite ,  is not linearly separable for any input with .  
\end{corollary}

\begin{proof}
By Theorem \ref{theo:1dim_oversmoothing}, if , then  and, therefore,  for any column . The proof of Proposition \ref{prop:impossible_separation} showed that the classes of such a graph cannot be linearly separated for any such feature matrix . \end{proof}

\section{Sheaf Learning Proof}

\SheafLearning*
\begin{proof}
Assume that the node features are -dimensional and, therefore, the graph feature matrix has shape . Define the finite set  containing the concatenated features of the nodes for all the oriented edges  of the graph. Then, because each  is unique, for any dimension , there exists a (well-defined) function  sending . We now show that this function can be extended to a smooth function  and, therefore, it can be approximated by an MLP due to the Universal Approximation Theorem~\citep{hornik1989multilayer, hornik1991approximation}.

Let  be an index set for the elements of . Then, because  is finite, for any , we can find a sufficiently small neighbourhood  such that  and  for . Furthermore, for each , we can find a (smooth) bump function  such that  and  if . Then, the function  is smooth and .
\end{proof}

\section{Additional model details and hyperparameters}\label{app:hyperparams}

\paragraph{Hybrid transport maps.} Consider the transport maps  appearing in the off-diagonal entires of the sheaf Laplacian . When learning a sheaf Laplacian, there exists the risk that the features are not sufficiently good in the early layers (or in general) and, therefore, it might be useful to consider a hybrid transport map of the form , where  is the direct sum of two matrices and  represents a fixed (non-learnable map). In particular, we consider maps of the form  which essentially appends a diagonal matrix with  and  on the diagonal to the learned matrix. From a signal processing perspective, these correspond to a low-pass and a high-pass filter that could produce generally useful features. We treat the addition of these fixed parts as an additional hyper-parameter.  

\paragraph{Adjusting the activation magnitudes.} We note that in practice we find it useful to learn an additional parameter  (i.e. a vector of size ) in the discrete version of the models:

This allows the model to adjust the relative magnitude of the features in each stalk dimension. This is used across all of our experiments in the discrete models. 

\paragraph{Augmented normalised sheaf Laplacian.} Similarly to GCN which normalises the Laplacian by the augmented degrees (i.e. , where  is the usual diagonal matrix of node degrees), we similarly use  for normalisation to obtain greater numerical stability. This is particularly helpful when learning general sheaves as it increases the numerical stability of SVD. 

\begin{table}[h]
    \centering
    \caption{Hyper-parameter ranges for the discrete and continous models.}
    \resizebox{0.9\textwidth}{!}{\begin{tabular}{l| cc}
    \toprule 
    & 
    \textbf{Discrete Models} &
    \textbf{Continous Models} \\ 
    \midrule 
    
    Hidden channels &  (WebKB) and  (others)        &  \\
    Stalk dim              &  &  \\
    Layers          &  & N/A \\
    Learning rate   &  (WebKB) and  (others) & Log-uniform  \\
    Activation      & ELU & ELU \\
    Weight decay (regular parameters) & Log-uniform  & Log-uniform  \\
    Weight decay (sheaf parameters) & Log-uniform  & Log-uniform  \\
    Input dropout & Uniform  & Uniform  \\
    Layer dropout & Uniform  & N/A \\
    Patience (epochs) &  (Wiki) and  (others) & 50 \\
    Max training epochs &  (Wiki) and  (others) & 50. \\
    Integration time & N/A & Uniform . \\
    Optimiser & Adam~\citep{kingma2014adam} & Adam \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:hyper-range}
\end{table} 


\paragraph{Hyperparameters and training procedure.} We train all models for a fixed maximum number of epochs and perform early stopping when the validation metric has not improved for a pre-specified number of patience epochs. We report the results at the epoch where the best validation metric was obtained for the model configuration with the best validation score among all models. We use the hyperparameter optimisation tools provided by Weights and Biases~\citep{wandb} for this procedure. The complete hyperparameter ranges we optimised over can be found in Table \ref{tab:hyper-range}. All models were trained and fine-tuned on an Amazon AWS p2.xlarge machine containing 8 NVIDIA K80 GPUs and using a 2.3 GHz (base) and 2.7 GHz (turbo) Intel Xeon E5-2686 v4 Processor.

\subsection{Computational Complexity}\label{app:complexity}

We can split the computational complexity into the following computational steps:
\begin{enumerate}
    \item \textbf{The linear transformation . }  is a  matrix and  is an  matrix. Therefore, the complexity is . 
    \item \textbf{Message Passing. } Since  is a sparse matrix, the message passing is implemented as a sparse-dense matrix multiplication . When the restriction maps are diagonal, the complexity of this operation is , since the multiplication of each block matrix in  and block vector in  reduces to a an element-wise vector multiplication. When the restriction maps are non-diagonal, the complexity is  because each matrix-vector multiplication is  and we need to perform  of them for each node and edge. 
    \item \textbf{Learning the Sheaf. } Assume we learn the restriction maps via , where  converts the  matrix into a -sized vector. This operation has to be performed for each incident node-edge pair. Therefore, the complexity is , when learning diagonal maps since  is a  matrix. When learning a non-diagonal matrix, the number of rows of  is  and the complexity becomes . Note, however, that in general, the complexity of learning the restriction maps can be significantly reduced to  (in the diagonal case) and  (in the non-diagonal case) by, for instance, using an MLP with constant hidden-size. 
    \item \textbf{Constructing the Laplacian. } To build the Laplacian, we need to perform the matrix-matrix multiplications involved in computing each of the blocks. The complexity of that is  in the diagonal case and  in the non-diagonal case. Computing the normalisation of the Laplacian is  in the diagonal case and  in the non-diagonal case.
\end{enumerate}

Putting everything together, the final complexity is  in the diagonal case and  in the non-diagonal case. When learning the sheaf via an MLP with constant hidden size, the complexity reduces to  (same as GCN) and , respectively. 

For learning orthogonal matrices, we rely on the library Torch Householder \citep{obukhov2021torchhouseholder} which provides support for fast transformations with large batch sizes. 
\begin{figure*}[t]
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig/2d_biartite.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig/2d_biartite_test.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig/2d_transport_hist.png}
    \end{subfigure}
    \caption{(\textit{Left}) Train accuracy as a function of diffusion time. (\textit{Middle}) Test accuracy as a function of diffusion time. (\textit{Right}) Histogram of the learned rotation angle of the  transport maps. The performance of the bundle model is superior to that of the one-dimensional sheaf. The transport maps learned by the model are aligned with our expectation: the model learns to rotate more (i.e. to move away) the neighbours belonging to different classes than the neighbours belonging to the same class. }
    \label{fig:3d_synthetic}
\end{figure*}

\section{Additional Experiments}\label{app:extra_experiments}

In this section, we provide a series of additional experiments and ablation studies. 


\paragraph{Two-dimensional synthetic experiment.} In the main text we focused on a synthetic example involving sheaves with one-dimensional stalks. We now consider a graph with three classes and two-dimensional features, with edge homophily level . We use  of the nodes for training and  for testing. First, we know that a discrete vector bundle with two-dimensional stalks that can solve the task in the limit exists from Theorem \ref{theo:orth_separation}, while based on Proposition \ref{prop:impossible_separation} no sheaf with one-dimensional stalks can perfectly solve the tasks. 

Therefore, similarly to the synthetic experiment in the main text, we compare two similar models learning the sheaf from data: one using  stalks and another using  stalks. As we see from Figure \ref{fig:3d_synthetic}, the discrete vector bundle model has better training and test-time performance than the one-dimensional counterpart. Nonetheless, none of the two models manages to match the perfect performance of the ideal sheaf on this more challenging dataset. From the final subfigure, we also see that the model learns to rotate more across the heterophilic edges in order to push away the nodes belonging to other classes. The prevalent angle of this rotation is  radians, which is just under , where  is the number of classes. Thus the model learns to position the three classes at approximately equal arc lengths from each other for maximum linear separability.

\paragraph{Continous Models.} To also understand how the continuous version of our models performs against other PDE-based GNNs we include a category of such SOTA models:
CGNNs~\citep{xhonneux2020continuous}, GRAND~\citep{chamberlain2021grand}, and BLEND~\citep{chamberlain2021blend}. Results are included in Table~\ref{tab:cont_table}. Generally, continuous models do not perform as well as the discrete ones because they are constrained to use the same set of weights for the entire integration time and cannot use dropout. Therefore, the model capacity is difficult to increase without overfitting. Nonetheless, our continuous models generally outperform other state-of-the-art continuous models, which also share the same limitations. Finally, we note that the baselines were fine-tuned over the same hyper-parameter ranges as in Table \ref{tab:hyper-range}

\begin{table*}[t]
    \centering
    \caption{Results on node classification datasets sorted by their homophily level. Top three models are coloured by \textbf{\textcolor{red}{First}}, \textbf{\textcolor{blue}{Second}}, \textbf{\textcolor{violet}{Third}}. Our models are marked {\bf NSD}.}
    \resizebox{1.0\textwidth}{!}{\begin{tabular}{l ccccccccc}
    \toprule 
         &
         \textbf{Texas} &  
         \textbf{Wisconsin} & 
         \textbf{Film} &
         \textbf{Squirrel} &
         \textbf{Chameleon} &
         \textbf{Cornell} &
         \textbf{Citeseer} & 
         \textbf{Pubmed} & 
         \textbf{Cora} \\
         
         Hom level &
         \textbf{0.11} &
         \textbf{0.21} & 
         \textbf{0.22} & 
         \textbf{0.22} & 
         \textbf{0.23} &
         \textbf{0.30} &
         \textbf{0.74} &
         \textbf{0.80} &
         \textbf{0.81} \\ 
         
         \#Nodes &
         183 &
         251 & 
         7,600 &
         5,201 & 
         2,277 &
         183 &
         3,327 &
         18,717 &
         2,708 \\
         
         \#Edges &
         295 &
         466 & 
         26,752 & 
         198,493 & 
         31,421 &
         280 &
         4,676 &
         44,327 &
         5,278 \\
         
         \#Classes &
         5 &
         5 & 
         5 &
         5 &
         5 & 
         5 &
         7 &
         3 &
         6 \\ 






























\midrule
         
        \textbf{Cont Diag-NSD} &
          &
          &
          & 
          & 
          &
          & 
          &
          &
         \\
         
         \textbf{Cont} \textbf{-NSD} &
          &
          &
          & 
          & 
          &
          & 
          &
          &
         \\
         
         \textbf{Cont Gen-NSD} &
          &
          &
          & 
          & 
          &
          & 
          &
          &
          \\  
         
         \midrule
         
        BLEND &
           &
           &
           & 
           & 
           &
          & 
          &
          &
          \\ 
         
         GRAND &
           &
           &
           & 
           & 
           &
           & 
          &
          &
         \\
        
         CGNN &
           &
           &
           & 
           & 
           &
           & 
           &
           &
          \\
         
         \bottomrule
         
    \end{tabular}
    }
    \label{tab:cont_table}
\end{table*} 

\paragraph{Positional encoding ablation.} Based on Proposition \ref{prop:sheaf_learning} we proceed to analyse the impact of increasing the expressive power of the model by making the nodes more distinguishable. For that, we equip our datasets with additional features consisting of graph Laplacian positional encodings as originally done in \citet{dwivedi2020benchmarkgnns}. In Table \ref{tab:pos_enc} we see that positional encodings do indeed improve the performance of the continuous models compared to the numbers reported in the main table. Therefore, we conclude that the interaction between the problem of sheaf learning and that of the expressivity of graph neural networks represents a promising avenue for future research. 

\begin{table*}[ht]
    \centering
    \caption{Ablation study for positional encodings. Positional encodings improve performance on some of our models.}
    \resizebox{0.7\textwidth}{!}{\begin{tabular}{l cccc}
    \toprule 
         & 
         \textbf{Eigenvectors} &
         \textbf{Texas} &  
         \textbf{Wisconsin} & 
         \textbf{Cornell} \\
    
    \midrule
         
    \multirow{3}{*}{Cont Diag-SD} & 
    0 & 82.97  4.37 &  & 80.00  6.07 \\ & 
    2 &  & 85.69  3.73 & 81.62  8.00 \\ & 
    8 &  & 86.28  3.40 &  \\ & 
    16 & 82.70  3.86 & 85.88  2.75 & 81.08  7.25 \\ \midrule
    
    \multirow{3}{*}{Cont -SD} & 
    0 & 82.43  5.95 & 84.50  4.34 & 72.16  10.40 \\ &
    2 & 84.05  5.85 & 85.88  4.62 & 83.51  9.70\\ & 
    8 &  &  &  \\ & 
    16 & 83.78  6.16 & 85.88  2.88 & 83.51  6.22 \\ \midrule
    
     \multirow{3}{*}{Cont Gen-SD} & 
    0 &  & 85.29  3.31 &  \\ &
    2 & 83.24  4.32 & 84.12  3.97 & 81.08  7.35 \\ & 
    8 & 82.70  5.70 & 84.71  3.80 & 83.24  6.82 \\ & 
    16 & 82.16  6.19 &  & 82.16  6.07 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:pos_enc}
\end{table*} 

\paragraph{Visualising diffusion.} To develop a better intuition of the limiting behaviour of sheaf diffusion for node classification tasks we plot the diffusion process using an oracle discrete vector bundle for two graphs with  (Figure \ref{fig:sheaf_diffusion_3C}) and  (Figure \ref{fig:sheaf_diffusion_4C}) classes. The diffusion processes converge in the limit to a configuration where the classes are rotated at  from each other, just like in the cartoon diagrams of Figure \ref{fig:orth_separation_all_proofs}. Note that in all cases, the classes are linearly separable in the limit. 

We note that this approach generalises to any number of classes, but beyond  it is not guaranteed that they will be linearly separable in . However, they are still well separated. We include an example with  classes in Figure \ref{fig:sheaf_diffusion_10C}.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/static_animation.png}
    \caption{Sheaf diffusion process disentangling the  classes over time. The nodes are coloured by their class.}
    \label{fig:sheaf_diffusion_4C}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/static_animation_3C.png}
    \caption{Sheaf diffusion process disentangling the  classes over time. The nodes are coloured by their class.}
    \label{fig:sheaf_diffusion_3C}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/static_animation_10C.png}
    \caption{Sheaf diffusion process disentangling the  classes over time. The nodes are coloured by their class.}
    \label{fig:sheaf_diffusion_10C}
\end{figure}


\end{document}