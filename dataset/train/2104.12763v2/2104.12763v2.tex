\appendix\newpage
\onecolumn

\section{Model details and hyperparameters}
\label{sec:appendix_hyperparameters}

\paragraph{Pre-training hyperparameters}
MDETR follows the pre-train then fine-tune strategy by first training on our constructed combined dataset for 40 epochs followed by fine-tuning on the respective downstream task. We train our model using AdamW~\cite{loshchilovDecoupledWeightDecay2019}, a variant of Adam~\cite{kingmaAdamMethodStochastic2017} better suited for weight decay. We use exponential moving average (EMA) with a decay rate of 0.9998, and a weight-decay of . The backbone and the transformer have a constant learning rate of respectively  and  for 35 epochs, after which their learning rate is reduced by a factor of 10. For the language model's learning rate, we use a linear decay with warmup schedule, increasing linearly to  during the first 1\% of the total number of steps, then decreasing linearly back to 0 for the rest of the training.
\begin{figure*}[h]
 \centering
 \includegraphics[width=0.82\textwidth]{parts/images/MDETR_loss_discrete.png}
 \caption{Illustration of the soft-token classification loss. For each object, the model predicts a distribution over the token positions in the input sequence. The weight of the distribution should be equally spread over all the tokens that refer to the predicted box.}
 \label{fig:soft_loss}
\end{figure*}

\paragraph{Flickr30k} Our results on Flickr30k are evaluated using the pre-trained model, without any additional fine-tuning as we found that it brings no additional gains.
For evaluation, we must rank the boxes associated with each phrase. Since there might be several phrase in the same sentence, we must provide a ranking for each and every such phrase. To that end, we use the prediction from the soft-token classification loss. In the example depicted in Fig \ref{fig:soft_loss}: to rank the boxes for the phrase ``a cat", we use the probability mass that each query assigns to the positions that correspond to ``a cat" in the sentence ``a cat with white paws jumps over a fence in front of a yellow tree" (in this example, the first few tokens). Through this approach,  the red box is found to be the highest-ranked box. On the other hand, if we want the boxes corresponding to ``the fence", we sort them according to the corresponding token positions, and in this case we find the green box as the top-scoring one.

\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.2\textwidth}
        \centering
        \includegraphics[height=2.5in]{parts/images/flickr_ex1.png}
        \caption{``one small boy climbing a pole with the help of another boy on the ground"}
    \end{subfigure}\hspace{2mm}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[height=2.5in]{parts/images/flickr_ex4.png}
        \caption{``A man talking on his cellphone next to a jewelry store"}
    \end{subfigure} \\
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[height=2in]{parts/images/flickr_ex2.png}
        \caption{``A man in a white t-shirt does a trick with a bronze colored yo-yo"}
    \end{subfigure}
    \caption{Examples of phrase grounding on the Flickr30k dataset}
    \label{fig:flickr_examples}
\end{figure*}




\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/phrase_cut_lamp.png}
        \caption{Query: ``street lamp"}
    \end{subfigure}\begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/phrase_cut_logo.png}
        \caption{Query: ``major league logo"}
    \end{subfigure} 
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/phrasecut_zebras.png}
        \caption{Query: ``zebras on savanna"}
    \end{subfigure}
    \caption{Qualitative segmentation examples on the phrasecut dataset
    \label{fig:phrasecut_examples}}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/refclevr_ex1.png}
        \caption{Query: ``Any other things that are the same color as the partially visible thing(s)"}
    \end{subfigure}\begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/refclevr_ex2.png}
        \caption{Query: ``Any other things that are the same material as the first one of the object(s) from right"}
    \end{subfigure} 
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/refclevr_ex3.png}
        \caption{Query: ``The second one of the shiny object(s) from front that are to the left of the second one of the metallic thing(s) from front"}
    \end{subfigure}
    \caption{Qualitative example from the CLEVR-REF+ dataset. When the model predicts a box that is referred to, we display it in green. The other boxes are intermediate reasoning steps and are depicted in blue.
    \label{fig:phrasecut_examples}}
\end{figure*}



\paragraph{Referring Expression Comprehension} For the Referring Expression Comprehension task, there is a stark difference in how the data is presented to the model in terms of density of annotation. For all other datasets that we use in our pre-training, each noun phrase in the sentence is annotated with its respective box, if available. On the other hand, in RE comprehension, the task is to align the whole referring expression with the corresponding box, possibly by needing to disambiguate between different occurrences of the same category of object. In other words, our model now needs to predict one box per expression. A problem with this setting is that our box-token contrastive alignment as well as soft token prediction losses get very diluted signal if we align the whole sentence to the box. To alleviate this, we pre-process the text using Spacy \cite{spacy} to extract the root of the sentence using a dependency parser. The tokens from this root phrase are used to align to the box.
Fine-tuning on this dataset is therefore crucial for good performance. We fine-tune for 5 epochs on the RefCOCO, RefCOCO+ and RefCOCOg datasets with a learning rate of  for the backbone and  for the rest of the network. We use a learning rate drop by a factor of 10 after 3 epochs. For the text encoder we use a learning rate of , with a linear decay with warmup schedule, warming up over the first 1\% of steps and then decaying to 0 linearly. At inference time, to detect a given expression, we feed it to the model alongside the image. We then rank the 100 detected boxes according to the probability that the box corresponds to an actual object (as opposed to a ``no object"). If  is the probability mass assigned to the ``no object" label, then we rank by increasing order of , or equivalently by decreasing order of . We show an example in Fig \ref{fig:refexp_examples} of the box predicted by our model for the corresponding referring expressions. 
In addition there is quite some variety in the type of text annotations from the three datasets. Both RefCOCO and RefCOCO+ were collected in a timed game setting whereas RefCOCOg was not. This led to differences in the length and diversity of language used in the different datasets. RefCOCO+ disallowed usage of location words to describe objects or disambiguate between multiple occurrences of the same object, focusing more on appearance based descriptions. RefCOCOg consists of expressions more than twice the length (on average) of the others and with more flowery and descriptive language.

We also fine-tuned the EfficientNetb5 model on these datasets but did not see much improvement over the EfficientNetB3 model, and we believe this is due to the smaller size of these datasets causing the larger model to overfit. 

\paragraph{PhraseCut}
Detection: For this phase, we use a batch size of 64, a learning rate of  for the text encoder and backbone and  for the rest of the network, and exponential moving average (EMA) of the network weights with a decay of 0.9998. segmentation: For this stage, we use a lr of  and no EMA. See \cite{carion2020end} for additional details. 


\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=40mm,]{parts/images/refcoco_ex.png}
        \caption{``brown bear"}
    \end{subfigure}\hspace{-3mm}
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=60mm]{parts/images/refcoco+_ex.png}
        \caption{``zebra facing away"}
    \end{subfigure} \\
    \begin{subfigure}[t]{0.3\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{parts/images/refcocog_ex1.png}
        \caption{``the man in the red shirt carrying baseball bats"}
    \end{subfigure}
    \hspace{2mm}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth,scale=0.5]{parts/images/refcocog_ex2.png}
        \caption{``the front most cow to the right of the other cows"}
    \end{subfigure}\caption{Examples from RefCOCO, RefCOCO+ and RefCOCOg datasets. Fig(a) taken from RefCOCO, Fig(b) from RefCOCO+ and Fig(c) and (d) are taken from RefCOCOg, in which the expressions are much longer on average and contain more descriptive language than in RefCOCO and RefCOCO+. Even when the expressions are long, we train our model to align the box to the root of the phrase, for eg. ``the man" in (c). The model however, still has access to the whole text and uses it to disambiguate between the two men in the image.
    \label{fig:refexp_examples}}
\end{figure*}








\section{CLEVR Experiments}
\label{appendix_section:CLEVR_results_detailed}
\subsection{Dataset details}
\label{sec:clevr_dataset_details}
The CLEVR dataset consists of 3D-rendered scenes containing between 3 and 10 objects of various shapes, material, size and color. Each of these scenes is associated with about 10 questions that are formulated about the visible objects, generated from a fixed set of templates. Each question is guaranteed to be answerable, and the annotations further provide a functional program that describe how to compute the answer using elementary reasoning steps. The total training set contains 70k images and slightly less than 700k questions. Overall, the visual aspect of this task, \emph{ie} the scene parsing, is not really challenging by modern standards, since the set of objects is limited, unambiguous, and there are no visual distractors. The only challenging cases occur in the event of heavy occlusion, where it might be hard to make out the shape of the occluded object, or in some cases where the question requires comparing ambiguous spatial relations (eg. asking which object is the closest to the camera in a setting where they are visually nearly tied). On the other hand, the text understanding aspect is more involved, since the questions can be quite complex, involving up to 20 reasoning steps.
Unlike several successful approaches to CLEVR~\cite{johnson_inferring_2017, Hudson2018CompositionalAN, mascharkaTransparencyDesignClosing2018, yiNeuralSymbolicVQADisentangling2019} , MDETR doesn't incorporate any special inductive bias to cope with such complex reasoning tasks. In this section, we show that despite its relatively straight-forward formulation, our approach competes with state-of-the-art models on the question answering task.

The first ingredient required for training MDETR is bounding box annotations for objects in the image. The original CLEVR dataset doesn't provide any, so we use the scene graphs from the dataset to re-create the original scene in the 3D-renderer Blender, then use some of its functionalities to extract the segmentation masks of the visible parts of the objects, and deduce the bounding boxes from that. The main complication is that the original rendering involved some non-deterministic jittering of the camera's position and rotation, leading to some potential discrepancies in the computed boxes. To minimize the error, we use the known 3D position of each object, as well as their known 2D location in the rendered image to optimize the camera parameters using a gradient-based approach. The final boxes obtained using this approach are accurate within a 10-pixel error margin, which we deem appropriate for our purposes.

The second ingredient required is the alignment between bounding boxes and tokens in the question. MDETR is trained to predict \textit{only} objects that are referred to in the question. For example, in the question ``What is the color of the cube in front of the small cylinder?", we provide an annotation for both the small cylinder (an intermediate step) and the cube (the main subject), and none of the other objects present in the scene. We use the functional programs that are part of the original CLEVR annotations to extract this set of objects, along with their corresponding text tokens in the original question.

\setlength{\tabcolsep}{4pt}
\begin{table*}[t]
\begin{center}
\small
 \begin{tabular}{cccccccccccccc} 
 \toprule
Method & \multicolumn{6}{c}{CLEVR} & \multicolumn{2}{c}{CLEVR-Humans} & \multicolumn{2}{c}{CoGenT} &CLEVR-Ref+ \\ [0.5ex]
        & Overall & Count & Exist & Comp. Num & Query     & Comp. Att   & Before FT & After FT & TestA & TestB& Acc  \\
 \midrule 
 MAttNet\cite{yu_mattnet_2018}&-&-&-&-&-&-&-&-&-&-&60.9\\
 MGA-Net\cite{Zheng_2020_ACCV}&-&-&-&-&-&-&-&-&-&-&80.1\\
 FiLM\cite{perezFiLMVisualReasoning2017} &97.7&94.3&99.1&96.8&99.1&99.1 &56.6 & 75.9 & 98.3 & \textbf{78.8}&-\\

 MAC \cite{Hudson2018CompositionalAN}   & 98.9    & 97.1 & 99.5  & 99.1 & 99.5 & 99.5 & 57.4 & 81.5 & - & - &-\\
 NS-VQA\cite{yiNeuralSymbolicVQADisentangling2019} & \textbf{99.8} & \textbf{99.7} & \textbf{99.9} & \textbf{99.8} & 99.8 & 99.8 & -&67.8&\textbf{99.8} & 63.9 &-\\
 OCCAM \cite{wangInterpretableVisualReasoning2020} & 99.4 & 98.1 & 99.8 & 99.0 & 99.9 & 99.9 & - & - & - & - &- \\
 MDETR & 99.7 & 99.3 & \textbf{99.9} & 99.4 & \textbf{99.9} & \textbf{99.9} & \textbf{59.9} & \textbf{81.7} & \textbf{99.8} & 76.7 &\textbf{100}\\
\bottomrule
\end{tabular}
\caption{Results on CLEVR-based datasets. We report accuracies on the test set of CLEVR, including the detail by question type. On CLEVR-Humans, we report accuracy on the test set before and after fine-tuning. On CoGenT, we report performance when the model is trained in condition A, without finetuning on condition B. On CLEVR-Ref+, we report the accuracy on the subset where the referred object is unique. *indicates method uses external program annotations}
\label{tab:clevrresults}
\end{center}
\end{table*}




\begin{figure*}
 \centering
 \includegraphics[width=\textwidth]{parts/images/MDETR_qa2.pdf}
 \caption{During MDETR pre-training, the model is trained to detect all objects mentioned in the question. To extend it for question answering, we provide QA specific queries in addition to the object queries as input to the transformer decoder. We use specialized heads for different question types.}
 \label{fig:QA}
\end{figure*} 

\subsection{Training details}
\label{sec:clevr_training_details}
\textbf{Model} We use a ResNet-18~\cite{he2016deep} from Torchvision, pre-trained on ImageNet~\cite{ILSVRC15} as the convolutional backbone. For the text-encoder, we use a pre-trained DistilRoberta~\cite{Sanh2019DistilBERTAD} from HuggingFace \cite{Wolf2020TransformersSN}.
The final transformer is the same as DETR, with 6 encoder layers as well as 6 decoder layers, and 8 attention heads in the attention layers. We reduce the number of object queries to 25, since the maximum number of objects to be detected is low.

\textbf{Pre-training} We first train the model only on the modulated detection objective, on our CLEVR-Medium subset, for 30 epochs. Following DETR training procedure, the transformer and the backbone use a learning rate of respectively  and , and we reduce them by a factor of 10 at the 20th epoch. The text encoder uses a linear decay with warmup schedule, with a warm-up to  over the first 1\% of the training steps. 

\textbf{QA-finetuning} For question answering, we take our pre-trained checkpoint, add the (untrained) question queries and their corresponding heads, then train on the full CLEVR dataset for 30 epochs, following the exact same learning rate schedule, with both the modulated detection as well as question answering losses.  As depicted in Fig.12, we use additional queries in the transformer decoder to answer each type of question in CLEVR: numerical, binary and attributes. We supervise each of these heads using a standard cross-entropy loss. We monitor the accuracy on the validation set to apply early-stopping. Finally, for CLEVR-Humans, we further fine-tune for 60 epochs, with the learning-rate drop occuring at epoch 40.

\subsection{Results and discussion}
The results are collected in Table \ref{tab:clevrresults}. On CLEVR, we closely match the performance of NS-VQA\cite{yiNeuralSymbolicVQADisentangling2019}, a method that uses external supervision in the form of ground-truth program annotations, and clearly surpass the performance of methods which like us, don't use this extra supervision signal. We then evaluate the generalization capability of our model. 

\textbf{CLEVR-Humans} \cite{johnson_inferring_2017} is a dataset of human-generated questions on CLEVR images. It tests the robustness of the model to new vocabulary and and different reasoning primitives. In a zero-shot setting, we improve substantially over the best competing model. We credit this improvement to our pre-trained language model. After fine-tuning, the gap narrows, suggesting that additional developments may be required to further improve performance. 

\textbf{CoGenT} is a test for compositional generalization. The evaluation protocol consists in training on a set A, where the spheres can be any color but the cubes are either gray, blue, brown or yellow, and the cylinders are red,
green, purple or cyan. We then evaluate in a zero-shot manner on a split B which has the opposite color-shape pairs for cubes and cylinders. Similar to other models, we observe a significant generalization gap. On closer inspection, the biggest drop in accuracy occurs on questions querying the shape of an object (from 99.98\% on testA to 34.68\% on testB), suggesting that the model has learnt strong spurious biases between shape and color. 

\textbf{CLEVR-REF+} Finally, we evaluate our model on CLEVR-REF+\cite{liuCLEVRRefDiagnosingVisual2019}, a referring expression comprehension dataset built on CLEVR images. For each object query, we train an additional binary head to predict whether or not the query corresponds to an object being referred to (as opposed to an auxiliary object in the sentence, that we detect as well). Following \cite{liuCLEVRRefDiagnosingVisual2019}, we evaluate accuracy on the subset of expressions that refer to a unique object, measured as whether the top ranked box has an IoU of at least 0.5 with the target box. Using the aforementioned binary prediction to rank the boxes, our model correctly ranks in first position a valid box for each of the examples of the validation set, leading to an accuracy of 100\%, greatly outperforming prior work.


\subsection{Ablations}
\label{sec:clevr_ablations}
We use CLEVR as a test bed to ablate several aspects of our model. Depending on the ablation, we report either the accuracy on the question answering task on the validation set of CLEVR, and/or the detection performance on this dataset, measured as a class-agnostic Average Precision (AP). When inspecting the modulated detection capabilities of the model, we use class agnostic Average Precision (AP) to evaluate the model. In the unconditional detection case, DETR is able to detect all boxes perfectly. When evaluated on the task of modulated detection, the AP metric therefore captures the model's capability for text understanding since now only the boxes relevant to the query must be detected. 
To put this in context, a model that detects all boxes even when given a text query (thereby ignoring the text completely) gets an AP of around 60. The goal is to achieve an AP close to 100 which would imply the model only finds the relevant boxes. 

\subsubsection{Loss ablations}
We first ablate the various parts of our loss. The results are summarized in Table~\ref{tab:clevr_abl_med} We report modulated detection results on the CLEVR-Medium subset, that we constructed by removing data-points from CLEVR where the same object is referred to by distinct parts of the question. In these ablations, we consider only the performance of the detector, and not the question answering capability. As a result, the queries related to question answering are not present and we do not propagate any QA related loss.

\textbf{Contrastive loss} We first ablate the impact of the contrastive loss, by training a model without it. In this situation, the alignment must occur solely through the soft-token classification loss. 
As shown in Table~\ref{tab:clevr_abl_med}, removing this loss results in a drastic drop in AP. More specifically, when evaluating the model, it becomes apparent that it is able to filter the objects based on some attributes (in particular their shape and size) but not others (in particular color and texture). It is unclear what drives the model in this local sub-optima, nor what statistical shortcut it is leveraging to correctly identify shapes and sizes. However, it shows that solely predicting the spans of the text query associated with each object is not sufficient to learn proper alignment. The contrastive loss, which forces object-queries to be similar to their corresponding text-token, is thus necessary.

\textbf{Soft-token classification loss}
We now study whether predicting the text-spans associated with each object is necessary, provided that we propagate the contrastive loss. Instead of predicting a distribution over span, we construct a simplified version of MDETR which only predicts a binary label for each object query: ``object" or ``no-object" (). This formulation is equivalent to the vanilla DETR classification loss, with one object class. We observe similar results as the previous ablation, namely a sharp decline in AP and a model that only understands half of the attributes correctly. We thus conclude that both ingredients of our loss are indeed required.

\subsubsection{Question answering ablations}

Finally in Table~\ref{tab:clevr_abl_hard} we ablate two aspects of our training recipe that differ with previous approaches:
\begin{itemize}
    \item \textbf{Curriculum}: We evaluate a model trained directly on the full CLEVR training set, without our modulated detection pretraining on CLEVR-medium. Similarly as the previous section, the model learns to detect only a subset of attributes, leading to poor QA accuracy.
    \item \textbf{Single QA head} In our approach, we train a specialized head for each type of question (numerical, binary, or categorical over the attributes). This differs from previous approaches that usually cast it as a single classification over all possible answers. As shown in  Table~\ref{tab:clevr_abl_hard}, this separation has a big impact on the final accuracy. We hypothesize that it enables the attention pattern for each question type to specialize accordingly to the task, there-by yielding better performance. 
\end{itemize}


\begin{table}[t]
\begin{center}
\small
\begin{tabular}{lc}
 \toprule
 Model & AP \\
 \midrule
Baseline & 99.0\\
\ - contrastive loss& 83.2\\
\ - soft-token classification & 87.7\\
 \bottomrule
\end{tabular}
\caption{Ablation results on modulated detection on CLEVR-Medium. We report the class-agnostic AP. See text for details.
\label{tab:clevr_abl_med}}
\end{center}
\end{table}
\begin{table}[t]
\begin{center}
\small
\begin{tabular}{lcc}
 \toprule
 Model & Detection AP & QA accuracy\\
 \midrule
Baseline & 99.0 & 99.7\\
No curriculum & 89.7&68.2\\
Single QA head & 99.0 & 90.1\\
 \bottomrule
\end{tabular}
\caption{Ablation results on the validation set of CLEVR. We report the class-agnostic AP and the question answering accuracy. See text for details.
\label{tab:clevr_abl_hard}}
\end{center}
\end{table}








\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{parts/images/vinvl.png}
         \caption{Current object detection pipeline outputs, predicting all possible objects in the image. This extensive annotation is essential to multi-modal understanding systems that treat detection as a black box.\label{fig:main_idea_1}}
     \end{subfigure}
     ~
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{parts/images/main_idea.png}
         \caption{ \Alg predicts boxes relevant to the caption and labels them with the corresponding spans from the text. 
         Here we use the caption: ``blond boy wearing blue shorts. a black surf-board."\label{fig:main_idea_2}}
         
     \end{subfigure}
        \caption{Modulated detection using MDETR vs detection output for a current state-of-the-art multi-modal understanding system. Image taken from \cite{zhang2021vinvl}\label{fig:three graphs}}
\end{figure}

\section{Dataset constructions}
\label{sec:appendix_datasets}
\textbf{MS COCO} On the COCO dataset, we include annotations from the referring expressions datasets (RefCOCO \cite{yu_modeling_2016}, RefCOCO+ \cite{yu_modeling_2016} and RefCOCOg \cite{mao_generation_2016} datasets). 
By construction, in this dataset, each referring expression is a whole sentence that describes one object in the image, where the constituent noun phrases from the sentences are not themselves annotated. For example, in Figure \ref{fig:refexp}, the caption would be ``the person in  the grey shirt with a watch on their wrist.", where only the person would be annotated and not the grey shirt or their watch. To avoid ambiguity, we perform some text pre-processing using SpaCy \cite{spacy} to extract the \textit{root} of the referring expression. This is used in our soft token prediction as well as the contrastive alignment loss for aligning to the referred box. The auxiliary objects (in this example the shirt and the watch) are ignored altogether.



\textbf{Visual-Genome}
We use annotations from VG regions, a dataset having diverse descriptions of a wide variety of objects, often having a very high degree of descriptive detail and covering several concepts. 
By construction, the VG dataset comprises a lot of redundant annotations. We detect redundant sentences by normalizing them (removing all punctuation, stop-words, and lower-casing), then testing for equality. Once we found a pair of equivalent sentences, two cases arise:
\begin{itemize}
    \item The corresponding boxes are highly overlapping (IoU  0.7). In this case, we consider both annotations to be redundant, and we keep only one of them.
    \item The boxes are non-overlapping. The most likely explanation is that the sentence is under-specified and actually corresponds to several distinct objects in the image. In this case, we merge the two data-points together, and the resulting annotations comports two boxes for this sentence.
\end{itemize}
We iterate recursively this process until no equivalent sentences remain.

In some cases, the VG annotations provide information about the object referred to in the sentence. For example, if the region is tagged ``the cat on the white table", in some cases the individual boxes for the cat and the table are available. In this case, we discard the region box and use the individual boxes instead.
We note that despite our merging strategy, it may remain some region description that do not canonically refer to a unique object in the image, but for which we don’t have ground-truth annotations for the other objects that also match the said region description. Despite the noise it introduces in the training process, we don’t pursue extra efforts to try and fix these situations.
In addition, we also use questions from the GQA dataset \cite{hudson_gqa_2019}, where bounding box annotations are provided for key phrases in the questions. 





\begin{figure*}[t!]
    \centering

    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.3in]{parts/images/pink.png}
        \label{ele1}
        \caption{Text prompt: ``A pink elephant."}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.3in]{parts/images/blue.png}
        \caption{Text prompt: ``A blue elephant."}
    \end{subfigure}
    \\
     \begin{subfigure}[t]{0.45\textwidth}
         \centering
        \includegraphics[height=1.3in]{parts/images/normal.png}
         \caption{Text prompt: ``A normal elephant."}
     \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.3in]{parts/images/blue_pink_normal.png}
        \caption{Text prompt: ``A pink elephant. A blue elephant. A normal elephant"}
    \end{subfigure}\caption{Qualitative results on unseen attributes combinations. While the model correctly singles out the pink elephant (a), it incorrectly includes the pink elephant when prompted about the blue one (b). In (c), we show that the model does not understand what a ``normal elephant`` looks like. However, in (d), when prompted about all three elephants at once, it is able to assign the correct label to each of them, by process of elimination.}
\end{figure*}






\section{Evaluating grounded detection}
\label{sec:eval_GD}

The main evaluation metric proposed to evaluate grounded detection in datasets like Flickr30K entities\cite{plummer2015flickr30k} is Recall@, that is measuring whether a given model is able to rank the ``correct'' box amongst the top  it produces. The correctness of a box is decided by computing the Intersection-over-Union (IoU) between the proposed box and the ground-truth box, and deemed correct if the IoU is above a predetermined threshold, generally 0.5.
While this kind of evaluation is well-suited for tasks where there is a clear one-to-one mapping between phrases and boxes, for example in Referring Expression Comprehension tasks, we argue that in general grounded detection tasks, they fall short of properly evaluating the performance of the models. Specifically, they run into the following issues:
\begin{enumerate}
    \item \textbf{Multiple boxes for a given phrase}: Since the recall@k metric implies a single box per phrase, it is not clear how to extend it to situations where a given phrase refers to several distinct objects in the image \nico{TODO image}. In the absence of clear guidelines, various authors have adopted divergent approaches to deal with that. Specifically, some \cite{li2019visualbert,kim2018bilinear} consider the predicted box to be correct if it has an IoU with \emph{any} of the ground-truth boxes. We refer to this protocol as the \textsc{Any}-Protocol. Others \cite{plummer2020revisiting,yang2019fast} first merge all the ground-truth boxes associated to the phrase by considering the smallest enclosing box. Then they proceed to compute the IoU as usual, using this merged box as the ground-truth one. We refer to this protocol as the \textsc{Merged-Boxes}-Protocol.
    
    Arguably, both methods have drawbacks: the first one keeps the atomicity of each instance but fails to evaluate whether the model found all the referred instances. The second one looses the fine-grained instance in favor of a box that may be unreasonably bloated if the instances are spread apart. 

    \item \textbf{Multiple phrases for a given box}. In some cases, the same object is referred to multiple times in the sentence. Sometimes, the corresponding phrases are exact duplicates of each other, or close synonyms (eg ``a guy'' and ``a man''), but sometimes the co-references are more subtle. One such example include referring alternatively to a group (eg ``a couple'') or to a sub-constituent (eg ``the woman'').
    
    Under the current evaluation protocol, each phrase is evaluated independently, even if they refer to the same object. As such, it does not test the model's understanding of co-references, which is arguably an important aspect of learning grounded representations.
    
\end{enumerate}

Recognising the discrepancy in the evaluation procedures in the literature and the difficulties it creates in comparing various approaches, we also evaluate MDETR under the merged-box protocol, as described in Sect~\ref{sec:merged_boxes}.

\subsection{Evaluation under the \textsc{Merged-Boxes}-Protocol}
\label{sec:merged_boxes}
In MDETR, the dataset creation process operates under the assumption that a phrase is associated to all the individual boxes that correspond to it. As a result, MDETR does not naturally predict the merged-boxes that would be required by the \textsc{Merged-Boxes}-Protocol. For that reason, it is necessary to fine-tune the model on a version of Flickr30k entities where the boxes have been merged appropriately. We note that the \textsc{Any}-Protocol did not require such fine-tuning.


\section{Error Analysis}

To better understand the failures of the model on the grounding task, we provide a small-scale error analysis. We evaluate our best model, the EfficientNetB5 variant, on the validation set of Flickr30k. We manually inspect the first 100 errors made by the model. The results are summarized in Fig \ref{fig:errors} and we detail the types of errors we uncovered in the following:
\begin{itemize}
    \item Issues with the ground truth annotations
    \begin{description}
\item [Ambiguous GT location] This corresponds to phrases that don't have a canonical localization. They mostly correspond to scene elements, such as ``beach" or ``tree".
\item [Inconsistent annotations (when several objects are referred)] In the \textsc{Any}-Protocol, if a phrase corresponds to several objects, then in principle every single instance should be individually annotated. However, we find some inconsistencies in the annotations, for example some instances that are missed, or some distinct instances that are annotated using the same box.
\item [Imprecise GT box] This corresponds to cases where the provided box is not adequate. It is either too big (not tight), or too small, cutting out a part of the object.
\item[Wrong GT] In those cases, the annotated box(es) don't correspond to the correct referred object at all.
\end{description}
\item Grounding mistakes
    \begin{description}
\item [Wrong instance] The model picks an instance of the correct type (including adjective modifiers, if any), however it is not the correct instance when taking into account context from the rest of the sentence. 
\item[Wrong object] The model picks the wrong object, and it is not of the correct type. This usually occurs on long-tail concepts that the model doesn't seem to know about.
\item[OCR] The phrase refers to written text, thus requiring OCR abilities from the model, which it doesn't have.
\end{description}
\item Detection issues
    \begin{description}
\item [Imprecise box prediction] The model is clearly selecting the right object, however the predicted box isn't quite precise enough. This happens on small objects as well as elongated objects, where a relatively small L1 error can cause a low IoU with the ground truth.

\end{description}
\end{itemize}

Overall, we find that on the analyzed subset, more than half the errors stem from issues in the ground-truth annotations. Extrapolating to the rest of the dataset, this would imply a label noise of around 10\%, which might be detrimental to make significant further progress on the task.


\begin{figure}[t] \centering
\begin{tikzpicture}
\pie[color={myorange!10, myorange!20, myorange!30, myorange!40, myblue!30, myblue!40, myblue!50, mygreen!40}]{
12/Ambiguous GT location, 
18/Inconsistent annotations (when several objects are referred), 
13/Imprecise GT box, 
15/Wrong GT object,
7/Wrong instance,
14/Wrong object,
2/OCR,
19/Imprecise box prediction
}
\end{tikzpicture}
\caption{Break down of the first 100 errors from the EfficientNetB5 model on the Flickr30k validation set. Yellow shade corresponds to mistakes in the ground truth annotations. Blue shade corresponds to errors made by the model in grounding. Green shade corresponds to errors made by the model in accurate localization. See text for more details.}
\label{fig:errors}
\end{figure}


\section{Experiments on VQA2}
\label{sec:vqa2}
In our visual question answering experiments on the GQA dataset, we always had access to bounding box information for the questions. Only during fine-tuning on the balanced set for 10 epochs, we do not supervise the detection losses. On the other hand, for datasets such as VQA2~\cite{balanced_vqa_v2}, we do not have access to any box annotations and the supervision comes solely from the question answering loss. We fine-tune two of our models --- pre-trained on the joint dataset, as well as the model fine-tuned on the all-split of GQA --- on the VQA v2 dataset for 25 epochs. The results are reported in Table~\ref{tab:vqa2}.  
While these results are not state-of-the-art on the VQA2 benchmark, they are still quite reasonable with respect to current literature. This show that our method can be extended to tasks where we do not have the dense supervision (in the form of bounding boxes and their alignment to the text) that we otherwise expect in tasks reported in this paper.  

\begin{table}[h]
\begin{center}
\small
\begin{tabular}{lcccccccc}
 \toprule
 Pre-training & \multicolumn{4}{c}{Test-Dev} & \multicolumn{4}{c}{Test-Std}\\
              & Overall & Yes-no & Other & Number & Overall & Yes-no & Other & Number \\
 \midrule
Modulated detection on combined dataset (40 epochs) & 70.49 & 86.74 & 55.17 & 60.04 & - & - & - & -\\
+ GQA-\textit{all} (5 epochs) & 70.64 & 86.74 & 55.26 & 60.33 & 70.63 & 86.79 & 55.12 & 60.12  \\
 \bottomrule
\end{tabular}
\caption{VQA v2 results
\label{tab:vqa2}}
\end{center}
\end{table}
