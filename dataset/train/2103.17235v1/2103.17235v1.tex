\documentclass[journal]{IEEEtran} 
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{footnote}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{XXX, VOL. XX, NO. XX, XXXX}
{Tomar \MakeLowercase{\textit{et al.}}: FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation}

\usepackage{array}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{hyperref}
\usepackage{subcaption}
\makesavenoteenv{tabular}


\widowpenalty10000
\clubpenalty10000
\usepackage[acronym]{glossaries}

\acrodef{GI}{gastrointestinal}
\acrodef{AI}{Artificial Intelligence} 
\acrodef{ML}{Machine Learning}
\acrodef{DL}{deep learning}
\acrodef{CNN}{Convolutional Neural Network}
\acrodef{CRC}{Colorectal Cancer}
\acrodef{WCE}{Wireless Capsule Endoscopy}  
\acrodef{FCN}{Fully Convolutional Network}
\acrodef{ASPP}{Atrous Spatial Pyramidal Pooling}
\acrodef{SGDR}{Stochastic Gradient Descent with Restart}
\acrodef{SGD}{Stochastic Gradient Descent}
\acrodef{MSE}{Mean Square Error}
\acrodef{ReLU}{Rectified Linear Unit}
\acrodef{BN}{Batch Normalization}
\acrodef{FPS}{Frame Per Second}
\acrodef{GAN}{Generative Adversarial Network}
\acrodef{mIoU}{mean Intersection over Union}
\acrodef{SOTA}{state-of-the-art}
\acrodef{DSC}{Dice Coefficient}
\acrodef{CADx}{Computer Aided Diagnosis}
\acrodef{ISIC}{international skin imaging collaboration}
\acrodef{RNN}{Recurrent  Neural  Network}
\acrodef{RL}{Recurrent Learning}
\acrodef{FCNN}{Fully Convolutional Neural Network}
\acrodef{TTA}{Test-Time Augmentation}
\acrodef{SE}{Squeeze and Excitation}
\acrodef{DRIVE}{Digital Retinal Images for Vessel Extraction}
\begin{document}
\title{{FANet:} A Feedback Attention Network for Improved Biomedical Image Segmentation}
\author{Nikhil Kumar Tomar, 
Debesh Jha, \IEEEmembership{Member, IEEE}, 
Michael A. Riegler, \IEEEmembership{Member, IEEE}, 
H{\aa}vard D. Johansen, \IEEEmembership{Member, IEEE},
Dag Johansen, \IEEEmembership{Member, IEEE}, 
Jens Rittscher, \IEEEmembership{Member, IEEE},
P{\aa}l Halvorsen, \IEEEmembership{Member, IEEE}, and 
Sharib Ali, \IEEEmembership{Member, IEEE}
\thanks{N. K. Tomar is with SimulaMet, Oslo, Norway.}
\thanks{D. Jha is with SimulaMet, Oslo, Norway and UiT The Arctic University of Norway, Troms{\o}, Norway (corresponding email: debesh@simula.no).}
\thanks{M. A. Riegler is with SimulaMet, Oslo, Norway.}
\thanks{H.D. Johansen and D. Johansen are with UiT The Arctic University of Norway, Troms{\o}, Norway.}
\thanks{J. Rittscher is with the Department of Engineering Science, University of Oxford, Oxford, UK.}
\thanks{P. Halvorsen is with SimulaMet and Oslo Metropolitan University, Oslo, Norway.}
\thanks{S. Ali is with the Department of Engineering Science, University of Oxford, and Oxford NIHR Biomedical Research Centre, Oxford, UK (corresponding email: sharib.ali@eng.ox.ac.uk).}
}
\maketitle

\begin{abstract}
With the increase in available large clinical and experimental datasets, there has been substantial amount of work being done on addressing the challenges in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learnt feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed \textit{feedback attention} model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of the proposed FANet. 
\end{abstract}

\begin{IEEEkeywords}
Medical image segmentation, deep learning, feedback attention, colon polyps, skin lesion, retinal vessels, cell nuclei, lung segmentation
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{I}MAGE segmentation is one of the most studied problems in computer vision, where the main goal is to classify each pixel of an image to a specific class instance. This can either be a pixel of arbitrary objects such as cars or humans and pixels of healthy and sick medical findings. Substantial progresses have been made in imaging modalities such as X-ray, Computerized Tomography (CT), Magnetic Resonance Imaging (MRI), endoscopy imaging, fundus imaging, Electron Microscopy (EM), and histology imaging. While \ac{ML} methods provide improved performance over the traditional computer vision methods, most of them require ground truth labels from domain experts, which comes with some challenges. For example, due to the poor image quality of most biomedical imaging data, expert interpretations can be affected. Moreover, complex and irregular shapes of the object-of-interest in these imaging data, such as cell nuclei in EM, can pose challenges for model training. Although it is difficult to address these biases and provide sample-specific priors, designing a model that is capable of pruning and self-rectification for each sample during training can help to improve model performance. 

Current developments of \acp{CNN}, \acp{RNN}, and attention modules have improved automated methods in biomedical image analysis. While \acp{CNN} helps to overcome the limitations of handcrafted features used in traditional \ac{ML} methods by learning features in a supervised end-to-end manner, they require the estimation of millions of parameters. As a result, a large and diverse training dataset is required to avoid overfitting.~\acp{RNN} can be used to preserve the model compactness and can be effectively used in resource-constrained settings~\cite{Wang:ICCV2019}. However, \acp{RNN} are known for their memory inefficient memory-bandwidth-bound computation and model complexity as they hold their internal state to process the data~\cite{bai2018empirical}. Additionally, in many applications, applying visual attention mechanism in deep learning has shown promising results and improvements in terms of model usability~\cite{xu2015show, Ashish:NIPS2017}. Attention allows networks to focus on a concrete class instance, thereby penalizing non-specific regions. Therefore, it is required to adhere with the knowledge transfer that integrates the past information, similar to recurrent networks, and use them to provide stronger acceptance of learned features belonging to the region-of-interest as an attention mechanism~\cite{Tilk2016BidirectionalRN,SCHLEMPER2019197}.



While most deep learning-based segmentation methods are trained on large datasets to avoid overfitting, existing methods do not incorporate any sample-specific information learned in the previous training epochs. In current deep learning approaches, only aggregated average weights are used for information propagation through gradients. A mask-guided contrastive attention model was used by Song et al.~\cite{Song_2018_CVPR} to deal with the background clutter. Unlike classical training mechanism and motivated by the work of Song et al.~\cite{Song_2018_CVPR}, we propose to propagate the sample-specific mask output from the previous epoch to the successive epoch in a recursive fashion. Such a feedback mechanism can provide with prior information that can help to learn sample variability, thereby enabling to train effectively on diverse and small or large datasets. Here, iterative prediction can be used to prune the predicted masks during the inference (see Fig.~\ref{fig:fig0-examplary}). This allows to both locally and globally learn to rectify the mask output from the learnt weights. Unlike \ac{TTA}~\cite{WANG201934}, where different transforms are utilized to mimic the sample representations that are better learnt by the network, we interactively rectify masks during training. To our knowledge, FANet is the first deep learning model that has the ability to self-rectify its predictions without requiring transformations, ensemble strategies, and prior sample-specific knowledge. We use a single end-to-end trainable network that allows information propagation during both train and test time.

Including a feedback mechanism as a part of training is central to our novel FANet approach for semantic segmentation. The predicted map of each sample from the previous epoch is used to provide attention unified with the current state feature map. We call this architecture,  Feedback Attention Network (FANet), as it uses an attention mechanism to different feature scales in the network, allowing it to capture variability in image samples. Additionally, our residual block with \ac{SE} layer allows us to improve channel interdependencies, which can be critical to tackle image quality issues. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/Figure-0_new.pdf}
    \caption{Semantic segmentation using our FANet architecture. Otsu threshold is used for generating the initial mask used during  iteration. Then the predictions are iteratively updated with the predicted mask. It can be observed that already at the 2 iteration, the results converge. The corresponding feature maps before and after feedback attention at the last decoder layer of our FANet are shown as color images on the right.}\label{fig:fig0-examplary}
\end{figure*}
The main contributions made in this work can be summarized as follows:
\begin{enumerate}


    \item \textbf{Feedback attention learning --- } A novel mechanism to utilize the variability present in each training sample. The mask outputs are propagated from one to subsequent epochs to repress the unwanted feature clutter.

   \item \textbf{Iterative refining of prediction masks --- } Using feedback information helps in refining the predicted masks in training as well as inference. During testing, we iterate over the input image and keep updating the input mask with the predicted mask.
\item \textbf{Embedded run-length encoding strategy --- } Binary mask outputs of each samples are efficiently compressed before being propagated to the next epoch. This provides a memory efficient mechanism for passing sample specific masks.
\item \textbf{Systematic evaluation --- } Experiments on seven vastly different biomedical datasets suggest that FANet outperforms other \ac{SOTA} algorithms.

    \item \textbf{Efficient training --- } FANet achieves near \ac{SOTA} performance with far fewer training epochs. 
\end{enumerate}

\section{Related work}
\label{relatedwork}
In this section, we summarize the most relevant advances in medical image segmentation, highlight advances in feedback attention networks, and summarise the recent contributions to iterative refinement methods for image segmentation. 
\subsection{Biomedical image segmentation}
The basis of most modern \ac{CNN}-based semantic segmentation architectures are either \ac{FCN}~\cite{long2015fully} or an encoder-decoder architecture such as U-Net~\cite{ronneberger2015u} originally designed for cell segmentation~\cite{ronneberger2015u}. Various modifications of these networks have been proposed both for semantic segmentation of natural images~\cite{zhong2020squeeze,zhao2017pyramid,wang2020deep} and biomedical image segmentation~\cite{fan2020pranet,zhou2018unet,zhou2019unet++,jha2019resunet++,jha2020doubleu,guo2020sa,ibtehaz2020multiresunet,oktay2018attention,wang2020boundary}. In general, in the encoder, the image content is encoded using multiple convolutions to capture from low-level to high level features, whereas in the decoder part of the network the prediction masks are obtained by multiple upsampling mechanism and deconvolution. Methods like PSPNet~\cite{zhao2017pyramid} and DeepLab~\cite{chen2017deeplab} incorporate convolutional feature maps of varying resolutions to effectively segment both small and large sized objects. While PSPNet used a pyramid pooling module, DeepLab used \ac{ASPP} for encoding the multi-scale contextual information. Both PSPNet and DeepLab based architectures have been used widely in the medical imaging community~\cite{hassan2020,sun2019colorectal,jha2019resunet++}.

Recently, Cheng et al.~\cite{Cheng:CVPR2020} proposed the Panoptic-DeepLap method with a class-agnostic regression, for instance, segmentation that uses a dual-ASPP and dual-decoder structure. Wang et al.~\cite{Huiyu:ECCV2020} proposed an attention-based panoptic instance segmentation showing improved performance than the former. High-Resolution Network (HRNet)~\cite{wang2020deep} employed exchange of information across different resolutions to resolve the issue of loss of high-resolution feature representations. Application of these architectures in the biomedical imaging community is, however, yet to be explored.

\subsection{Feedback attention networks}
Visual attention has been widely used in computer vision for object detection~\cite{chen2017sca-cnn}, image segmentation~\cite{CY2016Attention,ye2019cross} and pose estimation~\cite{Chu2017MulticontextAF}. Attention mechanisms have also been utilized for posing explicit focus on the target region in medical imaging~\cite{SCHLEMPER2019197,LUNDERVOLD2019102}. Attention U-Net~\cite{oktay2018attention} used a gated operation in the U-Net architecture to focus on the target abdominal regions of CT datasets. Feedback mechanism for attention using two U-Net architectures with shared weights was used for cell segmentation~\cite{tsuda2020feedback,shibuya2020feedback}. The latter used a standard U-Net architecture with the second U-Net incorporating ConvLSTM~\cite{NIPS2015_07563a3f} to store the feature map (input-to-state) from the first U-Net network. However, feedback is only applied to the same epoch with state-to-state transitions. On the contrary, our approach utilizes a feedback mechanism that propagates information flow from the previous epoch to the current epoch in an attention mechanism setting. We employ the predicted masks from the previous epoch as hard attention to prune the segmentation output.
\subsection{Iterative refinement for segmentation}
Inherent variation in object shape and changes in scale pose significant challenges in biomedical imaging. An iterative refinement of the segmentation mask by feeding the input image and the predicted segmentation mask to a modified U-Net architecture was done by Mosinska et al.~\cite{mosinska2018beyond}. Similarly, iterative update of latent space and minimization of the Structure Similarity Index Measure (SSIM) loss was used to refine the predicted segmentation maps during test time in~\cite{Prashant:ECCV20}.

Recently, iterative refinement strategies have also been used for pose estimation~\cite{newell2016stacked,ramakrishna2014pose,wei2016convolutional} that used consecutive modules for refinement of the predictions with a loss function for the evaluation of output in each module. These iterative refinement processes show improved predictions and are able to handle domain shifts or object shape variability without requiring very deep networks~\cite{Prashant:ECCV20}. A major bottleneck in these methods is the number of iterations required for convergence. Unlike these methods, our proposed FANet provides attention to the specific region-of-interest and can prune the segmentation predictions in less than ten iterations without requiring any optimization scheme, i.e., simple weight updates are used. 

\section{Method}
\label{methodology}
This section introduces the main components required to build the proposed architecture. We present the overall architecture used along with the proposed feedback attention learning mechanism. 
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Figure-1_new.pdf}
    \caption{Feedback attention network with squeeze and excite residual (SE-Residual) block and MixPool block. (a) SE-Residual block integrated with squeeze and excite layer uses  convolution to concatenate the high-resolution feature representation with the the encoded feature vector. (b) MixPool block represents attention mechanism in our network. The input mask is downscaled to the corresponding layer feature map size  which is fused with the masked feature map representation  for hard attention of input feature in that layer . Finally, the attenuated feature map  and the feature maps  are both concatenated. c) Proposed FANet showing the complete network architecture. Encoder-decoder architecture with skip-connections (in dotted arrows) from SE-Residual blocks to preserve high- and intermediate resolution feature representations and MixPool block connections (with solid arrows) that allow to feedback the previous mask predictions.} \label{fig:proposedarchitecture}
\end{figure*}



\begin{table*} [t!]
\footnotesize
 \caption{{Details of the biomedical datasets used in our experiments. ``Train", ``Train after aug." and ``Test" denote the number of training image sample, number of training sample after image augmentation, and number of test sample used in the study. }}
    \label{table:datasettable}
    \centering
          \begin{tabular}{@{}l|l|l|l|l|l|l@{}} 
            \toprule
        \textbf{Dataset} &\textbf{Images} &\textbf{Size}  &\textbf{Train} &\textbf{Train after Aug.} & \textbf{Test} &\textbf{Application}\\ 
              \hline
              \hline
Kvasir-SEG~\cite{jha2020kvasir} & 1000 & Variable & 880 & 16720 & 120 &Colonoscopy\\  CVC-ClinicDB~\cite{bernal2015wm} & 612 &  & 490 & 14210 & 61 &Colonoscopy\\  2018 Data Science Bowl~\cite{caicedo2019nucleus} & 670 &  & 335 & 10720 & 134 & Nuclie \\  ISIC 2018  ~\cite{codella2018skin,tschandl2018ham10000}& 2596 & Variable & 1815 & 39930 & 259 & Dermoscopy\\ EM dataset~\cite{cardona2010integrated} & 30 &  & 24 & 384 & 3 & Cell \\DRIVE Database~\cite{staal2004ridge} & 40 &  & 20 & 640 & 20 & Retina Vessel\\CHASE-DB1~\cite{owen2009measuring} & 28 &  & 20 & 640 & 8 & Retina Vessel\\\bottomrule
        \multicolumn{4}{l}{{ Lesion Boundary Segmentation}}
\end{tabular}
\vspace{-5mm}
\end{table*}	

\subsection{SE-Residual block}
Deeper networks improve the performance of the model significantly, but, increase in depth can cause either a vanishing or exploding gradients problem~\cite{he2016deep}. To deal with this, we take advantage of shortcut connections between layers in the residual learning paradigm. Our SE-Residual block uses two 33 convolutions and an identity mapping, where each convolution layer is followed by a \ac{BN} layer and a \ac{ReLU} non-linear activation function. The identity mapping is used to connect the input and the output of the convolution layer (Fig.~\ref{fig:proposedarchitecture} a). 

Similar to the work by Hu et al.~\cite{Hu_2018_CVPR}, we add a \ac{SE} layer in the residual network. The \ac{SE} layer acts as a content-aware mechanism that re-weights each channel accordingly to create robust representations. Hence, it allows the network to become more sensitive to significant features while suppressing irrelevant features. This goal is accomplished in two steps.~First, the feature maps are squeezed by using the global average pooling to get a global understanding of each channel. The squeeze operation results in a  feature vector of size , where  refers to the number of channels. In the second step: excitation, this feature vector is feed through a two-layered feed-forward neural network, where the number of features is first reduced and then expanded to the original size . Now, this  sized vector represents the weight of the original feature maps, which is used to scale each channel.

\subsection{MixPool block}
The proposed MixPool block is used in multiple layers of our FANet architecture. This block facilitates the flow of samplewise feedback information between consecutive epochs providing a hard attention to the learnt features in our SE-Residual block. The layer provides focus to the relevant features in both contraction path and expansion path layers. Here, first feature maps of each layer from the SE-Residual block  is passed through a  convolution followed by \ac{BN} and a \ac{ReLU} activation function resulting in . Then, we apply a  convolution and a sigmoid activation function  with a threshold of 0.5 to obtain the binary mask  to contribute to the \textit{spatial attention map generation} given by:

We then apply appropriate max pooling on the input mask (from the previous epoch) to resize it to the size of the spatial attention map.  After that, we apply a union operation on both the resized input mask and the spatial attention map. This confirms that we get the feature from both the feedback and the spatial attention maps to further create a new spatial attention map. Next, we apply the new combined spatial attention map to the original feature map by an element-wise multiplication operation that suppresses the irrelevant features and enhances the important ones. The enhanced and the original feature maps are then followed by a  convolution, \ac{BN}, and a \ac{ReLU}. Finally, we concatenate the output of both activation functions, which constitutes the output of our MixPool block given by:

\noindent{where}  denotes the concatenation operator,  is element-wise multiplication and  represents the union operation.

\subsection{Proposed FANet architecture}
\label{Proposedarchitecture}
The block diagram of FANet is illustrated in Fig.~\ref{fig:proposedarchitecture}. It uses an encoder-decoder design common to many semantic segmentation architectures. We combine the strength of a residual network enhanced with \ac{SE} as SE-Residual block, MixPool block that facilitates the attention and propagation of information flow from the current learning paradigm and that of the previous epoch. We implement a recurrent learning mechanism in both encoder and decoder layers that allows to achieve efficient segmentation. The MixPool block uses the previous segmentation map, which contains the information from prior training and uses it to improve the semantic representation of the feature maps.

The proposed network architecture is a \ac{FCNN} consisting of four encoder and four decoder blocks. The encoder takes the input image, downsamples it gradually, and encodes it in a compact representation. Then, the decoder takes this compact representation and tries to reconstruct the semantic representation by gradually upsampling it and combining the features from the encoder. Finally,  we receive a pixel-wise categorization of the input image. Both the encoder and the decoder are built using the SE-Residual block, and an additional concatenation of the original resolution feature representation in the encoder is added at each resolution scale. This mechanism minimizes the loss of feature representations during downscaling and upscaling processes. 

Each encoder network starts with two SE-Residual blocks, which consist of two  convolutions and a shortcut connection known as identity mapping, connecting the input and output of the two convolution layers. Each convolution is followed by a \ac{BN} and a \ac{ReLU} activation function. The output of the second SE-Residual block acts as skip connection for the corresponding decoder block. After that, it is followed by the MixPool block, which has the previous epoch segmentation mask and provides a hard-attention over the incoming feature maps. This process is repeated for each of the downscaled layers. 

Each decoder network starts with a  transpose convolution that doubles the spatial dimensions of the incoming feature maps. These feature maps are concatenated with feature maps from the corresponding encoder block through skip connections. The skip connections help to propagate the information from the upper layers, which are sometimes lost due to the depth of the network. The skip connections are followed by two SE-Residual blocks, which help to eliminate the problem of vanishing gradient. The MixPool block that utilizes the segmentation mask from the previous epoch is then applied creating a hard-attention over the learnt feature maps. Next, we concatenate the feature maps from the last decoder block and the segmentation mask from the previous epoch. Finally, we apply a  convolution with the sigmoid activation function. The output of this is used to both minimize the training loss, using a combined binary cross-entropy and dice loss, and to generate segmentation masks that are stored as a run-length encoded (RLE) compression for each sample and propagated during the next epoch. The RLE is updated every epoch. 
Similarly, the network learns to adapt the weights in iterative training, this mechanism is also utilized during the test time. As shown in Fig.~\ref{fig:fig0-examplary}, test results are pruned in a few iterations during the test time. Unlike many methods in literature~\cite{mosinska2018beyond,Prashant:ECCV20}, here we utilize the same network and without any complimentary loss function optimization. 

\section{Experiments}
\label{experiments}
\subsection{Setup}
\subsubsection{Dataset and Evaluation Metrics}
To evaluate the proposed architecture, we have selected seven datasets that capture different segmentation tasks in biomedical imaging. The details of each dataset can be found in Table~\ref{table:datasettable}. The dataset images contain the images of organs and lesions acquired under different imaging protocols. For the retina vessel segmentation task, we use DRIVE and CHASE-DB1 datasets. These two datasets are aimed at various diseases related to diseases of retina vessels, such as hypertensive retinopathy, retinal vein occlusion, and retinal artery occlusion. The ISIC 2018 dataset, which is a dermoscopy dataset that is useful in the diagnosis of skin cancer, is the third dataset focused on medical imaging data. This dataset contains a wide variety of skin cancer images of different sizes and shapes, which helps in a better understanding of the disease. We have further included Kvasir-SEG and CVC-ClinicDB colonoscopy datasets. These datasets contain the image frames extracted from different colonoscopy interventions and are focused on colorectal polyps that are one of the cancer precursors in the colon and rectum. It highly increases the chance of avoiding lethal cancer by early detection. In addition, we have included two datasets acquired from biological imaging and are aimed at understanding of the cellular processes. These include the 2018 Data Science Bowl and the EM datasets. The 2018 Data Science Bowl dataset contains images with a large number of variable shaped nuclei acquired from different cell types, magnification, and imaging modalities. This dataset is designed for automated nuclei segmentation. Similarly, the EM dataset contains the transmission electron microscopy images of the neural structures of the Drosophila nerve cord. This dataset is aimed at the automated segmentation of the neural structures. 

To evaluate SOTA deep learning methods and our proposed FANet, we have used standard evaluation metrics that includes \ac{DSC} (a.k.a. F1), \ac{mIoU}, precision, and recall. We have additionally calculated specificity for those datasets where this metric was previously used for benchmarking.  

\subsubsection{Implementation details}
All the experiments are performed on a Volta 100 GPU and an NVIDIA DGX-2 system using the PyTorch 1.6. framework. Our model is trained for 100 epochs \textit{(empirically set)} using an Adam optimizer with a learning rate of  for all the experiments except for the \ac{DRIVE} and the CHASE-DB1 dataset where the learning rate was adjusted to  due to the small size of the training dataset. Datasets were chosen such that the efficiency of our model could be compared to the \ac{SOTA} methods. A combination of binary cross-entropy and dice loss has been used as the loss function. ReduceLROnPlateau callback was used to monitor the learning rate and adjust it to obtain optimal training performance. All the images used in the study were resized to  except for the 2018 Data Science Bowl and the CVC-ClinicDB dataset, where images were resized to . Data augmentation, such as random crop, flipping, rotation, elastic transformation, grid distortion, optical distortion, grayscale conversion, random brightness, contrast, channel, and course dropout were used. 



\subsubsection{Ablation study}
In order to demonstrate the strength of our proposed FANet architecture, we perform a thorough ablation study. For this, we have used all seven datasets and evaluated on several metrics for baseline (FANet without MixPool), baseline with MixPool, and the combination of baseline, MixPool, and feedback (proposed).
\subsection{Results}
\label{result}
Below we present quantitative results on seven different biomedical imaging datasets and compare with corresponding SOTA methods. 
\subsubsection{Results on Kvasir-SEG} 
\begin{table}[!t]
\footnotesize
\caption{Results on the Kvasir-SEG~\cite{jha2020kvasir}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l|l@{}}
\toprule
\textbf{Method} &\textbf{Backbone} &\textbf{F1} &\textbf{mIoU} &\textbf{Recall}& \textbf{Prec.} \\ 
\hline
\hline
{U-Net}~\cite{ronneberger2015u} & - & 0.5969 & 0.4713 & 0.6171 & 0.6722 \\ ResUNet~\cite{zhang2018road} & - & 0.6902 & 0.5721 & 0.7248 & 0.7454 \\ {ResUNet++}~\cite{jha2019resunet++} & - & 0.7143 & 0.6126 & 0.7419 & 0.7836 \\{FCN8}~\cite{long2015fully}  &  VGG 16 & 0.8310 & 0.7365 & 0.8346 & 0.8817 \\HRNet~\cite{wang2020deep}& - & 0.8446 & 0.7592 & 0.8588 & 0.8778 \\ {DoubleU-Net}~\cite{jha2020doubleu} & VGG 19 & 0.8129 & 0.7332 & 0.8402 & 0.8611 \\ {PSPNet}~\cite{zhao2017pyramid}  & ResNet50 & 0.8406 & 0.7444 & 0.8357 & 0.8901 \\ DeepLabv3+~\cite{chen2018encoder} & MobileNet & 0.8425 & 0.7575 & 0.8377 & 0.9014 \\ {DeepLabv3+}~\cite{chen2018encoder} & ResNet50 & 0.8572 & 0.7759 & 0.8616 & 0.8907 \\ {DeepLabv3+}~\cite{chen2018encoder}& ResNet101 & 0.8643 & 0.7862 & 0.8592 & \textbf{0.9064} \\ 
U-Net~\cite{ronneberger2015u} & VGG19 & 0.7535 & 0.6571 & 0.7364 & 0.8565 \\ \textbf{FANet} & - & \textbf{0.8803} & \textbf{0.8153} & \textbf{0.9058} & 0.9005 \\ 
\bottomrule
\end{tabular}
\label{tab:kvasirseg}
\end{table}

Kvasir-SEG~\cite{jha2020kvasir} is a publicly available polyp segmentation dataset acquired from clinical colonoscopy (endoscopy) procedures. This dataset has been widely used for algorithm benchmarking. We have trained our model and compared it with recent \ac{SOTA} methods on Kvasir-SEG. A comparison with widely accepted segmentation methods with different backbones (see Table~\ref{tab:kvasirseg}) shows that our approach is improved performance compared to the \ac{SOTA} methods (on the same train-test split). Our FANet outperforms all the \ac{SOTA} methods on almost all metrics. While outperforming most U-Net and its variants, it can be observed that FANet achieved an F1 score of 0.8803, which is 1.6\% and 3.57\% better than the most accurate DeepLabv3+ with ResNet101 backbone and the recent HRNet. 
\subsubsection{Results on CVC-ClinicDB dataset}
\begin{table}[!t]
\footnotesize
\centering
\caption{Results on the CVC-ClinicDB~\cite{bernal2015wm}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{mIoU }& \textbf{Recall} & \textbf{Precision} \\ 
\hline
\hline
U-Net (MICCAI’15)~\cite{ronneberger2015u} & 0.8230 & 0.7550 & -  & - \\ U-Net++ (TMI’19)~\cite{zhou2018unet} & 0.7940 & 0.7290 & - & -  \\ ResUNet-mod~\cite{zhang2018road} & 0.7788 & 0.4545 & 0.6683 & 0.8877 \\ ResUNet++~\cite{jha2019resunet++}  & 0.7955 & 0.7962 & 0.7022 & 0.8785 \\ SFA~(MICCAI’19)~\cite{fang2019selective} & 0.7000 & 0.6070 & - & - \\ PraNet~\cite{fan2020pranet} & 0.8990 & 0.8490 & - & - \\ \textbf{FANet} &\textbf{0.9355} & \textbf{0.8937} & \textbf{0.9339} & \textbf{0.9401} \\ \bottomrule
\end{tabular}
\label{tab:cvclinicDB}

\end{table}
CVC-ClinicDB is another commonly used dataset for colonoscopy image analysis. FANet architecture outperforms all the \ac{SOTA} methods on this dataset by a large margin with F1 of 0.9355, \ac{mIoU} of 0.8937, recall of 0.9339, and precision of 0.9401 (see Table~\ref{tab:cvclinicDB}). FANet achieves the best trade-off between recall and precision compared to the ResUNet-based architectures~\cite{zhang2018road,jha2019resunet++}. The strength of the FANet can be observed by the large improvement of 23.17\% in the recall and 5.24\% in the precision over the \ac{SOTA} ResUNet++~\cite{jha2019resunet++}. The recall suggests that our method is more clinically preferable than the \ac{SOTA}. A higher recall is desired in the systems used for clinical diagnosis~\cite{gilvary2019missing}.
\subsubsection{Results on 2018 Data Science Bowl} 
\begin{table}[!t]
\footnotesize
\centering
\caption{Results on the 2018 Data Science Bowl~\cite{caicedo2019nucleus}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l|l@{}}
\toprule
\textbf{Method} &\textbf{Backbone} & \textbf{F1}  & \textbf{mIoU} &\textbf{Recall}& \textbf{Prec.} \\
\hline
\hline
U-Net~\cite{ronneberger2015u} & ResNet101  & 0.7573 & 0.9103 & - & -  \\ U-Net++~\cite{zhou2019unet++} & ResNet101  & 0.8974  & \textbf{0.9255} & - & -    \\ DoubleU-Net~\cite{jha2020doubleu} & VGG19  & 0.7683 & 0.8407 & 0.6407 & \textbf{0.9596}  \\
\textbf{FANet} & None  &\textbf{0.9176}& 0.8569 & \textbf{0.9222} & 0.9194\\ \bottomrule
\end{tabular}
\label{tab:DSB}
\end{table}
Cell nuclei segmentation in microscopy imaging is a common task in the biological image analysis~\cite{caicedo2019nucleus}. We used the publicly available 2018 Data Science Bowl (DSB) challenge dataset and compared our results with the \ac{SOTA} methods. Table~\ref{tab:DSB} shows that FANet produces an F1 of 0.9176, \ac{mIoU} of 0.8569, and recall of 0.9222 with an improvement of 2.02\% in F1 with respect to \ac{SOTA} UNet++~\cite{zhou2019unet++} and 28.15\% improvement in recall compared to the best performing DoubleU-Net~\cite{jha2020doubleu}. In general, FANet achieves the best trade-off between precision and recall compared to the \ac{SOTA} methods resulting in the highest F1 score (0.9176). The qualitative results with 2018 DSB also show that the predicted FANet produces high-quality segmentation masks for cell nuclei with respect to the ground truth (see Figure~\ref{fig:qualitativeresult}). 


\subsubsection{Results on ISIC 2018 dataset}
\begin{table}[!t]
\footnotesize
\centering
\caption{Results on the ISIC 2018 (Skin Caner Segmentation)~\cite{codella2018skin,tschandl2018ham10000}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l|l@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{mIoU }& \textbf{Recall}& \textbf{Spec.} & \textbf{Prec.} \\ 
\hline
\hline
U-Net~\cite{ronneberger2015u} & 0.6740  & 0.5490 & 0.7080 & 0.9640 & -    \\ Attention U-Net~\cite{oktay2018attention} & 0.6650  & 0.5660 & 0.7170 & 0.9670 & -    \\ R2U-Net~\cite{alom2018recurrent} & 0.6790  & 0.5810 & 0.7920 & 0.9280 & -    \\ Attention R2U-Net~\cite{alom2018recurrent} &  0.6910  & 0.5920 & 0.7260 & 0.9710 & -    \\ BCDU-Net (d=1)~\cite{azad2019bi} & 0.8470  & - & 0.7830 & 0.9800 & -    \\ BCDU-Net (d=3)~\cite{azad2019bi} & 0.8510  & -  & 0.7850 & \textbf{0.9820} & -       \\ \textbf{FANet} & \textbf{0.8731} & \textbf{0.8023} & \textbf{0.8650} & 0.9611 & \textbf{0.9235} \\ \bottomrule
\end{tabular}
\label{tab:ISIC2018}
\end{table}

Skin cancer is one of the most commonly diagnosed cancers in the US. Early detection of melanoma can improve the five-year survival rate and help prevent it in 99\% of the cases~\cite{cancerfigures}. Table~\ref{tab:ISIC2018} shows the results on the publicly available International Skin Imaging Collaboration (ISIC) 2018 dataset. FANet outperformed all the methods on almost all evaluation metrics (F1, mIoU, and recall). FANet achieved 0.8731 on F1 and recall of 0.8650 with an improvement of 2.21\% and 8.00\%, respectively, over the most accurate \ac{SOTA} BCDU-Net (d=3) method. A competitive specificity and precision were also recorded. From the qualitative results in Figure~\ref{fig:qualitativeresult}, we can see that the input mask produced by Otsu thresholding shows under segmentation, which is improved significantly using FANet. The masks produced by FANet have smooth boundaries. 

\subsubsection{Results on DRIVE dataset}
\begin{table}[!t]
\footnotesize
\centering
\caption{Results on the DRIVE dataset~\cite{staal2004ridge}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l|l@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{mIoU} & \textbf{Recall}& \textbf{Spec.} & \textbf{Prec.} \\ 
\hline
\hline
U-Net~\cite{ronneberger2015u} & 0.8174 & - & 0.7822 & 0.9808 & - \\ Residual U-Net~\cite{alom2018recurrent} & 0.8149 & - & 0.7726 & 0.9820 & - \\ Recurrent U-Net~\cite{alom2018recurrent}  & 0.8155 & - & 0.7751 & 0.9816 & - \\ R2U-Net~\cite{alom2018recurrent} &0.8171 & - &0.7792 &0.9813 & - \\ DenseBlock-UNet~\footnote{\url{https://github.com/DeepTrial/Retina-VesselNet}} &0.8146 & - &0.7928 &0.9776 & - \\ DUNet~\cite{jin2019dunet} & 0.8190 & - &0.7863 &0.9805 & - \\ IterNet~\cite{li2020iternet} &\textbf{0.8218} &- &0.7791 &0.9831 & -  \\ IterNet(Patched)~\cite{li2020iternet} &0.8205 &-  &0.7235 & \textbf{0.9838} &- \\ \textbf{FANet} &0.8183 & \textbf{0.6927} & \textbf{0.8215} &0.9826 & \textbf{0.8189} \\ \bottomrule
\end{tabular}
\label{tab:Drivedataset}
\end{table}

The automated segmentation of vessels in fundus images can assist in the diagnosis and treatment of diabetic retinopathy. The quantitative result on the publicly available DRIVE dataset is presented in Table~\ref{tab:Drivedataset}. We can observe that the proposed FANet achieves an F1 score of 0.8183, \ac{mIoU} of 0.6927, recall of 0.8215, and precision of 0.8189. The proposed method achieves an improvement of 4.24\% in the recall over \ac{SOTA} IterNet~\cite{li2020iternet}. Although the F1 of the IterNet is 0.35\% higher than FANet, the recall is relatively lower, and other metrics such as \ac{mIoU} and precision are not presented. For our proposed FANet, the precision of 0.8189 is well balanced with the obtained recall. The higher recall produced by FANet shows that our method is more clinically relevant. The quality of the segmentation masks in Figure\ref{fig:qualitativeresult} demonstrates the efficiency of FANet. 


\subsubsection{Results on CHASE-DB1 dataset}
\begin{table}[!t]
\footnotesize
\centering
\caption{Results on the CHASE-DB1 dataset~\cite{owen2009measuring}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l|l@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{mIoU }& \textbf{Recall}& \textbf{Spec.} & \textbf{Prec.} \\ 
\hline
\hline
U-Net~\cite{ronneberger2015u} & 0.7993 & - & 0.7840 & 0.9880 & - \\ DenseBlock-UNet~\footnote{\url{https://github.com/DeepTrial/Retina-VesselNet}} & 0.8005 & - &0.8177 &0.9848 & - \\ DUNet~\cite{jin2019dunet} & 0.8000 & - &0.7858 &0.9880 & - \\ IterNet~\cite{li2020iternet} & 0.8072 & - &0.7969 & \textbf{0.9881} & -  \\ \textbf{FANet} & \textbf{0.8108} & \textbf{0.6820} & \textbf{0.8544} & 0.9830 & \textbf{0.7722} \\ \bottomrule
\end{tabular}
\label{tab:chaseDB}
\end{table}
CHASE-DB1 is the second retinal image segmentation dataset used to evaluate our method. For this dataset, there is no official training and test split. We have used 20 images to train our model and 8 images to test as reported in the work of Li et al.~\cite{li2020iternet}. From Table~\ref{tab:chaseDB}, we can observe that our method achieved the highest F1 of 0.8108, \ac{mIoU} of 0.6820, and the highest recall of 0.8544. FANet achieved an improvement of 3.67\% in the recall compared to the SOTA DenseBlock-UNet. 




\begin{figure}[t!]
    \centering
    \includegraphics[clip, width = \columnwidth]{figures/figure_7.png}
    \caption{Qualitative results of FANet on seven biomedical image segmentation datasets. The initial ``input mask'' is an output from \textit{Otsu thresholding} for which a threshold of 0.5 was applied. The final ``output mask'' is the predicted segmentation mask from FANet model.}
    \label{fig:qualitativeresult}
\end{figure}





\subsubsection{Results on EM dataset}
\begin{table}[!t]
\footnotesize
\caption{Results on the EM dataset~\cite{cardona2010integrated}}
\vspace{0.1cm}
\begin{tabular}{@{}l|l|l|l|l|l@{}}
\toprule
\textbf{Method} &\textbf{F1} &\textbf{mIoU} &\textbf{Recall} &\textbf{Specificity} & \textbf{Prec.} \\ 
\hline
\hline
U-Net~\cite{ronneberger2015u}  & - & 0.8830 & - & - & - \\ Wide U-Net~\cite{zhou2018unet} & - & 0.8837 & - & - & - \\ U-Net++~\cite{zhou2019unet++} &- &0.8933 &- &- &- \\ \textbf{FANet}& \textbf{0.9547} & \textbf{0.9134} & \textbf{0.9568} & \textbf{0.8096} & \textbf{0.9529} \\ 
\bottomrule
\end{tabular}
\label{tab:emdataset}
\end{table}


\begin{table*}[!t]
\footnotesize
\centering
\caption{Detailed ablation study of the FANet architecture. Flop is calculated in terms of GMac. ``Rec" stands for Recall, ``Prec" stands for precision, ``Spec" stands for Specificity, ``Acc" stands for Accuracy, and ``Param" stands for total number parameters. B1 -- B4 denotes different network configurations.~\label{tab:ablationstudy}}
\vspace{0.1cm}
\def\arraystretch{1.1}
\begin{tabular}{@{} p{4.2cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.75cm}|p{0.7cm}|p{0.7cm}|p{0.8cm}|p{1.4cm} @{}}

\toprule

\textbf{Method}  &\textbf{mIoU} & \textbf{F1}    & \textbf{Rec}& \textbf{Prec}& \textbf{Spec}& \textbf{Acc} & \textbf{F2} & \textbf{Param} & \textbf{Flops} & \textbf{FPS} & \textbf{Image Size}\\ \hline
\hline

\multicolumn{12}{@{}l}{\textbf{Dataset: Kvasir-SEG}}                   \\ \midrule
Baseline (FANet without MixPool, \textbf{B1}) & 0.7732  & 0.8516 & 0.8835 & 0.8710    & 0.9783      & 0.9563   & 0.8614 & 5.76M  & 70.38 & 104.20 &   \\ Baseline + MixPool (\textbf{B2})  & 0.6378  & 0.7302 & 0.6982 & \textbf{0.9098}   & \textbf{0.9815} & 0.9412   & 0.7039 & 7.72M  & 94.75 & 66.75 &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3}) & 0.7688  & 0.8460 & 0.9047 & 0.8479    & 0.9576      & 0.9474   & 0.8699 & 5.78M  & 76.53 & 101.10 &  \\ Baseline + MixPool + feedback (\textbf{B4}) & \textbf{0.8153}  & \textbf{0.8803} & \textbf{0.9058} & 0.9005  & 0.9794 & \textbf{0.9667} & \textbf{0.8872} & 7.72M  & 94.75 & 68.18  &   \\ \midrule


\multicolumn{12}{@{}l}{\textbf{Dataset: CVC-ClinicDB}}    \\    \midrule                 
Baseline (FANet without MixPool, \textbf{B1}) & 0.8619  & 0.9166 & 0.9310 & 0.9247    & 0.9934      & 0.9877   & 0.9194 & 5.76M & 70.38 & 103.46 &   \\ Baseline + MixPool (\textbf{B2})                               & 0.8541  & 0.9108 & 0.9026 & 0.9296    & 0.9943      & 0.9864   & 0.9048 & 7.72M  & 94.75 & 67.490 &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3}) & 0.8729  & 0.9162 & 0.9052 & \textbf{0.9462} & 0.9941  & 0.9889 & 0.9093 & 5.78M & 76.53 & 99.03 &   \\ Baseline + MixPool + feedback (\textbf{B4})  & \textbf{0.8937}  & \textbf{0.9355} & \textbf{0.9339} & 0.9401  & \textbf{0.9948} & \textbf{0.9916} & \textbf{0.9342} & 7.72M  & 94.75 & 67.910  &   \\ \hline


\multicolumn{12}{@{}l}{\textbf{Dataset: 2018 Data Science Bowl}}    \\  \midrule
Baseline (FANet without MixPool, \textbf{B1}) & 0.8495  & 0.9121 & 0.9047 & 0.9283  & 0.9871  & 0.9800  & 0.9068 & 5.76M & 70.38 & 114.82 &   \\ Baseline + MixPool (\textbf{B2})  & 0.8158  & 0.8893 & 0.8665 & \textbf{0.9289}    & \textbf{0.9887}      & 0.9751   & 0.8733 & 7.72M  & 94.75 & 69.64  &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3}) & 0.8552  & 0.9165 & 0.9189 & 0.9199  & 0.9863  & \textbf{0.9802} & 0.9173 & 5.78M & 76.53 & 100.27 &   \\ Baseline + MixPool + feedback (\textbf{B4}) & \textbf{0.8569}  & \textbf{0.9176} & \textbf{0.9222} & 0.9194    & 0.9860       & 0.9800     & \textbf{0.9195} & 7.72M  & 94.75 & 69.22  &   \\ \hline

\multicolumn{12}{@{}l}{\textbf{Dataset: ISIC 2018}}             \\  \midrule
Baseline (FANet without MixPool, \textbf{B1}) & 0.7908  & 0.8647 & \textbf{0.9033} & 0.8780     & 0.9151 & 0.9151   & \textbf{0.8778} & 5.76M & 70.38 & 111.95 &   \\ Baseline + MixPool (\textbf{B2})  & 0.7486  & 0.8303 & 0.8049 & 0.9214    & \textbf{0.9617}      & 0.9211   & 0.8081 & 7.72M  & 94.75 & 65.91  &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3})  & \textbf{0.8078}  & \textbf{0.8780} & 0.8746 & \textbf{0.9252} & 0.9614 & \textbf{0.9374} & 0.8719 & 5.78M & 76.53 & 99.06  &   \\ Baseline + MixPool + feedback (\textbf{B4})  & 0.8023  & 0.8731 & 0.8650 & 0.9235    & 0.9611  & 0.9351   & 0.8630  & 7.72M  & 94.75 & 71.02  &   \\ \hline

\multicolumn{12}{@{}l}{\textbf{Dataset: DRIVE}}           \\   \midrule
Baseline (FANet without MixPool, \textbf{B1})  & 0.6912  & 0.8172 & 0.8048 & \textbf{0.8339}    & \textbf{0.9846}  & \textbf{0.9687}  & 0.8093 & 5.76M & 70.38 & 103.68 &   \\ Baseline + MixPool (\textbf{B2})   & 0.6895  & 0.8161 & \textbf{0.8219} & 0.8145 & 0.9820 & 0.9678   & 0.8190  & 7.72M  & 94.75 & 68.47  &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3}) & \textbf{0.6928}  & \textbf{0.8183} & 0.8124 & 0.8280 & 0.9839 & \textbf{0.9687}   & 0.8142 & 5.78M & 76.53 & 95.30  &   \\ Baseline + MixPool + feedback (\textbf{B4})  & 0.6927  & \textbf{0.8183} & 0.8215 & 0.8189 & 0.9826    & 0.9683   & \textbf{0.8197} & 7.72M  & 94.75 & 70.66  &   \\ \hline

\multicolumn{12}{@{}l}{\textbf{Dataset: CHASE-DB1}}                   \\   \midrule
Baseline (FANet without MixPool, \textbf{B1})                  & 0.6419  & 0.7816 & 0.7876 & 0.7768    & 0.9848      & 0.9723   & 0.7850 & 5.76M & 70.38 & 95.77  &   \\ Baseline + MixPool (\textbf{B2})                              & 0.5419  & 0.7009 & 0.8116 & 0.6209    & 0.9664      & 0.9565   & 0.7625 & 7.72M  & 94.75 & 65.03  &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3}) & \textbf{0.6877}  & \textbf{0.8147} & 0.8372 & \textbf{0.7948}    & \textbf{0.9855}      & 0\textbf{.9760}   & 0.8279 & 5.78M & 76.53 & 99.00  &   \\ Baseline + MixPool + feedback (\textbf{B4})  & 0.6820  & 0.8108 & \textbf{0.8544} & 0.7722    & 0.9830      & 0.9749   & \textbf{0.8363} & 7.72M  & 94.75 & 71.67  &   \\ \hline


\multicolumn{12}{@{}l}{\textbf{Dataset: EM}}               \\  \midrule
Baseline (FANet without MixPool, \textbf{B1}) & 0.9128  & 0.9544 & \textbf{0.9597} & 0.9495    & 0.7946      & 0.9263   & \textbf{0.9575} & 5.76M & 70.38 & 79.59  &   \\ Baseline + MixPool (\textbf{B2})     & 0.9121  & 0.9540  & 0.9596 & 0.9488    & 0.7918      & 0.9257   & 0.9573 & 7.72M  & 94.75 & 59.15  &   \\ Baseline + MixPool(E1, D4) + feedback (\textbf{B3}) & 0.9042  & 0.9497 & 0.9404 & \textbf{0.9594}    & \textbf{0.8378}      & 0.9198   & 0.9441 & 5.78M & 76.53 & 90.62  &   \\ Baseline + MixPool + feedback (\textbf{B4}) & \textbf{0.9134} & \textbf{0.9547} & 0.9568 & 0.9529  & 0.8096 & \textbf{0.9271}   & 0.9559 & 7.72M  & 94.75 & 70.70  &   \\ \hline
\bottomrule
\end{tabular}
\end{table*}

EM dataset aims to develop an automatic \ac{ML} algorithm for the segmentation of the neural structures so that difficulties due to manual labeling can be resolved. Table~\ref{tab:emdataset} shows the quantitative results on the EM dataset. The proposed FANet also obtains F1 of 0.9547, \ac{mIoU} of 0.9134, and a recall of 0.9568. The presented results demonstrate that FANet produces \ac{SOTA} results, surpassing other recent methods in terms of \ac{mIoU} metric that was used by other methods for comparison. 

\subsection{Qualitative results}
The qualitative results on all seven datasets are presented in Fig.~\ref{fig:qualitativeresult}. It can be observed that for colonoscopy datasets (Kvasir-SEG and CVC-ClinicDB), even though the initial input mask covers the entirety of the image, our model is able to prune and provide accurate masks. The same can be observed for the two retina vessel segmentation datasets, DRIVE and CHASE-DB1. It can be observed that our model is able to segment the challenging retinal vessels, including small retinal vessel bifurcations, and it well resembles the ground truth mask. For the 2018 DSB, ISIC-2018, and EM cell data, again, the input masks are finely rectified, achieving close to ground truth results by the proposed FANet model.

\subsection{Ablation study}
\label{ablationstudy}
In this section, we ablate our model architecture and present extensive experimental results that show the effectiveness of proposed FANet.  To evaluate the contribution of the MixPool block and the feedback, we created the following configurations:
\begin{enumerate}
    \item \textbf{Baseline} (\textbf{B1}): It refers to the FANet without MixPool block in the model architecture.
    
    \item \textbf{Baseline + MixPool} (\textbf{B2}): We integrate the MixPool block in all the encoder blocks and decoder blocks. During the inference, we directly apply the trained model weights with the Ostu thresholding (initial input mask) only once, i.e., no feedback mechanism is used.
    
    \item \textbf{Baseline + MixPool(E1, D4) + Feedback} (\textbf{B3}): Here, we integrate the MixPool block in the first encoder block and the last decoder block. Feedback is used during the inference.
    
    \item \textbf{Baseline + MixPool + Feedback} (\textbf{B4}): This is the final FANet architecture, with MixPool block in all encoder and decoder blocks and the feedback mechanism is used during the inference.
    
\end{enumerate}
Table~\ref{tab:ablationstudy} presents the ablation results on these four configurations performed on all seven datasets. Below we provide detailed analyses of the use of different model architectural settings and validate them with the above described four network configurations (\textbf{B1-B4}):
\subsubsection{Effectiveness of MixPool block}
The MixPool block is an essential part of the proposed FANet architecture. It uses the previously predicted mask as the attention to improve the semantically meaningful features and allows higher-level abstractions.  The effectiveness of the MixPool block can be evaluated by comparing the network configurations B1 and B4.

From the experiments in Table~\ref{tab:ablationstudy}, we can conclude that the B4 outperforms the B1 on all the datasets. On the F1 metric, B4 shows an improvement of 2.87\% on the Kvasir-SEG dataset, 1.89\% improvement on the CVC-ClinicDB, 0.55\% improvement on the 2018 Data Science Bowl dataset, 0.84\% improvement on the ISIC 2018 dataset, 0.11\% improvement on the DRIVE dataset, 2.92\% improvement on the CHASE-DB1 dataset, and a 0.03\% improvement on the EM dataset. These performance gains are significant and thus demonstrate the effectiveness of the use of MixPool block in the proposed FANet. 
\subsubsection{Optimum position of Mixpool block in FANet architecture}
The positioning of the MixPool is an important factor determining the performance of the model. In the FANet (B4), we integrate the MixPool block in all the encoder blocks and the decoder blocks. In B3, we integrate MixPool block in the first encoder block and the last decoder block only. To evaluate the effectiveness of the integrating MixPool block, we compare B3 with B4 in Table~\ref{tab:ablationstudy}. It can be observed that out of the seven datasets, on three datasets, i.e., Kvasir-SEG, CVC-ClinicDB, and 2018 Data Science Bowl, a  significant improvement in B4 is observed as compared to the B3. On the F1 metric, we can observe that B4 achieves an improvement of 3.43 \% on Kvasir-SEG, 1.93 \% on CVC-ClinicDB, 0.11 \% on the 2018 Data Science Bowl, and 0.5 \% on the EM dataset. 
\subsubsection{Significance of feedback during evaluation}
The proposed architecture uses the feedback information (input mask) while training. This feedback mechanism is also used during the evaluation. To check the effectiveness of the feedback mechanism, we compare the B2 (FANet without feedback) with the B4 (FANet with feedback) in Table~\ref{tab:ablationstudy}. On all the datasets, we used feedback during inference and compared its performance with the model without feedback. We can observe that the majority of performance gains in \ac{mIoU} and F1. For Kvasir-SEG,  B4 shows a 17.75 \% improvement in the  mIOU, 15.01 \% improvement in the F1, and a 20.76 \% improvement in the Recall. Likewise, on the CVC-ClinicDB, we can see that B4 has 3.96 \% improvement in  \ac{mIoU} and 2.27 \% improvement in the F1.

\section{Conclusion}
\label{conclusion}
With the FANet architecture, we proposed a novel approach for biomedical image segmentation that can self-rectify the predicted masks. By introducing a feedback mechanism, we achieved an improvement on seven publicly available biomedical datasets when compared with existing \ac{SOTA} methods. Our approach required far fewer epochs for training and is well-suited to diverse biomedical imaging datasets. The feedback mechanism integrated in FANet design effectively acts as hard attention that is used with the existing feature maps to boost the strength of feature representations. The experimental results demonstrated that the proposed architecture achieves accurate and consistent segmentation results across several biomedical imaging datasets despite its simple and straightforward network architecture. The ablation study also revealed that the FANet requires less training time to achieve near \ac{SOTA} performance. In the future, we will use a contrastive learning approach to improve the performance of FANet further on additional multimodal biomedical images. 




\bibliographystyle{IEEEtran}
\bibliography{references} 

\end{document}