\documentclass[10pt,journal]{IEEEtran}




\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.pdf,.jpg,.gif,.png}




\usepackage{multirow,setspace,verbatim,amsfonts,graphicx,amsmath,amsthm,amsbsy,amssymb,epsfig,url,cite}
\usepackage{amsthm}
\usepackage{gensymb}
\usepackage{graphicx}

\usepackage{amsfonts}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz,graphicx}
\usepackage{mathrsfs}

\usepackage{algorithmicx}

\usepackage{array}
\usepackage{booktabs}



\usepackage{amsfonts}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz,graphicx}
\usepackage{mathrsfs} 
\usepackage[algo2e]{algorithm2e} 

\usepackage{algorithm}
\usepackage{array,multirow}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{subfiles}
\usepackage{titling}


\newcommand{\LE}{\mathcal{L}_{\lambda, \mu}}
\newcommand{\T}{{t}_{\rm max}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\OH}{\mathcal{H}}

\newcommand{\K}{\kappa}
\newcommand{\ds}{\displaystyle}
\renewcommand{\Re}{{\mathbb{R}}}


\newcommand{\Bbc}{\boldsymbol{\cal B}}

\newcommand{\Dbc}{\boldsymbol{\cal D}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\Gammab}{\boldsymbol{\Gamma}}
\newcommand{\bA}{{\mathbf{A}}}
\newcommand{\bB}{{\mathbf{B}}}


\newcommand{\Gbc}{\boldsymbol{\cal G}}
\newcommand{\Lbc}{\boldsymbol{\cal L}}

\newcommand{\bd}{\mathbf{d}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bg}{{\mathbf{g}}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}


\newcommand{\bP}{\mathbf{P}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\GG}{\mathbf{G}}
\newcommand{\bGam}{\mathbf{\Gamma}}
\newcommand{\II}{\mathbf{I}}
\newcommand{\bLambda}{{\mathbf{\Lambda}}}
\newcommand{\bS}{\mathbf{S}}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\circul}{\mathfrak{C}}




\newcommand{\Cm}{{\mathcal C}}
\newcommand{\Ab}{{\mathbf A}}
\newcommand{\Bb}{{\mathbf B}}
\newcommand{\Cb}{{\mathbf{C}}}
\newcommand{\Db}{{\mathbf D}}
\newcommand{\Eb}{{\mathbf E}}
\newcommand{\Fb}{{\mathbf F}}
\newcommand{\Gb}{{\mathbf G}}
\newcommand{\Hb}{{\mathbf H}}
\newcommand{\Ib}{{\mathbf I}}
\newcommand{\Jb}{{\mathbf J}}
\newcommand{\Kb}{{\mathbf K}}
\newcommand{\Lb}{{\mathbf L}}
\newcommand{\Mb}{{\mathbf M}}
\newcommand{\Nb}{{\mathbf N}}
\newcommand{\Ob}{{\mathbf O}}
\newcommand{\Pb}{{\mathbf P}}
\newcommand{\Qb}{{\mathbf Q}}
\newcommand{\Rb}{{\mathbf R}}
\newcommand{\Sb}{{\mathbf S}}
\newcommand{\Tb}{{\mathbf T}}
\newcommand{\Ub}{{\mathbf U}}
\newcommand{\Vb}{{\mathbf V}}
\newcommand{\Wb}{{\mathbf W}}
\newcommand{\Xb}{{\mathbf X}}
\newcommand{\Yb}{{\mathbf Y}}
\newcommand{\Zb}{{\mathbf Z}}

\newcommand{\ab}{{\mathbf a}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\cb}{{\mathbf c}}
\newcommand{\db}{{\mathbf d}}
\newcommand{\eb}{{\mathbf e}}
\newcommand{\fb}{{\mathbf f}}
\newcommand{\gb}{{\mathbf g}}
\newcommand{\hb}{{\mathbf h}}
\newcommand{\ib}{{\mathbf i}}
\newcommand{\jb}{{\mathbf j}}
\newcommand{\kb}{{\mathbf k}}
\newcommand{\lb}{{\mathbf l}}
\newcommand{\mb}{{\mathbf m}}
\newcommand{\nb}{{\mathbf n}}
\newcommand{\ob}{{\mathbf o}}
\newcommand{\pb}{{\mathbf p}}
\newcommand{\qb}{{\mathbf q}}
\newcommand{\rb}{{\mathbf r}}
\renewcommand{\sb}{{\mathbf s}}
\newcommand{\tb}{{\mathbf t}}
\newcommand{\ub}{{\mathbf u}}
\newcommand{\vb}{{\mathbf v}}
\newcommand{\wb}{{\mathbf w}}
\newcommand{\xb}{{\mathbf x}}
\newcommand{\yb}{{\mathbf y}}
\newcommand{\zb}{{\mathbf z}}
\newcommand{\Hs}{\mathscr{H}}
\newcommand{\Xs}{\mathscr{X}}
\newcommand{\Phib}{{\boldsymbol {\Phi}}}
\newcommand{\Psib}{{\boldsymbol {\Psi}}}
\newcommand{\Thetab}{{\boldsymbol {\Theta}}}
\newcommand{\Lambdab}{{\boldsymbol {\Lambda}}}
\newcommand{\Upsilonb}{{\boldsymbol {\Upsilon}}}
\newcommand{\Sigmab}{{\boldsymbol {\Sigma}}}
\newcommand{\Rd}{{\mathbb R}}
\newcommand{\Cd}{{\mathbb C}}
\newcommand{\deltab}{{\boldsymbol{\delta}}}
\newcommand{\mub}{{\boldsymbol{\mu}}}
\newcommand{\phib}{{\boldsymbol{\phi}}}
\newcommand{\psib}{{\boldsymbol{\psi}}}
\newcommand{\rhob}{{\boldsymbol {\rho}}}
\newcommand{\alphab}{{\boldsymbol {\alpha}}}
\newcommand{\0}{{\boldsymbol{0}}}
\newcommand{\Ybc}{{\boldsymbol{\mathcal Y}}}
\newcommand{\Sbc}{{\boldsymbol{\mathcal S}}}
\newcommand{\Qbc}{{\boldsymbol{\mathcal Q}}}
\newcommand{\Zbc}{{\boldsymbol{\mathcal Z}}}
\newcommand{\Abc}{{\boldsymbol{\cal A}}}
\newcommand{\Ebc}{{\boldsymbol{\cal E}}}
\newcommand{\Cbc}{{\boldsymbol{\cal C}}}
\newcommand{\thetab}{{\boldsymbol {\theta}}}
\newcommand{\chib}{{\boldsymbol {\chi}}}
\newcommand{\nub}{{\boldsymbol {\nu}}}
\newcommand{\Hbc}{{\boldsymbol{\mathcal H}}}
\newcommand{\Xbc}{{\boldsymbol{\mathcal X}}}
\newcommand{\Kbc}{{\boldsymbol{\mathcal K}}}
\newcommand{\Mbc}{{\boldsymbol{\mathcal M}}}
\newcommand{\Ed}{{{\mathbb E}}}
\newcommand{\Zd}{\mathbb{Z}}
\newcommand{\rank}{\textsc{rank}}
\newcommand{\hank}{\mathbb{H}}
\newcommand{\conv}{\mathscr{C}}
\newcommand{\Tconv}{\mathcal{T}}
\newcommand{\zerob}{\mathbf{0}}
\newcommand{\hbk}{\boldsymbol{\mathfrak{h}}}
\newcommand{\Rbc}{{\mathfrak{R}}}


\newcommand{\Ac}{{\mathcal A}}
\newcommand{\Bc}{{\mathcal B}}
\newcommand{\Cc}{{\mathcal C}}
\newcommand{\Dc}{{\mathcal D}}
\newcommand{\Ec}{{\mathcal E}}
\newcommand{\Fc}{{\mathcal F}}
\newcommand{\Gc}{{\mathcal G}}
\newcommand{\Hc}{{\mathcal{H}}}
\newcommand{\Ic}{{\mathcal I}}
\newcommand{\Jc}{{\mathcal J}}
\newcommand{\Kc}{{\mathcal K}}
\newcommand{\Lc}{{\mathcal L}}
\newcommand{\Mc}{{\mathcal M}}
\newcommand{\Nc}{{\mathcal N}}
\newcommand{\Oc}{{\mathcal O}}
\newcommand{\Pc}{{\mathcal P}}
\newcommand{\Qc}{{\mathcal Q}}
\newcommand{\Rc}{{\mathcal R}}
\newcommand{\Sc}{{\mathcal S}}
\newcommand{\Tc}{{\mathcal T}}
\newcommand{\Uc}{{\mathcal U}}
\newcommand{\Vc}{{\mathcal V}}
\newcommand{\Wc}{{\mathcal W}}
\newcommand{\Xc}{{\mathcal X}}
\newcommand{\Yc}{{\mathcal Y}}
\newcommand{\Zc}{{\mathcal Z}}



\newcommand{\vect}{\textsc{Vec}}


\newcommand{\etal}{\emph{et al}}

\newcommand{\re}{{\mathrm{Re}}}
\newcommand{\im}{{\mathrm{Im}}}
\renewcommand{\Re}{{\mathbb{R}}}
\newcommand{\taub}{\boldsymbol{\tau}}
\newcommand{\Tbc}{\boldsymbol{\cal T}}
\newcommand{\oneb}{\boldsymbol{1}}





\newtheorem{theorem}{Theorem}\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{property}{Property}
\newtheorem{question}{Question}
\renewcommand{\thelemma}{\arabic{section}.\arabic{lemma}}
\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}
\renewcommand{\thecorollary}{\arabic{section}.\arabic{corollary}}
\renewcommand{\theproposition}{\arabic{section}.\arabic{proposition}}
\renewcommand{\thedefinition}{\arabic{section}.\arabic{definition}}


\newcommand{\Ran}{\textsc{Ran}}
\newcommand{\id}{\mathrm{id}}

\usepackage{xcolor}


\newcommand{\beginsupplement}{\setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}}







\begin{document}


\title{ $k$-Space   Deep Learning for Accelerated MRI }
\date{\vspace{-4ex}}

\author{Yoseob~Han,~Leonard Sunwoo,~and~Jong~Chul~Ye,~\IEEEmembership{Senior Member,~IEEE}\thanks{Y. Han and J.C. Ye with the Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology (KAIST), 
		Daejeon 34141, Republic of Korea (e-mail: \{hanyoseob,jong.ye\}@kaist.ac.kr). L. Sunwoo is with the Department of Radiology, Seoul National University College of Medicine, Seoul National University Bundang Hospital, Seongnam, Republic of Korea.
		J.C. Ye is also with the Department of Mathematical Sciences, KAIST.} 
\thanks{This work is supported by Korea Science and Engineering Foundation, Grant
		number NRF2016R1A2B3008104.}}




\maketitle

\begin{abstract}
The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one of the state-of-the-art compressed sensing approaches that directly interpolates the missing $k$-space data using low-rank Hankel matrix completion. The success of ALOHA is due to the concise signal representation in the $k$-space domain thanks to the duality between structured low-rankness in the $k$-space domain and the image domain sparsity. Inspired by the recent mathematical discovery  that links convolutional neural networks to Hankel matrix decomposition using data-driven framelet basis,
here we  propose  a fully data-driven  deep learning algorithm for $k$-space interpolation. Our network can be also easily applied to non-Cartesian $k$-space trajectories by simply adding an additional regridding layer. Extensive numerical experiments show that the proposed deep learning method consistently outperforms  the existing image-domain deep learning approaches.
\end{abstract}

\begin{IEEEkeywords}
Compressed sensing MRI,  Deep Learning,  Hankel structured low-rank completion,  Convolution framelets
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


\section{Introduction}
\label{sec:introduction}


\IEEEPARstart{R}{ecently}, 
inspired by the tremendous success of deep learning \cite{krizhevsky2012imagenet,he2016deep,ronneberger2015u}, 
many researchers have investigated deep learning approaches  for MR reconstruction problems
and successfully demonstrated significant performance gain
\cite{wang2016accelerating,hammernik2018learning,lee2018deep,han2017deep,jin2017deep,schlemper2018deep,zhu2018image}.


In particular,
Wang et al \cite{wang2016accelerating}  used the deep learning reconstruction either as  an initialization  or  a  regularization term.
Deep network architecture using unfolded iterative compressed sensing (CS) algorithm was also proposed   to learn a set of  regularizers and associated filters
\cite{hammernik2018learning}. 
These works were followed by novel
extension using deep residual learning \cite{lee2018deep}, domain adaptation \cite{han2017deep}, data consistency layers \cite{schlemper2018deep}, etc. An extreme form of the neural network called Automated Transform by Manifold Approximation
(AUTOMAP) \cite{zhu2018image} even attempts to estimate the Fourier transform itself using fully connected layers.
All these pioneering works have consistently demonstrated superior reconstruction performances over the compressed sensing approaches \cite{lustig2007sparse,jung2009k,shin2014calibrationless,haldar2014low,jin2016general,ongie2016off}
at significantly lower run-time computational complexity.


\begin{figure}[!hbt] 	
\centerline{\includegraphics[width=0.65\linewidth]{fig/i-deep}}
\vspace{-0.4cm}
\centerline{\mbox{(a)}}
\vspace{0.5cm}
\centerline{\includegraphics[width=0.65\linewidth]{fig/i-cascade}}
\vspace{-0.4cm}
\centerline{\mbox{(b)}}
\centerline{\includegraphics[width=0.65\linewidth]{fig/AUTOMAP}}
\vspace{-0.4cm}
\centerline{\mbox{(c)}}
\centerline{\includegraphics[width=0.65\linewidth]{fig/k-deep}}
\vspace{-0.4cm}
\centerline{\mbox{(d)}}
\caption{Deep learning frameworks for accelerated MRI: (a) image domain learning  \cite{lee2018deep,han2017deep}, (b) cascaded network \cite{hammernik2018learning,wang2016accelerating,schlemper2018deep}, (c) AUTOMAP \cite{zhu2018image},
and (d) the proposed $k$-space learning.  IFT: Inverse Fourier transform. }
\vspace*{-0.5cm}
\label{fig:deepMR}
\end{figure}


Although the  end-to-end recovery approach like AUTOMAP \cite{zhu2018image} may directly recover the image without ever interpolating the missing $k$-space samples (see Fig.~\ref{fig:deepMR}(c)),
it works only for the sufficiently small size images due to its huge memory requirement for fully connected layers.
Accordingly, most of the popular deep learning MR reconstruction algorithms
are either in the form of  image domain post-processing as shown in Fig.~\ref{fig:deepMR}(a) \cite{lee2018deep,han2017deep}, or iterative updates between the $k$-space and the image domain using a cascaded network as shown in Fig.~\ref{fig:deepMR}(b) \cite{hammernik2018learning,wang2016accelerating,schlemper2018deep,aggarwal2019modl}.



One of the main purposes of this paper is  to reveal that the aforementioned approaches are not all the available options for MR deep learning, but there exists 
another  effective
deep learning approach.
In fact, as illustrated in Fig.~\ref{fig:deepMR}(d),
the proposed deep learning approach directly interpolates the missing $k$-space data so that accurate reconstruction can be obtained by simply taking
the Fourier transform of the interpolated $k$-space data.
In contrast to AUTOMAP  \cite{zhu2018image},  our network is implemented in the form of convolutional neural network (CNN) 
without requiring fully connected layer, so the GPU memory requirement for the proposed
$k$-space deep learning is minimal.


In fact,  our $k$-space deep learning scheme is inspired by the success of structured low-rank Hankel matrix approaches \cite{shin2014calibrationless,haldar2014low,jin2016general,ongie2016off,ye2016compressive},
exploiting  concise signal representation in the $k$-space domain   thanks to the duality between
the structured low-rankness in the $k$-space  and the image domain sparsity.
In addition, the recent theory of deep convolutional framelets \cite{ye2017deep} showed that
an encoder-decoder network can be regarded as a signal representation that emerges from  Hankel matrix decomposition.
Therefore,  by synergistically combining these findings, we  propose
a novel  $k$-space deep learning algorithms that  perform better and generalize well.
We further show that our deep learning approach for
 $k$-space interpolation can handle general $k$-space sampling patterns beyond the Cartesian trajectory, such as radial, spiral, etc. 
Moreover, our theory and empirical results  also shows that
multi-channel calibration-free $k$-space interpolation can be easily realized using the
proposed framework by simply stacking multi-coil $k$-space data along the channel direction as an input to feed in the neural
network.

We are aware of recent $k$-space neural network approaches called the scan-specific robust artificial neural networks for $k$-space interpolation (RAKI)
\cite{akccakaya2019scan}. Unlike the proposed method, RAKI considers scan-specific learning without training data, so the neural network weights needs to 
be recalculated for each $k$-space input data. On the other hand, the proposed method exploits the generalization capability of the neural network.  More specifically, even with the same trained weights, the trained neural network can generate diverse signal representation depending on the input  thanks to the combinatorial nature 
of the ReLU nonlinearity. This
 makes the neural network expressive and generalized well to the unseen test data. The theoretical origin of the expressiveness will be also discussed in this paper.

After the original version of this work was available on Arxiv,  there appear several  deep learning
algorithms exploiting $k$-space learning \cite{pramanik2018off,aggarwal2018multi}. These works are based on  hybrid formulation (see Fig.~\ref{fig:deepMR}(b))  and utilize the
deep neural network as a regularization term for $k$-space denoising. On the other hand,
 the our method
exploits the nature of  deep neural network as a generalizable and expressive $k$-space representation to directly
interpolate the missing $k$-space data.





\section{Mathematical Preliminaries}

\subsection{Notations}

In this paper, matrices are denoted by bold upper case letters, i.e. $\Ab,\Bb$, whereas
the vectors are represented by bold lower cases letters, i.e. $\xb,\yb$.
In addition, $[\Ab]_{ij}$ refers to the $(i,j)$-th element of the matrix $\Ab$, and $x[i]$ denotes the $i$-th element of the 
vector $\xb$.
The notation $\overline \vb\in \Rd^d$ for a vector $\vb\in \Rd^d$ denotes its flipped version, i.e. the indices of $\vb$ are time-reversed
such that $\overline\vb[n]=\vb[-n]$.
The $N\times N$ identity matrix is denoted as $\Ib_N$, while $\oneb_N$ is an $N$-dimensional vector
with 1's. 
The superscript $^T$ and $^\top$ for a matrix or vector denote
the transpose and Hermitian transpose, respectively.
$\Rd$ and $\Cd$ denote the real and imaginary fields, respectively.
$\Rd_+$ refers to the nonnegative real numbers.


\subsection{Forward Model for Accelerated MRI}




The spatial Fourier transform of an arbitrary smooth function $x:\RR^2\to\RR$ is defined by 
\begin{align*}
\hat{x}(\bk)=\mathcal{F}[x](\bk):=\int_{\RR^d} e^{-\iota\bk\cdot \rb}x(\rb)d\rb,
\end{align*}
with spatial frequency $\bk\in\RR^2$ and $\iota=\sqrt{-1}$.
Let  $\{\bk_n\}_{n=1}^N$, for some integer $N\in\NN$, be a collection of finite number of sampling points of the $k$-space
 confirming to the Nyquist sampling rate. 
 Accordingly, the discretized $k$-space data  $\widehat\xb\in \Cd^N$  is introduced by
\begin{equation}\label{eq:coil}
\widehat \xb = \begin{bmatrix} \hat x[0] &\cdots & \hat x[N-1]\end{bmatrix}, 
\quad \mbox{where} \quad \hat x[i] = \widehat x(\kb_i) . \end{equation}
For a given under-sampling pattern $\Lambda$ for accelerated MR acquisition, let 
  the downsampling operator $\Pc_\Lambda: \Cd^{N} \to \Cd^{N}$ 
  be defined as
  \begin{eqnarray}
  \left[\Pc_\Lambda[\hat \xb] \right]_i= \begin{cases}  \widehat x[i], &i \in \Lambda \\
0, &  \mbox{otherwise} \end{cases}   \   .
  \end{eqnarray}
Then, the under-sampled  $k$-space data is given by
\begin{eqnarray}\label{eq:fwd}
\hat \yb & :=\Pc_\Lambda[\hat \xb] 
\end{eqnarray}




\subsection{Low-Rank Hankel Matrix Approaches} \label{sec:theory}


From the undersampled data in \eqref{eq:fwd}, CS-MRI  \cite{lustig2007sparse,jung2009k} attempts to find the feasible solution  that has minimum non-zero support in some sparsifying
transform domain. This can be achieved
by finding a function $z:\Rd^2\to \Rd$ such that 
\begin{eqnarray}
\min_z &  \|\Tc z\|_1 \notag\\
\mbox{subject to } & \Pc_\Lambda[\hat \xb] = \Pc_\Lambda[\hat \zb]\end{eqnarray}
where $\Tc$ denotes the image domain sparsifyting transform
and 
\begin{equation}\widehat \zb = \begin{bmatrix} \widehat z(\kb_0) & \cdots  & \widehat z(\kb_{N-1}) \end{bmatrix}^T  \  .
\end{equation}
This optimization problem usually requires  iterative update between the $k$-space and the image domain after the discretization of  $z(\rb)$  \cite{lustig2007sparse,jung2009k}.


On the other hand, in  recent structured low-rank  matrix completion algorithms  \cite{shin2014calibrationless,haldar2014low,jin2016general,ongie2016off,ye2016compressive},
the compressed sensing problems was solved either by imposing the low-rank structured matrix penalty \cite{haldar2014low,ongie2016off}
or by converting it a direct $k$-space interpolation problem using low-rank structure matrix completion \cite{shin2014calibrationless,jin2016general}.
More specifically, let $\hank_d(\widehat \xb)$ denote a 
 Hankel matrix constructed from the $k$-space measurement $\widehat \xb$ in \eqref{eq:coil}, where $d$ denotes the
 matrix pencil size (for more details on the construction of Hankel matrices and their relation to the convolution, see Section I in Supplementary Material).
Then,
 if the underlying signal $x(\rb)$ in the image domain is sparse and  described as the signal with the finite rate of innovations (FRI) with rate $s$ \cite{vetterli2002sampling},
  the associated Hankel matrix $\hank_d(\hat \xb)$ with  $d>s$
 is low-ranked  \cite{ye2016compressive,jin2016general,ongie2016off}.
  Therefore, if some of $k$-space data  are missing,
we can construct an appropriate weighted Hankel matrix with missing elements such that the missing elements are recovered 
using low-rank Hankel matrix completion approaches \cite{candes2009exact}:
\begin{eqnarray}\label{eq:EMaC}
(P)
 &\min\limits_{\widehat \zb\in \Cd^N } & \rank~ \hank_d (\widehat \zb)  \\
&\mbox{subject to } & \Pc_\Lambda[\widehat\xb ] = \Pc_\Lambda[\widehat \zb]  \nonumber  \  .
\end{eqnarray}
While the low-rank Hankel matrix completion problem $(P)$  can be solved in various ways, 
one of the main technical huddles is its relatively large computational complexity for matrix factorization and  large memory 
requirement for storing Hankel matrix. 
Although several new approaches have been proposed to solve these problems \cite{ongie2017fast}, the following section shows that a deep learning approach is a novel and efficient way to solve this problem.

\section{Main Contribution}

\subsection{ALOHA as a signal representation}

Consider the following image regression problem under the  low-rank Hankel matrix constraint:
\begin{eqnarray}
 \quad & \min_{\widehat\zb\in \Cd^{N}}  & \left\|x- \Fc^{-1}[ \widehat\zb]\right\|^2 \label{eq:imgcost}  \\
&\mbox{subject to }  &\rank~ \hank_{d}\left( \widehat\zb\right) =s ,  \quad
  \Pc_\Lambda \left[\widehat\xb\right]= \Pc_\Lambda \left[\widehat \zb\right],\quad  \label{eq:ccost}
\end{eqnarray}
where  $s$ denotes the estimated rank. In the above formulation,  the cost in \eqref{eq:imgcost} is defined in the image domain to minimize the
errors in the image domain, whereas
the low-rank Hankel matrix constraint in \eqref{eq:ccost} is imposed in the $k$-space after the $k$-space weighting.



Now, we convert the complex-valued constraint in \eqref{eq:ccost}
to a  real-valued constraint.
The procedure is as follows. First, 
the operator  $\Rbc:\Cd^{N } \to \Rd^{N\times 2}$ is defined as 
\begin{eqnarray}\label{eq:Rbc}
\Rbc[\widehat\zb]:= \begin{bmatrix}\re(\hat \bz) & \im (\hat \bz) \end{bmatrix},\quad \forall \widehat\zb \in \Cd^{N}
\end{eqnarray}
where $\re(\cdot)$ and $\im(\cdot)$ denote the real and imaginary part of the argument.
Similarly, we define its inverse operator  $\Rbc^{-1}:\Rd^{N\times 2} \to \Cd^{N}$ as \begin{eqnarray}\label{eq:Za}
\Rbc^{-1}[\widehat\Zb]:= \hat \zb_1+\iota \hat \zb_2,\quad \forall \widehat\Zb :=[\zb_1~ \zb_{2}]\in \Rd^{N\times 2}
\end{eqnarray}
Then, as shown in Section II in Supplementary Material, 
we can approximately convert \eqref{eq:ccost} to an 
optimization problem with real-valued constraint:
\begin{eqnarray}
(P_A) &\min_{\widehat\zb \in \Cd^{N}}  & \left\|x- \Fc^{-1}[\widehat\zb ]\right\|^2  
\end{eqnarray}
\begin{eqnarray*}
\mbox{subject to}& \rank  \hank_{d|2}\left( \Rbc[\widehat\zb ]\right) =Q  \leq 2s , \notag  \\
& \Pc_\Lambda \left[\widehat\xb\right]= \Pc_\Lambda \left[\widehat \zb \right] .\quad   \label{eq:rcost}
\end{eqnarray*}

In the recent theory of deep convolutional framelets \cite{ye2017deep}, 
this low-rank constraint optimization problem was addressed  using  learning-based signal representation.
More specifically,
 for any  $\widehat \zb\in \Cd^N$, let the Hankel structured matrix $\hank_{d|2}\left(\Rbc[\widehat \zb ]\right)$
 have  the singular value decomposition
$\Ub \Sigmab \Vb^{\top}$, 
where $\Ub =[\bu_1 \cdots \bu_Q] \in \Rd^{N\times Q}$ and $\Vb=[\bv_1\cdots \bv_Q]\in \Rd^{2d \times Q}$ denote the left and the right singular vector bases matrices, respectively;
$\Sigmab=(\sigma_{ij})\in\mathbb{R}^{Q\times Q}$ is the diagonal matrix with singular values.  
Now, consider
matrix pair  $\Psib$, $\tilde \Psib\in \Rd^{2d \times Q}$ 
\begin{eqnarray}\label{eq:Psi}
\Psib:=
\begin{pmatrix}
\psib^{1}_{1} & \cdots & \psib^{1}_{Q}
\\
\psib^{2}_{1} & \cdots &\psib^{2}_{Q}
\end{pmatrix}
\,\text{ and }\,
\widetilde{\Psib}:=
\begin{pmatrix}
\tilde\psib^{1}_{1} & \cdots & \tilde\psib^{1}_{Q}
\\
\tilde\psib^{2}_{1} & \cdots &\tilde\psib^{2}_{Q}
\end{pmatrix}\end{eqnarray}
that 
satisfy the low-rank  projection constraint:
\begin{eqnarray}\label{eq:projection}
 \Psib \tilde \Psib^{\top} = \Pb_{R(\Vb)} ,
 \end{eqnarray}
 where $\Pb_{R(\Vb)}$ denotes the projection matrix to the range space of $\Vb$.
 We further introduce the generalized pooling and unpooling matrices $\Phib,\widetilde\Phib\in \Rd^{N\times M}$ \cite{ye2017deep} that
 satisfies the condition
\begin{eqnarray}\label{eq:projectionU}
 \widetilde\Phib \Phib^{\top} =\Ib_N, \end{eqnarray} 
 Using Eqs. \eqref{eq:projection} and \eqref{eq:projectionU},  we can obtain the following matrix equality:
\begin{eqnarray}\label{eq:B}
\hank_{d|2}\left(\Rbc[\widehat \zb ]\right) = \widetilde\Phib \Phib^{\top}\hank_{d|2}\left(\Rbc[\widehat \zb ]\right) \Psib \tilde \Psib^{\top} =  \widetilde\Phib \Cb \tilde \Psib^{\top},  \label{eq:equiv}
\end{eqnarray}
where 
\begin{eqnarray}\label{eq:C}
\Cb :=  \Phib^{\top}\hank_{d|2}\left(\Rbc[\widehat \zb ]\right)  \Psib \quad   \in  \Rd^{N\times Q}
\end{eqnarray}
By taking the generalized inverse of Hankel matrix,  \eqref{eq:B} can be converted
to the framelet basis representation  \cite{ye2017deep}.
Moreover, one of the
most important observations in  \cite{ye2017deep}
is  that  the resulting framelet basis representation  can be equivalently represented by
single layer encoder-decoder convolution architecture:
\begin{eqnarray} \label{eq:decomp0}
\Rbc[\widehat \zb ]= \left(\widetilde\Phib \Cb\right) \circledast g(\tilde \Psib),  ~\mbox{where}~
 \Cb =  \Phib^\top \left(  \Rbc[\widehat \zb ]\circledast  h(\Psib)\right) 
\label{eq:finsuf}
\end{eqnarray}
and  
 $\circledast$ denotes the multi-channel input multi-channel output convolution.
The second and the first part of \eqref{eq:finsuf} correspond to the encoder and decoder layers with the corresponding
 convolution filters  
$h(\Psib) \in\RR^{2d \times Q}$ and $g\left(\widetilde{\Psib}^{(\jmath)}\right)\in\RR^{d Q\times 2}$:
\begin{align*}
&h(\Psib):=
\begin{pmatrix}
\overline\psib^{1}_{1} & \cdots & \overline\psib^{1}_{Q}
\\
\overline\psib^{2}_{1} & \cdots & \overline\psib^{2}_{Q}
\end{pmatrix}
\ , ~~
g\left(\widetilde{\Psib}\right):=
\begin{pmatrix}
\widetilde{\psib}^{1}_{1} & \widetilde{\psib}^{2}_{1}
\\
\vdots & \vdots
\\
\widetilde{\psib}^{1}_{Q} &  \widetilde{\psib}^{2}_{Q}
\end{pmatrix},
\end{align*}
which are obtained by reordering the matrices $\Psib$ and $\widetilde\Psib$ in \eqref{eq:Psi}.
Specifically,  $\overline \psib_i^{1}\in \Rd^{d}$ (resp. $\overline \psib_i^{2}\in \Rd^d$) denotes the $d$-tap
encoder convolutional filter applied to the  real (resp. imaginary) component of the $k$-space data to 
generate the $i$-th channel output.
In addition, $g(\tilde\Psib)$ is a reordered version of $\tilde\Psib$ so that 
 $\tilde \psib_i^{1}\in \Rd^d$ (resp. $\tilde \psib_i^{2}\in \Rd^d$) corresponds to the $d$-tap
decoder convolutional filter to generate the  real (resp. imaginary) component of the  $k$-space data by
convolving with  the $i$-th channel input.
We can further use recursive application of
encoder-decoder representation for the resulting framelet coefficients $\Cb$ in \eqref{eq:decomp0}.
In Corollary 4 of our companion paper \cite{ye2019cnn}, we showed that 
the recursive application of the encoder-decoder
operations across the layers
increases the net length of the convolutional filters. 




 




Since
\eqref{eq:finsuf} is a general form of the signals that are associated with a Hankel structured matrix,
 we are interested in using it to  estimate bases for $k$-space interpolation.
 Specifically, we consider a complex-valued signal space $\Hbc$ determined by the filters $\Psib$ and $\tilde \Psib$: \begin{eqnarray}\label{eq:H0}
\Hbc{(\Psib,\tilde\Psib)} &=& \left\{  \zb  \in \Cd^{N} \,\Big|\,\  \Rbc[ \zb ]=  \Phib^\top \left(\Cb \circledast g(\tilde \Psib)\right), \right. \notag \\
&& \left.  \Cb = (\tilde\Phib  \Rbc[ \zb]) \circledast   h(\Psib) \right\} \  . \end{eqnarray}
Then, the ALOHA formulation $P_A$ can be equivalently represented by
\begin{eqnarray}
(P_A') & \min\limits_{ \widehat\zb  \in \Hbc(\Psib,\tilde\Psib)}\min\limits_{ \Psib,\tilde\Psib}  &\left\|x- \Fc^{-1}[ \widehat\zb ]\right\|^2 \notag  \\
&\mbox{subject to } &
 \Pc_\Lambda \left[\widehat\xb\right]= \Pc_\Lambda \left[\widehat \zb \right],\quad  \notag
\end{eqnarray}
In other words, ALOHA is  to find the optimal filter $\Psib,\tilde\Psib$ and the associated $k$-space
signal $\widehat\zb \in \Hbc(\Psib,\tilde\Psib)$  that satisfies the data consistency conditions.
In contrast to the standard CS approaches in which signal presentation in the image domain is separately applied
from the data-consistency constraint in  the $k$-space,
 the success of ALOHA over CS can be contributed to more efficient signal representation in the $k$-space domain
 that simultaneously take care of the data consistency in the $k$-space domain.

\subsection{Generalization and Depth}
 



To allow  training for neural networks, the problem formulation
in $(P_A')$  should be decoupled into two steps: the learning phase to estimate $\Psib,\tilde\Psib$
from the training data,  and the inference phase to estimate the
interpolate signal $\widehat \zb$ for the given filter set $\Psib,\tilde\Psib$.
Although we have revealed the relations between ALOHA  and encoder-decoder architecture, the derivation is for specific input signal and it is not clear how 
 the relations would translate when training is performed over multiple training data set,
and the trained network can be  generalized to the unseen test data.
Given that
the  sparsity prior in dictionary learning enables the selection of appropriate basis functions from the dictionary for each given
input, one may conjecture that
there should be similar mechanisms in deep neural networks that enable adaptation  to the specific input signals.


\begin{figure}[!h] 	
\centering
{\includegraphics[width=0.7\linewidth]{fig/partition}}
\caption{An example of $\Rd^2$ input space partitioning for the case of two-channel three-layer ReLU neural network.
Depending on input $k$-space data, a partition and its associated linear representation are selected. }\label{fig:input}
\end{figure}



In Section IV of Supplementary Material,  we show that the ReLU nonlinearities indeed plays a critical role
in  the adaptation and generalization.
In fact, 
in our companion paper \cite{ye2019cnn},  
we have shown that ReLU offers  combinatorial convolution frame basis selection depending on each input image.
More specifically,
 thanks to ReLU, a trained filter set produce  large number of 
  partitions in the input space as shown in Fig.~\ref{fig:input},
 in which each region shares the same linear signal representation. Therefore, depending on each $k$-space  input data,  a particular region and its associated linear representation are selected.
Moreover,  we show that
the number of input space partition and the
associated linear representation increases exponentially with the depth, channel and the skipped connection. 
By synergistically exploiting the efficient signal representation  in the $k$-space domain,
this enormous expressivity from the same filter sets can make the $k$-space deep neural
network more powerful than the conventional image domain learning.



For the more details on the theoretical aspect of  deep neural networks, see  Section IV of Supplementary Material
or our companion paper \cite{ye2019cnn}.






\begin{figure}[!t] 	
\centerline{\includegraphics[width=1\linewidth]{fig/architecture_skip}}
\vspace*{-0.7cm}
\centerline{\mbox{(a)}}
\vspace*{0.2cm}
\centerline{\includegraphics[width=1\linewidth]{fig/architecture_skip_wgt}}
\vspace*{-0.7cm}
\centerline{\mbox{(b)}}
\caption{Overall reconstruction flows of the proposed method with (a) skipped connection, and (b) skipped connection and weighting layer. IFT denotes the inverse Fourier transform. For the parallel imaging, the input and output are multi-coil $k$-space data, after stacking them along
the channel direction. }
\label{fig:architecture}
\end{figure}

\subsection{Extension to parallel imaging}

In \cite{jin2016general}, we formally showed that when
$\{\hat \xb_i\}_{i=1}^P$ denote the $k$-space measurements from $P$ receiver coils,
the following extended Hankel structured matrix is low-ranked:
\begin{eqnarray}\label{eq:PHankel}
\hank_{d|P}(\widehat\Xb) = \begin{bmatrix} \hank_d(\widehat\xb_1) & \cdots & \hank_d(\widehat\xb_P) \end{bmatrix}
\end{eqnarray}
where $$\widehat\Xb=\begin{bmatrix} \widehat\xb_1 & \cdots & \widehat\xb_P \end{bmatrix} \in \Cd^{N\times P} \ .$$
Thus, similar to the single channel cases, the date-driven decomposition of the extended Hankel 
matrix in \eqref{eq:PHankel} can be represented by stacking the each $k$-space data along the channel
direction and applies the deep neural network for the given multi-channel data.
Therefore, except the number of input and output channels,  the network
structure for parallel imaging data is  identical to the single channel $k$-space learning. 






\subsection{Sparsification}


To further improve the performance of the structured matrix completion approach, in \cite{ye2016compressive}, we showed that 
even if the image $x(\rb)$ may not be sparse,   it can be often converted to a sparse signal.


 For example, the outmost skipped connection for the residual learning is another way to make the signal sparse.
Note that  fully sampled $k$-space data $\hat \xb$ can be represented by
$$\widehat \xb = \widehat\yb + \Delta \widehat\xb,$$
where $\widehat\yb$ is the undersampled $k$-space measurement in \eqref{eq:fwd},
and $\Delta\widehat \xb$ is the residual part of $k$-space data that should be estimated.
In practice, some of the low-frequency part of $k$-space data including the DC component
are acquired in the undersampled measurement so that the image component from the residual
$k$-space data $\Delta\widehat\xb$ are mostly high frequency signals, which are sparse. Therefore,
$\Delta\widehat\xb$ has low-rank Hankel matrix structure, which can be effectively processed using the deep neural network.
This can be easily implemented using a skipped connection  before the deep neural network as shown in Fig.~\ref{fig:architecture}(a).
However, the skipped connection also works beyond the sparsification.  
In our companion paper \cite{ye2019cnn} (which is also repeated in Section IV of Supplementary Material), we showed that
the skipped connection at the inner layers makes the  frame basis more expressive.
Therefore,  we conjecture that the skipped connections play dual roles in our $k$-space learning.

Second, we can  convert a signal to an innovation signal  using a 
 shift-invariant transform represented by the whitening filter $h$ such that
 the resulting innovation signal
  $z = h \ast x$
 becomes an FRI signal \cite{vetterli2002sampling}.  
  For example,  many MR images can be sparsified using finite difference or wavelet transform
 \cite{jin2016general}.  
 This implies that the Hankel matrix from the weighted $k$-space data,
 $\hat z (\kb) = \hat h(\kb)\hat x (\kb)$ are low-ranked, 
where the weight $\hat h(\kb)$ is determined
  from the finite difference or Haar wavelet transform \cite{jin2016general,ye2016compressive}.
  Thus, the deep neural network is applied to the weighted $k$-space data to estimate
  the missing  spectral data $\hat  h(\xb)\hat x(\kb)$, after which the original $k$-space data is obtained
  by dividing with the same weight, i.e. $\hat x (\kb) =  \hat z(\kb)/\hat h(\kb)$.
 This can be easily implemented using a weighting and unweighting layer as shown in Fig.~\ref{fig:architecture}(b).


In this paper, we consider these
two strategies to investigate which strategy is better for different sampling trajectories.

\subsection{Extension to General Sampling Patterns}

Since the Hankel matrix formulation in ALOHA  is based on the Cartesian coordinate, we add extra regridding layers to handle the non-Cartesian sampling trajectories. 
Specifically, for radial and spiral trajectories, the non-uniform fast Fourier transform (NUFFT) was used to perform the regridding to the Cartesian coordinate. 
For Cartesian sampling trajectory, the regridding layer using NUFFT is  not necessary, and
we instead perform a zero-filling in the unacquired $k$-space regions as an initialization step.



\section{Implementation}





\begin{figure}[!t] 	
\centering
{\includegraphics[width=1\linewidth]{fig/backbone}}
\caption{A network backbone of the proposed method. The input and output are complex-valued.}
\label{fig:backbone}
\end{figure}

\subsection{Network Backbone}

The network backbone  follows the  U-Net\cite{ronneberger2015u} which consists of convolution, batch normalization, rectified linear unit (ReLU), and contracting path connection with concatenation as shown in Fig. \ref{fig:backbone}.
Here, the input and output are the complex-valued $k$-space data, while $\Rbc[\cdot]$ and $\Rbc^{-1}[\cdot]$ denote the operators 
in \eqref{eq:Rbc} and \eqref{eq:Za}, respectively, that convert complex valued input to two-channel real value signals and vice versa.
For parallel imaging, 
multi-coil $k$-space data are given as input and output after stacking them along channel direction. Specially,  in our parallel
imaging experiments, we use eight coils $k$-space data.


The yellow arrow  is the basic operator that consists of $3 \times 3$ convolutions followed by a rectified linear unit (ReLU) and batch normalization.
The same operation exists  between the separate blocks at every stage, but
the yellow arrows are omitted for visibility.
A red and blue arrows are $2 \times 2$ average pooling and average unpooling operators, respectively, located between the stages.
A violet arrow is the skip and concatenation operator. A green arrow is the simple $1 \times 1$ convolution operator generating interpolated $k$-space data from multichannel data.





\subsection{Network Training}

We use the $l_2$ loss  in the image domain  for training. For this, 
the Fourier transform operator is placed as the last layer to convert the interpolated $k$-space data to
the complex-valued image domain so that  the loss values are calculated for the reconstructed image.
Stochastic gradient descent (SGD) optimizer was used to train the network. For the IFT layer, the adjoint operation from SGD is also Fourier transform.
The size of mini batch is 4, and the number of epochs in single and multi coil networks is 1000 and 500, respectively. The initial learning rate is $10^{-5}$, which gradually dropped to $10^{-6}$ until 300-th epochs. The regularization parameter was $\lambda = 10^{-4}$. 


The labels for the network were the images generated from direct Fourier inversion from
fully sampled $k$-space data. The input data for the network was the regridded down-sampled $k$-space data from Cartesian, radial, and spiral trajectories.
The details of the downsampling procedure will be discussed later.
 For each trajectory, we train the network separately. 



The proposed network was implemented using MatConvNet toolbox in MATLAB R2015a environment \cite{vedaldi2015matconvnet}. 
Processing units used in this research are Intel Core i7-7700 central processing unit and GTX 1080-Ti graphics processing unit. Training time lasted about 5 days.
 


\section{Material and Methods}

\subsection{Data Acquisition}

The evaluations were performed on single coil and multi coils $k$-space data for various $k$-space trajectories
 such as Cartesian, radial, and spiral cases. 

For the Cartesian trajectory, knee $k$-space dataset (http://mridata.org/) were used. The raw data were acquired from 3D fast-spin-echo (FSE) sequence with proton density weighting included fat saturation comparison by a 3.0T whole body MR system (Discovery MR 750, DV22.0, GE Healthcare, Milwaukee, WI, USA). The repetition time (TR) and echo time (TE) were 1550 ms and 25 ms, respectively. There were 320 slices in total, and the thickness of each slice was 0.6 mm. The field of view (FOV) defined $160 \times 128$ mm$^2$ and the size of acquisition matrix is $320 \times 256$. The voxel size was 0.5 mm. The number of coils is 8. Eight coils $k$-space data was used for multi-coil $k$-space deep learning. 
In addition, to evaluate the performance of the algorithm  for the single coil experiment,  coil compression (http://mrsrl.stanford.edu/\~tao/software.html) was applied to obtain a single coil $k$-space data. 
For the Cartesian trajectory as shown in Fig. \ref{fig:trajectories}(a), the input $k$-space was downsampled to a Gaussian pattern using x4 acceleration factor in addition to the 10$\%$ auto-calibration signal (ACS) line. Therefore, the net acceleration factor is about 3 ($R=3$). 
Among the 20  cases of knee data,  18 cases were used for training, 1 case for validation, and the other for test.


\begin{figure}[!t] 	
\centering
{\includegraphics[width=0.9\linewidth]{fig/trajectories}}
\caption{Various under-sampling patterns: (a) Cartesian undersampling at $R=3$, (b) radial undersampling at $R=6$, and (c) spiral undersampling at $R=4$.
Magnified views are provided for radial and spiral trajectories. }
\label{fig:trajectories}
\end{figure}



For radial and spiral sampling patterns, a synthesized $k$-space data from Human Connectome Project (HCP) MR dataset (https://db.humanconnectome.org) were used.
Specifically, the multi-coil radial and spiral $k$-space data are generated using MRI simulator (http://bigwww.epfl.ch/algorithms/mri-reconstruction/).
The T2 weighted brain images contained within the HCP were acquired Siemens 3T MR system using a 3D spin-echo sequence. The TR and TE were 3200 ms and 565 ms, respectively. The number of coils was 8, but the final reconstruction was obtained as the absolute of the sum. 
The FOV was $224 \times 224 ~\rm{mm}^2$, and the size of acquisition matrix was $320 \times 320$. The voxel size was 0.7 mm. 
The total of 199 subject datasets was used in this paper. Among the 199 subject, 180 were used for network training,  10 subject  for validation, and the other subject  for test.
Fig. \ref{fig:trajectories}(b) shows the down-sampled $k$-space radial sampling patterns. 
The  downsampled radial $k$-space consists of only 83 spokes, which corresponds to 
$R=6$ acceleration factor compared to the 503 spokes for the fully sampled data that were used as the ground-truth.
On the other hand,  Fig. \ref{fig:trajectories}(c) shows the down-sampled spiral sampling pattern, composed of
4 interleaves that corresponds to $R=4$ acceleration compared to the
he full spiral trajectory with 16 interleaves. The spiral $k$-space trajectory was obtained with a variable density factor (VDF) of 2.5. 


\begin{figure}[!b] 	
\centerline{\includegraphics[width=1\linewidth]{fig/objective}}
\caption{Objective functions of a single coil for (a) Cartesian, (b) radial, and (c) spiral trajectories. Dashed and solid lines indicate an objective function of the train and validation phase, respectively.}
\label{fig:objective}
\end{figure}


\subsection{Performance Evaluation}

For quantitative evaluation, the normalized mean square error (NMSE) value was used, which is defined as
\begin{eqnarray}
	NMSE = \frac{\sum_{i=1}^{M} \sum_{j=1}^{N} [x^*(i,j) - {x}(i, j)]^2}{\sum_{i=1}^{M}\sum_{j=1}^{N}[x^*(i,j)]^2},
\end{eqnarray}
where $x$ and $x^*$ denote the reconstructed images and ground truth, respectively. $M$ and $N$ are the number of pixel for row and column.
We also use the peak signal to noise ratio (PSNR), which is defined by
\begin{eqnarray}
	PSNR &=& 20 \cdot \log_{10} \left(\frac{NM\|x^*\|_\infty}{\| x- x^*\|_2}\right) \  .
\label{eq:psnr}		 
\end{eqnarray}
We also use the structural similarity (SSIM) index  \cite{wang2004image}, defined as
\begin{equation}
	SSIM = \dfrac{(2\mu_{x}\mu_{x^*}+c_1)(2\sigma_{x x^*}+c_2)}{(\mu_{x}^2+\mu_{x^*}^2+c_1)(\sigma_{x}^2+\sigma_{x^*}^2+c_2)},
\end{equation}
where $\mu_{x}$ is a average of $x$, $\sigma_{x}^2$ is a variance of $x$ and $\sigma_{x x^*}$ is a covariance of $x$ and $x^*$. 
There are two variables to stabilize the division such as $c_1=(k_1L)^2$ and $c_2=(k_2L)^2$.
$L$ is a dynamic range of the pixel intensities. $k_1$ and $k_2$ are constants by default $k_1=0.01$ and $k_2=0.03$. 

For extensive comparative study, we also compared with the following algorithms:
 total variation (TV) penalized CS,  ALOHA \cite{jin2016general}, and four types of CNN models including the
 variational model \cite{hammernik2018learning}, a cascade model \cite{schlemper2018deep}, the cross-domain model called KIKI network  \cite{eo2018kiki}, and an image-domain model \cite{han2017deep}. In particular,  \cite{han2017deep}  is a representative
 example of  Fig. \ref{fig:deepMR}(a).
Specifically,  the image domain residual learning using the standard U-Net backbone in Fig.~\ref{fig:backbone} was used. 
Unlike the proposed network, the input and output are an artifact corrupted image and artifact-only image, respectively \cite{jin2017deep}.
In addition,  the variational model \cite{hammernik2018learning} and the cascade model \cite{schlemper2018deep}  represent Fig. \ref{fig:deepMR}(b). The cross-domain model is formed by linking the $k$-space model in Fig. \ref{fig:deepMR}(d) and the image-domain model in Fig. \ref{fig:deepMR}(a).  Unfortunately, Fig. \ref{fig:deepMR}(c) does not scale well due to the enormous memory requirement, so cannot be used in the comparative study. 
 For fair comparison, the cascade \cite{schlemper2018deep} and cross-domain \cite{eo2018kiki} networks were modified for
 parallel imaging.   All the neural networks were trained using exactly the same data set.
 For  ALOHA \cite{jin2016general} and the proposed method in Fig. \ref{fig:architecture}(b),
 the  total variation based $k$-space weighting was used.




\begin{figure}[!ht] 	
\centerline{\includegraphics[width=0.9\linewidth]{fig/result_cartesian_axial_single}}
\caption{Reconstruction results from Cartesian trajectory at $R=3$ in single coil.
The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.}
\label{fig:result_cartesian_axial_single}
\end{figure}

 \begin{figure}[!hbt] 	
\centerline
{\includegraphics[width=0.9\linewidth]{fig/result_cartesian_single}}
\caption{(i) Coronal and (ii) sagittal reformated reconstruction results from Cartesian trajectory at $R=3$ in single coil. The results from first to last rows indicate ground-truth, downsampled, total variation, image-domain learning and the proposed method. Yellow and red boxes illustrate the enlarged and difference views, respectively. The difference images were amplified five times. The number written to the images is the NMSE value. }
\label{fig:result_cartesian_single}
\end{figure}



 \section{Results}\label{sec:result}
 

To evaluate the performance of sparsifications  in the single coil, the proposed method was trained using the sparsifications as shown in as Figs. \ref{fig:architecture}(a)(b). Fig. \ref{fig:objective} shows the objective functions along the trajectories such as (a) Cartesian, (b) radial, and (c) spiral trajectory. In the Cartesian trajectory, the proposed network in Fig. \ref{fig:architecture}(b) produces the lowest curve in the validation phase (see red curves in Fig. \ref{fig:objective}(a)). The proposed network in Fig \ref{fig:architecture}(a) shows the best convergence during
the training, but the generalization at the test phase was not good (see green curves in Fig. \ref{fig:objective}(a)). In the non-Cartesian trajectories, the best convergence appears using the proposed network with only skipped connection in Fig. \ref{fig:architecture}(a) (see green curves in Fig. \ref{fig:objective}(b)(c)). 
Based on these convergence behaviour and generalization, the proposed network was trained with different sparsification schemes. The network in Fig. \ref{fig:architecture}(b) was trained for the Cartesian trajectory and the network in Fig. \ref{fig:architecture}(a) was used for the non-Cartesian trajectories such as radial and spiral trajectories.


\begin{table}[!t]
\centering
\resizebox{0.48\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c}
		\hline
			\ \multirow{2}{*}{Metric}	& Input  			& \multirow{2}{*}{Total variation}	& Image-domain 	& Ours \\ 
			\ 							& ( 1 coil, $\times 3$ )	& 		& learning		& ( Fig. \ref{fig:architecture}(b) )	\\ \hline\hline
			\ PSNR [dB]					& 33.9058			& 35.7112			& 35.5419		& \textbf{35.9586}	\\
			\ NMSE ($\times 10^{-2}$)	& 2.7779			& 1.9067			& 1.9567		& \textbf{1.7921}	\\			
			\ SSIM						& 0.7338 			& 0.7548			& 0.7447		& \textbf{0.7664}	\\ \hline
			\ Times (sec/slice)			& - 				& 0.1344			& 0.0272		& 0.0377			\\
		\hline
	\end{tabular}
		}
\caption{Quantitative comparison from Cartesian trajectory at $R=3$ in single coil. }
\label{tbl:result_cartesian_single}
\end{table}

\begin{figure*}[!t] 	
\centerline{\includegraphics[width=0.9\linewidth]{fig/result_cartesian_axial_multi}}
\centerline{\mbox{(a)}}
\vspace*{0.2cm}
\centerline{\includegraphics[width=0.9\linewidth]{fig/result_cartesian_multi}}
\centerline{\mbox{(b)}}
\caption{Reconstruction results from Cartesian trajectory at $R=3$ in multi coils: (a) axial, (b-i) coronal and (b-ii) sagittal reconstruction results. Yellow and red boxes illustrate the enlarged and difference views, respectively. The difference images were amplified five times. The number written to the images is the NMSE value.}
\label{fig:result_cartesian_multi}
\end{figure*}


\begin{table*}[!t]
\centering
\resizebox{0.95\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c|c|c|c|c}
		\hline
			\ \multirow{2}{*}{Metric}	& Input	& \multirow{2}{*}{Total variation}	& ALOHA		& Variational Net.	& Cascade Net. 	& KIKI Net. & Image-domain 	& Ours \\ 
			\ 							& ( 8 coils, $\times 3$ )	& 	& \cite{jin2016general}	& \cite{hammernik2018learning}	& \cite{schlemper2018deep} & \cite{eo2018kiki}	& learning		& ( Fig. \ref{fig:architecture}(b) )	\\ \hline\hline
			\ PSNR [dB]					& 34.2021	& 36.0689	& 36.1013	& 36.1428	& 36.6055	& 36.6847	& 35.8497	& \textbf{36.9931}	\\
			\ NMSE ($\times 10^{-2}$)	& 2.3384	& 1.5826	& 1.5812	& 1.5379	& 1.4196	& 1.4086	& 1.6497	& \textbf{1.3154}	\\			
			\ SSIM						& 0.7710 	& 0.7878	& 0.7914	& 0.7966	& 0.7982	& 0.8018	& 0.7776	& \textbf{0.8087}	\\ \hline
			\ Times (sec/slice)			& - 		& 1.0938	& 16.5554	& 0.1688	& 0.2219	& 0.5438	& 0.1188	& 0.1438			\\
		\hline
	\end{tabular}
		}
\caption{Quantitative comparison from Cartesian trajectory at $R=3$ in 8 coils parallel imaging. }
\label{tbl:result_cartesian_multi}
\end{table*}

 Fig. \ref{fig:result_cartesian_axial_single} shows the results of single coil reconstruction from Cartesian trajectory using the architecture with skipped connection and weighting layer as shown in Fig.~\ref{fig:architecture}(a).
 While all the algorithms provide good reconstruction results, the proposed method most accurately
recovered  high frequency edges and textures 
as shown in the enlarged images and difference images of Fig.~\ref{fig:result_cartesian_axial_single}.
Fig. \ref{fig:result_cartesian_single} shows the reformed images along the (i) coronal and (ii) sagittal directions. Again, the reformatted coronal and sagittal images by the proposed method
preserved the most detailed structures of underlying images without any artifact along the slice direction. 
 The quantitative comparison in Table~\ref{tbl:result_cartesian_single} in terms of   average PSNR, NMSE, and SSIM values also confirmed that
the proposed $k$-space interpolation method produced the best quantitative values in all area. The computation time of the
proposed method is slightly slower than the image-domain learning because of the weighting and Fourier transform operations,
but it is still about 3.5 times faster than the total variation penalized compressed sensing (CS) approach.

 Fig. \ref{fig:result_cartesian_multi} shows the parallel imaging results from  eight coil measurement. 
Because the ALOHA \cite{jin2016general} and the proposed method directly interpolate missing $k$-space, these methods clearly preserve textures detail structures as shown in  Fig. \ref{fig:result_cartesian_multi}. However, ALOHA \cite{jin2016general} is more than 100 times slower than the $k$-space deep learning as shown in Table \ref{tbl:result_cartesian_multi}.  
All CNN methods except the imaging-domain model outperform than CS methods. Although the cascade model \cite{schlemper2018deep} was performed with data consistency step, the method did not completely
overcome the limitations of image-domain learning. In the cross-domain learning \cite{eo2018kiki}, they proposed to train $k$-space and image-domain sequentially. 
The cross-domain network consists of deeper layers (100 layers; 25 layers $\times$ 4 individual models) than the proposed method, 
but the performance was worse than our method. As shown Table \ref{tbl:result_cartesian_multi},
the proposed method shows best performance in terms of average PSNR, NMSE, and SSIM values.


 
\begin{figure}[!t] 	
\centering
\centerline{\includegraphics[width=0.9\linewidth]{fig/result_radial_multi}}
\caption{Reconstruction results from radial trajectory at $R=6$ in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively.  The number written to the images is the NMSE value.}
\label{fig:result_radial}
\end{figure}
 
 

Fig. \ref{fig:result_radial} shows the reconstruction images from x6 accelerated radial sampling patterns using the architecture in Fig.~\ref{fig:architecture}(a) for 8 coils parallel imaging. The results for single coil are shown in Fig. S1 in Supplementary Material. 
The proposed $k$-space deep learning provided realistic image quality and preserves the detailed structures as well as the textures, but the image domain network failed to remove the noise signals and the total variation method did not preserve the realistic textures and sophisticated structures. Our method also provides much smaller NMSE values, as shown at the bottom of each Fig. \ref{fig:result_radial}  and Fig. S1 in Supplementary Material. Average PSNR, NMSE and SSIM values are shown in Table \ref{tbl:result_radial} and Table S1 in Supplementary Material for multi coils and single coil cases, respectively. The average values were calculated across all slices and 9 subjects. The proposed $k$-space deep learning provided the best quantitative values. 


\begin{figure}[!t] 	
\centerline{\includegraphics[width=0.9\linewidth]{fig/result_spiral_multi}}
\caption{Reconstruction results from spiral trajectory at $R=4$ in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.}
\label{fig:result_spiral}
\end{figure}


\begin{table}[!t]
\centerline{
\resizebox{0.48\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c}
		\hline
			\multirow{2}{*}{Metric}	& Input  	& \multirow{2}{*}{Total variation}	& Image-domain 	& Ours 	\\ 
									& ( 8 coils, $\times 6$ )	& 			& learning		& ( Fig. \ref{fig:architecture}(a) ) 	\\ \hline\hline
			PSNR [dB]				& 30.6396	& 38.2357		& 39.9140		& \textbf{50.8136}	\\
			NMSE ($\times 10^{-2}$)	& 3.6137	& 0.5998		& 0.4224		& \textbf{0.0353}	\\ 
			SSIM					& 0.5927	& 0.9528		& 0.9020		& \textbf{0.9908}	\\ \hline
\end{tabular}
		}
}
\vspace*{0.1cm}
\caption{Quantitative comparison from radial undersampling at $R=6$ in 8 coils parallel imaging. }



\label{tbl:result_radial}
\end{table}



 Fig. \ref{fig:result_spiral} shows the reconstruction images from x4 accelerated  spiral trajectory for 8 coils parallel imaging and Fig. S2 in Supplementary Material illustrate the single coil results, respectively.
Similar to the radial sampling patterns, the proposed method provides significantly improved image reconstruction results, and the average PSNR, NMSE and SSIM values in Table \ref{tbl:result_spiral} and Table S2 in Supplementary Material also confirm that the proposed method consistently outperform other method for all patients. 


\begin{table}[!t]
\centerline{
\resizebox{0.48\textwidth}{!}{
	\begin{tabular}{c|c|c|c|c}
		\hline
			\multirow{2}{*}{Metric}	& Input  	& \multirow{2}{*}{Total variation}	& Image-domain 	& Ours 	\\ 
									& ( 8 coils, $\times 4$ )	& 			& learning		& ( Fig. \ref{fig:architecture}(a) ) 	\\ \hline\hline
			PSNR [dB]				& 30.3733	& 38.7646		& 41.0581		& \textbf{53.5643}	\\
			NMSE ($\times 10^{-2}$)	& 4.0373	& 0.6130		& 0.3404		& \textbf{0.0201}	\\ 
			SSIM					& 0.6507	& 0.9487		& 0.8881		& \textbf{0.9940}	\\ \hline
			Times (sec/slice)		& -			& 7.8818		& 0.1375		& 0.1431			\\ \hline
	\end{tabular}
		}
}
\caption{Quantitative comparison from  spiral undersampling at $R=4$ in 8 coils parallel imaging.}
\label{tbl:result_spiral}
\end{table}

To evaluate the improvements of the proposed method, a radiologist (with 9 years of experience) thoroughly reviewed the reconstructed images. For radial trajectory images, the pyramidal tract (arrows in Fig. S3(i) of the Supplementary Material), a bundle of nerve fibers conducting motor signal from the motor cortex to the brainstem or to the spinal cord, was evaluated. It was noted that high signal intensity of the pyramidal tract was exaggerated on the TV method, which could be misdiagnosed as an abnormal finding. In addition, the ability to discriminate a pair of internal cerebral veins were evaluated. While the TV and image-domain learning methods can not differentiate the two veins, the proposed method is able to show the two internal cerebral veins separately (arrows on Fig. S3(ii) in Supplementary Material). With regard to spiral trajectory images (Fig. S4 in the Supplementary Material), the TV method shows bright dot-like artifacts along several slices (Fig. S4(i)). When the small T2 hyperintensity lesions in the left frontal lobe were evaluated, the TV method fails to demonstrate the lesions. The image domain learning preserves the lesions, but the margin of the lesions is blurry in the noisy background. The small lesions are clearly depicted on the proposed method (arrows in Fig. S4(ii)). Overall, the quality of image reconstruction of the proposed method was superior to that of other methods.



\section{Discussion}

In order to improve the performance of ALOHA, the matrix pencil size should be significantly large, which is not possible in standard ALOHA formulation due to the large memory requirement and extensive computational burden. 
We believe that one of the important reasons that the proposed k-space deep learning provides better performance than ALOHA is that  the cascaded convolution results in much longer filter length. Nevertheless, 
with the same set of trained filters, our network can  be adapted to different input images due to the
combinatorial nature of ReLU nonlinearity. We believe that this contributes to the benefits of k-space learning over ALOHA.
Moreover, efficient signal representation in  $k$-space domain, which is the key idea of ALOHA,  can 
synergistically work with the expressivity of the neural network to enable better performance than image domain learning.



\section{Conclusion}\label{sec:conclusion}

Inspired by a link between the ALOHA and deep learning,
this paper showed that fully data-driven $k$-space interpolation is feasible by using $k$-space deep learning
 and the image domain loss function. The proposed $k$-space interpolation network  outperformed the
existing image domain deep learning for various sampling trajectory.
As the proposed $k$-space interpolation framework is quite effective and also supported by novel theory, 
so we believe that this opens a new area of researches for many Fourier imaging problems.






\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition}, 2016, pp. 770--778.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for
  biomedical image segmentation,'' in \emph{International Conference on Medical
  Image Computing and Computer-Assisted Intervention}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2015, pp. 234--241.

\bibitem{wang2016accelerating}
S.~Wang, Z.~Su, L.~Ying, X.~Peng, S.~Zhu, F.~Liang, D.~Feng, and D.~Liang,
  ``Accelerating magnetic resonance imaging via deep learning,'' in
  \emph{Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium
  on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 514--517.

\bibitem{hammernik2018learning}
K.~Hammernik, T.~Klatzer, E.~Kobler, M.~P. Recht, D.~K. Sodickson, T.~Pock, and
  F.~Knoll, ``Learning a variational network for reconstruction of accelerated
  {MRI} data,'' \emph{Magnetic resonance in medicine}, vol.~79, no.~6, pp.
  3055--3071, 2018.

\bibitem{lee2018deep}
D.~Lee, J.~Yoo, S.~Tak, and J.~Ye, ``Deep residual learning for accelerated
  {MRI} using magnitude and phase networks,'' \emph{IEEE Transactions on
  Biomedical Engineering}, 2018.

\bibitem{han2017deep}
Y.~S. Han, J.~Yoo, and J.~C. Ye, ``Deep learning with domain adaptation for
  accelerated projection reconstruction {MR},'' \emph{Magnetic Resonance in
  Medicine, https://doi.org/10.1002/mrm.27106}, 2018.

\bibitem{jin2017deep}
K.~H. Jin, M.~T. McCann, E.~Froustey, and M.~Unser, ``Deep convolutional neural
  network for inverse problems in imaging,'' \emph{IEEE Transactions on Image
  Processing}, vol.~26, no.~9, pp. 4509--4522, 2017.

\bibitem{schlemper2018deep}
J.~Schlemper, J.~Caballero, J.~V. Hajnal, A.~N. Price, and D.~Rueckert, ``A
  deep cascade of convolutional neural networks for dynamic mr image
  reconstruction,'' \emph{IEEE transactions on Medical Imaging}, vol.~37,
  no.~2, pp. 491--503, 2018.

\bibitem{zhu2018image}
B.~Zhu, J.~Z. Liu, S.~F. Cauley, B.~R. Rosen, and M.~S. Rosen, ``Image
  reconstruction by domain-transform manifold learning,'' \emph{Nature}, vol.
  555, no. 7697, p. 487, 2018.

\bibitem{lustig2007sparse}
M.~Lustig, D.~Donoho, and J.~Pauly, ``Sparse {MRI}: The application of
  compressed sensing for rapid mr imaging,'' \emph{Magnetic Resonance in
  Medicine}, vol.~58, no.~6, pp. 1182--1195, 2007.

\bibitem{jung2009k}
H.~Jung, K.~Sung, K.~Nayak, E.~Kim, and J.~Ye, ``k-t {FOCUSS}: A general
  compressed sensing framework for high resolution dynamic {MRI},''
  \emph{Magnetic Resonance in Medicine}, vol.~61, no.~1, pp. 103--116, 2009.

\bibitem{shin2014calibrationless}
P.~J. Shin, P.~E. Larson, M.~A. Ohliger, M.~Elad, J.~M. Pauly, D.~B. Vigneron,
  and M.~Lustig, ``Calibrationless parallel imaging reconstruction based on
  structured low-rank matrix completion,'' \emph{Magnetic resonance in
  medicine}, vol.~72, no.~4, pp. 959--970, 2014.

\bibitem{haldar2014low}
J.~P. Haldar, ``Low-rank modeling of local $ k $-space neighborhoods (loraks)
  for constrained mri,'' \emph{IEEE transactions on medical imaging}, vol.~33,
  no.~3, pp. 668--681, 2014.

\bibitem{jin2016general}
K.~H. Jin, D.~Lee, and J.~C. Ye, ``A general framework for compressed sensing
  and parallel {MRI} using annihilating filter based low-rank {H}ankel
  matrix,'' \emph{IEEE Transactions on Computational Imaging}, vol.~2, no.~4,
  pp. 480--495, 2016.

\bibitem{ongie2016off}
G.~Ongie and M.~Jacob, ``Off-the-grid recovery of piecewise constant images
  from few {F}ourier samples,'' \emph{SIAM Journal on Imaging Sciences},
  vol.~9, no.~3, pp. 1004--1041, 2016.

\bibitem{aggarwal2019modl}
H.~K. Aggarwal, M.~P. Mani, and M.~Jacob, ``{MoDL}: Model-based deep learning
  architecture for inverse problems,'' \emph{IEEE transactions on medical
  imaging}, vol.~38, no.~2, pp. 394--405, 2019.

\bibitem{ye2016compressive}
J.~C. Ye, J.~M. Kim, K.~H. Jin, and K.~Lee, ``Compressive sampling using
  annihilating filter-based low-rank interpolation,'' \emph{IEEE Transactions
  on Information Theory}, vol.~63, no.~2, pp. 777--801, Feb. 2017.

\bibitem{ye2017deep}
J.~C. Ye, Y.~Han, and E.~Cha, ``Deep convolutional framelets: A general deep
  learning framework for inverse problems,'' \emph{SIAM Journal on Imaging
  Sciences}, vol.~11, no.~2, pp. 991--1048, 2018.

\bibitem{akccakaya2019scan}
M.~Ak{\c{c}}akaya, S.~Moeller, S.~Weing{\"a}rtner, and K.~U{\u{g}}urbil,
  ``Scan-specific robust artificial-neural-networks for k-space interpolation
  {(RAKI)} reconstruction: Database-free deep learning for fast imaging,''
  \emph{Magnetic resonance in medicine}, vol.~81, no.~1, pp. 439--453, 2019.

\bibitem{pramanik2018off}
A.~Pramanik, H.~K. Aggarwal, and M.~Jacob, ``Off-the-grid model based deep
  learning {(O-MODL)},'' \emph{arXiv preprint arXiv:1812.10747}, 2018.

\bibitem{aggarwal2018multi}
H.~K. Aggarwal, M.~P. Mani, and M.~Jacob, ``Multi-shot sensitivity-encoded
  diffusion {MRI} using model-based deep learning {(MODL-MUSSELS)},''
  \emph{arXiv preprint arXiv:1812.08115}, 2018.

\bibitem{vetterli2002sampling}
M.~Vetterli, P.~Marziliano, and T.~Blu, ``Sampling signals with finite rate of
  innovation,'' \emph{IEEE Transactions on Signal Processing}, vol.~50, no.~6,
  pp. 1417--1428, 2002.

\bibitem{candes2009exact}
E.~J. Cand{\`e}s and B.~Recht, ``Exact matrix completion via convex
  optimization,'' \emph{Foundations of Computational mathematics}, vol.~9,
  no.~6, p. 717, 2009.

\bibitem{ongie2017fast}
G.~Ongie and M.~Jacob, ``A fast algorithm for convolutional structured low-rank
  matrix recovery,'' \emph{IEEE Transactions on Computational Imaging}, vol.~3,
  no.~4, pp. 535--550, 2017.

\bibitem{ye2019cnn}
J.~C. Ye and W.~K. Sung, ``Understanding geometry of encoder-decoder {CNNs},''
  \emph{Proceedings of the 2019 International Conference on International
  Conference on Machine Learning (ICML). also available as arXiv preprint
  arXiv:1901.07647}, 2019.

\bibitem{vedaldi2015matconvnet}
A.~Vedaldi and K.~Lenc, ``Matconvnet: Convolutional neural networks for
  {M}atlab,'' in \emph{Proceedings of the 23rd ACM international conference on
  Multimedia}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2015, pp. 689--692.

\bibitem{wang2004image}
Z.~Wang, A.~C. Bovik, H.~R. Sheikh, and E.~P. Simoncelli, ``Image quality
  assessment: from error visibility to structural similarity,'' \emph{IEEE
  transactions on image processing}, vol.~13, no.~4, pp. 600--612, 2004.

\bibitem{eo2018kiki}
T.~Eo, Y.~Jun, T.~Kim, J.~Jang, H.-J. Lee, and D.~Hwang, ``Kiki-net:
  cross-domain convolutional neural networks for reconstructing undersampled
  magnetic resonance images,'' \emph{Magnetic resonance in medicine}, 2018.

\end{thebibliography}


\pagebreak
\subfile{supplement_final_arxiv.tex}
\end{document}
