\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}                \usepackage[nolist]{acronym}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyphenat}
\usepackage{pifont}
\usepackage{listings}
\usepackage{array}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{color}
\newcommand{\change}[1]{{\color{red}#1}}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


\begin{document}
\title{DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis}
\titlerunning{DIVA-DAF}

\begin{comment}
    \author{Anonymous Author\inst{1} \and
    Anonymous Author\inst{1} \and
    Anonymous Author\inst{1}}
    
    \authorrunning{Author et al.}
    
    \institute{\textit{Anonymous Group (AG)} \\
    Anonymous University, Anonymousstreet 1A, Anonymous City, Anonymous Country, \\
    \email{\{firstname.lastname\}@AnonymousUniversity.abc}}
\end{comment}

\author{Lars~V\"ogtlin \and
    Paul~Maergner \and
    Rolf~Ingold}
    
    \authorrunning{V\"ogtlin et al.}

    \institute{\textit{Document Image and Voice Analysis Group (DIVA)} \\
    University of Fribourg, Switzerland \\
    \email{\{firstname.lastname\}@unifr.ch}}


\maketitle              

\begin{acronym}[UMLX]
 \acro{HDIA}{Historical Document Image Analysis}
 \acro{DIA}{Document Image Analysis}
 \acro{ML}{Machine Learning}
 \acro{CUDA}{NVIDIA Compute Unified Device Architecture}
 \acro{CuDNN}{Deep Neural Network library}
 \acro{CNN}{Convolutional Neural Network}
 \acro{CLI}{Command Line Interface}
 \acro{DL}{Deep Learning}
 \acro{CV}{Computer Vision}
 \acro{NLP}{Natural Language Processing}
 \acro{PR}{Pattern Recognition}
 \acro{mIoU}{mean Intersection over Union}
\end{acronym} 
\begin{abstract}
In this paper, we introduce a new deep learning framework called DIVA-DAF. We have developed this framework to support our research on historical document image analysis tasks and to develop techniques to reduce the need for manually-labeled ground truth. We want to apply self-supervised learning techniques and use different kinds of training data. Our new framework aids us in performing rapid prototyping and reproducible experiments. We present a first semantic segmentation experiment on DIVA-HisDB using our framework, achieving state-of-the-art results. The DIVA-DAF framework is open-source, and we encourage other research groups to use it for their experiments.




\keywords{Framework, Open-Source \and Deep Learning \and Neural Networks \and Reproducible Research \and Machine Learning \and Historical Document Image Analysis.}
\end{abstract}     
\section{Introduction}
Deep learning (DL) methods have played a significant role in the last decade in increasing the performance of Computer Vision (CV) tasks and, as such, also historical document image analysis (HDIA) tasks. The drawback of common DL approaches is their enormous hunger for annotated data. This hunger is particularly problematic in HDIA since most tasks, like semantic labeling, require experts to label the document images correctly. For example, historical handwritten documents often contain comments surrounding or intersecting the main text. Distinguishing between these two classes is crucial for analyzing the document. However, this distinction often requires expert knowledge. Fig.~\ref{fig:hisdoc_problems} shows a few examples where the distinction between comment and the main text is difficult.

In our research, we want to reduce the requirement for manually-labeled ground truth for HDIA with the help of different techniques, like applying self-supervised learning~\cite{jingSelfsupervisedVisualFeature2020,cosmaSelfsupervisedRepresentationLearning2020} and synthesizing data~\cite{vogtlinGeneratingSyntheticHandwritten2021a}. We have created a new framework called DIVA-DAF that aids us with this research. Our framework allows us to perform rapid prototyping and reproducible experiments focusing on HDIA tasks. The framework is open-source~\footnote{\url{https://github.com/DIVA-DIA/DIVA-DAF}}, and we encourage other research groups to use it for their experiments. This paper introduces our framework to the research community and shows some initial experiments.







\begin{figure}[h!]
    \centering
\subfloat[CSG18, p.107\label{fig:csg18_107}]{
      \includegraphics[width=\textwidth]{figures/his_problem/csg18_107_cropped.jpg}
    }
    \vfill
    \subfloat[CSG863, p.131\label{fig:csg863_131}]{
      \includegraphics[width=\textwidth]{figures/his_problem/csg0863_131_max.jpg}
    }
    \caption{
    Two sample images of the DIVA-HisDB~\cite{simistiraDIVAHisDBPreciselyAnnotated2016} dataset: Each image (a, b) shows the problem of distinguishing comments and main text. Image (a) is a challenging document with interlinear glosses and letters of different sizes and colors. For a non-expert, it is hard to say if the curved line in image (b) is a comment or part of the main text.
}
    \label{fig:hisdoc_problems}
\end{figure}


A variety of general strategies exist to reduce the need for manually-labeled data. Four common strategies are transfer learning, data augmentation, synthesizing data, and unsupervised learning.
\begin{itemize}
    \item Transfer learning trains a deep neural network on a similar dataset, which provides more labeled data and then fine-tunes it on the actual small labeled target dataset~\cite{caruanaMultitaskLearning1997}.
    \item Data augmentation can be performed, e.g., by degrading real documents so that the existing ground truth can still be used~\cite{bairdDocumentImageDefect1992,kieuCharacterDegradationModel2012,seuretGradientdomainDegradationsImproving2015}.
    \item Synthesizing data is creating new documents based on existing real documents~\cite{capobiancoDocEmulToolkitGenerate2017,journetDocCreatorNewSoftware2017,vogtlinGeneratingSyntheticHandwritten2021a} or creating new documents from scratch.
    \item Unsupervised learning strategies are, e.g., autoencoders~\cite{masciStackedConvolutionalAutoEncoders2011,albertiPitfallUnsupervisedPreTraining2017a}. Another unsupervised strategy is self-supervised learning where the network is pretrained using a pretext task~\cite{cosmaSelfsupervisedRepresentationLearning2020}.
\end{itemize}

DIVA-DAF supports these strategies by offering a powerful configuration file system. This configuration file defines the training data and the task used for training the neural network and the neural network architecture. A network train with one configuration file can be trained further using another configuration file. The neural network architecture consists of a backbone and a header, which can both be adjusted separately using the configuration file. The framework logs each run in detail and stores its configuration file to ensure that the same experiment can be reproduced later. Additionally, the framework takes advantage of current hardware acceleration like GPU clusters. These features make our framework a powerful tool to conduct \ac{HDIA} experiments.

In the remainder of this paper, we provide an overview of related work in section 2. Then, we introduce our framework in section 3. Next, we describe the features of our framework in section 4. Afterward, we present a case study using our framework in section 5. Finally, we offer our conclusion and outlook in section 6.




%
 
\section{Related Work}
In our literature research, we came across different existing frameworks for \ac{DL} with which we share the general motivation as well as ideas.


Data Version Control~\cite{controlDataVersionControl2022} takes care of managing \ac{ML} project, including code and data.
This is achieved with a git-like \ac{CLI}.

Comet~\cite{CometSuperchargeMachine2022}, Neptune~\cite{NeptuneAiMetadata2022}, and Weights and Biases~\cite{biewaldExperimentTrackingWeights2020} take care of tracking your code, experiments, and results as well as visualizing them.
They work with a large amount of different machine learning libraries.

By making datasets, tasks, workflows, as well as results accessible for the public, OpenML~\cite{vanschorenOpenMLNetworkedScience2013} focused on the reproducibility of experiments.
The same focus was taken by DeepDIVA~\cite{albertiImprovingReproducibleDeep2019} for \ac{CV} as well as DeepZensols~\cite{landesDeepZensolsDeepNatural2021} for \ac{NLP}.
They made \ac{DL} experiment intuitive and fast to set up by providing different out-of-the-box experiments and visualizations.
Other tools like Tensorboard Sacred~\cite{greffSacredInfrastructureComputational2017}, CDE~\cite{guoCDEToolCreating2012}, FGBLab~\cite{arulkumaranFGLabMachineLearning2016}, Sumatra~\cite{davisonAutomatedCaptureExperiment2012}, and ReproZip~\cite{chirigatiReprozipComputationalReproducibility2016} store a verity of different information about code, dependencies, host information, system calls or files used to ensure reproducible research.
Cobra~\cite{vogtlinCobraCLITool2020} supports the user to create a reproducible project by keeping track of the code and the versions of the dependencies.

With CodaLab~\cite{CodaLab2022} users can run reproducible experiments and can rerun experiments of other users to validate their results.
It also allows to participate in different competitions or host a new competition.
All of these features are provided through a proprietary web interface.

Polyaxon~\cite{PolyaxonMachineLearning2022} lets you build, train, and monitor large-scale deep learning applications on their online open-source platform.
It is compatible with a large variety of \ac{DL} libraries.

Chainer~\cite{tokuiChainerDeepLearning2019} provides a wide range of \ac{DL} models for researchers in a flexible and intuitive fashion. 
The framework's focus is high-performance and distributed training, which is achieved with the help of standard Python libraries.

Orhei et al.~\cite{orheiEndToEndComputerVision2021} tackled the problem of creating a true end-to-end \ac{ML} platform, that allows to combine \ac{DL} approach together with classical \ac{PR} methods.
Additionally, their platform is constructed to be easily usable for research and educational purposes.

 
\section{DIVA-DAF - Document Analysis Framework}

In this section, we introduce our deep learning framework DIVA-DAF. We aim to develop a framework that allows rapid prototyping for historical document analysis while leveraging state-of-the-art deep learning technologies and architectures. Our framework is based on PyTorch~\cite{paszkePyTorchImperativeStyle2019} and open-source. We encourage the research community to use it for their experiments.

The intended application for DIVA-DAF is historical document image analysis. We want to train models using different amounts and types of training data and apply self-supervised learning methods. Therefore, we have made many design decisions with these tasks in mind. For example, the framework supports high-resolution scans of historical document image pages that would not directly fit into GPU memory by cropping the image.
The amount of training data can be adjusted easily, and it also supports training the network on different tasks. 

Overall, we plan to run an extensive amount of different research experiments using this framework. We want to ensure that we can quickly set up different kinds of experiments, easily keep track of them, and rerun them in a reproducible manner. Therefore, we have three main objectives while developing our framework: (1) Rapid prototyping, (2) Reproducibility, (3) State-of-the-art deep learning technologies. We achieved these objectives using several different features. In the following paragraphs, we overview each objective and the related features. In section~\ref{sec:features}, we give a more detailed overview of each feature.

Our framework allows rapid prototyping by defining each experiment using a configuration file. This configuration file can link to multiple other configuration files that precisely define each class's parameters in the system. For example, the experiment configuration file can contain another configuration file that defines the loss function class. The classes can be from the framework itself or an imported library. This procedure allows the user to set up experiments and modify parameters without coding anything. Additionally, DIVA-DAF is compatible with other deep learning frameworks. This compatibility allows the user to reuse existing implementations before implementing everything from scratch. These features allow the user to set up experiments rapidly.

The reproducibility of experiments is one of the critical issues of our time, as stated by~\cite{olorisadeReproducibilityMachineLearningBased2017}. Hutson even sees the research community in a reproducibility crisis~\cite{hutsonArtificialIntelligenceFaces2018}. Our framework allows the user to run reproducible experiments by automatically saving the experiment's configuration file alongside the results and network weights. It also saves the seed used to initialize the pseudo-random generators used during training to initialize the neural network weights. Using the configuration file with this seed, other researchers can quickly reproduce published results with our framework.

The foundation of DIVA-DAF is PyTorch, one of the most popular \\machine-learning frameworks. This foundation allows us to benefit from the vast community of researchers constantly implementing new ideas using PyTorch. It supports the latest GPU and server architectures. Researchers can use certain implementations directly in our framework, e.g., the neural network model implementations provided by the well-known Torchvision~\cite{paszkePyTorchImperativeStyle2019} library.

Our framework is open-source and publicly available for any scientific purpose. It is available on GitHub via this link: \url{https://github.com/DIVA-DIA/DIVA-DAF}. 


\subsection{Main Components}\label{subsec:components}
The framework consists of several components following an object-oriented programming paradigm. The framework's components are based on PyTorch-\\Lightning~\cite{falconPyTorchLightning2019}, a PyTorch research framework that focuses on the rapid development of research experiments. Four main components of our framework are (1) datamodule, (2) model, (3) task, (4) trainer (see Fig.~\ref{fig:framework_schema}). Compared to PyTorch-Lightning, we have separated the LightningModule into two components: the model, which defines the neural network architecture, and the task, which describes the task that we try to solve. By defining these components independently, we can solve the same task using different models or use the same model to solve different tasks.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.245]{figures/framework_schema.pdf}
    \caption{The module schema of DIVA-DAF. The orange modules are the main components, blue are additional components, and green is the configuration.}
    \label{fig:framework_schema}
\end{figure}

The datamodule handles the data and creates the different datasets needed for each stage (training, validation, testing). Furthermore, it calculates data statistics and defines special data handling, such as data augmentation and transformations.

The model specifies the neural network architecture by defining the backbone and the header. The backbone acts as the encoder part of the network and the header as the classifier. By defining these two parts separately, the framework can save them independently and combine them with other backbones or headers.

The task defines the workflow during training, validation, and testing. It requires four inputs: a loss function, an optimizer, a metric, and a model. Also, it produces the test output and provides the needed method to bring the network output into a loss or metric format. 

The trainer connects the different components of our framework and runs them. It executes the different stages - training, validation, and testing - and runs the neural network. It is also responsible for initializing the different hardware devices and moving data and models to the correct device. We use the default implementation of PyTorch-Lightning, but users could also exchange or modify this part if required.

Additionally to the four core modules, the framework contains several smaller components: loss, optimizer, metric, logger, callbacks, and plugins. Loss, optimizer, and metric are a subpart of the task module. The logger provides the possibility to use the user's favorite logging system (see section~\ref{subsec:features_logging}). With plugins and callbacks, which PyTorch-Lightning provides, the user can control the behavior of devices, stage loops, and other parts of the framework.

 
\section{Features}\label{sec:features}
This section will present the complementary features to the previous subsection~\ref{subsec:components}.
The most important feature of our framework and also the base of other features is the configuration system that we present in subsection~\ref{subsec:configs}.
Based on the configuration system, we introduce the out-of-the-box feature in subsection~\ref{subsec:outofthebox}.
In subsection~\ref{subsec:callbacks}, we will present the callback system of DIVA-DAF.
To further adapt the framework to the user's need, we will discuss the modularity of the framework in subsection~\ref{subsec:modules}.
Finally, in section~\ref{subsec:features_logging} we will present the logging capability of our framework. 



\subsection{Configurations}\label{subsec:configs}
We use configuration files to rerun an experiment quickly or share it with a colleague easily.
The configuration system of our framework is provided by Hydra~\cite{yadanHydraFrameworkElegantly2019}, which creates python objects based on configuration files in an elegant fashion.
These configuration files contain the python path to a class and the arguments to initialize the class.
The path can lead to a class implemented by the user or to one in a python package, e.g., Torchvision.
By combining the different configurations for the task, model, metric, and loss, the user can create a precise definition of an experiment. 
When the framework has successfully read all configuration files, it combines them and stores this run configuration in the specific experiment output folder.
Using this experiment configuration file, we can reproduce an experiment as all the parameters of each created object are stored.

Datamodules and tasks objects have an additional ability. 
Configurations can use run-time variables from these objects in their definitions.
For example, the user can set the number of output filters for a segmentation network depending on the datamodule at run-time.
In Listing~\ref{lst:datamodule_example}, we show how we can use run-time information of the datamodule object in the network configuration.



\begin{lstlisting}[frame=single, caption={Usage of the datamodule variable \texttt{num\_classes} which is set during run-time.}, label={lst:datamodule_example}]
_target_: pl_bolts.models.vision.UNet
num_classes: \times\times\times\times\sim\times python run.py experiment=cb55_full_run_unet.yaml
\end{lstlisting}

The run logs and the output of the testing stage will be stored in the experiment directory.
Each setup (30, 15, 1 training page(s)) was executed three times with three different seeds and then averaged.
We used for the three different setups always the same three seeds for the runs.
Using the same seeds ensures that the experiments start with the same initial network weights.


A configuration file change is not needed to run the experiment if only a few parameters need to be changed. Instead, parameters can be changed or added as a command-line argument. 
In this experiment, we only want to adjust the number of training pages and seeds, which we can do more quickly using the command line. 
The following command shows how to run the base experiment with 15 training pages and a seed of 2149823.
\begin{lstlisting}
\sim\sim^\ast\sim\pm\pm\sim\pm\pm\sim\pm\pm^\ast\sim$0.25h.
An explanation could be that the first page of the training set is an excellent overall representation of the dataset.

In Table~\ref{tab:segmentation_results_images}, you can observe that the segmentation quality decreases with fewer training examples.
Additionally, we can see that all setups struggle in certain sections to separate main text and comments.
Overall, every setup produces precise contours and has a clean border between classes.

 
\section{Conclusion and Future Work}
In this paper, we introduce DIVA-DAF. 
It is an open-source Python-based deep learning framework designed to create rapid prototypes and reproducible experiments for the historical \ac{DIA} community. 
It can be used on different hardware and takes full advantage of the different devices.
Users can easily use predefined experiments or adapt them to their needs.
It is possible to implement own tasks and datamodules straightforwardly due to the framework's abstract classes.


To improve the framework, we want to extend our set of tasks with more \ac{DIA} relevant scenarios to make it even more attractive for the community.
Also, we want to introduce self-supervised strategies to the framework to tackle the lack of manually-labeled ground truth.
To further support the users in analyzing their results, we work on classification activation maps and filter visualization techniques to add to the framework.
Overall, we continue to improve the general structure and efficiency of DIVA-DAF.
 



\bibliographystyle{splncs04}
\bibliography{bibliography}
\end{document}
