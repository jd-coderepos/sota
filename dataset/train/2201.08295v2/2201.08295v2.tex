\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}                \usepackage[nolist]{acronym}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyphenat}
\usepackage{pifont}
\usepackage{listings}
\usepackage{array}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{color}
\newcommand{\change}[1]{{\color{red}#1}}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


\begin{document}
\title{DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis}
\titlerunning{DIVA-DAF}

\begin{comment}
    \author{Anonymous Author\inst{1} \and
    Anonymous Author\inst{1} \and
    Anonymous Author\inst{1}}
    
    \authorrunning{Author et al.}
    
    \institute{\textit{Anonymous Group (AG)} \\
    Anonymous University, Anonymousstreet 1A, Anonymous City, Anonymous Country, \\
    \email{\{firstname.lastname\}@AnonymousUniversity.abc}}
\end{comment}

\author{Lars~V\"ogtlin \and
    Paul~Maergner \and
    Rolf~Ingold}
    
    \authorrunning{V\"ogtlin et al.}

    \institute{\textit{Document Image and Voice Analysis Group (DIVA)} \\
    University of Fribourg, Switzerland \\
    \email{\{firstname.lastname\}@unifr.ch}}


\maketitle              

\begin{acronym}[UMLX]
 \acro{HDIA}{Historical Document Image Analysis}
 \acro{DIA}{Document Image Analysis}
 \acro{ML}{Machine Learning}
 \acro{CUDA}{NVIDIA Compute Unified Device Architecture}
 \acro{CuDNN}{Deep Neural Network library}
 \acro{CNN}{Convolutional Neural Network}
 \acro{CLI}{Command Line Interface}
 \acro{DL}{Deep Learning}
 \acro{CV}{Computer Vision}
 \acro{NLP}{Natural Language Processing}
 \acro{PR}{Pattern Recognition}
 \acro{mIoU}{mean Intersection over Union}
\end{acronym} 
\begin{abstract}
In this paper, we introduce a new deep learning framework called DIVA-DAF. We have developed this framework to support our research on historical document image analysis tasks and to develop techniques to reduce the need for manually-labeled ground truth. We want to apply self-supervised learning techniques and use different kinds of training data. Our new framework aids us in performing rapid prototyping and reproducible experiments. We present a first semantic segmentation experiment on DIVA-HisDB using our framework, achieving state-of-the-art results. The DIVA-DAF framework is open-source, and we encourage other research groups to use it for their experiments.




\keywords{Framework, Open-Source \and Deep Learning \and Neural Networks \and Reproducible Research \and Machine Learning \and Historical Document Image Analysis.}
\end{abstract}     
\section{Introduction}
Deep learning (DL) methods have played a significant role in the last decade in increasing the performance of Computer Vision (CV) tasks and, as such, also historical document image analysis (HDIA) tasks. The drawback of common DL approaches is their enormous hunger for annotated data. This hunger is particularly problematic in HDIA since most tasks, like semantic labeling, require experts to label the document images correctly. For example, historical handwritten documents often contain comments surrounding or intersecting the main text. Distinguishing between these two classes is crucial for analyzing the document. However, this distinction often requires expert knowledge. Fig.~\ref{fig:hisdoc_problems} shows a few examples where the distinction between comment and the main text is difficult.

In our research, we want to reduce the requirement for manually-labeled ground truth for HDIA with the help of different techniques, like applying self-supervised learning~\cite{jingSelfsupervisedVisualFeature2020,cosmaSelfsupervisedRepresentationLearning2020} and synthesizing data~\cite{vogtlinGeneratingSyntheticHandwritten2021a}. We have created a new framework called DIVA-DAF that aids us with this research. Our framework allows us to perform rapid prototyping and reproducible experiments focusing on HDIA tasks. The framework is open-source~\footnote{\url{https://github.com/DIVA-DIA/DIVA-DAF}}, and we encourage other research groups to use it for their experiments. This paper introduces our framework to the research community and shows some initial experiments.







\begin{figure}[h!]
    \centering
\subfloat[CSG18, p.107\label{fig:csg18_107}]{
      \includegraphics[width=\textwidth]{figures/his_problem/csg18_107_cropped.jpg}
    }
    \vfill
    \subfloat[CSG863, p.131\label{fig:csg863_131}]{
      \includegraphics[width=\textwidth]{figures/his_problem/csg0863_131_max.jpg}
    }
    \caption{
    Two sample images of the DIVA-HisDB~\cite{simistiraDIVAHisDBPreciselyAnnotated2016} dataset: Each image (a, b) shows the problem of distinguishing comments and main text. Image (a) is a challenging document with interlinear glosses and letters of different sizes and colors. For a non-expert, it is hard to say if the curved line in image (b) is a comment or part of the main text.
}
    \label{fig:hisdoc_problems}
\end{figure}


A variety of general strategies exist to reduce the need for manually-labeled data. Four common strategies are transfer learning, data augmentation, synthesizing data, and unsupervised learning.
\begin{itemize}
    \item Transfer learning trains a deep neural network on a similar dataset, which provides more labeled data and then fine-tunes it on the actual small labeled target dataset~\cite{caruanaMultitaskLearning1997}.
    \item Data augmentation can be performed, e.g., by degrading real documents so that the existing ground truth can still be used~\cite{bairdDocumentImageDefect1992,kieuCharacterDegradationModel2012,seuretGradientdomainDegradationsImproving2015}.
    \item Synthesizing data is creating new documents based on existing real documents~\cite{capobiancoDocEmulToolkitGenerate2017,journetDocCreatorNewSoftware2017,vogtlinGeneratingSyntheticHandwritten2021a} or creating new documents from scratch.
    \item Unsupervised learning strategies are, e.g., autoencoders~\cite{masciStackedConvolutionalAutoEncoders2011,albertiPitfallUnsupervisedPreTraining2017a}. Another unsupervised strategy is self-supervised learning where the network is pretrained using a pretext task~\cite{cosmaSelfsupervisedRepresentationLearning2020}.
\end{itemize}

DIVA-DAF supports these strategies by offering a powerful configuration file system. This configuration file defines the training data and the task used for training the neural network and the neural network architecture. A network train with one configuration file can be trained further using another configuration file. The neural network architecture consists of a backbone and a header, which can both be adjusted separately using the configuration file. The framework logs each run in detail and stores its configuration file to ensure that the same experiment can be reproduced later. Additionally, the framework takes advantage of current hardware acceleration like GPU clusters. These features make our framework a powerful tool to conduct \ac{HDIA} experiments.

In the remainder of this paper, we provide an overview of related work in section 2. Then, we introduce our framework in section 3. Next, we describe the features of our framework in section 4. Afterward, we present a case study using our framework in section 5. Finally, we offer our conclusion and outlook in section 6.




%
 
\section{Related Work}
In our literature research, we came across different existing frameworks for \ac{DL} with which we share the general motivation as well as ideas.


Data Version Control~\cite{controlDataVersionControl2022} takes care of managing \ac{ML} project, including code and data.
This is achieved with a git-like \ac{CLI}.

Comet~\cite{CometSuperchargeMachine2022}, Neptune~\cite{NeptuneAiMetadata2022}, and Weights and Biases~\cite{biewaldExperimentTrackingWeights2020} take care of tracking your code, experiments, and results as well as visualizing them.
They work with a large amount of different machine learning libraries.

By making datasets, tasks, workflows, as well as results accessible for the public, OpenML~\cite{vanschorenOpenMLNetworkedScience2013} focused on the reproducibility of experiments.
The same focus was taken by DeepDIVA~\cite{albertiImprovingReproducibleDeep2019} for \ac{CV} as well as DeepZensols~\cite{landesDeepZensolsDeepNatural2021} for \ac{NLP}.
They made \ac{DL} experiment intuitive and fast to set up by providing different out-of-the-box experiments and visualizations.
Other tools like Tensorboard Sacred~\cite{greffSacredInfrastructureComputational2017}, CDE~\cite{guoCDEToolCreating2012}, FGBLab~\cite{arulkumaranFGLabMachineLearning2016}, Sumatra~\cite{davisonAutomatedCaptureExperiment2012}, and ReproZip~\cite{chirigatiReprozipComputationalReproducibility2016} store a verity of different information about code, dependencies, host information, system calls or files used to ensure reproducible research.
Cobra~\cite{vogtlinCobraCLITool2020} supports the user to create a reproducible project by keeping track of the code and the versions of the dependencies.

With CodaLab~\cite{CodaLab2022} users can run reproducible experiments and can rerun experiments of other users to validate their results.
It also allows to participate in different competitions or host a new competition.
All of these features are provided through a proprietary web interface.

Polyaxon~\cite{PolyaxonMachineLearning2022} lets you build, train, and monitor large-scale deep learning applications on their online open-source platform.
It is compatible with a large variety of \ac{DL} libraries.

Chainer~\cite{tokuiChainerDeepLearning2019} provides a wide range of \ac{DL} models for researchers in a flexible and intuitive fashion. 
The framework's focus is high-performance and distributed training, which is achieved with the help of standard Python libraries.

Orhei et al.~\cite{orheiEndToEndComputerVision2021} tackled the problem of creating a true end-to-end \ac{ML} platform, that allows to combine \ac{DL} approach together with classical \ac{PR} methods.
Additionally, their platform is constructed to be easily usable for research and educational purposes.

 
\section{DIVA-DAF - Document Analysis Framework}

In this section, we introduce our deep learning framework DIVA-DAF. We aim to develop a framework that allows rapid prototyping for historical document analysis while leveraging state-of-the-art deep learning technologies and architectures. Our framework is based on PyTorch~\cite{paszkePyTorchImperativeStyle2019} and open-source. We encourage the research community to use it for their experiments.

The intended application for DIVA-DAF is historical document image analysis. We want to train models using different amounts and types of training data and apply self-supervised learning methods. Therefore, we have made many design decisions with these tasks in mind. For example, the framework supports high-resolution scans of historical document image pages that would not directly fit into GPU memory by cropping the image.
The amount of training data can be adjusted easily, and it also supports training the network on different tasks. 

Overall, we plan to run an extensive amount of different research experiments using this framework. We want to ensure that we can quickly set up different kinds of experiments, easily keep track of them, and rerun them in a reproducible manner. Therefore, we have three main objectives while developing our framework: (1) Rapid prototyping, (2) Reproducibility, (3) State-of-the-art deep learning technologies. We achieved these objectives using several different features. In the following paragraphs, we overview each objective and the related features. In section~\ref{sec:features}, we give a more detailed overview of each feature.

Our framework allows rapid prototyping by defining each experiment using a configuration file. This configuration file can link to multiple other configuration files that precisely define each class's parameters in the system. For example, the experiment configuration file can contain another configuration file that defines the loss function class. The classes can be from the framework itself or an imported library. This procedure allows the user to set up experiments and modify parameters without coding anything. Additionally, DIVA-DAF is compatible with other deep learning frameworks. This compatibility allows the user to reuse existing implementations before implementing everything from scratch. These features allow the user to set up experiments rapidly.

The reproducibility of experiments is one of the critical issues of our time, as stated by~\cite{olorisadeReproducibilityMachineLearningBased2017}. Hutson even sees the research community in a reproducibility crisis~\cite{hutsonArtificialIntelligenceFaces2018}. Our framework allows the user to run reproducible experiments by automatically saving the experiment's configuration file alongside the results and network weights. It also saves the seed used to initialize the pseudo-random generators used during training to initialize the neural network weights. Using the configuration file with this seed, other researchers can quickly reproduce published results with our framework.

The foundation of DIVA-DAF is PyTorch, one of the most popular \\machine-learning frameworks. This foundation allows us to benefit from the vast community of researchers constantly implementing new ideas using PyTorch. It supports the latest GPU and server architectures. Researchers can use certain implementations directly in our framework, e.g., the neural network model implementations provided by the well-known Torchvision~\cite{paszkePyTorchImperativeStyle2019} library.

Our framework is open-source and publicly available for any scientific purpose. It is available on GitHub via this link: \url{https://github.com/DIVA-DIA/DIVA-DAF}. 


\subsection{Main Components}\label{subsec:components}
The framework consists of several components following an object-oriented programming paradigm. The framework's components are based on PyTorch-\\Lightning~\cite{falconPyTorchLightning2019}, a PyTorch research framework that focuses on the rapid development of research experiments. Four main components of our framework are (1) datamodule, (2) model, (3) task, (4) trainer (see Fig.~\ref{fig:framework_schema}). Compared to PyTorch-Lightning, we have separated the LightningModule into two components: the model, which defines the neural network architecture, and the task, which describes the task that we try to solve. By defining these components independently, we can solve the same task using different models or use the same model to solve different tasks.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.245]{figures/framework_schema.pdf}
    \caption{The module schema of DIVA-DAF. The orange modules are the main components, blue are additional components, and green is the configuration.}
    \label{fig:framework_schema}
\end{figure}

The datamodule handles the data and creates the different datasets needed for each stage (training, validation, testing). Furthermore, it calculates data statistics and defines special data handling, such as data augmentation and transformations.

The model specifies the neural network architecture by defining the backbone and the header. The backbone acts as the encoder part of the network and the header as the classifier. By defining these two parts separately, the framework can save them independently and combine them with other backbones or headers.

The task defines the workflow during training, validation, and testing. It requires four inputs: a loss function, an optimizer, a metric, and a model. Also, it produces the test output and provides the needed method to bring the network output into a loss or metric format. 

The trainer connects the different components of our framework and runs them. It executes the different stages - training, validation, and testing - and runs the neural network. It is also responsible for initializing the different hardware devices and moving data and models to the correct device. We use the default implementation of PyTorch-Lightning, but users could also exchange or modify this part if required.

Additionally to the four core modules, the framework contains several smaller components: loss, optimizer, metric, logger, callbacks, and plugins. Loss, optimizer, and metric are a subpart of the task module. The logger provides the possibility to use the user's favorite logging system (see section~\ref{subsec:features_logging}). With plugins and callbacks, which PyTorch-Lightning provides, the user can control the behavior of devices, stage loops, and other parts of the framework.

 
\section{Features}\label{sec:features}
This section will present the complementary features to the previous subsection~\ref{subsec:components}.
The most important feature of our framework and also the base of other features is the configuration system that we present in subsection~\ref{subsec:configs}.
Based on the configuration system, we introduce the out-of-the-box feature in subsection~\ref{subsec:outofthebox}.
In subsection~\ref{subsec:callbacks}, we will present the callback system of DIVA-DAF.
To further adapt the framework to the user's need, we will discuss the modularity of the framework in subsection~\ref{subsec:modules}.
Finally, in section~\ref{subsec:features_logging} we will present the logging capability of our framework. 



\subsection{Configurations}\label{subsec:configs}
We use configuration files to rerun an experiment quickly or share it with a colleague easily.
The configuration system of our framework is provided by Hydra~\cite{yadanHydraFrameworkElegantly2019}, which creates python objects based on configuration files in an elegant fashion.
These configuration files contain the python path to a class and the arguments to initialize the class.
The path can lead to a class implemented by the user or to one in a python package, e.g., Torchvision.
By combining the different configurations for the task, model, metric, and loss, the user can create a precise definition of an experiment. 
When the framework has successfully read all configuration files, it combines them and stores this run configuration in the specific experiment output folder.
Using this experiment configuration file, we can reproduce an experiment as all the parameters of each created object are stored.

Datamodules and tasks objects have an additional ability. 
Configurations can use run-time variables from these objects in their definitions.
For example, the user can set the number of output filters for a segmentation network depending on the datamodule at run-time.
In Listing~\ref{lst:datamodule_example}, we show how we can use run-time information of the datamodule object in the network configuration.



\begin{lstlisting}[frame=single, caption={Usage of the datamodule variable \texttt{num\_classes} which is set during run-time.}, label={lst:datamodule_example}]
_target_: pl_bolts.models.vision.UNet
num_classes: ${datamodule:num_classes}
\end{lstlisting}


\subsection{Out-of-the-box Deep Learning}\label{subsec:outofthebox}
The setup of DIVA-DAF is straightforward: The user clones the code from the GitHub repository\footnote{\url{https://github.com/DIVA-DIA/DIVA-DAF}} and create a new Anaconda\cite{anaconda} environment based on the shipped environment file (\texttt{conda\_env\_gpu.yaml}).
This environment contains all dependencies with the corresponding versions to run the framework.
If a researcher wants to run the framework with GPU, TPU, or IPU support, the appropriate drivers and the correct PyTorch support packages (CUDA, ROCm, etc.) must be installed.

In its current state, DIVA-DAF contains sample code for two different \acl{DIA} tasks: semantic segmentation and image classification.
These tasks can be combined with different datamodules (see section~\ref{subsec:components}) like DIVA-HisDB~\cite{simistiraDIVAHisDBPreciselyAnnotated2016} or CIFAR10~\cite{krizhevskyLearningMultipleLayers2009}.
Additionally, our framework provides default implementations and their corresponding configuration for models, losses, optimizers, and metrics.
For example, famous neural network architectures like ResNet~\cite{heDeepResidualLearning2016} and U-Net~\cite{ronnebergerUnetConvolutionalNetworks2015}, or the widely used ADAM optimizer~\cite{kingmaAdamMethodStochastic2014} are part of the framework.







\subsection{Callbacks}\label{subsec:callbacks}
The callback functionality is provided by the underlying PyTorch-Lightning~\cite{falconPyTorchLightning2019} library.
The user can inject code into specific parts of our framework with a callback.
Callbacks are self-contained programs that are invoked in a specific position in the code (see PyTorch-Lightning callback api~\footnote{\url{https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html}}).
Each hook has its own set of parameters that can be used for different jobs.
For example, if the user wants to save the output of each validation batch on each device, he can create a callback and overwrite the method \texttt{on\_validation\_batch\_end()}.
This method receives the following arguments: the trainer, the task, the outputs, the batch, the batch index, and the data loader index.
Based on these arguments, the user can now store the network output of each batch.



We already provide callbacks to save the different parts of the model, to check the compatibility between the backbone and the header of the model, and to visualize the gradients of the different layers of the network when using the Weights and Biases logger (see Fig.~\ref{fig:wandb_gradients}).

\begin{figure}[ht]
    \centering
    \includegraphics[scale=.25]{figures/WaB_gradients.PNG}
    \caption{We provide a callback to log the gradients of a neural network. Here you see the weights and biases change during training.
}
    \label{fig:wandb_gradients}
\end{figure}








\subsection{Modules}\label{subsec:modules}
Our framework is built in a modular fashion, as described in subsection~\ref{subsec:components}.
Each module is independent and can be replaced with a third-party or user-defined implementation.
For example, we want to use U-Net~\cite{ronnebergerUnetConvolutionalNetworks2015} for a segmentation task that takes as a parameter the number of classes we want to segment.
The library Lightning-Bolts~\cite{falconFrameworkContrastiveSelfsupervised2020} contains an implementation of this architecture.
We can use this class by writing a corresponding configuration file (see listing~\ref{lst:datamodule_example}) and adding it as a model to our experiment configuration.
The user can use library classes for the datamodule, the network models, the metrics, the losses, the optimizer, the logger, and the callbacks.
In Table~\ref{tab:compatibility} we show a list of all libraries that are compatible with our framework and their functionality:

\begin{table}
    \caption{List of compatible libraries and their modules at the moment of writing. The \cmark denotes that a certain library provides a certain module and \xmark~that it does not. In parentheses modules which we did not test.}\label{tab:compatibility}
    \begin{tabular}{ P{0.17\textwidth} P{0.09\textwidth} P{0.09\textwidth} P{0.09\textwidth} P{0.12\textwidth} P{0.15\textwidth} P{0.12\textwidth} P{0.1\textwidth} }
        \hline
        Library &  Model & Metric & Loss & Optimizer & Datamodule & Callbacks & Logger\\
        \hline 
        PyTorch             & \xmark & \xmark & \cmark & \cmark & \xmark & \xmark & \xmark\\
        Torchvision         & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark\\
        Torchmetric         & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark\\
        Lightning           & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark\\
        Bolts               & \cmark & \xmark & \xmark & \xmark & \cmark & (\cmark) & \xmark\\
        \hline
    \end{tabular}
\end{table}

Users of the framework also can create their own modules.
To conveniently implement a task or datamodule, the user can inherit from the corresponding abstract base class of the desired module.
These abstract classes provide general variables and methods, giving the user a good starting point for creating their modules.



\subsection{Logging}\label{subsec:features_logging}
To keep track of experiments, we use the logging functionality of PyTorch-Lightning.
They provide the most common loggers like Weights and Biases~\cite{biewaldExperimentTrackingWeights2020} (see Fig.~\ref{fig:wandb_example}) or Tensorboard.
The framework allows using multiple loggers simultaneously.
Besides using a cloud-based logger, like Weights and Biases, the user can also use a local logger like the CSV logger.
The CSV logger writes all the logging information into the local experiment folder for later use.
Logging is not limited to scalar data like metric or loss information. 
It is also possible to log figures, images, or histograms, but each logger needs to do this individually.
As with all the other modules of our framework, users can also implement their own logger or adapt an existing one.


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.24]{figures/WaB_example.PNG}
    \caption{A typical dashboard on Weights and Biases. On the left side we see a list of experiments and on the right side the plots of the logged information during the training stage.}
    \label{fig:wandb_example}
\end{figure}


 
\newpage
\section{Case Study}
A typical task in historical \acf{DIA} is semantic segmentation.
Each pixel of an input image gets assigned one of a predefined set of classes.
This output can further be used for tasks like text line segmentation~\cite{albertiLabelingCuttingGrouping2019} or handwriting recognition~\cite{joHandwrittenTextSegmentation2019}.

We present how to perform semantic segmentation on the Cod. Bodmer 55 of the well known DIVA-HisDB~\cite{simistiraDIVAHisDBPreciselyAnnotated2016} dataset (see Fig.~\ref{fig:dataset}). 
The dataset contains 30 pages for training, 10 pages for validation, and 10 pages for testing. Each page has a dimension of 4872$\times$6496 pixels with a resolution of 600 dpi.
Each pixel belongs to one of 8 classes (background, main text body, decoration, comment, main text body + comment, main text body + decoration, comment + decoration,  main text body + decoration + comment).

\begin{figure*}[ht] 
    \centering
    
    \subfloat[CB55, p. 25r]{
    \includegraphics[width=\columnwidth]{figures/example_data/cb55_0025r_cropped.jpg}\label{fig:dataset_cb55_A}}
    \hfill
    \subfloat[CB55, p. 5v]{
    \includegraphics[width=\columnwidth]{figures/example_data/cb55_5v_cropped.jpg}\label{fig:dataset_cb55_B}}
    
    \caption{
    Sample pages of the medieval manuscripts Cod. Bodmer 55 of DIVA-HisDB.
    }
    \label{fig:dataset}
\end{figure*}

DIVA-DAF provides a task module for semantic segmentation. We have to select a datamodule and bring the dataset into the expected format.
We decide to process the page in crops of size 256$\times$256 pixel because of the size of the full-page images. Precisely, we have prepared the training and validation data by cropping each page into 1376 300$\times$300 pixel patches with 50\% overlap. Then, we take a random 256$\times$256 crop from each patch during each training and validation epoch. Accordingly, we select the datamodule available in DIVA-DAF that works with this kind of data (\texttt{DIVAHisDB/datamodule\_cropped.py}). 

We use a random initialized vanilla U-Net~\cite{ronnebergerUnetConvolutionalNetworks2015} architecture ($\sim$31 million parameters), which has eight output layers.
The network was trained using the ADAM~\cite{kingmaAdamMethodStochastic2014} optimizer with a learning rate of \texttt{0.001} for \texttt{50} epochs with a batch size of 16.
As a loss, we used the Cross-Entropy loss.
We run all of our experiments on a server with 4 $\times$ NVIDIA 1080 GTX with 8GB of GPU memory each, an Intel i7-5960X CPU, and 64 GB of RAM.
To evaluate the networks output after testing we use the official evaluation tool~\cite{albertiOpenEvaluationTool2017a} of the ICDAR 2017 competition~\cite{simistiraICDAR2017CompetitionLayout2017}.
The tool computes the \ac{mIoU} and the F1-score between the prediction of our network and the ground truth.

In our repository you will find under \texttt{configs/experiment} a file \texttt{cb55\_full\_run\_unet.yaml}, that contains all above listed parameters and configurations.
We will train the network with the train/val split and automatically execute the testing routine after training.
To run this experiment you start the framework with the following command:
\begin{lstlisting}
$ python run.py experiment=cb55_full_run_unet.yaml
\end{lstlisting}

The run logs and the output of the testing stage will be stored in the experiment directory.
Each setup (30, 15, 1 training page(s)) was executed three times with three different seeds and then averaged.
We used for the three different setups always the same three seeds for the runs.
Using the same seeds ensures that the experiments start with the same initial network weights.


A configuration file change is not needed to run the experiment if only a few parameters need to be changed. Instead, parameters can be changed or added as a command-line argument. 
In this experiment, we only want to adjust the number of training pages and seeds, which we can do more quickly using the command line. 
The following command shows how to run the base experiment with 15 training pages and a seed of 2149823.
\begin{lstlisting}
$ python run.py experiment=cb55_full_run_unet.yaml 
    datamodule.selection_train=15 +seed=2149823
\end{lstlisting}

\begin{table}[ht]
    \caption{Results of semantic segmentation on the testset of DIVA-HisDBs CB55 with a U-Net. All results are averages over three runs with different random seeds. All our networks were trained for 50 epochs.}
    \label{tab:segmentation_results}
    \begin{tabular}{ P{0.09\textwidth} P{0.09\textwidth} P{0.16\textwidth} P{0.15\textwidth} P{0.15\textwidth} P{0.15\textwidth} P{0.14\textwidth}}
        \hline
        Authors & Year & \#train pages & Model & Run time & \ac{mIoU}[\%] & F1-Score[\%] \\
        \hline
\cite{studerComprehensiveStudyImageNet2019a} & 2019 & 30 & SegNet  & $\sim$8h    & 86.90 & N/A\\
        \cite{studerComprehensiveStudyImageNet2019a} & 2019 & 30 & DeepLabV3  & $\sim$8h & 92.90 & N/A\\
        \cite{simistiraICDAR2017CompetitionLayout2017} & 2017 & 30 & FCN$^\ast$ & N/A   & 98.35 & N/A\\
        \cite{simistiraICDAR2017CompetitionLayout2017} & 2017 & 30 & ResNet18     & N/A    & 98.64 & N/A\\
        \hline
        Ours & 2022 & 1  & U-Net & $\sim$0.25h & 90.93$\pm$3.09 & 94.93$\pm$1.89\\
        Ours & 2022 & 15 & U-Net & $\sim$2.75h & 96.33$\pm$0.44 & 98.03$\pm$0.20\\
        Ours & 2022 & 30 & U-Net & $\sim$3.5h  & 97.26$\pm$0.20 & 98.63$\pm$0.11\\
        \hline
    \end{tabular}
    $^\ast$: Fully-convolutional Neural Network
\end{table}


\begin{table}
    \centering
    \caption{Visual results of semantic segmentation on the testset of DIVA-HisDBs CB55 with a U-Net. In the images you see comments(red), main text(purple), decoration(yellow), and main text + decoration(turquoise). We can see that the quality of the segmentation decreases with the amount of training pages.}
    \label{tab:segmentation_results_images}
    \begin{tabular}{ P{0.18\textwidth} | P{0.39\textwidth} P{0.39\textwidth} }
        & &\\
        &  CB55 p. 98v & CB55 p. 102v \\
        \hline
        
        & &\\
        
        \parbox[m]{0.17\textwidth}{ \vspace*{-7em} Original} &
        \includegraphics[scale=.78]{figures/result_images/cb55-98v/e-codices_fmb-cb-0055_0098v_max-series-ori.pdf} & \includegraphics[scale=.8]{figures/result_images/cb55-102v/e-codices_fmb-cb-0055_0102v_max-series-ori.pdf} \\
        
        & &\\
        \parbox[m]{0.17\textwidth}{ \vspace*{-7em} 30 Pages} &
        \includegraphics[scale=.78]{figures/result_images/cb55-98v/e-codices_fmb-cb-0055_0098v_max-series-30.pdf} &
        \includegraphics[scale=.8]{figures/result_images/cb55-102v/e-codices_fmb-cb-0055_0102v_max-series-30.pdf} \\
       
        & &\\
        \parbox[m]{0.17\textwidth}{ \vspace*{-7em}15 Pages} &
        \includegraphics[scale=.78]{figures/result_images/cb55-98v/e-codices_fmb-cb-0055_0098v_max-series-15.pdf} &
        \includegraphics[scale=.8]{figures/result_images/cb55-102v/e-codices_fmb-cb-0055_0102v_max-series-15.pdf}\\
       
        & &\\
        \parbox[m]{0.17\textwidth}{ \vspace*{-7em}1 Page} &
        \includegraphics[scale=.78]{figures/result_images/cb55-98v/e-codices_fmb-cb-0055_0098v_max-series-1.pdf} &
        \includegraphics[scale=.8]{figures/result_images/cb55-102v/e-codices_fmb-cb-0055_0102v_max-series-1.pdf}\\
      
        & &\\
        \parbox[m]{0.17\textwidth}{ \vspace*{-7em}Ground Truth} &
        \includegraphics[scale=.78]{figures/result_images/cb55-98v/e-codices_fmb-cb-0055_0098v_max-series-gt.pdf} &
        \includegraphics[scale=.8]{figures/result_images/cb55-102v/e-codices_fmb-cb-0055_0102v_max-series-gt.pdf}\\
        
        \hline
    \end{tabular}
\end{table}

In Table~\ref{tab:segmentation_results} the averages of the different setups we ran and some state-of-the-art results.
We can see that our system with 30 training pages is close to the current state-of-the-art reported in the ICDAR 2017 competition~\cite{simistiraICDAR2017CompetitionLayout2017}.
With 15 pages our model outperforms both the SegNet and the DeepLabV3 reporeted by Studer et al.~\cite{studerComprehensiveStudyImageNet2019a}.
Even with one training page, we outperform the SegNet, and we achieve this with a training time of $\sim$0.25h.
An explanation could be that the first page of the training set is an excellent overall representation of the dataset.

In Table~\ref{tab:segmentation_results_images}, you can observe that the segmentation quality decreases with fewer training examples.
Additionally, we can see that all setups struggle in certain sections to separate main text and comments.
Overall, every setup produces precise contours and has a clean border between classes.

 
\section{Conclusion and Future Work}
In this paper, we introduce DIVA-DAF. 
It is an open-source Python-based deep learning framework designed to create rapid prototypes and reproducible experiments for the historical \ac{DIA} community. 
It can be used on different hardware and takes full advantage of the different devices.
Users can easily use predefined experiments or adapt them to their needs.
It is possible to implement own tasks and datamodules straightforwardly due to the framework's abstract classes.


To improve the framework, we want to extend our set of tasks with more \ac{DIA} relevant scenarios to make it even more attractive for the community.
Also, we want to introduce self-supervised strategies to the framework to tackle the lack of manually-labeled ground truth.
To further support the users in analyzing their results, we work on classification activation maps and filter visualization techniques to add to the framework.
Overall, we continue to improve the general structure and efficiency of DIVA-DAF.
 



\bibliographystyle{splncs04}
\bibliography{bibliography}
\end{document}
