\documentclass[copyright,creativecommons]{eptcs}
\providecommand{\event}{QAPL 2012} 

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}
\usepackage{wrapfig} \usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{prop}{Prop}[section]
\newtheorem{proposition}{Prop}[section]
\newtheorem{example}{Example}[section]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{hypo}{Condition}[section]
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\lbbracket}{[\![}
\newcommand{\rbbracket}{]\!]}
\newcommand{\lb}{[\![}
\newcommand{\rb}{]\!]}
\newcommand{\FOLD}{\hbox{\sl fold}}
\newcommand{\foo}{\hbox{\sl foo}}
\newcommand{\fooa}{\hbox{\sl foo}_a}
\newcommand{\fooA}{\hbox{\sl foo}_A}
\newcommand{\foob}{\hbox{\sl foo}_b}
\newcommand{\fooB}{\hbox{\sl foo}_B}
\newcommand{\ListFoo}{\hbox{\sl listFoo}}
\newcommand{\pe}{\hbox{\sl pathestimate}}
\newcommand{\graph}{\hbox{\sl graph}}
\newcommand{\cnt}{\hbox{\sl count}}
\newcommand{\mrk}{\hbox{\sl mark}}
\newcommand{\Cone}{C1}
\newcommand{\Ctwo}{C2}
\newcommand{\Cthree}{C3}
\newcommand{\Cfour}{C4}
\newcommand{\Kn}{k_{N^*}}
\newcommand{\En}{\epsilon_{N^*}}
\newcommand{\AND}{\mathrel{\wedge}}
\newcommand{\fs}[1]{#1_{\upharpoonright a}}
\lstdefinelanguage{pscode}
  {morekeywords={while,for,return},
  sensitive=false,
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}


\renewenvironment{proof}{\noindent {\bf Proof}\quad}{\qed}
\title{A non-local method for robustness analysis of\\ floating point programs
      \thanks{This work has been partially supported by the project ANR-09-BLAN-0345-02  CPP.}}
\author{Ivan Gazeau, Dale Miller, and Catuscia Palamidessi\\
INRIA and LIX, Ecole Polytechnique}
\def\titlerunning{A non-local method for robustness analysis of
  floating point programs}
\def\authorrunning{I. Gazeau, D. Miller, and C. Palamidessi}
\begin{document}
\maketitle

\begin{abstract}
Robustness is a standard correctness property which intuitively means
that if the input to the program changes less than a fixed small
amount then the output changes only slightly.
This notion is useful in the analysis of rounding error for floating
point programs because it helps to establish bounds on output errors
introduced by both measurement errors and by floating point
computation.
Compositional methods often do not work since key constructs---like
the conditional and the while-loop---are not robust.
We propose a method for proving the robustness of a while-loop.
This method is non-local in the sense that instead of breaking the
analysis down to single lines of code, it checks certain global
properties of its structure.
We show the applicability of our method on two standard algorithms:
the CORDIC computation of the cosine and Dijkstra's shortest path
algorithm.\\ 

\noindent {\bf Keywords: } Program analysis,
floating-point arithmetic,
robustness to errors.
\end{abstract}


\section{Introduction}





Programs using floating point arithmetic are often used for critical
applications and it is therefore fundamental to develop methods to
establish the correctness of such programs.  A central problem in
dealing with floating point programs is the propagation of errors due
to the digitization of analog quantities and the introduction of
floating point errors during computation.  As is well known, floating
point arithmetic on these representations is quite different from real
number arithmetic: for example, addition is neither commutative nor
associative \cite{goldberg91surveys}.




The developers of floating point programs would like to think in terms
of real number semantics instead of the more ad hoc and complicated
semantics given by some specific definition of floating point
arithmetic, such as the IEEE standard 754 \cite{ieee08}.  A central
problem in trying to reason about floating point programs is that in
dealing with non-continuous operators such as the conditional and the
while-loop, floating point errors can result in what appears to be
erratic behavior.  The problem is that these constructs are in general
\emph{non-robust}: small variations in the data can cause large
variations in the results.

When the program contains non-robust operators, traditional
compositional methods do not work well. Decomposing the
correctness of a looping program using Hoare triples, for example,
usually requires either introducing abstractions (e.g., approximations)
which can then make conclusions too imprecise, or to undergo a very
complex and intricate proof.

In this paper, we will take a different approach: we shall describe
some programs where such erratic behavior is recognized and find a way
to reason and bound all of that behavior.  By moving away from the
reasoning using Hoare's style emphasis on local and compositional
analysis of a looping program, we are able to avoid reasoning about
individual erratic behaviors: instead, we will treat such behaviors as
an aggregate and try to bound the behavior of that aggregate.

To illustrate such a possibility in reasoning, consider Dijkstra's
minimal path algorithm \cite{dijkstra59}.  This greedy algorithm moves
from a source node to its neighbors, always picking the node with the
least accumulated path from the source.  If one makes small changes to
the distances labeling edges, then the least path distance will change
also by a small amount: that is, this algorithm is continuous.
However, the actual behavior of the loop and the marking of subsequent
nodes can vary greatly with small changes to edge lengths.  Our
approach to reasoning will allow us to view all of these apparently
erratic choices of intermediate paths as an aggregate on which we are
able to establish the robustness of the entire algorithm.



\paragraph{Plan of the paper}
In the next section we introduce the concept of robustness and we
relate it to the notions of continuity and -Lipschitz.
Section~\ref{sec:control} contains our main contribution: a schema for
reasoning about robustness in programs 
and its correctness. We then show the applicability of our proposal in
two main examples: The CORDIC algorithm for computing cosine,
presented in Section~\ref{sec:cordi}, and Dijkstra's shortest-path
algorithm, presented in Section~\ref{sec:dijkstra}.  In
Section~\ref{sec:related} we discuss some related
work. Section~\ref{sec:conclude} concludes and discusses some future
lines of research.


\section{Robustness of floating-point programs}

Robustness is a standard concept from control theory
\cite{pettersson96dc,parsec09}.  In the case of programming languages,
there are two definitions of robustness that have been considered.
One definition used by Chaudhuri et al \cite{chaudhuri10popl}
considered robustness to be based on continuity.  Later Chaudhuri et
al \cite{chaudhuri11sigsoft} considered a stronger notion of
robustness, namely the -Lipschitz property: that is, 
changes to the input to a program lead to only proportionally bounded
changes to the output.
Another approach was used by Majumdar et al in
\cite{majumdar09rtss,majumdar10memocode} where robustness is
formulated as ``if the input of the program changes by an amount less
than , where  is a {\em fixed} constant, then the
output changes only slightly."  In our paper, we propose a more
flexible and general notion of robustness that generalizes both of
these concepts.  We now motivate and explain our notion of robustness
in more detail.

The notions of robustness considered in
\cite{chaudhuri10popl,chaudhuri11sigsoft} are mainly useful for
\emph{exact semantics}, namely when we do not take into account the
errors introduced by the representation and/or the computation. In
this case, the only deviation  comes from the  error of the input. The
continuity property, that for a function  on reals is defined as:  

ensures that the correct output can be approximated when we can
approximate the input closely enough.   This notion of robustness,
however, is too weak in many settings, because a small  variation in
the input can cause an unbounded change in the output. The
-Lipschitz property, defined as

amends this problem because it bounds the variation in the output
linearly by the variation in the input.






In our setting, however, the -Lipschitz property is too
strong. This is due to the following  reasons:
\begin{enumerate}
\item If we consider a \emph{finite precision semantics}, like
  floating point implementations, the constant factor  can become
  much bigger than the one optimal for the exact semantics. For
  instance, assume that the available representations are the numbers
  in the set  and rounding is done
  by taking the lower value, and observe that a function like , which is -Lipschitz in the exact semantics,
  is only -Lipschitz in this approximate semantics. Indeed, there
  exist two values that differ by just  and return a result
  that differ by . For example, take  and  :
  we have that  and , but
  the   second result will be rounded down to .


\item There are algorithms that have a desired precision  as a
  parameter and are considered correct as long as the result differs
  by at most  from the results of the mathematical function they
  are meant to implement.  A program of this kind may be discontinuous
  (and therefore not -Lipschitz) even if it is considered to be a
  correct implementation of a -Lipschitz function.  The phenomenon
  is illustrated by the following program  which is meant to
  compute the inverse of a strictly increasing function
   whose inverse is -Lipschitz for some
  .
{\begin{lstlisting}[language=pscode]
f(i){ y=0;
      while(g(y) < i){
	   y = y+e; }
      return y; }
\end{lstlisting}}   
 
The program  approximates   with precision   in the sense that 

Given the above inequality, we would like to consider the  program 
as robust, even though the function it computes is discontinuous (and
hence not -Lipschitz, for any ). 

\end{enumerate}

These two observations lead us to define another property,
, to capture robustness:  

This property amends the two previous problems by setting  to
 in the first example and to  in the second example.
It also extends the usual definition of the -Lipschitz property, which can be expressed as . 

Now, we want to extend this definition to allow for several variables
and for other metric spaces besides : e.g., probability
distributions, intervals arithmetic etc.  Thus, we consider, instead,
two metric spaces: one for input (, ) and the other for the
return value (,).  Hence, our robustness property
 becomes


Finally, since we are studying small deviation, it is not useful to
get this property for any  and  in  but rather when they are
close: i.e., , for suitable values .
In convex spaces, this property can be easily extended to  
pairs of inputs  having distance more than  by using intermediate
values.  So, finally, in this paper we propose the property , 
described in the following definition.

\begin{definition}\label{def:k-epsilon-delta}
Let  and  be metric spaces with distance  and 
respectively,  a function, , and let
.  We define the
property  for the function  as follows: 

\end{definition}




\section{A schema and its correctness}\label{sec:control}
The main characteristic  of our schema is to subdivide the code into
several parts instead of analyzing it line by line. 
Our template, which we show in a moment, divides the data structures
in an algorithm into two parts, called  and .  Here,  is the
witness to the progress of the algorithm: in particular, the stopping
condition will only depend on  (and the input).   The structure 
is used to accumulate results that provide the answer when the
stopping condition is satisfied.

\subsection{The schema structure definition}\label{sec:schema}
Instead of
presenting a formal definition of program schema and matching of
code, we illustrate these with the schema in Figure~\ref{fig:main}.  

\begin{wrapfigure}{l}{60mm}
\begin{lstlisting}[language=pscode]
foo(i){
     a = a0;
     b = b0;
     while(S(i,a)){
	  c = O(a,b,c,i);
	  a = M(a,c);
	  b = N(i,b,c);
     }
     return b; }
\end{lstlisting}
\caption{The main template}\label{fig:main}
\end{wrapfigure}

Here, the schema variables {\tt a}, {\tt b}, {\tt c}, etc, denote
tuples of program variables such that no program variable occurs
twice among these schema variables. Program expressions such as 
\begin{lstlisting}
c = O(a,b,c,i);
\end{lstlisting}
denotes a program phrase that computes new values for the variables denoted
by {\tt c} from values of variables in the tuples {\tt a}, {\tt b},
{\tt c}, and {\tt i}.  The actual computation here will be denoted by
{\tt O}.  This looping program initializes the variables in {\tt a} and
{\tt b} with the values in the tuples {\tt a0} and
{\tt b0}, respectively.  The stopping condition for the loop is given
by the boolean valued expression {\tt S(i,a)} and the result of the
program is the tuple of values denoted by the variables in {\tt b}. 

We shall assume that all program variables are typed in the usual way:
variables may range over the values in their associated type.  Our
analysis of the metric properties of a looping program will, however,
consider that tuples of variables, for example, {\tt a} and {\tt b} in
Figure~\ref{fig:main}, range over some {\em metric space} on the
Cartesian product of the variables in the tuple.


\subsection{A sufficient condition for robustness}\label{sec:robustness}



\begin{wrapfigure}{r}{60mm}
\begin{lstlisting}[language=pscode]
ListFoo(i){
     a = a0;
     b = b0;
     j = 0;
     while(! S(i,a)){
	  c = O(a,b,c,i);
          j = j+1;
          l[j] = c;
	  a = M(a,c);
	  b = N(i,b,c); }
     return l; }
\end{lstlisting}
\caption{Collecting {\tt c} values in a list}\label{fig:listfoo}
\end{wrapfigure}

We shall now prove that a program having the generic structure of 
given in Figure~\ref{fig:main} has, under certain conditions, the
property  for some .

The aim of our method is to postpone the analysis of the exact
semantics of commands as far as possible. In order to begin the
analysis without specific knowledge of this semantics, we need to
manipulate other programs made from the functions , , and 
that have been identified.  For example, the program  in
Figure~\ref{fig:listfoo} will be used to extract the list of values
of  obtained for a particular execution of  with input .
The new lines added to  will assume the usual semantics for
natural numbers.







We now define two new programs.  The first is the 
 program given below: it has the same shape as 
but instead of setting {\tt c} by the computation of {\tt O(a,b,c,i)}, it
sets {\tt c} with the values of a list given in input. Naturally, the stop
condition for the loop is now that all elements of the list have been
accessed. Note that since  was just used in the computation of ,
the commands affecting  are now useless and can be removed. 
\begin{lstlisting}[language=pscode]
foo_b(l,i){
//   a = a0;
     b = b0;
     for(int j = 0; j < l.length; j++ ){
	  c = l[j];
//	  a = M(a,c);
	  b = N(i,b,c); }
     return b; }
\end{lstlisting}
We have used Java-style instructions such as  for the
length of the list  and  for the  element of the list
.  (The  syntax is used to form a comment.)  We define the new
function .  
Notice that . 

The second program  is the same program as  except
that  is returned instead of . In this program, the lines where
 is set are now useless. 
\begin{lstlisting}[language=pscode]
foo_a(l){
     a = a0;
//   b = b0;
     for(int j = 0; j < l.length; j++ ){
	  c = l[j];
	  a = M(a,c);
//	  b = N(i,b,c);
     }
     return a; }
\end{lstlisting}
Finally, we define . 
The two function  and  and relations between them will
be used to indirectly analyze the program .



In what follows, we use the following conventions: the domain of the
variables , , , and  are , ,  and ,
respectively, and  and  are some determined constants of type
 and  respectively.  For every type , the expression 
denote the type of lists of type .

We now introduce four conditions that need to hold to prove that the
 program satisfies  for appropriate
values of , , and .  These conditions apply to
{\em eight} parameters: namely, ,
.  Condition  expresses the property
 for the transformed program , condition
 expresses the fact that there is a relationship between the
values stored in  and the values stored in , and condition
 and  address the stability of the stop condition
.

\begin{hypo}[\Cone]\label{C1} \quad
.
\end{hypo}



The next condition states that whenever two inputs  and  are
within a  of each other then it is the case that if their
images in  (under )  are close, then their images in 
(under ) are close.

\begin{hypo}[\Ctwo]\label{C2}

\end{hypo}


The stopping condition  should satisfy the following two
conditions.  The first expresses that the boundary of the region  cannot vary too much.
\begin{hypo}[\Cthree]\label{C3}

\end{hypo}

The following condition on  states that the diameter of the region
 is as small as the desired precision.
\begin{hypo}[\Cfour]\label{C4}

\end{hypo}

Finally, our main theorem is the following.
\begin{theorem}
If the program  terminates and the conditions \Cone, \Ctwo,
\Cthree, and \Cfour\ hold, then  holds for
the function computed by  with  and
.
\end{theorem}


\begin{proof}
In the proof, we will use these two observations:
\begin{enumerate}
\item \label{O2} Since  is obtained from the computation of
  , and since  replaces the result of  by this
  list, if we compute  we are replacing each value for 
  by itself. Therefore we have that . 



\item \label{O5} In the execution of , the final value of 
  that satisfies the stopping condition  is . 
\end{enumerate}

By the observation~\ref{O2}, proving the theorem is equivalent to
proving 

By condition \Cone, choosing , we have 

By definition of , we have

From observation~\ref{O5},  holds.
By condition~\Cthree \, (instantiating  with ) we derive that:

Hence, by observations~\ref{O5} and ~\ref{O2},  also
holds. From inequality~\eqref{A} and condition~\Cfour, we derive

From the last inequality and from  inequality~\eqref{A}, we derive, using
the triangle inequality

From condition~{\Ctwo}  and inequality~\eqref{B}, we have

From inequalities~\eqref{D} and \eqref{C}, using the triangle inequality, we derive

Finally, we define  and .
\end{proof}

\section{Example:  the CORDIC algorithm for computing cosine}\label{sec:cordi}
In this section  we apply our method to a program implementing  the CORDIC algorithm~\cite{volder59}, and we prove that  it is
.

CORDIC (COordinate Rotation DIgital Computer) is a class of simple and
efficient algorithms to compute  hyperbolic and trigonometric
functions using only basic arithmetic (addition, subtraction and
shifts), plus table lookup. The notions behind this computing
machinery were motivated by the need to calculate the trigonometric
functions and their inverses in real time navigation systems. Still
now-a-days, since the CORDIC algorithms  require  only  simple integer
math, CORDIC is the preferred implementation of math functions on
small  hand calculators.

CORDIC is a successive approximation algorithm:  A sequence of
successively smaller rotations based on binary decisions drives the
algorithm towards 
the value we want to find.  The CORDIC version illustrated in the
program below  computes the cosine of any angle in .  

{\lstinputlisting[language=C]{cordic_simplified.c}}
Note that this program makes call to trigonometric functions like
cosine itself. But in the actual implementation, as it is explained in
the comments, these calls (that are done on values divided by successive powers of two) are
stored in a database so that no computation of these functions is actually done.

\subsection{Scheme instantiation}
To apply our method, we have first of all to instantiate the schema variables  , ,   (cf. Section~\ref{sec:schema}) with a suitable partition of the variables of the program.
The variables  are determined: they must be instantiated  with the variables which represent the  
input. 

In this example the partition for the variables will be the
following.

{\begin{lstlisting}
A := double theta;
B := double x,y;
C := double sigma;
I := double beta;
\end{lstlisting}}

We now must define a suitable metric on the types of the  variables in  and . We choose the  following:
\begin{itemize}
\item  is the usual distance on .
\item  is the  norm on .
\end{itemize}

Now we need to identify the stopping condition . This is given by:
\begin{lstlisting}
S(beta,theta) := | theta - beta | <= e
\end{lstlisting}


Then, we need to instantiate the functions , ,  of the schema with suitable regions of code.
We choose these as follows:

{\lstinputlisting[language=C]{cordic_matching.c}} 

Finally, we need to prove that the conditions \Cone, \Ctwo, \Cthree, and \Cfour \, (cf. Section~\ref{sec:robustness}) are satisfied. 

\subsection{Proofs of the conditions}

\paragraph{\Cone: }
This condition can be proved for the following program by such standard
techniques as abstract interpretation or Hoare triples.

{\lstinputlisting[language=C]{cordic_N.c}}


\paragraph{\Ctwo: 
}
This part of the proof is rather technical. The interested reader can
find it in the appendix of \cite{gazeau12hal}. 
The proof of ~\Ctwo \, is the most difficult part of this example. We have proved it ``by hand'', and we do not
claim that there is an easy way to automate it.  
However, this proof points out that  we can prove
the intended property  without considering the whole semantics of the program,  but
just the relevant properties. 

\paragraph{\Cthree: }
The instantiation of  corresponds to , so \Cthree\, is given by the condition:

We can satisfy this property by setting , , and
.

\paragraph{\Cfour: }
\Cfour \, can be rewritten, once we instantiate  to 

Which is true for .



\section{Example: Dijkstra's shortest path algorithm}\label{sec:dijkstra}

In this section we apply our method   to Dijkstra's shortest path algorithm. This is an algorithm that, given a graph, 
computes the shortest path between a source and any vertex of the
graph. We will prove, by instantiating our schema,  that the following program  implementing the Dijkstra's algorithm can be proved 
in the semantic of real numbers using our theorem. 

In the following program we use some conventions: the number of
vertices is  fixed to , all 
vertices are connected,  and the maximum  value for a path is  (some
stand-in of infinity).



{\lstinputlisting[language=C]{dijkstra_simplified.c}}


\subsection{Scheme instantiation}
To apply our theorem, we have to instantiate the scheme variables  , ,  with some variables of the program.  The variables of  are instantiated with the variables that represent the input. 
We choose the following instantiation:  contains the variables  and ,  the
array of double  and  the variable  which identify the
current vertex to propagate. 

{\scriptsize\begin{lstlisting}
A := int count;int mark[w];
B := pathestimate[w];
C := int u;
I := graph[w][w];
\end{lstlisting}}

We now have to choose a suitable metric on the types of the variables, and we choose the following:  is the  norm on an
array of real numbers,  is the  norm on array of real numbers  and  is the identity metric: that is, the
distance between two elements of  is   if they are the same
elements and it is  otherwise.  

Next, we  identify the stopping condition:
\begin{lstlisting}
S(graph,<count,mark>) := count >= w
\end{lstlisting}
Finally, we identify the functions , , 
with the following regions of code:
{\lstinputlisting[language=C]{dijkstra_matching.c}}

We now have to prove that the conditions \Cone, \Ctwo, \Cthree\, and
\Cfour\, hold for the given instantiations. 

\subsection{Proof of the conditions}

\paragraph{\Cone: }
For all ,  is  -Lipschitz and  does not depend on .
The proof of this condition can be done by using standard technical (such as Hoare
triples or abstract interpretation) on the following program.
{\lstinputlisting[language=C]{dijkstra_N.c}}
In an exact semantics (with real numbers), this program is -Lipschitz as any element of  is  the sum of some elements of .
If the analysis is done with an exact semantics (with real numbers), we
are able to prove that this program is -Lipschitz.

\paragraph{\Ctwo: }
The proof for \Ctwo\ is rather technical. The basic idea is however
quite simple. Indeed, the  structure is a set in a discrete space
on which elements are added. So we prove that whatever the order of
the element is  is constant. This is done by showing that local
transpositions do not change the result. So the principle should
apply in other algorithms with the same  structure. 
The complete proof can be found in the appendix of \cite{gazeau12hal}.

\paragraph{\Cthree: }
Since the instantiation of  is \lstinline{count >= w}, 
the stopping condition does not depend on  (when the number of nodes  is fixed).
Hence, the formula is satisfied for  with the constant  and .

\paragraph{\Cfour: }
Since  is a singleton for every  (it corresponds to the state where all the nodes are marked), the property
holds for .

\section{Related Work}\label{sec:related}



Static analysis via abstract interpretation can be an effective method
for deriving precise bounds on deviations
\cite{goubault01sas,goubault11vmcai}.  Since such static analysis is
generally limited to analyzing code line-by-line, significant over
approximations might be necessary.  For example, when encountering an
``if'' instruction (or a looping construct), a static analyzer will
have to assume that either the control flow is not perturbed by the
finite-precision errors (often unrealistic) or the results from the
two branches of the conditional must be merged (often causing
significant over-approximation).  In our examples here, control flow
can be perturbed a great deal by precision errors and merging both
branches is not a solution as the program is not locally continuous.
Our method is useful for solving this problem since it avoids narrowly
analyzing the semantics of the conditional.



In the two papers \cite{chaudhuri11sigsoft,chaudhuri10popl},
robustness analysis is done for the Dijkstra's algorithm. The authors
split their analysis into two parts: first they prove the continuity
of the algorithm and second they prove it is piecewise robust. The
problem of discontinuity that can occur at some point of the execution
is solved through an abstract language syntax for loops.  Like in our
theorem, this syntax need additional conditions (mainly the
commutativity for two observable equivalent commands).  However, their
abstract language is more specific than our theorem: CORDIC is not in
the scope of these papers which also means their conditions are
simpler and their proofs are more directed than ours.  The other
distinction is in the semantics of the language. Their paper aims at
furnishing the whole semantics which is an exact one and computational
errors are treated qualitatively with the argument that a robust
program is not sensitive to small variations. With our analysis, we
give a quantitative definition of what small enough means.  The last
difference is our design for analyzing non-local-robustness.  We prefer
to consider non-local behaviors as happening and solving them by a program
transformation using pattern than to rewrite the program in a syntax
that hide the non-local behavior.


\section{Future work and conclusion}\label{sec:conclude}


We have presented a theorem that allows us to prove the robustness of some
floating point programs.  This theorem is abstract enough to be
applicable in a number of rather different programs: here, we
illustrate its use with programs to compute cosine using the CORDIC
method and to compute the shortest path in a graph. 





For future work, we would like to address a key possible weakness of
our method: it is currently tied to a particular template.  Although
that template is presented abstractly, there should certainly be ways
to improve the generality beyond the matching of a template.  Also,
since the property 
(Definition~\ref{def:k-epsilon-delta}) is more general than both
-Lipschitz and the other definitions of robustness
\cite{majumdar09rtss,majumdar10memocode}, we would like to explore
applications of this property to cases where neither of the other
definitions work.

Condition \Ctwo\ is, at least in the examples considered in this paper,
the most difficult condition to verify.  This suggests that we might
consider more restrictive conditions that would entail \Ctwo.

\paragraph{Acknowledgments:}  We would like to thank Eric Goubault and
Jean Goubault-Larrecq for many useful discussions on the topic of this
paper and for the helpful comments of the anonymous reviewers. 

\bibliographystyle{eptcs}
\bibliography{bib}
\end{document}
