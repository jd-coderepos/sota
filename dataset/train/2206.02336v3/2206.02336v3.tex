\pdfoutput=1


\documentclass[11pt,a4paper]{article}

\usepackage{ACL2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{xspace,mfirstuc,tabulary}

\newcommand{\ex}[1]{{\sf #1}}


\usepackage{microtype}
\usepackage{inconsolata}

\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{makecell}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{parskip}
\usepackage{float}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{bm}
\usepackage{amsopn}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{multibib}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{colortbl}
\usepackage{bbm}
\usepackage{bbding}
\usepackage{tabularx}
\usepackage{natbib}




\definecolor{darkgreen}{RGB}{0,150,0}


\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\Zeqi}[1]{\zeqicomment{[Zeqi: #1]}}
\newcommand{\zeqicomment}[1]{\textcolor{blue}{\ding{46}~{\sf}~#1}}
\newcommand{\SZ}[1]{\shizhuocomment{[Shizhuo: #1]}}
\newcommand{\shizhuocomment}[1]{\textcolor{orange}{\ding{46}~{\sf}~#1}}
\newcommand{\grayline}{\rowcolor[gray]{.90}}
\newcommand{\redcell}[1]{\textcolor{red}{#1}}
\newcommand{\greencell}[1]{\textcolor{green}{#1}}
\newcommand{\bluecell}[1]{\textcolor{blue}{#1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  
\newcommand{\bei}{\textcolor{pink}}



\usepackage[normalem]{ulem} \newcommand\hl{\bgroup\markoverwith
  {\textcolor{yellow}{\rule[-.5ex]{2pt}{2.5ex}}}\ULon}

\usepackage[most]{tcolorbox}

\title{Making Large Language Models Better Reasoners with Step-Aware Verifier}

\author{Yifei Li\Thanks{Work was done during an internship at Microsoft Research Asia.}, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, \\
\textbf{Jian-Guang Lou}\textbf{, Weizhu Chen} \\
 National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University \\
 Microsoft Corporation \\
\small{{\texttt{\{yifeili, zeqi.lin, v-shizzhang, qifu, bei.chen, jlou, wzchen\}@microsoft.com}}} \\
\small{{\texttt{liyifei@stu.pku.edu.cn}}} \\
}






\date{}

\begin{document}
\maketitle
\begin{abstract}



Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from  to  in problem-solving rate. In this paper, we present \textsc{DiVeRSe} (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. \textsc{DiVeRSe} has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate \textsc{DiVeRSe} on the latest language model \emph{code-davinci-002} and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K ).




\end{abstract}

\section{Introduction}



Large pretrained language models (PLMs) have shown remarkable performance on various natural language processing tasks, either by few-shot learning with prompts \cite{radford2019language,le2021many,jin-etal-2022-good} or by fine-tuning \cite{houlsby2019parameter, hu2021lora, he2022towards}. However, despite the increasing size and capacity of PLMs such as GPT-3 with 175B parameters \cite{brown2020language} and PaLM with 540B parameters \cite{chowdhery2022palm}, their reasoning abilities are still limited and often require multiple steps to produce correct answers, especially for tasks involving arithmetic, commonsense, or inductive reasoning \cite{cobbe2021training}.




Recent works \cite{wei2022chain,least2most,fewshotreason2022,lampinen2022can} have demonstrated that PLMs possess some latent reasoning capabilities, but they need carefully designed prompts to activate them. For instance, \citet{wei2022chain} proposed chain-of-thought reasoning, which inserts multi-step reasoning paths before generating the final answers, and achieved significant improvement on the GSM8K arithmetic benchmark \cite{cobbe2021training}. \citet{selfconsistency} further introduced a voting mechanism to select the most consistent answer among different reasoning paths, and achieved state-of-the-art results on several reasoning benchmarks using the PaLM model \cite{chowdhery2022palm}. Building on these successes, this paper continues this line of research and advances the reasoning capabilities of PLMs in three aspects, as illustrated in Figure \ref{fig:overview}.


\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/ours.pdf}
\caption{Our proposed method, \textsc{DiVeRSe} (\textbf{Di}verse \textbf{Ve}rifier on \textbf{R}easoning \textbf{S}t\textbf{e}p).} 
\label{fig:overview}
\end{figure}

\begin{figure}[t]
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Chain-Of-Thought Reasoning for GSM8K Math Word Problem,fontupper=\footnotesize,fonttitle=\scriptsize]
\textbf{Q}: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?

\textbf{A}:  There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.

...

\textbf{Q}: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every
morning and bakes muffins for her friends every day with four. She sells the remainder for \2 * 9 = 74.4\% \to 83.2\%81.9\% \to 88.7\%99.3\% \to 99.8\%86.6\% \to 87.0\%79.5\% \to 94.9\%67.0\%\to 95.9\%M_1M_2M=M_1\times M_2M_1=5M_2=20\mathbf{[CLS]}\mathbbm{1}_{\mathbf{y}_i=\mathbf{y}}\mathbf{y}_i=\mathbf{y}f(\cdot)7\to 9\to 188\alpha\mathcal{L}_0\mathcal{L}_1S_{i,1}, S_{i,2}, ..., S_{i,|S_i|}\mathbf{z}_i\text{label}_{i,j}S_{i,j}f'(\text{input}_i, j)S_{i,j}f'(\text{input}_i, j)S_{i,j}\text{label}_{i,j}^{a}^{b}^{c}^{d}^{e}^{f}^{g}^{h}abcdefgh0.55/7/75201,0001,000\langle\text{question}, \text{answer}\rangle1\times 10^{-5}128\alpha0.0/0.1/0.2/0.31800 each. He gets 1 card that is worth \1000. There are 30 more cards worth an average of \1800 each for a total of \4000 and another card worth \4000 + \5000 \colorbox{green}{(72.1\%)}. There were 30 more cards worth an average of \50 * 30 = 1500 \colorbox{yellow}{(63.7\%)}. Mark made a profit of \1500 = 6500 \colorbox{red}{(40.5\%)}. The answer is 6500.

\textbf{Answer:} \redcell{\textbf{6500 (Wrong)}}\qquad
\textbf{Score:} \colorbox{yellow}{} &

The cheaper cards are worth a total of 30 * 50 = 1500 \colorbox{green}{(84.4\%)}. So all the cards together are worth 1500 + 4000 + 1000 = 6500 \colorbox{green}{(83.3\%)}. The cost for the cards was 1800 * 3 = 5400 \colorbox{green}{(80.1\%)}. So he made a profit of 6500 - 5400 = 1100 \colorbox{green}{(70.0\%)}. The answer is 1100.

\textbf{Answer:} \bluecell{\textbf{1100 (Correct)}}\qquad
\textbf{Score:} \colorbox{green}{} \\
\bottomrule[2pt]
\end{tabular}
}
\caption{A GSM8K example (\textit{code-davinci-002}) with step-level scores given by the step-aware verifier. The scores can not only improve the performance but also help the understanding of where the reasoning paths start to be incorrect.}
\label{tab:step_case}
\end{table*}

\section{Main Results}
\label{section:results}

Table \ref{tab:overall_results} shows the overall experimental results.
We mainly compare \textsc{DiVeRSe} with two baselines: (1) greedily decoding a single reasoning path \cite{wei2022chain}, referred to as \emph{Greedy Decode}; (2) sampling   reasoning paths, then select the final answer via majority voting \cite{selfconsistency}, referred to as \emph{Self-Consistency}.

\subsection{Effectiveness}
Experimental results clearly demonstrate that \textsc{DiVeRSe} can bring significant and consistent improvements over recent strong baselines.
The improvements are across different models (\emph{davinci}, \emph{text-davinci-002} and \emph{code-davinci-002}) as well as different reasoning skills (eight tasks in three reasoning skills).
Taking GSM8K as an example, compared to \emph{Greedy Decoding} and \emph{Self-Consistency}, \textsc{DiVeRSe} brings improvements of  on \emph{davinci},  on \emph{text-davinci-002}, and  on \emph{code-davinci-002}.
Compared to \emph{Self-Consistency}, \textsc{DiVeRSe} achieves average improvements of  on the three reasoning skills, respectively.



\subsection{Comparing to Previous SOTAs}
In Table \ref{tab:overall_results}, we also compare \textsc{DiVeRSe} with:
(1) previous SOTA results based on fine-tuning;
(2) recent SOTA results \cite{wei2022chain} based on PaLM \cite{chowdhery2022palm}, a gigantic language model with 540 billion parameters.\footnote{\textsc{DiVeRSe} can also be applied to PaLM, but PaLM is not publicly available.}

On all the five arithmetic reasoning tasks, \textsc{DiVeRSe} (with \emph{code-davinci-002}) achieves new SOTA results, with an average improvement of .
On the two commonsense reasoning tasks, the performance of \textsc{DiVeRSe} is slightly lower () than that of PaLM-based self-consistency.
We speculate that the reason might be: these two commonsense reasoning tasks are multiple-choice tasks rather than open-ended generation tasks, resulting in more false-positive exemplars in the pseudo exemplar base (Details will be discussed in Section \ref{sec:noise}).
Regarding inductive reasoning, \textsc{DiVeRSe} achieves a surprisingly good performance of  on the CLUTRR task, outperforming ()  previous SOTA result with fine-tuning \cite{sinha2019clutrr}.\footnote{\citet{sinha2019clutrr} also introduced a method with  accuracy. We do not take it into the comparison, as this method requires a domain-specific system with complicated rules to extract a knowledge graph for each input text.}

\section{Case Study}


Table \ref{tab:step_case} shows an example of step-level scores given by the step-aware verifier.
Steps in the correct reasoning path have relatively high scores, while the scores in the wrong reasoning path show where the path starts to be wrong.
This indicates that besides improving the performance, the step-aware verifier can also bring interpretability to show the step-level correctness.
We also show some extra examples of majority-voting in Table \ref{tab:code_davinci_case}.


\section{Analysis}
We also conduct ablation experiments and analysis to investigate the keys to the success of \textsc{DiVeRSe}.

\begin{table}[t]
\renewcommand\arraystretch{1.2}
\small
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lccc}
\toprule[2pt]
Method & GSM8K & CQA & CLUTRR \\
\midrule[1.5pt]
\multicolumn{3}{l}{\underline{davinci:}} \\
 & 18.9 & 57.4 & 42.5 \\
\grayline  & \textbf{21.3} & \textbf{57.5} & \textbf{45.9} \\ \hline
\multicolumn{3}{l}{\underline{text-davinci-002:}} \\
 & 58.2 & 72.9 & 34.9 \\
\grayline  & \textbf{61.3} & \textbf{77.3} & \textbf{35.6} \\ \hline
\multicolumn{3}{l}{\underline{code-davinci-002:}} \\
 & 76.7 & 77.3 & 35.6 \\
\grayline  & \textbf{80.0} & \textbf{78.8} & \textbf{43.8} \\
\bottomrule[2pt]
\end{tabular}
}
\caption{The effectiveness of diverse prompts () compared to pure sampling decoding \cite{selfconsistency}, under majority voting.}
\label{tab:cross_consistency}
\end{table}

\begin{table}[t]
\renewcommand\arraystretch{1.2}
\small
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{lc}
\toprule[2pt]
 & \multicolumn{1}{c}{GSM8K} \\
\midrule[1.5pt]
 & 76.7 \\
\grayline  & \textbf{80.0} \\
 & 79.8 \\
 & 73.0 \\
\bottomrule[2pt]
\end{tabular}
}
\caption{GSM8K majority voting results for different  settings on \emph{code-davinci-002}.}
\label{tab:mix}
\end{table}

\subsection{The Effectiveness of Diverse Prompts}
\label{sec:power_of_diverse_prompts}




By diversifying both prompts and reasoning paths (), we consistently improve performance over the sampling decoding approach () of \citet{selfconsistency}, as shown in Table \ref{tab:cross_consistency}. Both methods use majority voting. Table \ref{tab:mix} further reveals that neither only using diverse prompts nor only using sampling is optimal. In other words, \emph{the best performance is achieved by combining diverse prompts and sampling}. Moreover, Figure \ref{fig:diversity} demonstrates that \textit{diverse prompts lead to more diverse reasoning paths}. We hypothesize that this diversity contributes to the performance improvement by: (1) making correct results more distinguishable from varied errors during inference; and (2) providing more diverse negative samples for enhancing the verifier's generalizability during training.



\begin{figure}[t]
\centering
\subfigure{
\includegraphics[width=3.6cm]{figures/num_of_different_rps.pdf}
\label{fig:num_of_different_rps}
}
\subfigure{
\includegraphics[width=3.6cm]{figures/num_of_different_final_answers.pdf}
\label{fig:num_of_different_final_answers}
}
\caption{Diverse prompts increase the diversity of GSM8K reasoning paths and their final answers. This is beneficial for the voting verifier. Left: the average number of distinct reasoning paths per question (we consider two reasoning paths to be the same if they have the same intermediate result chain as shown in Figure \ref{fig:step}). Right: the average number of distinct final answers per question.}
\label{fig:diversity}
\end{figure}


\subsection{The Effectiveness of Voting Verifier}
\label{sec:power_of_voting_verifier}

We compare three algorithms to conclude the agreement from diverse reasoning paths: majority voting, verifier, and voting verifier.
Table \ref{tab:the_power_of_expectation} shows the results. 
\emph{Compared to majority voting, our voting verifier can significantly and consistently boost reasoning performance across different tasks and different language models}.
Verifier without voting often outperforms majority voting, but extending it to voting verifier can further boost the performance.







\begin{table}[t]
\renewcommand\arraystretch{1.2}
\small
\centering
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lccc}
\toprule[2pt]
Method & \multicolumn{1}{c}{GSM8K} & \multicolumn{1}{c}{CQA} & \multicolumn{1}{c}{CLUTRR} \\
\midrule[1.5pt]
\multicolumn{3}{l}{\underline{davinci:}} \\
Voting & 21.3 & 57.4 & 45.9 \\
Verifier & 27.0 & 74.1 & \textbf{93.2} \\
\grayline Voting Verifier & \textbf{30.6} & \textbf{75.0} & 92.5 \\ \hline
\multicolumn{3}{l}{\underline{text-davinci-002:}} \\
Voting & 61.3 & 77.3 & 35.6 \\
Verifier & 62.7 & 77.9 & \textbf{93.8} \\
\grayline Voting Verifier & \textbf{68.9} & \textbf{79.2} & \textbf{93.8} \\ \hline
\multicolumn{3}{l}{\underline{code-davinci-002:}} \\
Voting & 80.0 & 75.4 & 43.8 \\
Verifier & 65.9 & \textbf{78.8} & \textbf{95.9} \\
\grayline Voting Verifier & \textbf{82.3} & \textbf{78.8} & \textbf{95.9} \\
\bottomrule[2pt]
\end{tabular}
}
\caption{The effectiveness of voting verifier. All exepriments in this table use .}
\label{tab:the_power_of_expectation}
\end{table}

\begin{figure}[t]
\centering
\subfigure[The number of correct reasoning paths containing redundant steps.]{
\includegraphics[width=3.5cm]{figures/step_correct_analysis.pdf}
\label{fig:step_correct_analysis}
}
\subfigure[With the step-aware mechanism, incorrect paths contain more correct steps.]{
\includegraphics[width=3.5cm]{figures/step_incorrect_analysis.pdf}
\label{fig:step_incorrect_analysis}
}
\caption{Human evaluation on GSM8K shows the effectiveness of the step-aware mechanism for verifier.}
\end{figure}



\subsection{The Effectiveness of Step-aware Verifier}
\label{sec:step_level}



We evaluate the impact of incorporating step-level information into the voting verifier of \textsc{DiVeRSe}.
Table \ref{tab:step_level} shows the performance of \textsc{DiVeRSe} with and without the step-aware mechanism on both the GSM8K  and the CommonsenseQA datasets.
We find that \emph{using the step-aware verifier improves the performance in most of the experiments}.
The only exception is \emph{code-davinci-002} on GSM8K, where the step-aware verifier slightly lowers the performance.
We hypothesize that \emph{code-davinci-002} is more capable of generating high-quality reasoning paths, and thus does not benefit much from the step-level information.

\paragraph{Detailed Human Evaluation of Reasoning Steps.}
We further analyze the quality of generated reasoning steps, by asking human annotators to judge whether the GSM8K reasoning steps produced by \textsc{DiVeRSe} (with/without step-aware mechanism) are good or not.
Here ``good'' means not only correct formulas and calculation results but also textual fluency and logical coherence.



We further examine the quality of the reasoning steps generated by \textsc{DiVeRSe} (with/without step-aware mechanism) for GSM8K, by asking human annotators to rate them based on correctness, fluency, and coherence.
For each test question, we compare three reasoning paths produced by \textit{code-davinci-002}: the one with the highest verifier score, the one with the highest step-aware verifier score, and a randomly chosen one.
The annotators (master students) label any incorrect or unsatisfactory reasoning steps in each path (single-blind) and explain why.
We collect annotations for 200 test questions, half of which have correct final answers from all three paths, and half of which have incorrect final answers from all three paths.





We find that \textbf{all the reasoning paths with correct final answers are also correct in every intermediate step}, which shows that \textit{code-davinci-002} can reliably generate accurate reasoning steps, not just lucky guesses.
However, we also find that \textbf{many of the correct reasoning paths have unnecessary steps}.
Figure \ref{fig:step_correct_analysis} shows that  of the random paths have redundant steps, and the verifier can lower this percentage to .
We also find that \textbf{the step-aware verifier can further eliminate redundant reasoning steps} from  to .



\begin{figure}[t]
	\centering
\includegraphics[width=0.98\columnwidth]{figures/step_error_type_analysis.pdf}
	\caption{The distribution of error types in incorrect reasoning steps.}
    \label{fig:step_error_type_analysis}
\end{figure}



Furthermore,  for the incorrect reasoning paths, we find that \textbf{the step-aware mechanism helps produce more correct steps before making mistakes}.
For each failed test question, we compare the number of correct steps in the path with the highest verifier score and the path with the highest step-aware verifier score (by human evaluation).
Figure \ref{fig:step_incorrect_analysis} shows that for / of the failed test cases, the step-aware verifier generates more/fewer correct steps than the verifier without the step-aware mechanism.



\begin{table}[t]
\renewcommand\arraystretch{1.2}
\small
\centering
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lcc}
\toprule[2pt]
 & \multicolumn{1}{c}{GSM8K} & \multicolumn{1}{c}{CommonsenseQA} \\
\midrule[1.5pt]
\multicolumn{3}{l}{\underline{davinci:}} \\
\textsc{DiVeRSe} (without step) & 30.6 & 75.0 \\
\grayline \textsc{DiVeRSe} (with step) & \textbf{30.9} & \textbf{76.0} \\ \hline
\multicolumn{3}{l}{\underline{text-davinci-002:}} \\
\textsc{DiVeRSe} (without step) & 68.9 & 79.2 \\
\grayline \textsc{DiVeRSe} (with step) & \textbf{70.2} & \textbf{79.8} \\ \hline
\multicolumn{3}{l}{\underline{code-davinci-002:}} \\
\textsc{DiVeRSe} (without step) & \textbf{82.3} & 78.8 \\
\grayline \textsc{DiVeRSe} (with step) & 81.5 & \textbf{79.9} \\ \hline
\bottomrule[2pt]
\end{tabular}
}
\caption{The effectiveness of step-aware voting verifier, with .}
\label{tab:step_level}
\end{table}



\paragraph{Step Error Types.} Figure \ref{fig:step_error_type_analysis} shows the distribution of error types in the incorrect reasoning steps.
We see that  of the errors are caused by incorrect formulations (i.e., using wrong intermediate results or operators and generating invalid formulas, which lead to incorrect answers).
We also see that, although \textit{code-davinci-002} often makes division calculation errors (e.g., ), both the verifier and the step-aware verifier can effectively assign low scores to such paths, thus improving the performance.



\subsection{How Many Diverse Outputs Do We Need?}



Figure \ref{fig:diverse_outputs_number} shows the accuracy at different  values, where  is the number of reasoning paths sampled from the  generated paths for each question.
We observe that:
(1) the accuracy increases with more reasoning paths, but the improvement becomes marginal at ;
(2) \textsc{DiVeRSe} outperforms self-consistency significantly and consistently at different  values.

\subsection{How Many Training Data Do We Need?}

\textsc{DiVeRSe} requires a dataset with reasoning paths for training the verifier.
Figure \ref{fig:number_of_exemplars} shows how the size of this dataset affects the performance.
We observe that:
the performance is only reduced by about , even if the size of training data is cut by  (from  to ).
With the same reasoning paths, voting verifier performs better than majority voting, while verifier without voting causes significant performance drops.

\subsection{The Impact of the Number of Exemplars}
We conduct experiments for  ( is the number of exemplars used in each prompt) on GSM8K.
Figure \ref{fig:number_of_in_context_demos} shows the results.
We observe that: \emph{using 8 exemplars in each prompt can further boost the accuracy of GSM8K to .}



\begin{figure}[t]
	\centering
	\hspace*{-0.8cm}
	\includegraphics[width=0.55\textwidth]{figures/num_of_diverse_outputs_jun13.pdf}
	\caption{GSM8K accuracy at different  values (how many reasoning paths are used for each question).}
	\label{fig:diverse_outputs_number}
\end{figure}



\section{Related Work}

\paragraph{Reasoning Skills.}
Researchers in the literature have proposed many benchmarks requiring various reasoning skills, including
commonsense reasoning \cite{zellers-etal-2018-swag,talmor2019commonsenseqa,https://doi.org/10.48550/arxiv.1908.05739,geva2021did}
numerical reasoning \cite{dua-etal-2019-drop}, multi-hop reasoning \cite{yang-etal-2018-hotpotqa}, arithmetic reasoning \cite{koncel2015parsing,roy2015solving,miao2020diverse,patel2021nlp,cobbe2021training}, logical reasoning \cite{https://doi.org/10.48550/arxiv.2007.08124,https://doi.org/10.48550/arxiv.2002.04326}, inductive reasoning \cite{sinha2019clutrr} and tabular reasoning \cite{chen-etal-2020-hybridqa,https://doi.org/10.48550/arxiv.2105.07624}.

\paragraph{Reasoning with Symbolic Systems.}
Much research in the literature enhances the reasoning capabilities of machine learning systems by exploiting symbolic systems,
including knowledge graphs \cite{mihaylov-frank-2018-knowledgeable,bauer-etal-2018-commonsense,kundu-etal-2019-exploiting,10.1609/aaai.v33i01.33017208,lin-etal-2019-kagnet,ding-etal-2019-cognitive,https://doi.org/10.48550/arxiv.2005.00646,wang2022multi-level}, or question taxonomies \cite{dua-etal-2019-drop,andor-etal-2019-giving,hu-etal-2019-multi,wang-etal-2022-logic}.
Although these methods work well on specific benchmarks, they usually require domain-specific designs and human efforts, thus limiting the generalizability.

\paragraph{Reasoning via Language Models.}
This line of work aims to address reasoning tasks in a general sequence-to-sequence manner, empowered by reasoning-aware pre-training or fine-tuning of language models.
For example, \citet{deng-etal-2021-reasonbert} proposed to train the language model with crawled data from the internet;
\citet{asai-hajishirzi-2020-logic} proposed a logic-guided data augmentation method to pre-train the language model; 
\citet{shen2021generate,cobbe2021training} proposed to train a verifier to rank solutions sampled from fine-tuned language models;
\citet{geva-etal-2020-injecting,yoran-etal-2022-turning,campagna-etal-2020-zero,wang-etal-2022-logic} proposed to equip language models with reasoning abilities by generating training examples with human-designed templates;
\citet{pi2022reasoning} proposed to inject reasoning capabilities into language models by continual pre-training on program execution data.

 \begin{figure}[t]
	\centering
	\includegraphics[width=0.4\textwidth]{figures/repo_size_lyf.pdf}
	\caption{\textsc{DiVeRSe} performance (\emph{code-davinci-002}) on GSM8K with different sizes of the training dataset (without labeled reasoning paths).}
	\label{fig:number_of_exemplars}
\end{figure}

\paragraph{Reasoning via Prompting Gigantic Language Models.}
Gigantic language models like GPT-3 \cite{brown2020language} have demonstrated impressive few-shot learning capabilities in many tasks and have attracted many research interests on making gigantic language models better few-shot learners \cite{https://doi.org/10.48550/arxiv.2102.09690,https://doi.org/10.48550/arxiv.2104.08315,https://doi.org/10.48550/arxiv.2110.15943,liu-etal-2022-makes,https://doi.org/10.48550/arxiv.2104.08786,https://doi.org/10.48550/arxiv.2112.08633,https://doi.org/10.48550/arxiv.2202.12837}.
However, these methods struggle to address tasks requiring reasoning skills.
To mitigate this, recently there is a line of research that focuses on unleashing the reasoning capabilities of gigantic language models via better prompting strategies.
\citet{wei2022chain} proposed \emph{chain-of-thought reasoning}, of which the key insight is the insertion of multi-step reasoning paths before generating the final answers;
\citet{selfconsistency} proposed to improve chain-of-thought reasoning via \emph{self-consistency}, of which the key insight is to conclude the most consistent answer from different reasoning paths sampled from the language model;
\citet{least2most,https://doi.org/10.48550/arxiv.2205.09712} proposed to leverage gigantic language models to decompose questions into sub-questions, thereby addressing them in an iterative manner;
\citet{fewshotreason2022} proposed that gigantic language models can even be good zero-shot reasoners, by designing prompts that can induce language models to do reasoning step-by-step;
\citet{lampinen2022can} proposed building a prompt by selecting examples and explanations together, thus substantially improving performance over selecting examples alone.
Despite their great successes, these works come with their limitations.
This paper is a continuation of this line of research, focusing on diverse verifier on reasoning steps.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.4\textwidth]{figures/num_of_exemplars.pdf}
	\caption{\textsc{DiVeRSe} performance (\emph{code-davinci-002}) on GSM8K when each prompt contains  exemplars.}
	\label{fig:number_of_in_context_demos}
\end{figure}

\section{Conclusion and Future Work}


In this paper, we present \textsc{DiVeRSe}, a novel and general method to enhance the reasoning abilities of large language models. Our method builds on the idea of prompting language models with multi-step reasoning paths, but introduces three key innovations: diverse prompts, voting verifier, and stepwise verifier. The latter is especially novel and effective, as it verifies each reasoning step separately and we provides a detailed analysis of the model's behavior in each step. We demonstrate the superiority of \textsc{DiVeRSe} through extensive experiments. For instance, using \emph{code-davinci-002}, our method achieves state-of-the-art performance on most reasoning tasks, surpassing the 540B PaLM model with previous prompting techniques.

There are many directions for our future work.
(1) As discussed in Appendix \ref{sec:noise}, we will continue to investigate how to reduce or recognize false positive pseudo exemplars.
(2) We plan to investigate mechanisms to produce better diverse prompts than simple sampling.
(3) We will extend \textsc{DiVeRSe} to other tasks and continue to design better prompting techniques to elicit the power of gigantic language models. 

\section{Limitations}

\paragraph{Computing Resources.} Despite the surprising performance it achieves, our framework needs to be applied to large language models like GPT-3 or PaLM. Inference with these models costs more time and budgets than fine-tuning models like RoBERTa \cite{liu2019roberta}.

\paragraph{Faithfulness.} Although \textsc{DiVeRSe} can significantly improve the accuracy of final answers, we still cannot guarantee that the reasoning paths produced by the language models are 100 percent faithful.
This is the key challenge and future direction for this line of research (chain-of-thought reasoning).

\paragraph{More Training Data.} \textsc{DiVeRSe} needs more labeled data with well-annotated reasoning paths to construct diverse prompts, and it also needs a training dataset for supervising the verifier.
However, from another point of view, this limitation can also be regarded as a contribution that studies how chain-of-thought reasoning can be further improved if we have more training data than just a few exemplars.


\paragraph{Human Evaluation of Reasoning Steps.} We use human evaluation to measure the quality of the intermediate steps in reasoning paths since few current works provide reliable frameworks to evaluate the quality of reasoning steps.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage


\appendix
This is the Appendix for the paper: ``On the Advance of Making Language Models Better Reasoners''.



\section{Preliminaries}

\paragraph{Prompting.}

Prompting means prepending a few exemplars to the task input  and
generating the output  from the pretrained language model:

where  is the concatenation of  exemplars:

We denote \textbf{prompt} as the concatenation of the exemplars  and the input .

\paragraph{Reasoning Paths.}

For reasoning tasks that aim to generate an answer  for a question , \citet{wei2022chain} proposed the insertion of a reasoning path  before generating the answer :

where  is a text \textbf{reasoning path} of how the answer  is reasoned step-by-step for question .

Then, during inference, a reasoning path  will be generated before the answer :


Figure \ref{fig:background} demonstrates this idea in arithmetic reasoning (GSM8K), and Table \ref{tab:dataset_exemplars} demonstrates this idea in commonsense reasoning (StrategyQA) and inductive reasoning (CLUTRR).

\begin{figure}[t]
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Prompt \& Samples,fontupper=\footnotesize,fonttitle=\scriptsize]
\textbf{Q}: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?

\textbf{A}:  There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.

...

\textbf{Q}: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every
morning and bakes muffins for her friends every day with four. She sells the remainder for \2 * 9 = 2 = 2 = \\triangleright\trianglerightNNEKM_1M_1|E| < K * M_1\langle\text{question}, \text{answer}\rangleD(\mathbf{x}, \mathbf{y}^*)E(\mathbf{x,y^*})\in D\mathbf{z}\mathbf{y}E'E\cup E'-0.8\%3.0\%2,2902,290T_1T_2T_1T_250\%\langle e_1, r, e_2\ranglere_1e_2\langle \text{Mary}, \text{husband}, \text{Roy}\rangle\langle \text{Ernest}, \text{father}, \text{Roy}\ranglee_2re_133326238$ \\
\bottomrule[2pt]
\end{tabular}
}
\caption{Examples of \textit{code-davinci-002} on GSM8K.
Compared to \textit{self-consistency} (majority voting), \textsc{DiVeRSe} can select the correct-but-not-most answer out of the sampled candidates, thus improving the reasoning performance.}
\label{tab:code_davinci_case}
\end{table*}






















\end{document}
