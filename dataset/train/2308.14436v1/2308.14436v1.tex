\documentclass[sigconf,natbib=true,anonymous=False]{acmart}
\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[CIKM '23]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}






\usepackage{amsmath,multirow,booktabs,tabularx}
\usepackage{array} 
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{float}
\usepackage{anyfontsize}
\usepackage{color} \usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}  
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{parskip}
\usepackage{multirow}
\usepackage{bigstrut}

\def\mc{\mathcal}
\def \green#1{\textcolor[HTML]{16aa09}{#1}}
\def\bs{\boldsymbol}












\begin{document}

\title{Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA}
\author{Guanting Dong}
\authornote{Work done during internship at Meituan Inc. The first two authors contribute equally.}
\email{dongguanting@bupt.edu.cn}


\affiliation{\institution{Beijing University of Posts and Telecommunication, China}
}


\author{Rumei Li}
\authornotemark[1]
\email{lirumei@meituan.com}
\affiliation{\institution{Meituan Group, Beijing, China}
}

\author{Sirui Wang}
\email{wangsirui@meituan.com}
\affiliation{\institution{Meituan Group, Beijing, China}
}

\author{Yupeng Zhang}
\email{G0vi_qyx@buaa.edu.cn}
\affiliation{\institution{Beijing University of Aeronautics and Astronautics}
}


\author{Yunsen Xian}
\email{xianyunsen@meituan.com}
\affiliation{\institution{Meituan Group, Beijing, China}
}


\author{Weiran Xu}
\email{xuweiran@bupt.edu.cn}
\authornote{Weiran Xu is the corresponding author}


\affiliation{\institution{Beijing University of Posts and Telecommunication, China}
}
















\renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}
Knowledge Base Question Answering (KBQA) aims to answer natural language questions with factual information such as entities and relations in KBs. However, traditional Pre-trained Language Models (PLMs) are directly pre-trained on large-scale natural language corpus, which  poses challenges for them in understanding and representing complex subgraphs in structured KBs. To bridge the gap between texts and structured KBs, we propose a \textbf{S}tructured \textbf{K}nowledge-aware \textbf{P}re-training method (SKP). In the pre-training stage, we introduce two novel structured knowledge-aware tasks, guiding the model to effectively learn the implicit relationship and better representations of complex subgraphs. 
In downstream KBQA task, we further design an efficient linearization strategy and an interval attention mechanism, which assist the model to better encode complex subgraphs and shield the interference of irrelevant subgraphs during reasoning respectively.
Detailed experiments and analyses on WebQSP verify the effectiveness of SKP, especially the significant improvement in subgraph retrieval (+4.08\% H@10).
\end{abstract}
\keywords{KBQA, Structured Knowledge, Pre-training, Efficient Linearization, Interval Attention Mechanism}
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Understanding multi-modal content~natural language processing}
\ccsdesc[300]{Understanding multi-modal content~knowledge extraction}
\ccsdesc{Understanding multi-modal content~knowledge representations}
\ccsdesc[300]{Information access and retrieval~question answering and dialogue systems}
\ccsdesc[100]{Information access and retrieval~retrieval models}

\ccsdesc[300]{Integration and aggregation~knowledge graphs}


\keywords{Structured Knowledge-aware Pre-training for Question Answering over Knowledge Bases}
\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\section{Introduction}

Knowledge Base Question Answering (KBQA)  aims to seek answers to factoid questions from structured KBs \cite{auer2007dbpedia,bollacker2008freebase}. The existing methods to solve KBQA can be divided into two categories: Semantic parsing-based (SP-based) methods \cite{ye2021rng,das2022knowledge,lan2020query,sun2020sparqa} and embedding-based methods \cite{agarwal-etal-2021-knowledge,zhang2022subgraph,he2021improving,sun2020faithful,sun2018open,sun2019pullnet}. The former one heavily relies on the expensive annotation of the intermediate logic form such as SPARQL \cite{perez2009semantics}. Instead of parsing the questions, the latter one directly encodes and retrieves the candidate subgraphs, then obtains answers by a ranker or a generator \cite{saxena2022sequence,oguz2020unik,yu2022decaf}. Therefore, embedding-based framework is more suitable for the realistic dialogue system, which has gained attention in the research field.

Unfortunately, the existing embedding-based methods still encoun-\\
ter challenges in understanding and reasoning with structured knowledge. Traditional KBs, such as Freebase, typically comprise sets of subject-predicate-object (SPO) triplets. However, the PLMs are pre-trained on unstructured natural language texts, making it difficult to encode the implicit logical relationship in the structured subgraphs \cite{yang2020improving}. Moreover, to improve the efficiency, existing methods directly concatenate all the retrieved subgraphs during answer generation, which inevitably causes different types of subgraphs to interfere with each other in the encoding process \cite{de2022fido,yu-etal-2022-kg}.


\begin{figure*}
    \centering
    \resizebox{0.92\textwidth}{!}{
    \includegraphics{figures/kbqav3.pdf}}
    \vspace{-0.3cm}
    \caption{The overall architecture of our proposed SKP}
    \label{fig:main}
    \vspace{-0.2cm}
\end{figure*}
Previous works \cite{bordes2013translating,ji2015knowledge,sun2018open,sun2019pullnet,ju2022grape} primarily addresses these challenges by introducing external text corpus and specially designed framework to incorporate information from the documents. However, the required external resources may be hard to collect in practice. 
Besides, another branch of case-based methods \cite{das2022knowledge,ye2021rng,das2021case,yu-etal-2022-kg} integrate training cases or candidate subgraphs to prompt the model for reasoning. Nevertheless, these methods easily trigger additional noise and are limited by the data quality.
Therefore, it is essential to develop a method for PLMs to improve its ability of understanding structured knowledge and anti-interference, which facilitates techniques of KBQA to be widely applied to the realistic question answering system.

To address the above limitations, we propose \textbf{S}tructured \textbf{K}nowledge-aware \textbf{P}re-training method (SKP) to enhance the model's ability of understanding structured knowledge and encoding complex subgraphs. Inspired by masked language modeling (MLM) \cite{devlin2018bert} and contrastive learning \cite{chen2020simple}, we introduce two novel structured knowledge-aware pre-training tasks according to the characteristics of the data in subgraphs. These tasks effectively facilitate the learning of implicit relationships and representations in structured knowledge.
In the downstream KBQA task, we follow the previous retriever-reader framework, including subgraphs retrieval and answer generation. What distinguishes our approach is the introduction of an effective linearization strategy that significantly reduces the number of candidate subgraphs in the KB while preserving the structural semantic information. Furthermore, we initialize the retriever with pre-trained parameters to facilitate the transfer of upstream learned structured knowledge to the downstream model.  To better encode different subgraphs during reasoning, motivated by \citeauthor{liu2020k}, we design an interval attention mechanism, effectively guiding the model to shield the interference from irrelevant subgraphs. Our contributions can be summarized as follows: 

(1) We propose two novel structured knowledge-aware pre-training tasks to enhance the learning of implicit relationships and representations in subgraphs for KBQA

(2) We design an efficient linearization strategy and an interval attention mechanism to improve the model's ability to encode complex subgraphs and mitigate interference from irrelevant subgraphs during reasoning.

(3) Experiments and analyses on WebQSP show the effectiveness of SKP, especially huge improvements in the subgraph retrieval. Our source codes and datasets are available at Github\footnote{https://github.com/dongguanting/Structured-Knowledge-aware-Pre-training-for-KBQA} for further comparisons.









\section{Method}
\subsection{Problem Definition}
In this paper, we mainly focus on the embedding-based KBQA. Given a query , we first annotate the named entities in the query and link them to the nodes in the KB. Then some heuristic algorithm is applied to retrieve a set of query specified subgraphs , where  and  denote the entity set and the relation set respectively. Our task is to figure out the answers  to the query  from retrieved subgraphs . 

\subsection{Structured Knowledge-aware Pre-training}
The performance of KBQA depends heavily on understanding the implicit relationship of subgraphs . To further enhance the relation learning of PLMs, we propose two novel pre-training tasks, which are shown in Figure \ref{fig:main}(left).

\textbf{Knowledge-aware MLM (KM).} We follow the design of masked language modeling (MLM) in BERT \cite{devlin2018bert}. Given a subgraph , we linearize  in the form of natural languages, which can be formulate as "".
For each subgraph , we randomly replace one of the subject , relation  or object  with the special [MASK] symbol, and then try to recover them. It's worth noting that if the masked entity consists of multiple tokens, all of the component tokens will be masked.
The purpose of our design is to guide the PLMs to learn the implicit relationship between triplets. Hence, the loss function of the MLM is:

where  is the total number of masked tokens and  is the predicted probability of the token  over the vocabulary size.

\textbf{Knowledge Contrastive Discrimination (KCD).} To better discriminate different subgraph representations in semantic space, we introduce Knowledge Contrastive Discrimination. Specifically, given a subgraph , for postive samples, we randomly replace one of the subject , relation  or object  with the special [MASK] symbol. For negative samples, we adopt other samples in the batch. We use , ,  to indicate the representations of the original, the positive, and the negative input samples respectively. Meanwhile, we employ InfoNCE \cite{aitchison2021infonce} objective by bringing original samples  closer to their semantically similar positive samples  and further away from negative samples . We formulate  as follows:

where  denotes the number of total examples in the batch.  is a temperature hyperparameter and  is cosine similarity. Hence, we obtain the overall loss function L of our joint pre-training tasks:

where  and  denote the loss functions of the two tasks. In our experiments, we set .
\subsection{Downstream KBQA task}
We adopt a retriever-reader architecture, with dense passage retriever (DPR) 
 \cite{karpukhin2020dense} as retriever and fusion-in-decoder (FiD) \cite{izacard2020leveraging} as our reader. The overall architecture is illustrated in Figure \ref{fig:main}.
\begin{figure}[t]
\centering

\resizebox{.40\textwidth}{!}{\includegraphics{figures/kbqa_linear.pdf}}
\caption{Knowledge base linearization. We show examples of how we linearize valina triplets and complex triplets (multiple entities and relations with a central CVT node).}
\vspace{-0.2cm} 
\label{fig:linear}
\vspace{-0.2cm} 
\end{figure}


\textbf{Linearization.} Traditional KBs, which contains triplets composed of subject, predicate and object, are usually stored in Resource Description Format (RDF). For the vanilla triples which occupies 90\% of the total number of triples in KB, previous works \cite{das2022knowledge,yan2021large,yu2022decaf} simply flattens each triple without any merging and pruning. Different from them, our linearization strategy can be divided into 6 cases: (1) same subject and predicate, (2) same subject and object (3) same predicate and object (4) same subject (5) same predicate, and (6) same object. To preserve more information when converting all vanilla triples into subgraphs, we sequentially traverse the entire KB and prune triples based on the priority order of cases 1 to 6. We use 100 tokens as the threshold for converting and ensure that hard truncation does not occur. Each subgraph typically contains 2-3 vanilla triples . For complex(n-ary) triplets, freebase uses compound value types (CVTs) to convert a n-ary relation into multiple standard triplets. To better represent them, we convert a n-ary relation into a single sentence by forming a comma-separated clause for each predicate (Figure \ref{fig:linear}). In this way, linearized subgraphs preserve structural semantic information while greatly reduce the number of candidate subgraphs in KB (from 230 million to 112 million).






\textbf{Retriever.} Dense Passage Retriever (DPR) applies two fine-tuned BERTs \cite{devlin2018bert} to encode passages and questions respectively, and then retrieves the top-k passages based on the dot-product similarity. It further utilizes FAISS \cite{johnson2019billion} to conduct an similarity search. Through this step, we can retrieve  passages which are potentially relevant to the input question. We follow the standard setup of DPR for fine-tuning. One difference is that we initialize the retriever with pre-trained parameters, which transfer the upstream learned structured knowledge into downstream retriever. Detailed hyperparameter settings can be found in the implementations.









\textbf{Reader.} The Fusion-in-Decoder\cite{izacard2020leveraging} reader has demonstrated to be strongly effective in fusing information from a large number of documents. However, previous works directly perform cross-attention over concatenation of retrieved passages and questions, which inevitably causes retrieved subgraphs to interfere with each other in encoding process \cite{de2022fido}. To alleviate the above problem, we introduce \textbf{Interval Attention Mechanism (IAM)}. As shown in Figure \ref{fig:main}(d), through setting 1 or 0 for the cross-attention matrix, we make input question and all retrieved subgraphs visible to each other, and different types of subgraphs invisible to each other. We utilize the FiD with T5-large \cite{raffel2020exploring} and further integrate IAM into FiD encoder. We follow the original setting for all experiments which are introduced in implementation details.







\begin{table}[!tbp]
\centering
  \resizebox{0.45\linewidth}{!}{
  \begin{tabular}{l|c}
    \toprule
    \multirow{1}[2]{*}{\textbf{Methods}} & \multicolumn{1}{c}{\textbf{Hits@1}} \\
    \midrule
    GraftNet \cite{sun2018open} & 69.5 \\    
    PullNet \cite{sun2019pullnet} & 68.1 \\
    EMQL \cite{sun2020faithful}& 75.5\\
    Bert-KBQA \cite{yan2021large} & 72.9 \\
    NSM \cite{he2021improving} & 74.3 \\
    KGT5 \cite{saxena2022sequence} & 56.1 \\
    SR-NSM \cite{zhang2022subgraph} & 69.5 \\
    CBR-SUBG \cite{das2022knowledge} &72.1 \\
    EmbededKGQA \cite{saxena2020improving}& 72.5 \\
    DECAF(Answer only) \cite{yu2022decaf} &  74.7 \\
    UniK-QA \cite{oguz2020unik} &  77.9 \\
    \midrule
   \textbf{SKP} (Ours)& \textbf{79.6} \\
    \hspace{0.5cm} w/o IAM & 79.3 \\
    \hspace{0.5cm} w/o KCD & 79.1 \\
    \hspace{0.5cm} w/o KM & 78.7 \\
   \hspace{0.5cm} w/o KM+KCD& 78.4 \\
   \hspace{0.5cm} w/o KM+KCD+IAM& 77.9 \\
    \bottomrule
    \end{tabular}
    }
\caption{Experimental results on WebQSP dataset. “w/o" denotes the model performance without a specific module. * denotes the reproduced result.}
    
  \label{tab:main}\vspace{-0.6cm}
\end{table}

\section{Experiments}
\textbf{Datasets and Metric.} To evaluate the effectiveness of our approach, we conduct several experiments on WebQuestionsSP \cite{yih2016value} dataset which contains 4737 questions that are answerable using Freebase. We follow \citeauthor{sun2018open} and utilize 2848/250/1639 examples for training, validation, and test respectively. 
For QA performance, we use Hits@1 as our evaluation metric, the percentage of the model’s top-predicted answer being a “hit” (exact match) against one of the gold-standard answers.

\textbf{Implementation Details.} For the upstream structured knpwledge-aware pre-training, we use BERT-base-uncased \cite{devlin2018bert} from HuggingFace as backbone. In two pre-training settings, we set the batch size of BERT to 8 and the pre-training takes an average of 12 hours for 5 epochs. The corresponding learning rates are set to 1e-5. For the pre-training corpus, We randomly selected 1 million linearized subgraphs from Freebase as training data, which means no external KBs will be introduced in the pre-training stage. In order to align with previous work, we use STAGG \cite{yih2015semantic} as entity linking system to narrow down the search to a high-recall 2-hop neighborhood of the retrieved entities for each question. Then we convert KB relations in the 2-hop neighborhood into text, retrieve the most relevant ones using DPR to form 100 context passages, and feed them into the T5 FiD reader. We also align the hyperparameter settings of DPR and FID with UniK-QA \cite{oguz2020unik}. We will release code after the blind review. 

\textbf{Baselines.}
We mainly compare SKP with multiple state-of-the-art baselines
that use embedding-based method for KBQA as follow:
\textbf{GraftNet} \cite{sun2018open} 
, \textbf{PullNet} \cite{sun2019pullnet}
, \textbf{Bert-KBQA} \cite{yan2021large} 
, \textbf{EmbedKGQA} \cite{saxena2020improving}
, \textbf{NSM} \cite{he2021improving} 
, \textbf{SR-NSM} \cite{zhang2022subgraph}
, \textbf{EmQL} \cite{sun2020faithful} 
, \textbf{KGT5} \cite{saxena2022sequence} 
, \textbf{CBR-SUBG} \cite{das2022knowledge} 
, \textbf{UniK-QA} \cite{das2022knowledge} 
, \textbf{DeCAF} \cite{yu2022decaf}
, \textbf{DPR} \cite{karpukhin2020dense}. Specifically, for DPR, there are four variants as follows: (1) \textbf{DPR(NQ)}: DPR trained on training set of NQ dataset. (2) \textbf{DPR(NQ-adv)}: DPR trained on training set of NQ dataset and adversarial hard negatives samples. (3) \textbf{DPR(Multi)}: DPR trained on training set of Multi-dataset.(4) \textbf{DPR(WebQ)}: DPR trained on training set of WebQ dataset. It is worth noting that \citeauthor{oguz2020unik} has opened all the DPR variants. More details can be found in the original article and github of DPR.
















\subsection{Main Result}
Table \ref{tab:main} shows the main results compared to different embedding-based baselines on the WebQSP.  Results show that our proposed SKP greatly outperforms all the baselines on Hit@1,  especially 1.7\% higher than the SOTA method (UniK-QA). Besides, SKP has a significant performance degradation when removing both KM and KCD (79.5 to 78.4), showing that structured knowledge-aware pre-training tasks benefit the gap between natural language texts and structured subgraphs.



\begin{table}[t]
  \centering
  \resizebox{0.9\columnwidth}{!}{
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}}&\multicolumn{3}{c}{WebQSP} \\
    \cmidrule(lr){2-5}
    &Hits@10& Hits@20 & Hits@50 & Hits@100 \\
    \midrule
    \multicolumn{1}{c}{\textit{Baselines}} \\
    Bert-KBQA&35.91&48.74&66.9&77.03 \\
    DPR(NQ) &80.96& 86.21 &91.52 & 93.11 \\
    DPR(NQ-adv)&80.93& 80.64 &91.76 & 93.10 \\
    DPR(Multi)&87.22 &91.64 &93.95 &94.68 \\
    DPR(WebQ)&87.46 &91.92 &94.11 &94.87 \\
    UniK-QA &87.50 &- &- &95.85 \\
    \midrule
    \multicolumn{1}{c}{\textit{Our implementations}} \\
    SKP(NQ)&91.23 &93.35 &95.64 &96.32 \\
    SKP(WebQ) &\textbf{91.58} &\textbf{93.65} &\textbf{95.85} &\textbf{96.64} \\
    \bottomrule


   \vspace{-0.2cm} 
  \end{tabular}}
  \caption{The subgraph retrieval results on WebQSP test set. The content in parentheses is the specific dataset fine-tuned by retriever. We contacted \citeauthor{oguz2020unik} to get the retrieval result of UniK-QA.}
  \label{tab:retrieve}
  \vspace{-0.6cm}
  
\end{table}
\textbf{Ablation Studies.} We conduct an ablation study in table \ref{tab:main} to investigate the characteristics of the main components in SKP.  We can observe that: 1) the performance of SKP drops when removing any component, which suggests that every part of design is necessary. 2) The joint training of two pre-training tasks can promote each other, and even training them individually (79.2\% and 78.9\%) can outperform all the baselines by a certain margin. 3) The SKP only with interval attention mechanism has a positive effect as well (from 77.9 to 78.4), which proves the necessity of removing interference of subgraphs in the generation stage.


\textbf{Retrieval experiment.} To validate the effectiveness of SKP on understanding structured knowledge, we evaluate the subgraph retrieval results on WebQSP. We initialize our retriever with pre-trained parameters and fine-tune on the training samples of specific dateset. As shown in Table \ref{tab:retrieve}, our method far exceeds all the baseline methods in all settings (+4.08\% H@10). To align with the UniK-QA, we also fine-tune our method on the NQ dataset, which still outperforms the best baseline by 3.73\%(H@10) and 0.47\%(H@100). The results fully verify the effectiveness of our approach.


\textbf{Results over the Incomplete KB.} To further explore the robustness of our approach over the incomplete KB, we randomly remove 50\% of the KB facts in the retrieved subgraphs and conduct experiments on this incomplete version of the WebQSP dataset. The results are illustrated in Figure \ref{fig:50kb}. We can observe that: 1) Our approach consistently outperforms other baselines under both the full KB and the 50\% KB settings. 
2) With 50\% KB, adding structured knowledge-aware pre-training tasks and interval attention mechanism achieve more performance gain than with full KB (+1.6 vs +1.1/ + 0.9 vs +0.2), demonstrating the generalization ability of SKP in low-resource scenarios.
\begin{figure}[t]
\centering
\resizebox{.45\textwidth}{!}{\includegraphics{figures/50KB_v8.png}}
\vspace{-0.3cm} 
\caption{The performance comparison with the full KB and the 50\% KB. "w." denotes the SKP with specific component.}
\label{fig:50kb}
\vspace{-0.3cm} 
\end{figure}








\begin{figure}[t]
    \centering
    \subfigure[Baseline]{
        \includegraphics[width=.22\textwidth]{figures/output_unmask.pdf}
        \label{label_for_cross_ref_1}
    }
    \subfigure[SKP]{
	\includegraphics[width=.22\textwidth]{figures/output_mask.pdf}
        \label{label_for_cross_ref_2}
    }
    \vspace{-0.4cm}
    \caption{Visualization of the attention mechanism for FiD. }
    \label{fig:visual}
    \vspace{-0.2cm}
\end{figure}


\textbf{Visualization.} To gain an insight into the effectiveness of the interval attention mechanism, we visualize the cross attention weights of FiD encoder, as shown in Figure \ref{fig:visual}. Specifically, we visualize the cross attention matrix of the baseline method (Unik-QA) and SKP in 10 * 10 matrix dimension. Compared with baseline method, it is obvious that high cross attention weights are basically distributed on the diagonal of the matrix, which means that our individual subgraph is more inclined to interact with itself. This finding confirms that our framework can effectively shield the interference of irrelevant subgraphs during reasoning.

\section{Conclusion}
In this paper, we propose a \textbf{S}tructured \textbf{K}nowledge-aware \textbf{P}re-training method (SKP). Specifically, we present two novel pre-training tasks, for effectively learning the implicit relationship and better representation of complex subgraphs. In downstream KBQA task, we further design an efficient linearization strategy and an interval attention mechanism, which assist the model to better encode complex subgraphs and shield the interference of irrelevant subgraphs during reasoning. Detailed experiments and analyses on WebQSP verify the effectiveness of SKP, especially the significant improvements in subgraph retrieval (+4.08\% H@10).









\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}




















\end{document}
