\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{todonotes}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{amsmath, calc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{minitoc}
\usepackage{xcolor,colortbl}

\usepackage{adjustbox}
\usepackage{microtype} \usepackage[bottom]{footmisc}
\usepackage{caption}
\usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{multirow}
\usepackage{amsthm}
\usepackage{color}
\usepackage{MnSymbol}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{textcomp}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{csquotes}
\usepackage{xspace}
\newtheorem{theo}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newcommand{\abbr}[0]{DANN+ELS\xspace}
\newcommand{\ls}[0]{ELS\xspace}
\usepackage{url}
\newcommand{\yf}[1]{\textbf{\color{red}{ [Yi-Fan: #1]}}}
\newcommand{\fsl}[1]{{\centernot{#1}}}
\setlength{\fboxsep}{0pt}
\setlength{\fboxrule}{.1pt}
\newcommand{\Gray}[0]{\rowcolor{gray!20}}
\newcommand{\myref}[1]{Equ. (\ref{#1})}
\newcommand{\etc}[0]{\textit{etc}}
\newcommand{\wrt}[0]{\textit{w.r.t.},}
\newcommand{\ie}[0]{\textit{i.e., }}
\newcommand{\eg}[0]{\textit{e.g., }}
\newcommand{\etal}[0]{\textit{et al., }}
\newcommand{\aka}[0]{\textit{aka., }}
\newcommand{\iid}[0]{\textit{i.i.d}}
\newcommand{\di}[0]{DI } \newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\D}{\mathcal{D}}
\newcommand*{\circled}[1]{\lower.7ex\hbox{\tikz\draw (0pt, 0pt)circle (.5em) node {\makebox[1em][c]{\small #1}};}}
\title{Free Lunch for Domain Adversarial Training: Environment Label Smoothing}





\author{Yi-Fan Zhang\thanks{Work done during an internship at Alibaba Group.}, Xue Wang, Jian Liang,
 \\ \textbf{Zhang Zhang, Liang Wang, Rong Jin\thanks{Work done at Alibaba Group, and now affiliated with Twitter.}, Tieniu Tan} \\
National Laboratory of Pattern Recognition (NLPR), Institute of Automation\\
School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS) \\
 Machine Intelligence Technology, Alibaba Group. 
} 



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
A fundamental challenge for machine learning models is how to generalize learned models for out-of-distribution (OOD) data. Among various approaches, exploiting invariant features by Domain Adversarial Training (DAT) received widespread attention. Despite its success, we observe training instability from DAT, mostly due to over-confident domain discriminator and environment label noise. To address this issue, we proposed \textbf{E}nvironment \textbf{L}abel \textbf{S}moothing (\ls), which  
encourages the discriminator to output soft probability, which thus reduces the confidence of the discriminator and alleviates the impact of noisy environment labels. We demonstrate, both experimentally and theoretically, that \ls can improve training stability, local convergence, and robustness to noisy environment labels. By incorporating \ls with DAT methods, we are able to yield the state-of-art results on a wide range of domain generalization/adaptation tasks, particularly when the environment labels are highly noisy. The code is avaliable at https://github.com/yfzhang114/Environment-Label-Smoothing.





\end{abstract}

\vspace{-0.1cm}
\section{Introduction} 
\vspace{-0.1cm}
Despite being empirically effective on visual recognition benchmarks~\citep{russakovsky2015imagenet}, modern neural networks are prone to learning shortcuts that stem from spurious correlations~\citep{Geirhos_2020}, resulting in poor generalization for out-of-distribution (OOD) data. A popular thread of methods, minimizing domain divergence by Domain Adversarial Training (DAT)~\citep{ganin2016domain}, has shown better domain transfer performance, suggesting that it is potential to be an effective candidate to extract domain-invariant features. Despite its power for domain adaptation and domain generalization, DAT is known to be difficult to train and converge \citep{roth2017stabilizing,jenni2019stabilizing,arjovsky2017towards,sonderby2016amortised}. 

\setlength{\columnsep}{8pt}
\begin{wrapfigure}{r}{0.3\textwidth}
  \begin{center}
  \advance\leftskip+1mm
  \renewcommand{\captionlabelfont}{\footnotesize}
    \vspace{-0.39in}  
    \includegraphics[width=0.25\textwidth]{imgs/teaser.pdf}
    \vspace{-0.06in} 
    \caption{\textbf{A motivating example} of \ls with 3 domains on the VLCS dataset.}
        \label{fig:teaser}
  \end{center}
  \vspace{-0.16in} 
\end{wrapfigure}

The main difficulty for stable training is 
to maintain healthy competition between the encoder and the domain discriminator. Recent work seeks to attain this goal by designing novel optimization methods~\citep{acuna2022domain,rangwani2022closer}, however, most of them require additional optimization steps and slow the convergence. In this work, we aim to tackle the challenge from a totally different aspect from previous works, \ie the environment label design. 




Two important observations that lead to the training instability of DAT motivate this work: 
(i) The environment label noise from environment partition~\citep{creager2021environment} and training~\citep{thanh2019improving}. As shown in~\figurename~\ref{fig:teaser}, different domains of the VLCS benchmark have no significant difference in image style and some images are indistinguishable for which domain they belong. Besides, when the encoder gets better, the generated features from different domains are more similar. However, regardless of their quality, features are still labeled differently. As shown in~\citep{thanh2019improving,brock2018large}, \textit{discriminators will overfit these mislabelled examples and then has poor generalization capability}. (ii) To our best knowledge, DAT methods all assign one-hot environment labels to each data sample for domain discrimination, where the output probabilities will be highly confident. For DAT, \textit{a very confident domain discriminator leads to highly oscillatory gradients}~\citep{arjovsky2017towards,mescheder2018training}, which is harmful to training stability. The first observation inspires us to force the training process to be robust with regard to environment-label noise, and the second observation encourages the discriminator to estimate soft probabilities rather than confident classification. To this end, we propose \textbf{E}nvironment \textbf{L}abel \textbf{S}moothing (\ls), which is a simple method to tackle the mentioned obstacles for DAT. Next, we summarize the main methodological, theoretical, and experimental contributions.












\textbf{Methodology}: To our best knowledge, this is the first work to smooth environment labels for DAT. The proposed \ls yields three main advantages: (i) it does not require any extra parameters and optimization steps and yields faster convergence speed, better training stability, and more robustness to label noise theoretically and empirically; (ii) despite its efficiency, ELS is also easily to implement. People can easily incorporate ELS with any DAT methods in very few lines of code; (iii) ELS equipped DAT methods attain superior generalization performance compared to their native counterparts;


\textbf{Theories}: The benefit of \ls is theoretically verified in the following aspects. (i) \textit{Training stability}. We first connect DAT to Jensen–Shannon/Kullback–Leibler divergence minimization, where \ls is shown able to extend the support of training distributions and relieve both the oscillatory gradients and gradient vanishing phenomenons, which results in stable and well-behaved training. 
(ii) \textit{Robustness to noisy labels}. We theoretically verify that the negative effect caused by noisy labels can be reduced or even eliminated by \ls with a proper smooth parameter.
(iii) \textit{Faster non-asymptotic convergence speed}. We analyze the non-asymptotic convergence properties of DANN. The results indicate that incorporating with \ls can further speed up the convergence process. 
 In addition, we also provide the empirical gap and analyze some commonly used DAT tricks.



\textbf{Experiments:} (i) Experiments are carried out on various benchmarks with different backbones, including \textit{image classification, image retrieval, neural language processing, genomics data, graph, and sequential data}. \ls brings consistent improvement when incorporated with different DAT methods and achieves competitive or SOTA performance on various benchmarks, \eg average accuracy on Rotating MNIST (), worst group accuracy on CivilComments (), test ID accuracy on RxRx1 (), average accuracy on Spurious-Fourier dataset (). (ii) Even if the environment labels are random or partially known, the performance of \ls+ DANN  will not degrade much and is superior to native DANN. (iii) Abundant analyzes on training dynamics are conducted to verify the benefit of \ls empirically. (iv) We conduct thorough ablations on hyper-parameter for \ls and some useful suggestions about choosing the best smooth parameter considering the dataset information are given.



\vspace{-0.1cm}
\section{Methodology}
\vspace{-0.1cm}


For domain generalization tasks, there are  source domains . Let the hypothesis  be the composition of , where  pushes forward the data samples to a representation space  and  is the domain discriminator with softmax activation function. The classifier is defined as , where  is the number of classes. The cost used for the discriminator can be defined as:

where  is the prediction probability that  is belonged to . Denote  the class label, then the overall objective of DAT is

where  is the cross-entropy loss for classification tasks and MSE for regression tasks, and  is the tradeoff weight. We call the first term empirical risk minimization (ERM) part and the second term adversarial training (AT) part.
Applying \ls, the target in \myref{equ:dann_multiclass_main} can be reformulated as


\vspace{-0.1cm}
\section{Theoretical validation}
\vspace{-0.1cm}
In this section, we first assume the discriminator is optimized with no constraint, providing a theoretical interpretation of applying \ls. Then how \ls makes the training process more stable is discussed based on the interpretation and some analysis of the gradients. We next theoretically show that with \ls, the effect of label noise can be eliminated. Finally, to mitigate the impact of the \textbf{no constraint} assumption, the empirical gap, parameterization gap, and non-asymptotic convergence property are analyzed respectively. All omitted proofs can be found in the Appendix.
\subsection{Divergence Minimization Interpretation}\label{sec:js}


In this subsection, the connection between \ls/one-sided \ls and divergence minimization is studied. The advantages brought by \ls and why GANs prefer one-sided \ls are theoretically claimed. We begin with the two-domain setting, which is used in domain adaptation and generative adversarial networks. Then the result in the multi-domain setting is further developed.
\begin{prop}
Given two domain distributions  over , and a hypothesis class . We suppose  the optimal discriminator with no constraint, denote the mixed distributions with hyper-parameter  as . Then minimizing domain divergence by adversarial training with \textbf{\ls} is equal to minimizing , where  is the Jensen-Shanon (JS) divergence.
  \label{prop1}
\end{prop}
Compared to Proposition 2 in~\citep{acuna2021f} that adversarial training in DANN is equal to minimize . The only difference here is the mixed distributions , which allows more flexible control on divergence minimization. For example, when ,  which is the same as the original adversarial training; when ,  and , which means that this term will not supply gradients during training and the training process will convergence like ERM. In other words,  controls the tradeoff between algorithm convergence and adversarial divergence minimization. One main argue that adjusting the tradeoff weight  can also balance AT and ERM, however,  can only adjust the gradient contribution of AT part, \ie  and cannot affect the training dynamic of . For example, when  have disjoint support,  is always zero no matter what  is given. On the contrary, the proposed technique smooths the optimization distribution  of AT, making the whole training process more stable, but controlling  cannot do. In the experimental section, we show that in some benchmarks, the model cannot converge even if the tradeoff weight is small enough, however, when \ls is applied, \abbr attains superior results and without the need for small tradeoff weights or small learning rate.

As shown in~\citep{goodfellow2016nips}, GANs always use a technique called \textit{one-sided label smoothing}, which is a simple modification of the label smoothing technique and only replaces the target for real examples with a value slightly less than one, such as 0.9. Here we connect one-sided label smoothing to JS divergence and seek the difference between native and one-sided label smoothing techniques. See Appendix~\ref{sec:theo_gan_js} for proof and analysis. We further extend the above theoretical analysis to multi-domain settings, \eg domain generalization, and multi-source GANs~\citep{trung2019learning} (See Proposition~\ref{prop3} in Appendix~\ref{sec:ms_ls} for detailed proof and analysis.). We find that with \ls, a flexible control on algorithm convergence and divergence minimization tradeoffs can be attained.

\vspace{-0.1cm}
\subsection{Training Stability}\label{sec:stable}
\vspace{-0.1cm}

\textbf{Noise injection for extending distribution supports.} The main source of training instability of GANs is the real and the generated distributions have disjoint supports or lie on low dimensional manifolds~\citep{arjovsky2017towards,roth2017stabilizing}. Adding noise from an arbitrary distribution to the data is shown to be able to extend the support of both distributions~\citep{jenni2019stabilizing,arjovsky2017towards,sonderby2016amortised} and will protect the discriminator against measure 0 adversarial examples~\citep{jenni2019stabilizing}, which result in stable and well-behaved training. Environment label smoothing can be viewed as a kind of noise injection, \eg in Proposition~\ref{prop1},  where the noise is  and the two distributions will be more likely to have joint supports. 


\textbf{\ls relieves the gradient vanishing phenomenon.} As shown in Section~\ref{sec:js}, the adversarial target is approximating KL or JS divergence, and when the discriminator is not optimal, a such approximation is inaccurate. We show that in vanilla DANN, as the discriminator gets better, the gradient passed from discriminator to the encoder vanishes (Proposition~\ref{prop:stable} and Proposition~\ref{prop:stable_md}). Namely, \textit{either the approximation is inaccurate, or the gradient vanishes}, which will make adversarial training extremely hard~\citep{arjovsky2017towards}. Incorporating \ls is shown able to relieve the 
\begin{wrapfigure}{r}{0.3\textwidth}
  \begin{center}
  \advance\leftskip+1mm
  \renewcommand{\captionlabelfont}{\footnotesize}
    \vspace{-0.15in}  
    \includegraphics[width=0.3\textwidth]{imgs/convergence/norm.pdf}
    \vspace{-0.2in} 
    \caption{The sum of gradients provided to the encoder by the adversarial loss.}
    \label{fig:gradient_norm}
    \vspace{-0.4in} 
  \end{center}
\end{wrapfigure}
gradient vanishing phenomenon when the discriminator is close to the optimal one and stabilizes the training process.



\textbf{\ls serves as a data-driven regularization and stabilizes the  oscillatory gradients.} Gradients of the encoder with respect to adversarial loss remain highly oscillatory in native DANN, which is an important reason for the instability of adversarial training~\citep{mescheder2018training}. \figurename~\ref{fig:gradient_norm} shows the gradient dynamics throughout the training process, where the PACS dataset is used as an example. With \ls, the gradient brought by the adversarial loss is smoother and more stable. The benefit is theoretically supported in Section~\ref{app_sec:stable}, where applying \ls is shown similar to adding a regularization term on discriminator parameters, which stabilizes the supplied gradients compared to the vanilla adversarial loss. 

\vspace{-0.15cm}
\subsection{\ls meets noisy labels}
\vspace{-0.15cm}
To analyze the benefits of \ls when noisy labels exist, we  adopt the symmetric noise model~\citep{kim2019nlnl}. Specifically, given two environments with a high-dimensional feature  and environment label , assume that noisy labels  are generated by random noise transition with noise rate . Denote ,  the cross-entropy loss and  the smoothed noisy label, then minimizing the smoothed loss with noisy labels can be converted to

where  is the optimal smooth parameter that makes the classifier return the best performance on unseen clean data~\citep{wei2021smooth}. The first term in \myref{equ:main_smooth} is the risk under the clean label. The influence of both noisy labels and \ls are reflected in the last term of the \myref{equ:main_smooth}.  is the opposite of the optimization process as we expect. Without label smoothing, the weight will be  and a high noisy rate  will let this harmful term contributes more to our optimization. On the contrary, by choosing a smooth parameter , the second term will be removed. For example, if , the best smooth parameter is just .

\vspace{-0.15cm}
\subsection{Empirical Gap and Parameterization Gap}\label{sec:empirical:gap}
\vspace{-0.15cm}
Propositions in Section~\ref{sec:js} and Section~\ref{sec:stable} are based on two  unrealistic assumptions. (i) Infinite data samples, and (ii) the discriminator is optimized without a constraint, namely, the discriminator is optimized over infinite-dimensional space. In practice, only empirical distributions with finite samples are observed and the discriminator is always constrained to smaller classes such as neural networks~\citep{goodfellow2014generative} or reproducing kernel Hilbert spaces (RKHS)~\citep{li2017mmd}. Besides, as shown in~\citep{arora2017generalization,schfer2019implicit}, JS divergence has a large empirical gap, \eg \textit{let  be uniform Gaussian distributions , and  be empirical versions of  with  examples. Then we have  with high probability}. Namely, the empirical divergence cannot reflect the true distribution divergence.

A natural question arises: ``Given finite samples to multi-domain AT over finite-dimensional parameterized space, whether the expectation over the empirical distribution converges to the expectation over the true distribution?''. In this subsection, we seek to answer this question by analyzing the \textit{empirical gap and parameterization gap}, which is , where  is the empirical distribution of  and  is constrained. We first show that, let  be a hypothesis class of VC dimension , then for any , with probability at least , the gap is less than , where  and  is the number of samples in  (Appendix~\ref{sec:empirical_vc}). The above analysis is based on  divergence and the VC dimension; we further analyze the gap when the discriminator is constrained to the Lipschitz continuous and build a connection between the gap and the model parameters. Specifically, suppose that each  is -Lipschitz with respect to the parameters and use  to denote the number of parameters of . Then given a universal constant  such that when , we have with probability at least , the gap is less than  (Appendix~\ref{sec:empirical_nn}). Although the analysis cannot support the benefits of \ls, as far as we know, it is the first attempt to study the empirical and parameterization gap of multi-domain AT.

\vspace{-0.15cm}
\subsection{Non-Asymptotic Convergence}\label{sec:convergence}
\vspace{-0.15cm}
As mentioned in Section~\ref{sec:empirical:gap}, the analysis in Section~\ref{sec:js} and Section~\ref{sec:stable} assumes that the optimal discriminator can be obtained, which implies that both the hypothesis set has infinite modeling capacity and the training process can converge to the optimal result. If the objective of AT is convex-concave, then many works can support the global convergence behaviors~\citep{nowozin2016f,yadav2017stabilizing}. However, the convex-concave assumption is too unrealistic to hold true~\citep{nie2020towards,nagarajan2017gradient}, namely, the updates of DAT are no longer guaranteed to converge. In this section, we focus on the local convergence behaviors of DAT of points near the equilibrium. Specifically, we focus on the non-asymptotic convergence, which is shown able to more precisely reveal the convergence of the dynamic system than the asymptotic analysis~\citep{nie2020towards}. 


We build a toy example to help us understand the convergence of DAT. Denote  the learning rate,  the parameter for \ls, and  a constant. We conclude our theoretical results here (which are detailed in Appendix~\ref{sec:app_convergence}): (1) Simultaneous Gradient Descent (GD) DANN, which trains the discriminator and encoder simultaneously, has no guarantee of the non-asymptotic convergence. (2) If we train the discriminator  times once we train the encoder  times, the resulting alternating Gradient Descent (GD) DANN could converge with a sublinear convergence rate only when the . Such results support the importance of alternating GD training, which is commonly used during DANN implementation~\citep{gulrajani2021in}. (3) Incorporate \ls into alternating GD DANN speed up the convergence rate by a factor , that is, when , the model could converge.



\textbf{Remark.} In the above analysis, we made some assumptions \eg in Section~\ref{sec:convergence}, we assume the algorithms are initialized in a neighborhood of a unique equilibrium point, and in Section~\ref{sec:empirical:gap} we assume that the NN is L-Lipschitz. These assumptions may not hold in practice, and they are computationally hard to verify. To this end, we empirically support our theoretical results, namely, verifying the benefits to convergence, training stability, and generalization results in the next section.










\vspace{-0.15cm}
\section{Experiments}\label{sec:exps}
\vspace{-0.15cm}

To demonstrate the effectiveness of our \ls, in this section, we select a broad range of tasks (in Table~\ref{tab:benchmarks}), which are \textit{image classification}, \textit{image retrieval}, \textit{neural language processing}, \textit{genomics}, \textit{graph}, and \textit{sequential prediction tasks}. Our target is to include benchmarks with (i) various numbers of domains (from  to ); (ii) various numbers of classes (from  to ); (iii) various dataset sizes (from  to ); (iv) various dimensionalities and backbones (Transformer, ResNet, MobileNet, GIN, RNN). See Appendix~\ref{sec:exp_detail} for full details of all experimental settings, including dataset details, hyper-parameters, implementation details, and model structures. We conduct all the experiments on a machine with i7-8700K, 32G RAM, and four GTX2080ti. All experiments are repeated  times with different seeds and the full experimental results can be found in the appendix.


\begin{table}[]
\caption{\textbf{A summary on evaluation benchmarks.} Wg. acc. denotes worst group accuracy, 10 \%/ acc. denotes 10th percentile accuracy. GIN~\citep{xu2018powerful} denotes Graph Isomorphism Networks, and CRNN~\citep{gagnon2022woods} denotes convolutional recurrent neural networks.}\label{tab:benchmarks}
\adjustbox{max width=\linewidth}{\begin{tabular}{@{}ccccccc@{}}
\toprule
\rowcolor[HTML]{D7F2BC} 
{\color[HTML]{333333} \textbf{Task}}                             & {\color[HTML]{333333} \textbf{Dataset}}                                & {\color[HTML]{333333} \textbf{Domains}}                                 & {\color[HTML]{333333} \textbf{Classes}}             & {\color[HTML]{333333} \textbf{Metric}}                             & {\color[HTML]{333333} \textbf{Backbone}}                     & {\color[HTML]{333333} \textbf{\# Data Examples}}       \\ \midrule
{\color[HTML]{333333} }                                             & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Rotated MNIST}    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 6 rotated angles}         & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 10}   & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Avg. acc.}           & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} MNIST ConvNet} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 70,000}   \\
{\color[HTML]{333333} }                                             & {\color[HTML]{333333} PACS}                                     & {\color[HTML]{333333} 4 image styles}                                   & {\color[HTML]{333333} 7}                            & {\color[HTML]{333333} Avg. acc.}                                   & ResNet50                                                     & 9,991                                                  \\
{\color[HTML]{333333} }                                             & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} VLCS}             & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 4 image styles}           & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 5}    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Avg. acc.}           & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} ResNet50}      & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 10,729}   \\
{\color[HTML]{333333} }                                             & {\color[HTML]{333333} Office-31}                                & {\color[HTML]{333333} 3 image styles}                                   & {\color[HTML]{333333} 31}                           & {\color[HTML]{333333} Avg. acc.}                                   & ResNet50/ResNet18                                            & 4,110                                                  \\
{\color[HTML]{333333} }                                             & {\color[HTML]{333333} Office-Home}                                & {\color[HTML]{333333} 4 image styles}                                   & {\color[HTML]{333333} 65}                           & {\color[HTML]{333333} Avg. acc.}                                   & ResNet50/ViT                                            & 15,500                                                  \\
\multirow{-6}{*}{{\color[HTML]{333333} Images Classification}}      & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Rotating MNIST}   & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 8 rotated angles}         & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 10}   & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Avg. acc.}           & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} EncoderSTN}    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 60,000}   \\\hline
{\color[HTML]{333333} Image Retrieval}                              & {\color[HTML]{333333} MS}                                       & {\color[HTML]{333333} 5 locations}                                      & {\color[HTML]{333333} 18,530}                        & {\color[HTML]{333333} mAP, Rank }                               & MobileNet                                                     & 121,738                                                \\\hline
{\color[HTML]{333333} }                                             & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} CivilComments}    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 8 demographic groups}     & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 2}    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Avg/Wg acc.}         & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} DistillBERT}   & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 448,000} \\
\multirow{-2}{*}{{\color[HTML]{333333} Neural Language Processing}} & Amazon                                                          & 7676 reviewers                                                          & 5                                                   & 10 \%/Avg/Wg acc.                                                  & DistillBERT                                                  & 100,124                                                \\\hline
                                                                    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} RxRx1}            & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 51  experimental batch}   & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 1139} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Wg/Avg/Test ID acc.} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} ResNet-50}     & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 125,510} \\
\multirow{-2}{*}{Genomics and Graph}                                & OGB-MolPCBA                                                     & 120,084 molecular scaffold                                              & 128                                                 & Avg. acc.                                                          & GIN                                                          & 437,929                                                \\\hline
                                                                   & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Spurious-Fourier} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 3 spurious correlations}  & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 2}    & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} Avg. acc.}           & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} LSTM}          & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 12,000}   \\
 \multirow{-2}{*}{Sequential Prediction}                  & HHAR                                                            & 5 smart devices                                                         & 6                                                   & Avg. acc.                                                          & Deep ConvNets                                                & 13,674                                                 \\\bottomrule
\end{tabular}}
\end{table}
\vspace{-0.1cm}
\subsection{Numerical Results on Different Settings and Benchmarks}
\vspace{-0.1cm}
\textbf{Domain Generalization and Domain Adaptation on Image Classification Tasks.} We first incorporate \ls into SDAT, which is a variant of the DAT method and achieves the state-of-the-art performance on the Office-Home dataset. Table~\ref{tab:offce31} 
 and Table~\ref{tab:sotada} show that with the simple smoothing trick, the performance of SDAT is consistently improved, and on many of the domain pairs, the improvement is greater than . Besides, the \ls can also bring consistent improvement both with ResNet-18, ResNet-50, and ViT backbones. The average domain generalization results on other benchmarks are shown in Table~\ref{tab:dg_pacs_vlcs}.  We observe consistent improvements achieved by \abbr compared to DANN and the average accuracy on VLCS achieved by \abbr () clearly outperforms all other methods.  See Appendix~\ref{sec:exp_num} for \textit{Multi-Source Domain Generalization} performance, \textit{DG performance on Rotated MNIST}  and on \textit{Image Retrieval} benchmarks.


\textbf{Domain Generalization with Partial Environment labels.} One of the main advantages brought by \ls is the robustness to environment label noise. As shown in \figurename~\ref{fig:dann_random}, when all environment labels are known (GT), \abbr is slightly better than DANN. When partial environment labels are known, for example,  means the environment labels of  training data are known and others are annotated differently than the ground truth annotations, \abbr outperform DANN by a large margin (more than  accuracy when only  correct environment labels are given). Besides, we further assume the total number of environments is also unknown and the environment number is generated randomly. M=2 in \figurename~\ref{fig:dann_random} means we partition all the training data randomly into two domains, which are used for training then. With random environment partitions, \abbr consistently beats DANN by a large margin, which verifies that the smoothness of the discrimination loss brings significant robustness to environment label noise for DAT. 

\begin{table}[]
\centering
\scriptsize
\caption{\textbf{The domain adaptation accuracies (\%) on Office-31}.  denotes improvement of a method with \ls compared to that wo/ \ls.}\label{tab:offce31}
\begin{tabular}{@{}c|ccccccc@{}}
\toprule
            & \textbf{A - W} & \textbf{D - W} & \textbf{W - D} & \textbf{A - D} & \textbf{D - A} & \textbf{W - A} & \textbf{Avg} \\ \midrule
            & \multicolumn{7}{c}{{\color{brown}\textbf{ResNet18}}}                                                                                       \\
ERM~\citep{vapnik1998statistical}         & 72.2           & 97.7           & 100.0          & 72.3           & 61.0           & 59.9           & 77.2         \\
DANN~\citep{ganin2016domain}        & 84.1           & 98.1           & 99.8           & 81.3           & 60.8           & 63.5           & 81.3         \\\Gray
DANN+ELS    & 85.5           & 99.1           & 100.0          & 82.7           & 62.1           & 64.5           & 82.4         \\\Gray
 & 1.4            & 1.0            & 0.2            & 1.4            & 1.3            & 1.1            & 1.1          \\
SDAT~\citep{rangwani2022closer}        & 87.8           & 98.7           & 100.0          & 82.5           & 73.0           & 72.7           & 85.8         \\\Gray
SDAT+ELS    & \textbf{88.9}           & \textbf{99.3}           & \textbf{100.0 }         & \textbf{83.9}           & \textbf{74.1}           & \textbf{73.9}           & \textbf{86.7}         \\\Gray
 & 1.1            & 0.5            & 0.0            & 1.4            & 1.1            & 1.2            & 0.9          \\\hline\hline
            & \multicolumn{7}{c}{{{\color{brown}\textbf{ResNet50}}}}                                                                                       \\
ERM~\citep{vapnik1998statistical}         & 75.8           & 95.5           & 99.0           & 79.3           & 63.6           & 63.8           & 79.5         \\
ADDA~\citep{tzeng2017adversarial}        & 94.6           & 97.5           & 99.7           & 90.0           & 69.6           & 72.5           & 87.3         \\
CDAN~\citep{long2018conditional}        & 93.8           & 98.5           & 100.0          & 89.9           & 73.4           & 70.4           & 87.7         \\
MCC~\citep{MCC}         & 94.1           & 98.4           & 99.8           & \textbf{95.6}           & 75.5           & 74.2           & 89.6         \\
DANN~\citep{ganin2016domain}  & 91.3 & 97.2 & 100.0 & 84.1 & 72.9 & 73.6 & 86.5 \\\Gray
DANNELS & 92.2 & 98.5 & 100.0 & 85.9 & 74.3 & 75.3 & 87.7 \\\Gray
 & 0.9 & 1.3 & 0.0 & 1.8 & 1.4 & 1.7 & 1.2 \\
SDAT~\citep{rangwani2022closer}        & 92.7           & 98.9           & 100.0          & 93.0           & 78.5           & 75.7           & 89.8         \\\Gray
SDATELS  &\textbf{93.6}        & \textbf{99.0} & \textbf{100.0} & {93.4} & \textbf{78.7} & \textbf{77.5} & \textbf{90.4}               \\\Gray
 & 0.9          & 0.1         & 0.0 & 0.4 & 0.2 & 1.8 & 0.6              \\
\bottomrule
\end{tabular}\end{table}


\begin{table}[t]
\caption{The domain generalization accuracies (\%) on VLCS, and PACS.  denotes improvement of \abbr compared to DANN.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccc}\cmidrule[\heavyrulewidth]{1-11}
\multirow{2}{*}{\textbf{Algorithm}}& \multicolumn{5}{c}{{\color{brown}\textbf{PACS}}}& \multicolumn{5}{c}{{\color{brown}\textbf{VLCS}}}\\
\cmidrule{2-11}
 & A & C & P & S & Avg & C & L & S & V & Avg\\
\cmidrule{1-11}
\textsc{ERM~\citep{vapnik1998statistical}} & 87.8 ± 0.4 & 82.8 ± 0.5 & 97.6 ± 0.4 & 80.4 ± 0.6 & 87.2 & 97.7 ± 0.3 & 65.2 ± 0.4 & 73.2 ± 0.7 & 75.2 ± 0.4 & 77.8 \\
\textsc{IRM~\citep{arjovsky2020invariant}} & 85.7 ± 1.0 & 79.3 ± 1.1 & 97.6 ± 0.4 & 75.9 ± 1.0 & 84.6 & 97.6 ± 0.5 & 64.7 ± 1.1 & 69.7 ± 0.5 & 76.6 ± 0.7 & 77.2 \\
\textsc{DANN~\citep{ganin2016domain}}  & 85.4  1.2 & 83.1  0.8 & 96.3  0.4 & 79.6  0.8 & 86.1  & 98.6  0.8 & 73.2  1.1 & 72.8  0.8 & 78.8  1.2 & 80.8 \\
ARM~\citep{zhang2021adaptive}  & 85.0  1.2       & 81.4  0.2       & 95.9  0.3       & 80.9  0.5       & 85.8                & 97.6  0.6       & 66.5  0.3       & 72.7  0.6       & 74.4  0.7       & 77.8                  \\
Fisher~\citep{rame2021fishr}  & \textbf{——}       & \textbf{——}       & \textbf{——}       &\textbf{—— }     & 86.9                & \textbf{——}       & \textbf{——}       & \textbf{—— }      &\textbf{——}       & 76.2                  \\
\textsc{DDG}~\citep{zhang2021towards} & \textbf{88.9 ± 0.6} & \textbf{85.0 ± 1.9}  & 97.2 ± 1.2  &  \textbf{84.3 ± 0.7} & \textbf{88.9} & \textbf{99.1 ± 0.6}  & 66.5 ± 0.3  &  73.3 ± 0.6 & \textbf{80.9 ± 0.6} & 80.0 \\\Gray
\abbr & 87.8  0.8 & 83.8  1.6 & 97.1  0.4 & 81.4  1.3 & 87.5 & \textbf{99.1  0.3} & \textbf{73.2  1.1} & \textbf{73.8  0.9} & 79.9  0.9 & \textbf{81.5}  \\\Gray
 & 2.4 & 0.7 & 0.8 & 1.8 & 1.4 & 0.5 & 0 & 1   & 1.1 & 0.7 \\
\cmidrule[\heavyrulewidth]{1-11}
\end{tabular}}



\label{tab:dg_pacs_vlcs}
\vspace{-0.2cm}
\end{table}




\begin{table}[t]
\centering  
\caption{\textbf{Accuracy () on Office-Home for unsupervised DA} (with ResNet-50 and ViT backbone). SDATELS outperforms other SOTA DA techniques and improves SDAT consistently.}\label{tab:sotada}
\resizebox{\columnwidth}{!}{\begin{tabular}{c|c|cccccccccccc|c}
\toprule
\textbf{Method} & \textbf{Backbone}           & \textbf{A-C}                                                  & \textbf{A-P}                                                  & \textbf{A-R}                                                  & \textbf{C-A}                                         & \textbf{C-P}                                                  & \textbf{C-R}                                         & \textbf{P-A}                                         & \textbf{P-C}                                                  & \textbf{P-R}                                                  & \textbf{R-A}                                         & \textbf{R-C}                                                  & \textbf{R-P}                                                  & \textbf{Avg}                                                  \\\hline\hline
ResNet-50~\citep{he2016deep}       &                             & 34.9                                                         & 50.0                                                         & 58.0                                                         & 37.4                                                & 41.9                                                         & 46.2                                                & 38.5                                                & 31.2                                                         & 60.4                                                         & 53.9                                                & 41.2                                                         & 59.9                                                         & 46.1                                                         \\
DANN~\citep{ganin2016domain}            &                             & 45.6                                                         & 59.3                                                         & 70.1                                                         & 47.0                                                & 58.5                                                         & 60.9                                                & 46.1                                                & 43.7                                                         & 68.5                                                         & 63.2                                                & 51.8                                                         & 76.8                                                         & 57.6                                                         \\
CDAN~\citep{long2018conditional}           &                             & 49.0                                                         & 69.3                                                         & 74.5                                                         & 54.4                                                & 66.0                                                         & 68.4                                                & 55.6                                                & 48.3                                                         & 75.9                                                         & 68.4                                                & 55.4                                                         & 80.5                                                         & 63.8                                                         \\
MMD~\citep{zhang2019bridging}             &                             & 54.9                                                         & 73.7                                                         & 77.8                                                         & 60.0                                                & 71.4                                                         & 71.8                                                & 61.2                                                & 53.6                                                         & 78.1                                                         & 72.5                                                & 60.2                                                         & 82.3                                                         & 68.1                                                         \\
f-DAL~\citep{acuna2021f}           &                             & 56.7                                                         & 77.0                                                         & 81.1                                                         & 63.1                                                & 72.2                                                         & 75.9                                                & 64.5                                                & 54.4                                                         & 81.0                                                         & 72.3                                                & 58.4                                                         & 83.7                                                         & 70.0                                                         \\
SRDC~\citep{tang2020unsupervised}            &                             & 52.3                                                         & 76.3                                                         & 81.0                                                         & \textbf{69.5}                                       & 76.2                                                         & \textbf{78.0}                                       & \textbf{68.7}                                       & 53.8                                                         & 81.7                                                         & \textbf{76.3}                                       & 57.1                                                         & 85.0                                                         & 71.3                                                         \\
SDAT~\citep{rangwani2022closer}            &                             & 57.8                                                         & 77.4                                                         & 82.2                                                         & 66.5                                                & 76.6                                                         & 76.2                                                & 63.3                                                & 57.0                                                         & \textbf{82.2}                                                & 75.3                                                & 62.6                                                         & 85.2                                                         & 71.8                                                         \\
SDATELS      & & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{58.2}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{79.7}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{82.5}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 67.5} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{77.2}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 77.2} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 64.6} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{57.9}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{82.2}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 75.4} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{63.1}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{85.5}} & \cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{72.6}} \\
       &   \multirow{-8}{*}{{\color{brown} ResNet-50}}    & 0.4  & 2.3  & 0.3  & 1.0  & 0.6  & 1.0  & 1.3  & 0.9  & 0.0  & 0.1  & 0.5  & 0.3  & 0.8                                                                                                            \\\midrule
TVT~\citep{yang2021tvt}             &                             & \textbf{74.9}                                                         & 86.6                                                         & 89.5                                                         & 82.8                                                & 87.9                                                         & 88.3                                                & 79.8                                                & 71.9                                                         & 90.1                                                         & 85.5                                                & 74.6                                                         & 90.6                                                         & 83.6                                                         \\
CDAN~\citep{long2018conditional}            &                             & 62.6                                                         & 82.9                                                         & 87.2                                                         & 79.2                                                & 84.9                                                         & 87.1                                                & 77.9                                                & 63.3                                                         & 88.7                                                         & 83.1                                                & 63.5                                                         & 90.8                                                         & 79.3                                                         \\
SDAT~\citep{rangwani2022closer}            &                             & 70.8                                                         & 87.0                                                         & 90.5                                                         & 85.2                                                & 87.3                                                         & 89.7                                                & 84.1                                                & 70.7                                                         & 90.6                                                         & 88.3                                                & 75.5                                                         & 92.1                                                         & 84.3                                                         \\
SDATELS   &  \multirow{-4}{*}{{\color{brown} ViT}}  &\multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} 72.1}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{87.3}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{90.6}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{85.2}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{88.1}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{89.7}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{84.1}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{70.7}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{90.8}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{88.4}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{76.5}}} & \multicolumn{1}{c}{\cellcolor[HTML]{F3F3F3}{\color[HTML]{333333} \textbf{92.1}}} & \multicolumn{1}{c}{\textbf{84.6}} \\
      & & 1.3 & 0.3 & 0.1 & 0.0 & 0.8 & 0.0 & 0.0 & 0.0 & 0.2 & 0.1 & 1.0 & 0.0 & 0.3 \\
\bottomrule                             
\end{tabular}}

\end{table}






\textbf{Continuously Indexed Domain Adaptation.}
We compare \abbr with state-of-the-art continuously indexed domain adaptation methods. Table~\ref{tab:cida} compares the accuracy of various methods. DANN shows an inferior performance to CIDA. However, with \ls, \abbr boosts the generalization performance by a large margin and beats the SOTA method CIDA~\citep{wang2020continuously}. We also visualize the classification results on Circle Dataset (See Appendix~\ref{sec:data_detail_cls} for dataset details).~\figurename~\ref{fig:cida} shows that the representative DA method (ADDA) performs poorly when asked to align domains with continuous indices. However, the proposed \abbr can get a near-optimal decision boundary.


\begin{table}[]

\caption{\textbf{Rotating MNIST accuracy (\%) at the source domain and each target domain.}  denotes  the domain whose images are Rotating by . }\label{tab:cida}
\centering
\footnotesize
\begin{tabular}{@{}cccccccccc@{}}
\toprule
\multicolumn{10}{c}{{\color{brown}\textbf{Rotating MNIST}}}\\
\textbf{Algorithm}  & \textbf{(Source) }& \textbf{45}   & \textbf{90 }  & \textbf{135 } & \textbf{180}  & \textbf{225}  & \textbf{270}  & \textbf{315}  & \textbf{Average} \\ \midrule
ERM~\citep{vapnik1998statistical}       & 99.2      & 79.7 & 26.8 & 31.6 & 35.1 & 37.0   & 28.6 & 76.2 & 45.0      \\
ADDA~\citep{tzeng2017adversarial}& 97.6      & 70.7 & 22.2 & 32.6 & 38.2 & 31.5 & 20.9 & 65.8 & 40.3    \\
DANN~\citep{ganin2016domain}       & 98.4      & \textbf{81.4} & 38.9 & 35.4 & 40.0   & 43.4 & 48.8 & 77.3 & 52.1    \\
CIDA~\citep{wang2020continuously}       & \textbf{99.5}      & 80.0   & 33.2 & \textbf{49.3} & \textbf{50.2} & \textbf{51.7} & \textbf{54.6} & \textbf{81.0}   & 57.1    \\\Gray
\abbr      & 98.4      & \textbf{81.4}  & \textbf{55.0}   & 39.9 & 43.7 & 45.9 & 53.7 & 78.7 & \textbf{62.1}    \\\Gray
 & 0.0         & 0.0  & 16.1 & 4.5  & 3.7  & 2.5  & 4.9  & 1.4  & 10.0     \\ \bottomrule
\end{tabular}

\end{table}
\begin{figure}[t]
\centering
\subfigure[Domains.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/CIDA/data1.pdf}\label{fig:data_circle}
\end{minipage}}\subfigure[Ground Truth.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/CIDA/data2.pdf}\label{fig:data_circle_b}
\end{minipage}}\subfigure[ADDA.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/CIDA/ADDA.pdf}
\end{minipage}
}\subfigure[\abbr]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/CIDA/DANN.pdf}
\end{minipage}
}\centering
\caption{\textbf{Results on the \textit{Circle} dataset with 30 domains.} (a) shows domain index by color, (b) shows label index by color, where red dots and blue crosses are positive and negative data sample. Source domains contain the first 6 domains and others are target domains.} \label{fig:cida}
\vspace{-0.2cm}
\end{figure}

\textbf{Generalization results on other structural datasets and Sequential Datasets.} Table~\ref{tab:main_nlp} shows the generalization results on NLP datasets, and Table~\ref{tab:main_graph},~\ref{tab:main_graph2} show the results on genomics datasets. \abbr bring huge performance improvement on most of the evaluation metrics, \eg  test worst-group accuracy on CivilComments,  test ID accuracy on RxRx1, and  test accuracy on OGB-MolPCBA. Generalization results on sequential prediction tasks are shown in Table~\ref{tab:sp_fourier} and Table~\ref{tab:seq2}, where DANN works poorly but \abbr brings consistent improvement and beats all baselines on the Spurious-Fourier dataset.

\begin{table}[t]
\caption{\textbf{Domain generalization performance on neural language datasets.} The backbone is \textit{DistillBERT-base-uncased} and all results are reported over 3 random seed runs. }
\label{tab:main_nlp}
\adjustbox{max width=\textwidth}{\begin{tabular}{@{}ccccccc@{}}
\toprule
\multicolumn{7}{c}{{\color{brown}\textbf{Amazon-Wilds}}} \\
\textbf{Algorithm} &
  \textbf{Val Avg Acc} &
  \textbf{Test Avg Acc} &
  \textbf{Val 10\% Acc} &
  \textbf{Test 10\% Acc} &
  \textbf{Val Worst-group acc} &
  \textbf{Test Worst-group acc} \\ \midrule
ERM~\citep{vapnik1998statistical}      & \textbf{72.7  0.1} & \textbf{71.9  0.1} & \textbf{55.2  0.7} & 53.8  0.8 & 20.3  0.1& 4.2  0.2 \\
Group DRO~\citep{sagawa2020distributionally} & 70.7  0.6 & 70.0  0.6 & 54.7  0.0 & 53.3  0.0 & \textbf{54.2  0.3} & 6.3  0.2  \\
CORAL~\citep{sun2016deep}     & 72.0  0.3  & 71.1  0.3 & 54.7  0.0 & 52.9  0.8 & 30.0  0.2  & 6.1   0.1\\
IRM~\citep{arjovsky2020invariant}       & 71.5  0.3 & 70.5  0.3 & 54.2  0.8 & 52.4  0.8 & 32.2  0.8 & 5.3  0.2  \\
Reweight & 69.1  0.5  & 68.6  0.6   & 52.1  0.2   & 52.0  0.0      & 34.9   1.2            & 9.1  0.4                 \\
DANN~\citep{ganin2016domain}      & 72.1  0.2      & 71.3  0.1      & 54.6   0.0    & 52.9    0.6      & 4.4  1.3 & 8.0  0.0 \\\Gray
\abbr      &  72.3  0.1          &   71.5  0.1         &       54.7   0.0     &    \textbf{53.8 ± 0.0}        &  4.9 ± 0.6    &   \textbf{9.4 ± 0.0}   \\\Gray
      &    0.2        &      0.2      &     0.1       &       0.9     &    0.5  &   1.4   \\
\end{tabular}}

\adjustbox{max width=\textwidth}{\begin{tabular}{@{}ccccc@{}}
\toprule
\multicolumn{5}{c}{{\color{brown}\textbf{CivilComments-Wilds}}} \\
\textbf{Algorithm} & \textbf{Val Avg Acc} & \textbf{Val Worst-Group Acc} & \textbf{Test Avg Acc} & \textbf{Test Worst-Group Acc} \\ \midrule
Group DRO~\citep{sagawa2020distributionally}         & 90.4  0.4 & 65.0  3.8 & 90.2  0.3 & \textbf{69.1  1.8} \\
Reweighted        & 90.0  0.7 & 63.7  2.7 & 89.8  0.8 & 66.6  1.6 \\
IRM~\citep{arjovsky2020invariant}     & 89.0  0.7 & 65.9  2.8 & 88.8  0.7 & 66.3  2.1 \\
ERM~\citep{vapnik1998statistical}                     & \textbf{92.3  0.2} & 50.5  1.9 & \textbf{92.2  0.1} & 56.0  3.6 \\
DANN~\citep{ganin2016domain}                     &      87.0 ± 0.3      &       64.0 ± 2.0     &       87.0 ± 0.3     & 61.7 ± 2.2           \\\Gray
\abbr                     &  88.5 ± 0.4          &   \textbf{65.9 ± 1.1}         &      88.4 ± 0.4      &    66.0 ± 2.2        \\\Gray
       &   1.4           &       1.9     &      1.4      &       4.3                 \\\bottomrule
\end{tabular}
}

\end{table}

\begin{table}[t]
\caption{{Domain generalization performance on genomics dataset, RxRx1.}
}
\label{tab:main_graph}
\adjustbox{max width=\textwidth}{\begin{tabular}{@{}ccccccc@{}}
\toprule
\multicolumn{7}{c}{{\color{brown}\textbf{RxRx1-Wilds}}}\\
\textbf{Algorithm} &
  \textbf{Val Acc} &
  \textbf{Test ID Acc} &
  \textbf{Test Acc} &
  \textbf{Val Worst-Group Acc} &
  \textbf{Test ID Worst-Group Acc} &
  \textbf{Test Worst-Group Acc} \\ \midrule
ERM~\citep{vapnik1998statistical}                  & \textbf{19.4 ± 0.2}   & \textbf{35.9 ± 0.4}   & \textbf{29.9 ± 0.4}   & \textbf{—}           &\textbf{ —}           & \textbf{—}           \\
Group DRO~\citep{sagawa2020distributionally}             & 15.2 ± 0.1   & 28.1 ± 0.3   & 23.0 ± 0.3   & \textbf{—}           & \textbf{—}           & \textbf{—}           \\
IRM~\citep{arjovsky2020invariant}                  & 5.6 ± 0.4    & 9.9 ± 1.4    & 8.2 ± 1.1    & 0.8 ± 0.2 & 1.9 ± 0.4 & 1.5 ± 0.2 \\
DANN~\citep{ganin2016domain}                  & 12.7 ± 0.2 & 22.9 ± 0.1 & 19.2 ± 0.1 & \textbf{1.0 ± 0.1} & 4.6 ± 0.4 & 3.6 ± 0.0 \\\Gray
\abbr & 14.1 ± 0.1 & 26.7 ± 0.1 & 21.2 ± 0.2 & \textbf{1.1 ± 0.1} & \textbf{7.2 ± 0.3} & \textbf{4.2 ± 0.1 }\\\Gray
           & 1.4          & 3.8         & 2            & 0.1      & 2.6        & 0.6        \\ \bottomrule
\end{tabular}}
\vspace{-0.2cm}
\end{table}
\vspace{-0.2cm}






\begin{figure*}[htb]
\centering
\vspace{-0.2cm}
\subfigure[]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/gamma/random_label.pdf}
\vspace{-0.2cm}
\label{fig:dann_random}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/gamma/optimal_lambda.pdf}
\label{fig:gamma}
\vspace{-0.2cm}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/gamma/decay_gamma.pdf}
\label{fig:decap_gamma}
\end{minipage}\vspace{-0.2cm}
}\centering
\vspace{-0.2cm}
\caption{(a) Generalization performance of \abbr compared to DANN with partial correct environment label on the PACS dataset ( as target domain). (b) {The best  for each dataset.} Civil is the CivilComments dataset and OGB is the OGB-MolPCBA dataset. (c) Average generalization accuracy on the PACS dataset with different smoothing policies.} 
\vspace{-0.2cm}
\end{figure*}
\vspace{-0.1cm}
\subsection{Interpretation and Analysis}
\vspace{-0.1cm}
\noindent\textbf{To choose the best .}~\figurename~\ref{fig:gamma} visualizes the best  values in our experiments. For datasets like PACS and VLCS, where each domain will be set as a target domain respectively and has one best , we calculate the mean and standard deviation of all these  values. Our main observation is that, as the number of domains increases, the optimal  will also decrease, which is intuitive because more domains mean that the discriminator is more likely to overfit and thus needs a lower  to solve the problem. An interesting thing is that in~\figurename~\ref{fig:gamma}, PACS and VLCS both have  domains, but VLCS needs a higher . \figurename~\ref{fig:data_sample} shows that images from different domains in PACS are of great visual difference and can be easily discriminated. In contrast, domains in VLCS do not show significant visual differences, and it is hard to discriminate which domain one image belongs to. The discrimination difficulty caused by this inter-domain distinction is another important factor affecting the selection of .




\textbf{Annealing .} To achieve better generalization performance and avoid troublesome parametric searches, we propose to gradually decrease  as training progresses, specifically, , where  are the current training step and the total training steps. \figurename~\ref{fig:decap_gamma} shows that annealing  achieves a comparable or even better generalization performance than fine-grained searched .

\begin{figure}[h]
\centering


\subfigure[Classification loss.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/convergence/step5_cls_loss.pdf}
\end{minipage}}\subfigure[Avg accuracy of source domains.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/convergence/step5_acc_source.pdf}
\end{minipage}}\subfigure[Acc on the target domain.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{imgs/convergence/step5_acc_target.pdf}
\end{minipage}
}
\centering
\caption{\textbf{Training statistics on PACS datasets.} Alternating GD with  is used. All other parameters setting are the same and only on the default hyperparameters
and without the fine-grained parametric search.} \label{fig:pacs_training}
\vspace{-0.2cm}
\end{figure}
\vspace{-0.2cm}
\textbf{Empirical Verification of our theoretical results.} We use the PACS dataset as an example to empirically support our theoretical results, namely verifying the benefits to convergence, training stability, and generalization results. In \figurename~\ref{fig:pacs_training}, 'A' is set as the target domain and other domains as sources. Considering \ls, we can see that in all the experimental results, \abbr with appropriate  attains high training stability, faster and stable convergence, and better performance compared to DANN. In comparison, the training dynamics of native DANN is highly oscillatory, especially in the middle and late stages of training.



\vspace{-2mm}
\section{Related Works}
\vspace{-0.2cm}





\noindent\textbf{Label Smoothing and Analysis} is a technique from the 1980s, and independently re-discovered by~\citep{szegedy2016rethinking}. Recently, label smoothing is shown to reduce the vulnerability of neural networks~\citep{warde201611} and reduce the risk of adversarial examples in GANs~\citep{salimans2016improved}. Several works seek to theoretically or empirically study the effect of label smoothing.~\citep{chen2020investigation} focus on studying the minimizer of the training error and finding the optimal smoothing parameter.~\citep{xu2020towards} analyzes the convergence behaviors of stochastic gradient descent with label smoothing. However, as far as we know, no study focuses on the effect of label smoothing on the convergence speed and training stability of DAT.

\noindent\textbf{Domain Adversarial Training}~\citep{ganin2016domain} using a domain discriminator to distinguish the source and target domains and the gradients of the discriminator to the encoder are reversed by the Gradient Reversal layer (GRL), which achieves the goal of learning domain invariant features.~\citep{schoenauer2019multi,zhao2018adversarial} extend generalization bounds in DANN~\citep{ganin2016domain} to multi-source domains and propose multisource domain adversarial networks.~\citep{acuna2022domain} interprets the DAT framework through the lens of game theory and proposes to replace gradient descent with high-order ODE solvers.~\citep{rangwani2022closer} finds that enforcing the smoothness of the classifier leads to better generalization on the target domain and presents Smooth Domain Adversarial Training (SDAT). The proposed method is orthogonal to existing DAT methods and yields excellent optimization properties theoretically and empirically.

For space limit, the related works about domain adaptation, domain generalization, and adversarial Training in GANs are in the appendix.


\vspace{-0.2cm}
\section{Conclusion}
In this work, we propose a simple approach, \ie \ls, to optimize the training process of DAT methods from an environment label design perspective, which is orthogonal to most existing DAT methods. Incorporating \ls into DAT methods is empirically and theoretically shown to be capable of improving robustness to noisy environment labels, converge faster, attain more stable training and better generalization performance. As far as we know, our work takes a first step towards utilizing and understanding label smoothing for environmental labels.  Although \ls is designed for DAT methods, reducing the effect of environment label noise and a soft environment partition may benefit all DG/DA methods, which is a promising future direction.




\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023/iclr2023_conference}

\clearpage
\newpage
\appendix
\begin{center}
{\LARGE \textbf{Appendix}}
\end{center}

{
  \hypersetup{hidelinks}
  \tableofcontents
\noindent\hrulefill
}

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}} 
\section{Proofs of Theoretical Statements}

\begin{table}[b]
\caption{Notations.}\label{tab:notations}
\begin{tabular}{ll}
\toprule
\multicolumn{1}{c}{Symbol} & \multicolumn{1}{c}{Description}                                 \\\midrule
           & Distributions for source domain, target domain, and domain . \\
           & Empirical distributions for source domain, target domain, and domain . \\
           & Density functions for source domain, target domain, and domain . \\
           & Data samples from source domain, target domain, and domain . \\
     & \begin{tabular}[c]{@{}l@{}}Feature distributions of  respectively,\\ which is also termed .\end{tabular}\\
           & Density functions for  respectively. \\
           & Data samples from . \\
 & Support sets for hypothesis, discriminator, and feature encoder.\\
 & Hypothesis, discriminator, the optimal discriminator, and feature encoder.\\
 & Number of training distributions, number of data samples in .\\
 & Hyper-parameter for the environment label smoothing.\\
 & -divergence and Empirical -divergence.\\
\bottomrule
\end{tabular}
\end{table}
The commonly used notations and their corresponding descriptions are concluded in Table~\ref{tab:notations}.

\subsection{Connect Environment Label Smoothing to JS Divergence Minimization}\label{sec:theo_js}
To complete the proofs, we begin by introducing some necessary definitions and assumptions. 

\begin{definition}
(-divergence~\citep{2006Analysis}). Given two domain distributions  over , and a hypothesis class , the \textit{-divergence} between  is

\label{define1}
\end{definition}

\begin{definition}
(Empirical -divergence~\citep{2006Analysis}.)
For an symmetric hypothesis class , one can compute the \textit{empirical -divergence} between two empirical distributions  and  by computing

where  is the number of data samples of  and  respectively and  is the indicator function which is 1 if predicate  is true, and 0 otherwise.
\label{define2}
\end{definition}

Vanilla DANN estimating the ``min'' part of \myref{equ:h_divergence} by a domain discriminator, that models the probability that a given input is from the source domain or the target domain. Specially, let the hypothesis  be the composition of , where  is a additional hypothesis and  pushes forward the data samples to a representation space . DANN~\citep{2006Analysis} seeks to approximate the -divergence of \myref{equ:h_divergence} by

where the sigmoid activate function is ignored for simplicity,  is the prediction probability that  is belonged to  and  is the prediction probability that  is belonged to . Applying environment label smoothing, the target can be reformulated to 

When , \myref{equ:dann_ls} is equal to \myref{equ:dann_ori} and no environment label smoothing is applied. Then we prove the proposition~\ref{prop1}

\noindent\textbf{Proposition \ref{prop1}.} \textit{Suppose  the optimal domain classifier with no constraint and mixed distributions  with hyper-parameter , then , where  is the Jensen-Shanon (JS) divergence.}

\begin{proof}
Denote the injected source/target density as , where  is the density of  respectively.
We can rewrite \myref{equ:dann_ls} as:

We first take derivatives and find the optimal :

For simplicity, we use  denote  respectively and ignore the . Plugging~\myref{equ:opt_h} into \myref{equ:dann_ls} we can get

Let  
 two distribution densities that are the convex combinations of , we have 
  , and . 
Then  in \myref{equ:dann_ls_1} can be rearranged to

 in \myref{equ:dann_ls_1} can be rearranged to

By plugging the rearranged  and  into \myref{equ:dann_ls_1}, we get

\end{proof}

\subsection{Connect One-sided Environment Label Smoothing to JS Divergence Minimization}\label{sec:theo_gan_js}
\begin{prop}
Given two domain distributions  over , where  is the read data distribution and  is the generated data distribution. The cost used for the discriminator is:

where . Suppose  the optimal discriminator with no constraint and mixed distributions  with hyper-parameter . Then to minimize domain divergence by adversarial training with \textbf{one-sided environment label smoothing} is equal to minimize , where  is the Jensen-Shanon (JS) divergence.
  \label{prop2}
\end{prop}

 \begin{proof}
Applying \textit{one-sided environment label smoothing}, the target can be reformulated to 

where  is a value slightly less than one,  is the density of  respectively. By taking derivatives and finding the optimal  we can get . Plugging the optimal  into the original target we can get:

where  are two mixed distributions and  are their densities. 
\end{proof}
Our result supplies an explanation to ``\textit{why GANs only use one-sided label smoothing rather than native label smoothing}''. That is, if the density of real data in a region is near zero , native environment label smoothing will be dominated by only the generated sample densities because .
Namely, the discriminator will not align the distribution between generated samples and real samples, but enforce the generator to produce samples that follow the fake mode . In contrast, one-sided label smoothing reserves the real distribution density as far as possible, that is, , which avoids divergence minimization between fake mode to fake mode and relieves model collapse. 

\subsection{Connect Multi-Domain Adversarial Training to KL Divergence Minimization}\label{sec:ms_ls}
\begin{prop}
Given domain distributions  over , and a hypothesis class . Suppose  the optimal discriminator with no constraint and mixed distributions , and  with hyper-parameter . Then to minimize domain divergence by adversarial training w/wo \textbf{environment label smoothing} is equal to minimize , and  respectively, where  is the Kullback–Leibler (KL) divergence.
\label{prop3}
\end{prop}
\begin{proof}
We restate corresponding notations and definitions as follows. Given  domains . Let the hypothesis  be the composition of , where  pushes forward the data samples to a representation space  and the domain discriminator with softmax activation function is defined as . Denote  the feature distribution of  which is encoded by encoder . The cost used for the discriminator can be defined as:

Denote  the density of feature distribution . For simplicity, we ignore . Applying lagrange multiplier and taking the first derivative with respect to each , we can get

where  is the lagrange variable and  is because the constraint . Denote  is a mixed distribution and  is the density. Then we have


where  is the KL divergence. With \textbf{environment label smoothing}, the target is 

Take the same operation as \myref{equ:opt_multidomain} we can get

Denote  a set of mixed distributions and  the corresponding densities. Plugging \myref{equ:opt_multidomain_ls} to the target we can get

\end{proof}


\subsection{Training Stability Brought by Environment Label Smoothing}\label{sec:app_stable}
Let  two distributions and  their induced distributions projected by encoder  over feature space. We first show that if  are disjoint or lie in low dimensional manifolds, there is always a perfect discriminator between them. 

\begin{theo}
(Theorem 2.1. in~\citep{arjovsky2017towards}.) If two distribution  have support contained on two disjoint compact subsets  and  respectively, then there is a smooth optimal discriminator   that has accuracy  and  for all .
\label{theo:stable1}
\end{theo}


\begin{theo}
(Theorem 2.2. in~\citep{arjovsky2017towards}.) Assume two distribution  have support contained in two closed manifolds  and  that don’t perfectly align and don’t have full dimension. Both  are assumed to be  continuous in their respective manifolds. Then, there is a smooth optimal discriminator   that has accuracy , and for almost all ,  is smooth in a neighbourhood of  and .
\label{theo:stable2}
\end{theo}

Namely, if the two distributions have supports that are disjoint or lie on low dimensional manifolds, the optimal discriminator will be accurate on all samples and its gradient will be zero almost everywhere. Then we can study the gradients we pass to the generator through a discriminator. 

\begin{prop}
Denote  a differentiable function that induces distributions  with parameter , and  a differentiable discriminator. If Theorem~\ref{theo:stable1} or~\ref{theo:stable2} holds, given a -optimal discriminator , that is \footnote{The constraint on  is because the optimal discriminator has zero gradients almost everywhere, and  is a constraint on the prediction accuracy.}, assume the Jacobian matrix of  given  is bounded by , then we have 


\label{prop:stable}
\end{prop}
\begin{proof}
Theorem~\ref{theo:stable1} or~\ref{theo:stable2} show that in \myref{equ:dann_ls},  is locally one on the support of  and zero on the support of . Then, using Jensen’s inequality, triangle inequality, and the chain rule on these supports, the gradients we pass to the generator through a discriminator given  is

where the fifth line is because we have  when  is small enough and . Similarly we can get the gradients given  is

Here  because  is locally zero on the support of . Then we have 

where  is equal to the gradient of native DANN in \myref{equ:dann_ori} times , namely

which shows that as our discriminator gets better, the gradient of the encoder vanishes. With environment label smoothing, we have 

which alleviates the problem of gradients vanishing.
\end{proof}

\subsection{Training Stability Analysis of Multi-Domain settings}\label{sec:app_stable_md}
Let  a set of data distributions and  their induced distributions projected by encoder  over feature space. Recall that the domain discriminator with softmax activation function is defined as , where  denotes the probability that  belongs to . To verify the existence of each optimal discriminator , we can easily replace  in Theorem~\ref{theo:stable1} and Theorem~\ref{theo:stable2} by  respectively. Namely, if distribution  and  have supports that are disjoint or lie on low dimensional manifolds,  can perfectly discriminate samples within and beyond  and its gradient will be zero almost everywhere.

\begin{prop}
Denote  a differentiable function that induces distributions  with parameter , and  corresponding differentiable discriminators. If optimal discriminators for induced distributions exist, given any -optimal discriminator , we have , assume the Jacobian matrix of  given  is bounded by , then we have 


\label{prop:stable_md}
\end{prop}
\begin{proof}
Following the proof in Proposition~\ref{prop:stable}, we have

where the second line is because for ,  is locally one and other optimal discriminators  are all locally zero, thus we have , and .  is the gradient that passed to the generator by native multi-domain DANN (\myref{equ:dann_multiclass_main}). Environment label smoothing leads to another term, that is  and avoid gradients vanishing. Consider all distributions, we have


\end{proof}

\subsection{\ls stabilize the oscillatory gradient}\label{app_sec:stable}

For the clarity of our proof, the notations here is a little different compared to other sections. Let  be the cross-entropy loss for class , we denote  is the encoder and  is the classification parameter for all domains, then the adversarial loss function for a given sample  with domain index  here is 

We compute the gradient:

where  denotes . When  is small (e.g., ), the gradient will be further pullback towards 0.
Similarly, for  and , we have

then with proper choice of  (e.g., ), the gradient w.r.t  and  will also shrink towards zero.



\subsection{Environment label smoothing meets noisy labels}\label{sec:noisy..............}

In this subsection, we focus on binary classification settings and adopt the symmetric noise model~\citep{kim2019nlnl}. Some of our proofs follow~\citep{wei2021smooth} but different results and analyses are given. The symmetric noise model is widely accepted in the literature on learning with noisy labels and generates the noisy labels by randomly flipping the clean label to the other possible classes. Specifically, given two environment with high-dimensional feature  environment label , denote noisy labels  is generated by a noise transition matrix , where  denotes denotes the probability of flipping the clean label  to the noisy label , \ie . Let  denote the noisy rate, the binary symmetric transition matrix becomes:

Suppose  are drawn from a joint distribution , but during training, only samples with noisy labels are accessible from . Denote  and  the cross-entropy loss, minimizing the smoothed loss with noisy labels can then be converted to

Let , according to the law of total probability, we have \myref{equ:65} is equal to


recall that , the above equation is equal to

Assume  is the optimal smooth parameter that makes the corresponding classifier return the best performance on unseen clean data distribution~\citep{wei2021smooth}. Then the above equation can be converted to


namely minimizing the smoothed loss with noisy labels is equal to optimizing two terms, 

where  is the risk under the clean label. The influence of both noisy labels and \ls are reflected in the last term of \myref{equ:68}. Considering the reverse optimization term , which is the opposite of the optimization process as we expect. Without label smoothing, the weight of  will be  and a high noisy rate  will let this harmful term contributes more to our optimization. In contrast, by choosing the smooth parameter ,  will be removed. For example, if the noisy rate is zero, the best smooth parameter is just .

\subsection{Empirical Gap Analysis Adopted from Vapnik-Chervonenkis framework}\label{sec:empirical_vc}
\begin{theo}
(Lemma 1 in~\citep{ben2010theory}) Given Definition~\ref{define1} and Definition~\ref{define2}, let  be a hypothesis class of VC dimension . If empirical distributions  and  all have at least  samples, then for any , with probability at least ,

\label{theo:empirical_dann}
\end{theo}

Denote convex hull  the set of mixture distributions, , where  is standard -simplex. The convex hull assumption is commonly used in domain generalization setting~\citep{zhang2021towards,albuquerque2020generalizing}, while none of them focus on the empirical gap. Note that  in domain generalization setting is intractable for the unseen target domain  is unavailable during training. We thus need to convert  to a tractable objective. Let , where , and  is the element within  which is closest to the unseen target domain. Then we have 

The explanation follows~\citep{zhang2021towards} that the first term corresponds to “To what extent can the convex combination of the source domain approximate the target domain”. The minimization of the first term requires diverse data or strong data augmentation, such that the unseen distribution lies within the convex combination of source domains. We dismiss this term in the following because it includes  and cannot be optimized.
Follows Lemma 1 in~\citep{albuquerque2020generalizing}, the second term can be bounded by,

namely the second term can be bounded by the combination of pairwise -divergence between source domains. The cost (\myref{equ:dann_multiclass_main}) used for the multi-domain adversarial training can be seen as an approximation of such a target. Until now, we can bound the empirical gap with the help of Theorem~\ref{theo:empirical_dann}

where  is the number of samples in  and . 


\subsection{Empirical Gap Analysis Adopted from Neural Net Distance}\label{sec:empirical_nn}
\begin{prop}
(Adapted from Theorem A.2 in~\citep{arora2017generalization}) Let  a set of distributions and  be empirical versions with at least  samples each. We assume that the set of discriminators with softmax activation function \footnote{There might be some confusion here because in Section~\ref{sec:app_stable} we use  as the parameters of encoder . The usage is just for simplicity but does not mean that  have the same parameters.} are -Lipschitz with respect to the parameters  and use  denote the number of parameter . There is a universal constant  such that when , we have with probability at least  over the randomness of ,

\label{prop4}
\end{prop}
\begin{proof}
For simplicity, we ignore the parameter  when using . According to the following triangle inequality, below we focus on the term  and other terms have the same results. 

Let  be a finite set such that every  is within distance  of a , which is also termed a -net. Standard construction given a  satisfying , namely there aren’t too many distinct discriminators in . By Chernoff bound, we have

Therefore, when  for large enough constant , we can union bound over all . With probability at least , for all , we have . Then for every , we can find a  such that . Therefore

Namely we have

The result verifies that for the multi-domain adversarial training, the expectation over the empirical distribution converges to the expectation over the true distribution for all discriminators given enough data samples.
\end{proof}

\subsection{Convergence theory}\label{sec:app_convergence}

In this subsection, we first provide some preliminaries before domain adversarial training convergence analysis. We then show simultaneous gradient descent DANN is not stable near the equilibrium but alternating gradient descent DANN could converge with a sublinear convergence rate, which support the importance of training encoder and discriminator separately. Finally, when incorporated with environment label smoothing, alternating gradient descent DANN is shown able to attain a faster convergence speed.

\subsubsection{Preliminaries}
The \textbf{asymptotic convergence analysis} is defined as applying the “ordinary differential equation (ODE) method” to analyze the convergence properties of dynamic systems. Given a discrete-time system characterized by the gradient descent:

where  is the gradient and  is the learning rate. The important technique for analyzing asymptotic convergence analysis is \textit{Hurwitz condition}~\citep{nonlinearsystem}: if the Jacobian of the dynamic system  at a stationary point  is Hurwitz, namely the real part of every eigenvalue of  is positive then the continuous gradient dynamics are asymptotically stable.

Given the same discrete-time system and Jacobian , to ensure the \textbf{non-asymptotic convergence}, we need to provide an appropriate range of  by solving , where  is the spectrum of . Namely, we can get constraint of the learning rate, which thus is able to evaluate the minimum number of iterations for an -error solution and could more precisely reveal the convergence performance of the dynamic system than the asymptotic analysis~\citep{nie2020towards}. 

\begin{theo}
(Proposition 4.4.1 in~\citep{nonlinear2002dim}.) Let  be a continuously differential function on an open subset  in  and let  be so that 

1. , and 

2. the absolute values of the eigenvalues of the Jacobian . 

Then there is an open neighborhood  of  so that for all , the iterates  is locally converge to . The rate of convergence is at least linear. More precisely, the error  is in  for  where  is the eigenvalue of  with the largest absolute value. When ,  will not converge and when ,  is either converge with a sublinear convergence rate or cannot converge.
\label{theo:convergence}
\end{theo}

Finding fixed points of  is equivalent to finding solutions to the nonlinear equation  and the Jacobian is given by:

where both  are not symmetric and can therefore have complex eigenvalues. The following Theorem shows when a fixed point of  satisfies the conditions of Theorem~\ref{theo:convergence}.

\begin{theo}
(Lemma 4 in~\citep{mescheder2017numerics}.) Assume  only has eigenvalues with negative real-part and let , then the eigenvalues of the matrix  lie in the unit ball if and only if

\label{theo:convergence2}
\end{theo}
Namely, both the maximum value of  and  determine the maximum possible learning rate. Although~\citep{acuna2021f} shows domain adversarial training is indeed a three-player game among classifier, feature encoder, and domain discriminator, it also indicates that the \textbf{complex eigenvalues with a large imaginary component are originated from encoder-discriminator adversarial training}.  Hence here we only focus on the two-player zero-sum game between the feature encoder, and domain discriminator. One interesting thing is that, from non-asymptotic convergence analysis, we can get a result (Theorem \ref{theo:convergence2}) that is very similar to that from the Hurwitz condition (Corollary 1 in~\citep{acuna2021f}:  and ).


\subsubsection{A Simple Adversarial Training Example}

According to Ali Rahimi's test of times award speech at NIPS 17, \textit{simple experiments, simple theorems are the building blocks that help us understand more complicated systems}. Along this line, we propose this toy example to understand the convergence of domain adversarial training. Denote  two Dirac distribution where both  and  are float number. In this setting, both the encoder and discriminator have exactly one parameter, which is  respectively\footnote{One may argue that neural networks are non-linear, but \textit{Theorem 4.5 from~\citep{nonlinearsystem}} shows that one can “linearize” any non-linear system near equilibrium and analyze the stability of the linearized system to comment on the local stability of the original system.}. The DANN training objective in \myref{equ:dann_ori} is given by

where  and \textbf{the unique equilibrium point of the training objective in \myref{equ:dann_dirac} is given by .} We then recall the update operators of simultaneous and alternating Gradient Descent, for the former, we have 

For the latter, we have , and  are defined as

If we update the discriminator  times after we update the encoder  times, then the update operator will be . To understand convergence of simultaneous and alternating gradient descent, we have to understand when the Jacobian of the corresponding update operator has only eigenvalues with absolute value smaller than 1.

\subsubsection{Simultaneous gradient descent DANN}

\begin{prop}
The unique equilibrium point of the training objective in \myref{equ:dann_dirac} is given by . Moreover, the Jacobian of  at the equilibrium point has the two eigenvalues
  
  namely  will never satisfies the second conditions of Theorem~\ref{theo:convergence} whatever  is, which shows that this continuous system is generally not linearly convergent to the equilibrium point.
\label{prop:simdann}
\end{prop}
\begin{proof}
The Jacobian of  is
 
 
The derivation result of  should have been 

Since the equilibrium point , for points near the equilibrium, we ignore high-order infinitesimal terms \eg  . We can thus obtain the derivation of the second line. The eigenvalues of the second-order matrix  
are , and then the eigenvalues of  is . Obviously  and the proposition is completed.
\end{proof}

\subsubsection{Alternating gradient descent DANN}
\begin{prop}
The unique equilibrium point of the training objective in \myref{equ:dann_dirac} is given by . If we update the discriminator  times after we update the encoder  times. Moreover, the Jacobian of  (\myref{equ:dann_alt}) has eigenvalues

where .  for  and  otherwise. Such result indicates that although alternating gradient descent does not converge linearly to the Nash-equilibrium, it could converge with a sublinear convergence rate. 
\label{prop:altdann}
\end{prop}
\begin{proof}
The Jacobians of alternating gradient descent DANN operators (\myref{equ:dann_alt}) near the equilibrium are given by:

similarly we can get 
 
  As a result, the Jacobian of the combined update operator  is
 
An easy calculation shows that the eigenvalues of this matrix are

Let , we can get . If , namely , then . To satisfy , we have , which conflicts with the assumption. That is , and in this case .
\end{proof}
\subsubsection{Alternating gradient descent \abbr}
Incorporate environment label smoothing to \myref{equ:dann_dirac}, the target is revised into:

\begin{prop}
The unique equilibrium point of the training objective in \myref{equ:dans_dirac} is given by . If we update the discriminator  times after we update the encoder  times. Moreover, the Jacobian of  (\myref{equ:dann_alt}) has eigenvalues

where .  for  and  otherwise. Such result indicates that alternating gradient descent \abbr could converge faster than alternating gradient descent DANN. 
\label{prop:altdans}
\end{prop}
\begin{proof}
The operator for alternating gradient descent \abbr is , and  near the equilibrium are given by:

The Jacobians of alternating gradient descent \abbr~operators  near the equilibrium are given by:

As a result, the Jacobian of the combined update operator  is
 
An easy calculation shows that the eigenvalues of this matrix are

Similarly to the proof of Proposition~\ref{prop:altdann}, let , we can get . Only when ,  are on the unit circle, namely . Compared to the result in Proposition~\ref{prop:altdann}, which is , the additional  enables us to choose more large learning rate and could converge to an small error solution by fewer iterations.
\end{proof}












\begin{table}[b]
\caption{Hyper-parameters for different benchmarks. : learning rate and weight decay for the encoder and classifier; : learning rate and weight decay for the domain discriminator; : batch size during training; : the discriminator is trained  times once the encoder and classifier are trained; : tradeoff weight for the gradient penalty; : tradeoff weight for the adversarial loss. The default  for Adam and AdamW optimizer is 0.99 and the momentum for SGD optimizer is 0.9. / means domain discriminators trained on the dataset use GRL but not alternating gradient descent.}
\label{tab:params}
\adjustbox{max width=\textwidth}{\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\textbf{Task}                                                                          & \textbf{Datsets} & \textbf{} & \textbf{} & \textbf{} & \textbf{} & \textbf{} & \textbf{bsz} & \textbf{} & \textbf{} & \textbf{} \\ \midrule
\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Images \\ Classification\end{tabular}}      & Rotated MNIST    & 1E-03           & 1E-03           & 0.5                & 0E+00              & 0.0                & 64           & 1                    & 1                  & 0.5                \\
                                                                                       & PACS             & 5E-05           & 5E-05           & 0.5                & 0E+00              & 0.0                & 32           & 5                    & 1                  & 0.5                \\
                                                                                       & VLCS             & 5E-05           & 5E-05           & 0.5                & 0E+00              & 0.0                & 32           & 5                    & 1                  & 0.5                \\
                                                                                       & Office-31(ResNet50)        & 1E-02           & 1E-02           & SGD                & 1E-3              & 1E-3                & 32           & /                    & 0                  & 1.00               \\
                                                                                       & Office-Home (ResNet50)        & 1E-02           & 1E-02           & SGD                & 1E-3              & 1E-3                & 32           & /                    & 0                  & 1.00               \\
                                                                                       & Office-31 (ViT)        & 2E-03           & 2E-03           & SGD               & 1E-3              & 1E-3                & 24           & /                    & 0                  & 1.00               \\
                                                                           & Office-Home (ViT)        & 2E-03           & 2E-03           & SGD               & 1E-3              & 1E-3                & 24           & /                    & 0                  & 1.00               \\
                                                                                       & Rotating MNIST   & 2E-04           & 2E-04           & 0.9                & 5E-04              & 5E-04              & 100          & 1                    & 0                  & 2.00               \\\hline
\begin{tabular}[c]{@{}c@{}}Image \\ Retrieval\end{tabular}                             & MS               & 1E-02           & 1E-02           & SGD                & 5E-04              & 5E-04              & 80           & 1                    & 0                  & 1.00               \\\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Neural Language \\ Processing\end{tabular}} & CivilComments    & 1E-05           & 1E-05           & SGD                & 1E-02              & 0                  & 16           & 1                    & 0                  & 1.00               \\
                                                                                       & Amazon           & 1E-04           & 2E-04           & AdamW              & 1E-02              & 0                  & 8            & 1                    & 0                  & 0.11               \\\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Genomics \\ and Graph\end{tabular}}      & RxRx1            & 1E-04           & 2E-04           & 0.9                & 1E-05              & 0                  & 72           & 1                    & 0                  & 0.11               \\
                                                                                       & OGB-MolPCBA      & 8E-04           & 1E-02           & 0.9                & 1E-05              & 0                  & 32           & 1                    & 0                  & 0.11               \\\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sequential\\ Prediction\end{tabular}}       & Spurious-Fourier & 4E-04           & 4E-04           & 0                  & 1E-03              & 0                  & 78           & 3                    & 1.25               & 1.56               \\
                                                                                       & HHAR             & 3E-03           & 1E-03           & 0.5                & 0E+00              & 0                  & 13           & 4                    & 3.5                & 12                 \\ \bottomrule
\end{tabular}}
\end{table}

\section{Extended Related Works}

\textbf{Domain adaptation and domain generalization} \citep{MuaBalSch13,sagawa2020distributionally,li2018learning,blanchard2021domain,li2018deep,zhang2021towards} aims to learn a model that can extrapolate well in unseen environments. Representative methods like AT method~\citep{ganin2016domain} proposed the idea of learning domain-invariant representations as an adversarial game. This approach led to a plethora of methods including state-of-the-art approaches~\citep{zhang2019bridging,acuna2021f,acuna2022domain}. In this paper, we propose a simple but effective trick, \ls, which benefits the generalization performance of methods by using soft environment labels.

\noindent\textbf{Adversarial Training in GANs} is well studied and many theoretical results of GANs motivate the analysis in this paper. \eg divergence minimization interpretation~\citep{goodfellow2014generative,nguyen2017dual}, generalization of the discriminator~\citep{arora2017generalization,thanh2019improving}, training stability~\citep{thanh2019improving,schfer2019implicit,arjovsky2017towards,arjovsky2017wasserstein}, nash equilibrium~\citep{pmlr-v119-farnia20a,nagarajan2017gradient}, and gradient descent in GAN optimization~\citep{nagarajan2017gradient,gidel2018variational,chen2018training}. Multi-domain image generation is also related to this work, generalization to the JSD metric has been explored to address this challenge~\citep{gan2017triangle,pu2018jointgan,trung2019learning}. However, most of them have to build  pairwise critics, which is  expensive when  is large.  GAN~\citep{tao2018chi} firstly attempts to tackle the challenge and only needs  critics.

\section{Additional Experimental Setups}\label{sec:exp_detail}
\subsection{Dataset Details and Experimental Settings}\label{sec:data_detail}
In this subsection, we introduce all the used datasets and the hyper-parameters for reproducing the experimental results in this work. We have uploaded the codes for all experiments in the supplementary materials to make sure that all the results are reproducible. All the main hyper-parameters for reproducing the experimental results in this work are shown in Table~\ref{tab:params}.
\subsubsection{Images Classification Datasets}\label{sec:data_detail_cls}
\textbf{Experimental settings.} \textit{For DG and multi-source DG tasks}, all the baselines are implemented using the codebase of Domainbed \citep{gulrajani2021in} and we use as encoders ConvNet for RotatedMNIST (detailed in Appdendix D.1 in \citep{gulrajani2021in}) and ResNet-50 for the remaining datasets. The model selection that we use is test-domain validation, one of the three selection methods in \citep{gulrajani2021in}. That is, we choose the model maximizing the accuracy on a validation set that follows the same distribution of the test domain. \textit{For DA tasks}, all baselines implementation and hyper-parameters follows~\citep{deepda}. For \textit{Continuously Indexed Domain Adaptation tasks}, all baselines are implemented using PyTorch with the same architecture as~\citep{wang2020continuously}. Note that although our theoretical analysis on non-asymptotic convergence is based on alternating Gradient Descent, current DA methods mainly build on Gradient Reverse Layer. For a fair comparison, in our experiments considering domain adaptation benchmarks, we also use GRL as default and let the analysis in future work.


\noindent\textbf{Rotated MNIST} \citep{ghifary2015domain} consists of 70,000 digits in MNIST with different rotated angles where domain is determined by the degrees .

\noindent\textbf{PACS} \citep{li2017deeper} includes 9, 991 images with 7 classes  dog, elephant, giraffe, guitar, horse, house, person  from 4 domains  art, cartoons, photos, sketches. 

\noindent\textbf{VLCS} \citep{torralba2011unbiased} is composed of 10,729 images, 5 classes  bird, car, chair, dog, person  from domains Caltech101, LabelMe, SUN09, VOC2007. 

\noindent\textbf{Office-31}~\citep{saenko2010adapting} contains contains  images, 31 object categories in three domains:  Amazon, DSLR, and Webcam.


\textbf{Office-Home}~\citep{venkateswara2017deep}: consists of 15,500 images from 65 classes and 4 domains:  Art (Ar), Clipart (Cl), Product (Pr) and Real World (Rw) .

\noindent\textbf{Rotating MNIST}~\citep{wang2020continuously} is adapted from regular MNIST digits with mild rotation to significantly Rotating MNIST digits. In our experiments,  is set as the source domain and others are unlabeled target domains. The chosen baselines include Adversarial Discriminative Domain Adaptation (ADDA~\citep{tzeng2017adversarial}), and CIDA~\citep{wang2020continuously}. ADDA merges data with different domain indices into one source and one target domain. DANN divides the continuous domain spectrum into several separate domains and performs adaptation between multiple source and target domains. For Rotating MNIST, the seven target domains contain images rotating by \{\} degrees, respectively.

\noindent\textbf{Circle Dataset}~\citep{wang2020continuously} includes 30 domains indexed from 1 to 30 and~\figurename~\ref{fig:data_circle} shows the 30 domains in different colors (from right to left is  respectively).  Each domain contains data on a circle and the task is binary classification.~\figurename~\ref{fig:data_circle_b} shows positive samples as red dots and negative samples as blue crosses. In our experiments, We use domains 1 to 6 as source domains and the rest as target domains.

\subsubsection{Image Retrieval Datasets}

\textbf{Experimental settings.}
Following previous generalizable person ReID methods, we use MobileNetV2~\citep{sandler2018mobilenetv2} with a multiplier of  as the backbone network, which is pretrained on ImageNet~\citep{deng2009imagenet}. Images are resized to  and the training batch size  is set to . The SGD optimizer is used to train all the components  with a learning rate of , a momentum of  and a weight decay of . The learning rate is warmed up in the first  epochs and decayed to its  and  at  and  epochs.

We evaluate the proposed method by Person re-identification (ReID) tasks, which aims to find the correspondences between person images from the same identity across multiple camera views. The training datasets include CUHK02~\citep{Li_2013_CVPR}, CUHK03 ~\citep{Li_2014_CVPR}, Market1501~\citep{Zheng_2015_ICCV}, DukeMTMC-ReID~\citep{Zheng_2017_ICCV}, and CUHK-SYSU PersonSearch~\citep{xiao2016end}. The unseen test domains are VIPeR~\citep{viper}, PRID~\citep{prid}, QMUL GRID~\citep{grid}, and i-LIDS~\citep{ilids}. Details of the training datasets are summarized in Table~\ref{tab:traindata} and the test datasets are summarized in Table~\ref{tab:testdata}. All the assets (\ie datasets and the codes for baselines) we use include a MIT license containing a copyright notice and this permission notice shall be included in all copies or substantial portions of the software.

 \begin{table}[]
 \centering
 \caption{Training Datasets Statistics.}\label{tab:traindata}
  \begin{tabular}{ccc}
  \toprule
    Dataset & IDs &Images \\ \hline
    CUHK02& 1,816 & 7,264 \\ 
                      CUHK03& 1,467 & 14,097 \\ 
                   DukeMTMC-Re-Id & 1,812 & 36,411 \\ 
                     Market-1501& 1,501 & 29,419 \\ 
                     CUHK-SYSU& 11,934 & 34,547 \\ \bottomrule
  \end{tabular}
\end{table}
 \begin{table}[]
 \centering
  \caption{Testing Datasets statistics.}\label{tab:testdata}

  \begin{tabular}{c|c|c|c|c}
  \toprule
  \multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{ Probe} & \multicolumn{2}{c}{ Gallery} \\\cline{2-5} 
   &      Pr. IDs&      Pr. Imgs&      Ga. IDs&  Ga. imgs    \\\hline
           PRID&      100& 100     &   649   &  649   \\
           GRID&      125& 125     &  1025    &  1,025   \\
           VIPeR&      316&  316  &  316   &  316  \\
           i-LIDS&      60&60     &     60 &  60   \\\bottomrule
 \end{tabular}
\end{table}

\textbf{GRID}~\citep{grid} contains  probe images and  true match images of the probes in the gallery. Besides, there are a total of  additional images that do not belong to any of the probes. We randomly take out  probe images. The remaining  probe images and  images in the gallery are used for testing.

\textbf{i-LIDS}~\citep{ilids} has two versions, images and sequences. The former is used in our experiments. It involves  different pedestrian pairs observed across two disjoint camera views  and  in public open space. We randomly select  pedestrian pairs, two images per pair are randomly selected as probe image and gallery image respectively.

\textbf{PRID2011}~\citep{prid} has single-shot and multi-shot versions. We use the former in our experiments. The single-shot version has two camera views  and , which capture  and  pedestrians respectively. Only  pedestrians appear in both views.  During the evaluation,  randomly identities presented in both views are selected, the remaining  identities in view  constitute probe set and the remaining  identities in view  constitute gallery set.

\textbf{VIPeR}~\citep{viper} contains  pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views  and .  Each image pair was taken from an arbitrary viewpoint under varying illumination conditions. To compare to other methods, we randomly select half of these identities from camera view  as probe images and their matched images in view  as gallery images.

We follow the single-shot setting. The average rank-k (R-k) accuracy and mean Average Precision (AP) over  random splits are reported based on the evaluation protocol

\subsubsection{Neural Language Datasets}
 
\textbf{CivilComments-Wilds}~\citep{koh2021wilds} contains  comments on online articles taken from the Civil Comments platform. The input is a text comment and the task is to predicate whether the comment was rated as toxic, \eg, the comment \textit{Maybe you should learn to write a coherent sentence so we can understand WTF your point is} is rated as toxic and \textit{I applaud your father. He was a good man! We need more like him.} is not. Domain in CivilComments-Wilds dataset is an 8-dimensional binary vector where each component corresponds to whether the comment mentions one of the 8 demographic identities \{male, female, LGBTQ, Christian, Muslim, other religions, Black, White\}. 
 
\noindent \textbf{Amazon-Wilds}~\citep{koh2021wilds} contains  reviews from disjoint sets of users. The input is the review text and the task is to predict the corresponding 1-to-5 star rating from reviews of Amazon products. Domain  identifies the user who wrote the review and the training set has  domains. The 10-th percentile of per-user accuracies metric is used for evaluation, which is standard to measure model performance on devices and users at various percentiles in an effort to encourage good performance across many devices.
 
\subsubsection{Genomics and Graph datasets}


\noindent\textbf{RxRx1-wilds}~\citep{koh2021wilds} comprises images of cells that have been genetically perturbed by siRNA, which comprises  images of cells obtained by fluorescent microscopy. The output  indicates which of the  genetic treatments (including no treatment) the cells received, and  specifies  batches in which the imaging experiment was run.

\noindent\textbf{OGB-MolPCBA}~\citep{koh2021wilds} is a multi-label classification dataset, which comprises  molecules with  different structural scaffolds. The input is a molecular graph, the label is a 128-dimensional binary vector where each component corresponds to a biochemical assay result, and the domain  specifies the scaffold (\ie a cluster of molecules with similar structure). The training and test sets contain molecules with disjoint scaffolds; The training set has molecules from over  scaffolds. We evaluate models by averaging the Average Precision (AP) across each of the 128 assays.

\subsubsection{Sequential data}

\noindent\textbf{Spurious-Fourier}~\citep{gagnon2022woods} is a binary classification dataset (low-frequency peak (L) and high-frequency peak (H).), which is composed of one-dimensional signal. Domains  contain signal-label pairs, where the label is a noisy function of the low- and high-frequencies such that low-frequency peaks bear a varying correlation of  with the label and high-frequency peaks bear an invariant correlation of  with the label.

\noindent\textbf{HHAR}~\citep{gagnon2022woods} is a 6 activities classification dataset (Stand, Sit, Walk, Bike, Stairs up, and Stairs Down ), which is composed of recordings of 3-axis accelerometer and 3-axis gyroscope data. Specifically, the input  is recordings of 500 time-steps of a 6-dimensional signal sampled at 100Hz. Domain  consist of five smart device models: Nexus 4, Galaxy S3, Galaxy S3 Mini, LG Watch, and Samsung Galaxy Gears.  

\subsection{Backbone Structures}

Most of the backbones are ResNet-50/ResNet-18 and we follow the same setting as the reference works. Here we briefly introduce some special backbones used in our experiments,\ie ConvNet for Rotated MNIST, EncoderSTN for Rotating MNIST, DistillBERT for Neural Language datasets, and GIN for OGB-MoIPCBA.

\noindent\textbf{MNIST ConvNet.} is detailed in Table.~\ref{tab:convnet}. 

\noindent\textbf{DistillBERT.} We use the implementation from~\citep{wolf2019huggingface} and finetune a BERT-base-uncased models for neural language datasets. 
\begin{table}
\centering
\caption{Details of our MNIST ConvNet architecture. All convolutions use 3×3 kernels and ``same'' padding}\label{tab:convnet}
\begin{tabular}{@{}ll@{}}
\toprule
\# & Layer                                                     \\ \midrule
1  & \cellcolor[HTML]{FFFFFF}Conv2D (in=d, out=64)             \\
2  & \cellcolor[HTML]{FFFFFF}ReLU                              \\
3  & \cellcolor[HTML]{FFFFFF}GroupNorm (groups=8)              \\
4  & \cellcolor[HTML]{FFFFFF}Conv2D (in=64, out=128, stride=2) \\
5  & \cellcolor[HTML]{FFFFFF}ReLU                              \\
6  & \cellcolor[HTML]{FFFFFF}GroupNorm (8 groups)              \\
7  & \cellcolor[HTML]{FFFFFF}Conv2D (in=128, out=128)          \\
8  & \cellcolor[HTML]{FFFFFF}ReLU                              \\
9  & \cellcolor[HTML]{FFFFFF}GroupNorm (8 groups)              \\
10 & \cellcolor[HTML]{FFFFFF}Conv2D (in=128, out=128)          \\
11 & \cellcolor[HTML]{FFFFFF}ReLU                              \\
12 & \cellcolor[HTML]{FFFFFF}GroupNorm (8 groups)              \\
13 & \cellcolor[HTML]{FFFFFF}Global average-pooling            \\ \bottomrule
\end{tabular}
\end{table}
\noindent\textbf{EncoderSTN} use a four-layer convolutional neural network for the encoder and a three-layer MLP to make the prediction. The domain discriminator is a four-layer MLP. The encoder is incorporated with a Spacial Transfer Network (STN)~\citep{jaderberg2015spatial}, which takes the image and the domain index as input and outputs a set of rotation parameters which are then applied to rotate the given image.


\noindent\textbf{Graph Isomorphism Networks (GIN)~\citep{xu2018powerful}} combined with virtual nodes is used for OGB-MoIPCBA dataset, as this is currently the model with the highest performance in the Open Graph Benchmark. 

\noindent\textbf{Deep ConvNets~\citep{schirrmeister2017deep}} for HHAR combines temporal and spatial convolution,which fits this data well and we use the implementation in the BrainDecode Schirrmeister~\citep{schirrmeister2017deep} Toolbox. 


\section{Additional Experimental Results}
\subsection{Additional Numerical Results}\label{sec:exp_num}

\begin{table}
\caption{The domain generalization/adaptation accuracy on  Rotated MNIST.}\label{tab:dg_mnist}
\adjustbox{max width=\textwidth}{\setlength{\tabcolsep}{7.25pt}
\begin{tabular}{cccccccc}
\specialrule{0em}{8pt}{0pt}
\toprule
\multicolumn{8}{c}{{\color{brown}\textbf{Rotated MNIST}}}\\
\textbf{Algorithm}   & \textbf{0}           & \textbf{15}          & \textbf{30}          & \textbf{45}          & \textbf{60}          & \textbf{75}          & \textbf{Avg}         \\
\midrule
ERM~\citep{vapnik1998statistical}                  & 95.3  0.2       & 98.7  0.1       & 98.9  0.1       & 98.7  0.2       & 98.9  0.0       & 96.2  0.2       & 97.8                 \\
IRM~\citep{arjovsky2020invariant}                  & 94.9  0.6       & 98.7  0.2       & 98.6  0.1       & 98.6  0.2       & 98.7  0.1       & 95.2  0.3       & 97.5                 \\
DANN~\citep{ganin2016domain}                 & 95.9  0.1       & 98.6  0.1       & 98.7  0.2       & 99.0  0.1       & 98.7  0.0       & 96.5  0.3       & 97.9                 \\


ARM~\citep{zhang2021adaptive}                  & 95.9  0.4       & 99.0  0.1       & 98.8  0.1       & 98.9  0.1       & 99.1  0.1       & 96.7  0.2       & 98.1                 \\       \Gray
\abbr                & 96.3  0.1       & 98.7  0.1       & 98.9  0.3 &  \textbf{99.1  0.1}      & 98.7  0.0       & 96.9  0.5      & 98.1                 \\\Gray
                & 0.4       & 0.1       & 0.2      & 0.1        & 0.0     & 0.4       &     0.2           \\
\bottomrule
\end{tabular}}
\end{table}

\noindent\textbf{Multi-Source Domain Generalization.} IRM~\citep{arjovsky2020invariant} introduces specific conditions for an upper bound on the number of training environments required such that an invariant optimal model can be obtained, which stresses the importance of the number of training environments. In this paper, we reduce the training environments on the Rotated MNIST from five to three. As shown in Table~\ref{tab:multi-target}, as the number of training environment decreases, the performance of IRM fall sharply (\eg the averaged accuracy from  to ), and the performance on the most challenging domains  decline the most ( and ). In contrast, both ERM and \abbr~retain high generalization performances and \abbr~outperforms ERM in most domains. 

\textbf{Image Retrieval.} We compare the proposed \abbr with methods on a typical DG-ReID setting. As shown in Table~\ref{tab:reid_sota}, we implement DANN with various hyper-parameters while DANN always fails to converge on ReID benchmarks. As illustrated in Appendix~\figurename~\ref{fig:dro_training}, we compare the training statistics with the baseline, where DANN is highly unstable and attains inferior results. However, equipped with \ls~and following the same hyper-parameter as DANN, \abbr attains well-training stability and achieves either comparable or better performance when compared with recent state-of-the-art DG-ReID methods. See Appendix~\ref{sec:exp_analysis} for t-sne visualization and comparison.



\begin{minipage}{\textwidth}
\centering
 \begin{minipage}[t]{0.48\textwidth}
  \centering
  \tiny
\makeatletter\def\@captype{table}\makeatother\caption{Domain generalization performance on the OGB-MolPCBA dataset.}\label{tab:main_graph2}
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{3}{c}{{\color{brown}\textbf{OGB-MolPCBA}}}                                            \\ \midrule
\textbf{Algorithm} & \textbf{Val Avg Acc} & \textbf{Test Avg Acc} \\
ERM~\citep{vapnik1998statistical}                 &\textbf{ 27.8 ± 0.1 }                & \textbf{27.2 ± 0.3}                  \\
Group DRO~\cite{sagawa2020distributionally}            & 23.1 ± 0.6                 & 22.4 ± 0.6                  \\
CORAL~\cite{sun2016deep}              & 18.4 ± 0.2                 & 17.9 ± 0.5                  \\
IRM~\citep{arjovsky2020invariant}                 & 15.8 ± 0.2                 & 15.6 ± 0.3                  \\
DANN~\citep{ganin2016domain}                & 15.0 ± 0.6              & 14.1 ± 0.5               \\\Gray
\abbr               & 18.0 ± 0.3              & 17.2 ± 0.3               \\\Gray
         & 3.0                       & 3.1                        \\ \bottomrule
\end{tabular}
  \end{minipage}
  \begin{minipage}[t]{0.48\textwidth}
   \centering
   \tiny
\makeatletter\def\@captype{table}\makeatother\caption{Domain generalization performance on the Spurious-Fourier dataset.}\label{tab:sp_fourier}
 \begin{tabular}{@{}
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c @{}}
\toprule
\multicolumn{3}{c}{{\color{brown}\textbf{Spurious-Fourier dataset}}}        \\ \midrule
\textbf{Algorithm }           & \textbf{Train validation} & \textbf{Test validation} \\
ERM~\citep{vapnik1998statistical}                   & 9.7 ± 0.3               & 9.3 ± 0.1              \\
IRM~\citep{arjovsky2020invariant}                  & 9.3 ± 0.1               & 57.6 ± 0.8             \\
SD~\cite{pezeshki2021gradient}                   & 10.2 ± 0.1              & 9.2 ± 0.0            \\
VREx~\cite{krueger2021out}                 & 9.7 ± 0.2              & \textbf{65.3 ± 4.8}             \\
DANN~\citep{ganin2016domain}                 & 9.7 ± 0.1        & 11.1 ± 1.5       \\\Gray
\abbr & \textbf{10.7 ± 0.6}        & 15.6 ± 2.8       \\\Gray
           & 1.0                    & 4.5                   \\
\bottomrule
\end{tabular}
   \end{minipage}
   \vspace{-0.1cm}
\end{minipage}

\begin{table*}[t]
  \caption{Comparison with recent state-of-the-art DG-ReID methods. \textbf{——} denotes DANN cannot converge and attains infinite loss. }
  \label{tab:reid_sota}
\adjustbox{max width=\textwidth}{\centering
  \renewcommand\arraystretch{1.0}
  \renewcommand\tabcolsep{3.0pt}
  \begin{tabular}{c|cc|cccc|cccc|cccc|cccc}
\toprule
  \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{Average}}  & \multicolumn{4}{c|}{{\color{brown}\textbf{VIPeR}}} & \multicolumn{4}{c|}{{\color{brown}\textbf{PRID}}} & \multicolumn{4}{c|}{{\color{brown}\textbf{GRID}}} & \multicolumn{4}{c}{{\color{brown}\textbf{i-LIDS}}} \\
              &R-1 &AP & R-1 & R-5 & R-10 & AP&R-1& R-5 & R-10& AP& R-1& R-5 & R-10& AP & R-1& R-5 & R-10& AP \\ \hline
DIMN~\citep{Song_2019_CVPR}&47.5&57.9&51.2&70.2&76.0&60.1&39.2&67.0&76.7&52.0&29.3&53.3&65.8&41.1&70.2&{89.7}&{94.5}&78.4 \\ 
              DualNorm~\citep{jia2019frustratingly}&57.6&61.8&{53.9}&62.5&75.3&58.0&{60.4}&73.6&84.8&{64.9}&41.4&47.4&64.7&{45.7}&{74.8}&82.0&91.5&{78.5}\\ 
              DDAN~\citep{chen2020dual}&59.0&63.1&52.3&60.6&71.8&56.4&54.5&62.7&74.9&58.9&\textbf{  50.6}&62.1&73.8&55.7&{78.5}&85.3&92.5&81.5\\
             DIR-ReID~\citep{zhang2021learning}&63.8&71.2&58.5&76.9&\textbf{  83.3}&67.0&69.7&85.8&91.0&77.1&48.2&{67.1}&{76.3}&{\textbf{57.6}}&{79.0}&\textbf{  94.8}&{97.2}&{83.4}\\
MetaBIN~\citep{choi2021metabin} &64.2&71.9&59.3&76.8&81.9&67.6&70.6&86.5&91.5&78.2&47.3&66.0&74.0&56.4&79.5&93.0&\textbf{  97.5}&85.5\\ 
             Group DRO~\citep{sagawa2020distributionally} &57.1&65.9&48.5&68.4&77.2&57.8&66.1&86.5&90.6&74.8&38.7&58.8&66.6&48.6&74.8&90.8&96.8&81.9\\ 
{Unit-DRO~\citep{zhang2022generalizable}}&{ 65.4}&{ 72.8}&{\textbf{ 60.0}}&{\textbf{ 78.2}}&{82.8}&{ \textbf{68.4}}&\textbf{{ 73.5}}&{85.3}&{ 91.7}&\textbf{{79.4}}&47.5&{ \textbf{69.3}}&{ 77.4}&57.2&{ \textbf{80.7}}&{94.0}&97.0&{\textbf{ 86.2}}\\
              DANN~\citep{ganin2016domain} & \multicolumn{2}{c|}{\textbf{——}}  & \multicolumn{4}{c|}{\textbf{——}} & \multicolumn{4}{c|}{\textbf{——}} & \multicolumn{4}{c|}{\textbf{——}} & \multicolumn{4}{c}{\textbf{——}}\\\Gray
              {\abbr}&{ 64.2}&{ 72.1}&{ 59.3}&{ 76.4}&82.7&{ 67.4}&{ 69.6}&\textbf{{87.7}}&{ \textbf{91.7}}&{77.7}&48.1&{ 67.5}&{ \textbf{77.8}}&57.2&{ 79.8}&{94.7}&97.2&{ 86.1}\\
             \bottomrule
  \end{tabular}}
  \vspace{-2mm}
\end{table*}

\begin{table}[t]
\centering
\caption{Generalization performance on multiple unseen target domains.  denotes improvement of \abbr compared to DANN, and  is the hyper-parameter for environment label smoothing.}
\label{tab:multi-target}
\adjustbox{max width=\textwidth}{\setlength{\tabcolsep}{8.25pt}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multicolumn{8}{c}{{{\color{brown}\textbf{Rotated MNIST}}}} \\
 &\multicolumn{3}{c}{\textbf{Target domains }} &\multicolumn{3}{c}{\textbf{Target domains }} & \\\midrule
\textbf{Method}   & \textbf{0}    & \textbf{30 }  & \textbf{60}    & \textbf{15}    & \textbf{45}   & \textbf{75 }                  & \textbf{Avg}                  \\
\hline
ERM~\citep{vapnik1998statistical}& 96.0  0.3&   98.8  0.4  &  98.7  0.1  &  98.8  0.3   &   99.1  0.1  & 96.7  0.3 & 98.0  \\
IRM~\citep{arjovsky2020invariant} &  80.9  3.2   & 94.7  0.9 & 94.3  1.3 &  94.3  0.8   &   95.5  0.5  & 91.1  3.1 & 91.8  \\
DANN~\citep{ganin2016domain} & 96.6  0.2& 98.8  0.3 & 98.7  0.1 & 98.6  0.4 & 98.8  0.2 & 96.9  0.1 & 98.1 \\\Gray
\abbr & 96.7  0.4& 98.9  0.2 & 98.8  0.1 & 98.8  0.1 & 99.0  0.2 & 97.0  0.4 & 98.2\\\Gray
 & 0.1& 0.1 & 0.1 & 0.2 & 0.2 & 0.1 &  0.1\\
\bottomrule
\end{tabular}}
\vspace{1mm}
\end{table}

\begin{table}[]
\caption{Generalization performance on sequential benchmarks.  denotes improvement of \abbr compared to DANN.}
\label{tab:seq2}
\adjustbox{max width=\textwidth}{\begin{tabular}{@{}
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c @{}}
\toprule
\multicolumn{7}{c}{\color{brown}\textbf{HHAR}}                                                               \\ \midrule
\multicolumn{7}{c}{\textit{Train-domain validation}}                                                     \\
\textbf{Algorithm}               & \textbf{Nexus 4}        & \textbf{Galazy S3 }     & \textbf{Galaxy S3 Mini} & \textbf{LG watch}       & \textbf{Sam. Gear}       & \textbf{Average} \\
ID ERM                  & 98.91±0.24     & 98.44±0.15     & 98.68±0.15     & 90.08±0.28     & 80.63±1.33      & 93.35   \\
ERM                     & 97.64±0.15     & 97.64±0.09     & 92.51±0.46     & 71.69±0.14     & 61.94±1.04      & 84.28   \\\hline
IRM                     & 96.02±0.17     & 95.75±0.22     & 89.46±0.50     & 66.49±0.94     & 57.66±0.37      & 81.08   \\
SD                      & 98.14±0.01     & 98.32±0.19     & 92.71±0.09     & 75.12±0.18     & 63.85±0.28      & 85.63   \\
VREx                    & 95.81±0.50     & 95.92±0.23     & 90.72±0.10     & 69.04±0.23     & 56.42±1.57      & 81.58   \\
DANN &    &    &    &    &    &  80.99 \\
\abbr &    &    &    &    &    &  82.02 \\
 & 1.5   &   0.6   &  1.8  & 1.22  &       0.0          &   1.03      \\ \hline
\multicolumn{7}{c}{\textit{Oracle train-domain validation}}                                              \\
\textbf{Algorithm}               & \textbf{Nexus 4}        & \textbf{Galazy S3}      & \textbf{Galaxy S3 Mini} & \textbf{LG watch}       & \textbf{Sam. Gear}       & \textbf{Average} \\
ID ERM                  & 98.91±0.24     & 98.44±0.15     & 98.68±0.15     & 90.08±0.28     & 80.63±1.33      & 93.35   \\
ERM                     & 97.98±0.02     & 97.92±0.05     & 93.09±0.15     & 71.96±0.04     & 64.08±0.66      & 85.01   \\\hline
IRM                     & 96.02±0.17     & 95.75±0.22     & 89.91±0.25     & 68.00±0.34     & 57.77±0.42      & 81.49   \\
SD                      & 98.48±0.01     & 98.67±0.11     & 94.36±0.24     & 75.12±0.18     & 64.86±0.28      & 86.3    \\
VREx                    & 96.65±0.18     & 96.30±0.05     & 90.98±0.16     & 69.39±0.27     & 59.12±0.80      & 82.49   \\
DANN &    &    &    &    &    &  82.64 \\
\abbr &    &    &    &    &    &  {83.56} \\
  &     0.84           &   0.74   &  1.66   &      0.0          &     1.35            &     0.92    \\\midrule
\end{tabular}}
\end{table}

\subsection{Additional Analysis and Interpretation}\label{sec:exp_analysis}
\textbf{T-sne visualization.} We compare the proposed \abbr with MetaBIN and ERM through -SNE visualization. We observe a distinct division of different domains in~\figurename~\ref{fig:tsne-baseline_a} and~\figurename~\ref{fig:tsne-baseline_b}, which indicates that a domain-specific feature space is learned by the ERM. MetaBIN perform better than ERM and the proposed \abbr can learn more domain-invariant representations while keeping discriminative capability for ReID tasks. 



\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/gamma/pacs_vlcs.pdf}
    \caption{Data examples from the PACS and the VLCS datasets.}
    \label{fig:data_sample}
\end{figure}

\begin{figure}[htbp]
\centering
\subfigure[ERM (All).]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/tsne/erm_all.png}\label{fig:tsne-baseline_a}
\end{minipage}}\subfigure[MetaBIN (All).]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/tsne/metabin_all.pdf}
\end{minipage}}\subfigure[\abbr~(All).]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/tsne/dann_all.png}
\end{minipage}
}\\

\subfigure[ERM (Test)]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/tsne/erm_test.png}
\end{minipage}\label{fig:tsne-baseline_b}
}\subfigure[MetaBIN (Test).]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/tsne/metabin_test.pdf}
\end{minipage}
}\subfigure[\abbr~(Test).]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/tsne/dann_test.png}
\end{minipage}
}

\centering
\caption{\textbf{Visualization of the embeddings on training and test datasets.} Query and gallery samples of these unseen datasets are shown using different types of mark. Best viewed in color.} \label{fig:tsne_reid}
\end{figure}

\begin{figure}[htbp]
\centering
\subfigure[Domain discrimination loss.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/convergence/dann_loss.pdf}
\end{minipage}}\subfigure[Identity classification loss.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/convergence/dann_cls.pdf}
\end{minipage}}\subfigure[Average AP on the test set.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=2.0in]{imgs/convergence/dann_map.pdf}
\end{minipage}
}
\centering
\caption{\textbf{Training statistics on ReID datasets}.} \label{fig:dro_training}
\end{figure}


\subsection{Ablation Studies}





 

\end{document}
