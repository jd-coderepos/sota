\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath, bm, subfigure, epstopdf, url, pifont, overpic, cases}
\usepackage{latexsym, amssymb, bbding, multirow, makecell,  diagbox, enumitem, caption}
\usepackage{array, enumitem, soul, algorithm, algpseudocode}
\newcommand{\R}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\B}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\et}{\etal}
\newcommand{\cmark}{\text{\ding{51}}}\newcommand{\xmark}{\text{\ding{55}}}\def\comp{\ensuremath\mathop{\scalebox{.6}{}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\RT}[1]{\textcolor{blue}{\textbf{[RT:} \textit{#1}\textbf{]}}}
\newcommand{\MD}[1]{\textcolor{purple}{\textbf{[MD:} \textit{#1}\textbf{]}}}
\newcommand{\jinliang}[1]{{\color{blue}(jinliang: {#1})}}
\usepackage{arydshln}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{*} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{Hierarchical Conditional Flow: A Unified Framework for \\Image Super-Resolution and Image Rescaling}
\author{\hspace{-0.3cm}Jingyun Liang ~ Andreas Lugmayr ~ Kai Zhang\thanks{Corresponding author.} ~ Martin Danelljan ~ Luc Van Gool ~ Radu Timofte\\
Computer Vision Lab, ETH Zurich, Switzerland\quad  KU Leuven, Belgium\\
{\tt\small \hspace{-0.6cm}\{jinliang, andreas.lugmayr, kai.zhang, martin.danelljan, vangool, timofter\}@vision.ee.ethz.ch}\\ 
{\tt\small }\url{https://github.com/JingyunLiang/HCFlow}
}

\maketitle


\begin{abstract}
Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image super-resolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative log-likelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality.
\end{abstract}



\section{Introduction}
Normalizing flows~\cite{dinh2014nice, dinh2016realnvp, kingma2018glow, ho2019flow++, jaini2019polyflow, nielsen2020survae} are powerful deep generative probabilistic models that allow for efficient and exact likelihood calculation and sampling. They have been used in the generation of image~\cite{dinh2016realnvp, kingma2018glow}, blur kernel~\cite{liang2021fkp}, and audio~\cite{kim2018flowavenet} data. Recently, in the low-level vision community, normalizing flows have attracted much interest and have achieved promising progress for image super-resolution (SR)~\cite{lugmayr2020srflow} and image rescaling~\cite{xiao2020IRN}.

SRFlow~\cite{lugmayr2020srflow} is a seminal flow-based model for image SR. Unlike previous CNN-based models that learn a deterministic mapping from the low-resolution (LR) image to the high-resolution (HR) image, SRFlow learns the distribution of HR images and is able to generate diverse photo-realistic HR images. However, as shown in Fig.~\ref{fig:intro_srflow}, it treats the LR image as an external conditional prior and thus is not fully invertible between HR and LR image pairs, making it hard to be used for image rescaling. Another work IRN~\cite{xiao2020IRN} employs an invertible neural network to learn downscaling and upscaling for image rescaling. Since the model is bijective, it can recover the input HR image with high accuracy after downscaling. Nevertheless, as shown in Fig.~\ref{fig:intro_irn}, it assumes the high-frequency and low-frequency components of the image are independent to each other and thus lacks the ability to exploit their dependency for image SR.


\begin{figure}[!tbp]
\captionsetup{font=small}\scriptsize
\begin{center}
\subfigcapskip=0.2cm
\subfigure[SRFlow]{
\begin{overpic}[width=0.13\textwidth]{figures/intro_srflow1.pdf}\label{fig:intro_srflow}
\put(106,9){\color{black}{\small }}
\put(47,80){\color{black}{\small }}
\put(-15,9){\color{black}{\small }}
\end{overpic}}\hspace{1.5cm}
\subfigure[IRN]{
\begin{overpic}[width=0.13\textwidth]{figures/intro_irn1_1.pdf}\label{fig:intro_irn}
\put(106,18){\color{black}{\small }}
\put(-15,28){\color{black}{\small }}
\put(-15,10){\color{black}{\small }}
\end{overpic}}\vspace{0.cm}
\subfigure[HCFlow]{
\begin{overpic}[width=0.22\textwidth]{figures/intro_mcflow1.pdf}\label{fig:intro_mcflow}
\put(104,26){\color{black}{\small }}
\put(-9,43){\color{black}{\small }}
\put(-9,6){\color{black}{\small }}
\end{overpic}}
\end{center}
\vspace{-0.7cm}
\caption{The comparison between SRFlow~\cite{lugmayr2020srflow}, IRN~\cite{xiao2020IRN} and the proposed HCFlow. ,  and  denote HR image, LR image and the latent variable, respectively. Blue boxes are invertible neural networks, while green ones are non-invertible models (\eg, CNN). Solid bi-directional arrows denote bijective mappings, while dashed arrows represent conditional relations.}
\label{fig:intro}
\vspace{-0.2cm}
\end{figure}
\subfigcapskip=0.1cm

In this paper, we propose a hierarchical conditional flow (HCFlow) as a unified framework for both image SR and rescaling. As shown in Fig.~\ref{fig:intro_mcflow}, HCFlow is an invertible flow-based model for modelling the HR-LR relationship, in which the high-frequency component is hierarchically conditional on the low-frequency component of the image. More specifically, in the forward propagation, HCFlow learns to decompose the input HR image into the LR image and a latent variable. In the inverse propagation, it generates HR images based on the LR input and random samples of the latent variable. The modelling of the latent variable (high-frequency component) is conditional on the generated LR image (low-frequency component) in a hierarchical manner.

When trained for image SR, HCFlow is optimized by minimizing the negative log-likelihood loss on the basis of tractable Jacobian determinant computation. To further improve visual quality, we integrate a pixel loss, perceptual loss, and GAN loss in the inverse propagation to constrain the learned HR space. Moreover, HCFlow can be used for the image rescaling task. It can decompose the HR image to a visually-pleasing LR image and a latent variable that follows a simple distribution. In this case, HCFlow is trained as an encoder-decoder framework, in which the forward and inverse processes are jointly optimized. As HCFlow is bijective, it can recover the HR image faithfully by sampling from the latent space given the generated LR image.

Our contributions can be summarized as follows:
\begin{itemize}
  \vspace{-0.2cm}
  \item[1)] We propose a unified framework for image SR and image rescaling. It learns to model the LR image and the residual high-frequency component simultaneously. The high-frequency component is hierarchically conditional on the generated LR image.
  \vspace{-0.2cm}
  \item[2)] We propose additional losses to train normalizing flows, including pixel, perceptual, and GAN losses, which effectively enhances the HR image quality. 
  \vspace{-0.2cm}
  \item[3)] We perform extensive experiments on three tasks: general image SR, face image SR and image rescaling. HCFlow achieves state-of-the-art results on all tasks in terms of both quantitative metrics and visual quality.
\end{itemize}




\section{Related Work}
In this section, we will briefly review image SR and image rescaling with a particular focus on two highly related flow-based methods, \ie, SRFlow~\cite{lugmayr2020srflow} and IRN~\cite{xiao2020IRN}.

\label{sec:related_work}
\subsection{Image SR}
Image SR aims to reconstruct the HR image given the LR image. Since the pioneer work SRCNN~\cite{dong2014srcnn}, many CNN-based models have been proposed in recent years~\cite{dong2014srcnn, kim2016vdsr, ledig2017SRGAN, wang2018esrgan, zhang2018rcan, zhang2018RDN, liu2020RFANet, zhang2020usrnet, liang21swinir, kai2021bsrgan, liang21manet, zhang2021DPIR}. Most of them focus on delicate feature extraction module design and generate over-smoothed images when trained with the pixel loss. To remedy this, the perceptual loss~\cite{johnson2016perceptual, wang2018esrgan} and GAN loss~\cite{goodfellow2014generative, ledig2017SRGAN, wang2018esrgan, zhang2019ranksrgan} are introduced to improve the perceptual quality. Despite of above progresses, they usually learn a deterministic mapping between the LR image and HR image, which is unnatural for image SR since one LR image may correspond to multiple HR images. 

\vspace{0.1cm}
\noindent \textbf{SRFlow}~\cite{lugmayr2020srflow}\textbf{.}
Normalizing flows~\cite{dinh2014nice, dinh2016realnvp, kingma2018glow, ho2019flow++, jaini2019polyflow, nielsen2020survae, wolf2021deflow} provide a new possible solution for image SR. SRFlow designs a conditional flow to model the distribution of HR images, conditional on LR images. It can generate diverse photo-realistic images by sampling different latent variables. Our proposed HCFlow differs from SRFlow in two main aspects: First, SRFlow uses the LR image as an external conditional prior and maps the HR distribution to a simple latent distribution. Therefore, it cannot generate LR image and thus is not applicable for image rescaling. In contrast, HCFlow models the LR image and treats it as part of the latent space. Second, SRFlow basically follows the flow framework proposed in~\cite{dinh2016realnvp}, while HCFlow proposes a new framework with hierarchical conditional mechanism.


\subsection{Image Rescaling}
Image rescaling aims to downscale the HR image to a visually meaningful LR image, and then recover the HR image plausibly. Different from image SR that works on a given LR image space, image rescaling tries to maintain as much information from the HR image as possible for a better subsequent reconstruction, for the purpose of reducing the storage and bandwidth cost. In other words, it can define its own LR image space which is expected to be more informative than that by simple downscaling such as bicubic downscaling. In general, in image rescaling, the downscaling and upscaling processes are jointly modelled by an encoder-decoder framework~\cite{kim2018task, li2018learning, sun2020learned}, so that the downscaling model is optimized for the later upscaling operation.

\vspace{0.1cm}
\noindent
\textbf{IRN}~\cite{xiao2020IRN}\textbf{.}
Recently, IRN proposes to use a bijective invertible neural network to model the downscaling and upscaling processes. High-frequency component is well-captured and transformed to a structured latent space in training. In testing, the HR image can be recovered by inputting the generated LR image and a randomly sampled latent variable. In particular, IRN assumes the LR image and high-frequency component is independent to each other. These two components are divided apart and learned separately. By contrast, HCFlow assumes the removed high-frequency component is dependent on the LR image and thus employs a hierarchical conditional framework to model the LR image and the conditional distribution of the high-frequency component. Besides, although IRN designs a bijective mapping between HR and LR image pairs, it can only be trained by Monte Carlo simulation rather than maximum likelihood estimation (MLE). HCFlow can be trained in the same way for image rescaling, but it further models the LR image distribution and allows for tractable Jacobian determinant computation, making it possible for probabilistic modelling of HR and LR images when trained by MLE.


\begin{figure*}[!tbp]
\captionsetup{font=small}\scriptsize
\vspace{-0.5cm}
\begin{center}
\subfigcapskip=0.15cm
\subfigure[Forward propagation]{
\begin{overpic}[width=0.35\textwidth]{figures/SC_flow_tree12.pdf}
\put(94.8,12.8){\color{black}{\small }}
\put(75.5,24.5){\color{black}{\small }}
\put(75.5,3){\color{black}{\small }}
\put(38,3){\color{black}{\small }}
\put(57,35.5){\color{black}{\small }}
\put(57,14.5){\color{black}{\small }}
\put(21,14.5){\color{black}{\small }}
\put(38,47){\color{black}{\small }}
\put(38,26){\color{black}{\small }}
\put(2.2,26){\color{black}{\small }}

\put(94.8,18.8){\color{black}{\small }}
\put(75.8,30.5){\color{black}{\small }}
\put(75.8,9){\color{black}{\small }}
\put(37.3,9){\color{black}{\small }}
\put(57.3,41.5){\color{black}{\small }}
\put(57.3,20.5){\color{black}{\small }}
\put(21.2,20.5){\color{black}{\small }}
\put(38.5,54){\color{black}{\small }}
\put(38.5,32){\color{black}{\small }}
\put(2.4,32){\color{black}{\small }}
\end{overpic}}
\hspace{1.5cm}
\subfigure[Inverse propagation]{
\begin{overpic}[width=0.35\textwidth]{figures/SC_flow_tree22.pdf}
\put(94.8,12.8){\color{black}{\small }}
\put(75.5,24.5){\color{black}{\small }}
\put(75.5,3){\color{black}{\small }}
\put(38,3){\color{black}{\small }}
\put(57,35.5){\color{black}{\small }}
\put(57,14.5){\color{black}{\small }}
\put(21,14.5){\color{black}{\small }}
\put(38,47){\color{black}{\small }}
\put(38,26){\color{black}{\small }}
\put(2.2,26){\color{black}{\small }}

\put(93.8,18.8){\color{black}{\small }}
\put(75.8,30.5){\color{black}{\small }}
\put(75.8,9){\color{black}{\small }}
\put(38.5,9){\color{black}{\small }}
\put(57.3,41.5){\color{black}{\small }}
\put(57.3,20.5){\color{black}{\small }}
\put(21.2,20.5){\color{black}{\small }}
\put(38.5,54){\color{black}{\small }}
\put(38.5,32){\color{black}{\small }}
\put(2.4,32){\color{black}{\small }}
\end{overpic}}
\end{center}\vspace{-0.3cm}
\caption{Schematic computational graphs of the hierarchical conditional flow (HCFlow) with 3 flow levels. On level ,  (note that ) is decomposed to low-frequency component  and high-frequency component . The transformation between  and  is conditional on , as indicated by the blue arrows. The computation orders in forward and inverse propagation are shown on the top of each node.}
\label{fig:flow_tree}
\end{figure*}
\subfigcapskip=0.1cm


\section{Methodology}
\subsection{Preliminaries}
Flow-based models~\cite{dinh2014nice, dinh2016realnvp, kingma2016iaf, papamakarios2017maf, huang2018naf, kingma2018glow, ardizzone2018analyzingIRN, jaini2019polyflow, ho2019flow++, nielsen2020survae, liang2021fkp} aim to learn a bijective mapping between the target space and the latent space.
For a high-dimensional random variable (\eg, an image)  with distribution  and a latent variable  with simple tractable distribution  (\eg, multivariate Gaussian distribution), flow models generally use an invertible neural network  to transform  to : .
Conversely,  can be recovered from  by the inverse mapping . 

Generally,  is composed of a series of invertible transformations: . The intermediate variables are defined as  for . The input  and output  of  are  and , respectively. Concretely,  are flow layers such as squeeze layer, batch normalization layer, affine coupling layer, \etc.

According to the change of variable formula and the chain rule, for a sample , the log probability  can be calculated as

where  is the logarithm of the absolute value of the determinant of the Jacobian of  at . The flow model can thereby be optimized by minimizing the negative log-likelihood loss.


\begin{figure*}[!tbp]
\captionsetup{font=small}\scriptsize
\begin{center}
\hspace{-0.1cm}
\begin{overpic}[width=17.3cm]{figures/SC_flow_main.pdf}
\put(18.5,25.8){\color{black}{\small }}
\put(0.7,8){\color{black}{\small }}
\put(18.7,8){\color{black}{\small }}
\put(53.5,20.5){\color{black}{\small }}
\put(33.5,-2){\color{black}{\small }}
\put(53.9,-2){\color{black}{\small }}
\put(94,15.5){\color{black}{\small }}
\put(83.3,13.){\color{black}{ \fontsize{8}{5}\selectfont \rotatebox{270}{ {\makecell{{Squeeze}}}}}}
\put(64.2,11.8){\color{black}{ \fontsize{8}{5}\selectfont \rotatebox{270}{ {\makecell{{Split}}}}}}
\put(45.6,19.1){\color{black}{ \fontsize{6}{4}\selectfont \rotatebox{270}{ {\makecell{{Squeeze}}}}}}
\put(28.2,18.2){\color{black}{ \fontsize{6}{4}\selectfont \rotatebox{270}{ {\makecell{{Split}}}}}}
\put(16.8,17.3){\color{black}{ \fontsize{6}{4}\selectfont \rotatebox{0}{ {\makecell{Feature\\Extractor}}}}}
\put(41.5,8){\color{black}{ \fontsize{6}{4}\selectfont \rotatebox{0}{ {\makecell{Feature\\Extractor}}}}}

\end{overpic}
\end{center}\caption{The architecture of the hierarchical conditional flow (HCFlow) with 2 flow levels. For a HR image , we first squeeze, transform and split it to low-frequency component  and high-frequency component . Similarly,  is decomposed to  (\ie, the LR image in this case) and  in the next level.  and  are transformed to latent variables  and , conditional on  and  (note that  and  are feature extractors, \eg, CNN) respectively, in a hierarchical manner. The model is trained by negative log-likelihood loss, and can be further enhanced by pixel loss, perceptual loss and GAN loss.}
\label{fig:flow_main}
\end{figure*}

\subsection{Model Specification}
Both image SR and image rescaling try to reconstruct the HR image  given a LR image. Since the image degradation process (or image downscaling) is the inverse of image super-resolution (or image upscaling), we can model these two processes with an invertible bijective transformation: , where  and  are the generated LR image and the rest high-frequency component, respectively. As modelling the probability of natural images is a non-trivial task, it is reasonable to design a flow model conditional on the ground-truth LR image  as,
 

Ideally, we hope the model can generate exactly the same LR image as the ground-truth LR image. This can be formulated as a Dirac delta function  and further approximated by a multivariate Gaussian distribution as,

where  is a diagonal covariance matrix with all diagonal elements close to zero. Note that  is nearly equal to  in this case. By further mapping  to a standard multivariate Gaussian distribution , the flow model is defined as,
 

As we can see, part of the latent space is constrained to be the LR image space. In particular, decomposed high-frequency component  is conditional on another decomposed component . Once trained, following the forward direction, HCFlow can decompose the HR image  into LR image  and latent variable  that follows a simple distribution. Following the inverse direction, HCFlow can generate  given the LR image input  and a random sample  from the latent distribution, as it is an invertible bijective model.

Note that this model regards  as an input or output, rather than as an external conditional prior. Therefore, it is not explicitly conditional on  and is fully invertible between HR and LR image pairs. Besides, by approximating the the distribution of  with a multi-variate Gaussian distribution, it allows for tractable Jacobian determinant computation, so that the model can be optimized by maximum likelihood estimation (MLE).


\subsection{Model Architecture}
The multi-scale architecture proposed in RealNVP~\cite{dinh2016realnvp} is a popular normalizing flow architecture~\cite{lugmayr2020srflow, kingma2018glow, ho2019flow++}. It consists of  levels and at the end of each level, half of the dimensions are factored out. Generally, the factored out dimensions are directly Gaussianized for the computation of negative log-likelihood loss, lacking sufficient modelling of these dimensions. Therefore, based on the multi-scale architecture, we take a further step to model factored out dimensions conditional on the reserved dimensions.

As illustrated in Fig.~\ref{fig:flow_tree}, at each level ,  is decomposed to low-frequency component  and high-frequency component . Then,  is modelled by an additional flow that is conditional on the concatenation of tensors  from multiple flow levels. By this design, the reconstruction of high-frequency component is hierarchically conditional on frequencies reconstructed from all previous levels. In forward propagation, similar to the depth-first traversal of a binary tree, we first compute , , ...,  in order. Then, we model the factored out dimensions in a reverse order: , , ..., . In inverse propagation, we compute  and  level by level, from level  to level . Note that the determinant of Jacobian of the whole flow can still be efficiently computed, since the conditional relations between  and  can be represented as an upper triangle block matrix.

The detailed architecture of HCFlow is shown in Fig.~\ref{fig:flow_main}. For each level, the first layer is the squeeze layer, which transforms the  input to a  tensor by trading spatial size for number of channels. Then,  flow-steps are used for transforming the tensor and decomposing it into different components. More specifically, each flow-step consists of a sequence of three layers: Actnorm layer, invertible  convolution layer and affine coupling layer~\cite{dinh2016realnvp, kingma2018glow}. After that, the split layer is used to evenly split the tensor into two tensors  and  along the channel dimension. Note that, for the last level, we only keep 3 channels for  to make it fit the RGB space of the LR image. Next,  is fed to the next level, while  is input into an additional flow.

In the -th additional flow,  is transformed to the latent variable  by  flow-steps. Different from above flow-steps, we use conditional affine coupling layer~\cite{ardizzone2019guided, winkler2019learning} rather than ordinary affine coupling layer to obtain a conditional flow. In particular, we first upscale the conditional feature  from level  by  nearest neighbor interpolation, and concatenate it with . Then, we use a feature extractor  to extract image features, which act as the conditional feature  for level .
Note that the feature extractor only provides scale and shift for an affine coupling during both forward and inverse propagation. Hence the constrains on being invertible and having a tractable Jacobian do not hold for this part.
More formally, the hierarchical conditional mechanism of HCFlow is formulated as follows,

where conditional features of different levels are computed in a reverse order, from  to .

Particularly, for the last level, we directly model  by a Dirac delta function  instead of transforming it to another latent variable. This constrains part of the latent space to be the LR image space and implicitly makes the model be conditional on .



\subsection{Training Objectives}
\paragraph{Image SR.} When HCFlow is used for image SR, it can be trained by minimizing the negative log-likelihood loss,

which is unsupervised and converges stably. However, in practice, this loss converges slowly and does not provide strong supervision for image SR. To achieve better HR image PSNR, we can add  pixel loss on the generated SR image in inverse propagation, leading to a loss function as follows,

where  is the ground-truth HR image and  is the generated SR image by inputting the ground-truth LR image  and sampling the latent variable  with temperature . The added pixel loss can help the flow to learn the SR manifold centered around the PSNR-oriented SR image. Furthermore, we can add perceptual loss~\cite{johnson2016perceptual} and GAN loss~\cite{goodfellow2014generative} on the generated SR image to improve the visual quality. This is formulated as,

where  is the generated SR image by inputting  and sampling  with . Note that unlike the pixel loss that uses ,  is set to 0.8 or 0.9 to preserve the diversity of HR images.

\vspace{-0.4cm}
\paragraph{Image rescaling.}
Different from image SR, image rescaling aims to recover exactly the same HR image. Following ~\cite{xiao2020IRN}, we regard the invertible HCFlow as an encoder-decoder framework, in which the forward and inverse processes correspond to the encoding and decoding stages, respectively. The loss is as follows,

where  is the  pixel loss to ensure that, after downscaling and upscaling, the reconstructed image  is close to the input . Note that this loss would dramatically decrease the diversity of generated images. Besides,  is the  pixel loss on the LR image, which guides  to be close to the bicubic LR image , so as to generate visually-pleasing LR images in downscaling. The last term  is the  regularization on the latent variable .



\section{Experiments}
\subsection{Experimental Setup}
We conduct experiments on general image SR, face image SR and image rescaling to show the effectiveness of HCFlow. For image SR experiments, we train the model by three loss combinations: ,  and . The corresponding learned models are denoted as \textbf{HCFlow}, \textbf{HCFlow+} and \textbf{HCFlow++}, respectively.

\vspace{-0.4cm}
\paragraph{Image SR.}
For general image SR (), we set  to 2, 13 and 13, respectively. Two 13-block RRDB networks~\cite{wang2018esrgan} are used as feature extractors. More details on the architecture are provided in the supplementary. The model is trained on the training set of DIV2K~\cite{DIV2K} and Flickr2K~\cite{Flickr2K} with random flips. The crop patch size and mini-batch size are set to  and , respectively. Adam optimizer~\cite{kingma2014adam} with  and  is used for optimization. For HCFLow (with only ), the learning rate is  and reduced by half at , ,  and  of  iterations. We fine-tune HCFLow+ (with ) for  iterations from the pretrained HCFlow. The weight of  and  are  and , respectively. It is worth pointing out that we can achieve even higher PSNR (about 0.2dB) if we train HCFlow+ from scratch. Similarly, we can fine-tune HCFlow++ by further adding  and . The loss weighting parameters are , ,  and .


For face image SR (),  are set to 3, 13 and 13, respectively. Three 8-block RRDB networks are used as feature extractors. We train the model on the CelebA training set~\cite{liu2015celeba} and test it using first 5,000 images from the testing set. Following \cite{kingma2018glow, lugmayr2020srflow}, we crop and resize the HR images to the resolution of , and flip them randomly for data augmentation. Other training details are the same as general image SR.


\begin{table*}[!thbp]
\captionsetup{font=small}
\footnotesize
\center
\begin{center}
\caption[Caption for LOF]{Ablation study on latent space and conditional priors for general image SR (). Results are tested on DIV2K~\cite{DIV2K} validation set.}\vspace{-0.2cm}
\label{tab:ablation_latent}
\begin{tabular}{c|c|c|c|p{1.2cm}<{\centering}|p{1.2cm}<{\centering}|p{1.6cm}<{\centering}|p{1.6cm}<{\centering}|p{1.4cm}<{\centering}}
\hline
 Case & Latent Space &  \makecell{Conditional Prior\l=1\uparrow)} & \makecell{LPIPS\\tau=0.9\uparrow\mathbf{z}_2,\mathbf{z}_1\mathbf{z}_2,\mathbf{z}_1\mathbf{y}^*\mathbf{y}^*,\mathbf{y}_1\mathbf{y}_2,\mathbf{z}_2,\mathbf{z}_1\mathbf{y}^*\mathbf{y}^*,\mathbf{y}_1\mathbf{y}_2,\mathbf{z}_2,\mathbf{z}_1\mathbf{y}_2,\mathbf{z}_2,\mathbf{z}_1\mathbf{y}_2\mathbf{y}_1\mathbf{y}_2,\mathbf{z}_2,\mathbf{z}_1\mathbf{y}_2\mathbf{y}_2,\mathbf{y}_1\times 4\uparrow\uparrow\downarrow\downarrow\downarrow\uparrow\uparrow\uparrow\tau=0\tau=0.9\tau=0\tau=0.9\tau=0\tau=0.9\times 8\uparrow\uparrow\downarrow\downarrow\downarrow\uparrow\uparrow\uparrow\tau=0\tau=0.8\tau=0\tau=0.8\tau=0\tau=0.8\times 4L, K, P1\times 12.5\times 10^{-4}[100k,200k,300k, 400k]500k\lambda_1=1\lambda_2=5\times 10^{-2}\lambda_3=1\times 10^{-5}\textbf{y}_2\textbf{y}_2\textbf{y}^*\mathbf{y}_1\mathbf{y}_2\mathbf{y}^*\mathbf{y}_2\mathbf{y}_2\delta(\mathbf{y}_2-\mathbf{y}^*)\mathbf{y}_2\mathbf{y}^*\mathbf{y}_2\mathbf{y}^*\tau=0.9\tau=0.9\tau=0.9\times 4\times\tau=0.8\tau=0.8\tau=0.8\times 8\mathbf{y}_2\mathbf{z}_ll=1,2\mathbf{y}_l\times 4\tau=0\tau=0.9\times 4\times 80.10\sim0.35dB\times 4$) on the DIV2K~\cite{DIV2K} validation set. More results are shown in the supplementary.}
\label{fig:ir_visualresults}
\vspace{-0.5cm}
\end{figure}


\section{Conclusion}
In this paper, we proposed a unified framework, \ie, hierarchical conditional flow (HCFlow), for both image super-resolution and image rescaling. It learns a fully invertible mapping between HR image and LR image as well as the latent variable. Particularly, we learn the LR image space and design a hierarchical conditional mechanism between the latent variable (high-frequency component) and the LR image (low-frequency component). For image SR, HCFLow is trained by the negative log-likelihood loss, and is further enhanced by pixel loss, perceptual loss and GAN losses for better performance. For image rescaling, it is trained as an encoder-decoder framework, where the forward and inverse progresses are jointly optimized. Experiments demonstrate that HCFlow achieves state-of-the-art performance on general image SR, face image SR and image rescaling, in terms of both quantitative metrics and visual quality.

\vspace{0.2cm}
\noindent\textbf{Acknowledgements}~~ We thank Dr. Suryansh Kumar for helpful discussion. This work was partially supported by the ETH Zurich Fund (OK), a Huawei Technologies Oy (Finland) project, the China Scholarship Council and a Microsoft Azure grant. Special thanks goes to Yijue Chen.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{superresolution.bib}
}


\end{document}
