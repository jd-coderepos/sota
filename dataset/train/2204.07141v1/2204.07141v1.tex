\documentclass{article} 

\usepackage[table,dvipsnames]{xcolor}

\usepackage{arxivtemplate/arxiv,times}

\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsthm} \usepackage{color}

\usepackage{epsfig}
\usepackage{graphicx}

\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{bbold}
\usepackage[nice]{nicefrac}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{fontawesome}
\usepackage[font={footnotesize}]{caption}
\usepackage{subcaption}
\usepackage{stfloats}
\usepackage{pbox}
\usepackage{multirow}
\usepackage{microtype}

\definecolor{fbpurple3}{HTML}{f0ebf5}
\definecolor{fbteal2}{HTML}{199696}
\definecolor{fborange2}{HTML}{f06919}
\definecolor{fbApp}{HTML}{dee3e9}
\definecolor{fborange3}{HTML}{ffefe1}
\definecolor{fbApp}{HTML}{c8e7fa}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\defeq}{\coloneqq}
\newcommand{\im}[1]{{ \color{blue} I: #1}}
\newcommand{\aj}[1]{{ \color{red} AJ: #1}}
\newcommand{\nb}[1]{{ \color{cyan} NB: #1}}
\newcommand{\mr}[1]{{ \color{magenta} MR: #1}}
\newcommand{\mc}[1]{{\color{orange}[\textbf{Mathilde}:#1]}}

\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\usepackage{hyperref}
\usepackage{url}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\title{Masked Siamese Networks\\ for Label-Efficient Learning}

\author{\bf Mahmoud Assran\thanks{correspondence to massran@fb.com}\quad Mathilde Caron\quad Ishan Misra\quad Piotr Bojanowski \\ 
\bf Florian Bordes\quad Pascal Vincent\quad Armand Joulin\quad Michael Rabbat\quad
\bf Nicolas Ballas\
    p_{i,m} \defeq \text{softmax}\left( \frac{z_{i,m} \cdot {\bf q}}{\tau} \right),
 
    \overline{p} \defeq \frac{1}{MB}\sum^B_{i=1}\sum^M_{m=1} p_{i,m}.

    \label{eq:objective}
    \frac{1}{MB} \sum^B_{i=1}\sum^M_{m=1} H(p^+_i, p_{i,m}) - \lambda H(\overline{p}),
3mm]
        \multirow{4}{*}{DINO~\citep{caron2021emerging}} & ViT-S/16 & 800 & 38.9  0.4 & 48.9  0.3 & 58.5  0.1 \\
        & ViT-B/16 & 400 & 41.8  0.3 & 51.9  0.6 & 61.4  0.2 \3mm]
        \multirow{3}{*}{MAE~\citep{he2021masked}} & ViT-B/16 & 1600 & 8.2  0.3 & 25.0  0.3 & 40.5  0.2 \\
        & ViT-L/16 & 1600 & 12.3  0.2 & 19.3  1.8 & 42.3  0.3 \1mm]
& ViT-B/8 & 600 & 55.1  0.1 & 64.9  0.7 & 71.6  0.3 \\
        & ViT-B/4 & 300 & 54.3  0.4 & 64.6  0.7 & \cellcolor{fbApp}\bf 72.4  0.3 \1mm]
        Barlow-Tw.~\citep{zbontar2021barlow} & RN50 & 24M & 55.0 \\
        SimCLRv2~\citep{chen2020big} & RN50 & 24M & 57.9 \\
        PAWS~\citep{assran2021semi} & RN50 & 24M & 66.5 \1mm]
        BYOL~\citep{grill2020bootstrap} &  RN200 () & 250M & 71.2 \\
        SimCLRv2~\citep{chen2020big} & RN151+SK () & 795M & 74.9 \2mm]
        SimCLRv2~\citep{chen2020big} & RN50 & 24M & 800 & 71.7 \\
BYOL~\citep{grill2020bootstrap} & RN50 & 24M & 1000 & 74.4 \\
        DINO~\citep{caron2021emerging} & ViT-S/16 & 22M & 800 & 77.0 \\
        iBOT~\citep{zhou2021ibot} & ViT-S/16 & 22M & 800 & \bf 77.9 \\
        MSN & ViT-S/16 & 22M & 600 & \cellcolor{fbApp} 76.9 \\
        \midrule \midrule
        \multicolumn{5}{c}{\scriptsize\bf Comparing larger architectures}\1mm]
    \bf Method & CIFAR10 & CIFAR100 & iNat18 & iNat19 \\\toprule
    DINO & 99.0 & 90.5 & 72.0 & 78.2\\
    MSN & 99.0 & 90.5 & 72.1 & 78.1\\
    \bottomrule
    \end{tabular}
\end{table}
\begin{table}[h]
    \centering
    \caption{{\bf Linear Eval.~Transfer Learning} with a ViT-Base/16 pre-trained on ImageNet-1K. Across both tasks and various levels of supervision, MSN either outperforms or achieves similar results to DINO pre-training. The MSN model is trained with a masking ratio of 0.3; i.e., dropping 30\% of patches, and thus reduces the computational cost of pre-training relative to DINO.}
    \label{tb:transfer_lin}
    \begin{tabular}{l c c c}
    & \multicolumn{3}{c}{\bf Top 1}\2mm]
        & \multicolumn{4}{c}{Random Masking Ratio} \\
        \bf\small Architecture & 0.15 & 0.3 & 0.5 & 0.7 \\ \toprule
        ViT-S/16 & \cellcolor{fbApp}\bf 66.3 & 66.0 & 64.8 & -- \\
        ViT-B/16 & 68.8 & \cellcolor{fbApp}\bf 69.6 & -- & -- \\
        ViT-L/16 & \tt NaN & \tt NaN & \cellcolor{fbApp}\bf 70.1 & 69.4 \\
        \bottomrule
    \end{tabular}
\end{table}

When increasing the model size, we find that increasing the masking ratio (dropping more patches) is helpful for improving low-shot performance.
We also find that the ViT-L/16 runs with weak masking are unstable, while the runs with more aggressive masking are quite stable. However, we do not have sufficient evidence to claim that increasing the masking ratio always improves the stability of large ViT pre-training.

\paragraph{Augmentation Invariance and Low-Shot Learning}
We explore the importance of data-augmentation invariance for low-shot learning.
We pretrain a ViT-B/16 with MSN, where the teacher and anchor networks either share the input image view or use different input views; in both cases, the anchor view is always masked.
The views are constructed by applying random ColorJitter, Crop, Horizontal Flips, and GaussianBlur to the input image.
\begin{table}[h]
  \centering
  \caption{Impact of view-sharing during pre-training on low-shot accuracy (1\% of ImageNet-1K labels) of a ViT-B/16. The target view is constructed by applying random ColorJitter, Crop, Horizontal Flips, and GaussianBlur to the input image. When using the same image view, MSN finds a shortcut solution. Using color jitter prevents this pathological behaviour. Randomly applying additional geometric data transformations to the anchor further improves performance, demonstrating the importance of view invariance in the low-shot setting.}
  \label{tb:invariance_aug}
    \begin{tabular}{l | c}
    {\bf\small Anchor View Generation} & {\bf\small Top 1} \\\toprule
    Target View &  7.0 \\
    Target View + ColorJitter & 48.7  \\
    Target View + ColorJitter + Crop + Flip + GaussianBlur  & \cellcolor{fbApp}\bf 52.3 \\
    \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tb:invariance_aug} reports top-1 accuracy when evaluating with 1\% of ImageNet-1K labels. Sharing the view leads to a top-1 accuracy of ; MSN finds a shortcut solution relying on color statistics. Using different colors in the input views resolves this pathological behaviour and achieves a top-1 of . Further applying the geometric data-augmentations independently to the two views (as opposed to sharing views) further improves the performance to , showing the importance of learning view-invariant representations in the low-shot setting.

\paragraph{Random Masking Compute and Memory}
We look at the effect of the random masking ratio, i.e., the fraction of dropped patches from the global anchor view, on the computational requirements of large model pre-training.
In each iteration we also generate 10 focal views (small crops) of each input image; the random masking ratio has no impact on these views.
\begin{table}[h]
    \centering
    \caption{Impact of random masking ratio on GPU memory usage and runtime when pre-training a ViT-L/7. Measurements are conducted on a single AWS {\tt p4d-24xlarge} machine, containing 8 A100 GPUs, using a batch-size of 2 images per GPU. In each iteration we also generate 10 focal views (small crops) of each input image; the random masking ratio has no impact on these views. Using more aggressive masking of the global view progressively reduces device memory utilization and speeds up training.}
    \label{tb:mask_compute}
    \begin{tabular}{c | c c}
        Masking Ratio & Mem./GPU & Throughput \\\toprule
        0.0 & 26G & 415 imgs/s \\
        0.3 & 21G & 480 imgs/s \\
        0.5 & 18G & 525 imgs/s \\
        \rowcolor{fbApp}
        0.7 & 17G & 600 imgs/s \\
        \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tb:mask_compute} reports the memory consumption and throughput (imgs/s) of a ViT-L/7 model on a single AWS {\tt p4d-24xlarge} machine using a batch-size of 2 images per GPU.
As expected, using more aggressive masking of the global view progressively reduces device memory utilization and speeds up training.
For example, by randomly masking 70\% of the patches, we can use MSN to pre-train a full-precision ViT-Large with a patch-size of  on as few as 18 AWS {\tt p4d-24xlarge} machines.
Without masking, the same job requires over 42 machines when using the default batch-size of 1024 images.

\section{Conclusion}

We propose Masked Siamese Networks (MSNs), a self-supervised learning framework that leverages the idea of mask-denoising while avoiding pixel and token-level reconstruction. We demonstrate empirically that MSNs learn strong off-the-shelf representations that excel at label-efficient learning, while simultaneously improving the scalability of joint-embedding architectures.
By relying on view-invariant representation learning, MSN does require the specification of data transformations, and it may be that the optimal transformations and invariances are dataset and task dependant.
In future work, we plan to explore more flexible mechanisms to learn those transformations and also explore the use of equivariant representations.



\bibliography{refs.bib}
\bibliographystyle{arxivtemplate/arxiv.bst}
\vfill\pagebreak
\appendix

\section{Implementations Details}
\label{apndx:implementation}

In this appendix section we provide the implementation details for MSN pre-training and evaluation.

\subsection{MSN Pre-training}

We adopt similar hyper-parameter settings that have previously been reported in the self-supervised literature for training Vision Transformers~\citep{caron2021emerging,chen2020exploring}.
Specifically, for pre-training, we use the AdamW optimizer~\citep{loshchilov2017decoupled} with a batch-size of 1024.
We linearly warm up the learning-rate from  to  during the first 15 epochs, and decay it following a cosine schedule thereafter.
To construct the different image views, we apply the SimCLR data augmentations of~\citet{chen2020simple} to each sampled image; namely random crop, horizontal flip, color distortion, and Gaussian blur.
For each sampled image, we generate one large anchor view of size  pixels, and apply a random mask with a pre-specified masking ratio (0.15 for the ViT-S/16, 0.3 for the ViT-B/16 and ViT-B/8, and 0.7 for the ViT-L/7 and the ViT-B/4).
For each sampled image, we also generate 10 small focal anchor views of size  pixels.
We use a temperature of  for the anchor network, and a temperature of  for the target network.
Following the DINO method of~\citet{caron2021emerging}, we update the target network via an exponential moving average of the anchor network with a momentum value of , and linearly increase this value to  by the end of training.
Similarly, following~\citet{caron2021emerging}, weight decay is set to  and increased to  throughout training via a cosine schedule.
By default, we set the {\sc me-max} regularization weight  to  and apply Sinkhorn normalization to the targets~\citep{caron2020unsupervised} to avoid having to tune the {\sc me-max} regularization weight; however, in general, we observe stronger MSN performance when omitting Sinkhorn normalization (see Appendix~\ref{apndx:additional_ablations}).
We train with a 3-layer projection head with output dimension 256 and batch-normalization at the input and hidden layers, and use 1024 prototypes of dimension 256.
We observe that using more prototypes has little effect on training, but using too few prototypes can hurt performance (see Appendix~\ref{apndx:additional_ablations}).
We discard the projection head during evaluation, and always use the representations computed from the output of the target encoder trunk for evaluation.

\subsection{Low-Shot Evaluation}

To avoid overfitting, we freeze the weights of the pre-trained model and train a linear classifier on top using 1, 2 or 5 labeled samples per class.
Specifically, we take a single center crop of each labeled image, extract its representation using the pre-trained model, and then train a classifier on these representations using L-regularized logistic regression.
Following~\citep{caron2021emerging}, we use the {\tt cyanure} package~\citep{mairal2019cyanure} to run logistic regression on the extracted representations.
This objective is smooth and strongly-convex (i.e., has a unique minimizer) and can therefore be efficiently solved for using the {\tt cyanure} python numerical solver on a single CPU core. All low-shot evaluations (including the 1\% ImageNet-1K evaluation) are computed with this procedure, except for models pre-trained using MAE~\citep{he2021masked}, which benefit from using partial fine-tuning~\citep{he2021masked}.

Partial fine-tuning corresponds to fine-tuning the last block of the pre-trained model along with a linear head. MAE benefits from partial fine-tuning, but for sufficiently large models, such as the ViT-H/14, this leads to significant overfitting in the low-shot regime. Our results in Table~\ref{tb:lowshot_imagenet} and Figure~\ref{fig:lowshot} report the best performance across evaluation methods for MAE. In particular, all the MAE results are obtained via partial fine-tuning, except for the 1 image per class setting, and all results with the ViT-H/14 architecture, which use a linear head. We compare both protocols in more detail in Appendix~\ref{apndx:additional_ablations}. 

\subsection{Linear Evaluation}
For linear evaluation, we use a similar procedure as~\citet{he2021masked}.
Specifically, we use a large batch-size of 16,384 images and train a linear classifier for 100 epochs using a learning rate of , and decay it following a cosine schedule.
We only apply basic data augmentations; namely, random resized crops to a resolution of  pixels, and random horizontal flips.
We also L-normalize the representations before feeding them into the linear classifier, and optimize the classifier weights using SGD with Nesterov momentum.
We do not apply any weight-decay and do not use any warmup.

\subsection{Fine-Tuning Evaluation}
We follow the common practice for fine-tuning SSL pre-trained ViT models.
Specifically, we follow the setup of~\citep{touvron2021training,bao2021beit,he2021masked}.
We fine-tune a pre-trained ViT model for 100 epochs on the full supervised ImageNet-1K training data set using the AdamW~\citep{loshchilov2017decoupled} optimizer.
We use a batch size of 1024 with a learning rate of .
The learning rate is linearly warmed-up during the first 5 epochs and decayed with a cosine schedule thereafter.
A layer-wise decay of  is also applied, along with the data augmentations defined by RandAugment(, ) ~\citep{cubuk2019autoaugment}. We additionally use label smoothing set to , mixup~\citep{zhang2017mixup} set to , cutmix~\citep{yun2019cutmix} set to , and drop path set to .

\subsection{Transfer Learning}

\subsubsection{Linear Evaluation}
When performing linear evaluation for transfer learning, we freeze the weights of the ImageNet-1K pre-trained model and optimize a linear classifier on top. We resize each downstream image to  pixels, and take a single center crop of size  pixels. Next, we extract a representation of each image using the pre-trained model, and subsequently train a classifier on top using L-regularized logistic regression. 

\subsubsection{Fine Tuning}
When performing end-to-end fine-tuning for transfer learning, we follow the protocol of DeiT and DINO~\citep{touvron2021training,caron2021emerging}.  Models transferred to CIFAR10 and CIFAR100 are fine-tuned for 1000 epochs using a batch size of 768 and a learning rate of . Models transferred to iNat18 and iNat19 models are fine-tuned for 300 epochs using a batch size of 1024 and a learning of .
All transfer fine-tuning experiments use the data augmentations defined by RandAugment(, )~\citep{cubuk2019autoaugment}.
We also use label smoothing set to , mixup~\citep{zhang2017mixup} set to , cutmix~\citep{yun2019cutmix} set to , and drop path set to .
The learning rate is linearly warmed-up during the 5 first epochs and decayed with a cosine schedule thereafter.


\section{Theoretical Guarantees}
\label{apndx:theory}

In this section we describe how MSN pre-training provably avoids representation collapse.

Recall that in each iteration of pre-training, we sample a mini-batch of  images, and generate  anchor views of each image.
Here we show that MSN is guaranteed to avoid the trivial collapse of representations under the following assumption.

\begin{assumption}[Target Sharpening]
\label{ass:sharp}
The target  is sharpened, such that it is not equal to the uniform distribution.
\end{assumption}
\begin{proposition}[Non-Collapsing Representations]
\label{prop:collapse}
Suppose Assumption~\ref{ass:sharp} holds.
If  is such that the representations collapse, i.e.,  for all  and , then  for all .
\end{proposition}
\begin{proof}
For L-normalized representations and prototypes, the prediction  corresponding to the  view of the  image in the mini-batch is given by

where  is the prototype matrix with  learnable prototypes, each of dimension , and  is a scaler temperature.
Since  for all  and , it holds that , and therefore .
Now consider two separate cases.

{\bf Case 1:} The predictions are equal to the uniform distribution, i.e., , where  is the K-dimensional vector with each entry equal to . In that case, since, by Assumption~\ref{ass:sharp}, the targets  are sharpened such that they are not equal to the uniform distribution, it follows that , and hence .

{\bf Case 2:} The predictions are not equal to the uniform distribution, i.e., . In that case, we have that the average prediction across all the anchor views  is also not equal to the uniform distribution; i.e., , and hence .
\end{proof}

Proposition~\ref{prop:collapse} provides a theoretical guarantee that MSN is immune to the trivial collapse of representations.
In short, the underlying principle is that entropy maximization encourages the anchor predictions to utilize the full set of prototypes, thereby preventing collapse to a non-uniform distribution, while target sharpening encourages the anchor predictions to be confident, thereby preventing collapse to the uniform distribution.

Note that the sharpening mechanism defined in Section~\ref{sec:methodology} (i.e., applying a temperature  in the target network softmax) may not always satisfy Assumption~\ref{ass:sharp}, unless one introduces a simple tie-breaking rule.
In practice, such a rule is not necessary as the targets never become uniform (since we apply sharpening from the start of the training), although, it is important to use a sufficiently small temperature value in this case.

\section{Additional Ablations}
\label{apndx:additional_ablations}

\subsection{Sinkhorn Normalization}
By default, we set the {\sc me-max} regularization weight  to  and apply Sinkhorn normalization on the targets to avoid having to tune the {\sc me-max} regularization weight.
However, we find that tuning the {\sc me-max} regularization weight and omitting Sinkhorn normalization can result in better performance; cf.~Table~\ref{tb:sinkhorn}.
\begin{table}[h]
    \centering
    \caption{{\bf Effect of Sinkhorn normalization.} We train a ViT-S/16 with a masking ratio of 0.15, and explore the impact of Sinkhorn normalization during pre-training on low-shot performance with 1\% of ImageNet-1K. Tuning the {\sc me-max} regularization weight and omitting Sinkhorn normalization gives better performance.}
    \label{tb:sinkhorn}
    \begin{tabular}{c c c | c}
        \bf Architecture & \bf Target Normalization & \bf{\sc me-max} weight  & \bf Top 1 \\\toprule
        \multirow{3}{*}{ViT-S/16} & Sinkhorn & 1.0 & 66.4 \\
        & None & 1.0 & 60.8 \\
        & None & 5.0 & \bf\cellcolor{fbApp} 67.2 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Number of Prototypes}
By default we train with 1024 prototypes of dimension 256. In this section we explore the effect of the number of prototypes on low-shot performance.
We observe that using more prototypes has little effect on training, but using too few prototypes can hurt performance; cf.~Table~\ref{tb:prototypes}.
\begin{table}[h]
    \centering
    \caption{{\bf Effect of number of prototypes.} We train a ViT-B/16 with a masking ratio of 0.3, and explore the impact of the number of prototypes during pre-training on low-shot performance with 1\% of ImageNet-1K. Using more prototypes has little effect on training, but using fewer prototypes can degrade performance.}
    \label{tb:prototypes}
    \begin{tabular}{c c | c}
        \bf Architecture & \bf Prototypes & \bf Top 1 \\\toprule
        \multirow{3}{*}{ViT-B/16} & 512 & 67.6 \\
        & 1024 & \bf\cellcolor{fbApp} 69.5 \\
        & 2048 & \bf\cellcolor{fbApp} 69.5 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Masked Auto-Encoder Partial Fine-Tuning}
Here we explore the low-shot performance of MAE when relying on alternative evaluation strategies.
\citet{he2021masked} conjecture that using pixel reconstruction in their MAE objective results in encoder representations of a lower semantic level than other methods, which may explain their difficulty in training a linear classifier on the frozen features.
In Table~\ref{tb:mae-eval} we explore the effect of partial fine-tuning on the low-shot performance of pre-trained MAE models.
Partial fine-tuning corresponds to fine-tuning the last block of the pre-trained model along with a linear head on the available labeled samples.
As observed in~\citep{he2021masked}, MAE benefits from partial fine-tuning.
However, for sufficiently large models, such as the ViT-H/14, this leads to significant overfitting in the low-shot regime, where one must instead resort to linear evaluation.
We report the best numbers for MAE across the two low-shot adaptation strategies in Figure~\ref{fig:lowshot}.
\begin{table}[h]
    \centering
    \caption{{\bf MAE low-shot evaluations.} Top-1 low-shot validation accuracy for different training strategies with MAE pre-trained models. Partial fine-tuning corresponds to fine-tuning the last block of the pre-trained model along with a linear head on the available labeled samples. Linear evaluation corresponds to training a linear classifier on top of the frozen pre-trained encoder. MAE benefits from partial fine-tuning, but for sufficiently large models, such as the ViT-H/14, this leads to significant overfitting in the low-shot regime, where one must instead one must resort to linear evaluation.}
    \label{tb:mae-eval}
    \begin{tabular}{l l c c c}
        & & \multicolumn{3}{c}{\bf\small Top 1}\2mm]
        & & & \multicolumn{2}{c}{Eval. Masking Ratio} \\
        \bf\small Alg. & \bf\small Arch. & \bf\small Pre-train Masking Ratio & 0.0 & 0.7 &  \\\toprule
        DINO & ViT-B/16 & 0.0 & 67.0 & 63.1 & -3.9\\\midrule
        \multirow{2}{*}{MSN} & ViT-B/16 & 0.3 & 69.5 & 67.1 & -2.4\\
         & ViT-L/7 & 0.7 & 75.1 & 74.9 & \cellcolor{fbApp}\bf -0.2\\ 
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[t]
  \footnotesize
  \centering
  \caption{{\bf Robustness to missing patches (cosine-similarity).} Average Cosine Distance between masked and unmasked representations of the same image. We compare the representations learned with MSN masked pre-training to those learned with DINO when using a ViT-B/16 encoder. The MSN ViT-B/16 is pre-trained with a masking ratio of 0.3. The cosine distances are computed and averaged over the ImageNet-1k validation set. The cosine similarity between masked and unmasked representations of the same image is higher when pre-training with MSN, supporting the observation that masked-pretraining results in representations that are more robust to patch-removal.\\}
  \label{tb:robust_mask_cosine}
        {\small
       \begin{tabular}{l c c c c c}
        & \multicolumn{5}{c}{\bf Cosine Similarity} \2mm]
        & \multicolumn{5}{c}{Eval. Masking Ratio} \\
        \bf\small Alg. & 0.15 & 0.3 & 0.5 & 0.7 & 0.9 \\ \toprule
        DINO & 0.98 & 0.97 & 0.92 & 0.81 & 0.56\\ 
        MSN & \cellcolor{fbApp}\bf 0.99 & \cellcolor{fbApp}\bf 0.99 & \cellcolor{fbApp}\bf 0.99 & \cellcolor{fbApp}\bf 0.98 & \cellcolor{fbApp}\bf 0.97\\ \bottomrule
    \end{tabular}}
\end{table}

We also report the average cosine distance between masked and unmasked representations of the same image in Table~\ref{tb:robust_mask_cosine}.
As expected, the cosine similarity between masked and unmasked representations of the same image is higher when pre-training with MSN, supporting the observation that masked-pretraining results in representations that are more robust to patch-removal.


\section{Qualitative Analysis}
\label{apndx:qualitative}
We qualitatively investigate the properties of the MSN pre-trained representations.
We follow the RCDM framework~\citep{bordes2021high} and train a conditional generative diffusion model, which maps a learned image representation back to pixel space. Specifically, RCDM takes as input random noise and the representation vector of an image computed by an SSL model (either an MSN pre-trained model or a DINO pre-trained model in this analysis), and aims to reconstruct the image as close as possible to the original one through a diffusion process. 


By using RCDM to sample an image based on its SSL representation, we can visualize how different pre-training strategies affect the degree of information contained in the representation.
Qualities that vary across RCDM samples represent information that is not contained in the pre-trained representation. Qualities that are semantically common across samples represent information contained in the representation.

\subsection{Comparison with DINO}
We apply RCDM on top of either a DINO or MSN pre-trained ViT-B/8 encoder to generate images of resolution  pixels. RCDM is trained using unmasked images processed with the ViT-B/8 encoder. We then use masked images from the validation set at sampling time.

In Figure~\ref{fig:qualitiative_50percent}, we generate samples for RCDM when masking 50\% of the conditioning images.
The first column depicts images from the ImageNet validation set. 
The second column depicts the same image, but with 50\% of the patches masked. The representation of the masked image is used as conditioning for the RCDM diffusion model.
The subsequent columns in Figure~\ref{fig:qualitiative_50percent} show various images sampled from the conditioned RCDM diffusion model.  
We observe that the RCDM samples conditioned on the MSN representations (cf.~Figure~\ref{fig:qualitative_msn}) preserve the semantic category of the masked images, and remain visually close to the original image, despite the missing patches.
By contrast, the samples generated by the RCDM diffusion model conditioned on the DINO representations (cf.~Figure~\ref{fig:qualitative_dino}) are more blurry and do not preserve as well the semantic category of the masked images.

Figure~\ref{fig:qualitiative_80percent} depicts similar visualizations, but with 80\% of the patches masked.
In this case, even with 80\% of the patches missing, samples generated by RCDM conditioned on MSN representations preserve some of the structure in original images (cf.~Figure~\ref{fig:qualitative_msn_high_mask}).
On the other hand, conditioning on DINO representations leads to almost uniform background generation (cf.~Figure~\ref{fig:qualitative_dino_high_mask}). 


\subsection{MSN ViT-L/7 Visualizations}
We apply RCDM on top of the MSN pre-trained ViT-L/7 encoder to generate images with a resolution of  pixels. RCDM is trained using images with 70\% of patches masked. We then use masked images from the validation set (with various masking ratios) at sampling time, see Figures~\ref{fig:qualitiative_vitl7_100percent},~\ref{fig:qualitiative_vitl7_30percent}, and~\ref{fig:qualitiative_vitl7_10percent}.

Visualizations show that MSN  discards instance-specific information such as background, pose, and lighting, while retaining semantic information about the images, even when a large fraction of the patches are masked.

\begin{figure}[t]
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assetsv2/MSN-vitb8_ID.pdf}
        \caption{MSN Representations visualized on ImageNet validation set.}
        \label{fig:qualitative_msn}
    \end{subfigure}\vspace{1ex}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assetsv2/Dino-vitb8_ID.pdf}
        \caption{DINO Representations visualized on ImageNet validation set.}
        \label{fig:qualitative_dino}
    \end{subfigure}
    \caption{{\bf Visualizations of ViT-B/8 pre-trained representations computed from images with 50\% of patches masked.} First column: original image. Second column: image with 50\% of patches masked used to compute representations of an SSL pre-trained ViT-B/8 encoder. Other columns: RCDM sampling from generative model conditioned on SSL representation of masked image.}
    \label{fig:qualitiative_50percent}
\end{figure}
\begin{figure}[t]
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assetsv2/msn_vitb8_id02.pdf}
        \caption{MSN representations visualized on ImageNet validation set.}
        \label{fig:qualitative_msn_high_mask}
    \end{subfigure}\vspace{1ex}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assetsv2/dino_vitb8_id02.pdf}
        \caption{DINO representations visualized on ImageNet validation set.}
        \label{fig:qualitative_dino_high_mask}
    \end{subfigure}
    \caption{{\bf Visualizations of ViT-B/8 pre-trained representations computed from images with 80\% of patches masked.} First column: original image. Second column: image with 80\% of patches masked used to compute representations of an SSL pre-trained ViT-B/8 encoder. Other columns: RCDM sampling from generative model conditioned on SSL representation of masked image.}
    \label{fig:qualitiative_80percent}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assetsv2/msn_sampling_vitl7_100p.pdf}
    \caption{{\bf Visualizations of MSN pre-trained ViT-L/7 representations computed from unmasked images.} First column: original image. Other columns: RCDM sampling from generative model conditioned on MSN representation using a ViT-L/7 encoder. MSN representations are computed from unmasked images. Qualities that vary across samples represent information that the representation is invariant to; e.g., in this case, MSN discards background, pose, and lighting information. Qualities that are common across samples represent information contained in the pre-trained representation.}
    \label{fig:qualitiative_vitl7_100percent}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assetsv2/msn_sampling_vitl7_30p.pdf}
    \caption{{\bf Visualizations of MSN pre-trained ViT-L/7 representations computed from images with 70\% of patches masked.} First column: original image. Second column: image with 70\% of patches masked used to compute representations of an SSL pre-trained ViT-L/7 encoder. Other columns: RCDM sampling from generative model conditioned on SSL representation of masked image. Qualities that vary across samples represent information that the representation is invariant to; e.g., in this case, MSN discards background, pose, and lighting information. Qualities that are common across samples represent information contained in the pre-trained representation.}
    \label{fig:qualitiative_vitl7_30percent}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assetsv2/msn_sampling_vitl7_10p.pdf}
    \caption{{\bf Visualizations of MSN pre-trained ViT-L/7 representations computed from images with 90\% of patches masked.} First column: original image. Second column: image with 90\% of patches masked used to compute representations of an SSL pre-trained ViT-L/7 encoder. Other columns: RCDM sampling from generative model conditioned on SSL representation of masked image. Qualities that vary across samples represent information that the representation is invariant to; e.g., in this case, MSN discards background, pose, and lighting information. Qualities that are common across samples represent information contained in the pre-trained representation. Even with high-masking ratio, MSN retains semantic information about the images.}
    \label{fig:qualitiative_vitl7_10percent}
\end{figure}

\end{document}
