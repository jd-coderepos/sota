\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{threeparttable}
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{subcaption}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{soul}
\usepackage[misc]{ifsym}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage[nopar]{lipsum}
\usepackage{listings}
\usepackage[marginal]{footmisc}

\usepackage{dblfloatfix}
\usepackage{transparent}
\usepackage{arydshln}


\definecolor{myy}{RGB}{126,95,0}
\definecolor{mygray}{gray}{.9}
\definecolor{bblue}{RGB}{30,80,120}
\definecolor{mygray1}{gray}{.7}
\definecolor{ggray}{RGB}{127,127,127}

\newcommand{\pub}[1]{\color{gray}{\tiny{[{#1}]}}}
\newcommand{\baseline}[1]{\color{ggray}{\scriptsize{{#1}}}}
\newcommand{\wwg}[1]{{\textcolor{blue}{[WWG:] #1}}}
\def\1{\mathbbm{1}}


\usepackage{tabulary}
\newcolumntype{I}{!{\vrule width 1pt}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}


\makeatletter
\newcommand{\thickhline}{\noalign {\ifnum 0=`}\fi \hrule height 1pt
	\futurelet \reserved@a \@xhline
}
\makeatother

\newcommand\blfootnote[1]{\begingroup
	\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}

\newcommand{\tf}[1]{\textcolor{red}{[tfzhou: #1]}}
\newcommand{\mj}[1]{\textcolor{blue}{[meijie: #1]}}

\def\pascal{PASCAL VOC 2012}
\def\coco{COCO 2014}

\hyphenation{WSSS FCN CAM RCA}

\usepackage[accsupp]{axessibility}

\def\cvprPaperID{2450} \def\confName{CVPR}
\def\confYear{2022}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation}

\author{Tianfei Zhou,~~Meijie Zhang,~~Fang Zhao,~~Jianwu Li\\
	\small{ Computer Vision Lab, ETH Zurich}~~~\small{ Beijing Institute of Technology}~~~\small{
		Inception Institute of AI}\\
	\small\url{https://github.com/maeve07/RCA.git}
}


\maketitle

\begin{abstract}

		\vspace{-.5em}
	Learning semantic segmentation from weakly-labeled (\eg, image tags only) data  is challenging since it is hard to infer dense object regions from sparse semantic tags. Despite being broadly studied, most current efforts directly learn from limited semantic annotations carried by individual image or image pairs, and struggle to obtain integral localization maps. Our work alleviates this from a novel perspective, by exploring rich {semantic contexts} synergistically among {abundant} weakly-labeled training data for network learning \textbf{and} inference. In particular, we propose \underline{r}egional semantic \underline{c}ontrast and \underline{a}ggregation (RCA)~\blfootnote{ Equal contributions;  Corresponding author: Jianwu Li.}\!. RCA is equipped with a regional memory bank to store massive, diverse object patterns appearing in training data, which acts as strong support for exploration of dataset-level semantic structure. Particularly, we propose \textbf{i) semantic contrast} to drive network learning by contrasting massive categorical object regions, leading to a more holistic object pattern understanding,
	and \textbf{ii) semantic aggregation} to  gather diverse relational contexts in the memory to enrich semantic representations. In this manner, RCA earns a strong capability of fine-grained semantic understanding, and eventually establishes new state-of-the-art results on two popular benchmarks, \ie, PASCAL VOC 2012 and COCO 2014.
\vspace{-1em}

\end{abstract}

\section{Introduction}\label{sec:intro}

Semantic segmentation continues to be a fundamental task in computer vision, with numerous applications in autonomous driving, robotics, human-computer interactions and medical imaging analysis. While fully supervised systems have achieved tremendous progress, they are limited by the availability of pixel-level annotations, often harvested at great cost, even with smart interfaces~\cite{bearman2016s}. Weakly supervised semantic segmentation (\mbox{WSSS}) alternatively investigates whether this task can be adequately addressed with efficient and weak supervisory signals (\eg, image labels~\cite{wei2018revisiting,jiang2019integral,araslanov2020single,li2021group}, scribbles~\cite{lin2016scribblesup,vernaza2017learning,Liang2022TEL}, bounding boxes~\cite{dai2015boxsup,song2019box,lee2021bbam,oh2021background}). This work studies the form of image-level labels which can be obtained effortlessly,  and thus have been widely embraced in mainstream approaches.

In the absence of the true ``{image label}'' to ``{object region}" correspondence in training data, learning to map visual concepts to pixel regions is particularly challenging. The seminal work, \ie, class activation mapping (\mbox{CAM})~\cite{zhou2016learning}, solves this by mining regions from internal activation of an image classifier. However, the technique is prone to give sparse and incomplete object estimations, since the classifier is only driven to activate a small proportion of features with strong discriminative capability. To address this, {a prevalent of subsequent efforts} strive to learn more complete object regions by, \eg, {region growing} to expand initial responses~\cite{kolesnikov2016seed,wang2018weakly,huang2018weakly}, {adversarial erasing} in a hide-and-seek fashion~\cite{hou2018self,Singh_2017,wei2017object,lee2019ficklenet},  {feature enrichment} to collect within-image  contexts~\cite{wei2018revisiting,yao2021non}, {seeking auxiliary  saliency supervisions}~\cite{zeng2019joint,lee2021railroad,xu2021leveraging}, or self-supervised learning with pre-designed pretext tasks~\cite{shimoda2019self,wang2020self,chang2020weakly}.


\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{./Figure/Fig1.pdf}
	\put(-188,101.5){\scriptsize ~(\eg,  \cite{wei2018revisiting,jiang2019integral,araslanov2020single,wang2020weakly,lee2021railroad,xu2021leveraging})}
	\put(-49,101.5){\scriptsize ~(\eg,  \cite{fan2020cian,sun2020mining,li2021group})}
	\vspace{-5pt}
	\captionsetup{font=small}
	\caption{\small \textbf{The main idea} promoted throughout the paper is that semantic contexts subserve localization of individual objects in WSSS. Our RCA thus performs dataset-level relation learning (c) to mine rich contextual knowledge from massive (ideally all) training samples, rather than from an individual image (a) or image pair (b). This enables our model to procure in-depth semantic pattern understanding, improving object localization eventually.}
	\label{fig:motivation}
	\vspace{-10pt}
\end{figure}






Though impressive, these methods use only single-image information for object localization (Fig.~\ref{fig:motivation}~(a)), neglecting inter-image contextual information. Image-level labels not only tell the categories appearing in each individual image, but also  unveil the semantic structure  of all images in the dataset. For each concept (\ie, \texttt{cat} in Fig.~\ref{fig:motivation}), the dataset contains numerous semantically similar but visually different instances; for any two different concepts (\eg, \texttt{cat} and \texttt{dog}), all their instances are semantically different, even though some may look very similar with each other. This \textit{a priori} knowledge should be exploited to gain more accurate semantic pattern understanding.
Though some preliminary attempts~\cite{fan2020cian,zhang2020inter,sun2020mining,zhou2021group} have been made towards this (Fig.~\ref{fig:motivation}~(b)), they focus on  pairwise~\cite{fan2020cian,zhang2020inter,sun2020mining} or quadruplet~\cite{zhou2021group} context modeling in a \textit{limited} number of images, and thus cannot guarantee a sufficient understanding of holistic semantic patterns in the entire dataset. In addition, all these methods favor pixel-wise relation modeling, which is rather difficult due to the lack of proper supervisory signal and causes prohibitive computation cost.




Motivated by above analysis, we propose regional semantic contrast and aggregation (RCA) to maximally exploit  contextual knowledge in visual data (Fig.~\ref{fig:motivation} (c)), aiming for comprehensive object pattern learning as well as effective CAM inference. In lieu of pixel-level relation modeling in~\cite{fan2020cian,sun2020mining,zhou2021group},  RCA prefers \emph{region-aware} representations that are more efficient and robust to noises. In particular, for each mini-batch image, we divide it into categorical  \emph{pseudo} regions according to an intermediate, coarse CAM, which is learned under the supervision of its single-image label. For each pseudo region, RCA establishes  its relations  to regions in all other images to facilitate dataset-level semantic context learning. For feasible computations, we associate RCA with a continuously-updated memory bank, which collects and preserves meaningful region semantics in the dataset as the training goes, and is applicable to both network learning and inference phases. During training, RCA explores semantic relations of regions in each mini-batch and the memory bank from two novel perspectives:
\begin{itemize}[leftmargin=*]
	\setlength{\itemsep}{0pt}
	\setlength{\parsep}{-2pt}
	\setlength{\parskip}{-0pt}
	\setlength{\leftmargin}{-8pt}
\vspace{-4pt}\item \textit{Semantic contrast}, which lets the model learn to discriminate all possible object regions in the dataset, promoting more holistic object pattern understanding. Particularly, for each pseudo region, {semantic contrast} enforces the network to pull its embedding close to memory embeddings of the same category and push apart those of different. Such a contrastive property well complements the classification objective (for each single image) to improve object representation learning.
	
	\item  \textit{Semantic aggregation}, which allows the model to gather dataset-level  contextual knowledge to yield more meaningful object representations. This is achieved via a non-parametric attention module which  summarizes memory representations  for each  image independently. In comparison with conventional \textit{intra-image} context learning schemes~\cite{chen20182,yuan2020object}, our semantic aggregation focuses on  \textit{inter-image}  context mining, and thus is able to capture more informative dataset-level semantics.
	

	
	\vspace{-4pt}
\end{itemize}

These two context modeling schemes  are indispensable to our model. Semantic contrast helps the network to learn more structured object embedding space from a holistic view, while semantic aggregation focuses on  improving  feature representations of each image by collecting diverse semantic contexts. In addition, semantic contrast is essential to maintain unique and informative memory embeddings, which is a prerequisite to yield reliable semantic aggregation. These two components work together to make  RCA a powerful WSSS model (see Table~\ref{table:ablation}).  Our RCA is flexible and can be easily incorporated into existing WSSS models. It shows consistently improved segmentation performance on challenging datasets (\ie, PASCAL VOC 2012~\cite{everingham2010pascal} and COCO 2014~\cite{lin2014microsoft}), on top of state-of-the-art WSSS models (\ie, OAA~\cite{jiang2019integral}, EPS~\cite{lee2021railroad}). 


\noindent\textbf{Main Contributions.} {\textbf{i)}} We study an essential yet long-ignored problem in \mbox{WSSS} to explore rich contexts among weakly labeled training data for network learning. This essentially narrows the gap between image-level semantic concepts and pixel-level  object regions.  Technically, {\textbf{ii)}} we introduce a robust contrastive learning algorithm for semantic contrast, which is able to learn effective representations from imperfect, pseudo region features, as well as   {\textbf{iii)}}  a non-parametric attention model for semantic aggregation to collect rich contextual knowledge from the entire dataset .




\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{./Figure/Fig2.pdf}
	\put(-439,106.5){\small}
	\put(-372,148){\small}
	\put(-290,102.5){\small}
	\put(-152,52.5){\small}
	\put(-253,154){\small MAP}
	\put(-255,145){(\small Eq.~\ref{eq:h})}
	\put(-287,137){\small}
	\put(-126,10){\small}
	\put(-260,116){\small}
	\put(-102,37){\small}
	\put(-300,34){\small}
	\put(-230,12){\small}
	\put(-200,12){\small}
	\put(-42,34){\small{\color{red}}}
	\put(-100,104){\small{\color{red}}}
	\put(-187,90){\small}
	\put(-223,118){\scriptsize}
	\put(-223,109){\scriptsize}
	\put(-223,100){\scriptsize}
	\put(-14,3){\small}
	\put(-80,3){\small}
	\put(-53,90.5){\small(\S\ref{sec:rsc})}
	\put(-199,60){\small(\S\ref{sec:rsa})}
	\put(-205,180){\small(\S\ref{sec:prr})}
	\vspace{-5pt}
	\captionsetup{font=small}
	\caption{\small  Detailed illustration of \textbf{regional semantic contrast and aggregation}. See \S\ref{sec:method} for more details.}
	\label{fig:framework}
	\vspace{-10pt}
\end{figure*}




\section{Related Work}\label{sec:relatedwork}


\noindent\textbf{Weakly Supervised Semantic Segmentation} is gaining popularity due to its practical value in reducing the burden of collecting pixel-level annotations at a large scale required by its fully-supervised counterparts \cite{wang2021hierarchical,zhou2021differentiable,zhou2020motion,zhou2021target,wang2021survey}
Here weak supervision may come in diverse forms, \eg, image-level labels~\cite{wei2016stc,chaudhry2017discovering,zhang2020causal,zhou2021group,wang2021multiple}, scribbles~\cite{lin2016scribblesup,vernaza2017learning}, bounding boxes~\cite{dai2015boxsup,khoreva2017simple,song2019box,oh2021background},  point clicks~\cite{bearman2016s,ke2021universal}. Among them, image-level labels gain the most attention due to its minimal annotation demand. However, since only the presence or absence of particular semantics is indicated, the task becomes extremely challenging. The pioneering work of~\cite{zhou2016learning} proposes to obtain coarse object localization maps (\ie, CAMs) from CNN-based image classifiers as seeds to generate pixel-level pseudo segmentation labels. Follow-up works expand coarse CAMs to obtain full extents of object regions by region growing~\cite{kolesnikov2016seed,wei2018revisiting,huang2018weakly}, using stochastic inference~\cite{lee2019ficklenet}, incorporating self-supervised learning~\cite{chang2020weakly, shimoda2019self, wang2020self}, exploring boundary constraints~\cite{chenweakly, lee2021railroad}, or alternatively mining and erasing object regions~\cite{hou2018self, wei2017object, li2018tell}. 


Past efforts only consider each image individually, ignoring the rich semantic context across different training images. Recent works~\cite{fan2020cian, sun2020mining} address cross-image semantic mining by computing semantic co-attention between each pair of images, while~\cite{zhou2021group} further enables high-order semantic mining from more images through a graph neural network architecture. Though impressive, these approaches still consider limited semantic context within a \textit{small} number of images (\ie,  in~\cite{fan2020cian, sun2020mining} and  in ~\cite{zhou2021group}). 
In contrast, our approach takes a further step to explore the learning of \emph{rich} relations from a \emph{large} number of weakly annotated data.  It is equipped with a \emph{pseudo-region} memory bank to store region-level semantic embedding for each category, which enables region-aware semantic contrast and aggregation for more comprehensive object pattern mining. 

\noindent\textbf{Contrastive Representation Learning}  is becoming increasingly attractive due to its great potential for \textit{un-/self-supervised} representation learning~\cite{sohn2016improved,oord2018representation,wu2018unsupervised,tian2020contrastive,chen2020simple,he2020momentum}. These approaches learn to compare samples in order to push apart dissimilar (or \emph{negative}) data pairs while pulling together similar (or \emph{positive}) pairs. Some approaches~\cite{caron2018deep,grill2020bootstrap,chen2021exploring} even achieve compelling performance without using any negative pairs. Beyond image-level instance discrimination, recent efforts~\cite{chaitanya2020contrastive,wang2021dense,xie2021propagate} explore pixel- or patch-level discrimination to learn visual representations that generalize better to downstream dense prediction tasks (\eg, semantic segmentation, object detection). Furthermore, \textit{supervised} contrastive learning has been studied in~\cite{khosla2020supervised} for image recognition and in~\cite{wang2021exploring} for supervised semantic segmentation. These methods extend the self-supervised setup (by leveraging label information) to contrast the set of all samples from the same class as positives against {the} negatives from other classes. Inspired by these advances, our approach performs dense contrative learning to improve object localization ability of neural networks, using \textit{weakly supervised} annotations. Our approach is naturally distinguished from the above dense representation learning methods which either neglect any annotations~\cite{chaitanya2020contrastive,wang2021dense,xie2021propagate} or require pixel-level supervisions~\cite{wang2021exploring}. 



\noindent\textbf{Relational Context Learning} is popular in image and video segmentation to augment feature embedding of each pixel by gathering useful representations from  its contextual pixels~\cite{zhang2018context,fu2019dual} or regions~\cite{chen20182,yuan2020object}. However, these methods are limited to capturing local contexts within each individual image, ignoring potential semantic contexts across different images. In sharp contrast, our semantic aggregation mines relational semantics across all images of  the entire dataset to gain more informative context learning.

\noindent\textbf{Non-Parametric Memory Bank} has been found feasible to remember a massive number of samples for learning good representations~\cite{wu2018unsupervised,he2020momentum,wang2020cross,wang2021exploring,misra2020self}. Our memory bank is inspired by these efforts, which however, is  unique in that \emph{i)} it stores consistent and expressive region-level semantics inferred from image-level labels; \emph{ii)} more importantly, it is also kept alive in the inference phase to provide holistic contextual knowledge for network inference.





\section{Our Approach}\label{sec:method}


\subsection{Problem Statement}\label{sec:prob}



\noindent\textbf{Task Setup.}  Following the standard setup, each training image  in the dataset  is associated with only an image-level label vector  for  pre-specified  categories. Here,  indicates the presence of class  in  and  otherwise. Given such coarse annotations, most current solutions follow a  two-phase pipeline to solve the task ``\textit{from classification to segmentation}'', \ie, training a \textit{classification network} first for identifying object regions corresponding to each category, which are then refined to produce pseudo segmentation labels as the supervision of a \textit{semantic segmentation network}. 




\noindent\textbf{Previous Solutions to \mbox{WSSS}.} 
Recent approaches~\cite{jiang2019integral,Zhang_2018,lee2021railroad}, in general, derive class-aware attention maps directly from a fully-convolutional network (\mbox{FCN}), which is proven to produce localization maps with the same quality as \mbox{CAM}~\cite{Zhang_2018}. Particularly, for a mini-batch image , its class-aware attention map  is generated as follows:

Here,  is an FCN network, typically corresponding to the convolutional part of a standard classifier (\eg, VGG \cite{simonyan2014very}, ResNet \cite{he2016deep}).   is the dense  embedding of , with  channels and  spatial size.  is a class-aware convolutional layer to produce , with each map  denoting network activation of the -th class. Next, a score vector  is derived from  via a global average pooling (GAP) layer, with  being the un-normalized score of the -th class. Finally,  is used for multi-label classification.


\noindent\textbf{Our Main Idea.} 
With above  descriptions of existing WSSS solutions, we find that they only exploit limited contextual cues in individual images, causing difficulties for more complete understanding of diverse semantic patterns.  To compensate for this limitation, we introduce a novel method, \ie, RCA, to perform semantic contrast and semantic aggregation over \textit{pseudo regions} of a large number of images (ideally the entire dataset). Both semantic contrast and semantic aggregation are supported by an external pseudo-region memory bank. Next, we will first describe the way to build initial pseudo-region representations (\S\ref{sec:prr}) as well as to construct the memory bank (\S\ref{sec:memorybank}). Then, we elaborate on semantic contrast (\S\ref{sec:rsc}) and semantic aggregation (\S\ref{sec:rsa}). The overall pipeline of RCA is illustrated in Fig.~\ref{fig:framework}. 



\subsection{Regional Semantic Contrast and Aggregation}

\subsubsection{Pseudo-Region Representation}\label{sec:prr}

For each mini-batch sample , we convert its dense embedding  (Eq.~\ref{eq:cam}) into a set of categorical region representations based on  (Eq.~\ref{eq:cam}). Particularly, for the -th category that appears in  (\ie, ), its region-level semantic information is summarized to a compact embedding vector  by masked average pooling (MAP)~\cite{siam2019amp}:
\vspace{-3pt}

where  is a binary mask, highlighting only strongly-activated pixels of class  in its activation map (\ie, ).  is an indicator function, and the threshold  is set to the mean value of . Here  is compact and lightweight, allowing for  feasible exploration of its relations with a massive number of pseudo regions mining from other samples.



\subsubsection{Pseudo-Region Memory Bank}\label{sec:memorybank}
Taking the inspiration from~\cite{xiao2017joint,wu2018unsupervised}, we setup a non-parametric and dynamic memory bank for RCA to store dataset-level regional semantic information. In particular, the memory bank  consists of  dictionaries,  \ie, , each for one category. Each  entry of  denotes a holistic region-aware representation   of the -th category in image  observed in the whole learning phase. At each training step, the memory bank will be  updated during backward propagation to involve new observations. In particular, the current feature vector  (Eq.~\ref{eq:h}) of image  will be smoothly updated into the memory representation  as follows:
\vspace{-3pt}

where  is the momentum for memory evolution.  We update  when the -th class appears in  (\ie, ) and its classification score is higher than a threshold , \ie, . Otherwise, we keep  as it was.








\noindent\textbf{Memory Mechanism Discussion.} 
Though memory bank has been widely utilized in recent methods~\cite{xiao2017joint,wu2018unsupervised,he2020momentum}, ours shows several unique and appealing characteristics that could lift more advantages to the task of \mbox{WSSS}. \textbf{First}, the memory is compartmentalized enough to compress each potential semantic hypothesis (\ie pseudo-region embedding) in each training sample individually and is able to well encode diverse semantic patterns of each category within weakly-labelled visual data; \textbf{Second}, the momentum updating scheme (Eq.~\ref{eq:momentum}) not only helps to gain \textit{consistent} memory features for semantic contrast (\S\ref{sec:rsc}) as ~\cite{wu2018unsupervised,he2020momentum}, but more crucially, offers \textit{comprehensive} representations that can accurately describe object semantics. More concretely, Eq.~\ref{eq:momentum} accumulates all intermediate states (\eg, \{\}) of each object region produced by the image classifier at different training epochs. These states have shown to be well complementary with each other~\cite{jiang2019integral}, and as a result of Eq.~\ref{eq:momentum}, each memory feature  will be gradually promoted to capture a more complete object region as the training goes. This eventually results in informative memory representations after training, which can be leveraged as reliable  contexts for  semantic aggregation (\S\ref{sec:rsa}). 


 


\subsubsection{Regional Semantic Contrast (RSC)} \label{sec:rsc}

We perform semantic contrast over \textit{pseudo}-region semantics for learning more discriminative dense representations. For each categorical pseudo-region embedding  (Eq.\!~\ref{eq:h}) in image , our objective is to increase its similarities to memory features   of the same class, while reducing the similarities to features   of different classes. We achieve this via a region-aware contrastive loss:
\vspace{-3pt}

where  is a temperature hyper-parameter scaling the distribution of distances, and  is the dot product between -normalized  and  (\ie, cosine similarity).

Eq.~\ref{eq:pNCE} falls into the regime of supervised contrastive learning~\cite{khosla2020supervised}, \ie, the labels of {} are given. Differently, in our context, the labels are weak and noisy, posing great challenges  to learn robust representations. 
To alleviate this problem, we develop \emph{region mixup} to regularize Eq.~\ref{eq:pNCE} to learn effective region representations, even from noisy samples. More specifically, for each region  in , we create a mixed region by linearly combining it with a region  in another mini-batch image. Here we assume that  regions  and  are from different categories, \ie, . The embedding of the mixed region  is computed as:
\vspace{-3pt}

where the coefficient  follows a Beta distribution  with two shape parameters set to a same  \cite{zhang2018mixup}. Then, we define a new region mixup contrastive loss:

It computes two  losses with respect to  and , which are  combined  by the same weight  used for region mixup (Eq.~\ref{eq:mixup}). Eq.~\ref{eq:mixup-nce} encourages the network to learn relative similarities for mixed regions, regularizing the model to learn robust representations from label-imperfect samples.



\subsubsection{Regional Semantic Aggregation (RSA)}\label{sec:rsa}

Context is widely recognized to be significant for pixel understanding~\cite{zhang2018context,yuan2020object,jin2021mining}, but prior approaches focus on intra-image context modeling, ignoring rich and valuable inter-image contexts. To alleviate this, we devise semantic aggregation to exploit dataset-level context cues in the memory bank for enhancing semantic understanding.  As stated in \S\ref{sec:memorybank}, our memory bank offers massive  signatures of semantic regions. While a large-scale memory bank could benefit semantic contrast~\cite{he2020momentum}, it contains over-complete (or redundant) representations and some are even noisy, making accurate context learning difficult. In addition, directly aggregating large-scale representations is computationally expensive, and  will greatly slow down the learning and inference procedures. 

To address these problems, we  compress the over-complete memory representations into a compact set of representative prototypes. For each class , we do -means clustering over all features  in  to obtain  prototype vectors (\ie, class centroids), organized in a matrix form  .  Here we use multiple prototypes (\ie, ) for each class to account for significant intra-class variations.
Next, all the categorical prototypes derived from the memroy bank  are concatenated together, delivering a holistic prototypical representation . Then, for each mini-batch image  with feature  (Eq.~\ref{eq:cam}), {we  calculate its affinity matrix  with the prototypical representation  as follows:}

where  and  are flattened into matrix representations for computational convenience.  indicates  matrix multiplication.  normalizes each row of the input.  Each entry in  reflects the normalized similarity between each row (\ie, feature) in  and each column (\ie, prototype) in . Based on the affinity matrix,  the contextual summaries for the feature embedding  {w.r.t.}  the prototypical representation  can be computed:

where  denotes an enriched feature representation of , which is further reshaped into . Finally, we concatenate   and the original feature  together:
\vspace{-3pt}

Here,   not only encodes intra-image local contexts in , but also captures inter-image global  contexts  in , thus enriching the representability for semantic understanding. 

\subsubsection{Class Activation Map Prediction}
Finally,  is fed into another class-aware convolutional layer   (Eq.~\ref{eq:cam}) to produce the final activation maps :
\vspace{-3pt}




\subsection{Detailed Network Architecture}\label{sec:arch}

Our classifier is comprised of four major components: \textbf{i)} The {backbone network}  (Eq.~\ref{eq:cam}) maps an input image  into a convolutional representation . Any FCN network can be used here, and we use two popular ones,  \ie, VGG16~\cite{simonyan2014very} and ResNet38~\cite{he2016deep}, for fair comparison with existing approaches. \textbf{ii)} The {class-wise convolutional layer}  (Eq.~\ref{eq:cam})  produces a class-aware attention map from feature embeddings. In our network, two independent  are used in Eq.~\ref{eq:cam} and Eq.~\ref{eq:o}, respectively. Each  is implemented as a  convolutional layer. \textbf{iii)} The {memory bank}   stores all region patterns in training data. Note that the memory bank is  removed at the inference phase, with only compressed global prototypical representations kept instead. This reduces the cost to maintain a large memory bank  during model deployment. \textbf{iv)} The loss function of our classifier is as follows:
	\vspace{-3pt}
	
	where each image  is supervised by the combination of three losses. The first term  is the region mixup contrastive loss (Eq.~\ref{eq:mixup-nce}), which is computed as the average loss of all regions appearing in . The second one is {an auxiliary cross-entropy loss}  for supervising the intermediate CAM prediction  (Eq.~\ref{eq:cam}), while the third loss is the main cross-entropy loss  imposing on the final CAM prediction  (Eq.~\ref{eq:o}).  The coefficients  and  balance the three  terms.
	
	



\section{Experiment}

\subsection{Experimental Setting}\label{sec:settings}

\noindent\textbf{Dataset.} The experiments are conducted on two datasets:\!\!
\begin{itemize}[leftmargin=*]
	\setlength{\itemsep}{0pt}
	\setlength{\parsep}{-2pt}
	\setlength{\parskip}{-0pt}
	\setlength{\leftmargin}{-10pt}
\vspace{-6pt}\item \textbf{PASCAL VOC 2012}~\cite{everingham2010pascal} is a gold standard benchmark for WSSS. It contains  images, which are split into  for , respectively. It provides pixel-level annotations for   categories. As common practices~\cite{huang2018weakly,lee2019ficklenet,zhang2020causal}, we use additional  images~\cite{hariharan2011semantic} for training.
	
	\item \textbf{\coco}~\cite{lin2014microsoft}  is a more challenging dataset, containing complex contextual interactions of  object classes, which attracts interests to verify the performance of our model in this dataset. We follow the official setting to use K images for \texttt{train} and K images for \texttt{val}. 
	\vspace{-4pt}
\end{itemize}



\definecolor{Gray}{gray}{0.5}
\newcommand{\demph}[1]{\textcolor{Gray}{#1}}
\definecolor{Highlight}{HTML}{39b54a}  \renewcommand{\hl}[1]{\textcolor{Highlight}{#1}}
\definecolor{mygreen}{HTML}{39b54a}  \newcommand{\reshl}[2]{
	\textbf{#1} \fontsize{7.5pt}{1em}\selectfont\color{mygreen}{ \textbf{#2}}
}

\begin{table}[t]
	\centering
	\small
	\tablestyle{1pt}{1.05}
	\begin{tabular}{|x{70}||x{80}|x{70}l|}
		\thickhline
&  \multicolumn{2}{c}{mIoU (\%)} & \\ \cline{2-4} 
\multirow{-2}{*}{variant} &  pseudo label  (\texttt{train})  & segmentation (\texttt{val}) & \\ \hline\hline	
		

		\demph{OAA}& \demph{-} & \demph{65.2}{} &\\ \hline
		
		OAA
		& {68.2} & {67.7} &  \\
		{w/} RSC (\S\ref{sec:rsc})
& \reshl{69.5}{1.3} & \reshl{69.3}{1.6} & \\ 
		{w/}  RSA (\S\ref{sec:rsa})
		& \reshl{68.5}{0.3} & \reshl{68.6}{0.9} & \\
		{w/}  RSC  \textit{and} RSA & & &\\
		(full model)& \multirow{-2}{*}{\reshl{71.4}{3.2}} & \multirow{-2}{*}{\reshl{70.6}{2.9}} & \\
		\hline
	\end{tabular}
	\vspace{-6pt}
	\captionsetup{font=small}
	\caption{\small\textbf{Ablation study} on  VOC 2012~\cite{everingham2010pascal}.
		``pseudo label'': generated pseudo labels on the \texttt{train} set; ``segmentation'': segmentation results on the \texttt{val} set.
	}
	\vspace{-10pt}
	\label{table:ablation}
	
\end{table}



\noindent\textbf{Evaluation Protocol.} We evaluate RCA in terms of \textbf{i)} semantic segmentation on  VOC 2012 \texttt{val}/\texttt{test} and COCO 2014 \texttt{val}, and \textbf{ii)} quality of generated pseudo segmentation labels on  VOC 2012 \texttt{train}. As conventions~\cite{jiang2019integral,lee2021railroad}, mean intersection-over-union (mIoU) is used as the  metric in both cases. The scores on VOC 2012 \texttt{test} are obtained from the official evaluation server.






\noindent\textbf{Implementation Details.} As stated in~\S\ref{sec:arch}, we test two commonly used backbones (\ie, VGG16 \cite{simonyan2014very}, ResNet38 \cite{he2016deep}) in RCA for the experiments. The weights of the backbones are loaded from ImageNet pre-trained weights. RCA is trained using the SGD optimizer with batch size {}, momentum {} and weight decay 5e-4. The initial learning rates are set to 1e-3 for the backbone and 1e-2 for other components, which are reduced by  per five epochs. We warm up the network in the first epoch by  using the cross-entropy losses only in Eq.~\ref{eq:loss}, \ie, . The network is trained  for  epochs in total.  For VOC 2012, we use an adaptive memory size for each class to store all region embeddings in the dataset, while for COCO 2014, the per-class memory size is set to  to avoid significant memory consumption. The -means prototype clustering in \S\ref{sec:rsa} is performed only once at the beginning of each epoch, and the per-class  prototype number is set to  by default. For the hyper-parameters, we empirically set the threshold ,  momentum , shape parameter ,  weights  and  to {}, {}, ,  and {}, respectively. 


Once the classifier is well trained, we generate class-aware attention maps  (Eq.~\ref{eq:o}) for each training image and regard them as foreground seeds. In line with~\cite{wu2021embedded,xu2021leveraging,lee2021railroad,jiang2019integral,li2021group}, we also compute a saliency map for each image using off-the-shelf models to estimate background cues. The final pseudo labels are obtained by combining the foreground and background cues together~\cite{jiang2019integral,li2021group}. Finally, with the pseudo masks as the supervision, we train  DeepLabV2~\cite{chen2017deeplab} using  the default hyper-parameter setting in~\cite{chen2017deeplab}. Dense CRF \cite{krahenbuhl2011efficient} is used as a post-processing routine to refine segmentation boundaries, as in \cite{lee2021railroad,wu2021embedded,li2021pseudo,xu2021leveraging,zhang2021complementary}.





\noindent\textbf{Baselines.} 
RCA is flexible and can be easily incorporated into most  WSSS models. In the experiments, we evaluate RCA based on two baselines, \ie, OAA~\cite{jiang2019integral} (due to its popularity) and EPS~\cite{lee2021railroad} (due to its overall best performance). For the conventional OAA, we build a stronger baseline OAA, by replacing its  saliency model with~\cite{liu2019simple}, which is widely-used by recent approaches~\cite{wu2021embedded,xu2021leveraging}.  EPS is the leading WSSS model nowadays; we use it to validate the efficacy of RCA, even with a strong baseline. 


\noindent\textbf{Reproducibility.} Our network is implemented in PyTorch and trained on four \mbox{NVIDIA} V100 cards. Testing is conducted on a single \mbox{NVIDIA} \mbox{RTX2080Ti} card. 




\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{./Figure/prototype.pdf}
	\vspace{-18pt}
	\captionsetup{font=small}
	\caption{\small \textbf{Visualization of affinity  (Eq.~\ref{eq:affinity})}. Each heatmap correspondes to a column in matrix , which is a dot-product between  a particular prototype with image feature . See \S\ref{sec:ablation} for details.}
	\label{fig:affinity}
	\vspace{-5pt}
\end{figure}

\subsection{Diagnostic Experiment}\label{sec:ablation}

We first ablate the core designs of RCA in terms of pseudo label quality on  {VOC} 2012 \texttt{train}. \mbox{VGG16} is used as the classification backbone by default.

\noindent\textbf{Semantic Contrast and Semantic Aggregation.} We  investigate the necessity to learn dataset-level visual contexts for WSSS. Table~\ref{table:ablation} summarizes the results. \textbf{First}, the variant ``{w/} RSC'' significantly improves against OAA in both pseudo label (\ie, ) and segmentation (\ie, ) performance, proving that by contrasting massive object regions, our model fulfills the goal of  more comprehensive object pattern understanding. \textbf{Second}, ``{w/} RSA'' only achieves marginal  performance gains. However, when integrating it with RSC together, our full model (\ie, ``{w/} RSC \textit{and} RSA'') achieves remarkable improvements in comparison with ``{w/} RSC'' ( vs  for pseudo label,  vs  for segmentation). This reveals that RSC, which helps to obtain informative memory representations, is essential for RSA to perform  reliable context aggregation.

To gain more insights into RSA, we visualize feature-prototype affinity  (Eq.~\ref{eq:affinity}) in Fig.~\ref{fig:affinity}. We see that our prototypes are able to attend to semantically meaningful regions, which could benefit object localization.






\noindent\textbf{Region Mixup.} The following table ablates the design of region mixup in \S\ref{sec:rsc}:
\vspace{-6pt}
\begin{table}[H]
	\small
	\centering	
		\tablestyle{1pt}{1.05}
		\begin{tabular}{|z{40}|x{90}x{90}|}
			\thickhline
variant 
			& {w/o} region mixup (Eq.~\ref{eq:pNCE}) & {w/} region mixup (Eq.~\ref{eq:mixup-nce})  \\ \hline		 
			mIoU (\%)           						 
			& 70.6 & 71.4  \\ \hline		 
	\end{tabular}
	\vspace{-10pt}
\end{table}
\noindent We find that after dropping region mixup, the mIoU score reduces by . This result reveals that region mixup  indeed helps the model learn more robust representations from noisy data (\ie, pseudo regions), leading to more accurate semantic understanding.

\noindent\textbf{Memory Updating Coefficient .} The table below shows accuracy of generated pseudo segmentation labels with different updating coefficients (Eq.~\ref{eq:momentum}):
\vspace{-6pt}
\begin{table}[H]
	\small
	\centering	
		\tablestyle{7pt}{1.05}
		\begin{tabular}{|r|cccccc|}
			\thickhline
coefficient                                    
			& 0  & 0.5 & 0.8 & 0.9 & 0.99 & 0.999\\ \hline		 
			mIoU (\%)           						 
			& 69.9 & 70.9 & 71.2 & 71.2 & 71.4 & 70.9 \\ \hline		 
	\end{tabular}
	\vspace{-10pt}
\end{table}
\noindent The optimal value is  (our default). Moreover, RCA is robust when  is in , showing that it is beneficial to update the memory in a relatively slow speed, but not too slow (\ie, ). When  is too small, the performance degrades; at the extreme of \emph{no momentum} (\ie, ), the model significantly  degrades. These results support our discussions in \S\ref{sec:memorybank} that momentum updating helps to earn more consistent and comprehensive memory representations, providing powerful assistance for both semantic contrast and semantic aggregation.

\noindent\textbf{Prototype Number .} The following table ablates the role of prototype number  in semantic aggregation (\S\ref{sec:rsa}):
\vspace{-6pt}
\begin{table}[H]
	\small\centering
		\tablestyle{8pt}{1.05}
		\begin{tabular}{|r|cccccc|}
			\thickhline
                                   & 1  & 10 & 20 & 50 & 100 & all \\ \hline		 
			mIoU (\%)           			   & 70.4 & 71.4 & 71.1 & 71.1 & 71.3 & 70.0  \\ \hline		 	
	\end{tabular}
	\vspace{-10pt}
\end{table}
\noindent  Note that for , we  average all the embeddings in each dictionary to obtain a single prototype vector for each category; for the setting ``all'', we use all memory embeddings as the prototypes without clustering. As seen from the table, RCA shows stable performance when  is in . At extreme cases, the model degrades due to severe information loss () or too many noisy embeddings (``all'').






\noindent\textbf{Memory Size.} By default, our memory bank stores all pseudo regions in the  dataset. However, the following table shows that our model is not sensitive to this setting:
\vspace{-6pt}
\begin{table}[H]
	\small
	\centering	
	\tablestyle{1pt}{1.05}
	\begin{tabular}{|z{50}|x{55}x{55}x{55}|}
		\thickhline
memory size 
		& 100 & 500 & all  \\ \hline		 
		mIoU (\%)           						 
		& 70.8 & 71.2 & 71.4  \\ \hline		 
	\end{tabular}
	\vspace{-10pt}
\end{table}
\noindent By storing only  or  region embeddings per class, the performance only degrades very slightly. This reveals that our model is scalable to larger-scale datasets (\eg, COCO 2014), for which we cannot afford caching all embeddings.





\subsection{Comparison with Prior Art}

\noindent\textbf{Object Localization.} Table~\ref{table:cam} reports the results of generated pseudo segmentation labels on  VOC 2012 \texttt{train}. Notably, RCA improves OAA by \textbf{3.2\%} and {\textbf{3.8\%}} when using VGG16 and ResNet38 as the classifier backbones, respectively. It also yields a solid improvement against EPS ( vs ). These results confirm the strong localization capability of our approach.




\noindent\textbf{Semantic Segmentation.} Table~\ref{table:voc} provides the comparison of RCA against representative methods on VOC 2012 \texttt{val} and \texttt{test}. As seen, RCA  brings solid gains over the two baselines (\ie, OAA and EPS). Using VGG16 (or ResNet38) as the classification backbone, RCA improves OAA by {} ({}) on \texttt{val},  {} ({}) on \texttt{test}. Consistent improvements ({}) are also seen  for EPS. In addition, RCA{\baseline{+EPS}} sets a new state-of-the-art.




Table~\ref{table:coco} summarizes the segmentation results on  {COCO 2014}~\cite{lin2014microsoft}. We observe that RCA surpasses  OAA and EPS  by {}  and {}, respectively. Remarkably, RCA{\baseline{+EPS}}, which employs VGG16 as the backbone, outperforms many ResNet-based models (\eg, AuxSegNet \cite{xu2021leveraging}).


\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{./Figure/visual-localization.pdf}
	\vspace{-18pt}
	\captionsetup{font=small}
	\caption{\small\textbf{Visualization of class activation maps} on VOC 2012 \texttt{train}. From left to right: input images, results of OAA, results from  (Eq.~\ref{eq:cam}) and  (Eq.~\ref{eq:o}) of our full model.}
	\label{fig:cam}
	\vspace{-6pt}
\end{figure}


\definecolor{Gray}{gray}{0.5}
\newcommand{\std}[1]{{\fontsize{5pt}{1em}\selectfont ~~}}
\newcommand{\res}[3]{
	\tablestyle{1pt}{1}
	\begin{tabular}{z{16}y{18}}
		{#1} &
		\fontsize{7.5pt}{1em}\selectfont{~({{#3}})}
\end{tabular}}


\begin{table}[t]
	\small
	
	\centering
	\tablestyle{1pt}{1.0}
	\begin{tabular}{|z{90}|x{65}||x{65}|}
		\thickhline
method &
		backbone &
		mIoU (\%)  \\ 
		\hline\hline
		{SS-WSSS~\pub{CVPR20}{~\cite{araslanov2020single}}}
		& {ResNet38} &  {62.2}   \\
		
		{ICD~\pub{CVPR20}~\cite{fan2020learning}}        
		& {VGG16} &  {62.2}  \\
		
		{SubCat~\pub{CVPR20}{~\cite{chang2020weakly}}}   
		& ResNet38 &  {63.4} \\
		
		{CONTA~\pub{NeurIPS20}{~\cite{zhang2020causal}}}   
		& ResNet38 &  {65.4} \\
		
		{GroupWSSS~\pub{TIP21}{~\cite{zhou2021group}}}   
		& VGG16 &  {65.7}\\
		
		{IRNet~\pub{CVPR19}{~\cite{ahn2019weakly}}}    
		& ResNet50 & {66.5} \\ 


		{BES~\pub{ECCV20}{~\cite{chenweakly}}}    
		& ResNet50 &  {67.2}\\
		
		{EDAM~\pub{CVPR21}{~\cite{wu2021embedded}}}   
		& ResNet38 &  {68.1}\\ \hline
		


OAA
		&\multirow{2}{*}{VGG16}  & {68.2}\\ 
		
		{\textbf{RCA}}\baseline{+OAA}
		&  & \reshl{\textbf{71.4}}{3.2} \\ \hdashline
		
		
		OAA
		& \multirow{2}{*}{ResNet38}  & {69.4}\\ 
		
		{\textbf{RCA}}\baseline{+OAA}
		&  & \reshl{\textbf{73.2}}{3.8}  \\ \hline
		


		{EPS~\pub{CVPR21}{~\cite{lee2021railroad}}}  
		& \multirow{2}{*}{ResNet38} & {71.4}\\ 
		
		{\textbf{RCA}}\baseline{+EPS}
		&  & \reshl{\textbf{74.1}}{2.7} \\ \hline
		
	\end{tabular}
	\vspace{-6pt}
	\captionsetup{font=small}
	\caption{\small\textbf{Quantitative performance of pseudo segmentation labels} on  VOC 2012~\cite{everingham2010pascal} \texttt{train}.}
	\vspace{-10pt}
	\label{table:cam}
\end{table}



\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{./Figure/visual-segmentation.pdf}
	\vspace{-18pt}
	\captionsetup{font=small}
	\caption{\small\textbf{Qualitative segmentation results} on VOC 2012 \texttt{val} (left) and COCO 2014 \texttt{val} (right). From left to right: input images, ground-truths, segmentation results of OAA as well as our RCA.}
	\label{fig:vis}
	\vspace{-8pt}
\end{figure*}





\begin{table}[t]
	\small
	\centering
	
	\tablestyle{1pt}{1.0}
	\begin{tabular}{|z{90}||x{65}x{65}|}
		\thickhline
&  \multicolumn{2}{c|}{mIoU (\%)} \\ 
\multirow{-2}{*}{method} &  \texttt{val} & \texttt{test} \\ \hline\hline	
		
		


SSNet~\pub{ICCV19}{~\cite{zeng2019joint}}  
		& {63.3}& {64.3} \\
		RNet~\pub{CVPR19}{~\cite{ahn2019weakly}} 
		& {63.5} & {64.8} \\ 
		CIAN~\pub{AAAI20}{~\cite{fan2020cian}}    
		& {64.3} & {65.3} \\
		FickleNet~\pub{CVPR19}{~\cite{lee2019ficklenet}}   
		& {64.9} & {65.3} \\
		SSDD~\pub{ICCV19}{~\cite{shimoda2019self}}    
		& {64.9}& {65.5} \\
		SEAM~\pub{CVPR20}{~\cite{wang2020self}}    
		& {64.5} & {65.7} \\
		
		SubCat~\pub{CVPR20}{~\cite{chang2020weakly}}    
		& {66.1} & {65.9} \\
		OAA~\pub{ICCV19}{~\cite{jiang2019integral}}   
		& {65.2} & {66.4} \\
		BES~\pub{ECCV20}{~\cite{chenweakly}}   
		& {65.7} & {66.6} \\
		CONTA~\pub{NeurIPS20}{~\cite{zhang2020causal}}    
		& {66.1} & {66.7} \\
		MCIS~\pub{ECCV20}{~\cite{sun2020mining}}   
		& {66.2} & {66.9}\\
		
		ICD~\pub{CVPR20}{~\cite{fan2020learning}}  
		& {67.8} & {68.0} \\
		CPN~\pub{ICCV21}{~\cite{zhang2021complementary}}  
		& {67.8} & {68.5} \\

		NSROM~\pub{CVPR21}{~\cite{yao2021non}}   
		& {68.3} & {68.5} \\
				AuxSegNet~\pub{ICCV21}{~\cite{xu2021leveraging}}  
		& {69.0} & {68.6} \\
		PMM~\pub{ICCV21}~\cite{li2021pseudo} 
		& {68.5} & {69.0} \\
		

				GroupWSSS~\pub{TIP21}{~\cite{zhou2021group}}  
		& {68.7} & {69.0} \\
		EDAM~\pub{CVPR21}{~\cite{wu2021embedded}}   
		& {70.9} & {70.6} \\ 


		SPML~\pub{ICLR21}~\cite{ke2021universal}   & {69.5} & {71.6} \\ \hline
		
		
		OAA
		& {67.7}& {67.4} \\
		
		{\textbf{RCA}}\baseline{+OAA}
		& \reshl{{70.6}}{2.9} & \reshl{{71.0}}{3.6} \\  \hdashline
		
		OAA
		& {68.1} & {68.2} \\
		
		{\textbf{RCA}}\baseline{+OAA}
		& \reshl{{71.1}}{3.0} & \reshl{{71.6}}{3.4}\\  
		
		\hline
		
		EPS~\pub{CVPR21}{~\cite{lee2021railroad}}   
		& {70.9}{} & {70.8}\\ 
		
		{\textbf{RCA}}\baseline{+EPS~\pub{CVPR21}{~\cite{lee2021railroad}}}
		& \reshl{{72.2}}{1.3} & \reshl{{72.8}}{2.0}\\  
		\hline
	\end{tabular}
	\vspace{-6pt}
	\captionsetup{font=small}
	\caption{\small\textbf{Quantitative  performance} on VOC 2012~\cite{everingham2010pascal} \texttt{val} and \texttt{test}. All models use ResNet as the segmentation backbone. ,  and  denote models using VGG16, ResNet38 or ResNet50 as the classification  backbone, respectively.}
	\vspace{-10pt}
	\label{table:voc}
\end{table}


\begin{table}[t]
	\small
	\tablestyle{1pt}{1.0}
	\begin{tabular}{|z{90}|x{65}||x{65}|}
		\thickhline
{method} & {backbone}  &  mIoU (\%) \\ \hline\hline
		BFBP~\pub{ECCV16}{~\cite{saleh2016built}} 		 		
		& VGG16 & {20.4}{} \\ 
		SEC~\pub{ECCV16}{~\cite{kolesnikov2016seed}}  		
		& VGG16 & {22.4}{} \\  	
		DSRG~\pub{CVPR18}{~\cite{huang2018weakly}}    	
		& VGG16 & {26.0}{} \\ 
		IAL~\pub{IJCV20}{~\cite{wang2020weakly}}    		
		& VGG16 & {27.7}{} \\ 
		GroupWSSS~\pub{TIP21}~\cite{zhou2021group} 			
		& VGG16 & {28.7}{} \\ 
		ADL~\pub{PAMI20}~\cite{choe2020attention} 
		& VGG16 & {30.8}{} \\ 
		SEAM~\pub{CVPR20}~\cite{wang2020self}  				
		& ResNet38 & {32.8}{ }\\ 
		CONTA~\pub{NeurIPS20}~\cite{zhang2020causal}  
		& ResNet38 & {32.8}{} \\ 
		AuxSegNet~\pub{ICCV21}~\cite{xu2021leveraging} 
		& ResNet38 & {33.9}{} \\  \hline
		OAA~\pub{ICCV19}{~\cite{jiang2019integral}} 
		& VGG16 & {24.6}{} \\ 
		\textbf{RCA}\baseline{+OAA~\pub{ICCV19}{~\cite{jiang2019integral}}}
		& VGG16 & \reshl{\textbf{26.7}}{2.1} \\ \hline
		EPS~\pub{CVPR21}{~\cite{lee2021railroad}}
		& VGG16 & {35.7}{} \\ 
		\textbf{RCA}\baseline{+EPS~\pub{CVPR21}{~\cite{lee2021railroad}}}
		& VGG16 & \reshl{\textbf{36.8}}{1.1} \\ 
\hline
		
		
	\end{tabular}
	\vspace{-6pt}
	\captionsetup{font=small}
	\caption{\small\textbf{Quantitative performance} on COCO 2014~\cite{lin2014microsoft} \texttt{val}. 	
}
	\vspace{-12pt}
	\label{table:coco}
\end{table}


\subsection{Visualization Result} 

\noindent\textbf{Object Localization.} 
Fig.~\ref{fig:cam} depicts some representative CAM predictions of OAA and RCA for training samples in PASCAL VOC 2012. As observed, our RCA is able to produce more integral object localization results across various challenging situations (\eg, tiny objects, scale variations). In addition, the final CAM predictions (Eq.~\ref{eq:o}) {are} more accurate than the intermediate ones (Eq.~\ref{eq:cam}), demonstrating the effectiveness of our core designs.

\noindent\textbf{Semantic Segmentation.} Fig.~\ref{fig:vis} illustrates some qualitative segmentation results of OAA and  RCA on  VOC 2012 \texttt{val} and COCO 2014 \texttt{val}. We find that RCA achieves more accurate segmentation results than OAA, showing remarkable capabilities in handling complex scenes, such as  small/large objects, multiple instances, occlusions.



























\section{Conclusion}

In this work, we present a novel approach, RCA, to learn semantic segmentation using image-level supervision only. To alleviate the limited available knowledge carried by image labels, our approach explores the possibility to discover rich semantic contexts from weakly-labeled training data for  learning. In particular, RCA is equipped with a continuously updated memory bank for storing massive historical pseudo-region features. The semantic relations between memory contents and mini-batch training samples are sufficiently exploited as additional supervisory signals (by semantic contrast) or holistic contextual cues (by semantic aggregation) to improve network learning and inference. Our approach is effective and principled, with extensive experiments manifesting its leading performance on popular benchmarks, \ie, PASCAL VOC 2012 and COCO 2014.





{
\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
