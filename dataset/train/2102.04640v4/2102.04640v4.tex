\section{Experiment}


\subsection{Dataset}

\noindent We use the following three popular datasets as the benchmark datasets.

\noindent \textbf{Stanford Online Products (SOP) }\cite{Song-DML-CVPR2016} contains 120,053 product images divided into 22,634 classes. The training set contains 11,318 classes with 59,551 images and the rest 11,316 classes with 60,502 images are for testing.



\noindent \textbf{VehicleID }\cite{liu2016deep} contains 221,736 images of 26,267 vehicle categories, where  13,134 categories with  110,178 images are used for training. Following the same test protocol as \cite{liu2016deep}, three test sets of increasing sizes are used for evaluation (termed small, medium, large), which contain 800 classes (7,332 images), 1,600 classes (12,995 images) and 2,400 classes (20,038 images), respectively.


\noindent \textbf{INaturalist }\cite{van2018inaturalist} is a large-scale animal and plant species classification dataset with 461,939 images from 8,142 classes. We follow the  setting from \cite{2020smoothap} by keeping 5,690 classes for training, and 2,452 unseen classes for testing.




\subsection{Experimental Setup}

\noindent We use the convolutional layers of ResNet-50 pretrained on ImageNet \cite{resnet2016} to perform the training. The models are optimized using Adam \cite{adam2014}, we set the initial learning rate , weight decay 4 × . During training, we randomly sample \textit{k} classes and  samples per class to form each mini-batch. Following standard practice, we resize images to 256 × 256, and randomly crop them to 224 × 224 as input. Random flipping () is used during training for data augmentation, and a single center crop of 224 × 224 is used during evaluation. We use 0 as the fixed random seed for all experiments to avoid seed-based performance fluctuations.  During training, we directly set  in Eq.~\ref{g_function} to 0.01 which is explored in \cite{2020smoothap}.

For all the datasets, every instance from each class is used in turn as the query , and the retrieval set  is formed as all the remaining instances. Recall@k (R@k) is adopted  as the main evaluation metric. In order to evaluate the generalization of the model, we also display dists@intra (Mean Intraclass Distance), dists@inter (Mean Interclass Distance) \cite{roth2020revisiting} and Normalized Mutual Information (NMI) \cite{schutze2008introduction} for further performance analysis.  BS represents mini-batch size.


\subsection{Performance of Different PNP Variants}

\noindent In this section, we investigate different variants of PNP on two popular benchmark datasets. The batch size of experiments in Table~\ref{form_PNP} and Table~\ref{form_vehicle} are both 112. We report the best performance of different variants and the impact of hyper-parameters is shown in Fig.~\ref{parameters}. 

We present R@k, Distance and NMI in Table~\ref{form_PNP} and Table~\ref{form_vehicle}. PNP-D consistenly outperforms PNP-O and PNP-I on two benchmark datasets for R@k and achieves larger dists@intra, dists@inter and NMI, which shows better generalization of the model \cite{roth2020revisiting}. Specifically, PNP-D is 2.1\% higher than PNP-O on SOP benchmark and 0.9\% higher on VehicleID for R@1. In contrast, PNP-I is worse than PNP on R@1, and it is about 1\% lower than PNP on SOP benchmark 1-2\% on VehicleID for R@1. 

\begin{figure}[t]

\centering

\includegraphics[width=7.5cm]{distribution.jpg}

\caption {Illustration of the effect of different variants. (a) training and test data (b) The network trained with PNP-I fails to separate all test classes due to the aggregation of all relevant instances. (c) The PNP-D successfully separates the test classes by keeping the intra-class variance.}
\label{toy_distribute}

\end{figure}


PNP-I assigns larger gradients to positive instances which have more negative ones before, thus it will enforce relevant images to gather and result in smaller dists@intra and dists@inter. In contrast, PNP-D assigns smaller gradients to such instances and slowly optimizes them by considering that they have a high probability of belonging to other centers of the corresponding category, and therefore it has larger dists@intra and dists@inter. By this strategy, PNP-D can adaptively achieve “multi-centers” for each category if necessary and automatically adjust the number of clusters. Such strategy also solves the error correction of noise by considering the noisy data as another center.


Previous works have found that larger dists@intra and dists@inter will produce better generalization \cite{roth2020revisiting}.  The results shown in Table~\ref{form_PNP} and Table~\ref{form_vehicle} further confirm such conclusion. Compared to PNP-I, PNP-D has larger dists@intra and dists@inter, and results in larger NMI, which shows better generalization of the model. Similar to \cite{roth2020revisiting}, we conduct one experiment to illustrate the performance of two loss functions.  Specifically, we use a fully-connected network with two layers and 30 neurons for each layer. The dimensions of input and embeddings are both 2D, and the embeddings are normalized on a unit circle. As shown in  Fig.~\ref{toy_distribute} (a), each of four training and test classes contains 150 samples, respectively. We train the networks using PNP-I and PNP-D, respectively. Fig.~\ref{toy_distribute} illustrates the effect of two loss functions. PNP-I fails to separate all test classes in Fig.~\ref{toy_distribute} (b) while PNP-D successfully separates them (with larger intra-class variance) in Fig.~\ref{toy_distribute} (c) . It shows that PNP-D enables the model to capture more information and exhibits stronger generalization to the unseen test classes by retaining the variance within the class in the training process.

\begin{figure}[t]

\centering

\includegraphics[width=7cm]{rank_small.jpg}

\caption{Image retrieval examples on three datasets. Images with green border are positive instances and these with red border are negative ones for the query.}
\label{rank_example}

\end{figure}






\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table}[t]

\liuhao

\centering




\setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{c|cccc}

\bottomrule

Method & R@1 & R@10 & R@100 & R@1K \\ \hline

\multicolumn{1}{l|}{Hist. \cite{2016histogramloss}} & 72.4& 86.1 &94.1 &98.3\\ 

\multicolumn{1}{l|}{Margin \cite{2017margin}} & 72.7& 86.2 &93.8 &98.0\\ 

\multicolumn{1}{l|}{Divide \cite{2019divide}} & 75.9&88.4&94.9&98.1\\ 

\multicolumn{1}{l|}{FastAP \cite{2019fastap}} &76.4 &89.0 &95.1 &98.2\\ 

\multicolumn{1}{l|}{MIC \cite{2019tripleth}} &77.2 &89.4 &95.6 &-\\ 

\multicolumn{1}{l|}{SoftTriplet \cite{2019softtriple}} &78.3 &90.3 &95.9 &-\\ 

\multicolumn{1}{l|}{RankMI \cite{2020RankMI}} &74.3 &87.9 &94.9 &98.3\\ 

\multicolumn{1}{l|}{Blackbox AP \cite{2020blackboxap}} &78.6 &90.5 &96.0 &98.7\\ 

\multicolumn{1}{l|}{Cont. w/M \cite{2020cont}} & 80.6 &91.6 &96.2 &98.7\\ 



\multicolumn{1}{l|}{Pnca++ \cite{2020proxynca++}} &80.7 &92.0 &96.7 &98.9 \\

\multicolumn{1}{l|}{Smooth-AP \cite{2020smoothap}} &80.1 &91.5 &96.6 &99.0 \\ 

\multicolumn{1}{l|}{DCML-MDW \cite{Zheng_2021_CVPR}} &79.8 &90.8 &95.8 &- \\ \hline
\multicolumn{1}{l|}{PNP-D (BS = 112)} & 80.1 & 91.5 & 96.7 & 99.0 \\ 



\multicolumn{1}{l|}{PNP-D (BS = 384)} &\textbf{81.1} &\textbf{92.2} &\textbf{96.8} &\textbf{99.0} \\ \bottomrule


\end{tabular}}
\caption{Performance comparison  on {SOP} (\%).}
\label{sota_sop}

\end{table}


\subsection{Comparison with State-of-the-art}

\noindent We compare PNP loss to the recent AP-based methods and a series of state-of-the-art deep metric learning methods on three standard benchmarks. 
\subsubsection*{SOP}

\indent\setlength{\parindent}{1em}For fair comparison, we use the same setting with \cite{2020smoothap}. Table~\ref{sota_sop} shows the performance on SOP, we observe that PNP-D achieves the state-of-the-art results. In particular, our model outperforms other methods on all evaluation metrics and outperforms AP approximating methods  by 0.5 - 4\% on R@1 when the same batch size (384) and dimension (512) are used. Note that, although current work (Cont. w/M \cite{2020cont}) uses memory techniques to sample from many mini-batches simultaneously for each iteration, our best model still outperforms this method, only using a single mini-batch on each iteration. 

\begin{table}[t]

\liuhao

\centering




\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{c|cc|cc|cc}

\bottomrule

\multirow{2}{*}{\moren{Method}}&\multicolumn{2}{c|}{Small} & \multicolumn{2}{c|}{Medium} &\multicolumn{2}{c}{Large}\\ \cline{2-7}
& R@1& R@5 &R@1& R@5 &R@1& R@5 \\ \hline

\multicolumn{1}{l|}{Divide \cite{2019divide}} & 87.7 &92.9 &85.7& 90.4& 82.9 &90.2\\ 

\multicolumn{1}{l|}{MIC. \cite{2019tripleth}} & 86.9 &93.4&-&-& 82.0 &91.0\\ 

\multicolumn{1}{l|}{FastAP \cite{2019fastap}} &91.9& 96.8& 90.6& 95.9& 87.5& 95.1\\ 

\multicolumn{1}{l|}{Cont. w/M \cite{2020cont}} & 94.7& 96.8& 93.7& 95.8& 93.0& 95.8\\ 

\multicolumn{1}{l|}{Smooth-AP \cite{2020smoothap}} &94.9 & 97.6 & 93.3 & 96.4 & 91.9 & 96.2 \\ \hline

\multicolumn{1}{l|}{PNP-D (BS = 112)}&94.6&97.4&92.9&96.3&91.4&95.9 \\

\multicolumn{1}{l|}{PNP-D (BS = 384)}& \textbf{95.5} & \textbf{97.8} &\textbf{94.2}& \textbf{96.9} & \textbf{93.2} & \textbf{96.6}  \\ \bottomrule


\end{tabular}}
\caption{Performance comparison  on {VehicleID} (\%).}
\label{sota_VID}

\end{table}

\begin{table}[t]

\liuhao


\centering




\setlength{\tabcolsep}{1.8mm}{


\begin{tabular}{c|cccc}

\bottomrule

Method & R@1 & R@4 & R@16 & R@32 \\ \hline

\multicolumn{1}{l|}{Triplet \cite{2017margin}} & 58.1& 75.5 &86.8 &90.7\\ 

\multicolumn{1}{l|}{Proxy NCA \cite{2017pcna}} &61.6&77.4&87.0&90.6\\ 

\multicolumn{1}{l|}{FastAP \cite{2019fastap}} &60.6 &77.0 &87.2 &90.6\\ 

\multicolumn{1}{l|}{Blackbox AP \cite{2020blackboxap}} &62.9 &79.0 &88.9&92.1\\ 

\multicolumn{1}{l|}{Smooth-AP \cite{2020smoothap}} & 65.9 & 80.9 & \textbf{89.8} & \textbf{92.7} \\ \hline

\multicolumn{1}{l|}{PNP-D (BS = 224)} &\textbf{66.6} &\textbf{81.1} &89.7 & 92.6 \\ \bottomrule



\end{tabular}}
\caption{Performance comparison  on {INaturalist} (\%).}
\label{sota_INaturalist}

\end{table}



\begin{table}[t]

\liuhao

\centering




\setlength{\tabcolsep}{4mm}{
\begin{tabular}{c|cccc}

\bottomrule

Method & R@1 & R@10 & R@100 & R@1K \\ \hline

\multicolumn{1}{l|}{Smooth-AP} &73.2&86.4&93.6&97.5\\ 

\multicolumn{1}{l|}{PNP-I} &67.5&82.7&92.3&97.5\\ 

\multicolumn{1}{l|}{PNP-D} &\textbf{73.8}&\textbf{87.1}&\textbf{94.2}&\textbf{98.0} \\ \bottomrule

\end{tabular}}
\caption{Evaluation of robustness on {SOP} (\%).}
\label{robustness_sop}

\end{table}


\begin{table}[t]

\liuhao

\centering




\setlength{\tabcolsep}{2.3mm}{
\begin{tabular}{c|cc|cc|cc}

\bottomrule

\multirow{2}{*}{\moren{Method}}&\multicolumn{2}{c|}{Small} & \multicolumn{2}{c|}{Medium} &\multicolumn{2}{c}{Large}\\ \cline{2-7}
& R@1& R@5 &R@1& R@5 &R@1& R@5 \\ \hline

\multicolumn{1}{l|}{Smooth-AP} &89.9&95.8&88.6&94.3&85.2&93.2\\ 

\multicolumn{1}{l|}{PNP-I}  &86.9&94.0&85.0&92.6&80.8&90.7\\ 

\multicolumn{1}{l|}{PNP-D} &\textbf{92.6}&\textbf{96.3}&\textbf{90.8}&\textbf{95.3}&\textbf{88.6}&\textbf{94.6}  \\ \bottomrule

\end{tabular}}
\caption{Evaluation of robustness on {VehicleID} (\%).}
\label{robustness_VID}

\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=8.3cm]{parameters.jpg}
\caption{Impact of different hyper-parameters on SOP and VehicleID. (a) steepness , (b) boundary b, (c) samples per class during mini batch sampling . }
\label{parameters}
\end{figure}








\subsubsection*{VehicleID}

\indent\setlength{\parindent}{1em}We further conduct experiments on VehicleID to verify the performance of PNP on large-scale retrieval datasets. Table~\ref{sota_VID} shows the results on the VehicleID dataset. For the same batch size (384) and dimension (512), we observe that PNP-D again achieves state-of-the-art performance on the large-scale VehicleID dataset on all the evaluation metrics. Such result shows that PNP-D also works better on large-scale datasets. 

\subsubsection*{INaturalist}

As shown in Table~\ref{sota_INaturalist}, our method also outperforms the best method by 0.7\% on R@1 for the experiments on INaturalist with the same batch size (224) and dimension (512). These results demonstrate that PNP is particularly suitable for large-scale retrieval datasets, which demonstrates its scalability to real-world retrieval problems. 




\subsection{Impact of Hyper-parameters}



\noindent To investigate the effect of different hyper-parameter settings, \textit{i.e.}, steepness  in Eq.~\ref{reci}, boundary   in Eq.~\ref{b}, samples per class , we train ResNet-50 on SOP and VehicleID. The random seed is fixed to 0 and the batch size is set to 112 for all experiments in this section.


\noindent \textbf{Steepness }: The influence of  of PNP-D is shown in Fig.~\ref{parameters} (a). We can find that larger  results in larger R@1. It can be explained that larger  corresponds to more stable optimization (correct positive instances with sufficient confidence), and thus achieves better performance. However, when  is too large, the training phrase will crash due to the gradient exploding. Therefore, we use as larger  as possible to get better performance.


\noindent \textbf{Boundary} : The influence of  for PNP-I is shown in Fig.~\ref{parameters} (b).  is the best on SOP while  is the best on VehicleID.  controls the biggest gradient of the PNP-I with respect to , and different datasets require different adjusting range. We report the best value for the comparison of different variants.

\noindent \textbf{Samples Per Class }: The influence of the samples per class  in a mini-batch is shown in Fig.~\ref{parameters} (c). We observe that  = 4 results in the highest R@1. The probable reason is that mini-batches are formed by sampling from each class, where a low value means a larger number of sampled classes and a higher probability of sampling hard-negative instances that violate the correct ranking order. Increasing the number of classes in the batch results in a better batch approximation of the true class distribution, allowing each training iteration to enforce a more optimally structured embedding space. However, too small value ( = 2) leads to worse performance because it can not give enough positive instances to violate the correct ranking order for training. 




\subsection{Evaluation of Robustness}

\noindent In order to evaluate the robustness and the performance of multi-centers per class, we perform extensive experiments on constructed datasets in this section. Specifically, we use the train set of SOP and VehicleID as benchmark datasets and merge every three categories to form one new class with multi-centers (at least three centers per class). As for evaluation, we still use original test set to evaluate the model. During the training, we use PNP-I, PNP-D and Smooth-AP as loss functions. 




The results are shown in Table~\ref{robustness_sop}, \ref{robustness_VID}. Compared with using original training set, these three methods all have performance loss on two benchmark datasets. Generally, PNP-D and Smooth-AP with similar gradient assignment strategy have less performance loss than PNP-I because of their adaptability to multi-centers, and PNP-D achieves the best performance. Specifically, for VehicleID, PNP-D exceeds the performance of Smooth-AP by 2 - 3\% and PNP-I by a significant 6 - 8\% on R@1. 


The above results show better robustness of PNP-D. Noisy data is inevitable in the real-world data and it will leads to the instability of training. The above experiments show that PNP-D can achieve comparable performance, even if there are a lot of wrong annotations in datasets. This is because PNP-D can automatically remove the interference of noisy data by putting it in a single center.



