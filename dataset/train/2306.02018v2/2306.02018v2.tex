

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures_Supp/Supp_01_Sketch_seq_text.pdf}
     \vspace{-1.5em}
    \caption{
    \small
    \textbf{Compositional sketch sequence-to-video generation}.
    We showcase five examples, each displaying a video generated from a sequence of sketches and a textual description.
    The final example additionally incorporates a style condition.
    }
    \label{fig:sketch_sequence}
    \vspace{-5mm}
\end{figure}



\appendix
\section*{Appendix}
In this Appendix, we first elaborate on more implementation details (\cref{sec:more_imple_details}) and present more experimental results  (\cref{sec:more_exp_results}).
Next, we provide a section of discussion (\cref{sec:discussion}) on the limitations and potential societal impact of \method. 



\section{More implementation details}
\label{sec:more_imple_details}

\textbf{Pre-training details.}
We adopt AdamW~\cite{loshchilov2017AdamW} as the default optimizer with a learning rate set to $5\times 10^{-5}$.
In total, \method is pre-trained for 400k steps, with the first and second stage being pre-trained for 132k steps and 268k steps, respectively.
In terms of two-stage pre-training, we allocate one fourth of GPUs to perform image pre-training, while the rest of the GPUs are dedicated to video pre-training.
We use center crop and randomly sample video frames to compose the video input whose $F = 16$, $H = 256$ and $W = 256$.
During the second stage pre-training, we adhere to~\cite{huang2023composer}, using a probability of $0.1$ to keep all conditions, a probability of $0.1$ to discard all conditions, and an independent probability of $0.5$ to keep or discard a specific condition.
Regarding the use of WebVid10M~\cite{2021Frozen}, we sample frames from videos using various strides to ensure frame rate equal to 4, aiming to maintain a consistent frame rate.


\textbf{The structure of 3D UNet as $\epsilon_{\theta}(\cdot, \cdot, t)$.}
To leverage the benefits of LDMs pre-trained on web-scale image data, \textit{i.e.}, Stable Diffusion\footnote{https://github.com/Stability-AI/stablediffusion}, we extend the 2D UNet to a 3D UNet by introducing temporal modeling layers.
Specifically, within a single UNet block, we employ four essential building blocks: spatial convolution, temporal convolution, spatial transformer and temporal transformer.
The spatial blocks are inherited from LDMs, while temporal processing blocks are newly introduced.
Regarding temporal convolution, we stack four convolutions with $1 \times 1 \times 3$ kernel, ensuring the temporal receptive field is ample for capturing temporal dependencies;
regarding temporal transformer, we stack one Transformer layer 
and accelerate its inference using flash attention~\cite{dao2022flashattention}.













\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures_Supp/Supp_02_Depth_seq_text.pdf}
     \vspace{-1.5em}
    \caption{
    \small
    \textbf{Compositional depth sequence-to-video generation}.
    We showcase five examples, each displaying a video generated from a sequence of depth maps and a textual description.
    The final example additionally incorporates a style condition.
    }
    \label{fig:depth_sequence}
    \vspace{-5mm}
\end{figure}




\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures_Supp/Supp_03_Motion_Transfer.pdf}
     \vspace{-1.5em}
    \caption{
    \small
    \textbf{Motion transfer.}
We showcase four examples, each displaying a video generated from a single image and motions.
In the first three examples, we transfer the motion patterns in a source video to the generated video by extracting and utilizing motion vectors.
The final example incorporates hand-crafted motions instead.
    }
    \label{fig:motion_transfer}
    \vspace{-5mm}
\end{figure}


\section{More experimental results} 
\label{sec:more_exp_results}


In this subsection, we aim to provide additional experiments that complement the findings presented in the main paper and showcase more versatile controlling cases.




\textbf{Compositional sketch sequence-to-video generation.}
Compositional training with sketch sequences enables \method to possess the ability of generation videos adhering to sketch sequences.
This generation paradigm lays more emphasis on the structure control, which differs from compositional sketch-to-video generation and can be viewed as video-to-video translation.
In Fig.~\ref{fig:sketch_sequence}, we exemplify this capacity.
We observe videos' fidelity to the provided conditions, including texts, sketches and style.



\textbf{Compositional depth sequence-to-video generation.}
Conducting compositional training with depth sequences allows \method to effectively generate videos in accordance with depth sequences.
In Fig.~\ref{fig:depth_sequence}, we illustrate this capability.
Videos generated with \method faithfully adhere to the given conditions, including text prompts, depth maps, and style.


\textbf{Motion transfer.}
Incorporating motion vectors as a composition of videos enables motion transferability.
In Fig.~\ref{fig:motion_transfer}, we conduct experiments to demonstrate such capability.
Through utilizing hand-crafted motion vectors or motion vectors extracted from off-the-shelf source videos, we can transfer the motion patterns to synthesized videos.

\textbf{Text-to-video generation performance.}
Although \method is not specifically tailored for text-to-video generation, its versatility allows \method to perform the traditional text-to-video generation task effectively.
In~\cref{tab:text_to_video}, we follow the evaluation settings in Video LDM~\cite{blattmann2023align_latents} to adopt Fr√©chet Video Distance (FVD) and CLIP Similarity (CLIPSIM) as evaluation metrics and present the quantitative results of text-to-video generation on MSR-VTT dataset compared to other existing methods.
The results in the table demonstrate that \method achieves competitive performance compared to state-of-the-art text-to-video approaches.
In addition,
\method outperforms our first-stage text-to-video pre-training, demonstrating that \method can achieve compositional generation without sacrificing its capability of text-to-video generation.
In the future, we aim to advance \method by leveraging stronger text-to-video models, enabling more flexible and controllable video synthesis.


\begin{table}[t]
\caption{
        \textbf{Text-to-video generation performance} on MSR-VTT.
    }
\centering
\begin{tabular}{l|ccc}
        Method   &  Zero-shot &  FVD $\downarrow$  &  CLIPSIM $\uparrow$ \\
        \shline
        GODIVA~\cite{wu2021godiva}  & No & - &  0.2402  \\
        N{\"u}wa~\cite{wu2022nuwa}  & No & - &  0.2439  \\
        CogVideo (Chinese)~\cite{hong2022cogvideo}  & Yes & - &  0.2614\\
        CogVideo (English)~\cite{hong2022cogvideo}  & Yes & 1294 & 0.2631  \\
        MagicVideo~\cite{zhou2022magicvideo}  & Yes & 1290 & -  \\
        Make-A-Video~\cite{singer2022make-a-video} & Yes & - & \textbf{0.3049}  \\
        Video LDM~\cite{blattmann2023align_latents} & Yes & - & 0.2929  \\
        \shline
        Text-to-video pre-training (First stage)  & Yes & 803 &  0.2876  \\
        \method   & Yes & \textbf{580} &  0.2932 \\
    \end{tabular}
    \label{tab:text_to_video}
    \vspace{-10pt}
\end{table}







\section{Discussion} 
\label{sec:discussion}

\textbf{Limitations.}
Due to the absence of a publicly available large-scale and high-quality dataset, we have developed \method using the watermarked WebVid10M dataset.
As a result, the synthesized videos contain watermarks, which affect the generation quality and lead to less visually appealing results.
Furthermore, in order to reduce the training cost, the resolution of the generated videos is limited to 256$\times$256.
Consequently, some delicate details might not be sufficiently clear.
In the future, we plan to utilize super-resolution models to expand the resolution of the generated videos to improve the visual quality.






\textbf{Potential societal impact.}
\method, as a generic video synthesis technology, possesses the potential to revolutionize the content creation industry, offering unprecedented flexibility and creativity, and hence, promising significant commercial advantages. 
Traditional content creation processes are labor- and cost-intensive. 
\method could alleviate these burdens by enabling designers to manipulate subjects, styles, and scenes through instructions spanning human-written text, and styles and subjects sourced from other images.
Moreover, \method could potentially revolutionize education industry by creating unique and customized video scenarios for teaching complex concepts.



However, it's necessary to note that \method also represents a dual-use technology with inherent risks to society. 
As with prior generative foundation models, such as Imagen Video~\cite{ho2022imagenvideo} and Make-A-Video~\cite{singer2022make-a-video}, \method inherits the implicit knowledge embedded within the pre-trained model (\textit{i.e.}, StableDiffusion) and the pre-trained dataset (\textit{i.e.}, WebVid and LAION). Potential issues include but not limited to the propagation of social biases (such as gender and racial bias) and the creation of offensive content.

Given that \method is a research-oriented project aimed at investigating compositionality in diffusion-based video synthesis, our primary focus lies in scientific exploration and proof of concept. 
If \method is deployed beyond the scope of research, we strongly recommend several precautionary measures to ensure its responsible and ethical use:
\textbf{(i)} Rigorous evaluation and oversight of the deployment context should be conducted;
\textbf{(ii)} Necessary filtering of prompts and generated content should be implemented to prevent misuse.







%
