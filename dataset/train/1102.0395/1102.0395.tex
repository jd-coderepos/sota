\documentclass[11pt,onecolumn,final]{article} \usepackage{a4}
\usepackage{fullpage}
\usepackage{expdlist}
\usepackage[latin1]{inputenc}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{mathrsfs}

\newcommand{\supcat}{\mathcal{S}} 

\newcommand{\LCP}[0]{\mathsf{LCP}}
\newcommand{\SUF}[0]{\mathsf{SA}}
\newcommand{\SF}[1]{S_{\SUF[#1]..n}}

\newcommand{\psv}[0]{\mathtt{PSV}}
\newcommand{\nsv}[0]{\mathtt{NSV}}
\newcommand{\rmq}[0]{\mathtt{RMQ}}

\newcommand{\lca}{\ensuremath{\textsc{Lca}}}
\newcommand{\sroot}{\ensuremath{\textsc{Root}}}
\newcommand{\ssize}{\ensuremath{\textsc{SubtreeSize}}}
\newcommand{\locate}{\ensuremath{\textsc{LeafLabel}}}
\newcommand{\parent}{\ensuremath{\textsc{Parent}}}
\newcommand{\ancestor}{\ensuremath{\textsc{IsAncestor}}}
\newcommand{\sdepth}{\ensuremath{\textsc{StringDepth}}}
\newcommand{\tdepth}{\ensuremath{\textsc{TreeDepth}}}
\newcommand{\scount}{\ensuremath{\textsc{LeafCount}}}
\newcommand{\fchild}{\ensuremath{\textsc{FirstChild}}}
\newcommand{\nsibling}{\ensuremath{\textsc{NextSibling}}}
\newcommand{\child}{\ensuremath{\textsc{Child}}}
\newcommand{\ithchild}{\ensuremath{\textsc{IthChild}}}
\newcommand{\childrank}{\ensuremath{\textsc{ChildRank}}}
\newcommand{\slink}{\ensuremath{\textsc{SuffixLink}}}
\newcommand{\nrs}{\ensuremath{\textsc{Nrs}}} \newcommand{\laq}{\ensuremath{\textsc{Laq}}}
\newcommand{\Null}{\ensuremath{\textsc{null}}}


\newcommand{\rank}{\ensuremath{\textit{rank}}}
\newcommand{\select}{\ensuremath{\textit{select}}}
\newcommand{\findopen}{\ensuremath{\textit{findopen}}}
\newcommand{\findclose}{\ensuremath{\textit{findclose}}}

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{prop}[definition]{Proposition}
\newcounter{theoremCounter}\newtheorem{theorem}[theoremCounter]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{problem}{Problem}\newtheorem{example}{Example}

\DeclareMathOperator*{\polylog}{polylg}
\DeclareMathOperator*{\polyloglog}{polylglg}
\DeclareMathOperator*{\argmin}{argmin}

\pagestyle{plain}

\title{Combined Data Structure for Previous- and Next-Smaller-Values}

\author{
  Johannes Fischer\thanks{Computer Science Department, Karlsruhe
    University, \texttt{johannes.fischer@kit.edu}}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Let  be a static array storing  elements from a totally ordered set. We present a data structure of optimal size at most  bits that allows us to answer the following queries on  in constant time, without accessing : (1) previous smaller value queries, where given an index , we wish to find the first index to the left of  where  is strictly smaller than at , and (2) next smaller value queries, which search to the right of . As an additional bonus, our data structure also allows to answer a third kind of query: given indices , find the position of the minimum in . Our data structure has direct consequences for the space-efficient storage of suffix trees.
\end{abstract}

\section{Introduction}
We consider the situation where a static array  can be preprocessed such that the following three queries can be answered in constant time: previous- and next-smaller-value-queries, where given a position  in , one searches for the next position  to the left (or right) of  with , and range minimum queries, where for two given indices  and  we look for the position of the minimum element within the subarray .

Our work is situated in the field of succinct data structures, where the aim is to store objects of size  from a universe of size  in  bits\footnote{Throughout this article,  denotes the binary logarithm.}, while still being able to perform all operations on the data as if they were uncompressed. All succinct data structures work in the word-RAM model of computation, where fundamental operations on a contiguous field of  bits can be performed in constant time ( is the word size, and we assume ).

Succinct data structures can be further classified into \emph{indexing} and \emph{encoding} data structures. An indexing data structure enhances an object (such as an array) with additional functionality (such as queries) and needs access to the object itself, whereas an encoding data structure recodes all necessary parts of the data for answering the queries without accessing the object.

For range minimum queries alone, there is a data structure in the encoding model of asymptotically optimal size  bits that allows to answer queries in constant time \cite{fischer10optimal}. Previous- and next-smaller-value queries originate from parallel computing \cite{berkman93optimal}. For all three queries combined, the only existing data structure uses  bits \cite{ohlebusch10cst++}.

In this short note, we present an encoding data structure of size at most  bits that allows to answer all three queries in constant time. It is interesting to note that although we do not have a closed formula for the exact size of our data structure, we prove that it is asymptotically optimal. The reason for this slight oddity is that we are not aware of a closed formula for the size  of the universe of objects that we encode; however, we prove that we encode them in an asymptotically optimal way.

Although our data structure is independent of the underlying array  and may have other applications, our research is clearly motivated by the compact storage of full-text indices \cite{navarro07compressed}. Precisely, we show that our data structure automatically yields the smallest compressed suffix tree with constant-time navigation (we refer the reader to Sect.~\ref{sect:cst} for more details and preliminary work on compressed suffix trees).

The rest of this note is structured as follows. Sect.~\ref{sect:preliminaries} introduces some notation and known results. Sect.~\ref{sect:main} presents the core idea of the paper, a combined data structure for s and -/-queries. Finally, Sect.~\ref{sect:cst} describes how that data structure yields improvements in compressed suffix trees.

\section{Preliminaries}
\label{sect:preliminaries}
For integers  and , we write  to denote the set , and  to denote . For a rooted tree  and a node , we write  to denote the subtree of  rooted at .

\subsection{Queries}
Let  be an array of totally ordered objects. For technical reasons, we define  as the ``artificial'' overall minima of the array. We start by formally defining previous smaller values:

\begin{definition}
  \label{def:psv}
  For , let  denote the \emph{previous smaller value} of position .
\end{definition}

As mentioned in the introduction, we also consider next smaller values and range minima, for completeness formally defined as follows.

\begin{definition}
  \label{def:nsv}
  For , let  denote the \emph{next smaller value} of position .
\end{definition}

\begin{definition}
  \label{def:rmq}
  For , let  denote a \emph{range minimum query} between positions  and . If the minimum in the query range is not unique, the leftmost (or rightmost) minimum is chosen as a representative.
\end{definition}

In the following, the subscript  from  etc.\ will be omitted if the underlying array  is clear from the context.

\subsection{LRM-Trees }
\label{sect:lrm}

LRM-Trees are the basis of our new data structure. They were introduced under this name as an internal tool for basic navigational operations in ordinal trees~\cite{sadakane10fully}, and, under the name of ``2d-Min Heaps,'' to encode integer arrays in order to support range minimum queries on them~\cite{fischer10optimal}.
 
\begin{definition}[Sadakane and Navarro~\cite{sadakane10fully}; Fischer~\cite{fischer10optimal}]
  \label{def:2dmin}
  The \emph{LRM-Tree} of  is an ordered labeled tree with vertices . For ,  is the parent node of . The children are ordered in increasing order from left to right.
\end{definition}

We note the following useful properties of the LRM-Tree (observe that we use nodes and array indices interchangeably throughout this article):

\begin{lemma}[Fischer~\cite{fischer10optimal}]
  \label{lemma:basic}
  Let  be the LRM-Tree of .
  \begin{enumerate}
  \item The node labels correspond to the preorder-numbers of  (counting starts at 0).
  \item Let  be a node in  with children . Then  for all .
  \item Again, let  be a node in  with children . Then  for all .
  \end{enumerate}
\end{lemma}

\subsection{Succinct Tree Encodings}
\label{sect:succinct_trees}
A rooted ordered tree on  nodes can be encoded in  bits in various ways such that it still permits (the simulation of) all navigational operations in constant time, such as BPS \cite{munro01succinct} or DFUDS \cite{benoit05representing}. Of particular importance to this article are methods based on tree covering (TC) \cite{geary06succinct,he07succinct,farzan08uniform}. They support most navigational operations on trees in constant time, among others , , , , , selecting the 'th child (), computing the rank of a child among its siblings (), and computing lowest common ancestors (). Farzan and Munro's approach \cite{farzan08uniform} has the further advantage that it can also optimally encode other types of trees, such as those described in the following section.

\subsection{Schr\"oder Trees}
\label{sect:schroeder}
The term \emph{Schr\"oder Tree} is used for various types of rooted ordered trees \cite{stanley99enumerative}: trees with no nodes of out-degree 1, trees with labeled edges, or trees with labeled nodes. For our purposes, we define them as follows.

\begin{definition}
  A \emph{Schr\"oder Tree} is a rooted ordered tree, where any node except the first child in a list of siblings may be colored red or blue. First children are always colored blue.
\end{definition}

The number of Schr\"oder Trees on  nodes is counted by the little Schr\"oder numbers . Although we do not have a closed formula for , it is known \cite{merlini04waiting} that  with . In particular, .

\section{Data Structure}
\label{sect:main}
In this section, we present the new data structure for answering // on an input array . We start by introducing the general ideas behind our data structure, and then show how this data structure can be encoded succinctly.

\subsection{Basic Solution}
\label{sect:basic}
The LRM-Tree (Def.~\ref{def:2dmin}) encodes all information for answering -queries in a natural way, as it suffices to move to the parent node of  for answering . It also captures all sufficient information for answering s:

\begin{lemma}[Fischer~\cite{fischer10optimal}]
  \label{lemma:rmqlca}
  For arbitrary nodes  and  in the LRM-Tree of , , let . Then if ,  is given by , and otherwise,  is given by the child of  that is on the path from  to .
\end{lemma}

Thus, it remains to show how -queries can be answered. It is easy to see that the LRM-Tree alone is not enough for this task: consider  and . These arrays have the same LRM-Tree (and hence the same answers to all s and -queries); yet, their -queries differ, as , and .

In principle, we could build another LRM-Tree  on the reversed sequence  for answering =queries, as . As this would double the space of the resulting data structure, we now present a more sophisticated solution.

The general idea of our data structure can be seen as follows. Recall property 3 of Lemma~\ref{lemma:basic}: the children  of a node  in the LRM-Tree are ordered decreasingly from left to right: . Now suppose we wish to calculate  for some , and assume that . Then , as all -values in the subtree  are strictly greater than at position  (property 2 of Lemma~\ref{lemma:basic}). If, on the other hand, , then the next ``candidate'' for  is  (assuming ), as again all -values in  are strictly greater than .

This suggests the following general approach. In the LRM-Tree  of , a node is colored \emph{red} if the corresponding value in  is smaller than the -value at its left sibling (if such a sibling exists). More formally, let  be a node in  with children . Then for all , node  is colored red if and only if . All other nodes (including the root) are colored blue. We call the resulting tree the \emph{Colored LRM-Tree}.

To get the connection to -queries, we need the following definition:
\begin{definition}
  \label{def:nrs}
  Let  the Colored LRM-Tree of , and let  be a node in  with children . The \emph{next red sibling}  of a node  is the leftmost sibling to the right of  that is colored red. If such a sibling does not exist, we define . In symbols, let . Then  if , and otherwise .
\end{definition}

We can then show the following lemma:

\begin{lemma}
  \label{lemma:nsv}
  Let  the Colored LRM-Tree of , and let  be a node in  with children , . Then
  
\end{lemma}
\begin{proof}
  We consider each case in turn.
  \begin{description}[\setlabelstyle{\normalfont}]
  \item[.]
    Let  be defined by . From Def.~\ref{def:nrs} and the fact that node  is red, we know that . Hence, we need to show that  for all . From property 1 of Lemma~\ref{lemma:basic}, we know that all values in  occur in . Because  is minimal and due to property 3 of Lemma~\ref{lemma:basic},  for . But due to property 2 of Lemma~\ref{lemma:basic},  for all  and all . Hence, .
  \item[.]
    Let . As above, we can show that  for all . It thus remains to show that . For the sake of contradiction, assume that , where we further distinguish between the cases ``'' and ``.'' If , then  (the parent node of ), so  is the right sibling of , a contradiction to the definition of . If , then again due to property 2 of Lemma~\ref{lemma:basic}, we have . So  contains , a contradiction to the size of , which is , as  contains exactly those elements from .
  \end{description}
\end{proof}

\subsection{Succinct Encoding}
\label{sect:succinct}
We represent the Colored LRM-Tree  from Sect.~\ref{sect:basic} similar to Farzan and Munro's succinct TC-encoding for ordinal trees \cite{farzan08uniform}. This approach is based on a two-level decomposition of the tree into mini- and micro-trees. In our scenario, the encoding of a micro-tree is simply its index in an enumeration of all Schr\"oder Trees of the micro-tree size (called ``enumeration code'' in \cite{farzan08uniform}). In total, this uses optimal  bits of space.

It remains to show how we implement the query algorithms for , , and .

As  and the parent-operation is directly supported by TC, we can directly focus on . Recall Lemma \ref{lemma:nsv}: given , we need to find  in order to answer . The -method can be implemented as the combination of \emph{modified} - and -operations, as they are described by Farzan and Munro \cite{farzan08uniform} (see \cite[p.~23]{farzan09phd} for further details). In particular, given node , we find the parent  of , and then determine the rank  of  among all its red siblings, from where we select the 'st red node. To this end, if  is a root of a mini- or micro-tree, we use a \emph{modified} fully indexable dictionary (FID) \cite{raman02succinct} to rank/select among the red nodes. These FIDs are similar to the ones already stored at each mini- or micro-tree root, with the difference that they index only the red nodes. Similar to the original analysis, their overall space contributes only  bits to the final space. If, on the other hand,  is not a mini- or micro-tree root, we use the lookup-tables stored along with the micro-trees to rank/select among the red nodes. These lookup-tables also use only  bits, as we use micro-trees of size . Finally, if , we move to the rightmost sibling  of  and count the subtree size at ; both operations are supported in  time by TC.

For implementing , we have to show how the operations in Lemma~\ref{lemma:rmqlca} can be performed in constant time. We cannot resort to the method described by Fischer \cite{fischer10optimal}, as it is inherently connected to DFUDS. We thus do the following: first compute ; this is supported by TC \cite{geary06succinct,he07succinct}. Then if  (otherwise we return ), compute the depth  of  (depth is supported by TC). Finally, compute the child of  that is on the path to  by a level-ancestor query  (supported by TC); this is the answer.

\begin{theorem}
  \label{thm:main1}
  For an array of  totally ordered objects, there is a data structure using  bits of space that supports s, - and -queries on  in  time, without accessing  at query time.
\end{theorem}

\subsection{Optimality}
It is easy to see that the encoding from Sect.~\ref{sect:succinct} is optimal. Given any data structure  supporting  and  on some underlying input array , we can reconstruct the Colored LRM-Tree  of , without knowing : We first create 's rightmost path  in a bottom-up manner, by successively querying , until arriving at . All nodes are initially colored blue. This leaves us with unprocessed intervals , which are handled recursively. During these recursive calls, suppose that a query  brings us to a node  which is already present in the (partial) LRM-Tree . Let  be the smallest child of  greater than  (i.e., the leftmost child of  to the right of ). We then check if , in which case we color  red. Otherwise (),  remains blue, as in this case . This procedure correctly reconstructs the Colored LRM-Tree  of .

As every Schr\"oder Tree is also a Colored LRM-Tree for some array  (starting at the root with children , set  to 0, and  to  or , depending on whether  is colored blue or red; the unprocessed intervals are handled recursively), we need at least  bits to encode  in the worst case. This proves the optimality of the data structure from Thm.~\ref{thm:main1}.

\section{Application to Compressed Suffix Trees}
\label{sect:cst}
The result from Thm.~\ref{thm:main1} has direct consequences for compressed suffix trees (CSTs). A suffix tree (ST) for a string  of length  is a compact trie storing all the suffixes of , in the sense that the characters on any root-to-leaf path spell out exactly a suffix. The ST is an extremely important data structure with applications in exact or approximate string matching, bioinformatics, and document retrieval, to mention only a few examples.

The drawback of STs is their huge space consumption of 20--40 times the text size ( bits in theory), even when using carefully engineered implementations. To reduce their size, in recent years several authors provided compressed variants of STs \cite{munro01space,grossi05compressed,sadakane07compressed,russo08fully,fischer09faster,ohlebusch09compressed,canovas10practical,ohlebusch10cst++,fischer10wee}.

We regard the CST as an abstract data type supporting the following operations (apart from the usual navigational operations on trees as those mentioned in Sect.~\ref{sect:succinct_trees}):  gives the number of leaves (suffixes) below ,  for a leaf  yields the position in  where the corresponding suffix begins,  gives 's string-depth (number of characters on the root-to- path),  gives the unique node  with root-to- label  if the root-to- label is  for some , and  gives the child  of  such that the label on the edge  starts with . Here and in the following,  denotes the underlying alphabet of size . See the first column of Tbl.~\ref{tbl:cst} for all operations (\emph{level ancestor queries} are excluded as we are not a aware of any actual algorithm that needs them in a suffix tree).

A CST on  can be divided into three components: (1) the \emph{suffix array} , specifying the lexicographic order of 's suffixes, defined by  (hence  captures information on the \emph{leaves}); (2) the \emph{LCP-array} , storing the lengths of the \emph{longest common prefixes} of lexicographically adjacent suffixes:  and for , , which is the string-depth of the LCA of the lexicographically 'th and 'st suffix (hence  captures information on \emph{internal} nodes); and (3) additional data structures for simulating the \emph{navigational operations}. The goal of a CST is to compress each of these three components.

\begin{table}[t]
  \small
  \centering
  \caption{Comparison of different CSTs (space in bits on top of  and ). The  is omitted in all operations. Trees \cite{russo08fully,fischer09faster,ohlebusch09compressed} are incomparable to our approach, as they use less space in exchange for higher navigation times.  denotes the time to compute the position of  in , which is  in most compressed suffix arrays.}
  \label{tbl:cst}
  \begin{tabular}{|l||c|c||c|c|c|c|}
    \hline
    & \textbf{\cite{russo08fully}} & \textbf{\cite{fischer09faster,canovas10practical}}& \textbf{\cite{munro01space,grossi05compressed,sadakane07compressed}} & \textbf{\cite{ohlebusch09compressed}} & \textbf{\cite{ohlebusch10cst++}} & \textbf{NEW}\\
    \hfill space &  &  &  &  &  & \boldmath \\ \hline\hline
    \sroot & 1 & 1 & 1 & 1 & 1 & \textbf{1}\\\hline
    \ancestor & 1 & 1 & 1 & 1 & 1 & \textbf{1}\\\hline
    \ssize & --- & --- & 1 & --- & --- & \textbf{---}\\\hline
    \scount & 1 & 1 & 1 & 1 & 1 & \textbf{1}\\\hline
    \locate &  &  &  &  &  & \boldmath\\\hline
    \sdepth &  &  &  &  &  & \boldmath \\\hline
    \parent &  &  & 1 &  & 1 & \textbf{1} \\\hline
    \fchild &  &  & 1 &  & 1 & \textbf{1} \\\hline
    \nsibling&  &  & 1 &  & 1 & \textbf{1} \\\hline
    \slink&  &  &  &  &  & \boldmath\\\hline
    \lca &  &  & 1 &  & 1 & \textbf{1}\\\hline
    \tdepth &  & --- & 1 & --- & --- & \textbf{---} \\\hline
    \child &  & \scriptsize  &  &  &  & \boldmath\\\hline
  \end{tabular}
\end{table}

We do not discuss here the different time/space tradeoffs for compressing  and ; we just mention that both can be compressed into space proportional to the entropy of the underlying text, at the cost of increased access times, which we denote by  and , respectively.

Of more interest to us is the fact that most recent CSTs \cite{fischer09faster,ohlebusch09compressed,ohlebusch10cst++} represent a node  as an interval  in  and base their navigation on s and -/-queries in . There are two basic strategies for supporting these queries: we can either use structures of size  \cite{fischer09faster,canovas10practical} or  \cite{ohlebusch09compressed} bits and substitute ``missing information'' by a (sub-)logarithmic number of lookups to  (indexing model), resulting in increased navigation time (see 3rd and 5th column Tbl.~\ref{tbl:cst}). The other option \cite{ohlebusch10cst++} is to use a data structure that computes // without needing access to the underlying LCP-array (encoding model).

Given these observations, the index from Thm.~\ref{thm:main1} almost directly yields a CST with  bits on top of  and  with constant-time support of all operations that do not necessarily need access to  or . See again Tbl.~\ref{tbl:cst} for a comparison. In particular, we get the smallest CST with constant-time navigation. Note that it is of utmost theoretical and practical importance to have the smallest possible data structure for the navigational component of a CST, as its -term is incompressible, whereas the space of the other two components of a CST ( and ) vanishes if the entropy of the underlying text does.

All suffix tree operations (apart from , , and ) from Tbl.~\ref{tbl:cst} can be implemented solely by performing s and -/-queries in , see \cite{fischer09faster,ohlebusch10cst++}. Only the implementation of the -operation relies on structures that are proprietary to \cite{ohlebusch10cst++} (and the one in \cite{fischer09faster} accesses ); we therefore give our own implementation as follows: let  be the node whose next sibling we want to compute. First check if  equals the root, and return  in this case. Otherwise, compute . If , return , as  does not have a right sibling in this case. We now know that  is the leftmost index of . To determine the rightmost index, check if , and return  in this case, as then  is the second-to-last child of . Otherwise, return , as the range minimum query returns a position in  where the string-depth of  is stored.

\begin{theorem}
  \label{thm:main2}
  Let  be a text of size  with characters from an alphabet of size . Given 's suffix array with access time  and its LCP-array with access time , there is a CST with additional  bits that supports the operations as indicated in the last column of Tbl.~\ref{tbl:cst}.\hfill \qed
\end{theorem}

Our CST resides in between \cite{ohlebusch09compressed} and \cite{ohlebusch10cst++}: it is smaller than \cite{ohlebusch10cst++} and larger than \cite{ohlebusch09compressed}, but equally fast as the larger of these \cite{ohlebusch10cst++}.

It is interesting to note that our  bits are also optimal for encoding the topology of a suffix tree, as it is a tree with exactly  leaves and no nodes of out-degree 1; the number of such trees is also counted by the little Schr\"oder number . However, we cannot make an optimality claim for the CST from Thm.~\ref{thm:main2}, as it builds on  and , who already capture the topology of the suffix tree.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}