\subsection{Problem Formalization}

We formalize an ad-hoc video search process as follows. We denote a specific video clip as  and a large collection of  \emph{unlabeled} video clips as . For an ad-hoc query in the form of a sentence , let  be a \textbf{c}ross-\textbf{m}odal \textbf{s}imilarity function that measures the semantic relevance between the query and a specific video. Accordingly, the search process boils down to sorting  in descending order in terms of  and returning the top ranked items for the given query. The computation of  requires proper embeddings of both  and  into a common cross-modal space. While visual CNNs are prerequisites for video embedding, sentence encoders are required for query embedding. Let  be a specific sentence encoder, which encodes the given query into a -dimensional real-valued vector, \ie .
Having  distinct sentence encoders  shall give us  vectors of varied dimensions . We aim for a model that effectively exploit the multiple sentence encoders for computing .

Next, we describe in brief sentence encoders investigated in this work in Section \ref{ssec:tes}, followed by the proposed sentence encoder assembly (SEA) model in Section \ref{ssec:tea}.

\subsection{Sentence Encoders in Use} \label{ssec:tes}

We consider five present-day sentence encoders, \ie Bag-of-Words (BoW), word2vec (w2v), GRU, bi-GRU and BERT. Among them, the first two are unigram, while the others are sequential models. Their main properties are summarized in Table \ref{tab:sent-encoders}.




\begin{table}[tb!]
\normalsize
\renewcommand\arraystretch{1}
\centering
\caption{\textbf{Five sentence encoders used in this paper}. The specific value of the vocabulary size  is dataset-dependent, which is 7,676 for MSR-VTT, 3,981 for TGIF and 2,917 for MSVD. The notation + means the vocabulary of GRU and bi-GRU is slightly bigger than  due to the inclusion of stopwords and special tokens for sequential modeling.}
\label{tab:sent-encoders}
\scalebox{0.75}{
\begin{tabular}{@{}|l|r|r|l|l|@{}}
\hline
\textbf{Encoder} & \textbf{Vocabulary} & \textbf{Dim.}  & \textbf{Training} & \textbf{Prior work} \\
\hline
BoW &  &  & Not trainable & \cite{habibian-pami17,w2vv,w2vvpp,dong-cvpr19} \\
\hline
w2v & 1.7 millions & 500 & \specialcell{pre-trained\footref{w2v-flickr}\\ and fixed} & \cite{w2vv,w2vvpp,wray-iccv19,tv19-alibaba} \\
\hline
GRU & + & 1,024 & \specialcell{trained from\\ scratch} & \cite{w2vv,w2vvpp,mithun-icmr18} \\
\hline
bi-GRU & + & 2,048 & \specialcell{trained from\\ scratch} & \cite{dong-cvpr19,tv19-alibaba} \\
\hline
BERT & 30,000 & 768 & \specialcell{pre-trained\footref{bert}\\ and fixed} & \cite{luo2020univilm,mm20-video-retrieval}  \\
\hline
\end{tabular}
}
\end{table} 
\textbf{1) BoW}. As a classical text encoder, BoW simply quantizes a given sentence  of  words with respect to a pre-specified vocabulary of  words. Let  be a function that counts the occurrence of the -th word in the sentence. Accordingly, we have the BoW encoding  as

Note that an AVS query is relatively short, often containing less than 10 words. Meanwhile, the vocabulary size is much larger, with a typical order of . As a consequence,  is a long and sparse vector. 

\textbf{2) w2v}. The w2v model~\cite{w2v} learns to produce word-level dense and semantic vectors by training a two-layer neural network on a large text corpus, with the goal to reconstruct linguistic contexts of words in the training text. As computing the reconstruction loss requires no extra manual annotation, w2v encodes millions of words with ease. We adopt a 500-dimensional w2v model\footnote{\url{https://github.com/danieljf24/w2vv} Note that while \cite{w2vv} performs image-to-text matching experiments on Flickr30k, its w2v model was trained on English tags of 30 million Flickr images, using the skip-gram algorithm.\label{w2v-flickr}} from \cite{w2vv}. 
We also tried alternatives such as GloVe~\cite{glove}, and found it less effective in our preliminary experiments. Let  be a lookup function that returns the embedding vector for the -th word of , we obtain w2v based sentence encoding by mean pooling, \ie


\textbf{3) GRU}. The Gated Recurrent Unit (GRU) network~\cite{gru} models the sequential information within a sentence by iteratively generating a sequence of recurrent hidden state vectors . In particular, the hidden state vector at time-step , , is jointly determined by the word embedding of the current word  and  , the hidden state vector at the previous time-step. Similar to LSTM~\cite{lstm}, the GRU network effectively prevents the vanishing gradient problem by introducing a gating mechanism to modulate the flow of information inside the unit. Meanwhile, as GRU has no separate memory cell, it has a simplified architecture and thus with less parameters to be trained. Following ~\cite{w2vvpp}, we obtain the GRU-based sentence encoding by mean pooling over the hidden vector sequence, \ie


\textbf{4) bi-GRU}. The bi-directional GRU (bi-GRU) network extends the forward GRU by including a backward GRU that encodes the sequence in a reverse order. Given  as hidden state vectors of the backward GRU, our bi-GRU based sentence encoding is obtained by 

where  denotes vector concatenation. Note that given forward and backward hidden vectors of the same size,  provides a richer representation than  at the cost of doubled parameters. Hence, we shall use either  or , but not both. 


\textbf{5) BERT}. The BERT model, built by stacking a number of  bi-directional Transformer blocks~\cite{bert}, generates word embeddings for a given sentence by progressively passing encodings through the multiple blocks. A Transformer block consists of a self-attention network and a feed-forward network~\cite{transformer}. The self-attention network accepts encodings of individual tokens from the previous Transformer block, weighs their importance to each other by a self-attention mechanism, and accordingly generates new encodings. These encodings are then fed in parallel into the feed-forward network to produce the output encodings of this block. In this work, we adopt the base version of BERT containing  blocks, which has been pre-trained on English Wikipedia and book corpora for masked language modeling and next sentence prediction\footnote{\url{https://github.com/google-research/bert}\label{bert}}. We obtain the BERT-based sentence encoding by mean pooling as 

where  denotes the embedding of the -th word produced by the second-last block. Note that we tried max pooling or using the embedding of the first / last token, and found these alternatives less effective than mean pooling.




With the sentence encoders introduced, we proceed to describe how to effectively combine them in an end-to-end framework.

\subsection{Sentence Encoder Assembly} \label{ssec:tea}

We propose to combine  distinct sentence encoders  in a generic multi-space multi-loss learning framework. 

\textbf{Multiple common spaces}. Our framework consists of  cross-modal matching subnetworks, each corresponding to a specific sentence encoder and learning its own common space. Each subnetwork, indexed by , consists of two fully connected (FC) layers, one on the text side to transform  into a -dimensional vector, and the other on the video side that transforms the video feature vector  into another -dimensional vector. Consequently, the sentence-video semantic relevance, denoted as , is computed as the cosine similarity between the two embedding:

where  and  indicate the two FC layers, each followed by a \textit{tanh} function to increase their learning capacity. We choose the cosine similarity as it is a widely used similarity metric for cross-modal matching~\cite{vsepp,liu-bmvc19,dong-cvpr19,miech-iccv19,sigir20-tce}. We also tried a Euclidean distance based similarity, which is however less effective\footnote{For \tea(\{BoW,w2v\}) trained with the Euclidean distance based similarity, its infAP scores on TV16/17/18/19 are 12.8/18.9/11.8/10.4, clearly lower than the cosine similarity counterpart (15.7/23.4/12.8/16.6).}. 

By simply averaging the similarities computed in the individual common spaces, we have the overall cross-modal similarity as 

Note that we do not go for more complicated alternatives, \eg weighing the individual similarities by self-attention mechanisms. 
Rather, we opt for this simple combination strategy, not only for preventing the risk of over-fitting. Such a strategy also encourages the individual common spaces to be good enough to be combined, as they are set to be equally important.

\textbf{Multi-loss learning}. We develop our loss function based on the improved triplet ranking loss (ITRL) by Faghri \etal \cite{vsepp}. While originally proposed for image-text matching, ITRL is now found to be effective for text-video matching~\cite{mithun-icmr18,w2vvpp,dong-cvpr19,liu-bmvc19,tv19-alibaba}. Unlike the classical triplet ranking loss that selects negative training examples by random, ITRL considers the negative that violates the ranking constraint the most (within a mini-batch) and thus deemed to be the most informative for improving the model being trained. Given a training sentence  with  as a video relevant w.r.t  and  as irrelevant, we express ITRL as 

where  is a positive hyper-parameter concerning the margin.




We argue that such a single loss is suboptimal for multi-space learning. Given a specific mini-batch, hard negative examples selected in terms of the combined similarity are not necessarily the most effective for learning the individual common spaces. Therefore, we choose to compute  per space, and accordingly learn to minimize their combined loss, \ie

In a similar spirit to similarity combination, we again treat all the sub losses equally. As exemplified in Fig. \ref{fig:hard-negative}, the combined loss lets the model be exposed to more diverse hard negatives. We empirically find that compared to the single loss, the combined loss provides around 30\% extra hard negatives per training epoch. 


\begin{figure}[tbh!]
\centering
\includegraphics[width=\columnwidth]{single_vs_combined_loss}
\caption{\textbf{Examples of hard negative videos automatically selected for specific sentences during training}. The first column (a) is selection based on the combined similarity in a single common space. The other columns indicate selections made based on individual similarities w.r.t (b) , (c) , and (d)  within the proposed multi-space and multi-loss framework. Using the combined loss allows the model to be exposed to more diverse hard negatives in a given batch.}
\label{fig:hard-negative}
\end{figure}



To sum up, the multi-space strategy provides a more flexible mechanism to exploit complementarities among the distinct sentence encoders. Meanwhile, given a specific mini-batch during training, the multi-loss strategy allows each common space to select its own hard negative example. More flexibility in encoder ensemble and more effectiveness for training together contributes to the superior performance of the proposed SEA method against the state-of-the-art.


