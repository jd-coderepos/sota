

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage[super]{nth}
\usepackage{enumitem}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\B}{\bfseries}
\newcommand{\rpm}{\tiny \raisebox{.2ex}{}}
\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\wo}[2]{{#1} - {#2}}
\newcommand\method{SimpleView}
\newcommand{\smallsec}[1]{\noindent {\bf #1:}}



\usepackage[accepted]{icml2021}

\icmltitlerunning{Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline}

\begin{document}

\twocolumn[
\icmltitle{Revisiting Point Cloud Shape Classification \\ with a Simple and Effective Baseline}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ankit Goyal}{princeton}
\icmlauthor{Hei Law}{princeton}
\icmlauthor{Bowei Liu}{princeton}
\icmlauthor{Alejandro Newell}{princeton}
\icmlauthor{Jia Deng}{princeton}
\end{icmlauthorlist}

\icmlaffiliation{princeton}{Department of Computer Science, Princeton University, NJ, USA}
\icmlcorrespondingauthor{Ankit Goyal}{agoyal@priceton.edu}
\icmlkeywords{Point Cloud}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Processing point cloud data is an important component of many real-world systems. As such, a wide variety of point-based approaches have been proposed, reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, we find that auxiliary factors like different evaluation schemes, data augmentation strategies, and loss functions, which are independent of the model architecture, make a large difference in performance. The differences are large enough that they obscure the effect of architecture. When these factors are controlled for, PointNet++, a relatively older network, performs competitively with recent methods. Second, a very simple projection-based method, which we refer to as SimpleView, performs surprisingly well. It achieves on par or better results than sophisticated state-of-the-art methods on ModelNet40 while being half the size of PointNet++. It also outperforms state-of-the-art methods on ScanObjectNN, a real-world point cloud benchmark, and demonstrates better cross-dataset generalization. Code is available at \url{https://github.com/princeton-vl/SimpleView}.
\end{abstract}

\section{Introduction}
\begin{figure}[t]
  \centering
  \resizebox{\columnwidth}{!}{\includegraphics{fig/accuracies.pdf}}
  \captionof{figure}{Performance of different models on ModelNet40. Those using  1024 points or normals are marked with star. We re-evaluate top-performing models across times based on reported results which include PointNet, PointNet++, DGCNN and RSCNN. Green points shows performance when the models are evaluated under the same protocol. Since the code for multi-scale version of RSCNN (RSCNN-MS) is not released, we are using the single-scale version (RSCNN-SS) in our evaluation.}
  \label{fig:accuracies}
\end{figure}
Processing 3D point cloud data accurately is crucial in many applications including autonomous driving ~\citep{navarro2010pedestrian,kidono2011pedestrian}, robotics~\citep{rusu2009close, correll2016analysis,mousavian20196} and scene understanding~\cite{aldoma2012tutorial}. In these settings, sensors like LIDAR produce unordered sets of points that correspond to object surfaces. Correctly classifying objects from this data is important for 3D scene understanding~\citep{uy2019revisiting}. While classical approaches for this problem have relied on hand-crafted features~\citep{arras2007using}, recent efforts have focused on the design of deep neural networks (DNNs) to learn features directly from raw point cloud data ~\citep{qi-pointnet-cvpr17}. Similar to image classification~\cite{yalniz2019billion,dosovitskiy2020image,szegedy2016rethinking}, deep learning-based methods have proven effective in point cloud classification.

The most widely adopted benchmark for comparing methods for point cloud classification has been ModelNet40~\citep{wu20153d}. The accuracy on ModelNet40 has steadily improved over the last few years from 89.2\% by PointNet~\citep{qi-pointnet-cvpr17} to 93.6\% by RSCNN~\citep{liu2019relation} (Fig.~\ref{fig:accuracies}). This progress is commonly perceived to be a result of better designs of network architectures. However, after performing a careful analysis of recent works we find two surprising results. First, we find that auxiliary factors including differing evaluation schemes, data augmentation strategies, and loss functions affect performance to such a degree that it can be difficult to disentangle improvements due to the network architecture. Second, we find that a very simple projection-based architecture works surprisingly well, outperforming state-of-the-art point-based architectures.


\begin{figure}[]
  \centering
  \resizebox{\columnwidth}{!}{\includegraphics{fig/simpleview_model.pdf}}
  \captionof{figure}{The \method{} Architecture. The depth images are colored only for illustration. {\method } takes in single channel depth images as input.}
  \label{fig:simpleview}
\end{figure}

In deep learning, as results improve on a benchmark, attention is generally focused on the novel architectures used to achieve those results. However, there are many factors beyond architecture design that influence performance including data augmentation and evaluation procedure. We refer to these additional factors as a method's \texttt{protocol}. A protocol defines all details orthogonal to the network architecture that can be controlled to compare differing architectures. Note that it is possible for some specific form of loss or data augmentation to be tied to a specific architecture and inapplicable to other architectures. In these cases, it would be inappropriate to treat them as part of the protocol. However, for all the methods we consider in this paper, their losses and augmentation schemes are fully compatible with each other and can be considered independently. 

We do experiments to study the effect of protocol and discover that it accounts for a large difference in performance, so large as to obscure the contribution of a novel architecture. For example, the performance of the PointNet++ architecture~\citep{qi2017pointnetplusplus} jumps from  to , when switching from its original protocol to RSCNN's protocol~\citep{liu2019relation}. We further find that the protocols that lead to the strongest performance rely on feedback from the test set, which differs from conventional evaluation setups. We re-evaluate prior architectures using the best augmentation and loss functions, while not using any feedback from the test set. We find that by taking protocol into account, the PointNet++ architecture performs competitively with more recent ones in various settings.

In addition to the surprising importance of protocol, in reviewing past approaches, another surprising discovery is that a very simple projection based baseline works very well. One needs to simply project the points to depth maps along the orthogonal views, pass them through a light-weight CNN and fuse the features. We refer to this baseline as \method{}.

Compared to previous projection-based method~\citep{roveri2018network, sarkar2018learning} for point-cloud classification, \method{} is very simple. Prior methods have developed special modules for view selection, rendering, and feature merging, as well as use larger CNN backbones that are pretrained on ImageNet (refer to Sec.~\ref{sec:related_work} for more details). In contrast, \method{} has no such special operations, and only requires simple point projections, a much smaller CNN backbone, and no ImageNet pretraining.

The discovery of \method{} is surprising because recent state-of-the-art results have all been achieved by point-based architectures of increasing sophistication. 
In recent literature, it is often assumed that point-based methods are the superior choice for point-cloud processing as they ``do not introduce explicit information loss"~\citep{guo2020deep}. Prior work has stated that ``convolution operation of these methods lacks the ability to capture nonlocally geometric features"~\citep{yan2020pointasnl}, that a projection-base method ``often demands a huge number of views for decent performance"~\citep{liu2019relation}, and that projection-based methods often ``fine-tune a pre-trained image-based architecture for accurate recognition"~\citep{liu2019relation}. It is thus surprising that a projection-based method could achieve state-of-the-art results with a simple architecture, only a few views, and no pretraining.

On ModelNet40, \method{} performs on par or better than more sophisticated state-of-the-art networks across various protocols, which includes the ones used by prior methods (Table.~\ref{tab:perf_all}) as well as our protocol (Table.~\ref{tab:our_protocol}). At the same time, {\method } outperforms state-of-the-art architectures on ScanObjectNN~\citep{uy2019revisiting}, a real-world dataset where point clouds are noisy (background points, occlusions, holes in objects) and are not axis-aligned. \method{} also demonstrates better cross-dataset generalization than prior works. Furthermore, \method{} uses less parameters than state-of-the-art networks (Table.~\ref{tab:our_protocol}).

Note that we are not proposing a new architecture or method, but simply evaluating a simple and strong projection-based baseline for point-cloud classification that is largely ignored in the literature.  We do not claim any novelty in the design of \method{} because all of its components have appeared in the literature. Our contribution is showing that such a simple baseline works surprisingly well, which is a result absent in existing literature. 

It is worth noting that one might think that projection-based methods are not directly comparable with point-based methods because projection-based methods may have the full mesh as input, as opposed to just a point cloud. While this is true for existing results in the literature, it is not the case with \method{}, whose input is the exact same point cloud given to a point-based method. In other words, \method{} is directly comparable to a point-based method because they solve the exact same task. 

In summary, our contributions are:
\setlist[itemize]{align=parleft,left=0pt..1em}
\vspace{-5mm}
\begin{itemize}[noitemsep]
    \item We show that training and evaluation factors independent of network architecture have a large impact on point-cloud classification performance.  With these factors controlled for, PointNet++ performs as well as more recent architectures.
    \item We demonstrate how \method, a very simple projection based baseline performs surprisingly well on point-cloud classification. It performs on par with or better than prior networks on ModelNet40 while using fewer parameters. It also outperforms state-of-the-art methods on real-world point-cloud classification and achieves better cross-dataset generalization.
\end{itemize}

\section{Related Work}
\label{sec:related_work}
\noindent\textbf{Point-Based Methods for Point-Cloud Analysis:} A broad class of DNNs have emerged to process 3D points directly~\citep{Simonovsky2017ecc,zaheer-deepsets-nips17,klokov-escape-iccv17,xu2018spidercnn,atzmon2018point,wang2018local,li-sonet-cvpr18,accv2018/Groh,ben20183dmfv,xie2018shapecontext,li-pointcnn-ar18,liu2019point2sequence,thomas2019kpconv,komarichev2019cnn,liu2019densepoint,yan2020pointasnl,su2018splatnet,zhang2019linked,liu2019point2sequence,atzmon2018point}. PointNet~\citep{qi-pointnet-cvpr17} proposed one of the first strategies, where features are updated for each point with MLP layers, and aggregated with global max pooling. However, no local comparisons are performed in PointNet, which motivates PointNet++\citep{qi2017pointnetplusplus}. PointNet++ breaks subsets of points into local regions that are processed first. More explicit modeling of the spatial relations between points is performed with more recent methods \citep{li-pointcnn-ar18,liu2019relation,wu2019pointconv}. For example, PointConv learns functions to define continuous 3D convolutions that can be applied to arbitrary sets of points in a neighborhood~\citep{wu2019pointconv}. RSCNN uses MLPs conditioned on the spatial relationship of two points to update and aggregate features around an individual sampled point~\citep{liu2019relation}. There exist many variations to these methods, but the emerging trend is an increase in sophistication. 

\noindent\textbf{Projection-Based Methods for Point-Cloud Classification: } Projection-based methods for point cloud classification have been proposed in the literature. Notably, ~\cite{roveri2018network} learn to predict viewing angles and classify images in an end-to-end differentiable way. They use the ResNet50 model, pretrained on ImageNet as their backbone and a depth-image generation pipeline. ~\cite{sarkar2018learning} propose a special multi-height rendering and feature merging scheme, and use a larger backbone network pretrained on ImageNet. ~\cite{ahmedepn} manually define important views for each object category, create binary edge maps, and train an ensemble of PointNet++ and CNN. However, numbers in~\cite{ahmedepn} are not directly comparable to other approaches as there is a manual alignment of objects in the test set which is different from the standard ModelNet40 test set. This was confirmed with the authors. It is worth noting that even though prior work has shown sophisticated operations to be useful for achieving good results, we find that when controlling for method protocols, strong performance can be achieved with fixed orthogonal views, a smaller network, no ImageNet pretraining, and simpler rendering of points.

\noindent\textbf{Projection-Based Methods for Other Point-Cloud Analysis Tasks: } There is a rich literature for using projection-based methods on various point-cloud analysis problems like segmentation~\citep{ladicky2010and,tighescalable,riemenschneider2014learning,qin2018deep,dai20183dmv,kalogerakis20173d,tatarchenko2018tangent}, reconstruction~\citep{pittaluga2019revealing} and rendering~\citep{aliev2019neural}. Notably,~\cite{boulch2017unstructured} use point cloud density to create scene meshes, which are then put into a mesh renderer to generate many image views at different scales. ~\cite{lawin2017deep} render a scene point cloud from 120 views for different modalities like color, depth, and surface normal. Information from multiple modalities is then fused to generate point-wise predictions. For a detailed survey of various projection approaches on different point-cloud processing tasks, we encourage readers to check the recent survey paper by~\citep{guo2020deep}. In this work, \method{} serves as a stripped-down projection-based baseline for point-cloud classification that uses a few orthogonal views and simple point projections. 
\begin{table*}[ht]
  \caption{Summary of various protocols.}
  \label{tab:protocols}
  \centering
  \begin{tabular}{l l l l l l}
    \toprule
    Protocol     & Data Augmentation & Model Selection & Loss & Ensemble & Training Points\\
    \midrule
    \textit{PointNet++}   & jitter, random rotation,& final model & cross-entropy & Rotation Vote & fixed \\
     & random scaling and trans. &  &  & &  \\
    \textit{DGCNN}      & random scaling and trans. & best test model  & smooth-loss & No vote & fixed \\
    \textit{RSCNN}     & random scaling and trans. & best test model & cross-entropy & Repeated Scaling Vote & resampled \\
    \textit{SimpleView} & random scaling and trans. & final model & smooth-loss & No vote & fixed \\
    \bottomrule
  \end{tabular}
\end{table*}
\noindent\textbf{3D shape Analysis using Rendered Images and Voxels: } Many works use images rendered from object meshes for 3D shape analysis~\citep{maturana-voxnet-iros15,wu20153d,yu-multiview-cvpr18,guo2016multi,shi2015deeppano,hackel2017semantic3d,song2016deep,song2014sliding,huang2016point,tchapmi2017segcloud}. MVCNN exemplifies this strategy by applying a shared CNN to many rendered views and max-pooling to aggregate features~\citep{su15mvcnn}. Subsequent approaches include RotationNet which trains the network to also predict the viewpoint for each image~\citep{kanezaki-rotationnet-cvpr18}, GVCNN which groups features from subsets of views together before aggregating into a final prediction~\citep{feng2018gvcnn}, and hypergraph methods that consider the correlation across training samples~\citep{zhang2018inductive,feng2019hypergraph}. One notable exception is \cite{qi-volumetric-cvpr16}, who use a multi-resolution variant of MVCNN, but instead of object meshes, use a voxelized version of the object for rendering. In contrast to the prior view-based methods that use object meshes with point connectivity information, and render images using basic shading and/or depth; \method{} takes as input raw point clouds.

Another class of methods is voxel-based methods that convert points to a fixed 3D grid instead, and use 3D CNNs~\citep{qi-volumetric-cvpr16,wu-3dshapenets-cvpr15,maturana-voxnet-iros15}. Given the added dimension, such methods are usually restricted to a much lower resolution to represent objects. Though some strategies such as octrees have been used to address those limitations~\citep{wang2017cnn}, the advantages to processing 3D data directly in this manner do not yet appear to outweigh the additional overhead introduced.


\section{Method Overview}
\subsection{Variations in Existing Protocols}
\label{sec:variation}
We analyze the key ingredients in the progress in point-cloud classification. Critical to our study is controlling for factors which are independent of network architecture. We refer to the factors as a method's \texttt{protocol}. A protocol used by one method can be transferred to another. For our study, we analyze a subset of the highest performing methods over the past few years. This choice was further based on availability and usability of official source-code. Specifically, we choose PointNet~\citep{qi-pointnet-cvpr17}, PointNet++~\citep{qi2017pointnetplusplus}, DGCNN~\citep{wang2018edgeconv} and RSCNN~\citep{liu2019relation}. Note that we also do direct comparisons to networks apart from the ones mentioned here (Table~\ref{tab:one_one}).

For our purposes, we do not consider any variations in input, namely the use of surface normals or more than 1024 points. Using normals or more points have been shown to improve performance in the literature. Our objective is to study factors that are not commonly perceived as a major source of performance increase. So we scope our analysis to the most widely adopted input scheme which uses 1024 points with only  coordinates.

\smallsec{Data Augmentation} Various data augmentation strategies like \texttt{jittering}, \texttt{random rotation along y-axis}, \texttt{random scaling} and \texttt{random translation}. Different methods use different combinations of these augmentations. PointNet and PointNet++ use all the above augmentations. However, as objects in ModelNet40 are aligned, \texttt{random rotation along y-axis} adversely affects the performance of a model. Hence recent methods, including RSCNN and DGCNN, do not use it. They use only \texttt{random translation} and \texttt{random scaling}. Some methods including PointCNN make a distinction between whether or not \texttt{random rotation along y-axis} is used, but it is not a common practice. 

\smallsec{Input Points} PointNet and PointNet++ use a fixed set of 1024 points per object to train the network. We refer to it as the \texttt{fixed points} strategy. RSCNN and PointCNN randomly sample points during each epoch, effectively exposing the model to more than 1024 points per object during the training process. We refer to this as the \texttt{resampled points} strategy.

\smallsec{Loss Function} \texttt{cross-entropy} (CE) is used by most of the methods. However, DGCNN uses \texttt{smooth-loss}, where the ground-truth labels are smoothed out before calculating cross-entropy. We observe that \texttt{smooth-loss} improves the performance of all network architectures. 

\smallsec{Selecting Model for Testing} PointNet and PointNet++ use the final converged model to evaluate on the test set. Since the number of epochs is a hyper-parameter that depends on factors like data, model, optimizer, and loss, in our experiments, we create a validation set from the training set to tune the number of epochs. We then retrain the model with the complete training set to the tuned number of epochs. We refer to this strategy as \texttt{final model selection}. We find that some methods including DGCNN and RSCNN evaluate the model on the test set after every epoch and use the best test performance as the final performance. We refer to this strategy as \texttt{best test model selection}.
\begin{table*}[ht]
  \caption{Performance of various architectures on ModelNet40. Protocol affects performance by a large amount. \method{} performs on par or better than prior architectures across protocols.}
  \label{tab:perf_all}
  \centering
  \begin{tabular}{l@{\hskip 4mm}c@{\hskip 4mm}c@{\hskip 4mm}c@{\hskip 4mm}c@{\hskip 4mm}c@{\hskip 4mm}cc}
    \hline
     Protocol       &  \multicolumn{2}{c}{PointNet++}  & \multicolumn{2}{c}{RSCNN} & \multicolumn{3}{c}{DGCNN} \\

\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
\cmidrule(lr){6-8}
    Architecture    &  no Vote    &  Vote    & no Vote &  Vote &  CE &  Smooth & Smooth \\
    & & & & & & &  (Best Run) \\ 
    \hline
    PointNet   & 89.0  0.2 & 89.1  0.2 & 90.0  0.3 & 90.1  0.2 & 90.1  0.2 & 90.5  0.1 & 90.7\\
    PointNet++ & 89.8  0.2 & 90.0  0.3 & 92.7  0.1 & \textbf{93.3}  0.3 & 92.6  0.2 & 93.1  0.2 & 93.3\\
    DGCNN      & 90.0  0.4 & 90.5  0.4 & 92.2  0.1 & 92.8  0.5 & 91.9  0.2 & 92.7  0.1 & 92.9\\
    RSCNN      & 89.4  0.1 & 90.2  0.2 & 92.1  0.1 & 92.5  0.2 & 91.9  0.2 & 92.5  0.1 & 92.6 \\
    \method{}    & \textbf{90.7}  0.3 & \textbf{91.0}  0.2 & \textbf{92.9}  0.2 & 93.2  0.1 & \textbf{93.1}  0.1 & \textbf{93.6}  0.3 & \textbf{93.9} \\
    \hline
  \end{tabular}
  \vspace{-2mm}
\end{table*}


\begin{table*}[ht]
\caption{DGCNN augmentation, best test model-selection and smooth loss improve the performance of all architectures.}
\label{tab:protocol_comparison}
\centering
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{2}{c}{Data Augmentation} &
\multicolumn{2}{c}{Model Selection}    &
\multicolumn{2}{c}{Loss}    & 
\multicolumn{4}{c}{Architecture}    \\ 
\cmidrule(lr){1-2}
\cmidrule(lr){3-4}
\cmidrule(lr){5-6}
\cmidrule(lr){7-10}
PN++ & DGCNN & Final & Best Test  & C.E. & Smooth & PointNet & PN++ & DGCNN & RSCNN \\
\midrule
\checkmark      &            &             & \checkmark & \checkmark &             &     &        &  &   \\ \checkmark      &            & \checkmark  &            & \checkmark &             &      &        &   &   \\ & \checkmark            & \checkmark  &            & \checkmark &             &      &        &   &   \\ & \checkmark & \checkmark   & &            & \checkmark  &     &       &   &  \\ \bottomrule
\end{tabular}
\end{table*}

\smallsec{Ensemble Scheme} Some methods use an ensemble to further improve the performance. PointNet and PointNet++ apply the final network to multiple rotated and shuffled versions of the point cloud, and average the predictions to make the final prediction. We refer to this strategy as \texttt{Rotation Vote}. The shuffling operation induces randomness in prediction for PointNet++ and RSCNN as they are not strictly invariant to the order of the points (Sec. 3.3 in \cite{qi2017pointnetplusplus}). Hence, while evaluating \texttt{Rotation Vote}, we do the inference 10 times per run for PointNet++ and RSCNN, and report mean and standard deviation. SimpleView and PointNet are invariant to the order of the points and hence are not affected by shuffling. Some methods, including RSCNN and DensePoint, create multiple randomly scaled and randomly sampled versions of a test object. They then evaluate the final network on these multiple versions of the object and average the prediction. Since the scaling is random, it makes the test set performance random as well. RSCNN and DensePoint repeat this procedure 300 times on the test set and report the best accuracy. We refer to it as \texttt{Repeated Scaling Vote}. DGCNN does not use any ensemble.

Table~\ref{tab:protocols} summarizes the \textit{PointNet++}, \textit{DGCNN} and \textit{RSCNN} protocols. Besides these three protocols, we also include variants of these protocols in table~\ref{tab:perf_all} and table~\ref{tab:one_one}, such as \textit{PointNet++ no Vote} (i.e. \textit{PointNet++} but without the \texttt{Rotation Vote}), \textit{DGCNN CE} (i.e. \textit{DGCNN} but with \texttt{CE loss} intead of \texttt{smooth loss}), \textit{DGCNN CE Final} (i.e. \textit{DGCNN CE} but with \texttt{final model selection} instead of \texttt{best test model selection}) and \textit{RSCNN no Vote} (i.e. \textit{RSCNN} but without the \texttt{Rotation Vote}). These protocols represent prototypical settings and have been used with slight modifications in many other prior works. 

For example, DeepSets~\citep{zaheer-deepsets-nips17} used the \textit{PointNet++ no Vote} protocol without jittering and translation; SO-Net~\citep{li-sonet-cvpr18} used the \textit{DGCNN CE} protocol with jittering and random scaling instead of random scaling and translation; 3DmFV~\citep{ben20183dmfv} used the \textit{DGCNN CE} protocol with additional jittering; PCNN~\citep{atzmon2018point} used the \texttt{DGCNN CE Final} protcol\footnote{It is unclear from code if the best test or final model selection is used. We assume final model selection to err on the side of caution.}; PointCNN~\citep{li-pointcnn-ar18} used the \textit{DGCNN CE} protocol with randomly sampled points and small () rotation augmentation; DensePoint~\citep{liu2019densepoint} used the \textit{RSCNN} protocol; PointASL~\citep{yan2020pointasnl} used the \textit{DGCNN CE} protocol but with additional point jittering augmentation and voting.

\smallsec{Our Protocol}
Based on our findings, we define our \textit{\method{}} protocol, which uses the best augmentation and loss functions while not using any information from the test set. Table~\ref{tab:protocol_comparison} shows that DGCNN's augmentation (i.e \texttt{random translation and scaling}) and \texttt{smooth-loss} improve performance of all prior networks, so we use them in the \textit{\method{}} protocol. Further, similar to PointNet, PointNet++ and DGCNN, we use the fixed dataset of 1024 points instead of re-sampling different points at each epoch. Re-sampling points for each epoch changes (increases) the training dataset. Hence to keep the training dataset same as very initial works (PointNet and PointNet++) that established the point cloud classification, we used fixed  set of 1024 points. We avoid any feedback from the test set and use the \texttt{final model selection}, where we first tune the number of epochs on the validation set then retrain the model on the entire train set. Lastly, similar to DGCNN, we do not use ensemble as it is more standard in Machine Learning to compare models without ensemble. 
\comment{We strongly disagree with using any feedback from the test set to select the final model. Using the best performing model on the test set breaks the held-out blind test set assumption, hence the performance on the test set is no longer a reliable indicator of generalization, which is a central goal in machine learning~\citep{bishop2006pattern}.}
\begin{table}
  \begin{minipage}[]{\columnwidth}
  \caption{Performance of various architectures on ModelNet40 when using the best data-augmentation and loss function; and not using any feedback from test set. \method{} outperforms prior architectures, while having fewest parameters and comparable inference time.}
  \label{tab:our_protocol}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{lcccc}
    \toprule
                               &  Acc.    & Class &  Para.   & Time   \\
    Architecture   &          & Acc.  & (M)      & (ms) \\
    \midrule
    PointNet   & 89.2  0.9 & 85.1  0.6 & 3.5 & \textbf{3.0}   \\
    PointNet++ & 92.7  0.1 & 90.0  0.3 & 1.7 & 20.7  \\
    DGCNN      & 91.9  0.3 & 89.1  0.3 & 1.8 & 7.3   \\
    RSCNN      & 91.7  0.3 & 88.5  0.4 & 1.3 & 4.3  \\
    \method{}  & \textbf{93.0}  0.4 & \textbf{90.5}  0.8 & \textbf{0.8} & 5.0      \\
    \bottomrule
  \end{tabular}}
\end{minipage}
  \hspace{2mm}
  \begin{minipage}[]{\columnwidth}
      \caption{Performance of various architectures on ScanObjectNN, and cross-dataset generalization. \method{} achieves state-of-the-art results and shows better cross dataset generalization. Numbers for prior works are from~\citep{uy2019revisiting}.}
        \label{tab:performance_scanobjectnn}
        \centering
        \resizebox{\columnwidth}{!}{
        \begin{tabular}{l@{\hskip 3mm}c@{\hskip 3mm}c@{\hskip 3mm}c}
            \toprule
                                       & TR: SONN  & TR: MN40  & TR: SONN \\
            Architecture   & TE: SONN  & TE: SONN  & TE: MN40  \\
            \midrule
            3DmFV~(Shabat et al.)                & 63.0 & 24.9 & 51.5 \\
            PointNet~(\citeauthor{qi-pointnet-cvpr17})       & 68.2 & 31.1 & 50.9 \\
            SpiderCNN~(\citeauthor{xu2018spidercnn})         & 73.7 & 30.9 & 46.6 \\
            PointNet++~(\citeauthor{qi2017pointnetplusplus}) & 77.9 & 32.0 & 47.4 \\
            DGCNN~(\citeauthor{wang2018edgeconv})            & 78.1 & 36.8 & 54.7 \\
            PointCNN~(\citeauthor{li-pointcnn-ar18})         & 78.5 & 24.6 & 49.2 \\
            \method{}                               & \B 79.50.5 & \B 40.51.4 & \B 57.92.1 \\
            \bottomrule
        \end{tabular}}
  \end{minipage}
\end{table}
\subsection{\method}
Given a set of points \method{}, projects them onto the six orthogonal planes to create sparse depth images. It then extracts features from the depth images using a CNN and fuses, which is then used to classify the point-cloud as shown in Fig.~\ref{fig:simpleview}. 

\smallsec{Generating Depth Images from Point Cloud} Let  be the coordinates of a point in the point cloud with respect to the camera. We apply perspective projection to get the 2D coordinate  of  at depth . We also do ablations with orthographic projection and found perspective projection to work slightly better (Table.~\ref{tab:ablation}). Since coordinates on image plane have to be discrete, we use  to be the final coordinate of  on the image plane. Multiple points may be projected to the same discrete location on the image plane. To produce depth value at an image location, we do ablations on two choices, one the minimum depth of all points, and other weighted average depth with more weight () given to closer points (Table.~\ref{tab:ablation}). Empirically, we find both perform similar with the later performing slightly better. This could be because of reduction in noise due to the averaging of nearby pixels on the surface. The depth images are of resolution 128 X 128.

\smallsec{{\method } Architecture} To make the number of parameters comparable to point-based methods, we use ResNet18 with one-fourth filters (ResNet18/4) as the backbone. For fusing features, we do ablation with two choices, pooling and concatenation. Empirically, we find concatenation to work better than pooling them (Table.~\ref{tab:ablation}). This could be because pooling features throws away the view information like which views are adjacent to one another. One concern could be that concatenation could make features sensitive to viewpoint, and hence the network could fail on rotated objects. However, empirically, we observe that this issue is largely mitigated by rotation augmentation and \method{} is able to achieve state-of-the-art performance on ScanObjectNN where objects are rotated. The point-clouds are scaled to be in , we keep the cameras at a distance of  units from the center with  fov. We also do ablations with different number of views, comparing only front views, three orthogonal views and six orthogonal views. We find that using all six views performs the best (Table~\ref{tab:ablation}). We do not use ImageNet pretraining, thus making the comparison with point-based methods strictly fair, without any additional data.

\section{Experiments}
\smallsec{ModelNet40} ModelNet40 is a the most widely adopted benchmark for point-cloud classification. It contains objects from 40 common categories. There are 9840 objects in the training set and 2468 in the test set. Objects are aligned to a common up and front direction. 

\smallsec{ScanObjectNN} ScanObjectNN is a recent real-world point cloud classification dataset. It consists of 15 classes, 11 of which are also in ModelNet40. There are a total of 15k objects in the dataset. Unlike ModelNet40, the objects in ScanObjectNN are obtained from real-world 3D scans. Hence, point clouds are noisy (occlusions, background points) and have geometric distortions such as holes. Also, unlike ModelNet40, the objects are not axis-aligned.

\subsection{Experiments on ModelNet40}
\begin{table*}[ht]
\caption{Ablation of various choices for \method{} on ModelNet40. The performance is evaluatated on the validation set.}
\label{tab:ablation}
\resizebox{1.95\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\toprule
         & \multicolumn{3}{c}{Number of Views} & \multicolumn{2}{c}{Image Projection} & \multicolumn{2}{c}{Feature Fusion} & \multicolumn{2}{c}{Image Depth} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-6}
\cmidrule(lr){7-8}
\cmidrule(lr){9-10}
         & 1          & 3          & 6         & Orthographic      & Perspective      & Pool            & Concat           & Minimum     & Weighted Avg.     \\ \midrule
Accuracy & 90.7  0.1 & 92.1  0.2 &  92.9  0.3 & 92.7  0.3 & 92.9  0.3 & 91.8  0.3 & 92.9  0.3 & 92.8  0.4 & 92.9  0.3 \\ \bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table*}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.85\linewidth]{fig/quant.pdf}
\vspace{-3mm}
\caption{Failure Cases for SimpleView and PointNet++. The first row shows cases where both SimpleView and PointNet++ fail; the second row shows cases where SimpleView succeeds but PointNet++ fails; the third row shows cases where SimpleView fails but PointNet++ succeeds.}
\label{fig:quant}
\end{figure*}
\smallsec{Implementation Details} We use PyTorch~\citep{pytorch} to implement all models and protocols while reusing the official code wherever possible. We use the official version of DGCNN and RSCNN. We confirm with the authors that the code for RSCNN-Multi, another version of RSCNN, is yet to be released. Hence we use the reported numbers of RSCNN-Multi in Table~\ref{tab:one_one}. PointNet and PointNet++ are officially released in TensorFlow~\citep{tensorflow15}. For PointNet, we adapt our code from PointNet.pytorch~\citep{pointnet_pytorch} as recommended in the official repository. For PointNet++, we adapt the model code from Pointnet2\_PyTorch~\citep{pointnet2_pytorch}. We further make sure that the third party PyTorch code closely matches the official TensorFlow code.
\begin{table*}[ht]
\centering
\caption{Performance of various architectures on ModelNet40 when using different amount of training data.}
\label{tab:per_dataset}
\begin{tabular}{cccccc}
\toprule
\% of Training Data & RSCNN & DGCNN & PointNet & PointNet++ & SimpleView \\ \midrule
 25 \% & 88.2  0.4 & 89.1  0.2 & 86.3  0.4 & 89.6  0.4 & 89.7  0.3 \\
 50 \% & 90.4  0.4 & 91.0  0.3 & 88.2  0.3 & 91.5  0.2 & 92.1  0.3 \\
 100 \%& 91.7  0.3 & 91.9  0.3 & 89.2  0.9 & 92.7  0.1 & 93.0  0.4\\
\bottomrule
\end{tabular}
\vspace{-4mm}
\end{table*}
\begin{table*}[ht]
\caption{Performance of various architectures on ModelNet40. Includes prior works not in Table~\ref{tab:perf_all}. * indicates small differences in protocol as identified in Sec.~\ref{sec:variation}}
\label{tab:one_one}
\centering
\begin{tabular}{lclccc}
\toprule
Architecture & \# Points & Closest Protocol & Acc. & PN++ Acc. & SimpleView Acc. \\ \midrule
DeepSets~(\citeauthor{zaheer-deepsets-nips17})    & 5000    & PointNet++ no Vote* & 90.0  0.3 &  89.8  0.2  & 90.7  0.3      \\
SO-Net~(\citeauthor{li-sonet-cvpr18}) & 2048   & DGCNN CE* & 90.9 & 92.6  0.2  & 93.1  0.1 \\
3DmFV~(\citeauthor{ben20183dmfv})      & 1024  & DGCNN CE*      & 91.4 & 92.6  0.2      & 93.1  0.1 \\
PCNN~(\citeauthor{atzmon2018point})      & 1024  & DCNN CE Final*                & 92.3 &  92.1  0.1  &   92.5  0.3 \\
PointCNN~(\citeauthor{li-pointcnn-ar18})   & 1024   & DGCNN CE*      & 92.5 & 92.6  0.2  & 93.1  0.1 \\
DensePoint~(\citeauthor{liu2019densepoint}) & 1024  & RSCNN no Vote   & 92.8 & 92.7  0.1     & 92.9  0.2\\
RSCNN-Multi~(\citeauthor{liu2019relation})      & 1024  & RSCNN no Vote   & 92.9 & 92.7  0.1      & 92.9  0.2\\
PointANSL~(\citeauthor{yan2020pointasnl}) & 1024  & DGCNN CE*      & 92.9 & 92.6   0.2     & 93.1  0.1 \\
\bottomrule
\end{tabular}
\end{table*}

We use Adam~\citep{kingma2014adam} with an initial learning rate of 1e-3 and a decay-on-plateau learning rate scheduler. The batch size and weight decay for each model are kept the same as the official version in Table~\ref{tab:perf_all}. We use a batch size of 18 and no weight decay for \method{}. To give the prior models the best chance on our protocol (Table ~\ref{tab:our_protocol}), we additionally tune their hyper-parameters on the validation set. We find that the official hyper-parameters already perform close to optimal. We train each model for 1000 epochs. Since there are small variations in final performance across different runs, we do 4 runs and report the mean and standard deviation.

\smallsec{Performance under Prior Protocols}
Table~\ref{tab:perf_all} shows the performance different architectures under various protocols. The mean performance of PointNet++ improves from 89.8\% to 93.3\% when we switch from the \textit{PointNet++ no Vote} to the \textit{RSCNN Vote} protocol. Similarly the performance of \method{} improves from 90.7\% to 93.6\% when we switch from \textit{PointNet++ no Vote} to \textit{DGCNN Smooth}. Since there is variance in performance across runs, we refrain from making any claims about absolute ordering between prior works. However, we do observe that in terms of mean performance, \method{} performs on par or better than other methods under all protocols. Note that in \textit{RSCNN Vote}, voting on the test set is done 300 times with reshuffled and randomly augmented points, from which the highest accuracy is selected. Hence models that have the largest variance in prediction, i.e. PointNet++ and RSCNN gain the most from it, as they are not strictly invariant to the order of points (Sec. 3.3 in ~\cite{qi2017pointnetplusplus}). 

\smallsec{Performance under the \textit{\method{}} Protocol} 
Table~\ref{tab:our_protocol} shows that \method{} outperforms prior architectures on our controlled protocol in terms of mean performance. \method{} has the fewest number of parameters and a competitive inference speed. Inference speed is measured on an NVIDIA 2080Ti averaged across 100 runs.
\comment{To measure the inference speed, we first warm-up an NVIDIA 2080Ti GPU by running the inference 10 times, then take the average inference speeds of 100 runs.}

Fig.~\ref{fig:quant} show examples where both SimpleView and PointNet++ fail, as well as examples where one of them fails and the other succeeds. Qualitatively, we find that the failure modes of SimpleView and PointNet++ are similar. We also find that a major failure mode in both SimpleView and PointNet++ is the confusion between the ‘flower\_pot’ and ‘plant’ category (see Appendix Fig.1 and Fig.2). This could be because of the lack of color information.  In Table~\ref{tab:per_dataset}, we show how the models perform using varying amount of training data. We find the SimpleView outperforms the state-of-the-art methods across different dataset sizes.

\smallsec{Comparison with More Methods} In Table~\ref{tab:one_one}, we do one-on-one comparison between \method{} and recent state-of-the-art methods, other than PointNet, PointNet++, RSCNN and DGCNN. We identify the closest protocol to the one used in the paper from the ones we evaluate. Table~\ref{tab:one_one} shows the competitiveness of PointNet++ and \method{} with other recent state-of-the-art methods.

\subsection{Experiments on ScanObjectNN}
\smallsec{Implementation Details} ScanObjectNN's official repository trains and evaluates the state-of-the-art models under the same protocol. We implement \method{} in TensorFlow and use the official ScanObjectNN protocol for fairness. This protocol is different from the \textit{SimpleView} protocol as it normalizes the point clouds and randomly samples points. We optimize our model with Adam. We use a batch size of 20 and no weight decay to train \method{} for 300 epochs with an initial learning rate 0.001, and use the final model for testing. We use standard image-based cropping and scaling augmentation to prevent over-fitting. The hyper-parameter for cropping and scaling is found on a validation set made from ScanObjecNN's train set. We conduct 4 runs for \method{}. ScanObjectNN does not use a fixed set of points during test time. It instead randomly samples points from the point cloud, which adds randomness to test set performance. Hence, we evaluate each run 10 times. We report the final performance as the mean and variance of the 40 evaluations (4 runs  10 evaluations per run).

\smallsec{Performance on ScanObjectNN} As shown in Table~\ref{tab:performance_scanobjectnn}, \method{} outperforms prior networks on ScanObjectNN. This shows the \method{} is effective in real world settings, with noisy and misaligned point clouds. For future reference, SimpleView gets an accuracy of  while using the best test model selection scheme. We also perform transfer experiments to test generalizability of \method{}. We train on ScanObjectNN and test on ModelNet40 and vice versa. Table~\ref{tab:performance_scanobjectnn} shows that \method{} transfers across datasets better than prior methods. 

\section{Discussion}
In this work, we demonstrate how auxiliary factors orthogonal to the network architecture have a large effect on performance for point-cloud classification. When controlling for these factors, we find that a relatively older method, PointNet++~\citep{qi2017pointnetplusplus}, performs competitively with more recent ones. Furthermore, we show that a simple baseline performs on par or better than state-of-the-art architectures. 

Our results show that for future progress we should control for protocols while comparing network architectures. Our code base could serve as a useful resource for developing new models and comparing them with prior works. Our results show that the evidence for point-based methods is not very strong when auxiliary factors are properly controlled for, and that SimpleView is a strong baseline. But our results are not meant to discourage future research on point-based methods. It is still entirely possible that point-based methods come out ahead with additional innovations. We believe it is beneficial to explore competing approaches, including the ones that are underperforming at a particular time,  as long as the results are compared in a controlled manner. 

Our analysis in this work was limited to point cloud classification, which is an important problem in 3D scene understanding and forms a critical part of object detection and retrieval systems. An exciting future direction would be to expand this analysis to other problems that involve point cloud data such as scene and part segmentation.

\smallsec{Acknowledgement} This work is partially supported by the Office of Naval Research under Grant N00014-20-1-2634.

\appendix
\section{Appendix}
\label{appendix}
\setcounter{figure}{0}
\renewcommand\thefigure{\Roman{figure}}
\setcounter{table}{0}
\renewcommand\thetable{\Roman{table}}
\begin{figure}[h]
    \includegraphics[width=0.9\linewidth]{fig/conf_SimpleView.pdf}
    \caption{Confusion matrix for SimpleView when trained under our protocol}
    \label{fig:confusion_simpleview}
\end{figure}
\begin{figure}[h]
    \includegraphics[width=0.9\linewidth]{fig/conf_PointNet++.pdf}
    \caption{Confusion matrix for PointNet++ when trained under our protocol}
    \label{fig:confusion_pointnet++}
\end{figure}
\appendix


\bibliography{example_paper}
\bibliographystyle{icml2021}

\end{document}
