

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{gensymb}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\usepackage{microtype}      \usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{mathtools}

\usepackage{graphicx}

\usepackage{caption}

\usepackage{enumitem}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{soul}
\setul{0.3ex}{0.2ex}
\setulcolor{blue}

\usepackage{color}
\definecolor{mydarkblue}{rgb}{0.33,1,1}
\hypersetup{colorlinks,citecolor={blue},urlcolor={blue}, linkcolor={red}}


\newcommand{\R}{\mathbb{R}}

\icmltitlerunning{Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups}

\begin{document}

\twocolumn[
\icmltitle{Exploiting Redundancy: Separable Group \\Convolutional Networks on Lie Groups}







\begin{icmlauthorlist}
\icmlauthor{David M. Knigge}{uva}
\icmlauthor{David W. Romero}{vu}
\icmlauthor{Erik J. Bekkers}{uva}
\end{icmlauthorlist}

\icmlaffiliation{uva}{Universiteit van Amsterdam, Amsterdam, The Netherlands}
\icmlaffiliation{vu}{Vrij Universiteit Amsterdam, Amsterdam, The Netherlands}

\icmlcorrespondingauthor{David Mattanja Knigge}{d.m.knigge@uva.nl}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions. In order to obtain equivariance to arbitrary affine Lie groups we provide a continuous parameterisation of separable convolution kernels. We evaluate our approach across several vision datasets, and show that our weight sharing leads to improved performance and computational efficiency. In many settings, separable G-CNNs outperform their non-separable counterpart, while only using a fraction of their training time. In addition, thanks to the increase in computational efficiency, we are able to implement G-CNNs equivariant to the  group; the group of dilations, rotations and translations of the plane.  -equivariance further improves performance on all tasks considered, and achieves state-of-the-art performance on rotated MNIST. Code is available on Github \footnote{\url{https://github.com/david-knigge/separable-group-convolutional-networks}}.
\end{abstract}

\section{Introduction}
\citet{minsky1988perceptrons} suggest that the power of the perceptron comes from its ability to \textit{learn to discard} irrelevant information. In other words; information that does not bear significance to the current task does not influence representations built by the network. According to \citet{minsky1988perceptrons}, this leads to a definition of perceptrons in terms of the \textit{symmetry groups} their learned representations are invariant to. Progress in geometric deep learning has shown the power of pro-actively equipping models with such geometric structure as inductive bias, reducing model complexity and improving generalisation and performance \citep{bronstein2017geometric}. An early example of such geometric inductive bias at work can be seen in the convolutional layer in a CNN \citep{lecun1998gradient}. CNNs have been instrumental in conquering computer vision tasks, and much of their success has been attributed to their use of the convolution operator, which commutes with the action of the translation group. This property, known as \textit{equivariance} to translation, comes about as a result of the application of the same convolution kernel throughout an input signal, enabling the CNN to learn to detect the same features at any location\break in the input signal, directly exploiting translational symmetries that naturally occur in many tasks.

Although invariance to object-identity preserving transformations has long been recognised as a desirable model characteristic in machine learning literature \citep{kondor2008group, cohen2013learning, sifre2014rigid}, only recently \citet{cohen2016group} introduced the Group Equivariant CNN (G-CNN) as a natural extension of the CNN \citep{lecun1998gradient}, generalising its equivariance properties to group actions beyond translation. The layers of a G-CNN are explicitly designed to be equivariant to such transformations, hence the model is no longer burdened with \textit{learning} invariance to transformations that leave object identity intact. It has since been shown that equivariant deep learning approaches may serve as a solution in fields that as of yet remain inaccessible to machine learning due to scarce availability of labelled data, or when compact model design due to limited computational power is required \citep{winkels20183d, linmans2018sample,bekkers2019b}.

\textbf{Complexity and redundancy issues impeding regular group convolutions}
\begin{figure*}
\centering
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/group-conv-kernel.png}}
        \caption{\vspace{-3mm}}
        \label{fig:nonsep-gconv-kernel}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/subgroup-conv-kernel.png}}
        \caption{\vspace{-3mm}}
        \label{fig:subgroup-gconv-kernel}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/r2-conv-kernel.png}}
        \caption{\vspace{-3mm}}
        \label{fig:r2-gconv-kernel}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/combined-separable-kernel.png}}
        \caption{\vspace{-3mm}}
        \label{fig:combined-separable-gconv-kernel}
    \end{subfigure}
\caption{In group convolutions on affine Lie groups, a feature map  defined over the group  is convolved with a filter , shown in Fig. \ref{fig:nonsep-gconv-kernel}. We propose separating this convolution into two sequential operations: a convolution over the subgroup  with a kernel , followed by a convolution over the spatial dimensions with a kernel , shown in Figs. \ref{fig:subgroup-gconv-kernel}, \ref{fig:r2-gconv-kernel} respectively. Importantly, this greatly reduces computational complexity while retaining equivariance properties, allowing for application of equivariant deep learning models to larger groups . This factorisation intuitively corresponds to composing  by sharing a reweighting of  along  with coefficients given by , shown in Fig. \ref{fig:combined-separable-gconv-kernel}.
\vspace{-2mm}}
\label{fig:sepgconvs}
\end{figure*}
A growing body of work shows applications of G-CNNs consistently and decisively outperforming classical CNNs \citep{worrall2017harmonic, weiler20183d, bekkers2018roto, esteves2018learning, bekkers2019b, worrall2019deep, sosnovik2021scale}. However, a practical challenge impeding application to larger groups is the computational complexity of regular group convolutions, which scales exponentially with the dimensionality of the group.
Furthermore, \citet{lengyel2021exploiting} show that group convolution filters in the original formulation of the G-CNN by \citet{cohen2016group} exhibit considerable redundancies along the group axis for the  and  groups. Similar observations motivated depthwise separable convolutions \citep{chollet2017xception}, which not only increased parameter efficiency but also model performance; observed correlations between weights are explicitly enforced with further parameter sharing through the use of kernels separable along spatial and channel dimensions. We address the observations of redundancy along with the scalability issues of regular G-CNNs in their current form. Our paper contains the following contributions:
\begin{itemize}[topsep=0pt,itemsep=-1pt, leftmargin=0.5cm]
    \item We introduce separable group convolutions for affine Lie groups , sharing the kernels for translation elements  along subgroup elements . See Fig. \ref{fig:sepgconvs} for an overview.
    \item We propose the use of a SIREN \citep{sitzmann2020implicit} as kernel parameterisation in the Lie algebra - imposing a fixed number of parameters per convolution kernel, regardless of the resolution at which this kernel is sampled, and ensuring smoothness over the Lie group.
    \item  Separable group convolutions allow us to build -CNNs, which we thoroughly experiment with. We show equivariance to  increases accuracy over a range of vision benchmarks.
    \item To achieve equivariance to continuous affine Lie groups, we propose a random sampling method over subgroups  for approximating the group convolution operation.
\end{itemize}
First, we position this work within the area of equivariant deep learning by giving an overview of related works, and explaining which current issues we are addressing with this work.
We derive separable group convolutions, and show how they may be applied to continuous groups. Lastly, we apply these ideas by experimenting with implementations for roto-translations in 2D (), dilation and translation in 2D () and dilation, rotation and translation in 2D (). 

\section{Related Work}
\textbf{Group equivariant convolutional neural networks} Broadly speaking, research on G-CNNs can be divided into two approaches. First, \textit{Regular} G-CNNs use the left-regular representation of the group of interest to learn representations of scalar functions over the group manifold, or a quotient space of the group. The left-regular representation acts on the convolution kernels, yielding an orbit of the kernel under the group action. Convolving the input using these transformed filters, a feature map defined over the group is obtained at each layer. This approach most naturally extends the conventional CNN, where convolution kernels are transformed under elements of the translation group. Regular G-CNNs have been implemented for discrete groups \citep{cohen2016group, winkels20183d, worrall2018cubenet}, compact continuous groups \citep{marcos2017rotation, bekkers2018roto} and arbitrary non-compact continuous Lie groups \citep{bekkers2019b, finzi2020generalizing, romero2020wavelet}. However, practical implementations for continuous groups often require some form of discretisation of the group, possibly introducing discretisation artefacts, and requiring a choice of \textit{resolution} over the group. For the second class, \textit{steerable} G-CNNs, representation theory is used to compute a basis of equivariant functions for a given group, which are subsequently used to parameterise convolution kernels \citep{cohen2016steerable, weiler20183d, weiler2018learning, sosnovik2019scale, sosnovik2021disco}. Although steerable G-CNNs decouple the cardinality of the group from\break the dimensionality of the feature maps, this approach is only compatible with compact groups.

The current paper may, in approach, be compared to \citet{bekkers2019b} and \citet{ finzi2020generalizing}, who define convolution kernels on the Lie algebra of continuous groups to enable convolutions on their manifold. Similarly, we make use of the Lie algebra and exponential map to obtain convolution kernels on the group, but separate the kernels by subgroups.

\citet{bekkers2019b} defines a set of basis vectors in the Lie algebra, which, when combined with the exponential map, allow for the identification of group elements by a vector in . Subsequently, a set of B-splines is defined on the algebra, which form a basis to expand convolution kernels in. A linear combination of these bases creates a locally continuous function on the Lie algebra defining a convolution kernel and its behaviour under transformations of the group. Although this method allows for direct control over kernel smoothness, the learned convolution filters are limited in their expressivity by their basis functions. \citet{finzi2020generalizing} instead use an MLP to learn convolution kernels on the Lie algebra, which in addition allows them to handle point cloud data. The MLP is constructed to learn kernel values at (arbitrary) relative offsets in the Lie algebra. In contrast, we propose to use SIRENs \citep{sitzmann2020implicit} to parameterise convolution kernels, as they have been shown to outperform other forms of MLPs in parameterising convolution kernels \citep{romero2021ckconv}, and offer more explicit control over kernel smoothness; a desirable property for addressing \break discretisation artefacts that occur when modelling features on continuous groups (see Appx. \ref{app:kernelsmoothnesscontinuous}).

\textbf{Separable filters in machine learning}
In image processing, spatially separable filters have long been known to increase parameter- and computational efficiency, and learning such constrained filters may even increase model performance \citep{rigamonti2013learning}. In \citet{sifre2014rigid}, authors investigate SE(2)-invariant feature learning through scattering convolutions, and propose separating the group convolution operation for affine groups into a cascade of two filterings, the first along the spatial dimensions , and the second along subgroup dimension . From this, authors derive a separable approach to the convolution operation with learnable filters as used in CNNs. This formulation has since been named the \textit{depthwise-separable} convolution \citep{chollet2017xception}, a special case of the Network-In-Network principle \citep{lin2013network} which forms the basis for the success of the Inception architectures \citep{szegedy2015going}. In depthwise separable convolutions, each input channel first gets convolved using a (set of) kernel(s) with limited spatial support. Afterwards, a 1x1 convolution is used to project the feature set detected in the input channels to the output space. \citet{chollet2017xception} speculates that the Inception architectures are successful due to the explicit separation of spatial and channel mapping functions, whereas in conventional CNNs, kernels are tasked with simultaneously mapping inter-channel and spatial correlations.

\citet{haase2020rethinking} argue that the original formulation of the depthwise-separable convolution reinforces inter-kernel correlations, but does not in fact leverage intra-kernel correlations. They propose an inverse ordering of the operations given in depthwise-separable convolutions, sharing the same spatial kernel along the input channels, and show convincing results. Extending this investigation of learned convolution filters to the original G-CNN \citep{cohen2016group}, \citet{lengyel2021exploiting} remark on the high degree of correlation found among filters along the rotation axis, and propose to share the same spatial kernel for every rotation feature map. We aim to generalise this approach, proposing separable convolutions on arbitrary affine Lie groups.
\section{Background}
In the following section we give the theoretical background for implementing group convolutions for arbitrary affine Lie groups. We assume familiarity with the basics of group theory and provide the relevant concepts in Appx. \ref{app:grouptheory} and Appx. \ref{app:cnnsgrouptheory}. For simplicity of notation, we initially assume that our input signal/feature map  has a single channel.

\textbf{Lifting convolutions} To preserve information on the \text{pose} of features in the input, an equivariant convolution operation is achieved by \textit{lifting} a function from the input space to (a homogeneous space of) the group. As we are interested in real signals, specifically image data living on , we assume the Lie group of interest  is taken in semidirect product with the domain of our data; . In group convolutions, a given kernel is left-acted by all transformations in , thereby generating a signal on the higher dimensional space  instead of . Hence, the output feature maps disentangle poses through a domain expansion, e.g. positions plus rotations or scales. For a given group element , kernel , and location  in the input domain , the lifting convolution is given by:

where  and  is the kernel  transformed via the action of group element  via , and with  the determinant of the matrix representation of  that acts on . The output of lifting convolutions yields a -feature map with the original two spatial input dimensions (), and an additional group dimension (). See Fig. \ref{fig:liftingconv}.

\textbf{Group convolutions}
Now that the data is lifted to the domain of the group, we continue with group convolutions in subsequent layers. Given a kernel  (now a function on ), Haar measures  and  on the group  and sub-group  respectively, group convolutions are given by:

Evaluating this convolution for every group element , we again obtain a function defined on . As we know , we can factor this operation into a transformation of a kernel  by a group element , , followed by a convolution at every spatial location in . See Fig. \ref{fig:groupconv}.

\textbf{Achieving invariance} Using lifting and group convolution operations, we can construct convolutional layers that co-vary with the action of the group and explicitly preserve pose information throughout the representations of the network. In most cases, we ultimately want a representation that is invariant to transformations of the input in order to achieve invariance to these identity-preserving transformations. This is achieved by aggregating the information at all group elements in a feature map with an operation invariant to the group action, e.g., max-, mean- or sum-projection. In practice, this is done after the last group convolution, and is followed by one or more fully connected layers.

\section{Separable Group Convolutions on Lie Groups} \label{sec:sep-ckgconvs}
\textbf{Redundancies in group convolution filters} Similar to \citet{haase2020rethinking,lengyel2021exploiting} we investigate parameter efficiency of learned convolution kernels to motivate separable filters. We train an -equivariant CNN on the Galaxy10 dataset (see Sec. \ref{sec:experiments} for experimental details) and analyse the resulting group convolution kernels. We apply PCA by treating the values of the group convolution kernel for each subgroup element  as distinct spatial kernels with  features. The ratio of variance explained by the first principle component gives an indication of the variability of the group convolution kernel along the subgroup axis. If the ratio of explained variance is high, the distinct spatial convolution kernels along the subgroup axis are well-characterised by a single shared kernel. In Fig. \ref{fig:redundancy}, results are shown for the group convolution layers in our -CNN before and after training. We find that during the training process, redundancy along the subgroup axis increases considerably. This motivates sharing a single spatial kernel along the subgroup elements, which can be achieved by separating the group convolution operation \break into a convolution over the subgroup , followed by a convolution over the spatial domain .
\begin{figure}
\begin{center}
\includegraphics[width=0.48\textwidth]{figures/Galaxy10.png}
\end{center}
\caption{A set of histograms showing redundancy in learned group convolution kernels. On the x-axis is the ratio of variance explained by the first principal component when applying PCA on the set of spatial kernels along the group axis of a group convolution kernel. Y-axis shows the proportion of kernels with this explained variance ratio, where all bins sum to 1. The number of spatial kernels is listed in the title of each subfigure. Throughout the training process, redundancy in the group convolution kernels increases. Left to right: subsequent layers in the network. \vspace{-2mm}}
\label{fig:redundancy}
\end{figure}

\textbf{Separable group convolutions} \label{sec:seperablegroupconvs} Let us assume that the convolution kernel  in Eq.~\ref{eq:groupconv} is separable. That is, .  In this case, we can derive the factorised separable group convolution as (see Appx. \ref{app:derivingsepgconv} for the full derivation):


Here  is a convolution kernel over the group , and  is a convolution kernel over the spatial domain . We set , with  constant along  and  constant along . This can be thought of as parameterising a convolution kernel  over the group  by sharing same spatial kernel  weighed by a value  at every input group element , see Fig. \ref{fig:separablekernel}. Importantly, this factorisation greatly increases the efficiency of the group convolution operation. Since  in Eq. \ref{eq:finalsepgconv} does not depend on , we can precompute it. As a result, convolving over a single channel of a feature map  of size  goes from  to . See Fig. \ref{fig:sepgroupconv} for a visual intuition of the separable group convolution.

\textbf{Defining convolution kernels on Lie algebras} In order to perform lifting- and group convolutions (Eqs.~\ref{eq:liftingconv},~\ref{eq:groupconv}), we need to evaluate our convolution kernels  at relative offsets  on the group. As in \citet{bekkers2017template,weiler2018learning,bekkers2019b, finzi2020generalizing}, we express our group convolution kernel  in analytical form, as a function of relative group elements yielding kernel values.\break Motivated by findings in \citet{romero2021ckconv}, we use a Sinusoidal Representation Network (SIREN) \citep{sitzmann2020implicit} as kernel parameterisation. SIRENs lead to great performance improvements over MLPs with  and  when parameterising convolution kernels (we replicate this comparison for G-CNNs in Appx. \ref{app:activationfunctions}). Furthermore, the SIREN offers explicit control of kernel smoothness through a frequency multiplier parameter : an important property discussed in Appx. \ref{app:kernelsmoothnesscontinuous}. Since the affine Lie group can be non-euclidean, and neural networks are functions generally defined over euclidean spaces, we resort to defining the kernel function on the Lie algebra of our group of interest \citep{bekkers2019b,finzi2020generalizing}. The kernel function  maps points in the Lie algebra (which may be associated with the relative offsets on the group we are convolving over by the exponential map) to kernel values, . For a given element , we have: , see Fig. \ref{fig:kernelongroup}. The separable kernels follow the same principle, separated by subgroups, see Appx.~\ref{app:architecture}.

\textbf{Approximating equivariance for compact continuous and non-compact continuous groups} In the case of small discrete subgroups , such as the group  of rotations by 90 degrees, we are able to perform lifting- and group convolution operations exactly equivariant to the action of the group, as the integral given in Eq. \ref{eq:groupconv} is tractable. We consider image data in particular, which is defined over a discrete grid , a subgroup of , making both integrals over the group discrete sums:

with  and  denoting the volume elements corresponding to the grid points. In the case of continuous groups, it is possible to either make a discretisation of the subgroup , or to approximate the group convolution by means of random sampling. To obtain a volumetrically uniform sampling grid over the group, we sample a set of  equidistant points in the Lie algebra as in \citet{bekkers2019b}, and map those to the group using the exponential map to obtain a grid , see Fig. \ref{fig:gridongroup}. For noncompact  such as the dilation group , we localize the support in the Lie algebra. In the case of compact continuous groups such as  we approximate the integral by convolving over a uniformly spaced grid  which is perturbed by left-multiplication with a uniformly randomly sampled group element , i.e.,  . By uniform sampling of  we obtain left-invariant  which only scales the overall convolution result, allowing us to omit it from \ref{eq:discretegconv}. We further let  and obtain the discrete separable group convolution for continuous groups:
-1 \jot]
    &= \sum_{\tilde{\boldsymbol{x}} \in \mathbb{Z}^2} (f * k_H)(\boldsymbol{\tilde{x}}, h) k_{\R^2}({h^{-1}}(\tilde{\boldsymbol{x}}-\boldsymbol{x}))\frac{1}{|h|} \label{eq:discretefinalsepgconv} \\
    &\text{where } (f * k_H)(\boldsymbol{\tilde{x}}, h)=\sum_{\tilde{h} \in \mathcal{H}_{\epsilon}} f(\tilde{\boldsymbol{x}}, \tilde{h})k_H (h^{-1}\cdot \tilde{h}) \nonumber. 

    \mathcal{T}: G \times \mathcal{X} \rightarrow \mathcal{X} \text{ and } \mathcal{T}_g: \mathcal{X} \rightarrow \mathcal{X},

    \mathcal{L}_g: f \rightarrow f' \text{ and for : } f'(a') = f(\mathcal{T}_{g^{-1}}(a')).

    \forall g \in G: \mathcal{L}_{g} \circ \Phi = \Phi \circ \mathcal{L}_{g}.

    k(g) &= k(\boldsymbol{x}, h) \nonumber\\
    &= k_{\mathbb{R}^2}(\boldsymbol{x}) k_H(h).

    (f *_{\mathrm{group}} k) (g)&=\int_G f(\tilde{g})k(g^{-1} \cdot \tilde{g})\,{\rm d}\mu(\tilde{g}) \nonumber \\
    &=\int_{\R^2}\int_H f(\tilde{\boldsymbol{x}}, \tilde{h})\mathcal{L}_{x^{-1}}\mathcal{L}_{h^{-1}}k(\tilde{\boldsymbol{x}}, \tilde{h})\frac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h}\nonumber  \\
    &=\int_{\R^2}\int_H f(\tilde{\boldsymbol{x}},\tilde{h})k(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}), h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h} \nonumber \\
    &\rightarrow \int_{\R^2}\int_H f(\tilde{\boldsymbol{x}},\tilde{h})k_{\mathbb{R}^2}(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}))k_H(h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h} \nonumber \\
    &= \int_{\R^2}\int_H f(\tilde{\boldsymbol{x}},\tilde{h})k_H(h^{-1}\cdot \tilde{h})k_{\mathbb{R}^2}(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}))\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h}\nonumber  \\
    &= \int_{\R^2}\left[\int_Hf(\tilde{\boldsymbol{x}},\tilde{h})k_H (h^{-1}\cdot \tilde{h})\,{\rm d}\tilde{h}\right]k_{\R^2}(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x})) \frac{1}{|h|}\,{\rm d}\tilde{\boldsymbol{x}}. \label{eq:sepgconv_1}

    g \cdot g' &= (\boldsymbol{x} + \boldsymbol{x}')\\
    g^{-1} &= -\boldsymbol{x}.

    \log g=\boldsymbol{x}.

    g \cdot g' &= \boldsymbol{R}_\theta \boldsymbol{R}_{\theta'} \nonumber \\
    &= \boldsymbol{R}_{\theta + \theta'}\\
    g^{-1} &+ \boldsymbol{R}_\theta ^{-1}.

    \log g = \begin{bmatrix} 0 & -\theta\mod2\pi  \\ \theta\mod2\pi & 0   \end{bmatrix}.

    g \cdot g' &= ss'\\
    h^{-1} &= s^{-1}.

    \log g = \ln s.

    g\cdot g' &= (\mathbf{x},\theta) \cdot (\mathbf{x}',\theta') \nonumber \\
    &= (\mathcal{T}_\theta( \mathbf{x}') + \mathbf{x}, \theta + \theta')\\
    g^{-1} &= (-\mathcal{T}_{-\theta} (\mathbf{x}), -\theta).

    g \cdot g' &= (\mathbf{x}, s) \cdot (\mathbf{x}', s') \nonumber \\
    &= (\mathcal{T}_s( \mathbf{x}') + \mathbf{x}, s  s')\\
    g^{-1} &= (-\mathcal{T}_{s^{-1}} (\mathbf{x}), s^{-1}).

    g \cdot g' &= ((\mathbf{x}, \theta), s) \cdot ((\mathbf{x}', \theta'), s') \nonumber\\
    &= ((\mathcal{T}_s(\mathcal{T}_\theta(\mathbf{x}')) + \mathbf{x}, \mathcal{T}_s(\theta') + \theta), s  s')  \nonumber\\
    &= ((\mathcal{T}_{s}(\mathcal{T}_{\mathcal{T}_s(\theta)}(\mathbf{x}')) + \mathbf{x}, \mathcal{T}_s(\theta') + \theta), s  s')  \nonumber\\
    \label{eq:line-commutativity}&= ((\mathcal{T}_s(\mathcal{T}_\theta(\mathbf{x}')) + \mathbf{x}, \theta' + \theta), s  s')\\
    g^{-1} &= (-(\mathcal{T}_{s^{-1}}(\mathcal{T}_{-\theta}(\mathbf{x})), -\theta), s^{-1}).

    \boldsymbol{y}^l = \sin(\omega_0 \boldsymbol{W}^l \boldsymbol{x}^{l-1} + \boldsymbol{b}^l)

    k^{i,j} (g) = k_H^{i,j}(\log_H h)k_{\mathbb{R}^2}^{j}(\boldsymbol{x}) \\

    k^{i,j} (g) = k_{\mathbb{R}^+}^{i,j}(\log_{\mathbb{R}^+} s)k^{j}_{\mathrm{SO(2)}}(\log_{\rm SO(2)}\theta)k_{\mathbb{R}^2}^{j}(\boldsymbol{x}) \\


\textbf{Model sizes}\label{app:modelsizes} In Tab. \ref{tab:parameters}, we report the number of trainable parameters for each model configuration. Throughout our experiments, we kept the number of channels in all our configurations constant, to fairly compare the expressivity of the learned representations in non-separable and separable group convolutions. This has as effect that the number of parameters in the separable implementations is larger than for non-separable implementations, due to our use of separate SIRENs to parameterise kernels over the different subgroups. To ensure that the difference in number of trainable parameters does not influence the comparison between separable and non-separable group convolution layers, we explicitly chose to over-parameterise our SIREN architecture, as shown in an additional ablation in Appx. \ref{app:overparameterisedsirens}. We only change SIREN hidden size when comparing our models to baselines proposed in related works as detailed in Appx. \ref{app:trainingregimes}.

\begin{table}[]
    \centering
    \caption{Number of trainable parameters for different implementations. For all groups and datasets, these numbers are kept constant.}
\label{tab:parameters}
    \begin{tabular}{ccc}
    \toprule
        \sc{Non-separable} &  \sc{Separable} & \sc{H-separable} \\
        \midrule
        742k & 803k & 864k\\
        \bottomrule
        \end{tabular}
        \vspace{-3mm}
\end{table}

\subsection{Experimental details}
\label{app:trainingregimes}
Here, we list training regimes for all experiments. We keep these as consistent as possible, as we are only interested in the effect of separating the group convolution operation, and in isolating the effect of incorporating equivariance into our models. We list information on all datasets, and any dataset-specific model configuration details we use in our experiments here.

\textbf{Optimizer} All architectures are trained with Adam optimisation \citep{kingma2014adam}, and   weight decay. All models are trained on a single Titan V.

\textbf{Rotated MNIST} The 62.000 MNIST images \citep{lecun1998gradient} are split into a training, validation and test set of 10.000, 2.000 and 50.000 images respectively, and randomly rotated to orientations between . Note that in \citep{weiler2018learning, weiler2019general}, the rotated MNIST dataset is augmented during training by transforming images with random continuous rotations. We only use train-time augmentation for the state-of-the-art results obtained in Tab. \ref{tab:rotmnistsota}, as detailed in Appx. \ref{app:rotmnistsota}. All models trained on rotated MNIST, except for the state-of-the-art runs detailed in \ref{app:rotmnistsota}, are trained for 200 epochs with a batch size of 128 and a learning rate of .

\textbf{Scaled MNIST} The 62.000 MNIST images are again split into a training, validation and test set of 10.000, 2.000 and 50.000 images respectively, but now randomly scaled by a factor [0.3, 1] and padded with zeros to retain the original resolution. No data-augmentations of any kind are used in the experiments on scaled MNIST. All models trained on scaled MNIST are trained for 200 epochs with a batch size of 128 and a learning rate of . 

\textbf{Scaled rotated MNIST} The 62.000 MNIST images are again split into a training, validation and test set of 10.000, 2.000 and 50.000 images respectively, but now randomly scaled by a factor [0.3, 1],  padded with zeros to retain the original resolution \textit{and} randomly rotated by orientations between . For the experiments on scaled rotated MNIST we do not use data-augmentation of any kind. All models trained on rotated scaled MNIST are trained for 200 epochs with a batch size of 128 and a learning rate of . 

\textbf{CIFAR10} We evaluate our models on the CIFAR10 dataset, containing 62.000  color images in 10 balanced classes \citep{krizhevsky2009learning}. All models trained on CIFAR10 are trained for 200 epochs with a batch size of 128 and a learning rate of .

\textbf{CIFAR100} We evaluate our models on the CIFAR100 dataset, containing 62.000  color images in 100 balanced classes \citep{krizhevsky2009learning}. All models trained on CIFAR10 are trained for 200 epochs with a batch size of 128 and a learning rate of .

\textbf{Galaxy10} This dataset contains 21785 69x69 color images of galaxies divided into 10 unbalanced classes. For Galaxy10, we limit batch size to 32 images, and scale learning rate accordingly to , as suggested by \citet{goyal2017accurate}.

\textbf{Achieving SOTA on rotated MNIST}\label{app:rotmnistsota}
To compare performance of our separable regular G-CNNs with previous work, we apply our implementation of  and -CNNs to achieve state of the art results on the rotated MNIST dataset, discussed in Sec. \ref{sec:rotmnistexperiments}.

We make some slight adjustment to our architecture and training regime: we reduce the SIREN to a single hidden layer of 64 units. The convolution over  is approximated using random sampling, while the convolution over  is approximated using a discretisation with a truncation of the scale group at . We use a batch size of 64, and a learning rate of  and train for 300 epochs. To fairly assess the impact of equivariance, we train a number of models without continuous rotation augmentations.

To compare with previous SOTA, achieved by \citet{weiler2019general}, we also train models \textit{with} continuous rotation augmentations. \citet{weiler2019general} does not specify performance results for models without train-time augmentations, therefore we also compare with the prior SOTA by \citet{weiler2018learning}, which trains models both with and without augmentations.

We show results in Tab. \ref{tab:rotmnistsota}. With -CNNs we achieve best performance with a resolution of 20 elements over . For the -CNNs, we achieve best performance with a resolution of 12 elements over  and 5 elements over . These results show that without train-time augmentation -CNNs outperform the previous SOTA by \citet{weiler2019general}, which was trained with continuous rotation augmentations. This improvement further increases when we add train-time augmentation by continuous rotations, to a test error of 0.59\%.

\textbf{Achieving competitive performance on CIFAR10} To further compare the viability and performance of our separable approach to group convolutions, following \citet{cohen2016group} we re-implement the All-CNN-C proposed by \citep{springenberg2014striving}, using our separable group convolution layers as drop-in replacement for the regular convolution layers. All experiments described in this section use this exact architecture.

In this experiment we looked to isolate the power of our separable group convolution layers in larger-scale models, which is why we deliberately chose a large but relatively simple architecture. We keep the number of channels constant with the original implementation by \citep{springenberg2014striving}. Because of the way our kernels are parameterised, it is hard to make a direct comparison in model performance in terms of representation expressivity, while keeping the number of trainable parameters equal to the original model. Therefore, we train three configurations, with different SIREN hidden sizes.

To obtain a model with approximately the same number of trainable parameters, for the first configuration we limit the hidden size of our SIRENs to 6 to arrive at 1.33m parameters, where the original model has 1.4m. Note that this limitation likely has considerable implications for the expressivity of the sampled group convolution kernels.

To push our separable group convolution layers further, we also train a model configuration with a SIREN hidden size of 5 units, to arrive at a model with 1.14m parameters (19\% smaller than the original model by \citet{springenberg2014striving}).

Lastly, to see how our layers would fare when unimpeded by limitations in SIREN size, we train a configuration with a SIREN hidden size of 16 units. Note that this model contains 3.22m parameters (representing an 128\% increase in parameter count to the original model).

All models are trained with a resolution of 8 elements randomly sampled over  and a discretisation of the scale group of 3 elements truncated at . We train all of these configurations both with and without data augmentation by padding the original image by at most 6 pixels and randomly cropping to the original resolution, and random horizontal flips. All models are trained for 300 epochs with a learning rate of .

Results are shown in Tab. \ref{tab:cifar10comp}. We can see that for approximately equal parameter counts, our separable group convolution layers equivariant to  outperform the original CNN baseline and the -G-CNN. Without data augmentation, our model with 6 hidden units is outperformed by the -G-CNN which is also equivariant to reflections. With augmentation (containing reflection augmentations) our model outperforms all baseline models we compare to by a significant margin. The smaller configuration with 5 units seems to limit kernel expressivity somewhat, although without data augmentation it still outperforms the translation equivariant original implementation of the All-CNN-C. It seems data augmentation throws the model off in this particular configuration. The larger configuration with 16 hidden units, both with and without data augmentation, significantly outperforms all baselines and other configurations.

\section{Additional experiments}
\subsection{Validating model invariance to transformation groups} \label{app:equivariancetest}
Following \citet{weiler2018learning}, we empirically assess the level of model-invariance to rotation transformations, by training on upright MNIST, and subsequently assessing performance on a test set of MNIST images that have been transformed by a subgroup element .

Each model is trained on the MNIST training set containing 60,000 images, and tested on 10,000 transformed images. Performance is tested for rotations over a range between , in 100 steps. The group convolution is approximated through random sampling. Results are shown in figures \ref{fig:mnistrot-equiv-eval-1} and \ref{fig:mnistrot-equiv-eval} for 1, 2 and 8 group elements for the separable and non-separable variants. We can see that equivariance error is very similar for separable and non-separable variants, and greatly reduces with increased resolution over the group. We do note that for 8 elements, the non-separable group convolution achieves slightly better performance. 

\begin{figure}
\centering
\begin{minipage}{.45\textwidth}
\centering
  \includegraphics[width=\linewidth]{figures/req-sep.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test error vs rotation angle of MNIST test set, when trained on upright MNIST, for separable -CNN.}
  \label{fig:mnistrot-equiv-eval-1}
\end{minipage}
\begin{minipage}{.45\textwidth}
\centering
  \includegraphics[width=\linewidth]{figures/req-nonsep.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test error vs rotation angle of MNIST test set, when trained on upright MNIST, for non-separable -CNN.}
  \label{fig:mnistrot-equiv-eval}
\end{minipage}
\end{figure}

\subsection{Support of the separable convolution kernels over the channel dimensions}
\label{app:gsep-vs-sep} As explained in Sec. \ref{sec:depthwisesep}, when factorising group convolution kernels into a sequence of convolutions, we are presented with a choice of kernel support over the input channel dimension  and output channel dimensions . In this additional ablation study, we investigate a number of possible configurations for the kernel factorisation, detailed in Tab. \ref{tab:factorisations}. In this table, factorisations are listed in terms of whether they separate convolutions over the group, and whether they separate input channel dimensions from spatial and subgroup dimensions. In those cases where neither spatial nor subgroup kernels provide support over the input channels, an additional kernel  is used which only depends on input channels  and output channels . Note that this directly corresponds to the application of depthwise separable convolutions as in \citet{haase2020rethinking}.

\begin{table}[]
    \centering
    \caption{Group convolution kernel factorisation in terms of input channels  and output channels .}
\label{tab:factorisations}
    \begin{tabular}{l|lll|l}
    \toprule
        \sc{Name} & \sc{Group sep.} & \sc{depthwise sep.} & \sc{-depthwise sep.} & \sc{Factorisation} \\
        \midrule
        Nonseparable & \xmark & \xmark & \xmark & \\
        Dseparable & \xmark & \cmark & \cmark & \\
        \midrule
        Separable & \cmark & \xmark & \cmark &  \\
        Gseparable & \cmark & \xmark & \xmark &  \\
        DGseparable & \cmark & \cmark & \cmark & \\
        \bottomrule
        \end{tabular}
        \vspace{-3mm}
\end{table}



For this experiment on CIFAR10, we use the same depthwise group separable CNN as in Sec. \ref{sec:rotmnistexperiments}, with the architecture described in Appx. \ref{app:architectureandparam}. To keep a roughly consistent numbers of parameters over all experiments, we vary the number of channels listed in Appx. \ref{app:architectureandparam}. For non-separable and separable CNNs we use a widening factor of 1 and 0.98 respectively. For Dseparable and DGseparable, we use a widening factor of 2.5, and for Gseparable a widening factor of 0.72. We train the models for 150 epochs, using adam optimisation, with a learning rate of .





Results, shown in Fig. \ref{fig:sepconfig} highlight that additional spatial-depthwise separability obtains highest performance on a fixed parameter budget, which is why we stick with additional depthwise separation of the spatial kernel, and name this approach the separable group convolution.

Interestingly, DSeparable convolutions perform significantly worse than any other configuration for higher numbers of group elements. We hypothesize this drop in performance results from drastically increasing the number of output channels over which the group convolution kernel is sampled, while keeping SIREN architecture constant. Viewing the SIREN as learning a convolution kernel in terms of a linear combination of nonlinear basis functions applied on the input coordinates, this would require a SIREN to express an increasingly complex function as a linear combination of the same number of basis functions. Possibly, the learned basis functions simply do not possess the expressive power to represent increasingly complex kernels required to process a larger range of dilation factors. The same phenomenon possibly shows for the GSeparable kernels, when increasing sampling resolution to more than 8 elements, as performance drops significantly here as well. When separating the group convolution kernels by subgroups, \textit{and} limiting support of one or both of the subgroup kernels over the input channels (Separable, DGSeparable), no such drop in performance is seen.

As increased computational efficiency is a main motivating factor for separable group convolutions, we also kept track of the process time per epoch for each different configuration, and visualised these in Fig. \ref{fig:time-sep}. Here, some notable differences arise. Only Separable and GSeparable implementations are more efficient than Nonseparable implementations for the group resolutions we test in this experiment. We think this may be attributed to the increased computational overhead induced by factorising a single Pytorch \texttt{conv2d} operation used in the Nonseparable implementation (which is heavily optimised) into a sequence of multiple convolution and matrix multiplication operations. We use SIRENs to parameterise the convolution kernels, which entails that the overhead induced by parameterising the kernel is larger for non group-separable implementations. This combination of factors likely contributes to the DSeparable implementation performing worst in these experiments in terms of computational efficiency, followed by the DGSeparable implementation. As we can see, both of these implementations increase sub-linearly dependent on the number of group elements sampled. The Nonseparable implementation increases super-linearly, and hence we conjecture that increasing the number of sampled group elements beyond 8 may comparatively reduce the overhead caused by factorisation to the point that DSeparable and DGSeparable implementations outperform the Nonseparable implementation. Again, the Separable implementation performs best.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\linewidth]{figures/separability.png}
        \caption{}
        \label{fig:sepconfig}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\linewidth]{figures/time-separability.png}
        \caption{}
        \label{fig:time-sep}
    \end{subfigure}
    \caption{(a) Test accuracy (\%) versus  resolution on CIFAR10 for different separability configurations. (b) Process time per epoch for different separability configurations.}
    \label{fig:separability}
\end{figure}



\subsection{How does SIREN architecture influence performance?} \label{app:overparameterisedsirens}
As explained, we keep the architecture of our SIRENs constant over all experiments. Because we use multiple SIRENs in separable architectures, and we want to isolate the effect of separability of the group convolution operation, we deliberately choose to over-parameterise the SIREN architectures, so as not to advantage the separable implementations by their increased parameter numbers. To this end, we look for the \textit{largest} size of hidden layer that does not negatively impact performance in the non-separable implementation.

We assess the impact of the size of the hidden layers in our SIREN on performance on rotated MNIST for separable and non-separable -CNN implementations. We sample at a resolution over  of 8 elements. Interestingly, results in Fig. \ref{fig:siren-arch} show that even with very small SIRENs - a hidden size of 4 or 8 - performance of the non-separable implementations is remarkable. Separable implementations clearly underperform for small SIREN sizes, leading us to hypothesize that the restriction in kernel expressivity due separability paired with under-parameterisation is detrimental to performance in these configurations.

For non-separable implementations, performance barely increases from 16 hidden units on, and starts to decrease after a size of 64 hidden units. For this reason, we choose a size of 64 hidden units in all SIRENs in our experiments.

We repeat the same experiment for -CNNs on CIFAR10, and visualise the results in Fig. \ref{fig:siren-arch-c10}. Here, performance for very small SIREN configurations is less impressive, but we still see a similar saturation around a SIREN size of 64 hidden units.

\begin{figure}
\centering
  \includegraphics[width=\linewidth]{figures/siren-arch.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test accuracy of -CNNs on rotated MNIST for different sizes hidden layers in the SIREN parameterisation.}
  \label{fig:siren-arch}
\end{figure}
\begin{figure}
\centering
  \includegraphics[width=\linewidth]{figures/siren-arch-C10.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test accuracy of -CNNs on CIFAR10 for different sizes hidden layers in the SIREN parameterisation.}
  \label{fig:siren-arch-c10}
\end{figure}


\subsection{Comparing activation functions for continuous convolution kernels} \label{app:activationfunctions}
To assess the impact of our use of SIRENs on the performance of our separable G-CNNs, we perform an additional ablation on rotated MNIST in which we compare with other activation functions. We train separable -CNNs with a resolution 8 group elements over , and only vary activation functions in the MLP parameterising the convolution kernels. The convolution on  is approximated through random sampling.

We look at other activation functions used in parameterising convolution kernels; ReLU \citep{wu2019pointconv}, LeakyReLU, and SiLU \citep{finzi2020generalizing}, see Tab. \ref{tab:activations}. These results conclusively show the power of sine as activation function, one of the reasons we chose to use SIRENs in our work.

\begin{table}[]
    \centering
    \caption{Test accuracy (\%) of -CNNs on rotated MNIST for different activation functions used in parameterisation of the convolution kernel.}
\label{tab:activations}
    \begin{small}
    \begin{tabular}{ccc}
    \toprule
        \sc{Activation} & \sc{Separable} & \sc{Test accuracy} \\
        \midrule
        \multirow{2}{*}{} & \xmark & 0.9855  \\
       &\cmark& \textbf{0.9906 } \\
        \cmidrule{1-3}
        \multirow{2}{*}{} & \xmark & 0.9721  \\
       &\cmark& 0.9743 \\
       \cmidrule{1-3}
        \multirow{2}{*}{} & \xmark &  0.9722    \\
       &\cmark& 0.9788  \\
       \cmidrule{1-3}
        \multirow{2}{*}{} & \xmark & 0.9651  \\
       &\cmark& 0.9595 \\
        \bottomrule
        \end{tabular}
        \end{small}
\end{table}

\subsection{Random sampling over non-compact groups}
As explained in Sec. \ref{sec:sep-ckgconvs}, for non-compact groups we approximate the group convolution through a discretisation of the transformation group of interest. Motivation for this decision is the fact that in non-compact groups, we have to deal with boundary effects of truncating the group, introducing equivariance error into the convolution operation. For dilation, further motivation for approximation through discretisation is the loss of information that occurs in for example natural images, as a result of downscaling a signal on a fixed resolution grid. Random sampling over the dilation group would have as effect that the representation built by a group convolution layer contains different spatial resolutions of information at every sampling step. This results in subsequent layers receiving noisy information at each iteration, which would impede model performance, with training being highly unstable for low resolutions over .

To empirically reinforce this hypothesis, we performed the same experiment with our -CNN on MNIST-scale described in Sec.  \ref{sec:experiments} with random sampling over the dilation group. Results, shown in Fig. \ref{fig:samplingscaledmnist}, highlight that indeed, random sampling over the dilation group leads to significantly worse performance. We also noticed great instability during the training process, seen in the variance of the results achieved with random sampling.

\begin{figure}
\centering
  \includegraphics[width=\linewidth]{figures/mnist-scale-samp.png}
  \caption{Test error (\%) of -CNNs on scaled MNIST for random sampling versus discretisation of the scale group.}
  \label{fig:samplingscaledmnist}
\end{figure}

\subsection{Do separable group convolutions reduce redundancy in G-CNNs?}
\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{figures/redundancy-sep.png}
\end{center}
\vspace{-2mm}
\caption{A set of histograms showing variance along the subgroup axis in separable group convolutions. In contrast to Fig. \ref{fig:redundancy}, here we show the variability of the convolution kernel along the subgroup axis. This may be seen as an inverse measure of redundancy. Here we can see that contrary to in non-separable group convolution kernels, variability along the group axis increases over the training process, indicating that redundancy decreases. Separable group convolutions thus solve the redundancy issues in group convolution kernels.
\vspace{-2mm}}
\label{fig:redundancy-sep}
\end{figure}
As explained, convolving with separable group convolution kernels is analogous to a group convolution with a kernel which shares a reweighting of a single spatial kernel along the group axis. This approach was motivated by redundancy observations in regular group convolution kernels along the group axis, seen in Fig. \ref{fig:redundancy}.

To assess whether the separable group convolution solves these redundancy issues in G-CNNs, we may want to perform a similar test of the variability of our introduced separable implementation along the group axis. Note that, if we were to reconstruct the full group convolution kernel  from  and  and apply the same PCA variance test as used to obtain Fig. \ref{fig:redundancy}, we would find that each group convolution could be fully explained by the first principal component (corresponding to the reshared spatial kernel). Instead, we measure the variability of our separable kernels by assessing the variance of , which lies along this axis. Results are shown in Fig. \ref{fig:redundancy-sep} for the separable -CNN trained on Galaxy10 dataset with a resolution of 8 elements on . Here, we observe the variance increasing in all layers over the training process. This indicates that indeed, separable group convolutions reduce parameter redundancy in G-CNNs.




\end{document}
