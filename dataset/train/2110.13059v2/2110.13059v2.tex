

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{gensymb}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\usepackage{microtype}      \usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{mathtools}

\usepackage{graphicx}

\usepackage{caption}

\usepackage{enumitem}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{soul}
\setul{0.3ex}{0.2ex}
\setulcolor{blue}

\usepackage{color}
\definecolor{mydarkblue}{rgb}{0.33,1,1}
\hypersetup{colorlinks,citecolor={blue},urlcolor={blue}, linkcolor={red}}


\newcommand{\R}{\mathbb{R}}

\icmltitlerunning{Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups}

\begin{document}

\twocolumn[
\icmltitle{Exploiting Redundancy: Separable Group \\Convolutional Networks on Lie Groups}







\begin{icmlauthorlist}
\icmlauthor{David M. Knigge}{uva}
\icmlauthor{David W. Romero}{vu}
\icmlauthor{Erik J. Bekkers}{uva}
\end{icmlauthorlist}

\icmlaffiliation{uva}{Universiteit van Amsterdam, Amsterdam, The Netherlands}
\icmlaffiliation{vu}{Vrij Universiteit Amsterdam, Amsterdam, The Netherlands}

\icmlcorrespondingauthor{David Mattanja Knigge}{d.m.knigge@uva.nl}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions. In order to obtain equivariance to arbitrary affine Lie groups we provide a continuous parameterisation of separable convolution kernels. We evaluate our approach across several vision datasets, and show that our weight sharing leads to improved performance and computational efficiency. In many settings, separable G-CNNs outperform their non-separable counterpart, while only using a fraction of their training time. In addition, thanks to the increase in computational efficiency, we are able to implement G-CNNs equivariant to the $\mathrm{Sim(2)}$ group; the group of dilations, rotations and translations of the plane.  $\mathrm{Sim(2)}$-equivariance further improves performance on all tasks considered, and achieves state-of-the-art performance on rotated MNIST. Code is available on Github \footnote{\url{https://github.com/david-knigge/separable-group-convolutional-networks}}.
\end{abstract}

\section{Introduction}
\citet{minsky1988perceptrons} suggest that the power of the perceptron comes from its ability to \textit{learn to discard} irrelevant information. In other words; information that does not bear significance to the current task does not influence representations built by the network. According to \citet{minsky1988perceptrons}, this leads to a definition of perceptrons in terms of the \textit{symmetry groups} their learned representations are invariant to. Progress in geometric deep learning has shown the power of pro-actively equipping models with such geometric structure as inductive bias, reducing model complexity and improving generalisation and performance \citep{bronstein2017geometric}. An early example of such geometric inductive bias at work can be seen in the convolutional layer in a CNN \citep{lecun1998gradient}. CNNs have been instrumental in conquering computer vision tasks, and much of their success has been attributed to their use of the convolution operator, which commutes with the action of the translation group. This property, known as \textit{equivariance} to translation, comes about as a result of the application of the same convolution kernel throughout an input signal, enabling the CNN to learn to detect the same features at any location\break in the input signal, directly exploiting translational symmetries that naturally occur in many tasks.

Although invariance to object-identity preserving transformations has long been recognised as a desirable model characteristic in machine learning literature \citep{kondor2008group, cohen2013learning, sifre2014rigid}, only recently \citet{cohen2016group} introduced the Group Equivariant CNN (G-CNN) as a natural extension of the CNN \citep{lecun1998gradient}, generalising its equivariance properties to group actions beyond translation. The layers of a G-CNN are explicitly designed to be equivariant to such transformations, hence the model is no longer burdened with \textit{learning} invariance to transformations that leave object identity intact. It has since been shown that equivariant deep learning approaches may serve as a solution in fields that as of yet remain inaccessible to machine learning due to scarce availability of labelled data, or when compact model design due to limited computational power is required \citep{winkels20183d, linmans2018sample,bekkers2019b}.

\textbf{Complexity and redundancy issues impeding regular group convolutions}
\begin{figure*}
\centering
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/group-conv-kernel.png}}
        \caption{$k:\mathbb{R}^2 \rtimes H \rightarrow \mathbb{R}$\vspace{-3mm}}
        \label{fig:nonsep-gconv-kernel}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/subgroup-conv-kernel.png}}
        \caption{$k_H:H\rightarrow \mathbb{R}$\vspace{-3mm}}
        \label{fig:subgroup-gconv-kernel}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/r2-conv-kernel.png}}
        \caption{$k_{\mathbb{R}^2}:\mathbb{R}^2\rightarrow \mathbb{R}$\vspace{-3mm}}
        \label{fig:r2-gconv-kernel}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \raisebox{-0.5\height}{\includegraphics[scale=0.12]{figures/combined-separable-kernel.png}}
        \caption{$k_H \cdot k_{\mathbb{R}^2}$\vspace{-3mm}}
        \label{fig:combined-separable-gconv-kernel}
    \end{subfigure}
\caption{In group convolutions on affine Lie groups, a feature map $f$ defined over the group $G=\mathbb{R}^n \rtimes H$ is convolved with a filter $k:\mathbb{R}^n \rtimes H \rightarrow \mathbb{R}$, shown in Fig. \ref{fig:nonsep-gconv-kernel}. We propose separating this convolution into two sequential operations: a convolution over the subgroup $H$ with a kernel $k_H: H \rightarrow \mathbb{R}$, followed by a convolution over the spatial dimensions with a kernel $k_{\R^n}: \mathbb{R}^n \rightarrow \mathbb{R}$, shown in Figs. \ref{fig:subgroup-gconv-kernel}, \ref{fig:r2-gconv-kernel} respectively. Importantly, this greatly reduces computational complexity while retaining equivariance properties, allowing for application of equivariant deep learning models to larger groups $G$. This factorisation intuitively corresponds to composing $k$ by sharing a reweighting of $k_{\mathbb{R}^2}$ along $H$ with coefficients given by $k_H$, shown in Fig. \ref{fig:combined-separable-gconv-kernel}.
\vspace{-2mm}}
\label{fig:sepgconvs}
\end{figure*}
A growing body of work shows applications of G-CNNs consistently and decisively outperforming classical CNNs \citep{worrall2017harmonic, weiler20183d, bekkers2018roto, esteves2018learning, bekkers2019b, worrall2019deep, sosnovik2021scale}. However, a practical challenge impeding application to larger groups is the computational complexity of regular group convolutions, which scales exponentially with the dimensionality of the group.
Furthermore, \citet{lengyel2021exploiting} show that group convolution filters in the original formulation of the G-CNN by \citet{cohen2016group} exhibit considerable redundancies along the group axis for the $p4m$ and $\mathbb{Z}^2$ groups. Similar observations motivated depthwise separable convolutions \citep{chollet2017xception}, which not only increased parameter efficiency but also model performance; observed correlations between weights are explicitly enforced with further parameter sharing through the use of kernels separable along spatial and channel dimensions. We address the observations of redundancy along with the scalability issues of regular G-CNNs in their current form. Our paper contains the following contributions:
\begin{itemize}[topsep=0pt,itemsep=-1pt, leftmargin=0.5cm]
    \item We introduce separable group convolutions for affine Lie groups $\R^n \rtimes H$, sharing the kernels for translation elements $x \in \R^n$ along subgroup elements $h\in H$. See Fig. \ref{fig:sepgconvs} for an overview.
    \item We propose the use of a SIREN \citep{sitzmann2020implicit} as kernel parameterisation in the Lie algebra - imposing a fixed number of parameters per convolution kernel, regardless of the resolution at which this kernel is sampled, and ensuring smoothness over the Lie group.
    \item  Separable group convolutions allow us to build $\mathrm{Sim(2)}$-CNNs, which we thoroughly experiment with. We show equivariance to $\mathrm{Sim(2)}$ increases accuracy over a range of vision benchmarks.
    \item To achieve equivariance to continuous affine Lie groups, we propose a random sampling method over subgroups $H$ for approximating the group convolution operation.
\end{itemize}
First, we position this work within the area of equivariant deep learning by giving an overview of related works, and explaining which current issues we are addressing with this work.
We derive separable group convolutions, and show how they may be applied to continuous groups. Lastly, we apply these ideas by experimenting with implementations for roto-translations in 2D ($\mathrm{SE(2)}$), dilation and translation in 2D ($\R^2 \rtimes \R^+$) and dilation, rotation and translation in 2D ($\mathrm{Sim(2)}$). 

\section{Related Work}
\textbf{Group equivariant convolutional neural networks} Broadly speaking, research on G-CNNs can be divided into two approaches. First, \textit{Regular} G-CNNs use the left-regular representation of the group of interest to learn representations of scalar functions over the group manifold, or a quotient space of the group. The left-regular representation acts on the convolution kernels, yielding an orbit of the kernel under the group action. Convolving the input using these transformed filters, a feature map defined over the group is obtained at each layer. This approach most naturally extends the conventional CNN, where convolution kernels are transformed under elements of the translation group. Regular G-CNNs have been implemented for discrete groups \citep{cohen2016group, winkels20183d, worrall2018cubenet}, compact continuous groups \citep{marcos2017rotation, bekkers2018roto} and arbitrary non-compact continuous Lie groups \citep{bekkers2019b, finzi2020generalizing, romero2020wavelet}. However, practical implementations for continuous groups often require some form of discretisation of the group, possibly introducing discretisation artefacts, and requiring a choice of \textit{resolution} over the group. For the second class, \textit{steerable} G-CNNs, representation theory is used to compute a basis of equivariant functions for a given group, which are subsequently used to parameterise convolution kernels \citep{cohen2016steerable, weiler20183d, weiler2018learning, sosnovik2019scale, sosnovik2021disco}. Although steerable G-CNNs decouple the cardinality of the group from\break the dimensionality of the feature maps, this approach is only compatible with compact groups.

The current paper may, in approach, be compared to \citet{bekkers2019b} and \citet{ finzi2020generalizing}, who define convolution kernels on the Lie algebra of continuous groups to enable convolutions on their manifold. Similarly, we make use of the Lie algebra and exponential map to obtain convolution kernels on the group, but separate the kernels by subgroups.

\citet{bekkers2019b} defines a set of basis vectors in the Lie algebra, which, when combined with the exponential map, allow for the identification of group elements by a vector in $\mathbb{R}^n$. Subsequently, a set of B-splines is defined on the algebra, which form a basis to expand convolution kernels in. A linear combination of these bases creates a locally continuous function on the Lie algebra defining a convolution kernel and its behaviour under transformations of the group. Although this method allows for direct control over kernel smoothness, the learned convolution filters are limited in their expressivity by their basis functions. \citet{finzi2020generalizing} instead use an MLP to learn convolution kernels on the Lie algebra, which in addition allows them to handle point cloud data. The MLP is constructed to learn kernel values at (arbitrary) relative offsets in the Lie algebra. In contrast, we propose to use SIRENs \citep{sitzmann2020implicit} to parameterise convolution kernels, as they have been shown to outperform other forms of MLPs in parameterising convolution kernels \citep{romero2021ckconv}, and offer more explicit control over kernel smoothness; a desirable property for addressing \break discretisation artefacts that occur when modelling features on continuous groups (see Appx. \ref{app:kernelsmoothnesscontinuous}).

\textbf{Separable filters in machine learning}
In image processing, spatially separable filters have long been known to increase parameter- and computational efficiency, and learning such constrained filters may even increase model performance \citep{rigamonti2013learning}. In \citet{sifre2014rigid}, authors investigate SE(2)-invariant feature learning through scattering convolutions, and propose separating the group convolution operation for affine groups into a cascade of two filterings, the first along the spatial dimensions $\R^n$, and the second along subgroup dimension $H$. From this, authors derive a separable approach to the convolution operation with learnable filters as used in CNNs. This formulation has since been named the \textit{depthwise-separable} convolution \citep{chollet2017xception}, a special case of the Network-In-Network principle \citep{lin2013network} which forms the basis for the success of the Inception architectures \citep{szegedy2015going}. In depthwise separable convolutions, each input channel first gets convolved using a (set of) kernel(s) with limited spatial support. Afterwards, a 1x1 convolution is used to project the feature set detected in the input channels to the output space. \citet{chollet2017xception} speculates that the Inception architectures are successful due to the explicit separation of spatial and channel mapping functions, whereas in conventional CNNs, kernels are tasked with simultaneously mapping inter-channel and spatial correlations.

\citet{haase2020rethinking} argue that the original formulation of the depthwise-separable convolution reinforces inter-kernel correlations, but does not in fact leverage intra-kernel correlations. They propose an inverse ordering of the operations given in depthwise-separable convolutions, sharing the same spatial kernel along the input channels, and show convincing results. Extending this investigation of learned convolution filters to the original G-CNN \citep{cohen2016group}, \citet{lengyel2021exploiting} remark on the high degree of correlation found among filters along the rotation axis, and propose to share the same spatial kernel for every rotation feature map. We aim to generalise this approach, proposing separable convolutions on arbitrary affine Lie groups.
\section{Background}
In the following section we give the theoretical background for implementing group convolutions for arbitrary affine Lie groups. We assume familiarity with the basics of group theory and provide the relevant concepts in Appx. \ref{app:grouptheory} and Appx. \ref{app:cnnsgrouptheory}. For simplicity of notation, we initially assume that our input signal/feature map $f$ has a single channel.

\textbf{Lifting convolutions} To preserve information on the \text{pose} of features in the input, an equivariant convolution operation is achieved by \textit{lifting} a function from the input space to (a homogeneous space of) the group. As we are interested in real signals, specifically image data living on $\mathbb{R}^2$, we assume the Lie group of interest $H$ is taken in semidirect product with the domain of our data; $G {=} \mathbb{R}^2 \rtimes H$. In group convolutions, a given kernel is left-acted by all transformations in $G$, thereby generating a signal on the higher dimensional space $G$ instead of $\mathbb{R}^2$. Hence, the output feature maps disentangle poses through a domain expansion, e.g. positions plus rotations or scales. For a given group element $h \in H$, kernel $k$, and location $\vec{x}$ in the input domain $\mathbb{R}^2$, the lifting convolution is given by:
\begin{equation}
\label{eq:liftingconv}
    (f *_{\text{lifting}} k) (g) = 
    \int_{\mathbb{R}^2} f(\tilde{\boldsymbol{x}})k_h(\tilde{\boldsymbol{x}} - \boldsymbol{x}) \,{\rm d}\tilde{\boldsymbol{x}}.
\end{equation}
where $g{=}(\boldsymbol{x},h)$ and $k_h{=}\frac{1}{| h|}\mathcal{L}_{h}[k]$ is the kernel $k:\mathbb{R}^2 \rightarrow \mathbb{R}$ transformed via the action of group element $h$ via $\mathcal{L}_h [k](\boldsymbol{x})\coloneqq k(h^{-1} \boldsymbol{x})$, and with $|h|$ the determinant of the matrix representation of $h$ that acts on $\mathbb{R}^2$. The output of lifting convolutions yields a $G$-feature map with the original two spatial input dimensions ($\mathbb{R}^2$), and an additional group dimension ($H$). See Fig. \ref{fig:liftingconv}.

\textbf{Group convolutions}
Now that the data is lifted to the domain of the group, we continue with group convolutions in subsequent layers. Given a kernel $k$ (now a function on $G$), Haar measures ${\rm d}\tilde{g}$ and ${\rm d}\tilde{h}$ on the group $G$ and sub-group $H$ respectively, group convolutions are given by:
\begin{align}
\label{eq:groupconv}
    &(f *_{\mathrm{group}} k) (g) \\ &=\int\limits_G f(\tilde{g})k(g^{-1} \cdot \tilde{g})\,{\rm d}\tilde{g} 
    = \int\limits_G f(\tilde{g})\mathcal{L}_g k( \tilde{g})\,{\rm d}\tilde{g} \nonumber\\
    &=\iint\limits_{\R^2 H} f(\tilde{\boldsymbol{x}}, \tilde{h})\mathcal{L}_{x}\mathcal{L}_{h}k(\tilde{\boldsymbol{x}}, \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h}\nonumber\\
    &=\iint\limits_{\R^2 H} f(\tilde{\boldsymbol{x}},\tilde{h})k({h^{-1}}(\tilde{\boldsymbol{x}}-\boldsymbol{x}), h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h}.
\end{align}
Evaluating this convolution for every group element $g\in G$, we again obtain a function defined on $G$. As we know $G{=}\mathbb{R}^2 \rtimes H$, we can factor this operation into a transformation of a kernel $k$ by a group element $h \in H$, $k_h {=} \mathcal{L}_h(k)$, followed by a convolution at every spatial location in $f$. See Fig. \ref{fig:groupconv}.

\textbf{Achieving invariance} Using lifting and group convolution operations, we can construct convolutional layers that co-vary with the action of the group and explicitly preserve pose information throughout the representations of the network. In most cases, we ultimately want a representation that is invariant to transformations of the input in order to achieve invariance to these identity-preserving transformations. This is achieved by aggregating the information at all group elements in a feature map with an operation invariant to the group action, e.g., max-, mean- or sum-projection. In practice, this is done after the last group convolution, and is followed by one or more fully connected layers.

\section{Separable Group Convolutions on Lie Groups} \label{sec:sep-ckgconvs}
\textbf{Redundancies in group convolution filters} Similar to \citet{haase2020rethinking,lengyel2021exploiting} we investigate parameter efficiency of learned convolution kernels to motivate separable filters. We train an $\mathrm{SE(2)}$-equivariant CNN on the Galaxy10 dataset (see Sec. \ref{sec:experiments} for experimental details) and analyse the resulting group convolution kernels. We apply PCA by treating the values of the group convolution kernel for each subgroup element $h \in H$ as distinct spatial kernels with $k\times k$ features. The ratio of variance explained by the first principle component gives an indication of the variability of the group convolution kernel along the subgroup axis. If the ratio of explained variance is high, the distinct spatial convolution kernels along the subgroup axis are well-characterised by a single shared kernel. In Fig. \ref{fig:redundancy}, results are shown for the group convolution layers in our $\mathrm{SE(2)}$-CNN before and after training. We find that during the training process, redundancy along the subgroup axis increases considerably. This motivates sharing a single spatial kernel along the subgroup elements, which can be achieved by separating the group convolution operation \break into a convolution over the subgroup $H$, followed by a convolution over the spatial domain $\R^2$.
\begin{figure}
\begin{center}
\includegraphics[width=0.48\textwidth]{figures/Galaxy10.png}
\end{center}
\caption{A set of histograms showing redundancy in learned group convolution kernels. On the x-axis is the ratio of variance explained by the first principal component when applying PCA on the set of spatial kernels along the group axis of a group convolution kernel. Y-axis shows the proportion of kernels with this explained variance ratio, where all bins sum to 1. The number of spatial kernels is listed in the title of each subfigure. Throughout the training process, redundancy in the group convolution kernels increases. Left to right: subsequent layers in the network. \vspace{-2mm}}
\label{fig:redundancy}
\end{figure}

\textbf{Separable group convolutions} \label{sec:seperablegroupconvs} Let us assume that the convolution kernel $k:G\rightarrow \R$ in Eq.~\ref{eq:groupconv} is separable. That is, $k(g) {=} k_{\mathbb{R}^2}(\boldsymbol{x})  k_H(h)$.  In this case, we can derive the factorised separable group convolution as (see Appx. \ref{app:derivingsepgconv} for the full derivation):
\begin{align}
    &(f *_{group} k)(g) \\&= \int\limits_{\R^2}\int\limits_H f(\tilde{\boldsymbol{x}},\tilde{h})k({h^{-1}}(\tilde{\boldsymbol{x}}-\boldsymbol{x}), h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h} \nonumber\\
    &= \int\limits_{\R^2} (f * k_H)(\boldsymbol{\tilde{x}}, h)k_{\R^2}({h^{-1}}(\tilde{\boldsymbol{x}}-\boldsymbol{x})) \frac{1}{|h|}\,{\rm d}\tilde{\boldsymbol{x}}. \label{eq:finalsepgconv} \\
    &\text{where } (f * k_H)(\boldsymbol{\tilde{x}}, h) = \int\limits_Hf(\tilde{\boldsymbol{x}},\tilde{h})k_H (h^{-1}\cdot \tilde{h})\,{\rm d}\tilde{h} \nonumber
\end{align}

Here $k_H$ is a convolution kernel over the group $H$, and $k_{\mathbb{R}^2}$ is a convolution kernel over the spatial domain $\mathbb{R}^2$. We set $k {=} k_H \cdot k_{\mathbb{R}^2}$, with $k_H$ constant along $\R^2$ and $k_{\mathbb{R}^2}$ constant along $H$. This can be thought of as parameterising a convolution kernel $k$ over the group $G$ by sharing same spatial kernel $k_{\R^2}$ weighed by a value $k_H(h)$ at every input group element $h \in H$, see Fig. \ref{fig:separablekernel}. Importantly, this factorisation greatly increases the efficiency of the group convolution operation. Since $(f * k_H)$ in Eq. \ref{eq:finalsepgconv} does not depend on $\boldsymbol{x}$, we can precompute it. As a result, convolving over a single channel of a feature map $f$ of size $|H| {\times} x {\times} y$ goes from $O(|H|^2 {\times} x {\times} y {\times} k^2)$ to $O(|H| {\times} x {\times} y {\times} (|H| + k^2))$. See Fig. \ref{fig:sepgroupconv} for a visual intuition of the separable group convolution.

\textbf{Defining convolution kernels on Lie algebras} In order to perform lifting- and group convolutions (Eqs.~\ref{eq:liftingconv},~\ref{eq:groupconv}), we need to evaluate our convolution kernels $k$ at relative offsets $g'$ on the group. As in \citet{bekkers2017template,weiler2018learning,bekkers2019b, finzi2020generalizing}, we express our group convolution kernel $k$ in analytical form, as a function of relative group elements yielding kernel values.\break Motivated by findings in \citet{romero2021ckconv}, we use a Sinusoidal Representation Network (SIREN) \citep{sitzmann2020implicit} as kernel parameterisation. SIRENs lead to great performance improvements over MLPs with $\mathrm{ReLU}, \mathrm{LeakyReLU}$ and $\mathrm{Swish}$ when parameterising convolution kernels (we replicate this comparison for G-CNNs in Appx. \ref{app:activationfunctions}). Furthermore, the SIREN offers explicit control of kernel smoothness through a frequency multiplier parameter $\omega_0$: an important property discussed in Appx. \ref{app:kernelsmoothnesscontinuous}. Since the affine Lie group can be non-euclidean, and neural networks are functions generally defined over euclidean spaces, we resort to defining the kernel function on the Lie algebra of our group of interest \citep{bekkers2019b,finzi2020generalizing}. The kernel function $k$ maps points in the Lie algebra (which may be associated with the relative offsets on the group we are convolving over by the exponential map) to kernel values, $k:\mathfrak{g} \rightarrow \R$. For a given element $g$, we have: $k(g){=}\mathrm{SIREN}(\log g)$, see Fig. \ref{fig:kernelongroup}. The separable kernels follow the same principle, separated by subgroups, see Appx.~\ref{app:architecture}.

\textbf{Approximating equivariance for compact continuous and non-compact continuous groups} In the case of small discrete subgroups $H$, such as the group $C_4$ of rotations by 90 degrees, we are able to perform lifting- and group convolution operations exactly equivariant to the action of the group, as the integral given in Eq. \ref{eq:groupconv} is tractable. We consider image data in particular, which is defined over a discrete grid $\mathbb{Z}^2$, a subgroup of $\R^2$, making both integrals over the group discrete sums:
\begin{align}
    &(f *_{\mathrm{group}} k)(g) \nonumber \\ &= \sum_{\tilde{\boldsymbol{x}} \in \mathbb{Z}^2}\sum_{\tilde{h} \in H} f(\tilde{\boldsymbol{x}}, \tilde{h})k(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}), h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \Delta\tilde{\boldsymbol{x}}\Delta\tilde{h},
    \label{eq:discretegconv}
\end{align}
with $\Delta \tilde{\boldsymbol{x}}$ and $\Delta \tilde{{h}}$ denoting the volume elements corresponding to the grid points. In the case of continuous groups, it is possible to either make a discretisation of the subgroup $H$, or to approximate the group convolution by means of random sampling. To obtain a volumetrically uniform sampling grid over the group, we sample a set of $n$ equidistant points in the Lie algebra as in \citet{bekkers2019b}, and map those to the group using the exponential map to obtain a grid $\mathcal{H}:=[h_e, ..., h_n]$, see Fig. \ref{fig:gridongroup}. For noncompact $H$ such as the dilation group $\R^+$, we localize the support in the Lie algebra. In the case of compact continuous groups such as $\mathrm{SO(2)}$ we approximate the integral by convolving over a uniformly spaced grid $\mathcal{H}$ which is perturbed by left-multiplication with a uniformly randomly sampled group element $h_{\epsilon} \sim {\rm d}\tilde{h}$, i.e.,  $h_{\epsilon} \mapsto \mathcal{H}_{\epsilon}:=h_{\epsilon} \mathcal{H}$. By uniform sampling of $H$ we obtain left-invariant $\Delta \tilde{h}$ which only scales the overall convolution result, allowing us to omit it from \ref{eq:discretegconv}. We further let $\Delta \tilde{\boldsymbol{x}}=1$ and obtain the discrete separable group convolution for continuous groups:
\begin{align}
&(f *_{group} k)(g) \\ &\approx \sum_{\tilde{\boldsymbol{x}} \in \mathbb{Z}^2}\sum_{\tilde{h} \in \mathcal{H}_{\epsilon}} f(\tilde{\boldsymbol{x}}, \tilde{h})k({h^{-1}}(\tilde{\boldsymbol{x}}-\boldsymbol{x}), h^{-1}\cdot \tilde{h})\dfrac{1}{|h|}  \nonumber\\[-1 \jot]
    &= \sum_{\tilde{\boldsymbol{x}} \in \mathbb{Z}^2} (f * k_H)(\boldsymbol{\tilde{x}}, h) k_{\R^2}({h^{-1}}(\tilde{\boldsymbol{x}}-\boldsymbol{x}))\frac{1}{|h|} \label{eq:discretefinalsepgconv} \\
    &\text{where } (f * k_H)(\boldsymbol{\tilde{x}}, h)=\sum_{\tilde{h} \in \mathcal{H}_{\epsilon}} f(\tilde{\boldsymbol{x}}, \tilde{h})k_H (h^{-1}\cdot \tilde{h}) \nonumber. 
\end{align}

During training and inference, randomly sampling over the rotation group yields an unbiased estimation \citep{wu2019pointconv}, making the network approximately equivariant to the continuous group $\mathrm{SO(2)}$. Discretisation makes the network exactly equivariant only to a discretised subgroup of $\mathrm{SO(2)}$, biasing the network to be equivariant to a fixed subset of transformations. In our experiments we show that randomly sampling outperforms discretisation when modelling equivariance to $\mathrm{SO(2)}$.



\textbf{Channel support of separable group convolution kernels} \label{sec:depthwisesep}In G-CNNs (as in CNNs), a feature map $f^l$ at layer $l$ generally has multiple channels  $i$; $f:G \rightarrow \R^{i}$, and is convolved with a set of $j$ kernels to produce an output $f^{l+1}:G \rightarrow \R^{j}$. Factorising the group convolution operation over subgroup and spatial dimensions presents us with a choice on how we define support of the respective convolution kernels over the input channels. We investigate a range of possibilities in an ablation study detailed in Appx.~\ref{app:gsep-vs-sep}, and show that on a fixed parameter budget, reducing the support of $k_{\mathbb{R}^n}$ to the output channels by sharing a reweighing of it along the input channels in an inverse depthwise separable manner, similar to \citet{haase2020rethinking}, consistently outperforms other separability configurations, both in terms of speed and performance. As such, we will keep to the additional spatial-depthwise separation and refer to this implementation as the separable group convolution. The separable group convolution kernel is factorised as: $k^{ij}(\boldsymbol{x},h)=k_H^{ij}(h)\cdot k^j_{\mathbb{R}^n}(\boldsymbol{x})$.

\textbf{Expressivity of separable group convolutions} Separable convolution kernels are strictly less expressive than their non-separable counterpart, but to what extent would this limit the expressivity of their learned representations? In separable group convolutions, we are sharing a weighted version of a single spatial kernel along the input group axis. In contrast, we could view non-separable group convolutions as having the ability to learn distinct spatial configurations of features along the group axis. 

For a visual example, see Appx.~\ref{app:featurescannotrecognise}. Although this reduction in expressivity could in theory prove limiting in the application of separable G-CNNs, in the next section we experimentally show that separable group convolutions in fact often outperform their non-separable counterpart.

\section{Experiments}\label{sec:experiments}
We empirically motivate the use of seperable group convolutions by studying the performance of three G-CNNs that incorporate equivariance to three distinct groups. As our goal is to isolate the effect of separating the group convolution, we use the same shallow ResNet architecture throughout all experiments, only varying the sampling resolution over the group, see Appx.~\ref{app:architectureandparam} for details. After the last convolution block, we apply max-projection over the remaining group dimensions to achieve an invariant representation. We experiment with three different groups acting on $\mathbb{R}^{2}$: the roto-translation group ($\mathrm{SE(2)}$), the translation-dilation group ($\mathrm{\R^2\rtimes \R^+}$) and the group of rotations, dilations and translations ($\mathrm{Sim(2)}$). For $\mathrm{SE(2)}$, we first evaluate the influence of random sampling versus discretisation of $\mathrm{SO(2)}$. Next, we assess the difference in performance of separable and non-separable group convolutions on transformed MNIST variants; MNIST-rot, MNIST-scale for $\mathrm{SE(2)}$ and $\mathrm{\R^2 \rtimes \R^+}$ respectively. Due to the reduction in computational complexity separable group convolutions bring, we are able to model equivariance to higher-dimensional groups: we implement two $\mathrm{Sim(2)}$-CNNs and evaluate them on MNIST-rot-scale. Lastly, to investigate the advantages equivariance brings in more complex problem settings, we experiment with all setups on three vision benchmark datasets; CIFAR10, CIFAR100 \citep{krizhevsky2009learning} and Galaxy10 \citep{leung2019deep}. See Appx.~\ref{app:trainingregimes} for more details on the datasets and training regimes used, and Appx.~\ref{app:equivariancetest} for an additional experiment empirically validating the equivariance properties of our models.

\textbf{Discretisation versus random sampling over compact subgroups} \label{app:discvsuniform} We investigate random sampling to approximate the group convolution. We trained $\mathrm{SE(2)}$-CNNs on rotated MNIST for different resolutions over $\mathrm{SO(2)}$, using random sampling and discretisation. Results are shown in Fig. \ref{fig:discvsunif}. We can clearly see the advantage of approximating the group convolution integral through random sampling, likely attributable to the fact that through random sampling we obtain an unbiased estimator for the convolution, whereas discretisation equates to a biased sampling of the rotation group. 

\textbf{Separable and non-separable G-CNN performance on MNIST variants}
Following \citet{cohen2016group,weiler2018learning,sosnovik2019scale, finzi2020generalizing} we conduct experiments on MNIST variants with separable and non-separable implementations of equivariant models for each group. Next, we discuss results per dataset.

\begin{figure}
\centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/disc-vs-unif.png}
        \caption{\vspace{-3mm}}
        \label{fig:discvsunif}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/mnist-rot.png}
        \caption{\vspace{-3mm}}
        \label{fig:mnistrot}    
    \end{subfigure}
\caption{(a) Test error vs. $\mathrm{SO(2)}$ resolution for separable $\mathrm{SE(2)}$-CNNs on MNIST-rot, discretisation vs. random sampling. (b) Test error versus sampling resolution of $\mathrm{SO(2)}$ on MNIST-rot for separable and non-separable $\mathrm{SE(2)}$-CNNs.}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/mnist-scale.png}
        \caption{\vspace{-3mm}}
        \label{fig:mnistscale}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/time.png}
        \caption{\vspace{-3mm}}
        \label{fig:traintime}
    \end{subfigure} 
    \caption{(a) Test error versus sampled extent of the dilation group for separable and non-separable $\mathrm{\R^2 \rtimes R^+}$-CNNs. (b) Process time per epoch (one pass over training and test sets) in seconds for different resolutions on $H$.}
    \label{fig:scalemnist-processtime}
\end{figure}

\textbf{\textit{Rotated MNIST}} \label{sec:rotmnistexperiments} The $\mathrm{SE(2)}$-CNNs are evaluated on rotated MNIST, the standard benchmark for rotation equivariant models. To assess the influence of sampling resolution over the group, we vary the number of sampled group elements from 1 to 20. The group convolution is approximated through random sampling. Results are shown in Fig.~\ref{fig:mnistrot}. We see the influence of increasing the resolution over the rotation group from 1 to 4 elements, after which the non-separable model saturates, and performance starts to drop when further increasing the number of sampled rotations. This is in line with findings by \citet{bekkers2018roto}, who note that as the group resolution increases, so does the possibility for overfitting on specific spatial configurations. The separable implementation saturates around 7 elements, but performance does not drop significantly beyond this point. This may imply that the reduction in kernel expressivity also has a regularising effect that benefits generalisation. In this experiment, separable group convolutions decisively outperform the non-separable variant.

\textbf{\textit{Scaled MNIST}} The $\R^2 {\rtimes} \R^+$-CNNs are evaluated on MNIST scale. The group convolution integral is approximated through discretisation. In this experiment, we retain the same resolution over the dilation group in all experiments, but instead vary the value at which the group is truncated, ranging from 1.0 at a grid of 1 element in the Lie algebra to 3.0 at a grid of 20 group elements. As in \citet{sosnovik2019scale}, we found that the inter-scale interactions in the group convolution operation reduced performance, likely because of increase in equivariance error due to truncation of the group. Therefore, we limit support of the group convolution kernel on $\R^+$ to two neighbouring elements for all experiments in this paper. Results are shown in Fig. \ref{fig:mnistscale}. Here, non-separable group convolutions outperform the separable variant, suggesting that the ability of non-separable group convolutions to model spatial feature patterns over different scales is beneficial for the scaled MNIST dataset.

\textbf{\textit{Scaled rotated MNIST}} Lastly, the $\mathrm{Sim(2)}$-CNN is benchmarked on MNIST-rot-scale. We limit the sampling resolution at each layer of the G-CNN to 2,4,6 and 8 elements for each subgroup. We test two implementations: (1) separable, where the group convolution is factorised into two convolutions, one over $\R^+ {\rtimes} \mathrm{SO(2)}$ and one over $\R^2$, and (2) $H$-separable, which factorises the convolution into three steps:  a convolution over $\R^+$, one over $\mathrm{SO(2)}$, and one over $\R^2$. As shown in Fig.~\ref{fig:sim2sep} the separable implementation turned out to be unstable for low $\mathrm{SO(2)}$ resolutions. We suspect that an approximation of $\mathrm{SO(2)}$ of only 2 rotation elements, paired with possible aliasing effects and equivariance error occurring over the dilation group, impedes the model from learning robust representations. For higher resolutions over $\mathrm{SO(2)}$, this effect seems to decrease. The results for the $H$-separable implementation (Fig.~\ref{fig:sim2dsep}), highlight a clearer increase in performance as the resolution over both groups increases, which suggests decoupling these information mappings stabilises the learning process.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/sim2-sep.png}
        \caption{\vspace{-3mm}}
        \label{fig:sim2sep}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/sim2-2dsep.png}
        \caption{\vspace{-3mm}}
        \label{fig:sim2dsep}
    \end{subfigure} 
    \caption{(a) Test error versus resolution over rotation and scale group for separable $\mathrm{Sim(2)}-$CNN. (b) Test error versus resolution over rotation and scale group for $\text{H}$-separable $\mathrm{Sim(2)}-$CNN.}
    \label{fig:se2-exps}
\end{figure}

\textbf{Application to computer vision benchmarks}
To evaluate the value of equivariance to different transformations groups in natural image classifcation, we apply the $\mathrm{SE(2)}$-, $\R^2{\rtimes} \R^+$- and $\mathrm{Sim(2)}$-CNNs to three vision datasets. For $\mathrm{SE(2)}$ and $\R^2 {\rtimes} \R^+$ we use resolutions of 4 and 8 group elements, and truncate the scale group at a value of $\sqrt{3}$, as we found this to work well for our experiments. For $\mathrm{Sim(2)}$, we use a resolution of 4 elements over the $\mathrm{SO(2)}$ group and 4 elements over the $\R^2{ \rtimes} \R^+$ subgroups and again truncate $\R^+$ at $\sqrt{3}$. On CIFAR10, as GPU memory allows, we also experiment with resolutions of 8 group elements for both subgroups. As baseline, we implement the same architecture of our non-separable G-CNN, but restrict it to $\mathbb{Z}^2$-equivariance. Results are summarised in Tab.~\ref{tab:results} and exhibit the following general pattern: For most groups, separable group convolutions outperform non-separable ones, and in all experiments best performance is achieved by a $\mathrm{Sim(2)}$-CNN. Next, we discuss results per dataset in-depth.

\textbf{\textit{CIFAR10}} First, we evaluate our models on the CIFAR10 dataset \citep{krizhevsky2009learning}. Results in Tab.~\ref{tab:results} show that on CIFAR10, the non-separable variant of our $\mathrm{SE(2)}$-CNN is outperformed by the translation equivariant baseline. With separable group convolutions and a group resolution of 8 elements, $\mathrm{SE(2)}$-CNN show improved performance. Interestingly, $\R^2 {\rtimes} \R^+$-CNNs outperform both baseline and $\mathrm{SE(2)}$-CNNs, so it seems scale invariance is a better inductive bias than rotation invariance in CIFAR10. Combining scale and rotation invariance leads to the best performance, with separable $\mathrm{Sim(2)}$-CNNs and a resolution of 8 scale and 8 rotation group elements achieving $89.38\%$ test accuracy. The $H$-separable $\mathrm{Sim(2)}$-CNN, which models no interactions between scale and rotation, performs slightly worse.
\begin{table}[]
    \centering
    \caption{Test accuracy (\%) on different vision benchmark datasets. Best performance per group convolution implementation is underlined in blue. Best overall performance is boldfaced. $\dagger$ Separable along rotation and scale dimensions.}
\label{tab:results}
    \begin{small}
\scalebox{0.68}{
    \begin{tabular}{cccccc}
    \toprule
        \sc{Group} &  \sc{No. elem} & \sc{Separable} & \sc{CIFAR10} & \sc{CIFAR100} & \sc{Galaxy10} \\
        \midrule
        \multirow{4}{*}{$\mathrm{SE(2)}$} & \multirow{2}{*}{4} & \xmark & 78.50($\pm$0.99) & \ul{48.39 ($\pm 1.28$)} & 85.58($\pm$0.45) \\
       &&\cmark&\ul{80.43($\pm$0.12)}& 48.11 ($\pm 0.25$) &  \ul{86.96($\pm$0.07)}\\
        \cmidrule{2-6}
       & \multirow{2}{*}{8}& \xmark & 78.59($\pm$0.98) & \ul{51.67 ($\pm 0.65$)}& 84.97($\pm$0.07)\\
       &&\cmark&\ul{82.00($\pm$0.69)} & 50.19 ($\pm 0.40$) & \ul{86.61($\pm$0.38)}\\
        \midrule
        \multirow{4}{*}{$\mathrm{\R \rtimes \R^+}$} & \multirow{2}{*}{4} & \xmark &  83.34($\pm$0.14) & 44.80 ($\pm 0.56$) & \ul{85.13($\pm$0.07)} \\
       &&\cmark & \ul{83.47($\pm$0.13)} & \ul{52.50 ($\pm 0.60$)} & 84.82($\pm$0.25)\\
        \cmidrule{2-6}
       & \multirow{2}{*}{8}& \xmark & 83.37($\pm$0.17) & 44.18($\pm 0.92$) &  84.83($\pm$0.22) \\
       &&\cmark & \ul{83.60($\pm$0.23)} & \ul{53.26 ($\pm 0.10$)} & \ul{85.30($\pm$0.06)} \\
        \midrule
        \multirow{4}{*}{$\mathrm{Sim(2)}$}& \multirow{2}{*}{$4\times4$} & \cmark &  \ul{86.63($\pm$0.60)} & 49.93 ($\pm 0.86$)& 84.90($\pm$0.19) \\
       &&\cmark$^{\dagger}$ &  85.65($\pm$0.66) & \bf{\ul{54.62 $(\pm 1.41)$}}& \bf{\ul{87.45($\pm$0.31)}} \\
        \cmidrule{2-6}
       & \multirow{2}{*}{$8\times8$}& \cmark & \bf{\ul{ 89.38($\pm$0.25)}} & - & - \\
       &&\cmark$^{\dagger}$ &  87.64($\pm$0.16) & - & -\\
        \midrule
        Baseline & - & - & 81.09($\pm0.11$) & 47.91($\pm1.50$) & 85.10($\pm0.42$)\\
        \bottomrule
        \end{tabular}}
        \end{small}
        \vspace{-5mm}
\end{table}
\begin{table*}[!htbp]
    \centering
    \caption{Test error (\%) on rotated MNIST for separable G-CNNs in comparison to other equivariant baselines: G-CNN \citep{cohen2016group}, H-Net \citep{worrall2017harmonic}, RED-NN \citep{salas2019red}, LieConv \citep{finzi2020generalizing}, SFCNN \citep{weiler2018learning}, $\mathrm{E(2)}$-NN \citep{weiler2019general}. {\small$\dagger$ Separable along dilation and rotations dimensions.} {\small + Train-time augmentation by continuous rotations.}}
\label{tab:rotmnistsota}
    \begin{small}
    \scalebox{1}{
    \begin{tabular}{ccccccc|cccc}
    \toprule
    \multicolumn{7}{c}{\textbf{Baseline Methods}}&\multicolumn{4}{c}{\textbf{Separable G-CNNs (Ours)}}\\
    \cmidrule{1-7}\cmidrule{8-11}
        G-CNN & H-Net & RED-NN & LieConv & SFCNN & SFCNN$_+$ & E(2)-NN$_+$ & $\mathrm{SE(2)}$ & $\mathrm{SE(2)}_+$ & $\mathrm{Sim(2)}^\dagger$ &  \sc{$\mathrm{Sim(2)}^\dagger_+$} \\
        \midrule
        2.28 & 1.69 & 1.39 & 1.24 & 0.88 & 0.714 & 0.68 & 0.89{\tiny$\pm.008$} & 0.66{\tiny $\pm$.023} &0.66{\tiny $\pm.009$}&\textbf{0.59{\tiny$\pm.008$}}\\
        \bottomrule
        \end{tabular}
}
    \end{small}
\end{table*}
\begin{table*}[!htbp]
    \centering
    \caption{Test error (\%) on CIFAR10 with All-CNN-C architecture, for our separable group convolutions in comparison to other baselines: All-CNN-C \citep{springenberg2014striving}, $p4$- and $p4m$-G-CNN \citep{cohen2016group}. {\small$\dagger$ Separable along dilation and rotations dimensions.} {\small + Train-time augmentation by random horizontal flips and random cropping. $n$-$\mathrm{Sim(2)}$-CNNs where $n$ is the SIREN hidden size in units. $6$-$\mathrm{Sim(2)}$-CNNs have approximately equal numbers of parameters to the original All-CNN-C.}}
\label{tab:cifar10comp}
    \begin{small}
    \scalebox{0.79}{
    \begin{tabular}{ccccc|cccccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Baseline Methods}}&\multicolumn{6}{c}{\textbf{Separable G-CNNs (Ours)}}\\
    \midrule 
    \multicolumn{1}{c}{1.4m param.}&\multicolumn{2}{c}{1.37m param.}&\multicolumn{2}{c}{1.27m param.}&\multicolumn{2}{c}{1.14m param.}&\multicolumn{2}{c}{1.33m param.}&\multicolumn{2}{c}{3.22m param.}\\
    \midrule
        All-CNN-C & $p4$-G-CNN & $p4$-G-CNN$_+$ &$p4m$-G-CNN & $p4m$-G-CNN$_+$ &  $5$-$\mathrm{Sim(2)}^\dagger$ &  $5$-$\mathrm{Sim(2)}^\dagger_+$ & $6$-$\mathrm{Sim(2)}^\dagger$ &  $6$-$\mathrm{Sim(2)}^\dagger_+$ & $16$-$\mathrm{Sim(2)}^\dagger$ &  $16$-$\mathrm{Sim(2)}^\dagger_+$ \\
        \midrule
        9.08 & 8.84 & 7.67 & 7.59  & 7.04 & 8.50 & 7.41 & 8.22 & 6.47 & 7.27 & \textbf{5.50}\\
        \bottomrule
        \end{tabular}
        }
    \end{small}
\end{table*}

\textbf{\textit{CIFAR100}} To assess the impact of separating spatial and group information in more complex problem settings, we experiment with CIFAR100. Notably, the non-separable $\R^2 {\rtimes} R^+$-CNNs perform well under baseline. We discuss this particular finding in Sec.~\ref{sec:discussion}. For $\mathrm{SE(2)}$-CNNs, non-separable convolutions outperform separable ones, suggesting the model benefits from modelling distinct spatial configurations at different poses. Again, best performance is achieved by the $\mathrm{Sim(2)}$-CNN.


\textbf{\textit{Galaxy10}} Lastly, we apply our models in the domain of astro-photography, on the Galaxy10 dataset \citet{leung2019deep}. Rotation and scaling symmetries exist naturally in this dataset, making it an interesting application for our invariant models. Here, results show little difference between performance of non-separable and separable approaches. Rotation invariance seems to be the stronger inductive bias in this dataset, but again, combining rotation and scale invariance leads to the best performance, with the $H$-separable $\mathrm{Sim(2)}$-CNN achieving $87.45\%$ test accuracy. The higher separable $\mathrm{Sim(2)}$ test error indicates that interactions between scale and rotation features for this dataset impede model performance, again, possibly due to overfitting.

\textbf{Comparison in training time between separable and non-separable group convolutions}
As one of the main motivators for the separable group convolution is its theoretical increase in computational efficiency, a comparison is made in training time between separable and non-separable G-CNNs. We tracked processing times per epoch, which consists of a train- and inference procedure, for the rotated MNIST experiments, and show them in Fig.~\ref{fig:traintime}. Separable group convolutions present a remarkable decrease in inference time, which may be leveraged to allow for model equivariance to larger groups.

\textbf{SOTA on rotated MNIST} To compare separable G-CNNs with related work, we finetune our models to competitive performance. With minimal adjustments to model and training regime, listed in Appx. \ref{app:rotmnistsota}, we are able to achieve state of the art performance on MNIST-rot, see Tab. \ref{tab:rotmnistsota}.

\textbf{Competitive performance on CIFAR10} We compare performance of our approach in a larger model. To this end, like \citep{cohen2016group}, we re-implement the All-CNN-C architecture by \citet{springenberg2014striving}, using our $\mathrm{Sim(2)}$ convolution layers as drop-in replacement. To also compare performance in absolute numbers of trainable parameters, we train three configurations, see Appx. \ref{app:trainingregimes}. We show that $\mathrm{Sim(2)}$-CNNs, with SIREN hidden sizes of 6 and 5 units, containing similar or smaller numbers of trainable parameters to the original All-CNN-C, improve model accuracy compared to CNNs or G-CNNs equivariant to $\mathrm{Sim(2)}$-subgroups $\mathbb{R}^2$ and $p4$, and with data augmentation 6-${\rm Sim(2)}$-CNNs also outperform $p4m$-G-CNNs, see Tab. \ref{tab:cifar10comp}. Increasing SIREN hidden size to 16 leads to even more significant improvements in performance.

\section{Discussion \& Future Work}\label{sec:discussion}
\textbf{On implicit kernel representations} Some of the non-separable configurations generated results below baseline, which is unexpected given that G-CNNs usually improve upon regular CNNs. We conjecture that these results are caused by the SIREN parameterisation, which take as input a coordinate vector of different units (mixing both spatial and sub-group $h$ coordinates). This could limit kernel expressivity, as sharing of a frequency multiplier $\omega_0$ sets kernel smoothness to be identical along spatial and subgroup dimensions. The fact that for the separable variant we use two distinct SIRENs would lift this restriction. Despite useful advantages of MLP parametrisations for the kernels, such as their flexibility and ease of implementation for arbitrary Lie groups, it remains an open problem to have full control over their smoothness; ideally one band-limits the kernel MLPs to the discretised resolution on each axis, which would be an important direction for future work.

\textbf{Conclusion} Motivated by observed redundancies in learned group convolution filters, we introduced separable group convolutions, a computationally efficient implementation of regular group convolutions. We showed that separable group convolutions not only drastically increase computational efficiency, but in many settings also outperform their non-separable counterpart. Furthermore, we demonstrated the value of separable group convolutions as a solution for modelling equivariance to larger groups; $\mathrm{Sim(2)}$-CNNs decivisely outperform models only equivariant to subgroups of $\mathrm{Sim(2)}$, clearly reinforcing equivariance as means of model generalisation.

\bibliography{example_paper}
\bibliographystyle{icml2022}


\newpage
\appendix
\onecolumn
\section{Theory and background}
\subsection{Group theoretic preliminaries}
\label{app:grouptheory}
In this section, group theoretical prerequisites used throughout the paper are briefly refreshed. This is by no means intended as an exhaustive exposition of the fundamentals of group theory, we only introduce those concepts relevant to the current work.

\textbf{Group.} A group is defined by a set $G$ of \textit{group elements}, along with a binary operator $\cdot:G \times G \rightarrow G$, called the \textit{group product}. The group product defines a way to combine each pair of elements $g_1,g_2 \in G$. For the binary operator $\cdot$ to be considered a group product, it needs to satisfy four constraints:
\begin{enumerate}
    \item Closure. $G$ is closed under $\cdot$; for all $g_1, g_2 \in G$ we have $g_1 \cdot g_2 \in G$.
    \item Identity. There exists an identity element $e$ s.t. for each $g \in G$, we have $e \cdot g = g \cdot e = g$.
    \item Inverse. For every element $g \in G$ we have an element $g^{-1} \in G$, s.t. $g \cdot g^{-1} = e$.
    \item Associativity. For every set of elements $g_1, g_2, g_3 \in G$, we have ($g_1 \cdot g_2) \cdot g_3 = g_1 \cdot (g_2 \cdot g_3) $.
\end{enumerate}

\textbf{Lie groups.} A Lie group is a group of which the elements form a smooth manifold. Since the group itself is not necessarily a vector space, combination of elements through addition or subtraction is not defined. However, to each Lie group $G$, an algebra $\mathfrak{g}$ may be associated, given by the tangent space of the Lie group at the identity $T_e(G)$. The Lie algebra may be interpreted as a vector space of infinitesimal generators of the group, a set of elements from which we can obtain the group $G$, by repeated application.

\textbf{Exponential and logarithmic map.} The exponential map $\exp:\mathfrak{g} \rightarrow G$ is a function mapping elements from the Lie algebra to the group. For many transformation Lie groups of interest this map is surjective, and it is possible to define an inverse mapping; the logarithmic map which maps from the group to the algebra. 

\textbf{Semi-direct product groups.} In practice, we are often only interested in data defined over $\R^d$, and hence in this paper only consider affine Lie groups of the form $G = \R^d \rtimes H$, where $\R^d$ is the translation group in $d$ dimensions and $H$ is a transformation Lie group of interest.

\textbf{Group action.} A group $G$ may have an action on a given space $\mathcal{X}$. Given a group element $g\in G$ and a set $\mathcal{X}$, the group action $\mathcal{T}_g$ defines what happens to any element of $x\in \mathcal{X}$ when we apply the transformation given by element $g$ to it. This action is given by:
\begin{equation}
    \mathcal{T}: G \times \mathcal{X} \rightarrow \mathcal{X} \text{ and } \mathcal{T}_g: \mathcal{X} \rightarrow \mathcal{X},
\end{equation}
such that for any two elements $g, h \in G$, we can combine their actions into a single action; $\mathcal{T}_{g,h} = \mathcal{T}_g \circ \mathcal{T}_h$. To avoid clutter, we write the action $\mathcal{T}_g(x)$ as $g\cdot x$.
Note that the action of a group $G$ on domain $\mathcal{X}$ also extends to functions defined on this domain, treated next.

\textbf{Left-regular representations} We extend the group action on $\mathcal{X}$ to \textit{square integrable functions} defined on $\mathcal{X}$; $\mathbb{L}_2 (X)$. Intuitively, any group action on $\mathcal{X}$ induces an action on functions on $\mathcal{X}$; as elements of the set $\mathcal{X}$ are transformed, a function on $\mathcal{X}$ is \textit{dragged along}. Commonly, this is expressed through the \textit{left-regular representations}. Imagine we have a function; $f: \mathcal{X}\rightarrow \mathbb{R}$. Let's say we want to reason about the function $f$ after transformation by group element $r$; let us denote this transformed function $f'$. We may inspect the value of this function for a given element of $\mathcal{X}$ by reasoning backwards from our transformed function. For example, to obtain the value of $f'$ for the transformed element $a'$, we find what the value of $f$ was for $a$ before applying $r$. This is done by applying the inverse of action $r$ to the transformed element $a'$. For any element of the set of square-integrable functions on $\mathcal{X}$; $f\in \mathbb{L}_2 (\mathcal{X})$ the left-regular representation of $g$ is given by:
\begin{equation}
    \mathcal{L}_g: f \rightarrow f' \text{ and for $a' \in \mathcal{T}_g(\mathcal{X})$: } f'(a') = f(\mathcal{T}_{g^{-1}}(a')).
\end{equation}




\textbf{Equivariance}. An operator is equivariant with respect to a group, if it commutes with the action of the group. For an operator $\Phi:\mathbb{L}_2(X)\rightarrow\mathbb{L}_2(Y)$:
\begin{align}
    \forall g \in G: \mathcal{L}_{g} \circ \Phi = \Phi \circ \mathcal{L}_{g}.
\end{align}

\subsection{Deriving separable group convolutions}
\label{app:derivingsepgconv}
If we set the kernel $k$ to be separable, meaning we parameterise it by multiplying a kernel $k_H$ which is constant along the spatial domain and a kernel $k_{\mathbb{R}^2}$ which is constant along the group domain:
\begin{align}
    k(g) &= k(\boldsymbol{x}, h) \nonumber\\
    &= k_{\mathbb{R}^2}(\boldsymbol{x}) k_H(h).
\end{align}
We can derive separable group convolutions as:
\begin{align}
    (f *_{\mathrm{group}} k) (g)&=\int_G f(\tilde{g})k(g^{-1} \cdot \tilde{g})\,{\rm d}\mu(\tilde{g}) \nonumber \\
    &=\int_{\R^2}\int_H f(\tilde{\boldsymbol{x}}, \tilde{h})\mathcal{L}_{x^{-1}}\mathcal{L}_{h^{-1}}k(\tilde{\boldsymbol{x}}, \tilde{h})\frac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h}\nonumber  \\
    &=\int_{\R^2}\int_H f(\tilde{\boldsymbol{x}},\tilde{h})k(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}), h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h} \nonumber \\
    &\rightarrow \int_{\R^2}\int_H f(\tilde{\boldsymbol{x}},\tilde{h})k_{\mathbb{R}^2}(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}))k_H(h^{-1}\cdot \tilde{h})\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h} \nonumber \\
    &= \int_{\R^2}\int_H f(\tilde{\boldsymbol{x}},\tilde{h})k_H(h^{-1}\cdot \tilde{h})k_{\mathbb{R}^2}(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x}))\dfrac{1}{|h|} \,{\rm d}\boldsymbol{\tilde{x}}\,{\rm d}\tilde{h}\nonumber  \\
    &= \int_{\R^2}\left[\int_Hf(\tilde{\boldsymbol{x}},\tilde{h})k_H (h^{-1}\cdot \tilde{h})\,{\rm d}\tilde{h}\right]k_{\R^2}(h^{-1}(\tilde{\boldsymbol{x}}-\boldsymbol{x})) \frac{1}{|h|}\,{\rm d}\tilde{\boldsymbol{x}}. \label{eq:sepgconv_1}
\end{align}

\subsection{Examples of different Lie groups} \label{app:detailsliegroups}
We implemented models equivariant to three different groups: $\mathrm{SE(2)}$, $\mathbb{R}^2 \rtimes \mathbb{R}^+$ and $\mathrm{Sim(2)}$. In this section, we describe these groups in more detail, and give definitions for the logarithmic map required in obtaining G-CNNs equivariant to these groups \citep{bekkers2019b}.

\textbf{The translation group $\mathbb{R}^2$}
The translation group in two dimensions $\mathbb{R}^2$, has group product and inverse for two elements $g=\boldsymbol{x},g'=\boldsymbol{x}' \in \mathbb{R}^2$:
\begin{align}
    g \cdot g' &= (\boldsymbol{x} + \boldsymbol{x}')\\
    g^{-1} &= -\boldsymbol{x}.
\end{align}
With logarithmic map:
\begin{align}
    \log g=\boldsymbol{x}.
\end{align}

\textbf{The rotation group ${\rm SO(2)}$}
The rotation group in two dimensions describes the set of continuous rotation transformations of the plane, and consists of all orthogonal matrices $\boldsymbol{R}$ with determinant 1. Its group product and inverse for two elements $g=\boldsymbol{R}_\theta, g'=\boldsymbol{R}_\theta' \in {\rm SO(2)}$ is given by:
\begin{align}
    g \cdot g' &= \boldsymbol{R}_\theta \boldsymbol{R}_{\theta'} \nonumber \\
    &= \boldsymbol{R}_{\theta + \theta'}\\
    g^{-1} &+ \boldsymbol{R}_\theta ^{-1}.
\end{align}
With logarithmic map:
\begin{align}
    \log g = \begin{bmatrix} 0 & -\theta\mod2\pi  \\ \theta\mod2\pi & 0   \end{bmatrix}.
\end{align}

\textbf{The dilation group ${\mathbb{R}^+}$}
The group of dilation transformations $\mathbb{R}^+$ has a group product and inverse that, for two elements $g=s, g=s' \in \mathbb{R}^+$ are given by:
\begin{align}
    g \cdot g' &= ss'\\
    h^{-1} &= s^{-1}.
\end{align}
The logarithmic map is given by:
\begin{align}
    \log g = \ln s.
\end{align}

\textbf{The Special Euclidean group $\text{SE}(2)$}
The Special Euclidean group in 2 dimensions describes the set of geometric transformations that are formed by combinations of rotations and translations in two dimensions. Each group element can be parameterised by two variables $\theta$ and $\mathbf{x}$, describing the rotation angle and translation vector, $g=(\theta, \mathbf{x})\in G$. For two elements $g, g' \in \text{SE}(2)$, the group product and inverse are given by:
\begin{align}
    g\cdot g' &= (\mathbf{x},\theta) \cdot (\mathbf{x}',\theta') \nonumber \\
    &= (\mathcal{T}_\theta( \mathbf{x}') + \mathbf{x}, \theta + \theta')\\
    g^{-1} &= (-\mathcal{T}_{-\theta} (\mathbf{x}), -\theta).
\end{align}
As we can see, to combine the two elements $g, g'$ we apply the action of the rotation part of $g$ to the translation of $g'$ before we combine it with the translation of $g$, we combine elements semi-directly. $\text{SE}(2)$ is a semidirect product of the translation group $\mathbb{R}^2$ and rotation group $\text{SO}(2)$. We write this as $\text{SE}(2) = \mathbb{R}^2 \rtimes \text{SO}(2)$. To simplify implementation, we separate the logarithmic map into logarithmic maps for ${\rm SO(2)}$ and $\mathbb{R}^2$ in our implementation.

\textbf{The dilation-translation group $\mathbb{R}^2 \rtimes \mathbb{R}^+$}
Another group of interest to our research topic is the translation-dilation group $\mathbb{R}^2\rtimes \mathbb{R}^+$, the group of translation and dilation transformations. Dilations occur frequently in natural images, in the form of scaling transformations of objects and scenes, as the distance between camera and object differs between images. Group elements are parameterised by a scaling factor $s$ and translation element $\mathbf{x}$, $g=(\mathbf{x}, s)\in G$. For two elements $g, g'$, group product and inverse are given by:
\begin{align}
    g \cdot g' &= (\mathbf{x}, s) \cdot (\mathbf{x}', s') \nonumber \\
    &= (\mathcal{T}_s( \mathbf{x}') + \mathbf{x}, s  s')\\
    g^{-1} &= (-\mathcal{T}_{s^{-1}} (\mathbf{x}), s^{-1}).
\end{align}
To simplify implementation, we separate the logarithmic map into logarithmic maps for ${\mathbb{R}^+}$ and $\mathbb{R}^2$ in our implementation.

\textbf{The Similarity group $\text{Sim}(2)$}
The similarity transformation group is the semi-direct product of the roto-translation group $\text{SE}(2)$ and the isotropic scaling group $\mathbb{R}^+$, and defines dilation-roto-translation transformations in two dimensions. Each group element can be parameterised by three variables $\theta$, $s$ and $\mathbf{x}$. For two elements $g, g' \in \text{Sim}(2)$ the group product and inverse are given by:
\begin{align}
    g \cdot g' &= ((\mathbf{x}, \theta), s) \cdot ((\mathbf{x}', \theta'), s') \nonumber\\
    &= ((\mathcal{T}_s(\mathcal{T}_\theta(\mathbf{x}')) + \mathbf{x}, \mathcal{T}_s(\theta') + \theta), s  s')  \nonumber\\
    &= ((\mathcal{T}_{s}(\mathcal{T}_{\mathcal{T}_s(\theta)}(\mathbf{x}')) + \mathbf{x}, \mathcal{T}_s(\theta') + \theta), s  s')  \nonumber\\
    \label{eq:line-commutativity}&= ((\mathcal{T}_s(\mathcal{T}_\theta(\mathbf{x}')) + \mathbf{x}, \theta' + \theta), s  s')\\
    g^{-1} &= (-(\mathcal{T}_{s^{-1}}(\mathcal{T}_{-\theta}(\mathbf{x})), -\theta), s^{-1}).
\end{align}
In Eq. \ref{eq:line-commutativity} we use the fact that the isotropic dilation group has no action on the rotation group. This is a consequence of the fact that $\mathbb{R}^+$ and $\text{SO}(2)$ are both abelian groups, and may be taken in direct product to create the group of dilation-rotation transformations. We again separate logarithmic maps by subgroup.

\subsection{Visualisations of operations in (separable) G-CNNs}
To ease the reading experience of this work, in this section we provide some additional group theoretic perspective on regular CNNs, and additional visualisations and for a number of operations used in (separable) group convolutions.

\textbf{CNNs from a group theoretic perspective} \label{app:cnnsgrouptheory} We give a brief treatment of ordinary CNNs  The ordinary convolution operation used in neural networks requires a definition of a convolution kernel $k$ on $\mathbb{R}^2$, as we are modulating a signal $f$ which itself lives on $\mathbb{R}^2$. The kernel $k$ is applied to $f$ on every location in the input space $\mathbb{R}^2$ to again yield a function over $\mathbb{R}^2$. Intuitively, this is the same as saying (1) we transform the convolution kernel $k$ under the action of every group element $\boldsymbol{x} \in \mathbb{R}^2$, to obtain a set of kernels $K=\{ \mathcal{L}_{\boldsymbol{x}}(k) | \boldsymbol{x} \in \mathbb{R}^2\}$, and (2) take the inner product of the input $f$ with each kernel in this set of transformed kernels $K$. By \textit{tying} the kernel weights used throughout the translation group, learned features are automatically generalised over spatial positions. This intuition is visualised in Fig. \ref{fig:cnnsgrouptheory}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/intuitivevisualisations/cnnsgrouptheory.png}
\caption[CNNs from a group theoretic perspective]{The convolution operation in a CNN. In upper section of the figure above, a spatial kernel $k$ is transformed by group element $\boldsymbol{x}_{(2, 2)}\in \mathbb{R}^2$ to yield $\mathcal{L}_{\boldsymbol{x}_{(2,2)}}(k)$. In the lower section, $k$ is transformed under the action of each element of (a discretisation of) the group $\mathbb{R}^2$, to yield a set $K$ of translated copies of $k$. Taking the inner product of $f_{in}$ with each of these copies yields a new feature map $f_{out}$ defined over $G=\mathbb{R}^2$.}
\label{fig:cnnsgrouptheory}
\end{figure}

\textbf{Lifting convolution} In Fig. \ref{fig:liftingconv} we show a visualisation of the lifting convolution for the $\mathrm{SE(2)}$ group.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/intuitivevisualisations/liftingconvolution.png}
\caption[Lifting Convolution]{An example of the lifting convolution for $G=\mathrm{SE(2)}=\mathbb{R}^2\rtimes \mathrm{SO(2)}$. In the upper section of the above figure, a spatial kernel $k$ is transformed by group element $(\boldsymbol{x}, \theta) \in \mathrm{SE(2)}$ with $\boldsymbol{x}=(2,2),\theta=90\degree$ to yield $\mathcal{L}_{\boldsymbol{x}_{(2,2)}}\mathcal{L}_{\theta_{90}}(k)$. In the lower section, $k$ is transformed under the action of each element of (a discretisation of) the group $\mathrm{SE(2)}$, to yield a set $K$ of translated and rotated copies of $k$. Taking the inner product of $f_{in}$ with each of these copies yields a new feature map $f_{out}$ defined over $G=\mathrm{SE(2)}$.}
\label{fig:liftingconv}
\end{figure}

\textbf{Group convolution} In Fig. \ref{fig:groupconv} we show a visualisation of the group convolution for the $\mathrm{SE(2)}$ group.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/intuitivevisualisations/groupconvolution.png}
\caption[Group Convolution]{An example of the group convolution for (a discretisation of) $G=\mathrm{SE(2)}=\mathbb{R}^2\rtimes \mathrm{SO(2)}$. In the upper section of the above figure, a group convolution kernel $k$ (defined over $G$) is transformed by group element $(\boldsymbol{x}, \theta) \in \mathrm{SE(2)}$ with $\boldsymbol{x}=(2,2),\theta=90\degree$ to yield $\mathcal{L}_{\boldsymbol{x}_{(2,2)}}\mathcal{L}_{\theta_{90}}(k)$. In the lower section, $k$ is transformed under the action of each element of (a discretisation of) the group $\mathrm{SE(2)}$, to yield a set $K$ of translated and rotated copies of $k$. Taking the inner product of $f_{in}$ with each of these copies yields a new feature map $f_{out}$ defined over $G=\mathrm{SE(2)}$.}
\label{fig:groupconv}
\end{figure}

\textbf{Separable group convolution kernel} In Fig. \ref{fig:separablekernel} we show a visualisation of a separable group convolution kernel.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/intuitivevisualisations/separablekernel.png}
\caption[Separable group convolution kernel]{The separable group convolution kernel parameterises the kernel $k:G\rightarrow\mathbb{R}$ as $k_H k_{\mathbb{R}^2}$. $k_H$ is defined over the subgroup $H$, with no spatial extent, whereas $k_{\mathbb{R}^2}$ is defined only on $\mathbb{R}^2$ with no extent over the group. The full group convolution kernel $k$ is obtained by repeating $k_{\mathbb{R}^2}$ along $H$, weighting each instance of $k_{\mathbb{R}^2}$ by its corresponding value for $h \in k_H$. In practice, it is computationally advantageous to apply the convolution operations in sequence.}
\label{fig:separablekernel}
\end{figure}

\textbf{Separable group convolution} In Fig. \ref{fig:sepgroupconv} we show a visualisation of the separable group convolution operation.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/intuitivevisualisations/separablegroupconvolution.png}
\caption[Separable group convolution]{An example of the separable group convolution for (a discretisation of) $G=\mathrm{SE(2)}=\mathbb{R}^2\rtimes \mathrm{SO(2)}$. In the upper section of the above figure, a kernel $k_H$ (defined over $H$) and a kernel $k_{\mathbb{R}^2}$ (defined over $\mathbb{R}^2$) are transformed by group element $(\boldsymbol{x}, \theta) \in \mathrm{SE(2)}$ with $\boldsymbol{x}=(2,2),\theta=90\degree$ to yield $\mathcal{L}_{\boldsymbol{x}_{(2,2)}}\mathcal{L}_{\theta_{90}}(k_H)$ and $\mathcal{L}_{\boldsymbol{x}_{(2,2)}}\mathcal{L}_{\theta_{90}}(k_{\mathbb{R}^2})$. In the lower section, $k_H$ and $k_{\mathbb{R}^2}$ are transformed under the action of each element of (a discretisation of) the group $\mathrm{SE(2)}$, to yield sets $K_H$ of transformed copies of $k_H$ and a set $k_{\mathbb{R}^2}$ of translated and rotated copies of $k$. Convolving $f_{in}$ with $K_H$ and $K_{\mathbb{R}^2}$ sequentially yields a feature map $f_{out}$ defined over $G=\mathrm{SE(2)}$.}
\label{fig:sepgroupconv}
\end{figure}

\textbf{Defining convolution kernels on Lie groups} In Fig. \ref{fig:gridongroup} we show how we obtain a grid on the Lie group $\mathrm{SO(3)}/\mathrm{SO(2)}$ by mapping from its algebra. In Fig. \ref{fig:kernelongroup} we show how we subsequently obtain a kernel on this group by defining a SIREN on its algebra.

\begin{figure}
\begin{minipage}{.5\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/intuitivevisualisations/kernel-on-lie-alg.png}
\captionsetup{width=.7\linewidth}
\caption[A kernel on the Lie algebra]{An example of a localised kernel $k$ on the quotient space $\mathrm{SO(3)/SO(2)}$. Although the SIREN parameterising the kernel is defined on $\mathfrak{g}$, we can associate the kernel values sampled on $\mathfrak{g}$ with the relevant elements $g' \in G$ through use of the grid $\mathcal{H}$ defined on $G$, which we map to $\mathfrak{g}$ via the logarithmic map.}
\label{fig:kernelongroup}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
\captionsetup{width=.9\linewidth}
\includegraphics[width=0.9\textwidth]{figures/intuitivevisualisations/grid-on-lie-alg.png}
\caption[A grid on the Lie algebra]{An example of a local kernel grid $\mathcal{H}$ on the quotient space $\mathrm{SO(3)/SO(2)}$. We obtain a volumetrically constant grid on $G$ by sampling a set of equidistant points in $\mathfrak{g}$ and mapping them to $G$ via the exponential map. The grid on $G$, show in this figure, then serves as input for the SIREN defined on $\mathfrak{g}$ via the logarithmic map.}
\label{fig:gridongroup}
\end{minipage}
\end{figure}
\subsection{Expressivity limitations of separable group convolutions}\label{app:featurescannotrecognise}
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/intuitionsepgconvs.png}
\caption[What features separable group convolutions cannot recognise]{An example of a configuration of features $f_{out}$ a \textit{single} separable group convolution kernel is unable to represent.}
\label{fig:intuitionlimitations}
\end{figure}
As discussed, separable group convolution kernels are strictly less expressive than their non-separable counterpart. For example, in the case of the roto-translation group $\mathrm{SE(2)}$, this would enable non-separable group convolution kernels to learn to represent features that are built up of different spatial configurations at different orientations.

We draw up a simplified example illustrated in Fig. \ref{fig:intuitionlimitations}. Assume we have an elementary feature type $e$, and a spatial kernel $k$ with which we can recognise this feature type in its canonical pose $\theta_0$. In our input $f_{in}$, we have three instances of the feature $e$, one under the canonical pose $\theta_0$, and two under a $90\degree$ rotation. Applying the lifting convolution using kernel $k$ for the group $G=\mathbb{R}^2 \rtimes C_4$ of translations and $90\degree$ rotations yields a feature map $f_{out}$ defined over $G$, with spatial feature maps for $\theta_0, ..., \theta_{270}$. The spatial feature map $f^{\theta_0}_{out}$ contains a response at a single spatial position. In contrast, the feature map $f^{\theta_{90}}_{out}$ contains a response at two spatial positions. The spatial configurations for the feature maps along $H$ are different. A single conventional group convolution kernel could learn to recognise these distinct spatial configurations along the subgroup axis, whereas a separable group convolution kernel could not, since it simply repeats (a weighted version of) the same spatial kernel $k_{\mathbb{R}^2}$ along the group axis.

Although this reduction in expressivity could theoretically prove limiting in the application of G-CNNs on vision tasks, our experiments show that in practice this rarely seems a problem, and may even help prevent overfitting.

\subsection{Kernel smoothness for group convolutions on continuous groups}\label{app:kernelsmoothnesscontinuous}
As mentioned, using SIRENs we are able to explicitly control kernel smoothness. We briefly elaborate on the importance of kernel smoothness in G-CNNs. In conventional CNNs, weights at distinct spatial locations are generally initialised independently. Because the kernels are only transformed using discrete translation operations $\boldsymbol{x}\in \mathbb{Z}^2$, translation equivariance is ensured by virtue of using the exact same weights values throughout all spatial locations.

In G-CNNs for continuous groups, the kernel is transformed under actions of a continuous transformation group of interest $H$ to obtain equivariance. However, in our convolution operation we are still using a discretised kernel; we are required to sample kernel values at \textit{different grid points} for different elements $h\in H$. We are no longer able to simply reuse the same weight values throughout the group as with regular CNNs. To this end, we define our kernels in an analytical form, which we can trivially evaluate at arbitrary grid points. Because our grid has a fixed resolution, the kernels sampled from this analytical form are susceptible to aliasing effects; the analytical kernel function may exhibit higher frequencies than can be captured in the discretisation of the kernel. We visualise the effects of aliasing in Fig. \ref{fig:aliasing}.

In short, for continuous groups, the group action transforms a signal smoothly as the group is traversed. To prevent discretisation artefacts, we want our kernel to exhibit the same smoothly transforming behaviour, hence we use SIRENs, as they offer explicit control over the smoothness of kernels in their analytical form.
\begin{figure}
\centering
\includegraphics[width=0.96\textwidth]{figures/aliasing.png}
\caption[Aliasing effects in analytical kernel functions]{Example of aliasing effects in analytical kernel parameterisations. Assume we are performing a lifting convolution to the $\mathrm{SE(2)}$ group. We evaluate two variants of our kernel function; $k^{\varphi}_l$ exhibiting low frequencies over the kernel domain, and $k^{\varphi}_h$ exhibiting high frequencies over kernel domain. We evaluate these functions for a spatial grid $\mathcal{H}_{g_e}$ to obtain a kernel under the identity rotation, and for a slightly rotated version of this grid $\mathcal{H}_{g_\epsilon}$. We see the kernel sampled from $k^{\varphi}_l$ changes smoothly, whereas the high frequency components in $k_h^{\varphi}$ lead to considerably different results for the two grids.}
\label{fig:aliasing}
\end{figure}

\section{Additional experimental details}

\subsection{Architectural details and parameterisation} \label{app:architectureandparam}
\textbf{Model architecture} \label{app:architecture} As architecture for our experiments we use a simple ResNet model \citep{he2016deep}. We use a single lifting convolution with 32 output channels, followed by two residual blocks of 2 group convolutions. The first block has 32 output channels, the second has 64 output channels. After the first residual block we apply max-pooling with kernel size 2 over the spatial dimensions of the feature map. After the last residual block, we apply max pooling over remaining spatial and subgroup dimensions, followed by two linear layers with batchnorm and ReLU in between. An overview is given in Fig. \ref{fig:architecture}.

\begin{figure}
\centering
  \includegraphics[width=0.6\linewidth]{figures/architecture.png}
  \caption{Architecture used over all experiments.}
  \label{fig:architecture}
\end{figure}

\textbf{Group convolution blocks and random sampling on the group} In our residual blocks \citep{he2016deep}, we subsequently convolve the input to the block $\boldsymbol{x}_{\text{in}}$ by two group convolutions, \texttt{gconv\_1} and \texttt{gconv\_2}, yielding $\boldsymbol{x}_{\text{out}}$ and apply elementwise addition with the input $\boldsymbol{x}_{\text{in}}$, followed by ReLU activation.

When approximating the group convolution through random sampling, we must take care to define the input and output of the two group convolution layers on the same grid over the group as the skip-connection to ensure a well-defined equivariant group convolution block. This may be done by adding a group shortcut layer which maps from the set of input elements on the group to the set of output elements. We implement this as a group convolution with a $1\times 1$ spatial extent. The group shortcut layer thus simultaneously serves as a channelwise projection from the input space of \texttt{gconv\_1} to the output space of \texttt{gconv\_2} \textit{and} maps from the input grid on $H$ of the first group convolution to the output grid on $H$ of the second group convolution.

\textbf{SIREN architecture and separable kernel parameterisation} All our kernels are parameterised by a SIREN  \citep{sitzmann2020implicit}. In a SIREN, output $\boldsymbol{y}^l$ for a layer $l$ and input $\boldsymbol{x}^{l-1}$ is defined by:
\begin{align}
    \boldsymbol{y}^l = \sin(\omega_0 \boldsymbol{W}^l \boldsymbol{x}^{l-1} + \boldsymbol{b}^l)
\end{align}
In this equation, $\omega_0$ acts as a multiplier for low dimensional frequencies found in the input domain (the grid of relative offsets on the group), which explicitly introduces higher frequencies, allowing the neural net to learn high-frequency functions (such as kernels). We found a value for $\omega_0$ of 10 to work well in all our experiments. For the SIREN, we used an architecture of two hidden layers of 64 units. In non-separable G-CNNs we have a single SIREN with a final layer mapping to a vector $\R^{c_{in} \times c_{out}}$. In separable G-CNNs, we use two SIRENs, the first mapping to a function the Lie algebra of the subgroup $H$; $\R^{c_{in} \times c_{out}}$, and the second mapping to $\R^{c_{out}}$. Formulating the kernel $k$ for a group element $g=(\boldsymbol{x}, h)$ in terms of $k_H$, $k_{\mathbb{R}^2}$, input channel $i$ and output channel $j$, and logarithmic map on H $\log_H$, we obtain:
\begin{equation}
    k^{i,j} (g) = k_H^{i,j}(\log_H h)k_{\mathbb{R}^2}^{j}(\boldsymbol{x}) \\
\end{equation}

Lastly, for $\text{H}$-separable $\mathrm{Sim(2)}$-CNNs we use three SIRENs, the first mapping to a function the Lie algebra of the subgroup $\mathbb{R}^+$; $\R^{c_{in} \times c_{out}}$, the second mapping $\mathrm{SO(2)}$ to $\mathbb{R}^{c_{out}}$, and the third mapping $\mathbb{R}^2$ to $\R^{c_{out}}$. one mapping to $\mathrm{SO(2)}$. Formulating the kernel $k$ for a group element $g=((\boldsymbol{x}, \theta), s)$ in terms of $k_{\mathrm{SO(2)}}$, $k_{\mathbb{R}^+}$, $k_{\mathbb{R}^2}$, input channel $i$ and output channel $j$, logarithmic maps $\log_{\mathbb{R}^+}$ and $\log_{\rm SO(2)}$ on $\mathbb{R}^+$ and $\mathrm{SO(2)}$ respectively we obtain:
\begin{equation}
    k^{i,j} (g) = k_{\mathbb{R}^+}^{i,j}(\log_{\mathbb{R}^+} s)k^{j}_{\mathrm{SO(2)}}(\log_{\rm SO(2)}\theta)k_{\mathbb{R}^2}^{j}(\boldsymbol{x}) \\
\end{equation}

\textbf{Model sizes}\label{app:modelsizes} In Tab. \ref{tab:parameters}, we report the number of trainable parameters for each model configuration. Throughout our experiments, we kept the number of channels in all our configurations constant, to fairly compare the expressivity of the learned representations in non-separable and separable group convolutions. This has as effect that the number of parameters in the separable implementations is larger than for non-separable implementations, due to our use of separate SIRENs to parameterise kernels over the different subgroups. To ensure that the difference in number of trainable parameters does not influence the comparison between separable and non-separable group convolution layers, we explicitly chose to over-parameterise our SIREN architecture, as shown in an additional ablation in Appx. \ref{app:overparameterisedsirens}. We only change SIREN hidden size when comparing our models to baselines proposed in related works as detailed in Appx. \ref{app:trainingregimes}.

\begin{table}[]
    \centering
    \caption{Number of trainable parameters for different implementations. For all groups and datasets, these numbers are kept constant.}
\label{tab:parameters}
    \begin{tabular}{ccc}
    \toprule
        \sc{Non-separable} &  \sc{Separable} & \sc{H-separable} \\
        \midrule
        742k & 803k & 864k\\
        \bottomrule
        \end{tabular}
        \vspace{-3mm}
\end{table}

\subsection{Experimental details}
\label{app:trainingregimes}
Here, we list training regimes for all experiments. We keep these as consistent as possible, as we are only interested in the effect of separating the group convolution operation, and in isolating the effect of incorporating equivariance into our models. We list information on all datasets, and any dataset-specific model configuration details we use in our experiments here.

\textbf{Optimizer} All architectures are trained with Adam optimisation \citep{kingma2014adam}, and  $1e^{-4}$ weight decay. All models are trained on a single Titan V.

\textbf{Rotated MNIST} The 62.000 MNIST images \citep{lecun1998gradient} are split into a training, validation and test set of 10.000, 2.000 and 50.000 images respectively, and randomly rotated to orientations between $[0, 2\pi)$. Note that in \citep{weiler2018learning, weiler2019general}, the rotated MNIST dataset is augmented during training by transforming images with random continuous rotations. We only use train-time augmentation for the state-of-the-art results obtained in Tab. \ref{tab:rotmnistsota}, as detailed in Appx. \ref{app:rotmnistsota}. All models trained on rotated MNIST, except for the state-of-the-art runs detailed in \ref{app:rotmnistsota}, are trained for 200 epochs with a batch size of 128 and a learning rate of $1 \cdot 10^{-4}$.

\textbf{Scaled MNIST} The 62.000 MNIST images are again split into a training, validation and test set of 10.000, 2.000 and 50.000 images respectively, but now randomly scaled by a factor [0.3, 1] and padded with zeros to retain the original resolution. No data-augmentations of any kind are used in the experiments on scaled MNIST. All models trained on scaled MNIST are trained for 200 epochs with a batch size of 128 and a learning rate of $1 \cdot 10^{-4}$. 

\textbf{Scaled rotated MNIST} The 62.000 MNIST images are again split into a training, validation and test set of 10.000, 2.000 and 50.000 images respectively, but now randomly scaled by a factor [0.3, 1],  padded with zeros to retain the original resolution \textit{and} randomly rotated by orientations between $[0, 2\pi)$. For the experiments on scaled rotated MNIST we do not use data-augmentation of any kind. All models trained on rotated scaled MNIST are trained for 200 epochs with a batch size of 128 and a learning rate of $1 \cdot 10^{-4}$. 

\textbf{CIFAR10} We evaluate our models on the CIFAR10 dataset, containing 62.000 $32\times32$ color images in 10 balanced classes \citep{krizhevsky2009learning}. All models trained on CIFAR10 are trained for 200 epochs with a batch size of 128 and a learning rate of $1 \cdot 10^{-4}$.

\textbf{CIFAR100} We evaluate our models on the CIFAR100 dataset, containing 62.000 $32\times32$ color images in 100 balanced classes \citep{krizhevsky2009learning}. All models trained on CIFAR10 are trained for 200 epochs with a batch size of 128 and a learning rate of $1 \cdot 10^{-4}$.

\textbf{Galaxy10} This dataset contains 21785 69x69 color images of galaxies divided into 10 unbalanced classes. For Galaxy10, we limit batch size to 32 images, and scale learning rate accordingly to $2.5\cdot10^{-5}$, as suggested by \citet{goyal2017accurate}.

\textbf{Achieving SOTA on rotated MNIST}\label{app:rotmnistsota}
To compare performance of our separable regular G-CNNs with previous work, we apply our implementation of $\mathrm{SE(2)}$ and $\mathrm{Sim(2)}$-CNNs to achieve state of the art results on the rotated MNIST dataset, discussed in Sec. \ref{sec:rotmnistexperiments}.

We make some slight adjustment to our architecture and training regime: we reduce the SIREN to a single hidden layer of 64 units. The convolution over $\mathrm{SO(2)}$ is approximated using random sampling, while the convolution over $\R^+$ is approximated using a discretisation with a truncation of the scale group at $s=\sqrt{3}$. We use a batch size of 64, and a learning rate of $1\cdot 10^{-4}$ and train for 300 epochs. To fairly assess the impact of equivariance, we train a number of models without continuous rotation augmentations.

To compare with previous SOTA, achieved by \citet{weiler2019general}, we also train models \textit{with} continuous rotation augmentations. \citet{weiler2019general} does not specify performance results for models without train-time augmentations, therefore we also compare with the prior SOTA by \citet{weiler2018learning}, which trains models both with and without augmentations.

We show results in Tab. \ref{tab:rotmnistsota}. With $\mathrm{SE(2)}$-CNNs we achieve best performance with a resolution of 20 elements over $\mathrm{SO(2)}$. For the $\mathrm{Sim(2)}$-CNNs, we achieve best performance with a resolution of 12 elements over $\mathrm{SO(2)}$ and 5 elements over $\mathrm{\R}^+$. These results show that without train-time augmentation $\mathrm{Sim(2)}$-CNNs outperform the previous SOTA by \citet{weiler2019general}, which was trained with continuous rotation augmentations. This improvement further increases when we add train-time augmentation by continuous rotations, to a test error of 0.59\%.

\textbf{Achieving competitive performance on CIFAR10} To further compare the viability and performance of our separable approach to group convolutions, following \citet{cohen2016group} we re-implement the All-CNN-C proposed by \citep{springenberg2014striving}, using our separable group convolution layers as drop-in replacement for the regular convolution layers. All experiments described in this section use this exact architecture.

In this experiment we looked to isolate the power of our separable group convolution layers in larger-scale models, which is why we deliberately chose a large but relatively simple architecture. We keep the number of channels constant with the original implementation by \citep{springenberg2014striving}. Because of the way our kernels are parameterised, it is hard to make a direct comparison in model performance in terms of representation expressivity, while keeping the number of trainable parameters equal to the original model. Therefore, we train three configurations, with different SIREN hidden sizes.

To obtain a model with approximately the same number of trainable parameters, for the first configuration we limit the hidden size of our SIRENs to 6 to arrive at 1.33m parameters, where the original model has 1.4m. Note that this limitation likely has considerable implications for the expressivity of the sampled group convolution kernels.

To push our separable group convolution layers further, we also train a model configuration with a SIREN hidden size of 5 units, to arrive at a model with 1.14m parameters (19\% smaller than the original model by \citet{springenberg2014striving}).

Lastly, to see how our layers would fare when unimpeded by limitations in SIREN size, we train a configuration with a SIREN hidden size of 16 units. Note that this model contains 3.22m parameters (representing an 128\% increase in parameter count to the original model).

All models are trained with a resolution of 8 elements randomly sampled over $\mathrm{SO(2)}$ and a discretisation of the scale group of 3 elements truncated at $\sqrt{3}$. We train all of these configurations both with and without data augmentation by padding the original image by at most 6 pixels and randomly cropping to the original resolution, and random horizontal flips. All models are trained for 300 epochs with a learning rate of $1\cdot 10^{-4}$.

Results are shown in Tab. \ref{tab:cifar10comp}. We can see that for approximately equal parameter counts, our separable group convolution layers equivariant to $\mathrm{Sim(2)}$ outperform the original CNN baseline and the $p4$-G-CNN. Without data augmentation, our model with 6 hidden units is outperformed by the $p4m$-G-CNN which is also equivariant to reflections. With augmentation (containing reflection augmentations) our model outperforms all baseline models we compare to by a significant margin. The smaller configuration with 5 units seems to limit kernel expressivity somewhat, although without data augmentation it still outperforms the translation equivariant original implementation of the All-CNN-C. It seems data augmentation throws the model off in this particular configuration. The larger configuration with 16 hidden units, both with and without data augmentation, significantly outperforms all baselines and other configurations.

\section{Additional experiments}
\subsection{Validating model invariance to transformation groups} \label{app:equivariancetest}
Following \citet{weiler2018learning}, we empirically assess the level of model-invariance to rotation transformations, by training on upright MNIST, and subsequently assessing performance on a test set of MNIST images that have been transformed by a subgroup element $h \in H$.

Each model is trained on the MNIST training set containing 60,000 images, and tested on 10,000 transformed images. Performance is tested for rotations over a range between $[0, 2\pi)$, in 100 steps. The group convolution is approximated through random sampling. Results are shown in figures \ref{fig:mnistrot-equiv-eval-1} and \ref{fig:mnistrot-equiv-eval} for 1, 2 and 8 group elements for the separable and non-separable variants. We can see that equivariance error is very similar for separable and non-separable variants, and greatly reduces with increased resolution over the group. We do note that for 8 elements, the non-separable group convolution achieves slightly better performance. 

\begin{figure}
\centering
\begin{minipage}{.45\textwidth}
\centering
  \includegraphics[width=\linewidth]{figures/req-sep.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test error vs rotation angle of MNIST test set, when trained on upright MNIST, for separable $\mathrm{SE(2)}$-CNN.}
  \label{fig:mnistrot-equiv-eval-1}
\end{minipage}
\begin{minipage}{.45\textwidth}
\centering
  \includegraphics[width=\linewidth]{figures/req-nonsep.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test error vs rotation angle of MNIST test set, when trained on upright MNIST, for non-separable $\mathrm{SE(2)}$-CNN.}
  \label{fig:mnistrot-equiv-eval}
\end{minipage}
\end{figure}

\subsection{Support of the separable convolution kernels over the channel dimensions}
\label{app:gsep-vs-sep} As explained in Sec. \ref{sec:depthwisesep}, when factorising group convolution kernels into a sequence of convolutions, we are presented with a choice of kernel support over the input channel dimension $i$ and output channel dimensions $j$. In this additional ablation study, we investigate a number of possible configurations for the kernel factorisation, detailed in Tab. \ref{tab:factorisations}. In this table, factorisations are listed in terms of whether they separate convolutions over the group, and whether they separate input channel dimensions from spatial and subgroup dimensions. In those cases where neither spatial nor subgroup kernels provide support over the input channels, an additional kernel $k_C$ is used which only depends on input channels $i$ and output channels $j$. Note that this directly corresponds to the application of depthwise separable convolutions as in \citet{haase2020rethinking}.

\begin{table}[]
    \centering
    \caption{Group convolution kernel factorisation in terms of input channels $i$ and output channels $j$.}
\label{tab:factorisations}
    \begin{tabular}{l|lll|l}
    \toprule
        \sc{Name} & \sc{Group sep.} & $H-$\sc{depthwise sep.} & $\mathbb{R}^2$\sc{-depthwise sep.} & \sc{Factorisation} \\
        \midrule
        Nonseparable & \xmark & \xmark & \xmark & $k^{ij}(\boldsymbol{x}, h)$\\
        Dseparable & \xmark & \cmark & \cmark & $k_C^{ij} k^{j}(\boldsymbol{x}, h)$\\
        \midrule
        Separable & \cmark & \xmark & \cmark & $k^{ij}_H(h)k^j_{\mathbb{R}^2} (\boldsymbol{x})$ \\
        Gseparable & \cmark & \xmark & \xmark & $k^{ij}_H(h)k^{ij}_{\mathbb{R}^2} (\boldsymbol{x})$ \\
        DGseparable & \cmark & \cmark & \cmark & $k_C^{ij} k^{j}_H(h)k^j_{\mathbb{R}^2} (\boldsymbol{x})$\\
        \bottomrule
        \end{tabular}
        \vspace{-3mm}
\end{table}



For this experiment on CIFAR10, we use the same depthwise group separable $\mathrm{\mathbb{R}^2\rtimes \mathbb{R}^+}-$CNN as in Sec. \ref{sec:rotmnistexperiments}, with the architecture described in Appx. \ref{app:architectureandparam}. To keep a roughly consistent numbers of parameters over all experiments, we vary the number of channels listed in Appx. \ref{app:architectureandparam}. For non-separable and separable $\mathrm{\mathbb{R}^2\rtimes \mathbb{R}^+}-$CNNs we use a widening factor of 1$\times$ and 0.98$\times$ respectively. For Dseparable and DGseparable, we use a widening factor of 2.5$\times$, and for Gseparable a widening factor of 0.72$\times$. We train the models for 150 epochs, using adam optimisation, with a learning rate of $1\cdot 10^{-4}$.





Results, shown in Fig. \ref{fig:sepconfig} highlight that additional spatial-depthwise separability obtains highest performance on a fixed parameter budget, which is why we stick with additional depthwise separation of the spatial kernel, and name this approach the separable group convolution.

Interestingly, DSeparable convolutions perform significantly worse than any other configuration for higher numbers of group elements. We hypothesize this drop in performance results from drastically increasing the number of output channels over which the group convolution kernel is sampled, while keeping SIREN architecture constant. Viewing the SIREN as learning a convolution kernel in terms of a linear combination of nonlinear basis functions applied on the input coordinates, this would require a SIREN to express an increasingly complex function as a linear combination of the same number of basis functions. Possibly, the learned basis functions simply do not possess the expressive power to represent increasingly complex kernels required to process a larger range of dilation factors. The same phenomenon possibly shows for the GSeparable kernels, when increasing sampling resolution to more than 8 elements, as performance drops significantly here as well. When separating the group convolution kernels by subgroups, \textit{and} limiting support of one or both of the subgroup kernels over the input channels (Separable, DGSeparable), no such drop in performance is seen.

As increased computational efficiency is a main motivating factor for separable group convolutions, we also kept track of the process time per epoch for each different configuration, and visualised these in Fig. \ref{fig:time-sep}. Here, some notable differences arise. Only Separable and GSeparable implementations are more efficient than Nonseparable implementations for the group resolutions we test in this experiment. We think this may be attributed to the increased computational overhead induced by factorising a single Pytorch \texttt{conv2d} operation used in the Nonseparable implementation (which is heavily optimised) into a sequence of multiple convolution and matrix multiplication operations. We use SIRENs to parameterise the convolution kernels, which entails that the overhead induced by parameterising the kernel is larger for non group-separable implementations. This combination of factors likely contributes to the DSeparable implementation performing worst in these experiments in terms of computational efficiency, followed by the DGSeparable implementation. As we can see, both of these implementations increase sub-linearly dependent on the number of group elements sampled. The Nonseparable implementation increases super-linearly, and hence we conjecture that increasing the number of sampled group elements beyond 8 may comparatively reduce the overhead caused by factorisation to the point that DSeparable and DGSeparable implementations outperform the Nonseparable implementation. Again, the Separable implementation performs best.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\linewidth]{figures/separability.png}
        \caption{}
        \label{fig:sepconfig}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\linewidth]{figures/time-separability.png}
        \caption{}
        \label{fig:time-sep}
    \end{subfigure}
    \caption{(a) Test accuracy (\%) versus $\mathbb{R}^2 \rtimes \mathbb{R}^+$ resolution on CIFAR10 for different separability configurations. (b) Process time per epoch for different separability configurations.}
    \label{fig:separability}
\end{figure}



\subsection{How does SIREN architecture influence performance?} \label{app:overparameterisedsirens}
As explained, we keep the architecture of our SIRENs constant over all experiments. Because we use multiple SIRENs in separable architectures, and we want to isolate the effect of separability of the group convolution operation, we deliberately choose to over-parameterise the SIREN architectures, so as not to advantage the separable implementations by their increased parameter numbers. To this end, we look for the \textit{largest} size of hidden layer that does not negatively impact performance in the non-separable implementation.

We assess the impact of the size of the hidden layers in our SIREN on performance on rotated MNIST for separable and non-separable $\mathrm{SE(2)}$-CNN implementations. We sample at a resolution over $\mathrm{SO(2)}$ of 8 elements. Interestingly, results in Fig. \ref{fig:siren-arch} show that even with very small SIRENs - a hidden size of 4 or 8 - performance of the non-separable implementations is remarkable. Separable implementations clearly underperform for small SIREN sizes, leading us to hypothesize that the restriction in kernel expressivity due separability paired with under-parameterisation is detrimental to performance in these configurations.

For non-separable implementations, performance barely increases from 16 hidden units on, and starts to decrease after a size of 64 hidden units. For this reason, we choose a size of 64 hidden units in all SIRENs in our experiments.

We repeat the same experiment for $\mathbb{R}^2\rtimes \mathbb{R}^+$-CNNs on CIFAR10, and visualise the results in Fig. \ref{fig:siren-arch-c10}. Here, performance for very small SIREN configurations is less impressive, but we still see a similar saturation around a SIREN size of 64 hidden units.

\begin{figure}
\centering
  \includegraphics[width=\linewidth]{figures/siren-arch.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test accuracy of $\mathrm{SE(2)}$-CNNs on rotated MNIST for different sizes hidden layers in the SIREN parameterisation.}
  \label{fig:siren-arch}
\end{figure}
\begin{figure}
\centering
  \includegraphics[width=\linewidth]{figures/siren-arch-C10.png}
  \captionsetup{width=.9\linewidth}
  \caption{Test accuracy of $\mathbb{R}^2\rtimes \mathbb{R}^+$-CNNs on CIFAR10 for different sizes hidden layers in the SIREN parameterisation.}
  \label{fig:siren-arch-c10}
\end{figure}


\subsection{Comparing activation functions for continuous convolution kernels} \label{app:activationfunctions}
To assess the impact of our use of SIRENs on the performance of our separable G-CNNs, we perform an additional ablation on rotated MNIST in which we compare with other activation functions. We train separable $\mathrm{SE(2)}$-CNNs with a resolution 8 group elements over $\mathrm{SO(2)}$, and only vary activation functions in the MLP parameterising the convolution kernels. The convolution on $\mathrm{SO(2)}$ is approximated through random sampling.

We look at other activation functions used in parameterising convolution kernels; ReLU \citep{wu2019pointconv}, LeakyReLU, and SiLU \citep{finzi2020generalizing}, see Tab. \ref{tab:activations}. These results conclusively show the power of sine as activation function, one of the reasons we chose to use SIRENs in our work.

\begin{table}[]
    \centering
    \caption{Test accuracy (\%) of $\mathrm{SE(2)}$-CNNs on rotated MNIST for different activation functions used in parameterisation of the convolution kernel.}
\label{tab:activations}
    \begin{small}
    \begin{tabular}{ccc}
    \toprule
        \sc{Activation} & \sc{Separable} & \sc{Test accuracy} \\
        \midrule
        \multirow{2}{*}{$\mathrm{Sine}$} & \xmark & 0.9855 $(\pm .0012)$ \\
       &\cmark& \textbf{0.9906 $(\pm .0002)$} \\
        \cmidrule{1-3}
        \multirow{2}{*}{$\mathrm{ReLU}$} & \xmark & 0.9721 $(\pm .0003)$ \\
       &\cmark& 0.9743 $(\pm .0034)$\\
       \cmidrule{1-3}
        \multirow{2}{*}{$\mathrm{LeakyReLU}$} & \xmark &  0.9722 $(\pm .0052)$   \\
       &\cmark& 0.9788 $(\pm .0013)$ \\
       \cmidrule{1-3}
        \multirow{2}{*}{$\mathrm{Swish}$} & \xmark & 0.9651 $(\pm .0012)$ \\
       &\cmark& 0.9595 $(\pm .0045)$\\
        \bottomrule
        \end{tabular}
        \end{small}
\end{table}

\subsection{Random sampling over non-compact groups}
As explained in Sec. \ref{sec:sep-ckgconvs}, for non-compact groups we approximate the group convolution through a discretisation of the transformation group of interest. Motivation for this decision is the fact that in non-compact groups, we have to deal with boundary effects of truncating the group, introducing equivariance error into the convolution operation. For dilation, further motivation for approximation through discretisation is the loss of information that occurs in for example natural images, as a result of downscaling a signal on a fixed resolution grid. Random sampling over the dilation group would have as effect that the representation built by a group convolution layer contains different spatial resolutions of information at every sampling step. This results in subsequent layers receiving noisy information at each iteration, which would impede model performance, with training being highly unstable for low resolutions over $\R^+$.

To empirically reinforce this hypothesis, we performed the same experiment with our $\R^2 \rtimes \R^+$-CNN on MNIST-scale described in Sec.  \ref{sec:experiments} with random sampling over the dilation group. Results, shown in Fig. \ref{fig:samplingscaledmnist}, highlight that indeed, random sampling over the dilation group leads to significantly worse performance. We also noticed great instability during the training process, seen in the variance of the results achieved with random sampling.

\begin{figure}
\centering
  \includegraphics[width=\linewidth]{figures/mnist-scale-samp.png}
  \caption{Test error (\%) of $\mathbb{R}^2\rtimes \R^+$-CNNs on scaled MNIST for random sampling versus discretisation of the scale group.}
  \label{fig:samplingscaledmnist}
\end{figure}

\subsection{Do separable group convolutions reduce redundancy in G-CNNs?}
\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{figures/redundancy-sep.png}
\end{center}
\vspace{-2mm}
\caption{A set of histograms showing variance along the subgroup axis in separable group convolutions. In contrast to Fig. \ref{fig:redundancy}, here we show the variability of the convolution kernel along the subgroup axis. This may be seen as an inverse measure of redundancy. Here we can see that contrary to in non-separable group convolution kernels, variability along the group axis increases over the training process, indicating that redundancy decreases. Separable group convolutions thus solve the redundancy issues in group convolution kernels.
\vspace{-2mm}}
\label{fig:redundancy-sep}
\end{figure}
As explained, convolving with separable group convolution kernels is analogous to a group convolution with a kernel which shares a reweighting of a single spatial kernel along the group axis. This approach was motivated by redundancy observations in regular group convolution kernels along the group axis, seen in Fig. \ref{fig:redundancy}.

To assess whether the separable group convolution solves these redundancy issues in G-CNNs, we may want to perform a similar test of the variability of our introduced separable implementation along the group axis. Note that, if we were to reconstruct the full group convolution kernel $k$ from $k_H$ and $k_\mathbb{R}^2$ and apply the same PCA variance test as used to obtain Fig. \ref{fig:redundancy}, we would find that each group convolution could be fully explained by the first principal component (corresponding to the reshared spatial kernel). Instead, we measure the variability of our separable kernels by assessing the variance of $k_H$, which lies along this axis. Results are shown in Fig. \ref{fig:redundancy-sep} for the separable $\mathrm{SE(2)}$-CNN trained on Galaxy10 dataset with a resolution of 8 elements on $\mathrm{SO(2)}$. Here, we observe the variance increasing in all layers over the training process. This indicates that indeed, separable group convolutions reduce parameter redundancy in G-CNNs.




\end{document}
