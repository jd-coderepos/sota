\documentclass[letterpaper,10pt,journal,final]{IEEEtran}
\pdfminorversion=4

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[scaled=0.95]{inconsolata}


\usepackage{latexsym,amsmath,amssymb,amsfonts,mathrsfs}
\usepackage{arydshln}   \usepackage{mathtools}
\usepackage{amsthm}
\usepackage{empheq}     

\usepackage{algorithm}
\usepackage{algpseudocode}
\makeatletter
\renewcommand{\ALG@beginalgorithmic}{\small}
\makeatother

\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{comment}
\usepackage{enumitem}  \usepackage[usenames,dvipsnames]{xcolor}
\usepackage{cite}
\usepackage[colorlinks=true,linkcolor=magenta,citecolor=blue,urlcolor=cyan,
            filecolor=red]{hyperref}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem*{criteria}{Test Criterion}
\theoremstyle{remark}
\newtheorem{remark}{Remark}



\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}  

\newcommand{\diag}{\operatorname{diag}}
\newcommand{\blkdiag}{\operatorname{blkdiag}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\im}{\operatorname{Im}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\kvec}{\operatorname{vec}}
\newcommand{\ikvec}{\operatorname{ivec}}
\newcommand{\sinch}{\operatorname{sinch}}
\newcommand{\Log}{\operatorname{Log}}
\newcommand{\eig}{\operatorname{eig}}
\newcommand{\sgn}{\operatorname{sgn}}

\newcommand{\minimize}[1]{\underset{#1}{\operatorname{minimize}}}  \newcommand{\st}{\operatorname{subject\ to}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg\,\!min}\;}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg\,\!max}\;}}
\newcommand{\argmini}[1]{\operatorname{arg}\,\!\underset{#1}{\operatorname{min}}\;}
\newcommand{\argmaxi}[1]{\operatorname{arg}\,\!\underset{#1}{\operatorname{max}}\;}

\newcommand{\mathbff}[1]{\boldsymbol{#1}}

\newcommand{\underopt}[2]{\underset{#2}{\operatorname{#1}}}
\newcommand{\yue}[1]{{\color{red}  {[Zuogong: #1]}}}














 



\title{\LARGE \bf
  System Aliasing in Dynamic Network Reconstruction: \\Issues on Low Sampling Frequencies
}
\author{Zuogong Yue, Johan Thunberg, Lennart Ljung, Ye Yuan and Jorge Gon\c{c}alves\thanks{This work was supported by Fonds National de la Recherche Luxembourg (Ref.~9247977).}\thanks{ Zuogong Yue, Johan Thunberg and Jorge Gon\c{c}alves are with
    Luxembourg Centre for Systems Biomedicine,
    6, avenue du Swing, L-4367 Belvaux, Luxembourg.}\thanks{ Lennart Ljung is with Department of Electrical Engineering, Link\"oping University, Link\"oping, SE-58183, Sweden.}\thanks{ Ye Yuan is with the School of Automation, Huazhong University of Science and Technology, Wuhan, 430073, China.}\thanks{\hspace*{0mm}For correspondence, \href{mailto:johan.thunberg@uni.lu}{\tt johan.thunberg@uni.lu}.}}

\hypersetup{
  pdftitle={
    System Aliasing in Network Reconstruction: Issues on Low Sampling Frequencies},
  pdfauthor={Zuogong YUE},
  pdfcreator={Emacs version 25.1 + AUCTeX version 11.90}}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
  Network reconstruction of dynamical continuous-time (CT) systems is motivated
  by applications in many fields. Due to experimental limitations, especially in
  biology, data could be sampled at low frequencies, leading to significant
  challenges in network inference. We introduce the concept of â€œsystem aliasingâ€
  and characterize the minimal sampling frequency that allows reconstruction of
  CT systems from low sampled data.  A test criterion is also proposed to check
  whether system aliasing is presented.  With no system aliasing, the paper
  provides an algorithm to reconstruct dynamic network from data in the presence
  of noise. In addition, when there is system aliasing we perform studies that
  add additional prior information of the system such as sparsity. This paper
  opens new directions in modelling of network systems where samples have
  significant costs. Such tools are essential to process the available data in
  applications subject to current experimental limitations.
\end{abstract}

\section{Introduction}

Many complex systems can be modeled as networks in applications to reveal and
illustrate interactions between measured variables. A common characteristic of
such networks is sparsity, where each variable in only involved in a few
interactions. Such sparse networks are often presented in systems in nature.  An
example of the latter is the interaction between species such as genes and
proteins in human cells; such interactions can be modeled by stochastic/ordinary
differential equations, e.g. \cite{palsson2011systems}.  Such network models in
biology help to understand, for instance, metabolic pathways, interactions
between DNAs/proteins, and furthermore contribute to pathology of disease
detection on or even clinical treatment to complicate diseases,
e.g. \cite{Bar-Joseph2012}.  Motivated by practical applications,
reconstruction of sparse (Boolean) networks turns to be critical as more
techniques have been available to acquire time-series data.

There has recently been quite some interest in the study of dynamic networks
from different perspectives: network
identifiability\cite{Goncalves2008,Hayden2016a}, network module
identifiability\cite{VandenHof2013}, network inference using discrete-time
approaches\cite{Chiuso2012,Yue2017a}, etc. With regard to network inference, the
factor that distinguishes itself from traditional system
identification\cite{Ljung1998} is the particular request on sparse
structures. To enhance sparsity, there are multiple methods are available:
LASSO\cite{Yuan2006}, iterative reweighted /
algorithms\cite{Candes2008,Chartrand2008}, Sparse Bayesian
Learning\cite{Tipping2001,Wipf2007}, etc.


It deserves to be emphasized that the discrete-time approach for network
inference is valid only if the sampling frequency is high enough, where the
discrete-time model shares the same network structure as the continuous one that
is the physical process (here we assume the dynamical systems evolve in
continuous time in nature).  To use discrete-time methods, one practical rule to
choose sampling frequencies is taking ten times the bandwidth of the underlying,
in this case assumed to be linear, systems\cite{Ljung1998}.  However, in
biological systems, most time-series data are sampled considerably slower than
this empirical frequency, e.g. ``high time-resolution'' time series in
\cite{He2012}, which usually cannot be solved by increasing sampling rates due to
various constraints in biological experiments.

There have been several studies on the identification of continuous-time
systems, e.g. \cite{Ljung2010,Garnier2003}. However, most methods request a high
sampling frequency to guarantee certain simplifications on theoretical
deductions or numerical calculations. Choosing a fairly low sampling frequency
may trigger the problem of ``system aliasing'', that is, multiple
continuous-time systems produce exactly the same output samples, while having
different network structures. To determine physical interconnections, it is
inevitable to resorting to the identification of continuous-time models.  With decrease of sampling frequencies, it becomes particularly
challenging, nearly intractable, in theory and computation to identify sparse
structures of continuous-time models.

In this paper, we first reveal the challenges due to the low sampling frequency
by examples in Section~\ref{sec:syst-alias-sysid} and then present a definition
of \emph{system aliasing}. A Nyquist-Shannon-like sampling theorem is presented
in Section~\ref{sec:no-system-aliasing} to determine the minimal sampling
frequency that avoids the effect of system aliasing.  Section~\ref{sec:methods}
presents an algorithm to reconstruct sparse networks in the case of no system
aliasing using low-sampling-frequency data. The case with system aliases is
discussed in Section~\ref{sec:syst-alias-bound-constraints}, which discusses the
feasibility of exploring the ground truth in theory. The last section,
Section~\ref{sec:simulations}, provides numerical examples to show performance
of the proposed methods.




\section{Problem Formulation}
\label{sec:problem-formulation}

Consider a filtered probability space
, where the
filtration is always assumed to be complete.  Let  be the
-dimensional standard -Brownian motion.
The physical plant/process in our study, as a dynamical
system in continuous time, is modeled by the following stochastic differential
equation

where  is stable,  is
symmetric and positive definitive, the initial  is a
Gaussian random variable with mean  and variance , , and  is interpreted
as \emph{disturbance} on the state variables (or called \emph{process noise}).  The solution
to \eqref{eq:dyna-sys-ss-cont} is an -adapted -dimensional
stochastic process  such that

where  and see \cite{Gall2016} for the definition of
stochastic integral. It is assumed that  are independent. The solution
 is \emph{strong}, that is,  is adapted to

(i.e. the complete -field generated by ; see
\cite{Gall2016} for details).  An input signal  has been
applied to the system, and the output  of the system is observed at the discrete
times ,

where ,
 and  is the sampling period. Here the measurement
noise is not included mainly due to that we have not yet given a definition of
network models (see \cite{Goncalves2008}) from state-space representations with
measurement noises.  The stochastic difference equation that relates the values of
the state variable  in \eqref{eq:dyna-sys-ss-cont} at the sampling
instants\cite[p.~82-85]{Astrom2012}\cite[chap.~2]{Garnier2008} is given by

where

 is the matrix exponential, and the Gaussian i.i.d.  has mean zero
and covariance matrix




The \emph{linear dynamic network} model of \eqref{eq:dyna-sys-ss-cont} is given as

where  and  are ,  and  matrices of
strictly-proper real-rational transfer functions respectively in terms of ,  is
the differential operator , and  is the
Gaussian white noise with zero mean and 
(e.g. see \cite{Hayden2016a}). The model \eqref{eq:dsf-cont} is called
\emph{Dynamical Structure Function} (DSF), firstly proposed in \cite{Goncalves2008}.
The network model defines path diagrams which show the interconnections between the
elements of the output variable.
\begin{definition}[\cite{Yue2017a}]
  \label{def:LTI-dynamic-network}
  Let  be a digraph,
  where the vertex set 
and the arc (directed edge) set  is defined by
  \begin{enumerate}[label=\roman*),itemindent=8pt]
  \item ,
  \item ,
  \item .
  \end{enumerate}
  Let  be a map defined as
  
  where  is a subset of single-input-single-output (SISO) (strictly) proper real rational transfer functions.
  We call the tuple  a (linear) \emph{dynamic network},   the (linear) \emph{dynamic capacity function} of , and  the \emph{underlying digraph} of , which is also called (linear) \emph{Boolean dynamic network}.
\end{definition}

This article focuses on the full-state measurement case, i.e. , where  in
\eqref{eq:dyna-sys-ss-cont} coincides with  in network models
\eqref{eq:dsf-cont}.
Concerning the network identifiability\cite{Goncalves2008}, we assume  to be
diagonal\footnote{This is not required in system identification, in which only the
  input-output behavior is concerned. Any state-space realization could be feasible
  solutions.} (DSF, \cite{Goncalves2008}) or particularly  (the model used in
\cite{VandenHof2013}).  Let the measurement be denoted by

We summarize the main problem in our study as follows:

\smallskip
\noindent\textbf{Main Problem}:
Given the finite signal  in full-state measurement (i.e. ), with probably
large  (the sampling period) and small  (the length of time series), infer the
dynamic network  (or the Boolean ), assuming that the
ground truth  is sparse and and  is diagonal.
\medskip

\begin{remark}
  \label{rmk:prob-hard-two-reasons}
  The problem is challenging due to the following two major reasons:
  \begin{itemize}
  \item Since  could be large, i.e. the sampling frequency is low, we have to
    estimate  in order to determine  or  (see
    Figure~\ref{fig:net-topol-A-Ad}).
  \item Since  does not approach infinity, the estimation of  from PEM
    (Prediction Error Minimization) or ML (Maximum Likelihood) may fail to
    identify  correctly by taking matrix logarithms, even though
    PEM/ML gives consistent estimation in theory.
\end{itemize}
\end{remark}

Throughout the text, by default, we always deal with \emph{primary} matrix functions,
including  (matrix exponential),  (matrix logarithm) and  (the
principal matrix logarithm in Theorem~\ref{thm:log-uniq-matrix}). \emph{Primary}
matrix functions refer to the ones defined via \emph{Jordan Canonical Form} or
equivalently via \emph{Polynomial Interpolation}, \emph{Cauchy Integral Theorem}
\cite[chap.~1]{Higham2008}. The \emph{primary} notion of matrix functions is of
particular interest and the most useful in applications\cite{Higham2008,Horn2003}.



\section{System Aliasing in Identification}
\label{sec:syst-alias-sysid}

\subsection{Observations on matrix logarithm}
\label{subsec:ambig-matrix-log}

Supposing that  has been perfectly estimated from samples, the estimate of the
 matrix for the continuous-time system is straightforwardly calculated by solving

via matrix logarithm.  However, referring to Theorem~\ref{thm:matr-logar-Gantmacher}
\cite{Higham2008}, the equation \eqref{eq:exp-hA-eq-Ad} has several (in fact infititely many)
solutions. Let us review the following observations on \eqref{eq:exp-hA-eq-Ad} to see
the troubles from low sampling frequencies (i.e. ).


\smallskip
\noindent\underline{Observation 1}: With the increase of , the Boolean structures of
 (i.e.  determined from ) and  become more and more different, as illustrated by Figure~\ref{fig:net-topol-A-Ad}.
  The sampling frequency () deserves to be emphasized as a core factor in the
  categorization of different cases in our study:
  \begin{itemize}
  \item {Case~I}: when  is ``very small'' such that  shares the same Boolean
    structure as . Indeed, one can see it by
    . Hence we can determine
     by identifying discrete-time models;
  \item {Case~II}: when  is ``large'' but the ground truth  is
    still the principle matrix logarithm of ;
  \item {Case~III}: when  is ``even larger'' such that the ground truth  is no
    longer the principle logarithm of .
  \end{itemize}
The general network model \eqref{eq:dsf-cont} of {Case~I} has been solved by
  discrete-time approaches , e.g. see \cite{Yue2017a,Chiuso2012}.  {Case~II}
  is what we mainly studied in this paper. We call both {Case~I} and
  {II} \emph{no system aliasing}, as defined and studied in later sections.


\begin{figure}[htb]   \centering
  \includegraphics[width=.3\textwidth]{m-scripts/sysid-expmA}
  \caption{This example randomly chooses a sparse  and . The
    Boolean networks are determined from  and  by
    Definition~\ref{def:LTI-dynamic-network} (treating  and  as 's),
    denoted by . We use  as the
    ground truth. Then  is compared with  and
    the same arcs are labeled as the correct ones.}
  \label{fig:net-topol-A-Ad}
\end{figure}

\smallskip
\noindent\underline{Observation 2}: Provided with a sparse , the corresponding 
could be dense, as the example in Figure~\ref{fig:logm-eg-1}.
\begin{figure}[htb]
  \centering
  \includegraphics[width=.42\textwidth]{m-scripts/eg1-logm}
    \caption{, , . }
    \label{fig:logm-eg-1}
\end{figure}

\smallskip
\noindent\underline{Observation 3}: Provided with a sparse , the corresponding 
can be also sparse. However, they have different Boolean structures (i.e. zeros at
different positions), as shown in Figure~\ref{fig:logm-eg-2}.
\begin{figure}[htb]
  \centering
    \includegraphics[width=.42\textwidth]{m-scripts/eg2-logm}
    \caption{, , . }
    \label{fig:logm-eg-2}
\end{figure}

\smallskip
\noindent\underline{Observation 4}: Even though the norm difference of  has been
very small, their matrix logarithms, e.g.  and  in
Figure~\ref{fig:logm-eg-4}, have significantly different Boolean structures.
\begin{figure}[htb]
  \centering
    \includegraphics[width=0.48\textwidth]{m-scripts/eg4-logm}
    \caption{,  and .}
    \label{fig:logm-eg-4}
\end{figure}

\smallskip
\noindent\underline{Observation 5}: There exists more than one solution, e.g.  and
 in Figure~\ref{fig:logm-eg-3}, and they have different Boolean structures.
\begin{figure}[htb]
  \centering
    \includegraphics[width=0.48\textwidth]{m-scripts/eg3-logm}
    \caption{, , , and
       is the ground truth . }
    \label{fig:logm-eg-3}
\end{figure}

Considering the problem formulation in Section~\ref{sec:problem-formulation}, one may
have already noticed the troubles on network reconstruction, which originate from the
matrix logarithms, due to the low sampling frequency.  The examples in
{Observation~1} clearly show that why we have to resort to the continuous-time
system identification to infer network structures. {Observation~2} and
{3} tell that there is no consistent relation between the sparsity of  and
.  {Observation~4} points out that the Boolean structures of the
principle logarithms of two  and  close in matrix norms could be
significantly different.  The example on {Observation~5} shows an even worse
case: the sample period is so large that the principle matrix logarithm is no longer
, which appears as other branches of matrix logarithm of , in which no robust
algorithm has yet been available.

\begin{remark}
  In a sum of the above observations, it tells us that, in the identification of  or ,
  \begin{itemize}
  \item   should be determined from  instead of , when  is
    large (w.r.t. ); (see {Observation~1,~4})
  \item  the sparsity penalty has to be imposed on  directly instead of ,
    when  is large (w.r.t. ); (see {Observation~2,~3,~4})
  \item  the  matrix should be estimated directly instead of via taking matrix
    logarithm of , in the presence of noise and a limited length of
    signals. (see {Observation~4})
  \end{itemize}
\end{remark}


\subsection{Definitions}
\label{subsec:definitions}

As shown in Section~\ref{subsec:ambig-matrix-log}, the -matrix has to be
identified in network reconstruction when the sampling frequency is low. In this
scenario, a ``good'' case is that the ground truth  stays as the principle
matrix logarithm of  (i.e. {Case~II}); otherwise, it becomes particularly
challenging (i.e. {Case~III}), e.g. Figure~\ref{fig:logm-eg-3}.  To clarify this
classification, we consequently present an important concept in network
reconstruction with low sampling frequencies, ``\emph{system aliasing}''.

Let  denote the vectorization of the matrix  formed by stacking the
columns of  into a single column vector; and  is defined by
.  denotes the imaginary part of the complex
number or vector .
\begin{definition}
  \label{def:E-for-sys-alias}

  
  where  contains .
\end{definition}

With this general notation, we present a definition of \emph{system aliasing}
only in terms of the  matrix in state-space representations and the sampling
period , which does not depend on specific identification methods or data.
Before presenting the concept of system aliasing, we have to assume no loss of
information of input signals during sampling, e.g. no inputs, or the continuous
input signal can be determined by input samples together with, for instance, the
zero-order holder. Otherwise, we have to include constraints of input signals in
our definition, which has not yet been studied.

\begin{definition}[System aliasing]
  \label{def:system-aliasing}

  Given  and , if there exists
   and  is called
  \emph{system alias} of  with respect to .  By default, we
  choose
  .
\end{definition}

We are particularly interested in ,
i.e. there is no issue of \emph{system aliasing}.  Note that the concept of
\emph{system aliasing} does not depend on specific data. It only depends on
system dynamics (e.g. the -matrix in \eqref{eq:dyna-sys-ss-cont}) and
sampling frequencies.  If the  matrix is specifically constructed by data
instead of , , where  denotes
the ground truth, tells that the underlying system is identifiable from the
given data (see \cite[Sec.~III-B]{Yue2016b}).  Obviously if we have {system
  aliasing} for the system with a specific sampling frequency, without extra
prior information on , the system is always not identifiable.





\section{No system aliasing: the minimal sampling frequency}
\label{sec:no-system-aliasing}



Provided with the definition of \emph{system aliasing}, a question comes first: what  satisfies . To answer this question, we need to introduce a theorem on matrix logarithm.
\begin{theorem}[principal logarithm {\protect \cite[Thm.~1.31]{Higham2008}}]
  \label{thm:log-uniq-matrix}
  Let  have no eigenvalues on . There is a unique logarithm  of P all of whose eigenvalues lie in the strip . We refer to  as the principal logarithm of  of write . If  is real then its principal logarithm is real.
\end{theorem}

To make the principal matrix logarithm  be well-defined, we always assume
that  has no negative real eigenvalues.  Let
.
By Theorem~\ref{thm:log-uniq-matrix} and \ref{thm:matr-logar-Gantmacher}, it always
holds that . To avoid
\emph{system aliasing}, it implies that  , i.e.
. It is summarized as the following lemma.

\begin{lemma}
  \label{lemma:A=Ahat}
  Let , which has no negative real eigenvalues,
  and . Then 
  (i.e. ) if and only if
  .
\end{lemma}

Given no other information on the system, consider the identification problem of 
using full-state measurement. It is necessary to decrease the sampling period 
until the ground truth falls into the strip of , and then the
principal logarithm refers to the ground truth , as illustrated in
Figure~\ref{fig:h-vs-eigv-identifi}. Otherwise, we would be bothered by \emph{system
  aliases} of  and be unable to make a decision, unless we know extra prior
information on .

\begin{figure}[htb]
 \centering
 \includegraphics[width=.4\textwidth]{figures/identifiability-eignv-h}
 \caption{The imaginary parts of all eigenvalues of  must lie into .  denotes the -th eigenvalue of  in Theorem~\ref{thm:matr-logar-Gantmacher}. The symbols ``{\tiny}'' denote the locations of .  is the maximal sampling period that allows taking principal logarithms to estimate , without facing troubles from system aliasing.}
 \label{fig:h-vs-eigv-identifi}
\end{figure}


\begin{theorem}[Nyquist-Shannon-like sampling theorem]
  \label{thm:identifi-largest-sampl-period}
  Considering equidistant sampling, to uniquely reconstruct the continuous-time
  system  from the corresponding discrete-time system  by taking the
  principal matrix logarithm, the sampling frequency  \textup{(rad/s)}
  must satisfy
  
  Equivalently, the sampling period  (i.e. ) should satisfy
  
\end{theorem}

\begin{proof}
  The result immediately follows by verifying the condition  in Lemma~\ref{lemma:A=Ahat}.
\end{proof}

Theorem~\ref{thm:identifi-largest-sampl-period} in continuous-time system
identification can be understood by analogy with the \emph{Nyquist-Shannon
  sampling theorem} in signal processing. The \emph{Nyquist-Shannon sampling
  theorem} gives conditions on sampling frequencies, by looking at spectral
information of signals, under which continuous signals can be uniquely
reconstructed from their discrete-time signals. As an analogy,
Theorem~\ref{thm:identifi-largest-sampl-period} addresses that continuous-time
LTI systems can be uniquely reconstructed from their discrete-time systems under
a condition that is built based on the spectral information of the  matrix.

Now we would like to show a property of matrix exponential and logarithm, which
further leads to a test criterion on system aliasing.  See
Appendix~\ref{appdix:test-criteria-sys-alias} for the proofs of
Lemma~\ref{lemma:diff-A-h1h2} and
Proposition~\ref{prop:pred-error-diff-distribution}.
\begin{lemma}
  \label{lemma:diff-A-h1h2}
  Considering  and , let
   be defined by . Then  if and only if  or .
\end{lemma}

\begin{proposition}
  \label{prop:pred-error-diff-distribution}
  Consider the dynamical system \eqref{eq:dyna-sys-ss-cont} without inputs (i.e.
  ), and two sampling periods  such that
  .  Let  and
  .  The one-step prediction errors w.r.t.  are defined as
  
  Assuming that , it yields
  
\end{proposition}



We have similar results for the case with inputs, as stated in
Proposition~\ref{prop:pred-error-diff-distribution-B} in
Appendix~\ref{appdix:test-criteria-sys-alias}, where we no longer require
 due to the benefits from inputs. Meanwhile, according to
the condition \eqref{eq:pred-err-diff-0-B-condition}, it is possible that a carefully
designed input signal invalidates the test criterion that is built by evaluating
, which in practice may not be a problem.
The results in Proposition~\ref{prop:pred-error-diff-distribution} and
Proposition~\ref{prop:pred-error-diff-distribution-B} can be understood by
Figure~\ref{fig:samples-diff-h1h2}, where the output prediction of  (that is
estimated from samples in ) presents different values from that of  in
another sampling period  and it results in that the expectation of one-step
prediction errors is no longer zero.  The test criterion on system aliasing is
summarized as follows:
\begin{figure}[htb]   \centering
  \includegraphics[width=.45\textwidth]{m-scripts/samples-h1h2}
  \caption{System responses of the dynamical system \eqref{eq:dyna-sys-ss-cont} with
    , where , and the input
    signal is the square wave with period . The dots in deep
    blue are samples with the sampling period .}
  \label{fig:samples-diff-h1h2}
\end{figure}

\begin{criteria}[system aliasing]
  Identify  by PEM or ML (denoting the estimates by )
  assuming no system aliasing under the sampling period , i.e. 
  asymptotically converges to .  Choose another sample period 
  such that , and sample by  the system responses
  with non-zero initial conditions or non-zero inputs (assuming
  \eqref{eq:pred-err-diff-0-B-condition} is satisfied). Use  to
  calculate the one-step prediction errors . Perform \emph{t-test} to
  obtain the \emph{p-value} to make decisions, where the null hypothesis is that
   comes from a normal distribution with mean zero and unknown
  variance. Rejecting the null hypothesis implies the existence of system aliasing.
\end{criteria}

\begin{figure}[htbp]   \centering
  \includegraphics[width=.5\textwidth]{figures/chartflow-test-criteria}
  \caption{A chart flow of the test criterion on system aliasing.}
  \label{fig:chartflow-test-criteria}
\end{figure}

\section{No system aliasing: sparse network reconstruction}
\label{sec:methods}


Considering the systems given by ~\eqref{eq:dyna-sys-ss-cont} and
\eqref{eq:dyna-sys-ss-output},
the likelihood function is determined by the multiplication rule for conditional
probability\cite{Astrom1980}

where  denotes the parameters under estimation, which parameterizes
.
With the assumption that  are jointly Gaussian,
the negative logarithmic likelihood function is

where ,
 denotes the conditional mean of , and
 the corresponding covariance matrix.  The optimal prediction of
 (i.e. ) is obtained using Kalman filters
(e.g. \cite{Astrom1980,Ljung2010}),
.5mm]
    \hat{x}(t_k | t_k) &= \hat{x}(t_k|t_{k-1}) + K(t_k) \epsilon(t_k) \\
    \displaystyle\frac{\mathrm{d}}{\mathrm{d}t} \hat{x}(t|t_k) &= A \hat{x}(t|t_k) +
              B u(t), \quad t_k \leq t \leq t_{k+1}\\
K(t_k) &= P(t_k|t_{k-1}) C^T \Lambda^{-1}(t_k) \
where the initial condition is .
Considering the equidistant sampling and assuming the input is constant over the
sampling periods, the matrix  and  appears in
\eqref{eq:pred-Kalman-filter} can be treated as constant matrices by using
steady-state Kalman filtering\cite[Sec.~3.6]{Astrom1980}.



Now consider the full-state measurement case (i.e. ) and restrict the noise to
process noise. The calculation of prediction  becomes
particularly simple since  in \eqref{eq:pred-Kalman-filter} always equals the
identity, which yields

where  are defined via  in \eqref{eq:formula-Ad-Bd}, \eqref{eq:disc-time-sys-wn-cov}.
Here we resolve  by using ,
where  is treated to be the deterministic and hence is removed from the
conditional variables, and takes the first sample as its value. This
simplification is due to the fact that  and the
measurement of  is available (using ), which also leads to that the
best estimation of the distribution of  is nothing better than a delta
function (even if including the probability assumption of  in maximum
likelihood, i.e.
 and
 takes the Gaussian density with mean  and covariance
matrix ). Alternatively, the likelihood function \eqref{eq:likelihood-func}
can be obtained directly by considering \eqref{eq:dyna-sys-ss-discr} without
using Kalman filtering. However, the above standard procedure values when we
deal with general cases (i.e. ).  Noticing the particular
parameterization \cite[p.~92,~206]{Ljung1998}, maximizing likelihood can be
performed as follows\footnote{Similar to \cite[p.~219]{Ljung1998}, one instead
  firstly minimizes the cost function analytically with respect to 
  for every fixed . Due to the particular parameterization, the resultant
  optimization no longer depends on .}:

    \label{eq:ML-2steps-PEM}
    \hat{\theta} &= \textstyle\operatorname{argmin}_{\theta} \sum_{k=1}^N \epsilon(t_k,\theta)^T \epsilon(t_k,\theta),\\
    \label{eq:ML-2steps-R}
    \hat{R}_d &= \textstyle\frac{1}{N} \sum_{k=1}^N
\epsilon(t_k,\hat{\theta}) \epsilon^T(t_k, \hat{\theta}),
  
where  is composed of .
To estimate , instead of minimizing the prediction error as
\eqref{eq:ML-2steps-PEM}, we impose the -penalty to favor the sparse
solution in network reconstruction. This is due to the observations in
Section~\ref{subsec:ambig-matrix-log}: the consistency of ML may fail to present
us with a correct network structure unless appropriate thresholds of zero for
each row of  are selected, which is hardly implemented in practice.

\begin{remark}
  \label{rmk:challenge-from-measurement-noise}
  If we include measurement noise\footnote{Assume that it is reasonable to
    determine the network by  similarly, even though we don't have network
    models well-defined from the state-space representations with measurement
    noise.}  or consider the output measurement case , the prediction
  includes the Kalman filter gain , which depends on  and the
  covariance of measurement noise.  It deserves to be emphasized that, due to
  the possible large sampling periods , the numerical tricks used in
  \cite{Astrom1980,Ljung2010} may no longer be valid to compute the gradient of
  the prediction error. We have to analytically calculate the gradient as far as
  possible until the numerical computation is no longer restricted by . This
  problem becomes fairly complicated.
\end{remark}



\subsection{The cost function in matrix forms and the gradients}
\label{sec:pred-errors-mat-gradient}

The reconstruction algorithm is supposed to infer a sparse network, i.e.  is
sparse.  Due to the nonlinear least-square cost function \eqref{eq:ML-2steps-PEM}, it
no longer satisfies the setup of Sparse Bayesian Learning proposed in
\cite{Tipping2001}. Here we enhance sparsity by heuristically imposing the -norm
of  as the penalty to the PEM cost function as the first tentative treatment.

Considering the measurement signal , let

where .
The matrix form of the -regularised PEM problem is formulated as

where  and  is the fixed and known
sampling period, and the  norm 
( denotes the -th element of ).  To avoid dealing with tensors, we
use the vectorized form of (\ref{eq:optim-probl-type-2}) as follows:

where , ,
, and  is the
\emph{Kronecker product}.

The problem \eqref{eq:optim-probl-type-2} is challenging in optimization by noticing
that it is: non-convex due to matrix exponential; not globally Lipschitz; and
non-differentiable. The intuitive idea here is to use the the Gauss-Newton framework,
in which each iteration is to solve a constrained -regularized linear least
square problem. Let

, and , which is the objective function of
\eqref{eq:optim-probl-kron-type-2}. Then  denotes the problem
\eqref{eq:optim-probl-kron-type-2} without -penalisation. The gradient of
 w.r.t.  is

where

and  is defined in Theorem~\ref{thm:kron-repr-frechet-deriv}. The function
 is integrable by noticing 
( denotes any matrix norm). The matrix function  and the
integration can be calculated numerically given .  To compute the gradient of
 w.r.t. , using the matrix identity 
again for the term of  in \eqref{eq:optim-probl-type-2}, it yields

Then it follows that

where

If we assume  is diagonal and  is calculated w.r.t. each diagonal
element of , then ,
where  is an  identity matrix and  is an
-dimensional row vector of 1's.  In a sum, the gradient of  is





\subsection{A special case: update A with fixed }
\label{subsec:special-case:-update-A}

The \emph{subspace method} in system identification presents us with nice
initial estimation of  (e.g. see \cite{Viberg2002}). Concerning the
task of network reconstruction, we would like to infer a sparse  from
data. As a special case, we only update  by solving
\eqref{eq:optim-probl-type-2} with  fixed to be . For simplicity, in
this subsection, let
 and .

A linear approximation of  in a neighbourhood of a given point  is

One may then use this approximation and formulate a -regularized linear least squares problem

which can be solved to obtain an approximate solution to \eqref{eq:optim-probl-kron-type-2}.
Resolving it in an iterative way amounts to a Gauss-Newton method. However,  is not necessary to be a \emph{descent direction} of \eqref{eq:optim-probl-kron-type-2}.

In the -th iteration, to guarantee the step  being a descent direction of \eqref{eq:optim-probl-kron-type-2}, the search direction  is instead computed from the following constrained optimization problem

where  denotes the subdifferential of  at , defined as


in which  denotes the -th element of .
One may have noticed that the constraint in the problem  is the definition of \emph{descent direction} for  at , except replacing  with  to guarantee the existence of minimum. The problem  is a convex optimization problem by noticing that  is a convex function, which is a \emph{pointwise supremum} over an infinite set of a linear function \cite[chap.~3]{Boyd2004}.
To solve the problem , we need to explore the constraint and derive an equivalent form (see Appendix~\ref{appdix:derivation-Pprim-P} for details), given as follows.

where

the identify matrix  is of a compatible dimension,  denotes the element-wise absolute value,  denotes the diagonal matrix built from vector , and the  function for vectors and matrices is extended from the standard signum function for real numbers, defined as follows: when ,  denotes a -dimensional vector whose -th element equals ; and when , .
Now the problem  can be easily modeled using CVX in MATLAB and solved by
standard optimization solvers \cite{cvx-manual}.

The iterate is updated via

where the step length  is determined by \emph{backtracking line search}.
Let  denote the \emph{directional derivative} of  at  in the direction , by subcalculus,
4pt]
                 &= \bar{g}(A_k)^T p_k + \lambda \|W(A_k) p_k\|_1.
  \end{array}

  \label{eq:line-search}
  f \big(A_k + \ikvec(s_kp_k)\big) \leq f(A_k) + \alpha s_k f'(A_k; p_k).
r(A_{k}), J(A_{k}), \nabla \phi(A_{k})\bar{g}(A_k), W(A_k)p_kP')f'(A_k; p_k)s_kA_{k}J(A_k)
\hspace*{-1.3mm}
    \scalebox{0.9}{}
  
  (P_2) \left\{
  \begin{array}{@{\,}l@{\;\;}l}
    \minimize{p_k \in \mathbb{R}^{2n^2}} &\|r(\theta_k) + J(\theta_k) p_k\|_2^2 + \lambda
                                           \|\kvec(A_k) + \Lambda p_k\|_1,\\
    \st & \sup_{g \in \partial f(\theta_k)} g^T p_k \leq 0,
  \end{array}
  \right.

  \partial f(\theta_k) \coloneqq \{ \nabla \phi(\theta_k) + \lambda \Lambda^T z : z \in J_1 \times \cdots J_{n^2} \},

  (P_2') \left\{
  \begin{array}{@{\,}l@{\;\;}l}
    \minimize{p_k \in \mathbb{R}^{2n^2}} &\|r(\theta_k) + J(\theta_k) p_k\|_2^2 + \lambda
                                           \|\kvec(A_k) + \Lambda p_k\|_1,\\
    \st & \bar{g}^T p_k + \lambda \|W(A_k) \Lambda p_k\|_1 \leq 0,
  \end{array}
  \right.

  \label{eq:sparsity-multiple-logm-bounded}
  \minimize{\hat{A} \in \mathscr{E}(A, I, h, \mathscr{S}_\kappa)}  \|\hat{A}\|_0.
  \vspace*{-1.5mm}

  \label{eq:def-set-S}
  \mathscr{E}(A, I, h, \mathscr{S}_\kappa) = \big\{\tilde{A} \in \mathscr{S}_\kappa: \exp(h \tilde{A}) = A_d \big\},

  \|h_Z(\hat{A})\|_F = \text{vec}(\hat{A})^T(Z^T \otimes Z^{-1})^T(Z^T \otimes Z^{-1})\text{vec}(\hat{A})

  \label{eq:norm-bounded-S}
  \mathscr{S}_\kappa = \big\{ \tilde{A} \in \mathbb{R}^{n \times n}: \|h_Z(\tilde{A})\|_F \leq \kappa \big\}.

  \mathscr{I}(\textit{j}, \delta) \coloneqq \delta^T M \delta + (2\textit{j} + \beta)^T M \delta,
  \label{eq:func-def-dist-Fro-Aij}
{1}
  \mathscr{I}(\textit{j}, \delta) = \left(\delta + \textit{j} + {\beta}/{2}\right)^T & M \left(\delta + \textit{j} + {\beta}/{2}\right) \nonumber\\
                       - \left(\textit{j} + {\beta}/{2}\right)^T & M \left(\textit{j} + {\beta}/{2}\right).
                       \label{eq:func-def-dist-Fro-Aij-compl-square-form}

    \label{eq:def-set-cal-S}
    \mathcal{S} \coloneqq \big\{\tilde{A} \in \mathbb{R}^{n \times n}: \exp(h \tilde{A}) = A_d \big\}.
  
    \mathscr{I}(\textit{j}^{(1)}, \textit{j}^{(2)} - \textit{j}^{(1)}) = 0.
  
    \left| \| h_Z(A) \|_F - \| h_Z(\bar{A}) \|_F \right| \geq M.
  
  \kappa_l &> \max\{0, \|h_Z(\bar{A})\|_F - M(\bar{A}) \},\\
  \kappa_u &< \|h_Z(\bar{A})\|_F + M(\bar{A}),
  
    \label{eq:jordan-form-matrix}
    
  
    \label{eq:mat-log-all-sols-main}
    A = Z U \diag(L_1^{j_1}, L_2^{j_2}, ..., L_p^{j_p}) U^{-1} Z^{-1},
  
    \label{eq:mat-log-all-sols}
    L_k^{j_k} = \log(J_k(\lambda_k)) + 2 j_k \pi i I_{m_k};
  
    f(J_k) \coloneqq
    \begin{bmatrix}
      f(\lambda_k) & f'(\lambda_k) & \cdots & \frac{f^{(m_k-1)}(\lambda_k)}{(m_k - 1)!} \\
      & f(\lambda_k) & \ddots & \vdots \\
      & & \ddots & f'(\lambda_k) \\
      & & & f(\lambda_k)
    \end{bmatrix}
  
    \label{eq:logm-classifiction-primary}
    A_j = Z \diag(L_1^{j_1}, L_2^{(j_2)}, ..., L_p^{(j_p)}) Z^{-1},
  
    \label{eq:logm-classification-nonprimary}
    A_j(U) = ZU \diag(L_1^{j_1}, L_2^{(j_2)}, ..., L_p^{(j_p)}) U^{-1}Z^{-1},
  f: \mathbb{C}^{n\times n} \rightarrow \mathbb{C}^{n \times n} X\in \mathbb{C}^{(n \times n)}
    \mathbb{C}^{n \times n} &\overset{L}{\longrightarrow} \mathbb{C}^{n \times n}\\
    E &\longmapsto L(A, E)
   f(A+E) - f(A) - L(A, E) = o(\| E \|). 
L_{\mathrm{exp}}(X,E) = \int_0^1e^{X(1-s)}Ee^{Xs}ds,

  \label{eq:exp-At-Frechet-deriv}
  e^{h A} = e^{h({A}_c + E)} = e^{h{A}_c} + L(h{A}_c, hE) + O(\|hE\|^2).

L_{\mathrm{Log}}(X,E) = \int_0^1(t(X-I) + I)^{-1}E(t(X-I)+I)^{-1})dt,

    \label{eq:vec-Frechet-deriv-exp}
    
   \frac{1}{2} \| A^T \oplus (-A) \| < \pi /2  (A \oplus B) = A\otimes I_n + I_m \otimes B, 
    \label{eq:pred-err-diff-0-B-condition}
    \begin{array}{l@{}l}
      \mathbb{E}\Big(
      &\big( \exp(h_2 A) - \exp(h_2 \hat{A}) \big) x(t_k)\ + \\
      &\displaystyle\int_0^{h_2} \big( \exp(sA)B -
      \exp(s\hat{A})\hat{B} \big) \mathrm{d}s\, u(t_k) \Big) \neq 0,
    \end{array}
  
    \mathbb{E}(\epsilon(t_k)) = 0, \quad
    \mathbb{E}(\hat{\epsilon}(t_k)) \neq 0.
  
  \label{eq:pred-err-diff-0-B-condition-special}
  \begin{array}{@{}l@{}l}
    &\big( \exp(h_2 \hat{A}) -I \big)
      \big( \exp(h_1 \hat{A}) -I \big)^{-1}
      \big( \exp(h_1 {A}) -I \big) \\
    &\neq \exp(h_2 A) - I.
  \end{array}

    \begin{array}{l@{\,}l@{\,}l@{}l}
      h_1A &= Z \diag(L_1^{j_1}, &\dots, L_p^{j_p}&) Z^{-1}, \\
      h_1\hat{A} &= Z \diag(L_1^{0}, &\dots, L_p^{0}&) Z^{-1}.
    \end{array}
  
    \begin{array}{l@{}l}
      &|\hat{\mu}_k' I_k - L_k^0| = |\hat{\mu}_k' I_k - \log(J_k(\lambda_k))| \\
      &= \Big| \hat{\mu}_k' I_k -
        \begin{bmatrix}
          \log(\lambda_k) & \log'(\lambda_k) & \cdots & * \\
          & \log(\lambda_k) & \ddots & \vdots \\
          & & \ddots & \log'(\lambda_k)\\
          & & & \log(\lambda_k)
        \end{bmatrix}
                \Big| \\
      &= \left( \hat{\mu}_k' - \log(\lambda_k) \right)^{m_k} = 0,
    \end{array}
  
    \begin{array}{l@{\,}l}
      \diag(L_1^0, \dots, L_p^0) &= U \diag(J_1(\hat{\mu}_1), \dots,
                                   J_p(\hat{\mu}_p)) U^{-1}, \
  where  and  denote the corresponding Jordan blocks.
  Therefore,  is equivalent to  for any , which implies . It leads to the conditions: , or  (i.e. ).
\end{proof}


\subsection{Proof of Proposition~\ref{prop:pred-error-diff-distribution}}
\label{appdix:subsec:proof-prop-pred-err-diff-dist}

\begin{proof}
  Considering the dynamical system \eqref{eq:dyna-sys-ss-cont}, it is obvious that
  . Now we evaluate the other expectation
  
  by Lemma~\ref{lemma:diff-A-h1h2}.
\end{proof}



\section{More details on the optimization problems}
\label{appdix:derivation-Pprim-P}

\subsection{Equivalent forms of }
\label{appdix:subsec:equivalent-forms-P1-P2}

The following shows how to derive  from . Consider  in , which implies . Recall that  where 
is an -dimensional vector. Then we have

where  and .

Consider the optimization , we go through the same procedure and obtain

where .

\subsection{Proof of Proposition~\ref{prop:no-zero-p-before-local-optima}}
\label{appdix:subsec:proofs-proposition-nonzero}

\begin{proof}
  Without loss of generality, suppose that there exists  and
   such that
   and .
  (Indeed, if , we apply
  Proposition~\ref{prop:zero-p-guarantee-local-optima} and obtain
  .)
  It implies that
  .
  Hence,  for small enough .
  Let  be defined by
  
  and hence 
  ( denotes any vector norm).  For simplicity, without any ambiguity,
  we use  to represent .  Then we have
  , which yields
  
  Now let us calculate this limit in a different way. Noting that  (since ) and
   (since ), we have
  
  Moreover, since , there exists  such that
  
  Now we recalculate the limit in \eqref{eq:small-o-limit} as follows
  
  which contradicts with \eqref{eq:small-o-limit}.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:zero-p-guarantee-local-optima}}
\label{appdix:subsec:proof-proposition-zero}

\begin{proof}
  Since , it yields
  . Note that, in our discussion,  is
  always fixed, and thus  denotes the subgradient of
   at . Now let us write 
  explicitly
  
  where
  
   denotes the -th element of ,
  and
  . Hence .
  Therefore,  with  implies .
\end{proof}



\section{Proofs for boundness of system aliases}
\label{appdix:proof-lemmas}

\subsection{Proof of Lemma~\ref{lemma:logm-As-condition-equality}}
\label{appdix:proof-lemma-1}

\begin{proof}
  Let 
, where , , and all other notations are given in \eqref{eq:logm-classifiction-primary}.
  \begin{figure*}[htb]
      
  \end{figure*}
  By using \eqref{eq:Ai-Aj-Frobenius-norm} for , we obtain
  
  which implies that  by definition. The first equality in \eqref{eq:Ai-Aj-Frobenius-norm} is due to the linear transformation .
\end{proof}


\subsection{Proof of Lemma~\ref{lemma:finite-elements-equiv-class}}
\label{appdix:proof-lemma-2}

\begin{proof}
  Let  denote  of  in \eqref{eq:mat-log-all-sols}, and  denotes  of . , therefore ,
where  denote the element-wise larger-or-equal relation. By Definition~\ref{def:equivalence-relation}, it is equivalent to show that  has finite solutions, given . We require  to satisfy the following condition:

for all .
Otherwise, supposing that there exists  such that  does not satisfy \eqref{eq:delta-bound-eqiv-class}, we will have

Let .
We have  and  is a finite set.
\end{proof}


\subsection{Proof of Lemma~\ref{lemma:finite-A-Fro-less-k}}
\label{appdix:proof-lemma-3}

\begin{proof}
  Let . Then we need to show there exists a finite number of  such that , which is equivalent to show that there exists a finite number of solutions  to .  must satisfy the following condition:
  
  for all . Otherwise, by supposing that there exists  such that  does not satisfy \eqref{eq:delta-bound-k-upperbound}leads to
  
  Note that the set of all  that satisfies \eqref{eq:delta-bound-k-upperbound} is finite, which finalizes the proof.
\end{proof}

\subsection{Proof of Proposition~\ref{thm:identifi-boundness-A}}
\label{appdix:subsec:proof-proposition-boundedness}

\begin{proof}
  Let  denote  of  in \eqref{eq:mat-log-all-sols},  be the number of 's that satisfy .
  Note that , which implies it is equivalent to show that  has a non-zero lower bound if not considering the 's that result in . We will prove it by contradiction. Assume this is not true, i.e.  there exists  such that . It implies that, arbitrarily given , there exists an infinite number of  such that , which is impossible since  (using the fact that ) has a finite number of solutions provided by Lemma~\ref{lemma:finite-A-Fro-less-k}.
\end{proof}












\bibliographystyle{IEEEtran}
\bibliography{./ref/library,./ref/ref}

\end{document}
