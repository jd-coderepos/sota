


\def\OURS{VFC\xspace}
\vspace{-1cm}
\maketitle
\vspace{-1cm}
\appendix

\part{Appendix} {\hypersetup{linkcolor=black} \parttoc}



\clearpage
\ificcvfinal\thispagestyle{empty}\fi

\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0} 
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{table}{0} 



This appendix to the main paper provides additional quantitative 
(Sec.~\ref{sec:app:exp})
and qualitative results
(Sec.~\ref{sec:app:qualitative}),
and further details on baselines and implementation
(Sec.~\ref{sec:app:details}).

\section{Quantitative results}\label{sec:app:exp}

In this section, we present results comparing standard versus Verb-Focused Constrastive (VFC) learning for all benchmarks (Sec.~\ref{subsec:app:base_vs_vfc}), comparison to state-of-the-art methods for MSR-VTT retrieval (Sec.~\ref{subsec:app:retrieval}), and additional ablations (Sec.~\ref{subsec:app:ablations}). 


\subsection{Standard vs.~Verb-Focus Contrastive (VFC) learning for all benchmarks}\label{subsec:app:base_vs_vfc}
We see in Tab.~\ref{tab:baseline-versus-ours} that our VFC learning performs better than standard contrastive learning (Baseline) for all verb-focused benchmarks on both zero-shot and fine-tuned settings while maintaining performance on more noun-focused benchmarks, such as MSR-VTT random MC. We observe that using the HardNeg-NCE loss, instead of standard NCE, further improves performance for all benchmarks on both zero-shot and fine-tuned settings.



\begin{table}
    \setlength{\tabcolsep}{2pt}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lc|cc|cc|cc|cc}
            \toprule
         &    &\multicolumn{2}{c|}{\textbf{MSR-VTT}} &    \multicolumn{2}{c|}{\textbf{K-400}} &   \multicolumn{2}{c|}{\textbf{NEXT-QA}} &   \multicolumn{2}{c}{\textbf{SVO}} \\
        &    & 3k val. & Verb$_H$ & all & verb & all & ATP$_{hard}$ & all & verb \\
  Method & loss    & MC & MC & top-1 & top-1 & MC & MC & AP & AP \\
\midrule
\textsc{Zero-shot} &\\ 
Baseline & NCE & 94.9 & 69.9  & 55.6 & 52.1 & 48.6 & 28.9 & 60.2 & 61.9 \\
\rowcolor{aliceblue} VFC & NCE & 94.9 & 78.3  & 58.5 & 56.7 & 51.0 & 31.3 & 61.5 & 63.9 \\
\rowcolor{aliceblue} VFC & HardNeg-NCE&  \textbf{95.1} & \textbf{80.5}  & \textbf{58.8} & \textbf{57.1} & \textbf{51.5} & \textbf{31.4} & \textbf{61.8} & \textbf{64.6} \\
\midrule
\midrule
\textsc{Fined-tuned} & \\ 
Baseline  & NCE & \textbf{96.8} & 73.8  & - & - & 57.3 & 37.8 & - & - \\
\rowcolor{aliceblue}  VFC & NCE & 96.2 & 84.8  & - & - & 58.4 & 38.3 & - & - \\
\rowcolor{aliceblue}  VFC & HardNeg-NCE & 96.2 & \textbf{85.2}  & - & - & \textbf{58.6} & \textbf{39.3} & - & - \\
\bottomrule
        \end{tabular}
    }
    \caption{
        \textbf{Standard vs.~Verb-Focus Contrastive learning for all benchmarks.} We report MSR-VTT random (3k val.) and Verb$_H$~\cite{park-etal-2022-exposing} multiple-choice accuracies, Kinetics-400 and Kinetics-verb top-1 accuracies, NEXT-QA and ATP$_{hard}$~\cite{buch2022revisiting} multiple-choice accuracies, and SVO-probes entire dataset and verb-focused Average Precision. We observe that our VFC learning performs better than standard contrastive learning (Baseline) for all verb-focused benchmarks on both zero-shot and fine-tuned settings, while maintaining performance on more noun-focused benchmarks, such as MSR-VTT random MC. We observe that using the HardNeg-NCE loss, instead of standard NCE, further improves performance for all benchmarks on both zero-shot and fine-tuned settings. }
    \label{tab:baseline-versus-ours}
\end{table}


\subsection{MSR-VTT retrieval}\label{subsec:app:retrieval}
We see in Tab.~\ref{tab:retrieval-MSR-VTT} that while our verb-focused pretraining drastically improves performance on verb-focused benchmarks -- such as Verb$_H$ split~\cite{park-etal-2022-exposing} MSR-VTT (see main paper Tab.~\ref{tab:main-results-MSR-VTT}) -- it maintains performance on noun-focused benchmarks such as MSR-VTT retrieval T2V (1k split) in a zero-shot setting. We perform comparably to InternVideo~\cite{wang2022internvideo} in a zero-shot setting, while using a significantly smaller setting both in terms of architecture (Intern-Video uses 2.8× more parameters and 12.4× more flops) and pretraining dataset size (they use 24× more data). In a fine-tuned setting, InternVideo surpasses VFC's performance. This is expected given our model parameters and flops are significantly smaller -- see number of parameters in Tab.~\ref{tab:retrieval-MSR-VTT}.


\begin{table}
    \setlength{\tabcolsep}{12pt}
    \centering
    \resizebox{0.99\linewidth}{!}{
        \begin{tabular}{l|cc}
            \toprule
                         \multicolumn{1}{c|}{} & & 1K val. \\
  Model   & \# params. & T$\rightarrow$V  R@1 \\
\midrule
\textsc{Zero-shot} &\\ 
VideoCLIP \cite{xu-etal-2021-videoclip} & -- & 10.4 \\
CLIP \cite{Radford2021CLIP}  & 151M  & 30.6  \\
InternVideo \cite{wang2022internvideo}$\ddagger$   & $\thickapprox$ 460M  & \textbf{40.7}  \\
\midrule
\rowcolor{aliceblue} VFC (Ours)  &164M  & 40.3 \\
\midrule
\midrule
\textsc{Fined-tuned} & \\ 
ClipBERT \cite{lei2021less}  & -- & 22.0 \\
MMT~\cite{gabeur2020mmt} & --  & 26.6 \\
VideoCLIP \cite{xu-etal-2021-videoclip}  & -- & 30.9 \\
CLIP-straight~\cite{clip-straight} &  151M & 31.2 \\
MMT (CLIP features) ~\cite{gabeur2020mmt}  & -- & 34.0 \\
C4CL-mP~\cite{park-etal-2022-exposing}  & 151M  & 43.1 \\
CLIP2Video~\cite{park-etal-2022-exposing}  &--  & 45.6 \\
InternVideo \cite{wang2022internvideo}$\ddagger$   & $\thickapprox$ 460M  & \textbf{55.2}  \\
  \midrule
\rowcolor{aliceblue}  VFC (Ours) & 164M & 44.5 \\
\bottomrule
        \end{tabular}
    }
    \vspace{0.2cm}
    \caption{
    \textbf{Results on MSR-VTT retrieval.}
We report T2V retrieval on the 1k split. While our VFC framework drastically improves performance on verb-focused benchmarks, including Verb$_H$ split~\cite{park-etal-2022-exposing} (see main paper Tab.~\ref{tab:main-results-MSR-VTT}), it maintains performance on noun-focused benchmarks such as the retrieval 1k split in the zero-shot setting. In the fine-tuned setting, InternVideo surpasses VFC's performance. $\ddagger$ InternVideo is concurrent unpublished work with a larger model (2.8× more parameters and 12.4× more flops), and has a larger pretraining dataset size (they use 24× more data). }
    \label{tab:retrieval-MSR-VTT}
\end{table}





\subsection{Additional ablations}\label{subsec:app:ablations}

Here, we present ablation results for video mining (Sec.~\ref{subsec:video-mining}), PaLM prompting (Sec.~\ref{subsec:input-output}), the verb phrase loss (Sec.~\ref{subsec:verb-phrase-ablation}), fine-tuning strategy (Sec.~\ref{subsec:freezing}) and calibration (Sec.~\ref{subsec:calibration-ablation}). We note that all ablations are performed with the standard NCE loss (not HardNeg-NCE).


\subsubsection{Video mining}\label{subsec:video-mining}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/video-mining-pairs.pdf}
    \caption{\textbf{Video mining:} We show examples of mined matching videos for generated hard negative captions. For ease of visualisation, we show a single frame per video. In some cases, as the top left corner, the mined video from VideoCC closely matches the hard negative caption. However, often, the new video-text pairs are noisy. For example, in the top right corner, the mined video contains a `red striped shirt' but no `baby sleeping'. In the bottom left example, there is a `woman in a pink bikini standing up' but no `woman massaging her legs'. Finally, in the bottom right example, although the video contains a `motorcycle', the person is not `dancing' and there are no `cones next to them'. }
    \label{fig:video-mining}

\end{figure*}

\begin{table}
    \setlength{\tabcolsep}{8pt}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{lc|lll}
    \toprule
    Method & \# pairs & ~~~~Verb$_{H}$ & ~~~~K-400  & ~~~~SMiT \\
    \midrule
    Baseline & 481K &  69.9 & 55.6 & 78.3 \\
    HN & 481K & 78.0 \green{\small{(+8.1)}}& 55.8 \gray{\small{(+0.2)}} & 78.6 {\gray{\small(+0.3)}}\\
    HN+VM & 1.22M & 78.7 \green{\small{(+8.8)}}& 51.8 \red{\small{(-3.8)}}& 75.0 \red{\small{(-3.3)}}\\
    \bottomrule
    \end{tabular}
    }
        \vspace{0.2cm}
    \caption{
    \textbf{Video Mining.} We report multi-choice accuracy on Verb$_H$~\cite{park-etal-2022-exposing}, Kinetics-400 top-1 accuracy and V2T R@1 on Spoken Moment in Time (validation set of our pretraining SMiT data). We observe that although our Video Mining (VM) approach improves performance on Verb$_H$, it causes a drop in performance on Kinetics and SMiT, which highlights that the additional video-text pairs are noisy. For experiments including hard negatives, we note that one hard negative is sampled for each video here.}
    \label{tab:video-mining}
\end{table}

An alternative to our proposed calibration strategy to avoid imbalances due to the addition of negative captions would be to avoid training with unpaired data at all, by mining a matching video $V^{\text{hard}}_{i_k}$ for each generated caption $T^{\text{hard}}_{i_k}$. 
We attempt this via CLIP-based text-to-video retrieval in a large video database. We next explain our pipeline in more detail.

Firstly, we generate hard negative verb captions with PaLM as explained in Sec.~\ref{sec:vfc} of the main paper. For each hard negative caption, we then perform text-to-image retrieval to find a matching video in the VideoCC~\cite{nagrani2022learning} database. Specifically, we calculate the cosine similarity between the hard negative caption CLIP text embedding and the average of the video frames' CLIP image embedding, for all videos in the database. We then keep the video with closest similarity to the hard negative caption to form a new video-text pair.
Finally, we apply a similarity threshold to keep only the best matching video-text pairs and add these to our training set. In practice, we experiment with different thresholds and find a value of 0.28 to work best, adding a total of 738K new video-text pairs to training (SMiT training set size is 481K). Note that we also experiment with text-to-text retrieval: in this case, we calculate the similarity between each hard negative caption and all VideoCC captions, and subsequently use the video corresponding to the closest VideoCC caption to form a new pair. However, we find this performs worse.  

We observe in Tab.~\ref{tab:video-mining} that our additional video-text pairs are noisy. In fact, although this approach improves performance on Verb$_H$, it causes a large drop in performance on Kinetics and SMiT (validation set of our pretraining data). Finding a video matching a specific, detailed and long caption is challenging (see qualitative examples in Fig.~\ref{fig:video-mining}). A video matching the caption may not exist in the VideoCC corpus and even if it did, for this method to be successful, the mined video must match the generated caption on the verbs (and CLIP is biased towards images and objects only, which is exactly the problem we are trying to solve). 







\subsubsection{Giving input-output example pairs to PaLM}\label{subsec:input-output}
To generate hard verb negative captions with PaLM, we also add four input-output pair examples to the prompt (see full prompt in Sec.~\ref{subsec:app:palm}) to increase the quality of the generated hard negatives. We observe in Tab.~\ref{tab:input-output} that the input-output pairs improve the performance on Verb$_H$ and Kinetics-400.

\begin{table}
    \setlength{\tabcolsep}{22pt}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c|cc}
    \toprule
    input-output pairs & Verb$_{H}$ & K-400  \\
    \midrule
      &  77.5 & 54.6 \\
    \checkmark &  78.0 & 55.8 \\
    \bottomrule
    \end{tabular}
    }
     \vspace{0.2cm}
    \caption{
    \textbf{Inclusion of input-output pairs in PaLM prompt.} We report multi-choice accuracy on Verb$_H$~\cite{park-etal-2022-exposing} and Kinetics-400 top-1 accuracy. We observe that including input-output pairs in the PaLM prompt for generating hard negative captions increases the performance on both benchmarks. We note that one hard negative is sampled for each video here.}
    \label{tab:input-output}
 
\end{table}

\subsubsection{Verb phrase loss}\label{subsec:verb-phrase-ablation}
We see in Tab.~\ref{tab:verb-phrase-v2t} that using only the video-to-text component of the verb phrase loss allows us to maintain performance on noun-focused benchmarks such as MSR-VTT retrieval, while also giving a performance boost on verb focused benchmarks Verb$_H$ and K-400. 

\begin{table}
    \setlength{\tabcolsep}{4pt}
    \centering
    \resizebox{0.99\linewidth}{!}{
        \begin{tabular}{lcc|ll|l}
            \toprule
             & & &  \multicolumn{2}{c|}{\textbf{MSR-VTT}} &  ~~~~\textbf{K-400} \\
             &\multicolumn{2}{c|}{Verb phrase loss}  & ~~~1k val. &  ~Verb$_H$ &  ~~~~~~~all \\
  Method   &  T$\rightarrow$V  & V$\rightarrow$T  & T$\rightarrow$V  R@1 & MC acc & ~~~~Top-1 \\
\midrule
 Baseline &   &  & 40.8 & 69.9 & 55.6 \\
 VFC (Ours) & \checkmark & \checkmark & 38.8 \red{\small{(-2.0)}} & 77.0 \green{\small{(+7.1)}} & 58.8 \green{\small{(+3.2)}}\\
 VFC (Ours) &  & \checkmark & 40.1 \gray{\small{(-0.7)}} & 76.3 \green{\small{(+6.4)}} & 58.5 \green{\small{(+2.9)}} \\
\bottomrule
        \end{tabular}
    }
     \vspace{0.2cm}
    \caption{
    \textbf{Verb phrase loss.}
We report MSR-VTT T2V retrieval on the 1k split, multi-choice accuracy on Verb$_H$~\cite{park-etal-2022-exposing} and Kinetics-400 top-1 accuracy. We observe that using only the video-to-text component of the verb phrase loss allows us to maintain performance on noun-focused benchmarks such as MSR-VTT retrieval, while also giving a performance boost on Verb$_H$ and K-400. For experiments including hard negatives, we note that one hard negative is sampled for each video here.}
    \label{tab:verb-phrase-v2t}
\end{table}



\subsubsection{Fine-tuning image and text towers}\label{subsec:freezing}
We experiment with different fine-tuning strategies: (i) fine-tuning both image and text towers, (ii) freezing the image CLIP backbone only (here, the sequence Transformer seqTrans and text tower are trained -- see Sec.~\ref{subsec:app:clip4clip} for more details on the CLIP4CLIP architecture), (iii) freezing the text tower only (here, seqTrans and image CLIP backbone are trained), (iv) freezing both image and text towers (here, only seqTrans is trained). We see in Tab.~\ref{tab:finetuning-image-text} that fine-tuning both image and text towers works best. We do not include setting (iv) as it performs very poorly.


\begin{table}
    \setlength{\tabcolsep}{6pt}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{lcc|ll}
    \toprule
    Method &  \SnowflakeChevron text & \SnowflakeChevron image & ~~~Verb$_{H}$ & ~~~~K-400  \\
    \midrule
    Baseline & & & 69.9 & 55.6 \\
    \midrule
    VFC (Ours) &  & &  76.3 & 58.5 \\
    VFC (Ours)& \checkmark & &  72.0 \red{\small{(-4.3)}} & 54.8 \red{\small{(-3.7)}} \\
    VFC (Ours) & & \checkmark  & 75.1 \red{\small{(-1.2)}} & 55.1 \red{\small{(-3.4)}} \\
    \bottomrule
    \end{tabular}
    }
     \vspace{0.2cm}
    \caption{
    \textbf{Fine-tuning image and text towers.} We report multi-choice accuracy on Verb$_H$~\cite{park-etal-2022-exposing} and Kinetics-400 top-1 accuracy. \SnowflakeChevron corresponds to freezing the image or text tower. We observe that fine-tuning both image and text towers works best. For experiments including hard negatives, we note that one hard negative is sampled for each video here.}
    \label{tab:finetuning-image-text}
\end{table}

\subsubsection{Calibration}\label{subsec:calibration-ablation}
As explained in Sec.~\ref{sec:vfc} of the main paper, our calibration strategy is composed of two steps: (1) ignoring hard negative captions from the other elements of the batch (denoted as `reducing $B$ effect', where $B$ is the batch size); (2) filtering the generated PaLM captions to have equal number of concept occurences in positive and negative pairs (denoted as $G_\omega \thickapprox S_\omega$). We show the effect of each of these steps in Tab.~\ref{tab:calibration-steps}. We observe that by combining both steps, we avoid a drop in performance on Kinetics-400, while maintaining a large performance improvement on Verb$_H$.


\begin{table}
    \setlength{\tabcolsep}{6pt}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{lcc|cc}
    \toprule
    Method &  reducing $B$ effect& $G_\omega \thickapprox S_\omega$ & Verb$_{H}$ & K-400  \\
    \midrule
    Baseline  & & & 69.9 & 55.6 \\
    HN & & & 80.5 & 54.5 \\
    HN & \checkmark & &  79.4 & 55.4 \\
    HN&  & \checkmark&  78.7 & 55.2 \\
    HN &\checkmark & \checkmark  & 78.0 & 55.8 \\
    \bottomrule
    \end{tabular}
    }
     \vspace{0.2cm}
    \caption{
    \textbf{Calibration strategy.} We report multi-choice accuracy on Verb$_H$~\cite{park-etal-2022-exposing} and Kinetics-400 top-1 accuracy. We observe that by combining both calibration steps, we avoid a drop in performance on Kinetics-400, while maintaining a large performance improvement on Verb$_H$. For experiments including hard negatives, we note that one hard negative is sampled for each video here.}
    \label{tab:calibration-steps}
\end{table}

\vspace{+1cm}
\section{Qualitative results}\label{sec:app:qualitative}

In this section, we present qualitative results on MSR-VTT (Sec.~\ref{sec:app:msr-vtt}) and NEXT-QA (Sec.~\ref{sec:app:next-qa}), further analysis of calibration on Kinetics-verb (Sec.~\ref{sec:app:calibration}), and comparisons of the use of PaLM \textit{versus} rule-based methods for hard negative (Sec.~\ref{sec:app:palm-comparison-neg}) and verb phrase (Sec.~\ref{sec:app:palm-comparison-vp}) generations.  

\vspace{+0.4cm}

\subsection{MSR-VTT}\label{sec:app:msr-vtt}
We show qualitative examples from the Verb$_{H}$ \cite{park-etal-2022-exposing} multiple choice evaluation in Figure~\ref{fig:msrvtt}. For each video sample, we show the 5 captions ranked in order of decreasing similarity for both our baseline and VFC models. We observe that the baseline model often mistakes the hard negative as matching the video. This effect is reduced when training with hard negatives, as proposed in our VFC method, enabling the correct caption to be retrieved from the 5 options. In some rare cases, as shown on the last row, the baseline model is correct but training with hard negatives causes the hard negative to have highest similarity with the video. For example, the model incorrectly ranks `\textit{a silent clip of a woman \textbf{smiling} at people}' higher than `\textit{a silent clip of a woman \textbf{screaming} at people}'.

\clearpage

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.90\textwidth]{./figures/msrvtt-examples.pdf}
     \includegraphics[width=0.90\textwidth]{./figures/msrvtt-examples-2.pdf}
    \caption{\textbf{MSR-VTT verb-focused benchmark:} We show qualitative examples from the Verb$_{H}$ \cite{park-etal-2022-exposing} multiple choice evaluation. For ease of visualisation, we only show a single frame per video. For each video sample, we show the 5 captions ranked in order of decreasing similarity for both our baseline and VFC models. We observe that the baseline model often mistakes the hard negative as matching the video; for example in the top left example, the caption `a deer is rolling across a road in a video game' is ranked higher than the correct answer `a deer is running across a road in a video game'. When training with hard negatives as in our VFC model, the model performance improves, retrieving the correct caption from the 5 options. On the bottom row, we show two failure cases where training with hard negatives causes the model to make a mistake; choosing the hard negative (`a group of people dancing at the camera', `a silent clip of a woman smiling at people') as the correct caption instead of (`a group of people yelling at the camera', `a silent clip of a woman screaming at people').}
    \label{fig:msrvtt}
\end{figure*}


\clearpage

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{./figures/nextqa-examples.pdf}
    \vspace{0.5cm}
    \caption{\textbf{NEXT-QA:} We show qualitative examples from the ATP$_{hard}$ \cite{buch2022revisiting} multiple choice evaluation. For ease of visualisation, we only show a single frame per video. For each video sample, we show the 5 answers ranked in order of decreasing similarity for both our baseline and VFC models in a zero-shot setting. We observe that our VFC model improves performance, retrieving the correct answer from the 5 options more often.}
    \label{fig:nextqa}
\end{figure*} 






\subsection{NEXT-QA}\label{sec:app:next-qa}
In Figure~\ref{fig:nextqa}, we show qualitative examples from the ATP$_{hard}$ \cite{buch2022revisiting} multiple choice evaluation. For each video sample, we show the 5 answers ranked in order of decreasing similarity for both our baseline and VFC models in a zero-shot setting. We observe that our VFC model improves performance, retrieving the correct answer from the 5 options more often.




\subsection{Kinetics-verb: Further analysis of calibration}\label{sec:app:calibration}
In Tab.~\ref{tab:calibration-supp}, we show further examples of confusion matrices comparing training performance with \textit{versus} without calibration on Kinetics-verb classes as in Tab.~\ref{tab:calibration} of the main paper. Once again, we observe that calibration reduces the effect of `attraction' points, which distort the feature space, by making $R_\omega$ the same for all verb phrase concepts.

\subsection{PaLM vs.~rule-based methods for hard negative generation}\label{sec:app:palm-comparison-neg}
In Fig.~\ref{fig:palm-vs-rule-hn}, we compare hard negative caption generation using PaLM to T5 and rule-based methods such as replacing detected verbs by random verbs or antonym verbs. We observe that LLM based methods result in linguistically and semantically viable sentences (which may not be guaranteed with random and antonym verb replacements). We also note that LLM based methods can change more than just the verb: (i) T5 and PaLM can replace the verb by a verb-noun pair and, (ii) PaLM can replace pronouns and determiners anywhere in the sentence (as opposed to T5 which can only replace the verb, see more details in Sec.~\ref{subsec:app:t5}), making the negative caption more linguistically correct.





\subsection{PaLM vs.~rule-based methods for verb phrase extraction}\label{sec:app:palm-comparison-vp}
In Fig.~\ref{fig:palm-vs-rule-vp}, we compare verb phrase extraction using PaLM to:
(i) using action labels for clips from the Moments in Time (MiT) dataset (these are available as SMiT data inherits from MiT~\cite{monfortmoments}) and
(ii) using a rule-based method (NLTK~\cite{bird2009natural}) to isolate verbs. 
We observe that using PaLM outperforms both: (i) MiT action labels can be general and conceal fine-grained action information in the video which can improve verb understanding, (ii) NLTK has difficulties extracting all verbs in a sentence, and can often mistake them for nouns; NLTK also cannot extract a verb phrase when a verb is not present in the sentence (e.g. for the caption `this is an aerial shot of a very nice waterfall', NLTK extracts no verb phrase while PaLM extracts `water flowing'); finally, our NLTK approach does not extract verb-noun pairs (e.g. for the caption `this is a video of two women who are doing gymnastics', NLTK extracts `doing' while PaLM extracts `doing gymnastics') -- this can be crucial for understanding the action in the video. We note that although NLTK could be used to extract verbs and nouns independently through PoS tagging, correctly assigning nouns to the matching verb is not always robust for long, complex sentences as in SMiT. Indeed, the average length of a sentence in SMiT is 18 words. 

\begin{table}[ht]
\vspace{0.5cm}
\setlength{\tabcolsep}{0.1pt}
\centering
\begin{tabular}{l r r}
\toprule
& ~~~~~~~~~~~~\small{w/o calibration} & ~~~~~~~~~~~~~\small{w/ calibration} \\
\vspace{+0.1cm}
\small{$R_{\omega}~~~\propto$} & \multirow{2}{*}{\includegraphics[width=0.25\linewidth]{figures/floor_without.pdf}} & \multirow{5}{*}{\includegraphics[width=0.25\linewidth]{figures/floor_with.pdf}}\\
\vspace{+0.1cm}
\small{mopping floor} & \\
\vspace{+0.1cm}
\small{cleaning floor} & \\
\\
\\
\\
\\
\midrule
\small{$R_{\omega}~~~\propto$} & \multirow{2}{*}{\includegraphics[width=0.27\linewidth]{figures/ball_without.pdf}} & \multirow{5}{*}{\includegraphics[width=0.27\linewidth]{figures/ball_with.pdf}}\\
\vspace{+0.1cm}
\small{dunking basketball} & \\
\vspace{+0.1cm}
\small{shooting basketball} & \\
\\
\\
\\
\\
\midrule
\small{$R_{\omega}~~~\propto$} & \multirow{2}{*}{\includegraphics[width=0.21\linewidth]{figures/nails_without.pdf}} & \multirow{5}{*}{\includegraphics[width=0.21\linewidth]{figures/nails_with.pdf}}\\
\vspace{+0.1cm}
\small{doing nails} & \\
\vspace{+0.1cm}
\small{cutting nails} & \\
\vspace{0.7cm}
\end{tabular}
    \caption{
    \textbf{Confusion matrix for Kinetics-verb classes.}
Without proper calibration, the verb phrases `mopping floor', `dunking basketball', `doing nails' become highly attractive in the video-text feature space. Our calibration mechanism alleviates this issue by making the ratio $R_\omega$ independent of verb phrases (see details in Sec.~\ref{sec:vfc} of the main paper).
    }
    \label{tab:calibration-supp}
\end{table}

\begin{figure}[t!]

    \centering
    \includegraphics[width=0.48\textwidth]{./figures/palm-vs-rule-hn.pdf}
    \caption{\textbf{PaLM vs.~rule-based methods for hard negative generation:} We compare hard verb negative caption generation using PaLM to T5 and rule-based methods such as replacing detected verbs by random verbs or antonym verbs.  We observe that randomly changing the verb often results in sentences which are linguistically and semantically incorrect, and that antonym verbs are often not present in NLTK~\cite{bird2009natural}. On the other hand, LLM based methods such as T5 and PaLM result in meaningful sentences. We note that LLM based methods can change more than just the verb: in the last row, replacing `texting' by `surfing the web' with T5 and `taking a selfie' with PaLM. In some cases, this can make it an easier negative: for example, in the third row, replacing `sitting' by `sitting with a woman' with T5.}
    \label{fig:palm-vs-rule-hn}
\end{figure} 

\section{Baselines \& Implementation details}\label{sec:app:details}

In this section, we present detailed descriptions of baselines (Sec.~\ref{subsec:app:baseline}), the CLIP4CLIP~\cite{Luo2021CLIP4Clip} architecture used in all our experiments (Sec.~\ref{subsec:app:clip4clip}), fine-tuning (Sec.~\ref{subsec:app:finetuning}) and evaluation protocols (Sec.~\ref{subsec:app:eval_proto}), the PaLM prompting procedure (Sec.~\ref{subsec:app:palm}), the T5 hard negative generation process (Sec.~\ref{subsec:app:t5}), and the Kinetics-verb split we propose (Sec.~\ref{subsec:app:kinetics-verb}). 

\subsection{Baselines}\label{subsec:app:baseline}

We describe in more detail baselines presented in the main paper for MSR-VTT, NEXT-QA, Kinetics-400 and SVO-Probes.

\clearpage

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{./figures/palm-vs-rule-vp.pdf}
    \caption{\textbf{PaLM vs.~rule-based methods for verb phrase extraction:} We compare verb phrase extraction using PaLM to:
(i)~using action labels for clips from the Moments in Time (MiT) dataset and (ii)~using a rule-based method such as NLTK~\cite{bird2009natural} to isolate verbs. In the top row, we show examples where NLTK outputs no label as a verb is not present in the sentence (first row, left) or is not detected (first row, right). In the second row, we show examples where extracting verbs with NLTK (e.g. `doing', `having') does not convey crucial information for understanding the action in the video. In the last two rows, we show examples where the MiT labels conceal valuable fine-grained action information in the video, whereas PaLM can recover this from the caption: (third row, left) the video is labelled as `gambling', PaLM extracts `playing cards'; (third row, right) the video is labelled as `stirring', PaLM extracts `pouring oil' and `adding vegetables'; (last row, left) the video is labelled as `playing', PaLM extracts `taking a bath'; (last row, right) the video is labelled as `streching', PaLM extracts `rotating arms' and `lunging'. Overall, our PaLM method of extracting verbs from captions performs best qualitatively and quantitatively (as shown in Tab.~\ref{fig:multi_ablat} (right) of the main paper).}
    \label{fig:palm-vs-rule-vp}
\end{figure} 


 



\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/clip4clip.pdf}
    \caption{\textbf{CLIP4CLIP Architecture:} Figure adapted from~\cite{Luo2021CLIP4Clip}. The model consists of a video encoder, text encoder and similarity calculator. Each frame is passed through ViT to obtain a frame representation at the output of the [class] token. The $T$ frame representations are then passed through a Transformer for sequence modelling and averaged with a mean pooling operation to obtain a video-level representation. The video representation is then compared to the text representation through the cosine similarity. }
    \label{fig:qualitative}
\end{figure*}




\noindent{\textbf{MSR-VTT.}} We show the performance of VideoCLIP~\cite{xu-etal-2021-videoclip}, CLIP~\cite{Radford2021CLIP} and InternVideo~\cite{wang2022internvideo} zero-shot. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. More details for the CLIP baseline can be found in~\cite{Radford2021CLIP}. InternVideo explores jointly using masked video modelling and video-language contrastive learning as pretraining objectives. In the fine-tuned setting, we compare to ClipBERT~\cite{lei2021less}, MMT~\cite{gabeur2020mmt}, VideoCLIP~\cite{xu-etal-2021-videoclip}, C4CL-mP~\cite{park-etal-2022-exposing}. ClipBERT focuses on sparse training to reduce video processing overhead and applying image-text pretraining for video-text tasks. MMT uses a multi-modal transformer to encode video and BERT~\cite{devlin-etal-2019-bert} for text. C4CL-mP corresponds to the CLIP4CLIP~\cite{Luo2021CLIP4Clip} reimplementation by Park et al.~\cite{park-etal-2022-exposing} with just mean pooling (without any Transformer Encoder for temporal modelling of frames). 


\noindent{\textbf{NEXT-QA.}}  We show the performance of CLIP zero-shot. More details for this baseline can be found in~\cite{Radford2021CLIP}. In the fine-tuned setting, we compare to HGA~\cite{jiang2020reasoning}: a deep heterogenous graph network which aligns inter- and intra- modality information (appearance, motion and text) to reason and answer the question. We also compare to ATP, Temp[ATP] and Temp[ATP]+ATP from~\cite{buch2022revisiting}. ATP consists of a Transformer which learns to select a single (frozen) CLIP frame embedding from a video, given the sequence of video frame embeddings and question embedding, for the task of video question answering. For training, they use a cross entropy loss over the answer set. Temp[ATP] is an extension of ATP, where the video is first partitioned into $k$ clips, and a single frame embedding is selected using ATP from each clip. These $k$ frame embeddings are then aggregated to a video-level representation using a Transformer, before being passed to the downstream task. Temp[ATP]+ATP corresponds to an ensemble of both ATP and Temp[ATP]. Finally we compare to VGT~\cite{xiao2022video}, which consists of a video graph transformer that explicitly encodes objects, relations and dynamics. VGT also uses disentangled video and text Transformers to better measure relevance between video and text.  




\noindent{\textbf{Kinetics-400.}} We show the performance of Flamingo~\cite{flamingo}, ActionCLIP~\cite{actionclip} and CLIP~\cite{Radford2021CLIP} zero-shot. Flamingo is a visual-language model, which leverages pretrained vision and language models and bridges them effectively by using gated cross-attention and dense layers.  ActionCLIP~\cite{actionclip} reformulates action recognition into a video-text matching problem within a multimodal contrastive learning framework. More details for CLIP can be found in~\cite{Radford2021CLIP}. 


\noindent{\textbf{SVO Probes.}} We show the performance of No-MRM–MMT (the best performing model in \cite{hendricks2021probing}) and CLIP~\cite{Radford2021CLIP} zero-shot. No-MRM-MMT corresponds to a multi-modal transformer (similar to the ViLBERT~\cite{vilbert} architecture) with a masked language modeling loss (MLM), an image-text matching loss (ITM) that classifies if an image-sentence pair are matching, but no masked region modeling loss (MRM). More details for the CLIP baseline can be found in~\cite{Radford2021CLIP}. 



\subsection{CLIP4CLIP Architecture}\label{subsec:app:clip4clip}

Here, we describe the CLIP4CLIP network architecture~\cite{Luo2021CLIP4Clip}, illustrated in Figure~\ref{fig:qualitative}, used in all our experiments. This architecture consists of three components: a video encoder, a text encoder, and a similarity calculator. We describe each component in detail next. We note that all three components are fine-tuned in all our experiments.

\noindent \textbf{Video encoder}. The pretrained CLIP (ViT-B/32)~\cite{Radford2021CLIP} image encoder is used to obtain frame representations. Specifically, video frames are first sampled from the video and reshaped into a sequence of flattened 2D patches. These patches are then linearly projected to 1D tokens before being inputted to ViT~\cite{dosovitskiy2020vit}, a 12-layer Transformer. The output from the [class] token is used as the video frame representation: given $T$ input frames, we obtain $T$ frame representations. In practice, we select 32 frames (with initial resolution $256 \times 256$, of which augmented crops of size $224 \times 224$ are taken) in a video at 25fps, at a stride of 2 frames for training. 

\noindent \textbf{Text encoder}. The CLIP pretrained text encoder is used to embed the caption. It corresponds to a 12-layer Transformer model; further details can be found in~\cite{Radford2021CLIP}. 

\noindent \textbf{Similarity calculator}. The goal is to learn a function to calculate the similarity between video-text pairs inputted to the model in such a way that video-text pairs which \textit{match} have a high similarity, and otherwise have a low similarity.  Therefore, we ultimately want to compare a text and video-clip representation. The ViT encoder outputs a representation for each of the sequence of frames  without any temporal modelling. We therefore first pass these frame embeddings (along with temporal positional embeddings) through a 4-layer Transformer encoder. We then apply a mean-pooling operation to the new frame embeddings to obtain a video-level representation. Finally, we calculate the cosine similarity between the video and text representations.

Following the protocol in~\cite{Luo2021CLIP4Clip}, the positional embeddings in the similarity calculator are initialised by repeating the position embedding from CLIP's text encoder. The Transformer encoder is initialised by the corresponding layers' weight of the pretrained CLIP image encoder. The rest is randomly initialised. 



\subsection{Fine-tuning details}\label{subsec:app:finetuning}

\noindent \textbf{MSR-VTT}. When fine-tuning on MSR-VTT, we use the 9K and 7K training split for the retrieval and multi-choice settings respectively. For the 9K split, we train for 100 epochs with a base learning rate of 1e-7, a weight decay of 1e-2 and temperature of 5e-3. For the 7K split, we train for 100 epochs with a base learning rate of 1e-7, a weight decay of 1e-2 and temperature of 5e-3. For both settings, we train with the hard negative contrastive loss and discard the verb phrase loss. Indeed, we use PaLM to generate hard negative captions for MSR-VTT, since it is a video-text retrieval dataset, similarly to SMiT. We sample 32 frames per video at 25~fps with a stride of 14.

\noindent \textbf{NEXT-QA}. For fine-tuning on NEXT-QA, we concatenate the question and answer pairs before passing them through the CLIP4CLIP text tower. We continue using the hard-negative cross-modal contrastive loss during fine-tuning, treating the four incorrect question-answer pairs as hard negatives. We discard the verb phrase loss. We train for 100 epochs with a base learning rate of 1e-6, a weight decay of 5e-2 and temperature of 1e-3. We maintain a batch size of 256. We sample 32 frames per video at 25~fps with a stride of 24.

\subsection{Evaluation protocols}\label{subsec:app:eval_proto}

\noindent \textbf{MSR-VTT}. For the standard setting, we evaluate text-to-video retrieval (R@1) on the 1K validation split and 3K Random MC. In the former, the model must associate the text to the correct video, among 1000 videos. For the latter, the model must associate the video to the right caption, among 5 captions, where the 4 negative captions are randomly chosen from other videos. For our verb-focused setting, we use the Verb$_{H}$ multiple choice (MC) validation split from \cite{park-etal-2022-exposing}. Verb$_{H}$ MC covers a subset of the videos in the 3K Random MC split, with 2,554 video-text instances, but the task is harder. In the Verb$_{H}$ MC setting, one of random negative captions is replaced by a \textit{hard verb negative}, where the correct sentence's verb has been modified manually in such a way that the new sentence is inconsistent with the video. We mark the model prediction as correct if the ground truth sentence among the 5 captions has the highest similarity score with the video. We sample 32 frames per video at 25 fps with a stride of 14.

\noindent \textbf{Kinetics-400}. We follow~\cite{Radford2021CLIP} to evaluate classification in a zero-shot setting: we feed in all class labels (without any prompt) to the text tower and mark the prediction as correct if the correct label has the highest similarity with the video. For the `Kinetics-verb' split, we restrict the evaluation to 97 classes which we manually identify as requiring verb understanding (see Sec.~\ref{subsec:app:kinetics-verb}). We note that we still feed in all 400 class labels for measuring classification on Kinetics-verb. We sample 32 frames per video at 25 fps with a stride of 14.

\noindent \textbf{NEXT-QA}. We concatenate the question and answer pairs before passing them through the CLIP4CLIP text tower. We mark the model prediction as correct if the correct question-answer pair among the 5 options has the highest similarity score with the video. We sample 32 frames per video at 25~fps with a stride of 24.

\noindent \textbf{SVO probes}. This is an image-text benchmark~\cite{hendricks2021probing}, specifically designed to measure progress in verb understanding. We evaluate our baseline and VFC framework on a subset of 12,936 images from the original 14,102 images since some images are no longer accessible (the corresponding urls are corrupted). In~\cite{hendricks2021probing}, the authors calculate the accuracy of positive and negative image-text pairs: they pass image-text pairs through their model and label an image–sentence pair as negative if the classifier output is $< 0.5$ and
positive otherwise. Our model confidences are calibrated differently, therefore we instead report Average Precision (AP). To evaluate on this dataset, we simply replicate the image 32 times as input to our video model.

\subsection{PaLM prompting}\label{subsec:app:palm}

\noindent\textbf{PaLM hard negative generation.} We include below our full prompt template for automatic generation of hard negatives. We insert the caption for which we want to generate hard verb negatives at \textbf{\{input caption\}}. \\
\noindent\rule{8.5cm}{0.4pt}
\textit{In this task, you are given an input sentence. Your job is to tell me 10 output sentences with a different meaning by only changing the action verbs. \\
Input: A man walks up to a woman holding an umbrella in a garden. \\
Outputs: \\
1) A man jumps up to a woman throwing an umbrella in a garden. \\
2) A man runs up to a woman opening an umbrella in a garden. \\
3) A man walks away from a woman buying an umbrella in a garden. \\
4) A man throws up on a woman carrying an umbrella in a garden. \\
5) A man punches a woman swinging an umbrella in a garden. \\
6) A man sits with a woman wrapping up her umbrella in a garden. \\
7) A man talks to a woman closing an umbrella in a garden. \\
8) A man flirts with a woman playing with an umbrella in a garden. \\
9) A man skips to a woman leaning on her umbrella in a garden. \\
10) A man sprints to a man losing her umbrella in a garden. \\
Input: Surfers ride the waves in an ocean. 
Outputs: \\ 
1) Surfers get hit by the waves in an ocean. \\
2) Surfers swimming in the waves in an ocean. \\
3) Surfers meditating by the waves in an ocean. \\
4) Surfers drowning in the waves in an ocean. \\
5) Surfers asking for help in the waves in an ocean. \\
6) Surfers teaming up in the waves in an ocean. \\
7) Surfers snorkeling in the waves in the ocean. \\
8) Surfers taking photos by the waves in the ocean. \\
9) Surfers getting ready to go into the waves in the ocean. \\
10) Surfers st}\textit{retching by the waves in the ocean. \\
Input: A dentist holds the replica of a human mouth he shows how important flossing your teeth is. \\
Outputs:\\
1) A dentist cleans the replica of a human mouth he presents how unimportant flossing your teeth is. \\
2) A dentist breaks the replica of a human mouth he screams how important flossing your teeth is.\\
3) A dentist fixes the replica of a human mouth he says how important flossing your teeth is. \\
4) A dentist buys the replica of a human mouth he explains how important brushing your teeth is. \\
5) A dentist plays with the replica of a human mouth he remembers about how important washing your teeth is. \\
6) A dentist tidies the replica of a human mouth he rambles on about how important breaking your teeth is. \\
7) A dentist rotates the replica of a human mouth he presents how important fracturing your teeth is. \\
8) A dentist places on his legs the replica of a human mouth he shows how important flossing your teeth is.\\
9) A dentist searches for the replica of a human mouth he shows how important grinding your teeth is. \\
10) A dentist picks up the replica of a human mouth he presents how important whitening your teeth is. \\
Input: Looks like a band playing on the stage and perhaps Community Center and people gathered around watching. \\
Outputs: \\
1) Looks like a band fighting on the stage and perhaps Community Center and people gathered around crying. \\
2) Looks like a band dancing on the stage and perhaps Community Center and people gathered around smiling. \\
3) Looks like a band singing on the stage and perhaps Community Center and people gathered around filming. \\
4) Looks like a band bowing on the stage and perhaps Community Center and people gathered around clapping.\\
5) Looks like a band making a speech on the stage and perhaps Community Center and people gathered around listening. \\
6) Looks like a band laughing on the stage and perhaps Community Center and people gathered around cheering.\\
7) Looks like a band working on the stage and perhaps Community Center and people gathered around standing.\\
8) Looks like a band holding hands on the stage and perhaps Community Center and people gathered around praying. \\
9) Looks like a band jumping on the stage and perhaps Community Center and people gathered around encouraging.\\
10) Looks like a band yelling on the stage and perhaps Community Center and people gathered around watching.\\
Input: \textbf{\{input caption\}}\\
Outputs: \\}
\noindent\rule{8.5cm}{0.4pt}

\noindent\textbf{PaLM verb phrase extraction.} We use PaLM to extract verb phrases from the original caption, where a verb phrase can correspond to a single verb or a verb-noun pair depending on the caption. We use PaLM-540B with output sequence length 256, beam size of 4,  and temperature of 0.2. We post-process the outputs by removing text after any newline character. We include our full prompt template for automatic extraction of verb phrases below. We insert the caption for which we want to extract a verb phrase at \textbf{\{input caption\}}. \\
\noindent\rule{8.5cm}{0.4pt}
\textit{In this task, you are given an input sentence. Your job is to output the action verb phrases. \\
Input: the young girl in the middle of the road she is dancing. \\
Output:  [`dancing'] \\
Input: a city area can be seen that has people in the walkways of runways. \\
Output: [] \\
Input: this is a video of a birthday and she has a green colored dress and they are cutting a cake there's a clown on the side and the parents seem to be clap. \\
Output: [`cutting cake', `clapping'] \\
Input: one woman is talking to the camera about being safe he has a shirt with pal pal on it in the greenery behind her.  \\
Output: [`talking to camera'] \\
Input: a bicycle with a specialized back wheel slides along a wet paper. \\
Output: [`sliding'] \\
Input: a person clicking an object that is connected to a speaker. \\
Output: [`clicking'] \\
Input: it's a video of a football game and one of the blue team is throwing the football really far into the endzone. \\
Output: [`throwing football'] \\
Input: this is a video of someone filing their nails. \\
Output: [`filing nails'] \\
Input: airplane with the words British Airways can be seen over top. \\
Output: [] \\
Input: man sitting standing at the front of the room is giving speech and asking an audience if they've ever heard of a specific song. \\
Output: [`standing', `giving speech', `asking'] \\
Input: it shows a video of a man talking on the phone yeah glasses and has a black phone. \\
Output: [`talking on phone'] \\
Input: hitchhiker is on the side of the road by a truck stop pulling a sign that says North. \\
Output: [`pulling a sign'] \\
Input: this is a video of a man on a ladder the man is cutting down a tree branch the man is wearing red. \\
Output: [`cutting tree'] \\
Input: on an indoor gym on a hard Brown meth there's a man young man with a barbell with lots of heavy weights on each side and he has it over his head stiff arm straight arm going to be and then he drops it on the floor while he does so you can hear the clanking of the weight that they smack against each other. \\
Output: [`dropping'] \\
Input: he is using a large chainsaw to cut inside of a tree branch. \\
Output: [`cutting tree'] \\
Input: I meant stacking up his cups for cup stacking concentration for a party. \\
Output: [`stacking cups'] \\
Input: a large field shown with garbage and water flowing through it. \\
Output: [`water flowing'] \\
Input: a washing machine washes the clothes. \\
Output: [`washing clothes'] \\
Input: \textbf{\{input caption\}}\\
Output: \\}
\noindent\rule{8.5cm}{0.4pt}

\noindent\textbf{PaLM positive generation.} We use PaLM to generate positive sentences where the verb in the original caption is changed to a synonym verb, but the remaining context is unchanged. We use PaLM-540B with output sequence length 512, beam size of 1, and temperature of 0.7. We post-process the outputs by removing text after any newline character and by filtering out candidates which contain the same verbs as the original caption. We include our full prompt template for automatic generation of positives below. We insert the caption for which we want to generate a positive sentence at \textbf{\{input caption\}}. \\
\noindent\rule{8.5cm}{0.4pt}
\textit{In this task, you are given an input sentence. Your job is to tell me 10 output sentences with the same meaning by only changing the action verbs. \\
Input: A man walks up to a woman holding an umbrella in a garden. \\
Outputs: \\
 1) A man strolls up to a woman holding an umbrella in a garden.  \\
 2) A man marches up to a woman holding an umbrella in a garden.  \\
 3) A man strides up to a woman holding an umbrella in a garden. \\ 
 4) A man wanders up to on a woman carrying an umbrella in a garden. \\
 5) A man tramps up to a woman holding an umbrella in a garden. \\
 6) A man steps up to with a woman holding an umbrella in a garden. \\
 7) A man wanders up to a woman holding an umbrella in a garden. \\ 
 8) A man treads up to a woman holding an umbrella in a garden. \\
 9) A man truges up to a woman holding an umbrella in a garden. \\
 10) A man treaks to a woman holding her umbrella in a garden. \\
Input: A dentist holds the replica of a human mouth he shows how important flossing your teeth is. \\
Outputs: \\ 
1) A dentist grasps the replica of a human mouth he shows how important flossing your teeth is. \\
2) A dentist carries the replica of a human mouth he shows how important flossing your teeth is. \\
3) A dentist clutches the replica of a human mouth he shows how important flossing your teeth is. \\
4) A dentist grips the replica of a human mouth he shows how important flossing your teeth is. \\
5) A dentist holds the replica of a human mouth he explains how important flossing your teeth is. \\
6) A dentist holds the replica of a human mouth he presents how important flossing your teeth is. \\
7) A dentist holds the replica of a human mouth he demonstrates how important flossing your teeth is. \\
8) A dentist holds the replica of a human mouth he communicates how important flossing your teeth is.\\
9) A dentist holds the replica of a human mouth he displays how important flossing your teeth is. \\
10) A dentist holds the replica of a human mouth he highlights how important flossing your teeth is.\\
Input: This is a video of somebody touching wood. \\
Outputs:\\
1) This is a video of somebody tapping wood. \\
2) This is a video of somebody stroking wood. \\
3) This is a video of somebody pressing wood. \\
4) This is a video of somebody handling wood. \\
5) This is a video of somebody patting wood. \\
6) This is a video of somebody brushing wood. \\
7) This is a video of somebody grazing wood. \\
8) This is a video of somebody poking wood. \\
9) This is a video of somebody caressing wood. \\
10) This is a video of somebody gripping wood.\\
Input: This is a video of a group of adults outside dancing. \\
Outputs: \\
 1) This is a video of a group of adults outside whirling. \\
 2) This is a video of a group of adults outside twirling.\\
 3) This is a video of a group of adults outside swaying. \\
 4) This is a video of a group of adults outside partying. \\
 5) This is a video of a group of adults outside getting down. \\
 6) This is a video of a group of adults outside spinning. \\
 7) This is a video of a group of adults outside bouncing. \\
 8) This is a video of a group of adults outside bopping.\\
 9) This is a video of a group of adults outside waltzing. \\
 10) This is a video of a group of adults outside prancing.\\
Input: \textbf{\{input caption\}}\\
Outputs: \\}
\noindent\rule{8.5cm}{0.4pt}

\subsection{T5 generations}\label{subsec:app:t5}

As well as using PaLM to generate hard verb negative captions, we experiment with using  a bidirectional language model, T5-Base~\cite{2020t5}: a 220 million parameter encoder-decoder Transformer. 
It is pretrained on the Colossal Clean Crawled Corpus (C4)~\cite{palm} on a multi-task mixture of unsupervised and supervised tasks, with all tasks being converted into a text-to-text format. 
T5 is trained with a Masked Language Modelling (MLM) loss, similarly to BERT~\cite{devlin-etal-2019-bert}, with minor differences. 
MLM involves masking certain tokens in an input sequence before passing them to the model, and tasking the model with predicting the masked spans. 

As T5 has been trained with a span-mask denoising objective, we use it at inference time in \textit{cloze} form (fill in the blanks) to replace words in captions by targeted masking. 
Specifically, our method consists of the following steps: \\ 
(1) \textbf{Verb Identification:} we start by identifying verbs in text captions, leveraging PoS tagging with NLTK~\cite{bird2009natural}. \\
(2) \textbf{T5 prediction:}
We then replace the verb tokens with a \texttt{[MASK]} token, and feed the masked sentence to T5. We keep the Top-$K$ phrases predicted by the model (with $K = 50$). 
Unlike~\cite{park-etal-2022-exposing}, we do not fine-tune T5 for verb modelling specifically, but rather use it in a zero-shot setting, which we find is sufficient to generate plausible negatives. \\
(3) \textbf{Negatives Filtering:}
The $K$ candidate sentences are then filtered to remove sentences which contain the same verbs as the original caption.

\subsection{Kinetics-verb}\label{subsec:app:kinetics-verb}
In order to assess our method's true verb understanding in the downstream task of action classification, we introduce `Kinetics-verb': a subset of 97 classes from Kinetics-400~\cite{Carreira_2017_CVPR} where we isolate classes that share a \textit{common noun} with another class, but have a \textit{different verb} (and therefore action).  We include the set of 97 classes below:

\noindent [\textbf{hair}: braiding hair, brushing hair, curling hair, dying hair, fixing hair, washing hair, getting a hair cut; \textbf{nails}: doing nails, cutting nails;  \textbf{legs}: waxing legs, massaging legs, shaving legs, stretching leg, swinging legs;  \textbf{hands}: washing hands, shaking hands,  \textbf{arm}: stretching arm, exercising arm, arm wrestling; \textbf{watermelon}: cutting watermelon, eating watermelon;  \textbf{floor}: mopping floor, cleaning floor, sanding floor, sweeping floor;  \textbf{baby}: baby waking up, carrying baby, crawling baby; \textbf{back}: waxing back, bending back, massaging back; \textbf{feet}: massaging feet, washing feet; \textbf{dog}: walking the dog, grooming dog, training dog; \textbf{cake}: eating cake, making a cake;  \textbf{guitar}: strumming guitar, playing guitar, tapping guitar; \textbf{cards}: shuffling cards, playing cards; \textbf{present}: wrapping present, opening present; \textbf{egg}: cooking egg, egg hunting, scrambling eggs; \textbf{shoes}: shining shoes, cleaning shoes; \textbf{pool}: cleaning pool, jumping into pool; \textbf{snow}: biking through snow, shoveling snow; \textbf{rope}: skipping rope, climbing a rope; \textbf{fish}: catching fish, feeding fish; \textbf{eyebrows}: filling eyebrows, waxing eyebrows; \textbf{computer}: using computer, assembling computer; \textbf{tree}:  climbing tree, planting trees, trimming trees; \textbf{car}: driving car, pushing car;  \textbf{golf}: golf chipping, golf driving, golf putting; \textbf{beer}:  drinking beer, tasting beer; \textbf{horse}:  grooming horse, riding or walking with horse; \textbf{paper}: folding paper, ripping paper, shredding paper; \textbf{fire}: extinguishing fire, juggling fire; \textbf{head}: shaking head, shaving head;  \textbf{water}: surfing water, water skiing, water sliding; \textbf{ice}: ice climbing, ice fishing, ice skating; \textbf{basketball}: dunking basketball, dribbling basketball, playing basketball, shooting basketball; \textbf{finger}: drumming fingers, finger snapping;  \textbf{baseball}:  catching or throwing baseball, hitting baseball; \textbf{soccer ball}: juggling soccer ball, kicking soccer ball]



















