\documentclass{article}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathabx}
\usepackage{url}
\usepackage{ucs}
\usepackage{braket}
\usepackage{natbib}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{abstract}

\newcommand{\todo}[1]{\textbf{\textcolor{red}{[[ToDo: #1]]}}}
\newcommand{\ignore}[1]{}
\newcommand{\given}{\,|\,}
\newcommand{\params}{\theta}
\newcommand{\as}{\star}

\title{Using the Expectation Maximization Algorithm with Heterogeneous Mixture Components for the Analysis of Spectrometry Data}

\author[1]{Dominik Kopczynski}
\author[2]{Sven Rahmann}

\affil[1]{Collaborative Research Center SFB~876, TU~Dortmund, Germany}
\affil[2]{Genome Informatics, Institute of Human Genetics, Faculty of Medicine, University of Duisburg-Essen, and University Hospital Essen, Germany}

\begin{document}
\maketitle
\begin{abstract}
Coupling a multi-capillary column (MCC) with an ion mobility (IM) spectrometer (IMS) opened a multitude of new application areas for gas analysis, especially in a medical context, as volatile organic compounds (VOCs) in exhaled breath can hint at a person's state of health.
To obtain a potential diagnosis from a raw MCC/IMS measurement, several computational steps are necessary, which so far have required manual interaction, e.g., human evaluation of discovered peaks.
We have recently proposed an automated pipeline for this task that does not require human intervention during the analysis.
Nevertheless, there is a need for improved methods for each computational step.
In comparison to gas chromatography / mass spectrometry (GC/MS) data, MCC/IMS data is easier and less expensive to obtain, but peaks are more diffuse and there is a higher noise level.
MCC/IMS measurements can be described as samples of mixture models (i.e., of convex combinations) of two-dimensional probability distributions.
So we use the expectation-maximization (EM) algorithm to deconvolute mixtures in order to develop methods that improve data processing in three computational steps: denoising, baseline correction and peak clustering.
A common theme of these methods is that mixture components within one model are not homogeneous (e.g., all Gaussian), but of different types.
Evaluation shows that the novel methods outperform the existing ones.
We provide Python software implementing all three methods and make our evaluation data available at \url{http://www.rahmannlab.de/research/ims}.
\end{abstract}








\section{Introduction}
\label{sec:intro}

\paragraph*{Technology background}
An ion mobility (IM) spectrometer (IMS) measures the concentration of volatile organic compounds (VOCs) in the air or exhaled breath by ionizing the compounds, applying an electric field and measuring how many ions drift through the field after different amounts of time.
A multi-capillary column (MCC) can be coupled with an IMS to pre-separate a complex sample by retaining different compounds for different times in the columns (according to surface interactions between the compound and the column).
As a consequence, compounds with the same ion mobility can be distinguished by their distinct retention times.

Recently, the MCC/IMS technology has gained importance in medicine, especially in breath gas analysis, as VOCs may hint at certain diseases like lung cancer, chronic obstructive pulmonary disease (COPD) or sarcoidosis~\citep{Westhoff/etal/2009a, bodeker2008peakcomp, bunkowski/etal/2009a, westhoff2010differentiation}.


\begin{figure*}[t]\centering
\includegraphics[width=0.98\linewidth]{mcc_im_measurement_raw}
\caption{
Visualization of a raw IMSC as a heat map.
X-axis: reduced inverse mobility  in ; 
y-axis: retention time~ in seconds; 
signal: white (lowest)  blue  purple  red  yellow (highest), reactant ion peak (RIP) at .
}
\label{fig:visualized-imsc}
\end{figure*}

A typical MCC/IMS measurement takes about ten minutes.
Within this time the MCC pre-separates the sample.
An IM spectrum is captured periodically every 100~ms.
The aligned set of captured IMS spectra is referred to an IM spectrum-chromatogram (IMSC) which consists of an~ matrix~, where~ is the set of retention time points (whenever an IM spectrum is captured, measured in seconds), and~ is the set of drift time points (whenever an ion measurement is made; in milliseconds).
To remove the influences of pressure, ambient temperature or drift tube size, a normalized quantity is used instead of drift time, namely the reduced mobility~ with units of~, as described by~\cite{eiceman/2010a}.
(Reduced) mobility is inversely proportional to drift time, so we consider the reduced inversed mobility (RIM)~ with units of~.
RIM and drift time are proportional, with the proportionality constant depending on the above external quantities.
As not mentioned otherwise, in the following we use  and  which corresponds to the voltage and length of our IMS drift tube.
We assume that all time points (or RIMs) are equidistant; 
so we may work with matrix indices~ and~ for convenience.
On average, an IM spectrum takes about , corresponding to  and an IMSC about~.
The signal values of an IMSC are digitized by an analog-digital converter with a precision of~12~bits.
Since the device can operate in positive and negative mode, the values range between~ and~.

Figure~\ref{fig:visualized-imsc} visualizes an IMSC as a heat map.
Areas with a high signal value are referred to as \emph{peaks}.
A peak is caused by the presence (and concentration) of a certain compound; the peak position~ indicates which compound is present, and the peak volume contains information about the concentration.

An inherent feature of IMS technology is that the drift gas is ionized, too, which results in a ``peak'' that is present at each retention time at a RIM of~ (Figure~\ref{fig:visualized-imsc}). 
It is referred to as the reactant ion peak (RIP).

\paragraph*{Related work and novel contributions}
A typical work flow from a raw IMSC to a ``diagnosis'' or classification of the measurement into one of two (or several) separate classes generally proceeds along the following steps described by \cite{daddario/etal/2014a}:
pre-processing, peak candidate detection, peak picking, and parametric peak modeling.

All methods of this paper are adaptations of the expectation-maximization (EM) algorithm, modified for their particular task.
The EM~algorithm (introduced by \cite{Dempster/etal/1977a}) is a statistical method frequently used for deconvolving distinct components in mixture models and estimating their parameters.
We summarize its key properties in Section~\ref{sec:algo:em}.

We previously used the EM~algorithm for peak modeling, at the same time decomposing a measurement into separate peaks. 
We introduced a model that describes the shape of a peak with only seven parameters using a two-dimensional shifted inverse Gaussian distribution function with an additional volume parameter \citep{Kopczynski/etal/2012a}.

We also evaluated different peak candidate detection and peak picking methods, comparing for example manual picking by an expert with state of the art tools like IPHEx \citep{bunkowski_2011}, VisualNow \citep{bader2007reduction}, and our own methods \citep{hauschild2013eval}.

In this work, we focus on pre-processing.
Pre-processing is a crucial step because it determines the difficulty and the accuracy with which peak candidates (and peaks) can be identified and correctly modeled.
It consists of several sub-tasks: denosing, baseline correction and smoothing.
We discuss novel methods for denoising with integrated smoothing (Section~\ref{sec:denoising}) and for baseline correction (Section~\ref{sec:baseline}).

A second focus of this work is on finding peaks that correspond to each other (and hence to the same measured compound) in several measurements of a dataset.
We introduce an EM-based clustering method (Section~\ref{sec:clustering}).
An accurate clustering is important for determining feature vectors for classification.
As the detected location of a peak may differ between several measurements, a clustering approach across measurements suggests itself.
Several clustering algorithms like~-means (first introduced by~\cite{macqueen1967kmeans}) or hierarchical clustering have the disadvantage that they need a fixed number of models or a threshold for the density within a cluster.
In practice, these algorithms are executed several times with an increasing number of clusters and take the best result with respect to a cost function penalized with model complexity.
DBSCAN~\citep{ester/etal/1996} is a clustering method which does not require a fixed cluster number.

We demonstrate that our proposed EM variants outperform existing methods for their respective tasks in Section~\ref{sec:eval} and conclude the paper with a brief discussion.
\section{Algorithms}
\label{sec:algo}

This section describes our adaptations of the EM~algorithm (summarized in Section~\ref{sec:algo:em}) for denoising (Section~\ref{sec:denoising}), baseline correction (Section~\ref{sec:baseline}) and peak clustering across different measurements (Section~\ref{sec:clustering}).
The first two methods use heterogeneous model components, while the last one dynamically adjusts the number of clusters.
For each algorithm, we present background knowledge, the specific mixture model, the choice of initial parameter values, the maximum likelihood estimators of the M-step (the E-step is described in Section~\ref{sec:algo:em}), and the convergence criteria.
For peak clustering, we additionally describe the dynamic adjustment of the number of components.
The algorithms are evaluated in Section~\ref{sec:eval}.

\subsection{The EM Algorithm for Mixture Models with Heterogeneous Components}
\label{sec:algo:em}

In all subsequent sections, variations of the EM~algorithm \citep{Dempster/etal/1977a} for mixture model deconvolution are used.
Here we summarize the algorithm and describe the E-step common to all variants.


A fundamental idea of the EM~algorithm is that the observed data~ is viewed as a sample of a mixture (convex combination)~ of probability distributions,

where ~indexes the~~different component distributions~, where  denotes all parameters of distribution~, and  is the collection of all parameters.
The mixture coefficients  satisfy  for all~, and .

We point out that, unlike in most applications, in our case the probability distributions  are of different types, e.g., a uniform and a Gaussian one.

The goal of mixture model analysis is to estimate the mixture coefficients~ and the individual model parameters~, whose number and interpretation depends on the parametric distribution~.

Since the resulting maximum likelihood parameter estimation problem is non-convex, iterative locally optimizing methods such as the Expectation Maximization (EM) algorithm are frequently used.
The EM~algorithm consists of two repeated steps:
The E-step (expectation) estimates the expected membership of each data point in each component and then the component weights~, given the current model parameters~.
The M-step (maximization) estimates maximum likelihood parameters~ for each parametric component~ individually, using the expected memberships as hidden variables that decouple the model.
As the EM~algorithm converges towards a local optimum of the likelihood function, it is important to choose reasonable starting parameters for~.

\paragraph{E-step}
The E-step is independent of the specific component distribution types and always proceeds in the same way, so we summarize it here once, and focus on the specific M-step in each of the following subsections.
To estimate the expected membership~ of data point~ in each component~, the component's relative probability at that data point is computed, i.e.,

such that~ for all~.
Then the new component weight estimates~ are the averages of~ across all data points,

where~ is the number of data points.

\paragraph{Convergence}
After each M-step of an EM~cycle, we compare  (old parameter value) and  (updated parameter value), where~ indexes the elements of~, the parameters of component~.
We say that the algorithm has converged when the relative change

drops below the threshold~, corresponding to~ precision, for all~.
(If , we set .)

\subsection{Denoising}
\label{sec:denoising}

\paragraph*{Background}
A major challenge during peak detection in an IMSC is to find peaks that only slightly exceed the background noise level.

As a simple method, one could declare each point~ as a peak whose intensity~ exceeds a given threshold.
In IMSCs, peaks become wider with increasing retention time, while their volume remains constant, so their height shrinks, while the intensity of the background noise remains at the same level.
So it is not appropriate to choose a constant noise level threshold, as peaks at high retention times may be easily missed.

To determine whether the intensity~ at coordinates~ belongs to a peak region or can be solely explained by background noise, we propose a method based on the EM~algorithm.
It runs in~ time where~ is the number of EM~iterations.
Before we explain the details of the algorithm, we mention that it does not run on the IMSC directly, but on a smoothed matrix~ containing local averages from a window with margin~;

for all~, .
Since the borders of an IMSC do not contain important information, we deal with boundary effects by computing~ in those cases as averages of only the existing matrix entries.

To choose the smoothing radius~, we consider the following argument.
For distinguishing two peaks, \cite{bodeker2008peak} introduced a minimum distance in reduced inverse mobility of~.
In our datasets (2500 drift times with a maximal value of reduced inverse mobility of~), this corresponds to~ index units, so we use~ index units to avoid taking to much noise into consideration.

\paragraph{Mixture model}
Based on observations of IMSC signal intensities, we assume that
\begin{itemize}
\item the noise intensity has a Gaussian distribution over low intensity values with mean~ and standard deviation~,

\item the true signal intensity has an Inverse Gaussian distribution with mean  and shape parameter~, i.e.,

\item there is an unspecific background component which is not well captured by either of the two previous distributions; we model it by the uniform distribution over all intensities,

and we expect the weight~ of this component to be close to zero in standard IMSCs, a deviation indicating some anomaly in the measurement.
\end{itemize}

We interpret the smoothed observed IMSC~ as a sample of a mixture of these three components with unknown mixture coefficients.
To illustrate this approach, consider Figure~\ref{fig:denoisingmodels}, which shows the empirical intensity distribution of an IMSC (histogram), together with the estimated components (except the uniform distribution, which has the expected coefficient of almost zero).
\begin{figure}[t]
  \centering
  \includegraphics[width=0.98\textwidth]{histogram-denoising-models}
  \caption{Histogram of a smoothed IMSC~ (green bars) and estimated distribution of the noise component (red line) and of the signal component (blue line). 
Parameters for both components were estimated with the EM~algorithm.}
  \label{fig:denoisingmodels}
\end{figure}

It follows that there are six independent parameters to estimate: , , ,  and weights  (noise, signal, background, where ).

\paragraph{Initial parameter values}
As the first and last~ of data points in each spectrum can be assumed to contain no signal, we use their intensities' empirical mean and standard deviation as starting values for~ and~, respectively.
The initial weight of the noise component is set to cover most points covered by this Gaussian distribution, i.e., .

We assume that almost all of the remaining weight belongs to the signal component, thus , and . 

To obtain initial parameters for the signal model, let  (the complement of the intensities that are initially assigned to the noise component).
We set  and  (which are the maximum likelihood estimators for Inverse Gaussian parameters).

\paragraph*{Maximum likelihood estimators}
In the maximization step (M-step) we estimate maximum likelihood parameters for the non-uniform components.
In all sums,~ extends over the whole matrix index set~.

\paragraph*{Final step}
After convergence (8--10 EM~loops in practice), the denoised signal matrix~ is computed as follows:

\subsection{Baseline Correction}
\label{sec:baseline}

\paragraph*{Background}
In an IMSC, the RIP with its long tail interferes with peak detection; it is present is each spectrum and hence called the \emph{baseline}.
The goal of this section is to remove the baseline and better characterize the remaining peaks.

We consider every chromatogram (column of the matrix shown in Figure~\ref{fig:visualized-imsc}) separately.
The idea is to consider intensities that appear at many retention times as part of the baseline.
By analyzing the histogram~ of chromatogram~ (with bin size~, since signal values are integers), we observe that frequently occurring signals that are produced by the IM device itself or by drift gas, build the highest peak in the histogram, consider Figure~\ref{fig:histogram-chromatograms} (top).
On the other hand, histograms of chromatograms that are only negligibly influenced by the RIP have a peak in the range of the background noise mean, see Figure~\ref{fig:histogram-chromatograms} (bottom).
\begin{figure}[t]\centering
\includegraphics[width=0.48\columnwidth]{histogram-chromatogram-rip}
\includegraphics[width=0.48\columnwidth]{histogram-chromatogram-line}
\caption{Two typical histograms of chromatogram signal intensities.\
Top: Most data points of a RIP chromatogram consist of high values with high variance.
Bottom: Far away from the RIP, only few values right of background noise are produced by peaks.
}
\label{fig:histogram-chromatograms}
\end{figure}

\paragraph{Mixture model}
We make the following assumption based on observations of chromatogram intensities:
\begin{itemize}
\item The intensities belonging to the baseline are normally distributed around their mean,

\item The remaining intensities belong to the signal of interest and can have any value above the baseline, so they are modeled by a uniform distribution between the minimum value~ and maximum value~ in the chromatogram~ at drift time~,

\end{itemize}

\paragraph*{Initial parameter values}
The start parameter for~ is the most frequent intensity in the chromatogram (the mode of the histogram); we also set~ and , .

\paragraph*{Maximum likelihood estimators}
The new values for mean and standard deviation of~ are estimated by the standard maximum likelihood estimators, weighted by component membership.
The following formulas apply to a single chromatogram~.


\paragraph*{Final step}
When the parameters converge, the baseline intensity for~ is estimated at~ (note that we omitted the index~ for~ and~, as each chromatogram is processed independently).
This baseline value is subtracted from each intensity in the chromatogram, setting resulting negative intensities to zero.
In other words, the new matrix is .



\subsection{Clustering}
\label{sec:clustering}

\paragraph*{Background}
Peaks in several measurements are described by their location in retention time and reduced inverse mobility (RIM).
Let~ be a union set of peak locations from different measurements with~ being the number of peaks and~ the retention time of peak~ and~ its RIM.
We assume that due to the slightly inaccurate capturing process, a peak (produced by the same compound) that appears in different measurements has slightly shifted retention time and RIM.
We introduce a clustering approach using standard 2-dimensional Gaussian mixtures, but with dynamically adjusting the number of clusters in the process.

\paragraph{Mixture model}
We assume that the measured retention times and RIMs belonging to peaks from the same compound are independently normally distributed in both dimensions around the (unknown) component retention time and RIM.
Let  be the parameters for component~, and let  be a two-dimensional Gaussian distribution for a peak location  with these parameters,

The mixture distribution is  with a yet undetermined number~ of clusters.
Note that there is no ``background'' model component.

\paragraph*{Initial parameter values}
In the beginning, we initialize the algorithm with as many clusters as peaks, i.e., we set~.
This assignment makes a background model obsolete, because all peaks are assigned to at least one cluster.
All clusters get as start parameters for  the original retention time and  RIM of peak location~, respectively, for~.

Remark that we are using in this description not the indices but the actual measures.
We set  and  according to the peak characterizations by \cite{bodeker2008peak}, dividing by~3 to let~ since due to the strong skewed peaks in retention time the area under the curve is asymmetric.

\paragraph*{Dynamic adjustment of the number of clusters}
After computing weights in the E-step, but before starting the M-step, we dynamically adjust the number of clusters by merging clusters whose centers are close.
Every pair~ of clusters is compared in a nested for-loop.
When  and , then clusters~ and~ are merged by summing the weights  and  for all~, and these are assigned to the location of the cluster with larger weight.
(The re-computation of the parameters happens immediately after merging in the maximization step.)
The comparison order may matter in rare cases for deciding which peaks are merged first, but since new means and variances are computed, possible merges that were omitted in the current iteration, will be performed in the next iteration.
This merging step is applied first time in the second iteration, since the cluster means need at least one iteration to move towards each other.

\paragraph*{Maximum likelihood estimators}
The maximum likelihood estimators for mean and variance of a two-dimensional Gaussian are the standard ones, taking into account the membership weights,

for all components~.

One problem using this approach emerges from the fact that initially each cluster contains only one peak, leading to an estimated variance of zero in many cases.
To prevent this, minimum values are enforced such that   and  for all~.

\paragraph*{Final step}
The EM~loop terminates when no merging occurs and the convergence criteria for all parameters are fulfilled.
The resulting membership weights determine the number of clusters as well as the membership coefficient of peak location~ to cluster~.
If a hard clustering is desired, the merging step has to be protocoled.
At the beginning all peak indexes are singletons within their own sets.
By merging, the sets of both peaks are merged.



\section{Evaluation}
\label{sec:eval}

In this section, we evaluate our algorithms against existing state-of-the-art ones on simulated data.
We first discuss general aspects of generating simulated IMSCs (Section~\ref{sec:eval:general}) and then report on the evaluation results for denoising (Section~\ref{sec:eval:denoising}), baseline correction (Section~\ref{sec:eval:baseline}) and peak clustering (Section~\ref{sec:eval:clustering}).

\subsection{Data Generation and Similarity Measure}
\label{sec:eval:general}

Since we do not have ``clean'' real data, we decided to simulate IMSCs and add noise with the same properties as observed in real IMS datasets.
We generate simulated IMSCs of~ retention time points and~ RIM points with several peaks (see below), subsequently add noise (see below), apply our and competing algorithms and compare the resulting IMSCs with the original simulated one.

\paragraph{Simulating IMSCs with peaks}
A peak in an IMSC can be described phenomenologically by a two-dimensional shifted inverse Gaussian (IG) distribution \citep{Kopczynski/etal/2012a}.
The one-dimensional shifted IG is defined by the probability density

where~ is an offset value.
The density of a peak is

where  is the volume of the peak and .

Since the parameters~ vary strongly on similar shapes, it is more intuitive to describe the function in terms of three descriptors, the mean~, the standard deviation~ and the mode~.
There is a bijection between~ and~ given by

and the model parameters~ can be uniquely recovered from these descriptors.

The descriptors are drawn uniformly from the following intervals (the unit for retention times is~s, the unit for RIMs is~, and volumes~ are given in arbitrary volume units):
\begin{center}\begin{tabular}{r@{ }c@{ }l}
 &  & \\
 &  & \\
 &  & \\
 &  &  \\
 &&  \\
 &  & \\
 &  & \\
\end{tabular}\end{center}



A simulated IMSC is generated as follows.
A number~ of peaks is determined randomly from an interval (e.g., 5--10).
Peak descriptors are randomly drawn for each peak from the above intervals, and the model parameters~ are computed for~.
The IMSC~ is generated by setting  for .

\paragraph{Generating noisy IMSCs}
Starting with a peak-containing IMSC, we add normally distributed noise with parameters ,  (both in signal units), estimated from background noise of original IMSCs, to each data point~.

Additionally, due to the device properties, the intensities in a spectrum are oscillating with a low frequency  that may change with retention time~.
Thus we add  to~ where~ is the intensity factor (note:  is our factor to compute drift times in RIMs).
Our tests showed that~ in practice.
The IMSC with added noise is called~.

\paragraph{Comparing IMSCs}
As a similarity measure betweens IMSCs~ and~, we use cosine similarity,

We have~, where~ means both that matrices are identical,~ means that the values are ``orthogonal'' and~ means that~.
In fact, the similarity measure is the cosine of the angle between the IMSCs when interpreted as vectors.

\subsection{Denoising}
\label{sec:eval:denoising}

\begin{figure}[t]\centering
\includegraphics[width=0.98\columnwidth]{histogram-denoising}
\caption{Histogram of different methods' cosine similarity scores by comparing 100 original IMSCs with their denoised counterparts after adding simulated noise.}
\label{fig:denoisingres}
\end{figure}

We compared our method with current denoising and smoothing methods:
(1) Gaussian smoothing, (2) a Savitzky-Golay filter and (3) a low-pass filter utilizing the fast Fourier transform.

We first set up 100 different simulated IMSCs of~ retention time points and~ RIM points, with 5--10 peaks, where the number of peaks is chosen randomly in this range, as described in Section~\ref{sec:eval:general}. 
These IMSCs are called~, .
We then add normal and sinusoid noise to each IMSC to obtain~, .
We denoise the~ using our algorithm and the three above methods.
Let the resulting matrices be~ (our EM~algorithm),~ (low-pass filter),~ (Gaussian smoothing) and~ (Savitzky-Golay).
We compare each of these resulting matrices to the initial, noise-free matrix~ using the cosine similarity measure described above.

We compute the cosine similarity~ between original and denoised IMSC.
with each algorithm~.
We show the histograms of the cosine similarity score of these 100 test cases in Figure~\ref{fig:denoisingres}.
The noisy IMSCs denoised with EM~denoising achieve higher similarity scores than by the other methods.


\subsection{Baseline Correction}
\label{sec:eval:baseline}

\begin{figure}[t]\centering
\includegraphics[width=0.98\columnwidth]{histogram-baseline-wn}
\caption{Histogram of cosine similarity between initial simulated IMSCs and after baseline correction of the baseline-modified IMSCs with different algorithms.}
\label{fig:baselineresults}
\end{figure}

We compare the EM~baseline correction from Section~\ref{sec:baseline} with two additional methods.
\begin{enumerate}
\item
The first method (``naive'') subtracts a spectrum containing only baseline points from all remaining spectra.
Typically the first spectrum in an IMSC (captured after~) consists only of a baseline, since even the smallest analytes need some time to pass the MCC.
After subtraction all negative values are set to zero.
\item
The second method (``median'') computes the median in every chromatogram separately and subtracts it from all values in the chromatogram.
Resulting negative values are set to~.
\end{enumerate}

We simulate IMSCs~ with 5--10 peaks for~, and add normally distributed noise with an overlayed sinusoidal wave, as described in Section~\ref{sec:eval:general}, and then add a baseline to each spectrum in the IMSC based on the following considerations.
\begin{enumerate}
\item In theory, the amount of molecules getting ionized before entering the drift tube (and hence the sum over all intensities within a spectrum) is constant over spectra.
In practice, the amount varies and is observed to be normally distributed with a mean of about~ signal units and a standard deviation of about~ signal units.
The signal intensity sum~ for the~-th spectrum is obtained by drawing from this normal distribution.
\item To obtain the signal intensity for non-peaks, we subtract the signal intensity consumed by simulated peaks in this spectrum. 
Let~ index peaks in a given IMSC, and let~ be the signal intensity of the~-th peak at coordinates~.
Thus we compute .
We repeat this process to obtain~ for every IMSC indexed by~.
\item The baseline~ is modeled by two Inverse Gaussian distributions, one for the RIP~( component) and one for the heavy tail~( component) of the RIP (cf.\ the work by \cite{bader2008preprocessing}, who used the log-normal distribution for the tail).

where~ was defined in Eq.~\eqref{eq:inversegauss} and the parameters are set to or uniformly drawn from
\begin{center}\begin{tabular}{r@{ }c@{ }l}
 &  & \\
 &  & \\
 &  & \\
 &  &  \\
 &  & \\
 &  & \\
 &  & \\
\end{tabular}\end{center}
where all units are~, except for~. We repeat this process for every IMSC to obtain~ for~.
\item The IMSC with baseline~ is obtained from the original IMSC~ by

for , , .
\end{enumerate}
We apply the three algorithms (EM, naive, median) to obtain~ (for our EM-based method),~ (naive) and~ (median) and measure the cosine similarity~ for all~ and algorithms~ and plot the results in Figure~\ref{fig:baselineresults}.
In average, the EM~baseline correction performs best in terms of cosine similarity.
Note that there is no explicit denoising performed in this experiment.




\subsection{Clustering}
\label{sec:eval:clustering}

\begin{figure}[t]\centering
\includegraphics[width=0.48\columnwidth]{histogram-fmi-wn}
\includegraphics[width=0.48\columnwidth]{histogram-nvi-wn}
\caption{
Histograms of Fowlkes-Mallows index (FMI; higher is better) and normalized variation of information (NVI; lower is better) comparing 100 simulated measurements containing partitioned peak locations with their clusters produced by the different methods.
EM~clustering achieves slightly better results than DBSCAN.
-means can also come up.
}
\label{fig:clusterresultswn}
\end{figure}

\begin{figure}[t]\centering
\includegraphics[width=0.48\columnwidth]{histogram-fmi}
\includegraphics[width=0.48\columnwidth]{histogram-nvi}
\caption{
Histograms similar to Figure~\ref{fig:clusterresultswn}, but in a more realistic scenario with noise added.
Again, EM~clustering achieves in both score better results in average.
-means performs less well because of insufficient choice of starting points when having much noise.
}
\label{fig:clusterresults}
\end{figure}


To evaluate peak clustering methods, we simulate peak locations according to locations in real MCC/IMS datasets, together with the true partition~ of peaks.

Most of the detected peaks appear in a small dense area early in the measurement, since many volatile compounds have a small chemical structure like ethanol or acetone.
Remaining peaks are distributed widely, which is referred to as the sparse area.
The areas have the following boundaries(in units of~(, s) from lower left to upper right point, cf.\ Figure~\ref{fig:visualized-imsc}:
\begin{center}\begin{tabular}{r@{ }l}
measurement: & \\
dense area: & \\
sparse area: & 
\end{tabular}\end{center}

Peak clusters are ellipsoidal and dense.
From \cite{bodeker2008peak} we know the minimum required distance between two peaks in order to be identified as two separate compounds.
We simulate peak cluster centroids, 30 in the dense area and 20 in the sparse area, all picked randomly and uniformly distributed.
We then randomly pick the number of peaks per cluster.
We also randomly pick the distribution of peaks within a cluster.
Since we do not know the actual distribution model, we decided to simulate with three models: normal~(n), exponential~(e) and uniform~(u) distribution with the following densities:

Here  is the coordinate of the centroid with RIM in  and retention time in~s.
For the normal distribution,  and .
For exponential distribution,  (reduced mobility width for in single cell within~) and~.
For the uniform distribution, we use an ellipsoid with radii  and .

We compared the EM~clustering with two common clustering methods, namely~-means and DBSCAN.
Since~-means needs a fixed~ for the number of clusters and appropriate start values for the centroids, we decided to take~-means++ (described by~\cite{arthur/etal/2007}) for estimating good starting values and give it an advantage by assigning the true number of partitions.
DBSCAN has the advantage that it does not need a fixed number of clusters, but on the other hand it has some disadvantages.
It finds clusters with non-linearly separable connections, but we assume that the partitions obey a kind of model with convex hull.
On the other hand it yields no parameters describing the clusters.
Such parameters can be very important when using the clusters as features for a consecutive classification.

To measure the quality of the clustering~ we take two measures in consideration: the Fowlkes-Mallows index (FMI) first described by~\cite{fowkles/etal/1983} as well as the normalized variation of information (NVI) score introduced by~\cite{reichart/etal/2009}.


For the FMI one has to consider all pairs of data points.
If two data points belong into the same true partition of~, they are called~\emph{connected}.
Accordingly, a pair of data points is called~\emph{clustered} if they are clustered together by the clustering method we want to evaluate.
Pairs of data points, which are marked as connected as well as clustered, are referred to as true positives (TP).
False positives (FP, not connected but clustered) and false negatives (FN, connected but not clustered) are computed, analogously.
The FMI is the geometric mean of precision and recall, let \hbox{} where~ is the partition set and~ the clustering.
Since \hbox{},  means no similarity between both clusterings and  means that the clusterings agree completely.
Although the FMI determines the similarity between two clusterings, it yields unreliable results when the number of clusters in both clusterings differs significantly.

Thus we use a second measure that considers clusters sizes only, the normalized variation of information (NVI).
To compute the NVI, an auxiliary~-dimensional matrix~ has to be set up.
Thereby~ determines the number of data points within partition~ that are assigned to cluster~.
Using entropies, we can now determine the NVI score.
Define

where~ is the number of data points.
 means no variation between original partition and clustered data.
An FMI score~ and NVI score~ indicates a perfect clustering.

For the first test we generated 100 sets of data points where the partitions is known, as previously described.
In the second step performed an EM~clustering as well as~-means and DBSCAN for every set.
Finally we computed the both scores FMI and NVI for all sets.
Our results show that even with the unfair~-means our EM~clustering performs best in terms FMI and NVI score.
It achieves in average best results, Figure~\ref{fig:clusterresultswn} shows two histograms of both FMI and NVI for all three methods.
Since this scenario is little realistic, we performed a second test.
The difference to the first test is that we insert 200 equally distributed peaks randomly into the measurement area.
All these peaks are singletons within the partition set.
We denote the additional peaks as noise.
After performing the second test, we can see that EM~clustering still achieves best results in average, whereas  -means completely fails although we forward the correct~, because of insufficient determination of start points and no noise handling.
All FMI and NVI scores from the second test are plotted as a histogram in Figure~\ref{fig:clusterresults}.





\section{Discussion and Conclusion}
\label{sec:discussion}

We have presented three novel methods for certain problems i.e. denoising, baseline correction and clustering.
All methods utilize a modified version of the EM~algorithm for a deconvolution of mixture models.
Since our research is located in spectra analysis of ion MCC/IMS devices, these methods are adjusted for this application field, but can easily be adapted for other purposes.
In all tests our methods performed with best results.
Because of lack of the truth behind original measurements, we simulated test data using properties of real MCC/IMS measurements.
All methods are being applied for automated breath gas analysis to improve the accuracy of disease prediction, as previously evaluated by~\cite{hauschild2013eval}.

Supplementary material (parameter lists for all denoising and baseline correction tests as well as peak lists for clustering) are available at \url{http://www.rahmannlab.de/research/ims}.




\paragraph*{Acknowledgements}
DK, SR are supported by the Collaborative Research Center (Sonderforschungsbereich,~SFB) 876 ``Providing Information by Resource-Constrained Data Analysis'' within project TB1, see \url{http://sfb876.tu-dortmund.de}.
\bibliographystyle{natbib}
\begin{thebibliography}{}

\bibitem[Arthur and Vassilvitskii(2007)Arthur and
  Vassilvitskii]{arthur/etal/2007}
Arthur, D. and Vassilvitskii, S. (2007).
\newblock K-means++: The advantages of careful seeding.
\newblock In {\em Proceedings of the Eighteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms\/}, SODA '07, pages 1027--1035. Society for Industrial
  and Applied Mathematics.

\bibitem[Bader {\em et~al.}(2007)Bader, Urfer, and
  Baumbach]{bader2007reduction}
Bader, S., Urfer, W., and Baumbach, J.~I. (2007).
\newblock Reduction of ion mobility spectrometry data by clustering
  characteristic peak structures.
\newblock {\em Journal of Chemometrics\/}, {\bf 20}(3-4), 128--135.

\bibitem[Bader {\em et~al.}(2008)Bader, Urfer, and
  Baumbach]{bader2008preprocessing}
Bader, S., Urfer, W., and Baumbach, J.~I. (2008).
\newblock Preprocessing of ion mobility spectra by lognormal detailing and
  wavelet transform.
\newblock {\em International Journal for Ion Mobility Spectrometry\/}, {\bf
  11}(1-4), 43--49.

\bibitem[B{\"o}deker {\em et~al.}(2008a)B{\"o}deker, Vautz, and
  Baumbach]{bodeker2008peakcomp}
B{\"o}deker, B., Vautz, W., and Baumbach, J.~I. (2008a).
\newblock Peak comparison in {MCC/IMS}-data -- searching for potential
  biomarkers in human breath data.
\newblock {\em International Journal for Ion Mobility Spectrometry\/}, {\bf
  11}(1-4), 89--93.

\bibitem[B{\"o}deker {\em et~al.}(2008b)B{\"o}deker, Vautz, and
  Baumbach]{bodeker2008peak}
B{\"o}deker, B., Vautz, W., and Baumbach, J.~I. (2008b).
\newblock Peak finding and referencing in {MCC}/{IMS}-data.
\newblock {\em International Journal for Ion Mobility Spectrometry\/}, {\bf
  11}(1), 83--87.

\bibitem[Bunkowski(2011)Bunkowski]{bunkowski_2011}
Bunkowski, A. (2011).
\newblock {\em {MCC}-{IMS} data analysis using automated spectra processing and
  explorative visualisation methods\/}.
\newblock Ph.D. thesis, University Bielefeld: Bielefeld, Germany.

\bibitem[Bunkowski {\em et~al.}(2009)Bunkowski, B{\"o}deker, Bader, Westhoff,
  Litterst, and Baumbach]{bunkowski/etal/2009a}
Bunkowski, A., B{\"o}deker, B., Bader, S., Westhoff, M., Litterst, P., and
  Baumbach, J.~I. (2009).
\newblock {MCC/IMS} signals in human breath related to sarcoidosis -- results
  of a feasibility study using an automated peak finding procedure.
\newblock {\em {Journal of Breath Research}\/}, {\bf 3}(4), 046001.

\bibitem[D'Addario {\em et~al.}(2014)D'Addario, Kopczynski, Baumbach, and
  Rahmann]{daddario/etal/2014a}
D'Addario, M., Kopczynski, D., Baumbach, J.~I., and Rahmann, S. (2014).
\newblock A modular computational framework for automated peak extraction from
  ion mobility spectra.
\newblock {\em BMC Bioinformatics\/}, {\bf 15}(1), 25.

\bibitem[Dempster {\em et~al.}(1977)Dempster, Laird, and
  Rubin]{Dempster/etal/1977a}
Dempster, A.~P., Laird, N.~M., and Rubin, D.~B. (1977).
\newblock Maximum likelihood from incomplete data via the {EM} algorithm.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)\/}, pages 1--38.

\bibitem[Eiceman and Karpas(2010)Eiceman and Karpas]{eiceman/2010a}
Eiceman, G.~A. and Karpas, Z. (2010).
\newblock {\em Ion mobility spectrometry\/}.
\newblock CRC press.

\bibitem[Ester {\em et~al.}(1996)Ester, Kriegel, Sander, and
  Xu]{ester/etal/1996}
Ester, M., Kriegel, H.-P., Sander, J., and Xu, X. (1996).
\newblock A density-based algorithm for discovering clusters in large spatial
  databases with noise.
\newblock In {\em Knowledge Discovery and Data Mining (KDD), Proceedings of
  first international conference\/}, volume~96, pages 226--231.

\bibitem[Fowlkes and Mallows(1983)Fowlkes and Mallows]{fowkles/etal/1983}
Fowlkes, E.~B. and Mallows, C.~L. (1983).
\newblock A method for comparing two hierarchical clusterings.
\newblock {\em Journal of the American Statistical Association\/}, {\bf
  78}(383), 553--569.

\bibitem[Hauschild {\em et~al.}(2013)Hauschild, Kopczynski, D'Addario,
  Baumbach, Rahmann, and Baumbach]{hauschild2013eval}
Hauschild, A.~C., Kopczynski, D., D'Addario, M., Baumbach, J.~I., Rahmann, S.,
  and Baumbach, J. (2013).
\newblock Peak detection method evaluation for ion mobility spectrometry by
  using machine learning approaches.
\newblock {\em Metabolites\/}, {\bf 3}(2), 277--293.

\bibitem[Kopczynski {\em et~al.}(2012)Kopczynski, Baumbach, and
  Rahmann]{Kopczynski/etal/2012a}
Kopczynski, D., Baumbach, J.~I., and Rahmann, S. (2012).
\newblock Peak modeling for ion mobility spectrometry measurements.
\newblock In {\em Signal Processing Conference (EUSIPCO), 2012 Proceedings of
  the 20th European\/}, pages 1801--1805. IEEE.

\bibitem[MacQueen(1967)MacQueen]{macqueen1967kmeans}
MacQueen, J. (1967).
\newblock Some methods for classification and analysis of multivariate
  observations.
\newblock In {\em Proceedings of the fifth Berkeley symposium on mathematical
  statistics and probability\/}, volume 1: Statistics, pages 281--297.
  University of California Press.

\bibitem[Reichart and Rappoport(2009)Reichart and
  Rappoport]{reichart/etal/2009}
Reichart, R. and Rappoport, A. (2009).
\newblock The {NVI} clustering evaluation measure.
\newblock In {\em Proceedings of the Thirteenth Conference on Computational
  Natural Language Learning\/}, pages 165--173. Association for Computational
  Linguistics.

\bibitem[Westhoff {\em et~al.}(2009)Westhoff, Litterst, Freitag, Urfer, Bader,
  and Baumbach]{Westhoff/etal/2009a}
Westhoff, M., Litterst, P., Freitag, L., Urfer, W., Bader, S., and Baumbach, J.
  (2009).
\newblock Ion mobility spectrometry for the detection of volatile organic
  compounds in exhaled breath of lung cancer patients.
\newblock {\em Thorax\/}, {\bf 64}, 744--748.

\bibitem[Westhoff {\em et~al.}(2010)Westhoff, Litterst, Maddula, B{\"o}deker,
  Rahmann, Davies, and Baumbach]{westhoff2010differentiation}
Westhoff, M., Litterst, P., Maddula, S., B{\"o}deker, B., Rahmann, S., Davies,
  A.~N., and Baumbach, J.~I. (2010).
\newblock Differentiation of chronic obstructive pulmonary disease ({COPD})
  including lung cancer from healthy control group by breath analysis using ion
  mobility spectrometry.
\newblock {\em International Journal for Ion Mobility Spectrometry\/}, {\bf
  13}(3-4), 131--139.

\end{thebibliography}

\end{document}
