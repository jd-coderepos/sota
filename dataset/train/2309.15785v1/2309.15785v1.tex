
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{bbding}
\usepackage{comment}


\title{One For All: Video Conversation is Feasible Without Video Instruction Tuning}




\author{{Ruyang Liu*}~$^{1}$
    ~~~{Chen Li}~$^{2}$
    ~~~{YiXiao Ge}~$^{2}$
    ~~~{Ying Shan}~$^{2}$
    ~~~Thomas H. Li$^{1}$ 
    ~~~{Ge Li \footnotesize{\Envelope}}$^{1}$\\
    {\small $^1$School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University} \\ 
    {\small $^2$Applied Research Center (ARC), Tencent PCG}\\ 
    {\tt\small \{ruyang@stu,geli@ece,thomas@\}.pku.edu.cn}\\
    {\tt\small\{palchenli,yixiaoge,yingsshan\}@tencent.com}\\  
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
The recent progress in Large Language Models (LLM) has spurred various advancements in image-language conversation agents, 
while how to build a proficient video-based dialogue system is still under exploration. 
Considering the extensive scale of LLM and visual backbone, minimal GPU memory is left for facilitating effective temporal modeling, which is crucial for comprehending and providing feedback on videos. 
To this end, we propose Branching Temporal Adapter (BT-Adapter), a novel method for extending image-language pretrained models into the video domain. Specifically, BT-Adapter serves as a plug-and-use temporal modeling branch alongside the pretrained visual encoder, which is tuned while keeping the backbone frozen. 
Just pretrained once, BT-Adapter can be seamlessly integrated into all image conversation models using this version of CLIP, enabling video conversations without the need for video instructions.
Besides, we develop a unique asymmetric token masking strategy inside the branch with tailor-made training tasks for BT-Adapter, facilitating faster convergence and better results.
Thanks to BT-Adapter, we are able to empower existing multimodal dialogue models with strong video understanding capabilities without incurring excessive GPU costs.
Without bells and whistles, BT-Adapter achieves (1) state-of-the-art zero-shot results on various video tasks using thousands of fewer GPU hours. (2) better performance than current video chatbots without any video instruction tuning. (3) state-of-the-art results of video chatting using video instruction tuning, outperforming previous SOTAs by a large margin.
\end{abstract}

\vspace{-0.8em}

\section{Introduction} \label{intro}
\renewcommand{\thefootnote}{ } 
\footnotetext{*Work done during internship at ARC Lab, Tencent PCG ~~~~~~~\Envelope Corresponding Author}


Since the past year, Large Language Model (LLM) chatbots \citep{touvron2023llama, wang2022self, ouyang2022training, zeng2022glm} have emerged as one of the most remarkable advancements within the AI community. Excitingly, the LLM-driven AI assistants have showcased impressive abilities in comprehension, reasoning, and engaging in conversations. 
The success of text-only dialogue systems has also sparked the development of image-language conversation agents \citep{liu2023visual, dai2023instructblip, zhu2023minigpt, gao2023llama}. These agents combine pretrained image models with LLMs, followed by instruction tuning, to create a fusion of visual and textual understanding for enhanced conversation capabilities.

In comparison to images, videos offer a more comprehensive representation of how humans perceive and interpret the world. Nevertheless, constructing a video-centric dialogue system is a more complex endeavor compared to the image-based one. Firstly, a typically multimodal agent, comprising LLM, a pretrained image encoder (usually CLIP-L \citep{radford2021learning}), and additional learnable modules, is already demanding in terms of GPU memory. The incorporation of video dialogue introduces even higher costs, as multiple frames need to be fed as inputs. Secondly, a pretrained encoder as potent and knowledge-enriched as CLIP is currently lacking in the realm of videos, leading to inferior visual understanding. Lastly, gathering video-text instruction data of comparable scale, quality, and diversity to images poses a significant challenge. Rather than building the video agent from scratch, an alternative and promising strategy involves adapting existing pretrained image-centric dialogue models to the video domain \citep{li2023videochat, maaz2023video, zhang2023video, luo2023valley}, which can leverage the abundant knowledge embedded within image models. 

\begin{figure*}[t] 
    \setlength{\abovecaptionskip}{-0.01 cm}
\centering
    \includegraphics[width=1\textwidth]{image/iclr_into.pdf} 
    \captionsetup{font={footnotesize}}
    \caption{The performance overview of our BT-Adapter. On the left, we report zero-shot Recall@1 on MSRVTT \citep{xu2016msr} vs. pretraining GPU hours. On the right, we provide a quantitative comparison of video conversations among existing video dialogue agents.}
    \label{image_intro}
    \vspace{-2em}
  \end{figure*}



Expanding pretrained image models to the video domain is an extensively explored area in computer vision. The crux of the matter lies in enhancing the capability of 2D models to model temporal dynamics \citep{bertasius2021space, carreira2017quo, tran2015learning}. 
However, implementing effective temporal modeling within image dialogue models presents notable challenges, primarily due to the trade-off between efficiency and effectiveness in CLIP-based temporal modeling techniques. Methods with strong temporal modeling capabilities, \textit{e.g.,} joint-ST modeling \citep{bertasius2021space, wang2022internvideo, li2023unmasked, xue2022clip} and interpolate-style modeling \citep{ni2022expanding, pan2022st}, often necessitate finetuning of the entire visual encoder, thereby exacerbating the already significant GPU memory consumption of video conversation models. Moreover, full-finetuning would lead to the loss of knowledge encapsulated in pretrained models, resulting in reduced performance for image-based conversations
In contrast, methods that focus on parameter-efficient temporal modeling aim to keep the image encoder frozen and introduce only a few trainable parameters, which are embraced by most video dialogue models. However, these approaches typically provide limited temporal modeling and struggle to capture crucial spatial-temporal features.

To tackle the problems in the aforementioned methods, we proposed Branching Temporal Adapter (BT-Adapter), a novel framework to migrate image-text pretrained models to the video domain. As the name implies, BT-Adapter incorporates a branching spatial-temporal module for temporal modeling. Alongside the pretrained image model, BT-Adapter inherits the parameter efficiency advantage from conventional adapters \citep{houlsby2019parameter} while concurrently achieving effective temporal modeling. Notably, unlike the plug-style adapter, BT-Adapter does not disrupt the forward progression of the pretrained models, thereby safeguarding the integrity of the pretrained multimodal knowledge. After being pretrained with any version of CLIP, BT-Adapter can be seamlessly integrated with all image conversation models using this version of CLIP to activate video conversation, without necessitating video instructions, \textit{e.g.}, openai-CLIP for LLaVa, Eva-CLIP for MiniGPT4 and InstructionBLIP. Additionally, we have devised a unique asymmetric masking mechanism that exclusively implements tube token masking within the BT-Adapter. Building upon this, we formulate two custom training objectives for the BT-Adapter: Masked Branching Token Alignment (MBTA) and Masked Branching Cross-modal Alignment (MBCA). This approach not only reduces computational demands and accelerates convergence but also yields improved outcomes.

To validate the effectiveness and efficiency of our BT-Adapter, we provide a detailed analysis of various temporal modeling strategies for video conversation in Sec. \ref{method_temp}. Furthermore, in Sec. \ref{exp}, we carry out extensive qualitative and quantitative experiments on BT-Adapter, encompassing both traditional video tasks and video conversations.
As depicted in Fig. \ref{image_intro}(left), a series of designs ensures that BT-Adapter is highly resource-efficient: our post-pretraining demands just \textbf{8 V100(32G)} GPUs in a mere \textbf{3 hours}, leading to a reduction in carbon emissions by more than \textbf{10240$\times$} and \textbf{2687$\times$} compared to CoCa \citep{yu2022coca} and InternVideo \citep{wang2022internvideo} respectively. Building upon this, we still achieve state-of-the-art results in zero-shot video-text retrieval. Regarding video conversation, as shown in Fig. \ref{image_intro}(right), we clearly demonstrate that fine-tuned BT-Adapter surpasses the previous state-of-the-art by a significant margin across all benchmarks, and BT-Adapter without instruction tuning has better average performance than fine-tuned SOTAs. 


\section{Related Work}
\vspace{-0.5em}
\noindent \textbf{Temporal Modeling on Image-Language Pretrained Models.}
With the wide application of pretrained image-language models \citep{radford2021learning,yu2022coca}, how to extend these pretrained multimodal models into the video domain has emerged as a novel yet critical research topic. 
Existing temporal modeling methods can be broadly categorized into several types. 
The most straightforward and commonly used one is joint-spatial-temporal modeling \citep{bertasius2021space, wang2022internvideo, li2023unmasked, xue2022clip}. By inputting all video tokens into the image encoder, this approach can effectively model temporal dependencies without other techniques. However, joint-ST modeling necessitates fine-tuning the entire encoder, resulting in significant computational costs and the degradation of pretrained knowledge. 
Another representative type is the interpolate-style temporal modeling, including separated-ST modeling \citep{bertasius2021space, zeng2023tvtsv2}, message token \citep{ni2022expanding}, and ST-Adapter \citep{pan2022st}. Inserting temporal modules between the pretrained spatial layers, interpolate-style modeling shares similar cons and pros with joint-ST modeling. Specifically, ST-Adapter is also known as the parameter-efficient temporal module, where the distinctions with our methods are detailed in Sec. \ref{similar_sec} in the Appendix.
Different from the former two types, concatenate-style temporal modeling \citep{luo2022clip4clip, fang2021clip2video} attaches temporal modules following the pretrained encoder, which allows for the freezing of the backbone and the preservation of knowledge. Nevertheless, this style offers limited temporal modeling, as it hardly captures crucial low-level spatial-temporal features. In Sec. \ref{method_temp}, we will experimentally compare the different temporal modeling methods for video dialogue.



In contrast to the preceding types, BT-Adapter adopts a branching structure for temporal modeling. Thanks to the intricate design, we circumvent the issues outlined above, enabling proficient temporal modeling, efficient parameter fine-tuning, and multimodal affinity simultaneously.
Most similar to our methods, STAN \citep{liu2023revisiting} also introduced branch-style temporal modeling. Nonetheless, there are three key distinctions between our work and STAN: (1) Our emphasis is on zero-shot image-to-video transfer and video conversations, areas that have never been explored by STAN. (2) Our performance is notably superior when the backbone is frozen, which is attributed to our unique design like temporal CLS token, zero-initialized temporal projection, and temporal selection. (3) We have introduced the novel training strategy and objectives tailored for the branching temporal structure. In Sec. \ref{similar_sec} in the Appendix, we will also provide an experimental comparison between BT-Adapter and its similar competitors, including ST-Adapter and STAN.

\noindent \textbf{Video Conversation.} 
Recent advancements in multimodal learning have been predominantly propelled by the fusion of visual models with LLM. \citet{yuan2021florence} initially demonstrated the potential of combining visual models with LLM. Blip-2 \citep{li2023blip} further proposed Q-former that maps visual tokens into the text embedding space. Subsequently, several methods presented visual instruction tuning \citep{liu2023visual,zhu2023minigpt,dai2023instructblip} for image-LLM to enable visual conversation. Most closely related to our topic, there have been several developments in the field of video-centric dialogue models over the past few months \citep{li2023videochat,zhang2023video,maaz2023video,luo2023valley}. 
Generally, these models consist of the visual encoder, LLM, and temporal module, tuned with video instruction data to realize video conversation. However, in pursuit of efficient training, the temporal modules utilized by these models, such as temporal position embedding \citep{zhang2023video} and temporal pooling \citep{maaz2023video,luo2023valley}, often exhibit limited temporal modeling capabilities. In contrast, our approach maintains parameter-efficient training while simultaneously delivering effective temporal modeling. Thanks to the benchmark initially raised by \citet{maaz2023video}, we can quantitatively illustrate the superiority of our methods in Sec. \ref{exp}. 
Moreover, we introduce a novel setting of zero-shot conversation by integrating pretrained temporal modules with image-centric dialogue models, demonstrating the feasibility of video conversation without any video instruction tuning. 

\vspace{-0.5em}

\section{Methodology} \label{method}

\vspace{-0.5em}

\begin{figure*}[t] 
\centering
    \includegraphics[width=1\textwidth]{image/iclr_method.pdf} 
    \captionsetup{font={footnotesize}}
    \caption{The overview of our model. The left side shows the model architecture and the data flow during pretraining. The right side depicts the pipeline of video conversation.}
    \label{image_method}
    \vspace{-0.2em}
\end{figure*}
  
In this section, we elaborate on how to efficiently enhance pretrained image-language models (\textit{e.g.,}, CLIP \citep{radford2021learning} and LLaVa \citep{liu2023visual}) with temporal modeling capabilities while preserving the multimodal pretrained knowledge. The main framework is depicted in Fig. \ref{image_method} . We will introduce the model architectures of BT-Adapter in Sec. \ref{method_arch}, the exploration of temporal modeling in video dialogue models in Sec. \ref{method_temp}, and the training strategy and objectives in Sec. \ref{method_training}.

\vspace{-0.5em}

\subsection{Branching Temporal Adapter} \label{method_arch}

\vspace{-0.5em}

As the pioneer of contrastive image-text pretraining, CLIP \citep{radford2021learning} has founded widespread application in various domains. Meanwhile, fundamental image dialogue models like Blip-2 \citep{li2023blip} and LLaVa \citep{liu2023visual} all employ CLIP as visual encoder. Hence, without losing the generalizability, we focus on the adaption of CLIP.

\noindent \textbf{Model Architecture.} 
To enable video input, within CLIP, we treat each frame as an individual image. Given a video with T frames, we divide each frame into N non-overlapping patches, represented as $V = \{V_i\}_{i=1}^T$ and $V_i = \{v_{i,j}\}_{j=0}^N$, where $v_{i,0}$ denotes the [CLS] token. Then, tokens in each frame are added with spatial position and fed independently into the $l^{th}$ CLIP layers:
\begin{equation}
\begin{aligned}
    V_i^{'(l-1)} = \mathrm{S\_attn}(\mathrm{LN}(V_i^{(l-1)})) + V_i^{(l-1)}, \\
    V_i^{(l)} = \mathrm{FFN}(\mathrm{LN}(V_i^{'(l-1)})) + V_i^{'(l-1)}, \label{s_att}
\end{aligned}
\end{equation}
where $\mathrm{S\_attn}$, $\mathrm{LN}$ and $\mathrm{FFN}$ denote spatial attention, layer normalization and feed-forward network.

In contrast to the plug-style adapter that consists of multiple independent modules, BT-Adapter is a continuous network operating as a branch alongside the main backbone. Inside BT-Adapter, we adopt divided space-time attention \citep{bertasius2021space}. Given all video tokens, we first gather patch tokens in the same position across different frames, obtaining $\hat{V} = \{\hat{V_j}\}_{j=1}^N$ and $\hat{V_j} = \{\hat{v}_{i,j}\}_{i=1}^T$. Then, tokens in each position are fed individually into the $l^{th}$ temporal layers:
\begin{equation}
    \hat{V}_j^{'(l-1)} = \mathrm{W_t} \cdot  \mathrm{T\_attn}(\mathrm{LN}(\hat{V}_j^{(l-1)})) + \hat{V}_j^{(l-1)}, \\  
\end{equation}
where $\mathrm{T\_attn}$ is the self-attention operated on the T dimension. $\mathrm{W_t}$ is an additional zero-initialized linear projection, which keeps the training stable during the adaption. Then, the position-wise tokens are reshaped into frame-wise tokens $\{\hat{V'}_i\}_{i=1}^T$, and fed into the spatial layer, following the same procedure as CLIP layer in Eq. \ref{s_att}.

\noindent \textbf{Backbone-Branch Interaction.} 
Unlike traditional adapters, which are added to every pretrained layer, our branching adapter has significantly fewer layers compared to the main backbone. Assuming we have K layers in the branch, BT-Adapter takes the output of the last K+1 CLIP layers as input, each layer on both sides corresponding pairwise. To construct the input of the first BT-Adapter layer from the $k^{th}$ CLIP layer, we first develop a new learnable video [CLS] token $\hat{v}_{0,0}^{(0)}$ to represent the entire video. Then, we concatenate $\hat{v}_{0,0}^{(0)}$ with $V^{(k)}$ and update the patch embeddings with frozen spatial positional embeddings and learnable temporal positional embeddings:
\begin{equation}
    \hat{v}_{i,j}^{(0)} = v_{i,j}^{(k)} + \mathrm{P}_i^t + \mathrm{P}_j^s, \\  
\end{equation}
where $P^s$ is the spatial position embedding shared with CLIP while $P^t$ is the temporal position embedding. For any other layer of BT-Adapter, we construct its input $\hat{V}^{(l)}$ from the previous branching layer and the CLIP layer at the same level with weight selecting as follows: 
\begin{equation}
    \hat{v}_{i,j}^{(l)} = \mathrm{Sigmoid(W_b)} \cdot \hat{v}_{i,j}^{(l-1)} + (1-\mathrm{Sigmoid(W_b)}) \cdot v_{i,j}^{(k+l-1)}, \\  
\end{equation}
where $\mathrm{W_b}$ is the learnable selective weight with zero initialization. At the final stage, the output of the branch and CLIP are combined for the out-of-the-box representation as follows:
\begin{equation}
    v = \mathrm{W_{v\_proj}}(\mathrm{LN}(\mathrm{Sigmoid(W_b)} \cdot \hat{v}_{0,0}^{(-1)} + (1-\mathrm{Sigmoid(W_b)}) \cdot \frac{1}{T} \sum_{i=1}^{T} v_{i,0}^{-1} )), \label{eq_combine}
\end{equation}
where $\mathrm{W_{v\_proj}}$ is the frozen weight in CLIP projecting the visual embedding into joint visual-text feature space. With CLIP kept frozen, the backbone encodes low-level spatial patterns and high-level aligned features, while the branch focuses on modeling temporal dependencies. In this way, we leverage strong temporal modeling capacities while preserving the pretrained knowledge intact.

\subsection{Temporal Modeling for Video Conversation} \label{method_temp}
In this section, we conduct an empirical study to explore potential temporal modeling strategies for video dialogue models, demonstrating the advantages of our approach. We argue that an ideal temporal modeling approach for video-LLM models should meet several criteria: it should be parameter-efficient, as dialogue models are already quite large; it should be multimodal-friendly, preserving as much multimodal alignment knowledge as possible; and it should be temporal-sensitive, delivering strong performance in time-sensitive scenarios. To measure the ``multimodal-friendly" and ``temporal-sensitive", we employ the ``Correctness of Information" and ``Temporal Understanding" metrics from the VideoChatGPT benchmark \citep{maaz2023video} respectively. ``parameter-efficient'' is simply decided by whether the CLIP can be frozen. Next, we will explain how we implement these methods for video conversation.

\begin{wraptable}{r}{10cm}
\setlength{\tabcolsep}{3pt}
\begin{center}
    \vspace{-1em}
      \caption{\small The finetuning results of different temporal modeling on video conversation. * means CLIP is frozen.} 
      \vspace{-1em}
      \label{method_study}
      \renewcommand\tabcolsep{9pt}
      \resizebox{1.\linewidth}{!}{
      \begin{tabular}{l|ccc|c|c} 
      \toprule
      \multirow{2}{*}{Model} & \multicolumn{1}{c|}{parameter-} & \multicolumn{1}{c|}{multimodal-} & \multirow{2}{*}{temporal-} & Correctness of & Temporal \\  
     & \multicolumn{1}{c|}{efficient} & \multicolumn{1}{c|}{friendly} & \multicolumn{1}{c|}{sensitive} & Information & Understanding \\ \midrule
     Baseline* &  &  & & 2.38 & 1.93 \\ 
     ST Pooling* & $\checkmark$ & $\checkmark$ & & 2.40 & 1.98 \\ 
     Joint-ST & & & $\checkmark$ & 1.92 & 2.11 \\ 
     Separate-ST & & & $\checkmark$ & 2.10 & 2.13 \\ 
     Separate-ST* & $\checkmark$ & & & 2.29 & 2.01 \\ 
     BT-Adapter* & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{2.55} & \textbf{2.26} \\ 
      \bottomrule
    \end{tabular} }
    \end{center}
    \vspace{-1.5em}
\end{wraptable}

\noindent \textbf{Spatial-Temporal Pooling.}
Following \citet{maaz2023video}, we do not integrate any module in CLIP and encode each frame independently. Then, all patch tokens $V \in \mathbf{R}^{T \times N}$ are pooled along the time ($T$) and spatial ($N$) dimensions, resulting in a total of $T+N$ tokens. These tokens are subsequently input into the LLM, which has much fewer tokens compared to joint or separate temporal modeling.

\noindent \textbf{Joint-ST modeling.} 
Following \citet{xue2022clip}, we incorporate spatiotemporal positional embeddings to 2D patches and feed all $T*N$ video tokens into CLIP and LLM simultaneously. In this way, tokens in any position or frame can attend to each other, providing a straightforward yet effective method for modeling temporal dependencies. To make training feasible, we utilize the FSDP \citep{zhao2023pytorch} to facilitate parameter and gradient sharing between GPUs.

\noindent \textbf{Separate-ST modeling.}
Unlike joint-ST modeling, separate-ST modeling retains and leverages CLIP's spatial layer. All $T*N$ patch tokens are fed in LLM. Following \citet{zeng2023tvtsv2}, we insert temporal attention before each CLIP layer and add temporal position embeddings to the input. Regarding the backbone, separate-ST modeling offers flexibility, allowing us to experiment with both frozen and unfrozen CLIP layers.

\noindent \textbf{Branch Temporal Adapter.}
We directly equip CLIP-L/14 with BT-Adapter in Sec. \ref{method_arch} to achieve video encoder. In LLava, LLM takes the output from the second-to-last layer of the visual encoder as its input. Hence, we also take the second-to-last output from both CLIP and BT-Adapter and combine them with learnable balance weight. Finally, combined patch tokens $V \in \mathbf{R}^{T \times N}$ are pooled along the time ($T$) dimension, resulting in only $N$ tokens for inputting into LLM. 

\noindent \textbf{Results.} We replace CLIP-L/14 in LLaVa with the implemented video encoders and then proceed with video instruction tuning. In all methods, we keep LLM frozen while opening the linear projection between the visual encoder and LLM. No pretraining is included for fair comparison.
Results are presented in Table \ref{method_study}. It can be observed that methods with a frozen CLIP tend to be parameter-efficient but perform poorly in terms of temporal modeling. On the other hand, methods with good temporal understanding suffer from high computation costs and knowledge loss. In contrast, our BT-Adapter achieves a balance by being parameter-efficient, multimodal-friendly, and temporal-sensitive simultaneously. This gives it a clear advantage over other temporal modeling methods for video conversation.


\subsection{Pretraining with Asymmetric Masking} \label{method_training}
As proved by previous studies \citep{zeng2023tvtsv2, wang2022internvideo, xue2022clip}, CLIP-based video encoder can harvest stronger performance on downstream video tasks after being post-pretrained on large-scale video-text data. Hence, we also involved our BT-Adapter with video-text pretraining. 
However, the expensive computation cost  has always been a bottleneck when scaling up the training. Inspired by the recent success of masked modeling in visual-language pretraining \citep{li2023scaling,tong2022videomae}, we develop a unique asymmetric masking strategy for BT-Adapter. Specifically, we maintain the tokens in the frozen CLIP unchanged while applying a tubular mask to the tokens in BT-Adapter. This mask randomly masks a certain percentage ($\rho\%$) of patch tokens in the same position across different frames. This approach allows us to maximize the retention of pretraining knowledge from CLIP while reducing spatial-temporal redundancy. Thanks to the asymmetric mask, we can maintain a high mask ratio ($\rho \ge 70$) without compromising performance, leading to a reduction of at least half of the computational budget. As a result, with the assistance of frozen backbone and token masking, we can accomplish the costy video-text pretraining in just a few hours.
Furthermore, based on asymmetric masking, we have devised two tailor-made training objectives for the branching temporal structure, in addition to the Video-Text Contrastive.

\noindent \textbf{Video-Text Contrastive (VTC).}  VTC is the most widely-used basic objective for cross-modal alignment. Given the global video feature $v$ in Eq. \ref{eq_combine} and global text feature $t$, we formulate $\mathcal{L}_{VTC}$ as:
\begin{equation}
     \mathcal{L}_{nce}(x,y) = - \frac{1}{B} \sum_{m=1}^B \mathrm{log} \frac{\mathrm{exp}(\tau x_{m} \cdot y_{n})}{\sum_{n=1}^B \mathrm{exp}(\tau x_{m} \cdot y_{n})} ,~~~ 
     \mathcal{L}_{VTC} = \mathcal{L}_{nce}(v,t) + \mathcal{L}_{nce}(t,v),
 \end{equation}
 where $B$ is the batch size and $m,n$ is the index in a batch and $\tau$ is the temperature scale.

\noindent \textbf{Masked Branching Token Alignment (MBTA).} 
Masked video modeling has been proven beneficial for spatial-temporal representation learning \citep{tong2022videomae}, but pixel reconstruction in \citet{tong2022videomae} is computationally expensive. In our approach, we have a frozen CLIP and the masked BT-Adapter, where the unmasked CLIP is naturally a teacher for the masked branch. Hence, we can conduct the token alignment in an end-to-end pretraining without extra forward propagation. 
Specifically, we compute the mean squared error (MSE) between the unmasked tokens in BT-Adapter and the corresponding tokens in CLIP in the last layer:
\begin{equation}
     \mathcal{L}_{MBTA} =  \frac{1}{(1-\rho) N T} \sum_{i=1}^{T} \sum_{j=1}^{N} ( \mathrm{W_{v\_proj}} \cdot v_{i,j}^{-1} - \mathrm{W'_{v\_proj}} \cdot \hat{v}_{i,j}^{-1})^2, ~~(i,j) \notin M,
 \end{equation}
 where $M$ is the set of masked tokens, and $\mathrm{W'_{v\_proj}}$ is a freshly initialized linear projection distinct from $\mathrm{W_{v\_proj}}$. Utilizing MBTA, we can constrain the masked branch to reconstruct and align the semantic information present in CLIP, leading to better multimodal results.

 \noindent \textbf{Masked Branching Cross-Modal Alignment (MBCA).} 
In our model, the backbone and the branch handle different aspects: CLIP encodes static spatial features, while BT-Adapter captures dynamic information. Therefore, we additionally align the branching patch tokens with the text embedding to enhance temporal learning, which is formulated as:
\begin{equation}
     \hat{v} = \frac{1}{(1-\rho) N T} \sum_{i=1}^{T} \sum_{j=1}^{N} \mathrm{W_{v\_proj}} \cdot \hat{v}_{i,j}^{-1}, ~~~
     \mathcal{L}_{MBCA} = \mathcal{L}_{nce}(\hat{v},t) + \mathcal{L}_{nce}(t,\hat{v}), ~~(i,j) \notin M.
 \end{equation}
Different from $\mathcal{L}_{VTC}$, we utilize the patch tokens for the alignment. This choice is made because the information is highly centralized in the CLS token for CLIP, whereas downstream applications like multimodal conversation or generation typically rely on the patch tokens as input.



\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
\centering
\caption{The zero-shot results of text-to-video retrieval on MSR-VTT, DiDemo, LSMDC, and ActivityNet. Source denotes the scale of pretraining data.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{l|c|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Source} & \multicolumn{3}{c|}{MSR-VTT} & \multicolumn{3}{c|}{DiDeMo} & \multicolumn{3}{c|}{LSMDC} & \multicolumn{3}{c}{ActivityNet}   \\

                                              &  & R@1   & R@5   & R@10  & R@1   & R@5   & R@10  & R@1  & R@5  & R@10   & R@1  & R@5  & R@10   \\
\hline
\textit{Non-CLIP models} & & & & & & & & & & & & & \\
Frozen \citep{bain2021frozen}   & 5M  & 24.7  & 46.9  & 57.2  & 	21.1  & 46.0  & 56.2  & - & - & - & - & - & - \\
Clover \citep{huang2023clover}   & 5M  & 26.4  & 49.5  & 60.0  & 29.5  & 55.2  & 66.3  & 17.4 & 29.2 & 38.2 & - & - & - \\
OmniVL \citep{wang2022omnivl}     & 14M  &  34.6 & 58.4 & 66.6  & 33.3 & 58.7 & 68.5  & - & - & - & - & - & - \\
HiTeA \citep{ye2022hitea}   & 5M  &  29.9 & 54.2 & 62.9  & 36.1 & 60.1 & 70.3  & 15.5 & 31.1 & 39.8 & - & - & - \\
Singularity \citep{lei2022revealing}   & 17M  &  34.0 & 56.7 & 66.7  & \textbf{37.1} & 61.7 & 69.9  & - & - & - & 30.6 & 55.6 &66.9 \\
VideoCoCa \citep{yan2022video}   & 100M  &  34.3 & 57.8 & 67.0  & - & - & -  & - & - & - & 34.5 & 63.2 & 76.6 \\ \hline
\textit{CLIP-L/14} & & & & & & & & & & & & & \\
CLIP \citep{radford2021learning}     & -  &  35.4 & 58.8 & 68.1  & 30.3 & 54.9 & 65.4  & 17.0 & 31.8 & 40.3 & 28.8 & 57.6 & 71.8 \\
ImageBind \citep{girdhar2023imagebind}   & -  &  36.8 & 61.8 & 70.0  & - & - & -  & - & - & - & - & - & - \\
InternVideo \citep{wang2022internvideo}     & 12.8M  & 40.7  & -  & -  & 31.5  & -  & -  & 17.6 & - & - & 30.7 & - & - \\
TVTSv2 \citep{zeng2023tvtsv2}     & 8.5M  & 38.2  & 62.4  & 73.2  & 34.6  & \textbf{61.9}  & 71.5  & 17.3 & 32.5 & 41.4 & - & - & - \\
UMT-L \citep{li2023unmasked}     & 5M  & 33.3  & 58.1  & 66.7  & 34.0  & 60.4  & 68.7  & \textbf{20.0} & \textbf{37.2} & 43.7 & 31.9 & 60.2 & 72.0 \\
\rowcolor[RGB]{207,234,241}  BT-Adapter      & 2M  & \textbf{40.9}  & \textbf{64.7}  & \textbf{73.5}  & 35.6  & \textbf{61.9}  & \textbf{72.6}  & 19.5 & 35.9 & \textbf{45.0} & \textbf{37.0}  & \textbf{66.7}  & \textbf{78.9}\\

\bottomrule
\end{tabular}}
\vspace{-0.5em}
\label{zs_retrieval}
\end{table*}

\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
\centering
\caption{The results of video conversation on video-based generative
performance benchmarking. FT and ZS mean with and without video instruction tuning respectively.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|>{\columncolor[RGB]{207,234,241}}c|>{\columncolor[RGB]{207,234,241}}c}
\toprule
Evaluation Aspect &	VideoLLaMA  & LLaMA-Adapter & VideoChat &	VideoChatGPT & Ours (ZS) & Ours (FT) \\ \hline
Temporal Understanding & 1.82& 1.98 & 1.94 & 1.98 & 2.13 & \textbf{2.34} \\
Correctness of Information & 1.96 & 2.03 & 2.23 & 2.40 & 2.16 & \textbf{2.68} \\
Detail Orientation & 2.18 & 2.32 & 2.50 & 2.52 & 2.46 & \textbf{2.69} \\
Contextual Understanding & 2.16 & 2.30 & 2.53 & 2.62 & 2.89 & \textbf{3.27} \\
Consistency & 1.79 & 2.15 & 2.24 & 2.37 & 2.20 & \textbf{2.46} \\
Mean &  1.98 & 2.16 & 2.29 & 2.38 & 2.46 & \textbf{2.69} \\
\bottomrule
\end{tabular}}
\label{chat_generate}
\end{table*}

\section{Experiments} \label{exp}
\vspace{-0.5em}

\subsection{Experiment Settings} 
\noindent \textbf{Tasks and Datasets.}
We have evaluated our BT-Adapter on two main aspects: traditional video tasks and video-centric dialogue. To begin with, we pretrained the BT-Adapter on WebVid-2M \citep{bain2021frozen}. For traditional video tasks, we consider zero-shot text-to-video retrieval and zero-shot video recognition, covering seven benchmarks: (a)MSR-VTT \citep{xu2016msr}. (b)DiDeMo \citep{anne2017localizing}. (c)LSMDC \citep{rohrbach2017movie}. (d)ActivityNet \citep{caba2015activitynet}. (e)Kinetic-400 \citep{carreira2017quo}. (f)HMDB-51 \citep{kuehne2011hmdb}. (g)UCF-101 \citep{soomro2012ucf101}. We used Recall@K (R@K) for retrieval tasks and Top-K Accuracy (A@K) for recognition tasks as the evaluation metrics.

For video dialogue, we conducted evaluations for the zero-shot video conversation (without instruction tuning) and instruction-tuned video conversation. The VideoChatGPT-100K \citep{maaz2023video} is employed for supervised instruction tuning. To assess the quality of the responses quantitatively, we employ the VideoChatGPT benchmark, which consists of five metrics for video-based generative performance benchmarking and six metrics for zero-shot question-answer evaluation. Additional details and settings of each dataset can be found in Sec. \ref{appendix_dataset} in the Appendix.

\noindent \textbf{Implementation Details.}
We employ openai-CLIP-L/14 as the backbone for its wide application. We adopt 4 layers of BT-Adapter in default. During pretraining, the masking ratio $\rho$ is set as 70\%. The temperature scale $\tau$ is fixed as 0.01 for contrastive loss. The weight of the three losses is $1:1:1$. It takes 3 hours to train one epoch on WebVid on 8 V100-32G GPUs. For video conversation, we implement the instruction tuning based on BT-Adapter-LLaVA. InstructionBLIP and miniGPT4 are also included for zero-shot evaluation. During instruction tuning, we update the linear projection and BT-Adapter, while keeping the rest architecture frozen. It takes 3 hours to train three epochs on 8 A100 40GB GPUs. More training details are listed in Appendix.

\begin{table}[ht]
  \begin{minipage}[b]{0.56\textwidth}
    \centering
    \caption{The results of video conversation zero-shot question-answering. FT and ZS mean with and without instruction tuning respectively.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{l|cc|cc|cc}
\toprule
\multirow{2}{*}{Method}  & \multicolumn{2}{c|}{MSVD-QA} & \multicolumn{2}{c|}{MSRVTT-QA} & \multicolumn{2}{c}{ActivityNet-QA} \\
 & Acc & Score & Acc & Score & Acc & Score \\ \hline
 VideoLLaMA  & 51.6 & 2.5 & 29.6 & 1.8 & 12.4 & 1.1 \\ 
 LLaMA-Adapter & 54.9 & 3.1 &43.8 &2.7 & 34.2 & 2.7 \\ 
 VideoChat  & 56.3 & 2.8 & 45.0 & 2.5 & 26.5 & 2.2 \\ 
 VideoChatGPT  & 64.9 & 3.3 & 49.3 & 2.8 & 35.2 & 2.7 \\ 
 \rowcolor[RGB]{207,234,241} Ours (ZS) & 67.0 & 3.6 & 51.2 & 2.9 & \textbf{46.1} & \textbf{3.2} \\ 
 \rowcolor[RGB]{207,234,241} Ours (FT) & \textbf{67.5} & \textbf{3.7} & \textbf{57.0} & \textbf{3.2} & 45.7 & \textbf{3.2} \\ 

\bottomrule
\end{tabular}}
\vspace{-1em}
\label{chat_QA}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.41\textwidth}
\raggedleft
    \caption{The results of zero-shot video conversation on different image-centric dialogue models.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
Method  & Temporal & Correctness \\ \hline
 LLaVA-Vicuna  & 1.78 & 2.06 \\ 
 \rowcolor[RGB]{207,234,241} +BT-Adapter & 2.13 & 2.16 \\  \hline
 MiniGPT4-Vicuna   & 1.88 & 2.48 \\ 
 \rowcolor[RGB]{207,234,241} +BT-Adapter & 2.56 & 2.71 \\ \hline
 MiniGPT4-LLama2  & 1.56 & 1.44 \\ 
\rowcolor[RGB]{207,234,241} +BT-Adapter & 2.15 & 1.81 \\  \hline
 

\bottomrule
\end{tabular}}
\vspace{-1em}
\label{chat_zs}
  \end{minipage}
\end{table}

\subsection{Quantitative evaluation} 
\textbf{Traditional Video Tasks.}
The results of zero-shot text-to-video retrieval are presented in Table \ref{zs_retrieval}. Compared to the previous SOTAs, BT-Adapter achieves competitive performance across all datasets, obtaining the best results on most metrics. For instance, in comparison to UMT and Singularity, although there is a slight lag in terms of R@1 on DiDeMo and LSMDC respectively, we have surpassed them by more than 5\% on MSRVTT and ActivityNet. Furthermore, we achieve superior results with significantly fewer pretraining scales and GPU hours than all the mentioned methods. For example, we use 130$\times$ fewer GPU hours than UMT, 560$\times$ fewer than TVTSv2, and 2687$\times$ fewer than InterVideo while still outperforming them. The results of zero-shot action recognition are posted in Sec. \ref{sec_zsaction} and Table \ref{zs_action} in Appendix.


\textbf{Video Dialogue.} 
Thanks to the benchmarks introduced by \citet{maaz2023video}, we can quantitatively compare the performance of various video conversation models. The results are presented in Table \ref{chat_generate} and Table \ref{chat_QA}, where we compare BT-Adapter with all existing video-centric dialogue models, including VideoLLaMA \citep{zhang2023video}, LLaMA-Adapter \citep{gao2023llama}, VideoChat \citep{li2023videochat}, and VideoChatGPT \citep{maaz2023video}. It is observed that our zero-shot model, even without any instruction tuning, outperforms methods that require instruction tuning on average. When fine-tuned using video instruction data, the superiority of our approach becomes even more pronounced. These results underscore the effectiveness of the BT-Adapter as a superior method for temporal modeling in video conversation models compared to existing approaches. Notably, our BT-Adapter exhibits a significantly larger performance margin over other methods on ActivityNet in both Table \ref{chat_QA} and \ref{zs_retrieval}, highlighting its particular strength in handling long video sequences. Moreover, we have integrated the BT-Adapter with various pretrained image conversation models. As illustrated in Table \ref{chat_zs}, pretrained BT-Adapter yields consistent advancement on all image-language chatbots without extra instruction tuning, underscoring the broad applicability of our method.

\subsection{Ablation Study} 

\begin{table}[ht]
  \begin{minipage}[b]{0.58\textwidth}
    \centering
    \caption{Ablation study on the structures of BT-Adapter. We report the results on zero-shot R@1 of MSRVTT and DiDemo retrieval and zero-shot video conversation.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
 Model & MSRVTT  & DiDemo & Temporal \\
 \hline
 CLIP (baseline) & 35.4 & 30.3 & 1.78 \\ 
 +4 layer separate-ST & 35.7 & 31.0 & 1.81 \\
 +branch modeling & 37.4 & 32.9 & 1.97 \\
 +backbone-branch interaction & \textbf{38.5} & \textbf{33.9} & \textbf{2.06} \\ 

\bottomrule
\end{tabular}}
\vspace{-0.5em}
\label{structure}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.38\textwidth}
\raggedleft
    \caption{Ablation study on the training objectives. We report the zero-shot R@1 of retrieval.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{cc|cc}
\toprule
MBTA  & MBCA & MSRVTT & DiDemo \\ \hline
 &  & 38.5 & 33.9 \\ 
 $\checkmark$ &  & 39.3 & 34.3 \\ 
   & $\checkmark$ & 40.1 & 34.9 \\ 
  $\checkmark$  & $\checkmark$ & \textbf{40.9} & \textbf{35.6} \\ 

\bottomrule
\end{tabular}}
\vspace{-0.5em}
\label{objective}
  \end{minipage}
\end{table}

\textbf{Model Structure.} 
To investigate the components within the BT-Adapter, we initiated our study by exclusively employing VTC for pretraining and subsequently evaluating the zero-shot performance across retrieval tasks and video conversation, as presented in Table \ref{structure}. Initially, we explored the implementation of separate-ST modeling within the last 4 layers. However, the resulting improvements were marginal. Subsequently, we transitioned the separate-ST network into the branch, where we witnessed notable progress. This shift underscores the efficacy of branching modeling. In the final step, we studied the interaction module, \textit{i.e.,} multi-level selective combination between backbone and branch. This addition led to further enhancements across all benchmark datasets. The more fine-grained ablations on model components can be found in Sec. \ref{appendix_ablation} in the appendix.

\begin{figure}[htbp] 
    \centering
    \begin{minipage}{0.48\textwidth}
\centering
      \includegraphics[width=1\textwidth]{image/iclr23_layernum.pdf} 
\vspace{-1.7em}
      \caption{Ablation study on the number of \\ BT-Adapter layers.}
      \label{layernum}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
\centering
      \includegraphics[width=1\textwidth]{image/iclr23_maskrate.pdf} 
\vspace{-1.6em}
      \caption{Ablation study on the masking rate of BT-Adapter during pretraining.}
      \label{maskrate}
    \end{minipage}
    \vspace{-1.5em}
\end{figure}

\begin{figure*}[t] 
\centering
    \includegraphics[width=1\textwidth]{image/chat.pdf} 
    \captionsetup{font={footnotesize}}
    \vspace{-1.5em}
    \caption{A qualitative result of video conversation. We present the answers from VideoChatGPT \citep{maaz2023video} (upper) and our BT-Adapter-LLaVa (down).}
    \label{visualize1}
    \vspace{-0.2em}
\end{figure*}

\textbf{Training Objectives.}
We investigated the influence of two innovative training objectives in Table \ref{objective}, where all experiments were conducted using BT-Adapter with a mask rate of 70\%. Our observations reveal that both MBTA and MBCA yield improvements in downstream results, with MBCA demonstrating more substantial progress. This outcome suggests that individually aligning the temporal module and the textual output effectively mitigates the limitations inherent in pretrained image-text models. Moreover, the combination of both objectives results in further advancements.

\textbf{Number of Branching Layers and Masking Rate.} 
The number of branching layers and the masking rate in our model inherently involve a trade-off between computational resources and performance. To determine the optimal settings, we conducted ablation experiments. Firstly, in Fig. \ref{layernum}, we present the results of zero-shot retrieval and conversation as we vary the number of layers. Notably, we observed that the performance improvement plateaus at around 4-6 layers. Secondly, in Fig. \ref{maskrate}, we provide results for zero-shot retrieval and the corresponding convergence GPU time across different masking rates. Our findings indicate that the results remain stable when the masking rate is set at or below 70\%, while higher masking rates significantly lower the training time. Therefore, we have chosen to maintain a masking rate of 70\%.

\subsection{Qualitative Results} 
In Figure \ref{visualize1}, we present a qualitative example of video conversation. Unlike the general description from VideoChatGPT, our model provides an informative and accurate response to a video-related question, highlighting the effectiveness of the BT-Adapter in video comprehension. Additional examples of video dialogues covering various aspects can be found in the Appendix.

\section{Conclusion}
This paper presents a novel approach to achieve parameter-efficient yet effective image-to-video adaptation and video conversation. Our proposed solution, named Branching Temporal Adapter (BT-Adapter), is a branching separate-ST network for temporal modeling. Building upon the BT-Adapter, we introduce an asymmetric masking technique along with two novel training objectives. Extensive experiments demonstrate the superiority of our model over other temporal modeling methods for video conversation. By seamlessly integrating the BT-Adapter with any pretrained image conversation model, we achieve video dialogues without necessitating manual video instruction tuning. With significantly lower computation costs, we attain state-of-the-art results across two zero-shot video tasks and video conversations.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\section{Datasets} \label{appendix_dataset}
\textbf{Test-to-Video Retrieval.}
The settings of the four zero-shot retrieval benchmarks are presented as follows: (1) \textbf{MSRVTT} \citep{xu2016msr}, a widely used video-text retrieval benchmark, comprises 10,000 YouTube videos, each accompanied by 20 captions. Our reported results are based on the 1K-A split, which consists of 9,000 training videos and 1,000 testing videos. For MSRVTT, we sample 12 frames for each video and set max token length as 12. (2) \textbf{DiDemo} \citep{anne2017localizing} comprises 10,611 videos gathered from Flickr, along with 40,000 sentences. To form queries, we concatenate all captions associated with a video. We use a frame number of 64 and a mask token length of 64, consistent with prior research. (3) \textbf{LSMDC} \citep{rohrbach2017movie} consists of 118,081 videos extracted from 202 movies. We configure it with a frame number of 12 and a maximum token length of 32. (4) \textbf{ActivityNet} \citep{caba2015activitynet} comprises 20,000 YouTube videos. To create queries, we concatenate all video descriptions into paragraphs. Our evaluation focuses on video-paragraph retrieval using the 'val1' split. We set the frame number and maximum token length to 64.

\textbf{Action Recognition.}
In all video recognition datasets, we refrain from utilizing templates such as ``a video of **'' and instead employ the tag itself as the textual query. The parameters for frame number and maximum token length are consistently configured at 16 and 12 respectively. The statistics pertaining to the three zero-shot action recognition benchmarks are provided below: (1) \textbf{Kinetics-400} \citep{carreira2017quo} is a widely recognized dataset for video action recognition. It comprises a substantial collection of 260,000 videos, each with an average duration of approximately 300 frames. The dataset encompasses a diverse set of 400 action classes. (2) \textbf{HMDB-51} \citep{kuehne2011hmdb} includes a total of 5,000 videos spanning 51 distinct action categories. The dataset is partitioned into training and test sets, with 3,500 videos allocated for training and 1,500 videos for testing. (3) \textbf{UCF-101} \citep{soomro2012ucf101} comprises a comprehensive collection of 13,000 videos, representing 101 unique action categories. Within this dataset, the training set consists of 9,500 videos, while the test set contains 3,500 videos.

\textbf{Video-Text pretraining.} We adopt the \textbf{WebVid2M} \citep{bain2021frozen} for pretraining, laying the foundation for the BT-Adapter's video encoding capabilities. WebVid2M is a substantial video-text pretraining dataset composed of short videos paired with textual descriptions, sourced from stock footage sites. This dataset is characterized by its vast scale, encompassing approximately 2.5 million video-caption pairs and totaling 12,000 video hours. The videos within WebVid2M exhibit a rich diversity of content. During the pretraining, we configured the frame number and maximum token length to be 8 and 32 respectively.

\textbf{Video Conversation.} 
VideoChatGPT benchmark \citep{maaz2023video} s the first benchmark designed for the quantitative evaluation of video conversation models. It was collaboratively annotated by ChatGPT and human annotators using the ActivityNet dataset, resulting in a dataset containing 100k video-text instruction pairs. For the video-based text generation benchmark, a test set was curated based on ActivityNet, which included captions and associated question-answer pairs obtained from human annotations. The evaluation pipeline used the GPT-3.5 model and assessed the model's performance in various aspects, including Correctness of Information, Detail Orientation, Contextual Understanding, Temporal Understanding, and Consistency. The pipeline assigns a relative score to the generated predictions on a scale of 1 to 5 for each of these aspects. For zero-shot question-answer evaluation, three open-source video QA datasets were employed: MSRVTT-QA, MSVD-QA, and ActivityNet-QA. Also, GPT was used as the zero-shot evaluation assistor to assign relative scores on a scale of 1 to 5 for generated answers.

\section{Implementation Details}
All experiments were conducted using PyTorch \citep{paszke2019pytorch}. The pretraining and zero-shot inference processes were implemented based on mmaction2.0 \citep{2020mmaction2}. Our configuration settings are detailed in Table \ref{implementation}, with the exception of specific cases where alternate configurations were used. It is noteworthy that our data augmentation techniques are notably simpler in comparison to those employed by other methods.

\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
\centering
\caption{Default implementation details for pretraining and instruction tuning.}
\vspace{-0.8em}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
Task & Video-Text Pretraining & Video Instruction Tuning  \\
\hline
num. BT-Adapter layers & 4 & 3 \\
num. CLIP layers & 24 & 23 \\
optimizer & AdamW, $\beta = (0.9, 0.98)$ & AdamW, $\beta = (0.9, 0.998)$  \\
weight decay & 0.05 & 0.1 \\
learning rate & 2e-6 (for BT-Adapter) & 2e-5 (for BT-Adapter and linear projection) \\
fp16 & \XSolid & \Checkmark \\
batch size & 640 & 4 \\
augmentation & RandomResizedCrop & CenterCrop  \\
training source &  8 V100-32G & 4 A100-40G \\

\bottomrule
\end{tabular}}
\vspace{-0.5em}
\label{implementation}
\end{table*}

\section{Zero-Shot Results on Action Recognition} \label{sec_zsaction}

\begin{wraptable}{r}{11cm}
\setlength{\tabcolsep}{3pt}
\begin{center}
    \vspace{-1em}
      \caption{\small The zero-shot results of video recognition on HMDB, UCF, and K400.} 
      \vspace{-1em}

      \renewcommand\tabcolsep{9pt}
      \resizebox{1.\linewidth}{!}{
       \begin{tabular}{l|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Method}  & \multicolumn{2}{c|}{HMDB-51} & \multicolumn{2}{c|}{UCF-101} & \multicolumn{2}{c}{K400}     \\
        
                                                        & A@1   & A@5   & A@1   & A@5   & A@1   & A@5     \\
        \hline
        JigsawNet \citep{qian2022rethinking}       &  38.7 & - & 56.0  & - & 45.9 & - \\
        CLIP \citep{radford2021learning}       &  45.0 & 74.4 & 73.5  & 92.7 & 59.1 & 82.8 \\
        X-Florence \citep{ni2022expanding}       &  48.4 & - & 73.2  & - & - & - \\
        InternVideo \citep{wang2022internvideo}    & -  & -  & -  & -  & 64.2  & -   \\
        TVTSv2 \citep{zeng2023tvtsv2}      & 52.1  & -  & 78.0  & - & 59.6  & -   \\
        \rowcolor[RGB]{207,234,241}  BT-Adapter      & \textbf{54.6}  & \textbf{79.7}  & \textbf{79.1}  & \textbf{96.2}  & \textbf{64.3}  & \textbf{86.7}    \\
        \bottomrule
        \end{tabular} 
        \label{zs_action}
    }
    \end{center}
    \vspace{-1.5em}
    
\end{wraptable}



The results of zero-shot video recognition are reported in Table \ref{zs_action}. Despite being pretrained solely on video-language datasets, BT-Adapter consistently contributes to the video-only task, achieving state-of-the-art zero-shot results among CLIP-based methods. Notably, even when compared to InternVideo, which employed self-supervised reconstruction during pretraining (proven to be more effective on single-modality tasks than contrastive learning), BT-Adapter still outperforms it, underscoring the effectiveness of BT-Adapter in video encoding and spatial-temporal modeling.

\section{Experimental Comparison With Similar Methods} \label{similar_sec}

\begin{wraptable}{r}{11cm}
\setlength{\tabcolsep}{3pt}
\begin{center}
    \vspace{-1em}
      \caption{\small The experimental comparison with closely related works.} 
      \vspace{-1em}
      \renewcommand\tabcolsep{9pt}
      \resizebox{1.\linewidth}{!}{
       \begin{tabular}{l|c|c|c|c}
        \toprule
        Method & MSR-VTT R@1 & Correctness & Temporal & GPU Hours \\ \hline
        CLIP(baseline) & 35.4 & 2.06 & 1.78 & - \\
        ST-Adapter & 33.6 & 1.52 & 1.71 & 2.5 \\
        STAN(open) & 40.5 & 1.84 & 1.77 & 22 \\
        STAN(frozen) & 38.1 & 2.07 & 1.92 & 3 \\
        \rowcolor[RGB]{207,234,241} BT-Adapter & 40.9 & 2.16 & 2.13 & 3 \\
        \bottomrule
        \end{tabular} 
        \label{similar_exp}
    }
    \end{center}
    \vspace{-1.5em}
\end{wraptable}

In this section, we conduct an experimental comparison between two closely related works, ST-Adapter \citep{pan2022st} and STAN \citep{liu2023revisiting}. ST-Adapter is also notably recognized as a parameter-efficient method for temporal modeling, while STAN also employs the branching temporal modeling strategy. We pretrain the three methods on MSRVTT for one epoch first, and the results of zero-shot performance on MSRVTT retrieval and video conversation are presented in Table \ref{similar_exp}. Initially, it is evident that ST-Adapter exhibits suboptimal results across all metrics. This outcome may be attributed to the fact that ST-Adapter is a single-modality temporal adapter, where the insertion of 3-D convolutions between transformer layers may lead to the rapid degradation of the pretrained multimodal knowledge. Next, we assess STAN under two conditions: with frozen CLIP and without. The results reveal that STAN, when used with an open CLIP, performs admirably in zero-shot retrieval tasks. However, it exhibits poorer outcomes in video conversation tasks, and it requires significantly longer pretraining hours. Conversely, when STAN is employed with a frozen CLIP, it shows improvements across all metrics, although it still falls short of BT-Adapter in all aspects. In contrast, BT-Adapter achieves both efficiency and effectiveness simultaneously, underscoring the superiority of our design over ST-Adapter and STAN in the context of zero-shot video encoding and video conversation.

\section{More Ablation Results} \label{appendix_ablation}

\begin{wraptable}{r}{8cm}
\setlength{\tabcolsep}{3pt}
\begin{center}
    \vspace{-1em}
      \caption{\small Ablation Studies on Temporal Projection and Backbone-Branch Combination.} 
      \vspace{-1em}
      \renewcommand\tabcolsep{9pt}
      \resizebox{1.\linewidth}{!}{
       \begin{tabular}{l|cc}
        \toprule
        Method & MSR-VTT R@1 & DiDemo R@1  \\ \hline
        None Projection & 39.1 & 33.5 \\
        Random Initialization & 39.3 & 34.0 \\
        Zero Initialization & \textbf{40.9} & \textbf{35.6} \\ \hline \hline
        Addition & 40.0 & 34.4 \\
        Weighted Selection & \textbf{40.9} & \textbf{35.6} \\  
        Concatenation & 39.8 & 34.5 \\ 
        \bottomrule
        \end{tabular} 
        \label{more_ablation}
    }
    \end{center}
    \vspace{-1.5em}
\end{wraptable}

\textbf{Temporal Projection and Initialization.} We examine the appropriate way for instantiating the temporal projection. As demonstrated in Table \ref{more_ablation}(above), random initialization for the projection yields performance results similar to those obtained without projection. In contrast, zero initialization outperforms them by a significant margin. This suggests that building temporal reasoning capability from scratch, as opposed to random initialization, mitigates adverse effects on the well-established spatial prior. Consequently, zero initialization is better suited for knowledge transfer from images to videos. \textbf{Backbone-Branch Combination.} We further perform an ablation study to explore the most effective method for combining the output from the backbone and the branch, considering three approaches: direct addition, weighted selection, and concatenation with subsequent linear projection. As illustrated in Table \ref{more_ablation}(below), weighted selection yields the most favorable results. This observation suggests that different layers and samples require distinct degrees of information from the backbone and the branch.

\section{More Qualitative Results}

\begin{figure*}[htb] 
\centering
    \includegraphics[width=1\textwidth]{image/chat_appendix1.pdf} 
    \captionsetup{font={footnotesize}}
    \vspace{-1.8em}
    \caption{Qualitative results of video conversation in terms of the sequence of actions in the video. We present the answers from VideoChatGPT (upper) and our BT-Adapter-LLaVa (down).}
    \label{visualize11}
    \vspace{-0.2em}
\end{figure*}

\begin{figure*}[htb] 
\centering
    \includegraphics[width=1\textwidth]{image/chat_appendix2.pdf} 
    \captionsetup{font={footnotesize}}
    \vspace{-1.8em}
    \caption{Qualitative results of video conversation in terms of actions in a specific frame of the video. We present the answers from VideoChatGPT (upper) and our BT-Adapter-LLaVa (down).}
    \label{visualize22}
    \vspace{-0.2em}
\end{figure*}

\begin{figure*}[htb] 
\centering
    \includegraphics[width=1\textwidth]{image/chat_appendix3.pdf} 
    \captionsetup{font={footnotesize}}
    \vspace{-1.8em}
    \caption{Qualitative results of video conversation in terms of unusual actions in the video. We present the answers from VideoChatGPT (upper) and our BT-Adapter-LLaVa (down).}
    \label{visualize33}
    \vspace{-0.2em}
\end{figure*}

\begin{figure*}[htb] 
\centering
    \includegraphics[width=1\textwidth]{image/chat_appendix4.pdf} 
    \captionsetup{font={footnotesize}}
    \vspace{-1.8em}
    \caption{Qualitative results of video conversation in terms of complex actions and scenes in a long video (3 min). We present the answers from VideoChatGPT (upper) and our BT-Adapter-LLaVa (down).}
    \label{visualize44}
    \vspace{-0.2em}
\end{figure*}

\begin{figure*}[!h] 
\centering
    \includegraphics[width=1\textwidth]{image/chat_appendix5.pdf} 
    \captionsetup{font={footnotesize}}
    \vspace{-1.8em}
    \caption{The bad case of our method in video conversation. We present the answers from VideoChatGPT (upper) and our BT-Adapter-LLaVa (down).}
    \label{visualize55}
    \vspace{-0.2em}
\end{figure*}

In Figures \ref{visualize11}, \ref{visualize22}, \ref{visualize33}, and \ref{visualize44}, we present a comprehensive overview of the qualitative results obtained in video dialogues, encompassing diverse aspects. These visualizations vividly illustrate the capacity of our BT-Adapter to provide contextually appropriate responses in a variety of scenarios where temporal sensitivity is paramount. These results serve to underscore the efficacy of the BT-Adapter in video understanding. In Figure \ref{visualize55}, we present a notable outlier case in which our method encounters challenges, where the BT-Adapter struggles to recognize the text content within the frames. This particular instance sheds light on the fact that, while the BT-Adapter diligently strives to preserve pretraining information to the greatest extent possible, it may still introduce some disruption to the pretraining knowledge compared to the fully concatenation-based modeling of VideoChatGPT.


\end{document}
