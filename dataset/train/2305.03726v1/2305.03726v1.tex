\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[preprint]{neurips_2023}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false, allcolors=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}\usepackage{pifont}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{hhline}
\usepackage{color, colortbl}
\usepackage{bm}
\usepackage{marvosym}
\newcommand{\eg}{\textit{e.g.,~}}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\title{Otter: A Multi-Modal Model with In-Context Instruction Tuning}

\author{
Bo Li\textsuperscript{*} \quad Yuanhan Zhang\textsuperscript{*} \quad Liangyu Chen\textsuperscript{*} \quad Jinghao Wang\textsuperscript{*} \\
\textbf{Jingkang Yang} \quad \textbf{Ziwei Liu}\textsuperscript{\Letter}
\and
S-Lab, Nanyang Technological University, Singapore\\
{\tt\small\{libo0013, yuanhan002, lchen025, c190209, jingkang001, ziwei.liu\}@ntu.edu.sg}\\
{\tt\small\url{https://github.com/Luodian/Otter}}
}

\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{Equal Contribution}\def\thefootnote{\arabic{footnote}}
\def\thefootnote{\Letter}\footnotetext{Corresponding Author}\def\thefootnote{\arabic{footnote}}
\begin{abstract}
Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our \textbf{M}ult\textbf{I}-\textbf{M}odal \textbf{I}n-\textbf{C}ontext \textbf{I}nstruction \textbf{T}uning (\textbf{MIMIC-IT}) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.

\end{abstract}

\section{Introduction \& Motivation}





Large language models (LLMs) have demonstrated significant universal capabilities in performing various tasks as few/zero-shot learners. These models are pre-trained on vast amounts of text data and have been showcased in recent research, such as GPT-2~\cite{gpt2} and GPT-3~\cite{gpt3}. 

Recent studies have highlighted the importance of instruction tuning in empowering LLMs, as exemplified by the boosting of GPT-3~\cite{gpt3} to InstrctGPT~\cite{instruct_gpt} and ChatGPT~\cite{chatgpt}, which follows natural language instructions effectively to accomplish real-world tasks and allows for customizing task-specific rules into instructions during downstream fine-tuning, enabling pre-trained models to comprehend user intents more effectively and produce accurate and relevant responses.

Similar attempts have been introduced in multi-modal models as well. LLaMA-Adapter~\cite{llama_adapater} aims to adapt LLaMA~\cite{llama} into an instruction following model by adding additional adapter modules and multi-modal prompts. Mini-GPT4~\cite{mini_gpt4} follows the architecture of BLIP-2~\cite{li2023blip} but replaces the language decoder with Vicuna~\cite{vicuna2023}, which supports longer answers. LLaVA~\cite{llava} utilizes the same CLIP~\cite{radford2021learning} vision encoder and Vicuna~\cite{vicuna2023} language decoder, and finetunes on their high-quality instruction dataset, curated by GPT-4~\cite{gpt4}.

Although these works have achieved excellent results and provided valuable insights, they share a minor common issue. Specifically, they either finetune the entire model or the connection part on task-specific data. For instance, a common practice is to use image-text data pairs from Caption~\cite{coco} or VQA~\cite{goyal2017making} tasks to align visual and language modules. While embedding visual information into the language model in this way can be effective, we question whether this practice is inherently task-dependent, as it relies on the task for which the data is used to train the alignment module.

Upon reflection, we have discovered that DeepMind Flamingo's~\cite{flamingo} upstream pretraining dataset, MultiModal MassiveWeb (M3W), has significant value in aligning visual and language information in a more natural manner. The dataset comprises HTML webpages, where all images and texts are arranged in an interleaved format. Specifically, a piece of text may describe an image (or videos) above or below it, and correlations may exist between images (or videos) and text in adjacent positions. This natural organization of context provides richer information than a caption dataset, where text only describes its corresponding image. Trained on this dataset, Flamingo achieves zero- and few-shot generalization and in-context learning ability, making it the GPT-3 moment in the multi-modal domain.

However, DeepMind has not released the Flamingo model and its M3W dataset to the public, potentially because of the model's exceptional performance that could cause astonishment prematurely. Nevertheless, the LAION-AI's OpenFlamingo project \cite{open_flamingo} has recently been made public, providing access to their corresponding MMC4~\cite{zhu2023multimodal} dataset in the same interleaved format on a larger scale. Consequently, community researchers can continue to follow the Flamingo series of works for further research and development in the field of multi-modal models. Although the OpenFlamingo model exhibits impressive multi-modal in-context learning abilities and executes tasks with given in-context examples, as an upstream pre-trained model, it still requires instruction tuning to perform downstream tasks more effectively. 

In our paper, we propose our \textbf{M}ult\textbf{I}-\textbf{M}odal \textbf{I}n-\textbf{C}ontext \textbf{I}nstruction \textbf{T}uning (\textbf{MIMIC-IT}) dataset and provide details on its construction in \cref{sec:ici}. We then introduce \textbf{Otter}, a multi-modal model with in-context instruction tuning based on OpenFlamingo. We illustrate the relationship between Otter and OpenFlamingo in~\cref{fig:otter_teaser}. Finetuned on MIMIC-IT dataset, our Otter model demonstrates improved instruction-following ability compared to OpenFlamingo, as shown in our qualitative analysis in \cref{sec:instruction_following}. Meanwhile, Otter is capable of learning to execute instructions with provided in-context learning examples, as shown in \cref{sec:in_context_learning}. From the engineering perspective, we optimized OpenFlamingo's implementation to make it more accessible to researchers. Our optimizations include optimizing the training requirements from at least 1$\times$ A100 GPU to only 4$\times$ RTX3090 GPUs and integrating it into Hugging Face Transformers~\cite{transformers} to simplify training and inference with a few lines of code. Our contributions facilitate further research and development in the field of multi-modal models.

\begin{figure}[tp]
    \centering
    \includegraphics[width=\textwidth]{figure/otter_teaser.pdf}
    \caption{\textbf{Otter Overview}. \textbf{Otter} is a multi-modal model finetuned on our proposed \textbf{MIMIC-IT} dataset, based on OpenFlamingo. Otter model exhibits the improved ability to execute tasks by following given instructions and leveraging in-context examples.}
    \label{fig:otter_teaser}
\end{figure}

We summarize our key contributions as follows:

\begin{itemize}
    \item We present the \textbf{M}ult\textbf{I}-\textbf{M}odal \textbf{I}n-\textbf{C}ontext \textbf{I}nstruction \textbf{T}uning (\textbf{MIMIC-IT}) dataset. Each data sample includes an instruction-image-answer triplet and its in-context examples.
    \item We introduce \textbf{Otter}, a multi-modal model with in-context instruction tuning based on OpenFlamingo, capable of the instruction following and executing new instructions with few in-context learning examples.
    \item Our optimizations to OpenFlamingo's implementation, including reducing the training requirements to 4$\times$ RTX3090 GPUs and integrating it into Huggingface Transformers to simplify the training and inference with only a few lines of code.
\end{itemize}


 \section{Related Work}
\subsection{Large-scale Multi-modal Models}
With the recent success of ChatGPT~\cite{chatgpt}, GPT-4~\cite{gpt4}, and other large language models~\cite{llama,alpaca,vicuna2023}, recent studies start to explore incorporating information from other modalities based on pretrained language models. These studies extend the capabilities of language models to more tasks and modalities, and can be categorized into two perspectives:

\textbf{System Design Perspective.} This perspective involves using ChatGPT~\cite{chatgpt} as a dispatch scheduler and connecting different expert models through it to allow for different visual tasks. Language prompts serve as an interface to call expert visual-language models within their respective task domains. Works in this category include VisualChatGPT~\cite{visual_chat_gpt}, HuggingGPT~\cite{hugginggpt}, Cola~\cite{cola}, X-GPT~\cite{zou2022generalized}, MM-REACT~\cite{yang2023mm}, and ViperGPT~\cite{suris2023vipergpt}. This approach has limitations in that each model cannot be trained individually on new tasks, and using ChatGPT~\cite{chatgpt} as a powerful instruction dispatch tool can result in high API query costs.

\textbf{End-to-End Trainable Models Perspective.} This perspective focuses on connecting models from different modalities into integrated end-to-end trainable models, also known as multi-modal foundation models. Early works in this field include Flamingo~\cite{flamingo}, which proposes a unified architecture for modeling language and vision and was later open-sourced as OpenFlamingo~\cite{open_flamingo} by LAION-AI. Other earlier works include BLIP-2~\cite{li2023blip}, which uses a lightweight Querying Transformer and two-stage bootstrap pretraining to connect information from the image to text modality. With the popularity of GPT-4~\cite{gpt4}, there has been an increased focus on this field since 2023. Enterprise-level product models include OpenAI's yet-to-be-released vision-language version of GPT-4~\cite{gpt4}, Google's PaLM-E~\cite{driess2023palm}, Baidu's ERNIE~\cite{ernie_bot}, Alibaba's Tongyi Qianwen~\cite{tongyi}, and Sensetime's SenseNova~\cite{sensenova}. Academic multi-modal efforts include a variety of models such as LLaMA-Adapters~\cite{llama_adapater}, Mini-GPT4~\cite{mini_gpt4}, and LLaVA~\cite{llava}. LLaMA-Adapters aims to adapt LLaMA~\cite{llama} into an instruction-following model with an additional adapters module and multi-modal prompts. Mini-GPT4 follows BLIP-2's~\cite{li2023blip} architecture but replaces the language decoder with Vicuna~\cite{vicuna2023}, which better supports longer responses and multi-round conversations. LLaVA connects text and image modalities through a trainable projector matrix, which is a simple lightweight linear layer. However, since LLaVA trains both the vision encoder and language decoder on their instructing tuning dataset, its cost is relatively high compared to others. In contrast, based on the Flamingo model, Otter trains a few cross-gated attention layers to connect visual and language information and establish attention between in-context examples, leaving the vision encoder and language decoder frozen.

\subsection{Multi-modal Instruction Tuning Dataset}
The concept of instruction tuning in multi-modal models was first introduced in Multi-Instruct~\cite{xu2022multiinstruct}, where 47 diverse multi-modal tasks covering 11 broad categories were organized. Each task comprises at least 5,000 instances (input-output pairs) from existing open-source datasets and 5 expert-written instructions. Multi-Instruct covers most multi-modal tasks that require visual understanding and multi-modal reasoning, such as Visual Question Answering~\cite{goyal2017making,zhu2016visual7w,suhr2017corpus}, Image Captioning~\cite{coco}, Image Generation~\cite{cc12m}, and Visual Relationship Understanding~\cite{vg}, among others. Similarly, Mini-GPT4~\cite{mini_gpt4} constructs its instruction following dataset by combining Conceptual Caption~\cite{cc3m,cc12m}, SBU~\cite{im2text}, and LAION~\cite{laion_400m} with handwritten instruction templates. More recently, LLaVA~\cite{llava} has brought the quality of an instruction tuning dataset to a higher level, as it was obtained by expanding the original captions of COCO~\cite{coco} images with handwritten seed instructions using GPT-4~\cite{gpt4} to provide more detailed descriptions and multi-round conversations.

To the best of our knowledge, the above-mentioned works are the only few that considered instruction tuning in multi-modal models. Our approach further differs from them in that we incorporate in-context examples into instruction tuning by grouping multiple similar instructions together to form a contextual example set. We are the first to propose the in-context instruction tuning paradigm in multi-modal models and to build the corresponding multi-modal in-context instruction tuning datasets.

 \section{Method}

In this section, we will introduce the details of the MIMIC-IT dataset in \cref{sec:ici}, our Otter's training details in \cref{sec:training}, and the integration with Hugging Fance ecosystem in \cref{sec:integrate_hf}.

\subsection{Multi-Modal In-Context Instruction Tuning}
\label{sec:ici}
The OpenFlamingo framework leverages the interleaved multi-modal MMC4 dataset to emerge in its few-shot, in-context learning capabilities. The MMC4 dataset is composed of image-text pairs derived from individual HTML files, with significant contextual relationships between different pairs, as depicted in Fig~\ref{fig:context}(a). An MMC4 training data sample contains (i) a queried image-text pair, where the text typically describes the image, and (ii) context, which includes the remaining image-text pairs from the same HTML file. The primary training objective of OpenFlamingo is to generate text for the queried image-text pair, and the paradigm of generating query text conditioned on in-context examples ensures OpenFlamingo's in-context learning capacity during the inference phase.

Our Multi-Modal In-Context Instruction Tuning (MIMIC-IT) dataset aims to augment OpenFlamingo's instruction comprehension capabilities while preserving its in-context learning capacity. To unleash OpenFlamingo's instruction-following potential, we compile data from visual-language tasks into image-instruction-answer triplets. Concurrently, to maintain OpenFlamingo's in-context learning capacity, we retrieve in-context examples for each triplet, which often lack correlated context, such as a visual question-answer data sample in VQAv2~\cite{vqa}. Specifically, each MIMIC-IT data sample consists of (i) a queried image-instruction-answer triplet, with the instruction-answer tailored to the image, and (ii) context. The context contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet, emulating the relationship between the context and the queried image-text pair found in the MMC4 dataset. The training objective for MIMIC-IT is to generate the answer within the queried image-instruction-answer triplet. The image-instruction-answer triplets are derived from (i) visual question-answer datasets, namely, VQAv2~\cite{vqa} and GQA~\cite{gqa}, (ii) visual instruction datasets, such as LLaVA~\cite{llava}, (iii) an in-progress, high-quality panoptic video scene graph dataset from the PVSG repository. For each video, we select 4-8 frames for instruction-following annotation, using the LLaVA dataset as a reference. We have developed three heuristics to construct the context for each image-instruction-answer triplet, as illustrated in Fig~\ref{fig:context}(b).
\begin{figure*}[tp]
    \centering
\includegraphics[width=\textwidth]{figure/context.pdf}
\caption{\textbf{Illustration of example data formats in MMC4 and MIMIC-IT}. (a) The illustration of the data format in the MMC4 dataset that are used OpenFlamingo. (b) Three heuristics to build the multi-modal in-Context instruction tuning (MIMIC-IT) dataset.}
    \label{fig:context}
\end{figure*}


\subsection{Training Details}
\label{sec:training}
Our approach adopts the OpenFlamingo training paradigm to train the Otter model. The pretrained OpenFlamingo model comprises a LLaMA-7B~\cite{llama} language encoder and a CLIP ViT-L/14~\cite{clip} vision encoder. To prevent overfitting and leverage pretrained knowledge, we freeze both the encoders and only finetune the Perceiver resampler module, cross-attention layers inserted into the language encoder and input/output embeddings of the language encoder. This results in approximately 1.3 billion trainable parameters for the Otter model.

To optimize our model, we employ the AdamW optimizer~\cite{adamw} with a starting learning rate of $10^{-5}$ and a batch size of 4. We train Otter for 6 epochs, with the learning rate scheduled using a cosine annealing scheduler. We also use gradient clipping of a threshold of 1.0 to prevent exploding gradients. 

During our training, we follow a specific format to prepare our training data. The format includes a combination of image, user instruction, \texttt{"GPT"}-generated answers \footnote{To support user-assistant conversations, we adopt \texttt{"GPT"} as the role label because it does not have any specific semantic meaning in vocabulary.}, and a special token known as the \texttt{[endofchunk]} token. We format the training data as follows:
\[ \texttt{<context> [image] User:<instruction> GPT:[answers] <answer>.[endofchunk]} \]
where the \texttt{[image]}, \texttt{[answer]}, and \texttt{[endofchunk]} tokens are unique and serve a specific purpose. We design such a chatbot-like format to train our model to improve the instruction-following and conversation generalizability of the model. The \texttt{[image]} and \texttt{[endofchunk]} tokens are originally from the OpenFlamingo training paradigm, while the \texttt{[answer]} token is a new introduction by us in training Otter. The \texttt{[answer]} token separates the answers from the instruction, so that, we mask all tokens after the \texttt{[answer]} token during training and set them as the prediction objectives of the model. We train our model using a cross-entropy loss. 

\subsection{Integratation with Hugging Face}
\label{sec:integrate_hf}
We have integrated Otter into Hugging Face Transformers~\cite{transformers} and trained it using the Hugging Face Accelerator\footnote{\href{https://HuggingFace.co/docs/accelerate/index}{Hugging Face Accelerator}}, which enables automatic mapping of the model weights to different GPU devices and offloading of overflowed weights to CPU or disk. Additionally, we use bf16 mixed precision during training. The total optimizations enable our model to be trained on 4$\times$RTX-3090 GPUs, each with 24GB memory.

Meanwhile, since Otter has been integrated into Hugging Face Transformers, it can now be reused with less than five lines of code, making it much easier for researchers to integrate into their respective training and inference pipelines (compared to the original OpenFlamingo implementation). We also provide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed to enable greater training efficiency and less memory consumption. 

To enable future research and convenience, we also provide a script for converting the original OpenFlamingo-9B checkpoint into the Hugging Face Model format. The converted checkpoint and our Otter model are uploaded and available on the Hugging Face model hub \url{luodian/openflamingo-9b-hf} and \url{luodian/otter-9b-hf}, respectively.




 \section{Demonstrations}

In this section, we show several examples of two types of demonstrations of Otter: (1) the ability to follow instructions in~\cref{sec:instruction_following}, and (2) the ability of learning to execute new instructions following provided in-context examples in~\cref{sec:in_context_learning}. Compared with OpenFlamingo, these results demonstrate the importance of in-context instruction tuning and the improvement of Otter.

\subsection{Following User Instructions}
\label{sec:instruction_following}

In~\cref{sec:ici}, we discussed how we finetuned Otter on visual instruction pairs to transform it into a powerful instruction follower. The results of our experiments are demonstrated in \cref{fig:instruction_following}, where we observe that Otter is able to provide more detailed descriptions of images and follow user instructions more accurately. This characteristic of Otter is attributed to the co-design of our model and data, which leverages the generalization ability of a strong language decoder and the rich variety of instructions present in the MIMIC-IT dataset. By fine-tuning on visual instruction pairs, Otter is able to learn the nuances of human language and accurately apply it to visual input.

\begin{figure*}[htp]
    \centering
\includegraphics[width=\textwidth]{figure/instruction_following.pdf}
\caption{\textbf{Comparisons of image captioning results between OpenFlamingo and Otter.} Otter provides more detailed and precise descriptions of the images.}
    \label{fig:instruction_following}
\end{figure*}

In order to evaluate Otter's reasoning capabilities, we designed a series of experiments to test its ability to handle complex scenarios that require a deeper understanding of the situation and more advanced commonsense reasoning. In \cref{fig:deeper_understanding}, we present an example where OpenFlamingo fails to identify important clues in the image, while Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question. In this particular example, Otter is able to correctly interpret the confusion around the traffic signal, while also taking into account relevant contextual factors such as accidents, traffic congestion, and disruptions to the flow of traffic. This level of reasoning ability is not present in standard image captioning models and represents a significant advance in the field of computer vision and natural language processing.

\begin{figure*}[htp]
    \centering
\includegraphics[width=\textwidth]{figure/deeper_understanding.pdf}
\caption{\textbf{Comparisons of situation understanding results between OpenFlamingo and Otter.} Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question.}
    \label{fig:deeper_understanding}
\end{figure*}

\subsection{Multi-Modal In-context Learning}
\label{sec:in_context_learning}

Otter is designed to support multi-modal in-context learning in a similar pattern to Flamingo~\cite{flamingo} and OpenFlamingo~\cite{open_flamingo}, which involves conditioning the language model on the corresponding media, such as an image that corresponds to a caption or a question-answer pair. In \cref{fig:in_context_learning_demo}, we demonstrate Otter's ability to perform visual question answering tasks, where users provide examples of instruction-answering pairs and images as contextual examples to prompt the model with subtasks, such as explaining memes or the danger of sports.

Despite the fact that user inputs are often short and restricted in these scenarios, Otter is able to provide more comprehensive answers. By leveraging the information contained in the images and other contextual examples, Otter is able to better understand the underlying meaning and context of the questions, resulting in more accurate and informative answers.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figure/in_context_learning_demo.pdf}
    \caption{\textbf{In-context learning examples of Otter.} Given two examples of instruction-answering pairs and an image, Otter is able to provide more comprehensive answers.}
    \label{fig:in_context_learning_demo}
\end{figure}
 \section{Conclusion}
In this work, we propose Otter, a multi-modal in-context learning foundation model with instruction tuning. Through partial finetuning on MIMIC-IT dataset, we observe that Otter can convert OpenFlamingo into a zero-shot visual instruction model with strong in-context learning abilities. With the assistance of rich instructions from images and videos, Otter generalizes to achieve better instruction-following and situation-understanding performances.

\subsection{Limitations}
\textbf{Language Hallucination.} Since Otter is built upon OpenFlamingo, which depends upon LLaMA, the hallucination issue of LLaMA is inherited by Otter. Also, the current Otter model may hallucinate the language that is not related to the image. This issue might be solved by introducing negative examples in the training data.

\subsection{Future Supports}
In the future, we plan to explore the integration of more efficient training schemas (\eg parameter-efficient finetuning such as LoRA~\cite{lora}) and more modalities (\eg 3D vision).


\textbf{Acknowledgement.} We thank Chunyuan Li and Jack Hessel for their advice and support, as well as the OpenFlamingo team for their great contribution to the open-source community. \clearpage
\bibliography{ref}
\bibliographystyle{plain}
\end{document}