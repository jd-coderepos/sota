[{'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Overall score', 'Score': '22.69'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Deductive', 'Score': '22.49'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Abductive', 'Score': '33.64'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Analogical', 'Score': '13.33'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Params', 'Score': '7B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'BenchLMM', 'Metric': 'GPT-3.5 score', 'Score': '39.13'}}]
