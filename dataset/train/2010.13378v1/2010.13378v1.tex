

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{array}
\usepackage{mathtools}
\usepackage[linguistics]{forest}
\usepackage{xcolor}
\usepackage{comment}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\makeatletter
\newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\makeatother

\title{Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning}

\author{Amir Pouran Ben Veyseh\textsuperscript{\rm 1}\thanks{\text{ } Equal contribution.}, \text{ } Nasim Nouri\printfnsymbol{1}, Franck Dernoncourt\textsuperscript{\rm 2},\\
{\bf Dejing Dou}\textsuperscript{\rm 1} and {\bf Thien Huu Nguyen}\textsuperscript{\rm 1,3} \\
\textsuperscript{\rm 1} Department of Computer and Information Science, University of Oregon,
\\Eugene, OR 97403, USA\\
\textsuperscript{\rm 2} Adobe Research, San Jose, CA, USA\\
\textsuperscript{\rm 3} VinAI Research, Vietnam\\
  \texttt{\{apouranb,dou,thien\}@cs.uoregon.edu}, \\ {\tt nasim.nourii@gmail.com}, {\tt dernonco@adobe.com}
}



\date{}

\begin{document}
\maketitle
\begin{abstract}
Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.



\end{abstract}


\section{Introduction}








Targeted Opinion Word Extraction (TOWE) is an important task in aspect based sentiment analysis (ABSA) of sentiment analysis (SA). Given a target word (also called aspect term) in the input sentence, the goal of TOWE is to identify the words in the sentence (called the target-oriented opinion words) that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence ``\textit{All warranties honored by XYZ (what I thought was a reputable company) are disappointing.}", ``\textit{disappointing}" is the opinion word for the target word ``\textit{warranties}" while the opinion words for the target word ``\textit{company}" would involve ``\textit{reputable}''. Among others, TOWE finds its applications in target-oriented sentiment analysis \citep{tang2015effective,xue2018aspect,Veyseh:20e} and opinion summarization \citep{wu2020latent}.






\begin{figure}
\small
\centering
\resizebox{.40\textwidth}{!}{
\begin{forest}
[disappointing
  [warranties
    [All]
    [honored
        [XYZ
            [by]]]]
  [are]
  [thought
    [what]
    [I]
    [company
        [was]
        [a]
        [reputable]]]
]
\end{forest}
}
\caption{\small The dependency tree of the example sentence.} \label{parse}
\end{figure}



The early approach for TOWE has involved the rule-based and lexicon-based methods \cite{hu2004mining,zhuang2006movie} while the recent work has focused on deep learning models for this problem \cite{fan2019target,wu2020latent}. One of the insights from the rule-based methods is that the syntactic structures (i.e., the parsing trees) of the sentences can provide useful information to improve the performance for TOWE \citep{zhuang2006movie}. However, these syntactic structures have not been exploited in the current deep learning models for TOWE \citep{fan2019target,wu2020latent}. Consequently, in this work, we seek to fill in this gap by extracting useful knowledge from the syntactic structures to help the deep learning models learn better representations for TOWE. In particular, based on the dependency parsing trees, we envision two major syntactic information that can be complementarily beneficial for the deep learning models for TOWE, i.e., the syntax-based opinion possibility scores and syntactic word connections for representation learning. First, for the syntax-based possibility scores, our intuition is that the closer words to the target word in the dependency tree of the input sentence tend to have better chance for being the opinion words for the target in TOWE. For instance, in our running example, the opinion word ``\textit{disappointing}" is sequentially far from its target word ``\textit{warranties}". However, in the dependency tree shown in Figure \ref{parse}, ``\textit{disappointing}" is directly connected to ``\textit{warranties}", promoting the distance between ``\textit{disappointing}" and ``\textit{warranties}" (i.e., the length of the connecting path) in the dependency tree as an useful feature for TOWE. Consequently, in this work, we propose to use the distances between the words and the target word in the dependency trees to obtain a score to represent how likely a word is an opinion word for TOWE (called syntax-based possibility scores). These possibility scores would then be introduced into the deep learning models to improve the representation learning for TOWE.

In order to achieve such possibility score incorporation, we propose to employ the representation vectors for the words in the deep learning models to compute a model-based possibility score for each word in the sentence. The model-based possibility scores also aim to quantify the likelihood of being an opinion word for each word in the sentence; however, they are based on the internal representation learning mechanism of the deep learning models for TOWE. To this end, we propose to inject the information from the syntax-based possibility scores into the models for TOWE by enforcing the similarity/consistency between the syntax-based and model-based possibility scores for the words in the sentence. The rationale is to leverage the possibility score consistency to guide the representation learning process of the deep learning models (using the extracted syntactic information) to generate more effective representations for TOWE. In this work, we employ the Ordered-Neuron Long Short-Term Memory Networks (ON-LSTM) \cite{Shen2019ordered} to obtain the model-based possibility scores for the words in the sentences for TOWE. ON-LSTM introduces two additional gates into the original Long Short-Term Memory Network (LSTM) cells that facilitate the computation of the model-based possibility scores via the numbers of active neurons in the hidden vectors for each word.







For the second type of syntactic information in this work, the main motivation is to further improve the representation vector computation for each word by leveraging the dependency connections between the words to infer the effective context words for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector of a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``\textit{warranties}" as the target word and ``\textit{reputable}" as the word we need to compute the representation vector. On the one hand, it is important to include the information of the neighboring words of ``\textit{reputable}" (i.e., ``\textit{company}") in the representation so the models can know the context for the current word (e.g., which object ``\textit{reputable}" is modifying). On the other hand, the information about the target word (i.e., ``\textit{warranties}" and possibly its neighboring words) should also be encoded in the representation vector for ``\textit{reputable}" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label (i.e., non-opinion word) for ``\textit{reputable}" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``\textit{I}'' in the representation for ``\textit{reputable}" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word, given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network (GCN) model \citep{kipf2017semi} to produce the final representation vectors for opinion word prediction.

Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words and those for the other words in the sentence. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several benchmark datasets.




































\section{Related Work}











Comparing to the related tasks, TOWE has been relatively less explored in the literature. In particular, the most related task of TOWE is opinion word extraction (OWE) that aims to locate the terms used to express attitude in the sentences \citep{htay2013extracting, shamshurin2012extracting}. A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences \citep{qiu2011opinion,liu2015fine,poria2016aspect,yin2016unsupervised,xu2018double}. Note that some previous works have also attempted to jointly predict the target and opinion words \citep{qiu2011opinion,liu2013opinion,wang2016recursive,wang2017coupled,li2017deep}; however, the target words are still not paired with their corresponding opinion words in these studies.



As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods (i.e., based on word distances or syntactic patterns) \citep{zhuang2006movie,hu2004mining} and the recent deep learning models \cite{fan2019target,wu2020latent}. Our model is different from the previous deep learning models as we exploit the syntactic information (i.e., dependency trees) for TOWE with deep learning.













\section{Model}



The TOWE problem can be formulated as a sequence labeling task. Formally, given a sentence  of  words:  with  as the target word (), the goal is to assign a label  to each word  so the label sequence   for  can capture the target-oriented opinion words for . Following the previous work \citep{fan2019target}, we use the BIO tagging schema to encode the label  for TOWE (i.e.,  for being at the \textbf{B}eginning, \textbf{I}nside or \textbf{O}utside of the opinion words respectively). Our model for TOWE consists of four components that would be described in the following: (i) Sentence Encoding, (ii) Syntax-Model Consistency, (iii) Graph Convolutional Neural Networks, and (iv) Representation Regularization.



\subsection{Sentence Encoding}







In order to represent the input sentence , we encode each word  into a real-valued vector  based on the concatenation of the two following vectors: (1) the hidden vector of the first wordpiece of  from the last layer of the BERT model \cite{Devlin:19}, and (2) the position embedding for . For this vector, we first compute the relative distance  from  to the target word  (i.e., ). Afterward, we retrieve the position embedding for  by looking up  in a position embedding table (initialized randomly). The position embeddings are fine-tuned during training in this work. The resulting vector sequence  for  will be then sent to the next computation step.



\subsection{Syntax-Model Consistency}
\label{sec:importance-encoder}

As presented in the introduction, the goal of this component is to employ the dependency tree of  to obtain the syntax-based opinion possibility scores for the words. These scores would be used to guide the representation learning of the models via the consistency with the model-based possibility scores. In particular, as we consider the closer words to the target word  in the dependency tree of  as being more likely to be the target-oriented opinion words, we first compute the distance  between each word  to the target word  in the dependency tree (i.e., the number of words along the shortest path between  and ). Afterward, we obtain the syntax-based possibility score  for  based on: .



In order to implement the possibility score consistency, our deep learning model needs to produce  as the model-based possibility scores the words  in  respectively. While the model-based score computation would be explained later, given the model-based scores, the syntax-model consistency for possibility scores would be enforced by introducing the KL divergence  between the syntax-based and model-based scores into the overall loss function to minimize:




As mentioned in the introduction, in this work, we propose to obtain the model-based possibility scores for TOWE using the Ordered-Neuron Long Short-Term Memory Networks (ON-LSTM) \cite{Shen2019ordered}. ON-LSTM is an extension of the popular Long Short-Term Memory Networks (LSTM) that have been used extensively in Natural Language Processing (NLP). Concretely, given the vector sequence  as the input, a LSTM layer would produce a sequence of hidden vectors  via:

in which  is set to zero vector,  is the element-wise multiplication, and ,  and  are called the forget, input, and output gates respectively.















A major problem with the LSTM cell is that all the dimensions/neurons of the hidden vectors (for the gates) are equally important as these neurons are active/used for all the step/word  in . In other words, the words in  have the same permission to access to all the available neurons in the hidden vectors of the gates in LSTM. This might not be desirable as given a NLP task, the words in a sentence might have different levels of contextual contribution/information for solving the task. It thus suggests a mechanism where the words in the sentences have different access to the neurons in the hidden vectors depending on their informativeness. To this end, ON-LSTM introduces two additional gates  and  (the master forget and input gates) into the original LSTM mechanism using the  activation function (i.e., )\footnote{ where .}:

The benefit of  is to introduce a hierarchy over the neurons in the hidden vectors of the master gates so the higher-ranking neurons would be active for more words in the sentence and vice verse (i.e., the activity of the neurons is limited to only a portion of the words in the sentence in this case). In particular, as  applies the softmax function on the input vector whose outputs are aggregated over the dimensions, the result of  represents the expectation of a binary vector of the form  (i.e., two consecutive segments of 0's and 1's). The 1's segment in this binary vector determines the neurons/dimensions activated for the current step/word , thus enabling the different access of the words to the neurons. In ON-LSTM, a word is considered as more informative or important for the task if it has more active neurons (or a larger size for its 1's segment) in the master gates' hidden vectors than the other words in the sentence. As such, ON-LSTM introduces a mechanism to estimate an informativeness score  for each word  in the sentence based on the number of active neurons in the master gates. Following \citep{Shen2019ordered}, we approximate  via the sum of the weights of the neurons in the master forget gates, i.e., . Here,  is the number of dimensions/neurons in the hidden vectors of the ON-LSTM gates and  is the weight of the -th dimension for the master forget gate  at .








An important property of the target-oriented opinion words in our TOWE problem is that they tend to be more informative than the other words in the sentence (i.e., for understanding the sentiment of the target words). To this end, we propose to compute the model-based opinion possibility scores  for  based on the informativeness scores  from ON-LSTM via: . Consequently, by promoting the syntax-model consistency as in Equation \ref{eq:klloss}, we expect that the syntactic information from the syntax-based possibility scores can directly interfere with the internal computation/structure of the ON-LSTM cell (via the neurons of the master gates) to potentially produce better representation vectors for TOWE. For convenience, we also use  to denote the hidden vectors returned by running ON-LSTM over the input sequence vector  in the following.
























\subsection{Graph Convolutional Networks}
\label{sec:syntax-encoder}



This component seeks to extract effective context words to further improve the representation vectors  for the words in  based on the dependency connections between the words for TOWE. As discussed in the introduction, given the current word , there are two groups of important context words in  that should be explicitly encoded in the representation vector for  to enable effective opinion word prediction: (i) the neighboring words of , and (ii) the neighboring words of the target word  in the dependency tree (i.e., these words should receive higher weights than the others in the representation computation for ). Consequently, in order to capture such important context words for all the words in the sentence for TOWE, we propose to obtain two importance score matrices of size  for which the scores at cells  are expected to weight the importance of the contextual information from  with respect to the representation vector computation for  in . In particular, one score matrix would be used to capture the syntactic neighboring words of the current words (i.e., ) while the other score matrix would be reserved for the neighboring words of the target word . These two matrices would then be combined and consumed by a GCN model \citep{kipf2017semi} for representation learning. 

Specifically, for the syntactic neighbors of the current words, following the previous GCN models for NLP \citep{Marcheggiani:17,Nguyen:18,Veyseh:19b}, we directly use the adjacency binary matrix  of the dependency tree for  as the importance score matrix for this group of words. Note that  is only set to 1 if  is directly connected to  in the dependency tree or  in this case. In the next step for the neighboring words of the target word , as we expect the closer words to the target word  to have larger contributions for the representation vectors of the words in  for TOWE, we propose to use the syntactic distances (to the target word)  and  of  and  as the features to learn the importance score matrix  for the words in this case. In particular,  would be computed by:  where  is a feed-forward network to convert a vector input with five dimensions into a scalar score and  is the {\it sigmoid} function. Given the importance score matrices  and , we seek to integrate them into a single importance score matrix  to simultaneously capture the two groups of important context words for representation learning in TOWE via the weighted sum:  where  is a trade-off parameter\footnote{Note that we tried to directly learn  from the available information from  and  (i.e., ). However, the performance of this model was worse than the linear combination of  and  in our experiments.}.

In the next step for this component, we run a GCN model over the ON-LSTM hidden vectors  to learn more abstract representation vectors for the words in . This step will leverage  as the adjacency matrix to enrich the representation vector for each word  with the information from its effective context words (i.e., the syntactic neighboring words of  and ), potentially improving the opinion word prediction for . In particular, the GCN model in this work involves several layers (i.e.,  layers in our case). The representation vector  for the word  at the -the layer of the GCN model would be computed by:

where  and  are the weight matrix and bias for the -th GCN layer. The input vector  for GCN is set to the hidden vector  from ON-LSTM (i.e., ) for all  in this case. For convenience, we denote  as the hidden vector for  in the last layer of GCN (i.e.,  for all ). We also write  to indicate that  are the hidden vectors in the last layer of the GCN model run over the input  and the adjacency matrix  for simplicity.

Finally, given the syntax-enriched representation vectors  from ON-LSTM and  from the last layer of GCN, we form the vector  to serve as the feature to perform opinion word prediction for . In particular,  would be sent to a two-layer feed-forward network with the softmax function in the end to produce a probability distribution  over the possible opinion labels for  (i.e., B, I, and O). The negative log-likelihood function  would then be used as the objective function to train the overall model: .








\subsection{Representation Regularization}
\label{sec:reg}
There are three groups of words in the input sentence  for our TOWE problem, i.e., the target word , the target-oriented opinion words (i.e., the words we want to identify) (called ), and the other words (called ). After the input sentence  has been processed by several abstraction layers (i.e., ON-LSTM and GCN), we expect that the resulting representation vectors for the target word and the target-oriented opinion words would capture the sentiment polarity information for the target word while the representation vectors for the other words might encode some other context information in . We thus argue that the representation vector for the target word should be more similar to the representations for the words in  (in term of the sentiment polarity) than those for . To this end, we introduce an explicit loss term to encourage such representation distinction between these groups of words to potentially promote better representation vectors for TOWE. In particular, let , , and  be some representation vectors for the target word , the target-oriented opinion words (i.e., ), and the other words (i.e., ) in . The loss term for the representation distinction based on our intuition (i.e., to encourage  to be more similar to  than ) can be captured via the following triplet loss for minimization:



In this work, the representation vector for the target word is simply taken from last GCN layer, i.e., . However, as  and  might involve sets of words, we need to aggregate the representation vectors for the individual words in these sets to produce the single representation vectors  and . The simple and popular aggregation method in this case involves performing the max-pooling operation over the representation vectors (i.e., from GCN) for the individual words in each set (i.e., our baseline). However, this approach ignores the structures/orders of the individual words in  and , and fails to recognize the target word for better customized representation for regularization. To this end, we propose to preserve the syntactic structures among the words in  and  in the representation computation for regularization for these sets. This is done by generating the target-oriented pruned trees from the original dependency tree for  that are customized for the words in  and . These pruned trees would then be consumed by the GCN model in the previous section to produce the representation vectors for  and  in this part. In particular, we obtain the pruned tree for the target-oriented opinion words  by forming the adjacency matrix  where  if both  and  belong to some shortest dependency paths between  and some words in , and 0 otherwise. This helps to maintain the syntactic structures of the words in  and also introduce the target word  as the center of the pruned tree for representation learning. We apply the similar procedure to obtain the adjacency matrix  for the pruned tree for . Given the two adjacency matrices for the pruned trees, the GCN model in the previous section is run over the ON-LSTM vectors , resulting in two sequences of hidden vectors for  and , i.e.,  and . Afterward, we compute the representation vectors  and  for the sets  and  by retrieving the hidden vectors for the target word returned by the GCN model with the corresponding adjacency matrices, i.e.,  and . Note that the application of GCN over the pruned trees and the ON-LSTM vectors makes  and  more comparable with  in our case. This completes the description for the representation regularizer in this work. The overall loss function in this work would be:  where  and  are the trade-off parameters.





















\section{Experiments}
\subsection{Datasets \& Parameters}


We use four benchmark datasets presented in \citep{fan2019target} to evaluate the effectiveness of the proposed TOWE model. These datasets contain reviews for restaurants (i.e., the datasets {\bf 14res}, {\bf 15res} and {\bf 16res}) and laptops, (i.e., the dataset {\bf 14lap}). They are created from the widely used ABSA datasets from the SemEval challenges (i.e., SemEval 2014 Task 4 (14res and 14lap), SemEval 2015 Task 12 (15res) and SemEval 2016 Task 5 (16res)). Each example in these datasets involves a target word in a sentence where the opinion words have been manually annotated. 









As none of the datasets provides the development data, for each dataset, we sample 20\% of the training instances for the development sets. Note that we use the same samples for the development data as in \citep{fan2019target} to achieve a fair comparison. We use the 14res development set for hyper-parameter fine-tuning, leading to the following values for the proposed model (used for all the datasets): 30 dimensions for the position embeddings, 200 dimensions for the layers of the feed-forward networks and GCN (with  layers), 300 hidden units for one layer of ON-LSTM, 0.2 for  in , and 0.1 for the parameters  and .











\subsection{Comparing to the State of the Art}

\begin{table*}[t!]
\begin{center}
\resizebox{.95\textwidth}{!}{
\begin{tabular}{l|ccc|ccc|ccc|ccc}
  & \multicolumn{3}{c}{14res} & \multicolumn{3}{c}{14lap} & \multicolumn{3}{c}{15res} & \multicolumn{3}{c}{16res} \\
  Model & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\
  \hline
  Distance-rule \shortcite{hu2004mining} & 58.39 & 43.59 & 49.92 & 50.13 & 33.86 & 40.42 & 54.12 & 39.96 & 45.97 & 61.90 & 44.57 & 51.83 \\
  Dependency-rule \shortcite{zhuang2006movie} & 64.57 & 52.72 & 58.04 & 45.09 & 31.57 & 37.14 & 65.49 & 48.88 & 55.98 & 76.03 & 56.19 & 64.62 \\ 
     LSTM \shortcite{liu2015fine} & 52.64 & 65.47 & 58.34 & 55.71 & 57.53 & 56.52 & 57.27 & 60.69 & 58.93 & 62.46 & 68.72 & 65.33 \\
    BiLSTM \shortcite{liu2015fine}  & 58.34 & 61.73 & 59.95 & 64.52 & 61.45 &  62.71 & 60.46 & 63.65 & 62.00 & 68.68 & 70.51 & 69.57 \\
 Pipeline \shortcite{fan2019target} & 77.72 & 62.33 & 69.18 & 72.58 & 56.97 & 63.83 & 74.75 & 60.65 & 66.97 & 81.46 & 67.81 & 74.01 \\
    TC-BiLSTM \shortcite{fan2019target} & 67.65 & 67.67 & 67.61 & 62.45 &  60.14 & 61.21 & 66.06 & 60.16 & 62.94 & 73.46 & 72.88 & 73.10 \\
     IOG \shortcite{fan2019target} & 82.85 & 77.38 & 80.02 & 73.24 & 69.63 & 71.35 & 76.06 & 70.71 & 73.25 & 82.25 & 78.51 & 81.69 \\
     LOTN \cite{wu2020latent} & 84.00 & 80.52 & 82.21 & 77.08 & 67.62 & 72.02 & 76.61 & 70.29 & 73.29 & 86.57 & 80.89 & 83.62 \\
        \hline
        \textbf{ONG (Ours)} & 83.23 & 81.46 & {\bf 82.33} & 73.87 & 77.78 & {\bf 75.77} & 76.63 & 81.14 & {\bf 78.81} & 87.72 & 84.38 & {\bf 86.01} \\
\end{tabular}
}
\end{center}
\caption{\label{tab:results} Test set performance (i.e., Precision (P), Recall (R) and F1 scores) of the models.
  }
\end{table*}

We compare the TOWE model in this work (called {\bf ONG} for ON-LSTM and GCN) with the recent models in \cite{fan2019target,wu2020latent} and their baselines. More specifically, the following baselines are considered in our experiments:



    1. \textbf{Rule-based}: These baselines employ predefined patterns to extract the opinion-target pairs that could be either {\bf dependency-based} \citep{zhuang2006movie} or {\bf distance-based} \citep{hu2004mining}.


    2. \textbf{Sequence-based Deep Learning}: These approaches apply some deep learning model over the input sentences following the sequential order of the words to predict the opinion words (i.e., {\bf LSTM}/{\bf BiLSTM} \citep{liu2015fine}, {\bf TC-BiLSTM} \citep{fan2019target} and {\bf IOG} \citep{fan2019target}).


    3. \textbf{Pipeline with Deep Learning}: This method utilizes a recurrent neural network to predict the opinion words. The distance-based rules are then introduced to select the target-oriented opinion words (i.e., {\bf Pipeline}) \cite{fan2019target}.


    4. \textbf{Multitask Learning}: These methods seek to jointly solve TOWE and another related task (i.e., sentiment classification). In particular, the {\bf LOTN} model in \cite{wu2020latent} uses a pre-trained SA model to obtain an auxiliary label for each word in the sentence using distance-based rules. A bidirectional LSTM model is then trained to make prediction for both TOWE and the auxiliary labels\footnote{Note that \citep{peng2019knowing} also proposes a related model for TOWE based on multitask deep learning. However, the models in this work actually predict general opinion words that are not necessary tied to any target word. As we focus on target-oriented opinion words, the models in \citep{peng2019knowing} are not comparable with us.}.
    








Table \ref{tab:results} shows the performance of the models on the test sets of the four datasets. It is clear from the table that the proposed ONG model outperforms all the other baseline methods in this work. The performance gap between ONG and the other models are large and significant (with ) over all the four benchmark datasets (except for LOTN on 14res), clearly testifying to the effectiveness of the proposed model for TOWE. Among different factors, we attribute this better performance of ONG to the use of syntactic information (i.e., the dependency trees) to guide the representation learning of the models (i.e., with ON-LSTM and GCN) that is not considered in the previous deep learning models for TOWE.














\subsection{Model Analysis and Ablation Study}

There are three main components in the proposed ONG model, including the ON-LSTM component, the GCN component and the representation regularization component. This section studies different variations and ablated versions of such components to highlight their importance for ONG.





{\bf ON-LSTM}: First, we evaluate the following variations for the ON-LSTM component: (i) {\bf ONG - KL}: this model is similar to ONG, except that the syntax-model consistency loss based on KL  is not included in the overall loss function, (ii) {\bf ONG - ON-LSTM}: this model completely removes the ON-LSTM component in ONG (so the KL-based syntax-model consistency loss is not used and the input vector sequence  is directly sent to the GCN model), and (iii) {\bf ONG\_wLSTM}: this model replaces the ON-LSTM model with the traditional LSTM model in ONG (so the syntax-model consistency loss is also not employed in this case as LSTM does not support the neuron hierarchy for model-based possibility scores). The performance for these models on the test sets (i.e., F1 scores) are presented in Table \ref{tab:on-lstm}.

\begin{table}[ht]
\centering
    \resizebox{.44\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        Model & 14res & 14lap & 15res & 16res \\ \hline
        ONG & 82.33 & 75.77 & 78.81 & 86.01 \\ \hline
        ONG - KL &  80.91 & 73.34 & 76.21 & 83.78 \\
        ONG - ON-LSTM & 78.99 & 70.28 & 71.39 & 81.13 \\
        ONG\_wLSTM &  81.03 & 73.98 & 74.43 & 82.81 \\
    \end{tabular}
    }
    \caption{Performance of the ON-LSTM's variations.}
    \label{tab:on-lstm}
\end{table}
As we can see from the table, the syntax-model consistency loss with KL divergence is important for ONG as removing it would significantly hurt the model's performance on different datasets. The model also becomes significantly worse when the ON-LSTM component is eliminated or replaced by the LSTM model. These evidences altogether confirm the benefits of the ON-LSTM model with the syntax-model consistency proposed in this work.

{\bf GCN Structures}: There are two types of importance score matrices in the GCN model, i.e., the adjacency binary matrices  for the syntactic neighbors of the current words and  for the syntactic neighbors of the target word. This part evaluates the effectiveness of these score matrices by removing each of them from the GCN model, leading to the two ablated models {\bf ONG - } and {\bf ONG - } for evaluation. Table \ref{tab:gcn-structure} provides the performance on the test sets for these models (i.e., F1 scores). It is clear from the table that the absence of any importance score matrices (i.e.,  or ) would decrease the performance over all the four datasets and both matrices are necessary for ONG to achieve its highest performance.
\begin{table}[ht]
\centering
    \resizebox{.4\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        Model & 14res & 14lap & 15res & 16res \\ \hline
        ONG & 82.33 & 75.77 & 78.81 & 86.01 \\ \hline
        ONG -  & 80.98 & 73.05 & 75.51 & 83.72 \\
        ONG -  & 81.23 & 74.18 & 76.32 & 85.20 \\
    \end{tabular}
    }
    \caption{Ablation study on the GCN structures.}
    \label{tab:gcn-structure}
\end{table}


{\bf GCN and Representation Regularization}: As the representation regularization component relies on the GCN model to obtain the representation vectors, we jointly perform analysis for the GCN and representation regularization components in this part. In particular, we consider the following variations for these two components: (i) {\bf ONG - REG}: this model is similar to ONG except that the representation regularization loss  is not applied in the overall loss function, (ii) {\bf ONG\_REG\_wMP-GCN}: this is also similar to ONG; however, it does not apply the GCN model to compute the representation vectors  and  for regularization. Instead, it uses the simple max-pooling operation over the GCN-produced vectors  of the target-oriented words  and the other words  for  and :  and , (iii) {\bf ONG - GCN}: this model eliminates the GCN model from ONG, but still applies the representation regularization over the representation vectors obtained from the ON-LSTM hidden vectors. In particular, the ON-LSTM hidden vectors  would be employed for both opinion word prediction (i.e.,  only) and the computation of ,  and  for representation regularization with max-pooling (i.e., ,  and ) in this case, and (iv) {\bf ONG - GCN - REG}: this model completely excludes both the GCN and the representation regularization models from ONG (so the ON-LSTM hidden vectors  are used directly for opinion word prediction (i.e.,  as in ONG - GCN) and the regularization loss  is not included in the overall loss function). Table \ref{tab:gcn-reg} shows the performance of the models on the test datasets (i.e., F1 scores).
\begin{table}[ht]
\centering
    \resizebox{.46\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        Model & 14res & 14lap & 15res & 16res \\ \hline
        ONG & 82.33 & 75.77 & 78.81 & 86.01 \\ \hline
        ONG - REG & 80.88 & 73.89 & 75.92 & 84.03 \\
        ONG\_REG\_wMP-GCN & 80.72 & 72.44  & 74.28 & 84.29 \\
        ONG - GCN & 81.01 & 70.88 & 72.98 & 82.58 \\
        ONG - GCN - REG & 79.23 & 71.04 & 72.53 & 82.13 \\
    \end{tabular}
    }
    \caption{Performance of the variations of the GCN and representation regularization components.}
    \label{tab:gcn-reg}
\end{table}

There are several important observations from this table. First, as ONG - REG is significantly worse than the full model ONG over different datasets, it demonstrates the benefits of the representation regularization component in this work. Second, the better performance of ONG over ONG\_REG\_wMP-GCN (also over all the four datasets) highlights the advantages of the GCN-based representation vectors  and  over the max-pooled vectors for representation regularization. We attribute this to the ability of ONG to exploit the syntactic structures among the words in  and  for regularization in this case. Finally, we also see that the GCN model is crucial for the operation of the proposed model as removing it significantly degrades ONG's performance (whether the representation regularization is used (i.e., in ONG - GCN) or not (i.e., in ONG - GCN - REG). The performance become the worst when both the GCN and the regularization components are eliminated in ONG, eventually confirming the effectiveness of our model for TOWE in this work.







\begin{table}[t!]
\begin{center}
\resizebox{.48\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
  & \multicolumn{2}{c|}{14res} & \multicolumn{2}{c}{14lap} \\ \cline{2-5}
  Distance & ONG & ONG\_REG & ONG & ONG\_REG \\
  & & \_wMP-GCN & & \_wMP-GCN \\
  \hline
  1 & 83.22 & 79.94 & 76.91 & 75.21 \\
  2 & 83.18 & 78.43 & 75.03 & 73.12  \\ 
     3 & 81.56 & 75.41 & 74.21 & 70.69 \\
    3  & 80.97 & 73.77 & 73.92 & 66.23 \\ \hline
& \multicolumn{2}{c|}{15res} & \multicolumn{2}{c}{16res} \\ \cline{2-5}
Distance & ONG & ONG\_REG & ONG & ONG\_REG \\
  & & \_wMP-GCN & & \_wMP-GCN \\ \hline
 1 & 79.92 & 74.29 & 86.52 & 83.33 \\
 2 & 78.04 & 73.33 & 87.31 & 83.27 \\
 3  & 77.71 & 70.91 & 84.77 & 78.63 \\
 3 & 76.98 & 68.88 & 84.05 & 77.13\\
\end{tabular}
}
\end{center}
\caption{\label{tab:reg-Analysis} The performance (i.e., F1 scores) of ONG and ONG\_REG\_wMP-GCN on the four data folds of the development sets for 14res, 14lap, 15res, and 16res. The data folds are based on the target-opinion distances of the examples (called Distance in this table).}
\end{table}



\textbf{Regularization Analysis}: This section aims to further investigate the effect of the dependency structures  and  (i.e., among the words in  and ) to gain a better insight into their importance for the representation regularization in this work. Concretely, we again compare the performance of the full proposed model ONG (with the graph-based representations for  and ) and the baseline model ONG\_REG\_wMP-GCN (with the direct max-pooling over the word representations, i.e.,  and ). However, in this analysis, we further divide the sentences in the development sets into four folds and observe the models' performance on those fold. As such, for each sentence, we rely on the longest distance between the target word and some target-oriented opinion word in  in the dependency tree to perform this data split (called the target-opinion distance). In particular, the four data folds for the development sets (of each dataset) correspond to the sentences with the target-opinion distances of 1, 2, 3 or greater than 3. Intuitively, the higher target-opinion distances amount to more complicated dependency structures among the target-oriented opinion word in  (as more words are involved in the structures). The four data folds are thus ordered in the increasing complexity levels of the dependency structures in .

Table \ref{tab:reg-Analysis} presents the performance of the models on the four data folds for the development sets of the datasets in this work. First, it is clear from the table that ONG significantly outperforms the baseline model ONG\_REG\_wMP-GCN over all the datasets and structure complexity levels of . Second, we see that as the structure complexity (i.e., the target-opinion distance) increases, the performance of both ONG and ONG\_REG\_wMP-GCN decreases, demonstrating the more challenges presented by the sentences with more complicated dependency structures in  for TOWE. However, comparing ONG and ONG\_REG\_wMP-GCN, we find that ONG's performance decreases slower than those for ONG\_REG\_wMP-GCN when the target-opinion distance increases (for all the four datasets considered in this work). This implies that the complicated dependency structures in  have more detrimental effect on the model's performance for ONG\_REG\_wMP-GCN than those for ONG, leading to the larger performance gaps between ONG and ONG\_REG\_wMP-GCN. Overall, these evidences suggest that the sentences with complicated dependency structures for the words in  are more challenging for the TOWE models and modeling such dependency structures to compute the representation vectors  and  for regularization (as in ONG) can help the models to better perform on these cases.





\begin{comment}

To better understand the effectiveness of the main contributions of this work, we compare the model with the simplified version of it where we remove the components that bring the main contributions. In Our model, we have two main novelties: (1) Incorporating syntax-based importance scores into model computations: We employ the dependency tree to compute the syntax-based importance scores of all words toward the given target. These importance scores are incorporated into the model via KL-Divergence between the syntax-based importance scores and the model-based importance scores obtained by ON-LSTM architecture. In order to analyze how this incorporation of the syntax-based importance scores into the model computation is important, we ablate the KL-Divergence loss (i.e., ) from the final loss function and retrain the model on training set of each dataset. This component is denoted by \textbf{KL} in our experiment. (2) Syntax-based regulation: In this work, we proposed a novel technique to regularize the information propagation in GCN so that only the target-related information could be propagated in the graph. In order to assess the usefulness of this technique, we ablate the syntax-based regularization loss (i.e., ) from the final loss function and re-train the model on the training set of each datasets. This component is denoted by \textbf{SYN-REG} in our experiment. In addition to ablating each component separately, we also ablate them together. Note that in this case the final loss function only consists of the  loss. The results for this experiment are demonstrated in Table \ref{tab:ablation}. From this table we observe that both components are requred for the model to achieve its best performance. In addition, removing both of them hurt the most, indicating these components do not interfere with each other. 

\begin{table}[ht]
    \centering
    \resizebox{.5\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        Model & 14res & 14lap & 15res & 16res \\ \hline
        Proposed Model & 82.33 & 75.77 & 78.81 & 86.01 \\ \hline
        -KL &  80.91 & 73.34 & 76.21 & 83.78 \\
        -SYN-REG & 80.88 & 73.89 & 75.92 & 84.03 \\
        - KL, -SYN-REG & 79.02 & 73.00 & 73.95 & 82.91
    \end{tabular}
    }
    \caption{Comparison of the performance of the ablated models on the test sets in terms of F1 score}
    \label{tab:ablation}
\end{table}

\subsection{Design Analysis}
In order to provide more insight into the effectiveness of the proposed architecture, in this section we analyze how alternative architectures perform in our model for the TOWE task. To this end, we analyze two major components in our model: (1) ON-LSTM: In our model, we employ ON-LSTM to infer the model-based importance scores and thereby incorporate syntax-based importance scores in our model computation. As ON-LSTM is an extension of the well-known LSTM architecture, we are interested to assess how much the improvement obtained by ON-LSTM is due to the original LSTM architecture and how much of it is due to the new architecture design of the ON-LSTM. To this end, we propose two baselines: (A) In the first baseline, we remove ON-LSTM and replace it with LSTM. Note that in this baseline the  is also removed from the final loss function. We denote this baseline by \textbf{LSTM} (B) We remove the ON-LSTM and the word representations (e.g., word embeddings) are direcly send to the GCN layer.  This baseline is denoted by \textbf{ON-LSTM/wo}. (2) Trainable Syntax Structure: As discussed in section \ref{sec:syntax-encoder}, in addition to the dependency tree, in our model, we employ a trainable dense graph in which its nodes are the words and the weights of the edges are function of the importance of the two heads of the edge toward the target word in the dependency tree. In order to analyze the importance of using both the trainable dense graph and the dependency tree in syntax encoder, we study two baselines: (A) We use only the dependency tree and discard the new trainable dense graph in the syntax encoder, i.e., we set the trade-off parameter  to one. This baseline is denoted by \textbf{SYN-FIX} (B) We use only the trainable dense graph and ignore the dependency tree to encode the syntactic structure of the sentence. i.e., we set the trade-off parameter  to zero. This baseline is denoted by \textbf{SYN-TRAIN}. The results for this experiment are shown in Table \ref{tab:config}. This table shows that none of the baselines achieve the same performance as the proposed model, indicating the effectiveness of the proposed architecture. Moreover, comparing LSTM and ON-LSTM/wo reveals the importance of contextualizing the word representations using a recurrent neural network (RNN), e.g., LSTM, as directly consuming the word representaions in GCN, i.e., the ON-LSTM/wo baseline, performs significantly worse than the baseline equipped with an RNN layer, i.e., the LSTM baseline. As another observation from this table, we see that dependency tree is more effective than the trainable dense graph as removing it will hurt more than removing the trainable dense graph. This is expected because the trainable graph is a dense graph and could introduce some non-relevant connections between words. However, this graph is still required to remedy some noisy or non-relevant connections in the dependency graph as removing the trainable graph hurt the model performance. 

\begin{table}[ht]
    \centering
    \resizebox{.5\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        Model & 14res & 14lap & 15res & 16res \\ \hline
        Proposed Model & 82.33 & 75.77 & 78.81 & 86.01 \\ \hline
        LSTM &  81.03 & 73.98 & 74.43 & 82.81 \\
        ON-LSTM/wo & 78.99 & 70.28 & 71.39 & 81.13 \\
        SYN-FIX & 81.23 & 74.18 & 76.32 & 85.20 \\
        SYN-TRAIN & 80.98 & 73.05 & 75.51 & 83.72 \\
    \end{tabular}
    }
    \caption{Comparison of the performance of different model design on the test sets in terms of F1 score}
    \label{tab:config}
\end{table}



\subsection{Syntax Encoder Analysis}
As discussed earlier, comparing to the recent deep learning models for TOWE task, our model is equipped with the syntactic structure of the sentence. This structure which is modeled by the syntax encoder component has two benefits in our model: (1) to enhance word representations using the GCN and (2) to preserve target-related information using the syntax-based regularization. In order to analyze these effects of the syntactic structure of the sentence, we conduct an experiment in which we eliminate or alter the components which are responsible for the aforementioned effects of the syntactic structure. First, we propose to use two separate GCNs, one in the prediction component that consumes ON-LSTM outputs and produce representations employed by the decoder and another one in the syntax-based regularization component to compute the regularization loss . We call this baseline \textbf{SEP-GCN}. In addition, as another baseline, we propose to remove the GCN therefore we also remove the syntax-based regularization component as it employs the GCN. We call this ablated model \textbf{Syntax-Free}. In addition to this baseline, we also introduce two methods as alternatives to the syntax-based regularization.  First, we propose to keep the GCN only for the syntax-based regularization. That is, the hidden states of the ON-LSTM are sent to: (1) the prediction component to make a prediction and to compute the prediction loss  and (2) the syntax-based regularization component to compute the regularization loss . We call this baseline \textbf{GCN-REG}. The second alternative method is to remove the GCN and compute the regularization loss  using the max pooling of the related words (i.e., the target, the opinion and the words on the dependency path between the target and the opinion words) and the non-related words. More specifically, the regularization loss is computed by , where ,  and  are max pooling across the sentence, the related and non-related words (respectively) and  is the hidden states from ON-LSTM. We call this baseline \textbf{MP-REG}. The F1 score on test sets for all of these baselines are reported in Table \ref{tab:gcn}. This table shows that none of these baselines could outperform the proposed model. Moreover, Syntax-Free has the worst performance among others, indicating the importance of the two effects of the syntax (i.e., enhancing word representations and preserving related information). In addition, comparing GCN-REG and MP-REG, we observe that GCN-REG achieves better performance on all datasets. This shows the usefulness of using GCN instead of max pooling for the syntax-based regularization.  

\begin{table}[ht]
    \centering
    \resizebox{.5\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        Model & 14res & 14lap & 15res & 16res \\ \hline
        Proposed Model & 82.33 & 75.77 & 78.81 & 86.01 \\ \hline
        SYN-Free &  79.23 & 71.04 & 72.53 & 82.13 \\
        GCN-REG & 81.31 & 71.92 & 73.10 & 83.05 \\
        MP-REG REG & 81.01 & 70.88 & 72.98 & 82.58 \\
        SEP-GCN & 80.59 & 73.42 & 74.52 & 83.91 \\
        Dep Feature & 79.23 & 73.98 & 74.21 & 84.76 \\
        MP-REG on GCN & 80.72 & 72.44  & 74.28 & 84.29
    \end{tabular}
    }
    \caption{Analysis of the application of the sentence structure. Numbers are F1 scores of the models on the test sets.}
    \label{tab:gcn}
\end{table}

\subsection{Case Study}
\label{sec:analysis}

In order to provide more instight into the performance of the model, we study the cases in which the proposed model correctly predicts the opinion words for the given target while the baseline \cite{fan2019target} fails to make a correct prediction. Table \ref{tab:example} shows three examples of these cases. In all of these cases, the target and the opinion words are sequentially far from each other in the sentence. Thus, a sequential model such as \cite{fan2019target} fails to effectively encode the dependency between the opinion and the target word. On the other hand, as our model enjoys the syntactic structure, it could efficiently encode the long dependencies between words. This capability of encoding long dependencies is due to the short distance between the target and the opinion word in the dependency tree. 

\begin{table*}[t!]
    \centering
    \begin{tabular}{ m{10cm}}
        \hline
       The \textbf{mouse} is a little bit different than what you're used to though- it has one big flat panel and one full bar (instead of two separate ones) to click with- but you get used to it \textbf{quite} quickly. \\ \hline
       We have had numerous \textbf{problems} with Vista, such as Adobe Flash player just quits and has to be uninstalled and then reinstalled, Internet Explore just quits and you lose whatever you were working on, also, the same \textbf{Windows update} has appeared on this computer since we got it and has been updated probably 400 times, the same update. \\ \hline
       Its pretty \textbf{fast} and does not have hiccups while I am using it for web browsing, uploading photos, watching movies (720p) on occasion and \textbf{creating presentations}. \\ \hline
    \end{tabular}
    \caption{The GCN-failure examples. The two entity mentions of interest are shown in bold in the sentences.}
    \label{tab:example}
\end{table*}

\end{comment}

\section{Conclusion}

We propose a novel deep learning model for TOWE that seeks to incorporate the syntactic structures of the sentences into the model computation. Two types of syntactic information are introduced in this work, i.e., the syntax-based possibility scores for words (integrated with the ON-LSTM model) and the syntactic connections between the words (applied with the GCN model with novel adjacency matrices). We also present a novel inductive bias to improve the model, leveraging the representation distinction between the words in TOWE. Comprehensive analysis is done to demonstrate the effectiveness of the proposed model over four datasets.





\section*{Acknowledgement}



This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2019-19051600006 under the Better Extraction from Text Towards Enhanced Retrieval (BETTER) Program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, the Department of Defense, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. This document does not contain technology or technical data controlled under either the U.S. International Traffic in Arms Regulations or the U.S. Export Administration Regulations.

\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}

\clearpage

\appendix

\iffalse
\section{Representation Regularization Analysis}
\label{app:rr}



\begin{table}[t!]
\begin{center}
\resizebox{.45\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
  & \multicolumn{2}{c|}{14res} & \multicolumn{2}{c}{14lap} \\ \cline{2-5}
  Distance & ONG & ONG\_REG & ONG & ONG\_REG \\
  & & \_wMP-GCN & & \_wMP-GCN \\
  \hline
  1 & 83.22 & 79.94 & 76.91 & 75.21 \\
  2 & 83.18 & 78.43 & 75.03 & 73.12  \\ 
     3 & 81.56 & 75.41 & 74.21 & 70.69 \\
    3  & 80.97 & 73.77 & 73.92 & 66.23 \\ \hline
& \multicolumn{2}{c|}{15res} & \multicolumn{2}{c}{16res} \\ \cline{2-5}
Distance & ONG & ONG\_REG & ONG & ONG\_REG \\
  & & \_wMP-GCN & & \_wMP-GCN \\ \hline
 1 & 79.92 & 74.29 & 86.52 & 83.33 \\
 2 & 78.04 & 73.33 & 87.31 & 83.27 \\
 3  & 77.71 & 70.91 & 84.77 & 78.63 \\
 3 & 76.98 & 68.88 & 84.05 & 77.13\\
\end{tabular}
}
\end{center}
\caption{\label{tab:reg-Analysis} \small The performance (i.e., F1 scores) of ONG and ONG\_REG\_wMP-GCN on the four data folds of the development sets for 14res, 14lap, 15res, and 16res. The data folds are based on the target-opinion distances of the examples (called Distance in this table).}
\end{table}

In section \ref{sec:reg}, we argue that the representation vector for the target word should be more similar to target-oriented opinion words' (i.e., ) than those for the other words in the sentence (i.e., ). In order to enforce this similarity constraint in the model, we introduce the triple loss for representation regularization in Equation \ref{eq:reg-loss} where  and  stand for the representation vectors for the target-oriented opinion words  and the other words  (respectively). In this work, we propose to compute the representation vectors  and  by running a GCN model over the target-oriented pruned dependency trees that are customized for the words in  and  (respectively). The rationale is to exploit the dependency structures among the words in  and  to improve the representation vectors  and  for the regularization purpose. In the main paper, we show that this graph-based representation computation for  and  is significantly better than directly performing max-pooling over the representation vectors for the words in  and , thereby demonstrating the benefit of the dependency structures for  and . In this section, we aim to further investigate the effect of such dependency structures (i.e., among the words in  and ) to gain a better insight into their importance for the representation regularization in this work. Concretely, we again compare the performance of the full proposed model ONG (with the graph-based representations for  and ) and the baseline model ONG\_REG\_wMP-GCN (with the direct max-pooling over the word representations, i.e.,  and ). However, in this analysis, we further divide the sentences in the development sets into four folds and observe the models' performance on those fold. As such, for each sentence, we rely on the longest distance between the target word and some target-oriented opinion word in  in the dependency tree to perform this data split (called the target-opinion distance). In particular, the four data folds for the development sets (of each dataset) correspond to the sentences with the target-opinion distances of 1, 2, 3 or greater than 3. Intuitively, the higher target-opinion distances amount to more complicated dependency structures among the target-oriented opinion word in  (as more words are involved in the structures). The four data folds are thus ordered in the increasing complexity levels of the dependency structures in .

Table \ref{tab:reg-Analysis} presents the performance of the models on the four data folds for the development sets of the datasets in this work. First, it is clear from the table that ONG significantly outperforms the baseline model ONG\_REG\_wMP-GCN over all the datasets and structure complexity levels of . Second, we see that as the structure complexity (i.e., the target-opinion distance) increases, the performance of both ONG and ONG\_REG\_wMP-GCN decreases, demonstrating the more challenges presented by the sentences with more complicated dependency structures in  for TOWE. However, comparing ONG and ONG\_REG\_wMP-GCN, we find that ONG's performance decreases slower than those for ONG\_REG\_wMP-GCN when the target-opinion distance increases (for all the four datasets considered in this work). This implies that the complicated dependency structures in  have more detrimental effect on the model's performance for ONG\_REG\_wMP-GCN than those for ONG, leading to the larger performance gaps between ONG and ONG\_REG\_wMP-GCN. Overall, these evidences suggest that the sentences with complicated dependency structures for the words in  are more challenging for the TOWE models and modeling such dependency structures to compute the representation vectors  and  for regularization (as in ONG) can help the models to better perform on these cases.







\fi

\iffalse
\section{Reproducibility Checklist}
\label{app:repo}

\begin{itemize}






\item \textbf{Source code with specification of all dependencies, including external libraries}: The source code along with a read-me file is added to the submission. The read-me file provides information about the dependencies including external libraries and instructions on how to run the training. 



\item \textbf{Description of computing infrastructure
used}: We use a single GeForce RTX 2080 GPU with 11GB memory in this work. PyTorch 1.1 is used to implement the models.


\item \textbf{Average runtime for each approach}: Each epoch of our full model ONG on average takes 5 minutes on the 14res dataset and 3 minutes for the other datasets (i.e., 14lap, 15res and 16res) and we train the model for 10 epochs. The best epoch is chosen based on the F1 score over the development set for each dataset.


\item \textbf{Number of parameters in the model}: Our full model ONG has 12 million parameters to be optimized during training. As our model uses similar hyperparameters for the datasets (i.e., fine-tuned on the development set of 14res), this number of parameters also apply for all the datasets. Also, note that we do not fine tune the word embeddings for the models in this work.

\item \textbf{Explanation of evaluation metrics used,
with links to code}: In the experiments, we evaluate the models using the same evaluation metrics and scripts as in the prior work (i.e., provided by \citep{fan2019target}).







\item \textbf{Bounds for each hyperparameter}: To train the proposed model ONG, we choose the learning rate from [-5, -4, -3, -2, -1, -0] for the Adam optimizer, the mini-batch size from , the dimensionality of the position embeddings from  , the dimensionality for the hidden vectors in the layers of all the feed-forward, GCN and ON-LSTM networks from , the numbers of layers for GCN and ON-LSTM from , and the trade-off parameters ,  and  in the overall loss function and the importance score matrix  from .



\item \textbf{Hyperparameter configurations for best-performing models}: The best values for the hyper-parameters are reported in the main paper.

\item \textbf{The method of choosing hyperparameter values and the criterion used to select among them}: We tune the hyperparameters for the proposed model ONG using a random search. All the hyperparameters are selected based on the F1 scores on the development set of 14res. The same hyper-parameters from this fine-tuning are then applied for the other datasets.



\end{itemize}





\fi

\end{document}
