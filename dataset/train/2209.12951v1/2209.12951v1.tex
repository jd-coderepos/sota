





\documentclass{MITcsail}
\usepackage{times}
\usepackage{calc}
\usepackage{xlop}
\usepackage{amsmath}
\usepackage{amsthm}  
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{color}
\usepackage{mathtools} 
\usepackage{algorithm,algorithmicx,algpseudocode}

\title{Liquid Structural State-Space Models}

\author
{Ramin Hasani~*~\footnote{Code Repository: \texttt{https://github.com/raminmh/liquid-s4}}, Mathias Lechner~*, Tsun-Hsuan Wang,~Makram Chahine,~Alexander Amini, Daniela Rus\\
\vspace{1em} \normalfont{\small 
Computer Science and Artificial Intelligence Lab (CSAIL) \\
Massachusetts Institute of Technology (MIT)\\
Cambridge, 02139, MA} \\
* indicates authors with equal contributions \\
Correspondence to \texttt{rhasani@mit.edu}
\vspace{2em}
}

\begin{document}

\maketitle
\thispagestyle{firstpagestyle} 

\begin{abstract}
A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32\% on the Long-Range Arena benchmark. On the full raw Speech Command recognition dataset Liquid-S4 achieves 96.78\% accuracy with 30\% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.
\end{abstract}

\section{Introduction}

Learning representations from sequences of data requires expressive temporal and structural credit assignment. In this space, the continuous-time neural network class of liquid time-constant networks (LTC) \citep{hasani2021liquid} has shown theoretical and empirical evidence for their expressivity and their ability to capture the cause and effect of a given task from high-dimensional sequential demonstrations \citep{lechner2020neural,vorbach2021causal}. Liquid networks are nonlinear state-space models (SSMs) with an input-dependent state transition module that enables them to learn to adapt the dynamics of the model to incoming inputs, at inference, as they are dynamic causal models \citep{friston2003dynamic}. Their complexity, however, is bottlenecked by their differential equation numerical solver that limits their scalability to longer-term sequences. How can we take advantage of LTC's generalization and causality capabilities and scale them to competitively learn long-range sequences without gradient issues, compared to advanced recurrent neural networks (RNNs) \citep{rusch2021coupled,erichson2021lipschitz,gu2020hippo}, convolutional neural networks (CNNs) \citep{lea2016temporal,romero2021ckconv,cheng2022classification}, and attention-based models \citep{vaswani2017attention}? 

In this work, we set out to leverage the elegant formulation of structural state-space models (S4) \citep{gu2022efficiently} to obtain linear liquid network instances that possess the approximation capabilities of both S4 and LTCs. This is because structural SSMs are shown to largely dominate advanced RNNs, CNNs, and Transformers across many data modalities such as text, sequence of pixels, audio, and time series \citep{gu2021combining,gu2022efficiently,gu2022parameterization,gupta2022diagonal}. Structural SSMs achieve such impressive performance by using three main mechanisms: 1) High-order polynomial projection operators (HiPPO) \citep{gu2020hippo} that are applied to state and input transition matrices to memorize signals' history, 2) diagonal plus low-rank parametrization of the obtained HiPPO \citep{gu2022efficiently}, and 3) an efficient (convolution) kernel computation of an SSM's transition matrices in the frequency domain, transformed back in time via an inverse Fourier transformation \citep{gu2022efficiently}. 

To combine S4 and LTCs, instead of modeling sequences by linear state-space models of the form , , (as done in structural and diagonal SSMs \citep{gu2022efficiently,gu2022parameterization}), we propose to use a linearized LTC state-space model \citep{hasani2021liquid}, given by the following dynamics: , . We show that this dynamical system can also be efficiently solved via the same parametrization of S4, giving rise to an additional convolutional Kernel that accounts for the similarities of lagged signals. We call the obtained model Liquid-S4. Through extensive empirical evaluation, we show that Liquid-S4 consistently leads to better generalization performance compared to all variants of S4, CNNs, RNNs, and Transformers across many time-series modeling tasks. In particular, we achieve SOTA performance on the Long Range Arena benchmark \citep{tay2020long} with an average of 87.32\%. To sum up, we make the following contributions:
\begin{enumerate}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
    \item We introduce Liquid-S4, a new state-space model that encapsulates the generalization and causality capabilities of liquid networks as well as the memorization, efficiency and scalability of S4.
    \item We achieve State-of-the-art performance on pixel-level sequence classification, text, speech recognition and all six tasks of the long-range arena benchmark with an average accuracy of 87.32\%. On the full raw Speech Command recognition dataset Liquid-S4 achieves 96.78\% accuracy with 30\% reduction in parameter. Finally on the BIDMC vital signs dataset Liquid-S4 achieves SOTA in all modes.
\end{enumerate}

\section{Related Works}

\textbf{Learning Long-Range Dependencies with RNNs.} 
Sequence modeling can be performed autoregressively with RNNs which possess persistent states \citep{little1974existence} originated from Ising \citep{brush1967history} and Hopfield networks \citep{hopfield1982neural,ramsauer2020hopfield}. Discrete RNNs approximate continuous dynamics step-by-steps via dependencies on the history of their hidden states,  and continuous-time (CT) RNNs use ordinary differential equation (ODE) solvers to unroll their dynamics with more elaborate temporal steps \citep{funahashi1993approximation}. 

CT-RNNs can perform remarkable credit assignment in sequence modeling problems both on regularly sampled, irregularly-sampled data \citep{pearson2003imbalanced,li2016scalable,belletti2016scalable,roy2020robust,foster1996wavelets,amigo2012transcripts,kowal2019functional}, by turning the spatiotemproal dependencies into vector fields \citep{chen2018neural}, enabling better generalization and expressivity \citep{massaroli2020dissecting,hasani2021liquid}. Numerous works have studied their characteristics to understand their applicability and limitations in learning sequential data and flows \citep{lechner2019designing,dupont2019augmented,durkan2019neural,jia2019neural,grunbacher2021verification,hanshu2020robustness,holl2020learning,quaglino2020snode,kidger2020neural,hasani2020natural,liebenwein2021sparse,gruenbacher2022gotube}.

However, when these RNNs are trained by gradient descent \citep{rumelhart1986learning,allen2019can,sherstinsky2020fundamentals}, they suffer from the vanishing/exploding gradients problem, which makes difficult the learning of long-term dependencies in sequences \citep{hochreiter1991untersuchungen,bengio1994learning}. This issue happens in both discrete RNNs such as GRU-D with its continuous delay mechanism \citep{che2018recurrent} and Phased-LSTMs \citep{neil2016phased}, and continuous RNNs such as ODE-RNNs \citep{rubanova2019latent}, GRU-ODE \citep{de2019gru}, Log-ODE methods \citep{morrill2020neural} which compresses the input time-series by time-continuous path signatures \citep{friz2010multidimensional}, and neural controlled differential equations  \citep{kidger2020neural}, and liquid time-constant networks (LTCs) \citep{hasani2021liquid}. 

Numerous solutions have been proposed to resolve these gradient issues to enable long-range dependency learning. Examples include discrete gating mechanisms in LSTMs \citep{hochreiter1997long,greff2016lstm,hasani2019response}, GRUs \citep{chung2014empirical}, continuous gating mechanisms such as CfCs \citep{hasani2021closed},  hawks LSTMs \citep{mei2017neural}, IndRNNs \citep{li2018independently}, state regularization \citep{wang2019state}, unitary RNNs \citep{jing2019gated}, dilated RNNs \citep{chang2017dilated}, long memory stochastic processes \citep{greaves2019statistical}, recurrent kernel networks \citep{chen2019recurrent}, Lipschitz RNNs \citep{erichson2021lipschitz}, symmetric skew decomposition \citep{wisdom2016full}, infinitely many updates in iRNNs \citep{kag2019rnns}, coupled oscillatory RNNs (coRNNs) \citep{rusch2021coupled}, mixed-memory RNNs \citep{lechner2021mixed}, and Legendre Memory Units \citep{voelker2019legendre}.

\noindent \textbf{Learning Long-range Dependencies with CNNs and Transformers.} RNNs are not the only solution to learning long-range dependencies. Continuous convolutional kernels such as CKConv \citep{romero2021ckconv} and \citep{romero2021flexconv}, and circular dilated CNNs \citep{cheng2022classification} have shown to be efficient in modeling long sequences faster than RNNs. There has also been a large series of works showing the effectiveness of attention-based methods for modeling spatiotemporal data. A large list of these models is listed in Table \ref{tab:lra}. These baselines have recently been largely outperformed by the structural state-space models \citep{gu2022efficiently}.

\noindent \textbf{State-Space Models.} SSMs are well-established frameworks to study deterministic and stochastic dynamical systems \citep{kalman1960new}. Their state and input transition matrices can be directly learned by gradient descent to model sequences of observations \citep{lechner2020gershgorin,hasani2021liquid,gu2021combining}. In a seminal work, \citet{gu2022efficiently} showed that with a couple of fundamental algorithmic methods on memorization and computation of input sequences, SSMs can turn into the most powerful sequence modeling framework to-date, outperforming advanced RNNs, temporal and continuous CNNs \citep{cheng2022classification,romero2021ckconv,romero2021flexconv} and a wide variety of Transformers \citep{vaswani2017attention}, available in Table \ref{tab:lra} by a significant margin. 

The key to their success is their diagonal plus-low rank parameterization of the transition matrix of SSMs via higher-order polynomial projection (HiPPO) matrix \citep{gu2020hippo} obtained by a scaled Legendre measure (LegS) inspired by the Legendre Memory Units \citep{voelker2019legendre} to memorize input sequences, a learnable input transition matrix, and an efficient Cauchy Kernel algorithm, results in obtaining structural SSMs named S4. It was also shown recently that diagonal SSMs (S4D) \citep{gupta2022diagonal} could be as performant as S4 in learning long sequences when parametrized and initialized properly \citep{gu2022parameterization,gu2022train}. There was also a new variant of S4 introduced as simplified-S4 (S5) \cite{smith2022simplified} that tensorizes the 1-D operations of S4 to gain a more straightforward realization of SSMs. Here, we introduce Liquid-S4, which is obtained by a more expressive SSM, namely liquid time-constant (LTC) representation \citep{hasani2021liquid} which achieves SOTA performance across many benchmarks.

\section{Setup and Methodology}
In this section, we first revisit the necessary background to formulate our Liquid Structural State-Space Models. We then set up and sketch our technical contributions. 

\subsection{Background}
We aim to design an end-to-end sequence modeling framework built by SSMs. A continuous-time SSM representation of a linear dynamical system is given by the following set of equations: 

\begingroup
\small

\endgroup

Here,  is an -dimensional latent state, receiving a 1-dimensional input signal , and computing a 1-dimensional output signal . , ,  and  are system's parameters. For the sake of brevity, throughout our analysis, we set  as it can be added eventually after construction of our main results in the form of a skip connection \citep{gu2022efficiently}. 

\noindent \textbf{Discretization of SSMs.} In order to create a sequence-to-sequence model similar to a recurrent neural network (RNN), we discretize the continuous-time representation of SSMs by the trapezoidal rule (bilinear transform)\footnote{} as follows (sampling step = ) \citep{gu2022efficiently}:
\begingroup
\small

\endgroup

This is obtained via the following modifications to the transition matrices:

\begingroup
\small

\endgroup

With this transformation, we constructed a discretized seq-2-seq model that can map the input  to output , via the \emph{hidden} state .  is the hidden transition matrix,  and  are input and output transition matrices, respectively. 


\noindent \textbf{Creating a Convolutional Representation of SSMs.} The system described by (\ref{eq:ss_disc}) and (\ref{eq:ss_disc_params}), can be trained via gradient descent to learn to model sequences, in a sequential manner which is not scalable. To improve this, we can write the discretized SSM in (\ref{eq:ss_disc}) as a discrete convolutional kernel. To construct the convolutional kernel, let us unroll the system (\ref{eq:ss_disc}) in time as follows, assuming a zero initial hidden states :

\begingroup
\small

\endgroup

The mapping  can now can be formulated into a convolutional kernel explicitly:

\begingroup
\small




\endgroup

Equation (\ref{eq:conv_kernel}) is a non-circular convolutional kernel. \cite{gu2022efficiently} showed that under the condition that  is known, it could be solved very efficiently by a black-box Cauchy kernel computation pipeline. 

\subsection{Liquid Structural State-Space Models}
In this work, we construct a convolutional kernel corresponding to a linearized version of LTCs \citep{hasani2021liquid}; an expressive class of continuous-time neural networks that demonstrate attractive generalizability out-of-distribution and are dynamic causal models \citep{vorbach2021causal,friston2003dynamic,hasani2020natural}. In their general form, the state of a liquid time-constant network at each time-step is given by the set of ODEs described below \citep{hasani2021liquid}: 

\begingroup
\small

\endgroup

In this expression,  is the vector of hidden state of size ,  is an input signal with  features,  is a time-constant state-transition mechanism,  is a bias vector, and  represents the Hadamard product.  is a bounded nonlinearity parametrized by . 

Our objective is to show how the liquid time-constant (i.e., an input-dependent state transition mechanism in state-space models can enhance its generalization capabilities by accounting for the covariance of the input samples. To do this, we linearize the LTC formulation of Eq. \ref{eq:ltc} in the following to better connect the model to SSMs. Let's dive in:

\noindent \textbf{Linear Liquid Time-Constant State-Space Model.} A Linear LTC SSM can be presented by the following coupled bilinear (first order bilinear Taylor approximation \citep{penny2005bilinear}) equation:

\begingroup
\small

\endgroup

Similar to (\ref{eq:ss_continuous}),  is an -dimensional latent state, receiving a 1-dimensional input signal , and computing a 1-dimensional output signal . , , and . Note that \textbf{D} is set to zero for simplicity. In (\ref{eq:ltc_continuous}), the first  is added element-wise to . This dynamical system allows the coefficient (state transition compartment) of state vector  to be input dependent which, as a result, allows us to realize more complex dynamics. 

\noindent \textbf{Discretization of Liquid-SSMs.} Similar to SSMs, Liquid-SSMs can also be discretized by a bilinear transform (trapezoidal rule) to construct a sequence-to-sequence model as follows:

\begingroup
\small

\endgroup

The discretized parameters , , and  are identical to that of (\ref{eq:ss_disc_params}), which are function of the continuous-time coefficients , , and , and the discretization step .

\noindent \textbf{Creating a Convolutional Representation of Liquid-SSMs.} Similar to (\ref{eq:unroll_ssm}), we first unroll the Liquid-SSM in time to construct a convolutional kernel of it. By assuming , we have:

\begingroup
\footnotesize

\endgroup

The resulting expressions of the Liquid-SSM at each time step consist of two types of weight configurations: 1. Weights corresponding to the mapping of individual time instances of inputs independently, shown in black in (\ref{eq:unroll_liquid_ssm}), and 2. Weights associated with all orders of auto-correlation of the input signal, shown in violet in (\ref{eq:unroll_liquid_ssm}). 
The first set of weights corresponds to the convolutional kernel of the simple SSM, shown by Eq. \ref{eq:conv_kernel} and Eq. \ref{eq:conv_kernel_done}, whereas the second set leads to the design of an additional input correlation kernel, which we call the \emph{liquid} kernel. These kernels generate the following input-output mapping:
\begingroup
\footnotesize

\endgroup

For instance, let us assume we have a 1-dimensional input signal  of length  on which we run the liquid-SSM kernel. We set the hyperparameters . This value represents the maximum order of the correlation terms we would want to take into account to output a decision. This means that the signal  in (\ref{eq:conv_kernel_ltc}) will contain all combinations of 2 order correlation signals , , 3 order ,  and 4 order signals , . The kernel weights corresponding to this auto-correlation signal would be: 



Here,  is a vector of length , and the kernel . This additional kernel takes the temporal correlation of incoming input samples into consideration. This way Liquid-SSM give rise to a more general sequence modeling framework. The liquid convolutional kernel,  is as follows:

\begingroup
\small

\endgroup

\begingroup
\small
\begin{algorithm}[t]
  \caption{\textsc{Liquid-S4 Kernel} - The S4 convolution kernel (highlighted in black) is used from \citet{gu2022efficiently} and \citet{gu2022parameterization}. Liquid kernel computation is highlighted in purple.}\label{alg:liquid-s4-convolution}
  \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \Require{S4 parameters , step size , {\color{violet}liquid kernel order , inputs seq length , liquid kernel sequence length }}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \Ensure{SSM convolution kernel  {\color{violet}and SSM liquid kernel } for } (Eq. \ref{eq:conv_kernel_done})
    \State 
    \Comment{Truncate SSM generating function (SSMGF) to length }
    \State
    
    \label{step:cauchy}
    \Comment{Black-box Cauchy kernel}
    \State
    
    \Comment{Woodbury Identity} \State 
    \Comment{Evaluate SSMGF at all roots of unity }
    \State 
    \Comment{Inverse Fourier Transform}
    \If{{\color{violet}Mode == KB}} \Comment{{\color{violet}Liquid-S4 Kernel as shown in Eq. \ref{eq:kb}}}
    \For{{\color{violet}  in }}
    \State {\color{violet} \Comment{ is a backward identity matrix}}
    \State {\color{violet} }
    \EndFor
    \ElsIf{{\color{violet}Mode == PB}} \Comment{{\color{violet}Liquid-S4 Kernel of Eq. \ref{eq:kb} with  reduced to Identity.}}
    \For{{\color{violet}  in }}
    \State {\color{violet} }
    \State {\color{violet} }
    \EndFor
    \EndIf
  \end{algorithmic}
\end{algorithm}
\endgroup

\noindent \textbf{How to compute Liquid-S4 kernel efficiently?}
\citet{gu2022efficiently} showed that the S4 convolution kernel could be computed efficiently using the following elegant parameterization tricks:

\begin{itemize}
\item To obtain better representations in sequence modeling schemes by SSMs, instead of randomly initializing the transition matrix , we can use the Normal Plus Low-Rank (NPLR) matrix below, called the Hippo Matrix \citep{gu2020hippo} which is obtained by the Scaled Legendre Measure (LegS) \citep{gu2021combining,gu2022efficiently}:
\begingroup
\small

\endgroup
\item The NPLR representation of this matrix is the following \citep{gu2022efficiently}:

Here,  is a unitary matrix,  is diagonal, and  are the low-rank factorization. Eq. \ref{eq:hippo} is Normal plus low rank with r = 1 \citep{gu2022efficiently}. With the decomposition \ref{eq:nplr}, we can obtain  over complex numbers in the form of Diagonal plus low-rank (DPLR) \citep{gu2022efficiently}.
\item Vectors  and  are initialized by  and  \citep{gu2022parameterization}. Both vectors are trainable.
\item Furthermore, it was shown in \citet{gu2022parameterization} that with  Decomposition \ref{eq:nplr}, the eigenvalues of \textbf{A} might be on the right half of the complex plane, thus, result in numerical instability. To resolve this, \citet{gu2022parameterization} recently proposed to use the parametrization  instead of .
\item Computing the powers of  in direct calculation of the S4 kernel  is computationally expensive. S4 computes the spectrum of  instead of direct computations, which reduces the problem of matrix powers to matrix inverse computation \cite{gu2022efficiently}. S4 then computes this convolution kernel via a black-box Cauchy Kernel efficiently, and recovers  by an inverse Fourier Transform (iFFT) \citep{gu2022efficiently}. 
\end{itemize}

 possess similar structure to the S4 kernel. In particular, we have:

\begin{proposition}
\label{prop:liquid kernel}
The liquid-S4 kernel for each order , , can be computed by the anti-diagonal transformation (flip operation) of the product of the S4 convolution kernel, , and a vector .
\end{proposition}

The proof is given in Appendix. Proposition \ref{prop:liquid kernel} indicates that the liquid-s4 kernel can be obtained from the precomputed S4 kernel and a Hadamard product of that kernel with the transition vector  powered by the chosen liquid order. This is illustrated in Algorithm \ref{alg:liquid-s4-convolution}, lines 6 to 10, corresponding to a mode we call KB, which stands for Kernel  B.

Additionally, we introduce a simplified Liquid-S4 kernel that is easier to compute while being as expressive as or even better performing than the KB kernel. To obtain this, we set the transition matrix  in Liquid-S4 of Eq. \ref{eq:kb}, with an identity matrix, only for the input correlation terms. This way, the liquid-s4 Kernel for a given liquid order  reduces to the following expression: 



We call this kernel Liquid-S4 - PB, as it is obtained by powers of the vector . The computational steps to get this kernel is outlined in Algorithm \ref{alg:liquid-s4-convolution} lines 11 to 15.



\opexpr{(62.75 + 89.02 + 91.20 + 89.50 + 94.8 + 96.66)}{b}
\opdiv*[maxdivstep=4]{b}{6}{q}{r}
 \opfloor{q}{2}{a}

\begin{table}[t] 
    \centering
    \caption{Performance on Long Range Arena Tasks. Numbers indicate validation accuracy (standard deviation). The accuracy of models denoted by * are reported from \citep{tay2020long}. Methods denoted by ** are reported from \citep{gu2022efficiently}. The rest of the models' performance results are reported from the cited paper. Liquid-S4 is used with its PB kernel.}
    \begin{adjustbox}{width=1\columnwidth}
    \begin{tabular}{lcccccc|c}
    \toprule
        Model & ListOps & IMDB & AAN	& CIFAR	& Pathfinder & Path-X & Avg. \\
        (input length) & 2048 & 2048 & 4000 & 1024 & 1024 & 16384 & \\
        \midrule
        Random & 10.00 & 50.00 & 50.00 & 10.00 & 50.00 & 50.00 & 36.67 \\
        Transformer \citep{vaswani2017attention} & 36.37 & 64.27 & 57.46 & 42.44 & 71.40 & x & 54.39 \\
        Local Att. \citep{tay2020long} & 15.82 &	52.98 &	53.39 &	41.46 &	66.63 &	x &	46.06 \\
        Sparse Transformer \citep{child2019generating} & 17.07	& 63.58 &	59.59 &	44.24 &	71.71	& x	& 51.24 \\
        Longformer \citep{beltagy2020longformer}  & 35.63	& 62.85	& 56.89	& 42.22	& 69.71	& x & 53.46 \\
        Linformer \citep{wang2020linformer} & 16.13	& 65.90	& 53.09	& 42.34	& 75.30	& x & 	50.55 \\
Reformer \citep{kitaev2019reformer} & 37.27 & 56.10 & 53.40 & 38.07 & 68.50 & x & 50.56 \\
Sinkhorn Trans. \citep{tay2020sparse} & 33.67 & 61.20 & 53.83 & 41.23 & 67.45 & x & 51.23 \\
BigBird \citep{zaheer2020big} & 36.05	& 64.02	& 59.29	& 40.83	& 74.87	& x &	55.01 \\
Linear Trans. \citep{katharopoulos2020transformers}& 16.13 & 65.90 & 53.09 & 42.34 & 75.30 & x & 50.46 \\
Performer  \citep{choromanski2020rethinking} & 18.01 & 65.40 & 53.82 & 42.77 & 77.05 & x & 51.18 \\
\midrule
FNet \citep{lee2021fnet} & 35.33 & 65.11 & 59.61 & 38.67 & 77.80 & x & 54.42 \\
Nyströmformer \citep{xiong2021nystromformer} & 37.15 & 65.52 & 79.56 & 41.58 & 70.94 & x &  57.46 \\
Luna-256 \cite{ma2021luna} & 37.25 & 64.57 & 79.29 & 47.38 & 77.72 & x & 59.37 \\
H-Transformer-1D \citep{zhu2021h} & 49.53 & 78.69 & 63.99 & 46.05 & 68.78 & x & 61.41 \\
\midrule
CDIL \citep{cheng2022classification} & 44.05 &  86.78 &  85.36 & 66.91 & 91.70 & x & 74.96\\
\midrule
DSS \citep{gupta2022diagonal} & 57.6 & 76.6 & 87.6 & 85.8 & 84.1 & 85.0 & 79.45 \\
S4 (original) \citep{gu2022efficiently} & 58.35 & 76.02 & 87.09 & 87.26 & 86.05 & 88.10 & 80.48 \\
S4-LegS \citep{gu2022parameterization} & 59.60 (0.07)  & 86.82 (0.13) & 90.90 (0.15)  & 88.65 (0.23)  & 94.20 (0.25)  & \underline{96.35} & 86.09 \\
S4-FouT \citep{gu2022parameterization} & 57.88 (1.90) & 86.34 (0.31)  & 89.66 (0.88) &  89.07 (0.19)  & \underline{94.46} (0.26) & x & 77.90 \\
S4-LegS/FouT \citep{gu2022train} & 60.45 (0.75) & 86.78 (0.26)  & 90.30 (0.28)  & 89.00 (0.26)  & \underline{94.44} (0.08)  &  x & 78.50 \\
S4D-LegS \citep{gu2022parameterization} & 60.47 (0.34)  &  86.18 (0.43) & 89.46 (0.14)  & 88.19 (0.26)  & 93.06 (1.24)  & 91.95 & 84.89 \\
S4D-Inv \citep{gu2022parameterization} & 60.18 (0.35)  & 87.34 (0.20)  & \underline{91.09} (0.01)  & 87.83 (0.37) & 93.78 (0.25) &  92.80 & 85.50 \\
S4D-Lin \citep{gu2022parameterization} & 60.52 (0.51)  & 86.97 (0.23)  & 90.96 (0.09)  & 87.93 (0.34)  & 93.96 (0.60)  & x & 78.39 \\
S5 \citep{smith2022simplified} & 61.00 & 86.51 & 88.26 & 86.14 & 87.57 & 85.25 & 82.46 \\
\midrule
\textbf{Liquid-S4} (ours) & \textbf{62.75} (0.2) & \textbf{89.02} (0.04) & \textbf{91.20} (0.01) & \textbf{89.50} (0.4) & \textbf{94.8} (0.2) & \textbf{96.66}(0.001) & \textbf{\opprint{a}} \\
& p = 5 & p=6  & p=2 & p=3 & p=2 & p=2 & \\
         \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:lra}
\end{table}

\noindent \textbf{Computational Complexity of the Liquid-S4 Kernel.} The computational complexity of the S4-Legs Convolutional kernel solved via the Cauchy Kernel is , where N is the state-size, and L is the sequence length [\citet{gu2022efficiently}, Theorem 3]. Liquid-S4 both in KB and PB modes can be computed in . The added time complexity in practice is tractable. This is because we usually select the liquid orders, , to be less than 10 (typically , and  which is the number of terms we use to compute the input correlation vector, , is typically two orders of magnitude smaller than the seq length. 



\section{Experiments with Liquid-S4} 

In this section, we present an extensive evaluation of Liquid-S4 on sequence modeling tasks with very long-term dependencies and compare its performance to a large series of baselines ranging from advanced Transformers and Convolutional networks to many variants of State-space models. In the following, we first outline the baseline models we compare against. We then list the datasets we evaluated these models on and finally present results and discussions. 

\noindent \textbf{Baselines.} We consider a broad range of advanced models to compare liquid-S4 with. These baselines include transformer variants such as vanilla Transformer \citep{vaswani2017attention}, Sparse Transformers \citep{child2019generating}, a Transformer model with local attention \citep{tay2020long}, Longformer \citep{beltagy2020longformer}, Linformer \citep{wang2020linformer}, Reformer \citep{kitaev2019reformer}, Sinkhorn Transformer \citep{tay2020sparse}, BigBird \citep{zaheer2020big}, Linear Transformer \citep{katharopoulos2020transformers}, and Performer \citep{choromanski2020rethinking}. We also include architectures such as FNets \citep{lee2021fnet}, Nystro\"mformer \citep{xiong2021nystromformer}, Luna-256 \citep{ma2021luna}, H-Transformer-1D \citep{zhu2021h}, and Circular Diluted Convolutional neural networks (CDIL) \citep{cheng2022classification}. We then include a full series of state-space models and their variants such as diagonal SSMs (DSS) \citep{gupta2022diagonal}, S4 \citep{gu2022efficiently}, S4-legS, S4-FouT, S4-LegS/FouT \citep{gu2022train}, S4D-LegS \citep{gu2022parameterization}, S4D-Inv, S4D-Lin and the Simplified Structural State-space models (S5) \citep{smith2022simplified}.


\begin{table}[t]
    \centering
    \caption{Performance on BIDMC Vital Signs dataset. Numbers indicate RMSE on the test set. The accuracy of models denoted by * is reported from \citep{gu2022parameterization}. The rest of the models' performance results are reported from the cited paper.}
    \begin{adjustbox}{width=0.65\columnwidth}
    \begin{tabular}{lccc}
    \toprule
     & &  \textbf{BIDMC} & \\
     \toprule
        Model & HR & RR & SPO2 \\
        \midrule
UnICORNN \citep{rusch2021coupled} & 1.39 & 1.06 & 0.869 \\
coRNN \citep{rusch2021coupled} & 1.81 & 1.45 & - \\
CKConv & 2.05 & 1.214 & 1.051 \\
NRDE \citep{morrill2021neural} & 2.97 & 1.49 & 1.29 \\
LSTM & 10.7 & 2.28 & - \\
Transformer & 12.2 & 2.61 & 3.02 \\
XGBoost \citep{tan2021time} & 4.72 & 1.67 & 1.52 \\
Random Forest \citep{tan2021time} & 5.69 & 1.85 & 1.74 \\
Ridge Regress. \citep{tan2021time} &  17.3 & 3.86 & 4.16 \\
\midrule
        S4-LegS \citep{gu2022parameterization} & 0.332 (0.013) & 0.247 (0.062) & 0.090 (0.006) \\ 
        S4-FouT \citep{gu2022parameterization} & 0.339 (0.020) & 0.301 (0.030) & \underline {0.068} (0.003) \\ 
        S4D-LegS \citep{gu2022parameterization} & 0.367 (0.001) & 0.248 (0.036) & 0.102 (0.001) \\ 
        S4-(LegS/FouT) \citep{gu2022parameterization} & 0.344 (0.032) & \underline{0.163} (0.008) & 0.080 (0.007) \\
        S4D-Inv \citep{gu2022parameterization} & 0.373 (0.024) & 0.254 (0.022) & 0.110 (0.001) \\ 
        S4D-Lin \citep{gu2022parameterization} & 0.379 (0.006) & 0.226 (0.008) & 0.114 (0.003) \\ 
        \midrule
        \textbf{Liquid-S4} (ours) & \textbf{0.303} (0.002) & \textbf{0.158} (0.001) & \textbf{0.066} (0.002)\\
        & p=3 & p=2 & p=4 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:bidmc}
\end{table}

\noindent\textbf{Datasets.} We first evaluate Liquid-S4's performance on the well-studied \textbf{Long Range Arena (LRA)} benchmark \citep{tay2020long}, where Liquid-S4 outperforms other S4 and S4D variants in every task pushing the state-of-the-art further with an average accuracy of \textbf{87.32\%}. LRA dataset includes six tasks with sequence lengths ranging from 1k to 16k. 

We then report Liquid-S4's performance compared to other S4, and S4D variants as well as other models, on the \textbf{BIDMC Vital Signals} dataset \citep{pimentel2016toward,goldberger2000physiobank}. BIDMC uses bio-marker signals of length 4000 to predict Heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2). 

We also experiment with the \textbf{sCIFAR dataset} that consists of the classification of flattened images in the form of 1024-long sequences into 10 classes.

Finally, we perform \textbf{RAW Speech Command (SC) recognition with FULL 35 LABELS} as conducted very recently in the updated S4 article \citep{gu2022efficiently}.\footnote{It is essential to denote that there is a modified speech command dataset that restricted the dataset to only 10 output classes and is used in a couple of works (see for example \protect\citep{kidger2020neural,gu2021combining,romero2021ckconv,romero2021flexconv}). Aligned with the updated results reported in \citep{gu2022efficiently} and \citep{gu2022parameterization}, we choose not to break down this dataset and use the full-sized benchmark.} SC dataset contains sequences of length 16k to be classified into 35 commands. \citet{gu2022efficiently} introduced a new test case setting to assess the performance of models (trained on 16kHz sequences) on sequences of length 8kHz. S4 and S4D perform exceptionally well in this zero-shot test scenario.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\textwidth]{images/liquid_s4.pdf}
    \caption{Performance vs Liquid Order in Liquid-S4 for A) ListOps, and B) IMDB datasets. (n=3)}
    \label{fig:liquid_order}
\end{figure}

\subsection{Results on Long Range Arena}

Table \ref{tab:lra} depicts a comprehensive list of baselines benchmarked against each other on six long-range sequence modeling tasks in LRA. We observe that Liquid-S4 instances (all use the PB kernel with a scaled Legendre (LegS) configuration) with a small liquid order, , ranging from 2 to 6, consistently outperform all baselines in all six tasks, establishing the new SOTA on LRA with an average performance of \textbf{87.32\%}. In particular, on ListOps, Liquid-S4 improves S4-LegS performance by more than 3\%, on character-level IMDB by 2.2\%, and on 1-D pixel-level classification (CIFAR) by 0.65\%, while establishing the-state-of the-art on the hardest LRA task by gaining \textbf{96.54\%} accuracy. Liquid-S4 performs on par with improved S4 and S4D instances on both AAN and Pathfinder tasks. 

\begin{table}[t]
    \centering
    \caption{Performance on sCIFAR dataset. Numbers indicate Accuracy (standard deviation). The accuracy of baseline models is reported from Table 9 of \citet{gu2022parameterization}.}
    \begin{adjustbox}{width=0.45\textwidth}
    \begin{tabular}{lccc}
\toprule
Model & Accuracy \\
\midrule
Transformer \citep{trinh2018learning} & 62.2 \\
FlexConv \citep{romero2021flexconv} & 80.82 \\
TrellisNet \citep{bai2018trellis} & 73.42 \\
LSTM \citep{hochreiter1997long} & 63.01 \\
r-LSTM \citep{trinh2018learning} & 72.2 \\
UR-GRU \citep{gu2020improving} & 74.4 \\
HiPPO-RNN \citep{gu2020hippo} & 61.1 \\
LipschitzRNN \citep{erichson2021lipschitz} & 64.2 \\
\midrule
S4-LegS \citep{gu2022parameterization} & \underline{91.80} (0.43) \\
S4-FouT \citep{gu2022parameterization} & 91.22 (0.25) \\
S4-(LegS/FouT) \citep{gu2022parameterization} & 91.58 (0.17) \\
\midrule
S4D-LegS \citep{gu2022parameterization} & 89.92 (1.69) \\
S4D-Inv \citep{gu2022parameterization} & 90.69 (0.06) \\
S4D-Lin \citep{gu2022parameterization} & 90.42 (0.03) \\
S5 \cite{smith2022simplified} &  89.66 \\
\midrule
Liquid-S4 (ours) & \textbf{92.02} (0.14) \\
& p=3\\
\bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:scifar}
\end{table}

The performance of SSM models is generally well-beyond what advanced Transformers, RNNs, and Convolutional networks achieve on LRA tasks, with the Liquid-S4 variants standing on top. It is worth noting that Liquid-S4 kernels perform better with smaller kernel sizes (See more details on this in Appendix); For instance, on ListOps and IMDB, their individual liquid-S4 kernel state-size could be as small as seven units. This significantly reduces the parameter count in Liquid-S4 in comparison to other variants.

\noindent\textbf{The impact of increasing Liquid Order .} Figure \ref{fig:liquid_order} illustrates how increasing the liquid order, , can consistently improve performance on ListOps and IMDB tasks from LRA. 

\subsection{Results on BIDMC Vital Signs}

\begin{table}[t] 
    \centering
    \caption{Performance on RAW Speech Command dataset with \textbf{FULL 35 Labels} and with the reduced ten classes.Numbers indicate validation accuracy. The accuracy of baseline models is reported from Table 11 of \citep{gu2022parameterization}.}
\begin{adjustbox}{width=0.65\columnwidth}
\begin{tabular}{lccc}
\toprule
& &  \multicolumn{2}{c}{\textbf{SC FULL Labels}} \\
\midrule
Model & Parameters & 16kHz & 8kHz \\
\midrule
InceptionNet \citep{nonaka2021depth} & 481K & 61.24 (0.69) & 05.18 (0.07) \\
ResNet-18 & 216K & 77.86 (0.24) & 08.74 (0.57) \\
XResNet-50 & 904K & 83.01 (0.48) & 07.72 (0.39) \\
ConvNet & 26.2M & 95.51 (0.18) & 07.26 (0.79) \\
\midrule
S4-LegS \citep{gu2022parameterization} & 307K & 96.08 (0.15) & 91.32 (0.17) \\
S4-FouT \citep{gu2022parameterization} & 307K & 95.27 (0.20) & 91.59 (0.23) \\
S4-(LegS/FouT) \citep{gu2022parameterization} & 307K & 95.32 (0.10) & 90.72 (0.68) \\
\midrule
S4D-LegS \citep{gu2022parameterization} & 306K & 95.83 (0.14) & 91.08 (0.16) \\
S4D-Inv \citep{gu2022parameterization} & 306K & \underline{96.18} (0.27) & \textbf{91.80} (0.24) \\
S4D-Lin \citep{gu2022parameterization} & 306K & \underline{96.25} (0.03) & \underline{91.58} (0.33) \\
\midrule
Liquid-S4 (ours) & \textbf{224K} & \textbf{96.78} (0.05) & 90.00 (0.25)  \\
& & p=2 & p=2 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:sc}
\end{table}
Table \ref{tab:bidmc} demonstrates the performance of a variety of classical and advanced baseline models on the BIDMC dataset for all three heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2) level prediction tasks. We observe that Liquid-s4 with a PB kernel of order , , and , perform better than all S4 and S4D variants. It is worth denoting that Liquid-S4 is built by the same parametrization as S4-LegS (which is the official S4 model reported in the updated S4 report \citep{gu2022efficiently}). In RR, Liquid-S4 outperforms S4-LegS by a significant margin of 36\%. On SpO2, Liquid-S4 performs 26.67\% better than S4-Legs. On HR, Liquid-S4 outperforms S4-Legs by 8.7\% improvement in performance. The hyperparameters are given in Appendix.


\subsection{Results on 1-D Pixel-level Image Classification}
Similar to the previous tasks, a Liquid-S4 network with PB kernel of order  outperforms all variants of S4 and S4D while being significantly better than Transformer and RNN baselines as summarized in Table \ref{tab:scifar}. The hyperparameters are given in Appendix.


\subsection{Results on Speech Commands}
Table \ref{tab:sc} demonstrates that Liquid-S4 with liquid order  achieves the best performance amongst all benchmarks on the 16KHz testbed with full dataset. Liquid-S4 also performs competitively on the half-frequency zero-shot experiment, while it does not realize the best performance. Although the task is solved to a great degree, the reason could be that liquid kernel accounts for covariance terms. This might influence the learned representations in a way that hurts performance by a small margin in this zero-shot experiment. The hyperparameters are given in Appendix.

It is essential to denote that there is a modified speech command dataset that restricts the dataset to only ten output classes, namely SC10, and is used in a couple of works (see for example \protect\citep{kidger2020neural,gu2021combining,romero2021ckconv,romero2021flexconv}). Aligned with the updated results reported in \citep{gu2022efficiently} and \citep{gu2022parameterization}, we choose not to break down this dataset and report the full-sized benchmark in the main paper. Nevertheless, we conducted an experiment with SC10 and showed that even on the reduced dataset, with the same hyperparameters, we solved the task with a SOTA accuracy of 98.51\%. The results are presented in Table \ref{tab:sc_1}.

\section{Conclusions}
We showed that structural state-space models could be considerably improved in performance if they are formulated by a linear liquid time-constant kernel, namely Liquid-S4. Liquid-S4 kernels are obtainable with minimal effort with their kernel computing the similarities between time-lags of the input signals in addition to the main S4 diagonal plus low-rank parametrization. Liquid-S4 kernels with Smaller parameter counts achieve SOTA performance on all six tasks of the Long-range arena dataset, on BIDMC heart rate, respiratory rate, and blood oxygen saturation, on sequential 1-D pixel-level image classification, and on Speech command recognition. 

\section*{Acknowledgments}
This work is supported by The Boeing Company and the Office of Naval Research (ONR) Grant N00014-18-1-2830.




\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Z.~Allen-Zhu and Y.~Li.
\newblock Can sgd learn recurrent neural networks with provable generalization?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10331--10341, 2019.

\bibitem[Amig{\'o} et~al.(2012)Amig{\'o}, Monetti, Aschenbrenner, and
  Bunk]{amigo2012transcripts}
J.~M. Amig{\'o}, R.~Monetti, T.~Aschenbrenner, and W.~Bunk.
\newblock Transcripts: An algebraic approach to coupled time series.
\newblock \emph{Chaos: An Interdisciplinary Journal of Nonlinear Science},
  22\penalty0 (1):\penalty0 013105, 2012.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018trellis}
S.~Bai, J.~Z. Kolter, and V.~Koltun.
\newblock Trellis networks for sequence modeling.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Belletti et~al.(2016)Belletti, Sparks, Franklin, Bayen, and
  Gonzalez]{belletti2016scalable}
F.~W. Belletti, E.~R. Sparks, M.~J. Franklin, A.~M. Bayen, and J.~E. Gonzalez.
\newblock Scalable linear causal inference for irregularly sampled time series
  with long range dependencies.
\newblock \emph{arXiv preprint arXiv:1603.03336}, 2016.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Brush(1967)]{brush1967history}
S.~G. Brush.
\newblock History of the lenz-ising model.
\newblock \emph{Reviews of modern physics}, 39\penalty0 (4):\penalty0 883,
  1967.

\bibitem[Chang et~al.(2017)Chang, Zhang, Han, Yu, Guo, Tan, Cui, Witbrock,
  Hasegawa-Johnson, and Huang]{chang2017dilated}
S.~Chang, Y.~Zhang, W.~Han, M.~Yu, X.~Guo, W.~Tan, X.~Cui, M.~Witbrock, M.~A.
  Hasegawa-Johnson, and T.~S. Huang.
\newblock Dilated recurrent neural networks.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Charlier et~al.(2021)Charlier, Feydy, Glaunès, Collin, and
  Durif]{JMLR:v22:20-275}
B.~Charlier, J.~Feydy, J.~A. Glaunès, F.-D. Collin, and G.~Durif.
\newblock Kernel operations on the gpu, with autodiff, without memory
  overflows.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (74):\penalty0 1--6, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-275.html}.

\bibitem[Che et~al.(2018)Che, Purushotham, Cho, Sontag, and
  Liu]{che2018recurrent}
Z.~Che, S.~Purushotham, K.~Cho, D.~Sontag, and Y.~Liu.
\newblock Recurrent neural networks for multivariate time series with missing
  values.
\newblock \emph{Scientific reports}, 8\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Chen et~al.(2019)Chen, Jacob, and Mairal]{chen2019recurrent}
D.~Chen, L.~Jacob, and J.~Mairal.
\newblock Recurrent kernel networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13431--13442, 2019.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
T.~Q. Chen, Y.~Rubanova, J.~Bettencourt, and D.~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in neural information processing systems}, pages
  6571--6583, 2018.

\bibitem[Cheng et~al.(2022)Cheng, Khalitov, Yu, and
  Yang]{cheng2022classification}
L.~Cheng, R.~Khalitov, T.~Yu, and Z.~Yang.
\newblock Classification of long sequential data using circular dilated
  convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:2201.02143}, 2022.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos,
  P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and
  Bengio]{chung2014empirical}
J.~Chung, C.~Gulcehre, K.~Cho, and Y.~Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[De~Brouwer et~al.(2019)De~Brouwer, Simm, Arany, and Moreau]{de2019gru}
E.~De~Brouwer, J.~Simm, A.~Arany, and Y.~Moreau.
\newblock Gru-ode-bayes: Continuous modeling of sporadically-observed time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7377--7388, 2019.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
E.~Dupont, A.~Doucet, and Y.~W. Teh.
\newblock Augmented neural odes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3134--3144, 2019.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and
  Papamakarios]{durkan2019neural}
C.~Durkan, A.~Bekasov, I.~Murray, and G.~Papamakarios.
\newblock Neural spline flows.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7509--7520, 2019.

\bibitem[Erichson et~al.(2021)Erichson, Azencot, Queiruga, Hodgkinson, and
  Mahoney]{erichson2021lipschitz}
N.~B. Erichson, O.~Azencot, A.~Queiruga, L.~Hodgkinson, and M.~W. Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=-N7PBXqOUJZ}.

\bibitem[Foster(1996)]{foster1996wavelets}
G.~Foster.
\newblock Wavelets for period analysis of unevenly sampled time series.
\newblock \emph{The Astronomical Journal}, 112:\penalty0 1709, 1996.

\bibitem[Friston et~al.(2003)Friston, Harrison, and Penny]{friston2003dynamic}
K.~J. Friston, L.~Harrison, and W.~Penny.
\newblock Dynamic causal modelling.
\newblock \emph{Neuroimage}, 19\penalty0 (4):\penalty0 1273--1302, 2003.

\bibitem[Friz and Victoir(2010)]{friz2010multidimensional}
P.~K. Friz and N.~B. Victoir.
\newblock \emph{Multidimensional stochastic processes as rough paths: theory
  and applications}, volume 120.
\newblock Cambridge University Press, 2010.

\bibitem[Funahashi and Nakamura(1993)]{funahashi1993approximation}
K.-i. Funahashi and Y.~Nakamura.
\newblock Approximation of dynamical systems by continuous time recurrent
  neural networks.
\newblock \emph{Neural networks}, 6\penalty0 (6):\penalty0 801--806, 1993.

\bibitem[Goldberger et~al.(2000)Goldberger, Amaral, Glass, Hausdorff, Ivanov,
  Mark, Mietus, Moody, Peng, and Stanley]{goldberger2000physiobank}
A.~L. Goldberger, L.~A. Amaral, L.~Glass, J.~M. Hausdorff, P.~C. Ivanov, R.~G.
  Mark, J.~E. Mietus, G.~B. Moody, C.-K. Peng, and H.~E. Stanley.
\newblock Physiobank, physiotoolkit, and physionet: components of a new
  research resource for complex physiologic signals.
\newblock \emph{circulation}, 101\penalty0 (23):\penalty0 e215--e220, 2000.

\bibitem[Greaves-Tunnell and Harchaoui(2019)]{greaves2019statistical}
A.~Greaves-Tunnell and Z.~Harchaoui.
\newblock A statistical investigation of long memory in language and music.
\newblock In \emph{International Conference on Machine Learning}, pages
  2394--2403, 2019.

\bibitem[Greff et~al.(2016)Greff, Srivastava, Koutn{\'\i}k, Steunebrink, and
  Schmidhuber]{greff2016lstm}
K.~Greff, R.~K. Srivastava, J.~Koutn{\'\i}k, B.~R. Steunebrink, and
  J.~Schmidhuber.
\newblock Lstm: A search space odyssey.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  28\penalty0 (10):\penalty0 2222--2232, 2016.

\bibitem[Gruenbacher et~al.(2022)Gruenbacher, Lechner, Hasani, Rus, Henzinger,
  Smolka, and Grosu]{gruenbacher2022gotube}
S.~A. Gruenbacher, M.~Lechner, R.~Hasani, D.~Rus, T.~A. Henzinger, S.~A.
  Smolka, and R.~Grosu.
\newblock Gotube: Scalable statistical verification of continuous-depth models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume 36, No 6, pages 6755--6764, 2022.

\bibitem[Grunbacher et~al.(2021)Grunbacher, Hasani, Lechner, Cyranka, Smolka,
  and Grosu]{grunbacher2021verification}
S.~Grunbacher, R.~Hasani, M.~Lechner, J.~Cyranka, S.~A. Smolka, and R.~Grosu.
\newblock On the verification of neural odes with stochastic guarantees.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume 35, No 13, pages 11525--11535, 2021.

\bibitem[Gu et~al.(2020{\natexlab{a}})Gu, Dao, Ermon, Rudra, and
  Re]{gu2020hippo}
A.~Gu, T.~Dao, S.~Ermon, A.~Rudra, and C.~Re.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in neural information processing systems}, 33,
  2020{\natexlab{a}}.

\bibitem[Gu et~al.(2020{\natexlab{b}})Gu, Gulcehre, Paine, Hoffman, and
  Pascanu]{gu2020improving}
A.~Gu, C.~Gulcehre, T.~Paine, M.~Hoffman, and R.~Pascanu.
\newblock Improving the gating mechanism of recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  3800--3809. PMLR, 2020{\natexlab{b}}.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and
  R{\'e}]{gu2021combining}
A.~Gu, I.~Johnson, K.~Goel, K.~Saab, T.~Dao, A.~Rudra, and C.~R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  linear state space layers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Goel, and Re]{gu2022efficiently}
A.~Gu, K.~Goel, and C.~Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=uYLFoz1vlAC}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Gupta, Goel, and
  R{\'e}]{gu2022parameterization}
A.~Gu, A.~Gupta, K.~Goel, and C.~R{\'e}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock \emph{arXiv preprint arXiv:2206.11893}, 2022{\natexlab{b}}.

\bibitem[Gu et~al.(2022{\natexlab{c}})Gu, Johnson, Timalsina, Rudra, and
  R{\'e}]{gu2022train}
A.~Gu, I.~Johnson, A.~Timalsina, A.~Rudra, and C.~R{\'e}.
\newblock How to train your hippo: State space models with generalized
  orthogonal basis projections.
\newblock \emph{arXiv preprint arXiv:2206.12037}, 2022{\natexlab{c}}.

\bibitem[Gupta(2022)]{gupta2022diagonal}
A.~Gupta.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock \emph{arXiv preprint arXiv:2203.14343}, 2022.

\bibitem[Hanshu et~al.(2020)Hanshu, Jiawei, Vincent, and
  Jiashi]{hanshu2020robustness}
Y.~Hanshu, D.~Jiawei, T.~Vincent, and F.~Jiashi.
\newblock On robustness of neural ordinary differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hasani et~al.(2019)Hasani, Amini, Lechner, Naser, Grosu, and
  Rus]{hasani2019response}
R.~Hasani, A.~Amini, M.~Lechner, F.~Naser, R.~Grosu, and D.~Rus.
\newblock Response characterization for auditing cell dynamics in long
  short-term memory networks.
\newblock In \emph{2019 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2019.

\bibitem[Hasani et~al.(2020)Hasani, Lechner, Amini, Rus, and
  Grosu]{hasani2020natural}
R.~Hasani, M.~Lechner, A.~Amini, D.~Rus, and R.~Grosu.
\newblock The natural lottery ticket winner: Reinforcement learning with
  ordinary neural circuits.
\newblock In \emph{Proceedings of the 2020 International Conference on Machine
  Learning}. JMLR. org, 2020.

\bibitem[Hasani et~al.(2021{\natexlab{a}})Hasani, Lechner, Amini, Liebenwein,
  Tschaikowski, Teschl, and Rus]{hasani2021closed}
R.~Hasani, M.~Lechner, A.~Amini, L.~Liebenwein, M.~Tschaikowski, G.~Teschl, and
  D.~Rus.
\newblock Closed-form continuous-depth models.
\newblock \emph{arXiv preprint arXiv:2106.13898}, 2021{\natexlab{a}}.

\bibitem[Hasani et~al.(2021{\natexlab{b}})Hasani, Lechner, Amini, Rus, and
  Grosu]{hasani2021liquid}
R.~Hasani, M.~Lechner, A.~Amini, D.~Rus, and R.~Grosu.
\newblock Liquid time-constant networks.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (9):\penalty0 7657--7666, May 2021{\natexlab{b}}.

\bibitem[Hochreiter(1991)]{hochreiter1991untersuchungen}
S.~Hochreiter.
\newblock Untersuchungen zu dynamischen neuronalen netzen [in german] diploma
  thesis.
\newblock \emph{TU M{\"u}nich}, 1991.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Holl et~al.(2020)Holl, Koltun, and Thuerey]{holl2020learning}
P.~Holl, V.~Koltun, and N.~Thuerey.
\newblock Learning to control pdes with differentiable physics.
\newblock \emph{arXiv preprint arXiv:2001.07457}, 2020.

\bibitem[Hopfield(1982)]{hopfield1982neural}
J.~J. Hopfield.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock \emph{Proceedings of the national academy of sciences}, 79\penalty0
  (8):\penalty0 2554--2558, 1982.

\bibitem[Jia and Benson(2019)]{jia2019neural}
J.~Jia and A.~R. Benson.
\newblock Neural jump stochastic differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9843--9854, 2019.

\bibitem[Jing et~al.(2019)Jing, Gulcehre, Peurifoy, Shen, Tegmark, Soljacic,
  and Bengio]{jing2019gated}
L.~Jing, C.~Gulcehre, J.~Peurifoy, Y.~Shen, M.~Tegmark, M.~Soljacic, and
  Y.~Bengio.
\newblock Gated orthogonal recurrent units: On learning to forget.
\newblock \emph{Neural computation}, 31\penalty0 (4):\penalty0 765--783, 2019.

\bibitem[Kag et~al.(2019)Kag, Zhang, and Saligrama]{kag2019rnns}
A.~Kag, Z.~Zhang, and V.~Saligrama.
\newblock Rnns incrementally evolving on an equilibrium manifold: A panacea for
  vanishing and exploding gradients?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[KALMAN(1960)]{kalman1960new}
R.~KALMAN.
\newblock A new approach to linear filtering and prediction problems.
\newblock \emph{J. Basic Eng., Trans. ASME, D}, 82:\penalty0 35--45, 1960.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
P.~Kidger, J.~Morrill, J.~Foster, and T.~Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6696--6707, 2020.

\bibitem[Kitaev et~al.(2019)Kitaev, Kaiser, and Levskaya]{kitaev2019reformer}
N.~Kitaev, L.~Kaiser, and A.~Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kowal et~al.(2019)Kowal, Matteson, and Ruppert]{kowal2019functional}
D.~R. Kowal, D.~S. Matteson, and D.~Ruppert.
\newblock Functional autoregression for sparsely sampled data.
\newblock \emph{Journal of Business \& Economic Statistics}, 37\penalty0
  (1):\penalty0 97--109, 2019.

\bibitem[Lea et~al.(2016)Lea, Vidal, Reiter, and Hager]{lea2016temporal}
C.~Lea, R.~Vidal, A.~Reiter, and G.~D. Hager.
\newblock Temporal convolutional networks: A unified approach to action
  segmentation.
\newblock In \emph{European Conference on Computer Vision}, pages 47--54.
  Springer, 2016.

\bibitem[Lechner and Hasani(2021)]{lechner2021mixed}
M.~Lechner and R.~Hasani.
\newblock Mixed-memory rnns for learning long-term dependencies in irregularly
  sampled time series.
\newblock \emph{OpenReview}, 2021.

\bibitem[Lechner et~al.(2019)Lechner, Hasani, Zimmer, Henzinger, and
  Grosu]{lechner2019designing}
M.~Lechner, R.~Hasani, M.~Zimmer, T.~A. Henzinger, and R.~Grosu.
\newblock Designing worm-inspired neural networks for interpretable robotic
  control.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pages 87--94. IEEE, 2019.

\bibitem[Lechner et~al.(2020{\natexlab{a}})Lechner, Hasani, Amini, Henzinger,
  Rus, and Grosu]{lechner2020neural}
M.~Lechner, R.~Hasani, A.~Amini, T.~A. Henzinger, D.~Rus, and R.~Grosu.
\newblock Neural circuit policies enabling auditable autonomy.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (10):\penalty0
  642--652, 2020{\natexlab{a}}.

\bibitem[Lechner et~al.(2020{\natexlab{b}})Lechner, Hasani, Rus, and
  Grosu]{lechner2020gershgorin}
M.~Lechner, R.~Hasani, D.~Rus, and R.~Grosu.
\newblock Gershgorin loss stabilizes the recurrent neural network compartment
  of an end-to-end robot learning scheme.
\newblock In \emph{2020 International Conference on Robotics and Automation
  (ICRA)}. IEEE, 2020{\natexlab{b}}.

\bibitem[Lee-Thorp et~al.(2021)Lee-Thorp, Ainslie, Eckstein, and
  Ontanon]{lee2021fnet}
J.~Lee-Thorp, J.~Ainslie, I.~Eckstein, and S.~Ontanon.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock \emph{arXiv preprint arXiv:2105.03824}, 2021.

\bibitem[Li et~al.(2018)Li, Li, Cook, Zhu, and Gao]{li2018independently}
S.~Li, W.~Li, C.~Cook, C.~Zhu, and Y.~Gao.
\newblock Independently recurrent neural network (indrnn): Building a longer
  and deeper rnn.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5457--5466, 2018.

\bibitem[Li and Marlin(2016)]{li2016scalable}
S.~C.-X. Li and B.~M. Marlin.
\newblock A scalable end-to-end gaussian process adapter for irregularly
  sampled time series classification.
\newblock In \emph{Advances in neural information processing systems}, pages
  1804--1812, 2016.

\bibitem[Liebenwein et~al.(2021)Liebenwein, Hasani, Amini, and
  Rus]{liebenwein2021sparse}
L.~Liebenwein, R.~Hasani, A.~Amini, and D.~Rus.
\newblock Sparse flows: Pruning continuous-depth models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22628--22642, 2021.

\bibitem[Little(1974)]{little1974existence}
W.~A. Little.
\newblock The existence of persistent states in the brain.
\newblock \emph{Mathematical biosciences}, 19\penalty0 (1-2):\penalty0
  101--120, 1974.

\bibitem[Ma et~al.(2021)Ma, Kong, Wang, Zhou, May, Ma, and
  Zettlemoyer]{ma2021luna}
X.~Ma, X.~Kong, S.~Wang, C.~Zhou, J.~May, H.~Ma, and L.~Zettlemoyer.
\newblock Luna: Linear unified nested attention.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 2441--2453, 2021.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Park, Yamashita, and
  Asama]{massaroli2020dissecting}
S.~Massaroli, M.~Poli, J.~Park, A.~Yamashita, and H.~Asama.
\newblock Dissecting neural odes.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3952--3963, 2020.

\bibitem[Mei and Eisner(2017)]{mei2017neural}
H.~Mei and J.~M. Eisner.
\newblock The neural hawkes process: A neurally self-modulating multivariate
  point process.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6754--6764, 2017.

\bibitem[Morrill et~al.(2020)Morrill, Kidger, Salvi, Foster, and
  Lyons]{morrill2020neural}
J.~Morrill, P.~Kidger, C.~Salvi, J.~Foster, and T.~Lyons.
\newblock Neural cdes for long time series via the log-ode method.
\newblock \emph{arXiv preprint arXiv:2009.08295}, 2020.

\bibitem[Morrill et~al.(2021)Morrill, Salvi, Kidger, and
  Foster]{morrill2021neural}
J.~Morrill, C.~Salvi, P.~Kidger, and J.~Foster.
\newblock Neural rough differential equations for long time series.
\newblock In \emph{International Conference on Machine Learning}, pages
  7829--7838. PMLR, 2021.

\bibitem[Neil et~al.(2016)Neil, Pfeiffer, and Liu]{neil2016phased}
D.~Neil, M.~Pfeiffer, and S.-C. Liu.
\newblock Phased lstm: Accelerating recurrent network training for long or
  event-based sequences.
\newblock In \emph{Advances in neural information processing systems}, pages
  3882--3890, 2016.

\bibitem[Nonaka and Seita(2021)]{nonaka2021depth}
N.~Nonaka and J.~Seita.
\newblock In-depth benchmarking of deep neural network architectures for ecg
  diagnosis.
\newblock In \emph{Machine Learning for Healthcare Conference}, pages 414--439.
  PMLR, 2021.

\bibitem[Pearson et~al.(2003)Pearson, Goney, and
  Shwaber]{pearson2003imbalanced}
R.~Pearson, G.~Goney, and J.~Shwaber.
\newblock Imbalanced clustering for microarray time-series.
\newblock In \emph{Proceedings of the ICML}, volume~3, 2003.

\bibitem[Penny et~al.(2005)Penny, Ghahramani, and Friston]{penny2005bilinear}
W.~Penny, Z.~Ghahramani, and K.~Friston.
\newblock Bilinear dynamical systems.
\newblock \emph{Philosophical Transactions of the Royal Society B: Biological
  Sciences}, 360\penalty0 (1457):\penalty0 983--993, 2005.

\bibitem[Pimentel et~al.(2016)Pimentel, Johnson, Charlton, Birrenkott,
  Watkinson, Tarassenko, and Clifton]{pimentel2016toward}
M.~A. Pimentel, A.~E. Johnson, P.~H. Charlton, D.~Birrenkott, P.~J. Watkinson,
  L.~Tarassenko, and D.~A. Clifton.
\newblock Toward a robust estimation of respiratory rate from pulse oximeters.
\newblock \emph{IEEE Transactions on Biomedical Engineering}, 64\penalty0
  (8):\penalty0 1914--1923, 2016.

\bibitem[Quaglino et~al.(2020)Quaglino, Gallieri, Masci, and
  KoutnÃ­k]{quaglino2020snode}
A.~Quaglino, M.~Gallieri, J.~Masci, and J.~KoutnÃ­k.
\newblock Snode: Spectral discretization of neural odes for system
  identification.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Gruber, Holzleitner, Adler, Kreil, Kopp, et~al.]{ramsauer2020hopfield}
H.~Ramsauer, B.~Sch{\"a}fl, J.~Lehner, P.~Seidl, M.~Widrich, L.~Gruber,
  M.~Holzleitner, T.~Adler, D.~Kreil, M.~K. Kopp, et~al.
\newblock Hopfield networks is all you need.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Romero et~al.(2021{\natexlab{a}})Romero, Bruintjes, Tomczak, Bekkers,
  Hoogendoorn, and van Gemert]{romero2021flexconv}
D.~W. Romero, R.-J. Bruintjes, J.~M. Tomczak, E.~J. Bekkers, M.~Hoogendoorn,
  and J.~van Gemert.
\newblock Flexconv: Continuous kernel convolutions with differentiable kernel
  sizes.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Romero et~al.(2021{\natexlab{b}})Romero, Kuzina, Bekkers, Tomczak, and
  Hoogendoorn]{romero2021ckconv}
D.~W. Romero, A.~Kuzina, E.~J. Bekkers, J.~M. Tomczak, and M.~Hoogendoorn.
\newblock Ckconv: Continuous kernel convolution for sequential data.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Roy and Yan(2020)]{roy2020robust}
D.~Roy and L.~Yan.
\newblock Robust landsat-based crop time series modelling.
\newblock \emph{Remote Sensing of Environment}, 238:\penalty0 110810, 2020.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Y.~Rubanova, T.~Q. Chen, and D.~K. Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5321--5331, 2019.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Rusch and Mishra(2021)]{rusch2021coupled}
T.~K. Rusch and S.~Mishra.
\newblock Coupled oscillatory recurrent neural network (co{\{}rnn{\}}): An
  accurate and (gradient) stable architecture for learning long time
  dependencies.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=F3s69XzWOia}.

\bibitem[Sherstinsky(2020)]{sherstinsky2020fundamentals}
A.~Sherstinsky.
\newblock Fundamentals of recurrent neural network (rnn) and long short-term
  memory (lstm) network.
\newblock \emph{Physica D: Nonlinear Phenomena}, 404:\penalty0 132306, 2020.

\bibitem[Smith et~al.(2022)Smith, Warrington, and
  Linderman]{smith2022simplified}
J.~T. Smith, A.~Warrington, and S.~W. Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock \emph{arXiv preprint arXiv:2208.04933}, 2022.

\bibitem[Tan et~al.(2021)Tan, Bergmeir, Petitjean, and Webb]{tan2021time}
C.~W. Tan, C.~Bergmeir, F.~Petitjean, and G.~I. Webb.
\newblock Time series extrinsic regression.
\newblock \emph{Data Mining and Knowledge Discovery}, 35\penalty0 (3):\penalty0
  1032--1060, 2021.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Yang, Metzler, and
  Juan]{tay2020sparse}
Y.~Tay, D.~Bahri, L.~Yang, D.~Metzler, and D.-C. Juan.
\newblock Sparse sinkhorn attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  9438--9447. PMLR, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{tay2020long}
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang,
  S.~Ruder, and D.~Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
T.~Trinh, A.~Dai, T.~Luong, and Q.~Le.
\newblock Learning longer-term dependencies in rnns with auxiliary losses.
\newblock In \emph{International Conference on Machine Learning}, pages
  4965--4974. PMLR, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voelker et~al.(2019)Voelker, Kaji{\'c}, and
  Eliasmith]{voelker2019legendre}
A.~Voelker, I.~Kaji{\'c}, and C.~Eliasmith.
\newblock Legendre memory units: Continuous-time representation in recurrent
  neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Vorbach et~al.(2021)Vorbach, Hasani, Amini, Lechner, and
  Rus]{vorbach2021causal}
C.~Vorbach, R.~Hasani, A.~Amini, M.~Lechner, and D.~Rus.
\newblock Causal navigation by continuous-time neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wang and Niepert(2019)]{wang2019state}
C.~Wang and M.~Niepert.
\newblock State-regularized recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  6596--6606, 2019.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wisdom et~al.(2016)Wisdom, Powers, Hershey, Le~Roux, and
  Atlas]{wisdom2016full}
S.~Wisdom, T.~Powers, J.~Hershey, J.~Le~Roux, and L.~Atlas.
\newblock Full-capacity unitary recurrent neural networks.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4880--4888, 2016.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{xiong2021nystromformer}
Y.~Xiong, Z.~Zeng, R.~Chakraborty, M.~Tan, G.~Fung, Y.~Li, and V.~Singh.
\newblock Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating
  self-attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume 35, No 16, pages 14138--14148, 2021.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
M.~Zaheer, G.~Guruganesh, K.~A. Dubey, J.~Ainslie, C.~Alberti, S.~Ontanon,
  P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17283--17297, 2020.

\bibitem[Zhu and Soricut(2021)]{zhu2021h}
Z.~Zhu and R.~Soricut.
\newblock H-transformer-1d: Fast one-dimensional hierarchical attention for
  sequences.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 3801--3815, 2021.

\end{thebibliography}




\clearpage
\beginsupplement

\section*{Supplementary Materials}

\section{Proof of Proposition \ref{prop:liquid kernel}}
\begin{proof}
This can be shown by unrolling the S4 convolution kernel and multiplying its components with , performing an anti-diagonal transformation to obtain the corresponding liquid S4 kernel:

For  (correlations of order 2), S4 kernel should be multiplied by . The resulting kernel would be:



\noindent We obtain the liquid kernel by flipping the above kernel to be convolved with the 2-term correlation terms (p=2): 



\noindent Similarly, we can obtain liquid kernels for higher liquid orders and obtain the statement of the proposition.
\end{proof}

\section{Hyperparameters}

\textbf{Learning Rate.} Liquid-S4 generally requires a smaller learning rate compared to S4 and S4D blocks.

\noindent \textbf{Setting  and } We set  for all experiments to 0.2, while the  was set based on the recommendations provided in \citep{gu2022train} to be proportional to .

\noindent \textbf{Causal Modeling vs. Bidirectional Modeling} Liquid-S4 works better when it is used as a causal model, i.e., with no bidirectional configuration.

\noindent \textbf{} We observed that liquid-S4 PB kernel performs best with smaller individual state sizes . For instance, we achieve SOTA results in ListOps, IMDB, and Speech Commands by a state size set to 7, significantly reducing the number of required parameters to solve these tasks.

\noindent \textbf{Choice of Liquid-S4 Kernel} In all experiments, we choose our simplified PB kernel over the KB kernel due to the computational costs and performance. We recommend the use of PB kernel. 

\noindent  \textbf{Choice of parameter  in liquid kernel.} In all experiments, start off by setting  or the liquidity order to 2. This means that the liquid kernel is going to be computed only for correlation terms of order 2. In principle, we observe that higher  values consistently enhance the representation learning capacity of liquid-S4 modules, as we showed in all experiments. We recommend  as a norm to perform experiments with Liquid-S4.

\noindent The kernel computation pipeline uses the PyKeops package \citep{JMLR:v22:20-275} for large tensor computations without memory overflow. 

\noindent All reported results are validation accuracy (similar to \citet{gu2022efficiently}) performed with 2 to 3 different random seeds, except for the BIDMC dataset, which reports accuracy on the test set.

\begin{table}[H]
    \centering
    \caption{Hyperparameters for obtaining best performing models. BN= Batch Normalization, LN = Layer normalization, WD= Weight decay.}
    \begin{adjustbox}{width=1\columnwidth}
    \begin{tabular}{lllllllllll}
        \toprule
               & \textbf{Depth} & \textbf{Features}   & \textbf{State Size} & \textbf{Norm} & \textbf{Pre-norm} & \textbf{Dropout} & \textbf{LR} & \textbf{Batch Size} & \textbf{Epochs} & \textbf{WD} \\
        \midrule
        \textbf{ListOps} & 9 & 128 & 7 & BN & True & 0.01 & 0.002 & 12 & 30 & 0.03 \\
        \textbf{Text (IMDB)} & 4 & 128 & 7 & BN & True & 0.1 & 0.003 & 8 & 50 & 0.01 \\
        \textbf{Retrieval (AAN)} & 6 & 256 & 64 & BN & False & 0.2 & 0.005 & 16 & 20 & 0.05\\
        \textbf{Image (CIFAR)} & 6 & 512 & 512 & LN & False & 0.1 & 0.01 & 16 & 200 & 0.03 \\
        \textbf{Pathfinder} & 6 & 256 & 64 & BN & True & 0.0 & 0.0004 & 4 & 200 & 0.03 \\
        \textbf{Path-X} & 6 & 320 & 64 & BN & True & 0.0 & 0.001 & 8 & 60 & 0.05 \\
        \midrule
        \textbf{Speech Commands} & 6 & 128 & 7 & BN & True & 0.0  & 0.008 & 10 & 50 & 0.05\\
        \midrule
        \textbf{BICMD (HR)} & 6 & 128 & 256 & LN & True & 0.0 & 0.005 & 32 & 500 & 0.01\\
        \textbf{BICMD (RR)} & 6 & 128 & 256 & LN & True & 0.0 & 0.01 & 32 & 500 & 0.01 \\
        \textbf{BICMD (SpO2)} & 6 & 128 & 256 & LN & True & 0.0 & 0.01 & 32 & 500 & 0.01 \\
        \midrule
        \textbf{sCIFAR} & 6 & 512 & 512 & LN & False & 0.1 & 0.01 & 50 & 200 & 0.03 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:hyperparams}
\end{table}






\begin{table}[H] 
    \centering
    \caption{Performance on RAW Speech Command dataset with the reduced ten classes (SC10) dataset.Numbers indicate validation accuracy. The accuracy of baseline models is reported from Table 5 of \citep{gu2022efficiently}. x stands for infeasible computation on a single GPU or not applicable as stated in Table 10 of \citep{gu2022efficiently}. The hyperparameters for Liquid-S4 are the same as the ones reported for Speech Commands Full Dataset reported in Table \ref{tab:hyperparams}.}
\begin{adjustbox}{width=0.4\columnwidth}
\begin{tabular}{lll}
\toprule
& \multicolumn{2}{c}{\textbf{SC10}} \\
\midrule
Model & 16kHz & 8kHz \\
\midrule
Transformer & x & x \\
Performer & 30.77 & 30.68 \\
ODE-RNN & x & x \\
NRDE & 16.49 & 15.12 \\
ExpRNN & 11.6 & 10.8 \\
LipschitzRNN & x & x \\
CKConv & 71.66 & 65.96 \\
WaveGAN-D &  96.25 & x \\
LSSL \citep{gu2021combining} & x & x \\
S4-LegS \citep{gu2022efficiently} & \text98.32 & \textbf{96.30} \\
\midrule
Liquid-S4 (ours) &  \textbf{98.51} & 95.9 \\
& p=2 & p=2 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:sc_1}
\end{table}


\end{document}
