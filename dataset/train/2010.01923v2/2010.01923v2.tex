

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{1159} 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Learning from Context or Names?\\An Empirical Study on Neural Relation Extraction \\
- Appendix -}

\author{Hao Peng$^{1}$\thanks{\quad Equal contribution}\hspace{0.5em}, Tianyu Gao$^{2*}$, Xu Han$^{1}$, Yankai Lin$^{3}$, Peng Li$^{3}$, Zhiyuan Liu$^{1}$\thanks{\quad Corresponding author e-mail: liuzy@tsinghua.edu.cn}\hspace{0.5em},  \\\textbf{Maosong Sun$^{1}$, Jie Zhou$^{3}$}\\
$^1$Department of Computer Science and Technology, Tsinghua University, Beijing, China\\
$^2$Princeton University, Princeton, NJ, USA\\
$^3$Pattern Recognition Center, WeChat AI, Tencent Inc, China\\
{\tt \{h-peng17,hanxu17\}@mails.tsinghua.edu.cn, tianyug@princeton.edu}
}

\begin{document}
\maketitle

\appendix

\section{Pre-training Details} 
\label{appendix:pretraining}

\paragraph{Pre-training Dataset} We construct a dataset for pre-training following the method in the paper. We use Wikipedia articles as corpus and Wikidata~\citep{vrandevcic2014wikidata} as the knowledge graph. Firstly, We 
use anchors to link entity mentions in Wikipedia corpus with entities in Wikidata. Then, in order to link more unanchored entity mentions, we adopt \texttt{spaCy}\footnote{\url{https://spacy.io/}} to find all possible entity mentions, and link them to entities in Wikidata via name matching. Finally, we get a pre-training dataset containing $744$ relations and $867,278$ sentences. We release this dataset together with our source code at our GitHub repository\footnote{\url{https://github.com/thunlp/RE-Context-or-Names}}.

We also use this dataset for MTB, which is slightly different from the original paper~\citep{soares2019matching}. The original MTB takes all entity pairs into consideration, even if they do not have a relationship in Wikidata. Using the above dataset means that we filter out these entity pairs. We do this out of training efficiency, for those entity pairs that do not have a relation are likely to express little relational information, and thus contribute little to the pre-training.

\paragraph{Data Sampling Strategy} For MTB~\citep{soares2019matching}, we follow the same sampling strategy as in the original paper.
For pre-training our contrastive model, we regard sentences labeled with the same relation as a ``bag''. Any sentence pair whose sentences are in the same bag is treated as a positive pair and as a negative pair otherwise. So there will be a large amount of possible positive samples and negative samples. We dynamically sample positive pairs of a relation with respect to the number of sentences in the bag. 

\paragraph{Hyperparameters} We use Huggingface's \texttt{Transformers}\footnote{\url{https://github.com/huggingface/transformers}} to implement models for both pre-training and fine-tuning and use AdamW~\citep{loshchilov2018fixing} for optimization. 
For most pre-training hyperparameters, we select the same values as \citet{soares2019matching}. We search hyperparameter batch size in \{$256$, $2048$\} and $P_\texttt{BLANK}$ in \{$0.3$, $0.7$\}. For MTB, batch size $N$ means that a batch contains $2N$ sentences, which form $N/2$ positive pairs and $N/2$ negative pairs. For CP, batch size $N$ means that a batch contains $2N$ sentences, which form $N$ positive pairs.
For negative samples, we pair the sentence in each pair with sentences in other pairs. 


We set hyperparameters according to results on supervised RE dataset TACRED (micro F$_1$).
Table \ref{tab:params} shows hyperparameters for pre-training MTB and our contrastive model (CP). The batch size of our implemented MTB is different from that in \citet{soares2019matching}, because in our experiments, MTB with a batch size of $256$ performs better on TACRED than the batch size of $2048$. 


\begin{table}[]
\small
    \centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Parameter} &  \textbf{MTB} & \textbf{CP} \\
        \midrule 
        Learning Rate & $3\times10^{-5}$ & $3\times10^{-5}$\\
        Batch Size & $256$ & $2048$ \\
        Sentence Length & $64$ & $64$ \\
        $P_{\texttt{BLANK}}$ & $0.7$ & $0.7$ \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for pre-training models. $P_{\texttt{BLANK}}$ corresponds to the probability of replacing entities with \texttt{[BLANK]}.}
    \label{tab:params}
\end{table}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{l|rrr}
        \toprule
        \textbf{Dataset} &  \textbf{Train} & \textbf{Dev} & \textbf{Test} \\
        \midrule 
        TACRED & 68,124 & 22,631 & 15,509 \\
        SemEval & 6,507 & 1,493 & 2,717 \\
        Wiki80 & 39,200 & 5,600& 11,200   \\
        ChemProt & 4,169 & 2,427& 3,469 \\
        FewRel &44,800 & 11,200& 14,000\\ 
        \bottomrule
    \end{tabular}
    \caption{Numbers of instances in train~/~dev~/~test splits for different RE datasets.}
    \label{tab:data_size}
\end{table}

\paragraph{Pre-training Efficiency}  MTB and our contrastive model have the same architecture as BERT$_{\small \textsc{base}}$~\citep{devlin2019bert}, so they both hold $110$M parameters approximately.
We use four Nvidia 2080Ti GPUs to pre-train models. Pre-training MTB takes $30,000$ training steps and approximately $24$ hours. Pre-training our model takes $3,500$ training steps and approximately $12$ hours. 


\section{RE Fine-tuning} 
\label{appendix:fine-tuning}

\paragraph{RE Datasets} We download TACRED from \texttt{LDC}\footnote{\url{https://catalog.ldc.upenn.edu/LDC2018T24}}, Wiki80, SemEval from \texttt{OpenNRE}\footnote{\url{https://github.com/thunlp/OpenNRE}}, ChemProt from \texttt{sciBert}\footnote{\url{https://github.com/allenai/scibert}}, and FewRel from \texttt{FewRel}\footnote{\url{https://github.com/thunlp/fewrel}}. Table~\ref{tab:data_size} shows detailed statistics for each dataset and Table~\ref{tab:data_size_different} demonstrates the sizes of training data for different supervised RE datasets in 1\%, 10\% and 100\% settings. 
For 1\% and 10\% settings, we randomly sample 1\% and 10\% training data for each relation (so the total training instances for 1\%~/~10\% settings are not exactly 1\%~/~10\% of the total training instances in the original datasets). As shown in the table, the numbers of training instances in SemEval and ChemProt for 1\% setting are extremely small, which explains the abnormal performance.




\begin{table}[t]
\small
    \centering
    \begin{tabular}{l|rrr}
        \toprule
        \textbf{Dataset} &  \textbf{1\%} & \textbf{10}\% & \textbf{100}\% \\
        \midrule 
        TACRED & 703 & 6,833& 68,124\\
        SemEval & 73 & 660& 6,507 \\
        Wiki80 & 400 & 3,920& 3,9200 \\
        ChemProt & 49 & 423& 4,169 \\
        \bottomrule
    \end{tabular}
    \caption{Numbers of training instances in supervised RE datasets under different proportion settings.}
    \label{tab:data_size_different}
\end{table}

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Parameter} & \textbf{Supervised RE} &  \textbf{Few-Shot RE} \\
        \midrule 
        Learning Rate & $3\times10^{-5}$ & $2\times10^{-5}$\\
        Batch Size & $64$ & $4$ \\
        Epoch & $6$ & $10$ \\
        Sentence Length & $100$ & $128$\\
        Hidden Size & 768 & 768 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for fine-tuning on relation extraction tasks (BERT, MTB and CP).}
    \label{tab:task_params}
\end{table}

\paragraph{Hyperparameters} 



Table~\ref{tab:task_params} shows hyperparameters when finetuning on different RE tasks for BERT, MTB and CP. For CNN, we train the model by SGD with a learning rate of $0.5$, a batch size of $160$ and a hidden size of $230$.
For few-shot RE, we use the recommended hyperparameters in \texttt{FewRel}\footnote{\url{https://github.com/thunlp/FewRel}}.






\paragraph{Multiple Trial Settings}  For all the results on supervised RE, we run each experiment 5 times using 5 different seeds~($42,43,44,45,46$) and select the median of 5 results as the final reported number. For few-shot RE, as the model varies little with different seeds and it is evaluated in a sampling manner, we just run one trial with $10000$ evaluation episodes, which is large enough for the result to converge. We report accuracy (proportion of correct instances in all instances) for Wiki80 and FewRel, and micro \texttt{F$_1$}\footnote{\url{https://en.wikipedia.org/wiki/F1_score}} for all the other datasets.








\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}


\end{document}
