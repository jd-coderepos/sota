

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{1159} 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Learning from Context or Names?\\An Empirical Study on Neural Relation Extraction \\
- Appendix -}

\author{Hao Peng\thanks{\quad Equal contribution}\hspace{0.5em}, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu\thanks{\quad Corresponding author e-mail: liuzy@tsinghua.edu.cn}\hspace{0.5em},  \\\textbf{Maosong Sun, Jie Zhou}\\
Department of Computer Science and Technology, Tsinghua University, Beijing, China\\
Princeton University, Princeton, NJ, USA\\
Pattern Recognition Center, WeChat AI, Tencent Inc, China\\
{\tt \{h-peng17,hanxu17\}@mails.tsinghua.edu.cn, tianyug@princeton.edu}
}

\begin{document}
\maketitle

\appendix

\section{Pre-training Details} 
\label{appendix:pretraining}

\paragraph{Pre-training Dataset} We construct a dataset for pre-training following the method in the paper. We use Wikipedia articles as corpus and Wikidata~\citep{vrandevcic2014wikidata} as the knowledge graph. Firstly, We 
use anchors to link entity mentions in Wikipedia corpus with entities in Wikidata. Then, in order to link more unanchored entity mentions, we adopt \texttt{spaCy}\footnote{\url{https://spacy.io/}} to find all possible entity mentions, and link them to entities in Wikidata via name matching. Finally, we get a pre-training dataset containing  relations and  sentences. We release this dataset together with our source code at our GitHub repository\footnote{\url{https://github.com/thunlp/RE-Context-or-Names}}.

We also use this dataset for MTB, which is slightly different from the original paper~\citep{soares2019matching}. The original MTB takes all entity pairs into consideration, even if they do not have a relationship in Wikidata. Using the above dataset means that we filter out these entity pairs. We do this out of training efficiency, for those entity pairs that do not have a relation are likely to express little relational information, and thus contribute little to the pre-training.

\paragraph{Data Sampling Strategy} For MTB~\citep{soares2019matching}, we follow the same sampling strategy as in the original paper.
For pre-training our contrastive model, we regard sentences labeled with the same relation as a ``bag''. Any sentence pair whose sentences are in the same bag is treated as a positive pair and as a negative pair otherwise. So there will be a large amount of possible positive samples and negative samples. We dynamically sample positive pairs of a relation with respect to the number of sentences in the bag. 

\paragraph{Hyperparameters} We use Huggingface's \texttt{Transformers}\footnote{\url{https://github.com/huggingface/transformers}} to implement models for both pre-training and fine-tuning and use AdamW~\citep{loshchilov2018fixing} for optimization. 
For most pre-training hyperparameters, we select the same values as \citet{soares2019matching}. We search hyperparameter batch size in \{, \} and  in \{, \}. For MTB, batch size  means that a batch contains  sentences, which form  positive pairs and  negative pairs. For CP, batch size  means that a batch contains  sentences, which form  positive pairs.
For negative samples, we pair the sentence in each pair with sentences in other pairs. 


We set hyperparameters according to results on supervised RE dataset TACRED (micro F).
Table \ref{tab:params} shows hyperparameters for pre-training MTB and our contrastive model (CP). The batch size of our implemented MTB is different from that in \citet{soares2019matching}, because in our experiments, MTB with a batch size of  performs better on TACRED than the batch size of . 


\begin{table}[]
\small
    \centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Parameter} &  \textbf{MTB} & \textbf{CP} \\
        \midrule 
        Learning Rate &  & \\
        Batch Size &  &  \\
        Sentence Length &  &  \\
         &  &  \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for pre-training models.  corresponds to the probability of replacing entities with \texttt{[BLANK]}.}
    \label{tab:params}
\end{table}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{l|rrr}
        \toprule
        \textbf{Dataset} &  \textbf{Train} & \textbf{Dev} & \textbf{Test} \\
        \midrule 
        TACRED & 68,124 & 22,631 & 15,509 \\
        SemEval & 6,507 & 1,493 & 2,717 \\
        Wiki80 & 39,200 & 5,600& 11,200   \\
        ChemProt & 4,169 & 2,427& 3,469 \\
        FewRel &44,800 & 11,200& 14,000\\ 
        \bottomrule
    \end{tabular}
    \caption{Numbers of instances in train~/~dev~/~test splits for different RE datasets.}
    \label{tab:data_size}
\end{table}

\paragraph{Pre-training Efficiency}  MTB and our contrastive model have the same architecture as BERT~\citep{devlin2019bert}, so they both hold M parameters approximately.
We use four Nvidia 2080Ti GPUs to pre-train models. Pre-training MTB takes  training steps and approximately  hours. Pre-training our model takes  training steps and approximately  hours. 


\section{RE Fine-tuning} 
\label{appendix:fine-tuning}

\paragraph{RE Datasets} We download TACRED from \texttt{LDC}\footnote{\url{https://catalog.ldc.upenn.edu/LDC2018T24}}, Wiki80, SemEval from \texttt{OpenNRE}\footnote{\url{https://github.com/thunlp/OpenNRE}}, ChemProt from \texttt{sciBert}\footnote{\url{https://github.com/allenai/scibert}}, and FewRel from \texttt{FewRel}\footnote{\url{https://github.com/thunlp/fewrel}}. Table~\ref{tab:data_size} shows detailed statistics for each dataset and Table~\ref{tab:data_size_different} demonstrates the sizes of training data for different supervised RE datasets in 1\%, 10\% and 100\% settings. 
For 1\% and 10\% settings, we randomly sample 1\% and 10\% training data for each relation (so the total training instances for 1\%~/~10\% settings are not exactly 1\%~/~10\% of the total training instances in the original datasets). As shown in the table, the numbers of training instances in SemEval and ChemProt for 1\% setting are extremely small, which explains the abnormal performance.




\begin{table}[t]
\small
    \centering
    \begin{tabular}{l|rrr}
        \toprule
        \textbf{Dataset} &  \textbf{1\%} & \textbf{10}\% & \textbf{100}\% \\
        \midrule 
        TACRED & 703 & 6,833& 68,124\\
        SemEval & 73 & 660& 6,507 \\
        Wiki80 & 400 & 3,920& 3,9200 \\
        ChemProt & 49 & 423& 4,169 \\
        \bottomrule
    \end{tabular}
    \caption{Numbers of training instances in supervised RE datasets under different proportion settings.}
    \label{tab:data_size_different}
\end{table}

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Parameter} & \textbf{Supervised RE} &  \textbf{Few-Shot RE} \\
        \midrule 
        Learning Rate &  & \\
        Batch Size &  &  \\
        Epoch &  &  \\
        Sentence Length &  & \\
        Hidden Size & 768 & 768 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for fine-tuning on relation extraction tasks (BERT, MTB and CP).}
    \label{tab:task_params}
\end{table}

\paragraph{Hyperparameters} 



Table~\ref{tab:task_params} shows hyperparameters when finetuning on different RE tasks for BERT, MTB and CP. For CNN, we train the model by SGD with a learning rate of , a batch size of  and a hidden size of .
For few-shot RE, we use the recommended hyperparameters in \texttt{FewRel}\footnote{\url{https://github.com/thunlp/FewRel}}.






\paragraph{Multiple Trial Settings}  For all the results on supervised RE, we run each experiment 5 times using 5 different seeds~() and select the median of 5 results as the final reported number. For few-shot RE, as the model varies little with different seeds and it is evaluated in a sampling manner, we just run one trial with  evaluation episodes, which is large enough for the result to converge. We report accuracy (proportion of correct instances in all instances) for Wiki80 and FewRel, and micro \texttt{F}\footnote{\url{https://en.wikipedia.org/wiki/F1_score}} for all the other datasets.








\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}


\end{document}
