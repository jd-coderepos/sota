\renewcommand{\thesection}{\Alph{section}}
\onecolumn
\thispagestyle{empty}
\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}}
\begin{center}\Large \setstretch{1.2} Supplementary Material for \\
\textbf{Differentiable Model Compression via Pseudo Quantization Noise}
\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}}
\end{center}
\setcounter{section}{0}
\numberwithin{equation}{section}
\counterwithin{table}{section}
\counterwithin{figure}{section}
\setcounter{secnumdepth}{1}
\renewcommand\theHsection{\Alph{section}}

We provide in Section~\ref{app:xps} all the details on the 
exact hyper-parameters, models, and datasets used for the results in Section~\ref{results} of the main paper.
Then, we provide supplementary results in Section~\ref{app:results},
in particular tables for the scatter plots given on Figures \ref{fig:teaser} and \ref{fig:cifar}.

\section{Detailed experimental setup}
\label{app:xps}

All experiments are conducted using NVIDIA V100 GPUs with either 16GB or 32GB RAM, depending on the applications
(with language modeling requiring larger GPUs) on an internal cluster.
For all models trained with QAT or \diffq, we do not quantize tensors with a size under 0.01 MB (0.1 MB for the DeiT model).

\subsection{\diffq{} hyper-parameters}

For all experiments, we use , ,
 and Gaussian noise. 
We observed on most models that taking  is
unstable, with the notable exception of Resnet-20.
We use a separate Adam optimizer~\citep{adam} for the logit parameters controlling the number of bits used, with a default momentum  and decay .
We use the default learning rate  for all task,
except language modeling where we use .
The remaining hyper-parameters are , the amount of penalty
applied to the model size, and , the group size. When  is not mentioned, it is set to the default value , which we found to be the best trade-off between the model freedom and the overhead from storing the number of bits used for each group. 

\subsection{Music Source Separation}

We train a Demucs source separation model~\citep{defossez2019music} (MIT license) with a depth of 6 and 64 initial hidden channels, on the MusDB dataset~\citep{musdb}\footnote{\url{https://sigsep.github.io/datasets/musdb.html}}, which is released under mixed licensing\footnote{\url{https://github.com/sigsep/website/blob/master/content/datasets/assets/tracklist.csv}}. 
All the training details are exactly as in~\citep{defossez2019music}.

\subsection{Language Modeling}

We trained a 16 layers transformer~\citep{vaswani2017attention} based language model on the Wikitext-103 text corpus~\citep{merity2016pointer}\footnote{\url{https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/}} released under the CC-BY-SA license,
following~\citet{baevski2018adaptive},
 and using the \texttt{Fairseq} framework~\citep{ott2019fairseq}, released under the MIT license. 
 We used the hyper-parameters and the script provided by \citep{fan2020training} in the \texttt{Fairseq} repository\footnote{\url{https://github.com/pytorch/fairseq/tree/master/examples/quant_noise}}, however, and unlike what they mention in their paper, this script does not include layer drop~\citep{fan2019reducing}.
 For \diffq, we tried the penalty levels  in , with group size , as well as  and .
 For LSQ, we used the same training hyper-parameters as \diffq, except we initialized the model
 to a pre-trained model and used a learning rate 10 time smaller for fine tuning. Without this initialization, LSQ was failing to get under 40 of perplexity.

\noindent{\bf Tied weights and \diffq.}
The model we trained was configured so that the word embedding in the first layer and the weight of the adaptive softmax are bound to the same value. It is important to detect such bounded parameters with \diffq, as otherwise, a different number of bits could be used for what is in fact, the very same tensor. Not only do we use a single bits logit parameter
when a parameter tensor is reused multiple times, but for each forward, we make sure that the pseudo quantization noise
is sampled only once and reused appropriately. Failure to do so led to a significant worsening of the performance
at validation time.


\subsection{Image classification}
\label{supp:image}

\noindent{\bf CIFAR10/100.}
On the CIFAR10/100 datasets, we train 3 different models: MobileNet-v1~\citep{howard2017mobilenets}, ResNet-18~\citep{he2016deep}, and a Wide-ResNet with 28x10 depth and width levels respectively~\citep{zagoruyko2016wide}.
All experiments are conducted on a single GPU with a batch size of 128, SGD with a learning rate of 0.1, momentum of 0.9,
weight decay of . The learning rate is decayed by a factor of 0.2 every 60 iterations.
To generate Figure~\ref{fig:cifar}, we evaluated \diffq for  in 
and the group size  in .

For LSQ~\citep{esser2020learned}, we initialize from a pre-trained model. We tested both training for 90 epochs with a cosine schedule as originally done, or keeping the original learning rate schedule, only reducing the initial learning rate from 0.1 to 0.01. The second option achieved better overall results and that is the one we report.
We also tried training with LSQ from a randomly initialized model, but that performed the worst of all approaches.

The dataset has been obtained from the \texttt{torchvision} package\footnote{\url{https://github.com/pytorch/vision}}.
The input images are augmented with a random crop of size 32 with padding of 4, and a random horizontal flip.
The RGB pixel values are normalized to mean 0 and standard deviation 1. We use the default split between
train and valid as obtained from the \texttt{torchvision} package.

\noindent{\bf CIFAR-10 - Resnet 20.}
We use the implementation of Resnet 20 from~\citet{Idelbayev18a}.
We train for 600 epochs with a batch size of 128, with a learning rate of 0.1, momentum of 0.9, weight decay of , and decrease the learning rate by a factor of 10, every 200 epochs.
We quantize all parameters except biases, we set the minimum number of bits to  as we observe this was stable for this particular task, and lower the maximum number of bits to . We use a group size  and a penalty  and a learning rate of , for the separate Adam optimizer used for the bits parameters, which allows stay stable while going under 2 bits per weight.

\noindent{\bf ImageNet.}
We train an EfficientNet as implemented by~\citep{rw2019timm} (Apache license),
as well as a DeiT vision transformer~\citep{touvron2020training} (MIT license) on the ImageNet dataset~\cite{imagenet_cvpr09}\footnote{\url{http://www.image-net.org/}}. We use the original dataset
split between train and valid. 
The images go through a random resize crop to 300px, a random horizontal flip, and pixel RGB values are normalized
to have zero mean and unit variance.

\noindent{\bf ImageNet - EfficientNet.}
We trained for 100 epochs, using RMSProp~\cite{tieleman2012lecture} as
implemented in the \texttt{timm} package\footnote{\url{https://github.com/rwightman/pytorch-image-models}}
with a learning rate of 0.0016, a weight decay of  and a momentum of 0.9. The learning rate is decayed
by a factor of 0.9875 with every epoch. As a warmup, the learning rate is linearly scaled from 0 to 0.0016 over the first 3 epochs. Following~\citep{rw2019timm}, we evaluate with an exponential moving average of the weights of the model, with
a decay of 0.99985. We use the random erase augmentation from \citep{rw2019timm}, as well as cutmix~\citep{yun2019cutmix},
with a probability of 0.2 and parameter to the beta distribution of 0.2.
All the models are trained on 8 GPUs. For \diffq, we used the penalties  in 

and the default group size .

\noindent{\bf ImageNet - DeiT.}
We use the official DeiT implementation by \citet{touvron2020training}\footnote{\url{https://github.com/facebookresearch/deit}}, with the default
training parameters, but without exponential moving averaging of the weights. More precisely, we trained for 300 epochs over 16 GPUs, with a batch size per GPU of 64, AdamW~\citep{loshchilov2017decoupled}, a weight decay of 0.05, learning rate of , cosine
learning rate scheduler, a learning rate warmup from  over 5 epochs and label smoothing~\citep{szegedy2016rethinking}. As data augmentation, we used color-jitter, random erase, and either cutmix or mixup~\citep{zhang2017mixup}.

For \diffq, we tested the penalty  in ,
and group size  in . We use a minimum number of bits of 3, instead of 2, as this led to better stability. 
We use Adam~\citep{adam} to optimize the bits parameters, with a learning rate of .

\noindent{\bf ImageNet - ResNet18 and ResNet50.}
We trained all models (\diffq and LSQ) for 400 epochs over 4 GPUs, with a batch size per GPU of 256, using  RMSProp~\cite{tieleman2012lecture} as implemented in the \texttt{timm} package\footnote{\url{https://github.com/rwightman/pytorch-image-models}} (also experimented with SGD however RMSProp provides better results), a weight decay of 0.05, learning rate of , where we multiply the learning by 0.9875 after every epoch. We used a learning rate warmup from  over 3 epochs and label smoothing~\citep{szegedy2016rethinking} with smoothing factor of 0.3. As data augmentation, we used color-jitter, random erase and cutmix using  with probability of 0.3.

For \diffq, we tested the penalty  in , and group size  in . We use a minimum number of bits of 2. We use Adam~\citep{adam} to optimize the bits parameters, with a learning rate of .

\section{Supplementary results}
\label{app:results}

\subsection{ImageNet}
\label{supp:imagenet}
On Table~\ref{tab:imagenet_supp} results are reported for \diffq and LSQ~\cite{esser2020learned} using ResNet-18 and ResNet-50 on ImageNet dataset. We compared different model sizes and compression rates. Results suggest that \diffq is superior both in terms of accuracy and smaller model size. 

Results marked with * are the ones reported in \cite{esser2020learned} using slightly better uncompressed model. For fair comparison we reported both numbers. Notice, \diffq achieves comparable and even superior results over LSQ also under considering this setting.

\begin{table}[t!]
\begin{center}
\begin{small}
\caption{Additional comparison between \diffq and LSQ~\cite{esser2020learned} for different model sizes and compression rates. Results marked with * are the ones reported in \citet{esser2020learned} using slightly better uncompressed baseline model. For fair comparison we reported both numbers. All results reported are for the best model accuracy.}
\label{tab:imagenet_supp}
\centering
\begin{sc}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
Model       & Method     & Top-1 Acc. (\%) & M.S. (MB)   \\ \midrule
\multicolumn{4}{c}{ImageNet}                 \\ \midrule
ResNet-18   & Uncompressed                  & 70.9       & 44.6  \\ 
\midrule
ResNet-18   & LSQ 8 bits~\cite{esser2020learned}        & 71.8       & 11.2 \\  
ResNet-18   & LSQ 4 bits~\cite{esser2020learned}        & 70.7       & 5.6  \\ 
ResNet-18   & LSQ 3 bits~\cite{esser2020learned}        & 69.0         & 4.2  \\ 
\midrule
ResNet-18   & LSQ* 8 bits~\cite{esser2020learned}        & 71.1       & 11.2 \\  
ResNet-18   & LSQ* 4 bits~\cite{esser2020learned}        & 71.1       & 5.6  \\ 
ResNet-18   & LSQ* 3 bits~\cite{esser2020learned}        & 70.2         & 4.2  \\ 
\midrule
ResNet-18   & \diffq~(Ours)       & 71.8       & 7.6  \\ 
ResNet-18   & \diffq~(Ours)       & 71.1       & 5.3  \\ 
ResNet-18   & \diffq~(Ours)       & 70.2       & 4.5  \\ 
ResNet-18   & \diffq~(Ours)       & 69.7       & 4.1  \\ 
\midrule
ResNet-50   & Uncompressed                       & 77.1       & 97.5  \\ 
\midrule
ResNet-50   & LSQ 8 bits~\cite{esser2020learned}        & 76.8       & 24.5 \\ 
ResNet-50   & LSQ 4 bits~\cite{esser2020learned}        & 76.2       & 12.3 \\ 
ResNet-50   & LSQ 3 bits~\cite{esser2020learned}        & 75.6       & 9.3  \\ 
\midrule
ResNet-50   & LSQ* 8 bits~\cite{esser2020learned}        & 76.8       & 24.5 \\ 
ResNet-50   & LSQ* 4 bits~\cite{esser2020learned}        & 76.7       & 12.3 \\ 
ResNet-50   & LSQ* 3 bits~\cite{esser2020learned}        & 75.8       & 9.3  \\ 
\midrule
ResNet-50   & \diffq~(Ours)       & 76.9       & 14   \\ 
ResNet-50   & \diffq~(Ours)       & 76.6       & 10.5 \\ 
ResNet-50   & \diffq~(Ours)       & 76.3       & 8.8    \\ 
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[t!]
\caption{Detailed results of QAT and \diffq on the CIFAR-10/100 datasets. For each architecture and dataset,
we provide the performance of the baseline, QAT models with 2 to 4 bits, and two \diffq runs: v1. is the smallest model that is within a small range of the baseline performance, v2. is the best model of comparable size with QAT 2 bits, selected from the pool of candidates described in Section \ref{supp:image}. For Wide-ResNet, we report a single variant of \diffq, as it is both the smallest and the one with the best accuracy.}
\label{tab:cifar_supp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l|l|cc|cc|cc}
\toprule
	&	 & \multicolumn{2}{c}{MobileNet} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{WideResNet} \\
\midrule
	&	 & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-10}}
& Uncompressed    	        & 90.9 		   & 12.3 & 95.3  & 42.7  & 95.3    & 139.2\\
\cmidrule(lr){2-8}
& QAT 2bits    				& 78.1         & 0.88 & 87.2  & 2.70  & 70.8    & 8.81  \\
& QAT 3bits    				& 88.2         & 1.26 & 94.0  & 4.03  & 94.3    & 13.16 \\
& QAT 4bits    				& 90.1         & 1.64 & 95.0  & 5.36  & 94.4    & 17.50 \\
\cmidrule(lr){2-8}
& LSQ 2 bits                & 10.0         & 0.88 & 95.0  & 2.70  & 81.9. & 8.81  \\
& LSQ 3 bits                & 90.8         & 1.26 & 95.3  & 4.03  & 88.8  & 13.16 \\
& LSQ 4 bits                & 90.9         & 1.64 & 95.2  & 5.36  & 89.9 & 17.50 \\
\cmidrule(lr){2-8}
 & \diffq v1 & 90.3 & 0.94 & 94.9 & 3.17 & 94.1 & 8.81\\
 & \diffq v2 & 87.9 & 0.91 & 93.9 & 2.71 & 94.1 & 8.81\\
\midrule\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-100}}
& Uncompressed    	            & 68.1 		   & 12.6 & 77.9 & 42.8 & 76.2  & 139.4\\
\cmidrule(lr){2-8}
& QAT 2bits    		            & 10.9         & 0.91  & 58.7 & 2.72  & 46.5  & 8.83  \\
& QAT 3bits    		            & 59.7         & 1.29  & 73.7 & 4.05  & 75.0  & 13.18 \\
& QAT 4bits    		            & 66.9         & 1.69  & 77.3 & 5.39  & 75.5  & 17.53 \\
\cmidrule(lr){2-8}
& LSQ 2 bits                    & 64.9         & 0.91 & 77.5  & 2.72  & 40.9  & 8.82  \\
& LSQ 3 bits                    & 67.7         & 1.29 & 77.7  & 4.05  & 55.6  & 13.18 \\
& LSQ 4 bits                    & 68.5         & 1.69 & 77.8  & 5.39  & 56.5  & 17.53 \\
\cmidrule(lr){2-8}
 & \diffq v1 & 68.5 & 1.10 & 77.6 & 4.82 & 75.3 & 8.83\\
 & \diffq v2 & 64.6 & 0.94 & 71.7 & 2.72 & 75.6 & 8.84\\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\subsection{CIFAR-10/100}
\label{supp:cifar}
We report on Table~\ref{tab:cifar_supp} the results on the CIFAR10/100 datasets, which are shown for CIFAR100 in Figure~\ref{fig:cifar} in the main paper. Results are presented using MobileNet-v1, ResNet-18, and WideResNet. For CIFAR100 the presented results used for creating Figure~\ref{fig:cifar} in the main paper.
As we cannot show all the \diffq runs, we selected for each model and dataset two versions: v1 is the smallest model that has an accuracy
comparable to the baseline (accuracy is greater than  times the baseline accuracy), while v2 is the model with the highest accuracy
that is comparable in size with the QAT 2 bits model (size must be smaller than  times the baseline size, except for MobileNet, for which we had to allow a 4\% relative increase in size. The penalty and group size selected with this procedure is displayed on Table~\ref{table_hyper_supp}.

\begin{table}[t!]
\caption{Penalty  and group size  for the v1 and v2 \diffq models reported on Table~\ref{tab:cifar_supp}}
\label{table_hyper_supp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|l|cc|cc|cc}
\toprule
	&	 & \multicolumn{2}{c}{MobileNet} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{WideResNet} \\
\midrule
	&	 &  &  &  &  &  &  \\
\midrule
\multirow{2}{*}{CIFAR-10}
 & \diffq v1 & 1 & 16 & 0.1 & 8 & 5 & 16\\
 & \diffq v2 & 5 & 8 & 5 & 4 & 5 & 16\\
\midrule\midrule
\multirow{2}{*}{CIFAR-100}
 & \diffq v1 & 1 & 16 & 0.05 & 4 & 5 & 16\\
 & \diffq v2 & 5 & 16 & 5 & 8 & 1 & 16\\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Looking first at v1 models, we achieve on all tasks and datasets
a model that is competitive with the baseline (sometimes even better),
with a model size that is smaller than a QAT 4 bits model (for instance more than 2MB saved on a ResNet-18 trained on CIFAR-10 compared to QAT 4 bits, for the same accuracy).

Now for v2, first note that as the minimum number of bits used by \diffq is exactly 2, it is not possible here to make a model smaller than QAT 2 bits. However, even with as little as 0.01 MB extra, \diffq can get up to 30\% increase in accuracy compared to QAT 2 bits (for a Wide ResNet). On all architectue and datasets,
the gain from \diffq over QAT 2 bits is at least 10\% accuracy.
This confirms in practice the bias of STE-based methods when the number
of bits is reduced, a bias that we already demonstrated in theory in Section~\ref{sec:counter}. In particular, it is interesting that the largest
improvement provided by \diffq is for the Wide ResNet model, which should be the easiest to quantize. But having the largest number of weights, it 
also likely the one that is the most sensitive to the oscillations of QAT quantized weights described in Section~\ref{sec:counter}.

\paragraph{Ablation.} Table~\ref{supp:tab:app_image_abb} summarizes the results of comparing QAT against \diffq for model quantization using a fixed number of bits using MobileNet, ResNet-18, and WideResNet on both CIFAR10 and CIFAR100. \diffq outperforms QAT, where this is especially noticeable while using 2 bits quantization, in which training is less stable for QAT. 

Next, we evaluated the affect of the group-size, , on model size and accuracy, by optimizing \diffq models using . When , we use a single group for the entire layer. Results for ResNet-18 using CIFAR-100 are summarized in Figure~\ref{fig:supp} (a). Interestingly, we observed that increasing , yields in a smaller model size on the expanse of a minor decrease in performance. However, when setting  model performance (model size and accuracy) is comparable to  for this task.

\begin{table*}[t!]
\caption{A comparison between QAT and \diffq while we consider a fixed number of bits for all model parameters, specifically using 2, 3, and 4 bits. Results are reported for CIFAR-10 and CIFAR-100 using MobileNet-v1, ResNet-18. and WideResNet. We report Accuracy (Acc.) and Model Size (M.S.).}
\label{supp:tab:app_image_abb}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|cc|cc|cc}
\toprule
	&	 & \multicolumn{2}{c}{MobileNet} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{WideResNet} \\
\midrule
	&	 & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ \\
\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{CIFAR-10}}
& Uncompressed    	        & 90.9 		   & 12.3 & 95.3  & 42.8  & 95.3    & 139.4\\
\cmidrule(lr){2-8}
& QAT 2bits    				& 78.1         & 0.88 & 87.2  & 2.70  & 70.8    & 8.81  \\
& QAT 3bits    				& 88.2         & 1.26 & 94.0  & 4.03  & 94.3    & 13.16 \\
& QAT 4bits    				& 90.1         & 1.64 & 95.0  & 5.36  & 94.4    & 17.50 \\
\cmidrule(lr){2-8}
& LSQ 2 bits                & 10.0         & 0.88 & 95.0  & 2.70  & 81.9 & 8.81  \\
& LSQ 3 bits                & 90.8         & 1.26 & 95.3  & 4.03  & 88.8  & 13.16 \\
& LSQ 4 bits                & 90.9         & 1.64 & 95.2  & 5.36  & 89.9 & 17.50 \\
\cmidrule(lr){2-8}
& \diffq 2bits	 			& 84.1  	   & 0.88 & 92.3  & 2.70  & 94.4    & 8.81   \\
& \diffq 3bits				& 89.7         & 1.26 & 94.4  & 4.03  & 94.4    & 13.16  \\
& \diffq 4bits				& 90.4         & 1.64 & 95.1  & 5.36  & 94.6    & 17.50  \\
\midrule\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{CIFAR-100}}
& Uncompressed    	            & 68.1 		   & 12.6 & 77.9 & 42.8 & 76.2  & 139.4\\
\cmidrule(lr){2-8}
& QAT 2bits    		            & 10.9         & 0.91  & 58.7 & 2.72  & 46.5  & 8.82  \\
& QAT 3bits    		            & 59.7         & 1.29  & 73.7 & 4.05  & 75.0  & 13.18 \\
& QAT 4bits    		            & 66.9         & 1.69  & 77.3 & 5.39  & 75.5  & 17.53 \\
\cmidrule(lr){2-8}
& LSQ 2 bits                    & 64.9         & 0.91 & 77.5  & 2.72  & 40.9  & 8.82  \\
& LSQ 3 bits                    & 67.7         & 1.29 & 77.7  & 4.05  & 55.6  & 13.18 \\
& LSQ 4 bits                    & 68.5         & 1.69 & 77.8  & 5.39  & 56.5  & 17.53 \\
\cmidrule(lr){2-8}
& \diffq 2bits	 			    & 17.2  	   & 0.91 & 66.6 & 2.72  & 72.8   & 8.82   \\
& \diffq 3bits				    & 60.1         & 1.29 & 76.7 & 4.05  & 76.9   & 13.18  \\
& \diffq 4bits				    & 66.8         & 1.69 & 77.5 & 5.39  & 76.9   & 17.53  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{table*}


\subsection{EfficientNet-b3 on ImageNet}
\label{supp:imnet}
On Table~\ref{supp:imagenet} we report the results for training
EfficientNet-b3~\cite{tan2019efficientnet} on the ImageNet dataset, matching the results reported on Figure~\ref{fig:teaser}.

\begin{table}[t!]
\caption{Image classification results for the ImageNet benchmark. Results are presented for \diffq and QAT using 4 and 8 bits using the EfficientNet-b3 model~\citep{tan2019efficientnet}. We report Top-1 Accuracy (Acc.) together with Model Size (M.S.).}
\label{supp:imagenet}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cc}
\toprule
		& Top-1 Acc. (\%)~ & M.S. (MB)~ \\
\midrule
Uncompressed    	                & 81.6 		   & 46.7 \\
\midrule
QAT 4bits    		                &  57.3        & 6.3  \\
QAT 8bits    		                & 81.3         & 12.0 \\
PQ ~\citep{fan2020training} & 80.0 & \textbf{3.1} \\
\midrule
\diffq ()  	                & 80.8      & 6.0 \\
\diffq ()    	    & \textbf{81.5}      & 8.7  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{figure}[t!]
\centering 
\subfigure[]{\label{fig:gs}\includegraphics[width=0.45\columnwidth]{figs/gs_ab.pdf}}
\subfigure[]{\label{fig:teaser}\includegraphics[width=0.48\columnwidth]{figs/imagenet_updated.pdf}}
\caption{\textbf{(a)}: \diffq results with various groups sizes ().  refers to a single group for the entire layer. For reference, we report the accuracy of the uncompressed model (42.8 MB). 
Models are Resnet-18 trained on CIFAR-100. 
\textbf{(b)}: ImageNet results using EfficientNet-B3 model. We plot the model size vs. model accuracy using different penalty levels. We additionally, present the uncompressed models (uncomp.) and Quantization Aware Training (QAT) using 4 and 8 bits.}
\label{fig:supp}
\end{figure}



As previously, we selected two versions of \diffq, one matching the size of QAT 8bits, and one smallest than QAT 4 bits. At 8 bits, \diffq achieves the same accuracy as the uncompressed baseline, for a slightly smaller model than QAT 8bits. As we lower the number of bits, we again see a clear advantage for \diffq, with both a smaller model (5.7MB against 6.1MB) than QAT 4bits, and significantly higher accuracy (76.8\% vs. 57.3\%).



The lower accuracy for QAT4 on ImageNet led us to take a closer look at the model performance. Figure~\ref{fig:supp} (b) depicts the model accuracy as a function of the number of epochs for both QAT4 and \diffq. Notice, similarly to the toy example presented in Section~\ref{sec:counter} training with QAT4 creates instability in the model optimization (especially near model convergence), which leads to significant differences in performance across adjacent epochs. When considering \diffq, model optimization is stable and no such differences are observed. 







\subsection{Activation Quantization for Language Modeling}

In Table~\ref{tab:app_text} we report language modeling results for a 16-layers Transformer models while applying activation quantization. Unlike the results in Table~\ref{tab:text} where we used per-channel activation quantization, here we report results with a histogram quantizer. Additionally when considering histogram quantizer, results suggest \diffq is superior to both QAT and QN when considering both model size and model performance.
\begin{table}[t!]
\caption{
Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization using a histogram quantizer. We compared \diffq to QAT and Quant-Noise (QN) method proposed by~\citet{fan2020training} (models with  were trained with layer drop of 0.2~\cite{fan2019reducing}, and 0.1 for the others.).}
\label{tab:app_text}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{llcc}
\toprule
	Weights & Activation	 & PPL~ & M. S. (MB)~ \\
\midrule
Uncompressed (Ours) & -  	   & 18.1 & 942\\
QAT 8bits    & -	& 18.2           & 236 \\
QAT 4bits   & - 	&   28.8         & 118 \\
\diffq  () & - & \textbf{18.0}          & 182 \\
\diffq  () & - & 18.5          & \textbf{113} \\
\midrule
    8 bits & 8 bits    & 19.5  & 236\\
QAT 8bits & 8 bits   	& 26.0  & 236 \\
QAT 4bits & 8 bits    	&   34.6   & 118 \\
\diffq  () & 8 bits & 19.1 & 182\\
\diffq  () & 8 bits & 19.2 & \textbf{113}\\
\midrule
Uncompressed  & -  	      & 18.3 & 942\\
QN 8 bits    & QN 8 bits & \textbf{18.7} & 236 \\
QN 4 bits    & QN 8 bits & 20.5 & 118 \\ 
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\subsection{Uniform Noise vs. Gaussian Noise}
In Table~\ref{supp:noise} we provide an empirical comparison between uniform noise and Gaussian noise using ResNet-18 on CIFAR10 for \diffq. We found that using Gaussian noise acheives the same model size with better accuracy levels.

\begin{table}[t!]
\caption{Empirical comparison between uniform noise and Gaussian noise using ResNet-18 on CIFAR10 for \diffq.}
\label{supp:noise}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cc}
\toprule
		Noise distribution & Top-1 Acc. (\%)~ & M.S. (MB)~ \\
\midrule
Uniform  	    & 86.9      & 2.7 \\
Gaussian  	    & 93.6      & 2.7  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}
