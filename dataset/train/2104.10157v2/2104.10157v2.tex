
\section{Experiments}

In the following section, we evaluate our method and design experiments to answer the following questions:
\begin{itemize}
    \item Can we generate high-fidelity samples from complex video datasets?
    \item How do different architecture design choices for VQ-VAE and prior network affect performance?
\end{itemize}


\subsection{Training Details}
All image data is scaled to  before training. For VQ-VAE training, we use random restarts for embeddings, and codebook initialization by copying encoder latents as described in \cite{dhariwal2020jukebox}. In addition, we found VQ-VAE training to be more stable (less codebook collapse) when using Normalized MSE for the reconstruction loss, where MSE loss is divided by the variance of the dataset. For all datasets except UCF-101, we train on  videos of sequence length . For the transformer, we train Sparse Transformers~\cite{child2019generating} with local and strided attention across space-time. Exact architecture details and hyperparameters can be found in Appendix~\ref{appendix:arch_hyperparams}. We achieve all results with a maximum of 8 Quadro RTX 6000 GPUs (24 GB memory). 


\subsection{Moving MNIST}
For Moving MNIST, VQ-VAE downsamples input videos by a factor of 4 over space-time (64x total reduction), and contains two residual layers with no axial-attention. We use a codebook of  codes, each -dim embeddings. To learn the single-frame conditional prior, we train a conditional transformer with  hidden features,  heads,  layers, and a ResNet-18 single frame encoder. Fig~\ref{fig:moving_mnist} shows several different generated trajectories conditioned on a single frame.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/bair_reconstructions.pdf}
    \caption{VQ-VAE reconstructions for BAIR Robot Pushing. The original videos are contained in green boxes and reconstructions in blue. }
    \label{fig:bair_recon}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/bair_1frame_cond_samples.pdf}
    \caption{BAIR Robot Pushing samples from a single-frame conditioned VideoGPT model. Frames highlighting in red are conditioning frames. Although all videos follow the same starting frame, the samples eventually diverge to varied trajectories.}
    \label{fig:bair_pushing}

\end{figure*}

\begin{table}[h]
    
        \centering
    
        \caption{FVD on BAIR}
        \label{table:bair_fvd}
        \begin{tabular}{ll}
            \toprule
            Method\textsuperscript{3}                   & FVD ()                \\ \midrule
            SV2P                    &             \\
            LVT                     &            \\
            SAVP                    &             \\
            DVD-GAN-FP              &             \\
            \textbf{VideoGPT (ours)}          &             \\
            TrIVD-GAN-FP           &              \\
            Video Transformer        &  \\ \bottomrule
        \end{tabular}
\end{table}

\subsection{BAIR Robot Pushing}

For BAIR, VQ-VAE downsamples the inputs by a factor of 2x over each of height, width and time dimensions. The embedding in the latent space is a -dimensional vector, which is discretized through a codebook with  codes. We use  axial-attention residual blocks for the VQ-VAE encoder and a prior network with a hidden size of  and  layers.

Quantitatively, Table~\ref{table:bair_fvd}\footnote{SV2P~\citep{babaeizadeh2017stochastic}, SAVP~\citep{lee2018stochastic}, DVD-GAN-FP~\citep{clark2019adversarial}, Video Transformer~\citep{weissenborn2019scaling}, Latent Video Transformer (LVT)~\citep{rakhimov2020latent}, and TrIVD-GAN~\citep{luc2020transformation} are our baselines} shows FVD results on BAIR, comparing our method with prior work. Although our method does not achieve state of the art, it is able to produce very realistic samples competitive with the best performing GANs.

Qualitatively, Fig~\ref{fig:bair_recon} shows VQ-VAE reconstructions on BAIR. Fig~\ref{fig:bair_pushing} shows samples primed with a single frames. We can see that our method is able to generate realistically looking samples. In addition, we see that VideoGPT is able to sample different trajectories from the same initial frame, showing that it is not simply copying the dataset.  

\subsection{ViZDoom}
For ViZDoom, we use the same VQ-VAE and transformer architectures as for the BAIR dataset, with the exception that the transformer is trained without single-frame conditioning. We collect the training data by training a policy in each ViZDoom environment, and collecting rollouts of the final trained policies. The total dataset size consists of 1000 episodes of length 100 trajectories, split into an 8:1:1 train / validataion / test ratio. We experiment on the Health Gathering Supreme and Battle2 ViZDoom environments, training both unconditional and action-conditioned priors. VideoGPT is able to capture complex 3D camera movements and environment interactions. In addition, action-conditioned samples are visually consistent with the input action sequence and show a diverse range of backgrounds and scenarios under different random generations for the same set of actions. Samples can be found in Appendix~\ref{appendix:vizdoom_samples}.\footnote{VGAN~\citep{vondrick2016generating}, TGAN~\cite{saito2017temporal}, MoCoGAN~\citep{tulyakov2018mocogan}, Progressive VGAN~\cite{acharya2018towards}, TGAN-F~\citep{kahembwe2020lower}, TGANv2~\cite{saito2018tganv2}, DVD-GAN~\cite{clark2019adversarial} are our baselines for IS on UCF-101.}

\begin{table}[h]
            \centering
        
        \caption{IS on UCF-101}
        \label{table:ucf_is}
\begin{tabular}{@{}cc@{}}
\toprule
Method\textsuperscript{4} & IS ()                      \\ \midrule
VGAN                      &                       \\
TGAN                      &                      \\
MoCoGAN                   &                      \\
Progressive VGAN          &                      \\
TGAN-F                    & \multicolumn{1}{l}{} \\
\textbf{VideoGPT (ours)}  & \multicolumn{1}{l}{} \\
TGANv2                    & \multicolumn{1}{l}{} \\
DVD-GAN                   & \multicolumn{1}{l}{}  \\ \bottomrule
\end{tabular}
\end{table}

\subsection{UCF-101}
UCF-101~\citep{soomro2012ucf101} is an action classification dataset with 13,320 videos from 101 different classes. We train unconditional VideoGPT models on  frame  and  videos, where the original videos have their shorter side scaled to  pixels, and then center cropped. 

Table~\ref{table:ucf_is} shows results comparing Inception Score\footnote{Inception Score is calculated using the code at \url{https://github.com/pfnet-research/tgan2}} (IS)~\citep{salimans2016improved} calculations against various baselines. Unconditionally generated samples are shown in Figure~\ref{fig:ucf}. Similarly observed in~\cite{clark2019adversarial}, we notice that that VideoGPT easily overfits UCF-101 with a train loss of  and test loss of , suggesting that UCF-101 may be too small a dataset of the relative complexity of the data itself, and more exploration would be needed on larger datasets.

\subsection{Tumblr GIF (TGIF)}
TGIF~\citep{li2016tgif} is a dataset of 103,068 selected GIFs from Tumblr, totalling around 100,000 hours of video. Figure~\ref{fig:tgif} shows samples from a trained unconditional VideoGPT model. We see that the video sample generations are able to capture complex interactions, such as camera movement, scene changes, and human and object dynamics. Unlike UCF-101, VideoGPT did not overfit on TGIF with a train loss of  and test loss .

\begin{figure*}[ht]
\begin{minipage}{\textwidth}
        \centering
    \includegraphics[width=\textwidth]{figures/ucf_samples.pdf}
    \caption{ UCF-101 unconditional samples}
    \label{fig:ucf}
\end{minipage}
\begin{minipage}{\textwidth}
        \centering
    \includegraphics[width=\textwidth]{figures/tgif_samples.pdf}
    \caption{ TGIF unconditional samples}
    \label{fig:tgif}
\end{minipage}


\end{figure*}

\subsection{Ablations}
In this section, we perform ablations on various architectural design choices for VideoGPT.

\textbf{Axial-attention in VQ-VAE increases reconstruction and generation quality.}
\begin{table}[H]
    \begin{minipage}{.5\textwidth}
        \centering

        \caption{Ablation on attention in VQ-VAE. R-FVD is with reconstructed examples}
        \label{table:abl_vqvae_attn}
        \begin{tabular}{@{}lll@{}}
            \toprule
            VQ-VAE Architecture & NMSE ()       & R-FVD ()   \\ \midrule
            No Attention        & 0.0041      & 15.3  \\
            With Attention      &  &  \\ \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}
We compare VQ-VAE with and without axial attention blocks as shown in Table~\ref{table:abl_vqvae_attn}. Empirically, incorporating axial attention into the VQ-VAE architecture improves reconstruction (NMSE) performance, and has better reconstruction FVD. Note that in order to take into account the added parameter count from attention layers, we increase the number of convolutional residual blocks in the "No Attention" version for better comparability. Fig~\ref{fig:bair_recon} shows samples of videos reconstructed by VQ-VAE with axial attention module. 

\textbf{Larger prior network capacity increases performance.}
\begin{table}[H]
        \centering
        \caption{Ablations comparing the number of transformer layers}
\label{table:abl_tfm_layers}
\begin{tabular}{@{}cccc@{}}
\toprule
Transformer Layers & bits/dim & FVD ()  \\ \midrule
2                  &    &       \\
4                  &    &        \\
8                  &    &        \\
16                 &    &        \\ \bottomrule
\end{tabular}
\end{table}
Computational efficiency is a primary advantage of our method, where we can first use the VQ-VAE to downsample by space-time before learning an autoregressive prior. Lower resolution latents allow us to train a larger and more expressive prior network to learn complex data distributions under memory constraints. We run an ablation on the prior network size which shows that a larger transformer network produces better results. Table~\ref{table:abl_tfm_layers} shows the results of training transformers of varied number of layers on BAIR. We can see that for BAIR, our method benefits from training larger models, where the bits per dim shows substantial improvement in increasing layers, and FVD and sample quality show increments in performance up until around 8 layers. 

\textbf{A balanced temporal-spatial downsampling in VQ-VAE latent space increase performance.}
\begin{table}[H]
\begin{minipage}{.5\textwidth}
\center
\caption{Ablations comparing different VideoGPT latent sizes on BAIR. R-FVD is the FVD of VQ-VAE reconstructions, and FVD* is the FVD between samples generated by VideoGPT and samples encoded-decoded from VQ-VAE. For each partition below, the total number of latents is the same with varying amounts of spatio-temporal downsampling}
\label{table:abl_latent_size}
\begin{tabular}{@{}cccc@{}}
\toprule
Latent Size                      & R-FVD () & FVD ()       & FVD* ()     \\ \midrule
          &                &           &           \\
            &               &           &           \\ \midrule
            &                &           &           \\ \midrule
          &                &           &           \\
          &                &           &           \\
         &                &           &           \\ \midrule
          &                &           &          \\
 &       &  &  \\ \midrule
          &                &           &          \\
         &                &           &          \\ \bottomrule
\end{tabular}
\end{minipage}

\end{table}

A larger downsampling ratio results in a smaller latent code size, which allows us to train larger and more expressive prior models. However, limiting the expressivity of the discrete latent codes may introduce a bottleneck that results in poor VQ-VAE reconstruction and sample quality. Thus, VideoGPT presents an inherent trade-off between the size of the latents, and the allowed capacity of prior network. Table~\ref{table:abl_latent_size} shows FVD results from training VideoGPT on varying latent sizes for BAIR. We can see that larger latents sizes have better reconstruction quality (lower R-FVD), however, the largest latents  does not perform the best sample-quality-wise due to limited compute constraints on prior model size. On the other hand, the smallest set of latents  and  have poor reconstruction quality and poor samples. There is a sweet-spot in the trade-off at around  where we observe the best sample quality.

In addition to looking at the total number of latents, we also investigate the appropriate downsampling for each latent resolution. Each partition in Table~\ref{table:abl_latent_size} shows latent sizes with the same number of total latents, each with different spatio-temporal downsampling allocations. Unsurprisingly, we find that a balance of downsampling ratio (, corresponding to latent size ) between space and time is the best, as opposed to downsampling over only space or only time. 

\textbf{Further increasing the number of latent codes does not affect performance.}
\begin{table}[H]
\begin{minipage}{.5\textwidth}
\centering
\caption{Ablations comparing the number of codebook codes}
\label{table:abl_n_codes}
\begin{tabular}{@{}cccc@{}}
\toprule
\# of Codes & R-FVD () & FVD () & bits/dim \\ \midrule
256         &                &     &       \\
1024        &                &     &       \\
4096        &                &     &       \\ \bottomrule
\end{tabular}
\end{minipage}
\end{table}
In Table~\ref{table:abl_n_codes}, we show experimental results for running VideoGPT with different number of codes in the codebooks. For all three runs, the VQ-VAE latent code vector has size . In the case of BAIR, we find that reconstruction quality improves with increasing the number of codes due to better expressivity in the discrete bottleneck. However, they ultimately do not affect sample quality. This may be due to the fact that in the case of BAIR, using 256 codes surpasses a base threshold for generation quality.

\textbf{Using one VQ-VAE codebook instead of multiple improves performance.}
\begin{table}[H]
\caption{Ablations comparing the number of codebooks}
\label{table:abl_n_codebooks}
\begin{tabular}{@{}cccc@{}}
\toprule
Latent Size                    & R-FVD () & FVD () & bits/dim \\ \midrule
    &                &     &       \\
 &                &                    &                 \\
 &                &     &      \\
 &                &     &      \\ \bottomrule
\end{tabular}
\end{table}
In our main results, we use one codebook for VQ-VAE. In Table~\ref{table:abl_n_codebooks}, we compare VideoGPT with different number of codebooks. Specifically, multiple codebooks is implemented by multiplying VQ-VAE's encode output channel dimension by  times, where  is the number of codebooks. The encoder output is then sliced along channel dimension, and each slice is quantized through a separate codebook. As a result, the size of the discrete latents are of dimension , as opposed to  when using a single codebook. Generally, multiple codebooks may be more favorable over increasing the downsampling resolution as multiple codebooks allows a combinatorially better scaling in bottleneck complexity. In our experiments, we increase the number of codebooks, and reduce spatio-temporal resolutions on latent sizes to keep the size of the latent space constant. We see that increasing the number of codebooks worsens sample quality performance, and the best results are attained at the highest resolution with one codebook. Nevertheless, incorporating multiple codebooks might shows its advantage when trained with a larger dataset or a different VQ-VAE architecture design. 