

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2020}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{mathrsfs, euscript}
\usepackage{color}
\usepackage{xcolor}
\usepackage{nccmath}
\usepackage{bm}




\icmltitlerunning{Continuously Indexed Domain Adaptation}

\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\crlref}[1]{Corollary~\ref{#1}}
\newcommand{\eqnref}[1]{Eqn.~\ref{#1}}

\newcommand{\hh}[1]{\textcolor{blue}{#1}}
\newcommand{\hw}[1]{\textcolor{red}{#1}}
\newcommand{\dk}[1]{\textcolor{purple}{#1}}
\newcommand{\bx} {{\bm x}}
\newcommand{\rv}[1]{\textcolor{blue}{#1}}

\newenvironment{Itemize}{\begin{itemize}\setlength{\itemsep}{0pt}\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}\setlength{\parskip}{0pt}}{\end{itemize}}
\setlength{\leftmargini}{10pt}



\begin{document}

\twocolumn[
\icmltitle{Continuously Indexed Domain Adaptation}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao Wang}{equal,mit}
\icmlauthor{Hao He}{equal,mit}
\icmlauthor{Dina Katabi}{mit}
\end{icmlauthorlist}

\icmlaffiliation{mit}{MIT Computer Science and Artificial Intelligence Laboratory, Massachusetts, USA}

\icmlcorrespondingauthor{Hao Wang}{hoguewang@gmail.com}


\icmlkeywords{Domain Adaptation, Deep Learning, Machine Learning}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\def\Blue{\color{blue}}
\def\Purple{\color{purple}}




\def\A{{\bf A}}
\def\a{{\bf a}}
\def\B{{\bf B}}
\def\b{{\bf b}}
\def\C{{\bf C}}
\def\c{{\bf c}}
\def\D{{\bf D}}
\def\d{{\bf d}}
\def\E{{\bf E}}
\def\e{{\bf e}}
\def\f{{\bf f}}
\def\F{{\bf F}}
\def\K{{\bf K}}
\def\k{{\bf k}}
\def\L{{\bf L}}
\def\H{{\bf H}}
\def\h{{\bf h}}
\def\G{{\bf G}}
\def\g{{\bf g}}
\def\I{{\bf I}}
\def\J{{\bf J}}
\def\R{{\bf R}}
\def\X{{\bf X}}
\def\Y{{\bf Y}}
\def\OO{{\bf O}}
\def\oo{{\bf o}}
\def\P{{\bf P}}
\def\p{{\bf p}}
\def\Q{{\bf Q}}
\def\q{{\bf q}}
\def\r{{\bf r}}
\def\s{{\bf s}}
\def\S{{\bf S}}
\def\t{{\bf t}}
\def\T{{\bf T}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\z{{\bf z}}
\def\Z{{\bf Z}}
\def\M{{\bf M}}
\def\m{{\bf m}}
\def\n{{\bf n}}
\def\U{{\bf U}}
\def\u{{\bf u}}
\def\V{{\bf V}}
\def\v{{\bf v}}
\def\W{{\bf W}}
\def\w{{\bf w}}
\def\0{{\bf 0}}
\def\1{{\bf 1}}

\def\AM{{\mathcal A}}
\def\EM{{\mathcal E}}
\def\FM{{\mathcal F}}
\def\TM{{\mathcal T}}
\def\UM{{\mathcal U}}
\def\XM{{\mathcal X}}
\def\YM{{\mathcal Y}}
\def\NM{{\mathcal N}}
\def\OM{{\mathcal O}}
\def\IM{{\mathcal I}}
\def\GM{{\mathcal G}}
\def\PM{{\mathcal P}}
\def\LM{{\mathcal L}}
\def\MM{{\mathcal M}}
\def\DM{{\mathcal D}}
\def\SM{{\mathcal S}}
\def\ZM{{\mathcal Z}}
\def\RB{{\mathbb R}}
\def\EB{{\mathbb E}}
\def\VB{{\mathbb V}}


\def\tx{\tilde{\bf x}}
\def\ty{\tilde{\bf y}}
\def\tz{\tilde{\bf z}}
\def\hd{\hat{d}}
\def\HD{\hat{\bf D}}
\def\hx{\hat{\bf x}}
\def\hR{\hat{R}}


\def\Ome{\mbox{\boldmath\unboldmath}}
\def\Om{\mbox{\boldmath\unboldmath}}
\def\bet{\mbox{\boldmath\unboldmath}}
\def\et{\mbox{\boldmath\unboldmath}}
\def\ep{\mbox{\boldmath\unboldmath}}
\def\ph{\mbox{\boldmath\unboldmath}}
\def\Pii{\mbox{\boldmath\unboldmath}}
\def\pii{\mbox{\boldmath\unboldmath}}
\def\Ph{\mbox{\boldmath\unboldmath}}
\def\Ps{\mbox{\boldmath\unboldmath}}
\def\tha{\mbox{\boldmath\unboldmath}}
\def\Tha{\mbox{\boldmath\unboldmath}}
\def\muu{\mbox{\boldmath\unboldmath}}
\def\Si{\mbox{\boldmath\unboldmath}}
\def\si{\mbox{\boldmath\unboldmath}}
\def\Gam{\mbox{\boldmath\unboldmath}}
\def\gamm{\mbox{\boldmath\unboldmath}}
\def\Lam{\mbox{\boldmath\unboldmath}}
\def\De{\mbox{\boldmath\unboldmath}}
\def\vps{\mbox{\boldmath\unboldmath}}
\def\Up{\mbox{\boldmath\unboldmath}}
\def\xii{\mbox{\boldmath\unboldmath}}
\def\Xii{\mbox{\boldmath\unboldmath}}
\def\Lap{\mbox{\boldmath\unboldmath}}
\newcommand{\ti}[1]{\tilde{#1}}

\def\tr{\mathrm{tr}}
\def\etr{\mathrm{etr}}
\def\etal{{\em et al.\/}\,}
\newcommand{\indep}{{\;\bot\!\!\!\!\!\!\bot\;}}
\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\vec{\text{vec}}
\def\cov{\text{cov}}
\def\dg{\text{diag}}

\newtheorem{observation}{\textbf{Observation}}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
\numberwithin{theorem}{section}
\numberwithin{lemma}{section}
\numberwithin{remark}{section}
\numberwithin{cor}{section}
 
\begin{abstract}
Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B).  However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains. 
In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the state-of-the-art domain adaption methods on both synthetic and real-world medical datasets\footnote{Code will soon be available at \url{https://github.com/hehaodele/CIDA}}.
\end{abstract}
 \section{Introduction}
Machine learning often assumes that training and test data come from the same distribution, so that the trained model generalizes well to the test scenario. This assumption breaks however when the model is trained and tested in distinct domains, i.e., different source and target domains.  Domain adaption (DA) leverages labeled data from the source domains and unlabeled data (or a limited amount of labeled data) from the target domains to significantly improve performance~\cite{bendavid,DANN,ADDA,MDD}.

Existing DA methods however focus on adaption among categorical domains where the domain index is just a label.  A common example would be to adapt a model from one image dataset to another, e.g., adapting from MNIST to SVHN. However, many real-world tasks require adaptation among continuously index domains. For example, in medical applications, one needs to adapt disease diagnosis and prognosis across patients of different ages, where age is a continuous domain index. Treating the age of the source and target domains as domain labels is unlikely to yield the best results because it does not take advantage of the relationship between the disease manifestation and the person's age. Similar issues appear in robotics. For example, underwater robots have to operate at different water depths and viscosity, and one expects that adaptation across datasets from different depths or viscosity (e.g., lake vs. sea) should take into account  the relationship between the robot operation and the physical properties of the liquid in which it operates. These examples highlight the limitations of current DA methods when applied to continuously indexed domains.  

So, how should we perform domain adaption across continuously indexed domains? We note that in the above examples the domain index plays the role of a distance metric -- i.e., it captures a similarity distance between the domains with respect to the task. Thus, one approach for addressing the problem is to modify traditional adversarial adaptation to make the discriminator regress the domain index using a distance-based loss, like the  or  loss. Although this is better than categorical DA, we show analytically that such treatment can lead to equilibriums with relatively poor domain alignments.  A better solution is to develop a probabilistic discriminator that models the domain index distribution. We show that such a discriminator not only successfully captures the underlying relation among domains, but also enjoys better theoretical guarantees in terms of domain alignment. 
We also note that our method can be naturally generalized to handle multi-dimensional domain indices, achieving further performance gain. For example, in medial applications the index can be a vector of age, blood pressure, activity level, etc. 



Our contributions are as follows:
\begin{itemize}
\item We identify the problem of adaptation across continuously indexed domains and propose continuously indexed domain adaptation (CIDA) as the first general DA method for addressing this problem. Further, we analyze our method and provide theoretical guarantees that CIDA aligns continuously indexed domains at equilibrium. 
\item We derive two advanced versions, probabilistic CIDA and multi-dimensional CIDA, to further improve performance and handle multi-dimensional domain indices, with minimal overhead.
\item 
We provide empirical results using both synthetic and real-world medical datasets which show that CIDA and its probabilistic and multi-dimensional variants significantly improve performance over the state-of-the-art DA methods for continuously indexed domains. 
\end{itemize}


 \section{Related Work}\label{sec:related}
\textbf{Adversarial Domain Adaptation.} Much prior work has focused on the problem of domain adaptation~\cite{CDANN,CDAN,MCD,GTA,MDD}. The key idea is to match the distributions of the source and target domains. This is achieved by matching their distributions' statistics either directly~\cite{MMD,DDC,CORAL} or with the help of an adversarial loss~\cite{DANN,CDANN,ADDA,MDD,UDA-SGD}. Adversarial domain adaptation is particularly popular due to its relatively strong theoretical insights~\cite{GAN,AMSDA,MDD,InvariantDA} and its compatibility with neural networks. It aligns the distributions of the source and target domains by generating an encoding indistinguishable from a perspective of discriminator that is trained to classify the domain of the data. In this paper, we build on adversarial domain adaptation and extend it to address continuously indexed domains. 

\textbf{Incremental Domain Adaptation.} Closest to our work are incremental DA approaches. Essentially they assume the domain shifts smoothly over time and try to incrementally adapt the source domain to multiple target domains. Different methods are used to perform categorical DA for each domain pair, such as optimal transport~\cite{CDOT}, adversarial loss~\cite{bitarafan2016incremental}, generative adversarial networks~\cite{wulfmeier2018incremental}, and linear transform~\cite{hoffman2014continuous}.  \Citet{CUA} notices such incremental adaptation procedure is prone to catastrophic forgetting, a tendency to forget the knowledge of previous domains while specializing to a new domain, and therefore proposes a replay technique to tackle the issue. Here we note several key differences between CIDA and the methods above. (1) These approaches incrementally perform pair-wise \emph{categorical DA}. Hence failure in adapting one domain pair can lead to catastrophic failures for all following pairs. (2) They only work on DA tasks with one single domain shifting dimension (usually `time'), while our method naturally generalizes to multi-dimensional settings. Such differences are empirically verified in \secref{sec:experiment}. 


 \section{Methods}
\label{sec:method}
In this section, we formalize the problem of adaptation among continuously indexed domains, and describe our methods for addressing the problem. We then provide theoretical guarantees for the proposed methods in \secref{sec:theory}.


\textbf{Problem.} We consider the unsupervised domain adaptation setting and assume a set of continuous domain indices , where  and  are the domain index sets for the source and the target domains, and  is part of a metric space (i.e., a metric like the Euclidian distance is defined over the set). The input and labels are denoted as  and , respectively. With access to the labeled data  from source domains () and unlabeled data  from target domains (), the goal is to accurately predict the labels  for data in the target domains. 

\textbf{Multi-Dimensional Domain Indices.} 
For clarity, we introduce our methods and theory in the context of unidimensional domain indices. However, they can directly apply to multi-dimensional domain indices. Later in \secref{sec:experiment}, we show that the ability of handling multi-dimensional domain indices brings further performance gains.


\subsection{Continuously Indexed Domain Adaptation (CIDA)}\label{sec:cida}
To perform adaptation across a continuous range of domains, we leverage the idea of learning domain-invariant encodings with adversarial training. We propose to learn an encoder\footnote{In general the encoder  can be probabilistic. For example,  can be generated from a Gaussian distribution whose mean and variance are given by .}  and a predictor  such that the distribution of the encodings  (or ) from all domains  are aligned so that all labels can be accurately predicted by the shared predictor . 
Formally, domain-invariant encodings require that . It implies that  and  are independent (), i.e.,  or equivalently . This is achieved with the help of a discriminator . In continuously indexed domains however, small changes in  should lead to small changes in the encoding. Thus, instead of classifying the encoding into categorical domains, the discriminator  in CIDA regresses the domain index. 

Formally, CIDA performs a minimax optimization with the value function  as:

where we have

where  and  denote the expectations taken over the entire data distribution  and the source data distribution . Note that the label  is only accessible in the source domains.  is the prediction loss (e.g., cross-entropy loss for classification tasks), and  is the domain index loss.  is a hyperparameter balancing both losses. The main difference between CIDA and traditional adversarial domain adaptation is that the discriminator loss  is a monotonic function of the metric defined over . 








\subsection{Variants of CIDA}\label{sec:pcida}
Note that there can be various designs for both  and . For example,  can either directly predict the domain index or predict its mean and variance, and  can be either the  or  loss. Different designs come with different theoretical guarantees. 

\textbf{Vanilla CIDA.} In the vanilla CIDA,  directly predicts the domain index, and correspondingly  is the  loss between the predicted and ground-truth domain index,

Vanilla CIDA above only guarantees matching the mean of the distribution  (see theoretical results in \secref{sec:theory}). 

Therefore in the following, we introduce an advanced version, dubbed probabilistic CIDA (PCIDA), which enjoys better theoretical guarantees to match both the mean and variance of the distribution . We note that PCIDA can be extended to match higher-order moments.

\textbf{Probabilistic CIDA.} The major improvement from CIDA to PCIDA is that in PCIDA, the discriminator predicts the distribution of  instead of providing point estimation. We start with the simplest probabilistic model, Gaussian distributions, where the discriminator  outputs the mean and variance of  as  and , respectively. To train such a discriminator, we use the negative log-likelihood as the loss function:

where . 

\textbf{Extension to Gaussian Mixture Models.} PCIDA can be naturally extended from a single Gaussian to a Gaussian mixture model (GMM) by using a mixture density network as the discriminator ~\cite{MDN} and the corresponding negative log-likelihood as .  \section{Theoretical Results}\label{sec:theory}
In this section, we provide theoretical guarantees for CIDA and PCIDA. As standard in adversarial domain adaption, we analyze a game in which the encoder aims to fool the discriminator and prevent it from inferring the domain index.  We first analyze a simplified game between the encoder and the discriminator (without the predictor) to gain insight of the aligned encodings. We then discuss the full three-player game and show our framework preserves the prediction power while aligning the encodings.

\subsection{Analysis for the Simplified Game}
We consider a simplified game which does not involve the predictor , defined by the  term in \eqnref{eq:full_game}:


We first analyze the equilibrium of the simplified game for CIDA.
Recall that, in CIDA, the discriminator  predicts the domain index  given the encoding  and the domain index loss  is the  loss. We show that in the equilibrium of CIDA, the encoder will align the mean of the conditional domain distribution  to the mean of the marginal domain distribution .

\lemref{lem:opt_dis_cida} below analyzes the discriminator  with the encoder  fixed and states that the optimal discriminator  outputs the mean domain index of all data with the same encoding . 
\begin{lemma}[\textbf{Optimal Discriminator for CIDA}]\label{lem:opt_dis_cida}
For E fixed, the optimal D is

where .
\end{lemma}
\begin{proof}
With E fixed, the optimal D

Notice that

is a quadratic form of  which achieves the minimum at .
\end{proof}


Assuming that  always achieves its optimum w.r.t  during the training, the minimax game in \eqnref{eq:simplified_game} can be reformulated as maximizing  where

where  denotes variance.

Next we analyze the virtual training criterion  for the encoder and derive the global optimum. 

\begin{lemma}[\textbf{Uniqueness of Constant Expectation}]\label{lem:unique_l2}
If there exists a constant  such that  for any , we have .
\end{lemma}

\begin{theorem}[\textbf{Global Optimum for CIDA}]\label{thm:iff_l2}
The global maximum of  is achieved if and only if the encoder  satisfies that the expectations of the domain index  over the conditional distribution  for any given  are identical to the expectation over the marginal distribution , i.e., .
\end{theorem}
\begin{proof}
We first show  and then show the equality is achieved when .
\begingroup\makeatletter\def\f@size{9}\check@mathfonts

\endgroup
By the convexity of  and Jensen's inequality, we have  and the equality is achieved when  is constant w.r.t. . By \lemref{lem:unique_l2} we have .
\end{proof}



As \thmref{thm:iff_l2} states, the vanilla CIDA using the  loss guarantees that the mean of the distribution  matches the mean of the marginal distribution .
It means that there is a risk the encoder  only aligns the mean of the distributions without exactly matching the entire distributions. However, surprisingly, we find that CIDA often achieves good empirical performance (see \secref{sec:experiment} for more details). Next, we analyze PCIDA and show that PCIDA enjoys better theoretical guarantees and matches both the mean and variance of the distribution .



Recall that in PCIDA, the discriminator  outputs the mean and variance of  as  and . We use the negative log-likelihood (\eqnref{eq:guass_loss}) as the domain loss . We start from analyzing the discriminator  when the encoder  is fixed. \lemref{lem:opt_dis_pcida} states that the optimal discriminator , given the encoding , will output the mean and variance of the domain index distribution  .

\begin{lemma}[\textbf{Optimal Discriminator for PCIDA}]\label{lem:opt_dis_pcida} With E fixed, the optimal D is

where , and .
\end{lemma}
Proof of \lemref{lem:opt_dis_pcida} is similar to that of \lemref{lem:opt_dis_cida} (see the Supplement for details).


Assuming discriminator  always reaches optimum, the virtual training criterion  for the encoder becomes:

Now we analyze  and provide PCIDA's global optimum.

\begin{lemma}[\textbf{Uniqueness of Constant Expectation and Variance}]\label{lem:unique}
If there exist constants  and  such that  and  for any , we have  and .
\end{lemma}


\begin{theorem}[\textbf{Global Optimum for PCIDA}]\label{thm:iff_gaussian}
In PCIDA (with the Gaussian model), the global optimum is achieved if and only if the mean and variance of the distribution  given any  are identical to those of the marginal distribution .
\end{theorem}

\begin{proof}
Given that

we analyze the upper bounds of the two terms separately.
For the first term, 
\begingroup\makeatletter\def\f@size{9}\check@mathfonts

\endgroup
For the second term, by the concavity of  and Jensen's inequality, we have that  the equality holds when  is constant w.r.t. . Further, in the proof of \thmref{thm:iff_l2}, we show that  and the maximum is achieved when  is constant w.r.t. . 
Together with \lemref{lem:unique}, we then have that  reaches the global optimal  if and only if  and  for all .
\end{proof}

\begin{cor}\label{thm:achieve_cida}
For both CIDA and PCIDA, the global optimum of  is achieved if the encoding of all domains (continuously indexed by ) are totally aligned, i.e., .
\end{cor}


\begin{remark}[\textbf{Matching Higher-Order Moments}]
By \thmref{thm:iff_l2} and \thmref{thm:iff_gaussian}, we show that CIDA using the  loss matches the mean of  while the PCIDA with the Gaussian model matches both the mean and variance of . Can we match higher-order moments? We believe our methodology can generalize to match higher-order moments by using PCIDA with more complex parametric probabilistic models. For example, one can use skew-normal distributions~\cite{azzalini2013skew} to match the third moment (skewness). 

\end{remark}



\subsection{Analysis of the Three-player Game}

We analyze the equilibrium state of the three-player game of  and  as defined in \eqnref{eq:full_game}. 
We divide the situation into two cases based on whether the domain index  is independent of the label .

\subsubsection{}
The domain index  is independent of the label  when it captures nuisance variations that are irrelevant to the task of predicting the label . In this case, we prove the following theorem showing that the optimal encoding captures all the information in the input  that is relevant to the predictive tasks while aligning the domain index distributions.

\begin{lemma}[\textbf{Optimal Predictor}]
\label{thm:opt_predictor}
Given the encoder , the prediction loss  where  is the entropy. The optimal predictor  that minimizes the prediction loss is .
\end{lemma}

Assuming the predictor  and the discriminator  are trained to achieve their optimal losses, by \lemref{thm:opt_predictor}, the three-player game (\eqnref{eq:full_game}) can be rewritten as following training procedure of the encoder ,
\vspace{-1.5mm}



\begin{theorem} \label{thm:full_game} If the encoder , the predictor  and the discriminator  have enough capacity and are trained to reach optimum, any global optimal encoder  has the
following properties:

	H(y|E^*(\x, u)) = H(y|\x, u) \label{optimal-encoder-a}\\
C_d(E^*) = \max_{E'} C_d(E') \label{optimal-encoder-b}

\end{theorem}
\begin{proof}
	Since  is a function of , by the data processing inequality, we have .

	
	Hence, . The equality holds if and only if  and . Therefore, we only need to prove that the optimal value of  is equal to  in order to prove that any global encoder  satisfies both \eqnref{optimal-encoder-a} and \eqnref{optimal-encoder-b}.
	
	We show that  can achieve  by considering the following encoder : . It can be examined that  and  which leads to  using \crlref{thm:achieve_cida}.
\end{proof}

\thmref{thm:full_game} shows that, at the equilibrium, the optimal encoder preserves all the information about label  contained in the data  and the domain index  while aligning the encoding cross domains.

Note that in general the encoder  is a probabilistic encoder that generates  stochastically. For example, one can use a probabilistic encoder parameterized by a natural-parameter network~\cite{NPN} and generate  using the reparameterization trick~\cite{VAE}. Empirically, we find that directly using a deterministic encoder also works favorably and therefore keep the encoder deterministic in~\secref{sec:experiment} for simplicity. 

\subsubsection{}
The domain index  is dependent of the label  when it contains information relevant to predicting .
In this case, discretization of the inherently continuous domain index  is necessary to perform categorical domain adaptation. However, this discretization inevitably loses information in  and could hurt the predictive task since . In contrast, our methods CIDA/PCIDA performs domain adaption with the continuous domain index , thus, can fully retain information in  that is relevant to the label .  \section{Experiments}\label{sec:experiment}
We evaluate CIDA and its variants on two toy datasets, one image dataset (\emph{Rotating MNIST}), and three real-world medical datasets. These empirical studies verify our theoretical findings in \secref{sec:method} and show that:
\begin{Itemize}
\item Using categorical domain adaption to align continuously indexed domains leads to poor alignment with marginal (or no) performance gain compared to no adaptation.\item CIDA aligns domains with continuous indices and achieves significant performance boost compared to categorical domain adaption methods.
\item PCIDA's ability to predict a distribution rather than a single value is helpful in avoiding bad equilibriums and improving prediction performance.
\item  The performance gains of CIDA and PCIDA increase with multi-dimensional domain indices.
\end{Itemize}


\begin{figure*}[!tb]
\vspace{-0pt}
\centering     \subfigure[Domains]{\includegraphics[width=0.23\textwidth]{figures/pred_cida_half_circle_domain.pdf}}
\subfigure[Ground Truth]{\includegraphics[width=0.23\textwidth]{figures/pred_cida_half_circle_gt.pdf}}
\subfigure[DANN]{\includegraphics[width=0.23\textwidth]{figures/pred_dann_half_circle_label.pdf}}
\subfigure[CDANN]{\includegraphics[width=0.23\textwidth]{figures/pred_zhao_half_circle_label.pdf}}
\vskip -0.3cm
\subfigure[ADDA]{\includegraphics[width=0.23\textwidth]{figures/pred_adda_half_circle_label.pdf}}
\subfigure[MDD]{\includegraphics[width=0.23\textwidth]{figures/pred_mdd_half_circle_label.pdf}}
\subfigure[CUA]{\includegraphics[width=0.23\textwidth]{figures/pred_cua_half_circle_label.pdf}}
\subfigure[CIDA (Ours)]{\includegraphics[width=0.23\textwidth]{figures/pred_cida_half_circle_label.pdf}}
\vspace{-12pt}
\caption{Results on the \emph{Circle} dataset with  domains. \figref{fig:toy-circle}(a) shows domain index by color. The first  domains are source domains, marked by green boxes. Red dots and blue crosses are positive and negative data samples. Black lines show the decision boundaries generated according to model predictions.}
\label{fig:toy-circle}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[!tb]
\vspace{-0pt}
\centering     \subfigure[Domains]{
\includegraphics[width=0.23\textwidth]{figures/pred_cida_sin_2period_domain.pdf}}
\subfigure[Ground Truth]{\includegraphics[width=0.23\textwidth]{figures/pred_cida_sin_2period_gt.pdf}}
\subfigure[DANN]{\includegraphics[width=0.23\textwidth]{figures/pred_dann_sin_2period_label.pdf}}
\subfigure[CDANN]{\includegraphics[width=0.23\textwidth]{figures/pred_zhao_sin_2period_label.pdf}}
\vskip -0.3cm
\subfigure[ADDA]{\includegraphics[width=0.23\textwidth]{figures/pred_adda_sin_2period_label.pdf}}
\subfigure[MDD]{\includegraphics[width=0.23\textwidth]{figures/pred_mdd_sin_2period_label.pdf}}
\subfigure[CUA]{\includegraphics[width=0.23\textwidth]{figures/pred_cua_sin_2period_label.pdf}}
\subfigure[CIDA (Ours) ]{\includegraphics[width=0.23\textwidth]{figures/pred_cida_sin_2period_label.pdf}}
\vspace{-12pt}
\caption{Results on the \emph{Sine} dataset with  domains. The first  domains are source domains marked by green boxes. Red dots and blue crosses are positive and negative data samples. Black lines show the decision boundaries generated according to model predictions.}
\label{fig:toy-sin}
\vskip -0.5cm
\end{figure*}



\subsection{Baselines and Implementations}
We compare variants of CIDA with state-of-the-art domain adaptation methods including Domain Adversarial Neural Network (\textbf{DANN})~\cite{DANN}, Conditional Domain Adversarial Neural Network (\textbf{CDANN})~\cite{CDANN}, Adversarial Discriminative Domain Adaptation
(\textbf{ADDA})~\cite{ADDA}, Margin Disparity
Discrepancy (\textbf{MDD})~\cite{MDD}, and Continuous Unsupervised Adaptation (\textbf{CUA})~\cite{CUA}.
ADDA and MDD merge data with different domain indices into one source and one target domains; DANN, CDANN, and CUA divide the continuous domain spectrum into several separate domains and perform adaptation between multiple source and target domains. CUA adapts from the source domains to each target domain one-by-one from the closest target to the farthest one.
For a fair comparison with CIDA, all baselines use both  and the domain index  as inputs to the encoder.


All methods are implemented using PyTorch~\cite{PyTorch} with the same neural network architecture.  is chosen from  and kept the same for all tasks associated with the same dataset (see the Supplement for more details about training).



\begin{table*}[!t]
\begin{footnotesize}
\vskip -0.3cm
\begin{center}
\caption{ \textbf{\emph{Rotating MNIST} accuracy (\%) for various adaptation methods.} We report the accuracy at the source domain and each target domain.  denotes the domain whose images are Rotating by  to . The last column shows the average accuracy across target domains. We use \textbf{bold face} to mark the best results.}
\label{tab:mnist}
\vspace{1mm}
\begin{tabular}{lcccccccccc}
\hline
Method & \# Target Domains &  (Source) &  &  &  &  &  &  &  & Average \\
\hline
Source-Only & - & 98.4 & 81.4 & 29.8 & 33.6 & 41.4 & 39.0 & 30.4 & 81.1 & 48.1 \\
ADDA        & 1 & 95.0 & 70.2 & 25.5 & 44.0 & 59.2 & 46.2 & 23.7 & 61.4 & 47.2 \\
DANN        & 1 & 98.1 & 80.1 & 44.8 & 42.2 & 43.6 & 46.8 & 57.3 & 79.3 & 56.3 \\
CUA         & 7 & 91.4 & 73.9 & 60.1 & 55.0 & 52.7 & 45.1 & 55.2 & 88.4 & 61.5 \\
CIDA (Ours) &  & \textbf{99.1} & 87.2 & 56.7 & 79.6 & 91.2 & \textbf{91.5} & \textbf{96.2} & \textbf{97.5} & 85.7 \\
PCIDA (Ours) &  & 98.6 & \textbf{90.1} & \textbf{82.2} & \textbf{90.5} & \textbf{91.9} & 87.1 & 80.0 & 88.2 & \textbf{87.1} \\
\hline
 \vspace{-10mm}
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}

\subsection{Toy Datasets}
To gain insight into the differences between CIDA and the baselines, we start with two toy datasets: \emph{Circle} and \emph{Sine}.


\textbf{Circle Dataset} includes  domains indexed from  to .
\figref{fig:toy-circle}(a) shows the  domains in different colors. We also use arrows to indicates domain  and domain . Each domain contains data on a circle. The task is binary classification. \figref{fig:toy-circle}(b) shows positive samples as red dots and negative samples as blue crosses. As shown in the figure, the ground-truth decision boundary continuously evolves with the domain index. We use domains  to  as source domains and the rest as target domains.
\figref{fig:toy-circle} compares the results of CIDA  with the baselines. The figure shows that overall categorical DA methods perform poorly when asked to align domains with continuous indices. CUA is the best performing baseline since it incrementally adapts  pairs of domains. Still, CUA's performance is inferior to CIDA which produces a more accurate decision boundary.





\textbf{Sine Dataset} includes  domains as shown in \figref{fig:toy-sin}(a). Each domain covers  the period of the sinosoid. We consider the first  domains as source domains and the rest as target domains.
\figref{fig:toy-sin} shows the results. The figure shows that it is very challenging for the baselines to capture the ground-truth decision boundary. The baselines either produce incorrect decision boundaries (ADDA and MDD) or only capture the correct trend with very rugged boundaries (DANN, CDANN and CUA). In contrast, CIDA can successfully recover the ground-truth boundary.
We also note that while CUA  performed better than the other baselines on the Circle dataset, it performed worse than DANN and CDANN on the Sine dataset. This is because CUA performs incremental pairwise adaptation; it fails on the pair , and this failure propagates to the following domains.

Overall, both the results from the Circle and Sine datasets demonstrate that CIDA captures the underlying relationship between the domain index and the classification task and leverages it to improve performance. In contrast, the baselines cannot accurately capture this relationship, and hence yield worse results.




\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{table*}[t]
\begin{footnotesize}
\vskip -0.3cm
\begin{center}
\caption{ \textbf{Accuracy (\%) for intra-dataset adaptation.} `\emph{SHHS}@Outside  \emph{SHHS}@(52,75]' means transferring from age range outside (52,75] (i.e., [44,52](75,90]) to (52,75] within \emph{SHHS}. `SO' is short for `Source-Only'. 
We use \textbf{bold face} mark the best results.
} 
\label{tab:intra}
\vspace{1mm}
\begin{tabular}{clcccccccc}
\hline
& Task  & SO  & ADDA & DANN & CDANN & MDD & CUA & CIDA & PCIDA \\
\hline
\multirow{3}{*}{\tabincell{c}{Domain\\Extrapolation}} & \emph{SHHS}@[44,52]  \emph{SHHS}@(52,90] & 77.4 & 78.0 & 77.1 & 77.5 & 77.7 & 77.4 & 79.8 & \textbf{80.6}  \\
 & \emph{MESA}@[54,58]  \emph{MESA}@(58,95]   & 80.1 & 80.7 & 79.9 & 80.4 & 80.3 & 80.1 & \textbf{82.7} & 82.5 \\
 & \emph{SOF}@[75,82]  \emph{SOF}@(82,90]   & 74.7 & 74.8 & 74.2 & 74.4 & 74.6 & 74.5 & \textbf{76.7} & \textbf{76.7} \\
\hline
\multirow{3}{*}{\tabincell{c}{Domain\\Interpolation}} & \emph{SHHS}@Outside  \emph{SHHS}@(52,75] & 82.4 & 81.7 & 82.5 & 82.3 & 82.5 & 82.4 & 82.2 & \textbf{83.7}  \\
& \emph{MESA}@Outside  \emph{MESA}@(58,75]   & 83.5 & 83.5 & 83.2 & 83.3 & 83.8 & 83.4 & 83.5 & \textbf{84.7} \\
& \emph{SOF}@Outside  \emph{SOF}@(79,86]   & 71.8 & 71.5 & 71.4 & 70.9 & 71.8 & 71.5 & 71.8 & \textbf{73.6}  \\
 \hline
\end{tabular}
\end{center}
\vskip -0.7cm
\end{footnotesize}
\end{table*} \begin{table*}[t]
\begin{footnotesize}
\begin{center}
\caption{ \textbf{Accuracy (\%) for cross-dataset adaptation.} We use \textbf{bold face} to mark the best results.}
\label{tab:cross}
\vspace{1mm}
\begin{tabular}{lcccccccc}
\hline
Task  & Source-Only  & ADDA & DANN & CDANN & MDD & CUA & CIDA & PCIDA \\
\hline
 \emph{SOF}  \emph{SHHS}
 & 75.6 & {76.0} & 75.2 & 75.6 & 75.8 & 75.3 & 75.9 & \textbf{80.1}  \\
  \emph{SOF}  \emph{MESA}
 & 74.0 & 75.1 & 74.6 & {75.2} & 74.9 & 73.6 & 74.8 & \textbf{80.0}  \\
  \emph{SHHS}  \emph{MESA}
 & 82.8& 83.0 & 82.6 & 82.1 & 83.0 & 82.1 & {83.2} & \textbf{85.3}  \\
  \emph{MESA}  \emph{SHHS}
 & 80.7 & {81.8} & 80.9 & 80.9 & 81.2 & 81.0 & 80.8 & \textbf{83.4}  \\
 \emph{SHHS}  \emph{SOF}
 & 78.7 & 79.5 & 79.0 & 79.2 & 79.7 & 79.1 & \textbf{81.1} & {80.9}  \\
 \emph{MESA}  \emph{SOF}
 & 75.9 & 76.6 & 77.0 & 76.9 & 76.9 & 76.0 & \textbf{79.3} & {79.0}  \\
 \hline
\end{tabular}
\end{center}
\vskip -0.7cm
\end{footnotesize}
\end{table*}


\subsection{Rotating MNIST}
We further evaluate our methods on the \emph{Rotating MNIST} dataset. The goal is to adapt from regular MNIST digits with mild rotation to significantly Rotating MNIST digits. We designate images that are Rotating by  to  as the labeled source domain, and assign images Rotating by  to  to the target domains. Naturally, the domain index is the rotation angle of the image. Since the target domain has a much larger range of rotation angles, we split the target domain into seven target domains for categorical domain adaptation baselines, DANN and CUA. These seven target domains contain images Rotating by  degrees, respectively. \tabref{tab:mnist} compares the accuracy our proposed CIDA/PCIDA with different baselines.
We can see ADDA and DANN hardly improve and sometimes even decrease the accuracy compared to not performing adaptation at all. This is because without capturing the underlying structure, adversarial encoding alignment may harm the transferability of the data. CUA's performs fairly well in target domains near source domains but poorly in distant domains.\footnote{We note that CUA's performance on our Rotating MNIST data is worse than in the original papers, possibly because our Rotating MNIST has images rotated by all angles as opposed to only  fixed angles. Also we are using different model architectures. Please refer to the Supplement for details.} On the other hand, CIDA and PCIDA can learn such domain structure and successfully adapt the knowledge from source domains to target domains (see the Supplement for more details such as model architectures).



\subsection{Healthcare Datasets}
\textbf{Dataset Description.}
We use three medical datasets, Sleep Heart Health Study (\emph{SHHS})~\cite{SHHS}, Multi-Ethnic Study of Atherosclerosis (\emph{MESA})~\cite{MESA} and Study of Osteoporotic Fractures (\emph{SOF})~\cite{SOF}. Each dataset contains full-night breathing signals of subjects and the corresponding sleep stage labels (`Awake', 'Light Sleep', `Deep Sleep', and `Rapid Eye Movement (REM)'). Breathing signals are split into 30-second segments with one label for each segment. We consider the task of sleep stage prediction, i.e., to predict the sleep stage label  given a breathing segment . This is a natural task in sleep studies and can be performed in the patient home by having them wearing a breathing belt. The breathing signal can then serve to predict sleep stages and also detect apnea (temporary cessation of breathing).

The datasets also contain subjects' information such as age, which is a natural domain index . \emph{SHHS}, \emph{MESA}, and \emph{SOF} include , , and  subjects, respectively. On average, there are  segments (i.e.,  hours of breathing signals) for each subject. Different datasets have different domain index distributions. For example, subjects' age range in \emph{SHHS} is , while the age ranges for \emph{MESA} and \emph{SOF} are  and , respectively. Apparently \emph{SOF} subjects are much older. \emph{SHHS} subjects and \emph{MESA} subjects have similar age ranges but the distributions are actually different (see the histogram plot in the Supplement).


\textbf{Intra-Dataset Adaptation.} We first evaluate our methods' performance on adaptation across continuously indexed domains within the same dataset using `age' as the domain index. We cover two cases:
\begin{Itemize}
\item \textbf{Domain Extrapolation.} For example, the source domain has data with a domain index (age) from the range [44,52], while the target domain contains data with a domain index range of (52,90]. 
\item \textbf{Domain Interpolation.} For example, in the source domain, the domain index range is [44,52](75,90], while in the target domain, the domain index range is (52,75]. 
\end{Itemize}


The first three rows of \tabref{tab:intra} show the results for domain extrapolation. One observation is that directly using categorical domain adaptation only achieves minimal performance boost compared to models trained only on the source domains (Source-Only). Some methods such as DANN and CUA achieve no or even negative performance improvement. On the other hand, CIDA variants can successfully transfer across subjects with different ages and significantly improve upon all baselines. Similarly, the last three rows in \tabref{tab:intra} show the results for domain interpolation. Note that Source-Only can already achieve satisfactory accuracy in domain interpolation, since the model naturally learns the average of data from both sides (e.g., [44,52](75,90]) and performs prediction for the data in the middle (e.g., (52,75]). For example, in the task `\emph{SHHS}Outside  \emph{SHHS}@(52,75]', Source-Only already has a high accuracy of , leaving little room for improvement. Interestingly, PCIDA can still further improve the accuracy by a tangible margin. This also shows PCIDA's ability to avoid bad equilibriums by using a discriminator that predicts distributions rather than values.

\begin{table}[t]
\begin{footnotesize}
\vskip -2mm
\begin{center}
\caption{\textbf{Accuracy (\%) for the multi-dimensional domain index setting.} The task is \emph{SHHS}@[44,52]  \emph{SHHS}@(52,90].}
\label{tab:multi}
\vskip 1mm
\begin{tabular}{cccc}
\hline
\# Dimensions  & Source-Only  & CIDA & PCIDA \\
\hline
 1 & 77.4 & 79.8 & \textbf{80.6} \\
 2 & 77.6 & 81.0 & \textbf{81.1} \\
 4 & 77.7 & 81.2 & \textbf{81.3} \\
11 & 77.7 & \textbf{82.6} & \textbf{82.6} \\
\hline
\end{tabular}
\end{center}
\vskip -6mm
\end{footnotesize}
\end{table}

\textbf{Cross-Dataset Adaptation.}
Most clinical trials collect data from a population with a specific medical condition, and exclude people who have other conditions. However in practice many patients have multiple medical conditions and hence doctors need to apply the results of a particular study outside the population for which the data is collected.  Thus, in this section we consider cross-dataset adaption. Specifically, we evaluate how different methods perform when transferring among the datasets \emph{SHHS}, \emph{MESA}, and \emph{SOF}. \tabref{tab:cross} shows the accuracy of all methods in these cross-dataset settings. We observe that categorical domain adaptation barely improves upon models trained with only source domains, while CIDA and PCIDA can naturally transfer across continuously indexed domains even in the cross-dataset setting with significant performance improvement. Interestingly, when the task is hard such as transferring from \emph{SOF}, a very old people dataset, to datasets with much more diverse age range, PCIDA becomes a clear winner. But when the task is relatively easier, such as transferring from datasets with diverse age range to a very old dataset, CIDA is marginally better than PCIDA; but, this latter difference in performance is minor, and PCIDA performs well across all scenarios.

We also note that \emph{SHHS} and \emph{MESA} are both diverse datasets with similar age distribution, which is why the Source-Only model already achieves high accuracy. Interestingly, even in such cases PCIDA can still achieve stable performance gain compared to all baselines.



\textbf{Multi-Dimensional Indices.} As mentioned in \secref{sec:method}, both CIDA and PCIDA naturally generalize to multi-dimensional domain indices. To demonstrate this feature, we leverage that the \emph{SHHS} dataset includes multiple variables per patient in addition to their age, such as their heart rate, their physical and emotional health scores, etc. We combine such variables with the person's age to create a multi-dimensional domain index. (see more details on different domain indices in the Supplement). \tabref{tab:multi} shows the accuracy for multi-dimensional CIDA/PCIDA. For reference, we report corresponding accuracy for Source-Only. Note that Source-Only takes both the breathing signals () and the domain index () as input, as is done in all previous experiments. As expected we can observe substantial improvement in accuracy with multi-dimensional domain indices. 

Note that in the case of multi-dimensional indices, \emph{domain extrapolation} means that the target domain indices are \emph{outside} the convex hull of the source domain indices. Similarly, \emph{domain interpolation} means that the target domain indices are \emph{inside} the convex hull of the source domain indices.  \section{Conclusion}
We identify the problem of adaptation across continuously indexed domains, propose a series of methods for addressing it, and provide supporting theoretical analysis and empirical results using both synthetic and real-world medical data. Our work demonstrates the viability of efficient adaptation across continuously indexed domains and its potential impact on important real-world applications. Future work could investigate the possibility of matching higher or even infinite order moments, and the application of the proposed methods to other datasets in robotics or the medical field.  
\section*{Acknowledgement}
We thank Guang-He Lee and Mingmin Zhao for the insightful and tremendously helpful discussions. We are also grateful to Xingjian Shi, Xiaomeng Li, Hongzi Mao as well as other members at NETMIT and CSAIL for their comments to improve this paper. We would also like to thank Daniel R. Mobley and NSRR for their help with the datasets. 


\bibliography{paper}
\bibliographystyle{icml2020}


\end{document}
