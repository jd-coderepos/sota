\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy 

\def\iccvPaperID{102} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Amulet: Aggregating Multi-level Convolutional Features \\ for Salient Object Detection}

\author{Pingping Zhang$^{\dagger}$\quad Dong Wang$^{\dagger}$\quad Huchuan Lu$^{\dagger}$\thanks{Prof.Lu is the corresponding author.}\quad Hongyu Wang$^{\dagger}$\quad Xiang Ruan$^{\ddagger}$\\
$^{\dagger}$Dalian University of Technology, China\quad\quad $^{\ddagger}$Tiwaki Co.Ltd\\
{\tt\small jssxzhpp@mail.dlut.edu.cn\quad \{wdice,lhchuan,whyu\}@dlut.edu.cn\quad ruanxiang@tiwaki.com}
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
Fully convolutional neural networks (FCNs) have shown outstanding performance in many dense
labeling problems.
One key pillar of these successes is mining relevant information from features in convolutional layers.
However, how to better aggregate multi-level convolutional feature maps for salient object detection is underexplored.
In this work, we present \textbf{Amulet}, a generic aggregating multi-level convolutional feature framework for salient object detection.
Our framework first integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and fine details.
Then it adaptively learns to combine these feature maps at each resolution and predict saliency maps with the combined features. Finally, the predicted results are efficiently fused to generate the final saliency map.
In addition, to achieve accurate boundary inference and semantic enhancement, edge-aware feature maps in low-level layers and the predicted results of low resolution features are recursively embedded into the learning framework.
By aggregating multi-level convolutional features in this efficient and flexible manner, the proposed saliency model provides accurate salient object labeling.
Comprehensive experiments demonstrate that our method performs favorably against state-of-the-art approaches in terms of near all compared evaluation metrics.
\end{abstract}

\section{Introduction}
Salient object detection, which aims to identify the most conspicuous objects or regions in an image, has received considerable amount of attention in recent years.
As a pre-processing step in computer vision, saliency detection has shown a great success in ranges of visual applications, \eg. object retargeting~\cite{ding2011importance,sun2011scale,Vinyals2014Show}, scene classification~\cite{siagian2007rapid,ren2014region}, visual tracking~\cite{borji2012adaptive,mahadevan2013biologically}, image retrieval~\cite{he2012mobile,gao20123} and semantic segmentation~\cite{donoser2009saliency}.
Despite decades of valuable research, salient object detection still remains an unsolved research problem because there are large variety of aspects that can contribute to define visual saliency, and it's hard to combine all hand-tuned factors or cues in an appropriate way.


Inspired by human visual attention mechanisms, many early existing methods~\cite{itti98,harel07,Federico12,jiang2013salient,yan2013hierarchical,yang2013saliency} in salient object detection leverage low-level visual features (\eg. color, texture and contrast) with heuristic priors to model and approximate human saliency.
These generic techniques are known to be useful for keeping fine image structures and reducing computation.
Representative methods have set the benchmark on several saliency detection datasets.
However, such low-level features and priors can hardly capture high-level semantic knowledge about the object and its surroundings.
Thus, these low-level feature based methods are very far away from distinguishing salient objects from the clutter background and can not generate satisfied predictions. 

In recent years, fully convolutional networks (FCNs), adaptively extracting high-level semantic information from raw images, have shown impressive results in many dense labeling tasks, such as image segmentation~\cite{long2015fully,noh2015learning,dai2016instance}, generic object extraction~\cite{li2016r,hariharan2015hypercolumns}, pose estimation~\cite{yang2016end} and contour detection~\cite{xie2015holistically}.
Motivated by these achievements, several attempts to utilize high-level features of FCNs, have been performed and delivered superior performance in predicting saliency maps~\cite{lee2016deep,li2015visual,liu2016dhsnet,wang2015deep,zhao2015saliency}.
Nevertheless, these state-of-the-art models mainly focus on the non-linear combination of high-level features extracted from the last convolutional layers.
Due to the lack of low-level visual information such as object edge, the predicted results of these methods tend to have poorly localized object boundaries.


From above discussions, we note that 1) how to simultaneously utilize multi-level potential saliency cues, 2) how to conveniently find the optimal multi-level feature aggregation strategy, and 3) how to efficiently preserve salient objects' boundaries should become the most intrinsic problems in salient object detection.
To resolve these problems, in this paper, we propose a generic aggregating multi-level convolutional feature framework, namely \textbf{Amulet}, which effectively utilizes multi-level features of FCNs for salient object detection.


Our main contributions are summarized as follows:
\begin{itemize}
\vspace{-2mm}
 \item
We propose a multi-level feature aggregation network, dubbed AmuletNet, which utilizes convolutional features from multiple levels as saliency cues for salient object detection.
AmuletNet integrates multi-level features into multiple resolutions, learns to combine these features at each resolution and predicts saliency maps in a recursive manner.
\vspace{-2mm}
 \item
We propose a deeply recursive supervision learning framework. It effectively incorporates edge-aware feature maps in low-level layers and the predicted results from low resolution features, to achieve accurate object boundary inference and semantic enhancement.
The resulting framework can be trained by end-to-end gradient learning, which uses single-resolution ground truth without additional annotations.
 \vspace{-2mm}
\item
The proposed model (only trained on the MSRA10K dataset~\cite{ChengPAMI}) achieves new state-of-the-art performance on other large-scale salient object detection datasets, including the recent DUTS~\cite{Wang2017CVPR}, DUT-OMRON~\cite{yang2013saliency}, ECSSD~\cite{yan2013hierarchical}, HKU-IS~\cite{zhao2015saliency}, PASCAL-S~\cite{li2014secrets}, SED~\cite{borj2015salient} and SOD~\cite{yan2013hierarchical}.
In addition, the model is fast on modern GPUs, achieving a near real-time speed of 16 fps.
\end{itemize}

\section{Related Work}
In this section, we briefly review existing representative models for salient object detection.
We also discuss the multi-level feature aggregation methods based on FCNs.
\subsection{Salient object detection}
Over the past decades, lots of salient object detection methods have been developed.
The majority of salient object detection methods are based on low-level hand-crafted features, \eg, image contrast~\cite{Federico12,jiang2013salient}, color~\cite{li2014secrets,borj2015salient}, texture~\cite{yan2013hierarchical,yang2013saliency}.
A complete survey of these methods is beyond the scope of this paper and we refer the readers to a recent survey paper~\cite{borji2015salient} for details.


Recently, deep learning based approaches, in particular the convolutional neural networks (CNNs), have delivered
remarkable performance in many recognition tasks.
A lot of research efforts have been made to develop various deep architectures for useful features that characterize salient objects or regions.
For instance, Wang \etal~\cite{wang2015deep} first propose two deep neural networks to integrate local pixel estimation and global proposal search for salient object detection.
Li \etal~\cite{li2015visual} predict the saliency degree of each superpixel by taking multi-scale features in multiple generic CNNs.
Zhao \etal~\cite{zhao2015saliency} also predict the saliency degree of each superpixel by taking global and local context into account, and detect salient objects in a multi-context deep CNN.
Though these methods achieve better results than traditional counterparts, none of them handle low-level details perfectly, and all of their models include several fully connected layers, which are computationally expensive and drop spatial information of input images.
To remedy above problems, Lee \etal~\cite{lee2016deep} propose to encode low-level distance map and high-level sematic features of deep CNNs for salient object detection.
Liu \etal~\cite{liu2016dhsnet} propose a deep hierarchical saliency network to learn enough global structures and progressively refine the details of saliency maps step by step via integrating local context
information.
In addition, Li \etal~\cite{LiYu16} design a pixel-level fully convolutional stream
and a segment-level spatial pooling stream to produce pixel-level saliency predictions.
Wang \etal~\cite{wang2016saliency} develop deep recurrent FCNs to incorporate the coarse predictions as saliency priors and stage-wisely refine the generated predictions.
In contrary to the above methods only used specific-level features, we observe that features from all levels are potential saliency cues and helpful for salient object detection.
In light of this observation, we develop a new multi-level feature aggregation approach based on deep FCNs, and show that beyond refining the predicted saliency map, the approach can also jointly learn to preserve object boundaries.
\subsection{Feature aggregation in FCNs}
Several works on visualizing deep CNNs~\cite{simonyan2013deep,zeiler2014visualizing,mahendran2015understanding,Wang2015Visual}  indicate that convolutional features at different levels describe the object and its surroundings from different views.
High-level semantic features helps the category recognition of image regions, while low-level visual features help to generate sharp, detailed boundaries for high-resolution prediction.
However, how to effectively and efficiently exploit multi-level convolutional features remains an open question. To this end, several valuble attempts have been performed.
The seminal FCN method~\cite{long2015fully} introduces skip-connections and adds high-level prediction layers to intermediate layers to generate pixel-wise prediction results at multiple resolutions.
The Hypercolumn method~\cite{hariharan2015hypercolumns} also integrates convolutional features from multiple middle layers and learns high-level dense classification layers.
The SegNet~\cite{segnet} and DeconvNet~\cite{noh2015learning} employ a convolutional encoder-decoder network with pooling index guided deconvolution modules to exploit the features from multi-level convolutional layers.
Similarly,~ the U-Net~\cite{ronneberger2015u} apply multiple skip-connections to construct a contracting path to capture context and a symmetric expanding path that enables precise localization.
The HED model~\cite{xie2015holistically} employs deeply supervised structures, and automatically learns rich hierarchical representations that are fused to resolve the challenging ambiguity in edge and object boundary detection.

Our proposed approach clearly differs from the above-mentioned methods in three aspects.
Firstly, our method aggregates multi-level features at multiple resolutions. We use a pre-trained FCN and integrate all level features into multiple resolutions at once. Our method can simultaneously incorporate coarse semantics and fine details. Although all above methods seem to be useful for aggregating multi-level features, their aggregation is carried out in a stage-wise manner rather than jointly integrating.
Secondly, our method employs a bidirectional information stream, which facilitates complement effect in prediction.
In contrary, all above-mentioned methods simply aggregate multiple level features from one direction, i.e., low to high or high to low.
Thirdly, our method is able to refine the coarse high-level semantic predictions by exploiting low-level visual features.
In particular, our method employs edge-aware feature maps of low-level layers into the prediction modules which help to preserve objects' boundaries.
\begin{figure*}
\begin{center}
\includegraphics[width=0.95\linewidth,height=8.0cm]{Amulet_V62.pdf}
\end{center}
\vspace{-4mm}
\caption{The overall architecture of our proposed \textbf{Amulet} model. Each colorful box is considered as a feature block. The arrows between blocks indicate the information stream. Given an input image (256$\times$256$\times$3), multi-level features are first generated by the feature extraction network (VGG-16~\cite{simonyan2014very}). Then feature integration is performed by resolution-based feature combination modules (RFCs). After that, deep recursive supervision (DRS) is employed to improve the interaction of multiple predictions. Finally, boundary preserved refinements (BPRs) are used to refine the predicted saliency maps. The final saliency map is the fused output of multiple predicted saliency maps.}
\label{fig:Amulet}
\vspace{-5mm}
\end{figure*}
\section{Aggregating Convolutional Feature Model}
In this section, we begin by describing the components of our proposed AmuletNet architecture in Section 3.1. Then we give the detailed formulas of our bidirectional information aggregating learning method in Section 3.2. In the end, we construct saliency inference based on the multi-level predictions of the proposed \textbf{Amulet}.
\subsection{AmuletNet architecture}
Our proposed AmuletNet consists of four components: multi-level feature extraction, resolution-based feature integration,~ recursive saliency map prediction~ and~ boundary preserved refinement.
The four main components are jointly trained to optimize the output saliency detection quality.
The overall architecture is illustrated in Fig.~\ref{fig:Amulet}.
{\flushleft\textbf{Multi-level feature extraction.}}
The first component of our architecture is a deep feature extraction network, which takes the input image and produces feature maps for convolutional feature integration.
We build our architecture on the VGG-16 model from~\cite{simonyan2014very}, which is well known for its elegance and simplicity, and at the same time yields nearly state-of-the-art results in image classification and good generalization properties. In the VGG-16 model there are five max-pooling stages with kernel size 2 and stride 2.
Given an input image with size $W\times H$, the output feature maps have size $\lfloor\frac{W}{2^5},\frac{H}{2^5}\rfloor$, thus a FCN model built upon the VGG-16 would output feature maps reduced by a factor of 32.
To balance the semantic context and fine image details, we remove the last pooling stage and enlarge the size of the input image.
This way, the output feature maps of our feature extraction network are rescaled by a factor of 16 with respect to the input image.
We take feature maps at five levels from the VGG-16 model: conv1-2 (which contains 64 feature maps), conv2-2 (128 feature maps), conv3-3 (256 feature maps), conv4-3 (512 feature maps) and conv5-3 (512 feature maps).
Note that our feature extraction network is extremely flexible in that it can be replaced and modified in various ways, such as using different layers or networks, \eg. VGG-19~\cite{simonyan2014very} and ResNet~\cite{He2016Deep}.\vspace{-2mm}
{\flushleft \textbf{Resolution-based feature integration.}}
Considering the inconsistent resolution of multi-level convolutional features, we propose a novel resolution-based feature combination structure, named RFC.
The RFC structure consists of both \textbf{shrink} and \textbf{extend} branches.
Assume $\textbf{I}$ is the input image; $\tau=\lfloor\frac{W}{2^l},\frac{H}{2^l}\rfloor$ is the target resolution of integrated feature maps, and identified by feature level $l(=0,1,...,L)$; $\textbf{F}_{n}(\textbf{I})$ denotes a 3D tensor, i.e., the feature maps generated by the feature extraction network with $n\times \tau$ resolution.
Thus, the proposed RFC generates the integrated feature maps by
\begin{equation}
  \label{equ:equ1}
\begin{aligned}
 \textbf{F}^{\tau} = \textbf{W}^{\tau}*\textbf{Cat}(S_{n}(\textbf{F}_{n}(\textbf{I});\psi_{n}),...,S_{1}(\textbf{F}_{1}(\textbf{I});\psi_{1}),\\ E_{1}(\textbf{F}_{1}(\textbf{I});\varphi_{1}),...,E_{m}(\textbf{F}_{m}(\textbf{I});\varphi_{m})),
\end{aligned}
\end{equation}
where $*$ represents convolution operation;$S_{n}(\cdot;\psi_{n})$ denotes the shrink operator parameterized by $\psi_{n}$ that aims to down-sample the input high-resolution feature maps by a factor of $n$, while the extend operator $E_{m}(\cdot;\varphi_{m})$ aims to up-sample the low-resolution ones by a factor of $m$.
The shrink operators can be convolution or pooling. The extend operators can be deconvolution or interpolation.
$\textbf{Cat}$ is the cross-channel concatenation.
$\textbf{W}^{\tau}$ is the parameter for combining the concatenated feature maps.
The details of RFC are shown in Fig.~\ref{fig:RFC}.
For our proposed AmuletNet, we take feature maps at five different levels ($L=4$) from the above feature extraction network.
We utilize RFCs to resize all level feature maps into the five spatial resolution by performing 64 convolution or deconvolution operations.
The generated features are concatenated into a tensor with 320 channels at each resolution.
Then we use a convolutional layer with $1\times 1$ kernel size to weight the importance of each feature map.
For computational efficiency, 64 convolutional kernels are used to combine each tensor into 64 integrated feature maps.
This way, each integrated feature map will simultaneously incorporate coarse semantics and fine details.
\begin{figure}
\begin{center}
\includegraphics[width=0.98\linewidth,height=3.5cm]{RFC_v2.pdf}
\end{center}
\vspace{-4mm}
\caption{Details of the RFC module. The RFC first takes feature maps with different resolutions and channels as input. Then shrink and extend operators resize the feature maps to the same spatial resolution and equal channels. Finally, the concatenation and $1\times1$ convolution are used to generate the integrated features.}
\label{fig:RFC}
\vspace{-5mm}
\end{figure}
\vspace{-5mm}
{\flushleft \textbf{Recursive saliency map prediction.}}
The integrated feature maps already contains various saliency cues, so we can use them to predict the saliency map.
A direct method is to deconvolute the integrated feature maps at each level into the size of the input image, and add a new convolutional layer to produce the predicted saliency map.
Although this method can detect salient objects from different levels, the inner connection of different-level predictions is missing.
As a result, the independent prediction is not satisfactory enough, both quantitatively and visually, and further
post-processing is needed~\cite{Li2016DeepSaliency,wang2016saliency}.
To facilitate the interaction of multiple predictions, we propose a recursive prediction architecture, i.e. Deep Recursive Supervision (DRS) in Fig.~\ref{fig:Amulet}, to hierarchically and progressively absorb high-level predictions and render pixel-wise supervised information.
The proposed DRS includes saliency map prediction modules (SMP) and the deeply supervised learning mechanism~\cite{xie2015holistically}.
The SMP incorporates autoregressive recurrent connections into the predictions from high-level to low.
In each level $l$, the SMP takes integrated feature maps $\textbf{F}^{\tau}$ and the high-level prediction $\textbf{P}^{l+1}$ as input, and produces the new prediction of this level as
\begin{equation}
  \label{equ:equ2}
\textbf{P}^{l}=
\left\{
\begin{aligned}
&\textbf{W}_{r}*\sigma(\textbf{W}_{F^{\tau}}\star_{s}\textbf{F}^{\tau}+\textbf{W}_{P^{l+1}}*\textbf{P}^{l+1}+\textbf{b}),l<L\\
&\textbf{W}_{F^{\tau}}\star_{s}\textbf{F}^{\tau}+\textbf{b}, l = L
\end{aligned}
\right.
\end{equation}
where $\star_{s}$ represents deconvolution operation with stride $s$ to ensure the same spatial size of the output prediction.
$\textbf{W}_{F^{\tau}}$ and $\textbf{W}_{P^{l+1}}$ are the integrated feature weight and the output prediction weight, respectively.
$\textbf{b}$ is the bias parameter.
$\sigma$ is the ReLU activation function.
$\textbf{W}_{r}$ is the recursive weight.
From Eq.(\ref{equ:equ2}) and Fig.~\ref{fig:Amulet}, we can see that multiple autoregressive recurrent connections
ensure that the new prediction has multiple paths from the input to the output, which facilitates effective information exchanges.
Besides, we employ deeply supervised learning into the SMPs.
This way, the pixel-wise supervised information from ground truth will guide the recursive saliency map prediction at each level, making the SMPs be able to propagate fine details back to the predictions of large contexts.
Thus, DRS can build a bidirectional information stream aggregation, which facilitates complement effect in prediction.
We will fully elaborate the bidirectional information aggregating learning in Section 3.2.
The experiments in Section 4.4 show the superiority of DRS over the deeply supervised learning in~\cite{xie2015holistically}.
\vspace{-5mm}
{\flushleft\textbf{Boundary preserved refinement.}}
To further improve the detection accuracy, we add boundary refinements by introducing short connections to the predicted results.
Our approach bases on the observation that low-level feature maps in the conv1-2 layer have edge-preserving properties~\cite{zeiler2014visualizing,mahendran2015understanding}.
We expect that these low-level features help to predict objects' boundary.
Besides, the features also have the same spatial resolution with respect to the input image.
For boundary refinement, a convolutional layer with $1\times1$ kernel size is first applied to the conv1-2 layer, yielding
boundary predictions $\textbf{B}^{l}$.
Then $\textbf{B}^{l}$ are added to the raw prediction for better aligned object boundaries,
\begin{equation}
  \label{equ:equ3}
\begin{aligned}
 \textbf{P}_{b}^{l} = \textbf{W}_{b}*\sigma(\textbf{B}^{l}+\textbf{P}^{l}),
\end{aligned}
\end{equation}
where $\textbf{W}_{b}$ is the refinement parameter.
ReLU is used so that the boundary prediction is in the range of zero to infinity.
Based on the boundary preserved refinements $\textbf{P}_{b}$, a additional convolutional layer is applied and learned to produce the fusion saliency prediction (FSP) as the final output.
\subsection{Bidirectional information aggregating learning}
Given the salient object detection training dataset $ S=\{(X_n,Y_n)\}^{N}_{n=1}$ with $N$ training pairs, where $X_n =
\{x^n_j,j = 1,...,T\}$ and $Y_n = \{y^n_j,j = 1,...,T\}$ are the input image and the binary ground-truth image with $T$ pixels, respectively.
$y^n_j = 1$ denotes the foreground pixel and $y^n_j = 0$ denotes the background pixel.
For notional simplicity, we subsequently drop the subscript $n$ and consider each image independently.
We denote $\textbf{W}$ as the parameters of the feature extraction network and RFCs.
Supposing the network has $M$ predictions, including one fused prediction and $M-1$ specific-level predictions. In our AmuletNet, we have $M =6$.
For the fused prediction, the loss function can be expressed as
\begin{equation}
  \label{equ:equ4}
\begin{aligned}
  \mathcal{L}_f(\textbf{W},w_f)= - \beta \sum_{j\in Y_{+}} \text{log~Pr}(y_{j}=1|X;\textbf{W},w_f)\\
  -(1-\beta)\sum_{j\in Y_{-}} \text{log~Pr}(y_{j}=0|X;\textbf{W},w_f),
\end{aligned}
\end{equation}
where $w_f$ is the classifier parameter for the fused prediction.
$Y_{+}$ and $Y_{-}$ denote the foreground and background label sets, respectively.
The loss weight $\beta = |Y_{+}|/|Y|$, and $|Y_{+}|$ and $|Y_{-}|$ denote the foreground and background pixel number, respectively.
Pr$(y_j =1|X;\textbf{W};w_f)\in [0,1]$ is the confidence score of the fused prediction that measures how likely the pixel belong to the foreground.


For the prediction at level $l$, the loss function can be represented by
\begin{equation}
  \label{equ:equ5}
\begin{aligned}
  \mathcal{L}_l(\textbf{W},\theta_l,w_l)= - \beta \sum_{j\in Y_{+}} \text{log~Pr}(y_{j}=1|X;\textbf{W},\theta_l,w_l)\\
  -(1-\beta)\sum_{j\in Y_{-}} \text{log~Pr}(y_{j}=0|X;\textbf{W},\theta_l,w_l),
\end{aligned}
\end{equation}
where $\theta_l = (w^r_l,w^{b}_l)$ is the parameter of the recursive prediction component and boundary refinement component in the prediction module.
$w_l$ is the classifier parameter for the prediction at level $l$.
Thus, the joint loss function for all predictions is obtained by
\begin{align}
  \mathcal{L}(\textbf{W},\theta,w)= \alpha_f \mathcal{L}_f(\textbf{W},w_f)
  +\sum_{l=0}^{L}\alpha_l\mathcal{L}_l(\textbf{W},\theta_l,w_l),
  \label{equ:equ6}
\end{align}
where $\alpha_f$ and $\alpha_l$ are the loss weights to balance each loss term. For simplicity and fair comparison, we set $\alpha_f=\alpha_l=1$ as used in~\cite{xie2015holistically}.
The above loss function is continuously differentiable, so we can use the stochastic gradient descent (SGD) method to obtain the optimal parameters,
\begin{align}
(\textbf{W}^{*},\theta^{*},w^{*})= \text{arg min}~\mathcal{L}(\textbf{W},\theta,w).
  \label{equ:equ7}
\end{align}

Our aggregating learning method has several significant differences with other deeply supervised implementations, i.e., DHS~\cite{liu2016dhsnet} and HED~\cite{xie2015holistically}.
In DHS and HED, the deep supervision is directly applied on side-outputs, while in our method the deep supervision is applied on multiple same resolution predictions.
According to Eq.(\ref{equ:equ2}), each recursive prediction contains the information of two predictions at least, endowing our method the capability to propagate the supervised information across deep layers in a bidirectional manner.
The bold black arrows in Fig.~\ref{fig:Amulet} illustrate the bidirectional information stream.
Besides, DHS needs to specify scales for side-outputs to minimize the multi-scale error, which requires additional annotation for each scale.
In contrast, the proposed method adaptively unify the scale information into the size of input images, without using multi-scale
annotations.
In addition, different from the methods used sigmoid classifiers in~\cite{liu2016dhsnet,xie2015holistically}, we use the following softmax classifier to evaluate the prediction scores:
\begin{align}
\text{Pr}(y_{j}=1|X;\textbf{W},\theta,w) = \frac{e^{z_1}}{e^{z_0}+e^{z_1}},
  \label{equ:equ8}
\end{align}
\begin{align}
\text{Pr}(y_{j}=0|X;\textbf{W},\theta,w) = \frac{e^{z_0}}{e^{z_0}+e^{z_1}},
  \label{equ:equ9}
\end{align}
where $z_0$ and $z_1$ are the score of each label of training data. In this way, each prediction of the AmultNet is composed of a foreground excitation map ($\textbf{M}^{fe}$) and a background excitation map ($\textbf{M}^{be}$).
We utilize $\textbf{M}^{fe}$ and $\textbf{M}^{be}$ of all-level predictions to generate the final fusion.
This strategy not only increases the pixel-level discrimination but also captures context contrast information.
\subsection{Saliency inference}
Although the architecture we use in this work can produce $M$ predictions computed by Eq.(\ref{equ:equ8}) with the optimal parameters $(\textbf{W}^{*},\theta^{*},w^{*})$, we observe that the quality of the predictions at different levels varies widely.
The more lower level, the better they are.
The fused prediction generally appears much better than other predictions.
For saliency inference, we can simply use the fused prediction as our final saliency map.
However, saliency inference emphasize the contrast between foreground and background.
Therefore, more biologically we utilize the mean contrast of different predictions to further improve the detection accuracy during saliency inference.
Formally, let $\textbf{M}_{l}^{fe} (\textbf{M}_{f}^{fe})$ and $\textbf{M}_{l}^{be} (\textbf{M}_{f}^{be})$ denote the foreground excitation map and background excitation map at level $l$ (of the fused prediction), respectively.
They can be computed by Eq.(\ref{equ:equ8}) and Eq.(\ref{equ:equ9}).
Thus, the final saliency map can be obtained by
\begin{align}
  \textbf{S} & = \sigma(\textbf{Mean}(\sum_{l=0}^{L}(\textbf{M}_{l}^{fe}-\textbf{M}_{l}^{be}))+(\textbf{M}_{f}^{fe}-\textbf{M}_{f}^{be})),
  \label{equ:equ10}
\end{align}
where $\textbf{Mean}$ is the pixel-wise mean and $\sigma$ is the ReLU activation function for clipping the negative values.
\section{Experiments}
\subsection{Experimental Setup}
\textbf{Datasets:} For the training, we utilize the MSRA10K dataset~\cite{ChengPAMI}, which includes 10,000 images with high quality pixel-wise annotations.
Most of the images in this dataset contain only one salient object.
To improve the varieties, we simply augment this dataset by mirror reflection and rotation techniques ($0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}$), producing 80,000 training images totally.


For the performance evaluation, we adopt seven public saliency detection datasets as follows.

\textbf{DUT-OMRON}~\cite{yang2013saliency}. This dataset has 5,168 high quality images. Images of this dataset have one or more salient objects and relatively complex background. Thus this dataset is more difficult and challenging, and provides more space of improvement for related research in saliency detection.

\textbf{DUTS}~\cite{zhao2015saliency}. This dataset is currently the largest saliency detection benchmark, and contains 10,553 training images (DUTS-TR) and 5,019 test images (DUTS-TE) with high quality pixel-wise annotations.
Both the training and test set contain very challenging scenarios for saliency detection.


\textbf{ECSSD}~\cite{yan2013hierarchical}. This dataset contains 1,000 natural images, which include many semantically meaningful and complex structures in their ground truth segmentation.

\textbf{HKU-IS}~\cite{zhao2015saliency}. This dataset has 4,447 images with high quality pixel-wise annotations.
Images of this dataset are well chosen to include multiple disconnected salient objects or objects touching the image boundary.

\textbf{PASCAL-S}~\cite{li2014secrets}. This dataset is generated from the PASCAL VOC dataset~\cite{Everingham2010ThePV} and contains 850 natural images.

\textbf{SED}~\cite{borj2015salient}. This dataset contains two subsets: \textbf{SED1} and \textbf{SED2}.
The \textbf{SED1} has 100 images each containing only one salient object, while the \textbf{SED2} has 100 images each containing two salient objects.

\textbf{SOD}~\cite{yan2013hierarchical}. This dataset has 300 images, and it was originally designed for image segmentation.
Pixel-wise annotation of salient objects was generated by~\cite{jiang2013salient}.
This dataset is challenging since many images contain multiple objects either with low contrast or touching the image boundary.

\textbf{Implementation Details:}
We implement our approach based on the MATLAB R2014b platform with the Caffe toolbox~\cite{jia2014caffe}.
We run our approach in a quad-core PC machine with an i7-4790 CPU (with 16G memory) and a NVIDIA Titan X GPU (with 12G memory).
We train our model using augmented images from the MSRA10K dataset. We do not use validation set and train the model
until its training loss converges.
The parameters of multi-level feature extraction layers are initialized from the VGG-16 model~\cite{simonyan2014very}.
For other convolutional layers, we initialize the weights by the ``msra'' method~\cite{He2015Delving}.
We use the SGD method to train our network with a momentum 0.9 and a weight decay 0.0001.
We set the base learning rate to 1e-8 and decrease the learning rate by 10\% when training loss reaches a flat.
The training process takes almost 16 hours and converges after 200k iterations with mini-batch size 8.
When testing, the proposed salient object detection algorithm runs at about \textbf{16 fps} with $256\times256$ resolution.
The source code can be found at \textcolor[rgb]{1,0,0}{http://ice.dlut.edu.cn/lu/}.

\textbf{Evaluation Metrics:}
We utilize three main metrics to evaluate the performance of different salient object detection algorithms, including the precision-recall (PR) curves, F-measure and mean absolute error (MAE)~\cite{borji2015salient}.
The precision and recall are computed by thresholding the predicted saliency map, and comparing the binary map with the ground truth.
The PR curve of a dataset demonstrates the mean precision and recall of saliency maps at different thresholds.
The F-measure is a harmonic mean of average precision and average recall, and can be calculated by
\vspace{-0.5mm}
\begin{align}
  F_{\beta} =\frac{(1+\beta^2)\times Precision\times Recall}{\beta^2\times Precision \times Recall}.
    \label{equ:equ19}
\end{align}
\vspace{-0.5mm}
We set $\beta^2$ to be 0.3 to weigh precision more than recall as suggested in~\cite{yan2013hierarchical}~\cite{wang2015deep}~\cite{borji2015salient}~\cite{yang2013saliency}.

We report the performance when each saliency map is binarized with an image-dependent threshold.
The threshold is determined to be twice the mean saliency of the image:
\begin{align}
T = \frac{2}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}S(x,y),
  \label{equ:equ3}
\end{align}
where $W$ and $H$ are width and height of an image, $S(x,y)$ is the saliency value of the pixel at
$(x,y)$. We report the average precision, recall and F-measure over each dataset.

The above overlapping-based evaluations usually give higher score to methods which assign high saliency score to salient pixel correctly.
However, the evaluation on non-salient regions can be unfair especially for the methods which successfully detect non-salient regions, but miss the detection of salient regions.
Therefore, we also calculate the mean absolute error (MAE) for fair comparisons as suggested by~\cite{borji2015salient}.
The MAE evaluates the saliency detection accuracy by
\vspace{-1mm}
\begin{align}
MAE = \frac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}|S(x,y)-G(x,y)|,
  \label{equ:equ3}
\end{align}
\vspace{-0.5mm}
where $G$ is the binary ground truth mask.
\subsection{Performance Comparison with State-of-the-art}
We compare our algorithm with other 11 state-of-the-art ones including 7 deep learning based algorithms (DCL~\cite{LiYu16}, DHS~\cite{liu2016dhsnet}, DS~\cite{Li2016DeepSaliency}, ELD~\cite{lee2016deep}, LEGS~\cite{wang2015deep}, MDF~\cite{zhao2015saliency}, RFCN~\cite{wang2016saliency})
and 4 conventional algorithms (BL\cite{tong2015bootstrap}, BSCA~\cite{qin2015saliency}, DRFI~\cite{jiang2013salient}, DSR~\cite{li2013saliency}).
For fair comparison, we use either the implementations with recommended parameter settings or the saliency maps provided by the authors.
\setlength{\tabcolsep}{2.95pt}
\begin{table*}
\vspace{-5mm}
\begin{center}
\doublerulesep=0.6pt
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|||c|c|c|c|c|c|c|c|||}
\hline
\multicolumn{4}{|c|}{}
&\multicolumn{4}{|c|}{DUT-OMRON}
&\multicolumn{4}{|c|}{DUTS-TE}
&\multicolumn{4}{|c|}{ECSSD}
&\multicolumn{4}{|c|}{HKU-IS}
&\multicolumn{4}{|c|}{PASCAL-S}
&\multicolumn{4}{|c|}{SOD}
\\
\hline
\multicolumn{4}{|c|}{Methods}
&\multicolumn{2}{|c|}{$F_\beta$}&\multicolumn{2}{|c|}{$MAE$}&\multicolumn{2}{|c|}{$F_\beta$}&\multicolumn{2}{|c|}{$MAE$}&\multicolumn{2}{|c|}{$F_\beta$}&\multicolumn{2}{|c|}{$MAE$}&\multicolumn{2}{|c|}{$F_\beta$}&\multicolumn{2}{|c|}{$MAE$}&\multicolumn{2}{|c|}{$F_\beta$}&\multicolumn{2}{|c|}{$MAE$}&\multicolumn{2}{|c|}{$F_\beta$}&\multicolumn{2}{|c|}{$MAE$}\\
\hline
\multicolumn{4}{|c|}{\textbf{Amulet}}
&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.6471}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.09761}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.7365}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.08517}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.8684}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.05874}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.8542}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.05214}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.7632}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.09824}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.7547}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.13998}}\\
\multicolumn{4}{|c|}{\textbf{Amulet}-1/1}
&\multicolumn{2}{|c|}{0.6413}&\multicolumn{2}{|c|}{0.10161}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.7320}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.08796}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.8678}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.05997}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.8460}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.05416}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.7634}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.09948}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.7512}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.14169}}\\
\multicolumn{4}{|c|}{\textbf{Amulet}-1/2}
&\multicolumn{2}{|c|}{0.6408}&\multicolumn{2}{|c|}{0.10178}&\multicolumn{2}{|c|}{0.7210}&\multicolumn{2}{|c|}{0.08807}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.8675}}&\multicolumn{2}{|c|}{0.05998}&\multicolumn{2}{|c|}{0.8456}&\multicolumn{2}{|c|}{0.05421}&\multicolumn{2}{|c|}{0.7629}&\multicolumn{2}{|c|}{0.09965}&\multicolumn{2}{|c|}{0.7509}&\multicolumn{2}{|c|}{0.14177}\\
\multicolumn{4}{|c|}{\textbf{Amulet}-1/4}
&\multicolumn{2}{|c|}{0.6392}&\multicolumn{2}{|c|}{0.10219}&\multicolumn{2}{|c|}{0.7169}&\multicolumn{2}{|c|}{0.08851}&\multicolumn{2}{|c|}{0.8659}&\multicolumn{2}{|c|}{0.06039}&\multicolumn{2}{|c|}{0.8439}&\multicolumn{2}{|c|}{0.05465}&\multicolumn{2}{|c|}{0.7615}&\multicolumn{2}{|c|}{0.10001}&\multicolumn{2}{|c|}{0.7503}&\multicolumn{2}{|c|}{0.14204}\\
\multicolumn{4}{|c|}{\textbf{Amulet}-1/8}
&\multicolumn{2}{|c|}{0.6356}&\multicolumn{2}{|c|}{0.10282}&\multicolumn{2}{|c|}{0.6942}&\multicolumn{2}{|c|}{0.08933}&\multicolumn{2}{|c|}{0.8625}&\multicolumn{2}{|c|}{0.06137}&\multicolumn{2}{|c|}{0.8397}&\multicolumn{2}{|c|}{0.05570}&\multicolumn{2}{|c|}{0.7584}&\multicolumn{2}{|c|}{0.10067}&\multicolumn{2}{|c|}{0.7492}&\multicolumn{2}{|c|}{0.14262}\\
\multicolumn{4}{|c|}{\textbf{Amulet}-1/16}
&\multicolumn{2}{|c|}{0.6266}&\multicolumn{2}{|c|}{0.10280}&\multicolumn{2}{|c|}{0.6891}&\multicolumn{2}{|c|}{0.09110}&\multicolumn{2}{|c|}{0.8523}&\multicolumn{2}{|c|}{0.06477}&\multicolumn{2}{|c|}{0.8327}&\multicolumn{2}{|c|}{0.05821}&\multicolumn{2}{|c|}{0.7469}&\multicolumn{2}{|c|}{0.10273}&\multicolumn{2}{|c|}{0.7421}&\multicolumn{2}{|c|}{0.14495}\\
\multicolumn{4}{|c|}{\textbf{Amulet}$_{BPR^{-}}$}
&\multicolumn{2}{|c|}{0.6301}&\multicolumn{2}{|c|}{0.12062}&\multicolumn{2}{|c|}{0.6912}&\multicolumn{2}{|c|}{0.09761}&\multicolumn{2}{|c|}{0.8647}&\multicolumn{2}{|c|}{0.06572}&\multicolumn{2}{|c|}{0.8402}&\multicolumn{2}{|c|}{0.06302}&\multicolumn{2}{|c|}{0.7533}&\multicolumn{2}{|c|}{0.1240}&\multicolumn{2}{|c|}{0.7201}&\multicolumn{2}{|c|}{0.15340}\\
\hline
\multicolumn{4}{|c|}{\textbf{DCL}~\cite{LiYu16}}
&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.6842}}&\multicolumn{2}{|c|}{0.15726}&\multicolumn{2}{|c|}{0.7141}&\multicolumn{2}{|c|}{0.14928}&\multicolumn{2}{|c|}{0.8293}&\multicolumn{2}{|c|}{0.14949}&\multicolumn{2}{|c|}{0.8533}&\multicolumn{2}{|c|}{0.13587}&\multicolumn{2}{|c|}{0.7141}&\multicolumn{2}{|c|}{0.18073}&\multicolumn{2}{|c|}{0.7413}&\multicolumn{2}{|c|}{0.19383}\\
\multicolumn{4}{|c|}{\textbf{DHS}~\cite{liu2016dhsnet}}
&\multicolumn{2}{|c|}{-}&\multicolumn{2}{|c|}{-}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.7301}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.06578}}&\multicolumn{2}{|c|}{0.8675}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.05948}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.8541}}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.05308}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.7741}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.09426}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.7746}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.12840}}\\
\multicolumn{4}{|c|}{\textbf{DS}~\cite{Li2016DeepSaliency}}
&\multicolumn{2}{|c|}{0.6028}&\multicolumn{2}{|c|}{0.12038}&\multicolumn{2}{|c|}{0.6323}&\multicolumn{2}{|c|}{0.09070}&\multicolumn{2}{|c|}{0.8255}&\multicolumn{2}{|c|}{0.12157}&\multicolumn{2}{|c|}{0.7851}&\multicolumn{2}{|c|}{0.07797}&\multicolumn{2}{|c|}{0.6590}&\multicolumn{2}{|c|}{0.17597}&\multicolumn{2}{|c|}{0.6981}&\multicolumn{2}{|c|}{0.18894}\\
\multicolumn{4}{|c|}{\textbf{ELD}~\cite{lee2016deep}}
&\multicolumn{2}{|c|}{0.6109}&\multicolumn{2}{|c|}{\textcolor[rgb]{0,1,0}{0.09240}}&\multicolumn{2}{|c|}{0.6277}&\multicolumn{2}{|c|}{0.09761}&\multicolumn{2}{|c|}{0.8102}&\multicolumn{2}{|c|}{0.07955}&\multicolumn{2}{|c|}{0.7694}&\multicolumn{2}{|c|}{0.07414}&\multicolumn{2}{|c|}{0.7180}&\multicolumn{2}{|c|}{0.12324}&\multicolumn{2}{|c|}{0.7116}&\multicolumn{2}{|c|}{0.15452}\\
\multicolumn{4}{|c|}{\textbf{LEGS}~\cite{wang2015deep}}
&\multicolumn{2}{|c|}{0.5915}&\multicolumn{2}{|c|}{0.13335}&\multicolumn{2}{|c|}{0.5846}&\multicolumn{2}{|c|}{0.13793}&\multicolumn{2}{|c|}{0.7853}&\multicolumn{2}{|c|}{0.11799}&\multicolumn{2}{|c|}{0.7228}&\multicolumn{2}{|c|}{0.11934}&\multicolumn{2}{|c|}{-}&\multicolumn{2}{|c|}{-}&\multicolumn{2}{|c|}{0.6834}&\multicolumn{2}{|c|}{0.19548}\\
\multicolumn{4}{|c|}{\textbf{MDF}~\cite{zhao2015saliency}}
&\multicolumn{2}{|c|}{\textcolor[rgb]{0,0,1}{0.6442}}&\multicolumn{2}{|c|}{\textcolor[rgb]{1,0,0}{0.09156}}&\multicolumn{2}{|c|}{0.6732}&\multicolumn{2}{|c|}{0.09986}&\multicolumn{2}{|c|}{0.8070}&\multicolumn{2}{|c|}{0.10491}&\multicolumn{2}{|c|}{0.8006}&\multicolumn{2}{|c|}{0.09573}&\multicolumn{2}{|c|}{0.7087}&\multicolumn{2}{|c|}{0.14579}&\multicolumn{2}{|c|}{0.7205}&\multicolumn{2}{|c|}{0.16394}\\
\multicolumn{4}{|c|}{\textbf{RFCN}~\cite{wang2016saliency}}
&\multicolumn{2}{|c|}{0.6265}&\multicolumn{2}{|c|}{0.11051}&\multicolumn{2}{|c|}{0.7120}&\multicolumn{2}{|c|}{0.09003}&\multicolumn{2}{|c|}{0.8340}&\multicolumn{2}{|c|}{0.10690}&\multicolumn{2}{|c|}{0.8349}&\multicolumn{2}{|c|}{0.08891}&\multicolumn{2}{|c|}{0.7512}&\multicolumn{2}{|c|}{0.13241}&\multicolumn{2}{|c|}{0.7426}&\multicolumn{2}{|c|}{0.16919}\\
\hline
\multicolumn{4}{|c|}{\textbf{BL}~\cite{tong2015bootstrap}}
&\multicolumn{2}{|c|}{0.4988}&\multicolumn{2}{|c|}{0.23881}&\multicolumn{2}{|c|}{0.4897}&\multicolumn{2}{|c|}{0.23794}&\multicolumn{2}{|c|}{0.6841}&\multicolumn{2}{|c|}{0.21591}&\multicolumn{2}{|c|}{0.6597}&\multicolumn{2}{|c|}{0.20708}&\multicolumn{2}{|c|}{0.5742}&\multicolumn{2}{|c|}{0.24871}&\multicolumn{2}{|c|}{0.5798}&\multicolumn{2}{|c|}{0.26681}\\
\multicolumn{4}{|c|}{\textbf{BSCA}~\cite{qin2015saliency}}
&\multicolumn{2}{|c|}{0.5091}&\multicolumn{2}{|c|}{0.19024}&\multicolumn{2}{|c|}{0.4996}&\multicolumn{2}{|c|}{0.19614}&\multicolumn{2}{|c|}{0.7048}&\multicolumn{2}{|c|}{0.18211}&\multicolumn{2}{|c|}{0.6544}&\multicolumn{2}{|c|}{0.17480}&\multicolumn{2}{|c|}{0.6006}&\multicolumn{2}{|c|}{0.22286}&\multicolumn{2}{|c|}{0.5835}&\multicolumn{2}{|c|}{0.25135}\\
\multicolumn{4}{|c|}{\textbf{DRFI}~\cite{jiang2013salient}}
&\multicolumn{2}{|c|}{0.5504}&\multicolumn{2}{|c|}{0.13777}&\multicolumn{2}{|c|}{0.5407}&\multicolumn{2}{|c|}{0.17461}&\multicolumn{2}{|c|}{0.7331}&\multicolumn{2}{|c|}{0.16422}&\multicolumn{2}{|c|}{0.7218}&\multicolumn{2}{|c|}{0.14453}&\multicolumn{2}{|c|}{0.6182}&\multicolumn{2}{|c|}{0.20651}&\multicolumn{2}{|c|}{0.6343}&\multicolumn{2}{|c|}{0.22377}\\
\multicolumn{4}{|c|}{\textbf{DSR}~\cite{li2013saliency}}
&\multicolumn{2}{|c|}{0.5242}&\multicolumn{2}{|c|}{0.13886}&\multicolumn{2}{|c|}{0.5182}&\multicolumn{2}{|c|}{0.14548}&\multicolumn{2}{|c|}{0.6621}&\multicolumn{2}{|c|}{0.17837}&\multicolumn{2}{|c|}{0.6772}&\multicolumn{2}{|c|}{0.14219}&\multicolumn{2}{|c|}{0.5575}&\multicolumn{2}{|c|}{0.21488}&\multicolumn{2}{|c|}{0.5962}&\multicolumn{2}{|c|}{0.23394}\\
\hline
\end{tabular}
\vspace{1mm}
\caption{The F-measure and MAE of different saliency detection methods on six large-scale saliency detection datasets. The best three results are shown in \textcolor[rgb]{1,0,0}{red},~\textcolor[rgb]{0,1,0}{green} and \textcolor[rgb]{0,0,1}{blue}. The proposed methods rank first or second on these datasets.}
\vspace{-7mm}
\label{table:fauc}
\end{center}
\end{table*}
\begin{figure*}
\begin{center}
\begin{tabular}{@{}c@{}c@{}c@{}c}
\includegraphics[width=0.245\linewidth,height=3.85cm]{duts.pdf} \ &
\includegraphics[width=0.245\linewidth,height=3.85cm]{ecssd.pdf} \ &
\includegraphics[width=0.245\linewidth,height=3.85cm]{hku.pdf} \ &
\includegraphics[width=0.245\linewidth,height=3.85cm]{pascal.pdf} \ \\
{\small(a) DUTS-TE} & {\small(b) ECSSD} & {\small(c) HKU-IS} & {\small(d) PASCAL-S}\\ \\
\end{tabular}
\vspace{-6mm}
\caption{The PR curves of the proposed algorithm and other state-of-the-art methods.
\label{fig:PR-curve}}
\end{center}
\vspace{-10mm}
\end{figure*}

\textbf{Quantitative Evaluation}. As shown in Tab.~\ref{table:fauc} and Fig.~\ref{fig:PR-curve}, the \textbf{Amulet} model can largely outperform other compared counterparts across all the datasets in terms of near all evaluation metrics, which convincingly demonstrates the effectiveness of the proposed method.
Results on the SED dataset and PR curves on the DUT-OMRON, SED and SOD datasets appear in the supplemental material due to the limitation of space.
From the results, we have other fundamental observations: (1) Our model improves the F-measure with a considerable margin on most of datasets, especially on large-scale datasets, such as DUTS-TE, ECSSD, HKU-IS.
And at the same time, our model generally decreases the MAE.
This indicates that our model is more convinced of the predicted regions and provides more accurate saliency maps.
(2) Although only trained on the MSRA10K dataset, our model significantly outperforms other algorithms that pre-trained on specific saliency datasets, such as LEGS and RFCN on PASCAL-S, MDF on HKU-IS.
The superior performance confirms that our model have good generalization abilities on other large-scale datasets.
(3) Our method is inferior to DHS on several datasets. However, these datasets are relatively small compared to the era of deep learning.


\textbf{Qualitative Evaluation}. Fig.~\ref{fig:map_examples} provides a visual comparison of our approach and other methods.
It can be seen that our method generates more accurate saliency maps in various challenging cases, \eg, low contrast between the objects and backgrounds (the first two rows), objects near the image boundary (the 3-4 rows) and multiple disconnected salient objects (the 5-6 rows). What's more, with our BPR component, our saliency maps provide more accurate boundaries of salient objects (the 1, 3, 4, 6 rows).
\begin{figure*}
\centering
\begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c}
\vspace{-1mm}
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{gt0234.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_Amulet.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_RFCN.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_DCL.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_DHS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_DS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_LEGS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_MDF.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_ELD.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0234_DRFI.png} \ \\
\vspace{-1mm}
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{gt0216.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_Amulet.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_RFCN.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_DCL.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_DHS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_DS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_LEGS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_MDF.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_ELD.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0216_DRFI.png} \ \\
\vspace{-1mm}
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{gt0142.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_Amulet.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_RFCN.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_DCL.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_DHS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_DS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_LEGS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_MDF.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_ELD.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0142_DRFI.png} \ \\
\vspace{-1mm}
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{gt0190.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_Amulet.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_RFCN.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_DCL.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_DHS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_DS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_LEGS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_MDF.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_ELD.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{0190_DRFI.png} \ \\
\vspace{-1mm}
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{gt100_1219.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_Amulet.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_RFCN.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_DCL.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_DHS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_DS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_LEGS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_MDF.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_ELD.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{100_1219_DRFI.png} \ \\
\vspace{-1mm}
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{gtb13vehicles_land000.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_Amulet.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_RFCN.jpg} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_DCL.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_DHS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_DS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_LEGS.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_MDF.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_ELD.png} \ &
\includegraphics[width=0.085\linewidth,height=1.25cm]{b13vehicles_land000_DRFI.png} \ \\
{\small (a)} & {\small(b)} & {\small(c)} & {\small(d)} & {\small(e)}& {\small(f)}& {\small(g)}
& {\small(h)}& {\small(i)}& {\small(j)}& {\small(k)} \\
\end{tabular}
\caption{Comparison of saliency maps. (a) Input images; (b) Ground truth; (c) Our method; (d) RFCN; (e) DCL; (f) DHS; (g) DS; (h) LEGS; (i) MDF; (j) ELD; (k) DRFI. The top four row and bottom two row images are from the ECSSD and SED dataset, respectively.
\label{fig:map_examples}}
\end{figure*}
\begin{figure*}
\vspace{-3mm}
\centering
\begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}}
\vspace{-0.5mm}
\includegraphics[width=0.105\linewidth,height=1.8cm]{40.jpg} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{96.pdf} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{16.pdf} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{0009.jpg} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{9_96.pdf} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{9_16.pdf} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{0976.jpg} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{0976_96.pdf} \ &
\includegraphics[width=0.105\linewidth,height=1.8cm]{0976_16.pdf} \ \\
{\small(a)} & {\small(b)} & {\small(c)} & {\small(d)} & {\small(e)} & {\small(f)}& {\small(g)} & {\small(h)} & {\small(i)}\\
\end{tabular}
\caption{Visual comparison of the Amulet algorithm with /without BPRs. (a)(d)(g) Input images; (b)(e)(h) Predictions of the \textbf{Amulet}; (c)(f)(i) Predictions of the $\textbf{Amulet}_{BPR^{-}}$. High resolution to see better.
\label{fig:Ablation}}
\vspace{-5mm}
\end{figure*}
\setlength{\tabcolsep}{1.75pt}
\subsection{Ablation Studies}
\textbf{Feature resolution effects.} To verify the importance of resolutions of integrated features, we additionally evaluate several variants of the proposed \textbf{Amulet} model with different scales.
\textbf{Amulet}-$1/n$ denotes the model that takes the integrated features reduced by a factor not larger than $n$, with respect to the input image.
The corresponding performance are also reported in Tab.~\ref{table:fauc}.
The results suggest that features of all levels are helpful for saliency detection, and with the increment of resolutions, our approach gradually achieves better performance.
In addition, even our simplest model (i.e., \textbf{Amulet}-1/16) can achieve better results than most of existing methods.
This fact further verifies the strength of our proposed methods.

\textbf{Boundary refinements.}
To verify the contributions of our proposed BPR, we also implement our proposed approach without BPRs, named $\textbf{Amulet}_{BPR^{-}}$, and report the performance in Tab.~\ref{table:fauc}.
It can be observed that without BPRs, our approach decreases the performance but not too much in F-measure.
But it leads to a large drop in MAE. This indicates that our proposed BPR is capable of detecting and
localizing the boundary of most salient objects, while other methods often fail at this fact.
Several visual examples are illustrated in Fig.~\ref{fig:Ablation}.
\subsection{Comparison with Other Aggregation Methods}
For fair comparison, we perform additional evaluations to verify the detection ability of different aggregation methods.
Specifically, we use the same augmented MSRA10K dataset to train the FCN-8s~\cite{long2015fully}, Hypercolumn (HC)~\cite{hariharan2015hypercolumns}, SegNet (SN)~\cite{segnet}, DeconvNet(DN)~\cite{noh2015learning} and HED~\cite{xie2015holistically} for saliency detection task.
All compared methods are based on the same VGG-16 model pre-trained on the ImageNet classification task~\cite{simonyan2014very}.
We drop the unnecessary components in each model and only focus on the feature aggregation part.
For our model, we use the simplest model (i.e., \textbf{Amulet}-1/16) without BPRs.
For each method, we find the optimal parameters to achieve its' best results.
The performance on the ECSSD dataset is listed in Tab.~\ref{table:aggregation}. As can be seen from Tab.~\ref{table:aggregation}, with the aggregation of multi-level features, our approach achieves better performance.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Methods &\textbf{FCN-8s}&\textbf{HC}&\textbf{SN}&\textbf{DN}&\textbf{HED}&\textbf{Ours}\\
\hline
$F_\beta$&0.8116&0.8187& 0.8145 & 0.8264 &0.8321&0.8521        \\
\hline
$MAE$    &0.1343&0.1193& 0.0947& 0.1435& 0.1022&0.0662       \\
\hline
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{The performance of different aggregations on ECSSD dataset. Other datasets have the similar performance trend.}
\label{table:aggregation}
\vspace{-6mm}
\end{table}
\section{Conclusion}

In this paper, we propose a generic aggregating multi-level convolutional feature framework for salient object detection.
Our framework can integrate multi-level feature maps into multiple resolutions, learn to combine feature maps, and predict saliency maps with the integrated features.
In addition, edge-aware maps and high-level predictions are embedded into the framework.
Experiments demonstrate that our method performs favorably against state-of-the-art approaches in saliency detection.
\vspace{-2mm}
{\small {\flushleft\textbf{Acknowledgment}}.
This paper is supported by the Natural Science Foundation of China \#61472060, \#61502070 and \#61528101.}
\vspace{-5mm}
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document} 