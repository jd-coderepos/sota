\begin{figure*}
\includegraphics[width=16cm]{fig3.pdf}
\caption{\textbf{Overview of our methods}. We take Mean Teacher as our SSOD baseline. In Mean Teacher, the teacher model generates pseudo labels from weakly augmented unlabeled data, and the student model is trained with a combination of ground-truths and pseudo labels. To alleviate the class imbalance in SSOD, we first design a memory module called CropBank which absorbs instance-level annotations including ground truths and pseudo labels. Thereafter, we utilize the CropBank to perform foreground-background rebalancing (FBR) and adaptive foreground-foreground rebalancing (AFFR) on strong augmented unlabeled data for adaptive class-rebalancing self-training (ACRST). To further improve ACRST, we introduce a two-stage pseudo-label filtering algorithm with classification confidence and high-level semantics.}
\label{fig:3}
\end{figure*}

\section{Method}
This section describes our solutions in detail.Section~\ref{3.1} defines the problem, while Section~\ref{3.2} introduces the baseline framework Mean Teacher. Section~\ref{3.3} defines the CropBank, and Section~\ref{3.4} introduces the proposed adaptive class self-training rebalancing (ACRST) algorithm. Section~\ref{3.5} illustrates the two-stage pseudo-label filtering method. Lastly, Section~\ref{3.6} introduces the selective supervision mechanism. The overview of the entire training process is shown in Figure~\ref{fig:3}.

\subsection{Problem Definition}\label{3.1}
Semi-supervised object detection aims to train detectors in a semi-supervised setting, where a small labeled dataset  and a large unlabeled dataset  are available. / presents the number of labeled/unlabeled data. contains bounding-box annotations including object locations, sizes, and categories in th labeled image . 

\subsection{Mean Teacher for Semi-supervised Object Detection}\label{3.2}
    This study takes the Mean Teacher~\cite{tarvainen2018mean} as the SSOD baseline. Mean Teacher consists of a teacher and a student model, in which the entire framework is optimized via a mutual learning mechanism. The training pipeline of Mean Teacher consists of two stages. 
    
    \textbf{Pre-training.} 
   The student model is pre-trained with a small amount of labeled data  via gradient back-propagation in a supervised manner. Thereafter, we initialize the teacher model with pre-trained model weights of the student model, which produce noisy-less pseudo-labels, thereby facilitating the subsequent training. 
    
    \textbf{Teacher-Student Mutual Learning. } 
    In the mutual learning stage, we train the student model with supervision signals consisting of ground truths and pseudo-labels. Once the student model is updated via the gradient back-propagation, the learned knowledge is feedback to the teacher model in an exponential moving average (EMA) mechanism,
        
        
    where  represents the model parameters of the student/teacher model, and  represents the total SSOD losses.
    
    By using Faster-RCNN\cite{2017Faster} as the detection module, the loss function of SSOD can be summarized as a combination of losses on labeled data  and unlabeled data .
        
    
    
where  is the RPN classification loss, is the RPN regression loss,  is the ROI classification loss,  is the ROI regression loss.  represents the annotation of the labeled image ,  represents the pseudo-labels of unlabeled image , and  is used to balance the supervised and unsupervised losses. Note that regression losses are removed in  in previous SSOD studies for denoising.

To succeed in SSOD, the teacher model must generate accurate pseudo-labels and maintain a reliable performance margin over the student model throughout the training. However, we observe that class imbalance in SSOD significantly hinders the performance of the teacher model.

\subsection{CropBank}\label{3.3}

Data-rebalancing algorithms have been proved to be the most simple and effective data-rebalancing method in classification tasks. However, their effectiveness is heavily impeded by strong interconnections on both foreground-background and foreground-foreground instances. To separate the entanglement, we propose a novel memory module called CropBank, which stores abundant instance-level annotations. The CropBank consists of two sub-banks, namely, Labeled CropBank  and Pseudo CropBank , where / represent the size of Labeled/Pseudo CropBank, /  represents ground truths/pseudo-labels of th labeled/unlabeled image.

In the implementation, the CropBank size is unlimited owing to the negligible memory consumption of instance-level annotations.  In the training stage,  is fixed once generated, while  is updated periodically with improved pseudo-labels in mutual training. We use CropBank as the basis to decouple instances and design adaptive class-rebalancing self-training (ACRST) to address the class imbalance. 

\begin{figure}[h!]
\includegraphics[width=8.5cm]{fig4.pdf}
\caption{\textbf{Data-rebalancing with instances in the CropBank.}}
\label{fig:4}
\end{figure}

\subsection{Adaptive Class-Rebalancing Self-Training}\label{3.4}

The self-training paradigm is an ideal solution to alleviate the lack of labeled data. However, its effectiveness is impeded by the inherent class imbalance in object detection tasks. Therefore, we propose ACRST to alleviate the class imbalance in SSOD. ACRST consists of foreground-background rebalancing (FBR) and adaptive foregroundâ€“foreground rebalancing (AFFR).

\subsubsection{Foreground-Background Rebalancing} \label{3.4.1}
\ 
\newline
Models trained on foreground-background imbalanced data tend to overfit excessive background instances. Foreground-background imbalance in object detection has been widely explored.  Various solutions have been proposed to alleviate such an imbalance, including loss-reweighting~\cite{2017Focal} and region refinement~\cite{2017Faster}. Unfortunately, these methods rely on ground truths unavailable in SSOD to guide the rebalancing procedure. Hence, we utilize abundant instance-level annotations including ground truths and pseudo-labels in the CropBank to perform foreground-background rebalancing. 

Given a training mini-batch , we fetch a set of foreground instances  from the CropBank  and  for each image  following a sampling distribution \emph{P}, where  is a foreground instance cropped from original images with annotation . Thereafter, a new training mini-batch   is generated as follows:
    
        
Where  denotes a binary mask of pasted objects and  denotes mixed annotations, in which fully occluded instances are removed from mixed image . In detail,  is a rectangular region cropped from the image based on instance-level annotations in the CropBank. During training,  is augmented and pasted thereafter to random locations of . This combined procedure increases the ratio of foreground instances in the training data for foreground-background rebalancing and also explores essential context semantics from a holistic perspective. 

Once mixed images are ready, we take them to train the detector as the pipeline of Mean Teacher. The rebalancing process is shown in Figure~\ref{fig:4}. As discussed in Section~\ref{3.6}, such a crop-and-paste operation enables higher model performance with selective supervision. 

\subsubsection{Adaptive Foreground-Foreground Rebalancing}\label{3.4.2}
\ 
\newline
FBR adequately alleviates the foreground-background imbalance with considerable attention on foreground instances. However, a random sampling distribution \emph{P}, such as uniform distribution, fails to correct the foreground-foreground imbalance. Hence, we propose an adaptive sampling probability distribution \emph{P} for foreground-foreground rebalancing. In particular, samples in neglected classes are selected more frequently during the training.

To measure the neglected degree of a class, we propose a novel criterion \emph{pseudo recall} (), which quantities the proportion of pseudo-labels to ground truths. In detail, we estimate the class distribution of unlabeled data from labeled data on account of distribution similarity between labeled and unlabeled data. Suppose there are  classes  in datasets. We calculate \emph{pseudo recall} for class  as
    
where  and  denote the number of pseudo-labels and ground truths of class , and  is the ratio of the unlabeled to labeled data. 

\emph{Pseudo recall} defines how neglected one class is under the SSOD setting. High  indicates that the detector is certain even overconfident on class . Consequently, lower sampling probabilities should be allocated to samples in class  for overfitting alleviation. By contrast, low  implies that the detector lacks confidence for detecting instances of class . Therefore, we should select these instances frequently. As a solution, we sort the classes in descending order according to \emph{pseudo recall} and design the following adaptively sampling strategy:
    
where  is the probability of selecting instances of class , and  is used to tune the sampling probability. This mechanism adaptively allocates higher/lower sampling rates to neglected/over-focused instances. Note that AFFR performs FBR simultaneously. There are numerous ways to rebalance class distribution, and we introduce an effective example. A potential problem with this mechanism is that the noise of pseudo-labels in the neglected classes is amplified. Therefore, we propose a two-stage pseudo-label filtering mechanism in Section~\ref{3.5}.

\subsection{Two-stage Pseudo-label Filtering}\label{3.5}

The proposed ACRST considerably alleviates the class imbalance in SSOD. However, its effectiveness is heavily affected by the quality of pseudo-labels. Once noise in the CropBank is selected improperly, it will be undesirably amplified in self-training. Consequently, we should filter noisy pseudo-labels from the teacher model predictions and store noisy-less pseudo-labels in the CropBank. In SSOD, the general filtering algorithm sets a threshold  to filter predictions with low classification confidence out. However, such single-stage filtering without additional semantics constraints is prone to produce noisy pseudo-labels. As a solution, we propose a semi-supervised multi-label classification module to learn high-level semantics (i.e., image-level pseudo-labels). Thereafter, we design a two-stage filtering algorithm with classification confidences and high-level semantics to generate accurate pseudo-labels.

\subsubsection{Semi-supervised Multi-label Classification.} 
\ 
\newline
The proposed semi-supervised multi-label classification module is devised based on Mean Teacher for the classification task. For each image , we particularly aim to predict its image-level pseudo-labels , where  is the total category number and  determines whether there are instances of class  in the image. In the training stage, predictions of the teacher model are converted to image-level pseudo-labels which supervise the student model. We utilize a focalâ€“binaryâ€“cross-entropy loss to optimize the student model.

    
\subsubsection{Two-stage Pseudo-label Filtering. } 
\ 
\newline
For bounding-box prediction  with classification score  of image  with image-level pseudo-label , we perform a two-stage filtering to get bounding-box pseudo-labels with low-noise. In the first stage, we filter predictions out with scores  to remove predictions with low objectness or wrong class labels. In the second stage, predictions whose classes activate negative in  (i.e., activation values are smaller than ) are removed. The second filtering stage utilizes high-level semantics to filter noisy predictions inconsistent with image-level pseudo-labels. With the two-stage filtering, the consistency of low- and high-level semantics are achieved. 

\subsection{Selective Supervision}\label{3.6}
In previous SSOD research, bounding-box regression losses are removed during training to alleviate noise. By contrast, utilizing regression losses in our framework is beneficial to achieve high SSOD performance, which is attributed to the CropBank module.
    
The contribution is two-fold. First, the CropBank alleviates noise from partially detected instances, which take a large proportion in bias predictions. Learning blindly with these noisy pseudo-labels will heavily aggravate the model performance.  However, when the partially detected bounding-boxes from the CropBank are cropped and pasted to training batches, they become independent and complete in the new background, thereby providing additional clean training supervisions. Second, the CropBank provides a detection-specific data augmentation method. The additional augmented data continuously improves the regression accuracy of pseudo-labels.

With selective supervision, loss function  in Equation \ref{equation_5} can be represented as follows:

where  denotes instances from the CropBank in .

