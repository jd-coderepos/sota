

\documentclass[runningheads]{llncs}
\usepackage{array}

\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{multirow}
\usepackage{relsize}
\usepackage{comment}
\usepackage{pifont}
\renewcommand{\ttdefault}{pcr}


\begin{document}
	\pagestyle{headings}
	\mainmatter

\title{Simultaneous Semantic Segmentation and
\\
    Outlier Detection
    in Presence of Domain Shift} 
\titlerunning{Semantic Segmentation and
    Outlier Detection
    in Presence of Domain Shift}
	\authorrunning{Bevandi{\' c} et al.}
\author{
Petra Bevandi{\' c}\thanks{This work has been partially supported by Croatian Science Foundation.} \and
Ivan Kre{\v s}o \and
Marin Or{\v s}i{\' c} \and
Sini{\v s}a {\v S}egvi{\' c}
}

\institute{
University of Zagreb, Faculty of Electrical Engineering and Computing,  Croatia
}


	\maketitle

\begin{abstract}
    Recent success on realistic
    road driving datasets
has increased interest in exploring
    robust performance 
    in real-world applications.
One of the major unsolved problems
    is to identify image content
    which can not be reliably recognized
    with a given inference engine.
We therefore study approaches
    to recover a dense outlier map
alongside the primary task
    with a single forward pass,
    by relying on shared convolutional features.
We consider semantic segmentation
    as the primary task and
    perform extensive validation
    on WildDash val (inliers),
    LSUN val (outliers),
    and pasted objects from
    Pascal VOC 2007 (outliers).
We achieve the best validation performance
    by training to discriminate inliers
    from pasted ImageNet-1k content,
    even though ImageNet-1k contains 
    many road-driving pixels,
    and, at least nominally, fails to account 
    for the full diversity of the visual world.
    The proposed two-head model performs
    comparably to the C-way multi-class model
    trained to predict 
    uniform distribution in outliers,
    while outperforming several
    other validated approaches.
We evaluate our best two models 
    on the WildDash test dataset
    and set a new state of the art 
    on the WildDash benchmark.
\end{abstract}
\section{Introduction}

Early computer vision approaches focused
on producing decent performance 
on small datasets.
This often posed overwhelming difficulties, 
so researchers seldom quantified 
the prediction confidence. An important mi\-le\-stone
was reached when generalization 
was achieved
on realistic datasets
such as Pascal VOC \cite{everingham10ijcv},
CamVid \cite{brostow08eccv},
KITTI \cite{geiger13ijrr}, and
Cityscapes \cite{cordts15cvpr}.
These datasets assume 
closed-world evaluation 
\cite{scheirer13pami} 
in which the training and test subsets 
are sampled from the same distribution.
Such setup has been able to provide 
a fast feedback on novel approaches 
due to good alignment 
with the machine learning paradigm.
This further accelerated development 
and led us to the current state of research
where all these datasets are mostly solved, 
at least in the strongly supervised setup.

Recent datasets further raise the bar
by increasing the number of classes
and image diversity.
However, despite this increased complexity, 
the Vistas \cite{neuhold17iccv} dataset
is still an insufficient proxy 
for real-life operation
even in a very restricted scenario 
such as road driving.
New classes like bike racks 
and ground animals were added,
however many important classes from 
non-typical or worst-case images 
are still absent.
These classes include 
persons in non-standard poses,
crashed vehicles, rubble,
fallen trees etc.
Additionally, real-life images 
may be affected by 
particular image acquisition faults 
including hardware defects, 
covered lens etc.
This suggests that foreseeing every possible
situation may be an elusive goal, 
and that our algorithms
should be designed to recognize 
image regions which are foreign to
the training distribution.

The described deficiencies 
emphasize the need
for a more robust approach
to dataset design.
First, an ideal dataset should identify 
and target a set of explicit hazards
for the particular domain
\cite{zendel18eccv}.
Second (and more important),
an ideal dataset should endorse 
open-set recognition paradigm
\cite{scheirer13pami}
in order to promote 
detection of unforeseen hazards.
Consequently, the validation (val) 
and test subsets should contain
various degrees of domain shift
with respect to the training distribution.
This should include 
moderate domain shift factors
(e.g.\ adverse weather, exotic locations),
exceptional situations 
(e.g.\ accidents, poor visibility, defects)
and outright outliers 
(objects and entire images from other domains).
We argue that the WildDash dataset
\cite{zendel18eccv}
represents a step in the right direction,
although further development 
would be welcome, especially
in the direction of enlarging 
the negative part of the test dataset \cite{blum19arxiv}.
Models trained for open-set evaluation
can not be required 
to predict an exact visual class 
in outlier pixels.
Instead, it should suffice that 
the outliers are recognized,
as illustrated in Fig.\,\ref{fig:approach}
on an image from the WildDash dataset.


\begin{figure}[htb]
\includegraphics[width=\textwidth]{simple_model.png}
  \caption{
    The proposed approach for simultaneous
    semantic segmentation and outlier detection.
    Our multi-task model predicts 
    i) a dense outlier map, and 
    ii) a semantic map with respect 
      to the 19 Cityscapes classes. 
    The two maps are merged to obtain
    the final outlier-aware 
    semantic predictions.
    Our model recognizes 
    outlier pixels (white) 
    on two objects
    which are foreign to Cityscapes:
    the ego-vehicle and the yellow forklift.
  }
  \label{fig:approach}
\end{figure}

This paper addresses simultaneous 
semantic segmentation and 
open-set outlier detection.
We train our models on inlier images
from two road-driving datasets: 
Cityscapes \cite{cordts15cvpr} 
and Vistas \cite{neuhold17iccv}.
We consider several 
outlier detection approaches 
from the literature
\cite{hendrycks17iclr,bevandic2018,hendrycks19iclr}
and validate their performance
on WildDash val (inliers),
LSUN val \cite{yu2015} (outliers),
and pasted objects from
Pascal VOC 2007 (outliers).
Our main hypotheses are
i) that training with noisy negatives 
  from a very large and diverse dataset
  such as ImageNet-1k \cite{deng09cvpr}
  can improve outlier detection,
and ii) that discriminative outlier detection and semantic
segmentation can share features
without significant deterioration of either task.
We confirm both hypotheses
by re-training our best models 
on WildDash val, Vistas and ImageNet-1k,
and evaluating performance
on the WildDash benchmark.





\section{Related Work}

Previous approaches to 
outlier detection in image data
are very diverse.
These approaches are based on
analyzing prediction uncertainty,
evaluating generative models,
or exploiting a broad secondary dataset 
which contains both outliers and inliers.
Our approach is also related
to multi-task models 
and previous work which explores
the dataset quality and dataset bias.

\subsection{Estimating Uncertainty 
  (or Confidence) of the Predictions}
Prediction confidence can be expressed 
as the probability of the winning class 
or max-softmax for short \cite{hendrycks17iclr}.
This is useful in image-wide
prediction of outliers,
although max-softmax 
must be calibrated \cite{guo17icml}
before being interpreted as .
The ODIN approach \cite{liang18iclr}
improves on \cite{hendrycks17iclr} 
by pre-processing input images 
with a well-tempered perturbation 
aimed at increasing 
the max-softmax activation.
These approaches are handy since they 
require no additional training.

Some approaches model the uncertainty
with a separate head which learns either
prediction uncertainty 
\cite{kendall17nips,lakshminarayanan17nips} 
or confidence \cite{devries18arxiv}.
Such training is able to recognize examples
which are hard to classify 
due to insufficient or inconsistent labels,
but is unable to deal with real outliers.

A principled information-theoretic approach 
expresses the prediction uncertainty as  
mutual information between 
the posterior parameter distribution
and the particular prediction \cite{smith18uai}.
In practice, the required expectations
are estimated with Monte Carlo (MC) dropout
\cite{kendall17nips}.
Better results 
have been achieved
with explicit ensembles
of independently trained models
\cite{lakshminarayanan17nips}.
However, both approaches 
require many forward passes
and thus preclude real-time operation.

Prediction uncertainty 
can also be expressed
by evaluating per-class 
generative models 
of latent features \cite{Lee2018ASU}.
However, this idea is not easily adaptable 
for dense prediction in 
which latent features 
typically correspond to many classes
due to subsampling and dense labelling.
Another approach would be 
to fit a generative model 
to the training dataset and 
to evaluate the likelihood of a given sample.
Unfortunately, this is very hard
to achieve with image data
\cite{nalisnick19iclr}. 

\subsection{Training with Negative Data}

Our approach is most related 
to three recent approaches 
which train outlier detection 
by exploiting a diverse 
negative dataset \cite{torralba2011}.
The approach called outlier exposure
(OE) \cite{hendrycks19iclr} 
processes the negative data
by optimizing cross entropy 
between the predictions 
and the uniform distribution.
Outlier detection has also been formulated
as binary classification \cite{bevandic2018}
trained to differentiate inliers 
from the negative dataset.
A related approach \cite{Vyas2018OutofDistributionDU} 
partitions the training data into K folds
and trains an ensemble of K
leave-one-fold-out classifiers.
However, this requires K forward passes.
while data partitioning 
may not be straight-forward.

Negative training samples
can also be produced 
by a GAN generator \cite{goodfellow14nips}.
Unfortunately, existing works 
\cite{lee18iclr,sabokrou2018adversarially}
have been designed for 
image-wide prediction in small images.
Their adaptation to dense prediction
on Cityscapes resolution
would not be straight-forward
\cite{brock19iclr}.

Soundness of training with negative data
has been challenged by \cite{shafaei2018}
who report under-average results 
for this approach.
However, their experiments average results 
over all negative datasets (including MNIST),
while we advocate for a very diverse  
negative dataset such as ImageNet.



\subsection{Multi-task Training 
  and Dataset Design}

Multi-task models attach 
several prediction heads
to shared features \cite{Caruana1997}.
Each prediction head has a distinct loss.
The total loss is usually expressed 
as a weighted sum \cite{ngiam2011}
and optimized in an end-to-end fashion.
Feature sharing brings important advantages 
such as cross-task enrichment
of training data \cite{bengio13pami}
and faster evaluation.
Examples of successful multi-task models
include combining
depth, surface normals and 
semantic segmentation 
\cite{Eigen2015PredictingDS},
as well as combining
classification, bounding box prediction 
and per-class instance-level segmentation
\cite{mask}.
A map of task compatibility 
with respect to knowledge transfer
\cite{zamir18cvpr}
suggests that many tasks are
suitable for multi-task training.

Dataset quality is  
as a very important issue 
in computer vision research.
Diverse negative datasets 
have been used to reduce false positives
in several computer vision tasks
for a very long time \cite{torralba2011}.
A methodology for analyzing 
the quality of road-driving datasets
has been proposed in \cite{zendel17ijcv}.
The WildDash dataset \cite{zendel18eccv} 
proposes a very diverse validation dataset
and the first semantic segmentation benchmark
with open-set evaluation \cite{scheirer13pami}.

\begin{comment}
\begin{itemize}
    \item outlier detection connected to uncertainty/confidence estimation:
    \begin{itemize}
        \item uncertainty estimated using max softmax \cite{hendrycks17iclr},
        needs calibration \cite{guo17icml} because of overfitting 
        \item improve max softmax with ODIN \cite{liang18iclr}
        \item learn to predict uncertainty
        \cite{kendall17nips,lakshminarayanan17nips} or confidence
        \cite{devries18arxiv}.
        \item total uncertainity is a combination of different types of
        uncertainties. \cite{kendall17nips} proposes two types of uncertainty:
        aleatoric and epistemic.
        \item \cite{smith18uai} proposes a way to calculate aleatoric and
        epistemic uncertainty using output entropy and MC dropout
        \item \cite{lakshminarayanan17nips} uses ensemble of several models
        instead of MC dropout
        \item \cite{Lee2018ASU} uses distance between features of the input sample
        and the closest class-conditional Gaussian distribution (behaves similarly
        to softmax on pixel level when applied to pixel level)
        \item advantage of these approaches - no need to retrain models
    \end{itemize}
    \item outlier detection connected to generative models and single class classification:
    \begin{itemize}
        \item hard for high dimensional data, doesn't scale well, hard to easily
        combine with existing classifiers
        \item \cite{goodfellow14nips} introduces GANs, discriminator can be
        used for outlier detection
        \item \cite{sabokrou2018adversarially} autoencoder in place of the
        generator, it is trained to denoise image.  During evaluation the
        autoencoder enhances inliers while distorting the outliers, making the
        two more separable.
        \item \cite{lee18iclr} train a GAN to generate images on the borders of
        the distribution.
    \end{itemize}
    \item training with outliers:
    \begin{itemize}
        \item \cite{hendrycks19iclr} introduces outliers during training. For
        outliers, KL divergence between model output and uniform distribution is
        minimized
        \item \cite{bevandic18arxiv} show that outlier pixels can be detected using a
        binary classifier that was trained to differentiate between inliers and
        and a larger, more diverse non-traffic scene dataset
        \item \cite{Vyas2018OutofDistributionDU} train an ensemble of classifiers;
        For each classifier some of the classes are ID and some are outlier
    \end{itemize}
    \item multitask DNNs:
    \begin{itemize}
        \item reduces computation, closer to real time application
        \item \cite{Caruana1997} describes the task of learning multiple tasks
        from the same representation
        \item tasks usually done as a weighted sum of losses (e.g. \cite{Eigen2015PredictingDS},
        \cite{ngiam2011})
        \item \cite{zamir18cvpr} show that some visual tasks
        are related and that related tasks can be solved in
        a single system
        \item \cite{dvornik2017blitznet} perform simultaneous
        object detection and segmentation
    \end{itemize}
    \item since the task here is dense prediction in both cases, it is 
    similar to multilabel classification:
    \begin{itemize}
        \item \cite{mask}, Mask R-CNN, performs classification into classes and classification into objects and background
    \end{itemize}
    \item domain shift and fair evaluation:
    \begin{itemize}
        \item \cite{torralba2011} analyzes the problem of dataset bias
        \item \cite{zendel17ijcv} analyze how to analyze the quality of  a driving scenes
        dataset
        \item \cite{zendel18eccv} introduce WildDash based on previous
        findings, wilddash validation is used as ID during testing
        \item wilddash benchmark contains outlier samples
        \item \cite{shafaei2018} define unbiased outlier detector evaluation -  outlier
        dataset used during training not used for outlier evaluation
    \end{itemize}
    \item improving generalization:
    \begin{itemize}
        \item \cite{kreso17cvrsuad}, \cite{orsic2019cvpr}  show that imagenet pretraining  improves
        generalization
        \item jittering during training, l2-regularization
        \item generalization through model design (complicated models
        prone to overfitting)
    \end{itemize}
\end{itemize}
\end{comment}

\section{Simultaneous Segmentation and Outlier Detection}
\label{ss:model}

Our method combines two distinct tasks:
outlier detection and semantic segmentation,
as shown in Fig.\,\ref{fig:approach}. 
We prefer to rely on shared features 
in order to promote fast inference
and synergy between tasks \cite{bengio13pami}.
We assume that a large, diverse and noisy
negative dataset is available
for training purposes
\cite{hendrycks19iclr,bevandic2018}.

\subsection{Dense Feature Extractor}

Our models are based on  
a dense feature extractor
with lateral connections
\cite{kreso17cvrsuad}.
The processing starts with a 
DenseNet \cite{huang17cvpr} or 
ResNet \cite{He2016DeepRL} backbone, 
proceeds with spatial pyramid pooling (SPP) 
\cite{he14eccv,zhao17cvpr}
and concludes with ladder-style upsampling
\cite{kreso17cvrsuad,lintsungyi17cvpr}.
The upsampling path consists of 
three upsampling blocks (U1-U3)
which blend low resolution features
from the previous upsampling stage
with high resolution features from the backbone.
We speed-up and regularize the learning
with three auxiliary classification losses
(cf.~Fig.\,\ref{fig:model}). 
These losses have soft targets corresponding
to ground truth distribution 
across the corresponding window
at full resolution \cite{kreso19arxiv}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{arch2b4.png}
  \caption{The proposed two-head model:
  the classification head recovers semantic segmentation
  while the outlier detection head 
  identifies pixels where 
  semantic segmentation may be wrong. 
The output is produced by combining
  these two dense prediction maps. 
}
  \label{fig:model}
  
\end{figure}

\subsection{Dense Outlier Detection}

There are four distinct approaches to formulate
simultaneous semantic segmentation and
dense outlier detection 
over shared features.
The C-way multi-class approach
attaches the standard classification head 
to the dense feature extractor
(C denotes the number of inlier classes).
The inlier probability 
is formulated as max-softmax.
If a negative set is available,
this approach can be trained 
to emit low max-softmax in outliers
by supplying a modulated 
cross entropy loss term
towards uniform distribution
\cite{lee18iclr,hendrycks19iclr}.
The modulation factor 

is a hyper-parameter. 
Unfortunately, training on outliers 
may compromise classification accuracy 
and generate false positive outliers
at semantic borders.

The C-way multi-label approach 
has C sigmoid heads.
The final prediction is the class 
with maximum probability max-,
whereas the inlier probability 
is formulated as max-.
Unfortunately, this formulation fails 
to address the competition between classes,
which again compromises 
classification accuracy.

The C+1-way multi-class approach 
includes outliers as the C+1-th class,
whereas the inlier probability is
a 2-way softmax between 
the max-logit over inlier classes
and the outlier logit.
To account for class disbalance
we modulate the loss due to outliers
with .
Nevertheless, this loss affects 
inlier classification weights,
which may be harmful 
when the negatives are noisy
(as in our case).

Finally, the two-head approach 
complements the C-way classification head
with a head which directly predicts 
the outlier probability
as illustrated in Fig.\,\ref{fig:model}. 
The classification head is trained on inliers
while the outlier detection head is trained 
both on inliers and outliers.
The outlier detection head uses 
the standard cross entropy loss
modulated with hyper-parameter . 
We combine the resulting prediction maps 
to obtain semantic segmentation 
into C+1 classes.
The outlier detection head overrides 
the classification head
whenever the outlier probability 
is greater than a threshold.
Thus, the classification head is unaffected 
by the negative data,
which provides hope to preserve 
the baseline semantic segmentation accuracy
even when training on 
extremely large negative datasets.

\subsection{Resistance to noisy outlier labels and sensitivity
to negative objects in positive context}

Training outlier detection on 
a diverse negative dataset
has to confront noise 
in negative training data.
For example, our negative dataset, ImageNet-1k, 
contains several classes (e.g.\ cab, streetcar)
which are part of the Cityscapes ontology.
Additionally, several stuff 
classes from Cityscapes
(e.g.\ building, terrain) 
occur in ImageNet-1k backgrounds.
Nevertheless, these pixels 
are vastly outnumbered by true inliers.
This is especially the case
when the training only considers 
the bounding box of the object 
which defines the ImageNet-1k class.


We address this issue 
by training our models 
on mixed batches 
with approximately equal share 
of inlier and negative images.
Thus, we perform many inlier epochs
during one negative epoch,
since our negative training dataset 
is much larger than the inlier ones. 
The proposed batch formation procedure
prevents occasional inliers from negative images
to significantly affect the training.
Additionally, it also favours 
stable development of batchnorm statistics.
Hence, the proposed training approach 
stands a fair chance to succeed.

\begin{comment}
We alleviate influence of noisy outliers
by forming batches with approximately 
equal share of inlier and negative images.
Consequently, a relatively small number 
of negative images with inlier pixels
can not significantly affect the training.
In practice, ImageNet-1k contains 
several classes (e.g.\ cab, streetcar)
which are part of the Cityscapes ontology.
Additionally, several stuff 
classes from Cityscapes
(e.g.\ building, terrain) 
occur in ImageNet-1k backgrounds.
Nevertheless, these pixels 
are vastly outnumbered by true outliers.
Hence, the proposed training approach 
stands a fair chance to succeed.
\end{comment}

We promote detection 
of outlier objects in inlier context
by pasting negative content 
into inlier training images.
We first resize the negative image 
to 5\% of the inlier image,
and then paste it at random.
We perform this before the cropping,
so some crops may contain only inlier pixels.
Unlike \cite{blum19arxiv},
we do not use the Cityscapes ignore class 
since it contains many inliers.



\begin{comment}
In this paper we propose to combine the results of 
two separate tasks - outlier detection and semantic segmentation - 
into a single output. We show that the two tasks may share
convolutional features, as can be seen in Fig.
\ref{fig:approach}.

The model in Fig. \ref{fig:model} is based on ladder densenet
which comprises a backbone (e.g. densenet \cite{huang17cvpr} or
resnet \cite{He2016DeepRL}), spatial
pyramid pooling (SPP) module and an upsampling path.
The upsampling path contains three upsampling blocks.
One upsampling block is fed the outputs of a backbone block 
and the previous upsampling block. These inputs are 
processed and combined into a single output. The output of
the final upsampling block is propagated to outlier detector and
to semantic segmentator.


We use auxiliary losses for segmentation 
along the upsampling path. 
They have soft targets
where the expected value of a pixel
is a distribution calculated
for the corresponding pooling window
of the ground truth.

Semantic segmentation and outlier detection
can be performed by two heads
or by a single head.

The single head can be trained 
only for segmentation.
Max-softmax of the segmentation 
can be used for used as an estimate
of the confidence in the output
of the model. Low confidence can 
indicate that a sample is an outlier.
Using outlier exposure, the model
can be explicitly trained to have low
max-softmax on outlier pixels.
This is done by minimizing
KL distance between uniform distribution
and the output of the model
for outlier pixels.

We can modify the single head by
swapping the softmax at the output of the
network with sigmods. In this setup,
it is possible for a pixel not to 
have any class. Furthermore, for each class,
pixels of all of the other classes,
as well as the outlier images,
can be used as negative examples.

Another possible modification of
the segmentation head for outlier
detection is by adding an additional
"unknown" class.

The model with two heads separates the
two tasks. The outlier detection head can
be implemented as a binary classifier. 
The binary head is trained using both
inlier and outlier images, while
the segmentation head ignores those
pixels.

\end{comment}

\section{Experiments}

We train most models on  
Vistas inliers \cite{neuhold17iccv}
by mapping original labels 
to Citysca\-pes \cite{cordts15cvpr} classes.
In some experiments we also use 
Cityscapes inliers 
to improve results
and explore influence of the domain shift. 
We train all applicable models on outliers 
from two variants of ImageNet-1k: 
the full dataset (ImageNet-1k-full) 
and the subset in which
bounding box annotations
are available (ImageNet-1k-bb).
In the latter case
we use only the bounding box 
for training on negative images 
(the remaining pixels are ignored)
and pasting into positive images.


We validate semantic segmentation 
by measuring mIoU on WildDash val
separately from outlier detection. 
We validate outlier detection
by measuring pixel level 
average precision (AP) 
in two different setups:
i) entire images are either
negative or positive, and
ii) appearance of negative
objects in positive context.
The former setup consists of
many assays
across WildDash val and random
subsets of LSUN images.
The LSUN subsets are dimensioned so that
the numbers of pixels in LSUN and WildDash val
are approximately equal.
Our experiments report mean and standard deviation
of the detection AP across 50 assays.
The latter setup involves WildDash val images
with pasted Pascal animals.
We select animals which take up
at least 1\% of the WildDash resolution,
and paste them at random in each WildDash image.


We normalize all images 
with ImageNet mean and variance,
and resize them so that 
the shorter side is 512 pixels.
We form training batches 
with random 512512 crops
which we jitter with horizontal flipping.
We set the auxiliary loss weight to 0.4 
and the classifier loss weight to 0.6.
We set =0.2, 
=0.05,
and =0.2. 
We use the standard Adam optimizer 
and divide the learning rate 
of pretrained parameters by 4.
All our models are trained 
throughout 75 Vistas epochs,
which corresponds to 
2 epochs of ImageNet-1k-full,  
or 5 epochs of ImageNet-1k-bb.
We detect outliers by thresholding 
inlier probability at .
Our models produce a dense index map
for C Cityscapes classes and 1 void class.
We obtain predictions at 
the benchmark resolution 
by bilinear upsampling.

We perform ODIN inference as follows.
First, we perform the forward pass
and the backward pass with respect 
to the max-softmax activation
(we use temperature T=10).
Then we determine 
the max-softmax gradient 
with respect to pixels.
We determine the perturbation
by multiplying the sign of the gradient 
with =0.001. 
Finally, we perturb 
the normalized input image,
perform another forward pass
and detect outliers according 
to the max-softmax criterion.

\begin{comment}
SiniÅ¡a moved this to section 3

When training with
outliers, we try to make sure that full outlier images make up
half of the batch. We do this by creating a batch with only
outlier images and then swap each of them with an inlier image
with a probability of 0.5. This inlier image can contain
only inlier pixels, but we can also
train with pasting. When training
that way, the ImageNet-1k image selected for swapping is 
rescaled to make its area equal
to 5\% of a randomly selected inlier image area and then pasted
into a random location of that image.

Usages of ImageNet-1k-full and ImageNet-1k-bb also differ slightly. When 
ImageNet-1k-full dataset is used, all of the pixels of the outlier 
image are marked as outlier. If used with pasting
the entire image is pasted into an
inlier image. When ImageNet-1k-bb dataset is used, only the pixels 
inside the bounding box are marked as outlier, the rest are set to 
ignore. If ImageNet-1k-bb is used in tandem with pasting, only
the rectangle containing the object is pasted into the inlier image.
\end{comment}




\subsection{Evaluation on the WildDash Benchmark}
\label{ss:benchmark}





Table \ref{table:bench_results} 
presents our results on 
the WildDash semantic segmentation benchmark, and compares them to other submissions
with accompanying publications.

\setlength{\tabcolsep}{4pt}
\begin{table}[htb!]
\begin{center}
\caption{Evaluation of the semantic segmentation
        models on WildDash bench}
\label{table:bench_results}
\begin{tabular}{|c||c|cccc|c|}
  \hline
  & \multicolumn{1}{c|}{Meta Avg}  
  & \multicolumn{4}{c|}{Classic} 
  & \multicolumn{1}{c|}{Negative}
\\
  \cline{2-7}
  Model
  & mIoU
  & mIoU
  & iIoU
  & mIoU
  & iIoU
  & mIoU\\
 &cla &cla&cla &cat&cat&cla\\
 \hline
 \hline
  \multicolumn{1}{|l||}{APMoE\_seg\_ROB \cite{kong2018pag}} & 22.2 & 22.5 & 12.6 & 48.1 & 35.2 & 22.8\\
  \hline
  \multicolumn{1}{|l||}{DRN\_MPC \cite{Yu2017}} & 28.3 & 29.1 & 13.9 & 49.2 & 29.2 & 15.9 \\
  \hline
  \multicolumn{1}{|l||}{DeepLabv3+\_CS \cite{chen2018encoder}} & 30.6 & 34.2 & 24.6 & 49.0 & 38.6 & 15.7\\ 
  \hline
  \multicolumn{1}{|l||}{LDN2\_ROB \cite{kreso18arxiv}} & 32.1 & 34.4 & 30.7 & 56.6 & 47.6 & 29.9\\
  \hline
  \multicolumn{1}{|l||}{MapillaryAI\_ROB \cite{bulo2017place}} &38.9 & 41.3 & \textbf{38.0} & 60.5 & \textbf{57.6} & 25.0\\
  \hline
  \multicolumn{1}{|l||}{AHiSS\_ROB \cite{meletis2018training}} & 39.0 & 41.0 & 32.2 & 53.9 & 39.3 & 43.6\\
  \hline
  \hline
  \multicolumn{1}{|l||}{LDN\_BIN 
    (ours, two-head)} & 
    41.8 & \textbf{43.8} & 
    37.3 & 58.6 & 53.3 & \textbf{54.3}\\
  \hline
  \multicolumn{1}{|l||}{LDN\_OE 
    (ours, C multi-class)} &
    \textbf{42.7} & 43.3 & 
    31.9 & \textbf{60.7} & 50.3 & 52.8\\
\hline
\end{tabular}
\end{center}
\end{table}



Our two models use the same backbone
(DenseNet-169 \cite{huang17cvpr})
and different outlier detectors.
The LDN\_OE model has 
a single C-way multi-class head.
The LDN\_BIN model has two heads
as shown in Figure \ref{fig:model}. 
Both models have been trained on
Vistas train, Cityscapes train, 
and WildDash val (inliers),
as well as on ImageNet-1k-bb 
with pasting (outliers).
Our models significantly outperform
all previous submissions on negative images,
while also achieving the highest 
meta average mIoU
(the principal benchmark metric)
and the highest mIoU for classic images.
We achieve the second-best iIoU score 
for classic images, which indicates 
underperformance on small objects. 
This is likely due to the fact that
we train and evaluate our models 
on half resolution images. 




\subsection{Validation of Dense 
  Outlier Detection Approaches}

Table \ref{table:OOD_detection} 
compares various approaches 
for dense outlier detection.
All models are based on DenseNet-169,
and trained on Vistas (inliers).
The first section shows the results 
of a C-way multi-class model 
trained without outliers,
where outliers are detected 
with max-softmax \cite{hendrycks17iclr}
and ODIN + max-softmax \cite{liang18iclr}.
We note that ODIN slightly improves 
the results across all experiments.

\begin{table}[htb]
\centering
\caption{Validation of dense 
  outlier detection approaches.
  WD denotes WildDash val.}
\label{table:OOD_detection}
\begin{tabular}{|c|c||c|c|c|}
  \hline
  Model & ImageNet &
    \multicolumn{1}{c|}{AP WD-LSUN} &
    \multicolumn{1}{c|}{AP WD-Pascal} &
    \multicolumn{1}{c|}{mIoU WD}\\
 \hline
 \hline
  \multicolumn{1}{|l|}{
    C multi-class} &
    \ding{55} & & 6.01 & 49.07
    \\
  \hline
  \multicolumn{1}{|l|}{
    C multi-class, ODIN} & 
    \ding{55} &  & 6.92 & \textbf{49.77} 
    \\
 \hline
 \hline
  \multicolumn{1}{|l|}{
    C+1 multi-class}& 
    \ding{51} & 
     & 33.59 & 45.60
    \\
 \hline
  \multicolumn{1}{|l|}{
    C multi-label} & 
    \ding{51} &  & \textbf{57.31} & 42.72
    \\
 \hline
  \multicolumn{1}{|l|}{
    C multi-class} 
    & \ding{51} &  & 41.72 & 46.69 
    \\
 \hline
  \multicolumn{1}{|l|}{two heads} 
    & \ding{51} &  & 46.83 & 47.37 
    \\
\hline
\end{tabular}
\end{table}


\begin{comment}
The outlier exposure model is the segmentation
model trained using outlier exposure.
All of these models use max-softmax as the criterion
for detecting outliers.
C+1 is the segmentation model with the additional class
being the outlier class. The criteria for measuring
AP is the difference between the
probability of the (C+1)st class and max-sofmax.
The sigmoid model is trained using sigmoid instead of 
softmax, which makes it possible for a network
not to classify a sample by having all of the 
outputs below 0.5. The criterion for measuring AP
is max-sigmoid.
The model denoted as two head is the model
that has a second, binary head. The probability of
outlier is used for measuring AP.
\end{comment}

The second section of the table shows 
the four dense outlier detection approaches
(cf.\ Section \ref{ss:model})
which we also train on 
ImageNet-1k-bb with pasting (outliers).
Columns 3 and 4 clearly show
that training with noisy and diverse negatives
significantly improves outlier detection. 
However, we also note a reduction 
of the segmentation score 
as shown in the column 5.
This reduction is lowest for 
the C-way multi-class model 
and the two-head model,
which we analyze next.

The two-head model is slightly worse
in discriminating WildDash val from LSUN,
which indicates that it is
more sensitive to domain shift
between Vistas train and WildDash val.
On the other hand, the two-head model achieves 
better inlier segmentation
(0.7 pp, column 5),
and much better outlier detection
on Pascal animals (5 pp, column 4).
A closer inspection shows 
that these advantages occur
since the single-head C-way approach
generates many false positive
outlier detections at semantic borders
due to lower max-softmax.

The C+1-way multi-class model performs the worst 
out of all models trained with noisy outliers.
The sigmoid model performs 
well on outlier detection
but underperforms on inlier segmentation.



\subsection{Validation of Dense 
  Feature Extractor Backbones}

Table \ref{table:backbone_val} explores 
influence of different backbones
to the performance of our two-head model. 
We experiment with ResNets and 
DenseNets of varying depths.
The upsampling blocks are connected 
with the first three DenseNet blocks,
as shown in Fig.\,\ref{fig:model}.
In the ResNet case, the upsampling blocks 
are connected with the last addition
at the corresponding subsampling level.
We train on Vistas (inliers) 
and ImageNet-1k-bb with pasting (outliers).

\begin{table}[htb]
\centering
\caption{Validation of backbones
  for the two-head model.
WD denotes WildDash val.
  }
\label{table:backbone_val}
\begin{tabular}{|c||c|c|c|}
  \hline
  Backbone & 
    \multicolumn{1}{c|}{AP WD-LSUN } & 
    \multicolumn{1}{c|}{AP WD-Pascal} &
    \multicolumn{1}{c|}{mIoU WD}\\
 \hline
 \hline
  \multicolumn{1}{|l||}{DenseNet-121} &  & 55.84 & 44.75 \\
 \hline
  \multicolumn{1}{|l||}{DenseNet-169} &  & 46.83 & 47.37\\
 \hline
  \multicolumn{1}{|l||}{DenseNet-201} &  &	36.88 &	\textbf{47.59} \\
 \hline
  \multicolumn{1}{|l||}{ResNet-34} &  & 47.24 & 45.17\\
 \hline
  \multicolumn{1}{|l||}{ResNet-50} &  & \textbf{56.18} & 41.65 \\
 \hline
  \multicolumn{1}{|l||}{ResNet-101} &  & 52.02 & 43.67 \\
\hline
\end{tabular}
\end{table}



All models achieve very good 
outlier detection in negative images.
There appears to be a trade-off between
detection of outliers at negative objects
and semantic segmentation accuracy.
We opt for better semantic segmentation results
since WildDash test does not have negative objects
in positive context.
We therefore use the DenseNet-169 backbone 
in most other experiments due to 
a very good overall performance.


\subsection{Influence of the Training Data}

Table \ref{table:dataset_results_in}
explores the influence of 
inlier training data 
to the model performance.
All experiments involve the two-head model 
based on DenseNet-169, which was trained on outliers from ImageNet-1k-bb with pasting.

\begin{table}[htb]
\centering
\caption{Influence of
  the inlier training dataset 
  to the performance of the two-head model 
  with the DenseNet-169 backbone.
  WD denotes WildDash val.
}
\label{table:dataset_results_in}
\begin{tabular}{|c||c|c|c|}
  \hline
  Inlier training dataset &
  AP WD-LSUN  & 
  AP WD-Pascal &
  mIoU WD\\
 \hline
  \hline
  \multicolumn{1}{|l||}{Cityscapes} & 
     & 13.85 & 11.12\\
 \hline
  \multicolumn{1}{|l||}{Vistas} & 
     & 46.83 & 47.17 \\
 \hline
  \multicolumn{1}{|l||}{Cityscapes, Vistas} &
     & \textbf{53.68} & \textbf{47.78} \\
\hline
\end{tabular}
\end{table}


The results suggest that there is 
a very large domain shift between 
Cityscapes and WildDash val.
Training on inliers from Cityscapes 
leads to very low AP scores,
which indicates that many WildDash val pixels
are predicted as outliers
with respect to Cityscapes.
This suggests that Cityscapes 
is not an appropriate training dataset 
for real-world applications.
Training on inliers from Vistas 
leads to much better results
which is likely due to greater variety 
with respect to camera, time of day, 
weather, resolution etc. 
The best results across the board 
have been achieved
when both inlier datasets
are used for training.

Table \ref{table:dataset_results_out} 
explores the impact of  
negative training data.
All experiments feature 
the two-head model with DenseNet-169
trained on inliers from Vistas.

\begin{table}[htb]
\centering
\caption{Influence of the 
  outlier training dataset 
  to the performance of our two-head model 
  with the DenseNet-169 backbone. 
  WD denotes WildDash val.
}
\label{table:dataset_results_out}
\begin{tabular}{|l|c||c|c|}
  \hline
  \multirow{1}*{Outlier training dataset} &
    outlier pasting &
    \multicolumn{1}{c|}{AP WD-Pascal} &
    \multicolumn{1}{c|}{mIoU WD}\\
 \hline
  \hline
  \multicolumn{1}{|l|}{ImageNet-1k-full} &
    no & 2.94 &  43.13\\
 \hline
  \multicolumn{1}{|l|}{ImageNet-1k-full} &
    yes & 45.96 & 43.68\\
 \hline
  \multicolumn{1}{|l|}{ImageNet-1k-bb} &
    yes & \textbf{46.83}  & \textbf{47.17} \\
\hline
\end{tabular}
\end{table}


The table shows that training 
with pasted negatives greatly improves
outlier detection on negative objects.
It is intuitively clear that
a model which never sees a border 
between inliers and outliers 
during training 
does not stand a chance
to accurately locate such borders 
during inference.

The table also shows that ImageNet-1k-bb
significantly boosts inlier segmentation, 
while also improving outlier detection
on negative objects.
We believe that this occurs because
ImageNet-1k-bb has a smaller overlap 
with respect to the inlier training data,
due to high incidence of Cityscapes classes
(e.g. vegetation, sky, road) 
in ImageNet backgrounds.
This simplifies outlier detection 
due to decreased noise in the training set,
and allows more capacity of
the shared feature extractor
to be used for the segmentation task.
The table omits outlier detection 
in negative images, 
since all models 
achieve over 99 \% AP
on that task.







\subsection{Comparing the Two-Head and 
  C-way Multi-class Models}
\label{ss:comparison}

We now compare our two models
from Table \ref{table:bench_results}
in more detail.
We remind that the two models 
have the same feature extractor
and are trained on the same data.
The two-head model performs better 
in most classic evaluation categories 
as well as in the negative category, 
however it has a lower meta average score.

\begin{comment}
\begin{table}[htb]
\centering
\caption{Performance of the bin and oe model
    on wilddash bench}
\label{table:bin_oe}
\begin{tabular}{|c||c|cccc|c|}
  \hline
  & \multicolumn{1}{c|}{Meta Avg}  
  & \multicolumn{4}{c|}{Classic} 
  & \multicolumn{1}{c|}{Negative}
\\
  \cline{2-7}
  Model
  & mIoU
  & mIoU
  & iIoU
  & mIoU
  & iIoU
  & mIoU
\\
 &cla &cla&cla &cat&cat&cla\\
 \hline
 \hline
  \multicolumn{1}{|l||}{two head} & 41.8 & \textbf{43.8} & \textbf{37.3} & 58.6 & \textbf{53.3} & \textbf{54.3}\\
  \hline
  \multicolumn{1}{|l||}{oe} & \textbf{42.7} & 43.3 & 31.9 & \textbf{60.7} & 50.3 & 52.8\\
\hline
\end{tabular}
\end{table}
\end{comment}

Table \ref{table:bin_oe_hazard} explores 
influence of WildDash hazards
\cite{zendel18eccv}
on the performance of the two models. 
The C-way multi-class model
has a lower performance drop 
in most hazard categories. 
The difference is especially large
in images with distortion and overexposure. 
Qualitative experiments show 
that this occurs since 
the two-head model tends 
to recognize pixels in 
images with hazards as outliers
(cf.\ Fig.\,\ref{fig:bench_bin_oe}).


\begin{table}[htb]
\begin{center}
\caption{Impact of hazards
  to performance of our 
  WildDash submissions.
  The hazards are
  image blur,
  uncommon road coverage,
  lens distortion,
  large ego-hood,
  occlusion,
  overexposure, 
  particles,
  dirty windscreen,
  underexposure, and 
  uncommon variations.
  LDN\_BIN denotes 
  the two-head model.
  LDN\_OE denotes
  the C-way 
  multi-class model.
  }
\label{table:bin_oe_hazard}
\begin{tabular}{|c||cccccccccccc|}
  \hline
  \multirow{2}*{Model}& \multicolumn{10}{c|}{
    Class mIoU drop across WildDash hazards
    \cite{zendel18eccv}}
\\
  \cline{2-11}
&blur
  &cov.&dist.&hood
  &occ.
  &over.&part.&screen
  &under.&\multicolumn{1}{c|}{var.}\\\hline
 \hline
  \multicolumn{1}{|l||}{\textsc{ldn\_bin}} &-14\%&-14\%&-22\%&-14\%&\textbf{-3\%}&-35\%&-3\%&-9\%&\textbf{-25\%}&\multicolumn{1}{c|}{-8\%}\\
  \hline
  \multicolumn{1}{|l||}{\textsc{ldn\_oe}} & \textbf{-11\%}&\textbf{-13\%}&\textbf{-7\%}&\textbf{-10\%}&-5\%&\textbf{-24\%}& \textbf{0\%}&\textbf{-6\%}&-30\%&\multicolumn{1}{c|}{\textbf{-7\%}}\\
  \hline

\end{tabular}
\end{center}
\end{table}


Fig.\,\ref{fig:bench_bin_oe} presents
a qualitative comparison of our two 
submissions to the WildDash benchmark.
Experiments in rows 1 and 2 show
that the two-head model 
performs better in classic images
due to better performance on semantic borders.
Furthermore, the two-head model is also better
in detecting negative objects in positive context 
(ego-vehicle, the forklift, and the horse). 
Experiments in row 3 show 
that the two-head model 
tends to recognize all pixels 
in images with overexposure 
and distortion hazards as outliers.
Experiments in rows 4 and 5 show that
the two-head model recognizes 
entire negative images as outliers,
while the C-way single-head model
is able to recognize positive objects
(the four persons) in negative images.
These experiments suggest
that both models are able to detect
outliers at visual classes 
which are (at least nominally)
not present in ImageNet-1k:
ego-vehicle, toy-brick construction,
digital noise, and text.
Fig.\,\ref{fig:bench2_bin_oe} 
illustrates space for further improvement
by presenting a few
failure cases on WildDash test.

\newcommand{\mylen}{0.49\textwidth}
\begin{figure}[htb]
  \centering
  \includegraphics[width=\mylen]{wd0001final.jpg}
  \hfill
  \includegraphics[width=\mylen]{wd0074final.jpg}
  \0.2em]
  \includegraphics[width=\mylen]{wd0033final.jpg}
  \hfill
  \includegraphics[width=\mylen]{wd0056final.jpg}
  \0.2em]
  \includegraphics[width=\mylen]{wd0144final.jpg}
  \hfill
  \includegraphics[width=\mylen]{wd0147final.jpg}
  \caption{Qualitative performance 
  of our two submissions 
  to the WildDash benchmark. 
  Each triplet contains 
  a test image (left),
  the output of the two-head model (center), 
  and the output of the model trained 
  to predict uniform distribution in outliers (right).
  Rows represent inlier images (1),
  outlier objects in inlier context (2), 
  inlier images with hazards (3),
  out-of-scope negatives (4),
  and abstract negatives (5).
  The two-head model produces 
  more outlier detections
  while performing better 
  in classic images 
  (cf.~Table\,\ref{table:bench_results}).
  }
  \label{fig:bench_bin_oe}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\mylen]{wd0002final.jpg}
  \hfill
  \includegraphics[width=\mylen]{wd0076final.jpg}
  \
\label{eq:interpolation}
 P'(Y_{ij} = y_{ij}|\boldsymbol{x}) = c_{ij}P(Y_{ij} = y_{ij}|\boldsymbol{x}) +
 (1-c_{ij})y_{ij}

 \mathcal{L}_\mathrm{MC} = 
-\mathlarger{\sum}\limits_{i,j \in G_x}
 \lambda_{y_{ij}}
  [\![z_{ij}=1 \wedge y_{ij} \leq N_{\mathrm{C}}]\!]
 \log P(Y_{ij} = y_{ij}|\boldsymbol{x}),\\
 P(Y_{ij} = y_{ij}|\boldsymbol{x}) = \frac{\exp s_{y_{ij}}^{ij}(\boldsymbol{x})}{\sum\limits_{c}\exp{s_{c}^{ij}(\boldsymbol{x})}}
 
 \mathcal{L}_\mathrm{ML} = 
-\mathlarger{\sum}\limits_{i,j \in G_x}
 \mathlarger{\sum}\limits_{c=1}^{N_c}
 [\![y_{ij} \leq N_{\mathrm{C}}]\!]
 \biggl(&[\![y_{ij}\neq c \vee z_{ij}=0]\!]
 \log \frac{1}{1+ \exp s_{y_{ij}}^{ij}(\boldsymbol{x})} \\
 + &[\![y_{ij}=c \wedge z_{ij}=1]\!]
 \log \frac{\exp s_{y_{ij}}^{ij}(\boldsymbol{x})}{1+ \exp s_{y_{ij}}^{ij}(\boldsymbol{x})}\biggr)
 
 \mathcal{L}_\mathrm{AUX} =  -\mathlarger{\sum}\limits_{r \in R}\mathlarger{\sum}\limits_{i,j \in G_{x}^{r}} [\![N_{ij}^r>\tfrac{r^2}{2}]\!]
 \mathbb{E}_{\boldsymbol{y}_{ij}^r}[\log P(Y_{ij}^r|\boldsymbol{x})],\\
 y_{ijc}^r  = \frac{1}{N_{ij}^r}
 \mathlarger{\sum}\limits_{l=ir}^{ir+r-1}
 \mathlarger{\sum}\limits_{k=jr}^{jr+r-1}
 [\![y_{kl}=c \wedge y_{kl}\leq N_{\mathrm{C}}]\!],\\
 N_{ij}^r =  
 \mathlarger{\sum}\limits_{l=ir}^{ir+r-1}
 \mathlarger{\sum}\limits_{k=jr}^{jr+r-1}
 [\![y_{kl}\leq N_{\mathrm{C}}]\!]
      
 \mathcal{L}_\mathrm{TH} = 
-\mathlarger{\sum}\limits_{i,j \in G_x}
 [\![z_{ij} \leq 1]\!]
 \log P(Z_{ij} = z_{ij}|\boldsymbol{x})
 
  \mathcal{L}_\mathrm{KL} = 
\mathlarger{\sum}\limits_{i,j \in G_{x}}
  [\![z_{ij}=0]\!]\mathrm{KL} (\mathcal{U} \parallel P(Y_{ij}|\boldsymbol{x}))
  \mathcal{L}_\mathrm{C} = 
-\mathlarger{\sum}\limits_{i,j \in G_{x}}
  [\![y_{ij} \leq N_{\mathrm{C}}]\!]
  \log (c_{ij}|\boldsymbol{x})
  \end{tabular} \\
\hline
\end{tabular}
\end{table}





\clearpage
\begin{table}[htb!]
\centering
\caption{Total training losses}
\label{table:total-losses}
\begin{tabular}{l|c}
  \hline
 Model & Total loss\\
 \hline
  \hline
 C multi-class, C+1 multi-class & 
  
 \\
 C multi-class with confidence head & 
 
 \\
 C multi-label &
 
\\
 Cmulti-class with outliers &
 
\\
 two heads &
  
\\
\hline
\end{tabular}
\end{table}
\subsection{Validation of Dense Oultier Detection Approaches}
Table \ref{table:OOD_detection} is an extension of Table 2
from the main paper. It provides results of outlier detection
and semantic segmentation for a model with a separate
confidence head \cite{devries18arxiv} and a semantic segmentation
model trained using Monte Carlo (MC) dropout after each of the dense
and upsampling layers, with epistemic uncertainty
used as a criterion for outlier detection \cite{kendall17nips,kendall15arxiv}.
The MC-dropout is also used during the inference phase,
meaning that the final output of the model is the mean value of
50 forward passes.
Both of the models trained with
dropout perform outlier detection
better than the baseline model. They, however,
perform worse than any of the
models trained with noisy negatives.
Out of all of the tested models, 
the model with the confidence head
provides the worst
outlier detection accuracy.
We observe a drop in semantic
segmentation accuracy on WildDash 
validation dataset
when compared to the baseline model
for all of the additional models.

\begin{table}[htb!]
\centering
\caption{Validation of dense 
  outlier detection approaches.
  WD denotes WildDash val.}
\label{table:OOD_detection}
\begin{tabular}{|c|c||c|c|c|}
  \hline
  Model & ImageNet &
    \multicolumn{1}{c|}{AP WD-LSUN} &
    \multicolumn{1}{c|}{AP WD-Pascal} &
    \multicolumn{1}{c|}{mIoU WD}\\
 \hline

 \hline




  \hline
  \multicolumn{1}{|l|}{
    C multi-class, dropout 0.2} & 
    \ding{55} &  & 11.87 & 49.13 
    \\
      \hline
  \multicolumn{1}{|l|}{
    C multi-class, dropout 0.5} & 
    \ding{55} &  & 13.18 & 47.25
    \\
   \hline
  \multicolumn{1}{|l|}{
    confidence head} & 
    \ding{55} &  & 4.41 & 45.38
    \\
\hline
\end{tabular}
\end{table}



Figure \ref{fig:pw_ood_det_models} gives a qualitative
insight into the results from Table 2 of the main paper.
It explores validation performance of different
outlier detector variants described in Section 3.2
of the main paper.

Row 1 contains images from WD-Pascal.
Each subsequent row shows the output of a
different outlier detector.
Row 2 corresponds to
the C-way multi-class model
trained without negative samples (baseline). 
This model assigns high
outlier probabilty at semantic borders. 
Consequently it detects borders of outlier patches. 
It is however unable to detect entire outlier objects.
Since the baseline model with ODIN,
the model with the confidence head
and the models trained with dropout
perform qualitatively similarly,
we omit those results for brevity.

Row 3 shows the output of the C+1-way multi-class model.
This model had the lowest AP score. 
Qualitative analysis suggests that this model 
makes more false outlier predictions
which in turn lowers the AP score.

Row 4 shows the C-way multi-label model and row 5
shows the C-way multi-class model. They perform
similarly, though the C-way multi-label model seems
to be more robust to domain shift (cf. column 4).
These models are more successful at outlier detection 
than the baseline model, though they also 
assign a high outlier probability to border pixels.

Row 6 shows the output of the two-head model. 
It is successful at detecting 
outliers without falsely detecting 
borders as outliers. Furthermore, its detections 
tend to be coarser than in other models.

All models trained with ImageNet-1k-bb have a problem
with large domain shift as exemplified in column 4.
This indicates that it would be very hard 
to achieve high AP scores on WD-Pascal
since images with large domain shift are perceived
as equally foreign as the pasted Pascal objects.
Column 3 shows that models trained using negative data
classify the windshield wiper as outlier. This is 
interesting when compared to the model which has 
seen WildDash validation during training (row 3 of 
Figure \ref{fig:bench_bin_oe_outlier_inlier}). This
is further discussed in Section 3.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.24\textwidth]{10final.jpg}
  \includegraphics[width=0.24\textwidth]{117final.jpg}
  \includegraphics[width=0.24\textwidth]{205final.jpg}
  \includegraphics[width=0.24\textwidth]{31final.jpg}
  \caption{Dense outlier detection 
  with models presented in Table 2.
  Row 1 shows Wilddash val images with 
  pasted PASCAL VOC 2007 animals. 
  Row 2 shows outlier probabilities 
  obtained with the C-way multi-class model
  trained without negatives.
  Red colour indicates a high probability 
  that a pixel is an outlier.
  Subsequent rows correspond 
  to models trained with noisy negatives:
  the C+1-way multi-class model (row 3), 
  the C-way multi-label model (row 4), 
  the C-way multi-class model (row 5) and 
  the two-head model (row 6). 
  }
  \label{fig:pw_ood_det_models}
\end{figure}

\subsection{Influence of the Training Data}
Figure \ref{fig:pw_ood_det_in_data} illustrates the
validation performance of the two-head model
depending on the inlier training dataset, which 
was shown in Table 4 of the paper.
Column 1 shows four validation images.

Column 2 presents the corresponding results 
of the two-head model trained on Cityscapes.
This model classifies
all of the WildDash pixels as outliers. 
This indicates that models trained on Cityscapes are
very sensitive to domain shift. 

Column 3 shows that the two-head model trained
on Vistas dataset is significantly better
at outlier detection. This improvement indicates that 
models trained on Vistas show more resilience
to domain shift.

Column 4 depicts a model trained 
on both Vistas and Cityscapes. 
It performs similarly to the model 
trained on Vistas.

Interestingly, the model
trained on Vistas shows more resilience to unusual
conditions (dark image in row 4, unusual vehicle in 
row 3), while the model trained on combined datasets 
shows more precision in normal situations
(grass in row 1, road in row 2).

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.98\textwidth]{0final.jpg}\\
  \includegraphics[width=0.98\textwidth]{148final.jpg}\\
  \includegraphics[width=0.98\textwidth]{149final.jpg}\\
  \includegraphics[width=0.98\textwidth]{190final.jpg}
  \caption{Outlier detection with two-head models
    trained on different inlier datasets.
    All models have been trained 
    with pasted noisy negatives 
    from ImageNet-1k-bb
    as presented in from Table 4.
    Column 1 contains Wilddash images 
    with pasted PASCAL VOC 2007 animals.
    Columns 2-4 show predictions of models
    trained on Cityscapes, Vistas, 
    and Cityscapes and Vistas, respectively.
    Red colour indicates a high probability 
    that a pixel is an outlier.
  }
  \label{fig:pw_ood_det_in_data}
\end{figure}

Figure \ref{fig:pw_ood_det_out_data} shows the
results of the two-head model depending on 
the negative training dataset (cf. Table 5).

The first column shows the validation images.

The second column shows the model trained 
using ImageNet-1k-full without pasting.
It tends to make coarse 
predictions. Consequently it is unable to
detect small outlier
patches.

The third column illustrates the model trained
using ImageNet-1k-full with pasting. It is better
at detecting outlier samples. It is however
sensitive to domain shift. This is because
of the increased overlap between positive and
negative images (in classes such as sky,
vegetation or road which appear often
in the backgrounds of ImageNet images).

The last column shows the model trained using
ImageNet-1k-bb with pasting which performs
the best both qualitatively and quantitatively.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.98\textwidth]{101final.jpg}\\
  \includegraphics[width=0.98\textwidth]{102final.jpg}\\
  \includegraphics[width=0.98\textwidth]{110final.jpg}\\
  \includegraphics[width=0.98\textwidth]{180final.jpg}
  \caption{Outlier detection with two-head models
    trained on different negative datasets.
    All models have been trained 
    by pasting negatives into 
    inliers from Vistas
    as presented in Table 5.
    Column 1 shows original Wilddash images 
    with pasted PASCAL VOC 2007 animals.
    Columns 2-4 show predictions of models
    trained with noisy negatives
    from Imagnet-1k-full (without pasting), 
    ImageNet-1k-full (with pasting), 
    and ImageNet-1k-bb (with pasting),
    respectively.
    Red colour indicates a high probability 
    that a pixel is an outlier.
  }
  \label{fig:pw_ood_det_out_data}
\end{figure}

\subsection{Comparing the Two-Head and 
  C-way Multi-class Models}
Figure \ref{fig:bench_bin_oe_normal} accompanies Table 1
of the main paper. It presents the combined output
of the two-head model (column 2) and the C-way model trained with
ImageNet-1k images (column 3) on images from 
WildDash test set (column 1).

The first four rows show images taken in normal
conditions, while the last two rows
show oulier images.

The C-way model tends to classify
small objects (cf. poles in image in row 3)
as well as distant objects (cf.
trucks in the distance in the image in the third row) 
as outliers. This model makes
more false outlier detections
in typical traffic scenes. 

Furthermore, the two-head model performs 
better in negative images.
Output of the C-way model on negative images
contains small patches not classified as outliers.

\newcommand{\mywt}{0.85\textwidth}
\begin{figure}[htb]
  \centering
  \includegraphics[width=\mywt]{wd0001final.jpg}
  \includegraphics[width=\mywt]{wd0007final.jpg}
  \includegraphics[width=\mywt]{wd0010final.jpg}
  \includegraphics[width=\mywt]{wd0011final.jpg}
  \includegraphics[width=\mywt]{wd0152final.jpg}
  \includegraphics[width=\mywt]{wd0154final.jpg}
  \caption{Qualitative performance 
    on the WildDash benchmark.
    Each triplet contains 
    a test image (left),
    the output of the two-head model (center), 
    and the output of the C-way multi-class model 
    trained to predict uniform distribution 
    in outliers (right).
    The two-head model does not produce
    false positives at semantic borders
    (rows 1-4).
    Both models correctly recognize
    outliers in negative images (rows 5-6).
  }
  \label{fig:bench_bin_oe_normal}
\end{figure}

Figure \ref{fig:bench_bin_oe_hazard} further
clarifies Table 6 from the main paper.
It illustrates impact of WildDash hazards 
on the output of the submitted models.

The results show that the two-head model
is more sensitive to overexposure and distortion
hazards, though it succeeds when the hazard
is not severe (rows 1-4).

Row 5 contains an example of occlusion. Both
of the models are able to successfully 
segment the torso or the person behind the pole but 
they struggle with the lower part.
Row 6 contains an example of the particles hazard
(rain on the windshiled), while
row 7 contains an example of the variation hazard
(with tank truck being an atypical example of a truck).

\begin{figure}[htb]
  \centering
  \includegraphics[width=\mywt]{wd0003final.jpg}
  \includegraphics[width=\mywt]{wd0018final.jpg}
  \includegraphics[width=\mywt]{wd0085final.jpg}
  \includegraphics[width=\mywt]{wd0086final.jpg}
  \includegraphics[width=\mywt]{wd0009final.jpg}
  \includegraphics[width=\mywt]{wd0060final.jpg}
  \includegraphics[width=\mywt]{wd0062final.jpg}
  \caption{Qualitative performance 
    on WildDash test images 
    with overexposure 
    and distortion hazards.
    Each triplet contains 
    a test image (left),
    the output of the two-head model (center), 
    and the output of the C-way multi-class model 
    trained to predict uniform distribution 
    in outliers (right).
    The two-head model rejects
    images with severe hazards.
  }

  \label{fig:bench_bin_oe_hazard}
\end{figure}

Figure \ref{fig:bench_bin_oe_outlier_inlier} expands
on the failure cases shown in Figure 4 of the main paper
which are worth exploring in future work.

The first two rows show images with animals
on roads. Animals are usually classified
as pedestrians. The two-head model classifies most
of the pixels of the image in row 1 as outliers.
Both of the models fail to classify the 
animals as outliers accurately.

The windshield wiper in the row 3 
is classified as an inlier. 
This is because the submitted models were trained 
on Wilddash val which contains examples of images 
with windshields wipers. 
Those pixels are ignored during training but 
they still influence the features 
extracted by the dense feature extractor.
This view is supported by
Figure \ref{fig:pw_ood_det_models}
where images in column 3 demonstrate
that a model trained only on Vistas 
classifies windshield wipers as outliers.
Handling ignored training pixels is a 
suitable direction for future work.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\mywt]{wd0013final.jpg}
  \includegraphics[width=\mywt]{wd0061final.jpg}
  \includegraphics[width=\mywt]{wd0134final.jpg}
  \caption{Qualitative performance 
  of our two submissions 
  to the WildDash benchmark.
  Each triplet contains 
  a test image (left),
  the output of the two-head model (center), 
  and the output of the model trained 
  to predict uniform distribution in outliers (right).
  Rows 1-2 show that our current models
  are unable to correctly detect small outlier objects. 
  Row 3 shows that the windshield wiper
  is recognized as inlier, which occurs
  due to its presence in WildDash val
  (cf.\ Figure \ref{fig:pw_ood_det_models}).
  }
  \label{fig:bench_bin_oe_outlier_inlier}
\end{figure}
\end{document}
