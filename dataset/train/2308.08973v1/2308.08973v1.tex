\pdfoutput=1


\documentclass[10.5pt]{article}

\usepackage{ACL2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{caption}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\setcounter{secnumdepth}{1} \usepackage{enumitem}
\usepackage{colortbl}  \usepackage{xcolor}
\usepackage{array}



\title{Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering}


\author{
  Jiahao Zhang\textsuperscript{\rm \dag} \quad
  Haiyang Zhang\textsuperscript{\rm \dag}\thanks{* Corresponding author} \quad
  Dongmei Zhang\textsuperscript{\rm \dag} \quad
  Yong Liu\textsuperscript{\rm \ddag}\quad
  Shen Huang\textsuperscript{\rm \ddag}\\
  \\
  \textsuperscript{\rm \dag}Beijing University of Posts and Telecommunications, Beijing, China \\
  \small \textsuperscript{}{\{relyourself, zhhy, zhangdm\}@bupt.edu.cn} \\
  \\
  \textsuperscript{\rm \ddag}Tencent Research, Beijing, China \\
  \small \textsuperscript{}{\{owenyongliu, springhuang\}@tencent.com}
}
\begin{document}


\maketitle

\begin{abstract}
Multi-hop QA involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains  multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete QA system, we incorporate a supervised reader or a zero-shot GPT-3.5.  Experimental results demonstrate that Beam Retrieval achieves a nearly 50\% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves (up to 28.8 points) the QA performance of zero-shot GPT-3.5\footnote{Preprint. Under review. Code is available at \url{https://github.com/canghongjian/beam_retriever}}.
\end{abstract}

\section{Introduction}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{images/example_fig3.pdf}
  \caption{An example of multi-hop QA from MuSiQue-Ans benchmark.This complicated 4-hop question requires the model to select relevant passages based on the question and previous chosen passages.}
  \label{fig:examples}
\end{figure}

Question Answering (QA) has been a mainstream research in natural language processing (NLP) for a long time. With the development of pretrained language models (PLMs), simple QA tasks can be solved by adopting a BERT-like PLM \cite{devlin-etal-2019-bert}. As a result, researchers have been increasingly drawn to more complex QA benchmarks, such as multi-hop QA. This presents a significant challenge, as it requires reasoning across multiple and diverse documents to accurately answer complicated multi-hop questions. Many high-quality multi-hop QA datasets have been introduced, such as HotpotQA \cite{yang-etal-2018-hotpotqa}, 2WikiMultiHopQA \cite{DBLP:journals/corr/abs-2011-01060} , MuSiQue \cite{musique} and so on. Figure~\ref{fig:examples} illustrates an example of an actual question taken from MuSiQue-Ans dataset.


Mainstream QA models for multi-hop QA often follow a retrieve-and-read paradigm \cite{retrieving_and_reading}, including a passage retriever to filter out extraneous information and a reader to obtain the final answer \cite{DBLP:conf/aaai/TuHW0HZ20, DBLP:journals/corr/abs-2107-11823, musique, DBLP:journals/corr/abs-2205-11729, DBLP:journals/corr/abs-2212-09512}. However, these methods have primarily focused on two-hop scenarios, exhibiting poor performance in more complex situations and providing low-quality context for downstream QA task.


Previous studies have proposed two types of retrievers for use in reading comprehension settings. One type is one-step methods. SAE \cite{DBLP:conf/aaai/TuHW0HZ20} and MuSiQue SA Selector \cite{musique} concatenate each candidate passage and the question as inputs fed to BERT, then select out the most relevant passages with the highest scores. Such methods do not utilize the dependency between relevant passages, resulting in a limited performance. The other type is two-step methods. S2G \cite{DBLP:journals/corr/abs-2107-11823} and FE2H \cite{DBLP:journals/corr/abs-2205-11729} select the first hop passage in the same way as one-step. At the second stage they identify the second hop relevant passage through pairing the selected passage with the other candidate passages. C2FM \cite{DBLP:journals/corr/abs-2212-09512} selects three passages at the first stage, then it combines them two by two and identifies the true passage pair at the second stage. Notice that C2FM will not utilize the unselected passages in the second stage, leaving limitations in retrieval. These methods show strong performance in retrieval. However, they are customized for two-hop issues in HotpotQA and become inapplicable when faced with datasets involving more hops. Furthermore, two-step methods exhibit limited robustness, as the entire retrieval process is susceptible to failure if the first stage identifies irrelevant passages. 
In conclusion, previous retrievers exhibit poor performance when handling questions with more than 2 hops. 

We observe that auto-regressive language generation is quite similar to multi-hop retrieval. Auto-regressive language models \cite{gpt1,t5,bart} decode next token based on previous generated tokens, while multi-hop retrieval identifies next hop passage based on previous selected passages (and the original question). Naturally we align multi-hop retrieval with language generation by introducing the concept of text decoding. It has been observed that some failures in previous retrieval systems may have been attributed to issues encountered during their early stages. To reduce the risk of missing hidden relevant passages, we employ the beam search paradigm to multi-hop retrieval and present Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA, by keeping track of multiple partial hypotheses of relevant passages at each step. We begin with the multi-hop question and select  passages with the highest scores from the candidate set with  passages, where scores are obtained by a classification head on top of an auto-encoder model. At subsequent hops, it takes  question-passage pairs as new inputs and selects the next hop's  relevant passages with the highest scores calculated by another classification head. This process continues until the last hop is reached. From this we can see Beam Retrieval is a general multi-hop retrieval framework, as it works in scenarios with more than two hops, just like the way as auto-regressive language generation. Furthermore, Beam Retrieval produces multiple, not just one, hypotheses of relevant passages at each step. This approach expands the search space, enhances the probability of obtaining the truly relevant passages, and mitigates the impact of retrieval errors that may occur in the early stages. 

To reduce the gap between training and reasoning, Beam Retrieval is designed to train using the same beam size  as it employs during reasoning. At each hop, Beam Retrieval computes the loss between nearly  hypotheses and the ground-truth relevant passages at each hop. This approach jointly optimizes the encoder and two classification heads by minimizing the combined loss across all hops. In summary, Beam Retrieval produces a chain of relevant passages with the highest score using a single forward pass, effectively learning the entire multi-hop retrieval process. Consequently, it trains and reasons in an end-to-end way.


Beam Retrieval can also serve as a plugin in QA domain, providing high-quality relevant context and enhancing the performance of downstream QA tasks. Based on Beam Retrieval, we implement a multi-hop QA system to extract the answers by incorporating a supervised reader \cite{DBLP:journals/corr/abs-2205-11729, DBLP:journals/corr/abs-2212-09512} or a zero-shot large language model (LLM) \cite{gpt3, gpt4} following conventional machine reading comprehension setting. We validate Beam Retrieval by extensive experiments on three benchmark datasets MuSiQue-Ans, HotpotQA and 2WikiMultihopQA, and experimental results demonstrate that Beam Retrieval surpasses all previous retrievers by a large margin. Consequently, Beam Retrieval substantially improves the QA performance of downstream QA reader on all three datasets.

We highlight our contributions as follows:
\begin{itemize}
\item We propose Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. Beam Retrieval keeps multiple hypotheses of relevant passages at each step and is adapted to question with a variable hop.
\item Our Beam Retrieval performs end-to-end training and inference with the same beam size, which optimizes an encoder and two classification heads by minimizing the combined loss across all hops, reducing the gap between training and reasoning.
\item We evaluate our multi-hop QA system on three multi-hop QA datasets to validate the effectiveness of Beam Retrieval. Beam Retrieval achieves a nearly 50\% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves (up to 28.8 points) the QA performance of zero-shot GPT-3.5.
\end{itemize}

\section{Related Work}

\paragraph{Multi-Hop QA}\label{related_work}
Multi-hop QA requires the model to reason over multiple and different scattered documents to give an answer of a complicated multi-hop question. Many high-quality multi-hop QA datasets have been introduced, such as HotpotQA \cite{yang-etal-2018-hotpotqa}, 2WikiMultiHopQA \cite{DBLP:journals/corr/abs-2011-01060} , MuSiQue \cite{musique} and so on. According to statistics in \cite{multihop_survey}, HotpotQA is the most commonly used dataset in this domain. However, HotpotQA is developed through direct crowdsourcing of 2-hop questions, without taking into account the complexity of composition. As a result, it has been demonstrated that this dataset can be largely solved without employing multi-hop reasoning, thereby enabling models to bypass the need for connected reasoning \cite{min-etal-2019-compositional, chen2019understanding, trivedi-etal-2020-multihop}. 2WikiMultiHopQA shares a similar format with HotpotQA, with the exception of incorporating an additional entity recognition task and extending the scope to include 4-hop questions. They use a limited set of hand-authored compositional rules. In comparison to these datasets, MuSiQue-Ans presents a considerably more challenging task, as it is less susceptible to being solved through disconnected reasoning. This dataset comprises questions that require 2 to 4 hops, further emphasizing its complexity.

\paragraph{Auto-Regressive Language Generation}
In recent years, there has been an increasing interest in open-ended language generation performed by large language models (LLMs) \cite{gpt4,llama}. Generally, LLMs  adopt either an encoder-decoder \cite{flan_t5} or a decoder-only \cite{gpt3} architecture, which generates text by predicting one word or token at a time, conditioning on the previously generated words or tokens. Besides the improved transformer architecture and massive unsupervised training data, better decoding methods have also played an important role. Decoding methods are applied to guide the language generation process and select the most appropriate output sequences based on certain criteria \cite{seq2seq_learning_with_nn, topk_sampling, topp_sampling}.
\section{Preliminary}

\subsection{Basic Decoding Methods}

Auto-regressive language models compute the conditional probability of each word in the target sequence based on the previous words. Let  represent the output of a decoder-only model given the sequence of tokens predicted so far, (), which for notational simplicity we write as . The output  (where  is the cardinality of the enumerated vocabulary )

The probability distribution over the next possible token being word  is the softmax:


Most decoding strategies strive to find the most likely overall sequence, i.e. pick a  such that:

where  denotes the initial context word sequence. Since no sub-exponential algorithm is available for determining the optimal decoded sequence \cite{chen-etal-2018-recurrent}, alternative approximation methods, such as greedy search and beam search, are utilized.

\paragraph{Greedy Search}
Greedy search is the simplest decoding method. It selects the word with the highest probability as its next word:

\paragraph{Beam Search}\label{beam_search}
Beam search approximates finding the most likely sequence by performing breadth-first search over an expanded search space. At time step  in decoding, the method keeps track of  partial hypotheses, denoted as , . The next set of partial hypotheses is chosen by expanding every path from the existing set of  hypotheses, and then choosing the  with the highest scores. Log-likelihood of the partial sequence is used as the scoring function, which is denoted as . This iterative process continues until hypotheses reach the  token or exceed the predefined maximum length.
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/model_structure2.pdf}
  \caption{A visualization of Beam Retrieval with a beam size of 2 for the example in Figure~\ref{fig:examples}. The left part shows how to obtain scores for each hypothesis, where M denotes the number of hypotheses at each hop, L denotes the max length of the hypotheses and h denotes the output dimension of the encoder. The right part shows how Beam Retrieval reasons and trains in an end-to-end way, where the red path refers to the ground-truth relevant passages.}
  \label{fig:framework}
\end{figure*}
\subsection{Problem Formulation}\label{score_function}
Given a -hop question  and a candidate set with  passages as , multi-hop retrieval aims to produce a relevant passages chain (). Most existing work formulates it as a one-step or two-step sequence labeling task, classifying every passage  as relevant or not. However, this method lacks generality and precision.

In contrast, we align multi-hop retrieval task with text decoding, proposing a more general retrieval framework with higher precision. Conceptually, a passage  corresponds to a token  and the question  corresponds to a special start token ``s''. Similarly, we also denote the output of a multi-hop retriever as , given the concatenated sequence of question and passages identified so far, (), which we write as  for short. The output . 

We use an auto-encoder language model as an encoder to derive embeddings for the concatenated sequence (). Subsequently, a fully connected layer is utilized to project the final dimension of the ``[CLS]'' representations of these embeddings into a 2-dimensional space, representing ``irrelevant'' and ``relevant'' respectively. The logit in ``relevant'' side serves as the score for the sequence. This scoring process is denoted by a function , and it is shown in Figure~\ref{fig:framework}.

The probability distribution over the next possible relevant passage being  is the softmax:



We should keep the uniqueness of each passage within the sequence, as there is no duplicated passages in the only one ground-truth relevant passage chain. This requirement differs from the text decoding process, where such uniqueness is not necessarily enforced.

\section{Beam Retrieval}\label{beam_retrieval_method}
Beam Retrieval is designed to handle a -hop multi-hop questions  and accurately selects the most relevant passages, providing nearly noiseless context for downstream QA tasks. In this section, we clarify how Beam Retrieval infers and trains in an end-to-end way, which is illustrated in Figure~\ref{fig:framework}.

\subsection{Scoring}
As described in Section~\ref{score_function}, every hypothesis will be scored at each step during beam search. Beam Retrieval also employs a scoring function  as illustrated in Figure~\ref{fig:framework}, which utilizes an encoder and two classification heads to obtain scores for each hypothesis of passages. At the first hop, for every passage  we concatenate ``[CLS] +  +  + [SEP]'' to the encoder and derive the encoded  representations , where  denotes the length of the concatenated sequence and  denotes the output dimension of the encoder. Then a classification head named ``'' project every  into a 2-dimensional space, representing  ``irrelevant'' and ``relevant'' respectively. We take the logit in ``relevant'' side as the score for the sequence . At subsequent hop , we concatenate ``[CLS] +  +  + ... +  +  + [SEP]'' for every . We use the same encoder but another classification head named ``'' to obtain the score of concatenate sequence  in the same way.  The structures of ``'' and ``'' are totally same, the only difference is ``'' handles a fixed  sequences while ``'' deals with a variable number of sequences in an expanded search space.
\subsection{End-to-End Inference}
Compared with previous customized two-step retrieval methods \cite{DBLP:journals/corr/abs-2107-11823, DBLP:journals/corr/abs-2205-11729, DBLP:journals/corr/abs-2212-09512}, Beam Retrieval employs the beam search paradigm to retrieve multiple relevant passages at each hop, discovering all the relevant passages of  in an end-to-end way. Let  be the predefined beam size. Starting from the question , Beam Retrieval pairs it with  passages in  and scores these  concatenated sequences through the encoder and , choosing the  passages with the highest scores as the first selected passages. At subsequent hop , Beam Retrieval keeps track of  partial hypotheses, denoted as , . Then we concatenate (, , ) for every  as input concatenated sequences. In this way Beam Retrieval expands the search space, producing  hypotheses of passages, where  is slightly less than  as we should keep the uniqueness of each passage within the sequence. Then we score these hypotheses using the encoder and , choosing the  hypotheses with the highest scores. This process continues until the last hop is reached, and we take the passage sequence with the highest score. 

Beam Retrieval finishes the multi-hop retrieval task using a single forward pass, where it calls  times encoder,  time  and  times . Additionally, as we can see in Figure~\ref{fig:framework}, for methods that select only one passage at a time, choosing irrelevant passage at first stage could result in the failure of the entire multi-hop retrieval process.
In conclusion, Beam Retrieval reduces the risk of missing hidden relevant passage sequences by keeping the most likely  hypotheses at each hop and eventually choosing the hypothesis that has the overall highest score.
\subsection{Jointly Optimization}
We jointly optimize the encoder,  and  across all hops in an end-to-end manner. Let  be the ground truth relevant passages. At the first hop, the loss can be represented as:


where  is the label of  and  is the score function described in Section~\ref{score_function}. 
At subsequent hop , the loss can be represented as:

where  is the label of . It is important to note that not all datasets offer the ground-truth relevant passage for each hop. Consequently, for  we define  under two scenarios: one with a provided order of relevant passages and another without a specified order. If the order of ground-truth relevant passages is given,  is set as: 


Otherwise  is set as:


Hence the overall training loss of Beam Retrieval can be:


\section{Experimental Setup}
\subsection{Datasets}
We focus on the retrieval part of Multi-hop QA and conduct experiments on three benchmark datasets MuSiQue-Ans \cite{musique}, distractor-setting of HotpotQA \cite{yang-etal-2018-hotpotqa} and 2WikiMultihopQA \cite{DBLP:journals/corr/abs-2011-01060}. MuSiQue-Ans, HotpotQA, 2WikiMultihopQA have 20K, 90K and 167K training instances, respectively. MuSiQue-Ans requires model to answer the complicated multi-hop questions, while HotpotQA and 2WikiMultihopQA additionally require model to provide corresponding supporting sentences. In the setting of Beam Retrieval augmented LLM, we evaluate our method on the partial part of three multi-hop datasets, where we use the 500 questions for each dataset sampled by \cite{trivedi-etal-2023-interleaving}. 

HotpotQA and 2WikiMultihopQA share a similar format and have 2-hop and 2,4-hop questions respectively. In both datasets, each question is accompanied by 10 passages, where only a few of them (2 in HotpotQA and 2 or 4 in 2WikiMultihopQA) are relevant to the question. Futhermore, 2WikiMultihopQA has entity-relation tuples support, but we do not use this annotation in our training or evaluation. Main experiments are conducted on MuSiQue-Ans, which has 2,3,4-hop questions and is more challenging, as it requires explicit connected reasoning \cite{musique}.





\subsection{Models}
\subsubsection{Beam Retrieval}
Beam Retrieval selects all the relevant passages in an end-to-end way. We employ the base and the large version of DeBERTa \cite{deberta} as our encoder. We use a single RTX4090 GPU and set the number of epochs to 16 and the batch size to 1 (here batch size means the number of examples taken from dataset, and the actual batch size is the hypotheses number ). Owing to our multiple calls of encoder during training, we set gradient checkpointing to True, otherwise it requires huge amount of memory. We use BERT-Adam with learning rate of 2e-5 for the optimization and set the max position embeddings to 512. Considering the long concatenated sequences, we adopt a truncation method. If the total length exceeds the max length, we calculate the average length of each passage and truncate the extra part. In addition, for effective training, we will stop the loss calculation if all the hypotheses do not hit the ground truth. To enhance the robustness of model, we shuffle the inner order of the concatenated passages within the hypothesis.

\subsubsection{Downstream Reader}
We implement a downstream reader to receive the retrieved relevant passages as the context , and we concatenate input ``[CLS] +  + [SEP] +  + [SEP]'' to feed our reader. Specifically, we conduct experiments with two types of reader: supervised setting and zero-shot LLM setting.

(i) \textbf{Supervised Reader} 
For MuSiQue-Ans dataset, we train a reading comprehension model following BertForQuestionAnswering \cite{devlin-etal-2019-bert, hugging_face}. For HotpotQA and 2WikiMultihopQA, we train a multi-task reader which extracts the answer and the supporting facts of the question, following FE2H \cite{DBLP:journals/corr/abs-2205-11729} and C2FM \cite{DBLP:journals/corr/abs-2212-09512}, where you can refer to Appendix~\ref{appendix_a} for details. 
For supervised setting, we employ the large version of DeBERTa for MuSiQue and 2WikiMultihopQA and the xxlarge version of DeBERTa for HotpotQA. We use a single RTX4090 GPU to train the large version reader and a single A100 to train the xxlarge version reader. We set the number of epochs to 12 and the batch size to 4. We use BERT-Adam with learning rate of 5e-6 for the optimization and set the max position embeddings to 1024. To enhance the robustness of model, we shuffle the inner order of the concatenated passages within the context. 

(ii)\textbf{Zero-Shot LLM}
In addition to the supervised reader above, we also incorporate a zero-shot LLM as the downstream reader to benchmark the QA performance of Beam Retrieval augmented LLM. For zero-shot LLM setting, we use \emph{gpt-3.5-turbo} provided from API of OpenAI\footnote{\url{https://openai.com/api/}}. We use the template described in Appendix~\ref{appendix_b} to obtain the answers directly.
\subsection{Evaluation Metrics}
Generally, we use Exact Match (EM) and F1 score to evaluate the retrieval performance. Retrieval EM means whether the passage-level prediction is totally same as the ground truth, while retrieval F1 is the harmonic mean of precision and recall, and both of them are irrespective of the inner order between relevant passages. In retrieve-and-read setting, retrieval EM is particularly critical, as missing relevant passages can significantly impact the performance of downstream reader. 

For MuSiQue-Ans, we report the standard F1 based metrics for answer (\textbf{An}) and support passages identification (\textbf{Sp}). Actually, \textbf{Sp} F1 in MuSiQue-Ans is equivalent to retrieval F1. For HotpotQA and 2WikiMultihopQA, we report the EM and F1 metrics for answer prediction task (\textbf{Ans}) and supporting facts prediction task (\textbf{Sup}). In Beam Retrieval augmented LLM setting, we report the answer F1.

\section{Results}


\paragraph{Appropriate Beam Size}
We first explore the influence of different beam size on MuSiQue-Ans dataset, as shown in Table~\ref{tab:beam_size}, where the encoder is base version. Interestingly, Beam Retrieval performs well even with a beam size of 1, and a beam size of 2 yields the most benefits, which is consistent with \cite{seq2seq_learning_with_nn}. It is worth mentioning that in our experimental setting, the candidate set size  ranges from 10 to 20. As the beam size expands, both the necessary training memory and training duration increase rapidly. For instance, a beam size of 4 demands approximately double the memory and triple the training duration in comparison to a beam size of 1. Due to these considerations, we do not conduct experiments with a beam size larger than 4. In conclusion, we employ beam sizes of 1 and 2 for Beam Retrieval in our subsequent experiments.
\begin{table}[htb]
\centering
\begin{tabular}{ccccc}
\hline
beam size & \textbf{EM}    & \textbf{F1}    & \textbf{Mem} (\%)    & \textbf{Speed} (\%) \\
\hline
1         & 74.18 & 87.46 & 100\% & 100\% \\
2         & \textbf{75.47} & \textbf{88.27} & 119\% & 58\% \\
3         & 74.56 & 87.84 & 150\%   & 42\% \\
4         & 74.43 & 87.65 & 194\%   & 36\% \\
\hline
\end{tabular}
\caption{Influence of different beam size among retrieval performance, training memory required and training speed. A beam of size 2 offers the optimal balance between retrieval performance and training costs.}
\label{tab:beam_size}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{cl|cc}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\ \cline{3-4} 
\multicolumn{2}{c|}{}                             & \textbf{EM}    & \textbf{F1}    \\ \hline
\rowcolor{gray!10} \multicolumn{4}{c}{\textbf{\textsl{MuSiQue-Ans}}} \\ \hline
                    & EE \cite{musique}               
                    & 21.47 & 67.61   \\
                   & SA \cite{musique}               
                    & 30.37 & 72.30         \\
& Ex(EE) \cite{musique}               
                    & 48.78 & 77.79      \\
& Ex(SA) \cite{musique}                
                    & 53.50 & 79.24     \\
& Beam Retrieval, beam size 1 & 77.37 & 89.77       \\ 
                    & Beam Retrieval, beam size 2 & \textbf{79.31} & \textbf{90.51}       \\ 
                    \hline
                    \rowcolor{gray!10} \multicolumn{4}{c}{\textbf{\textsl{HotpotQA}}} \\ \hline
&SAE \cite{DBLP:conf/aaai/TuHW0HZ20}     & 91.98     & 95.76  \\
        &SA Selector* \cite{musique}               & 93.06 & 96.43         \\
        &S2G \cite{DBLP:journals/corr/abs-2107-11823}  & 95.77     & 97.82 \\
&FE2H \cite{DBLP:journals/corr/abs-2205-11729} & 96.32     & 98.02 \\
        &C2FM \cite{DBLP:journals/corr/abs-2212-09512} & 96.85     & 98.32 \\
        &Beam Retrieval, beam size 1 & 97.29 & 98.55 \\
        &Beam Retrieval, beam size 2 & \textbf{97.52} & \textbf{98.68} \\
        \hline
        \rowcolor{gray!10} \multicolumn{4}{c}{\textbf{\textsl{2WikiMultihopQA}}} \\ \hline
        &SA Selector* \cite{musique}               & 98.25 & 99.13         \\
&Beam Retrieval, beam size 1 & \textbf{99.93} & \textbf{99.96} \\
                    \hline
\end{tabular}
\caption{Retrieval performance on the development set of MuSiQue-Ans, HotpotQA, 2WikiMultihopQA in comparison with previous work. SA Selector* indicates that we reproduce SA Selector by training it on the full HotpotQA and 2WikiMultihopQA. Beam Retrieval surpasses all previous retrievers by a large margin.}
\label{tab:retr_performance}
\end{table}

\begin{table*}[htb]
\centering
\begin{tabular}{cc|ccccc}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Methods}}}  & \multicolumn{2}{c}{\textbf{Answer}} & \multicolumn{2}{c}{\textbf{Supporting}} \\ \cline{3-6} 
\multicolumn{2}{c|}{}                             & \textbf{EM}    & \textbf{F1}    & \textbf{EM}    & \textbf{F1}   \\ \hline
\rowcolor{gray!10} \multicolumn{6}{c}{\textbf{\textsl{HotpotQA dev set}}} \\
\hline
                    & DFGN \cite{qiu-etal-2019-dynamically}                      & 55.42 & 69.23 & -  & -   \\
                    & HGN \cite{fang-etal-2020-hierarchical}                         &   -    &    82.22   & -      &  88.56        \\
                  & SAE \cite{DBLP:conf/aaai/TuHW0HZ20}                         & 67.70      & 80.75      &63.30       &87.38         \\
                   & S2G \cite{DBLP:journals/corr/abs-2107-11823}                         &      70.8 &      - &      65.7 &  -      \\
                & FE2H \cite{DBLP:journals/corr/abs-2205-11729}                        &      71.72 &  84.61     &  66.12     & 89.65         \\
                    & C2FM \cite{DBLP:journals/corr/abs-2212-09512}                        &      71.90 &     84.65  &   66.75    & 90.08         \\
                    & Beam Retrieval, beam size 1 & 72.09      &    85.19   &  67.06     &  90.29        \\
                    & Beam Retrieval, beam size 2 & \textbf{72.25} & \textbf{85.30} & \textbf{67.25}    &  \textbf{90.43}       \\ \hline
                    \rowcolor{gray!10} \multicolumn{6}{c}{\textbf{\textsl{2WikiHotpotQA test set}}} \\ \hline
 & CRERC \cite{CRERC}                      & 69.58 & 72.33 & 82.86  & 90.68   \\
                    & NA-Reviewer \cite{NA-Reviewer}                      &   76.73    & 81.91      &  89.61     &  94.31        \\
                    & BigBird-base model \cite{ho-etal-2023-analyzing}                         &  74.05     &   79.68    &  77.14     &  92.13        \\
                    & Beam Retrieval, beam size 1                        &   \textbf{88.47}    &    \textbf{90.87}   &   \textbf{95.87}    &  \textbf{98.15}        \\
                    \hline
\end{tabular}
\caption{Overall performance on the development set of HotpotQA and the test set of 2WikiMultihopQA in comparison with previous work. `-': score is unavailable. Beam Retrieval achieves SOTA in both datasets}
\label{tab:total_performance_HQ_2W}
\end{table*}

\paragraph{Beam Retrieval Performance}
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\linewidth]{images/gpt3_withBR.png}
  \caption{Answer F1 for \emph{gpt-3.5-turbo} under two conditions on three multi-hop datasets. Beam Retrieval substantially improves the zero-shot QA performance of LLM, which is even comparable to some supervised methods.}
  \label{fig:exp_gpt3}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{cl|cc}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{MuSiQue-Ans}} \\ \cline{3-4} 
\multicolumn{2}{c|}{}                             & \textbf{An}    & \textbf{Sp}    \\ \hline
                    & EE \cite{musique}               
                    & 40.7 & 69.4   \\
                   & SA \cite{musique}               
                    & 52.3 & 75.2         \\
& Ex(EE) \cite{musique}               
                    & 46.4 & 78.1      \\
& Ex(SA) \cite{musique}                
                    & 49.0 & 80.6     \\
& Beam Retrieval, beam size 1 & 66.9 & 90.0       \\ 
                    & Beam Retrieval, beam size 2 & \textbf{69.2} & \textbf{91.4}       \\ 
                    \hline
\end{tabular}
\caption{Overall performance on the test set of MuSiQue-Ans. Beam Retrieval achieves a new state-of-the-art.}
\label{tab:total_performance_musique}
\end{table}


We compare our Beam Retrieval with previous retrievers on three multi-hop datasets, as shown in Table~\ref{tab:retr_performance}. Beam Retrieval achieves new SOTA performance across all datasets, significantly outperforming existing methods even when using a beam size of 1, and notably attaining a nearly 50\% EM improvement (from 53.50 to 77.37\footnote{MuSiQue-Ans leaderboard: \url{https://leaderboard.allenai.org/musique_ans/submissions/public}}) on challenging MuSiQue-Ans. This result highlights the effectiveness of our proposed approach in handling more complex situations. As demonstrated in Table~\ref{tab:beam_size}, employing a beam size of 2 consistently improves performance on both MuSiQue-Ans and HotpotQA datasets, validating the benefits of an expanded search space. As the high-performance retrievers in HotpotQA are customized for two-hop issues, we do not reproduce them for the other two datasets. A large version encoder is employed for all datasets except 2WikiMultihopQA, where a base version encoder achieves a remarkable 99.9\% retrieval precision. Therefore we do not conduct further experiments with larger beam sizes or encoders for this dataset. 



\paragraph{Downstream QA Performance}
Table~\ref{tab:total_performance_musique} and Table~\ref{tab:total_performance_HQ_2W} compare multi-hop QA performance between Beam Retrieval augmented supervised reader (hereinafter referred to as Beam Retrieval) and other strong multi-hop systems across three datasets. Thanks to the retrieved high-quality context, Beam Retrieval with beam size of 2 achieves new SOTA on all three datasets. Specifically, on MuSiQue-Ans our Sp performance (91.4) is comparable to Human Score (93.9) reported in \cite{musique}. To evaluate the degree of enhancement Beam Retrieval can provide, we compare the QA performance of zero-shot GPT-3.5 under two conditions: one using all candidate passages (referred to as ``without BR"), and the other incorporating relevant passages retrieved by Beam Retrieval with beam size of 2 (referred to as ``with BR"), which is depicted in Figure~\ref{fig:exp_gpt3}. Beam Retrieval significantly boosts the zero-shot QA performance of LLM, yielding a 28.8-point improvement on the challenging MuSiQue-Ans, a 20.4-point improvement on HotpotQA, and a 15.5-point improvement on 2WikiMultihopQA.
\paragraph{Ablation Study}
\begin{table}[htb]
\centering
\begin{tabular}{cl|cc}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\ \cline{3-4} 
\multicolumn{2}{c|}{}                             & \textbf{EM}    & \textbf{F1}    \\ \hline
& Beam Retrieval       & 74.18 & 87.46   \\ 
                    & Beam Retrieval       & \textbf{75.47} & \textbf{88.27}   \\ 
                    & Beam Retrieval       & 74.56 & 87.84   \\ \hline
                    \rowcolor{gray!10} \multicolumn{4}{c}{\textbf{\textsl{w/o Consistent Beam Size}}} \\ \hline
                   & Beam Retrieval       & 74.31 & 87.84   \\
                    & Beam Retrieval       & 74.06 & 87.67   \\
                    & Beam Retrieval       & 75.13 & 88.17   \\ \hline
                    \rowcolor{gray!10} \multicolumn{4}{c}{\textbf{\textsl{w/o 2 Classfication Heads}}} \\ \hline
                    &BR with 4 Classfication Heads & 72.16 & 87.04   \\
                    &BR with 1 Classfication Head & 73.11 & 87.32   \\
                    \hline
\end{tabular}
\caption{Ablation study results on MuSiQue-Ans dataset. The subscript  indicates training with beam size  and reasoning with beam size .}
\label{tab:ablation}
\end{table}
To understand the strong performance of Beam Retrieval, we perform an ablation study by employing inconsistent beam sizes between training and reasoning and using different numbers of classification heads, as illustrated in Table~\ref{tab:ablation}. Performance declines when the training beam size differs from the reasoning beam size, and it drops more sharply as the gap between training and reasoning widens. We do not investigate situations where the reasoning beam size exceeds the training beam size, as it is evident that model cannot perform hard reasoning after easy training. We also vary the number of classification heads to verify if two heads are the optimal setting. First we use 4 classification heads as there are up to 4-hop questions and we arrange one head for one hop, however it results in a 2-point decrease in EM. Then we employ a unified classification head, which also leads to a one-point performance drop. These results confirm that using one head for the first hop and another head for subsequent hops is the best configuration.

\section{Conclusion}
We present Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Experimental results on three benchmark datasets prove the effectiveness of Beam Retrieval and demonstrate it could substantially improve the QA performance of downstream reader. In general, Beam Retrieval establishes a strong baseline for complex multi-hop QA, where we hope that future work could explore more advanced solutions. 

\bibliography{custom,anthology}
\bibliographystyle{acl_natbib}
\newpage
\appendix

\section{Multi-Task Supervised Reader}\label{appendix_a}
After receiving the relevant passages () from the retriever, our reader is expected to complete both the answer prediction task and the supporting facts prediction task. Following SAE \cite{DBLP:conf/aaai/TuHW0HZ20} and C2FM \cite{DBLP:journals/corr/abs-2212-09512}, we also implement a multi-task model to extract the answer and the supporting facts, jointly training the answer prediction and supporting sentence classification in a multi-task learning way.

We define three types of tasks: supporting facts prediction, answer type prediction, and answer span prediction. Following C2FM, we incorporate a special placeholder token ``d'' before each document title and a token ``e'' before each sentence to provide additional information and guide the model to predict at the sentence level.

We concatenate the question and the retrieved passage chain () as ``[CLS] + question + [SEP] +  +   + ... +  + [SEP]''. We denote the BERT-like PLM output as  where  is the length of the input sequence and  is the hidden dimension of the backbone model. For answer type prediction, we perform a 3-class ("Yes", "No" and "Span") classification, with the corresponding loss item denoted as  . To extract the supporting facts prediction, we apply a linear layer on  to classify each sentence as either a supporting facts sentence or not (using the sentence token ``e''), with its corresponding loss item denoted as . Similarly, we employ another linear layer to project  and identify the start and end positions of the answer, denoting the start position loss and the end position loss as  and , respectively, as introduced in BERT~\cite{devlin-etal-2019-bert}. Finally, the total answer span loss  is described using the following formulas.


where  is 0.5 in our setting. Formally, the total loss  can be jointly calculated as:


where  is 0.2 and  are 1 in our setting. Here each loss function is the cross-entropy loss.

\section{Zero-Shot GPT-3.5 Prompt}\label{appendix_b}
\begin{quote}\begin{scriptsize}\begin{verbatim}
You are a qa test machine, you need to answer the 
[Question] from the given [Context], you only need to 
come out the correct answer without any other words.
[Question]:
[Context]:
\end{verbatim}\end{scriptsize}\end{quote}

\end{document}
