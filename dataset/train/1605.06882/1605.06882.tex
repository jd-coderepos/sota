\documentclass[11pt]{article}
\usepackage[hmargin=1in,vmargin=0.95in]{geometry}
\usepackage{pdfpages}
\usepackage{color}
\usepackage{fixltx2e} \usepackage{mparhack} \usepackage{epsfig}
\usepackage[english]{babel}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{xcolor}
\usepackage{fmtcount} \usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{color}
\newcommand{\BO}{\mathcal{O}}



\usepackage{graphicx}
\usepackage{psfrag}


\renewcommand{\baselinestretch}{.96}
\newcommand{\graph}{G(\Gamma, d, p)}
\newcommand{\ElkinGraph}{\graph}
\newcommand*\eps{\varepsilon}
\renewcommand{\algorithmiccomment}[1]{// #1}

\def\cP{\mathcal{P}}
\def\cH{\mathcal{H}}
\def\cA{\mathcal{A}}
\def\cT{\mathcal{T}}
\def\cD{\mathcal{D}}
\def\cC{\mathcal{C}}
\def\cG{\mathcal{G}}
\def\cJ{\mathcal{J}}
\def\cF{\mathcal{F}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{main}[theorem]{Main Theorem}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{problem}[theorem]{Problem}


\newtheorem{defi}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

\newcommand{\diam}{\mathop{\mathrm{diam}}}
\newcommand{\maxpath}{\mathop{\mathrm{max\_path}}}
\newcommand{\disj}[1]{\mathop{\mathrm{disj_{#1}}}}
\newcommand{\length}{\mathop{\mathrm{length}}}
\newcommand{\dist}{\mathop{\mathrm{dist}}}
\newcommand{\path}{p}
\newcommand{\loong}{p_l}
\newcommand{\todo}[1]{{\begin{small}\sffamily \color{red}TODO:  #1 \end{small}}}
\newcommand{\todoI}[1]{}\newcommand{\blueI}[1]{}

\newcommand{\tsp}{\mathop{tsp}}
\newcommand{\tspapp}{\mathop{\overline{tsp}}}
\newcommand{\dep}{\mathop{dep}}
\newcommand{\depapp}{\mathop{\overline{dep}}}
\let\originalepsilon\epsilon
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\onlyThesis}[1]{{ \color{blue} #1 }}
\newcommand{\onlyPaper}[1]{{ \color{green} #1 }}
\newcommand{\blue}[1]{{ \color{blue} #1 }}
\newcommand{\green}[1]{{ \color{green} #1 }}


\def\EQ{\mbox{\tt EQ}}
\def\EQfunc{\mbox{\tt eq}}
\def\DISJ{\mbox{\tt DISJ}}
\def\DISJfunc{\mbox{\tt disj}}
\sloppy



\makeatletter
\newcommand*{\StartNewContent}{\let\OrigLabel\label \let\OrigRef\ref \renewcommand{\label}[1]{\OrigLabel{FULL:##1}}\renewcommand{\ref}[1]{\OrigRef{FULL:##1}}\renewcommand{\label@in@display}[1]{\ifx\df@label\@empty\else
            \@amsmath@err{Multiple \string\label's:
                label '\df@label' will be lost}\@eha
        \fi
        \gdef\df@label{FULL:##1}}}
\makeatother

\newif\iffull
\newif\ifshort





\begin{document}


\author{Benjamin Dissler ~~~~~~  Stephan Holzer\thanks{Part of the work was done at ETH Zurich. At MIT the author was supported by the following grants: AFOSR Contract Number FA9550-13-1-0042, NSF Award 0939370-CCF, NSF Award CCF-1217506, NSF Award number CCF-AF-0937274.}  ~~~~~~ Roger Wattenhofer\\
~~\small disslerb@ethz.ch ~~~~~~~~~~~ \small holzer@mit.edu  ~~~~~~~~~  \small wattenhofer@ethz.ch\\
  \small ETH Zurich  ~~~~~~~~~~~~~~~~~~~~~~  MIT  ~~~~~~~~~~~~~~~~~~~~~~~  ETH Zurich\\
}

\title{
Distributed Local Multi-Aggregation and Centrality Approximation
}
\maketitle
\thispagestyle{empty}
\begin{abstract}
We study local aggregation and graph analysis in distributed environments using the message passing model. We provide a flexible framework, where each of the nodes in a set --which is a subset of all nodes in the network--can perform a large range of common aggregation functions in its -neighborhood. We study this problem in the CONGEST model, where in each synchronous round, every node can transmit a different (but short) message to each of its neighbors. While the -neighborhoods of nodes in  might overlap and aggregation could cause congestion in this model, we present an algorithm that needs time  even when each of the nodes in  performs a different aggregation on its -neighborhood. The framework is not restricted to aggregation-trees such that it can be used for more advanced graph analysis. We demonstrate this by providing efficient approximations of centrality measures and approximation of minimum routing cost trees.
\end{abstract}

\clearpage
\pagestyle{plain}
\setcounter{page}{1}






\newpage
\fulltrue 



\section{Introduction}
Data aggregation and analysis is one of the most basic tasks at the heart of many distributed systems and the question of aggregating and analyzing information and networks itself as efficient as possible arises daily. The result of this is a huge body of work ranging from theoretical to practical aspects focusing on optimizing e.g. speed, space, bandwidth, energy, fault-tolerance and accuracy. As already \cite{kuhn2008distributed} stated,``the database community classifies aggregation functions
into three categories: \emph{distributive} (max, min, sum, count), \emph{algebraic} (plus, minus, average, variance), and holistic (median,  smallest, or  largest value). Combinations of these functions are believed to support a wide range of reasonable aggregation queries.''. Often one is also interested in computing a combination of these such as, e.g., ``What is the average of the 10\% largest values?''\cite{kuhn2008distributed}.

However, most of this work considers the case in which only one node in the network aggregates information (and then often broadcasts it to all other nodes). In reality, many nodes of a large network are independently interested in aggregating data at the same time 

and restricted to their local neighborhood. That is, all nodes in a subset  want to perform a (possibly different) aggregation, where  is the set of nodes in the network.  For example 1) a few nodes in a network want to know if certain data is stored in their neighborhood, or 2) cars participating in a vehicular ad hoc network (VANET) want to aggregate information on traffic, safety warnings and parking spots from their local neighborhood, and 3) nodes who turned idle search for the busiest nodes in their respective neighborhoods to take work from them. A simple approach is to just let all nodes in  perform their aggregation at the same time---however, this might lead to congestion and a worst case runtime of . 


In this paper we present a general framework for local multi-aggregation that allows a time- and message-optimal implementation of a wide range of aggregation functions. We do this in a general setting, where nodes in  aggregate data from their -neighborhood using shortest paths and achieve a runtime of  to do so. 


We show how to perform aggregation and graph analysis by aggregating through all possible shortest paths between any pair of nodes (not only using one path as it is usually done in an aggregation-tree). This is a powerful tool which enables us to provide an efficient approximation of Betweenness Centrality. We also present efficient approximations of Closeness Centrality and Minimum Routing Cost Trees. 


\subsection{Our Contribution}
We provide a framework for local multi-aggregation that takes care of scheduling messages sent between nodes in an efficient way. One only needs to specify how nodes process incoming messages. That is designing algorithms depending on the aggregation-function at hand, which transform received messages into new messages to be sent. Using our framework, one can aggregate information not only by using a tree, but using all possible shortest paths from a root node to any other node. This has advantages for advanced computations as we demonstrate later. Thus we show two variations of the algorithm:
\begin{enumerate}
\item only one (shortest) --path is needed for each  to perform the aggregation.
\item all shortest --paths are needed for each  to perform the aggregation.
\end{enumerate}
The last version is e.g. of interest for computing betweenness centrality measures, which is a measure that depends on all shortest paths, not only a single one. \footnote{In the algorithm we approximate the number of all shortest paths starting in a certain sampled set  of nodes. Note that it does not seem possible to approximate centrality measures without performing these  independent aggregations.}. 

To perform  independent (possibly different) aggregations in the -neighborhood of each of the nodes in , our algorithm takes time , which we show to be optimal (see Remark \ref{rem:lowerbound}). As an example of an aggregation-function, which can be plugged into our framework, we consider computing the maximum value
\todoI{more fkt.}
stored in the -neighborhood of each of the  nodes. Further aggregation functions can be implemented in a similar way. Different root nodes in  can perform different aggregations at the same time. Based on this, we show how to approximate centrality measures and minimum routing cost trees and obtain the following theorems:

\begin{theorem}\label{theo:DLGruntime}
Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} computes  valid directed leveled graphs with depth , in time . And Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} aggregates information through  directed leveled graphs with depth , in time .
\end{theorem}

\begin{theorem}\label{theo:tree}
The tree variations of Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} compute  trees  in time  and aggregate information through these  trees, in time .
\end{theorem}

\begin{theorem}\label{theo:bcapprox}
\ifshort
One can compute 
\fi
\iffull
Algorithm \ref{alg:BC_setup_controlling} computes 
\fi
an estimation of the betweenness centrality for at least  nodes. For all those nodes holds: for  and , if the betweenness centrality of a node  is  for some constant , then with probability at least  its betweenness centrality can  -approximated with computation of  samples of source nodes. 
\ifshort
This algorithm 
\fi
\iffull
Algorithm \ref{alg:BC_setup_controlling} 
\fi
 runs in time .
\end{theorem}

\begin{theorem}\label{theo:ccapprox}
In a weighted graph , when the weights  denote the distance between two nodes, 
\ifshort
one can approximate 
\fi
\iffull
Algorithm \ref{alg:CC} approximates 
\fi
closeness centrality of all nodes with an inverse additive error of  and runs in  time.
\end{theorem}

\begin{theorem}
Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT} computes a -approximation of the S-MRCT problem in time , when  is uniform or corresponds to the time needed to transmit a message through edge ..
\label{theo:mrctapprox}
\end{theorem}
\begin{theorem}\label{theo:mrctapprox-rand}
Algorithm~\textsc{MRCTrand} computes a -approximation of the S-MRCT problem in time , when  is uniform or corresponds to the time needed to transmit a message through edge .
\end{theorem}
Algorithm and proof of Theorem~\ref{theo:DLGruntime} is stated in Section \ref{sec:aggregate}.
Algorithm and proof of Theorem~\ref{theo:tree} is stated in Section \ref{sec:tree}.
Algorithm and proof of Theorem~\ref{theo:bcapprox} is stated in Section \ref{sec:BC}.
Algorithm and proof of Theorem~\ref{theo:ccapprox} is stated in Section \ref{sec:CC}.
Algorithm and proof of Theorem~\ref{theo:mrctapprox-rand} is stated in
\ifshort
Appendix \ref{FULL:app:MRCT}.
\fi
\iffull
Section \ref{app:MRCT}.
\fi


\subsection{Related Work}

\paragraph{(Local multi-)aggregation}
The authors of \cite{rajagopalan8data} survey the general area of data-aggregation in wireless sensor networks.
It is know \cite{anantvalee2007survey} that local multi-aggregation is useful in generalizations of Zone-Based Intrusion Detection Systems mentioned in. These systems detect local changes of the topology (assuming the graph does not change too fast) and react to it. The authors of \cite{Jiang:2008:DBT:1463434.1463440} perform local multi-aggregation in sensor networks to detect basic changes of the local environment, such as variations of temperature or concentration of toxic gas. If the value in one node passes a threshold, it starts a local aggregation. They also provide applications to fire fighters and emergency services in case of a flood. In \cite{Jiang:2008:DBT:1463434.1463440} changes of areal objects are detected by the cooperation of sensors in the locality of the changes and later reported to a central node.

\paragraph{Centrality}
Classical applications of Closeness Centrality \cite{bavelas1950communication,beauchamp1965} and Betweenness Centrality \cite{freeman1977} are in the area of social network analysis \cite{wasserman1994social}. E.g. Closeness Centrality indicates how fast information spreads and Betweeness Centrality indicates how easy it can be manipulated by certain nodes. However, these findings can often be transferred to networks based on electric devices. One example are simple routing protocols, where malicious nodes with high centrality have the power to manipulate large parts of the communication. Eppstein and Wang showed in \cite{eppstein:2001:fastCCapprox} a fast approximation algorithm to derive the closeness centrality of a node by sampling only a few nodes  and computing shortest paths from  to all nodes. Brandes \cite{brandes:2001:fasterBCalgo} was able to provide a a fast algorithm for computing betweenness centrality recursively, which Bader et al. \cite{bader:2007:BCapprox} extended to an even faster approximation using an adaptive sampling techique.

We continue by reviewing a few applications in network analysis and design independent of social networks: For distance approximation, when using the landmark method \cite{ng2002predicting,pias2003lighthouses}, it is mentioned in \cite{aggarwal2010survey} that a modification of \cite{rattigan2007graph} chooses landmarks using local closeness centrality. According to \cite{shao2013trinity}, this is  implemented e.g. in the Trinity Graph Engine.
A relation to network flows is established e.g. in \cite{borgatti2005centrality} for Closeness as well as Betweenness Centrality. The authors of \cite{daly2007social} propose a routing scheme based on Betweenness Centrality. \todoI{wiki has more stuff on routing}

\paragraph{MRCT}
Distributed approximations of MRCT in the CONGEST model were studied in \cite{hochuli:holzer:MRCST, khan2008efficient, sarma12}. The authors of \cite{khan2008efficient} showed how to use results of \cite{fakcharoenphol2003tight} to obtain a randomized -approximation of the MRCT in time . Observe that this might be  even in a graph with hop diameter . In \cite{hochuli:holzer:MRCST} two algorithms were presented for unweighted graphs and weighted graphs, where the weight of an edge corresponds to the traversal time of the edge. A deterministic one, that computes a -approximation in linear time  and a randomized one, that computes w.h.p. an -approximation in time  (and variations thereof). A lower bound of  for any randomized -error algorithm that computes a  -approximation was shown in \cite{sarma12} for weighted graphs. For a more detailed overview on work related to MRCT computations in various computational models, we refer the reader to \cite{hochuli:holzer:MRCST}.

\paragraph{Shortest Paths}
At the heart of our algorithms is an algorithm to compute  directed layered graphs rooted in nodes  in parallel. This algorithm is a modification of the -Shortest Paths algorithm of \cite{holzer2012optimal}. Furthermore our modification extends this -Shortest Paths algorithm to the weighted setting and to compute shortest paths only up to a certain distance that can be provided as input. Note that the -source detection algorithm by Lenzen and Peleg~\cite{lenzen2013efficient} can be extended to weighted graphs in a similar way and could be modified to compute directed layered graphs as well, but we developed our method that resulted in this paper independently and simultaneously~\cite{dissler-thesis,holzer2013phd}.




\section{Model, Definitions and Problems}\label{sec:model}

\paragraph{Model:} Our network is represented by an undirected graph . Nodes  correspond to processors (computers, sensor nodes or routers),
two nodes are connected by an edge from set  if they can communicate directly with each other. We denote the number of nodes of a graph by , and the number of its edges by . Furthermore we assume that each node has a unique ID in the range of , i.e. each node can be represented by  bits. 
Initially the nodes know only  the IDs of their immediate neighborhood, the number of nodes in  and the node with the lowest ID further denoted as node .\footnote{Note that for computing the schedule in our framework we do not need to know  and the node with smallest ID. These assumptions are only necessary for our applications such as centrality computation. The runtime of these applications depends on the diameter of the graph such that computing these values does not affect the runtime.} \\

An unweighted shortest path in  between two nodes  and  is a -path consisting of the minimum number of edges contained in any -path. Denote by  the unweighted distance between two nodes  and  in , which is the length of an unweighted shortest -path in . We also say  and  are  hops apart. By  we denote a graph's weight function and by  the non-negative weight of an edge in . For an edge  we often write  to denote . For arbitrary nodes  and , by  we define the weighted distance between  and , that is the weight of a shortest weighted path connecting  and . In case we consider a subgraph  of a graph , we denote by  and  the distances with respect to using only edges in .

In our model we consider only symmetric edge weights, i.e. an edge  has in both directions the same weight . We denote a graph with weighted edges as . By  we denote the -neighborhood of a node . That is in an unweighted graph all nodes in  that can be reached from  using  hops. And in a weighted graph all nodes that can be reached from  using paths with at most weight .
We use the convention that . Given a set , set  denotes the -neighborhood of . During the paper we denote the degree  of a node  by .

\begin{definition}
The weighted eccentricity  of a node  is the largest weighted distance to any other node in the graph, that is .\\
The unweighted (hop) eccentricity  of a node  is the largest hop distance to any other node in the graph, that is .
\end{definition}

\begin{definition}
The weighted diameter  of a graph  is the maximum weighted distance between any two nodes of the graph.
\\ 
The unweighted diameter (or hop diameter)  of a graph  is the maximum number of hops between any two nodes of the graph.
\end{definition}

Observe that  for unweighted graphs. In weighted graphs it is true that  as well as .

In this paper we need a graph substructure to describe shortest paths from a root node  to all other nodes. Such a representation can be found e.g. in a tree. However, for applications of our framework to e.g. calculating the Betweenness Centrality of a node in Section \ref{sec:BC}, we need to consider all shortest paths between two nodes, not just a single one as provided by a tree.
Therefore we need to define a graph structure which clearly indicates all possible shortest paths between a root node  and all other nodes . 


\begin{definition}[Tree ]
 Given a node , we denote the spanning tree of  that results from performing a breadth-first search () starting at  by .
\end{definition}


\begin{definition}[Directed Leveled Graph (DLG) , unweighted]
 Given a node  of a graph , partition all nodes  in disjoint subsets  with  if . Add an directed edge  to a set  for every edge  with  and .
That is, every shortest path from  to any other node  in  is a directed path in . We say  is rooted in .
A graphic is provided in 
\ifshort
Appendix \ref{FULL:sec:model}, Figure \ref{FULL:abb:leveledgraph}.
\fi
\iffull
Figure \ref{abb:leveledgraph}.
\fi

In the weighted case, we partition all nodes  in disjoint subsets  with  if . Add an directed edge  to set  for every edge  with  and .
That is, every shortest weighted path from  to any other node  in  is a directed weighted path in . 
A graphic is provided in 
\ifshort
Appendix \ref{FULL:sec:model}, Figure \ref{FULL:abb:leveledgraphweighted}.
\fi
\iffull
Figure \ref{abb:leveledgraphweighted}.
\fi

\end{definition}
\iffull
\begin{figure}[htb]
\begin{center}
\includegraphics[width=\linewidth]{graphLs_sets}
\caption{ on the left side and its leveled graph  rooted in  on the right.}\label{abb:leveledgraph}
\end{center}
\end{figure}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=\linewidth]{graph_weighted_Ls-sets}
\caption{ on the left side and its leveled graph  rooted in  on the right}\label{abb:leveledgraphweighted}
\end{center}
\end{figure}
\fi

\begin{definition}[parent, children, ancestors and leaves  in a DLG]
 Given a node , we define a parent of  in  a DLG  as any neighbor  of  connected by a directed edge  to . As a node can have several parents, we often consider a set of parents, formally defined as: . Children are all neighbors  of  connected by a directed edge  to . A leaf node in a DLG  is a node without any children.
\end{definition}


\begin{definition}[approximation] 
Given an optimization problem , denote by  the value of the optimal solution for  and by  the value of the solution of an algorithm  for .
Let . We say A is a -approximation (additive approximation) with additive error  for  if  for any input.
Let . Like in \cite{eppstein:2001:fastCCapprox} we say  is an inverse additive approximation with inverse additive error  for  if  for any input. 
We say  is -approximative (a one-sided multiplicative approximation) for  if  and  or if  and  for any input.
\end{definition}
\iffull
Like in \cite{eppstein:2001:fastCCapprox}, the inverse additive error is used in the closeness centrality approximation, see Section \ref{sec:CC}.
\fi
\begin{fact}\label{fact:ecc-approx-diam}
 For any node  we know that .\end{fact}


\subsection{Problem Statements}
We start by formally stating the problems we consider in this paper. First we state local multi-aggregation, which is not only of interest by itself but turns out to be at the heart of the other problems. Centrality-measures are graph-properties that are based on the number of shortest paths. Besides classical applications in social network analysis \cite{carrington2005models,wasserman1994social}, they have a number of applications in design and analysis of distributed networks such as \cite{aggarwal2010survey,borgatti2005centrality,daly2007social}. Minimum Routing Cost Spanning Trees can be used to minimize the average cost of communication in a network while keeping a sparse routing-structure \cite{hu1974optimum,wong1980worst}. 
 \begin{definition}(local multi-aggregation).
In the problem of local multi-aggregation, we are given a subset  of nodes  in a graph . Each of the nodes in  contains (several) values and each node in  wants to evaluate a (possibly different) aggregation-function based on (some of) these values stored in nodes of its -neighborhood.
We consider two variations:
\begin{enumerate}
\item only one (shortest) --path is needed for each  to perform the aggregation.
\item all shortest --paths are needed for each  to perform the aggregation.
\end{enumerate}\end{definition}


Betweenness centrality is a measure for centrality of a node , which is based on the number of shortest paths in a graph  node  is part of. In a graph , let  denote the number of shortest paths from  to  and let  denote the number of shortest paths from  to  that go through the node  for . Closeness centrality is another measure to identify important nodes in a graph. The closeness centrality of a node is the inverse of the average distance to all other nodes. At the heart of our solutions for these problems is the -SP problem and we define:

\begin{definition}[Betweenness Centrality, as stated in \cite{freeman1977}] .
\end{definition}

\begin{definition}[Closeness Centrality, as stated in \cite{beauchamp1965}] .
\end{definition}


\begin{definition}\label{def:MRCT}(Minimum Routing Cost Tree (MRCT) as stated in \cite{WuLBCRT99} ).
The MRCT problem~\cite{WuLBCRT99} is defined as follows. The routing cost  of a subgraph  of  is the sum of the routing costs of all pairs of vertices in the tree, i.e., . Our goal is to find a spanning tree with minimum routing cost.
Given a subset  of the vertices  in , in the -MRCT problem  \cite{hochuli:holzer:MRCST},\ our goal is to find a subtree  of  that spans  and has minimum routing cost with respect to . That is a tree  such that  is minimized. Here,  denotes the routing cost of  with respect to .
\end{definition}

\begin{definition}[-SP]
 Let  be a graph. In the -Shortest Paths (-SP) problem, we are
given a set  and need to compute the shortest path lengths between any pair of nodes in  such that in the end each node in  knows its distance to all nodes in .
\end{definition}




\section{Local Multi-Aggregation}
\label{sec:aggAlgo}
Many distributed algorithms to compute network properties can be stated in a way that they distribute and/or aggregate information through a tree or a directed leveled graph (DLG). For example computing the max-value of the -neighborhood (using a tree, see the text below), the sum of all values in the -neighborhood (using a tree, see the text below) and computing centrality measures (using a DLG, see Sections \ref{sec:BC} and \ref{sec:CC}).

By extending the algorithms of \cite{holzer2012optimal} and \cite{hochuli:holzer:MRCST}, we present an algorithm which can aggregate information in the -Neighborhood of a set of root nodes . Note that if information from the whole graph is needed,  can be set to .

All known exact computations of these properties need to compute a tree or DLG for every node , to evaluate 's dependency on other nodes.
Often the exact computation can be approximated by evaluating only the dependencies of a subset  of nodes and therefore computing only  trees or DLGs with depth  in respect to the root nodes .
We provide in Section \ref{sec:aggregate} an algorithm to compute  such DLGs rooted in the nodes of  in time .
Furthermore we provide an algorithm to aggregate information on these previously computed DLGs, again with a time complexity of .
Both algorithms are able to execute additional algorithms,  and  respectively within each node, to distribute and aggregate information about the desired network property, where the network property description replaces the index . These functions  and  can be chosen depending on the aggregating problem at hand.
A possible application with choices of  and  to compute the max-value in the -neighborhood of each root node  is shown in 
\ifshort
Appendix \ref{FULL:sec:aggAlgo}, Example \ref{FULL:ex:maxval}.
\fi
\iffull
Example \ref{ex:maxval}.

\begin{example}
(Computing the max-value of each k-Neighborhood of a set of nodes , , with our proposed algorithm.)
Each node  starts Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}. 
While spanning the DLGs  rooted in each node  in Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} no algorithm  needs to be specified. In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} the maximum node value needs to be aggregated. Therefore we define algorithm  as follows:  initializes on each node  for each root node  a variable  to store the highest value received from any child in , first the node's own value  is stored in  for every . On execution of  with message  and ID  from a child,  stores the value of  in  if . Then  is stored in  and sent to the parents of .
After the execution of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} each root node  stores in  the max-value of its own k-Neighborhood.
\label{ex:maxval}
\end{example}

\fi
 Or   and  to compute the sum of all values in the -neighborhood of each root node  is provided in 
\ifshort
Appendix \ref{FULL:sec:aggAlgo}, Example \ref{FULL:ex:sumval}.
\fi
\iffull
Example \ref{ex:sumval}.

\begin{example}\label{ex:sumval}
(Computing the sum of all values in each k-Neighborhood of a set of nodes , , with our proposed algorithm.)
Each node  starts Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}. 
While spanning the DLGs  rooted in each node  in Algorithm \iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} no algorithm  needs to be specified. In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} the sum of all node values in the k-Neighborhood needs to be aggregated. Therefore we define algorithm  as follows:  initializes on each node  for each root node  a variable  to store the sum of all values received from any child in  plus its own value . First the node's own value  is stored in  for every . On execution of  with message  and ID  from a child,  adds the value of  to . Then  is stored in  and sent to (only) one parent of . If node  has more than one parent, the message is sent to the parent node with the lowest ID. This results in the Tree Variation in Section \ref{sec:tree}.
After the execution of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} each root node  stores in  the sum of all values in its own k-Neighborhood.
\end{example}

\fi
\begin{remark}[Time optimality of Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}]\label{rem:lowerbound}
In a setting where edge-weights correspond to the transmission time through the corresponding edge, any solution for an algorithm to aggregate information from all nodes  to one root node  needs at least  time. Now consider the graph of Figure~\ref{fig:lowerbound}. There we have  root nodes  connected to a chain of  nodes and assume this chain determines the diameter . Assume each of the chain nodes stores  different values that are encoded using  bits each. Assume each root node  computes an aggregation function based on the 'th value of each chain node. Due to congestion, the time until all  values of the chain node at distance  to the root nodes arrives at the corresponding root nodes is . This can be extended to  in the unweighted case with .
\end{remark}
\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=.5\textwidth]{lowerbound}
	\end{center}
	\caption{Graph based on the construction used in Remark~\ref{rem:lowerbound}.}
	\label{fig:lowerbound}
\end{figure}

\subsection{Algorithm for Local Multi-Aggregation}\label{sec:aggregate}



First we describe Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} which computes  times a DLG with depth , one for each node in , and can distribute/broadcast information (depending on the aggregation function at hand) along these computed DLGs. In Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} we extend the -SP Algorithm of \cite{holzer2012optimal}, which computes shortest paths. (Note that alternatively one could also modify the -source detection algorithm by Lenzen and Peleg~\cite{lenzen2013efficient}.) In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}, we aggregate information efficiently through the computed DLGs.
\ifshort
\footnote{In addition to the descriptions in the text below, the pseudocode of Algorithms \textsc{DLGcomp} and \textsc{DLGagr} are stated in Appendix \ref{FULL:sec:aggregate} (for the convenience of the reviewer).}
\fi
Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} is designed as a subroutine and has two input parameters  and  as well as a function .
Parameter  is the number of DLGs  rooted in  which Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} has to compute.
Parameter  denotes the depth of the -neighborhood.

And function  specifies an algorithm executed along with the computation of the  DLGs.
Further a node  knows that it is in  (e.g. as it is sampled by some procedure or interested in performing an aggregation by itself).
\todoI{describe basic short idea of Algo}
Lets consider first only the special case where we compute a single DLG  rooted in node . We denote by \emph{-message} any messages sent during an execution of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} that belongs to the construction of the DLG rooted in node  (later we keep this term for different roots ).  Node  starts with sending an -message to all its neighbors  and in the next time slot, those neighbors  send an -message to their neighbors, except to .
A DLG  continues to grow as follows: in time slot  all nodes  at distance  receive a -message from their neighbors  with distance . If the first -message is received from neighbor  of node ,  is considered a parent of  in . However, if in the same time slot  receives further -messages from different neighbors , those are considered parents too. One time slot after receiving the first -message(s), node  computes an -message and sends it to all neighbors  which are not considered a parent of .


Now we say a -message received by  is considered \emph{valid}, if it is sent from a parent of  in . Each -message contains the ID of the root node  and the weighted distance  from  to the receiving node .




To simulate a weighted graph, the messages get delayed 
\iffull in Line \ref{line:li} \fi
 corresponding to the corresponding edge weight , if a message is to be sent from node  to . For unweighted graphs, the variable 
\iffull (Line \ref{line:li}) \fi
 contains the lowest ID in  which is not in .\todoI{details: what is , .}

While computing , the ID  and some additional information is stored and sent along with the -messages. Each node stores time  and parent  from which it received a -message
\iffull
 (Line \ref{line:tau} and \ref{line:tauplusone})
\fi
. This is needed to efficiently execute Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} later on. 
A node  stores in schedule  and in  arrival times and parent nodes  if  \iffull(Lines \ref{line:kvarstart} to \ref{line:exf} are only executed if , i.e. if , where  was received in a -message)\fi.
Nonetheless, -messages are still forwarded, even if the distance  in a -messages is larger than .
This is needed to ensure that -messages are properly delayed.

Furthermore each node  executes algorithm \iffull (Line \ref{line:exf})\fi, and can be used to distribute information in . Algorithm  is executed with parameters  each time  receives a valid -message from neighbor . Message  is the part of the -message computed by algorithm  on a parent node  of  in .
After the execution for every parent   of  in  with the corresponding ,  algorithm  computed and stored in  the message which is sent to the children of  in \iffull (Line \ref{line:sendreceivedist})\fi. Furthermore algorithm  is executed once on every node for definition and initialization of additional global variables on the node\iffull (Line \ref{line:DLGinitf})\fi.

With Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}, multiple leveled graphs  start growing in the same time slot. One  for each . This could lead to congestions. To prevent that, every time two Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} messages cross\footnote{That means both messages are received in the same time slot by the same node or both are sent in the same time slot through the same edge , one from  to  and one in the opposite direction.}, the Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} message with the higher ID is delayed one time slot. 
This is done\iffull in Line \ref{line:DLGputLdelay}\fi by putting the higher ID into queue  or by retransmitting the message in the next time slot through the same edge  again, respectively\iffull (due to the if-statement in Line \ref{line:DLGdelay})\fi.

Similar as in the proof of the S-SP algorithm \cite{holzer2012optimal}, we show that the total delay of any Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} is at most  when  many DLGs are constructed in parallel. And we show that despite of the delays we still construct valid leveled graphs.

\iffull
\begin{algorithm}[tp]
\small
\begin{algorithmic}[1]
\STATE ,
\STATE ;
\STATE ; \COMMENT{ time when message of DLG  reaches , for each parent  enumerated in }\label{line:tauinit}
\STATE ; \COMMENT{sp[v] := number of parents in shortest paths from  to }
\STATE ; \COMMENT{, }\label{line:omegainit}
\IF{}
        \STATE ;
        \STATE ;
        \STATE ;
\ENDIF
\STATE ;
\STATE \textbf{initialize} algorithm ;\label{line:DLGinitf}
\FOR{}\label{line:DLGdisploop}
        \FOR{}
                \STATE ; // smallest ID that is not delayed and ready to be scheduled \label{line:li}
        \ENDFOR
        \STATE within one time slot:\label{line:sendreceivedist}
        \newline \textbf{send}  to neighbor , receive  from ;
        \newline \textbf{send}  to neighbor , receive  from ;
\todoI{ vs. }        \newline 
        \newline \textbf{send}  to neighbor , receive  from ;
        
        \STATE ;
        \STATE ;
        \IF{}
                \STATE ;
        \ENDIF
        
        \FOR{}
                \IF['s message will be delayed due to .]{}\label{line:DLGdelay}
                        
                        \IF{} \label{line:treevarstart}
                                                \IF{} \label{line:kvarstart}
                                                        \STATE ;
                                                        \STATE ;\label{line:tau}
                                                        \STATE  neighbor ; \label{line:tauplusone}
                                                        \STATE \textbf{execute} algorithm ; \label{line:exf}
                                                \ENDIF
                                                \IF{}
                                                        \STATE ; \label{line:omegacomp}
                                                        \STATE ;
                                                        \IF{}
                                                                \STATE ; \label{line:DLGputLdelay}
                                                        \ENDIF
                                                \ELSE
                                                        \STATE ;
                                                \ENDIF
                                        \ENDIF \label{line:treevarend}        
                \ELSE
                        \STATE ; \COMMENT{'s message was successfully sent to neighbor .}
                \ENDIF
        \ENDFOR
\ENDFOR
\end{algorithmic}
\caption{\textsc{DLGcomp} compues  DLGs in the k-Neighborhood of the root nodes  \newline
(executed by node ) \newline 
        \textbf{Input:} , , \newline 
        \textbf{parameters passed to :} message  of a parent  of  in DLG }
\label{alg:DLGcomp}
\end{algorithm}
\fi


In  Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}, information gets aggregated according to Algorithm . To do this, the computed leveled graphs of each root node  get processed in a bottom-up fashion. 
Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} has three inputs: the number of DLGs , the depth  of the -neighborhood, which are needed to bound the runtime, and an algorithm , which can consist of an initialization part and a computation part. The initialization part is executed once on every node  starting the computation\iffull(loop of Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}, Line \ref{line:DLGloopagg})\fi. In the loop in time slot  a node  sends the information computed by  to its parent  in . Here schedule  is the time when  received a valid -message from neighbor . The schedule  was computed before in Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}\iffull (Line \ref{line:tau})\fi. 

The computation part of  is executed on a node  each time  received a message  from a child node  in . When  has received all messages  form all children  in  and  has executed  once for each of those messages,  computed and stored in  the information that is subsequently sent to all parents of  in .


\iffull
\begin{algorithm}[ht]
\begin{algorithmic}[1] 
\STATE \textbf{initialize} algorithm ; // and use same variables as Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}
\FOR{}\label{line:DLGloopagg}
        \STATE within one time slot:\label{line:sendreceiveagg}
        \newline
                          \hspace*{0.5cm}\textbf{foreach} ,  \textbf{such that}  \textbf{do}\\
\hspace*{1cm}\textbf{send}  to ;
        \newline \hspace*{0.5cm}\textbf{receive}  from neighbor ;
        \newline \hspace*{0.5cm}\textbf{receive}  from neighbor ;
        \newline \hspace*{0.5cm}
        \newline \hspace*{0.5cm}\textbf{receive}  from neighbor ;
        \FOR{}
                \IF{ empty}
                        \STATE \textbf{execute} algorithm ;\label{line:exg}
                \ENDIF
        \ENDFOR
\ENDFOR
\end{algorithmic}
\caption{\textsc{DLGagr} computes aggregation function (executed by node ) \newline
        \textbf{Input:} , ,   \newline
        \textbf{parameters passed to :} message  of a child  of  in DLG }
\label{alg:DLGagr}
\end{algorithm}
\fi




\begin{lemma}
Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} computes  valid directed leveled graphs, in time  for .
\label{lemma:algdisp}
\end{lemma}

\ifshort
We state the proof of Lemma \ref{lemma:algdisp} in Appendix \ref{FULL:sec:aggregate}.
\fi
\iffull
\begin{proof}
The proof is very similar to the proofs for the S-SP algorithm in \cite{holzer2012optimal}. For the \textit{correctness} we state a slightly adapted version of Lemma 12 in \cite{holzer2012optimal}.

\textbf{Correctness: }
First, assume that  and consider the computation of . At time t, each node  at weighted distance  from  receives a -message from all of 's neighbors  that are at distance  to . All edges incident to neighbors  that sent such a message are added to DLG , directed from  to .

Now consider the case, where the set  contains at least two nodes.
We analyze how the computations of other nodes affect the computation of . 
We say that a -message has \textit{`reached'} a node  through edge  from a neighbor  in time slot t, if the message was successfully received and has been removed from the delay queue  in time slot , or it was successfully received in time slot  and not put into the queue  at all.
It turns out that the first -messages of 's computation which \textit{reach}  are transmitted through the edges at distance  as in the case before. Consider two neighbors  of ; we can ignore the case that  has only one neighbor since this case trivially satisfies our claim.
A -message containing weighted distance , has \textit{reached}  through the edge  earlier than through edge  if and only if .
We show this by proving that the set of messages with lower IDs which delay the -messages is the same for both paths  and : Assume that the computation of  is delaying the -message sent through  at some point.
Then the -message reaches, in case the -message is coming from 's direction, node  earlier than the -message.
Even if an -message and a -message are transmitted in the same time slot to  (through  and , respectively), the -message delays the -message in the node by putting it into the queue , and the -message \textit{reaches}  earlier.
Thus the -message also delays the -message running through path , if it did not already delayed it earlier.

\textbf{Runtime: }
In case  contains only one element, the computation of  DLG , which is not delayed by other computations, takes at most  time steps. Because in time step  all nodes  at distance  receive a -message, as a consequence in time step  every node  with distance  receives or has received a -message. Since there are no nodes  with , the computation of  stops after  time steps.

We showed that a -message can be delayed at most one time slot by another -message with , and therefore by at most  time slots by all other  DLG-constructions. Thus Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} runs in .
\end{proof}
\fi

\begin{lemma}
Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} aggregates information through  directed leveled graphs, in time  for .\label{lemma:algagg}
\end{lemma}

\ifshort
We state the proof of Lemma \ref{lemma:algagg} in Appendix \ref{FULL:sec:aggregate}.
\fi
\iffull
\begin{proof}
\textbf{Runtime: }
In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}, a node  sends only messages to parent nodes in all DLGs  . The schedule to send the messages is the same as in Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} stored in , but reversed. Therefore the runtime of Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} is bound by the runtime of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}, which is .
The complexity of executing algorithms  and  inside of a node causes no additional communication. Since we are only interested in communication complexity, the total runtime is .

\textbf{Correctness: }
Since we use the schedule  of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} in reverse order to send messages, it is guaranteed that a node  receives all messages from all children in a DLG  before  sends the first message to any parent in .
\end{proof}
\fi





\begin{lemma}
Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} aggregate information in  time for .
\label{lem:kneighborhood}
\end{lemma}

\ifshort
We state the proof of Theorem \ref{lem:kneighborhood} in Appendix \ref{FULL:sec:aggregate}.
\fi
\iffull
\begin{proof}
\textbf{Runtime: }
In the k-Neighborhood restricted Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} a -message which is not delayed by other IDs, needs  time slots to reach all nodes .
We showed in Lemma \ref{lemma:algdisp} that a -message can be delayed at most  time slots due to other ID's.
Hence, to reach all nodes in its -neighborhood an -message needs at most  time slots.
For aggregating information Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} uses schedule , which was computed in the k-Neighborhood restricted Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and therefore is bound by the same number of time slots .

\textbf{Correctness: }
As long as a distance  in a -message is less or equal to , the message is processed the same way as in the restricted Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} ().
When  is larger than , then -messages with ID  do no longer extend the DLG , but just delay all -message with higher ID () the same way as in the restricted Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}.
As a consequence after the execution of the k-Neighborhood restricted Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}, all nodes  have the same information of DLG  stored as if the restricted Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} has been executed.
All nodes  that still send -messages do not contribute to 's aggregation but are necessary to ensure that the other tree's computations are delayed and trees are constructed correctly. 

By executing Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} a node  sends aggregation messages  back to a root node  if and only if , as mentioned above. Therefore Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} just aggregates information of the neighborhood  to a root node .
\end{proof}
\fi



\begin{proof}[Proof of Theorem \ref{theo:DLGruntime}]
Follows by combining Lemma \ref{lemma:algdisp} and \ref{lemma:algagg} and Lemma \ref{lem:kneighborhood}.
\end{proof}



\subsection{Tree Variation}
\label{sec:tree}

In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} we can aggregate information along all shortest --paths between a root node  and a node .
For some applications it is desirable to aggregate information only along one shortest --path, as for example in max-value/average-value aggregation or in a  -Shortest Paths to approximate problems such as closeness centrality.
That means a node  sends a message only to one parent in a DLG  while executing Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}.
The result is the same as when aggregating information through a tree  rooted in  (instead of a DLG ).
\todoI{Can be done inside aggregation fkt?}
For completeness we show an adaptation of Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} which computes and aggregates along trees instead of DLGs.
We provide a detailed description of the Tree Variation in Appendix 
\ifshort
\ref{FULL:app:tree}.
\fi
\iffull
\ref{app:tree}.
\fi







\section{Applications}

\subsection{Betweenness Centrality}
\label{sec:BC}

Brandes showed in \cite{brandes:2001:fasterBCalgo}, an algorithm for computing betweenness centrality recursively. Let  be the ratio of all shortest path between  and  that contain node  compared to all shortest paths between  and . We denote by  \todoI{how does it relate to BC?}the dependency of a node  on another node , defined by . The dependency can be calculated recursively as Brandes  \cite{brandes:2001:fasterBCalgo} stated:

\begin{theorem} (Recursive Betweenness Centrality Dependency, Brandes \cite{brandes:2001:fasterBCalgo} Theorem 6)

\label{theo:brandes:recursive}
\end{theorem}

Bader et al. showed in \cite{bader:2007:BCapprox} an adaptive sampling algorithm, which approximates the betweenness centrality with significantly reduced number of single-source shortest path computations for a node with high centrality when using Brandes' recursive algorithm.
\begin{theorem} (Bader et al \cite{bader:2007:BCapprox} Theorem 3).
For , if the centrality of a vertex  is  for some constant\todoI{constant ???} , then with probability  its betweenness centrality can be estimated to within a factor of  with  samples of source vertices.
\label{theo:bader:estimation}
\end{theorem}

This algorithm can be adapted efficiently in a distributed setting by using Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}.
We define an Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi \textsc{BCsetup} to perform multiple rounds as suggested by Bader in \cite{bader:2007:BCapprox}. In each round, Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} are executed with algorithms  and  as defined \ifshort below\fi\iffull in Algorithm \ref{alg:BC_f} and \ref{alg:BC_g}\fi. With  and  we state the Algorithms  and  of our framework specified for betweenness centrality approximation () being the function . 
In contrast to Bader, our algorithm samples not just the betweenness centrality dependency of one node in each round, but of multiple nodes.
Furthermore, Bader's algorithm concentrates on one node  and stops sampling if  can be approximated within an error  with probability . Our Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup} considers all nodes in the graph and stops after at least  nodes have been approximated within a similar multiplicative error  with probability  (where ).
\ifshort
\footnote{In addition to the descriptions in the text below, we provide pseudocode of Algorithms\iffull~\ref{alg:BC_setup_controlling}\fi \textsc{BCsetup},  and  in Appendix \ref{FULL:sec:algBC}.}
\fi

\subsubsection{Algorithm for Betweenness Centrality}\label{sec:algBC}






In Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup}, the idea is to select in multiple rounds multiple sample nodes  and calculate the betweenness centrality dependency  on all other nodes  for each . The algorithm stops if more than  nodes in  with high betweenness centrality are found,  is an input parameter. Set  is the set of nodes which are sampled in round , all sets  are disjoint and form together the set . 

During the algorithm each node  stores the sum of all the dependencies  of the nodes  sampled so far in variable . Furthermore the number  of sample nodes  that were sampled so far (in any of rounds ), need to be stored to indicate whether the node itself is sampled ().\todoI{why?}

All centralized communication to and from node 1 uses tree . In round   \iffull(Line \ref{line:BCprobstart} to \ref{line:BCprobend})\fi node 1 calculates the probability  (according to the proof in Theorem \ref{theo:bcapprox})

with which each node gets sampled. The value of  is broadcasted in the network and each node decides if itself is a sample node and reports back to node 1 if so.
\ifshort
If 
\fi
\iffull
 In Line \ref{line:BCaccsamples}, if 
\fi
 a node in  receives multiple messages in one time slot, it sends the sum of the message value to its parent in . The number of sample nodes  gathered by node 1 is then broadcasted again, this is needed to determine the runtime  in Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} and to maintain the number of sample nodes .

\iffull
\begin{algorithm}[htp]
\begin{algorithmic}[1]
\STATE \textbf{global} ; \COMMENT{sum of dependency scores , }
\STATE \textbf{global} ; \COMMENT{number of samples so far}
\STATE \textbf{global} ;
\STATE ; \COMMENT{approximated betweenness centrality of , initialized as undefined}
\IF{}\label{line:BCestimstart}
        \STATE estimate  and  by generating a spanning tree  and using Fact \ref{fact:ecc-approx-diam};
        \STATE \textbf{broadcast}  and  on ;
        \STATE ; \COMMENT{number of high BC nodes found so far}
\ELSE
        \STATE \textbf{wait} until value of  and  \textbf{received};
\ENDIF\label{line:BCestimend}
\FOR{}
        \STATE ;
        \IF{} \label{line:BCprobstart}
                \STATE \textbf{broadcast}  on ;
                \STATE \textbf{wait} for responses  for  time slots;
                \STATE ; \COMMENT{number of samples in this round}
                \STATE \textbf{broadcast}  through ; 
        \ELSE 
                \STATE  \textbf{receive} value of ;
                \IF{}
                        \STATE \textbf{set}  \textbf{with probability} ;
                        \IF{}
                                \STATE \textbf{send} (``1") to node  by ; \COMMENT{accumulate messages} \label{line:BCaccsamples}
                        \ENDIF
                        \STATE \textbf{receive} value of ;
                \ENDIF
        \ENDIF \label{line:BCprobend}
        \STATE \textbf{start} Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} with (, , );
        \STATE \textbf{start} Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} with (, , ); \label{line:BCstartagg}
        \STATE ;
        \IF{}
                \STATE ;
                \STATE ;
        \ENDIF
        \IF{} \label{line:BCcheck}
                \STATE ; \COMMENT{divided by two, since G is a undirected graph} \label{line:scalebc}
                \STATE \textbf{send} (``reached threshold", 1) to node  by using paths in ;  \COMMENT{accumulate messages} \label{line:BCaccthresh}
        \ENDIF
        \IF{}
                \STATE \textbf{wait to receive} message(s)  for up to  time slots;
                \STATE ;
                \IF {} \label{line:BCnhatcheck}
                        \STATE \textbf{broadcast} ``terminate";
                \ENDIF
        \ENDIF
\ENDFOR
\end{algorithmic}
\caption{\textsc{BCsetup}: Distributed Betweenness Centrality Approximation (executed by node ) 
\newline \textbf{Input:}  : threshold criterion, : number of nodes which have to reach the threshold }
\label{alg:BC_setup_controlling}
\vspace*{0.5cm}
\end{algorithm}
\fi


Next, all nodes simultaneously start to execute Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} with algorithm , whose pseudocode is specified in Algorithm
\ifshort
 \ref{FULL:alg:BC_f} in Appendix \ref{sec:algBC}.
\fi
\iffull
 \ref{alg:BC_f}.
\fi
In round  Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} computes a DLG  for every sample node . And  ensures that while computing the DLGs all nodes  compute the total number of shortest --paths () to all DLG root nodes . Therefore  needs to initialize the array  on each node , which stores at  the number of shortest path between  and \iffull (Lines \ref{line:fbcinitstart} to \ref{line:fbcinitend})\fi. A sample node  initializes the distance to itself with 1 

.
\iffull
\begin{algorithm}[htb]
\begin{algorithmic}[1]
\STATE \COMMENT{INITIALIZATION} \label{line:fbcinitstart}
\STATE \textbf{global} ; \COMMENT{ := total shortest paths from  to }
\IF{}
        \STATE ;\label{line:fbcinitend}\newline
\ENDIF

\STATE \COMMENT{COMPUTATION, :  root node ID of the message,  message of a parent in }
\STATE ;
\STATE ; \COMMENT{output}
\end{algorithmic}
\caption{}
\label{alg:BC_f}
\vspace*{0.5cm}
\end{algorithm}
\fi 
During the execution of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} in node  algorithm  processes messages () from parents  in DLG , which contains the value  from   to .
Algorithm  then updates the corresponding entry  by adding the value  received from parent . Entry  stores the correct value after all messages of parents in  have arrived. Node  then forwards  \iffull during the execution of Algorithm~\ref{alg:DLGcomp}~\textsc{DLGcomp} in Line \ref{line:sendreceivedist}\fi to all its children in  (encoded in message ).

In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} each node  adds the dependencies  of all sampled nodes  on itself to the sum . Each time a child node  in a DLG  sends a message to , algorithm , whose pseudocode is specified in Algorithm
\ifshort
 \ref{FULL:alg:BC_g} in Appendix \ref{FULL:sec:algBC}
\fi
\iffull
 \ref{alg:BC_g}
\fi
, gets executed. The message () of child  in  contains the dependency  and the  from  to . The dependencies \todoI{these are approx, should they be called ?} are then calculated recursively with the formula in Theorem \ref{theo:brandes:recursive} and stored in .
Entry  stores the correct value after all messages of children in  have arrived. Node  then forwards  \iffull during the execution of Algorithm~\ref{alg:DLGagr}~\textsc{DLGagr} in Line \ref{line:sendreceiveagg}\fi to all its parents in  (in message ). In a leaf node  of DLG , Algorithm  never gets executed and therefore forwards the initialization value of  to its parents in .


\iffull
\begin{algorithm}[htb]
\begin{algorithmic}[1]
\STATE \COMMENT{INITIALIZATION} 
\STATE \textbf{global} ; \COMMENT{ := dependency of the source vertex s on u.} \newline

\STATE \COMMENT{COMPUTATION, :  root node ID of the message,  message of a child in }
\STATE ;
\STATE ;\label{line:dep1bc}
\STATE ;\label{line:depbc}
\STATE ; \label{line:sumsbc}
\STATE ; \COMMENT{output}
\end{algorithmic}
\caption{}
\label{alg:BC_g}
\vspace*{0.5cm}
\end{algorithm}
\fi

After the execution of Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} each node checks \iffull in Line \ref{line:BCcheck}\fi if  passed the threshold  and reports back to node 1 if this is the case. Node  adds the number of passed threshold to , a variable that indicates how many nodes passed the threshold so far. In \cite{bader:2007:BCapprox} it is stated that  yields a good trade off between computation (time) and approximation quality. To prevent congestion when multiple nodes reached the threshold and therefore multiple messages are sent back to node 1, we use the sum-aggregation of our framework as seen in Example
\ifshort
 \ref{FULL:ex:sumval}.
\fi
\iffull
 \ref{ex:sumval}.
\fi

If not enough nodes reached the threshold so far, i.e. , another round with additional sample nodes gets initiated by node 1.

\begin{remark}
Note, that in an undirected graph the calculated value of  has to be divided by 2, since for every node pair  the betweenness dependency of the shortest --paths on  is computed twice, once as a part of  and once as a part of .
\end{remark}


\begin{lemma}
Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup} computes  for each node  and all samples  in time  in round , with a multiplicative approximation guarantee in the range of . 
\label{lemma:deproundapprox}
\end{lemma}
\ifshort
This is a key-lemma within the proof of Theorem \ref{theo:bcapprox} and we state the full proof in Appendix~\ref{sec:algBC}.
\fi

\iffull
\begin{proof}
\textbf{Runtime: }
The computation of broadcasting and aggregating  and  \iffull in Lines \ref{line:BCprobstart} to \ref{line:BCprobend} \fi takes  time slots. The execution of Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} is bounded when using our framework by , see Theorem \ref{theo:DLGruntime}. Therefore the runtime of Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup} is bound by .

\textbf{Correctness: }
As defined in Theorem \ref{theo:brandes:recursive}, to calculate  it suffices that a node  needs to know the number of shortest --paths  and for every child  of  in  the number of --paths  and the child's . The number of shortest --paths of a node  can be obtained by adding the number of shortest paths of all parents in  together, which is done in  along the execution of Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}. A leaf  in  has a betweenness dependency of zero per definition. Therefore Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} and  can start aggregating the missing parameters in the leafs of . At the end of the execution of Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} in \iffull Line \ref{line:BCstartagg} of \fi Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup}, each node  has the information to calculate .
\todoI{@stephan: this is the only reference to the whole shortest path proof}


While executing Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} to compute a  , a node  sends the total number of shortest paths () between  and  to its children. 
As the graph in Figure \ref{abb:example2n} shows, this number can be as large as  and therefore  bits are needed to transmit the exact number. In order to use not more than  bits per message, we approximated  by using a  bits:  bit for the most significant bits   of  and  bits to encode the exponent  of , that is . Therefore the approximation of  has a multiplicative approximation guarantee in the range of  for the betweenness centrality with . We start with the proof for the case of an unweighted graph. And state later in Lemma \ref{lemma:shortesp:weighted} that it holds too for weighted graphs.

\begin{figure}[h]
\begin{center}
\includegraphics{example2n}
\caption{Graph with  shortest --paths}\label{abb:example2n}
\end{center}
\end{figure}

Let  be the exact and  the approximated number of total shortest paths from a sample node  to , where  is the hop distance of  to .

In Equation \eqref{eq:tsp0} the number of shortest paths from a node  to itself is set to . This is done for simplicity--note that the actual number is --as it ensures that the number of shortest paths from  to ant neighbor is (at least) . And since there is only one shortest path between two nodes which are one hop apart, consisting of the edge between them, Equation \eqref{eq:tsp1} follows.
The exact computation of , which is dependent of the values received from parents of , is shown in Equation \eqref{eq:tsprec}. It is the sum of all shortest path from  to all parents of .



We proof by induction a lower bound for the approximation error at a node  dependent of the hop distance  from  to the root node .




From Equation \eqref{eq:tspclaim} we derive a maximum lower error bound independent of  for all nodes , as shown in Equation \eqref{eq:tsprecappend}, using the fact that .




Now let  be the exact and  the approximated betweenness dependency of a sample node  on , where  is the hop distance of  to . Let us assume  got approximated at every node with the maximum error .
All nodes without any children in  (leafs), e.g. nodes at maximum hop distance of , have no betweenness dependency of , since there are no shortest paths going \textit{through} them, this proves Equation \eqref{eq:bcrec0}.
Then the exact recursive computation of  is shown in Equation \eqref{eq:bcrec}.




Now we derive error bounds by evaluating the term . The maximum errors occur at (i) and at (ii).
With (i) and (ii) we show by induction an upper and lower bound for the multiplicative error of  in Equations \eqref{eq:depclaimi} to \eqref{eq:depstepii}.
\newline\newline
Proof by Induction on the lower approximation error bound of :

\newline
Proof by Induction on the upper approximation error bound of :


From the upper and lower approximation error bounds in a node  with a certain hop distance  to the root node, we can derive in Equations \eqref{eq:depclaimi} and \eqref{eq:depclaimii} a maximum error range for any node at any distance independent of , as shown in Equation \eqref{eq:depbounds}.
\todoI{ is later }


By using the fact, that  we derive in Equations \eqref{eq:epsilondeltamin} to \eqref{eq:delta} a tight bound for .







\begin{corollary}
For betweenness centrality on unweighted graphs we obtain an approximation of the total shortest paths which is guaranteed to be in the range of a  multiplicative factor, where , when assuming that the total shortest path error is multiplicatively bounded to be smaller than .
\label{cor:tsperrunw}
\end{corollary}
\todoI{
\begin{proof}
The Error Bound \eqref{eq:delta} holds for the dependencies calculated in Algorithm \ref{alg:BC_g}, Line \ref{line:dep1bc} and \ref{line:depbc}. By adding multiple dependencies, in Line \ref{line:sumsbc} of the same algorithm, the error bounds do not change. Neither do they change in Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup}, Line \ref{line:scalebc}, by scaling up the dependencies of  on  to the approximated betweenness centrality . This is true, as in sumations and when scaling equations, the multiplicative error can simply \todo{be factored out -- what does this mean?}. \blue{ and , it holds , d.h.  aendert sich nicht}
\label{lemma:tsperrw}
\end{proof}
}


\begin{lemma}
The approximation guarantee of a factor within the range  with  of the betweenness centrality for unweighted graphs holds for weighted graphs, too.
\label{lemma:shortesp:weighted}
\end{lemma}
To prove this, we need to introduce the shortest path diameter .
\begin{definition}[Shortest path diameter]
 of a graph G is the maximum number of hops of a shortest weighted path between any two nodes of the graph.\\
Note that .
\label{def:diamsp}
\end{definition}

\begin{proof}
The maximum hop distance between any leaf  in a DLG and its root node  can be longer than . Because a shortest weighted path is not bounded by  hops, but by  as defined in Definition \ref{def:diamsp}. That changes the maximum exponents in Equations \eqref{eq:tsprecappend}, \eqref{eq:depclaimi} and \eqref{eq:depclaimii} from  to . Since  is smaller than  too, the error bounds in Equations \eqref{eq:epsilondeltamin} and \eqref{eq:epsilondeltamax} do not change.
\end{proof} 








\end{proof}


\begin{proof}[Proof of Theorem \ref{theo:bcapprox}]
\textbf{Runtime: }
The computation of  and  \iffull in Lines \ref{line:BCestimstart} to \ref{line:BCestimend}\fi takes  time slots.
Each round takes  time as shown in Lemma \ref{lemma:deproundapprox}. And for sampling  nodes we need  rounds, since we start with  sample nodes and double this value in every round. This leads to a total time complexity of .

\textbf{Correctness: }Due to Theorem \ref{theo:bader:estimation}, the betweenness centrality  of a node  can be estimated with probability  by sampling the dependency  of  root nodes , if .
The approximation error given in Theorem \ref{theo:bader:estimation} is .

As shown in Lemma \ref{lemma:deproundapprox}, Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup} approximates those dependencies  within a with a multiplicative approximation guarantee in the range of . 
\iffull In Line \ref{line:BCcheck} and \ref{line:BCnhatcheck}, \fi
Algorithm\iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup} ensures that  root nodes have been sampled to approximate the value of  of a node  such that the betweenness centralities of at least  nodes are approximated close enough before terminating.
Combining both approximation errors and noting that , leads to a total error of .
\end{proof}

\begin{corollary}
Let  be the betweenness centrality of node . Then  rounds of communication are necessary to compute an -approximation of . Furthermore, in the same time our Algorithm \iffull~\ref{alg:BC_setup_controlling}\fi~\textsc{BCsetup} finds  w.h.p all other nodes with higher betweenness centrality than .
\end{corollary}
\fi 




\subsection{Closeness Centrality}
\label{sec:CC}

\todoI{It is mentioned in \cite{aggarwal2010survey} that a modification of \cite{rattigan2007graph} chooses landmarks using local closeness centrality - can we do that, too?}\blueI{I dont know. Didnt read it}

Eppstein and Wang showed in \cite{eppstein:2001:fastCCapprox} a fast approximation algorithm to derive the closeness centrality of a node by sampling only a few nodes  and computing shortest paths from  to all other nodes (-SP) instead of calculating all pairs shortest paths (APSP).
These ideas can be implemented efficiently in a distributed setting by using Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} of our proposed framework.
\ifshort 
A detailed description is given in Appendix \ref{FULL:sec:CC}.\\
\fi
\iffull
The Algorithm \ref{alg:CC} first computes  with a breath-first-search algorithm starting in node , then the value of  gets broadcasted through  in the network.
When a node has received , it joins  as a sample node with probability . The number of nodes joining set  are collected and broadcasted in Lines \ref{line:CCjoinSstart} to \ref{line:CCjoinSend}. In Line \ref{line:CCaccmessages}, if a node  receives multiple messages in one time slot, then in the next time slot node  sends the sum of the values of all messages to its parent in .
Now, in line \ref{line:CCexAlg}, each node executes Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp}. No algorithm  or  
specified for Closeness Centrality is needed, as Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} already computes the required distances.
After Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} is executed, every node knows its distances  to all sample nodes  and therefore the number of samples . Using those distances, every node  can approximate its closeness centrality . \blueI{note: no aggregation / Algo2 needed.}

\begin{algorithm}[htb]
\begin{algorithmic}[1]
\IF{}
        \STATE \textbf{compute} tree  and \textbf{estimate}  by using Fact \ref{fact:ecc-approx-diam};
        \STATE \textbf{broadcast} values of  and  on ;
\ELSE
        \STATE \textbf{wait} until values of  and  \textbf{received};
\ENDIF
\STATE  \textbf{joins}  with probability ;
\IF{} \label{line:CCjoinSstart}
        \STATE \textbf{send} (``1'') to node  by ; \COMMENT{accumulate messages}\label{line:CCaccmessages}
\ENDIF
\IF{}
        \STATE \textbf{wait} for responses ;
        \STATE ;
        \STATE \textbf{broadcast}  through ;
\ELSE
        \STATE \textbf{receive} value of ;
\ENDIF \label{line:CCjoinSend}
\STATE \textbf{execute} Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} with ; \label{line:CCexAlg}
\STATE  //** ; is the distance of  to sample node 
\end{algorithmic}
\caption{ Approximation (executed by node ) \newline Input:  }\label{alg:CC}
\vspace*{0.5cm}
\end{algorithm}

\begin{lemma}
Algorithm \ref{alg:CC} approximates closeness centrality of all nodes with an inverse additive error of  in time  in unweighted graphs.
\label{lemma:ccunweighted}
\end{lemma}

\begin{proof}
As stated by Epstein and Wang in \cite{eppstein:2001:fastCCapprox}, by using  samples, we can approximate the closeness centrality w.h.p. with an inverse additive error of . As shown in Theorem \ref{theo:DLGruntime}, Algorithm\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} takes  time to complete. Computing, estimating and/or broadcasting ,  and  takes in each case  time. Algorithm \ref{alg:CC} therefore takes  time for unweighed graphs, since  in unweighted graphs.
\end{proof} 



\begin{proof}[Proof of Theorem \ref{theo:ccapprox}]
In the proof of Epstein and Wang in \cite{eppstein:2001:fastCCapprox}, all  can be substituted by  and the proof is still valid. Hence, by using  samples we can approximate the closeness centrality w.h.p. with an inverse additive error of .

As shown in Lemma \ref{lemma:ccunweighted} Algorithm \ref{alg:CC} takes  time to complete. Thus, by sampling  root nodes, Algorithm \ref{alg:CC} needs  time .
\end{proof} 
\fi

\iffull
\subsection{Minimum Routing Cost Tree}
\label{app:MRCT}

The S-MRCT-Algorithm in \cite{hochuli:holzer:MRCST} is executed in a similar fashion as our proposed Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}, as we generalized the aggregation concept of the S-MRCT-Algorithm \cite{hochuli:holzer:MRCST}.
Therefore the S-MRCT-Algorithm as stated in \cite{hochuli:holzer:MRCST} can be simulated with the Tree Variation of our proposed framework, as stated in Algorithms\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing} and\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating} .


The S-MRCT-Algorithm in \cite{hochuli:holzer:MRCST} computes a (,2)-approximation of the S-Minimum Routing Cost Spanning Tree (S-MRCT) problem. It relies on the recursive computation of  routing costs as stated in Lemma \ref{lemma:recRC}. The number of nodes of the set  which are in the subtree of a child  is denoted as  and  \todoI{update} denotes the number of all other nodes in set .

\begin{lemma}[Lemma 5.6 in \cite{hochuli:holzer:MRCST}]\label{lemma:recRC}

\end{lemma}

Lemma \ref{lemma:recRC} in words: the routing cost  of a subtree  rooted in  of tree  is the sum of two sums. First, the sum of the routing cost  of all subtrees  rooted in the children  of . And second, the sum of the routing cost off all edges  from  to all its children . The product  denotes the number of --paths in  which include the edge , for . Note that we set the routing cost  to be  for a node  which is a leaf in . And  for the root node  of tree \todoI{Notation anpassen}.

The idea of the S-MRCT-Algorithm in \cite{hochuli:holzer:MRCST} is, in a first step to compute in parallel  trees  rooted in  for each node . And in a second step to compute the routing cost  for each tree  in parallel. Therefore the algorithm aggregates the routing costs  of the subtrees  in tree  in a bottom-up fashion with the formula stated in Lemma \ref{lemma:recRC}. Finally the algorithm selects the tree  with the minimum routing cost  and broadcasts the result in the network.

We use Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT}  and Algorithm  (whose pseudocode is stated in Algorithm \ref{alg:MRCT_g}) to transfer the S-MRCT-Algorithm from \cite{hochuli:holzer:MRCST} into our framework. The subalgorithm  denotes the algorithm  of our framework, specified for the S-MRCT approximation.
Since we are only interested in Trees and not in DLGs, the Tree Variation involving Algorithms\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}  and\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating}  is used. 
Assume each node  knows weather it is in set  and  knows . Further the weight of an edge denotes the routing cost for that edge.

Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT}  starts with estimating and distributing  in the graph (Line \ref{line:MRCTstart} to \ref{line:MRCTendestim}). This is needed to determine the runtime of Algorithms\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}  and\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating} . Algorithm\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}  executed in Line \ref{line:MRCTalgdist} is only needed to compute the trees  for Algorithm\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating}  for each node . Therefore we do not need algorithm  for the execution. During the aggregating in Line \ref{line:MRCTalgagg}, algorithm  executed inside Algorithm\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating}  computes the routing costs for the trees  by applying the recursive formula stated above. Algorithm  therefore uses the recursive relation of the routing cost stated in Lemma \ref{lemma:recRC}.

In  Line \ref{line:gmrctinitstart} to \ref{line:gmrctinitend}, each node  is initialized with zero routing cost  (per root node ) and  the number of root nodes  in subtree  is set to  if , and to  otherwise. In Line \ref{line:gmrctinitmsg}, the message  is initialized in case node  is a leaf in tree , and therefore is sent to the parent of  in  before  is executed the first time. If  is not a leaf in in tree , then the values of ,  and  will change during the execution of Algorithm\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating} .

Back in Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT}  in Lines \ref{line:MRCTinftystart} to \ref{line:MRCTinftyend}, after the execution of Algorithms\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}  and\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating} , all nodes  store the routing cost  of their tree  in , all other nodes  store . Then, in Line \ref{line:MRCTminstart} to \ref{line:MRCTminend} the tree with the smallest routing cost is selected and broadcast.



\begin{proof}[Proof of Theorem \ref{theo:mrctapprox}]
Note that our Algorithms compute exactly the same trees as Algorithm 1 in \cite{hochuli:holzer:MRCST}, as received messages (which determine the parent) from neighbors are forwarded based on the same rule (prioritizing neighbor IDs) in both algorithms.
As a consequence Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT}  computes the same results as the S-MRCT algorithm in \cite{hochuli:holzer:MRCST} by aggregating along the trees computed by Algorithm\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}. Lines \ref{line:MRCTstart} to \ref{line:MRCTendestim}, \ref{line:MRCTaggmin} and \ref{line:MRCTbroadmin} take time  to estimate, broadcast and aggregate along tree . Algorithms\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}  and\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating}  are bound by time , as seen before. Hence, Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT}  runs in time . Theorem 2 in \cite{hochuli:holzer:MRCST} states that the algorithm that we transfer into our setting computes a -approximation to an MRCT. Note that \cite{hochuli:holzer:MRCST} assumes that  is uniform or corresponds to the time needed to transmit a message through edge .
\end{proof}

\begin{proof}[Proof of Theorem \ref{theo:mrctapprox-rand}]
Algorithm \textsc{MRCTrand} is very similar to Algorithm~\ref{alg:MRCT} \textsc{MRCT}, only the set of root nodes is different. 
Like in \cite{hochuli:holzer:MRCST} we sample a set  of size  uniformly at random and then execute Algorithm\iffull~\ref{alg:MRCT}\fi~\textsc{MRCT} on set . In the end we choose the tree with smallest routing cost as the approximation. Theorem 3 in \cite{hochuli:holzer:MRCST} states this yields a -approximation to an MRCT. From the proof of Theorem \ref{theo:mrctapprox} we know that the runtime of opur implementation is . Note that \cite{hochuli:holzer:MRCST} assumes that  is uniform or corresponds to the time needed to transmit a message through edge .
\end{proof}

\begin{algorithm}[htb]
\begin{algorithmic}[1]
\IF{}\label{line:MRCTstart}
        \STATE \textbf{estimate}  by generating a spanning tree  and using Fact \ref{fact:ecc-approx-diam};
        \STATE \textbf{broadcast}  through ;
\ELSE
        \STATE \textbf{receive} value of ;
\ENDIF \label{line:MRCTendestim}
\STATE \textbf{execute} Algorithm\iffull~\ref{alg:TreeComputing}\fi~\textsc{TreeComputing}  with ; \label{line:MRCTalgdist}
\STATE \textbf{execute} Algorithm\iffull~\ref{alg:TreeAggregating}\fi~\textsc{TreeAggregating}  with ; \label{line:MRCTalgagg}
\IF{}\label{line:MRCTinftystart}
        \STATE ;
\ELSE
        \STATE ;
\ENDIF \label{line:MRCTinftyend}
\IF{} \label{line:MRCTminstart}
        \STATE \textbf{aggregate}  via ; \label{line:MRCTaggmin}
        \STATE \textbf{broadcast} `` is -approximation for S-MRCT''; \label{line:MRCTbroadmin}
\ENDIF \label{line:MRCTminend}
\end{algorithmic}
\caption{\textit{MRCT} Approximation (executed by node )}\label{alg:MRCT}
\vspace*{0.5cm}
\end{algorithm}

\begin{algorithm}[htb]
\begin{algorithmic}[1]
\STATE \COMMENT{INITIALIZATION} 
\STATE \textbf{global} ; \label{line:gmrctinitstart}
\IF{}
        \STATE ;
\ELSE
        \STATE ; 
\ENDIF \label{line:gmrctinitend}
\STATE \textbf{for each}  \textbf{do} ; \COMMENT{output if  is a leaf}\label{line:gmrctinitmsg}\newline

\STATE \COMMENT{COMPUTATION, :  root node ID of the message,  message of a child  in }
\STATE ;
\STATE ;
\STATE ;
\STATE ; \COMMENT{output}
\end{algorithmic}
\caption{}
\label{alg:MRCT_g}
\vspace*{0.5cm}
\end{algorithm}

\fi





\addcontentsline{toc}{section}{References} \bibliographystyle{abbrv}
\bibliography{references}

\begin{center}
\textbf{Appendix}
\end{center}



\subsection{Tree Variation}\label{app:tree}

In schedule  initialized in Algorithm \ref{alg:DLGcomp} (Line \ref{line:tauinit}) we only store the arrival time of the first -message per root node .
Therefore the second dimension of the array  representing the schedule can be omitted in both, Algorithm \ref{alg:DLGcomp} and \ref{alg:DLGagr}.
The number of parents  can be omitted too.
And since we have to handle only one -message per root node , we rewrite Lines \ref{line:treevarstart}--\ref{line:treevarend} of Algorithm \ref{alg:DLGcomp} as stated in Algorithm \ref{alg:TreeComputing}.


In Algorithm\iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} just a minor modification is required. 
The \textit{`For each'}-loop in Line \ref{line:sendreceiveagg} needs to iterate over one parent  per tree  only, since in a tree a node has at most one parent.

The fully rewritten Tree Variation of Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and \iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr} are stated in Algorithm \ref{alg:TreeComputing} and \ref{alg:TreeAggregating}.


\begin{proof}[Proof of Theorem \ref{theo:tree}]
By modifying Algorithms\iffull~\ref{alg:DLGcomp}\fi~\textsc{DLGcomp} and \iffull~\ref{alg:DLGagr}\fi~\textsc{DLGagr}, we only changed the space requirements inside the nodes  and did not change anything concerning the running time. Therefore the time complexity remains   after the modifications.
To show that the modification computes valid trees, note that a tree  is a subgraph of a DLG . A tree  can be computed from a DLG , by removing all but one edge to the parents at each node , except for the root node , which has no parent node at all. This is done in Algorithm \ref{alg:TreeComputing} by considering only the neighbor node  as a parent in tree , which sends the first -message.
\end{proof}











\begin{algorithm}[H]
\small
\begin{algorithmic}[1]
\STATE \COMMENT{On node globally usable variables:}
\STATE , ;
\STATE ; \COMMENT{ time when the first message of tree  reaches }
\STATE ; \COMMENT{, }
\IF{}
        \STATE ;
        \STATE ;
        \STATE ;
\ENDIF
\STATE ;
\STATE \textbf{initialize} algorithm ;
\FOR{}
        \FOR{}
                \STATE ;
        \ENDFOR
        \STATE within one time slot:
        \newline \textbf{send}  to neighbor , receive  from ;
        \newline \textbf{send}  to neighbor , receive  from ;
        \newline 
        \newline \textbf{send}  to neighbor , receive  from ;
        
        \STATE ;
        \STATE L_{delay} = \emptyset;
        \IF{}
                \STATE ;
        \ENDIF
        
        \FOR{}
                \IF['s message will be delayed due to .]{}
                        \IF{}
                                \STATE ;
                                \STATE  neighbor ;
                                \STATE \textbf{execute} algorithm ;
                                \STATE ;
                                \STATE ;
                                \IF{}
                                        \STATE ;
                                \ENDIF
                        \ELSE
                                \STATE ;
                        \ENDIF
                \ELSE
                        \STATE ; \COMMENT{'s message was successfully sent to neighbor .}
                \ENDIF
        \ENDFOR
\ENDFOR
\end{algorithmic}
\caption{computing  trees (executed by node ) \newline 
        \textbf{Input:} , ,  \newline 
        \textbf{passed parameters on execution of :} : message of the parent in tree , : message that is sent to children in tree  after execution}
\label{alg:TreeComputing}
\end{algorithm}


\begin{algorithm}[ht]
\begin{algorithmic}[1]
\STATE \textbf{initialize} algorithm ;
\FOR{}
        \STATE within one time slot:
        \newline \textbf{For each}  \\
                 \ \ \ \textbf{if}  \textbf{then}\\
                         \ \ \ \ \ \ \ \textbf{send}  to ;
        \newline \textbf{receive}  from neighbor ;
        \newline \textbf{receive}  from neighbor ;
        \newline 
        \newline \textbf{receive}  from neighbor ;
        \FOR{}
                \IF{}
                        \STATE \textbf{execute} algorithm ;
                \ENDIF
        \ENDFOR
\ENDFOR
\end{algorithmic}
\caption{Tree aggregation (executed by node ) \newline
        \textbf{Input:} , ,    \newline
        \textbf{passed parameters on execution of :} : message of a child in tree , : message that is sent to parents in tree  after execution}
\label{alg:TreeAggregating}
\end{algorithm}














\end{document}
