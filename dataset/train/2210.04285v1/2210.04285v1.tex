\documentclass[final,5p,times,twocolumn]{elsarticle}
\usepackage{amssymb}
\usepackage{fixltx2e}
\usepackage[dvipsnames,table,xcdraw]{xcolor}
\usepackage{textcomp}
\usepackage[fleqn]{mathtools}
\definecolor{MyGreen-rgb}{rgb}{0,0.91,0.04001}
\definecolor{MyGreen-hsb}{hsb}{0.34065,1,0.91}
\definecolor{MyGreen-gray}{gray}{0.5383}
\colorlet{st}{MyGreen-rgb!75!black}
\colorlet{lv}{MyGreen-hsb!20!blue}
\colorlet{du}{MyGreen-gray!85!black}
\usepackage{amsmath}
\usepackage{endnotes}
\usepackage{stackengine}
\usepackage{babel}
\usepackage{xr-hyper}
\usepackage[colorlinks]{hyperref}
\usepackage[nameinlink,capitalise]{cleveref}
\usepackage[normalem]{ulem}
\usepackage{tikz}
\usepackage{floatrow}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{epstopdf} 
\usepackage{subcaption}
\usepackage{caption}
\captionsetup[figure]{labelfont={bf},name={Fig.},labelsep=period}
\captionsetup[table]{
  skip=0pt,
  labelfont={bf},
  justification=raggedright,
  labelsep=newline, singlelinecheck=false }
\floatstyle{plaintop}
\restylefloat{table}
\captionsetup{skip=0pt}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[export]{adjustbox}
\usepackage{comment}



\makeatletter
\newcommand{\crefrangeconjunction}{--}
\newcommand{\mathleft}{\@fleqntrue\@mathmargin0pt}
\newcommand{\mathnew}{\@fleqntrue\@mathmargin53pt}
\newcommand{\mathcenter}{\@fleqnfalse}
\raggedbottom



\renewcommand{\figureautorefname}{Fig.}
\renewcommand{\subsectionautorefname}{section}
\renewcommand{\equationautorefname}{Eq.}
\renewcommand{\tableautorefname}{Table}
\newcommand\upstrut{\rule{0pt}{10pt}}
\newcommand\downstrut{\rule{0pt}{0.5pt}}
\newcommand\mystrut{\upstrut\downstrut}
\captionsetup{belowskip=0pt,aboveskip=0pt}


\makeatother

\externaldocument{supplementarymaterial}
\begin{document}

\begin{frontmatter}

\title{Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained Deep Neural Networks}

\author[a]{Samra Irshad\corref{aaa}}
\ead{sam.ershad@yahoo.com}
\author[b]{Douglas P.S. Gomes}
\author[c]{Seong Tae Kim}
\address[a]{Swinburne University of Technology, Hawthorn, Australia}
\address[b]{Victoria University, Melbourne, Australia}
\address[c]{Kyung Hee University, Yongin-si, Gyeonggi-do, South Korea}
\cortext[aaa]{Corresponding Author.}
\begin{abstract}
Background and Objective: Quantitative assessment of the abdominal region from clinically acquired CT scans requires the simultaneous segmentation of abdominal organs. Therefore, for the past two decades, automatic abdominal image segmentation has been the subject of intensive research to facilitate the health professionals easing the clinical workflow. Thanks to the availability of high-performance and powerful computational resources, deep learning-based methods have resulted in state-of-the-art performance for the segmentation of 3D abdominal CT scans. However, the complex characterization of organs with fuzzy and weak boundaries prevents the deep learning methods from accurately segmenting these anatomical organs. Specifically, the voxels on the boundary of organs are more vulnerable to misprediction due to the highly-varying intensity of inter-organ boundaries, and the misprediction of these voxels is detrimental to overall segmentation performance. This paper investigates the possibility of improving the abdominal image segmentation performance of the existing 3D encoder-decoder networks by leveraging organ-boundary prediction as a complementary task.\\

Method: To address the problem of abdominal multi-organ segmentation, we train the 3D encoder-decoder network to simultaneously segment the abdominal organs and their corresponding boundaries in CT scans via multi-task learning. The network is trained end-to-end using a loss function that combines two task-specific losses, i.e., complete organ segmentation loss and boundary prediction loss. We explore two different network topologies based on the extent of weights shared between the two tasks within a unified multi-task framework. In the first topology, the whole-organ prediction task and the boundary detection task share all the layers in the encoder-decoder network except for the last task-specific prediction layers. In contrast, the second topology employs a single shared encoder but two separate task-specific decoders. To evaluate the utilization of complementary boundary prediction task in improving the abdominal multi-organ segmentation, we use three state-of-the-art encoder-decoder networks: 3D UNet, 3D UNet, and 3D Attention-UNet. \\

Results: The effectiveness of utilizing the organs' boundary information for abdominal multi-organ segmentation is evaluated on two publically available abdominal CT datasets: Pancreas-CT and the BTCV dataset. The improvements shown in segmentation results (evaluated via Dice Score, Average Hausdorff Distance, Recall, and Precision) reveal the advantage of the multi-task training that forces the network to pay attention to ambiguous boundaries of organs. A maximum relative improvement of 3.5\% and 3.6\% is observed in Mean Dice Score for Pancreas-CT and BTCV datasets, respectively. All source codes are publically available on \url{https://github.com/samra-irshad/3d-boundary-constrained-networks}.
\end{abstract}

\begin{keyword}




Abdominal multi-organ segmentation \sep Fully convolutional neural networks \sep Boundary-constrained segmentation \sep Multi-task learning
\end{keyword}

\end{frontmatter}



\section{Introduction}
\label{sec:intro}
Multi-organ segmentation on abdominal Computed Tomography (CT) scans is an essential prerequisite for computer-assisted surgery and organ transplantation \cite{okada2012multi}, \cite{gibson2018automatic}. Particularly, quantitative assessment of abdominal regions enables accurate organ dose calculation, required in numerous radiotherapy treatment options. Erroneous delineation of abdominal organs prevents harnessing the benefits of radiotherapeutic advancements. In clinical practice, physicians delineate abdominal organs using manual segmentation tools, which are time-consuming, observer-dependent, and error-prone. With the increased use of imaging facilities and production of a large number of abdominal CT scans, the utilization of automated, robust, and efficient organ-delineation tools has become compulsory \cite{gibson2018automatic}, \cite{hu2017automatic}, \cite{article1kim}. Automatic segmentation tools delineate the abdominal structures much faster and overcome the issues like variability in human expertise and inherent subjectivity.

Abdominal CT scans often present weak inter-organ boundaries characterized by regions of similar voxel intensities, which in turn results in low-contrast representations. Such appearances are usually caused by the representation of abdominal soft tissues in a narrow band of Hounsfield (HU) values. Another factor that enhances the already complex representation of abdominal organs is the existence of artifacts occurring due to blood flow, respiratory, and cardiac motion. Accurate delineation of abdominal organs with unclear boundaries and complex geometrical shapes is one of the ongoing challenges that hurdles the abdominal-related clinical diagnosis. 
\begin{figure}[!h]
\begin{subfigure}[b]{.3\textwidth}
\hspace{-1mm}
\includegraphics[width=\textwidth]{Figures/annon_img_ct3.jpg} 
\caption{}
\label{fig:sub1}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\includegraphics[width=\textwidth]{Figures/annon_img_ct2.jpg} 
\caption{}
\label{fig:sub2}
\end{subfigure}\begin{subfigure}[b]{.3\textwidth}
\includegraphics[width=\textwidth]{Figures/panc_ct6_3d.jpg} 
\caption{}
\label{fig:sub3}
\end{subfigure}\caption{Exemplary 2D abdominal CT image showing the visual characteristics of organs. (a) 2D abdominal image, (b) Abdominal organs annotated on CT image: pancreas ( \raisebox{0.3em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), spleen ( \raisebox{0.3em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), liver ( \raisebox{0.3em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), stomach ( \raisebox{0.3em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), (c) 3D multi-organ voxel map.}\label{fig:ctscan2}
\end{figure}

Earlier methods proposed for the abdominal multi-organ segmentation mainly were based on multi-atlas \cite{xu2015efficient}, \cite{suzuki2012multi} or statistical models \cite{cerrolaza2015automatic}, \cite{okada2015abdominal}. Some methods also made use of handcrafted or learned features to segment abdominal organs \cite{campadelli2009automatic}, \cite{Selver2014SegmentationOA}. However, the recent Fully Convolutional Network (FCN) based approaches have presented better results due to the improved organ representation learning \cite{gibson2018automatic} \cite{heinrich2019obelisk}. Being able to preserve the image structure and provision of efficient learning as well as inference, FCN-based methods are currently considered state-of-the-art for abdominal multi-organ segmentation \cite{gibson2018automatic}, \cite{bobo2018fully}, \cite{sinha2020multi}, \cite{Lu2022}. Specifically, these networks follow the encoder-decoder architectural design \cite{ronneberger2015u}. In such networks, the shallow layers in the encoder aim to extract low-level features, and the deep layers encode high-level features. While the mirrored-decoder maps back the learned features to generate an output of the same size as input with skip connections assisting in retaining the crucial features extracted in the encoding path \cite{ronneberger2015u}.

Existing FCN-based methods for abdominal multi-organ segmentation employ either 2D or 3D convolutional architectures \cite{sinha2020multi}, \cite{bobo2018fully}. 2D methods process the CT scans in a slice-by-slice fashion and predict the organ labels on individual slices \cite{sinha2020multi}. Despite being memory- and parameter-efficient, 2D methods are unable to make full use of 3D contextual information \cite{gibson2018automatic}. 3D methods make use of rich volumetric context by processing the whole CT volume and generating voxel-maps in a single forward propagation pass, leading to better abdominal CT segmentation performance than 2D approaches \cite{Roth2018AnAO}, \cite{Zhou2018PerformanceEO}.

The existing 3D methods have primarily focused on designing better architectures for improved abdominal multi-organ representation learning \cite{bobo2018fully}, \cite{gibson2018automatic}. However, they treat all the anatomical parts within a single organ equally since they solely rely on voxel-level information and do not specifically focus on improving the segmentation of voxels in vulnerable regions/parts of organs. As an example, we highlight some of the important characteristics of abdominal organs in \autoref{fig:ctscan2}. From \Cref{fig:sub1,fig:sub2}, it can be noticed that the adjacent organs have weak contours which sometimes touch each other. As an example, observe the low-contrasted and touching boundaries between stomach ( \raisebox{0.3em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ) and pancreas ( \raisebox{0.3em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). Moreover, 3D multi-organ visualization in \autoref{fig:sub3} shows that the adjacent positioning of organs in the abdominal cavity aggravates the complex spatial relationship among the organs. Simultaneously segmenting the abdominal organs with soft contours and complex spatial relationships is a challenging task.

The boundaries of anatomical regions in medical scans serve as an important cue for facilitating manual and automated delineation \cite{Guo2021}. Numerous existing deep learning-based studies leveraged learning of features corresponding to boundary of regions for improved medical image segmentation via multitask learning paradigm \cite{chen2016dcan}, \cite{8363791}, \cite{8906008}, \cite{9313420}, \cite{MEI2020101988}. In recent years, deep multitask learning paradigm has been widely used due to its potential to solve multiple tasks in one forward propagation and ability to learn better representations because of the multiple supervisory signals \cite{Amyar2020MultitaskDL}, \cite{Du2019CrossInfoNetMI}. In this paper, we propose to improve the segmentation of abdominal organs on CT scans by enhancing the segmentation of boundary of organs. Particularly, we train the 3D deep learning networks to simultaneously predict the boundary and the entire region of organs. The inclusion of boundary information is motivated by the fact that the voxels on the boundary of organs are more vulnerable to misprediction because of their ambiguous appearance and complex relationship with adjacent organs. Specifically, our work makes the following contributions:
\begin{enumerate}[label=(\roman*)]
\item We develop an end-to-end trainable 3D multi-task learning framework that simultaneously predicts the voxel-labels of abdominal organs and their corresponding boundaries. By integrating the boundary features, our proposed boundary-constrained 3D deep learning framework focuses on the accurate prediction of the edges of organs in addition to whole organs.
\item Instead of relying on a single network topology, we explore and compare two network topologies for conducting multi-task learning. In the first topology, the whole encoder-decoder network is shared with separate task-specific prediction layers at the end for predicting boundaries and entire organs' maps. In the second topology, an encoder is shared with separate task-specific decoders for decoding the features, jointly learned by the shared encoder to predict the boundary and organ probability maps. With an extensive comparison, we reveal that integration of boundary features invariably improves the multi-organ segmentation performance, independent of the multi-task network design.
\item We utilize three state-of-the-art 3D encoder-decoder architectures, i.e., UNet \cite{cciccek20163d}, UNet \cite{zhou2019unet++}, and Attention-UNet \cite{schlemper2019attention} as baseline networks for evaluating the effect of incorporating boundary information. We modifiy each baseline architecture according to our proposed multi-task topologies. We demonstrate significant performance improvements with a negligible increase in trainable parameters.
\item We validate the performance of baseline and counterpart boundary-constrained models on two publically available datasets (Pancreas-CT \cite{Roth2016} and BTCV \cite{landmanbvc}) using Dice Score, Average Hausdorff Distance, Recall, and Precision. Furthermore, we conduct additional experiments to evaluate the improvement in the segmentation of regions around the boundaries. The results show that the boundary-constrained networks learn feature representations that focus on the accurate organs segmentation and the challenging parts around the border of the organs. 
\end{enumerate}
The rest of the article is organized as follows. In \autoref{sec:related}, we review the existing methods for abdominal multi-organ segmentation. \Cref{sec:method} describes our framework for incorporating the boundary information into the 3D fully convolutional networks, including the multi-task loss function and the details of boundary-constrained network topologies. Next, we describe the dataset specifications and implementation details in \autoref{sec:experiment}. We then present the experimental results, comparisons with existing single-task approaches, and in-depth performance analysis of boundary-constrained models in \autoref{sec:result}. Finally, we discuss the important highlights and some directions for future work in \autoref{sec:discuss} and present the conclusion in \autoref{sec:conc}. 
\begin{figure*}[!h]
\begin{subfigure}[b]{.45\textwidth}
\hspace{3mm}
\includegraphics[width=\textwidth]{Figures/mtl1_gen_new1.jpg} 
\caption{}
\label{fig:unet_mtl_gen1}
\end{subfigure}\hfill
\begin{subfigure}[b]{.42\textwidth}
\hspace{-5mm}
\includegraphics[width=\textwidth]{Figures/mtl2_gen_new1.jpg}
\caption{}
\label{fig:unet_mtl_gen1k}
\end{subfigure}
\caption{Multi-task topologies of 3D boundary-constrained network. (a) Multi-task topology with shared encoder-decoder network and task-specific prediction layers, and (b) Multi-task topology with shared encoder and task-specific decoders.}
\label{fig:unet_mtl_gen2}
\end{figure*}
\section{Related Work}\label{sec:related}
Segmentation of anatomical structures from abdominal scans is a prerequisite for various high-level CT-based clinical applications. Existing computerized tools for abdominal image segmentation are either based on deep learning or non-deep learning methods. In this section, we first briefly discuss the non-deep learning methods (\autoref{sec:classic}) and then present a review of deep learning-based methods for abdominal multi-organ segmentation (\autoref{sec:dnns}). We conclude this section with a discussion on multi-task deep neural networks being employed for complementary boundary learning task to improve medical image segmentation (\autoref{sec:boundary}).
\subsection{Non-deep learning-based abdominal organs segmentation}\label{sec:classic}
Earlier methods proposed for abdominal multi-organ segmentation have primarily utilized registration-based approaches \cite{cerrolaza2015automatic},  \cite{okada2015abdominal}. Among the registration-based approaches, the widely used ones include statistical shape models \cite{cerrolaza2015automatic}, \cite{okada2015abdominal} and multi-atlas label fusion techniques \cite{xu2015efficient}, \cite{suzuki2012multi}. The development of statistical models requires registration of training images for estimating the shape or appearance of anatomical organs followed by fitting constructed models to test images for generating segmentations \cite{heimann2009statistical}, \cite{cootes2001active}. Multi atlas-based methods utilize an atlas created using multiple labelled images in the training set, and the test image is segmented by propagating the reference segmentations. Atlases are constructed by capturing the prior anatomical knowledge relevant to target organs. However, it is difficult to build an adequate model to capture the large variability of the deformable organs with limited data \cite{xu2016evaluation}. Furthermore, the performance of both these approaches is restricted by image registration accuracy. 

Registration-free approaches train a classifier using either handcrafted or learned features to segment abdominal images \cite{campadelli2009automatic}. Extraction of robust and deformation-invariant features relies on expert knowledge about abdominal organs \cite{lombaert2014laplacian}. Having the ability to learn the features automatically, FCN-based methods, have rapidly replaced the traditional solutions that require image registration or handcrafted features and have shown improved performance for abdominal CT segmentation \cite{gibson2018automatic}, \cite{bobo2018fully}, \cite{sinha2020multi}, \cite{peng2019method}. 
\subsection{Fully Convolutional Networks for abdominal multi-organ segmentation}\label{sec:dnns}
In recent years, Fully Convolution Network (FCN) and its variants (e.g., UNet \cite{ronneberger2015u}) have become a common choice for medical image segmentation. This dominancy can be attributed to their ability to learn effective task representations and efficient inference. UNet has an encoder-decoder style architecture and consists of skip connections, joining the encoding and decoding layers on the same level. Despite being trained from scratch, UNet demonstrated state-of-the-art performance for various medical image segmentation tasks \cite{chen2017rbnet}, \cite{Dong2017AutomaticBT}. Built on top of UNet, several other modified architectures were subsequently proposed, e.g., UNet \cite{zhou2019unet++}, Attention-UNet \cite{schlemper2019attention}, etc. 

Existing deep learning-based studies for abdominal multi-organ segmentation have utilized 2D or 3D convolutional networks. 2D methods are less parameter-intensive; however, they cannot exploit the 3D contextual information and eventually provide sub-accurate organ-delineation performance. 3D convolutional networks are facilitated with 3D convolutions, 3D pooling, and 3D normalization to exploit the rich volumetric context and generate dense voxel-wise predictions \cite{cciccek20163d}. Advances in efficient 3D convolutional implementation and increased GPU memory have enabled the adoption of 3D convolutional models for abdominal multi-organ segmentation \cite{Zhang2020BlockLS}, \cite{hu2017automatic}.

Roth et al. \cite{Roth2018AnAO} proposed a cascaded architecture based on two 3D UNets where the first UNet is trained to separate the abdominal area from the background, and the latter utilized the output from the first UNet to simultaneously segment the abdominal organs. Peng et al. \cite{Peng2020AMO} delineated abdominal organs using 3D UNet with residual-learning based units (ResNets) to calculate patient-specific CT organ dose. In another study \cite{gibson2018automatic}, abdominal organs are segmented using a 3D FCN with dilated convolutions based densely connected units. Heinrich et al. \cite{heinrich2019obelisk} leveraged 3D deformable convolutions to spatially adapt the receptive field for abdominal multi-organ segmentation. In \cite{liu2020ct}, abdominal scans were segmented using a 3D deeply supervised patch-based UNet with grid-based attention gates to encourage the network to focus on useful salient features propagated through the skip connections. Some existing methods have employed post-processing steps, including level-sets \cite{hu2017automatic} and graph-cut \cite{article1kim} to refine initial segmentation maps obtained from 3D deep convolutional networks. 

Through the efforts mentioned above, the existing 3D methods have mostly emphasized developing better deep learning architectures and did not attempt to improve the segmentation of challenging parts of abdominal organs, e.g., voxels that belong to the contour of organs and regions within the vicinity of organ-contour. The fuzzy appearance of the boundary of organs and low contrast between the adjacent abdominal structures makes the voxels belonging to these regions more susceptible to wrong label prediction. 
\subsection{Boundary-constrained medical image segmentation}\label{sec:boundary}
Several existing deep learning-based medical image segmentation methods have utilized the boundary information of regions of interest to overcome the misprediction of boundary pixels \cite{chen2016dcan}, \cite{8363791}, \cite{8906008}, \cite{Lee_2020_CVPR}. In these methods, the networks are trained in a multi-task learning fashion to simultaneously predict the probability maps of entire organs and their corresponding boundaries. Most of these methods have resorted to the hard-parameter sharing technique, where a single network contains shared and task-specific parameters and is jointly trained to solve multiple tasks. 

Chen et al. \cite{chen2016dcan} segmented the glands and their corresponding boundaries via multi-task training. By training the model to learn the co-representations, the model achieved better gland segmentation performance than the single-task models. In \cite{Oda2018BESNetBS}, a dual-decoder-based network is presented that simultaneously detects the boundaries and predicts the semantic labels of cells. Features from the boundary-decoding path were concatenated with those learned in the entire cell region decoding path via additional skip connections. This led to the improved histopathological image segmentation performance. In \cite{Murugesan2019PsiNetSA}, boundary and distance maps were used for improved polyp and optic disk segmentation, respectively. Tan et al. \cite{8363791} proposed a multi-task medical image segmentation network consisting of a single encoder and separate dedicated arms for decoding regions and boundaries. The study was evaluated on numerous applications, including MR femur and CT kidney segmentation. Zhang et al., \cite{zhang2019net} presented a edge-based deeply supervised network for predicting the regions of interest and their corresponding boundaries. The method was validated for retinal, x-ray, and CT image segmentation. Wang et al. \cite{Wang2019AUT} proposed a two-parallel stream model in which each of the two streams was trained to segment region and detect boundary followed by fusion of contour and region prediction maps. Lee et al. \cite{Lee_2020_CVPR} proposed a framework that predicts boundary keypoint maps and makes use of adversarial loss for improved boundary preserving in medical image segmentation.

Given the challenge presented by voxels on the organs’ boundaries and the evidence in the literature that focusing on boundaries is beneficial for performance, we integrate the organs boundary prediction as an auxiliary task into the training of state-of-the-art 3D medical image segmentation networks. Since the design choice of network topology impacts the learning process, we explore two multi-task network designs and analyze their performance. The boundary co-training resulted in improved performance on abdominal CT segmentation tasks compared to the several state-of-the-art 3D fully convolutional baseline architectures.
\section{Proposed Method}\label{sec:method}
In this section, we first describe the boundary-constrained loss for training the 3D encoder-decoder network to simultaneously predict the boundaries and entire abdominal organ regions via multi-task learning (\Cref{sec:propose}), followed by an exhibition of our proposed multi-task network topologies (\Cref{sec:topo}). After that, we discuss the architecture of the 3D networks that we have as baselines in our work (\cref{sec:base}). Finally, we present the architectural design of the counterpart 3D boundary-constrained models (\Cref{sec:bound}).
\subsection{Boundary-Constrained Loss}\label{sec:propose}
Consider a 3D encoder-decoder network trained to predict the voxel labels of the abdominal CT scan with  dimensions, where , , and   denote the length, width, and depth of the scan, respectively. Such a network takes an abdominal multi-organ CT scan as an input and outputs a labelled voxel map of the same size as the input. To utilize the boundary information of abdominal organs for improved representation learning, we train the network to predict the 3D organ-semantic masks and 3D organ-boundaries in one forward propagation pass. We formulate this problem using a multi-task learning paradigm where multiple tasks are learned jointly using shared and task-specific representations. The loss  for this multi-task learning problem is a weighted combination of per-task losses, organ segmentation loss  and organ boundary detection loss . We use multi-class dice loss \cite{milletari2016v} for evaluating the performance of the multi-organ segmentation task, given as

where,  and  denote the 3D multi-organ probability map and ground-truth mask, respectively, of the  abdominal CT scan.  denotes the number of organ classes.

\vspace{-6mm}

where  represents the label probability of  voxel in  scan and  refers to the total number of voxels in a scan. 

To evaluate the model's performance in predicting the boundaries, we use binary cross-entropy loss (shown in Eq. \autoref{eq:eq3.6}). Binary cross-entropy loss for predicting 3D boundaries is given as

 and  represent the edge probability map and the corresponding ground-truth. ) represents the edge probability of the  voxel in  scan.  represents the weights of the entire deep multi-task encoder-decoder network.

The combined total loss  is minimized with respect to the parameters , as shown in \autoref{eq:eq3.1}. Thus our goal is to evaluate if a network can learn more robust features and subsequently produce improved organ segmentations by being trained to explicitly recognize the boundaries.

 and  represents the total number of CT scans in the training set and the weight assigned to the edge detection loss in \autoref{eq:eq3.1}, respectively.

We hypothesize that the additional boundary loss () would impose a larger penalty on erroneous contour voxels, and it subsequently pushes the optimization of the segmentation network towards the solutions with more accurate boundaries. Thus, one would potentialize the ability of a boundary-constrained network to extract features that account for the semantic abdominal organ regions and boundaries. 
\subsection{Boundary-Constrained Network Topologies}\label{sec:topo}
Multi-task learning is generally formulated via hard-parameter sharing and soft-parameter sharing. In the hard-parameter sharing paradigm, multiple tasks share a subset of jointly optimized parameters, whereas task-specific parameters are optimized separately. In soft-parameter sharing, each task is parameterized using its own set of parameters which are jointly regularized using constraints \cite{Ruder2017AnOO}. In practice, hard-parameter sharing approaches incur much less parameter and computational cost. In our work, we formulate the multi-task learning problem via hard-parameter sharing to train the encoder-decoder network to do multiple tasks, i.e., organ segmentation and boundary detection. For deep neural networks, the hard-parameter sharing approach is realized by sharing some network layers between the tasks while keeping some layers task-specific.  

We explore two different network topologies to conduct multi-task training, as shown in \Cref{fig:unet_mtl_gen1,fig:unet_mtl_gen1k}. The motivation to explore multiple topologies is to investigate the impact of sharing the larger and smaller number of parameters in the network between the two tasks. We explain these multi-task topologies below.
\subsubsection{Task-Specific Output Layers (TSOL)}\label{sec:tsl}
The first multi-task topology that we explore is formulated by appending two separate prediction layers for predicting the boundaries and semantic organ masks. This topology employs an encoder-decoder network whose weights are shared between the tasks, except for the last output layers, as shown in \autoref{fig:unet_mtl_gen1}. Technically, it encourages the use of compact and tightly shared feature representations. As evident, this configuration has negligibly fewer more parameters than the single-task network. We denote this configuration as TSOL. 
\subsubsection{Task-Specific Decoders (TSD)}\label{sec:tsd}
In second mutli-task topology, we modify the 3D encoder-decoder model to have a single shared encoder but two separate decoding arms for predicting the semantic regions and boundaries. The sibling-decoding arms upsample the region and boundary maps separately. This type of formulation ensures sparse representation sharing amongst the two tasks since decoders have been parameterized separately, as shown in \autoref{fig:unet_mtl_gen1k}. The presence of two synthesis paths results in having significantly more parameters than its counterpart single-task network. We refer to this configuration as TSD.  
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.14]{Figures/unet_mtl1_correct1.jpg} 
\caption{3D UNet-MTL-TSOL: Multi-task learning based 3D UNet with task-specific output layers for simultaneously predicting the organs and their boundaries.}
\label{fig:unetmtl1}
\end{figure}
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.13]{Figures/unet_mtl2_correct1.jpg} 
\caption{3D UNet-MTL-TSD: Multi-task learning based 3D UNet with task-specific decoders for simultaneously predicting the organs and their boundaries.}
\label{fig:unetmtl2}
\end{figure}
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.13]{Figures/unetplus_correct_new_mtl1__1_.jpg} 
\caption{3D UNet-MTL-TSOL: Multi-task learning based 3D UNet with task-specific layers for simultaneously predicting the organs and their boundaries.}
\label{fig:unetplusmtl1}
\end{figure}
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.13]{Figures/unetplus_correct_mtl2__1_.jpg} 
\caption{3D UNet-MTL-TSD: Multi-task learning based 3D UNet with task-specific decoders for simultaneously predicting the organs and their boundaries.}
\label{fig:unetplusmtl2}
\end{figure}
\subsection{3D Baseline models}\label{sec:base}
We use UNet \cite{cciccek20163d}, UNet \cite{zhou2019unet++} and Attention-UNet (Att-UNet) \cite{schlemper2019attention} as our baseline models. We illustrate these models in Supplementary material (see Figures S1 - S3). These architectures are based on encoder-decoder design and extended to segment 3D volumes by replacing the 2D convolutions, pooling, and upsampling with 3D counterparts. Each baseline model processes a 3D input scan with dimensions  and outputs a predicted organ-label map of the same size as input. The encoder of the model contains five convolutional blocks with pooling layers, and the decoder comprises four upsampling layers. Each convolutional block in the encoder consists of two convolutional layers with  filters, followed by batch normalization and Exponential Linear Unit (ELU) activation \cite{DBLP:journals/corr/ClevertUH15}. We use padded convolutions to keep the output dimensions of convolutional layers the same as the input dimensions. A  max pooling layer with a stride of two in each dimension is sandwiched between every two convolutional blocks for feature maps’ downsampling. The bilinear interpolation layers are used in the decoder to upsample the extracted feature maps in each dimension. The feature maps in the decoder are concatenated with the equal-sized representations learned in the encoder via skip connections. The concatenated feature maps are then transformed using convolutional blocks, similar to those used in the encoder. The last  convolutional layer maps the feature channels to the class labels, followed by a \textit{softmax} activation.

The resolution of the smallest feature map is  and the minimum and maximum feature count at the first and last encoding stage is 16 and 256, respectively. Note that the original UNet model is trained with deep supervision driven by output layers of UNet with varying depths; however, we train UNet without deep supervision to constrain the computational expense.
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.13]{Figures/attenunet_mtl1_correct.jpg} 
\caption{3D Att-UNet-MTL-TSOL: Multi-task learning based 3D Att-UNet with task-specific prediction layers for simultaneously predicting the organs and their boundaries.}
\label{fig:attenunetmtl1}
\end{figure}
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.13]{Figures/attenunet_correct_mtl2__4_.jpg} 
\caption{3D Att-UNet-MTL-TSD: Multi-task learning based 3D Att-UNet with task-specific decoders for simultaneously predicting the organs and their boundaries.}
\label{fig:attenunetmtl2}
\end{figure}
\subsection{3D Boundary-contrained models}\label{sec:bound}
To utilize the boundary information of the organs, we train the baseline models (given in \Cref{sec:base}) to predict the organ boundaries along with organs. We propose two multi-task network topologies (shown in \autoref{fig:unet_mtl_gen2}) for integrating the boundary information and for that, we modify each baseline model, i.e., 3D UNet, 3D UNet, and 3D Att-UNet, according to two multi-task learning-based topologies. 

To modify the baseline models according to the first multi-task topology (TSOL) (shown in \autoref{fig:unet_mtl_gen1}), we append a separate head at the end to predict boundaries along with the organs. We refer to these models as UNet-MTL-TSOL, UNet-MTL-TSOL, and Att-UNet-MTL-TSOL, as shown in \Cref{fig:unetmtl1,fig:unetplusmtl1,fig:attenunetmtl1}, respectively. To modify the baseline models according to the second boundary-constrained multi-task topology (TSD) (shown in \autoref{fig:unet_mtl_gen1k}), we modify the baseline models (3D UNet, 3D UNet, and 3D Att-UNet) to have separate decoding paths followed by prediction layers for predicting boundaries and organs. We refer to the models modified according to this topology as UNet-MTL-TSD, UNet-MTL-TSD, and Att-UNet-MTL-TSD and show them in \Cref{fig:unetmtl2,fig:unetplusmtl2,fig:attenunetmtl2}, respectively. Note from \autoref{fig:unetplusmtl2}, we only use the skip connections between the encoder with the greatest depth and boundary-decoder instead of utilizing feature maps extracted by nested-UNets of all depths. This design choice is made to constrain the number of parameters in UNet-MTL-TSD. Furthermore, observe from \autoref{fig:attenunetmtl2}, we do not employ the attention mechanism while decoding the boundary-features in Att-UNet-MTL-TSD.
\section{Experimental details}\label{sec:experiment}
This section first describes the datasets used to validate our study and the pre-processing we perform on the datasets (\Cref{sec:data}), followed by implementation details (\Cref{sec:impl}). Finally, we conclude this section by discussing the metrics used to evaluate baseline and boundary-constrained models (\Cref{sec:metrics}).
\subsection{Description of datasets and data preprocessing}\label{sec:data}
We utilize two publically available abdominal CT datasets (Pancreas-CT and BTCV) to evaluate baseline and boundary-constrained models. Abdominal scans in Pancreas-CT were acquired at the National Institutes of Health Clinical Center from pre-nephrectomy healthy kidney donors and subjects with neither major abdominal pathologies nor pancreatic cancer lesions \cite{Roth2016}. The BTCV dataset consists of abdominal scans acquired at the Vanderbilt University Medical Center from metastatic liver cancer patients or post-operative ventral hernia patients \cite{xu2016evaluation}.
\subsubsection{Pancreas-CT Dataset (TCIA-43)}
The pancreas-CT dataset \cite{Roth2016}, \cite{RothLFSLTS15} comprised 82 abdominal contrast-enhanced 3D CT scans and was initially provided with manually drawn contours of the pancreas \cite{Clark2013}, \cite{RothLFSLTS15}. Recently, 43 scans from this dataset have been re-annotated to include the segmentation of the liver, duodenum, stomach, esophagus, spleen, gallbladder, and left kidney \cite{Gibson2018}. Therefore, we use only 43 scans that have been re-annotated to incorporate labels for multiple organs. 

We first crop a region-of-interest from the CT scans and the corresponding ground-truth labels using the bounding box coordinates provided with the dataset \cite{Gibson2018}. The cropping step ensures the models are only fed with the foreground inputs without the redundant background region. The cropped region-of-interest from the CT scans and ground-truth labels are then resampled to a common dimension of  voxels. We randomly divide the available 43 studies into 28, 5, and 10 for training, validation, and test, respectively.
\subsubsection{BTCV Dataset}
BTCV was released \cite{landmanbvc}, \cite{xu2016evaluation} as a part of a challenge held in conjunction with MICCAI 2015. The challenge compared the abdominal organs' segmentation algorithms on 3D CT scans. Our work focuses on the segmentation of eight organs from the BTCV dataset, i.e., liver, duodenum, stomach, esophagus, spleen, gallbladder, left kidney, and pancreas. 

For the BTCV dataset, we utilize the bounding box coordinates given with the dataset for cropping the region-of-interest for both the CT scans and ground-truth labels \cite{Gibson2018}. Like the Pancreas-CT dataset, the cropped region-of-interest is then resampled to a common dimension of  voxels. Finally, we randomly divide the available 47 studies into 32, 5, and 10 for training, validation, and test, respectively. 

For both Pancreas-CT and BTCV dataset, we applied affine random transformations to augment the data but did not observe a significant difference in segmentation performance on the validation set. Therefore, we did not use any data augmentation. We used the same dataset splits for all the experiments. To analyze the occurrence of each organ in the dataset, we present the organs' occupancy ratio in \Cref{fig:organ_btvc,fig:organ_panc}.
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{Figures/panc_piechart_new_eps_converted_to.pdf} 
\caption{Organ occupancy ratio for Pancreas-ct dataset.}
\label{fig:organ_btvc}
\end{figure}
\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{Figures/bvct_new_piechart_eps_converted_to.pdf} 
\caption{Organ occupancy ratio for BTCV dataset.}
\label{fig:organ_panc}
\end{figure}
\subsection{Implementation details}\label{sec:impl}
All experiments are conducted using Pytorch \cite{paszke2017automatic} on two Nvidia Tesla P100, accessed through the HPC platform\footnote{\url{https://supercomputing.swin.edu.au/}}. We train all the baseline networks with a mini-batch of size 4, except for 3D UNet, which is trained with one batch size. These choices have been made according to available GPU memory. All baseline (single-task) networks are trained using multi-class dice loss as expressed in \Cref{eq:eq3.3,eq:eq3.4,eq:eq3.5}. For acquiring multi-class dice loss, the 3D predicted organ-segmentation maps are compared with 3D organ ground-truth maps. All the networks (baseline and boundary-constrained) are optimized via Adam optimizer \cite{kingma2014adam}. The learning rate is initially set to 0.001, decaying by a factor of 0.9 after every epoch. To assess the effect of changing values of training hyperparameters on validation segmentation performance, we conduct experiments to guide us in selecting the optimal settings for baseline models. The results for these experiments are given in Tables S1 and S2 in Supplementary section. We monitor the mean dice score on the validation set during training and utilize the model for testing that results in the highest dice coefficient on the validation set. 

We use a combination of multi-class dice loss and binary cross-entropy loss, as illustrated in \autoref{eq:eq3.1} for training boundary-constrained models. 3D organ-boundary predictions are compared with the 3D boundary ground-truths to obtain the binary cross-entropy loss. Since the datasets do not contain the boundary annotations of organs, we acquire the ground-truth boundaries by first eroding the multi-organ ground-truth labels and then taking the difference from the original ground-truth map. This process gives us boundary annotations of organs. For UNet-MTL-TSOL, UNet-MTL-TSD, Att-UNet-MTL-TSOL and Att-UNet-MTL-TSD, we use a batch of size two, whereas, for UNet-MTL-TSOL and UNet-MTL-TSD, we use a single batch size. These choices are made according to the available GPU memory. Note that the boundary predictions are only used in the training stage. During the validation stage, we only consider the organ predictions. 

We conduct a grid search (on the range from 0 to 2 with a step of 0.5) to find the optimal value of  (responsible for balancing the boundary detection loss). The exact value of  selected to balance the boundary loss for each boundary-constrained model are given in Tables S3 and S4 in Supplementary material. These tables also present the standard deviation between validation dice scores when different values of  are used. 
\subsection{Evaluation metrics}\label{sec:metrics}
We compare the predicted segmentation masks with the ground-truths to evaluate the segmentation performance of baseline and boundary-constrained models on test set. To do that, we utilize Dice Score, Recall, Precision, and Average Hausdorff Distance as metrics for assessing the quality of predicted segmentation masks. These metrics are calculated for each organ individually and then an average is taken across all subjects. All metrics are calculated by taking a mean of the 5 runs.
\begin{comment}
\subsubsection{Dice Coefficient}
The Dice Coefficient (DC) measures the overlap between two regions by calculating their intersection and dividing it by union (as shown in equation \autoref{eq:eq5}). It is the most commonly used metric in evaluating the performance of medical image segmentation algorithms. DC gives a measurement value between 0 (no overlap) and 1 (complete overlap). 

\end{comment}
\begin{table*}[ht] \centering
\resizebox{\columnwidth}{!}{\caption{Quantitative comparison between baseline and boundary-contrained models for abdominal multi-organ segmentation: Mean values \textpm\ Std. Dev. of Dice Score, Avg. HD, Recall, and Precision are shown. Results in \textbf{BOLD} indicate the best results corresponding to each baseline.}\label{table:Table 3.1}
\begin{tabular}{lllll@{\hspace{10mm}}llll}
\hline
\mystrut &\multicolumn{4}{c}{\textbf{Pancreas-CT dataset}}&\multicolumn{4}{c}{\textbf{BTCV dataset}}\\
\cline{2-9}
\mystrut \textbf{Method} & \makecell[c]{\textbf{Dice score}}& \makecell[c]{\textbf{Avg. Hd}}& \makecell[c]{\textbf{Recall}}& \makecell[c]{\textbf{Precision}}&  \makecell[c]{\textbf{Dice score}}& \makecell[c]{\textbf{Avg. Hd}}& \makecell[c]{\textbf{Recall}}& \makecell[c]{\textbf{Precision}}\\
\cline{2-9}
\mystrut 3D UNet  &    \makecell[c]{0.799\textsubscript{\textpm\ 0.013}} & \makecell[c]{0.795\textsubscript{\textpm\ 0.088}}  & 
\makecell[c]{0.786\textsubscript{\textpm\ 0.049}} & \makecell[c]{0.850\textsubscript{\textpm\ 0.082}}  &
\makecell[c]{0.753\textsubscript{\textpm\ 0.008}} & \makecell[c]{1.085\textsubscript{\textpm\ 0.098}}&
\makecell[c]{0.760\textsubscript{\textpm\ 0.085}} & \makecell[c]{0.769\textsubscript{\textpm\ 0.116}}\\
\mystrut 3D UNet-MTL-TSOL &   \makecell[c]{0.813\textsubscript{\textpm\ 0.009}}& \makecell[c]{0.748\textsubscript{\textpm\ 0.130}}   &  
\makecell[c]{\textbf{0.807\textsubscript{\textpm\ 0.055}}}& \makecell[c]{0.849\textsubscript{\textpm\ 0.079}}         &                                        
\makecell[c]{0.775\textsubscript{\textpm\ 0.004}} & \makecell[c]{0.989\textsubscript{\textpm\ 0.095}}&
\makecell[c]{\textbf{0.770\textsubscript{\textpm\ 0.086}}} & \makecell[c]{0.799\textsubscript{\textpm\ 0.106}}\\
 \mystrut 3D UNet-MTL-TSD&   \makecell[c]{\textbf{0.814\textsubscript{\textpm\ 0.005}}} & \makecell[c]{\textbf{0.703\textsubscript{\textpm\ 0.059}}} &      
\makecell[c]{0.798\textsubscript{\textpm\ 0.053}}& \makecell[c]{\textbf{0.859\textsubscript{\textpm\ 0.079}}}&
\makecell[c]{\textbf{0.776\textsubscript{\textpm\ 0.002}}} & \makecell[c]{\textbf{0.918\textsubscript{\textpm\ 0.047}}}&
\makecell[c]{0.764\textsubscript{\textpm\ 0.081}} & \makecell[c]{\textbf{0.802\textsubscript{\textpm\ 0.107}}}\\\hline
\mystrut 3D UNet &    \makecell[c]{\textbf{0.772\textsubscript{\textpm\ 0.009}}}&\makecell[c]{1.369\textsubscript{\textpm\ 0.114}}&                                    
\makecell[c]{\textbf{0.760\textsubscript{\textpm\ 0.089}}}&\makecell[c]{0.825\textsubscript{\textpm\ 0.106}}&
\makecell[c]{0.715\textsubscript{\textpm\ 0.011}} & \makecell[c]{1.419\textsubscript{\textpm\ 0.088}}&
\makecell[c]{0.707\textsubscript{\textpm\ 0.078}} & \makecell[c]{0.781\textsubscript{\textpm\ 0.136}}\\
\mystrut 3D UNet-MTL-TSOL &   \makecell[c]{\textbf{0.772\textsubscript{\textpm\ 0.014}}}	&\makecell[c]{\textbf{1.170\textsubscript{\textpm\ 0.161}}}&                                                       \makecell[c]{0.754\textsubscript{\textpm\ 0.073}}&\makecell[c]{0.829\textsubscript{\textpm\ 0.110}}&
\makecell[c]{\textbf{0.741\textsubscript{\textpm\ 0.011}}} & \makecell[c]{\textbf{1.189\textsubscript{\textpm\ 0.099}}}&
\makecell[c]{\textbf{0.733\textsubscript{\textpm\ 0.080}}} & \makecell[c]{\textbf{0.789\textsubscript{\textpm\ 0.121}}}\\
\mystrut 3D UNet-MTL-TSD &  \makecell[c]{0.771\textsubscript{\textpm\ 0.008}}   &  \makecell[c]{1.308\textsubscript{\textpm\ 0.269}}&                                                  
\makecell[c]{0.751\textsubscript{\textpm\ 0.077}}   &  \makecell[c]{\textbf{0.832\textsubscript{\textpm\ 0.111}}}&
\makecell[c]{0.723\textsubscript{\textpm\ 0.008}} & \makecell[c]{1.349\textsubscript{\textpm\ 0.027}}&
\makecell[c]{0.708\textsubscript{\textpm\ 0.084}} & \makecell[c]{0.783\textsubscript{\textpm\ 0.127}}\\\hline
\mystrut 3D Att-UNet  &  \makecell[c]{0.792\textsubscript{\textpm\ 0.015}} & \makecell[c]{0.825\textsubscript{\textpm\ 0.082}} &                 
\makecell[c]{0.788\textsubscript{\textpm\ 0.054}} & \makecell[c]{0.831\textsubscript{\textpm\ 0.093}}&
\makecell[c]{0.752\textsubscript{\textpm\ 0.008}} & \makecell[c]{1.313\textsubscript{\textpm\ 0.238}}&
\makecell[c]{0.754\textsubscript{\textpm\ 0.091}} & \makecell[c]{0.778\textsubscript{\textpm\ 0.112}}\\
\mystrut 3D Att-UNet-MTL-TSOL&  \makecell[c]{\textbf{0.820\textsubscript{\textpm\ 0.004}}}    & \textbf{\makecell[c]{0.673\textsubscript{\textpm\ 0.035}}}&                                                          \makecell[c]{\textbf{0.822\textsubscript{\textpm\ 0.050}}}    & \makecell[c]{0.839\textsubscript{\textpm\ 0.076}}&
\makecell[c]{0.769\textsubscript{\textpm\ 0.008}} & \makecell[c]{1.065\textsubscript{\textpm\ 0.175}}&
\makecell[c]{\textbf{0.769\textsubscript{\textpm\ 0.086}}} & \makecell[c]{0.790\textsubscript{\textpm\ 0.095}}\\
\mystrut 3D Att-UNet-MTL-TSD&  \makecell[c]{0.807\textsubscript{\textpm\ 0.005}}  &\makecell[c]{0.732\textsubscript{\textpm\ 0.049}}&                                                
 \makecell[c]{0.797\textsubscript{\textpm\ 0.050}} &\textbf{\makecell[c]{0.847\textsubscript{\textpm\ 0.080}}}&
\makecell[c]{\textbf{0.778\textsubscript{\textpm\ 0.004}}} & \makecell[c]{\textbf{0.919\textsubscript{\textpm\ 0.052}}}&
\makecell[c]{0.759\textsubscript{\textpm\ 0.077}} & \makecell[c]{\textbf{0.810\textsubscript{\textpm\ 0.104}}}\\\hline
\end{tabular}
}
\end{table*}
\begin{table}[ht] \centering
\resizebox{\columnwidth}{!}{\caption{Comparison of parameter-cost and computational time.}\label{table:Table 3.2}
\begin{tabular}{lll}
\hline
Method & \makecell[c]{No. of parameters}& \makecell[c]{Inference time (ms)}\\\hline
3D UNet  &\makecell[c]{5.89M (-)}&\makecell[c]{44}\\
3D UNet-MTL-TSOL &   \makecell[c]{5.89M (17)}	&\makecell[c]{53}\\
3D UNet-MTL-TSD& \makecell[c]{8.24M (2.35M)}	&\makecell[c]{68}\\\hline
3D UNet &    \makecell[c]{6.87M}	&\makecell[c]{63} \\
3D UNet - MTL-TSOL & \makecell[c]{6.87M (17)}	&\makecell[c]{79}\\
3D UNet - MTL-TSD& \makecell[c]{9.22M (2.35M)}	&\makecell[c]{81}\\\hline
3D Att-UNet  &  \makecell[c]{6.47M}	&\makecell[c]{77}\\
3D Att-UNet-MTL-TSOL&  \makecell[c]{6.47M (17)}	&\makecell[c]{78}\\
3D Att-MTL-TSD& \makecell[c]{8.82M (2.35M)}	&\makecell[c]{89}\\\hline
\end{tabular}
}
\end{table}
\begin{figure*}[!hbt]
\captionsetup[subfigure]{justification=centering}
\centering
\rotatebox[origin=c]{90}{\makebox[2in]{\textbf{Pancreas-CT dataset}}}\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[scale=0.3]{Figures/unet_mtl1_panc_new_eps_converted_to.pdf} 
\hfill
\includegraphics[scale=0.3]{Figures/unet_mtl2_panc_new1_eps_converted_to.pdf}
\caption{}
\label{fig:sub20}
\end{subfigure}\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[scale=0.3]{Figures/unetplus_mtl1_panc_new_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.3]{Figures/unetplus_mtl2_panc_new_eps_converted_to.pdf}
\caption{}
\label{fig:sub21}
\end{subfigure}\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[scale=0.3]{Figures/attenunet_mtl1_panc_new_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.3]{Figures/attenunet_mtl2_panc_new_eps_converted_to.pdf} 
\caption{}
\label{fig:sub22}
\end{subfigure}
\rotatebox[origin=c]{90}{\makebox[2in]{\textbf{BTCV dataset}}}\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[scale=0.3]{Figures/unet_mtl1_bvc_new_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.3]{Figures/unet_mtl2_bvc_new_eps_converted_to.pdf}
\caption{}
\label{fig:sub23}
\end{subfigure}\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[scale=0.3]{Figures/unetplus_mtl1_bvc_new_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.3]{Figures/unetplus_mtl2_bvc_new_eps_converted_to.pdf}
\caption{}
\label{fig:sub24}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[scale=0.3]{Figures/attenunet_mtl1_bvc_new_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.3]{Figures/attenunet_mtl2_bvc_new_eps_converted_to.pdf}
\caption{}
\label{fig:sub25}
\end{subfigure}
\caption{Examining the mean dice score computed by comparing predicted multi-organ segmentation masks with the ground-truth on validation set: First two rows (a-c) and last two rows (d-f) correspond to the dice score curves for Pancreas-ct dataset and BTVC dataset, respectively. Each figure compares the mean DC of the baseline (\textcolor{blue}{Blue}) and counterpart boundary-constrained model (\textcolor{red}{Red}) on the validation set, computed after each epoch. The baseline models are trained to predict the masks of the abdominal regions, whereas the boundary-constrained models are trained to predict the labels of organs and boundaries.}
\label{fig:valcurve}
\end{figure*}
\begin{comment}
\subsubsection{Average Hausdorff Distance}
Average Hausdorff distance (Avg. HD) is computed by taking the mean of directed average hausdorff distances from voxels in the predicted set of points  to the voxels in the ground-truth . It is shown in \autoref{eq:eq6}. The directed average hausdorff distance from  to  is given by the sum of all minimum distances from all voxels in  to  divided by the number of voxels in . 

In equation \autoref{eq:eq6},  and  denotes the number of voxels on predicted surface  and , respectively. Average hausdorff distance is known to be more stable and robust to presence of outliers \cite{576361} \cite{Aydin2021OnTU}, since it takes average of all the directed distances between two voxel-surfaces. It is measured in millimeters and a smaller value indicates higher segmentation accuracy.
\subsubsection{Recall}
Through Recall, we measure the portion of voxels correctly identified as positive positive voxels in the ground truth that are also identified as positive by the segmentation being evaluated. Analogously, T
\end{comment}
\section{Results and Analysis}\label{sec:result}
This section demonstrates the experimental results obtained from boundary-constrained abdominal segmentation models and compares them against the performance given by baseline models. For the sake of brevity, we denote the baseline and boundary-constrained models with the abbreviations below (shown in \textbf{bold}).
\begin{enumerate}[label=(\alph*)]
\item \textbf{3D UNet}: 3D UNet shown in Fig. S1 in Supplementary material.
\item \textbf{3D UNet-MTL-TSOL:} Boundary constrained 3D UNet with task specific output layers shown in \autoref{fig:unetmtl1}.
\item \textbf{3D UNet-MTL-TSD:} Boundary constrained 3D UNet with task specific decoders shown in \autoref{fig:unetmtl2}.
\item \textbf{3D UNet}: 3D UNet shown in Fig. S2 in Supplementary material.
\item \textbf{3D UNet-MTL-TSOL:} Boundary constrained 3D UNet with task specific output layers shown in \autoref{fig:unetplusmtl1}.
\item \textbf{3D UNet-MTL-TSD:} Boundary constrained 3D UNet with task specific decoders shown in \autoref{fig:unetplusmtl2}.
\item \textbf{3D Att-UNet}: 3D Att-UNet shown in Fig. S3 in Supplementary material.
\item \textbf{3D Att-UNet-MTL-TSOL:} Boundary constrained 3D Att-UNet with task specific output layers shown in \autoref{fig:attenunetmtl1}.
\item \textbf{3D Att-UNet-MTL-TSD:} Boundary constrained 3D Att-UNet with task specific decoders shown in \autoref{fig:attenunetmtl2}.
\end{enumerate}
\subsection{Quantitative Results}
\autoref{table:Table 3.1} summarizes the segmentation results for the Pancreas-CT and BTCV datasets. We report the mean Dice Score, mean Average Hausdorff Distance (Avg. HD), mean Recall, and mean Precision computed by comparing the predicted segmentation against the ground-truth. These measures are calculated on test sets of each dataset. \autoref{table:Table 3.1} shows that the boundary-constrained models achieve improved multi-organ segmentation on the abdominal CT scans. Firstly, the mean Dice scores are improved by 1.8\% (UNet vs. UNet-MTL-TSOL) and 3.5\% (Att-UNet vs. Att-UNet-MTL-TSOL), for Pancreas-CT dataset. The corresponding values of mean dice scores for BTCV dataset are improved by 3.1\% (UNet vs. UNet-MTL-TSD), 3.6\% (UNet vs. UNet-MTL-TSOL), and 3.5\% (Att-UNet vs. Att-UNet-MTL-TSD). The improved overlap between predicted segmentations and manually annotated masks can be attributed to the enhanced semantic representations learned by boundary-constrained models. 

Secondly, we observe that boundary-constrained models achieve a lower Avg. HDs for all the datasets than those obtained from baseline models as shown in \autoref{table:Table 3.1}. For example, after adding boundary information, the Avg. HD values of UNet, UNet, and Att-UNet are decreased by 11.5\%, 14.5\%, and 18.4\%, respectively, for the Pancreas-CT dataset. Likewise, a decrease of 15.4\%, 16.2\%, and 30\%, respectively, is seen for the BTCV dataset. Furthermore, we notice that the Avg. HD is still lower for the cases where boundary-constrained models obtained lower or equivalent mean Dice score, e.g., UNet-MTL-TSOL vs. UNet and UNet-MTL-TSD vs. UNet, for Pancreas-CT results. This indicates that even for the equivalent dice overlap, the performance of the boundary-constrained models in accurately predicting the boundaries is improved.

Thirdly, our boundary-constrained models achieve higher values of mean Recall and mean Precision for all the models and datasets except for UNet corresponding to the Pancreas-CT dataset, as shown in  \autoref{table:Table 3.1}. The utilization of boundary information has caused a decrease in false-negative rates and false-positive rates. Specifically, the mean recall is increased by 1.3\% (UNet vs. UNet-MTL-TSOL) and 4.3\% (Att-UNet vs. Att-UNet-MTL-TSOL) for Pancreas-CT dataset. For BTCV dataset, an increase of 1.3\% (UNet vs. UNet-MTL-TSOL), 3.7\% (UNet-MTL-TSOL vs. UNet), and 1.9\% (Att-UNet vs. Att-UNet-MTL-TSOL) is observed in Mean Recalls. Finally, we see a maximum improvement of 1.9\% and 4.3\% in values of mean Precision for Pancreas-CT and BTCV datasets, respectively. The improvment in mean Recalls and mean Precisions show the capability of boundary-constrained models in addressing the issues of under-segmentation and over-segmentation. 
\begin{table*}[ht] \centering
\resizebox{\columnwidth}{!}{\caption{Organ-wise Mean Dice scores \textpm\ Std. Dev. obtained from baseline and best performing boundary-constrained models: \textbf{Bold} shows the highest value corresponding to each baseline.}\label{table:Table 3.3}
\vspace{-1mm}
\begin{tabular}{lllllllll}
\hline
Method&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}& \makecell[c]{\raisebox{0.2em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}} &\makecell[c]{\raisebox{0.2em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{red}{red}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}\\
\hline
&\multicolumn{8}{c}{\textbf{Pancreas-CT dataset}}\\
\cline{2-9}
\mystrut 3D UNet &     \makecell[c]{\textbf{0.935\textsubscript{\textpm\ 0.008}}} 	&\makecell[c]{0.700\textsubscript{\textpm\ 0.035}} &	\makecell[c]{0.927\textsubscript{\textpm\ 0.014}	}&\makecell[c]{0.793\textsubscript{\textpm\ 0.030}}	&\makecell[c]{0.714\textsubscript{\textpm\ 0.024}}&	\makecell[c]{0.934\textsubscript{\textpm\ 0.009}}&	\makecell[c]{0.829\textsubscript{\textpm\ 0.024}}&	\makecell[c]{0.566\textsubscript{\textpm\ 0.031}}\\
3D UNet-MTL-TSD&      \makecell[c]{0.931\textsubscript{\textpm\ 0.008}} 
&\makecell[c]{\textbf{0.723\textsubscript{\textpm\ 0.028}}}&	
\makecell[c]{\textbf{0.937\textsubscript{\textpm\ 0.004}}}
&\makecell[c]{\textbf{0.820\textsubscript{\textpm\ 0.028}}}
&	\makecell[c]{\textbf{0.718\textsubscript{\textpm\ 0.014}}}
&	\makecell[c]{\textbf{0.947\textsubscript{\textpm\ 0.003}}}&	
\makecell[c]{\textbf{0.854\textsubscript{\textpm\ 0.015}}}&	
\makecell[c]{\textbf{0.583\textsubscript{\textpm\ 0.021}}}\\\hline
\mystrut 3D UNet & 
\makecell[c]{\textbf{0.917\textsubscript{\textpm\ 0.006}}}
&	\makecell[c]{0.613\textsubscript{\textpm\ 0.053}} 
&	\makecell[c]{0.897\textsubscript{\textpm\ 0.019}}
&	\makecell[c]{\textbf{0.802\textsubscript{\textpm\ 0.026}}}
&	\makecell[c]{\textbf{0.712\textsubscript{\textpm\ 0.013}}} 
&	\makecell[c]{0.899\textsubscript{\textpm\ 0.012}}	
&\makecell[c]{\textbf{0.818\textsubscript{\textpm\ 0.014}}}
&	\makecell[c]{0.516\textsubscript{\textpm\ 0.042}} \\
3D UNet - MTL-TSOL  & \makecell[c]{0.915\textsubscript{\textpm\ 0.005}}
&\makecell[c]{\textbf{0.621\textsubscript{\textpm\ 0.042}}}&
\makecell[c]{\textbf{0.915\textsubscript{\textpm\ 0.013}}}
&	\makecell[c]{0.774\textsubscript{\textpm\ 0.051}}&	
\makecell[c]{0.697\textsubscript{\textpm\ 0.013}}&
\makecell[c]{\textbf{0.913\textsubscript{\textpm\ 0.005}}}
&	\makecell[c]{\textbf{0.818\textsubscript{\textpm\ 0.032}}}
&	\makecell[c]{\textbf{0.519\textsubscript{\textpm\ 0.035}}}\\\hline
3D Att-UNet&\makecell[c]{0.927\textsubscript{\textpm\ 0.007}}
&	\makecell[c]{0.688\textsubscript{\textpm\ 0.021}}
&\makecell[c]{\textbf{0.927\textsubscript{\textpm\ 0.013}}}
&	\makecell[c]{0.772\textsubscript{\textpm\ 0.037}}
& \makecell[c]{0.700\textsubscript{\textpm\ 0.021}}
&	 \makecell[c]{0.929\textsubscript{\textpm\ 0.016}}
&\makecell[c]{0.827\textsubscript{\textpm\ 0.029}}
&\makecell[c]{0.565\textsubscript{\textpm\ 0.036}}\\
3D Att-UNet-MTL-TSOL&
\makecell[c]{\textbf{0.937\textsubscript{\textpm\ 0.008}}}
&	\makecell[c]{\textbf{0.724\textsubscript{\textpm\ 0.010}}}
&\makecell[c]{\textbf{0.927\textsubscript{\textpm\ 0.009}}}
&	\makecell[c]{\textbf{0.858\textsubscript{\textpm\ 0.017}}}
& \makecell[c]{\textbf{0.734\textsubscript{\textpm\ 0.005}}}
&	 \makecell[c]{\textbf{0.949\textsubscript{\textpm\ 0.004}}}
&\makecell[c]{\textbf{0.847\textsubscript{\textpm\ 0.018}}}
&\makecell[c]{\textbf{0.589\textsubscript{\textpm\ 0.023}}}\\\hline
&\multicolumn{8}{c}{\textbf{BTCV dataset}}\\
\cline{2-9}
3D UNet &     
\makecell[c]{0.878\textsubscript{\textpm\ 0.016}} 	
&\makecell[c]{0.659\textsubscript{\textpm\ 0.018}} 
&	\makecell[c]{0.897\textsubscript{\textpm\ 0.014}}	&
\makecell[c]{0.567\textsubscript{\textpm\ 0.020}}	&
\makecell[c]{0.698\textsubscript{\textpm\ 0.009}}&	
\makecell[c]{0.932\textsubscript{\textpm\ 0.008}}&
	\makecell[c]{0.813\textsubscript{\textpm\ 0.012}}&
		\makecell[c]{0.576\textsubscript{\textpm\ 0.023}}\\
3D UNet-MTL-TSD&      \makecell[c]{\textbf{0.897\textsubscript{\textpm\ 0.011}}}
&	\makecell[c]{\textbf{0.682\textsubscript{\textpm\ 0.009}}}
&	\makecell[c]{\textbf{0.912\textsubscript{\textpm\ 0.004}}}
&\makecell[c]{\textbf{0.608\textsubscript{\textpm\ 0.019}}}
&	\makecell[c]{\textbf{0.732\textsubscript{\textpm\ 0.016}}}
&	\makecell[c]{\textbf{0.937\textsubscript{\textpm\ 0.007}}}
&	\makecell[c]{\textbf{0.837\textsubscript{\textpm\ 0.008}}}
&	\makecell[c]{\textbf{0.599\textsubscript{\textpm\ 0.011}}}\\\hline
3D UNet & 
\makecell[c]{0.846\textsubscript{\textpm\ 0.016}}&	
\makecell[c]{0.559\textsubscript{\textpm\ 0.032}} &
	\makecell[c]{0.851\textsubscript{\textpm\ 0.030}}&
		\makecell[c]{0.552\textsubscript{\textpm\ 0.023}}&	
		\makecell[c]{0.691\textsubscript{\textpm\ 0.014}} &
			\makecell[c]{0.882\textsubscript{\textpm\ 0.008}}	&
			\makecell[c]{0.799\textsubscript{\textpm\ 0.015}}&	
			\makecell[c]{0.538\textsubscript{\textpm\ 0.028}} \\
3D UNet - MTL-TSOL  &         
\makecell[c]{\textbf{0.853\textsubscript{\textpm\ 0.023}}}
&\makecell[c]{\textbf{0.625\textsubscript{\textpm\ 0.018}}}
&	\makecell[c]{\textbf{0.891\textsubscript{\textpm\ 0.019}}}
&\makecell[c]{\textbf{0.575\textsubscript{\textpm\ 0.019}}}
&\makecell[c]{\textbf{0.694\textsubscript{\textpm\ 0.014}}}
&	\makecell[c]{\textbf{0.918\textsubscript{\textpm\ 0.004}}}
&	\makecell[c]{\textbf{0.829\textsubscript{\textpm\ 0.018}}}
&	\makecell[c]{\textbf{0.549\textsubscript{\textpm\ 0.032}}}\\\hline
3D Att-UNet & 
\makecell[c]{0.881\textsubscript{\textpm\ 0.020}} &
	\makecell[c]{0.668\textsubscript{\textpm\ 0.016}} &	
	\makecell[c]{0.882\textsubscript{\textpm\ 0.036}}&
	\makecell[c]{0.561\textsubscript{\textpm\ 0.009}}	&
	\makecell[c]{0.704\textsubscript{\textpm\ 0.023}}&	
	\makecell[c]{0.939\textsubscript{\textpm\ 0.006}}&	
	\makecell[c]{0.805\textsubscript{\textpm\ 0.007}}&	
	\makecell[c]{0.575\textsubscript{\textpm\ 0.007}}\\
3D Att-UNet-MTL-TSD&                             \makecell[c]{\textbf{0.913\textsubscript{\textpm\ 0.009}}}
&	\makecell[c]{\textbf{0.674\textsubscript{\textpm\ 0.006}}}
&\makecell[c]{\textbf{0.920\textsubscript{\textpm\ 0.015}}}
&	\makecell[c]{\textbf{0.603\textsubscript{\textpm\ 0.034}}}
&	\makecell[c]{\textbf{0.720\textsubscript{\textpm\ 0.009}}}
&\makecell[c]{\textbf{0.947\textsubscript{\textpm\ 0.007}}}
&\makecell[c]{\textbf{0.832\textsubscript{\textpm\ 0.022}}}
&	\makecell[c]{\textbf{0.608\textsubscript{\textpm\ 0.025}}}\\\hline
\end{tabular}
}
\end{table*}

\autoref{fig:valcurve} shows the performance curves based on the mean Dice scores, computed by comparing the predicted multi-organ segmentation with the ground-truth masks on the validation set. \Cref{fig:sub20,fig:sub21,fig:sub22} correspond to performance curves for the Pancreas-CT and \Cref{fig:sub23,fig:sub24,fig:sub25} for the BTCV dataset. It can be seen that the incorporation of boundary information has improved the mean Dice score as the training progresses.
\begin{table*}[ht] \centering
\resizebox{\columnwidth}{!}{\caption{Organ-wise Mean Avg. HDs \textpm\ Std. Dev. obtained from baseline and best performing boundary-constrained models: \textbf{Bold} shows the highest value corresponding to each baseline.}\label{table:Table 3.4}
\begin{tabular}{lllllllll}
\hline
Method&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}& \makecell[c]{\raisebox{0.2em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}} &\makecell[c]{\raisebox{0.2em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{red}{red}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}\\\hline
&\multicolumn{8}{c}{\textbf{Pancreas-CT dataset}}\\
\cline{2-9}
\mystrut 3D UNet &     
\makecell[c]{\textbf{0.261\textsubscript{\textpm\ 0.085}}} 	&
\makecell[c]{0.936\textsubscript{\textpm\ 0.279}} &	
\makecell[c]{0.184\textsubscript{\textpm\ 0.143}}	&
\makecell[c]{0.803\textsubscript{\textpm\ 0.356}}	&
\makecell[c]{0.788\textsubscript{\textpm\ 0.275}}&	
\makecell[c]{\textbf{0.234\textsubscript{\textpm\ 0.063}}}&
	\makecell[c]{\textbf{0.834\textsubscript{\textpm\ 0.234}}}&	
	\makecell[c]{2.321\textsubscript{\textpm\ 0.316}}\\
3D UNet-MTL-TSD&      \makecell[c]{0.317\textsubscript{\textpm\ 0.083}} 
&	\makecell[c]{\textbf{0.911\textsubscript{\textpm\ 0.244}}}
&	\makecell[c]{\textbf{0.150\textsubscript{\textpm\ 0.036}}}
&	\makecell[c]{\textbf{0.389\textsubscript{\textpm\ 0.074}}}
&	\makecell[c]{\textbf{0.683\textsubscript{\textpm\ 0.072}}}
&	\makecell[c]{0.259\textsubscript{\textpm\ 0.122}}
&	\makecell[c]{0.879\textsubscript{\textpm\ 0.304}}
&\makecell[c]{\textbf{2.038\textsubscript{\textpm\ 0.204}}}\\\hline
\mystrut 3D UNet & 
\makecell[c]{0.419\textsubscript{\textpm\ 0.069}}&	
\makecell[c]{1.852\textsubscript{\textpm\ 0.554}} &	
\makecell[c]{0.265\textsubscript{\textpm\ 0.153}}&	
\makecell[c]{1.569\textsubscript{\textpm\ 0.649}}&	
\makecell[c]{1.445\textsubscript{\textpm\ 0.494}} &	
\makecell[c]{0.711\textsubscript{\textpm\ 0.218}}	&
\makecell[c]{1.371\textsubscript{\textpm\ 0.457}}&	
\makecell[c]{3.315\textsubscript{\textpm\ 0.637}} \\
3D UNet - MTL-TSOL  &     
 \makecell[c]{\textbf{0.389\textsubscript{\textpm\ 0.055}}	}&
\makecell[c]{\textbf{1.718\textsubscript{\textpm\ 0.638}}}&	
\makecell[c]{\textbf{0.167\textsubscript{\textpm\ 0.048}}}&	
\makecell[c]{\textbf{0.873\textsubscript{\textpm\ 0.229}}}&	
\makecell[c]{\textbf{1.187\textsubscript{\textpm\ 0.328}}}&	
\makecell[c]{\textbf{0.675\textsubscript{\textpm\ 0.104}}}&	
\makecell[c]{\textbf{1.296\textsubscript{\textpm\ 0.483}}}&	
\makecell[c]{\textbf{3.058\textsubscript{\textpm\ 0.948}}}\\\hline
3D Att-UNet & 
\makecell[c]{0.363\textsubscript{\textpm\ 0.063}} &	
\makecell[c]{\textbf{0.928\textsubscript{\textpm\ 0.085}}} &	
\makecell[c]{\textbf{0.174\textsubscript{\textpm\ 0.106}}}&
\makecell[c]{0.527\textsubscript{\textpm\ 0.099}}	&
\makecell[c]{0.809\textsubscript{\textpm\ 0.277}}&	
\makecell[c]{0.442\textsubscript{\textpm\ 0.175}}&	
\makecell[c]{0.969\textsubscript{\textpm\ 0.336}}&	
\makecell[c]{2.384\textsubscript{\textpm\ 0.466}}\\
3D Att-UNet-MTL-TSOL&                         \makecell[c]{\textbf{0.255\textsubscript{\textpm\ 0.041}}}
&	\makecell[c]{0.993\textsubscript{\textpm\ 0.315}}	
&\makecell[c]{0.192\textsubscript{\textpm\ 0.033}}
&	\makecell[c]{\textbf{0.305\textsubscript{\textpm\ 0.075}}}
&	\makecell[c]{\textbf{0.559\textsubscript{\textpm\ 0.022}}}
&	 \makecell[c]{\textbf{0.154\textsubscript{\textpm\ 0.054}}}
&\makecell[c]{\textbf{0.766\textsubscript{\textpm\ 0.274}}}
&	\makecell[c]{\textbf{2.162\textsubscript{\textpm\ 0.434}}}\\\hline
&\multicolumn{8}{c}{\textbf{BTCV dataset}}\\
\cline{2-9}
3D UNet &     
\makecell[c]{0.474\textsubscript{\textpm\ 0.161}} 	&
\makecell[c]{\textbf{1.741\textsubscript{\textpm\ 0.179}}} &	
\makecell[c]{0.337\textsubscript{\textpm\ 0.077}}	&
\makecell[c]{1.750\textsubscript{\textpm\ 0.334}}	&
\makecell[c]{0.770\textsubscript{\textpm\ 0.140}}&	
\makecell[c]{0.636\textsubscript{\textpm\ 0.206}}&	
\makecell[c]{0.941\textsubscript{\textpm\ 0.155}}&	
\makecell[c]{2.032\textsubscript{\textpm\ 0.149}}\\
3D UNet-MTL-TSD&      
\makecell[c]{\textbf{0.329\textsubscript{\textpm\ 0.105}}}
&	\makecell[c]{1.787\textsubscript{\textpm\ 0.326}} 
&\makecell[c]{\textbf{0.253\textsubscript{\textpm\ 0.059}}}
&	\makecell[c]{\textbf{1.076\textsubscript{\textpm\ 0.136}}}
& \makecell[c]{\textbf{0.648\textsubscript{\textpm\ 0.168}}}
&	 \makecell[c]{\textbf{0.619\textsubscript{\textpm\ 0.226}}}
&	\makecell[c]{\textbf{0.778\textsubscript{\textpm\ 0.123}}}
&	 \makecell[c]{\textbf{1.849\textsubscript{\textpm\ 0.199}}}\\\hline
3D UNet
& \makecell[c]{\textbf{0.780\textsubscript{\textpm\ 0.220}}}&	
\makecell[c]{2.423\textsubscript{\textpm\ 0.510}} &	
\makecell[c]{0.574\textsubscript{\textpm\ 0.282}}&	
\makecell[c]{2.103\textsubscript{\textpm\ 0.284}}&
	\makecell[c]{1.034\textsubscript{\textpm\ 0.187}} &	
	\makecell[c]{\textbf{0.984\textsubscript{\textpm\ 0.136}}}	&
	\makecell[c]{1.153\textsubscript{\textpm\ 0.148}}&
		\makecell[c]{2.306\textsubscript{\textpm\ 0.206}} \\
3D UNet - MTL-TSOL  &         
\makecell[c]{0.794\textsubscript{\textpm\ 0.089}}	
&\makecell[c]{\textbf{1.842\textsubscript{\textpm\ 0.518}}}
&\makecell[c]{\textbf{0.328\textsubscript{\textpm\ 0.087}}}
& \makecell[c]{\textbf{1.630\textsubscript{\textpm\ 0.479}}}
&	 \makecell[c]{\textbf{0.908\textsubscript{\textpm\ 0.221}}}
&	\makecell[c]{1.080\textsubscript{\textpm\ 0.063}}
&	 \makecell[c]{\textbf{0.860\textsubscript{\textpm\ 0.248}}}
&	\makecell[c]{\textbf{2.073\textsubscript{\textpm\ 0.265}}}\\\hline
3D Att-UNet& 
\makecell[c]{0.577\textsubscript{\textpm\ 0.245}} &	
\makecell[c]{2.017\textsubscript{\textpm\ 0.418}} &	
\makecell[c]{0.404\textsubscript{\textpm\ 0.154}}&
\makecell[c]{2.889\textsubscript{\textpm\ 1.387}}	&
\makecell[c]{0.981\textsubscript{\textpm\ 0.659}}&	
\makecell[c]{0.435\textsubscript{\textpm\ 0.162}}&	
\makecell[c]{1.011\textsubscript{\textpm\ 0.133}}&
	\makecell[c]{2.193\textsubscript{\textpm\ 0.249}}\\
3D Att-UNet-MTL-TSD&                             	
\makecell[c]{\textbf{0.273\textsubscript{\textpm\ 0.055}}}&	
 \makecell[c]{\textbf{1.377\textsubscript{\textpm\ 0.123}}}
& \makecell[c]{\textbf{0.191\textsubscript{\textpm\ 0.062}}}
& \makecell[c]{\textbf{1.631\textsubscript{\textpm\ 0.456}}}
&	 \makecell[c]{\textbf{0.887\textsubscript{\textpm\ 0.248}}	}
&  \makecell[c]{\textbf{0.318\textsubscript{\textpm\ 0.202}}}
	&  \makecell[c]{\textbf{0.789\textsubscript{\textpm\ 0.149}}}
	&	 \makecell[c]{\textbf{1.888\textsubscript{\textpm\ 0.318}}}\\\hline
\end{tabular}
}
\end{table*}
\subsection{Computational Complexity and Architectural Analysis} 
\Cref{table:Table 3.2} reports parameter count (in million M) and each method's time to segment a single CT abdominal volume in the test phase. We also highlight the increase in parameter count (given in brackets) compared with the baseline model. Among the single-task baseline models, UNet is most parameter-extensive with 6.87  parameters. The parameter count of boundary-constrained models with TSOL topology have only 17 parameters more than the baseline models; however, these models showed significantly considerable improvements in the segmentation of abdominal organs. The shared encoder and decoder in the TSOL design enable the small parameter count while capacitating the segmentation algorithm to learn the masks and boundaries using separate task-specific output layers. Observing the second multi-task topology TSD, our model requires approximately 2.4 M more parameters than the baseline. 

Furthermore, we can see (from \Cref{table:Table 3.1}) that the boundary-constrained models take more time to segment a single volume at the inference stage. This behavior can be associated with the extended parameter size of boundary-constrained models since they are trained to predict a 3D boundary of organs in addition to region masks.

Analyzing the relationship between the multi-task network design and the segmentation performance from \Cref{table:Table 3.1}, we note there is not a single/fixed topology that led to the maximum improvement. For example, the TSD showed maximum improvement in mean DC and Avg HD corresponding to the Pancreas-CT dataset. Hd over the baseline UNet. However, when the performance of Att-UNet is compared with boundary-constrained models, we notice that the TSOL demonstrated the best results. This reveals that the multi-task network configuration that offers the best results varies depending on each baseline architecture. All in all, we found that integration of boundary information improved the multi-organ segmentation, independent of the network topology.
\subsection{Assesment of organ-wise segmentation performance}
To assess which specific organs benefitted greatly from incorporating boundary information for the segmentation task, we examine the mean Dice scores and mean Avg. HDs achieved by baseline, and best performing boundary constrained models (from \autoref{table:Table 3.1}) for each abdominal organ. We compute the Dice scores and Average Hausdorff distances for each organ individually and then average across all subjects. From \autoref{table:Table 3.3}, we can see both baseline and boundary-constrained models have yielded the highest mean Dice scores for liver ( \raisebox{0.3em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), spleen ( \raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and kidney ( \raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ) and lowest for duodenum ( \raisebox{0.3em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). However, boundary-based models have led to the maximum relative improvement for the gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), pancreas ( \raisebox{0.3em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and duodenum ( \raisebox{0.3em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). From \Cref{table:Table 3.4}, we observe that the boundary-constrained models have significantly improved the mean Avg. HD distances for the spleen ( \raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), kidney ( \raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). Finally, relating the increase in dice overlap to the organ occurrence (shown in Figures \ref{fig:organ_btvc} and \ref{fig:organ_panc}), we observe that the most significant improvement has occurred for the underweighted classes. In contrast, the boundary distances are maximally decreased for both small (e.g., gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ) and large structures like spleen ( \raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). Furthermore, the boundary-constrained models led to lower standard deviations of the mean Dice scores and Avg. HDs across different subjects which show the robustness of proposed models.
\begin{comment}
\autoref{table:Table 3.3} show that the highest increase in mean Dice score has occurred for pancreas ( \raisebox{0.3em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), duodenum ( \raisebox{0.3em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and esophagus ( \raisebox{0.3em}{\fcolorbox{red}{red}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). From \Cref{table:Table 3.4}, we observe that the boundary-constrained models significantly improved the mean Avg. HD distances for the spleen ( \raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), kidney ( \raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and liver ( \raisebox{0.3em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). Relating the increase in Dice scores to the organ occurrence (shown in Figures \ref{fig:organ_btvc} and \ref{fig:organ_panc}), we can observe that the most significant improvement in organ-wise dice score has occured for the rare organ classes. It shows that the incorporation of boundary information assisted in improving the segmentation of rare classes. 
\end{comment}
\begin{comment}
\begin{table*}[ht] \centering
\resizebox{\columnwidth}{!}{\caption{Organ-wise Mean Recalls obtained from baseline and best performing boundary-constrained models: \textbf{Bold} shows the highest value corresponding to each baseline.}\label{table:Table 3.4}
\begin{tabular}{lllllllll}
\hline
Method&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}& \makecell[c]{\raisebox{0.2em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}} &\makecell[c]{\raisebox{0.2em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{red}{red}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}\\\hline
&\multicolumn{8}{c}{\textbf{Pancreas-CT dataset}}\\
\cline{2-9}
\mystrut 3D UNet &     
\makecell[c]{0.957\textsubscript{\textpm\ 0.031}} 	&
\makecell[c]{\textbf{0.772\textsubscript{\textpm\ 0.111}}} &	
\makecell[c]{\textbf{0.929\textsubscript{\textpm\ 0.038}}}	&
\makecell[c]{0.885\textsubscript{\textpm\ 0.076}}	&
\makecell[c]{\textbf{0.770\textsubscript{\textpm\ 0.123}}}&	
\makecell[c]{0.944\textsubscript{\textpm\ 0.032}}&
	\makecell[c]{\textbf{0.903\textsubscript{\textpm\ 0.069}}}&	
	\makecell[c]{0.642\textsubscript{\textpm\ 0.141}}\\
	3D UNet-MTL-TSOL&      \makecell[c]{\textbf{0.959\textsubscript{\textpm\ 0.022}}} 
&	\makecell[c]{0.762\textsubscript{\textpm\ 0.112}}
&	\makecell[c]{0.928\textsubscript{\textpm\ 0.051}}
&	\makecell[c]{\textbf{0.902\textsubscript{\textpm\ 0.048}}}
&	\makecell[c]{0.747\textsubscript{\textpm\ 0.126}}
&	\makecell[c]{\textbf{0.945\textsubscript{\textpm\ 0.021}}}
&	\makecell[c]{0.887\textsubscript{\textpm\ 0.091}}
&\makecell[c]{\textbf{0.662\textsubscript{\textpm\ 0.137}}}\\\hline
\mystrut 3D UNet & 
\makecell[c]{0.931\textsubscript{\textpm\ 0.061}}&	
\makecell[c]{\textbf{0.743\textsubscript{\textpm\ 0.158}}} &	
\makecell[c]{\textbf{0.951\textsubscript{\textpm\ 0.029}}}&	
\makecell[c]{0.850\textsubscript{\textpm\ 0.120}}&	
\makecell[c]{\textbf{0.747\textsubscript{\textpm\ 0.136}}} &	
\makecell[c]{0.894\textsubscript{\textpm\ 0.097}}	&
\makecell[c]{0.846\textsubscript{\textpm\ 0.156}}&	
\makecell[c]{0.640\textsubscript{\textpm\ 0.198}} \\
3D UNet - MTL-TSOL  &     
 \makecell[c]{\textbf{0.943\textsubscript{\textpm\ 0.049}}}&
\makecell[c]{0.739\textsubscript{\textpm\ 0.173}}&	
\makecell[c]{0.946\textsubscript{\textpm\ 0.038}}&	
\makecell[c]{\textbf{0.862\textsubscript{\textpm\ 0.059}}}&	
\makecell[c]{0.728\textsubscript{\textpm\ 0.103}}&	
\makecell[c]{\textbf{0.917\textsubscript{\textpm\ 0.080}}}&	
\makecell[c]{\textbf{0.858\textsubscript{\textpm\ 0.131}}}&	
\makecell[c]{\textbf{0.642\textsubscript{\textpm\ 0.173}}}\\\hline
3D Att-UNet & 
\makecell[c]{\textbf{0.954\textsubscript{\textpm\ 0.030}}} &	
\makecell[c]{\textbf{0.746\textsubscript{\textpm\ 0.116}}} &	
\makecell[c]{0.924\textsubscript{\textpm\ 0.054}}&
\makecell[c]{\textbf{0.917\textsubscript{\textpm\ 0.040}}	}&
\makecell[c]{0.684\textsubscript{\textpm\ 0.118}}&	
\makecell[c]{0.918\textsubscript{\textpm\ 0.045}}&	
\makecell[c]{0.873\textsubscript{\textpm\ 0.072}}&	
\makecell[c]{\textbf{0.634\textsubscript{\textpm\ 0.136}}}\\
3D Att-UNet-MTL-TSOL&                         \makecell[c]{0.951\textsubscript{\textpm\ 0.024}}
&	\makecell[c]{0.726\textsubscript{\textpm\ 0.113}}	
&\makecell[c]{\textbf{0.927\textsubscript{\textpm\ 0.046}}}
&	\makecell[c]{0.892\textsubscript{\textpm\ 0.040}}
&	\makecell[c]{\textbf{0.744\textsubscript{\textpm\ 0.111}}}
&	 \makecell[c]{\textbf{0.941\textsubscript{\textpm\ 0.019}}}
&\makecell[c]{\textbf{0.897\textsubscript{\textpm\ 0.076}}}
&	\makecell[c]{0.633\textsubscript{\textpm\ 0.146}}\\\hline
&\multicolumn{8}{c}{\textbf{BTCV dataset}}\\
\cline{2-9}
3D UNet &     
\makecell[c]{0.902\textsubscript{\textpm\ 0.072}} 	&
\makecell[c]{0.667\textsubscript{\textpm\ 0.163}} &	
\makecell[c]{0.904\textsubscript{\textpm\ 0.044}}	&
\makecell[c]{0.577\textsubscript{\textpm\ 0.282}}	&
\makecell[c]{0.726\textsubscript{\textpm\ 0.073}}&	
\makecell[c]{0.914\textsubscript{\textpm\ 0.083}}&	
\makecell[c]{0.869\textsubscript{\textpm\ 0.126}}&	
\makecell[c]{0.601\textsubscript{\textpm\ 0.133}}\\   
3D UNet-MTL-TSOL&      
\makecell[c]{\textbf{0.932\textsubscript{\textpm\ 0.063}}}
&	\makecell[c]{\textbf{0.728\textsubscript{\textpm\ 0.148}}} 
&\makecell[c]{\textbf{0.929\textsubscript{\textpm\ 0.026}}}
&	\makecell[c]{\textbf{0.610\textsubscript{\textpm\ 0.296}}}
& \makecell[c]{\textbf{0.775\textsubscript{\textpm\ 0.060}}}
&	 \makecell[c]{\textbf{0.929\textsubscript{\textpm\ 0.078}}}
&	\makecell[c]{\textbf{0.881\textsubscript{\textpm\ 0.115}}}
&	 \makecell[c]{\textbf{0.613\textsubscript{\textpm\ 0.123}}}\\\hline
3D UNet
& \makecell[c]{0.917\textsubscript{\textpm\ 0.060}}&	
\makecell[c]{0.687\textsubscript{\textpm\ 0.191}} &	
\makecell[c]{\textbf{0.923\textsubscript{\textpm\ 0.048}}}&	
\makecell[c]{\textbf{0.622\textsubscript{\textpm\ 0.297}}}&
	\makecell[c]{0.751\textsubscript{\textpm\ 0.067}} &	
	\makecell[c]{0.889\textsubscript{\textpm\ 0.141}}	&
	\makecell[c]{0.828\textsubscript{\textpm\ 0.114}}&
		\makecell[c]{0.635\textsubscript{\textpm\ 0.117}} \\
		3D UNet - MTL-TSOL  &         
\makecell[c]{\textbf{0.929\textsubscript{\textpm\ 0.060}}	}
&\makecell[c]{\textbf{0.694\textsubscript{\textpm\ 0.170}}}
&\makecell[c]{0.920\textsubscript{\textpm\ 0.041}}
& \makecell[c]{0.610\textsubscript{\textpm\ 0.292}}
&	 \makecell[c]{\textbf{0.761\textsubscript{\textpm\ 0.076}}}
&	\makecell[c]{\textbf{0.904\textsubscript{\textpm\ 0.135}}}
&	 \makecell[c]{\textbf{0.856\textsubscript{\textpm\ 0.108}}}
&	\makecell[c]{\textbf{0.647\textsubscript{\textpm\ 0.106}}}\\\hline
3D Att-UNet& 
\makecell[c]{0.911\textsubscript{\textpm\ 0.081}} &	
\makecell[c]{0.693\textsubscript{\textpm\ 0.163}} &	
\makecell[c]{0.904\textsubscript{\textpm\ 0.045}}&
\makecell[c]{\textbf{0.613\textsubscript{\textpm\ 0.306}}}	&
\makecell[c]{0.734\textsubscript{\textpm\ 0.081}}&	
\makecell[c]{0.929\textsubscript{\textpm\ 0.060}}&	
\makecell[c]{0.849\textsubscript{\textpm\ 0.137}}&
	\makecell[c]{0.588\textsubscript{\textpm\ 0.128}}\\
	3D Att-UNet-MTL-TSOL&                             	
\makecell[c]{\textbf{0.922\textsubscript{\textpm\ 0.067}}}&	
 \makecell[c]{\textbf{0.720\textsubscript{\textpm\ 0.160}}}
& \makecell[c]{\textbf{0.913\textsubscript{\textpm\ 0.046}}}
& \makecell[c]{0.606\textsubscript{\textpm\ 0.306}}
&	 \makecell[c]{\textbf{0.768\textsubscript{\textpm\ 0.051}}}
&  \makecell[c]{\textbf{0.939\textsubscript{\textpm\ 0.063}}}
	&  \makecell[c]{\textbf{0.867\textsubscript{\textpm\ 0.102}}}
	&	 \makecell[c]{\textbf{0.590\textsubscript{\textpm\ 0.125}}}
\\\hline
\end{tabular}
}
\end{table*}
\end{comment}
\begin{comment}
\begin{table*}[ht] \centering
\resizebox{\columnwidth}{!}{\caption{Organ-wise Mean Precisions obtained from baseline and best performing boundary-constrained models: \textbf{Bold} shows the highest value corresponding to each baseline.}\label{table:Table 3.4}
\begin{tabular}{lllllllll}
\hline
Method&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}& \makecell[c]{\raisebox{0.2em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}} &\makecell[c]{\raisebox{0.2em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{red}{red}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}&\makecell[c]{\raisebox{0.2em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}}}\\\hline
&\multicolumn{8}{c}{\textbf{Pancreas-CT dataset}}\\
\cline{2-9}
\mystrut 3D UNet &   
\makecell[c]{\textbf{0.921\textsubscript{\textpm\ 0.069}}} 	&
\makecell[c]{0.698\textsubscript{\textpm\ 0.141}} &	
\makecell[c]{0.922\textsubscript{\textpm\ 0.039}}	&
\makecell[c]{0.755\textsubscript{\textpm\ 0.146}}	&
\makecell[c]{0.711\textsubscript{\textpm\ 0.140}}&	
\makecell[c]{0.927\textsubscript{\textpm\ 0.049}}&
	\makecell[c]{0.789\textsubscript{\textpm\ 0.120}}&	
	\makecell[c]{\textbf{0.564\textsubscript{\textpm\ 0.217}}}\\
3D UNet-MTL-TSD&      \makecell[c]{0.918\textsubscript{\textpm\ 0.118}} 
&	\makecell[c]{\textbf{0.710\textsubscript{\textpm\ 0.161}}}
&	\makecell[c]{\textbf{0.935\textsubscript{\textpm\ 0.030}}}
&	\makecell[c]{\textbf{0.765\textsubscript{\textpm\ 0.133}}}
&	\makecell[c]{\textbf{0.721\textsubscript{\textpm\ 0.173}}}
&	\makecell[c]{\textbf{0.955\textsubscript{\textpm\ 0.015}}}
&	\makecell[c]{\textbf{0.825\textsubscript{\textpm\ 0.119}}}
&\makecell[c]{0.555\textsubscript{\textpm\ 0.201}}\\\hline
\mystrut 3D UNet & 
\makecell[c]{0.916\textsubscript{\textpm\ 0.117}}&	
\makecell[c]{0.588\textsubscript{\textpm\ 0.273}} &	
\makecell[c]{\textbf{0.872\textsubscript{\textpm\ 0.112}}}&	
\makecell[c]{\textbf{0.777\textsubscript{\textpm\ 0.116}}}&	
\makecell[c]{\textbf{0.710\textsubscript{\textpm\ 0.150}}} &	
\makecell[c]{0.924\textsubscript{\textpm\ 0.089}}	&
\makecell[c]{0.811\textsubscript{\textpm\ 0.123}}&	
\makecell[c]{\textbf{0.483\textsubscript{\textpm\ 0.218}}} \\
3D UNet - MTL-TSD  &     
 \makecell[c]{\textbf{0.921\textsubscript{\textpm\ 0.082}}	}&
\makecell[c]{\textbf{0.603\textsubscript{\textpm\ 0.278}}}&	
\makecell[c]{0.870\textsubscript{\textpm\ 0.090}}&	
\makecell[c]{0.715\textsubscript{\textpm\ 0.154}}&	
\makecell[c]{0.681\textsubscript{\textpm\ 0.163}}&	
\makecell[c]{\textbf{0.925\textsubscript{\textpm\ 0.081}}}&	
\makecell[c]{\textbf{0.818\textsubscript{\textpm\ 0.114}}}&	
\makecell[c]{0.473\textsubscript{\textpm\ 0.227}}\\\hline
3D Att-UNet & 
\makecell[c]{0.912\textsubscript{\textpm\ 0.124}} &	
\makecell[c]{0.670\textsubscript{\textpm\ 0.165}} &	
\makecell[c]{\textbf{0.937\textsubscript{\textpm\ 0.033}}}&
\makecell[c]{0.703\textsubscript{\textpm\ 0.170}}	&
\makecell[c]{\textbf{0.772\textsubscript{\textpm\ 0.143}}}&	
\makecell[c]{\textbf{0	.946\textsubscript{\textpm\ 0.038}}}&	
\makecell[c]{0.803\textsubscript{\textpm\ 0.138}}&	
\makecell[c]{\textbf{0.565\textsubscript{\textpm\ 0.215}}}\\
3D Att-UNet-MTL-TSD&                         \makecell[c]{\textbf{0.929\textsubscript{\textpm\ 0.092	}}}
&	\makecell[c]{\textbf{0.742\textsubscript{\textpm\ 0.149}}	}
&\makecell[c]{	0.934\textsubscript{\textpm\ 0.055}}
&	\makecell[c]{\textbf{0.736\textsubscript{\textpm\ 0.133}}}
&	\makecell[c]{0.732\textsubscript{\textpm\ 0.167}}
&	 \makecell[c]{	0.945\textsubscript{\textpm\ 0.020}}
&\makecell[c]{\textbf{0.833\textsubscript{\textpm\ 0.125}}}
&	\makecell[c]{0.525\textsubscript{\textpm\ 0.212}}\\\hline
&\multicolumn{8}{c}{\textbf{BTCV dataset}}\\
\cline{2-9}
3D UNet &     
\makecell[c]{\textbf{0.882\textsubscript{\textpm\ 0.154}}} 	&
\makecell[c]{\textbf{0.334\textsubscript{\textpm\ 0.265}}} &	
\makecell[c]{0.899\textsubscript{\textpm\ 0.064}}	&
\makecell[c]{0.587\textsubscript{\textpm\ 0.317}}	&
\makecell[c]{0.702\textsubscript{\textpm\ 0.174}}&	
\makecell[c]{\textbf{0.956\textsubscript{\textpm\ 0.014}}}&	
\makecell[c]{0.776\textsubscript{\textpm\ 0.063}}&	
\makecell[c]{\textbf{0.377\textsubscript{\textpm\ 0.197}}}\\   
3D UNet-MTL-TSD&      
\makecell[c]{0.880\textsubscript{\textpm\ 0.138}}
&	\makecell[c]{0.330\textsubscript{\textpm\ 0.268}} 
&\makecell[c]{\textbf{0.906\textsubscript{\textpm\ 0.060}}}
&	\makecell[c]{\textbf{0.589\textsubscript{\textpm\ 0.310}}}
& \makecell[c]{\textbf{0.725\textsubscript{\textpm\ 0.167}}}
&	 \makecell[c]{0.950\textsubscript{\textpm\ 0.017}}
&	\makecell[c]{\textbf{0.804\textsubscript{\textpm\ 0.059}}}
&	 \makecell[c]{0.376\textsubscript{\textpm\ 0.192}}\\\hline
3D UNet
& \makecell[c]{0.822\textsubscript{\textpm\ 0.212}}&	
\makecell[c]{0.303\textsubscript{\textpm\ 0.286}} &	
\makecell[c]{0.818\textsubscript{\textpm\ 0.178}}&	
\makecell[c]{0.527\textsubscript{\textpm\ 0.316}}&
	\makecell[c]{0.672\textsubscript{\textpm\ 0.202}} &	
	\makecell[c]{0.927\textsubscript{\textpm\ 0.059}}	&
	\makecell[c]{0.782\textsubscript{\textpm\ 0.082}}&
		\makecell[c]{\textbf{0.332\textsubscript{\textpm\ 0.171}}} \\
3D UNet - MTL-TSOL  &         
\makecell[c]{\textbf{0.830\textsubscript{\textpm\ 0.214}}	}
&\makecell[c]{\textbf{0.313\textsubscript{\textpm\ 0.280}}}
&\makecell[c]{\textbf{0.880\textsubscript{\textpm\ 0.115}}}
& \makecell[c]{\textbf{0.578\textsubscript{\textpm\ 0.316}}}
&	 \makecell[c]{\textbf{0.678\textsubscript{\textpm\ 0.194}}}
&	\makecell[c]{\textbf{0.947\textsubscript{\textpm\ 0.031}}}
&	 \makecell[c]{\textbf{0.820\textsubscript{\textpm\ 0.078}}}
&	\makecell[c]{0.331\textsubscript{\textpm\ 0.183}}\\\hline
3D Att-UNet& 
\makecell[c]{0.879\textsubscript{\textpm\ 0.141}} &	
\makecell[c]{\textbf{0.335\textsubscript{\textpm\ 0.273}}} &	
\makecell[c]{0.874\textsubscript{\textpm\ 0.066}}&
\makecell[c]{0.550\textsubscript{\textpm\ 0.331}}	&
\makecell[c]{\textbf{0.708\textsubscript{\textpm\ 0.176}}}&	
\makecell[c]{\textbf{0.953\textsubscript{\textpm\ 0.015}}}&	
\makecell[c]{0.781\textsubscript{\textpm\ 0.070}}&
	\makecell[c]{\textbf{0.382\textsubscript{\textpm\ 0.184}}}\\
3D Att-UNet-MTL-TSD&                             	    
\makecell[c]{\textbf{0.902\textsubscript{\textpm\ 0.098}}}&	
 \makecell[c]{0.318\textsubscript{\textpm\ 0.259}}
& \makecell[c]{\textbf{0.922\textsubscript{\textpm\ 0.045}}}
& \makecell[c]{\textbf{0.565\textsubscript{\textpm\ 0.315}}}
&	 \makecell[c]{0.699\textsubscript{\textpm\ 0.169}}
&  \makecell[c]{0.948\textsubscript{\textpm\ 0.018}}
	&  \makecell[c]{\textbf{0.797\textsubscript{\textpm\ 0.076}}}
	&	 \makecell[c]{0.381\textsubscript{\textpm\ 0.195}}\\\hline
\end{tabular}
}
\end{table*}
\end{comment}
\begin{figure}[!hbt]
\begin{subfigure}[b]{.23\textwidth}
\includegraphics[width=\textwidth]{Figures/orig_eps_converted_to.pdf} 
\caption{}
\label{fig:origtrimap}
\end{subfigure}
\begin{subfigure}[b]{.23\textwidth}
\includegraphics[width=\textwidth]{Figures/pil_im_eps_converted_to.pdf}
\caption{}
\label{fig:labelmap}
\end{subfigure}
\begin{subfigure}[b]{.23\textwidth}
\includegraphics[width=\textwidth]{Figures/trimap4_eps_converted_to.pdf}
\caption{}
\label{fig:trmap4}
\end{subfigure}
\begin{subfigure}[b]{.23\textwidth}
\includegraphics[width=\textwidth]{Figures/trimap6_eps_converted_to.pdf}
\caption{}
\label{fig:trmap6}
\end{subfigure}
\caption{Trimap regions along boundary of organs shown on 2D abdominal images. (a) Original abdominal image, (b) Organ label-maps corresponding to 2D image, (c) Trimap region of 5 voxel width around boundaries of organs shown in gray color, and (d) Trimap region of 7 voxel width shown in gray color.
}\label{fig:trimap}
\end{figure}
\begin{figure}[ht]
\begin{subfigure}{.28\textwidth}
\includegraphics[scale=0.20]{Figures/unet_panc_plot_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.20]{Figures/unet_bvc_plot_eps_converted_to.pdf}
\caption{}
\end{subfigure}
\hspace*{\fill}
\begin{subfigure}{.28\textwidth}
\includegraphics[scale=0.20]{Figures/unetplus_panc_plot_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.20]{Figures/unetplus_bvc_plot_eps_converted_to.pdf}
\caption{}
\end{subfigure}
\hspace*{\fill}
\begin{subfigure}{.28\textwidth}
\includegraphics[scale=0.20]{Figures/attunet_panc_plot_eps_converted_to.pdf}
\hfill
\includegraphics[scale=0.20]{Figures/attunet_bvc_plot_eps_converted_to.pdf}
\caption{}
\end{subfigure}
\hspace*{\fill}
\caption{Evaluation of boundary segmentation via trimaps. In each subfigure, we have plotted the dice scores (y-axis) obtained by comparing the predicted and groundtruth segmentations in trimap regions with varying number of voxels (x-axis). The dice scores for both the baseline (\textcolor{st}{green}) and boundary-constrained counterparts (\textcolor{Purple}{purple}) are plotted. Top row of (a-c) corresponds to trimap segmentation comparison for Pancreas-CT dataset and bottom row corresponds to BTCV dataset.}
\label{fig:tri}
\end{figure}
\subsection{Segmentation performance along boundary of organs}
Unclear boundaries of organs and low inter-organ contrast prevent accurate segmentation of challenging regions around the organ boundaries on abdominal CT scans. To assess if the incorporation of boundary information has improved the segmentation of voxels within the close vicinity of organ boundaries, we evaluate the quality of predicted voxel-labels within these regions and compare them against the ones acquired via baseline methods. To do this, we generate trimaps \cite{4587417} \cite{Cheng2021BoundaryII} with different voxel-bands surrounding the boundaries of organs. Trimap is a narrow region along the boundary of an object which is utilized to evaluate the quality of segmentation within a specific distance from the object's contour. First, we generate trimap regions around the boundary of organs for predicted and ground-truth segmentations, and then, we compare the 3D trimaps by computing mean Dice scores between them. We show the exemplary trimap regions on 2D abdominal axial slices in \autoref{fig:trimap}. For the sake of compendious presentation, we have computed the trimap Dice scores only for TSOL network topology. \autoref{fig:tri} shows the mean Dice score plotted against the number of voxels the trimap contains. The top row shows trimap plots for the Pancreas-CT dataset, whereas the bottom row shows the trimap plots for the BTCV dataset. Our proposed boundary-constrained models consistently perform better than the baseline models in predicting the semantic labels within the vicinity of organ-boundaries, except for one case, i.e., (3D UNet vs 3D UNet-MTL-TSOL). 
\subsection{Qualitative Results}
\autoref{fig:visual} shows semantic segmentation predictions on a single 2D axial slice of the 3D scans. The first and second row correspond to segmentation results from the Pancreas-CT dataset whereas, the third and fourth row corresponds to results from the BTCV dataset. Each column (from left to right) illustrates the original abdominal 2D images (\autoref{fig:sub30}), ground-truth masks (\autoref{fig:sub31}), baseline model (UNet and Att-UNet) segmentations (\autoref{fig:sub32}), and segmentations acquired from the boundary-constrained counterparts (UNet-MTL-TSD and Att-UNet-MTL-TSOL) in (\autoref{fig:sub33}). We observe that the baseline models led to under-segmentations and over-segmentations, indicated in white boxes in \autoref{fig:sub32}. Furthermore, the segmentations generated by single-task baseline models show isolated and biologically implausible organs' parts. Moreover, comparing with the corresponding boundary-constrained segmentations (\autoref{fig:sub33}), the incorporation of boundary information has prevented the issue of mispredictions near boundary of organs and led to generation of biologically plausible segmentations. \autoref{fig:sub37} presents the 3D segmentations generated by baseline (UNet) and boundary-constrained model (UNet-MTL-TSD) along with the ground-truths from the Pancreas-CT (first row) and BTCV dataset (second row). Notice that the boundary-constrained segmentations (third column) are more similar to the ground-truth masks (first column) as compared to the baseline segmentations (second column). These qualitative results show the improvements induced by the use of organs borders in training the 3D fully convolutional models for abdominal organs segmentation.
\begin{figure*}[!hbt]
\centering
\begin{subfigure}{.23\textwidth}
\centering
\includegraphics[scale=0.25]{Figures/panc_16_case_orig.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/panc_15_case_orig.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/orig_scan17_gt_new_new.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/orig_scan17_gt_new_new1111.jpg}
\phantomcaption
\label{fig:sub30}
\end{subfigure}\begin{subfigure}{.23\textwidth}
\centering
\includegraphics[scale=0.25]{Figures/panc_16_case_gt.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/panc_15_case_gt.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/orig_scan17_gt_new.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/orig_bvc_new.jpg}
\phantomcaption
\label{fig:sub31}
\end{subfigure}
\begin{subfigure}{.23\textwidth}
\centering
\includegraphics[scale=0.25]{Figures/panc16_attenunet_new.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/panc16_attenunet_run1_15case_48slice.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/scan17_bvct_unet_new.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/bvc_unet_pred.jpg}
\phantomcaption
\label{fig:sub32}
\end{subfigure}
\begin{subfigure}{.23\textwidth}
\centering
\includegraphics[scale=0.25]{Figures/panc16_attenunetmtl_new.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/panc16_attenunetmtl_15case_48slice.jpg}
\hfill 
\includegraphics[scale=0.25]{Figures/scan17_unetmtl_new.jpg}
\hfill
\includegraphics[scale=0.25]{Figures/bvc_case38_gt.jpg}
\phantomcaption
\label{fig:sub33}
\end{subfigure}
\caption{Qualitative results for multi-organ segmentation between baseline and boundary-constrained models are shown. Rows 1-2 correspond to results for the Pancreas-CT dataset, whereas Rows 3-4 show results for the BTCV dataset. Columns 1-2 show the original image and the corresponding ground-truth mask overlayed on the 2D image, i.e., spleen ( \raisebox{0.3em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), left kidney ( \raisebox{0.3em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), liver ( \raisebox{0.3em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), stomach ( \raisebox{0.3em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and duodenum ( \raisebox{0.3em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ). Columns 3-4 illustrate the segmentation results related to baseline UNet and UNet-MTL-TSOL counterparts. White boxes indicate the segmented regions improved by incorporation of boundary information.}
\label{fig:visual}
\end{figure*}
\begin{figure}[!hbt]
\begin{subfigure}[b]{.27\textwidth}
\hspace{-2cm}
\includegraphics[trim={2cm 2cm 2cm 0.5cm},clip,width=\textwidth]{Figures/panc_ct6_3d.jpg}
\hfill
\includegraphics[trim={2cm 2cm 2cm 2cm},clip,width=\textwidth]{Figures/case1_panc_3d.jpg}
\phantomcaption
\label{fig:sub34}
\end{subfigure}\begin{subfigure}[b]{.27\textwidth}
\centering
\includegraphics[trim={2cm 2cm 2cm 0.5cm},clip,scale=0.23]{Figures/attenunetnew_new_panc.jpg}
\hfill
\includegraphics[trim={2cm 2cm 2cm 1.6cm},clip,width=\textwidth]{Figures/case1_pancattenunet_3d.jpg}
\phantomcaption
\label{fig:sub35}
\end{subfigure}
\begin{subfigure}[b]{.27\textwidth}
\centering
\includegraphics[trim={2cm 2cm 2cm 0.5cm},clip,scale=0.23]{Figures/attenunetmtl_new_panc.jpg}
\hfill
\includegraphics[trim={2cm 2cm 2cm 1.6cm},clip,width=\textwidth]{Figures/case1_pancmtl_3d.jpg}
\phantomcaption
\label{fig:sub36}
\end{subfigure}
\caption{Visualization of 3D segmentation predictions using Pancreas-CT and BTCV dataset. First column corresponds to abdominal groundtruth annotations, i.e., spleen ( \raisebox{0.3em}{\fcolorbox{Purple}{Purple}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), pancreas ( \raisebox{0.3em}{\fcolorbox{RawSienna}{RawSienna}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), left kidney ( \raisebox{0.3em}{\fcolorbox{Rhodamine}{Rhodamine}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), gallbladder ( \raisebox{0.3em}{\fcolorbox{st}{st}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), esophagus ( \raisebox{0.3em}{\fcolorbox{red}{red}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), liver ( \raisebox{0.3em}{\fcolorbox{BurntOrange}{BurntOrange}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), stomach ( \raisebox{0.3em}{\fcolorbox{lv}{lv}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), and duodenum ( \raisebox{0.3em}{\fcolorbox{du}{du}{\rule{0pt}{0pt}\rule{0pt}{0pt}}} ), whereas second and third column corresponds to volume labels predicted by UNet and UNet-MTL-TSD.}
\label{fig:sub37}
\end{figure}
\section{Discussion}\label{sec:discuss}
Accurate segmentation of abdominal organs from CT scans is required for numerous advanced clinical procedures such as computer-assisted surgery and organ transplantation. The low-contrasted appearance and weak edges of abdominal organs in CT scans adversely affect the accurate segmentation. In this paper, we propose to leverage boundary information of organs as an additional cue for improved 3D abdominal multi-organ segmentation. The boundary-constrained encoder-decoder network simultaneously learns to delineate the semantic abdominal regions and detect the boundaries of organs. This multi-task learning model exploits the statistics from more than one ground-truth source and subsequently retains features shared between the tasks. The boundary annotations of abdominal organs can be easily generated from the ground-truth masks and provide cost-free additional knowledge about the organs.

As reported by quantitative results in \autoref{table:Table 3.1}, the proposed boundary-constrained 3D encoder-decoder models achieve improved multi-organ segmentation across the majority of the baselines (3D UNet, 3D UNet, and 3D Att-UNet) and datasets (Pancreas-CT and BTCV). We have shown that the significant improvement in segmentation performance evaluated via Dice Score, Avg. HD, Recall, and Precision is caused by the improved segmentation of organs' boundaries and regions surrounding boundaries. Furthermore, significant improvements with a negligible increase in parameter count (0.0002\% -TSOL topology) reveal the benefit of regularizing the existing encoder-decoder segmentation models using boundary information.  

The reduction in Avg. HD for both datasets across all baselines depicts the advantage of informing the model about the vulnerable regions of the organ. The dramatic decrease in Avg. HD, ranging from 18\% to 30\%, shows that the model learned feature combinations that were expressive about the entire appearance of organs. We believe that training the models with auxiliary knowledge encourages learning more generalizable and discriminative features. Notably, the additional experiments that we have conducted to precisely assess the improvements in segmentation of regions in the vicinity of organ-boundaries further verify the superiority of training the segmentation model with complementary boundary information (shown in \autoref{fig:sub36}).

Since there may exist several different configurations through which a fully convolutional architecture can be designed under a multi-task learning paradigm, we explore two network topologies based on the extent of parameter-sharing between the tasks. Through our extensive comparison, we notice that an overly-shared multi-task network (TSOL) performs on par with the network designed to have an increased number of task-specific layers (TSD) (\autoref{table:Table 3.1}). This indicates that models with many parameters do not necessarily correspond to higher performance. Most importantly, we also found that incorporation of boundary information improved the multi-organ segmentation performance, regardless of the network topology. One of the critical challenges in designing 3D multi-task deep learning models is determining which layers should be shared while keeping the computational expense reasonable. In the future, we aim to investigate other network topologies for training the encoder-decoder model in a multi-task learning fashion. Another valuable extension of our work is to develop a mechanism/policy that can automatically dictate the sharing pattern of network layers between the two tasks.

As reported in \Cref{table:Table 3.3,table:Table 3.4}, the improvement in the segmentation performance of organs rarely occurring in the dataset reveals the efficacy of boundary-regularized models in compensating for their rare presence. The qualitative results shown in \Cref{fig:visual,fig:sub37} verify the positive impact of making the model aware of organ-boundaries during training. The ability of our model to simultaneously learn the improved representations of multiple organs is indicated by the qualitative examples in \Cref{fig:visual,fig:sub37}, where the boundary-constrain has reduced the occurrence of over- and under-segmented organs.
\section{Conclusion}\label{sec:conc}
In this paper, we leverage organ boundary information for an improved 3D abdominal multi-organ segmentation by addressing the challenge of unclear boundaries in low-contrasted CT scans. We demonstrate that boundary information can be seamlessly introduced in the training of 3D encoder-decoder models through different multi-task configurations. The experimental results show the boundary-constrained multi-organ segmentation outperforms the ones obtained from several FCN-based baseline models, including 3D UNet, 3D UNet, 3D Attention-UNet. Furthermore, we found that the multi-task topology that shows maximum improvement is not fixed and varies depending on the baseline architecture. This insight shows that the optimal utilization of auxiliary information cannot always be harvested through a single deep multi-task design but instead requires the exploration of different multi-task topologies. Our findings also reveal that leveraging organs boundary features improves the segmentation of underweighted organs like the gallbladder, pancreas, and duodenum with a negligible parameter-overhead. Additionally, the experimental results disclose that the boundary-constrained models improve the labelling of weak sub-parts of organs in the vicinity of boundaries. We believe the proposed 3D boundary-constrained models would be valuable for enhancing abdominal organ segmentation and utilizing those segmentations in relevant clinical applications.\\\\
\textbf{Conflict of interest statement:} On behalf of all authors, the corresponding author states that there is no conflict of interest.
\bibliographystyle{elsarticle-num} 
\bibliography{elsarticle_arxiv}









\end{document}
