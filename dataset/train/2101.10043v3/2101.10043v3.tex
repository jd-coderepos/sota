

\documentclass[final]{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{ dsfont }
\usepackage{ cite }
\usepackage{float}
\usepackage{color}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{float}
\renewcommand{\baselinestretch}{0.992} \normalsize
\newcommand{\yu}[1]{\textcolor{magenta}{YU: #1}}
\newcommand{\gustavo}[1]{\textcolor{blue}{GUSTAVO: #1}}
\newcommand{\yuanhong}[1]{\textcolor{orange}{YUANHONG: #1}}
\newcommand{\guansong}[1]{\textcolor{red}{GUANSONG: #1}}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}



\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{Unsupervised Anomaly Detection with Multi-scale Interpolated Gaussian Descriptors}
\newcommand\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\newcommand{\bblue}[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand{\bred}[1]{\textcolor{red}{\textbf{#1}}}

\author{\parbox{.8\linewidth}{\centering Yuanhong Chen   Yu Tian  Guansong Pang    Gustavo Carneiro    Australian Institute for Machine Learning, University of Adelaide \\
  South Australian Health and Medical Research Institute \\
} 
}



\maketitle


\begin{abstract}
Current unsupervised anomaly detection and pixel-wise anomaly localisation systems are commonly formulated as one-class classifiers that depend on an effective estimation of the distribution of normal images and robust criteria to identify anomalies.
However, the distribution of normal images estimated by current systems tends to be unstable for classes of normal images that are under-represented in the training set, and the anomaly identification criteria commonly explored in the field does not work well for multi-scale structural and non-structural anomalies. 
In this paper, we introduce a new unsupervised anomaly detection and localisation method designed to address these two issues.
More specifically, we introduce a normal image distribution estimation method that is robust 
to under-represented classes of normal images -- this method is based on adversarially interpolated descriptors from training images and a Gaussian classifier. 
We also propose a new anomaly identification criterion that can accurately detect and localise multi-scale structural and non-structural anomalies.
In extensive experiments on MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, our approach shows better results than the current state of the art in the standard experimental setup for unsupervised anomaly detection and localisation. 
Code is available at \url{https://github.com/tianyu0207/IGD}.  \end{abstract}

\section{Introduction}


Anomaly detection and pixel-wise anomaly localisation are critical tasks in the identification of defects on industry objects~\cite{mvtecad} or abnormalities from medical images~\cite{tian2020few,tian2021detecting,anogan,f-anogan}.
Given that most of the training sets available for this task only contain normal images, methods are usually formulated as one-class classifiers (OCC)~\cite{ratsch2000svm,hasan2016learning,zhang2016video,ravanbakhsh2017abnormal,ravanbakhsh2018plug,luo2017revisit,liu2018future, hinami2017joint,venkataramanan2019attention,dsvdd} -- these methods are commonly referred to as unsupervised anomaly detectors (UAD).
OCCs aim to first estimate the distribution of normal images in the training set and then use a criterion (e.g., distance to the one-class centre~\cite{dsvdd}) to detect and localise anomalies in test images.
Hence, the \textbf{functionality of OCCs} depends on the \textbf{normal image distribution generalisation} and on the \textbf{anomaly identification criteria}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{image/heatmap_hori.png}
    \caption{
    Qualitative results of our anomaly localisation results on the MVTec AD dataset (red = high probability of anomaly). Top, middle and bottom rows show the testing images, ground-truth masks and predicted heatmaps, respectively.} 
    \label{fig:mvtec_seg}
\end{figure}



When the estimated distribution of normal images~\cite{ocgan,dsvdd,gong2019memorizing} does not generalise well, it does not fit  under-represented types of normal images from the training set, resulting in false detection of anomalies.
Methods to constrain the distribution of normal images to lie at specific regions of the feature space can mitigate this issue~\cite{ocgan,dsvdd}, but they still do not provide a robust generalisation, as evidenced by their results.
A recent trend in the field to address this generalisation issue is based on self-supervision~\cite{golan2018deep,bergman2020classification}, which has been successful in synthetic anomaly detection problems, such as CIFAR10~\cite{krizhevsky2014cifar}.
However, self-supervision alone shows poor generalisation for anomaly detection in real-world data sets~\cite{golan2018deep,bergman2020classification}, such as MVTec AD~\cite{mvtecad}.
Moreover, there is no evidence that self-supervised approaches can work well for unsupervised anomaly localisation.  Therefore, although promising, it is unclear if self-supervision alone will be enough to produce competitive  real-world anomaly localisation results. 

The criteria to detect and localise image anomalies are usually based on the reconstruction loss between the original image and its reconstructed image obtained from a generative model fitted to represent the normal training data~\cite{liu2018future,ren2015unsupervised,xu2015learning,ionescu2019object,gong2019memorizing,nguyen2019anomaly,sabokrou2017deep,sabokrou2018adversarially,morais2019learning}.
Such reconstruction loss generally relies on the mean square error (MSE), which can miss multi-scale structural anomalies~\cite{pangguansong2020,SSIM}.
Recently, a method based on structural similarity index measure (SSIM)~\cite{SSIM} has been proposed~\cite{ae-ssim} to detect single-scale structural anomalies, but
anomalies can exhibit abnormality in varying scales.
Therefore, it is important that the criteria to identify anomalies take into account global and local reconstruction errors, with the local errors considering
multi-scale structural and non-structural anomalies.

In this paper, we introduce a novel unsupervised anoma-\\ly detection and localisation method that tackles the two issues described above.
\textbf{To address the normal image distribution generalisation}, we propose the \textbf{interpolated Gaussian descriptor (IGD)}. Our IGD consists of a new Gaussian Support Vector Data  Descriptor (GSVDD) model, which enables a robust estimation of the normal image distribution, driven by the adversarially constrained auto-encoder interpolation (ACAI)~\cite{berthelot2018understanding} that increases sample density of the training set in the latent space without adding more data in the training set.
\textbf{The improvement of the anomaly detection criteria} is
based on \textbf{a new combination of multi-scale structural similarity index measure (MS-SSIM)~\cite{SSIM,MS-SSIM,zhao2016loss}
and mean absolute error (MAE)} to accurately identify structural and non-structural anomalies of varying sizes. 
The \textbf{training} of our model \textbf{minimises a combined classification and reconstruction losses}, which is \textbf{new in anomaly detection}, to the best of our knowledge.
 
We evaluate our framework on MNIST~\cite{lecun2010mnist}, Fashion MNIST (FMNIST)~\cite{fmnist} CIFAR10~\cite{krizhevsky2014cifar}, MVTec AD~\cite{mvtecad}, a medical Hyper-Kvasir (colonoscopy)~\cite{borgli2020hyperkvasir} and LAG (glaucoma)~\cite{li2019attention} datasets. Our method shows the best results on all six datasets including synthetic anomaly detection problems, industrial defect detection and localisation, and malignant lesion detection.  This indicates that our approach can generalise well across different application domains. 




\section{Related Work}






\subsection{Unsupervised Anomaly Detection} 
\label{sec:unsupervised_anomaly_detection}

Unsupervised anomaly detection (UAD) is generally solved with OCCs~\cite{scholkopf2000support} that clusters the features extracted from normal training images around a particular region of the feature space, and classification is based on how close test image features are from this region. 
Initially, OCCs were explored with hand-crafted features~\cite{medioni2001event,basharat2008learning,wang2014learning,zhang2009learning}, but more recently, end-to-end deep learning models learn both the feature extractor and classifier together~\cite{dsvdd,Markovitz_2020_CVPR,bergmann2020uninformed,Park_2020_CVPR,morais2019learning,lsa,ocgan,bergman2020classification,zhou2020encoding,golan2018deep,wang2019gods,del2016discriminative,Cheng_2015_CVPR,Park_2020_CVPR}. 
A representative model of these approaches is  SVDD~\cite{svdd}, which forces normal image features to be inside a hyper-sphere with a pre-defined centre and a radius that is minimised to contain all training images. Then, test images that fall inside the learned hyper-sphere are classified as normal, and the ones outside are anomalous.
Although powerful, the hard boundary of SVDD can cause the model to overfit the training data -- this problem was tackled by Ruff et al.\cite{dsvdd} with a soft-boundary SVDD, but it still can overfit given that it does not optimise for other latent variables (e.g., hyper-sphere centre).


Other UAD methods rely on generative models.
In~\cite{ocgan}, a generative adversarial network (GAN) is trained to produce normal samples, and its discriminator is used to detect anomalies, but the hard training process of GANs represents a disadvantage of this approach.
Auto-encoders (AE)~\cite{liu2018future,ren2015unsupervised,xu2015learning,ionescu2019object,gong2019memorizing,nguyen2019anomaly,sabokrou2017deep,sabokrou2018adversarially,Burlina_2019_CVPR,venkataramanan2019attention,zong2018deep} are trained to reconstruct normal data, and the anomaly score is defined as the reconstruction error between the input and reconstructed images.  AE approaches depend on the MSE reconstruction loss, which does not work well for structural anomalies.  Alternatively, single-scale SSIM loss~\cite{ae-ssim} tends to work poorly for non-structural anomalies and structural anomalies of sizes outside that single scale.  

The robustness of anomaly detectors can be improved if they are fine-tuned from models pre-trained on other tasks, such as image classification~\cite{venkataramanan2019attention,bergman2020classification}, video understanding~\cite{sultani2018real,tian2019one,Wu2020not,zhong2019graph}, and object detection~\cite{ionescu2019object}. 
Venkataramanan et al.~\cite{venkataramanan2019attention} utilise a model pre-trained on ImageNet~\cite{imagenet} and CelebA~\cite{liu2018large} that shows promising results on several anomaly detection benchmarks. 
The state-of-the-art (SOTA) model by Bergmann et al.~\cite{bergmann2020uninformed} uses a student-teacher framework to predict anomalies with multiple shallow students to regress the output of a teacher network, which is pre-trained on ImageNet. 
Recent works~\cite{golan2018deep,bergman2020classification} use self-supervised training to increase the effectiveness of the learned embeddings. For instance, GeoTrans~\cite{golan2018deep} trains a classifier to predict the geometric transformation applied to normal training data, and combines the prediction of each type of transformation as the anomaly score for transformed testing images. 
Bergman et al.~\cite{bergman2020classification} extend SVDD~\cite{dsvdd} to construct a hyper-sphere for each type of data augmentation. During testing, they predict the probability that all transformed samples fall into their respective hyper-spheres.  
From the evidence above, self-supervised learning is effective only in data sets where the anomaly is represented by the whole image (e.g., a dog image is an anomaly for a system trained with ``normal'' cat images), such as CIFAR10.
Given the positive results  by~\cite{golan2018deep,bergmann2020uninformed}, we also rely on pre-training.



\begin{figure*}[t!]
\begin{center}
\includegraphics[width=0.95\linewidth]{image/structure_v6.png}
\end{center}
\caption{Our method introduces: 1) the Gaussian SVDD classifier trained to map normal images to the mean of a Gaussian distribution, trained with samples from images and interpolated features that are also used to train a critic network to uncover the mixing up coefficient ; 2) a combination of a MS-SSIM and MAE reconstruction losses to enable the detection of multi-scale structural and non-structural anomalies;
and 3) a learning based on a combined classification (GSVDD) and reconstruction (MS-SSIM+MAE) losses. 
}
\label{fig:train}
\end{figure*}


\subsection{Unsupervised Anomaly Localisation} 
\label{sec:unsupervised_anomaly_localisation}

Unsupervised anomaly localisation targets the segmentation of anomalous image pixels or patches, containing, for example, lesions from medical images~\cite{Li_2019_CVPR,liu2019photoshopping}, or defects in industry images~\cite{mvtecad,bergmann2020uninformed}.
The main idea explored is based on extending the image based OCC to a pixel-based OCC.
Then, testing produces a pixel-wise anomaly score map, where a pixel with a high score represents an anomalous pixel~\cite{akcay2018ganomaly,baur2018deep,ae-ssim,ravanbakhsh2019training,anogan}. 
The way such scores are computed are based on the same anomaly detection criteria presented above in Sec.~\ref{sec:unsupervised_anomaly_detection}.
In general, methods that can localise anomalies~\cite{f-anogan, venkataramanan2019attention, bergmann2020uninformed} are tuned to particular anomaly sizes and structure, which can cause then to miss anomalies outside that range of sizes and structure.  






\section{Method}

In this section, we introduce our \textbf{main contributions}, namely: 1) the 
\textbf{Interpolated Gaussian Descriptor (IGD)} that combines the adversarially constrained auto-encoder interpolation (ACAI)~\cite{berthelot2018understanding} and an extension for the SVDD model~\cite{svdd} to work with a Gaussian classifier trained with the EM algorithm~\cite{dempster1977maximum};  2) the \textbf{combination of MS-SSIM  and MAE} as criteria to identify structural and non-structural anomalies of varying sizes;
and 3) the learning based on a \textbf{combined classification and reconstruction losses}.








We assume the availability of a
training set that contains only normal samples and is denoted by , where  represents an RGB image of width  and height .  The testing set contains normal and anomalous images, where anomalous images have segmentation map annotations.  This testing set is defined by
, where , the segmentation map 
with the anomaly is denoted by  ( denotes a normal and  denotes an anomalous pixel in the image ) if , and  if .



The model is trained with the following loss (see Fig.~\ref{fig:train}):

where 
  is the loss for training the \textbf{GSVDD classifier},
 is for ACAI's \textbf{interpolation model}, and  for the \textbf{auto-encoder model} based on the MS-SSIM and MAE reconstruction losses.  
The loss in~\eqref{eq:loss}, which we explain in more detail in Sections~\ref{sec:IGD} and~\ref{sec:MS-SSIM_MAE}, is used to train two models: the global one works on the whole image , and the local model works on image patches , with  and , centred at pixel  ( is the image lattice).
During inference, the results from the global and local models are combined to produce multi-scale anomaly detection and localisation.
 Hence, the combination of the two models enables a multi-scale analysis of the image.






\subsection{Interpolated Gaussian Descriptor (IGD)}
\label{sec:IGD}





















Our proposed IGD model consists of the auto-encoder to reconstruct the input image, and the interpolated GSVDD classifier to constrain the latent space of the auto-encoder (See Fig.~\ref{fig:train}). 

\textbf{GSVDD Classifier:} The GSVDD constrains the latent space of the auto-encoder with the following loss:

where the GSVDD classifier is defined with ,  
 represents the encoder parameterised by , with  denoting the space of latent embeddings of the auto-encoder.  
The mean and standard deviation values above are estimated with  and , 
where  is a constant that regularises the estimation of  to prevent numerical instabilities during training. Our GSVDD replaces the hyper-sphere used in deep SVDD (DSVDD)~\cite{dsvdd} by a Gaussian, and treats the Gaussian mean and covariance as latent variables to be estimated during the expectation stage of the EM optimisation~\cite{dempster1977maximum}.  Such optimisation has the potential to be less sensitive to outliers compared with the one proposed for DSVDD~\cite{dsvdd}, which explicitly minimises the radius of the hyper-sphere representing the normal embeddings, but fixes the centre of this hyper-sphere at the first epoch. 

\textbf{Interpolation model:} To facilitate and regularise the GSVDD training, we linearly interpolate the latent embeddings from different input images, and estimate the interpolation coefficient with the \textbf{critic network}~\cite{berthelot2018understanding}.
The intuition behind this interpolation model
is to make the distribution of training normal samples denser in the latent embedding space, reducing the likelihood that anomaly embeddings may sit in the same region of the embedding space occupied by normal samples.
Unlike Mix-up~\cite{zhang2017mixup}, our  interpolation model, based on ACAI~\cite{berthelot2018understanding}, is a self-supervised method that does not rely on data augmentation on the input space and does not interpolate training labels, making it more adequate for our problem because it enforces a compact and dense distribution of normal samples to be estimated for the GSVDD classifier. 
The critic network is represented by

where  
represents the reconstruction of the interpolation of  and  (with , ,  , and  denoting a uniform distribution )~\cite{berthelot2018understanding}, and  denotes the decoder of the auto-encoder model. The goal of the critic network  is to predict the interpolation coefficient . 
The critic network in~\eqref{eq:critic} is similar to the discriminator in GAN~\cite{gan}, and relies on the following adversarial loss to be optimised~\cite{berthelot2018understanding}

where 
 is defined in~\eqref{eq:critic}, and
, with 
and  denoting a reconstruction of  by the auto-encoder.
The first term of this loss minimises the critic's prediction error for  and the second term regularises the training to ensure that the critic predicts  when the original image is interpolated with its own reconstruction in the image space . 


\textbf{Auto-encoder model:} The optimisation of the encoder and decoder models minimises the following loss

where  is a reconstruction of  by the auto-encoder, with the image reconstruction loss  based on a combined MS-SSIM and MAE reconstruction losses (to be defined below  in~\eqref{eq:global_reconstruction_loss}), and
 is a hyperparameter to weight the regularisation term. This regularisation fools the critic to output  for interpolated embeddings, independently of , following standard adversarial training~\cite{gan}.



\subsection{Detecting Structural and Non-structural Ano- malies}
\label{sec:MS-SSIM_MAE}

The combination of MS-SSIM and MAE losses to detect structural and non-structural anomalies of varying sizes is another contribution of our paper -- see Fig.~\ref{fig:multi-test}.
Mean square error (MSE) has been used in anomaly detection~\cite{ren2015unsupervised,xu2015learning,ionescu2019object,gong2019memorizing,venkataramanan2019attention} and it has been shown to be effective to detect non-structural anomalies, caused by global changes in the image.
However, MSE tends to be too brittle and produce high values when there are small (but global) localisation errors and it also fails to detect structural anomalies, represented by image abnormalities that have strong inter-dependencies between neighbouring pixels.
Also, the MSE loss can overfit to large reconstruction errors, which is commonly solved by replacing it with mean absolute error (MAE), which although more robust to overfitting, still has the same issues as MSE.
That motivated the use of SSIM~\cite{SSIM} in anomaly detection~\cite{ae-ssim}, but SSIM may fail to detect non-structural anomalies. 
Furthermore, previous methods do not explore multi-scale reconstruction losses, which is particularly important for structural anomalies of varying sizes.
Therefore, we propose to combine the multi-scale SSIM (MS-SSIM)~\cite{MS-SSIM} with MAE to enable the detection of structural and non-structural multi-scale anomalies.



The combined MAE and MS-SSIM reconstruction loss-\\es, denoted by  in~\eqref{eq:loss_AE}, is defined as:

where ,  denotes the MS-SSIM score~\cite{MS-SSIM} 
, with larger values indicating higher similarity between patches  of the original and reconstructed images. Definitions of the global and local MS-SSIM scores,  and ,
respectively, for the global (using the whole image) and local (using image patches) IGD models are demonstrated in Fig.~\ref{fig:multi-test}.
Note that our global and local IGD models use the same network architecture, but are trained separately using the combined MS-SSIM and MAE reconstruction loss~\eqref{eq:global_reconstruction_loss}, in addition to the other loss terms in~\eqref{eq:loss}.  





\subsection{Training and Inference of the Global and Local IGD}




To enable a fair comparison between our method and previous approaches in the field~\cite{bergmann2020uninformed,venkataramanan2019attention,bergman2020classification,golan2018deep}, we pre-train the encoders for the global and local IGD models either with self-supervised learning (SSL)~\cite{chen2020simple}
or knowledge distillation (KD)~\cite{bergmann2020uninformed,gou2020knowledge,hinton2015distilling}. 
We try both pre-training approaches because they have been shown to produce encoders that can fit more easily different types of downstream tasks.
For self-supervision pre-training, we follow the contrastive learning of visual representations using several types of data augmentation~\cite{chen2020simple} with the images from . 
For KD, we pre-train the encoder  to reconstruct the embeddings of the images in  produced by an ImageNet pre-trained model. 


After pre-training, we train the global and local IGD models separately, following an EM optimisation.  The E-step estimates the values of the latent variables  and  in~\eqref{eq:loss_GSVDD}.
For the M-step, we minimise the loss in~\eqref{eq:loss} by optimising  with respect to  in~\eqref{eq:loss_AE} and  in~\eqref{eq:loss_GSVDD},  with respect to  in~\eqref{eq:loss_AE}, and  with respect to  in~\eqref{eq:loss_critic}.  





\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{image/inference_vv2.png}
   \caption{Example of the multi-scale structural and non-structural anomaly localisation result for an MVTec AD~\cite{mvtecad} image, using both the local and global IGD models use the anomaly local score  in~\eqref{eq:localisation_loss^(G)lobal_local_MS-SSIM}. The global model tends to produce smooth results but with some mistakes, while the local model produces jagged results, but without the global mistakes, so by combining the two results, we obtain a smooth and correct anomaly heatmap.  }
    \label{fig:multi-test}
\end{figure}

During inference, \textbf{anomaly detection} is performed by combining the global and local IGD anomaly scores for a testing image  as in:

The global score in~\eqref{eq:anomaly_score_image} is defined as

where  denotes the reconstruction loss from~\eqref{eq:global_reconstruction_loss} and  denotes the GSVDD loss from~\eqref{eq:loss_GSVDD} (both computed with the global IGD model using the whole images), and  is the reconstruction of  produced by the auto-encoder.  The local score in~\eqref{eq:anomaly_score_image} is defined as

where  and  are the reconstruction and GSVDD losses computed from the local model, with  denoting an image patch of size  at pixel .
The use of max pooling of the local scores in~\eqref{eq:local_score_detection} facilitates detection of images that contain anomalies covering a small region of the image.
\textbf{Anomaly localisation} is computed for each pixel  to produce a local score with  

In~\eqref{eq:localisation_loss^(G)lobal_local_MS-SSIM}, we have

where  and  are defined in~\eqref{eq:global_reconstruction_loss} and 
 is a reconstruction of  produced by the global IGD model. The  in~\eqref{eq:localisation_loss^(G)lobal_local_MS-SSIM} is similarly defined using the local IGD model. Thus, the anomaly localisation final map is a 
heatmap with high values representing regions that are likely to contain anomalies, as displayed in Fig.~\ref{fig:multi-test}.






\section{Experiments}


\subsection{Datasets and Evaluation Metric}
\label{sec:datasets_evaluation}

\textbf{Datasets:} We use six different datasets to evaluate our methods, namely MNIST~\cite{lecun2010mnist}, Fashion MNIST~\cite{fmnist}, CIFAR10~\cite{krizhevsky2014cifar} and MVTec AD~\cite{mvtecad}, Hyper-Kvasir~\cite{borgli2020hyperkvasir} and LAG~\cite{li2019attention}. MNIST, Fashion MNIST and CIFAR10 have been widely used as benchmarks for image anomaly detection, and we follow the same experimental protocol as described in~\cite{ocgan,ADGAN,lsa,dsvdd,venkataramanan2019attention,bergmann2020uninformed,gong2019memorizing}. CIFAR10 contains 60,000 images with 10 classes. MNIST and Fashion MNIST contains 70,000 images with 10 classes of handwritten digits and fashion products, respectively. 

MVTec AD~\cite{mvtecad} is a recently released data set that contains 5,354 high-resolution real-world images of 15 different industry object and textures. 
The normal class of MVTec AD is formed by the images without defects and consists of 3,629 images for training and 467 images for testing.
The anomalous class has more than 70 categories of defects
(such as dents, structural fails, contamination, etc.) and contains 1,258 images. 
Furthermore, MVTec AD also provides pixel-wise ground truth annotations for all anomalies, allowing the evaluation of anomaly detection and localisation. 

We also tested our method on two publicly available medical datasets: Hyper-Kvasir~\cite{borgli2020hyperkvasir} and LAG~\cite{li2019attention} for polyp and glaucoma detection, respectively. For Hyper-Kvasir, we have 1,600 normal images without polyps in the training set and 500 in the testing set; and 1,000 abnormal images containing polyps in the testing set. For LAG, we have 2,343 normal images without glaucoma in the training set; and 800 normal images and 1,711 abnormal images with glaucoma for testing. 






\textbf{Evaluation Metric:} For \emph{anomaly detection}, we use area under the receiver operating characteristic curve (AUC) for all data sets~\cite{ocgan,ADGAN,lsa,dsvdd,venkataramanan2019attention,bergmann2020uninformed,gong2019memorizing}. 
On MNIST, Fashion MNIST and CIFAR10, we use the same protocol as~\cite{ocgan,dsvdd,lsa,venkataramanan2019attention,bergmann2020uninformed,golan2018deep,bergman2020classification}
, where training uses a single class as the normal data, with the nine remaining classes denoting the anomalous data, and inference relies on a non-augmented test image. 
We report the mean AUC over the 10 classes for the above three data sets. On MVTec AD, as suggested in~\cite{bergmann2020uninformed,venkataramanan2019attention}, we evaluate anomaly detection performance not only with AUC, but also with mean accuracy. For the colonoscopy and glaucoma detection datasets, we evaluate the methods using AUC. 
For \emph{anomaly localisation}, we follow~\cite{venkataramanan2019attention} and compute the mean pixel-level AUC between the generated heatmap and the ground truth segmentation map for each anomalous image in the testing set on MVTec AD.


\bgroup
\def\arraystretch{1.1}
\begin{table}[t!]
\centering
\resizebox{0.90\linewidth}{!}{\begin{tabular}{@{}c|c|ccc@{}}
\toprule
Pretrain &Method                   &	MNIST	        &	CIFAR10	 & FMNIST        \\		\midrule\midrule
\multirow{16}{*}{Scratch}&DAE~\cite{hadsell2006dimensionality}                         &	0.8766	        &	0.5358	 &  -       \\	
&VAE~\cite{vae}	                        &	0.9696	        &	0.5833     &  - 	\\
&KDE~\cite{bishop2006pattern}                         & 0.8140       &		0.6100	   &  -     \\
&OCSVM~\cite{oc-svm}	        &	0.9510	        &	0.5860     &  - 	\\	
&AnoGAN~\cite{anogan}	                    &	0.9127	        &	0.6179     &  - 	\\	
&DSVDD~\cite{dsvdd}	                    &	0.9480	        &	0.6481      &  -	\\	
&OCGAN~\cite{ocgan}	                    &	0.9750	        &	0.6566	    &  -    \\
&PixelCNN~\cite{van2016conditional}	                    & 0.6180	  	&	0.5510 	   &  -     \\
&CapsNet\textsubscript{PP}~\cite{li2020exploring}	                    &0.9770	   	&0.6120		 &  0.7650       \\
&CapsNet\textsubscript{RE}~\cite{li2020exploring}	                    &0.9250	   	& 0.5310		&  0.6790        \\
&ADGAN~\cite{ADGAN}	                    & 0.9680	   	& 0.6340	&  -	        \\
&LSA~\cite{lsa}	                    & 0.9750	   	& 0.6410		 &  0.8760       \\
&MemAE~\cite{gong2019memorizing}	                    &	0.9751      	&	0.6088	   &  -       \\	
&GradCon~\cite{gradcon}	                    & 0.9730	   	&	0.6640	    &  -      \\
&-VAE\textsubscript{u}~\cite{lamda-vae}	                    & 0.9820	   	& 0.7170	    &  0.8730     \\
&ULSLM~\cite{ulslm}	                    & 0.9490	   	& 0.7360		 &  -         \\ 
&\textbf{Ours}	            & \textbf{0.9869}		&	\textbf{0.7433}	 & \textbf{0.9201} \\\hline
\multirow{3}{*}{SSL}
&Rot-Net~\cite{golan2018deep}	        &	-        &	0.8160	  &  0.9350     \\	
 &Bergman et al.~\cite{bergman2020classification}	        &	-        &	0.8820	  &  0.9410     \\	
&\textbf{Ours}	            &	-	& 	\textbf{0.9125}	 & \textbf{0.9441} \\\hline
\multirow{3}{*}{Imagenet} &CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention} 	&	0.9860        &	0.7370	    &  0.8850      \\	
&Student-Teacher~\cite{bergmann2020uninformed}	        &	\textbf{0.9935}	        &	0.8196	  &  -      \\	
&\textbf{Ours}	            &	\textbf{0.9927}	&	\textbf{0.8368}	 & \textbf{0.9357} \\ 

\bottomrule
\end{tabular}}
\caption{\textbf{Anomaly detection:} mean AUC results on MNIST, CIFAR10 and Fashion MNIST. The results are split into scratch without any pre-training, KD or pretrained with Imagenet and self-supervised learning (SSL). Bold numbers represent the best result (within 0.5\%) for each data set, discriminated by scratch, SSL or Imagenet.}
\label{tab:auc_detection}
\end{table}




\bgroup
\def\arraystretch{1.4}
\begin{table*}[t]
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}{@{}cccccccccccccccccc@{}}
\toprule 
Metric   & Method                                                     & Bottle & Hazelnut & Capsule & Metal Nut & Leather & Pill  & Wood  & Carpet & Tile  & Grid  & Cable & Transistor & Toothbrush & Screw & Zipper & Mean  \\ \hline \hline
       \multirow{10}{*}{Accuracy}   & AVID~\cite{sabokrou2018adversarially}                      
         & 0.85   & 0.86     & 0.85    & 0.63      & 0.58    & 0.86  & 0.83  & 0.70    & 0.66  & 0.59  & 0.64  & 0.58       & 0.73       & 0.66  & 0.84   & 0.73  \\
         & AE\textsubscript{SSIM~\cite{ae-ssim}}                       
         & 0.88   & 0.54     & 0.61    & 0.54      & 0.46    & 0.60   & 0.83  & 0.67   & 0.52  & 0.69  & 0.61  & 0.52       & 0.74       & 0.51  & 0.80    & 0.63  \\
         & DAE~\cite{hadsell2006dimensionality}                       
         & 0.80    & 0.88     & 0.62    & 0.73      & 0.44    & 0.62  & 0.74  & 0.50    & 0.77  & 0.78  & 0.56  & 0.71       & \bred{0.98}       & 0.69  & 0.80    & 0.71  \\
         & AnoGAN~\cite{anogan}                                       
         & 0.69   & 0.50      & 0.58    & 0.50       & 0.52    & 0.62  & 0.68  & 0.49   & 0.51  & 0.51  & 0.53  & 0.67       & 0.57       & 0.35  & 0.59   & 0.55  \\ 
         & -VAE\textsubscript{u~\cite{lamda-vae}}             
         & 0.86   & 0.74     & \bblue{0.86}    & \bblue{0.78}      & 0.71    & 0.80   & 0.89  & 0.67   & 0.81  & 0.83  & 0.56  & 0.70        & 0.89       & 0.71  & 0.67   & 0.77  \\
         & LSA~\cite{lsa}                                             
         & 0.86   & 0.80      & 0.71    & 0.67      & 0.70     & 0.85  & 0.75  & \bblue{0.74}   & 0.70   & 0.54  & 0.61  & 0.50        & 0.89       & 0.75  & \bblue{0.88}   & 0.73  \\
         & CAVGA-D\textsubscript{u~\cite{venkataramanan2019attention}} 
         & 0.89   & 0.84     & 0.83    & 0.67      & 0.71    & \bblue{0.88}  & 0.85  & 0.73   & 0.70   & 0.75  & 0.63  & 0.73       & 0.91       & \bblue{0.77}  & 0.87   & 0.78  \\
         & CAVGA-R\textsubscript{u~\cite{venkataramanan2019attention}} 
         & \bblue{0.91}   & \bblue{0.87}     & \bred{0.87}    & 0.71      & 0.75    & \bred{0.91}  & 0.88  & \bred{0.78}   & 0.72  & 0.78  & 0.67  & 0.75       & \bblue{0.97}       & \bred{0.78}  & \bred{0.94}   & 0.82  \\ 
         & \textbf{Ours - KD}                                                  
         & \textcolor{red}{\textbf{0.95}}   & \bred{0.93}     & 0.80     & \bred{0.82}      & \bblue{0.87}    & 0.77  & \bred{0.94}  & 0.69   & \bred{0.90}   & \bred{0.92}  & \bblue{0.73}  & \bred{0.88}       & \bred{0.98}       & 0.58  & 0.85   & \bblue{0.84}  \\
         & \textbf{Ours - SSL}                                                 
         & \textcolor{red}{\textbf{0.95}}   & \bred{0.93}     & 0.81    & \bred{0.82}      & \bred{0.90}     & 0.74  & \bblue{0.89}  & 0.71   & \bblue{0.94}  & \bblue{0.90}   & \bred{0.79}  & \bblue{0.85}       & \bred{0.98}       & 0.67  & \bblue{0.88}   & \bred{0.85}  \\ \midrule
 \multirow{7}{*}{AUC}        & AnoGAN~\cite{anogan}                                       
         & 0.800    & 0.259    & 0.442   & 0.284     & 0.451   & 0.711 & 0.567 & 0.337  & 0.401 & 0.871 & 0.477 & 0.692      & 0.439      & 0.100   & 0.715  & 0.503 \\
         & GANomaly~\cite{akcay2018ganomaly}                          
         & 0.794  & 0.874    & 0.721   & 0.694     & 0.808   & 0.671 & 0.920  & 0.821  & 0.720  & 0.743 & 0.711 & 0.808      & 0.700        & \textcolor{red}{\textbf{1.000}}     & 0.744  & 0.782 \\
         & Skip-GANomaly~\cite{akccay2019skip}                        & 0.937  & 0.906    & 0.718   & 0.790      & 0.908   & 0.758 & 0.919 & 0.795  & 0.850  & 0.657 & 0.674 & 0.814      & 0.689      & \textcolor{red}{\textbf{1.000}}    & 0.663  & 0.805 \\
     & U-Net~\cite{u-net}                                         & 0.863  & 0.996    & 0.673   & 0.676     & 0.870    & 0.781 & 0.958 & 0.774  & 0.964 & 0.857 & 0.636 & 0.674      & 0.811      & \textcolor{red}{\textbf{1.000}}     & 0.750   & 0.819 \\
         & DAGAN~\cite{DAGAN}                                         & \textcolor{blue}{\textbf{0.983}}  & \textcolor{red}{\textbf{1.000}}        & 0.687   & 0.815     & \textcolor{blue}{\textbf{0.944}}   & 0.768 & \textcolor{blue}{\textbf{0.979}} & \textcolor{red}{\textbf{0.903}}  & 0.961 & 0.867 & 0.665 & 0.794      & \textcolor{blue}{\textbf{0.950}}       & \textcolor{red}{\textbf{1.000}}     & 0.781  & 0.873 \\
         & \textbf{Ours - KD}                                                  & \textcolor{red}{\textbf{1.000}}      & 0.986    & \textcolor{blue}{\textbf{0.907}}   & \textcolor{blue}{\textbf{0.886}}     & 0.922   & \textcolor{blue}{\textbf{0.870}}  & \textcolor{red}{\textbf{0.982}} & \textcolor{blue}{\textbf{0.828}}  & \textcolor{blue}{\textbf{0.979}} & \textcolor{red}{\textbf{0.979}} & \textcolor{blue}{\textbf{0.856}} & \textcolor{red}{\textbf{0.909}}     & \textcolor{red}{\textbf{0.997}}      & 0.815 & \textcolor{blue}{\textbf{0.969 }} & \textcolor{blue}{\textbf{0.926}} \\
         & \textbf{Ours - SSL}                                                 & \textcolor{red}{\textbf{1.000}}     & \textcolor{blue}{\textbf{0.997}}    & \textcolor{red}{\textbf{0.915}}   & \textcolor{red}{\textbf{0.913}}     & \textcolor{red}{\textbf{0.958}}   & \textcolor{red}{\textbf{0.873}} & 0.946 & \textcolor{blue}{\textbf{0.828 }} & \textcolor{red}{\textbf{0.991}} & \textcolor{blue}{\textbf{0.978}} & \textcolor{red}{\textbf{0.906}} &\textcolor{blue}{\textbf{0.906 }}     & \textcolor{red}{\textbf{0.997}}     & \textcolor{blue}{\textbf{0.825}} & \textcolor{red}{\textbf{0.970 }}  & \textcolor{red}{\textbf{0.934}} \\ \bottomrule
\end{tabular}}
\caption{
\textbf{Anomaly detection}: mean accuracy and AUC on MVTec AD produced by the SOTA and our method. Best result in \bred{red} and second best in \bblue{blue}.
}
\label{tab:auc_detection_mvtec}
\end{table*}
\subsection{Implementation Details}
We implement our framework using Pytorch~\cite{NEURIPS2019_9015}. The model was trained with Adam optimiser~\cite{kingma2014adam} using a learning rate of 0.0001, weight decay of , batch size of 64 images, 128 epochs for MNIST, Fashion MNIST and CIFAR 10, and 256 epochs for MVTec AD, Hyper-Kvasir and LAG. We defined the latent space to have  dimensions in~\eqref{eq:loss_GSVDD}.
Following~\cite{depthestimation2017, zhao2016loss}, we set  to balance the contribution of MAE and MS-SSIM losses in~\eqref{eq:global_reconstruction_loss} and~\eqref{eq:localisation_map}. 
We use Resnet18 and its reverse architecture as the encoder and decoder for both the global and local IGD models in Fig.~\ref{fig:multi-test}. 
When computing the accuracy of anomaly detection in MVTec AD, the threshold of the anomaly detection score  in~\eqref{eq:anomaly_score_image} (to classify an image as anomalous) is estimated using the mean loss on the validation set, composed of 150 normal training images.



We also apply self-supervised contrastive learning (SSL) pre-training for both our models.  Inspired by~\cite{golan2018deep}, we first apply rotation to augment the dataset by four folds (i.e., 0, 90, 180, 270 degrees). Then we use the InfoNCE loss in~\cite{chen2020simple} to contrast each data instance of the augmented dataset. In other words, our SSL can discriminate not only different image samples but also different rotation degrees.
For this SSL pre-training, we use the SGD optimiser with a learning rate of 0.01,  weight decay , batch size of 32, and 2,000 epochs. Once we obtain the pre-trained encoder with SSL, we remove the MLP layer and attach a linear layer to the backbone with fixed parameters. Note that this SSL is trained from scratch. 
In contrast to the vanilla self-supervised learning~\cite{chen2020simple} suggesting large batch size, we notice that a medium batch size yields significantly better performance for unsupervised anomaly detection. 
For the KD pre-training, we minimise the -2 norm between the 512-dimensional feature vector output from encoder and an intermediate layer of the ImageNet pre-trained ResNet18~\cite{resnet} with the same 512-dimensional features.
For this KD pre-training, we use the Adam optimiser with a learning rate of 0.0001,  weight decay , batch size of 64, and 50,000 iterations. Once we obtain the pre-trained encoder of KD, we fix the network parameters 
and attach a linear layer to reduce the dimensionality of the feature space to 128.







\subsection{Experiments on MNIST, Fashion MNIST and CIFAR10}




Table~\ref{tab:auc_detection} compares the unsupervised anomaly detection mean AUC results between our method and the current SOTA approaches in the field on MNIST, Fashion MNIST and CIFAR10.
The rows labelled as ``Scratch'' show results of models without using any pre-training, and the ones with ``SSL'' display results from models using self-supervised learning method~\cite{golan2018deep,bergman2020classification}. The ones with ``Imagenet'' show results from models that use Imagenet information through pre-training~\cite{venkataramanan2019attention} or KD~\cite{bergmann2020uninformed}. 
Our ``Scratch'' IGD 
outperforms all current SOTA methods on all three datasets without any pre-training (see Tab.~\ref{tab:auc_detection}-Scratch), surpassing the previous SOTA -VAE\textsubscript{u}~\cite{lamda-vae} by 5\% on FMNIST, 3\% on CIFAR10 and 0.5\% on MNIST in terms of the mean AUC. 
It is also interesting to note that our approach based on the proposed GSVDD obtains a 4.47\% and 26.44\% improvement on MNIST and CIFAR10, when compared with the original DSVDD~\cite{dsvdd}, suggesting the effectiveness of our Gaussian SVDD. 


Regarding pre-trained methods, in particular SSL approaches without using ImageNet information (see Tab.~\ref{tab:auc_detection}-SSL), our work achieves better results by a minimum 3\% mean AUC on CIFAR10 and 0.3\% mean AUC on FMNIST, compared with the current SOTA~\cite{bergman2020classification}.
Considering Imagenet pre-trained methods, CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} uses ImageNet~\cite{imagenet} and Celeb-A~\cite{liu2018large} pre-trained ResNet18 models for the encoder and discriminator, respectively, while ours uses a smaller network to distill the information from the ImageNet pre-trained ResNet18. We show that our model achieves the SOTA results on all three datasets when comparing with CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} and student-teacher network~\cite{bergmann2020uninformed}. 








\subsection{Experiments on MVTec AD}



We report the results for both anomaly detection and localisation on MVTec AD, which contains real-world images of industry objects and textures containing different types of anomalies. We also show the results based on SSL and Imagenet KD pre-trained models. 
The mean accuracy of anomaly detection is displayed in Tab.~\ref{tab:auc_detection_mvtec} over all image classes, where our KD pre-trained model outperforms the previous SOTA methods CAVGA-D\textsubscript{u} and CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} by 6\% and 2\%, respectively, and our SSL pre-trained model outperforms their approach by 7\% and 3\%, respectively. 
With KD pre-training, our model achieves the best accuracy results in ten categories of the MVTec AD. The shallow generative baselines, such as DAE, AE-SSIM and AnoGAN yield sub-optimal results on MVTec AD. When compared with methods recently considered to be the MVTec AD SOTA, such as LSA~\cite{lsa} and -VAE\textsubscript{u}~\cite{lamda-vae}, our approach shows more than 7\% improvement. 
We also show the AUC anomaly detection results in Tab.~\ref{tab:auc_detection_mvtec}, where our method, with SSL and ImageNet KD pre-training, surpasses all previous methods by at least 5.3\%, and produces the best results in eleven categories.  
In Tables~\ref{tab:auc_detection_mvtec} and~\ref{tab:auc_detection_mvtec}, we note that
self-supervised learning approaches generally do not work well without a powerful anomaly detector for real-world anomaly images, such as the ones on MVTec AD~\cite{bergman2020classification,golan2018deep}.
However, combining our IGD with the SSL pre-training, our model achieves the highest 93.4\% mean AUC and 85\% mean accuracy results and surpass the previous SOTA methods. 











\bgroup
\def\arraystretch{1}
\begin{table}
\centering
\small
\resizebox{0.45\linewidth}{!}{\begin{tabular}{@{}cc@{}}
\toprule
Method	                    &	MVTec AD      	\\	\midrule\midrule DAE~\cite{hadsell2006dimensionality}	    &	0.82        	\\	
AE\textsubscript{SSIM}~\cite{ae-ssim}	    &	0.87	        \\	
AVID~\cite{sabokrou2018adversarially}	    & 0.78		        \\
LSA~\cite{lsa}	    &	0.79	        \\ 
-VAE\textsubscript{u}~\cite{lamda-vae}	    &	0.86	        \\
AnoGAN~\cite{anogan}	                    &	0.74	        \\	ADVAE~\cite{ADVAE}	                    &	0.86	        \\
CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention}	&	0.85	        \\	
CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention}	&	0.89	        \\	
\textbf{Ours - KD}	            &	\textcolor{blue}{\textbf{0.91}}	\\
\textbf{Ours - SSL} & \textcolor{red}{\textbf{0.93}}	\\\bottomrule
\end{tabular}}
\caption{\textbf{Anomaly localisation:} mean pixel-level AUC results on the anomalous images of MVTec AD. Best result in \textcolor{red}{\textbf{red}} and second best in \textcolor{blue}{\textbf{blue}}.}
\vspace{-.18in}
\label{tab:MVTec-localisation-AUC}
\end{table}

For anomaly localisation, we compare our method and the SOTA using the mean pixel-level AUC of all anomalous images in the testing set of MVTec AD -- see Tab.~\ref{tab:MVTec-localisation-AUC}. Notice that our method with KD and SSL are better than the previous SOTA CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} by 2\% and 4\%, respectively.
It is also important to note that our architecture is lighter then CAVGA-R\textsubscript{u} given that it does not use an attention module and a visual discriminator.  Fig.~\ref{fig:mvtec_seg} shows anomaly localisation results on MVTec AD images, where red regions in the heatmap indicate higher anomaly probability. From this results, we can see that our approach can localise anomalous regions of different sizes and structures from different object categories.












\subsection{Experiments on Medical Datasets}

To show that our method can generalise to other domains, we evaluate our approach on two public medical datasets - Hyper-Kvasir for polyp detection and LAG for glaucoma detection. As shown in Tab.~\ref{tab:medical_auc}, our
SSL and KD based results achieve the best AUC results on both datasets. Our methods surpass the recent proposed CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} on both datasets by a minimum 0.9\% and maximum 3.8\%. Also, our model  performs better compared to the anomaly detector specifically designed for medical data, such as f-anogan~\cite{f-anogan} and ADGAN~\cite{liu2019photoshopping}. The abnormalities in medical data (i.e., colon polyps, glaucoma) are significantly different than the synthetic data and MVTec AD in terms of appearance and structural anomalies, suggesting that our model works in disparate domains.

\begin{table}[h]
\centering
\scalebox{0.85}{
\begin{tabular}{@{}ccc@{}}
\toprule 
Methods         & Hyper-Kvasir  & LAG  \\ \hline\hline
DAE~\cite{masci2011stacked}             & 0.705      & 0.651   \\
CAM~\cite{cam}                &           -      &   0.663      \\
GBP~\cite{springenberg2014striving}                &   -             & 0.787         \\
SmoothGrad~\cite{smilkov2017smoothgrad}                &  -               & 0.795  \\
OCGAN~\cite{perera2019ocgan}           & 0.813     &  0.737 \\
F-anoGAN~\cite{f-anogan}        & 0.907       & 0.778  \\
ADGAN~\cite{liuphotoshopping}           & 0.913      &   0.752    \\
CAVGA-~\cite{venkataramanan2019attention}     & 0.928    & 0.819       \\
\textbf{Ours - KD}      & \textcolor{blue}{\textbf{0.931}}        & \textcolor{blue}{\textbf{0.838}}   \\
\textbf{Ours - SSL}    & \textcolor{red}{\textbf{0.937}}        & \textcolor{red}{\textbf{0.857}}   \\\bottomrule
\end{tabular}
}
\caption{\textbf{Anomaly detection:} AUC results on two medical datasets: Hyper-Kvasir and LAG. Best result in \textcolor{red}{\textbf{red}} and second best in \textcolor{blue}{\textbf{blue}}.}
\label{tab:medical_auc}
\end{table}

\subsection{Ablation Study}


To investigate the effectiveness of each component of our method, we show the mean AUC results of our method and its ablated variants on CIFAR10 in Tab.~\ref{tab:ablation}.
The study starts with a model trained with the MS-SSIM reconstruction loss. This model  achieves a mean AUC of 68.52\%, which is higher than the 55.38\% AUC from DAE model~\cite{hadsell2006dimensionality} that relies on MSE as the reconstruction loss (see Tab.~\ref{tab:auc_detection}). 
ImageNet KD pre-training (PT) then boosts the results by 4.6\%.
To show the importance of our proposed GSVDD, DSVDD improves MS-SSIM and KD by 6.4\%, while GSVDD improves by 8.8\%. 
Our proposed GSVDD provides larger improvement than KD pre-training. Adding ACAI interpolation (INTER) further improves the AUC by 1.7\%. In addition to the results shown in Tab.~\ref{tab:ablation}, we also compare the results based on different reconstruction loss functions, namely: MS-SSIM, SSIM and MSE. MS-SSIM improves SSIM by 3.3\% AUC and MSE by 4.9\%AUC on CIFAR10. 

\begin{table}[H]
\centering
\scalebox{0.82}{
\begin{tabular}{ccccc|c}
\toprule MS-SSIM 	&	PT &	DSVDD	&	GSVDD	&	INTER	&	AUC - CIFAR10	\\ \midrule\midrule \checkmark	&		&	&	&		&			0.685	\\
\checkmark	&	\checkmark &			&		&		&	0.731	\\
\checkmark	&	\checkmark &		\checkmark	&		&		&	0.795	\\
\checkmark	&	\checkmark	& 		&	\checkmark	&		&	0.819	\\
\checkmark	&	\checkmark	& 	&	\checkmark	&	\checkmark	&	0.836	\\ \hline\bottomrule
\end{tabular}}
\caption{Ablation study of our method on CIFAR10 using anomaly detection mean AUC (MS-SSIM: the backbone deep autoencoder with MS-SSIM loss, PT: knowledge distillation pre-training, DSVDD: Deep SVDD, GSVDD: our Gaussian SVDD, INTER: ACAI interpolation).} \label{tab:ablation}
\end{table}


























\section{Conclusion}

In this paper, we presented a new unsupervised anomaly detection and localisation method which learns a distribution of normal images that generalises well to all classes of normal images, a new criterion to detect and localise structural and non-structural multi-scale anomalies, and a new training that combines classification and reconstruction losses.
Our learned distribution of normal images is based on the interpolated Gaussian descriptor (IGD) that provides a robust modelling of the normal image distribution with adversarially interpolated descriptors to regularise and facilitate the training of the proposed GSVDD.  
The proposed reconstruction loss can find structural and non-structural anomalies of varying sizes.
The IGD and reconstruction losses are then jointly optimised to produce a global and a local model that achieves the best performance in the field on MNIST, CIFAR10, Fashion MNIST, MVTec AD and two large scale medical datasets in terms of anomaly detection and localisation. 
We plan to study the use of GSVDD in the pixel-wise localisation of anomalies and the investigation of new self-supervised learning approaches specifically designed for anomaly detection.













\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
