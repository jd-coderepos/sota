

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[camera]{cvpr}      

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{abstract}
\usepackage{multirow}
\setlength{\abovecaptionskip}{3pt plus 2pt minus 1pt} \newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
 

\def\cvprPaperID{8836} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{\textcolor[rgb]{0.965,0.325,0.078}{N}\textcolor[rgb]{0.486,0.733,0}{Ü}\textcolor[rgb]{0,0.631,0.945}{W}\textcolor[rgb]{1,0.733,0}{A}: Visual Synthesis Pre-training for \textcolor[rgb]{0.965,0.325,0.078}{N}eural vis\textcolor[rgb]{0.486,0.733,0}{U}al \textcolor[rgb]{0,0.631,0.945}{W}orld cre\textcolor[rgb]{1,0.733,0}{A}tion}

\author{Chenfei Wu\thanks{Both authors contributed equally to this research.} \quad Jian Liang\samethanks[1] \quad Lei Ji \quad  Fan Yang \quad Yuejian Fang \quad Daxin Jiang \quad Nan Duan\thanks{Corresponding author.} \\
 {\small Microsoft Research Asia \quad Peking University} \\
{\tt\small\{chewu,leiji,fanyang,djiang,nanduan\}@microsoft.com}~~ {\tt\small\{j.liang@stu,fangyj@ss\}.pku.edu.cn}}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.85\textwidth]{imgs/Overview.pdf}
    \captionof{figure}{Examples of 8 typical visual generation and manipulation tasks supported by the NÜWA model.}
    \label{fig:Overview}
    
\end{center}
}]
\saythanks
\begin{abstract}

This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is \url{https://github.com/microsoft/NUWA}.

\end{abstract}

\section{Introduction}\label{sec:intro}

Nowadays, the Web is becoming more visual than ever before, as images and videos have become the new information carriers and have been used in many practical applications. With this background, visual synthesis is becoming a more and more popular research topic, which aims to build models that can generate new or manipulate existing visual data (i.e., images and videos) for various visual scenarios.

Auto-regressive models\cite{vanoordPixelRecurrentNeural2016,oordConditionalImageGeneration2016,wuGODIVAGeneratingOpenDomaIn2021,rameshZeroShotTexttoImageGeneration2021} play an important role in visual synthesis tasks, due to their explicit density modeling and stable training advantages compared with GANs\cite{brockLargeScaleGAN2019,tulyakovMocoganDecomposingMotion2018,xuAttnganFinegrainedText2018,radfordUnsupervisedRepresentationLearning2015}. 
Earlier visual auto-regressive models, such as PixelCNN\cite{oordConditionalImageGeneration2016}, PixelRNN\cite{vanoordPixelRecurrentNeural2016}, Image Transformer\cite{parmarImageTransformer2018}, iGPT\cite{chenGenerativepretrainingPixels2020}, and Video Transformer\cite{weissenbornScalingAutoregressiveVideo2020}, performed visual synthesis in a ``pixel-by-pixel'' manner.
However, due to their high computational cost on high-dimensional visual data, such methods can be applied to low-resolution images or videos only and are hard to scale up.



Recently, with the arise of VQ-VAE\cite{oordNeuralDiscreteRepresentation2017} as a discrete visual tokenization approach, efficient and large-scale pre-training can be applied to visual synthesis tasks for images (e.g., DALL-E\cite{rameshZeroShotTexttoImageGeneration2021} and CogView\cite{dingCogViewMasteringTexttoImage2021}) and videos (e.g., GODIVA\cite{wuGODIVAGeneratingOpenDomaIn2021}). Although achieving great success, such solutions still have limitations -- they treat images and videos separately and focus on generating either of them. This limits the models to benefit from both image and video data.

In this paper, we present NÜWA, a unified multimodal pre-trained model that aims to support visual synthesis tasks for both images and videos, and conduct experiments on 8 downstream visual synthesis, as shown in Fig.~\ref{fig:Overview}.
The main contributions of this work are three-fold:

\begin{itemize}
    \item We propose NÜWA, a general 3D transformer encoder-decoder framework, which covers language, image, and video at the same time for different visual synthesis tasks. It consists of an adaptive encoder that takes either text or visual sketch as input, and a decoder shared by 8 visual synthesis tasks.
    \item We propose a 3D Nearby Attention (3DNA) mechanism in the framework to consider the locality characteristic for both spatial and temporal axes. 3DNA not only reduces computational complexity but also improves the visual quality of the generated results.
    
    \item Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, NÜWA shows surprisingly good zero-shot capabilities not only on text-guided image manipulation, but also text-guided video manipulation.
    
\end{itemize}





\section{Related Works}\label{sec:rw}


\subsection{Visual Auto-Regressive Models}

The method proposed in this paper follows the line of visual synthesis research based on auto-regressive models. Earlier visual auto-regressive models \cite{oordConditionalImageGeneration2016, vanoordPixelRecurrentNeural2016, parmarImageTransformer2018, chenGenerativepretrainingPixels2020, weissenbornScalingAutoregressiveVideo2020} performed visual synthesis in a ``pixel-by-pixel'' manner.
However, due to the high computational cost when modeling high-dimensional data, such methods can be applied to low-resolution images or videos only, and are hard to scale up.

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{imgs/Pipeline_2.pdf}
	\caption{Overview structure of NÜWA. It contains an adaptive encoder supporting different conditions and a pre-trained decoder benefiting from both image and video data. For image completion, video prediction, image manipulation, and video manipulation tasks, the input partial images or videos are fed to the decoder directly.}
	\label{fig:Pipeline}
\end{figure*}
Recently, VQ-VAE-based~\cite{oordNeuralDiscreteRepresentation2017} visual auto-regressive models were proposed for visual synthesis tasks. By converting images into discrete visual tokens, such methods can conduct efficient and large-scale pre-training for text-to-image generation (e.g., DALL-E\cite{rameshZeroShotTexttoImageGeneration2021} and CogView\cite{dingCogViewMasteringTexttoImage2021}), text-to-video generation (e.g., GODIVA\cite{wuGODIVAGeneratingOpenDomaIn2021}), and video prediction (e.g., LVT\cite{rakhimovLatentVideoTransformer2020} and VideoGPT\cite{yanVideoGPTVideoGeneration2021}), with higher resolution of generated images or videos. 
However, none of these models was trained by images and videos together. But it is intuitive that these tasks can benefit from both types of visual data. 

Compared to these works, NÜWA is a unified auto-regressive visual synthesis model that is pre-trained by the visual data covering both images and videos and can support various downstream tasks. We also verify the effectiveness of different pretraining tasks in Sec.~\ref{sec:abl}. Besides, VQ-GAN\cite{esserTamingTransformersHighResolution2021} instead of VQ-VAE is used in NÜWA for visual tokenization, which, based on our experiment, can lead to better generation quality.


\subsection{Visual Sparse Self-Attention}

How to deal with the quadratic complexity issue brought by self-attention is another challenge, especially for tasks like high-resolution image synthesis or video synthesis.

Similar to NLP, sparse attention mechanisms have been explored to alleviate this issue for visual synthesis.
\cite{weissenbornScalingAutoregressiveVideo2020, rakhimovLatentVideoTransformer2020} split the visual data into different parts (or blocks) and then performed block-wise sparse attention for the synthesis tasks. 
However, such methods dealt with different blocks separately and did not model their relationships.
\cite{hoAxialAttentionMultidimensional2019, rameshZeroShotTexttoImageGeneration2021, wuGODIVAGeneratingOpenDomaIn2021} proposed to use axial-wise sparse attention in visual synthesis tasks, which conducts sparse attention along the axes of visual data representations. This mechanism makes training very efficient and is friendly to large-scale pre-trained models like DALL-E\cite{rameshZeroShotTexttoImageGeneration2021}, CogView\cite{dingCogViewMasteringTexttoImage2021}, and GODIVA\cite{wuGODIVAGeneratingOpenDomaIn2021}. However, the quality of generated visual contents could be harmed due to the limited contexts used in self-attention.
\cite{parmarImageTransformer2018, ramachandranStandaloneSelfattentionVision2019, childGeneratingLongSequences2019} proposed to use local-wise sparse attention in visual synthesis tasks, which allows the models to see more contexts. But these works were for images only.

Compared to these works, NÜWA proposes a 3D nearby attention that extends the local-wise sparse attention to cover both images to videos. We also verify that local-wise sparse attention is superior to axial-wise sparse attention for visual generation in Sec.~\ref{sec:abl}.

 








\section{Method}
\subsection{3D Data Representation} \label{sec:rep}
To cover all texts, images, and videos or their sketches, we view all of them as tokens and define a unified 3D notation , where  and  denote the number of tokens in the spatial axis (height and width respectively),  denotes the number of tokens in the temporal axis, and  is the dimension of each token. In the following, we introduce how we get this unified representation for different modalities.

Texts are naturally discrete, and following Transformer\cite{vaswaniAttentionAllYou2017}, we use a lower-cased byte pair encoding (BPE) to tokenize and embed them into . We use placeholder 1 because the text has no spatial dimension.

Images are naturally continuous pixels. Input a raw image  with height , width  and channel ,  VQ-VAE\cite{oordNeuralDiscreteRepresentation2017} trains a learnable codebook to build a bridge between raw continuous pixels and discrete tokens, as denoted in Eq.~(\ref{eq:zi})(\ref{eq:I}):


where  is an encoder that encodes  into  grid features ,  is a learnable codebook with  visual tokens, where each grid of  is searched to find the nearest token. The searched result  are embedded by  and reconstructed back to  by a decoder . The training loss of VQ-VAE can be written as Eq.~(\ref{eq:lv}):

where  strictly constraints the exact pixel match between  and , which limits the generalization ability of the model. Recently, VQ-GAN\cite{esserTamingTransformersHighResolution2021} enhanced VQ-VAE training by adding a perceptual loss and a GAN loss to ease the exact constraints between  and  and focus on high-level semantic matching, as denoted in Eq.~(\ref{eq:lp})(\ref{eq:lg}):


After the training of VQ-GAN,  is finally used as the representation of images. We use placeholder 1 since images have no temporal dimensions.

Videos can be viewed as a temporal extension of images, and recent works like VideoGPT\cite{yanVideoGPTVideoGeneration2021} and VideoGen\cite{zhangVideoGenGenerativeModeling2020} extend convolutions in the VQ-VAE encoder from 2D to 3D and train a video-specific representation. However, this fails to share a common codebook for both images and videos. In this paper, we show that simply using 2D VQ-GAN to encode each frame of a video can also generate temporal consistency videos and at the same time benefit from both image and video data. The resulting representation is denoted as , where  denotes the number of frames.

For image sketches, we consider them as images with special channels. An image segmentation matrix  with each value representing the class of a pixel can be viewed in a one-hot manner  where  is the number of segmentation classes. By training an additional VQ-GAN for image sketch, we finally get the embedded image representation . Similarly, for video sketches, the representation is . 


\subsection{3D Nearby Self-Attention}\label{sec:nearby}
In this section, we define a unified 3D Nearby Self-Attention (3DNA) module based on the previous 3D data representations, supporting both self-attention and cross-attention. We first give the definition of 3DNA in Eq.~(\ref{eq:Y}), and introduce detailed implementation in Eq.~(\ref{eq:nijk})(\ref{eq:yijk}):

where both  and  are 3D representations introduced in Sec.~\ref{sec:rep}. If , 3DNA denotes the self-attention on target  and if  , 3DNA is cross-attention on target  conditioned on .  denotes learnable weights.

We start to introduce 3DNA from a coordinate  under . By a linear projection, the corresponding coordinate  under  is . Then, the local neighborhood around  with a width, height and temporal extent  is defined in Eq.~(\ref{eq:nijk}),
\begin{small}

\end{small}
where  is a sub-tensor of condition  and consists of the corresponding nearby information that  needs to attend. With three learnable weights , the output tensor for the position  is denoted in Eq.~(\ref{eq:qijk})(\ref{eq:yijk}):
  
where the  position queries and collects corresponding nearby information in . This also handles , then  just queries the nearby position of itself. 3NDA not only reduces the complexity of full attention from  to ,  but also shows superior performance and we discuss it in Sec.~\ref{sec:abl}.

\subsection{3D Encoder-Decoder} \label{sec:ed}
In this section, we introduce 3D encode-decoder built based on 3DNA. To generate a target  under the condition of , the positional encoding for both  and  are updated by three different learnable vocabularies considering height, width, and temporal axis, respectively in Eq.~(\ref{eq:xijk})(\ref{eq:cijk}): 
  
Then, the condition  is fed into an encoder with a stack of  3DNA layers to model the self-attention interactions, with the th layer denoted in Eq.~(\ref{eq:encoder}):

Similarly, the decoder is also a stack of  3DNA layers. The decoder calculates both self-attention of generated results and cross-attention between generated results and conditions. The th layer is denoted in Eq.~(\ref{eq:decoder}).

where  denote the generated tokens for now. The initial token  is a special  token learned during the training phase.
\subsection{Training Objective}\label{sec:training}
We train our model on three tasks, Text-to-Image (T2I), Video Prediction (V2V) and Text-to-Video (T2V). The training objective for the three tasks are cross-entropys denoted as three parts in Eq.~(\ref{eq:loss}), respectively:

For T2I and T2V tasks,  denotes text conditions. For the V2V task, since there is no text input, we instead get a constant 3D representation  of the special word ``None''.  denotes the model parameters.


\section{Experiments} \label{sec:exp}
Based on Sec.~\ref{sec:training} we first pre-train NÜWA on three datasets: Conceptual Captions\cite{linMicrosoftCocoCommon2014} for text-to-image (T2I) generation, which includes 2.9M text-image pairs, 
Moments in Time\cite{monfortMomentsTimeDataset2019} for video prediction (V2V), which includes 727K videos, 
and VATEX dataset\cite{wangVatexLargescaleHighquality2019} for text-to-video (T2V) generation, which includes 241K text-video pairs. In the following, we first introduce implementation details in Sec.~\ref{sec:imp} and then compare NÜWA with state-of-the-art models in Sec.~\ref{sec:cmp}, and finally conduct ablation studies in Sec.~\ref{sec:abl} to study the impacts of different parts.







\subsection{Implementation Details} \label{sec:imp}
In Sec.~\ref{sec:rep}, we set the sizes of 3D representations for text, image, and video as follows.
For text, the size of 3D representation is . 
For image, the size of 3D representation is .
For video, the size of 3D representation is , where we sample 10 frames from a video with 2.5 fps. Although the default visual resolution is , we pre-train different resolutions for a fair comparison with existing models.
For the VQ-GAN model used for both images and videos, the size of grid feature  in Eq.~(\ref{eq:zi}) is , and the size of the codebook  is .

Different sparse extents are used for different modalities in Sec.~\ref{sec:nearby}. 
For text, we set , where  denotes that the full text is always used in attention. 
For image and image sketches, . 
For video and video sketches, . 

We pre-train on 64 A100 GPUs for two weeks with the layer  in Eq.~(\ref{eq:encoder}) set to 24, an Adam~\cite{Kingma_Adammethodstochastic_2014} optimizer with a learning rate of 1e-3, a batch size of 128, and warm-up 5\% of a total of 50M steps. The final pre-trained model has a total number of 870M parameters.



\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{imgs/T2I_2.pdf}
	\caption{Qualitative comparison with state-of-the-art models for Text-to-Image (T2I) task on MSCOCO dataset.}
	\label{fig:T2I}
	\vspace{-2mm}
\end{figure*}
\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{imgs/T2V_4.pdf}
	\caption{Quantitative comparison with state-of-the-art models for Text-to-Video (T2V) task on Kinetics dataset.}
	\label{fig:T2V_3}
	\vspace{-4mm}
\end{figure*}


\subsection{Comparison with state-of-the-art} \label{sec:cmp}
\vspace{-4mm}
\newcommand{\bwidth}{0.7cm}
\begin{table}[t]
\footnotesize
\caption{Qualitative comparison with the state-of-the-art models for Text-to-Image (T2I) task on the MSCOCO (256×256) dataset.}
\label{tab:t2i}
\tabcolsep=0.06cm
\begin{tabular}{p{1.9cm}p{0.9cm}p{\bwidth}p{\bwidth}p{\bwidth}p{0.7cm}p{\bwidth}p{1.1cm}}
\toprule
Model              & FID-0↓      & FID-1       & FID-2         & FID-4         & FID-8         & IS↑           & CLIPSIM↑ \\
\midrule
AttnGAN\cite{xuAttnganFinegrainedText2018}     & 35.2        & 44.0          & 72.0            & 108.0           & 100.0           & 23.3          & 0.2772        \\
DM-GAN\cite{zhuDmganDynamicMemory2019}     & 26.0          & 39.0          & 73.0            & 119.0           & 112.3         & \textbf{32.2} & 0.2838        \\
DF-GAN\cite{taoDfganDeepFusion2020}     & 26.0          & 33.8        & 55.9          & 91.0            & 97.0            & 18.7          & 0.2928        \\
  DALL-E\cite{rameshZeroShotTexttoImageGeneration2021}  & 27.5        & 28.0          & 45.5          & 83.5          & 85.0            & 17.9          & -        \\
CogView\cite{dingCogViewMasteringTexttoImage2021} & 27.1        & 19.4        & \textbf{13.9} & 19.4 & \textbf{23.6} & 18.2          & 0.3325        \\
XMC-GAN\cite{zhangCrossmodalContrastiveLearning2021} & \textbf{9.3}        & -        & - & -& - & 30.5          & -        \\

\midrule
NÜWA               & 12.9 & \textbf{13.8} & 15.7          & \textbf{19.3} & 24          & 27.2          & \textbf{0.3429}   \\
\bottomrule
\end{tabular}
\label{tab:metric}
\vspace{-3mm}
\end{table}

\newcommand{\swidth}{1cm}
\begin{table}[t]
\footnotesize
\begin{center}
\caption{Quantitative comparison with state-of-the-art models for Text-to-Video (T2V) task on Kinetics dataset.}
\label{tab:t2v}
\tabcolsep=0.11cm
\begin{tabular}{p{2.7cm}p{0.7cm}p{1.2cm}p{1.2cm}p{1.3cm}}
\toprule
Model             & Acc↑     & FID-img↓      & FID-vid↓    & CLIPSIM↑ \\
\midrule
T2V  (64×64) \cite{liVideoGenerationText2018}       & 42.6     & 82.13     & 14.65       & 0.2853        \\
SC  (128×128) \cite{balajiConditionalGANDiscriminative2019}        & 74.7     & 33.51     & 7.34        & 0.2915        \\
TFGAN (128×128)\cite{balajiConditionalGANDiscriminative2019}     & 76.2     & 31.76     & 7.19        & 0.2961        \\
\midrule
NÜWA (128×128)     & \textbf{77.9}     & \textbf{28.46}     & \textbf{7.05}         & \textbf{0.3012}        \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-6mm}
\end{table}
\begin{table}[t]
\footnotesize
\begin{center}
\caption{Quantitative comparison with state-of-the-art models for Video Prediction (V2V) task on BAIR (64×64) dataset.}
\label{tab:v2v}
\begin{tabular}{p{4.6cm}p{1.3cm}p{1.3cm}}
\toprule
Model                            & Cond. & FVD↓  \\
\midrule
MoCoGAN\cite{tulyakovMocoganDecomposingMotion2018}                 & 4     & 503   \\
SVG-FP\cite{dentonStochasticVideoGeneration2018}                   & 2     & 315   \\
CNDA\cite{finnUnsupervisedLearningPhysical2016a}                     & 2     & 297   \\
SV2P\cite{babaeizadehStochasticVariationalVideo2017a}                     & 2     & 263   \\
SRVP\cite{franceschiStochasticLatentResidual2020}                  & 2     & 181   \\
VideoFlow\cite{kumarVideoflowConditionalFlowbased2019}                & 3     & 131   \\
LVT\cite{rakhimovLatentVideoTransformer2020}                      & 1     & 126±3 \\
SAVP\cite{leeStochasticAdversarialVideo2018} &2 & 116 \\
DVD-GAN-FP\cite{clarkWhatDoesBert2019}   & 1     & 110   \\
Video   Transformer (S)\cite{weissenbornScalingAutoregressiveVideo2020}  & 1     & 106±3 \\
TriVD-GAN-FP\cite{lucTransformationbasedAdversarialVideo2020}             & 1     & 103   \\
CCVS\cite{moingCCVSContextawareControllable2021}                  & 1     & 99±2  \\
Video   Transformer (L)\cite{weissenbornScalingAutoregressiveVideo2020}  & 1     & 94±2  \\
\midrule
NÜWA                             & 1     & \textbf{86.9}  \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{imgs/S2I_2.pdf}
	\caption{Quantitative comparison with state-of-the-art models for Sketch-to-Image (S2I) task on MSCOCO stuff dataset.}
	\label{fig:S2I}
	\vspace{-4mm}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{imgs/I2I.pdf}
	\caption{Qualitative comparison with the state-of-the-art model for Image Completion (I2I) task in a zero-shot manner.}
	\label{fig:I2I}
	\vspace{-5mm}
\end{figure}



\textbf{Text-to-Image (T2I) fine-tuning}: We compare NÜWA on the MSCOCO\cite{linMicrosoftCocoCommon2014} dataset quantitatively in Tab.~\ref{tab:t2i} and qualitatively in Fig.~\ref{fig:T2I}. Following DALL-E\cite{rameshZeroShotTexttoImageGeneration2021}, we use  blurred FID score (FID-) and Inception Score (IS)\cite{salimansImprovedTechniquesTraining2016} to evaluate the quality and variety respectively, and following GODIVA\cite{wuGODIVAGeneratingOpenDomaIn2021}, we use CLIPSIM metric, which incorporates a CLIP\cite{radfordLearningTransferableVisual2021} model to calculate the semantic similarity between input text and the generated image. For a fair comparison, all the models use the resolution of . We generate 60 images for each text and select the best one by CLIP\cite{radfordLearningTransferableVisual2021}. In Tab.~\ref{tab:t2i}, NÜWA significantly outperforms CogView\cite{dingCogViewMasteringTexttoImage2021} with FID-0 of 12.9 and CLIPSIM of 0.3429. Although XMC-GAN\cite{zhangCrossmodalContrastiveLearning2021} reports a significant FID score of 9.3, we find NÜWA generates more realistic images compared with the exact same samples in XMC-GAN's paper (see Fig.~\ref{fig:T2I}). Especially in the last example, the boy's face is clear and the balloons are correctly generated. 







\textbf{Text-to-Video (T2V) fine-tuning}: We compare NÜWA on the Kinetics\cite{kayKineticsHumanAction2017} dataset quantitatively in Tab.~\ref{tab:t2v} and qualitatively in Fig.~\ref{fig:T2V_3}. Following TFGAN\cite{balajiConditionalGANDiscriminative2019}, we evaluate the visual quality on FID-img and FID-vid metrics and semantic consistency on the accuracy of the label of generated video. As shown in Tab.~\ref{tab:t2v}, NÜWA achieves the best performance on all the above metrics. In Fig.~\ref{fig:T2V_3}, we also show the strong zero-shot ability for generating unseen text, such as “playing golf at swimming pool” or “running on the sea”.

\textbf{Video Prediction (V2V) fine-tuning}: We compare NÜWA on BAIR Robot Pushing\cite{ebertSelfSupervisedVisualPlanning2017} dataset quantitatively in Tab.~\ref{tab:v2v}. Cond. denotes the number of frames given to predict future frames. For a fair comparison, all the models use 64×64 resolutions. Although given only one frame as condition (Cond.), NÜWA still significantly pushes the state-of-the-art FVD\cite{unterthinerAccurateGenerativeModels2018} score from 94±2 to 86.9.



\textbf{Sketch-to-Image (S2I) fine-tuning}: We compare NÜWA on MSCOCO stuff\cite{linMicrosoftCocoCommon2014} qualitatively in Fig.~\ref{fig:S2I}. NÜWA generates realistic buses of great varieties compared with Taming-Transformers\cite{esserTamingTransformersHighResolution2021} and SPADE\cite{parkSemanticImageSynthesis2019}. Even the reflection of the bus window is clearly visible.

\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{imgs/TI2I.pdf}
	\caption{Quantitative comparison with state-of-the-art models for text-guided image manipulation (TI2I) in a zero-shot manner.}
	\label{fig:TI2I}
	\vspace{-2mm}
\end{figure}

\textbf{Image Completion (I2I) zero-shot evaluation}: We compare NÜWA in a zero-shot manner qualitatively in Fig.~\ref{fig:I2I}. Given the top half of the tower, compared with Taming Transformers\cite{esserTamingTransformersHighResolution2021}, NÜWA shows richer imagination of what could be for the lower half of the tower, including buildings, lakes, flowers, grass, trees, mountains, etc.



\textbf{Text-Guided Image Manipulation (TI2I) zero-shot evaluation}: We compare NÜWA in a zero-shot manner qualitatively in Fig.~\ref{fig:TI2I}. Compared with Paint By Word \cite{bauPaintWord2021}, NÜWA shows strong manipulation ability, generating high-quality text-consistent results while not changing other parts of the image. For example, in the third row, the blue firetruck generated by NÜWA is more realistic, while the behind buildings show no change. This is benefited from real-world visual patterns learned by multi-task pre-training on various visual tasks. Another advantage is the inference speed of NÜWA, practically 50 seconds to generate an image, while Paint By Words requires additional training during inference, and takes about 300 seconds to converge.


\textbf{Sketch-to-Video (S2V) fine-tuning} and \textbf{Text-Guided Video Manipulation (TV2V) zero-shot evaluation}: As far as we know, open-domain S2V and TV2V are tasks first proposed in this paper. Since there is no comparison, we instead arrange them in Ablation Study in Section~\ref{sec:abl}.

More detailed comparisons, samples, including human evaluations, are provided in the appendix.

\subsection{Ablation Study}\label{sec:abl}




The above part of Tab.~\ref{tab:VQ-GAN} shows the effectiveness of different VQ-VAE (VQ-GAN) settings. We experiment on ImageNet\cite{russakovskyImageNetLargeScale2015} and OpenImages\cite{kuznetsovaOpenImagesDataset2020}.  denotes raw resolution,  denotes the number of discrete tokens. The compression rate is denoted as , where  is the quotient of  divided by . Comparing the first two rows in Tab.~\ref{tab:VQ-GAN}, VQ-GAN shows significantly better Fréchet Inception Distance (FID)\cite{heuselGansTrainedTwo2017} and Structural Similarity Matrix (SSIM) scores than VQ-VAE. Comparing Row 2-3, we find that the number of discrete tokens is the key factor leading to higher visual quality instead of compress rate. Although Row 2 and Row 4 have the same compression rate F16, they have different FID scores of 6.04 and 4.79. So what matters is not only how much we compress the original image, but also how many discrete tokens are used for representing an image. This is in line with cognitive logic, it’s too ambiguous to represent human faces with just one token. And practically, we find that  discrete tokens usually lead to poor performance, especially for human faces, and  tokens show the best performance. However, more discrete tokens mean more computing, especially for videos. We finally use a trade-off version for our pre-training:  tokens. By training on the Open Images dataset, we further improve the FID score of the  version from 4.79 to 4.31.




\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{imgs/VQGANSketch_2.pdf}
	\caption{Reconstruction samples of VQ-GAN and VQ-GAN-Seg.}
	\label{fig:VQ-GANSketch}
	\vspace{-2mm}
\end{figure}


\begin{table}[t]
\footnotesize
\begin{center}
\caption{Effectiveness of different VQ-VAE (VQ-GAN) settings.}
\label{tab:VQ-GAN}
\tabcolsep=0.1cm
\begin{tabular}{p{1.8cm}p{1.5cm}p{1.8cm}p{0.5cm}p{0.8cm}p{0.8cm}}
\toprule
Model     & Dataset    &  & Rate                    & SSIM   & FID   \\
\midrule
VQ-VAE     & ImageNet   &    &   F16                   & 0.7026 & 13.3 \\
VQ-GAN     & ImageNet   &    &   F16                   & 0.7105 & 6.04  \\
VQ-GAN     & ImageNet   &    &   F8                   & 0.8285 & 2.03  \\
VQ-GAN     & ImageNet   &    &    F16                 & 0.7213 & 4.79  \\
VQ-GAN     & OpenImages &    &    F16                  & 0.7527 & 4.31  \\
\midrule
Model     & Dataset    &  & Rate   & PA     & FWIoU \\
\midrule
VQ-GAN-Seg & MSCOCO     &    &  F16                  & 96.82  & 93.91 \\
VQ-GAN-Seg & VSPW       &    &  F16                 & 95.36  & 91.82 \\
\bottomrule
\end{tabular}
\end{center}
	\vspace{-2mm}
\end{table}




The below part of Tab.~\ref{tab:VQ-GAN} shows the performance of VQ-GAN for sketches. VQ-GAN-Seg on MSCOCO\cite{linMicrosoftCocoCommon2014} is trained for Sketch-to-Image (S2I) task and VQ-GAN-Seg on VSPW\cite{miaoVSPWLargescaleDataset2021} is trained for Sketch-to-Video (S2V) task. All the above backbone shows good performance in Pixel Accuracy (PA) and Frequency Weighted Intersection over Union (FWIoU), which shows a good quality of 3D sketch representation used in our model. Fig.~\ref{fig:VQ-GANSketch} also shows some reconstructed samples of 336×336 images and sketches.



Tab.~\ref{tab:pipe} shows the effectiveness of multi-task pre-training for the Text-to-Video (T2V) generation task. We study on a challenging dataset, MSR-VTT\cite{xuMsrvttLargeVideo2016}, with natural descriptions and real-world videos. Compared with training only on a single T2V task (Row 1), training on both T2V and T2I (Row 2) improves the CLIPSIM from 0.2314 to 0.2379. This is because T2I helps to build a connection between text and image, and thus helpful for the semantic consistency of the T2V task. In contrast, training on both T2V and V2V (Row 3) improves the FVD score from 52.98 to 51.81. This is because V2V helps to learn a common unconditional video pattern, and is thus helpful for the visual quality of the T2V task. As a default setting of NÜWA, training on all three tasks achieves the best performance.



Tab.~\ref{tab:sparse} shows the effectiveness of 3D nearby attention for the Sketch-to-Video (S2V) task on the VSPW\cite{miaoVSPWLargescaleDataset2021} dataset. We study on the S2V task because both the encoder and decoder of this task are fed with 3D video data. To evaluate the semantic consistency for S2V, we propose a new metric called Detected PA, which uses a semantic segmentation model\cite{yuanObjectcontextualRepresentationsSemantic2020} to segment each frame of the generated video and then calculate the pixel accuracy between the generated segments and input video sketch. The default NÜWA setting in the last row, with both nearby encoder and nearby decoder, achieves the best FID-vid and Detected PA. The performance drops if either encoder or decoder is replaced by full attention, showing that focusing on nearby conditions and nearby generated results is better than simply considering all the information.
We compare nearby-sparse and axial-sparse in two-folds. Firstly, the computational complexity of nearby-sparse is  and axis-sparse attention is . For generating long videos (larger ), nearby-sparse will be more computational efficient. Secondly, nearby-sparse has better performance than axis-sparse in visual generation task, which is because nearby-sparse attends to ``nearby'' locations containing interactions between both spatial and temporal axes, while axis-sparse handles different axis separately and only consider interactions on the same axis. 


\begin{table}[t]
\footnotesize
\begin{center}
\caption{Effectiveness of multi-task pre-training for Text-to-Video (T2V) generation task on MSRVTT dataset.}
\label{tab:pipe}
\begin{tabular}{p{2cm}p{2cm}p{1.5cm}p{1.2cm}}
\toprule
Model  & Pre-trained Tasks & FID-vid↓  & CLIPSIM↑ \\
\midrule
NÜWA-TV   & T2V    &  52.98   &      0.2314      \\
NÜWA-TV-TI  & T2V+T2I    &  53.92   &  0.2379     \\    
NÜWA-TV-VV   & T2V+V2V    &  51.81   &  0.2335     \\    
NÜWA   & T2V+T2I+V2V   &    \textbf{47.68} &  \textbf{0.2439}  \\    
\bottomrule
\end{tabular}
\end{center}
\vspace{-6mm}
\end{table}
\begin{table}[t]
\footnotesize
\begin{center}
\caption{Effectiveness of 3D nearby attention for Sketch-to-Video (S2V) task on VSPW dataset.}
\label{tab:sparse}
\begin{tabular}{p{1.5cm}p{0.9cm}p{0.9cm}p{1.4cm}p{1.6cm}}
\toprule
Model         & Encoder & Decoder & FID-vid↓ & Detected PA↑ \\
\midrule
NÜWA-FF    & Full & Full &35.21 &   0.5220         \\
NÜWA-NF    & Nearby & Full &33.63 &   0.5357         \\
NÜWA-FN & Full & Nearby & 32.06 &    0.5438          \\
NÜWA-AA    & Axis & Axis &29.18  &  0.5957          \\
NÜWA          & Nearby & Nearby &\textbf{27.79} &   \textbf{0.6085}     \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-6mm}
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{imgs/TV2V.pdf}
	\caption{Samples of different manipulations on the same video.}
	\label{fig:TV2V}
	\vspace{-6mm}
\end{figure}

Fig.~\ref{fig:TV2V} shows a new task proposed in this paper, which we call ``Text-Guided Video Manipulation (TV2V)''. TV2V aims to change the future of a video starting from a selected frame guided by text. All samples start to change the future of the video from the second frame. 
The first row shows the original video frames, where a diver is swimming in the water. 
After feeding ``The diver is swimming to the surface'' into NÜWA's encoder and providing the first video frame, NÜWA successfully generates a video with the diver swimming to the surface in the second row. 
The third row shows another successful sample that lets the diver swim to the bottom. What if we want the diver flying to the sky? The fourth row shows that NÜWA can make it as well, where the diver is flying upward, like a rocket.


\section{Conclusion}
In this paper, we present NÜWA as a unified pre-trained model that can generate new or manipulate existing images and videos for 8 visual synthesis tasks. Several contributions are made here, including (1) a general 3D encoder-decoder framework covering texts, images, and videos at the same time; (2) a nearby-sparse attention mechanism that considers the nearby characteristic of both spatial and temporal axes; (3) comprehensive experiments on 8 synthesis tasks. This is our first step towards building an AI platform to enable visual world creation and help content creators.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{wcf}
}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=\textwidth]{supp/Sparse.pdf}
    \captionof{figure}{Comparisons between different 3D sparse attentions. All samples assume that the size of the input 3D data is  . The illustrations in the upper part show which tokens (blue) need to be attended to generate the target token (orange). The matrices of the size  in the lower part show the attention masks in sparse attention (black denotes masked tokens).}
    \label{fig:Sparse}
\end{center}
}]
\thispagestyle{empty}
\appendix



\section{Comparisons between 3D Sparse Attentions}

\begin{table}[t]
\footnotesize
\begin{center}
\caption{Complexity of different 3D sparse attention.}
\label{tab:sparse}
\begin{tabular}{p{3.6cm}p{3.5cm}}
\toprule
Module         & Complexity \\
\midrule
3D full  &       \\
3D block-sparse~\cite{weissenbornScalingAutoregressiveVideo2020, rakhimovLatentVideoTransformer2020}     &       \\
3D axial-sparse~\cite{hoAxialAttentionMultidimensional2019, rameshZeroShotTexttoImageGeneration2021, wuGODIVAGeneratingOpenDomaIn2021}   &    \\
3D nearby-sparse (ours)    &       \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Fig.~\ref{fig:Sparse} shows comparisons between different 3D sparse attentions. Assume we have 3D data with the size of , the idea of 3D block-sparse attention is to split the 3D data into several fixed blocks and handle these blocks separately. There are many ways to split blocks, such as splitting in time, space, or both. The 3D block-sparse example in Fig.~\ref{fig:Sparse} considers the split of both time and space. The 3D data is divided into 4 parts, each has the size of . To generate the orange token, 3D block-sparse attention considers previous tokens inside the fixed 3D block. Although 3D block-sparse attention considers both spatial and temporal axes, this spatial and temporal information is limited and fixed in the 3D block especially for the tokens along the edge of the 3D block. Only part of nearby information is considered since some nearby information outside the 3D block is invisible for tokens inside it. The idea of 3D axial-sparse attention is to consider previous tokens along the axis. Although 3D axis-sparse attention considers both spatial and temporal axes, this spatial and temporal information is limited along the axes. Only part of nearby information is considered and some nearby information that does not in the axis will not be considered in the 3D axis attention. In this paper, we propose a 3D nearby-sparse, which considers the full nearby information and dynamically generates the 3D nearby attention block for each token. The attention matrix also shows the evidence as the attended part (blue) for 3D nearby-sparse is more smooth than 3D block-sparse and 3D axial-sparse.

Tab.~\ref{tab:sparse} shows the complexity of different 3D sparse attention.  denotes the spatial height, spatial width, and temporal length of the 3D data. Different sparse mechanisms have their computational advantages in different scenarios. For example, for long videos or high-resolution frames with large , usually , and 3D nearby-sparse attention is more efficient than 3D axial-sparse attention. If the 3D data can be split into several parts without dependencies, 3D block-sparse will be a good choice. For example, a cartoon with several episodes and each tells a separate story, we can simply split these stories as they share no relationship.

\section{Details of Multi-task Pre-training}


\begin{table}[t]
\footnotesize
\caption{Implementation details for two settings of NÜWA.}
\label{tab:detail}
\begin{tabular}{p{3.5cm}p{1.9cm}p{2.0cm}}
\toprule
Settings                                   & NÜWA-256                                   & NÜWA-336                                     \\

\midrule
VQGAN  image resolution                  & 256×256 & 336×336 \\
VQGAN  discrete tokens                  & 32×32 & 21×21\\
VQGAN   Compression Ratio                  & F8                                           & F16                                          \\
VQGAN   codebook dimension                 & 256                                          & 256                                          \\


\midrule
3DNA  hidden size                          & 1280                                         & 1280                                         \\
3DNA   number of heads                     & 20                                           & 20                                           \\
3DNA   dimension for each head           & 64                                           & 64                                           \\
\midrule
NÜWA   Encoder layers                      & 12                                           & 12                                           \\
NÜWA   Decoder layers                      & 24                                           & 24                                           \\
\midrule
\multirow{3}{*}{Multi-task pretraining datasets} & \multicolumn{2}{c}{Conceptual Captions} \\
& \multicolumn{2}{c}{Moments in Time} \\
& \multicolumn{2}{c}{Vatex} \\
\midrule[0.1pt]
\multirow{3}{*}{Multi-task input 3D size}  & 1×1×77 (T2I)                                  & 1×1×77 (T2I)                                  \\
                                           & 32×32×1 (V2V)                                 & 21×21×1 (V2V)                                 \\
                                           & 1×1×77 (T2V)                                  & 1×1×77 (T2V)                                  \\
                                           \midrule[0.1pt]
\multirow{3}{*}{Multi-task output 3D size} & 32×32×1 (T2I)                                 & 21×21×1 (T2I)                                 \\
                                           & 32×32×4 (V2V)                                 & 21×21×10 (V2V)                                \\
                                           & 32×32×4 (T2V)                                 & 21×21×10 (T2V)                                \\
                                           \midrule
Training   batch size                      & \multicolumn{2}{c}{128}                                                                     \\
Training   learning rate                   & \multicolumn{2}{c}{}                                              \\
Training   steps                           & \multicolumn{2}{c}{50M}      \\         \bottomrule
                                                    
\end{tabular}
\end{table}

Tab.~\ref{tab:detail} shows the implementation details of two NÜWA settings used in this paper. Both NÜWA-256 and NÜWA-336 models are trade-off between image quality and video length (number of video frames). As the image quality highly relies on compression ratio and number of discrete tokens, and low compression ratio and large discrete tokens are key factors for high quality image. However, as the total capacity of the model is limited, the number of discrete tokens per image and the number of video frames (images) are a compromise.

Note that  NÜWA-256 adopts a compression ratio of F8 and the discrete tokens is 32×32, while NÜWA-336 adopts a compression ratio of F8 and the discrete tokens is only 21×21. To make a fair comparison with current state-of-the-art models, we adopt NÜWA-256 with more discrete tokens to generate high quality images. However, NÜWA-256 can only generate videos with 4 frames considering the efficiency of transformer. To handle relatively long videos, NÜWA-336 with 
fewer discrete tokens can generate videos with 10 frames. As a result, NÜWA-336 significantly relieves the pressure of the auto-regressive models in the second stage, especially for videos. NÜWA-336 is the default setting to cover both images and videos. 



For both models, note that we did not over-adjust the parameters and just use the same learning rate of  and 50M training steps.


\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{supp/cogview.pdf}
	\caption{Human evaluation on MSCOCO dataset for Text-to-Image (T2I) task.}
	\label{fig:supp_cogview}
		\vspace{-4mm}

\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=3.4in]{supp/vqgan.pdf}
	\caption{Human evaluation on MSCOCO dataset for Image Completion (I2I) task.}
	\label{fig:supp_vqgan}
	\vspace{-4mm}
\end{figure}
\section{Human Evaluation}
Fig.~\ref{fig:supp_cogview} presents human comparison results between CogView\cite{dingCogViewMasteringTexttoImage2021} and our NÜWA on the MSCOCO dataset for Text-to-Image (T2I) task. We randomly selected 2000 texts and ask annotators to compare the generated results between two models including both visual quality and semantic consistency. The annotators are asked to choose among three options: better, worse, or undetermined. In the visual quality part, There are 62\% votes for our NÜWA model, 15\% undetermined, and 23\% votes for CogView, which shows NÜWA generates more realistic images. In the semantic consistency part, although 67\% of votes cannot determine which model is more consistent with the text, NÜWA also wins the remaining 21\% votes. Although CogView is pretrained on larger text-image pairs than NÜWA, our model still benefits from multi-task pretraining, as text-videos pairs provides high-level semantic information for text-to-image generation.

Fig.~\ref{fig:supp_vqgan} shows human comparison results between VQ-GAN\cite{esserTamingTransformersHighResolution2021} and our NÜWA model on the MSCOCO dataset for the Image Completion (I2I) task. We use similar settings as Fig.~\ref{fig:supp_cogview}, but removed semantic consistency as there is no text input for this task. The comparison results show that there are 89\% votes for NÜWA, which shows the strong zero-shot ability of NÜWA. 



\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/t2i_1.pdf}
	\caption{More samples of Text-to-Image (T2I) task generated by NÜWA.}
	\label{fig:supp_t2i_1}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/t2i_2.pdf}
	\caption{More samples of Text-to-Image (T2I) task generated by NÜWA.}
	\label{fig:supp_t2i_2}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/s2i_1.pdf}
	\caption{More samples of Sketch-to-Image (S2I) task generated by NÜWA.}
	\label{fig:supp_s2i_1}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/s2i_2.pdf}
	\caption{More samples of Sketch-to-Image (S2I) task generated by NÜWA.}
	\label{fig:supp_s2i_2}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/i2i_1.pdf}
	\caption{More samples of the Image Completion (I2I) task generated by NÜWA.}
	\label{fig:supp_s2i_1}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/i2i_2.pdf}
	\caption{More samples of the Image Completion (I2I) task generated by NÜWA.}
	\label{fig:supp_s2i_2}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/ti2i_1.pdf}
	\caption{More samples of the Text-Guided Image Manipulation(TI2I) task generated by NÜWA.}
	\label{fig:supp_ti2i_1}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/ti2i_2.pdf}
	\caption{More samples of the Text-Guided Image Manipulation(TI2I) task generated by NÜWA.}
	\label{fig:supp_ti2i_2}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/t2v_1.pdf}
	\caption{More samples of the Text-to-Video (T2V) task generated by NÜWA.}
	\label{fig:supp_v2v_1}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/t2v_2.pdf}
	\caption{More samples of the Text-to-Video (T2V) task generated by NÜWA.}
	\label{fig:supp_v2v_2}
\end{figure*}
\begin{figure*}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{supp/s2v_1.pdf}
	\caption{More samples of Sketch-to-Video (S2V) task generated by NÜWA.}
	\label{fig:supp_v2v_1}
\end{figure*}
\begin{figure*}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{supp/s2v_2.pdf}
	\caption{More samples of Sketch-to-Video (S2V) task generated by NÜWA.}
	\label{fig:supp_v2v_1}
\end{figure*}
\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/v2v_1.pdf}
	\caption{More samples of the Video Prediction (V2V) task generated by NÜWA. Only one frame (see red box) is used as condition.}
	\label{fig:supp_v2v_1}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/v2v_2.pdf}
	\caption{More samples of the Video Prediction (V2V) task generated by NÜWA.Only one frame (see red box) is used as condition.}
	\label{fig:supp_v2v_2}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/tv2v_1.pdf}
	\caption{More samples of Text-Guided Video Manipulation (TV2V) task generated by NÜWA.}
	\label{fig:supp_v2v_2}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth]{supp/tv2v_2.pdf}
	\caption{More samples of Text-Guided Video Manipulation (TV2V) task generated by NÜWA.}
	\label{fig:supp_v2v_2}
\end{figure*}





\end{document}
