\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\reid}{re-id}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\sota}{the state-of-the-art}
\newcommand{\myparagraph}[1]{\noindent\textbf{#1.}}
\newcommand{\name}{\text{Deepchange}}
\usepackage{multirow}


\newcommand{\mcr}{\color{red}}
\newcommand{\mcb}{\color{blue}}
\newcommand{\mck}{\color{black}}



\def\ie{i.e.}
\def\eg{e.g.}
\def\ex{\textrm{ext}}
\def\conv{\textrm{conv}}
\def\aff{\textrm{aff}}
\def\dim{\textrm{dim}}
\def\diag{\textrm{diag}}
\def\Diag{\textrm{Diag}}
\def\rank{\textrm{rank}}
\def\rank{\textrm{rank}}
\def\sign{\textrm{sign}}
\def\spann{\textrm{span}}
\def\svd{\textrm{svd}}
\def\trace{\textrm{trace}}
\def\Diag{\textrm{Diag}}
\def\norm{\textrm{norm}}
\def\b{\textbf{b}}
\def\0{\textbf{0}}
\def\1{\textbf{1}}



\def\a{\boldsymbol{a}}
\def\b{\boldsymbol{b}}
\def\c{\boldsymbol{c}}
\def\e{\boldsymbol{e}}
\def\f{\boldsymbol{f}}
\def\v{\boldsymbol{v}}
\def\p{\boldsymbol{p}}
\def\q{\boldsymbol{q}}
\def\w{\boldsymbol{w}}
\def\x{\boldsymbol{x}}
\def\y{\boldsymbol{y}}
\def\z{\boldsymbol{z}}
\def\m{\boldsymbol{m}}
\def\l{\boldsymbol{l}}
\def\r{\boldsymbol{r}}
\def\u{\boldsymbol{u}}



\def\th{\boldsymbol{\theta}}
\def\II{\mathcal{I}}
\def\I{\mathbf{I}}
\def\III{\mathbb{I}}
\def\EE{\mathbb{E}}
\def\RR{\mathbb{R}}
\def\A{\mathcal{A}}
\def\B{\mathcal{B}}
\def\C{\mathcal{C}}
\def\D{\mathcal{D}}
\def\E{\mathcal{E}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\K{\mathcal{K}}
\def\J{\mathcal{J}}
\def\L{\mathcal{L}}
\def\M{\mathcal{M}}
\def\N{\mathcal{N}}
\def\P{\mathcal{P}}
\def\Q{\mathcal{Q}}
\def\U{\mathcal{U}}
\def\T{\mathcal{T}}
\def\S{\mathcal{S}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\Z{\mathcal{Z}}
\usepackage{graphicx,subfigure}

\usepackage{url}





\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}




\def\iccvPaperID{6221} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\iccvfinalcopy
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{ MaskCL: Semantic Mask-Driven Contrastive Learning for Unsupervised clothes-change Person Re-Identification}
\title{ MaskCL: Semantic Mask-Driven Contrastive Learning for Unsupervised Person Re-Identification with Clothes Change}









\author{
    Mingkun Li$^{\dag,1}$, Peng Xu$^{*,2}$, Chun-Guang Li$^{\dag,1}$, Jun Guo $^{\dag,1}$\\
    $^{1}$Beijing University of Posts and Telecommunications,
    $^{2}$Tsinghua University\\
    {\tt\small $^\dag$\{mingkun.li,lichunguang, guojun\}@bupt.edu.cn,$^*$peng$\_$xu@tsinghua.edu.cn}
}
\maketitle
\ificcvfinal\thispagestyle{empty}\fi 


\begin{abstract}






This paper considers a novel and challenging problem: unsupervised long-term person re-identification with clothes change. Unfortunately, conventional unsupervised person re-id methods are designed for short-term cases and thus fail to perceive clothes-independent patterns due to simply being driven by RGB prompt. To tackle with such a bottleneck, we propose a semantic mask-driven contrastive learning approach, in which silhouette masks are embedded into contrastive learning framework as the semantic prompts and cross-clothes invariance is learnt from hierarchically semantic neighbor structure by combining both RGB and semantic features in a two-branches network. 
{Since such a challenging re-id task setting is investigated for the first time, we conducted extensive experiments to evaluate \sota{} unsupervised short-term person re-id methods on five widely-used clothes-change \reid{} datasets.}
Experimental results verify that our approach outperforms the unsupervised \reid{} competitors by a clear margin, remaining a narrow gap to the supervised baselines. 

\end{abstract}






\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{figures/easysamplev5.pdf}
\end{center}
\vspace{-15pt}
   \caption{Illustration of hierarchical semantic clustering. At a lower level, clustering is used to disclose the instance neighbor structure; at a higher level, semantic features and RGB data are combined to expose the cluster neighbor structure.}
\label{fig:semantic_test}
\vspace{-15pt}
\end{figure}





\section{Introduction}
\label{sec:intro}
{
Person re-identification (\reid{}) aims to match  person identities of bounding box images that are captured from distributed camera views~\cite{ye:surveyarixv2020}. 
Most conventional studies in the field of unsupervised person re-id have only focused on the scenarios {\em without} clothes change~\cite{zhao:PAMI16,xiao2016learning,ge2018fd}.
However, re-id in such scenarios is unrealistic since that the majority of people may change their clothing everyday, if not more often. 
Thus, these studies may only be useful in the short-term re-id settings but fail when facing  
the long-term person \reid{} scenario with clothes changes. 

Recently, there are a few attempts to tackle with the long-term \reid{} task~\cite{Change:1,Change:2,change:7,change:8}, but all of them are supervised learning methods
with heavy reliance on large labeled training data. 
Unfortunately, it is extremely difficult to collect and annotate the person identity labels under the scenario of unconstrained clothes change, thus preparing the labeled re-id training data, \eg, \name{}~\cite{Deepchange}, in a realistic scenario is quite expensive and exhausting. 
}













{
Due to the significance of long-term person \reid{}, it is appealing to develop unsupervised method to approach long term person re-id problem without the tedious requirement of person identity labeling. 
This is a more complex but more realistic extension of the previous unsupervised short-term person re-id~\cite{li2018unsupervised,Ge:NIPS20, lmkpr} that different people may have similar clothes whilst the same person might wear variant clothes with very distinct appearances, as shown in Figure~\ref{fig:clothes-change}.
Unfortunately, previous studies of unsupervised person re-id have not dealt with the clothes change cases and existing methods will fail to perceive clothes-independent patterns due to simply being driven by RGB prompts~\cite{TIPlmk}. 
Specifically, most of the existing unsupervised methods \cite{dai:ACCV22} are cluster-based and the feature extractions are mostly dominated by color~\cite{TIPlmk}.
As a consequence, the clustering algorithm will blindly assign every training sample with a color-based pseudo label, which is error-prone with a large cumulative propagation risk and ultimately leads to sub-optimal solutions, such as simply grouping the individuals who wear the same clothing.
}


{
In this paper, we propose a novel semantic mask-driven contrastive learning framework, termed MaskCL, for attacking the unsupervised long-term person re-id challenge. 
In MaskCL, we embed the person silhouette mask as the semantic prompts into contrastive learning framework and learn cross-clothes invariance features of pedestrian images from a hierarchical semantic neighbor structure with a two-branches network.  
Specifically, MaskCL adopts a contrast learning framework to mine the invariance between semantic silhouette masks and RGB images to further assist the network in learning clothes-irrelevant features.
In the contrastive training stage, we employ RGB features to generate clusters, thus images of individuals wearing the same clothes tend to be clustered together owing to their greater resemblance.
At the meantime, since that silhouette masks contain rich clothes-invariant features, we use it as semantic prompts to combine with features from RGB images to unveil the hidden neighbor structure at the cluster level. 
Consequently, we fuse the clustering result (based on RGB features) and semantic neighbor sets (based on semantic prompts) to form a hierarchical neighbor structure and use it to drive model training for reducing feature disparities due to clothes change.
}

{
To provide a comprehensive evaluation and comparison, we evaluate recent unsupervised person re-id methods on five long-term person re-id datasets. Experimental results demonstrate that our approach significantly outperforms all short-term methods and even matches the  \sota{} fully supervised models on these datasets.
}



{
The contributions of the paper are highlighted as follows.
\begin{enumerate}
\item
To the best of our knowledge, this is the first attempt to study the unsupervised long-term person re-id with clothes change. 

\item 
We present a hierarchically semantic mask-based contrastive learning framework, in which person silhouette masks  are used as semantic prompt and a hierarchically semantic neighbor structure is constructed to learn cross-clothes invariance.
\item 
We conduct extensive experiments on five widely-used clothes-change \reid{} datasets to evaluate the performance of the proposed approach. 
These evaluation results can serve as a benchmarking ecosystem for the long-term person \reid{} community. \end{enumerate}
}


\begin{figure}[!thb]
\center
{\includegraphics[ width=0.9\linewidth]{figures/person_image_slim.pdf}}\\
\caption{Visualizing the intrinsic challenges in long-term person \reid{} with clothes change. We randomly selected $28$ images of a single individual from the \name{} \cite{Deepchange} dataset.
It is clear that the differences in appearance across different clothes (rows) are much more significant than that of the same clothes (each column).
}
\label{fig:clothes-change}
\vspace{-10pt}
\end{figure}



\section{Related Work}
\label{sec:related-work}
\subsection{Long-Term Person Re-Identification}
A few recent person \reid{} studies attempt  to tackle~\cite{Change:1,Change:2,change:3,change:4,change:5,change:6,change:7} the long-term clothes changing situations via supervised training, and emphasize the use of additional supervision for the general appearance features (\eg, clothes, color), to  enable  the model learn the cross-clothes features. For example, 
Yang \etal~\cite{Change:1} generates contour sketch images from RGB images and highlights the invariance between sketch and RGB.
Hong \etal~\cite{change:8} explore fine-grained body shape features by estimating masks with discriminative shape details and extracting pose-specific features. 
While these seminal works provide inspiring attempts for \reid{} with clothes change, there are still some limitations: 
a) Generating contour sketches or other side-information requires additional model components and will cause extra computational cost; 
b) To the best of our knowledge, all the existing models designed for \reid{} clothes change are supervised learning methods, with lower transferability to the open-world dataset settings. 
In this paper, we attempt to tackle with the challenging \reid{} with clothes change under unsupervised setting. 




\subsection{Unsupervised Person Re-Identification}
To meet the increasing demand in real life and to avoid the high consumption in data labelling, a large and growing body of literature has investigated unsupervised person re-id~\cite{Zheng:arXiv16,liu:CVPR19,Wang:CVPR18}.  
The existing unsupervised person Re-ID methods can be divided into two categories: a) unsupervised domain adaptation methods, which require a labeled source dataset and an unlabelled target dataset~\cite{liu:CVPR19,Bak:ECCV18,Peng:CVPR16,Wang:CVPR18}; and b) purely unsupervised methods that work with only an unlabelled dataset~\cite{TIPlmk,PPLR,ICE,dai:ACCV22}. 
However, up to date, the unsupervised person re-id methods are focusing on short-term scenarios, none of them taking into account the long-term re-id scenario. To the best of our knowledge, this is the first attempt to address the long-term person re-id in unsupervised setting. We evaluate the performance of the existing \sota{} unsupervised methods~\cite{Ge:NIPS20,TIPlmk,dai:ACCV22,lmkpr,PPLR,ICE} on five long-term person re-id datasets and set up a preliminary benchmarking ecosystem for the long-term person \reid{} community.












\begin{figure*}[!htb]
\begin{center}
\includegraphics[width=0.85\linewidth]{figures/jiegoutuv6.pdf}
\end{center}
\vspace{-5pt}
   \caption{Illustration for our proposed Semantic Mask-driven Contrastive Learning (MaskCL) framework.}
\label{fig:framework}
\vspace{-15pt}
\end{figure*}

\section{Our Proposed Approach: Semantic Mask-driven Contrastive Learning (MaskCL)}

This section presents a semantic mask-driven contrastive learning (MaskCL) approach for unsupervised long-term person re-id with clothes change. 

For clarity, we provide the flowchart of MaskCL in Fig.~\ref{fig:framework}. Our MaskCL is a two branches network: $F(\cdot|\Theta)$ and $F^\prime(\cdot|\Theta^\prime)$, which perceive RGB and 
semantic mask patterns, respectively, and $\Theta$ and $\Theta^\prime$ denote the parameters in the networks. 
The parameters $\Theta$ and $\Theta^\prime$ are optimized separately.  We also design a predictor layer $G(\cdot|\Psi)$ where $\Psi$ denotes the parameters subsequent to RGB branch.
In addition, we design a feature fusion layer $R(\cdot|\Omega)$, where $\Omega$ denotes the parameters.



Given an unlabeled pedestrian image dataset $\II = {\{I_i}\}_{i=1}^N$ consisting of $N$ samples,  we generate the corresponding pedestrian semantic mask dataset $\S = {\{S_i}\}_{i=1}^N$  through human parsing network such as SCHP~\cite{ParsingNet:TPAMI20}. 
For an input image $I_i\in \II$, 
we use the $I_i$ as the input of $F(\cdot|\Theta)$ and $S_i$ as the inputs of $F^\prime(\cdot|\Theta^\prime)$.
For simplicity, we denote the output features of the  $F(\cdot|\Theta)$ and $F^\prime(\cdot|\Theta^\prime)$ as $\x_i$ and $\tilde \x_i$, 
denote the output of the predictor layer in the $G(\cdot|\Psi)$ as $\z_i$, 
and denote the output of fusion layer $R(\cdot|\Omega)$ as $\f_i$, respectively, where $\x_i, \tilde \x_i, \z_i, \f_i \in \RR^D$. 



The training of MaskCL alternates between representation learning and hierarchical semantic structure construction. 
In hierarchical semantic structure creation, the hierarchical semantic neighbor structure is constructed by integrating RGB and semantic data to drive the training of the contrast learning framework, the details are described  in Section~\ref{sec:Hierarchical Semantic}. 
In feature learning, we introduce the person's silhouette masks as the semantic source and investigate cross-clothes features via contrastive learning.
In the training phase, MaskCL uses the hierarchical semantic neighbor structure as self-supervision information to train the $F(\cdot|\Theta)$, $F^\prime(\cdot|\Theta^\prime)$, $G(\cdot|\Psi)$ and $R(\cdot|\Omega)$. The details are described  in Section~\ref{sec:Contrastive-Learning-Framework}.
Furthermore, we build an adaptive learning strategy to automatically modify the hierarchical semantic neighbor selection, which can flexibly select neighbor clusters according to dynamic criteria. 
The details are described  in Section~\ref{sec:Curriculumneighbor}.


 In MaskCL, we maintain the three instance-memory banks $\M = \{ \boldsymbol{v}_i\}_{i=1}^N$, $\tilde \M = \{  \boldsymbol{ \tilde v}_i \}_{i=1}^N$, and $\hat \M = \{  \boldsymbol{\hat v}_i\}_{i=1}^N$, where $ \boldsymbol{v}_i,   \boldsymbol{ \tilde v}_i, \boldsymbol{\hat v}_i \in \RR^D$
to store the outputs of the two branches and the fusion layer,  respectively. 
Memory banks $\M$ and $\hat \M$ are initialized with $\X:=\{\x_1,\cdots,\x_N\}$ and $\tilde \M$ is initialized with $\tilde \X:=\{\tilde \x_1,\cdots,\tilde \x_N\}$, where $\X$ and $\tilde \X$ are the outputs of the network branches $F(\cdot|\Theta)$ and $F^\prime(\cdot|\Theta^\prime)$ pre-trained on ImageNet, respectively.





\subsection{Hierarchical Semantic Clustering}
\label{sec:Hierarchical Semantic}


To have a hierarchical semantic clustering, we sort the hierarchical semantic structure into two levels: a) low-level instance neighbors, and b) high-level semantic neighbors.

\myparagraph{Low-level instance neighbors} At beginning, we pre-train the two network branches $F(\cdot|\Theta)$ and $F^\prime(\cdot|\Theta^\prime)$ on ImageNet~\cite{Krizhevsky:NIPS12}, and use the branch $F(\cdot|\Theta)$ outputs features to yield $m$ clusters, which are denoted as $\C:=\{\C^{(1)}, \C^{(2)}, \cdots, \C^{(m)}\}$.  
We use the clustering results $\C$ to indicate the connection between neighbors at the instance level since that if samples are clustered together, it also indicates that their RGB features are similar.

\myparagraph{High-level semantic neighbors} 
Since that the semantic masks contain richer clothing invariance features, we use them to find those pedestrian samples that are similar at the semantic level, \eg, the same person wearing different clothing. To be specific, we fuse the RGB feature and the semantic feature to renew the representation of instance, and search the semantic neighbor at the cluster-level based on fusion features.
For an image $\I_i$, the fusion feature $\f_i$ is defined as:
\begin{equation}
\begin{aligned}
\f_i = F(concate(\x_i|\tilde \x_i)),
\end{aligned}
\label{eq:feature-fusion}
\end{equation}
where $concate(\cdot)$ denotes the operation of concatenating feature $\x_i$ and $\tilde \x_i$ at channel-wise. Based on the fused features, we define the cluster center $\boldsymbol{u}_{\omega(\I_i)}$ accordingly as follows:
\begin{equation}
\begin{aligned}
\boldsymbol{u}_{\omega(\I_i)} = \frac{1}{|\C^{(\omega(\I_i))}|} \sum_{I_j \in \C^{(\omega(\I_i))}}\f_j,
\end{aligned}
\label{eq:cluster_center_fusion}
\end{equation}
where $\omega(\I_i)$ is the cluster index of image $\I_i$. 


Having had cluster-center $\boldsymbol{U}= {\{\boldsymbol{u_i}}\}_{i=1}^m$, we find cluster-level semantic neighbors and construct the semantic neighbor set $\N$. 
Specifically, for cluster $\C^{(\ell)}$, we define the similarity between clusters $\C^{(\ell)}$ and $\C^{(i)}$ as 
\begin{equation}
\begin{aligned}
\D(\C^{(\ell)}, \C^{(i)}) = \frac{\boldsymbol{u}_\ell^{\top}}{\|\boldsymbol{u}_{\ell}\|_2} \frac{ \boldsymbol{u}_i}{\| \boldsymbol{u}_i \|_2}.
\end{aligned}
\label{eq:cluster_sim}
\end{equation}
We denote the semantic neighbor set of cluster $\C^{(\ell)}$ as $\N^{(\ell)}$, 
which includes the top-$k$ similar clusters $\C^{(i)}$ sorted by $\D(\C^{(\ell)}, \C^{(i)})$. 
Then, we form a semantic neighbor set\footnote{In abstract algebra, it is a {\it set class}, which is also a set and its elements are also sets.} $\A:= \{ \N^{(1)},\cdots, \N^{(m)} \}$. Because of the hierarchy property to define the set class $\A$, we call the process as the hierarchical semantic clustering. 


In the hierarchical semantic clustering stage, we construct neighbor structures based on some specific clustering algorithm and semantic neighbor searching, respectively. The clustering result of the output features $\X:=\{\x_1,\cdots,\x_N\}$ from $F(\cdot|\Theta)$ is used to generate the pseudo labels $\Y :=\{\y_1,\cdots,\y_N\}$, and semantic neighbor set $\A$ contains the semantic level neighbor index for each cluster. 
In the contrastive learning stage, both the clustering result and the semantic neighbor set are used as self-supervision information to train the model to learn cross-clothes features.


\subsection{Contrastive Learning Framework }
\label{sec:Contrastive-Learning-Framework}

To effectively explore the invariance features between RGB images and semantic masks, we construct three contrastive learning modules to train MaskCL assisted with the self-supervision information provided by the hierarchical semantic clustering as follows:
a) Prototypical contrastive learning module, which is used for contrast training between positive samples and negative pairs;
b) Cross-view contrastive learning module, which is used for contrast training between RGB images and semantic masks; and
c) Semantic neighbor contrastive learning module, which is used for contrast training between semantic neighbor clusters and negative pairs.


\myparagraph{Prototypical Contrastive Learning Module}  We apply prototypical contrastive learning to discover the hidden information inside the cluster structure. For the $i$-th instance, we denote its cluster index as $\omega(I_i)$, the center of $C^{\omega(I_i)}$ as the positive prototype, and all other cluster centers as the negative prototypes. 
We define the prototypical contrastive learning loss as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_P 
= & - ((1 - q_i)^2 \ln( q_i) \\
& + (1 - \tilde q_i)^2 \ln(\tilde q_i) + (1 - \hat q_i)^2 \ln(\hat q_i)),
\end{aligned}
\label{eq:prototype-loss}
\end{equation}
where $q_{i}$, $\tilde q_i$ and $\hat q_i$ measure the consistency between the outputs of $F(\cdot|\Theta)$, $F^\prime(\cdot|\Theta^\prime)$ and $R(\cdot|\Omega)$ and the related prototype computed with the memory bank and are defined as 
\begin{equation}
\begin{aligned}
       q_{i}=\frac{\exp(p_{{\omega(I_i)}}^\top{\x_i}/\tau)}{ \sum_{\ell = 1}^{m}\exp(p_{\ell}^\top {\x_i}/\tau)},\\
\tilde q_{i}=\frac{\exp(\tilde p_{{\omega(I_i)}}^\top{\tilde \x_i}/\tau)}{ \sum_{\ell = 1}^{m}\exp(\tilde p_{\ell}^\top {\tilde \x_i}/\tau)},\\
\hat q_{i}=\frac{\exp(\hat p_{{\omega(I_i)}}^\top{\f_i}/\tau)}{ \sum_{\ell = 1}^{m}\exp(\hat p_{\ell}^\top {\f_i}/\tau)},
\label{eq:softmax-cal}
\end{aligned}
\end{equation}
where $p_{{\omega(I_i)}}$ as the RGB prototype vector of the cluster $\C^{(\omega(I_i))}$ is defined by
\begin{align}
 p_{\omega(I_i)} = \frac{1}{|\C^{(\omega(I_i))}|} \sum_{I_j \in \C^{(\omega(I_i))}} \boldsymbol{v}_j,
\label{eq:cluster_center}
\end{align}
here $\boldsymbol{v}_j$ is the instance feature of image $I_j$ in $\M$, $\tilde p_i$ and $\hat p_i$ are calculated in the same way with corresponding instance memory bank $\tilde \M$ and $\hat \M$, respectively. 


The prototypical contrastive learning module performs 
contrastive learning between positive and negative prototypes to improve the discriminant ability for the networks $F(\cdot|\Theta)$ and $F^\prime(\cdot|\Theta^\prime)$ and the feature fusion layer $R(\cdot|\Omega)$. 




\myparagraph{Cross-view Contrastive Learning Module} To effectively train the contrastive learning framework across the two views, we design a cross-view constrastive module to mine the invariance between RGB images and semantic masks. To match the feature outputs of two network branches at both the instance level and cluster level, specifically, 
we introduce the negative cosine similarity of the outputs of $G(\cdot|\Psi)$ and $F^\prime(\cdot|\Theta^\prime)$ to define the two-level contrastive losses as follows:
\begin{align}
\mathcal{L}_C := -  \frac{\z^\top_i}{\|\z_i\|_2} \frac{\tilde \x_i}{\| \tilde \x_i \|_2} 
 - \frac{\z_i^\top}{\|\z_i\|_2} \frac{\tilde p_{\omega(I_i)}}{\| \tilde p_{\omega(I_i)} \|_2} \label{eq:contrastive-loss}
\end{align}
where $\| \cdot \|_2$ is the $\ell_2$-norm.

The cross-view contrastive learning module 
explores the invariance between RGB images and semantic mask and thus assist the network to mine the invariance information provided by the RGB image and the semantic mask, as well as imposing such a self-supervision information on the module for learning clothing-unrelated features.


\myparagraph{Semantic Neighbor Contrastive Learning Module}
To avoid training the model into degeneration that only push samples with similar appearance together, we design a semantic neighbor constrastive learning module.
Particularly, we propose a weighted semantic neighbor contrastive loss as follows: 
\begin{equation}
\begin{aligned}
\mathcal{L}_N = -\sum_{j \in \N^{(\omega(I_i))}} w_{ij} ( \ln(q_j)  + \ln(\tilde  q_j)  ),
\label{eq:Neighour_loss}
\end{aligned}
\end{equation}
where $\N^{(\omega(I_i))}$ is the set of semantic neighbors of cluster $\omega(I_i)$, and $w_{ij}$ is the weight, which is defined as
\begin{equation}
\begin{aligned}
w_{ij} = \D(\C^{(i)},\C^{(j)}) \cdot \B(\C^{(i)},\C^{(j)}),
\label{eq:Neighour_loss_detail}
\end{aligned}
\end{equation}
in which $ \D(\C^{(i)},\C^{(j)})$ is defined in Eq.~\eqref{eq:cluster_sim} and the $ \B(\C^{(i)},\C^{(j)}) $ is 
a Bernoulli sampling defined by
\begin{equation}
\begin{aligned}
\B(\C^{(i)},\C^{(j)}) = \text{Bern}(\x_j|\D(\C^{(i)},\C^{(j)})),
\label{eq:Bernoulli}
\end{aligned}
\end{equation}
where $\text{Bern}(\x_j | \D(\C^{(i)},\C^{(j)}))$ is Bernoulli trial that sampling $x_j$ with probability $ \D(\C^{(i)},\C^{(j)})$.
Owing to the semantic neighbor constrastive learning module trained via loss in Eq.~\eqref{eq:Neighour_loss}, the semantic neighbors will be pushed closer in the feature space. This will help model to investigate consistency across semantic neighbor clusters.




\subsection{Curriculum Nerighour Selecting }
\label{sec:Curriculumneighbor}
Sinc that in the early training stages, the model has weaker ability to distinguish samples, we hope the ability improve during training. 
To this end, we provide a curriculum strategy for searching neighbors which sets the search range accouding to the training progress. Specifically, we set the semantic neighbour searching range $k$ (which is defined in Section~\ref{sec:Hierarchical Semantic}) as
\begin{align}
k:=  t  \lfloor K / T \rfloor, 
\label{eq:Curriculumneighbor}
\end{align}
where $T$ is the total number of training epochs and $t$ is the current step, $K$ is a hyper-parameter (which will be discussed in Section~\ref{sec:Experiments}).


\subsection{Training and Inference Procedure for MaskCL}

\myparagraph{Training Procudure}
In MaskCL, the two branches are implemented with ResNet-50 \cite{he:CVPR16resnet} and do not share the parameters. We first pre-train the two network branches on ImageNet and use the learned features to initialize the three memory banks $\M$, $\tilde \M$, and $\hat \M$, respectively. 
In the training phase, we train both network branches and the fusion layer with loss:
\begin{align}
\mathcal{L} := \mathcal{L}_{P} + \mathcal{L}_{C} + \mathcal{L}_{N}. 
\label{eq:AllLoss}
\end{align}
We update the three instance memory banks $\M$, $\tilde \M$ and $\hat \M$, respectively, as follows:
\begin{align}
\boldsymbol{v}_i^{(t)} \leftarrow \alpha \boldsymbol{v}_i^{(t-1)} + (1-\alpha) \x_i,
\label{eq:update-cluster_1}\\
\boldsymbol{\tilde v}_i^{(t)} \leftarrow \alpha \boldsymbol{\tilde v}_i^{(t-1)} + (1-\alpha)\tilde \x_i, \\ 
\boldsymbol{\hat v}_i^{(t)} \leftarrow \alpha \boldsymbol{\hat v}_i^{(t-1)} + (1-\alpha)\f_i,
\label{eq:update-cluster_2}
\end{align}
where $\alpha$ is set as 0.2 by default.



\myparagraph{Inference Procedure} 
After training, we keep only the ResNet $F(\cdot|\Theta)$ in for inference. 
We compute the distances between each image in the query and each image in the gallery using the feature obtained from the output of the first branch $F(\cdot |\Theta)$. We then sort the distances in ascending order to discover the matched results. 


\begin{table*}[!htb]

\vspace{3pt}
\small
\begin{center}
\setlength{\tabcolsep}{4.5mm}{
\begin{tabular}{cccccccc}
\hline		
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Size} & \multicolumn{3}{c}{Subset} & \multirow{2}{*}{Identity} & \multirow{2}{*}{Camera} & \multirow{2}{*}{Clothes}\\
\cline{3-5}
& &Train&Query&Gallery& & & \\
\hline
Deepchange\cite{Deepchange}& 178, 407 & 75, 083 & 17, 527 &  62, 956 & 1, 121 & 17 & - \\
LTCC\cite{Change:1} &  17, 119 & 9, 576 & 493 & 7, 050 & 152 & 12 & 14\\
PRCC\cite{yang2019person} & 33, 698 & 17, 896 & 3, 543 & 12, 259 & 221 & 3 & - \\
Celeb-ReID\cite{celebreid} & 34, 186 & 20, 208 & 2, 972 & 11, 006 & 1, 052 & - & - \\
Celeb-ReID-Light\cite{celebreidlight} & 10, 842 &  9, 021 & 887  &  934 &  590 & - & -\\
VC-Clothes\cite{vcclothes}& 19, 060 & 9, 449 & 1, 020 & 8, 591 & 256 & 4 & 3\\
\hline
\end{tabular}
}

\end{center}
\vspace{-8pt}
\caption{Long-term Person Re-ID Datasets details.}
\label{tab:dataset}
\end{table*}




\begin{table*}[!htb]

\vspace{-6pt}
\small
\begin{center}
\setlength{\tabcolsep}{3.2mm}{
\begin{tabular}{cccccccccccc}
\hline		
\multirow{3}{*}{Method} & \multirow{3}{*}{Reference} & \multicolumn{4}{c}{LTCC}&\multicolumn{6}{c}{PRCC}\\
\cline{3-12}
& &\multicolumn{2}{c}{C-C}&\multicolumn{2}{c}{General}&\multicolumn{3}{c}{C-C}&\multicolumn{3}{c}{General}\\
\cline{3-12}  
& & mAP & R-1 &mAP & R-1 &mAP & R-1 & R-10 &mAP & R-1 & R-10\\

\hline
\multicolumn{2}{c}{Supervised Method(ST-ReID)} \\
\hline

HACNN~\cite{li:HACNNCVPR18} & CVPR'18 & 9.30 & 21.6 & 26.7 & 60.2 & -  & 21.8 & 59.4 & - & 82.5 & 98.1\\
PCB~\cite{sun:PCBECCV18} & ECCV'18 & 10.0 & 23.5 & 30.6 & 38.7 & 38.7 & 22.8 & 61.4  & 97.0 & 86.8 & 99.8\\
\hline
\multicolumn{2}{c}{Supervised Method(LT-ReID)} \\
\hline
CESD~\cite{qian:CESDACCV20} & ACCV'20 & 12.4 & 26.2 & 34.3 & 71.4 &  - & - & - &  - & - & -\\
RGA-SC~\cite{zhang:RGASCCVPR20} & CVPR'20 & 14.0 & 31.4 & 27.5  & 65.0 & - & 42.3 & 79.4 & - & 98.4 & 100\\
IANet~\cite{hou:IANETCVPR19} & CVPR'19 & 12.6 & 25.0 & 31.0 & 63.7& 45.9 & 46.3 & - & 98.3 & 99.4 & - \\
GI-ReID~\cite{jin:GI-ReIDCVPR22} & CVPR'22 & 10.4 & 23.7 & 29.4 & 63.2 &  - & - & - &  - & - & - \\

RCSANet~\cite{huang:RCSANetCVPR21} & CVPR'21 & - & - & - & - & 48.6 & 50.2 & - & 97.2 & 100 & -  \\
3DSL~\cite{chen:3DSLCVPR21} & CVPR'21 & 14.8 & 31.2 & - & - & - & 51.3 &-  & - & - & -\\
FSAM~\cite{hong:FSAMCVPR21} & CVPR'21 & 16.2 & 28.5 & 25.4 & 73.2 & - & 54.5 & 86.4 & -  & 98.8 & 100\\
CAL~\cite{CCgu:CVPR22} & CVPR'22 & 18.0 & 40.1 & 40.8 & 74.2 & 55.8 & 55.2 & - & 99.8 & 100 & -  \\ 
CCAT~\cite{ren:CCATIJCNN22} & IJCNN'22 & 19.5 & 29.1 & 50.2 & 87.2 & - &69.7 & 89.0 & - & 96.2 & 100\\ 
\hline
\multicolumn{2}{c}{Unsupervised Method} \\
\hline
SpCL~\cite{Ge:NIPS20} & NeurIPS'20 & 7.60 & 15.3  & 21.2 & 47.3 & 45.2 & 33.2 & 71.3 & 90.6 & 86.4 & 98.3 \\
C3AB~\cite{lmkpr} & PR'22 & 8.30 & 15.2 & 20.7 & 46.7 & 48.6 & 36.7 & 74.0 & 90.2 & 88.3 & 98.1 \\
CACL~\cite{TIPlmk} & TIP'22 & 6.20 & 9.80 & 22.3 & 45.6 & 52.1 & 41.7 & 79.8 & 94.7 & 90.9 & 99.9\\
CC~\cite{dai:ACCV22} & ACCV'22 & 6.00 & 7.40 & 11.0 & 17.0 & 46.3 & 34.4 & 74.4  & 94.4 & 90.2 & 99.9 \\
ICE~\cite{chen:ICCV21} & ICCV'21 & 7.10 & 14.5 & 28.4 & 61.1 & 48.0 & 34.8 & 74.2 & 95.9 & 93.6 & 99.9 \\
ICE*~\cite{chen:ICCV21} & ICCV'21 & 10.1 & 16.3  & 22.6 & 44.0 & 45.5 & 32.6 & 72.3 &95.7 & 93.3  & 99.8    \\
PPLR~\cite{cho:CVPR22} & CVPR'22 & 4.40 & 4.80 & 6.00 & 11.2  & 51.4 & 40.0 & 75.2  & 91.7 & 87.4 & 99.8  \\
\hline
\bf{MaskCL} & \bf{Ours}  & \bf{12.7} & \bf{25.5} & \bf{29.2} & \bf{59.8} &  \bf{55.1} & \bf{43.7} & \bf{79.2}  &  \bf{96.8} & \bf{95.2} & \bf{99.6}   \\
\hline
\end{tabular}
}
\end{center}
\vspace{-5pt}
\caption{Comparison to other state-of-the-art methods on LTCC and PRCC, `C-C' means clothes change setting, `General' means general setting. 
{ `*'  means do not use the camera label as side-information.
}}
\label{tab:SOTALTCC}
\vspace{-15pt}
\end{table*}


\begin{table}[!htb]

\small
\vspace{-6pt}
\begin{center}
\setlength{\tabcolsep}{1.8mm}{
\begin{tabular}{cccccc}
\hline		
\multirow{2}{*}{Method} & \multirow{2}{*}{Reference} & \multicolumn{2}{c}{Celeb-ReID}&\multicolumn{2}{c}{Celeb-Light}\\
\cline{3-6}  
& & mAP & R-1 & mAP & R-1\\
\hline
\multicolumn{2}{c}{Supervised Method(ST-ReID)} \\
\hline
TS~\cite{zheng:two-stramTOMM2017} & TOMM'17 & 7.80  & 36.3 & - & - \\
\hline
\multicolumn{2}{c}{Supervised Method(LT-ReID)} \\
\hline
MLFN~\cite{chang:MLFNCVPR18} & CVPR'18 & 6.00 & 41.4 & 6.30 & 10.6 \\
HACNN~\cite{li:HACNNCVPR18} & CVPR'18 & 9.50 & 47.6 & 11.5 & 16.2\\ 
PA~\cite{suh:Part-AlignedECCV18} & ECCV'18 & 6.40 & 19.4 & - & - \\
PCB~\cite{sun:PCBECCV18} & ECCV'18 & 8.20 & 37.1 & - & - \\
MGN~\cite{wang:MGNACMMM18} & MM'18 & 10.8 & 49.0 & 13.9 & 21.5 \\
DG-Net~\cite{zheng:DG-NetCVPR19} & CVPR'19 & 10.6 & 50.1 & 12.6 & 23.5\\
Celeb~\cite{huang:2SF-BPartIJCNN19} & IJCNN'19 & - & - & 14.0 & 26.8 \\
Re-IDCaps~\cite{huang:Re-IDCapsTCVSVT19} & TCSVT'19 & 9.80 & 51.2 & 11.2 & 20.3 \\
RCSANet~\cite{huang:RCSANetCVPR21} & CVPR'21 & 11.9 & 55.6 & 16.7 & 29.5 \\
\hline
\multicolumn{2}{c}{Unsupervised Method} \\
\hline
SpCL~\cite{Ge:NIPS20} & NeurIPS'20 & 4.60 & 39.6 & 3.60 & 5.30 \\
C3AB~\cite{lmkpr} & PR'22 & 4.80 & 41.0 & 3.70 & 5.10 \\
CACL~\cite{TIPlmk} & TIP'22  & 5.10 & 42.3 & 3.60  & 4.30 \\
CC~\cite{dai:ACCV22} & ACCV'22 & 3.40 & 32.8 & 3.20 & 4.40 \\
ICE~\cite{chen:ICCV21} & ICCV'21 & 4.90 & 40.7 & 5.00 & 7.10 \\
PPLR~\cite{cho:CVPR22} & CVPR'22 & 4.80 & 41.30 &  4.30 & 6.20\\
\hline
\bf{MaskCL} & \bf{Ours}  & \bf{6.70} & \bf{47.4} &  \bf{6.70} & \bf{11.7} \\
\hline
\end{tabular}
}
\end{center}
\vspace{-8pt}
\caption{Comparison to other state-of-the-art methods on Celeb-ReID and Celeb-ReID-Light.}
\label{tab:SOTACele}
\end{table}



\begin{table}[!htb]
\small
\vspace{-6pt}
\begin{center}
\setlength{\tabcolsep}{1.8mm}{
\begin{tabular}{cccccc}
\hline		
\multirow{3}{*}{Method} & \multirow{3}{*}{Reference} & \multicolumn{4}{c}{VC-Clothes}\\
\cline{3-6}
& &\multicolumn{2}{c}{C-C}&\multicolumn{2}{c}{General}\\
\cline{3-6}  
& & mAP & R-1 & mAP & R-1\\
\hline
\multicolumn{2}{c}{Supervised Method(ST-ReID)} \\
\hline
PCB~\cite{sun:PCBECCV18} & ECCV'18 &  30.9 & 34.5 & 83.3 & 86.2 \\
\hline
\multicolumn{2}{c}{Supervised Method(LT-ReID)} \\
\hline
HACNN~\cite{li:HACNNCVPR18} & CVPR'18 & 62.2 & 62.0 & 94.3 & 94.7\\
RGA-SC~\cite{zhang:RGASCCVPR20}& CVPR'20 & 67.4 & 71.1 & 94.8 & 95.4\\
FSAM~\cite{hong:FSAMCVPR21} & CVPR'21 & 78.9 & 78.6 & 94.8 & 94.7 \\
CCAT~\cite{ren:CCATIJCNN22} & IJCNN'22 & 76.8 & 83.5 & 85.5 & 92.7 \\
\hline
\multicolumn{2}{c}{Unsupervised Method} \\
\hline
SpCL~\cite{Ge:NIPS20} & NeurIPS'20 & 38.2 & 46.2 & 61.0 & 77.8 \\
C3AB~\cite{lmkpr} & PR'22 & 44.1 & 52.0 & 65.0 & 81.0\\
CACL~\cite{TIPlmk} & TIP'22  & 49.7 & 58.9 & 68.0 &82.4\\
CC~\cite{dai:ACCV22} & ACCV'22 & 25.7 & 31.0 & 45.1 & 62.4 \\
ICE~\cite{chen:ICCV21} & ICCV'21& 28.7 & 34.5 & 51.2 & 69.3 \\
ICE*~\cite{chen:ICCV21} & ICCV'21 & 28.5 & 31.4 & 51.8 & 70.1 \\
PPLR~\cite{cho:CVPR22} & CVPR'22 & 23.1 & 32.5 & 47.7 & 68.1\\

\hline
\bf{MaskCL} & \bf{Ours}  & \bf{61.7} & \bf{71.7} & \bf{73.3} & \bf{87.0} \\
\hline
\end{tabular}
}
\end{center}
\vspace{-5pt}
\caption{Comparison to other state-of-the-art methods on VC-Clothes.
}
\label{tab:SOTAVC}
\vspace{-15pt}
\end{table}






\section{Experiments}
\label{sec:Experiments}



\subsection{Experiment setting}

\myparagraph{Datasets}
\label{p:dataset-and-preprocessing}
We evaluate MaskCL on six clothes change re-id datasets: 
LTCC~\cite{Change:1}, PRCC~\cite{yang2019person}, VC-Clothes~\cite{vcclothes}, 
Celeb-ReID~\cite{celebreid}, 
Celeb-ReID-Light~\cite{celebreidlight} and Deepchange~\cite{Deepchange}.
Table~\ref{tab:dataset} shows an overview of dataset in details.















\myparagraph{Protocols and metrics}
\label{p:protocols-and-metrics}
Different from the traditional short-term person \reid{} setting, 
there are two evaluation protocols for long-term person \reid{}: a) general setting and b) clothes-change setting. Specifically, for a query image, the general setting is looking for cross-camera matching samples of the same identity, while the clothes-change setting additionally demands the same identity with inconsistent clothing.
For LTCC~\cite{Change:1} and PRCC~\cite{yang2019person}, we report performance with both clothes-change setting  and general setting. As for Celeb-ReID~\cite{celebreid}, Celeb-ReID-Light~\cite{celebreidlight} Deepchange~\cite{Deepchange} and VC-Clothes~\cite{vcclothes}, we report the general setting performance.

We use both Cumulated Matching Characteristics (CMC) and mean average precision (mAP) as retrieval accuracy metrics.


\myparagraph{Implementation details} In our MaskCL approach, we use ResNet-50~\cite{he:CVPR16resnet} pre-trained on ImageNet~\cite{Krizhevsky:NIPS12} for both  network branches.
The features dimension $D = 2048$.
We use the output $\x_i$ of the first branch $F(\cdot|\Theta)$ to perform clustering, where $\x_i \in  \RR^D$. 
The prediction layer $G(\cdot|\Psi)$ is a $D \times D$ full connection layer, the fusion layer $R(\cdot|\Omega)$ is a $2D \times D$ full connection layer. 
We optimize the network through Adam optimizer \cite{Kingma:arXiv2014} with a weight decay of 0.0005 and train the network with 60 epochs in total. The learning rate is initially set as 0.00035 and decreased to one-tenth per 20 epochs. 
The batch size is set to 64. 
The temperature coefficient $\tau$ in Eq.~\eqref{eq:softmax-cal} is set to $0.05$ and the update factor $\alpha$ in Eq.~\eqref{eq:update-cluster_1} and \eqref{eq:update-cluster_2} is set to $0.2$. The $K$ in Eq.~\eqref{eq:Curriculumneighbor} is set as 10 on LTCC, Celeb-ReID
and Celeb-ReID-Light, 3 on VC-Clothes and 5 on PRCC, the effect of using a different value of $K$ will be test later. 




\subsection{Comparison to the State-of-the-art Methods} 
\label{subsec:comp-to-sota}

\myparagraph{Competitors}
\label{p:baselines}
To construct a preliminary benchmarking ecosystem and conduct a thorough comparison, we evaluate some \sota{} of short-term unsupervised \reid{} models, which are able to achieve competitive performance under an unsupervised short-term setting, including SpCL~\cite{Ge:NIPS20}, CC~\cite{tanping:arxiv2020}, CACL~\cite{TIPlmk}, C3AB~\cite{lmkpr}, ICE~\cite{ICE}, PPLR~\cite{PPLR}.
We retrain and evaluate these unsupervised methods on long-term datasets, including LTCC, PRCC, Celeb-ReID, Celeb-ReID-Light and VC-Clothes. 
At the same time, we also compare with other supervised long-term person \reid{} methods such as HACNN~\cite{li:HACNNCVPR18}, PCB~\cite{sun:PCBECCV18}, CESD~\cite{qian:CESDACCV20}, RGA-SC~\cite{zhang:RGASCCVPR20}, IANet~\cite{hou:IANETCVPR19}, GI-ReID~\cite{jin:GI-ReIDCVPR22}, RCSANet~\cite{huang:RCSANetCVPR21}, 3DSL~\cite{chen:3DSLCVPR21}, FSAM~\cite{hong:FSAMCVPR21}, CAL~\cite{CCgu:CVPR22}, CCAT~\cite{ren:CCATIJCNN22}. 
The comparison results of the state-of-the-art unsupervised short-term  person \reid{} methods and supervised methods are shown in Tables~\ref{tab:SOTALTCC},~\ref{tab:SOTACele} and~\ref{tab:SOTAVC}.
From the results in these tables we can read that our MaskCL outperforms much better than all unsupervised short-term methods and even comparable to some supervised long-term methods.













Particularly, we can observe that MaskCL yields much higher performance than the short-term unsupervised person re-id methods in clothes-change setting. Yet, the differences are reduced and even vanished in general settings (\eg, ICE results on LTCC).  This further demonstrates the dependency of short-term re-id methods on clothes features for person matching. 
In addition, we discovered that MaskCL performs poorly on certain datasets, such as Celeb-ReID-Light; we will investigate the underlying causes later.


\subsection{Ablation Study}
In this section, we conduct a series of ablation experiments to evaluating each component in MaskCL architecture, \ie, the semantic neighbor contrastive learning module $\mathcal{L}_{N}$,  Bernoulli sampling weight $w$, separately.
In additional, we substitute the feature fusion operation with concatenation operations or a single branch feature. 



\myparagraph{The baseline Setting} In baseline network, we employ the prototypical contrastive learning module and the cross-view contrastive learning module introduced in Section~\ref{sec:Contrastive-Learning-Framework} and train the model with the corresponding loss functions. The training procedure and memory updating strategy are kept the same as MaskCL. 


The ablation study results are presented Table~\ref{tab:ablationstudy}. We can read that when each component is added separately, the performance increases. This verifies that each component contributes to improved the performance.
When using both $\mathcal{L}_{N}$ and the weighting $\mathcal{W}$, we notice a considerable gain in performance compared to using $\mathcal{L}_{N}$ alone. This suggests that improving the selection of neighbors may boost the effectiveness of training the network using $\mathcal{L}_{N}$.








\begin{table}[!htb]
\small
\begin{center}
\vspace{5pt}
\setlength{\tabcolsep}{1.1mm}{
\begin{tabular}{ccccccccc}
\hline		
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Components} &\multicolumn{2}{c}{VC}&\multicolumn{2}{c}{PRCC}&\multicolumn{2}{c}{LTCC}\\
\cline{2-9}  
& $\mathcal{L}_N$ &${w}$&mAP & R-1 &mAP & R-1 &mAP & R-1  \\
\hline
{C-C} \\
\hline
{ Baseline }   &  &   & 58.1 & 66.7   & 51.2 & 41.0  &  10.6 & 19.6   \\
{ +$\mathcal{L}_N$ }   & \checkmark &   & 60.3 & 71.1   & 53.9 &  42.4  & 12.6 & 25.3  \\
{ MaskCL}   & \checkmark & \checkmark  & 61.7 & 71.7  & 55.1 &   43.7  & 12.7 & 25.5  \\
\hline
{General} \\
\hline
{ Baseline }   &  &   & 71.3 & 83.6  & 94.6 &  92.2 & 25.0 & 54.0  \\
{ +$\mathcal{L}_N$ }   & \checkmark &   & 72.5 & 86.3  & 95.4 & 93.0 & 28.0 & 56.6 \\
{ MaskCL}   & \checkmark & \checkmark  & 73.3 & 87.0  & 96.8 & 95.2 & 29.2 & 59.8 \\
\hline

\end{tabular}
}
\vspace{2pt}
\caption{Ablation Study on LTCC, PRCC and VC-Clothes.}
\label{tab:ablationstudy}
\end{center}
\vspace{-15pt}
\end{table}



































\subsection{More Evaluations and Analysis}

\myparagraph{Different operations for semantic neighbor searching}
To determine the requirement of feature fusion, we prepare a set of tests employing different operations, \ie  using $\x$, $\tilde \x$ or $concate(\x|\tilde \x)$, to find the semantic neighbors.
The experimental results are listed in Table~\ref{tab:featurefusion}. We can read that using $\x$ or $concate(\x|\tilde \x)$ in the semantic neighbor searching stage will yield acceptable performance in finding semantic neighbors, but still inferior than the results of using fusion feature $\f$.


\begin{table}[!htb]
\small
\begin{center}
\vspace{-5pt}
\setlength{\tabcolsep}{1.8mm}{
\begin{tabular}{ccccccc}
\hline		
\multirow{2}{*}{Operation} &\multicolumn{2}{c}{VC}&\multicolumn{2}{c}{PRCC}&\multicolumn{2}{c}{LTCC}\\
\cline{2-7}  
& mAP & R-1 &mAP & R-1 &mAP & R-1  \\
\hline
\multicolumn{1}{c}{C-C} \\
\hline
{ $\x$} & 60.8 & 71.2  & 54.0 &  43.3 & 12.4 & 25.0  \\
{ $\tilde \x$} & 63.4 & 74.4  & 52.4 &  42.1 & 9.70 & 20.9  \\
{ $concate(\x | \tilde \x)$} & 60.8 & 71.5  & 52.7 &  40.2 & 12.1 & 21.7 \\
{ MaskCL} & 61.7 & 71.7  & 55.1 &   43.7  & 12.7 & 25.5  \\
\hline
\end{tabular}
}
\end{center}
\vspace{-4pt}
\caption{Experiments Results of Various Semantic Neighbor Searching Operations on LTCC, PRCC, and VC-Clothes.}
\label{tab:featurefusion}
\vspace{-4pt}
\end{table}


Interestingly, we notice of that using $\tilde x$ alone did produce the leading results on dataset VC-Clothes, even exceeding over MaskCL; whereas it produced the worst performance on datasets LTCC and PRCC. 
As demonstrated in Figure~\ref{fig:imagequality}, VC-Clothes is a synthetic dataset with superior pedestrian image quality. Thus the extracted semantic masks are also higher-quality compared to that on LTCC and PRCC.
This observation confirms that rich information on high-level semantic structure may be gleaned using just semantic source especially when the quality of semantic source is sufficient. 




\begin{figure}[!htb]
\begin{center}
\subfigure[PRCC]{\includegraphics[width=0.25\linewidth]{figures/prcc_person.pdf}}
   ~~~~
   \subfigure[LTCC]{\includegraphics[width=0.25\linewidth]{figures/ltcc_person.pdf}}
   ~~~~
  \subfigure[VC-Clothes]{\includegraphics[width=0.26\linewidth]{figures/vc_person.pdf}}
\end{center}
\vspace{-6pt}
   \caption{Comparison on Image Style and Silhouette Mask Quality on datasets LTCC, PRCC and VC-Clothes.}
\label{fig:imagequality}
\vspace{-10pt}
\end{figure}




\myparagraph{Performance Evaluation on Deepchange} 
Deepchange is the latest, largest, and realistic person \reid{} benchmark with clothes change. We conduct experiments to evaluate the performance of MaskCL and SpCL on this dataset, and report the results in Table~\ref{tab:Deepchange-MaskCL}. Moreover, we also listed the results of supervised training methods using the different backbones provided by Deepchange~\cite{Deepchange}.

\begin{table}[!htb]
\small
    \begin{center}
    \vspace{-5pt}
    \setlength{\tabcolsep}{2.2mm}{
    \begin{tabular}{ccccc}
    \hline		
     \multirow{2}{*}{Method}& \multirow{2}{*}{Backbone}&\multicolumn{3}{c}{Deepchange}\\
     \cline{3-5}  
    &  & mAP & R-1 &R-10 \\
    \hline
    \multicolumn{2}{c}{Supervised Method} \\
    \hline
    Deepchange~\cite{Deepchange} &{ResNet-50~\cite{he:CVPR16resnet}} & 9.62 &  36.6 & 55.5     \\
    Deepchange~\cite{Deepchange} &{ResNet101~\cite{he:CVPR16resnet}}  & 11.0 & 39.3 & 57.4 \\ 
    Deepchange~\cite{Deepchange} &ReIDCaps~\cite{huang2019beyond}   & 13.2 & 44.2 & 62.0 \\
    Deepchange~\cite{Deepchange} &ViT B16~\cite{dosovitskiy2020image}  & 14.9 & 47.9 & 67.3 \\
    \hline
    \multicolumn{2}{c}{Unsupervised Method} \\
    \hline
    {SpCL~\cite{Ge:NIPS20}}& ResNet-50\cite{he:CVPR16resnet} & 8.90 & 32.9 &47.2 \\
    {MaskCL}& ResNet-50\cite{he:CVPR16resnet} & 11.8 & 39.7  & 53.7 \\
    \hline
    \end{tabular}
    }
    \end{center}
    \vspace{-5pt}
    \caption{Experimental Results on Deepchange.}
    \label{tab:Deepchange-MaskCL}
    \vspace{-2pt}
    \end{table}






\myparagraph{Visualization}
To gain some intuitive understanding of the performance of our proposed MaskCL, we conduct a set of data visualization experiments on LTCC to visualize selected query samples with the top-10 best matching images in the gallery set, and display the visualization results in Figure~\ref{fig:vision_MaskCL_3}, where we also show the results of a competitive baseline CACL and more visualisation results are available in the {\textit{Supplementary Material}}.
Compared to CACL, MaskCL yields more precise matching results. The majority of the incorrect samples matched by CACL are in the same color as the query sample. These findings imply that MaskCL can successfully learn more clothing invariance features and hence identify more precise matches.

    \begin{figure}[!htb]
        \begin{center}
        \footnotesize
{\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=0.95\columnwidth]{figures/mask-cacl-vc.pdf}}
        \end{center}
        \vspace{-6pt}
        \caption{  
        Visualization of the top-10 best matched images on dataset VC-Clothes of CACL and MaskCL. The images with green and red frames are correctly matched and incorrectly matched images, respectively.
        }
        \label{fig:vision_MaskCL_3}
        \vspace{-10pt}
        \end{figure}   
        








{ 
\myparagraph{Neighbor Searching Selection}
To have some intuitive understanding of the model improvement during training process, 
we visualised the average proportion of correct identity images being sampled by Eq.~\eqref{eq:Bernoulli} from semantic neighbor set $\A$, and disaply the curves in Figure \ref{fig:benulisample}. 
As training continue, the proportion also increase. These trends confirm that MaskCL can consequently increasing similarity between different clusters of the same identity hence sampling more correct identiy instance from semantic neighbor sets.


    \begin{figure}[!htb]
        \begin{center}
        \footnotesize
    \subfigure[LTCC ]{\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=0.48\columnwidth]{figures/ltccsample.pdf}}
    \subfigure[Celeb-ReID Series]{\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=0.48\columnwidth]{figures/celeb.pdf}}  
        \end{center}
        \vspace{-6pt}
        \caption{ Visualization of the average proportion of correct identity images being sampled by Eq.~\eqref{eq:Bernoulli} from semantic neighbor set $\A$ in training process.}
        \label{fig:benulisample}
        \vspace{-10pt}
        \end{figure}   






\section{Conclusion}
\vspace{-5pt}


We addressed a challenging task: unsupervised long-term person \reid{} with clothes change. Specifically, we proposed a semantic mask-driven contrastive learning approach, termed MaskCL, which takes the silhouette masks as the semantic prompts in contrastive learning and finds hierarchically semantic neighbor to driven the training. 
By leveraging the semantic prompt and hierarchically semantic neighbor, MaskCL is able to effectively exploit the invariance within and between RGB images and silhouette masks to learn more effective cross-clothes features. We conducted extensive experiments on five widely-used
re-id datasets with clothes change to evaluate the performance. Experimental results demonstrated the superiority of our proposed approach. To the best of our knowledge, this is the first time, unsupervised long-term person \reid{} problem has been addressed. Our systematic evaluations can serve as a benchmarking ecosystem for the long-term person re-id community.










}

































































































































{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}