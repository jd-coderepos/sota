\documentclass{article}





\usepackage[square,numbers]{natbib}
\usepackage[final]{neurips_2022}










\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}         \usepackage[dvipsnames]{xcolor}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{tabularx}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{multirow}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[title]{appendix}
\usepackage{caption}
\usepackage{float}
\usepackage{comment}



\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{comment}

\newfloat{figtab}{htb}{fgtb}
\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother


\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{color}   


\usepackage{pifont}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\definecolor{antiquewhite}{rgb}{0.98, 0.92, 0.84}



\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{c.f.}} \def\Cf{\emph{C.f.}}
\def\etc{\emph{etc.}} \def\vs{\emph{vs.}}
\def\wrt{w.r.t.} \def\dof{d.o.f.}
\def\etal{\emph{et al.}}


\definecolor{remark}{rgb}{1,.5,0} 
\definecolor{citecolor}{rgb}{0,0.443,0.737} 
\definecolor{linkcolor}{rgb}{0.956,0.298,0.235} 
\definecolor{cyan}{rgb}{0.831,0.901,0.945}


\colorlet{dark-blue}{blue!65!black}
\colorlet{dark-green}{green!55!black}
\colorlet{dark-red}{red!80!black}
\colorlet{dark-yellow}{yellow!90!black}
\colorlet{white-blue}{blue!70!green}
\definecolor{mypink}{RGB}{219, 48, 122}
\hypersetup{
    colorlinks=true,citecolor=citecolor,filecolor=citecolor,linkcolor=linkcolor,urlcolor=magenta
}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\ours}{AdvStyle\xspace}


\title{Adversarial Style Augmentation for Domain Generalized Urban-Scene Segmentation}



\author{Zhun Zhong$^{\textcolor{mypink}{1}}\thanks{Equal contribution. ~~$\dagger$ Corresponding author.}~~^{\dagger}$ \quad Yuyang Zhao$^{\textcolor{mypink}{2} \ast}$ 
  \quad Gim Hee Lee$^{\textcolor{mypink}{2}}$
  \quad Nicu Sebe$^{\textcolor{mypink}{1}}$ \\
  $^{\textcolor{mypink}{1}}$ Department of Information Engineering and Computer Science, University of Trento \\
  $^{\textcolor{mypink}{2}}$ Department of Computer Science, National University of Singapore
}


\begin{document}


\maketitle

\begin{abstract}
In this paper, we consider the problem of domain generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic (source) data. The model is expected to perform well on unseen real (target) domains. Our study finds that the image style variation can largely influence the model's performance and the style features can be well represented by the channel-wise mean and standard deviation of images. Inspired by this, we propose a novel adversarial style augmentation (\textbf{AdvStyle}) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training. The learned adversarial style feature is used to construct an adversarial image for robust model training. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic-to-real semantic segmentation benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that we can achieve the state of the art. Moreover, AdvStyle can be employed to domain generalized image classification and produces a clear improvement on the considered datasets.
\end{abstract}

\section{Introduction}
\label{sec:intro}


Semantic segmentation plays a critical role in autonomous driving, which has achieved impressive improvements with the recent development of deep  networks~\cite{long2015fully,chen2018deeplab,badrinarayanan2017segnet,zhao2022ncdss}. However, these achievements have been mostly  attributed to large-scale labeled segmentation datasets, in which annotating pixel-wise labels is very expensive and time-consuming. Moreover, the model trained on one dataset commonly produces poor performance on unseen datasets captured in different conditions. This degradation phenomenon is mainly caused by domain shifts~\cite{robustnet}, including differences in weather, season, light, etc. 
For instance, the segmentation model trained on the dataset captured in sunny London will have low accuracy when deployed on the streets of Zurich in rainy weather.


\begin{figure*}[!t]
\centering
\includegraphics[width=0.85\linewidth]{figs/motivation.pdf}
\caption{(a) Examples of different datasets. The image styles from different datasets commonly are very different. (b) Style distribution of different datasets. We use image-level mean-variance as the style feature and show that the style distribution gap between different datasets are large. (c) Examples of changing style feature for a GTAV sample, including adding random noise and replacing the style feature with ones of samples from other datasets. (d) mIoU performance of changing styles for GTAV testing set, which is largely reduced after style changing.}
\label{fig:motivation}
\end{figure*}


To address the cross-domain problem, domain adaptation methods~\cite{luo2019taking,zhang2021prototypical} are designed to transfer the knowledge of labeled source data to unlabeled target data. However, one of their main drawbacks is that they require the use of target data during training, which cannot always be accessible in practice.
Another promising line is domain generalization (DG), which focuses on learning a generalizable model using only the labeled source domain.
To reduce the annotating cost and protect data privacy, the existing DG works~\cite{robustnet,DRPC,zhao2022style} in semantic segmentation commonly choose to learn the robust model with synthetic data, \textit{e.g.}, GTAV~\cite{gtav}. In this paper, we focus on this synthetic-to-real DG problem for semantic segmentation.

To eliminate the impact caused by the large domain gap between synthetic and real data, existing solutions mainly aim at augmenting the synthetic source data with extra real-world samples~\cite{DRPC,FSDR} or learning domain-invariant features with carefully designed modules~\cite{ibn,robustnet}. The key idea behind them is to avoid the model overfitting on the source domain. This work follows this idea and introduces a new augmenting approach in the perspective of image style for domain generalized semantic segmentation, which is motivated by the observations in Fig.~\ref{fig:motivation}. 
\textit{First}, when visualizing the samples of different datasets in Fig.~\ref{fig:motivation}\textcolor{black}{(a)} we observe that the image styles are quite different among them, \textit{e.g.}, the road color. The style distribution gap can also be observed in Fig.~\ref{fig:motivation}\textcolor{black}{(b)}.
\textit{Second}, the channel-wise mean and standard deviation of an image, which is called style feature in this paper, can well represent the image style. When changing the style feature, the image style of an example varies while the semantic content is well maintained (see Fig.~\ref{fig:motivation}\textcolor{black}{(c)}).
\textit{Third}, changing the style features of testing samples will largely deteriorate the model performance (see Fig.~\ref{fig:motivation}\textcolor{black}{(d)}). Importantly, when replacing the style features with the ones of CityScapes, which has a large distribution distance to GTAV (see red and green dots in Fig.~\ref{fig:motivation}\textcolor{black}{(b)}), the performance is lower than other style changing cases. This indicates that the model performance on testing data is highly related to the style distribution gap between training and testing data.



Taking the above observations into consideration, we argue that the image style is an important factor that affects the model performance and propose the adversarial style augmentation (\textbf{AdvStyle}) for domain generalized semantic segmentation. Specifically, AdvStyle contains two steps: adversarial style learning and robust model learning. 
In adversarial style learning, we first decompose the training sample into style feature and normalized image. The style feature is regarded as a learnable parameter, which is used to reconstruct a new training example together with the normalized image.
Then, we feed the reconstructed example into the segmentation model and optimize the style feature using the adversarial segmentation loss. The updated style feature is called adversarial style feature and is used to produce hard example in the following step.
In robust model training, we first generate an adversarial example by de-normalizing the normalized image with the learned adversarial style feature. The adversarial image and the original image are then used to train a robust model using the segmentation loss. In AdvStyle, the adversarial image is dynamically generated based on the current model. In this way, the model is always encouraged to update with difficult styles and thus will be more robust to style variations in unseen domains. In Fig.~\ref{fig:aug_show}, we show the comparison between AdvStyle and traditional augmentation methods. Our AdvStyle can significantly improve the model performance on unseen target domains and clearly outperforms the other augmentation methods. To summarize, our contributions are threefold:
\begin{itemize}[leftmargin=.2in]
\item We propose the novel adversarial style augmentation (AdvStyle) for domain generalized semantic segmentation, which can consistently improve the results on unseen real domains. AdvStyle introduces very limited learnable parameters (6-dim feature for each example) and can be easily implemented with different networks and DG methods.
\item Experiments on two synthetic-to-real DG benchmarks demonstrate the effectiveness of the proposed AdvStyle and show that we achieve new state-of-the-art DG performance.
\item We show that AdvStyle can also be applied to single DG in image classification and can produce state-of-the-art accuracy on two datasets.
\end{itemize}


\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/introduction.pdf}
\caption{Illustration of different data augmentation methods. We use GTA5 as the source domain and the ResNet-50 as the backbone. The mIoU given in parentheses is evaluated on CityScapes validation set for the model trained with the corresponding augmentation method.}
\label{fig:aug_show}
\end{figure*}


\section{Related Work}
\label{sec:related-work}


\textbf{Domain Generalization (DG).} To tackle the deficiency of annotated data, DG~\cite{DRPC, FSDR,robustnet,crossnorm,zhao2022style,chen2022maxstyle,zhao2021learning} is introduced to learn a robust model with one or multiple source domains, where the model is expected to perform well on unseen domains. Recent DG works in semantic segmentation mostly use synthetic data as the source domain, which can be automatically generated but have a large distribution gap to real-world datasets. One main stream of DG methods~\cite{DRPC, FSDR} focuses on augmenting training samples with extra real-world data from ImageNet~\cite{imagenet}. Learning domain-invariant features~\cite{robustnet, ibn, crossnorm} is another stream to narrow the domain gap. \cite{ibn} and \cite{robustnet}  leverage instance normalization~\cite{in} and whitening transformation to remove the domain-specific information, respectively.  \cite{crossnorm} exchanges style features of two samples and adjusts style features with the attention mechanism. Different from the above methods, our \ours generates new samples by learning adversarial styles using only the synthetic source data.


\textbf{Adversarial Training in DG.}
Adversarial training~\cite{goodfellow2015explaining} is initially proposed to learn a robust model that can combat imperceptible perturbations. In recent years, adversarial training is applied to single DG in image classification~\cite{volpi2018generalizing,qiao2020learning,qiao2021uncertainty,fan2021adversarially}, by regarding adversarial samples as augmented unseen samples.
\cite{volpi2018generalizing} is the first to introduce adversarial samples in DG by max-min iterative training procedure. Later methods form novel domains with the generated adversarial samples and learn a domain-invariant representation by meta-learning~\cite{qiao2020learning, qiao2021uncertainty} or adaptive normalization~\cite{fan2021adversarially}. Different from them, this paper adopts adversarial training for semantic segmentation and generates adversarial samples in the perspective of image style.

\textbf{Style Variation.}
Style features are widely studied in image translation~\cite{adain, dumoulin2016learned}. By varying the style features, the image style can be changed while semantic content will be maintained.
Inspired by this, recent works focus on generating data of novel distributions by modifying style features, which are used to train a more robust model.
One effective manner is to generate new styles by 
exchanging~\cite{crossnorm,zhao2021source} or mixing styles~\cite{zhou2021mixstyle} between samples. 
On the other hand, new styles can be generated by learnable modules~\cite{wang2021L2D}. Instead, we generate novel styles by adversarial training, which encourages the model to always optimize with hard stylized examples. {This work is also closely related to \cite{bhattad2020unrestricted}, which generates adversarial examples by colorizing. However, it requires a pre-trained colorization model to change the image color, which is much more complex than our AdvStyle. In addition, \cite{bhattad2020unrestricted} aims to impair the performance of models by adversarial examples. In contrast, our AdvStyle leverages the adversarial examples to improve the generalization ability of the segmentation model. This work also has a connection with ``Learning-to-Simulate''~\cite{ruiz2019learning2simulate}. However, \cite{ruiz2019learning2simulate} tries to learn good sets of parameters for an image rendering simulator in actual computer vision applications while we attempt to learn a generalized model for semantic segmentation.}

\section{Method}
\label{sec:method}
\textbf{Problem Definition.} Synthetic-to-real domain generalization (DG) focuses on training a robust model with \textit{one} labeled synthetic domain $\mathcal{S}$, where the model is expected to perform well on  unseen domains $\left\{\mathcal{T}_1, \mathcal{T}_2, \cdots \right\}$ of different real-world distributions. As stated by \cite{volpi2018generalizing}, the DG task can be formulated as solving the worst-case problem:
\begin{equation}
\label{eq:worst}
\min _{\theta} \sup _{\mathcal{T}: D(\mathcal{S}, \mathcal{T}) \leq \rho} \mathbb{E}_{T}\left[\mathcal{L}_{\text {task }}(\theta ; \mathcal{T})\right],
\end{equation}
where $\theta$ is the model parameters and $\mathcal{T}$ is the target domains. $\mathcal{L}_{\text {task}}$ denotes the task-specific loss function, which is the pixel-wise cross-entropy loss in this paper. $D(\mathcal{S}, \mathcal{T})$ denotes the distribution distance between the source domain and target domains in semantic space. It is constrained to be lower than $\rho$ for semantic consistency.
Inspired by Eq.~\ref{eq:worst}, we propose a novel approach to generate a dynamic source domain $\mathcal{S}^{+}$, which can help us to reduce the domain shifts between the synthetic domain $\mathcal{S}$ and real-world domains $\mathcal{T}$ during training.


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\linewidth]{figs/framework.pdf}
\caption{The framework of the proposed adversarial style augmentation.}
\label{fig:framework}
\end{figure*}


\subsection{Overview}

In the introduction, we show that the image style is an important factor that influences the DG performance. In addition, the channel-wise mean and standard deviation of an image, which is called style feature, can well represent the image style. Inspired by that, we propose the adversarial style augmentation approach (\textbf{\ours}) for domain generalized semantic segmentation. \ours can dynamically generate images with new styles during training and effectively improve the generalization ability of the segmentation model. \ours includes two steps: adversarial style learning and robust model training. In adversarial style learning, we first decompose the image into normalized image and style feature and then update the style feature by adversarial segmentation loss. In robust model training, we compose the adversarial image by the normalized image and learned adversarial style feature. The model is then optimized with both original and adversarial images. These two steps are implemented in each training iteration, enabling us dynamically generate hard stylized samples for the current model. Our overall framework is illustrated in Fig.~\ref{fig:framework}.


\subsection{Adversarial Style Learning}

Given an training image $x$ at each iteration, we first compute the channel-wise mean $\mu$ and standard deviation $\sigma$ of $x$ and then obtain the normalized image $\Bar{x}$ by normalization:
\begin{equation}
\label{eq:decompose}
\begin{aligned}
&\mu = \frac{1}{HW}\sum_{h\in H,w \in W} x_{h,w},\\ 
&\sigma = \sqrt{\frac{1}{HW}\sum_{h\in H,w \in W} (x_{h,w}-\mu)^2}, \hspace{1em} \Bar{x} = \frac{x - \mu}{\sigma},
\end{aligned}
\end{equation}
where $H$ and $W$ denote the spatial size of $x$.


After that, we initialize the adversarial style feature $\mu^{+}$ and $\sigma^{+}$ by $\mu$ and $\sigma$, which are regarded as learnable parameters. Then we reconstruct the image with $\mu^{+}$, $\sigma^{+}$ and $\Bar{x}$ and forward it into the network for loss computation. During the backward, the parameters of the network are fixed and the adversarial style feature is updated by:
\begin{equation}
\label{eq:style_update}
\begin{aligned}
\mu^{+} \leftarrow \mu^{+}+\gamma \nabla_{\mu^{+}} \mathcal{L}_{\text{seg}}\left(\theta; \hat{x} \right), \\ \hspace{1em}
\sigma^{+} \leftarrow \sigma^{+}+\gamma \nabla_{\sigma^{+}} \mathcal{L}_{\text{seg}}\left(\theta; \hat{x}\right),
\end{aligned}
\end{equation}
where $\gamma$ is the learning rate for adversarial style learning and $\mathcal{L}_{\text{seg}}$ is the cross-entropy loss. $\hat{x}=\Bar{x} \cdot \sigma^{+} + \mu^{+}$ is the reconstructed image. Notice that the style feature is optimized by the adversarial gradient of $\mathcal{L}_{\text{seg}}$. Indeed, the adversarial style learning process can be iterated multiple times.
In our experiment, we find that one-step adversarial style learning achieves similar results with multi-step ones but is much more efficient. Therefore, we only update the style feature once.

\subsection{Robust Model Training}

Given the learned adversarial style feature ($\mu^{+}$ and $\sigma^{+}$), we use it to generate the adversarial sample $x^{+}$ with the corresponding normalized image:
\begin{equation}
\label{eq:denorm}
    x^{+} = \Bar{x} \cdot \sigma^{+} + \mu^{+}.
\end{equation}
Then the original image $x$ and the generated adversarial image $x^{+}$ are forwarded to the model for optimization, which can be formulated by,
\begin{equation}
\label{eq:final-loss}
\min _{\theta} \mathcal{L}_{\text{seg}}(\theta ; x) + \mathcal{L}_{\text{seg}}(\theta ; x^{+}).
\end{equation}
The detailed training procedure and Pytorch-like pseudo-code can be found in the Appendix.~\ref{sec:alg-pytorch}. During testing, we directly input the original samples into the network without implementing \ours.


\subsection{Discussion} 

Adversarial data augmentation have been studied by several works for DG in image classification. Most of them~\cite{qiao2020learning, volpi2018generalizing} generate pixel-wise perturbations on the training image $x$, which usually require additional constraint loss to guarantee the semantic consistency in Eq.~\ref{eq:worst}. In addition, these works focus on the image classification task where the recognition result is mostly related to global feature. However, in semantic segmentation, the model needs to produce the per-pixel predictions so that it is more difficult to ensure the pixel-wise semantic consistency during adversarial learning. Instead, our \ours varies the style feature of the image, which will maintain the semantic content of most pixels and thus can well guarantee the pixel-wise semantic consistency for semantic segmentation. We conduct experiments in Table~\ref{table:aug}, which demonstrate the superiority of the proposed \ours over pixel-wise adversarial learning. 


\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setup}
\label{sec:seg_imple_details}


\textbf{Datasets and Implementation Details.}
For the synthetic-to-real domain generalization (DG), we use one of the synthetic datasets (GTAV~\cite{gtav} or SYNTHIA~\cite{synthia}) as the source domain and evaluate the model performance on three real-world datasets (CityScapes~\cite{CityScapes}, BDD-100K~\cite{bdd}, and Mapillary~\cite{mapillary}).
The details of the datasets can be found in the Appendix.~\ref{sec:dataset-seg}.
Following \cite{robustnet}, we use DeepLabV3+~\cite{deeplab} as the segmentation model. The segmentation model is constructed by three backbones, including MobileNetV2~\cite{sandler2018mobilenetv2}, ResNet-50~\cite{he2016deep} and ResNet-101. We adopt SGD optimizer with an initial learning rate 0.01, momentum 0.9 and weight decay 5$\times$10$^{-4}$ to optimize the model. The polynomial decay~\cite{liu2015parsenet} with the power of 0.9 is used as the learning rate scheduler. The learning rate of \ours $\gamma$ is set to 3. All models are trained for 40K iterations with a batch size of 16. Four widely used data augmentation techniques are used during training, including color jittering, Gaussian blur, random cropping and random flipping. The input image is randomly cropped to 768$\times$768 for training. The original image size is used for testing.

\textbf{Evaluation Metric.} 
Following~\cite{robustnet}, the model obtained by the last training iteration is used to evaluate the mIoU performance on the three real-world validation sets. For each method, we report the result averaged on 3 runs. When using GTAV as the source domain, we use the 19 shared semantic categories for training and evaluation. When using SYNTHIA as the source domain, we use 16 shared categories for training and evaluation, \textit{i.e.,} ignoring the train, truck, and terrain categories.



\subsection{Evaluation}

\textbf{Effectiveness of \ours on Different Models.} Our \ours is a model-agnostic method, which can be directly applied to different models without modifying the models. To verify the effectiveness of \ours, we apply it to models with different backbones and normalization modules. The backbones include MobileNetV2, ResNet-50 and ResNet-101. The normalization modules include vanilla batch norm (baseline), instance-batch norm (IBN-Net~\cite{ibn}), and instance selective whitening (ISW~\cite{robustnet}).
In Table~\ref{table:different-models}, we show the results of using GTAV as the source domain. 
We can make the following conclusions. First, injecting instance-batch norm (IBN-Net) and instance selective whitening (ISW) modules can consistently improve the performance of the baseline. Second, the proposed \ours can significantly enhance the generalization performance of the baseline model for all backbones. Specifically, the average mIoU is increased from 26.03\%, 27.42\% and 31.47\% to 32.11\%, 37.39\% and 37.34\% for MobileNetV2, ResNet-50 and ResNet-101, respectively. In addition, the baseline with \ours produces higher average mIoU than IBN-Net and ISW for all backbones. Third, when adding \ours, the results of IBN-Net and ISW can be further improved for all settings. For example, when using ResNet-101 as the backbone, \ours improves the average mIoU of IBN-Net and ISW by 6.09\% and 6.53\%, respectively. In Table~\ref{table:synthia}, we show the results of using SYNTHIA as the source domain and also observe clear improvements for \ours. These results verify the prominent advantage of our \ours on different models.


\begin{table}[!t]
\footnotesize
\setlength{\tabcolsep}{0.5pt}
\caption{Evaluation of the proposed AdvStyle on different methods (Baseline, IBN-Net~\cite{ibn} and ISW\cite{robustnet}) and backbones (MobileNetV2~\cite{sandler2018mobilenetv2}, ResNet-50~\cite{he2016deep}, and ResNet-101). All models are trained on the GTAV training set and tested on CityScapes (C), BDD-100K (B), and Mapillary (M) validation sets.
}
\begin{center}
\begin{tabular}{l|C{1cm}C{1cm}C{1cm}>{\columncolor[gray]{1}}C{1cm}|C{1cm}C{1cm}C{1cm}>{\columncolor[gray]{1}}C{1cm}|C{1cm}C{1cm}C{1cm}>{\columncolor[gray]{1}}C{1cm}}
\toprule
Methods& \multicolumn{4}{c|}{MobileNetV2} & \multicolumn{4}{c|}{ResNet-50}& \multicolumn{4}{c}{ResNet-101}\\
\cmidrule[.02cm]{2-13}
GTAV$\rightarrow$ & C & B & M & Mean & C & B & M & Mean & C & B & M & Mean\\
\midrule[.02cm]
\midrule[.02cm] 			
Baseline & 25.92 &25.73 &26.45 &26.03& 28.95   & 25.14      & 28.18      & 27.42& 32.97		&	30.77	&		30.68	&		31.47\\
\textbf{+AdvStyle}  & \bf 31.81	&	\bf	33.01	&\bf		31.50&\bf 32.11 & \bf 39.62	&	\bf	35.54&		\bf	37.00		&\bf	37.39& \bf 39.52&	\bf		36.39		&\bf	36.10	&\bf	37.34\\ 
\midrule[.02cm]
IBN-Net& 30.14 &27.66 &27.07 &28.29&33.85  & 32.30      & 37.75      & 34.63 &37.37	&		34.21		&	36.81	&		36.13\\ 
\textbf{+AdvStyle}  & \bf 32.45	& \bf	31.55	&	\bf 33.09&\bf 32.36&\bf 39.32&	\bf		36.42	&	\bf	40.82		&\bf	38.85& \bf 44.04&	\bf		39.96	&	\bf	42.67	&	\bf	42.22\\ 
\midrule[.02cm]
ISW& 30.86 &30.05 &30.67 &30.53&36.58    & 35.20 & 40.33 & 37.37 & 37.20	&		33.36	&		35.57	&		35.38\\ 
\textbf{+AdvStyle} & \bf 33.23	&\bf		31.84	&	\bf	32.00			&\bf 32.36 & \bf 39.60	&	\bf	38.59	&		\bf 41.89&	\bf		40.03 & \bf 43.44	&	\bf	40.32	&		\bf 41.96	&	\bf	41.91\\ 
\bottomrule
\end{tabular}
\end{center}
\label{table:different-models}
\end{table}


\begin{table}[!t]
\setlength{\tabcolsep}{3pt}
\caption{Results of using SYNTHIA as the source domain. The backbone is ResNet-101.} 
\begin{center}
\footnotesize
\begin{tabular}{l|c|c|c|c}
\toprule
Methods (SYNTHIA$\rightarrow$) & \multicolumn{1}{c|}{CityScapes} & \multicolumn{1}{c|}{BDD} & \multicolumn{1}{c|}{Mapillary} & \multicolumn{1}{c}{Mean}\\
\midrule
\midrule
{\cellcolor[gray]{1}}Baseline & 34.94		&	21.96	&		27.94&28.28 \\
\textbf{Baseline+AdvStyle}& \bf 37.59&	\bf 27.45&	\bf	31.76&	\bf 32.27\\
\cmidrule[.0001in]{1-5}
{\cellcolor[gray]{1}}IBN-Net& 35.83&			23.62		&	28.88	&		29.44\\
{\cellcolor[gray]{1}}\textbf{IBN-Net+AdvStyle} & \bf 38.72	&	\bf	28.55&	\bf 33.59&	\bf 33.62\\
\cmidrule[.0001in]{1-5}
{\cellcolor[gray]{1}}ISW& 35.27&			23.54	&	26.72	&		28.51\\
{\cellcolor[gray]{1}}\textbf{ISW+AdvStyle} &\bf 39.74&	\bf	28.33&\bf	32.87&	\bf 33.65\\
\bottomrule
\end{tabular}
\end{center}
\label{table:synthia}
\end{table}




\textbf{Comparison of Different Augmentation Techniques.} We investigate the impact of different augmentation methods, including color jittering, Gaussian blur, AdvPixel~\cite{volpi2018generalizing} and the proposed \ours. AdvPixel is a state-of-the-art method for domain generalized image classification. The main difference between AdvPixel and \ours is that AdvPixel learns pixel-wise adversarial example while \ours learns style-wise adversarial example. We reproduce AdvPixel in our setting and select the adversarial learning rate (=10) that achieves the best performance. The random cropping and random flipping are used in default. 

Results in Table~\ref{table:aug} show that all four augmentation methods can improve the generalization performance. Importantly, our \ours produces significant improvement compared to other three methods. Specifically, when using \ours, the average mIoU is increased from 22.91\% to 35.32\%. This improvement is about 8\% higher than the other 3 methods. Moreover, \ours is well complementary to color jittering and Gaussian blur. When combining these three methods, the mIoU is further improved in all target domains. Compared to AdvPixel, our \ours achieves clearly higher performance, no matter using color jittering and Gaussian blur. This demonstrates the advantage of learning adversarial style in domain generalized semantic segmentation.




\textbf{Comparison of Different Style-Aware Methods.} In Table~\ref{table:style-variation}, we compare \ours with three style-aware augmentation methods, including MixStyle~\cite{zhou2021mixstyle}, CrossStyle~\cite{crossnorm} and RandStyle. MixStyle mixes the styles of two samples with a convex weight while CrossStyle directly swaps the styles of two samples. RandStyle can be regarded a reduction of our \ours, which randomly adds Gaussian noise into the style feature. All style-aware methods are implemented on the image-level for fair comparison. We can find that (1) all style-aware methods can consistently improve the performance on all target domains and (2) \ours achieves the best results. The first finding verifies the effectiveness of augmenting image styles and the second finding shows the benefit of learning adversarial styles over other style-aware methods for domain generalized semantic segmentation.



\textbf{Parameter Analysis on the Adversarial Learning Rate.}
The proposed \ours has one important hyperparameter, \textit{i.e.}, the adversarial learning rate $\gamma$. To study the impact of $\gamma$, we vary it in the range of $[0.1, 40]$. Results in Fig.~\ref{fig:parameter} show that \ours can significantly improve the performance on all target domains even with a small value of $\gamma$. The best results are achieved when $\gamma$ is between 1 and 10. Assigning a too large value to $\gamma$ (\textit{e.g.}, 40) may produce unrealistic styles and thus hampers the model training.


\begin{table}[t]
\begin{minipage}{.5\linewidth}
\caption{Comparison of different augmentations. Source: GTAV; Backbone: ResNet-50. CJ: Color Jittering, GB: Gaussian Blur, AP: AdvPixel, Ours: AdvStyle.} 
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\begin{tabular}{cccc|c|c|c|c}
\toprule
CJ & GB & AP & Ours& \multicolumn{1}{c|}{CityScapes} & \multicolumn{1}{c|}{BDD} & \multicolumn{1}{c|}{Mapillary} & \multicolumn{1}{c}{Mean}\\
\midrule
\midrule
- & - & - & -&21.64		&	22.85&		24.22&	22.91\\
\midrule
\cmark & - & -& - &26.36&23.82&26.33&25.50\\
- & \cmark & -& - &25.77 &24.05 &	26.71 &	25.51\\
- & - &\cmark & -&23.34		&	28.42	&		30.64&			27.46\\
- & - &- &\cmark&\bf 37.51&\bf 33.74&\bf 34.73&\bf 35.32 \\
\midrule
\cmark &\cmark& - &-&28.95 & 25.14&28.18&27.42\\
\cmark &\cmark& \cmark &-&35.42&		33.28&		33.23		&	33.97\\
\cmark &\cmark& - &\cmark&\bf 39.62&\bf 35.54&\bf 37.00&\bf 37.39\\
\bottomrule
\end{tabular}
\label{table:aug}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{.46\linewidth}
\caption{Comparison of different style-aware methods. Source: GTAV; Backbone: ResNet-50.}
\centering 
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c|c}
\toprule
Methods & \multicolumn{1}{c|}{CityScapes} & \multicolumn{1}{c|}{BDD} & \multicolumn{1}{c|}{Mapillary} & \multicolumn{1}{c}{Mean}\\
\midrule
\midrule
Baseline & 28.95&25.14&28.18&27.42 \\
\midrule
RandStyle & 33.40	&		34.14 &			31.67 &			33.07 \\
MixStyle & 35.53	&	32.41		&	35.87		&	34.60\\
CrossStyle & 37.26	&	32.40	&		34.09	&		34.58 \\
\bf AdvStyle&\bf 39.62&\bf 35.54&\bf 37.00&\bf 37.39\\
\bottomrule
\end{tabular}
\label{table:style-variation}
\end{minipage}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figs/param4.pdf}
\caption{Parameter analysis on the adversarial learning rate.}
\label{fig:parameter}
\end{figure}





\subsection{Comparison with State-of-The-Art Methods}

In Table~\ref{table:sota}, we compare our method with state-of-the-art DG methods in semantic segmentation, including IBN-Net~\cite{ibn}, SW~\cite{pan2019switchable}, IterNorm~\cite{huang2019iterative}, ISW~\cite{robustnet}, DPRC~\cite{DRPC} and FSDR~\cite{FSDR}. The source domain is GTAV and the backbones are ResNet-50 and ResNet-101. Note that, since different methods use different segmentation networks (\textit{e.g.}, DeepLabV2~\cite{chen2018deeplab}, DeepLabV3+~\cite{deeplab} and FCN~\cite{long2015fully}), different training sets (\textit{e.g.}, the whole GTAV and the training set of GTAV), different training 
strategies (\textit{e.g.}, learning rate and optimizer) , different auxiliary data (\textit{e.g.}, ImageNet samples) and different evaluation manners (\textit{e.g.}, the best model and the last model), it is hard to compare them in an absolutely fair way. We show the results of each method as well as the absolute gain against the corresponding baseline. 



\begin{table}[!t]
\caption{Comparison with state-of-the-art domain generalization methods. All models use the GTAV as the source domain. For each backbone, models with the same ID are implemented with the same baseline. 
Models of ``ID=I, II and IV'' use the whole set (24,966) for training while models of ``ID=III'' only use the training set (12,403). The absolute gain of each model is calculated over the corresponding baseline. $\S$ denotes extra using the ImageNet images. $^*$ indicates using the best trained checkpoints for evaluating each target domain.} 
\centering
\footnotesize
\setlength{\tabcolsep}{2.3pt}
\begin{tabular}{l|c|l|cc|cc|cc|cc}
\toprule
Net& ID& Methods (GTAV)  & \multicolumn{2}{c|}{CityScapes} & \multicolumn{2}{c|}{BDD} & \multicolumn{2}{c|}{Mapillary} &\multicolumn{2}{c}{Mean} \\
\midrule
\multicolumn{1}{l|}{\multirow{12}{*}{\rotatebox{90}{\textbf{ResNet-50}}}} &I&{\cellcolor[gray]{1}}Baseline & 22.20   & - & \multicolumn{2}{c|}{\multirow{2}{*}{N/A}}  & \multicolumn{2}{c|}{\multirow{2}{*}{N/A}} & \multicolumn{2}{c}{\multirow{2}{*}{N/A}} \\ 
&I&{\cellcolor[gray]{1}}IBN-Net & 29.60  & 7.40 $\uparrow$ & & &  & &&\\ 
\cmidrule{2-11}
&II&{\cellcolor[gray]{1}}Baseline$^*$ & 32.45 & - & 26.73 & - & 25.66 & - & 28.28&-\\ 
&II&{\cellcolor[gray]{1}}DRPC$^{\S*}$& 37.42 & 4.97$\uparrow$& 32.14 & 5.41$\uparrow$ & 34.12 &   8.46$\uparrow$ & 34.56& 6.28$\uparrow$\\
\cmidrule{2-11}
&III&{\cellcolor[gray]{1}}Baseline& 28.95 & - & 25.14 & - & 28.18  &  -& 27.42&-\\
&III&{\cellcolor[gray]{1}}\textbf{Baseline+AdvStyle}& \bf 39.62 &10.67$\uparrow$& 35.54& 10.4$\uparrow$& 37.00&8.82$\uparrow$&37.39&9.97$\uparrow$\\
\cmidrule[.0001in]{2-11}
&III&{\cellcolor[gray]{1}}SW& 29.91 &0.96$\uparrow$&27.48 &2.34$\uparrow$&29.71&1.53$\uparrow$&29.03&1.61$\uparrow$\\
&III&{\cellcolor[gray]{1}}IterNorm& 31.81 &2.86$\uparrow$&32.70 &7.56$\uparrow$&33.88&5.7$\uparrow$&32.79&5.37$\uparrow$\\
\cmidrule[.0001in]{2-11}
&III&{\cellcolor[gray]{1}}IBN-Net& 33.85 & 4.90$\uparrow$&32.30 & 7.16$\uparrow$&37.75 & 9.57$\uparrow$&34.63&7.21$\uparrow$\\
&III&{\cellcolor[gray]{1}}\textbf{IBN-Net+AdvStyle} & 39.32&10.37$\uparrow$&			36.42&11.28$\uparrow$&		40.82	&12.64$\uparrow$	&	38.85&11.43$\uparrow$\\
\cmidrule[.0001in]{2-11}
&III&{\cellcolor[gray]{1}}ISW& 36.58 & 7.63$\uparrow$&35.20 & 10.06$\uparrow$ & 40.33 & 12.15$\uparrow$&37.37&9.95$\uparrow$\\
&III&{\cellcolor[gray]{1}}\textbf{ISW+AdvStyle} & 39.60	&10.65$\uparrow$	&	\bf 38.59	&13.45$\uparrow$	&	\bf 41.89&13.71	$\uparrow$&\bf 40.03 &12.61$\uparrow$\\
\midrule
\multicolumn{1}{l|}{\multirow{11}{*}{\rotatebox{90}{\textbf{ResNet-101}}}}&I&{\cellcolor[gray]{1}}Baseline${^*}$ & 33.4   & - & 27.3 & -  & 27.9 & -&29.53&-\\ 
&I&{\cellcolor[gray]{1}}IBN-Net${^*}$& 40.3 & 6.9$\uparrow$& 35.6 & 8.3$\uparrow$& 35.9 &8.0$\uparrow$&37.26&7.73$\uparrow$\\
&I&{\cellcolor[gray]{1}}FSDR$^{\S*}$ & 44.8 & 11.4$\uparrow$& 41.2 & 13.9$\uparrow$& 43.4  & 15.5$\uparrow$&43.13&13.6$\uparrow$\\
\cmidrule{2-11}
&II&{\cellcolor[gray]{1}}Baseline${^*}$& 33.56 & -& 27.76 & - & 28.33 & - &29.88&-\\ 
&II&{\cellcolor[gray]{1}}DRPC$^{\S*}$ & 42.53 &8.97$\uparrow$  & 38.72 & 10.96$\uparrow$ & 38.05&   9.72$\uparrow$ &39.76&9.88$\uparrow$\\ 
\cmidrule{2-11}
&III&{\cellcolor[gray]{1}}Baseline& 32.97&-&30.77&-&30.68&-&31.47&-\\
&III&{\cellcolor[gray]{1}}\textbf{Baseline+AdvStyle}& 39.52&6.55$\uparrow$&36.39&5.62$\uparrow$&36.10&5.42$\uparrow$&37.34&5.87$\uparrow$\\
\cmidrule[.0001in]{2-11}
&III&{\cellcolor[gray]{1}}IBN-Net& 37.37&4.40$\uparrow$&34.21&3.44$\uparrow$&36.81&6.13$\uparrow$&36.13&4.66$\uparrow$\\
&III&{\cellcolor[gray]{1}}\textbf{IBN-Net+AdvStyle} & 44.04&11.07$\uparrow$&39.96&9.19$\uparrow$&42.67&11.99$\uparrow$&42.22&10.75$\uparrow$\\
\cmidrule[.0001in]{2-11}
&III&{\cellcolor[gray]{1}}ISW& 37.20&4.23$\uparrow$&33.36&2.59$\uparrow$&35.57&4.89$\uparrow$&35.38&3.91$\uparrow$\\
&III&{\cellcolor[gray]{1}}\textbf{ISW+AdvStyle} &43.44&10.47$\uparrow$&40.32&9.55$\uparrow$&41.96&11.28$\uparrow$&41.91&10.44$\uparrow$\\
&III&{\cellcolor[gray]{1}}\textbf{ISW+AdvStyle}$^*$ & \bf 45.62&12.65$\uparrow$&\bf 41.71&10.97$\uparrow$& \bf 46.69&16.01$\uparrow$&\bf 44.67&13.20$\uparrow$\\
\cmidrule[.0001in]{2-11}
&IV&{\cellcolor[gray]{1}}ISW& 37.51&-&33.54&-&36.12&-&35.72&-\\
&IV&{\cellcolor[gray]{1}}\textbf{ISW+AdvStyle} &\bf 44.51&-&\bf39.27&-&\bf 43.48&-&\bf 42.42&-\\
\bottomrule
\end{tabular}
\label{table:sota}
\end{table}


From Table~\ref{table:sota}, we can make the following conclusions. \textbf{First}, when using the same baseline model, adding \ours can produce better results than IBN-Net, SW, IterNorm and ISW. Moreover, when applying \ours to IBN-Net or ISW, we achieve new state-of-the-art performance for both ResNet-50 and ResNet-101. 
\textbf{Second}, when compared across baselines, ``Baseline+\ours'' achieves the state-of-the-art mIoU for ResNet-50. On the other hand, when using ResNet-101 as the backbone, ``IBN-Net+\ours'' produces higher results than DRPC and comparable results with FSDR. Importantly, both FSDR and DRPC use extra ImageNet images and select the best training checkpoints for each target domain. Instead, ``IBN-Net+\ours'' only utilizes the source data and uses the last training checkpoint to evaluate all target domains. 
\textbf{Third}, when using the best checkpoint for evaluation, we (``ISW+\ours'') produce better performance than DRPC and FSDR, leading to the new state-of-the-art results under the ``best checkpoint setting''. \textit{Even so, we argue that it is more reasonable to use the last checkpoint for evaluating all target domains. This is because we can not always have the right labeled validation sets to select the best model for unseen domains in practice.}

{To make more fair comparisons with the state-of-the-art methods, we also use the whole set of GTAV for training ISW and ``ISW+AdvStyle''. The results with ResNet-101 are reported in the last two rows of Table~\ref{table:sota}. We can find that using the whole set of GTAV can produce higher results on the CityScapes and the Mapillary datasets.}



\subsection{Performance under Multi-Source Setting}

{In Table~\ref{table:multi-source}, we evaluate the models under the multi-source domain generalization setting, where both GTAV and SYNTHIA are used as the source domains. The compared methods include baseline~\cite{robustnet}, IBN-Net~\cite{ibn}, ISW~\cite{robustnet}, and our \ours. Clearly, AdvStyle consistently improves the results of ISW, further verifying the effectiveness of the proposed AdvStyle.}

\begin{table}[t]
\centering
\footnotesize
\caption{Results of using GTAV and SYNTHIA as the source domains. The backbone is ResNet-50.}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c|c}
\toprule
Methods (GTAV+SYNTHIA$\rightarrow$) & \multicolumn{1}{c|}{CityScapes} & \multicolumn{1}{c|}{BDD} & \multicolumn{1}{c|}{Mapillary} & \multicolumn{1}{c}{Mean}\\
\midrule
{\cellcolor[gray]{1}}Baseline~\cite{robustnet} & 35.46  & 25.09& 31.94  & 30.83 \\
{\cellcolor[gray]{1}}IBN-Net~\cite{ibn}& 35.55  & 32.18 & 38.09   &35.27\\
{\cellcolor[gray]{1}}ISW~\cite{robustnet}& 37.69  & 34.09& 38.49 &36.75 \\
\midrule
{\cellcolor[gray]{1}}\textbf{ISW+AdvStyle} &\bf 39.29 & \bf 39.26& \bf 41.14& \bf 39.90\\
\bottomrule
\end{tabular}
\label{table:multi-source}
\end{table}


\begin{figure*}[!t]
\centering
\includegraphics[width=.9\linewidth]{figs/prediction_visual.pdf}
\caption{Qualitative comparison of segmentation results. Source: GTAV; Backbone: ResNet-50.}
\label{fig:seg_vis}
\end{figure*}





\subsection{Visualization}

\textbf{Qualitative Comparison of Segmentation Results.} In Fig.~\ref{fig:seg_vis}, we compare the segmentation  results  for different methods on target domains. It is clear that, the proposed \ours can consistently improve the semgentation results for baseline, IBN and ISW models, especially for the easily-confused classes, \textit{e.g.}, road \textit{vs} sidewalk and building \textit{vs} sky. 


\textbf{t-SNE of Styles.} In Fig.~\ref{fig:style_tsne}, we visualize the style features generated by \ours during the training phase for the GTAV training set, where ResNet-50 is used as the backbone. 
{We can find that AdvStyle can continuously generate new style features that are different from the original distribution. The new style features have the chance to be located at the distributions of other datasets during the training process.
Moreover, \ours will also generate styles that are out of the distributions of the four datasets (G, C, B, M) and may appear in other unseen domains.}
The visualization results further demonstrate that \ours can encourage the model to meet more diverse and unseen styles during training, leading to a more robust model.





\subsection{Evaluation on Image Classification Task}


To verify the versatility of the proposed \ours, we evaluate it on single DG in image classification. Experiments are conducted on two popular DG datasets, \textit{i.e.}, Digits and PACS. The details of the datasets and implementation can be found in in the Appendix.~\ref{sec:details-of-single}.


\textbf{Results on Digits.} In Table~\ref{tab:digits}, we compare with the baseline (ERM~\cite{vapnik2013nature}) and 6 state-of-the-art methods, including CCSA~\cite{motiian2017unified}, d-SNE~\cite{xu2019d}, JiGen~\cite{carlucci2019domain}, ADA~\cite{volpi2018generalizing}, M-ADA~\cite{qiao2020learning} and ME-ADA~\cite{zhao2020maximum}. For \ours, we implement it with ERM and ME-ADA. It is clear that, our \ours can significantly improve the accuracy of ERM on all target domains. When applying \ours to the state-of-the-art method (ME-ADA), the performance is improved by a large margin, \ie, 8.0\% in average accuracy.


\textbf{Results on PACS.} We compare with the baseline (ERM~\cite{vapnik2013nature}) and three state-of-the-art DG methods, including JiGen~\cite{carlucci2019domain}, RSC~\cite{huangRSC2020} and L2D~\cite{wang2021L2D}. We reproduce JiGen, RSC and L2D with their official source codes. All methods use the same baseline (ERM). Results in Table~\ref{tab:pacs} show that JiGen and RSC produce limited improvements. Instead, our \ours can significantly increase the accuracy on all domains for both ERM and RSC.
Compared to the recent published work (L2D), our method (ERM+\ours) outperforms it by 2.4\% in average accuracy. 
In addition, \ours can also be applied to L2D and yields an improvement of 6.3\% in average accuracy.


The results on Digits and PACS validate that \ours can also be effectively applied to single domain generalized image classification and can achieve state-of-the-art accuracy.



\begin{figure*}[!t]
\centering
\includegraphics[width=.9\linewidth]{figs/style_tsne.pdf}
\caption{t-SNE visualization of adversarial style features during training.}
\label{fig:style_tsne}
\vspace{-.1in}
\end{figure*}


\begin{table}[t]
\begin{minipage}{0.5\linewidth}  
\centering
\caption{Accuracy of single domain generalization on Digits. MNIST is used as the training set, and the results on different testing domains are reported in different columns.}
\setlength{\tabcolsep}{2pt}
\footnotesize
\resizebox{0.99\columnwidth}{!}{
\begin{tabular}{@{}lccccclllllll@{}}\toprule
Method & SVHN & MNIST-M & SYN & USPS & Avg.\\ \midrule
ERM & 27.8 & 52.7 & 39.7  & 76.9  & 49.3 \\
CCSA & 25.9 & 49.3 & 37.3 & 83.7 & 49.1 \\
d-SNE & 26.2 & 51.0 & 37.8 & {\bf 93.2} & 52.1 \\
JiGen & 33.8 & 57.8 & 43.8 & 77.2 & 53.1 \\
ADA & 35.5 & 60.4 & 45.3 & 77.3 & 54.6 \\
M-ADA& 42.6 & 67.9 & 49.0 & 78.5 & 59.5 \\
ME-ADA& 42.6 & 63.3 & 50.4 & 81.0 & 59.3 \\
\midrule
\bf ERM+AdvStyle & 50.4& 73.4& 58.7& 81.6& 66.0 \\
\bf ME-ADA+AdvStyle &\bf 55.5& \bf 74.1& \bf 59.3 & 80.1 & \bf 67.3 \\
\bottomrule
\end{tabular}
}
\label{tab:digits}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}{0.45\linewidth}  
\footnotesize
    \setlength{\tabcolsep}{2.8pt}
    \centering
    \caption{Accuracy of single domain generalization on PACS. One domain (name in column) is used as the training (source) data and the other domains are used as the testing (target) data.}
    \begin{tabular}{c|cccc|c}
    \toprule
         Method & Art. & Car.& Ske. & Pho. & Avg.  \\
         \midrule
         ERM& 67.4 & 74.4 &51.4&42.6&58.9\\
         JiGen& 69.1& 74.6 & 52.4& 41.5&59.4\\
         RSC& 68.8 &74.5 &53.6 &41.9 & 59.7\\
         L2D&74.3&77.5&54.4&45.9&63.0\\
         \midrule
         \bf ERM+AdvStyle & {75.8} & 76.6 & 58.1& 51.1 &65.4\\
         \bf RSC+AdvStyle & 75.1& {78.0} & \textbf{58.9} & {55.5} & 66.8\\
         \bf L2D+AdvStyle & \bf 80.6& \textbf{78.4} & {58.3} & \textbf{59.7} & \bf 69.3\\
    \bottomrule
    \end{tabular}
\label{tab:pacs}
\end{minipage}
\end{table}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose a novel augmentation approach, called adversarial style augmentation (\textbf{\ours}), for domain generalization (DG) in semantic segmentation. \ours dynamically generates hard stylized images by learning adversarial image-level style feature, which can encourage the model learning with more diverse samples. With \ours, the model can refrain from the problem of overfitting on the source domain and thus can be more robust to the style variations of unseen domains. \ours is easy to implement and can be directly integrated with different models without modifying the network structures and learning strategies. Experiments on two synthetic-to-real settings show that \ours can largely improve the generalization performance and achieve state-of-the-art performance. In addition, \ours can be employed to single DG in image classification and obtain significant improvement. 

\section*{Acknowledgement}
This work is supported by the EU H2020 project AI4Media (No. 951911) and the PRIN project PREVUE  (Prot. 2017N2RK7K).



\newpage

\small
\bibliography{reference.bib}
\bibliographystyle{plainnat}

\appendix


\section{Details of Datasets in Domain Generalized Semantic Segmentation}
\label{sec:dataset-seg}
For the synthetic-to-real domain generalization (DG), we use one of the synthetic datasets (GTAV~\cite{gtav} or SYNTHIA~\cite{synthia}) as the source domain and evaluate the model performance on three real-world datasets (CityScapes~\cite{CityScapes}, BDD-100K~\cite{bdd}, and Mapillary~\cite{mapillary}). 

\textbf{Synthetic datasets.} 
GTAV~\cite{gtav} contains 24,966 images with the size of 1914$\times$1052. It is splited into 12,403, 6,382, and 6,181 images for training, validating, and testing. SYNTHIA~\cite{synthia} contains 9,400 images of 960$\times$720, where 6,580 images are used for training.

\textbf{Real-world datasets.}
We use the validation sets of the three real-world datasets for evaluation. CityScapes~\cite{CityScapes} contains 500 validation images of 2048$\times$1024, collected primarily in Germany. BDD-100K~\cite{bdd} and Mapillary~\cite{mapillary} contain 1,000 validation images of 1280$\times$720  and 2,000  validation images of 1920$\times$1080, respectively.

\section{Details of Single Domain Generalization in Image Classification}
\label{sec:details-of-single}

\textbf{Digits} includes five domains (MNIST~\cite{mnist}, SVHN~\cite{svhn}, MNIST-M~\cite{mnist-m-syn}, SYN~\cite{mnist-m-syn}, and USPS~\cite{USPS}) of 10 classes. We use MNIST as the source domain and evaluate the model performance on the other 4 domains. Following ADA~\cite{volpi2018generalizing}, we use the ConvNet architecture~\cite{mnist} as the model and use Adam optimizer with learning rate 10$^{-4}$ for optimization. The overall training iteration is set to 10,000 with a batch size of 32. We set the learning rate of \ours to 20,000\footnote{Due to the absent of batch normalization layer, the gradient is very small on the style feature. Therefore, we set a large learning rate for \ours.}. 

\textbf{PACS}~\cite{pacs} contains four domains (Artpaint, Cartoon, Sketch, and Photo) of 7 classes. For evaluation, we select one of them as the source domain and the other domains as the target domains. Following RSC~\cite{huangRSC2020}, we use the ResNet18~\cite{he2016deep} pretrained on ImageNet~\cite{imagenet} as the backbone and add a fully-connected layer as the classification head. We train the model by SGD optimizer. The learning rate is initially set to 0.004 and divided by 10 after 24 epochs. The model is trained for 30 epochs in total with a batch size of 128. The learning rate of \ours is set to 3.

\textbf{Baseline.} The baseline model is the vanilla empirical risk minimization (ERM)~\cite{vapnik2013nature}, which directly uses the source domain to train the model with classification loss.


\section{Position of AdvStyle}
We inject \ours at different positions (0-4) to verify the effectiveness of image level augmentation. 0-th indicates the image level. 1st-4th indicate the outputs of 1st-4th layer of ResNet, respectively. Results are shown in the Table below.
We can find that injecting \ours at 0th-2nd positions clearly improves the performance and the best result is achieved by applying at 0-th position. Moreover, applying \ours at a deep layer (\textit{e.g.}, 3rd or 4th) fails to improve or even hurts the performance, since more semantic content will be captured instead of styles as the layer deepens.
\begin{table}[h]
\centering
\footnotesize
\caption{Impact of injecting \ours at different positions.} 
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
Position & N/A &  0 & 1 & 2 & 3 & 4 \\
\midrule
Mean & 27.42 &  \bf 37.39 & 34.76 & 31.15 & 27.44 & 26.92\\
\bottomrule
\end{tabular}
\label{table:position}
\end{table}



\section{Comparison of Adversarial Augmentations}
AdvPixel~\cite{volpi2018generalizing} is a state-of-the-art method for domain generalized image classification. The main difference between AdvPixel and \ours is that AdvPixel learns pixel-wise adversarial example while \ours learns style-wise adversarial example. 
The model needs to produce the per-pixel predictions in semantic segmentation. In such a context, AdvPixel may distort the semantic content of original pixels during the pixel-wise adversarial learning. Instead, \ours varies the style feature of the image while retaining the semantic content of most pixels. Therefore, \ours can well guarantee the pixel-wise semantic consistency, making it more suitable for augmenting samples of segmentation. 
As shown in Table~\ref{table-sup:aug}, both \ours and AdvPixel improve the performance, while \ours outperforms AdvPixel by 3.42\% in mean mIoU. More interestingly, AdvPixel can serve to enhance \ours. We randomly select one adversarial augmentation from AdvPixel and \ours at each iteration. The performance yields an improvement of 0.81\% in mean mIoU. The above results verify the effectiveness of adversarial augmentations and the superiority of \ours.


\begin{table}[!ht]
\caption{Comparison of adversarial augmentations. Source: GTAV; Backbone: ResNet-50. CJ: Color Jittering, GB: Gaussian Blur, AP: AdvPixel, Ours: AdvStyle.} 
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccc|c|c|c|c}
\toprule
CJ & GB & AP & Ours& \multicolumn{1}{c|}{CityScapes} & \multicolumn{1}{c|}{BDD} & \multicolumn{1}{c|}{Mapillary} & \multicolumn{1}{c}{Mean}\\
\midrule
\cmark &\cmark& - &-&28.95 & 25.14&28.18&27.42\\
\cmark &\cmark& \cmark &-&35.42&		33.28&		33.23		&	33.97\\
\cmark &\cmark& - &\cmark& 39.62&35.54&\bf 37.00&37.39\\
\cmark &\cmark& \cmark &\cmark&\bf 40.65	&		\bf 37.16&	36.77& \bf 38.20\\
\bottomrule
\end{tabular}
\label{table-sup:aug}
\end{table}

\section{Variants of \ours}
In this section, we investigate two variants of \ours, which can further demonstrate the versatility of \ours. Also, we hope to provide some inspirations for future work.

\textbf{\ours in local patches.}
\ours can be applied to not only the whole image but also the local patches.
Specifically, we split each image into 4 patches evenly (top left, top right, bottom left, and bottom right), and regard the channel-wise mean and standard deviation of each patch as learnable parameters (four 6-dim features). Then the model is trained in the same way as AdvStyle. As shown in the Table~\ref{table:variants}, AdvStyle-Patches can further improve the performance on BDD and Mapillary. However, the mean improvement over all domains is marginal. 


\textbf{\ours in LAB color space.}
\ours is applied to RGB space in this paper, but it can also be applied to other color space, \eg, LAB color space.
To verify this, we first convert the RGB-sample to the counterpart LAB-sample and obtain the learnable mean and standard deviation. Then, we reconvert the LAB-sample to RGB-sample for adversarial learning and model optimization. This manner enables us to implement AdvStyle in the LAB space as well as to use the ImageNet-pretrained parameters. As shown in the Table~\ref{table:variants}, LAB-based AdvStyle (\ours-LAB) also significantly improves the performance on unseen domains but achieves lower results than RGB-based AdvStyle on two of the three benchmarks. On the other hand, converting between RGB and LAB will increase the training time due to the extra computation costs. 


\begin{table}[t]
\centering
\footnotesize
\caption{Results of \ours variants. The backbone is ResNet-50.}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c|c}
\toprule
Methods (GTAV$\rightarrow$) & \multicolumn{1}{c|}{CityScapes} & \multicolumn{1}{c|}{BDD} & \multicolumn{1}{c|}{Mapillary} & \multicolumn{1}{c}{Mean}\\
\midrule
{\cellcolor[gray]{1}}Baseline~\cite{robustnet} & 28.95  & 25.14 & 28.18  & 27.42\\
{\cellcolor[gray]{1}}{AdvStyle-LAB} & 37.09 &  32.89 & 37.13 & 35.70 \\
{\cellcolor[gray]{1}}{AdvStyle-Patches} & 39.50 & \bf 36.37 & \bf 37.42 & \bf 37.76 \\
\midrule
{\cellcolor[gray]{1}}\textbf{AdvStyle} &\bf 39.62 & 35.54 & 37.00 & 37.39 \\
\bottomrule
\end{tabular}
\label{table:variants}
\end{table}


\section{Quantitative Understanding of \ours} 

To demonstrate the effectiveness of \ours in narrowing the domain shift, we provide the quantitative analysis on the distribution of different datasets. Specifically, we computed the histograms of pixel values of four datasets (GTAV~\cite{gtav}, CityScapes~\cite{CityScapes}, BDD-100K~\cite{bdd}, Mapillary~\cite{mapillary}) and the AdvStyle-augmented dataset of GTAV which is generated by 4 epochs. The bin size is set to 8. For each dataset, the histograms of RGB channels are normalized by L1-norm and re-scaled ($\times$) by $\#$bins, and then are concatenated as the histogram feature. We estimate the distribution distance between two datasets by computing the KL-distance between their histogram features. Results are reported in Table~\ref{table:kl}.
We can observe that the AdvStyle-augmented dataset has a smaller distance to real datasets, verifying that \ours can narrow the gap between synthetic and real data. 
\begin{table}[!ht]
\centering
\footnotesize
\caption{Comparison of KL-distance between different datasets.}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c|c}
\toprule
Source & CityScapes $\downarrow$ & BDD $\downarrow$ & Mapillary $\downarrow$ & Mean $\downarrow$ \\
\midrule
GTAV & 0.5867 & 0.3421 & 0.3211 & 0.4166 \\
Adv-GTAV & \bf 0.5587 & \bf 0.3217 & \bf 0.3058 & \bf 0.3954  \\
\bottomrule
\end{tabular}
\label{table:kl}
\end{table}




\section{Algorithm and Pytorch-Like Pseudo-Code}
\label{sec:alg-pytorch}


The training procedure and Pytorch-like pseudo-code are shown in Alg.~\ref{alg:advstyle} and Fig.~\ref{fig:pseudo_code}, respectively.


\begin{algorithm}[!ht]
    \caption{The training procedure of AdvStyle.}
    \label{alg:advstyle}
    \textbf{Inputs:} labeled source domain $\mathcal{S}$, segmentation model $\mathcal{F}$ parameterized by $\theta$, batch size $N_b$, total training iterations $max\_iter$, adversarial learning rate $\gamma$, and model learning rate $\alpha$. \\
    \textbf{Outputs:} Optimized model $\mathcal{F}$ parameterized with $\theta$.\\
\begin{algorithmic}[1]
        \FOR{$i$ in $max\_iter$}
            \STATE Sample mini-batch $\mathcal{X}$ with $N_b$ images;
            \STATE  \textcolor{gray}{// Stage 1: Adversarial Style Learning.}
            \STATE Compute channel-wise mean $\mu$, standard deviation $\sigma$ and normalized images $\Bar{\mathcal{X}}$ with Eq.~2;
            \STATE Initialize adversarial style feature: $\mu^+ \leftarrow \mu$, $\sigma^+ \leftarrow \sigma$;
                \STATE Compute adversarial segmentation loss $- \mathcal{L}_{\text{seg}}$;
                \STATE Optimize $\mu^+$ and $\sigma^+$ with Eq.~3; 
            \STATE  \textcolor{gray}{// Stage 2: Robust Model Training.}
            \STATE Generate adversarial images $\mathcal{X}^{+}$ with $\Bar{\mathcal{X}}$, $\mu^+$ and $\sigma^+$ by Eq.~4;
            \STATE Compute the overall training loss $\mathcal{L}_{\text{seg}}(\theta; \mathcal{X})+\mathcal{L}_{\text{seg}}(\theta; \mathcal{X}^+)$ by Eq.~5;
            \STATE Optimize the segmentation model $\mathcal{F}$: $\theta \leftarrow \theta - \alpha \nabla_{\theta} \left( \mathcal{L}_{\text{seg}}(\theta; \mathcal{X})+\mathcal{L}_{\text{seg}}(\theta; \mathcal{X}^+) \right)$;
        \ENDFOR
        \STATE \textbf{Return} $\mathcal{F}$ parameterized with $\theta$.
    \end{algorithmic}
\end{algorithm}


\section{More Visualizations}
\label{sec:seg_results}
\textbf{Segmentation Results}. In Fig.~\ref{fig:seg_city}, Fig.~\ref{fig:seg_bdd}, and Fig.~\ref{fig:seg_maphillary}, we provide more segmentation results for the baseline and ``baseline+\ours''.

\textbf{Examples of \ours}. In Fig.~\ref{fig:style_comparsison}, we illustrate more examples generated by \ours.

\section{Limitations}
\label{sec:limitations}
The main limitation of \ours lies in the increase of training time. The computational cost of \ours is almost double of that of the baseline since one more forward-backward process is required to generate style-adversarial examples.
Another limitation is that despite generating hard examples, \ours cannot address severe environmental change in practice, \eg, rainy and snowy weather, since such conditions cannot be represent purely by style features. Those conditions, \eg, rain, snow and fog, can be added to source samples and adversarial-augmented samples manually to alleviate the problem.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.78\linewidth]{figs/pseudo_code.pdf}
\caption{The Pytorch-like pseudo-code of \ours.}
\label{fig:pseudo_code}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figs/prediction_visual_base_sup_city.pdf}
\vspace{-.1in}
\caption{Segmentation results on CityScapes. Source: GTAV; Backbone: ResNet-50.}
\label{fig:seg_city}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figs/prediction_visual_base_sup_bdd.pdf}
\vspace{-.1in}
\caption{Segmentation results on BDD-100K. Source: GTAV; Backbone: ResNet-50.}
\label{fig:seg_bdd}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figs/prediction_visual_base_sup_mapillary.pdf}
\vspace{-.1in}
\caption{Segmentation results on Mapillary. Source: GTAV; Backbone: ResNet-50.}
\label{fig:seg_maphillary}
\end{figure}



\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{figs/StyleComparison.pdf}
\vspace{-.1in}
\caption{Examples of adversarial style augmentation. Source: GTAV; Backbone: ResNet-50.}
\label{fig:style_comparsison}
\end{figure}


\end{document}