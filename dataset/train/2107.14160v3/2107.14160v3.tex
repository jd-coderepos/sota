

\clearpage
\begin{center}
    \Large
    \textbf{Appendix}
\end{center}

\begin{figure}[thb]
\begin{center}
\includegraphics[width=1.0\linewidth]{./overview.png}
\end{center}
   \vspace{-2ex}
   \caption{An overview of our framework (Figure 3 in the main paper).}
\label{fig: apdx_overview}
\vspace{-1ex}
\end{figure}

\setcounter{section}{0}
\section{Implementation Details}
This section first presents the adopted local geometric constraints between 2D and projected 3D bounding boxes in the enhanced baseline. Subsequently, we will elaborate on the details of training loss and inference
procedure.
\subsection{Local Geometric Constraints}
Our baseline FCOS3D~\cite{FCOS3D} only stiffly adjusts the output of networks to fit the requirements of 3D detection. There is no relationship or constraints between these predicted attributes, making this network hard to train, especially when the data is limited. Considering our detector can achieve 90\% accuracy on 2D vehicle detection, we add 2D localization into our targets and use it to regularize 3D outputs. Actually, this closed-loop and self-supervised approach is also consistent with what humans do in the annotation procedure~\cite{FLAVA}. In practice, as shown in Fig.~\ref{fig: apdx_overview}, we add a consistency loss (GIoU loss) between our estimated 2D boxes and the exterior 2D boxes of 3D predictions to enhance our baseline, which is particularly important on the small KITTI dataset. Note that due to the difficulty of regressing accurate depth, we use the ground truth depth for deriving the 3D bounding boxes when computing the consistency loss.

Here we provide an example to show the intuition behind this design. Typically when the data is limited, it is hard for the network to direct regress different 3D targets (offset, depth, orientation, \emph{etc.}) independently. For example, in Fig.~\ref{fig: local_geo}, the orientation of nearby large objects predicted by our baseline can be very inaccurate (the top line in the figure) even though it can be easily rectified with simple verification. So we add the more reliable 2D localization into our targets to regularize our 3D predictions. It turns out that the simple local constraint could alleviate this problem in the learning procedure while does not introduce extra computational costs to inference. The improved results after adding this constraint can be seen in Fig.~\ref{fig: local_geo} (the bottom line).

\begin{figure*}
\begin{center}
\includegraphics[width=0.9\linewidth]{./local_geo.png}
\end{center}
   \vspace{-2ex}
   \caption{The top line shows that it is easy to validate the accuracy of 3D predictions according to its exterior 2D bounding box. So we add the 2D localization into our targets and use the relatively reliable 2D boxes to regularize 3D predictions. This results in significant improvement as shown by the bottom line.}
\label{fig: local_geo}
\vspace{-2ex}
\end{figure*}

\subsection{Loss}
\noindent\textbf{Overall Loss Design}\quad We basically follow the loss design of FCOS3D except our proposed consistency loss and the adjustments for different datasets.

To have a brief review, firstly, we use the focal loss~\cite{RetinaNet} as the object classification loss:

where  is the class probability of a predicted box, and we follow the common settings,  and . For attribute classification on nuScenes, we use a simple softmax classification loss, denoted as .

For regression branch, we use the smooth L1 loss for each regression target except centerness:

The weights of  error are 1 and the weights of  on nuScenes are 0.05. We use the softmax classification loss and binary cross entropy (BCE) loss for direction classification and centerness regression, denoted as  and  respectively. For local geometric constraints, denote our predicted 2D boxes as , the minimum exterior 2D boxes of projected 3D boxes as , then the consistency loss is:

Finally, the total loss is:

 is the number of positive predictions and . Note that the attribute loss  and velocity loss in the  are only required in the nuScenes experiments.

\noindent\textbf{Specific Loss Designs for KITTI experiments}\quad Because the KITTI dataset has relatively limited samples and much more strict metrics, we adopt two specific loss designs for training the networks. First, we add an auxiliary key-points loss to enhance the local geometric consistency further. Denote the 2D offsets of eight key-points (eight corners of a 3D bounding box) relative to a foreground point as , and then we take these offsets as 16 additional dimensions of  in Eqn.~\ref{eqn: loc_loss} and set their weights to 0.2. To make the FPN-based learning stable, we normalize these offsets just as we normalize those offsets to four sides of a 2D box.

In addition, we use a much stronger uncertainty formulation for this multi-task learning problem as presented in~\cite{MultiTask}. Specifically, referring to its formulation of maximum likelihood and homoscedastic uncertainty, we formulate the depth loss as:
\vspace{-1.0ex}

Here  and  are the targets and predictions of depth,  represents the original smooth L1 loss with  and  is the variable for uncertainty. In practice, to make the learning easier, we train the network to predict the log variance  only for depth estimation, which is more numerically stable than directly predicting the variance. Correspondingly,  serves as the weight of depth loss. In this way, the depth loss will be adaptively weighted relative to other regression losses. Additionally, the uncertainty  can also be used as another confidence score to be multiplied when inference, such that predictions with more accurate depths will have particularly higher scores. Note that this strong uncertainty indicator can only bring a significant gain on KITTI experiments while seriously hurting the general performance as evaluated on the nuScenes dataset.  

\noindent\textbf{Alternative Depth Loss Designs}\quad Considering we have several intermediate depth predictions, such as ,  and  in Fig.~\ref{fig: apdx_overview}, a natural idea is to add intermediate supervisions for these predictions to guarantee that each branch can learn meaningful information. So we further defined several depth L1 losses for these predictions and tried to replace the original depth loss in the  with their weighted summation. It turns out that although this approach can make the training procedure more stable, it does not bring any performance gain. We also find that the framework never overfits to only relying on one kind of estimation even with only supervision for the final prediction, as to be shown in Sec.~\ref{sec: quant_depth}. It indicates that these predictions and components indeed work together from complementary aspects. 

\subsection{Inference}
The inference procedure is to forward the input image through the framework and obtain bounding boxes with their class scores, attribute scores (if necessary) and centerness predictions. We multiply the class score, the predicted centerness and the depth confidence score as the overall confidence for each prediction and conduct rotated Non-Maximum Suppression (NMS) in the bird view as most 3D detectors to get the final results.

\section{Explanation of Oracle Analyses}
In this section, we will explain more about our empirical analysis, from the specific settings to more details in the results.

\subsection{Reason for Replacing Dense Predictions}
First, we would like to emphasize one detail in our analysis, \emph{i.e.}, we replace the dense predictions from the direct output of detection head with oracles to purely observe the problems of our networks. In comparison, other alternatives exist, such as replacing the decoded dense output or predictions after post-processing, which can not reveal some entangling problem lying in the formulation. One example to show the difference between these two implementations is that we replace the offset with corresponding ground truth while the latter approach replaces the decoded  in the 3D space with targets.

\subsection{Comparison of Different Metrics}
As mentioned in the main paper, KITTI and nuScenes adopt different evaluation metrics. The former is relatively strict and the latter is more comprehensive. Specifically, for mAP of these two datasets, we regard predictions with 3D IoU larger than a threshold (0.7 or 0.5) as positive samples on KITTI while define the match by 2D center distance  in the bird eye view on nuScenes. The latter is a simpler criterion as it decouples the detection from object size and orientation. Therefore, we only plot points with category/location related oracles (classification, depth and offset) in the mAP analysis on nuScenes (Fig.~\ref{fig: apdx_oracle}). In addition, to be more specific, mAP is computed over several different matching thresholds,  meters, and all categories  on nuScenes:\\

Then we can see that it will also consider predictions with relatively inaccurate locations (like objects with the distance error larger than 2 meters but smaller than 4 meters). This difference is especially notable when discussing the improvements from depth score, which will be detailed in Sec.~\ref{sec: PR_curve}.

Finally we basically describe how the NuScenes Detection Score (NDS) is calculated. To begin with, we first define that predictions with center distance from the matching ground truth  will be considered as true positives (TP) and thus introduce 5 True Positive metrics, Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE) and Average Attribute Error (AAE). Given these metrics, we compute the mean TP metric (mTP) over all categories:\\
 
Then the NDS is calculated as follows:\\

Therefore, NDS is a combination of several decoupled metrics and could reflect the performance of 3D detectors from another perspective. See more details about the intermediate computation in its original paper~\cite{nuScenes}.

\subsection{Detailed Explanations and Conclusions}
Due to the space limitation in the main paper, we do not discuss much about the results shown in Fig.~\ref{fig: apdx_oracle}. Next, we will analyze it in detail and summarize a series of important conclusions.

\noindent\textbf{Basic Observations}\quad As shown in Fig.~\ref{fig: apdx_oracle}, we replace the predicted attributes with their ground truth values step by step and observe the performance improvements. We can see that:

\noindent 1. With only one oracle (circle dots), only depth can bring a considerable improvement (green lines). It shows that with current depth estimation, other predicted attributes do not drag down the performance, while with other predictions, the current accuracy of depth estimation is far not enough.

\noindent 2. With accurate depth, other oracles (triangle dots in the figures) could bring the expected performance gains. While with current depth estimation, even all the other predictions are accurate (green rhombus dots), the results are always disappointing, even almost like the baseline.

\noindent 3. Although KITTI and nuScenes are different in terms of category variety and metrics, the trend of these curves is the same. The difference is reflected in the importance of localization and classification oracles. Localization is more important on the KITTI, which has less category variety and more strict metrics. Classification is another important factor apart from depth on nuScenes, \emph{e.g.}, our monocular predictions with location oracle is still not better than the best LiDAR-based methods. In contrast, with an accurate depth and classification map, the performance is almost ideal.

From these observations, we can conclude that the inaccurate depth blocks \emph{all} the other sub-task predictions from improving the overall detection performance. Hence, as mentioned in the main paper, the current monocular 3D detection, especially 3D localization, can be actually reduced to the dominating instance depth estimation problem.

\noindent\textbf{Comparison with Best LiDAR-Based Methods}\quad There is an interesting phenomenon not much related to depth estimation in the above analysis, \emph{i.e.}, the comparison with best LiDAR-based methods on nuScenes. We can see that classification is particularly important on nuScenes, and our monocular predictions with location oracles are still not better than the state-of-the-art LiDAR-based methods. This result is a little dataset-specific. We conjecture it is because the classification for ten categories on nuScenes is relatively hard, or the annotation is mainly conducted in the point clouds, leading to missing objects in the images.
\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{./oracle.png}
\end{center}
   \vspace{-2ex}
   \caption{Oracle analyses with different datasets and metrics (Figure 2 in the main paper). From left to right: 3D IoU based mAP on KITTI, NuScenes Detection Score (NDS) and distance-based mAP on nuScenes. We replace our predictions with ground truth values step by step and observe the performance improvements. It can be seen that an accurate depth can bring significant performance improvement (green lines), and only with accurate depth can the improvements brought by other oracles be realized.}
\label{fig: apdx_oracle}
\vspace{-2ex}
\end{figure}

\section{Supplementary Experimental Results}
In this section, we will show more experimental results to help further understand our approach. First, we will provide toy examples to explain and validate our derived pairwise perspective relationship in the depth propagation. Subsequently, we make more detailed analyses in quantitative and qualitative ways to reveal the working mechanism and effect of our method.

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{./toy_exp.png}
\end{center}
   \vspace{-3ex}
   \caption{Inconsistent bottoms of different instances. Although all the objects in an image share similar heights for bottoms most of the time, corner cases still exist. Here we mark the heights of bottoms in the camera coordinates (down is the positive direction). This problem can be caused by the actual topography, \emph{e.g.}, pedestrians are on the step. It can also be caused by annotation noises, especially for different categories and distant objects. This observation is the foundation of our proposed edge pruning/gating scheme in the depth propagation.}
\label{fig: toy_exp}
\vspace{-1.5ex}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{./PR_curve.png}
\end{center}
   \vspace{-4ex}
   \caption{Comparison of PR curves for models with (solid line) and without (dotted line) depth score. The depth score encourages predictions with accurate depth while suppresses those with inaccurate depth, which results in higher precision under low recall and strict matching thresholds while lower precision under high recall. This problem is more notable for large objects like cars.}
   \vspace{-2.5ex}
\label{fig: PR_curve}
\end{figure}

\subsection{Basic Validation of Depth Propagation}
As shown in Fig.~\ref{fig: toy_exp}, we provide two samples with many objects in one image. We first have a brief review of the perspective relationship derived in the main paper. Given two objects 1 and 2, the relationship between their centers strictly satisfies:

Considering two objects share the same ground (bottom height), we can get the approximate relationship as follows:

where  denotes the depth,  denotes the distance between the projected 2D object center and the horizon line in the image,  is the 3D height of object center and  is the height of the 3D bounding box. Taking the left sample in Fig.~\ref{fig: toy_exp} as an example, the depths of the 8 cars are . With our derived relationship, we can estimate them with only the first 2 accurate depths: . We can see that similar to the general case of depth estimation, our propagation mechanism also yields more notable errors for distant objects, which has been analyzed in the main paper (The effect of  over  will be enlarged as the  decreases.)

Next, we can further observe the inconsistent bottoms problem shown in Fig.~\ref{fig: toy_exp}. We mark some representative instances in the figure. It can be seen that it is sometimes caused by the actual topography, like pedestrians and cars in the second sample. Nevertheless, the noise only exists between objects far away from each other most of the time. We conjecture this is related to the annotation pipeline, \emph{e.g.}, we tend to make use of nearby annotations when the information for labeling the current instance is inadequate. Alternatively, sometimes it is just because the LiDAR only sweeps the top part of the distant objects such that the annotator can not determine its bottom accurately.

In conclusion, although the ground constraint holds most of the time, it is still important to design mechanisms to avoid these possible noises and incorporate the geometric depth adaptively, such as the edge pruning/gating scheme and location-aware integration in the main paper.

\vspace{-0.5ex}
\subsection{Quantitative Analysis}
\vspace{-0.5ex}
\noindent\textbf{Difference Between Datasets}\label{sec: PR_curve}\quad
Here we mainly show the observation in the ablation study to explain the different effects from the same component on these two datasets. We take the depth score as an example. First, Tab. 7 in the main paper has shown the especially important role of depth score on the KITTI. However, it does not contribute much to the improvements on nuScenes. Specifically, it only brings about 0.3\% increase on NDS by reducing the mATE instead of boosting the mAP. To figure out the reason, we take a closer look at the performance from the Precision-Recall (PR) curve. As shown in Fig.~\ref{fig: PR_curve}, we can see that the depth score (solid line) significantly improves the precision under low recall and strict matching thresholds (like 0.5 and 1.0 meters, blue and yellow lines) while influences the performance under high recall and less strict cases (like 2.0 and 4.0 meters, green and red lines). This problem is especially notable for large objects. It reveals the effect of depth score from another perspective, \emph{i.e.}, it can overly suppress those predictions with inaccurate depth, of which we should be tolerant under some circumstances, like distant and small objects. Therefore, designing a more suitable depth score with better interval division methods or other approaches can be a direction worthy of further exploration.

\noindent\textbf{Mean AP for Multi-Class Detection on nuScenes}\quad
To present the multi-class detection results more comprehensively, we provide the mean AP results (over all the matching thresholds) for each category on nuScenes in Tab.~\ref{tab: ap_class}. We can see that our method shows the superiority especially on small (from pedestrian to barrier) and quite large objects (bus). Firstly, the better capability of handling objects with different scales should partly come from the leveraged well-developed backbone and FPN. Furthermore, our probabilistic and geometric depth also improves the accuracy of depth estimation, which is especially important for small objects.

\begin{table}\scriptsize
\caption{Average precision for each class on the nuScenes test benchmark. CV and TC are abbreviation of construction vehicle and traffic cone in the table.}
	\begin{center}
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
	\hline
	Methods & car & truck & bus & trailer & CV & ped & motor & bicycle & TC & barrier & mAP\\
	\hline\hline
	LRM0 & 0.467 & 0.21 & 0.17 & 0.149 & 0.061 & 0.359 & 0.287 & 0.246 & 0.476 & 0.512 & 0.294\\
	\hline
	MonoDIS~\cite{MonoDIS} & 0.478 & 0.22 & 0.188 & 0.176 & 0.074 & 0.37 & 0.29 & 0.245 & 0.487 & 0.511 & 0.304\\
	\hline
	CenterNet~\cite{CenterNet} (HGLS) & 0.536 & 0.27 & 0.248 & 0.251 & 0.086 & 0.375 & 0.291 & 0.207 & 0.583 & 0.533 & 0.338\\
	\hline
	Noah CV Lab & 0.515 & 0.278 & 0.249 & 0.213 & 0.066 & 0.404 & 0.338 & 0.237 & 0.522 & 0.49 & 0.331\\
	\hline
	PGD (Ours) & 0.561 & 0.299 & 0.285 & 0.266 & 0.134 & 0.441 & 0.397 & 0.314 & 0.605 & 0.561 & \textbf{0.386}\\
	\hline
	\end{tabular}
	\end{center}
	\vspace{-5ex}
	\label{tab: ap_class}
\end{table}

\begin{figure}
\begin{minipage}{.48\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{./prop_weights.png}
    \vspace{-5ex}
    \caption{Distribution of weights for the final integration in our PGD module.}
    \vspace{-2ex}
    \label{fig: prop_weights}
\end{minipage}
\hspace{1mm}
\begin{minipage}{.48\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{./weights_vs_depths.png}
    \vspace{-5ex}
    \caption{Scatter plot of location-aware weights with respect to depths.}
    \vspace{-2ex}
    \label{fig: weights_vs_depths}
\end{minipage}
\end{figure}

\noindent\textbf{Contributions of Each Depth Estimation}\label{sec: quant_depth}\quad
To understand the role of each component for depth estimation more clearly, we make statistics about the fusion weights. Firstly, for local depth estimation, we find that the direct regression accounts for about 25.6\% in the results, \emph{i.e.},  is about 0.256. It implies that the direct regression may be responsible for regressing the residual of the probabilistic estimation, which plays an auxiliary but important role according to the ablation study in the main paper (Tab. 7). On the other hand, for final integration, we make statistics for the location-aware weights  of predictions with matching ground truths before NMS on the validation set, and plot its distribution in Fig.~\ref{fig: prop_weights} (higher value means more contribution from local estimation). We can see that although the preliminary local estimation plays a more important role in many cases, the propagated geometric depth does contribute a lot to the overall estimation. In addition, we also plot the scatter diagram of these weights with respect to the estimated depth and different categories (Fig.~\ref{fig: weights_vs_depths} and \ref{fig: weights_cls}). We can see that the geometric depth contributes more to the estimation of very nearby (can be truncated in the image) and small objects like pedestrians, which is consistent with our common sense that these two cases are relatively hard such that we need to incorporate some contextual information in the reasoning procedure.

\begin{table}[htb]
\tiny
\begin{minipage}{.5\linewidth}
    \centering
    \vspace{-1.0ex}
    \caption{Ablation study for the depth unit setting with our lightweight model on nuScenes.}
    \vspace{-1ex}
	\begin{tabular}{c|c|c|c|c|c|c}
	\hline
	 (meters) & mAP & mATE & mASE & mAOE & mAAE & NDS\\
	\hline
	5 & 0.298 & 0.79 & 0.266 & 0.563 & 0.164 & 0.371\\
	10 & 0.303 & 0.775 & 0.265 & 0.548 & 0.164 & 0.376\\
	\hline
	\end{tabular}
	\label{tab: ablation_unit}
    \vspace{-2ex}
\end{minipage}
\hspace{4.5ex}
\begin{minipage}{.45\linewidth}
    \centering
    \vspace{-1.0ex}
    \caption{Ablation study for alternative depth division methods.}
    \vspace{-1.0ex}
    \begin{tabular}{c|c|c|c}
    \hline
     Methods & Easy & Mod. & Hard\\
     \hline
    Log & 9.91 & 8.68 & 7.95\\
    Linear & 18.63 & 14.49 & 13.25\\
    Uniform Log & 8.62 & 13.48 & 13.28\\
    Uniform & \textbf{19.10} & \textbf{16.04} & \textbf{14.83}\\
    \hline
    \end{tabular}
    \label{tab: ablation_div}
    \vspace{-2ex}
\end{minipage}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{./weights_cls.png}
\end{center}
    \vspace{-3ex}
    \caption{Location-aware weights of predictions from different categories.}
    \vspace{-3ex}
\label{fig: weights_cls}
\end{figure}

\noindent\textbf{Ablation Studies for Alternative Depth Division Methods}\quad
We also made ablation studies for alternative probabilistic depth settings, including the different settings for the depth unit  and different division methods to bucket the depth value into intervals. First, Tab.~\ref{tab: ablation_unit} shows that more fine-grained division can not bring performance gains. As for the division methods, we test several alternatives as shown in Tab.~\ref{tab: ablation_div}, among which \emph{Log} and \emph{Linear} refer to the spacing-increasing discretization (SID)~\cite{DORN} and linear-increasing discretization (LID)~\cite{Center3D}, respectively. We directly take their split points and compute the depth estimation with Eqn. 1 in the main paper. In contrast, \emph{Uniform Log} means that we take the split points that are uniformly distributed in the \emph{log} space as the base to compute the depth estimation in the \emph{log} space with Eqn. 1, and then apply the exponential transformation to get the final result. We can see that although the simplicity, our adopted uniform division method achieves the best performance. Note that this ablation study is conducted with . There may be different conclusions if we exploit more fine-grained divisions or use classification and residual regression to implement the probabilistic depth estimation.

\begin{table}[htb]
\tiny
\begin{minipage}[htb]{.48\linewidth}
    \centering
    \vspace{-2ex}
    \caption{Ablation study for alternative distance scores in the edge gating scheme on KITTI.}
    \vspace{-1ex}
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
     \multirow{2}*{Method} & \multicolumn{3}{c|}{AP IOU} & \multicolumn{3}{c}{AP IOU}\\
    \cline{2-7}
    ~ & Easy & Mod. & Hard & Easy & Mod. & Hard\\
    \hline
     3D bottoms & 15.18 & 11.96 & 10.72 & 46.27 & 37.99 & 33.09\\
     3D centers & 21.04 & 16.07 & 14.89 & 47.03 & 37.58 & 32.97\\
     2D centers & 21.36 & 16.60 & 15.60 & 50.57 & 39.78 & 34.18\\
    \hline
    \end{tabular}
	\label{tab: ablation_dist}
    \vspace{-2ex}
\end{minipage}
\hspace{4.5ex}
\begin{minipage}[htb]{.48\linewidth}
    \centering
    \vspace{-2ex}
    \caption{Depth error statistics for predictions having corresponding matching ground truths.}
    \begin{tabular}{c|c|c}
    \hline
    Methods & Mean Abs. Error (m)  & Mean Rel. Error \\
    \hline
    FCOS3D & 0.0528 & 4.27\% \\
    PGD (Ours) & 0.0483 & 3.63\% \\
    \hdashline
    Rel. Delta & -8.5\% & -15.0\%\\
    \hline
    \end{tabular}
    \label{tab: depth_error}
    \vspace{-2ex}
\end{minipage}
\end{table}

\noindent\textbf{Ablation Studies for Geometric Depth}\quad
Recall that we select three important factors for edge pruning and gating in the depth propagation graph. We also tried other alternatives for the distance score, including the height difference between 3D bottoms, the distance of 3D centers and our adopted 2D centers (Tab.~\ref{tab: ablation_dist}). It can be observed that using the 2D centers yields the best performance. We conjecture that it is because the 3D criteria are based on the inaccurate depths such that they are less reliable than the disentangled 2D distance.

\noindent\textbf{Depth Error Analysis}\quad
We have validated the efficacy of our method in the main paper by comparing the detection performance of our method and the baseline, especially in terms of the improved mean average precision (mAP) and the mean translation error (mATE). Here we further prove its effectiveness with the depth error analysis. We make depth error statistics for the predictions (before NMS) which have corresponding ground truths on the KITTI validation set (Tab.~\ref{tab: depth_error}). We can observe that our method significantly reduces the mean error of depth estimation, both on the absolute error and relative error ((Abs. and Rel. in Tab.~\ref{tab: depth_error}).

\subsection{Qualitative Analysis}
Then we show some qualitative results on nuScenes in Fig.~\ref{fig: qualitative} by drawing the predicted 3D bounding boxes in the six-view images and the top-view point clouds. We compare the results predicted by our model and the baseline FCOS3D to demonstrate the improvements in terms of depth estimation intuitively.
We can see that from the perspective of images, both detection results are appealing, especially for some small objects that are not labeled. For example, the barriers in the rear right camera are not labeled but detected by these two models.
However, from the bird-eye-view, the depth accuracy of the two methods is notably different, especially for those objects marked with red circles: The accuracy is significantly improved by our proposed method. It is also in line with the quantitative results (the mATE is reduced remarkably) and further validates the efficacy of our method.

\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{./qualitative.png}
\end{center}
   \vspace{-2ex}
   \caption{Qualitative analysis of detection results. 3D bounding box predictions are projected onto images from six different views and bird-view, respectively. Boxes from different categories are marked with different colors. We can see that the detection results of FCOS3D and PGD are both reasonable. However, from the bird-eye-view, the depth accuracy is remarkably improved by our method, especially for those objects marked with red circles.}
   \vspace{-2.5ex}
\label{fig: qualitative}
\end{figure*}

\label{sec:appendix}