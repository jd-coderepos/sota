
\documentclass{article}



\usepackage[nonatbib, final]{neurips_data_2022}









\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{amsmath}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{xspace}
\usepackage{todonotes}
\usepackage[etex=true,export]{adjustbox}
\usepackage{authblk}    \usepackage[capitalise]{cleveref}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{bbding}




\newcommand{\datasetnum}{\xspace}
\newcommand{\algnum}{\xspace}
\newcommand{\ourmethod}{USB\xspace}
\newcommand{\wyd}[1]{{\color{red}{[(WYD): #1]}}}
\newcommand{\wydd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WYD): #1}}
\newcommand{\bad}[1]{{\color{red}{#1}}}
\newcommand{\fan}[1]{{\color{brown}{[(Fan): #1]}}}

\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WJD): #1}}
\newcommand{\revision}[1]{{\color{black}{#1}}}
\newcommand{\wjd}[1]{{\color{cyan}{[(WJD): #1]}}}
\newcommand{\ch}[1]{{\color{blue}{[(CH): #1]}}}
\newcommand{\hwx}[1]
{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,size=\scriptsize]{(HWX): #1}}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\title{USB: A Unified Semi-supervised Learning Benchmark for Classification}





\author{\textbf{Yidong Wang}\thanks{Equal contribution. Yidong Wang did this work during his internship at MSRA and Westlake University.} , \textbf{Hao Chen}, \textbf{Yue Fan}, \textbf{Wang Sun}, \textbf{Ran Tao}, \textbf{Wenxin Hou},\\ \textbf{Renjie Wang}, \textbf{Linyi Yang}, \textbf{Zhi Zhou}, \textbf{Lan-Zhe Guo}, \textbf{Heli Qi}, \textbf{Zhen Wu}, \textbf{Yu-Feng Li},\\
  \textbf{Satoshi Nakamura},
  \textbf{Wei Ye},
  \textbf{Marios Savvides}, \textbf{Bhiksha Raj}, \textbf{Takahiro Shinozaki},\\ \textbf{Bernt Schiele}, \textbf{Jindong Wang}\thanks{Correspondence to: jindong.wang@microsoft.com, zhangyue@westlake.edu.cn.} , \textbf{Xing Xie},  \textbf{Yue Zhang}
}
\affil{\small{Microsoft Research Asia, Westlake University, Tokyo Institute of Technology,\\ Carnegie Mellon University, Max-Planck-Institut für Informatik, Tsinghua University,\\ Microsoft STCA, Nanjing University, Nara Institute of Science and Technology, Peking University}
}






\begin{document}

\maketitle


\begin{abstract}


Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) \revision{for classification} by selecting \datasetnum diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. \textbf{Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with \revision{TorchSSL}.}


\end{abstract}

\section{Introduction}

Neural models give competitive results when trained using supervised learning on sufficient high-quality labeled data~\cite{he2016deep,redmon2016you,hochreiter1997long,vaswani2017attention,yu2016automatic,gulati2020conformer,verma2021graphmix}.
However, it can be laborious and expensive to obtain abundant annotations for model training~\cite{ILSVRC15,wang2019glue}.
To address this issue, \textbf{semi-supervised learning (SSL)} emerges as an effective paradigm to improve model generalization with limited labeled data and massive unlabeled data \cite{reddy2018semi,zhu2005semi,zhu2009introduction,van2020survey,ouali2020overview,qi2020small}.

SSL has made remarkable progress in recent years \cite{zhai2019s4l,li2020dividemix,chen2020big,pham2021meta,sohn2020fixmatch,zhang2021flexmatch}, yet there are still several limitations with the popular evaluation protocol in the literature~\cite{oliver2018realistic,sohn2020fixmatch,zhang2021flexmatch}.
First, existing benchmarks are mostly constrained to plain computer vision (CV) tasks (i.e., CIFAR-10/100, SVHN, STL-10, and ImageNet classification~\cite{oliver2018realistic,berthelot2019remixmatch,sohn2020fixmatch,xu2021dash,zhang2021flexmatch}, as summarized in TorchSSL~\cite{zhang2021flexmatch}), precluding consistent and diverse evaluation over tasks in natural language processing (NLP), audio processing (Audio), etc., where the lack of labeled data is a general issue and SSL has gained increasing research attention recently~\cite{chen2020mixtext,baskar2019semi,cances2022comparison}. Second, the existing protocol \revision{(e.g., TorchSSL~\cite{zhang2021flexmatch})} can be mainly time-consuming and environmentally unfriendly because it typically trains deep neural models from scratch~\cite{berthelot2019mixmatch,berthelot2019remixmatch,xie2020unsupervised,sohn2020fixmatch,xu2021dash,zhang2021flexmatch}. Specifically, as shown in \tablename~\ref{tab-brief-sum-a}, it takes about  GPU days ( GPU days without ImageNet) to evaluate FixMatch~\cite{sohn2020fixmatch} with \revision{TorchSSL}~\cite{zhang2021flexmatch}. Such a high cost can make it unaffordable for research labs (particularly in academia) to conduct SSL research. Recently, the pre-training and fine-tuning paradigm~\cite{bert,liu2019roberta,hsu2021hubert,he2021masked} achieves promising results.
Compared with training from scratch, pre-training has much reduced cost in SSL.
However, there are relatively few benchmarks that offer a fair test bed for SSL with the pre-trained versions of neural models.

\begin{table}[t!]
\centering
\caption{A summary of datasets and training cost used in (a) the existing popular protocol and (b) \ourmethod. 
USB largely reduces the training cost while providing a diverse, challenging, and comprehensive benchmark covering a wide range of datasets from various domains.
Training cost is estimated by using FixMatch~\cite{sohn2020fixmatch} on a single NVIDIA V100 GPU from Microsoft Azure Machine Learning platform, except for ImageNet where 4 V100s are used. Experiments in (a) follow the settings in \cite{zhang2021flexmatch}. \revision{More results with different pre-trained backbones are available in Appendix~\ref{sec-appendix-backbone}.}
}
\label{tab-brief-sum}

\begin{subtable}[t]{\textwidth}\centering{
\caption{\revision{TorchSSL}~\cite{zhang2021flexmatch}}
\label{tab-brief-sum-a}
\resizebox{0.97\textwidth}{!}{\begin{tabular}{cccccc}
\toprule
Domain \& Backbone               & Dataset        & Classification Task & Hours  Settings  Seeds  & Total GPU Hours  & \begin{tabular}[c]{@{}l@{}}Total GPU Hours w/o ImageNet \end{tabular}  \\ 
\midrule
\multirow{5}{*}{CV, ResNets}    & CIFAR-10               & Natural Image        & 110  3  3 & \multirow{5}{*}{\makecell{8031 GPU Hours \\ (335 GPU Days)}}  & \multirow{6}{*}{\makecell{6687 GPU Hours \\ (279 GPU Days)}}  \\
                        & CIFAR-100               & Natural Image        & 300  3  3     \\ 
                        & SVNH           & Digital   & 108  3  3 \\
                        & STL-10           & Natural Image               & 225  3  3  \\
                        & ImageNet           & Natural Image               & 336 hours  4 GPUs \\ 
\bottomrule
\end{tabular}}
}
\end{subtable}

\begin{subtable}[t]{\textwidth}\centering{
\caption{USB}
\label{tab-brief-sum-b}
\resizebox{\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
Domain \& Backbone                & Dataset            & Classification Task  & Hours  Settings  Seeds & Total GPU Hours \\   
\midrule
\multirow{5}{*}{CV, ViTs}     & CIFAR-100               & Natural Image         & 11  2  3   & \multirow{15}{*}{\makecell{924 GPU Hours\\ (39 GPU Days)}} \\ 
                        & STL-10            & Natural Image               & 18  2  3 \\
                        & EuroSAT                 &  Satellite Image         & 10  2  3    \\  
                        & TissueMNIST             & Medical Image    & 8  2  3             \\ 
                        & Semi-Aves               & Fine-grained, Long-tailed Natural Image   & 13   1  3          \\ 
\cmidrule(r){1-4}
\multirow{5}{*}{NLP, Bert}    & IMDB                    & Movie Review Sentiment           & 8  2  3     \\  
& AG News                 & News Topic          & 6  2  3     \\ 
                        & Amazon Review           & Product Review Sentiment         & 8  2  3       \\
                        & Yahoo! Answer          & QA Topic         & 7  2  3       \\
                        & Yelp Review             & Restaurant Review Sentiment               & 8  2  3  \\
\cmidrule(r){1-4}
\multirow{5}{*}{\tabincell{c}{Audio, Wave2Vec 2.0 \\ and HuBert}} 
& GTZAN                   & Music Genre        & 12  2  3          \\ 
& UrtraSound8k            & Urban Sound Event          & 15  2  3           \\
 & FSDnoisy18k             & Sound Event     & 17  1  3               \\ 
& Keyword Spotting & Keyword           & 10  2  3          \\
& ESC-50                  & Environmental Sound Event   & 18  2  3      \\
\bottomrule
\end{tabular}}
}
\end{subtable}
\vspace{-.2in}
\end{table}



 

To address the above issues and facilitate general SSL research, we propose \textbf{USB: a \underline{U}nified \underline{S}SL \underline{B}enchmark} for classification~\footnote{The word `unified' means the unification of different algorithms on various application domains.}.
USB offers a \emph{diverse} and \emph{challenging} benchmark across five CV datasets, five NLP datasets, and five Audio datasets (\tablename ~\ref{tab-brief-sum-b}), enabling consistent evaluation over multiple tasks from different domains. Moreover, USB provides comprehensive evaluations of SSL algorithms with even fewer labeled data compared with TorchSSL, as the performance gap between SSL algorithms diminishes when the amount of labeled samples becomes large.
Benefiting from the rapidly developed neural architectures, we introduce pre-trained Transformers~\cite{vaswani2017attention} into SSL instead of training ResNets~\cite{he2016deep} from scratch to reduce the training cost for CV tasks.
Specifically, we find that using pre-trained Vision Transformers (ViT)~\cite{dosovitskiy2020image} can largely reduce the number of training iterations (e.g., by 80\% from 1,000k to 200k on CV tasks) without hurting the performance, and most SSL algorithms achieve even better performance with less training iterations.


As illustrated in Table~\ref{tab-brief-sum-b}, using USB, we spend only \textbf{39 GPU days} to evaluate the performance of an SSL algorithm (i.e., FixMatch) on a single NVIDIA V100 over these \textbf{15 datasets}, in contrast to \revision{TorchSSL}, which costs about \textbf{335 GPU days} on only \textbf{5 CV datasets} (279 GPU days on 4 CV datasets except for ImageNet).
To further facilitate SSL research, we open-source the codebase and pre-trained models~\footnote{\url{https://github.com/microsoft/Semi-supervised-learning}. We also provide the training logs of the experiments in this paper. Note that the results and training logs will be continuously updated/provided if we reorganize the codes for better use or add more algorithms and datasets. \revision{Microsoft Research Asia (MSRA) will provide both the support and resources for future updates.} } for unified and consistent evaluation of SSL methods. In addition, we also provide config files that contain all the hyper-parameters to easily reproduce our results reported in this work. We obtain some interesting findings by evaluating 14 SSL algorithms (Section~\ref{sec-results-analysis}): (1) introducing diverse tasks from diverse domains can be beneficial to comprehensive evaluation of an SSL algorithm; (2) pre-training is more efficient and can improve the generalization; (3) unlabeled data do not consistently improve the performance especially when labeled data is scarce.

To conclude, our contributions are three-fold:
\begin{itemize}
\itemsep=0em
    \item We propose USB: a unified and challenging semi-supervised learning benchmark \revision{for classification} with \datasetnum tasks on CV, NLP, and Audio for fair and consistent evaluations. \revision{To our humble knowledge, we are the first to discuss whether current SSL methods that work well on CV tasks generalize to NLP and Audio tasks.}
    \item We provide an environmentally friendly and low-cost evaluation protocol with pre-training \& fine-tuning paradigm, reducing the cost of SSL experiments. The advantages of USB as compared to other related benchmarks are shown in \tablename~\ref{tab:related-work}.
    \item We implement  SSL algorithms and open-source a modular codebase and config files for easy reproduction of the reported results in this work. we also provide documents and tutorials for easy modification. Our codebase is extensible and open for continued development through community effort, where we expect new algorithms, models, config files and results are constantly added.
\end{itemize}



\section{Related Work}

Deep semi-supervised learning originates from  model~\cite{rasmus2015semi}, where it solves the task of image classification by using consistency regularization that forces the model to output similar predictions when fed two augmented versions of the same unlabeled data. Subsequent methods can be classified as the variants of  model, where the difference lies in enforcing the consistency between model perturbation \cite{tarvainen2017mean}, data perturbation \cite{miyato2018virtual,xie2020unsupervised}, and exploiting unlabeled data \cite{sohn2020fixmatch,zhang2021flexmatch}. Since the best results in both CV and NLP are given by such algorithms, we choose them as typical representative methods in USB. While most SSL methods have seen their use in CV tasks, NLP has witnessed recent growth in SSL solutions~\cite{xie2020unsupervised,chen2020mixtext}. However, only some of the popular methods~\cite{xie2020unsupervised} in CV have been used in the NLP literature, probably because other methods give lower results or have not been investigated. This gives us motivation for evaluation of SSL methods on various domains in USB. 

As shown in Table 2, related benchmarks include Realistic SSL evaluation~\cite{oliver2018realistic} and TorchSSL~\cite{zhang2021flexmatch}. Realistic SSL evaluation~\cite{oliver2018realistic} has 4 SSL algorithms and 3 CV classification tasks and TorchSSL has 9 SSL algorithms and 5 CV classification tasks. Both of them are no longer maintained/updated. Thus it is of significance to build an SSL community that can continuously update SSL algorithms and neural models to boost the development of SSL. Besides, previous benchmarks mainly train the models from scratch, which is computation expensive and time consuming, since SSL algorithms are known to be difficult to converge \cite{athiwaratkun2018there}. In USB, we consider using pre-trained models to boost the performance while being more efficient and friendly to researchers.







In the following, we will first introduce the tasks, datasets, algorithms, and benchmark results of USB.
Then, the codebase structure of USB will be presented in \cref{sec-code-structure}.


\begin{table}[t]
\centering
\caption{The comparison between USB and other related benchmarks.}
\label{tab:related-work}
\resizebox{1\textwidth}{!}{\begin{tabular}{@{}cccccc@{}}
\toprule
Benchmark & \# SSL algorithms & Domian & \# Tasks & Pre-trained & Training hours using FixMatch \\ \midrule

Realistic SSL evaluation~\cite{oliver2018realistic}        & 4            & CV & 3 & \XSolidBrush & -  \\
TorchSSL~\cite{zhang2021flexmatch}        & 9            & CV & 5 & \XSolidBrush & 6687 \\ 
USB        & 14            & CV, NLP, Audio & 15 & \Checkmark & 924\\ \bottomrule
\end{tabular}}
\end{table}

\section{Tasks and Datasets}
\ourmethod consists of \datasetnum datasets from CV, NLP, and Audio domains.
Every dataset in USB is under a permissive license that allows usage for research purposes.
The datasets are chosen based on the following considerations: (1) the tasks should be diverse and cover multiple domains; (2) the tasks should be challenging, leaving room for improvement; (3) the training is reasonably environmentally friendly and affordable to research labs (in both the industry and academia).






\subsection{CV Tasks}


The details of the CV datasets are shown in Table~\ref{tab-usb-data}.
We include CIFAR-100~\cite{krizhevsky2009learning} and STL-10~\cite{coates2011analysis} from \revision{TorchSSL} since they are still challenging.
The TissueMNIST \cite{medmnistv1,medmnistv2}, EuroSAT \cite{helber2019eurosat,helber2018introducing}, and Semi-Aves \cite{su2021semi} are datasets in the domains of medical images, satellite images, and fine-grained natural images.
CIFAR-10~\cite{krizhevsky2009learning} and SVHN~\cite{netzer2011reading} in \revision{TorchSSL} are not included in USB because the state-of-the-art SSL algorithms~\cite{xie2020unsupervised,sohn2020fixmatch,xu2021dash} have achieved similar performance on these datasets to fully-supervised training with abundant fully labeled training data~\footnote{We highly recommend reporting ImageNet~\cite{ILSVRC15} results since it is a reasonable dataset for hill-climbing~\cite{sohn2020fixmatch,zheng2022simmatch,zhang2021flexmatch}. We also report and discuss ImageNet results in Appendix~\ref{sec-appendix-imagenet}.}.
SSL algorithms have a relatively large room for improvement on all chosen CV datasets in USB.
More details of these CV datasets in USB can be found in Appendix~\ref{sec-details-usb-cv}.

\begin{table}[t!]
\centering
\caption{Details of the datasets in USB.
Two \textit{\#Label per class} settings are chosen for each dataset except Semi-Aves and FSDnoisy18k, which have long-tailed distributed data.
Labeled data are sampled from the training data for each dataset except STL-10, Semi-Aves, and FSDNoisy18k, where the split of labeled and unlabeled data is pre-defined (e.g. 5,959 labeled images and 26,640 unlabeled images in Semi-Aves).
Following~\cite{sohn2020fixmatch,zhang2021flexmatch}, validation data are not provided for CV datasets. The NLP validation data are sampled from the original training datasets.
All test sets are kept unchanged.
}
\label{tab-usb-data}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Domain  & Dataset   & \#Label per class  & \#Training data   & \#Validation data & \#Test data  & \#Class                           \\ \midrule
 \multirow{5}{*}{CV}  & CIFAR-100 & 2 / 4              & 50,000 & - &10,000   & 100                 \\
& STL-10    & 4 / 10            & 5,000 / 100,000 & -  & 8,000  & 10                 \\
& EuroSat   & 2 / 4            & 16,200  & -   &5,400 & 10               \\
& TissueMNIST      & 10 / 50              & 165,466 & - & 47,280    & 8                                   \\
& Semi-Aves  & 15-53             & 5,959 / 26,640 & - & 4,000    & 200         \\ \midrule
\multirow{5}{*}{NLP} & IMDB                & 10 / 50              & 23,000  & 2,000 & 25,000  & 2  \\
& Amazon Review   & 50 / 200              & 250,000 & 25,000 & 65,000  & 5   \\
& Yelp Review       & 50 / 200              & 250,000 & 25,000 & 50,000  & 5     \\
& AG News            & 10 / 50              & 100,000 & 10,000 & 7,600  & 4 \\
& Yahoo! Answer      & 50 / 200              & 500,000 & 50,000 & 60,000 & 10   \\ \midrule

\multirow{5}{*}{Audio} & Keyword Spotting       & 5 / 20 & 18,538 & 2,577 & 2,567 & 10      \\
& ESC-50                & 5 / 10 & 1,200 & 400 & 400 & 50   \\
& UrbanSound8k          & 10 / 40 & 7,079 & 816 & 837 & 10   \\
& FSDnoisy18k           & 52-171 & 1,772 / 15,813 & - & 947 & 20   \\
& GTZAN                 & 10 / 40 & 7,000 & 1,500 & 1,500 & 10  \\

\bottomrule[1pt]
\end{tabular}}
\end{table}








\subsection{NLP Tasks}

The detailed dataset statistics of NLP tasks in USB are described in Table~\ref{tab-usb-data}. \revision{We mostly followed previous work in the NLP literature, and thus the existing datasets in USB cover most test sets used in the existing work~\cite{chen2020mixtext,li2021semi,xie2020unsupervised}.}
We include widely used IMDB~\cite{maas2011learning}, AG News~\cite{zhang2015character}, and Yahoo! Answer~\cite{chang2008importance} from the previous protocol~\cite{chen2020mixtext,li2021semi,xie2020unsupervised}, which are still challenging for SSL.
Since IMDB is a binary sentiment classification task, we further add Amazon Review~\cite{mcauley2013hidden} and Yelp Review~\cite{yelpwebsite} to evaluate SSL algorithms on more fine-grained sentiment classification tasks.
DBpedia is removed from the previous protocol~\cite{chen2020mixtext,li2021semi,xie2020unsupervised} because we find that the state-of-the-art SSL algorithms have achieved similar performance on it when compared with fully-supervised training.
For all tasks in NLP, we obtain the labeled datasets, unlabeled datasets, and validation sets by randomly sampling from their original training datasets while keeping the original test datasets unchanged, mainly following previous work~\cite{chen2020mixtext,li2021semi}. More details are in Appendix~\ref{sec-details-usb-nlp}.




\subsection{Audio Tasks}
USB includes five audio classification datasets as shown in Table~\ref{tab-usb-data}.
We choose the tasks to cover different domains such as urban sound (UrbanSound8k \cite{salamon2014dataset}, ESC-50 \cite{piczak2015dataset}, and FSDNoisy18k \cite{fonseca2019learning}), human sound (Keyword Spotting \cite{yang2021superb}), and music (GTZAN) \cite{gtzanwebsite}.
All chosen datasets are challenging even for state-of-the-art SSL algorithms.
For example, FSDNoisy18k is a realistic dataset containing a small labeled set and a large unlabeled set.
To the best of our knowledge, we are the first to systematically evaluate SSL algorithms on Audio tasks. Although there is a concurrent work ~\cite{cances2022comparison}, our study includes more algorithms and more datasets than ~\cite{cances2022comparison}. More details are in Appendix~\ref{sec-details-usb-audio}. 




\section{SSL Algorithms}
We implement 14 SSL algorithms in the codebase for USB, including  model~\cite{rasmus2015semi}, Pseudo Labeling~\cite{lee2013pseudo}, Mean Teacher~\cite{tarvainen2017mean}, VAT~\cite{miyato2018virtual}, MixMatch~\cite{berthelot2019mixmatch}, ReMixMatch~\cite{berthelot2019remixmatch}, UDA~\cite{xie2020unsupervised}, FixMatch~\cite{sohn2020fixmatch}, Dash~\cite{xu2021dash}, CoMatch~\cite{li2021comatch},
CRMatch~\cite{fan2021revisiting}, FlexMatch~\cite{zhang2021flexmatch}, AdaMatch~\cite{berthelot2021adamatch}, and SimMatch~\cite{zheng2022simmatch}, all of which exploit unlabeled data by encouraging invariant predictions to input perturbations \cite{van2020survey,ouali2020overview,yang2021survey,springenberg2015unsupervised,denton2016semi,dai2017good,kumar2017semi}. Such consistency regularization methods give the strongest performance in SSL since the model is robust to different perturbed versions of unlabeled data, satisfying the smoothness and low-density assumptions
in SSL~\cite{chapelle2009semi}.

\revision{The above SSL algorithms use Cross-Entropy (CE) loss on labeled data but differ in the way on unlabeled data. As shown in Table~\ref{tab-algs-components}, Pseudo Labeling~\cite{lee2013pseudo} turns the predictions of the unlabeled data into hard `one-hot' labels and treats the `one-hot' pseudo-labels as the supervision signals. Thresholding reduces the noisy pseudo labels by masking out the unlabeled samples whose maximum probabilities are smaller than the pre-defined threshold. Distribution Alignment aims to correct the output distribution to make it more in line with the target distribution (e.g., uniform distribution). Self-supervised learning, Mixup, and Stronger augmentations techniques also can help learn better representation. }More details of these algorithms can be found in Appendix~\ref{sec-detail-baseline}.
We summarize the key components exploited in the implemented consistency regularization based algorithms in \tablename~\ref{tab-algs-components}.



\begin{table}[t!]
\centering
\caption{Essential components used in  SSL algorithms supported in USB. PL, CR, Dist. Align., and W-S Aug., MSE, CE are the abbreviations for Pseudo Labeling, Consistency Regularization, Distribution Alignment, Weak-Strong Augmentation, Mean Squared Error, and Cross-Entropy, respectively. PL denotes hard `one-hot' labels adopted in CR Loss.}
\label{tab-algs-components}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cccccccc}
\toprule[1pt]
Algorithm     & PL  & CR Loss & Thresholding  & Dist. Align.  & Self-supervised & Mixup  &W-S Aug.   \\ 
\midrule
-Model & & MSE & & & & &\\
Pseudo Labeling &\checkmark & CE & & & & & \\
Mean Teacher & & MSE & & & & & \\
VAT & & CE & & & & &\\
MixMatch & & MSE & & & &\checkmark & \\
ReMixMatch & & CE  & &\checkmark & Rotation &\checkmark &\checkmark \\
UDA & & CE &\checkmark & & & &\checkmark \\
FixMatch &\checkmark &CE &\checkmark & & & &\checkmark \\
Dash & \checkmark &CE &\checkmark & & & &\checkmark \\
CoMatch & \checkmark & CE & \checkmark & \checkmark & Contrastive & & \checkmark \\
CRMatch & \checkmark & CE & \checkmark & & Rotation & & \checkmark \\
FlexMatch &\checkmark & CE &\checkmark & & & &\checkmark \\
AdaMatch &\checkmark & CE &\checkmark & \checkmark & & &\checkmark \\
SimMatch & \checkmark & CE & \checkmark & \checkmark & Contrastive & &\checkmark \\
\bottomrule[1pt]
\end{tabular}
}
\end{table}

\section{Benchmark Results}

For CV tasks, we follow~\cite{zhang2021flexmatch} to report the best number of all checkpoints to avoid unfair comparisons caused by different convergence speeds. For NLP and Audio tasks, we choose the best model using the validation datasets and then evaluate it on the test datasets.
In addition to mean error rate over the tasks, we use Friedman rank~\cite{friedman1937use,friedman1940comparison} to fairly compare the performance of different algorithms in various settings:

where  is the number of evaluation settings (i.e., how many experimental settings we use, e.g.,  in Table~\ref{tab-cv-results}), and  is the rank of an SSL algorithm in the -th setting.
We re-rank all algorithms to give final ranks based on their Friedman rankings.
Note that all ranks are in ascending order because the lower error rate corresponds to a better performance. The experimental setup is detailed in Appendix~\ref{sec-setup}. Note that `supervised' denotes training with the partially chosen labeled data while `fully-supervised' refers to training using all data with full annotations in our reported results.







The results for the 14 SSL algorithms on the datasets from CV, NLP, and Audio are shown in Table~\ref{tab-cv-results}, Table~\ref{tab-nlp-results}, and Table~\ref{tab-audio-results}, respectively. We adopt the pre-trained Vision Transformers (ViT)~\cite{vaswani2017attention,dosovitskiy2020image,bert,baevski2020wav2vec} instead of training ResNets~\cite{he2016deep} from scratch for CV tasks. For NLP, we adopt Bert~\cite{bert}. Wav2Vec 2.0~\cite{baevski2020wav2vec} and HuBert \cite{hsu2021hubert} are used for Audio. 

\subsection{CV Results}
\begin{table}[t!]
\centering
\caption{Error rate (\%) and Rank with CV tasks in USB. For Semi-Aves and STL10, as they have unlabeled sets, we do not report the fully-supervised results. \revision{We follow ~\cite{sohn2020fixmatch,zhang2021flexmatch,xie2020unsupervised} to show error rates as default.} }
\label{tab-cv-results}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc|c|c|c|c}
\toprule
Dataset & \multicolumn{2}{c|}{CIFAR-100}& \multicolumn{2}{c|}{STL-10} & \multicolumn{2}{c|}{Euro-SAT} & \multicolumn{2}{c|}{TissueMNIST} & \multicolumn{1}{c|}{Semi-Aves} & \multicolumn{1}{c|}{Friedman}   & \multicolumn{1}{c|}{Final}  & \multicolumn{1}{c}{Mean}\\ \cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\,

\# Label & \multicolumn{1}{c}{200} & \multicolumn{1}{c|}{400} & \multicolumn{1}{c}{20}  & \multicolumn{1}{c|}{40} & \multicolumn{1}{c}{20}  & \multicolumn{1}{c|}{40}  & \multicolumn{1}{c}{80}  & \multicolumn{1}{c|}{400}   & \multicolumn{1}{c|}{5,959} & \multicolumn{1}{c|}{rank}   & 
\multicolumn{1}{c|}{rank} & 
\multicolumn{1}{c}{error rate}\\ \cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\cmidrule(l){11-11} \cmidrule(l){12-12} \cmidrule(l){13-13}\
Fully-Supervised & 8.44\tiny{±0.07}                         & 8.44\tiny{±0.07}                         & -                    & -                     & 0.94\tiny{±0.07}                       & 0.89\tiny{±0.05}                       & 29.15\tiny{±0.13}                          & 29.10\tiny{±0.02}                           & -                         & -                                 & -                              & -                                   \\
Supervised       & 35.63\tiny{±0.36}                        & 26.08\tiny{±0.50}                        & 47.02\tiny{±1.48}                    & 26.02\tiny{±0.72}                     & 27.12\tiny{±1.26}                      & 16.90\tiny{±1.48}                      & 59.91\tiny{±2.93}                          & 54.10\tiny{±1.52}                           & 41.55\tiny{±0.29}                           & -                                 & -                              & -                                   \\
\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\cmidrule(l){11-11}\cmidrule(l){12-12} \cmidrule(l){13-13}\,
-model         & 36.24\tiny{±0.27}                        & 26.49\tiny{±0.64}                        & 44.38\tiny{±1.59}                    & 25.76\tiny{±2.37}                     & 24.51\tiny{±1.02}                      & 11.58\tiny{±1.32}                      & 56.79\tiny{±5.91}                          & \textbf{47.50\tiny{±1.71}}                  & 39.23\tiny{±0.36}                           & 10.11                             & 11                             & 34.72                               \\
Pseudo-Labeling  & 33.16\tiny{±1.20}                        & 25.29\tiny{±0.67}                        & 45.13\tiny{±4.08}                    & 26.20\tiny{±1.53}                     & 23.64\tiny{±0.90}                      & 15.61\tiny{±2.51}                      & 56.22\tiny{±4.01}                          & 50.36\tiny{±1.62}                           & 40.13\tiny{±0.09}                           & 9.89                              & 10                             & 35.08                               \\
Mean Teacher     & 35.61\tiny{±0.38}                        & 25.97\tiny{±0.37}                        & 39.94\tiny{±1.99}                    & 20.16\tiny{±1.25}                     & 26.51\tiny{±1.15}                      & 17.05\tiny{±2.07}                      & 61.40\tiny{±2.48}                          & 55.22\tiny{±2.06}                           & 38.52\tiny{±0.27}                           & 10.89                             & 14                             & 35.60                               \\
VAT              & 31.61\tiny{±1.37}                        & 21.29\tiny{±0.32}                        & 52.03\tiny{±0.48}                    & 23.10\tiny{±0.72}                     & 24.77\tiny{±1.94}                      & 9.30\tiny{±1.23}                       & 58.50\tiny{±6.41}                          & 51.31\tiny{±1.66}                           & 39.00\tiny{±0.30}                           & 10.11                             & 12                             & 34.55                               \\
MixMatch         & 37.43\tiny{±0.58}                        & 26.17\tiny{±0.24}                        & 48.98\tiny{±1.41}                    & 25.56\tiny{±3.00}                     & 29.86\tiny{±2.89}                      & 16.39\tiny{±3.17}                      & 55.73\tiny{±2.29}                          & 49.08\tiny{±1.06}                           & 37.22\tiny{±0.15}                           & 10.11                             & 12                             & 36.27                               \\
ReMixMatch       & \textbf{20.85\tiny{±1.42}}               & \textbf{16.80\tiny{±0.59}}               & 30.61\tiny{±3.47}                    & 18.33\tiny{±1.98}                     & \textbf{4.53\tiny{±1.60}}              & \textbf{4.10\tiny{±0.37}}              & 59.29\tiny{±5.16}                          & 52.92\tiny{±3.93}                           & \textbf{30.40\tiny{±0.33}}                  & 4.00                              & 1                              & 26.43                               \\
UDA              & 30.75\tiny{±1.03}                        & 19.94\tiny{±0.32}                        & 39.22\tiny{±2.87}                    & 23.59\tiny{±2.97}                     & 11.15\tiny{±1.20}                      & 5.99\tiny{±0.75}                       & 55.88\tiny{±3.26}                          & 51.42\tiny{±2.05}                           & 32.55\tiny{±0.26}                           & 6.89                              & 7                              & 30.05                               \\
FixMatch         & 30.45\tiny{±0.65}                        & 19.48\tiny{±0.93}                        & 42.06\tiny{±3.94}                    & 24.05\tiny{±1.79}                     & 12.48\tiny{±2.57}                      & 6.41\tiny{±1.64}                       & 55.95\tiny{±4.06}                          & 50.93\tiny{±1.23}                           & 31.74\tiny{±0.33}                           & 6.56                              & 6                              & 30.39                               \\
Dash             & 30.19\tiny{±1.34}                        & 18.90\tiny{±0.42}                        & 43.34\tiny{±1.46}                    & 25.90\tiny{±0.35}                     & 9.44\tiny{±0.75}                       & 7.00\tiny{±1.39}                       & 57.00\tiny{±2.81}                          & 50.93\tiny{±1.54}                           & 32.56\tiny{±0.39}                           & 7.44                              & 9                              & 30.58                               \\
CoMatch          & 35.68\tiny{±0.54}                        & 26.10\tiny{±0.09}                        & \textbf{29.70\tiny{±1.17}}           & 21.46\tiny{±1.34}                     & 5.25\tiny{±0.49}                       & 4.89\tiny{±0.86}                       & 57.15\tiny{±3.46}                          & 51.83\tiny{±0.71}                           & 41.39\tiny{±0.16}                           & 7.22                              & 8                              & 30.38                               \\
CRMatch          & 29.43\tiny{±1.11}                        & 18.50\tiny{±0.26}                        & 30.55\tiny{±2.01}                    & \textbf{17.43\tiny{±1.96}}            & 14.52\tiny{±1.34}                      & 7.00\tiny{±0.69}                       & \textbf{54.84\tiny{±3.05}}                 & 51.10\tiny{±1.59}                           & 31.97\tiny{±0.10}                           & 4.67                              & 2                              & 28.37                               \\
FlexMatch        & 27.08\tiny{±0.90}                        & 17.67\tiny{±0.66}                        & 37.58\tiny{±2.97}                    & 23.40\tiny{±1.50}                     & 7.07\tiny{±2.32}                       & 5.58\tiny{±0.57}                       & 57.23\tiny{±2.50}                          & 52.06\tiny{±1.78}                           & 33.09\tiny{±0.16}                           & 6.44                              & 5                              & 28.97                               \\
AdaMatch         & 21.27\tiny{±1.04}                        & 17.01\tiny{±0.55}                        & 36.25\tiny{±1.89}                    & 23.30\tiny{±0.73}                     & 5.70\tiny{±0.37}                       & 4.92\tiny{±0.87}                       & 57.87\tiny{±4.47}                          & 52.28\tiny{±0.79}                           & 31.54\tiny{±0.10}                           & 5.22                              & 3                              & 27.79                               \\
SimMatch         & 23.26\tiny{±1.25}                        & 16.82\tiny{±0.40}                        & 34.12\tiny{±1.63}                    & 22.97\tiny{±2.04}                     & 6.88\tiny{±1.77}                       & 5.86\tiny{±1.07}                       & 57.91\tiny{±4.60}                          & 51.14\tiny{±1.83}                           & 34.14\tiny{±0.30}                           & 5.44                              & 4                              & 28.12                              \\



\bottomrule
\end{tabular}
}
\end{table}





The results are illustrated in Table \ref{tab-cv-results}.
Thanks to the good initialization of representation on unlabeled data given by the pre-trained ViT, SSL algorithms, even without using thresholding techniques, often achieve much better performance than the previous performance shown in TorchSSL \cite{zhang2021flexmatch}. Among all the SSL algorithms, ReMixMatch \cite{berthelot2019remixmatch} ranks at the first and outperforms other SSL algorithms, due to the usage of Mixup, Distribution Alignment, and rotation self-supervised loss. Its superiority is especially demonstrated in the evaluation of Semi-Aves, a long-tailed and fine-grained CV dataset that is more realistic. Notice that SSL algorithms with self-supervised feature loss generally perform well than other SSL algorithms, e.g., CRMatch \cite{fan2021revisiting} and SimMatch \cite{zheng2022simmatch} rank second and fourth respectively. Adaptive thresholding algorithms also demonstrate their effectiveness, e.g., AdaMatch \cite{berthelot2021adamatch} and FlexMatch \cite{zhang2021flexmatch} rank at third and fifth respectively.  While better results of the evaluated SSL algorithms are obtained on CIFAR-100, Euro-SAT, and Semi-Aves, we also observe that the performance is relatively lower on STL-10 and TissueMNIST. The reason for lower performance on STL-10 might result from the usage of the self-supervised pre-trained model \cite{he2021masked}, rather than the supervised pre-trained model is used in other settings. Since TissueMNIST is a medial-related dataset, the biased pseudo-labels might produce a destructive effect that impedes training and leads to bad performance. The de-biasing of pseudo-labels and safe semi-supervised learning would be interesting topics in future work, especially for medical applications of SSL algorithms. 

\subsection{NLP Results}

\begin{table}[t!]
\centering
\caption{Error rate (\%) and Rank with NLP tasks in USB.}
\label{tab-nlp-results}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc|cc|c|c|c}
\toprule
Dataset & \multicolumn{2}{c|}{IMDB}& \multicolumn{2}{c|}{AG News} & \multicolumn{2}{c|}{Amazon Review}  & \multicolumn{2}{c|}{Yahoo! Answer} & 
\multicolumn{2}{c|}{Yelp Review} & \multicolumn{1}{c|}{Friedman}   & 
\multicolumn{1}{c|}{Final}   &\multicolumn{1}{c}{Mean}\\ \cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\,

\# Label & \multicolumn{1}{c}{20} & \multicolumn{1}{c|}{100} & \multicolumn{1}{c}{40}  & \multicolumn{1}{c|}{200} & \multicolumn{1}{c}{250}  & \multicolumn{1}{c|}{1000}   &  \multicolumn{1}{c}{500}  & \multicolumn{1}{c|}{2000} & 
\multicolumn{1}{c}{250}  & \multicolumn{1}{c|}{1000} & \multicolumn{1}{c|}{rank}   & 
\multicolumn{1}{c|}{rank} & \multicolumn{1}{c}{error rate}\\ \cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(l){12-12}\cmidrule(l){13-13} \cmidrule(l){14-14}\,
Fully-Supervised & 5.87\tiny{±0.01}                       & 5.84\tiny{±0.12}                        & 5.74\tiny{±0.30}                        & 5.64\tiny{±0.05}                         & 36.81\tiny{±0.05}                              & 36.88\tiny{±0.19}                               & 26.25\tiny{±1.07}                              & 25.55\tiny{±0.43}                               & 31.74\tiny{±0.23}                            & 32.70\tiny{±0.58}                             & -                                  & -                               & -                                    \\
Supervised       & 20.63\tiny{±3.13}                      & 13.47\tiny{±0.55}                       & 15.01\tiny{±1.21}                       & 13.00\tiny{±1.00}                        & 51.74\tiny{±0.63}                              & 47.34\tiny{±0.66}                               & 37.10\tiny{±1.22}                              & 33.56\tiny{±0.08}                               & 50.27\tiny{±0.51}                            & 46.96\tiny{±0.42}                             & -                                  & -                               &  -                                   \\
\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(l){12-12}\cmidrule(l){13-13} \cmidrule(l){14-14}\,
-Model         & 49.02\tiny{±1.37}                      & 27.57\tiny{±15.85}                      & 46.84\tiny{±6.20}                       & 13.44\tiny{±0.76}                        & 73.53\tiny{±6.92}                              & 48.27\tiny{±0.48}                               & 41.37\tiny{±2.15}                              & 32.96\tiny{±0.16}                               & 73.35\tiny{±2.31}                            & 52.02\tiny{±1.48}                             & 11.80                             & 12                             & 45.84                               \\
Pseudo-Labeling  & 26.38\tiny{±4.04}                      & 21.38\tiny{±1.34}                       & 23.86\tiny{±7.63}                       & 12.29\tiny{±0.40}                        & 53.00\tiny{±1.48}                              & 46.49\tiny{±0.45}                               & 38.60\tiny{±1.09}                              & 33.44\tiny{±0.24}                               & 55.70\tiny{±0.95}                            & 47.72\tiny{±0.37}                             & 10.60                             & 11                             & 35.89                               \\
Mean Teacher     & 21.27\tiny{±3.72}                      & 14.11\tiny{±1.77}                       & 14.98\tiny{±1.10}                       & 13.23\tiny{±1.12}                        & 51.67\tiny{±0.45}                              & 47.51\tiny{±0.24}                               & 36.97\tiny{±1.02}                              & 33.43\tiny{±0.22}                               & 51.07\tiny{±1.44}                            & 46.61\tiny{±0.34}                             & 9.30                              & 10                             & 33.09                               \\
VAT              & 32.59\tiny{±4.69}                      & 14.42\tiny{±2.53}                       & 15.00\tiny{±1.12}                       & 11.59\tiny{±0.94}                        & 50.38\tiny{±0.83}                              & 46.04\tiny{±0.28}                               & 35.16\tiny{±0.74}                              & 31.53\tiny{±0.41}                               & 52.76\tiny{±0.87}                            & 45.53\tiny{±0.13}                             & 8.40                              & 8                              & 33.50                               \\
UDA              & 9.36\tiny{±1.26}                       & 8.33\tiny{±0.61}                        & 18.73\tiny{±2.68}                       & 12.34\tiny{±1.90}                        & 52.48\tiny{±1.20}                              & 45.51\tiny{±0.61}                               & 35.31\tiny{±0.43}                              & 32.01\tiny{±0.68}                               & 58.22\tiny{±0.40}                            & 42.18\tiny{±0.68}                             & 8.70                              & 9                              & 31.45                               \\
FixMatch         & 8.20\tiny{±0.29}                       & \textbf{7.36\tiny{±0.07}}               & 22.80\tiny{±5.18}                       & 11.43\tiny{±0.65}                        & 47.85\tiny{±1.22}                              & 43.73\tiny{±0.45}                               & 34.15\tiny{±0.94}                              & 30.76\tiny{±0.53}                               & 50.34\tiny{±0.40}                            & 41.99\tiny{±0.58}                             & 5.60                              & 7                              & 29.86                               \\
Dash             & 8.93\tiny{±1.27}                       & 7.97\tiny{±0.53}                        & 19.30\tiny{±6.73}                       & 11.20\tiny{±1.12}                        & 47.79\tiny{±1.03}                              & 43.52\tiny{±0.07}                               & 35.10\tiny{±1.36}                              & 30.51\tiny{±0.47}                               & 47.99\tiny{±1.05}                            & 41.59\tiny{±0.61}                             & 5.10                              & 6                              & 29.39                               \\
CoMatch          & 7.36\tiny{±0.26}                       & 7.41\tiny{±0.20}                        & 13.25\tiny{±1.31}                       & 11.61\tiny{±0.42}                        & 48.98\tiny{±1.20}                              & 44.37\tiny{±0.25}                               & 33.48\tiny{±0.67}                              & \textbf{30.19\tiny{±0.22}}                      & 46.49\tiny{±1.42}                            & 41.11\tiny{±0.53}                             & 3.80                              & 3                              & 28.43                               \\
CRMatch          & 7.88\tiny{±0.24}                       & 7.68\tiny{±0.35}                        & 13.35\tiny{±1.06}                       & 11.36\tiny{±1.04}                        & 46.23\tiny{±0.85}                              & 43.69\tiny{±0.48}                               & 33.07\tiny{±0.68}                              & 30.62\tiny{±0.47}                               & 46.61\tiny{±1.02}                            & 41.80\tiny{±0.77}                             & 3.70                              & 2                              & 28.23                               \\
FlexMatch        & 7.35\tiny{±0.10}                       & 7.80\tiny{±0.24}                        & 16.90\tiny{±6.76}                       & 11.43\tiny{±0.91}                        & \textbf{45.75\tiny{±1.21}}                     & 43.14\tiny{±0.82}                               & 35.81\tiny{±1.09}                              & 31.42\tiny{±0.41}                               & 46.37\tiny{±0.74}                            & \textbf{40.86\tiny{±0.74}}                    & 4.10                              & 5                              & 28.68                               \\
AdaMatch         & 9.62\tiny{±1.26}                       & 7.81\tiny{±0.46}                        & \textbf{12.92\tiny{±1.53}}              & \textbf{11.03\tiny{±0.62}}               & 46.75\tiny{±1.23}                              & 43.50\tiny{±0.67}                               & \textbf{32.97\tiny{±0.43}}                     & 30.82\tiny{±0.29}                               & 48.16\tiny{±0.80}                            & 41.71\tiny{±1.08}                             & 4.00                              & 4                              & 28.53                               \\
SimMatch         & \textbf{7.24\tiny{±0.02}}              & 7.44\tiny{±0.20}                        & 14.80\tiny{±0.57}                       & 11.12\tiny{±0.15}                        & 47.27\tiny{±1.73}                              & \textbf{43.09\tiny{±0.50}}                      & 34.15\tiny{±0.91}                              & 30.64\tiny{±0.42}                               & \textbf{46.40\tiny{±1.71}}                   & 41.24\tiny{±0.17}                             & 2.90                              & 1                              & 28.34                               \\
\bottomrule  
\end{tabular}
}
\end{table}

The results of NLP tasks are demonstrated in Table \ref{tab-nlp-results}. The overall ranking of SSL algorithms in NLP is similar to that in CV. However, the SSL algorithm that works well in NLP does not always guarantee good performance in CV, which shows that the performance of SSL algorithms will be affected largely by data domains. For example, SimMatch which ranks first in NLP does not have the best performance in CV tasks (ranks fourth). The ranking of CoMatch is also increased in NLP, compared to that in CV. A possible reason is the different pre-training in backbones. For BERT, a masked language modeling objective is used during pre-training~\cite{bert}, thus the self-supervised feature loss might further improve the representation during fine-tuning with SSL algorithms. We observe that adaptive thresholding methods, such as FlexMatch and AdaMatch, consistently achieve good performance on both CV and NLP, even without self-supervised loss.
Note that we do not evaluate MixMatch and ReMixMatch on NLP and Audio tasks because we find that mixing sentences with different lengths harms the model's performance.

\subsection{Audio Results}

\begin{table}[t!]
\centering
\caption{Error rate (\%) and Rank with Audio tasks in USB. Fully-supervised result is not reported for FSDNoisy18k due to the unknown labels of its unlabeled set. }
\label{tab-audio-results}
\resizebox{\textwidth}{!}{

\begin{tabular}{l|cc|cc|cc|cc|c|c|c|c}
\toprule
Dataset & \multicolumn{2}{c|}{GTZAN}& \multicolumn{2}{c|}{UrbanSound8k} & \multicolumn{2}{c|}{Keyword Spotting} & \multicolumn{2}{c|}{ESC-50} & \multicolumn{1}{c|}{FSDnoisy} &
\multicolumn{1}{c|}{Friedman}   & 
\multicolumn{1}{c|}{Final}  & \multicolumn{1}{c}{Mean} \\ \cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\,

\# Label & \multicolumn{1}{c}{100} & \multicolumn{1}{c|}{400} & \multicolumn{1}{c}{100}  & \multicolumn{1}{c|}{400}   & \multicolumn{1}{c}{50}  & \multicolumn{1}{c|}{100}   &  \multicolumn{1}{c}{250}  & \multicolumn{1}{c|}{500} &  \multicolumn{1}{c|}{1,772} & \multicolumn{1}{c|}{rank}   & \multicolumn{1}{c|}{rank} &  \multicolumn{1}{c}{error rate}
 \\ \cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\cmidrule(l){11-11}\cmidrule(l){12-12}\cmidrule(l){13-13}\,
Fully-Supervised & 5.98\tiny{±0.32}           & 5.98\tiny{±0.32}           & 16.65\tiny{±1.71}          & 16.61\tiny{±1.71}          & 2.12\tiny{±0.11}          & 2.25\tiny{±0.02}          & 26.00\tiny{±2.13}          & 26.00\tiny{±2.13}          & -         & -                                 & -                              & -                                   \\
Supervised       & 52.16\tiny{±1.83}          & 31.53\tiny{±0.52}          & 40.42\tiny{±1.00}          & 28.55\tiny{±1.90}          & 6.80\tiny{±1.16}          & 5.25\tiny{±0.56}          & 51.58\tiny{±1.12}          & 35.67\tiny{±0.42}          & 35.20\tiny{±1.50}          & -                                 & -                              & -                                   \\
\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\cmidrule(l){11-11}\cmidrule(l){12-12}\cmidrule(l){13-13}\,
-Model         & 74.07\tiny{±0.62}          & 33.18\tiny{±3.64}          & 54.24\tiny{±6.01}          & 25.89\tiny{±1.51}          & 64.39\tiny{±4.10}         & 25.48\tiny{±4.94}         & 47.25\tiny{±1.14}          & 36.00\tiny{±1.62}          & 35.73\tiny{±0.87}          & 10.67                             & 12                             & 44.03                               \\
Pseudo-Labeling  & 57.29\tiny{±2.80}          & 33.93\tiny{±0.69}          & 42.09\tiny{±2.41}          & 27.00\tiny{±1.34}          & 7.82\tiny{±1.64}          & 5.16\tiny{±0.14}          & 49.33\tiny{±2.52}          & 35.58\tiny{±1.05}          & 35.34\tiny{±1.60}          & 10.00                             & 10                             & 32.62                               \\
Mean Teacher     & 51.40\tiny{±3.48}          & 31.60\tiny{±1.46}          & 41.70\tiny{±3.39}          & 28.91\tiny{±0.93}          & 5.95\tiny{±0.44}          & 5.39\tiny{±0.42}          & 50.25\tiny{±1.95}          & 37.33\tiny{±1.20}          & 35.83\tiny{±1.22}          & 10.33                             & 11                             & 32.04                               \\
VAT              & 79.51\tiny{±1.99}          & 35.38\tiny{±7.80}          & 49.62\tiny{±2.42}          & 27.68\tiny{±1.39}          & \textbf{2.18\tiny{±0.08}} & \textbf{2.23\tiny{±0.08}} & 46.42\tiny{±1.90}          & 36.92\tiny{±2.25}          & 32.07\tiny{±1.05}          & 8.33                              & 9                              & 34.67                               \\
UDA              & 46.56\tiny{±8.69}          & 23.62\tiny{±0.63}          & 37.28\tiny{±3.17}          & 20.27\tiny{±1.58}          & 2.52\tiny{±0.15}          & 2.62\tiny{±0.10}          & 42.75\tiny{±0.89}          & 33.50\tiny{±1.95}          & 30.80\tiny{±0.47}          & 6.33                              & 7                              & 26.66                               \\
FixMatch         & 36.04\tiny{±4.57}          & 22.09\tiny{±0.65}          & 36.12\tiny{±4.26}          & 21.43\tiny{±2.88}          & 4.84\tiny{±3.57}          & 2.38\tiny{±0.03}          & \textbf{37.75\tiny{±3.19}} & 30.67\tiny{±1.05}          & 30.31\tiny{±1.08}          & 4.00                              & 3                              & 24.63                               \\
Dash             & 47.00\tiny{±3.65}          & 23.42\tiny{±0.83}          & 42.02\tiny{±5.02}          & 22.26\tiny{±0.89}          & 5.70\tiny{±4.40}          & 2.52\tiny{±0.16}          & 48.17\tiny{±1.16}          & 32.75\tiny{±2.27}          & 33.19\tiny{±0.95}          & 7.56                              & 8                              & 28.56                               \\
CoMatch          & 36.93\tiny{±1.23}          & 22.20\tiny{±1.39}          & \textbf{30.59\tiny{±2.45}} & 21.35\tiny{±1.49}          & 11.39\tiny{±0.85}         & 9.44\tiny{±1.52}          & 40.17\tiny{±2.08}          & \textbf{29.83\tiny{±1.31}} & 27.63\tiny{±1.35}          & 5.11                              & 6                              & 25.50                               \\
CRMatch          & 40.58\tiny{±3.97}          & 22.64\tiny{±1.22}          & 39.47\tiny{±4.66}          & 20.11\tiny{±2.63}          & 2.40\tiny{±0.13}          & 2.49\tiny{±0.08}          & 42.67\tiny{±0.51}          & 33.58\tiny{±1.93}          & 30.45\tiny{±1.52}          & 5.00                              & 5                              & 26.04                               \\
FlexMatch        & 34.60\tiny{±4.07}          & 21.82\tiny{±1.17}          & 40.18\tiny{±2.73}          & 22.82\tiny{±3.10}          & 2.42\tiny{±0.08}          & 2.57\tiny{±0.25}          & 39.58\tiny{±0.59}          & 29.92\tiny{±1.85}          & \textbf{26.36\tiny{±0.55}} & 4.11                              & 4                              & 24.47                               \\
AdaMatch         & \textbf{31.38\tiny{±0.41}} & \textbf{20.73\tiny{±0.67}} & 35.76\tiny{±6.39}          & 21.15\tiny{±1.22}          & 2.49\tiny{±0.08}          & 2.49\tiny{±0.10}          & 39.17\tiny{±1.74}          & 31.33\tiny{±1.23}          & 27.95\tiny{±0.74}          & 2.89                              & 1                              & 23.61                               \\
SimMatch         & 32.42\tiny{±2.18}          & 20.80\tiny{±0.77}          & 31.70\tiny{±6.05}          & \textbf{19.55\tiny{±1.89}} & 2.57\tiny{±0.08}          & 2.53\tiny{±0.22}          & 39.92\tiny{±2.35}          & 32.83\tiny{±1.43}          & 28.16\tiny{±0.87}          & 3.67                              & 2                              & 23.39                              \\

\bottomrule
\end{tabular}
}
\end{table}

The results of Audio tasks are shown in Table \ref{tab-audio-results}. AdaMatch outperforms other algorithms in Audio tasks, while SimMatch demonstrates a similar performance to AdaMatch. An interesting finding is that CRMatch performs well on CV and NLP tasks, but badly in Audio tasks. We hypothesize that this is partially due to the noisy nature of the raw data in audio tasks. Except for Keyword Spotting, the gap between the performance of fully-supervised learning and that of SSL algorithms in Audio tasks is larger than in CV and NLP tasks. The reason behind this is probably that we exploit models that take waveform as input, rather than Mel spectrogram. Raw waveform might contain more noisy information that would be harmful to semi-supervised training. We identify exploring audio models based on Mel spectrogram as one of the future directions of USB.





\subsection{Discussion} 
\label{sec-results-analysis}

The evaluation results of SSL algorithms using USB are generally consistent with the results reported by previous work~\cite{oliver2018realistic,berthelot2019mixmatch,berthelot2019remixmatch,xie2020unsupervised,sohn2020fixmatch,zhang2021flexmatch}. However, using USB, we still provide some distinct quantitative and qualitative analysis to inspire the community. This section aims to answer the following questions: (1) Why should we evaluate an SSL algorithm on diverse tasks across domains? (2) Which option is better in the SSL scenario, training from scratch or using pre-training? (3) Does SSL consistently guarantee the performance improvement when using the state-of-the-art neural models as the backbones? 



\paragraph{Performance Comparisons}
Table~\ref{tab-rank-comparison} shows the performance comparison of SSL algorithms in CV, NLP and Audio tasks. Although the ranking of each SSL algorithm in each domain is roughly close, the differences between ranks of SSL algorithms in different domains cannot be ignored. 
\revision{For example}, FixMatch, CoMatch and CrMatch show large difference () on the ranks across domains, \revision{which indicates that NLP and Audio tasks may have different characteristics compared with CV tasks that are more amenable to certain types of SSL algorithms compared with others. From the task perspective, it is important to consider such characteristics for guiding the choice of SSL methods.}
\revision{From the benchmarking perspective,} it is \revision{useful} to introduce diverse tasks from multiple domains when evaluating an SSL algorithm. 



\begin{figure}[t!]
\centering
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/conv_classic.pdf}
    \caption{WRN-28-8 from scratch.}
    \label{fig-conv-speed-wrn}
\end{subfigure}\begin{subfigure}{0.48\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/conv_usb.pdf}
    \caption{Pre-trained ViT-S-P2-32.}
    \label{fig-conv-speed-pretrained-vit}
\end{subfigure}
\hfill
\caption{Comparison of test accuracy of SSL algorithms on CIFAR-100 with 400 labels. (a) Existing protocol which trains WRN-28-8 from scratch; (b) USB CV protocol which trains ImageNet-1K pre-trained ViT-S-P2-32, where S denotes small, P denotes patch size, and 32 is input image size.}
\label{fig-conv-all}
\end{figure}

\begin{figure}[t!]
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/fix_test_acc.pdf}
    \caption{Test accuracy.}
    \label{fig-test-fix}
\end{subfigure}\begin{subfigure}{0.48\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/fix_pl_acc.pdf}
    \caption{Pseudo-label accuracy.}
    \label{fig-pl-fix}
\end{subfigure}
\hfill
\caption{Pre-training ablation on CIFAR-400 with 400 labels. Test and pseudo-label accuracy are compared with WRN-28-8 without pre-training, pre-trained WRN-28-8, pre-trained ViT-S-P16-224, ViT-S-P2-32 without pre-training, and pre-trained ViT-S-P2-32.}
\label{fig-pl-test-acc}
\end{figure}


\paragraph{Effectiveness of Pre-training}
\label{sec-ablation-pretrain}
As shown in Figure~\ref{fig-conv-speed-wrn} and Figure~\ref{fig-conv-speed-pretrained-vit}, benefiting from the pre-trained ViT, the training becomes more efficient, and most SSL algorithms achieve higher optimal performance. 
Note that Pseudo Labeling, Mean Teacher,  model, VAT, and MixMatch barely converge if training WRN-28-8 from scratch. A possible reason is that the scarce labeled data cannot provide enough supervision for unlabeled data to form correct clusters. However, these methods can achieve sufficiently reasonable results when using pre-trained ViT. As illustrated in Figure~\ref{fig-pl-test-acc}, using ViT without pre-training performs the worst among different backbones. The reason can be that ViT is data hungry if trained from scratch~\cite{dosovitskiy2020image,han2022survey,touvron2021training}. However, after appropriate pre-training, ViT performs the best among all the backbones. In addition, we provide the T-SNE visualization of the features in Figure~\ref{fig-tsne-train}, where the pretrained ViT model demonstrates the most separable feature space after training.
In a word, pre-trained ViT makes the training more efficient and improves the generalization performance of SSL algorithms. For NLP tasks, we observe similar results, yet the improvement can be relatively less significant since pre-training is the de-facto fashion in the field.







\begin{table}[t!]
\centering
\caption{Final ranks of SSL algorithms. Note that the rank for CV tasks here is different from the ones in Table~\ref{tab-cv-results} because we ignore MixMatch and ReMixMatch here to remove the effects of their missing ranks in NLP and Audio.}
\label{tab-rank-comparison}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
 & -Model   & Pseudo-Labeling & Mean Teacher & VAT  & UDA & FixMatch & Dash & CoMatch & CRMatch & FlexMatch & AdaMatch & SimMatch    \\ 
\midrule

 CV &  10  & 9 & 12 & 11  & 6 & 5 & 8 & 7 & 1 & 4 & 2 & 3   \\
 NLP &  12  & 11 & 10 & 8  & 9 & 7 & 6 & 3 & 2 & 5 & 4 & 1 \\
 Audio &  12  & 10 & 11 & 9  & 7 & 3 & 8 & 6 & 5 & 4 & 1 & 2   \\ 
   &  2  & 2 & 2 & 3  & 3 & \textbf{4} & 2 & \textbf{4} & \textbf{4} & 1 & 3 & 2   \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t!]
\centering
\caption{This table shows how many times an SSL algorithm is worse than supervised training, where the numbers of total settings are 9, 10, and 9 for CV, NLP, and Audio respectively.}
\label{tab-robust-ssl}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
 & -Model   & Pseudo-Labeling & Mean Teacher & VAT & MixMatch & ReMixMatch & UDA & FixMatch & Dash & CoMatch & CRMatch & FlexMatch & AdaMatch & SimMatch    \\ 
\midrule

 CV  &  2  & 1  & 3 & 1   & 4 & 0 & 0 & 0 & 0 & 2 & 0  & 0 & 0 & 0   \\
 NLP  &  9  & 7 & 5 & 3  & - & - & 2 & 1 &  1 & 0 &  0 & 1 & 0 &  0  \\
 Audio  & 7 & 5 &  6 & 4 & - & -  & 0 & 0 & 1 & 2 & 0 & 0 & 0 &  0  \\            
\bottomrule
\end{tabular}
}
\end{table}


\paragraph{Robustness}
SSL sometimes hurts the generalization performance due to the large differences between the number of labeled data and the number of unlabeled data as shown in Table~\ref{tab-robust-ssl}. We refer to an SSL algorithm as a robust SSL algorithm if it is consistently better than the supervised training setting. SSL algorithms cannot always outperform supervised training especially when labeled data is scarce. We find that CRMatch, AdaMatch and SimMatch are relatively robust SSL algorithms in USB. Although previous work has done some research towards robust SSL when using support vector machine~\cite{LiZ15pami, noble2006support}, we hope that our finding can serve as the motivation to delve into deep learning based robust SSL methods.



\begin{figure}[t!]
\centering
\hfill
\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/tsne_train_wrn_wo_pretrain.pdf}
    \caption{WRN-28-8 from scratch.}
\end{subfigure}\hfill
\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/tsne_train_wrn_w_pretrain.pdf}
    \caption{Pre-trained WRN-28-8.}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/tsne_train_vit_w_pretrain.pdf}
    \caption{Pre-trained ViT-S-P2-32.}
\end{subfigure}
\hfill
\\
\hfill
\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/tsne_test_wrn_wo_pretrain.pdf}
    \caption{WRN-28-8 from scratch.}
\end{subfigure}\hfill
\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/tsne_test_wrn_w_pretrain.pdf}
    \caption{Pre-trained WRN-28-8.}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=0.95\linewidth]{saved_figs/tsne_test_vit_w_pretrain.pdf}
    \caption{Pre-trained ViT-S-P2-32.}
\end{subfigure}
\hfill
\caption{T-SNE visualization of FixMatch features on training data (first row) and testing data (second row) of CIFAR-100 (400 labels). Different colors refer to labeled data with different classes while unlabeled data is indicated by gray color.}
\label{fig-tsne-train}
\end{figure}



\section{Codebase Structure of USB}
\label{sec-code-structure}

In this section, we provide an overview of the codebase structure of USB, where four abstract layers are adopted. The layers include the core layer, algorithm layer, extension layer, and API layer in the bottom up direction as shown in \cref{fig:usb_code}. 

\textbf{Core Layer}. In the core layer, we implement the commonly used core functions for training SSL algorithms. Besides, the code regarding datasets, data loaders, and models used in USB is also provided in the core layer. For flexible training, we implement common training hooks similar to MMCV \cite{mmcv}, which can be modified and extended in the upper layers. 

\textbf{Algorithm Layer}. In the algorithm layer, we first implement the base class for SSL algorithms, where we initialize the datasets, data loaders, and models from the core layer. Instead of implementing SSL algorithms independently as in TorchSSL \cite{zhang2021flexmatch}, we further abstract the SSL algorithms, enabling better code reuse and making it easier to implement new algorithms.  Except for the standalone implementation of loss functions used in SSL algorithms and algorithm-specific configurations, we further provide algorithm hooks according to the algorithm components summarized in \cref{tab-algs-components}. The algorithm hooks not only highlight the common part of different algorithms but also allows for a very easy and flexible combination of different components to resemble a new algorithm or conduct an ablation study. Based on this, we support 14 core SSL algorithms in USB, with two extra supervised learning variants. More algorithms are expected to be added through continued extension of USB.

\textbf{Extension Layer}. The extension layer is where we further extend the core SSL algorithms to different applications. Continuted effort are made on the extension of core SSL algorithms to imbalanced SSL algorithms \cite{kim2020distribution,wang22margin,li2011semi,hyun2020class,wei2021crest,yang2020rethinking,fan2021cossl,oh2021distribution} and open-set SSL algorithms \cite{saito2021openmatch,guo2020safe,yu2020multi,luo2021consistency,huang2021universal}. Systematic ablation study can also be conducted in the extension layer by inheriting either the core components and algorithms from the core layer or the algorithm layer. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{saved_figs/usb_codebase.pdf}
    \caption{Structure of USB Codebase, consisting of 4 layers. The core layer provides the common functions, datasets, and models for SSL algorithms. The algorithm layer mainly implements the related SSL algorithms, with a high abstract level of algorithm components. Upon the algorithm layer, we use an extension layer for easy and flexible extension of core SSL algorithms. The top API layer supports a public python package \textsc{Semilearn}: {\ttfamily pip install semilearn}.}
    \label{fig:usb_code}
    \vspace{-.2in}
\end{figure}



\textbf{API Layer}. We wrap the core functions and algorithms in USB in the API layer as a public python package \textsc{Semilearn}. \textsc{Semilearn} is friendly for users from different backgrounds who want to employ SSL algorithms in new applications. Training and inference can be done in only a few lines of code with \textsc{Semilearn}. In addition, we provide the configuration files of all algorithms supported in USB with detailed parameter settings, which allows for reproduction of the results present in USB. 

\section{Limitation}

\revision{Our primary focus is on semi-supervised classification in this paper. However, there are other SSL tasks that the SSL community should not ignore.} USB currently does not include SSL tasks such as imbalanced semi-supervised learning~\cite{kim2020distribution,li2011semi,hyun2020class,wei2021crest,yang2020rethinking,fan2021cossl,oh2021distribution}, open-set semi-supervised learning~\cite{saito2021openmatch,guo2020safe,yu2020multi,luo2021consistency,huang2021universal}, semi-supervised sequence modeling~\cite{clark2018semi,chen2020seqvat,li2005semi,dai2015semi,baskar2019semi,wang2022exploiting}, semi-supervised text generation~\cite{liu-etal-2022-semi,he2019revisiting,chen2021simple}, semi-supervised regression~\cite{wasserman2007statistical,jean2018semi,kostopoulos2018semi,zhou2005semi,li2017learning}, semi-supervised object detection~\cite{tang2016large,tang2021proposal,xu2021end,tang2021humble,gao2019note,liu2020unbiased}, semi-supervised clustering~\cite{basu2002semi,bair2013semi,grira2004unsupervised,basu2004probabilistic}, etc.
In addition, we do not implement generative adversarial networks based SSL algorithms~\cite{kingma2014semi,springenberg2015unsupervised,odena2016semi,denton2016semi} and graph neural network based SSL algorithms~\cite{verma2021graphmix,feng2020graph,sankar2019meta,gong2022self,zhao2020uncertainty} in USB, which are also important to the SSL community. Moreover, it is of great importance to extend current SSL to distributional shift settings, such as domain adaptation~\cite{wang2018visual, wang2017balanced} and out-of-distribution generalization~\cite{wang2022generalizing}, as well as time series anaysis~\cite{du2021adarnn}.
We plan to evolve the benchmark in the future iterations over time by extending with more tasks. 


\section{Conclusion}
We constructed USB, a unified SSL benchmark \revision{for classification} that aims to enable consistent evaluation over multiple datasets from multiple domains and reduce the training cost to make the evaluation of SSL more affordable. With USB, we evaluate 14 SSL algorithms on 15 tasks across domains. We find that (1) although the performance of SSL algorithms is roughly close across domains, introducing diverse tasks from multiple domains is still necessary in the SSL scenario because the performance of SSL algorithms are not exactly steady across domains; (2) pre-training techniques can be helpful in the SSL scenario because it can not only accelerate the training but also improve the generalization performance; (3) unlabeled data sometimes hurts the performance especially when labeled data is extremely scarce. USB is a project for open extension and we plan to extend USB with more challenging tasks other than classification and introduce new algorithms. 


\section*{Acknowledgments}
We would like to thank the anonymous reviewers for their insightful comments and suggestions to help improve the paper. The computing resources of this study were mainly supported by Microsoft Asia and partially supported by High-Flyer AI.





















\bibliography{bibfile}
\bibliographystyle{unsrt}


\newpage

\appendix







\revision{
\section{Details of Datasets and in TorchSSL}

We provide the details of datasets of TorchSSL in Table~\ref{tab-cv-conv-data}. 
}

\begin{table}[htbp]
\centering
\caption{Details of CV datasets and \#labels used in TorchSSL. \#Label per class represents the number of chosen labeled data per class from the training data. The test data is kept unchanged except for ImageNet where we use the validation dataset as the test dataset.}
\label{tab-cv-conv-data}
\resizebox{0.6\textwidth}{!}{\begin{tabular}{cccccc}
\toprule[1pt]
Dataset   & \#Label per class  & \#Training data  & \#Test data  & \#Class                       \\ \midrule
CIFAR-10    & 4 / 25 / 100              & 50,000  &10,000  & 10                                 \\
CIFAR-100 & 4 / 25 / 100              & 50,000  &10,000   & 100                                \\
SVHN & 4 / 25 /100               & 604,388  & 26,032   & 10                                  \\
STL-10    & 4 / 25 /100            & 100,000  &10,000  & 10                                     \\
ImageNet  & 100                & 1,281,167 &50,000 & 1,000                                  \\ 

\bottomrule[1pt]
\end{tabular}}
\end{table}





\revision{
\section{Correlation between TorchSSL and USB}


Here we show the correlation between the mean error rates on TorchSSL and USB CV tasks.
We take the 14 algorithms considered in the main paper and show their mean performance on TorchSSL versus that on USB CV tasks in Figure \ref{fig:correlation}. Despite the fact that the Pearson correlation coefficient is 0.87, the final rank SSL algorithms is not consistent, which shows different adaptability of different methods when using pre-trained ViTs. For example, FlexMatch shows the best mean performance on USB while AdaMatch has the best mean performance on TorchSSL. Please refer to Table \ref{tab-rank-comparison} for more detailed rankings on CV, NLP, and Audio.
}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\linewidth]{saved_figs/correlation.pdf}
\caption{Correlation between TorchSSL and USB.}
\label{fig:correlation}
\end{figure}
    



\revision{
\section{Performance Results on ImageNet}
\label{sec-appendix-imagenet}
Although we have excluded ImageNet from USB, we provide an evaluation on ImageNet of MAE pre-trained ViT-B, using UDA \cite{xie2020unsupervised}, FixMatch \cite{sohn2020fixmatch}, FlexMatch \cite{zhang2021flexmatch}, CoMatch \cite{li2021comatch}, and SimMatch \cite{zheng2022simmatch}. We train these algorithms using 10 labels per class and 100 labels per-class, i.e., a total of 10,000 labels and 100,000 labels respectively, corresponding to roughly 1\% and 10\% of the total labeled data in ImageNet. For learning rate and weight decay, we follow the fine-tuning protocol in MAE \cite{he2021masked}, where we use AdamW with a learning rate of 1e-3 and weight decay of 0.05. We use 16 A100 to train each algorithm and set the batch size to 256 for both labeled and unlabeled data. Other algorithmic hyper-parameters stay the same as their original implementations. 

We present the results on ImageNet in \cref{tab:imagenet}. UDA and Fixmatch are near the bottom, similar to USB. SimMatch is still marked as one of the tops. Surprisingly, CoMatch does so well on ImageNet when it ranked only 9th on the USB benchmark. Also, while FlexMatch is the best on USB, it's pretty firmly behind CoMatch and SimMatch on ImageNet. 

\begin{table}[!h]
\centering
\caption{ImageNet \textbf{accuracy} results. We use MAE pre-trained ViT-B. }
\label{tab:imagenet}
\resizebox{0.45\columnwidth}{!}{\begin{tabular}{@{}l|c|c|c@{}}
\toprule
\multicolumn{1}{c|}{Method}    & 1w Labels                  & 10w Labels                 & Rank \\ \midrule
\multicolumn{1}{l|}{UDA}      & \multicolumn{1}{c|}{38.62} & \multicolumn{1}{c|}{62.37} & 5    \\
\multicolumn{1}{l|}{FixMatch} & \multicolumn{1}{c|}{37.93} & \multicolumn{1}{c|}{62.88} & 4    \\
\multicolumn{1}{l|}{FlexMatch} & \multicolumn{1}{c|}{39.13} & \multicolumn{1}{c|}{63.09} & 3 \\
\multicolumn{1}{l|}{CoMatch}  & \multicolumn{1}{c|}{44.32} & \multicolumn{1}{c|}{65.80}  & 2    \\
\multicolumn{1}{l|}{SimMatch} & \multicolumn{1}{c|}{46.48} & \multicolumn{1}{c|}{67.61} & 1    \\ \bottomrule
\end{tabular}}
\end{table}

}


\revision{
\section{Results with Different Pre-trained Backbones}
\label{sec-appendix-backbone}

In this section, we verify USB with different pre-trained backbones. Different pre-trained backbones do affect the performance of SSL algorithms, which makes it important to report results with multiple backbones. We will continuously update results with different backbones at \url{https://github.com/microsoft/Semi-supervised-learning}. Here we report several results in Table~\ref{tab:swin-cv}, Table~\ref{tab:roberta-yelp}, and Table~\ref{tab:hubert-ks-wav2vec-fsdnoisy}. Across the tasks, there is a pretty clear distinction between the performance of algorithms in the first half of the ranking list and the second half of the ranking list. While switching out backbones does not change the membership of these two halves, it does seem like the relative orderings within the top half can indeed vary a bit. 

To compare different backbones on CV tasks, we fine-tune pre-trained public Swin-Transformer \cite{liu2021Swin} with USB. We keep all hyper-parameters the same as in \cref{tab:hyper-cv}, and mainly evaluate on EuroSAT (32) and Semi-Aves (224). For EuroSAT, we change the input image size of the pre-trained Swin-S from 224 to 32, and the window size from 7 to 4 to accommodate the adapted input image size. For Semi-Aves, we adopt the original Swin-S. From the results in \cref{tab:swin-cv}, one can observe, that on EuroSAT (32), as we adopt 224 pre-trained Swin-S and change its input and window size, the results are inferior to ViT-32 reported in the paper, whereas on Semi-Aves (224), the results are better than ViT-S. An interesting finding is that CoMatch performs relatively better with Swin-S while CrMatch performs worse. This also shows the importance of constantly updating the backbone in the future development of USB.

For NLP tasks, we additionally experiment with RoBERTa \cite{liu2019roberta}. We train RoBERTa using the same hyper-parameters reported in \cref{tab:hyper-nlp}. RoBerta generally performs better than Bert as expected. The performance difference is both very close when using RoBerta or Bert.

Due to the fact that the audio tasks setting in the current version of USB being built upon raw waveforms, there are not many pre-trained models available to use. We report the results of HuBert \cite{hsu2021hubert} and Wave2Vecv2.0 \cite{baevski2020wav2vec} for audio tasks to compare different backbones. The difference between these two backbones selected mainly lies in pre-training data. Wave2Vecv2.0 is pre-trained using raw human voice data and HuBert is an improved model with a discrete clustering target. Thus we can observe from the results, that on human voice tasks Superb-KS, Wave2Vecv2.0 has better performance, whereas, on other tasks, HuBert is more robust and outperforms Wave2Vecv2.0.
}

\begin{table}[t!]
\centering
\caption{Swin-Transformer results on EuroSAT and Semi-AVES.}
\resizebox{0.5\textwidth}{!}{
\label{tab:swin-cv}
\begin{tabular}{l|cc|c}
\toprule
Dataset & \multicolumn{2}{c|}{EuroSAT}& \multicolumn{1}{c}{Semi-Aves} \\

\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-4}\,

\# Label  & 20                 & 40                & 5,959                  \\
\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-4}\,
Supervised       & 44.32\tiny{±1.10}           & 34.40\tiny{±1.44}           & 38.76\tiny{±0.21 }          \\
Fully-Supervised &  \multicolumn{2}{c|}{1.86\tiny{±0.10 }}                        & \multicolumn{1}{c}{ -}   \\ 

\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-4}\,
-Model        & 42.49\tiny{±3.21}         & 30.54\tiny{±1.37}         & 38.74\tiny{±0.60}          \\
Pseudo-Labeling  & 42.49\tiny{±3.21}         & 30.54\tiny{±1.37}         & 38.74\tiny{±0.60}          \\
Mean Teacher     & 35.85\tiny{±1.95}         & 19.62\tiny{±3.28}         & 33.37\tiny{±0.06}          \\
VAT              & 40.63\tiny{±2.68}         & 29.94\tiny{±1.87}         & 35.84\tiny{±0.36}          \\
UDA              & 18.15\tiny{±5.70}         & 12.09\tiny{±1.26}         & 29.28\tiny{±0.20}          \\
FixMatch        & 17.19\tiny{±3.46}         & 12.57\tiny{±1.28}         & 28.88\tiny{±0.22}          \\
Dash           & 18.04\tiny{±1.21}         & 12.98\tiny{±1.27}         & 28.69\tiny{±0.39}          \\
CoMatch       & 13.65\tiny{±1.42}         & 10.17\tiny{±0.68}         & 37.71\tiny{±0.31}          \\
CRMatch       & 30.28\tiny{±1.64}         & 22.39\tiny{±1.41}         & 29.22\tiny{±0.21}          \\
FlexMatch        & 10.46\tiny{±1.20}         & 9.06\tiny{±1.80}         & 30.19\tiny{±0.51}          \\
SimMatch        & 11.19\tiny{±1.01}         & 10.65\tiny{±1.64}         & 28.55\tiny{±0.13}          \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{table}[t!]
\centering
\caption{RoBERTa results on Yelp.}
 \label{tab:roberta-yelp}
\resizebox{0.35\textwidth}{!}{\begin{tabular}{@{}l|ll@{}}
\toprule
Dataset         & \multicolumn{2}{c}{Yelp}                                                  \\ \midrule
\# Labels         & \multicolumn{1}{c}{250}                                 & \multicolumn{1}{c}{1000}                                \\ \midrule
Supervised       & 42.56\tiny{±1.15 } & 39.00\tiny{±0.16 } \\ 
Fully-Supervised & \multicolumn{2}{c}{29.15\tiny{±0.12 }} \\ \midrule
Pseudo-Label     & 48.26\tiny{±0.02 } & 40.56\tiny{±0.16 } \\
MeanTeacher      & 49.41\tiny{±0.03 } & 44.36\tiny{±1.04 } \\
-Model   & 49.16\tiny{±2.04 } & 42.93\tiny{±0.88 } \\
VAT      & 43.04\tiny{±0.02 } & 39.24\tiny{±0.06}  \\
AdaMatch & 38.24\tiny{±0.02 } & 35.64\tiny{±0.06 } \\
UDA      & 40.13\tiny{±0.15 } & 38.98\tiny{±0.03}  \\
FixMatch & 39.82\tiny{±0.95 } & 37.42\tiny{±0.30 } \\
FlexMatch        & 39.11\tiny{±0.02 } & 36.84\tiny{±0.01 } \\
Dash     & 39.86\tiny{±1.01 } & 36.23\tiny{±0.21 } \\
CRMatch  & 40.08\tiny{±1.28 } & 35.85\tiny{±0.38 } \\
CoMatch  & 39.95\tiny{±0.86 } & 36.89\tiny{±0.22 } \\
SimMatch & 38.76\tiny{±0.68 } & 36.39\tiny{±0.34 } \\ \bottomrule
\end{tabular}}
\end{table}


\begin{table}[t!]
\centering
\caption{HuBert results on keyword Spotting and Wave2Vec2.0 results on FSDnoisy.}
\resizebox{0.5\textwidth}{!}{
\label{tab:hubert-ks-wav2vec-fsdnoisy}
\begin{tabular}{l|cc|c}
\toprule
Dataset & \multicolumn{2}{c|}{keyword Spotting}& \multicolumn{1}{c}{FSDnoisy} \\

\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-4}\,

\# Label  & 50                & 400                & 1,772                 \\
\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-4}\,
Supervised       & 8.95\tiny{±1.62}           & 6.31\tiny{±0.46}           & 33.54\tiny{±1.65 }          \\
Fully-Supervised &  \multicolumn{2}{c|}{2.41\tiny{±0.15 }}                        & \multicolumn{1}{c}{ -}   \\ 

\cmidrule(r){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-4}\,
-Model        & 87.86\tiny{±2.88}         & 72.89\tiny{±3.23}         & 35.97\tiny{±0.84}          \\
Pseudo-Labeling  & 25.59\tiny{±2.88}         & 13.02\tiny{±2.47}         & 35.23\tiny{±0.78}          \\
Mean Teacher     & 89.79\tiny{±0.30}         & 90.01\tiny{±0.02}         & 40.13\tiny{±1.70}          \\
VAT              & 2.27\tiny{±0.07}         & 2.43\tiny{±0.02}         & 34.21\tiny{±0.31}          \\
UDA              & 11.76\tiny{±0.06}         & 2.23\tiny{±0.16}         & 33.09\tiny{±1.03}          \\
FixMatch        & 11.63\tiny{±0.24}         & 8.93\tiny{±2.04}         & 33.09\tiny{±0.64}          \\
Dash           & 11.88\tiny{±0.15}         & 8.25\tiny{±4.22}         & 33.02\tiny{±1.39}          \\
CoMatch       & 15.96\tiny{±1.02}         & 10.34\tiny{±1.52}         & 30.24\tiny{±0.55}          \\
CRMatch       & 5.85\tiny{±1.19}         & 3.66\tiny{±0.33}         & 30.48\tiny{±0.65}          \\
FlexMatch        & 10.22\tiny{±1.10}         & 5.10\tiny{±3.70}         & 32.66\tiny{±4.09}          \\
SimMatch        & 9.43\tiny{±0.63}         & 5.47\tiny{±2.72}         & 29.57\tiny{±0.52}          \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Details of Datasets in USB}
\label{sec-details-usb}

\subsection{CV Tasks}
\label{sec-details-usb-cv}

\paragraph{CIFAR-100}

The CIFAR-100~\cite{krizhevsky2009learning} dataset is a natural image (3232 pixels) recognition dataset consisting 100 classes. There are 500 training samples and 100 test samples per class.

\paragraph{STL-10}
The STL-10~\cite{coates2011analysis} dataset is a natural color image (9696 pixels) recognition dataset consisting 10 classes. Particularly, each class has 500 training samples and 800 test samples. Apart from the labeled samples, STL-10 also provides 100,000 unlabeled samples. Note that the unlabeled samples contain other classes in addition to the ones in the labeled data.






\paragraph{EuroSat}
EuroSAT \cite{helber2019eurosat,helber2018introducing} dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27,000 labeled and geo-referenced samples.
Following \cite{zhai2019large}, we use the dataset with the optical R, G, B frequency bands, thus each image is of size .
We take the first 60\% images from each class as training set; the next 20\% as val set, and the last 20\% as test set.

\paragraph{TissueMNIST}
TissueMNIST \cite{medmnistv1,medmnistv2} is a medical dataset of human kidney cortex cells, segmented from 3 reference tissue specimens and organized into 8 categories.
The total 236,386 training samples are split with a ratio of 7 : 1 : 2 into training (165,466 images), validation (23,640 images) and test set (47,280 images).
Each gray-scale image is  pixels.

\paragraph{Semi-Aves}
Semi-Aves \cite{su2021semi} is a dataset of Aves (birds) classification, where 5,959 images of 200 bird species are labeled and 26,640 images are unlabeled. 
As class distribution mismatch hurts the performance \cite{saito2021openmatch}, we do not use out-of-class unlabeled data.
This dataset is challenging as it is naturally imbalanced.
The validation and test set contain 10 and 20 images respectively for each of the 200 categories in the labeled set.


\subsection{NLP Tasks}
\label{sec-details-usb-nlp}

\paragraph{IMDB}
The IMDB~\cite{maas2011learning} dataset is a binary sentiment classification dataset. There are 25,000 reviews for training and 25,000 for test. IMDB is class balanced which means the positive and negative reviews have the same number both for training and test. For USB, we draw 12,500 samples and 1,000 samples per class from training samples to form the training dataset and validation dataset respectively. The test dataset is unchanged.

\paragraph{Amazon Review}
The Amazon Review~\cite{mcauley2013hidden} dataset is a sentiment classification dataset. There are 5 classes (scores). Each class (score) contains 600,000 training samples and 130,000 test samples. For USB, we draw 50,000 samples and 5,000 samples per class from training samples to form the training dataset and validation dataset respectively. The test dataset is unchanged.

\paragraph{Yelp Review}
The Yelp Review~\cite{yelpwebsite} sentiment classification dataset has 5 classes (scores). Each class (score) contains 130,000 training samples and 10,000 test samples. For USB, we draw 50,000 samples and 5,000 samples per class from training samples to form the training dataset and validation dataset respectively. The test dataset is unchanged.



\paragraph{AG News}
The AG News~\cite{zhang2015character} dataset is a news topic classification dataset containing 4 classes. Each class contains 30,000 training samples and 1,900 test samples. For USB, we draw 25,000 samples and 2,500 samples per class from training samples to form the training dataset and validation dataset respectively. The test dataset is unchanged.






\paragraph{Yahoo! Answer}
The Yahoo! Answer~\cite{chang2008importance} topic classification dataset has 10 categories. Each class contains 140,000 training samples and 6,000 test samples. For USB, we draw 50,000 samples and 5,000 samples per class from training samples to form the training dataset and validation dataset respectively. The test dataset is unchanged.


\subsection{Audio Tasks}
\label{sec-details-usb-audio}

\paragraph{GTZAN}
The GTZAN dataset is collected for music genre classification of 10 classes and 100 audio recordings for each class. The maximum length of the recordings is 30 seconds and the original sampling rate is 22,100 Hz. We split 7,000 samples for training, 1,500 for validation, and 1,500 for testing. All recordings are re-sampled at 16,000 Hz.

\paragraph{UrbanSound8k}
The UrbanSound8k dataset \cite{salamon2014dataset} contains 8,732 labeled sound events of urban sounds of 10 classes, with the maximum length of 4 seconds. The original sampling rate of the audio recordings is 44,100 and we re-sample it to 16,000. It is originally divided into 10 folds, where we use the first 8 folds of 7,079 samples as training set, and the last two folds as validation set of size 816 and testing set of size 837 respectively.




\paragraph{FSDNoisy18k}
The FSDNoisy18 dataset \cite{fonseca2019learning} is a sound event classification dataset across 20 classes. It consists of a small amount of manually labeled data - 1,772 and a large amount of noisy data - 15,813 which is treated as unlabeled data in our paper. The original sample rate is 44,100 Hz, and the length of the recordings lies between 3 seconds and 30 seconds. We use the testing set provided for evaluation, which contains 947 samples.

\paragraph{Keyword Spotting (Superb-KS)} 
The Keyword spotting dataset is one of the tasks in Superb \cite{yang2021superb} for classifying the keywords. It contains speech utterances of a maximum length of 1 second and the sampling rate of 16,000. The training, validation, and testing set contain 18,538; 2,577; 2,567 recordings, respectively. For pre-processing, we remove the silence and unknown labels from the dataset. 

\paragraph{ESC-50} 
The ESC-50 \cite{piczak2015dataset} is a dataset containing 2,000 environmental audio recordings for 50 sound classes. The maximum length of the recordings is 5 seconds and the original sampling rate is 44,100. We split 1,200 samples as training data, 400 as validation data, and 400 as testing data. We also re-sample the audio recordings to 16,000 Hz during pre-processing.

\section{Details of Implemented SSL algorithms in USB}
\label{sec-detail-baseline}
\textbf{ model~\cite{rasmus2015semi}} is a simple SSL algorithm that forces the output probability of perturbed versions of unlabeled data be the same.  model uses Mean Squared Error (MSE) for optimization.

\textbf{Pseudo Labeling~\cite{lee2013pseudo}} turns the output probability of unlabeled data into the 'one-hot' hard one and makes the same unlabeled data to learn the pseudo 'one-hot' label. Unlike  model, Pseudo Labeling uses CE for optimization.

\textbf{Mean Teacher~\cite{tarvainen2017mean}} takes the exponential moving average (EMA) of the neural model as the teacher model. With Mean Teacher, the neural model forces itself to output a similar probability to the EMA teacher. Though the later SSL algorithms will not always choose the EMA model as the teacher, they often use the EMA model for validation/test cause it decreases the risk of neural models falling into the local optima.

\textbf{VAT~\cite{miyato2018virtual}} enhances the robustness of the conditional predicted label distribution around
each unlabeled data against an adversarial perturbation. In other words, VAT forces the neural model to give similar predictions on unlabeled data even facing a strong adversarial perturbation.
 
\textbf{MixMatch~\cite{berthelot2019mixmatch}} first introduces Mixup~\cite{zhang2018mixup} into SSL by taking the input as the mixture of labeled and unlabeled data and the output as the mixture of labels and model predictions on unlabeled data. Note that MixMatch also utilizes MSE as the unsupervised loss.

\textbf{ReMixMatch~\cite{berthelot2019remixmatch}} can be seen as the upgraded version of MixMatch. ReMixMatch improves MixMatch by (1) proposing stronger augmentation (i.e., Control Theory Augmentation (CTAugment)~\cite{berthelot2019remixmatch}) for unlabeled data; (2) using Augmentation Anchoring to force the model to output similar predictions to weakly augmented unlabeled data when fed strongly augmented data; (3) utilizing Distribution Alignment to encourage the marginal distribution of
predictions on unlabeled data to be similar to the marginal distribution of labeled data. 

\textbf{UDA~\cite{xie2020unsupervised}} also introduces strong augmentation (i.e., RandAugment~\cite{cubuk2020randaugment}) for unlabeled data. The core idea of UDA is similar to Augmentation Anchoring~\cite{berthelot2019remixmatch}, which forces the predictions of neural models on the strongly-augmented unlabeled data to be close to those of weakly-augmented unlabeled data. Instead of turning predictions into hard 'one-hot' pseudo-labels, UDA sharpens the prediction on unlabeled data. Thresholding technique is used to mask out unconfident unlabeled samples that are considered noise here. 


\textbf{FixMatch~\cite{sohn2020fixmatch}} is the upgraded version of Pseudo Labeling. FixMatch turns the predictions on weakly-augmented unlabeled data into hard 'one-hot' pseudo-labels and then further uses them as the learning signal of strongly-augmented unlabeled data. FixMatch finds that using a high threshold (e.g., 0.95) to filter noisy unlabeled predictions and take the rest as the pseudo-label can achieve very good performance.

\textbf{Dash~\cite{xu2021dash}} improves the FixMatch by using a gradually increased threshold instead of a fixed threshold, which allows more unlabeled data to participate in the training at the early stage. Moreover, Dash theoretically establishes the convergence rate from the view of non-convex optimization. 



\textbf{CoMatch~\cite{li2021comatch}} firstly introduces contrastive learning into SSL. Except for consistency regularizing on the class probabilities, it is also exploited on graph-based feature representations, which impose smooth constraints on pseudo-labels generated.

\textbf{CRMatch~\cite{fan2021revisiting}} proposed an improved consistency regularization framework which impose consistency
and equivariance on the classification probability and the feature level. 

\textbf{FlexMatch~\cite{zhang2021flexmatch}} firstly introduces the class-specific thresholds into SSL by considering the different learning difficulties of different classes. Specifically, the hard-to-learn classes should have a low threshold to speed up convergence while the easy-to-learn classes should have a high threshold to avoid confirmation bias.

\textbf{AdaMatch~\cite{berthelot2021adamatch}} is proposed mainly for domain adaption, but can also adapted to SSL. It is characterized by Relative Threshold and Distribution Alignment, where the relative threshold is adaptively estimated from EMA of the confidence on labeled data. 

\textbf{SimMatch~\cite{zheng2022simmatch}} extends CoMatch~\cite{li2021comatch} by considering semantic-level and instance-level consistency regularization. Similar similarity relationship of different augmented versions on the same data with respect to other instances is encouraged during training. In addition, a memory buffer consisting of predictions on labeled data is adopted to connect the two-level regularization.


\section{Experiment Details in USB}
\label{sec-setup}







\subsection{Setup for CV Tasks in USB}

\begin{table}[!htbp]
\centering
\caption{Hyper-parameters of CV tasks in USB.}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cccccc}\toprule
Dataset & CIFAR-100 & STL-10 & Euro-SAT & TissueMNIST & Semi-Aves \\\cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(l){6-6}
Image Size  & 32 & 96 & 32 & 32 & 224 \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Model    &  ViT-S-P4-32 & ViT-B-P16-96 & ViT-S-P4-32 & ViT-T-P4-32 & ViT-S-P16-224  \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Weight Decay&  \multicolumn{5}{c}{5e-4} \\ \cmidrule(r){1-1} \cmidrule(lr){2-6}
Labeled Batch size & \multicolumn{5}{c}{16} \\\cmidrule(r){1-1} \cmidrule(lr){2-6} 
Unlabeled Batch size & \multicolumn{5}{c}{16} \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Learning Rate & 5e-4 & 1e-4 & 5e-5  & 5e-5  & 1e-3 \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Layer Decay Rate & 0.5 & 0.95 &  1.0 &  0.95 & 0.65  \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Scheduler & \multicolumn{5}{c}{} \\ \cmidrule(r){1-1} \cmidrule(l){2-6}
Model EMA Momentum & \multicolumn{5}{c}{0.0}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Prediction EMA Momentum & \multicolumn{5}{c}{0.999}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Weak Augmentation & \multicolumn{5}{c}{Random Crop, Random Horizontal Flip} \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Strong Augmentation & \multicolumn{5}{c}{RandAugment \cite{cubuk2020randaugment}} \\
\bottomrule
\end{tabular}
}
\label{tab:hyper-cv}
\end{table}

For CV tasks in USB, we use ViT models \cite{dosovitskiy2020image}.
We find that directly using released ViT models leads to overfitting and one needs to fix the image resolution as the pre-trained resolution, as demonstrated in Paragraph~\ref{sec-ablation-pretrain}.
Instead, we pre-train our own ViT models on ImageNet-1K \cite{ILSVRC15}.
To match the number of parameters as the CNN models used in the classic setting, we use ViT-Tiny and ViT-Small with a patch size of  and image size of  for TissueMNIST, CIFAR-100 and EuraSAT, respectively; ViT-Small with a patch size of  and image size of  for Semi-Aves. For better transfer performance, we adopt an MLP before the final classifier during pre-training, as in \cite{wang2021revisiting}. For supervised pre-training on ImageNet-1K, we use Lamb optimizer with a learning rate of 0.05, and a weight decay of 0.03 for ViT-Tiny and a weight decay of 0.05 for ViT-Small. We adopt a large batch size of 4096 and train the networks for 300 epochs, with a linear learning rate warmup for the first 20 epochs. After the warmup, cosine scheduler is utilized. For augmentation, we use RandAugment \cite{cubuk2020randaugment}, along with Mixup \cite{zhang2018mixup} and CutMix \cite{yun2019cutmix}. We also use label smoothing of 0.1 during pre-training.  Since STL10 is a subset of ImageNet, we adopt unsupervised pre-training MAE \cite{he2021masked} of ViT-Small with image size of  to avoid cheating. 

For USB CV tasks, we adopt layer-wise learning rate decay as in \cite{liu2021Swin}. We tune the learning rate and layer decay rate on different datasets using FixMatch, and use the best configuration to train all SSL algorithms \footnote{We present the full tuning results in: \url{https://github.com/microsoft/Semi-supervised-learning}.}. The cosine annealing scheduler is similar to the classic setting but with total steps of  and a warm-up of  steps. The labeled and unlabeled batch size is both set to . Other algorithm-related hyper-parameters stay the same as in the original papers. 

\subsection{Setup for NLP Tasks in USB}
We use pre-trained BERT-Base \cite{bert} for all NLP tasks in USB.
We set the batch size of labeled data and unlabeled data to  for reducing the training time and GPU memory requirement.
To fine-tune the BERT-Base under USB, we adopt AdamW optimizer with weight decay of e. Similarly, we conduct a grid search of the learning rate and layer decay on different datasets using FixMatch and pick the best configuration to fine-tune other SSL algorithms.
We utilize the same cosine learning rate scheduler as in the classic setting with the total training steps of  and a warm-up of  steps.
We use the fine-tuned model without parameter momentum to conduct evaluations.
For all datasets, we cut the long sentence to satisfy the input length requirement of BERT-Base. For data augmentation, we adopt back-translation as the strong augmentation \cite{xie2020unsupervised, chen2020mixtext}. Specifically, we use De-En and Ru-En translation with WMT19.

\begin{table}[!htbp]
\centering
\caption{Hyper-parameters of NLP tasks in USB.}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{cccccc}\toprule
Dataset &  AG News & Yahoo! Answer & IMDb & Amazom-5 & Yelp-5 \\\cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(l){6-6}
Max Length    &  \multicolumn{5}{c}{512} \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Model    &  \multicolumn{5}{c}{Bert-Base} \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Weight Decay&  \multicolumn{5}{c}{1e-4} \\ \cmidrule(r){1-1} \cmidrule(lr){2-6}
Labeled Batch size & \multicolumn{5}{c}{4} \\\cmidrule(r){1-1} \cmidrule(lr){2-6} 
Unlabeled Batch size & \multicolumn{5}{c}{4} \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Learning Rate & 5e-5 & 1e-4 & 5e-5 & 1e-5 & 5e-5 \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Layer Decay Rate & 0.65 & 0.65 & 0.75 & 0.75 & 0.75 \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Scheduler & \multicolumn{5}{c}{} \\ \cmidrule(r){1-1} \cmidrule(l){2-6}
Model EMA Momentum & \multicolumn{5}{c}{0.0}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Prediction EMA Momentum & \multicolumn{5}{c}{0.999}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Weak Augmentation & \multicolumn{5}{c}{None} \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Strong Augmentation & \multicolumn{5}{c}{Back-Translation \cite{xie2020unsupervised}} \\
\bottomrule
\end{tabular}
}
\label{tab:hyper-nlp}
\end{table}


\subsection{Setup for Audio Tasks in USB}
For Audio tasks, we adopt Wav2Vec 2.0 \cite{baevski2020wav2vec} and HuBert \cite{hsu2021hubert} as the pre-trained model.
The batch size of labeled data and unlabeled data is set to .
We keep the sampling rate of audios as .
We adopt AdamW optimizer with a weight decay of e, and search the learning rate and layer decay as before.
Other hyper-parameter settings are the same as NLP tasks. Mimicking RandAugment, for strong augmentation in audio tasks, we random sample 2 augmentations from the augmentation pool and random set the augmentation magnitude during training.


\begin{table}[!htbp]
\centering
\caption{Hyper-parameters of Audio tasks in USB.}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{cccccc}\toprule
Dataset &  GTZAN & Keyword Spotting & UrbanSound8k & FSDNoisy & ESC-50 \\\cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(l){6-6}
Sampling Rate    &  \multicolumn{5}{c}{16,000} \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Max Length    &  3.0 & 1.0 & 4.0 & 5.0 & 5.0  \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Model    &  \multicolumn{2}{c}{Wav2Vecv2-Base} & \multicolumn{3}{c}{HuBERT-Base}  \\\cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-6}
Weight Decay&  \multicolumn{5}{c}{5e-4} \\ \cmidrule(r){1-1} \cmidrule(lr){2-6}
Labeled Batch size & \multicolumn{5}{c}{8} \\\cmidrule(r){1-1} \cmidrule(lr){2-6} 
Unlabeled Batch size & \multicolumn{5}{c}{8} \\\cmidrule(r){1-1} \cmidrule(lr){2-6}
Learning Rate & 2e-5 & 5e-5 & 5e-5 & 5e-4 & 1e-4 \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Layer Decay Rate & 1.0 & 0.75 & 0.75 & 0.75 & 0.85 \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Scheduler & \multicolumn{5}{c}{} \\ \cmidrule(r){1-1} \cmidrule(l){2-6}
Model EMA Momentum & \multicolumn{5}{c}{0.0}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Prediction EMA Momentum & \multicolumn{5}{c}{0.999}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Weak Augmentation & \multicolumn{5}{c}{Random Sub-sample} \\\cmidrule(r){1-1} \cmidrule(l){2-6}
Strong Augmentation & \multicolumn{5}{c}{Random Sub-sample, Random Gain, Random Pitch, Random Speed} \\
\bottomrule
\end{tabular}
}
\label{tab:hyper-audio}
\end{table}


































\end{document}
