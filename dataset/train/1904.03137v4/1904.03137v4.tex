\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr} 
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage[toc,page]{appendix}
\usepackage{floatrow}
\pdfoutput=1 

\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\newfloatcommand{capbtabbox}{figure}[][\FBwidth]

\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\usepackage{multirow, makecell}
\usepackage{rotating}
\settowidth\rotheadsize{Minimal}



\cvprfinalcopy 

\def\cvprPaperID{6280} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{Learning to Remember: \\ A Synaptic Plasticity Driven Framework for Continual Learning} 

\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[2]{\texttt{#1}}

\author{Oleksiy Ostapenko\affmark[,*], Mihai Puscas\affmark[,*], Tassilo Klein\affmark[*], Patrick J\"ahnichen\affmark[], Moin Nabi\affmark[*]\\
\affaddr{\small \affmark[]Humboldt-Universit\"at zu Berlin},
\affaddr{\affmark[]University of Trento},
\affaddr{\affmark[*]SAP ML Research}
}



\maketitle

\begin{abstract}                  
  Models trained in the context of continual learning (CL) should be able to learn from a stream of data over an indefinite period of time. The main challenges herein are: 1) maintaining old knowledge while simultaneously benefiting from it when learning new tasks, and 2) guaranteeing model scalability with a growing amount of data to learn from. In order to tackle these challenges, we introduce Dynamic Generative Memory (DGM) - synaptic plasticity driven framework for continual learning. DGM relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking. Specifically, we evaluate two variants of neural masking: applied to (i) layer activations and (ii) to connection weights directly. Furthermore, we propose a dynamic network expansion mechanism that ensures sufficient model capacity to accommodate for continually incoming tasks. The amount of added capacity is determined dynamically from the learned binary mask. We evaluate DGM in the continual class-incremental setup on visual classification tasks. \end{abstract}




\section{Introduction}
\subfile{sections/introduction}

\section{Related Work}
\label{sec:relWork}
\subfile{sections/related}


\section{Dynamic Generative Memory}
\label{sec:method}
\subfile{sections/method}
\section{Experimental Results}
\subfile{sections/experiment}

\section{Conclusion}
\subfile{sections/conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{egpaper_final}
}


\newpage

\end{document}