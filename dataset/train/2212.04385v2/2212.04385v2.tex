\appendix
\label{sec:appendix}
{\large
\noindent\textbf{Appendices}
}
\vspace{1mm}

Section~\ref{sec:sup_dataset} presents more details about evaluation datasets and metrics. 
Model variants and training objectives are described in Section~\ref{sec:sup_model}.
Details about experimental setups are provided in Section~\ref{sec:sup_exprs}, and more comparisons against state-of-the-art methods are shown in Section~\ref{sec:sup_sota}.
Finally, we present several visualizations of failure cases in Section~\ref{sec:sup_viz}.



\begin{table*}[h]
\centering
\small
\resizebox{0.98\textwidth}{!}{\begin{tabular}{cccccccccccc} \toprule
\multirow{2}{*}{Task Type} & \multirow{2}{*}{Granularity} & \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Train} & \multicolumn{2}{c}{Val Seen} & \multicolumn{2}{c}{Val Unseen} & \multicolumn{2}{c}{Test Unseen} \\
& & & \#house & \#instr & \#house & \#instr & \#house & \#instr & \#house & \#instr \\ \midrule
\multirow{3}{*}{Instruction-following} & \multirow{3}{*}{Fine-grained} 
  & R2R~\cite{anderson2018vision}   & 61 & 14,039 & 56 & 1,021 & 11 & 2,349  & 18 & 4,173 \\
& & R2R-CE~\cite{krantz2020beyond}  & 61 & 10,819 & 53 & 778   & 11 & 1,839  & 18 & 3,408 \\
& & RxR~\cite{ku2020room}           & 60 & 79,467 & 58 & 8,813 & 11 & 13,652 & 17 & 12,249 \\ 
\midrule
Goal-oriented & Coarse-grained & REVERIE~\cite{qi2020reverie} & 60 & 10,466 & 46 & 1,423 & 10 & 3,521 & 16 & 6,292 \\ \bottomrule
\end{tabular}}
\vspace{-2.5mm}
\caption{Dataset statistics. \#house, \#instr denote the number of houses and instructions respectively.}\label{tab:dataset_stats}
\end{table*}



\section{Evaluation Datasets and  Metrics}\label{sec:sup_dataset}
Our approach is evaluated on R2R~\cite{anderson2018vision}, R2R-CE~\cite{krantz2020beyond}, RxR~\cite{ku2020room} and REVERIE~\cite{qi2020reverie} datasets, which are built upon the Matterport3D~\cite{chang2017matterport3d} indoor scene dataset. 
We summarize the dataset statistics in Tab.~\ref{tab:dataset_stats}. The four datasets differ in task type (instruction-following \textit{v.s.} goal-oriented) and instructions granularity (fine-grained \textit{v.s.} coarse-grained). 


\vspace{1mm}
\noindent\textbf{Room-to-Room (R2R)}~\cite{anderson2018vision} provides fine-grained (step-by-step) instructions. 
The agent is required to follow an instruction to reach the target location.
It receives panoramic observations at each viewpoint, and navigation is simplified as transporting among viewpoints on the predefined graph of an environment. 
The dataset contains 61 houses for training, 56 houses for validation in seen environments, 11 and 18 houses for validation and testing in unseen environments, respectively. 
Each instruction in R2R describes a full path, such as ``\textit{Head straight until you pass the wall with holes in it the turn left and wait by the glass table with the white chairs.}''.


\vspace{1mm}
\noindent\textbf{Room-to-Room in Continuous Environments (R2R-CE)}~\cite{krantz2020beyond} extends R2R in continuous environments.
It discards the predefined graph assumption, instead, requiring the agent to navigate freely with low-level actions (\eg, \texttt{FORWARD} 0.25m, \texttt{ROTATE} 15\degree) on 3D meshes~\cite{savva2019habitat}. 
R2R-CE contains a subset of R2R's instruction-path pairs because some paths are unusable on the 3D meshes. 


\vspace{1mm}
\noindent\textbf{Room-across-Room (RxR)}~\cite{ku2020room} is a more challenging instruction-following dataset, which emphasizes the role of language guidance and provides large-scale multilingual instructions (en-IN, en-US, hi-IN, te-IN). 
Instructions in RxR involve more descriptions of visual entities and relations, such as ``\textit{You are facing towards a wall, turn around and move forward. You are now standing in front of a glass cabin and on your right side you have a table with a computer. You can see a table with a black chair right in front of you. Walk between these two tables and move forward. Now you can see a table with orange chair on your right side. Move forward towards the centre table which is right in front of a couch and that is your end point.}''.
Besides, annotated paths in RxR are much longer than R2R ( times) and the ground truth paths are not the shortest path between the starting and ending points.



\vspace{1mm}
\noindent\textbf{REVERIE}~\cite{qi2020reverie} is a goal-oriented dataset which provides coarse-grained instructions. 
Instructions in REVERIE are concise and mainly describe target rooms and target objects, such as ``\textit{Go to the office and clean the black and white picture of a child}''. 
The task focuses more on knowledge and commonsense exploitation rather than instruction following. 
Furthermore, the agent must identify the target object from a set of candidates after reaching the desired location.
The dataset has 4,140 target objects in 489 categories, and each target viewpoint has 7 objects on average. 



\vspace{1mm}
\noindent\textbf{Evaluation Metrics}. VLN tasks mainly focus on the agent's generalization ability in unseen environments (val unseen and test unseen splits).
The main evaluation metrics of the above datasets are slightly different.
On the R2R/R2R-CE dataset, \texttt{SR} and \texttt{SPL} are the main metrics to evaluate the navigation accuracy and efficiency, where a predicted path is regarded as \textit{success} if the agent stops within 3 meters of the target location. 
RxR dataset does not have shortest-path prior, and it additionally takes \texttt{NDTW} and \texttt{SDTW} to measure the fidelity between predicted and annotated paths. The two metrics reflect how well the agent interprets and follows instructions. 
On the REVERIE dataset, annotated paths have shortest-path prior and the main navigation metrics are \texttt{SR} and \texttt{SPL}. A predicted path is considered as \textit{success} if the target object is visible within 3 meters at the endpoint. 
It additionally uses \texttt{RGS} and \texttt{RGSPL} to evaluate the object grounding capacity of the agent, an grounding is \textit{success} if the predicted and annotated objects are the same.



\section{Model Details}\label{sec:sup_model}
\begin{figure*}[h]
\centering
\includegraphics[width=0.88\textwidth]{fig/arch_reverie.pdf}
\vspace{-3mm}
\caption{
Adapt the proposed pre-training model to the REVERIE task. 
Additional object features  are fed into the metric map encoder to obtain multimodal object representations . 
We use MRC and OG tasks to learn cross-modal object reasoning and grounding.
}\label{fig:arch_reverie}
\vspace{-3mm}
\end{figure*}


\subsection{Adaptation to the REVERIE Dataset}
REVERIE provides candidate object annotations at each step and requires the agent to point out the target object when it stops.
As shown in Fig.~\ref{fig:arch_reverie}, we feed these candidates into the short-term branch (metric map encoder) to enable object grounding. 
Specifically, at each step, we first use the same ViT as in Section~\ref{sec:map} to extract object features .
After adding position embeddings (sine and cosine of orientations~\cite{hong2021vln,chen2022think,lin2021scene}), these object features are concatenated with view feature vectors  and fed into the pano encoder in Section~\ref{sec:map} to obtain contextual object embeddings. 
Then, cell and object embeddings are concatenated as the visual modality while encoded instructions as the linguistic modality.
We feed them into the short-term transformer to perform cross-modal reasoning as explained in Section~\ref{sec:metric_encoder}.
The output multimodal object representations  are learned via MRC and OG tasks, which will be detailed in the next section.



\subsection{Pre-training Objectives}\label{sec:pretrain_reverie}
\noindent\textbf{R2R/R2R-CE and RxR.} We sample pre-training tasks for each mini-batch to train the BEVBert model. The sampling ratio for R2R/R2R-CE and RxR datasets is .
We randomly chunk a sampled expert trajectory  from head to obtain  for offline map construction. 



\vspace{1mm}
\noindent\textbf{REVERIE.} Instructions in REVERIE mainly describe the rooms and target objects at endpoints. 
We do not employ MSI task due to the lack of intermediate path descriptions; instead, Masked Region Classification (MRC)~\cite{lu2019vilbert} and Object Grounding (OG)~\cite{lin2021scene} are used for final object reasoning and grounding. 
MRC aims to predict semantic labels of masked objects by reasoning over the surrounding objects and the instructions. We randomly mask objects with a 15\% probability and feed them into the metric map encoder.
The semantic labels of objects are class probability predicted by a ViT pre-trained on ImageNet~\cite{deng2009imagenet}. The task is optimized by minimizing the KL divergence between the predicted and target probability distribution. 
OG is a downstream-specific task. After obtaining the multimodal object representation , a two-layer feed-forward network is employed to predict the target object scores. Given the target object label  at step  (the endpoint), the task is optimized by minimizing the negative log-likelihood:
{\small

}We sample pre-training tasks for each mini-batch to train the model on REVERIE dataset, and the sampling ratio is .

\subsection{Layer Variants in Fine-tuning}
During fine-tuning, the computational graph gradually expands as the trajectory rolls out and may cause GPU out of memory in extreme cases.  
To alleviate the problem, for each transformer layer of the two map encoders, we cut off self-attention and cross-attention of the text branch, \ie, the language-to-vision cross-attention and the language-to-language self-attention. 
All transformer layers share the same text representations. We empirically found it does not severely hamper navigation performance, and the same phenomenon is also observed in RecBert~\cite{hong2021vln} and HAMT~\cite{chen2021history}.

\begin{figure*}[h]
\vspace{-3mm}
\centering
\includegraphics[width=0.90\textwidth]{fig/depth.pdf}
\vspace{-3mm}
\caption{
Qualitative visualization of the sensing and estimated depths. The top row represents depths of the original scale, and the bottom row presents downsized depths that have the same scale with grid features.
}\label{fig:depth}
\vspace{-3mm}
\end{figure*}


\subsection{Fine-tuning Objectives}
During fine-tuning, we alternatively run `teacher-forcing'  and `student-forcing' . The model is optimized by the mixed loss , formally:
{\small

}

In , the agent executes ground-truth actions  to follow the expert trajectory , and the loss is the accumulation of negative log-likelihoods of these expert actions.
In , the agent generates trajectory  by on-policy action sampling and supervised by pseudo labels .
Specifically, the action is sampled via the predicted action probability distribution at each step, and the pseudo labels are determined via goal-oriented or fidelity-oriented heuristics. 
A goal-oriented label is a ghost node that has the shortest path length to the final target location, while a fidelity-oriented label is a ghost node through which the sampled path has the highest fidelity (\texttt{NDTW}) with the expert path.
We use goal-oriented labels for R2R/R2R-CE and REVERIE, while fidelity-oriented labels for RxR because it does not have shortest-path prior.
Besides, OG loss is added for fine-tuning on REVERIE:
{\small

}

The `teacher-forcing' encourages the agent to follow the expert path, while the `student-forcing' encourages the agent to explore the environment. 
We set the balanced term  on R2R/R2R-CE and REVERIE datasets, while  on the RxR dataset. Because annotated paths in RxR are much longer, a small  can lead to unnecessary exploration and hamper navigation fidelity.


\section{Experimental Setups}\label{sec:sup_exprs}




\subsection{Depth Estimation}
This section details the depth ablation study in Section~\ref{sec:ablation}. 
We employ RedNet~\cite{jiang2018rednet} for depth estimation, which takes RGB images as inputs and uses a U-Net-like architecture for depth regression.
The estimated depths are outputs of a sigmoid layer, and downsized depth images also supervise intermediate layers to speed up convergence. 
We train the model in train-split houses of Matterport3D dataset~\cite{chang2017matterport3d}.
Then, a trained model is used to estimate depths for all viewpoints of all houses, and we retrain BEVBert with these pseudo depths.
We visualize some sensing and estimated depths in Fig.~\ref{fig:depth}.
Intuitively, estimated depths are of low quality and have noise, but their quality is similar to sensing depths' after downsampling.
This can also explain why BEVBert does not rely on very accurate depth images.



\subsection{Spatial and Numerical Tokens}
Tab.~\ref{tab:token} summarizes the token templates we used to extract spatial and numerical related instructions in Section~\ref{sec:analysis}.
Extracted instructions are grouped via the number of special tokens in each instruction.
To ensure reliable performance estimation, we omit those groups which contain less than 40 instructions.

\begin{table}[h]
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{l c }
\cmidrule[\heavyrulewidth]{1-2}
\textbf{Token Type} & \textbf{Token Templates} \\ \cmidrule[\heavyrulewidth]{1-2}
Spatial     & \makecell[l]{
on the left, on your left, to the left, to your left \\
left of, left side of, leftmost, on the right, \\
on your right, to the right, to your right, right of, \\
right side of, rightmost, near, nearest, behind, \\
between, next to, end of, edge of, front of, \\
middle of, top of, bottom of
}
\\ \cmidrule{1-2}
Numerical     &  \makecell[l]{
first, second, third, fourth, fifth, sixth, seventh, \\
eighth, one, two, three, four, five, six, seven, \\
eight, 1, 2, 3, 4, 5, 6, 7, 8
}
\\ \cmidrule{1-2}
\end{tabular}
\end{adjustbox}
\vspace{-2.5mm}
\caption{Templates of spatial and numerical tokens.}
\label{tab:token}
\vspace{-2.5mm}
\end{table}



\definecolor{Gray}{gray}{0.9}
\begin{table*}[h]
\centering
\resizebox{0.92\textwidth}{!}{\begin{tabular}{lccccc | ccccc | ccccc}
\toprule 
\multicolumn{1}{c}{} & \multicolumn{5}{c}{Val Seen} & \multicolumn{5}{c}{Val Unseen} & \multicolumn{5}{c}{Test Unseen} \\
\cmidrule(r){2-6} \cmidrule(r){7-11} \cmidrule(r){12-16}
\multicolumn{1}{c}{Methods} & 
\multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE} & \multicolumn{1}{c}{OSR} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{SPL}} & 
\multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE} & \multicolumn{1}{c}{OSR} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{SPL}} & 
\multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE} & \multicolumn{1}{c}{OSR} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{SPL}} \\
\midrule
Human                   
& - & - & - & - & - 
& - & - & - & - & - 
& 11.85 & 1.61 & 90 & 86 & 76 \\
\midrule
Seq2Seq~\cite{anderson2018vision}
& 11.33 & 6.01 & 53 & 39 & - 
& 8.39 & 7.81 & 28 & 21 & - 
& 8.13 & 7.85 & 27 & 20 & - \\
SF~\cite{fried2018speaker}
& - & 3.36 & 74 & 66 & - 
& - & 6.62 & 45 & 36 & - 
& 14.82 & 6.62 & - & 35 & 28  \\
Chasing~\cite{anderson2019chasing}
& 10.15 & 7.59 & 42 & 34 & 30
& 9.64 & 7.20 & 44 & 35 & 31
& 10.03 & 7.83 & 42 & 33 & 30 \\
RCM~\cite{wang2019reinforced}
& 10.65 & 3.53 & 75 & 67 & - 
& 11.46 & 6.09 & 50 & 43 & - 
& 11.97 & 6.12 & 50 & 43 & 38 \\
SM~\cite{ma2019self}
& - & 3.22 & 78 & 67 & 58
& - & 5.52 & 56 & 45 & 32 
& 18.04 & 5.67 & 59 & 48 & 35 \\
EnvDrop~\cite{tan2019learning}
& 11.00 & 3.99 & - & 62 & 59
& 10.70 & 5.22 & - & 52 & 48
& 11.66 & 5.23 & 59 & 51 & 47 \\
OAAM~\cite{qi2020object}
& - & - & 73 & 65 & 62
& - & - & 61 & 54 & 50
& - & - & 61 & 53 & 50 \\
AuxRN~\cite{zhu2020vision}
& - & 3.33 & 78 & 70 & 67
& - & 5.28 & 62 & 55 & 50
& - & 5.15 & 62 & 55 & 51 \\
Active~\cite{wang2020active}
& - & 3.20 & 80 & 70 & 52
& - & 4.36 & 70 & 58 & 40
& - & 4.33 & 71 & 60 & 41 \\
NvEM~\cite{an2021neighbor}
& 11.09 & 3.44 & - & 69 & 65
& 11.83 & 4.27 & - & 60 & 55
& 12.98 & 4.37 & 66 & 58 & 54 \\
SEvol~\cite{chen2022reinforced}
& 11.97 & 3.56 & - & 67 & 63
& 12.26 & 3.99 & - & 62 & 57
& 13.40 & 4.13 & - & 62 & 57 \\
SSM~\cite{wang2021structured}
& 14.70 & 3.10 & 80 & 71 & 62
& 20.70 & 4.32 & 73 & 62 & 45
& 20.40 & 4.57 & 70 & 61 & 46 \\
PREVAL~\cite{hao2020towards}\dag
& 10.32 & 3.67 & - & 69 & 65
& 10.19 & 4.71 & - & 58 & 53
& 10.51 & 5.30 & 61 & 54 & 51 \\
AirBert~\cite{guhur2021airbert}\dag
& 11.09 & 2.68 & - & 75 & 70
& 11.78 & 4.10 & - & 62 & 56 
& 12.41 & 4.13 & - & 62 & 57 \\
RecBert~\cite{hong2021vln}\dag
& 11.13 & 2.90 & - & 72 & 68
& 12.01 & 3.93 & - & 63 & 57
& 12.35 & 4.09 & 70 & 63 & 57 \\
REM~\cite{liu2021vision}\dag
& 10.88 & 2.48 & - & 75 & 72
& 12.44 & 3.89 & - & 64 & 58
& 13.11 & 3.87 & 72 & 65 & 59 \\
HAMT~\cite{chen2021history}\dag
& 11.15 & 2.51 & - & 76 & 72
& 11.46 & 3.65 & - & 66 & 61
& 12.27 & 3.93 & 72 & 65 & 60 \\
EnvEdit*~\cite{li2022envedit}\dag
& 11.18 & 2.32 & - & 77 & \color{blue}74
& 11.13 & 3.24 & - & 69 & \color{blue}64 
& 11.90 & 3.59 & - & 68 & \color{blue}64 \\
TD-STP~\cite{zhao2022target}\dag
& - & 2.34 & 83 & 77 & 73 
& - & 3.22 & 76 & 70 & 63 
& - & 3.73 & 72 & 67 & 61 \\
DUET~\cite{chen2022think}\dag
& 12.32 & 2.28 & 86 & 79 & 73 
& 13.94 & 3.31 & 81 & 72 & 60
& 14.73 & 3.65 & 76 & 69 & 59 \\
BEVBert (Ours)\dag
& 13.56 & \color{blue}2.17 & \color{blue}88 & \color{blue}81 & \color{blue}74
& 14.55 & \color{blue}2.81 & \color{blue}84 & \color{blue}75 & \color{blue}64
& 15.87 & \color{blue}3.13 & \color{blue}81 & \color{blue}73 & \color{blue}62 \\
\bottomrule
\end{tabular}}
\vspace{-2.5mm}
\caption{Comparison with state-of-the-art methods on R2R dataset. *Ensemble of three agents. \dag~denotes pre-training-based methods.}\label{tab:r2r_full}
\vspace{-2.5mm}
\end{table*}


\definecolor{Gray}{gray}{0.9}
\begin{table*}[h]
\centering
\resizebox{0.88\textwidth}{!}{\begin{tabular}{lcccc | cccc | cccc}
\toprule 
\multicolumn{1}{c}{} & \multicolumn{4}{c}{Val Seen} & \multicolumn{4}{c}{Val Unseen} & \multicolumn{4}{c}{Test Unseen} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(r){10-13}
\multicolumn{1}{c}{Methods} & 
\multicolumn{1}{c}{NE} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{NDTW}} & \multicolumn{1}{c}{\textbf{SDTW}} &
\multicolumn{1}{c}{NE} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{NDTW}} & \multicolumn{1}{c}{\textbf{SDTW}} &
\multicolumn{1}{c}{NE} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{NDTW}} & \multicolumn{1}{c}{\textbf{SDTW}} \\
\midrule
Human                   
& - & - & - & - 
& - & - & - & - 
& 0.9 & 93.9 & 79.5 & 76.9  \\
\midrule
LSTM~\cite{ku2020room}
& 10.7 & 25.2 & 42.2 & 20.7 
& 10.9 & 22.8 & 38.9 & 18.2
& 12.0 & 21.0 & 36.8 & 16.9 \\
EnvDrop+~\cite{shen2021much}
& - & - & - & -
& - & 42.6 & 55.7 & - 
& - & 38.3 & 51.1 & 32.4 \\
CLEAR-C~\cite{li2022clear}
& - & - & - & -
& - & - & - & -
& - & 40.3 & 53.7 & 34.9 \\
HAMT~\cite{chen2021history}
& - & 59.4 & 65.3 & 50.9
& - & 56.5 & 63.1 & 48.3
& 6.2 & 53.1 & 59.9 & 45.2 \\
EnvEdit*~\cite{li2022envedit}
& - & 67.2 & 71.1 & 58.5
& - & 62.8 & 68.5 & 54.6 
& 5.1 & 60.4 & 64.6 & 51.8 \\
BEVBert (Ours)
& 3.8 & 68.9 & 70.0 & 58.4
& 4.6 & 64.1 & 63.9 & 52.6
& - & - & - & - \\
BEVBert (Ours)
& \color{blue}3.2 & \color{blue}75.0 & \color{blue}76.3 & \color{blue}66.7
& \color{blue}4.0 & \color{blue}68.5 & \color{blue}69.6 & \color{blue}58.6
& \color{blue}4.8 & \color{blue}64.4 & \color{blue}65.4 & \color{blue}54.2 \\
\bottomrule
\multicolumn{12}{l}{\small{*Results from an ensemble of three agents.  Results without Marky synthetic instructions~\cite{wang2022less}.}}
\end{tabular}}
\vspace{-2.5mm}
\caption{Comparison with state-of-the-art methods on RxR dataset.}\label{tab:rxr_full}
\vspace{-2.5mm}
\end{table*}





\definecolor{Gray}{gray}{0.9}
\begin{table*}[h]
\centering
\resizebox{0.98\textwidth}{!}{\begin{tabular}{l ccccc | ccccc | ccccc }
\toprule
\multirow{3}{*}{Methods} & \multicolumn{5}{c}{Val seen} & \multicolumn{5}{c}{Val Unseen} & \multicolumn{5}{c}{Test Unseen} \\
\cmidrule(r){2-6} \cmidrule(r){7-11} \cmidrule(r){12-16} 
& \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding} & \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding} & \multicolumn{3}{c}{Navigation} & \multicolumn{2}{c}{Grounding} \\
& \multicolumn{1}{c}{OSR} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{SPL}} & \multicolumn{1}{c}{\textbf{RGS}} & \multicolumn{1}{c}{\textbf{RGSPL}} 
& \multicolumn{1}{c}{OSR} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{SPL}} & \multicolumn{1}{c}{\textbf{RGS}} & \multicolumn{1}{c}{\textbf{RGSPL}} 
& \multicolumn{1}{c}{OSR} & \multicolumn{1}{c}{\textbf{SR}} & \multicolumn{1}{c}{\textbf{SPL}} & \multicolumn{1}{c}{\textbf{RGS}} & \multicolumn{1}{c}{\textbf{RGSPL}} \\ 
\midrule
Human
& - & - & - & - & -
& - & - & - & - & -
& 81.51 & 53.66 & 86.83 & 77.84 & 51.44 \\
\midrule
Seq2Seq~\cite{anderson2018vision} 
& 35.70 & 29.59 & 24.01 & 18.97 & 14.96
& 8.07 & 4.20 & 2.84 & 2.16 & 1.63 
& 6.88 & 3.99 & 3.09 & 2.00 & 1.58 \\
RCM~\cite{wang2019reinforced} 
& 29.44 & 23.33 & 21.82 & 16.23 & 15.36
& 14.23 & 9.29 & 6.97 & 4.89 & 3.89 
& 11.68 & 7.84 & 6.67 & 3.67 & 3.14 \\
SMNA~\cite{ma2019self} 
& 43.29 & 41.25 & 39.61 & 30.07 & 28.98
& 11.28 & 8.15 & 6.44 & 4.54 & 3.61 
& 8.39 & 5.80 & 4.53 & 3.10 & 2.39 \\
FAST-MATTN~\cite{qi2020reverie} 
& 55.17 & 50.53 & 45.50 & 31.97 & 29.66
& 28.20 & 14.40 & 7.19 & 7.84 & 4.67 
& 30.63 & 19.88 & 11.6 & 11.28 & 6.08 \\
SIA~\cite{lin2021scene }
& 65.85 & 61.91 & 57.08 & 45.96 & 42.65
& 44.67 & 31.53 & 16.28 & 22.41 & 11.56 
& 44.56 & 30.80 & 14.85 & 19.02 & 9.20 \\
RecBERT~\cite{hong2021vln} 
& 53.90 & 51.79 & 47.96 & 38.23 & 35.61
& 35.20 & 30.67 & 24.90 & 18.77 & 15.27 
& 32.91 & 29.61 & 23.99 & 16.50 & 13.51 \\
AirBert~\cite{guhur2021airbert}
& 48.98 & 47.01 & 42.34 & 32.75 & 30.01
& 34.51 & 27.89 & 21.88 & 18.23 & 14.18 
& 34.20 & 30.26 & 23.61 & 16.83 & 13.28 \\
HAMT~\cite{chen2021history} 
& 47.65 & 43.29 & 40.19 & 27.20 & 25.18
& 36.84 & 32.95 & 30.20 & 18.92 & 17.28 
& 33.41 & 30.40 & 26.67 & 14.88 & 13.08 \\ 
TD-STP~\cite{zhao2022target} 
& - & - & - & - & -
& 39.48 & 34.88 & 27.32 & 21.16 & 16.56 
& 40.26 & 35.89 & 27.51 & 19.88 & 15.40 \\ 
DUET~\cite{chen2022think} 
& 73.86 & 71.75 & 63.94 & 57.41 & 51.14
& 51.07 & 46.98 & 33.73 & 32.15 & 23.03 
& 56.91 & 52.51 & 36.06 & 31.88 & 22.06 \\
BEVBert (Ours) 
& \color{blue}76.18 & \color{blue}73.72 & \color{blue}65.32 & \color{blue}57.70 & \color{blue}51.73
& \color{blue}56.40 & \color{blue}51.78 & \color{blue}36.37 & \color{blue}34.71 & \color{blue}24.44 
& \color{blue}57.26 & \color{blue}52.81 & \color{blue}36.41 & \color{blue}32.06 & \color{blue}22.09 \\
\bottomrule
\end{tabular}}
\vspace{-2.5mm}
\caption{Comparison with state-of-the-art methods on REVERIE dataset.}\label{tab:reverie_full}
\end{table*}
\section{More Comparisons with State-of-the-Art}\label{sec:sup_sota}
In Tab.~\ref{tab:r2r_full}, Tab.~\ref{tab:rxr_full} and Tab.~\ref{tab:reverie_full}, we present more comparisons with state-of-the-art methods on R2R, RxR and REVERIE, respectively. The main metrics of each dataset are highlighted. 
BEVBert also achieves state-of-the-art performance in seen splits on all metrics, but the performance is still far behind humans'.
For example, on the test unseen split of RxR dataset, humans can achieve 93.9 \texttt{SR} and 76.9 \texttt{SDTW}, while BEVBert has 64.4 \texttt{SR} and 54.2 \texttt{SDTW}.





\section{More Qualitative Examples}\label{sec:sup_viz}
We visualize some failure cases in Fig.~\ref{fig:early} and Fig.~\ref{fig:ambiguity}. We conclude the failure reasons as `early lost' and `ambiguity'.

Fig.~\ref{fig:early} presents four `early lost' cases. 
The agent loses the state tracking of navigation due to early mistakes, leading to too much backtracking in cases (a,b,c). 
However, it does not go back to the right path till the end. 
In case (d), the agent does not ``turn left'' after ``into the hallway''. This does not trigger backtracking, but the agent directly ``wait by the kitchen counter'' at the wrong location after seeing a counter.

Fig.~\ref{fig:ambiguity} shows four `ambiguity' cases. 
Some ambiguous instructions may confuse the agent, such as ``enter another bedroom straight ahead'' in case (a) and ``enter the second room on the left'' in case (b). 
In case (c), the agent ``walk to the end of the entrance way'' in the opposite direction. After reaching the hallway end, it has lost state tracking and cannot backtrack. 
In case (d), the agent does not know whether it has finished ``down the hallway'' or not, then makes an early ``turn right'' and stops in advance.
\begin{figure*}[h]
\centering
\includegraphics[width=0.81\textwidth]{fig/early.pdf}
\vspace{-3mm}
\caption{
Failure cases of `early lost' in val unseen splits of R2R. Yellow and green circles denote the start and target locations, respectively, and the red circles represent incorrect endpoints. 
}\label{fig:early}
\end{figure*}
\begin{figure*}[h]
\centering
\includegraphics[width=0.81\textwidth]{fig/ambiguity.pdf}
\vspace{-3mm}
\caption{
Failure cases of `ambiguity' in val unseen splits of R2R. Yellow and green circles denote the start and target locations, respectively, and the red circles represent incorrect endpoints. 
}\label{fig:ambiguity}
\end{figure*}
