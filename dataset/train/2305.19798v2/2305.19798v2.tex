\PassOptionsToPackage{numbers}{natbib}
\documentclass{article}
\usepackage[final]{neurips_2023}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[dvipsnames]{xcolor}

\usepackage{wrapfig,lipsum,booktabs}
\usepackage{graphicx} \usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb, bm}
\usepackage{amsbsy}
\usepackage{arydshln}
\usepackage{mathtools}
\newcommand\qh[1]{\textcolor{orange}{#1}}
\newcommand\fra[1]{\textcolor{blue}{#1}}
\newcommand\cyy[1]{\textcolor{RubineRed}{#1}}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,
	urlcolor=magenta,
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}




\title{Primal-Attention: Self-attention through  
	Asymmetric Kernel SVD in Primal Representation} 


\author{
	Yingyi Chen
	\thanks{Equal contribution.}\,
	\\
	ESAT-STADIUS \\
	KU Leuven, Belgium \\
	\texttt{yingyi.chen@esat.kuleuven.be} \\
	\And
	Qinghua Tao 
	\footnotemark[1]\,
	\\
	ESAT-STADIUS \\
	KU Leuven, Belgium \\
	\texttt{qinghua.tao@esat.kuleuven.be} \\
	\And
	Francesco Tonin \\
	ESAT-STADIUS \\
	KU Leuven, Belgium \\
	\texttt{francesco.tonin@esat.kuleuven.be} \\
	\And
	Johan A.K.~Suykens \\
	ESAT-STADIUS \\
	KU Leuven, Belgium \\
	\texttt{johan.suykens@esat.kuleuven.be} \\
}


\begin{document}
	
	
	\maketitle
	
	
	\begin{abstract}
		Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. 
		However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. 
		In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers.
		Through asymmetric KSVD, 
		\emph{i)} a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs;
		\emph{ii)} a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; 
		\emph{iii)}
		with KKT conditions, we prove that the stationary solution to the KSVD optimization in Primal-Attention yields a {zero-value} objective. In this manner, KSVD optimization can be implemented by simply minimizing a regularization loss, so that low-rank property is promoted without extra decomposition.
		Numerical experiments show state-of-the-art performance of our Primal-Attention with improved efficiency. 
		Moreover, we demonstrate that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-attention, further verifying the great potential of our method.
		To the best of our knowledge, this is the first work that provides a \emph{primal-dual representation} for the \emph{asymmetric kernel} in self-attention and successfully applies it to \emph{modelling} and \emph{optimization}\footnote{Our implementation is available at~\url{https://github.com/yingyichen-cyy/PrimalAttention}}.
		
		
		
		
		
	\end{abstract}
	
	
	\section{Introduction}
	\label{sec::intro}
	
	
	Transformers~\cite{vaswani2017attention} have become ubiquitous nowadays with state-of-the-art results in various tasks, such as natural language processing~\cite{kenton2019bert,brown2020language,raffel2020exploring}, computer vision~\cite{fan2021multiscale,liu2021swin,touvron2021training,chen2023jigsaw}, reinforcement learning~\cite{janner2021offline,chen2021decision,wu2022flowformer}, etc. 
	In the remarkable success of Transformers, the self-attention blocks play a key role, where the complicated dependencies between {the individuals} in data sequences can be depicted by using the established queries, keys, and values. 
	Despite the prevailing advantages, 
	theoretical understandings towards Transformers seem yet lagged behind its unprecedented empirical performance. 
	
	Recently, the kernel-based perspective has been proposed 
	{where the dot-product attention operation is shown} as a kernel matrix~\cite{tsai2019}. 
	This finding is quite encouraging by bridging kernels~\cite{vapnik1999overview} with Transformers, as kernel methods have long been well studied with good interpretation ability. 
	Following this spirit, different works have been proposed subsequently to improve self-attention, e.g.,~\cite{choromanski2021rethinking,ali2021xcit,nguyen2022improving,nguyen2022fourierformer,chi2022kerple,nguyen2023a}. 
	However, in these works, the applied kernel techniques rely on Mercer kernels~\cite{mercer1909}  requesting symmetry, which is inconsistent with the 
	{intrinsically asymmetric} setups of self-attention. 
	In~\cite{wright2021transformers}, it analytically characterizes the attention by asymmetric kernels based on Reproducing Kernel Banach Spaces (RKBS)~\cite{zhang2009reproducing}.
	{Nonetheless,} neither the asymmetry property nor the related optimization is utilized for improvements. 
	In~\cite{nguyen2023a}, self-attention is derived with a primal-dual representation from the support vector regression, which still adopts the technique for Mercer kernels.
	Moreover, in the cast supervised task, the assumed ground-truth  outputs of self-attention are practically non-existent, making it difficult to be applied in the optimization.
	
	\begin{figure}[t]
		\centering    
		{\includegraphics[width=\textwidth]{./images/heatmap.jpg}}
		\caption{
			Spectrum analysis
			of the self-attention matrix on ImageNet-1K~\cite{deng2009imagenet}.
			(a)-(c) plot the cumulative explained variance regarding the singular values of the attention matrix with mean and standard deviation of the chosen layers in pre-trained DeiT-Small/16~\cite{touvron2021training} and Primal.DeiT-Small/16 (ours): the attention matrix attains sharper singular value decays in deeper layers, also shown in (d).
			{Note that we also plot the cumulative explained variance curves of the self-attention matrix from the last layer, i.e.,~the 11-th layer denoted by ``L[11]'', of both models in (c).}
			Our method shows an enhanced low-rank property of the attention matrix upon the baseline.
		}
		\label{fig:overview:low-rank}
	\end{figure}
	
	
	In this work, we provide a novel perspective to interpret self-attention with a primal-dual representation based on asymmetric Kernel Singular Value Decomposition (KSVD), which fills
	the gap of dismissing the asymmetry between theory and implementation. 
	{Specifically, in this unsupervised setup, we propose to remodel self-attention in the primal representation, namely, Primal-Attention, and to optimize it accordingly.}
	{Our method is driven by two major motivations.}
	{Firstly, 
		we observe that attention matrices in Transformers can be
		low-rank, as shown in Figure~\ref{fig:overview:low-rank}(d), and this property becomes more significant towards deeper network layers.}
	{Secondly, the self-attention matrix is intrinsically an asymmetric kernel matrix~\cite{tsai2019,wright2021transformers}.}
	To this end, we propose KSVD for self-attention, which takes both low-rank and asymmetric properties into consideration. 
	To the best of our knowledge, this is the first work that provides a primal-dual representation for the asymmetric self-attention and applies it to modelling and optimization.
	The  contributions of this work are summarized as follows:
	\begin{itemize}
		\item We characterize self-attention by KSVD with asymmetric kernels. Different from existing works employing symmetric kernel-based methods, we take asymmetry into account so as to be more consistent with the real setups in self-attention. (Section~\ref{sec::problem_statesment})
		
		\item  We derive a primal-dual representation for self-attention {through KSVD}, and {propose} a novel attention in the primal, named Primal-Attention, avoiding the expensive kernel computation in the dual.
		With KSVD, the values are interpreted as the projection weights that yield maximal variances of features, and  the low-rank property can be 
		pursued by confining the projection numbers. 
		(Section~\ref{sec::primal_dual}, Section~\ref{sec::method})
		
		\item  We prove that the stationary solution to the derived KSVD leads to a zero-value objective 
		of the unconstrained primal problem.
		Therefore, the optimization of KSVD in Primal-Attention can be efficiently implemented by minimizing a regularization term added to the loss, with no need of extra decomposition operations. 
		(Section~\ref{sec::method})
		
		
		\item In numerical experiments, Primal-Attention achieves state-of-the-art performance on various datasets together with efficiency advantages over the canonical self-attention. 
		Moreover, we demonstrate that our deployed optimization from KSVD can regularize the attention with a sharper singular value decay, hence promoting learning more low-rank features, which is shown in Figure~\ref{fig:overview:low-rank}. (Section~\ref{sec::exp})
		
		
	\end{itemize}
	
	
	
	
	\section{Problem Statement: Self-attention with Asymmetric Kernel}
	\label{sec::problem_statesment}
	
	
	\paragraph{Self-attention}
	Let  be the input data sequence. 
	In self-attention~\cite{vaswani2017attention},
	the queries, keys and values output the linear projections of the input sequence, such that
	
	where , , and , commonly with the setup .
	The attention scores are then given by 
	.
	In the canonical self-attention, the ``softmax'' activation is then applied to bring non-linearity and positives, yielding the attention weights:
	
	Similar to~\cite{tsai2019}, the attention matrix, i.e., , can be interpreted as a kernel matrix with entries , where  serves as the kernel function. 
	Notice that in general, , leading to an asymmetric kernel where .
	
	
	Then, the attention output  in each head is attained as:
	
	In Transformers, multiple heads are commonly applied through the concatenation of all heads~\cite{vaswani2017attention}.
	
	
	
	
	\paragraph{Asymmetric Attention Matrix} 
	In kernel methods, rigorous works have been presented with Mercer kernels that are symmetric and positive semi-definite~\cite{mercer1909} through the kernel trick from Reproducing Kernel Hilbert Spaces (RKHS)~\cite{vapnik1999overview}. 
	On the other hand in Transformer~\cite{vaswani2017attention}, the attention kernel matrix is asymmetric as shown in \eqref{eq::attnweight}.  
	Existing works leverage the kernel interpretation for improving self-attention~\cite{choromanski2021rethinking,nguyen2022fourierformer,chi2022kerple,nguyen2023a}, however, their deployed kernel-based techniques all rely on Mercer kernels, which is inconsistent with the asymmetric {nature}.  
	Instead, asymmetry is allowed in kernel tricks from Reproducing Kernel Banach Spaces (RKBS)~\cite{zhang2009reproducing} as in the following Definition~\ref{def:asym:kernel}. 
	
	
	\begin{definition}
		[Definition 2~\cite{wright2021transformers}; Theorem 2.1~\cite{lin2022reproducing};\cite{georgiev2013construction}]
		\label{def:asym:kernel}
		For asymmetric kernels, the kernel trick from RKBS with the kernel function  can be defined by the inner product of two real measurable feature maps from Banach spaces  on , respectively:
		
	\end{definition}
	Based on Definition~\ref{def:asym:kernel}, the kernel matrix in self-attention can be characterized by the kernel trick from RKBS~\cite{wright2021transformers}, providing an analytical tool from the aspect of kernel representer theorem.
	
	\paragraph{SVD and Shifted Eigenvalue Problem}
	SVD factorizes a given -rank matrix  by two sets of orthonormal eigenbases:   with   of positive singular values and the columns of  and  as the \emph{left} and \emph{right singular vectors}, respectively~\cite{strang2006linear}. 
	,  reflect the subspace projections in relation to the columns and rows, as shown in  \eqref{shifted:eigen}, and contain different information residing in  due to the asymmetry. 
	When  is squared and symmetric, SVD boils down to the eigendecomposition with .  
	In~\cite{suykens2016svd}, a novel variational principle is proposed for SVD with Least Squares Support Vector Machines (LSSVM)~\cite{suykens2002least}, where the dual problem leads to a shifted eigenvalue problem in accordance to the decomposition theorem from Lanczos~\cite{lanczos1958linear} regarding SVD, i.e., Theorem \ref{theorem:lanczos}. This theorem is also of special importance in our work to the kernel extension of SVD  in self-attention under the framework of LSSVM.
	
	\begin{theorem}[Lanczos~\cite{lanczos1958linear}]\label{theorem:lanczos}
		Any non-zero matrix {} can be written as {}, where the matrices , ,  are defined by the shifted eigenvalue problem:
		
		where   and   satisfy  and , and  is a diagonal matrix with positive numbers.
	\end{theorem}
	
	
	
	
	
	
	
	
	\section{Primal-dual Representation of Self-attention based on Kernel SVD}
	\label{sec::primal_dual}
	In this section, we apply the kernel trick from RKBS to the asymmetric attention kernel, and derive self-attention with a primal-dual representation based on 
	Kernel SVD (KSVD). 
	Under this learning scheme, a new self-attention mechanism is proposed by remodelling the attention output in the primal representation, without explicit computation of the kernel matrix in the dual representation. 
	With the stationarity conditions, we flexibly implement the optimization of KSVD through an additional loss term, which can regularize the model to improved low-rank properties without extra decomposition.
	
	\paragraph{{KSVD optimization problem} in Primal and Dual}
	By Definition \ref{def:asym:kernel} of RKBS, the kernel function in the dual for the asymmetric attention kernel  in self-attention can be formulated by 
	,
	with two feature maps ,  related to queries and keys. 
	Recall the self-attention output in \eqref{eq::dual_output}, the values { resemble the dual variables} projecting the kernel matrix in the dual representation of kernel methods, whereas the kernel involved is asymmetric.
	In this regard, the nonlinear version of SVD under the framework of LSSVM~\cite{suykens2016svd} well fits the self-attention setup, hence is set as the basis for the following work with asymmetric kernels built upon RKBS. 
	{Differently, we extend the matrix SVD setups in~\cite{suykens2016svd} to the case of asymmetric attention matrix with two input data sources as queries and keys.} 
	Moreover,  we consider that in self-attention, values are input data-dependent, and thus generalize~\cite{suykens2016svd} with data-dependent projection weights.
	
	Given the sequence  , we start from the  primal optimization with KSVD:
	
	where we have the data-dependent projection weights ,  relying on parameters , 
	the feature maps
	, 
	the projection scores ,
	and the regularization coefficient  which is a positive diagonal matrix. The objective  in the primal optimization 
	maximizes the projection variances of ,  regarding queries and keys, and also involves a regularization term coupling the projections.
	The corresponding solution in the dual is characterized
	by the right and left singular vectors, which captures the directions with maximal projection variances, w.r.t.~rows (queries) and columns (keys) of the attention kernel matrix.
	Thus, with the formulated primal optimization in \eqref{eq:ksvd:lssvm:std:attention},  the learning in self-attention is interpreted  as a SVD problem on the attention matrix.
	
	For clarity, we elaborate our primal optimization problem in \eqref{eq:ksvd:lssvm:std:attention} as follows.
	\textit{i)} The projection weights are data-dependent, where 
	{} denotes a transformation matrix containing the information of the sequence data .
	Notably,  is a constant matrix once given , and we let it linearly depend on  in experiments.
	Moreover, when  is chosen as the identity matrix, it reconciles to the common setups in kernel methods in primal.
	\textit{ii)} The feature maps related to queries and keys, respectively, are  defined as , , where  and  denote the mappings composited on the linear projections  and  in \eqref{eq:q:k:v} of queries and keys.
	Note that we leave the choice of  and  later explained in Remark~\ref{rkm::feat_maps}.
	\textit{iii)} By projecting  with  weights , we obtain the projection scores  along the  directions, {usually }, which corresponds to the number of singular values of the induced kernel matrix in the dual optimization \eqref{shifted:eigen:ksvd:std:attention}.
	
	
	
	
	
	
	\begin{remark} [Variance maximization objective]\label{rmk:obj}
		In the formulated KSVD problem, the objective in the primal optimization \eqref{eq:ksvd:lssvm:std:attention} jointly maximizes the variances of the two 
		{projections ,  in the feature spaces determined by ,  along the directions , .}
	Within this context,  ,  
	learn to capture maximal information mutually residing in  and  regarding the queries and keys. 
\end{remark}

With Lagrangian duality and KKT conditions, we prove that the dual optimization problem to \eqref{eq:ksvd:lssvm:std:attention} leads to a shifted eigenvalue problem corresponding to the SVD on the asymmetric attention kernel , given by Theorem \ref{theorem:ksvd:dual}. 
The proof is provided in the Supplementary Material.



\begin{theorem} [Dual optimization problem of KSVD in self-attention]\label{theorem:ksvd:dual}
	With Lagrangian duality and the KKT conditions, the dual optimization problem of \eqref{eq:ksvd:lssvm:std:attention} leads to the shifted eigenvalue problem:
	
	where  is a positive diagonal matrix, and  , 
	 are the dual variables serving as the left and right singular vectors, respectively.  
	{The kernel trick to the asymmetric kernel matrix  is interpreted as .}
\end{theorem}



As shown in Theorem \ref{theorem:ksvd:dual}, the solutions collect non-zero  in \eqref{eq:ksvd:lssvm:std:attention} such that . 
Based on Lanczos' decomposition in Theorem \ref{theorem:lanczos}, we can then associate  with the non-zero singular values of the attention kernel  in \eqref{shifted:eigen:ksvd:std:attention}, and   with the left and right singular vectors of , such that . 
Therefore, formulas \eqref{eq:ksvd:lssvm:std:attention} and \eqref{shifted:eigen:ksvd:std:attention} provide the optimization problems of performing KSVD with the attention kernel matrix  in the primal and in the dual, respectively.




\paragraph{Self-attention as KSVD dual representation}
Firstly, we provide the primal-dual model representation of the derived KSVD  problem.
Secondly, we show that the dual representation of the model corresponds to the canonical self-attention.
The derivation details involving the KKT conditions are provided in the Supplementary Material.


\begin{remark}[Primal-dual representations of KSVD in self-attention] \label{rmk:primal:dual}
	In the KSVD formulations for the asymmetric kernel matrix in self-attention, with KKT conditions, the projection scores can be either represented in the primal using explicit feature maps or in the dual using  kernel functions:
	
\end{remark}

\begin{remark} [Correspondence of  KSVD and  canonical self-attention output] \label{rmk:e:score:self:attention}
	Recall the output  of canonical self-attention \eqref{eq::dual_output}, 
	it corresponds to the dual representation of the projection score  in  \eqref{eq:primal:dual:ksvd:std:attention}, i.e., .
	When the values {} in canonical self-attention are chosen as the dual variables {}, i.e., {, }, the values {} play the role of the right singular vectors of .
\end{remark}


From the perspective in Remark \ref{rmk:e:score:self:attention}, the  
{optimization goal} in self-attention is interpreted to jointly capture the maximal variances of ,  as in \eqref{eq:ksvd:lssvm:std:attention}, {where the projection scores can be denoted as ,  through the representations  \eqref{eq:primal:dual:ksvd:std:attention}.} 
{However,} the canonical self-attention only outputs the -score (). 
In this sense, the output of the canonical self-attention only considers the {projection scores} involving the right singular vectors of the asymmetric attention kernel .



\begin{figure}[t]
	\centering    
	{\includegraphics[width=0.98\textwidth]{./images/pipeline.jpg}}
	\caption{An illustration of Primal-Attention and canonical self-attention.
		Left: in \emph{canonical self-attention}, the asymmetric attention matrix  can be induced by two feature maps ,  through kernel trick from RKBS. 
		The values  serve as the dual variables projecting the kernel matrix  to the attention output. 
		The time and space complexity are { and }. 
		Right: in our \emph{Primal-Attention}, we use the two feature maps ,  in the primal to present the attention outputs, which are projected through the primal variables , . 
		The time and space complexity are  and .
		To align with the output size in the canonical self-attention, we add a linear map mapping from  to  with negligible memory increase after Primal-Attention's output.}
	\label{fig::pipeline}
	\vspace{-3mm}
\end{figure}

\section{Primal-Attention}
\label{sec::method}
\paragraph{Modelling}
It is quite remarkable that the attention output can be equivalently represented without the kernel expressions, avoiding the heavy computation of the kernel matrix. 
{Within} the context of KSVD, we further observe that there exists another set of projections  regarding the left singular vectors in  as  in \eqref{eq:primal:dual:ksvd:std:attention}, providing extra information residing in the asymmetric kernel matrix .

We derive a novel attention mechanism by leveraging the primal representation of KSVD, namely, Primal-Attention, where two explicit feature maps  are adopted. 
To fully exploit the asymmetry in the kernel matrix of self-attention, Primal-Attention concatenates the two sets of projections using both left and right singular vectors, and thus formulates the attention outputs {as follows:}

In Primal-Attention, the projection weights  in the primal play the role as the counterparts of the values in the dual. 
{Given  an identity matrix, our KSVD problem in \eqref{eq:ksvd:lssvm:std:attention} boils down to the data-independent projection weight case as in \cite{suykens2016svd}, which can thereby be regarded as a special case of our derived KSVD in Primal-Attention.
	In this case, the kernel trick in the asymmetric attention kernel becomes , .}



\begin{remark}[Choices of   for non-linearity]\label{rkm::feat_maps}
	The canonical self-attention {adopts softmax for introducing non-linearity to the  attention score matrix}; {within our setups and kernel trick,} it can be viewed as:  
	where 
	,  with
	,
	 and 
	~\cite{choromanski2021rethinking},
	and .
	The projection scores then correspond to 
	 and
	.
	In this case, two exponential feature maps need to be constructed and the normalization factor  to all samples needs to be computed.
	In this paper, we consider feature maps related to the Cosine similarity kernel on queries and keys, such that
	 and 
	 in all experiments. 
	It is easy to implement and able to bring both non-linearity and normalization to the feature maps.
\end{remark}



\paragraph{Optimization}
The KSVD problem can either be optimized in the primal as a constrained optimization problem, or in the dual as a shifted eigenvalue problem (SVD on the kernel matrix ).  
In Primal-Attention, we perform the optimization in the primal.
In the following Lemma \ref{prop:obj:0}, we prove the {zero-value} property of the primal optimization objective  in  \eqref{eq:ksvd:lssvm:std:attention} when it is evaluated at the stationary solution in \eqref{shifted:eigen:ksvd:std:attention}. 
The proof is provided in the Supplementary Material.

\begin{lemma}[A {zero-value} objective with stationary solutions]\label{prop:obj:0}
	The solutions of  to the shifted eigenvalue problem in the dual optimization \eqref{shifted:eigen:ksvd:std:attention} lead to the  {zero-value} objective  in the primal optimization \eqref{eq:ksvd:lssvm:std:attention}.
\end{lemma}


With the property in Lemma \ref{prop:obj:0}, rather than solving SVD problem on the kernel matrix  in the dual, the optimization of Primal-Attention is realized by minimizing the primal objective to zero:

where  is the task-oriented loss, e.g., the cross-entropy loss for classification tasks, the summation term over  denotes the additive objectives  of all attention blocks using the proposed Primal-Attention, {where  is implemented as the mean over all heads,} {and {} is the regularization coefficient.}
Specifically, for each head in the self-attention using Primal-Attention, we have

where  is automatically determined by the optimization.
KSVD optimization of Primal-Attention is easy to implement by adding a regularization loss. 
Hence, Primal-Attention not only represents self-attention with KSVD formulation in the primal, but utilizes the optimization of KSVD by regularizing the attention, satisfying the stationarity conditions of KSVD when reaching a {zero-value} objective.




		
		
		
		





\section{Numerical Experiments} 
\label{sec::exp}
In this section, we verify the effectiveness of our Primal-Attention applied in Transformers on {five well-established benchmarks: time series, long sequence modelling, reinforcement learning, image classification and language modelling.}
Notably, we consider two types of Transformers applied with our Primal-Attention, i.e., PrimalFormer (Primal.) and Primal..
\textit{i)} In PrimalFormer, Primal-Attention~\eqref{eq:outputs:Primal-Attention} is applied to all attention layers, regularized with the KSVD loss \eqref{eq:obj:std}. 
This setup is preferred when data shows relatively strong low-rank structure or the model redundancy is quite substantial.
\textit{ii)} Primal. refers to the baselines from the Transformer family with the last layer replaced by our Primal-Attention, which is in favour in large-scale data and complicated tasks where less information compression is desired in the learning, especially in shallower layers. 
To specify the Transformer backbone, we denote our method by ``Primal.Backbone'' herein.
Primal. serves as a flexible variant combined with 
different Transformer backbones,
and with KSVD optimization applied through an implicit low-rank regularization loss, advocating
to learn more informative features,
as in Figure~\ref{fig:overview:low-rank}.
The two main hyper-parameters of our method are the 
coefficient  in \eqref{eq::objective_function} and the number of projection directions  of KSVD in \eqref{eq:ksvd:lssvm:std:attention}.
In data-dependent 
cases
with  , we 
take a subset of  by uniformly sampling  points from  for efficiency aspects, {as the main patterns of a matrix can be retained with random linear projections shown by the Johnsonâ€“Lindenstrauss lemma~\cite{lindenstrauss1984extensions}.} 
Details are given in the Supplementary Material. 





\paragraph{UEA Time Series Classification} 
\label{subsec::timeseries}
UEA Time Series Classification Archive~\cite{bagnall2018uea} is the benchmark for the evaluation on temporal sequences.
Following~\cite{wu2022flowformer}, we select 10 multivariate datasets with pre-processed data according to~\cite{zerveas2021transformer}, and employ 2-layer Transformer as backbone with the hidden dimension 512 on 8 heads and the embedding dimension 64 for self-attention.
{The hyper-parameter search of our method is with  , .} 
Experiments are run on one NVIDIA Tesla P100 SXM2 16GB GPU.




\begin{table}[t!]
	\caption{Test accuracy (\%) on  the UEA time series classification archive benchmark~\cite{bagnall2018uea} with comparisons to canonical Transformer (Trans.), Linear Transformer (Linear.), Reformer (Re.), Longformer (Long.), Performer (Per.), cosFormer (Cos.), Flowformer (Flow.), YOSO-E and SOFT.}
	\label{tab::timeseries}
	\begin{center}
		\resizebox{\textwidth}{!}{
			\begin{tabular}{cccccccccccc}
				\toprule
				\multirow{2}{*}{Dataset / Model} 
				& Trans. & Linear. & Re. & Long. & Per. & YOSO-E & Cos. & SOFT & Flow. & \multicolumn{2}{c}{\bf Ours} 
				\\ \cmidrule(lr){11-12}
				& \cite{vaswani2017attention} & \cite{katharopoulos2020transformers} & \cite{kitaev2020reformer} & \cite{beltagy2020longformer} & \cite{choromanski2021rethinking} & \cite{zeng2021you} & \cite{qin2022cosformer} & \cite{lu2021soft} & \cite{wu2022flowformer}
				& Primal. & Primal.Trans. 
				\\ \midrule
				EthanolConcentration           
				& 32.7 & 31.9 & 31.9 & 32.3 & 31.2 & 31.2 & 32.3 & 33.5 & 33.8 
				& 33.1 & 35.4
				\\
				FaceDetection                  
				& 67.3 & 67.0 & 68.6 & 62.6 & 67.0 & 67.3 & 64.8 & 67.1 & 67.6      
				& 67.1 & 63.8
				\\
				HandWriting                    
				& 32.0 & 34.7 & 27.4 & 39.6 & 32.1 & 30.9 & 28.9 & 34.7 & 33.8       
				& 29.6 & 28.7            
				\\
				HeartBeat                      
				& 76.1 & 76.6 & 77.1 & 78.0 & 75.6 & 76.5 & 77.1 & 75.6 & 77.6
				& 76.1& 77.1             
				\\
				JapaneseVowels                 
				& 98.7 & 99.2 & 97.8 & 98.9 & 98.1 & 98.6 & 98.3 & 99.2 & 98.9
				& 98.4 & 98.9             
				\\
				PEMS-SF                        
				& 82.1 & 82.1 & 82.7 & 83.8 & 80.9 & 85.2 & 83.2 & 80.9 & 83.8   
				& 89.6 & 90.2              
				\\
				SelfRegulationSCP1             
				& 92.2 & 92.5 & 90.4 & 90.1 & 91.5 & 91.1 & 91.1 & 91.8 & 92.5
				& 92.5 & 92.5              
				\\
				SelfRegulationSCP2             
				& 53.9 & 56.7 & 56.7 & 55.6 & 56.7 & 53.9 & 55.0 & 55.6 & 56.1
				& 57.2 & 56.1              
				\\
				SpokenArabicDigits             
				& 98.4 & 98.0 & 97.0 & 94.4 & 98.4 & 98.9 & 98.4 & 98.8 & 98.8
				& 100 & 100       
				\\
				UWaveGestureLibrary            
				& 85.6 & 85.0 & 85.6 & 87.5 & 85.3 & 88.4 & 85.6 & 85.0 & 86.6
				& 86.3& 88.4              
				\\ \midrule
				Average Accuracy               
				& 71.9 & 72.4 & 71.5 & 72.0 & 71.9 & 72.2 & 71.5 & 72.2 & 73.0
				& 73.0 & \bf{73.1}               
				\\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-5mm}
\end{table}

\begin{table}[t!]
	\caption{Efficiency comparisons on running time and memory consumption on the UEA benchmark~\cite{bagnall2018uea} where running time (s/Epoch), the peak training memory usage (GB) are given. We report how much faster and how less memory each model uses than Transformer.}
	\label{tab::timeseries_efficiency}
	\begin{center}
		\resizebox{\textwidth}{!}{
				\begin{tabular}{ccccccccccccc}
					\toprule
					\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Dataset /\\ seq. length\end{tabular}} & Ethanol. & Face. & Hand. & Heart. & Jap. & PEMS-SF & SCP1 & SCP2 & Arabic. & UWave & {\bf Avg.~Time} & {\bf Memory} 
					\\ 
					& 1751 & 62 & 152 & 405 & 26 & 144 & 896 & 1152 & 83 & 315 & {\bf (s/Epoch)} & {\bf (GB)}   
					\\ \midrule
					Trans.~\cite{vaswani2017attention}   
					& 4.3 (1) & 10.7 (1) & 0.3 (1) & 0.7 (1) & 0.5 (1) & 0.6 (1) & 1.8 (1) & 1.8 (1) & 3.7 (1) & 0.3 (1) & 2.5 (1) & 10.9 (1)
					\\
					Flow.~\cite{wu2022flowformer} 
					& 2.4 (1.8) & 9.8 (1.1) & 0.3 (1.0) & 0.7 (1.0) & 0.6 (0.8) & 0.7 (0.9) & {\bf 1.4 (1.3)} & {\bf 1.3 (1.4)} & 4.4 (0.8) & 0.3  (1.0) & 2.2 (1.1) & 2.8 (3.9)        
					\\ \cmidrule(lr){2-11} \cmidrule(lr){12-13}
					{\bf Primal.Trans.}                                        
					& 3.3 (1.3) & {\bf 5.7 (1.9)} & 0.3 (1.0) & 0.7 (1.0) & 0.5 (1.0) & 0.7 (0.9) & 1.6 (1.1) & 1.6 (1.1) & 4.0  (0.9) & 0.3  (1.0) & {\bf 1.9 (1.3)} & 6.5 (1.7)       
					\\
					{\bf Primal.} 
					& {\bf 2.3 (1.9)} & 6.9 (1.6) & 0.4 (0.8) & {\bf 0.4 (1.8)} & 0.5 (1.0) & 0.8 (0.8) & {\bf 1.4 (1.3)} & {\bf 1.3 (1.4)} & 4.5 (0.8) & 0.4 (0.8) & {\bf 1.9 (1.3)} & {\bf 2.7 (4.0)}  
					\\ \bottomrule
			\end{tabular}}
	\end{center}
	\vspace{-5mm}
\end{table}
Table~\ref{tab::timeseries} reports the test accuracy of the compared recent methods, where the best result is in bold. 
Both our PrimalFormer and Primal. achieve comparable and better performance than the state-of-the-art results provided by Flowformer~\cite{wu2022flowformer}.
Notably, Primal. yields the best accuracy with  overall improvement upon the canonical Transformer~\cite{vaswani2017attention}, i.e., by replacing the softmax-based self-attention with our Primal-Attention in the last layer.
This shows the promising potential of Primal-Attention in enhancing temporal modelling capacity upon the canonical softmax self-attention. 
It is worth mentioning that our PrimalFormer applying the KSVD optimization with low-rank regularization to all layers also obtains good performance. 
This could be due to the fact that these datasets are relatively simple where model redundancy can exist, so that appropriately imposing the low-rank regularization in KSVD does not harm the model expressiveness for the data. 
{We also compare the running time and memory consumption with the canonical Transformer and Flowformer, which is proposed recently with state-of-the-art performances and efficiency. 
	Both our Primal.~and Primal.Trans.~consistently lead to improved efficiency than the canonical Transformer. 
	In Primal., all attention layers are implemented with the proposed Primal-Attention, while Primal.Trans.~applies the Primal-Attention in the last layer, making Primal.~a more efficient model. 
	We note that our Primal.~further surpasses Flowformer in most cases regarding both running time and memory.}

\paragraph{Long-Range Arena Benchmark} 
\label{subsec::lra}
Long-Range Arena (LRA)~\cite{tay2020long} is a benchmark for the long-sequence scenarios, including equation calculation (ListOps)~\cite{nangia2018listops}, review classification (Text)~\cite{maas2011learning}, document retrieval (Retrieval)~\cite{radev2013acl}, image classification (Image)~\cite{krizhevsky2009learning} and image spatial dependencies (Pathfinder)~\cite{linsley2018learning}.
We follow the settings in~\cite{xiong2021nystromformer} with PyTorch.
The Transformer backbone is set with 2 layers, hidden dimension 128 with 2 heads and embedding dimension 64 with mean pooling, where Reformer (Re.) uses 2 hashes, Performer (Per.) has 256 random feature dimensions and Linformer (Lin.) uses a projection dimension of 256.
{Our hyper-parameter is set from , .}
Experiments are conducted on a single NVIDIA Tesla V100 SXM2 32GB GPU.

From the reported top-1 test accuracy in Table~\ref{tab::lra}, our PrimalFormer shows better accuracy while achieves top efficiency (see Table~\ref{tab::efficiency}) than several efficient self-attention counterparts including Reformer, Performer and Linformer.
Notably, our model Primal.Trans.~achieves the state-of-the-art accuracy of , which is  higher than Transformer, and  higher than the currently best YOSO-E, showing that the deployed Primal-Attention is able to boost the performance upon canonical Transformer distinctively. 
On top of it, Primal.Trans.~has distinctively higher efficiency and require much less memory than the canonical Transformer as in Table~\ref{tab::efficiency}, {and our Primal.~outperforms all compared variants of efficient Transformer.} 
Compared to Table \ref{tab::timeseries} with simpler data, Primal.Trans.~further achieves performance gain over PrimalFormer. 
The reason can be that the shallower (the first) layer needs more model capacity for depicting the data patterns in the learning, while the deep (the second) layer captures less detailed features and is regularized with more informative feature learning through our Primal-Attention.  
{The efficiency of our Primal-Attention are further pronounced with longer sequences than that of the UEA benchmark in Table \ref{tab::timeseries_efficiency}.}

\begin{table}[t]
	\caption{Test accuracy (\%) on the LRA benchmark~\cite{tay2020long} with comparisons to 
		canonical Transformer (Trans.), Reformer (Re.), Performer (Per.),
		Linformer (Lin.), Nystr\"{o}mformer (Nystr\"{o}m.), Longformer (Long.) and YOSO-E. 
	}
	\label{tab::lra}
	\begin{center}
		\resizebox{0.85\textwidth}{!}{
			\begin{tabular}{cccccccccc}
				\toprule
				\multirow{2}{*}{Dataset (seq.~length)} 
				& Trans. & Re. & Per. & Lin. & Nystr\"{o}m. & Long. & YOSO-E  &\multicolumn{2}{c}{\bf Ours} 
				\\ \cmidrule(lr){9-10}
				& \cite{vaswani2017attention} & \cite{kitaev2020reformer} & \cite{choromanski2021rethinking} & \cite{wang2020linformer} & \cite{xiong2021nystromformer} & \cite{beltagy2020longformer} & \cite{zeng2021you} & Primal. & Primal.Trans.
				\\ \midrule
				ListOps (2K) 
				& 37.1 & 19.1 & 18.8 & 37.3 & 37.2 & 37.2 & 37.3 & 37.3 & 37.3                
				\\
				Text (4K)                            
				& 65.0 & 64.9 & 63.8 & 55.9 & 65.5 & 64.6 & 64.7 & 61.2 & 65.4                
				\\
				Retrieval (4K)                       
				& 79.4 & 78.6 & 78.6 & 79.4 & 79.6 & 81.0 & 81.2 & 77.8 & 81.0                
				\\
				Image (1K)  
				& 38.2 & 43.3 & 37.1 & 37.8 & 41.6 & 39.1 & 39.8 & 43.0 & 43.9               
				\\
				Pathfinder (1K)                     
				& 74.2 & 69.4 & 69.9 & 67.6 & 70.9 & 73.0 & 72.9 & 68.3 & 74.3                
				\\ \midrule
				Average Accuracy                 
				& 58.8 & 55.1 & 53.6 & 55.6 & 59.0 & 59.0 & 59.2 & 57.5 & \bf{60.4}           
				\\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-5mm}
\end{table}


\begin{table}[t]
	\caption{Efficiency comparisons on running time  and memory consumption on  LRA~\cite{tay2020long}.}
	\label{tab::efficiency}
	\begin{center}
		\resizebox{\textwidth}{!}{
			\begin{tabular}{ccccccccccc}
				\toprule
				\multirow{2}{*}{Model} & \multicolumn{5}{c}{Time (s/1K-steps)} & \multicolumn{5}{c}{Memory (GB)}     
				\\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
				& ListOps & Text & Retrieval & Image & Pathfinder & ListOps & Text & Retrieval & Image & Pathfinder 
				\\ \midrule
				Trans.~\cite{vaswani2017attention} 
				& 194.5 (1)& 694.8 (1)& 1333.7 (1)& 334.5 (1)& 405.5 (1) 
				& 5.50 (1)& 21.24 (1)& 18.72 (1)& 5.88 (1)& 5.88 (1)
				\\
				Nystr\"{o}m.~\cite{xiong2021nystromformer} 
				& 68.4 (2.8)& 120.9 (5.7)& 235.5 (5.7)& 179.5 (1.9)& 221.2 (1.8)
				& 0.89 (6.2)& 1.69 (12.6)& 3.29 (5.7)& 1.93 (3.0)& 1.93 (3.0)           
				\\
				Lin.~\cite{wang2020linformer} 
				& 63.4 (3.1)& 116.5 (6.0)& 226.2 (5.9)& 158.5 (2.1)& 204.0 (2.0)
				& 1.73 (3.2)& 3.45 (6.2)& 6.33 (3.0)& 3.45 (1.7)& 3.45 (1.7)         
				\\
				Per.~\cite{choromanski2021rethinking} 
				& 83.8 (2.3)& 157.5 (4.4)& 320.6 (4.2)& 211.4 (1.6)& 278.1 (1.5)
				& 1.67 (3.3)& 3.34 (6.4)& 6.28 (3.0)& 3.34 (1.8)& 3.34 (1.8)           
				\\
				Re.~\cite{kitaev2020reformer} 
				& 87.0 (2.2)& 168.5 (4.1)& 339.9 (3.9)& 223.7 (1.5)& 286.7 (1.4)
				& 1.64 (3.3)& 3.29 (6.5)& 6.09 (3.1)& 3.29 (1.8)& 3.29 (1.8)    
				\\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
				\bf{Primal.Trans.} 
				& 113.4 (1.7) & 367.6 (1.9) & 546.5 (2.4) & 212.1 (1.6) & 263.2 (1.5)
				& 5.24 (1.1) & 20.7 (1.0) & 18.59 (1.0)& 5.35 (1.1) & 5.35 (1.1)
				\\
				\bf{Primal.} 
				& \bf{56.5 (3.4)}& \bf{93.6 (7.4)}& \bf{185.3 (7.2)}& \bf{142.9 (2.3)}& \bf{180.0 (2.3)}
				& \bf{0.69 (7.9)}& \bf{1.37 (15.5)}& \bf{2.99 (6.3)}& \bf{1.39 (4.2)}& \bf{1.52 (3.9)}   
				\\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-5mm}
\end{table}



\paragraph{Reinforcement Learning}
\label{subsec::reinforcement}
We consider the offline reinforcement learning (RL) performance of our methods on D4RL benchmark~\cite{fu2020d4rl} designed for continuous control tasks.
We choose three different environments controlling the robot movement: HalfCheetah, Hopper and Walker.
Our experiments are conducted on three datasets pre-collected under different policies: Medium-Expert, Medium and Medium-Replay.
We follow the experimental settings in Flowformer~\cite{wu2022flowformer}, and also compare to Decision Transformer (DT)~\cite{chen2021decision} which is commonly considered as the baseline with state-of-the-art performances based on the canonical attention.
Note that offline RL is an auto-regressive task and our Primal-Attention is adapted to its causal version, as described in the Supplementary Material.
We adopt the architecture of 3 layers, hidden dimension 256 with 4 heads, and the embedding dimension 64. 
Our hyper-parameters are set as {, .}
Each experiment is run with 3 different seeds on one NVIDIA Tesla P100 SXM2 16GB GPU. 

Results in Table~\ref{tab::rl} demonstrate that our Primal. outperforms all compared methods with a distinctly higher average reward.
Specifically, our Primal.DT reaches  points higher than the state-of-the-art Flowformer \cite{wu2022flowformer}. 
Compared to the baseline Decision Transformer (DT), our Primal. only replaces the self-attention in the top layer and keeps other structures the same and manages to improve the average reward by  points.
This verifies the effectiveness of our Primal-Attention and the benefits of our low-rank regularization of KSVD for the generality of DT in offline reinforcement learning. 
{We evaluate the efficiency with comparisons to DT and Flowformer in Table~\ref{tab::rl:efficiency}, which give distinctively better results than other baselines in Table~\ref{tab::rl}. 
	Our Primal.DT achieves comparable time and memory efficiency as the most efficient baseline DT, while Flowformer shows significantly lower efficiency in both aspects. Recall that our Primal.DT achieves an average reward of 77.5, which is 5.3 points higher than DT and 4.0 points higher than Flowformer.}

\begin{table}[t]
\caption{Rewards on D4RL~\cite{fu2020d4rl} datasets. 
	We report mean and variance for 3 seeds.
	A higher reward and a lower deviation indicates better performance.
	We consider Decision Transformer (DT), Linear Transformer (Linear.), Reformer (Re.), Performer (Per.), cosFormer (Cos.) and Flowformer (Flow.).
}
\label{tab::rl}
\begin{center}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{ccccccccc}
			\toprule
			\multirow{2}{*}{Dataset}       
			& \multirow{2}{*}{Environment} & DT & Linear. & Re. & Per. & Cos. & Flow. & \bf{Ours}      
			\\ 
			& & \cite{chen2021decision} & \cite{katharopoulos2020transformers} & \cite{kitaev2020reformer} & \cite{choromanski2021rethinking} & \cite{qin2022cosformer} & \cite{wu2022flowformer} & Primal.DT 
			\\ \midrule
			\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Medium\\ -Expert\end{tabular}} 
			& HalfCheetah                  
			& 83.83.3 & 78.23.2 & 81.51.6 & 85.12.1 & 85.52.9 & 90.80.4 & {77.822.1}
			\\
			& Hopper                       
			& 104.02.5 & 107.20.9 & 104.29.8 & 93.513.9 & 98.17.4 & 109.91.0 & 111.50.2 
			\\
			& Walker                       
			& 107.70.6 & 67.227.3 & 71.41.8 & 72.62.4 & 100.514.5 & 108.00.4 & 108.90.1            
			\\ \cmidrule(lr){3-9}
			\multirow{3}{*}{Medium}        
			& HalfCheetah                 
			& 42.40.1 & 42.30.2 & 42.20.1 & 42.10.2 & 42.10.3 & 42.20.2 & 43.00.0
			\\
			& Hopper                      
			& 64.21.1 & 58.70.4 & 59.90.7 & 59.77.5 & 59.83.8 & 66.92.5 & 74.50.6          
			\\
			& Walker                       
			& 70.63.2 & 57.910.6 & 65.84.9 & 63.310.7 & 71.41.2 & 71.72.5 & 77.97.8 
			\\ \cmidrule(lr){3-9}
			\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Medium\\ -Replay\end{tabular}} 
			& HalfCheetah                  
			& 34.60.6 & 32.11.5 & 33.60.7 & 31.70.9 & 32.83.6 & 34.71.5 & 38.90.4            
			\\
			& Hopper                       
			& 79.77.4 & 74.37.0 & 66.12.6 & 64.624.2 & 59.316.5 & 75.514.5 & 88.512.5
			\\
			& Walker                       
			& 62.95.0 & 62.17.4 & 50.13.5 & 61.36.7 & 60.59.9 & 62.03.1 & 76.810.3           
			\\ \midrule
			\multicolumn{2}{c}{Average Reward}                           
			& 72.2{\bf 2.6} & 64.46.5 & 63.92.9 & 63.87.6 & 67.87.6 & 73.52.9 & {\bf 77.5}6.0         
			\\ \bottomrule
	\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}


\begin{table}[t]
\caption{{Efficiency comparisons on running time (s/1K-steps) and memory (GB) on D4RL~\cite{fu2020d4rl}.
	}
}
\label{tab::rl:efficiency}
\begin{center}
	\resizebox{0.85\textwidth}{!}{
			\begin{tabular}{ccccccccccc}
				\toprule
				\multirow{2}{*}{Model} & \multicolumn{2}{c}{Medium-Expert} & & \multicolumn{2}{c}{Medium} & & \multicolumn{2}{c}{Medium-Replay} \\
				\cmidrule{2-3}  \cmidrule{5-6} \cmidrule{8-9} 
				& Time & Memory &   & Time & Memory &   & Time & Memory
				\\ \midrule
				DT (average reward: 72.2) &  20.8 & 0.3 & & 20.8 &0.3 & &20.8 & 0.3 
				\\
				Flow. (average reward: 73.5) & 54.4& 1.5 & & 54.4&1.5& & 54.3& 1.5  \\
				Primal.DT  (average reward: {77.5}) & 23.5 & 0.3& & 23.4&0.3& & 23.3& 0.3 
				&
				\\ \bottomrule
		\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}



\paragraph{{Large-scale Experiments}}
\label{subsec::imagenet}
We evaluate the capability of our Primal. model with DeiT-Small/16~\cite{touvron2021training} as backbone on 
ImageNet-100~\cite{russakovsky2015imagenet} and ImageNet-1K~\cite{deng2009imagenet} for image classification task. 
We also experiment with the language modelling task on WikiText-103~\cite{merity2016pointer}. 
On ImageNet,
we train DeiT-Small/16 and our Primal.DeiT-Small/16 from scratch following the same training protocols in~\cite{touvron2021training} with 4 NVIDIA Tesla V100 SXM2 32GB GPUs. 
Our hyper-parameters are chosen as , . 
{On WikiText-103, we follow the setting in~\cite{peng2021random} where the sequence length is set as 512, the model consists of 6 decoder layers with 8 heads.
We implement the causal version of Primal-Attention in the last layer, i.e., Primal.Trans with , .
Models are trained from scratch on 4 NVIDIA Tesla V100 SXM2 32GB GPUs for
150K updates after 6K-steps warm-up.}

{Table~\ref{tab::large_scale}(a) provides the test accuracy, training time and memory on a single V100 GPU with batch size 256 on both ImageNet-100 and ImageNet-1K datasets.
There is only one set of time and memory since models follow the same training protocols on both datasets.
Our Primal. achieves better accuracy than DeiT-Small/16 on ImageNet-100.
It also achieves the same accuracy as baseline with less training time and memory on ImageNet-1K. 
On WikiText-103 in Table~\ref{tab::large_scale}(b), our method with default setups significantly reduces the perplexity by 2.0 points than Transformer, and achieves comparable performances with the well-tuned Flowformer, a latest SoTA model, with enhanced efficiency.
Results show that using Primal-Attention in the last layer helps the overall performance.}








\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{{Results on large-scale experiments including image classification and language modelling.}}
	\begin{minipage}{0.6\textwidth}
		\centering
		\scalebox{0.66}
		{\subfloat[Test accuracy (\%) and efficiency on ImageNet-100~\cite{russakovsky2015imagenet} and ImageNet-1K~\cite{deng2009imagenet}.]{
				\begin{tabular}{ccccc}
					\toprule
					Model & \begin{tabular}[c]{@{}c@{}}ImageNet-100 \\ (Top-1 Acc.)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ImageNet-1K \\ (Top-1 Acc.)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time \\ (s/1K-steps)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Memory \\ (GB)\end{tabular}  
					\\ \midrule
					DeiT-Small/16           
					& 74.2 & 79.8 & 2425.5 & 14.2          
					\\
					Primal.DeiT-Small/16 
					& \textbf{75.7} & 79.8 & \textbf{2330.2} & \textbf{14.0}
					\\ \bottomrule
				\end{tabular}
				\label{tab::imagenet}}}
	\end{minipage}\hfill
	\begin{minipage}{0.4\textwidth}
		\centering
		\scalebox{0.66}
		{\subfloat[Results on WikiText-103~\cite{merity2016pointer}.]{
				\begin{tabular}{cccc}
					\toprule
					\multirow{2}{*}{Model} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Perplexity\\ ()\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Time\\ (s/1K-steps)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Memory\\ (GB)\end{tabular}} 
					\\ 
					& & &  
					\\ \midrule
					Trans.~\cite{vaswani2017attention}                 
					& 33.0 & 3108.4 & 9.0                                   
					\\
					Flow.~\cite{wu2022flowformer}
					& \textbf{30.8} & 3998.4 & 10.5
					\\
					Primal.Trans.       
					& 31.0 & \textbf{3104.0} & \textbf{8.9}      
					\\ \bottomrule
				\end{tabular}
				\label{tab::wikitext}}}
	\end{minipage}\hfill
\label{tab::large_scale}
\vspace{-3mm}
\end{table}











\paragraph{Ablation Study on Using Two Projections , }
We conduct an ablation study with (w/) and without (w/o) projection scores, i.e., -scores, involving the left singular vectors, on LRA~\cite{tay2020long}. 
Table \ref{tab::lra:r:scores} shows that using both projections (w/ -scores) helps boost performances, verifying the effectiveness of learning with the two sides of singular vectors on an asymmetric attention kernel.

\begin{table}[t]
\caption{{Ablation on -scores involving left singular vectors on LRA~~\cite{tay2020long} with test accuracy (\%).} 
}
\label{tab::lra:r:scores}
\begin{center}
	\resizebox{0.7\textwidth}{!}{
			\begin{tabular}{cccccccc}
				\toprule
				Model & -scores & ListOps & Text & Retrieval & Image & Pathfinder & Avg.~Acc 
				\\ \midrule
				\multirow{2}{*}{Primal.}          
				& w/o 
				& 36.8 & 52.4 & 58.2 & 30.5 & 50.2 & 45.6   
				\\
				& w/  
				& \bf 37.3 & \bf 61.2 & \bf 77.8 & \bf 43.0	& \bf 68.3 & \bf 57.5
				\\ \cmidrule{3-8}
				\multirow{2}{*}{Primal.Trans.} 
				& w/o 
				& 37.1 & 65.1 & 79.2 & 42.8 & 72.8 & 59.4   
				\\
				& w/  
				& \bf 37.3 & \bf 65.4 & \bf 81.0 & \bf 43.9 & \bf 74.3 & \bf 60.4 
				\\ \bottomrule
		\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}









\section{Related work}
\label{sec::related_work}

{Since the pioneering work~\cite{tsai2019}, the kernel-based approaches have become popular in Transformers, in which the kernel interpretation on the attention matrix has been shed light on.}
FourierFormer~\cite{nguyen2022fourierformer} treats the canonical self-attention as non-parametric regression with {methodologies for} symmetric kernels.
\cite{chi2022kerple} considers relative positional embedding with conditional positive definite kernel.
\cite{nguyen2023a} treats self-attention operation as support vector regression {without considering the asymmetry in the deployed kernel methods, and the  supervised regression is not applied in optimzing the attention either}.
\cite{chen2021skyformer} addresses the issue of asymmetry, however, it {resorts to symmetrization by replacing the softmax attention with an approximated symmetric one}, {thereby still dismissing the asymmetry}. 
{These prior works deploy} kernel-based techniques {that are originally designed for symmetric kernels and request to suffice Mercer conditions}, which is inconsistent with the asymmetric {nature} in self-attention, {resulting in a nontrivial gap between the analytical understanding and numerical implementation towards revealing the rationale in Transformers}. 
{In \cite{wright2021transformers}, it leverages the kernel tricks from RKBS~\cite{zhang2009reproducing} that allows asymmetry and formulates attention} as a binary kernel learning problem via empirical risk minimization.
However, it is hard to find an explicit optimization accordingly in {implementing Transformers.} 
Nevertheless, \cite{wright2021transformers} {provides an analytical tool from the aspect of kernel representer theorem upon RKBS that allows asymmetry.}


Much literature has also devoted to improving the efficiency of the attention computation through different approximation techniques. 
In the related works addressing the attention mechanism,
Reformer~\cite{kitaev2020reformer} uses locally-sensitive hashing for sparse approximation. 
Performer~\cite{choromanski2021rethinking} approximates self-attention matrix with random features. Linformer~\cite{wang2020linformer} considers low-rank approximation with the help of random projections. 
Nystr\"{o}mformer~\cite{xiong2021nystromformer} utilizes the Nystr\"{o}m method by down sampling the queries and keys in the attention matrix. \cite{child2019generating} incorporates sparsity prior on attention. 
These works pose the focus on  reducing the computation of the attention kernel matrix from the canonical self-attention. 
Hence, these works all address how to solve the problem in the dual form involving the kernel matrix, while we work in a significantly different way, that is, in the primal form.




\section{Conclusion}
\label{sec::conclusion}
In this paper, we interpret the self-attention in Transformers with asymmetric kernels and construct a learning framework with SVD on asymmetric kernels (KSVD) under the setups of LSSVM. 
Within the context of KSVD, a primal-dual model representation is formulated for self-attention and 
a novel attention mechanism (Primal-Attention) is proposed accordingly by leveraging the primal representation. 
It is quite significant that with Primal-Attention, not only the computation of the attention kernel matrix in the dual can be avoided, but also the cast unsupervised KSVD optimization can be efficiently incorporated into the training  through an additional regularization loss for more informative low-rank property. The analytical derivations and numerical evaluations demonstrate our great potentials in bridging explicit model interpretability and state-of-the-art performances.
Future works can include developing different variants with the low-rank property, e.g., robust Transformers, investigating more general applications of Primal-Attention to a wide range of architectures and tasks.

\section*{Acknowledgements}
This work is jointly supported by the European Research Council under the European Unionâ€™s Horizon 2020 research and innovation program/ERC Advanced Grant E-DUALITY (787960), iBOF project Tensor Tools for Taming the Curse (3E221427), Research Council KU Leuven: Optimization framework for deep kernel machines C14/18/068,  KU Leuven Grant CoE PFV/10/002, The Research Foundationâ€“Flanders (FWO) projects: GOA4917N (Deep Restricted kernel Machines: Methods and Foundations), Ph.D./Postdoctoral grant, the Flemish Government (AI Research Program), EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI-Integrating Reasoning, Learning and Optimization), Leuven.AI Institute.

\bibliography{bibfile}
\bibliographystyle{unsrt} 

\appendix
~\\
\section*{Appendix}

In this material, we present the proofs of all analytical results in the paper and additional comments in Section~\ref{sec::proofs}.
More experimental details and results are also provided in Section~\ref{sec::more_exp}.
We also provide the broader impacts of our work in Section~\ref{sec::broader_impact}.

\section{Theoretical Proofs}
\label{sec::proofs}
In this section, we provide the proofs of all analytical results presented in the paper, covering Theorem~\ref{theorem:ksvd:dual}, Remark~\ref{rmk:primal:dual}, and Lemma~\ref{prop:obj:0}. 
Additional comments are also provided following each proof in this material.

\subsection{Proof of Theorem 3.2}

\begin{proof}[Proof of Theorem~\ref{theorem:ksvd:dual}]
Given the matrix  consists of sequence data , the primal optimization problem in self-attention of KSVD with the constructed data-dependent projection weights  is formulated as follows, i.e.,~\eqref{eq:ksvd:lssvm:std:attention} in the paper:

where the projection weights of the feature maps  can be further denoted as ,  relying on parameters
 and the constant transformation matrix , the regularization coefficient denoted by  is a positive diagonal matrix.

The Lagrangian of \eqref{eq::primal_fx} is

where two sets of dual variable vectors, i.e., , are introduced to the equality constraints regarding the projection scores  and , for , respectively.

By taking the partial derivatives to the Lagrangian~\eqref{eq:lagrange:obj:data:case3}, the Karush-Kuhn-Tucker (KKT) conditions then lead to:


By eliminating the primal variables ,  in KKT conditions~\eqref{eq:lssvm:kkt:nonlinear:data:case3}, we then have

which can be expressed in the matrix form as

with  and . 

Therefore, the optimization problem of KSVD in the dual yields the following shifted eigenvalue problem with an asymmetric kernel matrix , such that:

which collects the solutions corresponding to the non-zero entries in  such that 
. 
The asymmetric kernel  contains the entries induced as
, . 
From the Lanczos Decomposition Theorem~\cite{lanczos1958linear}, i.e., Theorem 2.2 in the paper, we can see that the solutions to the dual problem of KSVD in self-attention, i.e.,  and , correspond to the left and right singular vectors of the asymmetric kernel matrix , 
where  serves as  the corresponding  singular values. 
This completes the proof.

\end{proof}








\paragraph{Comments on Theorem 3.2} 
With the primal problem in~\eqref{eq:ksvd:lssvm:std:attention} in the paper, Theorem~\ref{theorem:ksvd:dual} provides the corresponding dual problem of KSVD formulated for self-attention. 
In~\cite{suykens2016svd}, a novel variational principle is proposed for SVD with LSSVMs, where a primal-dual formulation for the matrix (linear) SVD is derived. 
Our KSVD leverages the kernel-based learning framework from~\cite{suykens2016svd}, however, in addition to our specific application of interpreting self-attention, there are other significant differences and non-trivial novelties in our work: 
\begin{itemize}
\item[\emph{i)}]~\cite{suykens2016svd} mainly addresses the original SVD given any data matrix, while we formulate the non-linear extension leading to the asymmetric attention matrix in relation to the queries and keys. 
Additionally,~\cite{suykens2016svd} presents the optimization w.r.t.~a single projection direction in the linear SVD, while we generalize the formulation to multiple projection directions in the matrix form. 

\item[\emph{ii)}] The data sources for the two non-linear feature maps are related to the queries and keys, as opposed to~\cite{suykens2016svd} that specifies the two data sources as the rows and columns of the given data matrix. 
Therefore, our KSVD is more general in the data setups. 

\item[\emph{iii)}] Rather than using only the data-independent projection weights  as linear mappings in~\cite{suykens2016svd}, we propose the generalized form that allows extra transformation matrix dependent non-linearly on the sequence data for self-attention.
\end{itemize}

In particular, the benefits and motivations of our data-dependent projection weights are as follows: 
\begin{itemize}
\item[\emph{i)}] In the canonical self-attention, the values vary for different input sequence data, and later in Remark~\ref{rmk:primal:dual}, we show that the values can be regarded as playing the role of the dual variables in KSVD. Inspired by this property, we introduce input sequence data information to the corresponding primal variables.

\item[\emph{ii)}] In the proposed Primal-Attention, the data-dependent projection weights provide more degrees of freedom to improve model's representation ability.

\item[\emph{iii)}] Using data-dependent projection weights does not affect the derivation of the shifted eigenvalue problem in the dual.
Specifically, when the transformation matrix  is chosen as an identity matrix for a simpler structure, it boils downs to the data-independent case, where the  kernel  in self-attention is obtained with entries , .
\end{itemize}
Provided with the general form of the projections weights in \eqref{eq::primal_fx}, practitioners can flexibly adjust the KSVD setups for the self-attention implementation.  
Related empirical studies can be referred to Section~\ref{subsec::data_dependent} in this material.



\subsection{Proof of Remark~3.3}
With the derivations of the primal-dual optimization problems above, the primal-dual model representation of our KSVD problem can be provided correspondingly.
The proof of Remark~\ref{rmk:primal:dual} in the paper closely follows the proof of Theorem~\ref{theorem:ksvd:dual}, and we show it as follows.
\begin{proof}[Proof of Remark~\ref{rmk:primal:dual}]
The primal model representations for the self-attention outputs
in~\eqref{eq::primal_fx} are

The dual model representations for the self-attention outputs can be derived by eliminating the primal variables with~\eqref{eq:lssvm:kkt:nonlinear:data:case3}:

Further, with the kernel trick in the dual optimization problem \eqref{eq::shifted_eigen_fx}, i.e.,

we then attain the primal-dual representations of KSVD that allows data-dependent projection weights for self-attention as follows:

where , .
\end{proof}

\paragraph{Comments on Remark 3.3}
With Remark~\ref{rmk:primal:dual}, we can equivalently represent the projection scores in different ways, i.e., either through the feature maps in the primal or the kernel matrix in the dual. 
Under the framework of KSVD, the existing attention outputs can be interpreted as the projection scores  in the dual representation, where the values correspond to the dual variables . 
The primal-dual models provide versatile alternatives to represent and understand the attention outputs. 
Notably, the primal representation can avoid the computation of the kernel matrix which is widely considered as an obstacle to the computational efficiency of attention.
In addition, we find that there exists another set of projections reflecting the asymmetry information, i.e., . 
Motivated by the above, we propose our new self-attention mechanism, i.e., Primal-Attention in Section~\ref{sec::method} in the paper.



\subsection{Proof of Lemma 4.2}
Lemma~\ref{prop:obj:0} evaluates the objective value  in the primal optimization problem \eqref{eq::primal_fx} when the solutions satisfy the stationarity conditions in \eqref{eq::shifted_eigen_fx}. 
\begin{proof}[Proof of Lemma~\ref{prop:obj:0}]
Based on the KKT conditions in~\eqref{eq:lssvm:kkt:nonlinear:data:case3}, by eliminating the primal variables , the optimization objective  is given by


where in the last equation, we denote the dual variables corresponding to the -th projection direction, i.e., singular vectors in relation to the singular value ,  as ,  
, and . 

Based on both~\eqref{shifted:eigen:ksvd:std:attention} and Theorem~\ref{theorem:ksvd:dual} in the paper, we have the following equations:

Hence, we can rewrite

which leads to 

Note that \eqref{eq::kkt_fx}, \eqref{eq::equal} and \eqref{eq::zero_obj} also hold for the data-independent projection weights case where  is an identity matrix.
In this case, the entries in the induced asymmetric kernel  become ,
.
This completes the proof.
\end{proof} 

\paragraph{Comments on Lemma 4.2}
With Lemma~\ref{prop:obj:0}, we validate that the objective  \eqref{eq::primal_fx} in the primal optimization problem reaches zero when the stationarity conditions are satisfied, i.e., the singular vectors and their corresponding singular values in \eqref{eq::shifted_eigen_fx} are obtained. 
In the paper, the KSVD optimization for self-attention is realized by incorporating the objective  as an additional regularization loss to the original task-oriented loss, and then minimizing the total loss to zero as shown in~\eqref{eq::objective_function} and~\eqref{eq:obj:std} in the paper.
In this manner, we avoid solving the dual optimization that involves a SVD problem on a kernel matrix.
Moreover, as in the proof of Theorem~\ref{theorem:ksvd:dual}, we note that the regularization coefficient  in the primal optimization~\eqref{eq::primal_fx} corresponds to the singular values in the dual optimization~\eqref{eq::shifted_eigen_fx}.
With the SGD-based or AdamW-based algorithm, we flexibly integrate the hyper-parameter selection of  into the optimization by setting  as a learnable parameter.
In this case,  can be optimized together with other model parameters by simply minimizing the total loss in~\eqref{eq::objective_function} in the paper.



\section{More Experimental Results}
\label{sec::more_exp}

\subsection{Setup Details}
This section provides the implementation details of all experiments included in the paper.
Firstly, we outline the main algorithm of our Primal-Attention mechanism in Algorithm~\ref{alg::primal} for clarity.
{Note that in data-dependent cases with , we take a subset of
 by uniformly sampling  points from  for efficiency aspects, as the main patterns of a matrix can be retained with random linear projections shown by the Johnsonâ€“Lindenstrauss lemma~\cite{lindenstrauss1984extensions}. 
This will be illustrated in details in the following.}


\begin{algorithm}[ht]
\caption{Learning with Primal-Attention }
\label{alg::primal}
\begin{algorithmic}
	\Require 
	 is the input sequence to the attention block in Transformer, 
	mappings ,  defined in~\eqref{eq:ksvd:lssvm:std:attention} in the paper, 
	number of projection directions  defined in~\eqref{eq:ksvd:lssvm:std:attention} in the paper,
	regularization coefficient  defined in~\eqref{eq::objective_function} in the paper,
	\Ensure
	Transformation matrix  defined in~\eqref{eq:ksvd:lssvm:std:attention} in the paper is required if data-dependent projection weights are used.
	\If{Data-dependent projection weights}
	\State , ;
	\Comment{, 
		}
	\State ; 
	\Comment{compute -score for }
	\State ; 
	\Comment{compute -score for }
	\State ;
	\Comment{compute concatenated output with }
	\ElsIf{Data-independent projection weights}
	\State , ;
	\Comment{, 
		}
	\State ; 
	\Comment{, compute -score for }
	\State ; 
	\Comment{, compute -score for }
	\State ;
	\Comment{compute concatenated output with }
	\EndIf
\end{algorithmic}
\end{algorithm}

\paragraph{UEA Time Series}
The UEA time series benchmark~\cite{bagnall2018uea} consists of 30 datasets.
Following the setup in~\cite{wu2022flowformer}, we select 10 datasets for evaluation.
For all experiments of our PrimalFormer and Primal.Trans., we adopt the data-dependent projection weights for Primal-Attention, i.e., we have  
and .
On account that some datasets consist of long sequence samples, e.g., EthanolConcentration of length 1751,
SelfRegulationSCP1 of length 896,
SelfRegulationSCP2 of length 1152,
our choice of  should include more information about  for greater model flexibility while maintaining computational efficiency.
In this regard, we set  where  is a subset of the sequence data  by uniformly sampling  points (rows) from .
We set  for most cases, and set  for datasets including FaceDetection, HandWriting, JapaneseVowels, PEMS-SF and SpokenArabicDigits, since they have shorter sequence lengths.
In this manner, the size of the transformation matrix  is implemented as  with , reducing memory requirements especially for long sequence data with large .

\paragraph{Long-Range Arena}
Long-Range Arena (LRA)~\cite{tay2020long} consists of long-sequence scenarios: ListOps of 2K sequence length, Text of 4K length, Retrieval of 4K, Image of 1K and Pathfinder of 1K.
With joint consideration of performance and efficiency, for all experiments of our PrimalFormer and Primal.Trans.~in the paper, we adopt Primal-Attention with data-dependent projection weights and set , i.e., , for all cases.

\paragraph{Reinforcement Learning}
D4RL~\cite{fu2020d4rl} is a suite of continuous control tasks and datasets for benchmarking progress in offline reinforcement learning.
In this experiment, we consider Primal.DT with Decision Transformer (DT)~\cite{chen2021decision} as the backbone.
Specifically, we consider a three-layer DT with its self-attention in third layer replaced by our Primal-Attention.
As DT utilizes a causal self-attention mask which predicts actions autoregressively, to align with the causal structure, we propose the causal-version of Primal-Attention, namely, Causal Primal-Attention. 
For clarity, we attach the corresponding PyTorch-like pseudo code in Figure~\ref{listing::causal} in this material.
Note that for this task, we utilize Causal Primal-Attention with the simpler data-independent projection weights, i.e., , which helps to prevent overfitting in learning rewards in the RL training process.

\begin{figure}[t]
\centering    
{\includegraphics[width=\textwidth]{./images/pseudo.jpg}}
\caption{PyTorch-like pseudo code of Causal Primal-Attention for RL.
}
\label{listing::causal}
\end{figure}

\paragraph{Image Classification}
ImageNet-100~\cite{russakovsky2015imagenet} contains 100 classes of images
from ImageNet-1K~\cite{deng2009imagenet}.
On both ImageNet-100 and ImageNet-1K,
we use Primal.DeiT-Small/16 with standard DeiT-Small/16 as the backbone.
Specifically, the self-attention of the last layer of DeiT-Small/16 is replaced by our Primal-Attention using data-dependent projection weights with the setup , i.e.,~.

\paragraph{Language Modelling} We conduct the language modelling on the WikiText-103~\cite{merity2016pointer}, which aims to estimate the probability distribution of a token given the previous ones.
We replace the self-attention in the last layer of the Transformer baseline with our Causal Primal-Attention using data-dependent projection weights with the setup , i.e.,~.


\subsection{Further Ablation Studies}

\begin{table}[t]
\caption{Ablation study on the two main hyper-parameters  and . 
	We report test accuracy (\%) of PrimalFormer on the UEA time series classification archive benchmark~\cite{bagnall2018uea}.}
\label{tab::ablation_timeseries}
\begin{center}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{cccccccccccc}
			\toprule
			\multirow{2}{*}{Dataset} & \multirow{2}{*}{} & \multicolumn{4}{c}{} 
			& \multirow{2}{*}{Dataset} & \multirow{2}{*}{} & \multicolumn{4}{c}{} 
			\\ \cmidrule(lr){3-6} \cmidrule(lr){9-12}
			& & 0  & 0.1  & 0.2  & 0.5  & 
			& & 0  & 0.1  & 0.2  & 0.5  
			\\ \midrule
			\multirow{3}{*}{EthanolConcentration} 
			& 20 & 32.3 &  31.6 & 30.8 & 32.7     
			& \multirow{3}{*}{FaceDetection}
			& 20 & 64.2 & \bf 67.1 & 66.2 & 66.4     
			\\ 
			& 30 & 30.0 & \bf 33.1 & 31.9 & 30.4 &                          
			& 30 & 65.0 & 65.3 & 65.2 & 65.7   
			\\
			& 40 & 30.8 & 32.3 & \bf 33.1 & 31.9 &                          
			& 40 & 64.5 & 65.6 & 66.5 & 66.7  
			\\ \midrule
			\multirow{3}{*}{HandWriting}        
			& 20 & 26.7 & 28.4 & 27.3 & 26.9     
			& \multirow{3}{*}{HeartBeat}        
			& 20 & 75.1 & 72.7 & 74.2 & 75.1
			\\
			& 30 & 28.2 & 26.9 & \bf 29.6 & 25.9 &                          
			& 30 & 75.6 & 75.6 & 75.6 & \bf 76.1     
			\\
			& 40 & 26.0 & 26.5 & 27.7 & 27.5 &                          
			& 40 & \bf 76.1 & 75.1 & 72.2 & 74.6 
			\\ \midrule
			\multirow{3}{*}{JapaneseVowels}        
			& 20 & 98.1 & 98.1 & 97.3 & 97.8    
			& \multirow{3}{*}{PEMS-SF}     
			& 20 & 86.1 & 85.6 & 83.8 & 84.4
			\\
			& 30 & 98.1 & 97.6 & 97.3 & 97.6 &                          
			& 30 & 83.8 & 86.7 & 82.1 & 86.7     
			\\
			& 40 & 98.1 & 98.1 & 97.6 & \bf 98.4 &                            
			& 40 & 86.1 & \bf 89.6 & 86.7 & 85.0      
			\\ \midrule
			\multirow{3}{*}{SelfRegulationSCP1}     
			& 20 & 91.5 & \bf 92.5 & 90.8 & 91.8    
			& \multirow{3}{*}{SelfRegulationSCP2}
			& 20 & \bf 57.2 & 53.9 & 55.6 & 56.1     
			\\
			& 30 & 92.2 & 91.8 & 92.2 & \bf 92.5 &                          
			& 30 & 55.6 & 53.9 & 55.6 & 55.8     
			\\
			& 40 & 91.8 & 91.8 & 91.5 & 91.8 &                          
			& 40 & 52.9 & 56.1 & 53.9 & 53.3     
			\\ \midrule
			\multirow{3}{*}{SpokenArabicDigits}   
			& 20 & 100 & 100 & 100 & 100      
			& \multirow{3}{*}{UWaveGestureLibrary} 
			& 20 & \bf 86.3 & 83.8 & 83.8 & 84.7     
			\\
			& 30 & 100 & 100 & 100 & 100 &                          
			& 30 & 85.3 & 85.0 & 85.0 & 84.1     
			\\
			& 40 & 100 & 100 & 100 & 100 &                          
			& 40 & \bf 86.3 & 84.1 & 84.1 & 85.9     
			\\ \bottomrule
	\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}

\begin{table}[t]
\caption{Ablation study on hyper-parameter  with . 
	We report the rewards of Primal.DT on the D4RL datasets~\cite{fu2020d4rl}.
	A higher reward indicates better performance.}
\label{tab::ablation_rl}
\begin{center}
	\resizebox{0.77\textwidth}{!}{
		\begin{tabular}{cccccccccc}
			\toprule
			\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Dataset / \\ Environment \end{tabular}} 
			& \multicolumn{3}{c}{Medium-Expert} 
			& \multicolumn{3}{c}{Medium} 
			& \multicolumn{3}{c}{Medium-Replay} 
			\\ \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
			& =32 & =64 & =96 & =32 & =64 & =96 & =32 & =64      & =96      
			\\ \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
			HalfCheetah                                                               
			& 56.9 & 73.1 & \bf 75.6 
			& 42.9 & \bf 43.1 & 42.8 
			& \bf 39.5 & 37.9 & 39.3      
			\\
			Hopper                                                                    
			& 111.2 & \bf 112.0 & 111.1 
			& 66.7 & 63.1 & \bf 73.8 
			& 84.2 & \bf 91.7 & 87.3      
			\\
			Walker   
			& 108.7 & 108.9 & \bf 109.0 
			& 75.1 & \bf 77.1 & 77.0 
			& 71.8 & 70.4 & \bf 80.5      
			\\ \bottomrule
	\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}

\paragraph{Ablation on  and }
The numerical investigations are conducted on the two main hyper-parameters of our Primal-Attention, i.e., the coefficient  of the KSVD regularization loss and the number of projection directions .
We consider the UEA time series datasets.
The results of PrimalFormer, i.e., two-layer Transformer with Primal-Attentions, are given in Table~\ref{tab::ablation_timeseries} in this material.
\emph{Firstly}, compared to , a rough tuning of  improves the performance for most of  the datasets.
For example,  on FaceDetection leads to consistent improvement over .
This indicates that the KSVD optimization through the regularization loss  in~\eqref{eq:obj:std} in the paper indeed brings performance boost over its non-regularized counterpart.
\emph{Secondly}, even without the KSVD optimization, i.e., , our Primal-Attention already leads to good performance, such as the results on SpokenArabicDigits, SelfRegulationSCP2 with , and UWaveGestureLibrary with . 
This verifies that the new representation in Primal-Attention in~\eqref{eq:primal:dual:ksvd:std:attention} in the paper can effectively represent the self-attention and conduct effective learning in the attention outputs.
\emph{Thirdly}, effective learning features can be captured in less dimensions than the original embedding dimension and a performance boost can be potentially gained with even fewer dimensions through our formulated KSVD.
To be specific, the embedding dimension for each head is 64, and we set  in the experiments.
Recall that the average accuracy of the canonical Transformer is 71.9\% in Table~\ref{tab::timeseries} in the paper, while our PrimalFormer reaches 73.1\%, which is 1.2\% higher upon the canonical one.
These results hence show that an appropriate compression in the number of projection directions by KSVD could lead to performance improvements when the low-rank property is desired.
Note that since the  kernel matrix in the dual of our Primal-Attention is of size , we limit  up to , i.e., , as there exists at most  projection directions in the corresponding KSVD.
In general, larger  is preferred in more complicated tasks with more sophisticated dependency between samples in the sequence data.
For instance, the reward learning in RL is such a case where less information compression is desired.  
This can be verified by the results given in Table~\ref{tab::ablation_rl} in this material, where the best performance is attained with  as 64 or 96 in almost all cases.


\paragraph{Projection Weights}
\label{subsec::data_dependent}

\begin{table}[t]
\caption{Ablation on data-dependent and data-independent projection weights of our Primal-Attention mechanism. 
	We report the test accuracy (\%) of Primal.Trans.~on UEA time series datasets~\cite{bagnall2018uea}.}
\label{tab::ablation_weights_time}
\begin{center}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{cccccccccccc}
			\toprule
			\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Data- \\ dependent \end{tabular}} & \multicolumn{10}{c}{Dataset}        
			& \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Avg. \\ Acc.\end{tabular} } 
			\\ \cmidrule(lr){2-11}
			& \begin{tabular}[c]{@{}c@{}}Ethanol \\ Concen. \end{tabular} 
			& \begin{tabular}[c]{@{}c@{}}Face \\ Detec. \end{tabular} 
			& \begin{tabular}[c]{@{}c@{}}Hand \\ Writ. \end{tabular}  
			& \begin{tabular}[c]{@{}c@{}}Heart \\ Beat \end{tabular}  
			& \begin{tabular}[c]{@{}c@{}}JPN \\ Vowels \end{tabular}  
			& \begin{tabular}[c]{@{}c@{}}PEMS \\ -SF \end{tabular}                  
			& \begin{tabular}[c]{@{}c@{}}SelfRegu. \\ SCP1 \end{tabular} 
			& \begin{tabular}[c]{@{}c@{}}SelfRegu. \\ SCP2 \end{tabular}
			& \begin{tabular}[c]{@{}c@{}}Spoken \\ ArabicDig. \end{tabular} 
			& \begin{tabular}[c]{@{}c@{}}UWave \\ GestureLib. \end{tabular} 
			\\ \midrule
			No     
			& 34.6 & 63.5 & \bf 29.3 & 76.1 & \bf 99.2 & 88.4 & \bf 92.8 & \bf 58.3 & 100 & 87.2   
			& 72.9
			\\
			Yes                   
			& \bf 35.4 & \bf 63.8 & 28.7 & \bf 77.1 & 98.9 & \bf 90.2 & 92.5 & 56.1 & 100 & \bf 88.4
			& \bf 73.1
			\\ \bottomrule
	\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}

\begin{table}[t]
\caption{Ablation on data-dependent and data-independent projection weights of our Primal-Attention mechanism. 
	We report the test accuracy (\%) of Primal.Trans.~on the LRA benchmark~\cite{tay2020long}.}
\label{tab::ablation_weights_lra}
\begin{center}
	\resizebox{0.77\textwidth}{!}{
		\begin{tabular}{ccccccc}
			\toprule
			\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Data- \\ dependent \end{tabular}} & \multicolumn{5}{c}{Dataset}        
			& \multirow{2}{*}{Average Accuracy} 
			\\ \cmidrule(lr){2-6}
			& ListOps & Text & Retrieval & Image & Pathfinder &                           \\ \midrule
			No                   
			& 37.0 & 40.2 & 74.3 & 80.8 & \bf 65.6 & 59.6                           \\
			Yes                      
			& \bf 37.3 & \bf 43.9 & 74.3 & \bf 81.0 & 65.4 & \bf 60.4      
			\\ \bottomrule
	\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}

We investigate the effects of projection weights in the data-dependent and data-independent cases for Primal-Attention.
Tables~\ref{tab::ablation_weights_time} and~\ref{tab::ablation_weights_lra} in this material present the comparisons  between data-dependent  and data-independent projection weights used in  Primal.Trans.~on UEA time series datasets~\cite{bagnall2018uea} and also LRA benchmark~\cite{tay2020long}.
On both benchmarks, data-dependent projection weights case surpasses its data-independent counterpart.
The reason of these results can be that data-dependent projection weights help increasing the model's representation ability and capturing more informative features from the rather long sequences in these datasets.
Furthermore, for the data-dependent case, we set  where  is a subset of sequence data by uniformly sampling   points from . 
As shown in Table \ref{tab::ablation_timeseries} in this material, for a given  in each dataset, the increase of  does not make the results fluctuate much. 
Similar phenomenon is found on the LRA datasets during our experiments.
Therefore, for almost all experiments in the paper, we simply set  as default.
This can also serve as a mild suggestion for practitioners in implementation.
We note that data-dependent projection weights are not always in favor.
For example, in the RL tasks, model is prone to overfit the learning of rewards during training if we adopt the Primal-Attention with data-dependent projection weights.
Hence, we take the data-independent case instead.
In the generalized form of the projection weights with Primal-Attention, more possibilities of greater model representation ability are provided to fit various tasks and datasets.

\subsection{Further Remarks on Efficiency}
\paragraph{Efficiency with Primal-Attention in architectures} From our learning scheme in Figure~\ref{fig::pipeline} in the paper and the empirical efficiency analysis 
with Tables~\ref{tab::timeseries_efficiency}, \ref{tab::efficiency}, \ref{tab::rl:efficiency} and \ref{tab::large_scale} in the paper, we can see that the efficiency gain of the Transformers implemented with Primal-Attention 
over canonical baselines is influenced by two main factors,
\textit{i)} the number of Primal-Attention layers employed in the architecture, i.e., the more the better; 
\textit{ii)} sequence length of the training data, i.e., the longer the more significant:
\begin{itemize}
\item[\emph{i)}] With deep architectures, the efficiency can be further improved by replacing more layers with our Primal-Attention. 
Yet, in very deep Transformers, Primal-Attention is not necessarily always superior in performance when being applied to all layers, as the learning in shallow layers may not enjoy the benefits from the low-rank property from KSVD as much as the higher layers do. 
It would be interesting to explore a more generic implementation setup for Primal-Attention in very deep Transformers, as briefly mentioned in the last paragraph of possible future work in this material.
\item[\emph{ii)}] The length of the data sequence, i.e., , is also a key factor influencing the efficiency. 
By avoiding the computation of the  attention matrix, our Primal-Attention can gain better efficiency on longer-sequence datasets. 
Although ImageNet-1K is large-scale, currently Transformers treat each image as a sequence of length 197 (with {\tt cls} token), which is actually not too long (even compared to some UEA datasets as shown in Table~\ref{tab::timeseries_efficiency} in the paper). 
Hence, this is also a reason why our Primal.DeiT-Small/16 does not improve the efficiency significantly in Table~\ref{tab::large_scale}(a) in the paper. 
Similarly in WikiText-103, the data sequence length is 512, which is also not really long, hence the efficiency of our Primal.Trans.~is not always superior under the current setups.
\end{itemize}
\paragraph{Efficiency gain of Primal. in different Tasks}
The efficiency gain of Primal. over baseline is more significant on UEA and LRA, as the backbone has only 2 layers, hence replacing one layer makes a difference to the overall architecture. 
Moreover, UEA and LRA in general have longer training sequence length, which would signalize Primal-Attention's efficiency. 
In contrast, the backbones on D4RL, WikiText-103 and ImageNet have more layers where canonical self-attention layers are the majority structures in Primal. as shown in Table~\ref{tab::arch} in this material. 
Besides, the efficiency gain is less significant also due to the shorter training sequence lengths on these datasets. 

\begin{table}[t]
\caption{Architecture of Primal. on different datasets.}
\label{tab::arch}
\begin{center}
	\resizebox{0.73\textwidth}{!}{
		\begin{tabular}{cccc}
			\toprule
			Primal.  & {\tt canonical\_layer}+{\tt{[}primal\_layer{]}} & {\tt num\_head} & {\tt head\_dim} 
			\\ \midrule
			UEA & 1+{[}1{]} & 8 & 64        
			\\
			LRA & 1+{[}1{]} & 2 & 32        
			\\
			D4RL & 2+{[}1{]} & 4 & 64        
			\\
			WikiText-103 & 5+{[}1{]} & 8 & 64        
			\\
			ImageNet & 11+{[}1{]} & 6 & 64     
			\\ \bottomrule
	\end{tabular}}
\end{center}
\vspace{-5mm}
\end{table}



\section{Broader Impacts} \label{sec::broader_impact}

\paragraph{Societal Impacts}
In this work, we provide a new perspective to interpret self-attention through a KSVD problem with asymmetric kernels under the LSSVM framework. 
Compared to the canonical Transformers, our method is more efficient in tackling long-sequence datasets with our more efficient architectures that avoids the kernel matrix computation and also regularize the model with improved low-rank properties.
In this aspect, our method is more energy friendly as it can decrease the power consumption during training.



\paragraph{Possible Future Works}
We introduce a new self-attention mechanism from the primal perspective of the KSVD problem where feature maps are utilized rather than the kernels.
We currently work on the feature map corresponding to the Cosine similarity kernel in the paper that achieves state-of-the-art performances on the evaluated benchmarks. 
For more general setups and applications,  different feature maps and backbone architectures can be further investigated.
Therefore, it can extend our method to a wider range of tasks and possibly gain better performance under practical scenarios. 
These can be possible directions for future work.













		


























\end{document}