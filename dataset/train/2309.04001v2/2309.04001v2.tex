\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{hidelinks}


\def\x{{\mathbf x}}
\def\L{{\cal L}}

\newcommand{\squishlist}{
   \begin{list}{}
    { \setlength{\itemsep}{0pt}      \setlength{\parsep}{3pt}
      \setlength{\topsep}{3pt}       \setlength{\partopsep}{0pt}
      \setlength{\leftmargin}{1.0em} \setlength{\labelwidth}{1em}
      \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }

\newcommand{\Salnote}[1]{{\bf \color{blue}  [SA:{#1}]}}

\title{Multimodal Transformer for Material Segmentation}
\name{Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
\thanks{This work is supported in part by AFOSR award FA9550-21-1-0330.}}
\address{University of California, Riverside, 
Air Force Research Laboratory}




\begin{document}
\maketitle


\begin{abstract}
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05\% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4\%) and human (+9.1\%) classes. Ablation studies show that different modules in the fusion block are crucial for overall model performance. Furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials. The code and pretrained models will be made available at https://github.com/csiplab/MMSFormer.  

\end{abstract}
\begin{keywords}
multimodal image segmentation, material segmentation, multimodal fusion, transformer
\end{keywords}


\section{INTRODUCTION}
Image segmentation \cite{minaee2021image, cheng2001color} methods assign one class label to each pixel in an image. The segmentation map can be used for holistic understanding of objects or context of the scene. Image segmentation can be further divided into different types; examples include semantic segmentation \cite{guo2018semseg, wang2018semseg2}, instance segmentation \cite{hafiz2020instanceseg, gu2022instanceseg2}, panoptic segmentation \cite{kirillov2019panoptic, elharrouss2021panoptic2} and material segmentation \cite{Liang2022MCubeS, upchurch2022dense}. Each of these segmentation tasks are designed to address specific challenges and applications. 

Multimodal image segmentation \cite{zhang2021deep, guo2019deep2} aims to enhance the accuracy and completeness of the task by leveraging diverse sources of information, and potentially leading to a more robust understanding of complex scenes. In contrast to single-modal segmentation, the multimodal approach is more complex due to the necessity of effectively integrating heterogeneous data from different modalities. Key challenges arise from variations in data quality and attributes, distinct traits of each modality, and need to create models capable of accurately and coherently segmenting with the fused information.

Most of the existing multimodal segmentation methods are designed to work with specific modality pairs, such as RGB-Depth \cite{chen2020sa-gate, hazirbas2017fusenet, hu2019acnet}, RGB-Thermal \cite{li2023RSFNet, liang2023eaef, sun2019rtfnet}, and RGB-Lidar \cite{prakash2021TransFuser, zhao2021lifseg, li2023mseg3d}. Most of these methods generally do not work well with modality combinations different from the ones used in the original design. Recently, CMX \cite{zhang2023cmx} introduced a technique to fuse information from RGB and one other supplementary modality, but it is incapable of fusing more than two modalities at the same time. Some recent models have proposed techniques to fuse more than two modalities \cite{zhang2023CMNext, broedermann2023hrfuser, Liang2022MCubeS}. However, they either use very complex fusion strategies \cite{zhang2023cmx, broedermann2023hrfuser} or require additional information like semantic labels \cite{Liang2022MCubeS} for performing underlying tasks. 

In this paper, we propose a new model for multimodal material segmentation that we call MMSFormer. Our model uses a transformer \cite{xie2021segformer} architecture with a novel fusion block to perform multimodal material segmentation. 
In particular, our proposed fusion block uses channel attention to combine information across multiple modalities using a small number of linear layers. 
Such a design provides a simple, lightweight fusion block that can handle an arbitrary number of input modalities. 


To test our proposed MMSFormer and fusion block, we focus on multimodal material segmentation using MCubeS dataset \cite{Liang2022MCubeS}. The dataset consists of four different modalities: RGB, angle of linear polarization (AoLP), degree of linear polarization (DoLP) and near-infrared (NIR). We integrate the fusion block with MiX-Transformer \cite{xie2021segformer} to build MMSFormer that provides state-of-the-art performance for multimodal material segmentation \cite{Liang2022MCubeS}. A series of experiments highlight the ability of fusion block to combine different modality combinations, resulting in superior performance compared to current state-of-the-art methods. Ablation studies show that different input modalities assist in identifying different types of materials. Furthermore, as we add new input modalities, overall performance gradually increases. 

Main contributions of this paper can be summarized as follows. 
\squishlist
    \item We propose a new multimodal segmentation model called MMSFormer. The model incorporates our proposed fusion block that can fuse information from arbitrary (herterogeneous) combinations of modalities. 
\item Our model achieves new state-of-the-art performance on multimodal material segmentation (MCubeS) dataset \cite{Liang2022MCubeS}. Furthermore, our method achieves better performance for all modality combinations compared to the current leading models.
\item A series of ablation studies show that each module on the fusion block has important contribution towards the overall model performance and each input modality assists in identifying specific types of materials.
\squishend
Rest of the paper is structured as follows. Section~\ref{sec:related-work} presents a brief review of related work. We describe our model and fusion block in detail in Section~\ref{sec:proposed-model}. Section~\ref{sec:experiment} presents experimental results and ablation studies on multimodal material segmentation task with qualitative and quantitative analysis.


\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.80\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MMSFormer-Overall-2.eps}
         \caption{Overall architecture of MMSFormer}
         \label{fig:model}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.80\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MMSFormer-Fusion.eps}
         \caption{Proposed fusion block architecture}
         \label{fig:fusion}
     \end{subfigure}
    \caption{(a) Overall architecture of the proposed MMSFormer model. Each image passes through a modality-specific encoder where we extract hierarchical features. Then we fuse the extracted features using the proposed fusion block and pass the fused features to the decoder for predicting the segmentation maps. (b) Proposed multimodal fusion block. We first concatenate all the features along the channel dimension and pass it through MLP layer to fuse them. Then a mixer layer captures and mixes multi-scale features using parallel convolutions and MLP layers. We use Squeeze and Excitation block as channel attention in the residual connection.}
    \label{fig:model-architecture}
\end{figure*}

\section{RELATED WORK}
\label{sec:related-work}

The field of image segmentation has witnessed significant evolution, spurred by advancements in machine learning and computational capabilities. A significant improvement in this evolution came with the inception of fully convolutional networks (FCNs) \cite{long2015FCN, wu2019fastfcn}, which revolutionized the field by enabling pixel-wise predictions through the utilization of hierarchical features within convolutional neural networks (CNNs). This led to the development of a variety of CNN based models for different images segmentation tasks. One of the pioneering models that emerged from this era is U-Net \cite{ronneberger2015unet}. U-Net's distinctive feature lies in its utilization of skip-connections, which establish direct connections between the lower-resolution feature maps and their corresponding higher-resolution counterparts. DeepLabV3+ \cite{chen2018deeplabv3+} introduced dilated convolutions into the encoder, also known as atrous convolutions, allowing the expansion of the receptive field without increasing computational complexity significantly. PSPNet \cite{zhao2017pspnet} ventured into yet another dimension by introducing global context modules. These modules enable the model to gather information from a wide range of spatial scales, essentially integrating both local and global context into the segmentation process. Collectively, these models ushered in a new era of segmentation techniques, each addressing specific challenges in the field through innovative architectural components. 

In recent years, image segmentation field has witnessed a transformative shift with the emergence of transformer-based models. This represents a significant leap forward in the field, as these models utilize the power of attention mechanisms, which have proven to be exceptionally effective in handling complex image segmentation tasks. Some of the notable transformer-based models are PVT (Pyramid Vision Transformer) \cite{wang2021pvt}, SegFormer \cite{xie2021segformer}, and Mask2Former \cite{cheng2022mask2former}. PVT utilizes transformer based design for various computer vision tasks. SegFormer utilizes efficient self-attention and lightweight multilayer perceptron (MLP) decoder for simple and efficient semantic segmentation. Mask2Former uses masked-attention along with pixel decoder and transformer decoder for any segmentation task. Their success demonstrates the capacity of these models to provide state-of-the-art solutions in various segmentation tasks.

In the context of multimodal image segmentation, fusion of data from diverse sources \cite{zhang2021deep} has gained traction as a means to extract richer information and improve accuracy. A variety of models and fusion strategies have been proposed for multimodal segmentation tasks. Notably, the realm of RGB-Depth semantic segmentation has seen several advancements. The FuseNet \cite{hazirbas2017fusenet} model integrates depth feature maps into RGB feature maps, while SA-Gate \cite{chen2020sa-gate} employs Separation-and-Aggregation Gating to mutually filter and recalibrate RGB and depth modalities before fusion. Attention Complementary Module has been proposed by ACNet \cite{hu2019acnet} that extracts weighted RGB and depth features for fusion. The domain of RGB-Thermal image segmentation has also gained prominence. Recent models include RTFNet \cite{sun2019rtfnet} which achieves fusion through elementwise addition of thermal features with RGB, RSFNet \cite{li2023RSFNet} proposing Residual Spatial Fusion module to blend RGB and Thermal modalities, and EAEFNet \cite{liang2023eaef} utilizing attention interaction and attention complement mechanisms to merge RGB and Thermal features. A number of methods also focus on fusing RGB-Lidar data which include TransFuser \cite{prakash2021TransFuser} that employs Transformer blocks, whereas LIF-Seg \cite{zhao2021lifseg} relies on coarse feature extraction, offset learning, and refinement for effective fusion.

While the previously mentioned studies focus on specific pairs of modalities, recent research has demonstrated promising results in the fusion of arbitrary modalities. CMX \cite{zhang2023cmx} introduces a cross-modal feature rectification module and a feature fusion module to merge RGB features with supplementary modalities. Notably, this fusion technique showcases encouraging outcomes by effectively combining RGB with five distinct modalities individually. For multimodal material segmentation, the MCubeSNet \cite{Liang2022MCubeS} model is proposed, which can seamlessly integrate four different modalities to enhance segmentation accuracy. In the context of arbitrary modal semantic segmentation, CMNeXt \cite{zhang2023CMNext} introduces Self-Query Hub and Parallel Pooling Mixer modules, offering a versatile approach for fusing diverse modalities. Additionally, HRFuser \cite{broedermann2023hrfuser} employs multi-window cross-attention to fuse different modalities at various resolutions, thereby enriching model performance.

These recent advancements underscore the growing interest in exploring flexible and efficient strategies for fusing arbitrary modalities, pushing the boundaries of multimodal image segmentation and its application across diverse scenarios.


\section{PROPOSED MODEL}
\label{sec:proposed-model}

\subsection{Model Overview}
The overall architecture of our proposed MMSFormer model is shown in Figure\,\ref{fig:model}. We use modality specific encoders and shared MLP decoder. Modality specific encoders capture distinctive features from each input modality. Then we fuse the extracted features from different modalities with our proposed fusion block shown in Figure\,\ref{fig:fusion} and pass the fused features to the MLP decoder to predict the material segmentation labels. We use 4-stage encoders following most of the previous designs  \cite{wang2021pvt, xie2021segformer, chen2018deeplabv3+} to capture pyramidal features from each modality. In particular, we use Mix-Transformer-B2 (MiT-B2) \cite{xie2021segformer} as the encoder. We choose Mix-Transformer for various reasons. First, it can provide multi-scale features without positional encoding. Second, it uses efficient self-attention that reduces the number of parameters significantly. Third, it also works well with simple and lightweight MLP decoder.

Given a combination of modalities as input, each modality-specific encoder maps the corresponding image into modality-specific multi-scale features as

where  represents image for modality . The encoder for th modality generates four feature maps of shapes , ,  and , which we represent as .  

The encoder has a four-stage design, where each stage of the encoder generates feature maps of different shapes. We use four separate fusion blocks (one corresponding to each encoder stage) to fuse the features from each stage of the encoder. After extracting the features  from all the modalities, we pass them to the fusion block corresponding to the specific stage of the encoder. 
Each fusion block uses features extracted from all the modalities and fuse them to generate a combined feature representation .  denotes the fused features at th stage that can be represented as 

We pass the combined features  to the lightweight MLP decoder \cite{xie2021segformer} to predict the material segmentation labels. 

\subsection{Multimodal Fusion Block}
The fusion block is responsible for fusing the features extracted from the modality specific encoders. After extracting features  from each modality, we fuse them using the fusion block shown in Figure\,\ref{fig:fusion}. We have one fusion block for each of the four encoder stages. For the  fusion block, let us assume the input feature maps are given as  where , , and  can be different for different encoder stages. First, we concatenate the feature maps from  modalities along the channel dimension to get the combined feature map . Then we pass the features through an MLP layer that fuses the features by while reducing the channel dimension to . Let us denote the resulting features as . We represent the operation as 

 represents concatenation of features along the channel dimension and MLP computes linear combinations along the channel dimension. 

Many existing methods have shown that multi-scale features can improve performance for segmentation tasks. For instance, DeepLabV3+ \cite{chen2018deeplabv3+} uses dilated convolutions and CMNeXt \cite{zhang2023CMNext} uses parallel pooling to capture multi-scale features. Inspired by these methods, we design a Mix Feed Forward Network (MixFFN) for capturing and mixing multi-scale features. The MixFFN module consists of two MLP layers having parallel convolution layers in between them. We capture multi-scale features by applying parallel convolutions. Specifically, we apply , , and  convolutions to capture multi-scale features and add them with the residual feature. We apply two MLP layers, one before and one after the parallel convolutions, to mix information in the feature map. Ablation studies show that , , and   convolutions are sufficient for optimal model performance. However, to keep the number of parameter low and the fusion block lightweight, we use depth-wise convolution instead of normal convolution. These steps can be performed as 



Channel attention focuses on learning the importance or relevance of different channels within a feature tensor. The attention modules recalibrate  interdependence between channels and allows the model to select the most relevant features or channels while suppressing less important ones. This can lead to more effective feature representations for the underlying task. Channel information play an important role in enhancing model performance. Earlier work \cite{hu2019acnet, chen2020sa-gate, zhang2023cmx}  used different types of channel attentions to improve model performance for different multimodal segmentation tasks. Motivated by the previous studies, we apply Squeeze-and-Excitation block \cite{hu2019squeezeandexcitation} as channel attention in the residual connection.
We represent the final fused feature as 


Ablation studies show that applying channel attention is crucial to overall model performance and it boosts performance by 0.23\% mIoU. 


A simple decoder with multiple MLP layers is often sufficient to achieve optimal performance for transformer-based methods. This is because the effective receptive field of the Mix-Transformer encoders are relatively larger than most of the convolutional network-based encoders \cite{xie2021segformer}. To predict the final segmentation map , we pass the fused features  to an MLP decoder as 

First, the pyramidal features of different shapes go through an MLP layer to make the channel dimension same for all of them, followed by an upsample operation to unify the spatial dimension. Then they are concatenated and passed through another MLP layer to fuse the features. Finally, the last MLP layer predicts the segmentation maps from the fused features. 

In summary, the fusion block extracts and combines useful information from different modality combinations. Each module in the fusion block contributes towards the overall model performance, which we further discuss in the experiment section. 





\begin{table}[th]
\centering
\caption{Performance comparison on Multimodal Material Segmentation (MCubeS) dataset \cite{Liang2022MCubeS}. Here A, D, and N represent angle of linear polarization (AoLP), degree of linear polarization (DoLP), and near-infrared (NIR) respectively.}
\label{tab:performance-comparison}
\begin{tabular}{lcc}
    \toprule
    Method                                 & Modalities & mIoU (\%) \\
    \midrule
    \midrule
    DRConv  \cite{chen2021DRConv}            & RGB-A-D-N  & 34.63 \\
    DDF  \cite{zhou2021DDF}                  & RGB-A-D-N  & 36.16 \\
    TransFuser  \cite{prakash2021TransFuser} & RGB-A-D-N  & 37.66 \\
    DeepLabv3+  \cite{chen2018deeplabv3+}    & RGB-A-D-N  & 38.13 \\
    MMTM  \cite{joze2020mmtm}                & RGB-A-D-N  & 39.71 \\
    FuseNet  \cite{hazirbas2017fusenet}      & RGB-A-D-N  & 40.58 \\
    MCubeSNet  \cite{Liang2022MCubeS}        & RGB-A-D-N  & 42.46 \\
    CMNeXt (MiT-B2)  \cite{zhang2023CMNext}  & RGB-A-D-N  & 51.54 \\  
    MMSFormer (MiT-B2)                     & RGB-A-D-N  & \textbf{52.05} \\
    \bottomrule
\end{tabular}
\end{table} 
\begin{figure*}[th]
     \centering
     \includegraphics[width=\textwidth]{figs/ours-vs-cmnext.eps}
    \caption{Visualization of results for RGB and all modalities (RGB-A-D-N) prediction with CMNeXt \cite{zhang2023CMNext} and our proposed MMSFormer. For brevity, we only show the RGB image and Ground Truth material segmentation maps. Our model provides overall better results and correctly  identifies asphalt, gravel, and road markings as indicated in the rectangular bounding boxes.}
    \label{fig:ours-vs-cmnext}
\end{figure*}


\begin{table*}[th]
\centering
\caption{Per class \% IoU comparison on Multimodal Material Segmentation (MCubeS) \cite{Liang2022MCubeS} dataset. Our proposed {\it MMSFormer} model shows significant improvement in detecting {\it Gravel (+10.4\%)} and {\it Human (+9.1\%)} classes compared to the current state-of-the-art models while maintaining competitive performance in other classes.}
\setlength{\tabcolsep}{2pt}
\begin{tabularx}{\textwidth}{l|cccccccccccccccccccc|c}
    \toprule
    Methods &
    \rotatebox[origin=c]{90}{Asphalt} &
    \rotatebox[origin=c]{90}{Concrete} &
    \rotatebox[origin=c]{90}{Metal} &
    \rotatebox[origin=c]{90}{Road marking} &
    \rotatebox[origin=c]{90}{Fabric} &
    \rotatebox[origin=c]{90}{Glass} &
    \rotatebox[origin=c]{90}{Plaster} &
    \rotatebox[origin=c]{90}{Plastic} &
    \rotatebox[origin=c]{90}{Rubber} &
    \rotatebox[origin=c]{90}{Sand} &
    \rotatebox[origin=c]{90}{Gravel} &
    \rotatebox[origin=c]{90}{Ceramic} &
    \rotatebox[origin=c]{90}{Cobblestone} &
    \rotatebox[origin=c]{90}{Brick} &
    \rotatebox[origin=c]{90}{Grass} &
    \rotatebox[origin=c]{90}{Wood} &
    \rotatebox[origin=c]{90}{Leaf} &
    \rotatebox[origin=c]{90}{Water} &
    \rotatebox[origin=c]{90}{Human} &
    \rotatebox[origin=c]{90}{Sky} &
    \rotatebox[origin=c]{90}{Mean} \\ 
    \midrule
    \midrule
    
    MCubeSNet &
    85.7 &
    42.6 &
    47.0 &
    59.2 &
    12.5 &
    44.3 &
    3.0 &
    10.6 &
    12.7 &
    66.8 &
    67.1 &
    27.8 &
    65.8 &
    36.8 &
    54.8 &
    39.4 &
    73.0 &
    13.3 &
    0.0 &
    94.8 &
    42.9 \\
    
    CMNeXt &
    84.3 &
    44.9 &
    \textbf{53.9} &
    \textbf{74.5} &
    \textbf{32.3} &
    \textbf{54.0} &
    \textbf{0.8} &
    \textbf{28.3} &
    \textbf{29.7} &
    67.7 &
    66.5 &
    27.7 &
    68.5 &
    \textbf{42.9} &
    \textbf{58.7} &
    \textbf{49.7} &
    75.4 &
    \textbf{55.7} &
    18.9 &
    96.5 &
    51.5 \\
    
    MMSFormer &
    \textbf{87.7} &
    \textbf{45.7} &
    52.9 &
    71.2 &
    30.5 &
    51.0 &
    0.4 &
    27.9 &
    25.9 &
    \textbf{69.7} &
    \textbf{76.9} &
    \textbf{28.3} &
    \textbf{72.0} &
    41.4 &
    56.3 &
    47.2 &
    \textbf{76.8} &
    54.5 &
    \textbf{28.0} &
    \textbf{96.6} &
    \textbf{52.1} \\ 
    \bottomrule
\end{tabularx}
\label{tab:per-class-iou}
\end{table*} 
\begin{figure*}[th]
     \centering
     \includegraphics[width=\textwidth]{figs/ours.eps}
    \caption{Visualization of results for prediction using different modality combinations of our proposed MMSFormer model. Detection of concrete, gravel and road markings become more accurate as we add more modalities as shown in the rectangular bounding boxes.}
    \label{fig:ours-modality-combination}
\end{figure*}

\begin{table}[th]
\centering
\caption{Performance comparison (\% mIoU) on Multimodal Material Segmentation (MCubeS) \cite{Liang2022MCubeS} dataset for different modality combinations.}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lccc}
    \toprule
    Modalities & MCubeSNet \cite{Liang2022MCubeS} & CMNeXt \cite{zhang2023CMNext} & MMSFormer         \\
    \midrule
    \midrule
    RGB        & 33.70     & 48.16  & \textbf{50.07} \\
    RGB-A      & 39.10     & 48.42  & \textbf{51.28} \\
    RGB-A-D    & 42.00     & 49.48  & \textbf{51.57} \\
    RGB-A-D-N  & 42.86     & 51.54  & \textbf{52.05} \\
    \bottomrule
\end{tabular}
\label{tab:modality-wise-comparison}
\end{table} 
\begin{table*}[th]
\centering
\caption{Per class \% IoU comparison on Multimodal Material Segmentation (MCubeS) \cite{Liang2022MCubeS} dataset for different modality combinations. As we add modalities incrementally, overall performance increases gradually. This table also shows that specific modality combinations assist in identifying specific types of materials better.}
\setlength{\tabcolsep}{2pt}
\begin{tabularx}{\textwidth}{l|cccccccccccccccccccc|c}
    \toprule
    Modalities &
    \rotatebox[origin=c]{90}{Asphalt} &
    \rotatebox[origin=c]{90}{Concrete} &
    \rotatebox[origin=c]{90}{Metal} &
    \rotatebox[origin=c]{90}{Road marking} &
    \rotatebox[origin=c]{90}{Fabric} &
    \rotatebox[origin=c]{90}{Glass} &
    \rotatebox[origin=c]{90}{Plaster} &
    \rotatebox[origin=c]{90}{Plastic} &
    \rotatebox[origin=c]{90}{Rubber} &
    \rotatebox[origin=c]{90}{Sand} &
    \rotatebox[origin=c]{90}{Gravel} &
    \rotatebox[origin=c]{90}{Ceramic} &
    \rotatebox[origin=c]{90}{Cobblestone} &
    \rotatebox[origin=c]{90}{Brick} &
    \rotatebox[origin=c]{90}{Grass} &
    \rotatebox[origin=c]{90}{Wood} &
    \rotatebox[origin=c]{90}{Leaf} &
    \rotatebox[origin=c]{90}{Water} &
    \rotatebox[origin=c]{90}{Human} &
    \rotatebox[origin=c]{90}{Sky} &
    \rotatebox[origin=c]{90}{Mean} \\ 
    \midrule
    \midrule
    
    RGB &
    86.0 &
    44.3 &
    52.1 &
    70.1 &
    31.5 &
    54.5 &
    0.2 &
    23.0 &
    \textbf{28.6} &
    62.8 &
    62.4 &
    29.2 &
    70.2 &
    43.7 &
    \textbf{60.8} &
    47.3 &
    \textbf{77.3} &
    46.1 &
    14.9 &
    96.5 &
    50.1 \\
    
    RGB-A &
    86.8 &
    45.8 &
    \textbf{54.7} &
    69.6 &
    \textbf{35.7} &
    53.8 &
    \textbf{2.1} &
    \textbf{30.6} &
    28.1 &
    65.2 &
    63.8 &
    \textbf{30.1} &
    67.6 &
    42.5 &
    59.8 &
    46.1 &
    76.6 &
    \textbf{56.8} &
    13.3 &
    96.5 &
    51.3 \\
    
    RGB-A-D &
    87.0 &
    \textbf{47.5} &
    54.4 &
    70.6 &
    33.7 &
    \textbf{55.2} &
    0.5 &
    26.7 &
    27.5 &
    62.6 &
    66.9 &
    28.2 &
    69.9 &
    \textbf{45.2} &
    58.4 &
    \textbf{47.6} &
    75.7 &
    56.6 &
    20.7 &
    96.5 &
    51.6 \\ 

    RGB-A-D-N &
    \textbf{87.7} &
    45.7 &
    52.9 &
    \textbf{71.2} &
    30.5 &
    51.0 &
    0.4 &
    27.9 &
    25.9 &
    \textbf{69.7} &
    \textbf{76.9} &
    28.3 &
    \textbf{72.0} &
    41.4 &
    56.3 &
    47.2 &
    76.8 &
    54.5 &
    \textbf{28.0} &
    \textbf{96.6} &
    \textbf{52.1} \\
    \bottomrule
\end{tabularx}
\label{tab:per-class-iou-modality-combination}
\end{table*} 
\begin{table}[th]
\centering
\caption{Ablation study of the {\it Fusion Block} on Multimodal Material Segmentation (MCubeS) \cite{Liang2022MCubeS} dataset. All the four input modalities were used during training and testing. The table shows the contribution of different modules in fusion block in overall model performance.}
\setlength{\tabcolsep}{1pt}
\begin{tabular}{lc}
    \toprule
    Structure                                            & \% mIoU (Change) \\
    \midrule
    \midrule 
    MMSFormer (MiT-B2)                                   & \textbf{52.05}\\
    \midrule
    \hspace{3mm} - without channel attention             & 51.82 {\footnotesize \color{red} (-0.23)} \\
    \hspace{3mm} - 1x1 conv instead of channel attention & 50.58 {\footnotesize \color{red} (-1.47)} \\
    \hspace{3mm} - with 3x3, 7x7 \& 11x11 convolutions   & 51.63 {\footnotesize \color{red} (-0.42)} \\
    \hspace{3mm} - without parallel convolutions         & 51.59 {\footnotesize \color{red} (-0.46)} \\
    \hspace{3mm} - without MixFFN                        & 51.24 {\footnotesize \color{red} (-0.81)} \\
    \bottomrule
\end{tabular}
\label{tab:fusion-block-ablation-study}
\end{table} 


\section{EXPERIMENTS AND RESULTS}
\label{sec:experiment}
\subsection{Multimodal Material Segmentation Dataset}
Multimodal material segmentation (MCubeS) dataset 
\cite{Liang2022MCubeS} contains 500 sets of images from 42 street scenes. Each scene has images for four modalities: RGB, angle of linear polarization (AoLP), degree of linear polarization (DoLP), and near-infrared (NIR). The dataset provides annotated ground truth labels for both material and semantic segmentation for every pixel. The dataset is divided training set with 302 image sets, validation set with 96 image sets, and test set with 102 image sets. Each image has   pixels and a total of 20 class labels per pixel. 

\subsection{Implementation Details}

To ensure a fair comparison with prior models, we followed the same data preprocessing and augmentation strategies as employed in MCubeSNet \cite{Liang2022MCubeS} and CMNeXt \cite{zhang2023CMNext}. We used the Mix-Transformer-B2 (MiT-B2) \cite{xie2021segformer} architecture pretrained on the ImageNet \cite{deng2009ImageNet} dataset as the backbone for our model. Each modality has a separate MiT-B2 \cite{xie2021segformer} encoder, each of which is initialized with ImageNet \cite{deng2009ImageNet} pretrained weights. We used a shared MLP decoder from SegFormer \cite{xie2021segformer} and used random initialization for it. Training was conducted for 500 epochs for all the experiments. We trained and evaluated all our models using two NVIDIA RTX 2080Ti GPUs. We used PyTorch for model development. 

The initial learning rate was set to , and we utilized a polynomial learning rate scheduler with a power of 0.9 to dynamically adjust the learning rate during training. The first 10 epochs were designated as warm-up epochs with a learning rate of 0.1 times the original rate. For loss computation, we used the cross-entropy loss function. Optimization was performed using the AdamW \cite{loshchilov2017adamw} optimizer with an epsilon value of  and weight decay set to 0.01. A batch size of 4 was employed for all the experiments. 


\subsection{Comparison with Existing Methods}
We conducted a rigorous performance evaluation of our model compared to established baseline models. The comprehensive results are summarized in Table~\ref{tab:performance-comparison}. Our model achieves a mean intersection-over-union (mIoU) of 52.05\%, surpassing the current state-of-the-art model. Our method also shows significant improvement (more than 9.59\%) over all other baseline models available for this dataset. Thus, our model sets new state-of-the-art for the MCubeS dataset.

To further analyze the performance of our model, we conducted a per-class IoU analysis. The results are presented in Table~\ref{tab:per-class-iou}. The analysis reveal how our model performs on a class-by-class basis. Notably, our model demonstrates a substantial improvement in the detection of gravel and human classes, achieving an increase of 10.4\% and 9.1\% in IoU, respectively, compared to the current state-of-the-art model. While achieving these notable gains in specific classes, our model also maintained competitive performance across other classes, closely aligning with the current state-of-the-art model. 

We show material segmentation results predicted by CMNeXt \cite{zhang2023CMNext} model and our proposed MMSFormer model in Figure\,\ref{fig:ours-vs-cmnext}. For brevity, we only show RGB images and Ground Truth material segmentation maps in the illustrations. We show RGB only predictions and all modalities (RGB-A-D-N) predictions for both of the models. As shown in the rectangular bounding boxes, our proposed MMSFormer model identifies asphalt, gravel and road marking with greater accuracy than CMNeXt \cite{zhang2023CMNext} model for both RGB only and all modalities (RGB-A-D-N) predictions. 

\subsection{Contribution of Different Modalities}
A critical aspect of this work involves evaluating the effectiveness of our proposed fusion block in harnessing valuable information from diverse modalities. To analyze this effect, we trained our model with various combinations of modalities. The results are presented in Table~\ref{tab:modality-wise-comparison}. Our model exclusively trained on RGB data provided an mIoU score of 50.07\%, which 1.91\% grater than the current state-of-the-art model. We observe progressive improvement in performance as we incorporated additional modalities: AoLP, DoLP, and NIR. The integration led to incremental performance gains, with the mIoU increasing from 50.07\% to 51.28\%, then to 51.57\%, and ultimately reaching to 52.05\%. These findings serve as a compelling evidence that our fusion approach effectively leverages and fuses valuable information from different combination of modalities, resulting in a notable enhancement in segmentation performance. 
Furthermore, our model consistently outperforms the current state-of-the-art benchmark across all modality combinations. This consistent superiority underscores the robustness and versatility of our fusion block, demonstrating its ability to adapt and excel regardless of the specific modality combination provided. 

\subsection{Ablation Study on the Fusion Block}

We conducted a series of ablation studies aimed at dissecting the contributions of individual components within the fusion block to the overall model performance. The findings, as detailed in Table~\ref{tab:fusion-block-ablation-study}, shed light on the critical importance of these components. We used all four input modalities during training and testing in these experiments. First, we observed that the absence of channel attention in the residual connection had a negative impact, resulting in a reduction in performance by 0.23\%. Moreover, the substitution of channel attention with a  convolution led to a more significant performance drop of 1.47\%. This stark contrast underscores the unique capabilities of channel attention in capturing and leveraging crucial information effectively. Additionally, while comparing larger convolution kernel sizes (, , and ) to the originally employed (, , and ), we noted a decrease in performance by 0.42\%. This result underscores the significance of the carefully chosen convolution kernel sizes within the fusion block.

Furthermore, completely removing the parallel convolutions within the block led to a performance decline of 0.46\%, emphasizing their substantial contribution to the overall model's efficacy. Finally, the MixFFN module, encompassing two MLP layers and the parallel convolutions, was found to be exceptionally crucial for the fusion block performance, with a notable drop of 0.81\% in its absence. These comprehensive ablation studies collectively underscore the significance of every component within the fusion block, revealing that each module plays a distinct and vital role in achieving the overall performance of our model. 

\subsection{Ablation Study on Different Modality Combinations}

To analyze the contributions of various modalities in the identification of distinct materials, we conducted a series of ablation studies, focusing on per-class IoU for different modality combinations. The insights from these investigations are summarized in Table~\ref{tab:per-class-iou-modality-combination}. As we progressively integrate new modalities, a discernible pattern of performance improvement emerges for specific classes, which include asphalt, road marking, sand, gravel, cobblestone and human classes. Particularly noteworthy is the substantial assistance provided by NIR data in classifying sand, gravel, and human categories, leading to significant performance gains in these classes as NIR was added as an additional modality. 

Conversely, certain classes such as rubber, grass, and leaf exhibited a gradual performance decline as we introduced additional modalities. This suggests that RGB data alone suffices for accurately identifying these classes, and the inclusion of more modalities potentially introduces noise or redundancy that negatively impacts performance. Moreover, AoLP appears to be instrumental in enhancing the recognition of materials like concrete, metal, fabric, plaster, plastic, ceramic, and water. Similarly, DoLP is valuable in improving performance for classes like glass, brick, and wood. These findings underscore the relationship between different imaging modalities and the unique characteristics of different types of materials, demonstrating that specific modalities excel in detecting particular classes based on their distinctive traits. Example images are discussed in Figure~\ref{fig:ours-modality-combination}. 

In Figure\,\ref{fig:ours-modality-combination}, we presents some examples to show how adding different modalities help improve performance of segmentation. We show predictions for RGB, RGB-A, RGB-A-D and RGB-A-D-N inputs from our proposed MMSFormer model. As we add new modalities, the predictions become more accurate as shown in the bounding boxes. The illustrations show that the identification of concrete, gravel and road marking becomes more accurate with additional modalities. This also illustrates the capability of the fusion block to effectively fuse information from different modality combinations.




\section{CONCLUSION}
\label{sec:conclusion}
In this paper, we introduce a novel fusion block designed to combine useful information from various modality combinations. We also propose a new model called MMSFormer that integrates the proposed fusion block to accomplish multimodal material segmentation task. Experimental results illustrate the model's capability to efficiently fuse features from four distinct modalities, leading to a new state-of-the-art performance on the Multimodal Material Segmentation (MCubeS) dataset. Experiments also show that the fusion block can extract useful information from different modality combinations which helps the model to consistently outperform current state-of-the-art model. Performance increases gradually as we add new modalities. Several ablation studies further highlight how different components of the fusion block contribute to the overall model performance. Ablation studies also reveal that different modalities assist in identifying different types of materials. Future work may include investigating the model's performance with modalities beyond the four explored in this paper and other multimodal segmentation tasks. 


\ninept 
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}


\end{document}
