\documentclass{article}



\newcommand{\tablerule}{\noalign{\smallbreak \hrule \smallbreak}}
\newcommand{\tabletoprule}{\noalign{\hrule \smallbreak}}
\newcommand{\tablebottomrule}{\noalign{\smallbreak \hrule}}

\newcommand{\idspace}{\{0,1\}^d}
\newcommand{\kbuckets}{-buckets}
\newcommand{\kbucket}{-bucket}
\newcommand{\bi}[1]{ {\cB}_i(#1)}
\newcommand{\bix}{\bi{x}}
\newcommand{\di}[1]{ {\cD}_i(#1)}
\newcommand{\dix}{\di{x}}
\newcommand{\dht}{{\scshape dht}}
\newcommand{\dhts}{{\dht s}}
\newcommand{\id}{{\scshape id}}
\newcommand{\ids}{{\id s}}
\newcommand{\xor}{{\scshape xor}}
\newcommand{\ptwop}{{P2P}}
\newcommand{\polar}[1]{\widehat{#1}}
\newcommand{\pT}{\polar{T}}
\newcommand{\pTi}[1]{{T_{X_#1{\polar{X}_#1}}}}
\newcommand{\ck}{{c_k}}
\newcommand{\ckp}{{c_k'}}
\newcommand{\cks}{{c_k^*}}
\newcommand{\Knk}{{\cK_{n,k}}}

\newcommand{\origin}{\bar{0}}
\newcommand{\porigin}{\bar{1}}
\newcommand{\xone}{{x\polarorigin}}

\newcommand{\Ws}{{\seq{W_t}}}
\newcommand{\Ss}{{\seq{|S_t|}}}
\newcommand{\Bs}{{\seq{B_t}}}
\newcommand{\Bis}{(B_{t,i})_{i \ge 1}}


\newcommand{\Vertices}{\cV}
\newcommand{\Edges}{\cE}

\newcommand{\vectorx}{{\bf x}}

\newcommand{\inftrie}{{\mathbb T}}

\newcommand{\Thead}{{T_1^\prime}}
\newcommand{\Ttail}{{T_1^{\prime\prime}}}
\newcommand{\Ts}{{T_1^*}}
\newcommand{\Tss}{{T_1^{**}}}
\newcommand{\Th}{{{T}_1^*}}
\newcommand{\Thh}{{{T}_1^{**}}}
\newcommand{\Thhh}{{{T}_1^{***}}}

\newcommand{\ow}{{\overline{w}}}
\newcommand{\oDelta}{{\overline{\Delta}}}
\newcommand{\oV}{{\overline{V}}}
 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{array}
\usepackage{tikz}
\usepackage{float}
\usepackage[numbers]{natbib}
\usepackage[margin=1.45in]{geometry}
\usepackage[final]{changes}
\normalem



\newcommand{\R}{{\textbf R}}
\newcommand{\Z}{{\textbf Z}}
\newcommand{\N}{{\textbf N}}
\newcommand{\C}{{\textbf C}}
\newcommand{\Q}{{\textbf Q}}
\newcommand{\G}{{\textbf G}}
\newcommand{\Lt}{{\textbf L}}

\newcommand\cA{{\cal A}}
\newcommand\cB{{\cal B}}
\newcommand\cC{{\cal C}}
\newcommand\cD{{\cal D}}
\newcommand\cE{{\cal E}}
\newcommand\cF{{\cal F}}
\newcommand\cG{{\cal G}}
\newcommand\cH{{\cal H}}
\newcommand\cI{{\cal I}}
\newcommand\cJ{{\cal J}}
\newcommand\cK{{\cal K}}
\newcommand\cL{{\cal L}}
\newcommand\cM{{\cal M}}
\newcommand\cN{{\cal N}}
\newcommand\cO{{\cal O}}
\newcommand\cP{{\cal P}}
\newcommand\cQ{{\cal Q}}
\newcommand\cR{{\cal R}}
\newcommand\cS{{\cal S}}
\newcommand\cT{{\cal T}}
\newcommand\cU{{\cal U}}
\newcommand\cV{{\cal V}}
\newcommand\cW{{\cal W}}
\newcommand\cX{{\cal X}}
\newcommand\cY{{\cal Y}}
\newcommand\cZ{{\cal Z}}


\newcommand{\E}[1]{{\textbf E}\left[#1\right]}
\newcommand{\e}{{\textbf E}}
\newcommand{\V}[1]{{\textbf{Var}}\left(#1\right)}
\newcommand{\va}{{\textbf{Var}}}
\newcommand{\bP}{{\textbf P}}
\newcommand{\p}[1]{{\textbf P}\left\{#1\right\}}
\newcommand{\psub}[2]{{\textbf P}_{#1}\left(#2\right)}
\newcommand{\psup}[2]{{\textbf P}^{#1}\left(#2\right)}
\newcommand{\po}[1]{{\textbf P}_{\sigma}\left(#1\right)}
\newcommand{\pk}[1]{{\textbf P}_{k}\left(#1\right)}
\newcommand{\I}[1]{{\textbf 1}_{[#1]}}
\newcommand{\set}[1]{\left( #1 \right)}
\newcommand{\Cprob}[2]{{\textbf P}\set{\left. #1 \; \right| \; #2}}
\newcommand{\probC}[2]{{\textbf P}\set{#1 \; \left|  \; #2 \right. }}
\newcommand{\phat}[1]{\ensuremath{\hat{\textbf P}}\left(#1\right)}
\newcommand{\Ehat}[1]{\ensuremath{\hat{\textbf E}}\left[#1\right]}
\newcommand{\ehat}{\ensuremath{\hat{\textbf E}}}
\newcommand{\Esup}[2]{{E^{#1}}\left[#2\right]}
\newcommand{\esup}[1]{{E^{#1}}}
\newcommand{\Esub}[2]{{E_{#1}}\left[#2\right]}
\newcommand{\esub}[1]{{E_{#1}}}

\newcommand\inprobLOW{\rightarrow_p}
\newcommand\inprobHIGH{\,{\buildrel p \over \rightarrow}\,} 
\newcommand\inprob{{\inprobHIGH}}



\newcommand{\eql}{\,{\buildrel \cL \over =}\,}

\newcommand{\eqd}{\,{\buildrel {\rm def} \over =}\,}

\newcommand{\convergeas}[1]{\,{\buildrel {{#1}} \over \longrightarrow}\,}




\newcommand{\argmin}{{\rm arg\,min}}
\newcommand{\argmax}{{\rm arg\,max}}

\newcommand{\seq}[1]{{(#1)_{t \ge 0}}}
\newcommand{\seqone}[1]{{(#1)_{t \ge 1}}}

\newcommand{\n}{\{1,\ldots,n\}}

\newcommand{\ind}[1]{{\textbf{1}_{\left[#1\right]}}}
 



\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{prop}{Proposition}
 
\title{The Analysis of Kademlia for random IDs}

\author{
    Xing Shi Cai \hspace{8 mm} Luc Devroye\thanks{Research of the authors was supported
    by NSERC.}\\
    \small School of Computer Science, McGill University of Montreal, Canada,\\
    \small \texttt{xingshi.cai@mail.mcgill.ca} \hspace{8 mm} \texttt{lucdevroye@gmail.com}
}



\begin{document}

\maketitle

\begin{abstract}
Kademlia~\citep{Maymounkov02} is the de facto standard searching algorithm for
\ptwop\ (peer-to-peer) networks on the Internet.  In our earlier
work~\citep{Cai2013}, we introduced two slightly different models for Kademlia
and studied how many steps it takes to search for a target node by using
Kademlia's searching algorithm. The first model, in which nodes of the network
are labeled with deterministic \ids, had been discussed in that paper.  The
second one, in which nodes are labeled with random \ids, which we call the
Random \id\ Model, was only briefly mentioned.  Refined results with detailed
proofs for this model are given in this paper. Our analysis shows that with
high probability it takes about  steps to locate any node, where 
is the total number of nodes in the network and  is a constant that does not
depend on .
\end{abstract}

\section{Introduction to Kademlia}

A \ptwop\ (peer-to-peer) network~\citep{Schollmeier2001} is a decentralized
computer network which allows participating computers (\emph{nodes}) to share
resources.  Some \ptwop\ networks have millions of live nodes.  To allow
searching for a particular node without introducing bottlenecks in the network,
a group of algorithms \added{called} \dht\ (Distributed Hash Table)~\citep{Balakrishnan03}
was invented in the early 2000s, including Plaxton's
algorithm~\citep{Plaxton1999accessing}, Pastry~\citep{Rowstron01}, {\scshape
can}~\citep{Ratnasamy2001}, Chord~\citep{Stoica2001},
Koorde~\citep{Kaashoek2003koorde}, Tapestry~\citep{Zhao04}, and
Kademlia~\citep{Maymounkov02}. Among them, Kademlia is most widely used in
today's Internet.

In Kademlia, each node is assigned an \id\ selected uniformly at random from
 (\emph{\id\ space}), where  is usually ~\citep{Steiner07} or
~\citep{Crosby07}. The \emph{distance} between two nodes is calculated by
performing the bitwise exclusive or (\xor) operation over their \ids\ and taking the result
as a binary number. (In this work \emph{distance} and \emph{closeness} always
refer to the \xor\ distance between \ids.) 

Roughly speaking, a Kademlia node keeps a table of a few other nodes
(\emph{neighbors}) \replaced{whose distances are sufficiently diverse}{with diversified distances to
itself}.  So when a node
searches for an \id, it always has some neighbors close to its target.  By
inquiring these neighbors, and these neighbors' neighbors, and so on, the
node that is closest to the target \id\ in the network will be found
eventually. Other \dhts\ work in similar ways.  The differences mainly come
from how distance is defined and how neighbors are chosen. For a more detailed
survey of \dhts, see~\citep{Balakrishnan03}.

\section{The Random ID Model}

This section briefly reviews the Random \id\ Model for Kademlia defined
in~\citep{Cai2013}. Let  be the length of  binary \ids\
 chosen uniformly at random from  without
replacement. Consider  nodes indexed by . Let  be the
\id\ of node . 

Given two \ids\ , their \xor\ 
distance is defined by 

where  is the \xor\ operator


Let  be the length of the common prefix of  and . The 
nodes can be partitioned into  parts by their common prefix length with
 via

For each ,  tables (\emph{buckets}) of size \replaced{at most}{up to}  are
kept, where  is a fixed positive integer. Buckets are indexed by .  The bucket  is filled with \replaced{}{up to } indices drawn
uniformly at random from  without replacement. Note that the first
 bits of , if , agree with the first  bits of
, but the -th bit is different.

Searching for  initiated at node  proceeds as follows. Given
that ,  can only be in . Thus, all indices from
the bucket  of  are retrieved, say . From them, the one
having shortest distance to  is selected as . (In fact, any selection
algorithm would be sufficient for the results of this paper.) Note that

\added{Thus the choice of  does not depend on the exact distances from  to .}
Therefore, instead of the \xor\ distance, only the length of common prefix is
needed in the following analysis of searching.

The search halts if  or if the bucket is empty. In the latter case,
 is closest to  among all nodes. Otherwise we continue from .
Since , the maximal number of steps before
halting is bounded by .  Let  be the number of steps before halting in
the search of  when started from  (\emph{searching time}).  Then .

Treating  as strings consisting of zeros and ones, they can be
represented by a tree data structure called
~\citep{Szpankowski2011}.  The 's can be viewed as
subtrees.  Filling buckets is equivalent to choosing \replaced{at most}{up to}  leaves from each
of these subtrees.  Fig.\,\ref{fig:partition} gives an example of an \id\ trie.

\begin{figure}
\centering {
    \scalebox{0.8} {
        \includegraphics{fig-figure0.eps}
    }
}
\caption[]{An example of Kademlia \id\ trie. Given an \id\ , the
trie is partitioned into subtrees  and .  Node
 maintains a bucket for each of these subtrees containing \replaced{at most}{up to}  nodes
from the corresponding subtree.}
\label{fig:partition}
\end{figure}

\section{Main Results}

The structure of the model is such that nothing changes if  are
replaced by their coordinate-wise \xor\ with a given vector .
This is a mere rotation of the hypercube.
Thus, it can be assumed without loss of generality that , the rightmost branch in the \id\ trie.

If  for some , the searching time is \replaced{}{quite stable and
acceptable}, which is undoubtedly a contributing factor in Kademlia's success.
If , then it is not a useful upper bound of searching time
any more. However, in some
probabilistic sense,  can be much smaller than
---it can be controlled by the parameter , which measures the
amount of storage consumed by each node. The aim of this work is to investigate
finer properties of these random variables. In particular, the following
theorem is proved:
\begin{theorem}
\label{thm:random:target}
\added{Assume that . Let  be a fixed integer.}
Let 
denote convergence in probability. Then

where  is a function of  only:

In particular, .
\end{theorem}

\added{In the rest of the paper, we first show that once the search reaches a node that shares a
common prefix of length about  with , the search halts in  steps.
Thus it suffices to prove Theorem \ref{thm:random:target} for the time that it takes for this
event to happen.  Then we show that the \id{} trie is well balanced with high probability. Thus
when  is a power of , we can couple the search in the original trie with a search in a
trie that is a complete binary tree. It proves the theorem for this special case. After that, we
give a sketch of how to deal with general . At the end we briefly summarize some
implications of the theorem.}

\section{The Tail of the Search Time}

To keep the notation simple, let  and \replaced{and note that  is not necessarily
integer-valued}{assume that  is a power
of two. In Section~\ref{sec:general}, we sketch what to do when  is not a
power of two}.  Also, for analytic
purposes, define

Since  and , 


The importance of  follows from the fact that once the search reaches a node
 with , it takes very few steps to finish. 
\added{Let  be}
the number of search steps that depart from a node in the set
 for some , with the very first node in the search being . 

\begin{lemma}
    Theorem~\ref{thm:random:target} follows if 
    
    \label{lem:tail}
\end{lemma}
\begin{proof}
    \added{Let .}
     counts steps of the search departing from a node 
    in  .  Thus
    
    Noting that
    
    by linearity of expectation, 
    
    Thus, for all  fixed, 
    
    Therefore . For the expectation, note that
    
    by the lemma's assumption and the fact that .
\end{proof}

\section{Good Tries and Bad Tries}

Since the tail of search does not matter, define a new partition  of all nodes by
merging subtrees  for  as follows:

Let . It follows from~\eqref{eq:tree:expectation} that

or simply , where .
\added{Note that  is
    hypergeometric with parameters
    
    i.e., it corresponds to the selection of  balls without replacement from
    an urn of  balls of which  are
    white~\citep[chap.~6.3]{Johnson2005}.}

The analysis of  can be simplified if the 's are all close to their
expectations. To be precise, let  be the \emph{accuracy
parameter}. An \id\ trie is \emph{good}, if

for all . Otherwise it is called \emph{bad}.

\begin{figure}
\centering {
    \scalebox{1.1} {
        \includegraphics{fig-figure1.eps}
    }
}
\caption[]{The approximate sizes of subtrees in a good trie.}
\label{fig:tree}
\end{figure}

\begin{lemma}
    \label{lem:pleasant}
    The probability that an \id\ trie is bad is .
\end{lemma}

\begin{proof}
    It follows from the union bound that
    
    The fact used here is that  where
     is binomial .
For the binomial, .
\end{proof}

\section{Proof when  Is a Power of }

In this section,  is assumed to be a power of , i.e.,  is an integer.
The general case is treated in the next section.

\subsection{A Perfect Trie}

\label{sec:perfect}

Construct a coupled \id\ trie consisting of  as follows. If
, i.e., the size of the subtree  is at least its
expectation, let  for the  smallest indices in .
After this preliminary coupling, some 's are undefined. The indices 
for which  are undefined go into a global pool  of size

For a good trie, the size of the pool is at most


For a subtree  of size , take  indices  from
 and assign  a value, that is different from all other 's, and
that has .  Subtrees of this new trie have fixed
sizes of

A trie like this is called \emph{perfect}.  Indices  for which , i.e., , are called \emph{ghosts}. Other indices are
called \emph{normal}.

Next, refill the buckets according to the perfect trie, but keep buckets of normal
indices containing no ghosts unchanged.  Observe that a search step departing
at a normal index  proceeds precisely the same in both tries if bucket 
(with ) of  does not contain ghosts.  \added{Assuming that the original trie is good,
the
probability that a bucket \added{that corresponds to  for some } contains a ghost is not more than . This is because in the newly constructed prefect trie, the subtree 
contains no more than  proportion of ghost nodes.}

Let  denote the number of search steps starting from node  via node 
with  in the perfect trie. Then

where  is the event that at least one node in the buckets
encountered during a search is a ghost. Let  be the event that the
trie is good. It follows from Lemma~\ref{lem:pleasant} that

Therefore, Theorem~\ref{thm:random:target} follows if


\subsection{Filling the Buckets with Replacement}

\label{sec:replacement}

To deal with the problem that buckets are filled by sampling without
replacement, another coupling argument is needed. Let  be the probability
that the  items sampled with\deleted{out} replacement from a set of size  are
not all distinctive. Observe that by the union bound,

If , then bucket  of  has  elements drawn without
replacement from

Observe that

Hence, with probability , the sampling can be seen as having been
carried out with replacement. 

The coupling is as follows: for all  with  and all , mark bucket  of  with probability . When a bucket is
marked, replace its entries with  new entires drawn with replacement
conditioned on the existence of at least one duplicate entry. In this way,
all bucket entries are for a sampling with replacement. Let the search time,
starting still from , \added{be denoted }by . Let  be the event that during the
search \replaced{a marked}{an unmarked} bucket is encountered. Observe that

Therefore

So Theorem~\ref{thm:random:target} follows if


\subsection{Analyzing  Using a Sum of I.I.D.\ Random Variables}

\label{sec:geo1}

Let . Assume that step  of the search departs from node
 and reaches node . Let ,
i.e.,  represents the progress in this step. Then

Due to the recursive structure of a \replaced{perfect}{prefect} trie,
, although not i.i.d.,
should have very similar distributions. This intuition leads to the following
analysis of  by studying a sum of i.i.d.\ random variables.

One observation allows us to deal with truncated version of 's is as
follows:
\begin{lemma}
    Let  be a sequence of real numbers with . Define
    
    where  is also a real number.  Then
    
    \added{where we define the infimum of an empty set to be .}
    \label{lem:truncation}
\end{lemma}

\begin{proof}
    Let .
    \added{If  or , the lemma is trivially true. 
    So we assume .}
    By induction\added{ on },
    one can show that  if . 
    \added{Since , we have , which is the induction basis.
        If  for all  and , then
        }Therefore  if and only if .
\end{proof}

\noindent Let .
It follows from the previous lemma that

which is quite convenient as the distribution of  is easy to compute.

Assume again that step  of the search departs from node  with
. Consider one item, say , in bucket  of .
Recall that  is selected uniformly at random from all \replaced{indices }{indexes} with
.  Thus it follows from the structure of a perfect
trie, which is given by~\eqref{eq:Y}, that

Or shifted by ,

If truncated by , we obtain

Note that this is \emph{exactly} the distribution of a geometric  truncated by .

Recall that among all the values of  given by items in the
bucket  of , the one chosen as the next stop of the search gives the maximum.
Thus
 
Let  be i.i.d.\ geometric .  Let .
Then


Let  be a geometric  minus one. \added{Then .}
Let  be i.i.d.\ random variables distributed as .  Let
.  Using induction and the
previous argument about , one can show that

\added{For the induction basis, note that
    
    Assume that  for some .
    Then for all ,}
    
    Thus \eqref{eq:V:D} is proved.
    It then follows from Lemma~\ref{lem:truncation} and \eqref{eq:V:D} that

which makes  much easier to analyze.

Since  if and only if  are all smaller than , 

Therefore, by definition of ,

Readers familiar with renewal theory~\citep[chap.\ 4.4]{Durrett2010probability}
can immediately see that

which completes the proof of Theorem~\ref{thm:random:target} for  which is
power of . The following Lemma gives some more details.
\begin{lemma}
    If ,
    
    \label{lem:convergence}
\end{lemma}
\begin{proof}
    Since  is geometric ,
    
    In other words, , where  denotes
    stochastical ordering.  
    Let 
    
    Then  and  By the strong law of large
    number\added{s}, both  and  converge\deleted{s} to  almost surely.
    Therefore .
\end{proof}

\section{Proof for the General Case}

\label{sec:general}

In this section, the proof Theorem~\ref{thm:random:target} for  an
arbitrary integer is only sketched as most methods used here are very similar
to those in the previous section.

\subsection{An Almost Perfect Trie}

When  is not power of ,  is not guaranteed
to be an integer. So a perfect trie is not well defined 
any more.  However, let us define

Then the coupling argument for perfect tries used in Section~\ref{sec:perfect}
can still be applied, now replacing  by .

In this way, a trie consisting of  can be constructed, with
its subtrees having fixed sizes of

If the original trie is good, then the number of indices  for which , called \emph{ghosts}, is
bounded by

A trie with these properties is called \emph{almost perfect}.

Let  denote the number of search steps starting from node  via node 
with  in the almost perfect trie. If  and  are
coupled  the same way as they were in Section~\ref{sec:perfect}, then

where  is the event that at least one node in the buckets
encountered during a search is a ghost. Let  be the event that the
trie is good, which has probability  by Lemma~\ref{lem:pleasant}.
One can check that

Again, Theorem~\ref{thm:random:target} follows if


\subsection{Filling the Buckets with Replacement}

The coupling argument used in Section~\ref{sec:replacement} to deal the problem
that buckets are filled by sampling without replacement can be adapted for an
almost perfect trie.  Let  \deleted{is }be the probability that  items sampled
without replacement from a set of size  have
conflicts. Observe that, for  large enough,

Thus it follows from the union bound that



Let the search time of sampling without replacement be .  Let  and
 be coupled in the same way as they were in Section~\ref{sec:replacement}.
Let  be the event that during the search an unmarked bucket is encountered.
Since

one can check that

So once again, Theorem~\ref{thm:random:target} follows if


\subsection{Analyzing  Using a Sum of I.I.D.\ Random Variables}

Consider two partitions of a line segment  of length .  From left to
right, cut  into  consecutive intervals , with
, where  denotes the length of .  Again, from left to right,
cut  into infinite many consecutive intervals , with
. 

Observe that for ,  and  do not completely match since
 is wider than .  However, since , for , the distance between the right endpoints of  and  is at most
. Therefore, the total length of unmatched regions, which are are
called \emph{death zones}, is .

Let  and  be the same as in
Section~\ref{sec:geo1}.  A coupling between them can constructed as follows: pick
one point  uniformly at random from the entire . If  falls in
interval , let . If  falls in interval , let
.  Note that .  Also note that since

 is geometric  minus one, as desired.

Assume that . Pick  points from the line
segment starting from  to the right endpoint of .  Let 
such that the rightmost one of the  points falls into . Since

 is again the maximum of  i.i.d.\ geometric .

If not all the  points are in the range of , keep
picking more points until  of them are within this region.  Let
 such that the rightmost of the these  points falls into
. Chosen in this way,  has the same distribution as how
much progress one makes at step  of the search.  Therefore 

It follows from Lemma~\ref{lem:convergence} that if

then  as .

Let  be the event that at some step of the previous coupling, at least one
of the first  chosen points falls into death zones. Note that . Therefore,

So the proof of Theorem~\ref{thm:random:target} \replaced{when  is}{for  being} an arbitrary
integer is complete.

\section{Conclusions}

In a Kademlia system, \replaced{one often searches for a random \id{}}{which \id\ needs to be searched for is random}.  Although
 is the searching time for a fixed \id, Theorem~\ref{thm:random:target}
still holds if the target  is chosen uniformly at random from .

If  with , there is no essential difference between
sampling the  \ids\ with or without replacement from  as the
probability of a collision in sampling with replacement is . This is the
well known \emph{birthday problem}. Since in practice, a Kademlia system hands
out a new \id\ without checking its uniqueness, it is \deleted{is }wise to have 
since then a randomly generated \id\ \replaced{clashes}{does not clash} with any existing \id\
\deleted{except }with \deleted{a }very small probability.

\added{Recall that . }
\added{Since the terms in the sum decrease in , } can be bounded:

Here  denotes the natural logarithm of , and  is the -th harmonic number. Since ,

Thus, . \added{Since  when }, an increase in
storage by a factor of  \deleted{thus }results in a modest decrease in searching time by
a factor of .

In~\citep{Cai2013}, it has been proved that if  for fixed
, then

Thus Theorem~\ref{thm:random:target} implies that the above upper bound is not
far from tight when  is large. Table~\ref{table:mu:k} lists the numeric
values of  and  for .

\begin{table}
    \label{table:mu:k}
    \setlength{\tabcolsep}{12pt}
    \caption{Numeric values of  and .}
    \centering {
        \begin{tabular}{ l l l }
        \hline\noalign{\smallskip}
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{}         \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
        1  & 0.5000000000 & 0.6931471806      \\
        2  & 0.3750000000 & 0.4620981204      \\
        3  & 0.3181818182 & 0.3780802804      \\
        4  & 0.2853260870 & 0.3327106467      \\
        5  & 0.2635627530 & 0.3035681083      \\
        6  & 0.2478426396 & 0.2829172166      \\
        7  & 0.2358018447 & 0.2673294911      \\
        8  & 0.2261891923 & 0.2550344423      \\
        9  & 0.2182781689 & 0.2450176596      \\
        10 & 0.2116151616 & 0.2366523364      \\
        \hline
        \end{tabular}
    }
\end{table}

If , then  in probability as
.  The proof of Theorem~\ref{thm:random:target} is for fixed 
only, but one can verify that\added{ only minor changes are needed to make it work for} such modest
increase in  as a function of \deleted{ does not alter the results}.\added{ More specifically, to
make the coupling with searching in a perfect trie work, we only need to redefine  and
. And Lemma \ref{lem:convergence} needs to use a version of the weak law of
large numbers \cite[thm. 2.2.4]{Durrett2010probability} instead of the strong law of large numbers to
deal with the fact that  is not a constant anymore.}

\deleted{A more direct exercise shows that for ,  in
probability. }\added{If , we can show that
 in probability.  Note that here only an upper bound of  is needed.
Assuming that the \id{} trie is good, it can be proved that in each search step the length of the common
prefix of the current node and the target node increases by at least  with high
probability, where  is a constant depending on . Thus after at most  steps, the
current node and the target node are both in a subtree of size at most . Then the search
terminates after one more step.}

\bibliography{citation}{}
\bibliographystyle{abbrplainnat}

\end{document}
