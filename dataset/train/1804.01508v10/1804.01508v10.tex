\documentclass[11pt,a4paper]{article}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.0cm,bmargin=2.0cm,lmargin=2.6cm,rmargin=2.6cm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{stackengine}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode} 
\usepackage{makecell}
\usepackage{color}

\newcommand{\True}{\mbox{1}}
\newcommand{\False}{\mbox{0}}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}{Lemma}
\newtheorem{definition}{Definition}

\newcommand{\Input}{{\hspace*{\algorithmicindent} \textbf{Input }}}

\newcommand{\Output}{{\hspace*{\algorithmicindent} \textbf{Output }}}

\newcommand{\Notation}{{\hspace*{\algorithmicindent} \textbf{Notation }}}

\algnewcommand{\LineComment}[1]{\State  #1}


\newcommand{\Break}{\State \textbf{break} }

\title{The Tsetlin Machine -- A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with\\Propositional Logic\footnote{Source code and datasets from this paper can be found at {\tt https://github.com/cair/TsetlinMachine.} and {\tt https://github.com/cair/fast-tsetlin-machine-with-mnist-demo}}}
\author{Ole-Christoffer Granmo\thanks{Author's status: {\it Professor}. The author can be contacted at: Centre for Artificial Intelligence Research ({\tt https://cair.uia.no}), University of Agder, Grimstad, Norway.  E-mail: {\tt ole.granmo@uia.no}}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks. Unknown to many, there exists an arguably even simpler and more versatile learning mechanism, namely, the Tsetlin Automaton. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments through increment and decrement operations. In this paper, we introduce the \emph{Tsetlin Machine}, which solves complex pattern recognition problems with easy-to-interpret \emph{propositional formulas}, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel \emph{game}. Our theoretical analysis establishes that the Nash equilibria of the game align with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. We argue that the Tsetlin Machine finds the propositional formula that provides optimal accuracy, with probability arbitrarily close to unity. In five benchmarks, the Tsetlin Machine provides competitive accuracy compared with SVMs, Decision Trees, Random Forests, Naive Bayes Classifier, Logistic Regression, and Neural Networks. The Tsetlin Machine further has an inherent computational advantage since both inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on bit manipulation. The combination of accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains. Being the first of its kind, we believe the Tsetlin Machine will kick-start new paths of research, with a potentially significant impact on the AI field and the applications of AI.\\\\
{\bf Keywords:} Bandit Problem, Game Theory, Interpretable Pattern Recognition, Propositional Logic, Tsetlin Automata Games, Online Learning 
\end{abstract}

\section{Introduction}

Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks \cite{LeCun2015}.
Highly successful, deep neural networks often require huge amounts of training data and extensive computational resources. Unknown to many, there exists an arguably even more fundamental and versatile learning mechanism than the artificial neuron, namely, the Tsetlin Automaton, developed by M.L. Tsetlin in the Soviet Union in the early 1960s \cite{Tsetlin1961}.

In this paper, we address a long-standing challenge in the field of Finite State Learning Automata \cite{Narendra1989}, referred to as the \emph{vanishing signal-to-noise ratio problem}. This problem has hindered successful use of Tsetlin Automata in large-scale and complex pattern recognition, constraining such solutions to small-scale pattern recognition tasks.

\subsection{The Tsetlin Automaton}
Tsetlin Automata have been used to model biological systems, and have attracted considerable interest because they can learn the optimal action when operating in unknown stochastic
environments \cite{Tsetlin1961,Narendra1989}. Furthermore, they combine rapid and accurate convergence with low computational complexity.

In all brevity, the Tsetlin Automaton is one of the pioneering solutions to the well-known multi-armed bandit problem \cite{Robbins1952,Gittins1979}. It performs actions sequentially in an environment, and each action triggers either a reward or a penalty. Assuming two bantit arms, action , , is rewarded with probability , otherwise it is penalized (i.e., with probability ). The reward  probabilities are unknown to the automaton and may even change over time. Under such challenging conditions, the goal is to identify the action with the highest reward probability using as few attempts as possible.

\begin{figure}[!ht]
\centering
\includegraphics[width=5.0in]{Tsetlin_Automaton_Generic.pdf}
\caption{A Tsetlin Automaton for two-action environments.}
\label{figure:tsetlin_automaton}
\end{figure}
The mechanism driving the Tsetlin Automaton is surprisingly simple. Informally, as illustrated in Figure \ref{figure:tsetlin_automaton}, a Tsetlin Automaton is simply a fixed finite-state automaton \cite{Carroll1989} with an unusual interpretation:
\begin{itemize}
\item The current state of the automaton decides which action to perform. The automaton in the figure has  states. Action 1 is performed in the states with index  to , while Action 2 is performed in states with index  to .
\item The state transitions of the automaton govern learning. One set of state transitions is activated on reward (solid lines), and one set of state transitions is activated on penalty (dotted lines). As seen, rewards and penalties trigger specific transitions from one state to another, designed to reinforce successful actions (those eliciting rewards). 
\end{itemize}
Formally, a Two-Action Tsetlin Automaton can be defined as a quintuple \cite{Narendra1989}:

 is the set of internal states.  is the set of automaton actions.  is the set of inputs that can be given to the automaton.
An output function, , determines the next action performed by the automaton, 
given the current automaton state : 

Finally, a transition function, , determines the new automaton state from: (1) the current automaton state, ,  and (2) the response, , of the environment to the action performed by the automaton: 


Implementation-wise, a Tsetlin Automaton simply maintains an integer (the state index), and learning is performed through increment and decrement operations, according to the transitions specified by  (and depicted in Figure \ref{figure:tsetlin_automaton}). \emph{The Tsetlin Automaton is thus extremely simple computationally, with a very small memory footprint.}

\subsection{State-of-the-art in the Field of Finite State Learning Automata}
The simple Tsetlin Automaton approach has formed the core for more advanced finite state learning automata designs that solve a wide range of problems. This includes resource allocation \cite{Granmo2010g}, decentralized control \cite{Tung1996}, knapsack problems \cite{Granmo2007d}, searching on the line \cite{Oommen1997,Yazidi2014}, meta-learning \cite{Oommen2008}, the satisfiability problem \cite{Granmo2007c,Bouhmala2010}, graph colouring \cite{Bouhmala2010}, preference learning \cite{Yazidi2012b}, frequent itemset mining \cite{Haugland2014}, adaptive sampling \cite{Granmo2010}, spatio-temporal event detection \cite{Yazidi2013}, equi-partitioning \cite{Oommen1988}, streaming sampling for social activity networks \cite{Ghavipour2018}, routing bandwidth-guaranteed paths \cite{Oommen2007a}, faulty dichotomous search \cite{Yazidi2018}, learning in deceptive environments \cite{Zhang2016a}, as well as routing in telecommunication networks \cite{Oommen2007c}. The unique strength of all of these finite state learning automata solutions is that they provide state-of-the-art performance when problem properties are unknown and stochastic, while the problem must be solved as quickly as possible through trial and error.

Note that there exists another family of learning automata, referred to as \emph{variable structure learning automata} (the interested reader is referred to \cite{Thathachar2004}). Although still simple, these are significantly more complex than the Tsetlin Automaton because they need to maintain an action probability vector for sampling actions. Their success in pattern recognition has been limited to small scale problems, again being restricted by constrained pattern representation capability (linearly separable classes and simple decision trees) \cite{Thathachar2004,Barto1985,Narendra1989}.

\subsection{The Vanishing Signal-to-Noise Ratio Problem}

The ability to handle stochastic and unknown environments for a wide range of problems, combined with its computational simplicity and small memory footprint, make the Tsetlin Automaton an attractive building block for complex machine learning tasks. However, the success of the Tsetlin Automaton has been hindered by a   particularly adverse challenge, explored by Kleinrock and Tung in 1996 \cite{Tung1996}.

Complex problem solving requires teams of interacting Tsetlin Automata, and unfortunately, each team member introduces noise. This is due to the inherently decentralized and stochastic nature of Tsetlin Automata based decision-making. The automata independently decide upon their actions, directly based on the feedback from the environment. This is on one hand a strength because it allows problems to be solved in a decentralized manner. On the other hand, as the number of Tsetlin Automata grows, the level of noise increases. We refer to this effect as the \emph{vanishing signal-to-noise ratio problem}. This vanishing signal-to-noise ratio demands in the end an infinite number of states per Tsetlin Automaton, which in turn, leads to impractically slow convergence \cite{Narendra1989,Tung1996}.

\begin{table}[!bh]
    \centering
    \begin{tabular}{cccccccc}
            0 0 * 1 * 0 0 0\\
            * 0 * 1 * 0 0 0\\
            0 * * 1 * * * 0\\
            0 * * * * 0 0 *\\
            0 0 0 * * 0 0 0\\
            0 * 0 * * * 0 0\\
            0 0 * 1 * * * 0\\
            0 0 0 * 1 * * *\\
    \end{tabular}
    \caption{A bit pattern produced by the Tsetlin Machine for handwritten digits '1'. The '*' symbol can either take the value '0' or '1'. The remaining bit values require strict matching. The pattern is relatively easy to interpret for humans compared to, e.g., a neural network. It is also efficient to evaluate for computers. Despite this simplicity, the Tsetlin Machine produces bit patterns that deliver state-of-the-art pattern recognition accuracy for several datasets, demonstrated in Section \ref{sec:empirical_results}.}
    \label{tab:bit_pattern}
\end{table}

\subsection{Interpretable Pattern Recognition}

While this paper focuses on extending the field of Finite State Learning Automata, we acknowledge the extensive work on rule-based interpretable pattern recognition from other fields of machine learning. Decision tree learning \cite{Quinlan1986}, for instance, is one of the most common approaches to interpretable pattern recognition. In all brevity, learning of decision trees is based on greedy growing of decision rules, organized as a tree, by extending the leafs of the tree, one input variable at a time.  Recently, however, it has turned out that taking a global perspective on the production of decision rules has advantages over greedy local strategies. Heuristic approaches, such as alternating minimization, Block Coordinated Monte Carlo, and associative rule mining with randomized search, are used to learn rule sets, jointly optimizing \emph{rule sparsity} and classification accuracy \cite{GuolongSu2016,Wang2017}. These techniques typically require offline batch based learning and are mainly addressing smaller scale pattern recognition problems. 

In the above perspective, we here propose a completely new approach to global optimization of decision rule sets, founded on the Tsetlin Automaton and the bandit problem. We demonstrate further that decision rules can be learned on-line, under particularly noisy conditions. To establish a common reference point towards related work in interpretable pattern recognition, we include results on decision trees in our experiments on MNIST.

\subsection{Paper Contributions}
In this paper, we attack the limited pattern expression capability and vanishing signal-to-noise ratio of learning automata based pattern recognition, introducing the \emph{Tsetlin Machine}. The contributions of the paper can be summarized as follows:
\begin{itemize}
\item We introduce the Tsetlin Machine, which  solves complex pattern recognition problems with \emph{propositional formulas}, composed by a collective of Tsetlin Automata.
\item We eliminate the longstanding vanishing signal-to-noise ratio problem with a unique decentralized learning scheme based on game theory \cite{VonNeumann1947,Tsetlin1961}. The game we have designed allows thousands of Tsetlin Automata to successfully cooperate.
\item The game orchestrated by the Tsetlin Machine is based on resource allocation principles \cite{Granmo2007a}, in inter-play with frequent itemset mining  \cite{Haugland2014}. By allocating sparse pattern representation resources, according to the frequency of the patterns, the Tsetlin Machine is able to capture intricate unlabelled sub-patterns, for instance addressing the so-called Noisy XOR-problem.
\item Our theoretical analysis establishes that the Nash equilibria of the game are aligned with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones.
\item We further argue that the Tsetlin Machine finds a propositional formula that provides optimal pattern recognition accuracy, with probability arbitrarily close to unity.
\item The propositional formulas are represented as bit patterns. These bit patterns are relatively easy to interpret, compared to e.g. a neural network (see Table \ref{tab:bit_pattern} for an example bit pattern). This facilitates human quality assurance and scrutiny, which for instance can be important in safety-critical domains such as medicine.
\item The Tsetlin Machine is particularly suited for digital computers, being directly based on bit manipulation with AND-, OR-, and NOT operators. Both input, hidden patterns, and output are represented directly as bits, while recognition and learning rely on manipulating those bits.
\item In our empirical evaluation on five datasets, the Tsetlin Machine provides competitive performance in comparison with Multilayer Perceptron Networks, Support Vector Machines, Decision Trees, Random Forests, the Naive Bayes Classifier, and Logistic Regression.
\item In our experiments, it further turns out that the Tsetlin Machine requires less data than neural networks, outperforming even the Naive Bayes Classifier in data sparse environments.
\item Overfitting is inherently combated by leveraging frequent itemset mining \cite{Haugland2014}. Even while accuracy on the training data approaches \%, mean accuracy on the test data continues to increase as well. This is quite different from the behaviour of back-propagation in neural networks, where accuracy on test data starts to drop at some point, without proper regularization mechanisms.
\item We demonstrate how the Tsetlin Machine can be used as a building block to create more advanced architectures.
\end{itemize}
We believe that the combination of accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains. Being the first of its kind, we further believe it will kick-start completely new paths of research, with a potentially significant impact on the field of AI and the applications of AI.

\subsection{Paper Organization}

The paper is organized as follows. In Section \ref{sec:pattern_recognition_problem}, we define the exact nature of the pattern recognition problem we are going to solve, also introducing the crucial concept of sub-patterns.

Then, in Section \ref{sec:tsetlin_machine}, we cover the Tsetlin Machine in detail. We first present the propositional logic based pattern representation framework, before we introduce the Tsetlin Automata teams that write \emph{conjunctive clauses} in propositional logic. These Tsetlin Automata teams are in turn organized to recognize complex patterns. We conclude the section by presenting the \emph{Tsetlin Machine game} that we use to coordinate thousands of Tsetlin Automata, eliminating the vanishing signal-to-noise ratio problem.

In Section \ref{sec:theoretical_results}, we analyze pertinent properties of the Tsetlin Machine formally, and establish that the Nash equilibria of the game are aligned with the propositional formulas that solve the pattern recognition problem at hand. This allows the Tsetlin Machine as a whole to robustly and accurately uncover complex patterns with propositional logic.

In our empirical evaluation in Section \ref{sec:empirical_results}, we evaluate the performance of the Tsetlin Machine on five datasets: Flower categorization, digit recognition, board game planning, the Noisy XOR Problem with Non-informative Features, as well as the MNIST dataset.

The Tsetlin Machine has been designed to act as a building block in more advanced architectures, and in Section \ref{sec:building_block} we demonstrate how four distinct architectures can be built by interconnecting multiple Tsetlin Machines.

As the first step in a new research direction, the Tsetlin Machine also opens up a range of new research questions. In Section \ref{sec:conclusion}, we summarize our main findings and provide pointers to some of the open research problems ahead.

\section{The Pattern Recognition Problem}
\label{sec:pattern_recognition_problem}

We here define the pattern recognition problem to be solved by the Tsetlin Machine, starting with the input to the system. The input to the Tsetlin Machine is an observation vector of  propositional variables, ], with , . From this input vector, the Tsetlin Machine is to produce an output vector  of  propositional variables, , .\footnote{Note that we have decided to use the binary representation / to refer to the truth values \emph{False}/\emph{True}. These can be used interchangeably throughout the paper.}

Given an input , we assume that an independent underlying stochastic process of arbitrary complexity randomly produces either  or , for each output, . To deal with the stochastic nature of these processes, we take a probabilistic approach. In all brevity, all uncertainty is completely captured by the probability , that is, the probability that  takes the value  given the input . Being binary, the probability that  takes the value  follows: . Under these conditions, the optimal decision is to assign  the value, , with the largest posterior probability \cite{Duda2001}: .

\begin{figure}[!th]
\centering
\includegraphics[width=3.0in]{The_Pattern_Recognition_Problem.pdf}
\caption{A partitioning of the input space according to the posterior probability of the output variable , highlighting distinct sub-patterns in the input space, . Sub-patterns most likely belonging to output  can be found on the right side, while sub-patterns most likely belonging to  on the left.}
\label{figure:classification_problem}
\end{figure}

Now consider the complete set of possible inputs, . Each input  occurs with probability , and thus the joint input-output distribution becomes . 

As illustrated in Figure \ref{figure:classification_problem}, the input space  can be partitioned in two parts,  and . For observations in  it is optimal to assign  the value , while for partition  it is optimal to assign  the value .

We now come to the crucial concept of unlabelled sub-patterns. As illustrated in the figure,  sub-divides further into  sub-parts, forming distinct sub-patterns,  (the same goes for ). Together, these sub-patterns span the whole input space, apart from a minimal level of outlier patterns that occur with probability, , close to zero. That is, , for a minimal  (i.e., the outliers remaining after a sub-division of the input space into  parts).

Note that we do not have direct access to the above sub-patterns during pattern learning and recognition. Rather, we only observe samples 
from the joint input-output distribution . Which sub-pattern  is sampled from is unavailable to us. However, what we know, by definition, is that each sub-pattern  occurs with probability .

\emph{The challenging task we are going to solve is to learn all the sub-patterns merely by observing a limited sample from the joint input-output probability distribution , and by doing so, provide optimal pattern recognition accuracy}.

\section{The Tsetlin Machine}
\label{sec:tsetlin_machine}

We now present the core concepts of the Tsetlin Machine in detail. We first present the propositional logic based pattern representation framework, before we introduce the Tsetlin Automata teams that write \emph{conjunctive clauses} in propositional logic. These Tsetlin Automata teams are then organized to recognize complex sub-patterns. We conclude the section by presenting the Tsetlin Machine game that we use to coordinate thousands of Tsetlin Automata.

\subsection{Expressing Patterns with Propositional Formulas} 

The accuracy of a machine learning technique is bounded by its pattern representation capability. The Naive Bayes Classifier, for instance, assumes that input variables are independent given the output category \cite{Mitchell1997a}. When critical patterns cannot be fully represented by the machine learning technique, accuracy suffers. Unfortunately, compared to the representation capability of the underlying language of digital computers, namely, Boolean algebra\footnote{We found the Tsetlin Machine on propositional logic, which can be mapped to Boolean algebra, and vice versa.}, most machine learning techniques appear rather limited, with neural networks being one of the exceptions. Indeed, let  refer to an arbitrary propositional formula. With  input variables, , there are no less than  unique formula . Perhaps only a single one of them will provide optimal pattern recognition accuracy for the task at hand.

{\bf The Satisfiability Problem (SAT).} The representation power of propositional logic is perhaps best seen in light of the Satisfiabiliy (SAT) problem, which also can be solved using a team of Tsetlin Automata \cite{Bouhmala2010a}. The SAT problem is known to be NP-complete \cite{Cook1971} and  plays a central role in a number of applications in the fields of VLSI Computer-Aided design, Computing Theory, Theorem Proving, and Artificial Intelligence. A SAT problem is defined in so-called \emph{conjunctive normal form}. To facilitate Tsetlin Automata based learning, we will instead represent patterns using \emph{disjunctive normal form}. 

{\bf Patterns in Disjunctive Normal Form.} Briefly stated, we represent the relation between an input, , and the output, , using a propositional formula  in disjunctive normal form:

The formula consists of a disjunction of  conjunctive clauses, . Each conjunctive clause in turn, represents a specific sub-pattern governing the output .

{\bf Sub-Patterns and Conjunctive Clauses.} Each clause  in the propositional formula  has the form:

That is, the clause is a conjunction of \emph{literals}, where a literal is a propositional variable, , or its negation .
Here,  and  are non-overlapping subsets of the input variable indexes,  . The subsets decide which of the input variables take part in the clause, and whether they are negated or not. The input variables from  are included as is, while the input variables from  are negated.

For example, the propositional formula  consists of two conjunctive clauses, and four literals, , , , and . The formula evaluates to \True\ if
\begin{itemize}
    \item  and , or if
    \item  and .
\end{itemize}
All other truth value assignments evaluate to , and thus the formula captures the renowned XOR-relation.
\begin{definition}[The Problem of Pattern Learning with Propositional Logic]\label{def:classification_problem}
A set  of independent samples, , from the joint input-output probability distribution  is provided. In the Problem of Pattern Learning with Propositional Logic, one must determine the propositional formula  that evaluates to  iff  and to  iff , merely based on the samples in . 
\end{definition}
The above problem decomposes into identifying the conjunctive clauses  whose disjunction evaluates to  iff  (see Section \ref{sec:pattern_recognition_problem} for the definition of ).

\subsection{The Tsetlin Automata Team for Composing Clauses}

At the core of the Tsetlin Machine we find the conjunctive clauses, , from Eqn. \ref{eqn:clause}. For each clause , we form a team of  Tsetlin Automata, two Tsetlin Automata per input variable . Figure \ref{figure:clause_formation} captures the role of each of these Tsetlin Automata, and how they interact to form the clause .

\begin{figure}[!th]
\centering
\includegraphics[width=4.0in]{Clause_Formation.pdf}
\caption{The Tsetlin Automata team for composing a clause.}
\label{figure:clause_formation}
\end{figure}

As seen,  input variables, , are fed to the clause. The critical task of the Tsetlin Automata team is to decide which input variables to include in  and which input variables to include in . If a literal is excluded by its associated Tsetlin Automaton, it does not take part in the conjunction.  That is, Tsetlin Automaton  is responsible for deciding whether to "Include" or "Exclude" input variable , while another Tsetlin Automaton, , decides whether to "Include" or "Exclude" . The input variable  can thus take part in the clause  as is, take part in negated form, , or not take part at all.

As illustrated to the right in the figure, the clause is formed after each Tsetlin Automaton has made its decision (to include or exclude its associated literal). After these decisions have been made, resulting in a selection of literals, , the output of the clause can be calculated: .

\subsection{The Basic Tsetlin Machine Architecture}
We are now ready to build the complete Tsetlin Machine. We do this by assigning  clauses,  , to each output . The number of clauses  per output  is a meta-parameter that is decided by the number of sub-patterns associated with each . If the latter is unknown, an appropriate  can be found using a grid search, corresponding to selecting the number of hidden nodes in a neural network layer.

\begin{figure}[!t]
\centering
\includegraphics[width=5.0in]{Overall_Architecture_Basic.pdf}
\caption{The basic Tsetlin Machine architecture.}
\label{figure:architecture_basic}
\end{figure}

With the clause structure in place, we assign one Tsetlin Automata team,  , to each clause . As shown in Figure \ref{figure:architecture_basic}, the architecture consists of  conjunctive clauses, each formed by an independent Tsetlin Automata team. Each Tsetlin Automata team, , thus governs the selection of which literals to include in its respective clause, . The collective of teams accordingly addresses the whole pattern recognition problem. As indicated, the clauses corresponds to the hidden layer of a neural network, although instead of having neurons with nonlinear activation functions, we have formulas in propositional logic that evaluates to  or . That is, a single clause corresponds to a single neuron, however, can be represented more compactly in bit form.  

The basic Tsetlin Machine architecture can, accordingly, express any formula in propositional logic, constrained by the number of clauses. Therefore, this basic architecture is interesting by itself. However, real-world problems do not necessarily fit the pattern recognition problem laid out in Section \ref{sec:pattern_recognition_problem} exactly. This raises the need for additional robustness.

\subsection{The Extended Tsetlin Machine Architecture}

In order to render the architecture more robust towards noise and intricate real-world data, we now replace the OR operator with a summation operator and a threshold function. It turns out that this additional robustness also supports more compact representation of patterns.

\begin{figure}[!t]
\centering
\includegraphics[width=5.0in]{Overall_Architecture_Summation.pdf}
\caption{The extended Tsetlin Machine architecture, introducing clause polarity, a summation operator collecting "votes", and a threshold function arbitrating the final output.}
\label{figure:architecture_summation}
\end{figure}

Figure \ref{figure:architecture_summation} depicts the resulting extended architecture. Again, the architecture consists of a number of conjunctive clauses, each associated with a dedicated Tsetlin Automata team. However, instead of simply taking part in an OR relation, each clause, , , is now given a fixed polarity. For simplicity, we assign positive polarity to clauses with an odd index , while clauses with an even index are assigned negative polarity. In the figure, polarity is indicated with a '+' or '-'  sign, attached to the output of each clause.

Clauses with positive polarity contribute to a final output of , while clauses with a negative polarity contribute towards a final output of . The contributions can be seen as votes, with each clause either casting a vote, , or declining to vote, . A positive vote means that the corresponding clause has recognized a sub-pattern associated with output , while a negative vote means that the corresponding clause has recognized a sub-pattern associated with the opposite output, .

After the clauses have produced their output, a summation operator, , associated with the output , sums the votes it receives from the clauses, . Clauses with positive polarity contribute positively while those with negative polarity contribute negatively. Overall, the purpose is to reach a balanced output decision, weighting positive evidence against negative evidence:

The final output, , is decided by a threshold function

that outputs  if the outcome of the summation is larger than or equal to zero. Otherwise, it outputs .

The final output can thus be calculated directly from input  simply by summing the signed output of the  conjunctive clauses, followed by activating the threshold function:


\emph{The crucial remaining issue, then, is how to learn the conjunctive clauses from data, to obtain optimal pattern recognition accuracy. We attack this problem next.}

\subsection{The Tsetlin Machine Game for Learning Conjunctive Clauses}
\label{sec:tsetlin_machine_game}

We here introduce a novel game theoretic learning mechanism that guides the Tsetlin Automata stochastically towards solving the pattern recognition problem from Definition \ref{def:classification_problem}. The game is designed to deal with the problem of vanishing signal-to-noise ratio for large Tsetlin Automata teams that contain thousands of Tsetlin Automata.

\subsubsection{Tsetlin Automata Games}

Recall that a Tsetlin Automaton can be formally represented as a quintuple .
A game of Tsetlin Automata involves  Tsetlin Automata and is played over several rounds \cite{Narendra1989}. In each round of the game, every Tsetlin Automaton independently decide upon an action from . Thus, with two actions available to each automaton, there are  unique action configurations.

After the Tsetlin Automata have decided upon an action, the round ends with the Tsetlin Automata being penalized/rewarded. That is, they are individually rewarded/penalized based on the configuration of actions selected. To fully specify the game, we thus need to specify one reward probability for each Tsetlin Automaton, for each unique configuration of actions.

We refer to the above reward probabilities as the \emph{payoff matrix} of the game. As an example, with two action outcomes, , we need  reward probabilities to fully specify the payoff matrix for a game of  Tsetlin Automata players.

The complexity of the Tsetlin Machine game is immense, because the decisions of every single Tsetlin Automaton jointly decide the behaviour of the Tsetlin Machine as a whole. Indeed, under the right conditions, a single Tsetlin Automaton has the power to completely disrupt a clause by introducing a contradiction. The payoffs of the game must therefore be designed carefully, so that the Tsetlin Automata always are guided towards the optimal propositional formula  that solves the pattern recognition problem at hand. To complicate further, an explicit enumeration of the payoffs is impractical due to the potentially tremendous size of the game payoff matrix.

\subsubsection{Design of the Payoff Matrix}
We specify the payoffs associated with each cell of the game implicitly, so that they can be calculated lazily, on demand. In brief, we design the payoff matrix for the game based on the notion of:
\begin{itemize}
    \item{\bf True positive output.} We define \emph{true positive output} as correctly providing output .
    \item{\bf False negative output.} We define \emph{false negative output} as incorrectly providing the output  when the output should have been .
    \item{\bf False positive output.} We define \emph{false positive output} as incorrectly providing the output  when the output should have been .
    \item{\bf True negative output.} We define \emph{true negative output} as correctly providing the output .
\end{itemize}
By progressively reducing false negative and false positive output, and reinforcing true positive and true negative output, we intend to guide the Tsetlin Automata towards optimal pattern recognition accuracy. This guiding is based on what we will refer to as Type I and Type II Feedback. In the following, we will 
introduce these two types of feedback, considering clauses with positive polarity (for clauses with negative polarity, the two types of feedback swap roles).

\subsubsection{Type I Feedback -- Combating False Negative Output}\label{sec:type_i_feedback}

Type I Feedback is decided by two factors, connecting the players of the game together:
\begin{itemize}
    \item The choices of the Tsetlin Automata team  as a whole, summarized by the output of the clause  (the truth value of the clause).
    \item The truth value of the literal / assigned to the Tsetlin Automaton /.
\end{itemize}
Table \ref{table:type_i_feedback} contains the probabilities that we use to generate  Type I Feedback. For instance, assume that:
\begin{enumerate}
    \item Clause  evaluates to ,
    \item Literal  is , and
    \item Automaton  has selected the action \emph{Include Literal}.
\end{enumerate}
By examining the corresponding cell in Table \ref{table:type_i_feedback}, we observe that the probability of receiving a reward, , is , the probability of inaction, , is , while the probability of receiving a penalty, , is zero.

Note that the Inaction feedback is a novel extension to the Tsetlin Automaton, which traditionally receives either a Reward or a Penalty. When receiving the Inaction feedback, the Tsetlin Automaton is simply unaffected.

\begin{table}[bh!]
\centering
\begin{tabular}{c|ccccc}
\multicolumn{2}{r|}{{\it Truth Value of Target Clause}  }&\multicolumn{2}{c}{\True}&\multicolumn{2}{c}{\False}\\  
\multicolumn{2}{r|}{{\it Truth Value of Target Literal} /}&{\True}&{\False}&{\True}&{\False}\\
 \hline
 \hline
    \multirow{3}{*}{\bf Include Literal (/)}&\multicolumn{1}{c|}{}&&NA&&\\
    &\multicolumn{1}{c|}{}&&NA&&\\
  &\multicolumn{1}{c|}{}&&NA&&\\
  \hline
  \multirow{3}{*}{\bf Exclude Literal (/)}&\multicolumn{1}{c|}{}&&& &\\
  &\multicolumn{1}{c|}{}&&& &\\
  &\multicolumn{1}{c|}{}&&&&\\
  \hline
\end{tabular}
\caption{Type I Feedback --- Feedback from the perspective of a single Tsetlin Automaton deciding to either \emph{Include} or \emph{Exclude} a given literal / in the clause . Type I Feedback is triggered to increase the number of clauses that correctly evaluates to  for a given input .}
\label{table:type_i_feedback}
\end{table}

{\bf Boosting of True Positive Feedback (Column 1 in Table \ref{table:type_i_feedback}).} The feedback probabilities in Table \ref{table:type_i_feedback} have been selected based on mathematical derivations (see Section \ref{sec:theoretical_results}). For certain real-life data sets, however, it turns out that boosting rewarding of \emph{Include Literal} actions can be beneficial. That is, pattern recognition accuracy can be enhanced by boosting rewarding of these actions when they produce true positive outcomes. Penalizing of \emph{Exclude Literal} actions is then adjusted accordingly.  In all brevity, we boost rewarding in this manner by replacing  with  and  with  in Column 1 of Table \ref{table:type_i_feedback}.

{\bf Brief analysis of the Type I Feedback.} Notice how the reward probabilities are designed to "tighten" clauses up to a certain point. That is, the probability of receiving rewards when selecting \emph{Include Literal} is larger than the probability of receiving rewards when selecting \emph{Exclude Literal}. The ratio between the two probabilities is controlled by the parameter . In this manner,  effectively decides how "fine grained" patterns the clauses are going to capture. The larger the value of , the more the Tsetlin Automata team is stimulated to include literals in the clause. The only countering force is the input examples, , that do not match the clause. Obviously, the probability of encountering such examples grows as  is increased (the clause is "tightened"). When these forces are in balance, we have a Nash equilibrium as discussed further in the next section. 

 The above mechanism is a critical part of the Tsetlin Machine, allowing learning of any sub-pattern , no matter how infrequent, as decided by . This novel mechanism is studied both theoretically and empirically in the two following sections. As a rule of thumb, a large  leads to more "fine grained" clauses, that is, clauses with more literals, while a small  produces "coarser" clauses, with fewer literals included.

\subsubsection{Type II Feedback -- Combating False Positive Output }

Table \ref{table:type_ii_feedback} covers Type II Feedback, that is, feedback that combats false positive output. This type of feedback is triggered when the output is  when it should have been . Then we want to achieve the opposite of what we seek with Feedback Type I. In all brevity, we now seek to modify clauses that evaluate to , so that they instead evaluate to . To achieve this, for each offending clause, we identify the Tsetlin Automata that have selected the \emph{Exclude Literal} action and whose corresponding literal evaluates to . By merely switching from \emph{Exclude Literal} to \emph{Include Literal} for a single one of these, our goal is achieved. That is, since we are dealing with conjunctive clauses, simply including a single literal that evaluates to  makes the whole conjunction also evaluate to . In this manner, we guide the  Tsetlin Automata towards eliminating false positive output.
\begin{table}[bh!]
\centering
\begin{tabular}{c|ccccc}
\multicolumn{2}{r|}{\it Truth Value of Target Clause }&\multicolumn{2}{c}{\True}&\multicolumn{2}{c}{\False}\\  
\multicolumn{2}{r|}{\it Truth Value of Target Literal /}&{\True}&{\False}&{\True}&{\False}\\
 \hline
 \hline
    \multirow{3}{*}{\bf Include Literal (/)}&\multicolumn{1}{c|}{}&&&&\\
    &\multicolumn{1}{c|}{}&&&&\\
  &\multicolumn{1}{c|}{}&&&&\\
  \hline
  \multirow{3}{*}{\bf Exclude Literal (/)}&\multicolumn{1}{c|}{}&&&&\\
  &\multicolumn{1}{c|}{}&&& &\\
  &\multicolumn{1}{c|}{}&&&&\\
  \hline
\end{tabular}
\caption{Type II Feedback --- Feedback from the perspective of a single Tsetlin Automaton deciding to either \emph{Include} or \emph{Exclude} a given literal / in the clause . Type II Feedback is triggered to decrease the number of clauses that incorrectly evaluates to  for a given input .}
\label{table:type_ii_feedback}
\end{table}

Together, Type I Feedback and Type II Feedback interact to reduce the output error rate to a minimal level.

\subsubsection{The Tsetlin Machine Algorithm}

\begin{algorithm}
\caption{The Tsetlin Machine}
\label{alg:tsetlin_machine}

\Input{Training data , Number of clauses , Output index , Number of inputs , Precision , Threshold }

\Output{Completely trained conjunctive clauses  for } 

\begin{algorithmic} [1]
\Function{TrainTsetlinMachine}{}

\State  ProduceTsetlinMachine(m,o)
\Comment{\parbox[t]{.45\linewidth}{Produce  TsetlinAutomata (TA) for each clause , assigning  to  and  to . Both  and   belong to .}}
\State  \Comment{Collect each team  in }
\Repeat
    \State  GetNextTrainingExample()\Comment{Mini-batches, random selection, etc.}
    \State  ObtainConjunctiveClauses() \Comment{\parbox[t]{.45\linewidth}{The Tsetlin Automata Teams  make their decisions, producing the conjunctive clauses.}} 
    \For{} \Comment{Provide feedback for clauses with positive polarity.}
        \If{}
            \If{Random() }
                 \State TypeIFeedback() \Comment{\parbox[t]{.45\linewidth}{Output  activates Type I Feedback for clauses with positive polarity.}}
            \EndIf
        \ElsIf{}
            \If{Random() }
                \State TypeIIFeedback() 
                \Comment{\parbox[t]{.45\linewidth}{Output  activates Type II Feedback for clauses with positive polarity.}}
            \EndIf
        \EndIf
    \EndFor
    \For{} \Comment{Provide feedback for clauses with negative polarity.}
        \If{}
            \If{Random() }
                 \State TypeIIFeedback() \Comment{\parbox[t]{.45\linewidth}{Output  activates Type II Feedback for clauses with negative polarity.}}
            \EndIf
        \ElsIf{}
            \If{Random() }
                \State TypeIFeedback() 
                \Comment{\parbox[t]{.45\linewidth}{Output  activates Type I Feedback for clauses with negative polarity.}}
            \EndIf
        \EndIf
    \EndFor
   
\Until{StopCriteria()  \True}
\State \Return PruneAllExcludeClauses() \Comment{Return completely trained conjunctive clauses  for , after pruning clauses where all literals have been excluded.}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Type I Feedback - Combating False Negative Output}
\label{alg:type_i_feedback}

\Input{Input , Clause , Tsetlin Automata team , Number of inputs }

\begin{algorithmic} [1]
\Procedure{GenerateTypeIFeedback}{}

\For{}\Comment{Reward/Penalize all Tsetlin Automata in .}
    \State 
    \If{Random()  TypeIFeedback(Reward, Action(), , )}
        \State Reward() \Comment{Reward TA controlling .}
    \ElsIf{Random()  TypeIFeedback(Penalty, Action(), , )}
        \State Penalize() \Comment{Penalize TA controlling .}
    \EndIf
    
    \If{Random()  TypeIFeedback(Reward, Action(), , )}
        \State Reward() \Comment{Reward TA controlling .}
    \ElsIf{Random()  TypeIFeedback(Penalty, Action(), , )}
        \State Penalize()  \Comment{Penalize TA controlling .}
    \EndIf
\EndFor

\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Type II Feedback - Combating False Positive Output}
\label{alg:type_ii_feedback}

\Input{Input , Clause , Tsetlin Automata team , Number of inputs }

\begin{algorithmic} [1]
\Procedure{GenerateTypeIIFeedback}{}

\For{}
    \State 
    
    \If{Random()  TypeIIFeedback(Penalty, Action(), , )}
        \State Penalize() \Comment{Penalize TA controlling .}
    \EndIf

    \If{Random()  TypeIIFeedback(Penalty, Action(), , )}
        \State Penalize() \Comment{Penalize TA controlling .}
    \EndIf
\EndFor

\EndProcedure
\end{algorithmic}
\end{algorithm}

The step-by-step procedure for learning conjunctive clauses can be found in Algorithm \ref{alg:tsetlin_machine}. The algorithm takes a set of training examples, , as input. Based on this, it produces a propositional formula in conjunctive normal form, , for predicting the output, . 

We will now take a closer look at the algorithm, line-by-line.

{\bf Lines 2-3.} From the perspective of game theory, each Tsetlin Automaton, /, , , takes part in a large and complex game, consisting of multiple independent players. Every Tsetlin Automaton is assigned a user specified number of states, , per action, for learning which action to perform. The start-up state is then randomly set to either  or . Each Tsetlin Automaton / selects between two actions: Either to \emph{include} or \emph{exclude} a specific literal, /, in a specific clause, . These Tsetlin Automata are in turn organized into teams of  automata. Each team,  , is responsible for a specific clause , forming a subgame.

{\bf Line 5.} As seen in the algorithm, the learning process is driven by a set of training examples, , sampled from the joint input-output distribution , as described in Section \ref{sec:pattern_recognition_problem}. Each single training example  is fed to the Tsetlin Machine, one at a time, facilitating online learning.

{\bf Line 6.}  In each iteration, the Tsetlin Automata decide whether to include or exclude each literal from each of the conjunctive clauses. The result is a new  set of conjunctive clauses, , capable of predicting .

{\bf Lines 7-17.} The next step is to measure how well, , predicts the observed output  in order to provide feedback the Tsetlin Automata teams . As seen, feedback is generated directly based on the output of the summation function, , from Eqn. \ref{eqn:summation}. This part of the algorithm is particularly intricate, yet critical for the learning process. We therefore go through this part in more detail in the following paragraphs.

In order to maximize pattern representation capacity, we use a threshold value  as target for the summation . This mechanism is inspired by the finite-state automaton based resource allocation scheme for solving the knapsack problem in unknown and stochastic environments \cite{Granmo2007d}. The purpose of the mechanism is to ensure that only a few of the available clauses are spent representing each specific sub-pattern. This is to effectively allocate sparse pattern representation resources among competing sub-patterns. To exemplify, assume that the correct output is  for an input . If the votes accumulate to a total of  or more, neither rewards nor penalties are provided to the involved Tsetlin Automata. This leaves the Tsetlin Automata unaffected.
\\
\\
\\

{\bf Generating Type I Feedback.}  If the target output is , we randomly generate \emph{Type I Feedback} for each clause . The probability of generating Type I Feedback is:


{\bf Generating Type II Feedback.}  
If, on the other hand, the target output is , we randomly generate \emph{Type II Feedback} for each clause . The probability of generating Type II Feedback is:


Notice how the feedback vanishes as the number of triggering clauses correctly approaches /. This is a crucial part of effective use of the available pattern representation capacity. Indeed, if the existing clauses already are able to capture the pattern  faced, there is no need to adjust any of the clauses.

After Type I or Type II Feedback have been triggered for a clause, the invidual Tsetlin Automata within each clause is rewarded/penalized according to Algorithm \ref{alg:type_i_feedback} and Algorithm \ref{alg:type_ii_feedback}, respectively. In all brevity, rewarding/penalizing is directly based on Table \ref{table:type_i_feedback} and Table \ref{table:type_ii_feedback}.

{\bf Lines 18-28.} After the clauses with positive polarity have received feedback. The next step is to invert the role of Type I and Type II Feedback, and feed the resulting feedback to the clauses with negative polarity.

{\bf Line 29.} The above steps are iterated until a stopping criteria is fulfilled (for instance a certain number of iterations over the dataset), upon which the current clauses  are returned as the output of the algorithm.

The resulting propositional formula, returned by the algorithm,  has been composed by the Tsetlin Automata with the goal of predicting the output  with optimal accuracy.

\subsection{Implementation of Tsetlin Automata With Bit-wise Operators}

Small memory footprint and speed of operation can be crucial in complex and large scale pattern recognition. Being based on propositional formula, the Tsetlin Machine architecture can naturally be represented with bits and operated upon using bit-wise operators. However, it is not straightforward how to represent and update the Tsetlin Automata themselves since the state of each Tsetlin Automaton is an integer, and the action of an automaton is decided upon using a smaller-than operator.

One approach is to represent the state of 32 Tsetlin Automata with eight 32-bit integers, as laid out in Table \ref{tab:bit_representation}. The benefit of such a representation is that the action of each Tsetlin Automaton is readily available from the integer that represents bit 8, highlighted in bold in the table. This means that a bit mask for calculating the output of a clause always is available, without further computation. Employing this bit-based representation reduces memory usage four times, compared to using whole integer to represent the state of a single Tsetlin Automaton.

More importantly, it is possible to increment/decrement the state of 32 automata at a time by customized increment/decrement procedures, further increasing learning speed. As an example, for the MNIST dataset (cf. Section \ref{sec:empirical_results}), memory usage is approx. ten times smaller, learning speed 3.5 times higher, and classification speed 8 times higher, using this procedure.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c|c|c||c||c|c|c|c||c}
    &&&&&&&&&&\\
    \hline
   \textbf{Bit 8}&&&&&&&&&&\\
    &&&&&& &&&&\\
    Bit 2&&&&&&&&&&\\
    Bit 1&&&&&&&&&&\\
    \end{tabular}
    \caption{Bit-based representation of the Tsetlin Automata states, allowing the actions of each automaton to be obtained directly from bit 8.}
    \label{tab:bit_representation}
\end{table}

\section{Theoretical Analysis}
\label{sec:theoretical_results}

The reader may now have recognized that we have designed the Tsetlin Machine with mathematical analysis in mind, in order to facilitate a deeper understanding of our learning scheme. In this section, we argue that the Tsetlin Machine converges towards solving the problem of Pattern Learning with Propositional Logic from Definition \ref{def:classification_problem} with probability arbitrary close to unity.

\begin{figure}[!th]
\centering
\includegraphics[width=2.5in]{The_Proof_Subsets.pdf}
\caption{The pertinent subsets , , , and  for the proofs.}
\label{figure:pertinent_subsets}
\end{figure}

Let the propositional formula  solve a given pattern recognition problem, per Definition \ref{def:classification_problem}. To simplify notation, we will for the remainder of this section omit the index , and instead use  and  to respectively refer to any  and , . Without loss of generality, we further limit ourselves to consider one of the underlying sub-patterns  described in Section \ref{sec:pattern_recognition_problem}. By definition, there exists at least one clause   in  such that  for all inputs . Finally, let  be a literal (representing either  or ). Notice that we in the following focus on the sub-patterns belonging to class . The reasoning would follow along the same lines for class .

Our overall strategy for the proof consists of three steps: (1) show that  forms a Nash Equilibrium for the associated team of Tsetlin Automata; (2) show that other candidate clauses  do not form Nash Equilibria; and finally, (3) allude to the convergence properties of Tsetlin Automata games, combining multiple subgames into the full-blown Tsetlin Machine game.

Before we can complete the proof, we need to derive the expected reward of the actions. Figure \ref{figure:pertinent_subsets} illustrates pertinent pattern subsets that will help us do that. Firstly, let  be the subset of input, , that makes  evaluate to . Conversely, let  be the complement of . Let further  be the subset of  where a clause  evaluates to , and let  be an even further constrained subset where  also is . These four subsets are depicted in the figure, to guide the reader through the set calculations that follows.

We will use the notation  to refer to a subgame between the Tsetlin Automata that controls the composition of clause , that is, . Consider one of the Tsetlin Automata, , in the subgame . Notice that the payoffs it can receive are given in Table \ref{table:type_i_feedback} and Table \ref{table:type_ii_feedback} for the  whole range of subgame  outcomes, from the perspective of . Finally, let  be an input-output pair sampled from . 

\begin{mylemma}[]\label{lemma:exclude_literal}
The expected payoff of the action \emph{Exclude Literal} for automaton  within the subgame  is:

\end{mylemma}
\begin{proof}
In brief, we receive an expected fractional \emph{reward}  every time  becomes , except when both  and  evaluates to . In that case, we instead receive an expected fractional \emph{penalty} of  (see Table \ref{table:type_i_feedback}). Formally, we can reformulate this rewarding and penalizing as follows:

Furthermore, selecting the  \emph{Exclude Literal} when  and , while  evaluates to , provides a full \emph{penalty} (see Table \ref{table:type_ii_feedback}). As further seen in the table, false positive output never triggers penalties or rewards for the \emph{Include Literal} action. All this is by design in order to suppress the output  from  to combat false positive output. This additional effect can be formalized as follows:

\end{proof}

\begin{mylemma}[]\label{lemma:include_literal}
The expected payoff of the action \emph{Include Literal} for automaton  within the subgame  is:

\end{mylemma}
\begin{proof}
Using Table \ref{table:type_i_feedback}, we now simply establish that the expected feedback of action \emph{Include Literal} is symmetric to the expected feedback of action \emph{Exclude Literal}, apart from not being affected by Type II Feedback:


In other words, for the same reasons that \emph{Exclude Literal} has a negative expected payoff, \emph{Include Literal} has a positive one, and vice versa! \emph{This symmetry is by design to facilitate robust and fast learning in the game.}
\end{proof}

\begin{mytheorem}[]\label{theorem:nash_equilibrium}
Let  be a solution to a pattern recognition problem per Definition \ref{def:classification_problem}. Further, let the probability of observing erroneous class information be a constant, . Then every clause  in  is a Nash equilibrium in the associated Tsetlin Machine subgame .
\end{mytheorem}
\begin{proof}

We start our proof by reformulating Eqn. \ref{eqn:exclude_literal_expected} to incorporate the probability of erroneous class information, :

Further, we note that the Tsetlin Machine can be self-balancing, ensuring that  (due to how training examples are sampled for the Multi-Class Tsetlin Machine in Section \ref{sec:building_block}):

Finally, we note that . This is because , again due to the self-balancing nature of the Tsetlin Machine, and because  is a subset of . In the following, let . We can thus simplify the expected payoff of the \emph{Exclude Literal} action to:

Similarly, the expected payoff of the \emph{Include Literal} action can be simplified to:


Let us now consider an arbitrary Tsetlin Automaton, which controls, let us say, the inclusion or exclusion of literal . This produces two possible scenarios. Either the literal  is part of the clause  (the action selected is \emph{Include Literal}). Otherwise, the literal is not part of the clause  (the selected action is \emph{Exclude Literal}). We will now consider each of these scenarios and verify that both scenarios qualify as Nash equilibria.

{\bf Scenario 1: Literal included.} Let us first consider the situation where  is zero. This means that the output  is free of noise, and the problem becomes purely the extraction of the underlying sub-patterns. We only need to verify that the expected payoff of the \emph{Exclude Literal} action is negative. Multiplying by  leaves the polarity of the expression unchanged and we have: 

We know that  by Definition \ref{def:classification_problem} and due to the balancing effect of the Tsetlin Machine. Thus, clearly,  will always be smaller than . In other words, the expected payoff for \emph{Exclude Literal} is always negative. Hence, due to the symmetry, \emph{Include Literal} always has positive expected payoff, and is the preferred action, enforcing the equilibrium.

By allowing noisy output , the analysis becomes somewhat more complex:

Again, multiplying by  leaves the polarity of the expression unchanged and we have:

We now too note that , and
observe that  is larger than  and  is larger than . Hence, the expected payoff for \emph{Exclude Literal} is negative and we have a Nash Equilibrium.

Recall that the whole purpose of the above Nash equilibrium is to make sure that the patterns captured by the clause  is of an appropriate granularity, decided by , finely balancing \emph{Exclude Literal} actions against \emph{Include Literal} actions. This is combined with the combating of false positive output through targeted selection of \emph{Include Literal} actions.

To conclude, due to the established symmetry in payoff, switching from \emph{Include Literal} to \emph{Exclude Literal} leads to a net loss in expected payoff, providing a Nash equilibrium for the action \emph{Include Literal}.

{\bf Scenario 2: Literal excluded.} Again, consider the expected payoff of \emph{Exclude Literal} when  is zero:

With  excluded from , we know that  by definition, otherwise,  would have been included instead. In other words, the expected payoff of \emph{Exclude Literal} is always positive, while the expected payoff of \emph{Include Literal} becomes negative. In a similar manner, we can modify the above procedure for the noisy case. Hence, the Nash equilibrium!
\end{proof}

\begin{mytheorem}[]A conjunctive clause  that is not part of the solution  is not a Nash equilibrium.\label{nash_equilibrium}
\end{mytheorem}
\begin{proof}
This theorem follows from the proof for Theorem \ref{theorem:nash_equilibrium}. By invalidating any of the required conditions that made the clause  a Nash equilibrium, it can no longer be a Nash equilibrium. That is, by including more literals, for instance,  drops below . Conversely, starting with too many literals excluded,  it becomes advantageous to include the literals (positive expected payoff). A final possibility is that a clause captures another class than its target class. Such a configuration is highly unstable since Type II Feedback will aggressively move the Tsetlin Automata out of that configuration.
\end{proof}

We end this section by arguing that the Tsetlin Machine will converge to a solution  with probability arbirtrarily close to unity. Here follows a sketch for a proof. Any solution scheme that is capable of finding a single Nash equilibrium in a game will be able to solve each subgame   due to Theorem 1 and Theorem 2. This is because each subgame   is played independently of the other subgames, apart from the indirect interaction through the summation function  of the Tsetlin Machine. However, the feedback that connects the subgames only controls how often each game is activated. Indeed, a subgame is activated with probability 

for Type I Feedback, and with probability: 

for Type II Feedback. 

Together, these merely control the mixing factor of the two different types of feedback, as well as the frequency with which the subgame is played. Type II Feedback is self-defeating, eliminating itself by nature to a minimum. As an equilibrium is found in each subgame, eventually, all subgames are stopped being played, i.e.  always evaluates to either  or , and the Tsetlin Machine game has been solved.

The Tsetlin Automaton is one particularly robust mechanism for solving such coordination games, converging to a Nash equilibrium with probability arbitrarily close to unity.

\section{Empirical Results}
\label{sec:empirical_results}

In this section we evaluate the Tsetlin Machine empirically using five datasets:
\begin{itemize}
    \item {\bf Binary Iris Dataset.} This is the classical Iris Dataset, however, with features in binary form.
    \item {\bf Binary Digits Dataset.} This is the classical digits dataset, again with features in binary form.
    \item {\bf \emph{Axis \& Allies} Board Game Dataset.} This new dataset involving optimal move prediction in a minimalistic, yet intricate, mini-game from the \emph{Axis \& Allies} board game.
    \item {\bf Noisy XOR Dataset with Non-informative Features.} This artificial dataset is designed to reveal particular "blind zones" of pattern recognition algorithms. The dataset captures the renowned XOR-relation. Furthermore, the dataset contains a large number of random non-informative features to measure susceptibility towards the curse of dimensionality \cite{Duda2000}. To examine robustness towards noise we have further randomly inverted  of the outputs.
    \item {\bf MNIST Dataset.} The MNIST dataset is a larger scale dataset used extensively to benchmark machine learning algorithms. We have included this dataset to investigate the scalability of the Tsetlin Machine, as well as the behaviour of longer learning processes.
\end{itemize}

For these datasets, we form ensembles of  to  independent replications with different random number streams. We do this to minimize the variance of the reported results and to provide the foundation for a statistical analysis of the merits of the different schemes evaluated. 

Together with the Tsetlin Machine, we also evaluate several classical machine learning techniques using the same random number streams. This  includes Multilayer Perceptron Networks, the Naive Bayes Classifier, Support Vector Machines, and Logistic Regression. Where appropriate, the different schemes are optimized by means of hyper-parameter grid searches.
As an example, Figure \ref{figure:xor_s} captures the impact the  parameter of the Tsetlin Machine has on mean accuracy, for the Noisy XOR Dataset. Each point in the plot measures the mean accuracy of  different replications of the XOR-experiment for a particular value of . Clearly, accuracy increases with  up to a certain point, before it degrades gradually. Based on the plot, for the Noisy XOR-experiment, we decided to use an  value of .

\begin{figure}[!ht]
\centering
\includegraphics[width=6.0in]{XOR_s.pdf}
\caption{The mean accuracy of the Tsetlin Machine (y-axis) on the Noisy XOR Dataset for different values of the parameter  (x-axis).}
\label{figure:xor_s}
\end{figure}

\subsection{The Binary Iris Dataset}

We first evaluate the Tsetlin Machine on the classical Iris dataset\footnote{UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/iris].}. This dataset consists of 150 examples with four inputs (Sepal Length, Sepal Width, Petal Length and Petal Width), and three possible outputs (Setosa, Versicolour, and Virginica).

We increase the challenge by transforming the four input values into one consecutive sequence of  bits, four bits per float. It is thus necessary to also learn how to segment the  bits into four partitions, and extract the numeric information. We refer to the new dataset as the The Binary Iris Dataset.

We partition this dataset into a training set and a test set, with 80 percent of the data being used for training. We here randomly produce  training and test data partitions. For each ensemble, we also randomly reinitialize the competing algorithms, to gain information on stability and robustness. The results are reported in Table \ref{tab:accuracy_binary_iris_test}.

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique/Accuracy ()&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Networks&&&&&\\
    SVM&&&&&
    \end{tabular}
    \caption{The Binary Iris Dataset -- accuracy on test data.}
    \label{tab:accuracy_binary_iris_test}
\end{table}

The Tsetlin Machine\footnote{In this experiment, we use a Multi-Class Tsetlin Machine, described in Section 6.1. We also apply Boosting of True  Positive  Feedback  to  Include  Literal  actions as described in Section \ref{sec:type_i_feedback}.} used here employs  clauses, and uses an -value of  and a threshold  of . Furthermore, the individual Tsetlin Automata each has  states. This Tsetlin Machine is run for  epochs, and it is the accuracy after the final epoch that is reported. 
Propositional formulas with higher test accuracy are often found in preceding epochs because of the random exploration of the Tsetlin Machine. However, to avoid overfitting to the test set by handpicking the best configuration found, we instead simply use the last configuration produced.

In Table \ref{tab:accuracy_binary_iris_test}, we list mean accuracy with  confidence intervals,  and  percentiles, as well as the minimum and maximum accuracy obtained, across the  experiment runs we executed. As seen, the Tsetlin Machine provides the highest mean accuracy. However, for the ile scores, most of the schemes obtain  accuracy. This can be explained by the small size of the test set, which merely contains 30 examples. Thus it is easier to stumble upon a random configuration that happens to provide fault free classification. Since the test set is merely a sample of the corresponding real-world problem, it is reasonable to assume that higher mean accuracy translates to more robust performance overall.

The training set, on the other hand, reveals subtler differences between the schemes. The results obtained on the training set are shown in Table \ref{tab:accuracy_binary_iris_training}.  As seen, the SVM here provides best performance on average, while the Tsetlin Machine provides the second best accuracy. However, the large drop in accuracy from the training data to the test data for the SVM indicates overfitting on the training data.

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Network&&&&&\\
    SVM&&&&&
    \end{tabular}
    \caption{The Binary Iris Dataset -- accuracy on training data.}
    \label{tab:accuracy_binary_iris_training}
\end{table}

\subsection{The Binary Digits Dataset}

We next evaluate the Tsetlin Machine on the classical Pen-Based Recognition of Handwritten Digits Dataset\footnote{UCI Machine Learning Repository [http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits].}. The original dataset consists of 250 handwritten digits from 44 different writers, for a total number of  instances. We increase the challenge by removing the individual pixel value structure, transforming the  different input features into a sequence of  bits,  bits per pixel. We refer to the modified dataset as the The Binary Digits Dataset. Again we partition the dataset into training and test sets, keeping 80 percent of the data for training.

The Tsetlin Machine\footnote{In this experiment, we used a Multi-Class Tsetlin Machine, described in Section 6.1.  We also apply Boosting of True  Positive  Feedback  to  Include  Literal  actions as described in Section \ref{sec:type_i_feedback}.} used here contains  clauses, uses an -value of , and has a threshold  of . Furthermore, the individual Tsetlin Automata each has  states. The Tsetlin machine is run for  epochs, and it is the accuracy after the final epoch that is reported.

Table \ref{tab:accuracy_binary_digits_test} reports mean accuracy with  confidence intervals,  and  percentiles, as well as the minimum and maximum accuracy obtained, across the  experiment runs we executed. As seen, the Tsetlin Machine again clearly provides the highest accuracy on average, also when taking the  confidence intervals into account. For this dataset, the Tsetlin Machine is also superior when it comes to the maximal accuracy found across the  replications of the experiment, as well as for the ile results.

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique/Accuracy ()&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Network&&&&&\\
    SVM&&&&&
    \end{tabular}
    \caption{The Binary Digits Dataset -- accuracy on test data.}
    \label{tab:accuracy_binary_digits_test}
\end{table}

Performing poor on the test data and well on the training data indicates susceptibility to overfitting. Table \ref{tab:accuracy_binary_digits_training} reveals that the other techniques, apart from the Naive Bayes Classifier, perform significantly better on the training data, unable to transfer this performance to the test data.

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Network&&&&&\\
    SVM&&&&&
    \end{tabular}
    \caption{The Binary Digits Dataset -- accuracy on training data.}
    \label{tab:accuracy_binary_digits_training}
\end{table}

\subsection{The \emph{Axis \& Allies} Board Game Dataset}

Besides the two classical datasets, we also have built a new dataset based on the board game \emph{Axis \& Allies}\footnote{http://avalonhill.wizards.com/games/axis-and-allies}. 
We designed this dataset to exhibit intricate pattern structures, involving optimal move prediction in a minimalistic, yet subtle, subgame of \emph{Axis \& Allies}. Indeed, superhuman performance for the \emph{Axis \& Allies} board game has not yet been attained. In \emph{Axis \& Allies}, every piece on the board are potentially moved each turn. Additionally, new pieces are introduced throughout the game, as a result of earlier decisions. This arguably yields a larger search tree than the ones we find in Go and chess. Finally, the outcome of battles are determined by dice, rendering the game stochastic.

The \emph{Axis \& Allies} Board Game Dataset consists of  board game positions, exemplified in Figure \ref{figure:axis_and_allies_minigame}. Player 1 owns the "Caucasus" territory in the figure, while Player 2 owns "Ukraine" and "West Russia". At start-up, each player is randomly assigned 0-10 tanks and 0-20 infantry each. These units are their respective starting forces. For Player 2, they are randomly distributed among his two territories. The game consists of two rounds. First Player 1 attacks. This is followed by a counter attack by Player 2. In order to win, Player 1 needs to capture both of "Ukraine" and "West Russia". Player 2, on the other hand, merely needs to take "Caucasus".

\begin{table}[!bh]
    \centering
    \begin{tabular}{c|c|c|c|c|c||c|c|c|c}
    \multicolumn{6}{c||}{\bf At Start}&\multicolumn{4}{c}{\bf Optimal Attack}\\
    \hline
    \multicolumn{2}{c|}{\bf Caucasus}&\multicolumn{2}{c|}{\bf W. Russia}&\multicolumn{2}{c||}{\bf Ukraine}&\multicolumn{2}{c|}{\bf W. Russia}&\multicolumn{2}{c}{\bf Ukraine}\\
     \hline
    \bf Inf&\bf Tnk&\bf Inf&\bf Tnk&\bf Inf&\bf Tnk&\bf Inf&\bf Tnk&\bf Inf&\bf Tnk\\
    \hline
    \hline
    16&4&11&4&5&4&0&0&3&4\\
    19&3&6&1&6&3&7&2&12&1\\
    9&1&1&3&0&5&0&0&0&0
    \end{tabular}
    \caption{The \emph{Axis \& Allies} Board Game Dataset.}
    \label{table:aa_dataset}
\end{table}

To produce the dataset, we built an \emph{Axis \& Allies} Board Game  simulator. This allowed us to find the optimal attack for each assignment of starting forces. The resulting input and output variables are shown in Table \ref{table:aa_dataset}. The at start forces are to the left, while the optimal attack forces can be found to the right. In the first row, for instance, it is optimal for Player 1 to launch a preemptive strike against the armor in Ukraine (armor is better offensively than defensively), to destroy offensive power, while keeping the majority of forces for defense.

We use  of the data for training, and  for testing, randomly producing  different partitions of the dataset. The Tsetlin Machine employed here contains  clauses, and uses an -value of  and a threshold  of . Furthermore, the individual Tsetlin Automata each has  states. The Tsetlin machine is run for  epochs, and it is the accuracy after the final epoch that is reported.

Table \ref{tab:aa_test} reports the results from predicting output bit  among the  output bits (as representative for all of the bits). In the table, we list mean accuracy with  confidence intervals,  and  percentiles, as well as the minimum and maximum accuracy obtained, across the  experiment runs we executed. 
\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique/Accuracy ()&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Network&&&&&\\
    SVM&&&&&\\
    Random Forest&&&&&
    \end{tabular}
    \caption{The \emph{Axis \& Allies} Dataset -- accuracy on test data.}
    \label{tab:aa_test}
\end{table}
As seen in the table, apparently only the Tsetlin Machine and the neural network are capable of properly handling the complexity of the dataset, providing statistically similar performance. The Tsetlin Machine is quite stable performance-wise, while the neural network performance varies more.

However, the number of clauses needed to achieve the above performance is quite high for the Tsetlin Machine, arguably due to its flat one-layer architecture. Another reason that can explain the need for a large number of clauses can be the intricate nature of the mini-game of Axis \& Allies. Since we need an -value as large as , apparently, some of the pertinent sub-patterns must be quite fine-grained. Because the -value is global, all patterns, even the coarser ones, must be learned at this fine granularity. A possible next step in the research on the Tsetlin Machine could therefore be to investigate the effect of having clauses with different -values -- some with smaller values for the rougher patterns, and some with larger values for the finer patterns.

As a final observation, Table \ref{tab:aa_training} reports performance on the training data. Random Forest distinguishes itself by almost perfect predictions for the training data, thus clearly overfitting, but still performing well on the test set. The other techniques provide slightly improved performance on the training data, as expected.

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique/Accuracy ()&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Network&&&&&\\
    SVM&&&&&\\
    Random Forest&&&&&
    \end{tabular}
    \caption{The \emph{Axis \& Allies} Dataset -- accuracy on training data.}
    \label{tab:aa_training}
\end{table}

\begin{figure}[!th]
\centering
\includegraphics[width=3.0in]{Axis_and_Allies.pdf}
\caption{The \emph{Axis \& Allies} mini game.}
\label{figure:axis_and_allies_minigame}
\end{figure}

\subsection{The Noisy XOR Dataset with Non-informative Features}

We now turn to an artifical dataset, constructed to uncover "blind zones" caused by XOR-like relations. Furthermore, the dataset contains a large number of random non-informative features to measure susceptibility towards the curse of dimensionality \cite{Duda2000}. To examine robustness towards noise, we have further randomly inverted  of the outputs.

\begin{table}[!bh]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c||c}
    \bf &\bf &\bf &\bf &\bf &\bf &\bf &\bf &\bf &\bf &\bf &\bf &\bf \\
    \hline
    \hline
    0&1&0&1&1&0&1&1&0&1&1&0&1\\
    1&1&1&0&1&0&1&1&0&0&1&1&0\\
    0&0&1&1&0&1&1&1&1&0&1&0&0\\
    1&1&1&0&1&1&1&0&1&1&0&0&1
    \end{tabular}
    \caption{The Noisy XOR Dataset with Non-informative Features.}
    \label{tab:noisy_xor_dataset}
\end{table}

\begin{table}[!bh]
    \centering
    \begin{tabular}{c|c|c}
    {\bf No.}&{\bf Sign}&{\bf Clause Learned}\\
    \hline 
    \hline
    1&&\\
    2&&\\
    3&&\\
    4&&
    \end{tabular}
    \caption{Example of four clauses composed by the Tsetlin Machine for the XOR Dataset with Non-informative Features.}
    \label{tab:noisy_xor_dataset_clauses}
\end{table}

The dataset consists of  examples with twelve binary inputs, , and a binary output, . Ten of the inputs are completely random. The two remaining inputs, however, are related to the output  through an XOR-relation, . Finally,  of the outputs are inverted. Table \ref{tab:noisy_xor_dataset} shows four examples from the dataset, demonstrating the high level of noise. We partition the dataset into training and test data, using  of the data for training.

The Tsetlin Machine\footnote{In this experiment, we used a Multi-Class Tsetlin Machine, described in Section 6.1.} used here contains  clauses, and used an -value of  and a threshold  of . Furthermore, the individual Tsetlin Automata each has  states. The Tsetlin Machine is run for  epochs, and it is the accuracy after the final epoch, that we report.

Table \ref{tab:noisy_xor_dataset_clauses} contains four of the clauses produced by the Tsetlin Machine. Notice how the noisy dataset from Table \ref{tab:noisy_xor_dataset} has been turned into informative propositional formulas that capture the structure of the dataset.

The empirical results are found in Table \ref{tab:xor_test}. Again, we report mean accuracy with  confidence intervals,  and  percentiles, as well as the minimum and maximum accuracy obtained, across the  replications of the experiment. Note that for the test data, the output values are unperturbed. As seen, the XOR-relation, as expected, makes Logistic Regression and the Naive Bayes Classifier incapable of predicting the output value , resorting to random guessing. Both the neural network and the Tsetlin Machine, on the other hand, see through the noise and captures the underlying XOR pattern. SVM performs slightly better than the Naive Bayes Classifier and Logistic Regression, however, is clearly distracted by the added non-informative features (the SVM performs much better with fewer non-informative features).

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c|c|c|c|c}
         \bf Technique/Accuracy ()&\bf Mean&\bf ile &\bf ile&\bf Min.&\bf Max.\\
         \hline
    Tsetlin Machine&&&&&\\
    Naive Bayes&&&&&\\
    Logistic Regression&&&&&\\
    Multilayer Perceptron Network&&&&&\\
    SVM&&&&&
    \end{tabular}
    \caption{The Noisy XOR Dataset with Non-informative Features -- accuracy on test data.}
    \label{tab:xor_test}
\end{table}

Figure \ref{figure:xor_data_sizes} shows how accuracy degrades with less data, when we vary the dataset size from  examples to  examples. As expected, Naive Bayes and Logistic Regression guess blindly for all the different data sizes. The main observation, however, is that the accuracy advantage the Tsetlin Machine has over  neural networks increases with less training data. Indeed, it turns out that the Tsetlin Machine performs robustly with small training data sets in all of our experiments.

\begin{figure}[!ht]
\centering
\includegraphics[width=6.0in]{XOR_data_sizes.pdf}
\caption{Accuracy (y-axis) for the Noisy XOR Dataset for different training dataset sizes (x-axis).}
\label{figure:xor_data_sizes}
\end{figure}

\subsection{The MNIST Dataset}

\begin{figure}[!ht]
\centering
\includegraphics[width=6.0in]{learning_progress.pdf}
\caption{The mean test- and training accuracy per epoch for the Tsetlin Machine on the MNIST Dataset.}
\label{figure:learning_progress}
\end{figure}

We next evaluate the Tsetlin Machine on the MNIST Dataset of Handwritten Digits \footnote{http://www.pymvpa.org/datadb/mnist.html} \cite{LeCun1998}, also investigating how learning progresses, epoch-by-epoch, in terms of accuracy. Note that the experimental results reported here can be reproduced with the demo found at\\ {\tt https://github.com/cair/fast-tsetlin-machine-with-mnist-demo}.

The original dataset consists of  training examples, and  test examples. We binarize this dataset by replacing pixel values larger than  with  (with the original pixel grey tones ranging from  to ). Pixel values below or equal to  are replaced with . 

The Tsetlin Machine\footnote{In this experiment, we used a Multi-Class Tsetlin Machine, described in Section 6.1.  We also applied Boosting of True  Positive  Feedback  to  Include  Literal  actions, as described in Section \ref{sec:type_i_feedback}.} used here contains  clauses,  clauses per class, uses an -value of , and a threshold  of . Furthermore, the individual Tsetlin Automata each has  states. The Tsetlin machine is run for  epochs, and it is the accuracy after the final epoch that is reported.

As seen in Figure \ref{figure:learning_progress}, both mean test- and training accuracy increase almost monotonically across the epochs, however, affected by random fluctuation. Perhaps most notably, while the mean accuracy on the training data approaches 
\%, accuracy on the test data continues to increase as well, hitting \% after 400 epochs. This is quite different from what occurs with back-propagation on a neural network, where accuracy on test data starts to drop at some point due to overfitting, without proper regularization mechanisms.

The figure also show how varying the number of clauses and the threshold  affects accuracy and fluctuation. With more clauses available to express patterns, in combination with a higher threshold , both learning speed, stability and accuracy increases, however, at the cost of larger computational cost.

\begin{table}[!bh]
    \centering
    \begin{tabular}{r||c}
        \bf Technique&\bf Accuracy ()\\
        \hline
        {\it 2-layer NN, 800 HU, Cross-Entropy Loss}&\\
        Tsetlin Machine ( \%ile)&\\
        Tsetlin Machine (Mean)&\\
        Tsetlin Machine ( \%ile)&\\
        {\it K-nearest-neighbors, L3}&\\
        {\it 3-layer NN, 500+150 hidden units}&\\
        {\it 40 PCA + quadratic classifier}&\\
        {\it 1000 RBF + linear classifier}&\\
        Logistic regression&\\
        {\it Linear classifier (1-layer NN)}&\\
        Decision tree&\\
        Multinomial Naive Bayes&
    \end{tabular}
    \caption{A comparison of vanilla machine learning algorithms with the Tsetlin Machine, directly on the original unenhanced MNIST dataset (NN - Neural Network). }
    \label{tab:MNIST_comparison}
\end{table}

Table \ref{tab:MNIST_comparison} reports the mean accuracy of the Tsetlin Machine, across the  experiment runs we executed. As points of reference, results for other well-known algorithms have been obtained from {\tt http://yann.lecun.com/exdb/mnist/} and included in the table (in italic). Only the vanilla version of the algorithms, that has been applied directly on unenhanced MNIST data, is included here. The purpose of this selection is to strictly compare algorithmic performance. In other words, we do not consider the effect of enhancing the dataset (warping, distortion, deskewing), combining different algorithms (e.g., neural network combined with nearest neighbor, convolution schemes), or applying meta optimization techniques (boosting, ensemble learning, etc.). With such techniques, it is possible to significantly increase accuracy, with the best currently reported results being an accuracy of \% \cite{Wan2013}. Enhancing the vanilla Tsetlin Machine with such techniques is further work.

Additionally, as a further point of reference, we train and evaluate logistic regression, decision trees, and multinomial Naive Bayes on the \emph{binarized} MNIST dataset used by the Tsetlin Machine.

As seen in the table, the Tsetlin Machine provides competitive accuracy, outperforming e.g. K-nearest neighbor and a 3-layer neural network. It is outperformed by a 2-layer neural network with 800 hidden nodes, using cross entropy loss. However, note that the Tsetlin Machine operates upon the binarized MNIST data (the grey tone value of each pixel is either set to 0 or 1) , and thus has a disadvantage. Improved binarization techniques for the Tsetlin Machine is further work.

\section{The Tsetlin Machine as a Building Block in More Advanced Architectures}
\label{sec:building_block}

We have designed the Tsetlin Machine to facilitate building of more advanced architectures. We will here exemplify different ways of connecting multiple Tsetlin Machines in more advanced architectures.

\subsection{The Multi-Class Tsetlin Machine}

In some pattern recognition problems the task is to assign one of  classes to each observed pattern, . That is, one needs to decide upon a \emph{single} output value, . Such a multi-class pattern recognition problem can be handled by the Tsetlin Machine by representing  as bits, using multiple outputs, . In this section, however, we present an alternative architecture that addresses the multi-class pattern recognition problem more directly.

\begin{figure}[!th]
\centering
\includegraphics[width=4.5in]{Multi-class_Tsetlin_Machine.pdf}
\caption{The Multi-Class Tsetlin Machine.}
\label{figure:multi-class_tsetlin_machine}
\end{figure}

Figure \ref{figure:multi-class_tsetlin_machine} depicts the Multi-Class Tsetlin Machine\footnote{An implementation of the Multi-Class Tsetlin Machine can be found at {\tt https://github.com/cair/TsetlinMachine}.} which replaces the threshold function of each output  with a single {\bf argmax} operator. With the {\bf argmax} operator, the index  of the largest sum  is outputted as the final output of the Tsetlin Machine:

In this manner, each propositional formula , consisting of clauses , captures the pertinent aspects of the respective class  that it models. 

Training is done as described in Section \ref{sec:tsetlin_machine_game}, apart from one critical modification. Assume we have  for the current observation, . Then the Tsetlin Automata team behind  is trained as per  in the original Algorithm \ref{alg:tsetlin_machine}. Additionally, a random class  is selected. The Tsetlin Automata team behind  is then trained in accordance with  in the original algorithm (trained with opposite feedback, i.e., Type I Feedback becomes Type II Feedback, and vice versa).

\subsection{The Fully Connected Deep Tsetlin Machine}

Another architectural family is the Fully Connected Deep Tsetlin Machine \cite{Granmo2018b}, illustrated in Figure \ref{figure:deep_tsetlin_machine}. 
\begin{figure}[!th]
\centering
\includegraphics[width=4.0in]{Deep_Tsetlin_Machine.pdf}
\caption{The fully connected Deep Tsetlin Machine.}
\label{figure:deep_tsetlin_machine}
\end{figure}
The purpose of this architecture is to build composite propositional formulas, combining the propositional formula composed at one layer into more complex formula at the next. As exemplified in the figure, we here connect multiple Tsetlin Machines in a sequence. The clause output from  each Tsetlin Machine in the sequence is provided as input to the next Tsetlin Machine in the sequence. In this manner we build a multi-layered system.  For instance, if layer  produces two clauses  and , layer  can manipulate these further, treating them as inputs. Layer  could then form more complex formulas like , which can be rewritten as .

One simple approach for training such an architecture is indicated in the figure. As illustrated, each layer is trained independently, directly from the output target , exactly as described in Section \ref{sec:tsetlin_machine_game}. The training procedure is thus similar to the strategy Hinton et al. used to train their pioneering Deep Belief Networks, layer-by-layer, in 2006 \cite{Hinton2006}. Such an approach can be effective when each layer produces abstractions, in the form of clauses, that can be taken advantage of in the following layer.

\subsection{The Convolutional Tsetlin Machine}

We next demonstrate how self-contained and independent Tsetlin Machines can interact to build a Convolutional Tsetlin Machine \cite{Granmo2018d}, illustrated in Figure \ref{figure:convolutional_tsetlin_machine}.
\begin{figure}[!th]
\centering
\includegraphics[width=6.0in]{Convolutional_Tsetlin_Machine.png}
\caption{The Convolutional Tsetlin Machine.}
\label{figure:convolutional_tsetlin_machine}
\end{figure}
The Convolutional Tsetlin Machine is a deep architecture based on mathematical convolution, akin to Convolutional Neural Networks \cite{LeCun1998}. For illustration purposes, consider 2D images of size  as input. At the core of a Convolutional Tsetlin Machine we find a kernel Tsetlin Machine with a small receptive field. Each layer  of the Convolutional Tsetlin Machine operates as follows:
\begin{enumerate}
    \item A convolution is performed over the input from the previous Tsetlin Machine layer, producing one \emph{feature map} per output . Here, the Tsetlin Machine acts as a kernel in the convolution. In this manner, we reduce complexity by reusing the same Tsetlin Machine across the whole image, focusing on a small image patch at a time.  
    \item The feature maps produced are then down-sampled using a pooling operator, in a similar fashion as done in a Convolutional Neural Network,  before the next layer and a new Tsetlin Machine takes over. Here, the purpose of the pooling operation is to gradually increase the abstraction level of the clauses, layer by layer.
\end{enumerate}
A simple approach for training a Convolutional Tsetlin Machine is indicated in the figure. In brief, the feedback to the Tsetlin Machine kernel is directly provided from the desired end output , exactly as described in Section \ref{sec:tsetlin_machine_game}. The only difference is the fact that the input to layer  comes from the down-scaled feature map produced by layer . Again, this is useful when each layer produces abstractions, in the form of clauses, that can be taken advantage of at the next layer.

\subsection{The Recurrent Tsetlin Machine}

The final example is the Recurrent Tsetlin Machine \cite{Granmo2018c} (Figure \ref{figure:recurrent_tsetlin_machine}). In all brevity, the same Tsetlin Machine is here reused from time step to time step. By taking the output from the Tsetlin Machine of the previous time step as input, together with an external input from the current time step, an infinitely deep sequence of Tsetlin Machines is formed. This is quite similar to the family of Recurrent Neural Networks \cite{Schmidhuber2015}.
\begin{figure}[!th]
\centering
\includegraphics[width=4.0in]{Recurrent_Tsetlin_Machine.pdf}
\caption{The Recurrent Tsetlin Machine.}
\label{figure:recurrent_tsetlin_machine}
\end{figure}
Again, the architecture can be trained layer by layer, directly from the target output  of the current time step . However, to learn more advanced sequential patterns, there is a need for rewarding and penalizing that propagate back in time. How to design such a propagation scheme is presently an open research question.

\section{Conclusion and Further Work}
\label{sec:conclusion}

In this paper we proposed the Tsetlin Machine, an alternative to neural networks. The Tsetlin Machine solves the vanishing signal-to-noise ratio of collectives of Tsetlin Automata. This allows it to coordinate thousands of Tsetlin Automata. By equipping \emph{teams} of Tsetlin Automata with the ability to express patterns in propositional logic, we have enabled them to recognize complex patterns. Furthermore, we proposed a novel decentralized feedback orchestration mechanism. The mechanism is based on resource allocation principles, with the intent of maximizing effectiveness of sparse pattern recognition capacity. This mechanism effectively provides the Tsetlin Machine with the ability to capture unlabelled sub-patterns.

Our theoretical analysis reveals that the Tsetlin Machine forms a set of subgames where the Nash equilibria maps to propositional formulas that maximize pattern recognition accuracy. In other words, there are no local optima in the learning process, only global ones. This explains how the collectives of Tsetlin Automata are able to accurately converge towards complex propositional formulas that capture the essence of five diverse pattern recognition problems.  Overall, the Tsetlin Machine is particularly suited for digital computers, being merely based on simple bit manipulation with AND-, OR-, and NOT gates. Both input, hidden patterns, and output are expressed with easy-to-interpret bit patterns. In our empirical evaluations on five distinct benchmarks, the Tsetlin Machine provided competitive accuracy with respect to both Multilayer Perceptron Networks, Support Vector Machines, Decision Trees, Random Forests, the Naive Bayes Classifier and Logistic Regression. It further turns out that the Tsetlin Machine requires much less data than neural networks, even outperforming the Naive Bayes Classifier in data sparse environments. 

The Tsetlin Machine is a completely new tool for machine learning. Based on its solid anchoring in automata- and game theory, promising empirical results, and its ability to act as a building block in more advanced systems, we believe the Tsetlin Machine has the potential to impact the AI field as a whole, opening a wide range of research paths ahead.

By demonstrating that the longstanding problem of vanishing signal-to-noise ratio can be solved, the Tsetlin Machine further provides a novel game theoretic framework for recasting the problem of pattern recognition. Such a framework can provide new opportunities for introducing bandit algorithms into large-scale pattern recognition. It could for instance be interesting to investigate the effect of replacing the Tsetlin Automaton with alternative bandit algorithms, such as algorithms based on Thompson Sampling \cite{Thompson1933,Granmo2010f,May2012,Chapelle2011} or Upper Confidence Bounds \cite{Auer2003}. 

Further, the more advanced Fully Connected Deep Tsetlin Machine, the Convolution Tsetlin Machine, and the Recurrent Tsetlin Machine architectures form a starting point for further exploration. These architectures can potentially improve pattern representation compactness and even learning speed. However, it is currently unclear how these architectures can be effectively trained.

Lastly, the high accuracy and robustness of the Tsetlin Machine, combined with its ability to produce self-contained easy-to-interpret propositional formulas for pattern recognition, makes it attractive for applied research, such as in the safety-critical medical domain.

\section*{Acknowledgements}
I thank my colleagues from the Centre for Artificial Intelligence Research (CAIR), Lei Jiao, Xuan Zhang, Geir Thore Berge, Bernt Viggo Matheussen, Saeed Rahimi Gorji, Darshana Abeyrathna, Sondre Glimsdal, Morten Goodwin, Jivitesh Sharma, Ahmed Abouzeid, and Rahele Jafari, for comments that greatly improved the manuscript.

\section*{Code Availability}
Source code and datasets for the Tsetlin Machine, available under the MIT Licence, can be found at {\tt https://github.com/cair/TsetlinMachine} and {\tt https://github.com/cair/fast-} {\tt tsetlin-machine-with-mnist-demo}.

\section*{Data Availability}
The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request. 

\bibliographystyle{IEEEtran}
\bibliography{Mendeley}

\end{document}
