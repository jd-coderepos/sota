

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbding}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{2317} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{Clover \includegraphics[width=12pt]{figure/shamrock-emoji.png}: Towards A Unified Video-Language Alignment and Fusion Model   }

\author{
Jingjia Huang\thanks{Equal Contribution.}
  \and
  Yinan Li\thanks{Work done when interning at ByteDance Inc.}
  \and
  Jiashi Feng
  \and
  Xinglong Wu
  \and
  Xiaoshuai Sun\thanks{Corresponding Author.}
  \and
  Rongrong Ji
\
    \arg\max_{f,g}\left[\mathrm{s}\left(f(V), g(T^+)\right) - \mathrm{s}\left(f(V), g(T^-)\right)\right],
    \label{eq:cross-modal alignment}

\begin{aligned}
     L_{v} &= - \sum_{i=1}^B \left [
    \log \frac{e^{s(V_e^i, T^i_{{e}}) / \tau}}
    {e^{s(V_{e}^i, T^i_{{e}} ) / \tau} + Z } 
    + \log \frac{e^{s(V_e^i, T^i_{ {m}}) / \tau}} 
    {e^{s(V_{e}^i, T^i_{{m}} ) / \tau} + Z } \right.\\ 
    & \left. + \log \frac{e^{s(V_e^i, M^i_{{V_{m}f}}) / \tau}} 
    {e^{s(V_{e}^i, M^i_{{V_{m}f}} ) / \tau} + Z } \right ]
    , where\\
 Z &= \sum_{j \neq i}^B \left[ e^{s(V_{e}^i, T_e^j)/ \tau} + e^{s(V_{e}^i, T_{{m}}^j)/ \tau} + e^{s(V_{e}^i, M_{{V_{m}f}}^j)/ \tau}\right] .
    \end{aligned}
    \label{eq:exclusive-NCE v2t}

\begin{aligned}
    L_{v'} &= - \sum_{i=1}^B \left[
    \log \frac{e^{s(T^i_{{e}}, V_{e}^i) / \tau}}{\sum_{j=1}^B e^{s(T^i_{{e}}, V_e^j)/ \tau}} + \log \frac{e^{s(T^i_{{m}}, V_{e}^i) / \tau}}{\sum_{j=1}^B e^{s(T^i_{{m}}, V_e^j)/ \tau}}\right.\\
    &\left.+ \log \frac{e^{s(M^i_{{V_{m}f}}, V_{e}^i) / \tau}}{\sum_{j=1}^B e^{s(M^i_{{V_{m}f}}, V_e^j)/ \tau}} \right].
    \label{eq:exclusive-NCE t2v}
    \end{aligned}

\begin{aligned}
    L_{rank} &=  \max\left( 0, -\left(\text{sim}(V_e, T_e) / \tau - \text{sim}(V_e, T_m) / \tau \right) + \lambda \right)\\
    &+ \max\left(0, -\left(\text{sim}(V_e, T_e) / \tau - \text{sim}(V_m, T_e) / \tau \right) + \lambda \right), \\
\end{aligned}
\label{eq:ranking loss}

\begin{aligned}
L_{mlm} &= -\frac{1}{B} \sum_{i=1}^{B} \sum_{m_{t_j} \in  M^{i}_{T_{m}}} [(1 - p^{i}_{t_j})^\gamma p^{i}_{t_j}]
    \label{eq:mlm_loss}
\end{aligned}

    L = L_{TmA} + L_{rank} + L_{mlm}.
    \label{eq:all_loss}

 

\begin{table*}[t]
\centering
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{cccccccccccccc}
\toprule
\multicolumn{1}{l|}{\multirow{2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{2}{*}{Pre-training dataset}} & \multicolumn{4}{c|}{DiDeMo} & \multicolumn{4}{c|}{MSRVTT} & \multicolumn{4}{c}{LSMDC} \\ \cline{3-14} 
\multicolumn{1}{l|}{} & \multicolumn{1}{c|}{} & R@1 & R@5 & R@10 & \multicolumn{1}{c|}{MedR} & R@1 & R@5 & R@10 & \multicolumn{1}{c|}{MedR} & R@1 & R@5 & R@10 & MedR \\ \midrule
\multicolumn{14}{c}{Fine-tune} \\ \midrule
\multicolumn{1}{l|}{clipBert \cite{lei2021less}} & \multicolumn{1}{c|}{COCO\cite{chen2015microsoft}, VG\cite{krishna2017visual}} & 20.4 & 48.0 & 60.8  & \multicolumn{1}{c|}{6} & 22.0 & 46.8 & 59.9 & \multicolumn{1}{c|}{6} & - & - & - & - \\
\multicolumn{1}{l|}{Frozen \cite{bain2021frozen}} & \multicolumn{1}{c|}{W2M+C3M} & 31.0 & 59.8 & 72.4 & \multicolumn{1}{c|}{3} & 31.0 & 59.5 & 70.5 & \multicolumn{1}{c|}{3} & 15.0 & 30.8 & 39.8 & 20 \\
\multicolumn{1}{l|}{VIOLET \cite{fu2021violet}} & \multicolumn{1}{c|}{W2M+C3M+Y180M}& 32.6 & 62.8 & 74.7 & \multicolumn{1}{c|}{-} & 34.5 & 63.0 & 73.4 & \multicolumn{1}{c|}{-} & 16.1 & 36.6 & 41.2 & - \\
\multicolumn{1}{l|}{HD-VILA \cite{Xue_2022_CVPR}} & \multicolumn{1}{c|}{HDV100M} & 28.8 & 57.4 & 69.1 & \multicolumn{1}{c|}{4} & 35.6 & 65.3 & 78.0 & \multicolumn{1}{c|}{3} & 17.4 & 34.1 & 44.1 & 15 \\

\multicolumn{1}{l|}{ALPRO \cite{li2022alignandprompt}} & \multicolumn{1}{c|}{W2M+C3M} & 35.9 & 67.5 & 78.8 & \multicolumn{1}{c|}{3} & 33.9 & 60.7 & 73.2 & \multicolumn{1}{c|}{3} & - & - & - & - \\
\multicolumn{1}{l|}{TMVM \cite{lin2022text}} & \multicolumn{1}{c|}{W2M+C3M} & 36.5 & 64.9 & 75.4 & \multicolumn{1}{c|}{3} & 36.2 & 64.2 & 75.7 & \multicolumn{1}{c|}{3} & 17.8 & 37.1 & 45.9 & 13.5 \\
\multicolumn{1}{l|}{All-in-1 \cite{wang2022all}} & \multicolumn{1}{c|}{W2M+H100M} & 32.7 & 61.4 & 73.5 & \multicolumn{1}{c|}{-} & 37.9 & 68.1 & 77.1 & \multicolumn{1}{c|}{-} & - & - & - & - \\
\multicolumn{1}{l|}{OA-Trans \cite{wang2022object}} & \multicolumn{1}{c|}{W2M+C3M}& 34.8 & 64.4 & 75.1& \multicolumn{1}{c|}{3}  & 35.8 & 63.4 & 76.5 & \multicolumn{1}{c|}{3} & 18.2 & 34.3 & 43.7 & 18.5 \\
\multicolumn{1}{l|}{MILES \cite{ge2022miles}} & \multicolumn{1}{c|}{W2M+C3M}& 36.6 & 63.9 & 74.0 & \multicolumn{1}{c|}{3}  & 37.7 & 63.6 & 73.8 & \multicolumn{1}{c|}{3} & 17.8 & 35.6 & 44.1 & 15.5 \\ 
\multicolumn{1}{l|}{MCQ \cite{ge2022bridging}} & \multicolumn{1}{c|}{W2M+C3M} & 37.0 & 62.2 & 73.9  & \multicolumn{1}{c|}{3} & 37.6 & 64.8 & 75.1 & \multicolumn{1}{c|}{3} & 17.9 & 35.4 & 44.5 & 15 \\ 

\midrule
\multicolumn{1}{l|}{Clover (ours) } & \multicolumn{1}{c|}{W2M+C3M} & \textbf{50.1} & \textbf{76.7} & \textbf{85.6} & \multicolumn{1}{c|}{\textbf{1}} & \textbf{40.5} & \textbf{69.8} & \textbf{79.4} & \multicolumn{1}{c|}{\textbf{2}} & \textbf{24.8} & \textbf{44.0} & \textbf{54.5} & \textbf{8} \\ \midrule

\multicolumn{14}{c}{Zero-shot} \\ \midrule
\multicolumn{1}{l|}{MIL-NCE \cite{miech2020end}} & \multicolumn{1}{c|}{H100M} & - & - & - & \multicolumn{1}{c|}{-}  & 9.9 & 24.0 & 32.4 & \multicolumn{1}{c|}{29.6}& - & - & - & - \\
\multicolumn{1}{l|}{Frozen \cite{bain2021frozen}} & \multicolumn{1}{c|}{W2M+C3M}  & 21.1 & 46.0 & 56.2 & \multicolumn{1}{c|}{7} & 18.7 & 39.6 & 51.6 & \multicolumn{1}{c|}{10} & 9.3 & 22.0 & 30.1 & 51.0 \\
\multicolumn{1}{l|}{VIOLET \cite{fu2021violet}} & \multicolumn{1}{c|}{W2M+C3M+Y180M} & 23.5 & 49.8 & 59.8 & \multicolumn{1}{c|}{-} & 25.9 & \textbf{49.5} & 59.7 & \multicolumn{1}{c|}{-} & - & - & - & - \\
\multicolumn{1}{l|}{ALPRO \cite{li2022alignandprompt}} & \multicolumn{1}{c|}{W2M+C3M}  & 23.8 & 47.3 & 57.9 & \multicolumn{1}{c|}{6} & 24.1 & 44.7 & 55.4 & \multicolumn{1}{c|}{8} & - & - & - & - \\
\multicolumn{1}{l|}{OA-Trans \cite{wang2022object}} & \multicolumn{1}{c|}{W2M+C3M}  & 23.5 & 50.4 & 59.8 & \multicolumn{1}{c|}{6} & 23.4 & 47.5 & 55.6 & \multicolumn{1}{c|}{8} & - & - & - & - \\
\multicolumn{1}{l|}{MILES \cite{ge2022miles}} & \multicolumn{1}{c|}{W2M+C3M}  & 27.2 & 50.3 & 63.6 &  \multicolumn{1}{c|}{5} & 26.1 & 47.2 & 56.9 & \multicolumn{1}{c|}{7} & 11.1 & 24.7 & 30.6 & 50.7 \\ 
\multicolumn{1}{l|}{MCQ \cite{ge2022bridging}} & \multicolumn{1}{c|}{W2M+C3M}  & 25.6 & 50.6 & 61.1 &  \multicolumn{1}{c|}{5} & 26.0 & 46.4 & 56.4 & \multicolumn{1}{c|}{7} & 12.2 & 25.9 & 32.2 & 42 \\ 
\midrule
\multicolumn{1}{l|}{Clover (ours)} & \multicolumn{1}{c|}{W2M+C3M}  & \textbf{29.5} & \textbf{55.2} & \textbf{66.3} & \multicolumn{1}{c|}{\textbf{4}} & \textbf{26.4} & \textbf{49.5} & \textbf{60.0} & \multicolumn{1}{c|}{\textbf{6}} & \textbf{14.7} & \textbf{29.2} & \textbf{38.2} & \textbf{24} \\ \bottomrule
\end{tabular}}
\caption{Text-to-video retrieval performance  comparison under {fine-tune} and {zero-shot} setups. Here {higher} R@k (Recall K) and {lower} MedR (Median Recall) indicate better performance. W2M, C3M, H100M, HDV100M, Y180M are short for WebVid2M\cite{bain2021frozen}, CC3M\cite{sharma2018conceptual}, HowTo100M\cite{miech2019howto100m},
HD-VILA-100M\cite{Xue_2022_CVPR},
YT-Temporal-180M\cite{zellers2021merlot}, respectively.}
\vspace{-0.2cm}
\label{tab:retrieval_SOTA}
\end{table*}

\section{Experiments}
\subsection{Experiment Setup}
\noindent  \textbf{Pre-training datasets}.
Following recent work \cite{bain2021frozen, li2022alignandprompt, ge2022bridging}, we jointly pre-train our Clover on a video dataset WebVid2M \cite{bain2021frozen} with 2.5M video-text pairs and an image dataset Google Conceptual Captions (CC3M) \cite{sharma2018conceptual} with 3.3M image-text pairs (we only obtain 2.8M image-text pairs in CC3M due to image url broken). During pre-training, we treat image data as one frame video data.

\noindent  \textbf{Downstream tasks}. We evaluate our proposed models on the following downstream tasks. (a) \textbf{Text-to-Video Retrieval} on MSRVTT \cite{xu2016msr}, LSMDC  \cite{maharaj2017dataset} and DiDeMo \cite{anne2017localizing}. For DiDeMo, we follow \cite{lei2021less, liu2019use} and evaluate Clover on the paragraph-to-video retrieval, where text sentences for each video are concatenated together as one text query. We do not use the ground-truth proposal for fair comparison with previous works;  (b) \textbf{Multiple-choice QA} on TGIF-Action \cite{jang2017tgif},  TGIF-Transition \cite{jang2017tgif}, MSRVTT-MC \cite{yu2018joint} and LSMDC-MC \cite{torabi2016learning}; (c) \textbf{Open-Ended QA} on TGIF-Frame~\cite{jang2017tgif}, MSRVTT-QA \cite{xu2017video}, MSVD-QA \cite{xu2017video} and LSMDC-FiB \cite{maharaj2017dataset}. For retrieval tasks, we only use the two uni-modal encoders of Clover for fine-tuning and inference. We adopt the two encoders to get the video and text embeddings, and calculate their cosine similarity for retrieval. For QA tasks, we use all the modules for fine-tuning and inference. More details on these datasets and their evaluation usage are provided in the 
supplementary material. 


\begin{table*}[t]
\centering
\resizebox{0.80\textwidth}{!}{
\begin{tabular}{l|c|ccc|cc|cc|c}
\toprule
\multicolumn{1}{l|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{Pre-training dataset} & \multicolumn{3}{c|}{TGIF} & \multicolumn{2}{c|}{MSRVTT} & \multicolumn{2}{c|}{LSMDC} & MSVD \\ \cline{3-10} 
\multicolumn{1}{c|}{} &  & Action & Transition & Frame & MC & QA & MC & FiB & QA \\ \midrule
clipBert \cite{lei2021less} & COCO, VG & 82.8 & 87.8 & 60.3 & 88.2 & 37.4 & - & - & - \\
JuskAsk \cite{yang2021just} & HTVQA69M\cite{yang2021just} & - & - & - & - & 41.5 & - & - & 46.3 \\
ALPRO \cite{li2022alignandprompt} & W2M+C3M & - & - & - & - & 42.1 & - & - & 45.9 \\
All-in-1 \cite{wang2022all} & W2M+H100M & 92.7 & 94.3 & 64.2 & 92.0 & 42.9 & 83.1 & - & 47.9 \\
VIOLET \cite{fu2021violet} & W2M+C3M+Y180M & 92.5 & 95.7 & 68.9 & 91.9 & 43.9 & 82.8 & 53.7 & 47.9 \\
MERLOT \cite{zellers2021merlot} & Y180M & 94.0 & 96.2 & 69.5 & 90.9 & 43.1 & 81.7 & 52.9 & - \\ \midrule
Clover (ours) & W2M+C3M & \textbf{95.0} & \textbf{98.2} & \textbf{71.6} & \textbf{95.2} & \textbf{44.1} & \textbf{83.7} & \textbf{54.1} & \textbf{52.4} \\ \bottomrule
\end{tabular}}
\caption{Performance comparison on transferring to downstream video question answering tasks.}
\vspace{-0.4cm}
\label{tab:vqa_sota}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{0.90\textwidth}{!}{
\begin{tabular}{l|cccc|cccc|c|c}
\toprule
\multicolumn{1}{l|}{\multirow{2}{*}{Method}} & \multicolumn{4}{c|}{DiDeMo} & \multicolumn{4}{c|}{MSRVTT} & LSMDC-MC & TGIF-Frame \\ \cmidrule{2-11} 
\multicolumn{1}{c|}{} & R@1 & R@5 & R@10 & \multicolumn{1}{l|}{MedR} & R@1 & R@5 & \multicolumn{1}{l}{R@10} & \multicolumn{1}{l|}{MedR} & Acc & Acc \\ \midrule
Baseline  & 22.6 & 47.9 & 58.2 & 7 & 19.8 & 41.7 & 51.1 & 10 & 78.8 & 68.9 \\
+ TMA  & 24.7 & 49.7 & 60.0 & 6 & 22.7 & 42.2 & 52.2 & 9 & 80.0 & 69.3 \\
+ TMA+SM  & 25.3 & 49.5 & 60.5 & 6 & 22.5 & 42.7 & 52.0 & 9 & 80.5 & 69.4 \\
+ TMA+SM+RankL  & \textbf{26.4} & \textbf{51.1} & \textbf{61.3} & \textbf{5} & \textbf{23.4} & \textbf{43.3} & \textbf{52.4} & \textbf{9} & \textbf{80.7} & \textbf{69.7} \\ \bottomrule
\end{tabular}}
\caption{Effects of our pre-training tasks for Clover. We report \emph{zero-shot} text-video retrieval performance on DiDeMo and MSRVTT, and \emph{fine-tune} video QA performance on LSMDC-MC and TGIF-Frame. TMA, SM and RankL:  tri-modal alignment, semantic masking strategy and pair-wise ranking loss. }
\vspace{-0.2cm}
\label{tab:ablation}
\end{table*}

\begin{table}[]
\centering
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{l|c|c}
\toprule
Method & \begin{tabular}[c]{@{}c@{}}Averaged \textbf{similarity scores} \\ of positive pairs\end{tabular} & \begin{tabular}[c]{@{}c@{}}Averaged \textbf{similarity margin} between \\ positive and negative samples\end{tabular} \\ \midrule
w/o TMA & 0.56 & 0.30 \\ w TMA & 0.65 & 0.35 \\ \bottomrule
\end{tabular}}
\caption{Affect of TMA on the video and text representations in learned embedding space. The experiment is conducted on MSRVTT-1kA test set under zero-shot setting.}
\vspace{-0.5cm}
\label{tab:avg cos_sim}
\end{table}


\noindent  \textbf{Implementation details}.
\label{sec:implementation details}
Following \cite{fu2021violet}, we initialize the video encoder with a VideoSwin-Base \cite{liu2022video} model pre-trained on Kinetics-400 \cite{kay2017kinetics}. The text encoder is initialized from pre-trained Bert-Base~\cite{devlin2019bert}. The multi-modal encoder is initialized from the first three layers of the pre-trained Bert-Base model. We train Clover end-to-end   during both pre-training and fine-tuning. We pre-train Clover for 40 epochs, using a batch size of 1024 on 64 NVIDIA A100 GPUs. We use AdamW \cite{loshchilov2018decoupled} optimizer with a weight decay 0.005 and betas (0.9, 0.98). The learning rate is first warmed-up by 4 epochs to 5e-5 and then decays following a cosine annealing decay schedule. We resize all the video frames to 224  224 and split each frame into patches with a size of 32  32. We choose hyper-parameter ,  and . We set dimension  in our model. For each video, we randomly sample 8 frames while preserving their order in-between. For fine-tuning on retrieval tasks, we only fine-tune the uni-modal encoders of Clover with InfoNCE \cite{van2018representation} loss. For fine-tuning on video QA task, we add a simple MLPs that takes the multi-modal [CLS] embedding as input for classification, and optimize the whole Clover model with cross-entropy loss. During fine-tuning, we follow the conventional set-up in \cite{fu2021violet,luo2021clip4clip}, and all the fine-tuning experiments are performed on 8 NVIDIA A100 GPUs.\footnote{We provide our codes in the supplementary material and will release the codes to facilitate reproduction of our work.}




\begin{table*}[h]
\centering
\resizebox{0.90\linewidth}{!}{
\begin{tabular}{l|cc|ccc|c|c|c|c}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Model Architecture} & \multicolumn{3}{c|}{Training Objectives} & MSRVTT & DiDeMo & TGIF-Frame & LSMDC-MC \\ \cmidrule{2-10} 
 & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Two uni-modal \\ encoder\end{tabular}} & \begin{tabular}[c]{@{}c@{}}Cross-modal \\ encoder\end{tabular} & \multicolumn{1}{c|}{InfoNCE} & \multicolumn{1}{c|}{MLM} & \begin{tabular}[c]{@{}c@{}}TMA +\\  Rankloss\end{tabular} & R@1 & R@1 & Acc & Acc \\ \midrule
IND-A & \multicolumn{1}{c|}{\Checkmark} &  & \multicolumn{1}{c|}{\Checkmark} & \multicolumn{1}{c|}{} &  & 20.7 & 22.9 & - & - \\
IND-F & \multicolumn{1}{c|}{\Checkmark} & \Checkmark & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\Checkmark} &  & - & - & 69.0 & 79.0 \\
COMB & \multicolumn{1}{c|}{\Checkmark} & \Checkmark & \multicolumn{1}{c|}{\Checkmark} & \multicolumn{1}{c|}{\Checkmark} &  & 19.8 & 22.6 & 68.9 & 78.8 \\
Clover & \multicolumn{1}{c|}{\Checkmark} & \Checkmark & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\Checkmark} & \Checkmark & \textbf{23.4} & \textbf{26.4} & \textbf{69.7} & \textbf{80.7} \\ \bottomrule
\end{tabular}}
\caption{Comparisons with task-independently trained models (\emph{IND}) and naive combination of the uni- and cross- modal encoder (\emph{COMB})}
\vspace{-0.3cm}
\label{tab:ablation_comb_ind}
\end{table*}

\subsection{Comparing to State-of-the-art}
\noindent  \textbf{Text-to-video retrieval}. 
Tab.~\ref{tab:retrieval_SOTA} shows the retrieval results on the DiDeMo \cite{anne2017localizing}, MSRVTT \cite{xu2016msr} and LSMDC \cite{maharaj2017dataset} datasets with both zero-shot and fine-tuning settings. Our Clover surpasses previous works on all the datasets by a large margin. The significant performance gain under zero-shot evaluation demonstrates its stronger generalization ability. Specifically, Clover brings  gain in R@10 over MCQ \cite{ge2022bridging},  the current SOTA method on retrieval tasks with the  same pre-train data, averaged on the three datasets. Moreover, Clover outperforms VIOLET~\cite{fu2021violet}, though VIOLET adopts a considerably larger pre-training  dataset (YTT180M \cite{zellers2021merlot} of 180M visual-text pairs vs.\ WebVid2M+CC3M of only 5.5M samples).  Clover also presents superior  performance when fine-tuned on the downstream datasets, compared with the SOTAs. On all three datasets, Clover surpasses previous works by a large margin across all the metrics and obtains an average improvement of  on R@10. Moreover, different from some  previous work~\cite{lei2021less, fu2021violet, li2022alignandprompt} that adopts a joint multi-modal encoder and  requires exhaustively pairing every video with every text during retrieval, Clover directly deploys the video encoder and text encoder for retrieval tasks,  making it  much more efficient.


\noindent  \textbf{Video question answering}.
Tab.~\ref{tab:vqa_sota} reports the results of Clover and current   SOTAs on four video QA datasets. Though using much less pre-training data, Clover outperforms another unified VidL model \emph{i.e.,} VIOLET \cite{fu2021violet}  significantly. Moreover, compared to the current SOTA method MERLOT\cite{zellers2021merlot}, Clover brings  improvements of 1.0\% on TGIF-Action, 2.0\% on TGIF-Transition, 2.1\% on TGIF-Frame, 4.3\% on MSRVTT-MC, 1.0\% on MSRVTT-QA, 2.0\% on LSMDC-MC and 1.2\% on LSMDC-FiB. Note that JustAsk \cite{yang2021just} and MERLOT~\cite{zellers2021merlot} are specifically designed for video QA and trained on orders of magnitude larger datasets (\emph{e.g.,} HTVQA69M \cite{yang2021just}, YTT180M \cite{zellers2021merlot}). The experiment results again confirm the superiority of Clover.

\subsection{Analysis}
We conduct ablation experiments on two retrieval datasets (DiDeMo and MSRVTT) and two Video QA datasets (TGIF-Frame and LSMDC-MC). Due to the computational resource limit, we randomly sample 1 million video-text pairs  from WebVid2M \cite{bain2021frozen}    to build the WebVid-1M subset  for model pre-training under   all the ablation studies. We keep the model architecture unchanged but pre-train the model with only MLM and InfoNCE losses as a baseline, which  is similar to the baseline adopted in VIOLET \cite{fu2021violet}. The only difference is that we replace the ITM applied to the cross-modal encoder outputs in VIOLET \cite{fu2021violet} with InfoNCE applied to the outputs of uni-modal encoders and separate the text encoder from the cross-modal encoder. All the ablation experiments are conducted with batch size 1024 on 32 NVIDIA A100 GPUs.









\noindent  \textbf{Effect of tri-modal alignment}. Tri-modal alignment (TMA) aims to better correlate cross-modal alignment and   fusion. To evaluate the contribution of TMA, we remove the pair-wise ranking objective and semantic masking strategy, and employ the classical MLM task as in the baseline method. Compared with the baseline, TMA explicitly associates the outputs of uni-modal encoders with the outputs of the multi-modal encoder. As shown in Tab.~\ref{tab:ablation}, the model trained with TMA outperforms the baseline on both retrieval and video QA, demonstrating its effectiveness. For a better understanding of the effect of TMA, we further report averaged cosine similarity of videos and texts in MSRVTT dataset (in zero-shot setting) calculated by models trained with/without TMA, respectively.  For a fair comparison, we select all video queries (197 in total) that were successfully responded to by both models. We recall 100 results for each query, and take the ground-truth pairs as positive and others as negative. 
As shown in  Tab.~\ref{tab:avg cos_sim}, the model armed with tri-modal alignment assigns higher similarity scores to positive pairs compared to the model without it. Meanwhile, with  tri-modal alignment, the margin between the positive and negative samples is also increased. It reveals that with the help of tri-modal alignment, the distance between video and text that contain consistent semantics is  reduced, while the margin between the misaligned samples is increased. The results demonstrate that with fused multi-modal representation as anchors, tri-modal alignment helps the video and language modalities to be better aligned.

\begin{figure}[tb]
  \centering
  \includegraphics[width=1\columnwidth]{figure/vis_ret_arial}
 \vspace{-0.3cm}
  \caption{Qualitative results of zero-shot video to text retrieval results on MSRVTT \cite{xu2016msr}.}
   \label{fig:vis_ret}
   \vspace{-1\baselineskip}
\end{figure}


\noindent \textbf{Effect of semantic masking strategy}. We incorporate a semantic masking strategy to form the masked pairs in tri-modal alignment task.  As shown in Tab.~\ref{tab:ablation}, on more diverse evaluation benchmarks, e.g. DiDemo,  where each video is paired with 5 different ground-truth texts, using masked samples brings more gains. The result reveals that the proposed semantic masking strategy facilitates the model to capture key semantic information in more complex scenes and makes it achieve better results.


\noindent \textbf{Effect of pair-wise ranking}. To make the model capture the semantic information difference between the masked pairs and complete pairs, we adopt the pair-wise rank loss. As shown in Tab.~\ref{tab:ablation}, with pair-wise ranking loss, the performance of the model is further improved. We also show the visualization results below to further illustrate the effect of pair-wise ranking.

\noindent \textbf{Clover makes cross-modal alignment and fusion mutually improving}. We compare Clover with the ones that use the same architecture but employ different pre-training tasks in Tab.~\ref{tab:ablation_comb_ind}. \emph{IND-A} represents the model only trained with InfoNCE loss for cross-modal alignment. \emph{IND-F} indicates the model trained with MLM loss for cross-modal fusion. We also report results achieved by the model (\emph{i.e., COMB}) that simply combines the uni-modal encoders and multi-modal encoder as well as the training objectives. We can see that the simple combination   hurts the performance, while our Clover achieves superior performance than the task-independently trained \emph{IND-A} and \emph{IND-F}. It reveals that Clover is able to get the model's cross-modal alignment and fusion capability mutually enhanced. 

\begin{figure}[tb]
  \centering
  \includegraphics[width=1\columnwidth]{figure/vis_qa}
  \caption{Qualitative results of video question answering results on MSRVTT-QA \cite{xu2017video}.}
   \label{fig:vis_vqa}
   \vspace{-1\baselineskip}
\end{figure}

\noindent \textbf{Qualitative analysis}. 
In Fig.~\ref{fig:vis_ret} and Fig.~\ref{fig:vis_vqa}, we show the qualitative results of Clover and the baseline method on zero-shot video-text retrieval and VQA tasks. Specifically, we present the query videos with the matched texts, and the video-question pair with the answer. Clover empowers the model with stronger video-text understanding capability. 
For example, for the second video in Fig.~\ref{fig:vis_ret}, Clover returns the text with ``paper airplane", which is a more accurate description of the video content; for the first video in Fig.~\ref{fig:vis_vqa}, Clover generates a more accurate answer ``Lion" than the baseline answer ``Animal".
The results demonstrate the superiority of Clover.


\vspace{-0.2cm}
\section{Conclusion} \label{sec:conclusion}

In this paper, we present Clover, a new end-to-end Video-Language pre-training method for both high-efficiency video-text retrieval and video question answering. Clover introduces a novel Tri-modal Alignment task to better  align the representations from visual, text and fused modalities, which explicitly correlate the uni-modal encoder and multi-modal encoder.  It also introduces semantic masking strategy and pair-wise ranking loss to further improve the cross-modality modal training.  Extensive experiments conducted on the three retrieval datasets and eight video QA datasets clearly demonstrated, as a general video-text model, its   consistent superiority  for video-text understanding.  


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
