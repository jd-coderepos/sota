\section{Experiments}

We use the Visual Storytelling Dataset~\cite{huang2016visual}, consisting of 10,000 albums with 200,000 photos.
Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos. 

\subsection{Story Generation}

This task is to generate a 5-sentence story describing an album.
We compare our model with two sequence-to-sequence baselines: 1) an encoder-decoder model (enc-dec), where the sequence of album photos is encoded and the last hidden state is fed into the decoder for story generation,
2) an encoder-attention-decoder model~\cite{xu2015show} (enc-attn-dec) with weights computed using a soft-attention mechanism.
At each decoding time step, a weighted sum of hidden states from the encoder is decoded. For fair comparison, we use the same album representation (Sec.~\ref{sec:album_encoder}) for the baselines.

\begin{table}[t]
\footnotesize
\begin{center}
\begin{tabular}{l|c c c c}
\multicolumn{5}{c}{beam size=3}\\
\hline
& Bleu3 & Rouge & Meteor & CIDEr \\
\hline
enc-dec & 19.58 & 29.23 & 33.02 & 4.65 \\
enc-attn-dec & 19.73 & 28.94 & 32.98 & 4.96 \\
h-attn & 20.53 & 29.82 & 33.81 & 6.84 \\
h-attn-rank & \bf{20.78} & \bf{29.82} & \bf{33.94} & \bf{7.38} \\
\hline
h-(gd)attn-rank & 21.02 & 29.53 & 34.12 & 7.51\\
\hline
\end{tabular}
\end{center}
\vspace{-.4cm}
\caption{Story generation evaluation.}
\vspace{-.1cm}
\label{table:generation}
\end{table}



\begin{table}[t]
\footnotesize
\begin{center}
\begin{tabular}{l | l}
\hline
enc-dec (29.50\%) & h-attn-rank (70.50\%)\\
\hline
enc-attn-dec (30.75\%) & h-attn-rank (69.25\%) \\
\hline
\hline
h-attn-rank (30.50\%) & gd-truth (69.50\%) \\ 
\hline
\end{tabular}
\end{center}
\vspace{-.4cm}
\caption{Human evaluation showing how often people prefer one model over the other.}
\vspace{-.2cm}
\label{table:human}
\end{table}

\begin{table}[t]
\footnotesize
\begin{center}
\begin{tabular}{l | c c }
\hline
& precision & recall \\
\hline
DPP & 43.75\% & 27.41\% \\
\hline
enc-attn-dec & 38.53\% & 24.25\% \\
\hline
h-attn 		 & 42.85\% & 27.10\% \\
\hline
h-attn-rank  & \bf{45.51}\% & \bf{28.77}\% \\
\hline
\end{tabular}
\end{center}
\vspace{-.4cm}
\caption{Album summarization evaluation.}
\vspace{-.1cm}
\label{table:summarization}
\end{table}

\begin{table}[t]
\footnotesize
\begin{center}
\begin{tabular}{l | c c c c}
\hline
& R@1 & R@5 & R@10 & MedR \\
\hline
enc-dec      & 10.70\% & 29.30\% & 41.40\% & 14.5 \\
\hline
enc-attn-dec & 11.60\% & 33.00\% & 45.50\% & 11.0 \\
\hline  
h-attn       & 18.30\% & \bf{44.50}\% & \bf{57.60}\% & \bf{6.0} \\
\hline
h-attn-rank  & \bf{18.40}\% & 43.30\% & 55.50\% & 7.0 \\
\hline
\end{tabular}
\end{center}
\vspace{-.4cm}
\caption{1000 album retrieval evaluation.}
\vspace{-.1cm}
\label{table:retrieval}
\end{table}



We test two variants of our model trained with and without ranking regularization by controlling $\lambda$ in our loss function, denoted as h-attn (without ranking), and h-attn-rank (with ranking).
Evaluations of each model are shown in Table~\ref{table:generation}.
The h-attn outperforms both baselines, and h-attn-rank achieves the best performance for all metrics.
Note, we use beam-search with beam size=3 during generation for a reasonable performance-speed trade-off (we observe similar improvement trends with beam size = 1).\footnote{We also compute the $p$-value of Meteor on 100K samples via the bootstrap test~\cite{efron1994introduction}, as Meteor has better agreement with human judgments than Bleu/Rouge~\cite{huang2016visual}.
Our h-attn-rank model has strong statistical significance ($p=0.01$) over the enc-dec and enc-attn-dec models (and is similar to the h-attn model).}
To test performance under optimal image selection, we use one of the two ground-truth human-selected 5-photo-sets as an oracle to hard-code the photo selection, denoted as h-(gd)attn-rank.
This achieves only a slightly higher Meteor compared to our end-to-end model.

Additionally, we also run human evaluations in a forced-choice task where people choose between stories generated by different methods.
For this evaluation, we select 400 albums, each evaluated by 3 Turkers. Results are shown in Table~\ref{table:human}. 
Experiments find significant preference for our model over both baselines. As a simple Turing test, we also compare our results with human written stories (last row of Table~\ref{table:human}), indicating room for improvement of methods.


\subsection{Album Summarization}
We evaluate the precision and recall of our generated summaries (output by the photo selector) compared to human selections (the combined set of both human-selected 5-photo stories). 
For comparison, we evaluate enc-attn-dec on the same task by aggregating predicted attention and selecting the 5 photos with highest accumulated attention.
Additionally, we also run DPP-based video summarization~\cite{kulesza2012determinantal} using the same album features.
Our models have higher performance compared to baselines as shown in Table~\ref{table:summarization} (though DPP also achieves strong results, indicating that there is still room to improve the pointer network).

\vspace{-0.2cm}
\subsection{Output Example Analysis}
Fig.~\ref{fig:generation_a} and Fig.~\ref{fig:generation_b} shows several output examples of the joint album summarization and storytelling generation.
We compare our full model h-attn-rank with the baseline enc-attn-dec, as both models are able to do the album summarization and story generation tasks jointly.
In Fig.~\ref{fig:generation_a} and Fig.~\ref{fig:generation_b}, we use blue dashed box and red box to indicate the album summarization by the two models respectively.
As reference, we also show the ground-truth album summaries by randomly selecting 1 out of 2 human album summaries, which are highlighted with green box.
Below each album are their generated stories.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.98\textwidth]{figures/all_example_with_gd.jpg}
\caption{Examples of album summarization and storytelling by enc-attn-dec (blue), h-attn-rank (red), and ground-truth (green). We randomly select 1 out of 2 human album summaries as ground-truth here.}
\label{fig:generation_a}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.98\textwidth]{figures/all_example_with_gd2.jpg}
\caption{More examples of album summarization and storytelling by enc-attn-dec (blue), h-attn-rank (red), and ground-truth (green). We randomly select 1 out of 2 human album summaries as ground-truth here.}
\label{fig:generation_b}
\end{figure*}


\subsection{Album Retrieval}
Given a human-written story, we introduce a task to retrieve the album described by that story. 
We randomly select 1000 albums and one ground-truth story from each for evaluation. 
Using the generation loss, we compute the likelihood of each album $A_m$ given the query story $S$ and retrieve the album with the highest generation likelihood, $A = \mbox{argmax}_{A_m} p(S|A_m)$.
We use Recall@k and Median Rank for evaluation.
As shown in Table~\ref{table:retrieval}), we find that our models outperform the baselines, but the ranking term in Eqn.\ref{eqn:loss} does not improve performance significantly.

\section{Conclusion}
\vspace{-0.3cm}
Our proposed hierarchically-attentive RNN based models for end-to-end visual storytelling can jointly summarize and generate relevant stories from full input photo albums effectively. Automatic and human evaluations show that our method outperforms strong sequence-to-sequence baselines on selection, generation, and retrieval tasks.

