










\documentclass[runningheads]{llncs}
\usepackage{graphics,latexsym}

\usepackage{graphicx,epsfig,color}
\usepackage{graphicx}
\usepackage{enumerate}



\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\scrod}{\quad\nopagebreak}


\newcommand{\Opt}{{\rm Optimal}}
\newcommand{\App}{{\rm Approximate}}
\newcommand{\adiameter}{{\rm avediameter}}
\newcommand{\aveAllPair}{{\rm S}}
\newcommand{\dist}{{\rm dist}}
\newcommand{\timecomplexity}{O({n(\log\log n)\over \sum_{i=1}^n a_i})}
\newcommand{\prob}{{\rm Pr}}
\newcommand{\appsum}{{\rm apx\_sum}}


\begin{document}

\date{}


\title{Sublinear Time Approximate Sum via Uniform Random Sampling}
\author{Bin Fu\inst{1}\and Wenfeng Li\inst{2} \and Zhiyong Peng\inst{2}}
 \institute{{Department of Computer
Science\\
 University of Texas-Pan American,
 Edinburg, TX 78539, USA.\\
\email{binfu@cs.panam.edu}} \and {Computer School\\
Wuhan University,
 Wuhan, P. R. China\\
 \email{eyestar\_2008@126.com, peng@whu.edu.cn}} } \maketitle






\begin{abstract} We investigate the approximation for computing
the sum  with  an input of a list of nonnegative
elements . If all elements are in the range
, there is a randomized algorithm that can compute an
-approximation for the sum problem in time
, where  is a constant in . Our
randomized algorithm is based on the uniform random sampling, which
selects one element with equal probability from the input list each
time. We also prove a lower bound , which almost matches the upper bound, for this problem.
\end{abstract}



\centerline {{\bf Key words:}  Randomization;   Approximate Sum;
Sublinear Time.}

\section{Introduction}

Computing the sum of a list of elements has many applications. This
problem can be found in the high school textbooks. In the textbook
of calculus, we often see how to compute the sum of a list of
elements, and decide if it converges when the number of items is
infinite. Let  be a real number at least  . Real number
 is an -approximation for the sum problem
 if . When we have a huge number of data
items and need to compute their sum, an efficient approximation
algorithm becomes essential. Due to the fundamental importance of
this problem, looking for the sublinear time solution for it is an
interesting topic of research.

A similar problem is to compute the mean of a list of items
, whose mean is defined by
. Using  random samples, one can compute the
-approximation for the mean, or decides if it is at
most ~\cite{Hoefding63}. In~\cite{CanettiEvenGoldreich95},
Canetti, Even, and Goldreich showed that the sample size is tight.
In~\cite{MotwaniPanigrahyXu07}, Motwani, Panigrahy, and Xu showed an
 time approximation scheme for computing the sum of 
nonnegative elements.  A priority sampling approach for estimating
subsets were studied
in~\cite{AlonDuffieldLundThorup05,DuffieldLundThorup05,BroderFonturaJosifovskiKumarMotwaniNabarPanigrahyTomkinsXu06}.
Using  different cost and application models,  they tried to build a
sketch so that the sum of any subset can be computed approximately
via the sketch.

We feel the uniform sampling is more justifiable than the weighted
sampling. In this paper, we study the approximation for the sum
problem under both deterministic model and randomized model.  In the
randomized model, we still use the uniform random samplings, and
show how the time is reversely depend on the total sum . We also prove a lower bound that matches this time bound. An
algorithm of time complexity  for computing a list
of nonnegative elements  in  can be extended
to a general list of nonnegative elements. It implies an algorithm
of time complexity  for computing
a list of nonnegative elements of size at most  by converting
each  into , which is always in the range
.













\section{Randomized Algorithm for the Sum Problem}

In this section, we present a randomized algorithm for computing the
approximate sum of a list of numbers in .

\subsection{Chernoff Bounds}\label{chernoff-sec}
 The analysis of our randomized
algorithm often use  the well known Chernoff bounds, which are
described below. All proofs of this paper are self-contained except
the following famous theorems in probability theory.

\begin{theorem}[\cite{MotwaniRaghavan00}]\label{chernoff-theorem}
Let  be  independent random - variables,
where  takes  with probability . Let , and . Then for any ,
\begin{enumerate}
\item , and
\item
.
\end{enumerate}
\end{theorem}




We follow the proof of Theorem~\ref{chernoff-theorem} to make the
following versions (Theorem~\ref{ourchernoff2-theorem}, and
Theorem~\ref{chernoff3-theorem}) of Chernoff bound for our algorithm
analysis.



\begin{theorem}\label{chernoff3-theorem}
Let  be  independent random - variables,
where  takes  with probability at least  for . Let , and . Then for any
,
 .
\end{theorem}

\begin{theorem}\label{ourchernoff2-theorem}
Let  be  independent random - variables,
where  takes  with probability at most  for . Let . Then for any ,
.
\end{theorem}

Define  and
. Define
. We note that
 and  are always strictly less than 
for all . It is trivial for . For
, this can be verified by checking that the  function
 is decreasing and . This is because
 which is strictly less than  for all .
Thus,  is also decreasing, and less than  for all
.



\subsection{A Sublinear Time Algorithm}

In this section, we show an algorithm to compute the approximate sum
in a sublinear time in the cases that  is at least
 for any constant . This is a
randomized algorithm with uniform random sampling.

\begin{theorem}\label{main-thm}
Let  be a positive constant in . There is a
sublinear time algorithm such that given  a list of items
 in , it gives a
-approximation in the time .
\end{theorem}


\begin{definition}\label{partition-def}\scrod
\begin{itemize}
\item
For each interval  and a list of items , define  to be
the number of items of  in .

\item
For , and  in , a {\it
-partition} for  divides the interval
 into intervals 
 such that  for , and 
is the first element .

\item
For a set ,  is the number of elements in . For a list
 of items,  is the number of items in .
\end{itemize}
\end{definition}



A brief description of the idea is presented before the formal
algorithm and its proof. In order to get an
-approximation for the sum of  input numbers in the
list , a parameter  is selected with . For a -partition  for , Algorithm Approximate-Sum
below gives the estimation for the number of items in each  if
interval  has a sufficient number of items. Otherwise, those
items in  can be ignored without affecting much of the
approximation ratio. We have an adaptive way to do random samplings
in a series of phases. Let  denote the number of random samples
in phase . Phase  doubles the number of random samples of
phase  (). Let  be the input list of items in
the range . Let  be the number items in  from the
samples. For each phase, if an interval  shows sufficient
number of items from the random samples, the number of items
 in  can be sufficiently approximated by
. Thus, 
also gives an approximation for the sum of the sizes of items in
. The sum  for those
intervals  with large number of samples gives an approximation
for the total sum  of the input list.  In the early
stages,  is much smaller than . Eventually,
 will surpass . This happens when  is
more than  and  is close to the
sum  of all items from the input list. This
indicates that the number of random samples is sufficient for
approximation algorithm. For those intervals with small number of
samples, their items only form a small fraction of the total sum.
This process is terminated when ignoring all those intervals with
none or small number of samples does not affect much of the accuracy
of approximation. The algorithm gives up the process of random
sampling when  surpasses , and switches to use a
deterministic way to access the input list, which happens when the
total sum of the sizes of input items is .

The computation time at each phase  is . If phase  is
the last phase, the total time is , which is close to . Our final complexity upper bound is , where
 factor is caused by the probability amplification of
 stages and  intervals of the
 partition in the randomized algorithm.


\vskip 10pt





 {\bf Algorithm Approximate-Sum}


Input: a parameter, a small parameter , a failure
probability upper bound , an integer , a list  of 
items  in .




Steps:


\begin{enumerate}[1.]

\item
Phase :
\item\label{parameters-phase0}
\qquad Select   that satisfies
.

\item
\qquad Let  be a -partition  for .

\item\label{xi0-setting}
\qquad Let  be a parameter such that  for all large .

\item\label{first-alpha-setting}
\qquad Let .

\item\label{constant-setting-in-Approximate-Intervals}
\qquad Let parameters , and
.


\item
\qquad Let .

\item
End of Phase .

\item
Phase :



\item\label{loop-m-start}
\qquad Let .

\item
\qquad Sample  random items  from
the input list .

\item
\qquad Let  for
.




\item\label{I-j-loop}
\qquad For each ,

\item
\qquad\qquad if ,

\item\label{assign-hat-C}
\qquad\qquad then let  to
approximate .

\item\label{I-j-loop-end}
\qquad\qquad else let .



\item\label{loop-m-end}
\qquad Let  to
approximate .

\item\label{until-condition}
\qquad If  and  then
enter Phase .

\item
\qquad else

\item
\qquad\qquad If 

\item
\qquad\qquad then let 
to approximate .

\item
\qquad\qquad else let .


\item
\qquad\qquad Output  and terminate the algorithm.

\item
End of Phase .
\end{enumerate}


{\bf End of Algorithm}

\vskip 10pt

Several lemmas will be proved in order to show the performance of
the algorithm.  Let , and  be parameters
defined as those in the Phase 0 of the algorithm
Approximate-Sum.




\begin{lemma}\label{prelimary-lemma}\scrod
\begin{enumerate}
\item\label{k-bound}
For parameter  in , a -partition for
 has the number of intervals .

\item\label{g(x)-bound}
 when .

\item\label{x0-bound} The parameter
 can be set to be 
for line~\ref{xi0-setting} in the algorithm Approximate-Sum(.).

\item\label{g(x)-decreasing}
Function  is decreasing and  for every .
\end{enumerate}
\end{lemma}

\begin{proof} Statement~\ref{k-bound}:
The number of intervals  is the least integer with
. We have .

Statement~\ref{g(x)-bound}: By definition
, where  and
. We just need to prove that
 when . By Taylor
theorem . Assume .
We have


Statement~\ref{x0-bound}: We need to set up  to satisfy the
condition in line line~\ref{xi0-setting} in the algorithm. It
follows from statement~\ref{k-bound} and statement~\ref{g(x)-bound}.

Statement~\ref{g(x)-decreasing}: It follows from the fact that
 is decreasing, and less than  for each . We already
 explained in section~\ref{chernoff-sec}.
\end{proof}





We use the uniform random sampling to approximate the  number of
items in each interval  in the -partition. Due
to the technical reason, we estimate the failure probability instead
of the success probability.


\begin{lemma}\label{lemma.1}
Let  be the probability that the following statement is false
at the end of each phase:

(i) For each interval  with , .

Then for each phase in the algorithm, .
\end{lemma}


\begin{proof}
 An element of  in  is sampled (by an uniform sampling)
with probability . Let .
 For each interval  with , we discuss two
cases.

\begin{itemize}
\item
Case 1. .

In this case, . Note that  is
the number of elements in interval  among  random samples
 from . By
Theorem~\ref{ourchernoff2-theorem} (with ), with
probability at most , there are at least  samples
are from interval . Thus, the probability is at most  for
the condition of Case 1 to be true.

\item
Case 2. .

 By
Theorem~\ref{ourchernoff2-theorem}, we have .



By Theorem~\ref{chernoff3-theorem}, we have .


For each interval  with  and , we have  by line~\ref{assign-hat-C} in
Approximate-Sum(.).

There are  intervals . Therefore, with
probability at most ,
 the following is false: For each interval 
with , .
\end{itemize}

 By the
analysis of Case 1 and Case 2, we have   (see
statement~\ref{g(x)-decreasing} of Lemma~\ref{prelimary-lemma}).
Thus, the lemma has been proven.
\end{proof}




\begin{lemma}\label{lemma.2}
Assume that . Then
right after executing Phase  in Approximate-Sum, with
probability at most , the
following statement is false:

(ii) For each interval  with , A). ; and B). .
\end{lemma}


\begin{proof}
Assume that .
Consider each interval  with . We have that . An element of  in  is sampled with probability
.  By Theorem~\ref{ourchernoff2-theorem},
Theorem~\ref{chernoff3-theorem}, and Phase 0 of Approximate-Sum(.),
we have


Therefore, with probability at most ,
the following statement is false:

For each interval  with ,
.



If , then we have

\end{proof}






\begin{lemma}\label{lemma.3}
The total sum of the sizes of items in those s with  is at most .
\end{lemma}


\begin{proof} By Definition~\ref{partition-def}, we have
 for . We have that
\begin{itemize}
\item
the sum of sizes of items in  is at most ,
\item
for each interval  with , the
sum of sizes of items in  is at most  for .
\end{itemize}
The total sum of the sizes of items in those s with   is at most

\end{proof}



\begin{lemma}\label{lemma.4}
Assume that at the end of phase , for each  with
, ; and  if . Then  at the end of phase .
\end{lemma}

\begin{proof} By the assumption of the lemma, we have . For each interval  with ,
we have  by the
definition of -partition.  Thus,


By the condition of this lemma and Lemma~\ref{lemma.3},we have



 We have the following inequalities:

\end{proof}






\begin{lemma}\label{lemma.5}
With probability at most ,  at least one of the following statements is false:
\begin{enumerate}[A.]
\item\label{clm5-a}
For each phase  with ,  the condition  in
line~\ref{until-condition} of the algorithm is true.
\item\label{clm5-b}
If , then the algorithm stops some phase 
with .

\item\label{clm5-c}
If , then it stops  at a phase  in which the
condition  first becomes true, and outputs
.
\end{enumerate}
\end{lemma}

\begin{proof}
By Lemma~\ref{lemma.1}, with probability at most , the statement i of Lemma \ref{lemma.1} is
false for a fixed . The number of phases is at most 
since  is double at each phase. With probability , the statement i of Lemma
\ref{lemma.1} is false for each phase  with . Assume
that statement i of Lemma \ref{lemma.1} is true for every phase 
executed by the algorithm Approximate-Sum(.).

Statement~\ref{clm5-a}.  Assume that . We have .
Therefore, .


Since statement i of Lemma \ref{lemma.1} is true,  the condition of
Lemma~~\ref{lemma.4} is satisfied.  By Lemma~\ref{lemma.4},
.
 Since  (by line~\ref{constant-setting-in-Approximate-Intervals} in Approximate-Sum(.)), we have

Statement~\ref{clm5-b}. The variable  is doubled in each new
phase.

Assume that the algorithm enters phase  with .  We have

 Since
, .


By Lemma~\ref{lemma.4}, we have the inequality

By the setting at Phase 0 of the algorithm, we have

We have

Thus, it makes the condition at line~\ref{until-condition} in
Approximate-Sum(.) be false. Thus, the algorithm stops at some stage
 with  by the
setting at line~\ref{until-condition} in Approximate-Sum(.).

 Statement~\ref{clm5-c}. It follows from statement A and the
setting in line~\ref{until-condition} of the algorithm.
\end{proof}


\begin{lemma}\label{lemma.6}
The complexity of the algorithm is . In particular, the complexity is
 if 
 is fixed in .
\end{lemma}


\begin{proof} We check the size  of random samplings according by statement~\ref{clm5-b} and statement~\ref{clm5-c} of
Lemma~\ref{lemma.5} to determine when to stop the algorithm. We have
 by
Lemma~\ref{prelimary-lemma}. By the setting in
line~\ref{constant-setting-in-Approximate-Intervals} in
Approximate-Sum(.), we have



 Since  is doubled every phase,
and each  phase  costs  time. The total time of the
algorithm is , where phase  is the
last phase.


 The computational time complexity
   of the algorithm follows from statement~\ref{clm5-b} and statement~\ref{clm5-c} of
Lemma~\ref{lemma.5}.
\end{proof}



\begin{lemma}\label{app-sum-lemma} With probability at most
, at least one of the following statements is false after
executing the algorithm Approximate-Sum:
\begin{enumerate}[1.]
\item\label{item3b-app-sum-lemma}
If , then ;
\item\label{item3c-app-sum-lemma}
If , then ; and
\item\label{item4-app-sum-lemma}
It runs in  time. In particular, the complexity
of the algorithm is 
if  is fixed in .
\end{enumerate}
\end{lemma}



\begin{proof}
As  is doubled each new phase in Approximate-Intervals,
the number of phases is at most . With probability at most
 (by line~\ref{first-alpha-setting}
in Approximate-Intervals), at least one of the statements (i)
in Lemma \ref{lemma.1}, (ii) in Lemma \ref{lemma.2}, A, B, C in
Lemma \ref{lemma.5} is false.




Assume that the statements (i) in Lemma \ref{lemma.1}, (ii) in Lemma
\ref{lemma.2}, A, B, and C in Lemma \ref{lemma.5} are all true.


Statement~\ref{item3b-app-sum-lemma}: The condition of Statement~\ref{item3b-app-sum-lemma} implies . By Lemma \ref{lemma.4},  we have


 Since
, we have

We have the inequality


 Statement~\ref{item3c-app-sum-lemma} follows from
Statement~\ref{clm5-c} of Lemma~\ref{lemma.5}.


Statement~\ref{item4-app-sum-lemma} for the running time follows
from Lemma \ref{lemma.6}.

 Thus,
with probability at  most , at least one of the
statements~\ref{item3b-app-sum-lemma} to \ref{item4-app-sum-lemma}
is false.
\end{proof}




Now we have the proof for our main theorem.

\begin{proof}[for Theorem~\ref{main-thm}]
Let  and .  It follows from
Lemma~\ref{app-sum-lemma} via a proper setting for those parameters
in the algorithm Approximate-Sum(.).

 The -partition   for  can be generated in  time by Lemma~\ref{prelimary-lemma}.
Let  be a list of  numbers in . Pass  and  to Approximate-Sum(.), which returns an approximate
sum .

 By statement~\ref{item3b-app-sum-lemma} and
statement~\ref{item3c-app-sum-lemma} of Lemma~\ref{app-sum-lemma},
we have an -approximation for the sum problem with
failure probability at most . The computational time is
bounded by  by
statement~\ref{item4-app-sum-lemma} of  Lemma~\ref{app-sum-lemma}.
\end{proof}


\begin{definition} Let  be a function from  to  and a parameter .
Define  be the class of sum problem with an input of
nonnegative numbers  with .
\end{definition}

\begin{corollary} Assume that  is a function from  to  and  is a given constant  greater than .
There is a  time algorithm such that
given  a list of nonnegative numbers  in
, it gives a -approximation.
\end{corollary}

\begin{proof}
It follows from Theorem~\ref{main-thm}.
\end{proof}



We can extend our sublinear time algorithm to the more general list
of nonnegative elements.

\begin{theorem}
Assume that  is a positive constant in . Then there
is an  time algorithm to
compute -approximation for a list of nonnegative
numbers  of in the range .
\end{theorem}


\begin{proof}
A list of nonnegative elements  can be converted
into the list  in . It
follows from Theorem~\ref{main-thm}.
\end{proof}

\section{Lower Bound }

We show a lower bound for those sum problems with bounded sum of
sizes . The lower bound always matches the upper
bound.


\begin{theorem}\label{strong-lower-bound-theorem}  Assume  is
an nondecreasing unbounded function from  to  with
. Every randomized -approximation
algorithm for the sum problem in  needs
 time, where  is a constant greater than
, and  is an arbitrary small constant in
.
\end{theorem}

\begin{proof}
The first list  contains  elements of size ,
and its rest  items are  . The sum of numbers in the
first list is . Therefore, the first list is a sum
problem in .


The second list  contains  elements of value , and its
rest  items are  . The sum of numbers in the second list
is . Therefore, the second list is a sum problem in
.


Assume that an algorithm only has computational time  for computing -approximation for sum problems in  with . For each uniform random
sampling, with probability , it gets an number
greater than  in each . The algorithm has an 
probability to access at least one item greater than  in each
list in a path of computation. Therefore,   and  have the
same output for approximation by the same randomized algorithm.  If
 is a -approximation for the both sum problems, we have


We have  for .
This brings  a contradiction.

\end{proof}

\begin{corollary}
There is no  time randomized
approximation scheme algorithm for the sum problem.
\end{corollary}



\section{Conclusions}

We studied the approximate sum in a few models. We show that the
approximate sum can be computed in time  if the
input list in the range . Our lower bound almost matches the
upper bound. An interesting theoretical problem is to close the
small gap between the lower bound and upper bound for the
approximate sum problem.




\begin{thebibliography}{1}

\bibitem{AlonDuffieldLundThorup05}
N.~Alon, N.~Duffield, C.~Lund, and M.~Thorup.
\newblock Estimating arbitrary subset sums with few probes.
\newblock In {\em Proc. PODS}, pages 317--325, 2005.

\bibitem{BroderFonturaJosifovskiKumarMotwaniNabarPanigrahyTomkinsXu06}
A.~Broder, M.~Fontura, V.~Josifovski, R.~Kumar, R.~Motwani,
S.~Nabar,
  R.~Panigrahy, A.~Tomkins, and Y.~Xu.
\newblock Estimating corpus size via queries.
\newblock In {\em Proceedings of the 15th ACM international conference on
  Information and knowledge management (CIKM '06)}, pages 594--603, 2006.

\bibitem{CanettiEvenGoldreich95}
R.~Canetti, G.~Even, and O.~Goldreich.
\newblock Lower bounds for sampling algorithms for estimating the average.
\newblock {\em Information Processing Letters}, 53:17--25, 1995.

\bibitem{DuffieldLundThorup05}
N.~Duffield, C.~Lund, , and M.~Thorup.
\newblock Learn more, sample less: control of volume and variance in network
  measurements.
\newblock {\em IEEE Trans. on Information Theory}, 51:1756--1775, 2005.

\bibitem{Hoefding63}
W.~Hoefding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock {\em Journal of the American Statistical Association}, 58:13--30,
  1963.

\bibitem{MotwaniPanigrahyXu07}
R.~Motwani, R.~Panigrahy, and Y.~Xu.
\newblock Estimating sum by weighted sampling.
\newblock In {\em Proceedings of the 34th International Colloquium on Automata,
  Languages and Programming}, pages 53--64, 2007.

\bibitem{MotwaniRaghavan00}
R.~Motwani and P.~Raghavan.
\newblock {\em Randomized Algorithms}.
\newblock Cambridge University Press, 2000.

\end{thebibliography}


\end{document}
