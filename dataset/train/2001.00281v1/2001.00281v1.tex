\begin{table}[!ht]
\caption{Quantization results of ResNet50, MobileNetV2, and ShuffleNet on ImageNet. 
We abbreviate quantization bits used for weights as ``W-bit'' (for activations as ``A-bit''),
top-1 test accuracy as ``Top-1.''
Here, ``MP'' refers to mixed-precision quantization,
``No D'' means that none of the data is used to assist quantization, 
and ``No FT'' stands for no fine-tuning (re-training). 
Compared to post-quantization methods OCS~\cite{zhao2019improving}, OMSE~\cite{Kravchik_2019_ICCV}, and DFQ~\cite{nagel2019data}, \OURS achieves better accuracy.
$\text{\OURS}^\dagger$ means using percentile for quantization.
}
\label{tab:imagenet_main_text}

\subfloat[\footnotesize ResNet50]{
\centering
\centering
\small
\setlength\tabcolsep{1.3pt}
\begin{tabular}{p{8em} ccccccccccccccccccccccccccccc} \toprule
    Method      &{No D}&{No FT}                 &W-bit&A-bit&Size (MB) &Top-1\\
    \midrule
\ha Baseline                        &--&--      &32&32      &97.49              &77.72          \\
\midrule    
\ha OMSE~\cite{Kravchik_2019_ICCV}    &\xm&\xm    &4 &32      &12.28              &70.06          \\
\ha OMSE~\cite{Kravchik_2019_ICCV}    &\cm&\xm    &4 &32      &12.28              &74.98          \\
\ha PACT~\cite{choi2018pact}        &\cm&\cm    &4 &4       &12.19              &76.50          \\
\hc \OURS                           &\xm&\xm    &\AV &8     &\textbf{12.17}     &\textbf{75.80} \\
\hc $\text{\OURS}^\dagger$          &\xm&\xm    &\AV &8     &\textbf{12.17}     &\textbf{76.08} \\
\midrule    
\ha OCS~\cite{zhao2019improving}    &\cm&\xm    &6 &6       &18.46              &74.80          \\
\hc \OURS                           &\xm&\xm    &\AV &6     &\textbf{18.27}     &\textbf{77.43} \\
\midrule    
\hc \OURS                           &\xm&\xm    &8 &8       &{24.37}            &\textbf{77.67} \\
\bottomrule 
\end{tabular}
\label{tab:resnet50}
}

\subfloat[\footnotesize MobileNetV2]{
\centering
\small
\setlength\tabcolsep{1.3pt}
\begin{tabular}{p{8em} ccccccccccccccccccccccccccccc} \toprule
    Method      &{No D}&{No FT} &W-bit&A-bit        &Size (MB) &Top-1\\
    \midrule
\ha Baseline                        &--&--          &32&32      &13.37          &73.03          \\
\midrule
\hc \OURS                           &\xm&\xm        &\AV &8     &{1.67}         &\textbf{68.83} \\
\hc $\text{\OURS}^\dagger$          &\xm&\xm        &\AV &8     &{1.67}         &\textbf{69.44} \\
\midrule
\ha Integer-Only~\cite{jacob2018quantization}&\cm&\cm  &6 &6    &2.50           &70.90          \\
\hc \OURS                           &\xm&\xm        &\AV &6     &{2.50}         &\textbf{72.85} \\
\midrule
\ha RVQuant~\cite{park2018value}    &\cm&\cm        &8&8        &3.34           &70.29          \\
\ha DFQ~\cite{nagel2019data}        &\xm&\xm        &8&8        &3.34           &71.20          \\
\hc \OURS                           &\xm&\xm        &8&8        &{3.34}         &\textbf{72.91} \\
\bottomrule 
\end{tabular}
\label{tab:mobilenetv2}
}

\subfloat[\footnotesize ShuffleNet]{
\centering
\centering
\small
\setlength\tabcolsep{1.3pt}
\begin{tabular}{p{8em}cccccccc} \toprule
    Method      &{No D}&{No FT} &W-bit&A-bit    &Size (MB) &Top-1\\
    \midrule
\ha Baseline    &--&--              &32&32      &5.94               &65.07              \\
\midrule
\hc \OURS       &\xm&\xm            &\AV &8     &{0.74}             &\textbf{58.96}     \\
\midrule    
\hc \OURS       &\xm&\xm            &\AV &6     &{1.11}             &\textbf{62.90}     \\
\midrule    
\hc \OURS       &\xm&\xm            &8 &8       &{1.49}             &\textbf{64.94}     \\
\bottomrule 
\end{tabular}
\label{tab:shufflenet}
}
\end{table}


\section{Results}\label{sec:results}
In this section, we extensively test \OURS on a wide range of models and datasets.
We first start by discussing the zero-shot quantization of ResNet18/50, MobileNet-V2, and ShuffleNet on ImageNet in~\sref{sec:imagenet_result}. 
Additional results for quantizing ResNet152, InceptionV3, and SqueezeNext on ImageNet, as well as ResNet20 on Cifar10 are provided in Appendix~\ref{sec:extra_imagenet}. 
We also present results for object detection using RetinaNet tested on Microsoft COCO dataset in~\sref{sec:objective_detection_result}. 
We emphasize that all of the results achieved by \OURS are 100\% zero-shot without any need for fine-tuning. 


We also emphasize that we used exactly the same
hyper-parameters (e.g., the number of iterations to generate \rg) for all experiments, including the results on Microsoft COCO dataset.



\subsection{ImageNet}\label{sec:imagenet_result}
We start by discussing the results on the ImageNet dataset.
For each model, after generating \rg based on~\eref{eq:synthetic_gaussian}, we compute the sensitivity of each layer using~\eref{eq:sensivity_formula} for different bit precision.
Next, we use~\eref{eq:sensitivity_objective} and the Pareto frontier introduced in~\sref{sec:pareto_frontier} to get the best bit-precision configuration based on the overall sensitivity for a given model size constraint. 
We denote the quantized results as WwAh where w and h denote the bit precision used for weights and
activations of the NN~model.

We present zero-shot quantization results for ResNet50 in~\tref{tab:resnet50}.
As one can see, for W8A8 (i.e., 8-bit quantization for both weights and
activations), \OURS results in only 0.05\% accuracy degradation.
Further quantizing the model to W6A6, \OURS achieves 77.43\% accuracy, which is 
2.63\% higher than OCS~\cite{zhao2019improving}, even though our model is slightly smaller (18.27MB as compared to 18.46MB for OCS).\footnote{Importantly note that OCS requires access to the training data, while \OURS does not use any training/validation data.}
We show that we can further quantize ResNet50 down to just 12.17MB with mixed precision quantization, 
and we obtain 75.80\% accuracy. Note that this is 0.82\% higher than OMSE~\cite{Kravchik_2019_ICCV} with access to training data and 5.74\% higher than zero-shot version of OMSE. Importantly, note that OMSE keeps
activation bits at 32-bits, while for this comparison our results use 8-bits for the activation (i.e., $4\times$ smaller activation memory footprint than OMSE).
For comparison, we include results for PACT~\cite{choi2018pact}, a standard quantization method that requires access to training data and also requires fine-tuning. 

An important feature of the \OURS framework is that it can perform the quantization with very low
computational overhead. For example, the end-to-end quantization of ResNet50 takes less than 30 seconds
 on an 8 Tesla V100 GPUs (one epoch training time on this system takes 100 minutes).
In terms of timing breakdown, it takes 3s to generate the \rg, 12s to compute the sensitivity for all layers of ResNet50, and 14s to perform Pareto Frontier optimization.


We also show \OURS results on MobileNetV2 and compare it with both DFQ~\cite{nagel2019data} and fine-tuning based methods~\cite{park2018value,jacob2018quantization}, as shown in~\tref{tab:mobilenetv2}. 
For W8A8, \OURS has less than 0.12\% accuracy drop as compared to baseline, and it achieves 1.71\% higher
accuracy as compared to DFQ method. 

Further compressing the model to W6A6 with mixed-precision quantization for weights, \OURS can still outperform Integer-Only~\cite{jacob2018quantization} by 1.95\% accuracy, even though \OURS does not use any data or fine-tuning. 
\OURS can achieve 68.83\% accuracy even when the weight compression is 8$\times$, which corresponds to using 4-bit quantization for weights on~average.


We also experimented with percentile based clipping to determine the quantization range~\cite{li2019fully} (please see~\sref{sec:clipping} for details). The results corresponding to percentile based clipping
are denoted as $ZeroQ^\dagger$ and reported in~\tref{tab:imagenet_main_text}. We found
that using percentile based clipping is helpful for low precision quantization.
Other choices for clipping methods have been proposed in the literature.
Here we note that our approach is orthogonal to these improvements and that \OURS could
be combined with these methods. 




We also apply \OURS to quantize efficient and highly compact models such as ShuffleNet, whose model size is only 5.94MB.
To the best of our knowledge, there exists no prior zero-shot quantization results for this model.
\OURS  achieves a small accuracy drop of 0.13\% for W8A8.
We can further quantize the model down to an average of 4-bits for weights, which achieves a model size of
only 0.73MB, with an accuracy of 58.96\%.

\begin{table}[!htbp]
\caption{Object detection on Microsoft COCO using RetinaNet. 
By keeping activations to be 8-bit, our 4-bit weight result is comparable with recently proposed method~FQN~\cite{li2019fully}, which relies on fine-tuning.
(Note that FQN uses 4-bit activations and the baseline used in~\cite{li2019fully} is 35.6~mAP).} 
\label{tab:Detection}
\centering
\small
\setlength\tabcolsep{2 pt}
\begin{tabular}{p{6em}ccccccccccccccccccccccccccccc} \toprule
    Method      &No D&No FT    &W-bit&A-bit    &Size (MB)          &mAP\\
    \midrule
\ha Baseline                &\xm&\xm        &32&32          &145.10             &36.4          \\
\midrule
\ha FQN~\cite{li2019fully}  &\cm&\cm        &4&4  &18.13              &32.5\\
\hc \OURS                   &\xm&\xm        &\AV&8          &{18.13}            &\textbf{33.7} \\
\midrule
\hc \OURS                   &\xm&\xm        &\AV&6          &{24.17}            &\textbf{35.9} \\
\midrule
\hc \OURS                   &\xm&\xm        &8&8            &{36.25}            &\textbf{36.4} \\
\bottomrule 
\end{tabular}
\end{table}


We also compare with the recent Data-Free Compression (DFC)~\cite{haroush2019knowledge} method.
There are two main differences between \OURS and DFC. 
First, DFC proposes a fine-tuning method to recover accuracy for ultra-low precision cases. 
This can be time-consuming and as we show it is not necessary. 
In particular, we show that with mixed-precision quantization one can actually achieve higher accuracy without any need for fine-tuning. 
This is shown in~\tref{tab:resnet18_dfc} for ResNet18 quantization on ImageNet.
In particular, note the results for W4A4, where the DFC method without fine-tuning results in more than 15\% accuracy drop with a final accuracy of 55.49\%. 
For this reason, the authors propose a method with post quantization training, which can boost the accuracy to 68.05\% using W4A4 for intermediate layers, and 8-bits for the first and last layers.
In contrast, \OURS achieves a higher accuracy of 69.05\% without any need for fine-tuning.
Furthermore, the end-to-end zero-shot quantization of ResNet18 takes only 12s on an 8-V100 system (equivalent to $0.4\%$ of the 45 minutes time for one epoch training of ResNet18 on ImageNet).
Secondly, DFC method uses Inceptionism~\cite{mordvintsev2015inceptionism} to facilitate the generation of data with random labels, but it is hard to extend this for object detection and image segmentation tasks.

\begin{table}[!htbp]
\caption{Uniform post-quantization on ImageNet with ResNet18. We use percentile clipping for W4A4 and W4A8 settings. $\text{\OURS}^\dagger$ means using percentile for quantization.}
\label{tab:resnet18_dfc}
\centering
\small
\setlength\tabcolsep{2 pt}
\begin{tabular}{p{8em}ccccccccccccccccccccccccccccc} \toprule
    Method                          &{No D}&{No FT} &W-bit&A-bit    &Size (MB)      &Top-1\\
    \midrule
\ha Baseline                        &--&--          &32&32          &44.59          &71.47      \\
\midrule
\ha PACT~\cite{choi2018pact}        &\cm&\cm        &4 &4           &5.57           &69.20      \\
\ha DFC~\cite{haroush2019knowledge} &\xm&\xm        &4 &4           &5.58           &55.49\\
\ha DFC~\cite{haroush2019knowledge} &\xm&\cm        &4 &4           &5.58           &68.06\\
\hc \OURS                           &\xm&\xm        &\AV&4          &\textbf{5.57}  &\textbf{-- }\\
\hc $\text{\OURS}^\dagger$          &\xm&\xm        &\AV&4          &\textbf{5.57}  &\textbf{69.05}\\
\midrule
\ha Integer-Only\cite{jacob2018quantization}&\cm&\cm&6 &6           &8.36           &67.30      \\
\ha DFQ~\cite{nagel2019data}        &\xm&\xm        &6 &6           &8.36           &66.30      \\
\hc \OURS                           &\xm&\xm        &\AV&6          &\textbf{8.35}  &\textbf{71.30}\\
\midrule
\ha RVQuant~\cite{park2018value}    &\cm&\cm        &8&8            &11.15          &70.01      \\
\ha DFQ~\cite{nagel2019data}        &\xm&\xm        &8&8            &11.15          &69.70      \\
\ha DFC~\cite{haroush2019knowledge} &\xm&\cm        &8&8            &11.15          &69.57\\
\hc \OURS                           &\xm&\xm        &8&8            &{11.15}        &\textbf{71.43}\\
\bottomrule 
\end{tabular}
\end{table}


We include additional results of quantized ResNet152, InceptionV3, and SqueezeNext on ImageNet, as well as ResNet20 on Cifar10, in Appendix~\ref{sec:extra_imagenet}. 


\subsection{Microsoft COCO}
\label{sec:objective_detection_result}
Object detection is often much more complicated than ImageNet classification. To demonstrate the flexibility of our approach we also test \OURS on an object detection task on Microsoft COCO dataset. 
RetinaNet~\cite{lin2017focal} is a state-of-the-art single-stage detector,
and we use the pretrained model with ResNet50 as the backbone, which can achieve 36.4~mAP.\footnote{Here we use the standard mAP 0.5:0.05:0.95 metric on COCO dataset.}

One of the main difference of RetinaNet with previous NNs we tested on ImageNet is that some convolutional layers in RetinaNet are not followed by BN layers.
This is because of the presence of a feature pyramid network (FPN)~\cite{DBLP:journals/corr/LinDGHHB16}, and it means that the number of BN layers is slightly smaller than that of convolutional layers. 
However, this is not a limitation and the \OURS framework still works well.
Specifically, we extract the backbone of RetinaNet and create \rg.
Afterwards, we feed the \rg into RetinaNet to measure the sensitivity as well as to determine the activation range for the entire NN. 
This is followed by optimizing for the Pareto Frontier, discussed earlier. 

The results are presented in~\tref{tab:Detection}.
We can see that for W8A8 \OURS has no performance degradation. 
For W6A6, \OURS achieves 35.9 mAP.
Further quantizing the model to an average of 4-bits for the weights, \OURS achieves 33.7 mAP.
Our results are comparable to the recent results of FQN~\cite{li2019fully}, even
though it is not a zero-shot quantization method (i.e., it uses the full training dataset and requires fine-tuning). 
However, it should be mentioned that \OURS keeps the activations to be 8-bits, while FQN uses 4-bit activations. 

