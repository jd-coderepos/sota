

This section presents an overview of the Universal NER (\shortname) dataset.
\shortname{}~v1 adds an NER annotation layer to 18 datasets (primarily treebanks from UD) and covers 12 geneologically and typologically diverse languages: Cebuano, Danish, German, English, Croatian, Portuguese, Russian, Slovak, Serbian, Swedish, Tagalog, and Chinese\footnote{Languages sorted by their ISO 639-1/639-2 codes \citep{ISO639-1,ISO639-2}}. Overall, \shortname{}~v1 contains nine full datasets with training, development, and test splits over eight languages, three evaluation sets for lower-resource languages (\textsc{tl} and \textsc{ceb}), and a parallel evaluation benchmark spanning six languages.




\subsection{Dataset Statistics}
In \autoref{tab:dataset-stats} we report the number of sentences, tokens, and annotated entities for each dataset in \shortname{}.
The datasets in \shortname{} cover a wide range of data quantities: some provide a limited amount of evaluation data for a commonly low-resourced language, whereas others annotate thousands of training and evaluation sentences.

\begin{table*}[htbp]
\centering
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{ll lllr lllr lllr}
\toprule
& & \multicolumn{4}{c}{Train} & \multicolumn{4}{c}{Dev} & \multicolumn{4}{c}{Test} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
Lang. & Dataset & \texttt{LOC} & \texttt{ORG} & \texttt{PER} & \% Docs & \texttt{LOC} & \texttt{ORG} & \texttt{PER} & \% Docs & \texttt{LOC} & \texttt{ORG} & \texttt{PER} & \% Docs \\
\midrule
\textsc{da} & \texttt{ddt} & .875 & .778 & .959 & 100\% & .917 & .765 & .934 & 100\% & .882 & .805 & .975 & 100\% \\
\textsc{en} & \texttt{ewt} & .696 & .533 & .925 & 20\% & .786 & .640 & .949 & 20\% & .825 & .869 & .969 & 20\% \\
\textsc{pt} & \texttt{bosque} & .928 & .902 & .974 & 11\% & .850 & .885 & .980 & 25\% & .955 & .914 &.975 & 23\% \\
\textsc{sk} & \texttt{snk} & .840 & .743 & .900 & 100\% & .801 & .597 & .770 & 100\% & .837 & .621 & .823 & 100\% \\
\textsc{sv} & \texttt{talbanken} & .857 & .670 & .913 & 100\% & .800 & .461 & .888 & 100\% & .937 & .812 & .871 & 100\% \\
\textsc{zh} & \texttt{gsd} & .800 & .724 & .917 & 14\% & .795 & .661 & .956 & 100\% & .860 & .711  & .944 & 23\% \\
\midrule
\textsc{de} & \texttt{pud} & - & - & - & - & - & - & - & - & .709 & .840 & .812 & 6\% \\
\textsc{en} & \texttt{pud} & - & - & - & - & - & - & - & - & 1.00 & .936 & .966 & 6\% \\
\textsc{pt} & \texttt{pud} & - & - & - & - & - & - & - & - & .903 & .920 & .985 & 14\% \\
\textsc{ru} & \texttt{pud} & - & - & - & - & - & - & - & - & .719 & .531 & .891 & 100\% \\
\textsc{sv} & \texttt{pud} & - & - & - & - & - & - & - & - & .865 & .735 & .944 & 100\% \\
\textsc{zh} & \texttt{pud} & - & - & - & - & - & - & - & - & .752 & .776 & .971 & 20\% \\
\midrule
\textsc{ceb} & \texttt{gja} & - & - & - & - & - & - & - & - & .769 & 1.00 & .914 & 71\% \\
\multirow{2}{*}{\textsc{tl}} & \texttt{trg} & - & - & - & - & - & - & - & - & .833 & - & .957 & 100\% \\
 & \texttt{ugnayan} & - & - & - & - & - & - & - & - & .913 & - & - & 100\% \\

\bottomrule
\end{tabular}
\end{adjustbox}

\caption{Inter-annotator agreement scores for the datasets annotated natively for the Universal NER project. We don't report IAA for the datasets adapted from other sources, or from \texttt{zh\_gsdsimp}, which has nearly identical annotations to \texttt{zh\_gsd}.}
\label{tab:iaa-scores}
\end{table*}

The datasets in \shortname{} also cover a diverse range of domains, spanning web sources such as social media to more traditional provenances like news text.
\autoref{tab:uner-domains} in the appendix presents the full set of sources for the data, and the distribution of NER tags in each dataset, along with references to original treebank papers.
The variety in data sources leads to varied distributions of tags across datasets (\autoref{fig:train-dist}).


\subsection{Inter-Annotator Agreement}
\label{sec:iaa}
We calculate inter-annotator agreement (IAA, \autoref{tab:iaa-scores}) for each dataset in \shortname{} that was annotated with the above process and for which we have secondary annotations. Table~\ref{tab:iaa-scores} reports agreement as per-label F\textsubscript{1} score, using one annotator as ``reference,'' and the other as ``prediction.''


\paragraph{\texttt{ORG} vs \texttt{LOC} Confusion}
\label{ssec:olconfusion}
The agreement on \texttt{ORG} and \texttt{LOC} is generally lower than that on \texttt{PER}. The annotation guidelines allow that certain named entities may take either the \texttt{ORG} or \texttt{LOC} tag based on context. In certain cases, the context is underspecified, and there is a natural ambiguity. For example, a restaurant is a \texttt{LOC} when you go there to eat, but it is an \texttt{ORG} when it hires a new chef. A city is a \texttt{LOC} when you move there, but it is an \texttt{ORG} when it levies taxes. Officially, it is the \textit{city government} that levies taxes, but common usage allows, for example, ``\nerexample{Springfield}{ORG} charges a brutal income tax.'' CoNLL 2003 English also has this ambiguity, with many documents where city names, representing sports teams, are annotated as \texttt{ORG}.
This ambiguity was especially common in the \texttt{en\_ewt} train and validation splits, primarily in documents in the \textit{reviews} domain, which are short and very informal (e.g. ``we love pamelas'').












\subsection{Agreement with the \texttt{PROPN} POS Tag}
\label{ssection:propn}
The proper noun (\texttt{PROPN}) part-of-speech tag used in UD represents the subset of nouns that are used as the name of a specific person, place, or object \cite{nivre-etal-2020-universal}.
We hypothesize that named entities as defined in \shortname{} act roughly as a subset of these \texttt{PROPN} words or phrases, although not a strict subset due to divergent definitions.
To test this, we calculate the precision of the \shortname{} annotations against the UD \texttt{PROPN} tags (\autoref{tab:propn_overlap}, F\textsubscript{1} scores reported in \autoref{tab:propn_overlap_f1}).
Overall, precision is relatively high, with a mean precision of 0.745 across datasets.
Lower precision is often due to multi-word names containing non-\texttt{PROPN} words (e.g.,~``Catherine the Great'').
The differences in precision can also be due to language-specific \texttt{PROPN} annotation guidelines: for example, while the English PUD treebank tags the United States entity as ``\nerexample{United}{PROPN} \nerexample{States}{PROPN}'', Russian PUD tags it as {\fontencoding{T2A}\selectfont ``\nerexample{Соединенных}{{\fontencoding{T1}\selectfont ADJ}} \nerexample{Штатов}{{\fontencoding{T1}\selectfont NOUN}}''}.
\begin{table}[t!]
\centering

\begin{tabular}{llccc}
\toprule
Lang. & Dataset & Train & Dev & Test \\
\midrule
\textsc{da} & \texttt{ddt} & .709 & .729 & .722 \\
\textsc{en} & \texttt{ewt} & .890 & .895 & .892 \\
\textsc{hr} & \texttt{set} & .683 & .651 & .671 \\
\textsc{pt} & \texttt{bosque} & .864 & .881 & .844 \\
\textsc{sk} & \texttt{snk} & .803 & .783 & .688 \\
\textsc{sr} & \texttt{set} & .687 & .631 & .680 \\
\textsc{sv} & \texttt{talbanken} & .766 & .756 & .842  \\
\textsc{zh} & \texttt{gsd} & .605 & .624 & .616 \\
\textsc{zh} & \texttt{gsdsimp} & .601 & .604 & .617 \\
\midrule
\textsc{de} & \texttt{pud} & - & - & .712 \\
\textsc{en} & \texttt{pud} & - & - & .872 \\
\textsc{pt} & \texttt{pud} & - & - & .749 \\
\textsc{ru} & \texttt{pud} & - & - & .708 \\
\textsc{sv} & \texttt{pud} & - & - & .810 \\
\textsc{zh} & \texttt{pud} & - & - & .634 \\
\midrule
\textsc{ceb} & \texttt{gja} & - & - & .980 \\
\textsc{tl} & \texttt{trg} & - & - & .958 \\
\textsc{tl} & \texttt{ugnayan} & - & - & .654 \\

\bottomrule
\end{tabular}\caption{Comparing the overlap (Precision) between \shortname{} annotations and UD PROPN tags.}
\label{tab:propn_overlap}
\end{table}


\begin{figure*}[ht]
\centering
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/pud-comp-left.pdf}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/pud-comp-center.pdf}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/pud-comp-right.pdf}
\end{subfigure}
\caption{Cross-lingual comparison of NER Annotations on top of PUD treebanks. \textbf{Left}: Global distribution of tags for each PUD language. \textbf{Center}: Sentence-level agreement between languages for the number of entities. \textbf{Right}: Sentence-level agreement between languages for the identity of entities.}
\label{fig:pud-comp}
\end{figure*}

\subsection{Cross-lingual Agreement in \shortname{}}
\label{ssection:pud-agreement}
\shortname{} contains sentence-aligned evaluation sets for six languages (German, English, Portuguese, Russian, Swedish, and Chinese) that are annotated on top of the Parallel Universal Dependencies treebanks~\cite[PUD; ][]{zeman-etal-2017-conll}. \autoref{fig:pud-comp} summarizes the similarity of the NER annotations across these target languages in PUD.

We find that the overall distribution of tags is similar for the Western European languages (left panel): the English, German, and Swedish annotations contain very similar counts of \texttt{LOC} and \texttt{PER} entities, while there is slightly more variance in \texttt{ORG} tags. Portuguese has a similar distribution with slightly more \texttt{LOC} entities.
However, the Russian and Chinese annotations contain differing distributions from both these languages and each other. 

A similar trend occurs in the sentence-level pairwise agreement on entity counts and identities between languages (center). There is relatively high agreement on the number of entities between European languages, with Russian differing slightly more from English, German, Portuguese, and Swedish. However, the Chinese benchmark agrees less frequently: the Chinese annotations match other languages on the number of entities in 50.4\% of sentences; the other languages have an average agreement of 71.7--75.6\%. Pairwise agreement on the specific entities in a given sentence shows similar behavior, albeit with lower agreement overall (right).

Many of these annotation differences likely stem from the translation process. While the data is aligned at the sentence level, linguistic variation and translator decisions may cause an entity to be added to or removed from the sentence, or the concept to be expressed in a manner that no longer qualifies as a named entity under the annotation guidelines.\footnote{Consider the phrases:  \zh{“奧巴馬對在北卡羅來納大學運動場上的群眾說道。”} and ``he told the crowd gathered on a sports field at the University of North Carolina.'' In Chinese, \textit{Obama} (\zh{奧巴馬}) is referred to by name, whereas the English version uses a pronoun.} While we cannot directly measure inter-annotator agreement across languages because of the above differences, some variation also undoubtedly stems from annotation differences and errors, just as these cause disagreement between annotators on the same benchmark.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/heatmap.pdf}
\caption{Heatmap of micro F\textsubscript{1} scores on test sets with different fine-tuned models. y-axis indicates the dataset that the model is fine-tuned in, and the x-axis indicates the datasets that the models are evaluated on. \textbf{Left}: Model performance on datasets that contains the train, dev and test splits. The highlighted diagonal cells are the in-dataset results. \textbf{Center}: Model performance on the PUD datasets. \textbf{Right}: Model performance on all other datasets.}
\label{fig:baseline}
\end{figure*}

In the case of Chinese and English, we manually audited the annotation discrepancies. The differences in the \texttt{LOC} and \texttt{ORG} tags mostly stem from the confusion outlined in Section~\ref{ssec:olconfusion}.
Additionally, we saw more than 30 instances that could be explained by language-specific morphological inflection rules.
Specifically, country names are used directly to modify the following nouns in Chinese as opposed to English using the adjectival form.\footnote{I.e., \zh{“韓國公司”} `South Korean company'. The Chinese word \zh{“韓國”} means the country `South Korea', and in this case, directly modifies the noun \zh{“公司”} `company'. This word was consequently labeled as \texttt{LOC}, whereas its English counterpart is \texttt{O}.}
Finally, the increase in \texttt{PER} entities can be best explained by the style of Chinese writing, which tends to transliterate non-Chinese names into Chinese and append the Latin name in parentheses; in these cases, each instance of the name would be tagged as a separate \texttt{PER} entity.\footnote{An example is ``\zh{聖羅斯季斯拉夫 （St. Rastislav)}'', in which the English name is parenthesized and kept in the Chinese sentence, causing both names to be annotated.}








