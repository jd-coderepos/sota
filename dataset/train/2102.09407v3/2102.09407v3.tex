

\documentclass[accepted]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{enumitem}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{pdfpages}
\usepackage{url}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{dsfont}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{icml2022}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\newcommand{\eg}{\emph{e.g.}~} 
\newcommand{\Eg}{\emph{E.g.}~}
\newcommand{\ie}{\emph{i.e.}~} 
\newcommand{\Ie}{\emph{I.e.}~}
\newcommand{\cf}{\emph{cf.}~} 
\newcommand{\Cf}{\emph{Cf.}~}
\newcommand{\etc}{\emph{etc.}~} 
\newcommand{\etal}{\emph{et al.}~}
\newcommand\mydots{\makebox[0.7em][c]{.\hfil.\hfil.}}

\newcommand{\ocol}{\cellcolor{blue!18}}
\newcommand{\bcol}{\cellcolor{orange!30}}
\newcommand{\gcol}{\cellcolor{green!30}}

\definecolor{blue}{rgb}{0.3, 0.3, 0.9}
\newcommand{\rtwo}{\textcolor{blue}{R2}} 

\definecolor{green}{rgb}{0.1, 0.5, 0.1}
\newcommand{\green}[1]{\textcolor{green}{#1}} 

\definecolor{orange}{rgb}{1, 0.5, 0}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\pagenumbering{arabic}

\icmltitlerunning{Adaptive Rational Activations to Boost Deep Reinforcement Learning}

\begin{document}

\twocolumn[
\icmltitle{Adaptive Rational Activations to Boost Deep Reinforcement Learning}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Quentin Delfosse}{yyy}
\icmlauthor{Patrick Schramowski}{yyy}
\icmlauthor{Martin Mundt}{yyy}
\icmlauthor{Alejandro Molina}{yyy}
\icmlauthor{Kristian Kersting}{yyy,comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{AI and Machine Learning Group, CS Department, TU Darmstadt, Germany}
\icmlaffiliation{comp}{Centre for Cognitive Science, TU Darmstadt, Germany}

\icmlcorrespondingauthor{Quentin Delfosse}{quentin.delfosse@cs.tu-darmstadt.de}

\icmlkeywords{Neural Plasticity Deep Reinforcement Learning, Rational Activation Functions}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. Specifically, neural plasticity should be critical in the context of constantly changing reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are particularly suitable for adaptable activation functions in deep neural networks. Inspired by residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version. The proposed joint rational activation allows for desirable degrees of flexibility, yet regularises plasticity to an extent that avoids overfitting by leveraging a mutual set of activation function parameters across layers. We demonstrate that equipping popular algorithms with (joint) rational activations leads to consistent improvements on Atari games, notably making DQN competitive to DDQN and Rainbow.\footnote{Rational library provided at \url{https://github.com/ml-research/rational_activations/}} \footnote{RL 
experiments provided at \url{https://github.com/ml-research/rational_rl/}}
\end{abstract}



\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{images/main_paper/af_evolution_through_time.pdf}
    \vspace{-0.3cm}
    \caption{Neural plasticity due to trainable activation functions allows Deep RL agents to adapt to environments of increasing complexity. Rational activations (left columns), with shared parameters in the last two layers, evolve together with their input distributions (shaded blue) when learning with DQN on Time Pilot. Each row corresponds to a state where a new, more challenging part of the environment (right column, e.g.~increasing enemy speed and complexity) has been uncovered and is additionally trained on.}
    \label{fig:evolving_af}    
\end{figure}

\section{Introduction}
Neural Networks' efficiency in approximating any function has made them the most used approximation function for many machine learning tasks. This is no different in reinforcement learning (RL), where the introduction of the DQN algorithm \citep{mnih2015human} has sparked the development of various neural solutions. 
In concurrence with neuroscientific explanations of brainpower residing in combinations stemming from trillions of connections \citep{Garlick2002UnderstandingTN}, present advances have emphasised the role of the neural architecture \citep{liu2018progressive, XieZLL19}. As such, RL improvements have first been mainly obtained through a focus on enhancing algorithms \citep{Mnih2016asynchronous, Haarnoja2018SAC, Banerjee2021ISAC} and only recently by searching for well-performing architectural patterns \citep{miao2021rldarts}. 

However, research has also progressively shown that individual neurons shoulder more complexity than initially expected, with the latest results demonstrating that dendritic compartments can compute complex functions (\eg XOR) \citep{gidon2020dendritic}, previously categorised as unsolvable by single-neuron systems.
This finding seems to have renewed interest in activation functions \citep{georgescu2020non, Misra20mish}. In fact, many functions have been adopted across different domains \citep{RedmonDGF16, BrownMRSKDNSSAA20, SchulmanWDRK17}. To reduce the bias introduced by a fixed activation function and achieve higher expressive power, one can further learn which activation function is performant for a particular task \citep{zoph2016neural, liu2018progressive}, learn to combine arbitrary families of activation functions \citep{manessi2018learning}, or find coefficients for polynomial activations as weights to be optimised \citep{goyal2019learning}. 

Whereas these prior approaches have all contributed to their respective investigated scenarios, there exists a finer approach that elegantly encapsulates the challenges brought on by reinforcement learning problems. Specifically, we can learn rational activation functions \citep{molina2019pad}. Not only can rational activation functions converge to any continuous function, but they have further been proven to be better approximants than polynomials in terms of convergence \citep{telgarsky2017neural}. Even more crucially, their ability to adapt while learning equips a model with high neural plasticity, \ie capability to adjust to the environment and its transformations \citep{Garlick2002UnderstandingTN}.
We argue that adapting to environmental changes is essential, making rational activation functions particularly suitable for dynamic RL environments. 
To provide a visual intuition, we showcase an exemplary evolution of two rational activation functions together with their respective changing input distributions in the dynamic ``Time Pilot'' environment in Fig.~\ref{fig:evolving_af}.

In this work, we propose the use of plasticity via rational activations in deep RL agents, as a central element to satisfy the requirements originating from diverse and dynamic environments. Apart from demonstrating the suitability of adaptive activation functions, we further identify and address potential caveats to overcome remaining practical barriers. Our specific contributions are:
\begin{description}
\item[(i)] We motivate why neural plasticity is a key aspect for Deep RL agents and that rational activations are adequate as adaptable activation functions. For this purpose, we not only highlight that rational activation functions adapt their parameters over time, but further prove that they can dynamically embed residual connections, which we refer to as residual plasticity. 
\item[(ii)] As the introduction of additional representational capacity could hinder generalisation, we then propose a joint rational variant. By making use of weight-sharing across rational activations in different layers, we thus include regularisation, necessary for RL \citep{farebrother2018regdqn, RoyBHNP20regul, YaratsKF21Image}. 
\item[(iii)] We empirically demonstrate that the inclusion of rational activations brings significant improvements to DQN and Rainbow algorithms on Atari games and that our joint variant further increases performance. 
\item[(iv)] Finally, we investigate the overestimation phenomenon of predicting too large return values, which has previously been argued to originate from an unsuitable representational capacity of the learning architecture \citep{van2016deep}. As a result of our introduced (rational) neural and residual plasticity, such overestimation can practically be reduced.
\end{description}
We proceed as follows. We start off by arguing in favour of plasticity for deep reinforcement learning. Then we show how rational activation functions provide a particularly suitable candidate to provide plasticity in neural networks and present our empirical evaluation. Before concluding, we touch upon related work. 



\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{images/main_paper/flexible_vs_fixed_af_pelu.pdf}
    \vspace{-0.6 cm}
    \caption{Neural plasticity is essential for reinforcement learning. Human normalised mean scores for rigid DQN agents, agents with non-rational, rational, tempered, and regularised plasticity are shown with standard deviation across 5 random seeded experimental repetitions. Larger scores are better. Tempered plasticity, allowing initial adaptation to the environments, but not their transformations in experimental repetitions, performs better on stationary environments. Regularised plasticity performs well across all environment types. Figure best viewed in colour. A full description of the environments and their types is provided in Appendix~\ref{app:env_classification}.
    \label{fig:neural_plasticity}}
\end{figure*}


\section{Plasticity via Rational Activation Functions for Deep RL} 
Let us start by arguing why deep reinforcement learning agents require extensive plasticity and show that parametric rational activation functions provide appropriate means to add such additional plasticity to the network. 

As motivated in the introduction, RL is subject to inherent distribution shifts. During the training process, agents progressively uncover new states (input drift) and, as the policy improves, the cumulative reward signal is modified (output drift). More precisely, for input drifts, we can distinguish environments according to how much they change through learning. For simplicity, we categorise according to three intuitive categories: stationary, dynamic and progressive environments. 
To illustrate, consider the example of Atari 2600 games. Kangaroo, Space Invaders and Tennis can be characterised as stationary since the game's input distribution does not change significantly. 
Asterix, Enduro and Q*bert are dynamic environments, as different inputs are provided to the agents early in the game; hence no policy improvement is required to uncover (input) distribution shifts. 
On the contrary, Jamesbond, Seaquest, and Time Pilot are progressive environments. The agent needs to master early stages before being provided with additional states, \ie exposed to a significant shift in the input distribution. 

How do we efficiently improve RL agents' ability to adapt to environments and their changes? To deal with these distribution shifts, our agents require high neural plasticity and thus benefit from adaptive architectures. To elaborate further in our work, let us consider the popular DQN algorithm \citep{mnih2015human}, that employs a -parameterised neural network to approximate the Q-value function of a state  and action . This network is updated following the Q-learning equation:
. In addition to network connectivity playing an important role, we now highlight the importance of individual neurons by modifying the network architecture of the algorithm via the use of learnable activation functions, to show that they are a presently underestimated component. To emphasise the utility of the upcoming proposed rational and joint rational activation functions, we will interleave early results into this section. The latter serves the primary purpose to not only motivate the suitability of the rational parameterisation to provide plasticity, but also discern the individual benefits of (joint-) rational activations, in the spirit of ablation studies.

\subsection{Rational Neural Plasticity}
A rational function of order  is a universal approximator, defined by:

where  and  are learnable parameters (corresponding to  parameters in total).

In Fig.~\ref{fig:neural_plasticity}, we show that our proposition to use such a rational parametrisation substantially enhances RL agents. 
More precisely, by comparing agents with rigid networks (a fixed Leaky ReLU baseline) to agents with rational plasticity (\ie with a rational activation function at each layer), we see that using rational functions boosts the agents to super-human performances on 7 out of 9 games. The acquired extra neural plasticity seems to play a significant role in these Atari environments, especially in progressive ones.

In order to discern the benefits of general plasticity through any adaptive activation function, over the proposed use of the rational parametrisation, Fig.~\ref{fig:neural_plasticity} additionally includes PELU \citep{TrottierGC17}. PELU is a parametrised ELU that uses 3 parameters to control its slope, saturation and exponential decay, and has been shown to outperform other learnable alternatives on classification tasks \citep{Godfrey2019AnEO}. However, in contrast to the rational parameterisation, it seems to fall behind and only boosts the agents to super-human performance on 3 out of 9 games (contrary to 7). This implies that the type of parameterisation provided by the rational activation choice is particularly suitable.

To highlight the desirability of rational activations even further, we additionally distinguish between the plasticity of agents towards their specific environment and the plasticity allowing them to adapt while the environment itself is changing. To this end, we also show agents with rational activations that are tempered in Fig.~\ref{fig:neural_plasticity}. They have been extracted from agents with rational plasticity, that adapted to their specific environment. The plasticity of the rational activation functions is then tempered (``stopped'') in repeated application to emphasise the necessity to continuously adapt during training. Whereas providing agents with such tempered, tailored to the task, activations already boosts performances, rational plasticity at all times seems essential, particularly in the dynamic and progressive environments.

\subsection{Rational Residual Plasticity}
The prior paragraphs have showcased the advantage of agents equipped with rational activation functions, rooted in their ability to update their parameters over time. However, we argue that the observed boost in performance is not only due to parameters adapting to distributional drifts. In addition to neural plasticity, rational activations can embed one of the most popular techniques to stabilise training of deep neural networks; namely, they can dynamically make use of a residual connection. We refer to this as residual plasticity. 

\textbf{Rationals are closed under residual connection and provide residual plasticity.}
We here show that using a residual connection together with a rational function is equivalent to using a rational with strictly higher degree in the numerator.

\begin{theorem}
\label{thm:bigtheorem}
Let R be a rational activation function of order (m, n). If , then R embeds a residual connection.
\end{theorem}
\begin{proof} 
Let us consider a rational function R  P/Q of order , with coefficients  of P and  of Q (with ). \\
We denote by  (resp. ) the Hadamard product (resp. division). Let  be a tensor corresponding to the input of the rational function of an arbitrary layer in a given neural network. We derive \hspace{0pt} . \\ 
Furthermore, we use  to denote the tensor containing the powers up to  of the tensor . Note that  can be understood as a generalised Vandermonde tensor, similar as introduced in \citep{xu2016generalized}. \\
For , let  be the weighted sum over the tensor elements of the last dimension.

Now, we apply the rational activation function R with residual connection to :

where  (with  and  for ),\\
 with ,   for all  and  for all . \\
 is a rational function of order ,  with . 
\end{proof}
In other words, rational activation functions of order  can embed residual connections if necessary. Using the same degrees for numerator and denominator certifies asymptotic stability, but our derived configuration allows rationals to implicitly use residual connections.
Importantly, note that these potential residual connections are not rigid, as these functions can progressively learn  for all , \ie we have residual plasticity.

\textbf{Rationals to potentially replace residual blocks.} 
Recall that residual neural networks (ResNets) were initially introduced following the intuition that it is easier to optimise the residual mapping than to optimise the original, unreferenced mapping \citep{HeZRS16}. 
Formally, residual blocks of ResNets propagate an input  through two paths: a transforming block of layers that preserves the dimensionality () and a residual connection (identity). 
In very deep ResNets, it has been observed that feature re-combinations does not occur inside the residual blocks but that transitions to new levels of representations occur during dimensionality changes \cite{VeitWB16, greff2016highway}. 

To investigate this hypothesis, \citeauthor{VeitWB16} have conducted lesioning experiments, where a residual block is removed from the network, and surrounding ones are fine-tuned to recover. Whereas we emphasize that we do not claim that the residual in rationals can replace entire convolutional blocks or that they are generally equivalent, we hypothesize that under the conditions investigated by \citeauthor{VeitWB16} of very deep networks, residual blocks could learn complex activation function-like behaviours. To test this conjecture, we repeat the lesioning experiments, but also test replacing the lesioned block with a rational function that satisfies the residual connection condition derived above. Results are provided in appendix (\cf \ref{sec:res_exp}) and show that such rational functions can efficiently replace some residual blocks of very deep ResNets. 

\subsection{Natural Rational Regularisation} 
We have motivated and shown that the combination of neural and residual plasticity form the central pillars for why rational activation functions are desirable in deep RL. In particular for dynamic and progressive environments, rational plasticity has been observed to provide a substantial boost over alternatives. However, if we circle back to this figure and take a more careful look at the stationary environments, we can observe that our previously investigated tempered rational plasticity (for emphasis, initially allowed to tailor to the task but later ``stopped'' in experimental repetition) can also have an upper edge over full plasticity. The extra rational plasticity at all times might reduce the generalisation capabilities of its agents, particularly on non-diverse stationary environments. In fact, prior works have highlighted the necessity for regularisation methods in RL \citep{farebrother2018regdqn, RoyBHNP20regul, YaratsKF21Image}. 

We thus propose a naturally regularised rational activation version. For this regularised variant, we again draw inspiration from residual blocks. In particular, \citeauthor{greff2016highway} have indicated that sharing the weights can improve learning performances, as shown in Highway \citep{lu2016small} and Residual Networks \citep{liao2016bridging}. In the spirit of these findings, we propose the regularised \textit{joint rationals}, where the key idea is to constraint the input to propagate through different layers but always be activated by the same learnable rational activation function. Rational functions thus share a mutual set of parameters across the network. 
As observable in Fig.~\ref{fig:neural_plasticity}, this regularised form of plasticity increases the agents' scores in the stationary environments and does not deteriorate performances in the progressive ones.


\begin{table*}[t]
\vskip -0.09in
\caption{Neural plasticity leads to vast performance improvements. Normalised mean scores and standard deviations (in percentage, \cf Appendix \ref{app:experiments_details} for the equation) of rigid baselines (\ie DQN and DDQN with Leaky ReLU, DQN with SiLU and SiLU + dSiLU), as well as DQN with plasticity: using PELU, rational (full) and joint-rational (regularised), are reported over five experimental random seeded repetitions (larger mean values are better). The best results are highlighted in \textbf{bold} and runner-ups denoted with  markers. The last rows summarise the number of times best mean scores were obtained by each agent and the number of super-human performances.}
\label{tab:results}
\center
\begin{tabular}{@{}llllllll@{}}
\toprule
\multicolumn{1}{c}{Algorithm}    & \multicolumn{3}{c}{DQN}                                                                                      & \multicolumn{1}{c}{DDQN}                 & \multicolumn{3}{c}{DQN with Plasticity}                                                                                  \\ \midrule
\multicolumn{1}{c}{Activation} & \multicolumn{1}{c}{LReLU} & \multicolumn{1}{c}{SiLU}              & \multicolumn{1}{c}{d+SiLU}               & \multicolumn{1}{c}{LReLU}                & PELU                                  & \multicolumn{1}{c}{rational}               & \multicolumn{1}{c}{joint-rational}         \\ \midrule
Asterix                          & 1.85\tiny1.2         & 0.52\tiny0.6                     & \multicolumn{1}{l|}{2.14\tiny1.4}   & \multicolumn{1}{l|}{48.9\tiny17.7}  & 25.8\tiny3.7                     & \textbf{242}\tiny23.5             & 168\tiny32.6\normalsize   \\
Battlezone                       & 11.4\tiny7.0         & 21.2\tiny15.0                    & \multicolumn{1}{l|}{11.3\tiny6.7}   & \multicolumn{1}{l|}{68.2\tiny34.8}  & 46.6\tiny19.5                    & 70.1\tiny2.1\normalsize  & \textbf{77.4}\tiny8.7              \\
Breakout                         & 558\tiny166          & 93.9\tiny57.6                    & \multicolumn{1}{l|}{11.7\tiny14.0}  & \multicolumn{1}{l|}{286\tiny122}    & 788\tiny79.2                     & 1134\tiny130\normalsize  & \textbf{1210}\tiny36.0             \\
Enduro                           & 16.3\tiny21.3        & 37.0\tiny17.7                    & \multicolumn{1}{l|}{0.37\tiny0.5}   & \multicolumn{1}{l|}{47.7\tiny18.1}  & 24.5\tiny42.6                    & \textbf{141}\tiny15.0             & 129\tiny14.7\normalsize   \\
Jamesbond                        & 8.62\tiny6.4         & 6.08\tiny3.7                     & \multicolumn{1}{l|}{5.28\tiny4.4}   & \multicolumn{1}{l|}{10.7\tiny11.1}  & 74.2\tiny51.5                    & 308\tiny48.5\normalsize  & \textbf{312}\tiny59.5              \\
Kangaroo                         & 11.8\tiny12.5        & 128\tiny95.6\normalsize & \multicolumn{1}{l|}{13.9\tiny18.5}  & \multicolumn{1}{l|}{17.2\tiny14.5}  & 57.7\tiny14.6                    & 107\tiny43.1                      & \textbf{193}\tiny86.8              \\
Pong                             & 101\tiny5.5          & 96.1\tiny12.0                    & \multicolumn{1}{l|}{104\tiny3.3}    & \multicolumn{1}{l|}{91.3\tiny30.8}  & 106.4\tiny2.2                    & 107.0\tiny2.4\normalsize & \textbf{107.3}\tiny2.7             \\
Qbert                            & 55.4\tiny17.1        & 14.2\tiny17.0                    & \multicolumn{1}{l|}{2.74\tiny0.2}   & \multicolumn{1}{l|}{74.0\tiny21.7}  & 101\tiny6.6                      & \textbf{120}\tiny2.8              & 117\tiny4.9\normalsize    \\
Seaquest                         & 0.57\tiny0.4         & 3.67\tiny4.1                     & \multicolumn{1}{l|}{0.18\tiny0.2}   & \multicolumn{1}{l|}{2.17\tiny0.9}   & 9.21\tiny2.5                     & 16.3\tiny0.5\normalsize  & \textbf{18.4}\tiny3.3              \\
Skiing                           & -90.7\tiny37.9       & -111\tiny-0.7                    & \multicolumn{1}{l|}{-85.5\tiny43.4} & \multicolumn{1}{l|}{-86.9\tiny46.6} & -111\tiny-.7                     & \textbf{-59.5}\tiny60.7           & -60.2\tiny56.1\normalsize \\
Spaceinvaders                    & 33.9\tiny4.3         & 33.1\tiny11.9                    & \multicolumn{1}{l|}{32.4\tiny12.4}  & \multicolumn{1}{l|}{31.0\tiny1.0}   & 50.1\tiny3.3\normalsize & 42.3\tiny3.1                      & \textbf{95.1}\tiny17.7             \\
Tennis                           & 8.94\tiny17.3        & 26.3\tiny53.3                    & \multicolumn{1}{l|}{78.5\tiny64.3}  & \multicolumn{1}{l|}{32.1\tiny51.6}  & 106\tiny53.3                     & 257.8\tiny2.8\normalsize & \textbf{258.3}\tiny5.2             \\
Timepilot                        & 14.9\tiny14.3        & 19.3\tiny31.0                    & \multicolumn{1}{l|}{18.3\tiny38.1}  & \multicolumn{1}{l|}{6.61\tiny7.5}   & 124\tiny26.1                     & \textbf{341}\tiny105              & 253\tiny11.0\normalsize   \\
Tutankham                        & 0.03\tiny2.8         & 58.2\tiny48.6                    & \multicolumn{1}{l|}{2.89\tiny4.0}   & \multicolumn{1}{l|}{24.4\tiny-0.4}  & 91.6\tiny29.3                    & 130\tiny10.7\normalsize  & \textbf{134}\tiny29.3              \\
Videopinball                     & 440\tiny123          & 55.8\tiny61.9                    & \multicolumn{1}{l|}{-4.03\tiny32.5} & \multicolumn{1}{l|}{626\tiny241}    & 299\tiny168                      & \textbf{1616}\tiny1026            & 906\tiny539\normalsize    \\ \midrule
\textbf{\# Wins}                 & 0/15                      & 0/15                                  & \multicolumn{1}{l|}{0/15}                & \multicolumn{1}{l|}{0/15}                & 0/15                                  & 6/15                                   & 9/15                                    \\
\textbf{\# Super-Human}          & 3/15                      & 1/15                                  & 1/15                                     & 2/15                                     & 6/15                                  & \textbf{11/15}                         & \textbf{11/15}                          \\ \bottomrule
\end{tabular}

\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{images/main_paper/rainbow_games_scores.pdf} \vspace{-0.33 cm}
\caption{Networks with rational (Rat.) and regularized (Reg.) rational plasticity compared to rigid baselines (DQN, DDQN and Rainbow) over five random seeded runs on eight Atari 2600 games. The resulting mean scores (lines) and standard deviation (transparent area) during training are shown.
As one can see, DDQN does not resolve performance drops but only delays them (e.g. particularly pronounced on Seaquest). 
A figure including the evolution of every agent on all Atari 2600 games is provided in Appendix~\ref{app:score_evo_complete}. Best viewed in colour.}
\label{fig:raibow_scores_evo}
\end{figure*}

\section{Empirical Evidence for Plasticity}
\label{sec:Empirical}
Our intention here is to investigate the benefits of neural plasticity through rational networks for deep reinforcement learning. That is, we investigated the following questions: 
\begin{description}
\item[\textbf{(Q1)}] Do neural networks equipped with rational plasticity outperform rigid baselines?
\item[\textbf{(Q2)}] Can neural plasticity make up for more heavy algorithmic RL advancements? 
\item[\textbf{(Q3)}] Can plasticity address the overestimation problem?
\item[\textbf{(Q4)}] As plasticity can be obtained via adding parameters, how many more parameters would rigid networks need to measure up to rational ones? 
\end{description}

To this end, we compare\footnote{ GPU hours, carried out on a DGX-2 Machine with Nvidia Tesla V100 with 32GB.} our rational networks using the original DQN algorithm \citep{mnih2015human} on 15 different games of the Atari 2600 domain \citep{Brockman2016OpenAIG} and compare these architectures to ones equipped with the Leaky ReLU baseline, the learnable PELU, as well as SiLU () and its derivative dSiLU. \citeauthor{elfwing2018sigmoid} showed that SiLU or a combination of SiLU (on convolutional layers) and its derivative (on fully connected layers) perform better than ReLU in DQN agents on several games \yrcite{elfwing2018sigmoid}. SiLU and dSiLU are ---to our knowledge--- the only activation functions specifically designed for RL applications. We then compare increased neural plasticity provided by (joint-)rational networks to algorithm improvements, namely the Double DQN (DDQN) method \citep{van2016deep}, that tackles DQN's overestimation problem, as well as Rainbow \citep{hessel2018rainbow}. Rainbow incorporates multiple algorithm improvements brought to DQN---Double Q-learning, prioritised experience replay, duelling network architecture, multi-step target, distributional learning and stochastic networks---and is widely used also as a baseline \citep{Lin20space, Hafner21discretewm}. We further explain how neural plasticity can help readdress overestimation. Finally, we evaluate the number of additional weights needed by rigid networks to approximate rational ones.

In practice, we used safe rational activation functions \citep{molina2019pad}, i.e. we used the absolute value of the sum in the denominator to avoid poles. 
This stabilises training and makes the function continuous without creating instabilities. Rationals are shared across layers (adding only  parameters per layer) or through the whole network for the regularised version. We base our experiments on the original neural networks used by the DQN, DDQN and SiLU authors and the same hyper-parameters (\cf Appendix~\ref{app:details_rl}) across all the Atari agents.
For a fair comparison, we report performances using the normalised (\cf Eq.~\ref{eq:score_formulae} in Appendix) mean and standard deviation of the scores obtained by fully trained agents over five seeded reruns for every (D)DQN agent.  However, since often only the best performing RL agent (among the reruns) is reported in the literature, we also provide tables of such scores (\cf Appendix~\ref{app:all_scores_tables}). For the Rainbow algorithm, we unfortunately can only report the results of single runs. A single run took more than 40 days on an NVIDIA Tesla V100 GPU; Rainbow is known to be computationally quite demanding \citep{ObandoCeron2021RevisitingRP}.

{\bf (Q1) DQN with neural plasticity is better than rigid baselines.} 
To start off, we compared RL agents with additional plasticity (from PELU and rationals) to rigid DQN baselines: Leaky ReLU, as well as agents equipped with SiLU and SiLU+dSiLU activation functions.

The results summarised in Tab.~\ref{tab:results} confirm what our earlier figure had shown, but on a larger scale. As one can see, DQN agents with plasticity clearly outperform their rigid activation counterparts. Furthermore, RL agents with functions of the SiLU family outperform Leaky ReLU ones on less than half of the tested games. More importantly, they do not perform better than non-rigid rational networks. DQN with regularised plasticity even obtains higher mean scores than the non-regularised versions 9 out of 15 times. 
Actually, DQN with non-rational plasticity (PELU) obtained super-human performances on 6 out of 15 games, in contrast to the 11 out of 15 times of agents with rational plasticity.

This clearly shows that plasticity, and above all rational plasticity pays off for (deep) RL agents, providing an affirmative answer to \textbf{Q1}.


{\bf (Q2) DQN with neural plasticity can beat more complex RL approaches such as Rainbow.} 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/main_paper/overestimation_bar_plot.pdf}
    \vspace{-0.33 cm}
    \caption{Plasticity naturally reduces overestimation. Relative overestimation values (log scale, smaller values are better, see Eq.~\ref{eq:overestimation}) of both rigid DQN and DDQN, as well as DQN with rational and regularised rational plasticity. Each trained agent is evaluated on 100 completed games (5 random seeds per game per agent, \ie 20 completed games for each seed). Agents with rational plasticity lower overestimation values as much or further than rigid DDQN ones, which has specifically been introduced to this end. Figure best viewed in colour.}
    \label{fig:overestimation_bar_plot}
\end{figure*}
Fig.~\ref{fig:raibow_scores_evo} shows the learning curves of Rainbow and DQN, both with Leaky ReLU baselines, as well as with full and regularised plasticity.
While Rainbow is computationally much heavier ( times slower than DQN in our experiments, with higher memory needs), its rigid form never outperforms the much simpler and more efficient DQN with neural plasticity, and its rational versions dominate in only 1 out of 8 games (Enduro). Rainbow even lost to vanilla DQN on 3 games.

Therefore, DQN agents with rational plasticity are competitive alternatives to the complicated and expensive Rainbow method, answering question \textbf{(Q2)} affirmatively.

{\bf (Q3) Neural plasticity directly tackles the overestimation problem.}
Revisiting Fig.~\ref{fig:raibow_scores_evo}, one can see that Rainbow variants are worst on dynamic environments such as Jamesbond, Time Pilot and particularly Seaquest.
For these games, the performance of rigid (Leaky ReLU) DQN progressively decreases. 

Such drops are well known in the literature and are typically attributed to the overestimation problem of DQN. This overestimation is 
due to the combination of bootstrapping, off-policy learning and a function approximator (neural network) operating by DQN. \citeauthor{van2016deep} showed that inadequate flexibility of the function approximator (either insufficient or excessive) can lead to a slight overestimation of a state-action pair \yrcite{van2016deep}. The max operator in the update rule of DQN then propagates this overestimation while learning with the replay buffer. The overestimated states can stay in the buffer long before the agent revisit (and thus update) them. This can lead to catastrophic performance drops. To mitigate this problem, \citeauthor{van2016deep} introduced a second network to separate action selection from action evaluation, resulting in Double DQN (DDQN). 

We have compared the original DDQN approach (\ie equipped with Leaky ReLU), which is rigid, to vanilla DQN with neural plasticity on Atari games. As one can see in Tab.~\ref{tab:results},  
DQN with rational plasticity even outperforms the more complex DDQN algorithm on every considered Atari game. This reinforces the affirmative answer to \textbf{(Q1)} from earlier on. 

More importantly, we have computed the relative overestimation values of the (D)DQN, both with and without neural plasticity, following:

where the return  corresponds to , with the observed reward  and the discount factor . 

The results are summarised in Fig.~\ref{fig:overestimation_bar_plot}. 
As one can see, plasticity helps to reduce overestimation drastically. The game for which DDQN substantially reduces the overestimation are Jamesbond, Kangaroo, Tennis, Time Pilot and Seaquest. For these games, DDQN obtains the best performances among all rigid variants only on Jamesbond (\cf Tab.~\ref{tab:results}). 
Moreover, Fig.~\ref{fig:raibow_scores_evo} reveals that the performance drops of corresponding DDQN rigid agents are only delayed and not prevented. The performance drops thus happen after the 200th epoch, after which the training of RL agents is usually stopped. 

Whereas we agree that overestimation might play a role in the performance drops on progressive environments (\cf Fig.~\ref{fig:raibow_scores_evo}: Jamesbond, TimePilot and Seaquest), it cannot fully explain the phenomena.
Instead, RL agents with higher neural plasticity can handle these games much better while only having a few more parameters. Hence, we advocate that neural plasticity better deals with distribution shifts due to the dynamics of games with progressive environments. 

Perhaps surprisingly, (regularised) rational plasticity not only works well on challenging progressive environments but also on much simpler ones such as Enduro, Pong and Q*bert, where more flexibility is likely to hurt. 
Luckily, overestimation due to high flexibility does not happen here, as one can see in Fig.~\ref{fig:overestimation_bar_plot}. 
Moreover, the activation functions learned for these games have a simpler profile than those learned on more complicated games like Kangaroo and Time Pilot (\cf Appendix~\ref{app:learned_afs}). The rational activation functions thus seem to adapt to the environment's complexity and the policy they need to model. This clearly provides an affirmative answer to \textbf{(Q3)}.

\textbf{(Q4) Adding parameters through rationals efficiently augments plasticity.}
Compared to rigid alternatives, the joint-rational networks embed  more parameters in total and always outperform (\cf Tab.~\ref{tab:results}) PELU ones (that have 12 additional parameters). Our proposed method to add plasticity via rational activation functions thus efficiently augments the capacity of the network. However, ReLU layers can theoretically approximate rational functions \cite{telgarsky2017neural}. Augmenting the number of layers (or neurons per layer) is thus, theoretically, a costly alternative to augment the plasticity. How many parameters are practically needed in rigid networks to obtain similar performances? Searching for bigger equivalent architectures for RL agents is tedious, as RL training curves possess considerably more variance and noise than SL ones \cite{miao2021rldarts}, but this question is not restricted to reinforcement learning. We thus answer it by highlighting the generality of our insights, demonstrated by further investigation on a classification scenario, in addition to the RL experiments of Tab~\ref{tab:results}. We report our findings in Tab.~\ref{tab:relu_rat_cifar_comp}, where networks with rational activations are shown to not only outperform Leaky ReLU ones at the same amount of parameters, but also to outperform deeper and more heavily parametrised neural networks (indicated by the pairs of colours). For example, a rational activated VGG4 not only performs better than a rigid Leaky ReLU VGG4 at 1.37M parameters, but even performs similarly to the 4.71M parameters VGG6 that uses Leaky ReLU.


\begin{table}[t]
\vskip -0.08in
\tiny
\caption{Shallow Rational Networks perform as deeper Leaky ReLU ones. VGG networks with different numbers of layers are evaluated on CIFAR10 and CIFAR100. Rational VGG4 has similar performances as VGG6 network, with  times less parameters, and Rational VGG6 outperforms VGG8, with two times less parameters. Shaded colour pairs included for emphasis.}
\vskip 0.15in
\centering
\resizebox{\columnwidth}{!}{
\setlength\tabcolsep{2.pt}
\small
\begin{tabular}{@{}l|l|ll|ll|ll@{}}
\toprule
\multicolumn{2}{l|}{Architecture}          & \multicolumn{2}{c|}{VGG4}                             & \multicolumn{2}{c|}{VGG6}                             & \multicolumn{2}{c}{VGG8}                             \\ \midrule
\multicolumn{2}{l|}{Activation}            & \multicolumn{1}{c}{LReLU} & \multicolumn{1}{c|}{Rat.} & \multicolumn{1}{c}{LReLU} & \multicolumn{1}{c|}{Rat.} & \multicolumn{1}{c}{LReLU} & \multicolumn{1}{c}{Rat.} \\ \midrule
\multirow{2}{*}{C10}    & Train    & 83.0±.3                   & \bcol 87.1±.6             & \bcol 86.9±.1             & \ocol 89.2±.2             & \ocol 86.7±.2             & \gcol 89.2±.1                  \\
                            & Test     & 80.0±1.                   & \bcol 84.3±.5             & \bcol 83.2±.6             & \ocol 85.4±.6             & \ocol 83.4±1.             & \gcol 85.8±.9                  \\ \midrule
\multirow{2}{*}{C100}   & Train    & 64.7±.8                   & \bcol 70.2±.0             & \bcol 70.6±.6             & \ocol 75.4±.1             & \ocol 71.8±.3             & \gcol 74.3±.3                  \\
                            & Test     & 56.6±.8                   & \bcol 58.9±.6             & \bcol 59.0±.5             & \ocol 58.4±.3             & \ocol 57.1±1.             & \gcol 59.1±.9                  \\ \midrule
\multicolumn{2}{l|}{\# Net params} & \multicolumn{2}{c|}{1.37M}                            & \multicolumn{2}{c|}{4.71M}                            & \multicolumn{2}{c}{9.27M}                            \\ \bottomrule
\end{tabular}}
\label{tab:relu_rat_cifar_comp}
\vskip -0.15in
\end{table}


All experimental results together clearly show that neural plasticity through rational networks considerably benefits deep reinforcement learning in highly efficient manner.

\section{Related Work}
Next to the related work discussed throughout the paper, our work on plasticity is also related to research lines on neural architecture search and activation functions in deep reinforcement learning. 



\textbf{Neural Architectures for Deep Reinforcement Learning.}
\citeauthor{Cobbe2019QuantifyingGI} showed that the architecture of IMPALA \citep{Espeholt18IMPALA}, notably containing residual blocks, improved the performances over the original Nature-CNN network used in DQN \yrcite{Cobbe2019QuantifyingGI}. Motivated by these findings, \cite{miao2021rldarts} recently applied neural architecture search to RL tasks and demonstrated that the optimal architecture and its complexity highly depend on the environment. Their search provides different architectures for different environments, with varying activation functions across layers and potential residual connections.
Continuously modifying the complexity of the neural network based on the noisy reward signal in a complex architectural space is extremely resource demanding, particularly for large scale problems. Many RL specific problems, such as noisy rewards \citep{henderson2018matters}, input interdependency \citep{mnih2015human}, policy instabilities \citep{Haarnoja2018SAC}, sparse rewards, difficult credit assignment \citep{Mesnard2021credit}, complicate an automated architecture search. 

\textbf{The choice of activation functions.}
Many different activation functions have been adopted across different domains (\eg Leaky ReLU in YOLO \citep{RedmonDGF16}, Tanh in PPO \citep{SchulmanWDRK17}, GELU in GPT-3 \citep{BrownMRSKDNSSAA20}), indicating that the relationship between the choice of activation functions and performance of neural networks is highly dependent on the task, architecture, hyper-parameters and the dataset (or environment). As shown in this paper, parametric functions provide plasticity. \citeauthor{molina2019pad} showed that rationals outperform other learnable activation function types on a set of SL tasks \yrcite{molina2019pad}. \citeauthor{telgarsky2017neural} showed that rationals are locally better approximants than polynomials \yrcite{telgarsky2017neural}. Finally, \citeauthor{BoulleNT20} later showed that composing low-degree rational functions require fewer parameters to approximate a ReLU Network \yrcite{BoulleNT20}. While this is indeed more efficient in terms of parameters, compositions of rationals functions need to be computed sequentially, which is slower when using gradient descent methods. 

\section{Limitations and Future Work}
We have shown the benefits of rational activation functions for deep RL as a consequence of both their neural and residual plasticity. In our derivation for closure under residuals, we have deduced that the degree of the polynomial in the numerator needs to be greater than that of the denominator. Correspondingly, we have based our empirical investigations on the degrees (5, 4). Interesting future work would be to further automatically select suitable degrees that fulfil this condition. 
As additional future avenues, one should also explore neural plasticity in more advanced deep RL approaches, including short term memory \citep{Kapturowski19r2d2}, finer exploration strategy \citep{BadiaSVGPKTAPBB20NGU}, or include neural plasticity into architecture search, without having to perform a combinatorial search across possible activation functions.



\section{Conclusion}
In this work, we have highlighted the central role of neural plasticity in deep reinforcement learning algorithms, and have motivated the use of rational activation functions, as a lightweight way of boosting RL agents performances. We derived a condition, under which rational functions embed residual connections. Then the naturally regularised joint-rational activation function was developed inspired by weight sharing in residual networks.

The simple DQN algorithm equipped with these (regularised) rational forms of plasticity becomes a competitive alternative to more complicated and costly algorithms, such as Double DQN and Rainbow. Fortunately, the complexity of these rational functions also seem to automatically adapt to the one of the environment used for training. Their use could be a substitute for more expensive architectural searches. We thus hope that they will be adopted in future deep reinforcement learning algorithms, as they can provide agents with the necessary neural plasticity required by stationary, dynamic and progressive reinforcement learning environments.


\bibliography{icml_arxiv_paper}
\bibliographystyle{icml2022}


\newpage
\appendix
\onecolumn
\section{Appendix}

As mentioned in the main body, the appendix contains additional materials and supporting information for the following aspects: results on replacing residual blocks with rational activation functions (\ref{sec:res_exp}), every final and maximal scores obtained by the reinforcement learning agents used in our experiments (\ref{app:all_scores_tables}), the evolutions of these scores (\ref{app:score_evo_complete}), the different environment types with illustrations of their changes (\ref{app:env_classification}), graphs of the learned rational activation functions (\ref{app:learned_afs}) and technical details for reproducibility (\ref{app:experiments_details}).

\subsection{Residual block learn of deep ResNet learn activation function-like behavior.}
\label{sec:res_exp}
We present in this section lesioning experiments, where a residual block is lesioned from a pretrained Residual Network, and the surrounding blocks are fine-tuned (with a learning rate of ) for  epochs. These lesioning experiments were first conducted by \citeauthor{VeitWB16} \yrcite{VeitWB16}. We also perform rational lesioning, where we replace a block by an (identity initialised)\footnote{all weights are initially set to  but  (and ), both set to .} rational activation function (instead of removing the block), and train the activation function along with the surrounding blocks. The used rational functions have the same order as in every other experiment (), that satisfies the rational residual property derive in the paper). We report recovery percentages, computed following:

We also provide the amount of dropped parameters of each lesioning.

\begin{table}[H]
\vspace{-0.33cm}
\caption{Rational functions improve lesioning. The recovery percentages for finetuned networks after lesioning \citep{VeitWB16} of a ResNet layer's (L) block (B) are shown. Residual blocks were lesioned, \ie replaced with the identity (Base) or a rational from a pretrained ResNet101 (44M parameters). Then, the surrounding blocks (and implanted rational activation function) are retrained for 15 epochs. Larger percentages are better, best results are in \textbf{bold}.}
\label{tab:surgery_comp}
\vskip 0.15in
\centering
\setlength\tabcolsep{2.pt}
\begin{tabular}{@{}llllll@{}}
\toprule
Recovery (\%)                  & Lesioning      & L2B3           & L3B19         & L3B22         & L4B2         \\  \toprule
\multirow{2}{*}{Training} & Original \citep{VeitWB16} & 100.9           & 90.5           & 100          & 58.9          \\ & Rational (ours)    & \textbf{101.1 } & \textbf{104} & \textbf{120} & \textbf{91.1} \\ \midrule
\multirow{2}{*}{Testing}  & Original \citep{VeitWB16} & \textbf{93.1}   & 97.1           & 81.6           & 81.7          \\ & Rational (ours)      & 90.5            & \textbf{97.6}  & \textbf{91.5}  & \textbf{85.3} \\ \midrule
\multicolumn{2}{c}{\% dropped params}               & 0.63            & 2.51           & 2.51           & 10.0          \\ 
\bottomrule
\end{tabular}
\end{table}

As the goal is to show that flexible rational functions can achieve similar modelling capacities to the residual blocks, we did not apply regularisation methods and mainly focused on training accuracies. We can clearly observe that rational activation functions lead to performance improvements that even surpass the original model, or are able to maintain performances when the amount of dropped parameters rises.









\subsection{Complete raw scores table for Deep Reinforcement Learning}
\label{app:all_scores_tables}
Through this work, we showed the performance superiority of reinforcement learning agents that embed additional plasticity provided by learnable rational activation functions. We used human normalised scores (\cf Eq.~\ref{eq:score_formulae}) for readability. For completeness, we provide in this section the final raw scores of every trained agent. As many papers provide the maximum obtained score among every epoch and every agent, even if we consider it to be an inaccurate and noisy indicator of the performances, for which random actions can still be taken (because of -greedy strategy also being used in evaluation). A fairer indicator to compare methods is the mean score. We thus also provide final mean scores (of agents retrained among 5 seeded reruns) with standard deviation. We start off by providing the human scores used for normalisation (provided by \citeauthor{van2016deep}, in Table 5), then provide final mean and maximum obtained raw scores of every agent.

\textbf{Human scores used for normalisation:}\\
Asterix: , \ \ \ Battlezone: , \ \ \ Breakout: , \ \ \ Enduro: , \ \ \ Jamesbond: , \ \ \ Kangaroo: , \ \ \ Pong: , \ \ \ Q*bert: , \ \ \ Seaquest: , \ \ \ Skiing: , \ \ \ Space Invaders: , \ \ Tennis: ,  \ \ \ Time Pilot: ,  \ \ \ Tutankham: ,  \ \ \ Video Pinball: 

\newpage
\textbf{Final mean scores of other agents:}

\begin{table}[H] 

\setlength\tabcolsep{2 pt}
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lllllllll@{}}
\toprule
Algorithm                       & \multicolumn{1}{c}{Random}              & \multicolumn{3}{c}{DQN}                                                                          & \multicolumn{1}{c}{DDQN}                  & \multicolumn{3}{c}{DQN with Plasticity}                                                     \\ \midrule
Network type                    & \multicolumn{1}{c}{-}                   & \multicolumn{1}{c}{LReLU} & \multicolumn{1}{c}{SiLU} & \multicolumn{1}{c}{d+SiLU}                & \multicolumn{1}{c}{LReLU}                 & \multicolumn{1}{c}{PELU} & \multicolumn{1}{c}{full}       & \multicolumn{1}{c}{regularised} \\ \midrule
\multicolumn{1}{l|}{Asterix}    & \multicolumn{1}{l|}{67.9\tiny2.2}  & 206\tiny90           & 107\tiny45          & \multicolumn{1}{l|}{228\tiny108}     & \multicolumn{1}{l|}{3723\tiny1324}   & 1998\tiny275        & \textbf{18109\tiny1755}   & 12621\tiny2436             \\
\multicolumn{1}{l|}{Battlezone} & \multicolumn{1}{l|}{788\tiny38}    & 4464\tiny2291        & 7612\tiny4877       & \multicolumn{1}{l|}{4429\tiny2183}   & \multicolumn{1}{l|}{22775\tiny11265} & 15807\tiny6320      & 23403\tiny701             & \textbf{25749\tiny2837}    \\
\multicolumn{1}{l|}{Breakout}   & \multicolumn{1}{l|}{0.14\tiny01}   & 155\tiny46           & 26.2\tiny16         & \multicolumn{1}{l|}{3.4\tiny3.89}    & \multicolumn{1}{l|}{79.4\tiny33.8}   & 219\tiny22          & 315\tiny36                & \textbf{336\tiny10}        \\
\multicolumn{1}{l|}{Enduro}     & \multicolumn{1}{l|}{0\tiny0}       & 121\tiny158          & 274\tiny131         & \multicolumn{1}{l|}{2.77\tiny3.41}   & \multicolumn{1}{l|}{353\tiny134}     & 181\tiny315         & \textbf{1043\tiny111}     & 957\tiny109                \\
\multicolumn{1}{l|}{Jamesbond}  & \multicolumn{1}{l|}{6.39\tiny0.41} & 37.6\tiny23.6        & 28.4\tiny13.8       & \multicolumn{1}{l|}{25.5\tiny16.2}   & \multicolumn{1}{l|}{45.2\tiny40.7}   & 275\tiny187         & 1122\tiny176              & \textbf{1137\tiny216}      \\
\multicolumn{1}{l|}{Kangaroo}   & \multicolumn{1}{l|}{14.2\tiny0.9}  & 335\tiny342          & 3500\tiny2607       & \multicolumn{1}{l|}{393\tiny504}     & \multicolumn{1}{l|}{484\tiny395}     & 1586\tiny398        & 2940\tiny1175             & \textbf{5266\tiny2365}     \\
\multicolumn{1}{l|}{Pong}       & \multicolumn{1}{l|}{-20.2\tiny0}   & 15.9\tiny2           & 14.1\tiny4.3        & \multicolumn{1}{l|}{16.9\tiny1.2}    & \multicolumn{1}{l|}{12.4\tiny11}     & 17.8\tiny0.8        & 18\tiny0.9                & \textbf{18.1\tiny1}        \\
\multicolumn{1}{l|}{Q*bert}     & \multicolumn{1}{l|}{40.6\tiny2.8}  & 6715\tiny2058        & 1754\tiny2048       & \multicolumn{1}{l|}{371\tiny28}      & \multicolumn{1}{l|}{8954\tiny2616}   & 12143\tiny795       & \textbf{14436\tiny336}    & 14080\tiny593              \\
\multicolumn{1}{l|}{Seaquest}   & \multicolumn{1}{l|}{20.1\tiny0.4}  & 250\tiny162          & 1504\tiny1677       & \multicolumn{1}{l|}{94.6\tiny87.2}   & \multicolumn{1}{l|}{898\tiny353}     & 3740\tiny991        & 6603\tiny200              & \textbf{7461\tiny1321}     \\
\multicolumn{1}{l|}{Skiing}     & \multicolumn{1}{l|}{-16104\tiny92} & -27365\tiny4794      & -29890\tiny4        & \multicolumn{1}{l|}{-26725\tiny5485} & \multicolumn{1}{l|}{-26892\tiny5881} & -29912\tiny10       & \textbf{-23487\tiny7624}  & -23582\tiny7058            \\
\multicolumn{1}{l|}{Space Inv.} & \multicolumn{1}{l|}{51.6\tiny1.1}  & 531\tiny62           & 520\tiny169         & \multicolumn{1}{l|}{509\tiny176}     & \multicolumn{1}{l|}{490\tiny15}      & 759\tiny48          & 650\tiny45                & \textbf{1395\tiny251}      \\
\multicolumn{1}{l|}{Tennis}     & \multicolumn{1}{l|}{-23.9\tiny0.0} & -22.4\tiny3.0        & -19.4\tiny9.2       & \multicolumn{1}{l|}{-10.4\tiny11.1}  & \multicolumn{1}{l|}{-18.4\tiny8.9}   & -5.6\tiny9.2        & 20.5\tiny0.5              & \textbf{20.6\tiny0.9}      \\
\multicolumn{1}{l|}{TimePilot}  & \multicolumn{1}{l|}{688\tiny30}    & 1428\tiny739         & 1644\tiny1566       & \multicolumn{1}{l|}{1594\tiny1918}   & \multicolumn{1}{l|}{1016\tiny401}    & 6818\tiny1323       & \textbf{17632\tiny5242}   & 13261\tiny576              \\
\multicolumn{1}{l|}{Tutankham}  & \multicolumn{1}{l|}{3.51\tiny0.54} & 3.55\tiny4.3         & 81.9\tiny66         & \multicolumn{1}{l|}{7.41\tiny5.96}   & \multicolumn{1}{l|}{36.4\tiny0}      & 127\tiny40          & 179\tiny15                & \textbf{184\tiny40}        \\
\multicolumn{1}{l|}{VideoPinb.}                      & \multicolumn{1}{l|}{6795\tiny461}                       & 45683\tiny11383      & 11730\tiny5941      & \multicolumn{1}{l|}{6439\tiny3336}                        & \multicolumn{1}{l|}{62151\tiny21791}                      & 42051\tiny15356     & \textbf{149712\tiny91219} & 86942\tiny48143            \\ \bottomrule
\end{tabular}
}
\caption{Final mean raw scores (with std. dev.) of rigid baselines (\ie DQN and DDQN with Leaky ReLU, DQN with SiLU and SiLU + dSiLU), as well as DQN with full plasticity (\ie using rational activation functions) and regularised plasticity (\ie using joint-rational ones) on Atari 2600 games, averaged over  seeded reruns (larger mean values are better).}
\label{app:all_scores}
\end{table}

\textbf{Maximum obtained scores:}

\begin{table}[H]
\centering
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{@{}lllllllll@{}}
\toprule
Algorithm                       & \multicolumn{1}{c}{Random}  & \multicolumn{3}{c}{DQN}                                                            & \multicolumn{1}{c}{DDQN}    & \multicolumn{3}{c}{DQN with Plasticity}                                               \\ \midrule
Network type                    & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{LReLU} & \multicolumn{1}{c}{SiLU} & \multicolumn{1}{c}{d+SiLU}  & \multicolumn{1}{c}{LReLU}   & \multicolumn{1}{c}{PELU} & \multicolumn{1}{c}{full} & \multicolumn{1}{c}{regularised} \\ \midrule
\multicolumn{1}{l|}{Asterix}    & \multicolumn{1}{l|}{71}     & 9250                      & 3400                     & \multicolumn{1}{l|}{3800}   & \multicolumn{1}{l|}{20150}  & 9300                     & 84950                    & 49700                           \\
\multicolumn{1}{l|}{Battlezone} & \multicolumn{1}{l|}{843}    & 88000                     & 81000                    & \multicolumn{1}{l|}{70000}  & \multicolumn{1}{l|}{97000}  & 68000                    & 78000                    & 94000                           \\
\multicolumn{1}{l|}{Breakout}   & \multicolumn{1}{l|}{0}      & 427                       & 370                      & \multicolumn{1}{l|}{344}    & \multicolumn{1}{l|}{411}    & 430                      & 864                      & 864                             \\
\multicolumn{1}{l|}{Enduro}     & \multicolumn{1}{l|}{0}      & 1243                      & 928                      & \multicolumn{1}{l|}{1041}   & \multicolumn{1}{l|}{1067}   & 1699                     & 1946                     & 1927                            \\
\multicolumn{1}{l|}{Jamesbond}  & \multicolumn{1}{l|}{6}      & 5600                      & 5750                     & \multicolumn{1}{l|}{700}    & \multicolumn{1}{l|}{7500}   & 6150                     & 9250                     & 13300                           \\
\multicolumn{1}{l|}{Kangaroo}   & \multicolumn{1}{l|}{15}     & 14800                     & 15600                    & \multicolumn{1}{l|}{10200}  & \multicolumn{1}{l|}{13000}  & 12400                    & 16200                    & 16800                           \\
\multicolumn{1}{l|}{Pong}       & \multicolumn{1}{l|}{-20}    & 21                        & 21                       & \multicolumn{1}{l|}{21}     & \multicolumn{1}{l|}{21}     & 21                       & 21                       & 21                              \\
\multicolumn{1}{l|}{Q*bert}     & \multicolumn{1}{l|}{45}     & 19425                     & 11700                    & \multicolumn{1}{l|}{5625}   & \multicolumn{1}{l|}{19200}  & 18900                    & 24325                    & 25075                           \\
\multicolumn{1}{l|}{Seaquest}   & \multicolumn{1}{l|}{20}     & 7440                      & 8300                     & \multicolumn{1}{l|}{740}    & \multicolumn{1}{l|}{15830}  & 14860                    & 9100                     & 26990                           \\
\multicolumn{1}{l|}{Skiing}     & \multicolumn{1}{l|}{-15997} & -5987                     & -6505                    & \multicolumn{1}{l|}{-6267}  & \multicolumn{1}{l|}{-5359}  & -5495                    & -5368                    & -5612                           \\
\multicolumn{1}{l|}{Space Inv.} & \multicolumn{1}{l|}{53}     & 2435                      & 2205                     & \multicolumn{1}{l|}{2460}   & \multicolumn{1}{l|}{2290}   & 2030                     & 2490                     & 3790                            \\
\multicolumn{1}{l|}{Tennis}     & \multicolumn{1}{l|}{-23}    & 8                         & 1                        & \multicolumn{1}{l|}{-1}     & \multicolumn{1}{l|}{4}      & -1                       & 24                       & 36                              \\
\multicolumn{1}{l|}{Time Pilot} & \multicolumn{1}{l|}{730}    & 11900                     & 15500                    & \multicolumn{1}{l|}{12500}  & \multicolumn{1}{l|}{12200}  & 16300                    & 72000                    & 28000                           \\
\multicolumn{1}{l|}{Tutankham}  & \multicolumn{1}{l|}{4}      & 249                       & 267                      & \multicolumn{1}{l|}{267}    & \multicolumn{1}{l|}{274}    & 397                      & 334                      & 309                             \\
\multicolumn{1}{l|}{VideoPinb.} & \multicolumn{1}{l|}{7599}   & 998535                    & 950250                   & \multicolumn{1}{l|}{338512} & \multicolumn{1}{l|}{991669} & 322655                   & 997952                   & 998324                          \\ \bottomrule
\end{tabular}
}
\caption{Maximum obtained scores (with std. dev.) of rigid baselines (\ie DQN and DDQN with Leaky ReLU, DQN with SiLU and SiLU + dSiLU), as well as DQN with full plasticity (\ie using rational activation functions) and regularised plasticity (\ie using joint-rational ones) on Atari 2600 games, averaged over  seeded reruns (larger values are better).}
\end{table}

\newpage
\textbf{Final mean and maximum obtained scores of Rainbow agents:}
\begin{table}[H]
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
Evaluation                         & \multicolumn{3}{c}{\textbf{Final Mean Scores}}                          & \multicolumn{3}{c}{Max. Obtained Scores}      \\ \midrule
\multicolumn{1}{l|}{Plasticity}    & rigid & full  & \multicolumn{1}{l|}{regularised} & rigid & full  & regularised \\ \midrule
\multicolumn{1}{l|}{Breakout}      & 52     & 279   & \multicolumn{1}{l|}{\textbf{303}}         & 383    & \textbf{569}   & \textbf{569}         \\
\multicolumn{1}{l|}{Enduro}        & 844    & \textbf{1473}  & \multicolumn{1}{l|}{1470}        & 1388   & \textbf{1973}  & 1964        \\
\multicolumn{1}{l|}{Kangaroo}         & 40    & \textbf{2157} & \multicolumn{1}{l|}{2139}  & \textbf{6300}  & 6000 & 4800   \\
\multicolumn{1}{l|}{Q*bert}         & 149    & \textbf{11931} & \multicolumn{1}{l|}{11551}       & 16125  & \textbf{23550} & \textbf{23550}       \\
\multicolumn{1}{l|}{Seaquest}      & 82     & 247   & \multicolumn{1}{l|}{\textbf{282}}         & 920    & \textbf{1280}  & \textbf{1280}        \\
\multicolumn{1}{l|}{Space Inv.} & 595    & \textbf{1263}  & \multicolumn{1}{l|}{1157}        & 2070   & \textbf{3395}  & 2875        \\
\multicolumn{1}{l|}{Time Pilot}     & 3926   & 5386  & \multicolumn{1}{l|}{\textbf{6411}}        & 12700  & \textbf{15900} & \textbf{15900}       \\ \bottomrule
\end{tabular}
\caption{Final mean and maximum obtained scores obtained by rigid Rainbow agents (\ie using Leaky ReLU), as well as Rainbow with full (\ie using rational activation functions) and regularised (\ie using joint-rational ones) plasticity (only 1 run because of computational cost, larger values are better).}
\end{table}

\newpage
\subsection{Evolution of the scores on every game}
\label{app:score_evo_complete}
The main part present some graphs that compares performance evolutions of the Rainbow and DQN agents with plasticity, as well as Rigid DQN, DDQN and Rainbow agents. We provide hereafter the evolution of the scores of every tested DQN and the DDQN agents on the complete game set. DQN agents with higher plasticity are always the best-performing ones. Experiments on several games (e.g. Jamesbond, Seaquest) show that using DDQN does not prevent the agent's performance drop but only delays it. 

\begin{figure}[H]
\centering
\includegraphics[width=.85\textwidth]{images/appendix/scores_evolutions_all_games.pdf} 
\caption{Smoothed (\cf Eq. \ref{eq:smooth}) evolutions of the scores on every tested game for DQN agents with full (\ie using rational activation functions) and regularised (\ie using joint-rational ones) plasticity, and original DQN agents using Leaky ReLU, SiLU and SiLU+dSiLU, as well as for DDQN agents with Leaky ReLU.}
\end{figure}

\subsection{Environments types: stationary, dynamics and progressive}
\label{app:env_classification}
The used environments have been separated in 3 categories, describing their potential changes through agents learning. This categorisation is here illustrated with frames of the tested games. As one can see: Breakout, Kangaroo, Pong, Skiing, Space Invaders, Tennis, Tutankham and VideoPinball can be categorised as \textbf{stationary environment}, as changes are minimal for the agents in these games. Asterix, BattleZone, Q*bert and Enduro present environment changes, that are early reached by the playing agents, and are thus \textbf{dynamic environments}. Finally, Jamesbond, Seaquest and Time Pilot correspond to \textbf{progressive environments}, as the agents needs to master early changes to access new parts of these environments.

\begin{figure}[H]
\centering
\includegraphics[width=.85\textwidth]{images/appendix/games_all.pdf} \caption{Images extracted from DQN agents with full plasticity playing the set of 15 Atari 2600 games used in this paper. Stationary environments (e.g. Pong, Video Pinball) do not evolve during training, dynamic ones provide different input/output distributions that are early accessible in the game (e.g Q*bert, Enduro) and progressive ones (e.g. Jamesbond, Time Pilot) require the agent to improve for the it to evolve.}
\label{fig:score_evo_complete}
\end{figure}

\subsection{Learned rational activation functions}
\label{app:learned_afs}
We have explained in the main text how rational functions of agents used on different games can exhibit different complexities. This section provides the learned parametric rational functions learned by DQN agents with full plasticity (left) and by those with regularised plasticity (right) after convergence for every different tested game of the gym Atari 2600 environment.
Kernel Density Estimations (with Gaussian kernels) of input distributions indicates where the functions are most activated. Rational functions from agents trained on simpler games (\eg Enduro, Pong, Q*bert) have simpler profiles (\ie fewer distinct extremas).

\begin{figure}[H]
\centering
\includegraphics[width=0.83\textwidth]{images/appendix/all_rationals_first_part.pdf} \end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/appendix/all_rationals_second_part.pdf}
\caption{Profiles (dark blue) and input distributions (light blue) of rational functions (left) and joint-rational ones (right) of DQN agents on the different tested games. (Joint-)rational functions from agents of simpler games have simpler profiles (\ie fewer distinct extrema).}
\label{fig:act_func_shapes}
\end{figure}









\subsection{Technical details to reproduce the experiments}
\label{app:experiments_details}
We here provide details on our experiments for reproducibility.
\subsubsection{Supervised Learning Experiments}
\label{app:details_sl}
For the lesioning experiment, we used an available\footnote{\url{https://download.pytorch.org/models/resnet101-5d3b4d8f.pth}} pretrained Residual Network (original). We then remove the corresponding block (and potentially replace it with an identity initialised rational activation function) (surgered). We finetune the new models, allowing for optimisation of the previous and next layers (and potentially the rational function) for  epochs with SGD (learning rate of ).

For the classification experiments conducted on CIFAR10 and CIFAR100, we let every network learn for  epochs. We use SGD as the optimisation algorithm, with a learning rate of  and  as batch size. The VGG networks contain successive VGG blocks that all consist of  convolutional layers,  input channels and  output channels, stride  and padding , followed by an activation function, and  Max Pooling layer. For each used architecture, the (, , ) parameters of the successive blocks are:

\begin{itemize}
    \item VGG4: 
    \item VGG6: 
    \item VGG8: 
\end{itemize}

The output of these blocks is then passed on to a classifier (linear layer). Only activation functions differ between the Leaky ReLU and the Rational versions.

\subsubsection{Reinforcement Learning Experiments}
\label{app:details_rl}
To ease the reproducibility of our the reinforcement learning experiments, we used the \textit{Mushroom RL} library \citep{deramo2020mushroomrl}. We used states consisting of 4 consecutive grey-scaled images, downsampled to .

\textbf{Network Architecture.}
The input to the network is thus a 84x84x4 tensor containing a rescaled, and gray-scaled, version of the last four frames. The first convolution layer convolves the input with 32 filters of size 8 (stride 4), the second layer has 64 layers of size 4 (stride 2), the final convolution layer has 64 filters of size 3 (stride 1). This is followed by a fully-connected hidden layer of 512 units. \\
All these layers are separated by the corresponding activation functions (either Leaky ReLU, SiLU, SiLU for convolution layers and dSiLU for linear ones, PELU, rational functions and joint-rational ones (of order  and , initialised to approximate Leaky ReLU). 

\textbf{Hyper-parameters.}
We evaluate the agents every K steps, for K steps. The target network is updated every K steps, with a replay buffer memory of initial size K, and maximum size K, except for Pong, for which all these values are divided by . The discount factor  is set to  and the learning rate is . We do not select the best policy among seeds between epochs. We use the simple -greedy exploration policy, with the  decreasing linearly from  to  over 1M steps, and an  of  is used for testing.

The only difference from the evaluation of \cite{mnih2015human} and of \cite{van2016deep} evaluation is the use of the Adam optimiser instead of RMSProp, for every evaluated agent.

\textbf{Normalisation techniques.}
To compute human normalised scores, we used the following equation:



For readability, the curves plotted in the Fig.~\ref{fig:raibow_scores_evo} and Fig.~\ref{fig:score_evo_complete} are smoothed following:

with .



\end{document}
