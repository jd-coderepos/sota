\section{Experiments}
\label{seq:experiments}



\subsection{Datasets and evaluation measure}
\myparagraph{Single frame.} We evaluate our single frame models on
the MPII Multi-Person dataset~\cite{andriluka14cvpr}. 
We report all intermediate results on a validation set of $200$ images
sampled uniformly at random (MPII Multi-Person Val), while major
results and comparison to the state of the art are reported on the
test set.

\myparagraph{Video.} In order to evaluate video-based models we
introduce a novel \videodata~dataset\footnote{Dataset is available at \url{pose.mpi-inf.mpg.de/art-track}.}.
 To this end we manually selected challenging
keyframes from MPII Multi-Person dataset. Selected
keyframes represent crowded scenes with highly articulated
people engaging in various dynamic activities. In addition to each
keyframe, we include +/-$10$ neighboring frames from the
corresponding publicly available video sequences, and annotate every
second frame\footnote{The annotations in the original key-frame are kept unchanged.}.
Each body pose was annotated following the standard annotation
procedure~\cite{andriluka14cvpr}, while maintaining person identity
throughout the sequence. In contrast to MPII Multi-Person where some
frames may contain non-annotated people, we annotate all people
participating in the activity captured in the video, and add ignore
regions for areas that contain dense crowds (e.g. static spectators in
the dancing sequences).  In total, our dataset consists of $28$
sequences with over $2,000$ annotated poses.



\myparagraph{Evaluation details.} 
The average precision (AP) measure~\cite{pishchulin16cvpr} is used for evaluation of pose estimation
accuracy. For each algorithm we also report run time \timecnn~of the proposal generation
and \timeinfer~of the graph partitioning stages. All time measurements were conducted on a single
core Intel Xeon $2.70$GHz. Finally we also evaluate tracking perfomance using standard MOTA metric \cite{Bernardin:2008:CLE}.

Evaluation on our \videodata~dataset is performed on the full
frames using the publicly available evaluation kit
of~\cite{andriluka14cvpr}. On MPII Multi-Person we follow the official
evaluation
protocol\footnote{http://human-pose.mpi-inf.mpg.de/\#evaluation} and
evaluate on groups using the provided rough group location and scale.

\subsection{Single-frame models}
\tabcolsep 1.5pt
\begin{table}[tbp]
 \scriptsize
  \centering
  \begin{tabular}{@{} l c ccc ccc c | c  cc@{}}
    \toprule
Setting& Head   & Sho  & Elb & Wri & Hip & Knee & Ank & AP & \timecnn & \timeinfer\\
\midrule








    




\bufull, \labeling & 90.0  & 84.9  & 71.1  & 58.4  & 69.7  & 64.7 & 54.7 & 70.5 &   \textbf{0.18} & 3.06 \\ \bufull            & 91.2  & 86.0  & \textbf{72.9}  & 61.5  & 70.4  & 65.4 & 55.5 & 71.9 &   \textbf{0.18} & 0.38 \\ \busparse          & 91.1  & \textbf{86.5}  & 70.7  & 58.1  & 69.7  & 64.7 & 53.8 & 70.6 &   \textbf{0.18} & 0.22 \\ \midrule
     \tdbushort{} + \spatprop & \textbf{92.2}  & 86.1  & 72.8  & \textbf{63.0}  & \textbf{74.0}  &  \textbf{66.2} & \textbf{58.4} & \textbf{73.3} &  0.94{\color{red}\footnotemark[7]} & \textbf{0.08} \\ 



\bottomrule
  \end{tabular}
 \vspace{0.75em}
\caption[]{Effects of various variants of $\regression$ model on pose estimation performance (AP) on
  MPII Multi-Person Val and comparison to the best variant of \tdbushort model. }
\label{tab:mpii-single-frame-val:regression}
\end{table}
 We compare the performance of different variants of our \bulong~(\bushort) and
\tdbulong~(\tdbushort) models introduced in Sec.~\ref{sec:bu} and Sec.~\ref{sec:person-cond}.  For
\bushort~we consider a model that (1) uses a fully-connected graph with up to $1,000$ detection
proposals and jointly performs partitioning \textit{and} body-part labeling similar
to~\cite{insafutdinov16eccv} (\bufull, \labeling); (2) is same as (1), but labeling of detection
proposals is done based on detection score (\bufull); (3) is same as (2), but uses a sparsely-connected
graph (\busparse). The results are shown in
Tab.~\ref{tab:mpii-single-frame-val:regression}\footnote{\label{cnntime}Our current implementation
  of \tdbushort~operates on the whole image when computing person-conditioned proposals and computes
  the proposals sequentially for each person. More efficient implementation would only compute the
  proposals for a region surrounding the person and run multiple people in a single batch. Clearly
  in cases when two people are close in the image this would still process the same image region
  multiple times. However the image regions far from any person would be excluded from processing
  entirely. On average we expect similar image area to be processed during proposal generation stage
  in both \tdbushort~and \busparse,~and expect the runtimes \timecnn~to be comparable for both
  models.}.  \bufull, \labeling{} achieves $70.5$\% AP with a median inference run-time
\timeinfer~of $3.06$ s/f. \bufull{} achieves $8\times$ run-time reduction ($0.38$ vs. $3.06$ s/f):
pre-labeling detection candidates based on detection score significantly reduces the number of
variables in the problem graph. Interestingly, pre-labeling also improves the performance ($71.9$
vs. $70.5$\% AP): some of the low-scoring detections may complicate the search for an optimal
labeling. $\busparse$ further reduces run-time ($0.22$ vs. $0.38$ s/f), as
it reduces the complexity of the initial problem by sparsifying the graph, at a price of a drop in
performance ($70.6$ vs. $71.9$\% AP).


In Tab.~\ref{tab:mpii-single-frame-val:detection} we compare the variants of the \tdbushort~model.
Our $\tdshort$~approach achieves $71.7$\% AP, performing on par with a more complex $\bufull$. 
Explicit spatial propagation
(\tdshort{}+\spatprop ) further improves the results ($72.5$ vs. $71.7$\% AP). The largest
improvement is observed for ankles: progressive prediction that conditions on the close-by parts in
the tree hierarchy reduces the distance between the conditioning signal and the location of the
predicted body part and simplifies the prediction task. Performing inference
(\tdbushort{}+\spatprop) improves the performance to $73.3$\% AP, due to more optimal assignment of
part detection candidates to corresponding persons. Graph simplification in \tdbushort{}
 allows to further reduce the inference time for graph partitioning ($0.08$ vs. $0.22$ for
$\busparse$).


\tabcolsep 1.5pt
\begin{table}[tbp]
 \scriptsize
  \centering
  \begin{tabular}{@{} l c ccc ccc cc@{}}
    \toprule
    Setting& Head   & Sho  & Elb & Wri & Hip & Knee & Ank & AP \\
    \midrule
    \tdshort & 91.6  & 84.7  & \textbf{72.9}  & \textbf{63.2}  & 72.3  & 64.7 & 52.8 & 71.7 \\ \tdshort{} + \spatprop & 90.7  & 85.0  & 72.0  & 63.1  & 73.1  & 65.0 & 58.3 & 72.5 \\ \tdbushort{} + \spatprop & \textbf{92.2}  & \textbf{86.1}  & 72.8  & 63.0  & \textbf{74.0}  & \textbf{66.2} & \textbf{58.4} & \textbf{73.3} \\

\bottomrule
  \end{tabular}
 \vspace{0.75em}
\caption[]{Effects of various versions of $\detection$ model on pose estimation performance (AP) on MPII Multi-Person Val.}
\label{tab:mpii-single-frame-val:detection}
\end{table}
 
\tabcolsep 1.5pt
\begin{table}[tbp]
 \scriptsize
  \centering
  \begin{tabular}{@{} l c ccc ccc ccc@{}}
    \toprule
    Setting& Head   & Sho  & Elb & Wri & Hip & Knee & Ank  & AP & \timeinfer \\
    \midrule


    \bufull & \textbf{91.5}  & \textbf{87.8}  & 74.6  & 62.5  & 72.2  & 65.3 & 56.7 & 72.9 & 0.12\\ \tdbushort + \spatprop & 88.8  & 87.0  & \textbf{75.9}  & \textbf{64.9}  & \textbf{74.2}  & \textbf{68.8} & \textbf{60.5} & \textbf{74.3}  & \textbf{0.005}\\ 

    \midrule
    \deepercut~\cite{insafutdinov16eccv} & 79.1  & 72.2  & 59.7 & 50.0 & 56.0  & 51.0 & 44.6 & 59.4 & 485 \\ \deepercut~\cite{insafutdinov16arxiv} & 89.4  & 84.5  & 70.4  & 59.3  & 68.9  & 62.7 & 54.6 & 70.0 & 485\\
Iqbal\&Gall~\cite{Iqbal_ECCVw2016} & 58.4  & 53.9  & 44.5  & 35.0  & 42.2  & 36.7 & 31.1 & 43.1 & 10\\
    \bottomrule
  \end{tabular}
 \vspace{0.75em}

\caption[]{Pose estimation results (AP) on MPII Multi-Person Test.}
  \label{tab:mpii-single-frame:sota}
  \vspace{-1.7em}
\end{table}
 \begin{figure*}
  \centering
  \begin{tabular}{c c c c c}

  \begin{sideways}\bf \small~~~Single Frame \end{sideways}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0002_0005_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0002_0010_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0002_0016_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0002_0021_sticks}\\

  \begin{sideways}\bf \small~~~~~~Tracking \end{sideways}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0002_0005_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0002_0010_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0002_0016_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0002_0021_sticks}\\\vspace{0.1cm}
    ~& (a) & (b)  & (c) & (d) \\
  \begin{sideways}\bf \small~~~Single Frame \end{sideways}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0006_0001_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0006_0005_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0006_0015_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_static/sequence_0006_0017_sticks}\\

  \begin{sideways}\bf \small~~~~~~Tracking \end{sideways}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0006_0001_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0006_0005_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0006_0015_sticks}&
  	\includegraphics[height=0.129\linewidth]{figures_camera_ready/seq_video/sequence_0006_0017_sticks}\\\vspace{0.1cm}
    ~& (e) & (f)  & (g) & (h) \\



\end{tabular}
\caption{Qualitative comparison of results using single frame based
     model (\busparse{}) vs. articulated tracking (\busparsevid{}). See \url{http://youtube.com/watch?v=eYtn13fzGGo} for the supplemental material showcasing our results.}
\label{fig:qualitative_video}
\end{figure*}
 


\myparagraph{Comparison to the State of the Art.} We compare the
proposed single-frame approaches to the state of the art on MPII
Multi-Person Test and WAF~\cite{eichner10eccv} datasets. Comparison on
MPII is shown in Tab.~\ref{tab:mpii-single-frame:sota}. Both $\bufull$
and $\tdbushort$ improve over the best published result of
\deepercut{}~\cite{insafutdinov16arxiv}, achieving $72.9$ and $74.3$\%
AP respectively vs. $70.0$\% AP by \deepercut{}. For the $\tdbushort$
the improvements on articulated parts (elbows, wrists, ankles, knees)
are particularly pronounced. We argue that this is due to using the network
that is directly trained to disambiguate body parts of different
people, instead of using explicit geometric pairwise terms that only
serve as a proxy to person's identity. Overall, the performance of our
best $\tdbushort$ method is noticeably higher ($74.3$ vs. $70.0$\%
AP). Remarkably, its run-time \timeinfer~of graph partitioning stage is $5$ orders of magnitude
faster compared to $\deepercut$. This speed-up is due to two
factors. First, $\tdbushort$ relies on a faster solver
\cite{levinkov16arxiv} that tackles the graph-partitioning problem via
local search, in contrast to the exact solver used in
\cite{insafutdinov16eccv}. Second, in the case of $\tdbushort$ model
the graph is sparse and a large portion of the computation is
performed by the feed-forward CNN introduced in
\Sec.~\ref{sec:person-cond}. On WAF~\cite{eichner10eccv} dataset
\tdbushort~substantially improves over the best published result
($87.7$ vs. $82.0$\% AP by~\cite{insafutdinov16arxiv}). We refer to
supplemental material for details.





\subsection{Multi-frame models}

\myparagraph{Comparison of video-based models.} Performance of the
proposed video-based models is compared in
Tab.~\ref{tab:mpii-multi-video:overall}. Video-based models outperform
single-frame models in each case. $\bufullvid$ slightly outperforms
$\bufull$, where improvements are noticeable for ankle, knee and
head. $\busparsevid$ noticeably improves over $\busparse$ ($73.1$
vs. $71.6$\% AP). We observe significant improvements on the most
difficult parts such as ankles ($+3.9$\% AP) and wrists ($+2.6$\%
AP). Interestingly, $\busparsevid$ outperforms $\bufull+\temporal$:
longer-range connections such as, \eg, head to ankle, may introduce
additional confusion when information is propagated over
time. Finally, $\tdbuvid$ improves over $\tdbushort$ ($+0.7$\%
AP). Similarly to $\busparsevid$, improvement is most prominent on
ankles ($+1.8$\% AP) and wrists ($+0.9$\% AP). Note that even the
single-frame \tdbushort{} outperforms the best temporal \bushort{}
model. 
We show examples of articulated tracking on \videodata~in Fig.~\ref{fig:qualitative_video}. Temporal reasoning helps in cases
when image information is ambiguous due to close proximity of multiple
people. For example the video-based approach succeeds in correctly
localizing legs of the person in Fig.~\ref{fig:qualitative_video} (d)
and (h). 


\myparagraph{Temporal features.} We perform an ablative experiment on the \videodata~ dataset to
evaluate the individual contribution of the temporal features introduced in
Sec.~\ref{subsection:temporal}. The Euclidean distance alone achieves $72.1$ AP, adding DeepMatching
features improves the resuls to $72.5$ AP, whereas the combination of all features achieves the best result
of $73.1$ AP (details in supplemental material).



\tabcolsep 1.5pt
\begin{table}[tbp]
 \scriptsize
  \centering
  \begin{tabular}{@{} l c ccc ccc ccc@{}}
    \toprule
    Setting& Head   & Sho  & Elb & Wri & Hip & Knee & Ank & AP \\
    \midrule
    \bufull & 84.0  & 83.8  & 73.0  & 61.3  & 74.3  & 67.5 & 58.8 & 71.8 \\
    \quad + \temporal & 84.9  & 83.7  & 72.6  & 61.6  & 74.3  & 68.3 & 59.8 & 72.2  \\

    \midrule
    \busparse & 84.5  & 84.0  & 71.8  & 59.5  & \textbf{74.4}  & 68.1 & 59.2 & 71.6 \\
    \quad + \temporal & \textbf{85.6}  & 84.5  & 73.4  & 62.1  & 73.9  & 68.9 & 63.1 & 73.1 \\ 

    \midrule
    \tdbushort + \spatprop & 82.2  & 85.0  & 75.7  & 64.6  & 74.0  & 69.8 & 62.9 & 73.5 \\ \quad + \temporal      & 82.6  & \textbf{85.1}  & \textbf{76.3}  & \textbf{65.5}  & 74.1  & \textbf{70.7} & \textbf{64.7} & \textbf{74.2} \\ \bottomrule
  \end{tabular}
 \vspace{0.75em}
\caption[]{Pose estimation results (AP) on \videodata.} \label{tab:mpii-multi-video:overall}
\end{table}
 


\myparagraph{Tracking evaluation.} In Tab.~\ref{tab:mpii-multi-video:mota} we present results of the
evaluation of multi-person articulated body tracking. We treat each body joint of each person as a
tracking target and measure tracking performance using a standard multiple object tracking accuracy
(MOTA) metric \cite{Bernardin:2008:CLE} that incorporates identity switches, false positives and
false negatives\footnote{Note that MOTA metric does not take the confidence scores of detection or
  track hypotheses into account. To compensate for that in the experiment in
  Tab.~\ref{tab:mpii-multi-video:mota} we remove all body part detections with a score 
  $\leq 0.65$ for \busparse~and $\leq 0.7$ for \tdbushort~prior to evaluation.}. We experimentally compare to a baseline model that first tracks people
across frames and then performs per-frame pose estimation.
To track a person we use a reduced version of our algorithm that operates on the two head
joints only. This allows to achieve near perfect person tracking results in most cases. Our tracker still fails
when the person head is occluded for multiple frames as it does not incorporate long-range
connectivity between target hypothesis. We leave handling of long-term occlusions for the future
work.
For full-body tracking we use the same inital head tracks and add them to the set of body part
proposals, while also adding must-link and must-cut constraints for the temporal edges corresponding
to the head parts detections.
The rest of the graph remains unchanged so that at inference time the body parts can be freely
assigned to different person tracks. For the \busparse{}~the full body tracking improves performance
by $+5.9$ and $+5.8$ MOTA on wrists and ankles, and by $+5.0$ and $+2.4$ MOTA on elbows and knees
respectively. \tdbushort~benefits from adding temporal connections between body parts as well, but
to a lesser extent than \busparse{}. The most significant improvement is for ankles ($+1.4$
MOTA). \busparse{}~also achieves the best overall score of $58.5$ compared to $55.9$ by \tdbushort.
 This is surprising since \tdbushort~outperformed \busparse~on the pose
estimation task (see Tab.~\ref{tab:mpii-single-frame-val:regression} and
\ref{tab:mpii-single-frame:sota}). We hypothesize that limited improvement of \tdbushort~could be
due to balancing issues between the temporal and spatial pairwise terms that are estimated
independently of each other.
\tabcolsep 1.5pt
\begin{table}[tbp]
 \scriptsize
  \centering
  \begin{tabular}{@{} l c ccc ccc ccc@{}}
    \toprule
    Setting & Head   & Sho  & Elb & Wri & Hip & Knee & Ank & Average \\
    \midrule

Head track + \busparse  & 70.5  & 71.7  & 53.0  & 41.7  & 57.0 & 52.4 & 41.9 & 55.5 \\
  \quad \quad + \temporal & \textbf{70.6}  & \textbf{72.7}  & \textbf{58.0}  & \textbf{47.6}  & \textbf{57.6} & \textbf{54.8} & \textbf{47.7} & \textbf{58.5} \\




    \midrule

Head track + \tdbushort & 64.8  & 69.4  & 55.4  & 43.4  & 56.4 & 52.2 & 44.8 & 55.2 \\
   \quad \quad + \temporal & 65.0  & 69.9  & 56.3  & 44.2  & 56.7 & 53.2 & 46.1 & 55.9 \\



    \bottomrule
  \end{tabular}
 \vspace{0.75em}
\caption[]{Tracking results (MOTA) on the \videodata.} \label{tab:mpii-multi-video:mota}
  \vspace{-1.5em}
\end{table}
 




