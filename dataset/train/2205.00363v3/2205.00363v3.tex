\section{Experiments}\label{sec:exp}

In this section, we test VLMs on VSR. We first introduce baselines and experimental configurations in \Cref{sec:baseline}, then experimental results and analysis in \Cref{sec:main_results}. Then we discuss the role of frame of reference using experiments in \Cref{sec:exp_rf} and finally conduct sample efficiency analysis in \Cref{sec:sample_efficiency}.

\subsection{Baselines and Experiment Configurations}\label{sec:baseline}

\paragraph{Baselines.} For finetuning-based experiments, we test three popular VLMs: VisualBERT \citep{li2019visualbert}\footnote{\href{https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre}{huggingface.co/uclanlp/visualbert-nlvr2-coco-pre}}, 
LXMERT \citep{tan2019lxmert}\footnote{\href{https://huggingface.co/unc-nlp/lxmert-base-uncased}{huggingface.co/unc-nlp/lxmert-base-uncased}},
ViLT \citep{kim2021vilt}\footnote{\href{https://huggingface.co/dandelin/vilt-b32-mlm}{huggingface.co/dandelin/vilt-b32-mlm}}. All three models are stacked Transformers \citep{vaswani2017attention} that take image-text pairs as input. The difference mainly lies in how or whether they encode the position information of objects. We report only finetuned results but not direct inferences from off-the-shelf checkpoints since some of their pretraining objectives are inconsistent with the binary classification task of VSR, thus requiring additional engineering.

We additionally test the alt text pretrained dual-encoder CLIP \citep{radford2021learning} as an off-the-shelf baseline model (no finetuning).\footnote{\href{https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K}{huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K}} We follow \citet{clip_vsr} to construct negation or antonym of each individual relation. E.g., ``facing'' $\rightarrow$ ``facing away from'' and ``ahead of'' $\rightarrow$ ``not ahead of''. For each sample, we compare the embedding similarity of the image-caption pair and that of the negated caption. If the original pair has a higher probability then the model prediction is \texttt{True}, otherwise \texttt{False}. We call this method CLIP (w/ prompting). We only report direct prompting results without finetuning since CLIP finetuning is expensive.









\paragraph{Experimental configurations.} 
We save checkpoints every 100 iterations and use the best-performing checkpoint on dev set for testing. All models are run three times using three random seeds. All models are trained with AdamW optimiser \citep{loshchilov2018decoupled}. The hyperparameters we used for training the three VLMs
are listed in \Cref{tab:hparams}. 

\begin{table}[!ht] \small
\centering
\setlength{\tabcolsep}{2.3pt}
\begin{tabular}{lccccccc}
\toprule
model & lr &  batch size & epoch & token length \\
\midrule
VisualBERT & 2e-6 & 32 & 100 & 32 \\
LXMERT & 1e-5 & 32 & 100 & 32 \\
ViLT & 1e-5 & 12 & 30 & max \\
\bottomrule
\end{tabular}
\caption{A listing of hyperparameters used for all VLMs (``lr'': learning rate).
}
\label{tab:hparams}
\end{table}



\subsection{Experimental Results}\label{sec:main_results}


\begin{figure*}[th!]
    \centering
\begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/performance_by_relation_random_split_v4.png}
\caption{random split}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/performance_by_relation_zeroshot_split_feb_20_v1.png}
\caption{zero-shot split}
\end{subfigure}
\caption{Performance (accuracy) by relation on the random (upper) and zero-shot (lower) split test sets. Relation order sorted by frequency (high to low from left to right). Only relations with more than 15 and 5 occurrences on the random and zero-shot tests respectively are shown. }
\label{fig:performance_by_rel}
\end{figure*}

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
model$\downarrow$ & random split & zero-shot split \\
\midrule
human ceiling & \multicolumn{2}{c}{95.4}   \\
\midrule
CLIP {\scriptsize(w/ prompting)} & 56.0 & 54.5 \\
\midrule
VisualBERT &  55.2$_{\pm1.4}$ & 51.0$_{\pm1.9}$  \\ 
ViLT & \textbf{69.3}$_{\pm0.9}$ &  \textbf{63.0}$_{\pm0.9}$  \\ 
LXMERT & \textbf{70.1}$_{\pm0.9}$ & 61.2$_{\pm0.4}$  \\ 
\bottomrule
\end{tabular}
\caption{Model performance on VSR test set. CLIP is applied without finetuning but with carefully engineered prompts while the other three smaller models are finetuned on the training set.}
\label{table:results}
\end{table}

In this section, we provide both quantitative and qualitative results of the four baselines. Through analysing the failure cases of the models, we also highlight the key abilities needed to solve this dataset.

As shown in \Cref{table:results}, the best-performing models on the random split are LXMERT and ViLT, reaching around 70\% accuracy while VisualBERT is just slightly better than the chance level. On the zero-shot split, all models' performance decline substantially and the best model ViLT only obtains 63.0\% accuracy.
The off-of-the-shelf CLIP model obtains around 55\% on both sets, indicating its weaknesses in spatial reasoning echoing \citet{subramanian2022reclip}'s findings.
Overall, these results lag behind the human ceiling by more than 25\% and highlight that there is very substantial room for improving current VLMs. 

\paragraph{Explicit positional information matters.} Both LXMERT and ViLT outperform VisualBERT by large margins ($>$10\%) on both splits.
This is expected since LXMERT and ViLT encode explicit positional information while VisualBERT does not. LXMERT has position features as part of the input which encodes the relative coordinates of objects within the image. ViLT slices an image into patches (instead of object regions) and uses positional encodings to signal the patches' relative positions.
VisualBERT, however, has no explicit position encoding. \citet{bugliarello-etal-2021-multimodal,positional2022} also highlight the importance of positional encodings of VLMs, which agrees with our observations.

\paragraph{Random split vs. zero-shot split.} 
It is worth noting that the performance gap between the random and zero-shot splits is large.
As we will show in~\cref{sec:sample_efficiency}, the underlying cause is not likely to be the number of training examples, but rather that concept zero-shot learning is fundamentally a challenging task.
The gap suggests that disentangling representations of concepts and relations is challenging for current models.

\paragraph{Sensitiveness to random seeds.}
Model performance varies by about one to two percentage points.
These fluctuations illustrate the importance of
always reporting the average performance of multiple runs to make sure the conclusion is reliable.



\begin{figure}[th!]
  \centering
        \centering
    \begin{subfigure}[b]{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/000000134738.jpg}
    \caption{Caption: \textit{The hair drier is facing away from the person}. Label: \texttt{False}.}
     \end{subfigure}\hfill
     \begin{subfigure}[b]{0.8\linewidth}
         \centering
    \includegraphics[width=\linewidth]{images/000000434410.jpg}
         \caption{Caption: \textit{The bench is in front of the person}. Label: \texttt{True}.}
     \end{subfigure}
    
    \caption{LXMERT failed on both examples.}
    \label{fig:hair_drier_facing}
\end{figure}


\paragraph{Performance by relation.} We give performance by relation for all three finetuned models on both random and zero-shot splits in \Cref{fig:performance_by_rel}. The order from left to right is sorted by the frequency of relations in the dataset (within each split). Interestingly, there does not seem to be any correlation between performance and frequency of the relation, hinting that specific relations are hard not due to an insufficient number of training examples but because they are fundamentally challenging for current VLMs. Any relation that requires recognising orientations of objects seems to be hard, e.g., ``facing'', ``facing away from'', ``parallel to'' and ``at the back of''. As an example, LXMERT failed on the two examples in \Cref{fig:hair_drier_facing} which require understanding the front of a hair drier and a person respectively.
In this regard, left-right relations such as ``at the left/right side of'' and ``left/right of'' are difficult because the intrinsic reference frame requires understanding the orientation of objects. As an example, in \Cref{fig:potted_plant_bench}, all three models predicted \texttt{False}, but in the intrinsic frame (i.e., from the bench's point of view), the potted plant is indeed at the right.




\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=0.96\linewidth]{images/performance_by_meta_cat_random_split_v4.png}
    \caption{random split}
     \end{subfigure}
     \begin{subfigure}[b]{\linewidth}
         \centering
    \includegraphics[width=0.96\linewidth]{images/performance_by_meta_cat_zeroshot_split_feb_20_v1.png}
    \caption{zero-shot split}
     \end{subfigure}
\caption{Performance by categories of relations, on the random and zero-shot split test sets. For legend information, see \Cref{fig:performance_by_rel}.}
\label{fig:performance_by_meta_cat}
\end{figure}

To get a more high-level understanding of the relations' performance, we group model performance by the categories of \citet{marchi2021cross}: ``Adjacency'', ``Directional'', ``Orientation'', ``Projective'', ``Proximity'', ``Topological'' and ``Unallocated'' (also shown in \Cref{tab:spatial_relations}). The results are shown in \Cref{fig:performance_by_meta_cat}. ``Orientation'' is the worst performing group on the random split, and on average all models' performances are close to the chance level. When comparing random and zero-shot splits, performance has declined to some extent for almost all categories and models.
The decrease in ``Proximity'' is particularly drastic across all models -- it declined from close to 75\% accuracy in random split to chance level in zero-shot split. ``Proximity'' contains relations such as ``close to'', ``near'' and ``far from''. We believe it is due to the fact that the notion of proximity is relative and very much dependent on the nature of the concept and its frequent physical context. E.g., for a ``person'' to be ``near'' an indoor concept such as ``oven'' is very different from a person being ``near'' a frequent outdoor object such as ``train'' or ``truck''. Since the zero-shot split prevents models from seeing test concepts during training, the models have a poor grasp of what counts as ``close to'' or ``far from'' for these concepts, thus generalising poorly.





\paragraph{Other Errors.} While certain relations are intrinsically hard, we have observed other types of errors that are not bounded to specific relations. Here we give a few examples. Some instances require complex reasoning. In \Cref{fig:cow_back_car}, the model needs to recognise that both the cow and the back of the car are in the car's side mirror and also infer the relative position of the back of the car and the cow. It is perhaps no surprise that two of the three models predicted wrongly. Some other examples require common sense. E.g., in \Cref{fig:person_cow}, we can infer the person and the cow's moving direction and can then judge if the cow is ahead of the person. 
LXMERT failed on this example. 
In \Cref{fig:annotation_instance} (right), the model needs to infer that the main body of the cat is hidden behind the laptop. Interestingly, all three models predicted this example correctly.


\begin{figure}[!tbp]
  \centering
    \includegraphics[width=0.8\linewidth]{images/000000512796.jpg}
    \caption{Caption: \textit{The cow is at the back of the car}. Label: \texttt{True}. LXMERT and VisualBERT predicted \texttt{False}.}
    \label{fig:cow_back_car}
\end{figure}





\begin{figure*}[!thp]
    \centering
    \includegraphics[width=\linewidth]{images/vsr_sample_efficiency_feb_20.pdf}
\caption{Sample efficiency analysis: model performance under different amounts of training data (100-shot, 25\%, 50\%, 75\% and 100\% of training set). Results on both the random and zero-shot split test sets are shown. As training data increases, the performance plateaus on both sets but the flattening trend is more obvious on the zero-shot split.}
    \label{fig:sample_efficiency}
\end{figure*}

\subsection{Case Study on Reference Frames}\label{sec:exp_rf}

As discussed in \Cref{sec:relative_vs_intrinsic}, different frames of reference can be used in natural language and it would be helpful to understand whether our models recognise them.
We argue that the task of identifying frame of reference itself is very hard for current models. However, learning to recognise frames of reference helps the task of visual spatial reasoning.

Firstly, we conduct a case study on left/right-related relations. We additionally label the reference frames of all true statements containing any of the left/right-related relations. We exclude all data points that can be interpreted in both intrinsic and relative frames to slightly reduce the complexity of the task.
Then we finetune a ViLT checkpoint to predict the reference frame based on the true statement and the image. The model's performance on test set is shown in the upper half of \Cref{table:rf}. We can see that reference frame prediction is an extremely hard task for the model. This is presumably because it requires taking into account a 3D viewpoint and simulating transformations between different viewpoints.

Secondly, we use this model trained with reference frame labels to initialise the VSR task model and further finetune it on the VSR task (only the left/right relations). The test results are shown in the lower part of \Cref{table:rf}.\footnote{Note that the reference frame train/dev/test sets are derived from the VSR task split -- so no data leakage is possible from train to dev and test sets even after the intermediate pretraining.} We see a clear positive transfer from reference frame prediction task to the VSR task.
This suggests that learning to recognise reference frames can indeed help downstream visual spatial reasoning. This makes sense since simulating the transformation of intrinsic/relative frames could be an intermediate reasoning step in detecting whether a statement is true/false.


\begin{table}[]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\multicolumn{4}{c}{\emph{Reference frame prediction task}} \\
\midrule
model$\downarrow$ & Precision & Recall & F1  \\
\midrule
ViLT & 59.2$_{\pm3.7}$ & 59.7$_{\pm5.8}$ & 56.9$_{\pm4.4}$ \\
\toprule
\multicolumn{4}{c}{\emph{VSR task (left/right subset)}} \\
\midrule
model$\downarrow$ &  & \multicolumn{2}{c}{Accuracy} \\
\midrule
\multicolumn{2}{l}{ViLT} & \multicolumn{2}{c}{54.2$_{\pm0.6}$} \\
\multicolumn{2}{l}{ViLT + rf\_trained} &  \multicolumn{2}{c}{\textbf{59.2}$_{\pm1.8}$} \\
\bottomrule
\end{tabular}
\caption{ViLT model performance on the reference frame prediction task (upper half; we report macro-averaged Precision/Recall/F1 since the binary classification task is imbalanced); and VSR task using original pretrained checkpoint or the reference frame prediction task trained checkpoint (accuracy reported).}
\label{table:rf}
\end{table}


\subsection{Sample Efficiency}\label{sec:sample_efficiency}
In order to understand the correlation between model performance and the number of training examples, we conduct sample efficiency analysis on VSR. The results are plotted in \Cref{fig:sample_efficiency}. For the minimum resource scenario, we randomly sample 100 shots from the training sets of each split. Then we gradually increase the number of training examples to be 25\%, 50\% and 75\% of the whole training sets. 
Both LXMERT and ViLT have a reasonably good few-shot capability and can be quite performant with 25\% of training data. LXMERT, in particular, reaches above 55\% accuracy with 100 shots on both splits.
The zero-shot split is substantially harder and most models appear to have already plateaued at around 75\% of the training set. For the random split, all models are increasing performance with more data points, though improvement slows down substantially for LXMERT and ViLT after 75\% of training data. The fact that LXMERT has the best overall few-shot capability may be suggesting that LXMERT's pretrained object detector has a strong inductive bias for the VSR dataset as it does not need to learn to recognise concept boundaries and classes from scratch. However, this advantage from LXMERT seems to fade away as the number of training examples increases. 




