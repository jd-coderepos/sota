\section{Experiments}

Evaluation is performed on three popular RGB-D datasets: 
\begin{itemize}
\item NYUv2~\cite{nyuv2}: NYUv2 contains of  RGB-D images with pixel-wise labels. We follow the -class settings and the standard split with  training images and  testing images. 
\item SUN-RGBD~\cite{sunrgbd,Janoch2011dataset}: This dataset have  categories of objects and consists of  RGB-D images, with  as training and  as testing. 
\item Stanford Indoor Dataset (SID)~\cite{2017arXiv170201105A}: SID contains  RGB-D images with  object categories. We use Area  and  as training, and Area  as testing.
\end{itemize}

Four common metrics are used for evaluation: pixel accuracy (Acc), mean pixel accuracy of different categories (mAcc), mean Intersection-over-Union of different categories (mIoU), and frequency-weighted IoU (fwIoU). Suppose  is the number of pixels with ground truth class  and predicted as class ,  is the number of classes and  is the number of pixels with ground truth class , the total number of all pixels is . The four metrics are defined as follows: Acc = , mAcc = , mIoU = , fwIoU = .


\paragraph{\bfseries{Implementation Details}}
For most experiments, DeepLab with a modified VGG-16 encoder (c.f. Table~\ref{table:netarch}) is the baseline. Depth-aware CNN based on DeepLab outlined in Table~\ref{table:netarch} is evaluated to validate the effectiveness of our approach and this is referred as ``D-CNN" in the paper. We also conduct experiments with combining HHA encoding~\cite{eccv14hha}. Following \cite{fcn,xiaojuaniccv17,eigeniccv15}, two baseline networks consume RGB and HHA images separately and the predictions of both networks are summed up in the last layer. This two-stream network is dubbed as ``HHA". To make fair comparison, we also build depth-aware CNN with this two-stream fashion and denote this as ``D-CNN+HHA".
In ablation study, we further replace VGG-16 with ResNet-50~\cite{resnet} as the encoder to have a better understanding of the functionality of depth-aware operations.

We use SGD optimizer with initial learning rate , momentum  and batch size . The learning rate is multiplied by  for every  iterarions.  is set to . (The impact of  is studied in Section~\ref{sec:ablation}.) The dataset is augmented by randomly scaling, cropping, and color jittering. We use PyTorch deep learning framework. Both depth-aware convolution and depth-aware average pooling operators are implemented with CUDA acceleration. Code will be released.
\vspace{-10pt}

\subsection{Main Results}
\label{sec:mainresults}
Depth-aware CNN is compared with both its baseline and the state-of-the-art methods on NYUv2 and SUN-RGBD dataset. It is also compared with the baseline on SID dataset.
\paragraph{\bfseries{NYUv2}}
Table~\ref{table:nyud2scratch} shows quantitative comparison results between D-CNNs and baseline models. Since D-CNN and its baseline are in different function space, all networks are trained from scratch to make fair comparison in this experiment. Without introducing any parameters, D-CNN outperforms the baseline by incorporating geometric information in convolution operation. Moreover, the performance of D-CNN also exceeds ``HHA" network by using only half of its parameters. This effectively validate the superior capability of D-CNN on handling geometry over ``HHA".  
\vspace{-10pt}
\begin{table}
\begin{center}
\newcolumntype{C}{>{\centering\arraybackslash}p{3.5em}}
\newcolumntype{E}{>{\centering\arraybackslash}p{7em}}
\begin{tabular}{c|ECCE}
	\Xhline{3\arrayrulewidth}
& Baseline& HHA & D-CNN & \small{D-CNN+HHA}\\
\hline
Acc (\%)&50.1&59.1&60.3 &\bf{61.4}\\
mAcc (\%)&23.9&30.8&\bf{39.3} &35.6\\
mIoU (\%)&15.9&21.9&\bf{27.8} & 26.2\\
fwIoU (\%)&34.2&43.0&44.9 &\bf{45.7}\\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Comparison with baseline CNNs on NYUv2 test set. Networks are trained from scratch.}\vspace{-40pt}
\label{table:nyud2scratch}
\end{table}

\begin{table}
	\begin{center}
		\newcolumntype{C}{>{\centering\arraybackslash}p{3.5em}}
		\newcolumntype{D}{>{\centering\arraybackslash}p{2.6em}}
		\newcolumntype{E}{>{\centering\arraybackslash}p{7em}}
		\begin{tabular}{c|DDDD|CCE}
			\Xhline{3\arrayrulewidth}
			&\cite{fcn}&\cite{eigeniccv15}&\cite{yang_cvpr17}&\cite{xiaojuaniccv17}& \small{HHA} & D-CNN & \small{D-CNN+HHA}\\
			\hline
			mAcc (\%)& 46.1 & 45.1&53.8 & 55.2 &51.1&53.6&\bf{56.3}\\
			mIoU (\%)& 34.0 & 34.1&40.1 & 42.0 &40.4 &41.0&\bf{43.9}\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{center}
	\caption{Comparison with the state-of-the-arts on NYUv2 test set. Networks are trained from pre-trained models.}\vspace{-20pt}
	\label{table:nyud2}
\end{table}
We also compare our results with the state-of-the-art methods. Table~\ref{table:nyud2} illustrates the good performance of D-CNN. In this experiment, the networks are initialized with the pre-trained parameters in \cite{deeplab}. Long et al.~\cite{fcn} and Eigen et al.~\cite{eigeniccv15} both use the two-stream network with HHA/depth encoding. Yang et al.~\cite{yang_cvpr17} compute optical flows and superpixels to augment the performance with spatial-temporal information. D-CNN with only one VGG network is superior to their methods. Qi et al.~\cite{xiaojuaniccv17} built a 3D graph on the top of VGG encoder and use RNN to update the graph, which introduces more network parameters and higher computation complexity. As is shown in Table~\ref{table:nyud2}, D-CNN is already comparable with these state-of-the-art methods. By incorporating HHA encoding, our method achieves the state-of-the-art on this dataset. Figure~\ref{fig:nyud2} visualizes qualitative comparison results on NYUv2 test set. 


\begin{figure}
	\centering
\newcolumntype{C}{>{\centering\arraybackslash}p{5em}}
	\begin{tabular}{CCCCCCC}
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/000_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/001_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/004_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/005_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/009_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/015_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/016_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/nyuv2/020_dcnnhha.jpg}
		\\
		\tiny{RGB}& \tiny{Depth}& \tiny{GT}& \tiny{Baseline} & \tiny{HHA }&\tiny{D-CNN}& \tiny{DCNN+HHA}
	\end{tabular}
	\caption{Segmentation results on NYUv2 test dataset. ``GT" denotes ground truth. The white regions in ``GT" are the ignoring category. Networks are trained from pre-trained models.}\vspace{-20pt}
	\label{fig:nyud2}
\end{figure}

\paragraph{\bfseries{SUN-RGBD}}
The comparison results between D-CNN and its baseline are listed in Table~\ref{table:sunrgbdscratch}. The networks in this table are trained from scratch. D-CNN outperforms baseline by a large margin. Substituting the baseline with the two-stream ``HHA" network is able to further improve the performance.

By comparing with the state-of-the-art methods in Table~\ref{table:sunrgbd}, we can further see the effectiveness of D-CNN. Similarly as in NYUv2, the networks are initialized with pre-trained model in this experiment.  Figure~\ref{fig:sunrgbd} illustrates the qualitative comparison results on SUN-RGBD test set. Our network achieves comparable performance with the state-of-the-art method~\cite{xiaojuaniccv17}, while their method is more time-consuming. We will further compare the runtime and numbers of model parameters in Section~\ref{sec:time}. 
\vspace{-10pt}
\begin{table}
\begin{center}
\newcolumntype{C}{>{\centering\arraybackslash}p{3.5em}}
\newcolumntype{E}{>{\centering\arraybackslash}p{7em}}
\begin{tabular}{c|ECCE}
	\Xhline{3\arrayrulewidth}
& Baseline& HHA & D-CNN & \small{D-CNN+HHA}\\
\hline
Acc (\%)& 66.6&72.6&72.4&\bf{72.9}\\
mAcc (\%)& 31.5&37.9&38.6&\bf{41.2}\\
mIoU (\%)& 22.8&28.8&29.7&\bf{31.3}\\
fwIoU (\%)& 51.4&58.5&58.2&\bf{59.3}\\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Comparison with baseline CNNs on SUN-RGBD test set. Networks are trained from scratch.}\vspace{-10pt}
\label{table:sunrgbdscratch}
\end{table}

\begin{table}
\begin{center}
\newcolumntype{C}{>{\centering\arraybackslash}p{3.5em}}
\newcolumntype{D}{>{\centering\arraybackslash}p{2.6em}}
\newcolumntype{E}{>{\centering\arraybackslash}p{7em}}
\begin{tabular}{c|DD|CCE}
	\Xhline{3\arrayrulewidth}
&\cite{lstmcf}&\cite{xiaojuaniccv17}& HHA & D-CNN & D-CNN+HHA\\
\hline
mAcc (\%)& 48.1 & \bf{55.2}  & 50.5&51.2& 53.5\\
mIoU (\%)& - & \bf{42.0}  & 40.2&41.5& \bf{42.0}\\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Comparison with the state-of-the-arts on SUN-RGBD test set. Networks are trained from pre-trained models.}\vspace{-25pt}
\label{table:sunrgbd}
\end{table}

\begin{figure}[tb]
	\centering
\newcolumntype{C}{>{\centering\arraybackslash}p{5em}}
	\begin{tabular}{CCCCCCC}
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/000_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/001_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/004_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/003_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/005_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/007_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/010_dcnnhha.jpg}
		\\
		\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_image.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_depth.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_seggt.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_baseline.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_hha.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_dcnn.jpg}
		&\includegraphics[width=0.13\textwidth]{figures/sunrgbd/008_dcnnhha.jpg}
		\\
		\tiny{RGB }& \tiny{Depth }& \tiny{GT }& \tiny{Baseline} & \tiny{HHA }&\tiny{D-CNN}& \tiny{DCNN+HHA}
	\end{tabular}
	\caption{Segmentation results on SUN-RGBD test dataset. ``GT" denotes ground truth. The white regions in ``GT" are the ignoring category. Networks are trained from pre-trained models.}
	\label{fig:sunrgbd}\vspace{-10pt}
\end{figure}
\paragraph{\bfseries{SID}} The comparison results on SID between D-CNN with its baseline are reported in Table~\ref{table:SID}. Networks are trained from scratch. Using depth images, D-CNN is able to achieve  IoU over CNN while preserving the same number of parameters and computation complexity.
\vspace{-10pt}
\begin{table}
	\begin{center}
		\newcolumntype{C}{>{\centering\arraybackslash}p{3.5em}}
		\newcolumntype{E}{>{\centering\arraybackslash}p{7em}}
		\begin{tabular}{c|cc}
			\Xhline{3\arrayrulewidth}
			& Baseline&  D-CNN\\
			\hline
			Acc (\%)& 64.3&\bf{65.4}\\
			mAcc (\%)& 46.7&\bf{55.5}\\
			mIoU (\%)& 35.5&\bf{39.5}\\
			fwIoU (\%)& 48.5&\bf{49.9}\\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{center}
	\caption{Comparison with baseline CNNs on SID Area . Networks are trained from scratch.}\vspace{-40pt}
	\label{table:SID}
\end{table}

\subsection{Ablation Study}
\label{sec:ablation}
In this section, we conduct ablation studies on NYUv2 dataset to validate efficiency and efficacy of our approach. Testing results on NYUv2 test set are reported.

\paragraph{\bfseries{Depth-aware CNN}}
To verify the functionality of both depth-aware convolution and depth-aware average pooling, the following experiments are conducted.
\begin{itemize}
\item VGG-1: \texttt{Conv1\_1}, \texttt{Conv2\_1}, \texttt{Conv3\_1}, \texttt{Conv4\_1}, \texttt{Conv5\_1} and \texttt{Conv6} in VGG-16 are replaced with depth-aware convolution. This is the same as in Table~\ref{table:netarch}.
\item VGG-2: \texttt{Conv4\_1}, \texttt{Conv5\_1} and \texttt{Conv6} in VGG-16 are replaced with depth-aware convolution. Other layers remain the same as in Table~\ref{table:netarch}.
\item VGG-3: The depth-aware average pooling layer listed in Table~\ref{table:netarch} is replaced with regular pooling. Other layers remain the same as in Table~\ref{table:netarch}.
\end{itemize}
Results are shown in Table~\ref{table:depthconv}. Compared to VGG-2, VGG-1 adds depth-aware convolution in bottom layers. This helps the network propagate more fine-grained features with geometric relationships and increase segmentation performance by  in IoU. The depth-aware average pooling operation is able to further promote the accuracy.   

\begin{table}
	\begin{center}
		\newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
		\newcolumntype{E}{>{\centering\arraybackslash}p{5em}}
		\begin{tabular}{c|cc|ccc}
			\Xhline{3\arrayrulewidth}
			& Baseline& HHA& VGG-1    & VGG-2 & VGG-3  \\
			\hline
			Acc (\%)&50.1  &59.1&\bf{60.3} & 56.0  & 59.3  \\
			mAcc (\%)&23.9 &30.8&\bf{39.3} & 32.2  & 39.2    \\
			mIoU (\%)&15.9 &21.9&\bf{27.8} & 22.4  & 27.4     \\
			fwIoU (\%)&34.2&43.0&\bf{44.9} & 40.2  & 44.0     \\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{center}
	\caption{Results of using depth-aware operations in different layers. Experiments are conducted on NYUv2 test set. Networks are trained from scratch.}
	\vspace{-10pt}
	\label{table:depthconv}
\end{table}

We also replace VGG-16 to ResNet-50 as the encoder. To build depth-aware ResNet, the \texttt{Conv3\_1}, \texttt{Conv4\_1}, and \texttt{Conv5\_1} in ResNet-50 are replaced with depth-aware convolution. The networks are initialized with parameters pre-trained on ADE20K~\cite{zhou2017scene}. Detailed architecture and training details for ResNet can be found in Supplementary Materials. Results are listed in Table~\ref{table:resnet}. 

\begin{table}
	\begin{center}
		\newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
		\newcolumntype{E}{>{\centering\arraybackslash}p{5em}}
		\begin{tabular}{c|ccc}
			\Xhline{3\arrayrulewidth}
			& ResNet& ResNet-D-CNN& VGG-D-CNN  \\
			\hline
			Acc (\%)& 68.9 &\bf{69.6} &69.4 \\
			mAcc (\%)&50.2 &53.3 &\bf{53.6} \\
			mIoU (\%)&38.8 &\bf{41.5} &41.0 \\
			fwIoU (\%)&54.4 &54.4 &\bf{54.5} \\
			\Xhline{3\arrayrulewidth}
		\end{tabular}
	\end{center}
	\caption{Results of using depth-aware operations in ResNet-50. ``VGG-D-CNN" denotes the same network and result as in Table~\ref{table:nyud2}. Networks are trained from pre-trained models.}
		\vspace{-20pt}
	\label{table:resnet}
\end{table}

\paragraph{\bfseries{Depth Similarity Function}} We modify  and  to further validate the effect of different choices of depth similarity function on performance. We conduct the following experiments:

\begin{itemize}
	\item :  is set to . The network architecture is the same as Table~\ref{table:netarch}. 
	\item :  is set to . The network architecture is the same as Table~\ref{table:netarch}. 
	\item :  is set to . The network architecture is the same as Table~\ref{table:netarch}. 
	\item clip: The network architecture is the same as Table~\ref{table:netarch}.  is defined as 
	
\end{itemize}

Table~\ref{table:depthsim} reports the test performances with different depth similarity functions. Though the performance varies with different , they are all superior to baseline and even ``HHA". The result of clip is also comparable with ``HHA". This validate that the effectiveness of using a depth-sensitive term to weight the contributions of neurons. \vspace{-10pt}
\begin{table}
\begin{center}
\newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
\newcolumntype{E}{>{\centering\arraybackslash}p{5em}}
\begin{tabular}{c|cc|CCCC}
	\Xhline{3\arrayrulewidth}
 & Baseline& HHA&&  &  & clip\\
\hline
Acc (\%)&50.1  &59.1&\bf{60.3} &  58.5 &  58.5 &  53.0  \\
mAcc (\%)&23.9 &30.8&\bf{39.3} &  35.2 &  35.9 &  29.8   \\
mIoU (\%)&15.9 &21.9&\bf{27.8} &  24.9 &  25.3 &  20.1   \\
fwIoU (\%)&34.2&43.0&\bf{44.9} &  42.6 &  42.9 &  37.5   \\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Results of using different  and . Experiments are conducted on NYUv2 test set. Networks are trained from scratch.}
	\vspace{-40pt}
\label{table:depthsim}
\end{table}

\paragraph{\bfseries{Performance Analysis}} To have a better understanding of how depth-aware CNN outperforms the baseline, we visualize the improvement of IoU for each semantic class in Figure~\ref{fig:breakdown}(a). The statics shows that D-CNN outperform baseline on most object categories, especially these large objects such as ceilings and curtain. Moreover, we observe depth-aware CNN has a faster convergence than baseline, especially trained from scratch. Figure~\ref{fig:breakdown}(b) shows the training loss evolution with respect to training steps. Our network gains lower loss values than baseline.

\begin{figure}
	\centering
	\begin{tabular}{cc}
	\includegraphics[width=.65\textwidth]{figures/iouimprovement.png}
	&
	\includegraphics[width=.3\textwidth]{figures/trainingloss.png}\\
(a)&(b)
	\end{tabular}
	\vspace{-10pt}
	\caption{Performance Analysis. (a) Per-class IoU improvement of D-CNN over baseline on NYUv2 test dataset. (b) Evolution of training loss on NYUv2 train dataset. Networks are trained from scratch.}
	\vspace{-15pt}
	\label{fig:breakdown}
\end{figure}



\subsection{Model Complexity and Runtime Analysis}
\label{sec:time}
Table~\ref{table:time} reports the model complexity and runtime of D-CNN and the state-of-the-art method~\cite{xiaojuaniccv17}. In their method, kNN takes  runtime at least, where N is the number of pixels. We leverage the grid structure of raw depth input. Without increasing any model parameters, D-CNN is able to incorporate geometric information in CNN efficiently.
\vspace{-10pt}

\begin{table}
\begin{center}
\newcolumntype{C}{>{\centering\arraybackslash}p{3.5em}}
\newcolumntype{E}{>{\centering\arraybackslash}p{5em}}
\begin{tabular}{c|EC|CCCC}
	\Xhline{3\arrayrulewidth}
& Baseline & HHA & \cite{xiaojuaniccv17} & D-CNN \\
\hline
net. forward (ms)& 32.5 & 64.2 & 214 & 39.3\\
\# of params & 47.0M & 92.0M & 47.25M & 47.0M\\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Model complexity and runtime comparison. Runtime is tested on Nvidia 1080Ti, with input image size . }
	\vspace{-40pt}
\label{table:time}
\end{table}