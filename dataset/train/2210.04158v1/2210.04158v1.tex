\documentclass[journal]{IEEEtran}
\usepackage{amssymb}
\usepackage[figuresright]{rotating}
\usepackage[tableposition=top]{caption}
\usepackage{amsmath,subfigure,epsfig,bbding}
\usepackage{epstopdf}
\usepackage{multicol}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{color}
\usepackage[mathscr]{eucal}
\newcommand{\red}[1]{\textcolor{red}{{#1}}}
\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}
\DeclareCaptionFont{blue}{\color{blue}}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage[colorlinks=true,
            linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=blue]{hyperref}

\newcommand{\DOI}[1]{doi: \href{https://doi.org/#1}{#1}}
\def\hlinew#1{\noalign{\ifnum0=`}\fi\hrule \@height #1 \futurelet
   \reserved@a\@xhline}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}


\def\UrlAlphabet{\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X \do\Y\do\Z}
\def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0}

\begin{document}

\title{HVS Revisited: A Comprehensive Video Quality Assessment Framework}

\author{Ao-Xiang~Zhang,~\IEEEmembership{Student Member,~IEEE,} Yuan-Gen~Wang,~\IEEEmembership{Senior Member,~IEEE}\\
Weixuan Tang,~\IEEEmembership{Member,~IEEE},
Leida Li,~\IEEEmembership{Member,~IEEE}, Sam Kwong,~\IEEEmembership{Fellow,~IEEE}

\thanks{


A.-X. Zhang and Y.-G. Wang are with the School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou 510006, China (e-mail: zax@e.gzhu.edu.cn; wangyg@gzhu.edu.cn).

W. Tang is with the Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou 510006, China (email: tweix@gzhu.edu.cn).

L. Li is with the School of Artificial Intelligence, Xidian University, Xian 710071, China (e-mail: ldli@xidian.edu.cn).

S. Kwong is with the Department of Computer Science, City University of Hong Kong, Hong Kong (e-mail: cssamk@cityu.edu.hk).
}}

\maketitle

\begin{abstract}
Video quality is a primary concern for video service providers.
In recent years, the techniques of video quality assessment (VQA) based on deep convolutional neural network (CNN) have been developed rapidly.
Although existing works attempt to introduce the knowledge of the human visual system (HVS) into VQA, there still exhibit limitations that prevent full exploitation of HVS, including an incomplete model by few characteristics and insufficient connections among these characteristics.
To overcome these limitations, this paper revisits HVS with five representative characteristics, and further reorganizes their connections.
Based on the revisited HVS, a no-reference VQA framework called HVS-5M (NRVQA framework with five modules simulating HVS with five characteristics) is proposed.
It works in a domain-fusion design paradigm with advanced network structures.
On the side of the spatial domain, the visual saliency module applies SAMNet to obtain a saliency map.
And then, the content-dependency and the edge masking modules respectively utilize ConvNeXt to extract the spatial features, which have been attentively weighted by the saliency map
for the purpose of highlighting those regions that human beings may be interested in.
On the other side of the temporal domain,
to supplement the static spatial features,
the motion perception module utilizes SlowFast to obtain the dynamic temporal features.
Besides, the temporal hysteresis module applies TempHyst to simulate the memory mechanism of human beings, and comprehensively evaluates the quality score according to the fusion features from the spatial and temporal domains.
Extensive experiments show that our HVS-5M outperforms the state-of-the-art VQA methods.
Ablation studies are further conducted to verify the effectiveness of each module towards the proposed framework.
\end{abstract}

\begin{IEEEkeywords}
No-reference video quality assessment, human visual system, visual saliency, content-dependency, edge masking,   motion perception, temporal hysteresis.
\end{IEEEkeywords}

\section{Introduction}



\IEEEPARstart{R}{ecent} years have witnessed an explosive growth of
``we-media''.
It is estimated that there are about 4 billion video views per day on Facebook \cite{Facebook}. However, storing and delivering these vast amounts of video data
greatly stresses video service providers \cite{Mao2022}.
It is necessary to apply video coding to
reduce the storage capacity, and balance the tradeoff between the coding efficiency and the video quality.
Therefore, video quality assessment (VQA) has become a hot research topic \cite{Zhang2019}, \cite{Wu2021}, \cite{CSPT}, \cite{Xing2022}.
Subjective VQA is a manual rating by human beings which is time-consuming and labor-consuming \cite{Ou2021}.
By contrast, objective VQA is an automatic predicting by machines, and thus is more widely used in real application scenarios.
Since the scoring indicator of VQA, i.e., mean opinion score (MOS), is related to the visual effect of human beings, it is of great benefit to introduce human visual system (HVS) into VQA \cite{Xian2022,Zhang2022}.

Early HVS-based VQA methods utilized hand-crafted features to handle synthetic distortions.
In order to accurately simulate the texture masking of HVS, Ma \emph{et al.} \cite{Ma2013} developed a mutual masking strategy to extract the spatial information of video. Galkandage \emph{et al.} \cite{Galkandage2017} incorporated binocular suppression and recurrent excitation.
Saad \emph{et al.} \cite{VBLIINDS} proposed a non-distortion specific evaluation model relied on the video scenes in discrete cosine transform domain, and analyzed the types of motion that occurred in the video to predict the video quality.
Korhonen proposed TLVQM \cite{TLVQM} to reduce the complexity of feature extraction with a two-level method, which obtained the low and high complexity features from an entire video sequence and several representative video frames, respectively.
Wang and Li \cite{Wang2007} proposed a statistical model of human visual speed perception, which can estimate the motion information and the perceptual uncertainty in the video.













Recently, with the rapid development of deep learning techniques, CNN-based (Convolutional Neural Network)
VQA methods have significantly improved the evaluation of in-the-wild videos.
The in-the-wild videos are referred to as the authentically distorted ones, which are often hard to be annotated due to the lack of pristine ones.
To solve the problem of insufficient training data,
You and Korhonen \cite{You2019} utilized the extended short-term memory network to predict the video quality based on the features extracted from small video cube clips by 3D-CNNs. Li \emph{et al.} \cite{VSFA} applied the gated recurrent unit (GRU) \cite{GRU} to obtain the video quality score according to the frame-level content features extracted from ResNet \cite{ResNet}. Zhang and Wang \cite{TiVQA} utilized texture features to complement content features, further improving \cite{VSFA}.
However, all these methods had not fully exploited the temporal information within videos, which led to poor performance on LIVE-VQC containing rich motion-related contents.
Chen \emph{et al.} \cite{RIRNet} proposed to fuse motion information from different temporal frequencies in an efficient manner, and further applied a hierarchical distortion description to quantify the temporal motion effect.
Li \emph{et al.} \cite{Li2022} proposed a model-based transfer learning approach and applied motion perception to VQA.
Besides, some methods attempted to introduce visual saliency into VQA. For instance, Guan \emph{et al.} \cite{Guan2022} established a quality-aware visual attention module to obtain the frame-level quality scores, which were integrated into the video quality score through an end-to-end structure of visual and memory attention.
Vagar \cite{Vagar2022} proposed a parallel CNN structure, which firstly extracted the temporally pooled and saliency weighted features of the video, and then independently mapped them to the quality scores for further fusion.

\begin{figure}[t]\centering
\setlength{\belowcaptionskip}{-0.18cm}
{\includegraphics[width=3.85in]{Figure/HVS.pdf}}


\caption{Illustration of our revisited version of HVS.}
\label{fig:HVS}
\end{figure}


Despite their good performance, these methods have some potential limitations.
Firstly, only few characteristics of HVS have been exploited, and thus the simulated sensory function is incomplete. Besides, the connections among different characteristics have not been well organized, failing to facilitate their applications to VQA.
Secondly, some methods have not fully considered temporal information. The combination of spatial and temporal information is to be explored.
Thirdly, the effect of edge masking has not been introduced in VQA. In fact, shape edges can not only be utilized to conceal distortions in edge masking, but also be served as effective spatial features.
Fourthly, although early attempts have been made, how to effectively apply visual saliency to VQA is still challenging.












To address the above problems, this paper proposes a
general no-reference video quality assessment framework called HVS-5M (NRVQA framework with five modules simulating HVS with five characteristics).
The foundation of HVS-5M is our revisited version of HVS, wherein the connections of the five representative characteristics of HVS are reorganized.
On this basis, HVS-5M is designed in the domain-fusion paradigm.
Specifically, on the side of the spatial domain, the edge masking and the content-dependency modules are utilized to extract the frame-level spatial features, which are then weighted by the saliency map from the visual saliency module according to the attention mechanism.
On the other side of the temporal domain, to supplement the spatial features,
the motion perception module is utilized to extract the video-level temporal features.
Furthermore, the temporal hysteresis module simulates the memory mechanism of human beings, and outputs the quality score according to the fusion features integrated from the spatial static ones and temporal dynamic ones.
By this means, the quality of a given video can be comprehensively represented from different aspects.















The contributions of this work are summarized as follows.
\begin{itemize}
\item The mechanism of HVS is revisited, wherein five representative characteristics are selected to model the function of sensory organ in a relatively simple and comprehensive manner, and their connections are reorganized to facilitate its application to VQA.

\item A video quality assessment framework simulating HVS with five modules, called HVS-5M, is proposed, wherein these modules cooperatively work in the domain-fusion paradigm.
In particular, to the best of our knowledge, this is the first to introduce edge masking and a new scheme to apply visual saliency to VQA.

\item Experimental results show that our proposed HVS-5M achieves state-of-the-art (SOTA) performance on various mainstream video datasets, including four in-the-wild ones. Ablation studies are further conducted to verify the effectiveness of its different modules.
\end{itemize}

The rest of paper is organized as follows. HVS is first revisited from a neurophysiology perspective in Section \ref{HVS revisited}.
Then, the proposed HVS-5M is described in Section \ref{proposed method}.
Extensive experimental results are presented in Section \ref{sec:exp}.
Finally, conclusions are drawn in Section \ref{conclusion}.










\section{Human Visual System Revisited} \label{HVS revisited}

Human visual system (HVS) is responsible for detecting and interpreting the perceived spectral information to build a representation of the surrounding environment, which consists of sensory organ and parts of the central nervous system.

Specifically, the function of sensory organ has many complicated characteristics.
In order to demonstrate the function of sensory organ in a relative simple but comprehensive manner,
we revisit HVS, and formulate it with five representative characteristics.
\textit{Visual saliency} is a bottom-up, stimulus-driven signal, which indicates that a specific location is sufficiently different from the surrounding environment and deserves human attention \cite{Visual Saliency1}. \textit{Content-dependency} is referred to as a phenomenon that human preference is highly dependent on the observed content \cite{content-dependency4}.
\textit{Edge masking} indicates that the effect of masking is more likely to occur at positions with richer edge information \cite{Edge Masking}.
\textit{Motion perception} is the process of inferring the speed and direction of various elements in a dynamic scene.
\textit{Temporal hysteresis} \cite{Temporal-Memory} indicates that
the memory of elements with bad impression can last for longer than those with good impression.






\begin{figure*}[t]\centering
\setlength{\belowcaptionskip}{-0.18cm}
{\includegraphics[width=7.25in]{Figure/Framework.pdf}}
\caption{Illustration of the proposed HVS-5M, which includes five modules.}\label{framework}
\end{figure*}


On this basis, in order to apply HVS to VQA in a more efficient manner, we try to establish connections for the five characteristics in HVS, as shown in Fig. \ref{fig:HVS}.
According to the existing study \cite{P&M-path}, the parvocellular pathway (P-path) and magnocellular pathway (M-path) are two major pathways of the central nervous system.
Specifically, P-path can distinguish subtle spatial details \cite{P-cells}, while M-path is capable of detecting temporal motion information \cite{M-cells1}.
Therefore, we correspondingly divide those five characteristics into spatial or temporal ones.
On the side of the spatial characteristics, human beings could be attracted by salient regions (reflected in visual saliency) at first sight.
And then, under the guidance of the salient regions,
content (reflected in content-dependency) and edge (reflected in edge masking) are used to represent semantics and details, respectively.
On the other side of the temporal characteristics, motion information (reflected in motion perception)
is used to represent dynamic temporal changes.
Finally, the above semantics, details, and temporal changes are fused into a sequence of elements along time flow,
wherein the elements with bad impression are highlighted (reflected in temporal hysteresis).
By this means, these five characteristics can collaboratively work in HVS.
In this paper, we focus on reorganizing the connections of the five representative characteristics within our revisited version of HVS, and take it as a starting point for the research of VQA.






\section{Proposed Method} \label{proposed method}


In order to simulate the HVS mechanism, the proposed HVS-5M follows the domain-fusion paradigm and consists of five modules from two domains, as shown in Fig. \ref{framework}.

The first branch operates on frame level in spatial domain.
The visual saliency module extracts the saliency map for each frame.
Then, the content-dependency module and the edge masking module are responsible for extracting the content and the edge feature maps, respectively. These two feature maps are further adjusted by the saliency map in the attention manner for the purpose of highlighting the key regions, and then integrated into statistics as spatial features.
The second branch operates on video level in temporal domain, wherein the motion perception module aims at capturing the motion feature maps of a video sequence, and then combines them into temporal features.
Finally, the temporal hysteresis module simulates the memory mechanism of human beings,
and comprehensively evaluates the quality score according to the fusion features
from the spatial and temporal domains.
The detailed descriptions of each module are given in the following subsections, and the notations of features are given in Table \ref{variable}.



\subsection{Visual Saliency Module}

In HVS, it is widely acknowledged that human attention is attracted by visual salient region within an image \cite{Visual Saliency1,Visual Saliency2}.
To simulate HVS, the proposed HVS-5M also adopts the saliency map as an attention mask to highlight those regions that human beings may be interested in.
In the following of this paper, the video is supposed to have  frames and the -th frame is denoted as .
Considering both accuracy and computational complexity,
a lightweight network SAMNet \cite{SAMNet} pre-trained on ImageNet-22k \cite{ImageNet} is adopted to extract the saliency map  for each frame as

In SAMNet, a stereoscopically attentive multi-scale module is designed for effective and efficient multi-scale learning,
which enables each channel at each spatial position to adjust the weights of each branch.
Based on this module, a lightweight encoder-decoder network is utilized for salient object detection.
Note that ImageNet-22k is a large-scale image dataset with 22,000 categories.
The SAMNet pre-trained on ImageNet-22k has the ability to extract saliency map for a variety of images, and can fulfill the demand of visual saliency in HVS.
The output of SAMNet is the saliency map .



Note that each element of  can be interpreted as probability, which lies within the range of [0,1].
Via element-wise multiplying such saliency map with the feature maps extracted in the subsequent modules, it is expected that the model can
suppress the features within regions that human beings are not interested in.
However, as shown in Fig. \ref{Saliency map}, it can be observed that within the saliency map extracted by SAMNet, a majority of elements tend to 0.
Under the guidance of such saliency map, the model may excessively suppress some regions which may still contain certain useful spatial information, and cannot
bring the potential of visual saliency to full play.
Therefore, to better adapt the saliency map to the task of VQA, we perform adaptive adjustment on the saliency map, which is formulated in Eq. \eqref{eq:adaptive}.
Specifically, we multiply the elements in  by 255.
To further distinguish the significant regions from the non-significant ones, the elements greater than threshold, denoted as , are added by 350, while those less than  are added by 250.
And then, to make use of the saliency map in the subsequent modules, such saliency map is downsampled into the same size of the outputted feature maps in the content-dependency module and the edge masking module.
In our HVS-5M,  is empirically set to 100.
5pt]
	\widehat{\mathbf{A}}_n=\text{resize}\left( \mathbf{A}_n \times 255 +350 \right),  \quad \text{otherwise}.\\
\end{array} \right.
\label{Content extract}
\mathbf{C}_n=\text{CNN}\left( \mathbf{I}_{n} \right).\\
\label{}
\widetilde{\mathbf{C}}_{n}=\mathbf{C}_{n}\otimes \widehat{\mathbf{A}}_n,
\label{mean&std_for_content1}
\mathbf{C}^\text{mean}_{n}=\text{GP}_{\text{mean}}\left( \widetilde{\mathbf{C}}_{n} \right),
\label{mean&std_for_content2}
\mathbf{C}^\text{std}_{n}=\text{GP}_{\text{std}}\left( \widetilde{\mathbf{C}}_{n} \right).\\

\mathbf{D}_{n}=\text{Concat} \left(  \text{Canny}_{u,l}\left( \mathbf{I}_{n\!\:}^{\text{R}} \right), \text{Canny}_{u,l}\left( \mathbf{I}_{n\!\:}^{\text{G}} \right), \text{Canny}_{u,l}\left( \mathbf{I}_{n\!\:}^{\text{B}} \right)\right).
\label{}
\mathbf{E}_n=\text{CNN}\left( \mathbf{D}_{n} \right).\\
\label{}
{\widetilde{\mathbf{E}}_n}=\mathbf{E}_n \otimes \widehat{\mathbf{A}}_n,\\
\label{}
{\mathbf{E}^\text{mean}_n}=\text{GP}_{\text{mean}}\left( {\widetilde{\mathbf{E}}_n} \right),
\label{}
{\mathbf{E}}^\text{std}_{n}=\text{GP}_{\text{std}}\left( {\widetilde{\mathbf{E}}_n} \right).\\
 \label{Motion extraction}
\mathbf{M}=\text{SlowFast}\left( \left\{ \mathbf{I}_n \right\} _{n=1}^{N} \right) \ .
\label{}
{\mathbf{M}}^\text{mean}=\text{GP}_{\text{mean}}\left( \mathbf{M} \right),
\label{}
{\mathbf{M}}^\text{std}=\text{GP}_{\text{std}}\left( \mathbf{M} \right).
\label{}
\mathbf{S}_{n}=\text{Concat}\left( \mathbf{C}^\text{mean}_{n},\mathbf{C}^\text{std}_{n},\mathbf{E}^\text{mean}_{n},\mathbf{E}^\text{mean}_{n} \right).
\label{}
\mathbf{T}=\text{Concat}\left( {\mathbf{M}}^\text{mean}, {\mathbf{M}}^\text{std} \right) .
\label{}
\mathbf{F}_n=\text{Concat}\left( \text{Sample}\left( \left\{ \mathbf{S}_n \right\}_{n=1}^{N}\right), \mathbf{T}\right).
\label{}
\hat{\mathbf{F}}_n=\text{FC}\left( \mathbf{F}_{n} \right).
\label{}
q_n=\text{FC}\left( \text{GRU}\left( \hat{\mathbf{F}}_n \right) ,\text{GRU}\left( \hat{\mathbf{F}}_{n-1} \right) \right).
\label{}
x_{n}=\left\{
             \begin{array}{lr}
            q_n, & n = 1, \\
             \mathop{\text{min}\ q_k}\limits_{k\in V_{pre}},  &  n>1,\\
             \end{array}
\right.
\label{}
y_n = \sum_{k\in V_{next}}\ q_kw^k_n,
\label{}
w^k_n = \frac{e^{-q_k}}{\sum_{j\in V_{next}}e^{-q_j}}, k\in V_{next},
\label{}
q_n^{\prime} = \gamma x_n + ( 1 - \gamma)y_n,
\label{}
Q = \frac{1}{N} \sum^{N}_{n=1}q_n^{\prime}.
\label{}
\mathscr{L}=\mathscr{L}_{\text{SRCC}}+ \mathscr{L}_{\text{PLCC}}.
\label{}
\mathscr{L}_{\text{SRCC}}=1-\text{SRCC},
\label{}
\text{SRCC}=\frac{\sum_{i=1}^N{\left( y_{r}^{_i}-\bar{y}_r \right)}\left( \hat{y}_{r}^{i}-\hat{\bar{y}}_r \right)}{\sqrt{\sum_{i=1}^N{\left( y_{r}^{i}-\bar{y}_r \right)}^2}\sqrt{\sum_{i=1}^N{\left( \hat{y}_{r}^{i}-\hat{\bar{y}}_r \right)}^2}},
\label{}
\mathscr{L}_{\text{PLCC}}=\left( 1-\mathrm{PLCC} \right) /2,
\label{}
	\mathrm{PLCC}=\frac{\begin{matrix} \sum_{i=1}^N (y_{i}-\bar{y})(\hat{y_{i}}-\hat{\bar{y}}) \end{matrix}}{\sqrt{\begin{matrix} \sum_{i=1}^N (y_{i}-\bar{y})^{2}\end{matrix}} \sqrt{\begin{matrix} \sum_{i=1}^N (\hat{y_{i}}-\hat{\bar{y}})^{2}\end{matrix}}},

\noindent where  and  denote the mean value of the ground truth and the predicted quality scores, respectively.

Our proposed HVS-5M is built on Pytorch 3.8 and trained on two GeForce RTX A10 GPU cards. SAMNet \cite{SAMNet}, ConvNeXt-XL \cite{ConvNeXt}, and SlowFast \cite{SlowFast} are adopted in HVS-5M.
The dimension of the fusion features is reduced to 256 via fully-connected layer before being fed into the GRU network \cite{GRU}, and the hidden size of the GRU unit is set to 72.
 and  in the temporal hysteresis module are set to 12 and 0.5, respectively.
Adam optimizer \cite{Adam} is used for optimization, wherein the learning rate is initialized as  and decayed by 0.2 for every two epochs.
The results are averaged over 10 runs under the same setup.



\renewcommand\arraystretch{1.15}
\begin{table*}[]
\caption{Performance evaluations on cross datasets.}\label{Cross-Dataset1}
\centering
\fontsize{9.2}{9.2}\selectfont
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{c|ccccc|ccccc}
\Xhline{1.1pt}
Training                 & \multicolumn{5}{c|}{KoNViD-1k}                        & \multicolumn{5}{c}{LIVE-Qualcomm}                    \\ \hline
\multirow{2}{*}{Testing} & CVD2014        &  & LIVE-Qualcomm &  & LIVE-VQC      & KoNViD-1k      &  & CVD2014       &  & LIVE-VQC      \\   \cline{2-2} \cline{4-4} \cline{6-7} \cline{9-9} \cline{11-11}
                         & SRCC \quad   PLCC   &  & SRCC \quad   PLCC  &  & SRCC  \quad  PLCC  & SRCC \quad   PLCC   &  & SRCC \quad   PLCC  &  & SRCC \quad   PLCC  \\ \hline
NIQE    \cite{NIQE}                 & 0.3856 \quad 0.4410  &  & 0.1807 \quad 0.1672 &  & 0.4573 \quad 0.4025 & 0.4564 \quad 0.3619  &  & 0.3856 \quad 0.4410 &  & 0.4573 \quad 0.4025 \\
BRISQUE   \cite{BRISQUE}               & 0.4626 \quad 0.5060  &  & 0.3061 \quad 0.3303 &  & 0.5805 \quad 0.5788 & 0.4370 \quad 0.4274  &  & 0.4626 \quad 0.5060 &  & 0.5805 \quad 0.5788 \\
VSFA     \cite{VSFA}                & 0.6278 \quad 0.6216  &  & 0.5574 \quad 0.5769 &  & 0.6792 \quad 0.7198 & 0.6643 \quad 0.6116  &  & 0.5348 \quad 0.5606 &  & 0.6425 \quad 0.6819 \\
TLVQM     \cite{TLVQM}               & 0.3569 \quad 0.3838  &  & 0.4730 \quad 0.5127 &  & 0.5953 \quad 0.6248 & 0.0347 \quad 0.0467  &  & 0.4893 \quad 0.4721 &  & 0.4091 \quad 0.3559 \\
VIDEVAL   \cite{VIDEVAL}               & 0.6494 \quad 0.6638  &  & 0.4048 \quad 0.4351 &  & 0.5318 \quad 0.5329 & 0.1812 \quad -0.3441 &  & 0.6059 \quad 0.6244 &  & 0.4314 \quad 0.4122 \\
CNN-TLVQM  \cite{CNN-TLVQM}              & 0.6828 \quad 0.7226  &  & 0.6050 \quad 0.6223 &  & 0.7132 \quad 0.7522 & 0.0854 \quad 0.0216  &  & 0.2367 \quad 0.2388 &  & 0.0693 \quad 0.1040 \\
GSTVQA    \cite{Chen2021}             & 0.7972 \quad 0.7984  &  & 0.6200 \quad 0.6666 &  & 0.6797 \quad 0.7327 & 0.6694 \quad 0.6258  &  & \textbf{0.7046} \quad 0.6665 &  & 0.6201 \quad 0.6100   \\
HVS-5M                 &   \textbf{0.8042} \quad  \textbf{0.8097}             &  &   \textbf{0.6927} \quad  \textbf{0.7179}             &  &              \textbf{0.7728} \quad  \textbf{0.7995}  &      \textbf{0.7100} \quad  \textbf{0.6810}           &  &   0.7017 \quad  \textbf{0.6987}             &  &   \textbf{0.7626} \quad  \textbf{0.7577}             \\ \hline  \hline
Training                 & \multicolumn{5}{c|}{LIVE-VQC}                         & \multicolumn{5}{c}{CVD2014}                          \\ \hline
\multirow{2}{*}{Testing} & KoNViD-1k      &  & CVD2014       &  & LIVE-Qualcomm & KoNViD-1k      &  & LIVE-Qualcomm &  & LIVE-VQC      \\ \cline{2-2} \cline{4-4} \cline{6-7} \cline{9-9} \cline{11-11}
                         & SRCC \quad   PLCC   &  & SRCC \quad   PLCC  &  & SRCC \quad   PLCC  & SRCC  \quad  PLCC   &  & SRCC   \quad  PLCC  &  & SRCC  \quad  PLCC  \\ \hline
NIQE     \cite{NIQE}                & 0.4564 \quad 0.3619  &  & 0.3856 \quad 0.4410 &  & 0.1807 \quad 0.1672 & 0.4564 \quad 0.3619  &  & 0.1807 \quad 0.1672 &  & 0.4573 \quad 0.4025 \\
BRISQUE  \cite{BRISQUE}                & 0.4370 \quad 0.4274  &  & 0.4626 \quad 0.5060 &  & 0.3601 \quad 0.3303 & 0.4370 \quad 0.4274  &  & 0.3061 \quad 0.3303 &  & 0.5805 \quad 0.5788 \\
VSFA    \cite{VSFA}                 & 0.6584 \quad 0.6666  &  & 0.5061 \quad 0.5415 &  & 0.5094 \quad 0.5350 & 0.5759 \quad 0.5636  &  & 0.3256 \quad 0.3718 &  & 0.4600 \quad 0.4783 \\
TLVQM    \cite{TLVQM}                & 0.6023 \quad 0.5943  &  & 0.4553 \quad 0.4749 &  & 0.6415 \quad 0.6534 & 0.5437 \quad 0.5052  &  & 0.3334 \quad 0.3838 &  & 0.5397 \quad 0.5527 \\
VIDEVAL   \cite{VIDEVAL}               & 0.5007 \quad -0.4841 &  & 0.5702 \quad 0.5171 &  & 0.3021 \quad 0.3602 & 0.1918 \quad -0.3260 &  & 0.1208 \quad 0.3315 &  & 0.4751 \quad 0.5167 \\
CNN-TLVQM   \cite{CNN-TLVQM}             & 0.6431 \quad 0.6304  &  & 0.6300 \quad 0.6568 &  & 0.6574 \quad 0.6696 & 0.5779 \quad 0.5489  &  & 0.4410 \quad 0.4712 &  & 0.5209 \quad 0.5592 \\
GSTVQA  \cite{Chen2021}               & 0.7085 \quad 0.7074  &  & 0.6894 \quad 0.6645 &  & 0.5952 \quad 0.6245 & 0.6230 \quad 0.5764  &  &  0.4187 \quad 0.4965 &  & 0.5817 \quad 0.5751 \\
HVS-5M                 &  \textbf{0.7487} \quad  \textbf{0.7317}              &  &   \textbf{0.7509} \quad  \textbf{0.7508}            &  &   \textbf{0.7446} \quad  \textbf{0.7239}            &     \textbf{0.6979} \quad  \textbf{0.6788}           &  &   \textbf{0.5791} \quad  \textbf{0.5664          } &  &              \textbf{0.6745} \quad  \textbf{0.6929} \\ \Xhline{1.1pt}
\end{tabular}}
\end{table*}



\begin{table}[]
\caption{Performance evaluations on cross datasets.}\label{Cross-Dataset2}
\centering
\fontsize{9.2}{9.2}\selectfont
\begin{tabular}{cccc}
\toprule[1.1pt]
Training                 & \multicolumn{3}{c}{LSVQ}         \\ \midrule
\multirow{2}{*}{Testing} & KoNViD-1k     &  & LIVE-VQC      \\ \cmidrule(lr){2-2} \cmidrule(lr){4-4}
                         & SRCC \quad   PLCC  &  & SRCC \quad   PLCC  \\ \midrule
BRISQUE  \cite{BRISQUE}                & 0.646 \quad 0.647   &  & 0.524 \quad 0.536   \\
TLVQM    \cite{TLVQM}                & 0.732 \quad 0.724   &  & 0.670 \quad 0.691   \\
VIDEVAL   \cite{VIDEVAL}               & 0.751 \quad 0.741   &  & 0.630 \quad 0.640   \\
VSFA     \cite{VSFA}                & 0.784 \quad 0.794   &  & 0.734 \quad 0.772   \\
PVQ (w/o)  \cite{LSVQ}               & 0.782 \quad 0.781 &  & 0.747 \quad 0.776 \\
PVQ (w)   \cite{LSVQ}                & 0.791 \quad 0.795 &  & 0.770 \quad 0.807\\
BVQA-2022  \cite{Li2022}                  & 0.839 \quad 0.830   &  &   \textbf{0.816} \quad 0.824
            \\
HVS-5M                 &     \textbf{0.857} \quad \textbf{0.855} &  &              0.810 \quad \textbf{0.832} \\ \bottomrule[1.1pt]
\end{tabular}
\end{table}





\begin{table*}[]
\caption{Performance evaluations on mixed datasets. ``KoN'' indicates to KoNViD-1k \cite{KoNViD-1k}, ``VQC'' indicates to LIVE-VQC \cite{LIVE-VQC}, ``Qua'' indicates to LIVE-Qualcomm \cite{LIVE-Qualcomm}, and ``CVD'' indicates to CVD2014 \cite{CVD2014}.}\label{Mixed-Dataset}
\centering
\fontsize{9.2}{9.2}\selectfont
\setlength{\tabcolsep}{0.5mm}{
\begin{tabular}{cclclclclcl}
\toprule[1.1pt]
\multirow{3}{*}{Training set}               & \multicolumn{10}{c}{Testing set}                                                                                                                                             \\ \cmidrule(lr){2-11}
                                         & \multicolumn{2}{c}{CVD} & \multicolumn{2}{c}{KoN} & \multicolumn{2}{c}{Qua} & \multicolumn{2}{c}{VQC} & \multicolumn{2}{c}{Weighted average} \\
                                         & \multicolumn{2}{c}{MDTVSFA \ HVS-5M} & \multicolumn{2}{c}{MDTVSFA \ HVS-5M}   & \multicolumn{2}{c}{MDTVSFA \ HVS-5M}       & \multicolumn{2}{c}{MDTVSFA \ HVS-5M}  & \multicolumn{2}{c}{MDTVSFA \ HVS-5M}              \\ \midrule
KoN+CVD                        & \multicolumn{2}{c}{0.8552 \quad \ \textbf{0.8948}}  & \multicolumn{2}{c}{0.7742 \quad \ \textbf{0.8380}}    & \multicolumn{2}{c}{0.6484 \quad \ \textbf{0.6592}}        & \multicolumn{2}{c}{0.6933 \quad \ \textbf{0.7684}}    & \multicolumn{2}{c}{0.7498 \quad \ \textbf{0.8090}}               \\
KoN+VQC                  & \multicolumn{2}{c}{0.6325 \quad \ \textbf{0.7990} }  & \multicolumn{2}{c}{0.7974 \quad \ \textbf{0.8278}}    & \multicolumn{2}{c}{0.6995 \quad \ \textbf{0.7358}}        & \multicolumn{2}{c}{0.7461 \quad \ \textbf{0.8515}}   & \multicolumn{2}{c}{0.7575 \quad \ \textbf{0.8224}}               \\
KoN+Qua                   & \multicolumn{2}{c}{0.6933 \quad \ \textbf{0.7264}}  & \multicolumn{2}{c}{0.7835 \quad \ \textbf{0.8332}}    & \multicolumn{2}{c}{0.8169 \quad \ \textbf{0.8320}}         & \multicolumn{2}{c}{0.6968 \quad \ \textbf{0.7645}}    & \multicolumn{2}{c}{0.7544 \quad \ \textbf{0.8038}}               \\
Qua+CVD                     & \multicolumn{2}{c}{0.8636 \quad \ \textbf{0.8835}}  & \multicolumn{2}{c}{0.6691 \quad \ \textbf{0.7068}}    & \multicolumn{2}{c}{0.7882 \quad \ \textbf{0.8160}}         & \multicolumn{2}{c}{0.6153 \quad \ \textbf{0.7248}}   & \multicolumn{2}{c}{0.6865 \quad \ \textbf{0.7403}}               \\
Qua+VQC                   & \multicolumn{2}{c}{0.5849 \quad \ \textbf{0.7168}}  & \multicolumn{2}{c}{0.6843 \quad \ \textbf{0.7421}}    & \multicolumn{2}{c}{0.8010 \quad \ \textbf{0.8234}}        & \multicolumn{2}{c}{0.7434 \quad \ \textbf{0.8417}}   & \multicolumn{2}{c}{0.7003 \quad \ \textbf{0.7732}}               \\
VQC+CVD                & \multicolumn{2}{c}{0.8375 \quad \ \textbf{0.8757}}  & \multicolumn{2}{c}{0.7378 \quad \ \textbf{0.7582}}    & \multicolumn{2}{c}{0.6796 \quad \ \textbf{0.7040}}         & \multicolumn{2}{c}{0.7277 \quad \ \textbf{0.8398}}   & \multicolumn{2}{c}{0.7402 \quad \ \textbf{0.7869}}               \\
KoN+CVD+Qua        & \multicolumn{2}{c}{0.8412 \quad \ \textbf{0.8867}}  & \multicolumn{2}{c}{0.7659 \quad \ \textbf{0.8302}}    & \multicolumn{2}{c}{0.8158 \quad \ \textbf{0.8517}}        & \multicolumn{2}{c}{0.6851 \quad \ \textbf{0.7785}}   & \multicolumn{2}{c}{0.7572 \quad \ \textbf{0.8246}}               \\
KoN+VQC+Qua         & \multicolumn{2}{c}{0.6423 \quad \ \textbf{0.7489}}  & \multicolumn{2}{c}{0.7906 \quad \ \textbf{0.8167}}    & \multicolumn{2}{c}{0.8003 \quad \ \textbf{0.8443}}        & \multicolumn{2}{c}{0.7475 \quad \ \textbf{0.8352}}    & \multicolumn{2}{c}{0.7647 \quad \ \textbf{0.8191}}               \\
KoN+VQC+CVD              & \multicolumn{2}{c}{0.8303 \quad \ \textbf{0.8836}}  & \multicolumn{2}{c}{0.7859 \quad \ \textbf{0.8206}}    & \multicolumn{2}{c}{0.7013 \quad \ \textbf{0.7376}}        & \multicolumn{2}{c}{0.7443 \quad \ \textbf{0.8397}}   & \multicolumn{2}{c}{0.7718 \quad \ \textbf{0.8245}}               \\
Qua+VQC+CVD          & \multicolumn{2}{c}{0.8365 \quad \ \textbf{0.8899}}  & \multicolumn{2}{c}{0.7153 \quad \ \textbf{0.7454}}    & \multicolumn{2}{c}{0.8118 \quad \ \textbf{0.8363}}         & \multicolumn{2}{c}{0.7212 \quad \ \textbf{0.8460}}    & \multicolumn{2}{c}{0.7385 \quad \ \textbf{0.7955}}               \\
KoN+VQC+Qua+CVD & \multicolumn{2}{c}{0.8292 \quad \ \textbf{0.8976}}  & \multicolumn{2}{c}{0.7793 \quad \ \textbf{0.8178}}    & \multicolumn{2}{c}{0.8045 \quad \ \textbf{0.8462}}        & \multicolumn{2}{c}{0.7352 \quad \ \textbf{0.8474}}         & \multicolumn{2}{c}{0.7753 \quad \ \textbf{0.8366}}               \\ \bottomrule[1.1pt]
\end{tabular}}
\end{table*}


\begin{table*}[]
\caption{Performance evaluations of ablation studies.}\label{Ablation study}
\centering
\fontsize{9.2}{9.2}\selectfont
\setlength{\tabcolsep}{0.75mm}{

\begin{tabular}{cccccccccccccc}
\toprule[1.1pt]
\multirow{2}{*}{Model}                   & CVD2014                &  & LIVE-Qualcomm   &  & KoNViD-1k  &  & LIVE-VQC    &  & YouTube-UGC  &  & LSVQ-test &  & LSVQ-test-1080p\\ \cmidrule(lr){2-2} \cmidrule(lr){4-4} \cmidrule(lr){6-6} \cmidrule(lr){8-8}  \cmidrule(lr){10-10} \cmidrule(lr){12-12} \cmidrule(lr){14-14}
                                & SRCC \ \   PLCC     &  & SRCC  \ \  PLCC    &  & SRCC \ \   PLCC    &  & SRCC \ \   PLCC      &  & SRCC  \ \  PLCC &  & SRCC  \ \  PLCC &  & SRCC  \ \  PLCC \\ \midrule
Baseline                              & 0.8729 \ \ \textbf{0.8853}  &  & 0.8090   \ \  \textbf{0.8210} &  &  \textbf{0.8530} \ \  \textbf{0.8562}  &  & \textbf{0.8441}  \ \ \textbf{0.8422}  &  &   \textbf{0.8520}   \ \  \textbf{0.8451}  &  &      \textbf{0.8785}   \ \  \textbf{0.8723} &  &      \textbf{0.7977}   \ \ \textbf{0.8172}       \\
Variant I                  & \textbf{0.8795} \ \  0.8799 &  & \textbf{0.8122}  \ \   0.8189 &  &   0.8361 \ \  0.8372  &  & 0.8334 \ \  0.8284  &  &    0.8471   \ \  0.8389  &  &      0.8767 \ \   0.8715  &  & 0.7960  \ \   0.8146  \\

Variant II                           & 0.8619 \ \  0.8627  &  & 0.7729   \ \  0.7789 &  &   0.8096  \ \  0.8012 &  & 0.8139 \ \ 0.8075  &  & 0.8208   \ \  0.8139   &  &      0.8342   \ \  0.8415  &  &      0.7569   \ \  0.7805       \\
Variant III                       & 0.8509 \ \ 0.8523  &  & 0.7911 \ \   0.8081 &  &      0.8379 \ \  0.8371  &  & 0.8323 \ \ 0.8318  &   &  0.8422   \ \ 0.8365 &  &      0.8544   \ \  0.8588  &  &      0.7573   \ \  0.7809    \\

Variant IV                   & 0.8654 \ \ 0.8705 &  & 0.7897  \ \   0.8146 &  &     0.8340 \ \  0.8356 &  & 0.7685 \ \  0.7873   &   & 0.8355   \ \ 0.8352  &  &      0.8479  \ \  0.8538  &  &      0.7542   \ \ 0.7902    \\
Variant V                   & 0.8724 \ \ 0.8753 &  & 0.8072  \ \   0.8124 &  & 0.7314 \ \  0.7273 &  & 0.7989 \ \  0.7750  &   &    0.8136   \ \  0.7825  &  &      0.7940   \ \  0.7732   &  &      0.7556   \ \  0.7585    \\

Variant VI   &     0.8695  \ \        0.8756                         &  &0.8003  \ \        0.8197                   &  &       0.8047  \ \        0.7994  &  &    0.8138  \ \        0.8171 &          &  0.8055  \ \        0.8040    &  &      0.8528   \ \  0.8460  &  &      0.7589   \ \  0.7795     \\
\bottomrule[1.1pt]
\end{tabular}}
\end{table*}



\subsection{Performance Evaluations on Individual Datasets} \label{individual datasets}
In this part, the performance is evaluated on individual datasets, wherein the training set, the validation set, and the testing set come from the same dataset. The results are given in Table \ref{Individual Dataset}, and the following observations can be made.



\begin{itemize}
\item Our proposed HVS-5M achieves SOTA performance on five different datasets. Compared with the existing best-performed BVQA-2022 \cite{Li2022}, the average improvement is 1.5\% and 1.5\% for SRCC and PLCC on six datasets.




\item The improvement over BVQA-2022 is especially significant on the largest LSVQ dataset, which is 3.3\% and 2.8\% for SRCC and PLCC. By contrast, HVS-5M is less effective on the smallest LIVE-Qualcomm dataset. The reason is that its sample quantity may not be enough to activate the potential of HVS-5M with large architecture.


\item The performance of IQA is generally worse than that of VQA, since IQA only operates on spatial domain, and cannot perceive the motion information in temporal domain.
    Such results verify the necessity of the  domain-fusion design paradigm in HVS-5M.
\end{itemize}







\subsection{Performance Evaluations on Categorical Subsets} \label{categorical subsets}

In this part, the performance is evaluated on categorical subsets generated from KoNViD-1k \cite{KoNViD-1k}, LIVE-VQC \cite{LIVE-VQC}, and YouTube-UGC \cite{YouTube-UGC}.
One subset consists of videos with one same attribute, and can emphatically reflect the VQA performance in specific aspect.
According to \cite{VIDEVAL}, three types of categorical subsets are considered as follows.

\begin{itemize}
\item Resolution is an important factor affecting the quality of a video. To further investigate the performance of VQA methods at different resolutions, we select 402 1080p videos, 564 720p videos, and 607 below 480p videos as three subsets. The results are reported in Table \ref{resolution}.

\item Content can also affect the quality of video, which relies on personal preference of human beings. Correspondingly, we select 134 Screen Contents, 70 Animations, and 180 Gamings as three subsets. The results are reported in Table \ref{content}.

\item Evaluation of high quality and low quality videos is an important benchmark for VQA methods. We divide all videos into 1469 low quality videos and 1458 high quality ones by
    threshold 3.5537. The results are reported in Table \ref{quality}.
\end{itemize}

From the above three tables, it can be observed that our proposed HVS-5M achieves the best performance in almost all categorical subsets.
In particular, in the case of Gaming subsets, HVS-5M outperforms BVQA-2022 \cite{Li2022} by 22.0\% and 11.1\% on SRCC and PLCC.
The only exception is the case of subset of high quality video, where HVS-5M is slightly inferior to BVQA-2022.







\begin{table}[]
\caption{Performance evaluations of different models.}\label{Saliency threshold}
\centering
\fontsize{9.2}{9.2}\selectfont
\begin{tabular}{cclclcl}
\toprule[1.1pt]
\multirow{2}{*}{Different models}    & \multicolumn{2}{c}{CVD2014}         & \multicolumn{2}{c}{} & \multicolumn{2}{c}{LIVE-Qualcomm}   \\ \cmidrule(lr){2-3} \cmidrule(lr){6-7}
    & \multicolumn{2}{c}{SRCC \quad  PLCC}     & \multicolumn{2}{c}{} & \multicolumn{2}{c}{SRCC  \quad PLCC}     \\  \midrule
  & \multicolumn{2}{c}{0.8778 \quad  0.8851} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{0.8110 \quad  0.8207}  \\
  & \multicolumn{2}{c}{0.8820 \quad   0.8908}  & \multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{0.8163}  \quad \textbf{0.8226}} \\
  & \multicolumn{2}{c}{\textbf{0.8854} \quad  \textbf{0.8934}} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{0.8149 \quad  0.8201} \\
  & \multicolumn{2}{c}{0.8801 \quad  0.8903} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{0.8067  \quad 0.8148} \\
 (baseline) & \multicolumn{2}{c}{0.8729 \quad  0.8853} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{0.8090  \quad 0.8210}   \\
Variant I  & \multicolumn{2}{c}{0.8795 \quad  0.8799} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{0.8122  \quad 0.8189} \\  \bottomrule[1.1pt]
\end{tabular}
\end{table}



 \subsection{Performance Evaluations on Cross Datasets} \label{cross datasets}
In this part, the performance is evaluated on cross datasets. Since different video datasets have great differences in duration, content, and resolution, evaluations on cross datasets can verify the generalization ability and robustness of VQA methods.
Following the settings in \cite{Chen2021} and \cite{LSVQ}, we conduct two experiments on cross datasets, and the results are given in Table \ref{Cross-Dataset1} and Table \ref{Cross-Dataset2}, respectively. The following observations can be made.




\begin{itemize}
\item Table \ref{Cross-Dataset1} shows the results of training models on four relatively smaller datasets. It can be observed that our proposed HVS-5M outperforms existing methods in most cases. Especially, in the case of being trained on LIVE-VQC and tested on LIVE-Qualcomm, HVS-5M outperforms CNN-TLVQM \cite{CNN-TLVQM} by 13.3\% and 8.1\% on SRCC and PLCC, respectively, demonstrating that our method has stronger generalization ability on different datasets.

\item Table \ref{Cross-Dataset2} shows the results of training models on the largest LSVQ dataset. It can be observed that in the case of being tested on KoNViD-1k, the proposed HVS-5M outperforms BVQA-2022 \cite{Li2022} by 2.1\% and 3.0\% on SRCC and PLCC, while in the case of being tested on LIVE-VQC, the proposed HVS-5M obtains comparable performance as BVQA-2022.

\end{itemize}



\begin{figure}[t]\centering

\setlength{\belowcaptionskip}{-0.18cm}
\centerline{\includegraphics[width=3.8in]{Figure/Variant_saliency.pdf}}
\setlength{\abovecaptionskip}{-0.15cm}


\caption{The video frame and its corresponding saliency map.} \label{Variant saliency}
\end{figure}


 \subsection{Performance Evaluations on Mixed Datasets} \label{mixed datasets}
In this part, the performance is evaluated on mixed datasets. Nowadays, most of the methods based on deep learning are data-driven, and their performance relies on the quantity of datasets. Therefore, it is beneficial to utilize multiple datasets for mixed training, and thus the evaluations on mixed datasets are of practical value.
The results are given in Table \ref{Mixed-Dataset}.




\begin{itemize}
\item Compared with MDTVSFA \cite{MDTVSFA}, the SRCC of our proposed HVS-5M shows significant improvement in all cases. In particular, in the case of being trained on the mixed dataset of KoNViD-1k and LIVE-VQC, and tested on CVD2014, HVS-5M achieves
    26.3\% improvement over MDTVSFA on SRCC. 

\item However, increasing the quantity of training data may not necessarily bring performance improvement for both our HVS-5M and MDTVSFA.
    For example, regarding the mixture of KoNViD-1k and LIVE-VQC as the baseline training set. The performance of HVS-5M tested on KoNViD-1k drops from 0.8278 to 0.8167 when adding LIVE-Qualcomm into the training set. Similar results can be observed from MDTVSFA. The reason is that the added training set LIVE-Qualcomm and the testing set KoNViD-1k have great difference in resolution and time duration. This indicates that blindly increasing the quantity of training samples may not be the best choice. Instead, we should consider the distribution of the training set and the testing set.
\end{itemize}












\subsection{Ablation studies} \label{ablation studies}






In this part, ablation studies are conducted on the proposed framework. Specifically, HVS-5M is taken as baseline, and the impact of different modules are investigated by Variants I, II, III, IV, and V, respectively, and the impact of CNN architectures are investigated by Variant VI.








\begin{enumerate}[]
\item  \textit{Variant I.} The visual saliency module is disabled.
\item  \textit{Variant II.} The content-dependency module is disabled.
\item  \textit{Variant III.} The edge masking module is disabled.
\item  \textit{Variant IV.} The motion perception module is disabled.
\item  \textit{Variant V.} The fully-connected network is utilized to obtain the quality score, rather than TempHyst in the temporal hysteresis module.
\item  \textit{Variant VI.} ConvNeXt is replaced with ResNet-50 in the content-dependency and the edge masking module.

\end{enumerate}



The experimental results are presented in Table \ref{Ablation study}, and the following observations can be made from these results.







\begin{itemize}
\item The proposed HVS-5M outperforms the variants in most cases, indicating that HVS-5M can play its potential to full play when the five characteristics in the revisited HVS work in the collaborative manner.





\item Disabling the visual saliency module in Variant I brings performance degradation on four datasets, except LIVE-Qualcomm and CVD2014.
    Note that  is set to 100 according to the debug experiments on KoNViD-1k, and HVS-5M with such setting has achieved SOTA performance on five different datasets.
    The setting of  depends on the characteristics of datasets, and further debugging  can bring greater improvement on specific dataset.
As shown in Table \ref{Saliency threshold}, setting  to 50 and 60 are better options on LIVE-Qualcomm and CVD2014, respectively.
Besides, from Fig. \ref{Variant saliency}, it can be observed that our HVS-5M indeed focuses on the significant regions within video.


\item Disabling the content-dependency module in Varint II and disabling the edge masking module in Varint III lead to average performance degradation of 4.2\% and 2.5\% for SRCC on six datesets, respectively. Specifically, on CVD2014, the performance degradation by Variant III is even more significant than that by Variant II, indicating that the edge features may be more essential than the content features under certain circumstances.



\item Replacing TempHyst with fully-connected network in the temporal hysteresis module in Variant V brings the most significant performance degradation among all variants.
    Specifically, the degradations are 16.6\% and 17.7\% for SRCC and PLCC on KoNViD-1k, indicating that the memory mechanism in such module plays an important role in HVS-5M.

\item Replacing ResNet-50 with ConvNeXt in the content-dependency and the edge masking module in Variant VI also leads to significant performance degradation of 3.5\% and 2.8\% for SRCC and PLCC averaging on six datasets.
    It can be inferred that our proposed HVS-5M framework is flexible, and its performance can be further boosted with more advanced network architectures.

\end{itemize}


















\section{Conclusions} \label{conclusion}
In this paper, we propose a new NRVQA framework called HVS-5M. The foundation of HVS-5M is the revisited HVS, wherein five well-connected characteristics are utilized to model the function of sensory organ in a relatively simple and comprehensive manner.
HVS-5M follows the domain-fusion design paradigm, and simultaneously extracts the frame-level spatial features and the video-level temporal features, which are integrated to obtain the final quality score.
Extensive experiments have been conducted to evaluate the performance of HVS-5M, and the following conclusions can be drawn.
\begin{enumerate}[]
\item Evaluations on individual dataset show that HVS-5M achieves the SOTA performance on various mainstream datasets.

\item Evaluations on categorical subsets, cross datasets, and mixed datasets show that HVS-5M has strong generalization ability and robustness.

\item Ablation studies verify the contributions of five characteristics in HVS-5M, and the effectiveness of ConvNeXt.
\end{enumerate}

To the best of our knowledge, in the area of VQA, this paper makes the first attempt by introducing the characteristics of edge masking, and a new scheme to apply visual saliency.
The proposed framework represents a promising direction for objective VQA. Further investigations may include the following aspects.
Firstly, more characteristics of HVS can be explored to better simulate the function of sensory organ.
Secondly, there is still improvement room for the application of visual saliency.
Thirdly, to bring the potential of learning ability to full play, it is desired to train the model in an end-to-end manner.
\begin{thebibliography}{5}
\bibliographystyle{ieeetr} \small 





\bibitem{Facebook}
\emph{99Firms.} Facebook Video Statistics. [Online] Available: https://99firms.com/blog/facebook-video-statistics/.

\bibitem{Mao2022}
Y. Mao, M. Wang, S. Wang, and S. Kwong, ``High efficiency rate control for versatile video coding based on composite cauchy distribution'', \emph{IEEE Transactions on Circuits and Systems for Video Technology}, vol. 32, no. 4, pp. 2371-2384, 2021.

\bibitem{Zhang2019}
Y. Zhang, H. Zhang, M. Yu, S. Kwong, and Y.-S. Ho, ``Sparse representation-based video quality assessment for synthesized 3D videos'', \emph{IEEE Transactions on Image Processing}, vol. 29, pp. 509-524, 2019.

\bibitem{Wu2021}
J. Wu, Y. Liu, L. Li, W. Dong, and G. Shi, ``No-reference video quality assessment with heterogeneous knowledge ensemble'', in \emph{ACM International Conference on Multimedia (MM),} pp. 4174–4182, 2021.

\bibitem{CSPT}
P. Chen, L. Li, J. Wu, W. Dong, and G. Shi, ``Contrastive self-supervised pre-training for video quality assessment,'' \emph{IEEE Transactions on Image Processing}, vol. 31, pp. 458–471, 2021.

\bibitem{Xing2022}
F. Xing, Y.-G. Wang, H. Wang, L. Li, and G. Zhu, ``StarVQA: Space-time attention for video quality assessment,'' \emph{arXiv preprint arXiv:2108.09635}, 2021.

\bibitem{Ou2021}
F.-Z. Ou, X. Chen, R. Zhang, Y. Huang, S. Li, J. Li, Y. Li, L. Cao, and Y.-G. Wang, ``SDD-FIQA: Unsupervised face image quality assessment with similarity distribution distance'', in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 7670-7679, 2021.

\bibitem{Xian2022}
W. Xian, M. Zhou, B. Fang, and S. Kwong, ``A content-oriented no-reference perceptual video quality assessment method for computer graphics animation videos'', \emph{Information Sciences}, vol. 608, pp. 1731-1746, 2022.

\bibitem{Zhang2022}
Y. Zhang, H. Liu, Y. Yang, X. Fan, S. Kwong, and C. C. J. Kuo, ``Deep learning based just noticeable difference and perceptual quality prediction models for compressed video'', \emph{IEEE Transactions on Circuits and Systems for Video Technology}, vol. 32, no. 3, pp. 1197-1212, 2022.

\bibitem{Galkandage2017}
C. Galkandage, J. Calic, S. Dogan, and J. Guillemaut, ``Stereoscopic video quality assessment using binocular energy,'' \emph{IEEE Journal of Selected Topics in Signal Processing,} vol. 11, no. 1, pp. 102-112, 2017.

\bibitem{BRISQUE}
A. Mittal, A. K. Moorthy, and A. C. Bovik, ``No-reference image quality assessment in the spatial domain,'' \emph{IEEE Transactions on Image Processing}, vol. 21, no. 12, pp. 4695-4708, 2012.


\bibitem{VBLIINDS}
M. A. Saad, A. C. Bovik, and C. Charrier, ``Blind prediction of natural video quality,'' \emph{IEEE Transactions on Image Processing}, vol. 23, no. 3, pp. 1352-1365, 2014.

\bibitem{Wang2007}
Z. Wang and Q. Li, ``Video quality assessment using a statistical model of human visual speed perception,'' \emph{Journal of the Optical Society of America A}, vol. 24, no. 12, pp. B61–B69, 2007.

\bibitem{Guan2022}
X. Guan, F. Li, Y. Zhang, and P. C. Cosman, ``End-to-end blind video quality assessment based on visual and memory attention modeling,'' \emph{IEEE Transactions on Multimedia}, to be published, doi: 10.1109/TMM.2022.3189251.

\bibitem{Vagar2022}
D. Varga, ``No-reference video quality assessment using multi-pooled, saliency weighted deep features and decision fusion,'' \emph{Sensors}, vol. 22, no. 6, 2209, 2022.




\bibitem{ResNet}
K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image recognition,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 770-778, 2016.



\bibitem{LIVE-Qualcomm}
D. Ghadiyaram, J. Pan, A. C. Bovik, A. K. Moorthy, P. Panda, and K.-C. Yang, ``In-capture mobile video distortions: A study of subjective behavior and objective algorithms,'' \emph{IEEE Transactions on Circuits and Systems for Video Technology}, vol. 28, no. 9, pp. 2061-2077, 2018.

\bibitem{CVD2014}
M. Nuutinen, T. Virtanen, M. Vaahteranoksa, T. Vuori, P. Oittinen, and J. H\"akkinen, ``CVD2014—a database for evaluating no-reference video quality assessment algorithms,'' \emph{IEEE Transactions on Image Processing}, vol. 25, no. 7, pp. 3073-3086, 2016.

\bibitem{LIVE-VQC}
Z. Sinno and A. C. Bovik, ``Large-scale study of perceptual video quality,'' \emph{IEEE Transactions on Image Processing}, vol. 28, no. 2, pp. 612–627, 2019.

\bibitem{KoNViD-1k}
V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Szir\'anyi, S. Li, and D. Saupe, ``The konstanz natural video database (KoNViD-1k),'' in \emph{Eleventh International Conference on Quality of Multimedia Experience (QoMEX)}, pp. 1-6, 2017.



















\bibitem{P&M-path}
L. C. L. Silveira and V. H. Perry, ``The topography of magnocellular projecting ganglion cells (M-ganglion cells) in the primate retina,'' \emph{Neuroscience,} vol. 40, pp. 217–237, 1991.

\bibitem{P-cells}
X. Xu, J. M. Ichida, J. D. Allison, J. D. Boyd, A. B. Bonds, and V. A. Casagrande, ``A comparison of koniocellular, magnocellular and parvocellular receptive field properties in the lateral geniculate nucleus of the owl monkey (\emph{Aotus trivirgatus}),'' \emph{The Journal of Physiology,} vol. 531, pp. 203–218, 2001.

\bibitem{M-cells1}
A. M. Jeffries, N. J. Killian, and J. S. Pezaris, ``Mapping the primate lateral geniculate nucleus: A review of experiments and methods,'' \emph{Journal of Physiology-Paris,} vol. 108, pp. 3–10, 2014.

\bibitem{M-cells2}
A. Cheng, U. T. Eysel, and T. R. Vidyasagar, ``The role of the magnocellular pathway in serial deployment of visual attention,'' \emph{European Journal of Neuroscience,} vol. 20, pp. 2188–2192, 2004.

\bibitem{LGN}
M. J. Tov\'{e}e, ``An introduction to the visual system,'' \emph{Cambridge University Press,} 2008.

\bibitem{Visual Saliency1}
R. Desimone and J. Duncan, ``Neural mechanisms of selective visual attention,'' \emph{Annual Review of Neuroscience,} vol. 24, pp. 193–222, 1995.

\bibitem{Visual Saliency2}
L. Itti and C. Koch, ``Computational modelling of visual attention,'' \emph{Nature Reviews Neuroscience,} vol. 2, pp. 194–203, 2001.

\bibitem{Edge Masking}
A. Vassilev, ``Contrast sensitivity near borders: Significance of test stimulus form, size and duration,'' \emph{Vision Research,} vol. 13, pp. 719–730, 1973.

\bibitem{Motion perception}
A. M. Derrington, H. A. Allen, and L. Delicato, ``Visual mechanisms of motion analysis and motion perception,'' \emph{Annual Review of Psychology,} vol. 55, no. 1, pp. 181–205, 2004.

\bibitem{Vision Transformer}
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \emph{International Conference on Learning Representations (ICLR),} pp. 1–12, 2021.







\bibitem{You2019}
J. You and J. Korhonen, ``Deep neural networks for no-reference video quality assessment,'' in \emph{IEEE International Conference on Image Processing (ICIP)}, pp. 2349–2353, 2019.





\bibitem{Ma2013}
L. Ma, K. N. Ngan, and L. Xu, ``Reduced reference video quality assessment based on spatial HVS mutual masking and temporal motion estimation,'' in \emph{International Conference on Quality of Multimedia Experience (QoMEX)}, pp. 1-6, 2013.



\bibitem{Yang2005}
F. Yang, S. Wan, Y. Chang, and H. R. Wu, ``A novel objective no reference metric for digital video quality assessment,'' \emph{IEEE Signal Processing Letters}, vol. 12, no. 10, pp. 685-688, 2005.

\bibitem{RIRNet}
P. Chen, L. Li, L. Ma, J. Wu, and G. Shi, ``RIRNet: Recurrent-in-recurrent network for video quality assessment,'' in \emph{ACM International Conference on Multimedia (MM),} pp. 834–842, 2020.



\bibitem{VSFA}
D. Li, T. Jiang, and M. Jiang, ``Quality assessment of in-the-wild videos,'' in \emph{ACM International Conference on Multimedia (MM),} pp. 2351-2359, 2019.

\bibitem{GRU}
K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio, ``Learning phrase representations using RNN encoder-decoder for statistical machine translation,'' \emph{arXivpreprint arXiv}:1406.1078, 2014.

\bibitem{Canny}
J. Canny, ``A computational approach to edge detection,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 8, no. 6, pp. 679-698, 1986.

\bibitem{SAMNet}
Y. Liu, X.-Y. Zhang, J.-W. Bian, L. Zhang, and M.-M. Cheng, ``SAMNet: Stereoscopically attentive multi-scale network for lightweight salient object detection,'' \emph{IEEE Transactions on Image Processing}, vol. 30, pp. 3804–3814, 2021.

\bibitem{ConvNeXt}
Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, ``A convnet for the 2020s,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 11976-11986, 2022.

\bibitem{M-cell1}
M. Livingstone and D. Hubel, ``Segregation of form, color, movement, and depth: Anatomy, physiology, and perception,'' \emph{Science}, vol. 240, no. 4853, pp. 740–749, 1988.

\bibitem{M-cell2}
D. C. Van Essen and J. L. Gallant, ``Neural mechanisms of form and motion processing in the primate visual system,'' \emph{Neuron,} vol. 13, no. 1, pp. 1–10, 1994.

\bibitem{M-cell3}
A. M. Derrington and P. Lennie, ``Spatial and temporal contrast sensitivities of neurones in lateral geniculate nucleus of macaque,'' \emph{The Journal of physiology,} vol. 357, no. 1, pp. 219–240, 1984.

\bibitem{M-cell4}
D. J. Felleman and D. C. Van Essen, ``Distributed hierarchical processing in the primate cerebral cortex,'' \emph{Cerebral cortex,} vol. 1, no. 1, pp. 1–47, 1991.

\bibitem{SlowFast}
C. Feichtenhofer, H. Fan, J. Malik, and K. He, ``SlowFast networks for video recognition,'' in \emph{International Conference on Computer Vision (ICCV),} pp. 6201–6210, 2019.

\bibitem{Kinetic-400}
W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman, ``The kinetics human action video dataset,'' \emph{arXivpreprint arXiv}:1705.06950, 2017.

\bibitem{content-dependency1}
Z. Duanmu, K. Ma, and Z. Wang, ``Quality-of-experience of adaptive video streaming: Exploring the space of adaptations,'' in \emph{ACM International Conference on Multimedia (MM),} pp.1752-1760, 2017.

\bibitem{content-dependency2}
R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ``The unreasonable efectiveness of deep features as a perceptual metric,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 586-595, 2018.

\bibitem{content-dependency3}
C. G. Bampis, Z. Li, A. K. Moorthy, I. Katsavounidis, A. Aaron, and A. C. Bovik, ``Study of temporal effects on subjective video quality of experience,'' \emph{IEEE Transactions on Image Processing}, vol. 26, no. 11, pp. 5217–5231, 2017.

\bibitem{content-dependency4}
M. Mirkovic, Z. Vrgovic, D. Culibrk, D. Stefanovic, and A. Anderla, ``Evaluating the role of content in subjective video quality assessment,'' \emph{The Scientific World Journal}, vol. 2014, 2014.





\bibitem{ImageNet}
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and  F.-F. Li, ``ImageNet: A large-scale hierarchical image database,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 248-255, 2009.





\bibitem{Temporal-Memory}
K. Seshadrinathan and A. C. Bovik, ``Temporal hysteresis model of time varying subjective video quality,'' in \emph{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),} pp. 1153-1156, 2011.

\bibitem{YouTube-UGC}
Y. Wang, S. Inguva, and B. Adsumilli, ``YouTube UGC dataset for video compression research,'' in \emph{IEEE International Workshop on Multimedia Signal Processing (MMSP),} pp. 1–5, 2019.

\bibitem{LSVQ}
Z. Ying, M. Mandal, D. Ghadiyaram, and A. C. Bovik, ``Patch-VQ: ‘Patching Up’ the video quality problem,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),}  pp. 14019–14029, 2021.

\bibitem{VIDEVAL}
Z. Tu, Y. Wang, N. Birkbeck, B. Adsumilli, and A. C. Bovik, ``UGC-VQA: Benchmarking blind video quality assessment for user generated content,'' \emph{IEEE Transactions on Image Processing}, vol. 30, pp. 4449–4464, 2021.

\bibitem{CNN-TLVQM}
J. Korhonen, Y. Su, and J. You, ``Blind natural video quality prediction via statistical temporal features and deep spatial features'' in \emph{ACM International Conference on Multimedia (MM),} pp. 3311–3319, 2020.

\bibitem{Adam}
D. P. Kingma and J. Ba, ``Adam: A method for stochastic optimization,'' \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{SRCC}
M. Blondel, O. Teboul, Q. Berthet, and J. Djolonga, ``Fast differentiable sorting and ranking,'' in \emph{International Conference on Machine Learning (ICML),} pp. 950–959, 2020.

\bibitem{NIQE}
A. Mittal, R. Soundararajan, and A. C. Bovik, ``Making a `completely blind' image quality analyzer,'' \emph{IEEE Signal Processing Letters,} vol. 20, no. 3, pp. 209–212, 2013.

\bibitem{IL-NIQE}
L. Zhang, L. Zhang, and A. C. Bovik, ``A feature-enriched completely blind image quality evaluator,''  \emph{IEEE Transactions on Image Processing}, vol. 24, no. 8, pp. 2579–2591, 2015.

\bibitem{GM-LOG}
W. Xue, X. Mou, L. Zhang, A. C. Bovik, and X. Feng, ``Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features,'' \emph{IEEE Transactions on Image Processing}, vol. 23, no. 11, pp. 4850–4862, 2014.

\bibitem{HIGRADE}
D. Kundu, D. Ghadiyaram, A. C. Bovik, and B. L. Evans, ``No-reference quality assessment of tone-mapped HDR pictures,'' \emph{IEEE Transactions on Image Processing}, vol. 26, no. 6, pp. 2957–2971, 2017.

\bibitem{FRIQUEE}
D. Ghadiyaram and A. C. Bovik, ``Perceptual quality prediction on authentically distorted images using a bag of features approach,'' \emph{Journal of Vision,} vol. 17, no. 1, pp. 1-25 (32), 2017.

\bibitem{CORNIA}
P. Ye, J. Kumar, L. Kang, and D. Doermann, ``Unsupervised feature learning framework for no-reference image quality assessment,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 1098–1105, 2012.

\bibitem{HOSA}
J. Xu, P. Ye, Q. Li, H. Du, Y. Liu, and D. Doermann, ``Blind image quality assessment based on high order statistics aggregation,'' \emph{IEEE Transactions on Image Processing}, vol. 25, no. 9, pp. 4444–4457, 2016.

\bibitem{CONTRIQUE}
P. C. Madhusudana, N. Birkbeck, Y. Wang, B. Adsumilli, and A. C. Bovik, ``Image quality assessment using contrastive learning,'' \emph{IEEE Transactions on Image Processing}, vol. 31, pp. 4149–4161, 2022.

\bibitem{VGG}
K. Simonyan and A. Zisserman, ``Very deep convolutional networks for large-scale image recognition,'' in \emph{International Conference on Learning Representationsf (ICLR),} pp. 1–14, 2015.

\bibitem{KonCept512}
V. Hosu, H. Lin, T. Sziranyi, and D. Saupe, ``KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment,'' \emph{IEEE Transactions on Image Processing}, vol. 29, pp. 4041–4056, 2020.

\bibitem{PaQ-2-PiQ}
Z. Ying, H. Niu, P. Gupta, D. Mahajan, D. Ghadiyaram, and A. C. Bovik, ``From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality,'' in \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 3575–3585, 2020.

\bibitem{VIIDEO}
A. Mittal, M. A. Saad, and A. C. Bovik, ``A completely blind video integrity oracle,'' \emph{IEEE Transactions on Image Processing}, vol. 25, no. 1, pp. 289–300, 2016.



\bibitem{TLVQM}
J. Korhonen, “Two-level approach for no-reference consumer video quality assessment,” \emph{IEEE Transactions on Image Processing}, vol. 28, no. 12, pp. 5923–5938, 2019.

\bibitem{Chen2021}
B. Chen, L. Zhu, G. Li, F. Lu, H. Fan, and S. Wang, ``Learning generalized spatial-temporal deep feature representation for no-reference video quality assessment,'' \emph{IEEE Transactions on Circuits and Systems for Video Technology}, vol. 32, no. 4, pp. 1903–1916, 2021.



\bibitem{MDTVSFA}
D. Li, T. Jiang, and M. Jiang, ``Unified quality assessment of in-the-wild videos with mixed datasets training,'' \emph{International Journal of Computer Vision,} vol. 129, no. 4, pp. 1238-1257, 2021.

\bibitem{RAPIQUE}
Z. Tu, X. Yu, Y. Wang, N. Birkbeck, B. Adsumilli, and A. C. Bovik, ``RAPIQUE: Rapid and accurate video quality prediction of user generated content,'' \emph{IEEE Open Journal of Signal Processing,} vol. 2, pp. 425-440, 2021.


\bibitem{Li2022}
B. Li, W. Zhang, M. Tian, G. Zhai, and X. Wang, ``Blindly assess quality of in-the-wild videos via quality-aware pre-training and motion perception,'' \emph{IEEE Transactions on Circuits and Systems for Video Technology}, to be published, doi: 10.1109/TCSVT.2022.3164467.

\bibitem{CONVIQT}
P. C. Madhusudana, N. Birkbeck, Y. Wang, B. Adsumilli, and A. C. Bovik, ``CONVIQT: Contrastive video quality estimator'',  \emph{arXiv preprint arXiv:2206.14713}, 2022.

\bibitem{TiVQA}
A.-X. Zhang and Y.-G Wang, ``Texture information boosts video quality assessment,'' in \emph{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),} pp. 2050-2054, 2022.

\bibitem{LBP}
T. Ojala, M. Pietikainen, and D. Harwood, ``Performance evaluation of texture measures with classification based on Kullback discrimination of distributions,'' in \emph{IEEE International Conference on Pattern Recognition (ICPR),} pp. 582-585, 1994.

\vspace{-1cm}


\end{thebibliography}
\end{document}
