





\documentclass[final,3p,times]{elsarticle}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[cmex10]{amsmath}


\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig}
\usepackage[]{placeins}
\usepackage[]{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage[subnum]{cases}
\usepackage{tabularx}

\usepackage[]{hyperref} \usepackage[]{amsrefs} 

\newcommand{\nn}{\nonumber}
\newcommand\noi{\noindent}

\renewcommand{\algorithmicforall}{\textbf{for each}}

\makeatletter
\renewcommand\appendix{\par
  \setcounter{section}{0}\setcounter{subsection}{0}\setcounter{equation}{0}\setcounter{table}{0}\setcounter{figure}{0}\gdef\theequation{\@Alph\c@section.\arabic{equation}}\gdef\thefigure{\@Alph\c@section.\arabic{figure}}\gdef\thetable{\@Alph\c@section.\arabic{table}}\gdef\thesection{\Alph{section}}\@addtoreset{equation}{section}\@addtoreset{table}{section}\@addtoreset{figure}{section}}
\makeatother


\begin{document}
\begin{frontmatter}
\title{A parallel space saving algorithm for frequent items  \\ and the Hurwitz zeta distribution}
\author [unile] {Massimo~Cafaro\corref{cor1}}
\ead{massimo.cafaro@unisalento.it}
\cortext[cor1]{Corresponding author}
\author [unile] {Marco Pulimeno}
\ead{marco.pulimeno@unisalento.it}
\author [comp] {Piergiulio Tempesta}
\ead{p.tempesta@fis.ucm.es}
\address[unile]{University of Salento, Lecce, Italy}
\address[comp]{Departamento de F\'{\i}sica Te\'{o}rica II, Facultad de F\'{\i}sicas, Universidad
Complutense, 28040 -- Madrid, Spain and Instituto de Ciencias Matem\'aticas, C/ Nicol\'as Cabrera, No 13--15, 28049 Madrid, Spain}


\begin{abstract} We present a message-passing based parallel version of the Space Saving algorithm designed to solve the --majority problem. The algorithm determines in parallel frequent items, i.e., those whose frequency is greater than a given threshold, and is therefore useful for iceberg queries and many other different contexts. We apply our algorithm to the detection of frequent items in both real and synthetic datasets whose probability distribution functions are a Hurwitz and a Zipf distribution respectively. Also, we compare its parallel performances and accuracy against a parallel algorithm recently proposed for merging summaries derived by the Space Saving or Frequent algorithms.
\end{abstract}

\begin{keyword}
Frequent items, Space saving algorithm, Message--passing.
\end{keyword}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newdefinition{rmk}{Remark}
\newproof{pf}{Proof}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\newdefinition{defn}{Definition}
\newtheorem{conj}{Conjecture}
\newtheorem{exmp}{Example}
\newtheorem{case}{Case}

\end{frontmatter}



\section{Introduction}
\label{intro}
Discovering frequent items is a  data mining problem that attracted many researchers, owing to its relevance to  applications in  several domains. The problem is also known in the literature, depending on the specific application, as \textit{hot list analysis} \cite{Gibbons}, market basket analysis \cite{Brin} and  \textit{iceberg query} \cite{Fang98computingiceberg}, \cite{Beyer99bottom-upcomputation}. Additional applications include network traffic analysis \cite{DemaineLM02},  \cite{Estan}, \cite{Pan}, the analysis of web logs \cite{Charikar}, Computational and theoretical Linguistics \cite{CICLing}, ecological field studies \cite{Mouillot}, etc.
Several sequential solutions have been provided. In their survey \cite{Cormode}, Cormode and Hadjieleftheriou classify existing algorithms as being either \emph{counter} or \emph{sketch} based. Misra and Gries \cite{Misra82} proposed the first counters--based sequential algorithm, which has been rediscovered independently by Demaine et al. \cite{DemaineLM02} and Karp et al. \cite{Karp}. Recently designed counters--based algorithms include \emph{LossyCounting}
\cite{Manku02approximatefrequency} and \emph{Space Saving} \cite{Metwally2006}. In particular, Space Saving has been shown to be the most efficient and accurate algorithm among counters--based ones \cite{Cormode}, which motivates our choice of designing a parallel version of this algorithm.
Notable sketch--based solutions are \emph{CountSketch} \cite{Charikar} and \emph{CountMin} \cite{Cormode05}. In the parallel setting, we presented in \cite{cafaro-tempesta} a message-passing based parallel version of the \emph{Frequent} algorithm, whilst \cite{Zhang2013} presents a shared-memory parallel version.  A parallel version of the Lossy Counting algorithm has been proposed in \cite{Zhang2012}. Parallel versions of the Space Saving algorithm for shared-memory architectures have been designed in \cite{Roy2012} and \cite{Das2009}. A GPU (Graphics Processing Unit) accelerated algorithm for frequent items appeared in \cite{Govindaraju2005} and \cite{Erra2012}. Novel shared-memory parallel algorithms for frequent items were recently proposed in \cite{Tangwongsan2014}.

A similar problem, mining frequent itemsets, is strongly related to the problem of association pattern mining, which originated from the analysis of market basket data. Such datasets are basically sets of items bought by customers, traditionally referred to as transactions. Association pattern mining entails discovering the so-called association rules between sets of items. Frequent itemsets (also known in the literature as frequent patterns) are those sets of items determined by the mining process; association rules are simple implications stated as , in which both  and  are sets of items. Among the many algorithms appeared in the literature, we recall here recent work including \cite{fis1}, \cite{fis2}, \cite{fis3} and \cite{fis4}.

In this paper, we investigate how to parallelize the \emph{Space Saving} algorithm, and we design our algorithm in the context of message--passing architectures. To the best of our knowledge, this is the first parallel version of the Space Saving algorithm for message-passing architectures. Therefore it is the only one that can solve arbitrarily large problems on millions of cores, owing to intrinsic hardware limits related to shared-memory architectures preventing scalability of SMP (Symmetric Multi-Processing) nodes to higher processor counts. Indeed, current SMP nodes are equipped with at most a few dozens of processors. We prove the correctness of the algorithm, and then analyze its parallel complexity proving its cost--optimality for .

Another original aspect of this work is that we apply our algorithm to the study of frequent items in datasets whose probability distribution function is a \emph{Hurwitz distribution}. This distribution generalizes the classical Zipf distribution and is based on a well-known generalization of the Riemann zeta function, i.e. the Hurwitz function. We shall show that our parallel algorithm is especially suitable for treating these kind of datasets. We stress that the relevance of the Hurwitz distribution is very general. Indeed, the presence of an extra parameter makes it a more flexible tool than the classical Zipf one. To the best of our knowledge, this work offers the first example of application of the Hurwitz distribution in dataset analysis.

Before stating the problem solved by our algorithm, we need to recall a few basic definitions from multiset theory \cite{Syropoulos01}. We shall use a calligraphic capital letter to denote a multiset, and the corresponding capital Greek letter to denote its \textit{underlying} set. We are given a dataset  consisting of  elements, and an integer , with .


\begin{defn}
\label{multiset}
A multiset  is a pair where  is some set, called the underlying set of elements, and  is a function.
The generalized indicator function of  is 

 
\noindent where the integer--valued function , for each , provides its  \textit{frequency} (or multiplicity), i.e., the number of occurrences of  in . 
\noi The cardinality of  is expressed by




\noi whilst the cardinality of the underlying set  is


\end{defn}

\noindent A multiset (also called a \emph{bag}) essentially is a set where the duplication of elements is allowed. In the sequel,  will play the role of a finite
input array, containing  elements.

We can now state the problem formally.

\begin{defn}
\label{def2}
Given a \emph{multiset} , with , a --majority element (or \emph{frequent item}) is an element  whose \emph{frequency}  is such
that  .
\end{defn}

\textbf{Statement of the problem}. \textit{The --majority problem takes as input an array  of  numbers, and requires as output the set }.

Therefore, the --majority problem entails finding the set of elements whose frequency is greater than a given threshold controlled by the parameter . It is worth noting here that when , the problem reduces to the well known majority problem \cite{Moore81}, \cite{Fischer82}.

This article is organized as follows. We recall the sequential Space Saving algorithm in Section \ref{spacesaving}. Our parallel space saving algorithm is presented in Section \ref{pss-algorithm}. We prove its correctness in Section \ref{correctness},
analyze it and prove its cost--optimality for  in Section \ref{pss-analysis}. We provide and discuss in Appendix experimental results concerning the application of our algorithm to both real and synthetic datasets governed by a Zipf--Mandelbrot and by a Hurwitz distribution. In particular, we also compare our algorithm with another parallel algorithm designed and implemented by us starting from a sequential algorithm by Agarwal et al \cite{Agarwal}. Finally, we draw our conclusions in Section \ref{conclusions}.

\section{The space saving algorithm}
\label{spacesaving}

We recall here a few basic facts related to the sequential Space Saving algorithm that will be used later. The algorithm uses exactly  counters in order to solve the -majority problem sequentially, and allows estimating the maximum error committed when computing the frequency of an item. 
Space Saving works as described by the pseudocode of Algorithm \ref{ss}. We denote by ,  and  respectively the element monitored by the th counter of , the corresponding estimated frequency and error committed in the estimation. When processing an item which is already monitored by a counter, its estimated frequency is incremented by one. When processing an item which is not already monitored by one of the available counters, there are two possibilities. If a counter is available, it will be in charge of monitoring the item and its estimated frequency is set to one.  Otherwise, if all of the counters are already occupied (their frequencies are different from zero), the counter storing the item with minimum frequency is incremented by one. Then the monitored item is evicted from the counter and replaced by the new item. This happens since an item which is not monitored can not have occurred in the input a number of times greater than the minimal frequency. The algorithm assumes that the item has occurred exactly a number of times equal to the frequency stored by the minimum counter, estimating by excess its frequency and introducing an error which is at most the minimum frequency. We keep track of this error, as done in \cite{Metwally2006}, by storing for each monitored item its error .

Let  be the input multiset,  the multiset of all of the monitored items and their respective counters at the end of the sequential Space Saving algorithm's execution, i.e., the algorithm's summary data structure. Let   be the sum of the frequencies stored in the counters,  the exact frequency of an item ,  its estimated frequency,  the minimum frequency in  and  the error of item , i.e. an over-estimation of the difference between the estimated and exact frequency. It is worth noting here that  when  . 
The following relations hold (as proved in \cite{Metwally2006}) for each item :









If an item , at the end of the algorithm's execution, has an estimated frequency  less than the required threshold,  can be excluded from the output, since it can not be a frequent item. Instead, if we keep track of the error  and  is greater than or equal to the threshold, then  is a frequent item. All of the other output items are only \emph{potential} frequent items.


\begin{algorithm}
\begin{algorithmic}[1]
\Require , an array; , first index of  to be processed; , last index of  to be processed; , the -majority parameter
\Ensure a summary containing --majority candidate elements
\Procedure {SpaceSaving}{}
\State 
\For{ to }
	\If{ is monitored}
		\State let  be the counter of  
		\State 
	\Else
		\State let  be the element with least hits
		\State  
		\State 
		\State 
	\EndIf
\EndFor
\State \Return 
\EndProcedure
\caption{Space saving.}
\label{ss}
\end{algorithmic}
\end{algorithm}


\section{A parallel space saving algorithm}

\label{pss-algorithm}

The pseudocode of Algorithm \ref{pss} describes our parallel Space Saving algorithm. We assume that the input array  is initially read by an application calling our function implementing the algorithm; for instance, every process reads the input from a file or a designated process reads it and broadcast it to the other processes. The initial call is \textit{ParallelSpaceSaving} , where  consists of  elements,  is the number of processors (or cores) we use in parallel and  is the -majority parameter. Each processor is assigned a unique rank; ranks are numbered from 0 to . The algorithm determines in parallel --majority candidates. We recall here that, indeed, some of the candidates returned may be false positives as in the sequential counterpart.

The algorithm works as follows. In the initial domain decomposition, each processor determines the indices of the first and last
element related to its block, by applying a simple block distribution, in which each processor is responsible for either  or  elements.

Then, each processor determines , a stream summary data structure storing its local candidates, their corresponding estimated frequencies and errors, by using the well-known  algorithm designed by Metwally et al. \cite{Metwally2006}, shown in the pseudocode as the \textit{SpaceSaving} function. An hash table  is then built, storing the local candidates as keys and their corresponding counters (estimated frequencies and errors) as values. This hash table is then sorted in ascending order by counters' frequency and used as input for the parallel reduction, whose purpose is to determine global candidates for the whole array.
This step is carried out by means of the  function, shown as Algorithm. \ref{pssr}.

Assuming that the parallel reduction returns the result to the processor whose rank is 0, then that processor prunes the global candidates removing all of the items below the threshold required to be frequent items and returns the results. The  function, which is not shown here to save space, is just a linear scan in which every item's frequency is compared against the threshold and, if the frequency is greater than or equal to the threshold, then the item is returned in  as a \emph{potential} frequent item.

The parallel reduction determines global candidates for the whole array and works as shown in Algorithm \ref{pssr}. In each sub-step of the reduction, a processor receives as input from two processors   and  their  hash tables, that shall be called from now on   and  respectively. These data structures contain local items as keys and their counters storing estimated frequencies and errors. For a generic summary , we denote by  the number of  items in , and respectively with ,   and  the element monitored by the th counter of , the corresponding estimated frequency and the error committed in the estimation. 

The reduction works as follows. For both input summaries  we have .  We determine  as the minimum among the frequencies of  if , otherwise . Similarly, we determine  for .
Then, we combine the two summaries by calling the \textit{COMBINE} function, shown as pseudocode in Algorithm \ref{combine}. We scan the first hash table, and for each item in  we check if the item also appears in  by calling the \textit{FIND} function. In this case, we insert the entry for the item in , storing as its estimated frequency (respectively as its error) the sum of its frequency and the frequency of the corresponding item in  (respectively the sum of its error and the error of the corresponding item in ), and remove the item from . Otherwise, we insert the entry for the item storing as its estimated frequency (respectively as its error) the sum of its frequency and the minimum  (respectively the sum of its error and the minimum ). 

We then scan the second hash table. Since each time an item in  was also present in  we removed that item from , now  contains only items that do not appear in . For each item in  we simply insert the item in  and in the corresponding counter we store as estimated frequency (respectively as its error) the sum of its frequency and the minimum  (respectively the sum of its error and the minimum ). Finally, the entries in  are sorted by the counters' frequency and this hash table is returned.

Note that for the  summary returned by the \textit{COMBINE} function it holds that . Indeed,   may contain up to  items in the worst case (i.e., when all of the items in both  and  are different). 

However, we need to return at most  items. Therefore, if  (the number of entries with nonzero counter's frequency is at most ), we return  as . Otherwise, we remove the first  items and then return  as , which contains exactly the  items with the largest frequencies.

As an implementation detail, in the \textit{COMBINE} function it is possible to avoid using the  hash table altogether. Indeed, when scanning  one can simply update the frequency of the current item being processed, and when scanning  each item will be inserted into . At the end,  is returned. However, we prefer to use  in the interest of clarity, noting that the overall space complexity of \textit{COMBINE} is  in either case.




\begin{algorithm}
\begin{algorithmic}[1]
\Require , an array; , the length of ; , the number of processors; , the -majority parameter
\Ensure an hash table containing --majority candidate elements
\Procedure {ParallelSpaceSaving}{}
\Comment{The  elements of the input array  are distributed to the  processors so that each one is responsible for either  or
 elements; let  and  be respectively the indices of the first and last element of the sub-array handled by the process with rank
; ranks are numbered from 0 to }
\State 
\State 

\State 
\Comment{determine local candidates}
\State let  be an hash table storing  pairs in 
\State sort  by counters' frequency in ascending order
\State 
\Comment{determine the global candidates for the whole array}

\If{} \Comment{we assume here that the processor with rank 0 holds the final result of the parallel reduction}
	\State  
	\State \Return  
\EndIf

\EndProcedure
\caption{Parallel space saving.}
\label{pss}
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\begin{algorithmic}[1]
\Require , : hash tables ordered by counters' frequency; : the -majority parameter; 
	the hash tables store pairs , a monitored item  is used as key and a counter  as object, including the estimated frequency  and the error  of the item 
\Ensure an hash table, which is the \textit{merged summary} 
\Procedure {ParallelReduction}{}
        \Comment  is the number of items in the hash table 
	\If{}
		\State let  be the first counter in 
		\State 
	\Else
		\State 
	\EndIf
	\If{}
		\State let  be the first counter in 
		\State 
	\Else
		\State 
	\EndIf
\State 

\If{}
	\State \Return  as ;
\Else 
	\State 
	\State remove first  items from 
	\State \Return  as ; \Comment return the last  items
\EndIf

\EndProcedure
\caption{Parallel reduction for space saving summaries.}
\label{pssr}
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\begin{algorithmic}[1]
\Require , : hash tables ordered by counters' frequency; , the minimum of counters' frequency in ; , the minimum of counters' frequency in ; , the -majority parameter
\Ensure an hash table, which is the \textit{combined summary} 
\Procedure {combine}{,  , , , }
\State let  be an empty hash table
\ForAll{ in }
	\State 
	\State 	
	\State 
	\If{}
		\State 
		\State 
		\State 
		\State 
	\Else
		\State 
		\State 
		\State 
	\EndIf
\EndFor

\ForAll{ in }
	\State 
	\State 	
	\State 
	\State 
	\State 
\EndFor

\State sort  by counters' frequency in ascending order
\State \Return 
\EndProcedure
\caption{Combine.}
\label{combine}
\end{algorithmic}
\end{algorithm}



\section{Correctness}
\label{correctness}

In this Section we formally prove that our Parallel Space Saving Algorithm is correct when executed on  processors. We decompose the original array (i.e. multiset) of data  in  subarrays  , namely . Here
the  operator denotes the \textit{join operation} \cite{Syropoulos01}, which is the sum of the frequency functions as follows:
.
Let the sub--array  be assigned to the processor , whose rank is denoted by , with . Let also  denote the cardinality of , with .

The first step of the algorithm consists in the execution of the sequential Space Saving algorithm (which has already been proved to be correct by its authors),  on the subarray
assigned to each processor . Therefore, in order to prove the overall correctness of the algorithm, we just need to demonstrate that the parallel reduction is correct.
Our strategy is to prove that if a single sub-step of the parallel reduction is correct, then we can naturally extend the proof to the  steps of the whole parallel reduction.

We begin by proving a couple of preliminary results necessary for the proof of correctness of our parallel algorithm; both results are related to the combined summary  obtained by Algorithm \ref{combine}. We present in Table \ref{notation} the notation used throughout this Section, and recall here that we use a calligraphic capital letter to denote a multiset, and the corresponding capital Greek letter to denote its \textit{underlying} set.

\begin{table}
\renewcommand{\arraystretch}{1.3}
 \caption{Notation}
      \label{notation}
	\centering
    \begin{tabularx}{\textwidth}{@{} |c|X| @{}}
    \hline
    Notation & Description  \\ \hline \hline
     &  A generic multiset (input or summary) \\ \hline
     & Underlying set of    \\ \hline
     & Cardinality of  \\ \hline
     & Cardinality of the underlying set of  \\ \hline
     & Let  be a summary related to an input multiset ; given an item ,  is the estimated frequency of item  in  \\ \hline
     & Exact frequency of item  in , an input multiset\\ \hline
     & Estimated 
    error of item  in , a summary related to an input multiset \\ \hline
     & Minimum of counters' frequency in ; we let  if  \\ \hline
    \end{tabularx}
    \end{table}



Mathematically, we can express the combine operation as shown by the following two equations:





As shown in eq. (\ref{eq08}), if an item belongs to both summaries, we update its estimated frequency by summing up the estimated frequencies of the counters corresponding to the item in the two summaries. If an item belongs to only one summary, we update its estimated frequency by adding the minimum frequency stored in the other summary. At the same time we can estimate the error for each item, as shown by eq. (\ref{eq09}). This combining step leads to a summary  storing at most  distinct items. This summary includes all of the frequent items belonging to the set of items which is the union of the underlying sets related to the two input summaries.



\begin{lem}
\label{lemma1}
Let  and  be two summaries related respectively to the input sub-arrays  and , with . Let  be the intermediate summary obtained combining  and , and let .





\noi The following relation holds:


\end{lem} 

\noi where .

\begin{proof}
Let ,  and  . Then, . It follows that 



\noi Since  and , and observing that  and , it follows that eq. (\ref{eq11}) reduces to



\noi Therefore, we can rewrite eq. (\ref{eq10}) as



\noi This equation expresses the fact that the sum of the frequencies stored in  can be computed according to the way we combine the summaries in eq. (\ref{eq08}). Precisely, if  and , then  and  share all of the elements, so that



\noi Otherwise, for items belonging to just one of the summaries, we add to their frequencies the minimum frequency of the other summary. In other words, besides their frequency (which is taken into account by ) we also add exactly .

\end{proof}





\begin{lem}
\label{lemma2}
Let the summaries ,  and , the input multisets  and  and the quantity  be defined as in Lemma \ref{lemma1}. Assume that the following inequalities hold for each item : 






\noi Similarly, assume that the following inequalities hold for each item : 





\noi Then, for each item  we have:




 
\end{lem}


\begin{proof}


The summary  is derived from  and  by applying eqs. (\ref{eq08}) and (\ref{eq09}), so that, in order to prove eq. (\ref{eq19}) we need to distinguish three cases:

\begin{enumerate}
  \item Let : . But, by definition in this case it is , , 
        and , so that, taking into account eqs. (\ref{eq15}) and (\ref{eq17}), eq. (\ref{eq19}) holds;
  
  \item Let : following the same reasoning as before, taking into account eqs. (\ref{eq15}) and (\ref{eq18}) and that in this case it is by definition  and , we obtain , so that . Rewriting  as , and  as  we obtain eq. (\ref{eq19});

  \item Let : immediate, taking into account eqs. (\ref{eq16}) and (\ref{eq17}), since this case is symmetric to the previous one.
\end{enumerate}

To prove eq. (\ref{eq20}), taking into account eqs. (\ref{eq16}) and (\ref{eq18}) we obtain for an item : , i.e., .

\end{proof}



Now we can formally prove the correctness of our parallel algorithm. Let us consider how it works. Before engaging in the parallel reduction, each processor applies the sequential Space Saving algorithm to its local input, producing an hash table data structure containing at most  counters with estimated frequency greater than zero. In the parallel reduction, we merge pairs of data structures until we output the final result. 

We start by proving the following

\begin{thm}
\label{pr-single-step}
A single reduction sub-step correctly merges its two input summaries.
\end{thm}

\begin{proof}
Let  and  be two summaries related respectively to the input sub-arrays  and , with . Let  be the intermediate summary obtained combining  and  by using the \textit{COMBINE} function, and let  be the final merged summary. 


We are going to prove that if eqs. (\ref{eq05}) - (\ref{eq07}) hold for  and  and, if it is verified a relaxed version of eq. (\ref{eq04}), i.e., for a summary  it holds that



\noi then these properties  continue to be true also for  (it is worth noting here that eq. (\ref{eq21}) also holds for summaries produced by the sequential Space Saving algorithm). We shall show that this is enough to guarantee the correctness of the merge operation. 
 
The \textit{merge} operation is done in two steps and provides as output a summary of at most  items. In the first step we combine the input summaries as shown in eqs. (\ref{eq08}) and (\ref{eq09}). This combining step leads to an intermediate summary  storing at most  distinct items.

In the second and final step, we analyze  in order to return the final output. If  holds at most  entries (i.e., ), we return  as the output, i.e., . However, if  holds more than  entries (the data structure may hold in the worst case up to  entries), we need to select and return  entries. In this case, we simply return as  the last  entries in , i.e., those corresponding to the items with greatest frequency (the entries are sorted by counters' frequency). 

We start by noting that in the summary  generated by the first step a counter's frequency still represents an excess estimation of the monitored item, as in Space Saving. As before, let , and . 








By Lemma \ref{lemma1}, eq. (\ref{eq10}), if , then  (indeed, when   then , when  then ) and the merged summary  coincides with . In that case, since by eq. (\ref{eq21})  and , we have that , so that eq. (\ref{eq21}) also holds for . Otherwise, if , in order to obtain the final merged summary, we return in  the  items in  with the highest frequencies. Precisely, let the entries in  be sorted in ascending order with regard to the counters' frequencies. Then,




\noi where the sum is extended over the first  entries. We observe that , owing to the fact that the counters appear in sorted order, and the estimated frequencies stored in each of the initial  counters we are discarding are greater than or equal to . In this case too, it follows that eq. (\ref{eq21}) holds for . Indeed,



We have to prove that the other properties are verified as well. In particular we have to show that the error bound guaranteed by the sequential Space Saving algorithm is preserved by the merge operation. In order to do this, we observe that  is such that (see the similar proof of Lemma 3.3 in \cite{Metwally2006})



At the same time, , because the frequency of each item is greater than or equal to the minimum. Therefore we have:



Observing that  and taking into account eq. (\ref{eq25}) and the fact that eq. (\ref{eq21}) also holds for , we can bound  as follows:



At last, taking into account Lemma \ref{lemma2}, eqs. (\ref{eq19}) and (\ref{eq20}) and the way we construct , we have that, for each item  (i.e., for each distinct item in the input ):



and



\noi showing that eqs. (\ref{eq05}) - (\ref{eq07}) also hold for .

\end{proof}

\noi It is worth noting here that a single reduction step (i.e., a parallel execution with  processors) is fully equivalent to a sequential algorithm for merging two data summaries. Therefore, Theorem \ref{pr-single-step} states the correctness of this algorithm.  We can now prove the following

\begin{prop}
\label{pr-whole}
The whole parallel reduction correctly merges its input summaries.
\end{prop}

\begin{proof}

The correctness of the whole parallel reduction follows straightforwardly. Indeed, it is enough noting that in the initial step of the parallel reduction we process summaries derived by applying locally in each processor Space Saving, and eqs. (\ref{eq05}) - (\ref{eq07}) and (\ref{eq21}) hold for these summaries. By Theorem \ref{pr-single-step}, the merge operation used in each sub-step of the parallel reduction outputs a summary for which eqs. (\ref{eq05}) - (\ref{eq07}) and (\ref{eq21}) continue to hold and whose error is still within the bound on the error committed estimating the frequencies guaranteed by Space Saving. Therefore, at the end of the  steps required for the whole reduction, the final output summary correctly provides the frequent items for the whole input.

\end{proof}

The main result of this Section is the following 

\begin{thm} Algorithm \ref{pss} correctly determines frequent items in parallel.
\end{thm}

\begin{proof}
The result follows immediately from Theorem \ref{pr-single-step} and Proposition \ref{pr-whole}.
\end{proof}


\section{Parallel complexity}
\label{pss-analysis}
In this Section, we discuss the parallel complexity of the proposed parallel Space Saving algorithm. We assume, in the following analysis, that . The assumption is justified by the fact that it is verified in all of the cases of practical interest for this application.

At the beginning of the algorithm, the workload is balanced using a block
distribution; this is done with two simple  assignments; therefore, the complexity of the initial domain decomposition is . Next,
we determine local candidates in each subarray using the sequential Space Saving algorithm. Owing to the block distribution and to the fact that Space Saving is linear in the
number of input elements, the complexity of this step is . Then, we engage in a parallel reduction to
determine the global candidates for the whole input array. The whole reduction requires in the worst case .

Indeed, in the initial step we combine the input summaries in , by using hash tables. For each item in the  hash table, we try to find in  time a corresponding item in the  hash table. Then, we insert in the  hash table the entry for the item, again in  time and, if we have found the item, we delete the corresponding entry from  in  time. Since there are at most  entries, this requires . We then scan the entries in   (there can be at most  entries, this happens when the items in the two hash tables are all distinct, otherwise there will be less than  entries because we remove corresponding items from  each time we find a match). For each entry in , we simply insert the corresponding item in  in  time. Therefore, processing  requires in the worst case  time.

In the second step, we simply return the combined summaries if the total number of entries in  is less than or equal to , otherwise, we return the last  entries in sorted order of . The time required is .

To recap, since we do  work in each step of the parallel reduction,  by assumption and there are  such steps, the overall complexity of the reduction is  . The communication cost, i.e., the amount of data exchanged in the parallel reduction is 



\noi since  by assumption.

Finally, the worst case complexity of the  function is , since this is just a linear scan in which we compare the frequency of each item against the threshold required to be a frequent item, and put an item in the   summary if its frequency is greater than or equal to the required threshold.

It follows that the overall  complexity of the parallel Space Saving algorithm is . We are now in the position to state the following Theorem:

\begin{thm}
The algorithm is cost--optimal for .
\end{thm}

\begin{proof}
Cost--optimality requires by definition that asymptotically  where   represents the time spent on one processor (sequential time) and  the time spent on
 processors. The sequential algorithm requires  in the worst case, and the parallel complexity of our algorithm is  when . It follows from
the definition that the algorithm is cost--optimal for .
\end{proof}

Cost--optimality is an important theoretical property of parallel algorithms, since it implies linear speedup (equal to ) and efficiency equal to 1. Moreover,  cost--optimality also implies good scalability of the algorithm when using smaller sized parallel machines equipped with a limited number of processors. Indeed, scaling down a cost--optimal algorithm on a reduced number of processors will result in a fast algorithm, while scaling down a non cost--optimal algorithm may even result in a parallel algorithm doing more work and being slower than the corresponding best sequential algorithm. 

We proceed with the analysis of isoefficiency and scalability. The sequential algorithm has complexity ; the parallel overhead is . In our case,  .  The isoefficiency relation \cite{Grama93} is then . Finally, we derive the scalability function of this parallel system
\cite{Quinn03}.

This function shows how memory usage per processor must grow to maintain efficiency at a desired level. If the isoefficiency relation is  and  denotes the
amount of memory required for a problem of size , then  shows how memory usage per processor must increase to maintain the same level of efficiency. Indeed, in
order to maintain efficiency when increasing , we must increase  as well, but on parallel computers the maximum problem size is limited by the available memory, which is
linear in . Therefore, when the scalability function  is a constant , the parallel algorithm is perfectly scalable;  represents instead the limit for
scalable algorithms. Beyond this point an algorithm is not scalable (from this point of view).
In our case the function describing how much memory is used for a problem of size  is given by . Therefore,  with  given by the
isoefficiency relation.

\section{Conclusions}
\label{conclusions}


To the best of our knowledge, we have designed and implemented the first message-passing based parallel version of the Space Saving algorithm to solve the --majority problem. In particular, we have shown that our algorithm retains all of the key features of the sequential Space Saving algorithm. Besides proving its formal correctness, we have applied our algorithm to the detection of frequent items in real datasets and in synthetic datasets whose probability distribution functions are a Hurwitz and a Zipf distribution respectively. Extensive experimental results on both synthetic and real datasets have been reported and discussed in Appendix, clearly showing that our algorithm outperforms the parallel version of the algorithm designed by Agarwal et al. with regard to precision, total error and average relative error, while providing overall comparable parallel performances with linear speedup.



\section*{Acknowledgment}
We are indebted to the unknown referees for enlightening observations, which helped us to improve the paper. The authors would also like to thank G. Cormode and M. Hadjieleftheriou for making freely available their sequential implementation of the Space Saving algorithm. We are also grateful to Prof. Palpanas of Paris Descartes University for providing us with the real datasets used in the experiments. The research of M. Cafaro has been supported by CMCC, Italy, under the grant FISR Gemina project, Italian Ministry of Education, University and Research. The research of P. Tempesta has been supported by the grant FIS2011--22566, Ministerio de Ciencia e Innovaci\'on, Spain.


\appendix
\section{Experimental results}
\label{appendix}



We report here the experimental results we have obtained running the parallel Space Saving algorithm on an IBM iDataPlex cluster. Each SMP node is configured with two 2.6 Ghz octa-core Xeon Sandy Bridge E5-2670 CPUs  with 20 MB level 3 cache and 64 GB of main memory. The interconnection network is  Infiniband 4x FDR-10 (Fourteen Data Rate) 40 Gbp/s, which provides 5 GB/s unidirectional bandwidth. Our parallel implementation, developed in C++ using MPI,  is based on the sequential source code for the \emph{Space Saving} algorithm developed in \cite{Cormode-code}.

In order to assess the merits of our parallel algorithm, we also compare it with a second parallel algorithm which we have designed and implemented starting from a sequential algorithm by Agarwal et al \cite{Agarwal}. The authors designed their algorithm for merging Frequent summaries, and then proved that for Space Saving summaries subtracting the minima from their respective summaries (if a summary possesses  counters) makes them isomorphic to Frequent summaries, so that their algorithm can be reused (see Lemma 2 in \cite{Agarwal}).

In the \emph{ParallelAgarwal} algorithm (see Algorithm \ref{pa}) each processor starts by executing the Space Saving algorithm on its local sub-array. Then, just before engaging in the parallel reduction, if the \emph{local} summary holds  nonzero counters, the minimum frequency, which is stored in the first counter \emph{local}[1], is subtracted from each counter. It follows that the \emph{local} summary stores at most  counters, so that the algorithm by Agarwal et al. shown as the \emph{AgarwalParallelReduction} (see Algorithm \ref{par}), can be applied. The input of the parallel reduction is an hash table, storing the entries in  sorted by counters' frequency.

\begin{algorithm}
\begin{algorithmic}[1]
\Require , an array; , the length of ; , the number of processors; , the -majority parameter
\Ensure an hash table containing --majority candidate elements
\Procedure {ParallelAgarwal}{}
\Comment{The  elements of the input array  are distributed to the  processors so that each one is responsible for either  or
 elements; let  and  be respectively the indices of the first and last element of the sub-array handled by the process with rank
; ranks are numbered from 0 to }
\State 
\State 
\State 
\Comment{determine local candidates}

	\If{} \Comment  is the number of items in the stream summary  with nonzero frequency
		\State 
		\For{ to }
			\State 
		\EndFor
	\EndIf
\State let  be an hash table storing  pairs in 
\State sort  by counters' frequency in ascending order
\State 
\Comment{determine the global candidates for the whole array}
\If{} \Comment{we assume here that the processor with rank 0 contains the final result of the parallel reduction}
	\State \Return 
\EndIf
\EndProcedure
\caption{Parallel algorithm by Agarwal et al.}
\label{pa}
\end{algorithmic}
\end{algorithm}

Although the algorithm is presented in the context of merging two summaries, it can actually be used in parallel as a reduction operator, owing to the fact that the authors also proved a bound on the output error, which is within the error affecting the input summaries.


\begin{algorithm}
\begin{algorithmic}[1]
\Require , : hash tables; , -majority parameter (the number of counters is at most );
\Ensure an hash table containing --majority candidate elements
\Procedure{AgarwalParallelReduction}{} \Comment{a merged summary of  and }
\State 
\State sort  by counters' frequency in ascending order
\If{}
	\State \Return ;
\Else \Comment{prune counters in }

	\State 
	\Comment{determine frequency to be subtracted}
	\State 
		\State 	
		\State 
	\Comment{subtract this frequency from the last  counters}
	\For{ to }
		\State 
		\State 
		\State 	
		\State 
		\State 
	\EndFor
	\State remove first  items from 
	\State \Return ;
\EndIf
\EndProcedure
\caption{Parallel Reduction by Agarwal et al.}
\label{par}
\end{algorithmic}
\end{algorithm}

The parallel reduction works as follows. It starts combining the two data sets, by calling the \textit{AGARWAL-COMBINE} function. Let  be the combined summary. Scanning the first hash table, for each item in  the function checks if the item also appears in . In this case, it inserts the entry for the item in , storing as its estimated frequency the sum of the item's frequency and the frequency of the corresponding item in , and removes the item from . Otherwise, the function inserts the entry for the item storing as its estimated frequency its frequency in . 

The function then scans the second hash table. Since each time an item in  was also present in  it was removed from , now  contains only items that do not appear in . For each item in  it simply inserts the item in  and in the corresponding counter it stores as estimated frequency its frequency in . 

 This could entail the use of up to  counters in the worst case, when  and  share no item. Let  be the number of counters in . The entries in  are sorted by the counters' frequency in ascending order, and, if  the algorithm returns . Otherwise, a pruning operation is required. The combine step can be performed with a constant number of sorts and scans of summaries of size . Then, the algorithm subtracts from the last  counters the frequency of the --th counter, removes the first  counters and returns the remaining  counters, whose frequency has been corrected. The algorithm requires in the worst case time linear in the total number of counters, i.e.,  if implemented as described in \cite{Agarwal} using an hash table. 

In the experiments, we tested our algorithm against the one from Agarwal et al. on both synthetic and real datasets. Regarding synthetic datasets, the input distributions used in our experiments are the Riemann--Hurwitz distribution (Hurwitz for short), and its particular case, the Zipf distribution, which is one of the most used in experiments related to sequential algorithms for frequent items. We recall that the Zipf distribution has associated the probability density function (p.d.f.)



\noindent where  is a positive real parameter controlling the skewness of the distribution and



\noindent is the Riemann zeta function \cite{iwaniec-kowalski}. The Hurwitz distribution has p.d.f.



\noindent where



\noindent is the Riemann--Hurwitz zeta function. Both functions play a crucial role in analytic number theory \cite{iwaniec-kowalski} \cite{Tempesta}. 

The real datasets we used come from different domains \cite{Dallachiesa}. All of the datasets are publicly available, and two of them (Kosarak and Retail) have been widely used and reported in the data mining literature. Overall, the four datasets are characterized by a diversity of statistical characteristics, which we report in Table \ref{data}. 

\textbf{Kosarak:} This is a click-stream dataset of a Hungarian online news portal. It has been anonymized, and consists of transactions, each of which is comprised of several integer items. In the experiments, we have considered every single item in serial order.

\textbf{Retail:} This dataset contains retail market basket data coming from an anonymous Belgian store. Again, we consider all of the items belonging to the dataset in serial order.

\textbf{Q148:} Derived from the KDD Cup 2000 data, compliments of Blue Martini, this dataset contains several data. The ones we use for our experiments are the values of the attribute “Request Processing Time Sum” (attribute number 148), coming from the ``clicks'' dataset. A pre-processing step was required, in order to obtain the final dataset. We had to replace all of the missing values (appearing as question marks) with the value of 0.

\textbf{Nasa:} Compliments of NASA and the Voyager 2 Triaxial Fluxgate Magnetometer principal investigator, Dr. Norman F. Ness, this dataset contains several data. We selected the “Field Magnitude (F1)” and “Field Modulus (F2)” attributes from the Voyager 2 spacecraft Hourly Average Interplanetary Magnetic Field Data. A pre-processing step was required for this dataset: having selected the data for the years 1977-2004, we removed the unknown values (marked as 999), and multiplied all values by 1000 to convert them to integers (since the original values were real numbers with precision of 3 decimal points). The values of the two attributes were finally concatenated. In our experiments, we read all of the values of the attribute “F1”, followed by all of the values of the attribute “F2”.

\begin{table}
\renewcommand{\arraystretch}{1.3}
 \caption{Statistical characteristics of the real datasets}
      \label{data}
	\centering
    \begin{tabular}{|c |  c |  c  | c | c |}
    \hline
      & Kosarak & Retail & Q148 & Nasa  \\ \hline
      \hline
    Count &  8019015 & 908576 & 234954 & 284170 \\ \hline
    Distinct items & 41270 & 16470 & 11824 & 2116 \\ \hline
    Min & 1 & 0 & 0 &  0  \\ \hline 
    Max & 41270 & 16469 & 149464496 & 28474 \\ \hline 
    Mean & 2387.2 & 3264.7 & 3392.9 & 353.9  \\ \hline
    Median & 640 & 1564 & 63 & 120 \\ \hline
    Std. deviation & 4308.5 & 4093.2 & 309782.5 & 778.1 \\ \hline
    Skewness & 3.5 & 1.5 & 478.1 & 6.5  \\ \hline
    \end{tabular}
    \end{table}

Denoting with   the true frequency of an item and with  the corresponding frequency reported by an algorithm, then the absolute error is, by definition, the difference . The (absolute) total error is then defined as the sum of the absolute errors related to the items reported by an algorithm. Similarly, the absolute relative error is defined as , and the average relative error is derived by averaging the absolute relative errors over all of the measured frequencies.

Precision is defined as the total number of true -majority elements reported over the total number of items reported. Therefore, this metric quantifies the number of false positives reported by an algorithm in the output data summary. Recall is instead the total number of true -majority elements reported over the number of true -majority elements given by an exact algorithm. It follows that an algorithm is correct if an only if its recall is equal to 1 (or 100\%); both algorithms under test have already been proved to be formally correct and their recall in all of the tests is indeed equal to 1.

\subsection{Real datasets: error}

In this Section, we report the experimental results obtained on the real datasets. We do not report on the performances, owing to the fact that processing the largest dataset on a single processor requires just a few milliseconds. Since the datasets are real, the only parameter we can vary is . In the following tests,  has been varied from 100 to 1000, in steps of 100 (owing to the statistical characteristics of the real datasets). 

We report the total error, the precision and the average relative error (denoted from now on as ARE). 

Figure \ref{tekn} presents the results related to the total error for the Kosarak and Nasa datasets, whilst Figure \ref{teqr} is related to the Q148 and Retail datasets. Note that we use a logarithmic scale for the total error values, since some of the curves would otherwise be too close to distinguish them. As shown, our algorithm outperforms the Agarwal et al. algorithm for all of the datasets under test, with very low and close to zero total error for both the Kosarak and Q148 datasets. The values for both the Nasa and the Retail datasets are about an order of magnitude smaller than the corresponding values obtained by Agarwal et al. 

Regarding the precision, the results in Figures \ref{preckn} (Kosarak and Nasa datasets) and \ref{precqr} (Q148 and Retail datasets) are also clear evidence of the superiority of our algorithm. As shown, the algorithm by Agarwal et al. achieves a precision almost equal to zero for all of the datasets under test. Our algorithm exhibits a precision close to one for both Kosarak and Q148. For the Nasa dataset, the precision is between 0.55 and 0.85 for  in the range [100 - 300], and steadily increases towards 1.0 for  in the range [400 - 1000]. Similarly, for the Retail dataset, the precision is between 0.5 and 1.0 for  in the range [100 - 400], and steadily increases towards 0.7 for  in the range [500 - 1000].

Finally, Figures \ref{arekn} (Kosarak and Nasa datasets) and \ref{areqr} (Q148 and Retail datasets) are related to the average relative error, with our algorithm clearly outperforming the other. Our algorithm exhibits ARE values close to zero for both Kosarak and Q148. For the Nasa dataset, our algorithm's ARE values are steadily decreasing from 0.2 to 0. The same behavior is observed for the Retail dataset, where our algorithm exhibits ARE close to zero for , close to one for , equal to 0.5 for  and then steadily decreasing ARE values from 0.62 to 0.26 in the range [300 - 1000].

 
\begin{figure*}[hbt]
  \centering
  \begin{tabular}{ c c }
	\subfloat[Kosarak and Nasa datasets]{
           \includegraphics[scale=0.5]{tekn-real.eps}
           \label{tekn}
    } & 
           
    \subfloat[Q148 and Retail datasets]{
          \includegraphics[scale=0.5]{teqr-real.eps}
          \label{teqr}
    } 
  	
\end{tabular}
           
 \caption{Real datasets: Total Error varying  on  cores}
 \label{real-te}
 
\end{figure*}



\begin{figure*}[h!]
  \centering
  \begin{tabular}{ c c }
		
	\subfloat[Kosarak and Nasa datasets]{
           \includegraphics[scale=0.5]{preckn-real.eps}
           \label{preckn}
    } & 
           
    \subfloat[Q148 and Retail datasets]{
          \includegraphics[scale=0.5]{precqr-real.eps}
          \label{precqr}
    } 

  	
\end{tabular}
           
 \caption{Real datasets: Precision varying  on  cores}
 \label{real-prec}
 
\end{figure*}



\begin{figure*}[h!]
  \centering
  \begin{tabular}{ c c }
	          
    \subfloat[Kosarak and Nasa datasets]{
           \includegraphics[scale=0.5]{arekn-real.eps}
           \label{arekn}
     } & 
           
    \subfloat[Q148 and Retail datasets]{
          \includegraphics[scale=0.5]{areqr-real.eps}
          \label{areqr}
     }

  	
\end{tabular}
           
 \caption{Real datasets: Average Relative Error varying  on  cores}
 \label{real-are}
 
\end{figure*}







\subsection{Synthetic datasets: error}
\label{error}
We have carried out several experiments, with the aim of analyzing the error committed by the algorithms under test. We have fixed  in all of the simulations involving the Hurwitz distribution. Indeed, for integer values of the parameter , the Hurwitz distribution becomes the Zipf one (with a shifted value of the  parameter). As usual, we report the total error, precision and ARE. 

The following experiments related to the error are characterized by the input size , the parameter  and the skew  of the distribution; for each experiment we have determined the corresponding total error,  average relative error and precision. In particular, in the first experiment we fixed  and  letting  vary from 1000 to 10,000 in steps of 1000. In the second experiment, ,  and  varies from 100,000,000 to 1,000,000,000 in steps of 100,000,000. Finally, in the third experiment we fixed ,  and  varies from 0.5 to 3.0 in steps of 0.5. Table \ref{error-experiments} recaps the experiments carried out. For each different value of ,  and  the algorithms have been run 20 times using a different seed for the pseudo-random generator associated to the distribution (using the same seeds in the corresponding executions of different algorithms). For each input distribution generated, the algorithm has been run on up to 8 cores (one core per node), and the results have been averaged for each number of cores, over all of the runs. The input elements are 32 bits unsigned integers.

We also computed for each mean the corresponding mean's 95\% confidence interval (by using the Student  distribution). Even though we have determined the total error, ARE and precision for each different value of , we only report here the results for  to save space, taking into account that the observed behavior did not change for  (and, of course, the behavior for  was identical for both algorithms since no parallel reduction actually took place).



\begin{table}
\renewcommand{\arraystretch}{1.3}
 \caption{Design of  error experiments for Zipfian and Hurwitz distributions }
      \label{error-experiments}
	\centering
	\tiny
	    \begin{tabular}{|c|c|c|c|}
    \hline
    Experiment &  &  &   \\ \hline
    1 &  500 &[1, 10] in steps of 1 & 1.5 \\ \hline
    2 &  [100 , 1000] in steps of 100 & 2 & 1.5 \\ \hline
    3 &  500 & 2 & [0.5, 3.0] in steps of 0.5 \\ \hline
    \end{tabular}
    \normalsize
    \end{table}


 
We begin with the analysis of the total error. For Experiment 1, as shown in Figure \ref{tek}, the total error committed by our algorithm for both input distributions is practically zero for every value of , whilst the total error of the algorithm by Agarwal et al. decreases when  increases but still attains a very high value even for . Regarding Experiment 2, depicted in Figure \ref{ten}, again our algorithm is affected by a total error close to zero for both input distributions independently of the value of . On the contrary, the total error of the algorithm by Agarwal et al. steadily increases with  and is already very high even for the smallest value of . In Experiment 3, for both input distributions as shown in Figure \ref{tesk}, our algorithm is affected by total error close to zero. The algorithm by Agarwal et al. on the other hand, performs well only for skew values in the set , whilst the total error explodes for values in the set , attaining its maximum value for . To recap, our algorithm outperforms the other with regard to the total error in all of the experiments.

Regarding the ARE, as shown in Figures \ref{arek}, \ref{aren} and \ref{aresk}, our algorithm clearly outperforms the other algorithm for both the input distributions, with ARE values practically equal to zero for the whole set of  and  values under test in Experiments 1 and 2. For Experiment 3, our algorithm shows an ARE value slightly greater than zero  only for ; however, it's worth noting here that  is the only case in which there are no frequent items.

Finally, we analyze the precision attained. As shown in Figures \ref{preck} and \ref{precn}, in the Experiments 1 and 2 our algorithm clearly outperforms the algorithm by Agarwal et al. for both the input distributions. We obtain precision values equal to one for the whole set of  and  values under test, whilst the Agarwal et al. algorithm's precision is always less than 0.1. For Experiment 3, depicted in Figure  \ref{precsk}, our algorithm provides excellent performances with precision equal to one for skew values in the set  .  We note here that the precision is zero for both algorithms when , which is consistent with our previous observation (when discussing the ARE values) that in this case there are no frequent items. The precision obtained using the algorithm by Agarwal et al. reaches its maximum value (less than 0.2 nevertheless) for , and then steadily decreases again. Therefore, in each of the different scenarios, the precision provided by our parallel algorithm is for all of the practical purposes identical to the precision attained by the sequential Space Saving algorithm, so that our main goal when designing the algorithm has been achieved.


\begin{figure*}[h!p]
  \centering
  \begin{tabular}{ccc}
     \subfloat[Total Error varying ]{
           \includegraphics[scale=0.5]{tek-synthetic.eps}
           \label{tek}
        } &
        
      \subfloat[Precision varying ]{
           \includegraphics[scale=0.5]{preck-synthetic.eps}
           \label{preck}
        } &
        
        
       \subfloat[ARE varying ]{
           \includegraphics[scale=0.5]{arek-synthetic.eps}
           \label{arek}
        }

        
 \end{tabular}
 
 \caption{Experiment 1, Total Error, Precision and ARE varying  on  cores} \label{exp1}
\end{figure*}


\begin{figure*}[h!p]
  \centering
  \begin{tabular}{ccc}
    
     \subfloat[Total Error varying ]{
          \includegraphics[scale=0.5]{teni-synthetic.eps}
          \label{ten}
        } &
        
         
     \subfloat[Precision varying ]{
          \includegraphics[scale=0.5]{precni-synthetic.eps}
          \label{precn}
        } &

     \subfloat[ARE varying ]{
          \includegraphics[scale=0.5]{areni-synthetic.eps}
          \label{aren}
        }
        
 \end{tabular}
 
 \caption{Experiment 2, Total Error, Precision and ARE varying  on  cores} \label{exp2}
\end{figure*}



\begin{figure*}[h!p]
  \centering
  \begin{tabular}{ccc}
            
      \subfloat[Total Error varying ]{
          \includegraphics[scale=0.5]{tesk-synthetic.eps}
          \label{tesk}
        } &
        
      \subfloat[Precision varying ]{
          \includegraphics[scale=0.5]{precsk-synthetic.eps}
          \label{precsk}
        } &
        
        
     \subfloat[ARE varying ]{
          \includegraphics[scale=0.5]{aresk-synthetic.eps}
          \label{aresk}
        }
        
 \end{tabular}
 
 \caption{Experiment 3, Total Error, Precision and ARE varying  on  cores} \label{exp3}
\end{figure*}







\subsection{Synthetic datasets: performances}

We have designed and carried out some performance experiments characterized by the following parameters: the input size ,  and the skew . For each input distribution generated, the algorithm has been run twenty times on up to 8 cores, and the results have been averaged for each number of cores, over all of the runs.  The input elements are 32 bits unsigned integers. Table \ref{experiments} reports the values actually used in each of the performance experiments.

\begin{table}
\renewcommand{\arraystretch}{1.3}
 \caption{Design of  performance experiments for Zipfian and Hurwitz distributions }
      \label{experiments}
	\centering
    \begin{tabular}{|c |  c |  c  | c | }
    \hline
    Experiment &  &  &   \\ \hline \hline
    4 &  4,000,000,000 & 2,000 & 1.5 \\ \hline
    5 &  4,000,000,000 & 3,000 & 3.0 \\ \hline
    \end{tabular}
    \end{table}

 As shown in Table \ref{experiments}, we have fixed  to 4 billions of input items and, in each experiment, we vary the values of  and . Figures \ref{exp4} and \ref{exp5}, related to Experiments 4 and 5, respectively, show the performances for both the Zipfian and Hurwitz distributions with regard to running time, speedup and efficiency. Similar results were obtained with other settings.
 
 \begin{figure*}[h!p]
  \centering
  \begin{tabular}{ c c }
     \subfloat[Zipfian]{
           \includegraphics[scale=0.6]{z11r.eps}
           \label{z11r}
        } &
        
     \subfloat[Hurwitz]{
          \includegraphics[scale=0.6]{h11r.eps}
          \label{h11r}
        } \\

        
     \subfloat[Zipfian]{

           \includegraphics[scale=0.6]{z11se.eps}
           \label{z11se}
        } &

     \subfloat[Hurwitz]{
\includegraphics[scale=0.6]{h11se.eps}
          \label{h11se}
        }

\end{tabular}

 \caption{Experiment 4: Running Time, Speedup and Efficiency} \label{exp4}
\end{figure*}



\begin{figure*}[h!p]
  \centering
  \begin{tabular}{ c c }
     \subfloat[Zipfian]{
           \includegraphics[scale=0.6]{z22r.eps}
           \label{z22r}
        } &
      
     \subfloat[Hurwitz]{
          \includegraphics[scale=0.6]{h22r.eps}
          \label{h22r}
        } \\
 
        
     \subfloat[Zipfian]{
           \includegraphics[scale=0.6]{z22se.eps}
           \label{z22se}
        } &
    
     \subfloat[Hurwitz]{
          \includegraphics[scale=0.6]{h22se.eps}
          \label{h22se}
        }


\end{tabular}

\caption{Experiment 5: Running Time, Speedup and Efficiency} \label{exp5}
\end{figure*}



It is immediate to verify that the performances of our parallel Space Saving algorithm are comparable to the performances of the algorithm by Agarwal et al. for both Zipfian and Hurwitz distributions, with regard to overall running time, speedup and efficiency. In particular, the measured speedup shows in general a linear behavior, with corresponding efficiency close to 1 (or 100\%). It is worth noting here that both algorithms exhibit, in some cases, a slightly superlinear speedup. This phenomenon, observed experimentally, is due to the cluster's memory hierarchy and to related cache effects. So-called superlinear speedups, i.e., speedups which are greater than the number of processors/cores \cite{Bader}, are a source of confusion because in theory this phenomenon is not possible according to  Brent's principle \cite{brent74} (which states that a single processor can simulate a -processor algorithm with a uniform slowdown factor of ).

Experimentally, a superlinear  speedup can be observed without violating Brent's principle when the storage space required to run the code on a particular instance exceeds the
memory available on the single-processor machine, but not that of the parallel machine used for the simulation. In such a case, the sequential code needs to swap to secondary
memory (disk) while the parallel code does not, therefore yielding a dramatic slowdown of the sequential code. On a more modest scale, the same problem could occur one level
higher in the memory hierarchy, with the sequential code constantly cache-faulting while the parallel code can keep all of the required data in its cache subsystems. A
sequential algorithm using  bytes of memory will use only  bytes on each processor of a  processor parallel system, so that it is easier to keep all of the data in
cache memory on the parallel machine. This is exactly what happened in our simulations.

We recall here that other possible sources of superlinear speedup include some brute--force search problems and the use of a suboptimal sequential algorithm. A parallel system
might exhibit such behavior in search algorithms. In search problems performed by exhaustively looking for the solution, suppose the solution space is divided among the
processors for each one to perform an independent search. In a sequential  implementation the different search spaces are attacked one after the other, while in parallel they
can be done simultaneously, and one processor may find the solution almost immediately, yelding a superlinear speedup.
A parallel system might also exhibit such behavior when using a suboptimal sequential algorithm: each processing element spends less than the time required by the sequential
algorithm divided by   solving the problem. Generally, if a purely deterministic parallel algorithm were to achieve  better than  times the speedup over the current
sequential algorithm, the parallel algorithm (by Brent's principle) could be emulated on a single processor one parallel part after another, to achieve a faster serial program,
which contradicts the assumption of an optimal serial program.


We recall here that in experiment 5 we used a skew value equal to 3.0, which corresponds to highly skewed distributions of no real practical interest. However, we did these tests anyway for completeness, to test the performances of the algorithms also in this case. Even though both algorithms under test show comparable performances for all of the practical purposes, our algorithm outperforms the one by Agarwal et al. with regard to the error committed, as shown in Section \ref{error}.




\bibliographystyle{elsarticle-num}
\bibliography{bibliography}

\end{document}
