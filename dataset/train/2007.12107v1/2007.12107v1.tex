

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{array,xcolor,colortbl}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{enumitem}

\usepackage[breaklinks=true,
colorlinks = true,
linkcolor = blue,
urlcolor  = blue,
citecolor = black,
anchorcolor = blue]{hyperref}
\usepackage{breakcites}

\newcommand{\ssp}[1]{\,{#1}\,}

\definecolor{HL}{rgb}{0.9,0.9,0.9}

\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}

\newcommand{\dataQ}{x}
\newcommand{\annot}{y}
\newcommand{\dataC}{z}
\newcommand{\DataC}{Z}
\newcommand{\clsSet}{\DataC}
\newcommand{\DataQ}{\mathcal{X}}
\newcommand{\Annot}{\mathcal{Y}}

\newcommand{\crop}{\text{crop}}

\newcommand{\QueryEncoder}{\mathcal{F}^\mathrm{qry}}
\newcommand{\ClassEncoder}{\mathcal{F}^\mathrm{cls}}
\newcommand{\Aggregator}{\mathcal{A}}
\newcommand{\Predictor}{\mathcal{P}}

\newcommand{\featQ}{\mathrm{f}^{\mathrm{qry}}}
\newcommand{\featC}{\mathrm{f}^{\mathrm{cls}}}
\newcommand{\featA}{\mathrm{f}^{\mathrm{agg}}}

\newcommand{\Loss}{\mathcal{L}}
\newcommand{\Cls}{C}
\newcommand{\Shape}{S}
\newcommand{\ClsBase}{\Cls_\text{base}}
\newcommand{\ClsNovel}{\Cls_\text{novel}}
\newcommand{\Obj}{\mathsf{Obj}}

\newcommand{\predImg}{\mathsf{img}}
\newcommand{\predCls}{\mathsf{cls}}
\newcommand{\predBox}{\mathsf{box}}
\newcommand{\predMask}{\mathsf{mask}}
\newcommand{\predAng}{\mathsf{ang}}
\newcommand{\predAzi}{\mathsf{azi}}
\newcommand{\predEle}{\mathsf{ele}}
\newcommand{\predInp}{\mathsf{inp}}







\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{2712}  

\title{Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild}

\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Few-Shot Object Detection and Viewpoint Estimation}
\author{Yang Xiao\inst{1}
\and
Renaud Marlet\inst{1,2}
}
\authorrunning{Y. Xiao and R. Marlet}
\institute{LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-VallÃ©e, France \and valeo.ai, Paris, France
}
\maketitle


\begin{abstract}

Detecting objects and estimating their viewpoint in images are key tasks of 3D scene understanding. Recent approaches have achieved excellent results on very large benchmarks for object detection and viewpoint estimation. However, performances are still lagging behind for novel object categories with few samples. 
In this paper, we tackle the problems of few-shot object detection and few-shot viewpoint estimation. We propose a meta-learning framework that can be applied to both tasks, possibly including 3D data. 
Our models improve the results on objects of novel classes by leveraging on rich feature information originating from base classes with many samples.  
A simple joint feature embedding module is proposed to make the most of this feature sharing. 
Despite its simplicity, our method outperforms state-of-the-art methods by a large margin on a range of datasets, including PASCAL VOC and MS COCO for few-shot object detection, and Pascal3D+ and ObjectNet3D for few-shot viewpoint estimation. And for the first time, we tackle the combination of both few-shot tasks, on ObjectNet3D, showing promising results.
Our code and data are available at \url{http://imagine.enpc.fr/~xiaoy/FSDetView/}.





\keywords{Few-shot learning, Meta learning, Object detection, Viewpoint estimation.}
\end{abstract}


\begin{figure}[t]
    \centering
    \includegraphics[width=1.\linewidth]{fig/Teaser.png}
    \caption{\textbf{Few-shot object detection and viewpoint estimation.} 
    Starting with images labeled with bounding boxes and viewpoints of objects from base classes, and given only a few similarly labeled images for new categories (top), we predict in a query image the 2D location of objects of new categories, as well as their 3D poses, leveraging on just a few arbitrary 3D class models (bottom).}
    \label{fig:teaser}
\end{figure}


\section{Introduction}

Detecting objects in 2D images and estimate their 3D pose, as shown in Fig.\,\ref{fig:teaser}, is extremely useful for tasks such as 3D scene understanding, augmented reality and robot manipulation. With the emergence of large databases annotated with object bounding boxes and viewpoints, deep-learning-based methods have achieved very good results on both tasks. 
However these methods, that rely on rich labeled data, usually fail to generalize to \emph{novel} object categories when only a few annotated samples are available.
Transferring the knowledge learned from large base categories with abundant annotated images to novel categories with scarce annotated samples is a \emph{few-shot learning} problem.


To address few-shot detection, some approaches simultaneously tackle few-shot classification and few-shot localization by disentangling the learning of category-agnostic and category-specific network parameters~\cite{MetaDet2019}. 
Others attach a reweighting module to existing object detection networks~\cite{YOLO-FS2019,metarcnn2019}.
Though these methods have made significant progress, current few-shot detection evaluation protocols suffer from statistical unreliability and the prediction depends heavily on the choice of support data, which makes direct comparison difficult~\cite{wang2020few}.

In parallel to the endeavours made in few-shot object detection, recent work proposes to perform category-agnostic viewpoint estimation that can be directly applied to novel object categories without retraining~\cite{starmap2018,Xiao2019PoseFromShape}.
However, these methods either require the testing categories to be similar to the training ones~\cite{starmap2018}, or assume the exact CAD model to be provided for each object during inference~\cite{Xiao2019PoseFromShape}.
Differently, the meta-learning-based method MetaView~\cite{Tseng2019FewShotVE} introduces the category-level few-shot viewpoint estimation problem and addresses it by learning to estimate category-specific keypoints, requiring extra annotations. 
In any case, precisely annotating the 3D pose of objects in images is far more tedious than annotating their 2D bounding boxes, which makes few-shot viewpoint estimation a non-trivial yet largely under-explored problem.

In this work, we propose a consistent framework to tackle both problems of few-shot object detection and few-shot viewpoint estimation. 
For this, we exploit, in a meta-learning setting, task-specific class information present in existing datasets, i.e., images with bounding boxes for object detection and, for viewpoint estimation, 3D poses in images as well as a few 3D models for the different classes.  
Considering that these few 3D shapes are available is a realistic assumption in most scenarios.
Using this information, we obtain an embedding for each class and condition the network prediction on both the class-informative embeddings and instance-wise query image embeddings through a feature aggregation module.
Despite its simplicity, this approach leads to a significant performance improvement on novel classes under the few-shot learning regime.

Additionally, by combining our few-shot object detection with our few-shot viewpoint estimation, we address the realistic joint problem of learning to detect objects in images and to estimate their viewpoints from only a few shots.
Indeed, compared to other viewpoint estimation methods, that only evaluate in the ideal case with ground-truth (GT) classes and ground-truth bounding boxes, we demonstrate that our few-shot viewpoint estimation method can achieve very good results even based on the predicted classes and bounding boxes.

To summarize, our contributions are:
\begin{itemize}[nosep]
    \item We define a simple yet effective unifying framework that tackles both few-shot object detection and few-shot viewpoint estimation.
    \item We show how to leverage just a few arbitrary 3D models of novel classes to guide and boost few-shot viewpoint estimation.
    \item Our approach achieves state-of-the-art performance on various benchmarks.
    \item We propose a few-shot learning evaluation of the new joint task of object detection and view-point estimation, and provide promising results.
\end{itemize}



\section{Related work}

Since there is a vast amount of literature on both object detection and viewpoint estimation, we focus here on recent works that target these tasks in the case of limited annotated samples.

\subsubsection{Few-shot Learning.}
Few-shot learning refers to learning from a few labeled training samples per class, which is an important yet unsolved problem in computer vision~\cite{Li2006OneshotLO,Hariharan2016LowShotVR,Vinyals2016MatchingNF}. One popular solution to this problem is meta-learning~\cite{Koch2015SiameseNN,Bertinetto2016LearningFO,Andrychowicz16,Wang2016LearningTL,Vinyals2016MatchingNF,Snell2017PrototypicalNF,Hu2017RelationNF,Ravi2017OptimizationAA,HyperNetworks45823,lee2019meta,Hu2020Empirical}, where a meta-learner is designed to parameterize the optimization algorithm or predict the network parameters by "learning to learn".
Instead of just focusing on the performance improvement on novel classes, some other work has been proposed for providing good results on both base and novel classes~\cite{Hariharan2016LowShotVR,Gidaris2018DynamicFV,Qi2017LowShotLW}.
While most existing methods tackle the problem of few-shot image classification, we find that other few-shot learning tasks such as object detection and viewpoint estimation are under-explored.

\subsubsection{Object Detection.}
The general deep-learning models for object detection can be divided into two groups: proposal-based methods and direct methods without proposals.
While the R-CNN series~\cite{14rcnn,He2014SpatialPP,girshickICCV15fastrcnn,renNIPS15fasterrcnn,He2017MaskR} and FPN~\cite{Lin2016FeaturePN} fall into the former line of work, the YOLO series~\cite{Redmon2015YouOL,Redmon2016YOLO9000BF,Redmon2018YOLOv3AI} and SSD~\cite{Liu2016SSDSS} belong to the latter.
All these methods mainly focus on learning from abundant data to improve detection regarding accuracy and speed. Yet, there are also some attempts to solve the problem with limited labeled data.
Chen~\etal~\cite{LSTD2018} proposes to transfer a pre-trained detector to the few-shot task, while Karlinsky~\etal~\cite{Schwartz2018RepMetRM} exploits distance metric learning to model a multi-modal distribution of each object class.

More recently, Wang \etal~\cite{MetaDet2019} propose specialized meta-strategies to disentangle the learning of category-agnostic and category-specific components in a detection model.
Other approaches based on meta-learning learn a class-attentive vector for each class and use these vectors to reweight full-image features~\cite{YOLO-FS2019} or region-of-interest (RoI) features~\cite{metarcnn2019}.
Object detection with limited labeled samples is also addressed by approaches targeting weak supervision~\cite{Song2014WeaklysupervisedDO,Bilen2015WeaklySD,Diba2016WeaklySC,shen2019discovery} and zero-shot learning~\cite{Bansal2018ZeroShotOD,Rahman2018ZeroShotOD,Zhu2019ZeroShot}, but these settings are different from ours.


\subsubsection{Viewpoint Estimation.}
Deep-learning methods for viewpoint estimation follow roughly three different paths: direct estimation of Euler angles~\cite{ViewpointsKeypoints2015,Su2015RenderFC,Mousavian20163DBB,Kehl2017SSD6DMR,xiang2018posecnn,Xiao2019PoseFromShape}, template-based matching~\cite{Hinterstoier2012ModelBT,Massa2015DeepE2,Sundermeyer2018Implicit3O}, and keypoint detection relying on 3D bounding box corners~\cite{Rad2017BB8AS,Tekin2017RealTimeSS,Grabner20183DPE,Oberweger2018MakingDH,Pitteri2019CorNetG3} or semantic keypoints~\cite{Pavlakos20176DoFOP,starmap2018}.

Most of the existing viewpoint estimation methods are designed for known object categories or instances; very little work reports performance on unseen classes~\cite{Tulsiani2015PoseIF,starmap2018,Pitteri2019CorNetG3,Tseng2019FewShotVE,Xiao2019PoseFromShape}. Zhou et al.~\cite{starmap2018} propose a category-agnostic method to learn general keypoints for both seen and unseen objects, while Xiao et al.~\cite{Xiao2019PoseFromShape} show that better results can be obtained when exact 3D models of the objects are additionally provided.
In contrast to these category-agnostic methods, Tseng et al.~\cite{Tseng2019FewShotVE} specifically address the few-shot scenario by training a category-specific viewpoint estimation network for novel classes with limited samples.

Instead of using exact 3D object models as~\cite{Xiao2019PoseFromShape}, we propose a meta-learning approach to extract a class-informative canonical shape feature vector for each novel class from a few labeled samples, with random object models.
Besides, our network can be applied to both base and novel classes without changing the network architecture, while~\cite{Tseng2019FewShotVE} requires a separate meta-training procedure for each class and needs keypoint annotations in addition to the viewpoint.



\section{Approach}

In this section, we first introduce the setup for few-shot object detection and few-shot viewpoint estimation (Sect.\,\ref{sec:fsSetup}).
Then we describe our common network architecture for these two tasks (Sect.\,\ref{sec:network}) and the learning procedure (Sect.\,\ref{sec:LearningProc}).


\subsection{Few-shot Learning Setup}
\label{sec:fsSetup}

We have training samples  for our two tasks, and a few 3D shapes.
\begin{itemize}[topsep=1pt,itemsep=1pt]
    \item For object detection,  is an image,  indicates the class label  and bounding box  of each object~ in the image. \item For viewpoint estimation,  represents an object of class  pictured in bounding box  of an image ,  is the 3D pose (viewpoint) of the object, given by Euler angles. \end{itemize}
For each class , we consider a set  of \emph{class data} (see Fig.\,\ref{fig:ClassData}) to learn from using meta-learning:
\begin{itemize}[topsep=1pt,itemsep=1pt]
    \item For object detection,  is made of images  plus an extra channel with a binary mask for bounding box  of . \item For viewpoint estimation,  is an additional set of 3D models of class .
\end{itemize}    
At each training iteration, class data  is randomly sampled in  for each . 


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{fig/ClassDataSamples.png} \\
\caption{Example of class data for object detection (left) \& viewpoint estimation (right).}
\label{fig:ClassData}
\end{figure}



In the few-shot setting, we have a partition of the classes  with many samples for base classes in  and only a few samples (including shapes) for novel classes in .  The goal is to transfer the knowledge learned on base classes with abundant samples to little-represented novel classes.



\begin{figure}[t]
    \centering
    \includegraphics[width=1.\linewidth]{fig/PipelineDet.png} \\
    (a) {\bf Few-shot object detection.} \\ 
\text{\;}
\includegraphics[width=1.\linewidth]{fig/PipelineView.png} \\
    (b) {\bf Few-shot viewpoint estimation.} \\
    \caption{
    \textbf{Method overview.}\newline
    \hspace*{3mm}(a)~For object detection, we sample for each class~ one image  in the training set containing an object  of class~, to which we add an extra channel for the binary mask  of the ground-truth bounding box  of object . Each corresponding vector of class features  (red) is then combined with each vector of query features  (blue) associated to one of the region of interest  in the query image, via an aggregation module. Finally, the aggregated features  pass through a predictor that estimates a class probability  and regresses a bounding box .
\newline\hspace*{3mm}(b) For few-shot viewpoint estimation, class information is extracted from a few point clouds with coordinates in normalized object canonical space, and the output of the network is the 3D pose represented by three Euler angles.
    }
    \label{fig:overview}
\end{figure}



\subsection{Network Description}
\label{sec:network}


Our general approach has three steps that are visualized in Fig~\ref{fig:overview}.
First, query data  and class-informative data  pass respectively through the query encoder  and the class encoder  to generate corresponding feature vectors.
Next, a feature aggregation module  combines the query features with the class features.
Finally, the output of the network is obtained by passing the aggregated features through a task-specific predictor :
\begin{itemize}[topsep=1pt,itemsep=1pt]
    \item For object detection, the predictor estimates a classification score and an object location for each region of interest (RoI) and each class.
    \item For viewpoint estimation, the predictor selects quantized angles by classification, that are refined using regressed angular offsets.
\end{itemize}



\subsubsection{Few-shot object detection.}
We adopt the widely-used Faster R-CNN~\cite{renNIPS15fasterrcnn} approach in our few-shot object detection network (see Fig.\,\ref{fig:overview}(a)).
The query encoder  includes the backbone, the region proposal network (RPN) and the proposal-level feature alignment module. 
In parallel, the class encoder  is here simply the backbone sharing the same weights as , that extracts the class features from RGB images sampled in each class, with an extra channel for a binary mask of the object bounding box~\cite{YOLO-FS2019,metarcnn2019}.
Each extracted vector of query features is aggregated with each extracted vector of class features before being processed for class classification and bounding box regression:

where  is the set of all training classes, and where  and  are the predicted classification scores and object locations for the  RoI in query image  and for class .
The prediction branch in Faster R-CNN is class-specific: the network outputs  classification scores and  box regressions for each RoI. The final predictions are obtained by concatenating all the class-wise network outputs.



\subsubsection{Few-shot viewpoint estimation.}
For few-shot viewpoint estimation, we rely on the recently proposed PoseFromShape~\cite{Xiao2019PoseFromShape} architecture to implement our network.
To create class data , we transform the 3D models in the dataset into point clouds by uniformly sampling points on the surface, with coordinates in the normalized object canonical space. The query encoder  and class encoder  (cf.\ Fig.\,\ref{fig:overview}(b)) correspond respectively to the image encoder ResNet-18~\cite{He2015ResNet} and shape encoder PointNet~\cite{Qi2016PointNetDL} in PoseFromShape.
By aggregating the query features and class features, we estimate the three Euler angles using a three-layer fully-connected (FC) sub-network as the predictor:

where  indicates that the query features are extracted from the image patch after cropping the object.
Unlike the object detection making a prediction for each class and aggregating them together to obtain the final outputs, here we only make the viewpoint prediction for the object class  by passing the corresponding class data through the network.
We also use the mixed classification-and-regression viewpoint estimator of~\cite{Xiao2019PoseFromShape}: the output consists of angular bin classification scores and within-bin offsets for three Euler angles: azimuth (), elevation (), and in-plane rotation ().


\subsubsection{Feature aggregation.}
In recent few-shot object detection methods such as MetaYOLO~\cite{YOLO-FS2019} and Meta R-CNN~\cite{metarcnn2019}, feature are aggregated by reweighting the query features  according to the output  of the class encoder :

where  represents channel-wise multiplication and  has the same number of channels as . 
By jointly training the query encoder  and the class encoder  with this reweighting module, it is possible to learn to generate meaningful reweighting vectors~. ( and  actually share their weights, except the first layer~\cite{metarcnn2019}.) 

We choose to rely on a slightly more complex aggregation scheme. The fact is that feature subtraction is a different but also effective way to measure similarity between image features~\cite{ammiratoTDID18,Kuo2019ShapeMaskLT}. The image embedding  itself, without any reweighting, contains relevant information too. Our aggregation thus concatenates the three forms of the query feature: 

where  represents channel-wise concatenation. The last part of the aggregated features in Eq.\,\eqref{eq:featAgg} is independent of the class data. 
As observed experimentally (Sect.\,\ref{sec:ExpDet}), this partial disentanglement does not only improve few-shot detection performance, it also reduces the variation introduced by the randomness of support samples.



\subsection{Learning Procedure}
\label{sec:LearningProc}

The learning consists of two phases: \emph{base-class training} on many samples from base classes (), followed by \emph{few-shot fine-tuning} on a balanced small set of samples from both base and novel classes ().
In both phases, we optimize the network using the same loss function. 


\subsubsection{Detection loss function.}
Following Meta R-CNN~\cite{metarcnn2019}, we optimize our few-shot object detection network using the same loss function:

where  is applied to the output of the RPN to distinguish foreground from background and refine the proposals,  is a cross-entropy loss for box classifier,  is a smoothed-L1 loss for box regression, and  is a cross-entropy loss encouraging class features to be diverse for different classes~\cite{metarcnn2019}.

\subsubsection{Viewpoint loss function.}
For the task of viewpoint estimation, we discretize each Euler angle with a bin size of 15 degrees and use the same loss function as PoseFromShape~\cite{Xiao2019PoseFromShape} to train the network:

where  is a cross-entropy loss for angle bin classification of Euler angle , and  is a smoothed-L1 loss for the regression of offsets relatively to bin centers.
Here we remove the meta loss  used in object detection since we want the network to learn useful inter-class similarities for viewpoint estimation, instead of the inter-class differences for box classification in object detection.


\subsubsection{Class data construction.}
For viewpoint estimation, we make use of all the 3D models available for each class (typically less than 10) during both training stages.
By contrast, the class data used in object detection requires the label of object class and location, which is limited by the number of annotated samples for novel classes.
Therefore, we use large number of class data for base classes in the base training stage (typically , as in Meta R-CNN~\cite{metarcnn2019}) and limit its size to the number of shots for both base and novel classes in the -shot fine-tuning stage ().

For inference, after learning is finished, we construct once and for all class features, instead of randomly sampling class data from the dataset, as done during training. For each class~, we average all corresponding class features used in the few-shot fine-tuning stage:

This corresponds to the offline computation of all red feature vectors in Fig.\,\ref{fig:overview}(a).



\section{Experiments}

In this section, we evaluate our approach and compare it with state-of-the-art methods on various benchmarks for few-shot object detection and few-shot view-point estimation. 
For a fair comparison, we use the same splits between base and novel classes~\cite{YOLO-FS2019,Tseng2019FewShotVE}.
For all the experiments, we run 10 trials with random support data and report the average performance.


\subsection{Few-Shot Object Detection}
\label{sec:ExpDet}

We adopt a well-established evaluation protocol for few-shot object detection \cite{YOLO-FS2019,MetaDet2019,metarcnn2019} and report performance on PASCAL VOC~\cite{PascalVOC10,PascalVOC15} and MS-COCO~\cite{Lin2014MicrosoftCOCO}.


\begin{table}[t]
	\addtolength{\tabcolsep}{4pt}
	\begin{center}
	\caption{{\bf Few-shot object detection evaluation on PASCAL VOC.} 
	We report the mAP with IoU threshold 0.5 (AP50) under 3 different splits for 5 novel classes with a small number of shots.
	*Results averaged over multiple random runs.
	}
	\label{tab:DetVOC}
    \vspace{-2mm}
	\scalebox{0.64}{
	\begin{tabular}{l | ccccc | ccccc | ccccc}
	\toprule
	& \multicolumn{5}{c|}{Novel Set 1} & \multicolumn{5}{c|}{Novel Set 2} & \multicolumn{5}{c}{Novel Set 3} \\
	Method \textbackslash\ Shots & 1 & 2 & 3 & 5 & 10 & 1 & 2 & 3 & 5 & 10 & 1 & 2 & 3 & 5 & 10 \\
	\midrule
	LSTD~\cite{LSTD2018} & 8.2  & 1.0  & 12.4 & 29.1 & 38.5 & 11.4 & 3.8  & 5.0  & 15.7 & 31.0 & 12.6 & 8.5  & 15.0 & 27.3 & 36.3 \\
	MetaYOLO~\cite{YOLO-FS2019} & 14.8 & 15.5 & 26.7 & 33.9 & 47.2 & 15.7 & 15.2 & 22.7 & 30.1 & 40.5 & {\bf 21.3} & 25.6 & 28.4 & 42.8 & 45.9 \\
	MetaDet*~\cite{MetaDet2019} & 18.9 & 20.6 & 30.2 & 36.8 & 49.6 & {\bf 21.8} & 23.1 & 27.8 & 31.7 & 43.0 & 20.6 & 23.9 & 29.4 & 43.9  & 44.1 \\
	Meta R-CNN*~\cite{metarcnn2019} & 19.9 & 25.5 & 35.0 & 45.7 & 51.5 & 10.4 & 19.4 & 29.6 & 34.8 & 45.4 & 14.3 & 18.2 & 27.5 & 41.2 & 48.1 \\
	TFA* w/fc~\cite{wang2020few} & 22.9 & 34.5 & 40.4 & 46.7 & 52.0 & 16.9 & 26.4 & 30.5 & 34.6 & 39.7 & 15.7 & 27.2 & 34.7 & 40.8 & 44.6 \\
	TFA* w/cos~\cite{wang2020few} & {\bf 25.3} & {\bf 36.4} & 42.1 & 47.9 & 52.8 & 18.3 & {\bf 27.5} & {30.9} & 34.1 & 39.5 & 17.9 & 27.2 & 34.3 & 40.8 & 45.6 \\
	\rowcolor{HL}
	Ours* & {24.2} & {35.3} & {\bf 42.2} & {\bf 49.1} & {\bf 57.4} & {21.6} & {24.6} & {\bf 31.9} & {\bf 37.0} & {\bf 45.7} & {21.2} & {\bf 30.0} & {\bf 37.2} & {\bf 43.8} & {\bf 49.6} \\
	\bottomrule
	\end{tabular}}
\newline \newline
\addtolength{\tabcolsep}{2pt}
	\caption{{\bf Few-shot object detection evaluation on MS-COCO.}
	We report the mean Averaged Precision and mean Averaged Recall on the 20 novel classes of COCO.
	*Results averaged over multiple random runs.}
	\label{tab:DetCOCO}
    \vspace{-2mm}
    \scalebox{0.64}{
	\begin{tabular}{c l |ccc ccc |ccc ccc}
	\toprule
	& & \multicolumn{6}{c|}{Average Precision} & \multicolumn{6}{c}{Average Recall} \\
	Shots & Method & 0.5:0.95 & 0.5 & 0.75 & S & M & L & 1 & 10 & 100 & S & M & L \\
	\midrule
	\multirow{8}{*}{10} & LSTD~\cite{LSTD2018} & 3.2 & 8.1 & 2.1 & 0.9 & 2.0 & 6.5 & 7.8 & 10.4 & 10.4 & 1.1 & 5.6 & 19.6 \\
	& MetaYOLO~\cite{YOLO-FS2019} & 5.6 & 12.3 & 4.6 & 0.9 & 3.5 & 10.5 & 10.1 & 14.3 & 14.4 & 1.5 & 8.4 & 28.2 \\
	& MetaDet*~\cite{MetaDet2019} & 7.1 & 14.6 & 6.1 & 1.0 & 4.1 & 12.2 & 11.9 & 15.1 & 15.5 & 1.7 & 9.7 & 30.1 \\
	& Meta R-CNN*~\cite{metarcnn2019} & 8.7 & 19.1 & 6.6 & 2.3 & 7.7 & 14.0 & 12.6 & 17.8 & 17.9 & {\bf 7.8} & 15.6 & 27.2 \\
	& TFA* w/fc~\cite{wang2020few} & 9.1 & 17.3 & 8.5 & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
	& TFA* w/cos~\cite{wang2020few} & 9.1 & 17.1 & 8.8 & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
	\rowcolor{HL}
	& Ours* & {\bf 12.5} & {\bf 27.3} & {\bf 9.8} & {\bf 2.5} & {\bf 13.8} & {\bf 19.9} & {\bf 20.0} & {\bf 25.5} & {\bf 25.7} & {7.5} & {\bf 27.6} & {\bf 38.9} \\
	\midrule
	
	\multirow{8}{*}{30} & LSTD~\cite{LSTD2018} & 6.7 & 15.8 & 5.1 & 0.4 & 2.9 & 12.3 & 10.9 & 14.3 & 14.3 & 0.9 & 7.1 & 27.0 \\
	& MetaYOLO~\cite{YOLO-FS2019} & 9.1 & 19.0 & 7.6 & 0.8 & 4.9 & 16.8 & 13.2 & 17.7 & 17.8 & 1.5 & 10.4 & 33.5 \\
	& MetaDet*~\cite{MetaDet2019} & 11.3 & 21.7 & 8.1 & 1.1 & 6.2 & 17.3 & 14.5 & 18.9 & 19.2 & 1.8 & 11.1 & 34.4 \\
	& Meta R-CNN*~\cite{metarcnn2019} & 12.4 & 25.3 & 10.8 & 2.8 & 11.6 & 19.0 & 15.0 & 21.4 & 21.7 & {\bf 8.6} & 20.0 & 32.1 \\
	& TFA* w/fc~\cite{wang2020few} & 12.0 & 22.2 & 11.8 & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
	& TFA* w/cos~\cite{wang2020few} & 12.1 & 22.0 & 12.0 & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
	\rowcolor{HL}
	& Ours* & {\bf 14.7} & {\bf 30.6} & {\bf 12.2} & {\bf 3.2} & {\bf 15.2} & {\bf 23.8} & {\bf 22.0} & {\bf 28.2} & {\bf 28.4} & {8.3} & {\bf 30.3} & {\bf 42.1} \\
	\bottomrule
	\end{tabular}}
	\end{center}
\end{table}


\subsubsection{Experimental setup.}


PASCAL VOC 2007 and 2012 consist of 16.5k train-val images and 5k test images covering 20 categories.
Consistent with the few-shot learning setup in~\cite{YOLO-FS2019,MetaDet2019,metarcnn2019}, we use VOC 07 and 12 train-val sets for training and VOC 07 test set for testing. 15 classes are considered as base classes, and the remaining 5 classes as novel classes. For a fair comparison, we consider the same 3 splits as in \cite{YOLO-FS2019,MetaDet2019,metarcnn2019,wang2020few}, and for each run we only draw  random shots from each novel class where .
We report the mean Average Precision (mAP) with intersection over union (IoU) threshold at 0.5 (AP50).
For MS-COCO, we set the 20 PASCAL VOC categories as novel classes and the remaining 60 categories as base classes.
Following~\cite{Liu2016SSDSS,renNIPS15fasterrcnn}, we report standard COCO evaluation metrics on this dataset with .


\subsubsection{Training details.}
We use the same learning scheme as~\cite{metarcnn2019}, which uses the SGD optimizer with an initial learning rate of  and a batch size of 4. 
In the first training stage, we train for 20 epochs and divide the learning rate by 10 after each 5 epochs. In the second stage, we train for 5 epochs with learning rate of  and another 4 epochs with a learning rate of .


\subsubsection{Quantitative results.}
The results are summarized in Table~\ref{tab:DetVOC} and~\ref{tab:DetCOCO}.
Our method outperforms state-of-the-art methods in most cases for the 3 different dataset splits of PASCAL VOC, and it achieves the best results on the 20 novel classes of MS-COCO, which validates the efficacy and generality of our approach.
Moreover, our improvements on the difficult COCO dataset (around 3 points in mAP) is much larger than the gap among previous methods. This demonstrates that our approach can generalize well to novel classes even in complex scenarios with ambiguities and occluded objects.
By comparing results on objects of different sizes contained in COCO, we find that our approach obtains a much better improvement on medium and large objects while it struggles on small objects. 


\begin{table}[t]
\addtolength{\tabcolsep}{10pt}
    \centering
    \caption{{\bf Ablation study on the feature aggregation scheme.}
    Using the same class splits of PASCAL VOC as in Table~\ref{tab:DetVOC}, we measure the performance of few-shot object detection on the novel classes. We report the average and standard deviation of the AP50 metric over 10 runs.  is the query features and  is the class features.}
    \vspace{-2mm}
    \label{tab:DetAblation}
    \scalebox{0.65}{
    \begin{tabular}{l  c c c c c c}
    \toprule
    & \multicolumn{2}{c}{Novel Set 1} & \multicolumn{2}{c}{Novel Set 2} & \multicolumn{2}{c}{Novel Set 3}\\
    Method  \ Shots & 3 & 10 & 3 & 10 & 3 & 10 \\
    \midrule
     &  &  &  &  &  &  \\
    
     &  &  &  &  &  &  \\
    
     &  &  &  &  &  &  \\
    
     &  &  &  &  &  &  \\
    
     &  &  &  &  &  &  \\
    \bottomrule
    \end{tabular}}
\end{table}

\subsubsection{Different feature aggregations.}
We analyze the impact of different feature aggregation schemes. 
For this purpose, we evaluate -shot object detection on PASCAL VOC with . 
The results are reported in Table~\ref{tab:DetAblation}.
We can see that our feature aggregation scheme  yields the best precision. 
In particular, although the difference  could in theory be learned from the individual feature vectors , the network performs better when explicitly provided with their subtraction.
Moreover, our aggregation scheme significantly reduces the variance introduced by the random sampling of few-shot support data, which is one of the main issues in few-shot learning.  



\subsection{Few-Shot Viewpoint Estimation}
\label{sec:ExpView}

Following the few-shot viewpoint estimation protocol proposed in~\cite{Tseng2019FewShotVE}, we evaluate our method under two settings: \emph{intra}-dataset on ObjectNet3D~\cite{objectnet3d16} (reported in Tab.\,\ref{tab:ViewIntra}) and \emph{inter}-dataset between ObjectNet3D and Pascal3D+~\cite{pascal3d14} (reported in Tab.\,\ref{tab:ViewInter}). 
In both datasets, the number of available 3D models for each class vary from 2 to 16.
We use the most common metrics for evaluation: Acc30, which is the percentage of estimations with a rotational error smaller than , and MedErr, which computes the median rotational error measured in degrees. 
Complying with previous work~\cite{starmap2018,Tseng2019FewShotVE}, we only use the non-occluded and non-truncated objects for evaluation and assume in this subsection that the ground truth classes and bounding boxes are provided at test time.

\begin{table}[t]
\addtolength{\tabcolsep}{6pt}
\centering
	\caption{{\bf Intra-dataset 10-shot viewpoint estimation evaluation.} 
	We report Acc30() / MedErr() on the same 20 novel classes of ObjectNet3D for each method, while 80 are used as base classes. All models are trained and evaluated on ObjectNet3D.}
	\vspace{-2mm}
	\label{tab:ViewIntra}
	\scalebox{0.65}{
	\begin{tabular}{l cccccc c}
	\toprule
	Method & bed & bookshelf & calculator & cellphone & computer & door & f\_cabinet \\ \midrule
	StarMap+F~\cite{starmap2018} & 0.32 / 47.2 & 0.61 / 21.0 & 0.26 / 50.6 & 0.56 / 26.8 & 0.59 / 24.4 & - / - & 0.76 / 17.1 \\
	StarMap+M~\cite{starmap2018} & 0.32 / 42.2 & 0.76 / 15.7 & 0.58 / 26.8 & 0.59 / 22.2 & 0.69 / 19.2 & - / - & 0.76 / 15.5 \\
	MetaView~\cite{Tseng2019FewShotVE} & 0.36 / 37.5 & 0.76 / 17.2 & {\bf 0.92} / 12.3 & 0.58 / 25.1 & 0.70 / 22.2 & - / - & 0.66 / 22.9 \\
	
	Ours & {\bf 0.64} / {\bf 14.7} & {\bf 0.89} / {\bf 8.3} & {0.90} / {\bf 8.3} & {\bf 0.63} / {\bf 12.7} & {\bf 0.84} / {\bf 10.5} & {\bf 0.90} / {\bf 0.9} & {\bf 0.84} / {\bf 10.5} \\ \midrule \midrule
	
	Method & guitar & iron & knife & microwave & pen & pot & rifle \\
	\midrule
	StarMap+F~\cite{starmap2018} & 0.54 / 27.9 & 0.00 / 128 & 0.05 / 120 & 0.82 / 19.0 & - / - & 0.51 / 29.9 & 0.02 / 100 \\
	StarMap+M~\cite{starmap2018} & 0.59 / 21.5 & 0.00 / 136 & 0.08 / 117 & 0.82 / 17.3 & - / - & 0.51 / 28.2 & 0.01 / 100 \\
	MetaView~\cite{Tseng2019FewShotVE} & {0.63} / {24.0} & 0.20 / {76.9} & ~0.05 / \textbf{97.9} & 0.77 / 17.9 & - / - & 0.49 / 31.6 & ~0.21 / {\bf 80.9} \\
	
	Ours & {\bf 0.72} / {\bf 17.1} & {\bf 0.37} / {\bf 57.7} & {\bf 0.26} / {139} & {\bf 0.94} / {\bf 7.3} & {\bf 0.45} / {\bf 44.0} & {\bf 0.74} / {\bf 12.3} & {\bf 0.29} / {88.4} \\ \midrule \midrule
	
	Method & shoe & slipper & stove & toilet & tub & wheelchair & \cellcolor{HL} TOTAL \\
	\midrule
	StarMap+F~\cite{starmap2018} & - / - & 0.08 / 128 & 0.80 / 16.1 & 0.38 / 36.8 & 0.35 / 39.8 & 0.18 / 80.4 & \cellcolor{HL}0.41 / 41.0 \\
	StarMap+M~\cite{starmap2018} & - / - & 0.15 / 128 & 0.83 / 15.6 & 0.39 / 35.5 & 0.41 / 38.5 & 0.24 / 71.5 & \cellcolor{HL}0.46 / 33.9 \\
	MetaView~\cite{Tseng2019FewShotVE} & - / - & 0.07 / 115 & 0.74 / 21.7 & 0.50 / 32.0 & 0.29 / 46.5 & 0.27 / 55.8 & \cellcolor{HL}0.48 / 31.5 \\
	
	Ours & {\bf 0.51} / {\bf 29.4} & {\bf 0.25} / {\bf 96.4} & {\bf 0.92} / {\bf 9.4} & {\bf 0.69} / {\bf 17.4} & {\bf 0.66} / {\bf 15.1} & {\bf 0.36} / {\bf 64.3} & \cellcolor{HL}\textbf{0.64} / \textbf{15.6} \\ \bottomrule
	\end{tabular}}
\end{table}

\subsubsection{Training details.}
The model is trained using the Adam optimizer with a batch size of 16. During the base-class training stage, we train for 150 epochs with a learning rate of . For few-shot fine-tuning, we train for 50 epochs with learning rate of  and another 50 epochs with a learning rate of .


\subsubsection{Compared methods.}
For few-shot viewpoint estimation, we compare our method to MetaView~\cite{Tseng2019FewShotVE} and to two adaptations of StarMap~\cite{starmap2018}.
More precisely, the authors of MetaView~\cite{Tseng2019FewShotVE} re-implemented StarMap with one stage of ResNet-18 as the backbone, and trained the network with MAML~\cite{Finn2017MAML} for a fair comparison in the few-shot regime (entries StarMap+M in Tab.\,\ref{tab:ViewIntra}-\ref{tab:ViewInter}).
They also provided StarMap results by just fine-tuning it on the novel classes using the scarce labeled data (entries StarMap+F in Tab.\,\ref{tab:ViewIntra}-\ref{tab:ViewInter}).



\subsubsection{Intra-dataset evaluation.}
We follow the protocol of~\cite{Tseng2019FewShotVE,Xiao2019PoseFromShape} and split the 100 categories of ObjectNet3D into 80 base classes and 20 novel classes.
As shown in Table~\ref{tab:ViewIntra}, our model outperforms the recently proposed meta-learning-based method MetaView~\cite{Tseng2019FewShotVE} by a very large margin in overall performance:  points in Acc30 and half MedErr (from  down to ).
Besides, keypoint annotations are not available for some object categories such as door, pen and shoe in ObjectNet3D.
This limits the generalization of keypoint-based approaches~\cite{starmap2018,Tseng2019FewShotVE} as they require a set of manually labeled keypoints for network training.
By contrast, our model can be trained and evaluated on all object classes of ObjectNet3D as we only rely on the shape pose. 
More importantly, our model can be directly deployed on different classes using the same architecture, while MetaView learns a set of separate category-specific semantic keypoint detectors for each class.
This flexibility suggests that our approach is likely to exploit the similarities between different categories (e.g., bicycle and motorbike) and has more potentials for applications to robotics and augmented reality.


\begin{table}[t]
\addtolength{\tabcolsep}{6pt}
\centering
	\caption{{\bf Inter-dataset 10-shot viewpoint estimation evaluation.}
	We report Acc30() / MedErr() on the 12 novel classes of Pascal3D+, while the 88 base classes are in ObjectNet3D. All models are trained on ObjectNet3D and tested on Pascal3D+.
	}
	\vspace{-2mm}
	\label{tab:ViewInter}
	\scalebox{0.65}{
	\begin{tabular}{l ccccccc}
	\toprule
	Method & aero & bike & boat & bottle & bus & car & chair \\
	\midrule
	StarMap+F~\cite{starmap2018} & 0.03 / 102 & 0.05 / 98.8 & 0.07 / 98.9 & 0.48 / 31.9 & 0.46 / 33.0 & 0.18 / 80.8 & 0.22 / 74.6 \\
	StarMap+M~\cite{starmap2018} & 0.03 / 99.2 & 0.08 / 88.4 & 0.11 / 92.2 & 0.55 / 28.0 & 0.49 / 31.0 & 0.21 / 81.4 & 0.21 / 80.2 \\
	MetaView~\cite{Tseng2019FewShotVE} & 0.12 / 104 & 0.08 / 91.3 & 0.09 / 108 & 0.71 / 24.0 & 0.64 / 22.8 & 0.22 / 73.3 & 0.20 / 89.1 \\
	
	Ours & \textbf{0.24} / \textbf{65.0} & \textbf{0.34} / \textbf{52.4} & \textbf{0.27} / \textbf{77.3} & \textbf{0.88} / \textbf{12.6} & \textbf{0.78} / \textbf{8.2} & \textbf{0.49} / \textbf{34.0} & \textbf{0.33} / \textbf{77.4} \\ \midrule \midrule
	
	Method & table & mbike & sofa & train & tv & \multicolumn{2}{c}{\cellcolor{HL} TOTAL} \\
	\midrule
	StarMap+F~\cite{starmap2018} & 0.46 / 31.4 & 0.09 / 91.6 & 0.32 / 44.7 & 0.36 / 41.7 & 0.52 / 29.1 & \multicolumn{2}{c}{\cellcolor{HL} 0.25 / 64.7} \\
	StarMap+M~\cite{starmap2018} & 0.29 / 36.8 & 0.11 / 83.5 & 0.44 / 42.9 & 0.42 / 33.9 & 0.64 / 25.3 & \multicolumn{2}{c}{\cellcolor{HL} 0.28 / 60.5} \\
	MetaView~\cite{Tseng2019FewShotVE} & 0.39 / 36.0 & 0.14 / 74.7 & 0.29 / 46.2 & 0.61 / 23.8 & 0.58 / 26.3 & \multicolumn{2}{c}{\cellcolor{HL} 0.33 / 51.3} \\
	
	Ours & \textbf{0.60} / \textbf{21.2} & \textbf{0.41} / \textbf{45.2} & \textbf{0.58} / \textbf{21.3} & \textbf{0.71} / \textbf{12.6} & \textbf{0.78} / \textbf{19.1} & \multicolumn{2}{c}{\cellcolor{HL} \textbf{0.52} / \textbf{28.3}} \\ \bottomrule
	\end{tabular}}
\end{table}

\subsubsection{Inter-dataset evaluation.}
To further evaluate our method in a more practical scenario, we use a source dataset for base classes and another target dataset for novel (disjoint) classes.
Using the same split as MetaView~\cite{Tseng2019FewShotVE}, we use all 12 categories of Pascal3D+ as novel categories and the remaining 88 categories of ObjectNet3D as base categories.
Distinct from the previous intra-dataset experiment that focuses more on the cross-category generalization capacity, this inter-dataset setup also reveals the cross-domain generalization ability.

As shown in Tab.\,\ref{tab:ViewInter}, our approach again significantly outperforms StarMap and MetaView.
Our overall improvement in inter-dataset evaluation is even larger than in intra-dataset evaluation: we gain  points in Acc30 and again divide MedErr by about 2 (from  down to ).
This indicates that our approach, by leveraging viewpoint-relevant 3D information, not only helps the network generalize to novel classes from the same domain, but also addresses the domain shift issues when trained and evaluated on different datasets.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/FSView.png}
\vspace{-6mm}
\caption{{\bf Qualitative results of few-shot viewpoint estimation.}
We visualize results on ObjectNet3D and Pascal3D+. For each category, we show three success cases (the first six columns) and one failure case (the last two columns). CAD models are shown here only for the purpose of illustrating the estimated viewpoint. 
}
\label{fig:fsView}
\end{figure}



\subsubsection{Visual results.}
We provide in Fig.\,\ref{fig:fsView} visualizations of viewpoint estimation for novel objects on ObjectNet3D and Pascal3D+.
We show both success (green boxes) and failure cases (red boxes) to help analyze possible error types.
We visualize four categories giving the largest median errors: iron, knife, rifle and slipper for ObjectNet3D, and aeroplane, bicycle, boat and chair for Pascal3D+.
The most common failure cases come from objects with similar appearances in ambiguous poses, e.g., iron and knife in ObjectNet3D, aeroplane and boat in Pascal3D+.
Other failure cases include the heavy clutter cases (bicycle) and large shape variations between training objects and testing objects (chair).


\subsection{Evaluation of Joint Detection and Viewpoint Estimation}
\label{sec:ExpDetView}

To further show the generality of our approach in real-world scenarios, we consider the joint problem of detecting objects from novel classes in images and estimating their viewpoints.  The fact is that evaluating a viewpoint estimator on ground-truth classes and bounding boxes is a toy setting, not representative of actual needs.  On the contrary, estimating viewpoints based on predicted detections is much more realistic and challenging.


To experiment with this scenario, we split ObjectNet3D into 80 base classes and 20 novel classes as in Sect.\,\ref{sec:ExpView}, and train the object detector and viewpoint estimator based on the abundant annotated samples for base classes and scarce labeled samples for novel classes.
Unfortunately, the codes of StarMap+F/M and MetaView are not available.  The only available information is the results on perfect, ground-truth classes and bounding boxes available in publications.  We thus have to reason relatively in terms of baselines.  Concretely, we compare these results obtained on ideal input to the case where we use predicted classes and bounding boxes, in the 10-shot scenario. As an upper bound, we also consider the ``all-shot'' case where all training data of the novel classes are used.


\begin{table}[t]
\addtolength{\tabcolsep}{5pt}
\begin{center}
\caption{\textbf{Evaluation of joint few-shot detection and viewpoint estimation.}
We report correct prediction percentages on novel classes of ObjectNet3D, first using the ground-truth classes and bounding boxes, then the estimated classes and boxes given by our object detector.
Predicted bounding boxes are considered correct with a IoU threshold at 0.5 and estimated viewpoints are considered correct with a rotation error less than . Ours (all-shot) is learned on all training data of the novel classes.}
\vspace{-2mm}
\label{tab:DetView}
    \scalebox{0.6}{
    \begin{tabular}{l c c c c c c c c c c c c c c c c c c c c >{\columncolor{HL}}c}
    \toprule
    Method & \rotatebox[origin=c]{90}{bed} & \rotatebox[origin=c]{90}{bshelf} & \rotatebox[origin=c]{90}{calc} & \rotatebox[origin=c]{90}{cphone} & \rotatebox[origin=c]{90}{comp} & \rotatebox[origin=c]{90}{door} & \rotatebox[origin=c]{90}{fcabin} & \rotatebox[origin=c]{90}{guit} & \rotatebox[origin=c]{90}{iron} & \rotatebox[origin=c]{90}{knife} & \rotatebox[origin=c]{90}{micro} & \rotatebox[origin=c]{90}{pen} & \rotatebox[origin=c]{90}{pot} & \rotatebox[origin=c]{90}{rifle} &
    \rotatebox[origin=c]{90}{shoe} & \rotatebox[origin=c]{90}{slipper} & \rotatebox[origin=c]{90}{stove} & \rotatebox[origin=c]{90}{toilet} & \rotatebox[origin=c]{90}{tub} & \rotatebox[origin=c]{90}{wchair} & \rotatebox[origin=c]{90}{mean} \\
    \midrule
    \multicolumn{22}{c}{\bf Evaluated using ground-truth classes and bounding boxes (viewpoint estimation)} \\
    \midrule
    StarMap+M\cite{starmap2018} & 32 & 76 & 58 & 59 & 69 & -- & 76 & 59 & 0 & 8 & 82 & -- & 51 & 1 & -- & 15 & 83 & 39 & 41 & 24 & 46 \\
    MetaView\cite{Tseng2019FewShotVE} & 36 & 76 & 92 & 58 & 70 & -- & 66 & 63 & 20 & 5 & 77 & -- & 49 & 21 & -- & 7 & 74 & 50 & 29 & 27 & 48 \\
    Ours (10-shot) & 64 & 89 & 90 & 63  & 84 & 90 & 84 & 72 & 37 & 26 & 94 & 45 & 74 & 29 & 51 & 25 & 92 & 69 & 66 & 36 & 64 \\
    Ours (all-shot) & 81 & 92 & 96 & 65 & 91 & 93 & 89 & 83 & 58 & 28 & 95 & 51 & 81 & 48 & 63 & 53 & 94 & 86 & 77 & 70 & 75 \\
    \midrule
    \multicolumn{22}{c}{\bf Evaluated using predicted classes and bounding boxes (detection + viewpoint estimation)} \\
    \midrule
    Ours (10-shot) & 55 & 76 & 74 & 52 & 57 & 69 & 63 & 70 & 44 & 8 & 57 & 22 & 55 & 12 & 6 & 19 & 80 & 65 & 56 & 21 & 48 \\
    Ours (all-shot) & 65 & 80 & 82 & 56 & 62 & 70 & 66 & 75 & 48 & 9 & 60 & 27 & 61 & 20 & 8 & 32 & 83 & 71 & 67 & 38 & 54 \\
    \bottomrule
    \end{tabular}}
    \vspace{-4mm}
\end{center}
\end{table}

As recalled in Tab.\,\ref{tab:DetView}, our few-shot viewpoint estimation outperforms other methods by a large margin when evaluated using ground-truth classes and bounding boxes in the 10-shot setting.
When using predicted classes and bounding boxes, accuracy drops for most categories.
One explanation is that viewpoint estimation becomes difficult when the objects are truncated by imperfect predicted bounding boxes, especially for tiny objects (e.g., shoes) and ambiguous objects with similar appearances in different poses (e.g., knifes, rifles).
Yet, by comparing the performance gap between our method when tested using predicted classes and boxes and MetaView when tested using ground-truth classes and boxes, we find that our approach is able to reach the same viewpoint accuracy of , which is a considerable achievement.


\section{Conclusion}

In this work, we presented an approach to few-shot object detection and viewpoint estimation that can tackle both tasks in a coherent and efficient framework.
We demonstrated the benefits of this approach in terms of accuracy, and significantly improved the state of the art on several standard benchmarks for few-shot object detection and few-shot viewpoint estimation.
Moreover, we showed that our few-shot viewpoint estimation model can achieve promising results on the novel objects detected by our few-shot detection model, compared to the existing methods tested with ground-truth bounding boxes.

\subsubsection{Acknowledgements.}
We thank Vincent Lepetit and Yuming Du for helpful discussions.


\clearpage
\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}
