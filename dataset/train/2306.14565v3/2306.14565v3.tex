\subsection{GAVIE Evaluation}
We show two full examples of the text prompt for GAVIE in (i) Fig. \ref{fig:prompt_neg31}, \ref{fig:prompt_neg32}, \ref{fig:prompt_neg33} and (ii) Fig. \ref{fig:prompt_neg41}, \ref{fig:prompt_neg42}, \ref{fig:prompt_neg43}. We first leverage the bounding boxes and dense captions as the "visual" input. We provide the human instructions and responses from different models in Fig. \ref{fig:prompt_neg32} and Fig. \ref{fig:prompt_neg42}. Furthermore, we ask GPT4 to pretend as a smart teacher and score (0-10) the answers according to the image content and instructions. There are two criteria. \textit{(1) Accuracy: whether the response is accurate concerning the image content}. \textit{(2) Relevancy: whether the response directly follows the instruction}. After that, GPT4 is required to generate a score and reason. Fig. \ref{fig:prompt_neg33}  and Fig. \ref{fig:prompt_neg43} show the full evaluation output from GAVIE.







\subsubsection{\textit{GPT4-Assisted Visual Instruction Evaluation (GAVIE)} vs. Human Evaluation}


This section provides insights into the \textit{GAVIE} via human evaluation. Here, we randomly select 40 image-instruction instances from the evaluation set. The human assessment is carried out by three experts specializing in NLP. The questionnaire consists of 40 questions randomly shuffled for each expert. The questionnaire takes about 20 minutes to complete on average. Each question includes an instruction, an image, and responses from 4 different LMMs. We provide instructions for experts as follows: 

\textit{"As for each question, there are an instruction, an image, and several answers. Suppose you are a smart teacher, please score the answers according to the two criteria. (1) Accuracy: whether the response is accurate concerning the image content. (2) Relevancy: whether the response directly follows the instruction without unrelated answers. There are four options for the scores (1) Very Poor, (2) Poor, (3) Good, (4) Excellent."}

\begin{table}[h]
\setlength\tabcolsep{3pt}
\centering
\small
\begin{tabular}{lcccccc}
\toprule[1.5pt]
Evaluator& Ours & MiniGPT4 & LLaVA & InstructBLIP & MMGPT & mPLUG-Owl\\
\midrule
\textit{Expert1(1-4)}& \textcolor{red}{3.48} & \textcolor{blue}{2.61} & \textcolor{green}{2.87} & \textcolor{orange}{3.00} & \textcolor{magenta}{1.90}&\textcolor{black}{2.90}\\
\textit{Expert2(1-4)} &\textcolor{red}{3.58}& \textcolor{green}{2.23}& \textcolor{blue}{2.07}& \textcolor{orange}{2.48}& \textcolor{magenta}{1.05}&\textcolor{black}{2.27}\\
\textit{Expert3(1-4)} & \textcolor{red}{3.33} & \textcolor{blue}{2.58} & \textcolor{green}{2.89} & \textcolor{orange}{2.94} & \textcolor{magenta}{1.38}&\textcolor{black}{2.91}\\
\midrule
\textit{GAVIE-Accuracy (0-10)} & \textcolor{red}{6.58} & \textcolor{blue}{4.14} & \textcolor{green}{4.36} & \textcolor{orange}{5.93} & \textcolor{magenta}{0.91}&\textcolor{black}{4.84}\\
\textit{GAVIE-Relevancy (0-10)}  & \textcolor{red}{8.46} & \textcolor{blue}{5.81} & \textcolor{green}{6.11} & \textcolor{orange}{7.34} & \textcolor{magenta}{1.79}&\textcolor{black}{6.35}\\
\bottomrule
\end{tabular}
\vspace{0.06in}
\caption{GAVIE vs. Human Evaluation. GAVIE scores roughly align with the expert ratings. Numbers highlighted with \textcolor{red}{red}, \textcolor{orange}{orange}, black, \textcolor{green}{green}, \textcolor{blue}{blue}, and \textcolor{magenta}{magenta} indicate rank 1 to 6.
}
\label{tab:appendix_human}
\vspace{-0.2in}
\end{table}

To evaluate the results quantitatively, we assign different scores for the options: \textit{Very Poor=1, Poor=2, Good=3, Excellent=4}. From Tab. \ref{tab:appendix_human}, all experts agree that the output from our model is the best, followed by InstructBLIP in second place, and MMGPT performs the worst. The observation is similar to that of \textit{GAVIE} evaluation results. Although the ranking orders of MiniGPT4 and LLaVA from experts are not always the same as that of \textit{GAVIE}, the scores assigned to them are fairly close. One possible reason is that the answers from MiniGPT4 and LLaVA tend to be longer, making them more challenging for humans to evaluate. 

\subsubsection{Stability of \textit{GPT4-Assisted Visual Instruction Evaluation (GAVIE)}}
This section investigates the stability of \textit{GAVIE}. Precisely, we execute GAVIE 5 times on the model predictions. We leverage two metrics to measure the stability of GAVIE on each instance: Mean and Standard Deviation (STD). The average scores of the evaluation set are shown in the following table. From the perspective of the Mean, the ranking order of ACCURACY and RELEVANCY is the same as Tab. \ref{tab:appendix_human}. As for the Standard Deviation in Tab. \ref{tab:stability1}, it ranges from 0.65 to 2.46. From our observation, the ACCURACY and RELEVANCY scores of an instance may vary between different times, but they belong to the same grade level. Specifically, RELEVANCY has four grade levels: (1) The response is completely relevant (9-10), (2) The response is mostly relevant (6-8), (3) The response is partly relevant (3-5), (4) The response is seldom relevant (0-2). ACCURACY has four grade levels: (1) The response is completely accurate (9-10), (2) The response has minor errors (6-8), (3) The response is partly accurate (3-5), (4) The response is mostly or completely wrong (0-2).

\begin{table}[h]
\setlength\tabcolsep{3pt}
\centering
\small
\begin{tabular}{lcccccc}
\toprule[1.5pt]
Metric& Ours & MiniGPT4 & InstructBLIP & MMGPT& mPLUG-Owl & LLaVA\\
\midrule
\textsc{Accuracy(GPT4)}-Mean&6.60&3.76&5.29&0.87&4.84&3.80\\
\textsc{Relevancy(GPT4)}-Mean&8.37&5.35&6.83&1.71&6.35&5.65\\
\midrule
\textsc{Accuracy(GPT4)}-STD&2.42&2.46&2.42&0.65&1.96&2.37\\
\textsc{Relevancy(GPT4)}-STD&1.30&1.99&1.88&0.81&1.48&2.18\\
\bottomrule
\end{tabular}
\vspace{0.06in}
\caption{Evaluation of the stability of \textit{GAVIE}. We run \textit{GAVIE} 5 times on the randomly selected instances from the evaluation set. \textit{Mean} and \textit{Standard Deviation(STD)} are calculated to measure the stability. The metric scores of \textsc{Accuracy(GPT4)} and \textsc{Relevancy(GPT4)} are from 0 to 10.
}
\label{tab:stability1}
\vspace{-0.2in}
\end{table}

\subsection{More Experiments}
\subsubsection{\textit{Do LMMs perform better on Positive or Negative Instructions? }}
Our evaluation set consists of positive and negative instances. We divide it into two sets and analyze the model performance on each. As shown in Fig.~\ref{fig:posneg}, baseline models, including MiniGPT4, LLaVa, and InstructBLIP,  perform better on positive instances than negative ones, as the training data adopted by these models do not contain negative instructions. MMGPT performance poorly on both sets due to many repetitive phrases in the response. In addition, we found that the degradation of LLaVA is the most severe. We hypothesize that the synthetic answers for instruction tuning in LLaVA are generally longer and involve more unrelated information. In contrast, our model performs the best in both sets. InstructBLIP performs with higher scores than other LMMs because of the effectiveness of its instruction-aware visual encoder to extract image information.





\subsubsection{\textit{Do LMMs perform better on different formats and lengths of instructions? }}

From Tab \ref{tab:Interrogative}, LMMs perform with higher scores on interrogative instructions than declarative, but the difference is relatively small. Even though recent visual instruction tuning datasets lack diverse declarative instructions, the LMMs built on LLM are powerful enough to understand and follow the declarative instructions. From Fig. \ref{fig:length}, current LMMs achieve better results in short instructions than long ones since longer instructions contain more information, making it more challenging. 

\subsection{Prompt Design}
\subsubsection{Positive Instance Generation based on Visual Genome Dataset}
We show two full examples of our input prompts in (i) Fig. \ref{fig:prompt_pos11}, \ref{fig:prompt_pos12}, \ref{fig:prompt_pos13} and (ii) Fig. \ref{fig:prompt_pos21}, \ref{fig:prompt_pos22}, \ref{fig:prompt_pos23}. In Fig. \ref{fig:prompt_pos11} and Fig. \ref{fig:prompt_pos21}, we first present the images for the two examples, but they are not included in the text prompt for GPT4. As for the text input, we leverage the groundtruth bounding boxes and dense captions to represent the visual content as if GPT4 can see the image. After that, we randomly select 10 tasks from the 16 seeds and ask GPT4 to generate 20 instances for these tasks. Additionally, there can be more than one caption describing the same object with different attributes, such as "\textit{woman wearing a long dress}" and "\textit{woman wearing a yellow dress}" in Fig. \ref{fig:prompt_pos11}. Although we present the bounding box coordinates of each caption to GPT4, it can be easily confused, treating them as two instances, one in a long dress and the other in a yellow dress. To mitigate this issue, we add "\textit{highly overlapping bounding boxes may refer to the same object}" into the prompt to help GPT4 understand the "visual" input better. To enrich the instructions, we ask GPT4 to generate instances in both declarative and interrogative formats. We also explicitly instruct GPT4 with \textit{"The answers should be less than 30 words"} as a requirement to reduce the chance of generating extra unrelated information in the training data. In order to make the output of GPT4 in a good format, we also ask GPT4 to generate an instruction, an answer, and a task name in order at the end of the prompt (Fig. \ref{fig:prompt_pos11} and Fig. \ref{fig:prompt_pos21}). The full output of instructions and answers are shown in Fig. \ref{fig:prompt_pos12}, \ref{fig:prompt_pos13} and Fig. \ref{fig:prompt_pos22}, \ref{fig:prompt_pos23}. We also present more positive instances with the output from different LMMs in Fig. \ref{fig:demo_pos1}, \ref{fig:demo_pos2}, \ref{fig:demo_pos3}.

\subsubsection{Positive Instance Generation based on Chart Images}
We collect chart images from \citep{tang2023vistext}, which has human-annotated captions describing the construction and patterns of charts. We instruct GPT-4 to generate question-answers pairs with captions as visual input. The detailed prompt is shown in Fig. \ref{fig:chart_prompt1}. We also present more positive instances with the output from different LMMs in Fig. \ref{fig:demo_chart_example1}.

\begin{figure*}[h]
    \centering
      \includegraphics[width=1.0\textwidth]{chart_prompt1.pdf}
    \caption{An example prompt for text-only GPT4 we use to generate instruction and answers for chart images. The sentence in \textcolor{blue}{BLUE} is the captions of the chart.}
    \label{fig:chart_prompt1}
    \vspace{-0.2in}
\end{figure*}






\subsubsection{Negative Instance Generation - Nonexistent/Existent Object Manipulation}
We show two full examples of our input prompts in (i) Fig. \ref{fig:prompt_neg11}, \ref{fig:prompt_neg12} and (ii) Fig. \ref{fig:prompt_neg21}, \ref{fig:prompt_neg22}. In Fig. \ref{fig:prompt_neg11} and Fig. \ref{fig:prompt_neg21}, we present the images to help readers understand dense captions better but they are not included in the text prompt for GPT4. We leverage the bounding boxes and dense captions as the "visual" input. As for  \textit{Nonexistent object Manipulation} in \ref{fig:prompt_neg11}, we ask GPT4 to generate 6 instructions with nonexistent elements (nonexistent objects, nonexistent activities, nonexistent attributes, nonexistent interactions). As for  \textit{Existent object Manipulation} in \ref{fig:prompt_neg21}, we ask GPT4 to generate 6 instructions of existing objects with wrong attributes. At the end of the text prompt, we ask GPT4 to generate an instruction and a reason to explain why the instruction is inconsistent with the image in order. The reason is regarded as the answer for the instruction in our training data. Fig. \ref{fig:prompt_neg12} and Fig. \ref{fig:prompt_neg22} show the full output from GPT4. We also present more negative instances with the output from different LMMs in Fig. \ref{fig:demo_neg1}, \ref{fig:demo_neg2}. 

\subsubsection{Negative Instance Generation - Knowledge Manipulation}
As for the \textit{Neg3: knowledge manipulation}, we use GPT4 to manipulate the knowledge in the captions, including named entities and events.

\begin{figure*}[h]
    \centering
      \includegraphics[width=1.0\textwidth]{chart_neg1.pdf}
    \caption{An example prompt for text-only GPT4 we use to generate negative instruction. The next step is to transfer the ouput into an interrogative sentence whose answer is "yes" or "no". }
    \label{fig:chart_neg1_prompt}
    \vspace{-0.2in}
\end{figure*}

As shown in Fig. \ref{fig:chart_neg1_prompt}, GPT4 manipulates the "Japan", "COVID-19" and "April" in the original captions. After that, we instruct GPT4 to transfer the output sentence into an interrogative sentence whose answer is "yes" or "no". Finally, we combine "No." and the original answer as the final answer: \textit{Question: Did the image show the cumulative influenza cases in France by region of infection from March to October 2020? Answer: No. Cumulative COVID-19 cases in Japan by place of infection from April to October 2020".}


\subsubsection{Prompt Design for Evaluating Knowledge Hallucination}
As for the knowledge level hallucination, we will use the groundtruth answers as a reference and compare them with predictions of models. A prompt example for GPT4 is shown in Fig. \ref{fig:chart_evaluation}:

\begin{figure*}[h]
    \centering
      \includegraphics[width=1.0\textwidth]{chart_evaluation.pdf}
    \caption{An example prompt for text-only GPT4 we use to evaluate knowledge manipulation instruction. The sentences in \textcolor{blue}{BLUE} are the questions, reference answers, and predictions of models.}
    \label{fig:chart_evaluation}
    \vspace{-0.2in}
\end{figure*}


.



\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{sub1.pdf}
         \vspace{-0.2in}
         \caption{Accuracy Performance.}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{sub2.pdf}
         \vspace{-0.2in}
         \caption{Relevancy Performance.}
         \label{fig:three sin x}
     \end{subfigure}
        \caption{Evaluation results on positive and negative instructions by \textit{GAVIE}. }
        \label{fig:posneg}
\vspace{-0.1in}
\end{figure}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{length1.pdf}
         \vspace{-0.2in}
         \caption{Accuracy Performance.}
         \label{length1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{length2.pdf}
         \vspace{-0.2in}
         \caption{Relevancy Performance.}
         \label{length2}
     \end{subfigure}
        \caption{Evaluation results on different instruction lengths by \textit{GAVIE}. }
        \label{fig:length}
\vspace{-0.2in}
\end{figure}


\begin{table}[h]
\centering
\small
\begin{tabular}{lcccccc}
\toprule[1.5pt]
Categories & Metric &Ours & MiniGPT4 & LLaVA & InstructBLIP & MMGPT\\
\midrule
Interrogative&\textsc{Accuracy(GPT4)} & \textbf{6.61} &  4.14 & 4.60 & 5.95 & 1.01\\
Interrogative&\textsc{Relevancy(GPT4)} & \textbf{8.46} &  6.20 & 5.88 & 7.67 & 2.00\\
\midrule
Declarative&\textsc{Accuracy(GPT4)} & \textbf{6.50} & 3.98 & 3.82 & 5.47 & 0.90\\
Declarative&\textsc{Relevancy(GPT4)} & \textbf{8.21} & 5.39 & 5.84 & 6.64 & 1.62\\
\bottomrule[1.5pt]
\end{tabular}
\vspace{0.05in}
\caption{
Evaluation results on Interrogative Instructions and Declarative Instructions by \textit{GAVIE}. The metric scores of \textsc{Accuracy(GPT4)} and \textsc{Relevancy(GPT4)} are in a scale of 0 to 10.
}
\label{tab:Interrogative}
\vspace{-0.2in}
\end{table}


\subsection{More Dataset Statistic}
I summarized the popular words in the knowledge manipulation generated by GPT4 in Fig. \ref{fig:know_dis} and found they mainly include six categories: event, number, date, persons, place, and others. Some examples are shown below.

\textit{Canada, increase, decrease, lowest, 2009, United States, 2016, employment, unemployment, higher, 2013, 2017, 2015, drop, minimum, worst, consistent, kingdom, x-axis, y-axis, under, Italy, pie, bar...}



\begin{table}[t]
\setlength\tabcolsep{1pt}
\centering
\small
\begin{tabular}{lcrccccccccc}
\toprule[1.5pt]
Perception& Existence&Count & Position & Color & Posters & Celebrity& Scene & Landmark & Artwork & OCR\\
\midrule
Original MiniGPT4 & 68.33&55.00&43.33&75.00&41.84&54.41&71.75&54.00&60.50&57.50\\
\textbf{Finetuned MiniGPT4} & 115.0&88.33&68.33&96.67&71.42&72.35&122.00&104.34&77.50&80.00&\\
Original mPLUG-Owl & 120.00&50.00&50.00&55.00&136.05&100.29&135.50&159.25&96.25&65.00\\
\textbf{Finetuned mPLUG-Owl} & 165.00&111.67&86.67&165.00&139.04&112.65&147.98&160.53&101.25& 110.0\\
\midrule
\end{tabular}
\vspace{2mm}
\caption{Completed experiments of \textit{Perception} on MME \cite{fu2023mme} benchmark. }
\label{tab:mme_preception}
\vspace{-0.1in}
\end{table}

\begin{table}[t]
\setlength\tabcolsep{3pt}
\centering
\small
\begin{tabular}{lcccc}
\toprule[1.5pt]
Cognition &Commonsense Reasoning& Numerical Calculation&Text Translation & Code Reasoning\\
\midrule
Original MiniGPT4 & 59.29&45.00&0.00&40.00\\
\textbf{Finetuned MiniGPT4} & 76.42&55.00&77.50&67.50\\
Original mPLUG-Owl & 78.57&60.00&80.00&57.50\\
\textbf{Finetuned mPLUG-Owl} & 100.71&70.00&85.00&72.50\\
\midrule
\end{tabular}
\vspace{2mm}
\caption{Completed experiments of \textit{Cognition} on MME \cite{fu2023mme} benchmark. }
\label{tab:mme_cog}
\vspace{-0.1in}
\end{table}

\begin{figure*}[h]
    \centering
\centerline{\includegraphics[width=1\textwidth]{knowledge_distribution.pdf}}
    \caption{Distribution of Knowledge Manipulations. The knowledge mainly includes six categories: event, number, date, persons, place, and others. }
    \label{fig:know_dis}
\end{figure*}





\begin{figure*}[h]
    \centering
\centerline{\includegraphics[width=1\textwidth]{fuxiao-pos-example1.pdf}}
    \caption{The first example for generating positive instruction and answers (Part1). The image is not included in the text prompt. }
    \label{fig:prompt_pos11}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-pos-example1-result.pdf}
    \caption{The first example for generating positive instruction and answers (Part2).}
    \label{fig:prompt_pos12}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-pos-example1-result2.pdf}
    \caption{The first example for generating positive instruction and answers (Part3).}
    \label{fig:prompt_pos13}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-pos-example2.pdf}
    \caption{The second example for generating positive instruction and answers (Part1). The image is not included in the text prompt.}
    \label{fig:prompt_pos21}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-pos-example2-result.pdf}
    \caption{The second example for generating positive instruction and answers (Part2).}
    \label{fig:prompt_pos22}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-pos-example2-result2.pdf}
    \caption{The second example for generating positive instruction and answers (Part3).}
    \label{fig:prompt_pos23}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-neg1-example1.pdf}
    \caption{The example for generating negative instruction and answers by nonexistent object manipulation (Part1). The image is not included in the text prompt.}
    \label{fig:prompt_neg11}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-neg1-example-result.pdf}
    \caption{The example for generating negative instruction and answers by nonexistent object manipulation (Part2).}
    \label{fig:prompt_neg12}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-neg2-example1.pdf}
    \caption{The example for generating negative instruction and answers by existent object manipulation (Part1). The image is not included in the text prompt.}
    \label{fig:prompt_neg21}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-neg2-example-result2.pdf}
    \caption{The first example for generating negative instruction and answers by existent object manipulation (Part2).}
    \label{fig:prompt_neg22}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-eval1-example1.pdf}
    \caption{The first example for the GPT4-Assisted Visual Instruction Evaluation (Part1). The image is not included in the text prompt.}
    \label{fig:prompt_neg31}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-eval1-example1-xia.pdf}
    \caption{The first example for the GPT4-Assisted Visual Instruction Evaluation (Part2).}
    \label{fig:prompt_neg32}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-eval1-example1-result.pdf}
    \caption{The first example for the GPT4-Assisted Visual Instruction Evaluation (Part3).}
    \label{fig:prompt_neg33}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-eval2-example1.pdf}
    \caption{The second example for the GPT4-Assisted Visual Instruction Evaluation (Part1). The image is not included in the text prompt.}
    \label{fig:prompt_neg41}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-eval2-example2.pdf}
    \caption{The second example for the GPT4-Assisted Visual Instruction Evaluation (Part2).}
    \label{fig:prompt_neg42}
\end{figure*}

\begin{figure*}[h]
    \centering
      \includegraphics[width=1\textwidth]{fuxiao-eval2-example-result.pdf}
    \caption{The second example for the GPT4-Assisted Visual Instruction Evaluation (Part3).}
    \label{fig:prompt_neg43}
\end{figure*}


\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.8\textwidth]{neg11.pdf}
         \caption{All LMMs except ours produce inconsistent descriptions with the images. MiniGPT4 also has repetitive sentences.}
         \label{fig:NEG11}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.8\textwidth]{neg12.pdf}
         \caption{MiniGPT4, LLaVA, and InstructBLIP respond with long descriptive answers about the "painting", which does not exist in the image. Multimodal-GPT produces a question about the "painting," but it does not address the instruction. }
         \label{fig:NEG13}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.8\textwidth]{neg13.pdf}
         \caption{MiniGPT4, LLaVA, and Multimodal-GPT respond with a long descriptive answer about the "pink flowers", which do not exist in the image. Although InstructBLIP's answer is brief, it's still false.}
         \label{fig:NEG12}
     \end{subfigure}
        \caption{Model predictions on negative instruction examples of Neg1:\textit{"Nonexistent object Manipulation"}. \textcolor{red}{RED} represents the nonexistent elements in the images.}
        \label{fig:demo_neg1}
\end{figure}
\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{neg21.pdf}
         \caption{All LMMs except ours produce inconsistent descriptions with the images. Our model can point out that the loose strands do not form a butterfly pattern. }
         \label{fig:NEG21}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{neg22.pdf}
         \caption{All LMMs except ours produce false attributes of windows in the images. Our model can point out that the windows are white instead of red. }
         \label{fig:NEG22}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{neg23.pdf}
         \caption{All LMMs except ours produce inconsistent descriptions with the images. Our model can point out that the woman with green hair doesn't have a black backpack on her shoulder. }
         \label{fig:NEG23}
     \end{subfigure}
        \caption{Model predictions on negative instruction examples of Neg2:\textit{"Existent object Manipulation"}. \textcolor{red}{RED} represents the wrong attributes of existent objects in the images.}
        \label{fig:demo_neg2}
        
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos1.pdf}
         \caption{Object Detection Task. MiniGPT4, LLaVa, and MMGPT can't locate the objects correctly.}
         \label{fig:pos1}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos2.pdf}
         \caption{Multi-choice VQA Task. InstructBLIP can't follow the instruction well by generating anything. LLaVA's answer seems reasonable, but it incorrectly detects the clouds in the image. }
         \label{fig:pos2}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos3.pdf}
         \caption{Referential Expression Grounding Task. Other LMMs can't directly address the instruction by giving a general answer, "kitchen," while our model can clearly predict the "floor". }
         \label{fig:pos3}
     \end{subfigure}
        \caption{Positive instruction demos with different tasks and predictions from different models.}
        \label{fig:demo_pos1}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos4.pdf}
         \caption{Image Anomaly Detection Task. Our model and LLaVA point out there is a normal image without unusual elements, while MiniGPT4, InstructBLIP, and MMGPT describe the image instead of addressing the instruction.}
         \label{fig:demo2_pos1}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos5.pdf}
         \caption{Visual Entailment Task. }
         \label{fig:demo2_pos2}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos6.pdf}
         \caption{Object Interaction Analysis Task. All LMMs except ours describe the wrong location of the book. LLaVA generates long text with unrelated information to address the instruction.}
         \label{fig:demo2_pos3}
     \end{subfigure}
        \caption{Positive instruction demos with different tasks and predictions from different models. }
        \label{fig:demo_pos2}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos7.pdf}
         \caption{Dense Description Task. All LMMs except ours incorrectly recognize the color of the bridle. InstructBLIP also fails to answer the "state of the horse's bridle" in the instruction.}
         \label{fig:demo3_pos1}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos8.pdf}
         \caption{Image Caption Task. LLaVA and InstructBLP fail to follow the instruction for generating one caption. Instead, they generate long text with several sentences.}
         \label{fig:demo3_pos2}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{pos9.pdf}
         \caption{Activity Recognition Task.}
         \label{fig:demo3_pos3}
     \end{subfigure}
        \caption{Positive instruction demos with different tasks and predictions from different models. }
        \label{fig:demo_pos3}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=1.0\textwidth]{chart_example.pdf}
         \caption{More examples of chart instruction data and knowledge manipulation examples.}
         \label{fig:demo_chart33}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=1.0\textwidth]{chart1.pdf}
         \caption{Comparison between current LMMs when the input is Knowledge Manipulation instruction.}
         \label{fig:demo_chart11}
     \end{subfigure}
      \caption{Examples of Knowledge Manipulations. }
        \label{fig:demo_chart_example1}
     
\end{figure}
