[{'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'HallusionBench', 'Metric': 'Question Pair Acc', 'Score': '1.57'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '31.7Â±0.1'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'Params', 'Score': '7B'}}]
