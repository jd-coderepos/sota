


\documentclass[journal,onecolumn,11pt]{IEEEtran}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage{amsmath,bm,algorithm,algorithmic,float}
\usepackage{amssymb}
\usepackage[thmmarks,amsmath,amsthm,hyperref]{ntheorem}
\usepackage{multirow}
\usepackage{slashbox, color}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\renewcommand{\baselinestretch}{1.45}
\ifCLASSINFOpdf
\else
\fi










\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{ Sparse Fast Fourier Transform for Exactly and Generally -Sparse Signals by Downsampling and Sparse Recovery}
\author{Sung-Hsien~Hsieh,
        Chun-Shien~Lu,~\IEEEmembership{Member,~IEEE}
        and~Soo-Chang~Pei,~\IEEEmembership{Fellow,~IEEE} \thanks{S.-H. Hsieh is with the Institute of Information Science, Academia Sinica,
Taipei 115, Taiwan, and also with the Graduate Institute of Communication Engineering, National Taiwan University, Taipei 106, Taiwan.}
\thanks{C.-S. Lu is with Institute of Information Science, Academia Sinica, Taipei,
Taiwan (e-mail: lcs@iis.sinica.edu.tw).}
\thanks{S.-C. Pei is with the Graduate Institute of Communication Engineering,
National Taiwan University, Taipei 106, Taiwan.}
}


\markboth{}{}



\maketitle

\begin{abstract}
Fast Fourier Transform (FFT) is one of the most important tools in digital signal processing.
FFT costs  for transforming a signal of length .
Recently, Sparse Fourier Transform (SFT) has emerged as a critical issue addressing how to compute a compressed Fourier transform of a signal with complexity being related to the sparsity of its spectrum.

In this paper, a new SFT algorithm is proposed for both exactly -sparse signals (with  non-zero frequencies) and generally -sparse signals (with  significant frequencies), with the assumption that the distribution of the non-zero frequencies is uniform.
The nuclear idea is to downsample the input signal at the beginning; then, subsequent processing operates under downsampled signals, where signal lengths are proportional to .
Downsampling, however, possibly leads to ``aliasing''.
By the shift property of DFT, we recast the aliasing problem as complex Bose-Chaudhuri-Hocquenghem (BCH) codes solved by syndrome decoding.
The proposed SFT algorithm for exactly -sparse signals recovers  frequencies with computational complexity  and probability at least  under , where  is a user-controlled parameter.

For generally -sparse signals, due to the fact that BCH codes are sensitive to noise, we combine a part of syndrome decoding with a compressive sensing-based solver for obtaining  significant frequencies.
The computational complexity of our algorithm is , where the Big-O constant of  is very small and only a simple operation involves .
Our simulations reveal that  does not dominate the computational cost of sFFT-DT.

In this paper, we provide mathematical analyses for recovery performance and computational complexity, and conduct comparisons with known SFT algorithms in both aspects of theoretical derivations and simulation results.
In particular, our algorithms for both exactly and generally -sparse signals are easy to implement.
\end{abstract}

\begin{IEEEkeywords}
Compressed Sensing, Downsampling, FFT, Sparse FFT, Sparsity.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


\section{Introduction}\label{sec:intro}
\subsection{Background and Related Work}
\IEEEPARstart{F}{ast} Fourier transform (FFT) is one of the most important approaches for fast computing discrete Fourier transform (DFT) of a signal with time complexity , where  is the signal length.
FFT has been used widely in the communities of signal processing and communications.
How to outperform FFT, however, remains a challenge and persistently receives attention.

Sparsity is inherent in signals and has been exploited to speed up FFT in the literature.
A signal of length  is called exactly -sparse if there are  non-zero frequencies with .
On the other hand, a signal is called generally -sparse if all frequencies are non-zero but we are only interested in keeping the first -largest (significant) frequencies in terms of magnitudes and ignore the remainder.
Instead of computing all frequencies, Sparse Fourier Transform (SFT) has emerged as a critical topic and aim to compute a compressed DFT, where the time complexity is proportional to .

A. C. Gilbert \cite{Gilbert2014} {\em et al.} propose an overview of SFT and summarize a common three-stage approach:
1) identify locations of non-zero or significant frequencies;
2) estimate the values of the identified frequencies; and
3) subtract the contribution of the partial Fourier representation computed from the first two stages from the signal and go back to stage 1.
Some prior works are briefly described as follows.

M. A. Iwen \cite{Iwen2010} proposes a sublinear-time SFT algorithm based on Chinese Remainder Theorem (CRT) with computational complexities
(a)  with a non-uniform failure probability per signal and
(b)  with a deterministic recovery guarantee.
Iwen's algorithm can work for general  with the help of interpolation.
Although the algorithm offers strong theoretical analysis, the empirical experiments show that it suffers Big-O constants. For example, in Fig. 5 of \cite{Iwen2010}, it shows to outperform FFTW under  and .
The approximation error bounds in \cite{Iwen2010} are further improved in \cite{Iwen2013}.

H. Hassanieh {\em et al.} propose so-called Sparse Fast Fourier Transform (sFFT) \cite{Haitham2012}\cite{Haitham2012_1}. The idea behind sFFT is to subsample fewer frequencies (proportional to ) since most of frequencies are zero or insignificant.
Nevertheless, the difficulty is which frequencies should be subsampled as the locations and values of the  non-zero frequencies are unknown. To cope with this difficulty, sFFT utilizes the strategies of filtering and permutation introduced in \cite{Gilbert2005}, which can increase the probability of capturing useful information from subsampled frequencies.
For exactly -sparse and general -sparse signals, sFFT costs  and , respectively.
In their simulations, sFFT is faster than FFTW \cite{Frigo2005} (a very fast C subroutine library for computing FFT) for exactly -sparse signals with .


Even though sFFT \cite{Haitham2012}\cite{Haitham2012_1} is outstanding, there are some limitations, summarized as follows:
1) Filtering and permutation are operated on the input signal. These operations are related to .
Thus, the complexity of sFFT still involves  and cannot achieve the theoretical ideal complexity .
2) sFFT only guarantees that it succeeds with a constant probability ({\em e.g.}, ).
3) The implementation of sFFT for generally -sparse signals is very complicated as it involves too many parameters that are difficult to set.\footnote{In fact, according to our private communication with the authors of \cite{Haitham2012}\cite{Haitham2012_1}, they would not recommend implementing this code since it is not trivial. The authors also suggest that it is not easy to clearly illustrate which setting will work best because of the constants in the Big-O functions and because of the dependency on the implementation. The authors themselves did not implement it since they believed that the constants would be large and that it would not realize much improvement over FFTW.}

Ghazi {\em et al.} \cite{Ghazi2013} propose another algorithm based on Prony's method for exactly -sparse signals.
The basic idea is similar to our previous work \cite{Hsieh2013}.
The key difference is that Ghazi {\em et al.}'s method recovers all  non-zero frequencies once, while we propose a top-down strategy to solve  non-zero frequencies iteratively. Furthermore, due to different parameter settings and root finding algorithms, Ghazi's SFT costs  along with different big-O constants. The comparison between these two methods in terms of computational complexity and recovery performance will be discussed later in Sec. \ref{ssec:howtochoose_a_and_d}.

S. Heider {\em et al.}'s method \cite{Heider2013} combines Prony-like methods with quasi random sampling and band pass filtering.
Compared with our method, they estimate the positions and values of non-zero frequencies in each band based on the ESPRIT method instead of syndrome decoding.
ESPRIT requires more computational cost resulting in the total complexity being .
Their proof also shows  that is more strict than  in our case for exactly- sparse signal.

Pawar and Ramchandran \cite{Pawar2013} propose an algorithm, called FFAST (Fast Fourier Aliasing-based Sparse Transform), which focuses on exactly -sparse signals.
Their approach is based on filterless downsampling of the input signal using a constant number of co-prime downsampling factors guided by CRT.
These aliasing patterns of different downsampled signals are formulated as parity-check constraints of good erasure-correcting sparse-graph codes.
FFAST costs  but relies on the constraint that co-prime downsampling factors must divide .
Moreover, the smallest downsampling factor bounds FFAST's computational cost.
For example, if  and , the smallest downsampling factor is .
In this case, the computational cost of calculating FFT of a downsampled signal with length  is higher than .
Actually, these limitations are possibly harsh.


We summarize and compare the SFT algorithms reviewed above in Table \ref{Table: SFT comparision} in terms of the number of samples, computational complexity, and assumption regarding sparsity.
More specifically, the number of samples decides how much information SFT algorithms require in order to reconstruct -sparse signals.
It is especially important for some applications, including Analog-to-Digital converter, which are benefited by low sampling rates.
Moreover, the assumption of a certain range of sparsity guarantees that SFT algorithms can have high quality of reconstruction.
We can find from Table \ref{Table: SFT comparision} that our algorithms have the lowest computational complexity, the lowest number of samples, and the best range of sparsity for exactly -sparse signals.
Although the sparsity constraint  seems to be more tough for generally -sparse signals in our method,
for a (very) sparse signal we still can solve it by assuming that its sparsity is higher than the true one with more computational cost.
In the simulations, we show that the Big-O constants for both exactly -sparse and generally -sparse signals are actually small, implying the practicability of our proposed approaches for real implementation.


\begin{table}[t]
\fontsize{7.5pt}{1em}\selectfont
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{4pt}
\caption{Comparison between SFT algorithms in terms of computational complexity, required samples and assumptions.}
\label{Table: SFT comparision}
\doublerulesep=0pt
\begin{tabular}[tc]{|c||c|c|c|c|c|c|}
\hline
 \multirow{2}{*}{}&    \multicolumn{3}{c|}{Exactly -sparse signal}&  \multicolumn{3}{c|}{Generally -sparse signal}  \\
\cline{2-7} & Samples  & Complexity & Assumption & Samples & Complexity & Assumption   \\ \hline\hline
\cite{Iwen2010}&   & &  & & &    \\ \hline
\cite{Haitham2012_1}&  &  &  &  &  &    \\ \hline
\cite{Ghazi2013}&  & &   & &  &     \\ \hline
\cite{Heider2013}&  &  &  & void & void & void    \\ \hline
\cite{Pawar2013}&  & & ,  & void & void & void   \\ \hline
This paper&  & &  &   &  &    \\ \hline
\end{tabular}
\end{table}

\subsection{Our Contributions}
In our previous work \cite{Hsieh2013}, we propose a SFT algorithm, called sFFT-DT, based on filterless downsampling with time complexity of  only for exactly -sparse signals.
The idea behind sFFT-DT is to downsample the input signal in the time domain before directly conducting all subsequent operations on the downsampled signals.
By choosing an appropriate downsampling factor to make the length of a downsampled signal be , no operations related to  are required in sFFT-DT.
Downsampling, however, possibly leads to ``aliasing,'' where different frequencies become indistinguishable in terms of their locations and values.
To overcome this problem, the locations and values of these  non-zero entries are considered as unknown variables and the ``aliasing problem'' is reformulated as ``Moment Preserving Problem (MPP)''.
Furthermore, sFFT-DT is conducted in a manner of a top-down iterative strategy under different downsampling factors, which can efficiently reduce the computational cost.
In comparison with other CRT-based approaches \cite{Heider2013}\cite{Pawar2013} that require multiple co-prime integers dividing , our method only needs the downsampling factor to divide  but does not suffer the co-prime constraint, implying that sFFT-DT has more freedom for .

In this paper, we further examine the accurate computational cost and theoretical performance of sFFT-DT for exactly -sparse signals.
We derive the Big-O constants of computational complexity of sFFT-DT and show that they are smaller than those of Ghazi {\em et al.}'s sFFT \cite{Ghazi2013}.
In addition, sFFT-DT is efficient due to , which makes it useful whatever the sparsity  is.
Finally, all operations of sFFT-DT are solved via analytical solutions but those of Ghazi {\em et al.}'s sFFT involve a numerical root finding algorithm, which is more complicated in terms of hardware implementation.

In the context of SFT, sparsity  plays an important role.
The performance and computational complexity of previous SFT algorithms \cite{Haitham2012}\cite{Haitham2012_1}\cite{Ghazi2013}\cite{Pawar2013} have been analyzed based on the assumption that sparsity  is known in advance.
In practice, however,  is unknown and is an input parameter decided by the user.
If  is not guessed correctly, the performance is degraded and/or the computational overhead is higher than expected because the choice of some parameters depends on .
In this paper, we propose a simple solution to address this problem and relax this impractical assumption.
We show that the cost for deciding  is the same as that required for sFFT-DT with known .

In addition to conducting more advanced theoretical analyses, we also study sFFT-DT for generally -sparse signals in this paper.
For generally -sparse signals, since all frequencies are non-zero, each frequency of a downsampled signal is composed of significant and insignificant frequencies due to aliasing.
To extract significant components from each frequency, the concept of sparse signal recovered from fewer samples, originating from compressive sensing (CS) \cite{Donoho2006}, is employed since significant entries are ``sparse''. A pruning strategy is further used to exclude locations of insignificant terms.
We prove the sufficient conditions of robust recovery, which means reconstruction error is bounded, with time complexity  under .
The empirical experiments show that the Big-O constant of sFFT-DT is small and outperforms FFT when  and .

Finally, we conclude that our methods are easy to implement and are demonstrated to outperform the state-of-the-art in terms of theoretical analyses and simulation results.

\subsection{Organization of This Paper}
The remainder of this paper is organized as follows.
In Sec. \ref{Sec: sFFT-DT: Exact K-Sparse}, we describe the proposed method for exactly -sparse signals.
Our method for generally -sparse signals will be expounded in Sec. \ref{Sec: sFFT-DT: General K-Sparse}.
Conclusions are provided in Sec. \ref{Sec: Conclusions}.



\section{sFFT-DT for Exactly -Sparse Signals}\label{Sec: sFFT-DT: Exact K-Sparse}
We describe the proposed method for exactly -sparse signals and provide analyses for parameter setting, computational complexity, and recovery performance.
The proposed method contains three steps.
\begin{itemize}
  \item[1.] Downsample the original signal in the time domain.
  \item[2.] Calculate Discrete Fourier Transform (DFT) of the downsampled signal by FFT.
  \item[3.] Use the DFT of the downsampled signal to locate and estimate  non-zero frequencies..
\end{itemize}
Steps 1 and 2 are simple and straightforward.
Thus, we focus on Step 3 here.

Throughout the paper, common notations are defined as follows.
Let  be the input signal in the time domain, and let  be DFT of .
 is the DFT matrix such that  with  and .

\subsection{Problem Formulation}\label{ssec:DFT Property}
Let  be the signal downsampled from an original signal , where , , and integer   is a downsampling factor.
The length of the downsampled signal  is 
Let  be DFT of , where
\small

\normalsize
The objective here is to locate and estimate  non-zero frequencies of  from .

Note that each frequency of  is a sum of  terms of .
When more than two terms of  are non-zero, ``aliasing'' occurs, as illustrated in Fig. \ref{fig:Iterative Pyramid}.
Fig. \ref{fig:Iterative Pyramid}(a) shows an original signal in the frequency domain, where only three frequencies are non-zero (appearing at normalized frequencies = , , and ).
Fig. \ref{fig:Iterative Pyramid}(b) shows the downsampled signal in the frequency domain when , where the downsampled frequency at  incurs aliasing; {\em i.e.}, the frequency of  at  collides with the one at .
In Fig. \ref{fig:Iterative Pyramid}(b), we solve all non-zero downsampled frequencies once, no matter whether aliasing occurs or not.
This procedure is called non-iterative sFFT-DT and will be discussed in detail later.
Instead of solving all of the downsampled frequencies once, Fig. \ref{fig:Iterative Pyramid}(c) illustrates an example of iteratively solving frequencies.
At the first iteration, the downsampled frequency without aliasing at  is solved.
This makes the remaining downsampled frequencies more sparse.
Then, the signal is downsampled again with .
At the second iteration, we solve the downsampled frequency with aliasing at .
This procedure, called iterative sFFT-DT, will be discussed further in Sec. \ref{ssec:ItMethod}.


\begin{figure}[h]
  \centering{\epsfig{figure=IterativePyramid.eps,width=4.25 in}}
\hfill
\caption{Aliasing and its iterative solver. (a) Original signal in frequency domain. (b) Downsampled signal in frequency domain with . If we want to solve all frequencies once, it requires  FFTs. (c) Similar to (b), however, the frequency (at normalized frequency ) at  is solved first and requires  FFTs. (d) Remaining frequency (at ) requires  extra FFTs at .}
\label{fig:Iterative Pyramid}
\end{figure}

In the following, we describe how to solve the aliasing problem by introducing the shift property of DFT.
Let , where  denotes the shift factor.
Each frequency of  is denoted as:
\small

\normalsize
Thus, Eq. (\ref{eq:xd with shift}) degenerates to Eq. (\ref{eq:xd}) when . In practice, all we can obtain are 's for different 's.

For each downsampling factor , there will be no more than  terms on the right side of Eq. (\ref{eq:xd with shift}), where
each term contains two unknown variables,  and .
Let , , denote the number of terms on the right side of Eq. (\ref{eq:xd with shift}).
Therefore, we need  equations to solve these  variables, and  is within the range of .
By taking the above into consideration, the problem of solving the  unknown variables on the right side of Eq. (\ref{eq:xd with shift}) can be formulated\footnote{In the previous version \cite{Hsieh2013}, it is interpreted as a moment preserving problem (MPP). Specifically, solving MPP is equivalent to solving complex BCH codes, where the syndromes produced by partial Fourier transform are consistent with moments.} via BCH codes as:
\small

\normalsize
where  is known and denoted as   while  and  represent unknown  and , respectively, for  and . To simplify the notation, we let  and .

It is trivial that no aliasing occurs if , irrespective of the downsampling factor.
Under this circumstance, we have , , , and , according to Eq. (\ref{eq:MPT obejective fun}).
We obtain that  and .
After some derivations, we can solve  and assign  at the position .
The above solver only works under a non-aliasing environment with .
Nevertheless, when aliasing appears ({\em i.e.}, ), it fails.

To solve the aliasing problem, it is observed from Eq. (\ref{eq:MPT obejective fun}) that all we know are 's for , called syndromes in BCH codes.
Thus, we utilize syndrome decoding \cite{MacWilliams11977}, which is also equivalent to the solver presented in Ghazi {\em et al.}'s sFFT.
Syndrome decoding is discussed in the next subsection.

\subsection{Syndrome Decoding}\label{ssec:MPT}
Note that Eq. (\ref{eq:MPT obejective fun}) is nonlinear and cannot be solved by simple linear matrix operations.
On the contrary, we have to solve 's first, such that Eq. (\ref{eq:MPT obejective fun}) becomes linear.
Then, 's can be solved by matrix inversion.
Thus, the main difficulty is how to solve 's given known syndromes.
According to \cite{Szego1975}, given the unique syndromes with , , ..., , there must exist the corresponding orthogonal polynomial equation, , with roots 's for .
That is, 's can be obtained as the roots of .
The steps for syndrome decoding are as follows. \\
Step (i): Let the orthogonal polynomial equation  be:
\small

\normalsize
The relationship between  and the syndromes is as follows:
\small

\normalsize
Eq. (\ref{eq:auxiliary fun of MPT}) can be formulated as , where , , and .
Thus, Eq. (\ref{eq:auxiliary fun of MPT}) can be solved by matrix inversion  to obtain 's.\\
Step (ii): Find the roots of  in Eq. (\ref{eq:poly fun of MPT}).
These roots are the solutions of , ,..., respectively.\\
Step (iii): Substitute all 's into Eq. (\ref{eq:MPT obejective fun}), and solve the resulting equations to obtain 's.

Tsai \cite{Tsai1985} showed a complete analytical solution composed of the aforementioned three steps for , based on the constraint that .
Nevertheless, for the aliasing problem considered here, the constraint is , as indicated in Eq. (\ref{eq:xd with shift}).
We have also derived the complete analytical solution accordingly for .
Please see Appendix in Sec. \ref{Sec: Appendix}.
The analytical solutions for a univariate polynomial with  cost  operations.
Since there are  frequencies, the computational cost of syndrome decoding is .
For , Step (i) still costs , according to the Berlekamp-Massey algorithm \cite{Massey1963}, which is well-known in Reed-Solomon decoding \cite{MacWilliams11977}.
In addition, Step (iii) is designed to calculate the inverse matrix of a Vandermonde matrix and costs  \cite{NChen2008}.
There is, however, no analytical solution of Step (ii) for .
Thus, numerical methods of root finding algorithms with finite precision are required.
A fast algorithm proposed by Pan \cite{Pan2002} can approximate all of the roots with , where the detailed proof was shown in \cite{Ghazi2013}.
If , Step (ii) will dominate the cost of syndrome decoding.

It is noted that the actual number of collisions for each frequency,  (), is unknown in advance.
In practice, we choose a maximum number of collision  and expect  for all downsampled frequencies. Under the circumstance,  syndromes are required for syndrome decoding.
If 's of all downsampled frequencies are smaller than or equal to , the syndrome decoding perfectly recovers all of the frequencies; {\em i.e.,} it resolves all non-zero values and locations of .
Otherwise, the non-zero entries of  cannot be recovered due to insufficient information.
Although a larger  guarantees better recovery performance, it also means that more syndromes and higher computational cost are required.

In sum, the cost of syndrome decoding consists of two parts. Since the size of a downsampled signal is , the cost of generating the required syndromes via FFT is , which is called the ``P1 cost of syndrome decoding'' hereafter.
Second, as previously mentioned, solving the aforementioned Steps (i), (ii), and (iii) will cost  for  and cost  for , where either of which is defined as the ``P2 cost of syndrome decoding''.
Lemma \ref{lemma:cost of non-iterative sFFT-DT} summarizes the computational cost of syndrome decoding.
\begin{lemma}
\label{lemma:cost of non-iterative sFFT-DT}
Give  and , sFFT-DT, including generating syndromes by FFTs and syndrome decoding, totally costs  for  and  for .
\end{lemma}

So far, our method of solving all downsampled frequencies is based on fixing downsampling factor  (and ), as an example illustrated in Fig. \ref{fig:Iterative Pyramid} (b).
In this case, we call this approach, non-iterative sFFT-DT.
Its iterative counterpart, iterative sFFT-DT,
will be described later in Sec. \ref{ssec:ItMethod} and Sec. \ref{ssec:exact_algorithm}.



\subsection{Analysis}\label{ssec:howtochoose_a_and_d}

In this section, we first will study the relationship between  and , and analyze the probability of a downsampled frequency with number of collisions larger than .
Second, we will discuss computational complexity and recovery performance of our non-iterative sFFT-DT.
Third, we will compare non-iterative sFFT-DT with Ghazi {\em et al.}'s sFFT \cite{Ghazi2013}.
In addition, the Big-O constant of complexity is induced in order to highlight the computational simplicity of non-iterative sFFT-DT.
Finally, we will conclude by presenting an iterative sFFT-DT approach to reduce computational cost further.

\subsubsection{Relationship between Maximum Number of Collisions and Downsampling Factor}
Now, we consider the relationship between  and .
If  is set to , then we always can recover any  without errors but the computational cost
will be larger than that of FFT.
Thus, it is preferable to set smaller , which is still feasible when  is uniformly distributed.
For each frequency, the number of collisions, , will be small with higher probability if  is small enough, as Lemma \ref{lemma:probability of bin} illustrates
\begin{lemma}
\label{lemma:probability of bin}
Suppose  non-zero entries distribute uniformly ({\em i.e.}, with probability ) in .
Let  denote the probability that there is at least a downsampled frequency with number of collisions  when the downsampling factor is .
Then, , where  is Euler's.
And non-iterative sFFT-DT obtains perfect recovery with probability at least .
\end{lemma}

\begin{proof}
For each downsampled frequency, the probability of  is , which is smaller than .
Under this circumstance, the probability of at least a downsampled frequency with  is  bounded  by .
Thus, we can derive:
\small

\end{proof}
\normalsize
The probability that  can be perfectly reconstructed using sFFT-DT is  since  results in the fact that the syndrome decoding cannot attain the correct values and locations in the frequency domain.
Furthermore, since  is controlled by , , and , it can be very low based on an appropriate setting.
Let  denote the ratio of the length () of a downsampled signal to .
Our empirical observations, shown in Fig. \ref{fig:aliasing}, indicate the probability of collisions at different 's.
For , the probability of collisions is very close to .
\begin{figure}[h]
  \centering{\epsfig{figure=Aliasing.eps,width=3.7in}}
\hfill
\caption{The probability of collisions for  at different 's, where  denotes the number of collisions. The results show that , in fact, seldom occurs.}
\label{fig:aliasing}
\end{figure}


\subsubsection{Computational Cost and Recovery Performance}
According to computational cost in Lemma \ref{lemma:cost of non-iterative sFFT-DT} and probability for perfect recovery in Lemma \ref{lemma:probability of bin}, we have Theorem \ref{theorem:probability of sfftdt v1}.
\begin{theorem}
\label{theorem:probability of sfftdt v1}
If non-zero frequencies of  distribute uniformly, given  and , sFFT-DT perfectly recovers  with the probability at least  and the computational cost  for  and  for .
\end{theorem}


Based on different parameter settings in Theorem \ref{theorem:probability of sfftdt v1}, we can further distinguish our sFFT-DT from Ghazi {\em et al.}'s sFFT \cite{Ghazi2013} in terms of recovery performance and computational cost as follows.
(sFFT-DT): Set  and .
We have the probability of perfect recovery, , and computational cost, .\\
(Ghazi {\em et al.}'s sFFT): Set  and .
We have  and computational cost . \\
Furthermore, Ghazi {\em et al.}'s sFFT aims to maximize the performance without the constraint of .
Thus, it requires to use an extra root finding algorithm \cite{Pan2002} with complexity being related to the signal length .

On the contrary, sFFT-DT achieves the ideal computational cost, which is independent of , but with the lower bound of successful probability degrading to  for large .
Under this circumstance, sFFT-DT is seemingly unstable.
Nevertheless, if we consider the recovery performance in terms of energy, sFFT-DT can guarantee that most of frequencies are estimated correctly, as Theorem \ref{theorem:probability of sfftdt v2} indicates.
To prove this, we first define some parameters here.
Let , where  is the user-defined parameter, and let  with  representing the proportion of frequencies that cannot be successfully recovered.

\begin{theorem}
\label{theorem:probability of sfftdt v2}
If non-zero frequencies of  distribute uniformly, given  and , sFFT-DT recovers at least  frequencies of  with the probability at least , and computational cost  for  and  for .
\end{theorem}
\begin{proof}
We extend  derived in Lemma \ref{lemma:probability of bin} as  to represent the probability that at least  frequencies with  is derived as:

Let  and plug it in Eq. (\ref{Eq: tauK entries}).
We obtain the result that at least  frequencies of  cannot be solved with probability .
In other words, there are at most  frequencies of  that cannot be solved with probability .
We complete this proof.
\end{proof}
By choosing appropriate  and , sFFT-DT performs better with successful probability converging to  when  increases, implying that it can work for .
For example, by setting  and , where  is , we have .
In this case, let  and it means that sFFT-DT correctly recovers at least  frequencies with probability at least , which converges to  when  is large enough.


In addition, we further analyze the practical cost of additions and multiplications in detail along with the Big-O constants of computational complexity and find that the Big-O constants in Ghazi's sFFT are larger than those in sFFT-DT.
More specifically, recall that the computational cost of sFFT-DT is composed of two parts:
performing FFTs for obtaining syndromes (P1 cost) and solving Steps (i), (ii), and (iii) of syndrome decoding (P2 cost).
Since  was set in our simulations, the Big-O constants for FFT are  for addition and  for multiplication\footnote{Recall that the P1 cost is . Under the situation that  is  and , the Big-O constant is , where  comes from the constant of additions of FFT \cite{Saidi1994}.}.
Since the P2 cost in sFFT-DT is relatively smaller than the P1 cost, it is ignored.

In contrast to sFFT-DT, the Big-O constants of the P1 cost in Ghazi's sFFT \cite{Ghazi2013} are about  for addition and  for multiplication ( must be larger than or equal to ; otherwise Ghazi {\em et al.}'s sFFT cannot work).
Nevertheless, the Big-O constants of one of the Steps (i) and (iii) within the P2 cost need about  for addition and  for multiplication (the detailed cost analysis is based on \cite{NChen2008}). Even though we do not take Step (ii) into account due to the lack of detailed analysis, the Big-O constants for multiplication in sFFT-DT are far smaller than those of Ghazi {\em et al.}'s sFFT, especially for multiplications.
In addition, for hardware implementation, Ghazi {\em et al.}'s sFFT is more complex than sFFT-DT (due to its analytical solution) because an extra numerical procedure for root finding is required and the computational cost involves .
We conclude that there are two main advantages in sFFT-DT, compared to Ghazi's sFFT \cite{Ghazi2013}.
First, the Big-O constants of sFFT-DT are smaller than those of Ghazi {\em et al.}'s sFFT.
Second, our analytical solution is hardware-friendly in terms of implementation.

On the other hand, when the signal is not so sparse with  approaching  ({\em e.g.},  and ), the cost of  FFTs in a downsampled signal is almost equivalent to that of one FFT in the original signal.
To further reduce the cost, a top-down iterative strategy is proposed in Sec. \ref{ssec:ItMethod}.

It also should be noted that the above discussions (and prior works) are based on the assumption that  is known.
In practice,  is unknown in advance.
Unfortunately, how to automatically determine K is ignored in the literature.
Instead of skipping this problem, in this paper, we present a simple but effective strategy in Sec. \ref{ssec:how to decide K} to address this issue.

\subsection{Top-Down Iterative Strategy for Iterative sFFT-DT}\label{ssec:ItMethod}
In this section, an iterative strategy is proposed to solve the aliasing problem with an iterative increase of the downsampling factor  according to our empirical observations that the probability of aliasing decreases
fast with the increase of  and the fact that when  is increased,  is increased as well.
The idea is to solve downsampled frequencies from  to  iteratively.
During each iteration, the solved frequencies are subtracted from  to make  more sparse.
Under this circumstance,  subsequently is set to be larger values to reduce computational cost without sacrificing the recovery performance.
Fig \ref{fig:Iterative Pyramid} illustrates such an example.
In Fig. \ref{fig:Iterative Pyramid}(b), if we try to solve all aliasing problems in the first iteration,  FFTs are required, since the maximum value of  is .
On the other hand, if we first solve the downsampled frequencies with  (at normalized frequency = ), it costs  FFTs, as shown in Fig. \ref{fig:Iterative Pyramid}(c).
Since  FFTs are insufficient for solving the aliasing problem completely under , extra  FFTs are required to solve a more ``sparse'' signal.

The key is how to calculate the  extra FFTs in the above example with lower cost.
Since a more sparse signal is generated by subtracting the solved frequencies from ,  can be set to be larger to further decrease the cost of FFT.
As shown in Fig. \ref{fig:Iterative Pyramid}(d),  extra FFTs can be done quickly with a larger  (=) to solve the downsampled frequency (at normalized frequency = ) with .
Consequently,  is doubled iteratively in our method and the total cost is dominated by that required at the first iteration.

The proposed method with the top-down iterative strategy is called iterative sFFT-DT.

\subsection{Iterative sFFT-DT: Algorithm for Exactly K-Sparse Signals}\label{ssec:exact_algorithm}

In this section, our method, iterative sFFT-DT, is developed and is depicted in Algorithm \ref{Table:our algorithm}, which is composed of three functions, \textbf{main}, \textbf{SubFreq}, and \textbf{SynDec}.
Basically, iterative sFFT-DT solves downsampled frequencies from  to  with an iterative increase of .
Note that, its variation, non-iterative sFFT-DT, solves all downsampled frequencies with  and fixed .

At the initialization stage, the sets  and , recording the positions of solved and unsolved frequencies, respectively, are set to be empty.
 and  are initialized.
The algorithmic steps are explained in detail as follows.

Function \textbf{main}, which is executed in a top-down manner by doubling the downsampling factor iteratively, is depicted from Line 1 to Line 16.
In Lines 3-4, the input signal  is represented by two shift factors  and .
Then they are used to perform FFT to obtain  and  in Lines 5-6.
In Line 7, the function \textbf{SubFreq}, depicted between Line 17 and Line 22, is executed to remove frequencies from  and  that were solved in previous iterations.
The goal of function \textbf{SubFreq} is to make the resulting signal more sparse.

Line 9 in function \textbf{main} is used to judge if there are still unsolved frequencies.
In particular, the condition , initially defined in Eq. (\ref{eq:xd with shift}), may imply:
1) 's for all  are zero, meaning that there is no unsolved frequency and
2) 's are non-zero but their sum is zero, meaning that there exist unsolved frequencies.
To distinguish both,  for  is a sufficient condition.
More specifically, if  is less than or equal to , it is enough to distinguish both by checking whether any one of the  equations is not equal to .
If yes, it implies that at least a frequency grid is non-zero; otherwise, all 's are definitely zero.
Moreover, Line 9 is equivalent to checking  equations at the 'th iteration.
At , two equations ( and ) are verified to ensure that all frequencies with  are distinguished.
At , if , it is confirmed that 's are non-zero at the previous iteration.
On the contrary, if , extra 2 equations ( and ) are added to ensure that all frequencies with  are distinguished.
Thus, at the 'th iteration, there are in total  equations checked.

In Line 11, the function \textbf{SynDec}, depicted in Lines 23-35 (which was described in detail in Sec. \ref{ssec:MPT}), solves frequencies when aliasing occurs. sFFT-DT iteratively solves downsampled frequencies from  to .
Nevertheless, we do not know 's in advance. For example, it is possible that some downsampled frequencies with  are solved in the first three iterations, and these solutions definitely fail.
In this case, the solved locations do not belong to  (defined in Sec. \ref{ssec:DFT Property}). On the contrary, if the downsampled frequency is solved correctly, the locations must belong to .
Thus, by checking whether or not the solution satisfies the condition,  for all  (Line 30), we can guarantee that all downsampled frequencies are solved under correct 's.
Finally, the downsampling factor is doubled, as indicated in Line 14, to solve the unsolved frequencies in an iterative manner.
This means that the downsampled signal in the next iteration will become shorter and can be dealt faster than that in the previous iterations.

\begin{algorithm}[!htb]
\fontsize{11pt}{0.9em}\selectfont
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\caption{Iterative sFFT-DT for exactly -sparse signals.}
\label{Table:our algorithm}
\begin{tabular}[t]{p{17.7cm}l}
\textbf{Input:} , ;\quad \textbf{Output:} ; \\
\textbf{Initialization:} , , , , ; \\
\hline\hline
01. \textbf{function} \textbf{main}()\\
02. \quad\textbf{for}  to  \\
03. \quad\quad  for ;\\
04. \quad\quad  for ;\\
05. \quad\quad ;\\
06. \quad\quad ;\\
07. \quad\quad ;\\
08. \quad\quad \textbf{for}  to \\
09. \quad\quad \quad \textbf{if} ( or  or ) \\
10. \quad\quad \quad \quad  for ; \\
11. \quad\quad \quad \quad ;\\
12. \quad\quad \quad \textbf{end if}\\
13. \quad\quad \textbf{end for} \\
14. \quad\quad ; \\
15. \quad\quad All elements in  modulo .\\
16. \quad\textbf{end for}\\
17. \textbf{function} \textbf{SubFreq} \\
18. \quad \textbf{for} \\
19. \quad \quad  mod ;\\
20. \quad \quad ;\\
21. \quad \quad ;\\
22. \quad \textbf{end for}\\
23. \textbf{function} \textbf{SynDec} \\
24. \quad \textbf{if} \\
25. \quad \quad ;\;\;;\\
26. \quad \textbf{else} \\
27. \quad \quad  Solve the aliasing problem with  by \\
    \quad \quad \quad \;\;syndrome decoding, described in Sec. \ref{ssec:MPT}. \\
28. \quad \textbf{end if} \\
29. \quad  for all ; \\
30. \quad  \textbf{if} ()  for all \\
31. \quad \quad ;\\
32. \quad \quad  for all ;\\
33. \quad \textbf{else} \\
34. \quad \quad  ;\\
35. \quad  \textbf{end if}\\
\hline
\end{tabular}
\end{algorithm}
\normalsize

\subsection{Performance and Computational Complexity of Iterative sFFT-DT}\label{ssec:conv_analysis}
We first discuss the complexity of iterative sFFT-DT.
The cost of the outer loop in function \textbf{main} (Steps 5 and 6) is bounded by two FFTs.
As mentioned in Theorem \ref{theorem:probability of sfftdt v2},  is set to be , the dimensions of  and  are , and FFT costs  in the first iteration.
Since  is doubled iteratively, the total cost of   iterations is still bounded by .
In addition, the function \textbf{SubFreq} costs  operations due to .

The inner loop of the function \textbf{main} totally runs  times, which is not related to the outer loop, since at most  frequencies must be solved.
The cost at each iteration is bounded by the function \textbf{SynDec}.
Recall that the P2 cost, as described in Sec. \ref{ssec:MPT}, requires .
More specifically, since  is doubled iteratively,  can be derived to depend on  from the initial setting .
Therefore, \textbf{SynDec} at the 'th iteration costs  and requires   in total.
That is, the inner loop (Steps 813) costs , given an initial downsampling factor of  and .

In sum, the proposed algorithm, iterative sFFT-DT, is dominated by ``FFT'' and costs  operations.
Now, we discuss Big-O constants for operations of addition and multiplication, respectively.
Since  is doubled iteratively, the P1 cost of syndrome decoding gradually is reduced in the later iterations.
The total cost is , where .
Due to the fact that iterative sFFT-DT possibly recovers  with less than  iterations, the benefit in reducing the computational cost depends on the number of iterations.
In the worst case, the cost is about  under .
Recall that the P1 cost of syndrome decoding in non-iterative sFFT-DT is .
With , the Big-O constants in non-iterative sFFT-DT are two times larger than those in iterative sFFT-DT. Similarly, in the best case ({\em i.e.}, 's of all frequencies are ), the former is about  times larger than the latter. Thus, it is easy to further infer Big-O constants of iterative sFFT-DT. For instance, since the Big-O constant of addition for non-iterative sFFT-DT is , the Big-O constants for iterative sFFT-DT addition range from  (the best case) to  (the worst case) and those for multiplication range from  to .

As for recovery performance in iterative sFFT-DT, since the downsampling factor  is doubled along with the increase of iterations, a question, which naturally arises, is if a larger downsampling factor leads to more new aliasing artifacts.
If yes, these newly generated collisions possibly degrade the performance of iterative sFFT-DT.
If no, the iterative style is good since it reduces computational cost and maintains recovery performance.

In Lemma \ref{lemma:probabilityofaliasing}, we prove that the probability of producing new aliasing artifacts after a sufficient number of iterations will approach zero.

\begin{lemma}
\label{lemma:probabilityofaliasing}
Suppose  non-zero entries of  distribute uniformly ({}, with probability ).
Let  be the probability that new aliasing artifacts are produced at the 'th iteration in iterative sFFT-DT. Let  be the number of frequencies with  at the 'th iteration (, ). If , we have .
\end{lemma}

\begin{proof}
According to Algorithm \ref{Table:our algorithm}, after the first iteration (), all downsampled frequencies with only  aliasing term are solved.
Thus, we focus on discussing the probability of producing new aliasing artifacts under .
By the same idea of Lemma \ref{lemma:probability of bin}, we can define  to be the probability that there is a downsampled frequency with  aliasing terms.
In the second iteration. , however, some non-zero frequencies have been solved in previous iterations.
Thus, the number of remaining non-zero frequencies are no longer  and  and should be modified.
In other words, the number of downsampled frequencies with  must be less than  for  and  becomes the upper bound of the probability that there exists a downsampled frequency of producing new aliasing artifacts.

According to our iterative sFFT-DT algorithm, let  for .
We can derive:
\small

\normalsize
Eq. (\ref{Eq: Bound of aliasing}) converges to  when . By initializing  properly, almost  frequencies can be solved in the first few iterations. This makes  easy to be satisfied.
Under this circumstance,  would be a good choice. By replacing  with , we can derive .
When  increases to be large enough, the probability of  will be small since . \end{proof}

Lemma \ref{lemma:probabilityofaliasing} indicates the probability of producing new aliasing artifacts in an asymptotic manner.
This provides us the information that the probability of producing new aliasing finally converges to zero.
In our simulations, we actually observe that the exact probability with new aliasing is very low under , implying that the iterative approach can reduce the computational cost and maintain the recovery performance effectively.


\subsection{A Simple Strategy for Estimating Unknown Sparsity  }\label{ssec:how to decide K}
As previously described, the sparsity  of a signal is important in deciding the downsampling factor .
Nevertheless,  is, in general, unknown.
In this section, we provide a simple bottom-up strategy to address this issue.

First, we set a large downsampling factor , and then run sFFT-DT.
If there is any downsampled frequency that cannot be solved, then  is halved and sFFT-DT is applied to solve  again.
When  is halved iteratively until the condition in either Theorem \ref{theorem:probability of sfftdt v1} or Theorem \ref{theorem:probability of sfftdt v2} is satisfied, sFFT-DT guarantees one to stop with the probability indicated in either Theorem \ref{theorem:probability of sfftdt v1} or Theorem \ref{theorem:probability of sfftdt v2}.
This strategy needs the same computational complexity required in sFFT-DT with known  because the cost with  is  and the total cost is .
Thus, sFFT-DT with the strategy of automatically determining  costs double the one with known .


\subsection{Simulation Results for Exactly K-Sparse Signals}\label{Sec: Experimental Results}
Our method\footnote{Our code is now available in http://www.iis.sinica.edu.tw/pages/lcs/publications\_en.html (by searching ``Others'').}, iterative  sFFT-DT, was verified and compared with FFTW (using the plan of FFTW\_ESTIMATE  (http://www.fftw.org/)), sFFT-v3 \cite{Haitham2012} (its code was downloaded from http://spiral.net/software/sfft.html), GFFT (using the plan of GFFT-Fast-Rand, which is an implementation of \cite{Iwen2010} and is discussed in \cite{Segal2012} in detail (its code was downloaded from http://sourceforge.net/projects/gopherfft/)), and Ghazi {\em et al.}'s sFFT \cite{Ghazi2013} for exactly -sparse signals.
The simulations for sFFT-DT, FFTW, GFFT, and Ghazi {\em et al.}'s sFFT were conducted with an Intel CPU Q6600 and  GB RAM under Win 7.
sFFT-v3 was run in Linux because the source code was released in Linux's platform.
The signal  in time domain was produced as follows:
1) Generate a -sparse signal  and
2)  is obtained by inverse FFT of .

For sFFT-DT, the initial  is set according to , based on Theorem \ref{theorem:probability of sfftdt v2}.
For sFFT-v3,  was automatically assigned, according to the source code.
For Ghazi {\em et al.}'s sFFT,  and , where  is involved to enforce  being an integer.
If  is an integer, .

The comparison of computational time is illustrated in Fig. \ref{fig:Computation Time}.
Fig. \ref{fig:Computation Time}(a) shows the results of computational time versus sparsity under .
For , our algorithm outperforms FFTW.
Moreover, sFFT-v3 \cite{Haitham2012}\cite{Haitham2012_1} is only faster than FFTW when  and is comparable to Ghazi {\em et al.}'s sFFT.
We can also observe from Fig. \ref{fig:Computation Time}(a) that Ghazi {\em et al.}'s sFFT is slower than iterative sFFT-DT because the P2 cost of syndrome decoding in Ghazi {\em et al.}'s sFFT dominates the computation.
Compared to sFFT-v3 and Ghazi {\em et al.}'s sFFT, our method, iterative sFFT-DT, is able to deal with FFT of signals with large .
GFFT demonstrates the worst results as it crashes when  under .
Fig. \ref{fig:Computation Time}(b) shows the results of computational time versus signal dimension under fixed .
It is observed that the computational time of iterative sFFT and Ghazi {\em et al.}'s sFFT is invariant to , but our method is the fastest.

\begin{figure*}[!t]
\begin{minipage}[b]{.48\linewidth}
\centering{\epsfig{figure=Computation.eps,width=3.55in}}
  \centerline{(a)}
\end{minipage}
\begin{minipage}[b]{.48\linewidth}
\centering{\epsfig{figure=ComutationFixedK.eps,width=3.55in}}
  \centerline{(b)}
\end{minipage}
\hfill
\caption{Comparison of computational time for exact K-sparse signals. (a) Computational time vs. sparsity under . (b) Computational time vs. signal dimension under  and .}
\label{fig:Computation Time}
\end{figure*}

Moreover, according to Theorem \ref{theorem:probability of sfftdt v1}, the performance of non-iterative sFFT-DT seems to be inferior to that of Ghazi {\em et al.}'s sFFT.
Nevertheless, the successful probability described in Theorem \ref{theorem:probability of sfftdt v1} is merely a lower bound.
In our simulations, we compare the recovery performance among three approaches: non-iterative sFFT-DT, iterative sFFT-DT, and Ghazi {\em et al.}'s sFFT \cite{Ghazi2013}.
The parameters for both proposed approaches and Ghazi {\em et al.}'s sFFT were set based on Theorem \ref{theorem:probability of sfftdt v1}.

We have the following observations from Fig. \ref{fig:Performance}, where signal length is .
First, although the theoretical result derived in Theorem \ref{theorem:probability of sfftdt v1} indicates that the performance decreases along with the increase of , it is often better than Ghazi {\em et al.}'s sFFT \cite{Ghazi2013}.
In fact, it is observed that the performance of Ghazi {\em et al.}'s sFFT oscillates.
The oscillation is due to the fact that the floor operation in  (from ) acts like a discontinuous function and leads to large variations of setting .
The recovery performance would benefit by setting small  at the expense of requiring greater computational cost.
Second, iterative sFFT-DT degrades the recovery performance gradually as  increases while, at the same time, the number of collisions () decreases as well.
That is the reason the performance returns to  when  under the case that .

\begin{figure}[t]
\begin{minipage}[b]{.98\linewidth}
\centering{\epsfig{figure=sFFTPerformance.eps,width=3.55in}}
\end{minipage}
\hfill
\caption{Recovery performance comparison among non-iterative sFFT-DT, iterative sFFT-DT, and Ghazi {\em et al.}'s sFFT \cite{Ghazi2013} for exact -sparse signal. The signal length is .}
\label{fig:Performance}
\end{figure}


\section{(Non-Iterative) sFFT-DT for Generally -Sparse Signals}\label{Sec: sFFT-DT: General K-Sparse}
For sparse FFT of a generally -sparse signal , the goal is to compute an approximate transform  satisfying:
\small

\normalsize
where  is exactly -sparse and  is generally -sparse. Without loss of generality, we assume that all frequencies in  are non-zero.
Similar to exactly -sparse signals, we assume that  significant frequencies (with the first  largest magnitudes) of  distribute uniformly.

Due to generally -sparsity of , the right-hand side of Eq. (\ref{eq:xd}) will contain  terms.
When solving syndrome decoding, the remaining insignificant terms will perturb the coefficients of polynomial in Eq. (\ref{eq:poly fun of MPT}).
In addition, how to estimate the roots for perturbed polynomial is an ill-conditioned problem ({\em i.e.}, Wilkinson's polynomial \cite{Wilkinson1959}).
Thus, instead of directly estimating roots by syndrome decoding, we reformulate the aliasing problem in terms of an emerging methodology, called Compressive Sensing (CS) \cite{Donoho2006}\cite{Candes2008}, that has been received much attention recently.

Compressive sensing (CS) is originally proposed for sampling signals under the well-known Nyquist rate.
If the signal follows the assumption that it is sparse in some transformed domain, CS shows that the signal can be recovered from fewer samples, even though the signal is interfered by noises.
The model of CS is formulated as:

where  is a sparse signal,  is a sensing matrix,  is the samples (also called measurements),  is a signal noise, and  is a measurement noise.
It should be noted that  must satisfy either the restricted isometry property (RIP) \cite{Candes2005}\cite{Gan2009} or mutual incoherence property (MIP) \cite{Donoho2001}\cite{Welch1974} for successful recovery with high probability.
It has been shown that Gaussian random matrix and partial Fourier matrix \cite{Candes2006-F} are good candidates to be .

For sFFT-DT of a generally -sparse signal, we formulate the aliasing problem as the CS problem shown in Eq. (\ref{Eq: Random Projection}).
The strategy based on CS is motivated by the following facts:
1) The magnitudes of significant terms must be larger than those of insignificant terms.
2) The number of significant terms is less than that of insignificant terms.
Thus, estimating the locations and values of significant terms is
consistent with the basic assumption in the context of CS.

Unlike iterative sFFT-DT for exactly -sparse signals, the iterative approach cannot work for generally -sparse signals since one cannot guarantee that an exact solution can be attained at each iteration without propagating recovery errors for subsequent iterations.
Therefore, we only study non-iterative sFFT-DT for generally -sparse signals.

In this section, how to formulate the aliasing problem as the CS problem is described in Sec. \ref{Sec: Refinement}.
We discuss CS-based performance along with the sufficient conditions for CS successful recovery in Sec. \ref{Sec: Analysis CS}.
In Sec. \ref{Sec: decide a}, we propose a pruning strategy along with proofs to improve the recovery performance and reduce the computational cost.
The detailed algorithm is described in Sec. \ref{Sec: algorithm for general K sparse}.
The computational time analysis and simulations are, respectively, described in Sec. \ref{Sec: Complexity  for General Ksparse Signal} and Sec. \ref{Sec: experimental result of General}.



\subsection{Problem Formulation}\label{Sec: Refinement}
Recall the BCH codes in Eq. (\ref{eq:MPT obejective fun}), where the locations () and values () as variables.
Since all candidate locations are known and belong to , instead of considering both  and  as variables in Eq. (\ref{eq:MPT obejective fun}), only  are thought of as unknown variables here.
Then, we reformulate the aliasing problem in terms of the CS model as:
\small

\normalsize
where  is the value at ()'th frequency for  and  is the shift factor for .
Let the left-hand side in Eq. (\ref{eq:CS Formulation}) be  as in Eq. (\ref{Eq: Random Projection}) and let the right-hand side be  with .
It should be noted that  is composed of  (significant terms with  non-zero frequencies) and  (insignificant terms with  non-zero frequencies).
Therefore, , , and  and .
In fact, Eq. (\ref{eq:CS Formulation}) can degenerate to Eq. (\ref{eq:MPT obejective fun}).
For example, Eq. (\ref{eq:MPT obejective fun}) is expressed as , where  and  is a matrix in which 'th entry is .
If  is the matrix by pruning the columns of  corresponding to insignificant terms, then .

To solve  given , Eq. (\ref{eq:CS Formulation}) have infinite solutions since .
Conventionally, two strategies \cite{Donoho2006}, -minimization and greedy approaches, are popularly used for sparse signal recovery in CS.
Among them, Subspace Pursuit (SP) \cite{Dai2009} is one of the greedy algorithms and requires  for solving Eq. (\ref{eq:CS Formulation}).
SP runs at most  times, leading to the total cost of SP being .
Similar to exactly -sparse signal, we choose  as the maximum number of collisions for all downsampled frequencies. Thus, the maximum cost of solving SP is .
Since  can be chosen as a constant to ensure that most of downsampled frequencies satisfy  by tuning an appropriate  shown in Theorem \ref{theorem:probability of sfftdt v2}, the cost of SP finally is simplified into .

The next step is how to set , which is very important and related to computational complexity and recovery performance.
In fact,  is directly related to the sampling rate in CS.
Candes and Wakin \cite{Candes2008} pointed out that   must satisfy  to recover  given  and .
If , then .
In other words,  is also a parameter that impacts the size of .
This will make the cost of solving Eq. (\ref{eq:CS Formulation}) related to  and lead to massive computational overhead, which is unacceptable as sFFT-DT must be faster than FFT.
Thus, in sFFT-DT,  is forced to be  and the total cost of SP becomes .
In other words, we generate at most  syndromes for solving Eq. (\ref{eq:CS Formulation}).
Since  is fixed, it is expected to degrade performance when  becomes large.
We will discuss the recovery performance in Sec. \ref{Sec: Analysis CS} under this setting.





Finally, we discuss the relationship between shift factors 's and , in which both affect the recovery performance in CS.
From the theory of CS, the performance also depends on mutual coherence of , which is defined as:
\small

\normalsize
In this case, the phase difference between  and  is , as defined in Eq. (\ref{eq:CS Formulation}).
Recall  ().
If we set , the maximum shift  is encountered and the phase difference between  and  still approaches  with  ().
Under this circumstance,  and perfect sparse recovery will become impossible.
Thus, 's are uniformly drawn from .
This makes , in fact, be a partial Fourier random matrix and its mutual coherence will be small, as shown in \cite{Candes2006-F}.

In sum, sFFT-DT for generally -sparse signals first performs FFTs of downsampled signals with random shift factors and then for each downsampled frequency, the aliasing problem is reformulated in terms of CS model solved by subspace pursuit.

\subsection{Analysis of CS-based Approach}\label{Sec: Analysis CS}
In this section, we describe the recovery performance and computational cost based on the CS model-based solver, indicated in Eq. (\ref{eq:CS Formulation}).
For subsequent discussions, we let , where  and  represent vectors keeping significant and insignificant terms, respectively.

First, we introduce the definition of Restricted Isometric Property (RIP) for performance analysis as:
\begin{definition}
\label{definition:RIP}
Let  and . Suppose there exists a restricted isometry constant (RIC)  of a matrix  such that for each  submatrix  of  and for every  we have:
 
The matrix  is said to satisfy the -restricted isometry property.
\end{definition}
In addition, the performance analysis of SP \cite{Dai2009} is shown in Theorem \ref{theorem:sp performance}.
\begin{theorem}
\label{theorem:sp performance}
Let  be generally -sparse and let .
Suppose that the sampling matrix satisfies RIP with parameter .
Then,
\small

\normalsize
where  is the output of SP and  is an -sparse vector minimizing .
\end{theorem}
It should be noted that, in our case, we formulate the aliasing problem at each downsampled frequency as CS problem solved by SP.
Thus, Theorem \ref{theorem:sp performance} is applied to analyze reconstruction error at each downsampled frequency.
By summing the errors with respect to all downsampled frequencies, we show the total error is still bounded.
\begin{theorem}
\label{theorem:sFFT performance without no pruning}
Let  be generally -sparse.
Given , , , and  is the output of sFFT-DT.
If  in Eq. (\ref{eq:CS Formulation}) satisfies RIP with parameter , then we obtain recovery error
 where

with probability at least  and computational complexity  by solving the aliasing problem in sFFT-DT with SP.
\end{theorem}
\begin{proof}
First, we relax the term  in Eq. (\ref{Eq: SP error}) as .
Let  , , and  represent the significant terms, insignificant terms, and number of collisions at 'th downsampled frequency, respectively.
Thus,  is the reconstruction error at 'th downsampled frequency.
It should be noted that, in our case, 
for all downsampled frequencies.
The total error is derived as:

For each downsampled frequency labeled , we have (1)  if , then  and
(2) if , then ,
where  is a soft-thresholding operator keeping the first  largest entries in magnitude and setting the others to zero.
Based on the above conditions, we can derive

The last inequality is due to the fact that  is the vector keeping the insignificant terms such that .
In addition, for  with , it contains at most  significant frequencies in the worst case.
Thus,  leaves  significant frequencies and
 always holds.

On the other hand, when Theorem \ref{theorem:probability of sfftdt v2} holds, it implies at most  downsampled frequencies with number of collisions larger than .
Then, the cardinality of  is  and .

Finally, the computational cost consists of the costs of generating the required syndromes and running SP.
In similar to exactly- sparse case, given  and , FFTs for downsampled signals totally cost  and the computational cost of SP discussed in Sec. \ref{Sec: Refinement} is .
Thus, we complete this proof.
\end{proof}

It is important to check if the sufficient condition  holds.
How to compute RIC for a matrix, however, is a NP-hard problem.
But we can know that RIC is actually related to , , and .
When fixing ,  must satisfy  such that  holds with high probability, implying that sFFT-DT works well under .
The term, , in Theorem \ref{theorem:sFFT performance without no pruning} also reveals that  results in better performance.
To further improve this result in Theorem \ref{theorem:sFFT performance without no pruning}, we propose a pruning strategy to prune  into , which benefits the recovery performance and computational cost.
Specifically, we are able to reduce recovery errors and easily achieve the sufficient condition if  is replaced by  and reduce the computational cost of SP to become .

\subsection{Pruning Strategy}\label{Sec: decide a}
One can observe from Eq. (\ref{eq:CS Formulation}) that the 'th column of  corresponds to the location  and the root .
The basic idea of pruning is to prune as many locations/roots corresponding to insignificant terms as possible.
Here, the pruning strategy contains three steps:
\begin{itemize}
  \item[1.] Estimate the number of collisions, , for each downsampled frequency by singular value decomposition (SVD) for the matrix  presented in Eq. (\ref{eq:auxiliary fun of MPT}).
  \item[2.] Form the polynomial presented in Step (ii) of syndrome decoding, and substitute all roots in  into the polynomial and reserve the locations with the first  smallest errors in .
  \item[3.] According to reserved locations, prune  to yield .
\end{itemize}

\subsubsection{Step 1 of Pruning}
In Step 1 of the pruning strategy, we estimate  's for all downsampled frequencies. By doing SVD for the matrix in Eq. (\ref{eq:auxiliary fun of MPT}), there are  singular values for each downsampled frequency. Collect all  singular values from all downsampled frequencies and index each singular value according to which downsampled frequency it is from.
The first  largest singular values will vote which downsampled frequency includes the significant term.

Now, we show why the strategy is effective.
We redefine the problem in Eq. (\ref{eq:auxiliary fun of MPT}) for generally -sparse signals.
Let  be a set containing all indices of significant terms and let  be the one defined for insignificant terms, where , ,  and .
Thus, the syndrome in Eq. (\ref{eq:MPT obejective fun}) can be rewritten as , where
 and .
Now, the matrix  in Eq. (\ref{eq:auxiliary fun of MPT}) can be rewritten as:
\small

\normalsize
where  acts like a ``noise'' matrix produced by insignificant components and  comes from significant terms.
 can also be expressed as:
\small

\normalsize
where   is a column vector, as defined in Eq. (\ref{eq:MPT obejective fun}).

It is worth noting that Eq. (\ref{eq:formulation of another M for general}) is similar to SVD.
Nevertheless, there are some differences between them:
(1) 's are complex but not real and 's are not normalized;
(2) 's are not orthogonal vectors; and
(3) The actual SVD of  is , where  denotes a conjugate transpose.

To alleviate the difference (1), Eq. (\ref{eq:formulation of another M for general}) is rewritten as:
\small

\normalsize
where  and  for .
Thus, we have .
As for the difference (3), it can be solved by symmetric SVD (SSVD) \cite{Angelika1988} instead of SVD.
However, the singular values of SSVD have been proven to be the same as those of SVD for the same matrix.
Thus, Eq. (\ref{eq:transformed formulation of another M for general}) can directly use SVD for matrix  to obtain singular values.

On the other hand, the difference (2) is inevitable since 's are not orthogonal, leading to the fact that the singular values of  are not directly equal to the magnitudes of frequencies 's.
However, they are actually related.
In \cite{Takos2008}, Takos and Hadjicostis actually explore the relationship between the eigenvalues of  and signal values.
But their proofs are based on real BCH codes, implying  is a Hermitian matrix.
It is not appropriate for our case because  is, in fact, a complex symmetric matrix.
Thus, we develop another theorem illustrating the relationship between singular value and signal values.
First, Lemma \ref{lemma:singular value of sum of two matrices} in \cite{Hogben2007} illustrates the singular values of sum of matrices.
\begin{lemma}
\label{lemma:singular value of sum of two matrices}
For any matrices  and , let  and let  be a function returning the 'th largest singular value with .
Then,

holds for .
\end{lemma}
Second, for both matrices  and  given in Eq. (\ref{eq:formulation of another M for general}), we explore the upper bound of singular values of  and the lower bound of singular values of , where both bounds are used as the sufficient condition of correctly determining the number  of collisions in aliasing.
Specifically, Lemma \ref{lemma:singular value of sum of two matrices} is used to derive the upper bound of singular values of  whatever  is.
But the lower bound of singular values of  is non-trivial only when  because it becomes 0 for .
\begin{lemma}
\label{lemma:singular value and M}
For any , the singular values of  satisfy 
where .
In addition, for , the singular values of  satisfy

where .
\end{lemma}
\begin{proof}
Since , we have

Similarity, for ,  with . Then, . We complete this proof.
\end{proof}

Combined with Lemma \ref{lemma:singular value of sum of two matrices} and Lemma \ref{lemma:singular value and M}, we can derive the following theorem.
\begin{theorem}
\label{theorem:successfuly determine a}
If  and all downsampled frequencies satisfy , then sFFT-DT correctly decides the number of collisions.
\end{theorem}
\begin{proof}
For all downsampled frequencies satisfying , Lemma \ref{lemma:singular value of sum of two matrices} and Lemma \ref{lemma:singular value and M} induce the fact:

In addition, for all downsampled frequencies with ,

As a result, if , it implies that sFFT-DT can correctly determine the number of collisions, ({\em i.e.}, distinguish the downsampled frequencies with  and those with , by finding the first  largest singular values).
\end{proof}

Remark: It should be noted that Theorem \ref{theorem:successfuly determine a} only holds for all downsampled frequencies with . The probability that there is no downsampled frequency with  shown in Lemma \ref{lemma:probability of bin} is at most . By setting  larger, it means that Theorem \ref{theorem:successfuly determine a} holds with higher probability as .



In fact, Theorem \ref{theorem:successfuly determine a} also reveals that fact that it is more difficult to satisfy  when  becomes large enough.
In other words, one needs to force  such that  in order to correctly determine .

\subsubsection{Step 2 of Pruning}
After determining 's for all downsampled frequencies, Step 2 of the pruning strategy runs the following procedure to know which locations should be pruned:
\begin{itemize}
  \item[(a).] Solve .
  \item[(b).] Let 
  \item[(c).]  is the set of collecting all  with the first  smallest .
\end{itemize}
This procedure is similar to syndrome decoding except that Step (ii) in Sec. \ref{ssec:MPT} is changed.
As mentioned above, due to the ill-conditioned problem such as Wilkinson's polynomial, the problem of approximating the roots, given the coefficients with noisy perturbation, is ill-conditioned.
Instead of finding roots by solving the polynomial, since the set including all candidate roots, , is finite, we substitute all candidate roots in  into the polynomial and store the roots with the first  smallest errors in the set , as also adopted in \cite{Takos2008}.

\subsubsection{Step 3 of Pruning}
By feeding  into Step 3 of the pruning strategy, we can decide which columns of  should be pruned according to the following criterion.
If the root belongs to , its corresponding column is preserved; otherwise, it is pruned.
Finally, let  be the outcome after pruning and let it be used to replace  in Eq. (\ref{eq:CS Formulation}).




\subsection{Non-iterative sFFT-DT: Algorithm for Generally K-Sparse Signals}\label{Sec: algorithm for general K sparse}

For generally -sparse signals, sFFT-DT solves the aliasing problem once, as shown in Algorithm 2, which integrates the pruning strategy and CS-based approach.
The function \textbf{main} contains four parts.
For clarity, Fig. \ref{fig:flowchart} illustrates the flowchart of sFFT-DT for generally -sparse signals and we describe each part as follows.
Part 1: In Lines 2-9, several downsampled signals are generated for performing FFTs with different shift factors.
Specifically, the downsampled signals in Lines 2-5 are prepared for the pruning strategy and those in Lines 6-9 are used for CS recovery problem.
To distinguish between these two, the signals for Lines 2-5 are represented by  and those for Lines 6-9 are represented by .
Part 2: Lines 11-21 run the Step 1 of the pruning strategy and decide the number of significant terms in the downsampled frequencies.  is a set used to save all singular values of 's (defined in Eq. (\ref{eq:formulation of M for general })) corresponding to frequencies.
Part 3: Lines 23-25 run the Step 2 and Step 3 of the pruning strategy, where  collects the roots corresponding to insignificant terms.
According to , we can prune  and output .
Part 4: Given , Lines 26-28 solve the CS recovery problem by Subspace Pursuit, as mentioned in Sec. \ref{Sec: Refinement}.

\begin{figure}[t]
\begin{minipage}[b]{.98\linewidth}
\centering{\epsfig{figure=flowchart_G.eps,width=5.0 in}}
\end{minipage}
\hfill
\caption{Flowchart of sFFT-DT for generally -sparse signals.}
\label{fig:flowchart}
\end{figure}

\begin{algorithm}[h]
\fontsize{11pt}{0.9em}\selectfont
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\caption{sFFT-DT for generally -sparse signals.}
\label{Table:our algorithm for general K-sparse signal}
\begin{tabular}[t]{p{17.7cm}l}
\textbf{Input:} , ;\quad \textbf{Output:} ; \\
\textbf{Initialization:} , , , , ; \\
\hline\hline
01. \textbf{function} \textbf{main}()\\
02. \quad\textbf{for}  to  \\
03. \quad\quad  for ;\\
04. \quad\quad  for ;\\
05. \quad\textbf{end for}\\
06. \quad Generate  in Sec. \ref{Sec: Refinement};\\
07. \quad\textbf{for}  to  \\
08. \quad\quad  for ;\\
09. \quad\textbf{end for}\\
10. \quad Do FFT of all 's, 's to obtain 's and 's.\\
11. \quad \textbf{for}  to \\
12. \quad\quad \quad  for ; \\
13. \quad\quad \quad Use 's to form  defined in Sec. \ref{Sec: decide a}; \\
14. \quad\quad \quad Do SVD of  and put singular values into \\
\quad\quad\quad\quad \quad  the set ;    \\
15. \quad \textbf{end for} \\
16. \quad Find the first  largest singular values from \\
\quad\quad\quad  and save them as .\\
17. \quad \textbf{for}  to \\
18. \quad\quad \textbf{if} ( originates from the 'th frequency) \\
19. \quad \quad \quad  of the 'th frequency increases by 1;\\
20. \quad \quad \textbf{end if}\\
21. \quad \textbf{end for} \\
22. \quad \textbf{for}  to \\
23. \quad \quad  for ; \\
24. \quad \quad Run Step 2 and 3 of pruning strategy in Sec. \ref{Sec: decide a};\\
25. \quad \quad and output .\\
26. \quad \quad  for ; \\
27. \quad \quad Solve Eq. (\ref{eq:CS Formulation}) given  by SP and assign  \\
28. \quad \quad  for . \\
29. \quad \textbf{end for} \\
30. \textbf{end} \textbf{function}\\
\hline
\end{tabular}
\end{algorithm}
\renewcommand\arraystretch{1}

\begin{table}[t]
\fontsize{7.5pt}{1em}\selectfont
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{4pt}
\caption{The effect of pruning in terms of computational cost and recovery performance under  and  .}
\label{Table: pruning comparision}
\doublerulesep=2pt
\begin{tabular}[tc]{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
{\LARGE \textcolor{white}{o}}K& & &  & &  &  &  &  &  &  &  &  &  \\ \hline
Time Cost without Pruning (Sec)& 5.731& 4.287& 3.315 & 3.813 & 4.459 & 8.681 & 15.10 & 23.61 & 50.27 & 101.22 & 217.28 & 463.21 & 989.41  \\ \hline
Time Cost with Pruning (Sec)& 0.021&  0.022 & 0.033 & 0.053 & 0.101 & 0.211 & 0.321 & 0.674 & 1.237 & 2.524 & 5.138 & 9.918 &19.539 \\ \hline
 without Pruning (dB)& -66.1& -51.9& -36.4 & -24.6 & -13.9 & -2.37 & 11.34 & 21.6 & 28.7 & 29.3 & 29.7 & 29.9 & 29.9 \\ \hline
  with Pruning (dB)& 4.67& 10.1& 14.8 & 20.1 & 23.1 & 24.9 & 27.7 & 29.7 & 29.9 & 29.9 & 29.9 & 29.9 & 29.9 \\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\fontsize{7.5pt}{1em}\selectfont
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{4pt}
\caption{The effect of pruning in terms of computational cost and recovery performance under  and  .}
\label{Table: pruning comparision 2}
\doublerulesep=2pt
\begin{tabular}[tc]{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
{\LARGE \textcolor{white}{o}}K& & &  & &  &  &  &  &  &  &  &  & \\ \hline
Time Cost without Pruning (Sec)& 5.693& 4.436& 3.761 & 3.903 & 4.634 & 8.511 & 16.20 & 31.61 & 51.92 & 108.49 & 229.31 & 492.01 &1032.94 \\ \hline
Time Cost with Pruning (Sec)& 0.021&  0.023 & 0.031 & 0.056 & 0.097 & 0.187 & 0.335 & 0.622 & 1.343 & 2.724 & 5.605 & 10.492 &20.034 \\ \hline
 without Pruning (dB)& -66.4& -53.3& -40.4 & -28.2 & -15.9 & -4.97 & 9.19 & 19.3 & 19.9& 19.9& 19.9& 19.9& 19.9 \\ \hline
  with Pruning (dB)& 0.04& 1.56 & 6.78 & 12.1 & 16.4 & 18.1 & 19.3 & 19.7 & 19.9& 19.9& 19.9& 19.9& 19.9 \\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\fontsize{7.5pt}{1em}\selectfont
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{4pt}
\caption{The effect of pruning in terms of computational cost and recovery performance under  and  .}
\label{Table: pruning comparision 3}
\doublerulesep=2pt
\begin{tabular}[tc]{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
{\LARGE \textcolor{white}{o}}K& & &  & &  &  &  &  &  &  &  &  &  \\ \hline
Time Cost without Pruning (Sec)& 5.611& 4.627& 3.802 & 3.892 & 4.561 & 8.639 & 16.39 & 30.32 & 59.14& 124.12 & 273.21 & 522.52& 1095.42 \\ \hline
Time Cost with Pruning (Sec)& 0.023&  0.029 & 0.038 & 0.052 & 0.125 & 0.212 & 0.326 & 0.644 & 1.227 & 2.321 & 4.732 & 9.327 & 19.394\\ \hline
 without Pruning (dB)& -73.1& -60.6& -48.5 & -36.3 & -24.3 & -11.6 & 2.27 & 9.53 & 9.97 & 9.98 & 9.99 & 9.99 & 9.99\\ \hline
  with Pruning (dB)& -1.19& -0.41& 0.85 & 2.59 & 6.03 & 8.83 & 9.69 & 9.94 & 9.98 & 9.99 & 9.99 & 9.99 & 9.99  \\ \hline
\end{tabular}
\end{table}


\begin{figure*}[!t]
\begin{minipage}[b]{.48\linewidth}
  \centering{\epsfig{figure=ComputationNoise.eps,width=3.45in}}
  \centerline{(a)}
\end{minipage}
\begin{minipage}[b]{.48\linewidth}
  \centering{\epsfig{figure=ComutationFixedK_General.eps,width=3.45in}}
  \centerline{(b)}
\end{minipage}
\hfill
\caption{Comparison between non-iterative sFFT-DT and FFTW for generally -sparse signals. (a) Computational time vs. sparsity under  and . (b) Computational time vs. signal dimension under  and . }
\label{fig:general computational time}
\end{figure*}

\subsection{Computational Complexity of sFFT-DT for Generally -Sparse Signals}\label{Sec: Complexity  for General Ksparse Signal}
In this section, we analyze the computational cost of sFFT-DT for generally K-sparse signals based on Theorem \ref{theorem:probability of sfftdt v2} for the four parts of the Main function. 
Part 1 is to do FFT for downsampled signals, and it costs .
Part 2 solves SVD of  for each downsampled frequency.
Since SVD will totally run  times, Part 2 will cost , according to \cite{Angelika1988}.
Part 3 costs  for computing coefficients of polynomial and  for estimating  for all  in Sec. \ref{Sec: decide a}.
Finally, CS recovery problem in Part 4 depends on the cost of SP.
With the pruning strategy, SP costs .
Thus, the total cost in Part 4 is  since SP runs  times, as described in Sec. \ref{Sec: Refinement}.
Thus, the total computational cost of sFFT-DT is bounded by .

Consequently, the computational cost of sFFT-DT for generally -sparse signals still is impacted by  and  as in the exactly- sparse case.
If significant frequencies distribute uniformly, both  and  can be set based on Theorem \ref{theorem:probability of sfftdt v2}.
In this case, since  is a constant, the computational cost is bounded by Part 1 and Part 3, which is .
It should be noted that the Big-O constant of  is very small because only Step 2 of pruning in Line 24 involves  and the operation of estimating  for all  is simple.
Thus, as shown in our experimental results,  does not dominate the computational cost of sFFT-DT.
But the Big-O constants of the generally -sparse case are still larger than those of the exactly -sparse case because the former needs more syndromes.

\subsection{Simulation Results for Generally -Sparse Signals}\label{Sec: experimental result of General}
The simulation environment is similar to the one described in Sec. \ref{Sec: Experimental Results}.
We only compare sFFT-DT with FFTW because sFFT \cite{Haitham2012}\cite{Haitham2012_1} does not release the code and the code of sFFT for the generally -sparse case is difficult to implement (as mentioned in the footnote on Page 3).
Therefore, no experimental results for generally -sparse signals were shown in their papers or websites.

Here, the test signals were generated from the mixture Gaussian model as:
\small

\normalsize
where  is the active probability that decides which Gaussian model is used and . For each test signal, its significant terms is defined as , as described in Sec. \ref{Sec: sFFT-DT: General K-Sparse}, and  is the output signal obtained from sFFT-DT.
We also define
 as:
\small

\normalsize
where  is the function of calculating the mean squared error.
If , then  means the signal-to-noise ratio between significant terms and insignificant terms.
In our simulations, the parameter setting was , , and  ranges from  to  dB.

Tables \ref{Table: pruning comparision}, \ref{Table: pruning comparision 2}, and \ref{Table: pruning comparision 3}  show the efficiency of pruning.
We can see that sFFT-DT with pruning outperforms its counterpart without pruning in terms of computational cost and recovery performance.
The performance degrades when  becomes larger as predicted in Theorem \ref{theorem:sFFT performance without no pruning}.
Moreover, we can observe from Table \ref{Table: pruning comparision}  Table \ref{Table: pruning comparision 3} that  no matter  is, the condition for achieving perfect approximation in sFFT-DT, {\em i.e.}, , is always .
The phenomenon is consistent with the reconstruction error bound in Theorem \ref{theorem:sFFT performance without no pruning}. Specifically, the reconstruction error bound, , is affected by  and  .
However, when  is small,  and thus  is equal to .
In other words, the reconstruction error bound is linear to .
This is a good property as the reconstruction quality of sFFT-DT is inversely proportional to the energy of insignificant terms, .

The comparison of computational time between sFFT-DT and FFTW is depicted in Fig. \ref{fig:general computational time}.
Fig. \ref{fig:general computational time}(a) shows the results of computational time versus signal sparsity under fixed .
It is observed that sFFT-DT is remarkably faster than FFTW, except for the cases with .
Fig. \ref{fig:general computational time}(b) shows the results of computational time versus signal dimension under fixed .
It is apparent that the computational time of sFFT-DT is not related to .

In sum, compared with \cite{Haitham2012}\cite{Haitham2012_1}, the proposed sFFT-DT for generally -sparse signals is the first algorithm with the reasonable Big-O constants and is verified to be faster than FFTW.


\section{Conclusions}\label{Sec: Conclusions}
We have presented new sparse Fast Fourier Transform methods based on downsampling in the time domain (sFFT-DT) for both exactly -sparse and generally -sparse signals in this paper.
The accurate computational cost and theoretical performance lower bound of sFFT-DT are proven for exactly -sparse signals.
We also derive the Big-O constants of computational complexity of sFFT-DT and show that they are smaller than those of MIT's methods \cite{Haitham2012}\cite{Haitham2012_1}\cite{Ghazi2013}.
In addition, sFFT-DT is more hardware-friendly, compared with other algorithms, since all operations of sFFT-DT are linear and involved in an analytical solution.
On the other hand, previous works, such as \cite{Haitham2012}\cite{Haitham2012_1}\cite{Ghazi2013}, are based on the assumption that sparsity  is known in advance.
To address this issue, we proposed a simple solution to estimate  and relax this impractical assumption.
We show that the extra cost for deciding  is the same as that required for sFFT-DT with known .
Moreover, we extend sFFT-DT to generally -sparse signals in this paper.
To solve the interference from insignificant frequencies in aliasing, we first reformulate the aliasing problem as CS-based model solved by subspace pursuit and present a pruning strategy to further improve the recovery performance and computational cost.

Overall, theoretical complexity analyses and simulation results demonstrate that our sFFT-DT outperforms the state-of-the-art.




\section{Acknowledgment}
This work was supported by National Science Council under grants NSC 100-2628-E-001-005-MY2 and NSC 102-2221-E-001-022-MY2.


\section{Appendix}\label{Sec: Appendix}
\footnotesize
The analytical solution of solving Step (ii) in syndrome decoding with  is

Similarly, the solution with  is

Then, the solution with  is





\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



\bibliographystyle{IEEEbib}	\bibliography{refs}		

\end{document}
