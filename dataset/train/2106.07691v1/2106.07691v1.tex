\subsection{Automatically Solving APT}
\label{section-nap}

Since human studies can be time-consuming and costly, we trained a paraphrase generator to perform APT. We used T5$_{base}$ \cite{raffel2020exploring}, as it achieves SOTA on paraphrase generation \cite{niu2020unsupervised, bird2020chatbot, li2020agent} and trained it on TwitterPPDB (Section \ref{sec-related}). Our hypothesis was that if T5$_{base}$ is trained to maximize the APT reward (Equation \ref{eq-dollar}), its generated sentences will be more likely to be $APT$. We generated paraphrases for sentences in MSRP and those in TwitterPPDB itself, hoping that since T5$_{base}$ is trained on TwitterPPDB, it would generate better paraphrases ($MI$ with lower BLEURT) for sentences coming from there. The proportion of sentences generated by T5$_{base}$ is shown in Table \ref{tbl:proportion}. We call this dataset $AP_{T5}$, the generation of which involved two phases:

\noindent \textbf{Training:} To adapt T5$_{base}$ for APT, we implemented a custom loss function obtained from dividing the cross-entropy loss per batch by the total reward (again from Equation \ref{eq-dollar}) earned from the model's paraphrase generations for that batch, provided the model was able to reach a reward of at least 1. If not, the loss was equal to just the cross-entropy loss. We trained T5$_{base}$ on TwitterPPDB
for three epochs; each epoch took about 30 hours on one NVIDIA Tesla V100 GPU due to the CPU bound BLEURT component. More epochs \textit{may} help get better results, but our experiments showed that loss plateaus after three epochs.

\noindent \textbf{Generation:} Sampling, or randomly picking a next word according to its conditional probability distribution, introduces non-determinism in language generation. \citet{fan2018hierarchical} introduce top-k sampling, which filters $k$ most likely next words, and the probability mass is redistributed among only those $k$ words. Nucleus sampling (or top-p sampling) \cite{holtzman2020curious} reduces the options to the smallest possible set of words whose cumulative probability exceeds $p$, and the probability mass is redistributed among this set of words. Thus, the set of words changes dynamically according to the next word's probability distribution. We use a combination of top-k and top-p sampling with $k=120$ and $p=0.95$ in the interest of lexical and syntactic diversity in the paraphrases. For each sentence in the source dataset (MSRP\footnote{We use the official train split released by \citet{dolan2005automatically} containing 4076 sentence pairs.} and TwitterPPDB for $AP^M_{T5}$ and $AP^{Tw}_{T5}$ respectively), we perform five iterations, in each of which, we generate ten sentences. If at least one of these ten sentences passes $APT$, we continue to the next source sentence after recording all attempts and classifying them as $MI$ or non-$MI$. If no sentence in a maximum of 50 attempts passes $APT$, we record all attempts nonetheless, and move on to the next source sentence. For each increasing iteration for a particular source sentence, we increase $k$ by $20$, but we also reduce $p$ by $0.05$ to avoid vague guesses. Note the distribution of $MI$ and non-$MI$ in the source datasets does not matter because we use only the first sentence from the sentence pair.

\begin{figure*}[t]
    \centering
    \subfloat[Subfigure 1 list of figures text][$AP_H$]
        {
        \includegraphics[width=0.31\textwidth]{diagrams/app.png}
        \label{fig:subfig1}
        }
    \subfloat[Subfigure 2 list of figures text][$AP^M_{T5}$]
        {
        \includegraphics[width=0.31\textwidth]{diagrams/msrp.png}
        \label{fig:subfig2}
        }
    \subfloat[Subfigure 3 list of figures text][$AP^{Tw}_{T5}$]
        {
        \includegraphics[width=0.31\textwidth]{diagrams/twitterppdb.png}
        \label{fig:subfig3}
        }\\
    \caption{BLEURT distributions on adversarial datasets. All figures divide the range of observed scores into 100 bins. Note that $APT$ sentence pairs are also $MI$, whereas those labeled `MI' are not $APT$.}
    \label{fig:bleurt}
\end{figure*}

\vspace{-5pt}
\subsection{Dataset Properties}
T5$_{base}$ trained with our custom loss function generated $APT$-passing paraphrases for ($56.19\%$) of starting sentences. This is higher than we initially expected, considering how difficult APT proved to be for humans (Table \ref{tbl:proportion}). Noteworthy is that only $6.09\%$ of T5$_{base}$'s attempts were $APT$. This does not mean that the remaining $94\%$ of attempts can be discarded, since they amounted to the negative examples in the dataset.
Since we trained it on TwitterPPDB itself, we expected that T5$_{base}$ would generate better paraphrases, as measured by a higher chance of passing $APT$ on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table \ref{tbl:proportion}, which shows that T5$_{base}$ was able to generate an $APT$ passing paraphrase for 84.8\% of the sentences in TwitterPPDB.

The composition of the three adversarial datasets can be found in Table \ref{tbl:proportion}. These metrics are useful to understand the capabilities of T5$_{base}$ as a paraphrase generator and the ``paraphrasability" of sentences in MSRP and TwitterPPDB. For instance, T5$_{base}$'s attempts on TwitterPPDB tend to be $MI$ much less frequently than those on MSRP and human's attempts on MSRP + PPNMT. This might be because in an attempt to generate syntactically dissimilar sentences, the T5$_{base}$ paraphraser also ended up generating many semantically dissimilar ones as well.

To visualize the syntactic and lexical disparity of paraphrases in the three adversarial datasets, we present their BLEURT distributions in Figure \ref{fig:bleurt}. As might be expected, the likelihood of a sentence pair being $MI$ increases as BLEURT score increases (recall that $APT$-passing sentence pairs are simply $MI$ pairs with BLEURT scores $<= 0.5$), but Figure \ref{fig:bleurt} shows that the shape of this increase is not straightforward, and differs among the three datasets. 



\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c c|c|c|}
\hline
\textbf{No.} & \multirow{2}{*}{\begin{tabular}{@{}c@{}}\textbf{Source} \\ \textbf{Dataset}\end{tabular}} & \textbf{Source Sentence} & \textbf{Attempt} & \textbf{BLEURT} & \textbf{MI} \\
& & & & &\\
\hline
\hline
& & \multicolumn{2}{|c|}{$AP_H$} & & \\[4pt]
\cline{3-4}
1 & PPNMT &
So, can we please get out of here? & 
So is it okay if we please go? &
-0.064 & 1 \\[4pt]

2 & PPNMT &
You're crying. &
I did not cry &
-1.366 & 0 \\[4pt]

3 & PPNMT &
Treatment successful. &
The treatment was succesful. &
-0.871 & 1 \\[4pt]

4 & PPNMT &
Break a leg! &
Fracture a leg! &
0.408 & 1 \\[4pt]

5 & MSRP &
Two years later, the insurance coverage would begin. &
The insurance will start in two years &
0.281 & 1 \\[4pt]

6 & MSRP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}Evacuation went smoothly, although passengers \\ weren't told what was going on, Hunt said.\end{tabular}} &
Hunt told that Evacuation went smoothly. &
-0.298 & 0 \\
& & & & &\\[4pt]

\hline
\hline

& & \multicolumn{2}{|c|}{$AP_{T5}$} & & \\[4pt]
\cline{3-4}
7 & MSRP & 
Friday, Stanford (47-15) blanked the Gamecocks 8-0. &
Stanford (47-15) won 8-0 over the Gamecocks on Friday. &
0.206 & 1 \\[4pt]

8 & MSRP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}Revenue in the first quarter of the year dropped  \\ 15 percent from the same period a year earlier.\end{tabular}} &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}Revenue declined 15 percent in the first quarter \\ of the year from the same period a year earlier.\end{tabular}} &
0.698 & 1 \\
& & & & &\\[4pt]

9 & MSRP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}A federal magistrate in Fort Lauderdale ordered \\ him held without bail.\end{tabular}} &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}In Fort Lauderdale, Florida, a federal magistrate \\ ordered him held without bail.\end{tabular}} &
0.635 & 0 \\
& & & & &\\[4pt]

10 & TP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}16 innovations making a difference for poor \\ communities around the world.\end{tabular}} &
16 innovative ideas that tackle poverty around the world. &
0.317 & 1 \\
& & & & &\\[4pt]

11 & TP &
This is so past the bounds of normal or acceptable . &
This is so beyond the normal or acceptable boundaries. &
0.620 & 1 \\[4pt]

12 & TP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}The creator of Atari has launched a new VR company \\ called Modal VR.\end{tabular}} &
Atari creator is setting up a new VR company! &
0.106 & 0 \\
& & & & &\\[4pt]

\hline

\end{tabular}
}
\caption{Examples from adversarial datasets. The source dataset (TP short for TwitterPPDB) tells which dataset the sentence pair comes from (and whether it is in $AP^M_{T5}$ or $AP^{Tw}_{T5}$ for $AP_{T5}$). All datasets have $APT$ passing and failing $MI$ and non-$MI$ sentence pairs.}
\label{tbl:examples}
\end{table*}

As might be expected, humans are much more skilled at APT than T5$_{base}$, as shown by the fact that the paraphrases they generated have much lower mean BLEURT scores (Figure \ref{fig:bleurt}), and the ratio of $APT$ vs non-$APT$ sentences is much higher (Table \ref{tbl:proportion}). As we saw earlier, when T5$_{base}$ wrote paraphrases that were low on BLEURT, they tended to become non-$MI$ (e.g., line 12 in Table \ref{tbl:examples}). However, T5$_{base}$ did generate more $APT$-passing sentences with a lower BLEURT on Twitter-PPDB than on MSRP, which may be a result of overfitting T5$_{base}$ on TwitterPPDB. Furthermore, all three adversarial datasets have a distribution of $MI$ and non-$MI$ sentence pairs balanced enough to train a model to identify paraphrases.

Table 3 has examples from $AP_H$ and $AP_{T5}$ showing the merits and shortcomings of T5, BLEURT, and RoBERTa$_{large}$ (the MI detector used). Some observations from Table \ref{tbl:examples} include:
\begin{itemize}
    \item \textit{Lines 1 and 3:} BLEURT did not recognize the paraphrases, possibly due to the differences in words used. RoBERTa$_{large}$ however, gave the correct MI prediction (though it is worth noting that the sentences in line 1 are questions, rather than truth-apt propositions).
    \item \textit{Line 4:} RoBERTa$_{large}$ and BLEURT (to a large extent since it gave it a score of 0.4) did not recognize that the idiomatic phrase `break a leg' means `good luck' and not `fracture.'
    \item \textit{Lines 6 and 12:} There is a loss of information going from the first sentence to the second and BLEURT and MI both seem to have understood the difference between summarization and paraphrasing.
    \item \textit{Line 7:} T5 not only understood the scores but also managed to paraphrase it in such a way that was not syntactically and lexically similar, just as we wanted T5 to do when we fine-tuned it.
    \item \textit{Line 9:} T5$_{base}$ knows that Fort Lauderdale is in Florida but RoBERTa$_{large}$ does not.
\end{itemize}

%
