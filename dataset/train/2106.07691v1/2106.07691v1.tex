\subsection{Automatically Solving APT}
\label{section-nap}

Since human studies can be time-consuming and costly, we trained a paraphrase generator to perform APT. We used T5 \cite{raffel2020exploring}, as it achieves SOTA on paraphrase generation \cite{niu2020unsupervised, bird2020chatbot, li2020agent} and trained it on TwitterPPDB (Section \ref{sec-related}). Our hypothesis was that if T5 is trained to maximize the APT reward (Equation \ref{eq-dollar}), its generated sentences will be more likely to be . We generated paraphrases for sentences in MSRP and those in TwitterPPDB itself, hoping that since T5 is trained on TwitterPPDB, it would generate better paraphrases ( with lower BLEURT) for sentences coming from there. The proportion of sentences generated by T5 is shown in Table \ref{tbl:proportion}. We call this dataset , the generation of which involved two phases:

\noindent \textbf{Training:} To adapt T5 for APT, we implemented a custom loss function obtained from dividing the cross-entropy loss per batch by the total reward (again from Equation \ref{eq-dollar}) earned from the model's paraphrase generations for that batch, provided the model was able to reach a reward of at least 1. If not, the loss was equal to just the cross-entropy loss. We trained T5 on TwitterPPDB
for three epochs; each epoch took about 30 hours on one NVIDIA Tesla V100 GPU due to the CPU bound BLEURT component. More epochs \textit{may} help get better results, but our experiments showed that loss plateaus after three epochs.

\noindent \textbf{Generation:} Sampling, or randomly picking a next word according to its conditional probability distribution, introduces non-determinism in language generation. \citet{fan2018hierarchical} introduce top-k sampling, which filters  most likely next words, and the probability mass is redistributed among only those  words. Nucleus sampling (or top-p sampling) \cite{holtzman2020curious} reduces the options to the smallest possible set of words whose cumulative probability exceeds , and the probability mass is redistributed among this set of words. Thus, the set of words changes dynamically according to the next word's probability distribution. We use a combination of top-k and top-p sampling with  and  in the interest of lexical and syntactic diversity in the paraphrases. For each sentence in the source dataset (MSRP\footnote{We use the official train split released by \citet{dolan2005automatically} containing 4076 sentence pairs.} and TwitterPPDB for  and  respectively), we perform five iterations, in each of which, we generate ten sentences. If at least one of these ten sentences passes , we continue to the next source sentence after recording all attempts and classifying them as  or non-. If no sentence in a maximum of 50 attempts passes , we record all attempts nonetheless, and move on to the next source sentence. For each increasing iteration for a particular source sentence, we increase  by , but we also reduce  by  to avoid vague guesses. Note the distribution of  and non- in the source datasets does not matter because we use only the first sentence from the sentence pair.

\begin{figure*}[t]
    \centering
    \subfloat[Subfigure 1 list of figures text][]
        {
        \includegraphics[width=0.31\textwidth]{diagrams/app.png}
        \label{fig:subfig1}
        }
    \subfloat[Subfigure 2 list of figures text][]
        {
        \includegraphics[width=0.31\textwidth]{diagrams/msrp.png}
        \label{fig:subfig2}
        }
    \subfloat[Subfigure 3 list of figures text][]
        {
        \includegraphics[width=0.31\textwidth]{diagrams/twitterppdb.png}
        \label{fig:subfig3}
        }\\
    \caption{BLEURT distributions on adversarial datasets. All figures divide the range of observed scores into 100 bins. Note that  sentence pairs are also , whereas those labeled `MI' are not .}
    \label{fig:bleurt}
\end{figure*}

\vspace{-5pt}
\subsection{Dataset Properties}
T5 trained with our custom loss function generated -passing paraphrases for () of starting sentences. This is higher than we initially expected, considering how difficult APT proved to be for humans (Table \ref{tbl:proportion}). Noteworthy is that only  of T5's attempts were . This does not mean that the remaining  of attempts can be discarded, since they amounted to the negative examples in the dataset.
Since we trained it on TwitterPPDB itself, we expected that T5 would generate better paraphrases, as measured by a higher chance of passing  on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table \ref{tbl:proportion}, which shows that T5 was able to generate an  passing paraphrase for 84.8\% of the sentences in TwitterPPDB.

The composition of the three adversarial datasets can be found in Table \ref{tbl:proportion}. These metrics are useful to understand the capabilities of T5 as a paraphrase generator and the ``paraphrasability" of sentences in MSRP and TwitterPPDB. For instance, T5's attempts on TwitterPPDB tend to be  much less frequently than those on MSRP and human's attempts on MSRP + PPNMT. This might be because in an attempt to generate syntactically dissimilar sentences, the T5 paraphraser also ended up generating many semantically dissimilar ones as well.

To visualize the syntactic and lexical disparity of paraphrases in the three adversarial datasets, we present their BLEURT distributions in Figure \ref{fig:bleurt}. As might be expected, the likelihood of a sentence pair being  increases as BLEURT score increases (recall that -passing sentence pairs are simply  pairs with BLEURT scores ), but Figure \ref{fig:bleurt} shows that the shape of this increase is not straightforward, and differs among the three datasets. 



\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c c|c|c|}
\hline
\textbf{No.} & \multirow{2}{*}{\begin{tabular}{@{}c@{}}\textbf{Source} \\ \textbf{Dataset}\end{tabular}} & \textbf{Source Sentence} & \textbf{Attempt} & \textbf{BLEURT} & \textbf{MI} \\
& & & & &\\
\hline
\hline
& & \multicolumn{2}{|c|}{} & & \4pt]

2 & PPNMT &
You're crying. &
I did not cry &
-1.366 & 0 \4pt]

4 & PPNMT &
Break a leg! &
Fracture a leg! &
0.408 & 1 \4pt]

6 & MSRP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}Evacuation went smoothly, although passengers \\ weren't told what was going on, Hunt said.\end{tabular}} &
Hunt told that Evacuation went smoothly. &
-0.298 & 0 \\
& & & & &\4pt]
\cline{3-4}
7 & MSRP & 
Friday, Stanford (47-15) blanked the Gamecocks 8-0. &
Stanford (47-15) won 8-0 over the Gamecocks on Friday. &
0.206 & 1 \4pt]

9 & MSRP &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}A federal magistrate in Fort Lauderdale ordered \\ him held without bail.\end{tabular}} &
\multirow{2}{*}{\begin{tabular}{@{}c@{}}In Fort Lauderdale, Florida, a federal magistrate \\ ordered him held without bail.\end{tabular}} &
0.635 & 0 \\
& & & & &\4pt]

11 & TP &
This is so past the bounds of normal or acceptable . &
This is so beyond the normal or acceptable boundaries. &
0.620 & 1 \4pt]

\hline

\end{tabular}
}
\caption{Examples from adversarial datasets. The source dataset (TP short for TwitterPPDB) tells which dataset the sentence pair comes from (and whether it is in  or  for ). All datasets have  passing and failing  and non- sentence pairs.}
\label{tbl:examples}
\end{table*}

As might be expected, humans are much more skilled at APT than T5, as shown by the fact that the paraphrases they generated have much lower mean BLEURT scores (Figure \ref{fig:bleurt}), and the ratio of  vs non- sentences is much higher (Table \ref{tbl:proportion}). As we saw earlier, when T5 wrote paraphrases that were low on BLEURT, they tended to become non- (e.g., line 12 in Table \ref{tbl:examples}). However, T5 did generate more -passing sentences with a lower BLEURT on Twitter-PPDB than on MSRP, which may be a result of overfitting T5 on TwitterPPDB. Furthermore, all three adversarial datasets have a distribution of  and non- sentence pairs balanced enough to train a model to identify paraphrases.

Table 3 has examples from  and  showing the merits and shortcomings of T5, BLEURT, and RoBERTa (the MI detector used). Some observations from Table \ref{tbl:examples} include:
\begin{itemize}
    \item \textit{Lines 1 and 3:} BLEURT did not recognize the paraphrases, possibly due to the differences in words used. RoBERTa however, gave the correct MI prediction (though it is worth noting that the sentences in line 1 are questions, rather than truth-apt propositions).
    \item \textit{Line 4:} RoBERTa and BLEURT (to a large extent since it gave it a score of 0.4) did not recognize that the idiomatic phrase `break a leg' means `good luck' and not `fracture.'
    \item \textit{Lines 6 and 12:} There is a loss of information going from the first sentence to the second and BLEURT and MI both seem to have understood the difference between summarization and paraphrasing.
    \item \textit{Line 7:} T5 not only understood the scores but also managed to paraphrase it in such a way that was not syntactically and lexically similar, just as we wanted T5 to do when we fine-tuned it.
    \item \textit{Line 9:} T5 knows that Fort Lauderdale is in Florida but RoBERTa does not.
\end{itemize}

%
