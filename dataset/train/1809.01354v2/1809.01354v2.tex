\section{Introduction}




Human matting, which aims at extracting humans from natural images with high quality, has a wide variety of applications, such as mixed reality, smart creative composition, live streaming, film production, \emph{etc}.
For example, in an e-commerce website, smart creative composition provides personalized creative image to customers.
This requires extracting fashion models from huge amount of original images and re-compositing them with new creative designes.
In such a scenario, due to the huge volume of images to be processed and in pursuit of a better customer experience,
it is critical to have an automatic high quality extraction method.
Fig.~\ref{human_matting} gives an example of smart creative composition with automatic human matting in a real-world e-commerce website.

Designing such an automatic method is not a trivial task.
One may think of turning to either semantic segmentation or image matting techniques.
However, neither of them can be used by itself to reach a satisfactory solution.
On the one hand, semantic segmentation, which directly identifies the object category of each pixel, 
usually focuses on the coarse semantics and is prone to blurring structural details.
On the other hand, image matting, widely adopted for fine detail extractions, usually requires user interactions and therefore is not suitable in data-intensive or time-sensitive scenarios such as smart creative composition.
More specifically,
for an input image , matting is formulated as a decomposition into foreground , background  and alpha matte  with a linear blend assumption:

where for color images, there are 7 unknown variables but only 3 known variables, and thus, this decomposition is severely under-constrained~\cite{levin2008closed}.
Therefore, most matting algorithms~\cite{levin2008closed,chen2013knn,aksoy2017designing,xu2017deep} need to take user designated trimaps or scribbles as extra constraints.







\begin{figure}
  \centering
\includegraphics[width=0.99\linewidth]{fig/intro_4.jpeg}\\
  \caption{Semantic Human Matting (SHM) and its applications. SHM takes natural image (top left) as input and outputs corresponding alpha matte (bottom left). The predicted alpha matte could be applied to background editting (top right) and smart creative composition (bottom right)}
  \label{human_matting}
\end{figure}




In this paper, we propose a unified method, Semantic Human Matting (SHM), which integrates a semantic segmentation module with a deep learning based matting module to automatically extract the alpha matte of humans.
The learned semantic information distinguishing foreground and background is employed as an implicit constraint
to a deep matting network which complements the capability of detail extraction.
A straightforward way to implement such a method is to train these two modules separately and feed the segmentation results as trimaps into the matting network.
However, this intuitive approach does not work well~\cite{shen2016deep}.
The reason is that 
semantic segmentation aims at classifying each pixel and is able to roughly distinct humans from background, whereas the goal of matting is to assign to each pixel a more fine grained float opacity value of foreground without determining the semantics.
They are responsible for recovering coarse segmentations and fine details respectively, and therefore they need to be carefully handled in order to cooperate properly towards high quality human matting.
Shen \emph{et al.}~\cite{shen2016deep} use a closed form matting~\cite{levin2008closed} layer through which the semantic information can directly propagate and constitute the final result.
But with deep learning based matting, the matting module is highly nonlinear and trained to focus on structural patterns of details, thus the semantic information from input hardly retains.
To combine the coarse semantics and fine matting details exquisitely, we propose a novel fusion strategy which naturally gives a probabilistic estimation of the alpha matte.
It can be viewed as an adaptive ensemble of both high and low level results on each pixel.
Further, with this strategy, the whole network automatically amounts the final training error to the coarse and the fine, and thus can be trained in an end-to-end fashion.








We also constructed a very large dataset with high quality annotations for the human matting task.
Since annotating details is difficult and time-consuming, high quality datasets for human matting are valuable and scarce.
The most popular alphamatting.com dataset~\cite{rhemann2009perceptually} has made significant contributions to the matting research. Unfortunately it only consists of 27 training images and 8 testing images.
Shen \emph{et al}.~\cite{shen2016deep} created a dataset of 2,000 images, but it only contains portrait images.
Besides, the groundtruth images of this dataset are generated with closed form matting~\cite{rhemann2009perceptually} and KNN matting~\cite{chen2013knn} and therefore can be potentially biased.
Recently, Xu \emph{et al}.~\cite{xu2017deep} built a large high quality matting dataset, with 202 distinct human foregrounds.
To increase the volume and diversity of human matting data that benefit the learning and evaluation of human matting, we collected another 35,311 distinct human images with fine matte annotations.
All human foregrounds are composited with different backgrounds and the final dataset includes 52,511 images for training and 1,400 images for testing.
More details of this dataset will be discussed in Section~\ref{dataset}.





Extensive experiments are conducted on this dataset to empirically evaluate the effectiveness of our method.
Under the commonly used metrics of matting performance, our method can achieve comparable results with the state-of-the-art interactive matting methods~\cite{levin2008closed,chen2013knn,aksoy2017designing,xu2017deep}.
Moreover, we demonstrate that our learned model generalizes to real images with justifying plenty of natural human images crawled from the Internet.

To summarize, the main contributions of our work are three-fold:

1. To the best of our knowledge, SHM is the first automatic matting algorithm that learns to jointly fit both semantic information and high quality details with deep networks.
Empirical studies show that SHM achieves comparable results with the state-of-the-art interactive matting methods.

2. A novel fusion strategy, which naturally gives a probabilistic estimation of the alpha matte, is proposed to make the entire network properly cooperate.
It adaptively ensembles coarse semantic and fine detail results on each pixel which is crucial to enable end-to-end training.

3. A large scale high quality human matting dataset is created.
It contains 35,513 unique human images with corresponding alpha mattes.
The dataset not only enables effective training of the deep network in SHM but also contributes with its volume and diversity to the human matting research.

\section{Related works}



In this section, we will review semantic segmentation and image matting methods that most related to our works.

Since Long \emph{et al}. \cite{long2015fully} use Fully Convolutional Network (FCN) to predict pixel level label densely and improve the segmentation accuracy by a large margin, FCN has became the main framework for semantic segmentation and kinds of techniques have been proposed by researchers to improve the performance. Yu \emph{et al}.~\cite{yu2015multi} propose dilated convolutions to increase the receptive filed of the network without spatial resolution decrease, which is demonstrated effective for pixel level prediction. Chen \emph{et al}.~\cite{chen2016deeplab} add fully connected CRFs on the top of network as post processing to alleviate the "hole" phenomenon of FCN. In PSPNet~\cite{zhao2017pyramid}, network-in pyramid pooling module is proposed to acquire global contextual prior. Peng \emph{et al}.~\cite{peng2017large} state that using large convolutional kernels and boundary refinement block can improve the pixel level classification accuracy while maintaining precise localization capacity. With the above improvements, FCN based models trained on large scale segmentation datasets, such as VOC \cite{pascal-voc-2012} and COCO \cite{lin2014microsoft}, have achieved the top performances in semantic segmentation. However, these models can not be directly applied to semantic human matting for the following reasons.
1) The annotations of current segmentation datasets are relative "coarse" and "hard" to matting task. Models trained on these datasets do not satisfy the accuracy requirement of pixel level location and floating level alpha values for matting.
2) Pixel level classification accuracy is the only consideration during network architecture and loss design in semantic segmentation. This leads the model prone to blurring complex structural details which is crucial for matting performance.

In the past decades, researchers have developed variety of general matting methods for natural images. Most methods predict the alpha mattes through sampling \cite{chuang2001bayesian,wang2007optimized,gastal2010shared,he2011global,shahrian2013improving} or propagating \cite{sun2004poisson,grady2005random,levin2008closed,chen2013knn,aksoy2017designing} on color or low-level features. With the rise of deep learning in computer vision community, several CNN based methods \cite{cho2016natural,xu2017deep} have been proposed for general image matting. Cho \emph{et al}. \cite{cho2016natural} design a convolutional neural network to reconstruct the alpha matte by taking the results of the closed form matting \cite{levin2008closed} and KNN matting \cite{chen2013knn} along with the normalized RGB color image as inputs. Xu \emph{et al}. \cite{xu2017deep} directly predict the alpha matte with a pure encoder decoder network which takes the RGB image and trimap as inputs and achieve the state-of-the-art results.
However, all the above general image matting methods need scribbles or trimap obtained from user interactions as constraints and so they can not be applied in automatic way.

Recently, several works \cite{shen2016deep,zhu2017fast} have been proposed to make an automatic matting system.
Shen \emph{et al}. \cite{shen2016deep} use closed form matting~\cite{levin2008closed} with CNN to automatically obtain the alpha mattes of portrait images and back propagate the errors to the deep convolutional network. Zhu et al.~\cite{zhu2017fast} follow the similar pipeline while designing a smaller network and a fast filter similar to guided filter~\cite{he2010guided} for matting to deploy the model on mobile phones.
Despite our method and the above two works both use CNNs to learn semantic information instead of manual trimaps to automate the matting process, our method is quite different from theirs:

\begin{figure*}[hbt]
  \includegraphics[width=0.95\linewidth]{fig/dataset_intro.jpg}
  \caption{\label{dataset_intro} Compositional images and corresponding alpha mattes in our dataset. The first three columns come from the general matting dataset create by Xu \emph{et al}. \cite{xu2017deep} and the last three columns come from model images we collected from an e-commerce website. It's worth notting that images shown here are all resized to the same height.}
\end{figure*}

1) Both the above methods use the traditional methods as matting module, which compute the alpha matte by solving the matting equation (Eq.~\ref{equa_composition}) and may introduce artifacts when the distributions of foreground and background color overlap~\cite{xu2017deep}.
We employ a FCN as matting module so as to directly learn complex details in a wide context which have been shown much more robust by~\cite{xu2017deep}.
2) By solving the matting equation, these method can directly affect the final perdition with the input constraints and thus propagate back the errors.
However, when the deep matting network is adopted, the cooperation of coarse semantics and find details must be explicitly handled.
Thus a novel fusion strategy is proposed and enables the end-to-end training of the entire network.




























\section{Human matting dataset}\label{dataset}


\begin{table}[t]
\caption{Configuration of our human matting dataset.}
\centering
\begin{tabular}{c cc cc}
\toprule
   \multirow{2}{*}{Data Source}  & \multicolumn{2}{c}{Train Set}  & \multicolumn{2}{c}{Test Set} \\
             &   Foreground         & Image  &   Foreground    & Image \\ \midrule
   DIM\cite{xu2017deep} &  182        & 18,200      &   20    & 400   \\
   Model                &  34,311      & 34,311      &   1,000  & 1,000  \\
   Total                &  34,493      & 52,511      &   1,020  & 1,400  \\ \bottomrule

\end{tabular}
\label{tab:dataset_config}
\end{table}




As a newly defined task in this paper, the first challenge is that semantic human matting encounters the lack of data.
To address it, we create a large scale high quality human matting dataset.
The foregrounds in this dataset are humans with some accessories(\emph{e.g.}, cellphones, handbags).
And each foreground is associated with a carefully annotated alpha matte.
Following Xu \emph{et al}.\cite{xu2017deep}, foregrounds are composited onto new backgrounds to create a human matting dataset with 52,511 images in total.
Some sample images in our dataset are shown in Fig. \ref{dataset_intro}.

In details, the foregrounds and corresponding alpha matte images in our dataset comprise:
\begin{itemize}

\item \textbf{Fashion Model dataset.}
 More than 188k fashion model images are collected from an e-commerce website, whose alpha mattes are annotated by sellers in accordance with
 commercial quality. Volunteers are recruited to carefully inspect and double-check the mattes to remove those even only with small flaws. It takes almost 1,200 hours to select 35,311 images out of them. The low pass rate (18.88 \%) guarantees the high standard of the alpha matte in our dataset.


\item \textbf{Deep Image Matting (DIM) dataset~\cite{xu2017deep}.} We also select all the images that only contain human from DIM dataset, resulting 202 foregrounds.
\end{itemize}
The background images are  from COCO dataset and the Internet.
We ensure that background images do not contain humans.
The foregrounds are split into train/test set, and the configuration is shown in Table \ref{tab:dataset_config}.
Following \cite{xu2017deep}, each foreground is composited with N backgrounds.
For foregrounds from Fashion Model dataset, due to their large number, N is set to 1 for both training and testing dataset.
For foregrounds from DIM dataset, N is set to 100 for training dataset and 20 for testing dataset, as in \cite{xu2017deep}.
All the background images are randomly selected and unique.

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table}[t]
\caption{The properties of the existing matting datasets.}
\centering
\begin{tabular}{cccc}
  \toprule
  Datasets & Foreground  & Image & Annotation \\
  \midrule
  alpha matting~\cite{rhemann2009perceptually}    &    35 Objects       &    35   &  Manually \\  
  Shen et al.~\cite{shen2016deep} &  2,000 Portraits       &   2,000    & CF \cite{levin2008closed}, KNN \cite{chen2013knn} \\
DIM~\cite{xu2017deep}  &  493 Objects  &   49,300  & Manually  \\
  Our dataset &  35,513 Humans      & 52,511        & Manually  \\ 
  \bottomrule
\end{tabular}
\label{tab:dataset_components}
\end{table}

\begin{figure*}[hbt]
  \includegraphics[width=0.95\linewidth]{fig/pipeline-new-2}
  \caption{Overview of our semantic human matting method.
  	Given an input image, a \emph{T-Net}, which is implemented as PSPNet-50, is used to predict the 3-channel trimap. The predicted trimap is then concatenated with the original image and fed into the \emph{M-Net} to predict the raw alpha matte.
  	Finally, both the predicted trimap and raw alpha matte are fed into the \emph{Fusion Module} to generate the final alpha matte according to Eq. \ref{equa_t_m_fusion}.
  	The entire network is trained in an end-to-end fashion.
}
  \label{pipeline}
\end{figure*}

Table \ref{tab:dataset_components} shows the comparisons of basic properties between existing matting datasets and ours. Compared with previous matting datasets, our dataset differs in the following aspects:
1) The existing matting datasets contain hundreds of foreground objects, while our dataset contains 35,513 different foregrounds which is much larger than others;
2) In order to deal with the human matting task, foregrounds containing human body are needed.
However, DIM\cite{xu2017deep} dataset only contains 202 human objects.
The dataset proposed by Shen et al. \cite{shen2016deep} consists of portraits, which are limited to heads and part of shoulders.
In contrast, our dataset has a larger diversity that might cover the whole human body, \emph{i.e.} head, arms, legs \emph{etc}. in various poses, which is essential for human matting;
3) Unlike the dataset of Shen et al \cite{shen2016deep} which is annotated by Closed From \cite{levin2008closed}, KNN \cite{chen2013knn} and therefore can be potentially biased, all 35,513 foreground objects are manually annotated and carefully inspected, which guarantees the high quality alpha mattes and ensures the semantic integrity and unicity.
The dataset not only enables effective training of the deep network in SHM but also contributes with its volume and diversity to the human matting research.

\begin{comment}

We would like to highlight the following features of our human matting datasets:
1) Our dataset is the first large scale human matting dataset with high quality alpha mattes, which also ensure the semantic integrity and unicity; 
2) The dataset has more than 35k unique human instances, which not only is large in number, but also has a large diversity including different age, poses, \emph{etc};
3) The compositional images in this dataset have various complex backgrounds, which makes them close to natural images. 
Therefore, our dataset is more suitable for the human matting task than others.

\end{comment}







\section{Our method}


Our SHM is targeted to automatically pull the alpha matte of a specific semantic pattern---the humans.
Fig~\ref{pipeline} shows its pipeline.
The SHM takes an image (usually 3 channels representing RGB) as input, and directly outputs a 1-channel alpha matte image with identical size of input.
Note that no auxiliary information (\emph{e.g.} trimap and scribbles) is required.

SHM aims to simultaneously capture both coarse semantic classification information and fine matting details.
We design two subnetworks to separately handle these two tasks.
The first one, named \emph{T-Net}, is responsible to do pixel-wise classification among foreground, background and unknown regions; while the second one, named \emph{M-Net}, takes in the output of \emph{T-Net} as semantic hint and describes the details by generating the raw alpha matte image.
The outputs of \emph{T-Net} and \emph{M-Net} are fused by a novel \emph{Fusion Module} to generate the final alpha matte result.
The whole networks are trained jointly in an end-to-end manner.
We describe these submodules in detail in the following subsections.

\subsection{Trimap generation: T-Net}

The \emph{T-Net} plays the role of semantic segmentation in our task and roughly extract foreground region.
Specifically, we follow the traditional trimap concept and define a 3-class segmentation---the foreground, background and unknown region.
Therefore, the output of \emph{T-Net} is a 3-channel map indicating the possibility that each pixel belongs to each of the 3 classes.
In general, \emph{T-Net} can be implemented as any of the state-of-the-art semantic segmentation networks~\cite{long2015fully,yu2015multi,chen2016deeplab,zhao2017pyramid,peng2017large}.
In this paper, we choose PSPNet-50~\cite{zhao2017pyramid} for its efficacy and efficiency.


\subsection{Matting network: M-Net}



Similar to general matting task~\cite{xu2017deep}, the \emph{M-Net} aims to capture detail information and generate alpha matte.
The \emph{M-Net} takes the concatenation of 3-channel images and the 3-channel segmentation results from \emph{T-Net} as 6-channel input.
Note that it differs from DIM~\cite{xu2017deep} which uses 3-channel images plus 1-channel trimap (with 1, 0.5, 0 to indicate foreground, unknown region and background respectively) as 4-channel input.
We use 6-channel input since it conveniently fits the output of \emph{T-Net} and we empirically find that with 6-channel or 4-channel input have nearly equal performance.







As shown in Fig.~\ref{pipeline}, the \emph{M-Net} is a deep convolutional encoder-decoder network. The encoder network has 13 convolutional layers and 4 max-pooling layers, while the decoder network has 6 convolutional layers and 4 unpooling layers. The hyper-parameters of encoder network are the same as the convolutional layers of VGG16 classification network expect for the "conv1" layer in VGG16 that has 3 input channels whereas 6 in our M-Net.
The structure of \emph{M-Net} differs from DIM~\cite{xu2017deep} in following aspects:
1) \emph{M-Net} has 6-channel instead of 4-channel inputs; 2) Batch Normalization is added after each convolutional layer to accelerate convergence; 3) "conv6" and "deconv6" layers are removed since these layers have large number of parameters and are prone to overfitting.




\begin{comment}
	
\section{Our method}



Motivated by the divide and conquer idea, we carefully design a deep fully convolutional network that contains three stages (see Fig. \ref{pipeline}) to address the semantic human matting. The first stage, named T-Net in this paper, is a fully convolutional network in charge of trimap generation.
The second stage, named M-Net, is a convolutional encoder-decoder network in charge of structural details matting. It first concatenates the original RGB image with the predicted
trimap along the channel dimension to form a 6-channel image as input, and then predict a raw alpha matte. As shown in Fig. \ref{pipeline} (c), it is obvious that the M-Net successfully learns the opacity of human edges and structural details in the transitional regions of foreground and background, but may contains some unexpected noise in the definite foreground and background, which exactly coincides with our design intent that M-Net only concentrates on the transitional regions. The third stage, named T-M fusion layer, is a differentiable trimap-matte fusion operation in charge of automatic fusion the result of the predicted trimap from stage one and raw alpha matte from stage two. In our experiments, we find it is not easy to directly train the entire deep network in Fig. \ref{pipeline} end to end only with the alpha matte supervision. Therefore, we design a three stage training strategy that works well for our method: 1) we first train the T-Net as a three class segmentation task with the supervision of trimaps generated from the alpha mattes until it converged in this stage. 2) Instead of directly using the trimaps predicted by the trained T-Net, we use the trimaps generated from alpha mattes with RGB images as inputs to train the M-Net. 3) After T-Net and M-Net are both converged in stage one and stage two, we
we train the train the entire network end to end with the aid of T-M fusion layer, generated trimap and alpha mattes supervision. We will describe our network and the training strategy in details in the following sections.





\begin{figure*}[hbt]
  \includegraphics[width=0.95\linewidth]{fig/pipeline.png}
  \caption{\label{pipeline} Overview of our semantic human matting method. Given an input image (a), firstly, we use T-Net to predict the trimap (b). Then we concatenate the input image (a) and predicted trimap (b) as inputs of M-Net to predict the raw alpha matte (c). Finally, we feed the predicted trimap (b) and raw alpha matte (c) to the T-M (Trimap-Matte) fusion layer to generate the final alpha matte (d). We train the whole network in an end-to-end fashion.  and  denote feature map concatenation and gather operation respectively. The red region, green region and black region in image (b) denote foreground, background and unknown region of predicted trimap respectively.}
\end{figure*}


\begin{figure*}[hbt]
  \includegraphics[width=0.95\linewidth]{fig/pipeline-new}
  \caption{\label{pipeline} Overview of our semantic human matting method. Given an input image (a), firstly, we use T-Net to predict the trimap (b). Then we concatenate the input image (a) and predicted trimap (b) as inputs of M-Net to predict the raw alpha matte (c). Finally, we feed the predicted trimap (b) and raw alpha matte (c) to the T-M (Trimap-Matte) fusion layer to generate the final alpha matte (d). We train the whole network in an end-to-end fashion.  and  denote feature map concatenation and gather operation respectively. The red region, green region and black region in image (b) denote foreground, background and unknown region of predicted trimap respectively.}
\end{figure*}


\subsection{Trimap generation stage: T-Net}
The first stage of our method is T-Net. In fact, T-Net acts the role of semantic segmentation in our task. Therefore, we can utilize any the state of the art semantic segmentation networks \cite{long2015fully,yu2015multi,chen2016deeplab,zhao2017pyramid,peng2017large}. Considering the trade off between effect and efficiency, in this paper, we choose PSPNet50 \cite{zhao2017pyramid} for its effectiveness and easy implementation. It utilize some successful network designs in classification and semantic segmentation in previous works, such as residual connection proposed in \cite{he2016deep}, dilated convolution proposed in \cite{yu2015multi}. In order to capture and fuse the global contextual prior, it creates a network-in pyramid pooling module  on the top of resnet50 to improve the segmentation performance. We refer readers to \cite{zhao2017pyramid} for more details. In order to make the training effective, we pretrain the T-Net in our human matting dataset to get a good initialization before the end to end training.

\textbf{Implementation details:} To train T-Net, we generate the trimap ground truths offline following the common method used in matting area: dilated from their ground truth alpha mattes. To avoid overfitting, we augment the dataset online by randomly rotation, cropping and horizontal flipping during training as follows: 1) Images are firstly resized such that their shorter edge is 1000 pixels. 2) Then large image patches (800x800) are cropped from the resized images to ensure large batch training. 3) Cropped image patches are rotated by a angle sampled uniformly from -10 to 10 degree. 4) Finally, image patches are horizontal flipped with a probability of 0.5. We adopt cross entropy as our loss function. As the T-Net consist of ResNet50 (from conv1 to fc6 layer), a pyramid pooling module and additional classification convolutional layer, we initialize the ResNet50 layers with the weight trained on ImageNet classification task and the rest layers with kaiming normal. [....add lr, batchsize...]


\subsection{Matting stage: M-Net}\label{M-Net}
The second stage of our method is M-Net that is in charge of matting. Instead of directly using the trimaps predicted by the trained T-Net, we use variant trimaps generated online from the alpha matte to train M-Net in this stage based on following motivations:

1. We expect first train a outstanding alpha matte prediction network when a RGB image and corresponding trimap generated from alpha matte ground truth or user interactions are given in this stage, just behaves like classical matting algorithms.

2. We then use the weights of the well trained network in this stage to initialize our M-Net in the following end to end training stage that using the trimap predicted from T-Net which may has some defects. We expect the end to end finetune will make our T-Net and M-Net adapt to each other and predict a better alpha matte compared to a independent trimap and alpha prediction stage.


\textbf{Network structure:} The structure of our M-Net illustrated in Fig. is similar to \cite{xu2017deep}. It is a deep convolutional encoder-decoder network. The encoder network has 13 convolutional layers and 4 max-pooling layers, while the decoder network has 6 convolutional layers and 4 unpooling layers. The hyper-parameters of encoder network are the same as the convolutional layers of VGG16 classification network expect for the "conv1" layer in VGG16 that has 3 channel input whereas 6 in our M-Net. Compared with the matting network used in \cite{xu2017deep}, our T-Net mainly has two differences in structure. The fist one is our M-Net has 6 channel input (RGB image with 3 channels trimap) for the convenience of end to end training. The second one is that we add batch normalization after each convolutional layer and remove the max pooling after "conv5", "conv6" and "deconv6" layers that used in \cite{xu2017deep}, for we observe that "conv6" and "deconv6" has large proportion of weights but easy to overfit and small performance improvement, whereas batch normalization accelerate our network converging to a much better result.

\textbf{Losses:} we adopt the alpha prediction loss and compositional loss which are introduced by Xu et al. \cite{xu2017deep} to our M-Net. The alpha prediction loss  in equation (\ref{equa_alpha_loss}) is the weighted average absolute difference between the ground truth alpha  and predicted alpha  at each pixel , where  is a location specific weight,  is the number of image pixels, and  is a small value just for numerical stability.

The compositional loss  is the same to alpha prediction loss except it is the weighted average absolute difference between the ground truth compositional image value and the predicted image value that is composited by the predicted alpha matte with the corresponding ground truth foreground and background according to equation (\ref{equa_composition}).

The weight  in equation (\ref{equa_alpha_loss}) and (\ref{equa_comp_loss}) is crucial for training M-Net to concentrate on foreground edges and structural details. In our experiment, we simply set it to 1 if the pixel is inside the unknown regions of trimap, while 0 otherwise. The finally loss of our T-Net is the average of the above two loss:

Where  is set to 0.5 in our experiments.

\textbf{Implementation details:} Although our human matting dataset has more than 34k human instances, most of them are models from an e-commerce website and have much less complex structural details as shown in the last three columns in Fig. \ref{dataset_intro}. It is vulnerable to overfitting if we train our M-Net on this dataset directly. Therefore, before end to end training detailed in the next section, we train a much general matting model on the general matting dataset provided by Xu et al. \cite{xu2017deep}. Specifically, we filter out the 20 human alpha mattes with their foregrounds from this general matting dataset and obtain 411 alpha mattes with their foregrounds as our training dataset in this stage. We then composite this 411 foregrounds onto 41100 backgrounds (100 backgrounds per foreground) with corresponding ground truth alpha mattes. It is noting that this 41100 images contains kinds of objects as foregrounds including human and the foregrounds and backgrounds used in our human matting test dataset described before do not appear in this training dataset. We augment this general dataset similar to \cite{xu2017deep}. We find it is crucial for the performance of M-Net to augment the kernel sizes of erosion and dilation that operated on the alpha mattes to generated trimaps online.

As the encoder part of our M-Net is the same as the convolutional parts of the batch normalization version of VGG16 except for the first layer, we initialize these layers with the weights trained on ImageNet. For the first layer, we initialize the extra 3 channels with zeros. For the decoder parts, we initialize them with Xavier. We select Adam as our optimizer. We first set the learning rate to be , when the validation loss no longer descend, we set it to  until the model converged.
\end{comment}

\subsection{Fusion Module}
The deep matting network takes the predicted trimap as input and directly computes the alpha matte.
However, as shown in Fig.~\ref{pipeline}, it focuses on the unknown regions and recovers structual and textural details only.
The semantic information of foreground and background is not retained well.
In this section, we describe the fusion strategy in detail.

We use ,  and  to denote the foreground, background and unknown region channel that predicted by T-Net before softmax. Thus the probability map of foreground  can be written as

We can obtain  and  in the similarly way.
It is obvious that , where \textbf{1} denotes an all-1 matrix that has the same width and height of input image.
We use  to denote the output of M-Net.

Noting that the predicted trimap gives the probability distribution of each pixel belonging the three categories, foreground, background and unknown region.
When a pixel locates in the unknown region, which means that it is near the contour of a human and constitutes the complex structural details like hair, matting is required to accurately pull the alpha matte.
At this moment, we would like to use the result of matting network, , as an accurate prediction.
Otherwise, if a pixel locates outside the unknown region, then the conditional probability of the pixel belonging to the foreground is an appropriate estimation of the matte, \emph{i.e.}, .
Considering that  is the probability of each pixel belonging to the unknown region, a probabilistic estimation of alpha matte for all pixels can be written as

where  denotes the output of \emph{Fusion Module}. As , we can rewrite Eq.~\ref{equa_t_m_fusion_derivation} as


Intuitively, this formulation shows that the coarse semantic segmentation is refined by the matting result with details, and the refinement is controlled explicitly by the unknown region probability.
We can see that when  is close to 1,  is close to 0, so  is approximated by , and
when  is close to 0,  is approximated by .
Thus it naturally combines the coarse semantics and fine details.
Furthermore, training errors can be readily propagated through to corresponding components, enabling the end-to-end training of the entire network.


\subsection{Loss}
Following Xu \emph{et al}.~\cite{xu2017deep}, we adopt the alpha prediction loss and compositional loss.
The alpha prediction loss is defined as the absolute difference between the groundtruth alpha  and predicted alpha .
And the compositional loss is defined as the absolute difference between the groundtruth compositional image values  and predicted compositional image values .
The overall prediction loss for  at each pixel is

where  is set to 0.5 in our experiments.
It is worth noting that unlike Xu \emph{et al}.~\cite{xu2017deep} which only focus on unknown regions, in our automatic settings, the prediction loss is summed over the entire image.

In addition, we need to note that the loss  forms another decomposition problem of groundtruth matte, which is again under-constrained.
To get a stable solution to this problem, we introduce an extra constraint to keep the trimap meaningful.
A classification loss  for the trimap over each pixel is thus involved.


Finally, we get the total loss

where we just keep  to a small value to give a decomposition constraint, e.g. 0.01 throughout our paper.


\subsection{Implementation Detail}\label{sec_implementaton}

The pre-train technique~\cite{hinton2006fast} has been widely adopted in deep learning and shown its effectiveness.
We follow this common practice and first pre-train the two sub-netowrks \emph{T-Net} and \emph{M-Net} separately and then finetune the entire net in an end-to-end way.
Further, when pre-training the subnetwork, extra data with large amount specific to sub-tasks can also be empolyed to sufficiently train the models.
Note that the dataset used for pre-training should not overlap with the test set.

\paragraph{\textbf{T-Net pre-train}}
To train T-Net, we follow the common practice to generate the trimap ground truth by dilating the groundtruth alpha mattes.
In training phase, square patches are randomly cropped from input images and then uniformly resized to 400400.
To avoid overfitting, these samples are also augmented by randomly performing rotation and horizontal flipping.
As our \emph{T-Net} makes use of PSPNet50 which is based on ResNet50~\cite{he2016deep}, we initialize relevant layers with off-the-shelf model trained on ImageNet classification task and randomly initialize the rest layers.
The cross entropy loss for classification(\emph{i.e.},  in Eq.~\ref{total_loss}) is employed.



\paragraph{\textbf{M-Net pre-train}} 
We follow deep matting network training pipeline as~\cite{xu2017deep} to pre-train \emph{M-Net}.
Again, the input of \emph{M-Net} is a 3-channel image with the 3-channel trimap generated by dilating and eroding the groundtruth alpha mattes.
Worth noting that, we find it is crucial for the performance of matting to augment the trimap with different kernel sizes of dilating and eroding, since it makes the result more robust to the various unknown region widths.
For data augmentation, the input images are randomly cropped and resized to 320320.
Entire DIM~\cite{xu2017deep} dataset is empployed during pre-training \emph{M-Net} regardless whether it contains humans, since the \emph{M-Net} focuses on the local pattern rather than the global semantic meaning.
The regression loss same as  term in Eq.~\ref{total_loss} is adopted.
   


\paragraph{\textbf{End-to-end training}}

End-to-end training is performed on human matting dataset and the model is initialized by pre-trained \emph{T-Net} and \emph{M-Net}.
In training stage, the input image is randomly cropped as 800800 patches and fed into \emph{T-Net} to obtain semantic predictions.
Considering that \emph{M-Net} needs to be more focused on details and trained with large diversity,
augmentations are performed on the fly to randomly crop different patches (320320, 480480, 640640 as in~\cite{xu2017deep}) and resize to 320320.
Horizontal flipping is also randomly adopted with 0.5 chance.
The total loss in Eq.~\ref{total_loss} is used.
For testing, the feed forward is conducted on the entire image without augmentation. More specifically, when the longer edge of the input image is larger than 1500 pixels, we first scale it to 1500 for the limitation of GPU memory. We then feed it to the network and finally rescale the predicted alpha matte to the size of the original input image for performance evaluation. In fact, we can alternatively perform testing on CPU for large images without losing resolution.


















\begin{comment}
\subsection{End to end training stage}

Although we have got a well trained T-Net and M-Net that can predict trimaps and alpha mattes when the input images and input images with corresponding trimaps are given respectively, we can not train the network end to end, because of the argmax operation is needed to generate trimap from T-Net as input to M-Net while argmax is not differentiable. In this section, we introduce a differentiable layer, namely T-M fusion layer, which takes the output of T-Net and M-Net as inputs and output the final alpha matte.

\textbf{T-M fusion layer:} We use ,  and  to denote the foreground, background and unknown region channel that predicted by T-Net before sofmax. Thus the probability map of foreground  can be written as

We can obtain  and  in the similarly way. It is obvious that , where \textbf{1} denotes a all-1 matrix that has the same with and height of input image. We use  to denote the output of M-Net, then the output of T-M fusion layer that fuse the output of T-Net and M-Net can be written as

Where  denotes the output of T-M fusion layer. As , we can rewrite equation (\ref{equa_t_m_fusion_derivation}) as

It is shown that T-M fusion layer defined in equation (\ref{equa_t_m_fusion}) takes foreground  and unknown region  channel predicted by T-Net and raw alpha matte  predicted by M-Net as inputs, and outputs the final alpha matte by adding a weighted raw alpha matte to the foreground channel. What's more, the operation defined above is differentiable and can be easy implemented by any deep learning framework.

Finally, we can minimize the following alpha predict loss function similar to equation \ref{equa_alpha_loss} to obtain the finally alpha matte through T-M fusion layer

Since ,  and  are unknown with respect to the entire network, it is not easy to obtain a satisfactory solution by directly optimizing the above loss function defined in equation (\ref{equa_t_m_fusion_loss}). We use two common technique to handle this difficulty. The first one is pretraining the T-Net and M-Net described in the previous sections, which makes our network converge to a well solution before end to end training. The second one is adding a regularization term in our end to end training loss function. In this paper, we add the cross-entropy loss  used in T-Net pretraining stage to constraint the  and  used in equation (\ref{equa_t_m_fusion_loss}). In addition to , we also add a compositional loss  similar to that used in M-Net pretraining stage to our end to end training. Therefore, our finally end to end training loss is

Where ,  and  is a weight that balanced these three loss and is set to 0.5, 0.5 and 1.0 in this paper.

\textbf{Implementation details:} We first initialize the weights of the entire network with the pretrained T-Net and M-Net. Then we finetune the entire network on our human matting training dataset. We combine the data augmentation used in the pretraining stage of T-Net and M-Net and predict the finally alpha mattes as  follows:

1. The input images with corresponding alpha mattes and trimap ground truths are resized, cropped, rotated and flipped by the same parameters described in Trimap generation stage. T-Net takes these large image patches (800x800) as inputs and predicts the trimaps.

2.  three types of small image patches (320x320, 480x480 and 640x640) are randomly cropped from the above large image patches that ensure the center of the cropped small patches are in the unknown regions of trimaps. Then all these small image patches are resized to 320x320 and then are feed to the M-Net as inputs. We record the information of the small image patches with respect to the large image patches, such as locations, resize scales and flipping. These informations are used to recover the trimaps predicted by T-Net corresponding to the small image patches. M-Net takes these small patches with the corresponding predicted trimaps as inputs and outputs the raw alpha mattes.

3. T-M fusion layer takes the predicted trimaps and raw alpha mattes of the small image patches as inputs and outputs final alpha mattes. we compute alpha predict loss  and compositional loss  on the small image patches, while trimap cross entropy loss  on the large image patches.

We use adam as our optimizer and set the learning rate to  unchanged during the end to end training.

\end{comment}


\section{Experiments}

\subsection{Experimental Setup}

We implement our method with PyTorch~\cite{paszke2017automatic} framework.
The \emph{T-Net} and \emph{M-Net} are first pre-trained and then fine-tuned end to end as described in Section~\ref{sec_implementaton}.
During end-to-end training phase,  we use Adam as the optimizer.
The learning rate is set to  and the batch size is 10.

\paragraph{\textbf{Dataset}}

We evaluate our method on the human matting dataset, which contains 52,511 training images and 1,400 testing images as described in Section~\ref{dataset}.


\paragraph{\textbf{Measurement}}

Four metrics are used to evaluate the quality of predicted alpha matte~\cite{rhemann2009perceptually}: SAD, MSE, Gradient error and Connectivity error.
SAD and MSE are obviously correlated to the training objective, and the Gradient  error and Connectivity error are proposed by~\cite{rhemann2009perceptually} to reflect perceptual visual quality by a human observer.
To be specific, we normalize both the predicted alpha matte and groundtruth to 0 to 1 when calculating all these metrics.
Further, all metrics are caculated over entire images instead of only within unknown regions and averaged by the pixel number.

\begin{comment}

where  and  are the predicted alpha matte and ground truth alpha matte respectively. K is the number of pixels in alpha matte.  and ||.|| are functions and distance types used to calculate the corresponding matte errors.
\end{comment}

\paragraph{\textbf{Baselines}}
In order to evaluate the effectiveness of our proposed methods, we compare our method with the following state-of-the-art matting methods\footnote{Implementations provided by their authors are used except for DIM. We implement the DIM network with the same structure as \emph{M-Net} except with 4 input channels for a fair comparision.}:
Closed Form (CF) matting \cite{levin2008closed} , KNN matting \cite{chen2013knn}, DCNN matting \cite{cho2016natural} , Information Flow Matting (IFM) \cite{aksoy2017designing} and Deep Image Matting (DIM) \cite{xu2017deep}.
Noting that, all these matting methods are interactive and need extra trimaps as input.
For a fair comparison, we provide them with predicted trimaps by the well pretrained \emph{T-Net}.
We denote these methods as \textbf{PSP50 + }, where  represents the above methods.

To demonstrate the results of applying semantic segmentation to matting problem, we also design the following baselines:
\begin{itemize}
  \item PSP50 Seg: a PSPNet-50 is used to extract humans via the predicted mask. The groundtruth mask used to train this network is obtained by binarizing the alpha matte with a threshold of 0.
  \item PSP50 Reg: a PSPNet-50 is trained to predict the alpha matte as regression with L1 loss.
\end{itemize}



\begin{table}
	\caption{The quantitative results on human matting testing dataset. The best results are emphasized in bold}
	\label{tab:quan_res_hmtds_no_trimap}
	\begin{tabular}{lllll}
		\toprule
		Methods & \tabincell{c}{SAD \\ ()} & \tabincell{c}{MSE \\ ()} & \tabincell{c}{Gradient \\ ()} & \tabincell{c}{Connectivity \\ ()} \\
		\midrule
		PSP50 Seg & 14.821 & 11.530 & 52.336 & 44.854\\
PSP50 Reg & 10.098 & 5.430 & 15.441 & 65.217\\
PSP50+CF \cite{levin2008closed} & 8.809 & 5.218 & 21.819 & 43.927\\
		PSP50+KNN \cite{chen2013knn} & 7.806 & 4.390 & 20.476 & 56.328\\
		PSP50+DCNN \cite{cho2016natural} & 8.378 & 4.756 & 20.801 & 50.574\\
		PSP50+IFM \cite{aksoy2017designing} & 7.576 & 4.275 & 19.762 & 52.470\\
		PSP50+DIM \cite{xu2017deep} & 6.140 & 3.834 & 19.414 & 41.884\\
\midrule
Our Method & \textbf{3.833} & \textbf{1.534} & \textbf{5.179} & \textbf{36.513} \\
		\bottomrule
	\end{tabular}
\end{table}



\subsection{Performance Comparison}

In this section, we compare our method with the state-of-the-art matting methods with generated trimaps and designed baselines on the human matting testing dataset.
Trimaps are predicted by the pre-trained \emph{T-Net} and are provided to interactive matting methods.
The quantitative results are listed in Table~\ref{tab:quan_res_hmtds_no_trimap}.

The performances of binary segmentation and regression are poor.
Since complex structural details as well as the concepts of human are required in this task, the results show that it is hard to learn them simultaneously with a FCN network.
Using the trimaps predicted by the same PSP50 network, DIM outperforms the other methods, such as CF, KNN, DCNN and IFM. It is due to the strong capabilities of deep matting network to model complex context of images.
We can see that our method performs much better than all baselines.
The key reason is that our method successfully coordinate the coarse semantics and fine details with a probabilistic fusion strategy which enables a better end-to-end training.


Several visual examples are shown in Fig.~\ref{fig:case_show_hmtds}.
Compared to other methods (from column 2 to column 4), our method can not only obtain more "sharp" details, such as hairs, but also have much little semantic errors which may benefit from the end-to-end training.



\begin{figure*}[t]
	\footnotesize
	\begin{center}
		\begin{tabular}{cccccccc}
\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_img_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_reg_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_ifm_p_3c_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dim5_p_3c_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_ifm_lt_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dim5_lt_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_e2e_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/5025666458_576b974455_o_gt_show}\\
Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & TrimapGT+IFM \cite{aksoy2017designing} & TrimapGT+DIM \cite{xu2017deep} & Our method & Alpha GT\\
			
\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_img_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_reg_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_ifm_p_3c_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dim5_p_3c_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_ifm_lt_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dim5_lt_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_e2e_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/godiva_close_00015_gt_show}\\
Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & TrimapGT+IFM \cite{aksoy2017designing} & TrimapGT+DIM \cite{xu2017deep} & Our method & Alpha GT\\
			
\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_img_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_reg_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_ifm_p_3c_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_dim5_p_3c_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_ifm_lt_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_dim5_lt_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_e2e_show} & \hspace{-0.3cm}
			\includegraphics[width=0.12\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_gt_show}\\
Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & TrimapGT+IFM \cite{aksoy2017designing} & TrimapGT+DIM \cite{xu2017deep} & Our method & Alpha GT\\
			
		\end{tabular}
	\end{center}
	\caption{\label{fig:case_show_hmtds} The visual comparison results on the semantic human matting testing dataset}
\end{figure*}

\subsection{Automatic Method vs. Interactive Methods}

\begin{table}
	\caption{The quantitative results of our method and several state-of-the-art matting methods that need trimap on the semantic human matting testing dataset.}
	\label{tab:quan_res_hmtds_with_trimap}
	\begin{tabular}{lllll}
		\toprule
Methods & \tabincell{c}{SAD \\ ()} & \tabincell{c}{MSE \\ ()} & \tabincell{c}{Gradient \\ ()} & \tabincell{c}{Connectivity \\ ()} \\
		\midrule
		TrimapGT+CF & 6.772 & 2.258 & 9.0390 & 34.248\\
		TrimapGT+KNN & 8.379 & 3.413 & 16.451 & 83.458\\
		TrimapGT+DCNN & 6.760 & 2.162 & 9.753 & 44.392\\
		TrimapGT+IFM & 5.933 & 1.798 & 8.290 & 54.257\\
		TrimapGT+DIM & \textbf{2.642} & \textbf{0.589} & \textbf{3.035} & \textbf{25.773}\\
\midrule
		Our Method & 3.833 & 1.534 & 5.179 & 36.513\\
		\bottomrule
	\end{tabular}
\end{table}

We compare our method with state-of-the-art interactive matting methods taking the groundtruth trimaps as inputs, which are generated by the same strategy used in \emph{T-Net} pretraining stage.
We denote the baselines as \emph{TrimapGT + }, where  represents 5 state-of-the-art matting methods including CF \cite{levin2008closed} , KNN \cite{chen2013knn}, DCNN \cite{cho2016natural}, IFM \cite{aksoy2017designing} and DIM \cite{xu2017deep}.
Table~\ref{tab:quan_res_hmtds_with_trimap} shows the comparisons.
We can see that the result of our automatic method trained by end-to-end strategy is higher than most interactive matting methods, and is slightly inferior to TrimapGT+DIM.
Note that our automatic method only takes in the original RGB images, while interactive TrimapGT +  baselines take additional groundtruth trimaps as inputs.
Our \emph{T-Net} could infer the human bodies and estimate coarse predictions which are then complemented with matting details by \emph{M-Net}.
Despite slightly higher test loss, our automatic method is visually comparable with DIM, the state-of-the-art interactive matting methods, as shown in Fig.~\ref{fig:case_show_hmtds} (column "TrimapGT+DIM" 
\emph{vs.} "Our method").
 





\subsection{Evaluation and Analysis of Different Components}

\begin{table}
  \caption{Evaluation of Different Components.}
  \label{tab:ablation_study}
  \begin{tabular}{lllll}
    \toprule
Methods & \tabincell{c}{SAD \\ ()} & \tabincell{c}{MSE \\ ()} & \tabincell{c}{Gradient \\ ()} & \tabincell{c}{Connectivity \\ ()} \\
    \midrule
    no end-to-end & 7.576 & 4.275 & 19.762 & 52.470\\
    no Fusion & 4.231 & 2.146 & 5.230 & 56.402\\
    no  & 4.536 & 2.278 & 5.424 & 52.546 \\
    \midrule
    Our Method & \textbf{3.833} & \textbf{1.534} & \textbf{5.179} & \textbf{36.513}\\
  \bottomrule
\end{tabular}
\end{table}


\paragraph{\textbf{The Effect of End-to-end Training}}
In order to evaluate the effectiveness of the end-to-end strategy,
we compare our end-to-end trained model with that using only pre-trained parameters (\emph{no end-to-end}).
The results are listed in Table \ref{tab:ablation_study}. 
We can see that network trained in end-to-end manner performs better than \emph{no end-to-end}, which shows the effectiveness of the end-to-end training.

\paragraph{\textbf{The Evaluation of \emph{Fusion Module}}}
To validate the importance of the proposed \emph{Fusion Module}, we design a simple baseline that directly outputs the result of \emph{M-Net}, \emph{i.e.} .
It is trained with the same objective as Eq.~\ref{total_loss}.
We compare the performance between our method with \emph{Fusion Module} and this baseline without \emph{Fusion Module} in Table \ref{tab:ablation_study}.
We can see that our method with \emph{Fusion Module} achieves better performance than the baseline. 
Especially note that although other metrics remain relatively small, the Connectivity error of baseline gets quite large.
It can be due to a blurring of the structural details when predicting the whole alpha matte only with \emph{M-Net}.
Thus the designed fusion module, which leverages both the coarse estimations from \emph{T-Net} and the fine predictions from M-Net, is crucial for better performance.

\paragraph{\textbf{The Effect of Constraint }}
In our implementation, we introduce a constraint for the trimap, \emph{i.e.} .
We train a network removing it to investigate the effect of such a constraint.
We denote the network trained in this way as \emph{no }.
The performance of this network is shown in Table \ref{tab:ablation_study}.
We can see that the network without  performs better than that without end-to-end training, but is worse than the proposed method.
This constraint makes the trimap more meaningful and the decomposition in  more stable.



	
\begin{figure*}[h!]
	\begin{center}
		\begin{tabular}{cccc}
\includegraphics[width=0.24\linewidth]{fig/visualization/TB22AyFdhSYBuNjSspjXXX73VXa_381329993_img} & \hspace{-0.3cm}
			\includegraphics[width=0.24\linewidth]{fig/visualization/TB22AyFdhSYBuNjSspjXXX73VXa_381329993_pred_trimap} & \hspace{-0.3cm}
			\includegraphics[width=0.24\linewidth]{fig/visualization/TB22AyFdhSYBuNjSspjXXX73VXa_381329993_pred_raw} & \hspace{-0.3cm}
			\includegraphics[width=0.24\linewidth]{fig/visualization/TB22AyFdhSYBuNjSspjXXX73VXa_381329993_pred_alpha}\\
			(a) & (b) & (c) & (d)\\

		\end{tabular}
	\end{center}
	\caption{\label{fig:vis} Intermediate results visualization on a real image. (a) an input image, (b) trimap predicted by T-Net, (c) raw alpha matte predicted by M-Net, (d) fusion result according to Eq. \ref{equa_t_m_fusion}.}
\end{figure*}

\begin{figure*}[h!]
\begin{center}
\begin{tabular}{cccccc}
\includegraphics[width=0.16\linewidth]{fig/compare_real_images/hailuo_420006964_RF_quanmeitiyongtu_crop_img} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/hailuo_420006964_RF_quanmeitiyongtu_crop_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/hailuo_420006964_RF_quanmeitiyongtu_crop_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/hailuo_420006964_RF_quanmeitiyongtu_crop_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/hailuo_420006964_RF_quanmeitiyongtu_crop_e2e} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/1}\\
  Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method & Composition \\
\includegraphics[width=0.16\linewidth]{fig/compare_real_images/13_img} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/13_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/13_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/13_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/13_e2e} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/2}\\
  Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method & Composition \\   
\includegraphics[width=0.16\linewidth]{fig/compare_real_images/6_img} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/6_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/6_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/6_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/6_e2e} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/3}\\
  Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method & Composition \\   
\includegraphics[width=0.16\linewidth]{fig/compare_real_images/ousuwcnn_img} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/ousuwcnn_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/ousuwcnn_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/ousuwcnn_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/ousuwcnn_e2e} & \hspace{-0.3cm}
  \includegraphics[width=0.16\linewidth]{fig/compare_real_images/4} \\
  Image & PSP50 Reg & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method & Composition \\   
\end{tabular}
\end{center}
\caption{\label{fig:case_show_real_images} The visual comparison results on the real images.}
\end{figure*}



\paragraph{\textbf{Visualization of Intermediate Results}}

To better understand the mechanism of SHM, we visualize the intermediate results on a real image shown in Fig \ref{fig:vis}.
The first column (a) shows the original input image, the second column (b) shows the predicted foreground (green), background (red) and unknown region (blue) from T-Net, the third column (c) shows the predicted alpha matte from M-Net, and the last column (d) shows the fusion result of the second column (b) and the third column (c) according to Eq.~\ref{equa_t_m_fusion}.
We can see that the \emph{T-Net} could segment the rough estimation of human main body, and automatically distinguish the definite human edges where predicted unknown region is narrower and structural details where predicted unknown region is wider.
In addition, with the help of the coarse prediction provided by \emph{T-Net}, \emph{M-Net} could concentrate on the transitional regions between foreground and background and predict more structural details of alpha matte.
Further, we combine the advantages of \emph{T-Net} and \emph{M-Net} and obtain a high quality alpha matte with the aid of \emph{Fusion Module}.












\begin{comment}
\begin{figure*}[t]\begin{center}
\begin{tabular}{ccccc}
\includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_img} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_seg} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_reg} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_cf_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_knn_p_3c} \\
  Image & PSP50 Segmentation \cite{zhao2017pyramid} & PSP50 Regression & PSP50+Trimap+CF \cite{levin2008closed} & PSP50+Trimap+KNN \cite{chen2013knn}\\
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dcnn_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_ifm_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dim5_p3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_e2e} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/5025666458_576b974455_o_gt}\\
  PSP50+Trimap+DCNN \cite{cho2016natural} & PSP50+Trimap+IFM \cite{aksoy2017designing} & PSP50+Trimap+DIM \cite{xu2017deep} & Our method & GT\\
\includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_img} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_seg} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_reg} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_cf_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_knn_p_3c} \\
  Image & PSP50 Segmentation \cite{zhao2017pyramid} & PSP50 Regression & PSP50+Trimap+CF \cite{levin2008closed} & PSP50+Trimap+KNN \cite{chen2013knn}\\
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dcnn_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_ifm_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dim5_p3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_e2e} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/godiva_close_00015_gt}\\
  PSP50+Trimap+DCNN \cite{cho2016natural} & PSP50+Trimap+IFM \cite{aksoy2017designing} & PSP50+Trimap+DIM \cite{xu2017deep} & Our method & GT\\
\includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_img} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_seg} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_reg} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_cf_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_knn_p_3c} \\
  Image & PSP50 Segmentation \cite{zhao2017pyramid} & PSP50 Regression & PSP50+Trimap+CF \cite{levin2008closed} & PSP50+Trimap+KNN \cite{chen2013knn}\\
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_dcnn_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_ifm_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_dim5_p_3c} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_000000161506_e2e} &
  \includegraphics[width=0.19\linewidth]{fig/compare_shmds/TB26BbicOAKL1JjSZFoXXagCFXa_2258875223_gt}\\
  PSP50+Trimap+DCNN \cite{cho2016natural} & PSP50+Trimap+IFM \cite{aksoy2017designing} & PSP50+Trimap+DIM \cite{xu2017deep} & Our method & GT\\

\end{tabular}
\end{center}
\caption{\label{fig:case_show_hmtds} The visual comparison results on the semantic human matting testing dataset}
\end{figure*}
\end{comment}

\begin{comment}
\begin{figure*}[t]
\begin{center}
\begin{tabular}{cccccc}
\includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_img} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_seg} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_reg} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dcnn_p_3c} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_ifm_p_3c} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dim5_p_3c}\\
  Image & PSP50 Segmentation \cite{zhao2017pyramid} & PSP50 Regression & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep}\\

  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dcnn_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_ifm_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_dim5_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_img} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_000000138413_e2e} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/5025666458_576b974455_o_gt}\\

  AlphaGT+DCNN \cite{cho2016natural} & Alpha+IFM \cite{aksoy2017designing} & Alpha+DIM \cite{xu2017deep} & DPM \cite{shen2016deep} & Our method & GT\\

\includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_img} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_seg} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_reg} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dcnn_p_3c} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_ifm_p_3c} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dim5_p_3c}\\
  Image & PSP50 Segmentation \cite{zhao2017pyramid} & PSP50 Regression & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep}\\

  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dcnn_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_ifm_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_dim5_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_img} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_g0_24hHn3c_m_b49debc00db44a4eb634d14f18e1dbee_e2e} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/godiva_close_00015_gt}\\

  AlphaGT+DCNN \cite{cho2016natural} & Alpha+IFM \cite{aksoy2017designing} & Alpha+DIM \cite{xu2017deep} & DPM \cite{shen2016deep} & Our method & GT\\

\includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_img} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_seg} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_reg} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_dcnn_p_3c} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_ifm_p_3c} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_dim5_p_3c}\\
  Image & PSP50 Segmentation \cite{zhao2017pyramid} & PSP50 Regression & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep}\\

  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_dcnn_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_ifm_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_dim5_lt} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_img} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_000000393995_e2e} &
  \includegraphics[width=0.16\linewidth]{fig/compare_shmds/TB2U67SbfAkyKJjy0FfXXaxhpXa_2060173564_gt}\\

  AlphaGT+DCNN \cite{cho2016natural} & Alpha+IFM \cite{aksoy2017designing} & Alpha+DIM \cite{xu2017deep} & DPM \cite{shen2016deep} & Our method & GT\\

\end{tabular}
\end{center}
\caption{\label{fig:case_show_hmtds} The visual comparison results on the semantic human matting testing dataset}
\end{figure*}
\end{comment}

\begin{comment}
\begin{figure*}[h!]
\begin{center}
\begin{tabular}{cccccccc}
\includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_cf_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_knn_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/hailuo_420006964_RF_全媒体用途_crop_e2e}\\
  Image & PSP50 Regression & PSP50+CF \cite{cho2016natural} & PSP50+KNN \cite{chen2013knn} & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method\\
\includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_cf_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_knn_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/13_e2e}\\
  Image & PSP50 Regression & PSP50+CF \cite{cho2016natural} & PSP50+KNN \cite{chen2013knn} & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method\\
\includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_cf_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_knn_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/6_e2e}\\
  Image & PSP50 Regression & PSP50+CF \cite{cho2016natural} & PSP50+KNN \cite{chen2013knn} & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method\\
\includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_reg} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_cf_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_knn_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_img} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_ifm_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_dim5_p_3c} & \hspace{-0.3cm}
  \includegraphics[width=0.12\linewidth]{fig/compare_real_images/ousuwcnn_e2e}\\
  Image & PSP50 Regression & PSP50+CF \cite{cho2016natural} & PSP50+KNN \cite{chen2013knn} & PSP50+DCNN \cite{cho2016natural} & PSP50+IFM \cite{aksoy2017designing} & PSP50+DIM \cite{xu2017deep} & Our method\\
\end{tabular}
\end{center}
\caption{\label{fig:case_show_real_images} The visual comparison results on the real images.}
\end{figure*}
\end{comment}







\subsection{Applying to real images}

Since the images in our dataset are composited with annotated foregrounds and random backgrounds, to investigate the ability of our model to generalize to real-world images,
we apply our model and other methods to plenty of real images for a qualitative analysis.
Several visual results are shown in Fig.~\ref{fig:case_show_real_images}.
We find that our method performs well on real images even with complicated backgrounds.
Note that the hair details of the woman in the first image of Fig.~\ref{fig:case_show_real_images} are only recovered nicely by our method.
Also, the fingers in the second image are blurred incorrectly by other methods, whereas our method distinguishes them well.
Compsition examples of the foregrounds and new backgrounds with the help of automatically predicted alpha matte are illustrated in the last column of Fig.~\ref{fig:case_show_real_images}.
We can see these compositions have high visual quality.
More results can be found in supplementary materials.

\begin{comment}
	


\paragraph{\textbf{virtual photo studio}}

\begin{figure*}
\subfigure[]{
\label{application1} \includegraphics[width=0.99\linewidth]{fig/application.jpg}
}
\subfigure[]{
\label{application2} \includegraphics[width=0.99\linewidth]{fig/application2.jpg}}
\caption{Application of our SHM, virtual background for: (a) photographic model (b) live streaming model, images in the red bracket are the original natural images.}
\label{application} \end{figure*}


\paragraph{\textbf{livestreaming}}
\end{comment}

\section{Conclusion}

In this paper, we focus on the human matting problem which shows a great importance for a wide variety of applications. 
In order to simultaneously capture global semantic information and local details, we propose to cascade a trimap network and a matting network, as well as a novel fusion module to generate alpha matte automatically.
Furthermore, we create a large high quality human matting dataset.
Benefitting from the model structure and dataset, 
our automatic human matting achieves comparable results with state-of-the-art interactive matting methods.

\section*{Acknowledgement}

We thank Jian Xu for many helpful discussions and valuable suggestions, and Yangjian Chen, Xiaowei Li, Hui Chen, Yuqi Chen for their support on developing the image labeling tool, and Min Zhou for some comments that improved the manuscript.





