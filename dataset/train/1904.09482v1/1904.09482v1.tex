\section{Experiments}
\label{sec:exp}

\begin{table*}[htb!]
	\begin{center}
		\begin{tabular}{l|l|c|c|c|c|c}
			\hline \bf Corpus &Task& \#Train & \#Dev & \#Test   & \#Label &Metrics\\ \hline \hline
			\multicolumn{6}{@{\hskip1pt}r@{\hskip1pt}}{Single-Sentence Classification (GLUE)} \\ \hline
			CoLA & Acceptability&8.5k & 1k & 1k & 2 & Matthews corr\\ \hline
			SST-2 & Sentiment&67k & 872 & 1.8k & 2 & Accuracy\\ \hline \hline
			\multicolumn{6}{@{\hskip1pt}r@{\hskip1pt}}{Pairwise Text Classification (GLUE)} \\ \hline
			MNLI & NLI& 393k& 20k & 20k& 3 & Accuracy\\ \hline
            RTE & NLI &2.5k & 276 & 3k & 2 & Accuracy \\ \hline
            WNLI & NLI &634& 71& 146& 2 & Accuracy \\ \hline
			QQP & Paraphrase&364k & 40k & 391k& 2 & Accuracy/F1\\ \hline
            MRPC & Paraphrase &3.7k & 408 & 1.7k& 2&Accuracy/F1\\ \hline
			\multicolumn{5}{@{\hskip1pt}r@{\hskip1pt}}{Text Similarity (GLUE)} \\ \hline
			STS-B & Similarity &7k &1.5k& 1.4k &1 & Pearson/Spearman corr\\ \hline

\multicolumn{6}{@{\hskip1pt}r@{\hskip1pt}}{Relevance Ranking (GLUE)} \\ \hline \hline
			QNLI & QA/NLI& 108k &5.7k&5.7k&2& Accuracy\\ \hline \hline


		\end{tabular}
	\end{center}
\caption{Summary of the GLUE benchmark.
	}
	\label{tab:datasets}
\end{table*}

We evaluate the MT-DNN trained using Knowledge Distillation, termed as {\NMNAME} in this section, on the General Language Understanding Evaluation (GLUE) benchmark. GLUE is a collection of nine NLU tasks as in Table~\ref{tab:datasets}, including question answering, sentiment analysis, text similarity and textual entailment. 
We refer readers to \citet{wang2018glue} for a detailed description of GLUE.
We compare {\NMNAME} with existing state-of-the-art models including BERT \cite{bert2018}, STILT  \cite{phang2018sentence}, Snorkel MeTal \cite{hancock2019snorkel}, and {\MNAME} \cite{liu2019mt-dnn}. Furthermore, we investigate the relative contribution of using knowledge distillation for MTL with an ablation study.










\subsection{Implementation details}
\label{subsec:impl}
Our implementation is based on the PyTorch implementations of  MT-DNN\footnote{https://github.com/namisan/mt-dnn} and BERT\footnote{https://github.com/huggingface/pytorch-pretrained-BERT}.
We used Adamax \cite{kingma2014adam} as our optimizer with a learning rate of 5e-5 and a batch size of 32. The maximum number of epochs was set to 5. 
A linear learning rate decay schedule with warm-up over 0.1 was used, unless stated otherwise.
We also set the dropout rate of all the task-specific layers as 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. 
To avoid the gradient explosion issue, we clipped the gradient norm within 1. 
All the texts were tokenized using wordpieces, and were chopped to spans no longer than 512 tokens.


To obtain a set of diverse single models to form ensemble models (teachers), we first trained 6 single {\MNAME}s, initialized using Cased/Uncased BERT models as \cite{hancock2019snorkel} with a different dropout rate, ranged in \{0.1, 0.2, 0.3\}, on the shared layers, while keeping other training hyperparameters the same as aforementioned. 
Then, we selected top 3 best models according to the results on the MNLI and RTE development datasets. 
Finally, we fine-tuned the 3 models on each of the MNLI, QQP, RTE and QNLI tasks to form four task-specific ensembles (teachers), each consisting of 3 single MT-DNNs fine-tuned for the task.
The teachers are used to generate soft {\SLABEL}s for the four tasks as Equation \ref{eqn:teacher}, described in Section \ref{sec:knowledge-distillation}. 
We only pick four out of nine GLUE tasks to train teachers  to investigate the generalization ability of {\NMNAME}, i.e., its performance on the tasks with and without teachers.






\begin{table*}[htb!]
\small
	\begin{center}
		\begin{tabular}{l|@{\hskip1pt}l@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c|@{\hskip1pt}c|@{\hskip1pt}c |@{\hskip1pt} c |@{\hskip1pt} c|@{\hskip1pt} c}
			\hline \bf Model &CoLA&	SST-2 &MRPC& STS-B&QQP&MNLI-m/mm&QNLI&RTE&WNLI&AX &\textbf{Score}\\ 
& 8.5k &67k &3.7k &7k &364k &393k &108k &2.5k &634 & & \\ \hline \hline
			BiLSTM+ELMo+Attn $^1$&36.0 &90.4 &84.9/77.9 &75.1/73.3 &64.8/84.7 &76.4/76.1 &79.8 &56.8 &65.1 &26.5 &70.0 \\ \hline
			\begin{tabular}{@{}c@{}}Singletask Pretrain \\Transformer $^2$   \end{tabular}
			 &45.4 &91.3 &82.3/75.7&82.0/80.0 &70.3/88.5 &82.1/81.4 &87.4 &56.0 &53.4  &29.8 &72.8 \\ \hline
			GPT on STILTs $^3$ &47.2 &93.1 &87.7/83.7 &85.3/84.8 &70.1/88.1 &80.8/80.6 &- &69.1 &65.1 &29.4 &76.9 \\ \hline
			BERT$_{\text{LARGE}}$$^4$ & 60.5 &94.9 &89.3/85.4 &87.6/86.5 &72.1/89.3 &86.7/85.9 &92.7 &70.1 &65.1	&39.6 & 80.5\\ \hline
        {\MNAME}$^5$ & 61.5 &95.6 &90.0/86.7 &88.3/87.7 &72.4/89.6 &86.7/86.0	&- &75.5 &65.1	&40.3 &82.2 \\ \hline
Snorkel MeTaL $^6$ &63.8&	\textbf{96.2}&	91.5/88.5&	\textbf{90.1/89.7}&	73.1/89.9&	87.6/87.2&93.9&80.9&65.1&39.9 &83.2 \\ \hline	        
        ALICE $^*$& 63.5&	95.2&	\textbf{91.8/89.0}&	89.8/88.8&	 \textbf{74.0/90.4}&	\textbf{87.9/87.4}&	95.7&	80.9&	65.1&	40.7& 83.3 \\ \hline
			\textbf{\NMNAME} &\textbf{65.4}   &95.6  &91.1/88.2 &89.6/89.0 &72.7/89.6 &87.5/86.7 & \textbf{96.0} & \textbf{85.1}&65.1 &\textbf{42.8} &\textbf{83.7} \\ \hline \hline
{Human Performance} &66.4&97.8&86.3/80.8    &92.7/92.6	&59.5/80.4	&92.0/92.8	&91.2	&93.6	&95.9 &- & 87.1\\ \hline
		\end{tabular}
	\end{center}
\caption{GLUE test set results scored using the GLUE evaluation server. The number below each task denotes the number of training examples. The state-of-the-art results are in \textbf{bold}. {\NMNAME} uses BERT\textsubscript{LARGE} to initialize its shared layers. 
	All the results are obtained from \href{https://gluebenchmark.com/leaderboard}{https://gluebenchmark.com/leaderboard} on April 1, 2019. Note that Snorkel MeTaL is an ensemble model.  
	- denotes the missed result of the latest GLUE version. 
	$^*$ denotes the unpublished work, thus not knowing whether it is a single model or an ensemble model. 
	For QNLI, we treat it as two tasks, pair-wise ranking and classification task on v1 and v2 training datasets, respectively, and then merge results on the test set.  
	Model references: $^1$:\protect\cite{wang2018glue} ; $^2$:\protect\cite{gpt2018}; $^3$: \protect\cite{phang2018sentence}; $^4$:\protect\cite{bert2018}; $^5$: \protect\cite{liu2019mt-dnn}; $^6$: \protect\cite{hancock2019snorkel}. }
	\label{tab:glue_test}
\end{table*}

\begin{table*}
	\begin{center}
		\begin{tabular}{@{\hskip1pt}l|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|@{\hskip1pt}c|c @{\hskip1pt}|c@{\hskip1pt}}
			\hline \bf Model &MNLI-{m/mm} &QQP &RTE &QNLI(v2) &MRPC &CoLa &SST-2  &STS-B\\ \hline \hline
			BERT$_{\text{LARGE}}$& 86.3/86.2 &91.1/88.0 &71.1 &92.4 &89.5/85.8 &61.8 &93.5 &89.6/89.3\\
			\hline
            {\MNAME}  &87.1/86.7 &\textbf{91.9}/89.2 &83.4 &92.9 &91.0/87.5 &63.5 &\textbf{94.3} &90.7/90.6\\ \hline
            {\NMNAME} &\textbf{87.3/87.3} & \textbf{91.9/89.4} & \textbf{88.6}& \textbf{93.2} &\textbf{93.3/90.7} &\textbf{64.5} & \textbf{94.3}& \textbf{91.0/90.8}\\ \hline \hline
            {\MNAME-ensemble}  &88.1/87.9 &92.5/90.1 &86.7 &93.5 &\color{blue}{\textit{93.4/91.0}} &\color{blue}{\textit{64.5}} & \color{blue}{\textit{94.7}} &\color{blue}{\textit{92.1/91.6}}\\ \hline

		\end{tabular}
	\end{center}
\caption{GLUE dev set results. The best result on each task produced by a single model is in \textbf{bold}.
MT-DNN uses BERT\textsubscript{LARGE} as their initial shared layers. 
	{\NMNAME} is the MT-DNN trained using the proposed knowledge distillation based MTL. 
	MT-DNN-ensemble denotes the results of the ensemble models described in Section~\ref{subsec:impl}. The ensemble models on MNLI, QQP, RTE and QNLI are used as teachers in the knowledge distillation based MTL, while the other ensemble modes, whose results are in {\color{blue}{\textit{blue and italic}}}, 
	are not used as teachers.
}
	\label{tab:glue_dev}
\end{table*}



\subsection{GLUE Main Results}
\label{subsec:results}
We compare {\NMNAME} with a list of state-of-the-art models that have been submitted to the GLUE leaderboard. 

\paragraph{BERT\textsubscript{LARGE}} This is the large BERT model released by \citet{bert2018}, which we used as a baseline. We used single-task fine-tuning to produce the best result for each GLUE task according to the development set. 

\paragraph{MT-DNN} This is the model described in Section \ref{sec:mt-dnn} and \citet{liu2019mt-dnn}. We used the pre-trained BERT\textsubscript{LARGE} model to initialize its shared layers, refined the shared layers via MTL on all GLUE tasks,
and then perform a fine-tune for each GLUE task using the task-specific data.
\paragraph{{\NMNAME}} This is the MT-DNN model trained using knowledge distillation as described in Section \ref{sec:knowledge-distillation}. 
{\NMNAME} uses the same model architecture as that of MT-DNN. But the former is trained with the help from four task-specific ensembles (teachers). {\NMNAME} is optimized for the multi-task objectives that are based on the hard correct {\SLABEL}s, as well as the soft {\SLABEL}s produced by the teachers if available. 
After knowledge distillation based MTL, {\NMNAME} is further fine-tuned for each task using task-specific data to produce the final predictions for each GLUE task on blind test data for evaluation.








The main results on the official test datasets of GLUE are reported in Table \ref{tab:glue_test}. 
Compared to other recent submissions on the GLUE leaderboard, {\NMNAME} is the best performer,
creating a new state-of-the-art result of 83.7\%. 
The margin between {\NMNAME} and the second-best model ALICE is 0.5\%,  larger than the margin of 0.1\% between the second and the third (and the fourth) places. 
It is worth noting that {\NMNAME} is a single model while Snorkel MetaL \cite{hancock2019snorkel} is an ensemble model. The description of ALICE is not disclosed yet.

Table \ref{tab:glue_test} also shows that {\NMNAME} significantly outperforms MT-DNN not only in overall score but on 7 out of 9 GLUE tasks, including the tasks without a teacher.
Since {\NMNAME} and MT-DNN use the same network architecture, and are trained with the same initialization and on the same datasets, the improvement of {\NMNAME} is solely attributed to the use of knowledge distillation in MTL. 


We note that the most significant per-task improvements are from CoLA (65.4\% vs. 61.5\%) and RTE (85.1\% vs. 75.5\%). Both tasks have relatively small amounts of in-domain data. Similarly, for the same type of tasks, the improvements of {\NMNAME} over {\MNAME} are much more substantial for the tasks with less in-domain training data e.g., for the two NLI tasks, the improvement in RTE is much larger than that in MNLI;  for the two paraphrase tasks, the improvement in  MRPC is larger than that in QQP. 
These results suggest that knowledge distillation based MTL is  effective at improving model performance for not only tasks with teachers but also ones without teachers, and more so for tasks with fewer in-domain labels.





\subsection{Ablation Study}


We perform an ablation study to investigate how effective it can distill knowledge from the ensemble models (teachers) to a single MT-DNN (student). To this end, we compare the performance of the ensemble models with the corresponding student model. 

The results on dev sets are shown in Table \ref{tab:glue_dev}, where MT-DNN-ensemble are the task-specific ensemble models trained using the process described in Section \ref{sec:knowledge-distillation}. We only use four ensemble models (i.e., the models for MNLI, QQP, RTE, QNLI) as teachers. The results of the other ensemble models (i.e., MRPC, CoLa, SST-2, STS-B) are reported to show the effectiveness of the knowledge distillation based MTL at improving the performance on tasks without a teacher.

We can draw several conclusions from the results in Table \ref{tab:glue_dev}. First, {\NMNAME} significantly outperforms MT-DNN and BERT\textsubscript{LARGE} across multiple GLUE tasks on the dev sets, which is consistent with what we observe on test sets in Table \ref{tab:glue_test}. 
Second, comparing {\NMNAME} with MT-DNN-ensemble, we see that the {\NMNAME} successfully distills knowledge from the teachers. Although the distilled model is  simpler than the teachers, it retains nearly all of the improvement that is achieved by the ensemble models. More interestingly, we find that incorporating knowledge distillation into MTL improves the model performance on the tasks where no teacher is used. On MRPC, CoLA, and STS-B, the performance of {\NMNAME} is much better than MT-DNN and is close to the ensemble models although the latter are not used as teachers in MTL.











