We first describe the structure of sequential EHR data and our notation, then follow with a general framework for predictive analysis in healthcare using EHR, followed by details of the \alg method.

\textbf{EHR Structure and our Notation.} The EHR data of each patient can be represented as a time-labeled sequence of multivariate observations. Assuming we use $r$ different variables, the $n$-th patient of $N$ total patients can be represented by a sequence of $T^{(n)}$ tuples $(t_i^{(n)}, \mathbf{x}_i^{(n)}) \in \mathbb{R}\times\mathbb{R}^{r} , i=1, \ldots, T^{(n)}$. The timestamps $t_i^{(n)}$ denotes the time of the $i$-th visit of the $n$-th patient and $T^{(n)}$ the number of visits of the $n$-th patient. To minimize clutter, we describe the algorithms for a single patient and have dropped the superscript $(n)$ whenever it is unambiguous. The goal of predictive modeling is to predict the label at each time step $\mathbf{y}_i \in \{0,1\}^{s}$ or at the end of the sequence $\mathbf{y} \in \{0,1\}^{s}$. The number of labels $s$ can be more than one.

For example, in encounter sequence modeling (ESM) \cite{choi2015doctor}, each visit (\textit{e.g.} encounter) of a patient's visit sequence is represented by a set of varying number of medical codes $\{c_1, c_2, \ldots, c_n\}$. $c_j$ is the $j$-th code from the vocabulary $\mathcal{C}$. Therefore, in ESM, the number of variables $r=|\mathcal{C}|$ and input $\xb_i \in \{0,1\}^{|\mathcal{C}|}$ is a binary vector where the value one in the $j$-th coordinate indicates that $c_j$ was documented in $i$-th visit. Given a sequence of visits $\xb_1, \ldots, \xb_T$, the goal of ESM is, for each time step $i$, to predict the codes occurring at the next visit $\xb_2, \ldots, \xb_{T+1}$, with the number of labels $s=|\mathcal{C}|$.

In case of learning to diagnose (L2D) \cite{lipton2015learning}, the input vector $\xb_i$ consists of continuous clinical measures. If there are $r$ different measurements, then $\xb_i \in \mathbb{R}^{r}$. The goal of L2D is, given an input sequence $\xb_1, \ldots, \xb_T$, to predict the occurrence of a specific disease ($s=1$) or multiple diseases ($s > 1$). 
Without loss of generality, we will describe the algorithm for ESM, as L2D can be seen as a special case of ESM where we make a single prediction at the end of the visit sequence.

In the rest of this section, we will use the abstract symbol $\mathrm{RNN}$ to denote any recurrent neural network variants that can cope with the vanishing gradient problem \cite{bengio1994learning}, such as LSTM \cite{hochreiter1997long}, GRU \cite{cho2014learning}, and IRNN \cite{le2015simple}, with any depth (number of hidden layers). 

\subsection{Preliminaries on Neural Attention Models}
\label{sec:attention}
Attention based neural network models are being successfully applied to image processing \cite{ba2014multiple,mnih2014recurrent,gregor2015draw,xu2015show}, natural language processing \cite{bahdanau2014neural,hermann2015teaching,rush2015neural} and speech recognition \cite{chorowski2015attention}. The utility of the attention mechanism can be seen in the language translation task \cite{bahdanau2014neural} where it is inefficient to represent an entire sentence with one fixed-size vector because neural translation machines finds it difficult to translate the given sentence represented by a single vector. 

Intuitively, the attention mechanism for language translation works as follows: given a sentence of length $S$ in the original language, we generate $\mathbf{h}_1, \ldots, \mathbf{h}_{S}$, to represent the words in the sentence. To find the $j$-th word in the target language, we generate attentions $\alpha_i^j$ for $i=1, \ldots, S$ for each word in the original sentence. Then, we compute the context vector $\mathbf{c}_j = \sum_{i}\alpha_i^j\mathbf{h}_i$ and use it to predict the $j$-th word in the target language.  In general, the attention mechanism allows the model to focus on a specific word (or words) in the given sentence when generating each word in the target language. 

We rely on a conceptually similar temporal attention mechanism to generate interpretable prediction models using EHR data. Our model framework is motivated by and mimics how doctors attend to a patientâ€™s needs and explore the patient record, where there is a focus on specific clinical information (e.g., key risk factors) working from the present to the past.


%
  
\subsection{Reverse Time Attention Model \alg}
\label{ssec:model}
Figure \ref{fig:architecture} shows the high-level overview of our model, where a central feature is to delegate a considerable portion of the prediction responsibility to the process for generating attention weights. This is intended to address, in part, the difficulty with interpreting RNNs where the recurrent weights feed past information to the hidden layer. Therefore, to consider both the visit-level and the variable-level (individual coordinates of $\xb_i$) influence, we use a linear embedding of the input vector $\xb_i$.  That is, we define
\begin{equation}
\vb_i = \Wb_{emb} \xb_i,  \tag{Step 1}
\end{equation}
\noindent where $\vb_i \in \mathbb{R}^{m}$ denotes the embedding of the input vector $\xb_i \in \mathbb{R}^{r}$, $m$ the size of the embedding dimension, $\Wb_{emb} \in \mathbb{R}^{m \times r}$ the embedding matrix to learn. We can alternatively use more sophisticated yet interpretable representations such as those derived from multilayer perceptron (MLP)~\cite{erhan2009visualizing,le2013building}. MLP has been used for representation learning in EHR data \cite{choi2016multi}.


We use two sets of weights, one for the visit-level attention and the other for variable-level attention, respectively. The scalars $\alpha_1, \ldots, \alpha_i$ are the visit-level attention weights that govern the influence of each visit embedding $\vb_1, \ldots, \vb_i$. The vectors $\bm{\beta}_1, \ldots, \bm{\beta}_i$ are the variable-level attention weights that focus on each coordinate of the visit embedding $v_{1,1}, v_{1,2}, \ldots, v_{1,m}, \ldots, v_{i,1}, v_{i,2}, \ldots, v_{i,m}$. 

We use two RNNs, $\mathrm{RNN}_{\alpha}$ and $\mathrm{RNN}_{\bm{\beta}}$, to separately generate $\alpha$'s and $\bm{\beta}$'s as follows,
\begin{align}
\gb_i, \gb_{i-1}, \ldots, \gb_1 &= \mathrm{RNN}_{\alpha}(\vb_i, \vb_{i-1}, \ldots, \vb_1), \nonumber \\
e_j &= \wb_{\alpha}^{\top} \gb_j +  b_{\alpha}, \quad \text{for} \quad j = 1, \ldots, i \label{eq:alpha} \nonumber \\
\alpha_1, \alpha_2, \ldots, \alpha_i &= \soft(e_1, e_2, \ldots, e_i) \tag{Step 2} \nonumber\\
\hb_i, \hb_{i-1}, \ldots, \hb_1 &= \mathrm{RNN}_{\bm{\beta}}(\vb_i, \vb_{i-1}, \ldots, \vb_1) \nonumber \\
\bm{\beta}_j &= \tanh\big(\Wb_{\bm{\beta}}\hb_j + \bb_{\bm{\beta}}\big)  \quad \text{for}\quad j = 1, \ldots, i, \tag{Step 3}\label{eq:beta}
\end{align}
where $\gb_i \in \mathbb{R}^{p}$ is the hidden layer of $\mathrm{RNN}_{\alpha}$ at time step $i$, $\hb_i \in \mathbb{R}^{q}$ the hidden layer of $\mathrm{RNN}_{\bm{\beta}}$ at time step $i$ and $\wb_{\alpha} \in \mathbb{R}^{p}, b_{\alpha} \in \mathbb{R}, \Wb_{\bm{\beta}} \in \mathbb{R}^{m \times q}$ and $\bb_{\bm{\beta}} \in \mathbb{R}^{m}$ are the parameters to learn. The hyperparameters $p$ and $q$ determine the hidden layer size of $\mathrm{RNN}_{\alpha}$ and $\mathrm{RNN}_{\bm{\beta}}$, respectively. Note that for prediction at each timestamp, we generate a new set of attention vectors $\alpha$ and $\bm{\beta}$. For simplicity of notation, we do not include the index for predicting at different time steps. 
In \ref{eq:alpha}, we can use Sparsemax \cite{martins2016softmax} instead of Softmax for sparser attention weights.

\begin{figure}[t]
\centering
\includegraphics[scale=0.25]{./figs/retain}
\caption{Unfolded view of \alg's architecture: Given input sequence $\xb_1, \ldots, \xb_i$, we predict the label $\yb_i$. 
\textbf{Step} $\bm{1}$: Embedding, 
\textbf{Step} $\bm{2}$: generating $\alpha$ values using $\mathrm{RNN}_{\alpha}$, 
\textbf{Step} $\bm{3}$: generating $\bm{\beta}$ values using $\mathrm{RNN}_{\bm{\beta}}$, 
\textbf{Step} $\bm{4}$: Generating the context vector using attention and representation vectors, and 
\textbf{Step} $\bm{5}$: Making prediction.
Note that in Steps 2 and 3 we use RNN in the reversed time.}
\label{fig:architecture}
\end{figure}



As noted, \alg generates the attention vectors by running the RNNs backward in time;  i.e., $\mathrm{RNN}_{\alpha}$ and $\mathrm{RNN}_{\bm{\beta}}$ both take the visit embeddings in a reverse order $\vb_i, \vb_{i-1}, \ldots, \vb_1$. Running the RNN in reversed time order also offers computational advantages since the reverse time order allows us to generate $e$'s and $\bm{\beta}$'s that dynamically change their values when making predictions at different time steps $i=1, 2, \ldots, T$.
This ensures that the attention vectors are modified at each time step, increasing the computational stability of the attention generation process.\footnote{For example, feeding visit embeddings in the original order to $\mathrm{RNN}_{\alpha}$ and $\mathrm{RNN}_{\bm{\beta}}$ will generate the same $e_1$ and $\bm{\beta}_1$ for every time step $i=1, 2, \ldots, T$. Moreover, in many cases, a patient's recent visit records deserve more attention than the old records. Then we need to have $e_{j+1}>e_{j}$ which makes the process computationally unstable for long sequences.} 


Using the generated attentions, we obtain the context vector $\cbb_i$ for a patient up to the $i$-th visit as follows,
\begin{equation}
\cbb_i = \sum_{j=1}^{i} \alpha_j \bm{\beta}_j \odot \vb_j, \tag{Step 4} \label{eq:context}
\end{equation}
where $\odot$ denotes element-wise multiplication. We use the context vector $\cbb_i \in \mathbb{R}^{m}$ to predict the true label $\yb_i \in \{0,1\}^{s}$ as follows,
\begin{equation}
\widehat{\yb}_{i} = \mathrm{Softmax}(\Wb\cbb_i + \bb), \label{eq:softmax} \tag{Step 5}
\end{equation}
where $\Wb \in \mathbb{R}^{s \times m}$ and $\bb \in \mathbb{R}^{s}$ are parameters to learn. We use the cross-entropy to calculate the classification loss as follows, 
\begin{equation} 
\mathcal{L}(\xb_1, \ldots, \xb_T) = -\frac{1}{N}\sum_{n=1}^{N} \frac{1}{T^{(n)}} \sum_{i=1}^{T^{(n)}} \Big( \yb_{i}^{\top} \log(\widehat{\yb}_{i}) + (\mathbf{1} - \yb_{i})^{\top}  \log(\mathbf{1} - \widehat{\yb}_{i})  \Big) \label{eq:cross_entropy}
\end{equation}
where we sum the cross entropy errors from all dimensions of $\widehat{\yb}_{i}$. In case of real-valued output $\yb_i \in \mathbb{R}^{s}$, we can change the cross-entropy in Eq.~\eqref{eq:cross_entropy} to, for example, mean squared error.

Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention mechanism for NLP \cite{bahdanau2014neural} where the words are encoded by RNN and the attention weights are generated by MLP. In contrast, our method uses MLP to embed the visit information to preserve interpretability and uses RNN to generate two sets of attention weights, recovering the sequential information as well as mimicking the behavior of physicians. 
Note that we did not use the timestamp of each visit in our formulation. Using timestamps, however, provides a small improvement in the prediction performance. We propose a method to use timestamps in	 Appendix \ref{sec:timestamp}.
