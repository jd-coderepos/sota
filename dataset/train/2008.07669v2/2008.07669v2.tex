\documentclass{article} 

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[hyperfootnotes=false]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsmath,amsfonts,amsthm}       \usepackage{mathtools}      \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{subcaption}
\usepackage{bbm}
\usepackage{multirow}
\usepackage[inline]{enumitem}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage[capitalise]{cleveref}  \usepackage{comment}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[font=small]{caption}


\graphicspath{{figures/}{../figures/}}
\usepackage{import}
\usepackage{subfiles}
\providecommand{\main}{.}

\newtoggle{arxiv}
\toggletrue{arxiv}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{claim}{Claim}
\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
{\renewcommand\theinnercustomthm{#1}\innercustomthm}
{\endinnercustomthm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Rn}[1]{(-\infty, #1]}
\renewcommand{\d}{\mathop{}\!\mathrm{d}}
\newcommand{\dd}{\mathop{}\!\mathrm{d}} \newcommand{\partialt}{\partial t} \newcommand{\ppt}{\frac{\partial}{\partial t}}
\newcommand{\ddt}{\frac{d}{d t}}
\newcommand{\defeq}{\coloneqq}
\newcommand{\norm}[1]{\left\|{#1}\right\|} \providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\rec}{rec}
\DeclareMathOperator{\coef}{coef}
\DeclareMathOperator{\hippo}{hippo}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\usepackage[numbers,sort]{natbib}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}
\newlength{\defbaselineskip}
\setlength{\defbaselineskip}{\baselineskip}
\setlength{\marginparwidth}{0.8in}


\title{HiPPO: Recurrent Memory with Optimal Polynomial Projections}
\usepackage{authblk}
\author[$\dagger$]{Albert Gu\thanks{Equal contribution. Order determined by coin flip.}}
\author[$\dagger$]{Tri Dao\samethanks}
\author[$\dagger$]{Stefano Ermon}
\author[$\ddagger$]{Atri Rudra}
\author[$\dagger$]{Christopher R{\'e}}
\affil[$\dagger$]{Department of Computer Science, Stanford University}
\affil[$\ddagger$]{Department of Computer Science and Engineering, University at Buffalo, SUNY\vspace{4pt}}
\affil[ ]{{\texttt{\{albertgu,trid\}@stanford.edu}, \texttt{ermon@cs.stanford.edu}, \texttt{atri@buffalo.edu}, \texttt{chrismre@cs.stanford.edu}}}

\begin{document}

\maketitle

\begin{abstract}
  A central problem in learning from sequential data is representing cumulative
  history in an incremental fashion as more data is processed.
  We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series
  by projection onto polynomial bases.
  Given a measure that specifies the importance of each time step in the past,
  HiPPO produces an optimal solution to a natural \emph{online function approximation} problem.
  As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles,
  and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs.
  This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales
  through time to remember all history, avoiding priors on the timescale.
  HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates,
  and bounded gradients.
  By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can
  empirically capture complex temporal dependencies.
  On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new
  state-of-the-art accuracy of 98.3\%.
  Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40\% accuracy.
\end{abstract}




\section{Introduction}
\label{sec:intro}

  Modeling and learning from sequential data is a fundamental problem in
  modern machine learning, underlying tasks such as language modeling, speech
  recognition, video processing, and reinforcement learning.
  A core aspect of modeling long-term and complex temporal dependencies is \emph{memory}, or
  storing and incorporating information from previous time steps.
  The challenge is learning a representation of the entire cumulative history using bounded storage,
  which must be updated online as more data is received.
  

  One established approach is to model
  a state that evolves over time as it incorporates more information.
  The deep learning instantiation of this approach is the recurrent neural network (RNN),
  which is known to suffer from a limited memory horizon~\citep{lstm,jaeger2004harnessing,pascanu2013difficulty} (e.g., the ``vanishing gradients'' problem).
  Although various heuristics have been proposed to overcome this,
  such as gates in the successful LSTM and GRU~\citep{lstm, cho2014learning},
  or higher-order frequencies in the recent Fourier Recurrent
  Unit~\citep{zhang2018learning} and Legendre Memory Unit
  (LMU)~\citep{voelker2019legendre},
  a unified understanding of memory remains a challenge.
  Furthermore, existing methods generally require priors on the sequence length or timescale and are ineffective outside this range~\citep{tallec2018can, voelker2019legendre}; this can be problematic in settings with distribution shift (e.g.\ arising from different instrument sampling rates in medical data~\cite{saab2020weak,shah2018temple}).
  Finally, many of them lack theoretical guarantees on how well they capture long-term
  dependencies, such as gradient bounds.
  To design a better memory representation, we would ideally
  (i) have a unified view of these existing methods,
  (ii) be able to address dependencies of any length without priors on the timescale,
  and
  (iii) have a rigorous theoretical understanding of their memory mechanism.







  Our insight is to phrase \emph{memory} as a technical problem of \emph{online
    function approximation} where a function $f(t) : \R_+ \to \R$ is summarized by storing its
  optimal coefficients in terms of some basis functions.
  This approximation is evaluated with respect to a measure that
  specifies the importance of each time in the past.
  Given this function approximation formulation, orthogonal polynomials (OPs) emerge as a natural basis since
  their optimal coefficients can be expressed in closed form~\citep{chihara}.
  With their rich and well-studied history~\citep{szego}, along with their
  widespread use in approximation theory~\citep{trefethen2019approximation} and
  signal processing~\citep{proakis2001digital}, OPs bring a library of
  techniques to this memory representation problem.
  We formalize a framework, \textbf{HiPPO} (high-order polynomial projection
  operators), which produces operators that project arbitrary functions onto the space of
  orthogonal polynomials with respect to a given measure.
  This general framework allows us to analyze several families of measures,
  where this operator, as a closed-form ODE or linear recurrence, allows fast incremental updating of the optimal polynomial approximation as the input function is revealed through time.


  By posing a formal optimization problem underlying recurrent sequence models,
  the HiPPO framework (Section~\ref{sec:framework})
  generalizes and explains previous methods, unlocks new methods
  appropriate for sequential data at different timescales, and comes with several theoretical guarantees.
  (i) For example, with a short derivation we exactly recover as a special case
  the LMU~\citep{voelker2019legendre} (Section~\ref{subsec:high_order_projection}), which proposes an update rule that projects
  onto fixed-length sliding windows through time.\footnote{The LMU was originally motivated by spiking neural networks in
    modeling biological nervous systems; its derivation is not self-contained but a
    sketch can be pieced together from~\citep{voelker2019legendre,voelker2018improving,voelker2019dynamical}.}
  HiPPO also sheds new light on classic techniques such as the gating mechanism
  of LSTMs and GRUs, which arise in one extreme using only low-order degrees in
  the approximation (Section~\ref{subsec:low_order_projection}).
  (ii)
    By choosing more suitable measures, HiPPO yields a novel mechanism (Scaled Legendre, or LegS) that
    always takes into account the function's full history instead of a sliding window.
    This flexibility removes the need for hyperparameters or priors on the sequence length,
    allowing LegS to generalize to different input timescales.
    (iii)
    The connections to dynamical systems and approximation theory
    allows us to show several theoretical benefits of HiPPO-LegS:
    invariance to input timescale, asymptotically more efficient updates, and
    bounds on gradient flow and approximation error (Section~\ref{sec:theory_legs}).
    

  We integrate the HiPPO memory mechanisms into RNNs, and empirically show that they outperform baselines on standard tasks used to benchmark long-term dependencies.
  On the permuted MNIST dataset, our hyperparameter-free HiPPO-LegS method achieves a new state-of-the-art
  accuracy of 98.3\%,
  beating the previous RNN SoTA by over 1 point and even outperforming models
  with global context such as transformers (Section~\ref{subsec:membenchmark}).
  Next, we demonstrate the timescale robustness of HiPPO-LegS on a novel
  trajectory classification task, where it is able to generalize to unseen
  timescales and handle missing data whereas RNN and neural ODE baselines fail
  (Section~\ref{subsec:exp-timescale}).
  Finally, we validate HiPPO's theory, including computational efficiency and
  scalability, allowing fast and accurate online function reconstruction over
  millions of time steps (Section~\ref{subsec:exp-scalability}).
  Code for reproducing our experiments is available at \url{https://github.com/HazyResearch/hippo-code}.







 

\section{The HiPPO Framework: High-order Polynomial Projection Operators}
\label{sec:framework}

We motivate the problem of online function approximation with projections
as an approach to learning memory representations
(Section~\ref{subsec:hippo-setup}).
Section~\ref{subsec:hippo-framework} describes the general
HiPPO framework to derive memory updates, including a precise definition of the technical problem 
we introduce, and an overview of our approach to solving it.
Section~\ref{subsec:high_order_projection} instantiates the framework to recover
the LMU and yield new memory updates (e.g.\ HiPPO-LagT), demonstrating the generality
of the HiPPO framework.
Section~\ref{sec:discretization} discusses how to convert the main continuous-time results into practical discrete versions.
Finally in Section~\ref{subsec:low_order_projection} we show how gating
in RNNs is an instance of HiPPO memory.


\subsection{HiPPO Problem Setup}
\label{subsec:hippo-setup}





Given an input function $f(t) \in \mathbb{R}$ on $t \ge 0$, many problems require operating on the cumulative \emph{history} $f_{\le t} := f(x) \mid_{x \le t}$ at every time $t \ge 0$,
in order to understand the inputs seen so far and make future predictions.
Since the space of functions is intractably large, the history cannot be perfectly memorized and must be compressed; we propose the general approach of projecting it onto a subspace of bounded dimension.
Thus, our goal is to maintain (online) this compressed representation of the history.
In order to specify this problem fully, we require two ingredients: a way to quantify the approximation, and a suitable subspace.


\paragraph{Function Approximation with respect to a Measure.}
Assessing the quality of an approximation requires defining a distance in function space.
Any probability measure $\mu$ on $[0, \infty)$ equips the space of square integrable functions with inner product
$
  \langle f, g \rangle_\mu = \int_0^\infty f(x) g(x) \d \mu(x),
$
inducing a Hilbert space structure $\mathcal{H}_\mu$ and corresponding norm $\| f \|_{L_2(\mu)} = \langle f, f \rangle_\mu^{1/2}$.

\paragraph{Polynomial Basis Expansion.}
Any $N$-dimensional subspace $\mathcal{G}$ of this function space is a suitable candidate for the approximation.
The parameter $N$ corresponds to the order of the approximation, or the size of the compression;
the projected history can be represented by the $N$ coefficients of its expansion in any basis of $\mathcal{G}$.
For the remainder of this paper, we use the polynomials as a natural basis, so that $\mathcal{G}$ is the set of polynomials of degree less than $N$.
We note that the polynomial basis is very general; for example, the Fourier basis $\sin(nx), \cos(nx)$ can be seen as polynomials on the unit circle $(e^{2\pi i x})^n$ (cf.\ \cref{sec:derivation-fourier}).
In \cref{sec:hippo-framework-details}, we additionally formalize a more
general framework that allows different bases other than polynomials by tilting the measure with another function.

\paragraph{Online Approximation.}
Since we care about approximating $f_{\le t}$ for every time $t$, we also let the measure vary through time.
For every $t$, let $\mu^{(t)}$ be a measure supported on $(-\infty, t]$ (since $f_{\le t}$ is only defined up to time $t$).
Overall, we seek some $g^{(t)}\in\mathcal{G}$ that minimizes $\|f_{\le t} - g^{(t)}\|_{L_2(\mu^{(t)})}$.
Intuitively, the measure $\mu$ controls the importance of various parts of the input domain,
and the basis defines the allowable approximations.
The challenge is how to solve the optimization problem in closed form given $\mu^{(t)}$,
and how these coefficients can be maintained online as $t \to \infty$.




\subsection{General HiPPO framework}
\label{subsec:hippo-framework}

We provide a brief overview of the main ideas behind solving this problem, which provides a surprisingly simple and general strategy for many measure families $\mu^{(t)}$.
This framework builds upon a rich history of the well-studied \emph{orthogonal polynomials} and related transforms in the signal processing literature.
Our formal abstraction (\cref{def:hippo}) departs from prior work on sliding transforms in several ways, which we discuss in detail in \cref{sec:rw-ops}.
For example, our concept of the time-varying measure allows choosing $\mu^{(t)}$ more appropriately, which will lead to solutions with qualitatively different behavior.
\cref{sec:hippo-framework-details} contains the full details and formalisms of our framework.

\paragraph{Calculating the projection through continuous dynamics.}
As mentioned, the approximated function can be represented by the $N$ coefficients of its expansion in any basis;
the first key step is to choose a suitable basis $\{g_n\}_{n < N}$ of $\mathcal{G}$.
Leveraging classic techniques from approximation theory, a natural basis is the set of orthogonal polynomials for the measure $\mu^{(t)}$,
which forms an orthogonal basis of the subspace.
Then the coefficients of the optimal basis expansion are simply $c^{(t)}_n := \langle f_{\le t}, g_n \rangle_{\mu^{(t)}}$.

The second key idea is to differentiate this projection in $t$, where
differentiating through the integral (from the inner product
$\langle f_{\leq t},  g_n \rangle_{\mu^{(t)}}$) will often lead to a self-similar relation
allowing $\frac{d}{dt} c_n(t)$ to be expressed in terms of $(c_k(t))_{k\in [N]}$ and $f(t)$.
Thus the coefficients $c(t) \in \R^N$ should evolve as an ODE, with dynamics determined by $f(t)$.

\paragraph{The HiPPO abstraction: online function approximation.}
\begin{definition}\label{def:hippo}
    Given a time-varying measure family $\mu^{(t)}$ supported on $(-\infty, t]$, an
    $N$-dimensional subspace $\mathcal{G}$ of polynomials, and a continuous
    function $f \colon \mathbb{R}_{\geq 0} \to \mathbb{R}$,
    HiPPO defines a \emph{projection} operator $\proj_t$ and a \emph{coefficient extraction} operator $\coef_t$ at every time $t$, with the following properties:
    \begin{enumerate}[label=(\arabic*), topsep=0em, partopsep=0.0em, itemsep=0em, parsep=0.1em, leftmargin=2em]
      \item $\proj_t$ takes the function $f$ restricted up to time $t$,
      $f_{\leq t} \defeq f(x) \mid_{x \leq t}$, and maps it to a
      polynomial $g^{(t)} \in \mathcal{G}$, that minimizes the approximation error $\|f_{\leq t} - g^{(t)}\|_{L_2(\mu^{(t)})}$.
        \item $\coef_t: \mathcal{G} \to \mathbb{R}^N$ maps the polynomial $g^{(t)}$
        to the coefficients $c(t) \in \mathbb{R}^N$ of the basis of orthogonal
        polynomials defined with respect to the measure $\mu^{(t)}$.
    \end{enumerate}
    The composition $\coef \circ \proj$ is called $\hippo$, which is an operator
    mapping a function $f: \R_{\geq 0} \to \R$ to the optimal projection coefficients
    $c: \R_{\geq 0} \to \R^N$, i.e.\ $(\hippo(f))(t) = \coef_t(\proj_t(f))$.
\end{definition}

For each $t$, the problem of optimal projection
$\proj_t(f)$ is well-defined by the above inner products,
but this is intractable to compute naively.
Our derivations (\cref{sec:derivations}) will show that
the coefficient function $c(t) = \coef_t(\proj_t(f))$ has the form of an ODE
satisfying $\frac{d}{dt} c(t) = A(t) c(t) + B(t) f(t)$ for some $A(t) \in \R^{N \times N}$, $B(t) \in \R^{N \times 1}$.
Thus our results show how to tractably obtain $c^{(t)}$ \emph{online} by solving an
ODE, or more concretely by running a discrete recurrence.
When discretized, HiPPO takes in a sequence of real values and produces a
sequence of $N$-dimensional vectors.






Figure~\ref{fig:framework} illustrates the overall framework when we use uniform measures.
Next, we give our main results showing $\hippo$ for several concrete instantiations of the framework.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/hippo.pdf}
  \caption{
    \textbf{Illustration of the HiPPO framework.}
    (1) For any function $f$, (2) at every time $t$ there is an optimal projection $g^{(t)}$ of $f$ onto the space of polynomials, with respect to a measure $\mu^{(t)}$ weighing the past.
    (3) For an appropriately chosen basis, the corresponding coefficients $c(t)\in\R^N$ representing a compression of the history of $f$ satisfy linear dynamics.
    (4) Discretizing the dynamics yields an efficient closed-form recurrence for online compression of time series $(f_k)_{k\in\N}$.
  }
  \label{fig:framework}
\end{figure}


\subsection{High Order Projection: Measure Families and HiPPO ODEs}
\label{subsec:high_order_projection}

Our main theoretical results are instantiations of HiPPO for various measure families $\mu^{(t)}$.
We provide two examples of natural sliding window measures and the corresponding projection operators.
The unified perspective on memory mechanisms allows us to derive these closed-form solutions with the same strategy, provided in
Appendices \ref{sec:derivation-legt},\ref{sec:derivation-lagt}.
The first explains the core Legendre Memory Unit (LMU)~\citep{voelker2019legendre} update in a principled way and characterizes its limitations,
while the other is novel, demonstrating the generality of the HiPPO framework.
\cref{sec:derivations} contrasts the tradeoffs of these measures
(\cref{fig:measures}), contains proofs of their derivations, and derives
additional HiPPO formulas for other bases such as Fourier (recovering the
Fourier Recurrent Unit~\citep{zhang2018learning}) and Chebyshev.





The \textbf{translated Legendre (LegT)} measures assign uniform weight to the most recent history $[t-\theta, t]$.
There is a hyperparameter $\theta$ representing the length of the sliding window,
or the length of history that is being summarized.
The \textbf{translated Laguerre (LagT)} measures instead use the exponentially decaying measure, assigning more importance to recent history.
\begin{equation*}
    \textbf{LegT}: \mu^{(t)}(x) = \frac{1}{\theta} \mathbb{I}_{[t-\theta, t]}(x)
    \qquad
    \textbf{LagT}: \mu^{(t)}(x)
    = e^{-(t-x)} \mathbb{I}_{(-\infty, t]}(x)
    =
    \begin{cases}
      e^{x-t} & \mbox{if } x \le t \\
      0 & \mbox{if } x > t
    \end{cases}
\end{equation*}





\begin{theorem}
  \label{thm:legt-lagt}
  For LegT and LagT, the $\hippo$ operators satisfying \cref{def:hippo} are given by linear time-invariant (LTI) ODEs $\ddt c(t) = - A c(t) + B f(t)$, where $A \in \R^{N \times N}, B \in \R^{N \times 1}$:

  \small
  \begin{minipage}{.60\linewidth}
    \textnormal{\textbf{LegT}:}
    \vspace{-1em}
    \begin{equation}
      \label{eq:translated-legendre-dynamics}
      A_{nk} =
      \frac{1}{\theta}
      \begin{cases}
        (-1)^{n-k} (2n+1) & \mbox{if } n \ge k \\
        2n+1 & \mbox{if } n \le k
      \end{cases},
      \quad
      B_n = \frac{1}{\theta} (2n+1) (-1)^n
    \end{equation}
  \end{minipage}\hfill \begin{minipage}{.35\linewidth}
    \textnormal{\textbf{LagT}:}
    \vspace{-1em}
    \begin{equation}
      \label{eq:laguerre-dynamics}
      A_{nk} =
      \begin{cases}1 & \mbox{if } n \ge k \\
        0 & \mbox{if } n < k \\
      \end{cases},
      \quad
      B_n = 1
    \end{equation}
  \end{minipage}

\end{theorem}





Equation~\eqref{eq:translated-legendre-dynamics} proves the LMU update \citep[equation (1)]{voelker2019legendre}.
Additionally, our derivation (Appendix~\ref{sec:derivation-legt}) shows that outside of the projections,
there is another source of approximation.
This sliding window update rule requires access to $f(t-\theta)$, which is no longer available;
it instead assumes that the current coefficients $c(t)$ are an accurate enough model of the function $f(x)_{x \le t}$ that $f(t-\theta)$ can be recovered.





\subsection{HiPPO recurrences: from Continuous to Discrete Time with ODE Discretization}
\label{sec:discretization}

Since actual data is inherently discrete (e.g.\ sequences and time series),
we discuss how the HiPPO projection operators can be discretized using standard techniques,
so that the continuous-time HiPPO ODEs become discrete-time linear recurrences.





In the continuous case, these operators consume an input function $f(t)$ and produce an output function $c(t)$.
The discrete time case (i) consumes an input sequence $(f_k)_{k \in \N}$, (ii) implicitly defines a function $f(t)$ where $f(k \cdot \Delta t) = f_k$ for some step
size $\Delta t$, (iii) produces a function $c(t)$ through the ODE dynamics,
and (iv) discretizes back to an output sequence $c_k := c(k \cdot \Delta t)$.


The basic method of discretizating an ODE $\frac{d}{dt} c(t) = u(t, c(t), f(t))$ chooses a step size $\Delta t$ and performs the discrete updates
$c(t+\Delta t) = c(t) + \Delta t \cdot u(t, c(t), f(t))$.\footnote{This is known as the Euler method, used for illustration here; our experiments use the more numerically stable Bilinear and ZOH methods.
\cref{sec:discretization-full} provides a self-contained overview of our full discretization framework.}
In general, this process is sensitive to the \emph{discretization step size} hyperparameter $\Delta t$.

Finally, we note that this provides a way to seamlessly handle timestamped data, even with missing values:
the difference between timestamps indicates the (adaptive) $\Delta t$ to use in discretization~\citep{chen2018neural}.
\cref{sec:discretization-full} contains a full discussion of discretization.




\subsection{Low Order Projection: Memory Mechanisms of Gated RNNs}
\label{subsec:low_order_projection}

As a special case, we consider what happens if we do not incorporate higher-order polynomials in the projection problem.
Specifically, if $N=1$, then the discretized version of HiPPO-LagT \eqref{eq:laguerre-dynamics} becomes
$c(t + \Delta t) = c(t) + \Delta t(-Ac(t) + Bf(t)) = (1 - \Delta t)c(t) + \Delta t f(t)$, since $A = B = 1$.
If the inputs $f(t)$ can depend on the hidden state $c(t)$ and the discretization step size $\Delta t$ is chosen adaptively (as a function of input $f(t)$ and state $c(t)$),
as in RNNs, then this becomes exactly a \emph{gated} RNN.
For instance, by stacking multiple units in parallel and choosing a specific update function,
we obtain the GRU update cell as a special case.\footnote{The LSTM cell update is similar, with a parameterization known as ``tied'' gates \cite{greff2016lstm}.}
In contrast to HiPPO which uses one hidden feature and projects it onto high order polynomials,
these models use many hidden features but only project them with degree 1.
This view sheds light on these classic techniques by showing how they can be derived from first principles.



 

\section{HiPPO-LegS: Scaled Measures for Timescale Robustness}
\label{sec:theory_legs}

Exposing the tight connection between online function approximation and memory allows us to produce memory mechanisms with better theoretical properties, simply by choosing the measure appropriately.
Although sliding windows are common in signal processing (\cref{sec:rw-ops}), a more intuitive approach for memory should \emph{scale} the window over time to avoid forgetting.

Our novel \textbf{scaled Legendre measure (LegS)} assigns uniform weight to all history $[0, t]$:
$\mu^{(t)} = \frac{1}{t} \mathbb{I}_{[0, t]}$. App~\ref{sec:derivations}, Fig.~\ref{fig:measures} compares LegS, LegT, and LagT visually, showing the advantages of the scaled measure.


Simply by specifying the desired measure, specializing the HiPPO framework (Sections~\ref{subsec:hippo-framework}, \ref{sec:discretization})
yields a new memory mechanism (proof in \cref{sec:derivation-legs}).
\begin{theorem}
  \label{thm:legs}
  The continuous- \eqref{eq:scaled-legendre-dynamics}
  and discrete- \eqref{eq:legs-discrete} time dynamics for \textbf{HiPPO-LegS} are:
  \small

  \begin{minipage}{.35\linewidth}
    \begin{align}
      \ddt c(t) &= -\frac{1}{t} A c(t) + \frac{1}{t} B f(t)
      \label{eq:scaled-legendre-dynamics}
      \\
      c_{k+1} &= \left( 1-\frac{A}{k} \right) c_k + \frac{1}{k} B f_k
     \label{eq:legs-discrete}
    \end{align}
  \end{minipage}\hfill
  \begin{minipage}{.52\linewidth}
    \begin{align*}
      A_{nk}
      =
      \begin{cases}
        (2n+1)^{1/2}(2k+1)^{1/2} & \mbox{if } n > k \\
        n+1 & \mbox{if } n = k \\
        0 & \mbox{if } n < k
      \end{cases},
      \qquad
      B_n = (2n+1)^{\frac{1}{2}}
    \end{align*}
  \end{minipage}
\end{theorem}

We show that HiPPO-LegS enjoys favorable
theoretical properties: it is invariant to input timescale, is fast to compute,
and has bounded gradients and approximation error.
All proofs are in \cref{sec:hippo-theory}.

\paragraph{Timescale robustness.}
As the window size of LegS is adaptive, projection onto this measure is intuitively robust to timescales.
Formally, the HiPPO-LegS operator is \emph{timescale-equivariant}:
dilating the input $f$ does not change the approximation coefficients.
\begin{proposition}
    \label{prop:timescale}
    For any scalar $\alpha > 0$, if $h(t) = f(\alpha t)$, then
    $\hippo(h)(t) = \hippo(f)(\alpha t)$.
    \\ In other words, if $\gamma : t \mapsto \alpha t$ is any dilation function, then
    $\hippo(f \circ \gamma) = \hippo(f) \circ \gamma$.
\end{proposition}

Informally, this is reflected by HiPPO-LegS having \emph{no timescale hyperparameters};
in particular, the discrete recurrence \eqref{eq:legs-discrete} is invariant to the discretization step size.\footnote{\eqref{eq:legs-discrete} uses the Euler method for illustration; HiPPO-LegS is invariant to other discretizations (\cref{sec:discretization-full}).}
By contrast, LegT has a hyperparameter $\theta$ for the window size, and both LegT and LagT have a step size hyperparameter $\Delta t$ in the discrete time case.
This hyperparameter is important in practice; \cref{subsec:low_order_projection} showed that $\Delta t$ relates to the gates of RNNs, which are known to be sensitive to their parameterization~\cite{jozefowicz2015empirical,tallec2018can,gu2020improving}.
We empirically demonstrate the benefits of timescale robustness in \cref{subsec:exp-timescale}.









\paragraph{Computational efficiency.}
In order to compute a single step of the discrete HiPPO update, the main operation is multiplication by the (discretized) square matrix $A$.
More general discretization specifically requires fast multiplication for any matrix of the form $I + \Delta t \cdot A$ and $(I - \Delta t \cdot A)^{-1}$ for arbitrary step sizes $\Delta t$.
Although this is generically a $O(N^2)$ operation, LegS operators use a fixed $A$ matrix with special structure
that turns out to have fast multiplication algorithms for any discretization.\footnote{It is known that large families of structured matrices related to orthogonal polynomials are efficient~\cite{de2018two}.}
\begin{proposition}\label{prop:efficiency}
    Under any generalized bilinear transform discretization (cf.\
    \cref{sec:discretization-full}), each step of the HiPPO-LegS recurrence in equation~\eqref{eq:legs-discrete} can be computed in $O(N)$ operations.
\end{proposition}

\cref{subsec:exp-scalability} validates the efficiency of HiPPO layers in practice, where unrolling the discretized versions of \cref{thm:legs} is 10x faster than standard matrix multiplication as done in standard RNNs.

\paragraph{Gradient flow.}
Much effort has been spent to alleviate the \emph{vanishing gradient problem} in RNNs~\cite{pascanu2013difficulty},
where backpropagation-based learning is hindered by gradient magnitudes decaying exponentially in time.
As LegS is designed for memory, it avoids the vanishing gradient issue.
\begin{proposition}\label{prop:gradient-bound}
    For any times $t_0 < t_1$, the gradient norm of HiPPO-LegS operator
    for the output at time $t_1$ with respect to input at time $t_0$ is
    $\left\| \frac{\partial c(t_1)}{\partial f(t_0)} \right\| = \Theta\left( 1/t_1 \right)$.
\end{proposition}


\paragraph{Approximation error bounds.}
The error rate of LegS decreases with the smoothness of the input.

\begin{proposition}\label{prop:approximation_error}
  Let $f \colon \mathbb{R}_+ \to \mathbb{R}$ be a differentiable function, and let
  $g^{(t)} = \proj_t(f)$ be its projection at time $t$ by
  HiPPO-LegS with maximum polynomial degree $N-1$.
  If $f$ is $L$-Lipschitz then $\norm{f_{\leq t} - g^{(t)}} = O(tL/\sqrt{N})$.
  If $f$ has order-$k$ bounded derivatives then
  $\norm{f_{\leq t} - g^{(t)}} = O(t^k N^{-k+1/2})$.
\end{proposition}

 

\section{Empirical Validation}
\label{sec:experiments}

The HiPPO dynamics are simple recurrences that can be easily incorporated into various models.
We validate three claims that suggest that when incorporated into a simple RNN, these methods--especially HiPPO-LegS--yield a recurrent architecture with improved memory capability.
In Section~\ref{subsec:membenchmark}, the HiPPO-LegS RNN outperforms
other RNN approaches in benchmark long-term dependency tasks for RNNs.
Section~\ref{subsec:exp-timescale} shows that HiPPO-LegS RNN is much more robust to
timescale shifts compared to other RNN and neural ODE models.
Section~\ref{subsec:exp-scalability} validates the distinct
theoretical advantages of the HiPPO-LegS memory mechanism, allowing fast and
accurate online function reconstruction over millions of time steps.
Experiment details and additional results are described in \cref{sec:experiment-details}.



\paragraph{Model Architectures.}
We first describe briefly how HiPPO memory updates can be incorporated into a
simple neural network architecture, yielding a simple RNN model reminiscent of
the classic LSTM.
Given inputs $x_t$ or features thereof $f_t = u(x_t)$ in any model, the HiPPO
framework can be used to memorize the history of features $f_t$.
Thus, given any RNN update function $h_{t} = \tau(h_{t-1}, x_t)$, we simply replace
$h_{t-1}$ with a projected version of the entire history of $h$, as described in
Figure~\ref{fig:cell}.
The output of each cell is $h_t$, which can be passed through any downstream module (e.g.\ a classification head trained with cross-entropy) to produce predictions.

We map the vector $h_{t-1}$ to 1D with a learned encoding before passing to
$\hippo$ (full architecture in App.~\ref{subsec:model_architectures}).


\subsection{Long-range Memory Benchmark Tasks}
\label{subsec:membenchmark}


\paragraph{Models and Baselines.}
We consider all of the HiPPO methods (\textbf{LegT}, \textbf{LagT}, and \textbf{LegS}).
As we show that many different update dynamics seem to lead to LTI systems that give sensible results (\cref{subsec:high_order_projection}), we additionally consider the \textbf{Rand} baseline that uses random $A$ and $B$ matrices (normalized appropriately) in its updates,
to confirm that the precise derived dynamics are important.
LegT additionally considers an additional hyperparameter $\theta$, which should be set to the timescale of the data if known a priori; to show the effect of the timescale, we set it to the ideal value as well as values that are too large and small.
The \textbf{MGU} is a minimal gated architecture, equivalent to a GRU without the reset gate. The HiPPO architecture we use is simply the MGU with an additional $\hippo$ intermediate layer.

We also compare to several RNN baselines designed for long-term dependencies, including the
\textbf{LSTM}~\cite{lstm}, \textbf{GRU}~\cite{chung2014empirical}, \textbf{expRNN}~\citep{lezcano2019cheap}, and \textbf{LMU}~\citep{voelker2019legendre}.\footnote{In our experiments, LMU refers to the architecture in~\citep{voelker2019legendre} while LegT uses the one described in \cref{fig:cell}.}

All methods have the same hidden size in our experiments.
In particular, for simplicity and to reduce hyperparameters, HiPPO variants tie the memory size $N$ to the hidden state dimension $d$, so that all methods and baselines have a comparable number of hidden units and parameters.
A more detailed comparison of model architectures is in \cref{subsec:model_architectures}.



\paragraph{Sequential Image Classification on Permuted MNIST.}
The permuted MNIST (pMNIST) task feeds inputs to a model pixel-by-pixel in the order of a fixed permutation.
The model must process the entire image sequentially -- with non-local structure -- before
outputting a classification label, requiring learning long-term dependencies.

\cref{tab:pmnist} shows the validation accuracy on the pMNIST task for
the instantiations of our framework and baselines.
We highlight that LegS has the best performance of all models.
While LegT is close at the optimal hyperparameter $\theta$, its performance can fall off drastically for a mis-specified window length.
LagT also performs well at its best hyperparameter $\Delta t$.

\cref{tab:pmnist} also compares test accuracy of our methods against reported results from the literature, where the LMU was the state-of-the-art for recurrent models.
In addition to RNN-based baselines, other sequence models have been evaluated on this dataset, despite being against the spirit of the task
because they have global receptive field instead of being strictly sequential.
With a test accuracy of 98.3\%, HiPPO-LegS sets a true state-of-the-art accuracy on the permuted MNIST dataset.



\begin{minipage}{.38\linewidth}\iftoggle{arxiv}{\vspace*{-0.3em}{\vspace*{0.6em}}}
  \centering
  \includegraphics[width=\linewidth]{figures/rnncell2.pdf}
  \captionof{figure}{HiPPO incorporated into a simple RNN model.
  $\hippo$ is the HiPPO memory operator which projects the history of the $f_t$ features depending on the chosen measure.
  }
  \label{fig:cell}
\end{minipage}\hfill
\begin{minipage}{.58\linewidth}
    \small
        \centering
        \begin{tabular}{@{}ll@{}}
            \toprule
            Method                     & Val. acc. (\%) \\
            \midrule
            \textbf{-LegS}        & \textbf{98.34}           \\
            -LagT            & 98.15                    \\
            -LegT $\theta = 200$  & 98.0                     \\
            -LegT $\theta = 20$   & 91.75                    \\
            -Rand                 & 69.93 \\
            \midrule
            LMU                        & 97.08                    \\
            ExpRNN                     & 94.67                    \\
            GRU                        & 93.04                    \\
            MGU                        & 89.37                    \\
            RNN                        & 52.98                    \\
            \bottomrule
        \end{tabular}\hfill
        \begin{tabular}{@{}lll@{}}
            \toprule
            Model                                    & Test acc.        \\
            \midrule
            \textbf{HiPPO-LegS}                                & \textbf{98.3} \\
            \midrule
            LSTM~\citep{gu2020improving}              & 95.11         \\
            r-LSTM~\citep{trinh2018learning}         & 95.2          \\
            Dilated RNN~\citep{chang2017dilated}      & 96.1          \\
            IndRNN~\citep{indrnn}                     & 96.0          \\
            URLSTM~\citep{gu2020improving}            & 96.96         \\
            LMU~\citep{voelker2019legendre}           & 97.15         \\
            \midrule
            Transformer~\citep{trinh2018learning}     & 97.9          \\
            TCN~\citep{bai2018empirical} & 97.2          \\
            TrellisNet~\citep{trellisnet}             & 98.13         \\
            \bottomrule
        \end{tabular}
    \captionof{table}{\textbf{(Left)} pMNIST validation, average over 3 seeds. Top: Our methods. Bottom: RNN baselines.
      \textbf{(Right)} Reported test accuracies from previous works. Top: Our
      methods. Middle: Recurrent models. Bottom: Non-recurrent models requiring global receptive field.
    }
    \label{tab:pmnist}
\end{minipage}

\paragraph{Copying task.}
This standard RNN task \cite{arjovsky2016unitary} directly tests memorization, where models must regurgitate a sequence of tokens seen at the beginning of the sequence.
It is well-known that standard models such as LSTMs struggle to solve this task.
\cref{sec:experiment-details}  shows the loss for the Copying task with length $L=200$.
Our proposed update LegS solves the task almost perfectly, while LegT
is very sensitive to the window length hyperparameter.
As expected, most baselines make little progress.








\iftoggle{arxiv}{}{\vspace*{-1em}}

\subsection{Timescale Robustness of HiPPO-LegS}
\label{subsec:exp-timescale}

\paragraph{Timescale priors.}
Sequence models generally benefit from priors on the timescale,
which take the form of additional hyperparameters in standard models.
Examples include the ``forget bias'' of LSTMs which needs to be modified to address long-term dependencies~\cite{jozefowicz2015empirical,tallec2018can},
or the discretization step size $\Delta t$ of HiPPO-Lag and HiPPO-LegT (\cref{sec:discretization}).
The experiments in \cref{subsec:membenchmark} confirm their importance.
\cref{fig:copy200} (Appendix) and \cref{tab:pmnist} ablate these hyperparameters,
showing that for example the sliding window length $\theta$ must be set correctly for LegT.
Additional ablations for other hyperparameters are in \cref{sec:experiment-details}.



\paragraph{Distribution shift in trajectory classification.}
Recent trends in ML have stressed the importance of understanding robustness under distribution shift, when training and testing distributions are not i.i.d.
For time series data, for example, models may be trained on EEG data from one hospital, but deployed at another using instruments with different sampling rates~\citep{shah2018temple, saab2020weak}; or a time series may involve the same trajectory evolving at different speeds.
Following~\citet{kidger2020neural}, we consider the Character Trajectories dataset~\citep{bagnall2018uea},
where the goal is to classify a character from a sequence of pen stroke measurements, collected from one user at a fixed sampling rate.
To emulate timescale shift (e.g.\ testing on another user with slower handwriting),
we consider two standard time series generation processes:
(1) In the setting of sampling an underlying sequence at a fixed rate, we change the test sampling rate; crucially, the sequences are variable length so the models are unable to detect the sampling rate of the data.
(2) In the setting of irregular-sampled (or missing) data with timestamps, we scale the test timestamps.





Recall that the HiPPO framework models the underlying data as a continuous function and interacts with discrete input only through the discretization.
Thus, it seamlessly handles missing or irregularly-sampled data by simply evolving according to the given discretization step sizes (details in \cref{sec:discretization-full}).
Combined with LegS timescale invariance (Prop.~\ref{prop:timescale}), we expect HiPPO-LegS to work automatically in all these settings.
We note that the setting of missing data is a topic of independent interest and we compare against SOTA methods, including the GRU-D~\cite{che2018recurrent} which learns a decay between observations,
and neural ODE methods which models segments between observations with an ODE.

\cref{tab:charactertrajectories} validates that standard models can go catastrophically wrong when tested on sequences at different timescales than expected.
Though all methods achieve near-perfect accuracy ($\geq$ 95\%) without distribution shift,
aside from HiPPO-LegS, no method is able to generalize to unseen timescales.




\begin{table}[ht]
    \small
    \centering
    \caption{Test set accuracy on Character Trajectory classification on out-of-distribution timescales.   }
    \begin{tabular}{lccccccc}
        \toprule
        Model                     & \textbf{LSTM} & \textbf{GRU} & \textbf{GRU-D} & \textbf{ODE-RNN} & \textbf{NCDE} & \textbf{LMU} & \textbf{HiPPO-LegS} \\
        \midrule
        100Hz $\to$ 200Hz         & 31.9          & 25.4         & 23.1           & 41.8             & 44.7          & 6.0          & \textbf{88.8}       \\
        200Hz $\to$ 100Hz         & 28.2          & 64.6         & 25.5           & 31.5             & 11.3          & 13.1         & \textbf{90.1}       \\
        \midrule
        Missing values upsample   & 24.4          & 28.2         & 5.5            & 4.3              & 63.9          & 39.3         & \textbf{94.5}       \\
        Missing values downsample & 34.9          & 27.3         & 7.7            & 7.7              & 69.7          & 67.8         & \textbf{94.9}       \\
        \bottomrule
    \end{tabular}
    \label{tab:charactertrajectories}
\end{table}




\subsection{Theoretical Validation and Scalability}
\label{subsec:exp-scalability}

We empirically show that HiPPO-LegS can scale to capture dependencies
across millions of time steps, and its memory updates are computationally
efficient (processing up to 470,000 time steps/s).

\paragraph{Long-range function approximation.}
We test the ability of
different memory mechanisms in approximating an input function, as described in
the problem setup in Section~\ref{subsec:hippo-setup}.
The model only consists of the memory update (Section~\ref{sec:theory_legs}) and
not the additional RNN architecture.
We choose random samples from a continuous-time band-limited white noise
process, with length $10^6$.
The model is to traverse the input sequence, and then asked to reconstruct the
input, while maintaining no more than 256 units in memory (\cref{fig:function_approx}).
This is a difficult task; the LSTM fails with even sequences of length 1000 (MSE
$\approx$ 0.25).
As shown in Table~\ref{tab:mse_speed}, both the LMU and HiPPO-LegS are able to
accurately reconstruct the input function, validating that HiPPO can solve the
function approximation problem even for very long sequences.
\cref{fig:function_approx} illustrates the function and its approximations, with
HiPPO-LegS almost matching the input function while LSTM unable to do so.

\paragraph{Speed.}
HiPPO-LegS operator is computationally efficient both in theory
(Section~\ref{sec:theory_legs}) and in practice.
We implement the fast update in C++ with Pytorch binding and show in
Table~\ref{tab:mse_speed} that it can perform 470,000 time step updates per second on a single CPU
core, 10x faster than the LSTM and LMU.\footnote{The LMU is only known to be fast with the simple forward Euler
discretization~\citep{voelker2019legendre}, but not with more sophisticated methods such as bilinear and
ZOH that are required to reduce numerical errors for this task.}

\begin{minipage}{.4\linewidth}
  \small
  \centering
  \begin{tabular}{lll}
      \toprule
      Method     & Error  & Speed  \\
      & (MSE) & (elements / sec) \\
      \midrule
      LSTM       & 0.25 & 35,000             \\
      LMU        & 0.05 & 41,000             \\
      HiPPO-LegS & 0.02 & 470,000            \\
      \bottomrule
  \end{tabular}
  \captionof{table}{Function approximation error after 1 million time steps, with 256 hidden units.}
  \label{tab:mse_speed}
\end{minipage}\hfill
\begin{minipage}{.6\linewidth}\centering
  \includegraphics[width=\linewidth]{figures/function_approx.pdf}
  \captionof{figure}{Input function and its reconstructions.}
  \label{fig:function_approx}
\end{minipage}

\subsection{Additional Experiments}
\label{subsec:sequencetasks}

We validate that the HiPPO memory updates also perform well on more generic
sequence prediction tasks not exclusively focused on memory.
Full results and details for these tasks are in \cref{sec:experiment-details}.

\paragraph{Sentiment classification task on the IMDB movie review dataset.}
Our RNNs with HiPPO memory updates
perform on par with the LSTM, while other 
long-range memory approaches such as expRNN perform poorly on this more generic task
(\cref{sec:imdb}).


\paragraph{Mackey spin glass prediction.} This physical simulation task tests the
ability to model chaotic dynamical systems. HiPPO-LegS outperforms the LSTM,
LMU, and the best hybrid LSTM+LMU model from~\cite{voelker2019legendre},
reducing normalized MSE by $30\%$ (\cref{sec:mackey}).

 

\section{Conclusion}
\label{sec:conclusion}

We address the fundamental problem of memory in
sequential data by proposing a framework (HiPPO) that poses the abstraction
of optimal function approximation with respect to time-varying measures.
In addition to unifying and explaining existing memory approaches,
HiPPO unlocks a new method (HiPPO-LegS) that takes a first step toward timescale robustness
and can efficiently handle dependencies across millions of time steps.
We anticipate that the study of this core problem
will be useful in improving a variety of sequence models,
and are excited about future work on integrating our memory mechanisms with other models in addition to RNNs.
We hope to realize the benefits of long-range memory on large-scale tasks such
as speech recognition, video processing, and reinforcement learning.





 

\subsubsection*{Acknowledgments}

We thank
Avner May, Mayee Chen, Dan Fu,
Aditya Grover,
and Daniel L\'{e}vy
for their helpful feedback.
We gratefully acknowledge the support of DARPA under Nos.\ FA87501720095 (D3M),
FA86501827865 (SDH), and FA86501827882 (ASED); NIH under No.\ U54EB020405
(Mobilize), NSF under Nos.\ CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to
Velocity), and 1937301 (RTML); ONR under No.\ N000141712266 (Unifying Weak
Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM,
Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson,
Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance,
Google Cloud, Stanford HAI AWS cloud credit, Swiss Re, and members of the Stanford DAWN project: Teradata,
Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.
The U.S.\ Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright notation thereon.
Any opinions, findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect the views,
policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or
the U.S.\ Government.
Atri Rudraâ€™s research is supported by NSF grant CCF-1763481.


\bibliography{hippo}
\bibliographystyle{plainnat}

\appendix

\newpage

\subfile{src/rw}


\subfile{src/background}

\subfile{src/framework_details}

\subfile{src/derivations}

\subfile{src/theory}

\subfile{src/experiment_details}







\end{document}
