

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{amsmath,amssymb}
\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{array}
\usepackage{makecell}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2020}

\icmltitlerunning{PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions}

\begin{document}

\twocolumn[
\icmltitle{PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions}







\begin{icmlauthorlist}
	\icmlauthor{Zhengyang Shen}{math}
	\icmlauthor{Lingshen He}{info}
	\icmlauthor{Zhouchen Lin}{info}
	\icmlauthor{Jinwen Ma}{math}
\end{icmlauthorlist}

\icmlaffiliation{math}{School of Mathematical Sciences and LMAM, Peking University, Beijing 100871}
\icmlaffiliation{info}{Key Lab. of Machine Perception (MoE), School of EECS, Peking University, Beijing 100871}


\icmlcorrespondingauthor{Zhouchen Lin}{zlin@pku.edu.cn}
\icmlcorrespondingauthor{Jinwen Ma}{jwma@math.pku.edu.cn}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Recent research has shown that incorporating equivariance into neural network architectures is very helpful, and there have been some works investigating the equivariance of networks under group actions. However, as digital images and feature maps are on the discrete meshgrid, corresponding equivariance-preserving transformation groups are very limited. 

In this work, we deal with this issue from the connection between convolutions and partial differential operators (PDOs). In theory, assuming inputs to be smooth, we transform PDOs and propose a system which is equivariant to a much more general continuous group, the -dimension Euclidean group. In implementation, we discretize the system using the numerical schemes of PDOs, deriving approximately equivariant convolutions (PDO-eConvs). Theoretically, the approximation error of PDO-eConvs is of the quadratic order. It is the first time that the error analysis is provided when the equivariance is approximate. Extensive experiments on rotated MNIST and natural image classification show that PDO-eConvs perform competitively yet use parameters much more efficiently. Particularly, compared with Wide ResNets, our methods result in better results using only  parameters.
\end{abstract}

\section{Introduction \label{intro}}
In the past few years, convolutional neural network (CNN) models have become the dominant machine learning methods in the field of computer vision for various tasks, such as image recognition, objective detection and semantic segmentation. Compared with fully-connected neural networks, a significant advantage of CNNs is that they are shift equivariant: shifting an image and then feeding it through a number of layers is the same as feeding the original image and then shifting the resulted feature maps. In other words, the translation symmetry is preserved by each layer. Also, the equivariance property brings in weight sharing, with which we can use parameters more efficiently.

Motivated by this, Cohen and Welling \yrcite{cohen2016group} proposed group equivariant CNNs (G-CNNs), showing how convolutional networks can be generalized to exploit larger groups of symmetries, including rotations and reflections. G-CNNs are equivariant to the group  or \footnote{Generally, the group , which we will use in Section \ref{section4}, denotes the group generated by translations, reflections and rotations by . The group  denotes the group only generated by translations and rotations by .}, and work on square lattices. In addition, Hoogeboom et al. \yrcite{hoogeboom2018hexaconv} proposed HexaConv and showed how one can implement planar convolutions and group convolutions over hexagonal lattices, instead of square ones. As a result, the equivariance is expanded to . However, it seems impossible to design CNNs that are equivariant to the rotation angles other than  () and  () as there does not seem to exist other rotational symmetric discrete lattices on the 2D plane, if one considers equivariance in the ways as \cite{cohen2016group} and \cite{hoogeboom2018hexaconv}.

In order to exploit more symmetries, Weiler et al. \yrcite{weiler2018learning} employed harmonics as steerable filters to achieve exact equivariance to larger transformation groups in the continuous domain. However, they are difficult to preserve strong equivariance when operating on discrete pixel grids, for two main reasons: (i) When a harmonic is sampled on grids with a low rate, it could appear as a lower harmonic, which introduces aliasing artifacts. (ii) With Gaussian radial profiles as radial functions, harmonics ranged out of the sampled kernel support, leading to a high equivariance error on implementation.

From another point of view, a conventional convolutional filter can also be viewed as a linear combination of PDOs, which was proposed by \cite{ruthotto2018deep}. With this new understanding, we assume inputs are smooth functions, and then show how to transform the PDOs and get a system which is exactly equivariant to a much more general continuous transformation group, the -dimension Euclidean group. To implement our theory on discrete digital images, we discretize the system using the numerical schemes of PDOs and get approximately equivariant convolutions. Particularly, the discretized convolutions can achieve a quadratic order equivariance approximation, and it is the first time that the error analysis is provided when the equivariance is approximate. As the derived equivariant convolutions are based on PDOs, we refer to them as PDO-eConvs.

We evaluate the performance of PDO-eConvs on rotated MNIST and natural image classification tasks. Extensive experiments verify that PDO-eConv produces very competitive results and is significantly efficient on parameter learning.. 

Our contributions are as follows:

\begin{itemize}
	\item With the assumption that inputs are smooth, we use PDOs to design a system that is equivariant to a much more general continuous group, the -dimensional Euclidean group.
	\item The equivariance is exact in the continuous domain. It becomes approximate only after the discretization. Moreover, it is the first time that the error analysis is provided when the equivariance is approximate. To be specific, the approximation error of PDO-eConvs is of the quadratic order, indicating a precise approximation.
	\item Extensive experiments on PDO-eConvs show that our methods perform competitively and have significant parameter efficiency. 
\end{itemize}

\section{Prior and Related Work}
\subsection{Equivariant CNNs}

Lenc \& Vedaldi \yrcite{lenc2015understanding} showed that the AlexNet CNN \cite{krizhevsky2012imagenet} trained on ImageNet spontaneously learned representations that are equivariant to flips, scalings and rotations, which supported the idea that equivariance is a good inductive bias for CNNs. Cohen \& Welling \yrcite{cohen2016group,cohen2017steerable} succeeded in incorporating equivariance into neural networks. However, these methods can only deal with a -fold rotational symmetry for images with square pixels. Hoogeboom et al. \yrcite{hoogeboom2018hexaconv} alleviated this limit by implementing planar convolutions and group convolutions over hexagonal lattices. Consequently, they can deal with a -fold rotational symmetry.

Since there does not seem to have more rotational symmetries on lattices in the 2D plane, some works designed approximately equivariant networks w.r.t. larger groups. Zhou et al. \yrcite{zhou2017oriented} and Marcos et al. \yrcite{marcos2017rotation} utilized bilinear interpolation to help produce feature maps at different orientations. They are inherently approximately equivariant. By comparison, ours is exactly equivariant in the continuous domain. Worral et al. \yrcite{worrall2017harmonic} used harmonics to extract features and achieve equivariance to 360-rotation, but the equivariance is destroyed after Gaussion-resampling. Weiler et al. \yrcite{weiler2018learning} and Weiler \& Cesa  \yrcite{cesa2019general} employed harmonics as steerable filters to achieve exact equivariance w.r.t. larger groups in the continuous domain, but the equivariance is difficult to preserve in the discrete domain due to aliasing artifacts and limited kernel support. So they used much larger filters to achieve approximate equivariance, resulting in CNNs with a large computational burden. By contrast, PDO-eConvs can use a relatively small kernel size to achieve theoretically guaranteed exact equivariance in the discrete domain, which makes big difference. 



There are also some empirical approaches for enforcing equivariance. A commonly utilized technique is data augmentation, see e.g. \cite{krizhevsky2012imagenet}. The basic idea is to enrich the training set by transformed samples. Laptev et al. \yrcite{laptev2016ti} used parallel siamese architectures for the considered transformation set and applying the transformation-invariant pooling (TI-Pooling) operator on their outputs. Jaderberg et al. \yrcite{jaderberg2015spatial} applied a differentiable module to actively transform feature maps, and then Esteves et al. \yrcite{esteves2018polar} used this method to help enforce equivariance under rotation and scale transformations. In \cite{sabour2017dynamic,hinton2018matrix}, capsules are used to represent the location information and enforce equivariance. However, these methods learn the transformations directly from datas, which are inferior to those methods incorporating equivariance into architectures for lack of interpretability and reliability.

\subsection{The Relationship between Convolutions and PDOs}

There have been extensive works \cite{jain1978partial,witkin1987scale-space,koenderink1984the,perona1990scale-space,osher1990feature-oriented} utilizing PDOs to process images. The relationship between convolutions and PDOs was presented in \cite{dong2017image,ruthotto2018deep}, where the authors translated convolutional filters to linear combinations of PDOs, and this approximation has good analytical properties. Some works \cite{long2018pde,long2019pde} used this new understanding to help design CNNs. Also, this relationship is an important theoretical foundation of our work.

Actually, there exist some works using PDOs to investigate equivariance. Liu et al. \yrcite{liu2013toward} designed a partial differential equation (PDE) using a linear combination of equivariant PDOs and proposed learning based PDEs, which are naturally shift and rotation equivariant. Fang et al. \yrcite{fang2017feature} further adopted this technique on face recognition task. However, the capacity of learning based PDEs cannot be compared with that of nowadays widely used CNNs.


\section{Mathematical Framework}
In this section we design a group equivariant system using PDOs. To make concepts and notations more explicit, we give a preliminary introduction of groups and equivariance formally. 

\subsection{Prior Knowledge}
\textbf{The Isometry Group}\quad In mathematics, the isometry group is a group consisted of isometry transformations, which preserve the distance of any two points. Particularly, the Euclidean group is the largest isometry group defined on , which we denote as . Given , the isometry transformation is:

where  is an orthogonal matrix, i.e., , and . When , the transformations in (\ref{11}) compose the translation group . Without ambiguity, we use  to denote the translation group in the following text. When ,  degenerates to the orthogonal group, , which contains all the orthogonal transformations, including reflections and rotations. We use  to parameterize .  and  are both subgroups of , 
and   ( is a semidirect-product). We use  to represent the element in , where  and  represent a translation and an orthogonal transformation, respectively. Restricting the domain of  and , we can also use this representation to parametrize any subgroup of . 

\textbf{Actions on Functions}\quad Inputs and intermediate feature maps can be naturally modeled as functions defined in the continuous domain. To be specific, we model the input  as a smooth function defined on  and the intermediate feature map  as a smooth function defined on , where the smoothness of  means that if we use the representation  mentioned above, the feature map  is smooth w.r.t.  when  is fixed. So  can also be viewed as a function defined on  with infinite channels indexed by . We use  and \footnote{For the simplicity of our theory, we require that . However, in implementation, we only require that . The requirement on  is the same.} to denote the function spaces of  and , respectively .

In this way, transformations like rotations and reflections on inputs and feature maps can be mathematically formulated. Here, we introduce two transformations used in our theory.
\begin{itemize}
	\item Suppose that  and , then the transfomation  acts on  in the following way\footnote{We use  to denote that an operator acts on a function.}: 
	


	\item Suppose that  and , then  acts on  in the following way: 	
	
	where  is group product on . Using the representation of , it is of the
	following more detailed form:
	 
	where  is the representation of .
	
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.6\columnwidth]{equivariance.pdf} \caption{The transformation  can be preserved by the mapping .}
	\label{equivariance}
\end{figure}

\textbf{Equivariance}\quad Equivariance measures how the outputs of a mapping transform in a predictable way with the transformation of the inputs. Here, we formulate it in detail. Let  be a mapping from the input feature space to the output feature space and  is a group. A group equivariant  satisfies that

where  can be any input feature map in the input feature space, and  and  denote how the transformation  acts on input features and output features, respectively. 

That is, transforming an input  by a transformation  (forming ) and then passing it through the mapping  should give the same result as first mapping  through  and then transforming the representation. The schema of equivariance is shown in Figure \ref{equivariance}. It is easy to see that if each layer of a network is equivariant, the equivariance can be preserved by the network.

\subsection{Group Equivariant Differential Operators}
We refer to  as a polynomial of  variables parameterized by .  denotes the derivative with respect to the th coordinate of . Obviously, as a polynomial of PDOs ,  is a linear combination of PDOs parameterized by . For example, if , then . 

\subsubsection{Under Orthogonal Transformation}
We transform these PDOs with orthogonal matrices, and define the following differential operator:

where 

and  is an orthogonal matrix. As a compact format, we can also rewrite (\ref{21}) as 

where , which is a gradient operator. Particularly, the canonical operator . From another point of view, the transformation on PDOs can also be viewed as that we transform the coordinate frame according to , and then conduct differential operators on the new coordinate frame (see Figure \ref{rotated}). Particularly, PDOs can be viewed as steerable filters in the sense of \cite{helor1996canonical}, because the transformed versions of PDOs can be expressed as linear combinations of PDOs.

\begin{figure}
	\centering
	\includegraphics[width=0.9\columnwidth]{rotated_coordinate.pdf} \caption{Transformation over coordinate frame.}
	\label{rotated}
\end{figure}

Next, we employ 's to define two differential operators  and . To be specific, we use  to deal with inputs, which maps an input  to a feature map defined on : ,

Then we use  to deal with the resulting feature maps, which maps one feature map  to another feature map defined on : \\
,

where  is an orthogonal matrix and  is a measure on . As for , we use the subscript  to distinguish the differential operators parameterized by different 's. The  on the right hand side should be viewed as a function defined on  indexed by  when the operator  acts on it.

We now show that the above two operators are equivariant under orthogonal transformations and describe how the outputs transform w.r.t. the transformations of inputs.

\begin{theorem}
	If  and , the following rules are satisfied:
	
	where  and  are defined in (\ref{T1}), (\ref{T2}), (\ref{1}) and (\ref{Phi}), respectively.
	\label{theorem1}
\end{theorem}

\begin{proof}
	To prove (\ref{e1}), we need to prove that ,
	
	We first show that
	\small{
		}
	The derivation from the third line to the fourth line is due to the orthogonality of . Thus for any element  in , we have
	
	Furthermore,
	
	Then we have that for any elements  and  in ,
	
	In this way, it is easy to prove that (\ref{detail}) is satisfied for all the differential operator terms in . Finally, as  is a linear combination of above terms, (\ref{detail}) is satisfied. Easily, (\ref{e1}) is satisfied.
	
	As for (\ref{e2}), similarly, ,
	
	The derivation from the third line to the fourth line is due to (\ref{detail}). So (\ref{e2}) is satisfied.  
\end{proof}

Furthermore, as differential operators are naturally translation-equivariant, it is easy to verify that  and  are also equivariant over . Consequently, according to the working spaces, we set a  as the first layer,
followed by multiple 's, inserted by pointwise nonlinearities, e.g., ReLUs, that do not disturb the equivariance. Finally, we can get a system where equivariance can be preserved across multiple layers.

\subsubsection{Under Subgroup of Orthogonal Transformation \label{subgroup}}
The above theorem can be easily extended to subgroups of . Here we consider a subgroup  with the form , where  is a subgroup of . 
Similarly, we denote the smooth feature map defined on  as  and the function space as .

The definition of the differential operator  is the similar with (\ref{1}):

where the only difference is that . If  is a discrete group, the differential operator  is:

where . Following (\ref{T1}) and (\ref{T2}), we can define  and , where . We can get the similar result:

Easily, they are also equivariant w.r.t. .

\section{PDO-eConvs \label{section4}}
In this section, we apply our theory to D digital images, and derive approximately equivariant convolutions in the discrete domain. As they are designed using PDOs, we refer to them as PDO-eConvs. To begin with, we show how to apply PDOs on discrete images and feature maps with convolutional filters, respectively.

\subsection{Differential Operators Acting on Discrete Features}

We can view discrete digital images as samples from smooth functions defined on the 2D plane. Formally, we assume that an image data  represents a two-dimensional grid function obtained by discretizing a smooth function  at the cell-centers of a regular grid with  cells and a mesh size , i.e., for 

where  and . 

Accordingly, intermediate feature maps in CNNs are multi-channel matrices. Similarly, 
it can be seen as the  discretizations of continuous functions defined on , where  and  is a subgroup of . Formally, a feature map  represents a three-dimensional grid function sampled from a smooth function .
For ,

where  and  which represents its channel index. Here, for ease of presentation, we only consider that inputs and intermediate feature maps are all single-valued functions, and the theory can be easily extended to multi-valued functions.

With the understanding that features are sampled from continuous functions, we can implement differential operations on features. Particularly, we use convolutions to approximate differential operations, which have been widely used in image processing. For example, the operator acting on images and feature maps can be approximated by the following  convolutional filter with quadratic precision:

where  denotes the convolution operation.

\subsection{From Group Equivariant Differential Operators to PDO-eConvs}
Firstly, we choose the polynomial  from the connection between differential operators and convolutions. Ruthotto \& Haber \yrcite{ruthotto2018deep} showed that we can relate a  convolutional filter to a differential operator, , which is a linear combination of  linearly independent PDOs\footnote{For ease of presentation, we denote the identity operator as , and view it as a  special PDO.}.

In addition, we observe that all differential operators in (\ref{combination}) can be approximated using  convolutional filters (see Supplementary Material 1.1) with quadratic precision. It is to say that we can always approximate the differential operators defined in (\ref{combination}) using a  filter with quadratic precision. For this reason, we choose 

In this way,  equals , which is also the canonical differential operator of 's, indexed by the identity matrix. Using the transformation in (\ref{21}), we can calculate all the expressions of 's easily. Particularly, these transformed differential operators share the same parameters , indicating greater parameter efficiency.

In computation, we observe that some new partial derivatives, e.g., , may occur in some 's, where . Fortunately, the orders of these new partial derivatives are all below five, and we can use the filters with the size of  (see Supplementary Material 1.2) to approximate them with quadratic precision. 

Now we investigate the group we use. According to (\ref{Phi}) and (\ref{6}), if  is a continuous group, we need to conduct integration. However, for the computation issue, it seems impossible to consider all the orthogonal transformations in .
So we consider  to be a discrete subgroup of . Still, our theory is satisfied for feature maps defined on  (see Section \ref{subgroup}). 
Particularly, noting that  is generated by reflections and rotations, we set the subgroup  to be generated by reflections and rotations by . 
As a result, . If without reflections, . Discrete groups  and  have been introduced in Section \ref{intro}.

Finally, we discretize the equivariant differential operator  with corresponding convolutional filters. As a result, we can get a new operator, , which is actually a set of convolution operators indexed by :

where  indexes all the filters we use,  are derived by substituting (\ref{21}) into (\ref{aa}) and  is the convolutional filter related to the PDO  (e.g.,  and  are related to  and , respectively), then 


Similarly, we can get a new convolution operator  by discretizing (\ref{6}). Without ambiguity, we also use  to denote the corresponding convolution operation. To be specific,

where  is a group product on the group , which respresents the channel index of , and .

We refer to  and  as PDO-eConvs, because they are equivariant convolutions based on PDOs. Following \cite{cohen2016group}, we replace all the conventional convolutions in an existing CNN with our PDO-eConvs, and get the corresponding group equivariant CNN w.r.t. .

\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\columnwidth]{rotated_filters.pdf} \caption{The canonical convolutional filter  and its rotated version .}
	\label{filters}
\end{figure}
Let us have a more detailed look at (\ref{20}). Some convolutional filters like  are of size , thus for some ,  is also of size , while the canonical convolutional filter  is of size . We can explain the phenomenon in this way. By definition, the differential operator  is transformed from . Intuitively, we can also view the convolutional filter  as a transformed version of . We assume the transformation to be the rotation. As shown in Figure \ref{filters},  is a rotated version of , which overflows the original  area. So it makes sense to use a larger filter to represent some transformed filters.
That  is sufficient is because the rotated  mask can always be covered by a  square, noting that .

\subsection{Approximation Error of Equivariance}
When we discretize the differential operators  and , errors occur, leading to equivariance disturbance. Nonetheless, we can still achieve approximate equivariance. Here, we analyze the approximation error of our PDO-eConvs. 



\begin{theorem}
	,
	
	where transformations such as rotations or mirror reflections acting on images are defined as  and transformations acting on feature maps are .
\end{theorem}

\begin{proof}
	, the operator  is a linear combination of differential operators and  is a combination
	of corresponding convolution operators. Hence if  is a smooth function,
	
	i.e.,
	
	Easily, we have
	
From (\ref{e1}) we know that the left hand sides of (\ref{l1}) and (\ref{l2}) equal, hence the right hand sides of the two equation are the same, which results in (\ref{appro}). We can prove (\ref{14}) analogously.  
\end{proof}

\subsection{Weight Initialization Scheme \label{initialize}}
An important practical issue in the training phase is an appropriate initialization of weights.
When the variances of weights are chosen too high or too low, 
the signals propagating through the network are amplified or suppressed exponentially with depth. 
Glorot \& Bengio \yrcite{glorot2010understanding} and He et al. \yrcite{he2015delving} investigated this problem and proposed widely used initialization schemes. However, our filters are not parameterized in a pixel basis but as linear combinations of several PDOs,
thus the above-mentioned initialization schemes cannot directly be adopted for our PDO-eConvs.

To be specific, we consider the canonical filter  in each PDO-eConv, and initialize it with He's initialization scheme \cite{he2015delving}. Then we initialize the parameters  of the PDO-eConv by solving the linear equation

with the initialized . In this way, the canonical filter is initialized with He's initialization scheme. Since other filters are obtained by transforming the canonical filters, they also have appropriate variances. We initialize each  in (\ref{Phiconv}) in the same way. We use this method to initialize all the PDO-eConvs in experiments and all the experiments are implemented using Tensorflow.

\section{Experiments}
\subsection{Rotated MNIST}

The most commonly used dataset for validating rotation-equivariant algorithms is MNIST-rot-12k \cite{larochelle2007empirical}. It contains the handwritten digits of the classical MNIST, rotated by a random angle from  to  (full angle). This dataset contains 12,000 training images and 50,000 test images, respectively. We randomly select 2,000 training images as a validation set. We choose the model with the lowest validation error during training. For preprocessing, we normalize the images using the channel means and standard deviations.

\textbf{Without Data Augmentation}\quad Firstly, we evaluate the performance of PDO-eConvs on MNIST-rot-12k without data augmentation via the CNN architecture used in \cite{cohen2016group}. It contains  layers of  convolutions,  channels in each layer, ReLU functions, batch normalization \cite{ioffe2015batch}, and max pooling after layer . 


We consider the group  and replace each convolution by a -convolution, divided the number of filters by , in order to keep the numbers of parameters nearly the same. Thus we use  filters on each layer. Particularly, batch normalization should be implemented with a single scale and a single bias per PDO-eConv map to preserve equivariance. 

The model is trained using the Adam algorithm \cite{kingma2014adam} with a weight decay of . We use the weight initialization method introduced in Section \ref{initialize} for PDO-eConvs and Xavier initialization \cite{glorot2010understanding} for the fully connected layer. We train using batch size  for  epochs. The initial learning rate is set to  and is divided by  at  and  of the total number of training epochs. We set the dropout rate as .
\begin{table}[t]
	\caption{Error rates on MNIST-rot-12k without data augmentation.}\smallskip
	\centering
	\linespread{1.2}\selectfont
	\resizebox{0.8\columnwidth}{!}{
		\smallskip\begin{tabular}{lcc}
			\hline
			Network & Test Error () & params\\
			\hline
			ScatNet-2 \cite{bruna2013invariant} & 7.48 & -\\
			PCANet-2 \cite{chan2015pcanet} & 7.37 & -\\
			TIRBM \cite{sohn2012learning} & 4.2 & -\\
			\hline
			ORN-8 (ORNAlign) \cite{zhou2017oriented} & 2.25 & 0.53M\\
			TI-Pooling \cite{laptev2016ti} & 2.2 & 13.3M\\
			\hline
			CNN & 5.03 & 22k\\
			G-CNN \cite{cohen2016group}& 2.28 & 25k\\
			\textbf{PDO-eConv (ours)} & \textbf{1.87} & 26k\\
			\hline
		\end{tabular}
	}
	\label{rot}
\end{table}

As shown in Table \ref{rot}, with comparable numbers of parameters, our proposed PDO-eConv achieves  test error, outperforming conventional CNN () and G-CNN (), which is equivariant on group . This is mainly because that our model is rotation-equivariant w.r.t. smaller rotation angles, which brings in better generalization. ORN- also deals with an -fold rotational symmetry and adopts an extra strategy, ORNAlign, to refine feature maps. Compared with ORN-8 (ORNAlign), our method still results in lower test error, using far fewer numbers of parameters (26k vs. 0.53M). TI-Pooling is a representative model of transformation-invariant CNNs, which use parallel siamese architectures. Compared with it, PDO-eConv performs better ( vs. ) using far fewer parameters (26k vs. 13.3M) and has much lower computational complexity.

\begin{table}[b]
	\caption{Competitive results on MNIST-rot-12k.}\smallskip
	\centering
	\linespread{1.2}\selectfont
	\resizebox{0.8\columnwidth}{!}{
		\smallskip\begin{tabular}{lcc}
			\hline
			Method & Test Error () \\
			\hline
			H-Net \cite{worrall2017harmonic} & 1.69\\
			OR-TIPooling \cite{zhou2017oriented} & 1.54\\
			RotEqNet \cite{marcos2017rotation} & 1.09 \\
			PTN-CNN \cite{esteves2018polar} & 0.89\\
			E2CNN \cite{cesa2019general} & 0.716 \\
			SFCNN \cite{weiler2018learning} & 0.714\\		
			\hline
			\textbf{PDO-eConv (ours)} & \textbf{0.709} \\
			\hline
		\end{tabular}
	}
	\label{sota}
\end{table}

\textbf{Competitive Result with Data Augmentation}\quad We compare the performance of our PDO-eConv with some more competitive models, using data augmentation and a larger model with  layers. These layers have 16, 16, 32, 32, 32, 64 and 64 output channels, respectively. We use spatial pooling and orientation pooling after the final PDO-eConv layer, in order to get rotation-invariant features. Following \cite{weiler2018learning}, we augment the dataset with continuous rotations during training time. This model is trained using stochastic gradient descent (SGD) and a Nesterov momentum \cite{sutskever2013importance} of  without dampening. We train this model for  epochs, starting with a learning rate of  and reducing it gradually to .

As shown in Table \ref{sota}, E2CNN and SFCNN achieve  and  test error on rotated MNIST, respectively. Compared with SFCNN, our method achieves a comparable result,  test error, using only  parameters. To be specific, our method uses 0.65M parameters, while SFCNN needs 6.5M parameters. Also, SFCNN used a much larger architecture and larger kernel sizes ( and ), which relate to a much larger computational cost. E2CNN replicates the architecture used in SFCNN, so it also relates to a huge computational cost.

\subsection{Natural Image Classfication}
Although most objects in natural scene images are up-right, rotations could exist in small scales. Besides, equivariance to a transformation group brings in more parameter sharing, which may improve the parameter efficiency. Here we evaluate the performance of our PDO-eConvs on two common natural image datasets, CIFAR-10 (C10) and CIFAR-100 (C100) \cite{krizhevsky2009learning}, respectively.

The two CIFAR datasets consist of colored natural images with  pixels. C10 consists of images drawn from 10 classes and C100 from 100. The training and the test sets contain 50,000 and 10,000 images, respectively. We randomly select 5,000 training images as a validation set. We choose the model with the lowest validation error during training. We adopt a standard data augmentation scheme (mirroring/shifting) \cite{lee2015deeply} that is widely used for these two datasets. For preprocessing, we normalize the images using the channel means and standard deviations.

To evaluate our method, we take ResNet \cite{he2016identity} as the basic model, which consists of an initial convolution layer, followed by three stages of  convolution layers using  filters at stage , followed by a final classification layer ( layers in total). We replace all convolution layers of ResNets by our PDO-eConvs and implement batch normalization with a single scale and a single bias per PDO-eConv map. Also, we scale the number of filters to keep the numbers of parameters approximately the same. All the models are trained using SGD and a Nesterov momentum \cite{sutskever2013importance} of  without dampening. We train using batch size  for  epochs, weight decay of . The initial learning rate is set to  and is divided by  at  and  of the total number of training epochs. Similarly, we use the weight initialization method introduced in Section \ref{initialize} for our PDO-eConvs and Xavier initialization for the fully connected layer. We report the results of our methods in Table \ref{natural images}.

\begin{table}
	\caption{Results on the natural image classification benchmark. In the second column,  is the group where equivariance can be preserved.}\smallskip
	\centering
	\linespread{1.2}\selectfont
	\resizebox{1.0\columnwidth}{!}{ \smallskip
		\centering
		\linespread{1.3}\selectfont
		\begin{tabular}{l|cc|cc|c}
			\hline
			Method &  & Depth& C10 & C100  & params\\
			\hline
ResNet \cite{he2016identity}  &  & 26  &11.5 & 31.66 & 0.37M\\
			HexaConv \cite{hoogeboom2018hexaconv} &   & 26& 9.98 &-  & 0.34M\\
			&  & 26 & 8.64 &-  & 0.34M\\
			PDO-eConv (ours) &   & 26 & 5.65 & 27.13 & 0.36M \\
			&   & 26& 5.38 & 27.00  & 0.37M\\
			\hline
			ResNet &   & 44& 5.61 & 24.08  & 2.64M\\
			G-CNN \cite{cohen2016group} &  & 44 & 4.94 & 23.19  & 2.62M\\
			PDO-eConv (ours) &   & 44& 3.68 & 20.01  & 2.62M\\
			\hline
			ResNet &  & 1001 & 4.92 & 22.71 & 10.3M\\
			Wide ResNet \cite{zagoruyko2016wide} &  & 26  & 4.00 & 19.25 & 36.5M\\
			G-CNN \cite{cohen2016group} &  & 26 & 4.17 & -  & 7.2M\\
			\textbf{PDO-eConv (ours)} &  & 26 & \textbf{3.50} & \textbf{18.40}  & 4.6M\\
			\hline
		\end{tabular}
		\label{natural images}
	}
	\label{table1}
\end{table}

Following HexaConv, we use our PDO-eConvs to establish models that are equivariant to group  (), where  and  (). Using comparable numbers of parameters, our methods perform significantly better than HexaConv ( vs.  on C10). In addition, HexaConvs require extra memory to store hexagonal images while our PDO-eConvs do not need so. 

We evaluate PDO-eConvs using ResNet-44, where  and . Compared with G-CNNs, our PDO-eConvs achieve significantly better performance using comparable numbers of parameters ( vs.  on C10, and  vs.  on C100). When evaluated on ResNet-26, where  , PDO-eConv results in  test error, much better than  resulted from G-CNN, yet using much fewer parameters (4.6M vs. 7.2M). This is mainly because that PDO-eConvs can deal with an -fold rotational symmetry, which exploit more rotational symmetries compared with G-CNN.

Finally, we compare our models with deeper ResNets (ResNet-1001) and wider ResNets (Wide ResNet). As shown in Table \ref{natural images}, PDO-eConvs perform betterr ( vs.   in C10 and  vs.  in C100) using only  parameters (4.6M vs. 36.5M). Particularly, PDO-eConvs can also be viewed as introducing a weight sharing scheme across channels, and the results indicate that our method can not only save parameters, but also improve the performance remarkably.

\section{Conclusion}

We utilize PDOs to design a system which is exactly equivariant to a much more general continuous group, the -dimension Euclidean group. We use numerical schemes to implement these PDOs and derive approximately equivariant convolutions, PDO-eConvs. Particularly, we provide an error analysis and show that the approximation error is of the quadratic order. Extensive experiments verify the effectiveness of our method.

In this work, we only conduct experiments on 2D images. Actually, our theory can deal with the data with any dimension. We will explore more possibilities in the future.

\section*{Acknowledgements}
This work was supported by the National Key Research and Development Program of China under grant 2018AAA0100205. Z. Lin is supported by NSF China (grant no.s 61625301 and 61731018), Major Scientific Research Project of Zhejiang Lab (grant no.s 2019KB0AC01 and 2019KB0AB02), Beijing Academy of Artificial Intelligence, and Qualcomm.

\bibliography{example_paper}
\bibliographystyle{icml2020}


\appendix
\section*{Supplementary Material}
\section{Numerical Schemes of Partial Differential Operators}
\subsection{Filters of Size }



\subsection{Filters of Size }






\end{document}
