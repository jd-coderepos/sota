\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{color,xcolor}
\usepackage{pifont}
\usepackage{subfigure}
\usepackage{colortbl}

\usepackage[accsupp]{axessibility}  



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{TransPose: Keypoint Localization via Transformer}

\author{Sen Yang \quad Zhibin Quan \quad Mu Nie \quad Wankou Yang\thanks{Corresponding author.}\
\mathbf{A}=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right).
\label{grad}\begin{aligned}
G\left(\mathbf{A}_{i,j} \right) 
&\approx\mathbf{A}_{i,j}\cdot\mathbf{W}_f
\cdot\mathbf{W}_v^\top+\mathbf{W}_f=\mathbf{A}_{i,j}\cdot\mathbf{K} + \mathbf{B}
\end{aligned}

\quad\theta^*=\arg\max_{\theta}h_{i^*}(\theta,I).

\mathbf{J}=\left\lbrace j| \mathbf{A}_{i,j}\left( \theta^*,I\right) \geq\delta\right\rbrace,

\operatorname{Encoder}\left(\rho \left(\mathbf{X} \right)\right) = \rho \left(\operatorname{Encoder}\left(\mathbf{X} \right)\right),

\begin{aligned}
P E_{(2i, p_y, :)} &=\sin \left( 2\pi* p_y /(H*10000^{2i / \frac{d}{2}}) \right), \\
P E_{(2 i+1, p_y,:)} &=\cos \left( 2\pi* p_y /(H*10000^{2i / \frac{d}{2}} )\right),  \\
P E_{( 2i,: ,p_x)} &=\sin \left( 2\pi* p_x /(W*10000^{2i / \frac{d}{2}} )\right), \\
P E_{( 2 i+1,:,p_x)} &=\cos \left( 2\pi* p_x /(W*10000^{2i / \frac{d}{2}} )\right),
\end{aligned}

\begin{aligned}
\mathbf{Z}=&\operatorname{LayerNorm}\left( \operatorname{MultiheadSelfAttention}\left( \mathbf{X}\right) + \mathbf{X} \right),  \\
\mathbf{X^*}=&\operatorname{LayerNorm}\left( \operatorname{FFN}\left( \mathbf{Z}\right) + \mathbf{Z} \right),
\end{aligned}

\begin{aligned}
\frac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{x}_j}&=\frac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{z}_i}\frac{\partial \boldsymbol{z}_i}{\partial \boldsymbol{x}_j}\\
&=\frac{\partial f(\boldsymbol{z}_i)}{\partial \boldsymbol{z}_i}(\boldsymbol{1}+\frac{\partial \mathbf{w}_i\mathbf{V}}{\partial \boldsymbol{x}_j})\\
&\approx \mathbf{W}_f(\boldsymbol{1}+\frac{\partial \mathbf{w}_{i,0}\boldsymbol{v}_0+...+\mathbf{w}_{i,j}\boldsymbol{v}_j+...+\mathbf{w}_{i,L-1}\boldsymbol{v}_{L-1}}{\partial \boldsymbol{x}_j})\\
&=\mathbf{W}_f(\boldsymbol{1}+\frac{\partial \mathbf{w}_{i,j}\boldsymbol{v}_j}{\partial \boldsymbol{x}_j})\\
&=\mathbf{W}_f(\boldsymbol{1}+\frac{\partial \mathbf{A}_{i,j}\mathbf{W}_v^\top\boldsymbol{x}_j}{\partial \boldsymbol{x}_j})\\
\end{aligned}

\begin{aligned}
G\left(\mathbf{A}_{i,j} \right) &=\mathbf{W}_f(\boldsymbol{1}+\frac{\partial \mathbf{A}_{i,j}\mathbf{W}_v^\top\boldsymbol{x}_j}{\partial \boldsymbol{x}_j})\\
\\
&=\mathbf{W}_f\left( \boldsymbol{1}+\mathbf{A}_{i,j}\mathbf{W}_v^\top\right) \\
&=\mathbf{A}_{i,j}\mathbf{W}_f\mathbf{W}_v^\top+\mathbf{W}_f\\
&=\underbrace{\mathbf{A}_{i,j}}_{\textbf{Image-Specific: dynamic weights}}\cdot\underbrace{\mathbf{W}_f
	\cdot\mathbf{W}_v^\top+\mathbf{W}_f}_{\textbf{Learned: static weights}}\\
&=\mathbf{A}_{i,j}\cdot\mathbf{K} + \mathbf{B}
\end{aligned}

where  are static weights shared across all positions.  We can see that the function  is approximately linear with , \emph{i.e.}, the degrees of contribution to the prediction  directly depend on its attention scores at those locations.

The last attention layer in Transformer Encoder, whose attention scores are seen as the image-specific weights, aggregate contributions from all locations according to attention scores and finally form the maximum activations in the output heatmaps. Though the layers in FFN and head cannot be ignored\footnote{1. Assuming that the used convolutions extract feature in a limited patch, the global interactions mostly occur at the attention layers. 2. The layer normalization does not affect the interactions between locations.}, they are position-wise operators, which almost linearly transform the attention scores from all the positions with the same transformation. In addition,  where  is the position embedding. Because , the position embedding values also affect the attention scores to some extent.   




\begin{figure*}[h]
	\centering
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas for \textbf{input 1}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_123_T-R-A4.pdf}
		
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas for \textbf{input 1}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_123_T-H-A4.pdf}
	}
	
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas for \textbf{input 2}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_286_T-R-A4.pdf}
		
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas for \textbf{input 2}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_286_T-H-A4.pdf}
	}
	
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas for \textbf{input 3}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_204_T-R-A4.pdf}
		
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas for \textbf{input 3}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_204_T-H-A4.pdf}
		
		
	}
	\quad
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas for \textbf{input 4}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_307_T-R-A4.pdf}
		
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas for \textbf{input 4}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_307_T-H-A4.pdf}
		\label{tph-x}
	}
	
	
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas for \textbf{input 5}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_394_T-R-A4.pdf}
		
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas for \textbf{input 5}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_394_T-H-A4.pdf}
	}
	
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas for \textbf{input 5}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_467_T-R-A4.pdf}
		
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas for \textbf{input 5}.]{
		\includegraphics[width=0.475\linewidth]{figures_pdf/final_attention_map_467_T-H-A4.pdf}
	}
	
	
	\caption{Predicted locations and the dependency areas for different types of keypoints in different models: TP-R-A4 (left column) and TP-H-A4 (right column). In each sub-figure, the first one is the original input image plotted with predicted skeleton. The other maps visualized by the defined dependency area () of the attention matrix in the last layer with a threshold value (0.00075). The predicted location of a keypoint is annotated by a WHITE color pentagram () in each sub-map. Redder area indicates higher attention scores.}
	\label{appendix::final_attention}
\end{figure*}

\begin{figure*}
	\centering
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas of each keypoint in different attention layers.]{
		\includegraphics[width=0.45\linewidth]{figures_pdf/attention_map_91_dependency_T-R-A4.pdf}
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas of each keypoint in different attention layers.]{
		\includegraphics[width=0.45\linewidth]{figures_pdf/attention_map_91_dependency_T-H-A4.pdf}
	}
	\subfigure[\textbf{TP-R-A4:} predictions and dependency areas of each keypoint in different attention layers.]{
		\includegraphics[width=0.45\linewidth]{figures_pdf/attention_map_123_dependency_T-R-A4.pdf}
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and dependency areas of each keypoint in different attention layers.]{
		\includegraphics[width=0.45\linewidth]{figures_pdf/attention_map_123_dependency_T-H-A4.pdf}
	}
	\subfigure[\textbf{TP-R-A4:} predictions and affect areas of each keypoint in different attention layers.]{
		\includegraphics[width=0.45\linewidth]{figures_pdf/attention_map_694_affect_T-R-A4.pdf}
	}
	\quad
	\subfigure[\textbf{TP-H-A4:} predictions and affect areas of each keypoint in different attention layers.]{
		\includegraphics[width=0.45\linewidth]{figures_pdf/attention_map_694_affect_T-H-A4.pdf}
	}
	\caption{\textbf{Dependency areas} (the first two rows) and \textbf{Affected areas} (the last row) in different attention layers for different input images.}
	\label{appendix::dependency-and-affect}
\end{figure*}


\begin{table}
	\renewcommand{\arraystretch}{0.9}
	\centering
	
	\begin{tabular}{c|c}
		\toprule[0.1em]
		
		Backbone &ResNet-S\\
		\midrule \multirow{2}{*}{Stem}&Conv-k7-s2-c64, BN, ReLU\\
		&Pooling-k3-s2\\
		\hline
		\multirow{4}{*}{Blocks}& 3Bottleneck-c64\\
		&  Bottleneck-s2-c128 \\
		&  3Bottleneck-c128\\
		& Conv-k1-s1-c256 \\
		
		\bottomrule[0.1em]
	\end{tabular}
	\caption{The detailed configurations for ResNet-S. Conv-k7-s2-c64 means a convolutional layer with 77 kernel size, 2 stride, and 64 output channels, followed by a BN and ReLU; the same below. The Bottleneck-c64 includes Conv-k1-s1-c64-BN-ReLU, Conv-k3-s1-c64-BN-ReLU, and Conv-k1-s1-c256-BN. Bottleneck-c128 includes Conv-k1-s1-c128-BN-ReLU, Conv-k3-s1-c128-BN-ReLU, and Conv-k1-s1-c512-BN. See details in~\cite{he2016deep}.}
	\label{resnet}
\end{table}

\begin{table}
	\renewcommand{\arraystretch}{0.9}
	\centering
	\begin{tabular}{c|c}
		\toprule[0.1em]
		
		Backbone &HRNet-S-W32(48)\\
		\midrule \multirow{3}{*}{Stem}&Conv-k3-s2-c64, BN, ReLU\\
		&Conv-k3-s2-c64, BN, ReLU\\
		&4Bottleneck-c64\\
		\hline
		\multirow{3}{*}{Blocks}& transition1stage2\\
		&  transition2stage3 \\
		& Conv-k1-s1-c64(92) \\
		
		\bottomrule[0.1em]
	\end{tabular}
	\caption{The detailed configurations for HRNet-S-W32(48). More detailed information about the transition layer and stage blocks are described in the HRNet paper~\cite{sun2019hrnet}.}
	\label{hrnet}
\end{table}




\label{cnn_detail}
\section{Architecture Details}
We report the architecture details of ResNet-S and HRNet-S-W32(48) in Tab.~\ref{resnet} and Tab.~\ref{hrnet}. The ResNet-S* only differs from ResNet-S in that ResNet-S* has 10 Bottleneck-c128 blocks. More details about HRNet-W32 and HRNet-W48 are described in~\cite{sun2019hrnet}.



\section{More Attention Maps Visualizations}
In this section, we show more visualization results of the attention maps from TP-R-A4 (TransPose-R-A4) and TP-H-A4 (TransPose-H-A4) models. The attention maps of the last attention layers of two models are shown in Fig.~\ref{appendix::final_attention}. The attention maps in different attention layers of two models are shown in Fig.~\ref{appendix::dependency-and-affect}.

\end{document}