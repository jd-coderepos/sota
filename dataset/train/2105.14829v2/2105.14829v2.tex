\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}

\IEEEoverridecommandlockouts                              

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption} 
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{array,multirow,graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{todonotes}
\usepackage{subcaption}
\usepackage{amsmath}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bg}{\mathbf{g}}

\newcommand{\ot}{\bo_t}
\newcommand{\st}{\bs_t}
\newcommand{\at}{\ba_t}
\newcommand{\gt}{\bg_t}

\newcommand{\aNbp}{\ba^\pi}
\newcommand{\aNbpt}{\at^\pi}

\newcommand{\otp}{\bo_{t+1}}
\newcommand{\stp}{\bs_{t+1}}
\newcommand{\atp}{\ba_{t+1}}
\newcommand{\op}{\mathcal{O}}
\newcommand{\opt}{\op_t}
\newcommand{\kl}{D_\text{KL}}
\newcommand{\ent}{\mathcal{H}}
\newcommand{\states}{\mathcal{S}}
\newcommand{\actions}{\mathcal{A}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\qattn}{Q}
\newcommand{\qcritic}{Q^{\pi}}
\newcommand{\qattnp}{\theta}
\newcommand{\qcriticp}{\rho}
\newcommand{\pip}{\phi}
\newcommand{\ctr}{f}
\newcommand{\trajectory}{\ensuremath{\tau}}
\newcommand{\obs}{\mathbf{o}}
\newcommand{\rgb}{\mathbf{b}}
\newcommand{\depth}{\mathbf{d}}
\newcommand{\pcd}{\mathbf{p}}
\newcommand{\proprio}{\mathbf{z}}
\newcommand{\replay}{\mathcal{D}}
\newcommand{\matxx}[1]{{\mathtt #1}}
\newcommand{\vecXX}[1]{{\mathbf {#1}}}
\def \matR {{\matxx{R}}}
\def \vect {{\vecXX{t}}}
\def \vece {{\vecXX{e}}}
\def \vech {{\vecXX{h}}}
\def \vecq {{\vecXX{q}}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmaxtwod}{argmax\textit{2D}}
\DeclareMathOperator*{\maxtwod}{max\textit{2D}}
\newcommand{\diff}[1]{\textcolor{blue}{#1}}

\graphicspath{{images/}}

\title{Q-attention: Enabling Efficient Learning for Vision-based Robotic Manipulation}



\author{Stephen James$^{1}$ and Andrew J. Davison$^{1}$\thanks{$^{1}$Stephen James and Andrew J. Davison are with the Dyson Robotics Lab, Imperial College London. {\tt\footnotesize(e-mail: slj12@ic.ac.uk; ajd@doc.ic.ac.uk)}}
\thanks{This work was supported by Dyson Technology Ltd}
}

\begin{document}

\maketitle

\begin{abstract}
Despite the success of reinforcement learning methods, they have yet to have their breakthrough moment when applied to a broad range of robotic manipulation tasks. This is partly due to the fact that reinforcement learning algorithms are notoriously difficult and time consuming to train, which is exacerbated when training from images rather than full-state inputs. As humans perform manipulation tasks, our eyes closely monitor every step of the process with our gaze focusing sequentially on the objects being manipulated. With this in mind, we present our Attention-driven Robotic Manipulation (ARM) algorithm, which is a general manipulation algorithm that can be applied to a range of sparse-rewarded tasks, given only a small number of demonstrations. ARM splits the complex task of manipulation into a 3 stage pipeline: (1) a Q-attention agent extracts relevant pixel locations from RGB and point cloud inputs, (2) a next-best pose agent that accepts crops from the Q-attention agent and outputs poses, and (3) a control agent that takes the goal pose and outputs joint actions. We show that current learning algorithms fail on a range of RLBench tasks, whilst ARM is successful. Videos found at: \url{https://sites.google.com/view/q-attention}.
\end{abstract}

\begin{IEEEkeywords}
Deep Learning in Grasping and Manipulation, Learning from Demonstration, Reinforcement Learning
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{D}{espite} their potential, continuous-control reinforcement learning (RL) algorithms have many flaws: they are notoriously data hungry, often fail with sparse rewards, and struggle with long-horizon tasks. The algorithms for both discrete and continuous RL are almost always evaluated on benchmarks that give shaped rewards \cite{brockman2016openai, tassa2018deepmind}, a privilege that is not feasible for training real-world robotic application across a broad range of tasks. Motivated by the observation that humans focus their gaze close to objects being manipulated \cite{land1999roles}, we propose an Attention-driven Robotic Manipulation (ARM) algorithm that consists of a series of algorithm-agnostic components, that when combined, results in a method that is able to perform a range of challenging, sparsely-rewarded manipulation tasks. 

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{tasks_v2}
\caption{The 8 RLBench tasks used for evaluation. Current learning algorithms fail on all tasks, whilst our method succeeds within a modest number of steps. Note that the position and orientation of objects are placed randomly at the beginning of each episode.}
\label{fig:rlbench_tasks}
\vspace{-5px}
\end{figure}

Our algorithm operates through a pipeline of modules: our novel \textbf{Q-attention} module first extracts relevant pixel locations from RGB and point cloud inputs by treating images as an environment, and pixel locations as actions. Using the pixel locations we crop the RGB and point cloud inputs, significantly reducing input size, and feed this to a next-best-pose continuous-control agent that outputs 6D poses, which is trained with our novel \textbf{confidence-aware critic}.  These goal poses are then used by a control algorithm that continuously outputs motor velocities.

As is common with sparsely-rewarded tasks, we improve initial exploration through the use of demonstrations. However, rather than simply inserting these directly into the replay buffer, we use a \textbf{keyframe discovery} strategy that chooses relevant keyframes along demonstration trajectories. This is crucial for training our Q-attention agent, as the keyframes act as explicit supervision to help guide the Q-attention to choose relevant areas to crop. Without this, many crops used by the next-best pose agent would likely be uninformative in the initial phase of training, leading to poor system performance. Rather than storing the transition from an initial state to a keyframe state, we use our \textbf{demo augmentation} method which also stores the transition from intermediate points along a trajectories to the keyframe states; thus greatly increasing the proportion of initial demo transitions in the replay buffer.

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{architecturev2}
\caption{Summary and architecture of our method. RGB and organised point cloud crops are made by extracting pixel locations from our Q-attention module. These crops are then fed to a continuous control RL algorithm that suggests next-best poses that is trained with a confidence-aware critic. The next best pose is given to a goal-condition control agent that outputs joint velocities. Conv block represented as \textit{Conv(\#channels, filter size, strides)}.}
\label{fig:architecture}
\end{figure*}

All of these improvements result in an algorithm that generalises to new configurations not seen in the demonstrations, and starkly outperforms other state-of-the-art methods when evaluated on 8 RLBench \cite{james2019rlbench} tasks (Figure \ref{fig:rlbench_tasks}) that range in difficulty. To summarise, we propose our main contribution, Q-attention: an off-policy hard attention mechanism that is learned via Q-Learning, rather than the on-policy hard attention and soft attention that is commonly seen in the NLP and vision. This work is the first to propose an off-policy hard attention module which is crucial because our demonstration data, by definition, is off-policy, and therefore renders existing hard attention approaches unusable for demonstration-driven RL. Along with this contribution, we also provide 2 main implementation details: \textbf{(1)} A confidence-aware Q function that predicts pixel-wise Q values and confidence values, resulting in improved actor-critic stability. \textbf{(2)} A keyframe discovery and demo augmentation method that go hand-in-hand to improve the utilisation of demonstrations in RL.

\section{Related Work}

The use of reinforcement learning (RL) is prevalent in many areas of manipulation, such as pushing \cite{finn2017deep}, peg insertion \cite{zeng2018learning}, ball-in-cup \cite{kober2009policy}, cloth manipulation \cite{matas2018sim}, and grasping \cite{kalashnikov2018qt, james2019sim}. Despite the abundance of work in this area, there has yet to be a general manipulation method that can tackle a range of challenging, sparsely-rewarded tasks without needing access to privileged simulation-only abilities (e.g. reset to demonstrations \cite{nair2018overcoming}, asymmetric actor-critic \cite{pinto2017asymmetric}, reward shaping \cite{james20163d, rajeswaran2017learning}, and auxiliary tasks \cite{james2017transferring}).

Crucial to our method is the proposed Q-attention. Soft and hard attention are prominent methods in both natural language processing (NLP) \cite{bahdanau2014neural, vaswani2017attention, devlin2018bert} and computer vision \cite{xu2015show, zhang2019self}. Soft attention deterministically multiplies an attention map over the image feature map, whilst hard attention uses the attention map stochastically to sample one or a few features on the feature map (which is optimised by maximising an approximate variational lower bound or equivalently via (on-policy) REINFORCE \cite{williams1992simple}). Given that we perform non-differentiable cropping, our Q-attention is closest to hard attention, but with the distinction that we learn this in an off-policy way. This is key, as `traditional' hard attention is unable to be used in an off-policy setting. We therefore see Q-attention as an off-policy hard attention. A noteworthy use of visual attention in robotics comes from recent work in visual navigation \cite{lee2020perceptual}, which used model predictive control (MPC) to select regions of interest (ROIs) which are subsequently used for low level vehicle actions.

Related to our Q-attention are methods where pixels from a top-down camera act as high-level actions for tasks such as grasping~\cite{morrison2018closing}, pushing~\cite{zeng2018learning}, and pick-and-place~\cite{zeng2020transporter}. However, it is unclear how these can extend beyond top-down pick-and-place tasks, such as some of the ones featured in this paper, e.g. stacking wine; our paper presents a full 6-DoF manipulation system that can extend to a range of tasks, not just top-down ones. Another related paper \cite{wang2021generalization} takes many random crops to act as candidate keypoints in an 3-DoF top-down imitation learning setting; our method instead learns to deterministically choose the most relevant crop and is part of a 6-DoF RL manipulation system.

Our proposed confidence-aware critic (used to train the next-best pose agent) takes its inspiration from the pose estimation community \cite{wang2019densefusion, wada2020morefusion}. There exists a small amount of work in estimating uncertainty with Q-learning in discrete domains \cite{clements2019estimating, hoel2020tactical}; our work uses a continuous Q-function to predict both Q and confidence values for each pixel, which lead to improved stability when training, and is not used during action selection.

Our approach makes use of demonstrations, which has been applied in a number of works \cite{vecerik2017leveraging, matas2018sim, kalashnikov2018qt, nair2018overcoming}, but while successful, they make limited use of the demonstrations and still can take many samples to converge. Rather than simply inserting these directly into the replay buffer, we instead leverage our keyframe discovery and demo augmentation to maximise demonstration utility. The idea of splitting a trajectory into parts (via our keyframe discovery) is not new. In \cite{akgun2012keyframe}, a sparse set of consecutive poses are constructed from a demo in order to repeat the same motion; our method is a robot learning algorithm that uses demonstrations and exploration to generalise to novel configurations. In \cite{krishnan2019swirl}, transitions that consistently occur across demonstrations are analysed and used to learn a sequence of local reward functions. Our keyframe discovery differs in that it individually analyses trajectories (and so can be used with a single demonstration). Note that these prior works rely on full-state information and have not been shown on vision-based manipulation tasks.

\section{Background}

\begin{algorithm*}[tb]
\caption{ARM}
\label{alg:arm}
\begin{algorithmic}
    \State Initialise Q-attention network $\qattn_{\qattnp}$, twin-critic networks $\qcritic_{\qcriticp_1}$, $\qcritic_{\qcriticp_2}$, and actor network $\pi_\pip$ with random parameters $\qattnp_1$, $\qattnp_2$, $\qcriticp_1$, $\qcriticp_2$, $\pip$
    \State Initialise target networks $\qattnp' \leftarrow \qattnp$, $\qcriticp'_1 \leftarrow \qcriticp_1$, $\qcriticp'_2 \leftarrow \qcriticp_2$
    \State Initialise replay buffer $\replay$ with demos and apply \textbf{keyframe discovery} and \textbf{demo augmentation}
\For{each iteration}
	    \For{each environment step}
	        \State $(\rgb_t, \pcd_t, \proprio_t) \leftarrow \obs_t$
	        \State $(x_t, y_t) \leftarrow \argmaxtwod_{\ba'} \qattn_{\qattnp}((\rgb_t, \pcd_t), \ba')$ \Comment{Use Q-attention to get pixel coordinates}
	        \State $\rgb_t', \pcd_t' \leftarrow crop(\rgb_t, \pcd_t, (x_t, y_t))$
	        \State $\aNbpt \sim \pi_\pip(\aNbpt|(\rgb_t', \pcd_t', \proprio_t))$ \Comment{Sample pose from the policy}
	        \State $\obs_{t+1}, \br \leftarrow env.step(\aNbpt)$  \Comment{Use motion planning to bring arm to the next-best pose.}
	        \State $\mathcal{D} \leftarrow \replay \cup \left\{(\obs_t, \aNbpt, \br, \obs_{t+1}, (x_t, y_t))\right\}$ \Comment{Store the transition in the replay pool}
	    \EndFor
    	\For{each gradient step}
    	    \State $\qattnp \leftarrow \qattnp - \hat \nabla_{\qattnp} J_{\qattn}(\qattnp)$ \Comment{Update Q-attention parameters}
    	    \State $\qcriticp_i \leftarrow \qcriticp_i -  \hat \nabla_{\qcriticp_i} J_{\qcritic}(\qcriticp_i)$ for $i\in\{1, 2\}$ \Comment{Update critic parameters}
    	    \State $\pip \leftarrow \pip -  \hat \nabla_\pip J_\pi(\pip)$ \Comment{Update policy weights}
    	    \State $\qattnp' \leftarrow \tau \qattnp + (1-\tau) \qattnp'$ \Comment{Update Q-attention target network weights}
    	    \State $\qcriticp'_i \leftarrow \tau \qcriticp_i + (1-\tau) \qcriticp'_i$ for $i\in\{1,2\}$ \Comment{Update critic target network weights}
    	\EndFor
    \EndFor
\end{algorithmic}
\end{algorithm*}

The reinforcement learning paradigm assumes an agent interacting with an environment consisting of states $\bs \in \states$, actions $\ba \in \actions$, and a reward function $R(\st,\at)$, where $\st$ and $\at$ are the state and action at time step $t$ respectively. The goal of the agent is then to discover a policy $\pi$ that results in maximising the expectation of the sum of discounted rewards: $\E_\pi [\sum_t \gamma^t R(\st, \at)]$, where future rewards are weighted with respect to the discount factor $\gamma \in [0, 1)$. Each policy $\pi$ has a corresponding value function $Q(s, a)$, which represents the expected return when following the policy after taking action $\ba$ in state $\bs$.

Our Q-attention module builds from Deep Q-learning \cite{mnih2015human}, a method that approximated the value function $Q_\psi$, with a deep convolutional network, whose parameters $\psi$ are optimised by sampling mini-batches from a replay buffer $\replay$ and using stochastic gradient descent to minimise the loss: $\E_{(\st, \at, \stp) \sim \replay} [ (\br + \gamma \max_{\ba'}Q_{\psi'}(\stp, \ba') - Q_{\psi}(\st, \at))^2]$, where $Q_{\psi'}$ is a target network; a periodic copy of the online network $Q_\psi$ which is not directly optimised. Our next-best pose agent builds upon SAC \cite{haarnoja2018soft}, however, the agent is compatible with any off-policy, continuous-control RL algorithm. SAC is a maximum entropy RL algorithm that, in addition to maximising the sum of rewards, also maximises the entropy of a policy: $\E_\pi [ \sum_t \gamma^t [ R(\st, \at)+\alpha \mathcal{H} (\pi (\cdot | \st ) ) ] ]$, where $\alpha$ is a temperature parameter that determines the relative importance between the entropy and reward. The goal then becomes to maximise a soft Q-function $\qcritic_{\qcriticp}$ by minimising the following Bellman residual:
\begin{multline}
\label{eq:softq}
J_{Q}(\theta) = \E_{(\st, \at, \stp) \sim \replay} [ ((\br + \gamma \qcritic_{\qcriticp'}(\stp, \pi_{\pip}(\stp)) \\ - \alpha \log \pi_{\pip}(\at|\st)) - \qcritic_{\qcriticp}(\st, \at))^2].
\end{multline}

The policy is updated towards the Boltzmann policy with temperature $\alpha$, with the Q-function taking the role of (negative) energy. Specifically, the goal is to minimise the Kullback-Leibler divergence between the policy and the Boltzman policy:
\begin{equation}
\pi_{\mathrm{new}}=\arg\min_{\pi^{\prime} \in \Pi} \mathrm{D}_{\mathrm{KL}}\left(\pi^{\prime}\left(\cdot | s_t\right) \| \frac{ \frac{1}{\alpha}\exp \left(Q^{\pi_{\mathrm{old}}}\left(s_t, \cdot\right)\right)}{Z^{\pi_{\mathrm{old}}}\left(s_t\right)}\right).
\end{equation}

Minimising the expected KL-divergence to learn the policy parameters was shown to be equivalent to maximising the expected value of the soft Q-function:
\begin{equation}
\label{eq:softpi}
J_{\pi}(\pip) = \E_{\st \sim \replay} [ \E_{\ba \sim \pi_{\pip}} [ \alpha \log (\pi_{\pip} (\at | \st)) - \qcritic_{\qcriticp}(\st, \at)] ].
\end{equation}

\section{Method}

Our method can be split into a 3-phase pipeline. Phase 1 (Section \ref{sec:phase1}) consists of a high-level pixel agent that selects areas of interest using our novel Q-attention module. Phase 2 (Section \ref{sec:phase2}) consists of a next-best pose prediction phase where the pixel location from the previous phase is used to crop the incoming observations and then predict a 6D pose. Finally, phase 3 (Section \ref{sec:phase3}) is a low-level control agent that accepts the predicted next-best pose and executes a series of actions to reach the given goal. Before training, we fill the replay buffer with demonstrations using our keyframe discovery and demo augmentation strategy (Section \ref{sec:demo_augmentation}) that significantly improves training speed. The full pipeline is summarised in Figure \ref{fig:architecture} and Algorithm \ref{alg:arm}.

All experiments are run in RLBench \cite{james2019rlbench}, a large-scale benchmark and learning environment for vision-guided manipulation built around CoppeliaSim \cite{rohmer2013v} and PyRep \cite{james2019pyrep}. The system assumes we are operating in a partially observable Markov decision process (POMDP), where an observation $\obs$ consists of an RGB image, $\rgb$, an organised point cloud, $\pcd$, and proprioceptive data, $\proprio$ (consisting of end-effector pose and gripper open/close state). Actions consist of a 6D (next-best) pose and gripper action, and the reward function is sparse, giving $100$ on task completion, and $0$ for all other transitions. All three phases (Q-attention, next-best pose, and control module) operate in the same POMDP, but at different action modes. The reward is given (and shared) once all phases have been executed.
At each time step, we extract the RGB image $\rgb$ and depth image $\depth$ from the front-facing camera. Using known camera intrinsics and extrinsics, we process each depth image to produce a point cloud $\pcd$ (in world coordinates) projected from the view of the front-facing camera, producing a $(H \times W \times 3)$ `image'.


\subsection{Q-attention}
\label{sec:phase1}

Motivated by the role of vision and eye movement in the control of human activities \cite{land1999roles}, we propose a Q-attention module that, given RGB and organised point cloud inputs, outputs 2D pixel locations of the next area of interest. With these pixel locations, we crop the RGB and organised point cloud inputs and thus drastically reduce the input size to the next stage of the pipeline. Our Q-attention is explicitly learned via Q-learning, where images are treated as the `environment', and pixel locations are treated as the `actions'.

Given our Q-attention function $\qattn_\qattnp$, we extract the coordinates of pixels with the highest value:
\begin{equation}
\label{eq:qatentionxy}
(\bx_t, \by_t) = \argmaxtwod_{\ba'} \qattn_{\qattnp}(\st^q, \ba'),
\end{equation}
where $\bs^q = (\rgb, \pcd)$ for brevity. The parameters of the Q-attention are optimised by using stochastic gradient descent to minimise the loss:
\begin{multline}
J_{\qattn}(\qattnp) = \E_{(\st^q, \at^q, \stp^q) \sim \replay} [ (\br + \gamma \maxtwod_{\ba'}\qattn_{\qattnp'}(\stp^q, \ba') \\ - \qattn_{\qattnp}((\st^q, \at^q))^2 + \lVert \qattn \rVert],
\end{multline}
where $\qattn_{\qattnp'}$ is the target Q-function, and $\lVert \qattn \rVert$ is an $L2$ loss on the per-pixel output of the Q function (which we call \textit{Q regularisation}); in practice, we found that this leads to increased robustness against the common problem of overestimation of Q values. The Q-attention network follows a light-weight U-Net style architecture \cite{ronneberger2015u}, which is summarised in Figure \ref{fig:architecture}. Example per-pixel outputs of the Q-attention are shown in Figure \ref{fig:Q-attention_vis}. With the suggested coordinates from Q-attention, we perform a ($16 \times 16$) crop on both the ($128 \times 128$) RGB and organised point cloud data: $\rgb', \pcd' = crop(\rgb, \pcd, (\bx, \by))$.

The module shares similar human-inspired motivation to the attention seen in NLP \cite{bahdanau2014neural, vaswani2017attention, devlin2018bert} and computer vision \cite{xu2015show, zhang2019self}, but differs in its formulation. Soft attention multiplies an attention map over the image feature map, whilst hard attention uses the attention map to sample one or a few features on the feature maps or inputs. Given that we perform non-differentiable cropping, we categorise our Q-attention as hard attention, but with a core difference: `traditional' hard attention is optimised (on-policy) by maximising an approximate variational lower bound or equivalently via REINFORCE \cite{williams1992simple}, whereas our Q-attention is trained off-policy; this is crucial because our demonstration data, by definition, is off-policy, and therefore renders existing hard attention approaches unusable for demonstration-driven RL.

\subsection{Next-best Pose Agent}
\label{sec:phase2}

Our next-best pose agent accepts cropped RGB $\rgb'$ and organised point cloud $\pcd'$ inputs, and outputs a 6D pose. This next-best pose agent is run every time the robot reaches the previously selected pose. We represent the 6D pose via a translation $\vece \in \mathcal{R}^3$ and a unit quaternion $\vecq \in \mathcal{R}^4$, and restrict the $w$ output of $\vecq$ to a positive number, therefore restricting the network to output unique unit quaternions. The gripper action $\vech \in \mathcal{R}^1$ lies between 0 and 1, which is then discretised to a binary open/close value. The combined action therefore is $\aNbp = \{\vece, \vecq, \vech\}$.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{qattention_heatmap}
\caption{Visualising the Q values across 4 different points in time on 4 tasks. At each step, RGB and organised point cloud crops are made by extracting pixel locations that have the highest Q-value. Red squares indicated where the crop would be taken. We can see that as time progresses, the attention strength shifts depending on progress in the task; e.g. \textit{`stack\_wine'} starts with high attention on the bottle, but after grasping, attention shifts to the wine rack. Videos of the Q-attention can be found on the project website.}
\label{fig:Q-attention_vis}
\vspace{-15pt}
\end{figure}

To train this next-best pose agent, we use a modified version of SAC \cite{haarnoja2018soft}. Usually when predicting Q-values from visual inputs, the soft Q-function (Equation \ref{eq:softq}) aggregates the pixel features (e.g. via global max-pooling) and predicts a single Q-value. In our work, we propose a confidence-aware soft Q-function. Recent work in 6D pose estimation \cite{wang2019densefusion, wada2020morefusion} has seen the inclusion of a confidence score $c$ with the pose prediction output for each dense-pixel. Inspired by this, we augment our Q function with a per-pixel confidence $c_{ij}$, where we output a confidence score for each Q-value prediction (resulting in a $(16 \times 16 \times 2)$ output). To achieve this, we weight the per-pixel Bellman loss with the per-pixel confidence, and add a confidence regularisation term:
\begin{multline}
J_{\qcritic}(\qcriticp) = \E_{(\st^\pi, \aNbpt, \stp^\pi) \sim \replay} [ ((\br + \gamma \qcritic_{\qcriticp'}(\stp^\pi, \pi_{\pip}(\stp^\pi)) \\ - \alpha \log \pi (\aNbpt|\st^\pi)) - \qcritic_{\qcriticp}(\st^\pi, \aNbpt))^2 c - w \log(c)],
\end{multline}
where $\bs^\pi = (\rgb', \pcd', \proprio)$ for brevity. With this, low confidence will result in a low Bellman error but would incur a high penalty from the second term, and vice versa. We use the Q value that has the highest confidence when training the actor. The intuition for this modification is that accurate Q-values from the critic is crucial for stable actor-critic training, and so choosing the highest confident value from a number of candidates allows for a more stable actor update. As an aside, we also tried applying this confidence-aware method to the policy, though empirically we found no improvement. In practice we make use of the clipped double-Q trick \cite{fujimoto2018addressing}, which takes the minimum Q-value between two Q networks, but have omitted in the equations for brevity. Finally, the actor's policy parameters can be optimised by minimising the loss as defined in Equation \ref{eq:softpi}.

\subsection{Control Agent}
\label{sec:phase3}

Given the next-best pose suggestion from the previous stage, we give this to a goal-conditioned control function $\ctr(\st, \gt)$, which given state $\st$ and goal $\gt$, outputs motor velocities that drives the end-effector towards the goal; in our case $\bg = \aNbp$, i.e. the next-best pose is the goal. This function can take on many forms, but two noteworthy solutions would be either motion planning in combination with a feedback-control or a learnable policy trained with imitation/reinforcement learning. Given that the environmental dynamics are limited in the benchmark, we opted for the motion planning solution.

Given the target pose, we perform path planning using the SBL \cite{sanchez2003single} planner within OMPL \cite{sucan2012ompl}, and use Reflexxes Motion Library for on-line trajectory generation. If the target pose is out of reach, we terminate the episode and supply a reward of $-1$. This path planning and trajectory generation is conveniently encapsulated by the \textit{`ABS\_EE\_POSE\_PLAN\_WORLD\_FRAME'} action mode in RLBench \cite{james2019rlbench}.

\subsection{Keyframe Discovery \& Demo Augmentation}
\label{sec:demo_augmentation}

\begin{figure}[]
\centering
\includegraphics[width=1.0\linewidth]{augmentation}
\caption{Demo augmentation stores the transition from an intermediate point along the trajectories to the keyframe states, thereby increasing the proportion of initial demo transitions in the replay buffer. Here, the black line represents a trajectory, `\textbf{!}' represents keyframes, and dashed blue lines represent the augmented transitions to the keyframes.}
\label{fig:augmentation}
\end{figure}

In this section, we outline how we maximise the utility of given demonstrations in order to complete sparsely reward tasks. We assume to have a teacher policy $\pi^*$ (e.g. motion planners or human teleoperatives) that can generate trajectories consisting of a series of states and actions: $\trajectory = [(\bs_1, \ba_1), \ldots, (\bs_T, \ba_T)]$. In this case, we assume that the demonstrations come from RLBench \cite{james2019rlbench}. 

The \textbf{keyframe discovery} process iterates over each of the demo trajectories $\trajectory$ and runs each of the state-action pairs $(\bs, \ba)$ through a function $K: \mathbb{R}^D \rightarrow \mathbb{B}$ which outputs a Boolean deciding if the given trajectory point should be treated as a keyframe. The keyframe function $K$ could include a number of constraints. In practice we found that performing a disjunction over two simple conditions worked well; these included (1) change in gripper state (a common occurrence when something is grasped or released), and (2) velocities approaching near zero (a common occurrence when entering pre-grasp poses or entering a new phase of a task). It is likely that as tasks get more complex, $K$ will inevitably need to become more sophisticated via learning or simply through more conditions, e.g. sudden changes in direction or joint velocity, large changes in pixel values, etc. Figure \ref{fig:keyframes} shows RGB observations from the keyframe discovery process from 4 tasks.

At each keyframe, we use the known camera intrinsics and extrinsics to project the end-effector pose at state $\stp$ into the image plane of state $\st$, giving us pixel locations of the end-effector at the next keyframe. 

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{keyframe_detection_example}
\caption{Visualising RGB observations of keyframes from the keyframe discovery process on 4 tasks. Here $k$ is the keyframe number. Videos of the keyframe discovery can be found on the project website.}
\label{fig:keyframes}
\vspace{-15pt}
\end{figure}

Using this keyframe discovery method, each trajectory results in $N=length(keyframes)$ transitions being stored into the replay buffer. To further increase the utility of demonstrations, we apply \textbf{demo augmentation} which stores the transition from an intermediate point along the trajectories to the keyframe states. Formally, for each point $(\st, \at)$ along the trajectory starting from keyframe $k_i$, we calculate the transformation of the end-effector pose (taken from $\st$) at time step $t$ to the end-effector pose at the time step associated with keyframe $k_{i+1}$. This transformation can then be used as the action for the next-best pose agent. We repeat this process for every $M$th point along the trajectory (which we set to $M=5$). The demo augmentation is visualised in Figure \ref{fig:augmentation}.


\section{Results}

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{main_results}
\caption{Learning curves for 8 RLBench tasks. Methods include Ours (ARM), behavioural cloning (BC), SAC+AE~\cite{yarats2019improving}, DAC~\cite{kostrikov2018discriminator} (an improved, off-policy version of GAIL~\cite{ho2016generative}), SQIL~\cite{reddy2019sqil}, and DrQ~\cite{kostrikov2020image}. ARM uses the 3-stage pipeline (Q-attention, next-best pose, and control agent), while baselines use the 2-stage pipeline (next-best pose and control agent). Note that baselines operate at the same action-mode as ARM, i.e. they output the next-best pose and then motion planning is used to bring the arm to the pose. All methods receive 100 demos which are stored in the replay buffer prior to training. Solid lines represent the average evaluation over 5 seeds, where the shaded regions represent the $min$ and $max$ values across those trials.}
\label{fig:results_rlbench}
\end{figure*}

In this section, we aim to answer the following questions: (1) Are we able to successfully learn across a range of sparsely-rewarded manipulation tasks? (2) Which of our proposed components contribute the most to our success? (3) How sensitive is our method to the number of demonstrations and to the crop size? To answer these, we benchmark our approach using RLBench \cite{james2019rlbench}. Of the 100 tasks, we select 8 (shown in Figure \ref{fig:rlbench_tasks}) that we believe to be achievable from using only the front-facing camera. We leave tasks that require multiple cameras to future work. RLBench was chosen due to its emphasis on vision-based manipulation benchmarking and because it gives access to a wide variety of tasks with demonstrations. Each task has a sparse reward of $+1$ which is given only on task completion, and $0$ otherwise. 

The first of our questions can be answered by attending to Figure \ref{fig:results_rlbench}. We selected a range of common baselines from the imitation learning and reinforcement learning literature; these include: behavioural cloning (BC), SAC+AE~\cite{yarats2019improving}, DAC~\cite{kostrikov2018discriminator} (an improved, off-policy version of GAIL~\cite{ho2016generative}), SQIL~\cite{reddy2019sqil}, and DrQ~\cite{kostrikov2020image}. The baselines do not have our main contribution (Q-attention), but do have the keyframe discovery and demo augmentation. All methods receive the exact same 100 demonstration sequences, which are loaded into the replay buffer prior to training. The baseline agents are architecturally similar to the next-best pose agent, but with a few differences to account for missing Q-attention (and so receives the full, uncropped RGB and organised point cloud data). The reinforcement learning baseline does not have the confidence-aware critic, and so output single Q-values rather than per-pixel values. Specifically, the architecture uses the same RGB and point cloud fusion as shown in Figure \ref{fig:architecture}. Feature maps from the shared representation are concatenated with the reshaped proprioceptive input and fed to both the actor and critic. The baseline actor uses 3 convolution layers (64 channels, $3 \times 3$ filter size, 2 stride), whose output feature maps are maxpooled and sent through 2 dense layers (64 nodes) and results in an action distribution output. The critic baseline uses 3 residual convolution blocks (128 channels, $3 \times 3$ filter size, 2 stride), whose output feature maps are maxpooled and sent through 2 dense layers (64 nodes) and results in a single Q-value output. All methods use the LeakyReLU activation, layer normalisation in the convolution layers, learning rate of $3\times10^{-3}$, soft target update of $\tau=5^{-4}$, and a reward scaling of $100$. Training and exploration were done asynchronously with a single agent (to emulate a real-world robot training scenario) that would continuously load checkpoints every $100$ training steps. 

The results in Figure \ref{fig:results_rlbench} show that baseline methods are unable to accomplish any RLBench tasks, whilst our method is able to accomplish the tasks in a modest number of environment steps. We suggest that the reason why our results starkly outperform other methods is because of two key reasons that go hand-in-hand: (1) Reducing the input dimensionality through Q-attention that immensely reduces the burden on the (often difficult and unstable to train) continuous control algorithm; (2) Combining this with our keyframe discovery method that enables the Q-attention network to quickly converge and suggest meaningful points of interest to the next-best pose agent. We wish to stress that perhaps given enough training time some of these baseline methods may eventually start to succeed, however we found no evidence of this. To get the reinforcement learning baselines to successfully train on these tasks, it would most likely need access to privileged simulation-only abilities (e.g. reset to demonstrations \cite{nair2018overcoming}, asymmetric actor-critic \cite{pinto2017asymmetric}, reward shaping \cite{rajeswaran2017learning}, or auxiliary tasks \cite{james2017transferring}); this would then render the approach unusable for real-world training. 


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablations_method}
         \caption{Ablation on components of ARM.}
         \label{fig:results_components_ablation}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablations_demos}
         \caption{Ablation on number of demos.}
         \label{fig:results_demo_size}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablations_crops}
         \caption{Ablation of crop size.}
         \label{fig:results_crops}
     \end{subfigure}
        \caption{Ablation study across the easier \textit{`take\_lid\_off\_saucepan'} task and harder \textit{`put\_rubbish\_in\_bin'} task.}
        \label{fig:ablation}
\vspace{-10pt}
\end{figure*}

In Figure \ref{fig:results_components_ablation}, we perform an ablation study to evaluate which of the proposed components contribute the most to the success of our method. To perform this ablation, we chose 2 tasks of varying difficulty: \textit{`take\_lid\_off\_saucepan'} and \textit{`put\_rubbish\_in\_bin'}. The ablation clearly shows that the Q-attention (combined with keyframe discovery) is crucial to achieving the tasks, whilst the demo augmentation, confidence-aware critic, and Q regularisation aid in overall stability and increase final performance. When swapping the Q-attention module with a soft attention \cite{xu2015show} module, we found that performance was similar to that of the `vanilla' baselines. This result is unsurprising, as soft attention is implicitly learned (i.e. without an explicit loss), where as our Q-attention is explicitly learned via (off-policy) Q-learning, and so it can make greater use of the highly-informative keyframes given from the keyframe discovery process. Note that we cannot compare to `traditional' hard attention because it requires on-policy training, as explained in Section \ref{sec:phase1}.

Figure \ref{fig:results_demo_size} shows how robust our method is when varying the number of demonstrations given. The results show that our method performs robustly, even when given 50\% fewer demos, however as the task difficulty increase (from \textit{`take\_lid\_off\_saucepan'} to \textit{`put\_rubbish\_in\_bin'}), the harmful effect of having less demonstrations is more severe. Our final set of experiments in Figure \ref{fig:results_crops} shows how our method performs across varying crop sizes. As the task difficulty increases, the harmful effect of a larger crop size becomes more prominent; suggesting that one of the key benefits of the Q-attention is to drastically reduce the input size to the next-best pose agent, making the RL optimisation much easier. It is clear that a trade-off must be made between choosing smaller crops to increase training size, and choosing larger crops to incorporate more of the surrounding area. We found that setting the crops at $16 \times 16$ across all tasks performed well.

\section{Conclusion}

We have presented our Attention-driven Robotic Manipulation (ARM) algorithm, which is a general manipulation algorithm that can be applied to a range of real-world sparsely-rewarded tasks. We validate our method on 8 RLBench tasks of varying difficulty, and show that many commonly used state-of-the-art methods catastrophically fail. We show that Q-attention (along with the keyframe discovery) is key to our success, whilst the confidence-aware critic and demo augmentation contribute to achieving high final performance. Combining manipulation and deep learning is still in its infancy, and therefore the potential negative societal impact is unclear; however, there is a discussion to be had about the role of automation in general and its potential socioeconomic impact.


Despite our experimental results, there are undoubtedly areas of weakness. The control agent (final agent in the pipeline) uses path planning and on-line trajectory generation, which for these tasks are adequate; however, this would need to be replaced with an alternative agent for tasks that have dynamic environments (e.g. moving target objects, moving obstacles, etc) or complex contact dynamics (e.g. peg-in-hole). We look to future work for swapping this with a goal-conditioned reinforcement learning policy, or similar. Another weakness is that we only evaluate on tasks that can be done with the front-facing camera; however we are keen to explore many of the other tasks RLBench has to offer by adapting the method to accommodate multiple camera inputs in future work. Finally, although our method does not use privileged simulation-only abilities, we believe it is still too sample inefficient to perform real world training in a reasonable amount of time (ideally less than 1 hour), and so we leave improving the sample efficiency even further for future work.

\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}