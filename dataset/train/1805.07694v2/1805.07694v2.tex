\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} \renewcommand{\algorithmicensure}{ \textbf{Output:}} 

\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition}

\author{
	Lei Shi \\
	Institute of Automation	\\
	Chinese Academy of Sciences\\
	{\tt\small lei.shi@nlpr.ia.ac.cn} 
	\and
	Yifan Zhang \\
	Institute of Automation	\\
	Chinese Academy of Sciences\\
	{\tt\small yfzhang@nlpr.ia.ac.cn} 
	\and
	Jian Cheng \\
	Institute of Automation	\\
	Chinese Academy of Sciences\\
	{\tt\small jcheng@nlpr.ia.ac.cn} 
	\and
	Hanqing Lu \\
	Institute of Automation	\\
	Chinese Academy of Sciences\\
	{\tt\small luhq@nlpr.ia.ac.cn} 
}

\maketitle

\begin{abstract}
        Traditional deep methods for skeleton-based action recognition usually structure the skeleton as a coordinates sequence or a pseudo-image to feed to RNNs or CNNs, which cannot explicitly exploit the natural connectivity among the joints. 
       Recently, graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, obtains remarkable performance for skeleton-based action recognition. 
       However, the topology of the graph is set by hand and fixed over all layers, which may be not optimal for the action recognition task and the hierarchical CNN structures. Besides, the first-order information (the coordinate of joints) is mainly used in former GCNs, while the second-order information (the length and direction of bones) is less exploited.
In this work, a novel two-stream nonlocal graph convolutional network is proposed to solve these problems. The topology of the graph in each layer of the model can be either uniformly or individually learned by BP algorithm, which brings more flexibility and generality. Meanwhile, a two-stream framework is proposed to model both of the joints and bones information simultaneously, which further boost the recognition performance. 
       Extensive experiments on two large-scale datasets, NTU-RGB+D and Kinetics, demonstrate the performance of our model exceeds the state-of-the-art by a significant margin.
	\end{abstract}
    


	\section{Introduction}	
    Action recognition methods based on skeleton data have been widely studied and drawn considerable attention, due to their strong adaptability to the dynamic circumstance and complicated background.
    Conventional deep learning based methods manually structure the skeleton to a joint coordinates vector or a pseudo-image, which is then fed into RNNs or CNNs to generate the prediction.
    However, representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints. The skeleton is naturally structured as a graph in a non-Euclidean space with the joints as vertices and the natural connections in human body as edges. The previous methods cannot explicitly exploit the graph structure of skeleton data and are difficult to generalize to skeletons with arbitrary forms.
    Recently, graph convolutional networks (GCNs), which generalizes convolution from image to graph, has been successfully adopted in many applications. For skeleton-based action recognition, \cite{yan_spatial_2018} firstly propose to apply GCN to directly model the skeleton data. They construct a spatial graph based on the physical connection of joints in human body and add the temporal edges between corresponding joints in consecutive frames. A distance-based sampling function is proposed for constructing graph convolution, which is then employed as the basic module to build the final spatiotemporal graph convolutional networks (ST-GCN).

    However, the skeleton graph employed in ST-GCN, which is heuristically predefined, merely represents the physical structure and is not guaranteed to be suitable for action recognition task. For example, it cannot capture the dependency between two hands as they lie far away in the defined graph. Besides, the structure of CNN is hierarchical where different layers contain different-level semantic information. Nevertheless, the graph applied in ST-GCN is shared among all the layers, which lacks flexibility and is insufficient to model the multi-level semantic information contained in all of the layers. Moreover, different samples may need different graphs, which can not be satisfied in the original ST-GCN.
   	Inspired by Non-local neural networks~\cite{wang_non-local_2017}, we propose a non-local graph convolutional neural networks to solve above problems. An adaptively learned global graph structure is set as the parameter of the model, which is trained and updated jointly with other parameters. This data-driven method increases the flexibility of the model and brings more generality to adapt to various tasks. Besides, different layers are applied with different graph parameters to better fit the hierarchical structure of CNNs. Moreover, an individual graph will be calculated according to each sample. The overall architecture of the non-local GCN block is shown in Figure~\ref{fig:nlgcn}.
    
    Another notable problem of traditional methods for skeleton based action recognition is that the feature vector attached to each vertex only contain three coordinates of the joints, which can be regarded as the first-order information of the skeleton data. The second-order information, which capture the feature of bones between joints, is neglected. The skeletons are always visualized with both hinged joints and rigid bones. It is nonintuitive to only employ the joints information to classify human activities. 
    To employ the second-order information of the skeletons, the feature of a bone is represented with a six-dimensional vector which contains its length and direction along three dimensions. Moreover, a two-stream framework is proposed to jointly model the first-order and second-order information, which is verified to be a good practice.
    
    To verify the superiority of the proposed model, extensive experiments are performed on two large-scale datasets, NTU-RGBD\cite{shahroudy_ntu_2016} and Kinetics\cite{kay_kinetics_2017}, where our model achieves the state-of-the-art performance on both of the datasets.
    The main contributions of our work include:
    \begin{itemize}
        \item A non-local graph convolutional block is proposed to adaptively learn the graph structure for different layers and samples, which is trained and updated jointly with model parameters in training process and can better suit the action recognition task.
        \item The second-order information, i.e. the length and direction of bones, is explicitly formulated and combined with the first-order information, i.e. joint coordinates, in a two-stream framework to further improve the recognition performance.
        \item On two large-scale datasets for skeleton-based action recognition, our model exceed the state-of-the-art by a significant margin. \end{itemize}      
    
	\section{Related work}
    \label{sec:relatedwork}
	\subsection{Skeleton-based action recognition}
    Traditional methods design hand-crafted features for skeleton-based action recognition\cite{vemulapalli_human_2014,fernando_modeling_2015}. Vemulapalli et al.~\cite{vemulapalli_human_2014} encode the skeletons with their rotations and translations in Lie group. Fernando et al.~\cite{fernando_modeling_2015} leverage rank pooling method to represent the data with the parameters of ranker.
    With the development of deep learning, recurrent neural networks become popular due to its advantage for modeling sequence data. \cite{du_hierarchical_2015} apply a hierarchical bi-directional RNN model to identify the skeleton sequence, which divides the skeleton into different parts and sends them to different subnetworks. \cite{song_end--end_2017} embed a spatiotemporal attention module in LSTM based model, so that the network can automatically pay attention to the discriminant spatiotemporal region of the skeleton sequence. \cite{zhang_view_2017} introduce the mechanism of view transformation in LSTM based model, which automatically translates the skeleton data to a more advantageous angle for action recognition.
    Recently, CNN has shown the superiority owing to its good parallel ability and easier training process compared with RNN. \cite{kim_interpretable_2017} apply a one-dimensional residual CNN to identify skeleton sequence where the coordinates of joints are directly concatenated. \cite{liu_enhanced_2017} manually design 10 kinds of spatiotemporal images for skeleton encoding, and enhance these images using visual and motion enhancement methods.
    \cite{liu_two-stream_2017} employ both coordinates and motion information of joints as input, and carefully design a transformer to rearrange the order of joints.
    \cite{li_skeleton_2017} employ multi-scale residual networks and various data-augmentation strategies for skeleton-based action recognition.
    Nevertheless, both the CNNs and RNN are failed to fully represent the structure of the skeleton, as they are embedded in the form of graphs instead of a 2D or 3D grids. \cite{yan_spatial_2018} employ graph convolution to directly model the raw skeletons, which eliminates the need of hand-crafted part assignment or traversal rules. 
    \subsection{Graph convolutional neural networks}
    There have been a lot of works for graph convolution, whose principle of constructing GCNs mainly follows two streams: spatial perspective and spectral perspective \cite{bruna_spectral_2014,niepert_learning_2016,henaff_deep_2015,kipf_semi-supervised_2016,duvenaud_convolutional_2015,defferrard_convolutional_2016}.
Spatial perspective methods directly perform the convolution filters on the graph vertices and their neighbors, which are extracted and normalized based on the manually designed rules \cite{niepert_learning_2016}.
    Different with the spatial perspective methods, spectral perspective methods utilize the eigenvalues and eigenvectors of graph Laplace matrices. 
    It performs the graph convolution in frequency domain with the help of graph Fourier transform \cite{shuman_emerging_2013}, which does not need to extract locally connected regions from graphs for each convolutional step \cite{bruna_spectral_2014,defferrard_convolutional_2016}. 
	\subsection{Non-local neural networks}
    The concept of non-local was first proposed in non-local means, which computes a weighted sum of all pixels in an image. It utilize all of the pixels according to their similarity with the center point. The idea is then successfully employed in many other applications. Recently, Wang et al.~\cite{wang_non-local_2017} propose the non-local neural network and achieve the remarkable performance in action recognition area. It present the non-local operations to capture long-range dependencies with deep neural networks, where each response of output feature map is calculated according to all of the features in input feature map.
    
	\section{Methods}
    In Section~\ref{gcn}, we briefly introduce the original spatial-temporal graph convolutional network (ST-GCN).
In Section~\ref{nlgcn}, we present the designed non-local graph convolution block. In Section~\ref{tsn}, we will describe the methods of utilizing the bone information to further boost the performance. 
    
	\subsection{Revisit the ST-GCN}
   	\label{gcn}
    In ST-GCN~\cite{yan_spatial_2018}, the skeleton graph is constructed with joints as vertices and bones as edges. In adjacent frames, the corresponding joints are connected as time edges. The attribute of each vertex is the coordinate vectors of the joint. The left sketch of Fig.~\ref{fig:skeleton} shows an example of the constructed spatial-temporal skeleton graph. Given the graph, multiple layers of spatial-temporal graph convolution operations are applied on the graph to get the high-level feature maps. The global average pooling and SoftMax classifier are then employed to predict the action categories.
    
    \begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\linewidth]{skeleton.pdf}
	\caption{(a).The spatial-temporal graph of skeleton. (b).The partition strategy. Different colors denote different subsets.}
	\label{fig:skeleton}
	\end{figure}

     For spatial dimension, each graph convolution operation is formulated as:

    where  is the feature map and  is the vertex of the graph.  is the sampling area, which is defined as the 1-distance neighbor of the target vertex. As an example, it is showed as colored points in the right sketch of Fig.~\ref{fig:skeleton}.  is the weighting function similar to the original convolution operation, which provides a weight vector corresponding to the given input. Note that the number of weight vectors is fixed while the vertexes in  is varied. A mapping function  is designed to map each vertexes with a unique weight vector. In detail, ST-GCN applies a partition strategy in the frame which divides the neighbors of a vertex into three subsets: 1).the vertex itself; 2).centripetal subset: the neighboring vertexes that are closer to the gravity center of the skeleton; 3).centrifugal subset: the neighboring vertexes that are further to the gravity center.  is the number of subsets to normalize the result. The right sketch of Fig.~\ref{fig:skeleton} shows this strategy, where different colors represent different subsets and each color is corresponded with the individual learnable weight vector.
    

    To implement the spatial graph convolution, the Eq.~\ref{eq:stgcn} is transformed into:
    
    Here,  is the  feature map where  denotes the number of vertexes,  denotes the temporal length and  denotes the number of input channels.  is similar with the  adjacency matrix, whose element  indicates whether the vertex  is in the subset of vertex .  is the normalized diagonal matrix.  is set to  to avoid the empty rows in .  denotes the kernel size of spatial dimension. With partition strategy designed above,  is .  which denotes the self-connections of vertexes.  denotes the connections of centripetal subset and  denotes the centrifugal subset.  is the  weight vector of the  convolution operation.  is a  attention map which indicates the importance of each vertex.  denotes the element-wise matrix multiplication, which means it can only effect the vertexes that are connected with current target.
    
    As for temporal dimension, because the number of neighbors for each vertex is fixed as  (the correspond joints in former frame and later frame), it is straightforward to perform the graph convolution similar with the classical convolution operation. Concretely, we perform a  convolution on the output feature map calculated above.
    
	\subsection{Non-local graph convolutional networks}
    \label{nlgcn}
    
    \begin{figure}[tb]
	\begin{center}
	\includegraphics{nlgcn.pdf}
	\caption{(a).The spatial-temporal graph of skeleton. (b).The partition strategy. Different colors denote different subsets.}
	\label{fig:nlgcn}	
	\end{center}
	\end{figure}

    The spatial-temporal graph convolution for skeleton data described above is calculated based on the given graph , which is manually designed according to the natural connections of human body. However, it is hard to say this structure is the best choice for action recognition. For example, there is no connection between hand and head in the official provided graph of NTU-RGBD dataset. But for many actions such as wiping face and touching head, the relationship between hand and head may be important. So, the connection relationship should not be constrained in the adjacent joints.
Besides, as the information is transferred from lower layers to higher layers, the semantic information contained in different layers is also varied. One shared graph structure can not adapt to this variety. The structure of graph should be updated along the message passing. Moreover, due to the deformation of skeleton for different classes, relations among joints in different samples should also be different. 

    To solve the problem, inspired by the non-local neural network, we propose the non-local graph convolution which directly focus on all of the joints to decide whether there are connections between pairs of vertexes. The graph structure is learned individually for different layers and samples in the training process in an end-to-end manner. Different with the original non-local block, our non-local graph convolution block contains three parts. The first part is the physical graph ( in Fig.~\ref{fig:nlgcn}) same as ST-GCN. The second part is a shared graph () which is same for different samples. It can represent the common pattern of connections between joints. The third part is a individual graph () which will learn a unique graph for different samples. It will capture the unique features of each sample. All of the three parts are important, which is verified in the ablation study in Section~\ref{sec:ablation}.
    
    In detail, according to the Eq.~\ref{eq:stgcni}, the structure of graph is actually decided by . For shared graph, we add   learnable parameter  to represent the connections between each pair of joints. The value of each element denote not only whether there exist edge between two joints, but also the tightness of the connection or the similarly of the two joints. Instead of directly replace the original  with , we make it as a residual connection which is added to . The value of  is initialed with 0. In this way, it can strengthen the flexibility of model without harming the original performance. 
    Note that it can also play the role of attention mechanism performed by  in Eq.~\ref{eq:stgcni} but is more flexible, because the element with  will always be  no matter what  is. We remove the  to avoid the redundant parameter and found it does not effect the performance.
    
    For individual graph, we apply the embedded Gaussian function to calculate the similarity of two joints:
    
    using  convolution to represent the embedding functions, the individual graph for each input feature map is calculated by:
    
    where the  operation normalizes the result of product to .  is the parameters of the  convolution and is initialed with 0. Same as the shared graph, it is also employed as a residual connection. Thus the Eq.~\ref{eq:stgcni} becomes:
    
    Fig.~\ref{fig:nlgcn} shows the structure of the basic non-local graph convolution block. A residual connection, similar with \cite{he_deep_2016}, is added for each block to allow it being inserted into any existed models without breaking its initial behavior.
    


    The convolution for temporal dimension is same as ST-GCN. Both the spatial GCN and temporal GCN are followed with the BN layer and Relu layer. One basic block is the combination of one spatial GCN, one temporal GCN and an additional dropout layer with drop rate as 0.5, showed in Fig.~\ref{fig:block}. Same as the original ST-GCN, a residual connection is added for each block.
    
\begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=0.8\linewidth]{block.pdf}
	\caption{The basic non-local GCN block. Conv\_s represent the spatial GCN and conv\_t represent the temporal GCN, both are followed with the BN and Relu layers. Besides, a residual connection is added for each block.}
	\label{fig:block}	
	\end{center}
	\end{figure}

    The non-local graph convolutional network (NLGCN) is the stack of these basic blocks, showed in Fig.~\ref{fig:stream}. There are totally  blocks. The number of output channels for each unit are , , , , , , ,  and , respectively. After that, a global average pooling layer is performed and the final output is sent to a SoftMax classifier to get the prediction. More details can be found in the code, which will be released afterwards.
    
    \begin{figure}[!htb]
	\begin{center}
	\includegraphics{stream.pdf}
	\caption{The structure of the NLGCN network. There are totally  NLGCN layers. The number of each layer represent the number of input channels, the number of output channels and the stride. GAP represent the global average pooling layer.}
	\label{fig:stream}	
	\end{center}
	\end{figure}
    
    \subsection{Two-stream networks}
    \label{tsn}
    Traditional methods employ coordinates of body joints as input, which have three channels along ,  and  axises. It is the first-order information which only relies on the single joint. However, the second-order information, which represents the bones between joints, is also important for recognition but is neglected or not emphasized. In this sense, we propose to explicitly model the second-order information, the bone information, with another stream to boost the action recognition. 
    In particular, the input graph of the new stream, which we call B-stream, has the same topological structure with the graph of original stream (J-stream). 
Each vertex of B-stream is a vector which represents the length and direction of the bone at the source of the current joint. 
    Since the skeleton data has no ring, each joint can be assigned with a unique bone except for the central joint, which will be filled with 0. The bone vector is calculated by the difference between coordinates of two body joints along the ,  and  axises.
    The overall architecture of 2s-NLGCN is shown in Fig.~\ref{fig:twostream}. The  scores of two streams are added to get the fused score and the final prediction. Other fusion methods are left as the future work.
    
    \begin{figure*}[!htb]
	\begin{center}
	\includegraphics[width=0.9\linewidth]{two-stream.pdf}
	\caption{The overall architecture of 2s-NLGCN. The scores of two stream are added to get the final prediction.}
	\label{fig:twostream}	
	\end{center}
	\end{figure*}

	\section{Experiments}
    To have a head-to-head comparison with ST-GCN, our experiments are conducted on the same two large-scale action recognition datasets: NTU-RGB+D \cite{shahroudy_ntu_2016} and Kinetics \cite{kay_kinetics_2017}. In particular, we first perform exhaustive ablation studies on NTU-RGB+D dataset to examine the contributions of the proposed model components to the recognition performance because it is relatively small compared with Kinetics. Then the final model is evaluated on both NTU-RGB+D and Kinetics to verify the generality and is compared with other state-of-the-art approaches. The joints and connections of two datasets are showed in Fig.~\ref{fig:dataset}.
    
	\begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=1\linewidth]{dataset.pdf}
	\caption{Left is the joint label of Kinetics dataset and right is the joint label of NTU-RGB+D dataset.}
	\label{fig:dataset}	
	\end{center}
	\end{figure}

    \subsection{Datasets}
    \paragraph{NTU-RGB+D:} NTU-RGB+D \cite{shahroudy_ntu_2016} is currently the largest and most widely used in-door captured action recognition dataset, which contains 56,000 action clips in 60 action classes. The clips are performed by 40 volunteers in different age groups ranging from 10 to 35. Each action is captured by 3 cameras at the same height but from different horizontal angles: . The dataset provides 3D joint locations of each frame detected by the Kinect depth sensors. There are 25 joints for each subject in the skeleton sequences while each clip has no more than 2 subjects. The original paper \cite{shahroudy_ntu_2016} of the dataset recommends two benchmarks: 
    1). Cross-subject (X-Sub): the dataset in this benchmark is divided into training set (40,320 clips) and validation set (16,560 clips), where the actors in two subsets are different.
    2).Cross-view (X-View): the training set in this benchmark contains 37,920 clips which are captured by camera 2 and 3, and the validation set contains 18,960 clips which are captured by camera 1.
    We follow this convention and report the top-1 accuracy on both benchmarks.
    
    \paragraph{Kinetics:} Kinetics \cite{kay_kinetics_2017} is a large-scale human action dataset which contains 300,000 videos clips in 400 classes. The video clips are sourced from YouTube videos and have a great variety. It only provides raw video clips without skeleton data. \cite{yan_spatial_2018} estimate the location of 18 joints on every frame of the clips with the public available OpenPose toolbox \cite{cao_realtime_2017}. 2 peoples are selected for multi-person clips based on the average joint confidence. We use their released data to evaluate our model. The dataset is divided into training set (240,000 clips) and validation set (20,000 clips). Follow the evaluation method in \cite{kay_kinetics_2017}, we train the models on the training set and report the top-1 and top-5 accuracies on the validation set.
	
    \subsection{Training details}
     






    \subsection{Ablation Study}
    \label{sec:ablation}
    We examine the effectiveness of the proposed components in two-stream non-local graph convolutional network (2s-NLGCN) in this section with the X-View benchmark on NTU-RGB+D dataset.














    \subsubsection{Non-local GCN.}
    As introduced in Section~\ref{nlgcn}, there are  parts for in NLGCN block, i.e. ,  and . We manually delete one of the part and show their performance in Tab.~\ref{tab:nlgcn}. It shows that adaptively learning the graph is benefit for action recognition and deleting anyone of the three parts will harm the performance. With all three parts together, the model get the best performance.


\begin{table}[htb]
 	\begin{center}
		\label{tab:nlgcn}
		\begin{tabular}{lc}
			\hline
			Methods     & Accuracy ()    \\
			\hline
			STGCN & 92.7        \\
            NLGCN wo/A  & 93.4          \\
            NLGCN wo/B  & 93.3          \\
			NLGCN wo/C  & 93.4       \\
            NLGCN     & 93.7          \\
			\hline
		\end{tabular}
        \end{center}
     	\caption{Comparison of the recognition accuracies when adding NLGCN block with or without ,  and .}
	\end{table}
    
    \subsubsection{Visualization of NLGCN block}
    Fig.\ref{fig:map} shows an example of the learned adjacent matrix by our model for second subset.
	The color of each element  in matrix represents the tightness of the connection between joint  and joint . The left is the original matrix employed in ST-GCN, which is same for different layers and different samples. The right is the learned matrix of the first layer of one sample. It is obviously different from the original matrix, which is more flexible and not constrained to the physical connections of the human body.
    \begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=1\linewidth]{map.pdf}
	\caption{Left is the original adjacent matrix for second subset. Right is the non-local adjacent matrix after adding  and  in Eq.~\ref{eq:nlstgcn}. The color of each element  in matrix represents the tightness of the connection between joint  and joint .}
	\label{fig:map}	
	\end{center}
	\end{figure}

    \begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=0.8\linewidth]{layer357.pdf}
	\caption{Visualization of the edges for  joint of right sketch in Fig.\ref{fig:dataset}. The size of the circle represents the tightness of the connection. From left to right is the visualization of different layers (,  and  layer in Fig.~\ref{fig:stream}).}
	\label{fig:layer357}	
	\end{center}
	\end{figure}

	Fig.\ref{fig:layer357} shows an example of edges connected to  joint in NTU-RGB+D dataset learned by our model. Each circle represents one joint, whose size indicates the tightness of the connection between current joint and the  joint. The examples showed from left to right are the visualizations of the second subset of ,  and  layers in Fig.~\ref{fig:stream}, respectively. It shows the connection and their tightness is individual for different layers. It verified our viewpoint that different layers contain different-level semantic information, which should own distinct graphs.
    
	\begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=0.8\linewidth]{sample012.pdf}
	\caption{Visualization of different samples.}
	\label{fig:sample012}
	\end{center}
	\end{figure}
    
    Fig.\ref{fig:sample012} shows the edges connected to  joint in NTU-RGB+D dataset for different samples. The learned parameter (adjacent matrix) is extracted from the second subset of  layer in model (Fig.~\ref{fig:stream}). It shows the model learned different connections for different samples, which  confirms our point of view and is more intuitive.
    
    
    \subsubsection{Two-stream.}
    Another important improvement is the utility of second-order information introduced in Section~\ref{tsn}. Here we compare the performance of using each kind of input data alone, showed as Js-NLGCN and Bs-NLGCN in Tab.~\ref{tab:twostream}, and the performance when combine them as described in Section~\ref{tsn}, showed as 2s-NLGCN in Tab.~\ref{tab:twostream}.
It shows that combining the two kinds of data as input outperforms one stream based methods, which verifies the importance of the second-order information (bones information) for skeleton based action recognition.


    \begin{table}[htb]
		\label{tab:twostream}
		\begin{center}
		\begin{tabular}{lc}
			\hline
			Methods     & Accuracy ()    \\
			\hline
			Js-NLGCN & 93.7       \\
			Bs-NLGCN & 93.2       \\
            2s-NLGCN & 95.1        \\
			\hline
		\end{tabular}
		\end{center}
        \caption{Recognition accuracy with different input modality.}
	\end{table}
   
    
    \subsection{Comparison with the state-of-the-art}
    
    We compare the final model with the state-of-the-art skeleton-based action recognition methods in both NTU-RGB+D dataset and Kinetics dataset. 
    \subsubsection{NTU-RGB+D dataset}
    In NTU-RGB+D dataset, our model is compared with one hand-craft feature based methods, i.e. Lie Group \cite{vemulapalli_human_2014}, three LSTM based methods, i.e. HBRNN \cite{du_hierarchical_2015}, STA-LSTM \cite{song_end--end_2017} and VA-LSTM \cite{zhang_view_2017}, three CNN based methods, i.e. TCN \cite{kim_interpretable_2017}, Synthesized CNN \cite{liu_enhanced_2017}, Motion+Trans+CNN \cite{li_skeleton-based_2017}, 3scale ResNet152 \cite{li_skeleton_2017} and one graph convolution based method, i.e. ST-GCN \cite{yan_spatial_2018}. These methods are briefly introduced in Section \ref{sec:relatedwork}. Table~\ref{ntu-rgbd} shows that the performance of deep learning based methods is generally better than hand-craft feature based methods, and CNN based methods are generally better than RNN based methods. Our model outperforms these methods even without using data-augmentation skills, which verifies the superiority of our model for skeleton-based action recognition.
    
      \begin{table}[htb]
      \begin{center}
      \label{ntu-rgbd}
		\begin{tabular}{lcc}
			\hline
			Methods     &X-Sub (\%)& X-View (\%)    \\
            \hline
            Lie Group \cite{vemulapalli_human_2014} & 50.1 & 82.8 \\
			\hline
			HBRNN \cite{du_hierarchical_2015} & 59.1  &    64.0  \\
			STA-LSTM  \cite{song_end--end_2017}  & 73.4 &    81.2  \\
            VA-LSTM  \cite{zhang_view_2017}  & 79.2 &    87.7  \\
            \hline
			Temporal Conv.  \cite{kim_interpretable_2017}   & 74.3&    83.1      \\
            Synthesized CNN \cite{liu_enhanced_2017}   & 80.0 &      87.2  \\
            Motion+Trans+CNN & 83.2  &      89.3  \\
            3scale ResNet152 \cite{li_skeleton_2017}  & 85.0  &      92.3  \\
            \hline
            ST-GCN \cite{yan_spatial_2018} & 81.5  &      88.3  \\
            \hline
            2s-NLGCN (ours)&  88.5 & 95.1 \\
			\hline
		\end{tabular}
      \end{center}
      \caption{Compare with the state-of-the-art methods in NTU-RGB+D.}
	\end{table}
    
    \subsubsection{Kinetics dataset}
    In Kinetics, our model is compared with one hand-crafted features based method, i.e. "Feature Encoding" \cite{fernando_modeling_2015}, one LSTM base methods, i.e. Deep LSTM \cite{shahroudy_ntu_2016}, one CNN based methods, i.e. TCN \cite{kim_interpretable_2017} and one graph convolution based method, i.e. ST-GCN \cite{yan_spatial_2018}. These methods are briefly introduced in Section \ref{sec:relatedwork}. The top-1 and top-5 recognition accuracies are reported in Table~\ref{kinetics}, where our model outperforms the other methods with a large margin.
    
    \begin{table}[htb]
    \begin{center}
    \label{kinetics}
		\begin{tabular}{lcc}
			\hline
			Methods     & Top-1 (\%)& Top-5 (\%)    \\
			\hline
			Feature Enc. \cite{fernando_modeling_2015} & 14.9  &    25.8  \\
			Deep LSTM  \cite{shahroudy_ntu_2016}   & 16.4 &    35.3  \\
			TCN  \cite{kim_interpretable_2017}   & 20.3&    40.0      \\
            ST-GCN  \cite{yan_spatial_2018}  & 30.7 &      52.8  \\
            \hline
            Js-NLGCN (ours)& 35.1& 57.1\\
            Bs-NLGCN (ours)& 33.3& 55.7\\
            2s-NLGCN (ours)&  36.1& 58.7\\
			\hline
		\end{tabular}
    \end{center}
	\caption{Compare with the state-of-the-art methods in Kinetics.}
	\end{table}
   
	\subsection{Conclusion}
    In this work, we propose a non-local graph convolution block for skeleton based action recognition, which can overcome the weakness of manual design of graph in ST-GCN.
Furthermore, we found that not only the joint but also the bone information is important for action recognition. So we represent the bone information with bone vector and propose a two-stream network to separately model the two input. The final model is evaluated on two large-scale action recognition datasets, NTU-RGB+D and Kinetics, and achieves the state-of-the-art performance.


{\small
\bibliographystyle{ieee}
\bibliography{Zotero}
}

\end{document}
