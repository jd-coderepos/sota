\documentclass[10pt,twocolumn,twoside]{IEEEtran}

\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsthm}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\usepackage{empheq} 
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{float}
\usepackage{cite}
\usepackage{enumitem}

\usepackage{gensymb}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{algorithm,algcompatible,amsmath}






\makeatother




\title{RAN-GNNs: breaking the capacity limits of graph neural networks}

\author{Diego Valsesia,~\IEEEmembership{Member,~IEEE},
        Giulia Fracastoro,~\IEEEmembership{Member,~IEEE},
        Enrico Magli,~\IEEEmembership{Fellow,~IEEE} \thanks{Diego Valsesia and Giulia Fracastoro contributed equally to this work. The authors are with Politecnico di Torino - Department of Electronics and Telecommunications,  Italy.  Email:{name.surname}@polito.it.}  }



\begin{document}
\maketitle

\begin{abstract}
Graph neural networks have become a staple in problems addressing learning and analysis of data defined over graphs. However, several results suggest an inherent difficulty in extracting better performance by increasing the number of layers. Recent works attribute this to a phenomenon peculiar to the extraction of node features in graph-based tasks, i.e., the need to consider multiple neighborhood sizes at the same time and adaptively tune them. In this paper, we investigate the recently proposed randomly wired architectures in the context of graph neural networks. Instead of building deeper networks by stacking many layers, we prove that employing a randomly-wired architecture can be a more effective way to increase the capacity of the network and obtain richer representations. We show that such architectures behave like an ensemble of paths, which are able to merge contributions from receptive fields of varied size. Moreover, these receptive fields can also be modulated to be wider or narrower through the trainable weights over the paths. We also provide extensive experimental evidence of the superior performance of randomly wired architectures over multiple tasks and four graph convolution definitions, using recent benchmarking frameworks that addresses the reliability of previous testing methodologies.
\end{abstract}

\section{Introduction} \label{sec:intro}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/graph_full.png}
    \caption{Random architectures aggregate ensembles of paths. This creates a variety of receptive fields (effective neighborhood sizes on the domain graph) that are combined to compute the output. Figure shows the domain graph where nodes are colored (red means high weight, blue low weight) according to the receptive field weighted by the path distribution of a domain node. The receptive field is shown at all the architecture nodes directly contributing to the output. Histograms represent the distribution of path lengths from source to architecture node.}
    \label{fig:full}
\end{figure*}

Data defined over the nodes of graphs are ubiquitous. Social network profiles \cite{hamilton2017inductive}, molecular interactions \cite{duvenaud2015convolutional}, citation networks \cite{sen2008collective}, 3D point clouds \cite{simonovsky2017dynamic} are just examples of a wide variety of data types where describing the domain as a graph allows to encode constraints and patterns among the data points. Exploiting the graph structure is crucial in order to extract powerful representations of the data. However, this is not a trivial task and only recently graph neural networks (GNNs) have started showing promising approaches to the problem. GNNs  \cite{wu2020comprehensive} extend the deep learning toolbox to deal with the irregularity of the graph domain. Much of the work has been focused on defining a graph convolution operation \cite{bronstein2017geometric}, i.e., a layer that is well-defined over the graph domain but also retains some of the key properties of convolution such as weight reuse and locality.
A wide variety of such graph convolution operators has been defined over the years, mostly based on neighborhood aggregation schemes where the features of a node are transformed by processing the features of its neighbors. Such schemes have been shown to be as powerful as the Weisfeiler-Lehman graph isomorphism test \cite{weisfeiler,xu2018powerful}, enabling them to simultaneuosly learn data features and graph topology.

However, contrary to classic literature on CNNs, few works \cite{li2019deepgcnsjournal,dehmamy2019understanding,xu2018representation,dwivedi2020benchmarking} addressed GNNs architectures and their role in extracting powerful representations. Several works, starting with the early GCN \cite{kipf2016semi}, noticed an inability to build deep GNNs, often resulting in worse performance than that of methods that disregard the graph domain, when trying to build anything but very shallow networks. This calls for exploring whether advances on CNN architectures can be translated to the GNN space, while understanding the potentially different needs of graph representation learning.

Li et al. \cite{li2019deepgcns} suggest that GCNs suffer from oversmoothing as several layers are stacked, resulting in the extraction of mostly low-frequency features. This is related to the lack of self-loop information in this specific graph convolution. It is suggested that ResNet-like architectures mitigate the problem as the skip connections supply high frequency contributions. Xu et al. \cite{xu2018representation} point out that the size of the receptive field of a node, i.e., which nodes contribute to the features of the node under consideration, plays a crucial role, but it can vary widely depending on the graph and too large receptive fields may actually harm performance. They conclude that for graph-based problems it would be optimal to learn how to adaptively merge contributions from receptive fields of multiple size. For this reason they propose an architecture where each layer has a skip connection to the output so that contributions at multiple depths (hence sizes of receptive fields) can be merged.  Nonetheless, the problem of finding methods for effectively increasing the capacity of graph neural networks is still standing, since stacking many layers has been proven to provide limited improvements \cite{li2019deepgcns, oono2019graph, alon2020bottleneck, nt2019revisiting}.

In this paper, we argue that the recently proposed randomly wired architectures \cite{xie2019exploring} are ideal for GNNs. In a randomly wired architecture, ``layers'' are arranged according to a random directed acyclic graph and data are propagated through the paths towards the output. Such architecture is ideal for GNNs because it realizes the intuition of \cite{xu2018representation} of being able of merging receptive fields of varied size. Indeed, the randomly wired GNNs (RAN-GNNs) can be seen as an extreme generalization of their jumping network approach where layer outputs can not only jump to the network output but to other layers as well, continuously merging receptive fields. Hence, randomly wired architectures provide a way of effectively scaling up GNNs, mitigating the depth problem and creating richer representations. Fig. \ref{fig:full} shows a graphical representation of this concept by highlighting the six layers directly contributing to the output, having different receptive fields induced by the distribution of paths from the input.

Our novel contributions can be summarized as follows: i) we are the first to analyze randomly wired architectures and show that they are generalizations of ResNets when looked at as ensembles of paths \cite{veit2016residual}; ii) we show that path ensembling allows to merge receptive fields of varied size and that it can do so \textit{adaptively}, i.e., trainable weights on the architecture edges can tune the desired size of the receptive fields to be merged to achieve an optimal configuration for the problem; iii) we introduce improvements to the basic design of randomly wired architectures by optionally embedding a path that sequentially goes through all layers in order to promote larger receptive fields when needed, and by presenting MonteCarlo DropPath, which decorrelates path contributions by randomly dropping architecture edges; iv) we provide extensive experimental evidence, using recently introduced benchmarking frameworks \cite{dwivedi2020benchmarking, hu2020ogb} to ensure significance and reproducibility, that randomly wired architectures consistently outperform ResNets, often by large margins, for four of the most popular graph convolution definitions on multiple tasks. 





\section{Background}

\subsection{Graph Neural Networks}


A major shortcoming of CNNs is that they are unable to process data defined on irregular domains. In particular, one case that is drawing attention is when the data structure can be described by a graph and the data are defined as vectors on the graph nodes. This setting can be found in many applications, including 3D point clouds \cite{wang2019dynamic,valsesia2018learning}, computational biology \cite{alipanahi2015predicting, duvenaud2015convolutional}, and social networks \cite{kipf2016semi}. However, extending CNNs from data with a regular structure, such as images and video, to graph-structured data is not straightforward if one wants to preserve useful properties such as locality and weight reuse.

GNNs redefine the convolution operation so that the new layer definition can be used on domains described by graphs. The most widely adopted graph convolutions in the literature rely on message passing, where a weighted aggregation of the feature vectors in a neighborhood is computed. The GCN \cite{kipf2016semi} is arguably the simplest definition, applying the same linear transformation to all the node features, followed by neighborhood aggregation and non-linear activation:

Variants of this definition have been developed, e.g., GraphSage \cite{hamilton2017inductive} concatenates the feature vector of node  to the feature vectors of its neighbors, so that self-information can also be exploited; GIN \cite{xu2018powerful} uses a multilayer perceptron instead of a linear transform, replaces average with sum to ensure injectivity and proposes a different way of computing the output by using all the feature vectors produced by the intermediate layers. These definitions are all isotropic because they treat every edge in the same way. It has been observed that better representation capacity can be achieved using anistropic definitions, where every edge can have a different transformation, at the cost of increased computational complexity. The Gated GCN \cite{bresson2017residual} and GAT \cite{velivckovic2017graph} definitions fall in this category.


\subsection{Randomly wired architectures}

In recent work, Xie et al. \cite{xie2019exploring} explore whether it is possible to avoid handcrafted design of neural network architectures and, at the same time, avoid expensive neural architecture search methods \cite{elsken2019neural}, by designing random architecture generators. They show that ``layers'' performing convolution, normalization and non-linear activation can be connected in a random architecture graph. Strong performance is observed on the traditional image classification task by outperforming state-of-the-art architectures. The authors conjecture that random architectures generalize ResNets and similar constructions, but the underlying principles of their excellent performance are unclear, as well as whether the performance translates to tasks other than image recognition or to operations other than convolution on grids.


\section{Randomly wired GNNs}



In this section, we first introduce randomly wired graph neural networks (RAN-GNNs) and the notation we are going to use. We then analyze their behavior when viewed as ensembles of paths.

\begin{figure}
  \centering
  \includegraphics[width=0.25\textwidth]{figures/arch_node.pdf}
    \caption{An architecture node is equivalent to a GNN layer.}
  \label{fig:architecture_node}
\end{figure}

A randomly wired architecture consists of a directed acyclic graph (DAG) connecting a source architecture node, which is fed with the input data, to a sink architecture node. One should not confuse the architecture DAG with the graph representing the GNN domain: to avoid any source of confusion we will use the terms \textit{architecture nodes} (edges) and \textit{domain nodes} (edges), respectively. A domain node is a node of the graph that is fed as input to the GNN. An architecture node is effectively a GNN layer performing the following operations (Fig. \ref{fig:architecture_node}): i) aggregation of the inputs from other architecture nodes via a weighted sum as in \cite{xie2019exploring}:

being  a sigmoid function,  the set of direct predecessors of the architecture node , and  a scalar trainable weight; ii) a non-linear activation; iii) a graph-convolution operation (without output activation); iv) batch normalization.


The architecture DAG is generated using a random graph generator. In this paper, we will focus on the Erd\H{o}s-Renyi model where the adjacency matrix of the DAG is a strictly upper triangular matrix with entries being realizations of a Bernoulli random variable with probability . If multiple input architecture nodes are randomly generated, they are all wired to a single global input. Multiple output architecture nodes are averaged to obtain a global output. Other random generators may be used, e.g., small-world and scale-free random networks have been studied in \cite{xie2019exploring}. However, a different generator will display a different behavior concerning the properties we study in Sec. \ref{sec:gradient_analysis}. 

\subsection{Randomly wired architectures behave like path ensembles}
\label{sec:gradient_analysis}


It has already been shown that ResNets behave like ensembles of relatively shallow networks, where one can see the ResNet architecture as a collection of paths of varied lengths \cite{veit2016residual}. More specifically, in a ResNet with  layers, where all layers have a skip connection except the first one and the last one, there are exactly  paths, whose lengths follow a Binomial distribution (i.e., the number of paths of length  from layer  to the last layer is ), and the average path length is  \cite{veit2016residual}. In this section, we show that a randomly wired neural network can also be considered as an ensemble of networks with varied depth. However, in this case, the distribution of the path length is different from the one obtained with the ResNet, as shown in the following lemma.
\begin{lemma}
Let us consider a randomly wired network with  architecture nodes, where the architecture DAG is generated according to an Erd\H{o}s-Renyi graph generator with probability . The average number of paths of length  from node  to the sink, where , is  and the average total number of paths from node  to the sink is . 
\label{lemma1}
\end{lemma}
\begin{proof}
Let us first consider the number of paths of length  from node  to the sink. We define the path length as the number of nodes in the path.  In a randomly wired network with  architecture nodes, we have that the first node of all the paths is node  and the last one is node  (i.e., the sink node). Therefore, the minimum path length is 2. If , the number of all possible paths of length  between node  and the sink is . Since in a path of length  there are  edges and each edge has probability  of being generated by the Erd\H{o}s-Renyi model, each one of the paths of length  has probability  of being present in the network. Thus, the expected number of paths with length  between node  and the sink is . If we set , we obtain the average number of paths of length  from source to sink . We can now compute the average total number of paths  as follows

where ,  and the fourth equality follows from the binomial theorem. If we set , we obtain the average total number of paths from source to sink .
\end{proof}
We can observe that if , the randomly wired network converges to the ResNet architecture. This allows to think of randomly wired architectures as generalizations of ResNets as they enable increased flexibility in the number and distribution of paths, instead of enforcing the use of all  paths.


\subsection{Receptive field analysis} \label{sec:receptive}

In the case of GNNs, we define the receptive field of a domain node as the neighborhood that affects the output features of that node. As discussed in Sec. \ref{sec:intro}, the work in \cite{xu2018representation} highlights that one of the possible causes of the depth problem in GNNs is that the size of the receptive field is not adaptive and may rapidly become excessively large. Inspired by this observation, in this section we analyze the receptive field of a randomly wired graph neural network. We show that the receptive field of the output is a combination of the receptive fields of shallower networks, induced by each of the paths. This allows to effectively merge the contributions from receptive fields of varied size. Moreover, we show that the trainable parameters along the path edges modulate the contributions of various path lengths and enable adaptive receptive fields, that can be tuned by the training procedure.

We first introduce a definition of the receptive field of a feedforward graph neural network\footnote{We use the term ``feedforward neural network'' to indicate an architecture made of a simple line graph, without skip connections: this is a representation of one path.}.
\begin{definition}
Given a feedforward graph neural network with  layers, the receptive field of radius  of a domain node is its -hop neighborhood. 
\end{definition} 
In a randomly wired architecture, each path induces a corresponding receptive field whose radius depends on the length of the path. Then, the receptive field at the output of the network is obtained by combining the receptive fields of all the paths. In order to analyze the contribution of paths of different lengths to the receptive field of the network, we introduce the concept of distribution of the receptive field radius of the paths. 
Notice that if we consider a feedforward network with  layers, the distribution of the receptive field radius is a delta centered in .

 
The following lemma allows to analyze the distribution of the receptive field radius in a randomly wired architecture.
\begin{lemma}
\label{lemma:derivative}
The derivative  of the output  of a randomly wired architecture with respect to the input  is

where  is the output of path ,  is the output of path  when we consider all the aggregation weights equal to 1, ,   is the set of all paths from source to sink,  is the number of architecture nodes,  is the set of paths from source to sink of length  and  is the set of edges of the path .
\end{lemma}
\begin{proof}
Direct computation.
\end{proof}


From Lemma \ref{lemma:derivative}, we can observe that the contribution of each path to the gradient is weighted by its corresponding architecture edge weights.
Thus, we can define the following distribution   of the receptive field radius:

where we have assumed that the gradient  depends only on the path length, as done in \cite{veit2016residual}. This is a reasonable assumption if all the architecture nodes perform the same operation. The distribution of the receptive field radius is therefore influenced by the architecture edge weights. Figure \ref{fig:path1} shows an example of how such weights can modify the radius distribution. If we consider  for all  and , we obtain that the radius distribution is equal to the path length distribution. In order to provide some insight into the role of parameter  in the distribution of the receptive field radius, we focus on this special case and analyze the distribution of the path lengths in a randomly wired architecture by introducing the following Lemma.

\begin{lemma}
Let us consider a randomly wired network with  architecture nodes, where the architecture DAG is generated according to a Erd\H{o}s-Renyi graph generator with probability . The average length of the paths from node  to the sink is .
\label{lemma:avg_length}
\end{lemma}
\begin{proof}
From Lemma 3.1, we can compute the average length of the paths from node  to the sink as follows

where we have neglected the higher order terms \cite{elandt1980survival}. The numerator in \eqref{eq:l_m} can be computed as follows

where ,  and the fourth equality is obtained differentiating the binomial theorem with respect to  .
Then, we obtain 

If we consider , i.e. the sink, we obtain .
\end{proof}

Therefore, if  and  for all  and  the radius distribution is a Binomial distribution centered in  (as in ResNets), instead when  the mean of the distribution is lower.  The path length distribution for different  values is shown in Fig. \ref{fig:path2}. This shows that, differently from feedforward networks, the receptive field of ResNets and randomly wired architectures is a combination of receptive fields of varied sizes, where most of the contribution is given by shallow paths, i.e. smaller receptive fields. The parameter  of the randomly wired neural network influences the distribution of the receptive field radius: a lower  value skews the distribution towards shallower paths, instead a higher  value skews the distribution towards longer paths. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/paths1.pdf}
    \caption{Distribution of receptive field radius (,  for unweighted,  for weighted).}
    \label{fig:path1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/paths2.pdf}
    \caption{Path distribution as function of architecture edge probability.}
    \label{fig:path2}
\end{figure}

After having considered the special case where  for all  and , we now focus on the general case. Since the edge architecture weights are trainable parameters, they can be adapted to optimize the distribution of the receptive field radius. This is one of the strongest advantages provided by randomly wired architectures with respect to ResNets. This is particularly relevant in the context of GNNs, where we may have a non-uniform growth of the receptive field caused by the irregularity of the graph structure \cite{xu2018representation}. Notice that the randomly wired architecture can be seen as a generalization of the jumping knowledge networks proposed in \cite{xu2018representation}, where all the architecture nodes, not only the last one, merge contributions from previous nodes.
We also remark that, even if we modify the ResNet architecture by adding trainable weights to each branch of the residual module, we cannot retrieve the behaviour of the randomly wired architecture. In fact, the latter has intrinsically more granularity than a ResNet: the expected number of architecture edge weights of a randomly wired network is , instead a weighted ResNet has only  weights. Ideally, we would like to weigh each path independently (i.e., directly optimizing the value of  in Eq. \eqref{eq:path_sum}). However, this is unfeasible because the number of parameters would become excessively high and the randomly wired architecture provides an effective tradeoff. Given an architecture node, weighting in a different way each input edge is important because to each edge corresponds a different length distribution of the paths going through such edge, as shown by the following Lemma. 

\begin{lemma}
Let us consider a randomly wired network with  architecture nodes, where the architecture DAG is generated according to a Erd\H{o}s-Renyi graph generator with probability . Given an edge  between the architecture nodes  and  where , the average length of the paths from the source to the sink going through that edge is .
\end{lemma}
\begin{proof}
From Lemma 3.3, we can compute the average length of the paths going through the edge  as follows

\end{proof}




\subsection{Sequential path}
In the previous sections we have shown that a randomly wired architecture behaves like an ensemble of paths merging contribution from receptive fields of varied size, where most of the contribution is provided by shallow paths. As discussed previously, this provides numerous advantages with respect to feedforward networks and ResNets. However, some graph-based tasks may actually benefit from a larger receptive field \cite{li2019deepgcns}, so it is interesting to provide randomly wired architectures with mechanisms to directly promote longer paths. Differently from ResNets, in a randomly wired neural network with  architecture nodes the longest path may be shorter than , leading to a smaller receptive field. In order to overcome this issue, we propose to modify the generation process of the random architecture by imposing that it should also include the sequential path, i.e., the path traversing all architecture nodes. This design of the architecture skews the initial path length distribution towards longer paths, which has the effect of promoting their usage. Nevertheless, the trainable architecture edge weights will ultimately define the importance of such contribution. Fig. \ref{fig:path1} shows an example of how including the sequential path changes the distribution of the receptive field radius.


\subsection{MonteCarlo DropPath regularization} \label{sec:droppath}

The randomly wired architecture offers new degrees of freedom to introduce regularization techniques. In particular, one could delete a few architecture edges during training with probability  as a way to avoid co-adaptation of architecture nodes. This is reminiscent of DropOut \cite{srivastava2014dropout} and DropConnect \cite{wan2013regularization}, although it is carried out at a higher level of abstraction, i.e., connections between ``layers'' instead of neurons. It is also reminiscent of techniques used in Neural Architecture Search \cite{zoph2018learning} and the approach used in ImageNet experiments in \cite{xie2019exploring}, although implementation details are unclear for the latter. 

We propose to use a MonteCarlo approach where paths are also dropped in testing. Inference is performed multiple times for different realizations of dropped architecture edges and results are averaged. This allows to sample from the full predictive distribution induced by DropPath, as in MonteCarlo DropOut \cite{gal2015dropout}.
The following lemma shows that MonteCarlo DropPath decorrelates the contributions of paths in Eq. \eqref{eq:path_sum} even if they share architecture edges, thus allowing finer control over the modulation of the receptive field radius.
\begin{lemma}
Let us consider two distinct paths  and  of a randomly wired network where the edges of the paths can be deleted with probability . Then, even if the two paths share some architecture edges, their contributions to the derivative , as
defined in Lemma \ref{lemma:derivative}, are decorrelated. \end{lemma}
\begin{proof}
Let us consider two paths  and  with at least one common edge, we can compute the covariance between these two paths as follows:


where  , , , , , and we have assumed all  deterministic.
\end{proof}







\section{Experimental results}

Experimental evaluation of GNNs is a topic that has recently received great attention. The emerging consensus is that benchmarking methods routinely used in past literature are inadequate and lack reproducibility. In particular, \cite{vignac2020choice} showed that commonly used citation network datasets like CORA, CITESEER, PUBMED are too simple and skew results towards simpler architectures or even promote ignoring the underlying graph. TU datasets are also recognized to be too small \cite{errica2019fair} and the high variability across splits does not allow for sound comparisons across methods. In order to evaluate the gains offered by randomly wired architectures across, we adopt recently proposed benchmarking frameworks such as the one in \cite{dwivedi2020benchmarking} and Open Graph Benchmarks \cite{hu2020ogb}. 


First, we use two datasets in \cite{dwivedi2020benchmarking} to analyse the performance differences between the baseline ResNet architecture, i.e., a feedforward architecture with skip connections after every layer, and the randomly wired architecture. We omit results on architectures without skip connections as these have already been shown to have poorer performance \cite{dwivedi2020benchmarking}.
We focus on the ZINC and CIFAR10 datasets. ZINC is one of the most popular real-world molecular datasets, and considers the task of property regression (constrained solubility) for the molecules represented as graphs. CIFAR10 is a well-known dataset for image classification, and, in this context, images are described by graphs of superpixels. 
For this experiment, we test four of the most commonly used graph convolution definitions: GCN \cite{kipf2016semi}, GIN \cite{xu2018powerful}\footnote{GIN and RAN-GIN compute the output as in \cite{xu2018representation}, using all architecture nodes.}, Gated GCN \cite{bresson2017residual}, and GraphSage \cite{hamilton2017inductive}. Notice that we do not attempt to optimize a specific method, nor we are interested in comparing one graph convolution to another. A fair comparison is ensured by running both methods with the same number of trainable parameters and with the same hyperparameters, keeping exactly the same ones used in \cite{dwivedi2020benchmarking}. The learning rate of both methods is adaptively decayed between  and  and the stopping criterion is validation loss not improving for 5 epochs after reaching the minimum learning rate. Results are averaged over 4 runs with different weight initialization and different random architecture graphs, drawn with . The random architectures use sequential paths (Sec. \ref{sec:receptive}), but no DropPath (Sec. \ref{sec:droppath}) for the ZINC experiment, and DropPath but no sequential paths for CIFAR10.

The results in Tables \ref{table:ZINC} and \ref{table:CIFAR} show the performance achieved by randomly wired architectures and their ResNets counterparts for increasing model capacity (number of architecture nodes or layers ). We can notice that randomly wired GNNs have compelling performance in many regards. The superscript reports the standard deviation among runs and the subscript reports the level of significance by measuring how many baseline standard deviations the average value of the random architecture deviates from the average value of the baseline. Results are in bold if they are at least  significant. First of all, randomly wired GNNs typically provide lower error or higher accuracy than their ResNet counterparts for the same number of parameters. Moreover, they are more effective at increasing capacity than stacking layers: while they are essentially equivalent to ResNets for very short networks (e.g., for ), they enable larger gains when additional layers are introduced. This is highlighted by Table \ref{table:gain}, which shows the relative improvement in mean absolute error or accuracy averaged over all the graph convolution definitions, with respect to the short 4-layer network, where random wiring and ResNets are almost equivalent. This table highlights that deeper ResNets always provide smaller gains with respect to their shallow counterpart than the randomly wired GNNs. This allows us to conclude that randomly wired GNNs are a more effective way of increasing model capacity.


\begin{table}
\caption{ZINC Mean Absolute Error.} \label{table:ZINC}
\setlength\tabcolsep{1.5pt} 
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{ccccc}
     &  &  &  &  \\ \hline
GCN &  &  &  &  \\
\textbf{RAN-GCN} &  &  &  & \\ \hline
GIN &  &  &  & \\
\textbf{RAN-GIN} &  &  &  &  \\ \hline
GatedGCN &  &  &  & \\
\textbf{RAN-GatedGCN} &  &  &  &  \\ \hline
GraphSage &  &  &  & \\
\textbf{RAN-GraphSage} &  &  &  &  \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\caption{CIFAR10 Accuracy.} \label{table:CIFAR}
\setlength\tabcolsep{2pt} 
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{ccccc}
     &  &  &  &  \\ \hline
GCN &  &  &  &  \\
\textbf{RAN-GCN} &  &  &  &  \\ \hline
GIN &  &   &  & \\
\textbf{RAN-GIN}&  &  &  &  \\ \hline
GatedGCN &  &  &  & \\
\textbf{RAN-GatedGCN} &  &  &  &  \\ \hline
GraphSage &  &  &  & \\
\textbf{RAN-GraphSage} &  &  &  &  \\ \hline
\end{tabular}
\end{table}

\begin{table}
\caption{Median Relative Gain over .} \label{table:gain}
\setlength\tabcolsep{3pt} 
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{ccccc}
                        &        &                 &                 &                 \\ \hline
\multirow{2}{*}{ZINC}   & ResNet &             &             &             \\ \cline{2-5}
                        & \textbf{Random} &   &    &    \\ \hline
\multirow{2}{*}{CIFAR10}& ResNet &             &             &             \\ \cline{2-5}
                        & \textbf{Random} &   &    &    \\ \hline         
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{\textrm{ogbg-molpcba} Average Precision.} 
\label{table:molpcba}
\setlength\tabcolsep{2pt} 
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cccc}
                         & Test AP               & Val AP                & No. params  \\ \hline
GCN                      &  &  &    \\ \hline
GIN                      &  &  &  \\ \hline
ChebNet                  &  &  &  \\ \hline
SIGN                     &  &  &  \\ \hline
\textbf{RAN-GIN}         &  &  &  \\ \hline
GCN+VN+FLAG              &  &  &  \\ \hline
GIN+VN+FLAG              &  &  &  \\ \hline
DeeperGCN+VN+FLAG        &  &  &  \\ \hline
\textbf{RAN-GIN+VN+FLAG} &  &  &  \\ \hline
GINE++VN                 &  &  &  \\ \hline
\end{tabular}
\end{table}


Moreover, we compare the proposed method against other state-of-the-art techniques to build graph neural networks, including methods that address the oversmoothing problem to build deeper GNNs (DeeperGCN) \cite{li2020deepergcn} or argue that going wide instead of deep is more effective to increase the capacity (SIGN) \cite{rossi2020sign}. This experiment is done on the ogbg-molpcba dataset from Open Graph Benchmarks \cite{hu2020ogb} and results are taken from the public leaderboard. We use a randomly wired version of GIN and compare results with two different setups: a vanilla RAN-GIN with the same number of parameters as GIN, and a larger RAN-GIN using the virtual node trick \cite{gilmer2017neural} and FLAG augmentations \cite{kong2020flag}. Both versions additionally use DropPath with . The results are reported in Table \ref{table:molpcba}. We can see that RAN-GIN (12 layers) significantly outperforms GIN and a number of other techniques for a comparable number of parameters. Furthermore, RAN-GIN with virtual node and FLAG augmentations reaches state-of-the-art performance on this benchmark, outperforming DeeperGCN and being very close to the recently proposed GINE \cite{brossard2020graph}\footnote{Notice that we did not test RAN-GINE.}.


\section{Ablation experiments}

In this section, we explore how some of the design choices for randomly wired GNNs can affect model performance.

\subsection{Architecture Edge probability}

We first investigate the impact of the probability  of drawing an edge in the random architecture. Table \ref{table:p_ablation} shows the results for a basic random architecture without DropPath nor embedded sequential path. It appears that an optimal value of  exists that maximizes performance. This could be explained by a tradeoff between size of receptive field and the ability to modulate it.

\begin{table}[t]
    \centering
    \caption{Edge probability, , RAN-GCN.}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{ccccc}
         &  &  &  &  \\ \hline
        ZINC &  &  &  &  \\
        CIFAR10 &  &  &  &  \\ \hline
    \end{tabular}
    \label{table:p_ablation}
    \vspace{-1pt}
\end{table}

\begin{table}[t]
    \centering
    \caption{DropPath on CIFAR10, RAN-GatedGCN. No sequential path embedding.} \label{table:droppath_ablation}
\setlength\tabcolsep{2pt} 
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{ccccc}
         &  &  &  \\ \hline
        None &  &  & \\
        DropPath & &  &  \\ \hline
    \end{tabular}
\end{table}
    
\begin{table}[]
    \centering
    \caption{DropPath on CIFAR10, RAN-GatedGCN. No sequential path embedding.} \label{table:pdrop_ablation}
\setlength\tabcolsep{2pt} 
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ccccc}
          \multicolumn{5}{c}{}                                                                             \\
          0                  & 0.005              & 0.01                        & 0.02               & 0.03               \\ \hline
  &  &  &  &  \\ \hline
\end{tabular}

\end{table}


\begin{table}[t!]
\centering
\caption{Sequential path embedding on ZINC, RAN-GatedGCN. No DropPath.} \label{table:linear_ablation}
\setlength\tabcolsep{2pt} 
\renewcommand{\arraystretch}{1.5}
\vspace{-11pt}
\centering
\small
\begin{tabular}{ccccc}
         &  &  &  \\ \hline
        Fully random &  &  &  \\
        Random+Sequential &  &  &  \\ \hline
    \end{tabular}
\end{table}


\subsection{DropPath}

The impact of DropPath on CIFAR10 is shown in Table \ref{table:droppath_ablation}. We found the improvement due to DropPath to be increasingly significant for a higher number of architecture nodes, as expected due to the increased number of edges. The value of the drop probability  was not extensively cross-validated. However, Table \ref{table:pdrop_ablation} shows that higher drop rates typically lowered performance.


\subsection{Embedded sequential path}

The impact of embedding a sequential path as explained in Sec. \ref{sec:receptive} is shown in Table \ref{table:linear_ablation}. It can be observed that its effect of promoting receptive fields with larger radius is useful on this task for any number of architecture nodes. We remark that, while we do not report results for the sake of brevity, this is not always the case and some tasks (e.g., CIFAR10) do not benefit from promoting larger receptive fields.






\section{Conclusion}

We showed how randomly wired architectures can boost the performance of GNNs by merging receptive fields of multiple size. Consistent and statistically significant improvements over a wide range of tasks and graph convolutions highlight how such constructions are more effective at increasing model capacity than building deep GNN by stacking several layers in a linear fashion, even when residual connections are used.


\bibliographystyle{IEEEtran}
\bibliography{biblio}

\end{document}