\section{Experiments}
\label{sec:experiment_setup}

\subsection{Dataset}

We conduct experiments on three SLU datasets: Fluent Speech Commands (FSC) \cite{lugosch2019speech}, SNIPS \cite{coucke2018snips}, and Smartlights \cite{saade2018spoken}.
Table~\ref{tab:statistics} summarizes their statistics in terms of the number of speakers and utterances.

\begin{table}[t]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lcccccccccccc}
    \toprule
    & \multicolumn{3}{c}{FSC (336)} & \multicolumn{3}{c}{SNIPS (7)} & \multicolumn{3}{c}{Smartlights (6)} \\
    & Train & Valid & Test & Train & Valid & Test & Train & Valid & Test \\
    \midrule
    \# Speakers   & 77 & 10 & 10 & 1 & 1 & 1 & 48 & 2 & 2 \\
    \# Utterances & 23,132 & 3,118 & 3,793 & 13,084 & 700 & 700 & 1,162 & 166 & 332 \\
    \bottomrule
    \end{tabular}
    \caption{
    Dataset Statistics of FSC, SNIPS, and Smartlights.
    They have 336, 7, and 6 intent classes, respectively.
    }
    \label{tab:statistics}
    \vskip -0.15in
\end{table} 
We use FSC for comparison with other works because it is one of the most widely used datasets to evaluate SLU systems.
The class of FSC dataset is a triple of (action, object, location), so the number of total classes is 336 ().
Test accuracy on FSC almost reaches 100\%, implying that there is little room to improve and evaluate the newly proposed method's effectiveness.
Following \cite{lugosch2019speech,cho2020speech}, we simulate a data shortage scenario using only 10\% of the speech-text pairs in training.
We randomly divide FSC dataset into ten parts and report the average accuracy on them.

In addition to FSC, we experiment on SNIPS and Smartlights to prove that our approach is generally applicable to other settings.
SNIPS is an NLU benchmark, so they only provide text utterances and their corresponding labels.
We generate speech data by Google's commercial speech synthesis toolkit\footnote{https://cloud.google.com/text-to-speech} similar to \cite{huang2020learning} to use SNIPS for SLU evaluation.
We use a single speaker option by setting as a basic voice type named \textit{en-US-Standard-B}.
Because other works \cite{huang2020learning,cao2020style} use their own speech synthesis methods and does not mention exact details to reproduce, a fair comparison between them and ours is impossible.

Smartlights\footnote{https://github.com/sonos/spoken-language-understanding-research-datasets} \cite{saade2018spoken} has two subsets, \textit{Close} field and \textit{Far} field.
The Close set is gathered by recording utterances using crowd-sourcing.
The Far set is collected by playing these recorded utterances with a neutral speaker and recording them at a distance of 2 meters.
Those two sets include the same set of utterances, but the Far set is much noisy.
We split each set into a training, validation, and test set as the ratio of the number of utterances to be 70:10:20 following \cite{bhosale2019end}.


\subsection{Model Configurations}

We borrow a pretrained vq-wav2vec BERT supported in the fairseq \cite{ott2019fairseq} library\footnote{https://github.com/pytorch/fairseq}.
Its vector quantization is based on the k-means clustering, and its BERT is a base size of 12-layers trained similar to \cite{liu2019roberta}.
Each token after the vector quantization represents 10ms of audio data.

DeepSpeech2 AM is a stack of two 2D-CNN layers and five bidirectional LSTM layers.
The two CNN layers have a channel size of 32, kernel sizes of  and , respectively, and strides of  and , respectively.
The hidden size of all LSTM layers is 768.

We use pretrained RoBERTa-base \cite{liu2019roberta} from the fairseq for the text BERT and a softmax layer for intent classification.


\subsection{Training Details}

We further pre-train vq-wav2vec BERT from the pre-trained weights on 960h of Librispeech \cite{panayotov2015librispeech}.
The learning rate is linearly decayed from  to 0 over 250k steps.
A slightly small learning rate is used for PT-KD because we initialize it from the pre-trained weights.
We use the batch size of 256 with 8 V100 GPUs and gradient accumulation.
To not forget general speech representations of vq-wav2vec BERT, we early stop when the MLM loss starts increasing, so training ends after the update of 10k steps. 

We fine-tune for 200 epochs with the learning rate annealed from  by the factor of  at each epoch.
We use  depending on the training set size.
For each dataset, we choose the best checkpoint in terms of validation accuracy.
We tune data augmentation hyperparameter values depending on the dataset in following ranges:
masking span length  and maximum masking ratios , .  


\subsection{Results and Discussion}

We do an ablation study to evaluate the effectiveness of components in our proposed method. 
Methods in the order of PT-KD, FT-KD, AM-PT, and DA starting from the baseline using our model architecture without any additional training techniques are incrementally added one-by-one.

\begin{table}[t]
    \centering
    \begin{tabular}{lrrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}*{Method}} & \multicolumn{2}{c}{Full} & \multicolumn{2}{c}{10} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & Valid & Test & Valid & Test  \\
    \midrule
    Lugosh et al. \cite{lugosch2019speech}          &    - & 96.6 & - & 88.9 \\
    \:\:+AM-PT \cite{lugosch2019speech}    &    - & 98.8 & - & 97.9 \\
    \:\:\:\:+FT-KD \cite{cho2020speech}         &    - & 99.0 & - & 98.1 \\
    Price \cite{price2020end}                       & 92.5 & 99.1 & - & - \\
    \:\:+DA                          & 94.4 & 99.4 & - & -  \\
    \:\:+AM-PT                            & 94.8 & 99.3 & - & - \\
    \:\:\:\:+DA                          & 96.6 & 99.5 & - & - \\
    \midrule
    \midrule
    VQ-BERT +DS2                                            & 93.1 & 98.9 & 87.3 & 97.0 \\
    \:\:+PT-KD                             & 94.1 & 99.0 & 90.7 & 98.5 \\
    \:\:\:\:+FT-KD                          & 96.2 & 99.6 & 93.3 & 99.2 \\
    \:\:\:\:\:\:+AM-PT                     & 96.4 & 99.6 & 94.3 & 99.3 \\ 
    \:\:\:\:\:\:\:\:+DA              & \textbf{97.8} & \textbf{99.7} & \textbf{96.2} & \textbf{99.5} \\
    \bottomrule
    \end{tabular}
    \caption{
    Results on FSC dataset. Because test accuracy is almost close to 100, we also report validation accuracy to compare between methods.
    }
    \label{tab:fsc}
    \vskip -0.15in
\end{table} 
Table~\ref{tab:fsc} display the results on FSC dataset.
Previous state-of-the-art \cite{price2020end} achieves almost perfect accuracy of 99.5\%.
Our best model surpasses that result by achieving 99.7\% test accuracy and comparable result with only 10\% of training pairs. 
Overall the gap between the full dataset setting and 10\% subset setting is reduced.
It indicates that our method is especially effective in a low-resource setting.
All of the training methods are beneficial to improve accuracy.

\begin{table}[t]
    \centering
\begin{tabular}{lrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}*{Method}} & \multirow{2}*{SNIPS} & \multicolumn{2}{c}{Smartlights} \\
    \cmidrule(lr){3-4} 
    & & \multicolumn{1}{c}{Close} & \multicolumn{1}{c}{Far}  \\
    \midrule
VQ-BERT +DS2            & 86.4 & 75.9 & 47.9 \\
    \:\:+PT-KD              & 88.3 & 81.3 & 51.2\\
    \:\:\:\:+FT-KD          & 95.3 & 84.6 & 59.6 \\
    \:\:\:\:\:\:+AM-PT      & \textbf{96.7} & 92.2 & 70.5 \\ 
    \:\:\:\:\:\:\:\:+DA     & 95.7 & \textbf{95.5} & \textbf{75.0} \\
    \bottomrule
    \end{tabular}
    \caption{
    Results on SNIPS and Smartlights dataset.
    We report test accuracy.
    }
    \label{tab:snips}
    \vskip -0.15in
\end{table} 
Table~\ref{tab:snips} shows the results on the SNIPS and Smartlights dataset.
We achieve fine accuracy on these datasets, although comparison with other works is difficult.
PT-KD and FT-KD boost accuracy significantly.
Data augmentation makes SLU model robust to a noisy environment and generalizes to unseen speakers.
However, data augmentation is not helpful in the SNIPS dataset.
We presume that this is because its utterances in the training set and the test set are clean and synthesized with a single speaker.
