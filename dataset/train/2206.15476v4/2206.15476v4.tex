\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[final]{neurips_data_2022}










\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[symbol]{footmisc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}


\newcommand{\eg}{\textit{e}.\textit{g}. }
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection}







\author[,,1]{Marius Dragoi\thanks{Equal contribution. Bitdefender Theoretical Research Team: \url{https://bit-ml.github.io/}}}
\author[,,1,2]{Elena Burceanu\samethanks}
\author[,,1,3]{Emanuela Haller\samethanks}
\author[1]{Andrei Manolache}
\author[1]{Florin Brad}

\affil[1]{Bitdefender, Romania}
\affil[2]{University of Bucharest}
\affil[3]{Politehnica University of Bucharest}





\begin{document}

\maketitle

\begin{abstract}
Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006\texttt{+}, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span ( years), with naturally occurring changes over time (\eg users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose \textbf{AnoShift}, a protocol splitting the data in IID, NEAR, and FAR testing splits. We validate the performance degradation over time with diverse models, ranging from classical approaches to deep learning. Finally, we show that by acknowledging the distribution shift problem and properly addressing it, the performance can be improved compared to the classical training which assumes independent and identically distributed data (on average, by up to  for our approach). Dataset and code are available at \url{https://github.com/bit-ml/AnoShift/}.

\end{abstract}



\section{Introduction}

Analyzing and developing Machine Learning algorithms under gradual distribution shifts is a problem of high interest in the research community. 
There is a growing enthusiasm for building benchmarks over existing or new datasets~\cite{clear, mind_the_gap, shift_auto_drive, intel_cl_benck, wilds}, that formulate a setup for isolating the shifting aspect and create a better ground for this research field. A better understanding of the distribution shift problem might lead to findings of underlying fundamental aspects, shedding new light on robustness and generalization problems. We argue that the distribution shift occurs naturally and gradually in a continuous data stream (\eg monitoring network traffic), allowing an in-depth analysis of the problem. On the other side, artificially generated scenarios usually exhibit sudden changes that do not simulate the natural shift problem. Yet, the annotation process for streaming data is quite difficult and expensive, considering the massive amount of data. 





From a practical point of view, continuous IT infrastructure monitoring has become essential for computer security and resilience. Recent anomaly detection and intrusion detection systems (IDS) obtain strong results on specific datasets but drastically fail in real-world scenarios \cite{nids_generalization}. Our experimental analysis proved a natural change of the Kyoto-2006+ data over the  years period when the data was collected. The shift is noticeable both over the input distribution and considering the performance of several anomaly detection systems. Several reasons behind the observed shift are: users leaving or coming to the network, per user interest changes leading to network interaction changes, updates to the software versions, patching old vulnerabilities but revealing new attack vectors for intruders.







\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.9\textwidth]{final_images/feat_shift_normals_abnormals.pdf}
    \end{center}
    \caption{\textbf{a)} The proposed \textbf{AnoShift} splits over Kyoto-2006\texttt{+} dataset. The \texttt{IID} (gray) testing split comes from the same temporal span as the \texttt{TRAIN} set (white), while \texttt{NEAR} (yellow) and \texttt{FAR} (blue) splits are from different time spans, with \texttt{NEAR} being closer to the training set than \texttt{FAR}. \textbf{b)} To highlight the utility of the proposed chronological protocol, we exemplify the continuous evolution of data, illustrating the distributions of normal and anomaly samples over the considered 10 years. We exemplify the evolution of the percent of recent connections that have the same source and destination IP addresses as the current connection (\href{http://www.takakura.com/Kyoto_data/BenchmarkData-Description-New.pdf}{feature 9 - Dst host srv count}).}
\label{fig:anoshift_splits_feat_distrib}
\end{figure}


To better assess the models' capabilities, we introduce a chronology based evaluation protocol, distinctly evaluating performance on test data splits (\texttt{IID}, \texttt{NEAR} and \texttt{FAR} - Fig.~\ref{fig:anoshift_splits_feat_distrib}) with different temporal distances towards the training set (\texttt{TRAIN} -Fig.~\ref{fig:anoshift_splits_feat_distrib}). We observe that the performance of anomaly detection models consistently degrades when tested on data from longer time horizons. Moreover, we prove that a basic distillation technique overcomes a classic IID (assuming independent and identically distributed data) training under gradual data shifts, proving that the awareness of the shift problem might lead to better solving the task.



Summarized, our \textbf{main contributions} are the following:
\begin{itemize}
\item We analyzed a large and commonly used dataset for the unsupervised anomaly detection task in network traffic (Kyoto-2006\text{+}) and demonstrated that it is affected by distribution shifts. The per-feature distributions and t-SNE show multiple changes over the years, and the Optimal Transport Dataset Distance gave us an estimate of its magnitude.
\item We propose a chronology-based benchmark, which focuses on splitting the test data based on its temporal distance to the training set, introducing three testing splits: \texttt{IID}, \texttt{NEAR}, \texttt{FAR} (Fig.~\ref{fig:anoshift_splits_feat_distrib}). This testing scenario proves to capture the in-time performance degradation of anomaly detection methods for classical to masked language models. This benchmark aims to enable a better estimate of the model's performance, closer to the real world performance. 
\item We prove that properly acknowledging the distribution shift may lead to better performing anomaly detection models than classical IID training. When facing distribution shift, a basic distillation technique positively impacts the performance by up to  on average.
\end{itemize}



\section{Related work}



\paragraph{Relation to benchmarks targeting distribution shift} Recently, there has been an increased amount of effort and focus in this direction, with several benchmarks emerging. They emphasize the non-stationary nature of the data, with various underlying reasoning. The most common approach is to search for gaps in the input data distribution that appear with time~\cite{clear, mind_the_gap}, taking into perspective that the world is continuously evolving; therefore, the data acquired continuously from it should exhibit the same behavior. Our work aligns with this perspective by working with traffic logs from a large university network over  years. In~\cite{clear}, the authors focus on how the appearance of basic objects changes from year to year, while~\cite{mind_the_gap} emphasizes the seasonal patterns that appear in news language (\eg elections, hurricanes). A second axis exploited for noticing shifts in data is the spatial one. In~\cite{intel_cl_benck}, geolocalization is used in conjunction with the time for guiding the shift. In~\cite{wilds}, the gap is based on higher level characteristics, like x-ray data from different hospitals, but also on geolocalization. In searching for the autonomous driving robustness, a more complex variation is provided in ~\cite{shift_auto_drive} following the weather, time of day, and congestion levels. Nevertheless, all works analyze the distribution shift for supervised tasks, focusing on NLP or Computer Vision. In~\cite{mind_the_gap}, the authors monitor the evolution of the perplexity metric, with models learned in a self-supervised manner as a masked language model. They emphasize the need to link the shift analysis to a downstream task, several supervised ones in their case. Differently, \textbf{AnoShift}, our benchmark proposal, tackles an unsupervised anomaly detection task under non-stationary data.

\paragraph{Relation to traffic anomalies} Models tackling Network Intrusion Detection are covered by lots of surveys~\cite{nids-survey, nids_survey_3, nids_survey_4}, structured around dataset variations, anomaly types, and methods variation. A fair amount of the approaches are supervised~\cite{compare_nids}, based on tree classifiers~\cite{tree}, modeling the task as a binary or multi-class anomaly (intrusion) classification. But we are interested here in the unsupervised setup~\cite{ensemble_autoenc_nids}. Usually, the best models are quite simple, most of them are shallow~\cite{shallow_deep_nids}, based on OC-SVM~\cite{ocsvm} or Isolation Forest~\cite{isoforest}, or very small neural nets~\cite{ensemble_autoenc_nids}. Several solutions introduce deep learning approaches for intrusion detection~\cite{ DL_anom_survey}, transforming the data into images~\cite{nids_images_svm}, or modeling the problem using GNNs~\cite{gnn_nids}. 

An important problem we identified in this area is that the datasets used for the task are easily saturated, mainly because they either lack variety (\eg simulated traffic patterns for anomalies) or have a very few annotated anomalies, or are small-scale, covering only several days~\cite{kddcup, NSL-KDD, cic_ids_2017, netflow_v1_iot, netflow_v2_iot, unsw_nb15, lanl, zyell_nctu, compare_nids}. In contrast, Kyoto-2006\texttt{+}~\cite{kyoto2006} spans over  years (2006-2016), containing continuous natural traffic logs from a large university network, within a sub-net of honeypots. Most of those datasets cover basic networks, but there are some oriented towards IOT traffic~\cite{netflow_v2_iot}, or even to the autonomous driving field, Internet of Vehicles~\cite{tree}. But another reason for saturation, is the IID training setup, as we will show in this work. These generalization problems are very acute, leading to weak performances for those algorithms when applied on real world data, or on a new dataset~\cite{nids_generalization}. With \textbf{AnoShift}, we highlight the IID training problem, by proposing a different training and evaluation setup based on temporal distances, closer to a realistic case.

\section{Chronological protocol}\label{sec:chronological_protocol}



We introduce a chronological protocol for building train and testing splits that can highlight the temporal evolution of data. Taking into consideration the timestamps of our data, we propose to build a training split (\texttt{TRAIN}) along with three different testing splits (\texttt{IID}, \texttt{NEAR} and \texttt{FAR}), comprising multiple years of data (Fig.~\ref{fig:anoshift_splits_feat_distrib}a)). The \texttt{TRAIN} and \texttt{IID} splits are extracted from the first period of time, and the \texttt{IID} tests should highlight the expected performance when there is no distribution shift between train and test. The \texttt{NEAR} and \texttt{FAR} splits are each extracted from different periods of time, where \texttt{NEAR} is closer to the training data and \texttt{FAR} is farther away.  We expect standard models to exhibit better performance on \texttt{NEAR} compared to \texttt{FAR}, which we experimentally prove in Sec.~\ref{sec:distrib-shift_models}. 
Our proposed benchmark will provide a better estimate of the expected performance when the model is deployed in the wild and exposed to the inevitable distribution shift of the data. To the best of our knowledge, \textbf{AnoShift} is the first to provide a proper scenario for studying the generalization capabilities of unsupervised learning models for anomaly detection. 

Our work revolves around Network Intrusion Detection Systems (NIDS), tackling the problem of distribution shifts that naturally appear in network traffic data. We work over the popular Kyoto-2006\texttt{+} dataset (Sec.~\ref{sec:kyoto}), which was collected over ten years, providing us with enough data to capture the temporal evolution. Starting from Kyoto-2006\texttt{+}, we introduce our \textbf{AnoShift} Benchmark (Sec.~\ref{sec:anoshift}) that proposes one training and three testing splits, which highlight the difficulty of dealing with data temporarily distant from the training set. 

\subsection{Kyoto-2006+}\label{sec:kyoto}

Kyoto-2006\texttt{+}~\cite{kyoto2006} is a reference dataset for Anomaly Detection over network traffic data~\cite{ring2019survey}. It is built on  years of real traffic data (Nov. 2006 - Dec. 2015), captured by a system of  honeypots in  sub-networks inside the Kyoto University. Briefly, a honeypot is a real or virtual machine simulating a regular computer (having an OS and multiple services running on it). Its purpose is to deceive an attacker into taking advantage of the vulnerabilities present on the honeypot machine (\eg software not updated). A honeypot does not request any connection on its own. So in such a scenario, almost all traffic coming to a honeypot machine is unsolicited and therefore considered malicious. By design, this type of dataset has a large percent of anomalies (89.5\% anomalies in Kyoto-2006\texttt{+}) compared to other anomaly detection datasets. The 14 conventional features of the dataset include 2 categorical ones like connection service type or flag of the connection and 12 numerical like the connection duration or the number of source bytes. We put more details about Kyoto-2006\texttt{+} in Appendix~\ref{appendix}. This dataset is spread across a very large period of time, and it contains exclusively real-world traffic, without simulated events.

\subsection{AnoShift benchmark}\label{sec:anoshift}
    
    To keep the natural distribution shift of the network traffic data, we sample a fixed number of normal samples per year (\#months25k for \texttt{TRAIN}, \texttt{NEAR}, and \texttt{FAR} and \#months2.5k for \texttt{IID}). The number of anomalous samples is chosen such that we maintain the proportion of normal vs. anomaly samples from the original yearly subset. We illustrate this process in Fig.~\ref{fig:anoshift_n_samples}. In Fig.~\ref{fig:anoshift_splits_feat_distrib} b) we illustrate the continuous evolution of the data features over the considered 10 years, comparing the distribution for one feature (\href{http://www.takakura.com/Kyoto_data/BenchmarkData-Description-New.pdf}{feature 9 - Dst host srv count}). Such behavior can be observed for the majority of features, a fact highlighted by our in-depth analysis from Sec.~\ref{sec:distrib-shift_metrics}. The \texttt{TRAIN} and \texttt{IID} samples are collected from , while the \texttt{NEAR} and \texttt{FAR} splits consist of  and  intervals. The protocol is illustrated in Fig.~\ref{fig:anoshift_splits_feat_distrib} and Fig.~\ref{fig:anoshift_n_samples}.
    




    


    
\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.99\textwidth]{final_images/counts_kyoto.pdf}
    \end{center}
    \caption{\textbf{a)} Yearly splits of the network traffic data from Kyoto-2006+ dataset, highlighting the proportion of normal and anomaly samples. \textbf{b)} Proposed train and test splits in our \textbf{AnoShift} benchmark. Considering that \texttt{TRAIN} and \texttt{IID} splits are sampled from the same time span, we have jointly represented them. Note that while for \texttt{TRAIN} , \texttt{NEAR} and \texttt{FAR} we extract the same number of normal samples per year, the \texttt{IID} split contains 10 times less normal samples. The anomaly samples are extracted such that we maintain the normal vs. anomaly proportion of the original data.}
    \label{fig:anoshift_n_samples}
\end{figure}
    
    \subsubsection{Experimental setup}
    \label{sec:exp_setup}
    
    \paragraph{Preprocess network traffic data} We use the 14 conventional features from the new version of the Kyoto dataset [2006-2015] and convert 3 of the 12 numerical features to categorical values by using an exponentially-scaled binning method between 0 and the maximum value of each feature, such that the bins have a higher density for smaller values and get increasingly wider towards larger values. We used a basis of 1.1, which results in 233 bins, where the width of the th bin is given by: . We keep the original percentage features (9 out of 12 numerical features), which are discretized in 100 values. Therefore, our preprocessing results in a fixed vocabulary size and each possible token is known apriori. See in Fig.~\ref{fig:kyoto_instance} a preprocessed sample. Our processing of the original dataset does not pose any privacy concerns since it does not contain any sensitive information, such as IP address. However, data binning constitutes another potential limitation in our work.
    
\begin{figure}[t]
    \begin{center}
        \includegraphics[width=1.0\textwidth]{final_images/kyoto_sample.png}
    \end{center}
    \caption{Examples of preprocessed Kyoto-2006\texttt{+} instances. See Appendix~\ref{appendix} for details.}
    \label{fig:kyoto_instance}
\end{figure}

    \paragraph{Metrics for anomaly detection} To analyze the performance of various models on our proposed benchmark, we use the labels (normal and anomaly) provided by the Kyoto-2006\texttt{+} dataset. As we deal with imbalanced sets, we study the ROC-AUC metric and also evaluate the PR-AUC metric, for both inliers and outliers (note that for a random classifier, PR-AUC for a specific class is close to the ratio of data in that specific class). We report the \texttt{IID}, \texttt{NEAR} and \texttt{FAR} performances as the arithmetic mean of performances over their associated yearly splits.
    




\section{Distribution shift analysis}\label{sec:distrib_shift_analysis}

    We perform an in-depth analysis of the proposed benchmark from three points of view. \textbf{First}, we study the inherent non-stationarity of the considered data, highlighting the natural shift between the years, considering both simple, per feature metrics and more complex metrics between distributions (Sec.~\ref{sec:distrib-shift_metrics}). \textbf{Second}, we analyze various anomaly detection models, highlighting the performance decrease when dealing with testing data that is temporarily distant from the training set (Sec.~\ref{sec:distrib-shift_models}). \textbf{Third}, we discuss the importance of acknowledging the data shift and emphasize the positive impact of a basic distillation technique over the standard IID approach (Sec.~\ref{sec:ack_shift}). We add supplementary discussions on the method in Appendix~\ref{appendix:discussions}.
    
    We run our experiments on an internal cluster with multiple GPU types: GTX 1080 Ti, GTX Titan X, RTX 2080 Ti, RTX Titan. We estimate that we need 5 days to reproduce the experiments on 1 GPU. The CPU training for OC-SVMs, IsolationForest, and LOF benchmarks takes ~3 days.
    
\subsection{Inherent non-stationarity}\label{sec:distrib-shift_metrics}
    

    \paragraph{Visualization of the data shifts} For a visual interpretation of the yearly shift, we have considered the unsupervised t-SNE~\cite{tsne} to illustrate the high dimensional data structure (PCA visualization available in Appendix ~\ref{appendix}). In Fig.~\ref{fig:tsne_all} we introduce the comparison between pairs of yearly splits and the whole figure can be interpreted as a similarity matrix, each cell  illustrating the similarity between point clouds of year  vs. year . Each row illustrates the point clouds of the corresponding year over all the other point clouds. At the same time, each column presents the point clouds of the corresponding year below all the other point clouds for a better understanding of the distribution shifts.  We observe that point clouds move away as we increase the temporal gap between their corresponding years. This confirms our intuition that the analyzed network traffic data is continuously shifting in time and emphasizes the need for a benchmark as \textbf{AnoShift} that can efficiently test the robustness of models under this inherent non-stationarity of natural data. 
    
\begin{figure}[t]
    \begin{center}
         \includegraphics[width=.9\textwidth]{final_images/tsne_all.pdf}
    \end{center}
    \caption{Comparison between yearly splits using t-SNE visualization. We observe that the discrepancy between point clouds increases with the temporal distance between splits, colors becoming more separated over time. The analysis is performed considering 2k randomly sampled points per split. Follow the 2007 row: see the orange cluster on top of clusters associated to the other years. It is very similar to its neighbours 2006-2008, and the similarity diminishes in time (see 2015).}
    \label{fig:tsne_all}
\end{figure}
    


    \paragraph{Per-feature shift} We further analyze whether the dataset's statistics at the feature level are changing from one year to another. Recall that we have 2 categorical features and 12 numerical ones. We extract the normalized histogram per year for each feature and compute the Jeffreys divergence~\cite{jeff_divergence} between those histograms. The Jeffreys divergence is a commonly used symmetrization for Kullback-Leibler divergence~\cite{kl_divergence}: , and it is proven to be both symmetric and non-negative. We highlight that such an analysis can only illustrate simple scenarios, studying the distribution change from the perspective of single feature changes. With all the considered baselines from Sec.\ref{sec:distrib-shift_models}, we have observed a significant decrease in performance for the years 2014 and 2015, leading to the intuition that this subset may have substantial differences from the others. Consequently, in Fig.~\ref{fig:jeff_kyoto}, we illustrate the Jeffreys divergence for two features that we find to have a large 2014-2015 distance, but also for a third one that has significant high values in the distance map on other years than the two.
    \begin{figure}[t]
        \begin{center}
            \includegraphics[width=1\textwidth]{final_images/jeff_feat_1_2_3.pdf}
        \end{center}
        \caption{Jeffreys divergence between Kyoto years. First two images represent features with a large 2014-2015 distance. The  one is for
        a feature with significant difference between the histograms across years. Note that it is difficult to predict the performance of the method on a new split, only based on those per feature distances between distributions.}
        \label{fig:jeff_kyoto}
    \end{figure}
    
    \paragraph{General shift} We next explore the distribution differences between dataset splits over time by using the Optimal Transport Dataset Distance method (OTDD)~\cite{otdd}. OTDD relies on optimal transport, a geometric method for computing distances between probability distributions for comparing datasets. This analysis shows how the splits move away from each other over time (see Fig.~\ref{fig:otdd_kyoto}). Compared with the per feature approach, this method allows us to gain a better intuition for the performance on a new split, giving us a single distance based on all features. We observe how the inliers (first image) nicely distances in OTDD value, directly correlated with the distance in time. As for the outliers (third image), it is noticeable that they are quite different between the splits of the first years. We notice that the distances between inliers and outliers (in the middle) show that \texttt{FAR} years' outliers are similar to \texttt{TRAIN} years' inliers, an observation that we empirically confirm in Tab.~\ref{tab:far_near}, where all models suffer from a steep descent in performance (bellow random). We run the method with the default parameters for DatasetDistance, over the standardized input of Kyoto, with one-hot encoded categorical variables,  times, with a randomly sampled 5k entries per year.

    \begin{figure}[t]
        \begin{center}
            \includegraphics[width=1\textwidth]{final_images/otdd_kyoto_5k.png}
        \end{center}
        \caption{Optimal Transport Dataset Distance for Kyoto. See distances between inliers (first), inliers and outliers (second), and outliers (third). The distances from inliers generally increase as you move further from the diagonal, showing large distances between \texttt{TRAIN} and \texttt{FAR} data. Moreover, notice in second image how outliers in the \texttt{FAR} splits are quite similar with inliers from \texttt{TRAIN}, also explaining the abrupt performance drop on farther data (Tab.~\ref{tab:far_near}).}
        \label{fig:otdd_kyoto}
\end{figure}



\subsection{Impact on IID models}\label{sec:distrib-shift_models}


    We introduce the \textbf{AnoShift} benchmark to understand better the impact of data shifts that naturally appear over time on the performance of anomaly detection models. We hope that the proposed splits will push forward the research in this direction and help build more robust models that can deal with mild to severe distribution changes between test and training sets. In this context, in the current section, we will study the performance degradation of various anomaly detection approaches, from \texttt{IID} to \texttt{NEAR} and \texttt{FAR} testing splits.
    
    \paragraph{Anomaly detection models} We have considered several unsupervised baselines, ranging from more classical approaches, like Isolation Forest~\cite{isoforest}, OC-SVM~\cite{ocsvm}, LocalOutlierFactor(LOF)~\cite{lof} and recent ECOD~\cite{ecod} and COPOD~\cite{copod}, to deep learning ones, like SO-GAAL~\cite{mo_gaal}, deepSVDD~\cite{deepsvdd}, AE~\cite{autoencoder} for anomalies, LUNAR~\cite{lunar}, InternalConstrastiveLearning~\cite{icl} and our proposed transformer for anomalies model, based on the BERT~\cite{bert} architecture. For part of the baselines, we have employed the PyOD library ~\cite{pyod}.

    \paragraph{BERT for anomalies} We use a simplified BERT architecture, without pretraining, with around 340k trainable parameters. We train the BERT model as a Masked Language Model (MLM), using a data collator that randomly masks a fraction  of the input sequence and optimizing a cross-entropy loss function between the model predictions at mask positions and the original tokens. We derive a sequence anomaly score by randomly masking a fraction  of tokens in the sequence and averaging the probabilities of the correct tokens at mask positions given by the classification layer over the vocabulary. At evaluation time, we average the score over  mask samplings. A detailed description of the model is introduced in Appendix~\ref{appendix}. In our experiments, we used .
    
    In Table~\ref{tab:far_near} we report the results of our experiments. Each baseline model was trained 3 times with a basic set of hyperparameters, and we reported the average results and the standard deviation. Both the OC-SVM and the LUNAR model were trained solely on  of the \texttt{TRAIN} set to reduce the computational burden. For all of the considered models, except ECOD, we observe a performance degradation between \texttt{NEAR} and \texttt{FAR} splits, highlighting that these anomaly detection models cannot cope with the distribution shift. In the case of ECOD, the performances of both \texttt{NEAR} and \texttt{FAR} splits are below random, making their relative order irrelevant. The \texttt{IID} evaluation, which is the most popular methodology, proves to give an illusion of high performance, as the performance quickly degrades once we consider a testing set from a different period. The evolution is also presented in Appendix~\ref{appendix}-Fig.~\ref{fig:perf_gap}, illustrating ROC-AUC along with PR-AUC for inliers and outliers. We observe a rapid degradation for inliers PR-AUC, indicating that normal data distribution is continuously changing, and the outliers detection may not be reliable. These experiments highlight the issues of current anomaly detection models and prove the benefits of the \textbf{AnoShift} benchmark. 
    
\begingroup
\setlength{\tabcolsep}{6pt} \begin{table}[t]
\begin{center}
    \caption{Performance evolution over time, for classical and deep methods: \texttt{IID} vs \texttt{NEAR} vs \texttt{FAR}. Notice that the ROC-AUC is dropping over time in all cases, except for BERT and SO-GAAL methods, showing this is a property of the method, rather than a problem with the dataset. More precisely, those methods model the outliers very well in the \texttt{NEAR} split (see PR-AUC-out), while the PR-AUC-in is dropping, confirming the distribution shift over time (see Appendix~\ref{appendix}). On the \texttt{FAR} split, almost all methods perform under-random. Best scores per split are shown in bold: the highest scores on \texttt{IID} and \texttt{NEAR} are achieved by deepSVDD, but BERT is still competitive on these two splits. The best performance on \texttt{FAR} is surprisingly achieved by COPOD. PR-AUC for inliers and outliers are available in Appendix~\ref{appendix}-Fig.~\ref{fig:perf_gap} and in Tab.~\ref{aptab:far_near_roc_pr}.}
    
    \begin{tabular}{c l c c c}\toprule
    & & \multicolumn{3}{c}{ROC-AUC } \\
    \cmidrule(lr){3-5}
    Type & Baselines & \texttt{IID} & \texttt{NEAR} & \texttt{FAR}  \\
    \midrule
    \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Classical}}} & 
    \textbf{OC-SVM}~\cite{ocsvm} (train 5\%)  & 76.86  \small 0.06 & 71.43  \small 0.29 & 49.57  \small 0.09\\    
    
    &\textbf{IsoForest}~\cite{isoforest} & 86.09  \small 0.54 & 75.26  \small 4.66 & 27.16  \small 1.69\\
    &\textbf{ECOD}~\cite{ecod} & 84.76 & 44.87 & 49.19 \\
    &\textbf{COPOD}~\cite{copod} & 85.62 & 54.24 & \textbf{50.42} \\
    &\textbf{LOF}~\cite{lof} & 91.50  \small 0.88 & 79.29  \small 3.33 & 34.96  \small 0.14 \\
    \cmidrule(lr){1-5}
    \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Deep}}} 
    & \textbf{SO-GAAL}~\cite{mo_gaal} & 50.48  \small 1.13 & 54.55  \small 3.92 & 49.35  \small 0.51\\
    &\textbf{deepSVDD}~\cite{deepsvdd} & \textbf{92.67}  \small 0.44  & \textbf{87.00}  \small 1.80  & 34.53  \small 1.62  \\
&\textbf{AE}~\cite{autoencoder} \textbf{for anomalies} & 81.00  \small 0.22 & 44.06  \small 0.57 & 19.96  \small 0.21 \\
    &\textbf{LUNAR}~\cite{lunar} (train 5\%) & 85.75  \small 1.95 & 49.03
     \small 2.57 & 28.19  \small 0.90 \\ 
    &\textbf{InternalContrastiveLearning}~\cite{icl} & 84.86  \small 2.14 & 52.26  \small 1.18 & 22.45  \small 0.52\\
    &\textbf{BERT~\cite{bert} for anomalies} & 84.54  \small 0.07 & 86.05  \small 0.25 & 28.15  \small 0.06 \\
    
\bottomrule
    \end{tabular}
       
    \label{tab:far_near}
\end{center}
\end{table}
\endgroup


    \paragraph{Performance on \texttt{FAR}} With all tested baselines, we notice a significant decrease in performance for 2014-2015 years for inliers, which motivates us to further investigate the particularities of this subset. We observe a large distance in the Jeffreys divergence between 2014-2015 and the rest of the years for 2 features: service type and the number of bytes sent by the source IP (see Fig.~\ref{fig:jeff_kyoto}). From the OTDD analysis in Fig.~\ref{fig:otdd_kyoto}, we observe that: first, the inliers from \texttt{FAR} are very distanced to training years; and second, the outliers from \texttt{FAR} are quite close to the training inliers. One root cause of those events can be the steep increase of the "DNS" traffic percentage (from 4\% to 37\%, in 2013, and 2014 respectively). This contributes to the distribution shift on \texttt{FAR}, explaining the low performance. 

    
    \paragraph{Monthly evaluation} In Fig.~\ref{fig:bert_per_month}, we take a closer look at the BERT's performance at month granularity and break down performance on inliers and outliers. First, notice how the inliers' performance gradually degrades over time, to an abrupt drop at farther months. This doubles the analysis from Sec.~\ref{sec:distrib-shift_metrics}, where we notice the difference between the \texttt{TRAIN} years and \texttt{FAR} (through Jeffreys and OTDD experiments). Second, we observe that on \texttt{IID} years, the anomalies are modeled quite poorly by our language model, resulting in a slightly lower \texttt{IID} performance in comparison with \texttt{NEAR}.
    
\begin{figure}[t]
    \begin{center}
        \includegraphics[width=1\textwidth]{final_images/bert_monthly_eval.pdf}
    \end{center}
    \caption{BERT for anomaly, evaluated on each month. We show the ROC-AUC, PR-AUC for inliers, and PR-AUC for outliers. The performance for the inliers is slowly decreasing  during \texttt{IID} and \texttt{NEAR} splits, dropping suddenly just before the \texttt{FAR} split, showing how the language model fails to recognize inliers once it moves further apart from the training data. On the other hand, there are parts of the \texttt{IID} split where the outliers are quite poorly modeled, explaining the slightly poor performance of BERT on \texttt{IID} when compared with \texttt{NEAR} split.}
    \label{fig:bert_per_month}
\end{figure}

\subsection{Addressing the shifted data}\label{sec:ack_shift}
     We next compare the performance of a BERT model in 3 training regimes: iid, finetune, and knowledge distillation, for subsets of 300k entries from each year. We use 2006-2010 as training data and evaluate 2011-2015 as individual splits. First, in the \textbf{a) iid mode}, we use sets of data starting from 2006 and gradually add each successive split from the train period, initializing a new model for each subset. Next, in the \textbf{b) finetune mode}, we start from the iid model trained on 2006 and gradually finetune it on each successive year in the train period. Finally, in the \textbf{c) distillation mode}, we start from the iid model of 2006 and reinitialize a same-sized model for each new split, which becomes a student for the previous model by combining the MLM loss with a KL divergence loss with the teacher predictions on the current split. The best performance is achieved by the final distilled model for every test split (see Fig.~\ref{fig:iid_finetune_distill}), outperforming iid and finetune by over  on average in ROC-AUC. It is worth noting that the effects of distillation are visible over time, with the iid method outperforming it in the first two iterations over the train splits. At all stages, the distillation method obtains the best performance on FAR data, providing a more robust training alternative to distribution shifts in data. The metrics are available in Appendix~\ref{appendix}-Tab.~\ref{tab:training_strategies} and pseudocode for the training modes is available in Appendix~\ref{appendix:pseudocode}.
    
    
    \begin{figure}[t]
        \begin{center}
            \includegraphics[width=1\textwidth]{final_images/iid_finetune_distill_all_years_differences.pdf}
        \end{center}
        \caption{ROC-AUC, PR-AUC-in, PR-AUC-out for Finetune and Distill strategies, relative to the iid. The performance is averaged over all training subsets. Even though the strategies have a high variance in general, the distill is clearly more robust over time when compared to iid and finetune.}
        \label{fig:iid_finetune_distill}
\end{figure}
    





\subsection{Discussions}
\paragraph{MLM as anomaly detector} Even though the BERT model greatly exceeds the number of parameters and the complexity of other classical baselines, its generalization performance on farther data is extremely low. The anomaly performance in our case is based on the perplexity score when predicting several masked features in the sample. So if the features are not correlated, the MLM model might be unable to learn something useful, which might result in learning some specific training set biases, failing to generalize on temporarily distant data (eg. lower score on FAR wrt other baselines). We did not investigate this, but we consider it an interesting direction for future work.

\paragraph{MLM with the training vocabulary} In a real world setup, we expect that the fraction of tokens that are previously unseen during training increases with temporal distance. The evaluation score might get artificially inflated due to mapping of unseen features to the UNK token, as for farther points it is easier to predict UNK instead of the right word. Alongside the requirement of a discrete vocabulary, this is another limitation of vocabulary based methods as opposed to other classical approaches. We did not investigate these effects, but it might constitute an interesting direction for future work.

\paragraph{Other considered datasets} To emphasize the Kyoto-2006\texttt{+} value, we briefly discuss here the other considered datasets and why we choose it in the end. We performed an in depth analysis over a large number of datasets, looking for two characteristics, essential for a distribution shift benchmark: it spreads over a large enough time-span, such that the distribution shift will naturally occur, rather than being synthetically injected, exhibiting sudden changes, and it is not solved already (existing methods do not report perfect scores on it). We first looked over a wide range of known \textbf{1. network traffic datasets} for intrusion detection, and after analysing them we concluded that most are artificially created, with injected samples, in very restricted scenarios. Only Kyoto-2016 was a proper one, extended over a long enough period of time for showing a natural distribution shift. We next focused our attention on \textbf{2. system logs}, since the time-span is usually more extensive in these dataset and the natural distribution shift is more probable to occur. But under our analysis (t-SNE, Jeffreys divergence, OTDD, multiple baselines), these datasets did not exhibit a clear distribution shift over time, so we decided to further analyse them until concludent results. Finally, we looked over \textbf{3. general multi-variate timeseries} datasets, but the most popular ones are quite small and almost perfectly solved already. We leave this exact numbers for the considered datasets in the Appendix~\ref{appendix:other_datasets}.


\section{Conclusion}
    Our approach highlights the true dimension of distribution shifts that appear in naturally and continuously evolving data streams. We analyze it in Kyoto-2006\texttt{+} network traffic dataset that spans over 10 years from multiple angles: visually with t-SNE, statistically with histogram distances, and by measuring its magnitude with an Optimal Transport approach. Next, we propose \textbf{AnoShift}, a chronology-based benchmark for anomaly detection, to enable the development of models that generalize better and are more robust to shifts in data. Further, we show that by acknowledging the shift and addressing it, the performance can be improved, obtaining a +3\% performance boost using a basic distillation technique.








\subsubsection*{Acknowledgments}
We thank Razvan Pascanu for guiding us on how to approach the subject and Ioana Pintilie for helping us with baselines for the rebuttal.



\bibliographystyle{plainnat}
\bibliography{neurips_benchmark}

\clearpage
\section*{Checklist}



\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? 
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{See Sec.~\ref{sec:kyoto} - paragraph 2.; Sec. \ref{sec:exp_setup} - paragraph 2.; Appendix~\ref{appendix:discussions}}
  \item Did you discuss any potential negative societal impacts of your work?
  
    \answerNo{Our work does not have a negative societal impact. Our benchmark proposal is tailored for finding intrusions in a computer network (not at the user level, but at the network level), by detecting anomalous traffic, in a more robust way than before, closer to the real scenario. One use-case is in the IT department of an company or university, where a person monitors the traffic alerts and prioritizes certain alerts based on the predictions of the robust models trained on our proposed benchmark.}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments (e.g. for benchmarks)...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{in the abstract and the appendix}.
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{see Sec.~\ref{sec:distrib_shift_analysis} and its subsections}.
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerYes{For each of the tested methods in the main experiment, we run it 3 times, with different seeds.} 
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{see Sec.~\ref{sec:distrib_shift_analysis} - paragraph 2} 

\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{We used the existing Kyoto-2006\texttt{+} dataset as raw data, as detailed in Sec.~\ref{sec:chronological_protocol}}
  \item Did you mention the license of the assets?
    \answerNA{The authors do not mention any kind of licence for the data, it is just publicly available.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{We included a GitHub repository with code resources and a repository with the preprocessed data in the suplimentary material - see Appendix.~\ref{sec:appendix_own}}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNo{We had several emails with the authors, describing them our purpose and asking for additional information.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerYes{see in Sec.~\ref{sec:exp_setup}}

\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}


\clearpage
\appendix

\section{Appendix}\label{appendix}



\paragraph{Kyoto-2006\texttt{+}} When an attack occurs, the honeypot saves the network access pattern and other metadata and, at some point, might decide to reboot the system and rewrite back the original configuration. The authors deployed another machine in the network to generate normal traffic data, with a mailing server and a DNS service for a single domain. All traffic data from this server was labeled as clean (the logs also include other protocols for managing the machine over ssh or http and https). The 14 conventional features of the dataset includes 2 categorical features: connection service type and flag of the connection and 12 numerical features: connection duration, number of source and destination bytes, number of connections with corresponding IP addresses in a timeframe of two seconds and the percentage of connections accessing the same service and their rate of "SYN" errors, the prevalence of the connection's source IP address and the requested service in the past 100 connections to the current destination IP (\href{http://www.takakura.com/Kyoto_data/BenchmarkData-Description-New.pdf}{Kyoto features}). The malicious traffic is further labeled using three software solutions: an Intrusion Detection Systems at the network level, an Antivirus product, and a shellcodes and exploits detector. In addition to those, the authors labeled other entries based on their prior history of connections from a specific IP and destination port. Additional features include source and destination IP addresses and ports, timestamp and protocol. We note that the generality of the original dataset imposes several limitations for our benchmark. More specific, the diversity of the normal traffic in a honeypot setup is quite restricted. Also, since the labeling is done using existing software and rules, the dataset's anomalies might be underestimated.

\paragraph{Visualization of the data shifts with PCA} For completeness, in Fig.~\ref{fig:pca_all} we illustrate the distribution shift between years using a PCA visualization of the point clouds associated with each year. We observe similar results as the t-SNE visualization presented in Fig.~\ref{fig:tsne_all}. 

\begin{figure}[t]
    \begin{center}
         \includegraphics[width=0.9\textwidth]{final_images/pca_all.pdf}
    \end{center}
    \caption{Comparison between yearly splits using PCA visualization. Similar to the t-SNE visualization, we observe that the discrepancy between point clouds increases with the temporal distance between splits, colors becoming more separated over time.}
    \label{fig:pca_all}
    \vspace{-1em}
\end{figure}



\paragraph{Performance evolution over time} In Tab.~\ref{aptab:far_near_roc_pr} and Fig.~\ref{fig:perf_gap} we present the full evaluation of considered baseline models on \texttt{IID}, \texttt{NEAR} and \texttt{FAR} splits. 

\paragraph{Training strategies for data shift} In Tab.~\ref{tab:training_strategies} we present the full ROC-AUC, and PR-AUC for inliers and outliers, for all three training strategies: iid, finetune and distill. 


\paragraph{BERT for Anomalies} We propose a simplified BERT architecture for detecting anomalies. The network input is tokenized by a WordLevel tokenizer which obtains tokens for the individual events in a system log sequence and, conversely, for the individual features of network traffic. Therefore, we have fixed-length sequences for Kyoto-2006. We train the BERT model as a Masked Language Model (MLM), using a data collator that randomly masks  of the input sequence, by optimizing a cross-entropy loss function between the model predictions at mask positions and the original tokens. We derive a sequence anomaly score by randomly masking  of tokens in the sequence and averaging the probabilities of the correct tokens at mask positions given by the classification layer over the vocabulary. The model is not pretrained and consists of two hidden layers of size , an intermediate size of  and  attention heads. It has a hidden dropout and attention dropout probabilities of , an epsilon of  for the normalization layer and a  standard deviation range for the truncated normal weight initialization. Our architecture totals  trainable parameters. For training we mask  of the input sequence and at evaluation time, we average over  mask samplings.

\begin{figure}
    \begin{center}
        \includegraphics[width=1\textwidth]{final_images/perf_gap.pdf}
    \end{center}
    \caption{Performance evolution over time: \texttt{IID} vs \texttt{NEAR} vs \texttt{FAR}.  We follow the evolution of ROC-AUC and PR-AUC for inliers and outliers. We observe a large performance gap between the considered splits, correlated with the temporal distance from the training set. (Best viewed in color)}
    \label{fig:perf_gap}
\end{figure}


\begingroup
\setlength{\tabcolsep}{4pt} \begin{table}[t]
\begin{center}
    \caption{Performance evolution over time for unsupervised methods: \texttt{IID} vs \texttt{NEAR} vs \texttt{FAR}. We report beside the ROC-AUC metric, also the PR-AUC for inliers and PR-AUC for outliers. With bold are the best results per split.}
    
    \begin{tabular}{c l c c c}\toprule
    Type & Unsupervised Baselines & \texttt{IID} & \texttt{NEAR} & \texttt{FAR}  \\
    \cmidrule(lr){3-5}
    & & \multicolumn{3}{c}{ROC-AUC (\%) } \\
    \midrule
    \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Classical}}} & \textbf{OC-SVM}~\cite{ocsvm} (train 5\%)  & 76.86  \small 0.06 & 71.43  \small 0.29 & 49.57  \small 0.09\\    
    &\textbf{IsoForest}~\cite{isoforest} & 86.09  \small 0.54 & 75.26  \small 4.66 & 27.16  \small 1.69\\
    &\textbf{ECOD}~\cite{ecod} & 84.76 & 44.87 & 49.19 \\
    &\textbf{COPOD}~\cite{copod} & 85.62 & 54.24 & \textbf{50.42} \\
    &\textbf{LOF}~\cite{lof} & 91.50  \small 0.88 & 79.29  \small 3.33 & 34.96  \small 0.14 \\
    \cmidrule(lr){2-5}
    \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Deep}}} 
    & \textbf{SO-GAAL}~\cite{mo_gaal} & 50.48  \small 1.13 & 54.55  \small 3.92 & 49.35  \small 0.51\\
    &\textbf{deepSVDD}~\cite{deepsvdd} & \textbf{92.67}  \small 0.44 & \textbf{87.00}  \small 1.80 & 34.53  \small 1.62 \\
&\textbf{AE}~\cite{autoencoder} \textbf{for anomalies} & 81.00  \small 0.22 & 44.06  \small 0.57 & 19.96  \small 0.21 \\
    &\textbf{LUNAR}~\cite{lunar} (train 5\%) & 85.75  \small 1.95 & 49.03
     \small 2.57 & 28.19  \small 0.90 \\ 
     &\textbf{InternalContrastiveLearning}~\cite{icl} & 84.86  \small 2.14 & 52.26  \small 1.18 & 22.45  \small 0.52\\
    &\textbf{BERT~\cite{bert} for anomalies} & 84.54  \small 0.07 & 86.05  \small 0.25 & 28.15  \small 0.06 \\
  

    \midrule
   
    && \multicolumn{3}{c}{PR-AUC inliers (\%) } \\
    \cmidrule(lr){3-5}
    \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Classical}}}
    & \textbf{OC-SVM}~\cite{ocsvm} (train 5\%) & 70.84  \small 0.13 & 41.38  \small 0.29 & \textbf{15.12}  \small 0.04\\ 
    & \textbf{IsoForest}~\cite{isoforest} & 83.68  \small 3.47 & 57.06  \small 10.27 & 9.16  \small 0.18 \\ 
    & \textbf{ECOD}~\cite{ecod} & 84.47 & 22.98 & 13.78\\ 
    & \textbf{COPOD}~\cite{copod} & \textbf{87.86}  & 29.25 & 14.55 \\ 
    & \textbf{LOF}~\cite{lof} & 84.11  \small 0.96 & 52.48  \small 4.56 & 10.15  \small 0.10 \\ 
    \cmidrule(lr){2-5}
    \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Deep}}} 
    & \textbf{SO-GAAL}~\cite{mo_gaal} & 58.65  \small 5.36 & 43.52  \small 11.62 & 10.68  \small 2.42\\ 
    & \textbf{deepSVDD}~\cite{deepsvdd} & 82.62  \small 0.52 & \textbf{71.71}  \small 4.85 & 10.02  \small 0.22 \\
& \textbf{AE}~\cite{autoencoder} \textbf{for anomalies} & 73.76  \small 0.09 & 26.16  \small 0.15 & 8.51  \small 0.01\\
    & \textbf{LUNAR}~\cite{lunar} (train 5\%) & 78.91  \small 1.69 & 29.36  \small 2.58 & 9.33  \small 0.11\\ 
    & \textbf{InternalContrastiveLearning}~\cite{icl} & 76.96  \small 2.12 & 27.28   \small 0.59 & 8.81  \small 0.05\\ 
    & \textbf{BERT~\cite{bert} for anomalies} & 74.61  \small 0.13 & 58.94  \small 0.69 & 8.22  \small 0.02 \\ 
    
    
    \midrule
    
    
    && \multicolumn{3}{c}{PR-AUC outliers (\%) } \\
    \cmidrule(lr){3-5}
    \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Classical}}} &
    \textbf{OC-SVM}~\cite{ocsvm} (train 5\%)& 67.94	  \small 0.21 & 85.70	  \small 0.16 & 87.27  \small 0.02\\ 
    & \textbf{IsoForest}~\cite{isoforest} & 81.46  \small 2.52 & 87.13  \small 2.08 & 78.33  \small 1.41 \\
    & \textbf{ECOD}~\cite{ecod} & 78.37 & 74.48 & 85.9\\ 
    & \textbf{COPOD}~\cite{copod}& 78.19 & 77.99 & 85.98\\ 
    & \textbf{LOF}~\cite{lof} & 83.86  \small 0.98 & 92.34  \small 1.26 & 81.99  \small 0.05 \\ 
    \cmidrule(lr){2-5}
    \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Deep}}} 
    & \textbf{SO-GAAL}~\cite{mo_gaal} & 70.38  \small 0.28 & 87.71  \small 0.74 & \textbf{92.67}  \small 0.13\\ 
    & \textbf{deepSVDD}~\cite{deepsvdd} & \textbf{92.65}  \small 0.64 & 94.15  \small 1.05 & 82.25  \small 0.48 \\
& \textbf{AE}~\cite{autoencoder} \textbf{for anomalies} & 78.99  \small 0.28 & 72.97  \small 0.38 & 75.71  \small 0.05\\
    & \textbf{LUNAR}~\cite{lunar} (train 5\%) & 88.01  \small 1.03 & 80.91  \small 0.62 & 79.45  \small 0.30 \\ 
    & \textbf{InternalContrastiveLearning}~\cite{icl} & 89.08  \small 0.87 & 81.93  \small 0.39 & 77.55  \small 0.50\\ 
    & \textbf{BERT~\cite{bert} for anomalies} & 89.83  \small 0.07 & \textbf{95.96}  \small 0.06 & 78.38   \small 0.02\\ 
    \bottomrule
    \end{tabular}
       
    \label{aptab:far_near_roc_pr}
\end{center}
\end{table}
\endgroup





We repeat the masking process n times and average over all repeats to improve consistency. The anomaly score formula is depicted in equation \ref{anom_score}, where we denote by  the classification layer of the model of parameters , by  the set of random binary masks of length k and mask probability p, where  are the initial tokens in the sequence and  is the j-th token in the sequence under mask i.






We motivate this metric with the observation that inlier data should consist of common tokens with a high retrieval probability given by the distribution of the training set, while outliers should usually have either rare tokens or unusual combinations of features, within the context given by the unmasked tokens. 


\begin{table}[t]
\caption{Training strategies: ROC-AUC (\%) for IID training vs Finetune vs Distil on Kyoto-2006\texttt{+}}
\setlength{\tabcolsep}{7pt} \begin{center}
\begin{tabular}{c clllc llllc }

\toprule
 & \shortstack{Train ON: \textbf{2006 ->}} & \multicolumn{1}{c}{\textbf{2007}} & \multicolumn{1}{c}{\textbf{2008}} & \multicolumn{1}{c}{\textbf{2009}} & \multicolumn{1}{c}{\textbf{2010}} \\ 
 \cmidrule{2-2}
\textbf{Strategy} & \shortstack{Test split} \\
\toprule

\multicolumn{1}{l}{\textbf{IID}} & 2011 & 88.95 & 88.93 & 88.27 & 89.92 \\
& 2012 & 95.85 & 90.96 & 86.28 & 86.63
\\ & 2013 & 94.05 &	87.00 & 80.79 & 81.87 \\ & 2014 & 28.56 & 24.35 & 22.64 & 21.16\\ & 2015 & 49.01 & 42.20 & 37.05 & 34.07\\
\midrule
\multicolumn{1}{l}{\textbf{Finetune}} & 2011 & 88.56 & 87.18 & 87.3 & 89.92\\
& 2012 & 95.78 & 89.31 & 85.18 & 89.14\\ & 2013 & 94.19 & 85.29 & 79.36 & 84.06\\ & 2014 & 32.98 & 22.50 & 20.50 & 20.57\\ & 2015 & 53.39 & 38.99 & 30.98 & 30.04\\
\midrule
\multicolumn{1}{l}{\textbf{Distil}}  & 2011 & 83.74 & 87.43 & 88.82 & 90.32\\
& 2012 & 95.10 & 91.96 & 88.78 & 91.51\\ & 2013 & 94.43 & 88.89 & 83.43 & 86.04\\ & 2014 & 43.65 & 26.69 & 23.55 & 23.13\\ & 2015 & 59.93 & 48.39 & 41.31 & 39.69\\

\bottomrule

\end{tabular}
\label{tab:training_strategies}
\end{center}
\end{table}

        
\paragraph{Impure training} Till now, we have considered anomaly detection models that were trained solely on the clean data of the \texttt{TRAIN} split. Further, we will study the performance evolution between \texttt{IID}, \text{NEAR} and \texttt{FAR} when the BERT model is trained on corrupt data containing mislabeled outliers in different percentages. In Fig.~\ref{fig:impure} we present the ROC-AUC for the 3 testing splits. We observe that the distribution shift is noticeable in the model performance even in this corrupt training setup. This validates the usefulness of the proposed chronological protocol when dealing with potentially mislabeled samples and highlights that the observed degradation over time is not a consequence of such dataset issues.



\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{final_images/impure.pdf}
    \end{center}
    \caption{Performance evolution of our BERT model on \texttt{IID}, \texttt{NEAR} and \texttt{FAR} splits, when training on a corrupt set of samples, containing different percentages of mislabeled data points. (Best viewed in color)}
    \label{fig:impure}
\end{figure}





\paragraph{Broader Impact} Our benchmark proposal is tailored for finding intrusions in a computer network (not at the user level, but at the network level), by detecting anomalous traffic, in a more robust way than before, closer to the real scenario. One use-case is in the IT department of an company or university, where a person monitors the traffic alerts and prioritizes certain alerts based on the predictions of the robust models trained on our proposed benchmark. Our work does not have a negative societal impact.



\subsection{Discussions and future work}
\label{appendix:discussions}











\paragraph{The inliers' natural distribution} For being able to annotate large amount of data, the network datasets stay either in a clean space, where almost everything is normal, or in a "dark" one, where every connection is considered infected. Kyoto-2006\texttt{+} lies in the second case, where the normal traffic is not very general, covering several behaviours. It might be interesting as future directions to find a way to combine the two cases towards a more general and unbiased dataset.

\paragraph{Pre-process through binning} We have performed the numerical to categorical conversion in order to make the dataset suitable for BERT based models, whose vocabularies would become too large otherwise. For a fair comparison, we consider it proper to use the same preprocessing for all the methods. We binarize 3 numerical features, transforming them into categorical ones (out of 12 total features). Namely, we convert only the float features: connection duration, number of source bytes and number of destination bytes into categorical ones (bins of values). We also perform experiments without preprocessing those numerical features (Tab.~\ref{aptab:numerical_preprocess}). Notice that the performance varies depending on the method and split, and it is not clear that one feature set is better than the other, over all the methods. Nevertheless, we see the same trend of performance drop across the three splits, supporting the claim of our work. Additionally, we observe that the evaluation on data without preprocessing numerical features instead of categorical ones achieves a better score on FAR. This might be explained by the fact that numerical binning induces a higher rarity of tokens in the FAR split compared to IID and NEAR, and therefore resulting in more uncertainty for the models.







\begingroup
\begin{table}[t]
\begin{center}
    \caption{Compare different numeric feature preprocessing. Notice that it is not clear that one feature set is better than the other, over all the methods. Nevertheless, we see the same trend of performance drop across the splits, supporting the claim of our work. }
    
    \begin{tabular}{l c ccc}
    \toprule
    Method & Feature binarization & \texttt{IID}      & \texttt{NEAR}    & \texttt{FAR}     \\
    \midrule
                 & & \multicolumn{3}{c}{ROC-AUC (\%) }  \\
    \cmidrule{3-5}
    OC-SVM~\cite{ocsvm}  & w  & 76.86 & 71.43 & 49.57 \\
             & w/o & 81.87 & 71.24	& 50.95 \\
    \cmidrule(lr){2-5}
    IsoForest~\cite{isoforest}     & w & 86.09 & 75.26 & 27.16 \\
             & w/o & 94.41 & 95.13 & 32.81  \\
    \cmidrule(lr){2-5}
    ECOD~\cite{ecod}  & w  & 84.76 & 44.87 & 49.19\\
             & w/o & 79.38 & 69.80 & 60.84 \\
    \cmidrule(lr){2-5}
    COPOD~\cite{copod}  & w  & 85.62 & 54.24 & 50.42\\
             & w/o & 79.03 & 65.67 & 60.12 \\
    \cmidrule(lr){2-5}
AE~\cite{autoencoder}  & w  & 81.00 & 44.06 & 19.96\\
             & w/o & 89.59 & 86.76 & 30.79 \\
    
    \bottomrule
    \end{tabular}
    \label{aptab:numerical_preprocess}
\end{center}
\end{table}
\endgroup









\paragraph{Gap between supervised and unsupervised learning} We evaluate several supervised learning methods for anomaly detection modeled as a binary classification task, on our AnoShift benchmark. We test several classical baselines: (SVM~\cite{cortes1995support}, RandomForest~\cite{liaw2002classification}, XGBoost~\cite{chen2015xgboost}), but also some attention-based deep learning methods (BERT with a classification head~\cite{devlin2018bert}, TabNet~\cite{arik2021tabnet}, SAINT~\cite{somepalli2021saint}). We report in Tab.~\ref{aptab:supervised_gap} ROC-AUC, AUC-PR for Inliers and for Outliers for the supervised baselines, where we also included our unsupervised BERT baseline for comparison. We plotted the results in Fig.~\ref{fig:supervised}. We observe highly saturated scores on \textbf{IID} and \textbf{NEAR} and a major performance degradation on \textbf{FAR}. The highest performing methods on \textbf{IID} and \textbf{NEAR}, XGBoost, BERT and Saint, achieve the lowest scores on \textbf{FAR} across all baselines.

\paragraph{Full Kyoto-2006\texttt{+} dataset} As previously described, AnoShift contains subsets of the full data, for allowing faster prototyping. We evaluate BERT for anomalies on the full Kyoto-2006+ yearly sets and observe that the ROC-AUC results are consistent with the subsets. The evaluation is performed on held-out test sets for each year and the results are available in Tab. \ref{tab:full_splits_bert}. The subsets as well as the full sets used in our experiments are available at \url{https://share.bitdefender.com/s/9D4bBE7H8XTdYDB}.


















\begin{figure}[t]
    \begin{center}
        \includegraphics[width=1.0\textwidth]{final_images/supervised.pdf}
    \end{center}
    \caption{Performance evaluation for several supervised learning baselines in a binary classification task on the Kyoto-2006+ dataset.}
    \label{fig:supervised}
\end{figure}



\begingroup
\begin{table}[t]
\begin{center}
    \caption{Performance evolution over time for supervised methods: \texttt{IID} vs \texttt{NEAR} vs \texttt{FAR}. We report the ROC-AUC, PR-AUC for inliers, and PR-AUC for outliers metrics. The performance degrades over time also in this supervised setting. Notice there is a large (and consistent) gap between the supervised methods and the unsupervised BERT baseline. Best score per split in bold.}
    
    \begin{tabular}{lccc}
    \toprule
    Supervised Baselines & \texttt{IID}      & \texttt{NEAR}    & \texttt{FAR}     \\
    \midrule
                 & \multicolumn{3}{c}{ROC-AUC (\%) }  \\
    \cmidrule(lr){2-4}
    XGB \cite{chen2015xgboost})          & \textbf{99.79}  \small 0.01     & 98.63  \small 0.03    & 38.66   \small 0.32  \\
    SVM \cite{cortes1995support}         & 89.95  \small 0.81     & 88.74  \small	0.51    & \textbf{55.92}  \small	0.21    \\
    RandomForest \cite{liaw2002classification} & 95.81  \small	0.13     & 94.58  \small	0.40    & 46.22  \small	0.73    \\
    SAINT \cite{somepalli2021saint})       & 99.33  \small	0.10     & \textbf{98.74}  \small 0.09     & 37.85  \small 0.65   \\
    TabNet \cite{arik2021tabnet}       & 95.49  \small	0.28     & 92.86  \small	0.64    & 45.80  \small	0.57      \\
    BERT \cite{devlin2018bert} - sup     & 99.20  \small	0.02     & 97.96  \small	0.08      & 30.42  \small	0.40    \\
    \midrule
    BERT \cite{devlin2018bert} - unsup   & 84.54  \small 0.07   & 86.05  \small 0.25       & 28.15  \small 0.06    \\
    \textbf{Difference} (best sup, BERT-unsup)   & +15.25    & +12.69      &+27.77    \\


    \midrule
     & \multicolumn{3}{c}{PR-AUC Inliers (\%) }  \\
    \cmidrule(lr){2-4}
    XGB \cite{chen2015xgboost})          & \textbf{99.64}  \small 0.01     &  97.49  \small	0.02   & 10.72  \small	0.05    \\
    SVM \cite{cortes1995support}         & 93.57  \small	0.10     & 92.96  \small	0.20    & \textbf{65.27}  \small	0.42   \\
    RandomForest \cite{liaw2002classification} & 94.85  \small	0.13     & 91.67  \small	0.91    & 12.27  \small	1.52    \\
    SAINT \cite{somepalli2021saint})        & 99.41  \small	0.08     & \textbf{98.89}  \small 0.10    & 40.95  \small	0.42    \\
    TabNet \cite{arik2021tabnet}       & 94.24  \small	0.87     & 88.45  \small	1.89    & 11.63  \small	0.72    \\
    BERT \cite{devlin2018bert} - sup     & 98.87  \small	0.04     & 93.37  \small	0.19    & 9.57  \small	0.06     \\
    \midrule
    BERT \cite{devlin2018bert} - unsup   & 74.61  \small 0.13     & 58.94  \small 0.69    & 8.22  \small 0.02    \\
    \textbf{Difference} (best sup, BERT-unsup)   & +25.03    & +39.95      &+57.05    \\

    \midrule
                 & \multicolumn{3}{c}{PR-AUC Outliers (\%) } \\
    \cmidrule(lr){2-4}
    XGB \cite{chen2015xgboost})         & \textbf{99.81}  \small 0.01     & \textbf{99.44}  \small 0.02    & 84.52  \small	0.21 \\
    SVM \cite{cortes1995support}         & 92.16  \small	0.22     & 90.93  \small	0.39    & 74.56  \small	0.14    \\
    RandomForest \cite{liaw2002classification} & 98.21  \small	0.06     & 98.34  \small	0.11    & \textbf{91.51}  \small 0.12    \\
    SAINT \cite{somepalli2021saint})        & 99.28  \small	0.11     & 98.63  \small	0.07    & 46.21  \small	1.11 \\
    TabNet \cite{arik2021tabnet}       & 98.11  \small	0.04     & 97.83  \small	0.16    & 91.42	  \small 0.27    \\
    BERT \cite{devlin2018bert} - sup     & 99.51  \small	0.02     & 99.20  \small	0.04    & 80.12  \small	0.07    \\
    \midrule
    BERT \cite{devlin2018bert} - unsup   & 89.83  \small 0.07     & 95.96  \small 0.06    & 78.38  \small 0.02    \\
    \textbf{Difference} (best sup, BERT-unsup)   & +9.98 & +3.48      &+13.13    \\

    \bottomrule
    \end{tabular}

    \label{aptab:supervised_gap}
\end{center}
\end{table}
\endgroup

\begingroup
\setlength{\tabcolsep}{4pt} \begin{table}[t]
\begin{center}
   \caption{BERT for anomalies ROC-AUC evaluation on the full sets in comparison with the subsets}
        \begin{tabular}{l c c c c c c c c c c c}
        
        \toprule
         &  & \multicolumn{7}{c}{ ROC-AUC} \\
        
        {Split} & \textbf{2006} & \textbf{2007} & \textbf{2008} & \textbf{2009} & \textbf{2010} & \textbf{2011} & \textbf{2012} &
        \textbf{2013} & \textbf{2014} & \textbf{2015} \\
        \midrule
        
        Full & 83.07 & 84.84 & 82.39 & 85.87 & 84.98 & 90.79 & 90.40 & 86.74 & 24.05 & 38.84   \\
        
        300k Subset & 82.20 & 84.63 & 83.80 & 85.60 & 83.51 & 88.03 & 88.12 & 82.31 & 22.10 & 36.90   \\

        \bottomrule
        
        \end{tabular}
        \label{tab:full_splits_bert}
\end{center}
\end{table}
\endgroup




\subsection{Pseudo-code for BERT training}
\label{appendix:pseudocode}
We present the three algorithms for each training strategies: IID~\ref{alg:iid_training}, Finetune~\ref{alg:finetune}, and Distilation~\ref{alg:distilation}. 

\begin{algorithm}
\caption{IID training}\label{alg:iid_training}
\begin{algorithmic}
\State 
\State 
\State 

\For{}

        \For{}
            
            \State {}  \Comment{Sample a binary mask of batch size with 0.15 probability}
            
            \State {}
            \State {} \Comment{Reconstruction loss for masked tokens}
            
            \State{compute loss gradients}
            \State{perform optimizer step}

        \EndFor 

\EndFor

\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Finetune strategy}\label{alg:finetune}
\begin{algorithmic}
\State 
\State 
\State 
\For{}

    \For{}

        \For{}
            
            \State   \Comment{Sample a binary mask of batch size with 0.15 probability}
            
            \State {}
            \State {} \Comment{Reconstruction loss for masked tokens}
            
            \State{compute loss gradients}
            \State{perform optimizer step}

        \EndFor 

    \EndFor

\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Distillation strategy}\label{alg:distilation}
\begin{algorithmic}
\State {}
\State {}
\State {}
\State { train IID on }

\For{}

    \State 

    \For{}

        \For{}
            
            \State   \Comment{Sample a binary mask of batch size with 0.15 probability}
            
            \State {}
            \State {}
            \State {}
            
            \State{compute loss gradients}
            \State{perform optimizer step}

        \EndFor
    \EndFor

    \State{}

\EndFor
\end{algorithmic}
\end{algorithm}



\subsection{Other considered datasets}
\label{appendix:other_datasets}
We show here the detailed process of how we choose the Kyoto-2006\texttt{+} dataset and why we consider it to be one of the few relevant in the distribution-shift context for stream-like data. We performed an in depth analysis over a large number of datasets. We wanted it to come from a stream-like data (as opposed to the less natural, existing benchmarks on images or text~\cite{clear, mind_the_gap, shift_auto_drive, intel_cl_benck, wilds} and we for two \textbf{characteristics that we consider essential for a distribution shift benchmark}:
\begin{itemize}
    \item It spreads over a large enough time-span, such that the distribution shift will naturally occur, (rather than being synthetically injected, exhibiting sudden changes)
    \item It is not solved already (existing methods do not report almost perfect scores on it)
\end{itemize}

\paragraph{Network traffic datasets}
We first looked over a wide range of known network traffic datasets for intrusion detection (see Tab.~\ref{aptab:net_traffic_datasets}), and after analysing them we concluded that most are artificially created, with injected samples, in very restricted scenarios. Only Kyoto-2016 was a proper dataset, extended over a long enough period of time for showing a natural distribution shift.



\begingroup
\begin{table}[t]
\begin{center}
    \caption{Network traffic datasets.}
    
    \begin{tabular}{lccc}
    \toprule
    Dataset  & \shortstack{Number\\ of\\ samples }& Time-span & Other details \\
    \midrule
    \href{https://www.unb.ca/cic/datasets/ids-2017.html}{CIC-IDS2017} & 3 mil & 5 days & Different attack types per day\\
    \href{https://www.unb.ca/cic/datasets/ids-2018.html}{CSE-CIC-IDS2018} &  4.5 mil & 17 days & Different attack types per day \\
    \href{https://research.unsw.edu.au/projects/unsw-nb15-dataset}{UNSW-NB15} & 2.5 mil   & 2 days & too small 	\\
    \href{https://research.unsw.edu.au/projects/bot-iot-dataset}{BoT-IoT} &  73 mil   &  4 days  & too small	\\
    \href{https://research.unsw.edu.au/projects/toniot-datasets}{ToN-IoT} & 22 mil  & 6 days & too small \\
    \href{https://www.unb.ca/cic/datasets/nsl.html}{NSL-KDD} & 0.15 mil  & 45 days & max reported ROC-AUC 99\%\\
    \href{https://arxiv.org/abs/1803.04967}{LANL} &  1.6 mil & 58 days   & max reported ROC-AUC 99\%\\
    AAD   & 1.8 mil  & 90 days  & internally build dataset, max ROC-AUC  98\%\\
    \href{http://www.takakura.com/Kyoto_data/}{Kyoto-2006\texttt{+}} &  806M & 10 years  \\
    \bottomrule
    \end{tabular}

    \label{aptab:net_traffic_datasets}
\end{center}
\end{table}
\endgroup


\paragraph{System logs datasets}
We next focused our attention on system logs, since the time-span is usually more extensive in these dataset and the natural distribution shift is more probable to occur. But under our analysis (t-SNE, Jeffreys divergence, OTDD, multiple baselines), these datasets did not exhibit a clear distribution shift over time, so we decided to further analyse them until concludent results. We used Drain and Spell as log parsers, and we report in Tab.~\ref{aptab:sys_logs_datasets} the results using the LogAnomaly~\cite{loganomaly} baseline.







\begingroup
\begin{table}[t]
\setlength{\tabcolsep}{4pt} \begin{center}
    \caption{System logs datasets.}
    
    \begin{tabular}{l cc ccc c}
    \toprule
    Dataset - Preprocessor  & \shortstack{Number\\ of\\ samples} & Time-span & IID (\%) & NEAR (\%) & FAR (\%) & Split proportion \\
    \midrule
        \href{https://zenodo.org/record/3227177#.YvZIGuxBwpM}{HDFS}   - Drain &11 mil &  40h       & 54 &  66 & 57 & 6-6-6\\
        \href{https://www.usenix.org/cfdr-data#hpc4}{BGL} -	Spell/Drain & 4.7 mil     &  214 days       & 67/68  &  43/73  & 45/35  &  2-3-2\\
        \href{https://www.usenix.org/cfdr-data#hpc4}{Thunderbird}	-  Spell/Drain  &     211 mil & 244 days        & 72/71  &  72/72  &  76/75  &  3-3-3\\
        \href{https://www.usenix.org/cfdr-data#hpc4}{Liberty}  &  266 mil     & 315 days      &  & & & grouped anomalies\\
        \href{https://www.usenix.org/cfdr-data#hpc4}{Spirit-CMU} - Spell     &  272 mil   & 570 days      & 80 &67& 72& 6-5-3\\
    \bottomrule
    \end{tabular}

    \label{aptab:sys_logs_datasets}
\end{center}
\end{table}
\endgroup

\paragraph{Multi-variate timeseries datasets}
We next looked over general multi-variate timeseries datasets, but the most popular ones are quite small and almost perfectly solved already (see Tab.~\ref{aptab:mts_datasets}).



\begingroup
\begin{table}[t]
\setlength{\tabcolsep}{4pt} \begin{center}
    \caption{Multi-variate timeseries datasets.}
    
    \begin{tabular}{lccc}
    \toprule
Dataset & \shortstack{Number \\of\\ samples} & Time-span &  \multicolumn{1}{p{3cm}}{\shortstack{max reported\\ unsup\\ ROC-AUC (\%)}} \\
    \midrule
    \href{https://smap.jpl.nasa.gov/data/}{SMAP - Soil Moisture Active Passive} & 0.5 mil     &  7-14 days  &  99 \\
    \href{https://itrust.sutd.edu.sg/testbeds/secure-water-treatment-swat/}{SWaT - Secure Water Treatment} &  0.9 mil & 11 days & 85 \\
    \href{https://itrust.sutd.edu.sg/testbeds/water-distribution-wadi/}{WADI - Water Distribution} &  0.96 mil & 16 days    & 90 \\
    \href{https://github.com/NetManAIOps/OmniAnomaly/tree/master/ServerMachineDataset}{SMD - Server Machine Dataset} & 1.4 mil & 35 days & 99 \\
    \href{https://zenodo.org/record/3549604#.YvZLEexBwpM}{MSDS - Multi-Source Distributed System} & 0.3 mil & days - months  & 91\\
    \href{https://github.com/eBay/RANSynCoders/tree/main/data}{PSM - Pooled Server Metrics} &  & 147 days & 98 \\
    \href{https://github.com/khundman/telemanom}{MSL - Mars Science Laboratory} &  0.13 mil & - & 99\\
    \href{https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/}{NAB - Numenta Anomaly Benchmark} &  0.37 mil  & - & 99 \\
    \href{https://physionet.org/content/svdb/1.0.0/}{MBA - MIT-BIH Supraventricular Arrhythmia} &  0.2 mil & 78 half-hour ECGs & 99 \\
    \bottomrule
    \end{tabular}

    \label{aptab:mts_datasets}
\end{center}
\end{table}
\endgroup



\clearpage














\section{Appendix}
\label{sec:appendix_own}
\paragraph{Raw Kyoto dataset documentation}
The dataset used in our proposed benchmark consists of a preprocessing of the Traffic Data from Kyoto University's Honeypots and results from a discretization of the numerical features in the original dataset, such that a language-modelling approach can be easily applied. The original dataset consists of 14 conventional features and 10 additional features. The conventional features in the original dataset includes connection duration, type of service, number of source and destination bytes, server rate errors percentage and flag of connection. We keep all the conventional features and apply a exponentially-scaled binning over the continuous values (duration, number of source and destination bytes) which results in 233 bins and a discretization of the percentage features in 100 distinct values. As an observation, some of the 10 additional features (the source and destination IP addresses, source and destination port numbers) might be useful when designed models (eg. graphs) focusing on connections between the nodes in the system. 


\paragraph{Our split proposal documentation}
We propose a yearly split of the dataset and group adjacent years into Train, NEAR data and FAR data, which we use to highlight the performance degradation of several benchmarks in time, due to the distributional shift of the data which we demonstrate with a comprehensive analysis. In our proposed split, Train data consists of the first 4 years (2006-2010), Near data of the following 3 years (2011-2013) and Far data of the last two available years (2014-2015). We publish the data in splits of single years, in csv format. The columns 0 to 13 are the discretized conventional features in the Kyoto-2006+ dataset, preserving the original order, column 14 contains the complete timestamp. Columns 15, 16 and 17 correspond to the first 3 additional features in the original data, namely IDS\_detection, Malware\_detection and Ashula\_detection, which indicates presence of alert triggers from the 3 IDS solutions: Symantec IDS, clamav and Ashula shellcode detector. Column 19 in the preprocessed dataset corresponds to the protocol used by the connection.
\paragraph{Intended uses}
We hope that our proposed benchmark shifts the general direction of treating network intrusion detection towards a timely fashion that suffers from distributional shift, hereby providing a better suited evaluation protocol for upcoming research in this field.

\paragraph{URL to dataset download}
We redirect our readers to the repository of the raw Kyoto dataset published by the Kyoto University at \url{https://www.takakura.com/Kyoto_data/} and provide a repository of data under our proposed processing at \url{https://share.bitdefender.com/s/9D4bBE7H8XTdYDB}, with subsets of 300000 instances and heldout sets of 30000 instances for each split, maintaining the original inlier to outlier ratio from the original data in each split, as well as the full processed splits, with each full split except 2006 being provided in two parts. We make the remark that the 2006 split contains fewer instance, due to data collection debuting in November.

\subsection{Code for dataset loading}
We publish our code as a public GitHub repository \url{https://github.com/bit-ml/AnoShift/}, containing the data preprocessing script that transforms the original data in our format, sample data manipulation notebooks, license and additional information.

\subsection{Author responsibility for violation of rights}
There is no sensitive data leaked in the preprocessed dataset. The authors are not aware of any possible violation of rights and take responsibility for the published data.

\subsection{Dataset hosting and long-term preservation}
The authors take full responsibility for the availability of the processed data in the provided repository. However, no statement can be made about the availability of the raw Kyoto-2006+ data published by the Kyoto University, as it depends on Takakura.com. To avoid further problems, we have published our preprocessed version.

\subsection{Licence}
We release our code under a BSD 3-Clause License, therefore allowing the redistribution and use in source and binary forms, with or without
modification, under the 3 clauses specified by the Berkeley Software Distribution License:

\begin{enumerate}
    \item{Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
    }
    \item{Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.}
    
    \item{Neither the name of the copyright holder nor the names of its
    contributors may be used to endorse or promote products derived from
    this software without specific prior written permission.}
\end{enumerate}





\end{document}
