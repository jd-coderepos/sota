

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage[font=small,skip=0pt]{caption}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{adjustbox}
\DeclareMathOperator*{\argmax}{argmax}


\aclfinalcopy 



\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Sentence Simplification with Memory-Augmented Neural Networks}
\author{Tu Vu, Baotian Hu, Tsendsuren Munkhdalai and Hong Yu \\
  University of Massachusetts Amherst, Amherst, MA 01003, USA\\
  \tt tuvu@cs.umass.edu\\
  University of Massachusetts Medical School, Worcester, MA 01655, USA\\
  \tt \{baotian.hu,hong.yu\}@umassmed.edu\\
  Microsoft Research, Montr\'eal, QC H3A 3H3, Canada\\
  \tt tsendsuren.munkhdalai@microsoft.com\\ \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Sentence simplification aims to simplify the content and structure of complex sentences, and thus make them easier to interpret for human readers, and easier to process for downstream NLP applications. Recent advances in neural machine translation have paved the way for novel approaches to the task. In this paper, we adapt an architecture with augmented memory capacities called Neural Semantic Encoders \cite{Munkhdalai:17a} for sentence simplification. Our experiments demonstrate the effectiveness of our approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.
\end{abstract}

\section{Introduction}
The goal of sentence simplification is to compose complex sentences into simpler ones so that they are more comprehensible and accessible, while still retaining the original information content and meaning. Sentence simplification has a number of practical applications.  On one hand, it provides reading aids for people with limited language proficiency~\cite{Watanabe:09,Siddharthan:03}, or for patients with linguistic and cognitive disabilities ~\cite{Carroll:99}. On the other hand, it can improve the performance of other NLP tasks ~\cite{Chandrasekar:96, Knight:00,Klebanov:04}.

Prior work has explored monolingual machine translation (MT) approaches, 
utilizing corpora of simplified texts, e.g., Simple English Wikipedia (SEW), and making use of statistical MT models, such as phrase-based MT (PBMT) ~\cite{Stajner:15,Coster:11,Wubben:12}, tree-based MT (TBMT) ~\cite{Zhu:10,Woodsend:11}, or syntax-based MT (SBMT) \cite{Xu:16}.

Inspired by the success of neural MT ~\cite{Sutskever:14,Cho:14}, recent work has started exploring neural simplification with sequence to sequence (Seq2seq) models, also referred to as encoder-decoder models.  Nisioi et al. ~\shortcite{Nisioi:17} implemented a standard LSTM-based Seq2seq model and found that they outperform PBMT, SBMT, and unsupervised lexical simplification approaches. Zhang and Lapata ~\cite{Zhang:17} viewed the encoder-decoder model as an agent and employed a deep reinforcement learning framework in which the reward has three components capturing key aspects of the target output: simplicity, relevance, and fluency. 

The common practice for Seq2seq models is to use recurrent neural networks (RNNs) with Long Short-Term Memory \cite[LSTM,][]{Hochreiter:97} or Gated Recurrent Unit \cite[GRU,][]{Cho:14} for the encoder and decoder \cite{Nisioi:17,Zhang:17}. These architectures were designed to be capable of memorizing long-term dependencies across sequences. Nevertheless, their memory is typically small and might not be enough for the simplification task, where one is confronted with long and complicated sentences. 

In this study, we go beyond the conventional LSTM/GRU-based Seq2seq models and propose to use a memory-augmented RNN architecture called Neural Semantic Encoders (NSE). This architecture has been shown to be effective in a wide range of NLP tasks \cite{Munkhdalai:17a}. The contribution of this paper is twofold:
\begin{figure}[h!]
\centering
\includegraphics[scale=0.28]{fig1.pdf}
\caption{Attention-based encoder-decoder model. The model may attend to relevant positions in the source sentence while decoding the simplification, e.g., to generate the target word \textit{won} the model may attend to the words \textit{received}, \textit{nominated} and \textit{Prize} in the source sentence.}
\label{fig1}
\vspace*{-3mm}
\end{figure}

(1) First, we present a novel simplification model which is, to the best of our knowledge, the first model that use memory-augmented RNN for the task. We investigate the effectiveness of neural Seq2seq models when different neural architectures for the encoder are considered. Our experiments reveal that the \textsc{NseLstm} model that uses an NSE as the encoder and an LSTM as the decoder performed the best among these models, improving over strong simplification systems. (2) Second, we perform an extensive evaluation of various approaches proposed in the literature on different datasets. Results of both automatic and human evaluation show that our approach is remarkably effective for the task, significantly reducing the reading difficulty of the input, while preserving grammaticality and the original meaning. We further discuss some advantages and disadvantages of these approaches.






\section{Neural Sequence to Sequence Models}


\subsection{Attention-based Encoder-Decoder Model}
\label{sec3.1}
Our approach is based on an attention-based Seq2seq model \cite{Bahdanau:15} (Figure \ref{fig1}). Given a complex source sentence , the model learns to generate its simplified version . The encoder reads through  and computes a sequence of hidden states :
\begin{center}
,
\end{center}
where  is a non-linear activation function (e.g., LSTM),  is the hidden state at time . Each time the model generates a target word , the decoder looks at a set of positions in the source sentence where the most relevant information is located. Specifically, another non-linear activation function  is used for the decoder where the hidden state  at time  is computed by:
\begin{center}
.
\end{center}
Here, the context vector  is computed as a weighted sum of the hidden vectors :
\begin{center}
, \hspace*{8mm}
,
\end{center}
where  is the dot product of two vectors. Generation is conditioned on  and all the previously generated target words :
\begin{center}
,\0.2ex] 
 \hline\hline
Newsela & 41,066& 30,193& 25.94& 15.89\\  
 WikiSmall & 113,368& 93,835& 24.26& 20.33\\ 
 WikiLarge & 201,841& 168,962& 25.17& 18.51\\ 
 \hline 
\end{tabular}\end{adjustbox}
\0.2ex] 
 \cline{2-7}
& \textbf{BLEU} & \textbf{SARI}  & \textbf{BLEU} & \textbf{SARI}  & \textbf{BLEU} & \textbf{SARI}  \\
 \hline\hline
 \textsc{Pbmt-R} & 18.19 & 15.77 & 46.31 & 15.97 &81.11 & 38.56 \\
 \textsc{Hybrid} & 14.46 & \textbf{30.00} & \textbf{53.94} & \textbf{30.46} & 48.97& 31.40  \\
 \textsc{Sbmt-Sari} & \multicolumn{2}{|c|}{{NA}} & \multicolumn{2}{|c|}{{NA}} & 73.08 & \textbf{39.96} \\
 \textsc{Dress} & 23.21 & 27.37  & 34.53 & 27.48 & 77.18 & 37.08 \\
 \textsc{Dress-Ls} & 24.30& 26.63 & 36.32 & 27.24 &  80.12  & 37.27 \\
  \hline\hline
  \textsc{LstmLstm-B} & 24.38 & 27.66  & 50.53 & 17.67 & 88.81 & 34.22 \\
 \textsc{NseLstm-B} & \textbf{26.31} & 27.42  & 53.42 &17.47 & \textbf{92.02} & 33.43 \\
   \hline\hline
  \textsc{LstmLstm-S} & 23.50 &28.67 & 31.32 & 28.04 & 
 81.95 & 35.45 
\\
 \textsc{NseLstm-S} & 22.62 &29.58 & 29.72 & 29.75  & 80.43 & 36.88 
\\
 \hline
\end{tabular}\end{adjustbox}
\0.2ex] 
 \cline{2-13}
& \textbf{F} & \textbf{A}  & \textbf{S} & \textbf{Avg.}  & \textbf{F} & \textbf{A}  & \textbf{S} & \textbf{Avg.}  &
\textbf{F} & \textbf{A}  & \textbf{S} & \textbf{Avg.}  \\
 \hline\hline
 \textsc{Reference} 
&4.58	&2.98	&3.99	&3.85 &4.63	&3.97	&3.59	&4.06 &4.59	&4.43	&2.38	&3.80 \\
  \hline\hline
 \textsc{Pbmt-R} &3.73	&\textbf{3.90}	&1.98	&3.20 &4.07	&4.11	&2.28	&3.49 &4.22	&4.09	&2.31	&3.54 \\
  \textsc{Hybrid} &2.77	&2.56	&2.41	&2.58 &3.21	&3.62	&2.56	&3.13 &2.63	&2.48	&2.26	&2.46 \\
 \textsc{Sbmt-Sari} &\multicolumn{4}{|c|}{{NA}} & \multicolumn{4}{|c|}{{NA}} &3.89	&3.87	&2.54	&3.43 \\
 \textsc{Dress} &3.98	&2.84	&2.93	&3.25 &4.35	&3.33	&3.49	&3.72 &4.56	&3.66	&2.63	&3.62 \\
 \textsc{Dress-Ls} &3.99	&2.90	&2.98	&3.29 &4.43	&3.33	&3.56	&3.77 &4.68	&3.88	&2.63	&3.73 \\
  \hline\hline
  \textsc{LstmLstm-B} &3.95	&2.93	&3.14	&3.34 & 4.42	&3.88	&2.65	&3.65 &\textbf{4.80}	&4.47	&1.89	&3.72 \\
 \textsc{NseLstm-B} &\textbf{4.26}	&3.13	&3.39	&\textbf{3.59} &\textbf{4.74}	&\textbf{4.22}	&2.49	&3.82 &4.73	&\textbf{4.58}	&1.94	&3.75 \\
   \hline\hline
  \textsc{LstmLstm-S} &4.24	&3.03	&\textbf{3.45} &3.57  &4.59	&3.40	&3.42	&3.80 &4.73	&4.23	&2.21	&3.72 \\
 \textsc{NseLstm-S} &3.83	&2.78	&3.01	&3.21 & 4.57	&3.28	&\textbf{3.81}	&\textbf{3.89} &4.65	&3.95	&\textbf{2.90} &\textbf{3.83} \\

 \hline
\end{tabular}\end{adjustbox}
\3mm]
\caption{Example model outputs on Newsela. Substitutions are shown in bold.}
\label{tbl4}
\vspace*{-2mm}
\end{table*}

On WikiSmall, \textsc{NseLstm-B} performed best on both Fluency and Adequacy. On WikiLarge, \textsc{LstmLstm-B} achieved the highest Fluency score while \textsc{NseLstm-B} received the highest Adequacy score. In terms of Simplicity and Average, \textsc{NseLstm-S} outperformed all other systems on both WikiSmall and WikiLarge.

As shown in Table \ref{tbl3}, neural models often outperformed traditional systems (\textsc{Pbmt-R}, \textsc{Hybrid}, \textsc{Sbmt-Sari}) on Fluency. This is not surprising given the recent success of neural Seq2seq models in language modeling and neural machine translation \cite{Zaremba:14,Jean:15}. On the downside, our manual inspection reveals that neural models learn to perform copying very well in terms of rewrite operations (e.g., copying, deletion, reordering, substitution), often outputting the same or parts of the input sentence.

Finally, as can be seen in Table \ref{tbl3}, \textsc{Reference} scored lower on Adequacy compared to Fluency and Simplicity on Newsela. On Wikipedia-based datasets, \textsc{Reference} obtained high Adequacy scores but much lower Simplicity scores compared to Newsela. This supports the assertion by previous work \cite{Xu:15} that SEW has a large proportion of inadequate simplifications.

\subsection{Correlations}
Table \ref{tbl5} shows the correlations between the scores assigned by humans and the automatic evaluation measures. There is a positive significant correlation between Fluency and Adequacy (0.69), but a negative significant correlation between Adequacy and Simplicity (-0.64). BLEU correlates well with Fluency (0.63) and Adequacy (0.90) while SARI correlates well with Simplicity (0.73). BLEU and SARI show a negative significant correlation (-0.54). The results reflect the challenge of managing the trade-off between Fluency, Adequacy and Simplicity in sentence simplification. \begin{table}[h!]
\centering
\begin{adjustbox}{max width=0.45\textwidth}
\begin{tabular}{| l | c c c c |} 
 \hline
 & \textbf{Adequacy} & \textbf{Simplicity} & \textbf{BLEU} & \textbf{SARI}\\
  \hline
   \hline
 \textbf{Fluency} & 0.69 & -0.03 & 0.63 & -0.48\\
 \textbf{Adequacy} & & -0.64 & 0.90 & -0.81\\
 \textbf{Simplicity} & & & -0.56 & 0.73\\
 \textbf{BLEU} & & & & -0.54\\
 
  \hline
\end{tabular}\end{adjustbox}
\3mm]
\caption{Pearson correlation between the scores assigned by humans and the automatic evaluation measures. Scores marked  are significant at }
\label{tbl5}
\vspace*{-3mm}
\end{table}

\section{Conclusions}
In this paper, we explore neural Seq2seq models for sentence simplification. We propose to use an architecture with augmented memory capacities which we believe is suitable for the task, where one is confronted with long and complex sentences. Results of both automatic and human evaluation on different datasets show that our model is capable of significantly reducing the reading difficulty of the input, while performing well in terms of grammaticality and meaning preservation.


\section{Acknowledgements}
We would like to thank Emily Druhl, Jesse Lingeman, and the UMass BioNLP team for their help with this work. We also thank Xingxing Zhang, Sergiu Nisioi for valuable discussions. The authors would like to acknowledge the reviewers for their thoughtful comments and suggestions.
\bibliography{naaclhlt2018}
\bibliographystyle{acl_natbib}

\end{document}
