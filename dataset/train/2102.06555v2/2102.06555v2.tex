\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage[usenames,dvipsnames]{xcolor}


\usepackage{amsmath,amsfonts,amssymb,bm}
\usepackage{graphics,float}
\newcommand{\scalar}[2]{\langle #1 , #2 \rangle}
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\newcommand{\vectd}{\mathbf{d}}
\newcommand{\vectf}{\mathbf{f}}


\newcommand{\bleu}[1]{{\color{blue}{#1}}}
\newcommand{\ver}[1]{{\color{green}{#1}}}
\newcommand{\rouge}[1]{{\color{red}{#1}}}

\def\alphab{\boldsymbol\alpha}
\def\betab{\boldsymbol\beta}
\def\epsilonb{\boldsymbol\epsilon}
\def\Sigmab{\boldsymbol\Sigma}
\def\Lambdab{\boldsymbol\Lambda}
\def\thetab{\boldsymbol\theta}
\def\gammab{\boldsymbol\gamma}


\def\G{\pi}
\def\GG{\boldsymbol\G}
\def\Gs{\pi^s}
\def\GGs{\boldsymbol\pi^s}
\def\Gv{\pi^v}
\def\GGv{\boldsymbol\pi^v}
\def\a{{\bf a}}
\def\b{{\bf b}}
\newcommand{\pibf}{{\mathbf{\pi}}}
\def\graph{{\text{graph}}}

\def\h{{\bf h}}
\def\g{{\bf g}}
\def\Gbf{{\bf G}}

\def\e{{\bf e}}
\def\w{{\bf w}}
\def\v{{\bf v}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\V{{\bf V}}
\def\Dbf{{\bf D}}
\def\Bbf{{\bf B}}
\def\Mbf{{\bf M}}
\def\Hbf{{\bf H}}
\def\rbf{{\bf r}}

\def\X{{\bf X}}
\def\Y{{\bf Y}}
\def\L{{\bf L}}
\def\R{{\mathbb{R}}}
\def\Abf{{\mathbf{A}}}
\def\Bbf{{\mathbf{B}}}
\def\Jbf{{\mathbf{J}}}

\def\U{{\mathbf{U}}}

\def\diag{{\text{diag}}}

\def\vec{{\text{vec}}}
\def\tr{{\text{tr}}}
\def\one{{\mathbf{1}}}
\newcommand{\CCOT}{\texttt{CCOT}}
\newcommand{\CCOTGW}{\texttt{CCOT-GW}}
\newcommand{\MovieL}{\textsc{MovieLens}}
\newcommand{\COOT}{\text{COOT}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\mbf}{\mathbf{m}}
\newcommand{\qbf}{\mathbf{q}}

\newcommand{\simplex}{\Sigma}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\featurespace}{C}
\newcommand{\mmspacex}{\mathcal{X}}
\newcommand{\mmspacey}{\mathcal{Y}}
\newcommand{\couplingset}{\Pi}
\newcommand{\Pm}{\mathcal{P}}
\newcommand{\C}{\mathbf{C}}

\newcommand{\Sp}{\mathbb{S}}
\newcommand{\gw}{GW}
\newcommand{\fgw}{FGW}

\newcommand{\wass}{W}
\newcommand{\sgw}{SGW}
\newcommand{\risgw}{RISGW}
\newcommand{\gm}{GM}
\newcommand{\D}{\Delta}
\newcommand{\Sn}{S_{n}}
\newcommand{\insided}{c}
\newcommand{\supp}{\text{supp}}
\newcommand{\Stief}{\mathbb{V}_{p}(\R^{q})}
\newcommand{\lebsm}{\mathcal{L}}
\newcommand{\comp}{comp}
\newcommand{\XX}{\mathfrak{X}}

\newcommand{\ft}[1]{{f_{#1}}}
\newcommand{\esp}[2]{{\underset{#1}{\E[#2]}}}
\newcommand{\integ}[1]{{[\![#1]\!]}}
\def\P{{\mathcal{P}}}
\def\Xcal{{\mathcal{X}}}
\def\Ycal{{\mathcal{Y}}}
\def\Zcal{{\mathcal{Z}}}
\def\Hcal{{\mathcal{H}}}
\def\Acal{{\mathcal{A}}}

\def\Tcal{{\mathcal{T}}}
\def\ot{{\Tcal}}
\def\mongeot{{M}}

\newcommand{\froeb}[2]{{\langle {#1},{#2} \rangle_{\F}}}
\newcommand{\dr}{\mathrm{d}}
\def\spt{{\text{spt}}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\green}[1]{{\color{green} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\orange}[1]{{\color{orange} #1}}
\newcommand{\purple}[1]{{\color{purple} #1}}
 \newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}

\usepackage{comment}

\usepackage{hyperref}
\usepackage[accepted]{icml2021}
\usepackage{graphicx}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{}}}}}
\makeatother

\begin{document}
	
	\twocolumn[
	\icmltitle{Online Graph Dictionary Learning}
	\begin{icmlauthorlist}
		\icmlauthor{C\'{e}dric Vincent-Cuaz}{maasai}
		\icmlauthor{Titouan Vayer}{ens}
		\icmlauthor{R\'{e}mi Flamary}{pol}
		\icmlauthor{Marco Corneli}{maasai,smi}
		\icmlauthor{Nicolas Courty}{ubs}
	\end{icmlauthorlist}
	\icmlaffiliation{ens}{ENS\,de\,Lyon,\,LIP\,UMR\,5668,\,Lyon,\,France}
	\icmlaffiliation{pol}{Ecole\,Polytechnique,\,CMAP,\,UMR\,7641,\,Palaiseau,\,France}
	\icmlaffiliation{ubs}{Univ.Bretagne-Sud,\,CNRS,\,IRISA,\,Vannes,\,France}
	\icmlaffiliation{maasai}{Univ.C{\^o}te\,d{'}Azur,\,Inria,\,CNRS,\,LJAD,\,Maasai,\,Nice,\,France}
	\icmlaffiliation{smi}{Univ.C{\^o}te\,d{'}Azur,\,Center\,of\,Modeling,\,Simulation\,\&\,Interaction,\\Nice,\,France\\}
	\icmlcorrespondingauthor{C\'{e}dric Vincent-Cuaz}{cedric.vincent-cuaz@inria.fr}
	\icmlkeywords{Machine Learning, ICML}
	\vskip 0.3in
	]
	\printAffiliationsAndNotice{}
	
	\begin{abstract}
		Dictionary learning is a key tool for representation learning, that explains the
		data as linear combination of few basic elements. Yet, this analysis is not
		amenable in the context of graph learning, as graphs usually belong to different
		metric spaces. We fill this gap by proposing a new online Graph Dictionary
		Learning approach, which uses the Gromov Wasserstein divergence for the data
		fitting term. In our work, graphs are encoded through their nodes' pairwise relations 
		and modeled as convex combination of graph
		atoms, {\em i.e.} dictionary elements, estimated thanks to  an online stochastic
		algorithm, which operates on a dataset of unregistered graphs with potentially
		different number of nodes. Our approach naturally extends to labeled graphs, and
		is completed by a novel upper bound  that can be used as a fast approximation of
		Gromov Wasserstein in the embedding space. We provide numerical evidences
		showing the interest of our approach for unsupervised embedding of graph
		datasets and for online graph subspace estimation and tracking.
		
	\end{abstract}
	\section{Introduction}
	
	{The question of how to build machine learning algorithms able to go beyond
		vectorial data and to learn from structured data such as graphs has been of great
		interest in the last decades. Notable applications can be found in molecule
		compounds} \citep{kriege-recognizing-2018}, brain connectivity
	\citep{ktena-distance-2017}, social networks \citep{yanardag-deep-2015}, time
	series \citep{cuturi-soft-dtw-2018}, trees \citep{day-optimal-1985} {or} images
	\citep{harchaoui-image-2007,bronstein2017geometric}. {Designing good representations for these data is challenging, as their
		nature is by essence non-vectorial, and requires dedicated modelling of their representing structures. Given
		sufficient data and labels, end-to-end approaches with neural networks have
		shown great promises in the last years \cite{wu2020comprehensive}. In this work, we focus on the unsupervised representation learning problem, where
		the entirety of the data might not be known beforehand, and is rather produced
		continuously by different sensors, and available through streams. {In this setting, tackling the non-stationarity of the underlying generating process is challenging~\citep{Ditzler2015}.  Good examples can be found, for instance, in the context of 
			dynamic functional connectivity~\citep{Heitmann2018} or network
			science~\citep{masuda2020guide}. As opposed to recent approaches focusing on dynamically varying graphs in online or continuous learning ~\citep{yang2018bandit,vlaski2018online,Wang2020StreamingGN},
we rather suppose in this work that \emph{distinct} graphs are made progressively available \cite{Zambon,Grattarola}. This setting is particularly challenging as the structure, the attributes or the number of nodes of each graph observed at a time step can differ from the previous ones. We propose to tackle this problem by learning a linear representation of graphs with online dictionary learning.}
		\vspace{1.5mm}
		\paragraph{Dictionary Learning (DL)}
		
		Dictionary Learning \citep{mairal2009online,schmitz2018wasserstein} is a field of unsupervised learning that aims at estimating a linear representation of the data, \ie\ to learn a linear subspace defined by the span of a family of vectors, called \emph{atoms}, which constitute a \emph{dictionary}. These atoms are inferred from the input data by minimizing a reconstruction error. These representations have been notably used in statistical frameworks such as data clustering \citep{ng-spectral-2002},
		recommendation systems \citep{bobadilla2013recommender} or dimensionality
		reduction  \citep{candes-robust-2009}. While DL methods mainly
		focus on vectorial data, 
		
		it is of prime interest to investigate flexible and interpretable
		factorization models applicable to \emph{structured data}. We also consider the dynamic or time varying version of the problem, where the data generating process may exhibit non-stationarity over time, yielding a problem of subspace change or tracking (see {\em e.g.} ~\cite{narayanamurthy2018nearly}), where one wants to monitor changes in the subspace best describing the data. In this work, we rely on optimal transport as a fidelity term to compare these structured data. 
		
		\paragraph{Optimal Transport for structured data}
		Optimal Transport (OT) theory provides a set of methods for comparing probability distributions, using, \emph{e.g.} the well-known Wasserstein
		distance \citep{villani-topics-2003}. 
		It has been notably} used by the machine learning community {in the
		context of distributional unsupervised learning}
	~\citep{arjovsky2017wasserstein, schmitz2018wasserstein,peyre-computational-2020}. Broadly speaking the interest of OT lies in its
	ability to provide correspondences, or relations, between sets of points.
	Consequently, it has recently garnered attention for learning tasks where
	the points are described by graphs/structured data (see  \emph{e.g.}
	\cite{DBLP:conf/aaai/NikolentzosMV17,maretic2019got,Togninalli19,xu2019scalable,vayer-optimal-nodate,barbe:hal-02795056}).
	One of the key ingredient in this case is to rely on the so called
	Gromov-Wasserstein (GW) distance
	\citep{memoli-gromovwasserstein-2011,sturm2012space} which is an OT
	problem adapted to the scenario in which the supports of the probability
	distributions lie in different metric spaces. The GW distance is
	particularly suited for comparing \emph{relational data}
	\cite{peyre2016gromov,solomon_entropic_2016} and, in a graph context, is
	able to find the relations between the nodes of two graphs when their
	respective structure is encoded through the pairwise relationship between
	the nodes in the graph. GW has been further studied for weighted directed
	graphs in \citep{chowdhury2019gromov} and has been extended to labeled
	graphs thanks to the Fused Gromov-Wasserstein (FGW) distance in
	\citep{vayer-fused-2018}. Note that OT divergences as losses for linear and
	non-linear DL over vectorial data have already been proposed in
	\citep{BTSSPP15,rolet2016fast, schmitz2018wasserstein} but the case of
	structured data remains quite unaddressed. A non-linear DL approach for
	graphs based on GW was proposed in \citep{xu_gromov-wasserstein_2019} but
	suffers from a lack of interpretability and high computational complexity
	(see discussions in Section \ref{sec:related_work}). To the best of our knowledge, a linear
	counterpart does not exist for now.

	\begin{figure}[!t]
		\includegraphics[width=\columnwidth]{images_png/fig1}\vspace{-8mm}
		\caption{ From a dataset of graphs with different number of nodes, our method builds
			a dictionary of graph atoms with an online procedure. It uses the
			Gromov-Wasserstein distance as data fitting term between a  convex
			combination of the atoms and a pairwise relations representation for graphs
			from the dataset.}
		\label{fig:f1}
		\vspace{-5mm}
	\end{figure}
	
	\paragraph{Contributions}
	In this paper we use OT distances between structured data to design a linear and online DL for undirected graphs. Our proposal is depicted in Figure~\ref{fig:f1}. It
	consists in a new factorization model for undirected graphs optionally having node attributes relying on (F)GW distance as data fitting term. We propose an online 
	stochastic algorithm to learn the dictionary which scales to large real-world
	data (Section \ref{subsec:algos}), and uses extensively novel derivations of
	sub-gradients of the (F)GW distance (Section \ref{subsec:GW_subgrad}).  An
	unmixing procedure projects the graph in an embedding space defined {\em w.r.t.}
	the dictionary (Section \ref{subsec:GU}). Interestingly enough, we prove that
	the GW distance in this embedding is upper-bounded by a Mahalanobis distance
	over the space of \emph{unmixing} weights, providing a reliable and fast
	approximation of GW (Section \ref{subsec:GW_all}). Moreover, this approximation
	defines a proper kernel that can be efficiently used for clustering and
	classification of graphs datasets (sections
	\ref{subsec:simu}-\ref{subsec:real1}). We empirically demonstrate the relevance
	of our approach for online subspace estimation and subspace tracking by
	designing streams of graphs over two datasets (Section \ref{subsec:real2}).
	
	\paragraph{Notations}
	The simplex of histograms with  bins is . Let
	denote   the set of symmetric matrices in . The Euclidean norm is denoted as  and  the Frobenius inner product. We denote the gradient of a function  over  at  in a stochastic context by . The number nodes in a graph is called  the \emph{order} of the graph.
	
	
	\section{Online Graph Dictionary Learning }
	
	\subsection{(Fused) Gromov-Wasserstein for graph similarity}
	\label{subsec:GW_all}
	A graph  with  nodes, can be regarded as a tuple  where  is a matrix that
	encodes a notion of similarity between nodes and  is
	a histogram, or equivalently a vector of weights  which models the relative importance of the nodes within the graph. Without any prior knowledge uniform weights can be chosen so that . The matrix  carries the neighbourhood information of the nodes and, depending on
	the context, it may designate the adjacency matrix, the Laplacian matrix \cite{maretic2019got} or the matrix of the shortest-path distances between the nodes~\citep{bavaud2010euclidean}. Let us now consider two graphs  and , {of potentially different orders (\emph{i.e} )}. The  distance between  and  is defined as the result of the following optimization problem:
	
	where  is the set of couplings between . 
	The optimal coupling  of the GW problem acts as a probabilistic matching of nodes which tends to associate pairs of nodes that have similar pairwise relations in  and , respectively. In the following we denote by  the optimal value of \eqref{eq:gwdef} or  by  when the weights are uniform.
	
	The previous framework can be extended to graphs with node attributes
	(typically  vectors). In this case we  use the Fused
	Gromov-Wasserstein distance (FGW) \cite{vayer-fused-2018,vayer-optimal-nodate}
	instead of GW. More precisely, a labeled graph  with  nodes can be
	described this time as a tuple  where  is the matrix of all features. Given two labeled graphs  and ,
	FGW aims at finding an optimal coupling by minimizing an OT cost which is a
	trade-off of a Wasserstein cost between the features and a GW cost between the
	similarity matrices. For the sake of clarity, we detail our approach in the GW context and refer the reader to the supplementary material for its extension to FGW.
	
	\subsection{Linear embedding and GW unmixing}\label{subsec:GU}
	
	\paragraph{Linear modeling of graphs} We propose to model a graph as a weighted sum of pairwise relation
	matrices. More precisely, given a graph  and a \emph{dictionary}
	 we want to find a linear representation  of the graph , as faithful as
	possible. The dictionary is made of pairwise relation matrices of graphs with
	order . Thus, each   is called an
	\emph{atom}, and  is referred as
	\emph{embedding} and denotes the coordinate of the graph  in the dictionary
	as illustrated in Fig.\ref{fig:f1}. We rely
	on the GW distance to assess the quality of our linear
	approximation and propose to minimize it to estimate its optimal embedding. In
	addition to being interpretable thanks to its linearity, we also propose to promote
	sparsity in the weights  similarly to sparse coding \citep{chen2001atomic}.
	Finally note that, when the pairwise matrices  are adjacency matrices
	and the dictionary atoms have components in , the model
	 provides a matrix whose components
	can be interpreted as probabilities of connection between the nodes.
	
	
	\paragraph{Gromov-Wasserstein unmixing}
	We first study the unmixing problem that consists in projecting a graph on the
	linear representation discussed above, \emph{i.e.} estimate the optimal
	embedding  of a graph .
	The unmixing problem can be expressed as the minimization of the GW
	distance between the similarity matrix associated to the graph and its linear representation in the dictionary:
	
	where  induces a \textbf{negative} quadratic regularization promoting sparsity on the
	simplex as discussed in \citet{li2016methods}. In order to solve the non-convex problem in \eqref{eq:unmix}, we propose to
	use a Block Coordinate Descent (BCD) algorithm \citep{tseng2001convergence}. 
	\begin{algorithm}[t]
		\caption{BCD for unmixing problem \ref{eq:unmix}}
		\label{alg:BCD1}
		\begin{algorithmic}[1]
			\STATE Initialize 
			\REPEAT
			\STATE Compute OT matrix  of , with CG algorithm ~\citep[Alg.1 \& 2]{vayer-fused-2018}.
			\STATE Compute the optimal  solving \eqref{eq:unmix} for a fixed
			 with CG algorithm. 
			\UNTIL{convergence}
		\end{algorithmic}
	\end{algorithm}
	
	The BCD (Alg.\ref{alg:BCD1}) works by alternatively updating the OT matrix of the GW distance and the embeddings . When  is fixed the problem
	is a classical GW  {which is a non-convex quadratic program. We 
		solve it} using a Conditional Gradient (CG) algorithm
	\citep{jaggi2013revisiting} based on \citep{vayer-optimal-nodate}.  Note that the use of the exact GW instead of a regularized
	proxy allowed us to keep a sparse OT matrix as well as
	to  preserve ``high frequency'' components of the graph, as opposed to
	regularized versions of GW \citep{peyre2016gromov,solomon_entropic_2016,xu2019gromov} that promotes dense OT matrices
	and leads to smoothed/averaged pairwise matrices. For a fixed OT matrix , the problem of finding  is a non-convex
	quadratic program and can also be tackled with a CG algorithm.  Note that for non-convex
	problems the CG algorithm is
	known to converge to a local
	stationary point \citep{lacoste-julien-convergence-2016}. In practice, we
	observed a typical convergence of the CGs in a few tens of iterations. The BCD
	itself converges in less than 10 iterations.
	\paragraph{Fast upper bound for GW} Interestingly, when two graphs belong to
	the linear subspace defined by our dictionary, there exists a proxy of the GW distance using a
	dedicated Mahalanobis distance as described in the next propositon: 
	\begin{proposition}
		\label{prop:embed_graph}
		For two embedded graphs with embeddings  and , assuming they share the same weights , the following inequality holds
		
		where  and .  is a positive semi-definite matrix hence engenders a Mahalanobis distance between embeddings.
	\end{proposition}
	As detailed in the supplementary material, this upper bound is obtained by considering the GW cost between the linear models calculated using the admissible coupling . The latter coupling assumes that both graph representations are aligned and therefore is a priori suboptimal. As such, this bound is not tight in general. However, when the embeddings are close, the optimal coupling matrix should be close to  so that Proposition \ref{prop:embed_graph} provides a reasonable proxy to the GW distance into our embedding space. In practice, this upper bound can be used to compute efficiently pairwise kernel matrices or to do retrieval of closest samples (see numerical experiments).
	
	\subsection{Dictionary learning and online algorithm}\label{subsec:algos}
	Assume now that the dictionary  is not known
	and has to be estimated from the data.
	We define a dataset of  graphs . Recall that each graph  of
	order  is summarized by its pairwise relation matrix  and weights  over nodes, as
	described in Section \ref{subsec:GW_all}.
	The DL problem, that aims at estimating the optimal dictionary
	for a given dataset can be expressed as:
	
	where . Note that the optimization problem above is a classical sparsity promoting
	dictionary learning on a linear subspace but with the important novelty
	that the reconstruction error is computed with the GW distance. This allows us to learn a graphs
	subspace of fixed
	order  using a dataset of graphs with various orders.
	The sum over the errors in \eqref{eq:dl} can be seen as an expectation and we
	propose to devise an online strategy to optimize the problem similarly to the
	online DL proposed in \citep{mairal2009online}. The main
	idea is to update the dictionary  with a stochastic
	estimation of the gradients on few dataset graphs (minibatch). At each
	stochastic update the unmixing problems are solved independently for each
	graph of the minibatch using a fixed dictionary 
	, {using the procedure described in Section
		\ref{subsec:GU}}. Then one can compute a gradient of the loss on the minibatch
	\emph{w.r.t} 
	 and proceed to a projected gradient step.  The
	stochastic update of the proposed
	algorithm is detailed in Alg.\ref{alg:GW1}. Note that it
	can be used on a finite dataset with possibly several epochs on the whole
	dataset or online in the presence of streaming graphs. We provide an example
	of such subspace tracking in Section \ref{subsec:real2}.  We will refer to our
	approach as GDL in the rest of the paper.

	\begin{algorithm}[t]
		\caption{GDL: stochastic update of atoms }
		\label{alg:GW1}
		\begin{algorithmic}[1]
			\STATE Sample a minibatch of graphs  .
			\STATE Compute optimal  by solving B independent unmixing problems with Alg.\ref{alg:BCD1}. 
			\STATE Projected gradient step with estimated gradients  (equation in supplementary), : \vspace{-2mm}
			
		\end{algorithmic}
	\end{algorithm}
	
	\paragraph{Numerical complexity} The numerical complexity of GDL depends on the complexity of each update. The main computational bottleneck is the unmixing procedure that relies on multiple resolution of GW
	problems. The complexity of solving a GW with the {CG} algorithm between two graphs of order  and  and computing
	its gradient is dominated by   operations
	\cite{peyre2016gromov,vayer-fused-2018}.
	Thus given dictionary
	atoms of order , the worst case complexity 
	can be only \textbf{quadratic} in
	the highest graph order in the dataset.  For instance, estimating embedding on dataset IMDB-M (see Section \ref{subsec:real1}) over 12 atoms takes on average  ms per graph (on processor i9-9900K CPU 3.60GHz). {We refer the reader to the
		supplementary for more details.}
	Note that in addition to scale well to large datasets thanks to the stochastic
	optimization, our method also leads to important speedups when using the
	representations as input feature for other ML tasks. For instance, we can use the upper bound in
	\eqref{eq:mah_gw} to compute efficiently kernels between graphs instead of
	computing all pairwise GW distances.
	\paragraph{GDL on labeled graphs}{We can also define the same DL procedure for labeled graphs using the FGW distance. The unmixing part defined in \eqref{eq:unmix} can be adapted by considering a linear embedding of the similarity matrix \emph{and} of the feature matrix parametrized by the \emph{same} . From an optimization perspective, finding the optimal coupling of FGW can be achieved  using a CG procedure so that Alg.\ref{alg:GW1} extends naturally to the FGW case. Note also that the upper bound
		of Proposition \ref{prop:embed_graph} can be generalized to this setting. This discussion is
		detailed in supplementary material.}
	
	\subsection{Learning the graph structure and distribution}
	\label{subsec:GW_subgrad}
	
	Recent researches have studied the use of potentially more general distributions
	
	on the nodes of graphs than the naive uniform ones commonly used. \citep{xu2019scalable} empirically explored the use of distributions induced by
	degrees, such as parameterized power
	laws, , where  with  and . They demonstrated the interest of this approach but also highlighted how hard it is to calibrate, which {advocates for learning these distributions.}
	With this motivation, we extend our GDL model defined in
	equation \ref{eq:dl} and propose to learn atoms of the form . In this setting we have
	two independent dictionaries modeling the relative importance of the nodes with
	, and their pairwise relations through . This
	dictionary learning problem reads:
	
	where  are the structure and distribution embeddings and the linear models are defined as:
	
	Here we exploit fully the GW
	formalism by estimating simultaneously the graph distribution 
	and its geometric structure . Optimization problem \ref{eq:dl_h} can
	be solved by an adaptation of stochastic Algorithm \ref{alg:GW1}. We estimate
	the structure/node weights unmixings
	 over a minibatch of graphs with a BCD (see Section
	\ref{subsec:algos}). Then we perform simultaneously a projected gradient step
	update of  and . More details are given in the supplementary.

	The optimization procedure above requires to have access to a gradient for
	the GW distance \emph{w.r.t.} the weights. To the best of our knowledge no
	theoretical results exists in the literature for finding such gradients. We
	provide below a simple way to compute a subgradient for GW weights from subgradients of the well-known Wasserstein distance:
	\begin{proposition}\label{eq:prop3}
		{Let  and  be two graphs. Let  be an optimal coupling of the GW problem between . We define the following cost matrix . Let  be the dual variables of the following linear OT problem:}
		
		Then  (\textit{resp} ) is a subgradient of the function  (\textit{resp} ).
	\end{proposition}
	The proposition above shows that the subgradient of GW \textit{w.r.t.} the
	weights can be found by solving a linear OT problem which corresponds to a Wasserstein distance. The ground cost  of this Wasserstein is moreover the gradient (\textit{w.r.t.} the couplings) of the optimal GW loss.
	Note that in practice the GW problem is solved with a CG algorithm which already
	requires to solve this linear OT problem at each iteration. In this way, after
	convergence, the gradient \textit{w.r.t.} the weights can be extracted for free
	from the last iteration of the CG algorithm. 
	The proof of proposition \ref{eq:prop3} is given in the supplementary material.
	
	\section{Related work and discussion \label{sec:related_work}}
	
	In this section we discuss the relation of our GDL framework with existing
	approaches designed to handle graph data. We first focus on existing contributions
	for graph representation in machine learning applications. Then, we discuss in more details the existing non-linear graph dictionary learning approach of \citep{xu_gromov-wasserstein_2019}.
	
	
	\paragraph{Graph representation learning} Processing of graph data
	in machine learning applications have traditionally been handled using implicit
	representations such as with graph kernels \citep{shervashidze09,vishwanathan2010graph}. Recent results have shown the
	interest of using OT based distances to measure graph similarities and to design new
	kernels~\citep{vayer-optimal-nodate,maretic2019got,chowdhury-generalized-2020}. However, one limit of kernel methods is
	that the representation of the graph is fixed \emph{a priori} and cannot be
	adapted to specific datasets. On the other hand, Geometric deep learning approaches \citep{bronstein2017geometric} attempt to learn
	the representation for structured data by means of deep learning~\citep{scarselli2008graph, perozzi2014deepwalk, niepert-learning-2016}. Graph Neural Networks \cite{wu2020comprehensive} have
	shown impressive performance for end-to-end supervised learning problems. Note
	that both kernel methods and many deep learning based representations for graphs suffer from the fundamental \emph{pre-image} problem, that prevents recovering actual
	graph objects from the embeddings. 
	Our proposed GDL aims at overcoming such a limit relying on an unmixing
	procedure that not only provides a simple vectorial representation on the
	dictionary but also allows a direct reconstruction of interpretable graphs (as
	illustrated in the experiments). A recent contribution potentially overcoming
	the pre-image problem  is~\citet{Grattarola}. In that paper, a variational
	autoencoder is indeed trained to embed the observed graphs into a constant
	curvature Riemannian manifold. The aim of that paper is to represent the graph
	data into a space where the statistical tests for change detection are easier.
	We look instead for a latent representation of the graphs that remains as
	interpretable as possible. As a side note, we point out that our GDL embeddings
	might be used as input for the statistical tests developed by
	\citep{Zambon,Zambon2019ChangePointMO} to detect stationarity changes in the
	stochastic process generating the observed graphs~(see for instance
	Figure~\ref{fig:online}) .
	\paragraph{Non-linear GW dictionary learning of graphs} 
	In a recent work,
	\citep{xu_gromov-wasserstein_2019} proposed a non-linear factorization of
	graphs using a regularized version of GW barycenters~\citep{peyre2016gromov} and
	denoted it as Gromov-Wasserstein Factorization (GWF). Authors propose to learn a dictionary  by 
	minimizing over  and  the
	quantity  where  is a GW barycenter. The main difference between GDL and this
	work lies in the linear representation of the approximated graph that we adopt whereas
	\citep{xu_gromov-wasserstein_2019} relies on the highly non-linear Gromov
	barycenter. As a consequence, the unmixing requires solving a complex
	bi-level optimization problem that is computationally expensive. Similarly,
	reconstructing a graph from this embedding requires again the resolution of a
	GW barycenter, whereas our linear reconstruction process is immediate. 
	In Section~\ref{sec:exp}, we show that our GDL
	representation technique compares favorably to GWF, both in terms of numerical complexity and
	performance.
	
	\section{Numerical experiments}\label{sec:exp}
	This section aims at illustrating the behavior of the approaches introduced so far for both clustering (Sections \ref{subsec:simu}-\ref{subsec:real1})  and online subspace tracking (Section \ref{subsec:real2}).
	
	
	\paragraph{Implementation details} The base OT solvers that are used in the
	algorithms rely on the POT toolbox \citep{flamary2017pot}.
	For our experiments, we considered the Adam algorithm \citep{kingma2014adam} as
	an adaptive strategy for the update of the atoms with a fixed dataset, but used
	SGD with constant step size for the online experiments in Section~\ref{subsec:real2}. The code is available at ~\href{https://github.com/cedricvincentcuaz/GDL}{https://github.com/cedricvincentcuaz/GDL}.
	
	\subsection{GDL on simulated datasets}\label{subsec:simu}
	The GDL approach discussed in this section refers to~\eqref{eq:dl}.
	First we illustrate it on datasets simulated according to the well understood Stochastic Block Model~\citep[SBM,][]{holland1983stochastic,wang1987stochastic} and show that we can recover embeddings and dictionary atoms corresponding to the generative structure.
	
	\paragraph{Datasets description}
	We consider two datasets of graphs, generated according to SBM, with various orders, randomly sampled in  .
	The first scenario () adopts three different generative structures (also
	referred to as \emph{classes}): dense (no clusters), two clusters and three
	clusters~(see Figures~\ref{fig:simplex}). Nodes are assigned to
	clusters into equal proportions. For each generative structure 100 graphs are
	sampled.
	The second scenario  considers the generative structure with two clusters,
	but with varying proportions of nodes for each block (see top of
	Figure~\ref{fig:interp_toy}), 150 graphs are simulated
	accordingly.
	In both scenarios we fix  as the probability of inter-cluster connectivity and  as the probability of intra-cluster connectivity. We consider adjacency matrices for representing the structures of the graphs in the datasets and uniform weights on the nodes.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{images_png/fig_simplex.pdf}\vspace{-4mm}
		\caption{Visualizations of the embeddings of the graphs from  with our GDL on 3 atoms. The positions on
			the simplex for the different classes are reported with no regularization
			(left) and sparsity promoting regularization (right). Three simulated graphs from  are shown in the middle and their positions
			on the simplex reported in red.} \label{fig:simplex}
		\vspace{-5mm}
	\end{figure}
	
	\paragraph{Results and interpretation} First we learn on dataset  a dictionary
	of 3 atoms of order 6. The unmixing coefficients for the
	samples in  are reported in Fig.~\ref{fig:simplex}. On the left, we see that the coefficients are not sparse on the simplex but the samples are clearly
	well clustered and graphs sharing the same class (i.e. color) are well separated.  
	When adding sparsity promoting regularization  (right part of the figure) the
	different classes are clustered on the corners of the simplex, thus suggesting
	that regularization leads to a more discriminant representation. The estimated atoms
	for the regularized GDL are reported on the top of Fig.~\ref{fig:f1} as both
	matrices  and their corresponding graphs.
	As it can be seen, the different SBM structures in  are recovered.
	Next we estimate on  a dictionary with 2 atoms of order 12. 
	The interpolation between the two estimated atoms for some samples is reported in Fig.~\ref{fig:interp_toy}.
	As it can be seen,  can be modeled as a one dimensional manifold where the
	proportion of nodes in each block changes continuously. We stress that the grey links on the bottom of Figure~\ref{fig:interp_toy} correspond to the entries of the reconstructed adjacency matrices. Those entries are in , thus encoding a probability of connection (see Section~\ref{subsec:GU}). 
	The darker the link, the higher the probability of interaction between the corresponding nodes. The possibility of generating random graphs using
	these probabilities opens the door to future researches.
	
	We evaluate in Fig.~\ref{fig:dist} the
	quality of the Mahalanobis upper bound in~\eqref{eq:mah_gw} as a proxy for the GW distance on
	. 
	On the left, one can see that the linear model allows us to recover the true GW distances between graphs most of the time.
	Exceptions occur for samples in the same class (i.e.  "near" to each other in terms of GW distance).
	The right part of the figure shows that the
	correlation between the Mahalanobis upper bound (cf. Proposition~\ref{prop:embed_graph}) and the GW distance between the
	embedded graphs is nearly perfect (0.999). This proves that our proposed upper bound
	provides a nice approximation of the GW distance between the input graphs, with a correlation of 0.96 (middle of the figure), at a much lower computational cost.  
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{images_png/interp_toy.pdf}\vspace{-2mm}
		\caption{On the top, a random sample of real graphs from  (two blocks). 
			On the bottom, reconstructed graphs as linear combination of two estimated atoms (varying proportions for each atom).} \label{fig:interp_toy}
	\end{figure}
	
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{images_png/truescaling_dist_gw_maha.png}\vspace{-1mm}
		\caption{Plot of the pairwise distances in  and their Pearson 
			correlation coefficients. GW distance between graphs versus its counterpart between the embedded graphs (left). GW distance between graphs versus  Mahalanobis distance between the embeddings (middle). GW distance between the embedded graphs versus Mahalanobis between the corresponding embeddings (right). } \label{fig:dist}
	\end{figure}
	
	
	\subsection{GDL on real data for clustering and classification}\label{subsec:real1} 
	We now show how our \emph{unsupervised} GDL procedure can be used to find meaningful representations for well-known graph classification datasets. The knowledge of the classes will be
	employed as a ground truth to validate our estimated embeddings in \emph{clustering} tasks. 
	For the sake of completeness, in supplementary material we also report the supervised
	classification accuracies of some recent supervised graph \emph{classification}
	methods (e.g. GNN, kernel methods) showing that our DL and embedding is
	also competitive for classification.
	\paragraph{Datasets and methods} We considered well-known benchmark datasets divided into three categories: i) IMDB-B and IMDB-M~\citep{yanardag-deep-2015} gather graphs without node attributes derived from social networks; ii) graphs with discrete attributes representing chemical compounds from MUTAG~\citep{debnath1991structure} and cuneiform signs from PTC-MR~\citep{krichene2015efficient}; iii) graphs with real vectors as attributes, namely  BZR, COX2~\citep{sutherland2003spline}
	and PROTEINS, ENZYMES~\citep{borgwardt2005shortest}. We benchmarked our models for clustering tasks with the following state-of-the-art OT models: 
	i) GWF ~\citep{xu_gromov-wasserstein_2019}, using the proximal point algorithm  detailed in that paper and exploring two configurations, i.e. with either fixed atom order (GWF-f) or random atom order (GWF-r, default for the method); ii) GW k-means (GW-k)  which is a k-means using GW distances and GW barycenter \citep{peyre2016gromov};
	iii) Spectral Clustering (SC) of ~\citep{shi2000normalized,stella2003multiclass} applied to the pairwise GW distance matrices or the pairwise FGW distance matrices for graphs with attributes. We complete these clustering evaluations with an ablation study of the effect of the negative quadratic regularization proposed with our models. As introduced in \eqref{eq:dl}, this regularization is parameterized by , so in this specific context we will distinguish GDL () from  ().
	
	\begin{table*}[t!]
		\caption{Clustering: Rand Index computed for benchmarked approaches on real datasets.}
		\label{tab:clustering}
		\begin{center}
			\scalebox{0.81}{
				
				\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
					\hline
					& \multicolumn{2}{|c|}{NO ATTRIBUTE} & \multicolumn{2}{|c|}{DISCRETE ATTRIBUTES} &\multicolumn{4}{c|}{REAL ATTRIBUTES} \\
					\hline
					MODELS & IMDB-B & IMDB-M& MUTAG& PTC-MR & BZR & COX2 & ENZYMES & PROTEIN\\
					\hline
					GDL \text{ } (ours) & 51.32(0.30) & 55.08(0.28) & 70.02(0.29) & 51.53(0.36) & 62.59(1.68) & 58.39(0.52) & 66.97(0.93) & 60.22(0.30)\\
					 (ours) &  & 55.41(0.20) &  &  &  &  & 66.79(1.12) & \\ \hline 
					GWF-r &51.24 (0.02) &  & 68.83(1.47) & 51.44(0.52) & 52.42(2.48) & 56.84(0.41)  & & 59.96(0.09)\\
					GWF-f &50.47(0.34)& 54.01(0.37)& 58.96(1.91)& 50.87(0.79) & 51.65(2.96) & 52.86(0.53) & 71.64(0.31) & 58.89(0.39)\\ 
					GW-k &50.32(0.02) & 53.65(0.07)& 57.56(1.50) & 50.44(0.35) & 56.72(0.50) & 52.48(0.12) & 66.33(1.42) & 50.08(0.01) \\
					SC & 50.11(0.10) & 54.40(9.45) &50.82(2.71) & 50.45(0.31) & 42.73(7.06) & 41.32(6.07) & 70.74(10.60)& 49.92(1.23) \\ 
					\hline
			\end{tabular}}
		\end{center}
	\end{table*}
	
	\paragraph{Experimental settings} For the datasets with attributes involving FGW, we tested 15 values of the trade-off parameter  via a logspace search in  and symmetrically
	 and select the one minimizing our objectives.
	For our GDL methods as well as for GWF, a first step consists into learning the atoms.
	A variable number of  atoms is tested, where  denotes the number of classes and , with a uniform number of atoms per class. 
	When the order  of each atom is fixed, for GDL and GWF-f, it is set to the
	median order in the dataset. The atoms are initialized by randomly sampling graphs from
	the dataset with corresponding order. We tested 4 regularization coefficients for both methods.
	
	The embeddings  are then computed and used as input for a
	k-means algorithm. However, whereas a standard Euclidean distance is used to
	implement k-means over the GWFs embeddings, we use the Mahalanobis
	distance from Proposition~\ref{prop:embed_graph} for the k-means clustering of the GDLs embeddings. Unlike
	GDL and GWF, GW-k and SC do not require any embedding learning step. Indeed,
	GW-k directly computes (a GW) k-means over the input graphs and SC is applied to
	the  GW distance matrix obtained from the input graphs. The cluster assignments
	are assessed by means of Rand Index ~\citep[RI,][]{rand1971objective}, computed
	between the true class assignment (known) and the one estimated by the different
	methods. For each parameter configuration (number of atoms, number of nodes and
	regularization parameter)
	we run each experiment five times, independently, with different random initializations. The mean RI was computed over the random
	initializations and the dictionary configuration leading to the highest RI was
	finally retained.
	
	\paragraph{Results and interpretation} Clustering results can be seen in Table
	\ref{tab:clustering}. The mean RI and its standard deviation are reported for each dataset and method.
	Our model outperforms or is at least comparable to the state-of-the-art OT based approaches for most of the datasets. Results show that the negative quadratic regularization proposed with our models brings additional gains in performance. Note that for this benchmark, we considered a fixed batch size for learning our models on labeled graphs, which turned out to be a limitation for the dataset ENZYMES. Indeed, comparable conclusions regarding our models performance have been observed by setting a higher batch size for this latter dataset and are reported in the supplementary material.
	This might be due to both a high number of heterogeneous classes and a high structural diversity of labeled graphs inside and among classes.
	
	We illustrate in Fig. \ref{fig:unmix_weights} the interest of the extension of
	GDL with estimated weights for IMDB-M dataset. We can see in the center-left
	part of the figure that, without estimating the weights, GDL can experience
	difficulties producing a model that preserves the global structure of the graph
	because of the uniform weights on the nodes. In opposition, simultaneously
	estimating the weights brings a more representative modeling (in the GW sense),
	as illustrated in the centred-right columns. The weights estimation can
	re-balance and even discard non relevant nodes, in the vein of attention mechanisms. We report in the supplementary material a companion study for clustering tasks which further supports our extension concerning the learning of node weights.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=1.04\linewidth]{images_png/weighted_gdl2.pdf}\vspace{-1mm}
		\caption{Modeling of two real life graphs from IMDB-M with our GDL
			approaches with 8 atoms of order 10. (left) original graphs 	from the dataset, (center left) linear model for GDL with uniform
			weights as in \eqref{eq:dl}, (center right) linear model for GDL with estimated
			weights  as in \eqref{eq:dl_h} and (right) different  on the
			estimated structure. }  \label{fig:unmix_weights}
	\end{figure}
\subsection{Online graph subspace estimation and change detection}\label{subsec:real2}
	
	Finally we provide experiments for online graph subspace estimation on simulated
	and real life datasets. We show that our approach can be used for subspace
	tracking of graphs as well as for change point detection of subspaces. 
	
	\paragraph{Datasets and experiments} In this section we considered two new large graph
	classification datasets: TWITCH-EGOS \citep{karateclub} containing social graphs without
	attributes belonging to 2 classes and TRIANGLES
	\citep{knyazev2019understanding} that is a simulated dataset of labeled graphs
	with 10 classes. Here we investigate how our approach fits to online data, \ie\ in the presence of a stream of graphs. The experiments are designed with
	different time segments where each segment streams graphs belonging to the same
	classes (or group of classes). The aim is to see if the method learns the current
	stream and detects or adapts to abrupt changes in the stream. For TWITCH-EGOS, we first streamed all graphs of a class (A), then graphs of the
	other class (B), both counting more than 60.000 graphs. All these graphs consist in a unique high-frequency (a hub structure) with sparse connections between non-central nodes (sparser for class B).  For TRIANGLES, the 
	stream follows the three
	groups A,B and C, with 10,000 graphs each, where  the labels  associated with each group are:
	,  and .
	
	\begin{figure}[t]
		\centering
		\hspace{-2mm}\includegraphics[width=.99\linewidth]{images_png/fig_twitch2.pdf}
\includegraphics[width=\linewidth]{images_png/fig_triangles2.pdf}\vspace{-4mm}
		\caption{Online GDL on dataset TWITCH-EGOS with 2 atoms of
			14 nodes each (top) and on TRIANGLES with 4 atoms of 17 nodes each (bottom).} \label{fig:online}
	\end{figure}
	
	\paragraph{Results and discussion} The online (F)GW losses and a running
	mean of these losses are reported for
	each dataset on the left part of Fig.~\ref{fig:online}. One the right part of the Figure, we
	report the average losses computed on several datasets containing data from each stream at some
	time instant along the iterations. First, the online learning for both datasets
	can be seen in the running means with a clear decrease of loss on each time
	segment. Also,  note that at each event (change of
	stream) a jump in terms of loss is visible suggesting that the method can
	be used for change point detection. Finally it is interesting to see on the
	TRIANGLES dataset that while the loss on Data B is clearly decreased during
	Stream B it increases again during Stream C, thus showing that our algorithm performs subspace tracking, adapting to the new data and forgetting old subspaces no longer necessary.  
	
	\section{Conclusion}
	
	We present a new \emph{linear} Dictionary Learning approach for
	graphs with different orders relying on the Gromov Wasserstein (GW) divergence, where graphs are modeled as convex combination of graph atoms. We design an online stochastic algorithm to efficiently learn our dictionary and propose a computationally light proxy to the GW distance in the described graphs subspace. Our experiments on clustering classification and online subspace tracking demonstrate the interest of our unsupervised representation learning approach. We envision several extensions to this work, notably in the context of
	graph denoising or graph inpainting.
	
	\section*{Acknowledgments}
	This work is partially funded through the projects OATMIL ANR-17-CE23-0012, OTTOPIA ANR-20-CHIA-0030 
	and 3IA C\^{o}te d'Azur Investments ANR-19-P3IA-0002 of the French National Research
	Agency (ANR). This research was produced within the framework of Energy4Climate
	Interdisciplinary Center (E4C) of IP Paris and Ecole des Ponts ParisTech. This
	research was supported by 3rd Programme d'Investissements d'Avenir
	ANR-18-EUR-0006-02. This action benefited from the support of the Chair
	"Challenging Technology for Responsible Energy" led by l'X – Ecole polytechnique
	and the Fondation de l'Ecole polytechnique, sponsored by TOTAL. This work is supported by the ACADEMICS grant of the IDEXLYON, project of the Université de Lyon, PIA operated by ANR-16-IDEX-0005.
	The authors are grateful to the OPAL infrastructure from Universit\'{e} C\^{o}te d'Azur for providing resources and support.
	
	\bibliography{citations}
	\bibliographystyle{icml2021}
	\newpage
	
	\onecolumn
	\section{Supplementary Material}
	\subsection{Notations \& definitions}\label{sec:defs}
	In this section we recall the notations used in the rest of the supplementary. 
	
	For matrices we note   the set of symmetric matrices in  and  the Frobenius inner product defined for real matrices  as  where  denotes the trace of matrices. Moreover  denotes the Hadamard product of , \ie\ . Finally  denotes the vectorization of the matrix .
	
	For vectors the Euclidean norm is denoted as  associated with the inner product . For a vector  the operator  denotes the diagonal matrix defined with the values of . If  is a positive semi-definite matrix we note  the pseudo-norm defined for  by . By some abuse of terminology we will use the term Mahalanobis distance to refer to generalized quadratic distances defined as . The fact that  is positive semi-definite ensures that  satisfies the
	properties of a pseudo-distance. 
	
	For a -D tensor  we note  the tensor-matrix multiplication, \ie\ given a matrix ,  is the matrix .
	
	The simplex of histograms (or \emph{weights}) with  bins is . For two histograms  the set  is the set of couplings between . 
	
	Recall that for two graphs  and  the  distance between  and  is defined as the result of the following optimization problem:
	
	In the following we denote by  the optimal value of \eqref{eq:gwdef_supp} or  by  when the weights are uniform. With more compact notations:
	
	where  is the -D tensor 
	
	For graphs with attributes we use the Fused Gromov-Wasserstein distance \cite{vayer-optimal-nodate}. More precisely consider two graphs  and  where  are the matrices of all features. Given  and a cost function  between vectors in  the  distance is defined as the result of the following optimization problem:
	
	In the following we note  the optimal value of \eqref{eq:fgwdef_supp} or by  when the weights are uniform. The term  will be called the \emph{Wasserstein objective} and denoted as  and the term  will be called the \emph{Gromov-Wasserstein objective} and denoted .
	
	\subsection{Proofs of the different results}
	\subsubsection{(F)GW upper-bounds in the embedding space}
	\begin{proposition}[Gromov-Wasserstein]
		\label{prop:embed_graph}
		For two embedded graphs with embeddings  and  over the set of pairwise relation matrices , with a shared masses vector , the following inequality holds
		
		where  and .  is a positive semi-definite matrix hence engenders a Mahalanobis distance between embeddings.
	\end{proposition}
	\paragraph{Proof.} Let consider the formulation of the GW distance as a Frobenius inner product (see \emph{e.g} \citep{peyre2016gromov}). Denoting  the optimal transport plan between both embedded graph and the power operation over matrices applied at entries level, 
	
	Using the marginal constraints of GW problem, \emph{i.e} , and the symmetry of matrices ,\eqref{eq:rawGW_embedded} can be developed as follow,
	
	
	With the following property of the trace operator:
	
	Denoting , \eqref{eq:GW_embedded1} can be expressed as:
	
	As  is a minimum of the GW objective, we can bound by above \eqref{eq:GW_embedded1} by evaluating the GW objective in , which is a sub-optimal admissible coupling.
	
	with . It suffices to prove that the matrix  is a PSD matrix to conclude that it defines a Mahalanobis distance over the set of embeddings  which bounds by above the GW distance between corresponding embedded graphs.
	Let consider the following reformulation of an entry  as follow,
	
	where . Hence with   ,  can be factorized as  and therefore is a PSD matrix.
	
	
	A similar result can be proven for the Fused Gromov-Wasserstein distance:
	\begin{proposition}[Fused Gromov-Wasserstein]
		For two embedded graphs with node attributes, with embeddings  and  over the set of pairwise relation matrices , and a shared masses vector , the following inequality holds ,
		
		with,
		
		Where  and , and , are PSD matrices and therefore their linear combination being PSD engender Mahalanobis distances over the unmixing space.
	\end{proposition} 
	\paragraph{Proof.}
	Let consider the optimal transport plan  of the  distance between both embedded structures.
	
	where  and  denotes respectively the Gromov-Wasserstein objective and the Wasserstein objective. 
	As a similar approach than for Proposition \ref{eq:mah_gw} can be used for the GW objective involved in \eqref{eq:FGW_bound1}, we will first highlight a suitable factorization of the Wasserstein objective . Note that for any feature matrices ,  with an euclidean ground cost can be expressed as follow using the marginal constraints on ,
	
	Returning to our main problem \ref{eq:FGW_bound1}, a straigth-forward development of its Wasserstein term  using \eqref{eq:FGW_matrix} leads to the following equality,
	
	Similarly than for the proof of Proposition 1,  is an optimal admissible coupling minimizing the FGW problem, thus \eqref{eq:FGW_bound1} is upper bounded by its evaluation in the sub-optimal admissible coupling . Let  the PSD matrix coming from the proof of Proposition \ref{prop:embed_graph}.
	
	Let   which is also a PSD matrix as it can be factorized as  with . 
	
	Let us denote ,  which is PSD  as convex combination of PSD matrices, hence engender a Mahalanobis distance in the embedding space. To summarize, \eqref{eq:FGW_bound2} holds ,
	

	\subsubsection{Proposition 3. Gradients of GW \textit{w.r.t.} the weights}
	In this section we will prove the following result:
	\begin{proposition}
		\label{grad_prop}
		{Let  and  be two graphs. Let  be an optimal coupling of the GW problem between . We define the following cost matrix . Let  be the dual variables of the following linear OT problem:}
		
		Then  (\textit{resp} ) is a subgradient of the function  (\textit{resp} ).
	\end{proposition}
	In the following  should be understood as . Let  and  be two graphs of order  and  with  and . Let  be an optimal solution of the GW problem \emph{i.e.} . We define . We consider the problem:
	
	We will first show that the optimal coupling for the Gromov-Wasserstein problem is also an optimal coupling for the problem \eqref{eq:linear_of_gw}, \ie\ . This result is based on the following theorem which relates a solution of a Quadratic Program (QP) with a solution of a Linear Program (LP):
	\begin{theorem}[Theorem 1.12 in \cite{murty-linear-1988}]
		\label{murty_theo}
		Consider the following (QP):
		
		Then if  is an optimal solution of \eqref{eq:qp_general} it is an optimal solution of the following (LP):
		
	\end{theorem}
	
	Applying Theorem \ref{murty_theo} to our case gives exactly that: 
	
	since  is an optimal solution of the GW problem and so . 
	
	
	
	Now let  be an optimal solution to the dual problem of \eqref{eq:linear_of_gw}. Then by strong duality it implies that:
	
	Since  we have:
	
	To prove Proposition \ref{grad_prop} the objective is to show that  is a subgradient of  (by symmetry the result will be true for ). In other words we want to prove that:
	
	This condition can be rewritten based on the following simple lemma:
	\begin{lemma}
		\label{lemmaone}
		The dual variable  is a subgradient of  if and only if:
		
	\end{lemma}
	
	\paragraph{Proof.}
	It is a subgradient if and only if:
	
	However using \eqref{eq:strong_dual} and the definition of  we have:
	
	So overall:
	
	
	
	In order to prove Proposition \ref{grad_prop} we have to prove that the condition in Lemma \ref{lemmaone} is satisfied. We will do so by leveraging the weak-duality of the GW problem as described in the next lemma:
	\begin{lemma}
		For any vectors  we define: 
		
		Let  be an optimal solution of the GW problem. Consider:
		
		where . Let  be the dual variables of the problem in \eqref{eq:eqlemmmm}. If  then  is a subgradient of 
	\end{lemma}
	\paragraph{Proof.}
	Let  be any weights vector be fixed. Recall that  so that:
	
	The Lagrangian associated to \eqref{eq:beforelag} reads:
	
	Moreover by weak Lagrangian duality:
	
	However: 
	
	So by considering the dual variable  defined previously we have:
	
	Now combining \eqref{eq:weak_duality} and \eqref{eq:duddu} we have:
	
	Since  we have proven that:
	 
	However Lemma \ref{lemmaone} states that  is a subgradient of  if and only if:
	 
	So combining \eqref{eq:eqeqeqeqe} with Lemma \ref{lemmaone} proves:
	
	However we have  by \eqref{eq:strong_dual_F}. So  using \eqref{eq:eqeqeqeqe} with . So we can only hope to have . 
	
	
	The previous lemma states that it is sufficient to look at the quantity  in order to prove that  is a subgradient of . Interestingly the condition  is satisfied which proves Proposition \ref{grad_prop} as sated in the next lemma:
	\begin{lemma}
		With previous notations we have . In particular  is a subgradient of  so that Proposition \ref{grad_prop} is valid. 
	\end{lemma}
	
	\paragraph{Proof.}
	We want to find:
	
	We define . Since  is optimal coupling for  by \eqref{eq:gpislp} then for all  we have  by the property of the optimal couplings for the Wasserstein problems. Equivalently: 
	
	Then:
	
	Which proves .
	
	
	
	\subsection{Algorithmic details}
	\subsubsection{GDL for graphs without attributes} 
	We propose to model a graph as a weighted sum of pairwise relation
	matrices. More precisely, given a graph  and a \emph{dictionary}
	  we want to find a linear representation  of the graph , as faithful as
	possible. The dictionary is made of pairwise relation matrices of graphs with
	order .  is referred as
	\emph{embedding} and denotes the coordinate of the graph  in the dictionary. We rely
	on the GW distance to assess the quality of our linear
	approximation and propose to minimize it to estimate its optimal embedding. 
	
	
	\subsubsection{Gromov-Wasserstein unmixing}
	We first study the unmixing problem that consists in projecting a graph on the
	linear representation discussed above, \emph{i.e.} estimate the optimal
	embedding  of a graph . Our GW unmixing problem reads as 
	
	
	where  induces a \textbf{negative} quadratic regularization promoting sparsity on the
	simplex as discussed in \citet{li2016methods}. In order to solve the non-convex problem in \eqref{eq:unmix}, we propose to
	use a Block Coordinate Descent (BCD) algorithms \citep{tseng2001convergence}. We fully detail the algorithm in the following and refer our readers to the main paper for the discussion on this approach.
	\begin{algorithm}[H]
		\caption{BCD for GW unmixing problem \ref{eq:unmix}}
		\label{alg:BCD1}
		\begin{algorithmic}[1]
			\STATE Initialize 
			\REPEAT
			\STATE Compute OT matrix  of , with CG algorithm ~\citep[Alg.1 \& 2]{vayer-fused-2018}.
			\STATE Compute the optimal  solving \eqref{eq:unmix} for a fixed
			 with CG algorithm \ref{alg:CGw}
			\UNTIL{convergence}
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}[H]
		\caption{CG for solving GW unmixing problem \emph{w.r.t}  given }
		\label{alg:CGw}
		\begin{algorithmic}[1]
			\REPEAT
			\STATE Compute , gradients  \emph{w.r.t}  of  following \eqref{eq:gradw_GW}.
			\STATE Find direction  
			\STATE Line-search: denoting ,
			
			\STATE 
			\UNTIL{convergence}
		\end{algorithmic}
	\end{algorithm}
	
	Partial derivates of the GW objective  \emph{w.r.t}  are expressed in \eqref{eq:gradw_GW}, and further completed with gradient of the negative regularization term .
	
	The coefficient of the second-order polynom involved in \eqref{eq:linesearchGW} used to solve the problem, are expressed as follow,
	
	
	
	\subsubsection{Dictionary Learning and online algorithm}
	
	Assume now that the dictionary  is not known
	and has to be estimated from the data.
	We define a dataset of  graphs . Recall that each graph  of
	order  is summarized by its pairwise relation matrix  and weights  over nodes.
	The DL problem, that aims at estimating the optimal dictionary
	for a given dataset can be expressed as:
	
	where . We refer the reader to the main paper for the discussion on the non-convex problem \ref{eq:dl}. To tackle this problem we proposed a stochastic algorithm \ref{alg:GW1}
	
	\begin{algorithm}[h]
		\caption{GDL: stochastic update of atoms }
		\label{alg:GW1}
		\begin{algorithmic}[1]
			\STATE Sample a minibatch of graphs  .
			\STATE Compute optimal  by solving B independent unmixing problems with Alg.\ref{alg:BCD1}. 
			\STATE Projected gradient step with estimated gradients  (see \eqref{eq:gradCs_GW}), : \vspace{-2mm}
			
		\end{algorithmic}
	\end{algorithm}
	Estimated gradients \emph{w.r.t}  over a minibatch of graphs  given unmixing solutions  read:
	
	\subsection{GDL for graph with nodes attribute}
	We can also define the same DL procedure for labeled graphs using the FGW distance. The unmixing part defined in \eqref{eq:unmix} can be adapted by considering a linear embedding of the similarity matrix \emph{and} of the feature matrix parametrized by the \emph{same} . 
	\subsubsection{Fused Gromov-Wasserstein unmixing}
	More precisely, given a labeled graph  (see Section \ref{sec:defs} ) and a \emph{dictionary}
	  we want to find a linear representation  of the labeled graph , as faithful as
	possible in the sense of the FGW distance. The FGW unmixing problem that consists in projecting a labeled graph on the linear representation discussed above reads as follow, , 
	
	
	
	where . A similar discussion than for the GW unmixing problem \ref{eq:unmix} holds. We adapt the BCD algorithm detailed in \ref{alg:BCD1} to labeled graphs in Alg.\ref{alg:BCD_fgw}, to solve the non-convex problem of \eqref{eq:unmix_fgw}.
	
	\begin{algorithm}[H]
		\caption{BCD for FGW unmixing problem \ref{eq:unmix_fgw}}
		\label{alg:BCD_fgw}
		\begin{algorithmic}[1]
			\STATE Initialize 
			\REPEAT
			\STATE Compute OT matrix  of , with CG algorithm ~\citep[Alg.1 \& 2]{vayer-fused-2018}.
			\STATE Compute the optimal  solving \eqref{eq:unmix_fgw} for a fixed
			 with CG algorithm \ref{alg:CGw_fgw}.
			\UNTIL{convergence}
		\end{algorithmic}
	\end{algorithm}
	\vspace{-4mm}
	\begin{algorithm}[H]
		\caption{CG for solving FGW unmixing problem \emph{w.r.t}  given }
		\label{alg:CGw_fgw}
		\begin{algorithmic}[1]
			\REPEAT
			\STATE Compute , gradients  \emph{w.r.t}  of \eqref{eq:unmix_fgw} given  following \eqref{eq:gradw_FGW}.
			\STATE Find direction  
			\STATE Line-search: denoting ,
			
			\STATE 
			\UNTIL{convergence}
		\end{algorithmic}
	\end{algorithm}
	
	Partial derivates of the FGW objective  \emph{w.r.t}  are expressed in equations \ref{eq:gradw_GW} and \ref{eq:gradw_FGW}, and further completed with gradient of the negative regularization term.
	
	where .
	The coefficients of the second-order polynom involved in \eqref{eq:linesearchGW} used to solve the problem, satisfy the following equations,
	
	
	\subsubsection{Dictionary Learning and online algorithm}
	
	Assume now that the dictionary  is not known
	and has to be estimated from the data.
	We define a dataset of  labeled graphs . Recall that each labeled graph  of
	order  is summarized by its pairwise relation matrix , its matrix of node features  and weights  over nodes.
	The DL problem, that aims at estimating the optimal dictionary
	for a given dataset can be expressed as:
	
	where . We refer the reader to the main paper for the discussion on the non-convex problem \ref{eq:dl} which can be transposed to problem \ref{eq:dl_fgw}. To tackle this problem we proposed a stochastic algorithm \ref{alg:FGW1}
	
	\begin{algorithm}[h]
		\caption{GDL: stochastic update of atoms }
		\label{alg:FGW1}
		\begin{algorithmic}[1]
			\STATE Sample a minibatch of graphs  .
			\STATE Compute optimal  by solving B independent unmixing problems with Alg.\ref{alg:BCD_fgw}. 
			\STATE Gradients step with estimated gradients  (see \eqref{eq:gradCs_GW}), and  (see \eqref{eq:gradAs_fgw}), . : \vspace{-2mm}
			
			
		\end{algorithmic}
	\end{algorithm}
	Estimated gradients \emph{w.r.t}  and  over a minibatch of graphs  given unmixing solutions  can be computed separately. The ones related to the GW objective are described in \eqref{eq:gradCs_GW}, while the ones related to the Wasserstein objective satisfy \eqref{eq:gradAs_fgw}:
	
	\subsection{Learning the graph structure and nodes distribution}
	Here we extend our GDL model defined in
	equation \ref{eq:dl} and propose to learn atoms of the form . In this setting we have
	two independent dictionaries modeling the relative importance of the nodes with
	, and their pairwise relations through . This
	dictionary learning problem reads:
	
	where  are the structure and distribution embeddings and the linear models are defined as:
	
	Here we exploit fully the GW
	formalism by estimating simultaneously the graph distribution 
	and its geometric structure . Optimization problem \ref{eq:dl_h} can
	be solved by an adaptation of stochastic Algorithm \ref{alg:GW1}. Indeed, in the light of the proposition \ref{grad_prop}, we can derive the following \eqref{eq:unmix_h1} between the input graph  and its embedded representation  and , given an optimal coupling  satisfying Proposition \ref{grad_prop}, 
	
	where  are dual potentials of the induced linear OT problem.
	
	First, with this observation we estimate
	the structure/node weights unmixings
	 for the graph . We proposed the BCD algorithm \ref{alg:BCD_extended} derived from the initial BCD \ref{alg:BCD1}. Note that the dual variables of the induced linear OT problems are centered to ensure numerical stability. 
	
	\begin{algorithm}[H]
		\caption{BCD for extended GW unmixing problem inherent to \eqref{eq:dl_h}}
		\label{alg:BCD_extended}
		\begin{algorithmic}[1]
			\STATE Initialize embeddings such as 
			\REPEAT
			\STATE Compute OT matrix  of , with CG algorithm ~\citep[Alg.1 \& 2]{vayer-fused-2018}.  From the finale iteration of CG, get dual potentials  of the corresponding linear OT problem (see Proposition \ref{grad_prop}).
			\STATE Compute the optimal  by minimizing \eqref{eq:unmix_h1} \emph{w.r.t}  given  with a CG algorithm. 
			\STATE Compute the optimal  solving \eqref{eq:unmix} given 
			 and  with CG algorithm \ref{alg:CGw}. 
			\UNTIL{convergence}
		\end{algorithmic}
	\end{algorithm}
	
	Second, now that we benefit from an algorithm to project any graph  onto the linear representations described in \ref{eq:GDLextended_rpz}, we extend the stochastic algorithm \ref{alg:GW1}. to the problem \ref{eq:dl_h}. This extension is described in algorithm \ref{alg:GW_extended}. 
	
	\begin{algorithm}[h]
		\caption{extended GDL: stochastic update of atoms }
		\label{alg:GW_extended}
		\begin{algorithmic}[1]
			\STATE Sample a minibatch of graphs  .
			\STATE Compute optimal embeddings  coming jointly with the set of OT variables  by solving B independent unmixing problems with Alg.\ref{alg:BCD_extended}. 
			\STATE Projected gradient step with estimated gradients  (see \eqref{eq:gradCs_GW}) and  (see \eqref{eq:gradhs_GW}), : \vspace{-2mm}
			
		\end{algorithmic}
	\end{algorithm}
	For a minibatch a graphs , once each unmixing problems are solved independently estimating unmixings  and the underlying OT matrix  associated with potential , we perform simultaneously a projected gradient step
	update of  and . The estimated gradients of \eqref{eq:dl_h} \emph{w.r.t}  reads ,
	
	
	\subsection{Numerical experiments}
	
	\subsubsection{Datasets}
	\vspace{-4mm}
	\begin{table}[!h]
		\centering
		\caption{Datasets descriptions}
		\label{tab:data}
		\scalebox{0.7}{
			\begin{tabular}{l|r|r|r|r|r|r|r|r}
				\hline
				datasets &  features &  \#graphs & \#classes & mean \#nodes  &  min \#nodes & max \#nodes & median \#nodes & mean connectivity rate\\ \hline
				IMDB-B &     None &  1000   &  2 & 19.77 & 12 & 136 & 17 & 55.53\\ \hline
				IMDB-M &     None & 1500 & 3 & 13.00 &  7 & 89 & 10 & 86.44 \\ \hline
				MUTAG &  & 188 & 2 & 17.93&10 & 28 & 17.5 & 14.79\\ \hline
				PTC-MR&  & 344 & 2 &  14.29& 2 & 64 & 13 & 25.1 \\ \hline
				BZR&  & 405 & 2 & 35.75& 13 & 57 & 35 & 6.70\\ \hline
				COX2&  &467& 2 &41.23& 32& 56 & 41 & 5.24 \\ \hline
				PROTEIN&  & 1113& 2 & 29.06& 4 & 620& 26 & 23.58\\ \hline
				ENZYMES&  & 600 & 6& 32.63 & 2 & 126 & 32 & 17.14\\ \hline
		\end{tabular}}
	\end{table}
	We considered well-known benchmark datasets divided into three categories: i) IMDB-B and IMDB-M~\citep{yanardag-deep-2015} gather graphs without node attributes derived from social networks; ii) graphs with discrete attributes representing chemical compounds from MUTAG~\citep{debnath1991structure} and cuneiform signs from PTC-MR~\citep{krichene2015efficient}; iii) graphs with real vectors as attributes, namely  BZR, COX2~\citep{sutherland2003spline}
	and PROTEINS, ENZYMES~\citep{borgwardt2005shortest}. Details on each dataset are reported in Table \ref{tab:data}
	\subsubsection{Settings}
	In the following, we detail the benchmark of our methods on supervised classification along additional (shared) considerations we made regarding the learning of our models.
	To consistently benchmark methods and configurations, as real graph datasets commonly used in machine learning literature show a high variance considering structure, we perform a nested cross validation (using 9 folds for training, 1 for testing, and reporting the average accuracy of this experiment repeated 10 times) by keeping same folds across methods. All splits are balanced \emph{w.r.t} labels. In following results, parameters of SVM are cross validated within  and .
	
	For our approach, similar dictionaries are considered for unsupervised classification presented in the main paper, than for the supervised classification benchmark detailed in the following. So we refer the reader to the main paper for most implementation details. For completeness, we picked a batch size of 16. We initialized learning rate on the structure  at 0.1. In the presence of node features, we set a learning rate on  of  if  and  otherwise. We optimized our dictionaries without features over 20 epochs and those with features over 40 epochs. In the following,  we denote GDL-w the SVMs derived from embeddings  endowed with the Mahalanobis distance. While GDL-g denotes the SVMs derived from embedded graphs with the (F)GW distance. \citep{xu_gromov-wasserstein_2019} proposed a supervised extension to their Gromov-Wasserstein Factorization (GWF), we refer to GWF-r and GWF-f when the dictionary atoms have random size or when we fix it to match our method. His supervised approach consists in balancing the dictionary objective with a classification loss by plugging a MLP classifier to the unconstrained embedding space. We explicitly regularized the learning procedure by monitoring the accuracy on train splits. Note that in their approach they relaxed constraints of their unmixing problems by applying a softmax on unconstrained embeddings to conduct barycenters estimation. Moreover, they constrain the graph atoms to be non-negative as it enhances numerical stability of their learning procedure. For fair comparisons, we considered this restriction for all dictionaries even if we did not observe any noticeable impact of this hypothesis on our approach. As for unsupervised experiments, we followed their architecture choices. We further validated their regularization coefficient in . Their model converge over 10 epochs for datasets without features, and 20 epochs otherwise.
	
	
	We also considered several kernel based approaches. (FGWK) The kernels  proposed by \citep{vayer-fused-2018}  where pairwise distances are computed using CG algorithms using POT library \citep{flamary2017pot}. To get a grasp of the approximation error from this algorithmic approach, we also applied the MCMC algorithm proposed by  \citep{chowdhury-generalized-2020} to compute FGW distance matrices with a better precision (S-GWK). As the proper graph representations for OT-based methods is still a question of key interest, we consistently benchmarked our approach and these kernels when we consider adjacency and shortest-path representations. Moreover, we experimented on the heat kernels over normalized laplacian matrices suggested by \citep{chowdhury-generalized-2020} on datasets without attributes, where we validated the diffusion parameter . We also reproduced the benchmark
	for classification on Graph Kernels done by \citep{vayer-fused-2018} by keeping their tested parameters for each method. (SPK)
	denotes the shortest path kernel \citep{borgwardt2005shortest}, (RWK) the
	random walk kernel \citep{gartner2003graph}, (WLK) the Weisfeler Lehman kernel
	\citep{vishwanathan2010graph}, (GK) the graphlet count kernel
	\citep{shervashidze09}. For real valued vector attributes, we
	consider the HOPPER kernel (HOPPERK) \citep{feragen2013scalable} and the
	propagation kernel (PROPAK) \citep{neumann2016propagation} . We built upon the
	GraKel library \citep{siglidis2020grakel}  to construct the kernels. 
	
	Finally to compare our performances to recent state-of-the-art models for supervised graph classification, we partly replicated the benchmark done by \citep{xu2018powerful}. We experimented on their best model GIN-0 and the model of \citep{niepert-learning-2016} PSCN. r. For both we used the Adam optimizer \citep{kingma2014adam} with initial learning rate 0.01 and decayed the learning rate by 0.5 every 50 epochs. The number of hidden units is chosen depending on dataset statistics as they propose, batch normalization \citep{ioffe2015batch} was applied on each of them. The batch size was fixed at 128. We fixed a dropout ratio of 0.5 after the dense layer \citep{srivastava2014dropout}. The number of epochs was 150 and the model with the best cross-validation accuracy averaged over the 10 folds was selected at each epoch.
	
	\subsubsection{Results on supervised classification} The accuracies of the nested-cross validation on described datasets are reported in Tables \ref{tab:res1}, \ref{tab:res2}, \ref{tab:res3}. First, we observe as anticipated that the model GIN-0 \citep{xu2018powerful} outperforms most of the time other methods including PSCN, which has been consistently argued in their paper. Moreover, (F)GW kernels over the embedded graphs built thanks to our dictionary approach consistently outperforms (F)GW kernels from input graphs. Hence, it supports that our dictionaries are able to properly denoise and capture discriminant patterns of these graphs, outperforming other models expect GNN on 6 datasets out of 8. The Mahalanobis distance over embeddings  demonstrates satisfying results compared to FGWK relatively to the model simplification it brings. We also observe consistent improvements of the classification performances when we use the MCMC algorithm \citep{chowdhury-generalized-2020} to estimate (F)GW pairwise distance matrices, for all tested graph representations reported. This estimation procedure for (F)GW distances is computationally heavy compared to the usual CG gradient algorithm \citep{vayer-fused-2018}. Hence, we believe that it could bring significant improvements to our dictionary learning models but would increase too consequently the run time of solving unmixing problems required for each dictionary updates. Finally, results over adjacency and shortest path representations interestingly suggest that their suitability \emph{w.r.t} (F)GW distance is correlated to the averaged connectivity rate (see \ref{tab:data}) in different ways depending on the kind of node features. We envision to study these correlations in future works.
	\vspace{-4mm}
	\begin{table}[!h]
		\caption{\textbf{Graphs without attributes:} Classification results of 10-fold nested-cross validation on real datasets. Best results are highlighted in bolt independently of the depicted model category, and the best performances from not end-to-end supervised methods are reported in italic. }
		\label{tab:res1}
		\begin{center}
			\scalebox{0.81}
			{\begin{tabular}{|l|r|r|r|}
					\hline
					category & model & IMDB-B & IMDB-M \\ \hline
					OT (Ours) & GDL-w (ADJ) &70.11(3.13)& 49.01(3.66) \\ 
					& GDL-g (ADJ) & \textit{72.06(4.09)} & \textit{50.64(4.41)} \\
					& GDL-w (SP) & 65.4(3.65) & 48.03(3.80) \\
					& GDL-g (SP) & 68.24(4.38) & 48.47(4.21) \\ \hline 			
					OT & FGWK (ADJ) & 70.8(3.54) &  48.89(3.93)\\  
					& FGWK (SP) & 65.0(3.69)& 47.8(3.84)\\
					& FGWK (heatLAP) & 67.7(2.76)& 48.11(3.96) \\
					& S-GWK (ADJ) & 71.95(3.87) & 49.97(3.95)\\
					& S-GWK (heatLAP) & 71.05(3.02) & 49.24(3.49)\\
					& GWF-r (ADJ) & 65.08(2.85) & 47.53(3.16)\\
					& GWF-f (ADJ) & 64.68(2.27) & 47.19(2.96) \\ \hline
					Kernels & GK (K=3) & 57.11(3.49) & 41.85(4.52) \\
					& SPK & 56.18(2.87) & 39.07(4.89) \\ \hline
					
					GNN & PSCN & 71.23(2.13)& 45.7(2.71)\\
					& GIN-0 & \textbf{74.7(4.98)}& \textbf{52.19(2.71)}\\ \hline
			\end{tabular}}
		\end{center}
	\end{table}
	\vspace{-5mm}
	\begin{table}[!h]
		\centering
		\caption{\textbf{Graphs with discrete attributes :} Classification results of 10-fold nested-cross validation on real datasets with discrete attributes (one-hot encoded). Best results are highlighted in bolt independently of the depicted model category, and the best performances from not end-to-end methods are reported in italic.}
		\label{tab:res2}
		\scalebox{0.81}
		{\begin{tabular}{|l|r|r|r|}
				\hline
				category & model & MUTAG &PTC-MR \\ \hline
				OT (Ours) & GDL-w (ADJ) & 81.07(7.81) & 55.26(8.01)\\
				& GDL-g (ADJ) & 85.84(6.86) & 58.45(7.73)\\
				& GDL-w (SP)  & 84.58(6.70)& 55.13(6.03)\\
				& GDL-g (SP)  & \textit{87.09(6.34)}& 57.09(6.59)\\ \hline
				OT & FGWK (ADJ)  & 82.63(7.16)& 56.17(8.85) \\  
				& FGWK (SP)  & 84.42(7.29)& 55.4(6.97)\\
				& S-GWK (ADJ)  & 84.08(6.93)& 57.89(7.54)\\
				& GWF-r (ADJ)  & -& -\\
				& GWF-f (ADJ)  & -& -\\ \hline
				Kernels & GK (K=3)  & 82.86(7.93)& 57.11(7.24) \\
				& SPK &  83.29(8.01)&  60.55(6.43)\\
				& RWK & 79.53(7.85)& 55.71(6.86)\\
				& WLK & 86.44(7.95)& \textit{63.14(6.59)}\\ \hline
				GNN & PSCN & \textbf{91.4(4.41)} & 58.9(5.12)\\
				& GIN-0  & 88.95(4.91)& \textbf{64.12(6.83)}\\ \hline
		\end{tabular}}
	\end{table}
	\vspace{-5mm}
	\begin{table}[!h]
		\centering
		\caption{\textbf{Graphs with vectorial attributes:} Classification results of 10-fold nested-cross validation on real datasets with vectorial features. Best results are highlighted in bolt independently of the depicted model category, and the best performances from not end-to-end supervised methods are reported in italic.}
		\label{tab:res3}
		\scalebox{0.81}
		{\begin{tabular}{|l|r|r|r|r|r|r|}
				\hline
				category & model & BZR & COX2 & ENZYMES & PROTEIN \\  \hline
				OT (ours) & GDL-w (ADJ)& 87.32(3.58)& 76.59(3.18) & 70.68(3.36) & 72.13(3.14)\\ 
				& GDL-g (ADJ) &  \textit{87.81(4.31)}& 78.11(5.13) & 71.44(4.19) & 74.59(4.95)\\ 
				& GDL-w (SP) &  83.96(5.51)& 75.9(3.81) & 69.95(5.01) & 72.95(3.68)\\ 
				& GDL-g (SP) &  84.61(5.89)& 76.86(4.91) & 71.47(5.98)& \textit{74.86(4.38)}\\ \hline
				OT& FGWK (ADJ) & 85.61(5.17) & 77.02(4.16) & 72.17(3.95) & 72.41(4.70)\\
				& FGWK (SP) & 84.15(6.39) & 76.53(4.68) & 70.53(6.21)& 74.34(3.27)\\
				&S-GWK (ADJ)& 86.91(5.49) & 77.85(4.35) & \textbf{73.03(3.84)} & 73.51(4.96)\\
				& GWF-r (ADJ) & 83.61(4.96) & 75.33(4.18) & 72.53(5.39)& 73.64(2.48)\\
				& GWF-f (ADJ) & 83.72(5.11) & 74.96(4.0) & 72.14(4.97)& 73.06(2.06)\\
				\hline
				
				Kernels & HOPPERK& 84.51(5.22) & \textit{79.68(3.48)}& 46.2(3.75) & 72.07(3.06)\\
				& PROPAK& 80.01(5.11) & 77.81(3.84)& 71.84(5.80)& 61.73(4.5)\\ \hline
				GNN & PSCN& 83.91(5.71) & 75.21(3.29)& 43.89(3.91) & 74.96(2.71)\\
				& GIN-0& \textbf{88.71(5.48)} & \textbf{81.13(4.51)}& 68.6(3.69) & \textbf{76.31(2.94)}\\ \hline
		\end{tabular}}
	\end{table}
	\newpage
	\subsubsection{Complementary results on unsupervised classification}
	\paragraph{vanilla GDL} As mentioned in section 4 of the main paper, we considered a fixed batch size for learning our models on labeled graphs, which turned out to be a limitation for the dataset ENZYMES. We report in table \ref{tab:clustering_enzymes} our models performance on this dataset for a batch size fixed to 64 instead of 32 within the framework detailed above. These results are consistent with those observed on the other datasets.
	\begin{table*}[h!]
		\vspace*{-5mm}
		\caption{Clustering : dataset ENZYMES}
		\label{tab:clustering_enzymes}
		\begin{center}
			\scalebox{0.81}{
				\begin{tabular}{|c|c|}
					\hline
					MODELS & ENZYMES \\
					\hline
					GDL &  71.83(0.18)\\
					& \\ 
					\hline
			\end{tabular}}
		\end{center}
	\end{table*}
	
	\vspace*{-5mm}
	\paragraph{extended version of GDL} We report here a companion study for clustering tasks which further supports our extension of GDL to the learning of node weights. As there is no Mahalanobis upper-bound for the linear models learned with this extension as their node weights are a priori different, we compare performances of K-means with GW distance applied on the embedded graphs produced with vanilla GDL, the extended version of GDL denoted here  and GWF. Similar considerations have been made for learning  than those detailed for GDL, and we completed these results with an ablation of the quadratic negative regularization parameterized by . Results provided in \ref{tab:clustering_GWkmeans} show that GW Kmeans applied to the graph representations from our method  leads to state-of-the-art performances.
	
\begin{table}[h!]
		\vspace*{-5mm}
		\caption{Clustering: RI from GW Kmeans on embedded graphs.}
		\label{tab:clustering_GWkmeans}
		\begin{center}
			\scalebox{0.81}{
				\begin{tabular}{|l|c|c|c|}
					\hline
					models &  & IMDB-B & IMDB-M\\
					\hline
					GDL \text{ } (ours)&  0 & 51.54(0.29)& 55.86(0.25) \\
					&  & 51.97(0.48) & 56.41(0.35) \\ \hline
GDL  (ours)& 0& 52.51(0.22) &  \\ 
					& &  & 56.95(0.25) \\
					\hline 
					GWF-r & NA &51.39(0.15) & 55.80(0.21)\\
					GWF-f & NA &50.93(0.39) & 54.48(0.26)\\
					\hline
			\end{tabular}}\end{center}
	\end{table}
	
	\subsubsection{Runtimes}
	We report in Table \ref{warp-tab:runtimes} averaged runtimes for the same relative precision of  to compute one graph embedding on learned dictionaries from real datasets.
	\begin{table}[h!]\vspace*{-5mm}
		\caption{Averaged runtimes.}
		\label{warp-tab:runtimes}
		\begin{center}
			\scalebox{0.8}{
				\begin{tabular}{|c|c|c|c|}
					\hline
					dataset & \# atoms & GDL  &  GWF \\
					\hline
					IMDB-B & 12 & 52 ms & 123 ms\\
					& 16 & 69 ms & 186 ms \\ \hline
					IMDB-M & 12 & 44 ms & 101 ms\\
					& 18 & 71 ms & 168 ms \\ \hline
			\end{tabular}}
		\end{center}
	\end{table}
\end{document}
