\documentclass[sigconf,nonacm]{acmart}


\usepackage{graphicx}
\usepackage{multirow}
\usepackage[figuresright]{rotating}

\usepackage{lscape}

\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\usepackage{balance}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}








\begin{document}

\title{Entity Matching using Large Language Models}

\author{Ralph Peeters}
\orcid{0000-0003-3174-2616}
\affiliation{\institution{Data and Web Science Group}
  \institution{University of Mannheim}
  \streetaddress{B6, 26}
  \city{Mannheim}
  \country{Germany}
  \postcode{68159}
}
\email{ralph.peeters@uni-mannheim.de}

\author{Christian Bizer}
\orcid{0000-0003-2367-0237}
\affiliation{\institution{Data and Web Science Group}
  \institution{University of Mannheim}
  \streetaddress{B6, 26}
  \city{Mannheim}
  \country{Germany}
  \postcode{68159}
}
\email{christian.bizer@uni-mannheim.de}

\renewcommand{\shortauthors}{Peeters and Bizer}



\begin{abstract}
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a scenario where task-specific training data is available. We compare different prompt designs as well as the prompt sensitivity of the models in the zero-shot scenario. We investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenario using the same pool of training data across the different approaches. Our experiments show that GPT4 without any task-specific training data outperforms fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets reaching F1 scores around 90\%. The experiments with in-context learning and rule generation show that all models beside of GPT4 benefit from these techniques (on average 5.9\% and 2.2\% F1), while GPT4 does not need such additional guidance in most cases. Finally, fine-tuning GPT3.5 leads to a 3\% higher performance than the best performing GPT4 model/prompt combination on three out of five datasets with comparable performance on the remaining two.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10002952.10003219.10003223</concept_id>
       <concept_desc>Information systems~Entity resolution</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Entity resolution}


\keywords{Entity Matching, Linked Data, Large Language Models}


\maketitle

\section{Introduction}
\label{sec:introduction}

Entity matching~\cite{,BarlaugNeural2021,Christen2012DataMC,elmagarmidDuplicateRecordDetection2007} is the task of discovering entity descriptions in different data sources that refer to the same real-world entity. Entity matching is a central step in data integration pipelines~\cite{christophides_end--end_2020} and forms the foundation of interlinking data on the Web~\cite{LinkDiscoverySurvey2017}. Application domains of entity matching include e-commerce, where offers from different vendors are matched for example for price tracking, and financial data integration, where information about companies from different sources is combined. While early matching systems relied on manually defined matching rules, supervised machine learning methods have become the foundation of entity matching systems~\cite{christophides_end--end_2020} today. This trend was reinforced by the success of neural networks~\cite{BarlaugNeural2021} and today most state-of-the art matching systems rely on pre-trained language models (PLMs), such as BERT or RoBERTa~\cite{liDeepEntityMatching2020,peetersSupervisedContrastiveLearning2022a,peeters2023wdc}.
The major drawbacks of using PLMs for entity matching are that (i) PLMs need a lot of task-specific training examples for fine-tuning and (ii) they are not very robust concerning unseen entities that were not part of the training data~\cite{akbarian2022probing,peeters2023wdc}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/data/prompt.png}
  \caption{Process of combining two entity descriptions into a prompt which is afterwards passed to the LLM.}
  \Description{Process of combining two entity descriptions into a prompt which is afterwards passed to the LLM.}
  \label{fig:prompt}
\end{figure} 
Large autoregressive language models (LLMs)~\cite{zhao2023survey} such as GPT, PaLM, Llama, and BLOOM have the potential to address both of these shortcomings. Due to being pre-trained on huge amounts of textual data as well as due to emergent effects resulting from the model size~\cite{wei2022emergent}, LLMs often have a better zero-shot performance compared to PLMs and are also more robust concerning unseen examples~\cite{brown2020language}. 

In this paper, we investigate using LLMs for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Figure \ref{fig:prompt} shows an example how LLMs can be used for entity matching. The two entity descriptions at the bottom of the figure are combined with the question whether they refer to the same real-word entity into a prompt. The prompt is afterwards passed to the LLM, which generates the answer shown at the top of Figure \ref{fig:prompt}. 
Our study covers hosted LLMs, such as GPT3.5 and GPT4,
as well as open source LLMs based on Llama2 which can be run
locally. We evaluate these models in a zero-shot scenario as well as a
scenario where task-specific training data is available and can be exploited for selecting demonstrations, generating matching rules, or fine-tuning the LLM. 

\textbf{Contributions:} This paper makes the following contributions:

\begin{enumerate}
    \item We compare the performance as well as the prompt sensitivity of the hosted LLMs GPT3.5 and GPT4 and the open source LLMs SOLAR and StableBeluga2 for the task of entity matching using different zero-shot prompt designs on five benchmark datasets covering the use cases product matching and bibliographic reference matching. We show that some models are less sensitive to variations of the prompts resulting in a high and rather stable performance. We further show that there is no single best prompt design for all models and identify for each model two prompt designs that generally work better than others across the datasets.   
    \item We compare the performance of the LLMs in the zero-shot scenario to the performance of two PLM-based matchers (RoBERTa and Ditto) which were fine-tuned using task-specific training data. The comparison shows that zero-shot GPT4 outperforms the fine-tuned PLMs on three out of five datasets by approximately 4\% F1. If the PLM-based matchers are tested with unseen, out-of-distribution entities, the gap between GPT4 and the PLMs widens to 34-57\% F1. As a result, LLMs do not face the two main shortcomings of PLMs, the need for large amounts of task-specific training data and poor generalization to out-of-distribution entities.
\item We analyze the impact of adding task-specific training data in the form of  demonstrations to the prompts using three selection techniques. We also investigate adding handwritten as well as learned matching rules to the prompts. We show that these techniques result in an average improvement of 5.9\% (in-context) and 2.2\% (rules) F1. In-context demonstrations generally leading to larger performance gains than the provision of rules.
    \item We fine-tune the GPT3.5 model using task-specific training data and show that the fine-tuning allows the model to reach (on two datasets) and even exceed (on the remaining three) the performance of GPT4 in zero-shot configuration. The fine-tuned model also shows a good generalization performance to other datasets in most configurations.
\end{enumerate}

\textbf{Structure:} The paper is structured as follows: Section~\ref{sec:experimentalsetup} introduces the experimental setup as well as the profile of the data sets that we use for the experiments. Section~\ref{sec:zeroshot} compares different prompt designs and different LLMs in the zero-shot setting, while Section~\ref{subsec:incontext} investigates whether the model performance can be improved by providing demonstrations for in-context learning. In Section~\ref{subsec:rules}, we show that adding higher-level matching knowledge in the form of natural language rules can be an alternative to in-context learning for improving model performance. Section~\ref{subsec:finetune} compares using the same training data to fine-tune an LLM with using the data to select demonstrations or learn rules.
Section~\ref{sec:relatedwork} compares our results to the related work, while Section~\ref{sec:conclusion} concludes the paper and summarizes the main findings. 

\textbf{Replicability:} All data and code used for the experiments presented in this paper is made publicly available\footnote{\url{https://github.com/wbsg-uni-mannheim/MatchGPT}} so that all experiments can be replicated. \section{Experimental Setup}
\label{sec:experimentalsetup}

This section provides details about the large language models, the benchmark datasets, the serialization of entity descriptions, and the evaluation metrics that are used in the experiments.

\textbf{Large Language Models:} We compare three hosted LLMs from OpenAI and two open source LLMs that are available for download and can be run on local machines. As hosted LLMs are only accessible via APIs, users are subject to the decisions of the hosting party which could for example lead to deprecation and removal of models or sudden increases in pricing that may make the usage infeasible to some users. As we show in Section \ref{sec:zeroshot} using the example of GPT3.5 in the versions from March and June of 2023, a later model may perform significantly worse on the task than its predecessor which is an uncertainty many users may want to avoid. Open source LLMs on the other hand, while requiring multiple high-performance GPUs to run efficiently depending on model size, can be run on local machines avoiding the dependence on the owner of the API and hosted models.
We compare the following LLMs in the experiments:

\begin{itemize}
    \item \textbf{gpt3.5-turbo-0301 (Turbo03):} This LLM has a size of 175B parameters and was used by OpenAI as the model behind ChatGPT in March 2023. The model is still available via the OpenAI API.
    \item \textbf{gpt3.5-turbo-0613 (Turbo06):} This model was released by OpenAI in June 2023. It is an updated version of the March model, also having a size of 175B parameters and being available on the OpenAI API.
    \item \textbf{gpt4-0613 (GPT4):} The largest LLM in this study is OpenAI's GPT4 which is said to have a size of 1.76T parameters. We use the June 2023 version of the model which is available via the OpenAI API.
    \item \textbf{SOLAR-0-70b-16bit (SOLAR):} SOLAR is one of the top-ranked open source LLMs on the huggingface leaderboard at the time of writing (72.95 average score). It is developed by upstage\footnote{\url{https://en.upstage.ai/}} and based on the 70B parameter version of Llama2~\cite{touvron2023llama} as backbone model. The authors state that this model is fine-tuned on Orca-~\cite{mukherjee2023orca} and Alpaca-style\footnote{\url{https://github.com/tatsu-lab/stanford_alpaca}} datasets, which are not further disclosed.
    \item \textbf{StableBeluga2 (Beluga2):} Beluga2 is another high-ranking open source LLM based on the 70B version of Llama2. It is developed by StabilityAI\footnote{\url{https://stability.ai/}} and reaches an average score of 71.42 on the huggingface leaderboard. The model is fine-tuned using an undisclosed Orca-style dataset.
\end{itemize}


The following sections and tables use the model naming scheme introduced above for brevity. To ensure replicability of the experiments, we set the temperature parameter to 0 for all LLMs to avoid randomness in the answer generation process. We use the langchain\footnote{\url{https://www.langchain.com/}} library for interaction with the OpenAI API as well as for template-based prompt generation.

\textbf{PLM Baselines:} We compare the performance of the LLMs to two PLM-based matching baselines, namely a fine-tuned RoBERTa model and the entity matching system Ditto:

\begin{itemize}
\item\textbf{RoBERTa:} The RoBERTa-base~\cite{liu_roberta_2019} model is the first baseline representing a PLM-based language model which has been shown to reach high performance on the entity matching task~\cite{liDeepEntityMatching2020,peetersDualobjectiveFinetuningBERT2021}.
We fine-tune the RoBERTa model for entity matching on the respective development sets for 50 epochs with early stopping after 10 epochs if the validation score does not improve. The batch size is set to 64 and the learning rate is linearly decreasing with warmup with a maximum value of 5e-5.
\item\textbf{Ditto:} The Ditto~\cite{liDeepEntityMatching2020} matching system is one of the first dedicated entity matching systems using PLMs. Ditto introduces various data augmentation and domain knowledge injection modules that can be added for model training. For our experiments, we use a RoBERTa-base language model and activate the data augmentation modules as proposed in the original paper where available. For WDC Products we apply the \textit{delete} operator. We fine-tune the Ditto model for 50 epochs with a batch size of 64 and a learning rate of 5e-5 on a linearly decreasing schedule with warmup.  
\end{itemize}

\textbf{Benchmark Datasets:}  We use five benchmark datasets to analyze the potential of LLMs for entity matching. Specifically, we evaluate on the recent e-commerce dataset WDC Products~\cite{peeters2023wdc}, which was created using schema.org annotations from thousands of e-shops on the Web as well as four widely-used but older entity matching benchmarks~\cite{kopckeEvaluationEntityResolution2010b}, three from the product domain and one from the bibliographical domain:

\begin{itemize}
    \item \textbf{WDC Products:} The WDC Products benchmark contains product offers spanning various product categories like electronics, clothing and tools for home improvement from thousands of different e-shops from the Web. The benchmark was built by extracting schema.org data from the Common Crawl. We use the most difficult version of the benchmark including 80\% corner-cases (hard positives and hard negatives) and use the following attributes: \textit{brand}, \textit{title}, \textit{currency} and \textit{price}.
    \item \textbf{Abt-Buy}: This benchmark dataset contains product offers from e-commerce vendors Abt.com and Buy.com that need to be matched. The products are from similar categories as those in WDC Products. It represents a less structured dataset as most attributes are highly textual. We use the attributes \textit{title} and \textit{price} in our experiments.
    \item \textbf{Walmart-Amazon:} The Walmart-Amazon benchmark represents a structured matching task in the product domain, specifically matching product offers from the Walmart product catalogue to the Amazon product catalogue. The types of products in this dataset are similar to WDC Products and Abt-Buy. We use the attributes \textit{brand}, \textit{title}, \textit{modelno}, \textit{price} in our experiments.
    \item \textbf{Amazon-Google:} While the WDC Products, Abt-Buy and Walmart-Amazon datasets contain tangible products from different product categories, the Amazon-Google dataset contains mainly offers for software products, e.g. different versions of the Windows operating system or image/video editing applications. We use the attributes \textit{brand}, \textit{title} and \textit{price} from this dataset.
    \item \textbf{DBLP-Scholar:} The task of this benchmark dataset is to match bibliographic entries from DBLP and Google Scholar. It represents an entity matching task from a very different domain compared to the other datasets. We use the following attributes in our experiments: \textit{authors}, \textit{title}, \textit{venue} and \textit{year}. 
\end{itemize}

\begin{table}[]
\centering
\caption{Statistics for the development sets and test sets for all datasets. In-context example selection and fine-tuning is performed on the training part of the development split while prompts are evaluated on the test splits.}
\label{tab:datasets}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
Dataset              & \multicolumn{3}{c|}{Development Set} & \multicolumn{3}{c}{Test Set} \\ \midrule
                     & \# Pairs     & \# Pos    & \# Neg    & \# Pairs  & \# Pos  & \# Neg \\ \midrule
Abt-Buy              & 7,659        & 822       & 6,837     & 1,206     & 206     & 1,000  \\
Amazon-Google        & 9,167        & 933       & 8,234     & 1,234     & 234     & 1,000  \\
DBLP-Scholar         & 22,965       & 4,277     & 18,688    & 1,250     & 250     & 1,000  \\
Walmart-Amazon       & 8,193        & 769       & 7,424     & 1,193     & 193     & 1,000  \\
WDC Products (80/20) & 5,000        & 1,000     & 4,000     & 1,239     & 259     & 989    \\ \bottomrule
\end{tabular}}
\end{table} 
For WDC Products, we use the training/validation/test split of size small.  For the other benchmark datasets, we use the splits established in the DeepMatcher paper~\cite{mudgalDeepLearningEntity2018}. As we perform a large number of experiments against the OpenAI API and as these experiments, especially for long prompts and for using the GPT4 model, result in relevant API usage fees, we down-sample all test sets to approximately 1250 entity pairs. Table \ref{tab:datasets} provides statistics about the numbers of positive (matches) and negative (non-matches) pairs in the development- and test sets of all benchmarks used in the experiments.

\textbf{Serialization:} For the serialization of product offer pairs into prompts, we serialize each offer as a string by concatenating attribute values but omitting the attribute names. The order of attributes for the serialization is the order shown in the dataset listing above. Figure \ref{fig:prompt} shows an example of this serialization practice for a pair of product offers. 

\textbf{Evaluation:} The responses gathered from the models are natural language text. In order to decide if a response refers to a positive matching decision regarding a pair of product offers, we apply simple pre-processing to the answer and subsequently parse for the word \textit{yes}. In any other case we assume the model decides on not matching. This rather simple approach turns out to be surprisingly effective as shown by Narayan et al.~\cite{foundationalWrangleVLDB2022}.  \section{Scenario 1: Zero-shot Prompting}
\label{sec:zeroshot}

In the first scenario, we analyze the impact of different prompt designs on the entity matching performance of the LLMs. Small variations in the prompts can have a large impact on the overall task performance~\cite{foundationalWrangleVLDB2022,zhao2021calibrate,promptingSurvey2023}. The extent of this \textit{prompt sensitivity} depends on the model. In this section, we will investigate the prompt sensitivity of the different models for the entity matching task and will identify the prompt designs that work best for each model. Afterwards, we will compare the performance of the LLMs to the PLM baselines.

\textbf{Prompt Building Blocks:} We construct prompts as a combination of smaller building blocks in order to allow the systematic evaluation of different prompt designs. Each prompt consists at least of a task description and the serialization of the pair of entity descriptions to be matched. In addition, the prompts may contain a specification of the output format as well as additional domain knowledge that might be helpful for performing the task. We evaluate for alternative task descriptions which formulate the task as a question using simple or complex wording combined with domain-specific or general terms. The alternative task descriptions are listed below: 
\begin{itemize}
    \item \textbf{domain-simple:} "Do the two product descriptions match?" / "Do the two publications match?"
    \item \textbf{domain-complex:} "Do the two product descriptions refer to the same real-world product?" / "Do the two publications refer to the same real-world publication?"
    \item \textbf{general-simple:} "Do the two entity descriptions match?"
    \item \textbf{general-complex:} "Do the two entity descriptions refer to the same real-world entity?"
\end{itemize}
The task description may be followed by a specification of the output format. We evaluate two formats: \textit{free} which does not restrict the answer of the LLM and \textit{force} which instructs the LLM to "Answer with 'Yes' if they do and 'No' if they do not". The prompt continues with the entity pair to be matched, serialized as discussed in Section \ref{sec:experimentalsetup}. Figure~\ref{fig:prompt} contains an example of a complete prompt implementing the prompt design \textit{general-complex-free}. Examples for all prompt designs are found in the repository accompanying this paper.
 In addition to the prompts we generate using these building blocks, we also evaluate the prompt designs proposed by Narayan et al.~\cite{foundationalWrangleVLDB2022}. The main difference of these designs compared to ours is that they put the serialization of the entities before the task description while we order the task description and the serialization the other way around.  
 


\textbf{Effectiveness:} Table \ref{tab:zero-shot-1} shows the results of our zero-shot experiments. With regards to overall performance, the GPT4 model outperforms all other LLMs on all datasets by at least 5\% F1 achieving an absolute performance of around 90\% on 4 of 5 datasets without requiring any task-specific training data.  When comparing the GPT3.5 versions, the best prompt of the more recent version (Turbo06) performs 5\% F1 worse than the best prompt of the earlier version across 3 of the 5 datasets, indicating that the changes made to the model in the newer version have degraded the performance on the entity matching task. Comparing the open source LLMs, the SOLAR model achieves 5-10\% higher F1 scores than Beluga2 across all datasets. SOLAR is able to surpass the best GPT3.5 result by 2 and 9\% F1 on two datasets, while achieving a similar result on DBLP-Scholar and falling behind by 7 and 9\% on the remaining two datasets. This result indicates that locally run open source LLMs can be a viable alternative to hosted LLMs such as OpenAI's GPT3.5 models. If maximum performance is required, none of the LLMs can reach the performance level of GPT4 in the zero-shot setting.

\textbf{Sensitivity:} We measure the prompt sensitivity of a LLM as the standard deviation (SD) of the F1 scores over all 10 prompt designs. We list this standard deviation in the lower section of Table~\ref{tab:zero-shot-1}. Comparing the prompt sensitivity of the models, the GPT4 model is most invariant to the wording of the prompt (mean-SD 2.82) and achieves high results with most of the prompt designs. When comparing the sensitivity of the GPT3.5 models, it becomes clear that not only the maximum performance of the newer version has suffered but also the prompt sensitivity has increased significantly over all datasets. For the open source LLMs, the better performing SOLAR has the highest prompt sensitivity among all models tested (mean-SD 16.18), essentially making a thorough prompt search a necessity. The Beluga2 model is much less sensitive to the prompt design resulting in a similar mean F1 performance in the zero-shot scenario. 

\textbf{Prompt to Model Fit:} The best result for each model is set bold in Table \ref{tab:zero-shot-1}, the second best result is underlined. This highlighting shows that there is no prompt design that performs best for most models. Taking the mean F1 scores as basis, the prompt design \textit{domain-complex} performs best for the GPT4, SOLAR, and Beluga2 models, while the prompt design \textit{domain-simple} works best for the GPT-Turbo models. Looking at the best performing prompts for each dataset, we observe the pattern that for each model two prompt designs appear in the top two prompt designs more often than the others. This suggests that while there is no overall best prompt design that works for all models across all datasets, there are two prompt designs that consistently produce good results across all datasets for the same model. 



\textbf{Comparison to PLM Baselines:} We compare the zero-shot performance of the LLMs to the performance of two PLM-based matchers: a fine-tuned RoBERTa model~\cite{liu_roberta_2019} and Ditto~\cite{liDeepEntityMatching2020}, a state of the art entity matching system which also relies on domain-specific training data. Table \ref{tab:baseline-vs-zero-shot} shows the overall best results for each LLM in comparison to the two PLM-based matchers on all datasets. For three out of the five datasets, GPT4 achieves higher performance than the best PLM baseline (2.65-4.71\% F1), while the performance for the other two datasets is 3.69 and 4.49\% F1 lower. This shows that GPT4 without using any task-specific training data is able to reach comparable results or even outperform PLMs that were fine-tuned using thousands of training pairs (see Table~\ref{tab:datasets}). The reliance on large amounts of task-specific training data to achieve good performance is one of the main shortcomings of PLMs as mentioned before. LLMs do not have this shortcoming due to their strong zero-shot performance that rivals or exceeds the performance of fine-tuned PLMs.

\textbf{Generalization:} A shortcoming of PLM-based matchers is their low robustness to out-of-distribution entities, e.g. entities that are not part of any entity pair in the training set~\cite{akbarian2022probing,peeters2023wdc}. In another set of experiments, we fine-tune RoBERTa and Ditto models using the A-B, W-A, A-G, D-S training sets. Afterwards, we test the fine-tuned models using the WDC Products test set, which contains different products that are thus unseen to the fine-tuned models. We report the results of these experiments in the "Unseen" rows at the bottom of Table \ref{tab:baseline-vs-zero-shot}. 
Compared to fine-tuning directly on the WDC Products development set (84.90\% F1 for Ditto), the transfer of fine-tuned models leads to large drops in performance ranging from 36 to 53\% F1 for Ditto and 22 to 47\% F1 for RoBERTa. StableBeluga2, which has the worst zero-shot performance on WDC Products among all LLMs, still performs better by 8\% F1 when compared to the best transferred PLM-based matcher and GPT4 outperforms it by 34\%. These results show that LLMs do not face the problem of low robustness on out-of-distribution entities that PLM-based matchers exhibit as a result of their fine-tuning.


\begin{table}[]
\centering
\caption{Comparison of F1 scores of the best zero-shot prompt per model with fine-tuned baselines on all datasets. Best result is bold, second best is underlined. The "Unseen" rows correspond to training on the respective dataset named in the column and applying the model to the WDC Products test set.}
\label{tab:baseline-vs-zero-shot}
\resizebox{0.9\columnwidth}{!}{\begin{tabular}{@{}lccccc@{}}
\toprule
Model                                                         & WDC                         & A-B                           & W-A                           & A-G                           & D-S                           \\ \midrule
Turbo03                                                       & 79.70                       & 87.39                         & 74.81                         & 63.72                         & 84.13                         \\
Turbo06                                                       & 74.96                       & 83.83                         & 69.25                         & 63.50                         & 83.04                         \\
GPT4                                                          & \textbf{89.61}              & \textbf{95.78}                & \textbf{89.67}                & 76.38                         & 89.82                         \\ \midrule
SOLAR                                                         & 72.95                       & 89.20                         & 83.11                         & 54.69                         & 84.91                         \\
Beluga2                                                       & 63.61                       & 85.79                         & 74.47                         & 49.59                         & 75.53                         \\ \midrule
RoBERTa                                                       & 77.53                       & 91.21                         & {\ul 87.02}                   & {\ul 79.27}                   & {\ul 93.88}                   \\
Ditto                                                         & {\ul 84.90}                 & {\ul 91.31}                   & 86.39                         & \textbf{80.07}                & \textbf{94.31}                \\ \midrule
\begin{tabular}[c]{@{}l@{}}$\Delta$ best \\ LLM/PLM\end{tabular} & {\color[HTML]{6434FC} 4.71} & {\color[HTML]{6434FC} 4.47}   & {\color[HTML]{6434FC} 2.65}   & {\color[HTML]{FE0000} -3.69}  & {\color[HTML]{FE0000} -4.49}  \\ \midrule
RoBERTa\textsubscript{Unseen}                                                & -                           & 55.52                         & 36.46                         & 31.00                         & 29.64                         \\
Ditto\textsubscript{Unseen}                                                 & -                           & 48.74                         & 31.55                         & 33.12                         & 32.82                         \\ \midrule
$\Delta$ RoBERTa\textsubscript{Unseen}                                          & -                           & {\color[HTML]{FE0000} -22.01} & {\color[HTML]{FE0000} -41.07} & {\color[HTML]{FE0000} -46.53} & {\color[HTML]{FE0000} -47.89} \\
$\Delta$ Ditto\textsubscript{Unseen}                                            & -                           & {\color[HTML]{FE0000} -36.16} & {\color[HTML]{FE0000} -53.35} & {\color[HTML]{FE0000} -51.78} & {\color[HTML]{FE0000} -52.08} \\ \bottomrule
\end{tabular}}
\end{table} \begin{table*}[]
\centering
\caption{Results of the zero-shot experiments for all LLMs and datasets. The first block presents mean F1 scores over all datasets, while the following blocks contain F1 scores for individual datasets. Best results are set bold, second best are underlined. }
\label{tab:zero-shot-1}
\resizebox{1.9\columnwidth}{!}{\begin{tabular}{@{}l|ccccc|ccccc|ccccc@{}}
\toprule
Prompt                & \multicolumn{5}{c|}{All Datasets (Mean F1)}                                           & \multicolumn{5}{c|}{WDC Products}                                                  & \multicolumn{5}{c}{Abt-Buy}                                                        \\ \midrule
                      & Turbo03        & Turbo06        & GPT4           & SOLAR          & Beluga2        & Turbo03        & Turbo06        & GPT4           & SOLAR          & Beluga2        & Turbo03        & Turbo06        & GPT4           & SOLAR          & Beluga2        \\ \midrule
domain-complex-force  & 72.15          & 69.32          & {\ul 87.31}    & 69.30          & \textbf{68.30} & 75.55          & \textbf{74.96} & {\ul 88.35}    & 67.93          & \textbf{63.61} & 76.48          & 67.11          & {\ul 95.15}    & 87.56          & {\ul 84.10}    \\
domain-complex-free   & 64.49          & 57.98          & \textbf{88.01} & \textbf{75.98} & {\ul 66.36}    & 68.66          & 64.93          & \textbf{89.61} & \textbf{72.95} & {\ul 54.97}    & 66.34          & 50.56          & \textbf{95.78} & {\ul 88.42}    & \textbf{85.79} \\
domain-simple-force   & \textbf{77.47} & 60.34          & 83.72          & 41.74          & 61.99          & {\ul 79.17}    & 38.24          & 83.72          & 26.71          & 44.19          & 86.03          & 81.11          & 93.56          & 66.45          & 79.36          \\
domain-simple-free    & 70.06          & \textbf{73.27} & 86.05          & 65.47          & 57.12          & 75.17          & {\ul 72.52}    & 84.50          & 53.44          & 43.79          & 73.66          & 81.82          & 94.38          & 79.22          & 75.90          \\
general-complex-force & 70.43          & 66.79          & 86.41          & 55.59          & 63.33          & 76.51          & 60.62          & 85.83          & 56.52          & 54.97          & 74.32          & {\ul 82.30}    & 94.40          & 85.04          & 83.51          \\
general-complex-free  & 58.82          & 60.89          & 86.58          & {\ul 73.70}    & 62.12          & 65.87          & 67.83          & 86.72          & {\ul 71.98}    & 51.38          & 61.47          & 55.86          & 94.87          & \textbf{89.20} & 84.07          \\
general-simple-force  & {\ul 74.40}    & 39.41          & 77.78          & 21.77          & 54.57          & 78.33          & 14.02          & 77.39          & 11.28          & 40.00          & \textbf{87.39} & 69.07          & 93.23          & 52.30          & 77.47          \\
general-simple-free   & 73.36          & {\ul 70.37}    & 82.58          & 43.50          & 47.66          & \textbf{79.70} & 69.71          & 83.41          & 31.02          & 30.16          & 83.30          & \textbf{83.83} & 92.77          & 72.73          & 73.41          \\
Narayan-simple        & 70.34          & 51.57          & 84.62          & 57.37          & 50.51          & 73.02          & 50.29          & 81.91          & 57.22          & 45.73          & {\ul 86.39}    & 78.61          & 92.42          & 75.28          & 68.66          \\
Narayan-complex       & 63.88          & 54.86          & 84.59          & 66.94          & 59.00          & 72.73          & 51.16          & 81.23          & 65.24          & 46.99          & 81.98          & 78.99          & 92.13          & 80.90          & 73.60          \\ \midrule
Mean                  & 69.54          & 60.48          & 84.77          & 57.91          & 59.10          & 74.47          & 56.43          & 84.27          & 51.43          & 47.58          & 77.74          & 72.93          & 93.87          & 77.71          & 78.59          \\
Standard deviation             & 5.31           & 9.70           & 2.82           & 16.18          & 6.34           & 4.28           & 17.87          & 3.42           & 20.13          & 8.78           & 8.43           & 11.23          & 1.17           & 11.01          & 5.44           \\
Std. dev. Top2        & 1.54           & 1.45           & 0.35           & 1.14           & 0.97           & 0.27           & 1.22           & 0.63           & 0.48           & 4.32           & 0.50           & 0.77           & 0.31           & 0.39           & 0.85           \\ \bottomrule
\end{tabular}}
\end{table*} \begin{table*}[]
\centering
\resizebox{1.9\columnwidth}{!}{\begin{tabular}{@{}l|ccccc|ccccc|ccccc@{}}
\toprule
Prompt                & \multicolumn{5}{c|}{Walmart-Amazon}                                                & \multicolumn{5}{c|}{Amazon-Google}                                                 & \multicolumn{5}{c}{DBLP-Scholar}                                                   \\ \midrule
                      & Turbo03        & Turbo06        & GPT4           & SOLAR          & Beluga2        & Turbo03        & Turbo06        & GPT4           & SOLAR          & Beluga2        & Turbo03        & Turbo06        & GPT4           & SOLAR          & Beluga2        \\ \midrule
domain-complex-force  & 67.88          & 60.26          & 89.00          & 74.92          & {\ul 69.64}    & 60.37          & {\ul 61.25}    & 75.61          & 43.75          & \textbf{49.59} & 80.46          & \textbf{83.04} & 88.44          & 72.32          & {\ul 74.58}    \\
domain-complex-free   & 52.80          & 42.59          & 89.33          & {\ul 81.61}    & \textbf{74.47} & 55.61          & 49.53          & {\ul 75.57}    & {\ul 52.03}    & 45.56          & 79.06          & 82.28          & {\ul 89.78}    & \textbf{84.91} & 71.03          \\
domain-simple-force   & {\ul 74.38}    & 57.52          & 88.78          & 44.36          & 63.32          & {\ul 63.64}    & 44.38          & 75.32          & 14.67          & {\ul 47.54}    & \textbf{84.13} & 80.45          & 77.21          & 56.50          & \textbf{75.53} \\
domain-simple-free    & 60.24          & \textbf{69.12} & 88.67          & 75.00          & 61.79          & \textbf{63.72} & \textbf{63.50} & 74.51          & 41.64          & 35.26          & 77.53          & 79.39          & 88.20          & 78.04          & 68.88          \\
general-complex-force & 64.26          & {\ul 68.77}    & \textbf{89.67} & 65.32          & 63.46          & 57.91          & 44.06          & 74.91          & 19.01          & 42.01          & 79.15          & 78.22          & 87.22          & 52.05          & 72.68          \\
general-complex-free  & 44.42          & 45.82          & {\ul 89.45}    & \textbf{83.11} & 66.46          & 45.88          & 51.91          & 74.38          & 44.31          & 40.50          & 76.47          & {\ul 83.03}    & 87.50          & {\ul 79.91}    & 68.18          \\
general-simple-force  & \textbf{74.81} & 31.03          & 86.41          & 26.79          & 59.59          & 49.56          & 15.73          & 53.60          & 5.79           & 31.02          & {\ul 81.89}    & 67.18          & 78.26          & 12.69          & 64.78          \\
general-simple-free   & 68.63          & 65.19          & 88.60          & 58.25          & 51.47          & 57.63          & 52.31          & 66.67          & 14.12          & 28.77          & 77.53          & 80.81          & 81.47          & 41.38          & 54.49          \\
Narayan-simple        & 68.42          & 42.45          & 84.72          & 62.30          & 49.81          & 56.35          & 29.61          & 75.70          & 39.62          & 34.34          & 67.52          & 56.90          & 88.37          & 74.40          & 54.02          \\
Narayan-complex       & 52.67          & 40.16          & 83.37          & 71.93          & 62.82          & 43.35          & 33.33          & \textbf{76.38} & \textbf{54.69} & 45.97          & 68.67          & 70.67          & \textbf{89.82} & 78.82          & 65.62          \\ \midrule
Mean                  & 62.85          & 52.29          & 87.80          & 64.36          & 62.28          & 55.40          & 44.56          & 72.27          & 32.96          & 40.06          & 77.24          & 76.20          & 85.63          & 63.10          & 66.98          \\
Standard deviation             & 9.59           & 12.82          & 2.08           & 16.69          & 7.09           & 6.66           & 13.99          & 6.76           & 16.80          & 6.93           & 5.05           & 8.18           & 4.53           & 21.49          & 7.18           \\
Std. dev. Top2        & 0.22           & 0.18           & 0.11           & 0.75           & 2.42           & 0.04           & 1.13           & 0.41           & 1.33           & 1.03           & 1.12           & 0.01           & 0.02           & 2.50           & 0.48           \\ \bottomrule
\end{tabular}}
\end{table*} \begin{table*}[]
\centering
\caption{Results of the few-shot and rule-based experiments in comparison to the best zero-shot prompt for all LLMs and datasets. The first block presents mean F1 scores over all datasets, while the following blocks contain F1 scores for individual datasets. Best result is bold, second best is underlined.}
\label{tab:few-shot-1}
\resizebox{1.9\columnwidth}{!}{\begin{tabular}{@{}l|c|ccccc|ccccc|ccccc@{}}
\toprule
Prompt                               &       & \multicolumn{5}{c|}{All Datasets (Mean)}                                                                                                              & \multicolumn{5}{c|}{WDC Products}                                                                                                                      & \multicolumn{5}{c}{Abt-Buy}                                                                                                                             \\ \midrule
                                     & Shots & Turbo03                     & Turbo06                      & GPT4                         & SOLAR                       & Beluga2                     & Turbo03                     & Turbo06                      & GPT4                         & SOLAR                        & Beluga2                     & Turbo03                     & Turbo06                      & GPT4                         & SOLAR                        & Beluga2                      \\ \midrule
                                     & 6     & 71.87                       & 70.28                        & 85.22                        & 63.10                       & 68.27                       & 61.70                       & 43.29                        & 85.71                        & 62.11                        & 60.69                       & 91.93                       & \textbf{93.14}               & 93.83                        & 70.97                        & 78.14                        \\
\multirow{-2}{*}{Fewshot-related}    & 10    & 71.89                       & 69.93                        & {\ul 86.64}                  & 64.64                       & 69.23                       & 57.46                       & 42.94                        & 86.45                        & 51.87                        & 62.50                       & 91.48                       & {\ul 92.98}                  & {\ul 94.35}                  & 75.86                        & 79.25                        \\ \midrule
                                     & 6     & 79.25                       & 75.75                        & 85.11                        & {\ul 79.73}                 & \textbf{77.33}              & {\ul 84.09}                 & \textbf{80.93}               & 86.55                        & 71.60                        & 70.50                       & 90.78                       & 83.51                        & 94.12                        & 85.47                        & 83.78                        \\
\multirow{-2}{*}{Fewshot-random}     & 10    & 80.62                       & \textbf{77.32}               & 85.77                        & \textbf{80.71}              & {\ul 77.13}                 & 83.27                       & {\ul 79.43}                  & 86.37                        & 70.35                        & {\ul 70.97}                 & 90.25                       & 84.39                        & 93.21                        & {\ul 87.96}                  & 84.33                        \\ \midrule
                                     & 6     & \textbf{83.73}              & 74.06                        & 86.59                        & 74.40                       & 75.50                       & \textbf{85.44}              & 78.65                        & {\ul 87.23}                  & 62.47                        & 69.68                       & {\ul 92.52}                 & 81.89                        & 93.36                        & 87.02                        & 84.56                        \\
\multirow{-2}{*}{Fewshot-handpicked} & 10    & {\ul 83.63}                 & {\ul 76.98}                  & 86.20                        & 76.95                       & 74.91                       & 83.33                       & 79.31                        & 86.72                        & 70.15                        & 69.20                       & \textbf{93.36}              & 86.65                        & 93.62                        & 86.14                        & 84.45                        \\ \midrule
Hand-written rules                   & 0     & 78.59                       & 70.95                        & 85.77                        & 77.24                       & 75.49                       & 79.85                       & 68.93                        & 85.71                        & \textbf{73.26}               & 70.35                       & 86.70                       & 74.81                        & 94.15                        & 86.44                        & {\ul 85.93}                  \\
Learned rules                        & 0     & 57.17                       & 67.33                        & 85.04                        & 75.69                       & 75.29                       & 71.81                       & 59.63                        & 87.06                        & {\ul 72.96}                  & \textbf{72.45}              & 87.50                       & 83.09                        & 93.40                        & 84.11                        & \textbf{87.08}               \\ \midrule
Best zero-shot                       & 0     & 77.95                       & 74.95                        & \textbf{88.25}               & 76.97                       & 69.80                       & 79.70                       & 74.96                        & \textbf{89.61}               & 72.95                        & 63.61                       & 87.39                       & 83.98                        & \textbf{95.78}               & \textbf{89.20}               & 85.79                        \\ \midrule
$\Delta$ best/ZS                        & -     & {\color[HTML]{6434FC} 5.78} & {\color[HTML]{6434FC} 2.37}  & {\color[HTML]{FE0000} -1.61} & {\color[HTML]{6434FC} 3.74} & {\color[HTML]{6434FC} 7.53} & {\color[HTML]{6434FC} 5.74} & {\color[HTML]{6434FC} 5.97}  & {\color[HTML]{FE0000} -2.38} & {\color[HTML]{6434FC} 0.31}  & {\color[HTML]{6434FC} 8.84} & {\color[HTML]{6434FC} 5.97} & {\color[HTML]{6434FC} 9.16}  & {\color[HTML]{FE0000} -1.43} & {\color[HTML]{FE0000} -1.24} & {\color[HTML]{6434FC} 1.29}  \\
$\Delta$ FS/ZS                          & -     & {\color[HTML]{6434FC} 5.78} & {\color[HTML]{6434FC} 2.37}  & {\color[HTML]{FE0000} -1.62} & {\color[HTML]{6434FC} 3.74} & {\color[HTML]{6434FC} 7.53} & {\color[HTML]{6434FC} 5.74} & {\color[HTML]{6434FC} 5.97}  & {\color[HTML]{FE0000} -2.38} & {\color[HTML]{FE0000} -1.35} & {\color[HTML]{6434FC} 7.36} & {\color[HTML]{6434FC} 5.97} & {\color[HTML]{6434FC} 9.16}  & {\color[HTML]{FE0000} -1.43} & {\color[HTML]{FE0000} -1.24} & {\color[HTML]{FE0000} -1.23} \\
$\Delta$ Rules/ZS                       & -     & {\color[HTML]{6434FC} 0.64} & {\color[HTML]{FE0000} -4.00} & {\color[HTML]{FE0000} -2.49} & {\color[HTML]{6434FC} 0.27} & {\color[HTML]{6434FC} 5.69} & {\color[HTML]{6434FC} 0.15} & {\color[HTML]{FE0000} -6.03} & {\color[HTML]{FE0000} -2.55} & {\color[HTML]{6434FC} 0.31}  & {\color[HTML]{6434FC} 8.84} & {\color[HTML]{6434FC} 0.11} & {\color[HTML]{FE0000} -0.89} & {\color[HTML]{FE0000} -1.63} & {\color[HTML]{FE0000} -2.76} & {\color[HTML]{6434FC} 1.29}  \\ \bottomrule
\end{tabular}}
\end{table*} \begin{table*}[]
\centering
\resizebox{1.9\columnwidth}{!}{\begin{tabular}{@{}l|c|ccccc|ccccc|ccccc@{}}
\toprule
Prompt                               &       & \multicolumn{5}{c|}{Walmart-Amazon}                                                                                                                    & \multicolumn{5}{c|}{Amazon-Google}                                                                                                                       & \multicolumn{5}{c}{DBLP-Scholar}                                                                                                                          \\ \midrule
                                     & Shots & Turbo03                      & Turbo06                      & GPT4                         & SOLAR                       & Beluga2                     & Turbo03                      & Turbo06                      & GPT4                         & SOLAR                        & Beluga2                      & Turbo03                       & Turbo06                       & GPT4                         & SOLAR                        & Beluga2                     \\ \midrule
                                     & 6     & 84.73                        & {\ul 78.70}                  & {\ul 91.19}                  & 75.69                       & 70.69                       & {\ul 78.15}                  & {\ul 80.16}                  & {\ul 84.27}                  & 51.05                        & 57.98                        & 42.86                         & 56.09                         & 71.08                        & 55.68                        & 73.84                       \\
\multirow{-2}{*}{Fewshot-related}    & 10    & 83.90                        & \textbf{80.27}               & \textbf{91.24}               & 78.29                       & 76.88                       & \textbf{78.42}               & \textbf{82.58}               & \textbf{85.21}               & 57.14                        & 50.61                        & 48.21                         & 50.88                         & 75.93                        & 60.06                        & 76.89                       \\ \midrule
                                     & 6     & \textbf{87.62}               & 75.26                        & 88.89                        & {\ul 84.03}                 & 77.98                       & 75.27                        & 69.68                        & 78.08                        & {\ul 72.45}                  & 69.16                        & 58.47                         & 69.37                         & 77.91                        & {\ul 85.11}                  & \textbf{85.21}              \\
\multirow{-2}{*}{Fewshot-random}     & 10    & {\ul 86.70}                  & 78.30                        & 89.00                        & \textbf{84.83}              & 73.92                       & 75.85                        & 73.27                        & 78.76                        & \textbf{74.57}               & \textbf{72.17}               & 67.01                         & 71.21                         & 81.51                        & \textbf{85.83}               & {\ul 84.25}                 \\ \midrule
                                     & 6     & 83.84                        & 67.86                        & 88.84                        & 83.33                       & 78.49                       & 73.11                        & 67.85                        & 76.92                        & 64.79                        & 69.28                        & \textbf{-}                    & -                             & -                            & -                            & -                           \\
\multirow{-2}{*}{Fewshot-handpicked} & 10    & 84.47                        & 73.96                        & 87.89                        & 83.21                       & 74.60                       & 73.37                        & 68.01                        & 76.57                        & 68.29                        & {\ul 71.40}                  & -                             & -                             & -                            & -                            & -                           \\ \midrule
Hand-written rules                   & 0     & 74.90                        & 61.97                        & 89.16                        & 83.02                       & \textbf{81.82}              & 68.81                        & 66.11                        & 72.47                        & 64.53                        & 62.33                        & {\ul 82.70}                   & {\ul 82.92}                   & {\ul 87.34}                  & 78.97                        & 77.00                       \\
Learned rules                        & 0     & 82.81                        & 71.68                        & 86.21                        & 83.37                       & {\ul 79.40}                 & 65.73                        & 54.92                        & 73.50                        & 62.32                        & 62.21                        & -                             & -                             & -                            & -                            & \textbf{-}                  \\ \midrule
Best zero-shot                       & 0     & 74.81                        & 69.25                        & 89.67                        & 83.11                       & 74.47                       & 63.72                        & 63.50                        & 76.38                        & 54.69                        & 49.59                        & \textbf{84.13}                & \textbf{83.04}                & \textbf{89.82}               & 84.91                        & 75.53                       \\ \midrule
$\Delta$ best/ZS                        & -     & {\color[HTML]{6434FC} 12.81} & {\color[HTML]{6434FC} 11.02} & {\color[HTML]{6434FC} 1.57}  & {\color[HTML]{6434FC} 1.72} & {\color[HTML]{6434FC} 7.35} & {\color[HTML]{6434FC} 14.7}  & {\color[HTML]{6434FC} 19.08} & {\color[HTML]{6434FC} 8.83}  & {\color[HTML]{6434FC} 19.88} & {\color[HTML]{6434FC} 22.58} & {\color[HTML]{FE0000} -1.43}  & {\color[HTML]{FE0000} -0.12}  & {\color[HTML]{FE0000} -2.48} & {\color[HTML]{6434FC} 0.92}  & {\color[HTML]{6434FC} 9.68} \\
$\Delta$ FS/ZS                          & -     & {\color[HTML]{6434FC} 12.81} & {\color[HTML]{6434FC} 11.02} & {\color[HTML]{6434FC} 1.57}  & {\color[HTML]{6434FC} 1.72} & {\color[HTML]{6434FC} 4.02} & {\color[HTML]{6434FC} 14.70} & {\color[HTML]{6434FC} 19.08} & {\color[HTML]{6434FC} 8.83}  & {\color[HTML]{6434FC} 19.88} & {\color[HTML]{6434FC} 22.58} & {\color[HTML]{FE0000} -17.12} & {\color[HTML]{FE0000} -11.83} & {\color[HTML]{FE0000} -8.31} & {\color[HTML]{6434FC} 0.92}  & {\color[HTML]{6434FC} 9.68} \\
$\Delta$ Rules/ZS                       & -     & {\color[HTML]{6434FC} 8.00}  & {\color[HTML]{6434FC} 2.43}  & {\color[HTML]{FE0000} -0.51} & {\color[HTML]{6434FC} 0.26} & {\color[HTML]{6434FC} 7.35} & {\color[HTML]{6434FC} 5.09}  & {\color[HTML]{6434FC} 2.61}  & {\color[HTML]{FE0000} -2.88} & {\color[HTML]{6434FC} 9.84}  & {\color[HTML]{6434FC} 12.74} & {\color[HTML]{FE0000} -1.43}  & {\color[HTML]{FE0000} -0.12}  & {\color[HTML]{FE0000} -2.48} & {\color[HTML]{FE0000} -5.94} & {\color[HTML]{6434FC} 1.47} \\ \bottomrule
\end{tabular}}
\end{table*} 


 \section{Scenario 2: With Training Data}
\label{sec:fewshotrulesfinetune}

Task-specific training data in the form of matching and non-matching entity pairs can be used to (i) add demonstrations to the prompts, (ii) learn textual matching rules, and (iii) fine-tune the LLMs.
In this section, we explore whether and how our zero-shot results can be improved by using task-specific training data. In order to make the results comparable, we use the same development sets (see Table~\ref{tab:datasets}) that we already used for fine-tuning the PLM-based matchers for all experiments. 

\subsection{In-Context Learning}
\label{subsec:incontext}

For the in-context learning experiments, we provide each LLM with a set of task demonstrations~\cite{liu-etal-2022-makes} as part of the prompt to help guide the models decisions. The provided demonstrations are then followed by the specific entity description pair for which the model should generate an answer. Figure \ref{fig:fewshot} shows an example of an in-context learning prompt with a single positive and a single negative demonstration. We vary the amount of demonstrations in each prompt from 6 to 10 with an equal amount of positive and negative examples. For the selection of the demonstrations, we compare three different heuristics:



\begin{itemize}
    \item \textbf{Hand-picked}: Hand-picked demonstrations are a set of up to 5 matching and 5 non-matching product offer pairs which were hand-selected by a human domain expert with the aim to cover potentially helpful corner cases. Take note that these examples are only selected from the pool of the training set of the WDC Products benchmark and subsequently supplied to all other datasets apart from DBLP-Scholar as demonstrations.
    \item \textbf{Random}: For this heuristic, task demonstrations are drawn randomly from the labeled training set of the corresponding benchmark.
\item \textbf{Related}: Related demonstrations are selected from the training set of the corresponding benchmark by calculating the Jaccard similarity between the string representation of the pair to be matched and all positive and negative pairs in the training set. The resulting similarity lists are sorted and the most similar examples are selected. Due to this process, related examples can be considered highly similar to the specific task at hand.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/data/fewshot.png}
  \caption{Example of a prompt containing a positive and a negative demonstration before asking for a decision.}
  \Description{Example of a prompt containing a positive and a negative demonstration before asking for a decision.}
  \label{fig:fewshot}
\end{figure} 
\textbf{Effectiveness:} Table \ref{tab:few-shot-1} shows the results of the in-context experiments in comparison to the best zero-shot baselines. Depending on the dataset/model combination the usefulness of in-context learning differs. The GPT4 model, which is the best performing model in Scenario 1, improves only on Amazon-Google and Walmart-Amazon by 9\% and 1.5\% F1 respectively when supplying related demonstrations. GPT4's performance on the other three datasets drops irrespective of the demonstration selection method meaning that the model does not need the additional guidance in these cases. For all other LLMs providing in-context examples usually leads to performance improvements, while the size of the improvements varies widely in the range of  1\% to 23\% F1.
Interestingly, the largest improvements for all models with in-context learning are observable on the Amazon-Google Dataset. As stated in Section \ref{sec:experimentalsetup}, this dataset contains software products which are somewhat different to the products in the other three product datasets that mainly contain tangible products like for example electronics, tools, or clothing items. The understanding of what constitutes matching products inherent to the LLMs seems to be more in-line with the actual task on WDC Products, Abt-Buy and Walmart-Amazon than on Amazon-Google as for all three of these datasets the zero-shot performance is high enough to beat the PLM-baselines as shown in Section \ref{sec:zeroshot}. Zero-shot performance on the DBLP-Scholar dataset on the other hand is 5\% F1 lower than the PLM-baselines, and even providing demonstrations does not further improve performance for the OpenAI LLMs. A manual check of the errors revealed that the LLMs often mistake matching publications as non-matches if the publication year differs. 

\textbf{Comparison of Selection Methods:} While providing demonstrations generally improves results for most LLMs and datasets, the demonstration selection method that works best varies depending on the dataset. The open-source LLMs generally reach the best performance when random demonstrations are provided. GPT4 on the other hand, which only improves on Walmart-Amazon and Amazon-Google, achieves the highest scores using related demonstrations, which points towards the model being able to better understand and transfer the specific patterns of such closely related examples to the current matching decision.
The handpicked demonstrations, which are selected from the WDC Products training data, are helpful for the Turbo models and Beluga2 on WDC Products but interestingly also lead to improvements on the other product datasets. We observe a similar generalization effect on a larger scale during fine-tuning in Section \ref{subsec:finetune}.


\subsection{Learning Matching Rules}
\label{subsec:rules}

 In this set of experiments, instead of providing the models with in-context examples, we provide a set of matching rules in the prompt in order to guide the model to find the correct solution. We differentiate between two kinds of rules (i) handwritten and (ii) learned rules. Handwritten rules are a simple set of rules provided by a domain expert and are depicted in Figure \ref{fig:rules}. For the learned rules, we pass the handpicked examples that were used for the in-context experiments to GPT4 and ask the model to generate matching rules from these examples. 
A subset of these rules is depicted in Figure \ref{fig:rules}. For the full list of learned rules, please refer to the project repository. When comparing hand-written and learned rules, it should be noted that the learned rule-set is significantly longer than the handwritten one, which is partly owed to the GPT4 model providing an example for the application of each rule. This leads to higher cost when querying hosted LLMs whose usage fees depend on the length of the prompt.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/data/rules.png}
  \caption{Example of a prompt containing handwritten matching rules for the product domain. A subset of the learned rules is depicted below. Each learned rule is usually accompanied by an example which we omit for brevity.}
  \Description{Example of a prompt containing handwritten matching rules for the product domain. A subset of the learned rules is depicted below. Each learned rule is usually accompanied by an example which we omit for brevity.}
  \label{fig:rules}
\end{figure} 
 \textbf{Effectiveness:} Table \ref{tab:few-shot-1} shows the results of providing matching rules in comparison to the best zero-shot prompt and the in-context experiments. The results show that GPT4 with matching rules does not improve over its best zero-shot performance and instead loses 1\% to 2.5\% F1. All other models see improvements of 1\% to 12\% F1 over zero-shot depending on model and dataset.The provision of matching rules can be helpful, especially for the open-source LLMs with Beluga2 achieving its highest scores on the three closely related product matching tasks WDC Products, Abt-Buy and Walmart-Amazon. Providing in-context demonstrations often leads to higher performance gains than providing matching rules for all models apart from GPT4, which performs best in the zero-shot configuration.



\subsection{Fine-Tuning the LLM}
\label{subsec:finetune}

In the final set of experiments, we fine-tune the Turbo06 model via the OpenAI API. Specifically, we use the training splits of each dataset to train a fine-tuned Turbo06 model and subsequently apply the fine-tuned models to all other datasets. We run fine-tuning for 3 epochs using the default parameters suggested by OpenAI.

\textbf{Effectiveness:} The results of the fine-tuned LLMs are shown in Table \ref{tab:fine-tuning}. When comparing the results to the best zero-shot performance of the same model (Turbo06), there is a substantial improvement of 11\% to 23\% F1 depending on the dataset. These improvements allow the fine-tuned Turbo06 model to nearly reach the performance of the best GPT4 zero-shot prompt on two datasets, missing only a single F1 point. On the remaining three datasets, the fine-tuned Turbo06 exceeds the performance of zero-shot GPT4 by 3\% F1. When compared to the best Turbo06 model leveraging in-context learning, fine-tuning leads to improvements of 1\% to 20\% F1 on 4 of 5 datasets while the best results on Amazon-Google are achieved with in-context learning. In summary, fine-tuning the model leads to improved results compared to the zero-shot version of the model rivaling the performance of the best GPT4 prompts. 

\textbf{Generalization:} We observe a generalization effect of models fine-tuned on one dataset to datasets from related domains. Transferring models between the product domains in general leads to improved performance of 8-20\% F1 over the best zero-shot prompts for all models unless transferred to WDC Products where all models experience a drop of at least 12\% F1 in performance compared to zero-shot. This suggests, that many of the patterns found in WDC Products are transferable to the other product datasets while the patterns of the other datasets may be too simple to be helpful for WDC Products which contains 80\% corner cases~\cite{peeters2023wdc}. Furthermore, all models fine-tuned on the datasets from the product domain exhibit good generalization to the publication domain, resulting in improvements of 5-8\% F1 over the best zero-shot prompt on DBLP-Scholar. The transfer of fine-tuned models has received quite some research attention in the context of PLM-based matchers~\cite{trabelsiDAMEDomainAdaptation2022,tuDomainAdaptationDeep2022} but often results in a significant loss of performance. As we have shown in Section \ref{sec:zeroshot}, LLM-based matchers are not susceptible to this problem in the zero-shot scenario. The fine-tuning experiments additionally show that tuning models on a related training set can further improve performance over the zero-shot results.


\begin{table}[]
\centering
\caption{F1-scores for fine-tuning Turbo06 and subsequent transfer to all datasets. Best result is bold, second best is underlined.}
\label{tab:fine-tuning}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lccccc@{}}
\toprule
Fine-tuned      & WDC                          & A-B                          & W-A                         & A-G                         & D-S                         \\ \midrule
WDC             & {\ul 88.34}                  & {\ul 94.69}                  & {\ul 90.55}                 & 77.82                       & 88.97                       \\
A-B             & 62.53                        & 94.18                        & 90.19                       & {\ul 79.84}                 & 89.36                       \\
A-G             & 48.66                        & 89.97                        & 87.26                       & 79.81                       & {\ul 91.01}                 \\
W-A             & 52.33                        & 92.80                        & \textbf{92.76}              & 78.80                       & 87.93                       \\
D-S             & 11.94                        & 61.39                        & 63.33                       & 52.25                       & \textbf{93.23}              \\ \midrule
Best ZS Turbo06 & 74.96                        & 83.83                        & 69.25                       & 63.50                       & 83.04                       \\
Best FS Turbo06 & 80.93                        & 93.14                        & 80.27                       & \textbf{82.58}              & 71.21                       \\
Best ZS GPT4    & \textbf{89.61}               & \textbf{95.78}               & 89.67                       & 76.38                       & 89.82                       \\
$\Delta$ GPT4/FT   & {\color[HTML]{FE0000} -1.27} & {\color[HTML]{FE0000} -1.09} & {\color[HTML]{6434FC} 3.09} & {\color[HTML]{6434FC} 3.46} & {\color[HTML]{6434FC} 3.41} \\ \bottomrule
\end{tabular}}
\end{table} 


 \section{Related Work}
\label{sec:relatedwork}



Entity matching is the task of discovering entity descriptions in different data sources that refer to the same real-world entity~\cite{,BarlaugNeural2021,Christen2012DataMC,elmagarmidDuplicateRecordDetection2007}. Entity matching is a central step in data integration pipelines~\cite{christophides_end--end_2020} and forms the foundation of interlinking data on the Web~\cite{LinkDiscoverySurvey2017,LinkedDataArticle2009}. 




\textbf{Entity Matching Methods:} Entity matching has been researched for over 50 years~\cite{fellegiTheoryRecordLinkage1969}. Early approaches involved domain experts hand-crafting matching rules~\cite{fellegiTheoryRecordLinkage1969}. Over time, advancements were made with unsupervised and supervised machine learning techniques resulting in improved matching performance~\cite{christophides_end--end_2020}. 
By the late 2010s, the sucess of deep learning in areas such as natural language processing and computer vision paved the way for early applications in entity matching~\cite{mudgalDeepLearningEntity2018,shahNeuralNetworkBased2018}.
The Transformer architecture~\cite{vaswaniAttentionAllYou2017} and pre-trained models like BERT~\cite{devlinBERTPretrainingDeep2019} and RoBERTa~\cite{liu_roberta_2019} revolutionized natural language processing, which has led the data integration community to also turn to these language models for entity matching~\cite{brunnerEntityMatchingTransformer2020,liDeepEntityMatching2020,peetersDualobjectiveFinetuningBERT2021,yaoEntityResolutionHierarchical2022}.
More recent work delved into the application of self-supervised and supervised contrastive losses~\cite{chenSimpleFrameworkContrastive2020,gaoSimCSESimpleContrastive2021,khoslaSupervisedContrastiveLearning2021} in combination with PLM encoder networks for entity matching~\cite{peetersSupervisedContrastiveLearning2022a,wangSudowoodoContrastiveSelfsupervised2022}.
Other studies have explored graph-based methods~\cite{geCollaborERSelfsupervisedEntity2021,yaoEntityResolutionHierarchical2022} and the application of domain adaptation techniques for entity matching~\cite{losterKnowledgeTransferEntity2021,trabelsiDAMEDomainAdaptation2022,tuDomainAdaptationDeep2022,akbarian2022probing}.



\textbf{LLM-based Entity Matching:} Narayan et al.~\cite{foundationalWrangleVLDB2022} were the first to experiment with using an LLM (GPT3) for entity matching as part of a wider study also covering data engineering tasks such as schema matching and missing value imputation. In our previous work ~\cite{peetersUsingChatGPTEntity2023a}, we employed ChatGPT for entity matching which we tested using different prompt designs on a single benchmark dataset. Compared to these two papers, we evaluate a wider range of more recent LLM models using an extended set of prompt designs. We present a deeper analysis of the prompt sensitivity of the models, the prompt to model fit, and the utility of in-context learning. We also investigate the utility of additional methods such as rule learning and LLM fine-tuning which were not covered in the existing work. The website~\footnote{\url{https://crfm.stanford.edu/helm/latest/?group=entity_matching}} of the HELM LLM benchmark~\cite{liang2022holistic} provides entity matching results for three down-sampled benchmark datasets (Abt-Buy, Beer, and iTunes-Amazon) using a single zero-shot prompt design but a wide range of different models. Compared to this work, we cover a wider range of prompt designs as well as more advanced techniques such as in-context learning, rule learning, and LLM fine-tuning. The HELM benchmark also uses smaller test sets ranging from 91 to 493 entity pairs compared our test sets that all contain around 1200 entity pairs (see Table~\ref{tab:datasets}). 

%
 \section{Conclusion}
\label{sec:conclusion}

This paper has explored using LLMs for entity matching and has compared the performance of LLM-based matchers to PLM-based matchers. We have investigated the prompt sensivity of the LLMs for entity matching task as well as the prompt to model fit.
We have shown that LLMs outperform PLM-based matchers on all product-related benchmark datasets. For three out of five benchmark datasets, GPT4 reaches a better performance than PLM-based matchers without requiring any task-specific training data. For the Amazon-Google dataset providing 10 related demonstrations enables GPT4 and Turbo06 to outperform the PLM baselines. For the bibliographic DBLP-Scholar dataset, the fine-tuned GPT-Turbo06 model reached a similar performance as the PLMs. 
We found that the best performing prompt design depends on the individual model and further showed that GPT4 has a lower prompt sensitivity compared to the other models which reduces the necessity of prompt search.
The best open source model SOLAR is able to reach a similar performance as the GPT-Turbo models while a performance gap remains compared to GPT4. PLM-based matchers are not robust concerning out-of-distribution entities even if they are from the same domain. The high performance of the LLMs in the zero-shot scenario showed that they do not suffer from this shortcoming. Fine-tuning the Turbo06 model led to further performance gains, allowing this LLM to reach and partly exceed the performance levels of a GPT4-based matcher. We demonstrated that fine-tuning the Turbo06 model on one product-related data set can also lead to performance gains over zero-shot for other product datasets. Finally, we observed that Turbo06 fine-tuned on product data shows cross-domain generalization as the fine-tuning also improves the performance on the DBLP-Scholar dataset. 


\balance



\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


\end{document}
