\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{images/model}
    \caption{Model overview of \golden{} Retriever.
    Given an open-domain multi-hop question, the model iteratively retrieves more context documents, and concatenates all retrieved context for a QA model to answer from.}
    \label{fig:model}
\end{figure*}

\section{Model}

In this section, we formally define the problem of open-domain multi-hop question answering, and motivate the architecture of the proposed \golden{} (Gold Entity) Retriever model.
We then detail the query generation components as well as how to derive supervision signal for them, before concluding with the QA component.

\subsection{Problem Statement}

We define the problem of \emph{open-domain multi-hop QA} as one involving a question , and  relevant (gold) supporting context documents  which contain the desired answer \footnote{In this work, we only consider extractive, or span-based, QA tasks, but the problem statement and the proposed method apply to generative QA tasks as well.}
These supporting documents form a chain of reasoning necessary to arrive at the answer, and they come from a large corpus of documents  where .
In this chain of reasoning, the supporting documents are usually connected via shared entities or textual similarities (\eg, they describe similar entities or events), but these connections do not necessarily conform to any predefined knowledge schema.

We contrast this to what we call the \emph{few-document setting} of multi-hop QA, where the QA system is presented with a small set of documents , where  comprise a small set of ``distractor'' documents that test whether the system is able to pick out the correct set of supporting documents in the presence of noise.
This setting is suitable for testing QA systems' ability to perform multi-hop reasoning given the gold supporting documents with bounded computational budget, but we argue that it is far from a realistic one.
In practice, an open-domain QA system has to locate all gold supporting documents from  on its own, and as shown in Figure \ref{fig:example}, this is often difficult for multi-hop  questions based on the original question alone, as not all gold documents are easily retrievable given the question.

To address this gold context discoverability issue, we argue that it is necessary to move away from a single-hop retrieve-and-read approach where the original question serves as the search query.
In the next section, we introduce \golden{} Retriever, which addresses this problem by iterating between retrieving more documents and reading the context for multiple rounds.

\subsection{Model Overview}

Essentially, the challenge of open-domain multi-hop QA lies in the fact that the information need of the user  cannot be readily satisfied by any information retrieval (IR) system that models merely the similarity between the question  and the documents.
This is because the true information need will only unfold with progressive reasoning and discovery of supporting facts.
Therefore, one cannot rely solely on a similarity-based IR system for such iterative reasoning, because the potential pool of relevant documents grows exponentially with the number of hops of reasoning.

To this end, we propose \golden{} (Gold Entity) Retriever, which makes use of the gold document\footnote{In \hotpotqa{}, documents usually describe entities, thus we use ``documents'' and ``entities'' interchangeably.} information available in the QA dataset at training time to iteratively query for more relevant supporting documents during each hop of reasoning.
Instead of relying on the original question as the search query to retrieve all supporting facts, or building computationally expensive search engines that are less interpretable to humans, we propose to leverage text-based IR engines for interpretability, and generate different search queries as each reasoning step unfolds.
In the very first hop of reasoning, \golden{} Retriever is presented the original question , from which it generates a search query  that retrieves supporting document \footnote{For notational simplicity,  denotes the supporting document needed to complete the -th step of reasoning. We also assume that the goal of each IR query is to retrieve one and only one gold supporting document in its top  results.}
Then for each of the subsequent reasoning steps (), \golden{} Retriever generates a query  from the question and the available context, .
This formulation allows the model to generate queries based on information revealed in the supporting facts (see Figure \ref{fig:model}, for example).


We note that \golden{} Retriever is much more efficient, scalable, and interpretable at retrieving gold documents compared to its neural retrieval counterparts.
This is because \golden{} Retriever does not rely on a QA-specific IR engine tuned to a specific dataset, where adding new documents or question types into the index can be extremely inefficient.
Further, \golden{} Retriever generates queries in natural language, making it friendly to human interpretation and verification.
One core challenge in \golden{} Retriever, however, is to train query generation models in an efficient manner, because the search space for potential queries is enormous and off-the-shelf IR engines are not end-to-end differentiable.
We outline our solution to this challenge in the following sections.

\subsection{Query Generation}

\begin{table*}
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{p{10.5cm}p{3.3cm}p{2.9cm}}
        \toprule
        \multicolumn{1}{c}{\textbf{Question}} & \textbf{Hop 1 Oracle} & \textbf{Hop 2 Oracle} \\
        \midrule
        What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell? & Corliss Archer in the film Kiss and Tell & Shirley Temple \\
        \midrule
        Scott Parkin has been a vocal critic of Exxonmobil and another corporation that has operations in how many countries? & Scott Parkin & Halliburton \\
        \midrule
        Are Giuseppe Verdi and Ambroise Thomas both Opera composers? & Giuseppe Verdi & Ambroise Thomas\\
        \bottomrule
    \end{tabular}
    }
    \caption{Example oracle queries on the \hotpotqa{} dev set.}
    \label{tab:oracle_examples}
\end{table*}

For each reasoning step, we need to generate the search query given the original question  and some context of documents we have already retrieved (initially empty).
This query generation problem is conceptually similar to the QA task in that they both map a question and some context to a target, only instead of an answer, the target here is a search query that helps retrieve the desired supporting document for the next reasoning step.
Therefore, we formulate the query generation process as a question answering task.

To reduce the potentially large space of possible queries, we favor a QA model that extracts text spans from the context over one that generates free-form text as search queries.
We therefore employ DrQA's Document Reader model \cite{chen2017reading}, which is a relatively light-weight recurrent neural network QA model that has demonstrated success in few-document QA. We adapt it to query generation as follows.


For each reasoning step , given a question  and some \emph{retrieval context}  which \emph{ideally} contains the gold supporting documents , we aim to generate a search query  that helps us retrieve  for the next reasoning step.
A Document Reader model is trained to select a span from  as the query

where  is the query generator. This query is then used to search for supporting documents, which are concatenated with the current retrieval context to update it

where  is the top  documents retrieved from the search engine using , and \footnote{In the query result, the title of each document is delimited with special tokens \texttt{<t>} and \texttt{</t>} before concatenation.}
At the end of the retrieval steps, we provide  as question along with  as context to the final few-document QA component detailed in Section \ref{sec:qa-component} to obtain the final answer to the original question.

To train the query generators, we follow the steps above to construct the retrieval contexts, but during training time, when  is not part of the IR result, we replace the lowest ranking document with  before concatenating it with  to make sure the downstream models have access to necessary context.

\subsection{Deriving Supervision Signal for Query Generation}
When deriving supervision signal to train our query generators, the potential search space is enormous for each step of reasoning even if we constrain ourselves to predicting spans from the context.
This is aggravated by multiple hops of reasoning required by the question.
One solution to this issue is to train the query generators with reinforcement learning (RL) techniques (\eg, REINFORCE \cite{sutton2000policy}), where \cite{nogueira2017task} and \cite{buck2017ask} are examples of one-step query generation with RL. However, it is computationally inefficient, and has high variance especially for the second reasoning step and forward, because the context depends greatly on what queries have been chosen previously and their search results.

Instead, we propose to leverage the limited supervision we have about the gold supporting documents  to narrow down the search space.
The key insight we base our approach on is that at any step of open-domain multi-hop reasoning, there is some \emph{semantic overlap} between the retrieval context and the next document(s) we wish to retrieve.
For instance, in our \emph{Armada} example, when the retrieval context contains only the question, this overlap is the novel itself; after we have expanded the retrieval context, this overlap becomes the name of the author, \emph{Ernest Cline}.
Finding this semantic overlap between the retrieval context and the desired documents not only reveals the chain of reasoning naturally, but also allows us to use it as the search query for retrieval.

Because off-the-shelf IR systems generally optimize for shallow lexical similarity between query and candidate documents in favor of efficiency,
a good proxy for this overlap is locating spans of text that have high lexical overlap with the intended supporting documents.
To this end, we propose a simple yet effective solution, employing several heuristics to generate candidate queries: computing the longest common string/sequence between the current retrieval context and the title/text of the intended paragraph ignoring stop words, then taking the contiguous span of text that corresponds to this overlap in the retrieval context.
This allows us to not only make use of entity names, but also textual descriptions that better lead to the gold entities.
It is also more generally applicable than question decomposition approaches \cite{talmor2018web,min2019multi}, and does not require additional annotation for decomposition.

Applying various heuristics results in a handful of candidate queries for each document, and we use our IR engine (detailed next) to rank them based on recall of the intended supporting document to choose one as the final oracle query we train our query generators to predict.
This allows us to train the query generators in a fully supervised manner efficiently.
Some examples of oracle queries on the \hotpotqa{} dev set can be found in Table~\ref{tab:oracle_examples}.
We refer the reader to Appendix \ref{sec:heuristics} for more technical details about our heuristics and how the oracle queries are derived.

\paragraph{Oracle Query vs Single-hop Query}
We evaluate the oracle query against the single-hop query, i.e., querying with the original question, on the \hotpotqa{} dev set.
Specifically, we compare the recall of gold paragraphs, because the greater the recall, the fewer documents we need to pass into the expensive neural multi-hop QA component.

We index the English Wikipedia dump with introductory paragraphs provided by the \hotpotqa{} authors\footnote{\url{https://hotpotqa.github.io/wiki-readme.html}} with Elasticsearch 6.7 \cite{gormley2015elasticsearch},
where we index the titles and document text in separate fields with bigram indexing enabled.
This results in an index with 5,233,329 total documents.
At retrieval time, we boost the scores of any search result whose title matches the search query better -- this results in a better recall for entities with common names (\eg, ``\emph{Armada}'' the novel).
For more details about how the IR engine is set up and the effect of score boosting, please refer to Appendix \ref{sec:elasticsearch}.

\begin{figure}
    \pgfplotstableread[row sep=\\,col sep=&]{
	k & recall \\
	1 & 68.16 \\
	2 & 77.31 \\
	5 & 84.01 \\
	10 & 87.85 \\
	20 & 91.15 \\
	50 & 93.59 \\
}\questionpone

\pgfplotstableread[row sep=\\,col sep=&]{
	k & recall \\
	1 & 0.00 \\
	2 & 16.48 \\
	5 & 30.13 \\
	10 & 36.91 \\
	20 & 43.08 \\
	50 & 49.17 \\
}\questionptwo

\pgfplotstableread[row sep=\\,col sep=&]{
	k & recall \\
	1 & 85.36 \\
	2 & 90.52 \\
	5 & 94.53 \\
	10 & 95.87 \\
	20 & 97.12 \\
	50 & 98.15 \\
}\oraclepone

\pgfplotstableread[row sep=\\,col sep=&]{
	k & recall \\
	1 & 75.07 \\
	2 & 79.41 \\
	5 & 85.28 \\
	10 & 88.33 \\
	20 & 90.57 \\
	50 & 93.00 \\
}\oracleptwo

\begin{tikzpicture}[every plot/.append style={thick},font=\small]
\begin{axis}[
xlabel={Number of Retrieved Documents},
xtick={1, 2, 5, 10, 20, 50},
xticklabels={1,2,5,10,20,50},
xmode=log,
ylabel={Dev Recall (\%)},
height=5.5cm,
ymin=0,
ymax=100,
legend style={at={(0.5,-0.27)}, anchor=north},
legend columns=2,
ymajorgrids=true,
grid style=dashed,
]
\addplot[dashed, color=blue, mark=o, mark options={solid}] plot table[x=k, y=recall]{\questionpone};
\addplot[dashed, color=orange, mark=x, mark options={solid}] plot table[x=k,y=recall]{\questionptwo};
\addplot[color=blue, mark=o] plot table[x=k, y=recall]{\oraclepone};
\addplot[color=orange, mark=x] plot table[x=k,y=recall]{\oracleptwo};
\legend{Single-hop , Single-hop , Oracle , Oracle }

\end{axis}
\end{tikzpicture}
     \caption{Recall comparison between single-hop queries and \golden{} Retriever oracle queries for both supporting paragraphs on the \hotpotqa{} dev set.
    Note that the oracle queries are much more effective than the original question (single-hop query) at retrieving target paragraphs in both hops.}
    \label{fig:question_vs_oracle}
\end{figure}

In Figure \ref{fig:question_vs_oracle}, we compare the recall of the two gold paragraphs required for each question in \hotpotqa{} at various number of documents retrieved (R@) for the single-hop query and the multi-hop queries generated from the oracle.
Note that the oracle queries are much more effective at retrieving the gold paragraphs than the original question in both hops.
For instance, if we combine R@5 of both oracles (which effectively retrieves 10 documents from two queries) and compare that to R@10 for the single-hop query, the oracle queries improve recall for  by 6.68\%, and that for  by a significant margin of 49.09\%\footnote{Since \hotpotqa{} does not provide the logical order its gold entities should be discovered, we simply call the document  which is more easily retrievable with the queries, and the other as .}
This means that the final QA model will need to consider far fewer documents to arrive at a decent set of supporting facts that lead to the answer.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{images/qa_model}
    \caption{Question answering component in \golden{} Retriever. (Best viewed in color)} \label{fig:qa_model}
\end{figure}

\subsection{Question Answering Component} \label{sec:qa-component}

The final QA component of \golden{} Retriever is based on the baseline model presented in \cite{yang2018hotpotqa}, which is in turn based on BiDAF++ \cite{clark2017simple}.
We make two major changes to this model.
\citet{yang2018hotpotqa} concatenated all context paragraphs into one long string to predict span begin and end offsets for the answer, which is potentially sensitive to the order in which these paragraphs are presented to the model.
We instead process them separately with shared encoder RNN parameters to obtain paragraph order-insensitive representations for each paragraph.
Span offset scores are predicted from each paragraph independently before finally aggregated and normalized with a global softmax operation to produce probabilities over spans.
The second change is that we replace all attention mechanism in the original model with self attention layers over the concatenated question and context.
To differentiate context paragraph representations from question representations in this self-attention mechanism, we indicate question and context tokens by concatenating a 0/1 feature at the input layer.
Figure \ref{fig:qa_model} illustrates the QA model architecture.
