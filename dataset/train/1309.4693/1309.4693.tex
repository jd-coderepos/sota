\documentclass{LMCS}

\def\dOi{9(3:26)2013}
\lmcsheading {\dOi}
{1--68}
{}
{}
{Dec.~14, 2012}
{Sep.~25, 2013}
{}

\ACMCCS{[{\bf Theory of computation}]: Models of
  computation---Concurrency---Process calculi} 
\keywords{Wireless Networks, Process Algebra, Testing Preorders,
  Probabilistic Systems}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{stmaryrd}
\usepackage{pi}
\usepackage{amstext}
\usepackage{xspace}
\usepackage{bbm}
\usepackage{nicefrac}

\usepackage{rules}
\usepackage{maths}
\usepackage{dist}
\usepackage{networks}

\usepackage{multirow}


\usetikzlibrary{arrows,patterns}



\newenvironment{ltspic}[1]
{
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=#1, 
semithick]
\tikzstyle{ltstate}=[shape=circle, fill=lightgray!80, text=black,minimum size=17pt,inner sep=2pt]
}
{
\end{tikzpicture}
}

\newcommand{\calC}{\ensuremath{{\mathcal{C}}}}
\newcommand{\calL}{\ensuremath{{\mathcal{L}}}}



\newcommand{\leaveout}[1]{ }

\definecolor{greenish}{rgb}{.24,.5,.26} \newcommand{\MHx}{\color{greenish}} \newcommand{\MHc}[1]{{\MHx   #1}}
\newcommand{\MHf}[1]{   \marginpar{\MHx{\fbox{\footnotemark}}} \footnotetext{~\MHx #1}}

\newcommand{\ACx}{\color{red}}
\newcommand{\ACc}[1]{ {\ACx   #1}}
\newcommand{\ACf}[1]{   \marginpar{\ACx{\fbox{\footnotemark}}} \footnotetext{~\ACx #1}}

\renewcommand{\MHc}[1]{{#1}}

\begin{document}
\title[Modelling Probabilistic Wireless Networks]{Modelling Probabilistic Wireless Networks}

\author[A.~Cerone]{Andrea Cerone\rsuper a}
\address{{\lsuper a}IMDEA Software Institute, Madrid, Spain}
\email{andrea.cerone@imdea.org}
\author[M.~Hennessy]{Matthew Hennessy\rsuper b}
\address{{\lsuper b}School of Computer Science and Statistics\\ Trinity College Dublin\\Ireland}
\email{matthew.hennessy@scss.tcd.ie }   
\thanks{{\lsuper{a,b}}The financial support of SFI is gratefully acknowledged.}

\begin{abstract}
  We propose a process calculus to model high level wireless systems, where the 
  topology of a network is described by a digraph. The calculus enjoys  
  features which are proper of wireless networks, namely broadcast 
  communication and probabilistic behaviour.

  We first focus on the problem of composing wireless networks, then we 
  present a compositional theory based on a probabilistic generalisation of the well 
  known may-testing and must-testing preorders. Also, we define an extensional semantics 
  for our calculus, which will be used to define both simulation and deadlock simulation 
  preorders for wireless networks. We 
  prove that our simulation preorder is sound with respect to 
  the may-testing preorder; similarly, the deadlock simulation preorder is 
  sound with respect to the must-testing preorder, for a large class of networks.\\
  We also provide a counterexample showing that completeness of 
  the simulation preorder, with respect to the may testing one, does not hold. 
  We conclude the paper with an application of our theory to probabilistic 
  routing protocols.
\end{abstract}

\maketitle




\section{Introduction}
\label{sec:intro}
Wireless networks have spread worldwide in the last decades; nowadays they are used in many areas, from domestic appliances to mobile phone networks, to the newer sensor infrastructures.
One of the main problems of wireless networks is 
that of defining and implementing protocols for providing to users the services 
for which the network has been designed; also, because of their distributed nature, 
a more challenging problem is that of ensuring in a rigorous, mathematical way, 
the correct behaviour of a network with respect to some specification.

This problem becomes even more difficult to tackle if we consider that 
often wireless networks run protocols whose behaviour is probabilistic. 
Such protocols are indeed very useful for improving the performance of 
wireless networks, examples being the use of probabilistic \emph{routing 
protocols} \cite{sample} or probabilistic protocols for \emph{collision avoidance} 
at the \emph{MAC-sublayer} of the \emph{TCP/IP} reference model \cite{macsurvey}. 
Further, problems for which there is no solution in   
a deterministic setting can be solved (in unbounded time) by introducing 
probabilistic behaviour in wireless networks \cite{BraTou85}.
 
Many different formal frameworks have been developed in the literature 
for defining and \linebreak 
analysing the behaviour of wireless networks 
\cite{nanz,Godskesen07,GwFM10,LaneseS10,omegacalc,merro,gallina2011,songphd,wang}; 
these differ in many details, 
the most important being the level of abstraction used to 
represent a wireless network, the computational power of stations 
of wireless networks and the mathematical structure 
used to represent the topology of wireless networks. 
However, each of these calculi have the following features in common: 
wireless networks are represented as a collection of stations (also called 
nodes, or locations) running code, and local broadcast is used as 
the only way of communication. Roughly speaking in local broadcast 
communication, whenever a node broadcasts 
a message only the nodes in its range of transmission are affected.

In this paper we propose another process calculus for modelling probabilistic 
wireless networks; the main concepts underlying our calculus can be 
summarised as follows:

\begin{enumerate}[label=(\roman*)]
\item The topology of a wireless network is static, that is mobility is not 
considered in our model. 
This restriction has been done to allow a more clear presentation of the 
topics treated in this paper; however, 
we could have used the approach described in \cite{francalanza2008} to 
introduce node mobility.
The network topology is 
described by a digraph ; 
intuitively vertices in this graph represent network locations, while 
an edge from a node to another is used for expressing that the latter is 
in the range of transmission of the former. 

\item A probabilistic process calculus is defined for assigning code to locations. 
The basic constructs allowed in our calculus are messages' broadcast and reception, 
internal actions, matching and process definitions; further, we allow a special clause 
 whose role will be presented shortly.  
The mapping that assigns code to locations is partial, meaning that locations can have no code 
assigned. At least informally, such nodes, which will be called \emph{external}, 
can be seen as terminals at which users can place code 
to test the behaviour of the network.

\item Communication between nodes is reliable; a message broadcast by a 
node along a channel  will be received by all the nodes in the sender's 
range of transmission, provided that they are waiting to receive a message 
along such a channel. In other words, our calculus is designed for describing 
wireless networks at the \emph{network layer} of the \emph{TCP/IP reference model}; 
reliable communication is not ensured at lower levels, where 
issues such as the possibility of collisions \cite{macsurvey} and synchronisation between 
nodes \cite{time} arise.
\end{enumerate}

\noindent One of the main goals of the paper is that of defining a compositional behavioural 
theory of wireless networks; given two wireless networks  and 
, we want to establish whether they can be distinguished by 
an external user. 
To accomplish this task, we need to address several different topics. 
First, it is necessary to define how two wireless networks can be composed 
together. 
This topic has already 
been addressed, for different process calculi, in \cite{merro,gallina2011,CHM12,bugliesi2012,songphd}.
Here we define an asymmetric operator 
 which can be used to extend one network with another. 
Despite being asymmetric, we show that the choice of the operator  is 
driven by  some natural requirements we require in general 
from a composition operator between networks. 
We remark that our theory 
of composition is restricted to a particular class of networks, 
which we call \emph{well-formed}.

Once we have chosen a suitable composition operator , 
we can define a compositional theory for wireless networks. 
In this paper we have chosen to focus on a probabilistic generalisation 
of the well-known \emph{De Nicola and Hennessy's testing preorders}, 
whose theory has been defined in \cite{DGHM09full} for a probabilistic 
version of \emph{CSP}.

Informally speaking, we can test a wireless network  via 
another wireless network  which can be composed 
 with the former (with respect to the operator ); 
that is, the network  is 
defined. Intuitively, the network 
 can be considered as an experiment 
in which the role of the testing component  is that of 
determining whether  satisfies some property for which 
the test has been designed for. The success of an experiment is 
denoted by the special construct of our calculus  mentioned above. 

Having this in mind, each computation of the network 
 induces a success outcome, denoting the probability 
of reaching a configuration in which the special clause  is enabled 
in such a computation. This induces a set of success outcomes for the network 
 by quantifying over all the possible computations for such a network. 

Knowing how to associate a set of success outcomes to a network, we can compare two 
networks  by quantifying over all possible tests , and comparing 
the sets of success outcomes of the experiments  and 
, provided that they are both defined. This leads to 
the definition of two testing preorders, the \emph{may-testing} preorder  
and \emph{must-testing} preorder , according to the way in which the sets 
of success outcomes for the two experiments above are compared. 

It is important to note that determining directly whether the statement  
 () 
is true is not easy, due to the quantification over all tests. 
Therefore there is the need to define a proof methodology for establishing 
if two networks can be related via the  () preorder. 
This is the main topic of our paper. 
To this end, we define an extensional 
semantics for our calculus of wireless networks; the actions in 
this semantics correspond to activities that can be observed 
by the external nodes. The main idea here is that of defining 
sound coinductive proof methods for the testing preorders, 
based on the extensional behaviour of networks. 

Since our calculus is equipped with local broadcast communication, 
we need to take care of some issues in the development of such proof 
methods; roughly speaking, the broadcast of a message to a 
set of external nodes can be simulated by a multicast of the same message 
which can be detected by the same set of external nodes. This leads 
to a non-standard definition of weak extensional actions, which will 
be used to define two coinductive relations between networks. The first one 
is the well-known \emph{simulation preorder} \cite{DGHM09full}; the second 
one is a novel preorder, called the \emph{deadlock simulation preorder}, 
which is obtained from the previous one by adding sensitivity to deadlock 
configurations. 
The main results of the papers are that, for a large class of networks, 
the simulation preorder is sound with respect to the may-testing preorder, 
while the inverse of the deadlock simulation preorder is sound with 
respect to the must-testing preorder. However, we provide a 
counterexample that shows that such proof methods fail 
to be complete.

The rest of the paper is organised as follows: 
in Section \ref{sec:background} we recall the mathematical 
tools needed for the development of our theory. 

In Section \ref{sec:lang} we define the syntax and intensional 
semantics of our calculus of wireless networks, and we prove 
some basic properties of our calculus. 

 In Section \ref{sec:compositional} we give the formal definition
  of the behavioural preorders between networks. This depends on how tests
  are applied to networks or more generally how networks are composed
  to form larger networks.  So we first define our composition
  operator , which is asymmetric, in
  Section~\ref{sec:comp.net} and then use it to develop the
  behavioural preorders  and  between networks. In
  Section~\ref{sec:justify} we return to our choice of composition
  operator , justifying it as the largest one which satisfies 
  three natural requirements. In addition, somewhat surprisingly, we show
  that any symmetric composition operator satisfying the natural requirements
  generates a degenerate behavioural theory. 

In Section \ref{sec:ext.sem} we define the extensional semantics of 
our calculus of wireless networks; here we also give the non-standard 
definition of weak extensional actions and we prove composition and 
decomposition results for them, with respect to the composition operator 
.

In Section \ref{sec:soundness} we define the notions of simulation and 
deadlock simulation preorders and we prove the main results of the paper, 
namely that the simulation preorder is sound with respect to the may-testing 
preorder, and the inverse of the deadlock simulation preorder is sound 
with respect to the must-testing preorder. Much of the technical 
development underlying these soundness results is relegated to the separate 
Section \ref{sec:technical}; this may be safely skipped by the uninterested reader.

In Section \ref{sec:completeness} we show that our proof methods fail to be 
complete; we also show the impossibility of defining 
a coinductive relation based on our notion of extensional actions, which characterises 
the may-testing relation. 

In Section \ref{sec:prob.routing} we consider an application of our theory 
by analysing a simple probabilistic, connectionless routing protocol, 
showing that it is behaviourally equivalent to a formal specification. 

We conclude our paper by summarising the topics we have covered and by illustrating 
the related work in Section \ref{sec:conclusions}. The topics 
covered in this paper were also the subject of an extended abstract 
\cite{extabs}.

\section{Background}
\label{sec:background}

In this Section we summarise the mathematical concepts, taken
from \cite{DGHM09full}, that will be needed throughout the paper.
First we introduce some basic concepts from  probability theory;
then we show how these can be used to model concurrent systems
which  exhibit  both probabilistic and non-deterministic
behaviour. 

Let  be a set; a function  is called a
(probability) sub-distribution over  if .  This quantity, , is called the mass of the
sub-distribution, denoted as . If ,
then we say that  is a (full) distribution. The support of a
sub-distribution , denoted , is the subset of
 consisting of all those elements which contribute to its mass,
namely . 

For any set , the empty sub-distribution  is the only sub-distribution with empty support, 
that is .
For each
, the point distribution  is defined to be the
distribution which takes value  at , and  elsewhere.  The set
of sub-distributions and distributions over a set  are denoted by
 and , respectively.

Given a family of sub-distributions ,
  is the partial 
real-valued function in  defined by
.
This is a partial operation because for a given  this sum  might not exist;
it is also a partial operation on sub-distributions because even if the sum does exist
it may be greater than .

Similarly, if  and  is a sub-distribution , then  is 
the sub-distribution over  such that .

It is not difficult to show that if  is a sequence 
of positive real numbers such that , 
and  is a family of sub-distributions 
over a set , then  
always defines a sub-distribution over .
Further, if  and each  is a distribution, then 
 is a distribution.

Finally, if  and  is a sub-distribution
over  then we use  to be the sub-distribution over 
defined by:

This definition can be generalised to two arguments functions; if  
 is a function, 
and  are two sub-distributions respectively 
over  and , then  denotes 
the sub-distribution over  defined as
\vspace{2 pt}

\noindent Now we turn our attention to probabilistic concurrent systems. The
formal model we  use to represent them is a 
generalisation to a probabilistic setting of 
Labelled Transition Systems (LTSs) \cite{milner}.
\begin{defi}\rm
A \emph{probabilistic labelled transition system} (pLTS) is a 4-tuple\\
, where
\begin{enumerate}[label=(\roman*)] \item  is a set of states,
\item  is a set of transition labels with a distinguished label ,
\item the relation  is a subset of ,
\item  is a (success) predicate over the states . 
\end{enumerate}
As usual, we will write  in lieu of .
\qed
\end{defi}\enlargethispage{9 pt}
Before discussing pLTSs, some definitions first: a pLTS whose state space is finite is said to 
be finite state; further, we say that a pLTS  
is finite branching if, for every , the set 
 \mu \in \Act_\tau 
 is finite.
 Finally, a finitary pLTS is one which is both finite 
state and finite branching.

We have included in the definition of a pLTS a success predicate  over states, which will
be used when testing processes. Apart from this, the 
only difference between LTSs and pLTSs is given by the definition of the transition relation; 
in the latter this is defined to be a relation (parametric in some action ) between states 
and distributions of states, thus capturing the concept of probabilistic behaviour. 

However, this modification introduces some difficulties when sequences of transitions performed by a 
given pLTS have to be considered, as the domain and the image of the transition relation do not 
coincide. To avoid this problem, we will focus only on distributions of states by defining 
transitions for them. The following Definition serves to this purpose:
\begin{defi}[Lifted Relations]
\label{def:lift}
Let  be a relation from states to 
 sub-distributions. Then
 is the smallest relation which satisfies 
\begin{itemize}
\item  implies 

\item If  is a finite index set and  for each  then
       whenever 
       .  \qed
\end{itemize}
\end{defi}
\noindent 
Lifting can also be defined for relations from states to probability distributions, 
by simply requiring  in the last constraint of the definition above.

Sometimes it will be convenient to consider also the lifting 
of relations of the form ; 
this is defined by first lifting the relation 
 to , 
by letting  iff  for some  such that 
. Then we obtain the relation  by 
applying Definition \ref{def:lift} to .

In a pLTS , each transition relation 
 can be lifted to
. 
With an abuse of notation, the latter will still be denoted as .

Lifted transition relations allow us to reason about the behaviour of pLTSs in terms of 
sequences of transitions; here we are mainly interested in the behaviour of a pLTS in 
the long run; that is, given a pLTS  
and a sub-distribution , we are interested in the sub-distributions 
 which can be reached from  after an unbounded 
number of transitions. 

For the moment we will focus only on internal actions of 
a pLTS, in which case the behaviour of a pLTS in the long run is captured by the 
concept of hyper-derivation:
\begin{defi}[Hyper-derivations]
\label{def:hypder}
  In a  pLTS a hyper-derivation consists of a collection of sub-distributions
, for ,
with the following properties:

 If  for each   and 
we call  a
\emph{hyper-derivative} of , and write
.
\qed
\end{defi}
\noindent
We will often write  in lieu of .

\begin{exa}
\label{ex:hyperderivative}
Let us illustrate how hyper-derivations can be inferred in a pLTS via a simple example. 
A central role in hyper-derivations will be played by the empty sub-distribution 
. Note that, in any pLTS , 
for any action  we have that .

Let us consider a pLTS whose state space is given by the set , with 
the only transition  
and with . 
This pLTS models a probabilistic experiment in which we continuously toss a fair coin 
until we obtain the outcome tail (represented by the state ), in which case we decree that the experiment succeeded; 
this last constraint is represented by letting .
It is well-known, from elementary probability arguments, 
that the probability of obtaining a success before the coin has been tossed  times
is , while in the long run the experiment will succeed with probability 
. 
This behaviour can be inferred by using hyper-derivations. For example, 
for any  we can consider the infinite sequence of transitions 


Note that the sequence of transitions above models a situation in which the 
experiment is stopped after the coin has been tossed  times. 
This is done by letting ; 
at least informally this means that the computation proceeds with 
probability  after the -th -transition has been performed.
The sequence of transitions above leads to the hyper-derivation 

That is, after  transitions have been performed the probability of having 
successfully terminated the experiment is .

Further, note that we can use hyper-derivations to describe the limiting behaviour of 
the experiment. In fact, we can consider the infinite sequence of 
transitions 

which leads to the hyper-derivation 

\end{exa}\medskip

\noindent Hyper-derivations can be seen as the probabilistic counterpart of the weak  action in 
LTSs; see \cite{DGHM09full} for a detailed discussion. 
Intuitively speaking, they represent fragments of computations obtained by performing only internal 
actions. The last constraint in Definition \ref{def:hypder} is needed since we introduced a success 
predicate in our model; as we see pLTSs as nondeterministic, probabilistic 
experiments, we require that a computation stops  
when the experiment succeeds, that is when a state  such that  
 has been reached.
States in which the predicate  
is true are called -successful.

Further, we are mainly interested in maximal computations of distributions. That is, we require a computation 
to proceed as long as some internal activity can be performed.
To this end, we  say that  if
\begin{itemize}
\item ,
\item for every  
,  implies . 
\end{itemize}
\noindent
This is a mild generalisation of the notion of \emph{extreme derivative} from \cite{DGHM09full}.
Note that the last constraint models exactly the requirement of performing some internal activity 
whenever it is possible;  In other words extreme derivatives correspond to a probabilistic
version of maximal computations.

\begin{exa}
Consider again the pLTS of Example \ref{ex:hyperderivative}. Here we have that 
the hyper-derivation , where , is not an extreme derivation, since  and . 
On the other hand, the hyper-derivation  is also an extreme derivation, 
since ; therefore .
\end{exa}

\begin{thm}\label{thm:hyper}
  In an arbitrary pLTS
  \begin{enumerate}[label=(\roman*)]
  \item  is reflexive and transitive,
  \item if  and , then ; 
   this is a direct consequence of the previous statement, and the definition of extreme derivatives,
  \item suppose , where  is an index set and 
  . If for any 
   for some , then , where 
  ,
  \item for all sub-distributions , there exists a sub-distribution  such that 
   .
  \end{enumerate}
\end{thm}
\begin{proof}
  See \cite{DGHM09full} for detailed proofs. 
\end{proof}

The last definition we need is that of convergent pLTSs. 
Intuitively these are pLTSs whose infinite computations 
have a negligible probability.
\begin{defi}[Convergence]
A pLTS  
is said to be convergent if  
for no state . 
\end{defi}
\noindent
At least informally,  means that 
there exists a computation rooted in  which contains 
only probability sub-distributions which 
can always perform a -action. 
See \cite{DGHM09full}, Section \textbf{6} 
for a detailed discussion on divergence in pLTSs.
The main property we will require from convergent pLTSs 
	is the following:

\begin{prop}
\label{prop:convergent.moves}
Let  be a distribution in a convergent pLTS. 
If  then . 
\end{prop}
\begin{proof}
This is an immediate consequence of \emph{Distillation of 
Divergence}, Theorem \textbf{6.20} of \cite{DGHM09full}.
\end{proof}

\section{The calculus}
\label{sec:lang}
In this Section we introduce our calculus for modelling wireless networks. 
In this calculus, a wireless network is modelled as a pair of the form 
, where  is a digraph representing the 
topology of a wireless network and  is a term which assigns code to 
nodes. 

The syntax of our calculus is presented in Section \ref{sec:syntax}; 
here we also give some basic examples of wireless networks. 
In Section \ref{sec:int.sem} we formalise how networks evolve by 
introducing an intensional semantics for our calculus; 
finally we prove some basic properties of our calculus in 
Section \ref{sec:properties}.

\subsection{Syntax}
\label{sec:syntax}
The calculus we present is designed to model broadcast systems, particularly wireless 
networks, at a high level. We do not deal with low level issues, such as collisions of 
broadcast messages or multiplexing mechanisms \cite{tanenbaum}; instead, 
we assume that network nodes use protocols  at the \emph{MAC level} \cite{macsurvey} 
to achieve a reliable communication 
between nodes.

Basically, the language will contain both primitives for sending and receiving messages and 
will enjoy the following features:
\begin{enumerate}[label=(\roman*)]
\item communication can be obtained through the use of different channels; although the physical medium 
for exchanging messages in wireless networks is unique, it is reasonable to assume that network nodes use 
some multiple access technique, such as \emph{TDMA} or \emph{FDMA} \cite{tanenbaum}, to setup and 
communicate through virtual channels,
\item communication is broadcast; whenever a node in a given network sends a message, it can be detected by 
all nodes in its range of transmission,
\item communication is reliable; whenever a node broadcasts 
a message and a neighbouring node (that is, 
a node in the sender's range of transmission) is waiting 
to receive a message on the same channel, then the 
message will be delivered to the receiver. This is not 
ensured if low level issues are considered, 
as problems such as message collisions \cite{macsurvey}
and nodes synchronisation \cite{time} arise .
\end{enumerate}

\begin{figure}[t]
\rule{\linewidth}{0.5mm}
  

  \caption{Syntax \label{fig:syntax}}
 \rule{\linewidth}{0.5mm} 
\end{figure}

\noindent The language for system terms, ranged over by  is given in
Figure~\ref{fig:syntax}.  Basically a system consists of a collection
of named nodes at each of which there is some running code.  The
syntax for this code is a fairly straightforward instance of a standard
process calculus, augmented by a probabilistic choice; code
descriptions have the usual constructs for channel based communication,
with input  being the unique binder. 
This gives rise to the standard notions of alpha-conversion, 
free and bound occurrences of variables in system terms and 
closed system terms.

We only consider the 
sub-language of well-formed system terms in which all node names have
at most one occurrence.  We use  to range over all closed
well-formed terms.  A (well-formed) system term can be viewed as a
mapping that assigns to node names the code they are executing. A
sub-term  appearing in a system term  represents node
 running code .
In the following we make use of standard conventions; we omit trailing 
occurrences of  and we use  to 
denote the system term .

Additional information such as the
connectivity between nodes of a network is needed to formalise
communications between nodes. 
Network connectivity is represented by a graph ; here  is a finite set of nodes
and  is an irreflexive relation 
between nodes in . 
Intuitively,  models the possibility for node  to detect 
broadcasts fired by .
 
We use the more graphic notation  to mean  and 
 for . 
Similarly we use  to denote
. 
Sometimes we also use the notations  for 
 and 
 to denote either  
or .

A \emph{network} consists of a pair ,
representing the system , from , executing relative to the connectivity graph
.  All nodes occurring in , , will appear in
 and the effect of  running  the code at 
will depend on the connectivity of  in .  But in general
there will be nodes in  which do not occur in ; let
;  we call this set 
 the \emph{interface}
of the network , and its elements are called \emph{external nodes}.  Intuitively
these are nodes which may be used to compose the network  with other networks, or to place code for testing the
behaviour of .  

In the following we use the meta-variables  to 
range over networks. Also, the notation introduced for system 
terms and connectivity graphs is extended to networks in 
the obvious way; for example, if , 
,  and 
 denotes .

\begin{figure}[t]
  
                         

 
  \caption{Example networks}
  \label{fig:ex1}
\end{figure}

\begin{exa}\label{ex:ex1}
Consider  described in Figure~\ref{fig:ex1}. 
There are six nodes, three occupied by code ,  and , and three in the interface
 ,
 and . Here, and in future examples,  we  differentiate between the interface
and the occupied (internal) nodes using shading. Suppose the code at nodes is given by

Then  can receive input from node  at its interface along the channel
; this is passed on to the internal node  using channel , where it is transformed
in some way, described by the function \footnote{For example, 
if we assume the set of closed values to be , f could be the mapping 
.}, and then forwarded to node , where  of the
time it is  broadcast to the external nodes  and . The remainder of the
time the message is lost. 

The network  has the same interface as , but has an extra internal node
 connected to ,
and  is only connected to one interface node  and the internal node . The nodes  and  
have the same code running as in , while nodes  and  will run the code

Intuitively, the behaviour of  is more complex than that of ; indeed, there is the 
possibility for a computation of  to deliver a value only to one between the external nodes 
 and , while this is not possible in . However,  of the times this message 
will be delivered to both these nodes, and thus it is more reliable than .
Suppose now that we change the code at the intermediate node  in \calM, 

In  the behaviour at the node  is non-deterministic; it may act like a perfect forwarder,
or one which is only 50\% reliable.  Optimistically it could be more reliable than , or pessimistically it 
could be less reliable than the latter. Further, there is no possibility for the network  to forward 
the message  to only one of the external nodes , , so that its behaviour is somewhat less complex than 
 that of .

As a further variation let  be the result of replacing the code at  with 

Here the behaviour is once more deterministic, with the probability that the message will be
eventually transmitted successfully through node  approaching  in the limit. Thus, this 
network is as reliable as ,  when the latter is viewed optimistically.
\end{exa}
\subsection{Intensional Semantics}
\label{sec:int.sem}
We now turn our attention on the operational semantics of networks.  
Following \cite{DGHM09full}, processes will be interpreted as probability 
distributions of states; such an interpretation is encoded by the function  
defined below: 

Sometimes we will need to consider the probability distribution associated to system terms; 
this is done by letting 

where  represents a distribution over , 
obtained by a direct application of Equation \eqref{eq:expected} 
to the function  which maps states into system terms. 
Similarly,  is obtained by 
applying Equation \eqref{eq:twoargexpected} to the function 
.

The intensional semantics of networks is defined incrementally. 
We first define a pre-semantics for states, which is then used 
for giving the judgements of (state based) networks.

\begin{figure}[t]
\rule{\linewidth}{0.5mm}
  
  \caption{Pre-semantics of states}
  \label{fig:stsem}

\rule{\linewidth}{0.5mm}
\end{figure}

The pre-semantics for states takes the form

where  is a closed state, that is containing no free occurrences of a variable,  
  is a distribution of states and  can take one of the forms  or . 
The deductive rules for inferring these judgements are given in Figure \ref{fig:stsem}
and should be self-explanatory. It assumes some mechanism for evaluating closed data-expressions
 to values . Note that we assume that definitions have the form , 
where  is a state; this is because actions for definitions are inherited by the state that is 
associated to them, and judgements are not defined for (probabilistic) processes. Also, note that 
we have a special state  for which no rule has been defined. The role of 
this construct will become clear in Section \ref{sec:compositional}.

\begin{figure}[t]
\rule{\linewidth}{0.5mm}
  
  \caption{Intensional semantics of networks}
  \label{fig:opsem}

\rule{\linewidth}{0.5mm}
\end{figure}

Judgements in the intensional semantics of networks take the form

where   is a network connectivity,  is a system from , 
and  is a distribution over ; 
intuitively this means that relative to the  connectivity  the 
system  can perform 
the action , and with probability  be transformed into 
the system , for every  . 
 The action labels can take the form
\begin{enumerate}[label=(\roman*)]
\item receive, , meaning that the value  is detected on 
channel  by all nodes in  which are reachable from  in ,

\item broadcast, : meaning the node  (occurring in
  , and therefore in ) broadcasts the value  on
  channel  to all nodes directly connected to  in 

\item internal activity, , meaning some internal activity performed 
by node .
\end{enumerate}

\noindent The rules for inferring judgements are given in
Figure~\ref{fig:opsem}. Here we have omitted the 
symmetric counterparts of rules  and 
.
Rule  models the capability for a node to broadcast a value  through 
channel , assuming the code running there
is capable of broadcasting along . 
Here the distribution  is in turn obtained from the residual of the state  after the broadcast action. 
\begin{exa}
  Consider the simple network  where  is 
  an arbitrary connectivity graph and the code  has the form 
. 

The pre-semantics of states determines that , 
using Rule 
Thus according to the rule  
\Rlts{broad} we have the judgement

\end{exa}\vspace{6 pt}

\noindent Rules  and  express how a node reacts 
when a message is broadcast; the first essentially models the capability of a node which is 
listening to a channel , and which appears in the sender's range of transmission, 
to receive the message correctly. The other two rules model situations in which a node is 
not listening to the channel used to broadcast a message, or it is not in the range of transmission of the sender;
 In both cases this node cannot detect the transmission at all.

The rules  and  model internal activities performed 
by some node of a system term; the latter (together with its symmetric 
counterpart) expresses the inability for a node which 
performs an internal activity to affect other nodes in a system term. Here again,  
is a distribution over , this time obtained by instantiating Equation \eqref{eq:twoargexpected} to 
the function .

Finally, rules  and  describe how communication 
between nodes of a network is handled; here the result of a synchronisation between 
an output and an input is again an output, thus modelling broadcast 
communication \cite{Prasad95}.





\subsection{Properties of the Calculus}
\label{sec:properties}
We conclude this section by summarising the main properties enjoyed by the 
intensional semantics of our calculus.

Here (and in the rest of the paper) 
it will be convenient to identify networks 
and distributions of networks
up to a structural congruence relation . This is first 
defined for states as the smallest equivalence relation which is 
a commutative monoid with 
respect to  and , and which satisfies the equations 
 if , 
 if  and 
 if 
.
For system terms, we let  
be the smallest equivalence relation which is a commutative 
monoid with respect to  and , and which satisfies the 
equation  implies  
for any node . Finally, we let  iff  and . 
Structural congruence is also defined for distributions of 
networks via the lifted relation . 
With an abuse of notation, the latter is still 
denoted as .

The properties that we prove in this section give an explicit 
form to the structure of a network  and 
a distribution  in the case that an action  can be inferred in the intensional semantics 
presented in Section \ref{sec:int.sem}.

\begin{prop}[Tau-actions]
\label{prop:tau}
Let  be a network; then 
 if and only if 
\begin{enumerate}[label=(\roman*)]
\item ,
\item 
\end{enumerate}
\end{prop}

\begin{proof}[Outline of the Proof]
We first need to prove a similar statement for states. 
Let  be a state; then  if and 
only if  for some  such that 
. The two implications 
of this statement are proved separately. 



To prove Proposition \ref{prop:tau} suppose first 
that  for 
some distribution . We show that 
,  
by structural induction on the proof of the derivation above. 

If the last rule applied is , then  
for some  such that  , . 

Since  then  for some 
process  such that , which also gives 
. 
By definition of structural congruence . Further, , and there 
is nothing left to prove.

If the last rule applied is , then 
 for some  such that 
; further, . 
By inductive hypothesis we have that  
and . 
By performing some simple calculations we find that 
 
and .

Conversely, suppose that ; in 
this case it suffices to perform a rule induction on the proof of 
the equivalence above to show that , where .
\end{proof}

\begin{prop}[Input]
\label{prop:input}
For any network  we have that 
 iff
\begin{enumerate}[label=(\roman*)]
\item , 
\item , 
\item for any , , 
\item for any , either  
or , 
\item .
\qed
\end{enumerate}

\end{prop}

\begin{prop}[Broadcast]
\label{prop:broadcast}
Let  be a network; then  for some  iff 
\begin{enumerate}[label=(\roman*)]
\item , 
where , 
\item , 
\item .
\qed
\end{enumerate}

\end{prop}

\noindent An immediate consequence of the results above is 
that actions in the intensional semantics 
are preserved by structurally congruent networks.
\begin{cor}
\label{cor:reduction}
Let  be two networks 
such that . 
If  then 
 for some 
 such that .
\qed
\end{cor}

Another trivial consequence that follows from 
the results above is that the intensional 
semantics does not change the structure 
of a network. 

\begin{defi}[Stable distributions]
A (node)-stable sub-distribution  is one for which 
whenever  
it follows that .
A distribution over networks is said to 
be (node)-stable if it has the form 
, and  
is a stable sub-distribution in 
.
\end{defi}
\begin{cor}
\label{cor:stable.dist}

Whenever  
then  is node-stable; further, 
for any  
we have that .\qed
\end{cor}


\section{Compositional Reasoning for Networks}
\label{sec:compositional}


The aim of this Section is to  
develop preorders of the form

Intuitively this means that the network  can be replaced by
, as a part of a larger overall network, without any loss of
behaviour. The intention is that the internal structure of the
  networks  should play no role in this comparison; the
  names used to identify their internal stations and the their
  communication topology should not be important. Intuitively the only
  behaviour to be taken into account in this extensional comparison is
  the reception of values at the their interface, the values
  subsequently broadcast at the interface.  

To formalise this concept 
we need to  say how networks are composed to form larger networks.
In  
Section~\ref{sec:comp.net} we propose a specific composition  operator, 
for this purpose, and briefly discuss  its properties.  We then use
this operator in Section~\ref{sec:ts} to say how network behaviour is determined. 
In Section~\ref{sec:maypreord}, we  give 
the formal definition of the behavioural preorders, in a relatively standard manner following 
\cite{dnh}; this section also treats some  examples.
The nature of these preorders depends on our particular choice of
composition  operator . In Section~\ref{sec:justify} we return
to this point and offer a justification for our choice; this section may
be safely ignored by the reader who is uninterested in this subtlety.

However first let us reconsider the informal requirements of the
proposed behavioural preorder (\ref{eq:behaviour}) above. 
We have already mentioned that it should not depend on the internal 
structure of . But equally well  
should not be able to  make any assumptions about the topology 
of their external  environment. 
\begin{figure}


                                   




 \caption{A well-formed and an ill-formed network}
\label{fig:wellformed}
\end{figure}

\begin{exa}
\label{ex:wellformed}
Consider the networks  and  depicted in 
Figure \ref{fig:wellformed}. At least intuitively the extensional 
behaviour of these two networks is the same: a broadcast of 
value  along channel  can be detected by the external nodes 
 and . However  makes an assumption about the
external environment, namely that there is a connection between
the external nodes  and . This slight difference 
can be  exploited to distinguish between them behaviourally.
 Suppose that we place the code  at 
node  and the code  at 
node  to test the behaviour of both networks. 
In practice, let , and consider the 
networks  and 
. 
In the first network the node  can detect both the 
broadcast fired by node  and node , leading to 
a state in which the special action  is enabled. 
However, the same is not possible in , since there 
is no connection between the external nodes  and . 
That is, node  can only detect the broadcast fired by 
node , ending in a state in which the special action 
 remains guarded by an input. As we will see later, 
the clause  plays a crucial role in distinguishing 
networks.
\end{exa}

The problem in  Example \ref{ex:wellformed} is caused 
by the presence of a connection between the two 
external nodes in the network  
of Figure \ref{fig:wellformed};
intuitively this represents an assumption of 
 about its external environment. To avoid this 
problem, we focus on a specific 
class of networks in which connections between 
external nodes are not allowed. Also, we 
require external nodes to have at least a connection 
with some internal node.

\begin{defi}[Well-Formed Networks]
\label{def:well.formed}
A network  is well-formed iff 
\begin{enumerate}[label=(\roman*)]
\item whenever  then 
either  or ,
\item whenever  and  
for no , then . 
\qed
\end{enumerate}
\end{defi}
\noindent 
Henceforth we will only focus on well-formed networks, unless stated otherwise. 
We denote the set of well-formed networks as .

Finally let us provide some definitions which will be useful in the sequel.
Let  be a (well-formed) network. We say that a 
node  is an \emph{input node} if  for some \footnote{Note that by well-formedness this implies 
.}; conversely, if a node  is 
such that  we say 
that  is an \emph{output node}. If we let  and  it is easy to check that 
.




\subsection{Composing Networks}
\label{sec:comp.net}
One use of composition operators is to enable compositional reasoning. For example the task of establishing

can be simplified if we can discover a common component, that is some  such that 

and 

for some composition operator ; then (\ref{eq:behav}) can be reduced to establishing 

assuming that the behavioural preorder in question, , is preserved by the composition operator . 

However another use of a composition operator is in the definition of the behavioural preorder  itself.
Intuitively we can define 

to be true if for every component  which can be composed
with both  and , the external
observable behaviour of the composite networks  and  are related in some appropriate way. 
is a \emph{testing network} which is probing  and
 for behavioural differences, along the lines used informally in 
Example~\ref{ex:wellformed}.  Intuitively this should
be \emph{black-box testing}, in which the tester, namely ,
should have no access to the internal stations of the networks being
tested, namely  and . All it can do is
place code at their external interfaces,  to transmit values and
examine the subsequent effects, as seen again at the interfaces.  

\begin{defi}[Composing networks]\label{def:comp.nets}
For any two networks  and 
let  be given by:

The composed connectivity graph  
is defined by letting  and  .
\qed 
\end{defi}
\noindent
The intuition here is that the composed network 
is constructed by \emph{extending} the network under test, ,
allowing code to be placed at its interface, and allowing completely
fresh stations to be added. These fresh stations can be used by the
tester to compute the results of probes made on . 

\begin{prop}
\label{prop:testp.assoc}
Suppose . 
Then 
\begin{enumerate}
\item , whenever it is defined 

\item , whenever both are defined 
\end{enumerate}
\end{prop} 
\begin{proof}
The two statements are proved separately. The proofs are given 
in a  separate appendix, Appendix \ref{sec:operator.results}; 
see pages \pageref{proof:testP.closed} and \pageref{proof:testP.assoc}.
\end{proof}

\begin{figure}[t]

  \caption{Network composition via \label{fig:notsymmetric}}
\end{figure}
There is an inherent asymmetry in the definition of our
composition operator; in  we allow  to
place code at the interface nodes of  but not the converse. As
a result the operator is not in general symmetric, as can be seen from
Example \ref{ex:notsymmetric}. 
\begin{exa}[ is asymmetric]
\label{ex:notsymmetric}
Let  be the networks depicted in Figure~\ref{fig:notsymmetric}. 
Here the network  is well-defined and 
depicted on the right of Figure \ref{fig:notsymmetric}. Intuitively, this is 
the network obtained by extending  with the information provided 
by ; these include the code running at nodes , a connection 
between such nodes, and a connection from , respectively, to a 
fresh node , whose running code is left unspecified. 
The network  is well-defined because 
none of the connections specified in  involve the node ; that is, when extending  with  it is ensured 
that the latter can interact with the node  only via the nodes . 

On the other hand, the composition  is not defined. Intuitively,  
in  nodes  and  can interact with the external environment only 
via the external node . This is in contrast 
with  the definition of , where node  can be used to broadcast messages 
to such nodes.
\end{exa}

One might wonder if a symmetric composition operator could be used in place of our 
; this point is discussed at length in Section~\ref{sec:justify}.
Nevertheless  is a natural operator, and the next result shows that it can be
used to construct all non-degenerate networks starting from single nodes. 
Let  be the collection of networks which contain exactly one occupied node, 
that is . 


\begin{prop}\label{prop:network.decomp}
  Suppose  is a network such that 
   is not empty.  Then  for some and .
\end{prop}
\begin{proof}
See Appendix \ref{sec:operator.results}, Page \pageref{proof:generators}.
\end{proof}
\noindent 
We use the term \emph{generating networks} to refer to elements of . 
The import of Proposition \ref{prop:network.decomp} is that all 
non-trivial well-formed networks can be constructed from basic generating 
networks, using . 

\leaveout{
The last property we prove for the extension operator  is that 
it is deeply connected to the symmetric operator  defined in Example \ref{ex:mas.op}. 
In fact, it turns out that every network of the form  
can be rewritten as , where  is determined 
completely by the topological structure of  and . Conversely, if 
 is defined, we can rewrite it as  
for some network .

\begin{defi} 
Let  be two networks. 
\begin{enumerate}[label=(\roman*)]
\item The \emph{symmetric counterpart} of  
with respect to ,   
is the network  such that 

\item The \emph{extension counterpart} of  with respect to 
 is the network  such that 

\end{enumerate}
\qed
\end{defi}

\begin{prop}
\label{prop:change}
Let  be two well-formed networks. 
\begin{enumerate}
\item if , then both 
 and  are defined and well-formed,
\item if  then , 
\item if  then , 
\item ,
\item .
\end{enumerate}
\end{prop}

\begin{proof}
See \cite{phdthesis}, propositions \textbf{4.2.3}, \textbf{4.2.4} and \textbf{4.2.5}.
\end{proof}

\begin{figure}




 \caption{Three networks  and , such that 
, .}
\label{fig:change}
\end{figure}

\begin{exa}[Change of connectivity graphs]
Consider the networks  and  depicted in 
Figure \ref{fig:change}. 
Here it is easy to show that ; conversely, 
. In practice,  is 
defined by adding to  the connections between internal nodes 
of  and internal nodes of  which are defined in the former network. 
On the other hand,  is defined by removing in 
 all the nodes that are internal in , together with the 
associated connections. 

This ensures that  and  are 
defined; further, by Proposition \ref{prop:change} 
they are also equivalent.
\end{exa}

The technical result below will be useful in the following.
\begin{lem}
\label{lem:input.struct}
Let , ,  be respectively two networks and 
a generating network such that ,  
are defined. Suppose also that .
If  then .
\end{lem}

\begin{proof}
Let , ; by straightforward calculations 
we can check that  
if and only if . 
Then if  it follows that , 
which by hypothesis gives that . But this is equivalent 
to .
\end{proof}




}
\subsection{Testing Structures}
\label{sec:ts}

The introduction of the composition operator  allows the development of 
a behavioural theory based on a probabilistic 
generalisation of  the  \textit{De Nicola} and \textit{Hennessy}  testing preorders \cite{dnh}. In order to develop 
such a framework, we will exploit the mathematical tools introduced in Section \ref{sec:background}; our aim is 
 to be able to relate networks with different connection topologies.

In our framework, as has already been indicated, testing can be summarised as follows: the network to be tested  is 
composed with another one, usually called a \emph{testing network}. 
The composition of these two networks is then isolated from the external environment, 
in the sense that no external agent (in our case nodes in the interface of the composed network) 
can interfere with its behaviour; we will shortly present how such a task can be 
accomplished. The composition of the two networks isolated from the external 
environment takes the name of \emph{experiment}. 



Once these two operations (composition with a test and isolation from the external environment) 
have been performed, the behaviour of the resulting experiment is analysed to check whether 
there exists a computation that yields a state which is successful. This task can be 
accomplished by relying on testing structures, which will be presented shortly.

At an informal level, successful states in our language coincide with those associated with networks 
where at least the code running at one node has the special \emph{action}  enabled. 
Since networks have probabilistic behaviour, each computation will be associated with 
the probability  of reaching a successful state; thus, every experiment will be 
associated with a set of success probabilities, by quantifying over all its computation.

Let us now look at how the procedure explained above can be formalised; the topic 
of composing networks has already been addressed in detail in Section \ref{sec:comp.net}, 
in which we defined the operator  and proved basic properties for it. To 
model experiments and their behaviour, we rely on the following mathematical structure.

\begin{defi}\label{def:testing}
  A \emph{Testing Structure} (TS) is a triple 
 where 
\begin{enumerate}[label=(\roman*)]
\item  is a set of states,

\item the relation  is a subset of ,

\item  is a success predicate over , that is . \qed
\end{enumerate}
\end{defi}
\noindent
Testing structures can be seen as (degenerate) pLTSs where the only
possible action corresponds to the internal activity , and the
transition  is defined to coincide with the reduction
relation . 
 Conversely every pLTS automatically determines a testing structure,
by concentrating on the relation .

Our goal is to turn a network into a testing structure. This amounts to defining, 
for networks, a reduction relation and the success predicate .
As we have mentioned in the beginning of this section, when converting a 
network into a testing structure, we want to  make it 
isolated from the external environment. 

When considering simpler process languages, like \emph{CCS} or \emph{CSP} 
(and, more generally, their probabilistic counterparts), 
processes are converted into testing structures by identifying the reduction 
relation with the internal activity ; that is, processes are 
not allowed to synchronise with some external agent via a visible action. 

Networks, however, are more complicated objects; here the nature of 
broadcast is non-blocking, meaning that a broadcast can be 
fired by a node in a network without requiring any synchronisation 
with a (possibly external) node. Thus we expect broadcast 
actions to induce reductions when converting a network into a testing structure.

On the other hand, input actions always originate from non-internal 
nodes. Hence they can be seen as external activities which can influence 
 the behaviour of a 
network. Therefore, input actions should not be included in the definition 
of the reduction relation for networks. 

Finally, the success predicate  is defined to be true for exactly 
those networks in which the success clause  is enabled in 
at least one node.


\begin{exa}\label{ex:ts}
The main example of a TS is given by

where 
\begin{enumerate}[label=(\roman*)]
\item  whenever
  \begin{enumerate}[label=(\alph*)]
  \item  for some 

  \item or,  for some value , node name  and channel 
\end{enumerate}
  
\item ~

\end{enumerate}
If  for some system term , we say that a network , where 
 is an arbitrary connectivity graph, is -successful, or simply successful. 
Note that when recording an -success we do not take into account the node involved.
  \end{exa}
\noindent
As TSs can be seen as pLTSs, we can use in an arbitrary TS the various constructions
introduced in Section~\ref{sec:background}. Thus the reduction relation 
  can be lifted to 
 and 
we can make use of the concepts of 
hyper-derivatives and extreme-derivatives, introduced in Section \ref{sec:background}, to 
model fragments of executions and maximal executions of a testing structure, respectively.
Hyper-derivations in testing structures are denoted with the symbol , while 
we use the symbol  for extreme derivations.

Below we provide two simple examples that show how to reason about the behaviour of the testing 
structures presented in Example \ref{ex:ts}.


\begin{exa}
\label{ex:hypder}
Consider the testing structure associated with the network  in
the center of Figure \ref{fig:counterex}, where the code  is given
by the definition . We can
show that, in the long run, this network will broadcast message  to
the external location  by exhibiting a hyper derivation for it
which terminates in the point distribution .  If we let  denote the configuration ,  we have the following 
hyper-derivation:

\noindent
Let . 
It is straightforward to check  that

and therefore we have the hyper-derivation . 
\end{exa}

An arbitrary network  can be tested by another (testing) network  provided 
 is well-defined.
Executions of the resulting testing structure will then be checked to establish whether 
the network  satisfies a property the test was designed for; in such a case, the testing component 
of an experiment will reach a -successful state. 

Executions, or maximal computations, correspond to
extreme derivatives in the testing structure associated with , 
as defined in Section~\ref{sec:background}. 
Since the framework is probabilistic, each execution (that is  extreme derivative)  will be associated with 
a probability value, representing the probability that it will lead to an -successful state.
Since the framework is also nondeterministic the possible results of this test application is given by a
non-empty set of probability values. 
\begin{defi}[Tabulating results]\label{def:resultsets}
 The \emph{value} of a sub-distribution in a TS is given by the function   
,  defined by
    .
Then the set of possible results from a sub-distribution   is defined by  
. 
\qed
\end{defi}




\begin{exa}

Let  be the network from Example \ref{ex:hypder} and 
consider the testing network  given in 
Figure \ref{fig:counterex}, where the code is determined by
. 
It is easy to check that  is well-defined and is equal to 
, where  is the connectivity 
graph containing the three nodes  and having the connections from  and .
So consider the testing structure associated with it; recall that we have the definition 
. 
For convenience  let  as in the previous example, 
 and .
Then we have the following  hyper-derivation for :

  were we recall that  denotes the empty sub-distribution, that is the one with .
  We have therefore the hyper-derivation 
  
  Further, the above hyper-derivation satisfies the constraints required by , defined in Section~\ref{sec:background}, and 
  therefore we have the extreme derivative . 
  Since     we can therefore deduce that .
  \end{exa}

\subsection{The behavioural preorders}
\label{sec:maypreord}

We now combine the concepts of the previous two sections to obtain our behavioural preorders.
We have seen how to associate a non-empty set of probabilities, tabulating 
the possible outcomes from applying the test  to the network . 
As explained in \cite{DGHM09full}
there are two natural ways to compare such sets, optimistically or pessimistically. 

\begin{defi}[Relating sets of outcomes]
\label{def:relsets}
Let ,  be two sets of values in 
.
\begin{enumerate}[label=(\roman*)]
\item The \emph{Hoare's Preorder} is defined by letting 
 whenever 
for any  there exists  
such that .
\item The \emph{Smith's Preorder} is defined by letting 
 if 
for any  there exists  
such that .\qed
\end{enumerate}
\end{defi}
\noindent Given two networks  we can relate their behaviour, 
when extended with a testing network , by comparing the success 
outcomes of  and  (provided both these 
networks are defined) via Definition \ref{def:relsets}. 
We can go further and consider what is the relationship between such sets of 
outcomes with respect to all possible tests  which can be used 
to extend the networks .
 
\begin{defi}[Testing networks]
\label{def:maytest}
 For  
  such that
  , 
  , we write 
  \begin{enumerate}
  
  \item  iff 
for every (testing) network  such that both 
         and   are defined,
         
  \item  iff for every (testing network)  such that 
  both  and  are defined, 
  
  \end{enumerate}
We use  as an abbreviation for   and 
. The relation  is defined similarly. 
Finally, we say that  iff both  and 
 hold, and  iff  
and .
~\qed
\end{defi}

Some explanation is necessary for the requirement on the interface of networks we have 
placed in Definition \ref{def:maytest}. 
This constraint establishes that two networks  and  are always distinguished 
if the sets of their input or output nodes differ. As we already mentioned, external nodes 
can be seen as terminals that can be accessed by the external environment to interact with 
the network. Roughly speaking, the constraint we have placed corresponds to the intuition 
that the external environment can distinguish two networks 
 and  by simply looking at the terminals that it can use to interact with these 
two networks. 



\begin{figure}
\begin{tikzpicture}
            \node[state](o1){};
           \node[state](o2)[below=of o1]{};
           \node[state](e)[left  =of o2]{}; 
           \node[state](o)[right =of o1]{}; 
 \path[tofrom]
       (o1) edge [thick] (o2)
       (o1) edge[thick] (o)
       (o2) edge[thick] (o); 
   \begin{pgfonlayer}{background}
    \node [background,fit=(o1) (o2) (o) (e)] {};
    \end{pgfonlayer}
  \end{tikzpicture}

\caption{ A test} 
\label{fig:testex}
\end{figure}

\begin{exa}\label{ex:testing} 
  Consider the testing network 

where the connectivity is described in Figure~\ref{fig:testex}. This can be used to test the networks 
 from Example~\ref{ex:ex1} in the testing structure of Example~\ref{ex:ts}. 
 
Intuitively the test sends the value  along the channel  at the node , awaits for results along the channel
 at the nodes  and . These results are processed at node , where success might be announced. 

The combined network
 is deterministic in this TS, although probabilistic, and so has only one extreme
derivative; .


A similar calculation shows that  ;  it 
therefore follows that  and . 

Consider now the networks  from Example \ref{ex:ex1}. 
Here  is both probabilistic and nondeterministic, and
 . Moreover 
 we have 


The combined network  is also deterministic, although it has 
\emph{limiting behaviour}; . Thus, in this case we 
have both  and 
. 
Further, we have that 
, but 
.
 \end{exa}

\begin{figure}


                                   



 \caption{Broadcast vs Multicast}
\label{fig:bcast}
\end{figure}

 \begin{exa}[Broadcast vs Multicast]\label{ex:bcast1}
   Consider the networks  and  in Figure~\ref{fig:bcast}. 
Intuitively in  the value  is (simultaneously) broadcast to both nodes 
and  while in  there is a multicast. More specifically  receives  from
mode  while in an independent broadcast  receives it from . 

This difference in behaviour can be detected (when we compare the networks optimistically) by the testing network

assuming  is different than ; here we assume  is the simple network which connects  to 
.  
Both  and  are well-formed and note that
they are both non-probabilistic. 

Because  simultaneously broadcasts to  and  the second value received by
 is always  and therefore the test never succeeds; . 
On the other-hand there is a possibility for the test succeeding when applied to , 
.  This is because in  node  might first transmit  to 
 after which  transmits  to ; now node  might transmit the value  to  
and assuming it is different than  we reach a success state. It follows that 
. 

Note that we can slightly modify the test  to show that we 
also have . To this end, let 

In this case we have that , while 
, and by definition 
.

One might also think it possible to use the difference between broadcast and multicast to design a test 
 for which 
 
and . 

For example, if we let  we obtain that . 
This is because because in  the second value received by  is always . 
However we also have that , since the
simultaneous broadcast in  can be simulated by a multicast in , 
by node  first broadcasting to  followed by
 broadcasting to . As this line of reasoning is independent from the test , 
it also applies to all those networks that can be used to test the behaviour of  and ; 
this leads to the intuition that , which will be proved formally later 
as a consequence of Example \ref{ex:bcast} and Theorem \ref{thm:may.sound}. 
Similarly, Theorem \ref{thm:must.sound} shows that .

\end{exa}

One pleasing property of the behavioural preorders is that they allow 
compositional reasoning over networks.
\begin{prop}[Compositionality]
\label{prop:compmay}
Let  be two networks such that  
(), and 
let  be another network such that both  and  
are defined. Then ) 
().
\end{prop}
\begin{proof}
A direct consequence of  being both associative and interface preserving.
\end{proof}
\noindent
We end this section with an 
application of this compositionality result.



\begin{figure}[t]
                                

  \caption{Two networks with a common sub-network\label{fig:similar}}


   \caption{Decomposition of the networks  and \label{fig:decomp}  }
\end{figure}
\begin{exa}
Consider the networks  and  in Figure \ref{fig:similar}, where the codes at the various
nodes are given by

 It is 
possible to write both of them respectively as  and 
, where the networks  and  
are depicted in Figure \ref{fig:decomp}. In order to prove that  (), it is therefore sufficient to focus on their 
respective sub-networks  and ,  and prove
 (). The equivalence of the two 
original networks will  then follow from a direct application 
of Proposition \ref{prop:compmay}.
\end{exa}


\subsection{Justifying the operator }
\label{sec:justify}


Here we revisit Definition~\ref{def:comp.nets} and in particular investigate the possibility
of using alternative composition operators. The remainder of the paper is independent of this
section and so it may be safely skipped by the uninterested reader. 

Here we take a more general approach to composition; rather than give a particular operator we discuss
natural properties we would expect of such operators. Let us just presuppose a \emph{consistency predicate}
 on pairs of networks determining when their composition should be defined. The only requirement
on  is that whenever it is  defined the resulting composite network is well-formed. 
Since the composite network should be  determined by that of its components this amounts to requiring that 


\noindent Given a consistency predicate satisfying (\ref{eq:iscons}) we can now generalise Definition~\ref{def:comp.nets} to 
give a range of different composition operators. 
\begin{defi}[General composition of  networks]\label{def:gen.comp.nets}
  Let  be a consistency  predicate on networks in \nets satisfying 
  (\ref{eq:iscons}). Then we define the associated partial composition
  relation by:

The connectivity graph  is defined as in Definition~\ref{def:comp.nets}.
\qed 
\end{defi}



\begin{exa}\label{ex:mas.op}
  Let  be the partialss binary predicate defined by letting 
 whenever
\begin{itemise}
\item 
\item    if and only if 
 , for every  and , 
 \item  if and only if 
 , for every  and ,
\end{itemise}

\noindent By definition this satisfies the requirement (\ref{eq:iscons}) above,
and intuitively it only allows the composition whenever the two
individual networks agree on the interconnections between internal and external nodes.

For notational convenience we denote the operator  with 
. It is easy to check that this operator 
is both associative and commutative.
\end{exa}



\begin{figure}[t]                                 


  \caption{A problem with the  composition operator}
\label{fig:counterex} 
 
\end{figure}

Thus a priori this composition operator  could equally well 
be used to develop the testing theory in Section~\ref{sec:maypreord}. 
Unfortunately the resulting theory would be degenerate.
\begin{exa}[Example~\ref{ex:mas.op} continued]
  Consider the networks  in Figure~\ref{fig:counterex}, 
  where we choose  and  for two different 
  values .
Then intuitively  and  
  should have different observable behaviour, observable by placing 
  a test at the node . However if the operator  is used 
  to combine a test with the network being observed they are 
  indistinguishable. 

   This is because if there is a  network   such that
   and  are well-defined then  can not be in
  .  For if  were in , then
  since  the definition of the operator
  implies that .  This in turn implies
  that , which is not true.

  Now since no testing network which can be applied to both  and  can place any code at , no difference
  can be discovered between them.
 \end{exa}

The question now naturally arises about which consistency predicate  
lead to reasonable composition operators  , in the sense 
that at least the resulting testing theories are not degenerate. 
We want to be able to compare networks with different connectivity graphs, and
possibly different nodes, such as  and  in Figure \ref{fig:counterex}. 
We also should not be able to change the
connectivity of the internal nodes of a network when we test it; we wish to implement
\emph{black-box} testing, where the nodes containing running code cannot 
be accessed directly.  

These informal requirements can be formulated as natural requirements on composition
operators. 
The
first says that the composed network is completely determined by the
components:
\begin{description}
\item[(I) Merge] the operator  should be determined by
  some predicate  using
  Definition~\ref{def:comp.nets}.
\end{description}

Intuitively the interface of a network is how their external behaviour
is to be observed. Since our aim is to enable 
compositional reasoning over networks, we would expect composition to preserve
interfaces:
\begin{description}
\item[(II) Interface preservation] If ,  and  can be composed with both,  that is both 
        and 
are well-defined,  then , 
      .
\end{description}

The final requirement captures the intuitive idea that reorganising
the internal structure of a network should not affect the ability to
perform a test; in fact the reorganisation is simply a renaming of
nodes. Let  be a permutation of node names. We use  to denote the result of applying  to the node
names in  and in the connectivity graph .
\begin{description}
\item[(III) Renaming] Suppose  is
  defined. Then  is also defined, provided  is a node permutation
  which satisfies
\begin{itemise}
  \item  for every 
  \item no  appears in the range of ; that is
     implies . 
\end{itemise}
\end{description}

\begin{exa}
The operator  does not satisfy \textbf{(III)}, as can be
seen using the simple networks in Figure~\ref{fig:counterex}; 
is obviously well-defined. However, consider the renaming  which swaps node names  to , which is valid with respect to 
; the network  is not defined, as .
A slight modification will demonstrate that interfaces are also not preserved by this operator. 
\end{exa}

\begin{prop}\label{prop:I2III}
Suppose  satisfies the conditions (I) - (III) above. Then 
  
  whenever   is defined. 

\end{prop}
\begin{proof}
By contradiction; let  and 
. Assume that  is a node included in 
, and that 
 is defined. 
Finally, let  be an arbitrary permutation such that 
 is defined. 
Note that the following statements are true: 
\begin{enumerate}
\item ,
\item ,
\item .
\item .
\end{enumerate}
\noindent For proving the last statement just note that 
 by hypothesis, hence 
. 
By definition of interface it follows that 
.

Let  be a node which is not contained in , nor in . 
Consider the permutation  which swaps nodes  and ; that is 
 and  for all . 
Since  we also have that , 
so that .
Further, the permutation  is consistent with condition (III), renaming, when 
applied to networks  and ; 
therefore  is 
defined.\\ 

Since , by (1) above it follows 
that ;  
by (2), we obtain that . These two statements ensure 
that .\\
As a direct consequence of (3) and condition (II), interface preservation, we 
also have that , 
but this contradicts (4).
\end{proof}

\begin{cor}
Let  be any 
symmetric composition operator which satisfies the conditions (I) - (III). 
Suppose  is well-defined, and of the form 
. Then  whenever . 
\end{cor}
\begin{proof}
  A simple consequence of the previous result.
\end{proof}
\noindent
What this means is that if we use such a symmetric operator when applying a test to
a network, as in Definition~\ref{def:testing}, then the resulting testing preorder will
be degenerate; it will not distinguish between any pair of nets. 
In some sense this result is unsurprising. For  to test  in  it must have code running at the
interface of . But, as we have seen, condition (III) more or less forbids  to have code running at the interface of .

Thus we have ruled out the possibility of basing our testing theory on a symmetric 
composition operator. The question now remains what composition operator is the most appropriate? 
We have already stated that conditions (I)-(III) are natural, 
and since there are no further obvious requirements we
could choose  the operator with greatest expressive power among those that satisfy conditions (I)-(III). 
Here an operator  is more expressive than another 
 if, whenever  is defined 
for any two arbitrary networks , 
then so is , and the result of the two compositions 
above is the same. 
The next Lemma shows that the operator which we are looking for is exactly .
\begin{lem}
  Let  be any consistency predicate satisfying  (\ref{eq:iscons}) above. Then 
 if   is defined so is  and 
 moreover .  
\end{lem}
\begin{proof}
  Obvious from the definition of  in Definition~\ref{def:comp.nets}. 
\end{proof}


\section{Extensional Semantics}
\label{sec:ext.sem}
As explained in papers such as \cite{RS08-dbtm,dpibook}, contextual
equivalences and preorders are determined by so-called \emph{extensional actions},
which consist of the observable activities which a system can
have with its external environment. 
We present an extensional Semantics for probabilistic 
wireless networks in Section \ref{sec:ext.act}. 
The final two sections develop technical properties of these actions,
which will be used in the later soundness proofs. They may be safely skipped
at first reading. 
Section \ref{sec:comp.decomp} is devoted to 
basic decomposition and composition results for 
extensional actions, while in 
Section \ref{sec:relating} we relate 
the extensional actions we introduced 
with the reduction relation of 
the testing structures associated with 
networks.

\subsection{Extensional Actions}
\label{sec:ext.act}

Here we design a pLTS whose set of actions can be detected 
(hence tested) by the external environment. 
The intensional semantics in Section~\ref{sec:lang}
already provides a pLTS and it is instructive to see why this is not
appropriate.

Consider  and  from
Figure~\ref{fig:counterex}, and suppose further that the code  and
, running at  and  respectively, is identical,
  .  Then we would expect  and
   to be behaviourally indistinguishable.  However 
 will have an output action, labelled , which is
not possible for .  So output actions cannot record
their source node. What  turns out to be important is the set of target
nodes.  For example if in  we added a new output node  to
the interface, with a connection from  then we would be able to
distinguish  from ; the required test
would simply place some appropriate testing code at the new node .

Now we present an extensional semantics for networks; here the
visible actions consist of activities which can be detected (hence
tested) by placing code at the interface of a network. In this
semantics we have internal, input and output actions.

\begin{defi}[Extensional actions]\label{def:sea}   The actions of the extensional semantics are defined as follows:
\begin{enumerate}

\item \textbf{internal}, ; some internal activity reduces the system
, relative to the connectivity , to some system , where . Here the internal 
activity of a network coincides either with some node performing a silent move  or broadcasting a value 
which cannot be detected by any node in the interface of the network itself. 

Formally,  whenever  and 
\begin{enumerate}\item 
\item or  for some value , channel  and node name 
satisfying  implies 
 \end{enumerate}

\noindent Note that  we are using the notation given in Section \ref{sec:lang}
for defining distributions.  Here  is a distribution over
 and so  is a distribution over
networks; however all networks in its support use the same network connectivity
.

\item \textbf{input}, ; an observer
  placed at node  can send the value  along the channel  to the network
. For the observer to be able to place the code at node  we must have
. 

Formally  whenever 
 and 
 \begin{enumerate}\item 

 \item 
\end{enumerate}

\item \textbf{output}, , where  is a non-empty set of nodes; an observer
    placed at any node  can receive the value  along the
    channel .  For this to happen each node  must be in
    , and there must be some code running at
    some node in  which can broadcast along channel  to each
    such .

Formally,  whenever 

 \begin{enumerate}[label=(\roman*)]
 \item   for some node 
 \item  . \qed
 \end{enumerate}
\end{enumerate}
\end{defi}

\noindent In the following we will use the metavariable  to range 
over extensional actions. These actions endow the set of networks with the
structure of a pLTS. Thus the terminology used for pLTSs is extended to networks, 
so that in the following we will use terms such as finitary networks or finite branching networks; 
we use the symbol  to denote hyper-derivations in the 
extensional pLTS of networks, and  to denote extreme derivations.
Also note that we allow an extensional actions to 
be performed only in the case that a network is not -successful. 
As we have already stated, we see pLTSs as non-deterministic probabilistic experiments 
whose success is obtained by reaching an -successful state. When an 
-successful state is reached, we require the experiment to terminate.


A trivial application of Corollary \ref{cor:reduction} ensures that extensional actions 
are preserved by structurally congruent networks.
Further, they do not change the topological structure of a network.

\begin{prop}
\label{prop:static.topology}
Suppose that , 
. 
Then , and  is 
node stable. Further, for any  
we have that .
\end{prop}

\begin{proof}
The definition of extensional actions ensures that 
whenever  
then  for some 
. 
The fact that for any  
we have that  
follows from Corollary \ref{cor:stable.dist}.
\end{proof}
Note that, if two distributions  
are node-stable, if  is defined for some  
, then 
 is defined for any . Thus, for node-stable distributions 
of networks it makes sense to lift the operator  
to distributions of networks, defined directly via an application 
of Equation \ref{eq:twoargexpected}. A similar argument holds 
for the symmetric operator .

In the following we will need \emph{weak} versions of extensional actions, which
abstract from internal activity, 
provided by the relation . Internal activity can be modelled by 
the hyper-derivation relation , which is a probabilistic generalisation of 
the more standard weak internal relation . 

\begin{defi}[Weak extensional actions]\label{def:wea}
  \begin{enumerate}\item Let  whenever we have the hyper-derivation 
        

  \item   whenever  

  \item Let  be the least relation satisfying:
    \begin{enumerate}[label=(\alph*)]
    \item  implies 

   \item  ,  , where 
            , implies 
               \qed
    \end{enumerate}
  \end{enumerate}
\end{defi}
\noindent
These weak actions endow the set of networks \nets with the structure
of another pLTS, called the \emph{extensional pLTS} and denoted by
. 



Some explanation is necessary for the non-standard definition of
output actions in Definition \ref{def:wea}(3).  Informally speaking, the definition of
weak extensional output actions expresses the capability of simulating
broadcast through multicast, which has already been observed 
in Example \ref{ex:bcast1}. A single (weak) broadcast action
detected by a set of nodes  can be matched by a sequence of weak
broadcast actions , detected
respectively by , provided that
the collection  is a partition of
. This constraint is needed to ensure that
\begin{enumerate}[label=(\roman*)]
\item every node in  will detect the transmitted value and
\item no node in  will detect the value more than once.
\end{enumerate}
As we will see in Section \ref{sec:soundness}, the ability of a multicast to 
simulate a broadcast is captured by the testing preorders. Roughly speaking, 
in a generic network  a broadcast can be converted into a multicast\footnote{
Note that this operation would require a change in the internal topology of .} 
leading to a network  such that . 
Conversely, in the must-testing setting a multicast in a network  
can be replaced by a broadcast leading to a network  such that 
.



\subsection{Composition and Decomposition Results}
\label{sec:comp.decomp}
In this Section we prove decomposition and composition results for 
the extensional actions introduced in the previous section. 
In its most general form, the results we want to develop can be 
summarised as follows: given a network , 
\begin{description}
\item[Strong Decomposition] for actions of the form  
we want to determine two actions of the form  and 
, where ,
\item[Weak Composition] conversely, given two actions of the form 
 and , 
we want to determine an action of the form .
\end{description}\smallskip

\noindent Unfortunately, the following example shows that this task cannot 
be achieved by relying solely on the extensional semantics. 
\begin{figure}


                                   




 \caption{A problem with decomposition of actions with respect to }
\label{fig:testpdec}
\end{figure}

\begin{exa}
\label{ex:testpdec}
Consider the networks  of Figure \ref{fig:testpdec}. 
It is straightforward to note that  is 
defined; further, , 
where , . 

One could wish to be able to infer this action from the broadcast action 
of the form  and an input action 
of the form . Unfortunately, this last action 
cannot be inferred, since in  node  cannot detect the broadcasts 
performed by node .
\end{exa}

The problem of Example \ref{ex:testpdec} arises because the connection between 
from node  to node , which is present in , is not 
present in the right hand side of the composition . 
As a consequence, the action  cannot be derived. 
However, note that we could still decompose the transition  if we were to focus on the code run by node  in , 
rather than on the network  itself.

\begin{exa}
\label{ex:testpdec2}
Consider again the networks  of Figure \ref{fig:testpdec}, and 
recall that . Given the action 
, where 
we recall that , we can 
now infer the 
extensional transition  
and the process transition .

The first transitions says that some node in  performs 
a broadcast which affects node , while the second one says that 
how the code which node  is running reacts to a broadcast. 
Therefore, it is not surprising to note that the two 
transitions  and 
 can be combined together to obtain 
the original transition .
\end{exa}

Example \ref{ex:testpdec2} leads us to the intuition that composition and decomposition 
results of extensional actions can be developed if we focus on composed networks of the 
form , where . Intuitively, 
such results can be obtained by reasoning on the 
extensional transitions of  and the 
process transitions performed by the only internal node in . 





\begin{prop}[Strong decomposition in \pLTSnets]\label{prop:decomp}
  Let  be two networks such that  
  is defined. Further, let  be such 
  that ; 
  Suppose , . 
  Then  for some 
   such that 
  
  \begin{enumerate}\item if  then either 
		\begin{enumerate}[label=(\roman*)]
			\item , , or
			\item  and , or 
			\item , 
			, or
			\item ,  and 
			, or
			\item , 
			, or 
			\item , 
			,  and 
			.
		\end{enumerate}
		
		\item if  then either
		 \begin{enumerate}[label=(\roman*)]
		 \item , 
		  and , or  
		 \item , 
		  and , or 
		 \item , 
		 ,  and , or
		 \item ,  
		 and , or 
		 \item , , 
		  and ,
		 \end{enumerate}
		
		\item if , where , then either 
		 \begin{enumerate}[label=(\roman*)]
		 \item ,  
		 and , or
		 \item , , , or 
		 \item , ,  
		 and , or
		 \item ,  
		 and , or  
		 \item ,  and .
		 \end{enumerate}
		\end{enumerate}
\end{prop}
\begin{proof}
  The proof of this Proposition is quite technical, and it is 
  therefore relegated to Appendix \ref{sec:decomposition.results}, Page \pageref{proof:decomp}.
\end{proof}

Next we consider how the weak actions performed by a stable sub-distribution of the form , can be inferred by a weak action performed by 
the node-stable distribution  and a process transition performed by the 
state , respectively. 
For our purposes it will suffice to combine a weak extensional transition with 
a strong process transition.

\begin{prop}[Weak/Strong composition in \pLTSnets]
\label{prop:wea.comp}
	Let  be a 
	node-stable sub-distributions of networks. 
	Let also  be such that  is well-defined. Further, 
	let ; 
	here note that the sets  and  are 
	completely determined by the connectivity graph .
	\begin{enumerate}\item Composition resulting in internal activity:
	\begin{enumerate}[label=(\roman*)]
\item
     
implies 
 ,

\item  implies 
, 

\item  and 
 implies 
, 

\item  and 
 implies 
, 

\item , 
 
and , then , 
then ,

\item for any , 
 and  implies 
. 
\end{enumerate}

\item Composition resulting in an extensional output:
\begin{enumerate}[label=(\roman*)]
\item  ,  
  implies 
  ,

\item ,   
  and  
  implies 
  ,

\item ,   
  and  
  implies 
  ,

\item , , 
 implies that 
, 

\item  for any  
and ,   implies that 
. 
\end{enumerate}

\item Composition resulting in an input:
\begin{enumerate}[label=(\roman*)]
\item  and 
	 implies 
	, 
	
\item  for , 
 and  
	implies ,

\item  for, 
 and  
	implies ,

\item , 
	 and 
       
      implies 
      , 

\item , 
	 and 
       
      implies 
      .
	\end{enumerate}
  \end{enumerate}
\end{prop}
\begin{proof}
  See Appendix \ref{sec:decomposition.results}, Page \pageref{proof:composition}.
\end{proof}



\subsection{Relating Extensional Actions and Reductions}
\label{sec:relating}
In this section we investigate the relationship between 
extensional actions and reductions, defined in Section 
\ref{sec:ts}. 

It follows immediately, from the definition of the 
reduction relation , that for any 
network  we have  
if and only if either  
or . 
However, this result is not true anymore if we 
focus on the extensional transitions performed 
by a distribution of networks. 
\begin{figure}
\begin{center}
\begin{tikzpicture}
          \node[state](m){}; 
          \node[state](o)[right=of m]{}; 
  \path[to]
       (m) edge [thick] (o);
   \begin{pgfonlayer}{background}
    \node [background,fit=(m)] {};
    \end{pgfonlayer}
    \end{tikzpicture}
\end{center}
\caption{Relating reductions with extensional actions.}
\label{fig:red.ext}
\end{figure}

\begin{exa}[Reductions and Extensional Actions] 
\label{ex:red.ext}
Consider the network distribution , 
where  is depicted in Figure \ref{fig:red.ext} and 

\noindent
Note that we have the reductions , and 
. These two 
reductions can be combined together to infer  
.

However, there exists no extensional action of the form ; in fact, 
the only possibility for the network  is 
to perform a -extensional action, while the network  can only perform an output action of 
the form . In order to infer the action 
 
we require that every network in  
performs the same action ; but as we have just noted, 
this is not true for .
\end{exa}

The problem in Example \ref{ex:red.ext} arises because reductions 
have been identified with two different activities of 
networks; internal actions and broadcasts of messages. 
However, it is possible to avoid this problem if we 
modify a network  by removing the nodes in its 
interface. As a consequence, the only activities allowed 
for such a network would be internal actions of the 
form .

\begin{defi}[Closure of a Network]
\label{def:net.closure}
Let  be a network; we define its closure 
 by letting 

\end{defi}
\noindent 
Obviously the operator  preserves well-formed networks.

The actions of a network of the form  are completely 
determined by those performed by , as the following result shows. 
\begin{prop}
\label{prop:ext.closure}
Suppose that , where 
either  or ; 
then . 

Conversely, if , then 
 and either 
\begin{enumerate}[label=(\roman*)]
\item  and , 
\item or  and .
\end{enumerate}
\end{prop}

\begin{proof}
Let . 
Suppose that  
for some  and . 
By definition of extensional actions there exists a node  
such that , and 
. 
By Proposition \ref{prop:broadcast} it follows that 
, with . 
By definition of the operator , 
, where 
  whenever 
 and .
By an application of propositions \ref{prop:broadcast} and 
\ref{prop:input} we obtain that \footnote{In reality these propositions ensure that 
, where . 
However, since the system term  is not changed when performing 
the operation , 
it can be proved that .}. By 
Definition \ref{def:sea}(1) we get 
that . 
But  is exactly .
The case  is 
treated similarly.

Conversely, suppose that . Since , 
it follows that it cannot be , nor . 
Therefore the only possibility is that . By Definition 
\ref{def:sea}(1) there are two possible cases. 

\begin{enumerate}
\item . In this 
case we have that  
and . 
It follows that , 
hence .

\item ; in this 
case we have that , 
since whenever  it also 
follows that . Let 
. 
If , by Definition \ref{def:sea}(1) 
we obtain that , 
otherwise Definition \ref{def:sea}(2) ensures that 
.\qedhere
\end{enumerate}

\end{proof}

\noindent Thus only -actions are allowed in networks of the form 
, and the extensional outputs performed 
by  are converted in -actions in . 
This relationship can be used to relate the reductions performed 
by the network  with the extensional actions of the 
network 

\begin{prop}
Let ; then 
 if and 
only if .
\end{prop} 

\begin{proof}
Follows immediately from Proposition \ref{prop:ext.closure} 
and the definition of reductions.
\end{proof}

\begin{cor}
\label{cor:extreme.reductions}
Let ; then 
 if 
and only if . 
Further,  
if and only if . 
\qed
\end{cor}

An important consequence of Corollary \ref{cor:extreme.reductions} 
is that the operator  does not affect the 
set of outcomes of a network. 
\begin{cor}
\label{cor:closure.results}
For any network , .
\end{cor}

\begin{proof}
Let , . 
Suppose that . Then 
, and 
. 
By Corollary \ref{cor:extreme.reductions} we have 
that , 
hence . 

The converse implication is proved analogously.
\end{proof}

\noindent Thus the operator  allows to relate weak 
extensional actions and reductions without affecting the 
set of outcomes of a network. As we will see in 
Section \ref{sec:soundness}, this operator is very 
helpful when exhibiting sound proof methods for the 
testing preorders.

\section{A Sound Proof Method for the Testing Preorders}

\label{sec:soundness}
In this Section we present the main results of the paper. 
Following \cite{DGHM09full} we introduce the notion of 
simulation between networks , and 
we prove that it is a sound proof technique for the 
may-testing preorder. This topic is addressed in Section 
\ref{sec:may.sound}.

In Section \ref{sec:must.sound} we give a similar 
result for the must testing relation. We introduce the 
concepts deadlocked network and terminal distributions. 
These will be used to define a novel coinductive 
relation for sub-distribution of networks, the \emph{deadlock 
simulation} . We prove that the inverse of this 
relation is sound with respect to the must-testing preorder 
. Here the use of sub-distributions is necessary, 
since the must-testing preorder is sensitive to divergence.

Finally, in Section \ref{sec:convergent} we focus on convergent 
networks; We show that for such networks a slight 
variation of deadlock simulations can be used as 
a sound proof method for both the may and must testing 
preorders.

\subsection{The May Case}
\label{sec:may.sound}
We begin this section by reviewing the standard definition 
of simulations for probabilistic systems, applied to 
our calculus of probabilistic networks.
\begin{defi}[Simulation preorder]\rm\label{def:sim}
In   we let 
  denote the largest relation in 
such that if  then:
\begin{itemize}
\item , ,
\item 
if , then 
  such that for every ,

\item otherwise, whenever , for , then
  there is a  such that 
   and .
\qed
\end{itemize}


\end{defi}
\noindent
This is a mild generalisation of the corresponding definition in \cite{DGHM09full} where we factor in the 
presence of the success predicate  and we compare only networks with the 
same input and output nodes. 

Our aim in this Section is to prove the following theorem:
\begin{thm}[Soundness for May-testing]
\label{thm:may.sound}
Suppose  are finitary networks.
Then   in   
implies .
\end{thm}

Before proving Theorem \ref{thm:may.sound}, let us review, this time with 
formal arguments, why our definition 
of weak extensional actions had to be so complicated, and why 
we decided to focus on well-formed networks.

\begin{exa}[On Well-formed Networks]
\label{ex:sound.wellformed}
Consider again the networks  of 
Example \ref{ex:wellformed}. Here we recall 
that  is not well-formed, as it has 
a connection between the two external nodes 
. 

Let  be the largest relation over 
(possibly non well-formed) networks which satisfies the requirements 
of Definition \ref{def:sim}.
It is immediate to show that , 
However,   
In fact, it suffices to consider the test 
, where 
 and  
to distinguish these two networks. Specifically 
, 
, 
and . 
Therefore, Theorem \ref{thm:may.sound} cannot 
be extended to non well-formed networks.
\end{exa}

\begin{exa}
\label{ex:bcast}
Consider the networks  and  in Figure \ref{fig:bcast},
discussed already in Example~\ref{ex:bcast1}.  It is easy to show that
both of them can perform the weak extensional action
. However, the inference of the
action is different for the individual networks; while in network
 it is implied by the execution of a single broadcast action,
detected by both nodes  and  simultaneously, in  this
is implied by a sequence of weak extensional actions .

It is therefore possible to exhibit a simulation between  and
, thus showing that ; 
Theorem~\ref{thm:may.sound} shows that this implies
.
\end{exa}

\begin{figure}[t]
                                 


 \caption{Ensuring soundness}
  \label{fig:counterex2}
\end{figure}

\begin{exa}
Soundness requires that the extensional output actions records the set of target nodes, rather than single nodes.
Consider the networks  depicted in Figure \ref{fig:counterex2}, where 
 and 
. 
Intuitively, network  can broadcast value  to either node  or node , 
but it cannot broadcast the message to both nodes. On the other hand, in  node  can 
broadcast value  simultaneously to both nodes  and .

Note that 
 because of the test
 given in Figure \ref{fig:counterex2}, where . 
In fact, in  both nodes  and  will receive the broadcast of value  along channel 
 performed by node ; 
each of these nodes will forward the received value to node . Therefore, node  will receive two values, one 
from node  and one from node , after which it will reach a successful state. That is, 
.

On the other hand, none of the 
computations of  leads to a successful configuration. 
There are in fact two possibilities; either node  broadcasts value  
to nodes  and , or node  broadcasts value  to nodes  
and .  Note that one of the effects of  node  (respectively ) broadcasting value 
 is that of preventing node  (respectively ) from performing a second 
broadcast. 
As a consequence, only one among nodes  will receive value  
along channel . When node  (respectively ) receives value , it will 
forward it to node . After this broadcast has been performed the network 
reaches a configuration in which node  is still waiting to receive a 
value along channel  before entering a successful state; further, 
the computation of the network cannot proceed anymore, 
since none of its nodes can perform a broadcast.
There are no other possible behaviours of 
, therefore we obtain . 

Since , but , 
it follows that .


We also have  because  can perform the output action labelled
, which can not be matched by . 

However suppose we were to restrict  in the definition of
extensional output actions, part (3) of Definition~\ref{def:sea}, to be
singleton sets of node names. Then in the resulting pLTS it is easy to
check that  can simulate . 
The broadcast of value  in network , which can be detected by both 
nodes  and , can be matched by either the broadcast performed by node  
(which can be detected by node ) or by the broadcast 
performed by node  (which can be detected by node ) in .
In other words, with the proposed 
simplification the resulting simulations would not be sound; that
is, Theorem~\ref{thm:may.sound} would no longer hold.
\end{exa}


Let us now turn to the proof of Theorem \ref{thm:may.sound};
it relies on the following two technical results, whose proofs are developed
in Section~\ref{sec:technical}. 
\begin{thm}[Compositionality]\label{thm:composition}
Let  be finitary networks such that , . 
Also, suppose  is a network such that both 
 and  are defined. 
Then  implies  
.
\end{thm}
\begin{proof}
  See Corollary~\ref{cor:composition} in Section~\ref{sec:technical.may.comp}
\end{proof}

\begin{thm}[Outcome preservation]\label{thm:results}
  In ,  implies
  .
\end{thm}
\begin{proof}
  See  Corollary~\ref{cor:results} in Section~\ref{sec:technical.result.pres}. 
\end{proof}

\textbf{Proof of Theorem~\ref{thm:may.sound}}: 
This is now a straightforward application of  Compositionality and Theorem~\ref{thm:results}. 

Let us assume that .
To prove the conclusion,  , we must show that 

for an arbitrary testing network  
such that both   and  are defined. 
For such a  Compositionality entails  
, and now we can apply 
Theorem~\ref{thm:results}. \qed

\subsection{The Must Case}
\label{sec:must.sound}
In this Section we give a sound proof method for the must-testing 
preorder. It has already been observed that, for standard 
probabilistic process calculi such as pCSP \cite{DGHM09full}, 
the must-testing can be characterised by looking at the 
set of actions which are not enabled in processes. 
This is because outputs in such a calculus are blocking 
actions; in order for an action to be performed, a synchronisation 
(either within the process or with the external environment) 
must occur. This leads to the notion of \emph{failure simulations}.

This is not true for broadcast systems, where the nature of 
the broadcast action is non-blocking. It has been observed 
in \cite{Ene02} that, if broadcast communication is assumed, 
then the must-testing relation can be used to observe only if  
a computation of a process cannot proceed (that is, no 
internal actions nor broadcasts are possible). 

Following this intuition, we readapt the notion of \emph{failure 
simulation} given in \cite{DGHM09full}. 
\begin{defi}[Deadlocked Networks, Terminal Distributions]
\label{def:deadlock}
The predicate  
is defined by letting  whenever the 
following conditions are met:
\begin{enumerate}[label=(\roman*)]
\item ,
\item ,
\item  
for any . \qed
\end{enumerate}
\noindent
Networks for which the predicate  is true 
are called \emph{deadlock networks}, or \emph{deadlocked}. 
Note that the term \emph{deadlock} network makes sense only 
in the reduction semantics. Deadlock networks 
are those whose  computation cannot proceed 
autonomously; however, it could be the case that an 
input from the external environment makes the network 
evolve in a distribution where the computation can 
proceed, thus resolving the deadlock.

A distribution  is said to be \emph{terminal} if 
any network in its support is either deadlocked or 
successful.
\end{defi}
\noindent
Next we present a notion of simulation which is sensitive 
to deadlocked networks:

\begin{defi}[Deadlock Simulations]
\label{def:ds}
The relation  is 
the largest relation such that whenever 
\begin{enumerate}[label=(\roman*)]
\item if  then  
for some  such that  for any 
, 
\item if  for some 
 then  
for some  such that .\qed
\end{enumerate}
\end{defi}
\noindent
We use the notation  for 
.
Note that deadlock simulations are sensitive to divergence. 
That is, whenever  and , 
then . 
To prove this, note first that for any relation 
 then 
whenever  we have that  
; this follows at once 
from Definition \ref{def:lift}. 
Thus if  and , 
by definition it follows that  
for some  such that ; 
but this means that , 
or equivalently .

Before discussing the soundness of deadlock simulations for must testing
let us discuss briefly the definition of 
deadlock simulations.
First, note that the deadlock simulation relation  is 
lifted to a relation between sub-distributions, rather than 
to a relation between (full) distributions. 
This is needed, since the Must-testing preorder is 
sensitive to divergence.

\begin{exa}[Divergence]
\label{ex:divergence}
Let , 
, 
where  is the connectivity graph containing 
the sole node  and no connections and 
. 

It is immediate to show that , 
since the move  cannot 
be matched by . 

Even more, it is straightforward to 
note that . Consider in fact the test 
,  
where  is the connectivity graph consisting of the 
sole node . Note that, since  
we also have , 
and therefore . Intuitively 
the last hyper-derivation can be inferred by always letting 
process  perform a -action in 
. On the other hand we have 
that , since the only possible 
transition for   is 
, 
and the latter is -successful.  
Since , but , 
it follows that .

However, suppose that Definition \ref{def:ds} is changed 
by only considering hyper-derivations of the form 
, where  is 
a distribution. 
In this case we would have that the only possible (weak) move 
for the network  above is , 
which can be matched by . 
Therefore we would have that , and 
since we already proved that  Theorem 
\ref{thm:must.sound} would no longer hold.
\end{exa}

Also, deadlock simulation is defined as a 
relation between networks and sub-distributions of 
states, rather than a relation between networks. 
This is in contrast with the definition of simulation 
given in Section \ref{sec:may.sound}, which 
has been defined as a relation between 
networks.
In fact, since deadlock simulation also considers 
sub-distributions the latter approach would have led to 
a less discriminating relation.

\begin{remark}
Note that, for any sub-distribution  
we have that ; 
in fact, it is straightforward to show that 
 for 
any extensional action , and 
since  we 
also have that  
for any .

Now suppose that deadlock simulation had 
been defined as a relation between networks, 
by letting  be the largest 
relation such that 
\begin{enumerate}[label=(\roman*)] 
\item if  then  
for some  such that  for any 
, 
\item whenever  then 
 with . 
\end{enumerate}\medskip

\noindent While Theorem \ref{thm:must.sound} would still 
hold with this definition of deadlock simulations, 
it is straightforward to show that if 
 then . As a consequence, whenever 
 it would 
follow that . 

Therefore we have that, for any non-empty sub-distribution 
, , but not
;
that is, the definition of deadlock simulation proposed 
above is less discriminating than the one given in 
Definition \ref{def:ds}.
\end{remark} 


The proof of soundness of deadlock simulations follows the same structure as
the corresponding proof for simulations in Section~\ref{sec:may.sound}. It relies on
the following two technical results. 

\begin{thm}[Outcome preservation]
\label{thm:ds.outcomes}
If  then 
. 
\end{thm}
\begin{proof}
  See Corollary~\ref{cor:ds.outcomes} in Section~\ref{sec:technical.result.pres}. 
\end{proof}

\begin{thm}[Compositionality]
\label{thm:ds.comp}
Let  be a network and  be a stable sub-distribution 
such that . Then, for any network  such 
that both  and  are defined 
it follows that .
\qed
\end{thm}

\begin{proof}

See Corollary~\ref{cor:ds.comp} in Section~\ref{sec:technical.must.comp}
\end{proof}



\begin{thm}[Soundness for Must-testing]
\label{thm:must.sound}
Let  be two finitary networks 
such that , . If  
then .
\end{thm}
\begin{proof}
Suppose that , 
and suppose that , 
. Note also that 
 is a stable distribution. 

Let  be a network such that both 
 and  
are defined. 
Compositionality, Corollary \ref{cor:ds.comp} 
gives that , while Theorem \ref{thm:ds.outcomes} 
states that . Since the 
testing network  has been chosen arbitrarily, it 
follows that .
\end{proof}




\subsection{Proof Methods for Convergent Networks}
\label{sec:convergent}

One of the main drawbacks of deadlock simulations 
is that they require the use of probability sub-distributions. 
As we have seen in example \ref{ex:divergence}, 
using sub-distributions is necessary for ensuring 
the validity of Theorem \ref{thm:must.sound}. We 
have also emphasised that this constraint is necessary since 
the must-testing preorder is sensitive to divergence. 

However, sub-distributions are no longer needed if we 
focus on convergent networks, that this those whose 
generated pLTS (with respect to the strong extensional semantics) 
does not contain a state  for which  
holds. 

\begin{defi}[Divergence-free Deadlock Simulations]
\label{def:dfdeadsim}
The relation  is defined 
as the largest relation such that whenever 

\begin{enumerate}[label=(\roman*)]
\item if  then  
for some  such that  whenever 
,
\item if  then 
 for some  
such that .
\end{enumerate}
\end{defi}

\noindent We write  for . 

\begin{thm}[Soundness for Convergent Networks, Must-testing]
\label{thm:must.dfsound}
Let  be two convergent 
networks such that  
and ; if  then 
.
\end{thm}

\begin{proof}
It suffices to show that  is included in 
. To this end, note that if 
 and  is a convergent 
network, then , by Proposition 
\ref{prop:convergent.moves}.
\end{proof}

Having a simpler sound proof technique is not the only 
advantage that we gain by focusing on convergent networks. 
In fact, if we make a further restriction and we compare networks 
whose codes running at nodes do not contain the success clause 
, it follows that the relation  is also included 
in the may-testing preorder . This restriction is justified 
since in general we require the tests applied to a network, 
rather than the networks to be tested, to contain the clause 
 to denote the success of an experiment. 

\begin{thm}[Soundness for Convergent Networks, May-testing]
\label{thm:may.dfsound}
A network  is proper if the term  does not 
contain any occurrence of the special clause . 

Let  be convergent, proper networks such that 
, . 
If  
then .
\end{thm}

\begin{proof}
It is trivial to note that the relation  is included in 
 when restricted to convergent, proper networks. 
The result follows then from Theorem \ref{thm:may.sound}.
\end{proof}

\section{Technical development}
\label{sec:technical}
In this section we collect the proofs of some technical results underlying our soundness
results; it may safely be skipped by the uninterested reader. 
The first to results concern the compositionality of the simulation preorders. The last 
outlines the proofs about \emph{Outcome preservation}. 
\subsection{Compositionality for  simulations}
\label{sec:technical.may.comp}

This section is devoted to the proof of Theorem~\ref{thm:composition}, namely
that the simulation preorder is preserved by  the extension operator . 
In general such compositionality results depends on \emph{decomposition} and \emph{(re-)composition}
results for the actions used in the definition of simulations. Definition~\ref{def:sim} 
uses \emph{weak} extensional actions, and providing \emph{decomposition} results for 
these would be a  difficult undertaking. Instead we  first give an alternative 
characterisation of the simulation preorder, for which \emph{decomposition results} for
\emph{strong} actions is sufficient. These (strong) decomposition results, and 
\emph{(re)-composition} results for \emph{weak} actions have already been given in 
Section~\ref{sec:comp.decomp}. 


\begin{defi}[Simple simulations]\rm\label{def:sims}
In   we let 
  denote the largest relation in 
such that if  then: 
\begin{itemize}
\item ,
\item 
if   then 
such that  for any ,

\item otherwise, 
 \begin{enumerate}[label=(\roman*)]
  \item whenever 
  there is a  with
   and .
 \end{enumerate}
  
\end{itemize}
\end{defi}

\begin{thm}[Alternative characterisation]\label{thm:altchar}
  In  ,
 if and only if , 
  provided that  and  are finitary networks.
\end{thm}

\begin{proof}(Outline)
The proof is similar in style to the one of Theorem 7.20 of \cite{DGHM09full}; however, there are some extra complications, mainly because of the more complicated definition of weak extensional actions. 
Here we report a detailed outline of the proof; we first prove that 
 is included in , then we show that, for finitary networks, 
the converse inclusion also holds.

Showing that the relation  is included in  is straightforward. 
We only need to show that  satisfies the constraints of Definition \ref{def:sims}.
Suppose that . Then this hypothesis ensures that 
\begin{itemize}
\item 
 and 
\item if  then  such that 
 for any .
\end{itemize}
Suppose however that  and  
for some . Then we also have that , from which 
it follows from the hypothesis  that there exists  
such that  and , which is exactly 
what we wanted to show.

It remains to show that, for finitary networks, the relation  is included in . 
Here the main difficulty consists in showing that, whenever , 
 and , then  
for some  such that . 
The proof of this statement is performed by a case analysis on the action ;

\begin{enumerate}
\item First suppose that .
This case can be proved in the analogous way of 
Theorem 7.20 of \cite{DGHM09full}. Note that we require networks to be finitary in this 
case, since the proof requires properties of hyper-derivations 
which in general are not satisfied by infinitary plTSs; see Lemma 6.12 of \cite{DGHM09full}.

\item Now suppose that  for some ; then  
implies that there exist  such that . 
Since  and , by the previous case  
for some  such that . Since , we can 
conclude that  for some  such that 
. Finally, since , 
by the previous case\footnote{Note that here it is necessary to decompose  as a sum 
of state-based networks, each of which can perform a weak -action.} we obtain that  for some 
 such that . 
Therefore we have shown that , 
or equivalently , and , which 
is exactly what we needed to prove.

\item Finally, suppose that  for some  and non-empty set of 
nodes . We perform an inner induction on the proof of the derivation ;
\begin{itemize}
\item  because . 
This case is identical to the one ,
\item  because , 
where , 
. Since , by the (inner) inductive hypothesis we have that 
 for some  such that . 
A second application of the inductive hypothesis to the last statement gives that  
for some  such that . Therefore we have shown that 
, or equivalently 
 (recall that  and ), 
and , as we wanted to prove.\qedhere
\end{itemize} 
\end{enumerate}
\end{proof}\smallskip

\noindent Theorem \ref{thm:altchar} enables us to exploit the results 
developed in Section \ref{sec:comp.decomp} for proving 
the compositionality of  with respect to the 
extension operator . Since such results are valid 
only in the case that a network  is composed
 with a generating network 
, we first focus on compositionality with respect 
to a generating network.

\begin{thm}\label{thm:single.comp}
  Suppose ,  
  and both  and 
 are defined. Then 
 implies 
.
\end{thm}

\begin{proof}
It suffices to show that the relation 

\noindent
satisfies the requirements of Definition \ref{def:sims}. 
We denote the network  with . 


Suppose that ; 
note that the definition of extensional output ensures that 
.
We need to show that , and . By 
Proposition \ref{prop:decomp} we know that 
, 
where ,  are determined according to 
six different cases, which are considered below.

\begin{enumerate}[label=(\roman*)]
\item ,  and 
; 
since  it follows that 
 for 
some  such that  
. Since 
, By Proposition 
\ref{prop:wea.comp} it follows that . 
Now it suffices to note that 
, 
as we wanted to prove
\item , 
 and .  
Since  it follows that 
, 
where . 
Since , that is  
, we can apply Proposition \ref{prop:wea.comp}(2)(ii)
to the weak extensional transition  and the process transition  
to infer ; since 
we are assuming that , the 
latter can be rewritten as  ; 
again, since , we have that 


\item , 
 and . This case is 
similar to the previous one, this time employing Proposition 
\ref{prop:wea.comp}(2)(iii)

\item ,  and 
. 
Since  we have that 
 for some  
such that . 
It follows from Proposition \ref{prop:wea.comp}(2)(iv) 
that . 
Now note that 

\item  and 
. Since  it follows that 
, hence by Proposition 
\ref{prop:wea.comp} we have that . 
Now the hypothesis  ensures that 
.


\end{enumerate}

\noindent The cases  and 
 are 
treated similarly, and are therefore left as an 
exercise for the reader.
\end{proof}

However, since generating networks can be used to generate 
all the networks in , Proposition  
\ref{prop:network.decomp}, we can 
easily generalise Theorem \ref{thm:single.comp} to arbitrary 
networks.

\begin{cor}[Compositionality: Theorem~\ref{thm:composition}]\label{cor:composition}
Let  be finitary networks such that , . 
Also, suppose  is a network such that both 
 and  are defined. 
Then  implies  
.
\end{cor}
\begin{proof}
 First note that the only elements used to define simulations 
 are network interfaces, the set of outcomes of networks and the extensional 
 transitions that networks can perform. These definitions are preserved by the structural congruence between networks
defined on page \pageref{sec:properties}.
 As a  consequence, 
 simulations are identified up-to structural congruence; 
 if ,  and 
 , it follows that .
 
 Suppose then that , and let  be 
 a network such that both  and  are defined. We show that 
  by induction on 
 .
 \begin{itemize}
 \item . In this case we have that 
 
 \item . By Proposition \ref{prop:network.decomp} 
there exist two networks  and  such that 
. 
In particular , where we have used the associativity 
of the operator , Proposition \ref{prop:testp.assoc}. 
Note that , hence by the inductive 
hypothesis it follows that . 
By Theorem \ref{thm:altchar} we obtain that , 
and Theorem \ref{thm:single.comp} gives that 
.
 Another application of \ref{thm:altchar} and the associativity of the operator  lead to 
 . 
 Therefore we have that 
 
 as we wanted to prove.\qedhere
 \end{itemize}
\end{proof}



\subsection{Compositionality for deadlock simulations}
\label{sec:technical.must.comp}


As we have already done in Section \ref{sec:technical.may.comp},
here we also rely on an alternative characterisation of the 
 preorder, which is more amenable to \emph{decomposition/composition}
results.



\begin{defi}[Simple Deadlock Simulation]
Let  be 
the largest relation such that whenever  
then ,  
for any ; further 
\begin{itemize}
\item if  then , 
\item otherwise, 
\begin{enumerate}[label=(\roman*)]
	\item if  then  
	such that  for any , 
	\item if , then  and .
\end{enumerate}
\end{itemize}
\end{defi}

\begin{thm}[Alternative Characterisation of Deadlock Simulations]
\label{thm:ds.altchar}
If  is a finitary network and  is a finitary 
sub-distribution of networks then 
 if and only if .
\end{thm}

\begin{proof}
The proof is identical to that of Theorem \ref{thm:altchar}.
\end{proof}

\begin{thm}[Single Node Compositionality for Deadlock Simulations]
\label{thm:ds.single.comp}
Let  be a network and  be a stable sub-distribution 
such that . Then, for any network 
 such that  and 
 are well-defined it follows 
that . 
\end{thm}

\begin{proof}
The proof is analogous to that of Theorem \ref{thm:single.comp}.
There are, however, some extra statements that we need to 
check. 

\begin{itemize}
\item If  
then . 
This can be proved by rewriting  
by inspecting the infinite 
sequence of -moves which constitute the 
hyper-derivation  
to build an hyper-derivation of the form . This step requires employing the composition and 
decomposition results developed in Section \ref{sec:comp.decomp}.

\item For any  such that 
 is defined,   
implies  and ,
\item For any stable sub-distribution  and network , 
suppose that , where  is such that 
 for any ; further, 
suppose that  
 for some  such that 
 for any . 
Then , and 
 for any .
\end{itemize}
The validity of the two statements above can be easily proved 
using the results developed in Section \ref{sec:comp.decomp}.
\end{proof}

\begin{cor}
\label{cor:ds.comp}[Theorem~\ref{thm:ds.comp}]
Let  be a network and  be a stable sub-distribution 
such that . Then, for any network  such 
that both  and  are defined 
it follows that .
\qed
\end{cor}
\begin{proof}
By induction on the number of nodes contained in 
, using Theorem 
\ref{thm:ds.altchar}. The proof is analogous to that of Corollary \ref{cor:composition}.
\end{proof}



\subsection{Outcome preservation}
\label{sec:technical.result.pres}

The aim of this section is to prove Theorem~\ref{thm:results} and 
Theorem~\ref{thm:ds.outcomes}, namely 
that simulations preserve, in the sense of Definition~\ref{def:relsets}, 
the outcomes  generated by networks.
To this end, we first need two technical results.

\begin{lem}
\label{lem:sim.closure}
If , then .
\end{lem}

\begin{proof}
It suffices to show that the relation 

satisfies the requirements of Definition \ref{def:sim}.
Let  such that , 
Since the only possibility for networks of the form 
 
is that of performing -extensional actions, we only have 
to check that if  then 
 for some  
such that  for any . 

Suppose then that . It follows 
immediately that , and since 
by hypothesis , we have that  for some  such that 
 for any . 
Therefore we have that . 
It follows that , hence any network  satisfies the predicate .
\end{proof} 

\begin{lem}
 \label{lem:results}
  Let  be stable distributions in   such that ; 
  then  such that . 
\end{lem}
\begin{proof}
	Note that in the statement of Lemma \ref{lem:results} 
	we use the extreme derivative associated with the reduction relation 
   defined for the testing structures associated with networks. 
  We have seen in Section \ref{sec:relating} that such extreme derivatives 
  do not coincide with weak -actions in the extensional semantics, 
  except in the case of closed distributions.

	Therefore, let us first prove the statement for closed distributions, 
	that is those whose networks in the support have an empty interface. 
	Let then  be such that .
	
  We have two different cases.
  \begin{enumerate}[label=(\roman*)]
  \item First suppose  is a point distribution . If the predicate  
    is equal to , . 
    In this case, we recall that Theorem \ref{thm:hyper} (4) ensures that 
    there exists at least one extreme derivation , for which 
     trivially holds. 
    
    Otherwise the predicate  is satisfied and  has to be 1. Since 
     we know , such that for all . Also, since  is closed, the hyper-derivation 
    above can be rewritten as . This means that ; 
    moreover, as every state in  is a successful state, we also have that 
    .

\item
Otherwise  can be written as 

where  for each  in the support of . 
By part (i) each  such that .
As an extreme derivative is also a hyper-derivative, we can combine these to obtain a hyper 
derivation for , using Theorem \ref{thm:hyper} (3). This leads to 

As for every ,  we have that 
 implies , this condition is respected 
also by all states in . Thus, the hyper derivation 
 is also an extreme derivation. 
Finally, the quantity  
can be rewritten as , leading to 

 
  \end{enumerate}
  
\noindent For more general distributions  it suffices to note 
that if  then . For such (closed) distributions 
of networks we have that , 
and . Now Corollary 
\ref{cor:extreme.reductions} ensures that  
for some  such that . 
Finally, by Corollary \ref{cor:closure.results} we obtain that 

which concludes the proof.
\end{proof}

\begin{cor}[Theorem~\ref{thm:results}]\label{cor:results}
   In ,  implies
  .
\end{cor}
\begin{proof}
	We first prove the result for closed distributions .
  Suppose . We have to find  a derivation  such
  that . 
  Since we are assuming that  is closed, then  implies 
  , Corollary \ref{cor:extreme.reductions}, 
  which in turn gives .
  We can use the definition of  to find a 
        derivation  such that . 
        Applying the previous lemma we obtain  such that 
        . By Theorem \ref{thm:hyper} we have that 
        , and Corollary \ref{cor:extreme.reductions} 
        gives . 

	Suppose now that  are not closed distributions. 
	In this case Corollary \ref{cor:closure.results} ensures that 
	
	\noindent and there is nothing left to prove. 
\end{proof}


\bigskip

We now repeat the above argument to show that outcomes are also preserved by
deadlock simulations; the details are quite similar. 
\begin{lem}
\label{lem:ds.closure}
Whenever  it follows that 
.
\end{lem}

\begin{proof}
If suffices to note that, for any network 
,  
if and only if . 
Using this fact the result can be proved as in Lemma 
\ref{lem:sim.closure}.
\end{proof}

The main use of Lemma \ref{lem:ds.closure} is that of 
showing that whenever  then 
the sets of outcomes of  and  are 
related in some appropriate manner. 

\begin{lem}  
\label{lem:ds.outcomes}
Suppose that  
for some terminal distribution . 
Then  for some 
 such that .
\end{lem}

\proof
First suppose that  is a closed 
distribution, that is for any network  we have that 
. 

\begin{enumerate}
\item if  we have two 
possible cases:
\begin{enumerate}[label=(\roman*)]
\item ; since 
we are assuming that  is a 
terminal distribution then it has to be 
, which implies 
. In this case 
we are ensured that  
for some , and , 
trivially holds.
\item ; in this 
case , 
hence . Therefore we have 
to show that  for some  
such that . Since , 
we have that  for some  
such that  for any , 
which is equivalent to . 
Further,  implies . 
Since , 
 
for any , which is equivalent to 
 for any , we 
also have .
\end{enumerate}

\item otherwise, , 
 with 
 and  
for any . This part of the Lemma can be 
proved as in Lemma \ref{lem:results}.
\end{enumerate}
Now let  be a general distribution, 
not necessarily closed. 
By Lemma \ref{lem:ds.closure} it follows that 
, 
therefore there exists a sub-distribution 
 such that , and . This in turn 
implies that , and 



\begin{cor}[Theorem~\ref{thm:ds.outcomes}]
\label{cor:ds.outcomes}
If  then 
. 
\end{cor}
\begin{proof}
First suppose that  is a closed sub-distribution; 
let  be a sub-distribution such that 
; we have to show that 
 for some  such that 
.
Note that, since  is 
closed, this is equivalent to . 
Further, it is straightforward to note that  is 
terminal. Since  it follows 
that  for some  
(which also implies )
such that . 
Since  is terminal, by Lemma 
\ref{lem:ds.outcomes} we also have that 
 and . Now we just need to combine the 
reductions  and 
 to obtain .

If  is not a closed sub-distribution, we have 
that ; 
since  is closed it follows that 
.
\end{proof}


\section{Failure of Completeness}
\label{sec:completeness}

Although the simulation preorder  provides a proof
methodology for establishing 
 that two networks are be related via
the testing preorder , it is not complete.
 
That is, it is possible to find two networks 
such that  holds, but  cannot be simulated
by . 
Similarly, for the must-testing preorder, we have that 
it is possible to exhibit two networks  
such that , but . 
This results are quite surprising, as simulation preorder has been
already proved to provide a characterisation of the may-testing
preorder for more standard process calculi such as pCSP, 
while the must-testing preorder has been proved to be characterised 
by failure simulations \cite{DGHM09full}. 
Here, for simplicity, we discuss the failure of completeness 
for the sole  preorder; however, the examples discussed 
here can be used to show that the relation  is also 
incomplete.
  
  The main problem that arises in our setting is that the  
  mathematical basis of simulation preorders rely on (full) probability distributions, which are a 
  suitable tool in a framework where a weak action from a process term has to be matched with the same 
  action performed by a distribution of processes.
  
  This is not true in our calculus; we have already shown that, due to
  the presence of local broadcast communication, it is possible to
  match a weak broadcast action with a sequence of outputs whose sets
  of target nodes are pairwise disjoint. This behaviour has been
  formalised by giving a non-standard  definition of weak extensional
  actions in Definition \ref{def:wea}. 
  
  Such a definition 
  captures the possibility of simulating a broadcast through a
  multicast only when the former action is performed with probability
  .
  
  However, when comparing distributions of networks we have to
also match actions which are performed with probabilities less than 1, at least
informally; here the simulation of broadcast using multicast runs into problems,
as the following example shows.





  \begin{exa}
  \label{ex:compfail}
  \begin{figure}[t]
  

                                   



   \caption{Two testing related networks}
  \label{fig:compfail}
  \end{figure}
  
  Consider the two networks ,  depicted in Figure \ref{fig:compfail}; 
  let 
  
  \noindent 
  In  a message is 
  broadcast to nodes  with probability , while in  two different broadcasts happen in 
  sequence. The first broadcast, which can be detected by node , happens with probability . 
  The second broadcast, detectable by node  happens with probability . As a result,  
  the overall probability of message  to be detected by both nodes  is again .
  
  We first show that , then we prove that 
  .\\ 
  For the first statement, we only supply informal details, as a complete proof would be 
  rather long and technical. 
  Consider a test , such that both 
   and  are defined. Without loss of generality, suppose that both , that is . 
  We consider only the most interesting case, that is when the testing component 
  reaches (with some probability ) an -successful configuration after network 
   broadcasts the message . In this case, 
  a computation fragment of  can 
  be summarised as follows:
  \begin{enumerate}
  	\item The testing component  performs some internal activity, thus leading to 
  	,
  	\item At this point, the network  
  	performs a -extensional action, specifically , where 
  	
  	
  \item The distribution  
  performs some other internal activity, that is  
  
  \item At this point, the distribution 
  broadcasts the message  with probability , causing 
  	the testing component to evolve in ; note that only nodes  and  are affected by the broadcast performed by node . 
  	After performing the broadcast, the tested network becomes deadlocked.
  \end{enumerate}
  	
	\noindent Consider now the network . 
  	For such a network, a matching computation will proceed as follows:
  	\begin{enumerate}
  	\item The testing component  performs the two sequences of internal activities as before, ending 
  	up in the distribution .
  	\item At this point, the network  performs a broadcast, , where 
		
  	Here note that, since the broadcast of message  fired by the network  
  	can be detected by the sole node , only the code running at this node is affected 
  	in the test. Further, the resulting distribution at this node is again ; 
  	the test component after the broadcast of message  to node  is then in the 
  	distribution 
  	.
    \item Before allowing the testing component to perform any
        activity, we require the distribution  
        to perform the second
        broadcast, which will be heard by node ; this 
        happens with probability . Further, such a broadcasts 
        affects the probability distribution of processes running at the sole node
        . Thus, after the second message has been broadcast by
        the tested network, the testing component will have the form
        . This is exactly the same configuration
        obtained in the first experiment, after  has broadcast the message to both nodes .  Further,
        note that the overall probability    delivering the 
        message to both the external nodes is again . 
        Finally, after the broadcast has been fired, the tested network reaches 
        a deadlocked configuration.
  \end{enumerate}
  
  \noindent We have shown that, whenever the broadcast of message
   by  affects the testing network  in some way, then  is able to
  multicast the message to both  and , causing  to behave in the same way. Note also that 
  , 
  so that the behaviour of the testing component  
  does not affect that of the tested networks. 
  Now the reader should 
  be convinced that .
  
  Next we show that it is the case that  cannot
  be simulated by . The proof is obtained by
  contradiction. Suppose that . 
  Since , 
  we have that  
  for some distribution  such that . 
  It is straightforward to note that whenever  then . 
  
  Recall that . 
  Since , 
  the decomposition property of lifted relations, 
  Definition \ref{def:lift} ensures that we can rewrite 
   as , 
  and , 
  
  Let us focus on the network . 
  This network is equipped with the extensional transition 
  . 
  Since , it follows that 
   
  for some distribution . We show that this is not possible. 
  
  This is because the only action that can be performed by 
   is ; in order for  to be 
  able to perform the weak action  
  we require the distribution  
  to perform a weak broadcast to node . 
  However, this is possible if every network in  can perform such an action; this is not true, since 
  , and the network  
  is deadlocked. 
  
  We have shown that , 
  which in turn gives . Since , and  is 
  the only hyper-derivative of , we conclude 
  that . 
  \end{exa}
  
  Note that the example above can be readapted to show that 
  deadlock simulations are incomplete with respect to the 
  must-testing relations. 
  In fact, for the networks ,  of 
  Example \ref{ex:compfail} it is easy to show that 
  , but 
  . 
  
 Example \ref{ex:compfail} has more serious consequences than just showing that simulation 
  preorder is not complete with respect to the may testing preorder. 
  One could in fact expect that the notion of simulation can be modified, 
  leading to a less discriminating preorder for networks which characterises 
  the  preorder. We show that this is not the case. 
  
  \begin{defi}[-Simulations]
  A relation  is 
  a -simulation if whenever  then 
  ,  
  and whenever  it follows that 
   for some  such that 
  . \qed
  \end{defi}
  Note that the definition of -simulations is very general, 
  since the only constraints that we have placed on them, 
  apart from the standard checks on the input and output nodes 
  in the interface of networks, is that a strong -action 
  has to be matched with a weak one. It follows at once 
  that  is a -simulation.
    
  \begin{thm}
  \label{thm:nolift}
  There exists no -simulation  such that
   iff .
\end{thm}
  
  \begin{proof}
   The proof is carried out by contradiction. 
   Suppose   
   is a -simulation such that  
   if and only if , and consider the 
   networks  from Example \ref{ex:compfail}.
    We have already proved that  
   and so by the hypothesis we have . 
   Note that , 
   where , 
   where  have been already defined in Example \ref{ex:compfail}. 
         
   Since  is a -simulation, 
   the -action performed by  has to 
   be matched by a hyper-derivation in ; 
   we have already noted that the only possible hyper-derivation 
   for such a network is given by . Therefore we have that 
   . 
   The decomposition property of lifted relations, Definition 
   \ref{def:lift} ensures that we can rewrite 
    as , and  
   , 
   . It is trivial to note that here the only 
   possibility is that . 
   Therefore , 
   and by hypothesis this implies that . 
   
   However, this is not possible. We show that there is 
   a test that distinguishes the network  
   from .
   Consider the test , 
   where  is the connectivity graph consisting of 
   the sole node  with no connections, while 
   . 
   It is straightforward to note that 
   . 
   However, for any  
   we have . If follows that 
   . That is, 
   . Contradiction. 
  \end{proof}

\section{Case Study: Probabilistic Routing}
\label{sec:prob.routing}
While our proof methods for relating probabilistic networks 
via the testing preorders are not complete, they are still 
useful for comparing practical examples of wireless networks. 
Even more, they can be used to perform a model-based verification
of network protocols, showing that their behaviour 
is consistent with respect to some formal specification. 
In this section we show how this can be done by proving the 
correct behaviour of a simple probabilistic routing protocol. 
For the sake of simplicity, we focus on an abstract implementation of 
a geographic routing protocol, in which much of the details are left 
unspecified. However, it is worth mentioning that 
the proposed implementation can be refined, leading to a concrete 
representation of the  \emph{SAMPLE} probabilistic routing protocol \cite{sample}.

By \emph{formal specification} we mean a network , while by \emph{network protocol} 
we mean a set of networks  whose elements share the same input and output nodes.
Proving that the behaviour of a protocol  is sound with respect 
to a formal verification  consists then in showing that 
for any network  it has to be 
.

Let us now turn our attention on how this task can be achieved 
for a probabilistic (connection-less) routing protocol. 
At least intuitively, the routing policy states that 
messages broadcast by a location in a network, 
called source, are 
eventually delivered to a desired node, called 
destination. 
For the sake of simplicity, here we consider a 
situation in which the source and the destination 
of a routing policy are two fixed external nodes, 
 and  respectively. 

Designing a specification for the routing policy is 
easy; however, there are some details that need to be 
taken into account. 
First, we need to introduce some mathematical 
tools that will enable us to equip a node in a 
network with some sort internal memory; this is necessary, 
since in a routing protocol nodes have to store 
the values they have received and which they 
have not yet forwarded to another 
node.

This can be done by relying on \emph{multisets}. 
Roughly speaking, a multiset  
is a set which can contain more than one copy of 
the same element. Formally, a multiset 
 from a set universe  is 
a function  
which assigns to each element  the 
number of copies of  contained in . 
For our purpose the universe  consists of the 
set of (closed) message values, and we only deal with 
finite multisets, that is those for which 
.

We denote with  the empty multiset, 
that is the multiset such that  
for any value , and we say that 
 if  for any value . 
We say that  if . 
Given a finite collection of multisets , the multiset  
is defined by letting . 

Finally, for any multiset  and a value , we denote 
with  the multiset such that 
 and 
 for any 
. Similarly, the multiset  
is defined by letting  if , 
 if  
and  for any 
.

\begin{figure}
\begin{tikzpicture}
          \node[state](i){}; 
          \node[state](m)[right=of i]{}; 
          \node[state](o)[right=of m]{};
 \path[to]
       (i) edge[thick] (m)
       (m) edge[thick] (o);
   \begin{pgfonlayer}{background}
    \node [background,fit=(m)] {};
    \end{pgfonlayer}
    \end{tikzpicture}
\caption{The specification  for the routing policy.}
\label{fig:routing.model}
\end{figure}

The second problem we need to tackle is that of ensuring 
that the specification we define for the routing policy 
is a finitary network. 
This is necessary because our proof techniques 
are valid only for such networks. As we will see, 
this can be accomplished by considering a more restricted 
routing policy, in which only a finite amount of messages 
will be routed from the source to the destination.

Let ; the specification we propose for the connection-less 
routing policy of  values is given by the network 
, where 
 is the connectivity graph depicted 
in Figure \ref{fig:routing.model}
and  is a system term (parametrised 
by a multiset  and an integer ) 
defined as 
\smallskip

\noindent Let us discuss the intuitive behaviour of a network of the form 
; at any given point, the internal node 
 can either receive a message from node , provided that 
there are still messages to be routed, or it can forward 
one of the messages in the multiset  to the 
output node , if any. Note that we require the use 
of multisets since any value  
can be broadcast more than once by the input node .

Formally, the behaviour of a network  
can be described as follows. 
\begin{prop}
\label{prop:routing.model.transitions}
For any  and finite multiset  
\begin{enumerate}
\item  is convergent and finitary,
\item  if and 
only if ,
\item if  then  
 if and only 
if , 
\item if  then  if and only if , 
\item  if 
and only if  and .\qed
\end{enumerate}
\end{prop}

\begin{figure}
     \begin{tikzpicture}
          \node[state](i){}; 
          \node[state](n1)[right=of i]{};
          \node(vdots)[right=of n1]{};
          \node[state](n2)[right=of vdots]{}; 
          \node[state](o)[right=of n2]{};
          \node[state](n3)[above=of vdots]{};
          \node[state](nj)[below=of vdots]{}; 
 \path[to]
       (i) edge [thick] (n1)
       (n2) edge[thick] (o)
       (n1) edge[thick] (n3)
       (n1) edge[thick] (vdots)
       (n1) edge[thick] (nj);
 \path[tofrom]
 				(n3) edge[thick] (n2)
 				(vdots) edge[thick] (n2)
 				(nj) edge[thick] (n2)
 				(n3) edge[thick] (vdots)
 				(nj) edge[thick] (vdots);   
   \begin{pgfonlayer}{background}
    \node [background,fit=(n1) (n2) (n3) (nj) (vdots)] {};
    \end{pgfonlayer}
    \end{tikzpicture}
\caption{The connectivity graph of the networks in the protocol 
.}
\label{fig:routing.impl.conngraph}
\end{figure}

Let us now define a protocol which is consistent with 
the specification . 
As we already mentioned, a protocol is a collection of 
networks. 
We consider only the set of networks of the form 
 which satisfy the 
following conditions.

\begin{enumerate}
\item ,
\item  for some 
, 
\item  if and only if , 
\item  if and only if ,
\item  for any , 
\item for any node  there exists a path 
from  to  in ,
\item for any node , there exists a probability 
distribution  such that 
, 
\item we assume a set of distinct channels  such that 
 for any , 
\item The system term  is in the 
support of a distribution , defined as 

where  and 

Here we use  to denote the process such that 

\end{enumerate}
\noindent
We denote with  the set of networks 
 described above. The connectivity graph 
of such networks is depicted in Figure \ref{fig:routing.impl.conngraph}

\begin{remark}
Note that we committed an abuse of notation in defining the 
distribution , by associating a 
process definition with a (probabilistic) process, rather than to 
a state. However, a process definition of the form  can be seen as the probabilistic 
process , where 
 and  is obtained from  by replacing 
each occurrence of  with .
\end{remark}
\noindent
Our aim is to show that for any  
we have that . 

Before supplying the details of the proof of the statement above, 
let us describe informally the behaviour of a distribution 
; we also discuss the 
requirements that we have placed on the structure of the 
connectivity graph . 
In a distribution  
a network is waiting to receive exactly  messages from node , 
and whose multiset of received messages which have been 
received but have not yet been forwarded to the external node  
is .
Note that we have placed many requirements in the definition of 
the connectivity graph of such networks; first we require that 
 is their only input node, while  is their only output node. 
This requirement is necessary, since to show that such 
networks are testing equivalent to the specification we 
have to ensure that they share with the latter the same sets 
of input and output nodes.

We require the connectivity graph of the networks in  to have 
a path from  to  for every . 
This condition is needed to ensure that messages detected by 
node  (which in turn have been broadcast by ) can flow 
through the network until reaching node , which in turn 
can broadcast the message to the output node . 
As we will see this always happens with probability .

The other constraints that we placed on the connectivity graph 
of  are purely technical; we require that 
the only node connected to  is , while the only node 
connected to  is . As we will see when discussing the 
code running at , nodes  and  
have the role of handling the values broadcast by , and 
which have to be forwarded to , respectively. 
We also require that  
for any . This constraint ensures that all 
the messages received by  have been broadcast by the 
input node ; note in fact that, in general, a node 
cannot detect the name of the node that fired 
a broadcast.

Let us now turn our attention to the code defined for 
the distribution 
. Here we assume a set of 
channels ;   
each node  can only detect messages 
broadcast along the channel . Intuitively, when a 
message is broadcast along channel  by a 
node , then it will be delivered to 
node . In other words, node  has 
selected  as the next hop in a routing path. 

We also assume a set of probability distributions 
. When a node  
wishes to select the next hop in a routing path, 
it selects it according to the probability distribution 
. Note that we require that  
if and only if , 
that is a node can be selected as the next hop in a routing 
path by  if and only if it is in the range of transmission 
of . Further, any neighbour of  can be selected 
as the next hop in a routing path. As we will see, this constraint 
ensures that, in unbounded time, a message stored in node  
will reach the node  with probability .

Any network distribution  
can be seen as a probability distribution of networks running a 
(connection-less) routing algorithm of  messages. 
Such an algorithm is designed by letting any node , with the exception of 
, to select the next hop 
in a routing path probabilistically among its neighbours. 
For node , the message is broadcast along channel  
with probability , thus forwarding it to the only output node 
.
Also, the message to be forwarded to a next-hop in a 
routing path by node  is selected non-deterministically 
among those stored in such a node, that is the nodes in 
the multiset . 



Roughly speaking, the behaviour of a network  
can be described as follows:
\begin{enumerate}
\item node  can receive a message  broadcast by node 
 along channel , provided . Then it stores it 
in the multiset associated to it,
\item At any given point, any node  
can select the next hop in a routing path among its neighbours. 
Then it selects the message to be forwarded non-deterministically 
among those stored in its internal multiset
\item At any given point, node  can broadcast one of 
the messages stored in its multiset along channel . 
This broadcast is detected by the output node .
\end{enumerate}

\noindent The behaviour of a network  
is similar, with the only exception that the first time 
each node receives a message, the next-hop of a routing 
path it chooses is fixed.

Let us now turn our attention to the extensional 
transitions performed by a network , 
and more generally by a distribution .
To this end it is useful to introduce 
some notation. 
First we define the (state based) processes

\noindent
and we note that any network  has the form 

\noindent
where . 
For such networks, we define . 
Intuitively, this function returns the multiset of values stored at 
node  in the network .

Finally, let  be the unique 
network such that .
This is the network where all the nodes that have to be routed are 
stored in the node ; therefore they are ready to be forwarded to 
the destination .

We are now ready to characterise the set of strong extensional 
transitions performed by any network .

\begin{prop}
\label{prop:routing.impl.strong}
For any network , 
\begin{enumerate} 
\item 
\begin{enumerate}[label=(\roman*)]
\item  
iff , 
\item otherwise  for some 
, 
\end{enumerate}
\item conversely, whenever  
then ,
\item if  then 
\begin{enumerate}[label=(\roman*)]
\item  
for some , 
\item conversely, whenever  then 
, 
\end{enumerate}
\item if  then  iff 
,
\item for any  then 
 for some 
,
\item conversely, whenever  then  and 
.\qed
\end{enumerate}
\end{prop}

However, in order to show that the specification 
 is testing equivalent to any network 
in  we have to characterise 
also the set of weak extensional actions performed 
by such networks. To this end, we first analyse 
the structure of any -extensional transition performed 
by any distribution . 

\begin{prop}
\label{prop:stop.reachable}
Let ,  be a finite multiset and 
suppose . 
\begin{enumerate}
\item ,
\item whenever , then 
.
\end{enumerate}
\end{prop}

\begin{proof}[Outline of the Proof]
First note that, for any  
Proposition \ref{prop:routing.impl.strong} 
ensures that  
implies . 

Let us focus on the proof of the first statement.
Let  
for some  and finite multiset . 
We actually prove a stronger statement than (1), 
that is that . 
First note that Theorem \ref{thm:hyper} (4) 
ensures that there exists a sub-distribution 
 such that . 
Such a distribution  has to be an element 
of the set ; 
further, any state in its support should not be 
able to perform an extensional -action. 
It follows from Proposition \ref{prop:routing.impl.strong} 
that the only possibility is that , or equivalently that 
 
for some . It remains to prove that 
.

This follows because the probability distribution used 
by any node , to select the next-hop in a 
routing path is defined so that any neighbour of 
 can be chosen with probability strictly greater 
than ; in particular, since we are 
assuming that there exists a path from node  to node , 
a node  whose distance to  is less than the 
distance between  and  can be selected with 
non-negligible probability. As a consequence, in 
the long run the average distance between the node 
where a message  is stored and 
the node  decreases to ; that is, 
with probability  message  is stored in 
the node . 
Since this line of reasoning is independent from 
the value , we also have that in the long run 
any message in  
will be stored in  with probability ; 
formally, .

Now statement (2) follows trivially. Whenever  
we have that , and 
by (1) above it follows that . 
\end{proof}

\begin{cor}
\label{cor:impl.convergent}
Any  is 
convergent.
\end{cor}

\begin{proof}
Suppose  for some ; 
then ; 
it follows that , 
hence . As a consequence, 
for no network  we have 
.
\end{proof}

The last step that we need to take is that of 
characterising the set of (weak) input and output 
transitions for any distribution . This 
can be done by using both propositions 
\ref{prop:routing.impl.strong} and 
\ref{prop:stop.reachable}.

\begin{prop}
\label{prop:routing.impl.weak}
Let  and  be a multiset. 
Then for any distribution ,

\begin{enumerate}[label=(\roman*)]
\item  with 
 for any  
if and only if , 
\item if  then 
\begin{enumerate}[label=(\roman*)] 
\item  for some 
 for some , 
\item conversely, whenever  then 
,
\end{enumerate}

\item if  then 
\begin{enumerate}[label=(\roman*)]
\item  for some 
, 
\item whenever  then 
, 
\end{enumerate}

\item if , 
\begin{enumerate}
\item  for 
some , 
\item conversely, whenever  it follows that .
\qed
\end{enumerate}
\end{enumerate}
\end{prop}

\noindent We are now ready to show that the protocol  
satisfies the specification .
\begin{thm}
\label{thm:routing.equivalence}
For any  and  
we have that . 
\end{thm}

\begin{proof}
Let  and  be a finite multiset. 
We have already noted that the network   
is finitary. Further, it is easy to show that any network 
 is finite state, 
and by Corollary \ref{cor:impl.convergent} it follows that 
it is also finitary.

Therefore, it suffices to show that for any  
we have both  and 
. Theorem 
\ref{thm:must.dfsound} gives that , while Theorem \ref{thm:may.dfsound} ensures that 
. 

In fact we prove a stronger statement. 
For any , finite multiset  
and network  
we have that , 
and conversely .
Theorem \ref{thm:routing.equivalence} follows by 
letting .

To this end, consider the relation

\noindent
We show that this relation satisfies the requirements of 
Definition \ref{def:dfdeadsim}. 
First suppose that . 
Then  by Proposition 
\ref{prop:routing.model.transitions}, and
Proposition \ref{prop:routing.impl.weak} ensures that 
 
for some  such that for any  
we have that . 

Now, suppose that . 
By Proposition \ref{prop:routing.model.transitions} we have 
two possible cases: 

\begin{enumerate}
\item ; in this case ; 
by Proposition \ref{prop:routing.impl.weak} we have 
that  for some , and trivially 
. 
\item ; here . 
The action  
can be matched by , 
where , 
again using Proposition \ref{prop:routing.impl.weak}.
\end{enumerate}

\noindent The last case we need to check is . 
This case is handled in the same way of the previous ones, again using 
Propositions \ref{prop:routing.model.transitions} 
and \ref{prop:routing.impl.weak}.

For the opposite implication, ,
it is sufficient to consider the converse relation , 
showing that it satisfies the requirements of Definition \ref{def:dfdeadsim}.
The proof is similar to the one above, this time by using 
Proposition \ref{prop:routing.impl.strong} to infer the 
structure of an extensional action of the form 
, and by matching 
it with an action performed by  
according to Proposition \ref{prop:routing.model.transitions}. 
Here it is important to note that every action of the 
form  is also a  
weak action, that is , 
and that a strong -extensional action of the form 
 can be matched by the weak 
action .
\end{proof}

\section{Conclusions}
\label{sec:conclusions}

In this paper we have developed a calculus for wireless systems, which enjoys both probabilistic behaviour 
and local broadcast communication. We have developed a theory based on the probabilistic testing preorders, 
and provided sound proof methods for finitary networks to prove that they can be related via our behavioural preorders.
We have applied our proof techniques to check that a probabilistic routing protocol is consistent 
with a given specification.

While testing theories have been analysed for process algebras \cite{Ene02} with broadcast communication 
over a flat topology, we believe that this is the first work that considers testing theories for 
a calculus which enjoys local broadcast communication.

In the past the development of formal tools for wireless networks has focused either on other forms of 
behavioural theories (such as variants of weak bisimulation) and the analysis of protocols. Here we give a brief 
review of the main works which have inspired our calculus.

To the best of our knowledge, the first paper describing a process calculus for broadcasting systems, \textbf{CBS}, is 
\cite{Prasad95}. In this paper the author presents a simple process calculus 
in which a synchronisation between a sender and a receiver is modelled as an 
output action, rather than an internal activity as in standard 
process calculi such as CCS. This allows multiple receivers to detect 
a message sent by a sender, thus implementing broadcast communication. 
In \cite{HenRat98} different notions of barbed congruence for a variant of CBS are introduced; 
these correspond to strong barbed congruence and weak barbed congruence. For each of 
them, a characterisation result in terms of strong and weak bisimulation, 
respectively, is proved. 

Another calculus to model broadcast systems, known as the -calculus 
and inspired by both CBS and the -calculus \cite{pibook}, is introduced in \cite{Ene01}; 
as the author points out, broadcast communication is modelled in the same style of 
CBS. In this paper the authors define three different behavioural equivalences, 
corresponding to barbed congruence, step equivalence and labelled bisimilarity. 
The author proves that such behavioural equivalences coincide. 

In \cite{Ene02} the authors define both the may and must testing preorders for processes of the 
-calculus, and they prove a characterisation result for each of them. 
The main contribution here lies in the characterisation of the must-testing 
preorder; as the authors point out, in fact, broadcast communication leads 
to a non-standard characterisation of the latter. In particular, the non-blocking 
nature of broadcast actions does not allow acceptance sets to be used in their 
characterisation result.

In the last decade, broadcast calculi have been modified in several ways by 
equipping processes with a topological structure, thus modelling wireless 
networks; the idea is that 
of representing a process as a set of locations, running different code 
for broadcasting and receiving messages; the topology defined for a process 
establishes how communication is modelled, for example by letting only some 
locations being able to detect the messages broadcast at another one.

In \cite{NanHan06} the authors propose to model the topological 
structure of a network by using a connectivity graph; a process 
is viewed as a set of locations running code, while a graph 
whose vertices are locations 
is used to determine how communication is carried out. Intuitively, 
a transmission originated at a given location can only be detected 
by those vertices which are connected to the former. The transition 
relation of processes is defined as parametric in a connectivity graph. 
This framework has been proposed by the authors as a basis for the 
analysis of security protocols in wireless networks.

In \cite{nanz} an allocation environment is used to represent the 
topological structure of a wireless networks. A wireless network 
is intended as a parallel composition of processes, each of which 
is associated with a set of locations to which the process belongs 
and a probability distribution 
over locations; intuitively, the latter describes the probability 
with which a message broadcast by the process is detected at a 
given location.

In \cite{restrbroad} the authors propose a \emph{restricted 
broadcast process theory} to model wireless networks. 
Here a network consists of a parallel composition 
of different processes; each process is associated with a location name, 
and a function between locations to sets of locations 
is used to represent the network topology. The authors 
propose the standard notion of weak bisimulation as 
the behavioural equivalence to be used to relate networks 
and they show a case study in which they prove the correctness 
of a routing protocol.

In \cite{GwFM10} an extension of the restricted broadcast process theory, 
the \emph{Computed Network Theory}, is proposed; here the expressive power of a network 
is augmented through different operators. 
For the resulting calculus, a variant of strong bisimulation is 
defined and proved to be a congruence. The main result in the paper is a sound 
axiomatisation of the strong bisimulation, thus 
enabling equational reasoning for wireless networks. The authors 
also show that the proposed axiomatisation is complete in a setting 
where only non-recursive networks are considered.
The \emph{Computed Network Theory} framework is also used in 
\cite{wfmodcheck} to check properties of mobile networks; 
the authors show how both the equational theory and model checking 
can be used to verify the correctness of a routing protocol.

In \cite{omegacalc} the authors view a network as a collection of 
processes, each of which is associated with one or more groups. 
Processes which belong to the same group are assumed to be 
neighbours; as a consequence, a broadcast performed by a 
process can be detected by all the processes which belong 
to at least one group of the broadcaster. 
The authors show that in their framework state reachability is 
a decidable problem; further, they introduce different notions 
of behavioural equivalences, based on late bisimilarity and its weak 
variant, and they show that such equivalences are in fact congruences.
Finally, they apply their calculus by formalising and analysing the 
behaviour of a leader election protocol and 
a routing protocol.

In \cite{LaneseS10} the authors describe wireless 
networks by using metric spaces; they assume that a network consists 
of a set of processes, each of which has an associated location 
and a radius of transmission; a metric distance over the set of locations 
is assumed to determine how communication is modelled. The authors 
describe the behaviour of a wireless network in terms of both a 
reduction semantics and a labelled transition semantics. These 
two semantics are proved to be equivalent up-to a notion of 
structural congruence. We remark that in their paper the authors 
assume that a communication between two stations consists of 
two phases, one for the beginning and one for termination. 
This allows the authors to model collision-prone communication. 

In \cite{wang} the authors present an extension of the calculus described above, 
in which node mobility and timed communication are introduced. The 
authors give both a reduction semantics and a labelled transition semantics, 
and they prove that they are congruent up-to structural congruence.

Another calculus for wireless networks in which collision-prone behaviour is taken 
into account is described in \cite{merro}. In their work, the authors 
describe a network as a set of processes running in parallel, each of 
which has a location name and a semantic tag associated with it; the latter 
consists of a set of locations names and it corresponds to the set 
of locations which can detect messages broadcast by the process. 
The calculus includes a notion of discrete-time, in the style of 
\cite{HenReg95}, and broadcasts of messages start and end at different 
time slots. The authors develop a notion of barbed congruence for 
wireless systems and they 
propose a sound, but not complete, characterisation result in terms 
of weak bisimulation. 

A variant of this calculus which considers only networks with 
flat topology is presented in \cite{CHM12}. Here the 
authors develop a notion of reduction barbed congruence for their 
calculus; they 
also introduce an extensional semantics whose induced weak 
bisimulation principle is proved to be sound and complete 
with respect to the barbed congruence.

In \cite{GodSon} the authors propose a model in which the topological 
structure of a network is represented as a graph whose vertices are 
locations; further, they assign 
to each edge in the graph a (possibly unknown) probability as a likelihood estimate of 
whether a message broadcast by a location at the starting end-point of an 
edge will be delivered to the location at the terminal end-point of the same. 
The proposed model also allows the network topology of a system to 
evolve according to a probabilistic mobility function. 
The authors prove that, in the proposed calculus, the logical equivalence 
defined over a variant of PCTL coincides with weak bisimulation. 

In \cite{songphd} several models for modelling probabilistic ad hoc networks 
are developed; the author first defines a probabilistic process calculus 
where connections between nodes are probabilistic. Behavioural theories 
based on bisimulation and temporal logics are defined for analysing the properties 
of networks in such a calculus. 
The presented calculus is then extended in order to model different features of 
wireless networks, such as exponential time delays and changes in the network topology. 

In \cite{LanotteM11} the authors define a language for wireless networks 
in which the code running at network locations contains both non-probabilistic 
and non-deterministic behaviour. The topological structure of a network 
is defined in the same way of \cite{merro}; the authors introduce a 
notion of simulation, parametrised in a probability value, in order 
to capture the concept of two networks exhibiting the same behaviour 
up-to such a probability. The model used to represent wireless networks 
and define their formal behaviour is that of a pLTS.

In \cite{gallina2011} the authors propose 
a probabilistic, energy-aware process calculus of networks. 
In this calculus nodes can move probabilistically 
among locations of a given metric space. Nodes can also choose the transmission 
radius of a broadcast in an optimal way with respect to energy consumption. 
The authors propose a notion of probabilistic barbed congruence, parametrised in a 
set of schedulers, for which they give a characterisation in terms of bisimulations. 
The authors also introduce a preorder which compare networks which exhibit 
the same behaviour according to the proposed contextual equivalence, but differ 
in terms of energy consumption.

In \cite{bugliesi2012} a variant of the calculus above is proposed, where 
energy consumption is no longer considered and the possibility of interferences in 
communications is introduced. The authors define a contextual equivalence in 
terms of probabilistic reduction barbed congruence, for which they develop a 
sound and complete proof technique based on bisimulations.

In \cite{ghassemi} a different approach is made to formalise a wireless 
network. The authors identify a network as a set of processes associated
 with a location address and a queue, representing the data 
at the datalink layer that a station has not yet broadcast. The calculus 
they use is a probabilistic generalisation of the restricted broadcast process theory 
of \cite{GwFM10}; here the sending primitive consists of a message to be broadcast and a probability 
rate, representing the likelihood that such a message will be sent. The 
model used to describe the behaviour of a system is that of 
Continuous Time Markov Automata.



\appendix

\section{Properties of the Operator }
\label{sec:operator.results}

\paragraph{\textbf{Proof of Proposition \ref{prop:testp.assoc}(1)}:} 
\label{proof:testP.closed}
Let , , 
and suppose  is defined. 
Then such a network is equal to , for which we have to verify two statements:

\begin{itemize}
\item  satisfies the constraints we 
have placed over all, possibly non well-formed, networks. These require 
 (that is, it does not contain replicated node names),
, and 
 being irreflexive.

\begin{enumerate}

\item ; note that if there were a node name  which appears more 
than once in , then we should have . This is 
because , so that  cannot appear more than once in , nor in . 
Thus the statement follows if we can prove that ; 
since  is defined, 
it follows that 
. 
Since ,
we also have , and there is nothing left to prove.

\item ; 
note that  and . Therefore we have that
 


\item  is irreflexive. Suppose ; We need to 
show that .
Note that we have either  or ; 
without loss of generality, assume . Since  is irreflexive, 
it follows that .
\end{enumerate}

\item  satisfies the constraints 
of Definition \ref{def:well.formed}. This amounts to prove the following:

\begin{enumerate}

\item for any  such that , 
either  or .

Let  be two nodes for which ; 
that is either  or . 
Without loss of generality, assume that .

In this case either  or . 
We only give details for the case in which , as the proof 
for the second case is analogous. 
Since , we have 
that either  or . 
If  then , while 
if , then . 
Thus, either  or .

\item Let  be a node such that  
for no node . Then .

Since  then either  or . 
Without loss of generality let . Also, since  
for no ,  and , then 
we also have that  for no . 

Thus we have that  and  for no . 
Since  is well-formed by hypothesis, we must have , from which it 
follows that .\qed
\end{enumerate}
\end{itemize}

\paragraph{\textbf{Proof of Proposition \ref{prop:testp.assoc}(2)}:}
\label{proof:testP.assoc}

It is sufficient to check that 
 and 
 
if and only if  and 
.
In fact, from this claim it follows that  is 
defined if and only if  is defined; the 
equality of these two networks follows from the associativity of both set 
union and parallel composition of system terms.

Let , 
 and . We prove the two implications above separately.

Suppose that 

We want to show that , and 
. The former 
statement is a straightforward consequence of Equation \eqref{hltest.eq:assoc.2}, 
since .
The second statement can be proved as follows: let . By Equation 
\eqref{hltest.eq:assoc.1} we have that , so that 
it remains to show . This is a trivial consequence 
of Equation \eqref{hltest.eq:assoc.2}; in fact, since , we 
also have , and therefore .

Now suppose that 

We need to show that , and 
. The first statement is 
an immediate consequence of Equation \eqref{hltest.eq:assoc.4}, by noticing 
that . 
For the second 
statement, let  be a node such that . By Equation 
\eqref{hltest.eq:assoc.3} we have that . Also, 
by Equation \eqref{hltest.eq:assoc.4} it holds that ; 
in fact, since , we also have , and therefore . 
Since  and , it follows that 
, as we wanted to prove.\hfill\qed

\paragraph{\textbf{Proof of Proposition \ref{prop:network.decomp}:}}
\label{proof:generators}.
Let  be a well-formed network, and assume that 
. That is, there exists a node name , a state  
and a system term  such that  . 

Let , where 
 is defined by 


Let also , where  is defined by letting 


We need to show the following facts:
\begin{enumerate}
\item \label{generators.1} ,
\item \label{generators.2} , 
\item \label{generators.3} ,
\item \label{generators.4} ,
\item \label{generators.5} ,
\item \label{generators.6} .
\end{enumerate}

Each of the statements above is proved separately. Note that \eqref{generators.6} 
follows by the hypothesis.

\begin{description}
\item[\textbf{Proof of Statement \ref{generators.1}}] .\\
First note that , so that it suffices 
to show that  is  well-formed.
To this end, we show that  satisfies both the constraints that 
we have placed over networks and those given in Definition \ref{def:well.formed}. 
\begin{enumerate}
\item ; this is trivial, since no node name can appear more 
than once in a system term which contains only one node name
\item ; this statement follows from the 
definition of ,  Equation \eqref{hltest.eq:generators.3}, which gives that .
\item  is irreflexive; note that , and the latter is irreflexive. Therefore,  has 
to be irreflexive as well.

\item Whenever  for 
some nodes , then either 
 or . 
Equivalently, we prove that whenever  
for some nodes , then either  or .

Suppose ; then either 
 or . 
Due to the arbitrariness of , it is sufficient to consider only the first case. 

By Definition of , Equation \eqref{hltest.eq:generators.4}, 
either  for some  such that , 
or  for some  such that . 
In the first case we obtain , in the second , and there is nothing left to prove.

\item If  and  for 
no , then , or 
equivalently . 

Note that if  then by Equation \eqref{hltest.eq:generators.3} either , 
in which case there is nothing to prove, or  
. By Equation \ref{hltest.eq:generators.4} we also 
have that , which contradicts the hypothesis.
\end{enumerate}

\item[\textbf{Proof of Statement \ref{generators.2}}] .\\
We need to show that  satisfies the standard requirements we placed over 
all networks, plus the requirements required for a network to be well-formed, that is those 
listed in Definition \ref{def:well.formed}
\begin{enumerate}
\item . 
By hypothesis we already know 
that , and since , it follows that 
no node name appears in  more than once.

\item . This follows immediately from  
Equation \eqref{hltest.eq:generators.1}.

\item  is irreflexive; this follows since, by Equation \eqref{hltest.eq:generators.2}, 
; the latter is irreflexive by hypothesis.

\item Whenever  for some nodes  and , 
then either  or . 

Due to the arbitrariness of node names , it is sufficient to show that 
the property holds whenever . 
Let  be two nodes such that . 
Note that by Equation \eqref{hltest.eq:generators.2} we have that ; since  is well-formed, it follows 
that either  or .

However, since , we also have that  and , 
Equation \eqref{hltest.eq:generators.1} also ensures that .

Thus either  or ; 
but since , and ,  
 is exactly .

\item if  and  for no , 
then . By Equation \ref{hltest.eq:generators.1} either , in 
which case there is nothing to prove, or there exists  such that . 
But this last case is not possible, since it contradicts the hypothesis.

\end{enumerate}

\item[\textbf{Proof of Statement \ref{generators.3}}] .\\
Let ; we need to show that . By Equation \ref{hltest.eq:generators.1} 
there are two possible cases:
\begin{enumerate}
\item , in which case  would contradict the hypothesis that 
, or 
\item ; 
again .
\end{enumerate}

\item[\textbf{Proof of Statement \ref{generators.4}}] .\\ 
Note that equations \eqref{hltest.eq:generators.1} and \eqref{hltest.eq:generators.3} ensure that 
 and , respectively. 
Therefore, it is sufficient to show that . 

Suppose that . There are two possible cases:
\begin{enumerate}
\item ; Since , either 
, in which case  by Equation \eqref{hltest.eq:generators.3}, 
or , in which case  by Equation \eqref{hltest.eq:generators.1},

\item ; since  is well-formed, there exists a node  
such that . Then either , in which case 
Equation \eqref{hltest.eq:generators.3} ensures that , or 
, in which case  by Equation \eqref{hltest.eq:generators.1}.
\end{enumerate}

\item[\textbf{Proof of Statement \ref{generators.5}}] .\\ 
This follows immediately from Equation \eqref{hltest.eq:generators.2} and the fact that . \hfill\qed
\end{description}

\section{Decomposition and Composition Results}
\label{sec:decomposition.results}
To prove Propositions \ref{prop:decomp} and \ref{prop:wea.comp}, 
we first need to prove the following statements for actions which 
can be derived in the intensional semantics: 
\begin{prop}[Weakening]
\label{prop:weakening}

Let  be a network, and let  be such 
that.
Then 

\noindent
where  ranges over the actions .
\end{prop}
\begin{proof}
By structural induction on the proof of the derivation 
.
\end{proof}

\begin{prop}[Strengthening]
\label{prop:strengthening}
Let  be a network, and let  such 
that .
Then 

\noindent
where  ranges over the actions .
\end{prop}

\begin{proof}
By structural induction on the proof of the transition 
.
\end{proof}

\paragraph{\textbf{Proof of Proposition \ref{prop:decomp}}}
\label{proof:decomp}
Let  be a network and  be 
a generating network such that  is defined. 
We prove only the first statement of Proposition \ref{prop:decomp}.  
The details for the other statements are similar. 

Suppose then that . 
By definition of extensional actions we have two possible cases.
\begin{enumerate}
\item  for some . 
We perform a case analysis on whether 
 or .
	\begin{itemize}
	\item If  then by Proposition \ref{prop:tau} we have that 
	 for some  such 
	that  and . Note that if we let  we can rewrite  as 
	; further, Proposition \ref{prop:tau} 
	gives that , which by definition of extensional 
	actions gives .
	\item If  then by Proposition \ref{prop:tau} we have that 
	 for some , while . 
	If we let  we can rewrite . 
	Further, by the definition of the rules in the intensional 
	semantics we have that .
	\end{itemize}
\item  for some  
such that . Again we have to consider two different cases. 
	\begin{itemize}
	\item . By Proposition \ref{prop:broadcast} we have that 
	 
	for some  such that  and , while 
	 for some 
	 such that . 
	It follows from Proposition \ref{prop:input} that 
	, where  and 
	 are such that 
	 
	and . Now we can apply Proposition \ref{prop:strengthening} 
	to the transitions 
	 and  to obtain 
	 
	and , respectively. 
	These two transitions induce, via an application of rule , the 
	transition ; 
	let . 
	The extensional transition induced by the broadcast derived for 
	 can be either an internal action or an extensional 
	broadcast, depending on the topology of . 
	First note that, since , we have that 
	. Therefore we have two possible cases
		\begin{itemize}
		\item If  then we have the 
		transition . 
		Now note that, since , we also have 
		that . 
		Then the transition  could have been derived 
		only via an application of either Rule  or Rule . 
		In both cases we have . 
		\item If  then we have 
		that . 
		In this case the transition  could have been derived 
		only via an application of either Rule  or 
		Rule . In the first case we have that 
		, while in the second case we obtain that 
		 and .	
		\end{itemize}
	Finally, it is now easy to show that .
	
	\item . In this case we have that , that is 
	. By Proposition \ref{prop:broadcast} 
	we have that  for some  such that 
	. This ensures that , where 
	. Further,  for some  such that 
	. 
	By applying Proposition \ref{prop:strengthening} to the last transition 
	we obtain . Whether this intensional 
	transition induces an extensional one depends on the topology 
	. 
		\begin{itemize}
		\item if  then we have the 
		extensional transition , 
		\item otherwise the transition above does not induce an extensional 
		input. However, in this case it is easy to show, using Proposition 
		\ref{prop:input} that . 
		\end{itemize}
	Finally, let . Note that 
	.
	\end{itemize}
\end{enumerate} 


\begin{lem}[Strong Composition of tau-actions]
\label{lem:tau.strong}
Let  be a network, and  be a generating network such that 
 is well-defined. 
If  then 
.
\end{lem} 
\begin{proof}
Let 
If ; then  
for some . By definition of extensional 
tau actions, there are two possibilities: 
\begin{enumerate}
\item  for some . 
By Proposition \ref{prop:weakening} we obtain that 
, and 
finally  by Rule 
.
Note that ; 
\item , and  for 
no ; in particular , which 
also gives . Therefore 
we can infer the transition . 
By Proposition \ref{prop:weakening} we have that 
 implies 
. 
Now we can apply Rule  to obtain the transition 
. 
Note that the last network can be rewritten as . Finally, since  for no 
 and , it follows that 
 for any . Hence we have 
the extensional transition 
. 

\end{enumerate}
\end{proof}

\paragraph{\textbf{Proof of Proposition \ref{prop:wea.comp}}}
\label{proof:composition}
We only prove statements (1)(i), (2)(i) and (2)(ii); details for the other statements 
are similar.

\begin{enumerate}
\item Suppose that  and let  be such that 
 is well-defined. We have to show that 
. 

We first prove a weaker result: 
if  then . 
To see why this is true, rewrite  as 
, where . 
Then there exists a collection of distributions  such that 
 and . 
We can apply Lemma \ref{lem:tau.strong} to each of such transitions to 
obtain 
. It follows that 


Now suppose that . Then there exist two collections 
if sub-distributions
 and  such that 
 and


For any , let , and define  analogously. 
Note that . 
Also, from the previous statement we can infer that 
. This last sub-distribution is exactly 
. Therefore we have that 
. It remains to note that 


\item Suppose now that  for some 
 with ; we have to show 
that 
. 

First note that, whenever , with 
 then . 
We leave the proof of this result to the reader. 
An immediate consequence of the result above is that whenever 
 and , then 
. 

Finally, suppose that , where . 
We proceed by induction on the definition of weak extensional outputs. 
\begin{itemize}
\item The base case is ; 
in this case we have that , as we wanted to prove. 
\item Suppose now that , where  and 
. Note that in this case  and 
. By inductive hypothesis we have that 
, 
and there is nothing left to prove.
\item Suppose that  for some  
such that . Also, suppose that . 
In this case we want to prove that 
, 
where . 
The proof of these statements relies on the following technical result, whose 
proof is left to the reader: if 
 and , then 
, 
where . 
Then the proof of the main result can be performed as in the previous case, 
by noting that if the transition  is induced by 
, 
where  and , then 
it cannot be  and . In this case it is necessary to rely 
on Proposition \ref{prop:wea.comp}(2)(i), which has already been proved.
\end{itemize}
\end{enumerate}




\bibliographystyle{plain}
\bibliography{broadcast}

\end{document}
