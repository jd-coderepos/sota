

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{times}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{marvosym}
\usepackage{makecell}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{caption}


\usepackage[accsupp]{axessibility}  

\newcommand{\highlightChange}{\color{red}}
\def\HC{\highlightChange}
\newcommand{\Note}[1]{{\color{blue} \bf \small [NOTE: #1]}}
\newcommand{\NoteSC}[1]{{\color{red} \bf \small [NOTE: #1]}}
\newcommand{\yiqi}[1]{\textcolor{blue}{#1}}
\newcommand{\Sout}[1]{{\HC\sout{#1}}}
\def\delete{\mbox{\color{blue} \bf \small DELETE:} \qquad}

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\DeclareMathOperator{\Id}{I}

\newcommand{\argmin}{\mathbf{arg min}}
\DeclareMathOperator{\ZeroMatrix}{\mathbf{0}}

\newcommand{\mypar}[1]{{\bf #1.}}

\def\a{\mathbf{a}}
\def\bb{\mathbf{b}}
\def\c{\mathbf{c}}
\def\x{\mathbf{x}}
\def\y{\mathbf{y}}
\def\h{\mathbf{h}}
\def\d{\mathbf{d}}
\def\p{\mathbf{p}}
\def\f{\mathbf{f}}
\def\ee{\mathbf{e}}
\def\s{\mathbf{s}}
\def\t{\mathbf{t}}
\def\vv{\mathbf{v}}
\def\u{\mathbf{u}}
\def\w{\mathbf{w}}
\def\z{\mathbf{z}}
\def\S{\mathcal{S} }
\def\shat{\hat{s}}
\def\N{\mathcal{N}}
\def\V{\mathcal{V}}
\def\E{\mathcal{E}}
\def\M{\mathcal{M}}
\def\U{\mathcal{U}}

\def\Pj{\mathbf{P}}
\def\X{\mathbf{X}}


\def\Aa{\mathbb{A}}
\def\Bb{\mathbb{B}}
\def\Cc{\mathbb{C}}
\def\Dd{\mathbb{D}}
\def\Ee{\mathbb{E}}
\def\Ff{\mathbb{F}}
\def\Gg{\mathbb{G}}
\def\Hh{\mathbb{H}}


\def\xww{\widetilde{\x}}
\def\sw{\widehat{\s}}
\def\sww{\widetilde{\s}}
\def\tw{\widehat{\t}}
\def\yw{\widehat{\y}}

\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\LL}{L}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\TV}{TV}
\DeclareMathOperator{\DFT}{F}
\DeclareMathOperator{\BL}{BL}
\DeclareMathOperator{\PL}{PL}
\DeclareMathOperator{\PC}{PC}
\DeclareMathOperator{\PPL}{PPL}
\DeclareMathOperator{\PBL}{PBL}
\DeclareMathOperator{\STV}{S}
\DeclareMathOperator{\Adj}{A}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\CC}{C}
\DeclareMathOperator{\D}{D}
\DeclareMathOperator{\F}{F}
\DeclareMathOperator{\HH}{H}
\DeclareMathOperator{\Eig}{\Lambda}
\DeclareMathOperator{\Q}{Q}
\DeclareMathOperator{\G}{G}
\DeclareMathOperator{\J}{J}
\DeclareMathOperator{\RR}{R}
\DeclareMathOperator{\Sr}{S}
\DeclareMathOperator{\Vm}{V}
\DeclareMathOperator{\Mm}{M}
\DeclareMathOperator{\EE}{E}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\Y}{Y}
\def\Xw{\widehat{\X}}
\DeclareMathOperator{\Um}{U}
\DeclareMathOperator{\Z}{Z}
\DeclareMathOperator{\W}{W}
\DeclareMathOperator{\Ss}{S}
\newcommand{\DSPG}{}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}       \newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}} \newcommand{\TBstrut}{\Tstrut\Bstrut} \newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\newcommand{\yg}[1]{{\color{blue!70!white}#1}}
\newcommand{\xw}[1]{{\color{red}#1}}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1763} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{EqMotion: Equivariant Multi-agent Motion Prediction \\ with Invariant Interaction Reasoning}
\author{Chenxin Xu\textsuperscript{1,2}, Robby T. Tan\textsuperscript{2}, Yuhong Tan\textsuperscript{1}, Siheng Chen\textsuperscript{1,3\footnotemark[1]},  \\ Yu Guang Wang\textsuperscript{1}, Xinchao Wang\textsuperscript{2}, Yanfeng Wang\textsuperscript{3,1}
\\\textsuperscript{1}Shanghai Jiao Tong University,  
\textsuperscript{2}National University of Singapore,
\textsuperscript{3}Shanghai AI Laboratory
\\
{\tt\small {\{xcxwakaka,tyheeeer,sihengc,yuguang.wang,wangyanfeng\}@sjtu.edu.cn},} 
\tt\small {\{robby.tan,xinchao\}@nus.edu.sg}
}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding author.}


\begin{abstract}

Learning to predict agent motions with relationship reasoning is important for many applications.
In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods.
To fill this gap, we propose EqMotion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. 
We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by . 
Code is available at \url{https://github.com/MediaBrain-SJTU/EqMotion}.
\end{abstract}



\section{Introduction}


\begin{figure}[t] 
\centering
\includegraphics[width=0.47\textwidth]{imgs/jewel.pdf}
\vspace{-3mm}
\caption{\small Motion equivariance and interaction invariance under the Euclidean
geometric transformation is a fundamental principle for a prediction model, but this principle is often overlooked by previous works. In this work, we propose EqMotion to fill this gap.}
\label{fig:jewel}
\vspace{-4mm}
\end{figure}

Motion prediction aims to predict future trajectories of multiple interacting agents given their historical observations. It is widely studied in many applications like physics \cite{battaglia2016interaction, kipf2018neural}, molecule dynamics \cite{chmiela2017machine}, autonomous driving \cite{levinson2011towards} and human-robot interaction \cite{li2019actional,xu2021invariant}.
In the task of motion prediction, an often-overlooked yet fundamental principle is that a prediction model is required to be equivariant under the Euclidean geometric transformation (including translation, rotation and reflection), and at the same time maintain the interaction relationships invariant. 
Motion equivariance here means that if an input motion is transformed under a Euclidean transformation, the output motion must be equally transformed under the same transformation. Interaction invariance means that the way agents interact remains unchanged under the input's transformation. Figure \ref{fig:jewel} shows real-world examples of motion equivariance and interaction invariance.


Employing this principle in a network design brings at least two benefits. First, the network will be robust to arbitrary Euclidean transformations. Second, the network will have the capability of being generalizable over rotations and translations of the data. This capability makes the network more compact, reducing the network's learning burden and contributing to a more accurate prediction. 

Despite the motion equivariance property being important and fundamental, it is often neglected and not guaranteed by most existing motion prediction methods. The main reason is that these methods transform the input motion sequence directly into abstract feature vectors, where the geometric transformations are not traceable, causing the geometric relationships between agents to be irretrievable. Random augmentation will ease the equivariance problem, but it is still unable to guarantee the equivariance property.
\cite{kofinas2021roto} uses non-parametric pre and post coordinate processing to achieve equivariance, but its parametric network structures do not satisfy equivariance. Some methods propose equivariant parametric network structures utilizing the higher-order representations of spherical harmonics \cite{thomas2018tensor,fuchs2020se} or proposing an equivariant message passing \cite{satorras2021n}, but they focus on the state-to-state prediction. This means that they use only one historical timestamp to predict one future timestamp. Consequently, these methods have limitations on utilizing motion's temporal information and modeling interaction relationships since a single-state observation is insufficient for both interaction modeling and temporal dependency modeling.


In this paper, we propose EqMotion, the first motion prediction model that is theoretically equivariant to the input motion under Euclidean geometric transformations based on the parametric network. The proposed EqMotion has three novel designs: equivariant geometric feature learning, invariant pattern feature learning and invariant interaction reasoning. 
To ensure motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable geometric feature through dedicated designs of equivariant operations. The geometric feature preserves motion attributes that are relevant to Euclidean transformations. To promote more comprehensive representation power, we introduce an invariant pattern feature learning module to complement the network with motion attributes that are independent of Euclidean transformations. The pattern features, cooperated into the geometric features, provide expressive motion representations by exploiting motions' spatial-temporal dependencies. 

To further infer the interactions during motion prediction, we propose an invariant interaction reasoning module, which ensures that the captured interaction relationships are invariant to the input motion under Euclidean transformations. 
The module infers an invariant interaction graph by utilizing invariant factors in motions. The edge weights in the interaction graph categorize agents' interactions into different types, leading to better interaction representation.


We conduct extensive experiments on four different scenarios to evaluate our method's effectiveness: particle dynamics, molecule dynamics, 3D human skeleton motion and pedestrian trajectories. Comparing to many task-specific motion prediction methods, our method is generally applicable and achieves state-of-the-art performance in all these tasks by reducing the prediction error by 24.0/30.1/8.6/9.2 respectively.
We also present that EqMotion is lightweight, and has a model size less than 30 of many other models' sizes. We show that EqMotion using only 5  data can achieve a comparable performance with other methods that take full data. As a summary, here are our contributions:



 We propose EqMotion, the first motion prediction model that theoretically ensures sequence-to-sequence motion equivariance based on the parametric network. With equivariance, EqMotion promotes more generalization ability of motion feature learning, leading to more robust and accurate prediction.


 We propose a novel invariant interaction reasoning module, in which the captured interactions between agents are invariant to the input motion under Euclidean geometric transformations. With this, EqMotion achieves more generalization ability and stability in the interaction reasoning.


 We conduct experiments on four types of scenarios and find that EqMotion is applicable to all these different 
tasks, and importantly outperforms existing state-of-the-art methods on all the tasks.


\label{sec:intro}

\section{Related Work}
\vspace{-1mm}
\textbf{Equivariant Networks.} Equivariance first draws high attention on the 2D image domain. Since CNN structure is sensitive to rotations, researchers start to explore rotation-equivariant designs like oriented convolutional filters \cite{cohen2016group,marcos2017rotation}, log-polar transform \cite{esteves2017polar}, circular harmonics \cite{worrall2017harmonic} or steerable filters \cite{weiler2018learning}. Meanwhile, GNN architectures~\cite{yang2020CVPR,yang2020NeurIPS,yang2022deep} exploring symmetries on both rotation and translation have been emerged. Specifically, \cite{ummenhofer2019lagrangian,sanchez2020learning} achieves partial symmetries by promoting translation equivariance. \cite{thomas2018tensor,fuchs2020se} builds filters using spherical harmonics allowing transformations between high-order representations, achieving the rotation and translation equivariance. 
\cite{finzi2020generalizing,hutchinson2021lietransformer} construct a Lie convolution to parameterize transformations into Lie algebra form. \cite{deng2021vector} proposes a series of equivariant layers for point cloud networks. \cite{jing2020learning} propose geometric vector perceptions for protein structure learning. Recently, EGNN \cite{satorras2021n} proposes a simple equivariant message passing form without using computationally expensive high-order representations. \cite{huang2022equivariant} further extends it by considering geometrical constraints. 
However, most existing methods are only applicable to state prediction, limiting 
models from exploiting sequence information. 
\cite{kofinas2021roto} uses pre and post coordinate processing to achieve motion equivariance but its network structure does not satisfy equivariance.
In this work, we propose an equivariant model based on the parametric network which is generally applicable to 
motion prediction tasks 
and achieves a more precise prediction.

\textbf{Motion Prediction.}
Motion prediction has wide application scenarios. \cite{battaglia2016interaction,mrowca2018flexible,sanchez2019hamiltonian} proposes graph neural networks for learning to simulate complex physical systems. \cite{kipf2018neural,graber2020dynamic,li2020evolvegraph,xu2022dynamic} both explicitly infer the interactions relationships and perform prediction in physical systems. For human or vehicle trajectory prediction, social forces \cite{helbing1995social,mehran2009abnormal}, Markov \cite{kitani2012activity,wang2007gaussian} process, and RNNs \cite{alahi2016social,morton2016analysis,vemula2018social} are first methods to employ. Multi-model prediction methods are further proposed like using generator-discriminator structures \cite{gupta2018social,hu2020collaborative}, multi-head output \cite{liang2020learning,tang2021collaborative}, conditional variational autoencoders \cite{mangalam2020not,lee2017desire,salzmann2020trajectron++,yuan2021agentformer,xu2022groupnet,xu2022dynamic,yang2022KF}, memory mechanisms \cite{xu2022remember,marchetti2020mantra}, Gaussian mixture distribution prediction \cite{graber2020dynamic,li2020evolvegraph}. The HD map information is specifically considered in the autonomous driving scenario \cite{hu2020collaborative,chai2019multipath,liang2020garden,casas2018intentnet,gao2020vectornet,liang2020learning,zhong2022aware}. For 3D human skeleton motion prediction, early methods are based on the state prediction \cite{lehrmann2014efficient,taylor2009factored}. Later RNN-based models considering the sequential motion states are proposed \cite{fragkiadaki2015recurrent,walker2017pose,jain2016structural,martinez2017human}. \cite{guo2019human,li2018convolutional} use spatial graph convolutions to directly regress the whole sequences. Besides, some methods \cite{mao2019learning, cai2020learning,mao2020history,li2022skeleton} specifically exploit the correlations between body joints. Multi-scale graphs are built by \cite{li2020dynamic,dang2021msr,li2021symbiotic} to capture different body-level dependencies. In this work, we propose a generally applicable motion prediction network which promotes Euclidean equivariance, a fundamental property but neglected by previous methods, to have a more robust and accurate prediction. 

\begin{figure*}[t] 
\centering
\includegraphics[width=0.96\textwidth]{imgs/network.pdf}
\vspace{-3.5mm}
\caption{\small EqMotion architecture. In EqMotion, we first 
use a feature initialization layer to initialize geometric features and pattern features. We then successively update the geometric features and the pattern features by the equivariant geometric feature learning and invariant pattern feature learning layers, obtaining expressive feature representation. We further propose an invariant reasoning module to infer an interaction graph used in equivariant geometric feature learning. Finally, we use an equivariant output layer to obtain the final prediction.}
\label{fig:network}
\vspace{-3mm}
\end{figure*}

\vspace{-1mm}
\section{Background and Problem Formulation}
\vspace{-1mm}
\subsection{Motion Prediction}
\vspace{-1mm}
Here we introduce the general problem formulation of motion prediction, which aims to generate future motions given the historical observations. Mathematically, consider  agents in a multi-agent system. Let  and  be the th agent's past and future motion, where  and  are the past and future timestamps and  is the dimension of the system space.  usually equals to 2 or 3 (corresponding to 2D or 3D case). The th timestamp locations  and  are -dimension vectors. The whole system's past and future motion is represented as  and . We aim to propose a prediction network  so that the predicted future motions  are as close to the ground-truth future motions  as possible.

\subsection{Equivariance and Invariance}
\vspace{-1mm}
The Euclidean geometric transformation has three basic forms: translation, rotation and reflection. The translation is modeled by a translation vector and rotation (or reflection) is modeled by an orthogonal rotation (or reflection) matrix. Let  be a translation vector  and  be a  rotation  matrix, we have the following definitions for equivariance and invariance:
\vspace{-1.5mm}
\begin{definition}
\label{def:equivariance}
Let  be an input,  be an operation and  be the corresponding output. The operation  is called equivariant under Euclidean transformation if

\vspace{-3mm}
\end{definition}
\begin{definition}
\vspace{-4mm}
\label{def:invariance}
Let  be an input,  be an operation and  be the corresponding output. The operation  is called invariant under Euclidean transformation if

\end{definition}
\vspace{-2mm}
Here, when a motion prediction network  satisfies Definition~\ref{def:equivariance},  is said motion equivariant. When an interaction reasoning model  satisfies Definition~\ref{def:invariance},  is considered interaction invariant. We also say the output  is equivariant/invariant (to the input motion under Euclidean transformation) if Definition 
\ref{def:equivariance}/\ref{def:invariance} is satisfied.


In the following section, we will introduce our motion prediction network along with geometric features that are equivariant under Euclidean transformations, and pattern features along with the interaction reasoning module are invariant under Euclidean transformations.



\section{Methodology} 
\vspace{-1mm}
In this section, we present EqMotion, a motion prediction network which is equivariant under Euclidean geometric transformations. The whole network architecture is shown in Figure \ref{fig:network}. The core of EqMotion is to successively learn equivariant geometric features and invariant pattern features by mutual cooperation in designed equivariant/invariant operations, which not only provide expressive motion and interaction representations, but also preserve equivariant/invariant properties.
For agents' motions , the overall procedure of the proposed EqMotion is formulated as

       & \mathbb{G}^{(0)}, \mathbf{H}^{(0)} = \mathcal{F}_{\rm IL}(\mathbb{X}), \label{eq:all1}\\
       &  \{\mathbf{c}_{ij} \} = \mathcal{F}_{\rm IRM}(\mathbb{G}^{(0)}, \mathbf{H}^{(0)}), \label{eq:all4}\\
       & \mathbb{G}^{(\ell+1)} = \mathcal{F}_{\rm EGFL}^{(\ell)}(\mathbb{G}^{(\ell)}, \mathbf{H}^{(\ell)},\{\mathbf{c}_{ij} \}), \label{eq:all2}\\
       & \mathbf{H}^{(\ell+1)} = \mathcal{F}_{\rm IPFL}^{(\ell)}(\mathbb{G}^{(\ell)}, \mathbf{H}^{(\ell)}), \label{eq:all3}\\
       & \widehat{\mathbb{Y}} = \mathcal{F}_{\rm EOL}(\mathbb{G}^{(L)}). \label{eq:all5}
    
Step (\ref{eq:all1}) uses an initialization layer  to obtain initial geometric features  and pattern features . 
For interaction relationship unavailable cases, Step (\ref{eq:all4}) uses an invariant interaction reasoning module  to infer an interaction graph  whose edge weight  is the interaction category between agent  and .
Step (\ref{eq:all2}) uses the th equivariant geometric feature learning layer  to learn the th geometric feature . 
Step (\ref{eq:all3}) uses the th invariant pattern feature learning layer  to learn the th pattern feature . Step (\ref{eq:all2}) and Step (\ref{eq:all3}) will repeat  times. Step (\ref{eq:all5}) uses an equivariant output layer  to obtain the final prediction .

Note that i) to incorporate the geometric feature's equivariance, we need to design equivariant operations for the initialization layer  and the equivariant 
geometric feature learning layer ; ii) to introduce the pattern feature's invariance, we need to design invariant operations for the initialization layer  and the invariant  
pattern feature learning layer ; and iii) the interaction graph categorizes agent's spatial interaction into different categories for better interaction representing. The interaction graph is invariant  due to the reasoning module  design. 
In subsequent sections, we elaborate the details of each step.


\subsection{Feature Initialization}
\label{sec:init}
\vspace{-1mm}
The feature initialization layer aims to obtain initial geometric features and pattern features while equipping them with different functionality. 
The initial geometric feature is denoted as  whose th agent's geometric feature consists of  geometric coordinates. The initial pattern feature is denoted as 
 whose th agent's pattern feature is a -dimensional vector. Given the past motions  whose the th agent's motion is , we obtain two initial features of the th agent: 

where  is a linear function,  is the mean coordinate of all agents all past timestamps.  is the difference operator to obtain the velocity .
 is the velocity magnitude sequence and  is the velocity angle sequence. The superscript  denotes the th element,  is vector 2-norm,  is the function calculating the angle between two vectors and  is an embedding function implemented by MLP or LSTM. 
 represents concatenation. 


The geometric feature preserves both equivariant property and motion attributes that are sensitive to Euclidean geometric transforms since we linearly combine the equivariant locations. 
The pattern feature remains invariant and is sensitive to motion attributes independent of Euclidean geometric transforms.
  

\subsection{Invariant Reasoning Module}
\vspace{-1mm}
\label{sec:reason}
For most scenarios, the interaction relationship is implicit and unavailable. 
Thus, we propose an invariant reasoning module for inferring the interaction category between agents. Note that we design the reasoning module to be invariant as the interaction category is independent of Euclidean transformations. The output of the reasoning module is an invariant interaction graph whose edge weight  is a categorical vector representing the type of interaction between agent  and .  is the interaction category number. To obtain the interaction categorical vector, we perform a message passing operation using the agent's initial pattern feature  and geometric feature :

where  is a column-wise -distance, , ,  are learnable functions implemented by MLPs.  is the softmax function and  is the temperature to control the smoothness of the categorical distribution. The interaction edge weights  will be end-to-end learned with the whole prediction network. 


\subsection{Equivariant Geometric Feature Learning}
\label{sec:geo update}
\vspace{-1mm}
The equivariant geometric feature learning process aims to find more representative agents' geometric features by exploiting their spatial and temporal dependencies, while maintaining the equivariance. 
During the process, we perform i) equivariant inner-agent attention to exploit the temporal dependencies; ii) equivariant inter-agent aggregation to model spatial interactions; and iii) equivariant non-linear function to further enhance the representation ability.


\textbf{Equivariant inner-agent attention}
To exploit the temporal dependency, we perform an attention mechanism on the coordinate dimension of the geometric feature since the coordinate dimension originates from the temporal dimension. 
Given the th agent's geometric feature  and pattern feature , we have  

where  is a function which learns the attention weight for per coordinate, and  is the mean coordinates summing up over all agents and all coordinates. 


The above operation will bring the following benefits: i) The learned attention weight is invariant and the learned dependencies between different timestamps of the motion will not be disturbed by irrelevant Euclidean transformations; and ii) The learned geometric feature is equivariant because of the coordinate-wise linear multiplication.  

\textbf{Equivariant inter-agent aggregation}
The equivariant inter-agent aggregation aims to model spatial interactions between agents. The key idea is to use the reasoned or provided interaction category to learn aggregation weights, and then use the weights to aggregate neighboring agents' geometric features. The aggregation operation reads

where  is the learned aggregation weights between agent  and ,  is a column-wise -distance,  is dot product and  is the th agent's neighboring set. For the th interaction category, we assign a function  that is implemented by MLP to model how the interaction works. 

The social influence from agent  to agent  is modeled by the coordinates' difference between the two agents. The intuition behind this design is that the mutual force between two objects is always in the direction of the line they formed. 


Note that for most scenarios, we do not have an explicit definition of interacted "neighbors", thus we use a fully-connected graph structure, which means every agent's neighbors include all other agents. For some special cases with massive nodes, like point clouds, we construct a local neighboring set by choosing neighbors within a distance threshold.


\textbf{Equivariant non-linear function}
According to Eq.~(\ref{eq:inner}) and Eq.~\eqref{eq:inter}, the operation of inner-agent attention and inter-agent aggregation are linear combinations with agents' coordinates. It is well known that non-linear operation is the key to improving neural networks' expressivity. Therefore, we propose an equivariant non-linear function to enhance the representation ability of our prediction network while preserving equivariance. The key idea is proposing a criterion with the invariance property for splitting different conditions, 
and for each condition design an equivariant equation.
Mathematically, the non-linear function is

where  and   is the learned query coordinates and key coordinates,  are learnable matrices for queries and keys.  is the mean coordinates over all agents and all coordinates. ,  is the th coordinate of  and . 
is the vector inner product.

For every geometric coordinate , we learn a query coordinate  and a key coordinate . We set the criterion as the inner product of the query coordinate and the key coordinate. If the inner product is positive, we directly take the value of the query coordinate as output; otherwise, we clip the query coordinate vector by moving out its parallel components with the key coordinate vector. Finally, we obtain the 
th agent's geometric features of the next layer  by gathering all the coordinates . The non-linear function is equivariant, since the criterion is invariant and the two equations under two conditions are equivariant. 

\subsection{Invariant Pattern Feature Learning}
\label{sec:pattern update}
\vspace{-1mm}
The invariant pattern feature learning aims to obtain a more representative agent pattern feature through interacting with neighbors. Here we learn the agent's pattern feature with an invariant message passing mechanism. Specially, we add the relative geometric feature difference into the edge modeling in the message passing to complement the information of relationships between agent absolute locations, which cannot be obtained by solely using the pattern features. Given the th agent's pattern feature  and geometric feature , the next layer's pattern feature  is obtained by
1mm]
    &\mathbf{p}_{i}^{(\ell)} = \sum_{j \in \mathcal{N}(i)}\mathbf{m}_{ij}^{(\ell)},
    \;\mathbf{h}_i^{(l+1)} = \phi_h^{(\ell)}([\mathbf{h}_i^{(\ell)};\mathbf{p}_{i}^{(\ell)}]).
\end{aligned}

 \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
    \widehat{\mathbf{Y}_i} = \mathbf{W}_{\rm out}(\mathbf{G}_i^{(\ell)}-\overline{\mathbb{G}}^{(\ell)})+\overline{\mathbb{G}}^{(\ell)},

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
    \mathbb{G}^{(0)}\mathbf{R}+ \mathbf{t},\, \mathbf{H}^{(0)} = \mathcal{F}_{\rm IL}(\mathbb{X}\mathbf{R} + \mathbf{t}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
    \{\mathbf{c}_{ij} \} = \mathcal{F}_{\rm IRM}(\mathbb{G}^{(0)}\mathbf{R}+ \mathbf{t}, \mathbf{H}^{(0)}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\mathbb{G}^{(l+1)}\mathbf{R}+ \mathbf{t} = \mathcal{F}_{\rm EGFL}^{(\ell)}(\mathbb{G}^{(\ell)}\mathbf{R}+ \mathbf{t}, \mathbf{H}^{(\ell)},\{\mathbf{c}_{ij} \}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\mathbf{H}^{(l+1)} = \mathcal{F}_{\rm IPFL}^{(\ell)}(\mathbb{G}^{(\ell)}\mathbf{R}+ \mathbf{t}, \mathbf{H}^{(\ell)}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{0pt}
\widehat{\mathbb{Y}}\mathbf{R}+ \mathbf{t} = \mathcal{F}_{\rm EOL}(\mathbb{G}^{(L)}\mathbf{R}+ \mathbf{t}).

        \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\widehat{\mathbb{Y}}\mathbf{R}+\mathbf{t} = \mathcal{F}_{\rm pred}(\mathbb{X}\mathbf{R}+\mathbf{t}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
    \mathbb{G}^{(0)}\mathbf{R}+ \mathbf{t},\, \mathbf{H}^{(0)} = \mathcal{F}_{\rm IL}(\mathbb{X}\mathbf{R} + \mathbf{t}).

\begin{aligned}
&\phi_{\rm init\_g}\big((\mathbf{X}_i\mathbf{R}+\mathbf{t})-\overline{\mathbb{X}}\mathbf{R}+\mathbf{t})\big)+\overline{\mathbb{X}} \mathbf{R}+\mathbf{t} \\
= \;&\mathbf{W}_{\rm init\_g}\big((\mathbf{X}_i\mathbf{R}-\overline{\mathbb{X}}\mathbf{R})\big)+\overline{\mathbb{X}}\mathbf{R}+\mathbf{t} \\
= \;& (\mathbf{W}_{\rm init\_g}(\mathbf{X}_i-\overline{\mathbb{X}})+\overline{\mathbb{X}})\mathbf{R}+\mathbf{t} \\
=\;& \mathbb{G}^{(0)}\mathbf{R}+ \mathbf{t}
\label{eq:init1}
\end{aligned}

\begin{aligned}
&\bigtriangleup (\mathbf{X}_i \mathbf{R}+\mathbf{t})=\bigtriangleup (\mathbf{X}_i)\mathbf{R} = \mathbf{V}_i \mathbf{R}, \\
&{||\mathbf{V}_i^t \mathbf{R}||_2^2}  = {\mathbf{V}_i^t}  \mathbf{R} \mathbf{R}^{\top} {\mathbf{V}_i^t}^{\top} = {\mathbf{V}_i^t}{\mathbf{V}_i^t}^{\top} = {||\mathbf{V}_i^t||_2^2} = \rho_i^t, \\
&\mathrm{angle}(\mathbf{V}_i^t \mathbf{R},\mathbf{V}_i^{t-1} \mathbf{R}) = \frac{\mathbf{V}_i^t \mathbf{R} (\mathbf{V}_i^{t-1} \mathbf{R})^\top }{||\mathbf{V}_i^t \mathbf{R}||_2 ||\mathbf{V}_i^{t-1} \mathbf{R}||_2} \\
& = \frac{\mathbf{V}_i^t \mathbf{R}\mathbf{R}^\top {\mathbf{V}_i^{t-1}}^\top}{||\mathbf{V}_i^t||_2 ||\mathbf{V}_i^{t-1}||_2}
=\frac{\mathbf{V}_i^t {\mathbf{V}_i^{t-1}}^\top}{||\mathbf{V}_i^t||_2 ||\mathbf{V}_i^{t-1}||_2} \\
&=\mathrm{angle}(\mathbf{V}_i^t,\mathbf{V}_i^{t-1}) = \theta_{i}^t, \\
&\phi_{\mathrm {init\_h}}([\, \rho_i;\theta_i\,])=\mathbf{h}_i^{(0)}.
\label{eq:init2}
\end{aligned}

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
    \{\mathbf{c}_{ij} \} = \mathcal{F}_{\rm IRM}(\mathbb{G}^{(0)}\mathbf{R}+ \mathbf{t}, \mathbf{H}^{(0)}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
  \mathrm{sm}\Big(\phi_{\rm rc}\big([\mathbf{h}_i^{\prime};\mathbf{h}_j^{\prime};||\mathbf{G}_i^{(0)}-\mathbf{G}_j^{(0)}||_{2,\mathrm {col}}]\big)/\tau\Big) =  \mathbf{c}_{ij}.

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\mathbb{G}^{(l+1)}\mathbf{R}+ \mathbf{t} = \mathcal{F}_{\rm EGFL}^{(\ell)}(\mathbb{G}^{(\ell)}\mathbf{R}+ \mathbf{t}, \mathbf{H}^{(\ell)},\{\mathbf{c}_{ij} \}).

\begin{aligned}
&\phi^{(\ell)}_{\rm att}(\mathbf{h}_i^{(\ell)}) \cdot (\mathbf{G}_i^{(\ell)}\mathbf{R}+ \mathbf{t} -(\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t}))+\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t} \\
   = \;  &\phi^{(\ell)}_{\rm att}(\mathbf{h}_i^{(\ell)}) \cdot (\mathbf{G}_i^{(\ell)}- \overline{\mathbb{G}}^{(\ell)})\mathbf{R}+\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t} \\
   = \; &(\phi^{(\ell)}_{\rm att}(\mathbf{h}_i^{(\ell)}) \cdot (\mathbf{G}_i^{(\ell)}- \overline{\mathbb{G}}^{(\ell)})+\overline{\mathbb{G}}^{(\ell)})\mathbf{R}+ \mathbf{t}\\
   \rightarrow \; &\mathbf{G}_i^{(\ell)}\mathbf{R}+ \mathbf{t}
\end{aligned}

\begin{aligned}
&\mathbf{G}^{(\ell)}_i \mathbf{R}+ \mathbf{t}+ \sum_{j \in \mathcal{N}_i} \mathbf{e}_{ij}^{(\ell)} \cdot (\mathbf{G}^{(\ell)}_i\mathbf{R}+ \mathbf{t}-(\mathbf{G}^{(\ell)}_j\mathbf{R}+ \mathbf{t})) \\
=\; & \mathbf{G}^{(\ell)}_i \mathbf{R}+ \mathbf{t}+ \sum_{j \in \mathcal{N}_i} \mathbf{e}_{ij}^{(\ell)} \cdot (\mathbf{G}^{(\ell)}_i-\mathbf{G}^{(\ell)}_j)\mathbf{R} \\
=\; & \big(\mathbf{G}^{(\ell)}_i + \sum_{j \in \mathcal{N}_i} \mathbf{e}_{ij}^{(\ell)} \cdot (\mathbf{G}^{(\ell)}_i-\mathbf{G}^{(\ell)}_j)\big)\mathbf{R}+ \mathbf{t} \\
\rightarrow \; & \mathbf{G}^{(\ell)}_i \mathbf{R}+ \mathbf{t}
\end{aligned}

\begin{aligned}
&\mathbf{W}_{\rm Q}^{(\ell)}\big(\mathbf{G}_i^{(\ell)}\mathbf{R}+ \mathbf{t}-(\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t} )\big) = \mathbf{Q}_i^{(\ell)}\mathbf{R}, \\
&\mathbf{W}_{\rm K}^{(\ell)}\big(\mathbf{G}_i^{(\ell)}\mathbf{R}+ \mathbf{t}-(\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t} )\big) = \mathbf{K}_i^{(\ell)}\mathbf{R},\\
& \mathbf{Q}_i^{(\ell)}\mathbf{R}{(\mathbf{K}_i^{(\ell)}\mathbf{R})}^\top = \mathbf{Q}_i^{(\ell)}\mathbf{R}\mathbf{R}^\top{\mathbf{K}_i^{(\ell)}}^\top = \mathbf{Q}_i^{(\ell)}{\mathbf{K}_i^{(\ell)}}^\top.
\end{aligned}

\begin{aligned}
\mathbf{q}_{i,c}^{(\ell)} \mathbf{R}     +\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t} = (\mathbf{q}_{i,c}^{(\ell)}      +\overline{\mathbb{G}}^{(\ell)})\mathbf{R}+ \mathbf{t} = \mathbf{g}_{i,c}^{(\ell+1)}\mathbf{R}+ \mathbf{t}
\end{aligned}

\begin{aligned}
&\mathbf{q}_{i,c}^{(\ell)}\mathbf{R} - \left\langle \mathbf{q}_{i,c}^{(\ell)}\mathbf{R}, \frac{\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}}{||\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}||_2}\right\rangle \frac{\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}}{||\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}||_2} +\overline{\mathbb{G}}^{(\ell)} \mathbf{R}+ \mathbf{t} \\
=\; & \mathbf{q}_{i,c}^{(\ell)}\mathbf{R} - \left\langle \mathbf{q}_{i,c}^{(\ell)}\mathbf{R}, \frac{\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}}{||\mathbf{k}_{i,c}^{(\ell)}||_2}\right\rangle \frac{\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}}{||\mathbf{k}_{i,c}^{(\ell)}||_2} +\overline{\mathbb{G}}^{(\ell)} \mathbf{R}+ \mathbf{t} \\
=\; & \mathbf{q}_{i,c}^{(\ell)}\mathbf{R} - \left\langle \mathbf{q}_{i,c}^{(\ell)}, \frac{\mathbf{k}_{i,c}^{(\ell)}}{||\mathbf{k}_{i,c}^{(\ell)}||_2}\right\rangle \frac{\mathbf{k}_{i,c}^{(\ell)}\mathbf{R}}{||\mathbf{k}_{i,c}^{(\ell)}||_2} +\overline{\mathbb{G}}^{(\ell)} \mathbf{R}+ \mathbf{t} \\
=\; & \left(\mathbf{q}_{i,c}^{(\ell)} - \left\langle \mathbf{q}_{i,c}^{(\ell)}, \frac{\mathbf{k}_{i,c}^{(\ell)}}{||\mathbf{k}_{i,c}^{(\ell)}||_2}\right\rangle \frac{\mathbf{k}_{i,c}^{(\ell)}}{||\mathbf{k}_{i,c}^{(\ell)}||_2} +\overline{\mathbb{G}}^{(\ell)}\right) \mathbf{R}+ \mathbf{t}\\
=\; & \mathbf{g}_{i,c}^{(\ell+1)}\mathbf{R}+ \mathbf{t}
\end{aligned}

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\mathbf{H}^{(l+1)} = \mathcal{F}_{\rm IPFL}^{(\ell)}(\mathbb{G}^{(\ell)}\mathbf{R}+ \mathbf{t}, \mathbf{H}^{(\ell)}).

       \setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\widehat{\mathbb{Y}}\mathbf{R}+ \mathbf{t} = \mathcal{F}_{\rm EOL}(\mathbb{G}^{(L)}\mathbf{R}+ \mathbf{t}).

    \begin{aligned}
&\mathcal{F}_{\rm EOL}(\mathbb{G}^{(L)}\mathbf{R}+ \mathbf{t}) \\
=\; & \big(\mathbf{W}_{\rm out}(\mathbf{G}_i^{(\ell)}\mathbf{R}+ \mathbf{t}-(\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t})\big)+\overline{\mathbb{G}}^{(\ell)}\mathbf{R}+ \mathbf{t} \\
=\; & \big(\mathbf{W}_{\rm out}(\mathbf{G}_i^{(\ell)}-\overline{\mathbb{G}}^{(\ell)})+\overline{\mathbb{G}}^{(\ell)}\big)\mathbf{R}+ \mathbf{t} \\
=\; & \widehat{\mathbb{Y}}\mathbf{R}+ \mathbf{t}
    \end{aligned}

    \begin{aligned}
        \mathbf{G}_i^{(\ell)} \leftarrow \phi_\rho(\rho_i) + \mathbf{G}_i^{(\ell)},
    \end{aligned}

    \mathcal{L} = \mathop{min}\limits_{i}||\mathbb{Y}-\widehat{\mathbb{Y}_i}||^2_2.

Through the loss, the optimal prediction will be optimized.

\section{Experiment Details}

\subsection{Dataset Description}

\subsubsection{Particle Dynamics}
We use the particle N-body simulation environment \cite{kipf2018neural} in a 3-dimensional space similar to \cite{satorras2021n,fuchs2020se}. The system contains 5 interacted particles. In the reasoning task, in the Springs simulation, particles will be randomly connected by a spring with a probability of 0.5. The particles connected by springs interact via forces given by Hooke’s law. In the Charged simulation, particles will be randomly charged or uncharged. The charged particles will repel or attract others via Coulomb forces. The probability of positive charged, uncharged and negative charged is 0.25, 0.5, and 0.25. We predicted the future motion of 20 timestamps given the historical observations of 20 timestamps. We use a downsampling rate of 100. We use 5k, 2k and 2k samples for training, validating and testing, respectively. In the prediction task, the setting is similar except the  probability of positive charged, uncharged and negative charged is 0.5, 0, and 0.5.

\subsubsection{Molecule Dynamics}
We adopt the MD17 \cite{chmiela2017machine} dataset which contains the motions of different molecules generated via a
molecular dynamics simulation environment. The goal is to predict the motions of every atom of the molecule. We randomly pick four kinds of molecules: Aspirin, Benzene, Ethanol and Malonaldehyde. We learn a prediction model for each molecule. 
We predicted the future motion of 10 timestamps given the observation of 10 timestamps. The raw data is a long sequence and we sample the trajectory with a sampling rate of 20 and a sampling gap of 400. We randomly pick 5k, 2k and 2k samples for training, validating and testing. 

\subsubsection{3D Human Skeleton Motion}
Human 3.6M (H3.6M) dataset \cite{ionescu2013human3} contains 7 subjects performing 15 classes of actions, and each subject has 22 body joints. All sequences are downsampled by two along time. Following previous paradigms \cite{martinez2017human,li2020dynamic}, the models are trained on the segmented clips in the 6 subjects and tested on the clips in the 5th subject.

\subsubsection{Pedestrian Trajectories}
ETH-UCY dataset \cite{lerner2007crowds,pellegrini2009you}, contains 5 subsets, ETH, HOTEL, UNIV, ZARA1, and ZARA2. 
In the dataset, pedestrian trajectories are captured at 2.5Hz in multi-agent social scenarios. 
Following the standard setting \cite{alahi2016social,gupta2018social,yuan2021agentformer}, we use 3.2 seconds (8 timestamps) to predict the 4.8 seconds (12 timestamps). 
We use the leave-one-out approach, training on 4 sets and testing on the remaining set.





\subsection{Implementation Details}
In all the experiments, we set the number of feature learning layers  to 4. We use the Adam optimizer to train the model on a single NVIDIA RTX-3090 GPU. All the MLPs have 2 layers with a ReLU activation function. 

\vspace{0.2cm}
\noindent\textbf{Particle Dynamics} We set the number of coordinates in the geometric feature  as 64 and the dimension of the pattern feature  as 64. The predefined category number  is 2. We set the batch size to 50 and use a learning rate of 5e-4. The model is trained for 200 epochs. 

\vspace{0.2cm}
\noindent\textbf{Molecule Dynamics} We set the number of coordinates in the geometric feature  as 64 and the dimension of the pattern feature  as 64. The predefined category number  is 2. We set the batch size to 50 and use a learning rate of 5e-4. The model is trained for 300 epochs. 

\vspace{0.2cm}
\noindent\textbf{Human Skeleton Motion} For short-term motion prediction, we set the number of coordinates in the geometric feature  as 72 and the dimension of the pattern feature  as 64. The predefined category number  is 4. We set the batch size to 100 and use a learning rate of 5e-4. The model is trained for 80 epochs. For long-term motion prediction, we set the number of coordinates in the geometric feature  as 96 and the dimension of the pattern feature  as 64. The predefined category number  is 4. We set the batch size to 100 and use an initial learning rate of 5e-4 with a decay rate of 0.8 for every 2 epochs. The model is trained for 100 epochs.

\vspace{0.2cm}
\noindent\textbf{Pedestrian Trajectories} We set the number of coordinates in the geometric feature  as 64 and the dimension of the pattern feature  as 64. The predefined category number  is 4. We set the batch size to 100 and use an initial learning rate of 8e-4/5e-4/1e-3/5e-4/1e-3 with a decay rate of 0.8/0.8/0.95/0.8/0.9 for every 2/2/2/2/2 epochs on eth/hotel/univ/zara1/zara2 subsets, respectively. The model is trained for 50 epochs.

\begin{table}[t]
  \centering
  \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{6pt}
  \footnotesize
  \caption{\small Effect of different numbers of learning layers on H3.6M.}
  \vspace{-3mm}
\small
    \begin{tabular}{c|ccccc}
    \toprule
        Layers  & 80ms & 160ms & 320ms & 400ms & Average \\
    \midrule
       1 & 9.5 & 21.4 & 46.7 &58.3 & 34.0\\
       2 & 9.3 & 20.7 &45.4&56.5 & 33.0   \\
       3 & \textbf{9.1} & 20.3 & 44.3 & 55.7 & 32.4\\
       \textbf{4} & \textbf{9.1} &\textbf{20.1}& \textbf{43.7} &\textbf{55.0} & \textbf{32.0}\\
       5 & \textbf{9.1}& 20.2& 43.9 & 55.2 & 32.1\\
    
    \bottomrule
    \end{tabular}\label{table:abltion_layers}\vspace{-5mm}
\end{table}


\begin{table*}[t]
\vspace{-0.3cm}
\setlength{\tabcolsep}{3pt}
\caption{\small Comparisons of short-term prediction on Human3.6M. Results at 80ms, 160ms, 320ms, 400ms in the future are shown.}
\vspace{-0.3cm}
\renewcommand\arraystretch{0.9}
\resizebox{\textwidth}{!}{
\tiny
\begin{tabular}{c|cccc|cccc|cccc|cccc} \hline
Motion & \multicolumn{4}{c|}{Walking}                                   & \multicolumn{4}{c|}{Eating}                                    & \multicolumn{4}{c|}{Smoking}                                   & \multicolumn{4}{c}{Discussion}                                \\ \hline
millisecond        & 80          & 160         & 320         & 400         & 80          & 160         & 320         & 400         & 80          & 160          & 320          & 400          & 80           & 160          & 320          & 400          \\ \hline
Res-sup.          & 29.4          & 50.8          & 76.0          & 81.5          & 16.8          & 30.6          & 56.9          & 68.7          & 23.0          & 42.6          & 70.1          & 82.7          & 32.9          & 61.2          & 90.9          & 96.2          \\
Traj-GCN                & 12.3          & 23.0          & 39.8          & 46.1          & 8.4           & 16.9          & 33.2          & 40.7          &  7.9           &  16.2          & 31.9          & 38.9          & 12.5          & 27.4          & 58.5          & 71.7          \\
DMGNN              & 17.3          & 30.7          & 54.6          & 65.2          & 11.0          & 21.4          & 36.2          & 43.9          & 9.0           & 17.6          & 32.1          & 40.3          & 17.3          & 34.8          & 61.0          & 69.8          \\
MSRGCN                &  12.2          &  22.7          &  38.6          &  45.2          &  8.4           & 17.1          &  33.0          &  40.4          & 8.0           & 16.3          &  31.3          &  38.2          &  12.0          &  26.8          &  57.1          &  69.7          \\
PGBIG                &   10.2          &   19.8          &   {34.5}          & {40.3}          & 7.0           & 15.1         & 30.6          &   {38.1}          &   {6.6}           &   {14.1}          &   {28.2}          &   {34.7}          &   {10.0}          &   {23.8}          &   {53.6}          &   {66.7}          \\ 
SPGSN                &   {10.1}          &   {19.4}          &   {34.8}          &   {41.5}          &   {7.1}           &   {14.9}          &   {30.5}          &   {37.9}          &   {6.7}           &   {13.8}          &   {28.0}          &   {34.6}          &   {10.4}          &   {23.8}          &   {53.6}          &   {67.1}          \\
EqMotion(Ours) & \textbf{9.0}& \textbf{17.5}& \textbf{32.6}& \textbf{39.2}&\textbf{6.3}&\textbf{13.6}&\textbf{28.9}&\textbf{36.5}&\textbf{5.5}&\textbf{11.3}&\textbf{23.0}&\textbf{29.3}&\textbf{8.2}&\textbf{18.8}&\textbf{42.1}&\textbf{53.9}\\\hline
Motion           & \multicolumn{4}{c|}{Directions}                                & \multicolumn{4}{c|}{Greeting}                                  & \multicolumn{4}{c|}{Phoning}                                   & \multicolumn{4}{c}{Posing}                                    \\ \hline
millisecond        & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          \\ \hline
Res-sup.          & 35.4          & 57.3          & 76.3          & 87.7          & 34.5          & 63.4          & 124.6         & 142.5         & 38.0          & 69.3          & 115.0         & 126.7         & 36.1          & 69.1          & 130.5         & 157.1         \\
Traj-GCN                & 9.0           & 19.9          & 43.4          &  53.7          & 18.7          & 38.7          & 77.7          & 93.4          & 10.2          & 21.0          & 42.5          & 52.3          & 13.7          & 29.9          &  66.6          &  84.1          \\
DMGNN              & 13.1          & 24.6          & 64.7          & 81.9          & 23.3          & 50.3          & 107.3         & 132.1         & 12.5          & 25.8          & 48.1          & 58.3          & 15.3          & 29.3          & 71.5          & 96.7          \\
MSRGCN                &  8.6           &  19.7          &  43.3          & 53.8          &  16.5          &  37.0          &  77.3          &  93.4          &  10.1          &  20.7          &  41.5          &  51.3          &  12.8          &  29.4          & 67.0          & 85.0          \\
PGBIG                &   {7.2}  &   {17.6} &   {40.9} &   {51.5} &   {15.2} &   {34.1} &   {71.6} &   {87.1} &   {8.3}  &   {18.3} &   {38.7} &   {48.4} &   {10.7} &   {25.7} &   {60.0} &   {76.6} \\ 
SPGSN               & 7.4          & 17.2          & 39.8          & 50.3          & 14.6         & 32.6          & {70.6}         & {86.4}         & 8.7 &18.3& 38.7& 48.5        & 10.7& 25.3& 59.9& 76.5               \\ 
EqMotion(Ours) & \textbf{6.3}&\textbf{15.8}&\textbf{38.9}&\textbf{50.1}&\textbf{12.7}&\textbf{30.1}&\textbf{68.3}&\textbf{85.2}&\textbf{7.4}&\textbf{16.7}&\textbf{36.9}&\textbf{47.0}&\textbf{8.2}&\textbf{18.9}&\textbf{43.4}&\textbf{57.5} \\ \hline
Motion           & \multicolumn{4}{c|}{Purchases}                                 & \multicolumn{4}{c|}{Sitting}                                   & \multicolumn{4}{c|}{Sittingdown}                               & \multicolumn{4}{c}{Takingphoto}                               \\ \hline
millisecond        & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          \\ \hline
Res-sup.          & 36.3          & 60.3          & 86.5          & 95.9          & 42.6          & 81.4          & 134.7         & 151.8         & 47.3          & 86.0          & 145.8         & 168.9         & 26.1          & 47.6          & 81.4          & 94.7          \\
Traj-GCN                & 15.6          & 32.8          & 65.7          &  79.3          & 10.6          &  21.9          & 46.3          & 57.9          & 16.1          &  31.1          &  61.5          &  75.5          & 9.9           &  20.9          & 45.0          & 56.6          \\
DMGNN              & 21.4          & 38.7          & 75.7          & 92.7          & 11.9          & 25.1          & 44.6          &   {50.2}          & 15.0          & 32.9          & 77.1          & 93.0          & 13.6          & 29.0          & 46.0          & 58.8          \\
MSRGCN                &  14.8          &  32.4          & 66.1          & 79.6          &  10.5          & 22.0          &  46.3          & 57.8          &  16.1          & 31.6          & 62.5          & 76.8          &  9.9           & 21.0          &  44.6          &  56.3         \\
PGBIG                &   {12.5} &   {28.7} &   \textbf{60.1} &   \textbf{73.3} &   {8.8}  &   {19.2} &   {42.4} &  53.8 &   {13.9} &   {27.9} &   {57.4} &   {71.5} &   {8.4}  &   {18.9} &   {42.0} &   {53.3} \\ 
SPGSN                & 12.8 &28.6 &61.0& 74.4         & 9.3& 19.4 &{42.3}& {53.6}          & 14.2 &27.7& 56.8& \textbf{70.7}       & 8.8& 18.9& \textbf{41.5}& \textbf{52.7}              \\ 
EqMotion(Ours) &\textbf{11.2} &\textbf{26.8} &{60.5} &75.2&\textbf{8.1}& \textbf{18.0} &\textbf{41.2}& \textbf{52.9}&\textbf{13.0}& \textbf{26.5}& \textbf{56.2}& \textbf{70.7}&\textbf{7.9}& \textbf{17.7}& 40.9& 52.8 \\\hline
Motion           & \multicolumn{4}{c|}{Waiting}                                   & \multicolumn{4}{c|}{Walking Dog}                                & \multicolumn{4}{c|}{Walking Together}                           & \multicolumn{4}{c}{Average}                                       \\ \hline
millisecond        & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          & 80           & 160          & 320          & 400          \\ \hline
Res-sup.          & 30.6          & 57.8          & 106.2         & 121.5         & 64.2          & 102.1         & 141.1         & 164.4         & 26.8          & 50.1          & 80.2          & 92.2          & 34.7          & 62.0          & 101.1         & 115.5         \\
Traj-GCN                & 11.4          & 24.0          & 50.1          & 61.5          & 23.4          & 46.2          & 83.5          & 96.0          &  10.5          & 21.0          & 38.5          & 45.2          & 12.7          & 26.1          & 52.3          & 63.5          \\
DMGNN              & 12.2          & 24.2          & 59.6          & 77.5          & 47.1          & 93.3          & 160.1         & 171.2         & 14.3          & 26.7          & 50.1          & 63.2          & 17.0          & 33.6          & 65.9          & 79.7          \\
MSRGCN                &  10.7          &  23.1          &  48.3          & 59.2          &  20.7          &  42.9          &  80.4          &  93.3          & 10.6          &  20.9          &  37.4         &  43.9          &  12.1          &  25.6          &  51.6          &  62.9         \\


PGBIG                &   {8.9}  &   {20.1} &   {43.6} &   {54.3} &   {18.8} &   {39.3} &   {73.7} &   {86.4} &   {8.7}  &   {18.6} &   {34.4} &   {41.0} &   {10.3}  &   {22.7}  &   {47.4}  &   {58.5} \\ 
SPGSN                & 9.2& 19.8& 43.1& 54.1        &  17.8& 37.2& \textbf{71.7}& \textbf{84.9}               & 8.9 &18.2& 33.8& 40.9        & 10.4 &22.3 &47.1& 58.3         \\ 
EqMotion(Ours) &\textbf{7.6}& \textbf{17.4}& \textbf{39.9}& \textbf{51.1}& \textbf{16.6}&\textbf{36.4} &{72.5}& {86.2}&\textbf{7.8} &\textbf{16.1}& \textbf{30.6}& \textbf{37.1}& \textbf{9.1} &\textbf{20.1} &\textbf{43.7}& \textbf{55.0}\\\hline
\end{tabular}
}
\label{tab:supp_human3.6_shortterm}
\vspace{-3mm}
\end{table*}



\begin{table*}[ht]
\caption{\small Comparisons of long-term prediction on Human3.6M. Results at 560ms and 1000ms in the future are shown.}
\vspace{-0.3cm}
\renewcommand\arraystretch{0.9}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc|cc} \hline
Motion & \multicolumn{2}{c|}{Walking}    & \multicolumn{2}{c|}{Eating}     & \multicolumn{2}{c|}{Smoking}     & \multicolumn{2}{c|}{Discussion}  & \multicolumn{2}{c|}{Directions} & \multicolumn{2}{c|}{Greeting}    & \multicolumn{2}{c|}{Phoning}         & \multicolumn{2}{c}{Posing}      \\ \hline
millisecond        & 560ms         & 1000ms         & 560ms         & 1000ms         & 560ms          & 1000ms         & 560ms          & 1000ms         & 560ms         & 1000ms         & 560ms          & 1000ms         & 560ms            & 1000ms           & 560ms          & 1000ms         \\ \hline
Res-Sup.          & 81.7          & 100.7          & 79.9          & 100.2          & 94.8           & 137.4          & 121.3          & 161.7          & 110.1         & 152.5          & 156.3          & 184.3          & 143.9            & 186.8            & 165.7          & 236.8          \\
Traj-GCN                & 54.1          & 59.8           & 53.4          & 77.8           & 50.7           & 72.6           & 91.6           & 121.5          &   {71.0}          & 101.8          &   {115.4}          & 148.8          & 69.2             & 103.1            &   {114.5}          &   {173.0}          \\
DMGNN              & 71.4          & 85.8           & 58.1          & 86.7           & 50.9           & 72.2           & 81.9           & 138.3          & 102.1         & 135.8          & 144.5          & 170.5          & 71.3             &   {108.4}             & 125.5          & 188.2          \\
MSRGCN                &   {52.7}          &   {63.0}           &   {52.5}          &   {77.1}           &   {49.5}           &   {71.6}           &   {88.6}           &   {117.6} & 71.2          &   {100.6}          & 116.3          &   {147.2}          &   {68.3}             & 104.4            & 116.3          & 174.3          \\
PGBIG                &   {48.1} &   {56.4}  &   {51.1} &   {76.0}  &   {46.5}  &   {69.5}  &   {87.1}  &   {118.2}          &  \textbf{69.3} &   \textbf{100.4} &   {110.2} &   {143.5} &   {65.9}    &   {102.7}   &   {106.1} &   {164.8} \\ 
SPGSN &46.9 &53.6& 49.8& {73.4}& 46.7 &68.6& 89.7& 118.6& 70.1& 100.5& 111.0& {143.2}& 66.7& 102.5& 110.3& 165.4 \\
EqMotion(Ours)  &\textbf{43.4} &\textbf{52.8}& \textbf{48.4}& \textbf{73.0}& \textbf{41.0} &\textbf{63.4}& \textbf{75.3}&\textbf{105.6}&70.4&101.3&\textbf{108.7}&\textbf{142.0}&\textbf{64.7}&\textbf{101.0} &\textbf{84.9}&\textbf{139.4}\\\hline
Motion          & \multicolumn{2}{c|}{Purchases}  & \multicolumn{2}{c|}{Sitting}    & \multicolumn{2}{c|}{Sitting Down} & \multicolumn{2}{c|}{Taking Photo} & \multicolumn{2}{c|}{Waiting}    & \multicolumn{2}{c|}{Walking Dog}  & \multicolumn{2}{c|}{Walking Together} & \multicolumn{2}{c}{Average}     \\ \hline
millisecond        & 560ms         & 1000ms         & 560ms         & 1000ms         & 560ms          & 1000ms         & 560ms          & 1000ms         & 560ms         & 1000ms         & 560ms          & 1000ms         & 560ms            & 1000ms           & 560ms          & 1000ms         \\ \hline
Res-Sup.          & 119.4         & 176.9          & 166.2         & 185.2          & 197.1          & 223.6          & 107.0          & 162.4          & 126.7         & 153.2          & 173.6          & 202.3          & 94.5            & 110.5            & 129.2        & 165.0          \\
Traj-GCN                & 102.0         & 143.5          & 78.3          & 119.7          &   {100.0}          &   {150.2}          &   {77.4}           &   {119.8}          & 79.4          & 108.1          & 111.9          & 148.9          & 55.0             &   {65.6}             & 81.6           & 114.3          \\
DMGNN              & 104.9         & 146.1          &   {75.5}          &   {115.4}          & 118.0          & 174.1          & 78.4           & 123.7          & 85.5         & 113.7          & 183.2          & 210.2          & 70.5             & 86.9            & 93.6          & 127.6          \\
MSRGCN                &   {101.6}         &   {139.2}          & 78.2          & 120.0          & 102.8          & 155.5          & 77.9           & 121.9          &   {76.3}          &   {106.3}          &   {111.9}          &   {148.2}          &   {52.9}             & 65.9             &   {81.1}           &   {114.2}          \\


PGBIG                &  {95.3} &   \textbf{133.3} &   \textbf{74.4} &   \textbf{116.1} &    \textbf{96.7}  &   \textbf{147.8} &   \textbf{74.3}  &   {118.6} &   \textbf{72.2} &   \textbf{103.4} &   {104.7} &   {139.8} &   {51.9}    &   {64.3}    &   {76.9}  &   {110.3} \\ 
SPGSN& 96.5& 133.9& 75.0& 116.2& 98.9& 149.9& 75.6& \textbf{118.2}& 73.5& 103.6& \textbf{102.4}& \textbf{138.0}& 49.8& 60.9& 77.4& 109.6\\
EqMotion(Ours) &\textbf{93.5}&134.5&74.7&116.6&{98.1}&{149.9}&76.7&122.0&71.4&104.6&104.8&141.2&\textbf{44.5}&\textbf{56.0}&\textbf{73.4}&\textbf{106.9}\\\hline

\end{tabular} 
}
\label{tab:supp_Human3.6long-term}
\end{table*}

\section{Further Experiment Results}


\noindent\textbf{Different numbers of layers}
Table \ref{table:abltion_layers} shows the effect of different numbers of feature learning layers  on the H3.6M dataset. We find that i) initially increasing  leads to better performance as a more comprehensive geometric feature and pattern feature will be learned; and ii) when the number of layers is sufficient, the performance tends to be stable. 




\section{Limitation and Future Work}
This work focuses on a generally applicable motion prediction method. In the future, we plan to expand the method by adding specific designs for different tasks to further improve the model performance. We also expect the method can use more types of data to assist prediction, such as images and videos that contain map information.


\end{document}
