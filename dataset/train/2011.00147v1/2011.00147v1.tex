\subsection{Dataset}
We validate our method on two representative domain adaptive semantic segmentation benchmarks, \emph{i.e.}
transferring from the synthetic images (GTAV or SYNTHIA) to the real images (Cityscapes).
Both GTAV and SYNTHIA are synthetic datasets. 
GTAV contains 24,966 images with the resolution around 1914  1024. 
SYNTHIA contains 9,400 images with the resolution of 1280  760. 
The Cityscapes consists of high quality street scene images captured from various
cities. Images are with resolution of 2048  1024. 
The dataset is split into a training set with 2,975 images and a validation set with 500 images. 
For the task GTAV  Cityscapes, we report the results on the common 19 classes.
For the task SYNTHIA  Cityscapes, we follow the conventional protocol \cite{chang2019all,chen2019domain,zou2019confidence,tsai2019domain},
reporting the results on 13 and 16 common classes.
For all of the tasks, the images in the Cityscapes training set are used to train the adaptation model,
and those in the validation set are employed to test the model's performance.
We employ the mean Intersection over Union (mIoU) as the evaluation metric 
which is widely adopted in the semantic segmentation tasks.

\subsection{Implementation Details}
We adopt the DeepLab-V2 \cite{chen2017deeplab} architecture
with ResNet-101 as the backbone.
As a common practice, we adopt the ImageNet \cite{deng2009imagenet} pretrained weights to initialize the backbone.
For the training, we train our model based on stochastic gradient descent (SGD) with momentum of 0.9 and weight decay of .
We employ the poly learning rate schedule with the initial learning rate at .
We train about 11K and 3K iterations with batch size of 8,
for GTAV  Cityscapes and SYNTHIA  Cityscapes respectively.
For all of the tasks, 
we resize images from both domains with the length of shorter edge randomly selected from , while keeping the aspect ratio.
The 730  730 images are randomly cropped for the training.
Random horizontal flipping is also employed.
The resolution for building the associations is at 92  92.
At the test stage, we first resize the image to 1460  730 as input and then upsample the output to 2048  1024 for evaluation.
In all of our experiments, we set , ,  to 0.75, 0.1, 0.01.
We set  to 10.0 which is equivalent to restricting the highest predicted probability among classes at around 0.999.



\subsection{Ablation Studies}
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{6pt}
\begin{table}[ht]
\caption{\label{Ablations} Ablation studies based on task GTAV  Cityscapes and SYNTHIA  Cityscapes.
The ``source-only'' means the model trained using source data only,
while ``source+target'' represents the adaptation with 
labeled source data and unlabeled target data.
The , , , and  in Eq. (\ref{full-obj}) are progressively added to show their effectiveness.
The mIoUs on the Cityscapes test set with respect to 19 and 16 classes are reported for the task GTAV  Cityscapes and SYNTHIA  Cityscapes, respectively.
Our full method is denoted as ``PLCA''.
The ``Sim-PLCA'' means the PLCA is trained 
by directly encouraging the pixel-wise similarities,
while the ``PLCA w/o. SAGG'' denotes PLCA is trained without the 
spatial aggregation module discussed in Section \ref{section-sa}.
}
\vspace{-4mm}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{ |l|c c| c c c | c | c|c|}
\hline
\multirow{2}{*}{Source Dataset}& \multicolumn{2}{c|}{source-only} & \multicolumn{6}{c|}{source  target} \\ 
\cline{2-9}
 &  & + & +  & +  & + & PLCA & Sim-PLCA & PLCA w/o. SAGG\\ 
\hline
GTAV  & 31.5 & 34.3  & 39.7 & 47.4  & 47.7 & \textbf{47.7} & 41.8 & 45.6      \\
SYNTHIA  & 35.4  &36.4 &  40.9  & 46.3   & 46.8  &  \textbf{46.8}  & 42.5 & 44.6 \\
\hline
\end{tabular}}
\end{center}
\end{table}


Table \ref{Ablations} demonstrates the improvement of mIoU compared to the ``source-only'' baseline,
by progressively adding each proposed module into the system.
It can be seen that each module contributes to the final success of the adaptation.
Finally, we achieve 47.7\% and 46.8\% mIoU with GTAV and SYNTHIA as the source respectively,
outperforming the ``source-only'' by +13.4\%  and +10.4\%.

\textbf{Effect of multi-level associations.}
As shown in Table \ref{Ablations}, 
additionally performing the association on predictions (\emph{i.e.} imposing ) brings obvious
improvement, implying the multi-level associations complement each other
to mine the similarities across domains and mitigate the domain shift.

\textbf{Effect of contrastively strengthening the associations.}
As discussed in Section \ref{section-ca}, for the associated pairs of source and target pixels, 
we can strengthen their connections by directly 
encouraging their feature similarities.
In Table \ref{Ablations},
we compare PLCA (which penalizes ) to 
``sim-PLCA'' (which directly encourages the feature similarities). 
It can be seen that PLCA performs significantly better than ``Sim-PLCA'',
verifying the effectiveness of contrastively strengthening the associations.

\textbf{Effect of spatial aggregation on target.}
We remove the spatial aggregation module discussed in Section \ref{section-sa} to verify its effect and necessity.
As shown Table \ref{Ablations}, as expected, without adopting the spatial aggregation on target, 
the mIoU of the adapted model decreases noticeably, implying the necessity of 
reducing the domain discrepancy for more diverse target pixels.








\subsection{Comparisons with the state-of-the-arts}
\begin{table}[t]
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{3pt}
\caption{Comparisons with the state-of-the-arts on 
task GTAV  Cityscapes. The methods we compared to can be roughly categorized into two groups, 
\emph{i.e.}
``AT'' and ``ST'' which denote 
the adversarial training based methods and self-training based methods,
respectively.
All the methods are based on DeepLab-V2 with ResNet-101 
as the backbone for a fair comparison.}
\vspace{-4mm}
\begin{center}
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|ccccccccccccccccccc|c}
\toprule
\multicolumn{22}{c}{\textbf{GTAV  Cityscapes}}                                     

\\ \hline
& \rotatebox{90}{Method}& \rotatebox{90}{road} & \rotatebox{90}{side.} & \rotatebox{90}{build.} & \rotatebox{90}{wall} & \rotatebox{90}{fence} & \rotatebox{90}{pole} & \rotatebox{90}{light} & \rotatebox{90}{sign} & \rotatebox{90}{veg.} & \rotatebox{90}{terrain} & \rotatebox{90}{sky}  & \rotatebox{90}{person} & \rotatebox{90}{rider} & \rotatebox{90}{car}  & \rotatebox{90}{truck} & \rotatebox{90}{bus}  & \rotatebox{90}{train} & \rotatebox{90}{motor} & \rotatebox{90}{bike} & mIoU
\\ \hline 
Source Only & & 34.8& 14.9& 53.4& 15.7& 21.5& 29.7& 35.5& 18.4& 81.9& 13.1& 70.4& 62.0& \textbf{34.4}& 62.7& 21.6& 10.7& 0.7 &\textbf{34.9}& 35.7 & 34.3 \\
\hline
AdaptSeg\cite{tsai2018learning} &  AT&86.5 & 36.0       & 79.9     & 23.4 & 23.3  & 23.9 & 35.2  & 14.8 & 83.4 & 33.3    & 75.6 & 58.5   & 27.6  & 73.7 & 32.5  & 35.4 & 3.9   & 30.1  & 28.1 & 42.4 
\\ 
ADVENT\cite{vu2019advent}   &    AT&   89.4 & 33.1     & 81.0       & 26.6 & \textbf{26.8}  & 27.2 & 33.5  & 24.7 & 83.9 & 36.7    & 78.8 & 58.7   & 30.5  & 84.8 & 38.5  & 44.5 & 1.7   & 31.6  & 32.4 & 45.5 
\\  
CLAN\cite{luo2019taking}     & AT&87.0   & 27.1     & 79.6     & 27.3 & 23.3  & 28.3 & 35.5  & 24.2 & 83.6 & 27.4    & 74.2 & 58.6   & 28.0    & 76.2 & 33.1  & 36.7 & 6.7   & 31.9  & 31.4 & 43.2 \\
DISE\cite{chang2019all} &AT& 91.5 &47.5& \textbf{82.5}& 31.3 &25.6 &33.0& 33.7 &25.8 &82.7 &28.8& \textbf{82.7} &62.4& 30.8 &85.2 &27.7 &34.5 &6.4 &25.2& 24.4 &45.4 \\
SSF-DAN \cite{Du_2019_ICCV}  & AT& 90.3&38.9&81.7& 24.8& 22.9&30.5&\textbf{37.0}&21.2&84.8&38.8&76.9&58.8&30.7& \textbf{85.7}&30.6&38.1&5.9&28.3&36.9&45.4\\

PatchAlign \cite{tsai2019domain} & AT &\textbf{92.3} &51.9 &82.1 &29.2 &25.1 &24.5 &33.8 &\textbf{33.0} &82.4 &32.8 &82.2 &58.6 &27.2 &84.3 &33.4 &46.3 &2.2 &29.5 &32.3 &46.5 \\

MaxSquare\cite{chen2019domain} & ST & 89.4 & 43.0 & 82.1 &30.5 & 21.3 & 30.3 & 34.7 & 24.0 & 85.3 & \textbf{39.4} & 78.2 & 63.0 & 22.9 & 84.6  & 36.4 & 43.0 & 5.5 & 34.7 & 33.5 & 46.4\\
CRST\cite{zou2019confidence} & ST& 91.0   & \textbf{55.4}     & 80.0       & 33.7 & 21.4  & \textbf{37.3} & 32.9  & 24.5 & 85.0  & 34.1    & 80.8 & 57.7   & 24.6  & 84.1 & 27.8  & 30.1 & \textbf{26.9}  & 26.0    & \textbf{42.3} & 47.1 \\





\hline
ours &  & 84.0 &30.4 &{82.4} & \textbf{35.3} &24.8 &32.2 &36.8 &24.5 &\textbf{85.5} &37.2 &78.6 &\textbf{66.9} &32.8 &85.5 &\textbf{40.4} &\textbf{48.0} &8.8 &29.8 &41.8 & \textbf{47.7} \\
\bottomrule 
\end{tabular}
}
\end{center}
\label{gta2city}
\vspace{-3mm}
\end{table}\begin{table}[t]
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{3pt}
\caption{Comparisons with the state-of-the-arts on
task SYNTHIA  Cityscapes.
We report the mIoUs with respect to 13 classes (excluding the 
``wall'', ``fence'', and ``pole'') and 16 classes.
All the methods are based on DeepLab-V2 with ResNet-101 
as the backbone for a fair comparison.
}
\vspace{-4mm}
\begin{center}
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|cccccccccccccccc|cc}
\toprule
\multicolumn{19}{c}{\textbf{SYNTHIA   Cityscapes}}                                                                                                                              
\\ 
\hline
& \rotatebox{90}{Method} & \rotatebox{90}{road} & \rotatebox{90}{side.} & \rotatebox{90}{build.} &\rotatebox{90}{wall*} & \rotatebox{90}{fence*} & \rotatebox{90}{pole*} &\rotatebox{90}{light} & \rotatebox{90}{sign} & \rotatebox{90}{veg.}  & \rotatebox{90}{sky}  & \rotatebox{90}{person} & \rotatebox{90}{rider} & \rotatebox{90}{car}  & \rotatebox{90}{bus} & \rotatebox{90}{motor} & \rotatebox{90}{bike} & mIoU & mIoU*
\\ \hline 
Source Only&&51.2& 21.8& 67.8& 8.2& 0.1& 26.2& 17.7& 17.3& 69.2& 67.1& 52.7& 22.8& 62.3& 31.6& 21.0& 46.1 & 36.4 & 42.2 \\
\hline

AdaptSeg\cite{tsai2018learning} & AT & 84.3 & 42.7     & 77.5  &&&    & 4.7   & 7.0    & 77.9 & 82.5 & 54.3   & 21.0    & 72.3 & 32.2 & 18.9  & 32.3 & &46.7
\\ 
CLAN\cite{luo2019taking}    & AT &81.3 & 37.0       & 80.1  &&&    & 16.1  & 13.7 & 78.2 & 81.5 & 53.4   & 21.2  & 73.0   & 32.9 & 22.6  & 30.7 && 47.8
\\  
SSF-DAN\cite{Du_2019_ICCV} & AT  & 84.6 & 41.7     & 80.8  &&&    & 11.5  & 14.7 & 80.8 & \textbf{85.3} & 57.5   & 21.6  & 82.0   & 36.0   & 19.3  & 34.5 && 50.0 \\
ADVENT\cite{vu2019advent}    &AT & 85.6 & 42.2     & 79.7 & 8.7 &0.4 &25.9     & 5.4   & 8.1  & 80.4 & 84.1 & 57.9   & 23.8  & 73.3 & 36.4 & 14.2  & 33.0   & 41.2 &48.0
\\  
DISE \cite{chang2019all} & AT & \textbf{91.7} & \textbf{53.5} &77.1 &2.5 &0.2 &27.1 &6.2 &7.6 &78.4 &81.2 &55.8 &19.2 &82.3 &30.3 &17.1 &34.3 &41.5 & 48.7 \\
PatchAlign \cite{tsai2019domain} & AT & 82.4 & 38.0 &78.6 &8.7 &0.6 &26.0 &3.9 &11.1 &75.5 &84.6 &53.5 &21.6 &71.4 &32.6 &19.3 &31.7 &40.0 &46.5 \\
 

MaxSquare\cite{chen2019domain} &ST& 82.9&40.7&80.3&10.2&0.8& 25.8&12.8&18.2&82.5&82.2&53.1&18.0&79.0&31.4&10.4&35.6&41.4&48.2\\
CRST \cite{zou2019confidence} & ST & 67.7 &32.2 &73.9 &10.7 &\textbf{1.6} &\textbf{37.4} &22.2 &\textbf{31.2} &80.8 &80.5 &60.8 &\textbf{29.1} &82.8 &25.0 &19.4 &45.3 &43.8 &50.1\\
\hline 
ours &  & 82.6& 29.0& \textbf{81.0} & \textbf{11.2} & 0.2& 33.6& \textbf{24.9} & 18.3& \textbf{82.8} & 82.3& \textbf{62.1} & 26.5& \textbf{85.6}& \textbf{48.9}& \textbf{26.8}& \textbf{52.2} & \textbf{46.8} & \textbf{54.0} \\
\bottomrule
\end{tabular}}
\end{center}
\label{syn2city}
\end{table}We compare our method with previous state-of-the-arts in Table \ref{gta2city} and Table \ref{syn2city}.
For previous methods, we directly cite the published results from corresponding papers for a fair comparison.
All the methods we compared to are based on DeepLab-V2 with ResNet-101 as 
the backbone, which is the same with our method.
Roughly speaking, previous methods can be categorized into two groups:
1) adversarial training based methods (denoted as ``AT''); 2) self-training based methods (denoted as ``ST'').
It can be seen our method outperforms previous state-of-the-art adversarial training based methods,
and performs comparable to or event better than the self-training based methods.

Specifically,  
for the task GTAV  Cityscapes and
task SYNTHIA  Cityscapes,
our method outperforms the adversarial training based method ``AdaptSeg''
by +5.3\% and +7.3\% respectively, and outperforms ``SSF\_DAN'' by +2.3\% and +4.0\% respectively.
The results verify that our way of taking the pixel-wise relationships into account benefits the adaptation.

From Table \ref{gta2city} and Table \ref{syn2city}, 
it can be seen that our method also performs favorably against previous 
state-of-the-art self-training based methods.
For the task GTAV  Cityscapes, 
we achieve 47.7\% mIoU, 
which outperforms CRST (47.1\%) by 0.6\%.
While for the task SYNTHIA  Cityscapes,
our method outperforms CRST by more than 3\%. For the self-training based methods, we notice pyCDA \cite{lian2019constructing} achieves 47.4\% and 46.7\% mIoUs on 
task GTAV  Cityscapes and SYNTHIA  Cityscapes
respectively.
However, it is based on the PSPNet architecture \cite{zhao2017pyramid}, 
which has been shown \cite{zhao2017pyramid,chen2018road} to perform 
 better than DeepLab-V2.
Thus it is not listed in the tables.
Despite this, we achieve slightly better results compared to it.
Moreover, our method is orthogonal to the self-training methods, 
and can be combined with self-training to improve the 
performance further. 


Overall, the results shown in Table \ref{gta2city} and Table \ref{syn2city} illustrate our method 
performs favorably against previous
state-of-the-arts, verifying the effectiveness of our method.

\subsection{Visualization}
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\textwidth]{vis_gtav_3samples.pdf}
\end{center}
\caption{\label{fig:mask_vis}
Visualization of predicted segmentation masks on the test images of Cityscapes.
From the left to the right, the original image, the ground-truth 
segmentation mask, the mask predicted by ``source-only'' network, and
the mask predicted by our PLCA are shown one by one.
Both of the ``source-only'' network and PLCA are trained with GTAV as the source.
}
\end{figure*}

We visualize the masks predicted by our PLCA and compare 
our results to those predicted by the ``source-only'' network in Fig.~\ref{fig:mask_vis}.
The masks predicted by PLCA are smoother
and contain less spurious areas than those predicted by 
the ``source-only'' model,
showing that with PLCA, the pixel-level accuracy of predictions
has been largely improved.
