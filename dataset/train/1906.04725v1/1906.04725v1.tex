\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/cog/teaser_new.png}
  \caption{Given input RGB and Depth images (left), we align oriented cuboids and transform observed data into a canonical coordinate frame.  
  For each voxel, we then extract (from left to right) point cloud density features, 
  3D normal orientation histograms, and \cog descriptors of back-projected image gradient orientations.
  \cog bins (left) are colored to show the alignment between instances. 
The value of the point cloud density feature is proportional to the voxel intensity, 
each 3D orientation histogram bin is assigned a distinct color,
and \cog features are proportional to the normalized energy in
each orientation bin, similarly to HOG descriptors~\cite{dalal2005histograms}.}
\label{fig:cog_bins}
\end{figure*}

\section{Modeling 3D Geometry \& Appearance}
\label{sec:cog}

Feature extraction is one of the most important steps for object detection algorithms. 2D object detectors typically
use either hand-crafted features based on image gradients~\cite{dalal2005histograms,felzenszwalb2010dpm} or learned features
from deep neural networks~\cite{girshick2014rich,girshick15fastrcnn,ren2015faster,he2016res,lin2017focal}. For 3D object 
detection systems with additional depth inputs, Gupta~\etal~\cite{gupta2014learning} use horizontal disparity, height above the ground, 
and the angle of the local surface normal to encode images as a three channel (HHA) map for learning with CNNs.  While convolutional processing of 2D images may be used to extract features from 2D bounding boxes, it does not directly model 3D cuboids.
Song~\etal~propose a deep sliding shape~\cite{song2016deep} method that 
combines TSDF features~\cite{song2014sliding} with standard
2D CNN features to describe 3D cuboids, but do not explicitly model 3D cuboid orientation.

Our object detectors are learned from 3D oriented cuboid annotations in the SUN-RGBD dataset~\cite{song2015sun}.
We discretize each cuboid into a  grid of (large) voxels, and extract features for these  cells.  Voxel dimensions are scaled to match the size of each instance. We use standard descriptors for the 3D geometry of the observed depth image, and propose a novel \emph{cloud of oriented gradient} (\cog) descriptor of RGB appearance. We also introduce simple extensions that improve its performance.


\subsection{Object Geometry: 3D Density and Orientation}
\subsubsection{Point Cloud Density}
Conditioned on a 3D cuboid annotation or detection hypothesis , suppose voxel  contains  points.  We use perspective projection to find the silhouette of each voxel in the image, and compute the area  of that convex region.  The \emph{point cloud density} feature for voxel  then equals .  Normalization gives robustness to depth variation of the object in the scene.  We normalize by the local voxel area, rather than by the total number of points in the cuboid as in some related work~\cite{song2014sliding}, to give greater robustness to partial object occlusions.


\subsubsection{3D Normal Orientations}
Various representations, such as spin images~\cite{johnson1999using}, have been proposed for the vectors normal to a 3D surface.  As in~\cite{song2014sliding}, we build a 25-bin histogram of normal orientations within each voxel, and estimate the normal orientation for each 3D point via a plane fit to its 15 nearest neighbors. This feature  captures the surface shape of cuboid  via patterns of local 3D orientations.


\subsection{Clouds of Oriented Gradients (\cog)}


The \emph{histogram of oriented gradient} (HOG) descriptor~\cite{dalal2005histograms} forms the basis for many effective object detection methods~\cite{pascal-voc-2012}.  Edges are a very natural foundation for indoor scene understanding, due to the strong occluding contours generated by common objects.  However, as gradient orientations are determined by 3D object orientation and perspective projection, HOG descriptors that are naively extracted in 2D image coordinates generalize poorly.

\begin{figure}[t]
\centering
   \includegraphics[width=0.99\linewidth]{figs/cog/cogVShog.pdf}
 \caption{For two corresponding voxels (red and green) on two chairs, we illustrate the orientation histograms that would be computed by a standard HOG descriptor~\cite{dalal2005histograms} in 2D image coordinates, and our \cog descriptor in which perspective geometry is used to align descriptor bins.  Even though these object instances are very similar, their 3D pose leads to wildly different HOG descriptors.}
\label{fig:cog_hog}
\end{figure}

To address this issue, some previous work has restrictively assumed that parts of objects are near-planar so that image warping may be used for alignment~\cite{fidler20123d}, or that all objects have a 3D pose aligned with the global ``Manhattan world coordinates'' of the room~\cite{hedau2010thinking}. 
The \emph{bag of boundaries} (BOB)~\cite{payet2011contours} descriptor builds separate gradient-based models for each of several distinct 3D viewpoints, rather than using geometry to generalize across 3D viewpoints.
Some previous 3D extensions of the HOG descriptor~\cite{buch20093d, scherer2010histograms} assume that a full 3D model is given. 
In recent work~\cite{song2016}, 3D cuboid hypotheses were used to aggregate standard 2D features from a deep convolutional neural network, but the deep features are not conditioned on object orientations.
Our \emph{cloud of oriented gradient} (\cog) feature accurately describes the 3D appearance of objects with complex 3D geometry, as captured by RGB-D cameras from any viewpoint.


\subsubsection{2D Gradient Computation} We compute gradients by applying filters ,  to the RGB channels of the unsmoothed 2D image. The maximum responses across color channels are the gradients  in the  and  directions, with corresponding magnitude .
We follow similar implementation details to the gradient computations used in HOG descriptors~\cite{dalal2005histograms}. 
The 2D unsigned gradients are then aggregated in each voxel to define our 3D \cog descriptor.


\subsubsection{3D Orientation Bins} 
The standard HOG descriptor~\cite{dalal2005histograms} for cell~ of object~ uses nine evenly spaced gradient histogram bins, 
.
For all object instances,
 is aligned with the horizontal image direction.
As shown in Fig.~\ref{fig:cog_hog}, HOG descriptors may thus be inconsistent for (even nearly identical) objects in distinct poses. 

Because objects from the same category typically have similar local 3D structure, for each oriented 3D cuboid proposal, we instead model local gradient statistics in a canonical 3D coordinate frame.
As illustrated in Fig.~\ref{fig:cog_bins}, we define nine evenly spaced 3D orientation bins 

on the front surface (-plane) of each voxel~ within the cuboid.
For all instances,  is aligned with the horizontal 3D -axis
(dark blue lines in Fig.~\ref{fig:cog_bins}). 
Given the camera's intrinsic matrix , and the extrinsic matrix 
encoding the relative 3D pose of cuboid~,
we use perspective projection to map 3D orientation bins to 2D image coordinates:

This transform aligns the 2D orientation bins for distinct 3D cuboids.
For each pixel that back-projects to 3D voxel~, we accumulate its unsigned 2D gradient in the corresponding projected orientation bin to define a nine-dimensional \cog feature .

Some previous work has warped images to align with fixed 2D orientation 
bins~\cite{hedau2010thinking}, but such affine transformations may be unstable for
objects with non-planar geometry. Our \cog descriptor can be seen as accumulating standard gradients with warped histogram bins, rather than warping images to match fixed orientation bins.  This innovation enables our later learning algorithms to better generalize to novel 3D views of complex objects.



\subsubsection{Normalization and Aliasing}  We bilinearly interpolate gradient magnitudes between neighboring orientation bins~\cite{dalal2005histograms}.  To normalize the histogram  for voxel  in cuboid , we then set  for a small . 
Accounting for all orientations and voxels, the dimension of the \cog feature  is . 


\subsection{Extensions of the COG Descriptor}

\subsubsection{View-to-Camera Features}
For single view RGB-D inputs, objects like nightstands and other furniture may only expose one planer surface to the camera. 
At test time, the features of a 3D cuboid proposal oriented away from the camera may
resemble those of a correct detection (see Fig.~\ref{fig:exp_view})
because voxel features are computed by first rotating the cuboid to a canonical 
coordinate frame. However, due to the self-occlusions that occur in real objects, the features modeled by the COG descriptor would in fact not be visible when objects are facing away from the camera.   
Therefore, we add features to represent 
objects' orientation with respect to the camera, and learn to distinguish implausible object hypotheses.


\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{figs/cog/cam_view_feat.pdf}
 \caption{A false positive 3D detection for the nightstand category that occurs without a view-to-camera feature  (top). 
          The COG feature is similar to that of a correct detection (bottom) due to bilateral symmetry.}
   \label{fig:exp_view}
\end{figure}

Specifically, we compute the cosine  of the angle between the cuboid orientation 
and its viewing angle from camera in horizontal direction. Then we define a set of radial basis functions of the form

and space the basis function centers  evenly between  with step size 0.2.  The bandwidth  was chosen using validation data.
Radial basis expansions are a standard non-linear regression method, 
and can be seen as a layer of a neural network. 
We expand the camera angle using this basis representation plus a bias feature, producing an 11-dimensional \emph{view-to-camera} feature .


\subsubsection{Expanded Cuboid Features}
Many object detection systems have a pre-processing stage that generates bounding box proposals that contain objects with well-defined boundaries, instead of amorphous background areas~\cite{alexe2012measuring}. Using a region proposal network to maximize the ``objectness'' score of predicted bounding boxes~\cite{kuo2015deepbox} is thus an essential first step for many state-of-the-art object detection systems~\cite{girshick2014rich,song2016deep}. 

Objectness scores are usually determined from the difference between local and surrounding appearances of each object. 
Instead of designing a separate pre-processing step, we build such contextual cues into our cuboid features. For each cuboid proposal, we expand its size to capture an additional layer of voxels in each direction, so that each cuboid is now described by  voxels. 

Before discussing the training algorithm, we preview the learned weights of \cog descriptors for the chair and toilet categories in Fig.~\ref{fig:expanded_cog}. Toilets are typically placed against the wall in cluttered bathrooms, while there is typically free space around chairs, and thus our expanded cuboid features capture differences between these categories that improve detection accuracy.

\begin{figure}[b]
\centering
\includegraphics[width=0.95\linewidth]{figs/cog/expanded_cog.pdf}
  \caption{Visualizing the learned weights for the COG (left) and expanded COG (right) features. Although chairs and toilets have similar geometric structures, the appearance of the 3D environment immediately surrounding them is different, producing local contextual cues captured by our expanded COG features.}
   \label{fig:expanded_cog}
\end{figure}


The structure of our expanded cuboid feature has some similarities to the ``zoom-out'' features originally proposed for 2D image segmentation~\cite{mostajabi2015feedforward}, and used by Song~\etal~\cite{song2016deep} for 3D detection. We provide ablation studies in Table~\ref{table:ablation}, and demonstrate that this extension is very effective in modeling the geometric structure surrounding each cuboid, improving object detection accuracy.



\subsection{Structured Prediction of Object Cuboids}
\label{sec:ssvm}
For each voxel  in some cuboid  annotated in training image , we have one point cloud density feature , 25 surface normal histogram features , and 9 COG appearance features . For each cuboid~, we have 12 camera view features . Using expanded features with  voxels, our overall representation of cuboid~ is then
. 
Cuboids are aligned via annotated orientations as illustrated in Fig.~\ref{fig:cog_bins},
using the gravity direction provided in the SUN-RGBD dataset~\cite{song2015sun}. 

For each object category  independently, using those images which contain visible instances of that category, our goal is to learn a prediction function 
 that maps an RGB-D image  to a 3D bounding box
.  Here  is the center of the cuboid in 3D,  is the cuboid orientation,  is the physical size of the cuboid along the three axes determined by its orientation, and  is a binary variable indicating whether the object is present in that area of the 3D scene. We assume objects have a base upon which they are typically supported, and thus  is a scalar rotation with respect to the ground plane. 

Given  training examples of category , we use an -slack formulation of the structural support vector machine (SVM) objective~\cite{joachims2009cutting} with margin rescaling constraints:

Here  are the features for oriented cuboid hypothesis  given RGB-D image ,  is the ground-truth cuboid annotation, and  is the set of possible alternative cuboids.
For training images with multiple instances, as in previous work on 2D detection~\cite{vedaldi09structured} we add multiple copies to the training set, each time removing the subset of 3D points contained in other instances.

Given some ground truth cuboid  and estimated cuboid , we define the loss function as follows. If a scene contains ground truth cuboid B and indicator variable , we compute

Here,  is the volume of the 3D intersection of the cuboids, divided by the volume of their 3D union.  The loss is bounded between 0 and 1, and is smallest when the  is near 1 \emph{and} the orientation error .  The loss approaches~1 if either position or orientation is completely wrong. If a scene does not contain any ground truth 
instances of the object and 
the indicator variable  for the cuboid proposal, the loss equals~0. We penalize all other cases with a loss of 1. We solve the loss-sensitive objective of Eq.~\eqref{eq:svm} using a cutting-plane method~\cite{joachims2009cutting}. 


\subsection{Cuboid Hypotheses}
We create cuboid proposals in a sliding-window fashion using discretized
3D world coordinates, with 16 candidate orientations. We
discretize cuboid sizes using empirical statistics of the cuboid annotations
in the training database:
 width quantiles, 
 depth quantiles, and 
height quantiles. Every combination of cuboid size, 3D
position on the ground plane (whose height is estimated as described in Sec.~\ref{sec:layout}), and 3D orientation is then evaluated.


\subsection{Relative Importance of 3D Cuboid Features}
We explore the relative importance of different features for the detection of 5 large objects
in Table~\ref{table:ablation}. We first trained our detector with geometric 
features only (Geom), with \cog only (COG), with both geometric and \cog features (Geom+\cog), 
adding the camera-view feature (Geom+COG+view), and finally utilizing the expanded 
cuboid feature (Geom+COG+view+expanded). 
The \cog feature and geometric features
have complementary advantages in 3D object detection, and combining
them leads to improved performance. 
The average accuracies of object detectors improve when additional features are added, 
demonstrating that each step of our feature design is effective.


\begin{table}[t]
\centering
\begin{tabular}{c|c|c|c|c|c} 
 &\includegraphics[width=.03\textwidth]{figs/icons/1.png}&\includegraphics[width=.03\textwidth]{figs/icons/15.png}&\includegraphics[width=.03\textwidth]{figs/icons/10.png}&\includegraphics[width=.03\textwidth]{figs/icons/4.png}&\includegraphics[width=.03\textwidth]{figs/icons/7.png} \\ \hline  
\scriptsize Geom&45.0&37.9&2.3&36.2&55.2 \\  
\scriptsize COG&52.5&42.8&6.7&22.6&49.6 \\
\scriptsize Geom+\cog     &53.0&49.8&12.8&39.0&63.6 \\ 
\scriptsize Geom+COG+view     &52.8&53.2&16.8&40.4&57.8 \\ 
\scriptsize  Geom+COG+view+expanded     &63.8&63.8&29.2&64.1&80.5  
\end{tabular}
  \caption{Average precision scores for five object categories (\emph{bed, bathtub, nightstand, chair, toilet}) given various sets of 3D cuboid features.}
\label{table:ablation}
\end{table}

