\section{Experiments}
We first describe the pretraining datasets including our WebVid-2M video-text dataset 
(Section~\ref{subsec:datasets1}), followed by the downstream datasets used for the evaluations
in our experiments (Section~\ref{subsec:datasets2}).
We then describe implementation details of our model (Section~\ref{subsec:implementation}).
Next, we ablate various training components on the MSR-VTT dataset,
in particular the effects of pretraining and our space-time attention modification (Section~\ref{subsec:ablation}),
and our proposed curriculum strategy (Section~\ref{subsec:curriculumexp}).
Then, we compare to the state of the art on
four benchmarks: MSR-VTT, MSVD, DiDeMo and LSMDC (Section~\ref{subsec:sota}).


\subsection{Pretraining Datasets}
\label{subsec:datasets1}
We jointly pretrain our model on image and video data.

\noindent\textbf{Video pretraining: The WebVid-2M Dataset.}
We scrape the web for a new dataset of videos with textual description annotations, called WebVid-2M. Our dataset consists of 2.5M video-text pairs, which is an order of magnitude larger than existing video captioning datasets (see Table~\ref{tab:datastats}).

The data was scraped from the web following a similar procedure to Google Conceptual Captions~\cite{sharma2018conceptual} (CC3M). We note that more than 10\% of CC3M images are in fact thumbnails from videos, which motivates us to use such video sources to scrape a total of 2.5M text-video pairs.
The use of data collected for this study is authorised via the Intellectual Property Officeâ€™s Exceptions to Copyright for Non-Commercial Research and Private Study\footnote{\url{www.gov.uk/guidance/exceptions-to-copyright/}}. We are currently performing further analysis of the dataset on its diversity and fairness.

Figure~\ref{fig:webvid2m} provides sample video-caption pairs. There are a variety of different styles used in caption 
creation, as can be seen from Figure~\ref{fig:webvid2m} (left to right) where the first video has a longer, poetic 
description compared to the succinct description for the second video. The third video caption has a less defined sentence 
structure, with keywords appended to the end, while the fourth video mentions a specific place (maldives). Time-specific information is important for the second and third example, where details such as ``talking on walkie-talkie'' or ``playing billiards'' would be missed when looking at certain frames independently.



\begin{table}[h!]
\centering
\caption{\textbf{Dataset Statistics:} We train on a new dataset mined from the web called WebVid2M. Our dataset is an order of magnitude larger than existing video-text datasets in the number of videos and captions. HowTo100M (highlighted in blue) is a video dataset with noisy, weakly linked text supervision from ASR.}
\footnotesize
\begin{tabular}{@{}llrrrr@{}}
\toprule
\multicolumn{1}{l}{\textbf{dataset}} & \multicolumn{1}{c}{\textbf{domain}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}\#clips\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}avg dur.\\ (secs)\end{tabular}}} & \multicolumn{1}{c}{\textbf{\#sent}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}} time\\ (hrs)\end{tabular}}} \\ \midrule


MPII Cook~\cite{rohrbach2012database} & cooking & 44 & 600 & 6K & 8\\ 

TACos~\cite{regneri2013grounding} & cooking & 7K & 360 & 18K & 15.9  \\ 


DideMo~\cite{anne2017localizing} & flickr & 27K & 28 & 41K & 87\\ 
MSR-VTT~\cite{xu2016msr} & youtube & 10K & 15 & 200K & 40 \\ 
Charades~\cite{sigurdsson2016hollywood} & home & 10K & 30 & 16K & 82 \\ 
LSMDC15~\cite{rohrbach2017movie} & movies & 118K & 4.8 & 118K & 158\\ 
YouCook II~\cite{zhou2018towards} & cooking & 14K & 316 & 14K & 176 \\ 
ActivityNet~\cite{krishna2017dense} & youtube & 100K & 180 & 100K & 849 \\ 
CMD~\cite{bain2020condensed} & movies & 34K & 132 & 34K & 1.3K \\
\textbf{WebVid-2M} & open  & \textbf{2.5M} & 18 & \textbf{2.5M} & \textbf{13K} \\ \rowcolor{aliceblue}
HT100M~\cite{miech2019howto100m} & instruction & 136M & 4 & 136M &  134.5K \\ 


\bottomrule            
\end{tabular}
\label{tab:datastats}
\end{table} 
We note that our video dataset is 10x smaller than HowTo100M in video duration and over 20x smaller in the number of paired clip-captions (Table \ref{tab:datastats}). Our dataset consists of manually generated captions, that are for the most part well formed sentences. In contrast, HowTo100M is generated from continuous narration with incomplete sentences that lack punctuation. The clip-text pairs are obtained from subtitles and may not be temporally aligned with the video they refer to, or indeed may not refer to the video at all~\cite{miech2019howto100m}. Our captions, on the other hand, are aligned with the video and describe visual content.


Moreover, there is no noise from imperfect ASR transcription and grammatical errors as is the case for HowTo100M. Our dataset also has longer captions on average (12 vs 4 words for HowTo) which are more diverse (Measure of Textual Lexical Diversity, MTLD~\cite{mccarthy2010mtld} = 203 vs 13.5).

\noindent\textbf{Image pretraining: Google Conceptual Captions~\cite{sharma2018conceptual}.}
This dataset consists of about 3.3M image and description pairs. Unlike the curated style of COCO images, Conceptual Captions (CC3M) images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions
are harvested from the Alt-text HTML attribute associated with web images.

\subsection{Downstream Datasets}
\label{subsec:datasets2}
We now describe the downstream text-video datasets that our model is evaluated on. \\
\noindent\textbf{MSR-VTT~\cite{xu2016msr}} contains
10K YouTube videos with 200K descriptions. Following other works~\cite{Liu19a}, we train on 9K train+val videos and report results on the 1K-A test set. \\
\noindent\textbf{MSVD~\cite{chen2011collecting}} consists of 80K English descriptions for 1,970 videos from YouTube, with each video containing 40 sentences each. We use the standard split of 1200, 100, and 670 videos for training, validation, and testing~\cite{patrick2020support,Liu19a}.

\noindent\textbf{DiDeMo~\cite{anne2017localizing}} contains 10K Flickr videos annotated with 40K sentences. Following \cite{lei2021less,Liu19a}, we evaluate paragraph-to-video retrieval, where all sentence descriptions for a video are concatenated into a single query. Since this dataset comes with localisation annotations (ground truth proposals), we report results with ground truth proposals (where only the localised moments in the video are concatenated and used in the retrieval set as done by~\cite{lei2021less}) as well as without (as done by~\cite{Liu19a}). 

\noindent\textbf{LSMDC~\cite{Rohrbach_2015_CVPR}} consists of 118,081 video clips sourced from 202 movies. The validation set contains 7,408 clips and evaluation is done on a test set of 1,000 videos from movies disjoint from the train and val sets. This follows the protocol outlined in~\cite{rohrbach2017movie}.

\noindent\textbf{Flickr30K~\cite{young-etal-2014-image}}. We also evaluate on a text-to-image retrieval benchmark to demonstrate the versatility of our model in that it can be used to achieve competitive performance in image settings as well as state-of-the art in video retrieval. The Flickr30K dataset contains 31,783 images with 5 captions per image. We follow the standard protocol of 1,000 images for validation, 1,000 images for testing and the remaining for training.

For downstream datasets with separate \texttt{val} and \texttt{test} splits, we train all models for 75 epochs and use the epoch with the lowest validation loss for reporting test results. For downstream datasets without a \texttt{val} set we report results at 50 epochs.

\subsection{Implementation Details}
\label{subsec:implementation}
All experiments are conducted with PyTorch~\cite{NEURIPS2019_9015}. Optimization is performed with Adam, using a learning rate of , we use batch sizes of 16, 24, and 96 for 8, 4, and 1-frame inputs respectively. The temperature hyperparameter  for the loss defined in Eq.~\ref{eq:loss1} \&~\ref{eq:loss2} is set to 0.05. The default pretraining is WebVid-2M and CC3M.

For the visual encoder, all models have the following:  attention blocks, patch size , sequence dimension , 12 heads and takes 4-frames as downstream input.

The text encoder of all models, unless specified otherwise, is instantiated as DistilBERT base-uncased~\cite{distilbert} pretrained on English Wikipedia and Toronto Book Corpus. The dimensionality of the common text-video space is set to 256. For visual augmentation, we randomly crop and horizontally flip during training, and center crop the maximal square crop at test time. All videos are resized to  as input. At test-time we compute clip-embeddings for the video with a stride of 2 seconds.
For paragraph-retrieval settings, we employ text augmentation during training by randomly sampling and concatenating a variable number of corresponding captions per video.

\noindent\textbf{Finetuning time.}
A large motivation for using pre-extracted expert models for video retrieval is to save computational cost. Finetuning our 4-frame model for 50 epochs on MSR-VTT takes 10 hours on 2 Quadro RTX 6000k GPUs (with 24GB RAM each), which is similar to other works using \textit{pre-extracted expert features}~\cite{patrick2020support}. This shows that our model is lightweight and can be finetuned end-to-end on the downstream video datasets quickly with sufficient pretraining (which is of one-time cost).
\subsection{Ablation Study}
\label{subsec:ablation}

\begin{table}\centering
\caption{\textbf{Pretraining sources:} The effect of different pretraining sources. We use 4 frames per video in both pretraining and finetuning. Pretraining is performed for 1 full epoch only. Results are presented on the 1K-A MSR-VTT test set for text-video retrieval. \textbf{R@k:} Recall@K. \textbf{MedR:} Median Rank}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lrrrrr}
\toprule
\textbf{Pre-training} & \textbf{\#pairs} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR}    \\
\midrule
-                   & -                   & 5.6     & 22.3     &  55  \\
ImageNet            &                     & 15.2    & 54.4     &  9.0     \\
HowTo-17M subset     & 17.1M                  & 24.1   & 63.9     & 5.0     \\
CC3M                & 3.0M                  & 24.5    & 62.7     & 5.0     \\
WebVid2M            & 2.5M                  & 26.0   & 64.9      & 5.0 \\
\textbf{CC3M + WebVid2M}    & 5.5M                   & \textbf{27.3}    & \textbf{68.1}         & \textbf{4.0} \\ \bottomrule
\end{tabular}
}
\label{tab:pretraining}
\end{table} 
In this section we study the effect of different pretraining strategies. In the Section~\ref{sec:arch_abl} of the Appendix, we provide architectural ablations on different temporal expansion methods, different visual backbones, different text backbones and the improvement when using our modified space-time attention block.

\noindent\textbf{Effect of pretraining.}
We compare performance on MSR-VTT with our model (i) trained from scratch, (ii) initialised with ImageNet weights and then 
finetuned, as well as (iii) initalised with ImageNet, and then pretrained on a number of different visual-text datasets before 
finetuning. For the video data, 4 frames are sampled at both pretraining and finetuning. Results on the MSR-VTT 1KA test set are shown in  Table~\ref{tab:pretraining}.
For HowTo100M, we pretrain on a random 17M subset due to computational constraints (the largest subset we could obtain at the time of writing) totalling 19K hours. To generate text-video pairs, we sample 5 contiguous speech-video pairs and concatenate them to form a longer video. This allows for robustness to the noisy alignment of speech and vision. We find that training on CC3M alone does reasonably well, outperforming the HowTo-17M subset. This demonstrates the benefit of our flexible encoder that can be cheaply trained on images and easily applied to videos. Training on WebVid2M also outperforms training on the HowTo17M subset, despite being much smaller, confirming that the HowTo100M dataset is noisy. The best performance is achieved by jointly training on both CC3M and WebVid2M, effectively exploiting image and video data. 

\subsection{Curriculum strategy}
\label{subsec:curriculumexp}
Next, we evaluate the ability of our curriculum schedule
to gradually learn the temporal dimension of videos
by increasing the input number of frames.
Table~\ref{tab:curric} summarises the results.
Here, we show performance when pretraining on WebVid2M and
finetuning on MSR-VTT.
We explore two types of expansion in time:
at pretraining and at finetuning stages.
First, we observe that a single frame is not sufficient
to capture the video content (18.8 R@1).
Performing the temporal expansion at pretraining stage
is better than doing so at finetuning (26.0 vs 24.9 R@1 with 4 frames).
Finally, we obtain similar performance (slightly better at R@5) at half the computational
cost in GPU hours by employing a curriculum strategy at pretraining (26.6 R@1).
For 8 frames, the curriculum is even more useful, as we start training on 1 frame and then move to 4 before finally moving to 8 frames. Here, we obtain similar or better performance than training on 8 frames from the start, with almost a third of the computational cost. This is to be expected, as fewer frames significantly reduces forward pass times and enables larger batch sizes.
Note that for a fair comparison, we allow the same number of training
iterations for each row in the table.
\begin{table}
\centering
\caption{\textbf{Effect of \#frames and curriculum learning:} The effect of a different number of input frames at pretraining and finetuning.  indicates a within-dataset curriculum learning strategy. Results are presented on the 1K-A MSR-VTT test set for text-video retrieval. Pretraining here is done on WebVid2M only, with a total budget of one epoch through the entire dataset. \textbf{PTT:} total pretraining time in hours.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}ccrrrr@{}}
\toprule
\textbf{PT \#frames}          & \textbf{FT \#frames} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR} & \textbf{PTT (hrs)} \\ \toprule
1                             & 1                    & 18.8         & 56.6          & 7.0 & 16.2          \\ \midrule
1                             & 4                    & 24.9         & 67.1          & 5.0 & 16.2           \\
4                             & 4                    & 26.0         & 64.9          & 5.0 & 45.6        \\
14               & 4                    & 26.6         & 65.5          & 5.0 & 22.1             \\ \midrule
8                             & 8                    & 25.4         & 67.3          & 4.0 & 98.0           \\
148 & 8                    & 27.4         & 67.3          & 4.0 & 36.0         \\ \bottomrule
\end{tabular}
}
\label{tab:curric}
\end{table}
 We further analyse our proposed temporal curriculum strategy and its effects on training time and accuracy. Figure~\ref{fig:curric} shows the zero-shot results on MSR-VTT for various checkpoints with and without curriculum. It shows that our curriculum method yields a significant training speedup with a gain in accuracy. Shorter frame models are able to pass through more of the dataset in a shorter amount of time, which can lead to significant performance benefits in a constrained setting.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/curric_plot.pdf}
    \caption{Plot showing the zero-shot performance (geometric mean of R@{1,5,10}) of various models on the MSR-VTT test set against their total training time in hours.  denotes a curriculum learning strategy.  denotes the multiple of dataset epochs completed.}
    \label{fig:curric}
\end{figure}

\input{tables/MSRVTT_sota}

\noindent\textbf{Expansion of temporal embeddings.}
We experiment with both zero padding and interpolation, and find that our model is robust to the type of temporal expansion strategy. More detailed results are provided in the Appendix, Section~\ref{sec:temp_expansion}.


\subsection{Comparison to the State of the Art}
\label{subsec:sota}
Results on MSR-VTT can be seen in Table~\ref{tab:msr-vtt-sota}.
We outperform all previous works, including many that pretrain on HowTo100M which is an order of magnitude larger than our pretraining dataset both in the number of hours (135K vs 13K) and in the number of caption-clip pairs (136M vs 5.5M). We also note that we outperform works that extract expert features (CE uses 9 experts, MMT uses 7) including object, motion, face, scene, sound and speech embeddings. We even outperform/perform on par with Support Set~\cite{patrick2020support}, which uses expert features from a 34-layer, R(2+1)-D model pretrained on IG65M, concatenated with ImageNet ResNet152 features, after which they add a transformer network and train end-to-end on HowTo100M. 

We also report zero-shot results (Table~\ref{tab:msr-vtt-sota}) with no finetuning on MSR-VTT, outperforming both MIL-NCE and Support Set that trains on HowTo100M. This shows that our model is more generalisable, and can be used out of the box, and also perhaps that the domain of WebVid-2M is closer to that of MSR-VTT than HowTo100M. We will release the weights of our models publicly.

For both the zero-shot and finetuned setting we show that the addition of the COCO Captions image dataset further boosts our state-of-the-art MSR-VTT performance, indicating that the model is not yet saturated and additional pretraining dataset will lead to even better downstream performance.

For MSVD~\cite{chen2011collecting}, we outperform all previous methods (Table ~\ref{tab:msvd-sota}). In particular, we outperform Support Set~\cite{patrick2020support} even though they train on an order of magnitude more data.
\begin{table}
\centering
\caption{Text-to-video retrieval results on the MSVD~\cite{chen2011collecting} test set.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method}  & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} \\ \midrule
VSE~\cite{kiros2014unifying}             & 12.3          & 30.1          & 42.3          & 14.0          \\
VSE++~\cite{faghri2017vse++}              & 15.4          & 39.6          & 53.0          & 9.0           \\
Multi. Cues~\cite{mithun2018learning}            & 20.3          & 47.8          & 61.1          & 6.0           \\
CE~\cite{Liu19a}                   & 19.8          & 49.0          & 63.8          & 6.0           \\
Support Set~\cite{patrick2020support}           & 23.0          & 52.8          & 65.8          & 5.0           \\
Support Set~\cite{patrick2020support} (HowTo PT)   & 28.4          & 60.0          & 72.9          & 4.0           \\
\textbf{Ours}    & \textbf{33.7} & \textbf{64.7} & \textbf{76.3} & \textbf{3.0}  \\ \bottomrule
\end{tabular}
}
\label{tab:msvd-sota}
\end{table} 

Results on DiDeMo can be found in Table~\ref{tab:didemo-sota}. Note that on this dataset, our zero-shot performance is equivalent to CLIPBERT's results with finetuning, and after we finetune our model on the DiDeMo training set we get an additional 14.2\% boost in R@1.
\begin{table}\centering
\caption{Text-to-video retrieval results on the DiDeMo test set. We show results with and without ground truth proposals (GT prop.) as well as with finetuning and without (zero-shot).}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}llrrrr@{}}
\toprule
\textbf{Method} & \textbf{GT prop.} & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} \\ \midrule
S2VT~\cite{venugopalan2014translating}            &          & 11.9          & 33.6          & -             & 13.0          \\   
FSE~\cite{zhang2018cross}             &          & 13.9          & 36.0          & -             & 11.0          \\
CE~\cite{Liu19a}&                    & 16.1          & 41.1          & -             & 8.3           \\
ClipBERT~\cite{lei2021less}        & \checkmark         & 20.4          & 44.5          & 56.7          & 7.0           \\
\textbf{Ours}   & \textbf{}          & \textbf{31.0} & \textbf{59.8} & \textbf{72.4} & \textbf{3.0}    \\
\textbf{Ours}   & \checkmark          & \textbf{34.6} & \textbf{65.0} & \textbf{74.7} & \textbf{3.0}  \\  
\midrule 
\textbf{Zero-shot} \\
\midrule 

\textbf{Ours}   &          & 21.1 & 46.0 & 56.2 & 7.0 \\
\textbf{Ours}   & \checkmark          & 20.2 & 46.4 & 58.5 &  7.0

\\\bottomrule
\end{tabular}
}
\vspace{-1em}
\label{tab:didemo-sota}
\end{table} 
We demonstrate further state-of-the-art results on LSMDC text-to-video retrieval.  We outperform all previous methods, except for MMT in Median Rank, which pretrains on HowTo100M, a dataset consisting of over 100M clip-text pairs and contains multiple experts as well as audio modalities. Our model uses visual information alone.
\begin{table}
\centering
\caption{Text-to-video retrieval results on the LSMDC test set.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method}  & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} \\ \midrule
JSFusion~\cite{yu2018joint}     & 9.1          & 21.2          & 34.1          & 36.0  \\
MEE~\cite{miech18learning}      & 9.3          & 25.1          & 33.4          & 27.0  \\
CE~\cite{Liu19a}                & 11.2          & 26.9          & 34.8          & 25.3  \\
MMT (HowTo100M)~\cite{gabeur2020multi}                & 12.9          & 29.9          & 40.1          & \textbf{19.3}  \\
\textbf{Ours}                   & \textbf{15.0} & \textbf{30.8} & \textbf{40.3} & 20.0  \\ \bottomrule
\end{tabular}
}
\label{tab:lsmdc-sota}
\end{table} \label{subsec:qualitative}



To demonstrate the effectiveness of our model for downstream video and image tasks, we additionally report results on Flickr30K the image retrieval dataset in Table~\ref{tab:flickr}. Unlike other works~\cite{lee2018stacked,chen2020imram,diao2021similarity} which utilise high resolution regions extracted using a Faster-RCNN detector, our model is single stage and does not require any object detections. We compare to works with a similar number of training image-text pairs, and find that our model is comparable. We also note that training on WebVid2M provides a sizeable boost (5\% improvement in R@1). Note that there are other recent text-image works such as UNITER~\cite{chen2020uniter} and OSCAR~\cite{li2020oscar}, however these are trained on almost twice the number of samples. Recent works scale this up even further to billions of samples (ALIGN~\cite{jia2021scaling}). 

\begin{table}\centering
\caption{Text-to-\textbf{image} retrieval results on the Flickr30K test set. ++ indicates additional datasets: COCO Captions, SBU Captions. VisGenObjects denotes Visual Genome object bounding box annotations used to pretrain an FRCNN object feature extractor.} 
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method} & \multicolumn{1}{l}{\textbf{Vis PT. size}} & \multicolumn{1}{l}{\textbf{R@1}} & \multicolumn{1}{l}{\textbf{R@5}} & \multicolumn{1}{l}{\textbf{R@10}} \\ \midrule
SCANM~\cite{lee2018stacked}           & VisGenObj (3.8M)                             & 48.6                             & 77.7                             & 85.2                              \\
IMRAM~\cite{chen2020imram}           & VisGenObj (3.8M)                             & 53.9                             & 79.4                             & 87.2                              \\
SGRAF~\cite{diao2021similarity}           & VisGenObj (3.8M)                             & 58.5                             & 83.0                             & 88.8                              \\
Ours            & CC (3.0M)                                 &               54.2   &                             83.2     &        89.8                           \\
Ours            & CC,WV-2M (5.5M)                      &                61.0           &                      87.5 &             92.7                      \\ \bottomrule
\end{tabular}
}
\label{tab:flickr}
\end{table}

%
 