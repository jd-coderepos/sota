
\begin{abstract}
  A critical variable of a satisfiable CNF formula is a variable that has the
  same value in all satisfying assignments.  Using a simple case
  distinction on the fraction of critical variables of a CNF formula, we
  improve the running time for
  3-SAT from  by Rolf~\cite{rolf2006}
  to .
  Using a different approach, Iwama et al.~\cite{istt10} very recently achieved a running time of .
  Our method nicely combines with theirs, yielding the currently fastest known algorithm with running time
  .
  We also improve the
  bound for 4-SAT from
  ~\cite{it04} to
  , where
   can be obtained using the
  methods of~\cite{it04} and~\cite{rolf2006}.
\end{abstract}

\section{Introduction}

The ideas behind the most successful algorithms for -SAT are
surprisingly simple.  In 1999, Paturi, Pudl\'ak, and Zane~\cite{ppz}
proposed the following algorithm.  Given a -CNF formula , we
choose a variable  uniformly at random from the  variables in
, choose a truth value , and set  to , thereby
replacing  by , and continue with .  The value  is chosen as follows: If the formula contains
the unit clause , we choose . If it contains , we
choose .  In these two cases, we say  was {\em forced}. If it
contains neither, we choose  randomly and say  was {\em
  guessed}.  Finally, if the formula contains both  and
, we can give up, since the formula is unsatisfiable. This
algorithm is usually called  after its three inventors.

Intuitively, if  is ``strongly constrained'', then the algorithm
encounters many unit clauses, hence it needs to guess significantly
fewer than  variables. On the other hand, if   is only
``weakly constrained'', it has multiple satisfying assignments,
making it easier to find one.
Paturi, Pudl\'ak and Zane~\cite{ppz} make this intuition precise and
show that  finds a satisfying assignment for a -CNF formula with probability at
least , provided there exists one.

A couple of years later, Paturi, Pudl\'ak, Saks, and Zane~\cite{ppsz}
came up with a simple but powerful idea. In a preprocessing step, they
apply a restricted version of resolution. This increases the number of
unit clauses the algorithm encounters and therefore increases its
success probability.
This gives an algorithm called .  If  has a unique
satisfying assignment, its success probability is quite good (for
-SAT, it is ), and the analysis is highly
elegant. The case of multiple satisfying assignments appears to be
much more difficult and has been the subject of several papers so far.
Iwama and Tamaki~\cite{it04} made a major step forward when they
observed that while the success probability of  deteriorates as
the number of satisfying assignments increases, that of Sch\"oning's
random walk algorithm~\cite{schoning1999} improves.  They quantified
this tradeoff and obtained an algorithm with a success probability of
\footnote{Using the new version of~\cite{ppsz} immediately gives the bound , as stated in~\cite{rolf2006}.}. We denote this combined algorithm, consisting of one run
of  and one run of Sch\"oning's random walk algorithm, by .\paragraphprev


\paragraphdef{The PPSZ paper.}
There are two versions of~\cite{ppsz}, which we call the old version
and the new version. For unique -SAT, both are the same, but for general
-SAT, the old version of~\cite{ppsz} gives a more complicated
analysis. The old version gives a better bound for 3-SAT and the new version gives a better bound for 4-SAT\@.

Only the new version is published, but the old version is still
available at the Citeseer cache\footnote{\htmladdnormallink{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1134}{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1134}}.
However, we have found some minor errors in that version.  There is also a
conference version~\cite{ppszconference} stating the results of the old
version of~\cite{ppsz}, but without most proofs. 
Rolf~\cite{rolf2006} improved the analysis of the old version to get a bound of . However~\cite{rolf2006} does not consider 4-SAT\@. We use the ideas of~\cite{rolf2006} for our improvement
of 4-SAT\@.
In Timon Hertli's
master thesis~\cite{thesis}, the old version of~\cite{ppsz} with the
result of~\cite{rolf2006} is presented in a self-contained way. We will reference that thesis for detailed proofs.


\subsection{Our Contribution}

Let  be a satisfiable CNF formula over  variables and  be a variable therein.
We call  {\em critical} if all satisfying assignments of  agree
on . Equivalently,  is critical if exactly one of the formulas
 and  is satisfiable. We denote by  the fraction of
critical variables, i.e., the number of critical variables divided by
; if , we define .

Our contribution consists of two statements: Theorem~\ref{theorem-critical} shows that for our purposes we only need to consider formulas with many critical variables. Point \ref{lemma-item-3} of Lemma~\ref{lemma-forced-critical} then implies that the success
probability of  increases if  has many critical variables. This is obtained by slightly modifying the existing analysis of~\cite{ppsz} and~\cite{rolf2006} by taking critical variables into account. However, Lemma~\ref{lemma-forced-critical} is somewhat technical and we need to embed it into a review of the existing analysis.
Theorem \ref{theorem-critical} is very simple, so we state it here:
\begin{theorem}
\label{theorem-critical}
Let  and  such that . Suppose algorithm  runs in time  and for every satisfiable -CNF formula  with  finds a satisfying assignment with probability at least . Then there exists an algrotihm  that runs in time  and for every satisfiable -CNF formula finds a satisfying assignment with probability at least .
\end{theorem}
Obviously we can turn  into a algorithm that finds a satisfying assignment in expected time .
\begin{proof}
By \emph{guessing}  variables we mean fixing in   variables chosen uniformly at random to values chosen uniformly at random, obtaining the formula  over at most  variables.  for each  repeats the following  times: Guess  variables and then run  on ; the running time bound is trivial. To bound the probability, we first claim that there exists a  such that  where  is the probability that after guessing  variables  is satisfiable and . Suppose this is not the case: Let  be the probability that after guessing  variables  is satisfiable and . Clearly  since  is satisfiable, and , as guessing one variable preserves satisfiability with probability at least . By the assumption, ; from this it is easy to show that . If , we have  by definition; hence  and , a contradiction. Now let  be the  given by the claim; we repeat  times an algorithm that has success probability at least ; as  this gives by a routine argument an algorithm with success probability at least .
\end{proof}

We improve the analysis for  for formulas with many critical
variables. In combination with Theorem~\ref{theorem-critical}, this
gives a success probability of 
for -SAT and  for -SAT\@.
Very recently, Iwama, Seto, Takai, and Tamaki~\cite{istt10} showed
how to combine an improved version of Sch\"oning's
algorithm~\cite{hofmeister02,bs03} with  and achieved expected running time of . We combine our improvement with
theirs to obtain a bound of .
\introductionAppendixExplanation

We analyze the algorithm , where  is a CNF formula.
 consists essentially of a call to ~\cite{ppsz} and to
~\cite{schoning1999}. In~\cite{it04} it was shown that
 has a better success probability than what the analysis of  and
 gives. Let  be the algorithm of~\cite{istt10} that improves .
\begin{theorem}
\label{p.t.3sat}
\sloppypar{
There exists an algorithm that for every satisfiable -CNF formula finds a satisfying assignment
with probability

and runs in subexponential time.
}
\end{theorem}
\begin{theorem}
\label{guess-istt}
\sloppypar{
There exists an algorithm that for every satisfiable -CNF formula finds a satisfying assignment
with expected running time
.
}
\end{theorem}
\theoremAppendixExplanation
\begin{theorem}
\label{guess-istt-weak}
\sloppypar{
There exists an algorithm that for every satisfiable -CNF formula finds a satisfying assignment
with expected running time
.
}\end{theorem}

\begin{theorem}
\label{p.t.4sat}
\sloppypar{
There exists an algorithm that for every satisfiable -CNF formula finds a satisfying assignment
with probability

and runs in subexponential time.
}
\end{theorem}
\sloppypar{
This is already very close to unique -SAT, which has a success probability of
. }
The benefit of Theorem~\ref{theorem-critical} is that when proving
Theorems~\ref{p.t.3sat} and~\ref{p.t.4sat}, we only need to consider
formulas with many critical variables.  For example, to prove
Theorem~\ref{p.t.3sat}, we choose  such that , i.e., . Then we have to bound from
below the success probability of  for -CNF formulas  with
.
\subsection{Notation}
We use the notational framework introduced in \cite{welzl05}. We
assume an infinite supply of propositional \emph{variables}. A
\emph{literal}  is a variable  or a complemented variable . A finite set  of literals over pairwise distinct variables is
called a \emph{clause} and a finite set of clauses is a formula
in \emph{CNF} (Conjunctive Normal Form). We say that a variable 
\emph{occurs} in a clause  if either  or  are contained
in it and that  occurs in the formula  if there is any clause
where it occurs. 
We write  or  to denote
the set of variables that occur in  or in , respectively. 
A clause containing exactly one literal is called a \emph{unit clause}. We
say that  is a 
\emph{-CNF} formula
if every clause has
size at most .  Let such an  be given and write
 and .

A \emph{assignment} is a function  which assigns a Boolean value to each variable. A literal
 (or ) is \emph{satisfied by}  if 
(or ). A clause is \emph{satisfied by}  if it
contains a satisfied literal and a formula is \emph{satisfied by}
 if all of its clauses are. A formula is \emph{satisfiable} if
there exists a satisfying truth assignment to its variables.

For an assignment  on  and a set , we denote
by  the assignment that corresponds to  on
variables of  and is flipped on variables of .

Given a CNF formula , we denote by  the set of assignments that
satisfy .

Formulas can be manipulated by permanently assigning values to
variables. If  is a given CNF formula and 
then assigning  satisfies all clauses containing 
(irrespective of what values the other variables in those closes are
possibly assigned later) whilst it truncates all clauses containing
 to their remaining literals. 

We will write  (and analogously ) to
denote the formula arising from doing just this.

We say that two clauses  and  conflict on a variable  if
one of them contains  and the other . We call  and
 a resolvable pair if they conflict in exactly one variable ,
and we define their \emph{resolvent} by . It is easy to see that if  contains a
resolvable pair , , then . A
resolvable pair ,  is -bounded if ,
, and .

By , we denote the set of clauses  that have an
-bounded resolution deduction from . By a straightforward
algorithm, we can compute  in time
~\cite{ppsz}.

By choosing an element u.a.r.\ from a finite set, we mean choosing it
uniformly at random. By choosing an element u.a.r. from an closed real
interval, we mean choosing it according to the continuous uniform
distribution over this interval. Unless otherwise stated, all random
choices are mutually independent.

We denote by  the logarithm to the base 2. For the logarithm to
the base , we write . We define .

\section{Proof of the Main Theorems}

\begin{algorithm}[t]
\caption{CNF formula , assignment , permutation }
\begin{algorithmic}
\STATE Let  be a 
partial assignment over , initially the empty assignment.
\STATE 
\FOR {all , according to }
\IF {}
\STATE 
\ELSIF {}
\STATE 
\ELSE 
\STATE 
\ENDIF
\STATE 
\ENDFOR
\RETURN 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\caption{CNF formula }
\begin{algorithmic}
  \STATE \COMMENT{this algorithm is used for 4-SAT} \STATE Choose
   u.a.r.\ from all assignments on  \STATE Choose
   u.a.r.\ from all permutations of  \RETURN
  
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\caption{CNF formula  assignment }
\begin{algorithmic}
\FOR { steps}
\IF { satisfies }
\RETURN 
\ENDIF
\STATE Select an arbitrary  not satisfied by 
\STATE Select a variable  u.a.r.\ from  and flip  in 
\ENDFOR
\RETURN 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\caption{CNF formula }
\begin{algorithmic}
\STATE
\COMMENT{this algorithm is used for 3-SAT}
\STATE Choose  u.a.r.\ from all assignments on 
\STATE 
\IF {}
\STATE 
\ENDIF
\RETURN 
\end{algorithmic}
\end{algorithm}



In the following let  be a fixed integer. Let  be a
satisfiable -CNF formula,  and . 
We first give the concepts from~\cite{ppsz} needed to understand Lemma~\ref{lemma-forced-critical}. Then we state the lemma and
use it to improve
the bounds on the success probability of  and  given sufficiently many critical variables. In Section~\ref{section-prooflemma8}, we prove Lemma~\ref{lemma-forced-critical} and also consider 4-SAT. Most concepts used in the proof are from~\cite{ppsz,rolf2006}. Our contribution is to exploit what these concepts yield for critical variables.\paragraphprev

\paragraphdef{Subcubes.}
 For  and , the
set  is called a \emph{subcube}. The variables in 
are called \emph{defining} variables and those in 
\emph{nondefining} variables. The subcube  has dimension
.  For example, if ,  and , then  contains
exactly the two assignments  and . Given a nonempty
set , there is a partition

where the  are pairwise disjoint subcubes, and  for all . See~\cite{ppsz} for a proof.  For
the rest of the paper, we fix such a partition for  being the set
of satisfying assignments. To estimate the success probability of
, consider the assignment  that  chooses
uniformly at random from .

Hence instead of analyzing  for an
assignment  sampled uniformly at random from all assignments, we fix  arbitrarily and
we think of  as being sampled from the subcube .  Let
 be the set of non-defining variables of this cube, and
 the set of defining variables.  Intuitively, if
 has small dimension, then  is likely to be close to
, thus  has a better success probability:
\begin{lemma}[\cite{it04}]
  .
  \label{lemma-schoening-IT}
\end{lemma}

\paragraphdef{Placements.}
As a next step, we analyze
 with  chosen uniformly at random from
 and the permutation also chosen from some subset of
permutations. A {\em placement} of the variables  is a function
, and a {\em uniform random placement} is defined
by chosing  uniformly at random from  independently
for each .  With probability , a uniform random placement is
injective and gives rise to a uniformly distributed permutation via
the natural ordering  on . For the rest of the paper, we
will view  as a placement rather than a permutation. Let 
be a measurable set of placements.  Then

The benefit of this is that we can tailor  towards our needs,
i.e., making the conditional probability  fairly large. This may come at
the cost of making  small.\\

\paragraphdef{Forced variables.}
Suppose the permutation  orders the
variables  as . Let  be a satisfying
assignment of . Imagine we call . The
algorithm applies bounded resolution to , obtaining  and sets the variables 
step by step to their respective values under , creating a
sequence of formulas by , where  for . Since
 is a satisfying assignment,  is the empty formula. We
say  is {\em forced} with respect to  and  if
 contains the unit clause  or . By
 we denote the set of variables  that are
forced with respect to  and . If  is not forced, we
say it is {\em guessed}. We denote by  the set
of guessed variables. Note that  returns 
if and only if  for all .  Furthermore, since  is chosen uniformly
at random from , we already have  for
all . Therefore

where the inequality comes from Jensen's inequality applied to the
convex function . Note that (\ref{ineq-success})
holds when taking  uniformly at random as well as when sampling
it from some set . Using linearity of expectation, we see that

Now if  is the unique satisfying assignment, then . For -SAT, one central result of~\cite{ppsz} is that
\begin{lemma}[\cite{ppsz}]
  Let  be a satisfiable -CNF formula with a unique satisfying
  assignment . Then for every , it holds that
  .
\label{lemma-unique-forced}
\end{lemma}

Combining the lemma with (\ref{ineq-success}) shows that  on
-CNF formulas with a unique satisfying assignment has a success
probability of at least . For the case of multiple satisfying assignments,
the lemma does not hold anymore.\paragraphprev

\paragraphdef{Critical variables.}
Let  be a satisfiable CNF formula and
 a variable. Recall that we call  {\em critical} if all satisfying assignments
of  agree on . The following observation is not difficult
to show:
\begin{observation}
  Let  be a satisfiable CNF formula and let  be the set of
  critical variables. Let  be the subcube as defined
  above. For a satisfying assignment , let  be the
  set of nondefining variables. 
  Then .
\end{observation}
\begin{lemma}
  Let  be a satisfiable -CNF formula and  be a
  satisfying assignment. There is a measurable set  of placements such that for  and
  , we have
  \begin{enumerate}
  \item ,
  \item  for all ,
  \item  for all critical .
      \label{lemma-item-3}
    \end{enumerate}
    \label{lemma-forced-critical}
\end{lemma}
The important part of the lemma is point \ref{lemma-item-3}, namely
that critical variables are forced with a larger probability than
non-critical ones.
\begin{proof}[Proof of Theorem~\ref{p.t.3sat}]
Using Theorem~\ref{theorem-critical}, we can assume .
Let  be the fraction of defining
  variables.
  Combining (\ref{sum-of-unforced}) with Lemma~\ref{lemma-forced-critical},
  we obtain
  
  The expected fraction of nondefining variables we have to guess is
  thus a little bit larger than in the case of a unique satisfying
  assignment, where it is . Together with
  (\ref{ineq-success}), we conclude that the success probability
  of  is at least
  
  Our bound on the success probability of  thus deteriorates with
  the number of defining variables. A bigger subcube  is
  better for . We combine this with the bound for Sch\"oning's
  algorithm from Iwama and Tamaki~\cite{it04}, stated 
  above in Lemma~\ref{lemma-schoening-IT}
  
  \sloppypar{
  The combined worst case is with , in which case both (\ref{eq-ppsz-prob}) and (\ref{eq-schoening-prob}) evaluate to . 
   Therefore for any , at least one of 
  and  has a success probability of .
  }
  \end{proof}
\begin{proof}[Proof of Theorem \ref{guess-istt-weak}]
Lemma 6 from~\cite{istt10} tells us that there is an algorithm  that improves  such that for all  we have, after preprocessing time ,

We want to prove that by replacing  with  in , we obtain expected running time of
  . Setting  and  gives  and . With this choice of , we have the following bound for  (obtained as in the previous proof, but with a different constant ):

The combined worst case is at  where
 and
, proving that the combined success probability is  (after preprocessing time ).
\end{proof}

\section{Proof of Lemma~\ref{lemma-forced-critical}}
\label{section-prooflemma8}
\subsection{Critical Clause Trees}
 Let
. Note that  and
.
A \emph{critical clause} for  w.r.t.\   is a clause where  satisfies exactly one literal and this literal is over . It can be easily seen that if the output of  should be , then exactly the critical clauses of  are the clauses that might turn into unit clauses. Note that the \emph{defining} variables are assumed to be set correctly, so we only need to consider critical clauses for \emph{nondefining} variables here.

We now define critical clause trees, a concept that tells us which critical clauses we can expect in a CNF formula after bounded resolution. Let  be a rooted tree in which every node is either labeled with a variable from  or is unlabeled. A \emph{cut} in a rooted tree is a set of nodes  such that the root is not in  and every path from the root to a leaf contains at least one node in . The \emph{depth} of a node is the distance to the root.  
For a set  of nodes,  denotes the set of variables occurring as labels in .
We say  is a \emph{critical clause tree} for  w.r.t.\  and  if the following properties hold:

\begin{enumerate}
\item The root is labeled by .
\item On any path from the root to a leaf, no two nodes have the same label.
\item For any cut  of the tree, there is a critical clause  w.r.t.\  where the satisfied literal is over  and every unsatisfied literal is over some variable in .
\end{enumerate}

\begin{wrapfigure}{r}{4cm}
\includegraphics{treeexample.pdf}
\caption{Example Critical Clause Tree}
\label{f.treeexample}
\end{wrapfigure}

It is shown in~\cite{ppsz} that we can construct a critical clause tree for  as follows: Start with the root labeled . Now we can repeatedly extend a leaf node . Let  be the set of labels that occur on the path from  to the root. If  does not satisfy , then we can extend the tree at that node: There is a clause  in  (not in ) not satisfied by . For each literal in  that is not satisfied by , we add a child to  labeled with the variable of that literal. If there are no such literals, we add an unlabeled node. As clauses of  have at most  literals, each node has at most  children. If the constructed tree has at most  nodes (as we do -bounded resolution), then it is a critical clause tree for  w.r.t.\  and .

We give a simple example: Let  For the all-one assignment and , we can get the tree shown in Figure \ref{f.treeexample} by the described procedure.  is a cut in this tree. We have ,  and , giving the required critical clause.

If  is the only satisfying assignment of , 
never satisfies , and we can build a tree where all leafs are at
depth . We call this a {\em full tree}. The
important observation is now that this also works if  is a
\emph{critical variable}, as in that case  also never
satisfies , as .

In the general case, however, the assignment  might satisfy  so that we cannot extend the tree. However if  consists only of \emph{nondefining} variables, then we know that  does not satisfy . Hence we can get a tree where every leaf not at depth  is labeled by a \emph{defining} variable. We define the trees  we will use in the analysis:
\begin{definition}
\label{p.d.trees}
For , construct the critical clause tree for  as
follows: If  is a critical variable, then construct  such that
all leaves are at depth , i.e., construct a full tree. Otherwise,
construct  such that all leaves not labeled by defining variables
are at depth .
\end{definition}

This means that a tree might just consist of a root where all children are labeled with defining variables, which essentially nullifies the benefits from resolution. To cope with this, we have to make defining variables more likely to occur at the beginning. We achieve this by choosing the set  of placements whose existence we claim in Lemma \ref{lemma-forced-critical} in a way such that exactly that happens. 

\begin{definition}
  A function  is called a \emph{nice distribution
    function} if  is non-decreasing, uniformly continuous,
  , ,  is differentiable except for finitely many
  points and .
\end{definition}
Compared with~\cite{ppsz}, we added the requirement . This will
mean that defining variables cannot be less likely to occur at the
beginning than nondefining variables. We now define a random placement where defining
variables are placed with distribution function :
\begin{definition}
  Let  be a nice distribution function. By , we define the
  random placement on  s.t.\  for  is u.a.r.\
  , and for  and , .
  \label{nice-random-placement}
\end{definition}

Assume that the variables are processed according to some placement
. Consider . If there is a cut  such that  for every , 
then  is forced, as the
corresponding critical clause has turned into a unit clause for .
Denote the probability that  is a cut in  by
.

For , let  be the smallest non-negative  that
satisfies  and . It was
shown in~\cite{ppsz} that if  is a full tree, then

 can be understood as follows: Take an infinite -ary tree and mark each node as ``dead'' with probability , except the root.  is the probability that this tree contains an infinite path that starts at the root and contains only ``alive'' nodes.

We have  and . For , we have  and for , we have . 
As , and by definition of  and of a cut, it is obvious that

 if  is a full tree.  If 
is not a full tree, we do not have any good bounds on .
In~\cite{rolf2006} it is shown that if  is not necessarily a full
tree, but a tree in which every leaf not at depth  is labeled by a
defining variable, then

where

Obviously , which means that the bound
(\ref{bound-full}) for full trees is at least as strong as the bound
(\ref{bound-not-full}) for general trees. The  term
corresponds to the tree that consists of a root where all
children are labeled with defining variables and are thus leaves
(remember that there are at most  children).  It takes a small
lemma to show that this tree and the full tree are the worst cases.
See~\cite{thesis} for details.  The following observation summarizes
this:
\begin{observation}
\label{p.o.trees}
If  is a \emph{critical} variable, then  If  is a noncritical \emph{nondefining} variable, then

\end{observation}
We want to find a set  of placements such that a placement
chosen uniformly at random from  behaves more or less like
.
\begin{lemma}[old version of \cite{ppsz}]
\label{p.l.Hlemma}
Let  be a nice distribution function. If ,
there is a set of placements  depending on  with the
following properties: Let  be the placement choosen
uniformly at random from . Then for any tree  with at most
 nodes we have

and

with

where  is the derivative of .
\end{lemma}
The proof of this lemma is long and complicated, see \refHtheory{}
in~\cite{thesis}. The case  is easy to handle:
The probability that all defining variables come at the beginning is substantial, and we are essentially in the
(good) unique case.  

Below we will show how to choose a good function  for the case
 and . To get an intuition, see Figure~\ref{f.hplot} for a
plot of  for . With this function, one obtains  and .
Together with Lemma~\ref{p.l.Hlemma} and Observation~\ref{p.o.trees},
we conclude that for a {\em critical} variable 

and for a non-critical non-defining variable 


\subsection{Choosing a good }

Let now . We choose  as in~\cite{rolf2006}: Let  be a parameter. With some appropriate parameters  and , we define  as follows:

\paragraphdef{3-SAT.}
\begin{wrapfigure}{r}{6cm}
\includegraphics[width=5.5cm]{hplot.pdf}
\caption{ for 3-SAT}
\label{f.hplot}
\end{wrapfigure}
To determine  and , we set the constraints 
 
(as , this right-hand side is equal to ) and 



If these constraints are satisfied,  is a nice distribution function that is differentiable on . Figure \ref{f.hplot} gives a plot of the  we use. Numerical optimization gives  and as before . See \refHevaluation{} in~\cite{thesis} for details of the computation.
This gives




This concludes the proof of Lemma~\ref{lemma-forced-critical}.\\

\paragraphdef{4-SAT.}
For 4-SAT, we use the  corresponding to the new version of~\cite{ppsz}. For some parameter , we let .
It turns out that the optimum is when . In that case it is easily seen that the bound for \PPSZ{} does not depend on , and hence we do not need \Schoening{}.
Numerical optimization gives  and . This implies the success probability , proving Theorem \ref{p.t.4sat}.

\section{Conclusion}
We have shown how to improve \PPSZ{} by a preprocessing step that
guarantees that a substantial fraction of variables will be critical. With this,
we were able to improve the bound for 3-SAT and 4-SAT
from~\cite{rolf2006}. We have also shown that our approach nicely
combines with the improvement by~\cite{istt10} by giving an even
better bound. In 4-SAT, we are
already very close to the unique case. We do not know if a more
refined choice of  (similar to~\cite{rolf2006}), possibly depending
on , allows us to close that gap.

It is interesting to see that we could make use of multiple assignments in the guessing step before considering just one assignment using the subcube partition.
\section*{Acknowledgments}
We thank Emo Welzl for many fruitful discussions and continous support and Konstantin Kutzkov for pointing us to~\cite{istt10}.
