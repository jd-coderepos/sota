In this section we analyze a simple algorithm for the densest subgraph problem. This algorithm simply samples  edges uniformly at random (without replacement) and solves the problem on the sampled subgraph, where factors of ,  and  are hidden in the  notation. 
Interestingly, we show that, with probability , this algorithm gives a -approximate solution using  bits of space.

In addition, this algorithm can be implemented with a running time of , using oracle access to the edge set, with the ability to sample an edge uniformly at random. This works by combining our sampling guarantee with the algorithm of
Charikar \cite{charikar2000greedy}. 

To the best of our knowledge, our -approximation algorithm is the fastest constant approximation algorithm for the densest subgraph problem. This algorithm can be implemented in the streaming setting with insertions and deletions to the edges (the \emph{strict turnstile} or \emph{dynamic graph} stream model), as well as the RAM model with oracle access to random edge samples (we discuss the latter model below
in the context of improving the running time of the best offline algorithms).

Before stating our main lemma first we need the following generalized version of the Chernoff inequality, that holds for negatively correlated random variables. We say Boolean random variables  are negatively correlated if for any arbitrary subset  of , and any arbitrary  we have  \cite{esfandiari2014online}.

\begin{lemma}[\cite{panconesi1997randomized}]\label{lm:chernoff}
Let  be a sequence of negatively correlated boolean (i.e.,  or ) random variables, and let . We have 

\end{lemma}

To use the above generalized version of the Chernoff bound, we need to show that our random variables are negatively correlated. The following lemma becomes very useful to show the random variables are negatively correlated.

\begin{lemma}\label{lm:negative}
Let  be a sequence of Boolean random variables, such that, exactly  of them are chosen to be  uniformly at random. Then the random variables  are negatively correlated. 
\end{lemma}
\begin{proof}
Let  be an arbitrary subset of  and let  be an arbitrary element of . On the one hand, the probability that  is . On the other hand, conditioned that for any element , we have ; the probability that  is . Clearly, , which gives us

This means the random variables  are negatively correlated. 
\end{proof}

\begin{algorithm*}\textbf{Input:} A graph .

\textbf{Output:} A -approximation of the densest subgraph of , w.pr. .

\begin{algorithmic}[1]
    \STATE Set 
    \IF{}
    	\STATE Find a densest subgraph of  using the algorithm of \cite{charikar2000greedy} (or any other algorithm).
    \ELSE
    	\STATE Sample  edges uniformly at random, without replacement from .
    	\STATE Let  be the sampled graph.
    	\STATE Find a densest subgraph of .
    \ENDIF
\end{algorithmic}

\caption{Finding Densest Subgraph}
\label{alg:DensestSubGraph}
\end{algorithm*}

\begin{lemma}\label{lm:kBoundDS}
Let  be a graph with vertex set  and edge set . Let  be the sampled graph in Algorithm \ref{alg:DensestSubGraph} and let .
We have for any , 

where  is the density of the subgraph of  induced by the vertices of .
\end{lemma}

\begin{proof}
	Let  be the random variable that indicates whether  exists in  or not, and
	let  be an arbitrary subset of  of size . By definition, the number of edges in  is . Let us denote this summation by . Then, we have
	
	where the first and the last equalities are by definition of density, and the third equality is by definition of .
	Lemma \ref{lm:negative} says that random variables  are negatively correlated. Thus, to bound , we can apply the following form of the Chernoff bound from Lemma \ref{lm:chernoff}.
	
	By setting  we have

On the other hand we have
	
Therefore, we have
	
If we set  to be the vertex set of , Inequality \ref{eq:Ubound} says that 
	
	which immediately gives us
	
	
	On the other hand, Inequality \ref{eq:Ubound} says that for each selection of , with probability , we can upper bound   by 
	. 
	As we have  such choices, applying a union bound we have

	If we set  to  we have
	
Therefore, by combining Inequalities \ref{eq:kbound1} and \ref{eq:kbound2} and applying the union bound we have
	
\end{proof}




In fact, the density of the densest subgraph of a graph  is at least as much as the density of  itself. Hence, one can lower bound  by the density of  which is . The following states this fact.

\begin{fact} \label{fc:boundOpt}
The density of the densest subgraph of  is at least , where  is the number of vertices in  and  is the number of edges.
\end{fact}

The following theorem bounds the approximation ratio of Algorithm \ref{alg:DensestSubGraph}. 

\begin{theorem}\label{thm:mainDS}
With probability , Algorithm \ref{alg:DensestSubGraph} is a -approximation algorithm for the densest subgraph problem.
\end{theorem}

\begin{proof}
	Recall that Lemma \ref{lm:kBoundDS} states for an arbitrary fixed , with probability at least  we have 
	. Using a union bound this holds for all , with probability . Thus, for some value of , with probability  we have , which means that Algorithm \ref{alg:DensestSubGraph} returns a -approximation. Now, if we set  to , or equivalently set  in Algorithm \ref{alg:DensestSubGraph} to , we have
	
\end{proof}



One can use -samplers \cite{jst11} 
to sample the  edges in Algorithm \ref{alg:DensestSubGraph} in a dynamic graph stream. However, maintaining these  samplers may need an update time as large as .
In Lemma \ref{lm:updatetime} we show how to sample  edges with  update time using the notion of min-wise independent hash functions.
\begin{definition}
Given , 
we say a hash function  is {\bf -approximately -min-wise independent} on a subset  of  
if for any  such that  we have

\end{definition}

\begin{theorem}\label{lm:min-wise}(Theorem 1.1 of \cite{feigenblat2011exponential})
There exist constants  such that for any  of size at most
, any -wise independent
family of functions  is -approximately -min-wise independent on . 

In addition, one can evaluate  on  items simultaneously
in total time  by using fast multipoint polynomial
evaluation (see, e.g., Theorem 13 of \cite{knpw11}, where this idea was used for a different
streaming problem). Further, one can spread the evaluation
of  on  items evenly across the next  stream updates, converting this amortized
 update time to a worst-case update time. 
\end{theorem}


We need the following generalized version of the Chernoff bound.

\begin{lemma}[\cite{schmidt1995chernoff}]\label{lm:Kchernoff}
Let  be the sum of -wise independent Boolean random variables. For any  such that , we have

\end{lemma}

\begin{lemma}\label{lm:updatetime}
For any number  of edge samples and constant , we can sample  edges in a dynamic stream (a stream with insertions and deletions to the edges), such that the statistical distance of our sampled edges to a uniformly random (without replacement) sample of  edges is . This sampling algorithm uses  space and has update time .
\end{lemma}
\begin{proof}
We apply Theorem \ref{lm:min-wise} with the  of that theorem equal to , where  is set to . We label each possible edge of our graph with a number in  and extend the domain and range of  to . Then, we apply the  of Theorem \ref{lm:min-wise} to the specific subset of  edges in our input graph. We apply Theorem \ref{lm:min-wise}
with . Theorem \ref{lm:min-wise} and the definition of min-wise independence imply that the statistical distance of the subset  of  minimum hash values of our edges under  is within  from  uniformly random samples without replacement. Indeed, the probability of choosing any set  is  rather than  had we had full independence, so summing the absolute values of these differences gives -distance at most , and so statistical distance at most  between the distributions. 
Note that Theorem \ref{lm:min-wise} implies
that  is also -wise independent (in the standard sense, not the min-wise
sense), and so  is -wise independent using our assumptions that  and
. 

Now we show how 
to maintain the  edges with the smallest hash values under  in a dynamic stream. 
We use  sparse recovery data structures,  each to recover  edges. Note that  is an upper bound on the total number of distinct edges in our graph. Upon the update (insertion or deletion) of an edge , we update each sparse recovery structure  such that . For the sparse recovery structure, we use the data structure of \cite{glps10}, which has  update time, space  and succeeds with probability  in returning all of the non-zero items
in a vector for which it is applied to, provided this number of non-zero items is at most . 

Let  be the number of distinct edges which hash under  to the -th sparse recovery
data structure . Then if , there always exists one of the  such that  (if , we can store the entire graph in  bits of space). Moreover,  for any given  is fairly concentrated around its expectation, since Theorem \ref{lm:min-wise} implies that  is also -wise independent, so we can bound its deviation using the generalized version of the Chernoff bound given by Lemma \ref{lm:Kchernoff}. Consider any  for which . Then we have 

Using that , the error probability is . We also run an -estimation algorithm, to estimate
each  up to a multiplicative factor of , with total space  and update time  \cite{cdim03}, 
and with failure probability . It follows
from the above that for an  for which , we have that  with
probability . It follows that our -estimate for this value of  will be in  with
probability . Hence, from , with probability  we will recover all values that hash to ,
and as argued above these are within statistical distance  from uniform.
\end{proof}

Combining Lemma \ref{lm:updatetime} with Theorem \ref{thm:mainDS} proves Theorem \ref{thm:mainStreamDens}.


To the best of our knowledge, the fastest known -approximation algorithm for the densest subgraph problem has a running time of . However, here we need to find the densest subgraph of a sampled graph with at most  edges. Thus, the running time of our algorithm is . To the best of our knowledge, this is the fastest -approximation algorithm for the densest subgraph problem.
Theorem \ref{thm:mainRAMDens} states this fact.
