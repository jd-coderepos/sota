In this section we analyze a simple algorithm for the densest subgraph problem. This algorithm simply samples $\tilde{O}(n)$ edges uniformly at random (without replacement) and solves the problem on the sampled subgraph, where factors of $1/\epsilon$, $\delta$ and $\log(n)$ are hidden in the $\tilde{O}$ notation. 
Interestingly, we show that, with probability $1-m^{-\delta}$, this algorithm gives a $(1-\epsilon)$-approximate solution using $\tilde{O}(n)$ bits of space.

In addition, this algorithm can be implemented with a running time of $\tilde{O}(n)$, using oracle access to the edge set, with the ability to sample an edge uniformly at random. This works by combining our sampling guarantee with the algorithm of
Charikar \cite{charikar2000greedy}. 

To the best of our knowledge, our $(0.5-\epsilon)$-approximation algorithm is the fastest constant approximation algorithm for the densest subgraph problem. This algorithm can be implemented in the streaming setting with insertions and deletions to the edges (the \emph{strict turnstile} or \emph{dynamic graph} stream model), as well as the RAM model with oracle access to random edge samples (we discuss the latter model below
in the context of improving the running time of the best offline algorithms).

Before stating our main lemma first we need the following generalized version of the Chernoff inequality, that holds for negatively correlated random variables. We say Boolean random variables $x_1,x_2,\dots,x_r$ are negatively correlated if for any arbitrary subset $S$ of $\{x_1,x_2,\dots,x_r\}$, and any arbitrary $a\in S$ we have $\Pr(a=1| \forall_{b\in S-{a}} b=1) \leq \Pr(a=1)$ \cite{esfandiari2014online}.

\begin{lemma}[\cite{panconesi1997randomized}]\label{lm:chernoff}
Let $x_1,x_2,\dots,x_r$ be a sequence of negatively correlated boolean (i.e., $0$ or $1$) random variables, and let $X=\sum_{i=1}^{r}x_i$. We have 
\begin{align*}
\Pr\left(|X-\E[X]| \geq \epsilon \E[X] \right) \leq 3 \exp(-\epsilon^2\E[X]/3).
\end{align*}
\end{lemma}

To use the above generalized version of the Chernoff bound, we need to show that our random variables are negatively correlated. The following lemma becomes very useful to show the random variables are negatively correlated.

\begin{lemma}\label{lm:negative}
Let $x_1,x_2,\dots,x_r$ be a sequence of Boolean random variables, such that, exactly $t$ of them are chosen to be $1$ uniformly at random. Then the random variables $x_1,x_2,\dots,x_r$ are negatively correlated. 
\end{lemma}
\begin{proof}
Let $S$ be an arbitrary subset of $\{x_1,x_2,\dots, x_r\}$ and let $a$ be an arbitrary element of $S$. On the one hand, the probability that $a=1$ is $\frac t r$. On the other hand, conditioned that for any element $b\in S \setminus \{a\}$, we have $b=1$; the probability that $a=1$ is $\frac {t-(|S|-1)}{r-(|S|-1)}$. Clearly, $\frac t r \geq \frac {t-(|S|-1)}{r-(|S|-1)}$, which gives us
$\Pr(a=1| \forall_{b\in S-{a}} b=1) \leq \Pr(a=1).$
This means the random variables $x_1,x_2,\dots,x_r$ are negatively correlated. 
\end{proof}

\begin{algorithm*}\textbf{Input:} A graph $G=(E_G,V)$.

\textbf{Output:} A $(1-\epsilon)$-approximation of the densest subgraph of $G$, w.pr. $1-m^{-\delta}$.

\begin{algorithmic}[1]
    \STATE Set $C=\frac {12n (4+\delta) \log(m)} { \epsilon^2}$
    \IF{$|E|\leq C$}
    	\STATE Find a densest subgraph of $G$ using the algorithm of \cite{charikar2000greedy} (or any other algorithm).
    \ELSE
    	\STATE Sample $C$ edges uniformly at random, without replacement from $G$.
    	\STATE Let $H$ be the sampled graph.
    	\STATE Find a densest subgraph of $H$.
    \ENDIF
\end{algorithmic}

\caption{Finding Densest Subgraph}
\label{alg:DensestSubGraph}
\end{algorithm*}

\begin{lemma}\label{lm:kBoundDS}
Let $G=(V,E_G)$ be a graph with vertex set $V$ and edge set $E_G$. Let $H=(V,E_H)$ be the sampled graph in Algorithm \ref{alg:DensestSubGraph} and let $p=\frac C {|E_G|}$.
We have for any $k$, 
\begin{align*}
\Pr\left( opt(G,k) - den_G(opt(H,k)) \geq \epsilon   opt(G)\right) \leq 6 \exp(k\cdot \log(m) -\frac{p k \epsilon^2  opt(G)} {12} ),
\end{align*}
where $den_G(opt(H,k))$ is the density of the subgraph of $G$ induced by the vertices of $opt(H,k)$.
\end{lemma}

\begin{proof}
	Let $x_e$ be the random variable that indicates whether $e$ exists in $E_H$ or not, and
	let $U$ be an arbitrary subset of $V$ of size $k$. By definition, the number of edges in $H[U]$ is $\sum_{e \in H[U]} 1 = \sum_{e \in G[U]} x_e$. Let us denote this summation by $X_U$. Then, we have
	\begin{align}\label{eq:expectedDS}
		\E[den(H[U])] &= \E \left[ \frac {\sum_{e \in G[U]} x_e}{|U|} \right ] 
		= \frac {\E[\sum_{e \in G[U]} x_e]}{|U|}  
		=  \frac {\sum_{e \in G[U]} p}{|U|}  
		= p \frac {\sum_{e \in G[U]} 1}{|U|} 
		= p\cdot den(G[U])
	\end{align}
	where the first and the last equalities are by definition of density, and the third equality is by definition of $p$.
	Lemma \ref{lm:negative} says that random variables $x_1,x_2,\dots,x_{|E_G|}$ are negatively correlated. Thus, to bound $X_U$, we can apply the following form of the Chernoff bound from Lemma \ref{lm:chernoff}.
	\begin{align*}
		\Pr\left(|X-\E[X]| \geq \epsilon' \E[X] \right) 
		\leq 3 \exp(-\epsilon'^2\E[X]/3)
	\end{align*}
	By setting $\epsilon'=p k \epsilon \frac{ opt(G)} {2 \E[X]}$ we have
\begin{align*}
		\Pr\left(|X-\E[X]| \geq \frac {p k \epsilon  opt(G)}{2} \right) 
		&\leq 3 \exp(-\frac{p^2 k^2 \epsilon^2  opt(G)^2} {4\E[X]^2} \cdot \frac{\E[X]}{3}) \\
		&\leq 3 \exp(-\frac{p^2 k^2 \epsilon^2  opt(G)^2} {12\E[X]} ) \\
		&\leq 3 \exp(-\frac{p k \epsilon^2  opt(G)} {12} ) &\text{Using Equality \ref{eq:expectedDS}}\\
	\end{align*}
On the other hand we have
	\begin{align*}
		\Pr\left(|X-\E[X]| \geq \frac{p k \epsilon  opt(G)}{2} \right) &=
		\Pr\left(\frac 1 p \frac {|X-\E[X]|}{k} \geq   \frac{\epsilon}{2}  opt(G) \right)\\ 
		&= \Pr\left(|\frac 1 p \frac {X}{k} -  \frac {\frac 1 p \E[X]}{k}| \geq   \frac{\epsilon}{2}  opt(G) \right) \\
		&= \Pr\left(|\frac 1 p den(H[U]) -  den(G[U])| \geq   \frac{\epsilon}{2}  opt(G) \right) &\text{Using Equality \ref{eq:expectedDS}}\\
	\end{align*}
Therefore, we have
	\begin{align} \label{eq:Ubound}
		\Pr\left(|\frac 1 p den(H[U]) -  den(G[U])| \geq   \frac{\epsilon}{2}  opt(G) \right) \leq 3 \exp(-\frac{p k \epsilon^2  opt(G)} {12} ).
	\end{align}
If we set $U$ to be the vertex set of $Opt(G,k)$, Inequality \ref{eq:Ubound} says that 
	\begin{align*}
		\Pr\left(  opt(G,k) - \frac 1 p den_H(opt(G,k))  \geq \frac{\epsilon}{2}   opt(G)\right) \leq 3 \exp(-\frac{p k \epsilon^2  opt(G)} {12} )
	\end{align*}
	which immediately gives us
	
	\begin{align}\label{eq:kbound1}
		\Pr\left(  opt(G,k) - \frac 1 p opt(H,k)  \geq \frac{\epsilon}{2}   opt(G)\right) \leq 3 \exp(-\frac{p k \epsilon^2  opt(G)} {12} ).
	\end{align}
	On the other hand, Inequality \ref{eq:Ubound} says that for each selection of $U$, with probability $1-3 \exp(-\frac{p k \epsilon^2  opt(G)} {12})$, we can upper bound $\frac 1 p den(H[U])$  by 
	$den(G[U]) + \frac{\epsilon}{2} opt(G)$. 
	As we have ${m \choose k}$ such choices, applying a union bound we have
\begin{align*}
		\Pr\left(\forall_{U:|U|=k}  \frac 1 p den(H[U]) - den(G[U])  \geq \frac{\epsilon}{2}   opt(G)\right) \leq 3{m \choose k} \exp(-\frac{p k \epsilon^2  opt(G)} {12} ).
	\end{align*}
	If we set $U$ to $opt(H,k)$ we have
	\begin{align}\label{eq:kbound2}
		\Pr\left(\frac 1 p opt(H,k) - den_G(opt(H,k))  \geq \frac{\epsilon}{2}   opt(G)\right) \leq 3{m \choose k} \exp(-\frac{p k \epsilon^2  opt(G)} {12} ).
	\end{align}
Therefore, by combining Inequalities \ref{eq:kbound1} and \ref{eq:kbound2} and applying the union bound we have
	\begin{align*}
		\Pr\left(   opt(G,k)- den_G(opt(H,k)) \geq \epsilon   opt(G)\right) &\leq 3({m \choose k}+1) \exp(-\frac{p k \epsilon^2  opt(G)} {12} )\\
		& \leq 3({m^k}+1) \exp(-\frac{p k \epsilon^2  opt(G)} {12} )\\
		& \leq 6 \exp(k\cdot \log(m) -\frac{p k \epsilon^2  opt(G)} {12} ).
	\end{align*}
\end{proof}




In fact, the density of the densest subgraph of a graph $G$ is at least as much as the density of $G$ itself. Hence, one can lower bound $opt(G)$ by the density of $G$ which is $\frac m n$. The following states this fact.

\begin{fact} \label{fc:boundOpt}
The density of the densest subgraph of $G$ is at least $\frac m n$, where $n$ is the number of vertices in $G$ and $m$ is the number of edges.
\end{fact}

The following theorem bounds the approximation ratio of Algorithm \ref{alg:DensestSubGraph}. 

\begin{theorem}\label{thm:mainDS}
With probability $1-m^{-\delta}$, Algorithm \ref{alg:DensestSubGraph} is a $(1-\epsilon)$-approximation algorithm for the densest subgraph problem.
\end{theorem}

\begin{proof}
	Recall that Lemma \ref{lm:kBoundDS} states for an arbitrary fixed $k$, with probability at least $1-6 \exp(k\cdot \log(m) -\frac{p k \epsilon^2  opt(G)} {12} )$ we have 
	$opt(G,k) - den_G(opt(H,k)) \leq \epsilon   opt(G)$. Using a union bound this holds for all $1\leq k\leq n$, with probability $1- \sum_{k=1}^n 6 \exp(k\cdot \log(m) -\frac{p k \epsilon^2  opt(G)} {12} )$. Thus, for some value of $k$, with probability $1-n \cdot 6 \exp(k\cdot \log(m) -\frac{p k \epsilon^2  opt(G)} {12} )$ we have $opt(G) - den_G(opt(H)) \leq \epsilon   opt(G)$, which means that Algorithm \ref{alg:DensestSubGraph} returns a $(1-\epsilon)$-approximation. Now, if we set $p$ to $\frac {12n (4+\delta) \log(m)} { \epsilon^2  m}$, or equivalently set $C$ in Algorithm \ref{alg:DensestSubGraph} to $\frac {12n (4+\delta) \log(m)} { \epsilon^2}$, we have
	\begin{align*}
		&1-n\times 6 \exp(k\cdot \log(m) -\frac{p k \epsilon^2  opt(G)} {12} ) \\
		&= 1-n\times 6 \exp(k\cdot \log(m) -\frac {12n (4+\delta) \log(m)} { \epsilon^2  m} \times\frac{ k \epsilon^2  opt(G)} {12} ) \\
		& \geq 1-n\times 6 \exp(k\cdot \log(m) -\frac {12n (4+\delta) \log(m)} { \epsilon^2  m} \times \frac{ k \epsilon^2  m} {12n} ) & \text{From Fact \ref{fc:boundOpt}}\\
		& = 1-n\times 6 \exp(k\cdot \log(m) -(4+\delta)k\cdot \log(m))\\
		& = 1-n\times 6 \exp(-(3 + \delta) k\cdot \log(m))\\
		& \geq 1- \exp(\log(n)+2 -(3 + \delta) k\cdot \log(m)) &\text{Since $e^2\geq 6$}\\
		& \geq 1- \exp(-\delta\cdot k\cdot \log(m))\\
		& \geq 1- \exp(-\delta\cdot \log(m))\\
		&=1- m^{-\delta}.
	\end{align*}
\end{proof}



One can use $L_0$-samplers \cite{jst11} 
to sample the $C$ edges in Algorithm \ref{alg:DensestSubGraph} in a dynamic graph stream. However, maintaining these $L_0$ samplers may need an update time as large as $C$.
In Lemma \ref{lm:updatetime} we show how to sample $C$ edges with $\tilde{O}(1)$ update time using the notion of min-wise independent hash functions.
\begin{definition}
Given $\epsilon > 0$, 
we say a hash function $h:[1,n]\rightarrow [1,n]$ is {\bf $\epsilon$-approximately $t$-min-wise independent} on a subset $X$ of $\{1, 2, \ldots, n\}$ 
if for any $Y\subseteq X$ such that $|Y|=t$ we have
\begin{align*}
Pr(\max_{y \in Y} h(y) < \min_{z\in X-Y} h(z)) = \frac1 {{|X| \choose |Y|}} (1 \pm \epsilon).
\end{align*}
\end{definition}

\begin{theorem}\label{lm:min-wise}(Theorem 1.1 of \cite{feigenblat2011exponential})
There exist constants $c', c'' > 1$ such that for any $X \subseteq \{1, 2, \ldots, n\}$ of size at most
$\epsilon n / c'$, any $c'' (t \log \log (1/\epsilon) + \log(1/\epsilon))$-wise independent
family of functions $h$ is $\epsilon$-approximately $t$-min-wise independent on $X$. 

In addition, one can evaluate $h$ on $t$ items simultaneously
in total time $t \cdot (\log(t/\epsilon))^{O(1)}$ by using fast multipoint polynomial
evaluation (see, e.g., Theorem 13 of \cite{knpw11}, where this idea was used for a different
streaming problem). Further, one can spread the evaluation
of $h$ on $t$ items evenly across the next $t$ stream updates, converting this amortized
$(\log(t/\epsilon))^{O(1)}$ update time to a worst-case update time. 
\end{theorem}


We need the following generalized version of the Chernoff bound.

\begin{lemma}[\cite{schmidt1995chernoff}]\label{lm:Kchernoff}
Let $X$ be the sum of $t$-wise independent Boolean random variables. For any $\epsilon\geq 1$ such that $t \geq \lfloor \epsilon^2\E[X]e^{-1/3} \rfloor$, we have
\begin{align*}
Pr(|X-\E[X]|>\epsilon \E[X]) < \exp(-\lfloor \epsilon^2 \E[X]/3 \rfloor).
\end{align*}
\end{lemma}

\begin{lemma}\label{lm:updatetime}
For any number $C \geq n$ of edge samples and constant $1\leq \delta$, we can sample $C$ edges in a dynamic stream (a stream with insertions and deletions to the edges), such that the statistical distance of our sampled edges to a uniformly random (without replacement) sample of $C$ edges is $O(e^{-\delta})$. This sampling algorithm uses $\tilde{O}(C)$ space and has update time $\tilde{O}(1)$.
\end{lemma}
\begin{proof}
We apply Theorem \ref{lm:min-wise} with the $n$ of that theorem equal to ${n \choose 2} c' / \epsilon$, where $\epsilon$ is set to $e^{-\delta}$. We label each possible edge of our graph with a number in $\{1, 2, \ldots, {n \choose 2}\}$ and extend the domain and range of $h$ to $\{1, 2, \ldots, n^2 c' / \epsilon\}$. Then, we apply the $X$ of Theorem \ref{lm:min-wise} to the specific subset of $m \leq {n \choose 2}$ edges in our input graph. We apply Theorem \ref{lm:min-wise}
with $t = \Theta(C \delta)$. Theorem \ref{lm:min-wise} and the definition of min-wise independence imply that the statistical distance of the subset $Y$ of $t$ minimum hash values of our edges under $h$ is within $e^{-\delta}$ from $t$ uniformly random samples without replacement. Indeed, the probability of choosing any set $Y$ is $\frac{1}{{|X| \choose |Y|}} (1 \pm \epsilon)$ rather than $\frac{1}{{|X| \choose |Y|}}$ had we had full independence, so summing the absolute values of these differences gives $\ell_1$-distance at most $\epsilon = e^{-\delta}$, and so statistical distance at most $e^{-\delta}/2$ between the distributions. 
Note that Theorem \ref{lm:min-wise} implies
that $h$ is also $\Theta(C \delta)$-wise independent (in the standard sense, not the min-wise
sense), and so $h$ is $\Omega(n)$-wise independent using our assumptions that $C \geq n$ and
$\delta \geq 1$. 

Now we show how 
to maintain the $C$ edges with the smallest hash values under $h$ in a dynamic stream. 
We use $\log(n^2)$ sparse recovery data structures, $sp_1,sp_2,\dots sp_{2\log(n)}$ each to recover $9\delta C$ edges. Note that $n^2$ is an upper bound on the total number of distinct edges in our graph. Upon the update (insertion or deletion) of an edge $e$, we update each sparse recovery structure $s_i$ such that $i \in [1, \lfloor \log_2(h(e))\rfloor]$. For the sparse recovery structure, we use the data structure of \cite{glps10}, which has $\tilde{O}(1)$ update time, space $\tilde{O}(C)$ and succeeds with probability $1-1/n^2$ in returning all of the non-zero items
in a vector for which it is applied to, provided this number of non-zero items is at most $9\delta C$. 

Let $Z_i$ be the number of distinct edges which hash under $h$ to the $i$-th sparse recovery
data structure $sp_i$. Then if $m \geq 4\delta C$, there always exists one of the $sp_i$ such that $4\delta C \leq \E[Z_i]\leq 8\delta C$ (if $m < 4\delta C$, we can store the entire graph in $\tilde{O}(C)$ bits of space). Moreover, $Z_i$ for any given $i$ is fairly concentrated around its expectation, since Theorem \ref{lm:min-wise} implies that $h$ is also $\Theta(C \delta)$-wise independent, so we can bound its deviation using the generalized version of the Chernoff bound given by Lemma \ref{lm:Kchernoff}. Consider any $Z_i$ for which $4\delta C \leq \E[Z_i] \leq 8\delta C$. Then we have 
\begin{align*}
\Pr(Z_i< 3\delta C \text{ or }Z_i > 9\delta C) &\leq \Pr(|Z_i-\E[Z_i]|>\delta C)  
\leq \Pr(|Z_i-\E[Z_i]|> \frac 1 8 \E[x])\\
& \leq \exp(-\left \lfloor (\frac 1 8)^2 \frac {\E[Z_i]} 3 \right \rfloor) \leq  \exp(-\Theta(C)) 
\end{align*}
Using that $C \geq n$, the error probability is $\exp(-n)$. We also run an $\ell_0$-estimation algorithm, to estimate
each $Z_i$ up to a multiplicative factor of $1.1$, with total space $\tilde{O}(1)$ and update time $\tilde{O}(1)$ \cite{cdim03}, 
and with failure probability $1/n^{\Omega(1)}$. It follows
from the above that for an $i$ for which $4\delta C \leq \E[Z_i] \leq 8\delta C$, we have that $Z_i \in [3\delta C, 9 \delta C]$ with
probability $1-\exp(-n)$. It follows that our $\ell_0$-estimate for this value of $Z_i$ will be in $[C, 10\delta C]$ with
probability $1-1/n^{\Omega(1)}$. Hence, from $sp_i$, with probability $1-1/n^{\Omega(1)}$ we will recover all values that hash to $sp_i$,
and as argued above these are within statistical distance $e^{-\delta}$ from uniform.
\end{proof}

Combining Lemma \ref{lm:updatetime} with Theorem \ref{thm:mainDS} proves Theorem \ref{thm:mainStreamDens}.


To the best of our knowledge, the fastest known $0.5$-approximation algorithm for the densest subgraph problem has a running time of $O(m+n)$. However, here we need to find the densest subgraph of a sampled graph with at most $C=\tilde{O}(n)$ edges. Thus, the running time of our algorithm is $\tilde{O}(n)$. To the best of our knowledge, this is the fastest $(0.5-\epsilon)$-approximation algorithm for the densest subgraph problem.
Theorem \ref{thm:mainRAMDens} states this fact.
