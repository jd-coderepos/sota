\section{Experiments}
We introduce the experimental settings in Section \ref{sec:setup}. To obtain relevant findings on how to build an effective consistency training framework for FST, we first empirically study the effects of multiple data perturbation methods in Section \ref{sec:dp} and prove the effectiveness of consistency training via comparisons with the base model. Then, we validate our consistency training model with different data filtering methods in Section \ref{sec:df} and demonstrate their additional effects on the SSL framework. Based on the findings in these two experiments, we further compare our best models with previous state-of-the-art models in Section \ref{sec:main}. We also include case studies in Section \ref{sec:main} to present some qualitative examples. Finally, we conduct low-resource experiments (Section \ref{sec:low}) to demonstrate our method's advantage when less parallel data are available. 


\subsection{Experimental Settings}
\label{sec:setup}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     Dataset    & Train & Val & Test & Unlabeled \\
     \hline
    E\&M      &  52595& 2877& 1416 & 200k\\
    \hline
    F\&R  &51967 &2788& 1432 & 200k\\
    \hline
    \end{tabular}
    \caption{The statistics of datasets.}
    \label{tab:dataset}
\end{table}

\begin{table*}[t]
    \centering
 \small
\begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{E\&M}} & \multicolumn{3}{c}{\textbf{F\&R}} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
     Model variants    & BLEU & Acc(\%)& HM & BLEU & Acc(\%) & HM \\
     \midrule
      base model  & 76.87  & 90.04 & 82.94 & 80.32 & 84.01 & 82.12 \\
      no-perturbation & 76.41 &	88.49 & 	82.01 & 79.22 &	84.46 &	81.75 \\	
      \midrule
      drop & 77.55 & 93.15 & 84.64& 80.53 &	86.56 &	83.44 \\
      swap & 77.90 &	93.43&	84.96& 81.07 &	85.96 & 83.44 \\
      mask & 77.52&	93.93&	84.94 & 80.69 &	86.41 &	83.45\\
      synonym &77.48 & 93.64 & 84.80 & 80.49 &	86.26 &	83.28 \\
      \midrule
      back-translation & 76.07 &	90.11&	82.50& 79.96 &	84.91 &	82.36 \\
      tf-idf & 76.89 & 92.58 &84.01 & 80.48 &	\textbf{86.94} &	83.58 \\
      \midrule
      abbr & 77.55 &	93.64 &	84.84 & 81.00 &	\textbf{86.94} &	\textbf{83.86} \\
      capital &77.54 &	93.15 &	84.63 & 80.74 &	85.74 &	83.16\\
      spell & \textbf{78.37} &\textbf{94.21}&	\textbf{85.56}& \textbf{81.09} &	85.59 &	83.28  \\
        \bottomrule
    \end{tabular}
\caption{Effects of different data perturbations in our approach on the test splits of GYAFC. The best scores among all the model variants are boldfaced.}
    \label{tab:da-res}
\end{table*}

\paragraph{Datasets} We evaluate our framework on the GYAFC~\cite{rao2018dear} benchmark for formality style transfer. It comprises crowdsourced informal-formal sentence pairs split into two domains, namely, E\&M and F\&R. The informal sentences in the dataset were originally selected from the same domains in Yahoo Answers L6 corpus\footnote{https://webscope.sandbox.yahoo.com/catalog.php
?datatype=l}. We focus on the \textit{informal-formal} style transfer because it is more realistic in applications.
We further collected massive amounts of informal sentences from each of the two domains in Yahoo Answers L6 corpus as the unsupervised data. The statistics of the datasets are presented in Table \ref{tab:dataset}. 





\begin{table*}[t]
    \centering
    \small
\begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{E\&M}} & \multicolumn{3}{c}{\textbf{F\&R}} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
     Model variants    & BLEU & Acc(\%)& HM & BLEU & Acc(\%) & HM \\
    
      \midrule
      spell (no-filter)& 78.37 & 94.21 &85.56 & 81.09 &	85.59 &	83.28  \\
      spell (+style) & 78.19 &	93.79 &	85.28 &\textbf{81.37} &	\textbf{86.41} &	\textbf{83.81 }\\
      spell (+bleu) & \textbf{78.75} &	\textbf{94.56} &	\textbf{85.94} &\textbf{81.11 }&	\textbf{86.34} &	\textbf{83.64 }\\
      spell (+lm) & 78.24 &	\textbf{94.56 }&	\textbf{85.63} & 80.93 &	\textbf{86.34} &	\textbf{83.55}\\
        \bottomrule
    \end{tabular}
\caption{Effects of different data filtering methods in our approach on the test splits of GYAFC. Scores larger than the no-filter variant are in \textbf{bold}.}
    \label{tab:filter-res}
\end{table*}
\paragraph{Implementation Details} We employ PyTorch \citep{pytorch} for all the experiments.  We pretrain a TextCNN style classifier on the supervised data for each domain of GYAFC, following the setting in \cite{lai-etal-2021-thank}.
The same classifier is adopted for both the style accuracy evaluation and the style strength filter in our SSL framework. We adopt HuggingFace Transformers \citep{transformers} library's implementation of pretrained T5-Large \citep{2020t5} as the base model. We adopt the Adam \citep{adam} optimizer with the initial learning rate $2\times 10^{-5}$ to train all the models.  More details of hyper-parameters and model configurations are provided in Appendix \ref{sec:des}.





\paragraph{Evaluation Metrics}
The main evaluation metric for FST is the BLEU score between the generated sentence and four human references in the test set. We adopt the corpus BLEU in NLTK~\cite{loper-bird-2002-nltk} following \citep{chawla-yang-2020-semi}. In addition, we also pretrained a TextCNN formality classifier to predict the formality of transferred sentences and calculate the accuracy (Acc.). Furthermore, we compute the harmonic mean of BLEU and style accuracy as an overall score, following the settings in \cite{lai-etal-2021-thank}. 













\subsection{Effects of Data Perturbation Methods}
\label{sec:dp}
In this experiment, we validate the effectiveness of our consistency training framework and compare the effects of different data perturbation methods. Specifically, we adopt the nine data perturbation methods introduced in Section \ref{sec:da} and include the \textit{no-perturbation} variant that indicates directly using an unlabeled sentence and its pseudo target to train the unsupervised loss. We adopted no data filtering strategy in this experiment to simplify the comparison. 

As shown in Table \ref{tab:da-res}, our framework could consistently improve the base model by using different perturbation methods; however, back-translation resulted in mostly lower results than the base model. This contradicts the conclusion in \cite{xie2020unsupervised} that back-translation is especially powerful for semi-supervised text classification. We attribute this to the fact that back-translation tends to change the entire sentence into a semantically similar but syntactically different sentence. Compared with other word-level perturbation strategies, back-translation triggers a larger mismatch between the perturbed input and the pseudo-target sentence generated from the unperturbed input, leading to a poor content preservation ability of the model. In contrast, simple word-level noises achieved consistently better results, especially spell error (\textit{spell}), random word swapping (\textit{swap}), and abbreviation replacing (\textit{abbr}). These three methods tend to alter the words but do not lose their information while other methods eliminate the entire word by deleting (\textit{drop, mask}) or replacing it with another word (\textit{synonym, tf-idf}). This may also cause a larger mismatch between the pseudo input and output. 

Hence, we draw the conclusion that \textit{simple word-level perturbations tend to bring more effects}. This differs from the observations in text classification~\cite{xie2020unsupervised} because content preservation is important in FST. In particular, we also found that \textit{spell} achieved the highest BLEU scores on both datasets. However, adding no perturbation even resulted in a worse performance than the base model. Moreover, \textit{capital} is also relatively weaker than the other two rule-based methods because it only changes the case of a chosen word. This suggests that the perturbation should not be too simple either. 


\begin{table*}[t]
    \centering
    \small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
        &  & \multicolumn{3}{c}{\textbf{E\&M}} & \multicolumn{3}{c}{\textbf{F\&R}} \\
\cmidrule(lr){3-5}   \cmidrule(lr){6-8}  
     Models   &  unlabeled data & BLEU & Acc(\%)& HM & BLEU & Acc(\%) & HM \\
     \midrule
      NMT\citep{rao2018dear} & no & 68.41 & - & - & 74.22 & - & - \\
  
      Hybrid Annotations$^\dagger$$^*$ \citep{xu2019formality} & yes & 69.28  & 89.83 & 78.23 & 74.36 & 82.96 & 78.42 \\
     NMT-Multi-task$^\dagger$ \citep{niu2018multi} & no & 72.01  & 88.84 & 79.54 & 75.35 & 80.03 & 77.62 \\
     GPT-CAT \citep{harnessing} & no & 71.39  & - & - & 77.26 & - & - \\
     Transformers (DA) \citep{zhang2020parallel}& yes &  74.24&- &- &77.97 & -& -\\
     CARI \citep{cari}& no & 74.31  & - & - & 78.05 & - & - \\
      Chawla's$^\dagger$ \citep{chawla-yang-2020-semi} & yes & 76.17  & 91.88 & 83.29 & 79.92 & 83.63 & 81.73 \\
     BART-large+SC+BLEU$^\dagger$ $^*$ \citep{lai-etal-2021-thank}& no& 76.50 & 94.42 & 84.52 & 79.25  & \textbf{90.69}& \textbf{84.58}\\
       \midrule
      Ours (base) & no & 76.87  & 90.04 & 82.94 & 80.32 & 84.01 & 82.12 \\
       Ours (best) & yes & \textbf{78.75}  & \textbf{94.56} & \textbf{85.94}& \textbf{81.37} & 86.41 & 83.81 \\
     
     
     
    
  
        \bottomrule
    \end{tabular}}
    \caption{Comparison between our approach and existing works on the test splits of GYAFC. $^\dagger$ indicates we recalculate the scores with our evaluation metrics for the output given in the paper. Otherwise, we copy the results from the paper. $^*$ indicates that the model used training data from both domains and is not comparable to our model.  }
    \label{tab:res}
\end{table*}


\subsection{Effects of Data Filtering}
\label{sec:df}
In this section, we analyze whether our proposed data filters are beneficial to the performance of our consistency training framework. Specifically, we chose the most effective data perturbation method \textit{spell} to analyze the effects of adding the three data filters: style strength (\textit{style}), content preservation (\textit{bleu}), and fluency (\textit{lm}) filters. As presented in Table \ref{tab:filter-res}, the results for different datasets and different filters have different tendencies. For example, adding the \textit{style} filter on the E\&M dataset caused negative effects while contributing the best results to the F\&R domain.

Although a filter does not necessarily improve the result, this is reasonable because filters result in less pseudo data for model training and it is difficult to control the trade-off between the quality and the quantity of selected data. Nevertheless, we still observe that the \textit{bleu} filter contributes to the highest performance of \textit{spell} for all the metrics on the E\&M domain, while \textit{style} benefits the performance of \textit{spell} the most on F\&R, leading to the best performing models of our approach\footnote{Empirically, we also found that mixing up three filters achieved no better results than a single filter, possibly because this filtered out too much pseudo data.}. 



\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
            
    & \multicolumn{2}{c}{Formality} & \multicolumn{2}{c}{Fluency} &\multicolumn{2}{c}{Meaning} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
     Model & E\&M & F\&R & E\&M & F\&R & E\&M & F\&R \\
     \midrule
    Chawla's & 1.46  &1.22 &1.85  &1.80 &\textbf{1.87} &1.88\\
    Ours (base) &1.42 & 1.28& 1.84 &1.82 & 1.86 & \textbf{1.95} \\ 
    Ours (best) &\textbf{1.55} &\textbf{1.41} & \textbf{1.88}& \textbf{1.85} &\textbf{1.87} & 1.88 \\ \bottomrule
    \end{tabular}}
    \caption{Human evaluation results.}
    \label{tab:human}
\end{table}


\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{E\&M}} & \multicolumn{2}{c}{\textbf{F\&R}} \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
     \#Parallel data   & BLEU & Acc(\%) & BLEU & Acc(\%)  \\
     \midrule
     100 (base) & 59.94 &	61.58 &	 65.13 &	49.32 	 \\ 
      
     100 (ours) & \textbf{64.40}	&\textbf{82.91} & \textbf{71.11} &\textbf{55.11} \\
     \midrule
     1000 (base) & 70.49 &	83.26 &	 75.36 &	\textbf{76.58}\\
     1000 (ours) & \textbf{72.22 }&	\textbf{85.81}  & \textbf{76.70} &	76.20 \\
     \midrule
     5000 (base) & 75.13 &	\textbf{89.55} &	 77.65 &	78.38 \\ 
     5000 (ours) & \textbf{75.67 }&	87.08 	& \textbf{78.87} &\textbf{81.01}	\\
     \midrule
     20000 (base) & 76.55 &	90.96  & 79.25 &	83.33 \\
     20000 (ours)& \textbf{76.59} &	\textbf{92.09} &	 \textbf{80.61} &\textbf{86.11} 	 \\
        \bottomrule
    \end{tabular}}
    \caption{Experimental results on test sets under low-resource settings with varied parallel data size.}
    \label{tab:few-shot}
\end{table}




\label{sec:case}

\begin{table*}[t]
  \small
    \centering{
    \begin{tabular}{lll}
        \toprule
      \multirow{5}{*}{Example 1} & Source  & \textit{I like natural / real girls, I don't like fake looking prissy drama queens.}\\ 
    &   Ours(best)  &  \textit{I like natural looking girls, not pretentious drama queens.}\\
    &   Ours(base) &  \textit{I like natural, real girls, I do not like fake looking, prissy drama queens.}\\
    &   Chawla’s &  \textit{I like natural and real girls , I do not like fake looking prissy drama queens .}\\
    &   Human-Annotation &  \textit{I like natural and real girls, not fake-looking, prissy drama queens.}\\
       
       
       \midrule
  \multirow{5}{*}{Example 2}  &    Source  & \textit{That's like Broke Back Mountain for little John Wanye.}\\ 
   &    Ours(best)  &  \textit{That is similar to ``Broke Back Mountain'' for John Wayne.}\\
    &   Ours(base) &  \textit{That is like ``Broke Back Mountain'' for John Wayne.}\\
   &    Chawla’s &  \textit{That is like Broke Back Mountain for little John Wanye .}\\
     &  Human-Annotation &  \textit{That is similar to ``Brokeback Mountain'' for young John Wayne.}\\
       \midrule
 \multirow{5}{*}{Example 3}   &    Source  & \textit{You guys don't have any reason to hate each other.}\\ 
    &   Ours(best)  &  \textit{You do not have any reason to hate each other.}\\
    &   Ours(base) &  \textit{You guys do not have any reason to hate each other.}\\
    &   Chawla’s &  \textit{You guys do not have any reason to hate each other .}\\
     &  Human-Annotation &  \textit{There is no reason for you two to dislike each other.}\\
       \bottomrule
    \end{tabular}}
    \caption{Examples sampled from the test set outputs.}
    \label{tab:case_study}
\end{table*}

\subsection{Comparison with Previous Works}
\label{sec:main}
We compare our best model with the following previous studies on GYAFC. 
\begin{itemize}
    \item \textbf{NMT} \citep{rao2018dear} is an LSTM-based encoder-decoder model with attention. 
    \item \textbf{GPT-CAT} \citep{harnessing} adopts GPT-2 and rule-based pre-processing for informal sentences. 
    \item \textbf{NMT-Multi-task} \citep{niu2018multi} jointly solves monolingual formality transfer and formality-sensitive machine translation via multi-task learning.
    \item \textbf{Hybrid Annotations} \citep{xu2019formality} trains a CNN discriminator in addition to the transfer model and adopts a cycle-reconstruction loss to utilize unsupervised data.
    \item \textbf{Transformers (DA)} \citep{zhang2020parallel} uses three data augmentation methods, including back-translation, formality discrimination, and multi-task transfer. 
    \item \textbf{CARI} \citep{cari} improves GPT-CAT by using BERT \citep{devlin2018bert} to select optimal rules to pre-process the informal sentences. 
    \item \textbf{Chawla's} \citep{chawla-yang-2020-semi} uses language model discriminators and maximizing mutual information to improve a pretrained BART-Large \citep{bart} model, along with a cycle-reconstruction loss to utilize unlabeled data.
    \item \textbf{BART-large+SC+BLEU} \citep{lai-etal-2021-thank} improves BART-large by incorporating reinforcement learning rewards to enhance style change and content preservation.
\end{itemize}
We also report the results of \textbf{Ours (base)}, our backbone T5-large model, and \textbf{Ours (best)}, our best performing models selected from Table \ref{tab:filter-res}. 

As observed in Table \ref{tab:res}, \textbf{Ours (best)} outperforms previous state-of-the-art models by a substantial margin and improves the BLEU scores from 76.17 and 79.92 to 78.75 and 81.37, respectively, on the E\&M and F\&R domains of the GYAFC benchmark. Although \textbf{BART-large+SC+BLEU} achieved better results on the Acc. of F\&R, the only released official outputs of \textbf{BART-large+SC+BLEU} were obtained from a model that was trained on the training data of both domains and adopted rewards to directly optimize style accuracy; hence, it is not directly comparable to our model. \textbf{Ours (best)} improves the fine-tuned T5-large baseline by a large margin as well, demonstrating the effectiveness of our SSL framework.

\paragraph{Human Evaluation} We also conduct human evaluation to better capture the quality of the models' outputs. Following \citep{zhang2020parallel}, we measure the \textit{Formality}, \textit{Fluency}, and \textit{Meaning Preservation} of generated sentences by asking two human annotators to assign a score ranging from \{0, +1, +2\} regarding each aspect. We randomly sampled 50 examples from the test set of each domain and compare the generated outputs of \textbf{Ours (base)}, \textbf{Ours (best)}, and the previous state-of-the-art \textbf{Chawla's} model trained on the single-domain data. In addition, the annotators were unaware of the corresponding model of each output. As shown in Table \ref{tab:human}, the human evaluation results are consistent with the automatic evaluation results: \textbf{Ours (base)} is competitive compared with \textbf{Chawla's}, while \textbf{Ours (best)} improves over the base model and outperforms the previous state-of-the-art on all the metrics, except that it presents lower results on \textit{Meaning} than \textbf{Ours (base)} on F\&R. More details on human evaluation can be found in Appendix \ref{sec:human}.

\paragraph{Qualitative Examples} We present some of the generated outputs of \textbf{Ours (base)}, \textbf{Ours (best)}, and \textbf{Chawla's} in Table \ref{tab:case_study}. It can be observed that all the models can produce high-quality outputs with considerable formality, meaning preservation and fluency. Nevertheless, \textbf{Ours (best)} exhibits a stronger capability to modify the original sentence, especially for some informal expressions, leading to the best performance on the \textit{Formality} metric. For example, it replaced ``like'' with ``similar to'' in Example 2 and deleted the informal word ``guys'' in Example 3. However, it may alter the original sentence so much that the meaning of the sentence is changed to some extent (Example 1). This may explain why \textbf{Ours (best)} achieves a lower \textit{Meaning} score than \textbf{Ours (base)} on F\&R.




\subsection{Low-Resource Experiments}
\label{sec:low}
We also simulate the low-resource settings by further reducing the size of available parallel data. Specifically, we randomly sample from the original training data with a size in the range of \{100, 1000, 5000, 20000\} and compare the results of the base model T5-Large with our SSL model. The size of unlabeled data remains 200k for each domain. We adopt the \textit{spell} data perturbation without any data filter and avoid exhaustive hyper-parameter tuning. Table \ref{tab:few-shot} demonstrates that our framework is especially effective under few-shot settings when only 100 parallel data are available. By comparing with previous state-of-the-art results on FST, we can observe that our approach can achieve competitive results with only 5000 ($<10\%$) parallel training data, and even better results with only 20000 ($<40\%$) parallel examples. 



