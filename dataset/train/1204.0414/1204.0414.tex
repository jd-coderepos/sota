\documentclass[orivec]{llncs} \usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[matrix,arrow,curve]{xy}
\CompileMatrices
\usepackage{lmodern}
\usepackage{listings}
\lstset{language=[Objective]Caml,columns=flexible,mathescape=true}
\lstset{literate={->}{}2 {<-}{}2 {<>}{}2 {infty}{}2 {<=}{}2 {>=}{}2 {xij}{}2 {yij}{}2}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{setspace}
\relpenalty=10000
\binoppenalty=10000
\usepackage{times}
\usepackage[small,compact]{titlesec}
\renewcommand{\labelitemi}{--}
\newcommand{\comment}[1]{}

\newcommand{\gramor}{\quad|\quad}
\newcommand{\TODO}[1]{\marginpar{\scriptsize #1}}
\newcommand{\resources}{\mathcal{R}}
\newcommand\set[1]{\{#1\}}
\newcommand\setof[2]{\set{#1\ /\ #2}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\fatone}{\bf 1}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Nint}[2]{[#1:#2[}
\newcommand{\C}{\mathcal{C}}
\newcommand{\nbd}{\nobreakdash-\hspace{0pt}}
\newcommand{\ie}{i.e.~}
\newcommand{\cf}{cf.~}
\newcommand{\wrt}{wrt~}
\newcommand{\resp}{resp.~}
\newcommand{\pone}{\mathbf{1}}
\renewcommand{\P}[1]{P_{#1}}
\newcommand{\V}[1]{V_{#1}}
\newcommand{\ce}{\mathop{\mathrm{e}}}
\newcommand{\ci}{\mathrm{i}}
\newcommand{\st}{\ |\ }
\newcommand{\qeq}{\quad=\quad}
\newcommand{\tstream}{\bigstar} \newcommand{\dui}{\vec{I}} \newcommand{\intp}[1]{\llbracket{#1}\rrbracket}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\vxym}[1]{\vcenter{\xymatrix{#1}}}
\newcommand{\capacity}[1]{\kappa_{#1}}
\newcommand{\tspace}[1]{\vec T(#1)}
\newcommand{\eg}{e.g.~}
\newcommand{\Mat}{\mathcal{M}}
\newcommand{\lang}[1]{\mathcal{L}(#1)}
\newcommand{\TS}[1]{\intp{#1}}
\newcommand{\transition}[1]{\overset{#1}\to}
\newcommand{\transitionpath}[1]{\overset{#1}\twoheadrightarrow}
\newcommand{\lbl}[1]{\ell(#1)}
\newcommand{\cone}{\text{\normalsize\textcircled{\scriptsize 1}}}
\newcommand{\ctwo}{\text{\normalsize\textcircled{\scriptsize 2}}}
\newcommand{\qtand}{\quad\text{and}\quad}
\newcommand{\D}{\mathcal{D}}
\newcommand{\glue}[1]{\oplus_{#1}}
\newcommand{\res}[1]{\backslash_{#1}}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\pa}[1]{\left(#1\right)}

\hyphenation{da-ta-ba-ses}





\title{Trace Spaces: an Efficient New Technique for State-Space Reduction}
\author{L. Fajstrup\inst{1}\and É. Goubault\inst{2}\and E. Haucourt\inst{2}\and S. Mimram\inst{2}\and M. Raussen\inst{1}}
\institute{Department of Mathematical Sciences, Aalborg University\and CEA,
  LIST\thanks{This work has been supported by the PANDA (``Parallel and
    Distributed Analysis'', ANR-09-BLAN-0169) French ANR project and by ESF project ACAT.}}

\hypersetup{
  pdftitle={\csname @title\endcsname},
  pdfauthor={L. Fajstrup, É. Goubault, E. Haucourt, S. Mimram, M. Raussen},
  unicode=true,
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=black
}

\renewcommand{\C}{\mathcal{C}}

\begin{document}





\maketitle

\begin{abstract}
  State-space reduction techniques, used primarily in model-checkers, all rely
  on the idea that some actions are independent, hence could be taken in any
  (respective) order while put in parallel, without changing the semantics. It
  is thus not necessary to consider all execution paths in the interleaving
  semantics of a concurrent program, but rather some equivalence classes. The
  purpose of this paper is to describe a new algorithm to compute such
  equivalence classes, and a representative per class, which is based on ideas
  originating in algebraic topology. We introduce a geometric semantics of
  concurrent languages, where programs are interpreted as directed topological
  spaces, and study its properties in order to devise an algorithm for computing
  dihomotopy classes of execution paths. In particular, our algorithm is able to
  compute a control-flow graph for concurrent programs, possibly containing
  loops, which is ``as reduced as possible'' in the sense that it generates
  traces modulo equivalence. A preliminary implementation was achieved, showing
  promising results towards efficient methods to analyze concurrent programs,
  with very promising results compared to partial-order reduction techniques.
\end{abstract}

\section*{Introduction}


Formal verification of concurrent programs is traditionally considered as a
difficult problem because it might involve checking all their possible
schedulings, in order to verify all the behaviors the programs may exhibit. This
is particularly the case for checking for liveness or reachability properties,
or in the case of verification methods that imply traversal of some important
parts of the graph of execution, such as model-checking \cite{modelchecking} and
abstract testing \cite{abstracttesting}. Fortunately, many of the possible
executions are equivalent (we say \emph{dihomotopic}) in the sense that one can
be obtained from the other by permuting independent instructions, therefore
giving rise to the same results. In order to analyze a program, it is thus
enough (and much faster) to analyze one representative in each dihomotopy class
of execution traces.

We introduce in this paper a new algorithm to reduce the state-space explosion
during the analysis of concurrent systems. It is based on former work of some of
the authors, most notably~\cite{raussen2010simplicial} where the notion of trace
space is introduced and studied, and also builds up considerably on the
geometric semantics approach to concurrent systems, as developed
in~\cite{tcs}. Some fundamentals of the mathematics involved can be found
in~\cite{grandis}. The main contributions of this article are the following: we
develop and improve the algorithms for computing trace spaces
of~\cite{raussen2010simplicial} by reformulating them in order to devise an
efficient implementation for them, we generalize this algorithm to programs
which may contain loops and thus exhibit an infinite number of behaviors, we
apply these algorithms to a toy shared-memory language whose semantics is given
in the style of~\cite{fajstrup2000infinitely}, but in this paper, formulated in
terms of d-spaces~\cite{grandis},
and we report on the implementation and experimentation of our algorithms on
trace spaces -- an industrial case-study using those methods is also detailed
in~\cite{rigorous-evidence}.


Stubborn sets~\cite{AVStubborn2}, sleep sets and persistent
sets~\cite{PGPWUsing} are among the most popular methods used for diminishing
the complexity of model-checking using transition systems; they are in
particular used in SPIN~\cite{spin}, with which we compare our work
experimentally in Section~\ref{benchmarks}. They are based on semantic
observations using Petri nets in the first case and Mazurkiewicz trace theory in
the other one. We believe that these are special forms of dihomotopy-based
reduction as developed in this paper when cast in our geometric framework, using
the adjunctions of~\cite{getco2010}. Of course, the trace spaces we are
computing have some acquaintance with traces as found in trace theory
\cite{tracetheory}: basically, traces in trace theory are points of trace
spaces, and composition of traces modulo dihomotopy is concatenation in trace
theory. Trace spaces are more general in that they consider general directed
topological spaces and not just partially commutative monoids; they also include
all information related to higher-dimensional (di-)homotopy categories, and not
just the fundamental category, as in trace theory. Trace spaces are also linked
with component categories, introduced by some of the authors \cite{apcs,apcs2},
and connected components of trace spaces can also be computed using the
algorithm introduced in \cite{concur05}.

\paragraph{Contents of the paper.}
We first define formally the programming language we are considering
(Section~\ref{language}) as well as an associated geometric semantics, (Section~\ref{geomsem}). We then
introduce an algorithm for computing an effective combinatorial representation
of trace spaces as well as an efficient implementation of it
(Section~\ref{computing}), and extend this algorithm in order to handle program
containing loops (Section~\ref{programswithloops}). Finally, we discuss various
applications, in particular to static analysis (Section~\ref{sec:static-anal})
and possible extensions of the algorithm and conclude.

\section{Geometric semantics of concurrent processes}
\comment{
\subsection{An informal introduction}
\label{informal}
Consider the following program consisting of two subprograms, which modify
variables, executed in parallel:

\noindent where assignments are supposed to be atomic. This program might be
scheduled in three different ways, respectively giving rise to the following
three interleavings of the instructions:

which might be represented graphically by a transition graph

Notice that the first two interleavings of~\eqref{eq:ex-interleavings} give rise
to the same resulting state (in the end  and ),
whereas the third produces a different state ( and
). The reason why the first two are equivalent is that the
instructions  and  ``commute'', \ie the way they
are scheduled cannot be observed, because they modify different variables: in
this sense the first two executions are equivalent, or \emph{dihomotopic}. Using
a terminology borrowed from category theory, one could say that the
diagram~ commutes, whereas the diagram~ does not; or, if we see
the transition graph as a 2-dimensional topological space, the square~
would be filled, whereas the square~ would be a hole. With that last
view, the algebraic topological notion of continuous deformation or
dihomotopy~\cite{tcs,grandis} coincides with local commutation of actions.

In most concurrent programming languages, the programmer is responsible for
ensuring that a variable (or more generally a shared resource) will not be
accessed concurrently by two processes.
This is usually done by using \emph{mutexes}, which are locks ensuring this
property. For instance the program~\eqref{eq:prog-ex} should be rewritten as

where the instruction~ locks the mutex~ and~ unlocks it (these
respectively correspond to \verb+pthread_mutex_lock+ and
\verb+pthread_mutex_unlock+ functions of the POSIX thread library), and mutexes
act in such a way that they cannot be locked by two processes at the same
time. In order to abstract away from the irrelevant details of the programming
language, we suppose that all involved variables are protected by mutexes
ensuring they will not be accessed by two processes at the same time, and
moreover we forget about the instructions other than control flow and mutex
manipulations since they determine both the structure of the program and whether
two schedulings of the program are dihomotopic or not. So, the
program~\eqref{eq:prog-ex} will be simplified into


In order to devise an algorithm for computing the dihomotopy classes of
interleavings, we shall use geometrical intuition and formalism by introducing a
semantics in which programs are interpreted by topological spaces. For instance,
the process  will be interpreted as a finite line

The execution of the process will be modeled as a path going from the left to
the right of the figure: the progression of time imposes a direction in paths of
our spaces. When the path reaches the point marked~, the program performs
the action~ and so on. At each point of the space, there is thus an
associated usage of resources; for instance, in all the points strictly between
the points~ and~, the mutex~ is taken but not the mutex~. In a
similar fashion, the process  is interpreted by a finite directed
line, and the process~\eqref{eq:prog-ex} as a cartesian product of the
interpretations of the two programs in parallel:

Again, an execution of the process will correspond to a continuous path going
from the lower-left to the upper-right corner (the beginning and end points),
which is always increasing (going up and right), such as the red path
corresponding to the interleaving . These are
called dipaths, and are going to be points in trace spaces, formally introduced
in Section \ref{sec:ts}.  Resource usage is also defined in each point of the
space. In particular, at the points in the interior of the gray square, the
mutex~ is taken twice (once by each process), and the semantics of mutexes
ensures that this situation does not happen. So in fact, any valid execution
path does not cross the gray square, which is called a \emph{forbidden region}
and is removed from the space (\ie it is a hole).

In order to determine the dihomotopy classes of paths in the space, the general
idea of the algorithm is to test for each hole all the possible schedulings. In
our example, the mutex~ is taken first either by the first or the second
process. More generally, we test for each hole a possible class of scheduling by
forbidding some process to take a mutex first, which amounts to removing the light
gray portion of the space in the examples below, and computing whether there
exists a path from the beginning to the end satisfying this scheduling.

The idea might seem simple, but it turns out to be difficult to handle correctly
and efficiently in the general case, as handled in the present
article.


}

\subsection{A toy shared-memory concurrent language}
\label{language}

In this paper, we consider a toy imperative shared-memory concurrent language as
grounds for experimentation. In this formalism, a program can be constituted of
multiple subprograms which are run in parallel. The environment provides a set
of resources~, where each resource~ can be used by
at most~ subprograms at the same time, the
integer~ being called the \emph{capacity} of the
resource~. In particular, a \emph{mutex} is a resource of capacity~.

Whenever a program wants to access a resource~, it should acquire a lock by
performing the action~ which allows access to~, if the lock is
granted. Once it does not need the resource anymore, the program can release the
lock by performing the action~, following again the notation set up by
Dijkstra~\cite{DijkstraPV}. If a subprogram tries to acquire a lock on a
resource~ when the resource has already been locked~ times, the
subprogram is stuck until the resource is released by an other subprogram. In
order to be realistic even though simple, the language considered here also
comprises a sequential composition operator~, a non-deterministic choice
operator~ and a loop construct~, with similar semantics as in regular
languages (it should be thought as a \texttt{while} construct), as well as a
parallel composition operator~ to launch two subprograms in parallel.

Programs~ are defined by the following grammar:

Programs are considered modulo a \emph{structural congruence}~ which
imposes that operators ,  and~ are associative and admit~ as
neutral element.
A \emph{thread} is a program which does not contain the parallel composition
operator~.


\comment{
\subsection{Trace semantics}
\label{sec:ts}
Suppose given an alphabet set~. Recall that a graph~ consists of
a set~ of \emph{vertices} (or \emph{states}) and a set~ of \emph{edges} (or \emph{transitions}). We sometimes
write~ for an edge~, and~ is called the
\emph{label} of the transition. The notion of transition graph is a common tool
in the study of semantics of programming languages. However, in order to
properly model concurrent computations, one should also consider commutations
between transitions.

\begin{definition}
  An \emph{asynchronous graph} \hbox{} consists of a graph 
  together with a set~ of \emph{independence tiles} which are pairs of paths
  of length~, with the same source and target, and with labels of the
  form~ and~, which we sometimes draw as
  
\end{definition}

\noindent
These are close to transition systems with
independence~\cite{MABCategories,MWSConcurrent}. Intuitively, a tile relating
two such paths means that the transitions~ and~ can be permuted in the
program, as in the tile~ in the introductory
example~\eqref{eq:ex-tg}. For the sake of simplicity, we only present
asynchronous graphs here, but it should be noted that they are particular cases
of a more general notion called \emph{cubical sets}~\cite{getco2010}, which is
able to model commutations between any number of events. All the developments
carried on here can be generalized to those.

Given two asynchronous graphs~ and~, their \emph{asynchronous tensor
  product} \hbox{} is defined as follows. Its underlying
graph~ is the so called ``cartesian product of graphs'' (which is not
actually a cartesian product in the category of graphs but only a tensor
product) defined by~ and the transitions are of the
form~ or~ when there
exists a transition~ in~ or in~ respectively (\ie
every transition in~ either comes from~ or from~). Its independence
tiles relate every two paths of the form

where the transitions~ and~ come
from~ and~ respectively.

From now on, suppose that~ is the set of
\emph{actions}. To every program~ we associate an asynchronous graph~
and two vertices~ and~ of~ (the \emph{beginning} and the
\emph{end}) defined inductively by
\begin{itemize}
\item  is the terminal graph (with one vertex and no edge),
\item  is the graph  (with two vertices
  and one edge),
\item  is the graph  (with two
  vertices and one edge),
\item  is the graph obtained from the disjoint union of~ and~
  by identifying~ with~, such that~ and~,
\item  is the graph obtained from the disjoint union of~ and~
  by identifying~ with~ and~ with~, such
  that  and~,
\item  is obtained from~ by identifying~ with~, such
  that ,
\item  is the graph~ with 
  and .
\end{itemize}
A \emph{total path} in such a graph is a path from the beginning to the end.

We write~ for the free monoid of words over~. Every path~
in an asynchronous graph~ (also called a \emph{trace}) is labeled by a
word~ in~. The set~ of
functions~ can be equipped with a structure of additive monoid
with the constant function equal to~ as unit, and the sum~ of two
functions~ and~ being defined pointwise, \ie as the function which to
every resource~ associates \hbox{}. The
\emph{resource function}  is the morphism of monoids
such that

In the following, we always suppose that the graph~ is such that for every
two paths~ and \hbox{}
with~ as source and the same target, we have
\hbox{}. This property can be enforced on programs by
a simple syntactic criterion~\cite{fajstrup2000infinitely}, based on a
well-bracketing condition (if we see resource locking and unlocking as an
opening and closing bracket respectively). Given a state~ reachable
from~, we write \hbox{} for any path
\hbox{}.


The \emph{asynchronous transition system}~ of a program~ is the
asynchronous graph obtained from~ by removing all the vertices~ not
satisfying \hbox{} for some resource
, as well as all edges and independence tiles involving
them. For instance the asynchronous graph associated to the
program~\eqref{eq:pv-ex} is the graph

with all the squares marked~ as independence tiles. We write~ for the
congruence on paths generated by~, called \emph{dihomotopy}: it is the
smallest equivalence relation such that~ for every pair of
paths~, and if~ then~ for every paths~ and~ for which the concatenations make
sense. The \emph{schedulings} of a program~ is the set of
paths~ quotiented by dihomotopy.  As we will see in
Section \ref{sec:ts}, this describes the connected components of the trace
space. In order to compute this trace space, it turns out to be convenient to
adopt a more geometrical point of view and replace the asynchronous graphs by
topological spaces (their geometric realizations).





}

\subsection{Geometric semantics}
\label{geomsem}

We introduce here a semantics based on
(directed) topological spaces. The geometric semantics will allow a
different representation of~ pairwise independent actions (as the surface of
an -cube) and~ truly concurrent actions as the full -cube.

We denote by~ the standard euclidean interval. A
\emph{path}~ in a topological space~ is a continuous map~, and
the points~ and~ are respectively called the \emph{source} and
\emph{target} of the path. Given two paths~ and~ such that , we
define their \emph{concatenation} as the path~ defined by


A topological space can be equipped with a notion of ``direction'' as
follows~\cite{grandis}:

\begin{definition}
  A \emph{directed topological space} (or \emph{d-space} for short) 
  consists of a topological space~ together with a set~ of paths in~
  (the \emph{directed paths}) such that
  \begin{enumerate}
  \item \emph{constant paths}: every constant path is directed,
  \item \emph{reparametrization}:  is closed under precomposition with (non
    necessarily surjective) increasing maps \hbox{}, which are called
    \emph{reparametrizations},
  \item \emph{concatenation}:  is closed under concatenation.
  \end{enumerate}
  A morphism of d-spaces , a \emph{directed map}, is a continuous
  function \hbox{} which preserves directed paths, in the sense that
  .
\end{definition}

\comment{\begin{example}
  Every topological space~ equipped with a partial order~ defines a
  d\nbd{}space by taking~ the set of paths~ which are increasing.
In particular, we often write~ for the d-space induced by the unit
  interval \hbox{} equipped with the usual total order. Notice that
  given a d-space~, the maps \hbox{} are the directed paths
  in~ and the maps~ are the reparametrizations.
\end{example}
}


\comment{\begin{wrapfigure}{r}{1cm}
  \vspace{-2.5ex}
  \begin{tikzpicture}[scale=0.5]
    \draw (1,1) circle (1);
    \draw[->] (2,1.01) -- (2,0.99);
    \draw[->] (0,0.99) -- (0,1.01);
  \end{tikzpicture}
\end{wrapfigure}
The circle~ in the complex plane
can be equipped with a structure of d-space with~ being the set of
paths~ of the form~ for some increasing
function~. Notice that in this case, the structure of directed spaces
is not induced by a partial order on the space, which makes d-spaces a more
general notion. \bigskip
}




The category of d-spaces is complete and cocomplete~\cite{grandis}. This allows
us to abstractly define some constructions on d-spaces, which extend usual
constructions on topological spaces, that we detail here explicitly by
describing the associated directed paths.
\begin{itemize}
\item The \emph{terminal d-space}  is the space reduced to one
  point.
\item The \emph{cartesian product}  of two d\nbd{}spaces~ and~
  has .
\item The \emph{disjoint union}  of two d\nbd{}spaces~ and~ is
  such that .
\item The \emph{amalgamation}  of two points  and  in a
  d\nbd{}space  is the d\nbd{}space~ where~ and  have been
  identified, together with the expected set of directed paths.
\item Given a d-space~ and a topological space~, the
  \emph{subspace}~ can be canonically equipped with a structure of d-space
  by~.
\end{itemize}
The geometric semantics of a program is defined using those constructions as
follows:

\begin{definition}
To every program~, we associate a d-space~ together with a pair of
  points , respectively called \emph{beginning} and \emph{end},
  and a \emph{resource function}  which indicates
  the number of locks the program holds at a given point. The definition of
  these is done by induction on the structure of~ as follows:

  \medskip
  \noindent
  \begin{tabular}{l|l}
    ,\quad ,\quad ,\quad 
    \\
    \hline
    ,\quad ,\quad ,
    &
    ,\quad ,\quad ,
    \\
    b=ax>0b\neq ax=0
    &
    b=ax=1b\neq ax<1
    \\
    \hline
    ,
    &
    ,
    \\
    ,\quad ,
    &
    ,\quad ,
    \\
    x\in G_px\in G_q
    &
    x\in G_px\in G_q
    \\
    \hline
    ,
    &
    ,
    \\
    ,\quad ,
    &
    ,\quad ,
    \\
    
    &
    
  \end{tabular}


  \medskip\noindent
  Given a program~, the \emph{forbidden region} is the d-space~ defined by
  
  The \emph{geometric realization} of a process~, is defined as the
  d\nbd{}space .
\end{definition}

\noindent
We sometimes write~ and~ for the beginning and the end points
respectively of a geometric realization, and say that a path~ is
\emph{total} when it has~ as source and~ as target. It is easy to
show that the geometric semantics of a program is well-defined in the sense that
two structurally congruent programs give rise to isomorphic geometric
realizations.



\begin{example}
The processes

  respectively have the following geometric realizations, which all consist of a
  space with some ``holes'', drawn in gray, induced by the forbidden region:
  
  The space in the middle is sometimes called the ``Swiss flag'' because of its
  form and is interesting because it exhibits both a deadlock and an unreachable
  region~\cite{tcs}.
\end{example}

\comment{
The fact that the definition of the geometric semantics resembles a lot the
trace semantics introduced in Section~\ref{sec:ts} can be explained by the fact
that it is in fact a ``geometrization'' of the trace semantics. Namely, if we
see a vertex as a point, an edge as a directed segment~, an independence
tile as a directed square~, and glue these topological spaces
according to how they are connected in the asynchronous graphs, then we recover
a subset of the geometric semantics (this process can be formally expressed
category using a coend): this process is called the \emph{geometric realization}
of a cubical set.
In particular, this implies that the schedulings in trace and geometric semantics
are essentially the same:
\begin{proposition}
  Given a program~, there is a (well-behaved) injection~ from the set
  of total paths of the trace semantics of~ to the set of total paths of the
  geometric semantics of~. Moreover, every total path in the geometric
  semantics is dihomotopic to a total path in the image of~; and two
  total paths in the trace semantics are dihomotopic if and only if their images
  under~ are dihomotopic.
\end{proposition}
The notion of dihomotopy in geometric semantics is formally introduced in
Definition~\ref{def:dihomotopy} below. We call any total path in the image
of~, dihomotopic to~ in the geometric semantics, a \emph{lifting}
of~.
}

\section{Computing trace spaces}
\label{computing}
\subsection{Trace spaces}
\label{tracespaces}
In topology, two paths~ and~ are often considered as equivalent when~
can be obtained by deforming continuously~ (or vice versa), this equivalence
relation being called \emph{homotopy}. The corresponding variant of this
relation in the case of directed topological spaces is called \emph{dihomotopy}
and is formally defined as follows. In the category of d-spaces, the
object~ is \emph{exponentiable}, which means that for every d-space~,
one can associate a d-space~ such that there is a natural bijection
between morphisms~ and morphisms~. The
underlying space of~ is the set of functions~ with the
compact-open topology (also called uniform convergence topology), and the
directed paths \hbox{} are the functions such that~ is increasing for every~. Finally, two paths are said to be
dihomotopic when one can be continuously deformed into the other:

\begin{definition}
  \label{def:dihomotopy}
  The \emph{dihomotopy} is defined as the smallest equivalence relation on paths
  such that two directed paths~ are dihomotopic when there exists
  a directed path~ with~ as source and~ as target.
\end{definition}

\begin{example}
  In the geometric semantics of the program ,
the two paths above the hole are dihomotopic, whereas the path below is not
  dihomotopic to the two others:
   
\end{example}

\noindent
The intuition underlying the geometric semantics is that two dihomotopic paths
correspond to execution traces differing by inessential commutations of
instructions, thus giving rise to the same result.

Given two points~ and~ of a d-space~, we write~ for the subset
of~ consisting of dipaths from~ to~. A \emph{trace} is the
equivalence class of a path modulo surjective reparametrization, and a
\emph{scheduling} is the equivalence class of a trace modulo dihomotopy. We write
 for the \emph{trace space} obtained from~ by
identifying paths equivalent up to reparametrization, and simply 
for . In particular, we have
 if and only if there exists a directed path
in~ going from~ to~.

In this section, we reformulate the algorithm for computing the trace
space~\hbox{} up to dihomotopy equivalence, originally introduced
in~\cite{raussen2010simplicial}, in order to achieve an efficient implementation
of it. For simplicity, we restrict here to spaces which are geometric
realizations of programs of the form

where the~ are built up only from~, concatenation, resource locking
and resource unlocking (extending the algorithm to programs which may contain
loops requires significant generalizations which are described in
Section~\ref{programswithloops}). In this case, the geometric realization is of
the form
\vspace{-2ex}

where  denotes the cartesian product of~ copies of~, and each
\hbox{} is a rectangle. We suppose here that
each~ is homothetic to the \nbd{}dimensional open rectangle, \ie each
directed interval~ is of the form~, and
generalize this at the end of the section. The restrictions on the form of the
programs are introduced here only to simplify our exposition: programs with
choice can be handled by computing the trace spaces on each branch and program
with loops can be handled by suitably unfolding the loops so that all the
possible behaviors are exhibited (a detailed presentation of this is given in
Section~\ref{programswithloops}, which will enable to handle the full
language). We suppose fixed a program with~ threads and~ forbidden open
rectangles, and consistently use the notations above.

\begin{example}
  \label{ex:geom-rel}
  The geometric realization of the programs
  
  are respectively
  \vspace{-4ex}
  
\end{example}

\subsection{The index poset}
\label{indexposet}
Let us come back to the second program of Example~\ref{ex:geom-rel}.  We will
determine the different traces, and their relationships in the trace space, by
combinatorially looking at the way they can turn around holes. To see this in
that example, we extend each hole in parallel to the axes, below or leftwards
from the holes, until they reach the boundary of the state space. These new
obstructions impose traces to go the other way around each hole: the existence
of deadlocks, given these new constraints in the trace space allows us to
determine whether traces going one way or the other around each hole exist. In
fact, this combinatorial information precisely computes all of the trace
space~\cite{raussen2010simplicial}.

In the second program of Example~\ref{ex:geom-rel}, there are four possibilities
to extend once each of the two holes:

Notice that there exists a total path in the first three spaces (as depicted
above), whereas there is none in the last one.

A simple way to encode the combinatorial information about the extension of
holes is through boolean matrices. We write~ for the poset of
 matrices, with~ rows (the number of holes ) and~ columns
(the dimension of the space, \ie the number of threads in the program), with
coefficients in~, with the pointwise ordering such that : we
have  whenever

where~ denotes the set  of integers and 
denotes the \nbd{}th coefficient of~. We also write  for
the subposet of~ consisting of matrices whose row vectors are all
different from the zero vector, and~ for the subposet
of~ consisting of matrices whose column vectors are all unit vectors
(containing exactly one coefficient~).

Given a matrix , we define~ as the subspace of~ obtained
by extending downwards each forbidden rectangle~ in every direction~
different from~ for every~ such that~. Formally,

where , see~\eqref{eq:ex-hole-ext} and
Example~\ref{ex:mat2d} below.


In order to study whether there is a total path in the space associated to a
matrix, we define a map \hbox{} by~ iff
, \ie there is no total path in~. A matrix~ is
\emph{dead} when  and \emph{alive} otherwise. The map~ can
easily be shown to be order preserving.

\begin{definition}
  We write
  
  for the set of (column) dead matrices and
  
  for the set of alive matrices (with non-empty rows), which is called the
  \emph{index poset} -- it is implicitly ordered by the
  relation~\eqref{eq:mat-order}.
\end{definition}

\begin{example}
  \label{ex:mat2d}
  In the example above, the three extensions of holes~\eqref{eq:ex-hole-ext} are
  respectively encoded by the following matrices:
  
  The last matrix is dead and the three others are alive. The last matrix being
  dead indicates that there is no way a trace can pass left of the upper left
  hole and carry on passing below the lower right hole.
\end{example}

\comment{
\begin{example}
  \label{ex:mat3d}
  The geometric semantics of the program constituted of three copies of the
  thread  in parallel, with , is
  \vspace{-2ex}
  
  The spaces~ corresponding to the matrices
  
  are respectively
  
  The first two matrices are alive, as shown by the drawn total paths.
\end{example}
}

A reason why the matrices in the index poset are convenient objects to study the
schedulings is that they are topologically very
simple~\cite{raussen2010simplicial}:
\begin{proposition}
  \label{prop:alive-dihom}
  For any matrix , the space  is either empty or
  contractible: any two paths with the same source~ and target~ are
  dihomotopic. In particular, for any matrix , the space
   is always contractible.
\end{proposition}

Our main interest in the index poset is that it enables us to compute the
schedulings (\ie maximal paths modulo dihomotopy) of the space: these
schedulings are in bijection with alive matrices in~ modulo an
equivalence relation called \emph{connexity}, which is defined as follows. Given
two matrices~, their \emph{intersection}  is
defined as the matrix~ such that .

\begin{definition}
  \label{def:connected}
  Two matrices~ and~ are \emph{connected} when their intersection does not
  contain any row filled with~.
\end{definition}

\noindent
The dihomotopy classes of total paths in~ can finally be computed thanks to
the following property:

\begin{proposition}
  \label{prop:conn-comp}
  The connected components of~ are in bijection with schedulings in~.
\end{proposition}

\begin{example}
  \label{ex:cube}
  Consider the program  where . The associated trace
  space~ is a cube minus a cube (as shown in Example~\ref{ex:delooping}).
  The matrices in~ are

  and they are all (transitively) connected. For instance,
  . The program~ thus has exactly one total scheduling, as expected.
\end{example}

Intuitively, alive matrices describe sets of dihomotopic total paths
(Proposition~\ref{prop:alive-dihom}) and the fact that two matrices have
non-zero rows in their intersection means that there are paths which satisfy
the constraints imposed by both matrices, \ie the two matrices describe the same
dihomotopy class of total paths.


\subsection{Computing dihomotopy classes}
\label{sec:homotopy-classes}
\label{algo}
The computation of the dihomotopy classes of total paths in the geometric
semantics~ of a given program will be performed in three steps:
\begin{enumerate}
\item we compute the set  of dead matrices,
\item we use~ to compute the index poset~,
\item we deduce the homotopy classes of total paths by quotienting~ by
  the connexity relation.
\end{enumerate}
These steps are detailed below.







Given a subset~ of  and an index~, we
write~ (by convention
\hbox{}). Given a matrix~, we define the
set of \emph{non-zero rows} of~ by \hbox{}. It can be shown that a matrix~ is dead if
and only if the space  contains a deadlock. From the characterization of
deadlocks in geometric semantics given in~\cite{fajstrup1998detecting}, the
following characterization of dead matrices can therefore be deduced:






\begin{proposition}
  A matrix  is in  iff it satisfies
  
\end{proposition}

\begin{example}
  In the example below with~ and~, the matrix~ is dead (we suppose that  and ):
  
\end{example}

\comment{
\begin{example}
  \label{exdead1}
  Consider the geometric semantics of the second program of
  Example~\ref{ex:geom-rel}. The minimal dead matrices are
  
\end{example}
}

The above proposition enables us to compute the set of dead matrices, for
instance by enumerating all matrices and checking whether they satisfy
condition~\ref{eq:dead} (a more efficient method is described in
Section~\ref{sec:implem}). From this set, the index poset~ can be
determined using the following property:

\begin{proposition}
  \label{prop:index-dead}
  A matrix  is not in~ iff there exists a matrix
  \hbox{} such that . In other words,  iff for
  every matrix \hbox{} there exists indexes  and
   such that  and~.
\end{proposition}

Notice that the poset~ is downward closed (because~ is order
preserving) and one is naturally interested in the subset~ of
\emph{maximal} matrices in order to describe
it. Proposition~\ref{prop:index-dead} provides a simple-minded algorithm for
computing (maximal) matrices in~. We write
. We then compute the sets~ of maximal
matrices~ such that for every  we have~. We
start from the set \hbox{} where~ is the matrix
containing only~ as coefficients. Given a matrix~, we
write~ for the matrix obtained from~ by replacing the
\nbd{}th coefficient by . The set~ is then computed
from~ by doing the following for all matrices~ such that
\hbox{}:
\begin{enumerate}
\item remove~ from~,
\item for every~ such that~,
  \begin{itemize}
  \item remove every matrix~ such that~,\item if there exists no matrix~ such that~,
    add~ to~.
  \end{itemize}
\end{enumerate}
The set~ is obtained as . If we remove the second point and
replace it by
\begin{enumerate}
\item[2'.] for every~ such that~ and
  ~, \hbox{add~ to~.}
\end{enumerate}
we compute a set~ such that~,
which is enough to compute connected components and has proved faster to compute
in practice.

\begin{example}
  \label{exnaive}
  Consider again Example \ref{ex:geom-rel}.
The algorithm starts with
  
  For , we must have  so we swap any of the two ones in the
  first row:
  
  Similarly for , we have to swap the bits on the second row so that
  :
  
  Finally, we have , excepting , so we swap the
  bits in position  and in position :
  
  Since we are only interested in maximal matrices, we end up with
  . The trace spaces corresponding to those matrices are
  the three first depicted in~\eqref{eq:ex-hole-ext}. None of those matrices
  being connected, the trace space up to dihomotopy consists of exactly 3
  distinct points.
\end{example}


Other implementations of the algorithm can be obtained by reformulating the
computation of~ as finding a minimal transversal in a
hypergraph, 
for which efficient algorithms have been
proposed~\cite{kavvadias1999evaluation}.


We have supposed up to now that the forbidden region was a
union of rectangles~, each such rectangle being a product of open
intervals~. The algorithm given above can easily be
generalized to the case where the rectangles~ can ``touch the boundary'' in
some dimensions, \ie the intervals~ are either of the form
 or  or  or . For example,
the process , with~, generates
such a forbidden region. We write~ for the \emph{boundary
  matrix}, which is the matrix such that  whenever  (\ie the
-th interval touches the lowest boundary in dimension~) and
\hbox{} otherwise. The matrices of~ are the matrices~ of the form \hbox{}, for some matrix~,
which satisfy~\eqref{eq:dead} and such that

where~ is the set of indexes of null columns of~.

\subsection{An efficient implementation}
\label{sec:implem}
In order to compute the set~ of dead matrices, the general idea is to
enumerate all the matrices~ and check whether they satisfy the
condition~\eqref{eq:dead}. Of course, a direct implementation of this idea would
be highly inefficient since there are  matrices in~. In order
to improve this, we try to detect ``as soon as possible'' when a matrix does not
satisfy the condition: we first fix the coefficient in the first column of~
and check whether it is possible for a matrix with this first column to be dead,
then we fix the second column and so on. In fact, we have to check that every
coefficient  such that~ satisfies~. Now,
suppose that we know some of the coefficients  for which~. We
therefore know a subset~ of the non-zero rows. If for one of
these coefficients we have~, we know that the matrix cannot
satisfy the condition~\eqref{eq:dead} because~. A similar reasoning can be held for
condition~\eqref{eq:dead-bounds}.

\begin{figure}[t]
{  \centering
{\small
\begin{lstlisting}
let rec compute_dead     =
  if  =  then  :=  :: ! else
    for  =  to  - 1 do
      try
        let  = not (Set.mem  ) in
        let  = Set.add   in
        let  = Array.copy  in
        if (,) = 1 then .() <- None else .() <- Some ;
        (match .() with
            | Some  -> if xij >= .() then raise Exit
            | None -> if .() <> infty then raise Exit);
        let  =
          let  =  in
          if not  then  else
            Array.mapi (fun  yrj ->
                if yrj <= yij then yrj else
                  match .() with
                    | None ->
                        if  <=  && yij <> infty then raise Exit; yij
                    | Some  ->
                        if xij >= yij then raise Exit; yij
                        ) 
        in
        compute_dead (+1)   
      with Exit -> ()
    done
\end{lstlisting}
}
}
 \vspace{-3ex}
 \caption{Algorithm for computing dead matrices.}
 \label{fig:dead-algo}
 \vspace{-4ex}
\end{figure}

The actual function computing the dead matrices is presented in
Figure~\ref{fig:dead-algo}, in pseudo-OCaml code. This recursive function
fills~-th column of the matrix~ (whose columns with index below~ are
supposed to be already fixed) and performs the check: it tries to set the -th
coefficient to~ (and all the others to~) for every~. If a
matrix beginning as~ (up to the -th column) cannot be dead, the
computation is aborted by raising the Exit exception. When all the columns have
been computed the matrix is added to the list  of dead matrices. Since a
matrix~ has at most one non-null coefficient in a given
column, it will be coded as an array of length~ whose -th element is
either None when all the elements of the -th column are null, or Some~
when the -th coefficient of the -th column is~ and the others are
. The argument  is the set of indexes of known non-null rows of~ and
 is an array of length~ such that .(). The
matrix  is the matrix previously noted~ used to perform the
check~\eqref{eq:dead-bounds}.
Notice that the algorithm takes advantage of the
fact that when the coefficient~ chosen for the -th column is already
in~ (\ie when the variable  is false) then many
computations can be spared because the coefficients~ are not
changed.



Once the set of dead matrices computed, the set~ of alive matrices is
then computed using the naive algorithm of Section~\ref{sec:homotopy-classes},
exemplified in Example \ref{exnaive}. We have also implemented a simple
hypergraph transversal algorithm \cite{Berge} but it did not bring significant
improvements, more elaborate algorithms might give better results
though. Finally, the representatives of traces are computed as the connected
components (in the sense of Proposition~\ref{prop:conn-comp}) of~, in a
straightforward way. An explicit sequence of instructions corresponding to every
representative~ can easily be computed: it corresponds to the sequence of
instructions crossed by any increasing total path in the d-space~.


\subsection{An example: the  dining philosophers}
\label{benchmarks}
In order to illustrate the performances of our algorithm, we present below the
computation times for the well-known ~dining philosophers
program~\cite{philosophers} whose schedulings are 
in , hence is pushing any algorithm that would determine the essential
schedules to its (exponential) limits.
It is constituted of~
processes~ in parallel, using~ mutexes~, defined by
\hbox{}, where the indexes on
mutexes~ are taken modulo~. Such a program generates~ distinct
schedulings, which our program finds correctly. The table below summarizes the
execution time and memory consumption for our tool \hbox{ALCOOL} (programmed in
OCaml), as well as for the model checker SPIN~\cite{spin} implementing partial
order reduction techniques. Whereas SPIN is not significantly slower, it consumes
much more memory and starts to use swap from  (thus failing to give an
answer in a reasonable time for ). Notice that the implementation of SPIN
is finely tuned and also benefits from \texttt{gcc} optimizations, whereas there
is room for many improvements in ALCOOL. In particular, most of the time is
spent in computing dead matrices and the algorithm of Section~\ref{sec:implem}
could be improved by finding a heuristic to suitably sort holes so that failures
to satisfy condition~\eqref{eq:dead} are detected earlier. The present algorithm
is also significantly faster than some of the author's previous
contribution~\cite{concur05}: for instance, it was unable to generate these
maximal dipaths because of memory requirements, for  philosophers with 
(in the benchmarks of~\cite{concur05}, it was taking already 13739s, on a 1GHz
laptop computer though, to generate just the component category for 9
philosophers).

\begin{center}
  \begin{tabular}{r|r|r|r|r|r}
    &sched.& ALC. (s)&ALC. (MB)&SP. (s)&SP. (MB)\\
    \hline
10&1022&5&4&8&179\\
    11&2046&32&9&42&816\\
    12&4094&227&26&313&3508\\
    13&8190&1681&58&&\\
    14&16382&13105&143&&\\
  \end{tabular}
\end{center}



Since the size of the output is generally exponential in the size of the input,
there is no hope to find an algorithm which has less than an exponential
worst-case complexity (which our algorithm clearly has). However, since our goal
is to program actual tools to very concurrent programs, practical improvements
in the execution time or memory consumption are really interesting from this
point of view. We have of course tried our tool on many more examples, which
confirm the improvement trend, and shall be presented in a longer version of the
article.





\section{Programs with loops}
\label{programswithloops}
\subsection{Paths in deloopings}
One of the most challenging part of verifying concurrent programs consists in
verifying programs with loops since those contain a priori an infinite number of
possible execution traces. We extend here the previous methodology and, given a
program containing loops, we compute a (finite!) automaton whose accepted paths
describe the schedulings of the program: this automaton, can thus be considered
as a control flow graph of the concurrent program. Of course, we are then able
to use the traditional methods in static analysis, such as abstract
interpretation, to study the program (this is briefly presented in
Section~\ref{sec:static-anal}). This section builds on some ideas being
currently developed by Fajstrup~\cite{LF2011}, however most of the properties
presented in this section are entirely new. To the best of our knowledge, this
is the first works in which geometric methods are used in order devise a
practical algorithm to handle programs containing loops. A particularly
interesting feature of our method lies in the fact that it consider the broad
``geometry of holes'' and can thus associate a small control flow graph to a
given program, see Section~\ref{sec:loops-implem}.

In the following, we suppose fixed a program of the form
 as in~\eqref{eq:prog}, with  threads. We write

for the associated ``looping program''. Our goal in this section is to describe
the schedulings of such a program~ (the restriction on the form of the
programs considered here was only done to simplify our presentation and the
methodology can be extended to handle all well-bracketed programs generated by
the grammar, without any essential technical difficulty added). Following
Section~\ref{geomsem}, its geometrical semantics consists of an
\nbd{}dimensional torus with rectangular holes. As previously, for
simplicity, we suppose that these holes do not intersect the boundaries, \ie
that  satisfies the hypothesis of Section~\ref{tracespaces}. Given an
-dimensional vector~ with coefficients in~, the
\emph{-delooping} of~, written~, is the program
, where~ denotes the
concatenation of~ copies of~. A \emph{scheduling} in~ is a
scheduling in the previous sense (\ie a total path modulo homotopy) in~ for
some vector~.

\begin{example}
  \label{ex:delooping}
  Consider the program  of Example~\ref{ex:cube}, where . Its geometric realization~ is pictured on the left, and its
  \nbd{}delooping  is pictured on the right.
  \vspace{-4ex}
  
\end{example}

Given two spaces~ and~ which are hypercubes with holes (which is the case
for the geometric realizations of the programs we are considering here), we
write  for the space obtained by identifying the -th target face
of the hypercube~ with the \nbd{}th source face of the hypercube~, and
call it the \emph{\nbd{}gluing} of~ and~. Formally, this can be defined
as in Section~\ref{geomsem} as , where the
relation~ identifies points~ and~ such that ,
 and~ for every dimension~, and directed paths
are defined in a similar fashion. Notice that, by definition, there is a
canonical embedding of  (\resp ) into , which will allow us to
implicitly consider~ (\resp ) as a subspace of  in the
following.

\begin{example}
  \label{ex:glue322}
  The -delooping of Example~\ref{ex:delooping} is
  
\end{example}

\noindent
More generally, any \nbd{}delooping~ of a program~ of the
form~\eqref{eq:prog} can be obtained by gluing copies  of~, indexed
by a vector  such that for every dimension~ with , we have
 (what we will simply write ).

\newcommand{\shadow}[2]{#2|_{#1}}

Given two scheduling matrices~ and~ encoding extensions of holes of such a
program~ (\cf Section~\ref{indexposet}), we reuse the notation and
write~ for the obvious matrix coding extension of holes in the
space~. At this point, it is crucial to notice that the holes
described by~ in the second copy of~ can have an effect on the first
copy of~ (when they are extended to~ in the direction~), what we call
the \emph{-shadow of~}, and write .

\begin{example}
  With the program~ of Example~\ref{ex:delooping}, consider the matrices
   and
  . We have , the space  is pictured on the left, and the -shadow  of~ is
  pictured on the right:
  
\end{example}

The above example makes clear that the space corresponding to a scheduling
 is of the form , \ie the holes in the first copy come either from~ or from shadows
of~. Moreover, the holes in the space  are hypercubes which
are products of intervals of the form , where each
interval~ is of the form  or  or ,
with . The shadows can therefore be coded as matrices (using a
slightly different coding from the one used up to now, the precise way they are
coded being quite irrelevant) and we write  for the matrix coding
the -shadow of~, which can easily be computed from~ and~. A
scheduling matrix  can obviously be seen as a particular ``shadow'', enabling
us to use the same notation for both, and we write  for the union of
two shadows  and , so that . Finally, given a
shadow~, the algorithm described in Section~\ref{sec:homotopy-classes} can
easily be adapted to the new coding in order to determine whether the
space~ is alive.

\subsection{The shadow automaton}
The trace space of a program  is not finite in the general case. We show
here that it can however be described as the set of paths of an automaton that
we call the \emph{shadow automaton}: this automaton provides us with a
\emph{finite presentation} of the set of schedulings.

Consider the -delooping~ of a program~. The space~ consists
of the gluing of copies of~ indexed by vectors~ such that 
and similarly, a scheduling~ of~ consists of the gluing of matrices
. Clearly, if some submatrix~ is dead then the whole matrix~ is
dead:

\begin{lemma}
  If a matrix~ is alive then all its submatrices~ are alive.
\end{lemma}

\noindent
However, the converse is not true because a scheduling~ might create a
deadlock with the shadows coming from matrices above it. For instance in
Example~\ref{ex:delooping}, the matrix  is not alive because the
space~ induced by the submatrix  is contained in
the space~, where  is a dead matrix:
\vspace{-1ex}


In order to generate all the possible schedulings  visited by a total path
in~, we therefore have to take in account the shadows dropped by
scheduling of copies of~ in its future. We will construct an automaton
which will consider the visited schedulings of the path, starting from the end,
and maintains the shadow they produce on the next state in a given
direction~, so that we can compute the possible previous matrices in
direction~ such that the whole matrix is not dead. Formally,

\begin{definition}
  \label{shadow-automaton}
  The \emph{shadow automaton} of a program~ is a non-deterministic automaton
  whose
  \begin{itemize}
  \item states are shadows
  \item transitions  are labeled by a direction 
    (with ) and a scheduling~
  \end{itemize}
  defined as the smallest automaton
  \begin{itemize}
  \item containing the empty scheduling~
  \item and such that for every state , for every direction~ and for
    every scheduling~ such that the scheduling  is alive, and 
    is maximal with this property, there is a transition
     with~.
  \end{itemize}
  All the states of the automaton are both initial and final.
\end{definition}

\begin{example}
  \label{ex:shadow-automaton}
  Consider the program  with  whose geometric semantics is a
  square with a square hole. The associated shadow automaton is
  \vspace{-2ex}
  

  \vspace{-4ex}\noindent For instance the transition
  
  is computed as follows: we take the shadow
  
  and compute its shadow in direction , \ie on the left, to compute the
  source of the transition. This shadow is
  , namely:
  .
\end{example}

The interest of the automaton lies in the fact that fully describes the possible
schedulings crossed by a total path in a scheduling of a delooping :
\begin{theorem}
  \label{thm:sa-path-lift}
  Suppose that~ is a scheduling of , obtained by gluing schedulings
   of . Then there exists a total path in~ going through the
  subspaces  in this order, such
  that  and  only differ by one coordinate~ (\ie the path
  exits from  through its \nbd{}th face),
if and only if there exists a path labeled as follows in the shadow automaton:
  
  for some states  and dimension~.
\end{theorem}

\begin{example}
  \label{ex:paths}
  With the program~ of Example~\ref{ex:shadow-automaton}, the following paths
  in the \nbd{}delooping
  \vspace{-2ex}
  
  are respectively witnessed by the following paths of the shadow automaton:
  
\end{example}

\subsection{Reducing the size of the shadow automaton}
The size of the shadow automaton grows very quickly when the complexity of the
trace space grows. For instance, for the program~ of
Example~\ref{ex:delooping}, the shadow automaton has already 19 states and 80
transitions. We describe here some ways to reduce the automaton while preserving
Theorem~\ref{thm:sa-path-lift}. Namely, we should remark that the automaton is
not minimal in the following sense. By Proposition~\ref{prop:alive-dihom}, given
a scheduling~ two total paths~ are necessarily homotopic: an alive
scheduling thus describes an homotopy class of total paths. By
Theorem~\ref{thm:sa-path-lift}, the schedulings ``visited'' by a total path
in~ are described by a path in the shadow automaton, therefore every
homotopy class of total paths in~ is described by at least one path in
the scheduling automaton. The shadow automaton is not minimal in the sense that
generally, an homotopy class is described by more than one path in the
scheduling automaton.

\paragraph{Determinization.}
First, our non-deterministic automaton can be determinized using classical
algorithms of automata theory, which in practice greatly reduce their size: the
determinized automaton for the program of Example~\ref{ex:delooping} has only 4
states and 24 transitions.

\begin{example}
  \label{ex:sh-det}
  The determinized automata for Examples~\ref{ex:shadow-automaton}
  and~\ref{ex:delooping} are respectively:
  
  with
  \vspace{-2ex}
  
  where ``'' means any direction . The state  is initial and all the
  states are final.
\end{example}












\paragraph{Quotient under connexity.}
A way to further reduce the automaton consists in quotienting the scheduling
matrices labeling the arrows of the automaton under the connexity relation of
Definition~\ref{def:connected} before determinizing the automaton, which is
formally justified by Proposition~\ref{prop:conn-comp}.

\begin{example}
  The shadow automaton corresponding to the program Example~\ref{ex:delooping}
  quotiented under connexity, determinized and minimized is simply the automaton
   where  up to connexity (the
  matrices  are those defined in Example~\ref{ex:sh-det}).
\end{example}

We are currently investigating further conditions in order to construct the
minimal automaton describing the trace space associated to a looping program,
but the conditions mentioned above are already providing us with promisingly
small automata.

\subsection{Preliminary implementation and benchmark}
\label{sec:loops-implem}
A preliminary implementation of the computation of the shadow automaton was
done. The algorithm implemented is currently quite simple, but we plan to
generalize the algorithm of Section~\ref{sec:implem} soon, which is not
complicated from a theoretical point of view but much more involved technically,
in order to achieve better performances. Most experiments lead so far are
already promising and make it clear that taking in account the geometry of the
state-space enables us to reduce, sometimes drastically, the size of the control
flow graph corresponding to the program to be analyzed.

\begin{example}
  \label{ex:two-phase}
  The \emph{two-phase locking protocol} is a simple discipline for distributed
  databases, in which the processes first lock all the mutexes for the resources
  they are going to use and free all of them in the
  end~\cite{gunawardena1994homotopy}. This can be modeled as a program 
  consisting of ~copies of the process 
   in parallel (each of these process is using 
  resources). For instance, the geometric semantics of  is shown
  below. Notice that this state space is equivalent to a space with only one
  hole up to dihomotopy. More generally, given , it can be shown that
  the geometric semantics of  is equivalent to , which our
  algorithm is able to take into account! Namely, the size of the shadow
  automaton associated to  only depends on~ whereas the number of
  states of the automaton produced by SPIN is exponential in  (with~
  fixed). Below are presented the size (states, transitions) of the
  non-deterministic automaton (, ), determinized automaton (,) and
  SPIN's automaton (, ) for the two-phase
  locking process described in Example~\ref{ex:two-phase}, for some values
  of~ and~.
  
\end{example}



\subsection{An Application to static analysis}
\label{sec:static-anal}
Now that we have the reduced shadow automaton, we can 
explain how one can perform static analysis by \emph{abstract
  interpretation}~\cite{systematic} on concurrent systems, in an economic
way. The systematic design and proof of correctness of such abstract analysis is
left for a future article, the aim of this section is to give an intuition why
the computations of Section~\ref{programswithloops} are relevant to static
analysis by abstract interpretation. The idea is to associate, to each node 
of the shadow automaton, a set of values  that program
variables can take if computation follows a transition path
whose last vertex is~. Among the actions the program can take along this
scheduling, we consider only the \emph{greedy} ones, that is the ones which
execute all possible actions permitted by the dihomotopy class of schedulings
ending by .

Suppose that we want to analyze the program

What are the possible sets of values reached, for , starting with ? The associated shadow automaton~ has been determined in
Example~\ref{ex:sh-det} (this automaton is reduced) together with relations,
that we will not be using in this article, yet. In many ways, this reduced
shadow automaton plays the role of a compact \emph{control flow graph} for
the program we are analyzing.
Calling 
 and
,  has the effect on environment:  and
 has as effect: .

We are now in a position to interpret the arrows of the shadow automaton as
simple \emph{abstract transfer functions} and produce a system of equations for
which we want to determine a least-fixed point, to get the invariant of the
program at the (multi-)control point which is the pair of the heads of the loops
of each process.
The interpretation on the shadow automaton now gives (ignoring the initial state  in that picture, for
simplicity's sake) can be graphically pictured as:

Given the abstract transfer functions on each edge of the shadow automaton,
we produce as customary the abstract semantic equations, one per node, by
joining all transfer functions correspond to ingoing edges to that node:

This set of semantic equations can be seen as a least-fixed point equation, that
we can solve using any of our favorite tool, for instance Kleene iteration and
widening/narrowing, on any abstract domain, such as the domain of intervals as
in the example below.
The least-fixed point formulation that we are looking for is thus , where~ is the function defined in~\eqref{eq:ai-fp} and
.
A Kleene iteration on this monotonic function~ on the lattice of intervals
over~ reveals that .

We have presented this example in order to show how the reduced shadow automaton
can be used in order to use usual static analysis methods on concurrent
programs, avoiding state-space explosion as much as possible. It has the
advantage of being short, however it does not really show the main interest of
our technique: the scheduling automaton allows us to take in account properties
which tightly depend on the way the synchronizations constraint the executions
of the programs.

\section{Conclusion and Future work}
\label{concl}
We have presented an algorithm in order to compute a finite presentation of the
trace space of concurrent programs, which may contain loops. An application to
abstract interpretation has also described but remains to be implemented. In
order to give a simple presentation of the algorithm, we have restricted
ourselves here to programs of a simple form (in particular, we have omitted
non-determinism). We shall extend our algorithm to more realistic programming
languages in a subsequent article. Our approach can also be applied to languages
with other synchronization primitives (monitors, send/recv, etc.), for which
there are simple geometric semantics available.  There are also many possible
general improvements of the algorithm; the most appealing one would perhaps be
to find a way to have a more modular way of computing the total schedulings by
combining locally computed schedulings in~ with varying
endpoints~ and~. In a near future, the schedulings provided by the
algorithm will be used by our tool ALCOOL to analyze concurrent programs using
abstract interpretation, thus providing one of the first tools able to do such a
static analysis on concurrent programs without forgetting most of the possible
synchronizations during their execution.

On the theoretical side, we envisage to study in details and use the structure
of the index poset~ which contains much more information than only the
schedulings of the program. Namely, it can be equipped with a structure of
\emph{prodsimplicial set} \cite{kozlov} (a structure similar to simplicial sets
but whose elements are products of simplexes), whose geometric realization
provides a topological space which is homotopy equivalent to the trace
space~~\cite{raussen2010simplicial}. This essentially means
that~ contains all the geometry of the trace space and we plan to try to
benefit from all the information it provides about the possible computations of
a program. Our ALCOOL prototype actually implements this computation -- using a
combinatorial presentation of the prodsimplicial sets known as \emph{simploidal
  sets} \cite{simploidal} -- which will be reported elsewhere.
















\bibliographystyle{plain}
\bibliography{papers}
\end{document}
