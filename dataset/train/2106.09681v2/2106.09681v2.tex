

\appendix
\clearpage
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}

\pagenumbering{Roman}  

\newpage
\vskip .375in
\begin{center}
{\Large \bf \inserttitle \\ \vspace{0.5cm} \large Appendix \par}
  \vspace*{24pt}
  {
  \par
  }
\end{center}



\section{Preliminary study on Vision Transformers (ViT)}
\label{sec:preliminary}

In this appendix we report the results associated with our preliminary study on high-resolution transformers. Most of the experiments were carried out on the ViT architecture~\cite{dosovitskiy2020image} with DeiT training~\cite{touvron2020deit}, and intended to analyze different aspects of transformers when considering images with varying resolution or high-resolution images specifically. 

\subsection{Impact of resolution versus patch size} 
\label{sec:res_patch}

\def \mysp {\hspace{12pt}}

\begin{figure*}[htb]
\begin{minipage}[c]{0.45\textwidth}
    \includegraphics[width=\linewidth]{figs/deit-s_resolution_analysis.pdf}
\end{minipage}
    \hfill
\begin{minipage}[c]{0.49\textwidth}
    \scalebox{0.75}{
    \begin{tabular}{@{\mysp}l@{\mysp}|@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}}
    \multicolumn{7}{c}{Variable patch size\vphantom{\rule[-10pt]{1pt}{10pt}} } \\
    \toprule
        Image Size & 80 & 112 & 160  & 256 & 320 & 384\\
        \midrule
        Patch Size & 5 & 7 & 10 &  16 & 20 & 24 \\
        \midrule        
        Top-1 & 78.2 & 79.7 & 80.5 & 80.7 & 80.9 & 80.7 \\
         \bottomrule
    \end{tabular}}
    \vspace*{10pt} \\ 
    \scalebox{0.75}{
    \begin{tabular}{@{\mysp}c@{\mysp}|@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}}
    \multicolumn{7}{c}{Variable number of tokens size \vphantom{\rule[-10pt]{1pt}{10pt}}} \\
    \toprule
        Image Size &  160  & 224  & 256 & 288 & 320 & 384\\
        \midrule
        \# of tokens  & 100  & 196  & 256 & 324 & 400 & 576  \\
        \midrule        
        Top-1 & 77.4  & 79.9  & 80.7 & 81.2 & 81.5 & 82.3 \\
         \bottomrule
    \end{tabular}}
\end{minipage}    
    \caption{\textbf{Impact of input resolution on accuracy for DeiT-S}. We consider different image resolutions, and either (1) \textcolor{red!80}{increase the patch size while keeping the number of tokens fixed}; or (2) \textcolor{blue!80}{keep the patch size fixed and use more tokens}. Larger input images are beneficial if the number of tokens increases. The impact of a change of a resolution for a constant number of patches (of varying size) is almost neutral.  
As one can observe, the main driver of performance is the number of patches. The patch size has a limited impact on the accuracy, except when considering very small ones. We have observed and confirmed similar trends with \ours models.
    \label{fig:imsize_patch_tokens}}

\end{figure*}



    



\subsection{Approximate attention models in ViT with DeiT training}
\label{sec:approx_att}


In Table A.1, we report the results that we obtain by replacing the Multi-headed Self-attention operation with efficient variants \cite{ho2019axial, shen2021efficient, wang2020linformer, wang2021pyramid} in the DeiT-S backbone.  First, we can notice that for all efficient self-attention choices there is a clear drop in performance compared to the Deit-S baseline. The spatial reduction attention (SRA) proposed in PVT \cite{wang2021pyramid} has a significantly weaker performance compared to the full-attention with a quadratic complexity that is more efficient than full-attention by only a constant factor . Linformer \cite{wang2020linformer} provides a better accuracy compared to SRA, however, it is also clearly weaker than full-attention. Moreover, Linformer does not have the flexibility of processing variable length sequences which limits its application in many computer vision tasks. Efficient attention \cite{shen2021efficient} provides a better trade-off than the aforementioned methods, with improved accuracy and linear complexity. However, it has a 3.6\% drop in performance compared to full-attention. Finally, axial attention \cite{ho2019axial} provides the strongest performance among the efficient attention variants we studied with a 1.5\% drop in accuracy compared to the baseline. We observe a saving in memory usage, but a drop in speed due to the separate row and column attention operations. Our observations are consistent with \cite{dosovitskiy2020image}.
\begin{table}[h!]
\small
\captionof{table}{\footnotesize \textbf{ImageNet Top-1 accuracy of efficient self-attention variants} (after 300 epochs of training).}\vspace*{3pt}
\centering 
\begin{tabular}{l|c|c}
     \toprule    
     Model & Complexity & Top-1  \\
     \midrule
     DeiT-S~\cite{touvron2020deit} &  & 79.9  \\
     \midrule
     SRA (Average Pool)~\cite{wang2021pyramid} &   & 73.5 \\
     SRA (Convolutional)~\cite{wang2021pyramid} &  & 74.0 \\
     Linformer (k=)~\cite{wang2020linformer} &  & 75.7 \\
     Efficient Transformer~\cite{shen2021efficient} &  & 76.3 \\
    Axial~\cite{ho2019axial} &  & 78.4 \\
     \bottomrule
\end{tabular}    
\end{table}


\subsection{Training and testing with varying resolution}

As discussed in the main manuscript, for several tasks it is important that the network is able to handle images of varying resolutions. This is the case, for instance, for image segmentation, image detection, or image retrieval where the object of interest may have very different sizes. We present an analysis of train/test resolution trade-off in Table A.2.

{\centering 
\begin{table}[h!]
    \captionof{table}{\footnotesize \textbf{Trade-off between train and test resolutions for DeiT.} MS refers to multi-scale training, where the models have seen images from different resolutions at training time.}\vspace*{3pt}
    \centering 
    \scalebox{0.85}{
    \begin{tabular}{c|ccccc|c}
        \toprule
         Test / Train & 160 & 224 & 256 & 288 & 320  & MS  \\
         \toprule
         160 & \textbf{77.2} & 75.9 & 73.3 & 68.2 & 59.6   &  76.3  \\224 & 78.0 & \textbf{79.9} & 79.9 & 79.0 & 77.9   & 79.6 \\ 256 & 77.3 & 80.4 & \textbf{80.7} & 80.2 & 79.9   & 80.6  \\ 288 & 76.3 & 80.4 & 81.0 & \textbf{81.2} & 80.8   & 81.0 \\ 320 & 75.0 & 80.1 & 80.9 & 81.3 & \textbf{81.5}   & 81.3 \\ \bottomrule
    \end{tabular}}
    \label{tab:multi-scale}
    
\end{table}}











\section{Additional details of training and our architecture} 

\subsection{Sinusoidal Positional Encoding}
\label{sec:position_encoding}

We adopt a sinusoidal positional encoding as proposed by \citet{vaswani2017attention} and adapted to the 2D case by \citet{carion2020end}. However we depart from this method in that we first produce this encoding in an intermediate 64-d space before projecting it to the working space of the transformers. More precisely, in our implementation each of the  and  coordinates is encoded using 32 dimensions corresponding to cosine and sine functions with different frequencies (16 frequency for each function). The encoding of both coordinates are eventually concatenated to obtain a 64 dimension 2D positional encoding. Finally, the 64 dimension positional encoding is linearly projected to the working dimension of the model .

\subsection{Obtaining Feature Pyramid for Dense Prediction}
\label{sec:fpn_api}

For state-of-the-art detection and segmentation models, FPN is an important component which provides features of multiple scales. We adapt \ours to be compatible with FPN detection and segmentation methods through a simple re-scaling of the features extracted from different layers. In particular, for models with 12 layers, we extract features from the 4, 6, 8 and 12 layers respectively. As for models with 24 layers, we extract features from 8, 12, 16 and 24 layers. Concerning the re-scaling of the features, the 4 feature levels are downsized by a ratio of 4, 8, 16 and 32 compared to the input image size. Feature downsizing is performed with max pooling and upsampling is achieved using a single layer of transposed convolutions with kernel size  and stride .  

\subsection{Hyper-parameters: LayerScale initialization and Stochastic Depth drop-rate} 
\label{sec:ls_sd_values}

We list the stochastic depth  and LayerScale initialization  hyperparameters used by each of our models in Table~\ref{tab:drop_layerscale}.

\begin{table}[htb]
    \centering
    \caption{\textbf{Hyperparameters used for training our models}, including the Stochastic depth drop rate  and LayerScale initialization .}
    \scalebox{0.9}{
    \begin{tabular}{c c |r r}
         \toprule
         Model & Patch size &  &    \\
         \midrule
         \ours-N12 & 8 \& 16 & 0.0 &  \\ 
         \ours-T12 & 8 \& 16 & 0.0 &  \\ 
         \ours-T24 & 8 \& 16 & 0.05 &  \\ 
         \ours-S12 & 8 \& 16 & 0.05 & 1.0 \\
         \ours-S24 & 8 \& 16 & 0.1 &   \\ 
         \ours-M24 & 8 \& 16 & 0.15 &  \\ 
         \ours-L24 & 16 & 0.25 &  \\ 
         \ours-L24 & 8  & 0.3 &  \\ 
         \bottomrule
    \end{tabular}}
    \label{tab:drop_layerscale}
\end{table}




\section{Pseudo-code}

We provide a PyTorch-style pseudo code of the Cross-covariance attention operation. The pseudo code resembles the Timm library \cite{rw2019timm} implementation of token self-attention. We show that XCA only requires few modifications, namely the  normalization, setting the learnable temperature parameters and a transpose operation of the keys, queries and values. 

\begin{algorithm}[htb]
\caption{Pseudocode of XCA  in a PyTorch-like style.}
\label{alg:code}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[language=python]
# self.qkv: nn.Linear(dim, dim * 3, bias=qkv_bias)
# self.temp: nn.Parameter(torch.ones(num_headss, 1, 1))

def forward(self, x):
    B, N, C = x.shape
    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)
    qkv = qkv.permute(2, 0, 3, 1, 4)
    q, k, v = qkv[0], qkv[1], qkv[2]   # split into query, key and value

    q = q.transpose(-2, -1) 
    k = k.transpose(-2, -1)         # Transpose to shape (B, h, C, N)
    v = v.transpose(-2, -1)

    q = F.normalize(q, dim=-1, p=2)    # L2 Normalization across the token dimension
    k = F.normalize(k, dim=-1, p=2)    

    attn = (k @ q.transpose(-2, -1))       # Computing the block diagonal cross-covariance matrix
    attn = attn * self.temp                # Adjusting the activations scale with temperature parameter
    attn = attn.softmax(dim=-1)            # d x d attention map

    x = attn @ v    # Apply attention to mix channels per token 
    x = x.permute(0, 3, 1, 2).reshape(B, N, C)  
    x = self.proj(x)
    return x
\end{lstlisting}
\end{algorithm}














\section{Additional results}

\subsection{More \ours models}

We present additional results for our \ours models in Table~\ref{tab:xct_models_details}. We include performance of 384384 images using a 1616 patch size as well as results for images with 224224 resolution using patch size of 88.

\begin{table}[htb]
    \caption{\footnotesize \textbf{ImageNet-1k top-1 accuracy of \ours} for additional combinations of image and patch sizes.\vspace*{3pt}
    }
    \centering
    \scalebox{0.72}{
    \begin{tabular}{l|cccr|rccc|rccc}
    \toprule
         Models & Depth &  & \#Blocks & \@params & \multicolumn{4}{c|}{} & \multicolumn{4}{c}{}   \\ 
         \cmidrule{6-9}
         \cmidrule{10-13}
         & & &   &  & GFLOPs & @224 & @224\alambic & @384 
         & GFLOPs & @224 & @224\alambic   & @384   \\
        \midrule
        \OURS-N12    & 12 & \multirow{1}{*}{128} & \multirow{1}{*}{4}  & 3M & 0.5 & 69.9 & 72.2 & 75.4 &    2.1 & 73.8 & 76.3 & 77.8 \\
        \OURS-T12    & 12 & \multirow{1}{*}{192} & \multirow{1}{*}{4}  & 7M & 1.2 & 77.1 & 78.6 & 80.9    & 4.8 & 79.7 & 81.2 & 82.4   \\
        \OURS-T24    & 24 & 192 & 4 & 12M &  2.3 & 79.4 & 80.4  & 82.6   & 9.2  & 81.9 & 82.6 &  83.7 \\
        \OURS-S12      & 12 & \multirow{1}{*}{384} & \multirow{1}{*}{8} & 26M & 4.8 & 82.0 & 83.3 & 84.7    & 18.9 & 83.4 & 84.2 &  85.1  \\ 
        \OURS-S-24      & 24 & 384 & 8 & 48M & 9.1 & 82.6 & 83.9 & 85.1 &   36.0 & 83.9 &  84.9 & 85.6  \\ 
        \OURS-M24      & 24 & 512 & 8  & 84M & 16.2  & 82.7 & 84.3 & 85.4  & 63.9 & 83.7 & 85.1 & 85.8    \\
        \OURS-L24      & 24 &\multirow{1}{*}{768} & \multirow{1}{*}{16} &  189M & 36.1 & 82.9 & 84.9 & 85.8 & 142.2 & 84.4 & 85.4 & 86.0 \\
    \bottomrule     
 \end{tabular}
 } \label{tab:xct_models_details}
\end{table}


\subsection{Transfer Learning}

 In order to further demonstrate the flexibility and generality of our models, we report transfer learning experiments in Table~\ref{tab:transfer} for models that have been pre-trained using ImageNet-1k and finetuned for other datasets including CIFAR-10, CIFAR-100~\cite{Krizhevsky2009LearningML}, Flowers-102~\cite{Nilsback08}, Stanford Cars~\cite{Cars2013} and iNaturalist~\cite{Horn2019INaturalist}.  We observe that the \ours models provide competitive performance when compared to strong baselines like ViT-B, ViT-L, DeiT-B and EfficientNet-B7.

\begin{table}
    \centering
    \caption{
    \textbf{Evaluation on transfer learning.}
    }\vspace*{3pt}
    \scalebox{0.9}{
    \begin{tabular}{@{\ }c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\ }}
    \toprule
    Architecture              
        & {CIFAR}
        & {CIFAR}  
        & {Flowers102} 
        & {Cars} 
        & {iNat} 
        & {iNat} 
      \\
    \midrule
    EfficientNet-B7~\cite{tan2019efficientnet}  & \underline{98.9} & \textbf{91.7}  & \textbf{98.8} & \textbf{94.7} & \_ & \_  \\
    ViT-B/16~\cite{dosovitskiy2020image}   & 98.1 & 87.1 & 89.5 & \_   & \_   & \_   \\
    ViT-L/16~\cite{dosovitskiy2020image}  & 97.9 & 86.4 & 89.7 & \_   & \_   & \_   \\
    Deit-B/16~\cite{touvron2020deit} \alambic  & \textbf{99.1} & 91.3 & \textbf{98.8} & 92.9 & \underline{73.7} & \underline{78.4}  \\
    \midrule
    \ours-S24/16 \alambic  & \textbf{99.1} & 91.2 & 97.4 & 92.8 & 68.8 & 76.1 \\
    \ours-M24/16 \alambic & \textbf{99.1} & \underline{91.4} & 98.2 & 93.4 & 72.6 & 78.1   \\
    \ours-L24/16 \alambic & \textbf{99.1} & 91.3 & \underline{98.3} & \underline{93.7} & \textbf{75.6} & \textbf{79.3 }\\

    \bottomrule
    \end{tabular}}

    \label{tab:transfer}
\end{table}



\subsection{Image Retrieval}

\paragraph{Context of this study. } 
Vision-based retrieval tasks such as landmark or particular object retrieval have been dominated in the last years by  methods extracting features from high-resolution images. Traditionally, the image description was obtained as the aggregation of local descriptors, like in VLAD~\cite{jegou2012aggregating}. Most of the modern methods now rely on convolutional neural networks~\cite{berman2019multigrain,Gordo2017EndtoEndLO,tolias2016particular}. 
In a recent paper, El-Nouby \emph{et al.}~\cite{el2021training} show promising results with vision transformers, however they also underline the inherent scalability limitation associated with the fact that ViT models do not scale well with image resolution. Therefore, it  cannot compete with convolutional neural networks whose performance readily improve with higher resolution images. Our \OURS models do not suffer from this limitation: our models scale linearly with the number of pixels, like convnets, and therefore makes it possible to use off-the-shelf methods initially developed for retrieval with high-resolution images. 



\subsubsection{Datasets and evaluation measure} 
In each benchmark, a set of query images is searched in a database of images and the performance is measured as the mean average precision. 

The Holidays~\cite{Jegou2008HammingEA} dataset contains images of 500 different objects or scenes. 
We use the version of the dataset where the orientation of images (portrait or landscape) has been corrected.
Oxford~\cite{Philbin07} is a dataset of building images, which corresponds to famous landmark in Oxford. A similar dataset has been produced for famous monuments in Paris and referred to as Paris6k~\cite{chum2007total}. 

\begin{table}[h!]
\begin{center}
\caption{\textbf{The basic statistics on the image retrieval datasets.}}\vspace*{3pt}
\scalebox{0.9}{\begin{tabular}{l|rccc}
    \toprule
         & \multicolumn{2}{c}{number of images} & \multirow{2}{*}{nb of instances}  \\
                 \cmidrule(lr){2-3}
         Dataset & database & queries &  &   \\
    \midrule
    Holidays     & 1491 & 500 & 500 \\
    R-Oxford     & 4993 & 70 & 26  \\
    \bottomrule
    \end{tabular}
}
\end{center}
\end{table}

We use the revisited version of the Oxford benchmark~\cite{radenovic2018revisiting}, which breaks down the evaluation into easy, medium and hard categories. 
We report results on the "medium" and "hard" settings, as we observed that the ordering of techniques does not change under the easy measures.



\subsubsection{Image representation: global and local description with \OURS} 

We consider three existing methods to extract an image vector representations from the pre-trained \OURS  models. 
Note that to the best of our knowledge, for the first time we extract local features from the output layer of a transformer layer, and treat them as patches fed to traditional state-of-the-art methods based on matching local descriptors or CNN. 
\paragraph{CLS token.} Similar to \citet{el2021training} with ViT, we use the final vector as the image descriptor. In this context, the introduction of class-attention layers can be regarded as a way to learn the aggregation method. 

\paragraph{VLAD. } We treat the patches before the class-attention layers as individual local descriptors, and aggregate them into a higher-dimensional vector by employing the Vector of locally aggregated Descriptors~\cite{jegou2012aggregating}. 

\paragraph{AMSK. } We also apply the aggregated selective match kernel from Tolias \emph{et al.}~\cite{tolias2016image}. This method was originally introduced for local descriptors, but got adapted to convolutional networks. To the best of our knowledge this is the state of the art on several benchmarks~\cite{tolias2020learning}.  
\medskip

For all these methods, we use the models presented in our main paper, starting from the version fine-tuned at resolution 384384. By default  the resolution is 768. This is comparable to the choice adopted in the literature for ResNet (e.g., 800 in the work by Berman et al.~\cite{berman2019multigrain}). 





\begin{table}[t]
\begin{center}
\caption{\label{tab:instance}
    \textbf{Instance retrieval experiments.} The default resolution is 768. The default class token size is 128 dimensions. The "local descriptor" representation extracted from the activations is in 128 dimensions. To our knowledge the state of the art with ResNet-50 on Holidays with Imagenet pre-training only is the Multigrain method~\cite{berman2019multigrain}, which achieves mAP=92.5\%. Here we compare against this method under the same training setting, i.e., off-the-shelf network pre-trained on Imagenet1k only and with the same training procedure and resolution. We refer the reader to Tolias \emph{et al.} \cite{tolias2020learning} for the state of the art on {}Oxford, which involves some training on the target domain with images depicted building and fine-tuning at the target resolution. 
    \label{tab:imageretrieval} 
}\vspace*{3pt}
\scalebox{0.9}{\begin{tabular}{l|c|ccc}
\toprule
Base model & parameters & \multicolumn{2}{c}{ROxford5k (mAP)} & Holidays (mAP) \\
\cmidrule{3-4}
 & &  Medium & Hard &   \\
\midrule
\multicolumn{5}{c}{\bf \ours -- class token} \\
\midrule
\ours-S12/16  &                & 30.1 &   8.7    &  86.0     \\
\ours-S12/8   &                & 33.2 & 12.1\pzo &  86.4    \\
\midrule  
\ours-S12/16  & resolution 224       & 12.7 &   2.4    &  71.5 \\
\ours-S12/16  & resolution 384       & 20.1 &   4.6    &  83.4 \\
\ours-S12/16  & resolution 512       & 26.6 &   5.8    &  84.6 \\
\ours-S12/16  & resolution 768       & 30.1 &   8.7    &  86.0 \\
\ours-S12/16  & resolution 1024      & 30.3 & 11.2\pzo &  86.3 \\
\midrule  
\ours-S12/16  & self-supervised DINO & 35.1 & 11.9\pzo &  87.3  \\
\ours-S12/8   & self-supervised DINO & 30.9 &   7.9    &  88.3   \\
\midrule   
\multicolumn{5}{c}{\bf \ours --  VLAD} \\  
\midrule  
\ours-S12/16  & k=256          & 36.6 & 11.6\pzo &  89.9        \\
\ours-S12/16  & k=1024         & 40.0 & 13.0\pzo &  90.7        \\
\midrule
\multicolumn{5}{c}{\bf \ours --  ASMK}\\
\midrule
\ours-S12/8  & k=1024          & 36.5 &   9.4    &  90.4        \\
\ours-S12/8  & k=65536         & 42.0 & 12.9\pzo &  92.3        \\
\ours-S12/16 & k=1024          & 35.2 & 11.5\pzo &  90.4        \\
\ours-S12/16 & k=65536         & 40.0 & 15.0\pzo &  92.0  \\
\midrule
\multicolumn{5}{c}{\bf ResNet-50 --  ASMK}\\
\midrule
Resnet50  & k=1024             & 41.6 &  14.6\pzo   &  86.0    \\
Resnet50  & k=65536            & 41.9 &  14.5\pzo   &  87.9 \\
Multigrain-resnet50 & k=1024   & 32.9 &  9.4        &  87.9 \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}




\subsubsection{Experimental setting: Image retrieval with models pretrained on Imagenet1k only}

We only consider models pre-trained on Imagenet-1k. Note that the literature reports significant improvement when learning or fine-tuning networks~\cite{radenovic2018fine,tolias2020learning}  on specialized datasets (e.g., of buildings for Oxford5k and Paris6k). We consider only \ours-S12 models, since they have a  number of  parameters comparable to that of ResNet-50. 
We report the results in Table~\ref{tab:imageretrieval}. 

\paragraph{Scaling resolution.} As expected increasing the resolution with \ours improves the performance steadily up to resolution 768. This shows that our models are very tolerant to resolution changes considering that they have been fine-tuned at resolution 384. 
The performance starts to saturates at resolution 1024, which led us to keep 784 as the pivot resolution. 

\paragraph{Self-supervision.} The networks \ours pre-trained with self-supervision achieve a comparatively better performance than their supervised counterpart on Holidays, however, we have the opposite observation for {}Oxford. 

\paragraph{Impact of Image description. } We adopt the class-token as the descriptor, and in our experiments we verified that this aggregation method is better than average and GeM pooling \cite{Boureau2010ATA,radenovic2018fine}. In Table~\ref{tab:imageretrieval} one can see there is a large benefit in employing a patch based method along with our \ours transformers: \OURS-VLAD performs significantly better than the CLS token, likely thanks to the higher dimensionality. This is further magnified with AMSK, where we obtain results approaching the absolute state of the art on Holidays, despite a sub-optimal training setting for image retrieval. This is interesting since our method has not been fine-tuned for retrieval tasks and we have not been adapted in any significant way beyond applying off-the-shelf this aggregation technique. 
A direct comparison with ResNet-50 shows that our \ours method obtains competitive results in this comparable setting, slightly below the ResNet-50 on {}Oxford but significantly better on Holidays. 

\nocite{thomee2016yfcc100m} 








\subsection{Runtime and Memory Usage}

We present the peak memory usage as well as the throughput of multiple models including full-attention and efficient vision transformers in Table~\ref{tab:peakmem}. Additionally, in Figure~\ref{fig:throughput} we plot the processing speed represented as millisecond per image as a function of image resolution for various models. We can observe that \ours provides a strong trade-off, possessing the best scalability in terms of peak memory, even when compared to ResNet-50. Additionally, the processing time scales linearly with respect to resolution, with only ResNet-50 providing a better trade-off on that front.


\begin{figure}[htb]
\centering
\includegraphics[trim=0 0 0 0, clip, scale=0.3]{figs/ims_xct.pdf}
\scriptsize
\caption{\textbf{We present the millisecond per image during inference of multiple models.} Our \ours-S12/16 model provides a speed up for images with higher resolution compared to existing vision transformers, especially the ones with quadratic complexity like DeiT and CaiT.
\label{fig:throughput}}
\end{figure}


\begin{table}[ht]
    \centering
    \caption{\footnotesize \textbf{Inference throughput and peak GPU memory usage} for our \OURS small model compared to other models of comparable size that include token self-attention. All models tested using batch size of 64 on a V100 GPU with 32GB memory.  \label{tab:peakmem}}\vspace*{3pt}
    \scalebox{0.74}{
    \begin{tabular}{l | c | c |c | c| c | c| c | c | c | c}
         \toprule
         Model & \#params  & ImNet & \multicolumn{8}{c}{Image Resolution}  \\
         \cmidrule{4-11}
          & () & Top-1 & \multicolumn{2}{c|}{224} & \multicolumn{2}{c|}{384} & \multicolumn{2}{c|}{512} & \multicolumn{2}{c}{1024} \\
         \cmidrule{4-11}
         & & @224 & im/sec & mem (MB) & im/sec & mem (MB) & im/sec & mem (MB) & im/sec & mem (MB) \\
         \midrule
         ResNet-50        & 25 & 79.0 & 1171 & 772      & 434 & 2078    & 245    & 3618 & 61 & 14178 \\ 
         \midrule
         DeiT-S           & 22 & 79.9 & 974 & \pzo433   & 263 & 1580    & 116    & 4020 & N/A & OOM \\
         CaiT-S12         & 26 & 80.8 & 671 & \pzo577   & 108 & 2581    & \pzo38 & 7117 & N/A & OOM \\
         PVT-Small        & 25 & 79.8 & 777 & 1266      & 256 & 3142    & 134    & 5354 & N/A & OOM \\
         Swin-T           & 29 & 81.3 & 704 & 1386      & 220 & 3890    & 120    & 6873 & 29 & 26915 \\
         \rowcolor{Goldenrod} \OURS-S12/16\!\! & 26 & 82.0 & 781 & \pzo731 & 266 & 1372    & 151    & 2128 & 37 & \pzo7312  \\
         \bottomrule
    \end{tabular}}
   
\end{table}






\subsection{Queries and Keys magnitude visualizations}

\begin{figure}[htb]
    \centering
    \vspace{0pt}
    \setlength\tabcolsep{0pt} \renewcommand{\arraystretch}{0.5}
    \scalebox{1.4}{\tiny 
\begin{tabular}{@{\hspace{-0pt}}l@{\hspace{2pt}}c@{\hspace{2pt}}c@{}}
            &  &  \3pt]
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/bike_lady_censored.jpg} &
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/bike_lady_L11_Q_censored.jpg} &
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/bike_lady_L11_K_censored.jpg} \\ 
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/dog_ball.jpg} &
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/dog_ball_L11_Q.jpg} &
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/dog_ball_L11_K.jpg} \\
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/tennis.jpg} &
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/tennis_L11_Q.jpg} &
          \includegraphics[width=0.18\linewidth]{figs/l2_vis/tennis_L11_K.jpg} \\
        \end{tabular}
        }\captionof{figure}{
    \textbf{Visualization of the queries  and keys  norm across the feature dimension.} We empirically observe that magnitude of patch embeddings in the queries and keys correlates with the saliency of their corresponding region in the image. 
    }
    \label{fig:l2_vis}
\end{figure}

Our XCA operation relies on the cross-covariance matrix of the queries  and keys  which are  normalized across the patch dimension. Therefore, each element in the  matrix represents a cosine similarity whose value is strongly influenced by the magnitude of each patch. In Figure~\ref{fig:l2_vis} we visualize the magnitude of patch embeddings in the queries and keys matrices. We observe that patch embeddings with higher magnitude corresponds to more salient regions in the image, providing a very cheap visualization and interpretation of which regions in the image contribute more in the cross-covariance attention. 
