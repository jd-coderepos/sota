
\section{Experiments}
\label{sec:experiments}


\subsection{Datasets}
We experiment on three citation graph datasets: Pubmed, Citeseer, Cora, and a biological graph: Protein-Protein Interactions (PPI).
We choose the aforementioned datasets because they are available online and are used by our baselines. The citation datasets are prepared by \cite{planetoid}, and the PPI dataset is prepared by \cite{sage}. Table \ref{table:dataset-stats} summarizes dataset statistics.



Each node in the citation datasets represents an article published in the corresponding journal. An edge between two nodes represents a citation from one article to another, and a label represents the subject of the article. Each dataset contains a binary Bag-of-Words (BoW) feature vector for each node. The BoW are extracted from the article abstract. Therefore, the task is to predict the subject of articles, given the BoW of their abstract and the citations to other (possibly labeled) articles. Following \cite{planetoid} and \cite{kipf}, we use 20 nodes per class for training, 500 (overall) nodes for validation, and 1000 nodes for evaluation. We note that the validation set is larger than training $|\mathcal{V}_L|$ for these datasets!

The PPI graph, as processed and described by \cite{sage}, consists of 24 disjoint subgraphs, each corresponding to a different human tissue. 20 of those subgraphs are used for training, 2 for validation, and 2 for testing, as partitioned by \cite{sage}.

\subsection{Baseline Methods}

For the citation datasets, we copy baseline numbers from \cite{kipf}. These include label propagation (LP, \cite{lp}); semi-supervised embedding (SemiEmb, \cite{semiemb}); manifold regularization (ManiReg, \cite{manireg}); skip-gram graph embeddings \citep[DeepWalk][]{deepwalk}; Iterative Classification Algorithm \citep[ICA, ][]{ica}; Planetoid \citep{planetoid}; vanilla GCN \citep{kipf}. For PPI, we copy baseline numbers from \citep{sage}, which include GraphSAGE with LSTM aggregation (SAGE-LSTM) and GraphSAGE with pooling aggregation (SAGE). Further, for all datasets, we use our implementation to run baselines DCNN \citep{diffusion-cnn}, GCN \citep{kipf}, and SAGE \citep[with pooling aggregation, ][]{sage}, as these baselines can be recovered as special cases of our algorithm, as explained in Section \ref{sec:nmodel}.

\subsection{Implementation}
We use TensorFlow\citep{tensorflow} to implement our methods, which we use to also measure the performance of baselines GCN, SAGE, and DCNN.
For our methods and baselines, all GCN and SAGE modules that we train are 2 layers\footnote{except as clearly indicated in Table \ref{table:deepgcn}}, where the first outputs 16 dimensions per node and the second outputs the number of classes (dataset-dependent). DCNN baseline has one layer and outputs 16 dimensions per node, and its channels (one per transition matrix power) are concatenated into a fully-connected layer that outputs the number of classes.
We use $50\%$ dropout and L2 regularization of $10^{-5}$ for all of the aforementioned models.








\begin{table*}[t]
\begin{center}
\begin{tabular}{lcccccc}
\textbf{Dataset} 
&\textbf{Type}
& \textbf{Nodes}
&\textbf{Edges}
&\textbf{Classes}
& \textbf{Features}
& \textbf{Labeled nodes}
\\
& & $|\mathcal{V}|$ & $|\mathcal{E}|$ & $C$ & $F$ & $|\mathcal{V}_L|$
\\ \hline
Citeseer    & citaction      &3,327 &4,732 &6  (single class)&3,703 & 120 \\
Cora          & citaction   &2,708 &5,429 &7 (single class) &1,433 & 140 \\
Pubmed     & citaction         &19,717 &44,338 &3 (single class)&500 & 60 \\
PPI           & biological   & 56,944&  818,716   & 121 (multi-class) & 50 & 44,906 \\
\end{tabular}
\end{center}
\caption{Dataset used for experiments.
	For citation datasets, 20 training nodes per class are observed, with $|\mathcal{V}_L| = 20 \times C$
}
\label{table:dataset-stats}
\end{table*}


\begin{table*}[t]
\begin{center}
\begin{tabular}{rlcccc}
& \textbf{Method}  & \textbf{Citeseer} & \textbf{Cora} & \textbf{Pubmed} & \textbf{PPI} \\ \hline
(a) & ManiReg \citep{manireg}& $60.1$ & $59.5$ & $70.7$ & --\\ 

(b) & SemiEmb \citep{semiemb}& $59.6$ & $59.0$ & $71.1$ & --\\ 

(c) & LP \citep{lp}& $45.3$ & $68.0$ & $63.0$ & --\\ 

(d) & DeepWalk \citep{deepwalk}& $43.2$ & $67.2$ & $65.3$ & --\\ 

(e) & ICA \citep{ica}& $69.1$ & $75.1$ & $73.9$ & --\\ 

(f) & Planetoid \citep{planetoid}& $64.7$ & $75.7$ & $77.2$ & --\\ 

(g) & GCN \citep{kipf}& $70.3$ & $81.5$ & $79.0$ & --\\ 

(h) & SAGE-LSTM \citep{sage}& --& --& --& $61.2$ \\ 

(i) & SAGE \citep{sage}& --& --& --& $60.0$ \\ 

\hline
(j) & DCNN (our implementation)& $71.1$ & $81.3$ & $79.3$ & $44.0$ \\ 

(k) & GCN (our implementation)& $71.2$ & $81.0$ & $78.8$ & $46.2$ \\ 

(l) & SAGE (our implementation)& $63.5$ & $77.4$ & $77.6$ & $59.8$ \\ 

\hline
(m) & $\textrm{N-GCN}$ (ours)& $\mathbf{72.2}$ & $\mathbf{83.0}$ & $\mathbf{79.5}$ & $46.8$ \\ 

(n) & $\textrm{N-SAGE}$ (ours)& $71.0$ & $81.8$ & $79.4$ & $\mathbf{65.0}$ \\ 

\end{tabular}
 \end{center}
\caption{
Node classification performance ($\%$ accuracy for the first three, citation datasets, and f1 micro-averaged for multiclass PPI), using data splits of \cite{planetoid, kipf} and \cite{sage}.
We report the test accuracy corresponding to the run with the highest validation accuracy.
Results in rows (a) through (g) are copied from \cite{kipf}, rows (h) and (i) from \citep{sage}, and (j) through (l) are generated using our code since we can recover other algorithms as explained in Section \ref{sec:nmodel}. Rows (m) and (n) are our models.
Entries with ``--'' indicate that authors from whom we copied results did not run on those datasets. Nonetheless, we run all datasets using our implementation of the most-competitive baselines.
}
\label{table:results}
\end{table*}

\begin{figure*}[t]
	\centering
\begin{subfigure}{0.3\linewidth}
	\makebox[\textwidth]{
		\includegraphics[trim={0cm 0 0 0},clip,width=\linewidth]{sweeps/plot2d_citeseer_gcn_fc_1.pdf}
	}
	\caption{N-GCN on Citeseer}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
	\makebox[\textwidth]{
		\includegraphics[trim={0cm 0 0 0},clip,width=\linewidth]{sweeps/plot2d_cora_gcn_fc_1.pdf}
	}
	\caption{N-GCN on Cora}
\end{subfigure}
	\begin{subfigure}{0.3\linewidth}
	\makebox[\textwidth]{
		\includegraphics[trim={0cm 0 0 0},clip,width=\linewidth]{sweeps/plot2d_pubmed_gcn_fc_1.pdf}
	}
	\caption{N-GCN on Pubmed}
\end{subfigure}

\begin{subfigure}{0.3\linewidth}
	\makebox[\textwidth]{
\includegraphics[trim={0cm 0 0 0},clip,width=\linewidth]{sweeps/plot2d_citeseer_sage_fc_1.pdf}
	}
	\caption{N-SAGE on Citeseer}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
	\makebox[\textwidth]{
\includegraphics[trim={0cm 0 0 0},clip,width=\linewidth]{sweeps/plot2d_cora_sage_fc_1.pdf}
	}
	\caption{N-SAGE on Cora}
	\end{subfigure}
	\begin{subfigure}{0.3\linewidth}
		\makebox[\textwidth]{
\includegraphics[trim={0cm 0 0 0},clip,width=\linewidth]{sweeps/plot2d_pubmed_sage_a_1.pdf}
		}
		\caption{N-SAGE on Pubmed}
\end{subfigure}
\caption{Sensitivity Analysis. Model performance when varying random walk steps $K$ and replication factor $r$. Best viewed with zoom. Overall, model performance increases with larger values of $K$ and $r$. In addition, having random walk steps (larger $K$) boosts performance more than increasing model capacity (larger $r$).}
	\label{fig:sensitivity}
\end{figure*}





\begin{table*}[t]
	\begin{center}
\begin{tabular}{rcccc}
 \textbf{Nodes per class} & 5 & 10 & 20 & 100 \\
\hline
DCNN (our implementation) & $63.0 \pm 1.0$ & $72.3 \pm 0.4$ & $79.2 \pm 0.2$ & $82.6 \pm 0.3$ \\
GCN (our implementation) & $64.6 \pm 0.3$ & $70.0 \pm 3.7$ & $79.1 \pm 0.3$ & $81.8 \pm 0.3$ \\
SAGE (our implementation) & $69.0 \pm 1.4$ & $72.0 \pm 1.3$ & $77.2 \pm 0.5$ & $80.7 \pm 0.7$ \\
\hline 

$\textrm{N-GCN}_\textrm{a}$ (ours) & $65.1 \pm 0.7$ & $71.2 \pm 1.1$ & $\mathbf{79.7} \pm 0.3$ & $\mathbf{83.0} \pm 0.4$ \\
$\textsc{N-GCN}_\textrm{fc}$ (ours) & $65.0 \pm 2.1$ & $71.7 \pm 0.7$ & $\mathbf{79.7} \pm 0.4$ & $82.9 \pm 0.3$ \\
$\textrm{N-SAGE}_\textrm{a}$ (ours) & $66.9 \pm 0.4$ & $73.4 \pm 0.7$ & $79.0 \pm 0.3$ & $82.5 \pm 0.2$ \\
$\textrm{N-SAGE}_\textrm{fc}$ (ours) & $\mathbf{70.7} \pm 0.4$ & $\mathbf{74.1} \pm 0.8$ & $78.5 \pm 1.0$ & $81.8 \pm 0.3$ \\
\end{tabular}
 \end{center}
\caption{Node classification accuracy (in $\%$) for our largest dataset (Pubmed) as we vary size of training data $\frac{|\mathcal{V}|}{C} \in \{5, 10, 20, 100\}$. We report mean and standard deviations on 10 runs.
We use a different random seed for every run (i.e. selecting different labeled nodes), but the same 10 random seeds across models.
Convolution-based methods (e.g. SAGE) work well with few training examples, but \textit{unmodified} random walk methods (e.g. DCNN) work well with more training data. Our methods combine convolution and random walks, making them work well in both conditions. 
}
\label{table:npc}
\end{table*}

\begin{figure*}[t]
	\centering
	\includegraphics[trim={0 0.5cm 0 0},clip,width=\linewidth]{feat_r_plot.pdf}
	\caption{
		Classification accuracy for the Cora dataset with 20 labeled nodes per class $(|\mathcal{V}| = 20\times C)$, but features removed at random, averaging 10 runs.
		We use a different random seed for every run (i.e. removing different features per node), but the same 10 random seeds across models.
	}
	\label{fig:featremoval}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[trim={0 0 0 0cm},clip,width=\linewidth]{feat_r_attention_gcn.pdf}
\caption{
	Attention weights ($m$) for $\textrm{N-GCN}_\textrm{a}$ 
when trained with feature removal perturbation on the Cora dataset. Removing features shifts the attention weights to the right, suggesting the model is relying more on long range dependencies.
	}
	\label{fig:attentionfeatremoval}
\end{figure*}


\subsection{Node Classification Accuracy}
Table \ref{table:results} shows node classification accuracy results.
We run 20 different random initializations for every model (baselines and ours), train using Adam optimizer \citep{adam} with learning rate of 0.01 for 600 steps, capturing the model parameters at peak validation accuracy to avoid overfitting.
For our models, we sweep our hyperparameters $r$, $K$, and choice of classification sub-network $\in \{\textrm{fc}, \textrm{a}\}$. For baselines and our models, we choose the model with the highest accuracy on validation set, and use it to record metrics on the test set in Table \ref{table:results}.

Table \ref{table:results} shows that N-GCN outperforms GCN \citep{kipf} and N-SAGE improves on SAGE for all datasets, showing that \textit{unmodified} random walks indeed help in semi-supervised node classification. Finally, our proposed models acheive state-of-the-art on all datasets.


\subsection{Sensitivity Analysis}
We analyze the impact of random walk length $K$ and replication factor $r$ on classification accuracy in Figure \ref{fig:sensitivity}. In general, model performance improves when increasing $K$ and $r$. We note utilizing random walks by setting $K>1$ improves model accuracy due to the additional information, not due to increased model capacity: Contrast $K=1, r>1$ (i.e. mixture of GCNs, no random walks) with $K>1, r=1$ (i.e. N-GCN on random walks) -- in both scenarios, the model has more capacity, but the latter shows better performance. The same holds for SAGE.


\subsection{Tolerance to feature noise}
We test our method under feature noise perturbations by removing node features at random. This is practical, as article authors might forget to include relevant terms in the article abstract,
and more generally not all nodes will have the same amount of detailed information.
Figure \ref{fig:featremoval} shows that when features are removed, methods utilizing unmodified random walks: N-GCN, N-SAGE, and DCNN, outperform convolutional methods including GCN and SAGE. Moreover, the performance gap widens as we remove more features. This suggests that our methods can somewhat recover removed features by \textit{directly} pulling-in features from nearby and distant neighbors.
We visualize in Figure \ref{fig:attentionfeatremoval} the attention weights as a function of \% features removed. With little feature removal, there is some weight on $\hat{A}^{0}$, and the attention weights for $\hat{A}^{1}, \hat{A}^{2}, \dots$ follow some decay function. Maliciously dropping features causes our model to shift its attention weights towards higher powers of $\hat{A}$.


\subsection{Random Walk Steps Versus GCN Depth}

$K$-step random walk will allow every node to accumulate information from its neighbors, up to distance $K$. Similarly, a $K$-layer GCN \citep{kipf} will do the same. The difference between the two was mathematically explained in Section \ref{sec:motivation}. To summarize: the former averages node feature vectors according to the random walk co-visit statistics, whereas the latter creates non-linearities and matrix multiplies at every step. So far, we displayed experiments where our models (N-GCN and N-SAGE) were able to use information from distant nodes (e.g. $K=5$), but for all GCN and SAGE modules, we used 2 GCN layer for baselines and our models.

Even though the authors of GCN \citep{kipf} and SAGE \citep{sage} suggest using two GCN layers, according by holdout validation, for a fair comparison with our models, we run experiments utilizing deeper GCN and SAGE are models so that its ``receptive field'' is comparable to ours. 

\begin{table}
	\begin{center}
		\begin{tabular}{c c  c c c}
 &  & \multicolumn{3}{}{Graph Conv Layer Dimensions} \\
Dataset & Model & $64\times C$&$64\times64\times C$&$64\times64\times64\times C$\\
\hline
Citeseer & GCN & 0.699&0.632&0.659 \\
Citeseer & SAGE & 0.668&0.660&0.674 \\
Cora & GCN & 0.803&0.800&0.780 \\
Cora & SAGE & 0.761&0.763&0.757 \\
Pubmed & GCN & 0.762&0.771&0.781 \\
Pubmed & SAGE & 0.770&0.776&0.775 \\
PPI & GCN & 0.460&0.461&0.466 \\
PPI & SAGE & 0.658&0.672&0.650 \\
\end{tabular}
 	\end{center}
  \caption{Performance of deeper GCN and SAGE models, both using our implementation. Deeper GCN (or SAGE) does not consistently improve classification accuracy, suggesting that N-GCN and N-SAGE are more performant and are easier to train. They use shallower convolution models that operate on multiple scales of the graph.}
  \label{table:deepgcn}
\end{table}

Table \ref{table:deepgcn} shows test accuracies when training deeper GCN and SAGE models, using our implementation. We notice that, unlike our method which benefits from a wider ``receptive field'', there is no direct correspondence between depth and improved performance.