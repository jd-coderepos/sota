\section{Experiments}
In this section, we report our experimental results.
We start with describing the experimental setups (\Cref{sec:setting}). Next, we separately present the experimentation under two different evaluation settings (\Cref{sec:relaxed}-\Cref{sec:consolidated}), followed by an analysis on the similarity score distribution of the obtained representations for matchable and dangling entities (\Cref{sect:viz}).
To faciliate the use of the contributed dataset and software, we have incorporated these resources into the OpenEA benchmark\footnote{\url{https://github.com/nju-websoft/OpenEA}}~\cite{OpenEA}.

\begin{table*}[!t]
	\centering
{\small
	\setlength{\tabcolsep}{1pt}
		\begin{tabular}{lcccccccccccccccccc}
			\toprule
			\multirow{2}{*}{Methods} &
			\multicolumn{3}{c}{ZH-EN} & \multicolumn{3}{c}{EN-ZH} & \multicolumn{3}{c}{JA-EN} & \multicolumn{3}{c}{EN-JA} & \multicolumn{3}{c}{FR-EN} & \multicolumn{3}{c}{EN-FR}\\
			\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16} \cmidrule(lr){17-19}
			& H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR \\ 
			\midrule
			MTransE & {.358} & {.675} & {.463} & .353 & {.670} & {.461} & {.348} & {.661} & {.453} & .342 & {.670} & .452 & {.245} & {.524} & {.338} & {.247} & {.531} & {.342} \\
\;\;w/ NNC & .350 & .668 & .457 & .356 & .664 & .460 & .340 & .657 & .441 & .336 & .630 & .445 & .253 & .539 & .343 & .251 & .536 & .343 \\
\;\;w/ MR & \textbf{.378} & \textbf{.693} & \textbf{.487} & \textbf{.383}& \textbf{.699} & \textbf{.491} & \textbf{.373} & \textbf{.686} & \textbf{.476} & \textbf{.374} & \textbf{.707} & .\textbf{485} & \textbf{.259} & \textbf{.541} & \textbf{.348} & \textbf{.265} & \textbf{.553} & \textbf{.360} \\
\;\;w/ BR & .360 & .678 & .468 & .357 & .675 & .465 & .344 & .660 & .451 & .346 & .675 & .456 & .251 & .525 & .342 & .249 & .531 & .343 \\
			\midrule
			AliNet & .332 & .594 & .421 & {.359} & .629 & .451 & .338 & .596 & .429 & {.363} & .630 & {.455} & .223 & .473 & .306 & .246 & .495 & .329 \\
\;\;w/ NNC & .321 & .598 & .415 & .335 & .608 & .428 & .330 & .602 & .422 & .344 & .627 & .439 & .212 & .467 & .294 & .230 & .476 & .312 \\
\;\;w/ MR & \textbf{.343} & \textbf{.606} & \textbf{.433} & \textbf{.364} & \textbf{.637} & \textbf{.459} & \textbf{.349} & \textbf{.608} & \textbf{.438} & \textbf{.377} & \textbf{.646} & \textbf{.469} & \textbf{.230} & \textbf{.477} & \textbf{.312} & \textbf{.252} & \textbf{.502} & \textbf{.335}\\
\;\;w/ BR & .333 & .599 & .426 & .357 & .632 & .451 & .341 & \textbf{.608} & .431 & .369 & .636 & .461 & .214 & .468 & .298 & .238 & .487 & .321 \\
			\bottomrule
	\end{tabular}}
	\caption{Entity alignment results (relaxed setting) of MTransE and AliNet on \dataset.}
	\label{tab:synthetic_ent_alignment}
\end{table*}

\subsection{Experimental Settings}\label{sec:setting}
We consider two evaluation settings. 
One setting is for the proposed problem setting with dangling entities, for which we refer as the \emph{consolidated evaluation setting}. 
We first detect and remove the dangling source entities and then search alignment for the left entities.
For this evaluation setting, we also separately assess the performance of the dangling detection module.
The other simplified setting follows that in previous studies \cite{JAPE,OpenEA} where the source entities in test set all have counterparts in the target KG, so no dangling source entities are considered. 
In this \emph{relaxed evaluation setting}, we seek to evaluate the effect of dangling entity detection on entity alignment and make our results comparable to previous work. 

\stitle{Evaluation Protocol}
For the \emph{relaxed evaluation setting}, given each source entity, the candidate counterpart list is selected via NN search in the embedding space. 
The widely-used metrics on the ranking lists are Hits@ (, H@ for short) and mean reciprocal rank (MRR).
Higher H@ and MRR indicate better performance.

For the \emph{consolidated setting},
we report precision, recall and F1 for dangling entity detection.
As for assessing the eventual performance of realistic entity alignment, since the dangling entity detection may not be perfect,
it is inevitable for some dangling entities to be incorrectly sent to the entity alignment module for aligning, while some matchable ones may be wrongly excluded.
In this case, H@ and MRR are not applicable for the consolidated entity alignment evaluation. 
Following a relevant evaluation setting for entity resolution in database \cite{DL4ER,EmbedER}, we also use precision, recall and F1 as metrics.
More specifically, if a source entity is dangling and is not identified by the detection module, the prediction is always regarded as incorrect. 
Similarly, if a matchable entity is falsely excluded by the dangling detection module, this test case is also regarded as incorrect since the alignment model has no chance to search for alignment. 
Otherwise, the alignment module searches for the NN of a source entity in the target embedding space and assesses if the predicated counterpart is correct. 

\begin{figure}[t]
	\centering
	\includegraphics[width=.96\linewidth]{figs/data_comp.pdf}
	\caption{Average neighbor overlap ratio of aligned entities in DBP15K and our \dataset.}
	\label{fig:data_comp}
\end{figure}

\begin{table*}[!t]
	\centering
{
	\small
	\setlength{\tabcolsep}{3pt}
		\begin{tabular}{llcccccccccccccccccc}
			\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Methods}} &
			\multicolumn{3}{c}{ZH-EN} & \multicolumn{3}{c}{EN-ZH} & \multicolumn{3}{c}{JA-EN} & \multicolumn{3}{c}{EN-JA} &  \multicolumn{3}{c}{FR-EN} & \multicolumn{3}{c}{EN-FR}\\
			\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
			&& Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 \\ 
			\midrule
			\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{{\scriptsize MTransE}}}} 
			& NNC & .604 & .485 & .538 & .719 & .511 & .598 & .622 & .491 & .549 & .686 & .506 & .583 & .459 & .447 & .453 & .557 & .543 & .550 \\
			& MR & .781 & .702 & .740 & .866 & .675 & .759 & .799 & .708 & .751 & .864 & .653 & .744 & .482 & .575 & .524 & .639 & .613 & .625 \\
			& BR & \textbf{.811} & \textbf{.728} & \textbf{.767} & .\textbf{892} & \textbf{.700} & \textbf{.785} & \textbf{.816} & \textbf{.733} & \textbf{.772} & \textbf{.888} & \textbf{.731} & \textbf{.801} & \textbf{.539} & \textbf{.686} & \textbf{.604} & \textbf{.692} & \textbf{.735} & \textbf{.713} \\
			\midrule
			\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{{\small AliNet}}}}
			& NNC & .676 & .419 & .517 & .738 & .558 & .634 & .597 & .482 & .534 & .761 & .120 & .207 & .466 & .365 & .409 & .545 & .162 & .250 \\
			& MR & .752 & .538 & .627 & .828 & .505 & .627 & .779 & .580 & .665 & \textbf{.854} & .543 & \textbf{.664} & \textbf{.552} & \textbf{.570} & \textbf{.561} & \textbf{.686} & .549 & \textbf{.609} \\
			& BR & \textbf{.762} & \textbf{.556} & \textbf{.643} & \textbf{.829} & \textbf{.515} & \textbf{.635} & \textbf{.783} & \textbf{.591} & \textbf{.673} & .846 & \textbf{.546} & .663 & .547 & .556 & .552 & .674 & \textbf{.556} & \textbf{.609} \\
			\bottomrule
	\end{tabular}}
	\caption{Dangling entity detection results on \dataset.}
	\label{tab:detection}
\end{table*}

\stitle{Model Configuration}\label{sec:config}
As described in \Cref{sec:dangling}, our dangling detection module has three variants, i.e., NN classification (NNC), marginal ranking (MR), and background ranking (BR). 
We report the implementation details of the entity alignment module (w/ MTransE or AliNet) in \Cref{appendix:config,appendix:setting}.
We initialize KG embeddings and model parameters using the Xavier initializer \cite{Xavier}, and use Adam \cite{Adam} to optimize the learning objectives with the learning rate  for MTransE and  for AliNet. 
Note that we do not follow some methods to initialize with machine translated entity name embeddings \cite{NMN_acl20}.
As being pointed out by recent studies \cite{JEANS,EVA,AttrGNN}, this is necessary to prevent test data leakage.
Entity similarity is measured by cross-domain similarity local scaling \cite{CSLS} for reduced hubness effects, as being consistent to recent studies \cite{AliNet,JEANS}.
We use a two-layer FFN in NNC. 
For MR, the margin is set as  for MTransE and  for AliNet.
BR randomly samples  target entities for each entity per epoch and .
Training is terminated based on F1 results of entity alignment on validation data.

\begin{table*}[!t]
	\centering
{
	\small
	\setlength{\tabcolsep}{3pt}
		\begin{tabular}{llcccccccccccccccccc}
			\toprule
			\multicolumn{2}{c}{\multirow{2}{*}{Methods}} &
			\multicolumn{3}{c}{ZH-EN} & \multicolumn{3}{c}{EN-ZH} & \multicolumn{3}{c}{JA-EN} & \multicolumn{3}{c}{EN-JA} &  \multicolumn{3}{c}{FR-EN} & \multicolumn{3}{c}{EN-FR}\\
			\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
			&& Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 & Prec. & Rec. & F1 \\ 
			\midrule
			\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{{\scriptsize MTransE}}}} 
			& NNC & .164 & .215 & .186 & .118 & .207 & .150 & .180 & .238 & .205 & .101 & .167 & .125 & .185 & .189 & .187 & .135 & .140 & .138 \\
			& MR & .302 & .349 & .324 & .231 & .362 & .282 & .313 & \textbf{.367} & \textbf{.338} & .227 & \textbf{.366} & .280 & .260 & \textbf{.220} & \textbf{.238} & .213 & \textbf{.224} & .218 \\
			& BR & \textbf{.312} & \textbf{.362} & \textbf{.335} & \textbf{.241} & \textbf{.376} & \textbf{.294} & \textbf{.314} & .363 & .336 & \textbf{.251} & .358 & \textbf{.295} & \textbf{.265} & .208 & .233 & \textbf{.231} & .213 & \textbf{.222} \\
			\midrule
			\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{{\small AliNet}}}} 
			& NNC & .121 & .193 & .149 & .085 & .138 & .105 & .113 & .146 & .127 & .067 & .208 & .101 & .126 & .148 & .136 & .086 & .161 & .112 \\
			& MR & \textbf{.207} & \textbf{.299} & \textbf{.245} & \textbf{.159} & \textbf{.320} & \textbf{.213} & \textbf{.231} & \textbf{.321} & \textbf{.269} & \textbf{.178} & \textbf{.340} & \textbf{.234} & \textbf{.195} & \textbf{.190} & \textbf{.193} & .160 & \textbf{.200} & .178 \\
			& BR & .203 & .286 & .238 & .155 & .308 & .207 & .223 & .306 & .258 & .170 & .321 & .222 & .183 & .181 & .182 & \textbf{.164} & \textbf{.200} & \textbf{.180} \\
			\bottomrule
	\end{tabular}}
	\caption{Entity alignment results on \dataset.}
	\label{tab:ent_alignment}
\end{table*}

\subsection{Relaxed Evaluation}\label{sec:relaxed}
We first present the evaluation under the relaxed entity alignment setting based on \Cref{tab:synthetic_ent_alignment}.
This setting only involves matchable source entities to test entity alignment,
which is an ideal (but less realistic) scenario similar to prior studies \cite{OpenEA}. 
We also examine if jointly learning to detect dangling entities can indirectly improve alignment.

As observed,
MTransE, even without dangling detection, can achieve promising performance on \dataset. 
The results are even better than those on DBP15K as reported by \citet{JAPE}.
We attribute this phenomenon to the robustness of this simple embedding method and our improved implementation (e.g., more effective negative sampling).
By contrast, although we have tried our best in tuning, the latest GNN-based AliNet falls behind MTransE.
Unlike MTransE that learns entity embeddings from a first-order perspective (i.e., based on triple plausibility scores), AliNet represents an entity from a high-order perspective by aggregating its neighbor embeddings, and entities with similar neighborhood structures would have similar representations.
However, the dangling entities in \dataset inevitably become spread noises in entity neighborhoods.
To further probe into this issue, we count the average neighbor overlap ratio of aligned entities in DBP15K and our \dataset. Given an entity alignment pair (), let  and  be the sets of their neighboring entities
respectively, where we also merge their aligned neighbors as one identity based on reference entity alignment. 
Then the neighbor overlap ratio of  and  is calculated as . 
We average such a ratio for both DBP15K and \dataset as given in \Cref{fig:data_comp}.
We can see that the three settings' overlap ratios in \dataset are all much lower than those in DBP15K.
Thus, \dataset poses additional challenges, as compared to DBP15K, specifically for those methods relying on neighborhood aggregation.
Based on results and analysis, we argue that methods performing well on the previous synthetic entity alignment dataset may not robustly generalize to the more realistic dataset with dangling cases.
The performance of both MTransE and AliNet is relatively worse on FR-EN, 
which has more entities (i.e., larger candidate search space) and a low neighborhood overlap ratio (therefore, more difficult to match entities based on neighborhood similarity).

Meanwhile,
we find that the dangling detection module can affect the performance of entity alignment.
In details, MR consistently leads to  improvement to both MTransE and AliNet. 
BR can also noticeably boost entity alignment on most settings.
This shows that learning to isolate dangling entities from matchable ones naturally provides indirect help to discriminate the counterpart of a matchable entity from irrelevant ones.
On the other hand, such indirect supervision signals may be consumed by the additional trainable parameters in NNC, causing its effect on entity alignment to be negligible. 
Overall, the observation here calls for more robust entity alignment methods and dangling detection techniques, and lead to further analysis (\Cref{sec:consolidated}).


\begin{figure}[t]
	\centering
	\includegraphics[width=.96\linewidth]{figs/accuracy.pdf}
	\caption{Accuracy of dangling entity detection.}
	\label{fig:accuracy}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=.96\linewidth]{figs/time.pdf}
	\caption{Average training time (seconds) of one epoch for dangling entity detection (MTransE variants).}
	\label{fig:time}
\end{figure}

\subsection{Consolidated Evaluation}\label{sec:consolidated}
We now report the experiment on the more realistic consolidated evaluation setting.
\Cref{tab:detection} gives the precision, recall and F1 results of dangling entity detection, and the final entity alignment performance is presented in \Cref{tab:ent_alignment}. 
In addition, \Cref{fig:accuracy} shows the accuracy of dangling entity detection.
We analyze the results from the following aspects.

\stitle{Dangling entity detection} 
Regardless of which alignment module is incorporated,
NNC performs the worst (e.g., the low recall and accuracy around ) 
among the dangling detection techniques, whereas BR generally performs the best. 
NNC determines whether an entity is dangling based on the difference vector of the entity embedding and its NN, instead of directly capturing the embedding distance which is observed to be more important based on the results by the other two techniques.
By directly pushing dangling entities away from their NNs in the embedding space, both MR and BR offer much better performance. 
Besides, BR outperforms MR in most cases. 
By carefully checking their prediction results and the actual distance of NNs, we find that the induced distance margin in BR better discriminates dangling entities from matchable ones than the pre-defined margin.

\stitle{Efficiency} 
We compare the average epoch time of training the three dangling detection modules for MTransE in \Cref{fig:time}. We conduct the experiment using a workstation with an Intel Xeon E5-1620 3.50GHz CPU and a NVIDIA GeForce RTX 2080 Ti GPU. 
Since NNC and MR need to search for NNs of source entities, both techniques spend much more training time 
that is saved by random sampling in BR. Overall, BR is an effective and efficient technique for dangling entity detection.

\stitle{Entity alignment}
Generally, for both MTransE and AliNet variants, MR and BR lead to better entity alignment results than NNC. 
MR and BR obtain higher precision and recall performance on detecting dangling entities as listed in \Cref{tab:detection}, resulting in less noise that enters the entity alignment stage.
By contrast, NNC has a low accuracy and thus introduces many noises.
As BR outperforms MR in dangling detection, it also achieves higher entity alignment results than MR on most settings.
We also notice that MR in a few settings, MR offer comparible or slightly better performance than BR.
This is because MR can enhance the learning of alignment modules (see \Cref{sec:relaxed} for detailed analysis), thus delivering improvement to the final performance. 
MTransE variants generally excels AliNet variants in both entity alignment (see \Cref{tab:synthetic_ent_alignment}) and dangling entity detection (see \Cref{tab:detection}) than AliNet, similar to the observation in \Cref{sec:relaxed}.

\stitle{Alignment direction} 
We find that the alignment direction makes a difference in both dangling entity detection and entity alignment. 
Using EN KG as the source is coupled with easier dangling detection than in other languages,
as the most populated EN KG contributes more dangling entities and triples to training than other KGs.
As for entity alignment, we find the observation to be quite the opposite, as using the EN KG as a source leads to noticeable drops in results.
For example, the precision of MTransE-BR is  on ZH-EN, but only  on EN-ZH.
This is because the EN KG has a larger portion of dangling entities. 
Although the dangling detection module performs well on the EN KG than on others, there are still much more dangling entities entering the alignment search stage, thus reducing the entity alignment precision.
This observation suggests that choosing the alignment direction from a less populated KG to the more populated EN KG can be a more effective solution.

\begin{figure}[t]
	\centering
	\includegraphics[width=.96\linewidth]{figs/histogram.pdf}
	\caption{Kernel density estimate plot of the test {\color{blue}{matchable}} and {\color{red}{dangling}} entities' similarity distribution with their nearest target neighbors in ZH-EN.}
	\label{fig:viz}
\end{figure}

\subsection{Similarity Score Distribution}\label{sect:viz}
To illustrate how well the BR technique distinguishes between matchable and dangling entities, we plot in \Cref{fig:viz} the distribution of similarity scores of each test entity and its NN.  
The plot illustrates BR has the expected effect to isolate dangling entities from their NNs, whereas matchable entities are generally placed closer to their NNs.
Yet, we can still see a modest overlap between the two NN similarity distributions of dangling and matchable entities, and a number of dangling entities still have a quite large NN similarity.
This also reveals the fact that the proposed problem setting of entity alignment with dangling cases has many remaining challenges that await further investigation.
