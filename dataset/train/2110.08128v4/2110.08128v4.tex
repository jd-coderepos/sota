


\appendix
\newpage
\begin{algorithm}[t]
\caption{ Training Algorithm of {\method}}
\label{alg:Framwork}
\begin{algorithmic}[1]
\REQUIRE
$\mathcal{G}=(\mathcal{V},\mathcal{E}, {X})$, $\mathcal{Y}_L$, $p$, $\alpha_C$, $\alpha_G$, $\alpha_\phi$ and $T$
\ENSURE $f_P$, $f_C$, $f_G$, $\phi_1$ and $\phi_2$
\STATE Train $f_P$ by optimizing Eq.(\ref{eq:pseudo}) w.r.t $\theta_P$
\STATE Obtain pseudo labels $\hat{\mathcal{Y}}^P$ with $f_P$
\REPEAT
\STATE Get combined predictions of $f_C$ and $f_G$ on $\mathcal{V}_{val}$
\STATE Calculate the upper level loss $\mathcal{L}_{val}$
\STATE Update $\phi_1$ and $\phi_2$ according to Eq.(\ref{eq:upper})
\FOR{$t=1$ to $T$} 
\STATE Obtain the lower level loss $\mathcal{L}_{train}$
\STATE Update $\theta_C$ and $\theta_G$ by Eq.(\ref{eq:lower})
\ENDFOR
\UNTIL convergence
\end{algorithmic}
\end{algorithm}

\begin{table}[t]
    \small
    \centering
\caption{The statistics of datasets.}
\begin{tabular}{lllcc}
    \toprule
    Dataset & Nodes & Edges & Classes & Hom. Ratio\\
    \midrule
    Wisconsin & 251 & 515 & 5 & 0.20\\
    Texas& 183 & 309 & 5 & 0.11\\
    Cornell & 183 & 280 & 5 & 0.30 \\
    Chameleon & 2,277 & 36,101 & 5 & 0.24\\
    Squirrel & 5,201 & 217,073 & 5 & 0.22 \\
    Crocodile & 11,631 & 360,040 & 5 & 0.25\\
    arxiv-year & 169,343 & 1,166,243 & 5 & 0.22\\
    \midrule
    Cora & 2,708 & 5,429 & 6 & 0.81\\
    Citeseer & 3,327 & 4,732 & 7 & 0.74 \\
    Pubmed & 19,717 & 44,338 & 3 & 0.8 \\
    \bottomrule
    \end{tabular}
    \vskip-1.5em
    \label{tab:datasets}
\end{table}

\section{Training Algorithm of {\method}} \label{app:alg}
The training algorithm of {\method} is shown in Algorithm~\ref{alg:Framwork}. In line 1 and 2, we firstly train the $f_P$ to obtain the required pseudo labels for label-wise message passing. From line 4 to 6, we get the combined predictions from $f_C$ and $f_G$ and update the model selection weights with Eq.(\ref{eq:upper}). From line 7 to 10, we update the model parameters $\theta_C$ and $\theta_G$ by minimizing $\mathcal{L}_{train}$ with Eq.(\ref{eq:lower}). The updating of model selection weights and model parameters are conducted iteratively until convenience.




\section{Additional Details of Experimental Settings}
\subsection{Implementation Details of {\method}}
\label{app:implement} 
For experiments on each heterophilic graph, we report the results on the 10 public dataset splits. For homophily graphs, we run each experiment 5 times on the provided public dataset split. 
The hidden dimension of $f_P$ is fixed as 64 for all graphs. For the $f_C$ on Texas and Wisconsin, a linear layer is firstly applied to transform the features followed by the label-wise graph convolutional layer. As for the other graphs, the label-wise graph convolutional layer is directly applied to the node features. The hidden layer dimension and weight decay rate are tuned based on the validation set by grid search. Specifically, we vary the hidden dimension and weight decay in $\{32,64,128,256\}$ and $\{0.05,0.005,0.0005,0.00005\}$, respectively .
As for the $f_G$ which deploys GCNII~\cite{chen2020simple} as the backbone, the hyperparameter settings are the same as the cited paper. During the training phase, the learning rate is set as 0.01 for all the parameters and model selection weights. The inner iteration step $T$ is set as 1. Our machine uses an Intel i7-9700k CPU with 64GB RAM. A Nvidia 2080Ti GPU is used to run all the experiments.


\subsection{Implementation Details of Compared Methods}
\label{app:baseline}
We adopt a two-layer MLP model on the datasets as baslines to show the effects of the graph structure and local context of the graphs. The hidden dimension is set the same as our {\method}. Apart from MLP,
we compare {\method} with the following representative and state-of-the-art GNNs that originally designed for graphs with homophily:  
\begin{itemize}[leftmargin=*]
    \item \textbf{GCN}~\cite{kipf2016semi}: This is a popular spectral-based Graph Convolutional Network, which aggregates the neighbor information and the centered node by averaging their representations. We apply the official code in \url{https://github.com/tkipf/pygcn}.
    \item \textbf{MixHop}~\cite{abu2019mixhop}: It adopts a graph convolutional layer with powers of the adjacency matrix. The official code in \url{https://github.com/samihaija/mixhop} is implemented for comparsion.
    \item \textbf{SuperGAT}~\cite{kim2020find}: This is a GAT model augmented by the self-supervision. In SuperGAT, apart from the classification loss on provided labels, a self-supervised learning task is deployed to further guide the learning of attention for better information propagation based on GAT~\cite{velivckovic2017graph}. The official code from the authors in \url{https://github.com/dongkwan-kim/SuperGAT} is used.
    \item \textbf{GCNII}~\cite{chen2020simple}: Based on GCN, residual connection and identity mapping are applied in GCNII to have a deep GNN for better performance. The experiments are run with the official implementation in \url{https://github.com/chennnM/GCNII}.
\end{itemize}
We also compare {\method} with the following baseline GNN models for heterophilic graphs:
\begin{itemize}[leftmargin=*]
    \item \textbf{FAGCN}~\cite{bo2021beyond}: FAGCN adaptively aggregates low-frequency and high-frequency signals from neighbors to improve the performance on heterophilic graphs. The implementation from authors in \url{https://github.com/bdy9527/FAGCN} is applied in our experiments.
    \item \textbf{SimP-GCN}~\cite{jin2021node}: A feature similarity preserving aggregation is applied to facilitate the representation learning on graphs with homophily and heterophily. We utilize the official code in \url{https://github.com/ChandlerBang/SimP-GCN}.
    \item \textbf{H2GCN}~\cite{zhu2020beyond}: H2GCN investigates the limitations of GCN on graphs with heterophily. And it accordingly adopts three key designs for node classification on heterophilic graphs. We conduct experiments with the official code from authors in \url{https://github.com/GemsLab/H2GCN}.
    \item \textbf{GPR-GNN}~\cite{chien2020adaptive}: This method introduces a new Generalized PageRank (GPR) GNN to adaptively learn the GPR weights that combine the aggregated representations in different orders. The learned GPR weights can be either positive or negative, which allows the GPR-GNN handle both heterophilic and homophilic graphs.  We adopt the official code from authors in \url{https://github.com/jianhao2016/GPRGNN}.
    \item \textbf{BM-GCN}~\cite{he2022block}: This is one of the most recent methods designed for graphs with heterophily, which achieves state-of-the-art results on heterophilic graphs.  A block-modeling is adopted to GCN to aggregate information from homophilic and heterophilic neighbors discrimatively. More specifically, the link between two nodes will be re-weighted based on  the soft labels of two nodes and the block-similarity matrix. The training and evaluation process is based on the official code in \url{https://github.com/hedongxiao-tju/BM-GCN}.
    \item \textbf{ASGC}~\cite{chanpuriya2022simplified}:  This method replaces the fixed feature propagation step of SGC~\cite{wu2019simplifying} with an adaptive propagation, which can be effective for both homophilic graphs and heterophilic graphs. We use the official code released in \url{https://openreview.net/forum?id=jRrpiqxtrWm}.
    \item \textbf{LINKX}~\cite{lim2021large}: This methods separately embed the adjacency matrix and node features with multilayer perceptrions and simple transformations. We use the official code from authors in \url{https://github.com/CUAI/Non-Homophily-Large-Scale}. 
    \item \textbf{GloGNN++}~\cite{li2022finding}: This method will learn a coefficient matrix to capture the correlations between nodes to aggregate information from global nodes in the graph. The values of the coefficient matrix can be signed and are derived from the optimization. In our experiments, we use the official code in \url{https://github.com/recklessronan/glognn}.
\end{itemize}

The model architecture and hyperparameters of the baselines are set according to the experimental settings provided by the authors for reproduction. For datasets that are not given reproduction details, the hyperparameters of baselines will be tuned based on the performance on validation set to make a fair comparison.

\section{Proof of Theorem 1}
\label{app:theorem1}
\begin{proof}
In this proof, we focus on nodes in class $i$ and class $j$, where $i \neq j$.
Since dimensions of the node feature are independent to each other, without loss of generality, we consider one dimension of the feature and aggregated representation for node $v$, which is denoted as $x_v$ and $z_v$.
For node $v$ in class $i$, the aggregated representation ${z}_{v}$ in GCN layer is rewritten as: 
\begin{equation}
    {z}_{v} = \sum_{u\in \mathcal{N}(v)}\frac{1}{|\mathcal{N}(v)|}{x}_u.
    \label{eq:agg}
\end{equation}   
With assumptions in Sec.~\ref{sec:GCN_analysis}, the expectation of aggregated representations of nodes in class $i$ can be written as:
\begin{equation}
    \mathbb{E}({z}_{v}|y_v=i) = h \cdot {\mu}_{ii} + \frac{1-h}{C-1} \sum_{k=1,k \neq i}^{C} {\mu}_{ik},
\end{equation}
Similarly, we can get the expectation of aggregated nodes representations in class $j$, i.e., $\mathbb{E}({z}_{v}|y_v=j)$. Then, the difference between $\mathbb{E}({z}_{v}|y_v=i)$ and $\mathbb{E}({z}_{v}|y_v=j)$ is 
\begin{equation}
    \begin{aligned}
    \Delta_{i,j} & =  |\mathbb{E}({z}_{v}|y_v=i)-\mathbb{E}({z}_{v}|y_v=j)| \\
    & = |h\cdot ({\mu}_{ii}-{\mu}_{jj}) + \frac{1-h}{C-1}(\mu_{ij}-\mu_{ji}) + \frac{1-h}{C-1}\sum_{k=1,k \neq i,j} ^C(\mu_{ik}-\mu_{jk})|  \\
    & = |\frac{hC-1}{C-1} (\mu_{ii}-{\mu}_{jj}) + \frac{1-h}{C-1}(\sum_{k=1}^C(\mu_{ik}-\mu_{jk}))| 
    \end{aligned}
\label{eq:expect_differ}
\end{equation}

We firstly consider the situation of $h \geq \frac{1}{C}$. When $h \geq \frac{1}{C}$, we can infer the upper bound of $\Delta_{i,j}$ as:
\begin{equation}
\begin{aligned}
    \Delta_{i,j} & \leq \frac{hC-1}{C-1} |\mu_{ii}-\mu_{jj}|+\frac{1-h}{C-1}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}| \\
    & = \frac{hC}{C-1}(|\mu_{ii} - \mu_{jj}|-\frac{1}{C}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}|) +\frac{1}{C-1}(\sum_{k=1}^C|\mu_{ik}-\mu_{jk}|-|\mu_{ii} - \mu_{jj}|),
\end{aligned}
\label{eq:differ}
\end{equation}

And the lower bound of $\Delta_{i,j}$ is:
\begin{equation}
\begin{aligned}
    \Delta_{i,j} & \geq \frac{hC-1}{C-1} |\mu_{ii}-\mu_{jj}|-\frac{1-h}{C-1}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}| \\
    & = \frac{hC}{C-1}(|\mu_{ii} - \mu_{jj}|+\frac{1}{C}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}|) -\frac{1}{C-1}(\sum_{k=1}^C|\mu_{ik} - \mu_{jk}| + |\mu_{ii} - \mu_{jj}|),
\end{aligned}
\label{eq:differ}
\end{equation}
Thus, when $|\mu_{ii}-\mu_{jj}|>|\mu_{ik} - \mu_{jk}|, \forall k \in \{1,...C\}$ and $h\geq \frac{1}{C}$,  both the upper bound and lower bound of $\Delta_{i,j}$ will decrease with the decrease of $h$.

Next, we will show that lower $h$ under the condition of $h \geq \frac{1}{C}$ will lead to higher variance of aggregated nodes. According to Eq.(\ref{eq:agg}), the variance of $\{{z}_{v}: y_v=i\}$ can be written as:
\begin{equation}
\begin{aligned}
    Var({z}_{v}|y_v=i) & = Var(\sum_{u\in \mathcal{N}(v)}\frac{1}{|\mathcal{N}(v)|}{x}_u | y_v=i) \nonumber
\end{aligned}
\label{eq:var_init}
\end{equation}
According to the assumption 1, the neighbor features are conditional independent to each other given the label of the center node. And for each neighbor node $u \in \mathcal{N}(v)$, we have $P(y_u =
y_v|y_v) = h, P(y_u = y|y_v) = \frac{1-h}{C-1}, \forall y \ne y_v$.
Therefore, for neighbor node $u \in \mathcal{N}(v)$ of node $v$ whose label is $i$, its features follow a mixed distribution:
\begin{equation}
\begin{aligned}
        & P({x}_u|y_v=i) \\
        = & \sum_{k=1}^C P(y_u=k|y_v=i) P({x}_u|y_u=k)\\
        = & h \cdot N(\mu_{ii},\sigma_{ii}) + \frac{1-h}{C-1} \sum_{k=1,k\neq i} N(\mu_{ik}, \sigma_{ik})
\end{aligned}
\end{equation}
Using the variance of mixture distribution, the variance of node $v$ in class $i$ can be derived as
\begin{equation}
    \begin{aligned}
        &Var({z}_{v}|y_v=i) 
        =  \frac{1}{d}  Var({x}_u | y_v=i) \\
        = & \frac{1}{d} (\mathbb{E}[Var({x}_u|y_u,y_v=i)] + Var[\mathbb{E}({x}_u|y_u, y_v=i)]) \\
        = & \frac{1}{d}\Big(h\sigma_{ii}^2+\frac{1-h}{C-1}\sum_{k=1,k\neq i}^C \sigma_{ik}^2 
         + h\mu_{ii}^2  +  \frac{1-h}{C-1}\sum_{k=1,k\neq i}^C \mu_{ik}^2 - (h\mu_{ii}+\frac{1-h}{C-1}\sum_{k=1,k\neq i}^C \mu_{ik})^2\Big) 
    \end{aligned}
    \label{eq:var_update}
\end{equation}
Let $\Bar{\mu_i} = \frac{1}{C}\sum_{k=1}^C \mu_{ik}$ and $\sigma_i^2 = \frac{1}{C}\sum_{k=1}^C(\mu_{ik}-\Bar{\mu}_i)^2$. 
Then Eq.(\ref{eq:var_update}) can be rewritten as the following equation:
\begin{equation}
    \begin{aligned}
        & Var({z}_v|y_v=i) \\
        & = \frac{1}{d}(\frac{hC-1}{C-1}\sigma_{ii}^2 + \frac{C-hC}{C-1}(\frac{1}{C}\sum_{k=1}^C\sigma_{ik}^2 + \sigma_i^2) \\
        & + \frac{hC-1}{C-1}\mu_{ii}^2 + \frac{C-hC}{C-1}\Bar{\mu}_{i}^2 - (h\mu_{ii}+\frac{1-h}{C-1}\sum_{k=1,k\neq i}^C \mu_{ik})^2) 
    \end{aligned}
    \label{eq:var_final}
\end{equation}
As $h \geq \frac{1}{C}$, we can set $p=\frac{hC-1}{C-1}, 0 \leq p \leq 1$ and $\frac{C-hC}{C-1}= 1-p$. For the last three terms of Eq.(\ref{eq:var_final}), we have:
\begin{equation}
\begin{aligned}
           & \frac{hC-1}{C-1}\mu_{ii}^2 + \frac{C-hC}{C-1}\Bar{\mu}_{i}^2 - (h\mu_{ii}+\frac{1-h}{C-1}\sum_{k=1,k\neq i}^C \mu_{ik})^2 \\
    & = p \mu_{ii}^2 + (1-p) \Bar{\mu}_{i}^2 - (p\mu_{ii} + (1-p) \Bar{\mu}_{i})^2
    \\
    & = p(1-p)(\mu_{ii} - \Bar{\mu}_{i})^2 \ge 0
\end{aligned}
\label{eq:add_terms}
\end{equation}
Combining Eq.(\ref{eq:var_final}) and Eq.(\ref{eq:add_terms}), we are able to get the lower bound of the variance as:
\begin{equation}
\begin{aligned}
    & Var({z}_v|y_v=i) \\
    \geq & \frac{hC-1}{d(C-1)}\sigma_{ii}^2 + \frac{C-hC}{d(C-1)}(\frac{1}{C}\sum_{k=1}^C\sigma_{ik}^2 + \sigma_i^2) \\
    = & \frac{hC}{d(C-1)}(\sigma_{ii}^2 - \sigma_i^2- \frac{1}{C}\sum_{k=1}^{C}\sigma_{ik}^2) 
    +  \frac{1}{d(C-1)}(C\sigma_i^2 + \sum_{k=1}^C\sigma_{ik}^2 - \sigma_{ii}^2)
\end{aligned}
\end{equation}
When $\sigma_i > \sigma_{ii}$, we know that with the decrease of $h$, the lower bound of $Var({z}_{v}|y_v=i)$ will increase. Similarly,  $Var({z}_{v}|y_v=j)$ will also increase with a lower $h$. Combining with $|\mathbb{E}({z}_{v}|y_v=i)-\mathbb{E}({z}_{v}|y_v=j)|$ will decrease with the decrease of $h$, we can conclude that when $h \geq \frac{1}{C}$, the graph with lower $h$ will lead to less discrimative aggregate representations. 



We then prove when $h < \frac{1}{C}$, the decreasing of $h$ will increase the discriminability of the aggregated representations by averaging. Specifically, with Eq.(\ref{eq:expect_differ}), we can infer that when $h < \frac{1}{C}$ the upper bound of $\Delta_{i,j}$ will be:

\begin{equation}
\begin{aligned}
    \Delta_{i,j} & \leq \frac{1-hC}{C-1} |\mu_{ii}-\mu_{jj}|+\frac{1-h}{C-1}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}| \\
    & = \frac{-hC}{C-1}(|\mu_{ii} - \mu_{jj}| + \frac{1}{C}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}|) +\frac{1}{C-1}(\sum_{k=1}^C|\mu_{ik}-\mu_{jk}|+|\mu_{ii} - \mu_{jj}|),
\end{aligned}
\label{eq:differ}
\end{equation}

And the lower bound of $\Delta_{i,j}$ is:
\begin{equation}
\begin{aligned}
    \Delta_{i,j} & \geq \frac{1-hC}{C-1} |\mu_{ii}-\mu_{jj}|-\frac{1-h}{C-1}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}| \\
    & = \frac{-hC}{C-1}(|\mu_{ii} - \mu_{jj}|-\frac{1}{C}\sum_{k=1}^C|\mu_{ik}-\mu_{jk}|) -\frac{1}{C-1}(\sum_{k=1}^C|\mu_{ik} - \mu_{jk}| - |\mu_{ii} - \mu_{jj}|),
\end{aligned}
\label{eq:differ}
\end{equation}
Thus, when $h < \frac{1}{C}$ and $|\mu_{ii}-\mu_{jj}|>|\mu_{ik} - \mu_{jk}|, \forall k \in \{1,...C\}$,  both the upper bound and lower bound of $\Delta_{i,j}$ will increase with the decrease of $h$.

For the variance of aggregated representations when $h < \frac{1}{C}$, we can infer its folowing upper bound with Eq.(\ref{eq:var_final}):
\begin{equation}
\begin{aligned}
    & Var({z}_v|y_v=i) \\
    \leq & \frac{hC-1}{d(C-1)}\sigma_{ii}^2 + \frac{C-hC}{d(C-1)}(\frac{1}{C}\sum_{k=1}^C\sigma_{ik}^2 + \sigma_i^2) \\
    = & \frac{hC}{d(C-1)}(\sigma_{ii}^2 - \sigma_i^2- \frac{1}{C}\sum_{k=1}^{C}\sigma_{ik}^2) 
    +  \frac{1}{d(C-1)}(C\sigma_i^2 + \sum_{k=1}^C\sigma_{ik}^2 - \sigma_{ii}^2)
\end{aligned}
\end{equation}
According to the assumption that $\sigma_i> \sigma_{ii}$, we know that with the decrease of $h$ under the condition of $h < \frac{1}{C}$ the upper bound of the $Var({z}_v|y_v=i)$ will decrease. We can have the same conlcusion for $Var({z}_v|y_v=j)$. Combining the trend that when $h < \frac{1}{C}$ $|\mathbb{E}({z}_{v}|y_v=i)-\mathbb{E}({z}_{v}|y_v=j)|$ will increase with the decrease of $h$, we can conclude that when $h < \frac{1}{C}$, the graph with lower $h$ will have more discriminative aggregate representations. 

When $h=\frac{1}{C}$, we can get
\begin{equation}
    \Delta_{i,j} = \frac{1}{C}|\sum_{k=1}^C(\mu_{ik}-\mu_{jk})|,
\end{equation}
\begin{equation}
    Var({z}_v|y_v=i) \geq \frac{1}{d}(\frac{1}{C}\sum_{k=1}^C\sigma_{ik}^2 + \sigma_i^2)|,
\end{equation}
If $\sigma_i > \sqrt{d}|\mu_{ik}-\mu_{ik}|, \forall k \in \{1,\dots, C\}$, we can get $Var({z}_v|y_v=i)>\Delta_{i,j}^2$. So when $h=\frac{1}{C}$ and $\sigma_i > \sqrt{d}|\mu_{ik}-\mu_{ik}|, \forall k \in \{1,\dots, C\}$ , the representations after the averaging process will be non-discrimative.

\end{proof}



\section{Proof of Theorem 2}
\label{app:theorem2}
\begin{proof}
In this proof, we also consider a center node $v$ in class $i$. And we focus on one dimension of the node feature and aggregated representation.
Specifically, for each dimension, the label-wise aggregation can be written as:
\begin{equation}
    {a}_{v,k} = \sum_{u\in \mathcal{N}_k(v)} \frac{1}{|\mathcal{N}_k(v)|} {x}_u,
    \label{eq:label_Init}
\end{equation}
where ${a}_{v,k}$ denotes the aggregated feature of neighbors  in class $k$. 
Since $u \in \mathcal{N}_{k}(v)$, we know node $u$'s features ${x}_u$ follows distribution as ${x}_u \sim N(\mu_{ik},\sigma_{ik})$. 
The mean of ${a}_{v,k}$ in Eq.(\ref{eq:label_Init}) is given as:
\begin{equation}
    \mathbb{E}({a}_{v,k}|y_v=i) = \mu_{ik}.
\end{equation}
Then the absolute difference between $\mathbb{E}({a}_{v,k}|y_v=i)$ and $ \mathbb{E}({a}_{v,k}|y_v=j)$ will be:
\begin{equation}
    \begin{aligned}
        \Delta_{i,j}^{k} &  = |\mathbb{E}({a}_{v,k}|y_v=i) - \mathbb{E}({a}_{v,k}|y_v=j)| = |\mu_{ik}-\mu_{jk}|.
    \end{aligned}
\end{equation}
Given the assumption that the features are conditionally independent given the label of center node, the variance of ${a}_{v,k}$ can be written as:
\begin{equation}
    Var({a}_{v,k}|y_v=i) = \left\{ \begin{array}{ll}
         \frac{1}{dh} \sigma_{ik}^2 & \mbox{if $k = i$ } ;\\
        \frac{C-1}{d(1-h)} \sigma_{ik}^2 & \mbox{else},\end{array} \right.
\end{equation}
In label-wise aggregation, we generally concatenate the $\{{a}_{v,k}:k\in \{1,\dots C\}\}$ for further classification. Therefore, the lower bound of discriminability can be given by the representation of the class that are most discriminative, which can be formally written as:
\begin{equation}
    k^*=\arg \max_{k} \frac{(\Delta_{i,j}^k)^2}{Var({a}_{v,k}|y_v=i)}
\end{equation}
When $h \geq \frac{1}{C}$, we can get:
\begin{equation}
    \begin{aligned}
        \frac{(\Delta_{i,j}^{k^*})^2}{Var({a}_{v,k^*}|y_v=i)} \geq \frac{dh|\mu_{ii}-\mu_{ji}|^2}{\sigma^2_{ii}} \geq \frac{d|\mu_{ii}-\mu_{ji}|^2}{C\sigma^2_{ii}}
    \end{aligned}
    \label{eq:lw_1}
\end{equation}
As for $h \leq \frac{1}{C}$, let $k \neq i$ we can infer that:
\begin{equation}
    \begin{aligned}
        \frac{(\Delta_{i,j}^{k^*})^2}{Var({a}_{v,k^*}|y_v=i)} & \geq \frac{d(1-h)|\mu_{ik}-\mu_{jk}|^2}{(C-1)\sigma^2_{ik}}  \geq \frac{d|\mu_{ik}-\mu_{jk}|^2}{C\sigma^2_{ik}}
    \end{aligned}
    \label{eq:lw_2}
\end{equation}

Therefore, if the condition that $|\mu_{ik} - \mu_{jk}| > \sqrt{\frac{C}{d}} \sigma_{ik}, \forall k \in \{1, \dots, C\}$ is met, we can infer from Eq.(\ref{eq:lw_1}) and Eq.(\ref{eq:lw_2}) that $\frac{(\Delta_{i,j}^{k^*})^2}{Var({a}_{v,k^*}|y_v=i)} > 1$ regardless the value of the homophily ratio $h$.
This shows that label-wise aggregation can preserve the context and ensure the high discriminability regardless the homophily ratio.
\end{proof}

\section{Additional Details and Experiments on Generated Graphs}

\begin{algorithm}[h!]
\caption{ Algorithm of Generating Graphs}
\label{alg:graph}
\begin{algorithmic}[1]
\REQUIRE
$\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})$, $\mathcal{Y}_L$, target homophily ratio $h$, and target node degree $d$
\ENSURE $\mathcal{G}'=(\mathcal{V}, \mathcal{E}', \mathbf{X})$
\STATE Split the edges $\mathcal{E}$ into heterophilic edges $\mathcal{E}_{n}$ and homophilic edges $\mathcal{E}_s$.
\IF{$|\mathcal{E}_s| \geq hd|\mathcal{V}|$}
    \STATE Sample $hd|\mathcal{V}|$ edges from $\mathcal{E}_s$ to get $\mathcal{E}_s'$
\ELSE
    \STATE Obtain $hd|\mathcal{V}|-|\mathcal{E}_s|$ homophilic edges by randomly link nodes in the same class
    \STATE  Combine $\mathcal{E}_s$ with added homophilic edges  to obtain $\mathcal{E}_s'$
\ENDIF
\STATE Randomly sample $d(1-h)|\mathcal{V}|$ edges from $\mathcal{E}_n$ as $\mathcal{E}_n'$
\STATE Get $\mathcal{E}'$ with $\mathcal{E}'=\mathcal{E}_n' \cup \mathcal{E}_s'$ 
\end{algorithmic}
\end{algorithm}


\subsection{Process of Graph Generation}
\label{sec:app_graphgen}
To verify the conclusion in Theroem~\ref{theorem:1}, we generate graphs with different homophily ratios and average degrees on the large-scale crocodile graph. Specifically, the average node degree of the target generated graphs is varied by $\{5,10,20\}$. For each node degree, we will sample the heterophilic edges, i.e., edges linking nodes in different classes, and homophilic edges, i.e., edges linking nodes in the same class from the original crocodile graph in different ratios to obtain realistic graphs with different heterophily levels. The homophily ratios of the generated graphs range from 0 to 0.9 with a step of 0.1. Since crocodile itself is a heterophilic graph that do not contain many homophilic edges, there could be no enough homophilic edges to obtain a graph with high homophily and node degrees. In this situation, we will randomly link nodes in the same class to get the required number of homophilic edges for graph generation. For the train/validation/test splits of generated graphs, they are the same as the original crocodile graph.
The algorithm of the graph generation process can be found in Algorithm~\ref{alg:graph}.


\begin{figure}[h!]
\centering
\begin{subfigure}{0.32\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/syn_d5.pdf} 
    \vskip -0.5em
    \caption{Average degree as 5}
\end{subfigure}
\begin{subfigure}{0.32\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/syn_d10.pdf} 
    \vskip -0.5em
    \caption{Average degree as 10}
\end{subfigure}
\begin{subfigure}{0.32\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/syn_d20.pdf} 
    \vskip -0.5em
    \caption{Average degree as 20}
\end{subfigure}
\caption{Comparisons between GCN, GAT and our {\method} on generated graphs. Note that model selection module is not adopted in {\method} in these experiments.}
\label{fig:syn_more}
\vskip -1em
\end{figure}

\subsection{More Experiments on Generated Graphs}
\label{sec:app_syn}
To verify our theoretical analysis that label-wise aggregation can lead to distinguishable representations regardless the heterophily levels under mild conditions, we also compare {\method} with GCN and GAT on the generated graphs with different homophily ratios and average node degrees.
The label-wise aggregation is conducted with the pseudo labels and provided ground-truth labels as it is described in Sec.\ref{sec:arch}. Since we only focus on the label-wise graph convolution in the experiments, the model selection module is removed here. The other settings are the same as description in Appendix~\ref{app:implement}.
The average results of 10 splits are shown in Fig.~\ref{fig:syn_more}. From this figure, we can observe that the performance of {\method} is much better than the GCN and GAT when the heterophily level is high. For example, when $h \approx 0.2$, both GCN and GAT can hardly outperforms MLP. By contrast, the accuracy of {\method} outperform GCN and GAT by around 10\%. This demonstrates the effectiveness of adopting label-wise aggregation in graph convolution. In addition, we can find that only adopting the model with label-wise graph convolution will give slightly worse performance than GCN/GAT when the homophily ratio is very high. This implies the necessity of deploying a model selection module. 


















\section{  Additional Experimental Results} \label{sec:app_result}

{ The additional experimental results on Cornell and Citeseer datasets are presented in Table~\ref{tab:add_homo} and Table~\ref{tab:add_hete}. The observations are similar to that of Table~\ref{tab:result}.}

\begin{table}[h!]
    \small
    \centering
    \caption{Additional comparisons with GNNs originally designed for graph with homophily.}
    {\scriptsize
    \begin{tabular}{lcccccc}
    \toprule
    Dataset & MLP & GCN &  MixHop & SuperGAT & GCNII & {\method} \\
    \midrule 
    Cornell & 79.2 $\pm 5.7$ & 57.3 $\pm 5.8$ & 79.5 $\pm 6.3$ & 57.3 $\pm 4.3$ & \underline{80.3 $\pm 5.3$} & \textbf{83.2} $\bf \pm 5.5$ \\
    Citeseer & 60.3 $\pm 0.4$ & 71.3 $\pm 0.3$ & 68.7 $\pm 0.3$ & \underline{72.2 $\pm 0.8$} &  72.0 $\pm 0.8$ & \textbf{72.3} $\bf \pm 0.4$\\
    \bottomrule
    \end{tabular}
    \vskip -1em
    }
    \label{tab:add_homo}
\end{table}

\begin{table}[h!]
    \small
    \centering
    \caption{Additional comparisons with GNNs designed for graph with heterophily.}
    \resizebox{0.98\textwidth}{!}{
    {\scriptsize
    \begin{tabular}{lccccccccc}
    \toprule
    Dataset & FAGCN & SimP-GCN &  H2GCN & GPRGNN &  BM-GCN & ASGC & LINKX & GloGNN+ & {\method} \\
    \midrule 
    Cornell & 78.3 $\pm 4.5$ & 81.4 $\pm 7.4$ & 79.7 $\pm 5.0$ & 77.6 $\pm 5.0$ & 74.6 $\pm 5.0$ &    79.2 $\pm 5.2$  & 77.8 $\pm 5.8$ & \textbf{85.9} $\bf \pm 4.4$ &\underline{83.2 $\bf \pm 5.5$} \\
    Citeseer & 71.7 $\pm 0.6$ & \underline{71.8 $\pm 0.8$} & 71.0 $\pm 0.5$ & 71.1 $\pm 0.9$  & 68.9 $\pm 1.0$ & 70.2 $\pm 0.2$& 51.6 $\pm 1.7$ & 66.7 $\pm 1.9$ & \textbf{72.3} $\bf \pm 0.4$\\
    \bottomrule
    \end{tabular}
    }
    }
    \vskip -1em
    \label{tab:add_hete}
\end{table}

\section{Impacts of Label-Wise Aggregation Layers} \label{app:layer}
In this section, we explore the sensitivity of {\method} on the depth of $f_C$, i.e., the number of layers of label-wise message passing. Since {\method} will not select $f_C$ for homophilic graphs. We only conduct the sensitivity analysis on heterophilic graphs. We vary the depth of $f_C$ as $\{2,3,\dots,6\}$.  The other experimental settings are the same as that described in Sec.~\ref{app:implement}. The results on Chameleon and Squirrel are shown in Fig.~\ref{fig:layer}.  From the figure, we find that our {\method} is insensitive to the number of layers, while the performance of GCN will drop with the increase of depth. This is because  aggregation of {\method} is performed label-wisely to capture the context information. Embeddings of nodes in different classes will not be smoothed to similar values even after many iterations.

\begin{figure}[h!]
\centering
\begin{subfigure}{0.24\columnwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/layer_crocodile.pdf} 
    \vskip -0.5em
    \caption{Crocodile}
\end{subfigure}
\begin{subfigure}{0.24\columnwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/layer_texas.pdf} 
    \vskip -0.5em
    \caption{Texas}
\end{subfigure}
\begin{subfigure}{0.24\columnwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/layer_chameleon.pdf} 
    \vskip -0.5em
    \caption{Chameleon}
\end{subfigure}
\begin{subfigure}{0.24\columnwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/layer_suqirrel.pdf} 
    \vskip -0.5em
    \caption{Squirrel}
\end{subfigure}
\caption{Classification accuracy with different model depth.}
\label{fig:layer}
\vskip -1em
\end{figure}


\section{Limitations of Our Work}
\label{sec:limitations}
In this paper, we conduct thoroughly theoretical and empirical analysis to show the impacts of heterophily levels to GCN.  And we demonstrate the GCN model can be largely affected by heterophily and give poor prediction results. To alleviate the issue brought by heterophily, we develop a novel label-wise graph convolutional network to preserve the heterophilic context to facilitate the node classification. However, there are some limitations of our work.  First, node labels are required for {\method} to obtain pseudo labels for label-wise graph convolution. However, in some tasks such as link prediction, labels are not available. Therefore, we will investigate how to obtain useful pseudo labels for applications that do not provide node labels. 
{
Second, in our theoretical analysis, we make several assumptions for simplification. Concretely, we conduct analysis on the d-regular graph.
Following~\cite{zhu2020beyond,luan2021heterophily}, we also make an assumption on the label distribution of neighbor nodes. In our analysis, the node features are simplified to normal distribution and dimensions of features are independent to each other. These assumptions may not hold for some real-world graphs. 
For example, node degrees of the real network can be unbalanced which will contradict the assumption of d-regularity. The label distributions and feature distributions of neighbor nodes can be much more complex. Therefore, we will investigate the theoretical analysis on more flexible assumptions in the future. Third, recent studies~\cite{luan2021heterophily,platonov2022characterizing} show that the edge homophily ratio used in this paper could have significant drawbacks especially when the distribution of classes is unbalanced. To address these drawbacks, new measures such as adjusted homophily and label informativeness are proposed~\cite{platonov2022characterizing}. We leave the extension of our analysis on these new homophily measures as the future work.
}



%
