

\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}

\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{mathtools}

\usepackage{graphicx}
\usepackage{booktabs} \usepackage{multirow} \usepackage[noblocks]{authblk}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{xspace}
\usepackage{nicefrac}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage[numbers]{natbib}
\bibliographystyle{abbrvnat}

\usepackage{array}
\newcommand{\head}[2]{\multicolumn{1}{>{\centering\arraybackslash}p{#1}}{\textsc{#2}}}



\usepackage{amsmath}
\usepackage{amsthm} \usepackage{mathtools,thmtools}
\usepackage{amsfonts}







\usepackage{enumitem}
\usepackage{bm}
\usepackage{xspace}
\usepackage{nicefrac}


\pdfstringdefDisableCommands{\renewcommand{\bm}[1]{#1}}

\usepackage[capitalise]{cleveref}
\usepackage{xcolor}
\usepackage{dsfont}

\usepackage[textsize=footnotesize,textwidth=0.75in,
            color=cyan,bordercolor=white]{todonotes}
\presetkeys{todonotes}{size=\footnotesize\sffamily\bfseries\boldmath\color{white}}{}

\newcommand{\ignore}[1]{}
\newcommand{\alert}[1]{{\textcolor{red}{#1}}}


\newcommand{\wrapalgo}[2][0.9\linewidth]
{\begin{center}\setlength{\fboxsep}{5pt}\fbox{\begin{minipage}{#1}
#2
\end{minipage}}\end{center}
\vspace{-0.25cm}
}


\newenvironment{smashbox}[1][]{\begin{minipage}{1.0\linewidth}\lineskiplimit=-\maxdimen
}{\end{minipage}
}






\declaretheoremstyle[
	    spaceabove=\topsep,
	    spacebelow=\topsep,
	    bodyfont=\normalfont\itshape,
    ]{theorem}

\declaretheorem[style=theorem,name=Theorem]{theorem}

\declaretheoremstyle[
	    spaceabove=\topsep,
	    spacebelow=\topsep,
	    bodyfont=\normalfont,
    ]{definition}








\declaretheorem[style=theorem,sibling=theorem,name=Lemma]{lemma}
\declaretheorem[style=theorem,sibling=theorem,name=Corollary]{corollary}
\declaretheorem[style=theorem,sibling=theorem,name=Claim]{claim}
\declaretheorem[style=theorem,sibling=theorem,name=Proposition]{proposition}
\declaretheorem[style=theorem,sibling=theorem,name=Fact]{fact}
\declaretheorem[style=theorem,sibling=theorem,name=Observation]{observation}
\declaretheorem[style=theorem,sibling=theorem,name=Conjecture]{conjecture}

\declaretheorem[style=theorem,numbered=no,name=Theorem]{theorem*}
\declaretheorem[style=theorem,numbered=no,name=Lemma]{lemma*}
\declaretheorem[style=theorem,numbered=no,name=Corollary]{corollary*}
\declaretheorem[style=theorem,numbered=no,name=Proposition]{proposition*}
\declaretheorem[style=theorem,numbered=no,name=Claim]{claim*}
\declaretheorem[style=theorem,numbered=no,name=Fact]{fact*}
\declaretheorem[style=theorem,numbered=no,name=Observation]{observation*}
\declaretheorem[style=theorem,numbered=no,name=Conjecture]{conjecture*}

\declaretheorem[style=definition,sibling=theorem,name=Definition]{definition}
\declaretheorem[style=definition,sibling=theorem,name=Remark]{remark}
\declaretheorem[style=definition,sibling=theorem,name=Example]{example}
\declaretheorem[style=definition,sibling=theorem,name=Question]{question}

\declaretheorem[style=definition,numbered=no,name=Definition]{definition*}
\declaretheorem[style=definition,numbered=no,name=Remark]{remark*}
\declaretheorem[style=definition,numbered=no,name=Example]{example*}
\declaretheorem[style=definition,numbered=no,name=Question]{question*}







\DeclareMathAlphabet{\mathbfsf}{\encodingdefault}{\sfdefault}{bx}{n}

\DeclareMathOperator*{\err}{error}
\DeclareMathOperator*{\argmin}{arg\!\min}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\trace}{Tr}
\DeclareMathOperator*{\diag}{diag}
\let\Pr\relax
\DeclareMathOperator{\Pr}{\mathbb{P}}

\newcommand{\mycases}[4]{{
\left\{
\begin{array}{ll}
    {#1} & {\;\text{#2}} \1ex]
    {#3} & {\;\text{#4}} \\triangleright \label{eq:accum}
  \gamma_t(i)
  =
  \sum_{s=1}^t g_s^2(i)
  ~,\qquad
  \forall ~ i \in [d]
  ~.

  w_{t+1}(i)
  =
  w_{t}(i) - \eta \frac{g_t(i)}{\sqrt{\gamma_t(i)}}
  ~,\qquad
  \forall ~ i \in [d]
  ~,
 \label{eq:regret-adagrad}
  \sum_{t=1}^T \ell_t(w_t) - \sum_{t=1}^T \ell_t(w^\star)
  =
  \O\lr{ D \sum_{i=1}^d \sqrt{\sum_{t=1}^T g_t^2(j)} }
  ,

  \sum_{t=1}^T \Lr{ \ell_t(w_t) - \ell_t(w^\star) }
  \leq
  2 D \sum_{i=1}^d \sqrt{\min_{r : S_r \ni i} \sum_{t=1}^T \max_{j \in S_r} g_t^2(j)}
  ~,

  \E[L(\overline{w}_T)] - L(w^\star)
  =
  \O\lr{
  \frac{1}{T} \sum_{i=1}^d \E \sqrt{\min_{r : S_r \ni i} \sum_{t=1}^T \max_{j \in S_r} g_t^2(j)} }
  \!.

  &\frac{1}{2\eta} \sum_{t=1}^T \Lr{ \norm{w_t-w^\st}_{H_t}^2 - \norm{w_{t+1}-w^\st}_{H_t}^2 }
  + \frac{\eta}{2} \sum_{t=1}^T \Lr{\norm{g_t}_{H_t}^*}^2
  ~.

  \sum_{t=1}^T \Lr{ \norm{w_t-w^\st}_{H_t}^2 - \norm{w_{t+1}-w^\st}_{H_t}^2 }
  &\leq
  \sum_{t=1}^T (\mi{t}^{1/2}-\mi{t-1}^{1/2}) \cdot (w_t-w^\st)^2
  \\
  &\leq
  \sum_{t=1}^T (\mi{t}^{1/2}-\mi{t-1}^{1/2}) \cdot \Lr{\norm{w_t-w^\st}_\infty^2 \onev{d}}
  \\
  &\leq
  \vphantom{\sum^T}
  D^2 \left(\mi{T}^{1/2} \cdot \onev{d}\right)
  \;=\;
  \vphantom{\sum^T}
  D^2 \trace(H_T) ~.

  \sum_{t=1}^T \Lr{\norm{g_t}_{G_t}^*}^2
  \leq
  \sum_{t=1}^T \Lr{\norm{g_t}_{G_T}^*}^2 + \trace(G_T)
  =
  \vphantom{\sum^T}
  \gamma_T^{-1/2} \cdot \gamma_T + \trace(G_T)
  =
  \vphantom{\sum^T}
  2\trace\lr{ G_T }
  .

  \sum_{t=1}^T \Lr{\norm{g_t}_{H_t}^*}^2
  \leq
  \sum_{t=1}^T \Lr{\norm{g_t}_{G_t}^*}^2
  \leq
  2\trace(G_T)
  \leq
  2\trace(H_T)
  ~.

  \sum_{t=1}^T \ell_t(w_t) - \ell_t(w^\star)
  \leq
  \lr{\frac{D^2}{2\eta} + \eta} \trace(H_T)
  ~.

  g_{s}^2(i)
  \leq
  \min_{r : S_r \ni i} \max_{j \in S_r} g_{s}^2(j)
  .

  \sum_{s=1}^t g_{s}^2(i)
  \leq
  \sum_{s=1}^t \min_{r : S_r \ni i} \max_{j \in S_r} g_{s}^2(j)
  \leq
  \min_{r : S_r \ni i} \sum_{s=1}^t \max_{j \in S_r} g_{s}^2(j)
  =
  \min_{r : S_r \ni i} \mx{t}(r)
  ~.

  \tmi{t+1}(i)
  =
  \min_{r : S_r \ni i} \max_{j \in S_r} \tmi{t}(j) + g^2_{t+1}(i)
  \geq
  \tmi{t}(i)
  ~.

  \tmi{t+1}(i)
  &=
  \min_{r : S_r \ni i} \max_{j \in S_r} \tmi{t}(j) + g^2_{t+1}(i)
  \\
  &\geq
  \tmi{t}(i) + g^2_{t+1}(i)
  \\
  &\geq
  \gamma_t(i) + g^2_{t+1}(i)
  \\
  &=
  \gamma_{t+1}(i)
  ~ .

  \tmi{t+1}(i)
  &=
  \min_{r : S_r \ni i} \max_{j \in S_r} \tmi{t}(j) + g^2_{t+1}(i)
  \\
  &\leq
  \min_{r : S_r \ni i} \max_{j \in S_r} \mi{t}(j) + g^2_{t+1}(i)
  \\
  &\leq
  \min_{r : S_r \ni i} \max_{j \in S_r} \mi{t}(j) + \min_{r : S_r \ni i}
  \max_{j \in S_r} g^2_{t+1}(j)
  \\
  &\leq
  \min_{r : S_r \ni i} \LR{ \max_{j \in S_r} \mi{t}(j) + \max_{j \in S_r} g^2_{t+1}(j) }
  \\
  &\leq \min_{r : S_r \ni i} \sum_{s=1}^{t+1} \max_{j \in S_r} g^2_{s}(j)
  \\
  &=
  \mi{t+1}(i) ~ ,

  \mi{t}(j)
  =
  \min_{r' : S_{r'} \ni j} \sum_{s=1}^t \max_{j' \in S_{r'}} g^2_{s}(j')
  \leq
  \sum_{s=1}^t \max_{j' \in S_r} g^2_{s}(j')
  .
  &\qedhere

\end{proof}



\section{More Experiments}
\label{sec:more-exper}

\subsection{Tightness of \NAME approximation}

We corroborate our discussion from \cref{sec:patterns} with an illustration
for both variants of \NAME, of the tightness of approximation of Adagrad's
second-order statistics. \cref{fig:sm3_2v1} demonstrates that overall \NAME
provides a tight approximation, with the \NAME-II performing significantly
better than \NAME-I, especially for higher-magnitude values.


\begin{figure}[h!]
  \centering
  \begin{tabular}{ccc}
    \includegraphics[width=0.3\linewidth]{sm3_2v1} &
    \includegraphics[width=0.3\linewidth]{sm3_2v1_atten} &
    \includegraphics[width=0.3\linewidth]{sm3_2v1_s}  \\
    {\small(a) Input embedding} & {\small (b) Attention layer} & {\small (c) Output softmax}
  \end{tabular}
\caption{The magnitude of the 100 largest accumulators~\cref{eq:accum} of
	Adagrad for the embedding layer of a Transformer model trained
  on the WMT'14 \entofr dataset. The accumulators are sorted by magnitude.}
\label{fig:sm3_2v1}
\end{figure}


\subsection{Results for basic Transformer on WMT'14 \entode}

In~\cref{en_de} we report results for the basic Transformer after training for
700,000 steps on \entode with a batch size of 1536.  As in previously
discussed experiments, SGD with momentum performs poorly compared to adaptive
optimizers and hence is not included in the comparison.

\begin{figure}[h!]
\begin{center}
\begin{minipage}[c]{0.45\columnwidth}
\vspace{0pt}
\includegraphics[width=\columnwidth]{en_de}
\end{minipage}
\hspace{0.025\linewidth}
\begin{minipage}[c]{0.45\columnwidth}
\vspace{0pt}
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
\head{1.25cm}{Optimizer} & \head{2.25cm}{Batch Size \mbox{p/core (total)}} &  \head{0.75cm}{BLEU} \\
\midrule
Adam & 48 (1536) & 27.15  0.002 \\
Adagrad & 48 (1536) & {\bf 27.42   0.001} \\
Adafactor & 48 (1536) & 26.88   0.002 \\
\NAME & 48 (1536) & 27.32   0.002 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{minipage}
\end{center}
\caption{Test log-perplexity (left) and BLUE scores (right) on of a Transformer
model trained on the WMT'14 \entode dataset.}
\label{en_de}
\end{figure}


\subsection{Activation patterns in convolutional networks}
\label{sec:conv-patterns}

We give additional evidence of self-formation of row and column activation
patterns which arise in convolutional image recognition models. See
\cref{fig:adagrad_lr_conv} for an illustration.

\begin{figure}[ht]
  \vspace{-0.2cm}
  \centering
  \begin{tabular}{c c}
    \includegraphics[height=115pt,width=115pt,trim=0 0 0 0]{conv} &
    \includegraphics[height=115pt,width=115pt,trim=0 0 0 0]{conv_softmax} \\
    {\small (a) Filter of 7x1x256x256 convolution \hspace{15pt}} & {\small (b) Output softmax \hspace{15pt}}
  \end{tabular}
\caption{Visualization of gradient square statistics~\cref{eq:accum} for
  different weight matrices using a AmoebaNet-D model. (Color intensities are in
  log~scale.)}
\label{fig:adagrad_lr_conv}
\end{figure}



\section{Details of Experiments}
\label{sec:exper-detail}

We report the settings of hyperparameters used in our experiments in
\cref{tbl:hparams}. We performed a grid search on the following
hyper-parameters: ,  , and  for each of the optimizers when
applicable. We were able to discard a large fraction of the search space for
learning rates, as large values typically cause instability and lower values
make the progress slow. We found we found  to work well for
almost all experiments (where batch size < 2048) except in the case of \NAME
on BERT-Large where  worked best for  and 
batch sizes.

\begin{table}[ht]
\centering
\begin{small}
\begin{tabular}{llclc}
\toprule
\head{1.5cm}{Experiment} & \head{1cm}{Optimizer}& \head{1.5cm}{Batch size} & \head{2cm}{Hyperparameters} & \head{1.5cm}{Warmup ()}    \\
\midrule
\multirow{4}{*}{Transformer \entode}  &  Adafactor & 1536 & , ,  & 10k  \\
 &  Adam & 1536 & ,  ,  & 10k \\
&  Adagrad & 1536 & ,   & 10k \\
&  SM3 & 1536 & ,  & 10k \\
\midrule
\multirow{4}{*}{Transformer \entofr}  &  Adafactor& 384 & , ,  & 40k    \\
 &  Adam& 384 & , ,  & 40k \\
&  Adagrad& 384 & ,   & 40k\\
&  SM3& 384 & ,  & 40k\\
&  Adafactor & 768 & , ,   & 40k\\
&  SM3  & 768 & ,  & 40k\\
\midrule
\multirow{4}{*}{BERT--Large}  &  Adafactor  & 1024 & , ,  & 10k  \\
 &  Adam  & 1024 & , ,  & 10k\\
&  Adagrad  & 1024 & ,  & 10k \\
&  SM3  & 1024 & ,    & 10k\\
&  SM3  & 2048 & ,   & 10k\\
&  SM3  & 8192 &  ,  & 2k\\
&  SM3 & 65536 &  ,  & 2k \\
\midrule
\multirow{2}{*}{AmoebaNet}  &  SGD  & 4096& , , =4.5k,  & 1.2k\\
					   &  SM3 & 4096 & ,  & 1.2k\\
\bottomrule
\end{tabular}
\end{small}
\caption{Hyperparameter setup used in our experiments.}
\label{tbl:hparams}
\end{table}

We also employed a short initial ``warmup'' stage for all optimizers. During
warmup we gradually increased the learning rate  from zero to its
maximal value during the first few thousand updates. This is a common
heuristic in training of deep models, where often a high learning rate setting
in the early stages of optimization causes instabilities and results in
failure to converge, colloquially called ``blowup''.
The choice of the number of steps used for warmup does not affect the eventual
performance of the trained models, and was chosen somewhat liberally before
tuning the rest of the hyperparameters. We would like to stress that in each
experiment, we used the same value for all optimizers. In the experiments with
BERT-Large using large batch sizes, warmup was very short as experimentally it
deemed almost unnecessary.

\begin{table}[ht]
\begin{center}
\begin{small}
\begin{tabular}{lllc}
\toprule
\head{2.5cm}{Experiment} & \head{2.5cm}{Optimizer} & \head{3cm}{LR Schedule (after warmup)} & \head{2cm}{Reference}\\
\midrule
Transformer & Adam, Adafactor &  & \cite{vaswani2017attention} \\
BERT & Adam, Adafactor &  & \cite{devlin18} \\
AmoebaNet-D & SGD (+momentum) &  & Folklore \\
All & Adagrad, SM3 &  & \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\caption{Learning rate schedules used by the algorithms we experimented with.
Here,  is the current time step,  is the base learning rate, 
is the decay constant,  is the staircase step interval,  is the
minimum learning rate for staircase schedule,  is the number of warmup
steps,  is the total number of training steps, and  is the size of the model.}
\label{learning_rate_rules}
\end{table}



We note that, compared to other optimizers, \NAME has a \emph{single}
hyper-parameter that requires tuning, the initial learning rate .
Concretely, past the warmup phase, \NAME does \emph{not} employ a schedule for
learning-rate decay which is often difficult to tune. \cref{learning_rate_rules}
we summarize the procedures for scheduling the learning rate of all
optimizers.

For experimenting with Adafactor, we made use of the implementation in the
Tensor2Tensor framework~\cite{tensor2tensor} and tuned the parameters as
described above. We found Adafactor to work quite well on translation tasks,
for which it was designed and optimized. Alas, we could not get it to work on
the BERT language models. Adafactor's implementation has numerous
hyperparameters which makes it extremely difficult to set up for new domains.

\end{document}
