[{'LEADERBOARD': {'Task': 'Unsupervised Machine Translation', 'Dataset': 'WMT2016 English-German', 'Metric': 'BLEU', 'Score': '29.7'}}, {'LEADERBOARD': {'Task': 'Unsupervised Machine Translation', 'Dataset': 'WMT2016 English-Romanian', 'Metric': 'BLEU', 'Score': '21'}}, {'LEADERBOARD': {'Task': 'Unsupervised Machine Translation', 'Dataset': 'WMT2014 English-French', 'Metric': 'BLEU', 'Score': '32.6'}}, {'LEADERBOARD': {'Task': 'Unsupervised Machine Translation', 'Dataset': 'WMT2014 French-English', 'Metric': 'BLEU', 'Score': '39.2'}}, {'LEADERBOARD': {'Task': 'Unsupervised Machine Translation', 'Dataset': 'WMT2016 German-English', 'Metric': 'BLEU', 'Score': '40.6'}}, {'LEADERBOARD': {'Task': 'Unsupervised Machine Translation', 'Dataset': 'WMT2016 Romanian-English', 'Metric': 'BLEU', 'Score': '39.5'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (High)', 'Score': '45.5'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (Middle)', 'Score': '58.4'}}, {'LEADERBOARD': {'Task': 'Zero-Shot Learning', 'Dataset': 'Story Cloze', 'Metric': 'Accuracy', 'Score': '72.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '52.5'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '53.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '175'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '41.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '63.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '57.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Tokens (Billions)', 'Score': '300'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '43.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '36.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '42.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '43.2'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '6.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '35.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '49.2'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '46.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '27.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '26'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '13'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '24.3'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '25.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '26.5'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '24.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '25.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '2.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '26.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '30.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '24.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '26.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '24.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '25.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '21.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '25.5'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '40.8'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '50.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '48.8'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '75.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '81.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '76.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '60.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OpenBookQA', 'Metric': 'Accuracy', 'Score': '65.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'DROP Test', 'Metric': 'F1', 'Score': '36.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '71.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Story Cloze', 'Metric': 'Accuracy', 'Score': '87.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'CoQA', 'Metric': 'Overall', 'Score': '85'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '29.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '92'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OBQA', 'Metric': 'Accuracy', 'Score': '57.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'QuAC', 'Metric': 'F1', 'Score': '44.3'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '41.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '25.3'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '14.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '53.2'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '51.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Easy)', 'Metric': 'Accuracy', 'Score': '71.2'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Easy)', 'Metric': 'Accuracy', 'Score': '68.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '70.2'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '49.4'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'F1', 'Score': '52'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '75.6'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A1', 'Score': '36.8'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A2', 'Score': '34'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A3', 'Score': '40.2'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '69%'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'The Pile', 'Metric': 'Bits per byte', 'Score': '0.7177'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '86.4'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Perplexity', 'Score': '1.92'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '76.2'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Perplexity', 'Score': '3.00'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '72.5'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Perplexity', 'Score': '3.56'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '70.3'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Perplexity', 'Score': '4.00'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '67.1'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Perplexity', 'Score': '4.60'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Penn Treebank (Word Level)', 'Metric': 'Test perplexity', 'Score': '20.5'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Penn Treebank (Word Level)', 'Metric': 'Params', 'Score': '175000M'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'WSC', 'Metric': 'Accuracy', 'Score': '80.1'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'Winograd Schema Challenge', 'Metric': 'Accuracy', 'Score': '80.1'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '79.3'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '78.9'}}]
