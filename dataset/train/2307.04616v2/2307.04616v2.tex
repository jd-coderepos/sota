\section{Experiments} \label{section:experiments}

Our code is based on PyTorch \cite{pytorch} and timm \cite{timm}. We use the VOLO \cite{volo} model as our baseline.

\subsection{Evaluation metrics} \label{section:metrics}

In this section, we present the model's performance using various metrics. For gender prediction and age prediction in classification benchmarks, we utilize the classification accuracy metric.

In regression age benchmarks, the model's performance is evaluated based on two metrics: Mean Absolute Error (MAE) and Cumulative Score (CS). MAE is calculated by averaging the absolute differences between the predicted ages and the actual age labels in the testing dataset. CS is computed using the following formula:



Here,  represents the total number of testing examples, while  denotes the count of examples for which the absolute error between the estimated age and the true age does not exceed  years.


\subsection{VOLO Experiments on Open Source Datasets}

First, we conducted experiments on IMDB-clean and UTKFace datasets to establish a good baseline and identify model limitations. In this section original images, annotations and data splits were taken.

For the age estimation task, \textbf{our baseline model, VOLO-D1}, was trained using only the face input. We employed the  optimizer with an initial learning rate of \num{1.5e-5} and a weight decay of \num{5e-5}. The model was trained for 220 epochs individually on both the IMDB-clean and UTKFace datasets. The base learning rate batch size was set to 192. At the start of training, we performed a warmup with  for 25 epochs with gradual increase.


The following data augmentations were applied during training:
\begin{itemize}
    \vspace{-0.2cm}\item RandAugment with a magnitude of 22 and bilinear resizing.
    \vspace{-0.2cm}\item Random bounding box jitter for position and size, both with a magnitude of 0.45.
    \vspace{-0.2cm}\item Reprob with .
    \vspace{-0.2cm}\item Random horizontal flip with .
\end{itemize}
Additionally, we incorporated  and - with .

We performed several experiments, exploring different parameters and loss functions. For age estimation, we tried  loss and  loss, but simple  yielded the best performance.

As shown in Table \ref{table:baseline_age_gender_results} our results are state-of-the-art without any additional data or advanced techniques on IMDB-clean and UTKFace datasets. 

\textbf{For the age \& gender VOLO-D1 model}, we followed the same training process. To address the discrepancy in the magnitudes of loss between age and gender, we weighted the gender loss with . We did not change anything else, including the number of epochs.

By adding a second age output to the model, we expected to observe the same effect as reported in the study \cite{multitask_kendall}, where a single model performs better than multiple separate models, leveraging the benefits of learning two tasks simultaneously. And, indeed, we obtained a significantly better MAE for the age, while also achieving impressive accuracy for gender classification. Please refer to Table \ref{table:baseline_age_gender_results} for the detailed results.


\begin{table*}[t]
\centering
\begin{tabular}{|p{2.1cm}|p{1cm}|
p{1.3cm}|p{0.84cm} p{0.84cm} p{0.84cm} |p{0.84cm} p{0.84cm} p{0.84cm}|p{0.84cm} p{0.84cm} p{0.84cm}|}
\hline
\multicolumn{3}{|c|}{} & \multicolumn{9}{c|}{Tested with} \\ [1ex] 
\multicolumn{3}{|c|}{} & \multicolumn{3}{c|}{Face} & \multicolumn{3}{c|}{Body} & \multicolumn{3}{c|}{Face\&Body} \\ [1ex] 
\hline
\hline
 \multicolumn{1}{|p{2.0cm}|}{Model} & \multicolumn{1}{p{1.1cm}|}{Train Set} & \multicolumn{1}{p{1.3cm}|}{Test Set} & \multicolumn{1}{p{0.83cm}}{MAE} & \multicolumn{1}{p{0.83cm}}{CS@5} & \multicolumn{1}{p{0.86cm}|}{Gender Acc}& \multicolumn{1}{p{0.83cm}}{MAE} & \multicolumn{1}{p{0.83cm}}{CS@5} & \multicolumn{1}{p{0.86cm}|}{Gender Acc} & \multicolumn{1}{p{0.83cm}}{MAE} & \multicolumn{1}{p{0.83cm}}{CS@5} & \multicolumn{1}{p{0.86cm}|}{Gender Acc} \\ [1ex]
\hline
VOLO-D1 & Lagenda & IMDB & 4.10 & 69.71 & \textbf{99.57} & - & - & - & - & - & - \\  
 &  & UTKFace & 3.82 & 72.64 & \textbf{98.87} & - & - & - & - & - & - \\   
 &  & Lagenda & 4.11 & 70.11 & 96.89 & - & - & - & - & - & - \\   
\hline
MiVOLO-D1 & IMDB & IMDB & 4.35 & 67.18 & 99.39 & 6.87 & 46.32 & 96.48 & 4.24 & 68.32 & 99.46 \\   
 &  & UTKFace & 5.12 & 59.10 & 97.66 & 6.36 & 47.74 & 95.57 & 5.10 & 97.72 & 59.46 \\   
 &  & Lagenda & 5.40 & 58.67 & 91.06 & 10.52 & 31.70 & 87.71 & 5.33 & 59.20 & 91.91 \\ 
\hline
MiVOLO-D1 & Lagenda & IMDB & 4.15 & 69.20 & 99.52 & 6.66 & 47.53 & 96.74 & \textbf{4.09} & \textbf{69.72} & 99.55 \\   
 &  & UTKFace & 3.86 & 72.06 & 98.81 & 4.62 & 63.81 & 98.69 & \textbf{3.70} & \textbf{74.16} & 98.84 \\  
 &  & Lagenda & 4.09 & 70.23 & 96.72 & 7.41 & 49.64 & 93.57 & \textbf{3.99} & \textbf{71.27} & \textbf{97.36} \\
\hline
\end{tabular}
\caption{Comparison of multi-input \ModelName-D1 and single-input VOLO-D1 age \& gender models accuracy. \textbf{Bold} indicates the best model for each benchmark. \dag\ marks the model that we release to the public domain. }
\label{table:mivolo_results}
\end{table*}


\subsection{MiVOLO Experiments on Open Source Datasets}

We made some minor adjustments to the training process for the \ModelName\ model. To reduce training time, we initialized the model from a single-input multi-output VOLO checkpoint. We initialized weights of the  block with the same weights as the  block.  The  was initialized with random parameters.

During training, we froze the  block since it was already trained. We trained the model for an additional 400 epochs, incorporating random dropout of the body input with a probability of 0.1, and random dropout of the face input with a probability of 0.5. Face inputs were only dropped for samples with suitable body crops. If a face input was dropped, the model received an empty (zero tensor) input for , and the same for empty body inputs.

These techniques were implemented to adapt the model for various mixed cases and to improve its understanding of input images, resulting in enhanced generalization. We also set the learning rate to \num{1e-5}. To preserve the structural integrity of the data, all augmentations, excluding jitter, are applied simultaneously.

The remaining parts of the training procedure are unchanged.

We conducted experiments on the IMDB-clean dataset using our \ModelName. Table \ref{table:mivolo_results} shows a comparison between the single-input VOLO and the multi-input MiVOLO. The results indicate that the best performance across all benchmarks is achieved by using both face and body crops. The model trained on our dataset consistently outperforms the one trained on IMDB.


To evaluate the quantitative performance of the \ModelName\ when only body images are available, we conducted an experiment where all faces were removed from the data. Additionally, we excluded any images that did not meet our specified requirements mentioned in Section \ref{section:multi_input_data_prep}. For IMDB-clean, UTKFace and Lagenda test datasets retained 84\%, 99.6\% and 89\% of images, respectively. Results are displayed in the Table \ref{table:mivolo_results} and Figure \ref{fig:lagenda_mae} (b).

\subsection{\DatasetNameShort \ experiments}

We repetead all previous experiments on our \DatasetNameShort\ trainset. We trained three variants of the model: VOLO-D1 face-only age, VOLO-D1 face-only age \& gender, and MiVOLO-D1 face + persons age \& gender. We kept all training parameters unchanged, following the same configuration as for the IMDB-clean dataset.

Please refer to Table \ref{table:baseline_age_gender_results} and Table \ref{table:mivolo_results} for the results. As expected, the amount of data played a crucial role in the performance of our \ModelName. We observed significant improvements and achieved SOTA results for the \DatasetNameShort, UTKFace, and IMDB-clean datasets by utilizing the face \& body multi-input approach. Remarkably, we also obtained satisfactory results for body-only inference. 

In Figure \ref{fig:boy_girl_example}, we provide an illustration of a successful recognition result without visible faces in a random picture sourced from the internet.
Model generalizes very well, even though it has never seen images like this with persons shown from the back.

Relationship between MAE and age for final models is shown in Figure \ref{fig:lagenda_mae} (a) and (b).

\subsection{Adience, FairFace, AgeDb benchmarks}

Due to the model's impressive generalization capabilities, we decided to apply \ModelName \ to the AgeDb \cite{moschoglou2017agedb} regression benchmark and to popular classification benchmarks such as FairFace \cite{fairface} and Adience \cite{adience}. As our model was not explicitly trained for classification tasks, we applied our final \ModelName-D1 age \& gender model to FairFace and Adience without any modifications. The only change made was mapping the regression output to classification ranges. As shown in Table \ref{table:classification_results}, we achieved SOTA results for the mentioned datasets without any additional changes.

\begin{table}[h!]
\centering
\begin{tabular}{|p{2.5cm}|p{0.83cm}|p{0.83cm}|p{0.83cm}|p{0.83cm}|} 
 \hline
 \multicolumn{1}{|c|}{Method} & \multicolumn{1}{c|}{Test Set} & \multicolumn{1}{p{0.83cm}|}{Age Acc} & \multicolumn{1}{p{0.83cm}|}{Age MAE} & \multicolumn{1}{p{0.83cm}|}{Gender Acc} \\ [0.5ex] 
 \hline\hline
 FairFace\cite{fairface} & FairFace & 59.70 & & 94.20 \\
 \textbf{MiVOLO-D1 Face\&Body} & FairFace & \textbf{61.07} & & \textbf{95.73} \1ex]
  \hline
 MWR \cite{ordinal_regress} & Adience & 62.60 & & - \\
 AL-ResNets-34 \cite{lstm_age} & Adience & 67.47 & & - \\
 Compacting \cite{compacting} & Adience & - & & 89.66 \\
 Gen MLP \cite{retina_arc} & Adience & - & & 90.66 \\
 \textbf{MiVOLO-D1 Face} & Adience & \textbf{68.69} & & \textbf{96.51} \1ex] 
 \hline
\end{tabular}
\caption{FairFace, Adience, AgeDB validation results using MiVOLO-D1 trained on LAGENDA train set.}
\label{table:classification_results}
\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=8.2cm]{fig/boy_girl_water.png}\\
\caption{An illustration of a case where the work is performed without faces on a random picture obtained from the internet.}
\label{fig:boy_girl_example} 
\end{figure}

\begin{figure}[htp]
\centering
    \subfloat[face \& body]{\includegraphics[width=7cm]{fig/error_imdb_mivolo_lag_crop.png}}
    \hfill
    \subfloat[body only]{\centering
        \includegraphics[width=7cm]{fig/error_mivolo_imdb_body_crop.png}} 
    \caption{Relationship between MAE and age for \ModelName. Tested on \DatasetNameShort\ benchmark using: a) face \& body; b) only body. }\label{fig:lagenda_mae}
\end{figure}
