\documentclass{article}
\usepackage{graphics}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{rotating}


\usepackage{bibunits}
\usepackage{wrapfig}
\usepackage{psfrag}



\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{url}
\usepackage{epsfig}
\usepackage{mathrsfs}
            
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
             
             
\newenvironment{proof}{\noindent {\bf Proof:}}{\hfill}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}


\newcommand{\ignore}[1]{}
             
\newcommand{\vcm}[1][1]{\vspace*{#1 cm}}
\newcommand{\hcm}[1][1]{\hspace*{#1 cm}}

\newcommand{\E}{{\mathbb E\/}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\angbrack}[1]{\left< #1 \right>}
\newcommand{\curlybrack}[1]{\left\{ #1 \right\}}
\newcommand{\sqbrack}[1]{\left[ #1 \right]}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\fr}[2]{\mbox{}}
\newcommand{\modulo}{ {\mbox{ \rm mod } }}
\newcommand{\logstar}{{\log^*}}

\newcommand{\Erdos}{Erd\H{o}s}
\newcommand{\Bollobas}{Bollob\'{a}s}
\newcommand{\Boruvka}{Bor\r{u}vka }
\newcommand{\Boruvkas}{Bor\r{u}vka's }
\newcommand{\Jarnik}{Jarn\'{\i}k }
\newcommand{\Nesetril}{Ne{\u{s}}et{\u{r}}il}

\newcommand{\Ex}{\mbox{\rm Ex}}
\newcommand{\mup}{\bar{m}}
\newcommand{\mdown}{\tilde{m}}
\newcommand{\Seq}{\mathscr{S}}
\newcommand{\Rec}{\mathscr{R}}
\newcommand{\RecDeque}{\mathscr{D}}
\newcommand{\subseq}{\,\prec\,}
\newcommand{\nsubseq}{\,\nprec\,}
\newcommand{\subseqe}{\,\bar{\subseq}\,}
\newcommand{\nsubseqe}{\,\bar{\nsubseq}\,}

\title{Splay Trees, Davenport-Schinzel Sequences,\\
and the Deque Conjecture}

\author{Seth Pettie\\ The University of Michigan}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We introduce a new technique to bound the asymptotic performance of splay trees.
The basic idea is to transcribe, in an indirect fashion, the rotations performed by the splay tree
as a Davenport-Schinzel sequence , none of whose subsequences are isomorphic to
fixed {\em forbidden subsequence}.
We direct this technique towards Tarjan's {\em deque conjecture} and prove that  deque operations
require  time, where  is the minimum number of applications of the inverse-Ackermann
function mapping  to a constant.  We are optimistic that this approach could be directed towards other
open conjectures on splay trees such as the {\em traversal} and {\em split} conjectures.
\end{abstract}

\section{Introduction}

Sleator and Tarjan proposed the {\em splay tree} \cite{ST85} as a self-adjusting alternative to traditional search trees like red-black trees and AVL-trees.
Rather than enforce some type of balance invariant, the splay tree simply adjusts its structure in response to the access pattern by rotating accessed
elements towards the root in a prescribed way; see Figure~\ref{fig:zigzig-zigzag}.  
By letting the access pattern influence its own shape, the splay tree can inadvertently learn to perform
optimally on a variety of access patterns.  For example, the {\em static optimality} theorem states that splay trees are no worse than any fixed search tree.
The {\em working set}, and {\em dynamic finger} theorems show that the access time is logarithmic in the distance to the accessed 
element, where {\em distance} is either temporal (working set \cite{ST85}) or with respect to key-space (dynamic finger \cite{ColeEtal00,Cole00}).
Sleator and Tarjan went a step further and conjectured that splay trees are, to within a constant factor, just as efficient as any dynamic binary search
tree, even one that knows the whole access sequence in advance.  Despite a fair amount of attention over the years, this {\em dynamic optimality} conjecture
is still open.  In fact, there is currently no non-trivial (i.e., sub-logarithmic) bound on the competitiveness of splay trees.
The difficulty of this problem stems from the fact that splay trees were deliberately designed not to ``play along.''  They do not adhere to any notion
of {\em good structure} that we might have and, more to the point, there is no reason to believe that splay trees mimic the behavior of
the optimal dynamic search tree.

\begin{figure}[h!]
\begin{center}
\scalebox{.5}{\includegraphics{zigzig-zigzag.pdf}}
\end{center}
\caption{\label{fig:zigzig-zigzag}Splay trees' restructuring heuristics.  After accessing an element  the tree rotates it to the root
position by repeatedly applying  a {\em zig-zig}, {\em zig-zag}, or {\em zig} as appropriate.
On the left is the zig-zig case:  and its parent  are both left children (or both right children); the edges
 and  are rotated in that order.  On the right is the zig-zag case; the edges  and  are
rotated in that order.  Not depicted is the zig case, when  is the tree root and the edge  is rotated.}
\end{figure}

The renewed interest in the dynamic optimality question is largely due to
Demaine et al.'s invention of {\em tango trees} \cite{DHIP04}.  
By appealing to the {\em interleave} lower bound of Wilbur \cite{Wilber89}
they show that tango trees are -competitive.  Tango trees make use of red-black trees
but it is easy to see that just about any standard binary search tree could be used as a black box in its place.
Wang et al.~\cite{WDS06} (see also \cite{Georg05}) showed that if splay trees are employed
instead of red-black trees it is possible to have -competitiveness and retain some properties
of splay trees, such as  amortized time per access and sequential access in linear time.  
Wang et al.~also extended their data structure to handle insertions and deletions.  

If one's immediate goal is to prove that splay trees are simply -competitive, it suffices to show that
they run in  time on any class of access sequences for which the optimal binary search tree runs in  time.
There is currently no ``theory'' of access sequences whose inherent complexity is linear.  It is, therefore, not too surprising that
all the major open conjectures on splay trees (corollaries of dynamic optimality)
concern sequences whose optimal complexity is linear.  
Whether one's goal is modest or ambitious, i.e., proving sub-logarithmic competitiveness or the full dynamic optimality conjecture,
the first step must be to understand how splay trees behave on very easy access sequences.
We restate below three unresolved conjectures on the optimality of splay trees \cite{Tar85,ST85,Lucas91}.

\begin{description}
\item[Deque Conjecture] Tarjan \cite{Tar85} conjectured that all double-ended queue operations\footnote{Also called a {\em deque}, pronounced ``deck.''} 
(push, pop, and their symmetric counterparts
inject and eject) take  amortized time if implemented with a splay tree.  A push makes the root of the splay tree the right child of a new vertex
and a pop splays the leftmost leaf to the root position and deletes it.  Inject and eject are symmetric.

\item[Traversal Conjecture] Sleator and Tarjan \cite{ST85} conjectured that for two binary search trees  and  (defined on the same node set)
accessing the elements in  by their preorder number in  takes linear time.

\item[Split Conjecture] Lucas conjectured \cite{Lucas91} 
that any sequence of {\em splittings} in a splay tree takes linear time.  A split at  consists of splaying
 to the root and deleting it, leaving two splay trees, each subject to more splittings.
\end{description}

Sundar \cite{Sundar92} established a bound of  on the time required to perform  deque operations, 
where  is the inverse-Ackermann function.  
Lucas \cite{Lucas91} showed that when the initial splay tree is a path (each node a left child), 
 split operations take  time.  Notice that the split conjecture subsumes
a special case of the deque conjecture, where only pops and ejects are allowed.  We are aware of no published 
work concerning the traversal conjecture.

\paragraph{Our Contributions.}
We introduce a new technique in the analysis of splay trees that is fundamentally
different from all previous work on the subject \cite{ST85,Tar85,Sundar92,ColeEtal00,Cole00,Georg04,Elmasry04b}.
The idea is to bound the time taken to perform a sequence of accesses by 
{\em transcribing} the rotations performed by the splay tree into a Davenport-Schinzel sequence , i.e.,
one avoiding a specific forbidden subsequence.  We apply this technique to the deque problem
and show that  deque operations take  time, 
where  is the number of applications of the inverse-Ackermann
function mapping  down to a constant.  This time bound is established by generating 
not one sequence  but a hierarchy of sequences, each of which avoids subsequences isomorphic
to .  Nearly tight bounds on the length of such sequences were given by 
Agarwal, Sharir, and Shor~\cite{ASS89}.  
We believe that a generalized version of this technique should be useful
in resolving other open conjectures on splay trees.  For instance, a particularly clean
way to prove the deque, split, or traversal conjectures would be to transcribe their
rotations as a generalized Davenport-Schinzel sequence with length .
Klazar and Valtr~\cite{KV94} showed that a large family of forbidden subsequences have
a linear extremal function.

\paragraph{Related Work.}
Iacono \cite{Iacono05} defined a weaker notion of dynamic optimality called {\em key independent} optimality.
One assumes that keys are assigned to elements randomly.  The optimal cost of a sequence of operations
is the expected optimal cost over all key assignments.  Iacono showed that any data structure having the
working set property is also optimal in the key independent sense.  
Blum et al.~\cite{BCK03} defined another weaker notion of dynamic optimality called dynamic {\em search} optimality.
In this cost model the (online) search tree can perform any number of rotations for free after each access, i.e.,
it only pays for actually doing searches.
Georgakopoulos ~\cite{Georg04}
showed that splay trees are competitive against a large class of dynamic {\em balanced} binary search trees,
which can be ``self adjusting'' in the sense that they change shape in preparation for future searches.
Iacono defined a {\em unified property} for search structures that subsumes the working set and dynamic finger
properties.  In his data structure~\cite{Iacono01} the access time for an element is logarithmic in its distance, 
where ``distance'' is a natural combination of temporal distance plus key-space distance.  Iacono's data structure \cite{Iacono01}
is not a binary search tree and it is currently open whether {\em any} offline binary search tree has the unified property.
In other words, it is not known to be a corollary of dynamic optimality.

Just after the invention of splay trees \cite{ST85}, Fredman et al.~\cite{F+86} invented the pairing heap
as a kind of self-adjusting version of Fibonacci heaps.  There is no obvious (and still interesting) analogue of dynamic optimality
for priority queues, though Iacono~\cite{Iac00} did show that pairing heaps possess an analogue of the working set property.
See Fredman~\cite{F99} and Pettie~\cite{Pet05a} for the best lower and upper bounds on the performance of pairing heaps.

List maintenance and caching algorithms (such as move-to-front or least-recently-used \cite{ST85b}) 
are sometimes described as being self adjusting heuristics.  In these problems the (asymptotic) dynamic optimality
questions are pretty well understood \cite{ST85b}, though the leading constants have not yet been pinned down~\cite{Albers98,ASW95}.

\paragraph{Organization.} In Section~\ref{sect:equiv} we describe a known reduction \cite{Tar85,Sundar92,Lucas91} 
from the deque problem to a restrictive system of path compressions.  In Section~\ref{sect:not} we define
some notation for Davenport-Schinzel sequences, path compressions, and slowly growing functions.
Section~\ref{sect:pc} introduces a recurrence relation for a type of path compression system and shows how
it can be analyzed easily using bounds on Davenport-Schinzel sequences.  Section~\ref{sect:deque}
gives the proof that  deque operations take  time.


\section{Deque Operations and Path Compression Schemes}\label{sect:equiv}

The relationship between deque operations on a splay tree and halving path compressions
on an arbitrary tree was noted by Lucas~\cite{Lucas90,Lucas91} and implicitly in \cite{Tar85,Sundar92}.  
Let us briefly
go through the steps of the reduction.  At the beginning of a phase we divide the -node splay tree
into left and right halves of size .  The phase ends when one half has been deleted due to pops
and ejects.   We ignore the right half for now and look at the binary tree induced by the left half; call it  and its root .
The root of this tree may or may not correspond to the root of the whole splay tree.  In any case, we imagine
rotating the nodes on the right spine across the root until the root has only one (left) child; call this tree .
Finally, we transform the binary tree  into a general tree  as follows.  The left
child of a vertex in  corresponds to its leftmost child in  and its right child in  corresponds
with its right sibling in .  See Figure~\ref{fig:equiv}.
\begin{figure}[h!]
\begin{center}
\scalebox{.4}{\includegraphics{equiv.pdf}}
\end{center}
\caption{\label{fig:equiv}Top row: the transformation from the splay tree (left nodes black, right nodes right) to  and .
Bottom: the effect of a pop on the splay tree and .}
\end{figure}
Notice that if  and its -parent  lie on the left spine of , and ,
rotating the edge 
 corresponds to making  the leftmost child of its grandparent in .  If  rotating
 makes  the root of  but does not change the structure of  or .  In an almost symmetric
way, if  and  lie on the right spine of  then rotating  in 
corresponds to rotating  in  and making  the leftmost child of its grandparent () in .
Notice that .  Observation~\ref{obs:equiv} summarizes the relationship
between deque operations and path compressions; see also~\cite{Lucas91,Sundar92}.

\begin{observation}\label{obs:equiv}
A pop operation corresponds to a halving path compression in  that begins at the leftmost leaf and terminates at ,
followed by a deletion of the leftmost leaf and a possible {\em root relocation} from  to its leftmost child.
A push operation causes a new leaf  to be added as the leftmost child of  in , followed by a root relocation from  to .
An eject operation corresponds to a halving path compression originating at  and terminating at some ancestor (not necessarily the root of ),
followed by a possible root relocation from  to .  An inject operation has no effect on .
\end{observation}

It is clear that if the amortized cost per deque operation in a phase is  then the overall cost for  deque operations
on an initial -node splay tree is ; see \cite{Tar85,Sundar92,Elmasry04b}.
Using Observation~\ref{obs:equiv} we can (and do) restrict our attention to bounding the total length of a sequence of halving path
compressions up the left spine of an arbitrary rooted tree.  However, we may still lapse into deque terminology.  The terms
``pop'' and ``push'', for instance, are basically equivalent to ``halving path compression'' and ``leaf addition.''

\paragraph{Related Work.}
Restricted forms of path compressions have been studied in a number of situations.
The most well known example is Hart and Sharir's result \cite{HS86} on the equivalence between
-free Davenport-Schinzel sequences and {\em generalized} postordered path compressions;
both have maximum length .  Loebl and \Nesetril{} \cite{LN97} and Lucas~\cite{Lucas90} independently proved
that standard postordered path compressions with the so-called {\em rising roots} condition take linear time.
Buchsbaum, Sundar, and Tarjan~\cite{B+95} generalized this result to what they called deque-ordered path compressions,
again assuming the rising roots condition.  Hart and Sharir~\cite{HS86} have conjectured that the rising roots condition 
is not essential and that standard postordered path compressions take linear time.
The path compressions corresponding to deque operations 
are similar to the special cases studied earlier.  Some differences are that the compressions are halving (not full),
and although the compressions are spinal, due to pushes and ejects they are not performed in postorder
and do not satisfy the rising roots condition.




\section{Notation}\label{sect:not}

We say two sequences are isomorphic if they are the same up to a renaming of symbols.
The relations  and  hold, respectively,
when  is a subsequence of  and when  is isomorphic to a subsequence
of .  A subsequence of  is any sequence derived by deleting symbols from .
A sequece  is called -regular if any two occurrences of the same symbol appear at distance at least .
We denote by  and  the length and alphabet size of , respectively.
Following Klazar~\cite{Klazar02} we let  be the maximum length of -free sequences:

\begin{definition}
\|\sigma\|
\end{definition}

It is known \cite{Klazar92} that , where the  depends on the length
and alphabet size of .  Nearly tight bounds on  are known \cite{ASS89} when
 is of the form .
Here  is the inverse-Ackermann function, which can be defined as follows.
If  is a strictly decreasing function on the positive integers 
, where  and .
Define 
and .  This definition of  differs from others in the literature \cite{Tar75,HS86}
by at most a small constant.

In this paper all trees are rooted and the children of any vertex assigned some left-to-right order.
The trees we deal with are occasionally restructured with {\em path compressions}. 
Let  be the parent of  at some specific time.  If  is a path,
where , performing a {\em total} compression of  means to reassign
, for all .  A {\em halving} compression of  sets
 for all odd .  If  is even a halving compression may set
.  A specific case of interest is when  is the tree root:
setting  causes  to be a tree root as well.
We say that  {\em originates} at  and {\em terminates} at .
A total/halving path compression that does {\em not} terminate at the tree root is {\em stunted}.
The {\em length} of a total/halving path compression is the number of vertices whose
parent pointers are altered.  We never consider compressions with zero length.

A postordering of a tree rooted at , having
children  from left to right, is the concatenation of the postorderings of
the subtrees rooted at  followed by .  The {\em spine} of a tree is the path from
its root to its leftmost leaf.  A path compression is spinal if it affects a subpath of the spine; it need
not include the leftmost leaf nor the tree root.  It is easy to see that a total/halving spinal compression
can be postorder preserving.  For a total compression  we just prepend 
to the preexisting left-right ordering on the children of .  For a halving compression we make  the new leftmost child
of  for all odd .

In order to use a clean inductive argument we look at a restrictive type of instance called
a {\em spinal compression system}.
The initial structure consists of a single path containing a mixture of {\em essential} nodes and
and {\em fluff} nodes.  The tree is manipulated through halving spinal path compressions, 
leaf deletions, and spontaneous compressions.   Let us
go through each of these in turn.  Whenever a fluff node becomes a leaf it is automatically deleted. 
The leftmost essential node (always a leaf) may be deleted at any time.
We are mainly interested in the total length of the ``official'' path compressions,
which are always halving and spinal.  Spontaneous compressions are any postorder preserving path compressions,
the cost of which we need not account for.
Let  be the maximum total length of the official compressions on an instance with 
 essential nodes and  fluff nodes, where at most  of the compressions are stunted.
In Section~\ref{sect:pc} we derive a recursive expression bounding  and in Section~\ref{sect:deque}
we relate  to the time required for deque operations.

\section{Recursive Bounds on Spinal Compression Systems}\label{sect:pc}

Consider an initial instance with  essential nodes and  fluff nodes.  We first divide up the path
into  blocks, each containing  essential nodes, where the bottom most node in each block
is essential.  The sequence of official compressions is partitioned into  epochs, where the th epoch
begins at the first compression when the leftmost leaf belongs to the th block and ends 
at the beginning of the th epoch.  Let  be the set of nodes, not including those in the th block, that are touched by some compression
in the th epoch.  It follows that at the commencement of the th epoch  is a single path; see Figure~\ref{fig:Ij}.
\begin{figure}[h!]
\begin{center}
\scalebox{.4}{\includegraphics{Ij.pdf}}
\end{center}
\caption{\label{fig:Ij}}
\end{figure}
At this time some members of  may be {\em affiliated}
with previous epochs.  It is always the case that nodes affiliated with any  appear as a connected subpath of the spine.
We call an essential node  {\em exposed} if it has no essential ancestor that either shares an affiliation with  or lies
in the same block as .
Let  be the set of essential exposed nodes.  
We call epoch  sparse 
if  and dense otherwise.  
If epoch  is dense we {\em affiliate} all nodes in  with .
We view  as a separate spinal compression system, where  is the set of essential nodes and all others in  
are fluff.  Notice that a subpath of  can be affiliated with  and a previous epoch, say .  If a compression
influences nodes affiliated with both  and  we charge the cost to the th spinal compression system and call its effect
on the th compression system a spontaneous compression.
If the th epoch is sparse we do not affiliate any nodes with .

\begin{lemma}\label{lem:rec-a}
Let  and .

\end{lemma}

\begin{proof}
Consider a compression  occurring in the th epoch.  
We can always break  into three parts: (i) a compression  inside the th block
followed by (ii) an edge  leading from the th block to some vertex in , followed
by (iii) a compression  lying entirely within .  Notice that if  is present it may be
either stunted or unstunted.  
However, if  is present then  (if present) must be an unstunted compression.
We call  {\em short} if it has unit cost, i.e., if only one node changes parent.
Whether  is stunted or not its cost is covered in the  term, 
where  is the number of fluffy nodes and  the
number of compressions that are stunted with respect to block .  
Similarly, if the th epoch is dense the cost of , whether stunted or not, is covered in the term
, where  is the number of stunted compressions that terminate in 
the spinal compression system on .  We account for the cost of  in one of two ways.  If  (and therefore )
is stunted we charge it to the compression; hence the  term.  If not, notice that after every unstunted compression 
the number of nodes whose grandparents are roots or nonexistent increases by at least one.  This can obviously happen
at most  times.
Consider the compression  when the th epoch is sparse.  If  is short we charge its cost to the compression; hence the other  term.
In general,  will intersect one or more previously spawned spinal compression systems.  That is, it can be divided up
into pieces , where for odd ,  intersects an existing compression system.
The costs of  are covered in the  terms.
By doubling this sum we can account for some of ; at the very least this includes those with unit cost.
Over all of epoch , the total length of the other mini-compressions of the form  ( even)
is ;  see Sundar~\cite{Sundar92}.\footnote{In Sundar's terminology each of these mini-compressions is a -cascade for some .}
By tripling the sum  we account for the cost of these mini-compressions as well.
\end{proof}

Lemma~\ref{lem:rec-a} looks as though it may be effectively vacuous.
We are trying to bound the growth of  and all Lemma~\ref{lem:rec-a} can say 
is that it depends on the magnitude of the  sets.  It does not even suggest a trivial
bound on their size.   Our strategy is to transcribe the  sets as a repetition-free
sequence  that avoids a specific forbidden subsequence.  It follows from the results of Agarwal et al.~\cite{ASS89}
that  is nearly linear in the size of its alphabet.
By choosing an appropriate block size  we can apply Lemma~\ref{lem:rec-a} to obtain useful bounds on .

Our transcription method for 
is very similar to the one used by Hart and Sharir \cite{HS86}.  
In Lemma~\ref{lem:abaabba} we show that  and .
Using bounds on  from Agarwal, Sharir, and Shor~\cite{ASS89} we 
are able to show, in Lemmas~\ref{lem:rec-b} and \ref{lem:alphastar} that  is .

Before giving the actual transcription method we give a simpler one, point out why it 
isn't quite what we need, then adjust it.  The transcription is based on an evolving labeling
of the nodes.  A label is simply a sequence of block/epoch indices, listed in {\em descending} order.
Since nodes can belong to several spinal compression systems a node may keep several labels,
one for each system.
At the commencement of the th epoch we
select out of  a subset  with several properties, one of which is that
 has at most one node from any block.  If the th epoch is dense
we prepend  to the label of each node in .  The sequence  consists
of the concatenation of all node labels, where the nodes are given in postorder.  An equivalent definition
is that  begins empty; whenever the leftmost leaf is deleted we append its label to  and continue.
Besides having the property that , we need
 to be repetition-free and contain not too many occurrences of any one symbol,
say, at most  occurrences.  To enforce this, if  we simply split it up into  pieces
and treat each piece as a distinct spinal compression system.  Thus, the number of systems could 
exceed the number of blocks/epochs.  There may be repetitions in  but not too many.  Note that the labels
of nodes appearing in any block share no symbols in common.  Therefore,  can only contain repetitions at block boundaries.
By removing at most  symbols from  we can eliminate all repetitions.  
Using the above two observations, let  be a repetition-free sequence derived from  in 
which no symbol occurs more than  times.

\begin{lemma}\label{lem:abaabba}
For , .
\end{lemma}

\begin{proof}
First note that if the lemma holds for  it holds for  as well.
We show that for any , . 
This would prove the lemma since, for any  and , 
.
Consider the commencement of the th epoch.  If no nodes affiliated with  appear on the path 
then all nodes labeled with  occur, in postorder, strictly before or strictly after all nodes labeled with ; see the left part of 
Figure~\ref{fig:abaabba}.  
\begin{figure}[h!]
\begin{center}
\scalebox{.4}{\includegraphics{abaabba.pdf}}
\end{center}
\caption{\label{fig:abaabba}}
\end{figure}
In this case
.  The more interesting case is when some interval of the nodes in  are affiliated with .
Recall that in our transcription method only exposed nodes were labeled and there could be at most one exposed node
in each set of nodes with a common affiliation.
Thus, only one node affiliated with  can be labeled .   All other nodes labeled
 occur strictly before or strictly after all nodes labeled .  Thus, after the appearance of  in  all nodes
labeled  would have been deleted.  We conclude that .
\end{proof}



Lemma~\ref{lem:rec-b} incorporates the recursive characterization of  from Lemma~\ref{lem:rec-a}
and the -freeness of  established in Lemma~\ref{lem:abaabba}.

\begin{lemma}\label{lem:rec-b}

where 

\end{lemma}

\begin{proof}
Here  represent the sizes of the spawned spinal compression systems.  They would 
correspond with the epochs/blocks if we did not artificially break them up to guarantee that each .
It follows that the number of spawned systems is at most .  By Lemma~\ref{lem:abaabba}
we have .  The term  reflects the cost of the th spawned compression system,
with  essential nodes and at most  fluff nodes.  By \cite{Sundar92} the number of stunted compressions (with greater than unit cost)
terminating in this system is at most .
\end{proof}

\begin{lemma}\label{lem:alphastar}

\end{lemma}

\begin{proof}
Let .  
From the bounds established by Hart and Sharir \cite{HS86}
and Agarwal et al.~\cite{ASS89} we only know that  and .\footnote{Klazar's 
and Valtr \cite{KV94} claimed that  could be had by fiddling with Hart and Sharir's proof \cite{HS86}.
This claim was later retracted \cite{Klazar02}.}
We apply Lemma~\ref{lem:rec-b} with
:


Thus .  Assume inductively that , where 
 is the minimum  such that  is less than some large enough constant.  (We need 
this constant threshold since  is not necessarily decreasing for very small .)


It is easy to see that  for  sufficiently large, from which it follows that
.
\end{proof}

\section{Upper Bounds on Deque Operations}\label{sect:deque}

\begin{theorem}
Starting with an -node splay tree, the time required for  deque operations
is .
\end{theorem}

\begin{proof}
We reduce the deque problem to a set of spinal compression systems.  The main difference
between these systems and the deque problem as a whole is that spinal compression systems
do not allow general insertion/deletion of leaves and the initial tree is always a single path.

We impose a linear order on all nodes that
ever appear in the splay tree.  A pushed node (injected node) precedes (proceeds)
all nodes that are currently in or were in the splay tree in the past.  
As in the general reduction from Section~\ref{sect:equiv} we divide the access sequence into phases.
At the beginning of each phase the nodes are separated into equally sized left and right sets, say  in each set.
We partition the left set into blocks of size .
Let the th period begin at the first pop of an element in the th block and end
when all of the th block has been popped.  Note that the pop that begins the th period
could delete {\em any} element from the th block, not necessarily the first.
Also note that periods are not necessarily disjoint.  For example, just after the th period begins
we could push  elements and then perform a pop.  This would have the effect of starting the
th period without ending the th period, where .  Only one period is {\em active}, namely, the 
one whose block contains the leftmost leaf in the tree.  All other periods are {\em on hold}.

Let  be the {\em set} of nodes that participate in a compression in the active part of the th period,
excluding those that lie in block .  Clearly the nodes in  lie on one path when the th period begins.
However, they are not necessarily contiguous.  For example, if the th period is put on hold, compressions
from later periods could evict non- nodes from the path between two  nodes.
If we put ourselves in one period's point of view we can assume without loss of generality that  {\em does}
form a contiguous segment since, by definition, this period never ``sees'' any non- nodes that would expose 
its misbelief.

Let  be the maximum cost of deque operations (i.e., the cost of push, pop, and eject operations that affect the left set)
when the total number of nodes is .
After the th period begins the th block functions as a mini-deque structure and its total cost at most .
Until the th period begins the block- nodes may participate in compressions, the total cost of which is .
(Whenever a node is evicted from the left spine of the tree induced by block- nodes it can only be seen again
after the th period begins.)
The cost of compressions inside the  sets can be expressed in terms of the  function.
Again, at the commencement of the th period we identify the exposed nodes .
We do nothing if it is sparse () and if it is dense, we affiliate all nodes in  with  and spawn a separate
spinal compression system on .
The proofs of Lemmas~\ref{lem:rec-a} and \ref{lem:rec-b} can easily be adapted to show that:

\noindent where .
A simple proof by induction (along the lines of Lemma~\ref{lem:alphastar}) shows that .
\end{proof}

\section{Conclusion}\label{sect:conclusion}

It's fair to say that previous analyses of splay trees could be characterized
as using potential functions~\cite{ST85,Georg04}, counting 
arguments~\cite{Tar85,Lucas91,Sundar92,Elmasry04b} or a complex synthesis 
of the two~\cite{ColeEtal00,Cole00}.  Although these techniques have been wildly successful
in proving (or nearly proving) the {\em corollaries} of dynamic optimality,
they have had no impact on the dynamic optimality question itself.  
The reason for this is probably the fact that so little is known about the behavior
of the optimal (offline) dynamic binary search tree; see \cite{Lucas88,BCK03}.
It is worth noting that the -competitiveness of tango trees and their
variants \cite{DHIP04,WDS06,Georg05} was established not by comparing them against
an optimal tree but one of Wilber's lower bounds on any binary search tree~\cite{Wilber89}.
(Contrast this technique with the more direct approach used by Sleator and Tarjan~\cite{ST85b}
in their proof of the dynamic optimality of Move-To-Front.)

The strategy taken in this paper is quite different from previous work and is clearly general
enough to be applied to other open problems concerning splay trees.
By transcribing the rotations performed by the splay tree into a Davenport-Schinzel sequence
avoiding  and  we were able to use an off-the-shelf result of Agarwal et al.~\cite{ASS89}
and avoid an unmanageable bookkeeping problem.
One direction for future research is to study the relationship between splay operations,
various transcription methods, and various forbidden subsequences.  
Of particular interest are those forbidden subsequences whose extremal function is linear
since it is these that would be fit for finally settling the deque, split, and traversal conjectures.
See Adamec et al.~\cite{AKV92} and Klazar and Valtr~\cite{KV94}  for a large family of 
linear forbidden subsequences.

{\small
\bibliographystyle{plain}
\bibliography{../references}
}

\end{document}
