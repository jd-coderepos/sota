

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}

\usepackage{paralist, tabularx}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}

\usepackage{amsthm} \newtheorem{innercustomAS}{AS}
\newenvironment{customAS}[1]
  {\renewcommand\theinnercustomAS{#1}\innercustomAS}
  {\endinnercustomAS}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}




\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{thm-restate}
\usepackage{booktabs}       \usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{tikz-cd} 
\usepackage{makecell}
\usepackage{thm-restate}
\usepackage{thmtools}
\usepackage{lipsum}
\usepackage{thm-restate}
\usepackage{hyperref}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsmath}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{enumitem}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage[capitalize]{cleveref}
\usepackage{adjustbox}
\usepackage{makecell, multirow}
\usepackage{stackrel} 
\newcommand{\tabtopvspace}{{\vspace{-0pt}}} 
\newcommand{\tabbottomvspace}{{\vspace{-0pt}}} 
\newcommand{\figtopvspace}{{\vspace{-10pt}}} 
\newcommand{\figbottomvspace}{{\vspace{-10pt}}} 
\newcommand{\eqtopvspace}{{\vspace{-8pt}}} 
\newcommand{\eqbottomvspace}{{\vspace{-3pt}}}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbb}
\newcommand{\mrm}{\mathrm}
\newcommand{\dnorm}{diagonal norm}
\newcommand\f{f_{\textnormal{2d}, n} }
\newcommand{\one}{\mathbf{1}}
\newcommand{\gat}{\text{GATv2}}

\newcommand{\mv}{{\bm{V}}}
\newcommand{\mx}{{\bm{X}}}

\newcommand{\method}{GPS\xspace}
\newcommand\R[1]{\mathbb{R}^{#1} }
\newcommand\norm[1]{\|#1\| }
\newcommand\tmp{\textsf{tmp}}
\newcommand\ps{\textsf{partialsum}}
\newcommand{\ip}[1]{\langle{#1}\rangle}
\newcommand{\gnvn}{\text{gn-vn}}
\newcommand{\vngn}{\text{vn-gn}}
\newcommand{\vn}{\text{vn}}
\newcommand{\gn}{\text{gn}}
\newcommand{\MLP}{\text{MLP}}
\newcommand{\DS}{\text{DeepSets}}
\newcommand{\ds}{\text{ds}}
\newcommand{\out}{\text{out}}
\newcommand\poly[1]{\text{poly}{#1}}
\newcommand{\pool}{\tau}


\newcommand{\gngn}{\text{gn-gn}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\tooned}[1]{\texttt{ReshapeTo1D}{(#1)}}

\newcommand{\gatii}{\texttt{GATv2}\xspace}
\newcommand{\pascal}{\texttt{PascalVOC-SP}\xspace}  \newcommand{\coco}{\texttt{COCO-SP}\xspace}  \newcommand{\pcqmcontact}{\texttt{PCQM-Contact}\xspace}
\newcommand{\pepfunc}{\texttt{Peptides-func}\xspace}
\newcommand{\pepstruct}{\texttt{Peptides-struct}\xspace}
\newcommand{\rbgraph}{\texttt{rag-boundary}\xspace}
\newcommand{\first}[1]{\textbf{#1}}
\newcommand{\second}[1]{#1}



\newtheorem{assumption}{\hspace{0pt}\bf AS\hspace{-0.075cm}}


\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\newcommand{\neighborhood}[2]{\mathcal{N}_{#1}^{#2}}
\newcommand{\extneighborhood}[2]{\overline{\mathcal{N}}_{#1}^{#2}}
\newcommand{\nmax}[2]{{#1}\mbox{\textbf{-max}}_{\mathcal{G}}(\mathbf{{#2}})}
\newcommand{\nmedian}[2]{{#1}\mbox{\textbf{-med}}_{\mathcal{G}}(\mathbf{{#2}})}

\newcommand{\wnmax}[2]{{#1}\mbox{\textbf{-}}\mbox{\textbf{max}}_{\mathcal{G}}^{\mathbf{\omega}}(\mathbf{{#2}})}
\newcommand{\wnmedian}[2]{{#1}\mbox{\textbf{-}}\mbox{\textbf{med}}_{\mathcal{G}}^{\mathbf{\omega}}(\mathbf{{#2}})}






\newenvironment{myproof}[1][]{{\noindent\bf Proof #1: }}
                         {\hfill\QED\medskip}
                         
\newenvironment{myproofnoname}{{\noindent\bf Proof:}}
                         {\hfill\QED\medskip}
                         
\newenvironment{myproof2}[1][\proofname]{\noindent \proof[ \bf{Proof #1}]}{\endproof}


\newenvironment{mylist}
{\begin{list}{}{
   \setlength{\itemsep  }{2pt} \setlength{\parsep    }{0in}
   \setlength{\parskip  }{0in} \setlength{\topsep    }{5pt}
   \setlength{\partopsep}{0in} \setlength{\leftmargin}{11pt}
   \setlength{\labelsep }{5pt} \setlength{\labelwidth}{-5pt}}}
{\end{list}\medskip}

\newcounter{excercise}
\newcounter{excercisepart}
\newcommand \excercise[1]{\addtocounter{excercise}{1} \setcounter{excercisepart}{0} \medskip
						  \noindent {\bf \theexcercise\ \, #1}}
\newcommand \excercisepart[1]{\addtocounter{excercisepart}{1} \medskip
						      \noindent {\it \Alph{excercisepart}\ \, #1}}





 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \definecolor{dark2green}{rgb}{0.1, 0.65, 0.3}
\definecolor{dark2orange}{rgb}{0.9, 0.4, 0.}
\definecolor{dark2purple}{rgb}{0.4, 0.4, 0.8}
\newcommand{\third}[1]{\textbf{\textcolor{dark2purple}{#1}}}


\usepackage[utf8]{inputenc} \usepackage{hyperref}       \usepackage{booktabs}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{multirow}
\usepackage{changepage,threeparttable} \usepackage{makecell}
\usepackage{colortbl}
\usepackage{xspace}
\usepackage{xcolor}  





\renewcommand\theadfont{\bmseries\normalsize}
\renewcommand\theadgape{}
\setcellgapes{2.5pt}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} \newcommand{\chen}[1]{[\textcolor{orange}{CC: #1}]}
\newcommand{\son}[1]{[\textcolor{green}{Son: #1}]}
\newcommand{\yusu}[1]{\textcolor{blue}{[Yusu]: #1}}
\newcommand{\yusumark}[1] {{\textcolor{red}{#1}}}
\icmltitlerunning{On the Connection Between MPNN and Graph Transformer}

\begin{document}

\twocolumn[
\icmltitle{On the Connection Between MPNN and Graph Transformer}





\icmlsetsymbol{equal}{*}
\begin{icmlauthorlist}
\icmlauthor{Chen Cai}{ucsd}
\icmlauthor{Truong Son Hy}{ucsd}
\icmlauthor{Rose Yu}{ucsd}
\icmlauthor{Yusu Wang}{ucsd}
\end{icmlauthorlist}
\icmlaffiliation{ucsd}{University of California San Diego, San Diego, USA}


\icmlcorrespondingauthor{Chen Cai}{c1cai@ucsd.edu}


\icmlkeywords{graph neural networks}

\vskip 0.3in
]





\printAffiliationsAndNotice{} 


\begin{abstract}
Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work \citep{kim2022pure} shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT.  
In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer \citep{choromanski2020rethinking,katharopoulos2020transformers}, then MPNN + VN with only  depth and  width can approximate a self-attention layer in Performer/Linear Transformer. 
Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with  width and  depth can approximate the self-attention layer arbitrarily well, where  is the input feature dimension. Lastly, under some assumptions, we provide an explicit construction of MPNN + VN with  width and  depth approximating the self-attention layer in GT arbitrarily well.
On the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly strong baseline, outperforming GT on the recently proposed Long Range Graph Benchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation on a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer and MPNN on the climate modeling task. 

\end{abstract}


\section{Introduction}
MPNN (Message Passing Neural Network) \cite{gilmer2017neural} has been the leading architecture for processing graph-structured data. Recently, transformers in natural language processing \citep{vaswani2017attention,kalyan2021ammus} and vision \citep{d2021convit,han2022survey} have extended their success to the domain of graphs. There have been several pieces of work \citep{,ying2021transformers,wu2021representing,kreuzer2021rethinking,rampavsek2022recipe, kim2022pure} showing that with careful position embedding \citep{lim2022sign}, graph transformers (GT) can achieve compelling empirical performances on large-scale datasets and start to challenge the dominance of MPNN. 
\begin{figure}[t!]
  \centering
  \includegraphics[width=1\linewidth]{./fig/mpnn+gt.pdf}
\caption{MPNN + VN and Graph Transformers.}
\label{fig:mpnn+gt}
\end{figure}

\begin{table*}[th!]
\centering
\caption{Summary of approximation result of MPNN + VN on self-attention layer.  is the number of nodes and  is the feature dimension of node features. The dependency on  is hidden. }
\tabtopvspace
\label{tab:theoretical-result}
\scalebox{1}{
\begin{tabular}{@{}lllll@{}}
\toprule
 & Depth & Width & Self-Attention & Note \\ \midrule
 \Cref{thm:constant-depth-constant-width} &  &  & Approximate & Approximate self attention in Performer \cite{choromanski2020rethinking} \\ 
 \Cref{thm:constant-depth} &  &  & Full & Leverage the universality of equivariant \DS{} \\
\Cref{thm:constant-width}  &  &  & Full & Explicit construction, strong assumption on  \\
\Cref{prop:gat-v2-selection} &  &  & Full & Explicit construction, more relaxed (but still strong) assumption on  \\ \bottomrule
\end{tabular}
}
\end{table*}

MPNN imposes a sparsity pattern on the computation graph and therefore enjoys linear complexity. It however suffers from well-known over-smoothing \citep{li2018deeper,oono2019graph,cai2020note} and over-squashing \citep{alon2020bottleneck,topping2021understanding} issues, limiting its usage on long-range modeling tasks where the label of one node depends on features of nodes far away. GT relies purely on position embedding to encode the graph structure and uses vanilla transformers on top. \footnote{GT in this paper refers to the practice of tokenizing graph nodes and applying standard transformers on top \citep{ying2021transformers,kim2022pure}. There exists a more sophisticated GT \citep{kreuzer2021rethinking} that further conditions attention on edge types but it is not considered in this paper. } It models all pairwise interactions directly in one layer, making it computationally more expensive. Compared to MPNN, GT shows promising results on tasks where modeling long-range interaction is the key, but the quadratic complexity of self-attention in GT limits its usage to graphs of medium size. Scaling up GT to large graphs remains an active research area \citep{wunodeformer}. 


Theoretically, it has been shown that graph transformers can be powerful graph learners \citep{kim2022pure}, i.e., graph transformers with appropriate choice of token embeddings have the capacity of approximating linear permutation equivariant basis, and therefore can approximate 2-IGN (Invariant Graph Network), a powerful architecture that is at least as expressive as MPNN \citep{maron2018invariant}. This raises an important question that \textit{whether GT is strictly more powerful than MPNN}. Can we approximate GT with MPNN?



One common intuition of the advantage of GT over MPNN is its ability to model long-range interaction more effectively. However, from the MPNN side, one can resort to a simple trick to escape locality constraints for effective long-range modeling: the use of an additional \emph{virtual node (VN)} that connects to all input graph nodes. On a high level, MPNN + VN augments the existing graph with one virtual node, which acts like global memory for every node exchanging messages with other nodes. Empirically this simple trick has been observed to improve the MPNN and has been widely adopted \citep{gilmer2017neural,hu2020open,hu2021ogb} since the early beginning of MPNN \citep{gilmer2017neural, battaglia2018relational}. However, there is very little theoretical study of MPNN + VN \citep{hwanganalysis}. 

In this work, we study the theoretical property of MPNN + VN, and its connection to GT. We systematically study the representation power of MPNN + VN, both for certain approximate self-attention and for the full self-attention layer, and provide a depth-width trade-off, summarized in \Cref{tab:theoretical-result}. In particular, 
\begin{itemize}
\item With  depth and  width, MPNN + VN can approximate one self-attention layer of Performer \citep{choromanski2020rethinking} and Linear Transformer \citep{katharopoulos2020transformers}, a type of linear transformers \citep{tay2020efficient}. \item Via a link between MPNN + VN with \DS{} \citep{zaheer2017deep}, we prove MPNN + VN with  depth and  width ( is the input feature dimension) is permutation equivariant universal, implying it can approximate self-attention layer and even full-transformers.  


\item Under certain assumptions on node features, we prove an explicit construction of  depth  width MPNN + VN approximating 1 self-attention layer arbitrarily well on graphs of size . 
Unfortunately, the assumptions on node features are rather strong, and whether we can alleviate them will be an interesting future direction to explore. 


\item Empirically, we show 1) that MPNN + VN works surprisingly well on the recently proposed LRGB (long-range graph benchmarks) datasets \citep{dwivedi2022long}, which arguably require long-range interaction reasoning to achieve strong performance 2) our implementation of MPNN + VN is able to further improve the early implementation of MPNN + VN on OGB datasets and 3) MPNN + VN outperforms Linear Transformer \citep{katharopoulos2020transformers} and MPNN on the climate modeling task. \end{itemize}


\section{Related Work}
\textbf{Virtual node in MPNN.}
The virtual node augments the graph with an additional node to facilitate the information exchange among all pairs of nodes. It is a heuristic proposed in \cite{gilmer2017neural} and has been observed to improve the performance in different tasks \citep{hu2021ogb,hu2020open}. Surprisingly, its theoretical properties have received little study. To the best of our knowledge, only a recent paper \citep{hwanganalysis} analyzed the role of the virtual node in the link prediction setting in terms of 1) expressiveness
of the learned link representation and 2) the potential impact on under-reaching and over-smoothing. 

\textbf{Graph transformer.}
Because of the great successes of Transformers in natural language processing (NLP) \citep{vaswani2017attention,wolf2020transformers} and recently in computer vision \citep{dosovitskiy2020image,d2021convit,liu2021swin}, there is great interest in extending transformers for graphs \citep{muller2023attending}. One common belief of advantage of graph transformer over MPNN is its capacity in capturing long-range interactions while alleviating over-smoothing \citep{li2018deeper,oono2019graph,cai2020note} and over-squashing in MPNN \citep{alon2020bottleneck,topping2021understanding}. 

Fully-connected Graph transformer \citep{dwivedi2020generalization} was introduced with eigenvectors of graph Laplacian as the node positional encoding (PE). Various follow-up works proposed different ways of PE to improve GT, ranging from an invariant aggregation of Laplacian's eigenvectors in SAN \citep{kreuzer2021rethinking}, pair-wise graph distances in Graphormer \citep{ying2021transformers}, relative PE derived from diffusion kernels in GraphiT \citep{mialon2021graphit}, and recently Sign and Basis Net \citep{lim2022sign} with a principled way of handling sign and basis invariance. 
Other lines of research in GT include combining MPNN and GT  \citep{wu2021representing,rampavsek2022recipe}, encoding the substructures \citep{chen2022structure}, GT for directed graphs \citep{geisler2023transformers}, and efficient graph transformers for large graphs \citep{wunodeformer}.

\textbf{Deep Learning on Sets.} Janossy pooling \citep{murphy2018janossy} is a framework to build permutation invariant architecture for sets using permuting \& averaging paradigm while limiting the number of elements in permutations to be .  Under this framework, \DS{} \citep{zaheer2017deep} and PointNet \citep{qi2017pointnet} are recovered as the case of . For case , self-attention and Relation Network \citep{santoro2017simple} are recovered \citep{wagstaff2022universal}. Although \DS{} and Relation Network \citep{santoro2017simple} are both shown to be universal permutation invariant, recent work \citep{zweig2022exponential} provides a finer characterization on the representation gap between the two architectures.  



\section{Preliminaries}
We denote  the concatenation of graph node features and positional encodings, where node  has feature . When necessary, we use  to denote the node 's feature at depth .  Let  be the space of multisets of vectors in . We use  to denote the space of node features and the  be the projection of  on -th coordinate.  denotes the 2-norm.  denotes the concatenation of .  stands for the set .


\begin{definition}[attention]
We denote key and query matrix as , and value matrix as  \footnote{For simplicity, we assume the output dimension of self-attention is the same as the input dimension. All theoretical results can be extended to the case where the output dimension is different from .}. Attention score between two vectors  is defined as . We denote  as the space of attention  for different . We also define unnormalized attention score  to be .
Self attention layer is a matrix function   of the following form: . \end{definition}

\subsection{MPNN Layer}
\begin{definition}[MPNN layer \citep{gilmer2017neural}]
An MPNN layer on a graph  with node features  at -th layer and edge features  is of the following form


Here  is update function,
 is message function where  is the edge feature dimension, 
 is permutation invariant aggregation function 
and  is the neighbors of node  in .
Update/message/aggregation functions are usually parametrized by neural networks. For graphs of different types of edges and nodes, one can further extend MPNN to the heterogeneous setting.  We use  to index graph nodes and  to denote the virtual node. 
\end{definition}






\begin{definition}[heterogeneous MPNN + VN layer]\label{def-hetero-mpnn-vn-layer} 
The heterogeneous MPNN + VN layer operates on two types of nodes: 1) virtual node and 2) graph nodes, denoted as \text{vn} and \text{gn}, and three types of edges: 1) \text{vn}-\text{gn} edge and 2) \text{gn}-\text{gn} edges and 3) \text{gn}-\text{vn} edges. It has the following form


for the virtual node, and 

for graph node. Here  for graph node  is the virtual node and  is the set of neighboring graph nodes.
\end{definition}
Our proof of approximating self-attention layer  with MPNN layers does not use the graph topology. Next, we introduce a simplified heterogeneous MPNN + VN layer, which will be used in the proof. It is easy to see that setting  to be 0 in \Cref{def-hetero-mpnn-vn-layer} recovers the simplified heterogeneous MPNN + VN layer.

\begin{definition}[simplified heterogeneous MPNN + VN layer]
\label{def:simplified-hetero-mpnn-vn}
A simplified heterogeneous MPNN + VN layer is the same as a heterogeneous MPNN + VN layer in \Cref{def-hetero-mpnn-vn-layer} except we set  to be 0. I.e., we have

for the virtual node, and

for graph nodes. 
\end{definition}

Intuitively, adding the virtual node (VN) to MPNN makes it easy to compute certain quantities, for example, the mean of node features (which is hard for standard MPNN unless the depth is proportional to the diameter of the graph). Using VN thus makes it easy to implement for example the mean subtraction, which helps reduce over-smoothing and improves the performance of GNN \citep{yang2020revisiting,zhao2019pairnorm}. See more connection between MPNN + VN and over-smoothing in \Cref{subsec:over-smoothing}. 




\subsection{Assumptions}
We have two mild assumptions on feature space  and the regularity of target function .

\begin{assumption}\label{AS-2} . This implies  is compact.  
\end{assumption} 

\begin{assumption}\label{AS-3}   for target layer . Combined with AS\ref{AS-2} on , this means  is both upper and lower bounded, which further implies  be both upper bounded and lower bounded. 
\end{assumption}
   

\section{-depth -width MPNN + VN for unbiased approximation of attention}
\label{sec:shallow-narrow-attention}
The standard self-attention takes  computational time, therefore not scalable for large graphs. Reducing the computational complexity of self-attention in Transformer is active research \citep{tay2020efficient}. 
In this section, we consider self-attention in a specific type of efficient transformers, Performer \citep{choromanski2020rethinking} and Linear Transformer \citep{katharopoulos2020transformers}. 



One full self-attention layer  is of the following form


where  is the softmax kernel . The kernel function can be approximated via  where the first equation is by Mercer's theorem and  is a low-dimensional feature map with random transformation. For Performer \citep{choromanski2020rethinking}, the choice of  is taken as  where  is i.i.d sampled random variable. For Linear Transformer \citep{katharopoulos2020transformers}, . 

By switching  to be , and denote , the approximated version of \Cref{equ:attention-kernel-repr} by Performer and Linear Transformer becomes 

where we use the matrix multiplication association rule to derive the second equality. 

The key advantage of \Cref{equ:modified-layer} is that  and  can be approximated by the virtual node, and shared for all graph nodes, using only  layers of MPNNs.  
We denote the self-attention layer of this form in \Cref{equ:modified-layer} as . Linear Transformer differs from Performer by choosing a different form of  in its self-attention layer .  

In particular, the VN will approximate  and , and represent it as its feature. Both  and  can be approximated arbitrarily well by an MLP with constant width (constant in  but can be exponential in ) and depth. Note that  but can be reshaped to 1 dimensional feature vector.

More specifically, the initial feature for the virtual node is , where  is the dimension of node features and  is the number of random projections .  
Message function + aggregation function for virtual node 
 is

 where  flattens a 2D matrix to a 1D vector in raster order. This function can be arbitrarily approximated  by MLP. Note that the virtual node's feature dimension is  (where recall  is the dimension of the feature map  used in the linear transformer/Performer), which is larger than the dimension of the graph node . This is consistent with the early intuition that the virtual node might be overloaded when passing information among nodes. The update function for virtual node   is just coping the second argument, which can be exactly implemented by MLP. 

VN then sends its message back to all other nodes, where each graph node  applies the update function  of the form

 to update the graph node feature. 

As the update function  can not be computed exactly in MLP, what is left is to show that error induced by using MLP to approximate  and  in \Cref{eq:vn-gn} and \Cref{eq:gn} can be made arbitrarily small. 


\begin{restatable}{theorem}{doubleconstant}
\label{thm:constant-depth-constant-width}
Under the AS\ref{AS-2} and AS\ref{AS-3}, MPNN + VN of  width and  depth can approximate  and  arbitrarily well. 
\end{restatable}

\begin{proof}
We first prove the case of .
We can decompose our target function as the composition of ,  and . 
By the uniform continuity of the functions, it suffices to show that 1) we can approximate , 2) we can approximate operations in  and  arbitrarily well on the compact domain, and 3) the denominator  is uniformly lower bounded by a positive number for any node features in .

For 1), each component of  is continuous and all inputs  lie in the compact domain so  can be approximated arbitrarily well by MLP with  width and  depth \citep{cybenko1989approximation}. 


For 2), we need to approximate the operations in  and , i.e., approximate multiplication, and vector-scalar division arbitrarily well.  As all those operations are continuous, it boils down to showing that all operands lie in a compact domain. By assumption AS\ref{AS-2} and AS\ref{AS-3} on  and input feature , we know that  lies in a compact domain for all graph nodes . As  is continuous, this implies that  lies in a compact domain ( is fixed), therefore the numerator lies in a compact domain. Lastly, since all operations do not involve , the depth and width are constant in .  

For 3), it is easy to see that  is always positive.  We just need to show that the denominator is bound from below by a positive constant. For Performer,  where . As all norm of input  to  is upper bounded by AS\ref{AS-2},  is lower bounded. As  is fixed, we know that , which implies that  is lower bounded by  which further implies that  is lower bounded. This means that  is lower bounded. 

For Linear Transformer, the proof is essentially the same as above. We only need to show that  is continuous and positive, which is indeed the case. 
\end{proof}

Besides Performers, there are many other different ways of obtaining linear complexity. In \Cref{subsec:ohter-linear-transformer}, we discuss the limitation of MPNN + VN on approximating other types of efficient transformers such as Linformer \citep{wang2020linformer} and Sparse Transformer \citep{child2019generating}.  

\section{ depth  width MPNN + VN}
\label{sec:shadow-wide-mpnn}

\begin{figure}[t!]
  \centering
  \includegraphics[width=1\linewidth]{./fig/connection.pdf}
\caption{The link between MPNN and GT is drawn via \DS{} in \Cref{sec:shadow-wide-mpnn} of our paper and Invariant Graph Network (IGN) in \citet{kim2022pure}. Interestingly, IGN is a generalization of \DS{} \citep{maron2018invariant}. }
\label{fig:mpnn+gt}
\end{figure}

We have shown that the MPNN + VN can approximate self-attention in Performer and Linear Transformer using only  depth and  width. 
One may naturally wonder whether MPNN + VN can approximate the self-attention layer in the \textit{full} transformer. In this section,
we show that MPNN + VN with  depth (number of layers), but with  width, can approximate 1 self-attention layer (and full transformer) arbitrarily well. 

The main observation is that MPNN + VN is able to exactly simulate (not just approximate) equivariant \DS{} \citep{zaheer2017deep}, which is proved to be universal in approximating any permutation invariant/equivariant maps \citep{zaheer2017deep,segol2019universal}. Since the self-attention layer is permutation equivariant, this implies that MPNN + VN can approximate the self-attention layer (and full transformer) with  depth and  width following a result on \DS{} from \citet{segol2019universal}.

We first introduce the permutation equivariant map, equivariant \DS{}, and permutation equivariant universality.

\begin{definition}[permutation equivariant map]
A map  satisfying  for all  and  is called permutation equivariant.
\end{definition}

\begin{definition}[equivariant \DS{} of  \citet{zaheer2017deep}]
Equivariant \DS{} has the following form , where  is a linear permutation equivariant layer and  is a nonlinear layer such as ReLU. 
The linear permutation equivariant layer in \DS{} has the following form , where ,  is the weights and bias in layer , and  is ReLU. \end{definition}

\begin{definition}[permutation equivariant universality]
\label{def:ds-universality}
Given a compact domain  of , permutation equivariant universality of a model  means that for every permutation equivariant continuous function  defined over , and any , there exists a choice of  (i.e., network depth),  (i.e., network width at layer ) and the trainable parameters of  so that  for all .
\end{definition}

The universality of equivariant \DS{} is stated as follows. 

\begin{theorem}[\citet{segol2019universal}]
\label{thm:ds-universality}
\DS{} with constant layer is universal. Using ReLU activation the width  ( is the width for -th layer of \DS{}) required for universal permutation equivariant
network satisfies .
\end{theorem}

We are now ready to state our main theorem. 

\begin{restatable}{theorem}{constantdepth}
\label{thm:constant-depth} 
MPNN + VN can simulate (not just approximate) equivariant \DS{}: . The depth and width of MPNN + VN needed to simulate \DS{} is up to a constant factor of the depth and width of \DS{}. 
This implies that MPNN + VN of  depth and  width is permutation equivariant universal, and can approximate self-attention layer and transformers arbitrarily well. 
\end{restatable}

\begin{table*}[t!]
    \caption{Baselines for \pepfunc (graph classification) and \pepstruct (graph regression). The performance metric is Average Precision (AP) for classification and MAE for regression. \textbf{Bold}: Best score.} \label{tab:experiments_peptides}
    \tabtopvspace
    \begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering
   \scalebox{0.8}{    
    \setlength\tabcolsep{4pt} \begin{tabular}{l c c c c c}\toprule
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{\# Params.}} & \multicolumn{2}{c}{\pepfunc} & \multicolumn{2}{c}{\pepstruct} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
                                    &                                      & \textbf{Test AP before VN}  & \textbf{Test AP after VN }                                    & \textbf{Test MAE before VN} & \textbf{Test MAE after VN } \\\midrule
    GCN                             & 508k                                 & 0.59300.0023            & 0.66230.0038                                                       & 0.34960.0013           & \first{0.24880.0021} \\
GINE                            & 476k                                 & 0.54980.0079            & 0.63460.0071                                                       & 0.35470.0045           & 0.25840.0011 \\
    GatedGCN                        & 509k                                 & 0.58640.0077            & 0.66350.0024                                                       & 0.34200.0013           & 0.25230.0016 \\
    GatedGCN+RWSE                   & 506k                                 & 0.60690.0035            & \first{0.66850.0062}                                               & 0.33570.0006           & 0.25290.0009 \\ \midrule
    Transformer+LapPE               & 488k                                 & 0.63260.0126            & -                                                                       & 0.25290.0016           & - \\
    SAN+LapPE                       & 493k                                 & 0.63840.0121            & -                                                                       & 0.26830.0043           & - \\
    SAN+RWSE                        & 500k                                 & 0.64390.0075            & -                                                                       & 0.25450.0012           & - \\
    \bottomrule
    \end{tabular}
}
    \end{adjustwidth}
\end{table*}


\begin{proof}
Equivariant \DS{} has the following form , where  is the linear permutation equivariant layer and  is an entrywise nonlinear activation layer.
Recall that the linear equivariant layer has the form . 
As one can use the same nonlinear entrywise activation layer  in MPNN + VN, it suffices to prove that MPNN + VN can compute linear permutation equivariant layer . Now we show that 2 layers of MPNN + VN can exactly simulate any given linear permutation equivariant layer . 


Specifically, at layer 0, we initialized the node features as follows: The VN node feature is set to 0, while the node feature for the -th graph node is set up as .  

At layer 1: VN node feature is , average of node features. The collection of features over  graph node feature is . 
We only need to transform graph node features by a linear transformation, and set the VN feature as the average of graph node features in the last iteration. Both can be exactly implemented in \Cref{def:simplified-hetero-mpnn-vn} of simplified heterogeneous MPNN + VN. 

At layer 2: VN node feature is set to be 0, and the graph node feature is . Here we only need to perform the matrix multiplication of the VN feature with , as well as add a bias . This can be done by implementing a linear function for . 

It is easy to see the width required for MPNN + VN to simulate \DS{} is constant.
Thus, one can use 2 layers of MPNN + VN to compute linear permutation equivariant layer , which implies that MPNN + VN can simulate 1 layer of \DS{} exactly with constant depth and constant width (independent of ). Then by the universality of \DS{}, stated in \Cref{thm:ds-universality}, we conclude that MPNN + VN is also permutation equivariant universal, which implies that the constant layer of MPNN + VN with  width is able to approximate any continuous equivariant maps. As the self-attention layer  and full transformer are both continuous and equivariant, they can be approximated by MPNN + VN arbitrarily well.      
\end{proof}
Thanks to the connection between MPNN + VN with \DS{}, there is no extra assumption on  except for being compact. The drawback on the other hand is that the upper bound on the computational complexity needed to approximate the self-attention with wide MPNN + VN is worse than directly computing self-attention when . 

\section{ depth  width MPNN + VN}
\label{sec:deep-vn}


The previous section shows that we can approximate a full attention layer in Transformer using MPNN with  depth but  width where  is the number of nodes and  is the dimension of node features. 
In practice, it is not desirable to have the width depend on the graph size. 

In this section, we hope to study MPNN + VNs with  width and their ability to approximate a self-attention layer in the Transformer. However, this appears to be much more challenging. Our result in this section only shows that for a rather restrictive family of input graphs (see Assumption \ref{AS-1} below), we can approximate a full self-attention layer of transformer with an MPNN + VN of  width and  depth. We leave the question of MPNN + VN's ability in approximate transformers for more general families of graphs for future investigation.








We first introduce the notion of  separable node features. This is needed to ensure that VN can approximately select one node feature to process at each iteration with attention , the self-attention in the virtual node. 









 \begin{definition}[ separable by ]
\label{vdeltaseparable}
Given a graph  of size  and a fixed  
and , we say node feature  of  is 
 separable by some  if the following holds. For any node feature , there exist weights  in attention score  
such that . We say set  is  
separable by  if every element  is  separable by .
 \end{definition}









\begin{table*}[t]
    \caption{Test performance in graph-level OGB benchmarks \citep{hu2020open}. Shown is the mean~~s.d.~of 10 runs. }
    \label{tab:results_ogb}
    \tabtopvspace
    \centering
\fontsize{8.5pt}{8.5pt}\selectfont
    \begin{tabular}{lccccc}\toprule
    \multirow{2}{*}{\textbf{Model}} &\textbf{ogbg-molhiv} &\textbf{ogbg-molpcba} &\textbf{ogbg-ppa} &\textbf{ogbg-code2} \\\cmidrule{2-5}
                               & \textbf{AUROC }    & \textbf{Avg.~Precision } & \textbf{Accuracy } & \textbf{F1 score } \\\midrule
    GCN                        & 0.7606  0.0097          & 0.2020  0.0024          & 0.6839  0.0084          & 0.1507  0.0018 \\
    GCN+virtual node           & 0.7599  0.0119          & 0.2424  0.0034          & 0.6857  0.0061          & 0.1595  0.0018 \\
    GIN                        & 0.7558  0.0140          & 0.2266  0.0028          & 0.6892  0.0100          & 0.1495  0.0023 \\
    GIN+virtual node           & 0.7707  0.0149          & 0.2703  0.0023          & 0.7037  0.0107          & 0.1581  0.0026 \\
    \midrule
SAN                        & 0.7785  0.2470          & 0.2765  0.0042          & --                           & -- \\
GraphTrans (GCN-Virtual)   & --                           & 0.2761  0.0029          & --                           & 0.1830  0.0024 \\
    K-Subtree SAT              & --                           & --                           & 0.7522  0.0056          & 0.1937  0.0028 \\\method                    & 0.7880  0.0101          & 0.2907  0.0028          & 0.8015  0.0033          & 0.1894  0.0024 \\ \midrule
    MPNN + VN + NoPE           & 0.7676  0.0172          & 0.2823  0.0026          & 0.8055  0.0038          & 0.1727  0.0017             & \\
    MPNN + VN + PE             & 0.7687  0.0136          & 0.2848  0.0026          & 0.8027  0.0026          & 0.1719  0.0013             & \\
    \bottomrule
    \end{tabular}
\end{table*}

The use of  separability is to approximate hard selection function arbitrarily well, which is stated below and proved in \Cref{subsec:assumptions}. 
\begin{restatable}[approximate hard selection]{lemma}{UniformSelection}
\label{lemma-uniform-selection}
Given  is  separable by  for some fixed ,  and , the following holds. For any  and , there exists a set of attention weights  in -th layer of MPNN + VN such that  for any . In other words, we can approximate a hard selection function  arbitrarily well on  by setting .  \end{restatable}



With the notation set up, We now state an extra assumption needed for deep MPNN + VN case and the main theorem. 
\begin{assumption}\label{AS-1} 
 is  separable by  for some fixed ,  and . 
\end{assumption}



\begin{restatable}{theorem}{mainthm}
\label{thm:constant-width}
Assume AS 1-3 hold for the compact set  and . Given any graph  of size  with node features , and a self-attention layer  on  (fix  in ), there exists a  layer of heterogeneous MPNN + VN with the specific aggregate/update/message function that can approximate  on  arbitrarily well. 
\end{restatable}

The proof is presented in the \Cref{sec:approximate-full-self-attention}. On the high level, we can design an MPNN + VN where the -th layer will select , an approximation of  via attention mechanism, enabled by \Cref{lemma-uniform-selection}, and send  to the virtual node. Virtual node will then pass the  to all graph nodes and computes the approximation of . Repeat such procedures  times for all graph nodes, and finally, use the last layer for attention normalization. A slight relaxation of AS\ref{AS-1} is also provided in the appendix. 













\begin{table*}[th!]
\caption{Evaluation on PCQM4Mv2~\cite{hu2021ogb} dataset.
For \method evaluation, we treated the \emph{validation} set of the dataset as a test set, since the \emph{test-dev} set labels are private.
    }
    \label{tab:results_pcqm4m}
    \centering
\fontsize{8.0pt}{8.0pt}\selectfont
    \begin{tabular}{lccccc}\toprule
    \label{tab:pcqm}
    \multirow{2}{*}{\textbf{Model}} &\multicolumn{3}{c}{\textbf{PCQM4Mv2}} \\\cmidrule{2-5}
    &\textbf{Test-dev MAE } &\textbf{Validation MAE } &\textbf{Training MAE} &\textbf{\# Param.} \\\midrule
    GCN &0.1398 &0.1379 & n/a &2.0M \\
    GCN-virtual &0.1152 &0.1153 & n/a &4.9M \\
    GIN &0.1218 &0.1195 & n/a &3.8M \\
    GIN-virtual &0.1084 &0.1083 & n/a &6.7M \\\midrule
    GRPE~\citep{park2022grpe} &0.0898 &0.0890 & n/a &46.2M \\
    EGT~\citep{hussain2022global} &0.0872 &0.0869 & n/a &89.3M \\
    Graphormer~\citep{shi2022benchmarking} &n/a &0.0864 &0.0348 & 48.3M \\\method-small &n/a &0.0938 &0.0653 &6.2M \\
    \method-medium &n/a &0.0858 &0.0726  &19.4M \\ \midrule
    MPNN + VN + PE (small)  &n/a &0.0942 &0.0617 &5.2M \\
    MPNN + VN + PE (medium)  &n/a &0.0867 &0.0703 &16.4M \\
    MPNN + VN + NoPE (small)  &n/a &0.0967 &0.0576 &5.2M \\
    MPNN + VN + NoPE (medium)  &n/a &0.0889 &0.0693 &16.4M \\
    \bottomrule
    \end{tabular}
\end{table*}

\section{Experiments}
We benchmark MPNN + VN for three tasks, long range interaction modeling in \Cref{subsec:lrgb}, OGB regression tasks in \Cref{subsec:ogb}, and focasting sea surface temperature in \Cref{sec:climate}. The code is available \url{https://github.com/Chen-Cai-OSU/MPNN-GT-Connection}. 

\subsection{MPNN + VN for LRGB Datasets}
\label{subsec:lrgb}
We experiment with MPNN + VN for Long Range Graph Benchmark (LRGB) datasets. Original paper \citep{dwivedi2022long} observes that GT outperforms MPNN on 4 out of 5 datasets, among which GT shows significant improvement over MPNN on \pepfunc and \pepstruct for all MPNNs. To test the effectiveness of the virtual node, we take the original code and modify the graph topology by adding a virtual node and keeping the hyperparameters of all models unchanged. 

Results are in \Cref{tab:experiments_peptides}. Interestingly, such a simple change can boost MPNN + VN by a large margin on \pepfunc and \pepstruct. Notably, with the addition of VN, GatedGCN + RWSE (random-walk structural encoding) after augmented by VN {\bf outperforms all transformers} on \pepfunc, and GCN outperforms transformers on \pepstruct. 

\subsection{Stronger MPNN + VN Implementation}
\label{subsec:ogb}
Next, by leveraging the modularized implementation from GraphGPS \citep{rampavsek2022recipe}, we implemented a version of MPNN + VN with/without extra positional embedding. Our goal is not to achieve SOTA but instead to push the limit of MPNN + VN and better understand the source of the performance gain for GT. 
In particular, we replace the GlobalAttention Module in GraphGPS with \DS{}, which is equivalent to one specific version of MPNN + VN. We tested this specific version of MPNN + VN on 4 OGB datasets, both with and without the use of positional embedding. The results are reported in Table \ref{tab:results_ogb}. Interestingly, even without the extra position embedding, our MPNN + VN is able to further improve over the previous GCN + VN \& GIN + VN implementation. 
The improvement on \textbf{ogbg-ppa} is particularly impressive, which is from 0.7037 to 0.8055. 
Furthermore, it is important to note that while MPNN + VN does not necessarily outperform GraphGPS, which is a state-of-the-art architecture using both MPNN, Position/structure encoding and Transformer, the difference is quite small -- this however, is achieved by a simple MPNN + VN architecture. 

We also test MPNN + VN on large-scale molecule datasets PCQMv2, which has 529,434 molecule graphs. 
We followed \citep{rampavsek2022recipe}
and used the original validation set as the test set, while we left out random 150K molecules for our validation set. 
As we can see from \Cref{tab:pcqm}, MPNN + VN + NoPE performs significantly better than the early MPNN + VN implementation: GIN + VN and GCN + VN. 
The performance gap between GPS on the other hand is rather small: 0.0938 (GPS) vs. 0.0942 (MPNN + VN + PE) for the small model and 0.0858 (GPS) vs. 0.0867 (MPNN + VN + PE) for the medium model. 


\subsection{Forecasting Sea Surface Temperature} \label{sec:climate}

In this experiment, we apply our MPNN + VN model to forecast sea surface temperature (SST). We are particularly interested in the empirical comparison between MPNN + VN and Linear Transformer \citep{katharopoulos-et-al-2020} as according to \Cref{sec:shallow-narrow-attention}, MPNN + VN theoretically can approximate Linear Transformer.

In particular, from the DOISST data proposed by \citep{ImprovementsoftheDailyOptimumInterpolationSeaSurfaceTemperatureDOISSTVersion21}, we construct a dataset of daily SST in the Pacific Ocean from 1982 to 2021, in the region of longitudes from  to  and latitudes from  to . Following the procedure from \citep{de2018deep,deBezenac2019} and \citet{wang2022metalearning}, we divide the region into 11 batches of equal size with 30 longitudes and 30 latitudes at 0.5-degree resolution, that can be represented as a graph of 900 nodes. 
The tasks are to predict the next 4 weeks, 2 weeks and 1 week of SST at each location, given 6 weeks of historical data. We train on data from years 1982--2018, validate on data from 2019 and test on data from 2020--2021. The number of training, validation, and testing examples are roughly 150K, 3K, and 7K. See details of dataset construction, model architectures, and training scheme in \Cref{appendix:climate}. 



We compare our model to other baselines including TF-Net \cite{Rui2020}, a SOTA method for spatiotemporal forecasting, Linear Transformer \citep{katharopoulos-et-al-2020,wang2020linformer} with Laplacian positional encoding (LapPE), and Multilayer Perceptron (MLP). We use Mean Square Error (MSE) as the metric and report the errors on the test set, shown in the \Cref{tab:climate}. 
We observe that the virtual node (VN) alone improves upon MPNN by ,  and  in 4-, 2- and 1-week settings, respectively. Furthermore, aligned with our theory in \Cref{sec:shallow-narrow-attention}, MPNN + VN indeed achieves comparable results with Linear Transformer and outperforms it by a margin of ,  and  in 4-, 2- and 1-week settings, respectively.




\section{Concluding Remarks}
In this paper, we study the expressive power of MPNN + VN under the lens of GT. If we target the self-attention layer in Performer and Linear Transformer, one only needs -depth  width for arbitrary approximation error. 
For self-attention in full transformer, we prove that heterogeneous MPNN + VN of either  depth  width or  depth  width (under some assumptions) can approximate 1 self-attention layer arbitrarily well. 
Compared to early results \citep{kim2022pure} showing GT can approximate MPNN, our theoretical result draws the connection from the inverse direction. 



\begin{table}
\label{tab:sst}
\caption{\label{tab:climate} Results of SST prediction.} \begin{center}
\scalebox{0.9}{
\begin{tabular}{lccc}
\midrule
\textbf{Model} & \textbf{4 weeks} & \textbf{2 weeks} & \textbf{1 week} \\
\midrule
MLP & 0.3302 & 0.2710 & 0.2121 \\
TF-Net  & 0.2833 & \textbf{0.2036} & \textbf{0.1462} \\
Linear Transformer + LapPE & 0.2818 & 0.2191 & 0.1610 \\
MPNN & 0.2917 & 0.2281 & 0.1613 \\
\midrule
MPNN + VN & \textbf{0.2806} & 0.2130 & 0.1540 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}
On the empirical side, we demonstrate that MPNN + VN remains a surprisingly strong baseline. Despite recent efforts, we still lack good benchmark datasets where GT can outperform MPNN by a large margin. Understanding the inductive bias of MPNN and GT remains challenging. For example, can we mathematically characterize tasks that require effective long-range interaction modeling, and provide a theoretical justification for using GT over MPNN (or vice versa) for certain classes of functions on the space of graphs? We believe making processes towards answering such questions is an important future direction for the graph learning community.    

\section*{Acknowledgement}
This work was supported in part by the U.S. Department Of Energy, Office of Science, U. S. Army Research Office under Grant W911NF-20-1-0334, Google Faculty Award, Amazon Research Award, a Qualcomm gift fund, and NSF Grants \#2134274, \#2107256, \#2134178, CCF-2217033, and CCF-2112665.

\bibliography{./main}
\bibliographystyle{icml2023}



\newpage
\onecolumn
\appendix
\section{Notations} 
We provide a notation table for references. 
\begin{table}[!htbp]
\caption{Summary of important notations.} 
\begin{center}
\scalebox{0.9}{
{
\begin{tabular}{@{}l|l@{}}
    \hline
    \toprule
    Symbol & Meaning \\
    \midrule
    \midrule
     & graph node features \\
     & graph node 's feature \\
     & approximated graph node 's feature via attention selection \\
     & A multiset of vectors in   \\
     & attention matrix of -th self-attention layer in graph transformer \\
     & feature space \\  & projection of feature space onto -th coordinate \\
     & -th linear permutation equivariant layer in \DS{} \\ 
     & full self attention layer; approximate self attention layer in Performer \\  
     &  virtual/graph node feature at layer  of heterogeneous MPNN + VN  \\
     & attention score in MPNN + VN \\
    
     & normalized attention score \\
     & normalized attention score with \gat{} \\
     & unnormalized attention score.  \\
     & unnormalized attention score with \gat{}.  \\
     & space of attentions, where each element  is of form  \\
     & upper bound on norm of all node features  \\
     & upper bound on the norm of  in target  \\
     & upper bound on the norm of attention weights of  when selecting  \\
    \midrule
     & update function \\
     & message function \\
     & aggregation function \\
    \bottomrule
\end{tabular}
}
}
\end{center}
\label{table:symbol_notation}
\end{table}





\section{ Heterogeneous MPNN + VN Layer with  Width Can Approximate  Self Attention Layer Arbitrarily Well}
\label{sec:approximate-full-self-attention}

\subsection{Assumptions}
\label{subsec:assumptions}

A special case of  separable is when , i.e., . We provide a geometric characterization of  being  separable. 
\begin{restatable}{lemma}{Geometriccharacterization}
\label{lemma:Geometric-characterization} Given  and ,  is  separable by     is not in the convex hull spanned by .  there are no points in the convex hull of .
\end{restatable}


\begin{proof}
The second equivalence is trivial so we only prove the first equivalence. By definition,  is  separable by     .


By denoting the , we know that , which implies that   can be linearly seprated from    is not in the convex hull spanned by , which concludes the proof. 
\end{proof}

\UniformSelection*
\begin{proof}
Denote  as the unnormalized . As  is  separable by , by definition we know that  holds for any  and . We can amplify this by multiple the weight matrix in  by a constant factor  to make . This implies that . This means after softmax, the attention score  will be at least . We can pick a large enough  such that  for any  and .
\end{proof}















\textbf{Proof Intuition and Outline.} On the high level, -th MPNN + VN layer will select , an approximation -th node feature  via attention mechanism, enabled by \Cref{lemma-uniform-selection}, and send  to the virtual node. Virtual node will then pass the  to all graph nodes and computes the approximation of . Repeat such procedures  times for all graph nodes, and finally, use the last layer for attention normalization. 

The main challenge of the proof is to 1) come up with message/update/aggregation functions for heterogeneous MPNN + VN layer, which is shown in \Cref{subsec-mpnn-form},
and 2) ensure the approximation error, both from approximating Aggregate/Message/Update function with MLP and the noisy input, can be well controlled, which is proved in \Cref{subsec-controling-error}.


We will first instantiate the Aggregate/Message/Update function for virtual/graph nodes in \Cref{subsec-mpnn-form}, and prove that each component can be either exactly computed or approximated to an arbitrary degree by MLP. Then we go through an example in \Cref{subsec-a-running-example} of approximate self-attention layer  with  MPNN + VN layers. The main proof is presented in \Cref{subsec-controling-error}, where we show that the approximation error introduced during different steps is well controlled. Lastly, in \Cref{subsec:relax-assumption} we show assumption on node features can be relaxed if a more powerful attention mechanism \gatii \citep{brody2021attentive} is allowed in MPNN + VN.  

\subsection{Aggregate/Message/Update
Functions}
\label{subsec-mpnn-form}


Let  be a multiset of vectors in . 
The specific form of Aggregate/Message/Update for virtual and graph nodes are listed below. Note that ideal forms will be implemented as MLP, which will incur an approximation error that can be controlled to an arbitrary degree. We use  denotes the virtual node's feature at -th layer, and  denotes the graph node 's node feature. Iteration index  starts with 0 and the node index starts with 1. 

\subsubsection{virtual node}\label{subsubsec-vn}
At -th iteration, virtual node 's feature  is a concatenation of three component  where the first component is the approximately selected node features , the second component is the  that is used to select the node feature in -th iteration. The last component is just a placeholder to ensure the dimension of the virtual node and graph node are the same. It is introduced to simplify notation.  



\emph{Initial feature} is . 

\emph{Message function + Aggregation function}  has two cases to discuss depending on value of . For ,


 where .  is the node 's feature, where the first  coordinates remain fixed for different iteration . 
 use attention  to approximately select -th node feature . 
Note that the particular form of attention  needed for soft selection is not important as long as we can approximate hard selection arbitrarily well. As the  contains  and  contains  (see definition of graph node feature in \Cref{subsubsec-gn}),  this step can be made as close to hard selection as possible, according to \Cref{lemma-approximation-node-feature}. 

In the case of ,
 simply returns . This can be exactly implemented by an MLP. 

\emph{Update function }:
Given the virtual node's feature in the last iteration, and the selected feature in virtual node  with ,

where  denotes the first  channels of .  denotes the selected node 's feature in Message/Aggregation function. 
 can be exactly implemented by an MLP for any . 


\subsubsection{Graph node}\label{subsubsec-gn}
Graph node 's feature  can be thought of as a concatenation of three components , where  \footnote{\tmp{} technicially denotes the dimension of projected feature by  and does not has to be in . We use  here to reduce the notation clutter.}, and . 

In particular,  is the initial node feature. The first  channel will stay the same until the layer .  stands for the unnormalized attention contribution up to the current iteration.  is a partial sum of the unnormalized attention score, which will be used for normalization in the -th iteration. 

\emph{Initial feature} .

\emph{Message function + Aggregate function:
}
is just ``copying the second argument'' since there is just one incoming message from the virtual node, i.e., . This function can be exactly implemented by an MLP. 

\emph{Update function}
 is of the following form. 

where  is the usual unnormalized attention score. Update function  can be arbitrarily approximated by an MLP, which is proved below. 

\begin{restatable}{lemma}{LemmaApproximateUpdateFunction}
\label{lemma-approximate-update-function}
Update function  can be arbitrarily approximated by an MLP from  to  for all .
\end{restatable}
\begin{proof}
We will show that for any , the target function
 is continuous and the domain is compact. By the universality of MLP in approximating continuous function on the compact domain, we know  can be approximated to arbitrary precision by an MLP. 

Recall that

it is easy to see that ,  is continuous. We next show for ,  is also continuous and all arguments lie in a compact domain. 

 is continuous because to a)  is continuous b) scalar-vector multiplication, sum, and exponential are all continuous. Next, we show that four component  all lies in a compact domain.

 is the initial node features, and by AS\ref{AS-2} their norm is bounded so  is in a compact domain.

 is an approximation of . As  is both upper and lower bounded by AS\ref{AS-3} for all  and  is bounded by AS\ref{AS-2},  is also bounded from below and above.  will also be bounded as we can control the error to any precision. 

 is an approximation of . For the same reason as the case above,  is also bounded both below and above.

 will be  at -th iteration so it will also be bounded by AS\ref{AS-2}.

Therefore we conclude the proof. 
\end{proof}



\subsection{A Running Example}
\label{subsec-a-running-example}
We provide an example to illustrate how node features are updated in each iteration. 

\textbf{Time }: 
All nodes are initialized as indicated in \Cref{subsec-mpnn-form}. Virtual node feature  . Graph node feature  for all .

\textbf{Time }:

For virtual node, according to the definition of  in \Cref{equ:vn-gn}, it will pick an approximation of , i.e. . Note that the approximation error can be made arbitrarily small. VN's node feature . 

For -th graph node feature,  , and . According to  in  \Cref{eqn-gn-update-function}, .

\textbf{Time }:

For the virtual node feature: similar to the analysis in time 1, VN's feature  now. Note that the weights and bias in  will be different from those in .

For -th graph node feature, as  and , according to  in  \Cref{eqn-gn-update-function}, . Here . We will use similar notations in later iterations. 
\footnote{To reduce the notation clutter and provide an intuition of the proof, we omit the approximation error introduced by using MLP to approximate
aggregation/message/update function, and assume the aggregation/message/update can be exactly implemented by neural networks. In the proofs, approximation error by MLP is handled rigorously. } 

\textbf{Time }:

Similar to the analysis above, .

.

\textbf{Time }:

. 

.

\textbf{Time }:

According to \Cref{subsubsec-vn}, in  iteration, the virtual node's feature will be . 



\textbf{Time  (final layer)}:

For the virtual node, its node feature will stay the same.

For the graph node feature, the last layer will serve as a normalization of the attention score (use MLP to approximate vector-scalar multiplication), and set the last channel to be 0 (projection), resulting in an approximation of . Finally, we need one more linear transformation to make the node feature become . The first  channel is an approximation of the output of the self-attention layer for node  where the approximation error can be made as small as possible. This is proved in \Cref{sec:approximate-full-self-attention}, and we conclude that heterogeneous MPNN + VN can approximate the self-attention layer  to arbitrary precision with  MPNN layers.

\subsection{Controlling Error}\label{subsec-controling-error}

On the high level, there are three major sources of approximation error: 1) approximate hard selection with self-attention and 2) approximate equation  with MLPs, and 3) attention normalization in the last layer. In all cases, we aim to approximate the output of a continuous map . However, our input is usually not exact  but an approximation of . We also cannot access the original map  but instead, an MLP approximation of , denoted as . The following lemma allows to control the difference between  and . 


\begin{lemma}
\label{lemma:approximation-meta-lemma}
Let  be a continuous map from compact set to compact set in Euclidean space. Let  be the approximation of  by MLP. If we can control  to an arbitrarily small degree, we can then control the error  arbitrarily small. 
\end{lemma}
\begin{proof}
By triangle inequality . 

For the first term , by the universality of MLP, we can control the error  in arbitrary degree. 

For the second term , as  is continuous on a compact domain, it is uniformly continuous by Heine-Cantor theorem. This means that we can control the  as long as we can control , independent from different . By assumption, this is indeed the case so we conclude the proof. 
\end{proof}

\begin{remark}
The implication is that when we are trying to approximate the output of a continuous map  on the compact domain by an MLP , it suffices to show the input is 1)  and 2)  can be made arbitrarily small. The first point is usually done by the universality of MLP on the compact domain \citep{cybenko1989approximation}. The second point needs to be shown case by case. 

In the \Cref{subsec-a-running-example}, to simplify the notations we omit the error introduced by using MLP to approximate aggregation/message/update functions (continuous functions on the compact domain of .) in MPNN + VN. \Cref{lemma:approximation-meta-lemma} justify such reasoning.  
\end{remark}


\begin{lemma}[ approximates .  approximates .]\label{lemma-approximation-node-feature}
For any  and , there exist a set of weights for message/aggregate functions of the virtual node such that  and .
\end{lemma}

\begin{proof}
By \Cref{lemma-uniform-selection} We know that  as  goes to infinity. Therefore we have

As  and  are fixed, we can make the upper bound as small as we want by increasing .

. As  is bounded from above and below, it's easy to see that  can be controlled to arbitrarily degree. 
\end{proof}

\mainthm*
\begin{proof}
-th MPNN + VN layer will select , an arbitrary approximation -th node feature  via attention mechanism. This is detailed in the message/aggregation function of the virtual node in \Cref{subsubsec-vn}. Assuming the regularity condition on feature space , detailed in AS\ref{AS-1}, the approximation error can be made as small as needed, as shown in \Cref{lemma-uniform-selection,lemma-approximation-node-feature}. 

Virtual node will then pass the  to all graph nodes, which computes an approximation of . This step is detailed in the update function  of graph nodes, which can also be approximated arbitrarily well by MLP, proved in \Cref{lemma-approximate-update-function}. By \Cref{lemma:approximation-meta-lemma}, we have an arbitrary approximation of , which itself is an arbitrary approximation of . 

Repeat such procedures  times for all graph nodes, we have an arbitrary approximation of  and . Finally, we use the last layer to approximate attention normalization , where . As inputs for attention normalization are arbitrary approximation of  and , both of them are lower/upper bounded according to AS\ref{AS-2} and AS\ref{AS-3}. Since the denominator is upper bounded by a positive number, this implies that the target function  is continuous in both arguments. By evoking \Cref{lemma:approximation-meta-lemma} again, we conclude that we can approximate its output  arbitrarily well. This concludes the proof. 

\end{proof}











\subsection{Relaxing Assumptions with More Powerful Attention}
\label{subsec:relax-assumption}
One limitation of \Cref{thm:constant-width} are assumptions on node features space : we need to 1) restrict the variability of node feature so that we can select one node feature to process each iteration. 2) The space of the node feature also need to satisfy certain configuration in order for VN to select it.  For 2), we now consider a different attention function for  in MPNN + VN that can relax the assumptions AS\ref{AS-1} on . 


\textbf{More powerful attention mechanism.} From proof of \Cref{thm:constant-width}, we just need  uniformly select every node in . The unnormalized bilinear attention  is weak in the sense that  has a linear level set. Such a constraint can be relaxed via an improved attention module \gatii. Observing the ranking of the attention scores given by \texttt{GAT} \citep{velivckovic2017graph} is unconditioned on the query node, \citet{brody2021attentive} proposed \gatii, a more expressive attention mechanism. 
In particular, the unnormalized attention score , where  is concatenation. We will let  to select features in . 

\begin{figure}[hbtp]
  \centering
  \includegraphics[width=.35\linewidth]{./fig/feat_space2.pdf}
\caption{In the left figure, we have one example of  being  separable, for which  can uniformly select any point (marked as red) . In the right figure, we change  in MPNN + VN to , which allows us to select more diverse feature configurations. The cluster in the middle cannot be selected by any  but can be selected by  according to \Cref{prop:gat-v2-selection}. }
\label{fig:convergence}
\end{figure}


\begin{restatable}{lemma}{gatvtwoUniversality}
\label{lemma-gatv2-universality}
 can approximate any continuous function from . For any , a restriction of  can approximate any continuous function from .
\end{restatable}
\begin{proof}
Any function continuous in both arguments of  is also continuous in the concatenation of both arguments. As any continuous functions in  can be approximated by   on a compact domain according to the universality of MLP \citep{cybenko1989approximation}, we finish the proof for the first statement.

For the second statement, we can write  as  block matrix and restrict it to cases where only  is non-zero. Then we have 
 
  which gives us an MLP on the first argument . By the universality of MLP, we conclude the proof for the second statement. 

\end{proof}


\begin{definition}
\label{delta-sepration}
Given , We call  is  nonlinearly separable if and only if . 
\end{definition}

\begin{customAS}{3'}
\label{AS-4} 
 is  nonlinearly separable for some . 
\end{customAS}

\begin{restatable}{proposition}{gatvtwoselection}
\label{prop:gat-v2-selection}
If  satisfies that  is -separated from  for any , the following holds. For any  and , there exist a  to select any . This implies that we can arbitrarily approximate the self-attention layer  after relaxing AS3 to AS3'. 
\end{restatable}
\begin{proof}
For any , as  is -separated from other , we can draw a region  that contains  and separate  from other , where the distance from  from other  is at least  according to the definition of \Cref{delta-sepration}. Next, we show how to construct a continuous function  whose value in  is at least 1 larger than its values in any other   . 



We set the values of  in  to be 1.5 and values of  in  to be 0. We can then interpolate  in areas outside of  (one way is to set the values of  based on ), which results in a continuous function that satisfies our requirement.   
By the universality of , we can approximate  to arbitrary precision, and this will let us select any . 
\end{proof}



















\section{On the Limitation of MPNN + VN}
\label{sec:limitation-of-approximate-transformer}
Although we showed that in the main paper, MPNN + VN of varying depth/width can approximate the self-attention of full/linear transformers, this does not imply that there is no difference in practice between MPNN + VN and GT. Our theoretical analysis mainly focuses on approximating self-attention without considering computational efficiency. In this section, we mention a few limitations of MPNN + VN compared to GT.  

\subsection{Representation Gap}
The main limitation of deep MPNN + VN approximating full self-attention is that we require a quite strong assumption: we restrict the variability of node features in order to select one node feature to process each iteration. Such assumption is relaxed by employing stronger attention in MPNN + VN but is still quite strong. 

For the large width case, the main limitation is the computational complexity: even though the self-attention layer requires  complexity, to approximate it in wide MPNN + VN framework, the complexity will become  where  is the dimension of node features.

We think such limitation shares a similarity with research in universal permutational invariant functions. Both \DS{} \citep{zaheer2017deep} and Relational Network \citep{santoro2017simple} are universal permutational invariant architecture but there is still a representation gap between the two \citep{zweig2022exponential}. Under the restriction to analytic activation functions, one can construct a symmetric function acting on sets of size  with elements in dimension , which can be efficiently
approximated by the Relational Network, but provably requires width exponential in  and  for the \DS{}. We believe a similar representation gap also exists between GT and MPNN + VN and leave the characterization of functions lying in such gap as the future work. 

\subsection{On The Difficulty of Approximating Other Linear Transformers}
\label{subsec:ohter-linear-transformer}
In \Cref{sec:shallow-narrow-attention}, we showed MPNN + VN of  width and depth can approximate the self-attention layer of one type of linear transformer, Performer. The literature on efficient transformers is vast \cite{tay2020efficient} and we do not expect MPNN + VN can approximate many other efficient transformers. Here we sketch a few other linear transformers that are hard to  approximate by MPNN + VN of constant depth and width. 

Linformer \citep{wang2020linformer} projects the  dimension keys and values to  suing additional projection layers, which in graph setting is equivalent to graph coarsening. As MPNN + VN still operates on the original graph, it fundamentally lacks the key component to approximate Linformer. 



We consider various types of efficient transformers effectively generalize the virtual node trick. By first switching to a more expansive model and reducing the computational complexity later on, efficient transformers effectively explore a larger model design space than MPNN + VN, which always sticks to the linear complexity.  


\subsection{Difficulty of Representing SAN Type Attention}
In SAN \citep{kreuzer2021rethinking}, different attentions are used conditional on whether an edge is presented in the graph or not, detailed below. One may wonder whether we can approximate such a framework in MPNN + VN. 

In our proof of using MPNN + VN to approximate regular GT, we mainly work with \Cref{def:simplified-hetero-mpnn-vn} where we do not use any \gngn{}   edges and therefore not leverage the graph topology. It is straightforward to use \gngn{} edges and obtain the different message/update/aggregate functions for \gngn{} edges non-\gngn{} edges. Although we still achieve the similar goal of SAN to condition on the edge types, it turns out that we can not arbitrarily approximate SAN. 

Without loss of generality, SAN uses two types of attention depending on whether two nodes are connected by the edge. Specifically, 

where  denotes element-wise multiplication and  . is a hyperparameter that tunes the amount of bias towards full-graph attention, allowing flexibility of the model to different datasets and tasks where the necessity to capture long-range dependencies may vary. 

To reduce the notation clutter, we remove the layer index , and edge features, and also consider only one-attention head case (remove attention index ). The equation is then simplified to

We will then show that \Cref{equ:san-simplified-attention} can not be expressed (up to an arbitrary approximation error) in MPNN + VN framework. To simulate SAN type attention, our MPNN + VN framework will have to first simulate one type of attention for all edges, as we did in the main paper, and then simulate the second type of attention between \gngn{} edges by properly offset the contribution from the first attention. This seems impossible (although we do not have rigorous proof) as we cannot express the difference between two attention in the new attention mechanism.









\section{Experimental Details}
\subsection{Dataset Description}
\textbf{ogbg-molhiv} and \textbf{ogbg-molpcba} \citep{hu2020open} are molecular property prediction datasets
adopted by OGB from MoleculeNet. These datasets use a common node (atom) and edge (bond)
featurization that represent chemophysical properties. 
The prediction task of ogbg-molhiv is a binary
classification of molecule's fitness to inhibit HIV replication. The ogbg-molpcba, derived from
PubChem BioAssay, targets to predict the results of 128 bioassays in the multi-task binary classification
setting.

\textbf{ogbg-ppa} \citep{wu2021representing} consists of protein-protein association (PPA) networks derived from
1581 species categorized into 37 taxonomic groups. Nodes represent proteins and edges encode the
normalized level of 7 different associations between two proteins. The task is to classify which of the
37 groups does a PPA network originate from.

\textbf{ogbg-code2} \citep{wu2021representing} consists of abstract syntax trees (ASTs) derived from the source
code of functions written in Python. The task is to predict the first 5 subtokens of the original
function's name. 

\textbf{OGB-LSC PCQM4Mv2} \citep{hu2021ogb} is a large-scale molecular dataset that shares the
same featurization as ogbg-mol* datasets. It consists of 529,434 molecule graphs. The task is to predict the HOMO-LUMO gap, a quantum physical property originally calculated using Density Functional Theory. True labels for original
test-dev and test-challange dataset splits are kept private by the OGB-LSC challenge organizers.
Therefore for the purpose of this paper, we used the original validation set as the test set, while we
left out random 150K molecules for our validation set.


\subsection{Reproducibility}
For LRGB results in \Cref{subsec:lrgb}, we reproduce the original results up to very small differences. 
\begin{table}[h]
    \caption{Reproduce the original results up to small differences. No VN is used. 
    }
    \label{tab:experiments_peptides_reproduce}
    \begin{adjustwidth}{-2.5         cm}{-2.5                    cm}\centering
    \scalebox{0.9}{
    \setlength\tabcolsep{4pt}        \begin{tabular}{l                c                           c                  c                             c                               c}\toprule
    \multirow{2}{*}{\textbf{Model}}  &\multirow{2}{*}{\textbf{\# Params.}}          &\multicolumn{2}{c}{\pepfunc} &\multicolumn{2}{c}{\pepstruct} \\                          \cmidrule(lr){3-4} \cmidrule(lr){5-6}
    &                                &\textbf{Test              AP (reproduce)}                &\textbf{Test                 AP                              }                 &\textbf{Test     MAE (reproduce)}               &\textbf{Test MAE         } \\\midrule
    GCN                              &508k                       &0.59180.0065 &0.59300.0023            &0.34680.0009              &0.34960.0013          \\
    GINE                             &476k                       &0.55950.0126 &0.54980.0079            &0.35320.0024              &0.35470.0045          \\
    GatedGCN                         &509k                       &0.58860.0027 &0.58640.0077            &0.34090.0011              &0.34200.0013          \\
    GatedGCN+RWSE                    &506k                       &0.60830.0032 &0.60690.0035            &0.33770.0025              &0.33570.0006          \\                 
    \bottomrule
    \end{tabular}
    }
\end{adjustwidth}
\end{table}

\subsection{The Role of Graph Topology}
In our experiments, we considered graph topology in experiments (i.e., message passing operates on both GN-VN (graph node-virtual node) and GN-GN edges). To understand the role of GN-VN and GN-GN edges, we carried out a set of new experiments where we discard the original graph topology, and only do message passing on GN-VN edges, for Peptides-func \& Peptides-struct datasets. The results are shown in \Cref{tab:graph_topology}.  

We observe that in general, MPNN + VN using GN-VN edges only perform slightly worse than MPNN + VN using both GN-VN and GN-GN edges. However, it still performs better than the standard MPNN without VN. We believe adding VN as a simple way of long-range modeling is the main reason we see good results on Peptides-func \& Peptides-struct datasets. Utilizing local graph topology in MPNN will further improve the performance. 

In general, combining local (message passing) and global modeling (such as GT and VN) in GNN is an active research direction, with novel applications in macromolecule (DNA, RNA, Protein) modeling. In the recent SOTA model GraphGPS \citep{rampavsek2022recipe},  MPNN is interleaved with GT. Consistent with our findings,  \citet{rampavsek2022recipe} also showed both the local component (MPNN) and global component (GT) contribute to the final performance.   

\begin{table}[h]
\label{tab:graph_topology}
\centering
\caption{Utilizing local graph topology in MPNN will further improve the performance on \pepfunc and \pepstruct{}.}
\scalebox{0.65}{
\begin{tabular}{lccc|ccc}
\toprule
& \multicolumn{3}{c}{\pepfunc AP } & \multicolumn{3}{c}{\pepstruct MAE } \\
\cline{2-7}
& \textbf{w/o VN (only graph topology)} & \textbf{w/ VN + graph topology} & \textbf{Only VN} & \textbf{w/o VN (only graph topology)} & \textbf{w/ VN + graph topology} & \textbf{Only VN} \\ \midrule
GCN &  &  &  &  &  &  \\
GINE &  &  &  &  &  &  \\
GatedGCN &  &  &  &  &  &  \\
GatedGCN+RWSE &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Additional Experiments}
We tested MPNN + VN on \pascal datasets and also observe improvement, shown in \Cref{tab:pascal}, although the improvement is not as large as that of \pepfunc and \pepstruct datasets. The best MPNN + VN model is GatedGCN + LapPE where the performance gap to the best GT model is rather small.  

\begin{table}[hbtp]
    \caption{Baseline experiments for \pascal and \coco with \rbgraph graph on SLIC compactness 30 for the node classification task. The performance metric is macro F1 on the respective splits (Higher is better). All experiments are run 4 times with 4 different seeds. 
The MP-GNN models are 8 layers deep, while the transformer-based models have 4 layers in order to maintain comparable hidden representation size at the fixed parameter budget of 500k. \textbf{Bold}: Best score.
    }
\label{tab:pascal}
    \begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering
    \scalebox{1}{
    \setlength\tabcolsep{4pt} \begin{tabular}{l c c c } \toprule
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\hspace*{-2em}\textbf{\# Params}} & \multicolumn{2}{c}{\pascal} \\ \cmidrule(lr){3-4} &                                                   & \textbf{Before VN + Test F1}                        & \textbf{After VN + Test F1 } \\ \midrule 
    GCN                             & 496k                                              & 0.12680.0060                                   & 0.19010.0040                 \\ GINE                            & 505k                                              & 0.12650.0076                                   & 0.11980.0073                 \\ GatedGCN                        & 502k                                              & 0.28730.0219                                   & 0.28740.0178                 \\ GatedGCN+LapPE                  & 502k                                              & 0.28600.0085                                   & \first{0.31030.0068}                 \\ \cmidrule(l){1-4}Transformer+LapPE               & 501k                                              & 0.26940.0098                                   & -                 \\ SAN+LapPE                       & 531k                                              & \first{0.32300.0039}                           & -              \\ SAN+RWSE                        & 468k                                              & \second{0.32160.0027}                          & -             \\ \bottomrule
    \end{tabular}
    }
    \end{adjustwidth}
\end{table}



\subsection{Predicting Sea Surface Temperature} \label{appendix:climate}
\label{appendix:climate}
In this experiment, we consider a specific physical modeling problem: forecasting sea surface temperature (SST), that is the water temperature close to the ocean's surface. SST is an essential climate indicator and plays a significant role in analyzing and monitoring the dynamics of weather, climate, and other biological systems for several applications in environmental protection, agriculture, and industry. We use the NOAA/NESDIS/NCEI Daily Optimum Interpolation Sea Surface Temperature (DOISST) version 2.1 proposed by \cite{ImprovementsoftheDailyOptimumInterpolationSeaSurfaceTemperatureDOISSTVersion21} as an improvement upon version 2.0 from \cite{Reynolds:2007}. 


We consider the daily SST data of the Pacific Ocean from 1982 to 2021, in the region of longitudes from  to  and latitudes from  
to . We reduce the resolution of the original data from -degree 
to -degree. 
Following the procedure from \cite{de2018deep}, \cite{deBezenac2019} and \cite{wang2022metalearning}, we divide the region into 11 square batches of equal size (see Table \ref{tab:Pacific-regions}), each contains exactly 30 longitudes and 30 latitudes that can be represented as a grid graph of 900 nodes in which we connect each node to its nearest 8 neighbors. We take time series from 1982 to 2018 as our training set, data in 2019 as our validation set, and data from 2020 to 2021 as our testing set. 
In our experiments, we set the history window  as 6 weeks (i.e. 42 days) and the prediction window  as 4 weeks (i.e. 28 days), 2 weeks (i.e. 14 days) or 1 week (i.e. 7 days). For each example, each node of the graph is associated with an input time series capturing the temperatures at the corresponding (longitude, latitude) for the last  days, and the task is to predict the output time series of temperatures for the next  days. 


We represent each time series as a long vector and the learning task is fundamentally a node-level regression task. We make sure that there is no overlapping among training, validation and testing sets (e.g., the output of a training example will \textit{not} appear in any input of another validation example). The number of training, validation, and testing examples are roughly 150K, 3K and 7K, respectively for each setting (see Table \ref{tab:noexamples}). We compare our MPNN + VN model with:
\begin{itemize}
\item Multilayer Perceptron (MLP) which treats both the input and output as long vectors and has 512 hidden neurons.
\item TF-Net \cite{Rui2020} with the setting as in the original paper.
\item Linear Transformer \cite{katharopoulos-et-al-2020} \cite{wang2020linformer}\footnote{The Linear Transformer implementation is publicly available at \url{https://github.com/lucidrains/linear-attention-transformer}} with Laplacian positional encoding (LapPE). We compute the first 16 eigenvectors as positions for LapPE.
\end{itemize}
Both MPNN and MPNN + VN have 3 layers of message passing with 256 hidden dimensions. We apply an MLP with one hidden layer of 512 neurons on top of the network to make the final prediction.

We train all our models with 100 epochs with batch size 20, initial learning rate , and Adam optimizer \cite{Adam2014}.

\begin{table}
\caption{\label{tab:noexamples} Number of training, validation and testing examples for each setting in the task of SST prediction.}
\begin{center}
\begin{tabular}{ccccc}
\toprule
\textbf{History window} & \textbf{Prediction window} & \textbf{Train size} & \textbf{Validation size} & \textbf{Test size} \\
\midrule
\multirow{2}{*}{6 weeks} & 4 weeks &  &  &  \\
& 2 weeks &  &  &  \\
& 1 week &  &  &  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{\label{tab:Pacific-regions} These are 11 regions of the Pacific in our experiment.}
\begin{center}
\begin{tabular}{ccc}
\toprule
\textbf{Index} & \textbf{Longitudes} & \textbf{Latitues} \\
\midrule
1 & [180.125, 194.875] & [-14.875, -0.125] \\
2 & [195.125, 209.875] & [-14.875, -0.125] \\
3 & [210.125, 224.875] & [-14.875, -0.125] \\
4 & [225.125, 239.875] & [-14.875, -0.125] \\
5 & [240.125, 254.875] & [-14.875, -0.125] \\
6 & [255.125, 269.875] & [-14.875, -0.125] \\
7 & [180.125, 194.875] & [0.125, 14.875] \\
8 & [195.125, 209.875] & [0.125, 14.875] \\
9 & [210.125, 224.875] & [0.125, 14.875] \\
10 & [225.125, 239.875] & [0.125, 14.875] \\
11 & [240.125, 254.875] & [0.125, 14.875] \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Connection to Over-Smoothing Phenomenon}
\label{subsec:over-smoothing}
Over-smoothing refers to the phenomenon that deep GNN will produce same features at different nodes after too many convolution layers. Here we draw some connection between VN and common ways of reducing over-smoothing. We think that using VN can potentially help alleviate the over-smoothing problem. In particular, we note that the use of VN can simulate some strategies people use in practice to address over-smoothing. We give two examples below. 

Example 1: In \cite{zhao2019pairnorm}, the two-step method (center \& scale) PairNorm is proposed to reduce the over-smoothing issues. In particular, PairNorm consists of 1) Center and 2) Scale






Where  is the node features after graph convolution and  is a hyperparameter. The main component for implementing PairNorm is to compute the mean and standard deviation of node features. For the mean of node features, this can be exactly computed in VN. For standard deviation, VN can arbitrarily approximate it using the standard universality result of MLP [5]. If we further assume that the standard deviation is lower bounded by a constant, then MPNN + VN can arbitrarily approximate the PairNorm on the compact set. 

Example 2: In \cite{yang2020revisiting} mean subtraction (same as the first step of PairNorm) is also introduced to reduce over-smoothing. As mean subtraction can be trivially implemented in MPNN + VN, arguments in \citep{yang2020revisiting} (with mean subtraction the revised power Iteration in GCN will lead to the Fiedler vector) can be carried over to MPNN + VN setting. 

In summary, introducing VN allows MPNN to implement key components of \cite{yang2020revisiting,zhao2019pairnorm}, we think this is one reason why we observe encouraging empirical performance gain of MPNN + VN. 



\end{document}