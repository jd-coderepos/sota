\documentclass[runningheads,a4paper]{llncs}


\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{smallsec}
\usepackage{times}
\usepackage{xfrac}
\usepackage{gastex}













\title{Minimizing Expected Cost Under Hard Boolean Constraints, with Applications to Quantitative Synthesis} 

\titlerunning{Minimizing Expected Cost Under Hard Boolean Constraints} 

\authorrunning{S. Almagor, O. Kupferman, and Y. Velner} 

\author{Shaull Almagor \and Orna Kupferman \and Yaron Velner}
\institute{School of Computer Science and Engineering, The Hebrew University, Israel.}








\newtheorem{remarknum}[theorem]{Remark}\newtheorem{xmpl}[theorem]{Example}
\renewenvironment{example}{\begin{xmpl}\rm}{\end{xmpl}}




\newcommand{\qedsymb}{\hfill{\rule{2mm}{2mm}}}
\def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}



\addtolength{\textwidth}{0.1cm}









\newcommand{\todo}[1]{\par\noindent{\raggedright\sc{TODO: #1}
    \par\marginpar{\Large \bf }}}

\newcommand{\short}[1]{}
\newcommand{\set}[1]{{\{#1\}}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Rat}{\mathbbm{Q}}
\newcommand{\RR}{\mathbbm{R}}
\newcommand{\con}{\cdot}
\newcommand{\restrict}[1]{\!\!\upharpoonright\!\! _{#1}}
\newcommand{\tuple}[1]{\langle #1  \rangle}
\newcommand{\zug}[1]{\langle #1  \rangle}
\newcommand{\trans}[1]{\stackrel{#1}{\to}}
\newcommand{\prob}{\mathit{prob}} 
\newcommand{\True}{\mathtt{True}}
\newcommand{\False}{\mathtt{False}}
\newcommand{\LTL}{{\ensuremath{\rm LTL}}\xspace}
\newcommand{\CTL}{{\rm CTL}}


\newcommand{\Next}{\mathsf{X}}
\newcommand{\Ev}{\mathsf{F}}
\newcommand{\Alw}{\mathsf{G}}
\newcommand{\Until}{\mathsf{U}}
\newcommand{\Release}{\mathsf{R}}

\newcommand{\Exists}{\mathsf{E}}
\newcommand{\All}{\mathsf{A}}

\newcommand{\BMC}{\texttt{BMC}} 

\newcommand{\K}{{\mathcal K}}
\newcommand{\T}{{\mathcal T}}

\newcommand{\atleast}{{\rm at\_least}}

\newcommand{\DFTs}{\mbox{\rm DFTs}\xspace}
\newcommand{\DLW}{\mbox{\rm DLW}\xspace}
\newcommand{\DLWs}{\mbox{\rm DLWs}\xspace}
\newcommand{\DPW}{\mbox{\rm DPW}\xspace}
\newcommand{\DPWs}{\mbox{\rm DPWs}\xspace}
\newcommand{\NPW}{\mbox{\rm NPW}\xspace}
\newcommand{\NPWs}{\mbox{\rm NPWs}\xspace}
\newcommand{\UPW}{\mbox{\rm UPW}\xspace}
\newcommand{\UPWs}{\mbox{\rm UPWs}\xspace}
\newcommand{\DFA}{\mbox{\rm DFA}\xspace}
\newcommand{\NFA}{\mbox{\rm NFA}\xspace}
\newcommand{\AFA}{\mbox{\rm AFA}\xspace}
\newcommand{\NBA}{\mbox{\rm NBA}\xspace}
\newcommand{\NGBA}{\mbox{\rm NGBA}\xspace}
\newcommand{\NFW}{\mbox{\rm NFW}\xspace}
\newcommand{\AFW}{\mbox{\rm AFW}\xspace}

\newcommand{\gap}{\vspace*{0.25 cm}}
\newcommand{\stam}[1]{}

\newcommand{\B}{{\cal B}}
\newcommand{\C}{{\cal C}}
\newcommand{\A}{{\cal A}}
\newcommand{\D}{{\cal D}}
\newcommand{\G}{{\cal G}}
\newcommand{\M}{{\cal M}}
\newcommand{\F}{{\cal F}}
\newcommand{\U}{{\cal U}}
\newcommand{\Ex}{{\mathbbm{E}}}
\newcommand{\RL}{{\cal R}_L}
\renewcommand{\P}{{\mathtt P}}
\newcommand{\MDP}{{\mathtt{MDP}}}


\renewcommand{\phi}{\varphi}
\newcommand{\mins}[1]{\min\set{#1}}
\newcommand{\maxs}[1]{\max\set{#1}}

\newcommand{\Inf}{\mbox{inf}}

\newcommand{\Paragraph}[1]{\paragraph*{#1}}
\newcommand{\LittleTitle}[1]{\noindent {\bf#1.}}

\newcommand{\ST}{ : \:}


\newcommand{\Fu}[2]{\ensuremath {{#1}{[#2]}}\xspace}
\newcommand{\FuLTL}[1]{\Fu{\LTL}{#1}}



\newcommand{\draft}{--- Draft of \today---}
\newcommand{\cons}{\cong}
\newcommand{\sen}{{\it sensed}}
\newcommand{\scost}{{\it scost}}
\newcommand{\stcost}{{\it scost}}
\newcommand{\cost}{{\it cost}}
\newcommand{\avg}{{\it avg}}
\newcommand{\slevel}{{\it slevel}}
\newcommand{\sindex}{{\it scost}}
\newcommand{\nsen}{{\rm non-sensed}}
\newcommand{\tAP}{{2^{I\cup O}}}
\newcommand{\tAPs}{{{(2^{I\cup O})}^*}}
\newcommand{\tAPo}{{{(2^{I\cup O})}^\omega}}
\newcommand{\tIN}{{2^I}}
\newcommand{\tOUT}{{2^O}}
\newcommand{\tINs}{{{(2^I)}^*}}
\newcommand{\tOUTs}{{{(2^O)}^*}}
\newcommand{\tINo}{{{(2^I)}^\omega}}
\newcommand{\tOUTo}{{{(2^O)}^\omega}}
\newcommand{\res}[1]{\langle #1  \rangle}
\newcommand{\abs}{{\rm abs}}

\newcommand{\rank}{\mathrm{rank}}
\newcommand{\qf}{q_\text{\ding{96}}}
\newcommand{\Aux}{\mathcal{H}}
\newcommand{\from}{\leftarrow}

\newcommand{\win}{{\rm win}}
\newcommand{\visit}{{\rm visit}}


\newcommand{\qres}{q_{\rm reset}}
\newcommand{\qrej}{q_{\rm rej}}

\newcommand{\Act}{A}
\newcommand{\MDPProb}{{\rm P}}
\newcommand{\MDPcost}{{\it cost}}

\newcommand{\asgn}{\ell}

\newcommand{\sIN}{\set{0,1,\bot_0,\bot_1}^I}

\newcommand{\start}{{\textsc{start}}}
\newcommand{\reject}{{\textsc{reject}}}
\newcommand{\reset}{{\textsc{reset}}}
\newcommand{\dL}{\triangleleft}
\newcommand{\dR}{\triangleright}


\newcommand{\buchi}{B\"uchi\xspace}

\newcommand{\costs}{{\it cost}_{\rm sure}}
\newcommand{\costsf}{{\it cost}_{\rm sure,<\infty}}

\newcommand{\obs}{{\cal O}}
\newcommand{\real}{{\tt real}}
\newcommand{\obsf}{{\tt obs}}

\newcommand{\gec}{{GEC}\xspace}
\newcommand{\sgec}{{SGEC}\xspace}
\newcommand{\gecs}{{GECs}\xspace}
\newcommand{\sgecs}{{SGECs}\xspace}
\newcommand{\attr}{{\rm Attr}}
\newcommand{\env}{{\rm env}}
\newcommand{\sys}{{\rm sys}}

\newcommand{\me}{{\substack{\rm max\\ \rm even}}}






\begin{document}
\maketitle
\begin{abstract}
In Boolean synthesis, we are given an \LTL specification, and the goal is to construct a transducer that realizes it against an adversarial environment. 
Often, a specification contains both Boolean requirements that should be satisfied against an adversarial environment, and multi-valued components that refer to the quality of the satisfaction and whose expected cost we would like to minimize with respect to a probabilistic environment. 

In this work we study, for the first time, mean-payoff games in which the system aims at minimizing the expected cost against a probabilistic environment, while surely satisfying an -regular condition against an adversarial environment.
We consider the case the -regular condition is given as a parity objective or by an \LTL formula.
We show that in general, optimal strategies need not exist, and moreover, the limit value cannot be approximated by finite-memory strategies. 
We thus focus on computing the limit-value, and give tight complexity bounds for synthesizing -optimal strategies for both finite-memory and infinite-memory strategies.

We show that our game naturally arises in various contexts of synthesis with Boolean and multi-valued objectives. Beyond direct applications, in synthesis with costs and rewards to certain behaviors, it allows us to compute the minimal sensing cost of -regular specifications -- a measure of quality in which we look for a transducer that minimizes the expected number of signals that are read from the input.
\end{abstract}

\section{Introduction}
\label{sec:intro}
{\em Synthesis\/} is the automated construction of a system from its specification: given a linear temporal logic (LTL)  formula  over sets  and  of input and output signals, we synthesize a 
system that {\em realizes\/}  \cite{Chu63,PR89a}. At each moment in time, the system reads a truth assignment, generated by the environment, to the signals in , and it generates a truth assignment to the signals in . Thus, with every sequence of inputs, the system associates a sequence of outputs. 
The system realizes  if all the computations that are generated by the interaction satisfy . 


One weakness of automated synthesis in practice is that it pays no attention to the quality of the synthesized system. Indeed, the classical setting is Boolean: a computation satisfies a specification or does not satisfy it. Accordingly, while the synthesized system is correct, there is no guarantee about its quality. This is a crucial drawback, as designers would be willing to give-up manual design only if automated-synthesis algorithms return systems of comparable quality. 
In recent years, researchers have considered extensions of the classical Boolean setting to a quantitative one, which takes quality into account. Quality measures can refer to the system itself, examining parameters like its size or its consumption of memory, sensors, voltage, bandwidth, etc., or refer to the way the system satisfies the specification. In the latter, we allow the designer to specify the quality of a behavior using quantitative specification formalisms~\cite{ABK16,BMM14,AFHMS05}. 
For example, rather than the Boolean specification requiring all requests to be followed by a grant, a quantitative specification formalism would give a different satisfaction value to a computation in which requests are responded immediately and one in which requests are responded after long delays.\footnote{Note that the polarity of some quality measures is negative, as we want to minimize size, consumption, costs, etc., whereas the polarity of other measures is positive,  as we want to maximize performance and satisfaction value. For simplicity, we assume that all measures are associated with costs, which we want to minimize.}

Solving the synthesis problem in the Boolean setting amounts to solving a two-player zero-sum game between the system and the environment. The goal of the system is to satisfy the (Boolean) specification, and the environment is adversarial. Then, a winning strategy for the system corresponds to a transducer that realizes the specification. 
In the quantitative setting, the goal of the system is no longer Boolean, as every play is assigned a cost by the specification. In the classical quantitative approach, we measure the satisfaction value in the worst-case semantics. Thus, the value of a strategy for the system is the maximal cost of a play induced by this strategy, and the goal of the system is to minimize this value. Recently, there is a growing interest also in the expected cost of a play, under a probabilistic environment. The motivation behind this approach is that the quality of satisfaction is a ``soft constraint'', and should not be measured in a worst-case semantics. Then, the game above is replaced by a mean-payoff Markov Decision Process (MDP): a game in which each state has a cost, inducing also costs to infinite plays (essentially, the cost of an infinite play is the limit of the average cost of prefixes of increased lengths). The goal is to find a strategy that minimizes the expected cost \cite{CKK15,CR15}.

While quantitative satisfaction refines the Boolean one, often a specification contains both Boolean conditions that should be satisfied against all environments, and multi-valued components that refer to the quality of the satisfaction and whose expectation we would like to minimize with respect to a probabilistic environment. Accordingly, 
researchers have suggested the {\em beyond worst-case} approach, where a specification has both hard and soft constraints, and the goal is to realize the hard constraints, while maximizing the expected satisfaction value of the soft constraints. In Section~\ref{related} below, we describe this approach and related work in detail.

In this work, we consider, for the first time, mean-payoff MDPs equipped with a parity winning condition (parity-MDPs, for short). The goal is to find a strategy that surely wins the parity game (that is, against an adversarial environment), while minimizing the expected cost of a play against a probabilistic environment. 
While the starting point in earlier related work is the MDP itself, possibly augmented by different objectives, 
our starting point depends on the application, and we view the construction of the MDP as an integral part of our contribution. We focus on two applications: synthesis with penalties to undesired scenarios and synthesis with minimal sensing.

Let us describe the two applications. We start with penalties to scenarios.
\stam{
Consider an  specification  over  and . Activating an output signal  may have a cost; for example, when the activation involves a use of a resource. Taking these costs into account, the input to the synthesis problem contains, in addition to , a cost function , where  is the cost of activating the signal . Accordingly, each assignment  has a cost , obtained by summing the costs of the signals in  (alternatively, one could consider costs of subsets of output signals that need not be decomposable). The cost of a computation  is then the mean cost . While the specification  is a hard constraint, as we only allow correct computations, minimizing the expected cost of computations with respect to  is a soft constraint. We show how the setting can be easily translated into solving our parity-MDPs. 


The function  above enable penalties to certain assignments, which correspond to scenarios of length . 
More elaborated cost functions refer to on-going regular scenarios. 
Power consumption, for example, is an important consideration in modern chip design, from portable servers
to large server farms. As the chips become more complex, the cost of powering a server
farm can easily outweigh the cost of the servers themselves, thus design teams go to great
lengths in order to reduce power consumption in their designs.
The most widely researched logical power saving techniques are \emph{clock gating}, in which a clock is prevented from making a ``tick'' if it is redundant (c.f., \cite{ARY09}), and \emph{power gating}, in which whole sections of the chip are powered off when not needed and then powered on again \cite{KFAGS07,ENY09}. The goal of these techniques is to reduce the number of changes in the values of signals, the main source of power consumption in chips. 
We show how we can augment the MDP so that regular scenarios,  like changes in the values of signals, are detected, inducing costs in the MDP.
}
Consider an  specification  over  and . Activating an output signal may have a cost; for example, when the activation involves a use of a resource. Taking these costs into account, the input to the synthesis problem includes, in addition to , a cost function  assigning cost to some assignments to output signals. The cost of a computation is then the mean cost of assignments in it. While the specification  is a hard constraint, as we only allow correct computations, minimizing the expected cost of computations with respect to  is a soft constraint. 
Assignments correspond to scenarios of length one. More elaborated cost functions refer to on-going regular scenarios. 
Power consumption, for example, is an important consideration in modern chip design, from portable servers
to large server farms. As the chips become more complex, the cost of powering a server
farm can easily outweigh the cost of the servers themselves, thus design teams go to great
lengths in order to reduce power consumption in their designs.
The most widely researched logical power saving techniques are \emph{clock gating}, in which a clock is prevented from making a ``tick'' if it is redundant (c.f., \cite{ARY09}), and \emph{power gating}, in which whole sections of the chip are powered off when not needed and then powered on again \cite{KFAGS07,ENY09}. The goal of these techniques is to reduce power consumption and the number of changes in the values of signals, the main source of power consumption in chips. 
The input to the problem of synthesis with penalties to scenarios includes, in addition to , a set of deterministic automata on finite words, each describing a undesired scenario and its cost. For example, it is easy to specify the scenario of ``value flip" with a two-state deterministic automaton.
We show how the setting can be easily translated into solving our parity-MDPs, thus generating systems that realize  with minimal expected cost. 


Our primary application considers activation of sensors. The quality measure of sensing was introduced in ~\cite{AKK14,AKK15}, as a measure for the detail with which a random input word needs to be read in order to realize the specification. In the context of synthesis, our goal is to construct a transducer that realizes the specification and minimizes the expected average number of sensors (of input signals) that are used along the interaction.
Thus, the hard constraint in the LTL specification, and the soft one is the expected number of active sensors. 
Giving up sensing has a flavor of synthesis with incomplete information \cite{KV00a}: the transducer has to realize the specification no matter what the incomplete information is. Thus, as opposed to the examples above, the modeling of cost involves a careful construction of the MDP to be analyzed, and also involves an exponential blow-up, which we show to be unavoidable.  In \cite{AKK15}, the problem was solved for safety specifications. Our solution to the parity-MDP problem enables a solution for full LTL. We also study the complexity of the problem when the input is an LTL formula, rather than a deterministic automaton. 


Back to parity-MDPs, we show that in general, optimal strategies need not exist. That is, there are parity-MDPs in which an infinite-state strategy can get arbitrarily close to some limit optimal value, but cannot attain it. Moreover, the limit value cannot be approximated by finite-memory strategies.  Accordingly, our solution to parity-MDPs suggests two algorithms. The first, described in Section~\ref{sec:infinite memory}, finds the limit value of all possible strategies, which corresponds to infinite-state transducers. The second, described in Section~\ref{sec:finite memory}, computes the limit value over all finite-memory strategies. 
The complexity of both algorithms is NPcoNP. Moreover, they are computable in polynomial time when an oracle to a two-player parity game is given.
Hence, our complexity upper bounds match the trivial lower bounds that arise from the fact that every solution to a parity-MDP is also a solution to a parity game.
For our applications, we show that the complexity of the synthesis problem for LTL specifications stays doubly-exponential, as in the Boolean setting, even when we minimize penalties to undesired scenarios or minimize  sensing.

\vspace*{-10pt}
\subsection{Related Work}
\label{related}
The combination of worst-case synthesis with expected-cost synthesis, dubbed {\em beyond worst-case synthesis\/}, was studied in~\cite{BFRR14a,CR15} for models that are closely related to ours. 
In~\cite{BFRR14a} the authors study mean-payoff MDPs, where both the hard constraints and the soft constraints are quantitative. Thus, a system needs to ensure a strict upper bound on the mean-payoff cost, while minimizing the expected cost. 
In~\cite{CR15}, multidimensional mean-payoff MDPs are considered. Thus, the MDP is equipped with several mean-payoff costs, and the goal is to find a system that ensures the mean-payoff in some of the mean-payoffs is below an upper bound, while minimizing the expected mean-payoffs (or rather, approximating their Pareto-curve).
 
In comparison, our work is the first to consider a hard Boolean constraint (namely the parity condition). This poses both a conceptual and a technical difference. Conceptually, when quantitative synthesis is taken as a refinement of Boolean synthesis, it is typically meant as a ranking of different systems that satisfy a Boolean specification. Thus, it makes sense for the hard constraint to be Boolean as well. 
Technically, combining Boolean and quantitative constraints gives rise to some subtleties that do not exist in the pure-quantitative setting. Specifically, when both the hard and the soft constraints are quantitative, a strategy can intuitively ``alternate'' between satisfying them. Thus, if while trying to meet the soft constraint the hard constraint is violated, we can switch to a worst-case strategy until the hard constraint is satisfied, and go back to trying to minimize the soft constraint. This alternation can be done infinitely often. In the Boolean setting, however, this alternation can violate the Boolean constraint. We note that 
unlike classical parity games, where the parity winning condition can be translated to a richer mean-payoff objective,
the parity winning condition in our parity-MDPs does not admit a similar translation.


Other works on MDPs and mean-payoff objectives tackle different aspects of quantitative analysis. In~\cite{Put14}, a solution to the expected mean-payoff value over MDPs is presented. In~\cite{CD11} and~\cite{CLGO14}, the authors study a combination of mean-payoff and parity objectives over MDPs and over stochastic two-player games. There, the goal of the system is to ensure with probability 1 that the parity condition holds and that the mean-payoff is below a threshold. This differs from our work in that the parity condition is not a hard constraint, as it is met only almost-surely, and in that the 
expected mean-payoff is not guaranteed to be
minimized. As detailed in the paper, these differences make the technical challenges very different.

Due to lack of space, some proofs appear in the appendix.
\section{Parity-MDPs}
\label{sec:prelim}

\stam{
A Markov chain  consists of a finite state space  and a 
  stochastic transition matrix . That is, for all , we have . Given an initial state , consider the vector  in which  and  for every . The {\em limiting distribution} of  is . The limiting distribution satisfies , and can be computed in polynomial time~\cite{GL97}.
}

A {\em parity Markov decision process\/} (Parity-MDP, for short) combines a parity game with a mean-payoff MDP. The game is played between Player~1, who models a 
system, and Player~2, who models the environment. The environment is adversarial with respect to the parity winning condition and is stochastic with respect to the mean-payoff objective. Formally, a parity-MDP is a tuple
, with the following components. The sets  and  are finite set of states, for Players 1 and 2, respectively. Let . Then,  is an initial state, and  and  are sets of actions for the players. Not all actions are available in all states: for every state , for , we use  to denote the finite set of actions available to Player  in the state . 
For , the transition function  is such that  is defined iff . Let . 
Note that  gets an action of Player~2 as a parameter. We distinguish between two approaches to the way the action is chosen. In the adversarial approach, it is Player~2 who chooses the action. In the stochastic approach, the choice depends on the (partial) function , where for every state  and , we have that  only if . Also, . Finally,  is a cost function, and  , for some , is a parity winning condition.

The parity-MDP   induces a {\em parity game\/} , obtained by omitting  and . In this game, we follow the adversarial approach to the environment. Thus, both players choose their actions. Formally, 
a {\em strategy} for Player   in , for  is a function  such that for , we have . Thus, a strategy suggests to Player  an available action given the history of the states traversed so far.  
Note that we do not consider randomized strategies, but rather deterministic ones. 
Our results in Section~\ref{sec: solving} show that this is sufficient, in the sense that the players cannot gain by using randomization.

Given strategies  and  for Players  and , the {\em play} induced  and  is 
is the infinite sequence of states  such that for every , if , for , then . For an infinite play , we denote by  the set of states that  visits infinitely often.
The play  of  is {\em parity winning} if  is even. 

The parity-MDP   also induces an {\em MDP\/} , obtained by omitting . In this game, we follow the stochastic approach to the environment and consider the distribution of plays when only a strategy for Player~1 is given. Formally, we first extend  to transitions as follows: For states  and , we define .
Then, a play of  with strategy  for Player~1 is an infinite sequence of states  such that for every , if , then , and if , then . 
The {\em cost} of a strategy  is the expected average cost of a random walk in  in which Player~1 proceeds according to . Formally, for  and for a prefix  of a play, let . Then, we define  and . 
The cost of a strategy  is then  .
We denote by  the random variable that associates  with a sequence of states , under the probability space induced by  with .

A {\em finite memory} strategy for  is described by a finite set  called {\em memory}, an initial memory , a {\em memory update function} , and an {\em action function}  such that  for every  and . 

A strategy is {\em memoryless} if it has finite memory  with . Note that a memoryless strategy depends only on the current state. Thus, we can describe a memoryless strategy by  . 
Let . That is,  is the expected cost of a game played on  in which Player~1 uses an optimal strategy.  

The following is a basic property of MDPs.
\vspace*{-5pt}
\begin{theorem}
	\label{thm:solving MDP in P}
Consider an MDP . Then,  can be attained by a memoryless strategy, which can be computed in polynomial time.
\end{theorem}
\vspace*{-5pt}

Recall that a strategy  for player 1 is winning in  if every play of  with  satisfies the parity condition . Note that we require {\em sure} winning, in the sense that all plays must be winning, rather than winning with probability 1 ({\em almost-sure} winning). On the other hand, the definition of cost in  considered strategies for Player~1 and ignore the parity winning condition. We now define the {\em sure cost\/} of the parity-MDP, which does take them into account. 
For a strategy  for Player~1, the sure cost of , denoted , is , if  is winning, and is  otherwise. The sure cost of  is then .
\vspace*{-8pt}
\paragraph*{End Components}
Consider a parity-MDP . An {\em end component\/} (EC, for short) is a set  such that for every state , the following hold.

\vspace*{-8pt}
\begin{enumerate}
\item If , then there exists an action  such that .
\item If , then for every  such that , it holds that  .
\item For every , there exist a path  and actions  such that for every , it holds that , and there exists an action  such that .
\end{enumerate}
Intuitively, the probabilistic player cannot force to leave , and Player~1 has positive probability of reaching every state in  from every other state. 

For an EC  and a state , we can consider the parity-MDP , in which the states are , the initial state is , and all the components are naturally restricted to . Since  is an EC, then this is indeed a parity-MDP. An EC  is {\em maximal\/} if for every nonempty  
, we have that  is not an EC.

\stam{
Consider a set . A {\em environment attractor} for , denoted  is defined inductively as follows. First, . Now, for every , let . Then, . It is well known that  can be computed in time polynomial in the description of . We analogously define the {\em system attractor} , by swapping the roles of Players  and .
}

\section{Solving Parity MDPs}
\label{sec: solving}
In this section we study the problem of finding the sure cost for an MDP. Recall that for MDPs, there always exists an optimal memoryless strategy. We start by demonstrating that for the sure cost of parity-MDPs, the situation is much more complicated.
\begin{theorem}
\label{no opt}
There is a parity-MDP  in which Player~1 does not have an optimal strategy (in particular, not a memoryless one) for attaining the sure cost of . Moreover, 
for every ,
Player~1 
may need
infinite memory in order to 
-approximate .
\end{theorem}
\begin{proof}
Consider the parity-MDP  appearing in Figure~\ref{fig:infinite mem}. Player~1 can decrease the cost of a play towards  by staying in the initial state. However, in order to ensure an even parity rank, Player~1 must either play  and reach a states with parity rank  and cost  w.p. , or play  but incur cost . A finite memory strategy for Player~1 must eventually play  from the initial state in every play,\footnote{Note that this also implies that randomized strategies could not be of help here.} thus the cost of every winning finite-memory strategy is . On the other hand, for every , there exists an infinite memory strategy  that gets cost at most .
Essentially (see Lemma~\ref{lem: GEC value} for a formal proof of the general case), the strategy  plays  for a long time. If the state with parity rank  is reached, it plays  for even longer, and otherwise plays .
\begin{figure}[ht]
\begin{center}
\begin{gpicture}(109,21)(0,0)
  \node[Nmarks=i,iangle=270](s0)(45,9){}
  \node[Nmarks=n](s1)(20,9){}
  \node[Nmarks=n,Nmr=0](s2)(70,9){}
  \node[Nmarks=n,Nmr=0](s4)(95,9){}
  \gasset{loopdiam=5}
  \drawloop[loopangle=90](s0){}
  \drawloop[loopangle=90](s1){}
  \drawedge[](s0,s1){}
  \drawedge[ELside=r](s2,s4){}
  \drawedge[curvedepth=-7,ELside=r](s4,s0){}
  \gasset{curvedepth=3}
  \drawedge[ELside=r](s0,s2){}
  \drawedge[](s2,s0){}
\end{gpicture}
\end{center}
 \caption{The Parity MDP . States of Player~1 are circles, these of Player~2 are squares, with outgoing edges marked by their probability. Each state is labeled by its parity rank (left) and cost (right). Player~1 has no optimal strategy and needs infinite memory for an  approximation. }
\label{fig:infinite mem}
\end{figure}

Finally, there is no optimal strategy for Player~1, as every strategy that plays  from the initial state eventually (i.e., as a response to some strategy of Player~2) gets cost  with some positive probability. However, a strategy that never plays  is not parity-winning.
\end{proof}


Following Theorem~\ref{no opt}, our solution to parity MDPs suggests two algorithms. The first, described in Section~\ref{sec:infinite memory}, finds the limit value of all possible strategies, which corresponds to infinite-state transducers. The second, described in Section~\ref{sec:finite memory}, computes the limit value over all finite-memory strategies. 
The complexity of both algorithms is NPcoNP. Moreover, they are computable in polynomial time when an oracle to a two-player parity game is given.
Hence, our complexity upper bounds match the trivial lower bounds that arise from the fact that every solution to a parity-MDP is also a solution to a parity game.






\vspace*{-8pt}
\subsection{Infinite-Memory Strategies}
\label{sec:infinite memory}
In this section we study the problem of finding the sure cost of a parity-MDP when infinite-memory strategies are allowed. 
We prove the upper bound in the following theorem. As stated above, the lower bound is trivial.

\begin{theorem}
\label{thm:parityMDP infinite NP cap co-NP}
Consider a parity-MDP . Then,  can be computed in NPco-NP, and is parity-games hard.
\end{theorem}

 
Consider a parity-MDP .
We first remove from  all states that are not sure-winning for Player~1 in . Clearly, every strategy that attains  cannot visit a state that is losing in  . Thus, we henceforth assume that all states in  are winning for Player~1 in . 
We say that an EC  of  is {\em good} (\gec, for short) if its maximal rank is even. That is,  is even. 

The idea behind our algorithm is as follows. W.p. 1, each play in  eventually reaches and visits infinitely often all states of some EC. Hence, when restricting attention to plays that are winning for Player~1 in , it must be the case that this EC is good. It follows that the sure cost of  is affected only by the properties of its \gecs.
Moreover, since  the minimal expected mean-payoff value is the same in all the states of an EC, we can consider only maximal \gecs and refer to the value of an EC, namely the minimal expected value that Player~1 can ensure while staying in the EC. Our algorithm constructs a new MDP (without ranks)  in which the cost of a state is the value of the maximal \gec it belongs to. If a state does not belong to a \gec, then we assign it a very high cost in , where the intuition is that Player~1 cannot benefit from visiting this state infinitely often. We claim that the sure cost in the parity-MDP   coincides with the cost of the MDP .

Formally, for an EC , let  be the set of the states of  with the maximal parity rank in . By definition, this rank is even when  is a \gec. 
Note that if  and  are \gecs and , then  is also a \gec. Thus, we can restrict attention to maximal \gec.
For a \gec , there exists a memoryless strategy  that maximizes the probability of reaching  from every state  while staying in . Moreover, since  is an EC, the probability of reaching  by playing  is strictly positive from every state . Let  be a state in . Consider the MDP . 
Since  is EC, we have that  is independent of the initial state . Thus, we can define  as  for some .

Recall that our algorithm starts by a preprocessing step that removes all states that are not sure-winning for Player~1 in .
It then finds the maximal \gecs of  (using a polynomial-time procedure that we describe in Appendix~\ref{app inf memory finding gecs}), and obtain an MDP  by assigning every state within a \gec  the cost , and assigning every state that is not inside a \gec cost , where  is the maximal cost that appears in . We claim that .
 
Before proving the claim, note that all the steps of the algorithm except for the preprocessing step that involves a solution of parity game require polynomial time. In particular, the strategies  above are computable in polynomial time by solving a reachability MDP, and, by Theorem~\ref{thm:solving MDP in P}, so does the final step of finding . 

\stam{
Next, intuitively, we assign every edge that is not inside a \gec a high cost. Since our cost function is on states, we add a state on each such edge, as follows. For every transition  for  such that  and  are not in the same \gec, or not in a \gec at all, we add a state  that replaces the transition from  to  by two transitions going through . We set , where  is the maximal cost that appears in . We take  since a transition through  consists of two transitions, instead of the original transition in .
Thus, we obtain an MDP , and we return .
}

\stam{
In order to find the maximal \gec of , we proceed as follows. 
\begin{enumerate}
\item Compute the maximal EC decomposition of .
\item For every maximal EC , if  is not good (i.e., the maximal parity rank in  is odd), remove it from the graph  and go to (1).
\item Once all the remaining components are good, return them.
\end{enumerate}
We remark that upon returning to step (1) from (2), it may be that the graph of  is not connected. Still, we find the decomposition in all the components.

We now turn to analyze the runtime of the algorithm.
First, computing the \gec of  can be done in polynomial time. Indeed, finding the maximal EC decomposition of  takes polynomial time (see \cite{CH14}
), and going over each component and checking that the maximal priority is even is clearly polynomial.
Next, finding the attractor takes polynomial, and removing the attractor can be done at most a polynomial number of times. 

The rest of the algorithm consists of adding a polynomial number of states with polynomial cost, and finding the value of an MDP, which by Theorem~\ref{thm:solving MDP in P} can be done in polynomial time.

Finally, we prove the correctness of the algorithm. 
We start with the following claim, which explains the correctness of our algorithm within \gec.
}

Proving that  involves the following steps (see Appendix~\ref{app inf memory} for the full proof).
First, proving  is not hard, as a play with a winning strategy  for Player~1 in  reaches and stays in some \gec  w.p. 1, and within , the best expected cost one can hope for is , which is exactly what the strategy  attains when played in .

Next, proving , we show how an optimal strategy  in  induces an -optimal strategy  in . We start with Lemma~\ref{lem: GEC value}, 
which 
justifies the 
costs within a \gec.
\begin{lemma}
\label{lem: GEC value}
Consider a \gec  in , and . Let , then for every  there exists a strategy  of

 with . 
\end{lemma}


Intuitively, in a good EC,  minimizes the expected mean-payoff and \emph{once in a while} it plays reachability, aiming to visit to a state with the maximal rank in the EC. Since the EC is good, this rank is even.
If reachability is not obtained after  rounds, for a parameter , then  gives up and aims at only surely satisfying the parity objective (our preprocessing step ensures that this is possible).
Otherwise, after reaching the maximal rank,  switches again to minimizing mean-payoff.
This process is repeated forever, increasing  in each iteration.
Hence, the probability that Player~1 eventually gives up can be bounded from above by an arbitrarily small . Accordingly, Player~1 can achieve a value that is arbitrarily close to .

Finally, we construct the -optimal strategy  in  as follows. 
The strategy  first mimics  for a large number of steps , or until an EC (in which  stays forever) is reached.
If a good EC is not reached, then  aims at only surely satisfying the parity objective. If a good EC is reached, then  behaves as prescribed above, per Lemma~\ref{lem: GEC value}. Since the probability of  reaching a good EC within  steps tends to , then Player~1 can achieve a value within  of .

\stam{
We start with the ``easy'' direction, proving that . Consider a zwinning strategy  for . With probability , the play of  in  reaches and stays in some \gec . From every state in , the minimal expected cost (when staying in ) is . Indeed,  is the cost of an MDP without the parity condition, which can only lower the minimal expected cost.
 Thus, we have that .

Consider the strategy  as a strategy for . Then, , and we conclude that .

For the other direction, we show that . Since  is an MDP, then there exists an optimal memoryless strategy  such that . We show that for every , there exists a winning strategy  for  such that . 

Observe that since  is memoryless and optimal, there exists a set of ECs   such that for every , once  reaches a state , it stays in  forever. Moreover, observe that every  must be a \gec. Indeed, the states outside a \gec in  have value , but from every state in  there exists a strategy that is parity-winning, and therefore ensures that a \gec is reached. Thus, if  gets stuck in an EC that is not good, we can modify it to reach a \gec, thus decreasing its cost. 

Let . There exists some  such that after  steps, w.p. at least  a play in  reaches a \gec in  (for  which we will fix later). We obtain  from  as follows.  simulates  for  steps. During this simulation, whenever  reaches a \gec ,  starts playing the strategy described in the proof of Lemma~\ref{lem: GEC value} for . After  steps,  plays a parity-winning strategy. 

Clearly  is parity-winning. In addition, by our choice of  and by Lemma~\ref{lem: GEC value}, it follows that w.p. at least , the cost of  is at most . Thus, , and for a small enough , this is at most .
}
\vspace*{-8pt}
\subsection{Finite-Memory Strategies}
\label{sec:finite memory}
In this section we study the problem of finding the sure cost of a parity-MDP, when restricted to finite memory strategies. For a parity-MDP , we define  is a finite memory strategy for . We prove the upper bound in the following theorem. As stated above, the lower bound is trivial.


\begin{theorem}
\label{thm:parityMDP finite NP cap co-NP}
Consider a parity-MDP . Then,  can be computed in NPco-NP, and is parity-games hard.
\end{theorem}

\vspace*{-5pt}
The general approach is similar to the one we took in Section~\ref{sec:infinite memory}. That is, we remove from  all states that are not sure-winning for Player 1 in , and proceed by reasoning about a certain type of ECs. However, for finite-memory strategies, we need a more restricted class of ECs than the \gecs that were used in Section~\ref{sec:infinite memory}. Indeed, a finite-memory strategy might not suffice to win the sure-parity condition in a \gec.

For a \gec , let  be the maximal odd priority in , with  if there are no odd priorities. We define .
 We say that a \gec  in  is {\em super good} (\sgec, for short) if from every state , there exists a finite-memory strategy  for  such that the play of  under  reaches  w.p. 1, and 
if the play does not reach , then it is parity winning.
We refer to  as a {\em witness} to  being a \sgec. If  is not a \sgec, we refer to the states of  that satisfy the above as {\em super-good states}.

We argue that \sgecs are the proper notion for reasoning about finite-memory strategies. Specifically, we show that in a \sgec, Player~1 can achieve -optimal expected cost with a finite-memory strategy, and that every finite-memory winning strategy reaches a \sgec w.p. 1.

Our algorithm finds the maximal \sgecs of  and obtain an MDP  in the same manner we did in Section~\ref{sec:infinite memory}, namely by assigning high weights to states not in \sgecs, and the optimal mean-payoff MDP value to states in \sgecs. As there, we claim that . The analysis of the algorithm as well as its concrete details, are, however, more intricate. 

We start by proving that the notion of maximal \sgecs is well defined. 
To this end, we present the following lemma, whose proof appears in Appendix~\ref{apx:existence of max SGEC}. Note that in the case of \gecs, the lemma was trivial.
\vspace*{-5pt}
\begin{lemma}
\label{lem:existence of max SGEC}
Consider \sgec  and , such that , then  is also a \sgec.
\end{lemma}
\vspace*{-5pt}
Intuitively, we prove this by considering witnesses  for  and  being \sgecs. We then modify  such that from every state in , it tries to reach  for  steps, for some parameter . Once  is reached,  takes over. If  is not reached,  attempts to reach . Thus, w.p. 1, the strategy reaches , and if it does not, it either reaches  infinitely often, or wins the parity condition.

Next, we note that unlike the syntactic definition of \gecs, the definition of \sgecs is semantic, as it involves a strategy. Thus, finding the maximal \sgecs adds another complication to the algorithm. In fact, it is not hard to see that even checking whether an EC is a \sgec is parity-games hard. Using techniques from~\cite{CD11}, we show in Appendix~\ref{apx:verify SGEC} that we can reduce the latter to the problem of solving a parity-\buchi game. 
We thus have the following lemma.
\vspace*{-5pt}
\begin{lemma}
\label{lem:verify SGEC}
Consider an EC  in a parity-MDP . We can decide whether  is a \sgec in NP co-NP, as well as compute a witness strategy and, if  is not a \sgec, find the set of super-good states.
\end{lemma}
\vspace*{-5pt}




Next, we show how to find the maximal \sgecs of . Essentially, for every odd rank , we can find the \sgecs whose maximal odd rank is  by removing all states with higher odd ranks, and recursively refining ECs by keeping only super-good states, using Lemma~\ref{lem:verify SGEC}. Thus, we have the following (see Appendix~\ref{apx:maximal SGEC decomposition} for complete details).
\vspace*{-5pt}
\begin{theorem}
\label{thm:maximal SGEC decomposition}
Consider a parity-MDP . We can find the maximal \sgecs of  in NPco-NP.
\end{theorem}
\vspace*{-5pt}


Theorem~\ref{thm:maximal SGEC decomposition} shows that our algorithm for computing  solves the problem in NPco-NP. 
It remains to prove its correctness.
First, Lemma~\ref{lem: SGEC value} justifies the assignment of costs within a \sgec.
\vspace*{-5pt}
\begin{lemma}
\label{lem: SGEC value}
Consider a \sgec  in  and a state  in . Let . 
Then, for every , there exists a finite-memory strategy  of  with . 
\end{lemma}
\vspace*{-10pt}
\begin{proof}
Let  be a memoryless strategy such that . By Theorem~\ref{thm:solving MDP in P} such a strategy exists. Let  be a finite-memory strategy that witnesses  being a \sgec.
For every , consider the strategy  that repeatedly plays  for  steps and then plays  until  is reached.
Since  and  are finite-memory, then  is finite memory. In addition, observe that  reaches  w.p. 1, and if  is not reached, then  is parity-winning. Thus  is parity-winning, and it reaches Step 1 infinitely often w.p. 1. Moreover, since  has finite memory, then for every , there is a bounded probability  that  reaches  within  steps, with . Thus, we get that , which concludes the proof.
\end{proof}

\vspace*{-10pt}
Lemma~\ref{lem: SGEC value} implies that we can approximate the optimal value of \sgecs with finite-memory strategies. 
It remains to show that it is indeed enough to consider \sgecs. Consider a finite-memory strategy . Then, w.p. 1,  reaches an EC. Let  be an EC with . The following lemma characterizes an assumption we can make on the behavior of  in such an EC.
\vspace*{-5pt}
\begin{lemma}
\label{lem:finite memory EC prob 1}
Consider a parity-MDP  and an EC . For every finite-memory strategy , if , then there exists a finite-memory strategy  such that for every , we have that  and every play of  from  stays in . Moreover, if  is parity winning, then so is .
\end{lemma}
\vspace*{-5pt}


Intuitively, we show that there exists some finite history  such that the strategy , which is  played after seeing the history , has the following property:  reaches and stays in , and w.p. 1 visits infinitely often all the states in , and in particular . 
For the proof, we consider the set . Since  has finite memory, it follows that this set is finite. Using this, we show that if  for every , then , which is a contradiction. 
Finally, since  is also parity winning, it follows that  above is also parity-winning, and is thus a witness for  being a \sgec. 
The full proof appears in Appendix~\ref{apx:finite memory EC prob 1}.

Finally, by Lemma~\ref{lem:finite memory stays in SGEC}, we can assume that once  reaches an EC , it stays in  and visits all its states infinitely often w.p. 1. Since  is parity-winning, it follows that  has a maximal even rank, and that  reaches  w.p. 1. Moreover, in every play that does not reach ,  wins the parity condition. We can thus conclude with the following Lemma, which completes the correctness proof of our algorithm for computing . See Appendix~\ref{apx:finite memory stays in SGEC} for the proof.
\vspace*{-5pt}
\begin{lemma}
\label{lem:finite memory stays in SGEC}
Consider a parity-MDP  and an EC . For every finite-memory strategy , if  is parity winning and , then  is a \sgec.
\end{lemma}


\vspace*{-20pt}
\subsection{Comparison with Related Work}
\label{subsec:overview_discussion}
Both our work and~\cite{BFRR14a,CR15} analyze ECs and reduce the problem to reasoning about an MDP that ignores the hard constraints.
The main difference with~\cite{BFRR14a} is that there, the hard and soft constraints have the same objective (i.e., worst-case mean-payoff value and expected-case mean-payoff value).
In~\cite{BFRR14a}, the strategy played for  rounds to satisfy the soft objective and then at most  rounds to satisfy the hard objective, for some constants  and .
In our setting, we cannot bound , and in fact it might be the case that Player~1 would play to satisfy the parity objective for the rest of the game (i.e., forever) even after reaching a super-good end component.

The difference with~\cite{CR15} is twofold. First, technically, the type of hard constraints in~\cite{CR15} is worst-case mean-payoff, whereas our setting uses the Boolean parity condition. In classical parity games, the parity condition can be reduced to a mean-payoff objective. Similar reductions, however, do not work in order to reduce our setting to the setting of~\cite{CR15}. Thus, our contribution is orthogonal to~\cite{CR15}. Secondly, Boolean constraints are conceptually different than quantitative constraints, and as we demonstrate in Section~\ref{sec:applications}, they arise naturally in quantitative extensions of Boolean paradigms.

We note that~\cite{CR15} also study a relaxation in which  almost-sure winning is allowed for the hard constraints. An analogue in our setting is to consider an almost-sure parity condition. We note that in such a setting, \gecs are sufficient for reasoning both about finite-memory and infinite-memory strategies. 
Moreover, the preprocessing involves solving an almost-sure parity MDP (without mean-payoff constraints), which can be done in polynomial time.
Thus, as is the case in~\cite{CR15}, we can compute the cost of an MDP with almost-sure hard constraints in polynomial time.




\section{Applications}
\label{sec:applications}

In this section we study two applications of parity-MDPs. Both extend the Boolean synthesis problem. 
Due to lack of space, our description is only an overview. Full definitions and details can be found in Appendix~\ref{app app}.
We start with some basic definitions.

For finite sets  and  of input and output signals, respectively, an {\em   transducer} is , where  is a set of states,  is an initial state,  is a total (deterministic) transition function, and  is a labeling function on the states. The run of  on a word  is the sequence of states  such that  for all . The {\em output} of  on  is then  where  for all . Note that the first output assignment is that of , and we do not consider . This reflects the fact that the environment initiates the interaction. The {\em computation of  on \/} is then 
.
When  is a finite set, we say that the transducer is  finite.

The synthesis problem gets as input a specification  and generates a transducer  that realizes ; namely, all the computations of  are in . The language  is typically given by an LTL formula \cite{Pnu81} or by means of an automaton of infinite words. 


\vspace*{-8pt}
\subsection{Penalties on Undesired Scenarios}
Recall that in the Boolean synthesis problem, the goal is to generate a transducer that associates with each infinite sequence of inputs an infinite sequence of outputs so that the result computation satisfies a given specification. Typically, some behaviors generated by the transducers may be less desired than others. For example, as discussed in Section~\ref{sec:intro}, designs that use fewer resources or minimize expensive activities are preferable. The input to the {\em synthesis with penalties\/} problem includes, in addition to the Boolean specification, languages of finite words that describe undesired behaviors, and their costs. 
The goal is to generate a transducer that realizes the specification and minimizes cost due to undesired behaviors. 

Formally, the input to the problem includes languages  of finite words over the alphabet  and a penalty function  specifying for each  the penalty that should be applied for generating a behavior in . As described in Section~\ref{sec:intro}, the language  may be local (that is, include only words of length 1) and thus refer only to activation of output signals, may specify short scenarios like flips of output signals, and may also specify rich regular scenarios. Note that we allow penalties also for behaviors that depend on the input signals. Intuitively, whenever a computation  includes a behavior in , a penalty of  is applied. Formally, if , then for every position , we define . That is,  points to the subset of languages  such that a word in  ends in position . Then, the cost of position , denoted , is . 
Finally, for a finite computation , we define its cost, denoted , as .

Let  be a deterministic parity automaton (DPW, for short) over the alphabet  that specifies the specification . 
We describe a parity-MDP whose solution is a transducer that realizes  with the minimal cost for penalties. The idea is simple: on top of the parity game  described above, we compose monitors that detect undesired scenarios. We assume that the languages  and are given by means of deterministic automata on finite words (DFWs)   where for every , we have that . That is,  accepts  iff there exists  such that . Essentially, we turn  into a parity-MDP by composing it with the DFWs . Then,  reaching an accepting state indicates that the penalty for  should be applied, which induces the costs in the parity-MDP. The probabilities in the parity-MDP are induced form the distribution of the assignments to input signals. The full construction can be found in Appendix~\ref{app penalties}. We note that an alternative definition can replace the DFWs  and the cost function  by a single weighted automaton that can be composed with .

\stam{
Let 
Let  and . We define the parity-MDP  where for every , , and , we have the following. The transition functions are , and , the cost function is given by  and , for the penally function , and the acceptance condition is . Finally, we assume that the environment behaves uniformly. That is, in every step it outputs every  with probability . Thus, . This assumption can easily be replaced by a different probabilistic model. 

It is easy to see that a winning strategy for Player~1 in  corresponds to a transducer that realizes , and that the cost of every computation is the average penalty along the computation. Thus, a solution to the synthesis with penalties problem amounts to solving . The size of  is polynomial in the size of the automata , and is exponential in . However, we observe that the role of  is only for the purpose of costs, and does not affect the parity constraints. Thus, we can solve the problem in NPco-NP in the size of the automata, and in time singly-exponential in .
Finally, if  is obtained by translating an LTL formula  into a DPW, then similarly to the case of Boolean synthesis, we can solve the problem in times doubly-exponential in the length of , polynomial in , and singly-exponential in .
}
 
\subsection{Sensing}
\label{sec:sensing}
Consider a transducer  . For a state  and a signal , we say that  is {\em sensed in\/}  if there exists a set  such that . Intuitively, a signal is sensed in  if knowing its value may affect the destination of at least one transition from . We use  to denote the set of signals sensed in . 
The {\em sensing cost\/} of a state  is . 
For a finite run  of , we define the sensing cost of , denoted , as . That is,  is the average number of sensors that  uses during . For a finite input sequence , we define the sensing cost of  in , denoted , as the sensing cost of the run of  on . 
Finally, the sensing cost of  is the expected sensing cost of input sequences of length that tends to infinity, which is parameterized by a distribution 
on  given by a sequence of distributions  such that  describes the distribution over  at time . For simplicity, we assume that the distribution is uniform. Thus,  for every . 
For the uniform distribution we have . 

Note that this definition also applies when the transducer is infinite. However, for infinite transducers, the limit in the definition of  might not exist, and we therefore define . Finally, for a realizable specification , we define  is an  transducer that realizes .

In~\cite{AKK15}, we study the sensing cost of {\em safety} properties. 
We show that there, a finite, minimally-sensing transducer, always exists (albeit of exponential size), and the problem of computing the sensing cost is EXPTIME-complete. In our current setting, however, a minimally-sensing transducer need not exist, and 
any approximation may require infinite memory. 
We demonstrate this with an example. 

\begin{example}
Let  and , and consider the specification . Thus,  states that either  holds infinitely often and  always holds, or, if  does not holds at a certain time, then henceforth,  holds iff  holds. Observe that once the system outputs , it has to always sense  in order to determine the output. 
The system thus has an incentive to always output . This, however, may render  false, as  need not hold infinitely often.

We start by claiming that every finite-memory transducer  that realizes  has sensing cost 1. Indeed, let  be the number of states in . A random input sequence contains the infix  w.p. 1. Upon reading such an infix,  has to output , as otherwise it would not realize  on a computation with suffix . Thus, from then on,  
senses
 in every state. So .

However, by using infinite-memory transducers, we can follow the construction in Section~\ref{sec:infinite memory} and reduce the sensing cost arbitrarily close to . Let . We construct a transducer  as follows. After initializing  to , the transducer  senses  and outputs  for  steps. If  does not hold during this time, then  outputs  and starts sensing  and outputting  accordingly. Otherwise, if  holds during this time, then  stops sensing  for  steps, while outputting . It then increases  by  and repeats the process. 
Note that  outputs  iff  does not hold for  consecutive positions at the -th round (which happens w.p. ). Thus, the probability of  outputting  in a random computation is bounded from above by , which tends to  as  tends to . Note that in the -th round,  senses  for only  steps, and then does not sense anything for  steps, so if  does not output , the sensing cost is . Thus, we have 
. \qed






\end{example}

We proceed by describing the general solution to computing the sensing cost of a specification.
Recall that synthesis of a DPW  is reduced to solving a parity game.
When sensing is introduced, it is not enough for the system to win this game, as it now has to win while minimizing the sensing cost. Intuitively, not sensing some inputs introduces incomplete information to the game: once the system gives up sensing, it may not know the state in which the game is and knows instead only a set of states in which the game may be. In particular, unlike usual realizability, a strategy that minimizes the sensing need not use the state space of the \DPW. 

\vspace*{-5pt}
\begin{theorem}
\label{thm:sensing to parity-MDP}
Consider a  specification  over . There exists a parity-MDP  such that .	
Moreover, the number of states of  is singly exponential in that of , and the number of parity ranks on  is polynomial in that of .
\end{theorem}
\vspace*{-10pt}
\begin{proof}
Conceptually, we follow the ideas of Boolean synthesis, by thinking of  as a parity game between the system and the environment, as described in Section~\ref{sec:application defs}. The proof is comprised of several steps. First, intuitively, we give the system an option to sense only some input signals , but require that then, the play must be winning for every assignment of the inputs that are not sensed. Then, we introduce costs induced by the number of sensed input signals in each state, and finally we add a uniform stochastic environment. 
Technically, however, the first step is done using automata, rather than games, and converts the   into a {\em universal parity automaton\/} (UPW) -- an automaton in which a the transition function maps each state and letter to more than a single successor state, and a word is accepted if all the runs on it are accepting. We use the universal branches of the UPW in order to model the several possible assignments to the input signals that are not sensed. Thus, in state  of , the system chooses a state , where  represents the inputs to be sensed. The environment then chooses an assignment  for the inputs, and the system chooses an output assignment . However, instead of the new state being , a universal transition is taken to every state  such that  for some   that agrees with  on every input in . Thus, effectively, the system has to play only according to the values of the sensed inputs. Note that the two players are not modeled in the automaton. Rather, their choices are represented by augmenting the alphabet to include a  component to represent the sensed inputs.
Using automata allows us to determinize the  back to a  that already captures sensing. We then convert the automaton to a parity-game, and proceed as described above.

For the formal details, see Appendix~\ref{app sen construction}.
\end{proof}
\stam{

Conceptually, we follow the ideas of Boolean synthesis, by thinking of  as a parity game between the system and the environment, as described in Section~\ref{sec:application defs}. The proof is comprised of several steps. First, intuitively, we give the system an option to sense only some input signals , but require that then, the play must be winning for every assignment of the inputs that are not sensed. Then, we introduce costs induced by the number of sensed input signals in each state, and finally we add a uniform stochastic environment. 
Technically, however, the first step is done using automata, rather than games, by converting the   to a , as we explain below. Using automata allows us to determinize the  back to a  that already captures sensing. We then convert the automaton to a parity-game, and proceed as described above.



We now turn to formalize this intuition. 
In the following, we identify a subset  with its characteristic function . 

Consider the  . We obtain from  the   with  defined as follows. Consider a letter . We think of  and  as truth assignments for the input and output signals, respectively, and we think of  as a set of sensed signals. Consider the set . Intuitively,  is the set of input assignments that agree with  on all the signals in . For a state , we define . 

Intuitively, when thinking of  as a game between the system and the environment, then at each step, the system chooses a set of sensed inputs  and an output . Then, the environment chooses a set of inputs , but in the next step the system can only see the inputs in  that are sensed in , and thus moves universally with every input that agrees with  on the sensed inputs in .

We proceed to determinize  to a DPW . We then obtain from  a parity game, as described above, with Player~1 (the system) controlling the set of sensed inputs and the output, and Player~2 (the environment) controlling the concrete inputs. Formally, the game 
 is defined as follows. The states are  and . The actions for Player~1 in every state are  and are  for Player~2 (we omit the state as the available actions are independent of the state). The transition function is defined as follows. For a state  and action  we have  as well as . 
For a state  and action  we have . 

Intuitively, the state  represents that  is in state , the system has chosen to sense the signals in , and the environment gave the concrete input . Then, the action  means that the system responded with output , and chose to sense  in the next step, taking the game to the state , where . Then, in state , the environment chooses a new concrete input .

We define the acceptance condition  as follows. For every  and , we have , and we arbitrarily set  (since  is visited only once, this has no effect).

Note that crucially, for every , the behavior of  from state  is identical to the behavior from . This follows from the universal transitions in . Thus, once Player~1 chooses , the inputs that are not sensed do not play a role. This captures the fact that every winning strategy for the system must only rely on the values  assigns to the sensed inputs .

Finally, the parity-MDP  is obtained from  by fixing Player~2 with a uniform-stochastic strategy and adding costs according to the number of sensed inputs at each state. Recall that the actions of Player~2 are . Thus, in state , the probability of Player~2 playing  is . Note that by our observation above, every , induce the same transitions. Thus, the probability of transition from state  to  is .

The cost function assigns cost  to states  and , for every  and .

We now proceed to analyze the correctness of the construction. Consider a (not necessarily finite) transducer  that realizes the specification . We identify with  a strategy  for  as follows. 
In state  we have . Then, the strategy  keeps track of the state of  as follows. When  is in state , and the state of the game is , let . Then, we have that . Observe that  is essentially implemented by the transducer . In particular, if  has finite state space, then  has finite memory.

We claim that . We start by showing that  is sure winning in  (equivalently, that it is a winning strategy for Player~1 in ). Consider an input sequence , let  and  be the states that  and  reach, respectively, when they interact on . let , then for every  such that  we have that . Thus, the behavior of  from  and from  is the same. It follows that  induces a realizing strategy for the UPW  (and hence a winning strategy for ), where the additional  component in the alphabet represents the sensing of the current state of . However, this is exactly the behavior prescribed by , so  is winning in . 

Next, observe that by the above, for every input sequence , the (prefix of the) play of  induced by Player~1 playing  and Player~2 playing  is  , and we have that , while for the run  of  on the first  letters of  we have that . By the definition of  and , we have . Moreover, the probabilities of  imply that every  such that  is played w.p. . Thus, by taking , we get . 

Since this is true for every realizing transducer , it follows that .

Conversely, consider a strategy  for . A-priori,  can behave differently in states  and  for . However, as we observed above, the construction of  (and thus of ) implies that  cannot decrease its cost by doing so, since the behavior of  is the same in both states. Thus, we can assume w.l.o.g that  only depends on the values  assigns to the sensed inputs . Now,  induces a (possibly infinite) transducer  in an obvious manner - whenever  outputs , the transducer outputs . Similar arguments as the converse direction show that , and thus , and we are done.
\end{proof}
}\vspace*{-12pt}
\begin{theorem}
\label{thm:compute sensing cost}
Consider a  specification  over . We can compute  in singly-exponential time. Moreover, the problem of deciding whether  is EXPTIME-complete.
\end{theorem}
\vspace*{-13pt}
\begin{proof}
We obtain from  a parity-MDP  as per Theorem~\ref{thm:sensing to parity-MDP}. Observe that the algorithm in the proof of Theorem~\ref{thm:parityMDP infinite NP cap co-NP} essentially runs in polynomial time, apart from solving a parity game, which is done in NPco-NP. However, deterministic algorithms for solving parity games run in time polynomial in the number of states, and singly-exponential in the number of parity ranks. Since the number of parity ranks in  is polynomial in that of , we can find  in time singly-exponential in the size of . Since , we are done.

For the lower bound, we note that the problem of deciding whether  is EXPTIME-hard even for a restricted class of automata, namely looping automata~\cite{AKK15}. 
\end{proof}

The input to the synthesis problem is typically given as an \LTL formula, rather than a \DPW. Then, the translation from  to a  involves a doubly-exponential blowup. Thus, a naive solution for computing the sensing cost of a specification given by an  formula is in 3EXPTIME. However, by translating the formula to a \UPW, rather than a \DPW, we show how we can avoid one exponent, thus matching the 2EXPTIME complexity of standard Boolean synthesis.
\vspace*{-5pt}
\begin{theorem}
\label{thm:LTL sensing cost}
Consider an \LTL specification  over . We can compute  in doubly-exponential time.
\end{theorem}
\vspace*{-12pt}
\begin{proof}
We start by translating  to a   of size single-exponential in the size of . This can be done, for example, by translating  to a nondeterministic \buchi automaton~\cite{VW94} and dualizing it. We then follow the proof of Theorem~\ref{thm:compute sensing cost}, by adding the universal transitions described there directly to the  . Thus, when we finally determinize the  to a , the size of the  is doubly-exponential, 
so 
computing the sensing cost can also be done in doubly-exponential time.
\end{proof}

\small
\bibliographystyle{plain}
\bibliography{../ok}

\normalsize

\appendix

\section{Calculating the Sure Cost in the Infinite-Memory Case}
\label{app inf memory}

Our algorithm uses the notion of attractors, defined below.
Consider a set . A {\em environment attractor} for , denoted  is defined inductively as follows. First, . Now, for every , let . Then, . It is well known that  can be computed in time polynomial in the description of . We analogously define the {\em system attractor} , by swapping the roles of Players  and .

\subsection{Finding the Maximal \gecs of }
\label{app inf memory finding gecs}

In order to find the maximal \gecs of , we proceed as follows. 
\begin{enumerate}
\item Compute the maximal EC decomposition of .
\item For every maximal EC , if  is not good (i.e., the maximal parity rank in  is odd), remove it from the graph  and go to (1).
\item Once all the remaining components are good, return them.
\end{enumerate}
Note that upon returning to step (1) from (2), it may be that the graph of  is not connected. Still, we find the decomposition in all the components.

It is not hard to see that all the steps of the algorithm are polynimial. In particular, finding the maximal EC decomposition of  takes polynomial time \cite{CH14}.

\subsection{Proof of Lemma \ref{lem: GEC value}}

Consider a memoryless strategy  that maximizes the probability to reach , and a memoryless strategy  whose expected cost in  is . By Theorem~\ref{thm:solving MDP in P}, such a strategy  exists.

We construct an infinite-memory strategy  that works in phases, as follows. In phase 1,  works in iterations. In iteration , the strategy  plays  for  steps. Then,  plays  for  steps, where  is a constant we determine later and  is the number of states of . If, during these  steps, the generated play reached , then we proceed to the next iteration. Otherwise,  goes to phase 2, in which it plays a parity-winning strategy (which exists, since every state in  is parity winning).

Clearly, if the play generated by  never reaches Phase 2, then playing  for  steps is the dominant factor, and we have that . Thus, it remains to bound the probability that the play reaches Phase 2. 
Denote by  the maximal probability that a play of  does not reach  within  steps, where the maximum is taken over all states of . Since  is strongly connected, it follows that . Thus, the probability of not reaching Phase 2 is bounded from below by . The latter expression converges to a number  in  that is 
inversely-related to . Therefore, by setting  large enough, we can lower the probability of reaching Phase 2 arbitrarily. Since the cost of a play after reaching Phase 2 is bounded from above by , the claim follows.


\subsection{A proof that }

We start with the ``easy'' direction, proving that . Consider a winning strategy  for . With probability , the play of  in  reaches and stays in some \gec . From every state in , the minimal expected cost (when staying in ) is . Indeed,  is the cost of an MDP without the parity condition, which can only lower the minimal expected cost.
 Thus, we have that .

Consider the strategy  as a strategy for . Then, , and we conclude that .

For the other direction, we show that . Since  is an MDP, then there exists an optimal memoryless strategy  such that . We show that for every , there exists a winning strategy  for  such that . 

Observe that since  is memoryless and optimal, there exists a set of ECs   such that for every , once  reaches a state , it stays in  forever. Moreover, observe that every  must be a \gec. Indeed, the states outside a \gec in  have value , but from every state in  there exists a strategy that is parity-winning, and therefore ensures that a \gec is reached. Thus, if  gets stuck in an EC that is not good, we can modify it to reach a \gec, thus decreasing its cost. 

Let . There exists some  such that after  steps, w.p. at least  a play in  reaches a \gec in  (for  which we will fix later). We obtain  from  as follows.  simulates  for  steps. During this simulation, whenever  reaches a \gec ,  starts playing the strategy described in the proof of Lemma~\ref{lem: GEC value} for . After  steps,  plays a parity-winning strategy. 

Clearly  is parity-winning. In addition, by our choice of  and by Lemma~\ref{lem: GEC value}, it follows that w.p. at least , the cost of  is at most . Thus, , and for a small enough , this is at most .


\section{Calculating the Sure Cost in the finite-Memory Case}
\label{apx:finite memory}

\subsection{Proof of Lemma~\ref{lem:existence of max SGEC}}
\label{apx:existence of max SGEC}
Assume w.l.o.g that the maximal odd priority in  is at least that of . Let  be witnesses to  and  being \sgec, respectively. We construct a witness  to  being a \sgec. In every state ,  behaves as  does. In a state ,  proceeds as follows. (1) It attempts to reach a state  (from which it behaves as ) within  steps, for a large enough  such that the probability of reaching  is positive (which exists, since  is an EC). (2) If  was not reached within  steps,  plays  until  is reached, and goes back to (1). 

Observe that . Clearly, the play under  reaches  w.p. 1. Moreover, if the play does not reach , then it is winning in the parity condition. Indeed, if the play under  reaches , then this holds (since  is a witness for  being a \sgec). Otherwise, the play of  either reaches  infinitely often, in which case it is winning in the parity condition, or it plays as  and does not reach , in which case it is parity winning, since  is a witness for  being a \sgec. 
We conclude that  is a \sgec.


\subsection{Proof of Lemma~\ref{lem:verify SGEC}}
\label{apx:verify SGEC}
Our solution proceeds as follows. We start by reducing the problem of deciding whether  is a \sgec to the problem of deciding whether there is a winning strategy in a parity-\buchi game, using techniques from~\cite{CD11}. We then show how the latter can be solved by a reduction to positive Mean-Payoff parity games.

A parity-\buchi game is a two player game  that is similar to a parity game, with the exception that the winning condition is composed of two conditions:  is a parity ranking function, and  is a set of {\em accepting states}. A play of  is winning for Player~1 iff it satisfies the parity condition , and visits  infinitely often.

We start by describing a reduction from the problem of deciding whether  is a \sgec to the problem of solving a parity-\buchi game. First, we check that  is a \gec. If  is odd, then  is not a \sgec and we are done.

Consider the parity game . We obtain from  a parity-\buchi game  as follows. 
First, we change every state in  to a \buchi accepting sink (while keeping the parity rank).

For every state  of Player~2 that is not in , we replace  with the gadget in Figure~\ref{fig:gadget}.
\begin{figure}[ht]

\begin{center}
\begin{gpicture}(94,30)(0,14)
  \node[Nmarks=n,Nmr=0](s)(10,29){}
  \node[Nmarks=r,Nmr=0](s2)(40,39){}
  \node[Nmarks=n](s1)(40,19){}
  \node[Nmarks=n](r)(70,39){}
  \node[Nmarks=n](t)(70,19){}
  \drawedge[](s,s2){}
  \drawedge[](s,s1){}
  \drawedge[](s2,r){}
  \drawedge[ELside=r,ELpos=25](s2,t){}
  \drawedge[ELside=l,ELpos=25](s1,r){}
  \drawedge[ELside=r](s1,t){}
  \gasset{curvedepth=3}
\end{gpicture}
\end{center}
 \caption{Gadget for the reduction in Lemma~\ref{lem:verify SGEC}. In , we have  and .}
\label{fig:gadget}
\end{figure}

Formally, we add the states , where  is a Player~1 state and  is a Player~2 state, whose successors are those of  (with the same available actions), and the successors of  are  and .

We set the parity ranks of the gadget to be , and for the \buchi objective, we set  and .

We claim that  is a \sgec iff Player~1 wins in  from every state. For the first direction, assume  is a \sgec, and let  be a witness strategy. Thus,  is finite memory strategy that reaches  w.p. 1, and wins in the parity condition in every play that does not reach .

We obtain from  a strategy  for Player~1 in  as follows.  plays similarly to , unless a state  as in the gadget  is reached, for some environment state . Then  chooses the neighbor that minimizes the distance to  (we assume w.l.o.g that in , the strategy  leads surely to ). We claim that  wins parity+\buchi in . Indeed, consider a strategy  for Player~2 in , and consider the play  induced by  and . Note that  induces a strategy in  by assigning each state  the action .
Assume by way of contradiction that  is not winning for Player~1. Thus, either the \buchi condition or the parity conditions do not hold. If the \buchi condition does not hold, then after a finite prefix, for every environment state ,  moves the play to  in the gadget (since ). Thus, however, eventually Player~1 forces the play to , which are \buchi-winning sinks, and this the \buchi condition and the parity conditions are satisfied. Thus, the \buchi condition holds. If the parity condition does not hold, then the play does not reach . Since  is a witness strategy for  being a \sgec, then every play in  induced by  and does not reach  is parity winning. Thus, the play in  induces similar parity ranks, with the exception of padding  ranks within the gadgets. In particular, this play is also parity winning in . Since this is true for every strategy , we conclude that  is parity-\buchi winning in .

conversely, assume that  is a parity-\buchi winning strategy in . In addition, we assume that  is finite memory. Since parity-\buchi is an -regular winning condition, then Player~1 has a finite-memory winning strategy. The strategy  induces a strategy for Player~1 in . We claim that this is a witness for  being a \sgec.
Indeed, similarly to the above, it is easy to see that  is parity winning if  is not reached. It remains to prove that  is reached w.p. 1. 

Since  has finite memory, then there exists  such that for every state  in , if Player~2 chooses  from every environment state  for  steps, then  reaches . However, w.p. 1, a stochastic environment chooses the same  choices that  would have chosen in the above  states. Thus, w.p. 1,  reaches .

This completes the reduction to parity-\buchi games.

Next, we reduce parity-\buchi games to Mean-payoff parity games by assigning every state in  payoff 1, and the rest payoff 0. Then, the goal is to win parity while having strictly positive long-run mean-payoff. These games can be solved in NPco-NP~\cite{CD11}.

In addition, in case  is not a \sgec, our solution finds the winning states for Player~1, which are the super-good states.


\subsection{Proof of Theorem~\ref{thm:maximal SGEC decomposition}}
\label{apx:maximal SGEC decomposition}
Intuitively, our algorithm works in two phases. First, for every odd rank , we find the maximal \sgec whose maximal odd rank is . Then, we choose among the \sgec the maximal ones. We start by describing a subroutine for the first phase.

Let  be the maximal parity rank in , and consider an odd rank . We compute the maximal \sgec with maximal odd rank  as follows.
\begin{enumerate}
\item Compute the maximal EC decomposition .
\item For every EC ,
\begin{enumerate}
\item Let . If , remove  from , and go to (1).
\item Decide if  is a \sgec. If it is, return it. Otherwise, find the set  of super-good states, remove  from , and go to (1).
\end{enumerate}
\end{enumerate}
Next, we run this subroutine for every odd  to obtain \sgec . Finally, for every , if , we remove  from the list. 

Clearly this algorithm has polynomially many iterations, and in each iteration we solve an NPco-NP problem, as per Lemma~\ref{lem:verify SGEC}. Thus, the algorithm solves the problem in NPco-NP.

It remains to prove the correctness of the algorithm.
By Lemma~\ref{lem:verify SGEC}, every component that is returned in the subroutine is a \sgec. Consider a \sgec  with maximal odd rank . In iteration  of the algorithm, none of the states of  are removed in steps 2a and 2b. Thus, the subroutine returns a \sgec  such that . Finally, by Lemma~\ref{lem:existence of max SGEC}, if , then there exists a \sgec  such that . Thus,  is also returned in the list, and will replace  and .
We conclude that the returned list contains exactly the maximal \sgec of .


\subsection{Proof of Lemma~\ref{lem:finite memory EC prob 1}}
\label{apx:finite memory EC prob 1}
Let  be a finite-memory strategy with memory . Consider a history . Let  be the memory element that  reaches after reading , we define the strategy  to be  when starting from . Note that the set  is finite, since  is finite.
We claim that there exists  that satisfies the conditions of the lemma. 

Indeed, assume by way of contradiction that for every  we have that . Thus, there exists  such that  for every . It follows that there exists  such that for every history , w.p. at least  the strategy  from  reaches either a state  or a state  such that there exists  that is not reachable from  under . Since  is independent of , and since this is true for every , we get that , in contradiction to the assumption. 

Let , then we conclude that . Assume by way of contradiction that for every  it holds that there exists a play of  from some state  that leaves  (which happens after a finite number of steps). Thus, there exists some  such that w.p. at least  (independent of ), for every  and every state  a play of  leaves  (since every  visits every state of  w.p. 1). This contradicts the fact that . 
We conclude that there exists  such that every play of  stays in  forever. 

In addition, since  is parity winning, and the parity condition is independent of the history, then  is parity winning too.


\subsection{Proof of Lemma~\ref{lem:finite memory stays in SGEC}}
\label{apx:finite memory stays in SGEC}
Let  be a strategy obtained as per Lemma~\ref{lem:finite memory EC prob 1}. Thus,  for every ,  every play of  from  stays in , and  is parity winning. We show that  is a \sgec by showing that  is a witness thereof.
Indeed, w.p. 1  visits every state of , and in particular  reaches  w.p. 1. In addition,  is parity-winning, so every play of  is parity winning, in particular plays that do not reach .
\qed

\section{Applications}
\label{app app}

\subsection{Automata, and the Boolean Synthesis Problem}
\label{sec:application defs}
\stam{
For finite sets  and  of input and output signals, respectively, an {\em   transducer} is , where  is a set of states,  is an initial state,  is a total (deterministic) transition function, and  is a labeling function on the states. The run of  on a word  is the sequence of states  such that  for all . The {\em output} of  on  is then  where  for all . Note that the first output assignment is that of , and we do not consider . This reflects the fact that the environment initiates the interaction. The {\em computation of  on \/} is then 
.
When  is a finite set, we say that the transducer is  finite.

The synthesis problem gets as input a specification  and generates a transducer  that realizes ; namely, all the computations of  are in . The language  is typically given by an LTL formula \cite{Pnu81} or by means of an automaton of infinite words. 
}

An {\em automaton\/} is a tuple 
 , where  is a finite set of states,  is an initial state,  is a transition function, and 
 is an acceptance condition. We define some acceptance conditions below.
The automaton  may run on finite or infinite words. 
A run of  on a finite word  is a sequence of states  such that  for all . When  is infinite, so is a run of  on it. For an infinite run , we denote by  the set of states that  visits infinitely often.
 
We consider two acceptance conditions. When  runs on finite words, we have that  is a set of accepting states. Then, a finite run  is accepting if . When  runs on infinite words, then  is a {\em parity\/} acceptance condition. For a state , we refer to  as the {\em rank} of . Then, an infinite run  is accepting if  is even.

The automata we consider are universal. Thus, 
a word  is accepted if all the runs of  on it are accepting.  
The language of , denoted , is the set of words that  accepts. 
If  for every  and , we say that  is {\em deterministic}. Note that in this case,  has exactly one run on every word.




The classical solution to the Boolean synthesis problem proceeds as follows. Consider a specification \DPW
 . We obtain from  a parity game
  , where , and . Thus, Player~2, the environment, controls the inputs and his actions correspond to assignments to the input signals. His states are the states of , and he moves to states that maintain the assignment he gives to the input signals. Then, Player~1, the system, controls the outputs and his actions correspond to assignments to the output signals. He moves in states that maintain the assignment to the input signals given by Player~2, and his transitions update the state of . Then,  in induced by . Formally, for every  and for every , we have that . It is not hard to see that a winning strategy for Player~1 in  induces a transducer that realizes  \cite{PR89a}. 
Finding a winning strategy for Player~1 amounts to solving a turn-based parity game, whose complexity is NPco-NP. Alternatively, deterministic algorithms for solving parity games run in time polynomial in the number of states, and singly-exponential in the number of parity ranks. When the starting point is an LTL formula , the translation to a \DPW involves a doubly-exponential blow up, but the index of the \DPW is only exponential, so the problem is 2EXPTIME-complete \cite{Ros92}.
 

\subsection{Synthesis with Penalties}
\label{app penalties}

Let 
Let  and . We define the parity-MDP  where for every , , and , we have the following. The transition functions are , and , the cost function is given by  and , for the penally function , and the acceptance condition is . Finally, we assume that the environment behaves uniformly. That is, in every step it outputs every  with probability . Thus, . This assumption can easily be replaced by a different probabilistic model. 

It is easy to see that a winning strategy for Player~1 in  corresponds to a transducer that realizes , and that the cost of every computation is the average penalty along the computation. Thus, a solution to the synthesis with penalties problem amounts to solving . The size of  is polynomial in the size of the automata , and is exponential in . However, we observe that the role of  is only for the purpose of costs, and does not affect the parity constraints. Thus, we can solve the problem in NPco-NP in the size of the automata, and in time singly-exponential in .
Finally, if  is obtained by translating an LTL formula  into a DPW, then similarly to the case of Boolean synthesis, we can solve the problem in times doubly-exponential in the length of , polynomial in , and singly-exponential in .

\subsection{Proof of Theorem~\ref{thm:sensing to parity-MDP}}
\label{app sen construction}

We identify a subset  with its characteristic function . 

Consider the  . We obtain from  the   with  defined as follows. Consider a letter . We think of  and  as truth assignments for the input and output signals, respectively, and we think of  as a set of sensed signals. Consider the set . Intuitively,  is the set of input assignments that agree with  on all the signals in . For a state , we define . 

Intuitively, when thinking of  as a game between the system and the environment, then at each step, the system chooses a set of sensed inputs  and an output . Then, the environment chooses a set of inputs , but in the next step the system can only see the inputs in  that are sensed in , and thus moves universally with every input that agrees with  on the sensed inputs in .

We proceed to determinize  to a DPW . We then obtain from  a parity game, as described above, with Player~1 (the system) controlling the set of sensed inputs and the output, and Player~2 (the environment) controlling the concrete inputs. Formally, the game 
 is defined as follows. The states are  and . The actions for Player~1 in every state are  and are  for Player~2 (we omit the state as the available actions are independent of the state). The transition function is defined as follows. For a state  and action  we have  as well as . 
For a state  and action  we have . 

Intuitively, the state  represents that  is in state , the system has chosen to sense the signals in , and the environment gave the concrete input . Then, the action  means that the system responded with output , and chose to sense  in the next step, taking the game to the state , where . Then, in state , the environment chooses a new concrete input .

We define the acceptance condition  as follows. For every  and , we have , and we arbitrarily set  (since  is visited only once, this has no effect).

Note that crucially, for every , the behavior of  from state  is identical to the behavior from . This follows from the universal transitions in . Thus, once Player~1 chooses , the inputs that are not sensed do not play a role. This captures the fact that every winning strategy for the system must only rely on the values  assigns to the sensed inputs .

Finally, the parity-MDP  is obtained from  by fixing Player~2 with a uniform-stochastic strategy and adding costs according to the number of sensed inputs at each state. Recall that the actions of Player~2 are . Thus, in state , the probability of Player~2 playing  is . Note that by our observation above, every , induce the same transitions. Thus, the probability of transition from state  to  is .

The cost function assigns cost  to states  and , for every  and .

We now proceed to analyze the correctness of the construction. Consider a (not necessarily finite) transducer  that realizes the specification . We identify with  a strategy  for  as follows. 
In state  we have . Then, the strategy  keeps track of the state of  as follows. When  is in state , and the state of the game is , let . Then, we have that . Observe that  is essentially implemented by the transducer . In particular, if  has finite state space, then  has finite memory.

We claim that . We start by showing that  is sure winning in  (equivalently, that it is a winning strategy for Player~1 in ). Consider an input sequence , let  and  be the states that  and  reach, respectively, when they interact on . let , then for every  such that  we have that . Thus, the behavior of  from  and from  is the same. It follows that  induces a realizing strategy for the UPW  (and hence a winning strategy for ), where the additional  component in the alphabet represents the sensing of the current state of . However, this is exactly the behavior prescribed by , so  is winning in . 

Next, observe that by the above, for every input sequence , the (prefix of the) play of  induced by Player~1 playing  and Player~2 playing  is  , and we have that , while for the run  of  on the first  letters of  we have that . By the definition of  and , we have . Moreover, the probabilities of  imply that every  such that  is played w.p. . Thus, by taking , we get . 

Since this is true for every realizing transducer , it follows that .

Conversely, consider a strategy  for . A-priori,  can behave differently in states  and  for . However, as we observed above, the construction of  (and thus of ) implies that  cannot decrease its cost by doing so, since the behavior of  is the same in both states. Thus, we can assume w.l.o.g that  only depends on the values  assigns to the sensed inputs . Now,  induces a (possibly infinite) transducer  in an obvious manner - whenever  outputs , the transducer outputs . Similar arguments as the converse direction show that , and thus , and we are done.

\end{document}
