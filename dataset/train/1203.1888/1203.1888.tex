\documentclass[letterpaper, 12pt]{article}


\pagestyle{plain}                                                      \setlength{\textwidth}{6.5in}     \setlength{\oddsidemargin}{0in}   \setlength{\evensidemargin}{0in}  \setlength{\textheight}{9in}    \setlength{\topmargin}{0in}       \setlength{\headheight}{0in}      \setlength{\headsep}{0in}         \setlength{\footskip}{.5in}       \setlength {\parskip}{8pt}                                             \bibliographystyle{abbrv}                                           
\newenvironment{proof}{\paragraph{\bf Proof:}}{\hspace*{\fill}}



\title{Matrix Representation of Iterative Approximate Byzantine Consensus in Directed Graphs\thanks{This research is supported
in part by
 National
Science Foundation award CNS 1059540 and Army Research Office grant W-911-NF-0710287. Any opinions, findings, and conclusions or recommendations expressed here are those of the authors and do not
necessarily reflect the views of the funding agencies or the U.S. government.}
}


\author{Nitin Vaidya\\ Department of Electrical and Computer Engineering\\ University of Illinois at Urbana-Champaign\\ nhv@illinois.edu} 


\thispagestyle{empty}
\pagestyle{empty}

\usepackage{graphicx}
\usepackage{amsmath}
\newcommand{\ARightarrow}{\stackrel{a}{\Rightarrow}}
\newcommand{\comment}[1]{}
\newcommand{\shortdividerline}{
\begin{center} \line(1,0){150} \end{center}
}
\newcommand{\dividerline}{\begin{center}\hrule\end{center}}



\usepackage{graphicx}
\usepackage{latexsym}

\newcommand{\nchoosek}[2]{{#1 \choose #2}}

\newcommand{\scriptf}{\mathcal{F}}
\newcommand{\scripte}{\mathcal{E}}
\newcommand{\scriptv}{\mathcal{V}}
\newcommand{\scriptt}{\mathcal{T}}
\newcommand{\scriptd}{\mathcal{D}}



\newcommand{\BlackBox}{\rule{2.6mm}{2.6mm}}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{condition}{Condition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newcommand{\fillblackbox}{\hspace*{\fill}}
\newcommand{\fillbox}{\hspace*{\fill}}
\newcommand{\fig}[1]{Figure~\ref{#1}}
\newcommand{\eqn}[1]{Equation~\ref{#1}}
\newcommand{\refsec}[1]{Section~\ref{#1}}
\newcommand{\num}[1]{(\romannumeral#1)}
\def\noflash#1{\setbox0=\hbox{#1}\hbox to 1\wd0{\hfill}}


\begin{document}


\date{~}

\maketitle

\centerline{March 8, 2012}

~

\begin{abstract}
This paper presents a proof of correctness of an iterative
approximate Byzantine
consensus (IABC) algorithm for directed graphs. The iterative algorithm allows
fault-free nodes to reach approximate conensus despite the presence of up
to  Byzantine faults. Necessary conditions on the underlying network
graph for the existence of a correct IABC algorithm were shown in our
recent work \cite{IBA_sync,us}. \cite{IBA_sync} also analyzed
a specific IABC algorithm and showed that it performs correctly in any
network graph that satisfies the necessary condition, proving that the
necessary condition is also sufficient. In this paper, we present an 
alternate proof of correctness of the IABC algorithm, using a familiar
technique based on transition matrices \cite{jadbabaie_consensus,Benezit,vaidyaII,Zhang}.\\

The key contribution of this paper is to exploit the
following observation: for a {\em given} evolution of the state vector
corresponding to the state of the fault-free nodes, 
many alternate state transition matrices may be chosen to model
that evolution correctly. For a given state evolution, we identify one approach
to suitably ``design'' the transition matrices so that the standard tools
for proving convergence can be applied to the Byzantine fault-tolerant
algorithm as well.
In particular, the transition matrix for each iteration
is designed such that each row of the matrix contains a large enough number
of elements that are bounded away from 0. \\


\end{abstract}







\newcommand{\deltaC}{\delta_{N_i^*[t]}}
\newcommand{\bfA}{{\bf A}}
\newcommand{\bfB}{{\bf B}}
\newcommand{\bfC}{{\bf C}}
\newcommand{\bfG}{{\bf G}}
\newcommand{\bfH}{{\bf H}}
\newcommand{\bfK}{{\bf K}}
\newcommand{\bfM}{{\bf M}}
\newcommand{\bfP}{{\bf P}}
\newcommand{\bfQ}{{\bf Q}}
\newcommand{\bfv}{{\bf v}}
\newcommand{\sH}{\mathcal{H}}
\newcommand{\T}[1]{\overline{#1}}


 
\section{Introduction}
\label{sec:intro}

Dolev et al. \cite{AA_Dolev_1986} introduced the notion of
{\em approximate Byzantine consensus} by relaxing the requirement
of {\em exact} consensus \cite{AA_nancy}.
The goal in approximate consensus is to allow the fault-free nodes to agree on values that are approximately equal to each other (and {\em not necessarily}
exactly identical). 
In presence of Byzantine faults, while {\em exact} consensus 
is impossible in {\em asynchronous} systems \cite{FLP_one_crash}, approximate
consensus is achievable \cite{AA_Dolev_1986}.
The notion of approximate consensus is of interest in {\em synchronous}
systems as well, since approximate consensus can be achieved using
simple distributed algorithms that do {\em not} require complete knowledge of
the network topology \cite{AA_convergence_markov}.



In this paper, we are interested in iterative algorithms
for achieving approximate Byzantine consensus in synchronous point-to-point
networks that are modeled by arbitrary {\em directed}\, graphs.
The {\em iterative
approximate Byzantine consensus} (IABC) algorithms of interest have
the following properties, which we will soon state more formally:
\begin{itemize}
\item {\em Initial state} of each node is equal to a real-valued
{\em input} provided to that node.
\item {\em Validity} condition: After each iteration of an IABC algorithm, the state of each fault-free node
must remain in the {\em convex hull} of the states of the fault-free nodes
at the end of the {\em previous} iteration.
\item {\em Convergence} condition:
For any , after a sufficiently large number of iterations,
the states of the fault-free nodes are guaranteed to be within 
of each other.
\end{itemize}
Certain IABC algorithms have been shown to satisfy the above properties
in {\em fully
connected} graphs \cite{AA_Dolev_1986,AA_nancy}, and in {\em arbitrary
directed} graphs satisfying a tight necessary condition 
\cite{IBA_sync,us}.
Please refer to \cite{IBA_sync,us} for a summary of the related work.

The main contribution of this paper is to develop an alternate proof of
correctness for a IABC algorithm, which was proved correct
in arbitrary graphs that satisfy a necessary condition developed
in our prior work \cite{IBA_sync}. 
The alternate proof is based on
transition matrices that capture the behavior of the IABC algorithm
executed by the fault-free nodes.  This work is inspired
by, and borrows some matrix analysis tools from, other work that also uses
transition matrices in 
related contexts \cite{jadbabaie_consensus,Benezit,vaidyaII,Zhang}.
This paper exploits the
following observation: for a {\em given} evolution of the state vector
corresponding to the state of the fault-free nodes, 
many alternate state transition matrices may potentially be chosen to emulate
that evolution correctly. For a given state evolution, we identify one approach
to suitably ``design'' the transition matrices so that the standard tools
can be applied to prove convergence of the Byzantine fault-tolerant
algorithm in {\em all networks} that satisfy a necessary condition
(proved in \cite{us}) on the network communication graph.
In particular, the transition matrix for each iteration
is designed such that each row of the matrix contains a large enough number
of elements that are bounded away from 0.

\section{Network and Failure Models}

\paragraph{Network Model:}
The system is assumed to be {\em synchronous}.
The communication network is modeled as a simple {\em directed} graph , where  is the set of  nodes, and  is the set of directed edges between the nodes in . 
 Node  can reliably transmit messages to node  if and only if
the directed edge  is in .
Each node can send messages to itself as well, however,
for convenience, we {\em exclude self-loops} from set .
That is,  for .
With a slight abuse of terminology, we will use the terms {\em edge}
and {\em link} interchangeably in our presentation.

For each node , let  be the set of nodes from which  has incoming
edges.
That is, .
Similarly, define  as the set of nodes to which node 
has outgoing edges. That is, .
Since we exclude self-loops from ,
 and . 
However, we note again that each node can indeed send messages to itself.
A necessary condition for correctness of an IABC algorithm for  is that
 \cite{IBA_sync}.

Node  is said to be an {\em incoming neighbor} of node ,
if . Similarly,  is said to be an {\em outgoing neighbor}
of node , if .


\paragraph{Failure Model:}
We consider the Byzantine failure model, with up to  nodes becoming faulty. A faulty node may {\em misbehave} arbitrarily. Possible misbehavior includes sending incorrect and mismatching (or inconsistent) messages to different neighbors. The faulty nodes may potentially collaborate with each other. Moreover, the faulty nodes are assumed to have a complete knowledge of the execution of
the algorithm, including the states of all the nodes,
contents of messages the other nodes send to each other,
the algorithm specification, and the network topology.

\section{Iterative Approximate Byzantine Consensus (IABC)}
\label{sec:iabc}

Each node  maintains state , with  denoting the state
of node  at the {\em end}\, of the -th iteration of the algorithm.
Initial state of node ,
, is equal to the initial {\em input}\, provided to node .
At the {\em start} of the -th iteration (), the state of
node  is .

Let  denote the set of faulty nodes.
Thus, the nodes
in  are non-faulty.\footnote{\normalsize For sets  and ,  contains elements that are in  but not in . That is, .} 
\begin{itemize}


\item .  is the largest state among the fault-free nodes at the end of the -th iteration.
Since the initial state of each node is equal to its input,
 is equal to the maximum value of the initial input at the fault-free nodes.

\item .  is the smallest state among the fault-free nodes at the end of the -th iteration.
 is equal to the minimum value of the initial input at the
fault-free nodes.
\end{itemize}
The following conditions must be satisfied by an IABC algorithm
in presence of up to  Byzantine faulty nodes:
\begin{itemize}
\item {\em Validity:} 

\item {\em Convergence:} .
Equivalently, , for
.
\end{itemize}


An iterative algorithm is said to be {\em correct} if it satisfies
the {\em validity} and {\em convergence} conditions.
We will prove the correctness of Algorithm 1 below
 in all graphs that satisfy the
necessary condition in Theorem 2 of \cite{us}. The algorithm
should be performed by each node  in the
-th iteration, . The faulty nodes may deviate from the
algorithm specification. If a fault-free node does not receive an
expected message from an incoming neighbor (in the {\em Receive step}
below), then that message is assumed to have some default value.


\vspace*{8pt}\hrule

{\bf Algorithm 1}
\vspace*{4pt}\hrule

~

Steps to be performed by node  in the -th iteration:
\begin{enumerate}

\item {\em Transmit step:} Transmit current state  on all outgoing edges.
\item {\em Receive step:} Receive values on all incoming edges. These values form
vector  of size .


\item {\em Update step:}
Sort the values in  in an increasing order, and eliminate
the smallest  values, and the largest  values (breaking ties
arbitrarily).
 Let  denote the identifiers of nodes from
whom the remaining  values were received, and let
 denote the value received from node .

For convenience, define .

Observe that
if  is fault-free, then .

Define

where
 
Recall that
 
because .
The ``weight'' of each term on the right-hand side of
(\ref{e_Z}) is , and these weights add to 1.

Observe that .


For future reference, let us define  as:

Note that .
Specifically,  is a positive constant
that is dependent only on  and the graph .

\end{enumerate}

~
\hrule

~

~

Similar algorithms have been proven to work correctly in
{\em fully connected} graphs \cite{AA_Dolev_1986,IBA_sync}
and {\em arbitrary directed} graphs
satisfying the necessary condition stated in
\cite{IBA_sync}.
In this paper, we provide an alternate proof of correctness
in such arbitrary graphs, using an alternate form of the
necessary condition \cite{us}.

\comment{
\section{Related Work}
\label{sec:related}

Some of the related work has already been discussed earlier in the paper.
In this section, we discuss other related work.

There have been previous attempts at achieving approximate
consensus iteratively in {\em partially} connected graphs. Kieckhafer and Azadmanesh
examined the necessary conditions in order to achieve ``local'' convergence in
synchronous \cite{AA_PCN_Local} and asynchronous \cite{AA_async_PCN} systems.
\cite{AA_PFCN} presents a specific class of networks in which convergence
condition can be satisfied using iterative algorithms.

Zhang and Sundaram \cite{Zhang}
consider a
{\em restricted} fault model in which the faulty nodes are restricted
to sending identical messages to their neighbors. In contrast, our
Byzantine fault model allows a faulty node to send different messages to different
neighbors.
In particular,
 under the {\em restricted} model,
Zhang and Sundaram \cite{Zhang} develop {\em sufficient}\,
conditions for iterative consensus algorithm assuming a ``local" fault
model (in their ``local'' model, a bounded number of each node's neighbors
may be faulty). LeBlanc and Koutsoukos \cite{Leblanc_HSCC_1} address a continuous
time version of the Byzantine consensus problem in {\em complete} graphs.
Under the above {\em restricted} fault model, as well as our fault model, LeBlanc and Koutsoukos \cite{Leblanc_HSCC_2}
have identified some sufficient conditions under which iterative consensus can be
achieved; however, these sufficient conditions are {\em not} tight.
For the {\em restricted} model, recently
LeBlanc et al. \cite{diss_Sundaram} have obtained tight necessary
and sufficient conditions; but these conditions are not adequate
for our Byzantine fault model.

Iterative approximate consensus algorithms that {\em do not} tolerate faulty behavior
have also been studied extensively (e.g., \cite{jadbabaie_consensus, AA_convergence_markov}).

}

\section{Matrix Preliminaries}

We use boldface upper case letters to denote matrices,
rows of matrices, and their elements. For instance,
 denotes a matrix,  denotes the -th row of
matrix , and  denotes the element at the
intersection of the -th row and the -th column
of matrix .

\begin{definition}
\label{d_stochastic}
A vector is said to be {\em stochastic} if all the elements
of the vector are {\em non-negative}, and the elements add up to 1.
A matrix is said to be row stochastic if each row of the matrix is a
stochastic vector.
\end{definition}

For a row stochastic matrix ,
 coefficients of ergodicity  and  are defined as
\cite{Wolfowitz}:

It  is easy to see that   and , and that the rows are all identical if and only if . Additionally,  if and only if .


The next result from \cite{Hajnal58} establishes a relation between the coefficient of ergodicity  of a product of row stochastic matrices, and the coefficients of ergodicity  of the individual matrices defining the product. 

\begin{claim}
\label{claim_delta}
For any  square row stochastic matrices , 

\end{claim}
Claim \ref{claim_delta} is proved in \cite{Hajnal58}. It implies that
if, for all ,  for some , then  will approach zero as  approaches . 


\begin{definition}
A row stochastic
 matrix  is said to be a {\em scrambling}\, matrix, if 
{\normalfont \cite{Hajnal58,Wolfowitz}}.
\end{definition}

In a scrambling matrix , since , for each pair of
rows  and , there exists a column  (which may depend on
 and ) such that
  and , and vice-versa \cite{Hajnal58,Wolfowitz}.
As a special case, if any one column of a row stochastic matrix 
contains only non-zero elements that are lower bounded by some
constant , then  must be scrambling, and . 

\comment{======================
\begin{definition}
\label{d_type}
Two matrices of identical size are said to be of the same ``type'' if
they contain non-zero elements in identical positions.
\end{definition}
Let us denote by  the {\em type} of matrix .
A partial order can be
defined on the matrix types. Specifically, for matrices  and ,
 provided that matrix  is non-zero in each position
where  is non-zero.

\begin{lemma}
\label{l_scambling_1}
For any two row stochastic matrices  of the same size,
if  and  is a scrambling matrix,
then  is a scrambling matrix.
\end{lemma}
\begin{proof}
Follows immediately from the definition of matrix {\em type}
and {\em scrambling} matrices.
\end{proof}

~

\begin{lemma}
\label{l_scrambling_2}
Consider a sequence  of square row stochastic matrices
with non-zero diagonals. For any subset  of ,

\end{lemma}
\begin{proof}
The proof follows from the definition of matrix {\em type}, and the fact
the row stochastic matrices above have non-zero diagonals. 
\end{proof}

~
=================================================}

\section{Matrix Representation of Algorithm 1}
\label{s_claim}

Recall that  is the set of faulty nodes.
Let .
Without loss of generality, suppose that nodes 1 through  are
fault-free, and if , nodes  through  are faulty.

Denote by  the column vector consisting of the initial states of
all the {\em fault-free} nodes.
Denote by , where , the column vector consistsing of
the states of all the {\em fault-free} nodes
at the end of the -th iteration, .
The -th element
of vector  is state . The size of the column
vector  is
.



~

\begin{claim}
\label{claim_1}
{
We can express the iterative update of the state
of a fault-free node  
performed in (\ref{e_Z}) using the matrix form in (\ref{e_matrix_i})
below,
where  satisfies the following four conditions.

}
In addition to , the row vector  
may depend on the state vector  as well as the
behavior of the faulty
nodes in . For simplicity, the notation  does not
explicitly represent this dependence. 
\begin{enumerate}
\item  is a {\em stochastic} row vector of size .
Thus,
, for , and


\item  equals  defined in Algorithm 1. 
Recall that .

\item  is non-zero
{\bf only if}   or .
\item At least  elements in  
are lower bounded by some constant , to be defined later
( is independent of ). 
Note that  is the set of fault-free
incoming neighbors of node .
\end{enumerate}

\end{claim}
\begin{proof}
The proof of this claim is presented in Section \ref{ss_claim_1} below.
The last condition above plays an important role in the proof, and the
main contribution of this paper is to ``design''  to make this
condition true.
\end{proof}

~



By ``stacking'' (\ref{e_matrix_i}) for different
, , we can
represent the state update for all the fault-free nodes together
using (\ref{e_matrix})
below, where  is a  matrix, with its -th row
being equal to  in (\ref{e_matrix_i}).

The four properties of  imply that  is a
row stochastic matrix with a non-zero diagonal.
Also, the -th row of  contains 
elements lower bounded by  ( will be defined later).
This property of  turns out to be important in proving
convergence of Algorithm 1.

 is said to be a {\em transition matrix}.

By repeated application of (\ref{e_matrix}), we obtain:









\subsection{Correctness of Claim \ref{claim_1}}
\label{ss_claim_1}

Figure \ref{f_sets} illustrates the various sets used here.
Some of the sets in this figure are not yet defined, and will be defined
later in the paper.
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{sets.eps}
\caption{Illustration of sets , , ,
,  and }
\label{f_sets}
\end{figure}



We prove the correctness of Claim \ref{claim_1} by constructing 
for  that satisfies the conditions in Claim \ref{claim_1}.
Recall that nodes 1 through  are fault-free, and the remaining
 nodes () are faulty.

Consider a fault-free node  performing the {\em Update step}
in Algorithm 1.
Recall that the largest  and the smallest  values are eliminated
from . Let us denote by  and , respectively, the set
of nodes\footnote{Although  and  may be different for each ,
for simplicity, we do not explicitly represent this dependence
on  in the notations  and .}  
from whom the largest  values and the smallest  values
were received by node  in iteration .
Thus,
, ,
and .


For any set of nodes  here, let  and  respectively
denote the number of faulty nodes, and the number of fault-free nodes, in set .
For instance,  and  denote, respectively, the number
of faulty and fault-free nodes in set .
Thus,

Let

That is, the number of faulty incoming neighbors of node  is denoted
as .
Therefore, ,
and




Then, it follows that



For fault-free node ,
we now define the elements of row .
We consider two cases separately: (i) ,
and (ii) . 

\subsubsection{}
\label{ss_1}

We know that  and .
Therefore,  implies that 
and . Thus, in this case, all the nodes in 
are fault-free.
\begin{itemize}
\item
For each 
, define .
Element  corresponds to the term  in (\ref{e_Z}).

Recall that , and that each node in 
in this case is fault-free.

\item 
For each  such that 
and , define .
\end{itemize}
Observe that with the above definition of elements of ,

In the above procedure, we have set  elements of 
equal to  (recall that ).

Now, because  and , we have
.
Also, in this case . Thus, it should
be easy to see that the conditions in Claim \ref{claim_1} are satisfied
by defining .


~

\subsubsection{}

Since ,
 implies that .
When , the necessary condition in \cite{IBA_sync} implies that
. Therefore, the set  is non-empty.
As per (\ref{e_Z}), each node  contributes 
to the new state  of node . We will define elements
of  to account for the contribution of each node .

Define subsets  and  such that
, , ,
and . That is, sets  and 
are subsets of  and , respectively, each of size ,
and containing only fault-free nodes.
Expressions (\ref{g_L}) and (\ref{g_S}) for  and 
imply that such subsets exist.

Let 

and 
Consider any node .
For each , ,

Therefore,
we can find weights  and 
 such that
 and

Clearly, at least one of the weights  and 
must be .
Now, observe that

The above equality is true independent of whether  is fault-free or faulty.
We will later use the above equality for the case when  is a faulty node.
When  is fault-free,  and we can similarly obtain the equality below.


We now use (\ref{e_Z}), (\ref{e_faulty}) and (\ref{e_faultfree}) to define elements
of  in the following four cases:
\begin{itemize}
\item {\bf Case 1: Node }\\ Define . This is obtained by observing in (\ref{e_Z}) that
the contribution of node  to the new state  is .
\item {\bf Case 2: Fault-free nodes in }\\
For each ,
define . This choice is motivated by (\ref{e_faultfree})
wherein the contribution of node  to  is .
In Case 2,  elements
of  are defined.
\item {\bf Case 3: Nodes in  and }\\ For , consider . In this
case, 

Similarly, for , consider . In this
case, 

These expressions are obtained by summing (\ref{e_faulty})
and (\ref{e_faultfree}), respectively, over the faulty and fault-free nodes
in , and then identifying the contribution
of each node in  and  to this sum.
Recall the earlier observation that at least one of 
and  must be  for each pair  where 
and .
Therefore, it follows that
at least  elements of  defined
in Case 3 must be .

\item {\bf Case 4: Nodes in }\\
These fault-free nodes have not yet been considered in Cases 1, 2 and 3.
For each node ,
we assign . 
\end{itemize}
Observe that above the definition of the elements of 
ensures that 

However, the contribution by the
faulty nodes in  in (\ref{e_Z}) is
now replaced by an
equivalent contribution by the nodes in  and .

Now let us verify that the four conditions in Claim \ref{claim_1} hold
for the above assignments to the elements of .
\begin{enumerate}
\item
Observe that all the elements of  are non-negative.
Case 1 specifies just .
The elements of  specified in Case 2 
add up to

Recall that for each , ,
 for .
Therefore, when added over all 
and , the elements of  specified in Case 3 
add up to

Therefore, when all the elements of  defined in Cases 1,
2 and 3 are added together, we get

because 
.
Now observe that the elements specified in Cases 1, 2 and 3 are clearly
.
In the expression for  in Case 3, observe that the two
summations on the right side together contain  terms,
and in these terms, observe that ,
 and . Therefore,
.
Similarly, we can show that  as well.

Thus, we have shown that  is a stochastic vector.

\item  as specified in Case 1.

\item Since  is defined to be non-zero only in Cases 1, 2 and 3,
which consider the nodes only in , it follows
that  is non-zero {\em only if} 
or .

\item
Cases 1 and 2 together set 
elements of  to be . We observed
earlier that Case 3 results in at least
 elements of  
being .
Also, observe that the elements of  specified
in Cases 1 and 2 are distinct from those specified in Case 3,
and that . Thus, overall, at
least

elements of  are set
.
Derivation of the above equation uses the facts that
 and
. Then
by defining  as below, condition 4 in Claim \ref{claim_1}
holds true.

\end{enumerate}

Therefore, Claim \ref{claim_1} is proved correct.

~


\subsection{Correspondence Between Sufficiency Condition and }

Let us define set  of subgraphs of  as follows.

Thus,  is the set of nodes in each graph in .

Let  denote . 
 depends on  and the underlying network, and it is finite.


\begin{claim}
\label{claim_suff}
Suppose that graph  satisfies the necessary condition in 
Theorem 2 in \cite{us}. Then it follows that in each ,
there exists at least one node that has directed paths to all the nodes in 
(consisting of the edges in ).
\end{claim}
\begin{proof}
The proof follows from Theorem 2 of \cite{us}.
\end{proof}

In this discussion, let us denote a graph by an italic upper case letter,
and the corresponding {\em connectivity matrix} using the same letter
in boldface upper case. Thus,
 will denote the connectivity matrix for graph ;
 is defined as follows:
(i) for ,
if there is a directed link from node  to node  in
graph  then ,
and (ii)  for .
Note that in our notation, the -th row of  (that is, )
corresponds to the incoming links at node , and the self-loop
at node .
The connectivity matrix  for any 
has a non-zero diagonal.


\begin{lemma}
\label{l_one_column}
For any ,  has at least one non-zero column.
\end{lemma}
\begin{proof}
By Claim \ref{claim_suff}, in graph  there exists at least one node, say
node , that has a directed path in  to all the remaining nodes in .
Since the length of the path from  to any other node in  can contain
at most  directed edges,
the -th column of matrix  will
be non-zero.\footnote{That is, all the elements of the column will be
non-zero (more precisely, positive, since the elements of matrix 
are non-negative).
Also, such a non-zero column will exist in  too.
We use the loose bound of  to simplify the presentation. }
\end{proof}

~


\begin{definition}
We will say that an element of a matrix is ``non-trivial'' if it is lower
bounded by .
\end{definition}

\begin{definition}
For matrices  and  of identical size, and
a scalar ,  provided
that  for all .
\end{definition}

\begin{lemma}
\label{l_H}
For any , there exists a graph  such that
.
\end{lemma}
\begin{proof}
Observe that the -th row of the transition matrix  corresponds to the state update
performed at fault-free node . Recall from Claim \ref{claim_1} that
the  is non-zero {\bf only if} link .
Also, by Claim \ref{claim_1},
 (i.e., the -th row of )
contains at least 
{\em non-trivial} elements corresponding
to {\bf fault-free} incoming neighbors of node  and itself (i.e., the
diagonal element).

Now observe that, for any subgraph , -th row of 
 contains exactly  non-zero elements,
including the diagonal element.

Considering the above two observations, and the definition of set ,
the lemma follows.
\end{proof}

~

\section{Correctness of Algorithm 1}


The proof below uses techniques also applied in prior work
(e.g., \cite{jadbabaie_consensus,Benezit,vaidyaII,Zhang}),
with some similarities to the arguments used in \cite{vaidyaII,Zhang}.

\begin{lemma}
\label{l_product_H}
In the product below of  matrices for consecutive
 iterations, at least one column is non-zero. 

\end{lemma}
\begin{proof}
Since the above product consists of  matrices
in ,
at least one of the  distinct connectivity matrices
in , say matrix , will appear in the above
product at least  times.

Now observe that: (i)
By Lemma \ref{l_one_column},  contains a non-zero
column, say the -th column is non-zero,
and (ii) all the  matrices in the product contain a non-zero diagonal.
These two observations together imply that the -th column in the above product 
is non-zero.
\end{proof}


Let us now define a sequence of matrices  such that
each of these matrices is a product of  of the
 matrices. Specifically,

Observe that



\begin{lemma}
\label{l_Q}
For ,  is a scrambling row stochastic matrix,
and  is bounded from above by a constant
smaller than 1.
\end{lemma}
\begin{proof}


 is a product of row stochastic matrices (), therefore,
 is row stochastic.

From Lemma \ref{l_H}, for each ,

Therefore, 

By using  in Lemma \ref{l_product_H},
we conclude that the matrix product on the left side
of the above inequality contains a non-zero column. Therefore,  contains
a non-zero column as well. Therefore,  is a scrambling matrix.

Observe that  is finite, therefore, 
is non-zero. Since the non-zero terms in  matrices are all 1,
the non-zero elements in 
must each be  1. Therefore, there exists a non-zero column in 
with all the elements in the column being .
Therefore .
\end{proof}

\begin{theorem}
\label{t}
Algorithm 1 satisfies the validity and the convergence conditions.
\end{theorem}
\begin{proof}

Since , and  is a row stochastic matrix, it
follows that
Algorithm 1 satisfies the validity condition.

By Claim \ref{claim_delta}, 

The above argument makes use of the facts that
 and .
Thus, the rows of  become identical in the limit.
This observation, and the fact that  together imply that
the state of the fault-free nodes satisfies the
convergence condition.


Now, the validity and convergence conditions
together imply that 
there exists a positive scalar  such that
 
where {\bf 1} denotes a column with all its elements being 1.

\end{proof}

\section{Extension of Above Results}
\label{s_extend}

In this paper, we
analyzed IABC Algorithm 1 designed for synchronous systems. Similar
analysis also applies for IABC Algorithm 2 presented
in \cite{us} for asynchronous systems. 

The analysis will also naturally extend to an IABC algorithm for the 
{\em partially synchronous algorithmic} model
presented in \cite{AA_convergence_markov}, which assumes a bounded
delay in propagation of state between neighbors, and a bounded delay between
consecutive state updates at each node in the network.
The generalization of Algorithm 1 to the
{\em partially synchronous algorithmic} model will allow a node ,
if performing state update in iteration , to form vector 
using the most recent known states of its incoming neighbors; these states
of the neighbors may correspond to any of the prior  iterations,
for some bounded .
A similar IABC algorithm can also be used 
in time-varying network topologies (i.e., networks wherein the set
of links available in iteration  varies with );
the above analysis will
then extend to time-varying topologies as well, with the
algorithm performing correctly so long as
the connectivity matrices for the graphs at different  
jointly satisfy some reasonable properties, as in \cite{jadbabaie_consensus,Benezit,vaidyaII}.



\section{Summary}

We presented a proof of validity and convergence of Algorithm 1 by
expressing the algorithm in the matrix form. The main contribution of
the paper is to express the algorithm in matrix form that allows us to
prove its convergence under certain necessary conditions on the underlying
communication graph. Thus, the proof implies that the necessary conditions
are also sufficient. 
The key to the proof is to ``design'' the transition matrix for each
iteration such that each row of the matrix contains a large enough number
of elements that are bounded away from 0.


\begin{thebibliography}{10}

\bibitem{AA_PFCN}
A.~Azadmanesh and H.~Bajwa.
\newblock Global convergence in partially fully connected networks (pfcn) with
  limited relays.
\newblock In {\em Industrial Electronics Society, 2001. IECON '01. The 27th
  Annual Conference of the IEEE}, volume~3, pages 2022 --2025 vol.3, 2001.

\bibitem{AA_async_PCN}
M.~H. Azadmanesh and R.~Kieckhafer.
\newblock Asynchronous approximate agreement in partially connected networks.
\newblock {\em International Journal of Parallel and Distributed Systems and
  Networks}, 5(1):26--34, 2002.
\newblock http://ahvaz.unomaha.edu/azad/pubs/ijpdsn.asyncpart.pdf

\bibitem{Benezit}
F. Benezit, V. Blondel, P. Thiran, J. Tsitsiklis, and M. Vetterli, “Weighted gossip: Distributed averaging using non-doubly stochastic
matrices,” in Proc. of IEEE International Symposium on Information Theory, June 2010, pp. 1753--1757.

\bibitem{AA_convergence_markov}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock {\em Parallel and Distributed Computation: Numerical Methods}.
\newblock Optimization and Neural Computation Series. Athena Scientific, 1997.

\bibitem{AA_Dolev_1986}
D.~Dolev, N.~A. Lynch, S.~S. Pinter, E.~W. Stark, and W.~E. Weihl.
\newblock Reaching approximate agreement in the presence of faults.
\newblock {\em J. ACM}, 33:499--516, May 1986.


\bibitem{AA_Fekete_aoptimal}
A.~D. Fekete.
\newblock Asymptotically optimal algorithms for approximate agreement.
\newblock In {\em Proceedings of the fifth annual ACM symposium on Principles
  of distributed computing}, PODC '86, pages 73--87, New York, NY, USA, 1986.
  ACM.

\bibitem{FLP_one_crash}
M.~J. Fischer, N.~A. Lynch, and M.~S. Paterson.
\newblock Impossibility of distributed consensus with one faulty process.
\newblock {\em J. ACM}, 32:374--382, April 1985.

\bibitem{Hajnal58}
J. Hajnal, ``Weak ergodicity in non-homogeneous Markov chains,” Proceedings of the Cambridge Philosophical Society, vol. 54, pp.
pp. 233--246, 1958.

\bibitem{jadbabaie_consensus}
A.~Jadbabaie, J.~Lin, and A.~Morse.
\newblock Coordination of groups of mobile autonomous agents using nearest
  neighbor rules.
\newblock {\em Automatic Control, IEEE Transactions on}, 48(6):988--1001,
  june 2003.

\bibitem{AA_PCN_Local}
R.~M. Kieckhafer and M.~H. Azadmanesh.
\newblock Low cost approximate agreement in partially connected networks.
\newblock {\em Journal of Computing and Information}, 3(1):53--85, 1993.


\bibitem{diss_Sundaram}
H.~LeBlanc, H.~Zhang, S.~Sundaram, and X.~Koutsoukos.
\newblock Consensus of multi-agent networks in the presence of adversaries
  using only local information.
\newblock {\em HiCoNs}, 2012.

\bibitem{Leblanc_HSCC_1}
H.~LeBlanc and X.~Koutsoukos.
\newblock Consensus in networked multi-agent systems with adversaries.
\newblock {\em 14th International conference on Hybrid Systems: Computation and
  Control (HSCC)}, 2011.

\bibitem{Leblanc_HSCC_2}
H.~LeBlanc and X.~Koutsoukos.
\newblock Low complexity resilient consensus in networked multi-agent systems
  with adversaries.
\newblock {\em 15th International conference on Hybrid Systems: Computation and
  Control (HSCC)}, 2012.

\bibitem{AA_nancy}
N.~A. Lynch.
\newblock {\em Distributed Algorithms}.
\newblock Morgan Kaufmann, 1996.


\bibitem{IBA_sync}
N.~H. Vaidya, L.~Tseng, and G.~Liang.
\newblock Iterative approximate Byzantine consensus in arbitrary directed
  graphs.
\newblock {\em CoRR}, abs/1201.4183v1 (January 2012), abs/1201.4183v2 (Februar 2012).
Available from {\tt http://arxiv.org}.

\bibitem{us}
N.~H. Vaidya, L.~Tseng, and G.~Liang.
\newblock Iterative approximate Byzantine consensus in arbitrary directed
  graphs -- Part II: Synchronous and asynchronous systems.
\newblock Technical report, University of Illinois at Urbana-Champaign,
  February 2012.
\newblock {\tt http://www.crhc.illinois.edu/wireless/papers/\\approx\_consensus\_II.pdf}

\bibitem{vaidyaII}
N. H. Vaidya, C. N. Hadjicostis, A. D. Dominguez-Garcia.
Distributed Algorithms for Consensus and Coordination in the Presence of Packet-Dropping Communication Links - Part II: Coefficients of Ergodicity Analysis Approach. September 2011. Available from {\tt http://arxiv.org/abs/1109.6392}.

\bibitem{Wolfowitz}
J. Wolfowitz, “Products of indecomposable, aperiodic, stochastic matrices,” Proceedings of the American Mathematical Society,
vol. 14, no. 5, pp. pp. 733--737, 1963.

\bibitem{Zhang}
H. Zhang and S. Sundaram. Robustness of Information Diffusion Algorithms to Locally Bounded Adversaries. In CoRR, volume abs/1110.3843, 2011. {\tt http://arxiv.org/abs/1110.3843}

\end{thebibliography}
\end{document}
