
\documentclass{article} \usepackage{iclr2023_conference,times}

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{xcolor}         \definecolor{darkblue}{rgb}{0.0,0.0,0.65}
\definecolor{darkred}{rgb}{0.68,0.05,0.0}
\definecolor{darkgreen}{rgb}{0.0,0.29,0.29}
\definecolor{darkpurple}{rgb}{0.47,0.09,0.29}
\hypersetup{
   colorlinks = true,
   citecolor  = darkblue,
   linkcolor  = darkred,
   filecolor  = darkblue,
   urlcolor   = darkblue,
 }

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      


\usepackage{wrapfig}
\usepackage{makecell} \usepackage{multirow} \usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath,bm,amsthm}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{listings}
\usepackage{verbatim}

\newcommand{\QQ}{\mathbb Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\CC}{\mathbb C}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\PP}{\mathbb P}
\newcommand{\EE}{\mathbb E}
\renewcommand{\SS}{\mathbb S}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO #1}}
\newcommand{\st}{\mrm{St}}
\newcommand{\gr}{\mrm{Gr}}
\renewcommand{\O}{O}
\newcommand{\std}[1]{$_{\pm #1}$}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand{\RN}[1]{\textup{\uppercase\expandafter{\romannumeral#1}}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\dout}{d_{\mrm{out}}}
\newcommand{\dfeat}{d_{\mrm{feat}}}

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{\newenvironment{rep#1}[1]{\def\rep@title{#2 \ref{##1}}\begin{rep@theorem}}{\end{rep@theorem}}}
\makeatother


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newreptheorem{proposition}{Proposition}


\newcommand{\sj}[1]{\textcolor{green}{[SJ: #1]}}
\newcommand{\dl}[1]{\textcolor{purple}{[DL: #1]}}
\newcommand{\rebut}[1]{\textcolor{black}{#1}} 



\title{Sign and Basis Invariant Networks for\\ Spectral Graph Representation Learning}


\author{Derek Lim$^{*}$, Joshua Robinson\thanks{Equal contribution.}\\
MIT CSAIL\\
\texttt{\{dereklim, joshrob\}@mit.edu}
\And Lingxiao Zhao  \\
Carnegie Mellon University\\
\And Tess Smidt\\
MIT EECS \& MIT RLE\\
\And
Suvrit Sra \\
MIT LIDS\\
\And
Haggai Maron \\
NVIDIA Research \\
\And Stefanie Jegelka \\
MIT CSAIL
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We introduce SignNet and BasisNet---new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if $v$ is an eigenvector then so is $-v$; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. 
When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases.
Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes.
Our code is available at 
\url{https://github.com/cptq/SignNet-BasisNet}.
\end{abstract}

\section{Introduction}


Numerous machine learning models process eigenvectors, which arise in various settings including principal component analysis, matrix factorizations, and operators associated to graphs or manifolds. An important example is the use of  Laplacian eigenvectors to encode information about the structure of a graph or manifold \citep{belkin2003laplacian,von2007tutorial,levy2006laplace}.
Positional encodings that involve Laplacian eigenvectors have recently been used to generalize Transformers to graphs \citep{kreuzer2021rethinking,dwivedi2020generalization}, and to improve the expressive power and empirical performance of graph neural networks (GNNs) \citep{dwivedi2022graph}. Furthermore, these eigenvectors are crucial for defining spectral operations on graphs that are foundational to graph signal processing and spectral GNNs~\citep{ortega2018graph, bruna2014spectral}.

However, there are nontrivial symmetries that should be accounted for when processing eigenvectors, as has been noted in many fields~\citep{eastment1982cross, rustamov2007laplace, bro2008resolving, ovsjanikov2008global}. For instance, if $v$ is an eigenvector, then so is $-v$, with the same eigenvalue. More generally, if an eigenvalue has higher multiplicity, then there are infinitely many unit-norm eigenvectors that can be chosen. Indeed, a full set of linearly independent eigenvectors is only defined up to a change of basis in each eigenspace. 
In the case of sign invariance, for any $k$ eigenvectors there are $2^k$ possible choices of sign. Accordingly, prior works on graph positional encodings randomly flip eigenvector signs during training in order to approximately learn sign invariance \citep{kreuzer2021rethinking, dwivedi2020benchmarking, kim2022pure}.  However, learning all $2^k$ invariances is challenging and limits the effectiveness of Laplacian eigenvectors for encoding positional information. Sign invariance is a special case of basis invariance when all eigenvalues are distinct, but general basis invariance is even more difficult to deal with.
In Appendix~\ref{appendix:higher_dim}, we show that higher dimensional eigenspaces are abundant in real datasets; for instance, 64\% of molecule graphs in the ZINC dataset have a higher dimensional eigenspace.


In this work, we address the sign and basis ambiguity problems by developing new neural networks---SignNet and BasisNet.
Under certain conditions, our networks are universal and can approximate any continuous function of eigenvectors with the proper invariances.
Moreover, our networks are theoretically powerful for graph representation learning---they can provably approximate and go beyond both spectral graph convolutions and powerful spectral invariants, which allows our networks to express graph properties like subgraph counts that message passing neural networks cannot. Laplacian eigenvectors with SignNet and BasisNet can provably approximate many previously proposed graph positional encodings, so our networks are general and remove the need for choosing one of the many positional encodings in the literature.
Experiments on molecular graph regression tasks, learning expressive graph representations, and texture reconstruction on triangle meshes illustrate the empirical benefits of our models' approximation power and invariances. 

\section{Sign and Basis Invariant Networks}
\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-10pt}
\centering
    \includegraphics[width=0.5\textwidth]{figs/LPE_symmetries.pdf}
\caption{Symmetries of eigenvectors of a symmetric matrix with permutation invariances (e.g. a graph Laplacian). A neural network applied to the eigenvector matrix (middle) should be invariant or equivariant to permutation of the rows (left product with a permutation matrix $P$) and invariant to the choice of eigenvectors in each eigenbasis (right product with a block diagonal orthogonal matrix $\mrm{Diag}(Q_1, Q_2, Q_3)$).}
\label{fig:symmetries}
\vspace{-15pt}
\end{wrapfigure}

For an $n \times n$ symmetric matrix, let $\lambda_1 \leq \ldots \leq \lambda_n$ be the eigenvalues and $v_1, \ldots, v_n$ the corresponding eigenvectors, which we may assume to form an orthonormal basis. For instance, we could consider the normalized graph Laplacian $L = I - D^{-1/2} A D^{-1/2}$, where $A \in \RR^{n \times n}$ is the adjacency matrix and $D$ is the diagonal degree matrix  of some underlying graph. For undirected graphs, $L$ is symmetric. Nonsymmetric matrices can be handled very similarly, as we show in Appendix~\ref{appendix:nonsymmetric}.

\textbf{Motivation.} Our goal is to parameterize a class of models $f(v_1, \ldots, v_k)$  taking $k$ eigenvectors as input in a manner that respects the eigenvector symmetries. 
This is because eigenvectors capture much information about data; Laplacian eigenvectors of a graph capture clusters, subgraph frequencies, connectivity, and many other useful properties~\citep{von2007tutorial, cvetkovic1997eigenspaces}.

A major motivation for processing eigenvector input is for graph positional encodings, which are additional features appended to each node in a graph that give information about the position of that node in the graph. These additional features are crucial for generalizing Transformers to graphs, and also have been found to improve performance of GNNs~\citep{dwivedi2020benchmarking, dwivedi2022graph}. Figure~\ref{fig:signnet_diagram} illustrates a standard pipeline and the use of our SignNet within it: the input adjacency, node features, and eigenvectors of a graph are used to compute a prediction about the graph. Laplacian eigenvectors are processed before being fed into this prediction model. Laplacian eigenvectors have been widely used as positional encodings, and many works have noted that sign and/or basis invariance should be addressed in this case \citep{dwivedi2020generalization, beaini2021directional, dwivedi2020benchmarking, kreuzer2021rethinking, mialon2021graphit, dwivedi2022graph, kim2022pure}.


\textbf{Sign invariance.} For any eigenvector $v_i$, the sign flipped $-v_i$ is also an eigenvector,
so a function $f: \RR^{n \times k} \to \RR^{\dout}$ (where $\dout$ is an arbitrary output dimension) should be \textit{sign invariant}:
\begin{equation}
    f(v_1, \ldots, v_k) = f(s_1 v_1, \ldots, s_k v_k)
\end{equation}
for all sign choices $s_i \in \{-1, 1\}$. That is, we want $f$ to be invariant to the product group $\{-1, 1\}^k$. This captures all eigenvector symmetries if the eigenvalues $\lambda_i$ are distinct and the eigenvectors are unit-norm.

\textbf{Basis invariance.} If the eigenvalues have higher multiplicity, then there are further symmetries. Let $V_1, \ldots, V_l$ be bases of eigenspaces---i.e., $V_i = \begin{bmatrix} v_{i_1}  & \ldots &  v_{i_{d_i}} \end{bmatrix} \in \RR^{n \times d_i}$ has orthonormal columns and spans the eigenspace associated with the shared eigenvalue $\mu_i = \lambda_{i_1} = \ldots = \lambda_{i_{d_i}}$. Any other orthonormal basis that spans the eigenspace is of the form $V_i Q$ for some orthogonal $Q \in O(d_i) \subseteq \RR^{d_i \times d_i}$ (see Appendix~\ref{appendix:eigenspace_background}). Thus, a function $f: \RR^{n \times \sum_{i=1}^l d_i} \to \RR^{\dout}$ that is invariant to changes of basis in each eigenspace satisfies 
\begin{equation}
    f(V_1, \ldots, V_l) = f(V_1 Q_1, \ldots, V_l Q_l), \qquad Q_i \in O(d_i).
\end{equation}
In other words, $f$ is invariant to the product group $O(d_1) \times \ldots \times O(d_l)$. The number of eigenspaces $l$ and the dimensions $d_i$ may vary between matrices; we account for this in Section~\ref{sec: multiple spaces}. As~$O(1) = \{-1, 1\}$, sign invariance is a special case of basis invariance when all eigenvalues are distinct.

\textbf{Permutation equivariance.} For GNN models that output node features or node predictions, one typically further desires $f$ to be invariant or equivariant to permutations of nodes, i.e., along the rows of each vector. 
Thus, for $f: \RR^{n \times d} \to \RR^{n \times \dout}$, we typically require $f(PV_1, \ldots, PV_l) = Pf(V_1, \ldots, V_l)$ for any permutation matrix $P \in \RR^{n \times n}$. Figure \ref{fig:symmetries} illustrates all of the symmetries.




\begin{figure}[ht]
    \centering
    \includegraphics[width=.98\columnwidth]{figs/SignNet_Diagram_v2.pdf}
    \caption{Pipeline for using node positional encodings. After processing by our SignNet, the learned positional encodings from the Laplacian eigenvectors are added as additional node features of an input graph ($[X, \mrm{SignNet}(V)]$ denotes concatenation).
    These positional encodings along with the graph adjacency and original node features are passed to a prediction model (e.g. a GNN).
    Not shown here, SignNet can also take in eigenvalues, node features and adjacency information if desired.}
    \label{fig:signnet_diagram}
\end{figure}

\subsection{Warmup: Neural Networks on One Eigenspace}\label{sec: warmup}

Before considering the general setting, we design neural networks that take a single eigenvector or eigenspace as input and are sign or basis invariant. These single subspace architectures will become building blocks for the general architectures. For one subspace, a sign invariant function is merely an even function, and is easily parameterized.
\begin{proposition}\label{prop:one_sign_invariant}
    A continuous function $h: \RR^n \to \RR^{\dout}$ is sign invariant if and only if
    \begin{equation}\label{eq:one_sign_ansatz}
     h(v) = \phi(v) + \phi(-v)   
    \end{equation}
    for some continuous $\phi: \RR^n \to \RR^{\dout}$. A continuous $h: \RR^n \to \RR^n$ is sign invariant and permutation equivariant if and only if \eqref{eq:one_sign_ansatz} holds for a continuous permutation equivariant $\phi: \RR^n \to \RR^n$.
\end{proposition}

In practice, we parameterize $\phi$ by a neural network. Any architecture choice will ensure sign invariance, while permutation equivariance can be achieved using elementwise MLPs, DeepSets~\citep{zaheer2017deep}, Transformers~\citep{vaswani2017attention}, or  most GNNs.

Next, we address basis invariance for a single $d$-dimensional subspace, i.e., we aim to parameterize maps $h: \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^n$ that are (a) invariant to right multiplication by $Q \in O(d)$, and (b) equivariant to permutations along the row axis. For (a), we use the mapping $V \mapsto V V^\top$ from $V$ to the orthogonal projector of its column space, which is $O(d)$ invariant.
Mapping $V \mapsto VV^\top$ does not lose information if we treat $V$ as equivalent to $VQ$ for any $Q \in O(d)$. This is justified by the classical first fundamental theorem of $O(d)$ \citep{kraft1996classical}, which has recently been applied in machine learning by \cite{villar2021scalars}.

Regarding (b), permuting the rows of $V$ permutes rows and columns of $VV^\top \in \RR^{n \times n}$. Hence, we desire the function  $\phi: \RR^{n \times n} \to \RR^n$ on $VV^\top$ to be equivariant to simultaneous row and column permutations: $\phi(PVV^\top P^\top) = P\phi(VV^\top)$.
To parameterize such a mapping from matrices to vectors, we use an invariant graph network ($\mrm{IGN}$)~\citep{maron2018invariant}---a neural network mapping to and from tensors of arbitrary order $\RR^{n^{d_1}} \to \RR^{n^{d_2}}$ that has the desired permutation equivariance.  We thus parameterize a family with the requisite invariance and equivariance as follows:
\begin{equation}
h(V) = \mrm{IGN}(V V^\top).
\end{equation}
Proposition~\ref{prop:ign_universal} states that this architecture universally approximates $O(d)$ invariant and permutation equivariant functions. The full approximation power requires high order tensors to be used for the $\mrm{IGN}$; in practice, we restrict the tensor dimensions for efficiency, as discussed in the next section.

\begin{proposition}\label{prop:ign_universal}
    Any continuous, $O(d)$ invariant $h: \RR^{n \times d} \to \RR^{\dout}$ is of the form $h(V) = \phi(VV^\top)$ for a continuous $\phi$. For a compact $\mc Z \subseteq \RR^{n \times d}$, maps of the form $V \mapsto \mrm{IGN}(VV^\top)$ universally approximate continuous  $h: \mc Z \subseteq \RR^{n \times d} \to \RR^n$ that are $O(d)$ invariant and permutation equivariant.
\end{proposition}


\subsection{Neural Networks on Multiple Eigenspaces}\label{sec: multiple spaces}

To develop a method for processing multiple eigenvectors (or eigenspaces), we first prove a general decomposition theorem (see Appendix~\ref{sec:universality} for more details). Our result reduces invariance for a large product group $G_1 \times \ldots \times G_k$ to the much simpler invariances for the smaller constituent groups $G_i$.

\begin{theorem}[informal]
Let a product of groups $G = G_1 \times \ldots \times G_k$ act on $\mc X_1 \times \ldots \times \mc X_k$. Under mild conditions, any continuous $G$-invariant function $f$ can be written $f(x_1, \ldots, x_k) = \rho(\phi_1(x_1), \ldots, \phi_k(x_k))$, where $\phi_i$ is $G_i$ invariant, and $\phi_i$ and $\rho$ are continuous If $\mc X_i = \mc X_j$ and $G_i = G_j$, then we can take $\phi_i = \phi_j$.
\end{theorem}
For eigenvector data, the $i$th eigenvector (or eigenspace) is in $\mc X_i$, and its symmetries are described by $G_i$. Thus, we can reduce the multiple-eigenspace case to the single-eigenspace case, and leverage the models we developed in the previous section.






\textbf{SignNet.} We parameterize our sign invariant network $f: \RR^{n \times k} \to \RR^{\dout}$ on eigenvectors $v_1, \ldots, v_k$~as:
\begin{equation}
    f(v_1, \ldots, v_k) = \rho\left([\phi(v_i) + \phi(-v_i)]_{i=1}^k \right),
\end{equation}
where $\phi$ and $\rho$ are unrestricted neural networks, and $[\cdot]_i$ denotes concatenation of vectors. The form $\phi(v_i) + \phi(-v_i)$ induces sign invariance for each eigenvector. Since we do not yet impose permutation equivariance here, we term this model \emph{Unconstrained-SignNet}.


To obtain a sign invariant \emph{and} permutation equivariant $f$ that outputs vectors in $\RR^{n \times \dout}$, we restrict $\phi$ and $\rho$ to be permutation equivariant networks from vectors to vectors, such as elementwise MLPs, DeepSets~\citep{zaheer2017deep}, Transformers~\citep{vaswani2017attention}, or most standard GNNs. We name this permutation equivariant version \textit{SignNet}. If desired, we can additionally use eigenvalues $\lambda_i$ and node features $X \in \RR^{n \times \dfeat}$ by adding them as arguments to $\phi$:
\begin{equation}
    f(v_1, \ldots, v_k, \lambda_1, \ldots, \lambda_k, X) = \rho\left([\phi(v_i, \lambda_i, X) + \phi(-v_i, \lambda_i, X)]_{i=1}^k \right).
\end{equation}

\textbf{BasisNet.} For basis invariance, let $V_i \in \RR^{n \times d_i}$ be an orthonormal basis of a $d_i$ dimensional eigenspace. Then we parameterize our \textit{Unconstrained-BasisNet}  $f$ by
\begin{equation}
    f(V_1, \ldots, V_{l}) = \rho\left([\phi_{d_i}(V_i V_i^\top)]_{i=1}^l \right),
\end{equation}
where each $\phi_{d_i}$ is shared amongst all subspaces of the same dimension $d_i$, and $l$ is the number of eigenspaces (i.e., number of distinct eigenvalues, which can differ from the number of eigenvectors $k$). As $l$ differs between graphs, we may use zero-padding or a sequence model like a Transformer to parameterize $\rho$. Again, $\phi_{d_i}$ and $\rho$ are generally unrestricted neural networks. To obtain permutation equivariance, we make $\rho$ permutation equivariant and let $\phi_{d_i} = \mrm{IGN}_{d_i} : \RR^{n^2} \to \RR^n$ be IGNs from matrices to vectors. For efficiency, we will only use matrices and vectors in the IGNs (that is, no tensors in $\RR^{n^p}$ for $p > 2$), i.e., we use 2-IGN~\citep{maron2018invariant}. Our resulting \textit{BasisNet} is
\begin{equation}
    f(V_1, \ldots, V_{l}) = \rho\left([\mrm{IGN}_{d_i}(V_i V_i^\top)]_{i=1}^l \right).
\end{equation}
\textbf{Expressive-BasisNet.} While we restrict SignNet to only use vectors and BasisNet to only use vectors and matrices,  higher order tensors are generally required for universally approximating permutation equivariant or invariant functions~\citep{keriven2019universal,maron2019universality,maehara2019simple}. Thus, we will consider a theoretically powerful but computationally impractical variant of our model, in which we replace $\rho$ and $\mrm{IGN}_{d_i}$ in BasisNet with IGNs of arbitrary tensor order. We call this variant \textit{Expressive-BasisNet}. Universal approximation requires $\mc O(n^n)$ sized intermediate tensors~\citep{ravanbakhsh2020universal}. 
We study Expressive-BasisNet due to its theoretical interest, and to juxtapose with the computational efficiency and strong expressive power of SignNet and BasisNet.


In the multiple subspace case, we can prove universality for some instances of our models through our decomposition theorem---see Section~\ref{sec:universality} for details. 
For a summary of properties and more details about our models, see Appendix~\ref{appendix:more_signnet_details}.





\section{Theoretical Power for Graph Representation Learning}

Next, we establish that our SignNet and BasisNet can go beyond useful basis invariant and permutation equivariant functions on Laplacian eigenvectors for graph representation learning, including: spectral graph convolutions, spectral invariants, and existing graph positional encodings.  Expressive-BasisNet can of course compute these functions,  but this section shows that the practical invariant architectures SignNet and BasisNet can compute them as well.

\subsection{SignNet and BasisNet strictly Generalize Spectral Graph Convolution}\label{sec:spectral_conv}

For node features $X \in \RR^{n \times \dfeat}$ and an eigendecomposition $V\Lambda V^\top$, a \emph{spectral graph convolution} takes the form $f(V, \Lambda, X) = \sum_{i=1}^n \theta_i v_i v_i^\top X = \rebut{V\mrm{Diag}(\theta)V^\top X}$, for some parameters $\theta_i$, that may optionally be continuous functions $h(\lambda_i) = \theta_i$ of the eigenvalues~\citep{bruna2014spectral,defferrard2016convolutional}. This family includes important functions like heat kernels and generalized PageRanks on graphs~\citep{li2019optimizing}. \rebut{A spectral GNN is defined as multiple layers of spectral graph convolutions and node-wise linear maps, e.g. $V\mrm{Diag}(\theta_2)V^\top \sigma\left(V\mrm{Diag}(\theta_1)V^\top XW_1 \right) W_2$ is a two layer spectral GNN.}
It can be seen (in Appendix~\ref{appendix:spectral_conv}) that spectral graph convolutions are permutation equivariant and sign invariant, and if $\theta_i = h(\lambda_i)$ (i.e. the transformation applied to the diagonal elements is parametric) they are additionally invariant to a change of bases in each eigenspace. 

Our SignNet and BasisNet can be viewed as generalizations of spectral graph convolutions, as our networks universally approximate all spectral graph convolutions of the above form. For instance, SignNet with $\rho(a_1, \ldots, a_k) = \sum_{i=1}^k a_k$ and $\phi(v_i, \lambda_i, X) = \frac{1}{2}\theta_i v_i v_i^\top X$ directly yields the spectral graph convolution. This is captured in  Theorem~\ref{prop:spectral_conv}, which we prove in Appendix~\ref{appendix:spectral_conv}. In fact, we may expect SignNet to learn spectral graph convolutions well, according to the principle of algorithmic alignment~\citep{xu2019can} (see Appendix~\ref{appendix:spectral_conv}); this is supported by numerical experiments in Appendix~\ref{sec:spectral_conv_exp}, in which our networks outperform baselines in learning spectral graph convolutions.

\begin{theorem}\label{prop:spectral_conv}
    SignNet universally approximates all spectral graph convolutions. BasisNet universally approximates all parametric spectral graph convolutions.
\end{theorem}

In fact, SignNet and BasisNet are strictly stronger than spectral graph convolutions; there are functions computable by SignNet and BasisNet that cannot be approximated by spectral graph convolutions or spectral GNNs. This is captured in Proposition~\ref{prop:signnet_strictly_greater_conv}: our networks can distinguish bipartite graphs from non-bipartite graphs, but spectral GNNs cannot for certain choices of graphs and node signals.

\begin{proposition}\label{prop:signnet_strictly_greater_conv}
\rebut{There exist infinitely many pairs of non-isomorphic graphs that SignNet and BasisNet can distinguish, but spectral graph convolutions or spectral GNNs cannot distinguish.}
\end{proposition}


\subsection{BasisNet can Compute Spectral Invariants}

Many works measure the expressive power of graph neural networks by comparing their power for testing graph isomorphism~\citep{xu2018powerful, sato2020survey}, or by comparing their ability to compute certain functions on graphs like subgraph counts~\citep{chen2020can, tahmasebi2020counting}. These works often compare GNNs to combinatorial invariants on graphs, especially the $k$-Weisfeiler-Leman ($k$-WL) tests of graph isomorphism~\citep{morris2021weisfeiler}.

While we may also compare with these combinatorial invariants, as other GNN works that use spectral information have done \citep{beaini2021directional}, we argue that it is more natural to analyze our networks in terms of \textit{spectral invariants}, which are computed from the eigenvalues and eigenvectors of graphs. There is a rich literature of spectral invariants from the fields of spectral graph theory and complexity theory~\citep{cvetkovic1997eigenspaces}. For a spectral invariant to be well-defined, it must be invariant to permutations and changes of basis in each eigenspace, a  characteristic shared by our networks.

The simplest spectral invariant is the multiset of eigenvalues, which we give as input to our networks. Another widely studied, powerful spectral invariant is the collection of graph angles, which are defined as the values $\alpha_{ij} = \norm{V_i V_i^\top e_j}_2$, where $V_i \in \RR^{n \times d_i}$ is an orthonormal basis for the $i$th adjacency matrix eigenspace, and $e_j$ is the $j$th standard basis vector, which is zero besides a one in the $j$th component. These are easily computed by our networks (Appendix~\ref{appendix:spectral_invariants}), so our networks inherit the strength of these invariants. We capture these results in the following theorem, which also lists a few properties that graph angles determine~\citep{cvetkovic1991some}. 

\begin{theorem}\label{prop:graph_angles}
    BasisNet universally approximates the graph angles $\alpha_{ij}$. The eigenvalues and graph angles (and thus BasisNet) can determine the number of length 3, 4, or 5 cycles, whether a graph is connected, and the number of length $k$ closed walks from any vertex to itself.
\end{theorem}

\textbf{Relation to WL and message passing.} In contrast to this result, message passing GNNs are not able to express any of these properties~(see \citep{arvind2020weisfeiler, garg2020} and Appendix~\ref{appendix:spectral_invariants}). Although spectral invariants are strong, \cite{furer2010power} shows that the eigenvalues and graph angles---as well as some strictly stronger spectral invariants---are not stronger than the 3-WL test (or, equivalently, the 2-Folklore-WL test). 
Using our networks for node positional encodings in  message passing GNNs allows us to go beyond graph angles, as message passing can distinguish all trees, but there exist non-isomorphic trees with the same eigenvalues and graph angles~\citep{furer2010power, cvetkovic1988constructing}.

\subsection{SignNet and BasisNet Generalize Existing Graph Positional Encodings}\label{sec:existing_pe}

Many graph positional encodings have been proposed, without any clear criteria on which to choose for a particular task. We prove (in Appendix~\ref{appdx: existing PE}) that our efficient SignNet and BasisNet can approximate many previously used graph positional encodings, as we unify these positional encodings by expressing them as either a spectral graph convolution matrix or the diagonal of a spectral graph convolution matrix.
\begin{proposition}\label{prop:approximate_positional}
    SignNet and BasisNet can approximate node positional encodings based on heat kernels~\citep{feldman2022weisfeiler} and random walks~\citep{dwivedi2022graph}. BasisNet can approximate diffusion and $p$-step random walk relative positional encodings~\citep{mialon2021graphit}, and generalized PageRank and landing probability distance encodings~\citep{li2020distance}.
\end{proposition}
We note that diagonals of spectral convolutions are used as feature descriptors in the shape analysis literature, such as for the heat kernel signature~\citep{sun2009concise} and wave kernel signature~\citep{aubry2011wave}. In the language of recent works in graph machine learning, these are node positional encodings computed from a discrete Laplacian of a triangle mesh. This connection appears to be unnoticed in recent works on graph positional encodings.

\section{Experiments}\label{sec:experiments}

We demonstrate the strength of our networks in various experiments. Appendix~\ref{appendix:more_signnet_details} shows simple pseudo-code and Figure~\ref{fig:signnet_diagram} is a diagram detailing the use of SignNet as a node positional encoding.

\subsection{Graph Regression}\label{sec:graph_regression}

\begin{table}[ht]
    \centering
    \caption{Results on the ZINC dataset with a $500$k parameter budget. All models use edge features. Numbers are the mean and standard deviation over 4 runs, each with different seeds.}
    \label{tab:zinc}
    {\small
    \begin{tabular}{lrrrrrr}
        \toprule
        Base model
        & \multicolumn{1}{c}{Positional encoding}  & \multicolumn{1}{c}{$k$} & \multicolumn{1}{c}{\#param} 
        & \multicolumn{1}{c}{Test MAE ($\downarrow$)}\\
        \midrule
   \multirow{7}{*}{GatedGCN} & No PE  & N/A & $492$k & $0.252_{\pm 0.007}$ \\
         & LapPE (flip)  & $8$ & $492$k & $0.198_{\pm 0.011}$ \\
         & LapPE (abs.)  & 8 & $492$k & $0.204_{\pm 0.009}$ \\
         & LapPE (can.)  & 8 & $505$k & $0.298_{\pm 0.019}$ \\
         & SignNet ($\phi(v)$ only) & $8$ & $495$k &
 $0.148_{\pm 0.007}$ \\
         & SignNet & $8$ & $495$k &
 $0.121_{\pm 0.005}$ \\
    & SignNet & All & $491$k &
     $\mathbf{0.100_{\pm 0.007}}$ \\
        \midrule
        \multirow{4}{*}{Sparse  Transformer} & No PE  & N/A & $473$k & $0.283_{\pm 0.030}$ \\
         & LapPE (flip)  & $16$ & $487$k & $0.223_{\pm 0.007}$ \\
         & SignNet  & $16$ & $479$k & $0.115_{\pm 0.008}$ \\
         & SignNet  & All & $486$k & $\mathbf{0.102_{\pm 0.005}}$ \\
        \midrule
        \multirow{4}{*}{GINE} & No PE  & N/A & $470$k & $0.170_{\pm 0.002}$ \\
         & LapPE (flip)  & $16$ & $470$k & $0.178_{\pm 0.004}$ \\
         & SignNet  & $16$ & $470$k & $0.147_{\pm 0.005}$ \\
         & SignNet & All & $417$k & $\mathbf{0.102_{\pm 0.002}}$ \\
         \midrule
         \multirow{4}{*}{PNA} & No PE  & N/A & $474$k  & $0.133_{\pm 0.011}$ \\
         & LapPE (flip)  & $8$ & $474$k & $0.132_{\pm 0.010}$ \\
         & SignNet  & $8$ & $476$k  & $0.105_{\pm 0.007}$ \\
         & SignNet  & All & $487$k  & $\mathbf{0.084_{\pm 0.006}}$ \\
        
        \bottomrule
    \end{tabular}
}
    \vspace{-5pt}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Comparison with SOTA methods on graph-level regression tasks.
    Numbers are test MAE, so lower is better. Best models within a standard deviation are bolded.}
    \label{tab:sota_graph}
    {\small
    \begin{tabular}{llll}
    \toprule
         & ZINC (10K) $\downarrow$ & ZINC-full $\downarrow$ & Alchemy (10k) $\downarrow$ \\
         \midrule
         GIN~\citep{xu2018powerful} & .170\std{.002} & .088\std{.002} & .180\std{.006} \\
         $\delta$-2-GNN~\citep{morris2020weisfeiler} & .374\std{.022} & .042\std{.003} & .118\std{.001}\\
         $\delta$-2-LGNN~\citep{morris2020weisfeiler} & .306\std{.044} & .045\std{.006} & .122\std{.003}\\
         SpeqNet~\citep{morris2022speqnets} & --- & --- & \textbf{.115\std{.001}} \\
         GNN-IR~\citep{dupty2022graph} & .137\std{.010} & --- & .119\std{.002} \\
         PF-GNN~\citep{dupty2021pf}  & .122\std{.01} & --- & \textbf{.111\std{.01}}\\
         Recon-GNN~\citep{cotta2021reconstruction} & .170\std{.006} & --- & .125\std{.001} \\
         \midrule
         SignNet (ours) & \textbf{.084\std{.006}} & \textbf{.024\std{.003}} & \textbf{.113\std{.002}} \\
         \bottomrule
    \end{tabular}
    }
\end{table}

We study the effectiveness of SignNet for learning positional encodings (PEs) from the  eigenvectors of the graph Laplacian on the ZINC dataset of molecule graphs~\citep{irwin2012zinc} (using the subset of 12,000 graphs from \citet{dwivedi2020benchmarking}). We primarily consider three settings: 1) No positional encoding, 2) Laplacian PE (LapPE)---the $k$ eigenvectors of the graph Laplacian with smallest eigenvalues are concatenated with existing node features, 3) SignNet positional features---passing the eigenvectors through a SignNet  and concatenating the output with node features. We parameterize SignNet by taking $\phi$ to be a $\text{GIN}$ \citep{xu2018powerful} and $\rho$ to be an $\text{MLP}$. We sum over $\phi$ outputs before the MLP when handling variable numbers of eigenvectors, so the SignNet is of the form $\mrm{MLP}\left(\sum_{i=1}^l \phi(v_i) + \phi(-v_i)\right)$ (see Appendix~\ref{appendix:graph_regression} for further details).
We consider four different base models that process the graph data and positional encodings: GatedGCN \citep{bresson2017residual}, a Transformer with sparse attention only over neighbours \citep{kreuzer2021rethinking}, PNA \citep{corso2020principal}, and GIN \citep{xu2018powerful} with edge features (i.e. GINE) \citep{hu2020strategies}. The total number of parameters of the SignNet and the base model is kept within a 500k budget.

Table \ref{tab:zinc} shows the results. For all 4 base models, the PE learned with SignNet yields the best test MAE (mean absolute error)---lower MAE is better. This includes the cases of PNA and GINE, for which Laplacian PE with simple random sign flipping was unable to improve performance over using no PE at all. Our best performing model is a PNA model combined with SignNet, which achieves $0.084$ test MAE. Besides SignNet, we consider two non-learned approaches to resolving eigenvector sign ambiguity---canonicalization and taking element-wise absolute values (see Appendix~\ref{appendix:graph_regression} for details). Results with GatedGCN show that these alternatives are not more effective than random sign flipping for learning positional encodings. We also consider an ablation of our SignNet architecture where we remove the sign invariance, using simply $\mrm{MLP}([\phi(v_i)]_{i=1}^k)$. Although the resulting architecture is no longer sign invariant, $\phi$ still processes eigenvectors independently, meaning that only two invariances ($\pm 1$) need be learned, significantly fewer than the $2^k$ total sign flip configurations. Accordingly, this non-sign invariant learned positional encoding achieves a test MAE of $0.148$, improving over the Laplacian PE ($0.198$) but falling short of the fully sign invariant SignNet ($0.121$). In all cases, using all available eigenvectors in SignNet significantly improves performance over using a fixed number of eigenvectors; this is notable as generally people truncate to a fixed number of eigenvectors in other works.
In Appendix~\ref{appendix:no_edge_features}, we also show that SignNet improves performance when no edge features are included in the data.

\textbf{Efficiency.} These significant performance improvements from SignNet come with only a slightly higher computational cost. For example, GatedGCN with no PE takes about 8.2 seconds per training iteration on ZINC, while GatedGCN with 8 eigenvectors and SignNet takes about 10.6 seconds; this is only a 29\% increase in time, for a reduction of test MAE by over 50\%. Also, eigenvector computation time is neglible, as we need only precompute and save the eigenvectors once, and it only takes 15 seconds to do this for the 12,000 graphs of ZINC.

\textbf{Comparison with SOTA.} In Table~\ref{tab:sota_graph}, we compare SignNet with other domain-agnostic state-of-the-art methods on graph-level molecular regression tasks on ZINC (10,000 training graphs), ZINC-full (about 250,000 graphs), and Alchemy~\citep{chen2019alchemy} (10,000 training graphs).
SignNet outperforms all methods on ZINC and ZINC-full.
Our mean score is the second best on Alchemy, and is within a standard deviation of the best.
We perform much better on ZINC (.084) than other state-of-the-art positional encoding methods, like GNN-LSPE (.090)~\citep{dwivedi2022graph}, SAN (.139)~\citep{kreuzer2021rethinking}, and Graphormer (.122)~\citep{ying2021transformers}.


\subsection{Counting Substructures and Regressing Graph Properties}\label{sec:count_substruct}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\columnwidth]{figs/substructures.pdf} \hspace{10pt}
     \includegraphics[width=0.45\columnwidth]{figs/properties.pdf}
    \caption{Counting substructures and regressing graph properties (lower is better). With Laplacian PEs, SignNet improves performance, while sign flip data augmentation (LapPE) is less consistent. 
    Mean and standard deviations are reported on 3 runs. All runs use the same 4-layer GIN base model. 
    }
    \label{fig:counting_sub}
    \vspace{-5pt}
\end{figure}



Substructure counts (e.g. of cycles) and global graph properties (e.g. connectedness, diameter, radius) are important graph features that are known to be informative for problems in biology, chemistry, and social networks \citep{chen2020can,holland1977method}. Following the setting of \citet{zhao2022from}, we show that SignNet with Laplacian positional encodings boosts the ability of simple GNNs to count substructures and regress graph properties. We take a 4-layer GIN as the base model for all settings, and for SignNet we use GIN as $\phi$ and a Transformer as $\rho$ to handle variable numbers of eigenvectors (see Appendix~\ref{sec:appdx count} for details). As shown in Figure \ref{fig:counting_sub}, Laplacian PEs with sign-flip data augmentation improve performance for counting substructures but not for regressing graph properties, while Laplacian PEs processed by SignNet significantly boost performance on all tasks. 

\subsection{Neural Fields on Manifolds}


\begin{table}
    \centering
    \caption{Test results for texture reconstruction experiment on cat and human models, following the experimental setting of \citep{koestler2022intrinsic}. We use 1023 eigenvectors of the cotangent Laplacian.}
    \label{tab:intrinsic_nf}
    {\small
    \begin{tabular}{lcccccccccc}
    \toprule
               & & \multicolumn{3}{c}{Cat} & & \multicolumn{3}{c}{Human}\\
                \cmidrule{3-5}  \cmidrule{7-9}
        Method & Params &  PSNR $\uparrow$ & DSSIM $\downarrow$ & LPIPS $\downarrow$ & &  PSNR $\uparrow$ & DSSIM $\downarrow$ & LPIPS $\downarrow$  \\
        \midrule
         Intrinsic NF & 329k & 34.25 & .099 & .189 & & 32.29 & \textbf{.119} & .330   \\
         Absolute value & 329k & 34.67 & .106 & .252 & & \textbf{32.42} & .132 & .363 \\
         Sign flip & 329k & 23.15 & 1.28 & 2.35 & & 21.52 & 1.05 & 2.71  \\
         SignNet & 324k & \textbf{34.91} & \textbf{.090} & \textbf{.147} & & \textbf{32.43} & .125 & \textbf{.316} \\
         \bottomrule
    \end{tabular}
    }
\end{table}

Discrete approximations to the Laplace-Beltrami operator on manifolds have proven useful for processing data on surfaces, such as triangle meshes~\citep{levy2006laplace}. Recently, \cite{koestler2022intrinsic} propose intrinsic neural fields, which use eigenfunctions of the Laplace-Beltrami operator as positional encodings for learning neural fields on manifolds. For generalized eigenfunctions $v_1, \ldots, v_k$, at a point $p$ on the surface, they parameterize functions $f(p) = \mrm{MLP}(v_1(p), \ldots, v_k(p))$. As these eigenfunctions have sign ambiguity, we use our SignNet to parameterize $f(p) = \mrm{MLP}(\, \rho(\, [\phi(v_i(p)) + \phi(-v_i(p))]_{i=1, \ldots, k}\,)\,)$, with $\rho$ and $\phi$ being MLPs.


Table~\ref{tab:intrinsic_nf} shows our results for texture reconstruction experiments on all models from \citet{koestler2022intrinsic}. The total number of parameters in our SignNet-based model is kept below that of the original model. We see that the SignNet architecture improves over the original Intrinsic NF model and over other baselines --- especially in the LPIPS (Learned Perceptual Image Patch Similarity) metric, which has been shown to be a typically better perceptual metric than PSNR or DSSIM~\citep{zhang2018unreasonable}. While we have not yet tested this, we believe that SignNet would allow even better improvements when learning over eigenfunctions of different models, as it could improve transfer and generalization. See Appendix~\ref{appendix:cat_viz} for visualizations and Appendix~\ref{appendix:texture} for more details.



\subsection{Visualization of Learned Positional Encodings}



\begin{figure}[ht]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/eigfunc_11.png}
    \caption{Eigvec 11}
    \end{subfigure} 
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/phi_pca_signnet11.png}
    \caption{$\phi(v_{11}) + \phi(-v_{11})$}
    \end{subfigure}  $\vline$
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/eigfunc_14.png}
    \caption{Eigvec 14}
    \end{subfigure} 
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/phi_pca_signnet14.png}
    \caption{$\phi(v_{14}) + \phi(-v_{14})$}
    \end{subfigure} \\[5pt]
    
    
    \caption{Cotangent Laplacian eigenvectors of the cat model and  first principal component of $\phi(v) + \phi(-v)$ from our trained SignNet. Our SignNet encodes bilateral symmetry, which is useful for reconstruction of the bilaterally symmetric texture.}
    \label{fig:cat_viz_small}
\end{figure}

To better understand SignNet, we plot the first principal component of $\phi(v) + \phi(-v)$ for two eigenvectors on the cat model in Figure~\ref{fig:cat_viz_small}. We see that SignNet encodes bilateral symmetry and structural information on the cat model.
 See Appendix~\ref{appendix:visualization} for plots of more eigenvectors and further~details.



\section{Related Work}

In this section, we review selected related work. A more thorough review is deferred to Appendix~\ref{appendix:more_related}.

\textbf{Laplacian eigenvectors in GNNs.} Various recently proposed methods in graph deep learning have directly used Laplacian eigenvectors as node positional encodings that are input to a message passing GNN~\citep{dwivedi2020benchmarking, dwivedi2022graph}, or some variant of a Transformer that is adapted to graphs~\citep{dwivedi2020generalization, kreuzer2021rethinking, mialon2021graphit, dwivedi2022graph, kim2022pure}. None of these methods address basis invariance, and they only partially address sign invariance for node positional encodings by randomly flipping eigenvector signs during training.


\textbf{Graph positional encodings.} Other recent methods use positional encodings besides Laplacian eigenvectors. These include positional encodings based on random walks~\citep{dwivedi2022graph,mialon2021graphit,li2020distance}, diffusion kernels on graphs~\citep{mialon2021graphit, feldman2022weisfeiler}, shortest paths~\citep{ying2021transformers,li2020distance}, and unsupervised node embedding methods~\citep{wang2022equivariant}. In particular, \cite{wang2022equivariant} use Laplacian eigenvectors for relative positional encodings in an invariant way, but they focus on robustness, so they have stricter invariances that significantly reduce expressivity (see Appendix~\ref{appendix:more_related_eigvec} for more details).
These previously used positional encodings are mostly ad-hoc, less general since they can be provably expressed by SignNet and BasisNet (see Section~\ref{sec:existing_pe}), and/or are expensive to compute (e.g., all pairs shortest paths). 



\section{Conclusion and Discussion}
SignNet and BasisNet are novel architectures for processing eigenvectors that are invariant to sign flips and choices of eigenspace bases, respectively. Both architectures are provably universal under certain conditions.
When used with Laplacian eigenvectors as inputs they provably go beyond spectral graph convolutions, spectral invariants, and a number of other graph positional encodings. These theoretical results are supported by experiments showing that SignNet and BasisNet are highly expressive in practice, and learn effective graph positional encodings that improve the performance of message passing graph neural networks. Initial explorations show that SignNet and BasisNet can be useful beyond graph representation learning, as eigenvectors are ubiquitous in machine learning.  

While we conduct experiments on graph machine learning tasks and a particular task on triangle meshes, SignNet and BasisNet should also be applicable to processing eigenvectors in other settings, such as recommender systems and tasks in shape analysis. Moreover, while we primarily consider eigenspaces in this work, sign invariance and basis invariance applies to any model that processes subspaces of a vector space; future work may explore our models on general subspaces.

\subsubsection*{Acknowledgments}

We thank anonymous reviewers of earlier versions of this work, especially those of the Topology, Algebra, and Geometry Workshop at ICML 2022, for providing useful feedback and suggestions. We thank Leonardo Cotta for a discussion about automorphism symmetries in real-world and random graphs (Appendix~\ref{appendix:automorphisms} and \ref{appendix:random_graphs}). We thank Truong Son Hy for sending us some useful PyTorch codes for invariant graph networks. Stefanie Jegelka and Suvrit Sra acknowledge support from NSF CCF-2112665 (TILOS AI Research Institute). Stefanie Jegelka also acknowledges support from NSF Award 2134108 and NSF Convergence Accelerator Track D 2040636 and NSF C-ACCEL D636 - CRIPT Phase 2. Suvrit Sra acknowledges support from NSF CAREER grant (IIS-1846088).  Joshua Robinson is partially supported by a Two Sigma fellowship. 



\bibliography{refs}
\bibliographystyle{iclr2023_conference}

\newpage

\appendix


\section{Universality for Multiple Spaces}\label{sec:universality}


While the networks introduced in the Section~\ref{sec: multiple spaces} possess the desired invariances, it is not immediately obvious whether they are  powerful enough to express \emph{all} functions with these invariances.
Under certain conditions, the universality of our architectures follows as a corollary of the following general decomposition result,  which may enable construction of universal  architectures for  other invariances as well.


\begin{theorem}[Decomposition Theorem]\label{thm:general_ansatz}
    Let $\mc X_1, \ldots, \mc X_k$ be  topological spaces, and let $G_i$ be a group acting on $\mc X_i$ for each $i$. We assume mild topological conditions on $\mc X_i$ and $G_i$ hold. For any continuous $f: \mc X = \mc X_1 \times \ldots \times \mc X_k \to \RR^{\dout}$  that is invariant to the action of $G = G_1 \times \ldots \times G_k$, there exists continuous $\phi_i$ and a continuous $\rho: \mc Z \subseteq \RR^a \to \RR^{\dout}$ such that
    \begin{equation}\label{eq:general_ansatz}
        f(v_1, \ldots, v_k) = \rho( \phi_1(v_1), \ldots,  \phi_k(v_k) ).
    \end{equation}
    Furthermore: (1) each $\phi_i$ can be taken to be invariant to $G_i$, (2) the domain $\mc Z$ of $\rho$ is compact if each $\mc X_i$ is compact, (3) if $\mc X_i = \mc X_j$ and $G_i = G_j$, then $\phi_i$ can be taken to be equal to $\phi_j$. 
\end{theorem}

This result says that when a product of groups $G$ acts on a product of spaces $\mc X$, for invariance to the product group $G$ it suffices to individually process each smaller group $G_i$ on $\mc X_i$ and then aggregate the results.
Along with the proof of Theorem \ref{thm:general_ansatz}, the mild topological assumptions are explained in Appendix~\ref{appendix:general_ansatz}.
The assumptions hold for sign invariance and basis invariance, when not enforcing permutation equivariance.
By applying this theorem, we can prove universality of some instances of our networks:

\begin{corollary}\label{cor:universal_net}
    Unconstrained-SignNet can represent any sign invariant function and Unconstrained-BasisNet can represent any basis invariant function. Expressive-BasisNet is a universal approximator of functions that are both basis invariant and permutation equivariant.
\end{corollary}

This result shows that Unconstrained-SignNet, Unconstrained-BasisNet, and Expressive-BasisNet 
take the correct functional form for their respective invariances  (proofs in Appendix \ref{appdx: niversality of SignNet and BasisNet}). \rebut{Note that Expressive-BasisNet approximates all sign invariant functions as a special case, by treating all inputs as one dimensional eigenspaces.} Further, note that we require Expressive-BasisNet's high order tensors to achieve universality when enforcing permutation equivariance. Universality under permutation equivariance is generally difficult to achieve when dealing with matrices with permutation symmetries~\citep{maron2019universality, keriven2019universal}, but it may be possible that more efficient architectures can achieve it in our setting.

Accompanying the decomposition result, we show a corresponding  universal approximation result (proof  in Appendix \ref{appdx: Universal Approximation}). Similarly to Theorem \ref{thm:general_ansatz}, the problem of approximating $G= G_1 \times \ldots \times G_k$ invariant functions  is reduced to approximating several $G_i$-invariant functions.


\section{More Details on SignNet and BasisNet}\label{appendix:more_signnet_details}


\begin{table}[ht]
    \def\checkyes{\textcolor{green}{\checkmark}}
    \def\xno{\textcolor{red}{$\times$}}
    
    \centering
    \caption{Properties of our architectures: Unconstrained-SignNet, SignNet, Unconstrained-BasisNet, and Expressive-BasisNet. The properties are: permutation equivariance, universality (for the proper class of continuous invariant functions), and computational tractability.}
    \label{tab:method_comparison}
    {\small
    \begin{tabular}{rccccc}
    \toprule
         & Unconstr.-SignNet & SignNet & Unconstr.-BasisNet & BasisNet & Expr.-BasisNet  \\
         \midrule
        Perm. equivariant & \xno & \checkyes & \xno & \checkyes & \checkyes \\
        Universal  & \checkyes & \xno & \checkyes & \xno & \checkyes \\
        Tractable & \checkyes & \checkyes & \checkyes & \checkyes & \xno  \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{8.5pt}\fontfamily{lmtt}\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{8pt}{9pt}\color{blue},
  keywordstyle=\fontsize{8pt}{9pt}\color{purple},
  stringstyle=\fontsize{8pt}{9pt}\color{blue},
  frame=tb,
  otherkeywords = {self},
}


\begin{figure}[ht]
    \centering
    \begin{minipage}{.6\columnwidth}
    \begin{lstlisting}[language=python,
        title={PyTorch-like pseudo-code for SignNet},
        captionpos=t]
    class SignNetGNN(nn.Module):
    
        def __init__(self, d, k, D1, D2, out_dim):
            self.phi = GIN(1, D1) # in dim=1, out dim=D1
            self.rho = MLP(k*D1, D2)
            self.base_model = GNN(d+D2, out_dim)
            
        def forward(self, g, x, eigvecs):
            # g contains graph information
            # x shape: n x d
            # eigvecs shape: n x k
            
            n, k = eigvecs.shape
            eigvecs = eigvecs.reshape(n, k, 1)
            pe = self.phi(g, eigvecs) + self.phi(g, -eigvecs)
            pe = pe.reshape(n, -1) # n x k x D1 -> n x k*D1
            pe = self.rho(pe)
            
            return self.base_model(g, x, pe)
    \end{lstlisting}
    \end{minipage}
    \caption{PyTorch-like pseudo-code for using SignNet with a GNN prediction model, where $\phi=\mrm{GIN}$ and $\rho=\mrm{MLP}$ as in the ZINC molecular graph regression experiments. Reshaping eigenvectors from $n \times k$ to $n \times k \times 1$ allows $\phi$ to process each eigenvector (and its negation) independently in PyTorch-like deep learning libraries.}
    \label{fig:signnet_code}
\end{figure}

In Figure~\ref{fig:signnet_diagram}, we show a diagram that describes how SignNet is used as a node positional encoding for a graph machine learning task. In Table~\ref{tab:method_comparison}, we compare and contrast properties of the neural architectures that we introduce. In Figure~\ref{fig:signnet_code}, we give pseudo-code of SignNet for learning node positional encodings with a GNN prediction model.

\subsection{Generalization Beyond Symmetric Matrices}\label{appendix:nonsymmetric}

In the main paper, we assume that the eigenspaces come from a symmetric matrix. This holds for many cases of practical interest, as e.g. the Laplacian matrix of an undirected graph is symmetric. However, we may also want to process directed graphs, or other data that have associated nonsymmetric matrices. Our SignNet and BasisNet generalize in a straightforward way to handle nonsymmetric diagonalizable matrices, as we detail here. Let $A \in \RR^{n \times n}$ be a matrix with a diagonalization $A = V \Lambda V^{-1}$, where $\Lambda = \mrm{Diag}(\lambda_1, \ldots, \lambda_n)$ contains the eigenvalues $\lambda_i$, and the columns of $V = \begin{bmatrix} v_1 & \ldots & v_n \end{bmatrix}$ are eigenvectors. Suppose we want to learn a function on the eigenvectors $v_1, \ldots, v_k$. Unlike in the symmetric matrix case, the eigenvectors are not necessarily orthonormal, and both the eigenvalues and eigenvectors can be complex.

\textbf{Real eigenvectors.} First, we assume the eigenvectors $v_i$ are all real vectors in $\RR^n$. We can take the eigenvectors to be real if $A$ is symmetric, or if $A$ has real eigenvalues~(see \cite{horn2012matrix} Theorem 1.3.29). Also, suppose that we choose the real numbers $\RR$ as our base field for the vector space in which eigenvectors lie. Note that for any scaling factor $c \in \RR \setminus \{0\}$ and eigenvector $v$, we have that $cv$ is an eigenvector of the same eigenvalue. If the eigenvalues are distinct, then the eigenvectors of the form $cv$ are the only other eigenvectors in the same eigenspace as $v$. Thus, we want a function to be invariant to scalings:
\begin{equation}
    f(v_1, \ldots, v_k) = f(c_1 v_1, \ldots, c_k v_k) \qquad c_i \in \RR \setminus \{0\}.
\end{equation}
This can be handled by SignNet, by giving unit normalized vector inputs:
\begin{equation}
    f(v_1, \ldots, v_k) = \rho\left(\left[\phi(v_i/ \norm{v_i}) + \phi(-v_i/\norm{v_i}) \right]_{i=1, \ldots, k} \right).
\end{equation}
Now, say have bases of eigenspaces $V_1, \ldots, V_l$ with dimensions $d_1, \ldots, d_l$. For a basis $V_i$, we have that any other basis of the same space can be obtained as $V_i W$ for some $W \in \mrm{GL}_\RR(d_i)$, the set of real invertible matrices in $\RR^{d_i \times d_i}$. Indeed, the orthonormal projector for the space spanned by the columns of $V_i$ is given by $V_i(V_i^\top V_i)^{-1}V_i^\top$. Thus, if $Z \in \RR^{n \times d_i}$ is another basis for the column space of $V_i$, we have that $V_i(V_i^\top V_i)^{-1}V_i^\top = Z(Z^\top Z)^{-1}Z^\top$, so 
\begin{equation}
    V_i(V_i^\top V_i)^{-1} V_i^\top Z = Z (Z^\top Z)^{-1} Z^\top Z = Z,
\end{equation}
so let $W = (V_i^\top V_i)^{-1}V_i^\top Z \in \RR^{d_i \times d_i}$. Note that $W$ is invertible, because it has inverse $(Z^\top Z)^{-1}Z^\top V_i$, so indeed $V_i W = Z$ for $W \in \mrm{GL}_\RR(d_i)$. Thus, basis invariance in this case is of the form
\begin{equation}
    f(V_1 \ldots, V_l ) = f(V_1 W_1, \ldots, V_l W_l) \qquad W_i \in \mrm{GL}_\RR(d_i).
\end{equation}
Note that the distinct eigenvalue invariance is a special case of this invariance, as $\mrm{G}_{\RR}(1) = \RR \setminus \{0\}$.
We can again achieve this basis invariance by using a BasisNet, where the inputs to the $\phi_{d_i}$ are orthogonal projectors of the corresponding eigenspace:
\begin{equation}
    f(V_1, \ldots, V_l) = \rho\left(\left[\phi_{d_i}(V_i (V_i^\top V_i)^{-1} V_i^\top) \right]_{i=1, \ldots, l} \right).
\end{equation}
Recall that if $V_i$ is an orthonormal basis, then the orthogonal projector is just $V_i V_i^\top$, so this is a direct generalization of BasisNet in the symmetric case.


\textbf{Complex eigenvectors.} More generally, suppose $V \in \CC^{n \times n}$ are complex eigenvectors, and we take the base field of the vector space to be $\CC$. The above arguments generalize to the complex case; in the case of distinct eigenvalues, we want
\begin{equation}
    f(v_1, \ldots, v_k) = f(c_1 v_1, \ldots, c_k v_k) \qquad c_i \in \CC \setminus \{0\}.
\end{equation}
However, this symmetry can not be as easily reduced to a unit normalization and a discrete sign invariance, as it can be in the real case. Nonetheless, the basis invariant architecture directly generalizes, so we can handle the case of distinct eigenvalues by a more general basis invariant architecture as well. The basis invariance is
\begin{equation}
    f(V_1, \ldots, V_l) = f(V_1 W_1, \ldots, V_l W_l) \qquad W_i \in \mrm{GL}_{\CC}(d_i).
\end{equation}
The orthogonal projector of the image of $V_i$ is $V_i(V_i^* V_i)^{-1}V_i^*$, where there are now conjugate transposes replacing the transposes. Thus, BasisNet takes the form:
\begin{equation}
    f(V_1, \ldots, V_l) = \rho\left(\left[\phi_{d_i}(V_i (V_i^* V_i)^{-1} V_i^*) \right]_{i=1, \ldots, l} \right).
\end{equation}

\subsection{Broader Impacts}\label{appendix:broader_impact}

We believe that our models and future sign invariant or basis invariant networks could be useful in a wide variety of applications. As eigenvectors arise in many domains, it is difficult to predict the uses of these models. We test on several molecular property prediction tasks, which have the potential for much positive impact, such as in drug discovery~\citep{stokes2020deep}. However, recent work has found that the same models that we use for finding beneficial drugs can also be used to design biochemical weapons~\citep{urbina2022dual}. Another major application of graph machine learning is in social network analysis, where positive (e.g. malicious node detection~\citep{pandit2007netprobe}) and negative (e.g. deanonymization~\citep{narayanan2009anonymizing}) uses of machine learning are possible. Even if there is no negative intent, bias in learned models can differentially impact particular subgroups of people. Thus, academia, industry, and policy makers must be aware of such potential negative uses, and work towards reducing the likelihood of them. 

\section{More on Eigenvalue Multiplicities}\label{appendix:eig_in_practice}

In this section, we study the properties of eigenvalues and eigenvectors computed by numerical algorithms on real-world data.

\subsection{Sign and Basis Ambiguities in Numerical Eigensolvers}

When processing real-world data, we use eigenvectors that are computed by numerical algorithms. These algorithms return specific eigenvectors for each eigenspace, so there is some choice of sign or basis of each eigenspace. The general symmetric matrix eigensolvers \texttt{numpy.linalg.eigh} and \texttt{scipy.linalg.eigh} both call LAPACK routines. They both proceed as follows: for a symmetric matrix $A$, they first decompose it as $A = QTQ^\top$ for orthogonal $Q$ and tridiagonal $T$, then they compute the eigendecomposition of $T = W\Lambda W^\top$, so the eigendecomposition of $A$ is $A = (QW)\Lambda (W^\top Q^\top)$. There are multiple ambiguities here: for diagonal sign matrices $S = \mrm{Diag}(s_1, \ldots, s_n)$ and $S' = \mrm{Diag}(s_1', \ldots, s_n')$, where $s_i, s_i' \in \{-1, 1\}$, we have that $A = QS(STS)SQ^\top$ is also a valid tridiagonalization, as $QS$ is still orthogonal, $SS = I$, and $STS$ is still tridiagonal. Also, $T = (WS')\Lambda (S'W^\top)$ is a valid eigendecomposition of $T$, as $WS'$ is still orthogonal.

In practice, we find that the general symmetric matrix eigensolvers \texttt{numpy.linalg.eigh} and \texttt{scipy.linalg.eigh} differ between frameworks but are consistent with the same framework. More specifically, for a symmetric matrix $A$, we find that the eigenvectors computed with the default settings in numpy tend to differ by a choice of sign or basis from those that are computed with the default settings in scipy. On the other hand, the called LAPACK routines are deterministic, so the eigenvectors returned by numpy are the same in each call, and the eigenvectors returned by scipy are likewise the same in each call.

Eigensolvers for sparse symmetric matrices like \texttt{scipy.linalg.eigsh} are required for large scale problems. This function calls ARPACK, which uses an iterative method that starts with a randomly sampled initial vector. Due to this stochasticity, the sign and basis of eigenvectors returned differs between each call.

\cite{bro2008resolving} develop a data-dependent method to choose signs for each singular vector of a singular value decomposition. Still, in the worst case the signs chosen will be arbitrary, and they do not handle basis ambiguities in higher dimensional eigenspaces. Other works have made choices of sign, such as by picking the sign so that the eigenvector's entries are in the largest lexicographic order~\citep{tam2022multiscale}. This choice of sign may work poorly for learning on graphs, as it is sensitive to permutations on nodes. For some graph regression experiments in Section~\ref{sec:graph_regression}, we try a choice of sign that is permutation invariant, but we find it to work poorly.


\subsection{Higher Dimensional Eigenspaces in Real Graphs}\label{appendix:higher_dim}

Here, we investigate the normalized Laplacian eigenspace statistics of real-world graph data. For any graph that has distinct Laplacian eigenvalues, only sign invariance is required in processing eigenvectors. However, we find that graph data tends to have higher multiplicity eigenvalues, so basis invariance would be required for learning symmetry-respecting functions on eigenvectors.

Indeed, we show statistics for multi-graph datasets in Table~\ref{tab:stats_multigraph} and for single-graph datasets with more nodes per graph in Table~\ref{tab:stats_onegraph}. For multi-graph datasets, we consider :
\begin{itemize}
    \item Molecule graphs: ZINC~\citep{irwin2012zinc,dwivedi2020benchmarking}, ogbg-molhiv~\citep{wu2018moleculenet,hu2020open}
    \item Social networks: IMDB-M, COLLAB \citep{yanardag2015deep,morris2020tudataset}, 
    \item Bioinformatics graphs: PROTEINS \citep{morris2020tudataset}
    \item Computer vision graphs: COIL-DEL \citep{riesen2008iam,morris2020tudataset}.
\end{itemize}
For single-graph datasets, we consider: 
\begin{itemize}
    \item  The $32 \times 32$ image grid as in Section~\ref{sec:spectral_conv_exp}
    \item Citation networks: Cora, Citeseer~\citep{sen2008collective}
    \item Co-purchasing graphs with Amazon Photo~\citep{mcauley2015image,shchur2018pitfalls}.
\end{itemize}

We see that these datasets all contain higher multiplicity eigenspaces, so sign invariance is insufficient for fully respecting symmetries. The majority of graphs in each multi-graph dataset besides COIL-DEL contain higher multiplicity eigenspaces. Also, the dimension of these eigenspaces can be quite large compared to the size of the graphs in the dataset. The single-graph datasets have a large proportion of their eigenvectors belonging to higher dimensional eigenspaces. Thus, basis invariance may play a large role in processing spectral information from these graph datasets.


\begin{table}
    \centering
    \caption{Eigenspace statistics for datasets of multiple graphs. From left to right, the columns are: dataset name, number of graphs, range of number of nodes per graph, largest multiplicity, and percent of graphs with an eigenspace of dimension $>$ 1.}
    \label{tab:stats_multigraph}
    \begin{tabular}{lccccc}
        \toprule
        Dataset & Graphs & \# Nodes & Max. Mult & \% Graphs mult. $>1$\\
        \midrule
        ZINC & 12,000 & 9-37 & 9 & 64.1 \\
        ZINC-full & 249,456 & 6-38 & 10 & 63.8 \\
        ogbg-molhiv & 41,127 & 2 - 222 & 42 & 68.0  \\
        IMDB-M & 1,500 & 7 - 89 & 37 & 99.9 \\
        COLLAB & 5,000 & 32 - 492 & 238 & 99.1 \\
        PROTEINS & 1,113 & 4 - 620 & 20 & 77.3 \\
        COIL-DEL & 3,900 & 3 - 77 & 4 & 4.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Eigenspace statistics for single graphs. From left to right, the columns are: dataset name, number of nodes, distinct eigenvalues (i.e. distinct eigenspaces),  number of unique multiplicities, largest multiplicity, and percent of eigenvectors belonging to an eigenspace of dimension $> 1$.}
    \label{tab:stats_onegraph}
    \begin{tabular}{lccccc}
        \toprule
        Dataset & Nodes & Distinct $\lambda$ & \# Mult. & Max Mult. & \% Vecs mult. $>1$  \\
        \midrule
        $32 \times 32$ image  & 1,024 & 513 & 3 & 32  & 96.9 \\
        Cora & 2,708 & 2,187 & 11 & 300 & 19.7 \\
        Citeseer & 3,327 & 1,861 & 12 & 491 &  44.8 \\
        Amazon Photo & 7,650 & 7,416 & 8 & 136 & 3.71 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Relationship to Graph Automorphisms}\label{appendix:automorphisms}

Higher multiplicity eigenspaces are related to automorphism symmetries in graphs. For an adjacency matrix $A$, the permutation matrix $P$ is an automorphism of the graph associated to $A$ if $PAP^\top = A$. If $P$ is an automorphism, then for any eigenvector $v$ of $A$ with eigenvalue $\lambda$, we have
\begin{equation}
    APv = PAP^\top P v = PAv = P\lambda v = \lambda Pv,
\end{equation}
so $Pv$ is an eigenvector of $A$ with the same eigenvalue $\lambda$. If $Pv$ and $v$ are linearly independent, then $\lambda$ has a higher dimensional eigenspace. Thus, under certain additional conditions, automorphism symmetries of graphs lead to repeated eigenvalues~\citep{sachs1983automorphism,teranishi2009eigenvalues}. 

\subsection{Multiplicities in Random Graphs}\label{appendix:random_graphs}

It is known that almost all random graphs under the Erd\H{o}s-Renyi model have no repeated eigenvalues in the infinite number of nodes limit~\citep{tao2017random}. Likewise, almost all random graphs under the Erd\H{o}s-Renyi model are asymmetric in the sense of having no nontrivial automorphism symmetries~\citep{erdos1963asymmetric}. These results contrast sharply with the high eigenvalue multiplicities that we see in real-world data in Section~\ref{appendix:higher_dim}. Likewise, many types of real-world graph data have been found to possess nontrivial automorphism symmetries~\citep{ball2018symmetric}. This demonstrates a potential downside of using random graph models to study real-world data: the eigenspace dimensions and automorphism symmetries of random graphs may not agree with those of real-world data.




\clearpage

\section{Visualization of SignNet output}\label{appendix:visualization}


\subsection{Cat Model Visualization}\label{appendix:cat_viz}

\begin{figure}[ht]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \newcommand{\wth}{.45}
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_01.png}
    \caption{Eigenvector 1}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet01.png}
    \caption{$\phi(v_1) + \phi(-v_1)$}
    \end{subfigure} \\[5pt]
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_09.png}
    \caption{Eigenvector 9}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet09.png}
    \caption{$\phi(v_9) + \phi(-v_9)$}
    \end{subfigure} \\[5pt]
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_11.png}
    \caption{Eigenvector 11}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet11.png}
    \caption{$\phi(v_{11}) + \phi(-v_{11})$}
    \end{subfigure} \\[5pt]
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_14.png}
    \caption{Eigenvector 14}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet14.png}
    \caption{$\phi(v_{14}) + \phi(-v_{14})$}
    \end{subfigure} \\[5pt]
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_1023.png}
    \caption{Eigenvector 1023}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet_1023.png}
    \caption{$\phi(v_{1023}) + \phi(-v_{1023})$}
    \end{subfigure}
    
    
    \caption{(Left) Cotangent Laplacian eigenvectors of the cat model.  (Right) First principal component of $\phi(v) + \phi(-v)$ from our trained SignNet.}
    \label{fig:cat_viz}
\end{figure}

In Figure~\ref{fig:cat_viz}, we plot the eigenvectors of the cotangent Laplacian on a cat model, as well as the first principal component of the corresponding learned $\phi(v) + \phi(-v)$ from our SignNet model that was trained on the texture reconstruction task. Interestingly, this portion of our SignNet encodes bilateral symmetry; for instance, while some eigenvectors differ between left feet and right feet, this portion of our SignNet gives similar values for the left and right feet. This is useful for the texture reconstruction task, as the texture regression target has bilateral symmetry.

\begin{figure}[h]
    \centering
    \includegraphics[width=.14\columnwidth]{figs/pca_signnet01.png} \hspace{20pt}
    \includegraphics[width=.14\columnwidth]{figs/pca_signnet02.png} \hspace{20pt}
    \includegraphics[width=.14\columnwidth]{figs/pca_signnet03.png}
    \caption{First three principal components of the full SignNet output on the cat model.}
    \label{fig:cat_full_signnet}
\end{figure}

We also show principal components of outputs for the full SignNet model in Figure~\ref{fig:cat_full_signnet}. This is not as interpretable, as the outputs are high frequency and appear to be close to the texture that is the regression target. If instead we trained the network on a task involving eigenvectors of multiple models, then we may expect the SignNet to learn more structurally interpretable mappings (as in the case of the molecule tasks).

\subsection{Molecule visualization}\label{appendix:molecule_viz}

\begin{figure}[ht]
    \newcommand{\wth}{.2}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_1.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_2.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_3.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_4.pdf} \\
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_5.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_6.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_7.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_8.pdf} \\
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_9.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_10.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_11.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_12.pdf} \\
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_13.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_14.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_15.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_16.pdf} \\
     \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_17.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_18.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_19.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_20.pdf} \\
     \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_21.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_22.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_23.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_24.pdf} 
    \caption{All normalized Laplacian eigenvectors of the fluorescein graph. The first principal components of SignNet's learned positional encodings do not exactly match any eigenvectors.}
    
    \label{fig:fluorescein_eigvec}
\end{figure}

\begin{figure}[ht]
    \newcommand{\wth}{.24}
    \hspace{2.2pt}\includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_1.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_2.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_23.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_eigvec_24.pdf}\\[10pt]
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_0.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_1.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_2.pdf}
    \hspace{2.1pt}\includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_3.pdf}
    \caption{Normalized Laplacian eigenvectors and learned positional encodings for the graph of  fluorescein. (Top row) From left to right: smallest and second smallest nontrivial eigenvectors, then second largest and largest eigenvectors. (Bottom row) From left to right: first four principal components of the output $\rho([\phi(v_i) + \phi(-v_i)]_{i=1,\ldots, n})$ of SignNet. 
    }
    \label{fig:visualize_molecule}
\end{figure}

To better understand SignNet, in Figure~\ref{fig:visualize_molecule} we visualize the learned positional encodings of a SignNet with $\phi=\text{GIN}$, $\rho=\text{MLP}$ (with a summation to handle variable eigenvector numbers)  trained on ZINC as in Section~\ref{sec:graph_regression}.
 SignNet learns interesting structural information such as min-cuts (PC 3) and appendage atoms (PC 2) that qualitatively differ from any single eigenvector of the graph. 

For this visualization we use a SignNet trained with a GatedGCN base model on ZINC, as in Section~\ref{sec:graph_regression}. This SignNet uses GIN as $\phi$ and $\rho$ as an MLP (with a sum before it to handle variable numbers of eigenvectors), and takes in all eigenvectors of each graph. See Figure~\ref{fig:fluorescein_eigvec} for all of the eigenvectors of fluorescein.


\section{More Related Work}\label{appendix:more_related}

\subsection{Graph Positional Encodings}

Various graph positional encodings have been proposed, which have been motivated for increasing expressive power or practical performance of graph neural networks, and for  generalizing Transformers to graphs. Positional encodings are related to so-called position-aware network embeddings~\citep{chami2020machine}, which capture distances between nodes in graphs. These include network embedding methods like Deepwalk~\citep{perozzi2014deepwalk} and node2vec~\citep{grover2016node2vec}, which have been recently integrated into GNNs that respect their invariances by \cite{wang2022equivariant}. Further, \cite{li2020distance} studies the theoretical and practical benefits of incorporating distance features into graph neural networks. \cite{dwivedi2022graph} proposes a method to inject learnable positional encodings into each layer of a graph neural network, and uses a simple random walk based node positional encoding. \cite{you2021identity} proposes a node positional encoding $\mrm{diag}(A^k)$, which captures the number of closed walks from a node to itself. \cite{dwivedi2020benchmarking} propose to use Laplacian eigenvectors as positional encodings in graph neural networks, with sign ambiguities alleviated by sign flipping data augmentation. \cite{srinivasan2019equivalence} theoretically analyze node positional embeddings and structural representations in graphs, and show that most-expressive structural representations contain the information of any node positional embedding.

While positional encodings in sequences as used for Transformers~\citep{vaswani2017attention} are able to leverage the canonical order in sequences, there is no such useful canonical order for nodes in a graph, due in part to permutation symmetries. Thus, different permutation equivariant positional encodings have been proposed to help generalize Transformers to graphs. \cite{dwivedi2020generalization} directly add in linearly projected Laplacian eigenvectors to node features before processing these features with a graph Transformer. \cite{kreuzer2021rethinking} propose an architecture that uses attention over Laplacian eigenvectors and eigenvalues to learn node or edge positional encodings. \cite{mialon2021graphit} uses spectral kernels such as the diffusion kernel to define relative positional encodings that modulate the attention matrix. \cite{ying2021transformers} achieve state-of-the-art empirical performance with simple Transformers that incorporate shortest-path based relative positional encodings. \cite{zhang2020graph} also utilize shortest-path distances for positional encodings in their graph Transformer. \cite{kim2021transformers} develop higher-order transformers (that generalize invariant graph networks), which interestingly perform well on graph regression using sparse higher-order transformers without positional encodings.

\subsection{Eigenvector Symmetries in Graph Representation Learning}\label{appendix:more_related_eigvec}

Many works that attempt to respect the invariances of eigenvectors solely focus on sign invariance (by using data augmentation)~\citep{dwivedi2020benchmarking, dwivedi2020generalization, dwivedi2022graph, kreuzer2021rethinking}. This may be reasonable for continuous data, where eigenvalues of associated matrices may be usually distinct and separated~(e.g. \cite{puny2021frame} finds that this empirically holds for covariance matrices of $n$-body problems). However, discrete graph Laplacians are known to have higher multiplicity eigenvalues in many cases, and in Appendix~\ref{appendix:higher_dim} we find this to be true in various types of real-world graph data. Graphs without higher multiplicity eigenspaces are easier to deal with; in fact, graph isomorphism can be tested in polynomial time on graphs of bounded multiplicity for adjacency matrix eigenvalues~\citep{babai1982isomorphism, LeightonMiller79}, with a time complexity that is lower for graphs with lower maximum multiplicities.

A recent work of \citet{wang2022equivariant} proposes full orthogonal group invariance for functions that process positional encodings. In particular, for positional encodings $Z \in \RR^{n \times k}$, they parameterize functions $f(Z)$ such that $f(Z) = f(ZQ)$ for all $Q \in O(k)$. This indeed makes sense for network embeddings like node2vec~\citep{grover2016node2vec}, as their objective functions are based on inner products and are thus orthogonally invariant. While they prove stability results when enforcing full orthogonal invariance for eigenvectors, this is a very strict constraint compared to our basis invariance. For instance, when $k=n$ and all eigenvectors are used in $V$, the condition $f(V) = f(VQ)$ implies that $f$ is a constant function on orthogonal matrices, since any orthogonal matrix $W$ can be obtained as $W = VQ$ for $Q = V^\top W \in O(n)$.
In other words, for bases of eigenspaces $V_1, \ldots, V_l$ and $V = \begin{bmatrix} V_1 & \ldots & V_l \end{bmatrix}$, \citet{wang2022equivariant} enforces $VQ \cong V$, while we enforce $V \mrm{Diag}(Q_1, \ldots, Q_l) \cong V$. While the columns of $V \mrm{Diag}(Q_1, \ldots, Q_l)$ are still eigenvectors, the columns of $VQ$ generally are not.


\subsection{Graph Spectra and Learning on Graphs}
More generally, graph spectra are widely used in analyzing graphs, and spectral graph theory \citep{chung1997spectral} studies the connection between graph properties and graph spectra. 
Different graph kernels have been defined based on graph spectra, which use robust and discriminative notions of generalized spectral distance \citep{verma2017hunt}, the spectral density of states \citep{huang2021density}, random walk return probabilities \citep{zhang2018retgk}, or the trace of the heat kernel \citep{tsitsulin2018netlsd}. 
Graph signal processing relies on spectral operations to define Fourier transforms, frequencies, convolutions, and other useful concepts for processing data on graphs~\citep{ortega2018graph}. The closely related spectral graph neural networks  \citep{wu2020comprehensive,balcilar2020analyzing} parameterize neural architectures that are based on similar spectral operations.


\section{Definitions, Notation, and Background}

\subsection{Basic Topology and Algebra Definitions}

We will use some basic topology and algebra for our theoretical results. A topological space $(\mc X, \tau)$ is a set $\mc X$ along with a family of subsets $\tau \subseteq 2^{\mc X}$ satisfying certain properties, which gives useful notions like continuity and compactness. From now on, we will omit mention of $\tau$, and refer to a topological space as the set $\mc X$ itself. For topological spaces $\mc X$ and $\mc Y$, we write $\mc X \cong \mc Y$ and say that $\mc X$ is homeomorphic to $\mc Y$ if there exists a continuous bijection with continuous inverse from $\mc X$ to $\mc Y$. We will say $\mc X = \mc Y$ if the underlying sets and topologies are equal as sets (we will often use this notion of equality for simplicity, even though it can generally be substituted with homeomorphism). 
For a function $f : \mc X \to \mc Y$ between topological spaces $\mc X$ and $\mc Y$, the image $\mrm{im} f$ is the set of values that $f$ takes, $\mrm{im} f =\{f(x) : x \in \mc X\}$. This is also denoted $f(\mc X)$.
A function $f: \mc X \to \mc Y$ is called a topological embedding if it is a homeomorphism from $\mc X$ to its image.

A group $G$ is a set along with a multiplication operation $G \times G \to G$, such that multiplication is associative, there is a multiplicative identity $e \in G$, and each $g \in G$ has a multiplicative inverse $g^{-1}$. A topological group is a group that is also a topological space such that the multiplication and inverse operations are continuous.

A group $G$ may act on a set $\mc X$ by a function $\cdot: G \times \mc X \to \mc X$. We usually denote $g \cdot x$ as $gx$. A topological group is said to act continuously on a topological space $\mc X$ if $\cdot$ is continuous. For any group $G$ and topological space $\mc X$, we define the coset $Gx = \{ gx : g \in G\}$, which can be viewed as an equivalance class of elements that can be transformed from one to another by a group element. The quotient space $\mc X / G = \{Gx : x \in \mc X \}$ is the set of all such equivalence classes, with a topology induced by that of $\mc X$. The quotient map $\pi : \mc X \to \mc X / G$ is a surjective continuous map that sends $x$ to its coset, $\pi(x) = Gx$.

For $x \in \RR^d$, $\norm{x}_2$ denotes the standard Euclidean norm. By the $\infty$ norm of functions $f: \mc Z  \to \RR^d$ from a compact $\mc Z$ to a Euclidean space $\RR^d$, we mean $\norm{f}_\infty = \sup_{z \in \mc Z} \norm{f(z)}_2$.

\subsection{Background on Eigenspace Invariances}\label{appendix:eigenspace_background}

 Let $V = \begin{bmatrix} v_1 & \ldots & v_d \end{bmatrix}$ and $W = \begin{bmatrix} w_1 & \ldots & w_d \end{bmatrix} \in \RR^{n \times d}$ be two orthonormal bases for the same $d$ dimensional subspace of $\RR^n$. Since $V$ and $W$ span the same space, their orthogonal projectors are the same, so $VV^\top = WW^\top$. Also, since $V$ and $W$ have orthonormal columns, we have $V^\top V = W^\top W = I \in \RR^{d \times d}$. Define $Q = V^\top W$. Then $Q$ is orthogonal because
 \begin{equation}
     Q^\top Q = W^\top VV^\top W = W^\top WW^\top W = I
 \end{equation}
 Moreover, we have that
 \begin{equation}
     VQ = VV^\top W = WW^\top W = W
 \end{equation}
 Thus, for any orthonormal bases $V$ and $W$ of the same subspace, there exists an orthogonal $Q \in O(d)$ such that $VQ = W$.

For another perspective on this, define the Grassmannian $\gr(d,n)$ as the smooth manifold consisting of all $d$ dimensional subspaces of $\RR^n$. Further define the Stiefel manifold $\st(d,n)$ as the set of all orthonormal tuples $\begin{bmatrix}v_1 & \ldots & v_d\end{bmatrix} \in \RR^{n \times d}$ of $d$ vectors in $\RR^n$. Letting $O(d)$ act by right multiplication, it holds that $\st(d,n) / O(d) \cong \gr(d,n)$. This implies that any $O(d)$ invariant function on $\st(d,n)$ can be viewed as a function on subspaces. See e.g. \cite{gallier2020differential} Chapter 5 for more information on this. We will use this relationship in our proofs of universal representation.


When we consider permutation invariance or equivariance, the permutation acts on dimensions of size $n$. Then a tensor $X \in \RR^{n^k \times d}$ is called an order $k$ tensor with respect to this permutation symmetry, where order 0 are called scalars, order 1 tensors are called vectors, and order 2 tensors are called matrices. Note that this does not depend on $d$; in this work, we only ever consider vectors and scalars with respect to the $O(d)$ action.


\section{Proofs of Universality}


We begin by proving the two propositions for the single subspace case from Section \ref{sec: warmup}.

\begin{repproposition}{prop:one_sign_invariant}
    A continuous function $h: \RR^n \to \RR^{\dout}$ is sign invariant if and only if
    \begin{equation}
     h(v) = \phi(v) + \phi(-v)   \tag{\ref{eq:one_sign_ansatz}}
    \end{equation}
    for some continuous $\phi: \RR^n \to \RR^{\dout}$. A continuous $h: \RR^n \to \RR^n$ is sign invariant and permutation equivariant if and only if \eqref{eq:one_sign_ansatz} holds for a continuous permutation equivariant $\phi: \RR^n \to \RR^n$.
\end{repproposition}



\begin{proof}
    If $h(v) = \phi(v) + \phi(-v)$, then $h$ is obviously sign invariant. On the other hand, if $h$ is sign invariant, then letting $\phi(v) = h(v)/2$ gives that $h(v) = \phi(v) + \phi(-v)$, and $\phi$ is of course continuous.

    If $h(v) = \phi(v) + \phi(-v)$ for a permutation equivariant $\phi$, then $h(-Pv) = \phi(-Pv) + \phi(Pv) = P\phi(-v) + P\phi(v) = P(\phi(v) + \phi(-v)) = Ph(v)$, so $h$ is permutation equivariant and sign invariant. If $h$ is permutation equivariant and sign invariant, then define $\phi(v) = h(v)/2$ again; it is clear that $\phi$ is continuous and permutation equivariant.
\end{proof}


\begin{repproposition}{prop:ign_universal}
    Any continuous, $O(d)$ invariant $h: \RR^{n \times d} \to \RR^{\dout}$ is of the form $h(V) = \phi(VV^\top)$ for a continuous $\phi$. For a compact domain $\mc Z \subseteq \RR^{n \times d}$, maps of the form $V \mapsto \mrm{IGN}(VV^\top)$ universally approximate continuous functions $h: \mc Z \subseteq \RR^{n \times d} \to \RR^n$ that are $O(d)$ invariant and permutation equivariant.
\end{repproposition}


\begin{proof}
    The case without permutation equivariance holds by the First Fundamental Theorem of $O(d)$ (Lemma~\ref{lem:first_fund}).
    
    For the permutation equivariant case, let $\mc Z' = \{ VV^\top : V \in \mc Z\}$ and let $\epsilon > 0$. Note that $\mc Z'$ is compact, as it is the continuous image of a compact set. Since $h$ is $O(d)$ invariant, the first fundamental theorem of $O(d)$ shows that there exists a continuous function $\phi: \mc Z' \subseteq \RR^{n \times n} \to \RR^n$ such that $h(V) = \phi(VV^\top)$. Since $h$ is permutation equivariant, for any permutation matrix $P$ we have that
    \begin{align}
        h(PV) & = P \cdot h(V)\\
        \phi(P V V^\top P^\top) & = P \cdot \phi(V V^\top),
    \end{align}
    so $\phi$ is a continuous permutation equivariant function from matrices to vectors. Then note that \cite{keriven2019universal} show that invariant graph networks (of generally high tensor order in hidden layers) universally approximate continuous permutation equivariant functions from matrices to vectors on compact sets of matrices.  Thus, an $\mrm{IGN}$ can $\epsilon$-approximate $\phi$, and hence $V \mapsto \mrm{IGN}(VV^\top)$ can $\epsilon$-approximate $h$.
\end{proof}

\subsection{Proof of Decomposition Theorem}\label{appendix:general_ansatz}

\tikzcdset{every label/.append style = {font = \normalsize}}
\tikzcdset{arrows={line width=.7pt}}
\begin{figure}[ht]
    \centering
    \begin{tikzcd}[row sep=large, column sep=large]
        & \mc X_1 \times \ldots \times \mc X_k \arrow{d}[left]{\pi = \pi_1 \times \ldots \pi_k} \arrow{dr}{f = \tilde f \circ \pi} \arrow[purple, bend right=40, dashed]{ddl}[left]{\phi = \psi \circ \pi \quad }  \\
        & \arrow[bend left=15]{dl}{\psi = \psi_1 \times \ldots \times \psi_k} (\mc X_1 / G_1) \times \ldots \times (\mc X_k / G_k) \arrow{r}[below]{\tilde f} & \RR^{\dout} \\
        \mc Z = \mrm{im}(\psi) \subseteq \RR^a \arrow[bend left=15]{ur}{\psi^{-1}} \arrow[dashed, bend right=40, purple]{urr}[below]{\rho = \tilde f \circ \psi^{-1}}
    \end{tikzcd}
    \caption{Commutative diagram for our proof of Theorem~\ref{thm:general_ansatz}. Black arrows denote functions from topological constructions, and red dashed lines denote functions that we parameterize by neural networks ($\phi = \phi_1 \times \ldots \times \phi_k$ and $\rho$).}
    \label{fig:cd_general_ansatz}
\end{figure}


Here, we give the formal statement of Theorem~\ref{thm:general_ansatz}, which provides the necessary topological assumptions for the theorem to hold. In particular, we only require the $G_i$ be a topological group that acts continuously on $\mc X_i$ for each $i$, and that there exists a topological embedding of each quotient space into some Euclidean space. That the group action is continuous is a very mild assumption, and it holds for any finite or compact matrix group, which all of the invariances we consider in this paper can be represented as.

A topological embedding of the quotient space into a Euclidean space is desired, as we know how to parameterize neural networks with Euclidean outputs and inputs, whereas dealing with a quotient space is generally difficult. Many different conditions can guarantee existence of such an embedding.
For instance, if the quotient space is a smooth manifold, then the Whitney Embedding Theorem (Lemma~\ref{lem:whitney}) guarantees such an embedding. Also, if the base space $\mc X_i$ is a Euclidean space and $G_i$ is a finite or compact matrix Lie group, then a map built from $G$-invariant polynomials gives such an embedding (\cite{gonzalez2003c} Lemma 11.13).

Figure~\ref{fig:cd_general_ansatz} provides a commutative diagram representing the constructions in our proof.
\newtheorem*{thm:general_ansatz}{Theorem~\ref{thm:general_ansatz}}
\begin{thm:general_ansatz}[Decomposition Theorem]
    Let $\mc X_1, \ldots, \mc X_k$ be  topological spaces, and let $G_i$ be a topological group acting continuously on $\mc X_i$ for each $i$. Assume that there is a topological embedding $\psi_i : \mc X_i / G_i \to \RR^{a_i}$ of each quotient space into a Euclidean space $\RR^{a_i}$ for some dimension $a_i$. 
    Then, for any continuous function $f: \mc X = \mc X_1 \times \ldots \times \mc X_k \to \RR^{\dout}$  that is invariant to the action of $G = G_1 \times \ldots \times G_k$, there exists continuous functions $\phi_i : \mc X_i \to \RR^{a_i}$ and a continuous function $\rho: \mc Z \subseteq \RR^{a} \to \RR^{\dout}$, where $a = \sum_i a_i$ such that
    \begin{equation}
        f(v_1, \ldots, v_k) = \rho( \phi_1(v_1), \ldots,  \phi_k(v_k) ).
    \end{equation}
    Furthermore: (1) each $\phi_i$ can be taken to be invariant to $G_i$, (2) the domain $\mc Z$ is compact if each $\mc X_i$ is compact, (3) if $\mc X_i = \mc X_j$ and $G_i = G_j$, then $\phi_i$ can be taken to be equal to $\phi_j$. 
\end{thm:general_ansatz}
\begin{proof}
    Let $\pi_i: \mc X_i \to \mc X_i / G_i$ denote the quotient map for $\mc X_i / G_i$. Since each $G_i$ acts continuously, Lemma~\ref{lem:product_quotient} gives that the quotient of the product space is the product of the quotient spaces, i.e. that
    \begin{equation}
        (\mc X_1 \times \ldots \times \mc X_k) / (G_1 \times \ldots G_k) \cong (\mc X_1 / G_1) \times \ldots \times (\mc X_k / G_k),
    \end{equation}
    and the corresponding quotient map $\pi: \mc X / G$ is given by 
    \begin{equation}
    \pi = \pi_1 \times \ldots \times \pi_k, \qquad \pi(x_1, \ldots, x_k) = (\pi_1(x_1), \ldots, \pi_k(x_k)).
\end{equation}
By passing to the quotient (Lemma~\ref{lem:pass_to_quotient}), there exists a continuous $\tilde f: \mc X /  G \to \RR^{\dout}$ on the quotient space such that $f = \tilde f \circ \pi$. By Lemma~\ref{lem:quotient_compact}, each $\mc X_i / G_i$ is compact if $\mc X_i$ is compact. Defining the image $\mc Z_i =  \psi_i(\mc X_i / G_i) \subseteq \RR^{a_i}$, we thus know that $\mc Z_i$ is compact if $\mc X_i$ is compact. 

Moreover, as $\psi_i$ is a topological embedding, it has a continuous inverse $\psi_i^{-1}$ on its image $\mc Z_i$. Further, we have a topological embedding $\psi: \mc X / G \to \mc Z = \mc Z_1 \times \ldots \times \mc Z_k$ given by $\psi = \psi_1 \times \ldots \times \psi_k$, with continuous inverse $\psi^{-1} = \psi_1^{-1} \times \ldots \times \psi_k^{-1}$.

Note that
\begin{equation}
    f = \tilde f \circ \pi = (\tilde f \circ \psi^{-1}) \circ (\psi \circ \pi).
\end{equation}
So we define
\begin{align}
    \rho & = \tilde f \circ \psi^{-1} & \rho: \mc Z \to \RR^{\dout}\\
    \phi_i & = \psi_i \circ \pi_i & \phi_i: \mc X_i \to \mc Z_i \\
    \phi & = \psi \circ \pi = \phi_1 \times \ldots \times \phi_k & \phi: \mc X \to \mc Z
\end{align}
Thus, $f = \rho \circ \phi = \rho \circ (\phi_1 \times \ldots \times \phi_k)$, so equation \eqref{eq:general_ansatz} holds. Moreover, the $\rho$ and $\phi_i$ are continuous, as they are compositions of continuous functions. Furthermore, (1) holds as each $\phi_i$ is invariant to $G_i$ because each $\pi_i$ is invariant to $G_i$.  Since each $\mc Z_i$ is compact if $\mc X_i$ is compact, the product $\mc Z = \mc Z_1 \times \ldots \times \mc Z_k$ is compact if each $\mc X_i$ is compact, thus proving (2).

To show the last statement (3), note simply that if $\mc X_i = \mc X_j$ and $G_i = G_j$, then the quotient maps are equal, i.e. $\pi_i = \pi_j$. Moreover, we can choose the embeddings to be equal, so say $\psi_i = \psi_j$. Then, $\phi_i = \psi_i \circ \pi_i = \psi_j \circ \pi_j = \phi_j$, so we are done.
\end{proof}



\subsection{Universality of SignNet and BasisNet}\label{appdx: niversality of SignNet and BasisNet}

Here, we prove Corollary~\ref{cor:universal_net} on the universal representation and approximation capabilities of our Unconstrained-SignNets, Unconstrained-BasisNets, and Expressive-BasisNets. We proceed in several steps, first proving universal representation of continuous functions when we do not require permutation equivariance, then proving universal approximation when we do require permutation equivariance.


\subsubsection{Sign Invariant Universal Representation}

Recall that $\SS^{n-1}$ denotes the unit sphere in $\RR^n$. As we normalize eigenvectors to unit norm, the domain of our functions on $k$ eigenvectors are on the compact space $(\SS^{n-1})^k$.

\begin{corollary}[Universal Representation for SignNet]\label{thm:no_feature_sign_inv}
    A continuous function $f: (\SS^{n-1})^k \to \RR^{\dout}$ is sign invariant, i.e. $f(s_1 v_1, \ldots, s_k v_k) = f(v_1, \ldots, v_k)$ for any $s_i \in \{-1, 1\}$, if and only if there exists a continuous $\phi: \RR^n \to \RR^{2n-2}$ and a continuous $\rho: \RR^{(2n-2)k} \to \RR^{\dout}$ such that
    \begin{equation}
        f(v_1, \ldots, v_k) = \rho\left([\phi(v_i) + \phi(-v_i)]_{i=1}^k \right).
    \end{equation}
\end{corollary}
\begin{proof}
    It can be directly seen that any $f$ of the above form is sign invariant.

    Thus, we show that any sign invariant $f$ can be expressed in the above form.
    First, we show that we can apply the general Theorem~\ref{thm:general_ansatz}. The group $G_i = \{1, -1\}$ acts continuously and satisfies that $\SS^{n-1} / \{1, -1\} =  \RR\PP^{n-1}$, where $\RR\PP^{n-1}$ is the real projective space of dimension $n-1$. Since $\RR\PP^{n-1}$ is a smooth manifold of dimension $n-1$, Whitney's embedding theorem states that there exists a (smooth) topological embedding $\psi_i: \RR \PP^{n-1} \to \RR^{2n-2}$~(Lemma~\ref{lem:whitney}).

    Thus, we can apply the general theorem to see that $f = \rho \circ \tilde \phi^k$ for some continuous $\rho$ and $\tilde \phi^k$. Note that each $\tilde \phi_i = \tilde \phi$ is the same, as each $\mc X_i = \SS^{n-1}$ and $G_i = \{1, -1\}$ is the same. Also, Theorem~\ref{thm:general_ansatz} says that we may assume that $\tilde \phi$ is sign invariant, so $\tilde \phi(x) = \tilde \phi(-x)$. Letting $\phi(x) = \tilde \phi(x)/2$, we are done with the proof.
\end{proof}


\subsubsection{Sign Invariant Universal Representation with Extra Features}

Recall that we may want our sign invariant functions to process other data besides eigenvectors, such as eigenvalues or node features associated to a graph. Here, we show universal representation for when we have this other data that does not possess sign symmetry. The proof is a simple extension of Corollary~\ref{thm:no_feature_sign_inv}, but we provide the technical details for completeness.

\begin{corollary}[Universal Representation for SignNet with features]
    For a compact space of features $\Omega \subseteq \RR^d$, let $f(v_1, \ldots, v_k, x_1, \ldots, x_k)$ be a continuous function $f: (\SS^{n-1} \times \Omega)^k  \to \RR^{\dout}$.

    Then $f$ is sign invariant for the inputs on the sphere, i.e. 
    \begin{equation}
    f(s_1 v_1, \ldots, s_k v_k, x_1, \ldots, x_k) = f(v_1, \ldots, v_k, x_1, \ldots, x_k) \qquad s_i \in \{1, -1\},
\end{equation}
if and only if there exists a continuous $\psi: \RR^{n+d} \to \RR^{2n-2+d}$ and a continuous $\rho: \RR^{(2n-2+d)k} \to \RR^{\dout}$ such that
    \begin{equation}
        f(v_1, \ldots, v_k) = \rho\left(\phi(v_1, x_1) + \phi(-v_1, x_1), \  \ldots, \  \phi(v_k, x_k) + \phi(-v_k, x_k) \right).
    \end{equation}
\end{corollary}
\begin{proof}
    Once again, the sign invariance of any $f$ in the above form is clear.

    We follow very similar steps to the proof of Corollary~\ref{thm:no_feature_sign_inv} to show that we may apply Theorem~\ref{thm:general_ansatz}. We can view $\Omega$ as a quotient space, after quotienting by the trivial group that does nothing, $\Omega \cong \Omega / \{1\}$. The corresponding quotient map is $\mrm{id}_\Omega$, the identity map. Also, $\Omega$ trivially topologically embeds in $\RR^d$ by the inclusion map.

    As $G_i = \{-1, 1\} \times \{1\}$ acts continuously, by Lemma~\ref{lem:product_quotient} we have that
    \begin{equation}
        (\SS^{n-1} \times \Omega) / (\{1, -1\} \times \{1\}) \cong (\SS^{n-1}/\{1, -1\}) \times (\Omega / \{1\}) \cong \RR\PP^{n-1} \times \Omega,
    \end{equation}
    with corresponding quotient map $\pi \times \mrm{id}_\Omega$, where $\pi$ is the quotient map to $\RR\PP^{n-1}$. 

    Letting $\tilde \psi$ be the embedding of $\RR\PP^{n-1} \to \RR^{2n-2}$ guaranteed by Whitney's embedding theorem (Lemma~\ref{lem:whitney}), we have that $\psi = \tilde \psi \times \mrm{id}_\Omega$ is an embedding of $\RR\PP^{n-1} \times \Omega \to \RR^{2n-2 + d}$. Thus, we can apply Theorem~\ref{thm:general_ansatz} to write $f = \rho \circ \tilde \phi^k$ for $\tilde \phi = (\tilde \psi \times \mrm{id}_\Omega) \circ (\pi \times \mrm{id}_\Omega)$, so
    \begin{equation}
        \tilde \phi(v_i, x_i) = (\tilde \psi(v_i), x_i),
    \end{equation}
    where $\tilde \phi(v_i, x_i) = \tilde \phi(-v_i, x_i)$. Letting $\phi(v_i, x_i) = \tilde \phi(v_i, x_i)/2$, we are done.
\end{proof}


\subsubsection{Basis Invariant Universal Representation}

Recall that $\st(d, n)$ is the Stiefel manifold of $d$-tuples of vectors $(v_1, \ldots, v_d)$ where $v_i \in \RR^n$ and $v_1, \ldots, v_d$ are orthonormal. This is where our inputs lie, as our eigenvectors are unit norm and orthogonal. We will also make use of the Grassmannian $\gr(d, n)$, which consists of all $d$-dimensional subspaces in $\RR^n$. This is because the Grassmannian is the quotient space for the group action we want, $\gr(d, n) \cong \st(d,n) / O(d)$, where $Q \in O(d)$ acts on $V \in \st(d,n) \subseteq \RR^{n \times d}$ by mapping $V$ to $VQ$~\citep{gallier2020differential}.

\begin{corollary}[Universal Representation for BasisNet]\label{cor:universal_basisnet}
    For dimensions $d_1, \ldots, d_l \leq n$ let $f$ be a continuous function on $\st(d_1, n) \times  \ldots \times \st(d_l, n)$. Further assume that $f$ is invariant to $\O(d_1) \times \ldots \times \O(d_l)$, where $\O(d_i)$ acts on $\st(d_i, n)$ by multiplication on the right. 

    Then there exist continuous $\rho: \RR^{\sum_{i=1}^l 2d_i(n-d_i)} \to \RR^{\dout}$ and continuous $\phi_i: \st(d_i, n) \to \RR^{2d_i(n-d_i)}$ such that
    \begin{equation}
        f(V_1, \ldots, V_l) = \rho\left( \phi_1(V_1), \ldots, \phi_l(V_l) \right),
    \end{equation}
    where the $\phi_i$ are $\O(d_i)$ invariant functions, and we can take $\phi_i = \phi_j$ if $d_i = d_j$.
\end{corollary}
\begin{proof}
    Letting $\mc X_i = \st(d_i, n)$ and $G_i = \O(d_i)$, it can be seen that $G_i$ acts continuously on $\mc X_i$. Also, we have that the quotient space $\st(d_i, n) / \O(d_i) = \gr(d_i, n)$ is the Grassmannian of $d_i$ dimensional subspaces in $\RR^n$, which is a smooth manifold of dimension $d_i(n-d_i)$. Thus, the Whitney embedding theorem (Lemma~\ref{lem:whitney}) gives a topological embedding $\psi_i: \gr(d_i, n) \to \RR^{2d_i(n-d_i)}$.

    Hence, we may apply Theorem~\ref{thm:general_ansatz} to obtain continuous $\O(d_i)$ invariant $\phi_i: \st(d_i, n) \to \RR^{2d_i(n-d_i)}$ and continuous $\rho: \RR^{\sum_{i=1}^l 2d_i(n-d_i)} \to \RR^{\dout}$, such that $f = \rho \circ (\phi_1 \times \ldots \times \phi_l)$. Also, if $d_i = d_j$, then $\mc X_i = \mc X_j$ and $G_i = G_j$, so we can take $\phi_i = \phi_j$.

\end{proof}

\subsubsection{Basis Invariant and Permutation Equivariant Universal Approximation}

With the restriction that $f(V_1, \ldots, V_l): \RR^{n \times \sum_i d_i} \to \RR^n$ be permutation equivariant and basis invariant, we need to use the impractically expensive Expressive-BasisNet to approximate $f$. Universality of permutation invariant or equivariant functions from matrices to scalars or matrices to vectors is difficult to achieve in a computationally tractable manner~\citep{maron2019universality,keriven2019universal, maehara2019simple}. One intuitive reason to expect this is that universally approximating such functions allows solution of the graph isomorphism problem~\citep{chen2019equivalence}, which is a computationally difficult problem. While we have exact representation of basis invariant functions by continuous $\rho$ and $\phi_i$ when there is no permutation equivariance constraint, we can only achieve approximation up to an arbitrary $\epsilon > 0$ when we require permutation equivariance.

\begin{corollary}[Universal Approximation for Expressive-BasisNets]
    Let $f(V_1, \ldots, V_l): \st(d_1, n) \times \ldots \times \st(d_l, n) \to \RR^n$ be continuous, $O(d_1) \times \ldots \times O(d_l)$ invariant, and permutation equivariant. Then $f$ can be $\epsilon$-approximated by an Expressive-BasisNet.
\end{corollary}
\begin{proof}
    By invariance, Corollary~\ref{cor:universal_basisnet} of the decomposition theorem shows that $f$ can be written as 
    \begin{equation}
        f(V_1, \ldots, V_l) = \rho\left(\varphi_{d_1}(V_1), \ldots, \varphi_{d_l}(V_l) \right)
    \end{equation}
    for some continuous $O(d_i)$ invariant $\varphi_{d_i}$ and continuous $\rho$. By the first fundamental theorem of $O(d)$ (Lemma~\ref{lem:first_fund}), each $\varphi_{d_i}$ can be written as $\varphi_{d_i}(V_i) = \phi_{d_i}(V_iV_i^\top)$ for some continuous $\phi_{d_i}$. Let 
    \begin{equation}
    \mc Z = \{(V_1 V_1^\top, \ldots, V_l V_l^\top) : V_i \in \st(d_i,n)\} \subseteq \RR^{n^2 \times l},
    \end{equation}
    which is compact as it is the image of the compact space $\st(d_1, n) \times \ldots \times \st(d_l, n)$ under a continuous function.
    Define $h: \mc Z \subseteq \RR^{n^2 \times l} \to \RR^n$ by 
    \begin{equation}
     h(V_1 V_1^\top, \ldots, V_l V_l^\top) = \rho\left(\phi_{d_1}(V_1 V_1^\top), \ldots, \phi_{d_l}(V_l V_l^\top) \right).
    \end{equation}
    Then note that $h$ is continuous and permutation equivariant from matrices to vectors, so it can be $\epsilon$-approximated by an invariant graph network~\citep{keriven2019universal}, call it $\widetilde{\mrm{IGN}}$. If we define $\tilde \rho = \widetilde{\mrm{IGN}}$ and $\mrm{IGN}_{d_i}(V_i V_i^\top) = V_i V_i^\top$ (this identity operation is linear and permutation equivariant, so it can be exactly expressed by an IGN), then we have $\epsilon$-approximation of $f$ by 
    \begin{equation}
        \widetilde{\mrm{IGN}}(V_1 V_1^\top, \ldots, V_l V_l^\top) = \tilde \rho\left(\mrm{IGN}_{d_1}(V_1 V_1^\top), \ldots, \mrm{IGN}_{d_l}(V_lV_l^\top) \right).
    \end{equation}
\end{proof}


\subsection{Proof of Universal Approximation for General Decompositions}\label{appdx: Universal Approximation}



\begin{theorem}\label{thm:decomp_universal}
Consider the same setup as Theorem \ref{thm:general_ansatz}, where $\mc X_i$ are also compact.
Let $\Phi_i$ be a family of $G_i$-invariant functions that universally approximate $G_i$-invariant continuous functions $\mc X_i \rightarrow \mathbb{R}^{a_i}$, and let $\mc R$ be a set of continuous function that universally approximate continuous functions $\mc Z \subseteq \RR^{a}  \rightarrow \mathbb{R}^{\dout}$ for every compact $\mc Z$, where $a = \sum_i a_i$. Then for any $\varepsilon > 0$ and any $G$-invariant continuous function $f :\mc X_1 \times \ldots \times \mc X_k \to \RR^{\dout}$ there exists $\phi \in \Phi$ and $\rho \in \mc R$ such that $\|f - \rho(\phi_1, \ldots , \phi_k)\|_\infty < \varepsilon$.
\end{theorem}
\begin{proof}
Consider a particular $G$-invariant continuous function $f : \mc X_1 \times \ldots \times \mc X_k \to \RR^{\dout}$. By Theorem \ref{thm:general_ansatz}  there exists  $G_i$-invariant  continuous functions $\phi'_i : \mc X_i \to \RR^{a_i}$ and a continuous function $\rho': \mc Z \subseteq \RR^{a} \to \RR^{\dout}$ (where $a = \sum_i a_i$) such that
    \begin{equation*}
        f(v_1, \ldots, v_k) = \rho'( \phi'_1(v_1), \ldots,  \phi'_k(v_k) ).
    \end{equation*} 
Now fix an $\varepsilon > 0$. For any $\rho \in \mc R$ and any  $\phi_i \in \Phi_i$ ($i=1, \ldots k$) we may bound the difference from $f$ as follows (suppressing the $v_i$'s for brevity),
\begin{align*}
    & \| f  - \rho( \phi_1, \ldots,  \phi_k ) \|_\infty\\
    &=  \| \rho'( \phi'_1, \ldots,  \phi'_k )  - \rho( \phi_1, \ldots,  \phi_k ) \|_\infty \\
    &= \| \rho'( \phi'_1, \ldots,  \phi'_k ) - \rho( \phi'_1, \ldots,  \phi'_k ) + \rho( \phi'_1, \ldots,  \phi'_k )  - \rho( \phi_1, \ldots,  \phi_k ) \|_\infty \\
     &\leq \|\rho'( \phi'_1, \ldots,  \phi'_k ) - \rho( \phi'_1, \ldots,  \phi'_k ) \|_\infty+ \| \rho( \phi'_1, \ldots,  \phi'_k )  - \rho( \phi_1, \ldots,  \phi_k ) \|_\infty \\
     &= \RN{1}+\RN{2}
\end{align*}
Now let $K' = \prod_{i=1}^k \text{im} \phi'_i$. Since each $\phi'_i$ is continuous and defined on a compact set $\mc X_i$ we know that $\text{im} \phi'_i$ is compact, and so the product $K$ is also compact. Since $K'$ is compact, it is contained in a closed ball $B(r)$ of radius $r > 0$ centered at the origin. Let $K$ be the closed ball $B(r+1)$ of radius $r+1$ centered at the origin, so $K$ contains $K'$ and a ball of radius $1$ around each point of $K'$. We may extend $\rho'$ continuously to $K$ as needed, so assume $\rho': K \to \RR^{\dout}$. By universality of $\mc R$ we may pick a particular $\rho: K \to \RR^{\dout}$, $\rho \in \mc R$ such that 
\begin{equation*}
   \RN{1} =  \sup_{\{v_i \in \mc X_i\}_{i=1}^k} \|  \rho'( \phi'_1, \ldots,  \phi'_k ) - \rho( \phi'_1, \ldots,  \phi'_k ) \|_\infty \leq \sup_{z \in K} \| \rho'(z) - \rho(z) \|_2 < \varepsilon / 2.
\end{equation*}
Keeping this choice of $\rho$, it remains only to bound $\RN{2}$. As $\rho$ is continuous on a compact domain, it is in fact uniformly continuous. Thus, we can choose a $\delta' > 0$ such that if $\norm{y-z}_2 \leq \delta'$, then $\norm{\rho(y) - \rho(z)}_\infty < \epsilon / 2$, and then we define $\delta = \min(\delta', 1)$.


Since $\Phi_i$ universally approximates $\phi'_i$  we may pick $\phi_i \in \Phi_i$ such that $\| \phi_i - \phi'_i \|_\infty < \delta/\sqrt{k}$, and thus $\| (\phi_1, \ldots , \phi_k) - (\phi'_1, \ldots \phi'_k) \|_\infty \leq \delta$. With this choice of $\phi_i$, we know that $\prod_{i=1}^k \mrm{im} \phi_i \subseteq K$ (because each $\phi_i(x_i)$ is within distance $1$ of $\phi'_i(x_i)$). Thus, $\rho(\phi_1(x_1), \ldots, \phi_k(x_k))$ is well-defined, and we have
\begin{align*}
   \RN{2} & = \|  \rho( \phi'_1, \ldots,  \phi'_k ) - \rho( \phi_1, \ldots,  \phi_k ) \|_\infty\\
   & = \sup_{\{x_i \in \mc X_i\}_{i=1}^k} \|  \rho( \phi'_1(x_1), \ldots,  \phi'_k(x_k) ) - \rho( \phi_1(x_1), \ldots,  \phi_k(x_k) ) \|_2\\
   & < \varepsilon / 2
\end{align*}
due to our choice of $\delta$, which completes the proof.
 \end{proof}


\section{Basis Invariance for Graph Representation Learning}



\subsection{Spectral Graph Convolution}\label{appendix:spectral_conv}

In this section, we consider spectral graph convolutions, which for node features $X \in \RR^{n \times \dfeat}$ take the form $f(V, \Lambda, X) = \sum_{i=1}^n \theta_i v_i v_i^\top X$ for some parameters $\theta_i$. We can optionally take $\theta_i = h(\lambda_i)$ for some continuous function $h: \RR \to \RR$ of the eigenvalues. This form captures most popular spectral graph convolutions in the literature~\citep{bruna2014spectral, hamilton2020graph, bronstein2017geometric}; often, such convolutions are parameterized by taking $h$ to be some analytic function such as a simple affine function~\citep{kipf2016semi}, a linear combination in a polynomial basis~\citep{defferrard2016convolutional, chien2021adaptive}, or a parameterization of rational functions~\citep{levie2018cayleynets, bianchi2021graph}.

First, it is well known and easy to see that spectral graph convolutions are permutation equivariant, as for a permutation matrix $P$ we have 
\begin{equation}
f(PV, \Lambda, PX) = \sum_i \theta_i P v_i v_i^\top P^\top P X = \sum_i \theta_i P v_i v_i^\top X = Pf(V, \Lambda, X).
\end{equation}
Also, it is easy to see that they are sign invariant, as $(-v_i)(-v_i)^\top = v_i v_i^\top$. However, if the $\theta_i$ do not depend on the eigenvalues, then the spectral graph convolution is not necessarily basis invariant. For instance, if $v_1$ and $v_2$ are in the same eigenspace, and we change basis by permuting $v_1' = v_2$ and $v_2' = v_1$, then if $\theta_1 \neq \theta_2$ the spectral graph convolution will generally change as well.

On the other hand, if $\theta_i = h(\lambda_i)$ for some function $h: \RR \to \RR$, then the spectral graph convolution is basis invariant. This is because if $v_i$ and $v_j$ belong to the same eigenspace, then $\lambda_i = \lambda_j$ so $h(\lambda_i) = h(\lambda_j)$. Thus, if $v_{i_1}, \ldots, v_{i_d}$ are eigenvectors of the same eigenspace with eigenvalue $\lambda$, we have that $\sum_{l=1}^d h(\lambda_{i_l}) v_{i_l} v_{i_l}^\top = h(\lambda) \sum_{l=1}^d v_{i_l} v_{i_l}^\top$. Now, note that $\sum_{l=1}^d v_{i_l} v_{i_l}^\top$ is the orthogonal projector onto the eigenspace~\citep{trefethen1997numerical}. A change of basis does not change this orthogonal projector, so such spectral graph convolutions are basis invariant.

Another way to see this basis invariance is with a simple computation. Let $V_1, \ldots, V_l$ be the eigenspaces of dimension $d_1, \ldots, d_l$, where $V_i \in \RR^{n \times d_i}$. Let the corresponding eigenvalues be $\mu_1, \ldots, \mu_l$. Then for any orthogonal matrices $Q_i \in O(d_i)$, we have
\begin{align}
    \sum_{i=1}^n h(\lambda_i) v_i v_i^\top & = \sum_{j=1}^l V_j h(\mu_j) I_{d_j} V_j^\top\\
& = \sum_{j=1}^l V_j  h(\mu_j) I_{d_j} Q_jQ_j^\top V_j^\top\\
& = \sum_{j=1}^l (V_j Q_j) h(\mu_j) I_{d_j} (V_j Q_j)^\top,
\end{align}
so the spectral graph convolution is invariant to substituting $V_j Q_j$ for $V_j$.

Now, we give the proof that shows SignNet and BasisNet can universally approximate spectral graph convolutions.
\newtheorem*{prop:spectral_conv}{Theorem~\ref{prop:spectral_conv}}
\begin{prop:spectral_conv}[Learning Spectral Graph Convolutions]
    Suppose the node features $X \in \RR^{n \times \dfeat}$ take values in compact sets. Then SignNet can universally approximate any spectral graph convolution, and both BasisNet and Expressive-BasisNet can universally approximate any parametric spectral graph convolution.
\end{prop:spectral_conv}
\begin{proof}
    Note that eigenvectors and eigenvalues of normalized Laplacian matrices take values in compact sets, since the eigenvalues are in $[0,2]$ and we take eigenvectors to have unit-norm. Thus, the whole domain of the spectral graph convolution is compact.

    Let $\varepsilon > 0$. First, consider a spectral graph convolution $f(V, \Lambda, X) = \sum_{i=1}^n \theta_i v_i v_i^\top X$. For SignNet, let $\phi(v_i, \lambda_i, X)$ approximate the function $\tilde \phi(v_i, \lambda_i, X) = \theta_i v_i v_i^\top X$ to within $\varepsilon/n$ error, which DeepSets can do since this is a continuous permutation equivariant function from vectors to vectors \citep{segol2019universal} (note that we can pass $\lambda_i$ as a vector in $\RR^n$ by instead passing $\lambda_i \mathbf{1}$, where $\mathbf{1}$ is the all ones vector). Then $\rho = \sum_{i=1}^n$ is a linear permutation equivariant operation that can be exactly expressed by DeepSets, so the total error is within $\varepsilon$. The same argument applies when $\theta_i = h(\lambda_i)$ for some continuous function $h$.
  
    For the basis invariant case, consider a parametric spectral graph convolution $f(V, \Lambda, X) = \sum_{i=1}^n h(\lambda_i) v_i v_i^\top X$. Note that if the eigenspace bases are $V_1, \ldots, V_l$ with eigenvalues $\mu_1, \ldots, \mu_l$, we can write the $f(V, \Lambda, X) = \sum_{i=1}^l h(\mu_j) V_j V_j^\top X$. Again, we will let $\rho=\sum_{i=1}^l $ be a sum function, which can be expressed exactly by DeepSets. Thus, it suffices to show that $h(\mu_j) V_j V_j^\top X$ can be $\epsilon/n$ approximated by a 2-IGN (i.e. an IGN that only uses vectors and matrices).

    Note that since $h$ is continuous, we can use an elementwise MLP (which IGNs can learn) to approximate $f_1(\mu\mathbf{11}^\top, VV^\top, X) = (h(\mu)\mathbf{11}^\top, VV^\top , X)$ to arbitrary precision (note that we represent the eigenvalue $\mu$ as a constant matrix $\mu \mathbf{11}^\top$). Also, since a 2-IGN can learn matrix vector multiplication~(\cite{cai2022convergence} Lemma 10), we can approximate $f_2(h(\mu)\mathbf{11}^\top, VV^\top, X) = (h(\mu)\mathbf{11}^\top, V V^\top X)$, as $V_i V_i^\top \in \RR^{n^2}$ is a matrix and $X \in \RR^{n \times \dfeat}$ is a vector with respect to permutation symmetries. Finally, we use an elementwise MLP to approximate the scalar-vector multiplication $f_3(h(\mu)\mathbf{11}^\top, VV^\top, X) = h(\mu) V V^\top X$. Since $f_3 \circ f_2 \circ f_1(\mu\mathbf{11}^\top, VV^\top, X) = h(\mu) VV^\top X$, and since 2-IGNs universally approximate each $f_i$, applying Lemma~\ref{lem:layer_universal} shows that a 2-IGN can approximate $h(\mu)VV^\top X$ to $\epsilon/n$ accuracy, so we are done. Since Expressive-BasisNet is stronger than BasisNet, it can also universally approximate these functions.
\end{proof}


From the proof, we can see that SignNet and BasisNet need only learn simple functions for the $\rho$ and $\phi$ when $h$ is simple, or when the filter is non-parametric and we need only learn $\theta_i$. \cite{xu2019can} propose the principle of algorithmic alignment, and show that if separate modules of a neural network each need only learn simple functions (that is, functions that are well-approximated by low-order polynomials with small coefficients), then the network may be more sample efficient. If we do not require permutation equivariance, and parameterize SignNet and BasisNet with simple MLPs, then algorithmic alignment may suggest that our models are sample efficient. Indeed, $\rho = \sum$ is a simple linear function with coefficients $1$, and $\phi(V, \lambda, X) = h(\lambda) V V^\top X$ is quadratic in $V$ and linear in $X$, so it is simple if $h$ is simple.

\begin{repproposition}{prop:signnet_strictly_greater_conv}
\rebut{There exist infinitely many pairs of non-isomorphic graphs that SignNet and BasisNet can distinguish, but spectral graph convolutions or spectral GNNs cannot distinguish.}
\end{repproposition}
\begin{proof}
    \rebut{
    The idea is as follows: we will take graphs $G$ and give them the node feature matrix $X_G = D^{1/2}\mathbf{1}$, i.e. each node has as feature the square root of its degree. Then any spectral graph convolution (or, the first layer of any spectral GNN) will map $V\mathrm{Diag}(\theta)V^\top X$ to something that only depends on the degree sequence and number of nodes. Thus, any spectral graph convolution or spectral GNN will have the same output (up to permutation) for any such graphs $G$ with node features $X_G$ and the same number of nodes and same degree sequence. On the other hand, SignNet and BasisNet can distinguish between infinitely many pairs of graphs $(G^{(1)}, G^{(2)})$ with node features $(X_{G^{(1)}}, X_{G^{(2)}})$ and the same number of nodes and degree sequence; this is because SignNet and BasisNet can tell when a graph is bipartite.
    }
    
    \rebut{For each $n \geq 5$, we will define $G^{(1)}$ and $G^{(2)}$ as connected graphs with $n$ nodes, with the same degree sequence. Also, we define $G^{(1)}$ to have node features $X^{(1)}_i = \sqrt{d_i^{(1)}}$, where $d_i^{(1)}$ is the degree of node $i$ in $G^{(1)}$, and similarly $G^{(2)}$ has node features $X^{(2)}_i = \sqrt{d_i^{(2)}}$. Now, note that $X^{(1)}$ is an eigenvector of the normalized Laplacian of $G^{(1)}$, and it has eigenvalue $0$. As we take the eigenvectors to be orthonormal (since the normalized Laplacian is symmetric), for any spectral graph convolution we have that}
    \begin{equation}
        \rebut{ \sum_{i=1}^n \theta_i v_i v_i^\top X^{(1)} =  \theta_1 v_1 v_1^\top X^{(1)} = \theta_1 D_1^{1/2}\mathbf{1} (D_1^{1/2}\mathbf{1})^\top D_1^{1/2}\mathbf{1} = \theta_1 \sum_{j=1}^n (d_j^{(1)}) D_1^{1/2}\mathbf{1}. }
    \end{equation}
    \rebut{Where $D_1$ is the diagonal degree matrix of $G^{(1)}$. Likewise, any spectral graph convolution outputs $\theta_1 \sum_j (d_j^{(2)}) D_2^{1/2} \mathbf{1}$ for $G^{(2)}$. Since $D_1$ and $D_2$ are the same up to a permutation, we have that any spectral graph convolution has the same output for $G^{(1)}$ and $G^{(2)}$, up to a permutation. In fact, this also holds for spectral GNNs, as the first layer will always have the same output (up to a permutation) on $G^{(1)}$ and $G^{(2)}$, so the latter layers will also have the same output up to a permutation.}

    \rebut{Now, we concretely define $G^{(1)}$ and $G^{(2)}$. This is illustrated in Figure~\ref{fig:example_graphs_1} and Figure~\ref{fig:example_graphs_2}. For $n=5$, let $G^{(1)}$ contain a triangle with nodes $w_1, w_2, w_3$, and have a path of length 2 coming out of one of the nodes in the triangle, say $w_1$ connects to $w_4$, and $w_4$ connects to $w_5$. This is not bipartite, as there is a triangle. Let $G^{(2)}$ be a bipartite graph that has 2 nodes on the left $(v_1, v_2)$ and 3 nodes on the right $(v_3, v_4, v_5)$. Connect $v_1 $ with all nodes on the right, and connect $v_2$ with $v_3$ and $v_4$.}
    
    \rebut{Note that both $G^{(1)}$ and $G^{(2)}$ have the same number of nodes and the same degree sequence $\{3, 2, 2, 2, 1\}$. Thus, spectral graph convolutions or spectral GNNs cannot distinguish them. However, SignNet and BasisNet can distinguish them, as they can tell whether a graph is bipartite by checking the highest eigenvalue of the normalized Laplacian. This is because the multiplicity of the eigenvalue 2 is the number of bipartite components. In particular, SignNet can approximate the function $\phi(v_i, \lambda_i, X) = \lambda_i$ and $\rho \approx \max_{i=1}^n$. Likewise, BasisNet can approximate the function $\phi_{d_i}(V_i V_i^\top, \lambda_i) = \lambda_i$ and $\rho \approx \max_{i=1}^l$.
    }
    
    \rebut{
    This in fact gives an infinite family of graphs that SignNet / BasisNet can distinguish, but spectral graph convolutions or spectral graph GNNs cannot. To see why, suppose we have $G^{(1)}$ and $G^{(2)}$ for some $n \geq 5$. Then we construct a pair of graphs on $n+1$ nodes with the same degree sequence. To do this, we add another node to the path of $G^{(1)}$, thus giving it degree sequence $\{3, 2, \ldots, 2, 1\}$. For $G^{(2)}$, we add a node $v_{n+1}$ to the side that $v_n$ is not contained on (e.g. for $n = 5$, we add $v_6$ to the left side, as $v_5$ was on the right), then connect $v_n$ to $v_{n+1}$ to also give a degree sequence $\{3, 2, \ldots, 2, 1\}$. Note that the non-bipartiteness of $G^{(1)}$ and bipartiteness of $G^{(2)}$ are preserved.
    }

\begin{figure}
    \centering
    \begin{tikzpicture}[main/.style = {draw, circle, node distance=1.25cm}, baseline=10pt] 
        \node[main] (1) at (0,2) {$w_1$};
        \node[main] (2) [below of=1] {$w_2$};
        \node[main] (3) [right of=2] {$w_3$};
        \node[main] (4) [above of=3] {$w_4$};
        \node[main] (5) [right of=4] {$w_5$};
        \node[] at (1, -1.25) {$G^{(1)}$};
        \draw[-] (1) edge (2);
        \draw[-] (2) edge (3);
        \draw[-] (1) edge (3);
        \draw[-] (1) edge (4);
        \draw[-] (4) edge (5);
        
        \node[main] (v1) at (6, 2) {$v_1$};
        \node[main] (v2) [below of=v1] {$v_2$};
        \node[main] (v3) [right of=v1] {$v_3$};
        \node[main] (v4) [below of=v3] {$v_4$};
        \node[main] (v5) [below of=v4] {$v_5$};
        \node[] at (6.75, -1.25) {$G^{(2)}$};
        \draw[-] (v1) edge (v3);
        \draw[-] (v1) edge (v4);
        \draw[-] (v1) edge (v5);
        \draw[-] (v2) edge (v3);
        \draw[-] (v2) edge (v4);
    \end{tikzpicture}
    \caption{\rebut{ Illustration of our constructed $G^{(1)}$ and $G^{(2)}$ for $n=5$, as used in the proof of Proposition~\ref{prop:signnet_strictly_greater_conv}.} }
    \label{fig:example_graphs_1}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}[main/.style = {draw, circle, node distance = 1.25cm}, baseline=10pt] 
    \node[main] (1b) at (8,2) {$w_1$};
    \node[main] (2b) [below of=1b] {$w_2$};
    \node[main] (3b) [right of=2b] {$w_3$};
    \node[main] (4b) [above of=3b] {$w_4$};
    \node[main] (5b) [right of=4b] {$w_5$};
    \node[main] (6b) [right of=5b] {$w_6$};
    \node[] at (9, -1.25) {$G^{(1)}$};
    \draw[-] (1b) edge (2b);
    \draw[-] (2b) edge (3b);
    \draw[-] (1b) edge (3b);
    \draw[-] (1b) edge (4b);
    \draw[-] (4b) edge (5b);
    \draw[-] (5b) edge (6b);
    
    \node[main] (v1b) at (15, 2) {$v_1$};
    \node[main] (v2b) [below of=v1b] {$v_2$};
    \node[main] (v3b) [right of=v1b] {$v_3$};
    \node[main] (v4b) [below of=v3b] {$v_4$};
    \node[main] (v5b) [below of=v4b] {$v_5$};
    \node[main] (v6b) [below of=v2b] {$v_6$};
    \node[] at (15.75, -1.25) {$G^{(2)}$};
    \draw[-] (v1b) edge (v3b);
    \draw[-] (v1b) edge (v4b);
    \draw[-] (v1b) edge (v5b);
    \draw[-] (v2b) edge (v3b);
    \draw[-] (v2b) edge (v4b);
    \draw[-] (v5b) edge (v6b);
\end{tikzpicture} 
\caption{\rebut{Illustration of our constructed $G^{(1)}$ and $G^{(2)}$ for $n=6$, as used in the proof of Proposition~\ref{prop:signnet_strictly_greater_conv}.}}
\label{fig:example_graphs_2}
\end{figure}
    
\end{proof}

\subsection{Existing Positional Encodings}\label{appdx: existing PE}

Here, we show that our SignNets and BasisNets universally approximate various types of existing graph positional encodings. The key is to show that these positional encodings are related to spectral graph convolution matrices and the diagonals of these matrices, and to show that our networks can approximate these matrices and diagonals.

\begin{proposition}\label{prop:diag_spectral_conv}
    If the eigenvalues take values in a compact set, SignNets and BasisNets universally approximate the diagonal of any spectral graph convolution matrix $f(V, \Lambda) = \mrm{diag}\left(\sum_{i=1}^n h(\lambda_i) v_iv_i^\top \right)$. BasisNets can additionally universally approximate any spectral graph convolution matrix $f(V, \Lambda) = \sum_{i=1}^n h(\lambda_i) v_i v_i^\top$.
\end{proposition}
\begin{proof}
 Note that the $v_i$ come from a compact set as they are of unit norm. The $\lambda_i$ are from a compact set by assumption; this assumption holds for the normalized Laplacian, as $\lambda_i \in [0,2]$. Also, as $\mrm{diag}$ is linear, the spectral graph convolution diagonal can be written $\sum_{i=1}^n h(\lambda_i) \mrm{diag}(v_i v_i^\top)$.

    Let $\epsilon > 0$. For SignNet, let $\rho = \sum_{i=1}^n$, which can be exactly expressed as it is a permutation equivariant linear operation from vectors to vectors. Then $\phi(v_i, \lambda_i)$ can approximate the function $\lambda_i \mrm{diag}(v_i v_i^\top)$ to arbitrary precision, as it is a permutation equivariant function from vectors to vectors~\citep{segol2019universal}. Thus, letting $\phi$ approximate the function to $\epsilon/n$ accuracy, SignNet can approximate $f$ to $\epsilon$ accuracy.

    Let $l$ be the number of eigenspaces $V_1, \ldots, V_l$, so $f(V, \Lambda) = \sum_{i=1}^l h(\mu_i) V_i V_i^\top$. For BasisNet, we need only show that it can approximate the spectral graph convolution matrix to $\epsilon / l$ accuracy, as a 2-IGN can exactly express the $\mrm{diag}$ function in each $\phi_{d_i}$, since it is a linear permutation equivariant function from matrices to vectors. A 2-IGN can universally approximate the function $f_1(\mu_i, V_iV_i^\top) = (h(\mu_i), V_iV_i^\top)$, as it can express any elementwise MLP. Also, a 2-IGN can universally approximate the scalar-matrix multiplication $f_2(h(\mu_i), V_i V_i^\top) = h(\mu_i)V_i V_i^\top$ by another elementwise MLP. Since $h(\mu_i)V_i V_i^\top = f_2 \circ f_1(\mu_i, V_i V_i^\top)$, Lemma~\ref{lem:layer_universal} shows that a single 2-IGN can approximate this composition to $\epsilon/l$ accuracy, so we are done.

\end{proof}

\begin{repproposition}{prop:approximate_positional}
    SignNet and BasisNet can approximate node positional encodings based on heat kernels~\citep{feldman2022weisfeiler} and random walks~\citep{dwivedi2022graph}. BasisNet can approximate diffusion and $p$-step random walk relative positional encodings~\citep{mialon2021graphit}, and generalized PageRank and landing probability distance encodings~\citep{li2020distance}.
\end{repproposition}
\begin{proof}
    We will show that we can apply the above Proposition~\ref{prop:diag_spectral_conv}, by showing that all of these positional encodings are spectral graph convolutions. The heat kernel embeddings are of the form $\mrm{diag}\left(\sum_{i=1}^n \exp(-t\lambda_i) v_i v_i^\top \right)$ for some choices of the parameter $t$, so they can be approximated by SignNets or BasisNets. Also, the diffusion kernel~\citep{mialon2021graphit} is just the matrix of this heat kernel, and the $p$-step random walk kernel is $\sum_{i=1}^n (1-\gamma \lambda_i)^p v_i v_i^\top$ for some parameter $\gamma$, so BasisNets can universally approximate both of these.

    For the other positional encodings, we let $v_i$ be the eigenvectors of the random walk Laplacian $I - D^{-1}A$ instead of the normalized Laplacian $I - D^{-1/2}AD^{-1/2}$. The eigenvalues of these two Laplacians are the same, and if $\tilde v_i$ is an eigenvector of the normalized Laplacian then $D^{-1/2} \tilde v_i$ is an eigenvector of the random walk Laplacian with the same eigenvalue~\citep{von2007tutorial}.

    Then with $v_i$ as the eigenvectors of the random walk Laplacian, the random walk positional encodings (RWPE) in \cite{dwivedi2022graph} take the form 
    \begin{equation}
        \mrm{diag}\left((D^{-1}A)^k\right) = \mrm{diag}\left(\sum_{i=1}^n (1-\lambda_i)^k v_i v_i^\top \right),
    \end{equation}
    for any choices of integer $k$.

    The distance encodings proposed in \cite{li2020distance} take the form
    \begin{equation}
        f_3(AD^{-1}, (AD^{-1})^2, (AD^{-1})^3, \ldots),
    \end{equation}
    for some function $f_3$. We restrict to continuous $f_3$ here; shortest path distances can be obtained by a discontinuous $f_3$ that we discuss below. Their generalized PageRank based distance encodings can be obtained by 
    \begin{equation}
        \sum_{i=1}^n \left(\sum_{k \geq 1} \gamma_k  (1-\lambda_i)^k\right) v_i v_i^\top
    \end{equation}
    for some $\gamma_k \in \RR$, so this is a spectral graph convolution. They also define so-called landing probability based positional encodings, which take the form
    \begin{equation}
        \sum_{i=1}^n (1-\lambda_i)^k v_i v_i^\top,
    \end{equation}
    for some choices of integer $k$. Thus, BasisNets can approximate these distance encoding matrices. 
\end{proof}
Another powerful class of positional encodings is based on shortest path distances between nodes in the graph~\citep{ying2021transformers,li2020distance}. Shortest path distances can be expressed in a form similar to the spectral graph convolution, but require a highly discontinuous function. If we define $f_3(x_1, \ldots, x_n) = \min_{i : x_i \neq 0} i$ to be the lowest index such that $x_i$ is nonzero, then we can write the shortest path distance matrix as $f_3(D^{-1}A, (D^{-1}A)^2, \ldots, (D^{-1}A)^n)$, where $f_3$ is applied elementwise to return an $n \times n$ matrix. As $(D^{-1}A)^k = \sum_{i=1}^n (1-\lambda_i)^k v_i v_i^\top$, BasisNets can learn the inside arguments, but cannot learn the discontinuous function $f_3$.


\subsection{Spectral Invariants}\label{appendix:spectral_invariants}

Here, we consider the graph angles $\alpha_{ij} = \norm{V_i V_i^\top e_j}_2$, for $i = 1, \ldots, l$ where $l$ is the number of eigenspaces, and $j = 1, \ldots, n$. It is clear that graph angles are permutation equivariant and basis invariant. These graph angles have been extensively studied, so we cite a number of interesting properties of them. That graph angles determine the number of length 3, 4 and 5 cycles, the connectivity of a graph, and the number of length $k$ closed walks is all shown in Chapter 4 of~\cite{cvetkovic1997eigenspaces}. Other properties may be of use for graph representation learning as well. For instance, the eigenvalues of node-deleted subgraphs of a graph $\mc G$ are determined by the eigenvalues and graph angles of $\mc G$; this may be useful in extending recent graph neural networks that are motivated by node deletion and the reconstruction conjecture~\citep{cotta2021reconstruction,bevilacqua2021equivariant,papp2021dropgnn,tahmasebi2020counting}.

Now, we prove that BasisNet can universally approximate the graph angles. The graph properties we consider in the theorem are all integer valued (e.g. the number of cycles of length 3 in a graph is an integer). Thus, any two graphs that differ in these properties will differ by at least 1, so as long as we have approximation to $\varepsilon < 1/2$, we can distinguish any two graphs that differ in these properties. Recall the statement of Theorem~\ref{prop:graph_angles}.

\newtheorem*{prop:graph_angles}{Theorem~\ref{prop:graph_angles}}
\begin{prop:graph_angles}
    BasisNet can universally approximate the graph angles $\alpha_{ij}$. The eigenvalues and graph angles (and thus BasisNets) can determine the number of length 3, 4, and 5 cycles, whether a graph is connected, and the number of length $k$ closed walks from any vertex to itself.
\end{prop:graph_angles}


\begin{proof}
    Note that the graph angles satisfy 
    \begin{equation}
     \alpha_{ij} = \norm{V_i V_i^\top e_j}_2 = \sqrt{e_j^\top V_i V_i^\top V_i V_i^\top e_j} = \sqrt{e_j^\top V_i V_i^\top e_j},   
    \end{equation}
    where $V_i$ is a basis for the $i$th adjacency matrix eigenspace, and $e_j^\top V_i V_i^\top e_j$ is the $(j,j)$-entry of $V_i V_i^\top$. These graph angles are just the elementwise square roots of the diagonals of the matrices $V_i V_i^\top$. As $f_1(V_i V_i^\top) = \mrm{diag}(V_i V_i^\top)$ is a permutation equivariant linear function from matrices to vectors, 2-IGN on $V_i V_i^\top$ can exactly compute this with 0 error. Then a 2-IGN can learn an elementwise MLP to approximate the elementwise square root $f_2(\mrm{diag}(V_i V_i^\top)) = \sqrt{\mrm{diag}(V_i V_i^\top)}$ to arbitrary precision. Finally, there may be remaining operations $f_3$ that are permutation invariant or permutation equivariant from vectors to vectors; for instance, the $\alpha_{ij}$ are typically gathered into a matrix of size $l \times n$ where the columns are lexicographically sorted ($l$ is the number of eigenspaces)~\citep{cvetkovic1997eigenspaces}, or we may have a permutation invariant readout to compute a subgraph count. A DeepSets can approximate $f_3$ without any higher order tensors besides vectors~\citep{zaheer2017deep,segol2019universal}.

    As 2-IGNs can approximate each $f_i$ individually, a single 2-IGN can approximate $f_3 \circ f_2 \circ f_1$ by Lemma~\ref{lem:layer_universal}. 
    Also, since the graph properties considered in the theorem are integer-valued, BasisNet can distinguish any two graphs that differ in one of these properties.
\end{proof}

To see that message passing graph neural networks (MPNNs) cannot determine these quantities, we use the fact that MPNNs cannot distinguish between two graphs that have the same number of nodes and where each node (in both graphs) has the same degree. For $k \geq 3$, let $C_k$ denote the cycle graph of size $k$, and $C_k + C_k$ denote the graph that is the union of two disjoint cycle graphs of size $k$. MPNNs cannot distinguish between $C_{2k}$ and $C_k + C_k$ for $k \geq 3$, because they have the same number of nodes, and each node has degree 2. Thus, MPNNs cannot tell whether a graph is connected, as $C_{2k}$ is but $C_k + C_k$ is not. Also, it cannot count the number of 3, 4, or 5 cycles, as $C_k + C_k$ has two $k$ cycles while $C_{2k}$ has no $k$ cycles. Likewise, any node in $C_k + C_k$ has more length $k$ closed walks than any node in $C_{2k}$. This is because any length $k$ closed walk in $C_{2k}$ has an analogous closed walk in $C_k + C_k$, but the nodes in $C_k + C_k$ also have a closed walk that completely goes around a cycle.





\section{Useful Lemmas}

In this section, we collect useful lemmas for our proofs. These lemmas generally only require basic tools to prove.  Our first lemma is a crucial property of quotient spaces.

\begin{lemma}[Passing to the quotient]\label{lem:pass_to_quotient}
    Let $\X$ and $\Y$ be topological spaces, and let $\X/G$ be a quotient space, with corresponding quotient map $\pi$. Then for every continuous $G$-invariant function $f: \X \to \Y$, there is a unique continuous $\tilde f: \X/G \to \Y$ such that
    $f = \tilde f \circ \pi$.
\end{lemma}
\begin{proof}
    For $z \in \X/G$, by surjectivity of $\pi$ we can choose an $x_z \in \X$ such that $\pi(x_z) = z$. Define $\tilde f: \X/G \to \Y$ by $\tilde f(z) = f(x_z)$. This is well-defined, since if $\pi(x_z) = \pi(x)$ for any other $x \in \X$, then $g x_z = x$ for some $g \in G$, so 
    \begin{equation}
        f(x) = f(gx_z) = f(x_z) = \tilde f(z),
    \end{equation}
    where the second equality uses the $G$-invariance of $f$. Note that $\tilde f$ is continuous by the universal property of quotient spaces. Also, $\tilde f$ is the  unique function such that $f = \tilde f \circ \pi$; if there were another function $h: \X/G \to \Y$ with $h(z) \neq \tilde f(z)$, then $h(z) \neq f(x_z)$, so $h(\pi(x_z)) = h(z) \neq f(x_z)$.
\end{proof}

Next, we give the First Fundamental Theorem of $O(d)$, a classical result that has been recently used for machine learning by \cite{villar2021scalars}. This result shows that an orthogonally invariant $f(V)$ can be expressed as a function $h(VV^\top)$. We give a proof that if $f$ is continuous, then $h$ is also continuous.
\begin{lemma}[First Fundamental Theorem of $O(d)$]\label{lem:first_fund}
    A continuous function $f: \RR^{n \times d} \to \RR^{\dout}$ is orthogonally invariant, i.e. $f(VQ) = f(V)$ for all $Q \in O(d)$, if and only if $f(V) = h(VV^\top)$ for some continuous $h$.
\end{lemma}
\begin{proof}
    If $f(V) = h(VV^\top)$, then we have $f(VQ) = h(VQQ^\top V^\top) = h(VV^\top)$ so $f$ is orthogonally invariant.

    For the other direction, invariant theory shows that the $O(d)$ invariant polynomials are generated by the inner products $v_i^\top v_j$, where $v_i \in \RR^d$ are the rows of $V$~\citep{kraft1996classical}. Let $p: \RR^{n \times d} \to \RR^{n \times n}$ be the map $p(V) = VV^\top$. Then \cite{gonzalez2003c} Lemma 11.13 shows that the quotient space $\RR^{n \times d} / O(d)$ is homeomorphic to a closed subset $p(\RR^{n \times d}) = \mc Z \subseteq \RR^{n \times n}$. Let $\tilde p$ refer to this homeomorphism, and note that $\tilde p \circ \pi = p$ by passing to the quotient (Lemma~\ref{lem:pass_to_quotient}). Then any continuous $O(d)$ invariant $f$ passes to a unique continuous $\tilde f: \RR^{n \times d} / O(d) \to \RR^{\dout}$ (Lemma~\ref{lem:pass_to_quotient}), so $f = \tilde f \circ \pi$ where $\pi$ is the quotient map. Define $h : \mc Z \to \RR^{\dout}$ by $h = \tilde f \circ \tilde p^{-1}$, and note that $h$ is a composition of continuous functions and hence continuous. Finally, we have that $h(VV^\top) = h(\tilde p \circ \pi(V)) = \tilde f \circ \pi(V) = f(V)$, so we are done.
\end{proof}

The next lemma allows us to decompose a quotient of a product space into a product of smaller quotient spaces.

\begin{lemma}\label{lem:product_quotient}
    Let $\mc X_1, \ldots, \mc X_k$ be topological spaces and $G_1, \ldots, G_k$ be topological groups such that each $G_i$ acts continuously on $\mc X_i$. Denote the quotient maps by $\pi_i: \mc X_i \to \mc X_i / G_i$. Then the quotient of the product is the product of the quotient, i.e.
    \begin{equation}\label{eq:product_quotient}
        (\mc X_1 \times \ldots \times \mc X_k) / (G_1 \times \ldots \times G_k) \cong (\mc X_1 / G_1) \times \ldots \times (\mc X_k / G_k),
    \end{equation}
    and $\pi_1 \times \ldots \times \pi_k: \mc X_1 \times \ldots \mc X_k \to (\mc X_1 / G_1) \times \ldots \times (\mc X_k / G_k)$  is quotient map.
\end{lemma}
\begin{proof}
    First, we show that $\pi_1 \times \ldots \times \pi_k$ is a quotient map. This is because 1. the quotient map of any continuous group action is an open map, so each $\pi_i$ is an open map, 2. the product of open maps is an open map, so $\pi_1 \times \ldots \times \pi_k$ is an open map and 3. a continuous surjective open map is a quotient map, so $\pi_1 \times \ldots \times \pi_k$, which is continuous and surjective, is a quotient map.

    Now, we need only apply the theorem of uniqueness of quotient spaces to show \eqref{eq:product_quotient} (see e.g. \cite{lee2013smooth}, Theorem A.31). Letting $q: \mc X_1 \times \ldots \times \mc X_k \to (\mc X_1 \times \ldots \times \mc X_k) / (G_1 \times \ldots \times G_k)$ denote the quotient map for this space, it is easily seen that $q(x_1, \ldots, x_k) = q(y_1 \ldots, y_k) $ if and only if $\pi_1 \times \ldots \times \pi_k(x_1, \ldots, x_k) = \pi_1 \times \ldots \times \pi_k(y_1, \ldots, y_k)$, since either of these is true if and only if there exist $g_i \in G_i$ such that $x_i = g_i y_i$ for each $i$. Thus, we have an isomorphism of these quotient spaces.
\end{proof}

The following lemma shows that quotients of compact spaces are also compact, which is useful for universal approximation on quotient spaces.

\begin{lemma}[Compactness of quotients of compact spaces]\label{lem:quotient_compact}
    Let $\X$ be a compact space. Then the quotient space $\X/G$ is compact.
\end{lemma}
\begin{proof}
    Denoting the quotient map by $\pi: \X \to \X/G$ and letting $\{U_\alpha\}_\alpha$ be an open cover of $\X/G$, we have that $\{\pi^{-1}(U_\alpha)\}_\alpha$ is an open cover of $\X$. By compactness of $\X$, we can choose a finite subcover $\{\pi^{-1}(U_{\alpha_i})\}_{i=1, \ldots, n}$. Then $\{\pi(\pi^{-1}(U_{\alpha_i}))\}_{i=1, \ldots, n} = \{U_{\alpha_i}\}_{i=1, \ldots, n}$ by surjectivity, and $\{U_{\alpha_i}\}_{i=1, \ldots, n}$ is thus an open cover of $\mc X/G$.
\end{proof}

The Whitney embedding theorem gives a nice condition that we apply to show that the quotient spaces $\mc X / G$ that we deal with embed into Euclidean space. It says that when $\mc X / G$ is a smooth manifold, then it can be embedded into a Euclidean space of double the dimension of the manifold. The proof is outside the scope of this paper.
\begin{lemma}[Whitney Embedding Theorem \citep{whitney1944self}]\label{lem:whitney}
    Every smooth manifold $\mc M$ of dimension $n > 0$ can be smoothly embedded in $\RR^{2n}$.
\end{lemma}

Finally, we give a lemma that helps prove universal approximation results. It says that if functions $f$ that we want to approximate can be written as compositions $f = f_L \circ \ldots \circ f_1$, then it suffices to universally approximate each $f_i$ and compose the results to universally approximate the $f$. This is especially useful for proving universality of neural networks, as we may use some layers to approximate each $f_i$, then compose these layers to approximate the target function $f$.

\begin{lemma}[Layer-wise universality implies universality]\label{lem:layer_universal}
    Let $\mc Z \subseteq \RR^{d_0}$ be a compact domain, let $\mc F_1, \ldots, \mc F_L$ be families of continuous functions where $\mc F_i$ consists of functions from $\RR^{d_{i-1}} \to \RR^{d_i}$ for some $d_1, \ldots, d_L$. Let $\mc F$ be the family of functions $\{f_L \circ \ldots f_1: \mc Z \to \RR^{d_L}, f_i \in \mc F_i\}$ that are compositions of functions $f_i \in \mc F_i$.

    For each $i$, let $\Phi_i$ be a family of continuous functions that universally approximates $\mc F_i$. Then the family of compositions $\Phi = \{\phi_L \circ \ldots \circ \phi_1 : \phi_i \in \Phi_i  \}$ universally approximates $\mc F$.
\end{lemma}
\begin{proof}
    Let $f = f_L \circ \ldots \circ f_1 \in \mc F$. Let $\tilde{\mc Z_1} = \mc Z$, and then for $i \geq 2$ let $\tilde{\mc Z_i} = f_{i-1}(\tilde{\mc Z}_{i-1})$. Then each $\tilde{\mc Z_i}$ is compact by continuity of the $f_i$. For $1 \leq  i < L$, let $\mc Z_i = \tilde{\mc Z_i}$, and for $i = L$ let $\mc Z_L$ be a compact set containing $\tilde{\mc Z_L}$ such that every ball of radius one centered at a point in $\tilde{\mc Z_L}$ is still contained in $\mc Z_L$.

Let $\epsilon > 0$. We will show that there is a $\phi \in \Phi$ such that $\norm{f - \phi}_\infty < \epsilon$ by induction on $L$. This holds trivially for $L=1$, as then $\Phi = \Phi_1$.

Now, let $L \geq 2$, and suppose it holds for $L-1$. By universality of $\Phi_L$, we can choose a $\phi_L: \mc Z_L \to \RR^{d_L} \in \Phi_L$ such that $\norm{\phi_L - f_L}_\infty < \epsilon/2$. As $\phi_L$ is continuous on a compact domain, it is also uniformly continuous, so we can choose a $\tilde{\delta} > 0$ such that $\norm{y-z}_2 < \tilde{\delta} \implies \norm{\phi_L(y) - \phi_L(z)}_2 < \epsilon / 2$.

Let $\delta = \min(\tilde \delta, 1)$. By induction, we can choose $\phi_{L-1} \circ \ldots \circ \phi_1, \phi_i \in \Phi_i$ such that 
\begin{equation}
\norm{\phi_{L-1} \circ \ldots \circ \phi_1 - f_{L-1} \circ \ldots \circ f_1}_\infty < \delta.
\end{equation}
Note that $\phi_{L-1} \circ \ldots \circ \phi_1(\mc Z) \subseteq \mc Z_L$, because for each $x \in \mc Z$, $\phi_{L-1} \circ \ldots \circ \phi_1(x)$ is within $\delta \leq 1$ Euclidean distance to $f_{L-1} \circ \ldots \circ f_1(x) \in \tilde{\mc Z_L}$, so it is contained in $\mc Z_L$ by construction. Thus, we may define $\phi = \phi_L \circ \ldots \circ \phi_1: \mc Z \to \RR^{d_L}$, and compute that
\begin{align}
    \norm{\phi - f}_\infty & \leq \norm{\phi - \phi_L \circ f_{L-1} \circ \ldots \circ f_1}_\infty + \norm{\phi_L \circ f_{L-1}\circ \ldots \circ f_1 - f}_\infty \\
& < \norm{\phi - \phi_L \circ f_{L-1} \circ \ldots \circ f_1}_\infty + \epsilon / 2,
\end{align}
since $\norm{\phi_L - f_L}_\infty < \epsilon / 2$. To bound this other term, let $x \in \mc Z$, and for $y = \phi_{L-1} \circ \ldots \circ \phi_1(x)$ and $z = f_{L-1} \circ \ldots \circ f_1(x)$, we know that $\norm{y-z}_2 < \delta$, so $\norm{\phi_L(y) - \phi_L(z)}_2 < \epsilon/2$ by uniform continuity. As this holds for all $x$, we have $\norm{\phi - \phi_L \circ f_{L-1} \circ \ldots \circ f_1}_\infty \leq \epsilon / 2$, so $\norm{\phi - f}_\infty < \epsilon$ and we are done.
\end{proof}


\section{Further Experiments}

\subsection{Graph Regression with no Edge Features}\label{appendix:no_edge_features}

\begin{table}[ht]
    {\small
    \centering
    \caption{Results on the ZINC dataset with 500k parameter budget and no edge features. Numbers are the mean and standard deviation over 4 runs each with different seeds.}
    \label{tab:zinc_no_edge}
    \begin{center}
    \begin{tabular}{lrrrr}
        \toprule
        Base model
        & \multicolumn{1}{c}{Positional encoding} & \multicolumn{1}{c}{$k$} &  \multicolumn{1}{c}{\#params} & 
        \multicolumn{1}{c}{Test MAE ($\downarrow$)} \\
        \midrule
        \multirow{3}{*}{GIN} & No PE & $16$ & $497$k & $0.348_{\pm 0.014}$ \\
         & LapPE (flip) & $16$ & $498$k & $0.341_{\pm 0.011}$ \\
         & SignNet & $16$ & $500$k & $\mathbf{0.238_{\pm 0.012}}$ \\
        \midrule
        \multirow{3}{*}{GAT} & No PE &  $16$ & $501$k & $0.464_{\pm 0.011}$ \\
         & LapPE (flip) &$16$ & $502$k & $0.462_{\pm 0.013}$ \\
         & SignNet &  $16$ & $499$k & $\mathbf{0.243_{\pm 0.008}}$ \\
        \bottomrule
    \end{tabular}
    \end{center}
}
\end{table}


All graph regression models in Table \ref{tab:zinc} use edge features for learning and inference. To show that SignNet is also useful when no edge features are available, we ran ZINC experiments without edge features as well. The results are displayed in Table \ref{tab:zinc_no_edge}. In this setting, SignNet still significantly improves the performance over message passing networks without positional encodings, and over Laplacian positional encodings with sign flipping data augmentation.

\subsection{Comparison with Domain Specific Molecular Graph Regression Models}

\begin{table}[ht]
    \centering
    \caption{Comparison with domain specific methods on graph-level regression tasks.
    Numbers are test MAE, so lower is better. Best models within a standard deviation are bolded.}
    \label{tab:domain_specific}
    {\small
    \begin{tabular}{llll}
    \toprule
         & ZINC (10K) $\downarrow$ & ZINC-full $\downarrow$ \\
         \midrule
         HIMP $\dagger$~\citep{fey2020hierarchical} & .151\std{.006} & .036\std{.002}\\
         CIN-small $\dagger$~\citep{bodnar2021weisfeiler} & .094\std{.004} & .044\std{.003}\\
         CIN $\dagger$~\citep{bodnar2021weisfeiler} & \textbf{.079\std{.006}} & \textbf{.022\std{.002}} \\
         \midrule
         SignNet (ours) & \textbf{.084\std{.006}} & \textbf{.024\std{.003}} \\
         \bottomrule
    \end{tabular}
    }
\end{table}

In Table~\ref{tab:domain_specific}, we compare our model against methods that have domain-specific information about molecules built into them:  HIMP~\citep{fey2020hierarchical} and CIN~\citep{bodnar2021weisfeiler}. We see that SignNet is better than HIMP and CIN-small on these tasks, and is within a standard deviation of CIN. The SignNet models are the same as the ones reported in Table~\ref{tab:sota_graph}. Once again, we emphasize that SignNet is domain-agnostic.

\subsection{Learning Spectral Graph Convolutions}\label{sec:spectral_conv_exp}



\begin{table}[ht]
    \centering
    {\small
    \caption{Sum of squared errors for spectral graph convolution regression (with no test set). Lower is better. Numbers are mean and standard deviation over 50 images from~\cite{he2021bernnet}.}
    \label{tab:spectral_conv}
    \begin{tabular}{lrrrrr}
        \toprule
        & \multicolumn{1}{c}{Low-pass} & \multicolumn{1}{c}{High-pass} & \multicolumn{1}{c}{Band-pass} & \multicolumn{1}{c}{Band-rejection} & \multicolumn{1}{c}{Comb}\\
        \midrule
        GCN & .111\std{.068} & 3.092\std{5.11} & 1.720\std{3.15} & 1.418\std{1.03} & 1.753\std{1.17} \\
        GAT & .113\std{.065} & .954\std{.696}  & 1.105\std{.964} & .543\std{.340} & .638\std{.446}  \\
        GPR-GNN & .033\std{.032} & .012\std{.007} & .137\std{.081} & .256\std{.197} & .369\std{.460} \\
        ARMA & .053\std{.029} & .042\std{.024} & .107\std{.039} & .148\std{.089} & .202\std{.116}  \\
        ChebNet & .003\std{.002}  & \textbf{.001}\std{.001} & .005\std{.003} & .009\std{.006} & .022\std{.016} \\
        BernNet & \textbf{.001}\std{.002} & \textbf{.001}\std{.001} & \textbf{.000}\std{.000}  & .048\std{.042}  & .027\std{.019} \\
        \midrule
        Transformer & 3.662\std{1.97} & 3.715\std{1.98} & 1.531\std{1.30} & 1.506\std{1.29} & 3.178\std{1.93} \\
        Transformer Eig Flip & 4.454\std{2.32} & 4.425\std{2.38} & 1.651\std{1.53} & 2.567\std{1.73} & 3.720\std{1.94} \\
        Transformer Eig Abs & 2.727\std{1.40} & 3.172\std{1.61} & 1.264\std{.788} & 1.445\std{.943} & 2.607\std{1.32} \\
        \midrule
        DeepSets SignNet & .004\std{.013} & .086\std{.405} & .021\std{.115} & .008\std{.037} &  \textbf{.003}\std{.016} \\
        Transformer SignNet & .003\std{.016} & .004\std{.025} & .001\std{.004}  & .006\std{.023} & .093\std{.641} \\
        DeepSets BasisNet & .009\std{.018} & .003\std{.015} & .008\std{.030} & \textbf{.004}\std{.011} & .015\std{.060} \\
        Transformer BasisNet & .079\std{.471} & .014\std{.038} & .005\std{.018} & .006\std{.016}  & .014\std{.051} \\
        \bottomrule
    \end{tabular}
}
\end{table}

To numerically test the ability of our basis invariant networks for learning spectral graph convolutions, we follow the experimental setups of \cite{balcilar2020analyzing, he2021bernnet}. We take the dataset of 50 images in \cite{he2021bernnet} (originally from the Image Processing Toolbox of \textsc{Matlab}), and resize them from 100$\times$100 to 32$\times$32. Then we apply the same spectral graph convolutions on them as in \cite{he2021bernnet}, and train neural networks to learn these as regression targets. As in prior work, we report sum of squared errors on the training set to measure expressivity.

We compare against message passing GNNs~\citep{kipf2016semi,velivckovic2018graph} and spectral GNNs~\citep{chien2021adaptive,bianchi2021graph,defferrard2016convolutional,he2021bernnet}. Also, we consider standard Transformers with only node features, with eigenvectors and sign flip augmentation, and with absolute values of eigenvectors. These models are all approximately sign invariant (they either use eigenvectors in a sign invariant way or do not use eigenvectors). We use DeepSets~\citep{zaheer2017deep} in SignNet and 2-IGN~\citep{maron2018invariant} in BasisNet for $\phi$, use a DeepSets for $\rho$ in both cases, and then feed the features into another DeepSets or a standard Transformer~\citep{vaswani2017attention} to make the final predictions. That is, we are only given graph information through the eigenvectors and eigenvalues, and we do not use message passing.

Table~\ref{tab:spectral_conv} displays the results, which validate our theoretical results in Section~\ref{sec:spectral_conv}. Without any message passing, SignNet and BasisNet allow DeepSets and Transformers to perform strongly, beating the spectral GNNs GPR-GNN and ARMA on all tasks. Also, our networks outperform all other methods on the band-rejection and comb filters, and are mostly close to the best model on the other filters. 

\section{Further Experimental Details}

\subsection{Hardware, Software, and Data Details}\label{appendix:other_exp_details}

All experiments could fit on one GPU at a time. Most experiments were run on a server with 8 NVIDIA RTX 2080 Ti GPUs. We run all of our experiments in Python, using the PyTorch~\citep{paszke2019pytorch} framework (\href{https://github.com/pytorch/pytorch/blob/master/LICENSE}{license URL}). We also make use of Deep Graph Library (DGL)~\citep{wang2019deep} (Apache License 2.0), and PyTorch Geometric (PyG)~\citep{fey2019fast} (MIT License) for experiments with graph data.

The data we use are all freely available online. The datasets we use are 
ZINC~\citep{irwin2012zinc},
Alchemy~\citep{chen2019alchemy},
the synthetic counting substructures dataset~\citep{chen2020can},
the multi-task graph property regression synthetic dataset~\citep{corso2020principal} (MIT License),
the images dataset used by \citet{balcilar2020analyzing} (GNU General Public License v3.0),
the cat mesh from \url{free3d.com/3d-model/cat-v1--522281.html} (Personal Use License),
and the human mesh from \url{turbosquid.com/3d-models/water-park-slides-3d-max/1093267} (\href{https://blog.turbosquid.com/turbosquid-3d-model-license/}{TurboSquid 3D Model License}). If no license is listed, this means that we cannot find a license for the dataset. As they appear to be freely available with permissive licenses or no licenses, we do not ask for permission from the creators or hosts of the data.

We do not believe that any of this data contains offensive content or personally identifiable information. The 50 images used in the spectral graph convolution experiments are mostly images of objects, with a few low resolution images of humans that do not appear to have offensive content. The only other human-related data appears to be the human mesh, which appears to be from a 3D scan of a human. 

\subsection{Graph Regression Details}\label{appendix:graph_regression}

\textbf{ZINC.} In Section \ref{sec:graph_regression} we study the effectiveness of SignNet for learning positional encodings to boost the expressive power, and thereby generalization, on the graph regression problem ZINC. In all cases we take our $\phi$ encoder to be an $8$ layer GIN with ReLU activation. The input eigenvector $v_i \in \mathbb{R}^n$, where $n$ is the number of nodes in the graph, is treated as a single scalar feature for each node. In the case of using a fixed number of eigenvectors $k$, the aggregator $\rho$ is taken to be an $8$ layer MLP with batch normalization and ReLU activation. The aggregator  $\rho$ is applied separately to the concatenatation of the $k$ different embeddings for each node in a graph, resulting in one single embedding per node. This embedding  is concatenated to the node features for 
that node, and the result passed as input to the base (predictor) model. We also consider using all available eigenvectors in each graph instead of a fixed number $k$. Since the total number of eigenvectors is a variable quantity, equal to the number of  nodes in the underlying graph, an MLP cannot be used for $\rho$. 
To handle the variable sized input in this case, we take $\rho$ to be an MLP preceded by a sum over the $\phi$ outputs. In other words, the SignNet is of the form $\mrm{MLP}\left(\sum_{i=1}^k \phi(v_i) + \phi(-v_i) \right)$ in this case.

As well as testing SignNet,  we also checked whether simple transformations that resolve the sign ambiguity of the Laplacian eigenvectors $p=(v_1, \ldots , v_k)$ could serve as effective positional encoding. We considered three options. First is to randomly flip the sign of each $\pm v_i$ during training. This is a common heuristic used in prior work on Laplacian positional encoding \citep{kreuzer2021rethinking,dwivedi2020benchmarking}. Second,  take the element-wise absolute value $|v_i|$. This is a non-injective map, creating sign invariance at the cost of destroying positional information. Third is a different canonicalization that avoids stochasticity and use of  absolute values by selecting the sign of each $v_i$ so that the majority of entries are non-negative, with ties broken by comparing the $\ell_1$-norm of positive and negative parts. When the tie-break also fails, the sign is chosen randomly. Results for GatedGCN base model on ZINC in Table \ref{tab:zinc} show that all three of these approaches are  significantly  poorer positional encodings compared to SignNet. 

Our training pipeline largely follows that of \cite{dwivedi2022graph}, and we use the GatedGCN and PNA base models from the accompanying implementation (see \url{https://github.com/vijaydwivedi75/gnn-lspe}). The Sparse Transformer base model architecture we use, which like GAT computes attention only across neighbouring nodes, is introduced by \cite{kreuzer2021rethinking}. Finally, the GINE implementation is based on the PyTorch Geometric implementation \citep{fey2019fast}. For the state-of-the-art comparison, all baseline results are from their respective papers, except for GIN, which we run.

\textbf{ZINC-full.} We also run our method on the full ZINC dataset, termed ZINC-full. The result we report for SignNet is a larger version of the GatedGCN base model with a SignNet that takes in all eigenvectors. This model has 994,113 parameters in total. All baseline results are from their respective papers, except for GIN, which is from~\citep{bodnar2021weisfeiler}.

\textbf{Alchemy.} We run our method and compare with the state-of-the-art on Alchemy (with 10,000 training graphs). We use the same data split as \citet{morris2020weisfeiler}.  Our base model is a GIN that takes in edge features (i.e. a GINE). The SignNet consists of GIN for $\phi$ and a Transformer for $\rho$, as in the counting substructures and graph property regression experiments in Section~\ref{sec:count_substruct}. The model has 907,371 parameters in total. Our training setting is very similar to that of \cite{morris2022speqnets}, as we build off of their code. We train with an Adam optimizer~\citep{kingma2014adam} with a starting learning rate of .001, and a minimum learning rate of .000001. The learning rate schedule cuts the learning rate in half with a patience of 20 epochs, and training ends when we reach the minimum learning rate. All baseline results are from their respective papers, except for GIN, which is from~\citep{morris2022speqnets}.

\subsection{Spectral Graph Convolution Details}

In Appendix~\ref{sec:spectral_conv_exp}, we conduct node regression experiments for learning spectral graph convolutions. The experimental setup is mostly taken from \cite{he2021bernnet}. However, we resize the $100 \times 100$ images to $32 \times 32$. Thus, each image is viewed as a $1024$-node graph. The node features $X \in \RR^n$ are the grayscale pixel intensities of each node. Just as in \cite{he2021bernnet}, we only train and evaluate on nodes that are not connected to the boundary of the grid (that is, we only evaluate on the $28 \times 28$ middle section). For all experiments we limit each model to 50,000 parameters. We use the Adam~\citep{kingma2014adam} optimizer for all experiments. For each of the GNN baselines (GCN, GAT, GPR-GNN, ARMA, ChebNet, BernNet), we select the best performing out of 4 hyperparameter settings: either 2 or 4 convolution layers, and a hidden dimension of size 32 or $D$, where $D$ is just large enough to stay with 50,000 parameters (for instance, $D = $ 128 for GCN, GPR-GNN, and BernNet).

We use DeepSets or standard Transformers as our prediction network. This takes in the output of SignNet or BasisNet and concatenates it with the node features, then outputs a scalar prediction for each node. We use a 3 layer output network for DeepSets SignNet, and 2 layer output networks for all other configurations. All networks use ReLU activations.

For SignNet, we use DeepSets for both $\phi$ and $\rho$. Our $\phi$ takes in eigenvectors only, then our $\rho$ takes the outputs of $\phi$ and the eigenvalues. We use three layers for $\phi$ and $\rho$.

For BasisNet, we use the same DeepSets for $\rho$ as in SignNet, and 2-IGNs for the $\phi_{d_i}$. There are three distinct multiplicities for the grid graph (1, 2, and 32), so we only need 3 separate IGNs. Each IGN consists of an $\RR^{n^2 \times 1} \to \RR^{n \times d'}$ layer and two $\RR^{n \times d''} \to \RR^{n \times d'''}$ layers, where the $d'$ are hidden dimensions. There are no matrix to matrix operations used, as the memory requirements are intensive for these $\geq 1000$ node graphs. The $\phi_{d_i}$ only take in $V_i V_i^\top$ from the eigenspaces, and the $\rho$ takes the output of the $\phi_{d_i}$ as well as the eigenvalues.



\subsection{Substructures and Graph Properties Regression Details}\label{sec:appdx count}
We use the random graph dataset from \cite{chen2020can} for counting substructures and the synthetic dataset from \cite{corso2020principal} for regressing graph properties. For fair comparison we fix the base model as a 4-layer GIN model with hidden size 128.
We choose $\phi$ as a 4-layer GIN (independently applied to every eigenvector) and $\rho$ as a 1-layer Transformer (independently applied to every node). Combined with proper batching and masking, we have a SignNet that takes Laplacian eigenvectors $V \in \RR^{n \times n}$ and outputs fixed size sign-invariant encoding node features $f(V, \Lambda, X) \in \RR^{n \times d}$, where $n$ varies between graphs but $d$ is fixed. We use this SignNet in our experiments and compare with other methods of handling PEs.

\subsection{Texture Reconstruction Details}\label{appendix:texture}

\begin{table}[ht]
    \centering
    \caption{Parameter settings for the texture reconstruction experiments.}
    {\small
    \begin{tabular}{lcccccc}
    \toprule
         & Params & Base MLP width & Base MLP layers & $\phi$ out dim & $\rho$ out dim & $\rho$, $\phi$ width \\
        \midrule
         Intrinsic NF & 328,579 & 128 & 6 &---&--- & ---\\
         SignNet & 323,563 & 108 & 6 & 4 & 64 & 8 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:texture_parameters}
\end{table}

We closely follow the experimental setting of \cite{koestler2022intrinsic} for the texture reconstruction experiments. In this work, we use the cotangent Laplacian~\citep{rustamov2007laplace} of a triangle mesh with the lowest $1023$ eigenvectors besides the trivial eigenvector of eigenvalue 0. We implemented SignNet in the authors' original code, which was privately shared with us. Both $\rho$ and $\phi$ are taken to be MLPs. Hyperparameter settings and number of parameters are given in Table~\ref{tab:texture_parameters}. We chose hyperparameters so that the total number of  parameters in the SignNet model was no larger than that of the original model.

\end{document}
