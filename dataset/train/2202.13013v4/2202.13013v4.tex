
\documentclass{article} \usepackage{iclr2023_conference,times}

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{xcolor}         \definecolor{darkblue}{rgb}{0.0,0.0,0.65}
\definecolor{darkred}{rgb}{0.68,0.05,0.0}
\definecolor{darkgreen}{rgb}{0.0,0.29,0.29}
\definecolor{darkpurple}{rgb}{0.47,0.09,0.29}
\hypersetup{
   colorlinks = true,
   citecolor  = darkblue,
   linkcolor  = darkred,
   filecolor  = darkblue,
   urlcolor   = darkblue,
 }

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      


\usepackage{wrapfig}
\usepackage{makecell} \usepackage{multirow} \usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath,bm,amsthm}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{listings}
\usepackage{verbatim}

\newcommand{\QQ}{\mathbb Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\CC}{\mathbb C}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\PP}{\mathbb P}
\newcommand{\EE}{\mathbb E}
\renewcommand{\SS}{\mathbb S}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO #1}}
\newcommand{\st}{\mrm{St}}
\newcommand{\gr}{\mrm{Gr}}
\renewcommand{\O}{O}
\newcommand{\std}[1]{}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand{\RN}[1]{\textup{\uppercase\expandafter{\romannumeral#1}}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\dout}{d_{\mrm{out}}}
\newcommand{\dfeat}{d_{\mrm{feat}}}

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{\newenvironment{rep#1}[1]{\def\rep@title{#2 \ref{##1}}\begin{rep@theorem}}{\end{rep@theorem}}}
\makeatother


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newreptheorem{proposition}{Proposition}


\newcommand{\sj}[1]{\textcolor{green}{[SJ: #1]}}
\newcommand{\dl}[1]{\textcolor{purple}{[DL: #1]}}
\newcommand{\rebut}[1]{\textcolor{black}{#1}} 



\title{Sign and Basis Invariant Networks for\\ Spectral Graph Representation Learning}


\author{Derek Lim, Joshua Robinson\thanks{Equal contribution.}\\
MIT CSAIL\\
\texttt{\{dereklim, joshrob\}@mit.edu}
\And Lingxiao Zhao  \\
Carnegie Mellon University\\
\And Tess Smidt\\
MIT EECS \& MIT RLE\\
\And
Suvrit Sra \\
MIT LIDS\\
\And
Haggai Maron \\
NVIDIA Research \\
\And Stefanie Jegelka \\
MIT CSAIL
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We introduce SignNet and BasisNet---new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if  is an eigenvector then so is ; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. 
When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases.
Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes.
Our code is available at 
\url{https://github.com/cptq/SignNet-BasisNet}.
\end{abstract}

\section{Introduction}


Numerous machine learning models process eigenvectors, which arise in various settings including principal component analysis, matrix factorizations, and operators associated to graphs or manifolds. An important example is the use of  Laplacian eigenvectors to encode information about the structure of a graph or manifold \citep{belkin2003laplacian,von2007tutorial,levy2006laplace}.
Positional encodings that involve Laplacian eigenvectors have recently been used to generalize Transformers to graphs \citep{kreuzer2021rethinking,dwivedi2020generalization}, and to improve the expressive power and empirical performance of graph neural networks (GNNs) \citep{dwivedi2022graph}. Furthermore, these eigenvectors are crucial for defining spectral operations on graphs that are foundational to graph signal processing and spectral GNNs~\citep{ortega2018graph, bruna2014spectral}.

However, there are nontrivial symmetries that should be accounted for when processing eigenvectors, as has been noted in many fields~\citep{eastment1982cross, rustamov2007laplace, bro2008resolving, ovsjanikov2008global}. For instance, if  is an eigenvector, then so is , with the same eigenvalue. More generally, if an eigenvalue has higher multiplicity, then there are infinitely many unit-norm eigenvectors that can be chosen. Indeed, a full set of linearly independent eigenvectors is only defined up to a change of basis in each eigenspace. 
In the case of sign invariance, for any  eigenvectors there are  possible choices of sign. Accordingly, prior works on graph positional encodings randomly flip eigenvector signs during training in order to approximately learn sign invariance \citep{kreuzer2021rethinking, dwivedi2020benchmarking, kim2022pure}.  However, learning all  invariances is challenging and limits the effectiveness of Laplacian eigenvectors for encoding positional information. Sign invariance is a special case of basis invariance when all eigenvalues are distinct, but general basis invariance is even more difficult to deal with.
In Appendix~\ref{appendix:higher_dim}, we show that higher dimensional eigenspaces are abundant in real datasets; for instance, 64\% of molecule graphs in the ZINC dataset have a higher dimensional eigenspace.


In this work, we address the sign and basis ambiguity problems by developing new neural networks---SignNet and BasisNet.
Under certain conditions, our networks are universal and can approximate any continuous function of eigenvectors with the proper invariances.
Moreover, our networks are theoretically powerful for graph representation learning---they can provably approximate and go beyond both spectral graph convolutions and powerful spectral invariants, which allows our networks to express graph properties like subgraph counts that message passing neural networks cannot. Laplacian eigenvectors with SignNet and BasisNet can provably approximate many previously proposed graph positional encodings, so our networks are general and remove the need for choosing one of the many positional encodings in the literature.
Experiments on molecular graph regression tasks, learning expressive graph representations, and texture reconstruction on triangle meshes illustrate the empirical benefits of our models' approximation power and invariances. 

\section{Sign and Basis Invariant Networks}
\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-10pt}
\centering
    \includegraphics[width=0.5\textwidth]{figs/LPE_symmetries.pdf}
\caption{Symmetries of eigenvectors of a symmetric matrix with permutation invariances (e.g. a graph Laplacian). A neural network applied to the eigenvector matrix (middle) should be invariant or equivariant to permutation of the rows (left product with a permutation matrix ) and invariant to the choice of eigenvectors in each eigenbasis (right product with a block diagonal orthogonal matrix ).}
\label{fig:symmetries}
\vspace{-15pt}
\end{wrapfigure}

For an  symmetric matrix, let  be the eigenvalues and  the corresponding eigenvectors, which we may assume to form an orthonormal basis. For instance, we could consider the normalized graph Laplacian , where  is the adjacency matrix and  is the diagonal degree matrix  of some underlying graph. For undirected graphs,  is symmetric. Nonsymmetric matrices can be handled very similarly, as we show in Appendix~\ref{appendix:nonsymmetric}.

\textbf{Motivation.} Our goal is to parameterize a class of models   taking  eigenvectors as input in a manner that respects the eigenvector symmetries. 
This is because eigenvectors capture much information about data; Laplacian eigenvectors of a graph capture clusters, subgraph frequencies, connectivity, and many other useful properties~\citep{von2007tutorial, cvetkovic1997eigenspaces}.

A major motivation for processing eigenvector input is for graph positional encodings, which are additional features appended to each node in a graph that give information about the position of that node in the graph. These additional features are crucial for generalizing Transformers to graphs, and also have been found to improve performance of GNNs~\citep{dwivedi2020benchmarking, dwivedi2022graph}. Figure~\ref{fig:signnet_diagram} illustrates a standard pipeline and the use of our SignNet within it: the input adjacency, node features, and eigenvectors of a graph are used to compute a prediction about the graph. Laplacian eigenvectors are processed before being fed into this prediction model. Laplacian eigenvectors have been widely used as positional encodings, and many works have noted that sign and/or basis invariance should be addressed in this case \citep{dwivedi2020generalization, beaini2021directional, dwivedi2020benchmarking, kreuzer2021rethinking, mialon2021graphit, dwivedi2022graph, kim2022pure}.


\textbf{Sign invariance.} For any eigenvector , the sign flipped  is also an eigenvector,
so a function  (where  is an arbitrary output dimension) should be \textit{sign invariant}:

for all sign choices . That is, we want  to be invariant to the product group . This captures all eigenvector symmetries if the eigenvalues  are distinct and the eigenvectors are unit-norm.

\textbf{Basis invariance.} If the eigenvalues have higher multiplicity, then there are further symmetries. Let  be bases of eigenspaces---i.e.,  has orthonormal columns and spans the eigenspace associated with the shared eigenvalue . Any other orthonormal basis that spans the eigenspace is of the form  for some orthogonal  (see Appendix~\ref{appendix:eigenspace_background}). Thus, a function  that is invariant to changes of basis in each eigenspace satisfies 

In other words,  is invariant to the product group . The number of eigenspaces  and the dimensions  may vary between matrices; we account for this in Section~\ref{sec: multiple spaces}. As~, sign invariance is a special case of basis invariance when all eigenvalues are distinct.

\textbf{Permutation equivariance.} For GNN models that output node features or node predictions, one typically further desires  to be invariant or equivariant to permutations of nodes, i.e., along the rows of each vector. 
Thus, for , we typically require  for any permutation matrix . Figure \ref{fig:symmetries} illustrates all of the symmetries.




\begin{figure}[ht]
    \centering
    \includegraphics[width=.98\columnwidth]{figs/SignNet_Diagram_v2.pdf}
    \caption{Pipeline for using node positional encodings. After processing by our SignNet, the learned positional encodings from the Laplacian eigenvectors are added as additional node features of an input graph ( denotes concatenation).
    These positional encodings along with the graph adjacency and original node features are passed to a prediction model (e.g. a GNN).
    Not shown here, SignNet can also take in eigenvalues, node features and adjacency information if desired.}
    \label{fig:signnet_diagram}
\end{figure}

\subsection{Warmup: Neural Networks on One Eigenspace}\label{sec: warmup}

Before considering the general setting, we design neural networks that take a single eigenvector or eigenspace as input and are sign or basis invariant. These single subspace architectures will become building blocks for the general architectures. For one subspace, a sign invariant function is merely an even function, and is easily parameterized.
\begin{proposition}\label{prop:one_sign_invariant}
    A continuous function  is sign invariant if and only if
    
    for some continuous . A continuous  is sign invariant and permutation equivariant if and only if \eqref{eq:one_sign_ansatz} holds for a continuous permutation equivariant .
\end{proposition}

In practice, we parameterize  by a neural network. Any architecture choice will ensure sign invariance, while permutation equivariance can be achieved using elementwise MLPs, DeepSets~\citep{zaheer2017deep}, Transformers~\citep{vaswani2017attention}, or  most GNNs.

Next, we address basis invariance for a single -dimensional subspace, i.e., we aim to parameterize maps  that are (a) invariant to right multiplication by , and (b) equivariant to permutations along the row axis. For (a), we use the mapping  from  to the orthogonal projector of its column space, which is  invariant.
Mapping  does not lose information if we treat  as equivalent to  for any . This is justified by the classical first fundamental theorem of  \citep{kraft1996classical}, which has recently been applied in machine learning by \cite{villar2021scalars}.

Regarding (b), permuting the rows of  permutes rows and columns of . Hence, we desire the function   on  to be equivariant to simultaneous row and column permutations: .
To parameterize such a mapping from matrices to vectors, we use an invariant graph network ()~\citep{maron2018invariant}---a neural network mapping to and from tensors of arbitrary order  that has the desired permutation equivariance.  We thus parameterize a family with the requisite invariance and equivariance as follows:

Proposition~\ref{prop:ign_universal} states that this architecture universally approximates  invariant and permutation equivariant functions. The full approximation power requires high order tensors to be used for the ; in practice, we restrict the tensor dimensions for efficiency, as discussed in the next section.

\begin{proposition}\label{prop:ign_universal}
    Any continuous,  invariant  is of the form  for a continuous . For a compact , maps of the form  universally approximate continuous   that are  invariant and permutation equivariant.
\end{proposition}


\subsection{Neural Networks on Multiple Eigenspaces}\label{sec: multiple spaces}

To develop a method for processing multiple eigenvectors (or eigenspaces), we first prove a general decomposition theorem (see Appendix~\ref{sec:universality} for more details). Our result reduces invariance for a large product group  to the much simpler invariances for the smaller constituent groups .

\begin{theorem}[informal]
Let a product of groups  act on . Under mild conditions, any continuous -invariant function  can be written , where  is  invariant, and  and  are continuous If  and , then we can take .
\end{theorem}
For eigenvector data, the th eigenvector (or eigenspace) is in , and its symmetries are described by . Thus, we can reduce the multiple-eigenspace case to the single-eigenspace case, and leverage the models we developed in the previous section.






\textbf{SignNet.} We parameterize our sign invariant network  on eigenvectors ~as:

where  and  are unrestricted neural networks, and  denotes concatenation of vectors. The form  induces sign invariance for each eigenvector. Since we do not yet impose permutation equivariance here, we term this model \emph{Unconstrained-SignNet}.


To obtain a sign invariant \emph{and} permutation equivariant  that outputs vectors in , we restrict  and  to be permutation equivariant networks from vectors to vectors, such as elementwise MLPs, DeepSets~\citep{zaheer2017deep}, Transformers~\citep{vaswani2017attention}, or most standard GNNs. We name this permutation equivariant version \textit{SignNet}. If desired, we can additionally use eigenvalues  and node features  by adding them as arguments to :


\textbf{BasisNet.} For basis invariance, let  be an orthonormal basis of a  dimensional eigenspace. Then we parameterize our \textit{Unconstrained-BasisNet}   by

where each  is shared amongst all subspaces of the same dimension , and  is the number of eigenspaces (i.e., number of distinct eigenvalues, which can differ from the number of eigenvectors ). As  differs between graphs, we may use zero-padding or a sequence model like a Transformer to parameterize . Again,  and  are generally unrestricted neural networks. To obtain permutation equivariance, we make  permutation equivariant and let  be IGNs from matrices to vectors. For efficiency, we will only use matrices and vectors in the IGNs (that is, no tensors in  for ), i.e., we use 2-IGN~\citep{maron2018invariant}. Our resulting \textit{BasisNet} is

\textbf{Expressive-BasisNet.} While we restrict SignNet to only use vectors and BasisNet to only use vectors and matrices,  higher order tensors are generally required for universally approximating permutation equivariant or invariant functions~\citep{keriven2019universal,maron2019universality,maehara2019simple}. Thus, we will consider a theoretically powerful but computationally impractical variant of our model, in which we replace  and  in BasisNet with IGNs of arbitrary tensor order. We call this variant \textit{Expressive-BasisNet}. Universal approximation requires  sized intermediate tensors~\citep{ravanbakhsh2020universal}. 
We study Expressive-BasisNet due to its theoretical interest, and to juxtapose with the computational efficiency and strong expressive power of SignNet and BasisNet.


In the multiple subspace case, we can prove universality for some instances of our models through our decomposition theorem---see Section~\ref{sec:universality} for details. 
For a summary of properties and more details about our models, see Appendix~\ref{appendix:more_signnet_details}.





\section{Theoretical Power for Graph Representation Learning}

Next, we establish that our SignNet and BasisNet can go beyond useful basis invariant and permutation equivariant functions on Laplacian eigenvectors for graph representation learning, including: spectral graph convolutions, spectral invariants, and existing graph positional encodings.  Expressive-BasisNet can of course compute these functions,  but this section shows that the practical invariant architectures SignNet and BasisNet can compute them as well.

\subsection{SignNet and BasisNet strictly Generalize Spectral Graph Convolution}\label{sec:spectral_conv}

For node features  and an eigendecomposition , a \emph{spectral graph convolution} takes the form , for some parameters , that may optionally be continuous functions  of the eigenvalues~\citep{bruna2014spectral,defferrard2016convolutional}. This family includes important functions like heat kernels and generalized PageRanks on graphs~\citep{li2019optimizing}. \rebut{A spectral GNN is defined as multiple layers of spectral graph convolutions and node-wise linear maps, e.g.  is a two layer spectral GNN.}
It can be seen (in Appendix~\ref{appendix:spectral_conv}) that spectral graph convolutions are permutation equivariant and sign invariant, and if  (i.e. the transformation applied to the diagonal elements is parametric) they are additionally invariant to a change of bases in each eigenspace. 

Our SignNet and BasisNet can be viewed as generalizations of spectral graph convolutions, as our networks universally approximate all spectral graph convolutions of the above form. For instance, SignNet with  and  directly yields the spectral graph convolution. This is captured in  Theorem~\ref{prop:spectral_conv}, which we prove in Appendix~\ref{appendix:spectral_conv}. In fact, we may expect SignNet to learn spectral graph convolutions well, according to the principle of algorithmic alignment~\citep{xu2019can} (see Appendix~\ref{appendix:spectral_conv}); this is supported by numerical experiments in Appendix~\ref{sec:spectral_conv_exp}, in which our networks outperform baselines in learning spectral graph convolutions.

\begin{theorem}\label{prop:spectral_conv}
    SignNet universally approximates all spectral graph convolutions. BasisNet universally approximates all parametric spectral graph convolutions.
\end{theorem}

In fact, SignNet and BasisNet are strictly stronger than spectral graph convolutions; there are functions computable by SignNet and BasisNet that cannot be approximated by spectral graph convolutions or spectral GNNs. This is captured in Proposition~\ref{prop:signnet_strictly_greater_conv}: our networks can distinguish bipartite graphs from non-bipartite graphs, but spectral GNNs cannot for certain choices of graphs and node signals.

\begin{proposition}\label{prop:signnet_strictly_greater_conv}
\rebut{There exist infinitely many pairs of non-isomorphic graphs that SignNet and BasisNet can distinguish, but spectral graph convolutions or spectral GNNs cannot distinguish.}
\end{proposition}


\subsection{BasisNet can Compute Spectral Invariants}

Many works measure the expressive power of graph neural networks by comparing their power for testing graph isomorphism~\citep{xu2018powerful, sato2020survey}, or by comparing their ability to compute certain functions on graphs like subgraph counts~\citep{chen2020can, tahmasebi2020counting}. These works often compare GNNs to combinatorial invariants on graphs, especially the -Weisfeiler-Leman (-WL) tests of graph isomorphism~\citep{morris2021weisfeiler}.

While we may also compare with these combinatorial invariants, as other GNN works that use spectral information have done \citep{beaini2021directional}, we argue that it is more natural to analyze our networks in terms of \textit{spectral invariants}, which are computed from the eigenvalues and eigenvectors of graphs. There is a rich literature of spectral invariants from the fields of spectral graph theory and complexity theory~\citep{cvetkovic1997eigenspaces}. For a spectral invariant to be well-defined, it must be invariant to permutations and changes of basis in each eigenspace, a  characteristic shared by our networks.

The simplest spectral invariant is the multiset of eigenvalues, which we give as input to our networks. Another widely studied, powerful spectral invariant is the collection of graph angles, which are defined as the values , where  is an orthonormal basis for the th adjacency matrix eigenspace, and  is the th standard basis vector, which is zero besides a one in the th component. These are easily computed by our networks (Appendix~\ref{appendix:spectral_invariants}), so our networks inherit the strength of these invariants. We capture these results in the following theorem, which also lists a few properties that graph angles determine~\citep{cvetkovic1991some}. 

\begin{theorem}\label{prop:graph_angles}
    BasisNet universally approximates the graph angles . The eigenvalues and graph angles (and thus BasisNet) can determine the number of length 3, 4, or 5 cycles, whether a graph is connected, and the number of length  closed walks from any vertex to itself.
\end{theorem}

\textbf{Relation to WL and message passing.} In contrast to this result, message passing GNNs are not able to express any of these properties~(see \citep{arvind2020weisfeiler, garg2020} and Appendix~\ref{appendix:spectral_invariants}). Although spectral invariants are strong, \cite{furer2010power} shows that the eigenvalues and graph angles---as well as some strictly stronger spectral invariants---are not stronger than the 3-WL test (or, equivalently, the 2-Folklore-WL test). 
Using our networks for node positional encodings in  message passing GNNs allows us to go beyond graph angles, as message passing can distinguish all trees, but there exist non-isomorphic trees with the same eigenvalues and graph angles~\citep{furer2010power, cvetkovic1988constructing}.

\subsection{SignNet and BasisNet Generalize Existing Graph Positional Encodings}\label{sec:existing_pe}

Many graph positional encodings have been proposed, without any clear criteria on which to choose for a particular task. We prove (in Appendix~\ref{appdx: existing PE}) that our efficient SignNet and BasisNet can approximate many previously used graph positional encodings, as we unify these positional encodings by expressing them as either a spectral graph convolution matrix or the diagonal of a spectral graph convolution matrix.
\begin{proposition}\label{prop:approximate_positional}
    SignNet and BasisNet can approximate node positional encodings based on heat kernels~\citep{feldman2022weisfeiler} and random walks~\citep{dwivedi2022graph}. BasisNet can approximate diffusion and -step random walk relative positional encodings~\citep{mialon2021graphit}, and generalized PageRank and landing probability distance encodings~\citep{li2020distance}.
\end{proposition}
We note that diagonals of spectral convolutions are used as feature descriptors in the shape analysis literature, such as for the heat kernel signature~\citep{sun2009concise} and wave kernel signature~\citep{aubry2011wave}. In the language of recent works in graph machine learning, these are node positional encodings computed from a discrete Laplacian of a triangle mesh. This connection appears to be unnoticed in recent works on graph positional encodings.

\section{Experiments}\label{sec:experiments}

We demonstrate the strength of our networks in various experiments. Appendix~\ref{appendix:more_signnet_details} shows simple pseudo-code and Figure~\ref{fig:signnet_diagram} is a diagram detailing the use of SignNet as a node positional encoding.

\subsection{Graph Regression}\label{sec:graph_regression}

\begin{table}[ht]
    \centering
    \caption{Results on the ZINC dataset with a k parameter budget. All models use edge features. Numbers are the mean and standard deviation over 4 runs, each with different seeds.}
    \label{tab:zinc}
    {\small
    \begin{tabular}{lrrrrrr}
        \toprule
        Base model
        & \multicolumn{1}{c}{Positional encoding}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\#param} 
        & \multicolumn{1}{c}{Test MAE ()}\\
        \midrule
   \multirow{7}{*}{GatedGCN} & No PE  & N/A & k &  \\
         & LapPE (flip)  &  & k &  \\
         & LapPE (abs.)  & 8 & k &  \\
         & LapPE (can.)  & 8 & k &  \\
         & SignNet ( only) &  & k &
  \\
         & SignNet &  & k &
  \\
    & SignNet & All & k &
      \\
        \midrule
        \multirow{4}{*}{Sparse  Transformer} & No PE  & N/A & k &  \\
         & LapPE (flip)  &  & k &  \\
         & SignNet  &  & k &  \\
         & SignNet  & All & k &  \\
        \midrule
        \multirow{4}{*}{GINE} & No PE  & N/A & k &  \\
         & LapPE (flip)  &  & k &  \\
         & SignNet  &  & k &  \\
         & SignNet & All & k &  \\
         \midrule
         \multirow{4}{*}{PNA} & No PE  & N/A & k  &  \\
         & LapPE (flip)  &  & k &  \\
         & SignNet  &  & k  &  \\
         & SignNet  & All & k  &  \\
        
        \bottomrule
    \end{tabular}
}
    \vspace{-5pt}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Comparison with SOTA methods on graph-level regression tasks.
    Numbers are test MAE, so lower is better. Best models within a standard deviation are bolded.}
    \label{tab:sota_graph}
    {\small
    \begin{tabular}{llll}
    \toprule
         & ZINC (10K)  & ZINC-full  & Alchemy (10k)  \\
         \midrule
         GIN~\citep{xu2018powerful} & .170\std{.002} & .088\std{.002} & .180\std{.006} \\
         -2-GNN~\citep{morris2020weisfeiler} & .374\std{.022} & .042\std{.003} & .118\std{.001}\\
         -2-LGNN~\citep{morris2020weisfeiler} & .306\std{.044} & .045\std{.006} & .122\std{.003}\\
         SpeqNet~\citep{morris2022speqnets} & --- & --- & \textbf{.115\std{.001}} \\
         GNN-IR~\citep{dupty2022graph} & .137\std{.010} & --- & .119\std{.002} \\
         PF-GNN~\citep{dupty2021pf}  & .122\std{.01} & --- & \textbf{.111\std{.01}}\\
         Recon-GNN~\citep{cotta2021reconstruction} & .170\std{.006} & --- & .125\std{.001} \\
         \midrule
         SignNet (ours) & \textbf{.084\std{.006}} & \textbf{.024\std{.003}} & \textbf{.113\std{.002}} \\
         \bottomrule
    \end{tabular}
    }
\end{table}

We study the effectiveness of SignNet for learning positional encodings (PEs) from the  eigenvectors of the graph Laplacian on the ZINC dataset of molecule graphs~\citep{irwin2012zinc} (using the subset of 12,000 graphs from \citet{dwivedi2020benchmarking}). We primarily consider three settings: 1) No positional encoding, 2) Laplacian PE (LapPE)---the  eigenvectors of the graph Laplacian with smallest eigenvalues are concatenated with existing node features, 3) SignNet positional features---passing the eigenvectors through a SignNet  and concatenating the output with node features. We parameterize SignNet by taking  to be a  \citep{xu2018powerful} and  to be an . We sum over  outputs before the MLP when handling variable numbers of eigenvectors, so the SignNet is of the form  (see Appendix~\ref{appendix:graph_regression} for further details).
We consider four different base models that process the graph data and positional encodings: GatedGCN \citep{bresson2017residual}, a Transformer with sparse attention only over neighbours \citep{kreuzer2021rethinking}, PNA \citep{corso2020principal}, and GIN \citep{xu2018powerful} with edge features (i.e. GINE) \citep{hu2020strategies}. The total number of parameters of the SignNet and the base model is kept within a 500k budget.

Table \ref{tab:zinc} shows the results. For all 4 base models, the PE learned with SignNet yields the best test MAE (mean absolute error)---lower MAE is better. This includes the cases of PNA and GINE, for which Laplacian PE with simple random sign flipping was unable to improve performance over using no PE at all. Our best performing model is a PNA model combined with SignNet, which achieves  test MAE. Besides SignNet, we consider two non-learned approaches to resolving eigenvector sign ambiguity---canonicalization and taking element-wise absolute values (see Appendix~\ref{appendix:graph_regression} for details). Results with GatedGCN show that these alternatives are not more effective than random sign flipping for learning positional encodings. We also consider an ablation of our SignNet architecture where we remove the sign invariance, using simply . Although the resulting architecture is no longer sign invariant,  still processes eigenvectors independently, meaning that only two invariances () need be learned, significantly fewer than the  total sign flip configurations. Accordingly, this non-sign invariant learned positional encoding achieves a test MAE of , improving over the Laplacian PE () but falling short of the fully sign invariant SignNet (). In all cases, using all available eigenvectors in SignNet significantly improves performance over using a fixed number of eigenvectors; this is notable as generally people truncate to a fixed number of eigenvectors in other works.
In Appendix~\ref{appendix:no_edge_features}, we also show that SignNet improves performance when no edge features are included in the data.

\textbf{Efficiency.} These significant performance improvements from SignNet come with only a slightly higher computational cost. For example, GatedGCN with no PE takes about 8.2 seconds per training iteration on ZINC, while GatedGCN with 8 eigenvectors and SignNet takes about 10.6 seconds; this is only a 29\% increase in time, for a reduction of test MAE by over 50\%. Also, eigenvector computation time is neglible, as we need only precompute and save the eigenvectors once, and it only takes 15 seconds to do this for the 12,000 graphs of ZINC.

\textbf{Comparison with SOTA.} In Table~\ref{tab:sota_graph}, we compare SignNet with other domain-agnostic state-of-the-art methods on graph-level molecular regression tasks on ZINC (10,000 training graphs), ZINC-full (about 250,000 graphs), and Alchemy~\citep{chen2019alchemy} (10,000 training graphs).
SignNet outperforms all methods on ZINC and ZINC-full.
Our mean score is the second best on Alchemy, and is within a standard deviation of the best.
We perform much better on ZINC (.084) than other state-of-the-art positional encoding methods, like GNN-LSPE (.090)~\citep{dwivedi2022graph}, SAN (.139)~\citep{kreuzer2021rethinking}, and Graphormer (.122)~\citep{ying2021transformers}.


\subsection{Counting Substructures and Regressing Graph Properties}\label{sec:count_substruct}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\columnwidth]{figs/substructures.pdf} \hspace{10pt}
     \includegraphics[width=0.45\columnwidth]{figs/properties.pdf}
    \caption{Counting substructures and regressing graph properties (lower is better). With Laplacian PEs, SignNet improves performance, while sign flip data augmentation (LapPE) is less consistent. 
    Mean and standard deviations are reported on 3 runs. All runs use the same 4-layer GIN base model. 
    }
    \label{fig:counting_sub}
    \vspace{-5pt}
\end{figure}



Substructure counts (e.g. of cycles) and global graph properties (e.g. connectedness, diameter, radius) are important graph features that are known to be informative for problems in biology, chemistry, and social networks \citep{chen2020can,holland1977method}. Following the setting of \citet{zhao2022from}, we show that SignNet with Laplacian positional encodings boosts the ability of simple GNNs to count substructures and regress graph properties. We take a 4-layer GIN as the base model for all settings, and for SignNet we use GIN as  and a Transformer as  to handle variable numbers of eigenvectors (see Appendix~\ref{sec:appdx count} for details). As shown in Figure \ref{fig:counting_sub}, Laplacian PEs with sign-flip data augmentation improve performance for counting substructures but not for regressing graph properties, while Laplacian PEs processed by SignNet significantly boost performance on all tasks. 

\subsection{Neural Fields on Manifolds}


\begin{table}
    \centering
    \caption{Test results for texture reconstruction experiment on cat and human models, following the experimental setting of \citep{koestler2022intrinsic}. We use 1023 eigenvectors of the cotangent Laplacian.}
    \label{tab:intrinsic_nf}
    {\small
    \begin{tabular}{lcccccccccc}
    \toprule
               & & \multicolumn{3}{c}{Cat} & & \multicolumn{3}{c}{Human}\\
                \cmidrule{3-5}  \cmidrule{7-9}
        Method & Params &  PSNR  & DSSIM  & LPIPS  & &  PSNR  & DSSIM  & LPIPS   \\
        \midrule
         Intrinsic NF & 329k & 34.25 & .099 & .189 & & 32.29 & \textbf{.119} & .330   \\
         Absolute value & 329k & 34.67 & .106 & .252 & & \textbf{32.42} & .132 & .363 \\
         Sign flip & 329k & 23.15 & 1.28 & 2.35 & & 21.52 & 1.05 & 2.71  \\
         SignNet & 324k & \textbf{34.91} & \textbf{.090} & \textbf{.147} & & \textbf{32.43} & .125 & \textbf{.316} \\
         \bottomrule
    \end{tabular}
    }
\end{table}

Discrete approximations to the Laplace-Beltrami operator on manifolds have proven useful for processing data on surfaces, such as triangle meshes~\citep{levy2006laplace}. Recently, \cite{koestler2022intrinsic} propose intrinsic neural fields, which use eigenfunctions of the Laplace-Beltrami operator as positional encodings for learning neural fields on manifolds. For generalized eigenfunctions , at a point  on the surface, they parameterize functions . As these eigenfunctions have sign ambiguity, we use our SignNet to parameterize , with  and  being MLPs.


Table~\ref{tab:intrinsic_nf} shows our results for texture reconstruction experiments on all models from \citet{koestler2022intrinsic}. The total number of parameters in our SignNet-based model is kept below that of the original model. We see that the SignNet architecture improves over the original Intrinsic NF model and over other baselines --- especially in the LPIPS (Learned Perceptual Image Patch Similarity) metric, which has been shown to be a typically better perceptual metric than PSNR or DSSIM~\citep{zhang2018unreasonable}. While we have not yet tested this, we believe that SignNet would allow even better improvements when learning over eigenfunctions of different models, as it could improve transfer and generalization. See Appendix~\ref{appendix:cat_viz} for visualizations and Appendix~\ref{appendix:texture} for more details.



\subsection{Visualization of Learned Positional Encodings}



\begin{figure}[ht]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/eigfunc_11.png}
    \caption{Eigvec 11}
    \end{subfigure} 
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/phi_pca_signnet11.png}
    \caption{}
    \end{subfigure}  
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/eigfunc_14.png}
    \caption{Eigvec 14}
    \end{subfigure} 
    \begin{subfigure}{.24\columnwidth}
    \centering
    \includegraphics[width=.6\columnwidth]{figs/phi_pca_signnet14.png}
    \caption{}
    \end{subfigure} \\label{eq:general_ansatz}
        f(v_1, \ldots, v_k) = \rho( \phi_1(v_1), \ldots,  \phi_k(v_k) ).
    
    f(v_1, \ldots, v_k) = f(c_1 v_1, \ldots, c_k v_k) \qquad c_i \in \RR \setminus \{0\}.

    f(v_1, \ldots, v_k) = \rho\left(\left[\phi(v_i/ \norm{v_i}) + \phi(-v_i/\norm{v_i}) \right]_{i=1, \ldots, k} \right).

    V_i(V_i^\top V_i)^{-1} V_i^\top Z = Z (Z^\top Z)^{-1} Z^\top Z = Z,

    f(V_1 \ldots, V_l ) = f(V_1 W_1, \ldots, V_l W_l) \qquad W_i \in \mrm{GL}_\RR(d_i).

    f(V_1, \ldots, V_l) = \rho\left(\left[\phi_{d_i}(V_i (V_i^\top V_i)^{-1} V_i^\top) \right]_{i=1, \ldots, l} \right).

    f(v_1, \ldots, v_k) = f(c_1 v_1, \ldots, c_k v_k) \qquad c_i \in \CC \setminus \{0\}.

    f(V_1, \ldots, V_l) = f(V_1 W_1, \ldots, V_l W_l) \qquad W_i \in \mrm{GL}_{\CC}(d_i).

    f(V_1, \ldots, V_l) = \rho\left(\left[\phi_{d_i}(V_i (V_i^* V_i)^{-1} V_i^*) \right]_{i=1, \ldots, l} \right).

    APv = PAP^\top P v = PAv = P\lambda v = \lambda Pv,
5pt]
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_09.png}
    \caption{Eigenvector 9}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet09.png}
    \caption{}
    \end{subfigure} \5pt]
    
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/eigfunc_14.png}
    \caption{Eigenvector 14}
    \end{subfigure} 
    \begin{subfigure}{.4\columnwidth}
    \centering
    \includegraphics[width=\wth\columnwidth]{figs/phi_pca_signnet14.png}
    \caption{}
    \end{subfigure} \10pt]
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_0.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_1.pdf}
    \includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_2.pdf}
    \hspace{2.1pt}\includegraphics[width=\wth\columnwidth]{figs/fluorescein_full_pca_3.pdf}
    \caption{Normalized Laplacian eigenvectors and learned positional encodings for the graph of  fluorescein. (Top row) From left to right: smallest and second smallest nontrivial eigenvectors, then second largest and largest eigenvectors. (Bottom row) From left to right: first four principal components of the output  of SignNet. 
    }
    \label{fig:visualize_molecule}
\end{figure}

To better understand SignNet, in Figure~\ref{fig:visualize_molecule} we visualize the learned positional encodings of a SignNet with ,  (with a summation to handle variable eigenvector numbers)  trained on ZINC as in Section~\ref{sec:graph_regression}.
 SignNet learns interesting structural information such as min-cuts (PC 3) and appendage atoms (PC 2) that qualitatively differ from any single eigenvector of the graph. 

For this visualization we use a SignNet trained with a GatedGCN base model on ZINC, as in Section~\ref{sec:graph_regression}. This SignNet uses GIN as  and  as an MLP (with a sum before it to handle variable numbers of eigenvectors), and takes in all eigenvectors of each graph. See Figure~\ref{fig:fluorescein_eigvec} for all of the eigenvectors of fluorescein.


\section{More Related Work}\label{appendix:more_related}

\subsection{Graph Positional Encodings}

Various graph positional encodings have been proposed, which have been motivated for increasing expressive power or practical performance of graph neural networks, and for  generalizing Transformers to graphs. Positional encodings are related to so-called position-aware network embeddings~\citep{chami2020machine}, which capture distances between nodes in graphs. These include network embedding methods like Deepwalk~\citep{perozzi2014deepwalk} and node2vec~\citep{grover2016node2vec}, which have been recently integrated into GNNs that respect their invariances by \cite{wang2022equivariant}. Further, \cite{li2020distance} studies the theoretical and practical benefits of incorporating distance features into graph neural networks. \cite{dwivedi2022graph} proposes a method to inject learnable positional encodings into each layer of a graph neural network, and uses a simple random walk based node positional encoding. \cite{you2021identity} proposes a node positional encoding , which captures the number of closed walks from a node to itself. \cite{dwivedi2020benchmarking} propose to use Laplacian eigenvectors as positional encodings in graph neural networks, with sign ambiguities alleviated by sign flipping data augmentation. \cite{srinivasan2019equivalence} theoretically analyze node positional embeddings and structural representations in graphs, and show that most-expressive structural representations contain the information of any node positional embedding.

While positional encodings in sequences as used for Transformers~\citep{vaswani2017attention} are able to leverage the canonical order in sequences, there is no such useful canonical order for nodes in a graph, due in part to permutation symmetries. Thus, different permutation equivariant positional encodings have been proposed to help generalize Transformers to graphs. \cite{dwivedi2020generalization} directly add in linearly projected Laplacian eigenvectors to node features before processing these features with a graph Transformer. \cite{kreuzer2021rethinking} propose an architecture that uses attention over Laplacian eigenvectors and eigenvalues to learn node or edge positional encodings. \cite{mialon2021graphit} uses spectral kernels such as the diffusion kernel to define relative positional encodings that modulate the attention matrix. \cite{ying2021transformers} achieve state-of-the-art empirical performance with simple Transformers that incorporate shortest-path based relative positional encodings. \cite{zhang2020graph} also utilize shortest-path distances for positional encodings in their graph Transformer. \cite{kim2021transformers} develop higher-order transformers (that generalize invariant graph networks), which interestingly perform well on graph regression using sparse higher-order transformers without positional encodings.

\subsection{Eigenvector Symmetries in Graph Representation Learning}\label{appendix:more_related_eigvec}

Many works that attempt to respect the invariances of eigenvectors solely focus on sign invariance (by using data augmentation)~\citep{dwivedi2020benchmarking, dwivedi2020generalization, dwivedi2022graph, kreuzer2021rethinking}. This may be reasonable for continuous data, where eigenvalues of associated matrices may be usually distinct and separated~(e.g. \cite{puny2021frame} finds that this empirically holds for covariance matrices of -body problems). However, discrete graph Laplacians are known to have higher multiplicity eigenvalues in many cases, and in Appendix~\ref{appendix:higher_dim} we find this to be true in various types of real-world graph data. Graphs without higher multiplicity eigenspaces are easier to deal with; in fact, graph isomorphism can be tested in polynomial time on graphs of bounded multiplicity for adjacency matrix eigenvalues~\citep{babai1982isomorphism, LeightonMiller79}, with a time complexity that is lower for graphs with lower maximum multiplicities.

A recent work of \citet{wang2022equivariant} proposes full orthogonal group invariance for functions that process positional encodings. In particular, for positional encodings , they parameterize functions  such that  for all . This indeed makes sense for network embeddings like node2vec~\citep{grover2016node2vec}, as their objective functions are based on inner products and are thus orthogonally invariant. While they prove stability results when enforcing full orthogonal invariance for eigenvectors, this is a very strict constraint compared to our basis invariance. For instance, when  and all eigenvectors are used in , the condition  implies that  is a constant function on orthogonal matrices, since any orthogonal matrix  can be obtained as  for .
In other words, for bases of eigenspaces  and , \citet{wang2022equivariant} enforces , while we enforce . While the columns of  are still eigenvectors, the columns of  generally are not.


\subsection{Graph Spectra and Learning on Graphs}
More generally, graph spectra are widely used in analyzing graphs, and spectral graph theory \citep{chung1997spectral} studies the connection between graph properties and graph spectra. 
Different graph kernels have been defined based on graph spectra, which use robust and discriminative notions of generalized spectral distance \citep{verma2017hunt}, the spectral density of states \citep{huang2021density}, random walk return probabilities \citep{zhang2018retgk}, or the trace of the heat kernel \citep{tsitsulin2018netlsd}. 
Graph signal processing relies on spectral operations to define Fourier transforms, frequencies, convolutions, and other useful concepts for processing data on graphs~\citep{ortega2018graph}. The closely related spectral graph neural networks  \citep{wu2020comprehensive,balcilar2020analyzing} parameterize neural architectures that are based on similar spectral operations.


\section{Definitions, Notation, and Background}

\subsection{Basic Topology and Algebra Definitions}

We will use some basic topology and algebra for our theoretical results. A topological space  is a set  along with a family of subsets  satisfying certain properties, which gives useful notions like continuity and compactness. From now on, we will omit mention of , and refer to a topological space as the set  itself. For topological spaces  and , we write  and say that  is homeomorphic to  if there exists a continuous bijection with continuous inverse from  to . We will say  if the underlying sets and topologies are equal as sets (we will often use this notion of equality for simplicity, even though it can generally be substituted with homeomorphism). 
For a function  between topological spaces  and , the image  is the set of values that  takes, . This is also denoted .
A function  is called a topological embedding if it is a homeomorphism from  to its image.

A group  is a set along with a multiplication operation , such that multiplication is associative, there is a multiplicative identity , and each  has a multiplicative inverse . A topological group is a group that is also a topological space such that the multiplication and inverse operations are continuous.

A group  may act on a set  by a function . We usually denote  as . A topological group is said to act continuously on a topological space  if  is continuous. For any group  and topological space , we define the coset , which can be viewed as an equivalance class of elements that can be transformed from one to another by a group element. The quotient space  is the set of all such equivalence classes, with a topology induced by that of . The quotient map  is a surjective continuous map that sends  to its coset, .

For ,  denotes the standard Euclidean norm. By the  norm of functions  from a compact  to a Euclidean space , we mean .

\subsection{Background on Eigenspace Invariances}\label{appendix:eigenspace_background}

 Let  and  be two orthonormal bases for the same  dimensional subspace of . Since  and  span the same space, their orthogonal projectors are the same, so . Also, since  and  have orthonormal columns, we have . Define . Then  is orthogonal because
 
 Moreover, we have that
 
 Thus, for any orthonormal bases  and  of the same subspace, there exists an orthogonal  such that .

For another perspective on this, define the Grassmannian  as the smooth manifold consisting of all  dimensional subspaces of . Further define the Stiefel manifold  as the set of all orthonormal tuples  of  vectors in . Letting  act by right multiplication, it holds that . This implies that any  invariant function on  can be viewed as a function on subspaces. See e.g. \cite{gallier2020differential} Chapter 5 for more information on this. We will use this relationship in our proofs of universal representation.


When we consider permutation invariance or equivariance, the permutation acts on dimensions of size . Then a tensor  is called an order  tensor with respect to this permutation symmetry, where order 0 are called scalars, order 1 tensors are called vectors, and order 2 tensors are called matrices. Note that this does not depend on ; in this work, we only ever consider vectors and scalars with respect to the  action.


\section{Proofs of Universality}


We begin by proving the two propositions for the single subspace case from Section \ref{sec: warmup}.

\begin{repproposition}{prop:one_sign_invariant}
    A continuous function  is sign invariant if and only if
    
    for some continuous . A continuous  is sign invariant and permutation equivariant if and only if \eqref{eq:one_sign_ansatz} holds for a continuous permutation equivariant .
\end{repproposition}



\begin{proof}
    If , then  is obviously sign invariant. On the other hand, if  is sign invariant, then letting  gives that , and  is of course continuous.

    If  for a permutation equivariant , then , so  is permutation equivariant and sign invariant. If  is permutation equivariant and sign invariant, then define  again; it is clear that  is continuous and permutation equivariant.
\end{proof}


\begin{repproposition}{prop:ign_universal}
    Any continuous,  invariant  is of the form  for a continuous . For a compact domain , maps of the form  universally approximate continuous functions  that are  invariant and permutation equivariant.
\end{repproposition}


\begin{proof}
    The case without permutation equivariance holds by the First Fundamental Theorem of  (Lemma~\ref{lem:first_fund}).
    
    For the permutation equivariant case, let  and let . Note that  is compact, as it is the continuous image of a compact set. Since  is  invariant, the first fundamental theorem of  shows that there exists a continuous function  such that . Since  is permutation equivariant, for any permutation matrix  we have that
    
    so  is a continuous permutation equivariant function from matrices to vectors. Then note that \cite{keriven2019universal} show that invariant graph networks (of generally high tensor order in hidden layers) universally approximate continuous permutation equivariant functions from matrices to vectors on compact sets of matrices.  Thus, an  can -approximate , and hence  can -approximate .
\end{proof}

\subsection{Proof of Decomposition Theorem}\label{appendix:general_ansatz}

\tikzcdset{every label/.append style = {font = \normalsize}}
\tikzcdset{arrows={line width=.7pt}}
\begin{figure}[ht]
    \centering
    \begin{tikzcd}[row sep=large, column sep=large]
        & \mc X_1 \times \ldots \times \mc X_k \arrow{d}[left]{\pi = \pi_1 \times \ldots \pi_k} \arrow{dr}{f = \tilde f \circ \pi} \arrow[purple, bend right=40, dashed]{ddl}[left]{\phi = \psi \circ \pi \quad }  \\
        & \arrow[bend left=15]{dl}{\psi = \psi_1 \times \ldots \times \psi_k} (\mc X_1 / G_1) \times \ldots \times (\mc X_k / G_k) \arrow{r}[below]{\tilde f} & \RR^{\dout} \\
        \mc Z = \mrm{im}(\psi) \subseteq \RR^a \arrow[bend left=15]{ur}{\psi^{-1}} \arrow[dashed, bend right=40, purple]{urr}[below]{\rho = \tilde f \circ \psi^{-1}}
    \end{tikzcd}
    \caption{Commutative diagram for our proof of Theorem~\ref{thm:general_ansatz}. Black arrows denote functions from topological constructions, and red dashed lines denote functions that we parameterize by neural networks ( and ).}
    \label{fig:cd_general_ansatz}
\end{figure}


Here, we give the formal statement of Theorem~\ref{thm:general_ansatz}, which provides the necessary topological assumptions for the theorem to hold. In particular, we only require the  be a topological group that acts continuously on  for each , and that there exists a topological embedding of each quotient space into some Euclidean space. That the group action is continuous is a very mild assumption, and it holds for any finite or compact matrix group, which all of the invariances we consider in this paper can be represented as.

A topological embedding of the quotient space into a Euclidean space is desired, as we know how to parameterize neural networks with Euclidean outputs and inputs, whereas dealing with a quotient space is generally difficult. Many different conditions can guarantee existence of such an embedding.
For instance, if the quotient space is a smooth manifold, then the Whitney Embedding Theorem (Lemma~\ref{lem:whitney}) guarantees such an embedding. Also, if the base space  is a Euclidean space and  is a finite or compact matrix Lie group, then a map built from -invariant polynomials gives such an embedding (\cite{gonzalez2003c} Lemma 11.13).

Figure~\ref{fig:cd_general_ansatz} provides a commutative diagram representing the constructions in our proof.
\newtheorem*{thm:general_ansatz}{Theorem~\ref{thm:general_ansatz}}
\begin{thm:general_ansatz}[Decomposition Theorem]
    Let  be  topological spaces, and let  be a topological group acting continuously on  for each . Assume that there is a topological embedding  of each quotient space into a Euclidean space  for some dimension . 
    Then, for any continuous function   that is invariant to the action of , there exists continuous functions  and a continuous function , where  such that
    
    Furthermore: (1) each  can be taken to be invariant to , (2) the domain  is compact if each  is compact, (3) if  and , then  can be taken to be equal to . 
\end{thm:general_ansatz}
\begin{proof}
    Let  denote the quotient map for . Since each  acts continuously, Lemma~\ref{lem:product_quotient} gives that the quotient of the product space is the product of the quotient spaces, i.e. that
    
    and the corresponding quotient map  is given by 
    
By passing to the quotient (Lemma~\ref{lem:pass_to_quotient}), there exists a continuous  on the quotient space such that . By Lemma~\ref{lem:quotient_compact}, each  is compact if  is compact. Defining the image , we thus know that  is compact if  is compact. 

Moreover, as  is a topological embedding, it has a continuous inverse  on its image . Further, we have a topological embedding  given by , with continuous inverse .

Note that

So we define

Thus, , so equation \eqref{eq:general_ansatz} holds. Moreover, the  and  are continuous, as they are compositions of continuous functions. Furthermore, (1) holds as each  is invariant to  because each  is invariant to .  Since each  is compact if  is compact, the product  is compact if each  is compact, thus proving (2).

To show the last statement (3), note simply that if  and , then the quotient maps are equal, i.e. . Moreover, we can choose the embeddings to be equal, so say . Then, , so we are done.
\end{proof}



\subsection{Universality of SignNet and BasisNet}\label{appdx: niversality of SignNet and BasisNet}

Here, we prove Corollary~\ref{cor:universal_net} on the universal representation and approximation capabilities of our Unconstrained-SignNets, Unconstrained-BasisNets, and Expressive-BasisNets. We proceed in several steps, first proving universal representation of continuous functions when we do not require permutation equivariance, then proving universal approximation when we do require permutation equivariance.


\subsubsection{Sign Invariant Universal Representation}

Recall that  denotes the unit sphere in . As we normalize eigenvectors to unit norm, the domain of our functions on  eigenvectors are on the compact space .

\begin{corollary}[Universal Representation for SignNet]\label{thm:no_feature_sign_inv}
    A continuous function  is sign invariant, i.e.  for any , if and only if there exists a continuous  and a continuous  such that
    
\end{corollary}
\begin{proof}
    It can be directly seen that any  of the above form is sign invariant.

    Thus, we show that any sign invariant  can be expressed in the above form.
    First, we show that we can apply the general Theorem~\ref{thm:general_ansatz}. The group  acts continuously and satisfies that , where  is the real projective space of dimension . Since  is a smooth manifold of dimension , Whitney's embedding theorem states that there exists a (smooth) topological embedding ~(Lemma~\ref{lem:whitney}).

    Thus, we can apply the general theorem to see that  for some continuous  and . Note that each  is the same, as each  and  is the same. Also, Theorem~\ref{thm:general_ansatz} says that we may assume that  is sign invariant, so . Letting , we are done with the proof.
\end{proof}


\subsubsection{Sign Invariant Universal Representation with Extra Features}

Recall that we may want our sign invariant functions to process other data besides eigenvectors, such as eigenvalues or node features associated to a graph. Here, we show universal representation for when we have this other data that does not possess sign symmetry. The proof is a simple extension of Corollary~\ref{thm:no_feature_sign_inv}, but we provide the technical details for completeness.

\begin{corollary}[Universal Representation for SignNet with features]
    For a compact space of features , let  be a continuous function .

    Then  is sign invariant for the inputs on the sphere, i.e. 
    
if and only if there exists a continuous  and a continuous  such that
    
\end{corollary}
\begin{proof}
    Once again, the sign invariance of any  in the above form is clear.

    We follow very similar steps to the proof of Corollary~\ref{thm:no_feature_sign_inv} to show that we may apply Theorem~\ref{thm:general_ansatz}. We can view  as a quotient space, after quotienting by the trivial group that does nothing, . The corresponding quotient map is , the identity map. Also,  trivially topologically embeds in  by the inclusion map.

    As  acts continuously, by Lemma~\ref{lem:product_quotient} we have that
    
    with corresponding quotient map , where  is the quotient map to . 

    Letting  be the embedding of  guaranteed by Whitney's embedding theorem (Lemma~\ref{lem:whitney}), we have that  is an embedding of . Thus, we can apply Theorem~\ref{thm:general_ansatz} to write  for , so
    
    where . Letting , we are done.
\end{proof}


\subsubsection{Basis Invariant Universal Representation}

Recall that  is the Stiefel manifold of -tuples of vectors  where  and  are orthonormal. This is where our inputs lie, as our eigenvectors are unit norm and orthogonal. We will also make use of the Grassmannian , which consists of all -dimensional subspaces in . This is because the Grassmannian is the quotient space for the group action we want, , where  acts on  by mapping  to ~\citep{gallier2020differential}.

\begin{corollary}[Universal Representation for BasisNet]\label{cor:universal_basisnet}
    For dimensions  let  be a continuous function on . Further assume that  is invariant to , where  acts on  by multiplication on the right. 

    Then there exist continuous  and continuous  such that
    
    where the  are  invariant functions, and we can take  if .
\end{corollary}
\begin{proof}
    Letting  and , it can be seen that  acts continuously on . Also, we have that the quotient space  is the Grassmannian of  dimensional subspaces in , which is a smooth manifold of dimension . Thus, the Whitney embedding theorem (Lemma~\ref{lem:whitney}) gives a topological embedding .

    Hence, we may apply Theorem~\ref{thm:general_ansatz} to obtain continuous  invariant  and continuous , such that . Also, if , then  and , so we can take .

\end{proof}

\subsubsection{Basis Invariant and Permutation Equivariant Universal Approximation}

With the restriction that  be permutation equivariant and basis invariant, we need to use the impractically expensive Expressive-BasisNet to approximate . Universality of permutation invariant or equivariant functions from matrices to scalars or matrices to vectors is difficult to achieve in a computationally tractable manner~\citep{maron2019universality,keriven2019universal, maehara2019simple}. One intuitive reason to expect this is that universally approximating such functions allows solution of the graph isomorphism problem~\citep{chen2019equivalence}, which is a computationally difficult problem. While we have exact representation of basis invariant functions by continuous  and  when there is no permutation equivariance constraint, we can only achieve approximation up to an arbitrary  when we require permutation equivariance.

\begin{corollary}[Universal Approximation for Expressive-BasisNets]
    Let  be continuous,  invariant, and permutation equivariant. Then  can be -approximated by an Expressive-BasisNet.
\end{corollary}
\begin{proof}
    By invariance, Corollary~\ref{cor:universal_basisnet} of the decomposition theorem shows that  can be written as 
    
    for some continuous  invariant  and continuous . By the first fundamental theorem of  (Lemma~\ref{lem:first_fund}), each  can be written as  for some continuous . Let 
    
    which is compact as it is the image of the compact space  under a continuous function.
    Define  by 
    
    Then note that  is continuous and permutation equivariant from matrices to vectors, so it can be -approximated by an invariant graph network~\citep{keriven2019universal}, call it . If we define  and  (this identity operation is linear and permutation equivariant, so it can be exactly expressed by an IGN), then we have -approximation of  by 
    
\end{proof}


\subsection{Proof of Universal Approximation for General Decompositions}\label{appdx: Universal Approximation}



\begin{theorem}\label{thm:decomp_universal}
Consider the same setup as Theorem \ref{thm:general_ansatz}, where  are also compact.
Let  be a family of -invariant functions that universally approximate -invariant continuous functions , and let  be a set of continuous function that universally approximate continuous functions  for every compact , where . Then for any  and any -invariant continuous function  there exists  and  such that .
\end{theorem}
\begin{proof}
Consider a particular -invariant continuous function . By Theorem \ref{thm:general_ansatz}  there exists  -invariant  continuous functions  and a continuous function  (where ) such that
     
Now fix an . For any  and any   () we may bound the difference from  as follows (suppressing the 's for brevity),

Now let . Since each  is continuous and defined on a compact set  we know that  is compact, and so the product  is also compact. Since  is compact, it is contained in a closed ball  of radius  centered at the origin. Let  be the closed ball  of radius  centered at the origin, so  contains  and a ball of radius  around each point of . We may extend  continuously to  as needed, so assume . By universality of  we may pick a particular ,  such that 

Keeping this choice of , it remains only to bound . As  is continuous on a compact domain, it is in fact uniformly continuous. Thus, we can choose a  such that if , then , and then we define .


Since  universally approximates   we may pick  such that , and thus . With this choice of , we know that  (because each  is within distance  of ). Thus,  is well-defined, and we have

due to our choice of , which completes the proof.
 \end{proof}


\section{Basis Invariance for Graph Representation Learning}



\subsection{Spectral Graph Convolution}\label{appendix:spectral_conv}

In this section, we consider spectral graph convolutions, which for node features  take the form  for some parameters . We can optionally take  for some continuous function  of the eigenvalues. This form captures most popular spectral graph convolutions in the literature~\citep{bruna2014spectral, hamilton2020graph, bronstein2017geometric}; often, such convolutions are parameterized by taking  to be some analytic function such as a simple affine function~\citep{kipf2016semi}, a linear combination in a polynomial basis~\citep{defferrard2016convolutional, chien2021adaptive}, or a parameterization of rational functions~\citep{levie2018cayleynets, bianchi2021graph}.

First, it is well known and easy to see that spectral graph convolutions are permutation equivariant, as for a permutation matrix  we have 

Also, it is easy to see that they are sign invariant, as . However, if the  do not depend on the eigenvalues, then the spectral graph convolution is not necessarily basis invariant. For instance, if  and  are in the same eigenspace, and we change basis by permuting  and , then if  the spectral graph convolution will generally change as well.

On the other hand, if  for some function , then the spectral graph convolution is basis invariant. This is because if  and  belong to the same eigenspace, then  so . Thus, if  are eigenvectors of the same eigenspace with eigenvalue , we have that . Now, note that  is the orthogonal projector onto the eigenspace~\citep{trefethen1997numerical}. A change of basis does not change this orthogonal projector, so such spectral graph convolutions are basis invariant.

Another way to see this basis invariance is with a simple computation. Let  be the eigenspaces of dimension , where . Let the corresponding eigenvalues be . Then for any orthogonal matrices , we have

so the spectral graph convolution is invariant to substituting  for .

Now, we give the proof that shows SignNet and BasisNet can universally approximate spectral graph convolutions.
\newtheorem*{prop:spectral_conv}{Theorem~\ref{prop:spectral_conv}}
\begin{prop:spectral_conv}[Learning Spectral Graph Convolutions]
    Suppose the node features  take values in compact sets. Then SignNet can universally approximate any spectral graph convolution, and both BasisNet and Expressive-BasisNet can universally approximate any parametric spectral graph convolution.
\end{prop:spectral_conv}
\begin{proof}
    Note that eigenvectors and eigenvalues of normalized Laplacian matrices take values in compact sets, since the eigenvalues are in  and we take eigenvectors to have unit-norm. Thus, the whole domain of the spectral graph convolution is compact.

    Let . First, consider a spectral graph convolution . For SignNet, let  approximate the function  to within  error, which DeepSets can do since this is a continuous permutation equivariant function from vectors to vectors \citep{segol2019universal} (note that we can pass  as a vector in  by instead passing , where  is the all ones vector). Then  is a linear permutation equivariant operation that can be exactly expressed by DeepSets, so the total error is within . The same argument applies when  for some continuous function .
  
    For the basis invariant case, consider a parametric spectral graph convolution . Note that if the eigenspace bases are  with eigenvalues , we can write the . Again, we will let  be a sum function, which can be expressed exactly by DeepSets. Thus, it suffices to show that  can be  approximated by a 2-IGN (i.e. an IGN that only uses vectors and matrices).

    Note that since  is continuous, we can use an elementwise MLP (which IGNs can learn) to approximate  to arbitrary precision (note that we represent the eigenvalue  as a constant matrix ). Also, since a 2-IGN can learn matrix vector multiplication~(\cite{cai2022convergence} Lemma 10), we can approximate , as  is a matrix and  is a vector with respect to permutation symmetries. Finally, we use an elementwise MLP to approximate the scalar-vector multiplication . Since , and since 2-IGNs universally approximate each , applying Lemma~\ref{lem:layer_universal} shows that a 2-IGN can approximate  to  accuracy, so we are done. Since Expressive-BasisNet is stronger than BasisNet, it can also universally approximate these functions.
\end{proof}


From the proof, we can see that SignNet and BasisNet need only learn simple functions for the  and  when  is simple, or when the filter is non-parametric and we need only learn . \cite{xu2019can} propose the principle of algorithmic alignment, and show that if separate modules of a neural network each need only learn simple functions (that is, functions that are well-approximated by low-order polynomials with small coefficients), then the network may be more sample efficient. If we do not require permutation equivariance, and parameterize SignNet and BasisNet with simple MLPs, then algorithmic alignment may suggest that our models are sample efficient. Indeed,  is a simple linear function with coefficients , and  is quadratic in  and linear in , so it is simple if  is simple.

\begin{repproposition}{prop:signnet_strictly_greater_conv}
\rebut{There exist infinitely many pairs of non-isomorphic graphs that SignNet and BasisNet can distinguish, but spectral graph convolutions or spectral GNNs cannot distinguish.}
\end{repproposition}
\begin{proof}
    \rebut{
    The idea is as follows: we will take graphs  and give them the node feature matrix , i.e. each node has as feature the square root of its degree. Then any spectral graph convolution (or, the first layer of any spectral GNN) will map  to something that only depends on the degree sequence and number of nodes. Thus, any spectral graph convolution or spectral GNN will have the same output (up to permutation) for any such graphs  with node features  and the same number of nodes and same degree sequence. On the other hand, SignNet and BasisNet can distinguish between infinitely many pairs of graphs  with node features  and the same number of nodes and degree sequence; this is because SignNet and BasisNet can tell when a graph is bipartite.
    }
    
    \rebut{For each , we will define  and  as connected graphs with  nodes, with the same degree sequence. Also, we define  to have node features , where  is the degree of node  in , and similarly  has node features . Now, note that  is an eigenvector of the normalized Laplacian of , and it has eigenvalue . As we take the eigenvectors to be orthonormal (since the normalized Laplacian is symmetric), for any spectral graph convolution we have that}
    
    \rebut{Where  is the diagonal degree matrix of . Likewise, any spectral graph convolution outputs  for . Since  and  are the same up to a permutation, we have that any spectral graph convolution has the same output for  and , up to a permutation. In fact, this also holds for spectral GNNs, as the first layer will always have the same output (up to a permutation) on  and , so the latter layers will also have the same output up to a permutation.}

    \rebut{Now, we concretely define  and . This is illustrated in Figure~\ref{fig:example_graphs_1} and Figure~\ref{fig:example_graphs_2}. For , let  contain a triangle with nodes , and have a path of length 2 coming out of one of the nodes in the triangle, say  connects to , and  connects to . This is not bipartite, as there is a triangle. Let  be a bipartite graph that has 2 nodes on the left  and 3 nodes on the right . Connect  with all nodes on the right, and connect  with  and .}
    
    \rebut{Note that both  and  have the same number of nodes and the same degree sequence . Thus, spectral graph convolutions or spectral GNNs cannot distinguish them. However, SignNet and BasisNet can distinguish them, as they can tell whether a graph is bipartite by checking the highest eigenvalue of the normalized Laplacian. This is because the multiplicity of the eigenvalue 2 is the number of bipartite components. In particular, SignNet can approximate the function  and . Likewise, BasisNet can approximate the function  and .
    }
    
    \rebut{
    This in fact gives an infinite family of graphs that SignNet / BasisNet can distinguish, but spectral graph convolutions or spectral graph GNNs cannot. To see why, suppose we have  and  for some . Then we construct a pair of graphs on  nodes with the same degree sequence. To do this, we add another node to the path of , thus giving it degree sequence . For , we add a node  to the side that  is not contained on (e.g. for , we add  to the left side, as  was on the right), then connect  to  to also give a degree sequence . Note that the non-bipartiteness of  and bipartiteness of  are preserved.
    }

\begin{figure}
    \centering
    \begin{tikzpicture}[main/.style = {draw, circle, node distance=1.25cm}, baseline=10pt] 
        \node[main] (1) at (0,2) {};
        \node[main] (2) [below of=1] {};
        \node[main] (3) [right of=2] {};
        \node[main] (4) [above of=3] {};
        \node[main] (5) [right of=4] {};
        \node[] at (1, -1.25) {};
        \draw[-] (1) edge (2);
        \draw[-] (2) edge (3);
        \draw[-] (1) edge (3);
        \draw[-] (1) edge (4);
        \draw[-] (4) edge (5);
        
        \node[main] (v1) at (6, 2) {};
        \node[main] (v2) [below of=v1] {};
        \node[main] (v3) [right of=v1] {};
        \node[main] (v4) [below of=v3] {};
        \node[main] (v5) [below of=v4] {};
        \node[] at (6.75, -1.25) {};
        \draw[-] (v1) edge (v3);
        \draw[-] (v1) edge (v4);
        \draw[-] (v1) edge (v5);
        \draw[-] (v2) edge (v3);
        \draw[-] (v2) edge (v4);
    \end{tikzpicture}
    \caption{\rebut{ Illustration of our constructed  and  for , as used in the proof of Proposition~\ref{prop:signnet_strictly_greater_conv}.} }
    \label{fig:example_graphs_1}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}[main/.style = {draw, circle, node distance = 1.25cm}, baseline=10pt] 
    \node[main] (1b) at (8,2) {};
    \node[main] (2b) [below of=1b] {};
    \node[main] (3b) [right of=2b] {};
    \node[main] (4b) [above of=3b] {};
    \node[main] (5b) [right of=4b] {};
    \node[main] (6b) [right of=5b] {};
    \node[] at (9, -1.25) {};
    \draw[-] (1b) edge (2b);
    \draw[-] (2b) edge (3b);
    \draw[-] (1b) edge (3b);
    \draw[-] (1b) edge (4b);
    \draw[-] (4b) edge (5b);
    \draw[-] (5b) edge (6b);
    
    \node[main] (v1b) at (15, 2) {};
    \node[main] (v2b) [below of=v1b] {};
    \node[main] (v3b) [right of=v1b] {};
    \node[main] (v4b) [below of=v3b] {};
    \node[main] (v5b) [below of=v4b] {};
    \node[main] (v6b) [below of=v2b] {};
    \node[] at (15.75, -1.25) {};
    \draw[-] (v1b) edge (v3b);
    \draw[-] (v1b) edge (v4b);
    \draw[-] (v1b) edge (v5b);
    \draw[-] (v2b) edge (v3b);
    \draw[-] (v2b) edge (v4b);
    \draw[-] (v5b) edge (v6b);
\end{tikzpicture} 
\caption{\rebut{Illustration of our constructed  and  for , as used in the proof of Proposition~\ref{prop:signnet_strictly_greater_conv}.}}
\label{fig:example_graphs_2}
\end{figure}
    
\end{proof}

\subsection{Existing Positional Encodings}\label{appdx: existing PE}

Here, we show that our SignNets and BasisNets universally approximate various types of existing graph positional encodings. The key is to show that these positional encodings are related to spectral graph convolution matrices and the diagonals of these matrices, and to show that our networks can approximate these matrices and diagonals.

\begin{proposition}\label{prop:diag_spectral_conv}
    If the eigenvalues take values in a compact set, SignNets and BasisNets universally approximate the diagonal of any spectral graph convolution matrix . BasisNets can additionally universally approximate any spectral graph convolution matrix .
\end{proposition}
\begin{proof}
 Note that the  come from a compact set as they are of unit norm. The  are from a compact set by assumption; this assumption holds for the normalized Laplacian, as . Also, as  is linear, the spectral graph convolution diagonal can be written .

    Let . For SignNet, let , which can be exactly expressed as it is a permutation equivariant linear operation from vectors to vectors. Then  can approximate the function  to arbitrary precision, as it is a permutation equivariant function from vectors to vectors~\citep{segol2019universal}. Thus, letting  approximate the function to  accuracy, SignNet can approximate  to  accuracy.

    Let  be the number of eigenspaces , so . For BasisNet, we need only show that it can approximate the spectral graph convolution matrix to  accuracy, as a 2-IGN can exactly express the  function in each , since it is a linear permutation equivariant function from matrices to vectors. A 2-IGN can universally approximate the function , as it can express any elementwise MLP. Also, a 2-IGN can universally approximate the scalar-matrix multiplication  by another elementwise MLP. Since , Lemma~\ref{lem:layer_universal} shows that a single 2-IGN can approximate this composition to  accuracy, so we are done.

\end{proof}

\begin{repproposition}{prop:approximate_positional}
    SignNet and BasisNet can approximate node positional encodings based on heat kernels~\citep{feldman2022weisfeiler} and random walks~\citep{dwivedi2022graph}. BasisNet can approximate diffusion and -step random walk relative positional encodings~\citep{mialon2021graphit}, and generalized PageRank and landing probability distance encodings~\citep{li2020distance}.
\end{repproposition}
\begin{proof}
    We will show that we can apply the above Proposition~\ref{prop:diag_spectral_conv}, by showing that all of these positional encodings are spectral graph convolutions. The heat kernel embeddings are of the form  for some choices of the parameter , so they can be approximated by SignNets or BasisNets. Also, the diffusion kernel~\citep{mialon2021graphit} is just the matrix of this heat kernel, and the -step random walk kernel is  for some parameter , so BasisNets can universally approximate both of these.

    For the other positional encodings, we let  be the eigenvectors of the random walk Laplacian  instead of the normalized Laplacian . The eigenvalues of these two Laplacians are the same, and if  is an eigenvector of the normalized Laplacian then  is an eigenvector of the random walk Laplacian with the same eigenvalue~\citep{von2007tutorial}.

    Then with  as the eigenvectors of the random walk Laplacian, the random walk positional encodings (RWPE) in \cite{dwivedi2022graph} take the form 
    
    for any choices of integer .

    The distance encodings proposed in \cite{li2020distance} take the form
    
    for some function . We restrict to continuous  here; shortest path distances can be obtained by a discontinuous  that we discuss below. Their generalized PageRank based distance encodings can be obtained by 
    
    for some , so this is a spectral graph convolution. They also define so-called landing probability based positional encodings, which take the form
    
    for some choices of integer . Thus, BasisNets can approximate these distance encoding matrices. 
\end{proof}
Another powerful class of positional encodings is based on shortest path distances between nodes in the graph~\citep{ying2021transformers,li2020distance}. Shortest path distances can be expressed in a form similar to the spectral graph convolution, but require a highly discontinuous function. If we define  to be the lowest index such that  is nonzero, then we can write the shortest path distance matrix as , where  is applied elementwise to return an  matrix. As , BasisNets can learn the inside arguments, but cannot learn the discontinuous function .


\subsection{Spectral Invariants}\label{appendix:spectral_invariants}

Here, we consider the graph angles , for  where  is the number of eigenspaces, and . It is clear that graph angles are permutation equivariant and basis invariant. These graph angles have been extensively studied, so we cite a number of interesting properties of them. That graph angles determine the number of length 3, 4 and 5 cycles, the connectivity of a graph, and the number of length  closed walks is all shown in Chapter 4 of~\cite{cvetkovic1997eigenspaces}. Other properties may be of use for graph representation learning as well. For instance, the eigenvalues of node-deleted subgraphs of a graph  are determined by the eigenvalues and graph angles of ; this may be useful in extending recent graph neural networks that are motivated by node deletion and the reconstruction conjecture~\citep{cotta2021reconstruction,bevilacqua2021equivariant,papp2021dropgnn,tahmasebi2020counting}.

Now, we prove that BasisNet can universally approximate the graph angles. The graph properties we consider in the theorem are all integer valued (e.g. the number of cycles of length 3 in a graph is an integer). Thus, any two graphs that differ in these properties will differ by at least 1, so as long as we have approximation to , we can distinguish any two graphs that differ in these properties. Recall the statement of Theorem~\ref{prop:graph_angles}.

\newtheorem*{prop:graph_angles}{Theorem~\ref{prop:graph_angles}}
\begin{prop:graph_angles}
    BasisNet can universally approximate the graph angles . The eigenvalues and graph angles (and thus BasisNets) can determine the number of length 3, 4, and 5 cycles, whether a graph is connected, and the number of length  closed walks from any vertex to itself.
\end{prop:graph_angles}


\begin{proof}
    Note that the graph angles satisfy 
    
    where  is a basis for the th adjacency matrix eigenspace, and  is the -entry of . These graph angles are just the elementwise square roots of the diagonals of the matrices . As  is a permutation equivariant linear function from matrices to vectors, 2-IGN on  can exactly compute this with 0 error. Then a 2-IGN can learn an elementwise MLP to approximate the elementwise square root  to arbitrary precision. Finally, there may be remaining operations  that are permutation invariant or permutation equivariant from vectors to vectors; for instance, the  are typically gathered into a matrix of size  where the columns are lexicographically sorted ( is the number of eigenspaces)~\citep{cvetkovic1997eigenspaces}, or we may have a permutation invariant readout to compute a subgraph count. A DeepSets can approximate  without any higher order tensors besides vectors~\citep{zaheer2017deep,segol2019universal}.

    As 2-IGNs can approximate each  individually, a single 2-IGN can approximate  by Lemma~\ref{lem:layer_universal}. 
    Also, since the graph properties considered in the theorem are integer-valued, BasisNet can distinguish any two graphs that differ in one of these properties.
\end{proof}

To see that message passing graph neural networks (MPNNs) cannot determine these quantities, we use the fact that MPNNs cannot distinguish between two graphs that have the same number of nodes and where each node (in both graphs) has the same degree. For , let  denote the cycle graph of size , and  denote the graph that is the union of two disjoint cycle graphs of size . MPNNs cannot distinguish between  and  for , because they have the same number of nodes, and each node has degree 2. Thus, MPNNs cannot tell whether a graph is connected, as  is but  is not. Also, it cannot count the number of 3, 4, or 5 cycles, as  has two  cycles while  has no  cycles. Likewise, any node in  has more length  closed walks than any node in . This is because any length  closed walk in  has an analogous closed walk in , but the nodes in  also have a closed walk that completely goes around a cycle.





\section{Useful Lemmas}

In this section, we collect useful lemmas for our proofs. These lemmas generally only require basic tools to prove.  Our first lemma is a crucial property of quotient spaces.

\begin{lemma}[Passing to the quotient]\label{lem:pass_to_quotient}
    Let  and  be topological spaces, and let  be a quotient space, with corresponding quotient map . Then for every continuous -invariant function , there is a unique continuous  such that
    .
\end{lemma}
\begin{proof}
    For , by surjectivity of  we can choose an  such that . Define  by . This is well-defined, since if  for any other , then  for some , so 
    
    where the second equality uses the -invariance of . Note that  is continuous by the universal property of quotient spaces. Also,  is the  unique function such that ; if there were another function  with , then , so .
\end{proof}

Next, we give the First Fundamental Theorem of , a classical result that has been recently used for machine learning by \cite{villar2021scalars}. This result shows that an orthogonally invariant  can be expressed as a function . We give a proof that if  is continuous, then  is also continuous.
\begin{lemma}[First Fundamental Theorem of ]\label{lem:first_fund}
    A continuous function  is orthogonally invariant, i.e.  for all , if and only if  for some continuous .
\end{lemma}
\begin{proof}
    If , then we have  so  is orthogonally invariant.

    For the other direction, invariant theory shows that the  invariant polynomials are generated by the inner products , where  are the rows of ~\citep{kraft1996classical}. Let  be the map . Then \cite{gonzalez2003c} Lemma 11.13 shows that the quotient space  is homeomorphic to a closed subset . Let  refer to this homeomorphism, and note that  by passing to the quotient (Lemma~\ref{lem:pass_to_quotient}). Then any continuous  invariant  passes to a unique continuous  (Lemma~\ref{lem:pass_to_quotient}), so  where  is the quotient map. Define  by , and note that  is a composition of continuous functions and hence continuous. Finally, we have that , so we are done.
\end{proof}

The next lemma allows us to decompose a quotient of a product space into a product of smaller quotient spaces.

\begin{lemma}\label{lem:product_quotient}
    Let  be topological spaces and  be topological groups such that each  acts continuously on . Denote the quotient maps by . Then the quotient of the product is the product of the quotient, i.e.
    
    and   is quotient map.
\end{lemma}
\begin{proof}
    First, we show that  is a quotient map. This is because 1. the quotient map of any continuous group action is an open map, so each  is an open map, 2. the product of open maps is an open map, so  is an open map and 3. a continuous surjective open map is a quotient map, so , which is continuous and surjective, is a quotient map.

    Now, we need only apply the theorem of uniqueness of quotient spaces to show \eqref{eq:product_quotient} (see e.g. \cite{lee2013smooth}, Theorem A.31). Letting  denote the quotient map for this space, it is easily seen that  if and only if , since either of these is true if and only if there exist  such that  for each . Thus, we have an isomorphism of these quotient spaces.
\end{proof}

The following lemma shows that quotients of compact spaces are also compact, which is useful for universal approximation on quotient spaces.

\begin{lemma}[Compactness of quotients of compact spaces]\label{lem:quotient_compact}
    Let  be a compact space. Then the quotient space  is compact.
\end{lemma}
\begin{proof}
    Denoting the quotient map by  and letting  be an open cover of , we have that  is an open cover of . By compactness of , we can choose a finite subcover . Then  by surjectivity, and  is thus an open cover of .
\end{proof}

The Whitney embedding theorem gives a nice condition that we apply to show that the quotient spaces  that we deal with embed into Euclidean space. It says that when  is a smooth manifold, then it can be embedded into a Euclidean space of double the dimension of the manifold. The proof is outside the scope of this paper.
\begin{lemma}[Whitney Embedding Theorem \citep{whitney1944self}]\label{lem:whitney}
    Every smooth manifold  of dimension  can be smoothly embedded in .
\end{lemma}

Finally, we give a lemma that helps prove universal approximation results. It says that if functions  that we want to approximate can be written as compositions , then it suffices to universally approximate each  and compose the results to universally approximate the . This is especially useful for proving universality of neural networks, as we may use some layers to approximate each , then compose these layers to approximate the target function .

\begin{lemma}[Layer-wise universality implies universality]\label{lem:layer_universal}
    Let  be a compact domain, let  be families of continuous functions where  consists of functions from  for some . Let  be the family of functions  that are compositions of functions .

    For each , let  be a family of continuous functions that universally approximates . Then the family of compositions  universally approximates .
\end{lemma}
\begin{proof}
    Let . Let , and then for  let . Then each  is compact by continuity of the . For , let , and for  let  be a compact set containing  such that every ball of radius one centered at a point in  is still contained in .

Let . We will show that there is a  such that  by induction on . This holds trivially for , as then .

Now, let , and suppose it holds for . By universality of , we can choose a  such that . As  is continuous on a compact domain, it is also uniformly continuous, so we can choose a  such that .

Let . By induction, we can choose  such that 

Note that , because for each ,  is within  Euclidean distance to , so it is contained in  by construction. Thus, we may define , and compute that

since . To bound this other term, let , and for  and , we know that , so  by uniform continuity. As this holds for all , we have , so  and we are done.
\end{proof}


\section{Further Experiments}

\subsection{Graph Regression with no Edge Features}\label{appendix:no_edge_features}

\begin{table}[ht]
    {\small
    \centering
    \caption{Results on the ZINC dataset with 500k parameter budget and no edge features. Numbers are the mean and standard deviation over 4 runs each with different seeds.}
    \label{tab:zinc_no_edge}
    \begin{center}
    \begin{tabular}{lrrrr}
        \toprule
        Base model
        & \multicolumn{1}{c}{Positional encoding} & \multicolumn{1}{c}{} &  \multicolumn{1}{c}{\#params} & 
        \multicolumn{1}{c}{Test MAE ()} \\
        \midrule
        \multirow{3}{*}{GIN} & No PE &  & k &  \\
         & LapPE (flip) &  & k &  \\
         & SignNet &  & k &  \\
        \midrule
        \multirow{3}{*}{GAT} & No PE &   & k &  \\
         & LapPE (flip) & & k &  \\
         & SignNet &   & k &  \\
        \bottomrule
    \end{tabular}
    \end{center}
}
\end{table}


All graph regression models in Table \ref{tab:zinc} use edge features for learning and inference. To show that SignNet is also useful when no edge features are available, we ran ZINC experiments without edge features as well. The results are displayed in Table \ref{tab:zinc_no_edge}. In this setting, SignNet still significantly improves the performance over message passing networks without positional encodings, and over Laplacian positional encodings with sign flipping data augmentation.

\subsection{Comparison with Domain Specific Molecular Graph Regression Models}

\begin{table}[ht]
    \centering
    \caption{Comparison with domain specific methods on graph-level regression tasks.
    Numbers are test MAE, so lower is better. Best models within a standard deviation are bolded.}
    \label{tab:domain_specific}
    {\small
    \begin{tabular}{llll}
    \toprule
         & ZINC (10K)  & ZINC-full  \\
         \midrule
         HIMP ~\citep{fey2020hierarchical} & .151\std{.006} & .036\std{.002}\\
         CIN-small ~\citep{bodnar2021weisfeiler} & .094\std{.004} & .044\std{.003}\\
         CIN ~\citep{bodnar2021weisfeiler} & \textbf{.079\std{.006}} & \textbf{.022\std{.002}} \\
         \midrule
         SignNet (ours) & \textbf{.084\std{.006}} & \textbf{.024\std{.003}} \\
         \bottomrule
    \end{tabular}
    }
\end{table}

In Table~\ref{tab:domain_specific}, we compare our model against methods that have domain-specific information about molecules built into them:  HIMP~\citep{fey2020hierarchical} and CIN~\citep{bodnar2021weisfeiler}. We see that SignNet is better than HIMP and CIN-small on these tasks, and is within a standard deviation of CIN. The SignNet models are the same as the ones reported in Table~\ref{tab:sota_graph}. Once again, we emphasize that SignNet is domain-agnostic.

\subsection{Learning Spectral Graph Convolutions}\label{sec:spectral_conv_exp}



\begin{table}[ht]
    \centering
    {\small
    \caption{Sum of squared errors for spectral graph convolution regression (with no test set). Lower is better. Numbers are mean and standard deviation over 50 images from~\cite{he2021bernnet}.}
    \label{tab:spectral_conv}
    \begin{tabular}{lrrrrr}
        \toprule
        & \multicolumn{1}{c}{Low-pass} & \multicolumn{1}{c}{High-pass} & \multicolumn{1}{c}{Band-pass} & \multicolumn{1}{c}{Band-rejection} & \multicolumn{1}{c}{Comb}\\
        \midrule
        GCN & .111\std{.068} & 3.092\std{5.11} & 1.720\std{3.15} & 1.418\std{1.03} & 1.753\std{1.17} \\
        GAT & .113\std{.065} & .954\std{.696}  & 1.105\std{.964} & .543\std{.340} & .638\std{.446}  \\
        GPR-GNN & .033\std{.032} & .012\std{.007} & .137\std{.081} & .256\std{.197} & .369\std{.460} \\
        ARMA & .053\std{.029} & .042\std{.024} & .107\std{.039} & .148\std{.089} & .202\std{.116}  \\
        ChebNet & .003\std{.002}  & \textbf{.001}\std{.001} & .005\std{.003} & .009\std{.006} & .022\std{.016} \\
        BernNet & \textbf{.001}\std{.002} & \textbf{.001}\std{.001} & \textbf{.000}\std{.000}  & .048\std{.042}  & .027\std{.019} \\
        \midrule
        Transformer & 3.662\std{1.97} & 3.715\std{1.98} & 1.531\std{1.30} & 1.506\std{1.29} & 3.178\std{1.93} \\
        Transformer Eig Flip & 4.454\std{2.32} & 4.425\std{2.38} & 1.651\std{1.53} & 2.567\std{1.73} & 3.720\std{1.94} \\
        Transformer Eig Abs & 2.727\std{1.40} & 3.172\std{1.61} & 1.264\std{.788} & 1.445\std{.943} & 2.607\std{1.32} \\
        \midrule
        DeepSets SignNet & .004\std{.013} & .086\std{.405} & .021\std{.115} & .008\std{.037} &  \textbf{.003}\std{.016} \\
        Transformer SignNet & .003\std{.016} & .004\std{.025} & .001\std{.004}  & .006\std{.023} & .093\std{.641} \\
        DeepSets BasisNet & .009\std{.018} & .003\std{.015} & .008\std{.030} & \textbf{.004}\std{.011} & .015\std{.060} \\
        Transformer BasisNet & .079\std{.471} & .014\std{.038} & .005\std{.018} & .006\std{.016}  & .014\std{.051} \\
        \bottomrule
    \end{tabular}
}
\end{table}

To numerically test the ability of our basis invariant networks for learning spectral graph convolutions, we follow the experimental setups of \cite{balcilar2020analyzing, he2021bernnet}. We take the dataset of 50 images in \cite{he2021bernnet} (originally from the Image Processing Toolbox of \textsc{Matlab}), and resize them from 100100 to 3232. Then we apply the same spectral graph convolutions on them as in \cite{he2021bernnet}, and train neural networks to learn these as regression targets. As in prior work, we report sum of squared errors on the training set to measure expressivity.

We compare against message passing GNNs~\citep{kipf2016semi,velivckovic2018graph} and spectral GNNs~\citep{chien2021adaptive,bianchi2021graph,defferrard2016convolutional,he2021bernnet}. Also, we consider standard Transformers with only node features, with eigenvectors and sign flip augmentation, and with absolute values of eigenvectors. These models are all approximately sign invariant (they either use eigenvectors in a sign invariant way or do not use eigenvectors). We use DeepSets~\citep{zaheer2017deep} in SignNet and 2-IGN~\citep{maron2018invariant} in BasisNet for , use a DeepSets for  in both cases, and then feed the features into another DeepSets or a standard Transformer~\citep{vaswani2017attention} to make the final predictions. That is, we are only given graph information through the eigenvectors and eigenvalues, and we do not use message passing.

Table~\ref{tab:spectral_conv} displays the results, which validate our theoretical results in Section~\ref{sec:spectral_conv}. Without any message passing, SignNet and BasisNet allow DeepSets and Transformers to perform strongly, beating the spectral GNNs GPR-GNN and ARMA on all tasks. Also, our networks outperform all other methods on the band-rejection and comb filters, and are mostly close to the best model on the other filters. 

\section{Further Experimental Details}

\subsection{Hardware, Software, and Data Details}\label{appendix:other_exp_details}

All experiments could fit on one GPU at a time. Most experiments were run on a server with 8 NVIDIA RTX 2080 Ti GPUs. We run all of our experiments in Python, using the PyTorch~\citep{paszke2019pytorch} framework (\href{https://github.com/pytorch/pytorch/blob/master/LICENSE}{license URL}). We also make use of Deep Graph Library (DGL)~\citep{wang2019deep} (Apache License 2.0), and PyTorch Geometric (PyG)~\citep{fey2019fast} (MIT License) for experiments with graph data.

The data we use are all freely available online. The datasets we use are 
ZINC~\citep{irwin2012zinc},
Alchemy~\citep{chen2019alchemy},
the synthetic counting substructures dataset~\citep{chen2020can},
the multi-task graph property regression synthetic dataset~\citep{corso2020principal} (MIT License),
the images dataset used by \citet{balcilar2020analyzing} (GNU General Public License v3.0),
the cat mesh from \url{free3d.com/3d-model/cat-v1--522281.html} (Personal Use License),
and the human mesh from \url{turbosquid.com/3d-models/water-park-slides-3d-max/1093267} (\href{https://blog.turbosquid.com/turbosquid-3d-model-license/}{TurboSquid 3D Model License}). If no license is listed, this means that we cannot find a license for the dataset. As they appear to be freely available with permissive licenses or no licenses, we do not ask for permission from the creators or hosts of the data.

We do not believe that any of this data contains offensive content or personally identifiable information. The 50 images used in the spectral graph convolution experiments are mostly images of objects, with a few low resolution images of humans that do not appear to have offensive content. The only other human-related data appears to be the human mesh, which appears to be from a 3D scan of a human. 

\subsection{Graph Regression Details}\label{appendix:graph_regression}

\textbf{ZINC.} In Section \ref{sec:graph_regression} we study the effectiveness of SignNet for learning positional encodings to boost the expressive power, and thereby generalization, on the graph regression problem ZINC. In all cases we take our  encoder to be an  layer GIN with ReLU activation. The input eigenvector , where  is the number of nodes in the graph, is treated as a single scalar feature for each node. In the case of using a fixed number of eigenvectors , the aggregator  is taken to be an  layer MLP with batch normalization and ReLU activation. The aggregator   is applied separately to the concatenatation of the  different embeddings for each node in a graph, resulting in one single embedding per node. This embedding  is concatenated to the node features for 
that node, and the result passed as input to the base (predictor) model. We also consider using all available eigenvectors in each graph instead of a fixed number . Since the total number of eigenvectors is a variable quantity, equal to the number of  nodes in the underlying graph, an MLP cannot be used for . 
To handle the variable sized input in this case, we take  to be an MLP preceded by a sum over the  outputs. In other words, the SignNet is of the form  in this case.

As well as testing SignNet,  we also checked whether simple transformations that resolve the sign ambiguity of the Laplacian eigenvectors  could serve as effective positional encoding. We considered three options. First is to randomly flip the sign of each  during training. This is a common heuristic used in prior work on Laplacian positional encoding \citep{kreuzer2021rethinking,dwivedi2020benchmarking}. Second,  take the element-wise absolute value . This is a non-injective map, creating sign invariance at the cost of destroying positional information. Third is a different canonicalization that avoids stochasticity and use of  absolute values by selecting the sign of each  so that the majority of entries are non-negative, with ties broken by comparing the -norm of positive and negative parts. When the tie-break also fails, the sign is chosen randomly. Results for GatedGCN base model on ZINC in Table \ref{tab:zinc} show that all three of these approaches are  significantly  poorer positional encodings compared to SignNet. 

Our training pipeline largely follows that of \cite{dwivedi2022graph}, and we use the GatedGCN and PNA base models from the accompanying implementation (see \url{https://github.com/vijaydwivedi75/gnn-lspe}). The Sparse Transformer base model architecture we use, which like GAT computes attention only across neighbouring nodes, is introduced by \cite{kreuzer2021rethinking}. Finally, the GINE implementation is based on the PyTorch Geometric implementation \citep{fey2019fast}. For the state-of-the-art comparison, all baseline results are from their respective papers, except for GIN, which we run.

\textbf{ZINC-full.} We also run our method on the full ZINC dataset, termed ZINC-full. The result we report for SignNet is a larger version of the GatedGCN base model with a SignNet that takes in all eigenvectors. This model has 994,113 parameters in total. All baseline results are from their respective papers, except for GIN, which is from~\citep{bodnar2021weisfeiler}.

\textbf{Alchemy.} We run our method and compare with the state-of-the-art on Alchemy (with 10,000 training graphs). We use the same data split as \citet{morris2020weisfeiler}.  Our base model is a GIN that takes in edge features (i.e. a GINE). The SignNet consists of GIN for  and a Transformer for , as in the counting substructures and graph property regression experiments in Section~\ref{sec:count_substruct}. The model has 907,371 parameters in total. Our training setting is very similar to that of \cite{morris2022speqnets}, as we build off of their code. We train with an Adam optimizer~\citep{kingma2014adam} with a starting learning rate of .001, and a minimum learning rate of .000001. The learning rate schedule cuts the learning rate in half with a patience of 20 epochs, and training ends when we reach the minimum learning rate. All baseline results are from their respective papers, except for GIN, which is from~\citep{morris2022speqnets}.

\subsection{Spectral Graph Convolution Details}

In Appendix~\ref{sec:spectral_conv_exp}, we conduct node regression experiments for learning spectral graph convolutions. The experimental setup is mostly taken from \cite{he2021bernnet}. However, we resize the  images to . Thus, each image is viewed as a -node graph. The node features  are the grayscale pixel intensities of each node. Just as in \cite{he2021bernnet}, we only train and evaluate on nodes that are not connected to the boundary of the grid (that is, we only evaluate on the  middle section). For all experiments we limit each model to 50,000 parameters. We use the Adam~\citep{kingma2014adam} optimizer for all experiments. For each of the GNN baselines (GCN, GAT, GPR-GNN, ARMA, ChebNet, BernNet), we select the best performing out of 4 hyperparameter settings: either 2 or 4 convolution layers, and a hidden dimension of size 32 or , where  is just large enough to stay with 50,000 parameters (for instance,  128 for GCN, GPR-GNN, and BernNet).

We use DeepSets or standard Transformers as our prediction network. This takes in the output of SignNet or BasisNet and concatenates it with the node features, then outputs a scalar prediction for each node. We use a 3 layer output network for DeepSets SignNet, and 2 layer output networks for all other configurations. All networks use ReLU activations.

For SignNet, we use DeepSets for both  and . Our  takes in eigenvectors only, then our  takes the outputs of  and the eigenvalues. We use three layers for  and .

For BasisNet, we use the same DeepSets for  as in SignNet, and 2-IGNs for the . There are three distinct multiplicities for the grid graph (1, 2, and 32), so we only need 3 separate IGNs. Each IGN consists of an  layer and two  layers, where the  are hidden dimensions. There are no matrix to matrix operations used, as the memory requirements are intensive for these  node graphs. The  only take in  from the eigenspaces, and the  takes the output of the  as well as the eigenvalues.



\subsection{Substructures and Graph Properties Regression Details}\label{sec:appdx count}
We use the random graph dataset from \cite{chen2020can} for counting substructures and the synthetic dataset from \cite{corso2020principal} for regressing graph properties. For fair comparison we fix the base model as a 4-layer GIN model with hidden size 128.
We choose  as a 4-layer GIN (independently applied to every eigenvector) and  as a 1-layer Transformer (independently applied to every node). Combined with proper batching and masking, we have a SignNet that takes Laplacian eigenvectors  and outputs fixed size sign-invariant encoding node features , where  varies between graphs but  is fixed. We use this SignNet in our experiments and compare with other methods of handling PEs.

\subsection{Texture Reconstruction Details}\label{appendix:texture}

\begin{table}[ht]
    \centering
    \caption{Parameter settings for the texture reconstruction experiments.}
    {\small
    \begin{tabular}{lcccccc}
    \toprule
         & Params & Base MLP width & Base MLP layers &  out dim &  out dim & ,  width \\
        \midrule
         Intrinsic NF & 328,579 & 128 & 6 &---&--- & ---\\
         SignNet & 323,563 & 108 & 6 & 4 & 64 & 8 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:texture_parameters}
\end{table}

We closely follow the experimental setting of \cite{koestler2022intrinsic} for the texture reconstruction experiments. In this work, we use the cotangent Laplacian~\citep{rustamov2007laplace} of a triangle mesh with the lowest  eigenvectors besides the trivial eigenvector of eigenvalue 0. We implemented SignNet in the authors' original code, which was privately shared with us. Both  and  are taken to be MLPs. Hyperparameter settings and number of parameters are given in Table~\ref{tab:texture_parameters}. We chose hyperparameters so that the total number of  parameters in the SignNet model was no larger than that of the original model.

\end{document}
