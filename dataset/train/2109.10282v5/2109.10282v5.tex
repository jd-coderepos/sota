\documentclass[letterpaper]{article} \usepackage{aaai23}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2023.1)
}
\nocopyright
\usepackage{amsmath}

\usepackage{booktabs}
\usepackage{multirow}






\setcounter{secnumdepth}{0} 





\title{TrOCR: Transformer-based Optical Character Recognition\\ with Pre-trained Models}







\author{Minghao Li\thanks{Work done during internship at Microsoft Research Asia.}, Tengchao Lv, Jingye Chen, Lei Cui, \\
Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei}

\affiliations{
Beihang University\\
Microsoft Corporation\\
\{liminghao1630, lizj\}@buaa.edu.cn\\
\{tengchaolv, v-jingyechen, lecu, yijlu, dinei, chazhang, fuwei\}@microsoft.com\\
}


\begin{document}

\maketitle

\begin{abstract}
Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely \textbf{TrOCR}, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \url{https://aka.ms/trocr}.
\end{abstract}

\section{Introduction}

Optical Character Recognition (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene photo or from subtitle text superimposed on an image. Typically, an OCR system includes two main modules: a text detection module and a text recognition module. Text detection aims to localize all text blocks within the text image, either at word-level or textline-level. The text detection task is usually considered as an object detection problem where conventional object detection models such as YoLOv5 and DBNet~\cite{liao2019realtime} can be applied. Meanwhile, text recognition aims to understand the text image content and transcribe the visual signals into natural language tokens. The text recognition task is usually framed as an encoder-decoder problem where existing methods leveraged CNN-based encoder for image understanding and RNN-based decoder for text generation. In this paper, we focus on the text recognition task for document images and leave text detection as the future work.

Recent progress in text recognition~\citep{diaz2021rethinking} has witnessed the significant improvements by taking advantage of the Transformer~\cite{vaswani2017attention} architectures. However, existing methods are still based on CNNs as the backbone, where the self-attention is built on top of CNN backbones as encoders to understand the text image. For decoders, Connectionist Temporal Classification (CTC) \cite{graves2006connectionist} is usually used compounded with an external language model on the character-level to improve the overall accuracy. Despite the great success achieved by the hybrid encoder/decoder method, there is still a lot of room to improve with pre-trained CV and NLP models: 1) the network parameters in existing methods are trained from scratch with synthetic/human-labeled datasets, leaving large-scale pre-trained models unexplored. 2) as image Transformers become more and more popular~\citep{dosovitskiy2020vit,touvron2020deit}, especially the recent self-supervised image pre-training~\citep{bao2021beit}, it is straightforward to investigate whether pre-trained image Transformers can replace CNN backbones, meanwhile exploiting the pre-trained image Transformers to work together with the pre-trained text Transformers in a single framework on the text recognition task. 

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{AAAI23 TrOCR/TrOCR-Arch-cropped.pdf}
\caption{The architecture of TrOCR, where an encoder-decoder model is designed with a pre-trained image Transformer as the encoder and a pre-trained text Transformer as the decoder.}
\label{fig:model}
\end{figure*}

To this end, we propose \textbf{TrOCR}, an end-to-end Transformer-based OCR model for text recognition with pre-trained CV and NLP models, which is shown in Figure \ref{fig:model}. Distinct from the existing text recognition models, TrOCR is a simple but effective model which does not use the CNN as the backbone. Instead, following~\citep{dosovitskiy2020vit}, it first resizes the input text image into  and then the image is split into a sequence of  patches which are used as the input to image Transformers. Standard Transformer architecture with the self-attention mechanism is leveraged on both encoder and decoder parts, where wordpiece units are generated as the recognized text from the input image. To effectively train the TrOCR model, the encoder can be initialized with pre-trained ViT-style models~\citep{dosovitskiy2020vit,touvron2020deit,bao2021beit} while the decoder can be initialized with pre-trained BERT-style models~\citep{devlin2019bert,liu2019roberta,dong2019unified, wang2020minilm}, respectively. Therefore, the advantage of TrOCR is three-fold. First, TrOCR uses the pre-trained image Transformer and text Transformer models, which take advantages of large-scale unlabeled data for image understanding and language modeling, with no need for an external language model. Second, TrOCR does not require any convolutional network for the backbone and does not introduce any image-specific inductive biases, which makes the model very easy to implement and maintain. Finally, experiment results on OCR benchmark datasets show that the TrOCR can achieve state-of-the-art results on printed, handwritten and scene text image datasets without any complex pre/post-processing steps. Furthermore, we can easily extend the TrOCR for multilingual text recognition with minimum efforts, where just leveraging multilingual pre-trained models in the decoder-side and expand the dictionary.

The contributions of this paper are summarized as follows:
\begin{enumerate}
    \item We propose TrOCR, an end-to-end Transformer-based OCR model for text recognition with pre-trained CV and NLP models. To the best of our knowledge, this is the first work that jointly leverages pre-trained image and text Transformers for the text recognition task in OCR. 
    \item TrOCR achieves state-of-the-art results with a standard Transformer-based encoder-decoder model, which is convolution free and does not rely on any complex pre/post-processing steps.
    \item The TrOCR models and code are publicly available at \url{https://aka.ms/trocr}.
\end{enumerate}




\section{TrOCR}


\subsection{Model Architecture}
TrOCR is built up with the Transformer architecture, including an image Transformer for extracting the visual features and a text Transformer for language modeling. We adopt the vanilla Transformer encoder-decoder structure in TrOCR. The encoder is designed to obtain the representation of the image patches and the decoder is to generate the wordpiece sequence with the guidance of the visual features and previous predictions. 

\subsubsection{Encoder}
The encoder receives an input image , and resizes it to a fixed size . Since the Transformer encoder cannot process the raw images unless they are a sequence of input tokens, the encoder decomposes the input image into a batch of  foursquare patches with a fixed size of ,
while the width  and the height  of the resized image are guaranteed to be divisible by the patch size . Subsequently, the patches are flattened into vectors and linearly projected to -dimensional vectors, aka the patch embeddings.  is the hidden size of the Transformer through all of its layers.

Similar to ViT \cite{dosovitskiy2020vit} and DeiT \cite{touvron2020deit}, we keep the special token ``[CLS]'' that is usually used for image classification tasks. The ``[CLS]'' token brings together all the information from all the patch embeddings and represents the whole image. Meanwhile, we also keep the distillation token in the input sequence when using the DeiT pre-trained models for encoder initialization, which allows the model to learn from the teacher model. The patch embeddings and two special tokens are given learnable 1D position embeddings according to their absolute positions. 












Unlike the features extracted by the CNN-like network, the Transformer models have no image-specific inductive biases and process the image as a sequence of patches, which makes the model easier to pay different attention to either the whole image or the independent patches.

\subsubsection{Decoder}
\label{sec:decoder}

We use the original Transformer decoder for TrOCR. The standard Transformer decoder also has a stack of identical layers, which have similar structures to the layers in the encoder, except that the decoder inserts the ``encoder-decoder attention'' between the multi-head self-attention and feed-forward network to distribute different attention on the output of the encoder. In the encoder-decoder attention module, the keys and values come from the encoder output, while the queries come from the decoder input. In addition, the decoder leverages the attention masking in the self-attention to prevent itself from getting more information during training than prediction. Based on the fact that the output of the decoder will right shift one place from the input of the decoder, the attention mask needs to ensure the output for the position  can only pay attention to the previous output, which is the input on the positions less than :




The hidden states from the decoder are projected by a linear layer from the model dimension to the dimension of the vocabulary size , while the probabilities over the vocabulary are calculated on that by the softmax function. We use beam search to get the final output.


\subsection{Model Initialization}
Both the encoder and the decoder are initialized by the public models pre-trained on large-scale labeled and unlabeled datasets.

\subsubsection{Encoder Initialization}

The DeiT~\cite{touvron2020deit} and BEiT~\cite{bao2021beit} models are used for the encoder initialization in the TrOCR models.
DeiT trains the image Transformer with ImageNet \cite{deng2009imagenet} as the sole training set. The authors try different hyper-parameters and data augmentation to make the model data-efficient. Moreover, they distill the knowledge of a strong image classifier to a distilled token in the initial embedding, which leads to a competitive result compared to the CNN-based models.

Referring to the Masked Language Model pre-training task, BEiT proposes the Masked Image Modeling task to pre-train the image Transformer. Each image will be converted to two views: image patches and visual tokens. They tokenize the original image into visual tokens by the latent codes of discrete VAE \cite{ramesh2021zero}, randomly mask some image patches, and make the model recover the original visual tokens. The structure of BEiT is the same as the image Transformer and lacks the distilled token when compared with DeiT.





\subsubsection{Decoder Initialization}

We use the RoBERTa \cite{liu2019roberta} models and the MiniLM \cite{wang2020minilm} models to initialize the decoder. Generally, RoBERTa is a replication study of \cite{devlin2019bert} that carefully measures the impact of many key hyperparameters and training data size. Based on BERT, they remove the next sentence prediction objective and dynamically change the masking pattern of the Masked Language Model. 

The MiniLM are compressed models of the large pre-trained Transformer models while retaining 99\% performance. Instead of using the soft target probabilities of masked language modeling predictions or intermediate representations of the teacher models to guide the training of the student models in the previous work. The MiniLM models are trained by distilling the self-attention module of the last Transformer layer of the teacher models and introducing a teacher assistant to assist with the distillation. 

When loading the above models to the decoders, the structures do not precisely match since both of them are only the encoder of the Transformer architecture. For example, the encoder-decoder attention layers are absent in these models.
To address this, we initialize the decoders with the RoBERTa and MiniLM models by manually setting the corresponding parameter mapping, and the absent parameters are randomly initialized.


\subsection{Task Pipeline}
In this work, the pipeline of the text recognition task is that given the textline images, the model extracts the visual features and predicts the wordpiece tokens relying on the image and the context generated before.
The sequence of ground truth tokens is followed by an ``[EOS]'' token, which indicates the end of a sentence. During training, we shift the sequence backward by one place and add the ``[BOS]'' token to the beginning indicating the start of generation. The shifted ground truth sequence is fed into the decoder, and the output of that is supervised by the original ground truth sequence with the cross-entropy loss. For inference, the decoder starts from the ``[BOS]'' token to predict the output iteratively while continuously taking the newly generated output as the next input.



\subsection{Pre-training}
We use the text recognition task for the pre-training phase, since this task can make the models learn the knowledge of both the visual feature extraction and the language model. The pre-training process is divided into two stages that differ by the used dataset. In the first stage, we synthesize a large-scale dataset consisting of hundreds of millions of printed textline images and pre-train the TrOCR models on that. 
In the second stage, we build two relatively small datasets corresponding to printed and handwritten downstream tasks, containing millions of textline images each. We use the existed and widely adopted synthetic scene text datasets for the scene text recognition task. 
Subsequently, we pre-train separate models on these task-specific datasets in the second stage, all initialized by the first-stage model. 



\subsection{Fine-tuning}
Except for the experiments regarding scene text recognition, the pre-trained TrOCR models are fine-tuned on the downstream text recognition tasks. The outputs of the TrOCR models are based on Byte Pair Encoding (BPE)~\cite{sennrich2015neural} and SentencePiece~\cite{kudo2018sentencepiece} and do not rely on any task-related vocabularies. 



\begin{table}[ht]
\centering
\small
\begin{tabular}{ccccc}
\hline
\textbf{Encoder} & \textbf{Decoder}  & \textbf{Precision} & \textbf{Recall} & \textbf{F1}    \\
\hline
             &   & 69.28              & 69.06           & 69.17          \\
             &   & 76.45              & 76.18           & 76.31          \\
ResNet50         &   & 66.74              & 67.29           & 67.02          \\
             &  & 77.03              & 76.53           & 76.78          \\
             &  & \textbf{79.67}     & \textbf{79.06}  & \textbf{79.36} \\
ResNet50         &  & 72.54              & 71.13           & 71.83          \\
\hline
\end{tabular}
\caption{Ablation study on the SROIE dataset, where all the models are trained using the SROIE dataset only.}
\label{tab:ablation}
\end{table}

\begin{table}
\centering
\begin{tabular}{lccc}
\hline
\textbf{Model}          & \textbf{Precision} & \textbf{Recall} & \textbf{F1}    \\ \hline
From Scratch            & 38.06              & 38.43           & 38.24          \\
+ Pretrained Model      & 72.95              & 72.56           & 72.75          \\
+ Data Augmentation     & 82.58              & 82.03           & 82.30          \\
+ First-Stage Pretrain  & 95.31              & 95.65           & 95.48          \\
+ Second-Stage Pretrain & \textbf{95.76}     & \textbf{95.91}  & \textbf{95.84} \\ \hline
\end{tabular}
\caption{Ablation study of pretrained model initialization, data augmentation and two stages of pre-training on the SROIE dataset.}
\label{tab:ablation2}
\end{table}

\subsection{Data Augmentation}
We leverage data augmentation to enhance the variety of the pre-training and fine-tuning data. Six kinds of image transformations plus keeping the original are taken for printed and handwritten datasets, which are random rotation (-10 to 10 degrees), Gaussian blurring, image dilation, image erosion, downscaling, and underlining. We randomly decide which image transformation to take with equal possibilities for each sample. For scene text datasets, RandAugment~\cite{cubuk2020randaugment} is applied following \cite{atienza2021vision}, and the augmentation types include inversion, curving, blur, noise, distortion, rotation, etc.



\section{Experiments}


\subsection{Data}


\subsubsection{Pre-training Dataset}

To build a large-scale high-quality dataset, we sample two million document pages from the publicly available PDF files on the Internet. Since the PDF files are digital-born, we can get pretty printed textline images by converting them into page images and extracting the textlines with their cropped images.
In total, the first-stage pre-training dataset contains 684M textlines.




We use 5,427 handwritten fonts\footnote{\tiny The fonts are obtained from \url{https://fonts.google.com/?category=Handwriting} and \url{https://www.1001fonts.com/handwritten-fonts.html}.} to synthesize handwritten textline images by the TRDG\footnote{\tiny\url{https://github.com/Belval/TextRecognitionDataGenerator}}, an open-source text recognition data generator. The text used for generation is crawled from random pages of Wikipedia. The handwritten dataset for the second-stage pre-training consists of 17.9M textlines, including IIIT-HWS dataset~\cite{krishnan2016generating}.
In addition, we collect around 53K receipt images in the real world and recognize the text on them by commercial OCR engines. According to the results, we crop the textlines by their coordinates and rectify them into normalized images.
We also use TRDG to synthesize 1M printed textline images with two receipt fonts and the built-in printed fonts. In total, the printed dataset consists of 3.3M textlines.
The second-stage pre-training data for the scene text recognition are MJSynth (MJ)~\cite{synth90ka} and SynthText (ST)~\cite{gupta2016synthetic}, totaling about 16M text images.

\subsubsection{Benchmarks}
The SROIE (Scanned Receipts OCR and Information Extraction) dataset (Task 2) focuses on text recognition in receipt images.
There are 626 receipt images and 361 receipt images in the training and test sets of SROIE.
Since the text detection task is not included in this work, we use cropped images of the textlines for evaluation, which are obtained by cropping the whole receipt images according to the ground truth bounding boxes.

The IAM Handwriting Database is composed of handwritten English text, which is the most popular dataset for handwritten text recognition. We use the Aachen's partition of the dataset\footnote{\tiny\url{https://github.com/jpuigcerver/Laia/tree/master/egs/iam}}: 6,161 lines from 747 forms in the train set, 966 lines from 115 forms in the validation set and 2,915 lines from 336 forms in the test set.

Recognizing scene text images is more challenging than printed text images, as many images in the wild suffer from blur, occlusion, or low-resolution problems. Here we leverage some widely-used benchmarks, including IIIT5K-3000 \cite{mishra2012top}, SVT-647 \cite{wang2011end}, IC13-857, IC13-1015 \cite{karatzas2013icdar}, IC15-1811, IC15-2077 \cite{karatzas2015icdar}, SVTP-645 \cite{phan2013recognizing}, and CT80-288 \cite{risnumawan2014robust} to evaluate the capacity of the proposed TrOCR.



\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Model} & \textbf{Recall} & \textbf{Precision} & \textbf{F1} \\ \hline
CRNN            & 28.71              & 48.58          & 36.09         \\
Tesseract OCR   & 57.50              & 51.93           & 54.57       \\ \hline
H\&H Lab & 96.35           & 96.52              & 96.43       \\
MSOLab   & 94.77           & 94.88              & 94.82       \\
CLOVA OCR & 94.3            & 94.88              & 94.59       \\ \hline

     & 95.89           & 95.74              & 95.82       \\
      & 96.37           & 96.31              & 96.34       \\
     & 96.59           & 96.57              & \textbf{96.58}       \\
\hline
\end{tabular}
\caption{Evaluation results (word-level Precision, Recall, F1) on the SROIE dataset, where the baselines come from the SROIE leaderboard (\url{https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=2}).}
\label{tab:sroie}
\end{table}


\subsection{Settings}

The TrOCR models are built upon the Fairseq~\cite{ott2019fairseq} which is a popular sequence modeling toolkit. For the model initialization, the DeiT models are implemented and initialized by the code and the pre-trained models from the timm library \cite{rw2019timm} while the BEiT models and the MiniLM models are from the UniLMâ€™s official repository\footnote{\tiny\url{https://github.com/microsoft/unilm}}. The RoBERTa models come from the corresponding page in the Fairseq GitHub repository.
We use 32 V100 GPUs with the memory of 32GBs for pre-training and 8 V100 GPUs for fine-tuning. For all the models, the batch size is set to 2,048 and the learning rate is 5e-5. We use the BPE and sentencepiece tokenizer from Fairseq to tokenize the textlines to wordpieces.

We employ the  resolution and  patch size for DeiT and BEiT encoders. The  has 12 layers with 384 hidden sizes and 6 heads. Both the  and the  have 12 layers with 768 hidden sizes and 12 heads while the  has 24 layers with 1024 hidden sizes and 16 heads.
We use 6 layers, 256 hidden sizes and 8 attention heads for the small decoders, 512 hidden sizes for the base decoders and 12 layers, 1,024 hidden sizes and 16 heads for the large decoders. For this task, we only use the last half of all layers from the corresponding RoBERTa model, which are the last 6 layers for the  and the last 12 layers for the . The beam size is set to 10 for TrOCR models.


We take the CRNN model \cite{shi2016end} as the baseline model. The CRNN model is composed of convolutional layers for image feature extraction, recurrent layers for sequence modeling and the final frame label prediction, and a transcription layer to translate the frame predictions to the final label sequence. To address the character alignment issue, they use the CTC loss to train the CRNN model. 
For a long time, the CRNN model is the dominant paradigm for text recognition.
We use the PyTorch implementation\footnote{\tiny\url{https://github.com/meijieru/crnn.pytorch}} and initialized the parameters by the provided pre-trained model. 

\subsection{Evaluation Metrics}
The SROIE dataset is evaluated using the word-level precision, recall and f1 score. If repeated words appear in the ground truth, they are also supposed to appear in the prediction. The precision, recall and f1 score are described as:


  


The IAM dataset is evaluated by the case-sensitive Character Error Rate (CER).
The scene text datasets are evaluated by the Word Accuracy. For fair comparison, we filter the final output string to suit the popular 36-character charset (lowercase alphanumeric) in this task.






\subsection{Results}

\subsubsection{Architecture Comparison}

We compare different combinations of the encoder and decoder to find the best settings. For encoders, we compare DeiT, BEiT and the ResNet-50 network. Both the DeiT and BEiT are the base models in their original papers. For decoders, we compare the base decoders initialized by  and the large decoders initialized by . 
For further comparison, we also evaluate the CRNN baseline model and the Tesseract OCR in this section, while the latter is an open-source OCR Engine using the LSTM network.


Table~\ref{tab:ablation} shows the results of combined models. From the results, we observe that the BEiT encoders show the best performance among the three types of encoders while the best decoders are the  decoders. Apparently, the pre-trained models on the vision task improve the performance of text recognition models, and the pure Transformer models are better than the CRNN models and the Tesseract on this task. According to the results, we mainly use three settings on the subsequent experiments:  (total parameters=62M) consists of the encoder of  and the decoder of MiniLM,  (total parameters=334M) consists of the encoder of  and the decoder of ,  (total parameters=558M) consists of the encoder of  and the decoder of . In Table~\ref{tab:ablation2}, we have also done some ablation experiments to verify the effect of pre-trained model initialization, data augmentation, and two stages of pre-training. All of them have great improvements to the TrOCR models.

\begin{table*}[ht]
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Model}      &\textbf{Architecture}             & \textbf{Training Data}   & \textbf{External LM} & \textbf{CER} \\ \hline

\cite{bluche2017gated}   &   GCRNN / CTC     & Synthetic + IAM          & Yes                  & 3.2          \\
\cite{michael2019evaluating}& LSTM/LSTM w/Attn  & IAM                      & No                   & 4.87         \\ 
\cite{wang2020decoupled}  &    FCN / GRU    & IAM                      & No                   & 6.4          \\

\cite{kang2020pay}      &    Transformer w/ CNN     & Synthetic + IAM          & No                   & 4.67         \\

\cite{diaz2021rethinking} & S-Attn / CTC      & Internal + IAM           & No                  & 3.53      \\
\cite{diaz2021rethinking}  &  S-Attn / CTC    & Internal + IAM          & Yes                  & 2.75         \\
\cite{diaz2021rethinking}  &  Transformer w/ CNN    & Internal + IAM          & No                  & 2.96         \\
\hline
            &  Transformer        & Synthetic + IAM          & No                   & 4.22             \\
             &  Transformer        & Synthetic + IAM          & No                   & 3.42             \\
             &   Transformer      & Synthetic + IAM          & No                   & 2.89             \\
\hline
\end{tabular}
\caption{Evaluation results (CER) on the IAM Handwriting dataset.}
\label{tab:iam}
\end{table*}


\begin{table*}
\centering
\begin{tabular}{ccccccc}\hline
\textbf{Model}       & \textbf{Parameters} & \textbf{Total Sentences} & \textbf{Total Tokens} & \textbf{Time}   & \textbf{Speed \#Sentences} & \textbf{Speed \#Tokens} \\
\hline
 & 62M  & 2,915           & 31,081       & 348.4s & 8.37 sentences/s  & 89.22 tokens/s \\
  & 334M & 2,915           & 31,959       & 633.7s & 4.60 sentences/s  & 50.43 tokens/s \\
 & 558M & 2,915           & 31,966       & 666.8s & 4.37 sentences/s  & 47.94 tokens/s \\
\hline
\end{tabular}
\caption{Inference time on the IAM Handwriting dataset.}
\label{tab:infer}
\end{table*}


\begin{table*}[t]
  \small
\centering
  \scalebox{1.02}{
  \setlength\tabcolsep{4pt}
  \begin{tabular*}{0.76\linewidth}{ c c c c c c c c c c }
    \toprule
    \multicolumn{1}{c}{} & \multicolumn{8}{c}{\bf Test datasets and \# of samples} \\
    \cmidrule{2-9}
    \multirow{2}{*}{\bf Model} & \bf IIIT5k & \bf SVT &  \multicolumn{2}{c}{\bf IC13} &  \multicolumn{2}{c}{\bf IC15} & \bf SVTP & \bf CUTE \\
     & 3,000 & 647 & 857 & 1,015 & 1,811 & 2,077 & 645 & 288 \\
    \midrule
        PlugNet \cite{mou2020plugnet} & 94.4 & 92.3 & -- & 95.0 & -- & 82.2 & 84.3 & 85.0 \\
        SRN \cite{9156632} & 94.8 & 91.5 & 95.5 & -- & 82.7 & -- & 85.1 & 87.8 \\
        RobustScanner \cite{yue2020robustscanner} & 95.4 & 89.3 & -- & 94.1 & -- & 79.2 & 82.9 & 92.4 \\
        TextScanner \cite{wan2020textscanner} & 95.7 & 92.7 & -- & 94.9 & -- & 83.5 & 84.8 & 91.6 \\
        AutoSTR \cite{zhang2020autostr} & 94.7 & 90.9 & -- & 94.2 & 81.8 & -- & 81.7 & -- \\
        RCEED \cite{cui_rceed} & 94.9 & 91.8 & -- & -- & -- & 82.2 & 83.6 & 91.7 \\
        PREN2D \cite{Yan_2021_CVPR} & 95.6 & 94.0 & 96.4 & -- & 83.0 & -- & 87.6 & 91.7 \\
        VisionLAN \cite{Wang_2021_ICCV_visionlan} & 95.8 & 91.7 & 95.7 & -- & 83.7 & -- & 86.0 & 88.5 \\
        Bhunia \cite{Bhunia_2021_ICCV_joint} & 95.2 & 92.2 & -- & 95.5 & -- & 84.0 & 85.7 & 89.7 \\
        CVAE-Feed.\textsuperscript{1} \cite{Bhunia_2021_ICCV_towards} & 95.2 & -- & -- & 95.7 & -- & \textbf{84.6} & 88.9 & 89.7 \\
        STN-CSTR \cite{https://doi.org/10.48550/arxiv.2102.10884} & 94.2 & 92.3 & 96.3 & 94.1 & 86.1 & 82.0 & 86.2 & -- \\
        ViTSTR-B \cite{atienza2021vision} & 88.4 & 87.7 & 93.2 & 92.4 & 78.5 & 72.6 & 81.8 & 81.3 \\
        CRNN \cite{shi2016end} & 84.3 & 78.9 & -- & 88.8 & -- & 61.5 & 64.8 & 61.3 \\
        TRBA \cite{Baek_2021_CVPR} & 92.1 & 88.9 & -- & 93.1 & -- & 74.7 & 79.5 & 78.2 \\
        ABINet \cite{Fang_2021_CVPR} & 96.2 & 93.5 & 97.4 & -- & 86.0 & -- & 89.3 & 89.2 \\
        Diaz \cite{diaz2021rethinking} & 96.8 & 94.6 & 96.0 & -- & 80.4 & -- & -- & -- \\
        PARSeq \cite{bautista2022scene} & \textbf{97.0} & 93.6 & 97.0 & 96.2 & 86.5 & 82.9 & 88.9 & 92.2 \\
        MaskOCR (ViT-B) \cite{lyu2022maskocr} & 95.8 & 94.7 & 98.1 & - & 87.3 & - & 89.9 & 89.2 \\
        MaskOCR (ViT-L) \cite{lyu2022maskocr} & 96.5 & 94.1 & 97.8 & - & \textbf{88.7} & - & 90.2 & 92.7 \\
    \midrule
     (Syn) & 90.1 & 91.0 & 97.3 & 96.3 & 81.1 & 75.0 & 90.7 & 86.8  \\
         (Syn) & 91.0 & 93.2 & 98.3 & 97.0 & 84.0 & 78.0 & 91.0 & 89.6  \\
        \midrule
         (Syn+Benchmark) & 93.4 & 95.2 & 98.4 & \textbf{97.4} & 86.9 & 81.2 & 92.1 & 90.6  \\
        (Syn+Benchmark)& 94.1 & \textbf{96.1} & \textbf{98.4} & 97.3 & 88.1 & 84.1 & \textbf{93.0} & \textbf{95.1} \\
    \bottomrule
  \end{tabular*}
  }
  \caption{Word accuracy on the six benchmark datasets (36-char), where ``Syn'' indicates the model using synthetic data only and ``Syn+Benchmark'' indicates the model using synthetic data and benchmark datasets. }
  \label{tab:scene}
\end{table*}


\subsubsection{SROIE Task 2}

Table~\ref{tab:sroie} shows the results of the TrOCR models and the current SOTA methods on the leaderboard of the SROIE dataset.
To capture the visual information, all of these baselines leverage CNN-based networks as the feature extractors while the TrOCR models use the image Transformer to embed the information from the image patches. For language modeling, MSO Lab~\cite{sang2019improving} and CLOVA OCR~\cite{sang2019improving} use LSTM layers and H\&H Lab~\cite{shi2016end} use GRU layers while the TrOCR models use the Transformer decoder with a pure attention mechanism. According to the results, the TrOCR models outperform the existing SOTA models with pure Transformer structures. It is also confirmed that Transformer-based text recognition models get competitive performance compared to CNN-based networks in visual feature extraction and RNN-based networks in language modeling on this task without any complex pre/post-process steps.


\subsubsection{IAM Handwriting Database}
Table~\ref{tab:iam} shows the results of the TrOCR models and the existing methods on the IAM Handwriting Database. According to the results, the methods with CTC decoders show good performance on this task and the external LM will result in a significant reduction in CER. By comparing the methods \cite{bluche2017gated} with the TrOCR models, the  achieves a better result, which indicates that the Transformer decoder is more competitive than the CTC decoder in text recognition and has enough ability for language modeling instead of relying on an external LM.
Most of the methods use sequence models in their encoders after the CNN-based backbone except the FCN encoders in \cite{wang2020decoupled}, which leads to a significant improvement on CER. Instead of relying on the features from the CNN-based backbone, the TrOCR models using the information from the image patches get similar and even better results, illustrating that the Transformer structures are competent to extract visual features well after pre-training. From the experiment results, the TrOCR models exceed all the methods which only use synthetic/IAM as the sole training set with pure Transformer structures and achieve a new state-of-the-art CER of 2.89. Without leveraging any extra human-labeled data, TrOCR even gets comparable results with the methods in \cite{diaz2021rethinking} using the additional internal human-labeled dataset. 

\subsubsection{Scene Text Datasets}
In Table~\ref{tab:scene}, we compare the  and  models of fine-tuning with synthetic data only and fine-tuning with synthetic data and benchmark datasets (the training sets of IC13, IC15, IIIT5K, SVT) to the popular and recent SOTA methods. 
Compared to all, the TrOCR models establish five new SOTA results of eight experiments while getting comparable results on the rest. Our model underperforms on the IIIT5K dataset, and we find some scene text sample images contain symbols, but the ground truth does not. It is inconsistent with the behavior in our pre-training data (retaining symbols in ground truth), causing the model to tend still to process symbols. There are two kinds of mistakes: outputting symbols but truncating the output in advance to ensure that the number of wordpieces is consistent with the ground truth, or identifying symbols as similar characters.

\subsubsection{Inference Speed}
Table~\ref{tab:infer} shows the inference speed of different settings TrOCR models on the IAM Handwriting Database. We can conclude that there is no significant margin in inference speed between the base models and the large models. In contrast, the small model shows comparable results for printed and handwriting text recognition even though the number of parameters is an order of magnitude smaller and the inference speed is as twice as fast. The low number of parameters and high inference speed means fewer computational resources and user waiting time, making it more suitable for deployment in industrial applications.







\section{Related Work}


\subsection{Scene Text Recognition}
For text recognition, the most popular approaches are usually based on the CTC-based models. \cite{shi2016end} proposed the standard CRNN, an end-to-end architecture combined by CNN and RNN. The convolutional layers are used to extract the visual features and convert them to sequence by concatenating the columns, while the recurrent layers predict the per-frame labels. They use a CTC decoding strategy to remove the repeated symbols and all the blanks from the labels to achieve the final prediction. 
\cite{su2014accurate} used the Histogram of Oriented Gradient (HOG) features extracted from the image patches in the same column of the input image, instead of the features from the CNN network. A BiLSTM is then trained for labeling the sequential data with the CTC technique to find the best match.
\cite{gao2019reading} extracted the feature by the densely connected network incorporating the residual attention block and capture the contextual information and sequential dependency by the CNN network. They compute the probability distribution on the output of the CNN network instead of using an RNN network to model them. After that, CTC translates the probability distributions into the final label sequence. 

The Sequence-to-Sequence models \cite{zhang2020sahan, wang2019scene, sheng2019nrtr, bleeker2019bidirectional, lee2020recognizing, atienza2021vision} are gradually attracting more attention, especially after the advent of the Transformer architecture~\cite{vaswani2017attention}. 
SaHAN \cite{zhang2020sahan}, standing for the scale-aware hierarchical attention network, are proposed to address the character scale-variation issue. The authors use the FPN network and the CRNN models as the encoder as well as a hierarchical attention decoder to retain the multi-scale features. 
\cite{wang2019scene} extracted a sequence of visual features from the input images by the CNN with attention module and BiLSTM. The decoder is composed of the proposed Gated Cascade Attention Module (GCAM) and generates the target characters from the feature sequence extracted by the encoder.
For the Transformer models, \cite{sheng2019nrtr} first applied the Transformer to Scene Text Recognition. Since the input of the Transformer architecture is required to be a sequence, a CNN-based modality-transform block is employed to transform 2D input images to 1D sequences.
\cite{bleeker2019bidirectional} added a direction embedding to the input of the decoder for the bidirectional text decoding with a single decoder, while \cite{lee2020recognizing} utilized the two-dimensional dynamic positional embedding to keep the spatial structures of the intermediate feature maps for recognizing texts with arbitrary arrangements and large inter-character spacing. \cite{9156632} proposed semantic reasoning networks to replace RNN-like structures for more accurate text recognition.
\cite{atienza2021vision} only used the image Transformer without text Transformer for the text recognition in a non-autoregressive way.

The texts in natural images may appear in irregular shapes caused by perspective distortion. \cite{shi2016robust, baek2019wrong, litman2020scatter, shi2018aster, zhan2019esir} addressed this problem by processing the input images with an initial rectification step. For example, thin-plate spline transformation \cite{shi2016robust, baek2019wrong, litman2020scatter, shi2018aster} is applied to find a smooth spline interpolation between a set of fiducial points and normalize the text region to a predefined rectangle, while \cite{zhan2019esir} proposed an iterative rectification network to model the middle line of scene texts as well as the orientation and boundary of textlines. \cite{baek2019wrong, diaz2021rethinking} proposed universal architectures for comparing different recognition models. 


\subsection{Handwritten Text Recognition}
\cite{memon2020handwritten} gave a systematic literature review about the modern methods for handwriting recognition. Various attention mechanisms and positional encodings are compared in the \cite{michael2019evaluating} to address the alignment between the input and output sequence. The combination of RNN encoders (mostly LSTM) and CTC decoders \cite{bluche2017gated, graves2008offline, pham2014dropout} took a large part in the related works for a long time. Besides, \cite{graves2008offline, voigtlaender2016handwriting, puigcerver2017multidimensional} have also tried multidimensional LSTM encoders. Similar to the scene text recognition, the seq2seq methods and the scheme for attention decoding have been verified in \cite{michael2019evaluating, kang2020pay, chowdhury2018efficient, bluche2016joint}. \cite{ingle2019scalable} addressed the problems in building a large-scale system.


\section{Conclusion}

In this paper, we present TrOCR, an end-to-end Transformer-based OCR model for text recognition with pre-trained models. Distinct from existing approaches, TrOCR does not rely on the conventional CNN models for image understanding. Instead, it leverages an image Transformer model as the visual encoder and a text Transformer model as the textual decoder. Moreover, we use the wordpiece as the basic unit for the recognized output instead of the character-based methods, which saves the computational cost introduced by the additional language modeling. Experiment results show that TrOCR achieves state-of-the-art results on printed, handwritten and scene text recognition with just a simple encoder-decoder model, without any post-processing steps. 



\bibliography{aaai23}

\end{document}
