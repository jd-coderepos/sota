

In this section, we give a theoretical analysis of local search for NAS,
including a complete characterization of its performance.
We present a general result which can be applied to any NAS search space.
We also give an experimental validation of our results at the end of the
section, which suggests that our theoretical results predict the performance
of real datasets reasonably well.

In a NAS application,
the topology of the search space is fixed and discrete, 
while the distribution of validation losses
is randomized and continuous,
due to the non-deterministic nature of training a neural network.
Therefore, we assume that the validation loss for a trained architecture 
is sampled from
a global probability distribution, and for each architecture, 
the validation losses of its neighbors are sampled from a local probability distribution.
Recall the definitions of  and  from the end of
Section~\ref{sec:prelim}.
Given a graph , each node  has a loss 
 sampled from a PDF which we denote by . 
For any two neighbors , the PDF for the validation loss  
of architecture  is given by .
Choices for the distribution  are constrained by the 
fixed topology of the search space, as well as the distribution .
In Appendix~\ref{app:method}, 
we discuss this further by formally defining
measurable spaces for all random variables in our framework.

Our main result is a formula for the fraction of nodes in the search space which 
are local minima, as well as a formula for the fraction of nodes  
such that the loss of  
is within  of the loss of the global optimum, for all 
In other words, we give a formula for the probability that the local search algorithm 
outputs a solution that is close to optimal.
Note that such a formula characterizes the performance of local search.
We give the full proofs for all of our results 
in Appendix~\ref{app:method}.
For the rest of this section, 
we assume for all , , and
we assume  is vertex transitive
(given , there exists an automorphism of  which maps  to ).
Let  denote the architecture with the global minimum loss, therefore
the support of the distribution of validation losses is a subset of  
That is,  
Technically, the integrals in this section are Lebesgue integrals. However, we use the more standard Riemann-Stieltjes notation for clarity. We also slightly abuse notation and
define  when 
In the following statements, we assume there is a fixed graph , and the 
validation accuracies are randomly assigned from a distribution defined by  
and . Therefore, the expectations are over the random draws from  and .
\footnote{
In particular, given a node  with validation loss 
the probability distribution for the validation loss of a neighbor depends only 
on  and , which makes the local search procedure similar to a 
Markov process.  Our experiments in Figure~\ref{fig:ls_baselines_201}
suggest this is a reasonable assumption in practice.
}


\begin{restatable}{rethm}{probopt}\label{thm:prob_opt}
Given , , , , , and ,


\end{restatable}


\begin{proof}[\textbf{Proof sketch.}]



To prove the first statement,
we introduce an indicator random variable to test if the 
architecture is a local minimum:  Then

Intuitively, in the proof of the second statement, we follow similar reasoning but multiply
the probability in the outer integral by the expected size of 's full preimage to weight
the integral by the probability a random point will converge to .
Formally, we introduce an indicator random variable on the architecture space 
that tests if a node will terminate on a local minimum that is
within  of the global minimum:

We use this random variable along with the first statement of the theorem,
to prove the second statement.

\end{proof}

In Appendix~\ref{app:method}, we use Theorem~\ref{thm:prob_opt}
along with Chebyshev's Inequality~\citep{chebyshev1867valeurs}
to show that, in the case where the validation accuracy of each architecture
has Gaussian noise, the expected number of local minima can be bounded in terms of
the standard deviation of the noise.
In the next lemma, we derive a recursive equation for 
We define the \emph{branching fraction} of graph  as ,
where  denotes the set of nodes which are distance  to  in .
For example, the branching fraction of a tree with degree  is  for all ,
and the branching fraction of a clique is  and  for all 
One more example is as follows.
In Appendix~\ref{app:experiments}, 
we show that the neighborhood graph of the
NASBench-201 search space is  and therefore its branching factor is


\begin{restatable}{relem}{geneqns}\label{lem:gen_eqns}
Given , , , , and ,
then for all , we have the following equations.


\end{restatable}

For some PDFs, it is not possible to find a closed-form solution
for  because arbitrary 
functions may not have closed-form antiderivatives.
By assuming there exists a function  such that  for all ,
we can use induction to find a closed-form expression for .
This includes the uniform distribution ( for ),
as well as distributions that are polynomials in .
In Appendix~\ref{app:method}, 
we use this to show that 
can be approximated by 
where 
Now we use a similar technique to give a closed-form expression for Theorem~\ref{thm:prob_opt}
when the local and global distributions are uniform.
We stress that this lemma is simply an application of Lemma~\ref{lem:gen_eqns},
and our main results (Theorem~\ref{thm:prob_opt} and Lemma~\ref{lem:gen_eqns})
hold without any assumptions on the local and global distributions.

\begin{restatable}{relem}{fulluniform}\label{lem:full_uniform}
If ,
then  and

\end{restatable}

\begin{proof}[\textbf{Proof sketch.}]
The probability density function of  is equal to 1 on  and 0 otherwise.
Then 
We use this in combination with Theorem~\ref{thm:prob_opt} to prove the first statement:


To prove the second statement, first
we use induction on the expression in Lemma~\ref{lem:gen_eqns}
to show that for all , 

We plug this into the second part of Theorem~\ref{thm:prob_opt}:

\end{proof}

In the next section, we show that our theoretical results can be used to
predict the performance of local search.




\begin{figure}
\centering \includegraphics[width=0.42\textwidth]{fig/global_histogram_cifar100.pdf}
\includegraphics[width=0.42\textwidth]{fig/global_histogram_ImageNet16-120.pdf}
\caption{
Histogram of validation losses for CIFAR-100 (top) and ImageNet16-120 (bottom) 
in NASBench-201,
fitted with the best values of  and  in Equation~\ref{eq:normal_pdf}.
}
\label{fig:single_histogram}
\end{figure}

\begin{figure*}
\centering \includegraphics[width=0.32\textwidth]{fig/cifar10_local_pdf.pdf}
\includegraphics[width=0.32\textwidth]{fig/cifar100_local_pdf.pdf}
\includegraphics[width=0.32\textwidth]{fig/imagenet16_local_pdf.pdf}
\caption{
Probability density function for CIFAR-10, CIFAR-100, and
ImageNet16-120 on NASBench-201. For each coordinate , a darker color indicates
that architectures with accuracy  and  are more likely to be neighbors.
}
\label{fig:201_local_pdfs}
\end{figure*}

\paragraph{Simulation Results.}
We run a local search simulation using the equations in
the previous section
as a means of experimentally validating our theoretical results
with real data (we use NASBench-201).
In order to use these equations, first we must approximate the local
and global probability density functions of the three 
datasets in NASBench-201.
We note that approximating these distributions are not feasible for large search
spaces; the purpose of our theoretical results are meant only to provide a deeper
understanding of local search and lay the groundwork for future studies.
We start by visualizing the probability density functions of the three datasets.
See Figure~\ref{fig:201_local_pdfs}.
We see the most density along the diagonal, meaning that architectures
with similar accuracy are more likely to be neighbors.
Therefore, we can approximate the PDFs by using the following equation:










This is a normal distribution with mean  
and standard deviation , truncated so that it is a valid PDF
in 
We note that prior work has also modeled architecture accuracies in NAS with
a normal distribution~\citep{real2019regularized}.
To model the global PDF for each dataset,
we plot a histogram of the validation losses and match them to the closest-fitting
values of  and .
See Figure~\ref{fig:single_histogram}.
The best values of  are  and  for CIFAR-10, CIFAR-100, 
and ImageNet16-120, respectively, and the best values for  are all .
To model the local PDF for each dataset, we compute
the random walk autocorrelation (RWA) on each dataset.
RWA is defined as the autocorrelation of the accuracies of points visited during a
random walk on the neighborhood 
graph~\citep{weinberger1990correlated, stadler1996landscapes},
and was used to measure locality in NASBench-101 in prior work~\citep{nasbench}.
For the full details of the steps taken to model the datasets in NASBench-201, 
see Appendix~\ref{app:experiments}.






Now we use Theorem~\ref{thm:prob_opt} to compute the 
probability that a randomly drawn architecture will converge to within 
of the global minimum when running local search.
Since there is no closed-form solution for the expression in Lemma~\ref{lem:gen_eqns},
we compute Theorem~\ref{thm:prob_opt} up to the 5th preimage.
We compare this to the experimental results on NASBench-201.
We also compare the performance of the NASBench-201 search space with validation losses drawn uniformly at random, 
to the performance predicted by Lemma~\ref{lem:full_uniform}.
Finally, we compare the preimage sizes of the architectures in NASBench-201 with
randomly drawn validation losses to the sizes predicted in Lemma~\ref{lem:gen_eqns}.
See Figure~\ref{fig:ls_baselines_201}.
Our theory exactly predicts the performance and the preimage sizes of the uniform 
random NASBench-201 dataset.
On the three image datasets, our theory predicts the performance
fairly accurately, but is not perfect due to our assumption that the distribution
of accuracies is unimodal.

