\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage[vlined,noend,boxed,algoruled]{algorithm2e}

\setlength{\algomargin}{9pt}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\newcommand{\tab}{\hspace{5mm}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{property}[theorem]{Property}
\newtheorem{definition}{Definition}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\BO}{\mathcal{O}}
\newcommand{\Var}{\operatorname{Var}}

\begin{document}

\title{Trade-offs between Selection Complexity and Performance when Searching the Plane without Communication}
\date{}
\author{
  Christoph Lenzen\\
  \texttt{clenzen@csail.mit.edu}
  \and
  Nancy Lynch\\
  \texttt{lynch@csail.mit.edu}
  \and
  Calvin Newport\\
  \texttt{cnewport@cs.georgetown.edu}
  \and
  Tsvetomira Radeva\\
  \texttt{radeva@csail.mit.edu}
}
\maketitle

\begin{abstract}
We consider the ANTS problem [Feinerman et al.] in which a group of agents collaboratively search for a target in a two-dimensional plane. Because this problem is inspired by the behavior of biological species, we argue that in addition to studying the {\em time complexity} of solutions it is also important to study the {\em selection complexity}, a measure of how likely a given algorithmic strategy is to arise in nature due to selective pressures. In more detail, we propose a new selection complexity metric , defined for algorithm  such that , where  is the number of memory bits used by each agent and  bounds the fineness of available probabilities (agents use probabilities of at least ). In this paper, we study the trade-off between the standard performance metric of speed-up, which measures how the expected time to find the target improves with , and our new selection metric. 

In particular, consider  agents searching for a treasure located at (unknown) distance  from the origin (where  is sub-exponential in ). For this problem, we identify  as a crucial threshold for our selection complexity metric. We first prove a new upper bound that achieves a near-optimal speed-up of  for . In particular, for , the speed-up is asymptotically optimal. By comparison, the existing results for this problem [Feinerman et al.] that achieve similar speed-up require . We then show that this threshold is tight by describing a lower bound showing that if , then with high probability the target is not found within  moves per agent. Hence, there is a sizable gap to the straightforward  lower bound in this setting. 


\end{abstract}


\thispagestyle{empty}
\setcounter{page}{0}

\pagebreak


\section{Introduction}
\label{sec:intro}


It is increasingly accepted by some biologists and computer scientists that the tools of distributed computation can improve our understanding of distributed biological processes~\cite{feinerman12disc, feinerman13, feinerman12podc}. A standard approach is to translate a biological process of interest (e.g., ant foraging~\cite{feinerman12disc, feinerman12podc} or sensory organ pre-cursor selection~\cite{afek11}) into a formal problem in a distributed computing model, and then prove upper and lower bounds on the problem. The aim is to use these bounds to gain insight into the behavior of the motivating biological process.

A recognized pitfall of this approach is \emph{incongruous analysis}, in which the theoretician focuses on metrics relevant to computation but not biology, or ignores metrics relevant to biology but not to computation.  Motivated by this pitfall, this paper promotes the use of {\em selection complexity} metrics for studying biologically-inspired distributed problems. Unlike standard metrics from computation, which tend to focus only on performance, selection complexity metrics instead attempt to measure the difficulty of a given algorithmic strategy arising in nature as the result of selective pressures. Roughly speaking, a solution with low selection complexity should be more likely to arise in nature than a solution with high selection complexity.

We argue that theoreticians studying biologically-inspired problems should evaluate solutions in terms of selection complexity in addition to focusing on standard performance metrics; perhaps even measuring the trade-off between the two classes of metrics. This paper provides a case study of this approach by fixing a standard biology-inspired problem and new selection complexity metric, and then bounding the trade-off between performance and selection complexity with respect to this metric. In doing so, we also obtain results regarding concurrent non-uniform random walks that are of independent mathematical interest.

We recognize that most papers on biology-inspired distributed problems implicitly address selection complexity in their fixed model constraints. Restricting agents to not have access to communication in the search problem, for example, is a constraint that likely lowers the selection complexity of solutions in the model. What is new about our approach is that we are capturing such complexity in a variable metric, allowing us to study the trade-offs between algorithmic power and performance more generally. This can provide insights beyond those gained by characterizing the capabilities of a given static set of constraints.


In this paper, we focus on the problem of  probabilistic non-communicating agents collaboratively searching for a target in a two-dimensional grid placed at (unknown) distance  (measured in number of hops in the grid) from the origin. We assume that  is sub-exponential in .\footnote{Note that an exponential number of agents finds the target quickly even if they employ simple random walks.} This problem is described and analyzed in recent work by Feinerman et al.~\cite{feinerman12podc} (referred to as the ANTS problem). The authors in ~\cite{feinerman12podc} argue that it provides a good approximation of insect foraging, and represents a useful intersection between biological behavior and distributed computation. The analysis in~\cite{feinerman12podc} focuses on the {\em speed-up} performance metric, which measures how the expected time to find the target improves with . The authors describe and analyze search algorithms that closely approximate the straightforward  lower bound for finding a target placed at distance  from the origin.
 
 \paragraph{Selection metric motivation.} We consider the selection complexity metric , which captures the bits of memory and probabilistic range used by a given algorithm. This combined metric is motivated by the fact that memory can be used to simulate small probability values, and such values give more power to algorithms, e.g.\ permitting longer directed walks with a given amount of memory. In more detail, for algorithm , we define , where  is the number of bits of memory required by the algorithm (note, , where  is the state set of the state machine representation of ), and  is the smallest value such that all probabilities used in  are bounded from below by . In Section \ref{sec:upper} and Section \ref{sec:lower}, we show that the choice of the selection metric arises naturally from the analysis of our algorithms and the lower bound.

We conjecture that, from a biological point of view, it is reasonable to assume that large values of  are associated with higher selection complexity. Clearly, algorithms relying on small probabilities are more sensitive to additive disturbances of the probability values. Hence, creating a small probability based on a single event is harder to accomplish, since the  event must not only have a strong bias towards one outcome, but also be well protected against influencing factors (like temperature, noise, etc.). On the other hand, using multiple independent events to simulate one with larger bias (also known as probability boosting) constitutes a hidden cost. Our model and algorithms make this cost explicit, by accounting for it in terms of the memory needed for counting such events.

\paragraph{Results.}
In this paper, we generalize the problem of~\cite{feinerman12podc} by now also considering the selection complexity metric . 
We identify , for target distance , as a crucial threshold for the  metric when studying the achievable speed-up in the foraging problem. In more detail, our lower bound proves that for any algorithm  such that , there is a placement of the treasure at distance  such that the probability that  finds the treasure in less than  moves per agent is polynomially small in , and the probability of finding a target placed randomly within this distance is . The {\em speed-up} in this case is bounded from above by , as opposed to the optimal speed-up of . At the core of our lower bound is a novel analysis of recurrence behavior of small Markov chains with probabilities of at least .

Concerning upper bounds, we note that the foraging algorithms in~\cite{feinerman12podc} achieve near-optimal speed-up in , but their selection complexity, as measured by , is higher than the  threshold identified by our lower bound: these algorithms require sufficiently fine-grained probabilities and enough memory to randomly generate and store, respectively, coordinates up to distance at least  from the origin; this entails . In this paper, we seek upper bounds that work for , the minimum value for which good speed-up is possible. With this in mind, we begin by describing and analyzing a very simple algorithm that is non-uniform in  (agents know the value of ) and has asymptotically optimal expected running time. It illustrates our main ideas of walking up to certain points in the plane while counting approximately, thus using little memory, and showing that this is sufficient for searching the plane efficiently. This algorithm uses a value of , which matches our lower bound result for  up to factor .

We generalize the ideas used in our simple algorithm to derive a solution that is uniform in . The main idea is to start with some estimate of  and keep increasing it while executing a corresponding version of our simple search algorithm described above for each such estimate. Our uniform algorithm solves the problem in   moves per agent in expectation (if , the algorithm matches the  lower bound), for . We remark that the increased running time is due to the fact that in order to keep the value of  small, we increase our estimate of  by a factor of  in each step, which may result in ``overshooting'' the correct distance by factor . Note that this suboptimal expected running time arises from enforcing  memory bits; otherwise, one is always free to use the only constant probabilities.


  \paragraph{Discussion.}
  An interesting question that arises from our results is the trade-off between  and  in the definition of : roughly speaking, more bits of memory might be of greater utility than having access to smaller probabilities. This seems intuitive given that smaller probability values can be simulated using additional memory  (e.g., to simulate a coin that returns heads with probability , flip a uniform coin  times while storing the number of coin tosses in the additional memory), but in general more precise probabilities cannot be used to simulate additional memory.
  
From a biological perspective, we do not claim that  is necessarily the {\em right} selection metric to use in studying such problems. We chose it because  and  seem to be important factors in search, and  they are potentially difficult to increase in nature. However, we recognize that the refinement and validation of such metrics require close collaboration with evolutionary biologists. In this paper, our main goal is to advertise the selection complexity approach as a promising tool for studying biology-inspired problems.

From a mathematical perspective, we emphasize that our lower bound result, in particular, is of independent interest. It is known that uniform random walks do not provide substantial speed-up in the plane searching problem~\cite{alon08}; the speed-up is bounded by . Our lower bound generalizes this observation from uniform random walks to probabilistic processes with bounded probabilities and small state complexities. 
 
\paragraph{Related Work.}

This work was initially inspired by the results in \cite{feinerman12disc} and \cite{feinerman12podc}, which originally introduced the problem studied here. More precisely, in \cite{feinerman12podc} the authors present an algorithm to find the target in optimal expected time , assuming that each agent in the algorithm knows the number  of agents (but not ). For unknown , they show that for every constant , there exists a uniform search algorithm that is -competitive, but there is no uniform search algorithm that is -competitive. In \cite{feinerman12disc}, Feinerman et al. provide multiple lower bounds on the advice size (number of bits of information the ants are given prior to the search), which can be used to store the value , some approximation of it, or any other information. In particular, they show that in order for an algorithm to be -competitive, the ants need advice size of  bits. Note that this result also implies a lower bound of  bits on the total size of the memory of the ants, but only under the condition that optimal speed-up is required. Our lower bound is stronger in that we show that there is an exponential gap of  for the maximum speed-up (with a sub-exponential number of agents in ). Similarly, the algorithms in \cite{feinerman12podc} need at least  bits of memory, as contrasted with our algorithm that uses  bits of memory. 

 Searching and exploration of various types of graphs by single and multiple agents are widely studied in the literature. Several works study the case of a single agent exploring directed graphs \cite{albers00, bender98, deng90}, undirected graphs \cite{panaite19, reingold05}, or trees \cite{diks02, gasieniec07}. Out of these, the following papers have restrictions on the memory used in the search: \cite{gasieniec07} uses  bits to explore an -node tree, \cite{bender98} studies the power of a pebble placed on a vertex so that the vertex can later be identified, \cite{diks02} shows that  bits of memory are needed to explore some -node trees, and \cite{reingold05} presents a -space algorithm for -connectivity. There have been works on graph exploration with multiple agents \cite{alon08, emek13, fraigniaud06}; while \cite{alon08} and \cite{fraigniaud06} do not include any memory bounds, \cite{emek13} presents an optimal algorithm for searching in a grid with constant memory and constant-sized messages in a model, introduced in \cite{emek13podc}, of very limited computation and communication capabilities. It should be noted that even though these models restrict the agents' memory to very few bits, the fact that the models allow communication makes it possible to simulate larger memory. 
 
 So far, in the above papers, we have seen that the metrics typically considered by computer scientists in graph search algorithms are mostly the amount of memory used and the running time. In contrast, biologists look at a much wider range of models and metrics, more closely related to the physical capabilities of the agents. For example, in \cite{arbilly10} the focus is on the capabilities of foragers to learn about different new environments, \cite{giraldeau00} considers the physical fitness of agents and the abundance and quality of the food sources, \cite{harkness85} considers interesting navigational capabilities of ants and assumes no communication between them, \cite{holder87} measures the efficiency of foraging in terms of the energy over time spent per agent, and \cite{robinson05} explores the use of different chemicals used by ants to communicate with one another.

\paragraph{Organization.}

In Section \ref{sec:model}, we present our system model assumptions and formally define the search problem and both the performance and selection metrics that we use to evaluate our algorithms. In Section \ref{sec:upper}, we present our algorithms, starting with a very simple non-uniform algorithm in Section \ref{sec:non-uniform} illustrating our main approach. In Section \ref{sec:uniform}, we generalize this approach to algorithms that are uniform in . In Section \ref{sec:lower}, we present a lower bound that matches our upper bounds in terms of the selection metric . We conclude by discussing some assumptions and possible extensions of our work in Section \ref{sec:discussion}. The appendix contains some definitions and math preliminaries used throughout the technical sections of the paper.

\section{Model}
\label{sec:model}

Our model is similar to the models considered in \cite{feinerman12disc, feinerman12podc}. We consider an infinite two-dimensional square grid with coordinates in . The grid is to be explored by  identical, non-communicating, probabilistic agents. Each agent is always located at a point on the grid. Agents can move in one of four directions, to one of the four adjacent grid points, but they have no information about their current location in the grid. Initially all agents are positioned at the origin. We also assume that an agent can return to the origin, and for the purposes of this paper, we assume this action is based on information provided by an oracle. In this case, the agent returns on a shortest path in the grid that keeps closest to the straight line connecting the origin to its current position. Note that the return path is at most as long as the path of the agent away from the origin; therefore, since we are interested in asymptotic complexity, we ignore the lengths of the return paths in our analysis. Next, we give a formal description of our model.

\paragraph{Agents.}
Each agent is modeled as a probabilistic finite state automaton; since agents are identical, so are their state automata. Each automaton is a tuple , where  is a set of states, state  is the unique starting state, and  is a transition function  , where  is a set of discrete probability distributions. Thus,  maps each state  to a discrete probability distribution  on , which denotes the probability of moving from state  to any other state in . 

For our lower bound in Section \ref{sec:lower}, it is convenient to use a Markov chain representation of each agent. Therefore, we can express each agent as a Markov chain with transition matrix , such that for each , , and start state . 

In addition to the Markov chain that describes the evolution of an agent's state, we also need to characterize its movement on the grid. We define a  labeling function  mapping each state  to an action the agent performs on the grid. For simplicity, we require origin. Using this labeling function, any sequence of states  is mapped to a sequence of moves in the grid  where none denotes no move in the grid (i.e.,  does not contribute to the derived sequence of moves) and origin means that the agent returns to the origin, as described above. 

\paragraph{Executions.}
An execution of an algorithm for some agent is given by a sequence of states from , starting with state , and coordinates of the associated movements on the grid derived from these states. Formally, an execution is defined as , where  is the start state, , and for each , applying the move  to point  results in point . For example, if , then  and . For the case where , we define  and , and for , we define . In other words, we ignore the movement of the agent on the way back to the origin, as mentioned earlier in this section. 

An execution of an algorithm with  agents is just an -tuple of executions of single agents. For our analysis of the lower bound, it is useful to assume a synchronous model. So, we define a \emph{round} of an execution to consist of one transition of each agent in its Markov chain. Note that we do not use such synchrony for our algorithms.

So far, we have described a linear execution of an algorithm with  agents. In order to consider probabilistic executions, note that the Markov chain  induces a probability distribution of executions in a natural way, by performing an independent random walk on  with transition probabilities given by  for each of the  agents.

\paragraph{Problem Statement.}
The goal is to find a target located at some vertex at distance (measured in terms of the max-norm) at most  from the origin in as few expected moves as possible. Note that measuring paths in terms of the max-norm gives is a constant-factor approximation of the actual hop distance. We will consider both uniform and non-uniform algorithms with respect to ; that is, the agents may or may not know the value of . 

It is easy to see (also shown in \cite{feinerman12podc}) that the expected running time is  even if agents know  and  and they can communicate with each other. This bound can be matched if the agents know a constant-factor approximation of  \cite{feinerman12podc}, but as mentioned in Section \ref{sec:intro}, the value of the selection metric  (introduced below) in that specific algorithm is . For simplicity, throughout this paper we will consider algorithms that are non-uniform in , i.e., the agents' state machine depends on . We can apply a technique from \cite{feinerman12podc}, that the authors use to make their algorithms uniform in , in order to generalize our results and obtain an algorithm that is uniform in both  and .


\paragraph{Metrics. } For the problem defined above, we consider both a performance and a selection metric and study the trade-off between the two. 
We will use the term \emph{step} of an agent interchangeably with a transition of the agent in the Markov chain. We define a \emph{move} of the agent to be a step that the agent performs in its Markov chain resulting in a state labeled up, down, left, or right. 

For our performance metric, we focus on the asymptotic running time in terms of  and ; more precisely, we are interested in the expected value of the metric : the minimum over all agents of the number of moves of the agent until it finds the target. Note that for the performance metric we exclude states labeled \emph{none} and \emph{origin} in an execution of an agent; we consider the \emph{none} states to be part of an agent's local computation, and we already argued that the \emph{origin} states increase the running time by at most a factor of two. For our lower bound, it is useful to define a similar metric in terms of the steps of an agent. We define the metric  to be the minimum over all agents of the number of steps of the agent until it finds the target.

The selection metric of a state automaton (and thus a corresponding algorithm) is defined as , where  is the number of bits required to encode all states from  and  is a lower bound on , the smallest non-zero probability value used by the algorithm. We further motivate this choice in Section \ref{sec:upper}, where we describe different trade-offs between the performance metric and the values of  and .

\section{Algorithms}
\label{sec:upper}
In this section, we begin by describing a non-uniform algorithm in  that finds the target in asymptotically optimal time. The main purpose for presenting this algorithm is to illustrate our main techniques in a very simple setting. This algorithm uses probability values of the form , which can easily be simulated using only biased coins that show heads with probability  for any  such that  is an integer multiple of . We show that the target can be found in asymptotically optimal time using  bits of memory. 

We then generalize this algorithm to work for the case of unknown . This ensures that closer targets are found faster by the algorithm than targets that are far away. The way we achieve this is by starting with an estimate of  equal to  and repeatedly increasing it until the target is found. For each such estimate we execute the non-uniform algorithm. Since  is not known by the algorithm anymore, we cannot easily pick fixed values for some of the parameters we use in the algorithm in order to guarantee asymptotically optimal results for all possible values of . Therefore, in our general algorithm the expected number of moves for the first agent to find the target becomes  for . Hence, for  the algorithm is asymptotically optimal with respect to both metrics, and we achieve non-trivial speed-up of  for any  (i.e.,  bits of memory).

\subsection{Non-uniform Algorithm}
\label{sec:non-uniform}

In this section we present an algorithm in which the value of  is available to the algorithm. We assume that ; the cases of  and  are straightforward. Our general approach is the following: each agent chooses a vertical direction (up or down) with probability , walks in that direction for a random number of steps that depends on , then does the same for the horizontal direction, and finally returns to the origin and repeats this process. We show that the minimum over all agents of the expected number of moves of the agent to find a target at distance up to  from the origin is at most . 

Let coin  denote a coin that shows tails with probability . Using this convention, the pseudocode of this simple routine is given in Algorithm~\ref{algo:non-uni-loop}, accompanied by a state machine representation showing that the algorithm can be implemented using only three bits of memory. Later in this section we show that a slightly modified version of the algorithm guarantees that .

\begin{figure}[t]
\begin{minipage}{\textwidth}
\begin{minipage}{.44\textwidth}
\begin{algorithm}[H]
\caption{Non-uniform search.}
\label{algo:non-uni-loop}
\While{\text{true}}{
	\If{coin  shows heads}{
  		\While{coin  shows heads}{
  			move up\phantom{Xp}
		}
	}
	\Else{
  		\While{coin  shows heads}{
  			move down\phantom{Xp}
		}
	}
	\If{coin  shows heads}{
  		\While{coin  shows heads}{
  			move left\phantom{Xp}
		}
	}
	\Else{
  		\While{coin  shows heads}{
  			move right\phantom{Xp}
		}
	}
	return to the origin
  }
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{.40\textwidth}
\begin{center}
\scalebox{.8}{
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto, node distance=3.6cm, semithick]
  \tikzstyle{every state}=[fill=none,draw=black,text=black, minimum size=1.2cm]

  \node[state] (A) {origin};
  \node[state] (B) [above of=A] {up};
  \node[state] (D) [below of=A] {down};
  \node[state] (C) [right of=A] {right};
  \node[state] (E) [left of=A] {left};

  \path (A) edge [loop above] node {} (A)
            edge [bend left] node {} (B)
            edge [bend left] node {} (D)
            edge [bend left] node {} (C)
			edge [bend left] node {} (E)            
            
        (B) edge [loop above] node [right,shift={(.2cm,-.3cm)}] {} (B)
        	edge [bend left] node {} (A)
        	edge [bend left] node [above,xshift=.3cm] {} (C)
        	edge [bend right] node [above,xshift=-.3cm] {} (E)
           
        (C) edge [loop right] node [above,yshift=.3cm] {} (A)
            edge [bend left]  node {} (A)
        
        (D) edge [loop below] node [right,shift={(.2cm,.3cm)}] {} (D)
        	edge [bend left] node {} (A)
        	edge [bend right] node [below,xshift=.3cm] {} (C)
        	edge [bend left] node [below,xshift=-.3cm] {} (E)
            
        (E) edge [loop left] node [above,yshift=.3cm] {} (A)
            edge [bend left]  node {} (A);
\end{tikzpicture}
}

State machine representation of Algorithm~\ref{algo:non-uni-loop}. State names match the values of the labeling function.

\end{center}
\end{minipage}
%
 \end{minipage}
\end{figure}


Denote by  the expected number of moves for an agent to complete an iteration of the outer loop of the algorithm. By independence of the random choices, this expectation does not depend on the considered iteration. Also, since agents are identical and independent, the value of  does not depend on the choice of agent either.
\begin{lemma}
\label{lem:exp_D}
.
\end{lemma}
\begin{proof}
In each iteration, an agent performs one move up or down for each consecutive toss of coin  showing heads, and then one move right or left for each consecutive toss of coin  showing heads. Each of these walks is  steps long in expectation, so it follows that .
\end{proof}

One may think that all we need to do now is to compute the expected number of iterations until some agent finds the target and multiply that by the expected number of moves to complete an iteration. However, this does not quite work, because the time to complete an iteration is not independent of whether the target is found or not. For instance, if the target is located at , then an agent finding the target cannot move up or down before going right, a constraint that decreases the expected number of steps taken in the iteration. 


However, since in each iteration an agent is, in fact, quite unlikely to find the target, the expectation cannot be affected a lot. For simplicity, we will use a fairly loose bound; we are interested in the asymptotic time complexity only, which is not affected. To this end, denote by  the expected number of moves on the grid an agent makes during an iteration conditioning on the event that the agent does not find the target in this iteration. Note that this event is only defined if the target is not located at the origin  and is automatically found right away; without loss of generality, we will assume that this is not the case.

\begin{lemma}
\label{lem:condition}
	 .
\end{lemma}
\begin{proof}

First, we bound the probability of an agent not finding the target in a given iteration of the main loop from below. Suppose that the target is located at . If , the pseudocode shows that with probability  coin  shows tails, so the agent does not move up, and consequently, with probability at least  it does not find the target in this iteration. Symmetrically, the agent does not move down with probability  and misses the target if , and it does not move left or right with probability  each and misses the target if . Overall, the target is missed in a given iteration with probability at least . We partition the probability space into the events: (1) the target is found during the given iteration, and (2) the target is not found during the given iteration. From the law of total expectation applied to this partition, it follows that 


In the last step, we bound the value of  by ignoring the first term in the sum, and using the fact above that the probability to miss the target in any given iteration is at least . We conclude that .
\end{proof}


\begin{lemma}
\label{lem:ria}
Let  be the expected number of moves until a fixed agent  finds the target in iteration , conditioning on the fact that agent  finds the target in iteration  and not in any previous iteration. Then, .
\end{lemma}
\begin{proof}
 Since we are conditioning on the fact that agent  finds the target in iteration  and not in any previous iteration, we know that agent  completes the first  iterations of the main loop and then moves to the target. The agent requires at most  moves to complete the first  iterations of the loop in expectation because we know the target is not found by  agent  in any of these iterations. Afterwards, in iteration , it will move to the target, which takes at most  moves. Thus, by Lemmas~\ref{lem:exp_D} and~\ref{lem:condition}, 


\end{proof}

Having examined the expected number of moves for a single agent to complete an iteration, next we calculate how likely it is that all agents miss the target in a single iteration of the main loop. In the following lemmas and theorems we  switch from considering a probability distribution for one agent to considering a probability distribution for all  agents.

\begin{lemma}
\label{lem:iterations}
	Denote by  the probability that no agent finds the target when each agent executes one iteration of the main loop. It holds that , where  is the total number of agents.
\end{lemma}

\begin{proof}
	Suppose the target is positioned at grid point  and consider a single agent performing an iteration of its main loop. With probability , it moves up and right. The walk up will halt after exactly  steps with probability

The walk right will perform  steps with probability at least . Hence, in each iteration, each agent finds the target with probability at least . Analogously, the same holds for a target located at , , and . 

Since iterations performed by different agents are independent of each other, it follows that . We use the binomial expansion of the right hand side for the case when :



For the case where , we approximate:



Therefore, we conclude that .

\end{proof}

\begin{theorem}
\label{thm:upper}
Let each of  agents execute a copy of Algorithm~\ref{algo:non-uni-loop}. The minimum over all agents of the expected number of moves of an agent to find a target within distance  from the origin is .
\end{theorem}
\begin{proof}
For , denote by  the event that some agent finds the target in iteration  of the main loop, but no agent finds it in any previous iteration . 
Denote by  the probability that no agent finds the target in iteration  of the main loop. 
Note that the events  are mutually exclusive. From the independence of random choices of different agents and iterations, it thus follows that


Let random variable  denote the number of moves until the first agent finds the target.


We partition event  into disjoint events , where  denotes the event that agent  is the agent with the smallest id that finds the target in iteration \footnote{We pick the agent with the smallest id just as a tie-breaker between agents; the ids of agents do not play an important role in the algorithm.}. By the definition of , we know that in any execution in , some agent finds the target in iteration .

Next, we bound the value of . By the Law of Total Expectation applied to the partition of event , it follows:



Let random variable  denote the number of moves an agent  takes to complete iteration . Note that because we condition on event , we know that the expected number of rounds for some agent to find the target is at least the expected number of rounds for the fixed agent  to find the target and to complete iteration . Therefore, it follows that: 



By the definition of , we know that  because the value of  is the same for any fixed agent , including the one with the smallest id. Using Lemma \ref{lem:ria}, we conclude that



Finally, we sum over all iterations to calculate the value of . Recall that we already calculated the value of .

By Lemma~\ref{lem:iterations}, we know that , so it follows that

\end{proof}

We now generalize this algorithm to one that uses probabilities lower bounded by  for some given . This is achieved by the following subroutine, which implements a coin that shows tails with probability  using a biased coin that shows tails with probability , for . 

\begin{algorithm}
\caption{coin(): Biased coin flip showing tails with probability .}
\label{algo:p_coin}
\For{} {
	\If{ shows heads}
	{
	\Return{heads}\phantom{Xp}
	}
}
\Return{tails}\phantom{Xp}\nllabel{line:p_tails}
\end{algorithm}

\begin{lemma}
\label{lem:p_coin}
	Algorithm~\ref{algo:p_coin} returns tails with probability  and requires  bits of memory.
\end{lemma}
\begin{proof}
	From the code it follows that the action on Line~\ref{line:p_tails} is performed only if none of the outcomes of the coin flips are tails. Since each coin shows tails with probability  and there is a total if  coin flips, the probability of all of them being tails is . Since the entire state of the algorithm is the loop counter, it can be implemented using  bits of memory.
\end{proof}


Next, we show how to combine Algorithm \ref{algo:non-uni-loop} and Algorithm \ref{algo:p_coin}, and we analyze the performance and selection complexity of the resulting algorithm. Given a biased coin , we construct Algorithm Non-Uniform-Search  by replacing the lines where coin  is tossed in Algorithm \ref{algo:non-uni-loop} with a copy of Algorithm \ref{algo:p_coin}, with parameters  and .


\begin{theorem}
\label{thm:algoA}
Let each of  agents execute a copy of Algorithm Non-Uniform-Search. The minimum over all agents of the expected number of moves for an agent to find a target in distance  from the origin is . Moreover, Algorithm Non-Uniform-Search satisfies .
\end{theorem}

\begin{proof}
	 By Lemma \ref{lem:p_coin}, Algorithm \ref{algo:p_coin} run with parameters  and  generates coin flips with probability  of showing tails. Therefore, the correctness of Algorithm Non-Uniform-Search follows from Theorem \ref{thm:upper}. Since Algorithm \ref{algo:p_coin} does not generate any moves of the agents on the grid, the time complexity of algorithm also follows from Theorem \ref{thm:upper}. 
	 
	 Finally, by Lemma \ref{lem:p_coin} and the fact that Algorithm \ref{algo:non-uni-loop} requires  bits to be implemented, it follows that .
\end{proof}


\subsection{Uniform Algorithm}
\label{sec:uniform}

In this section, we generalize the results from Section \ref{sec:non-uniform} to derive an algorithm that is uniform in . The main difference is that now each agent maintains an estimate of  that is increased until the target is found. For each estimate, an agent simply executes the corresponding variant of Algorithm Non-Uniform-Search. We show that for the algorithm in this section, the expected number of moves for the first agent to find a target at distance at most  from the origin is . Also, the algorithm uses only  bits of memory.

To simplify the presentation, we break up the main algorithm into subroutines. We begin by showing how to move in a given direction by a random number of moves that depends on the current estimate  of . In the following algorithm, recall that  is used to bound from below the smallest probability available to each agent by . We use an integer  as a parameter to the algorithm in order to generate different distance estimates .

\begin{algorithm}
\caption{walk(,, ): Move by a random number of moves in direction  that is roughly uniform on .}
\label{algo:goToPoint}
\While{coin()heads}{
  move one step in direction 
}
\end{algorithm}


\begin{lemma}
\label{lem:walk}
	For each , the probability that Algorithm~\ref{algo:goToPoint} performs exactly  moves is at least . The probability that the algorithm performs at least  moves is at least . The expected number of moves is smaller than . The algorithm requires  bits of memory.  
\end{lemma}

\begin{proof}
By Lemma~\ref{lem:p_coin}, the probability that the algorithm performs exactly  moves is

The probability that it performs at least  moves is . The expected number of moves is

\noindent Implementing the coin flip by Algorithm~\ref{algo:p_coin}, the memory requirement follows from Lemma~\ref{lem:p_coin}.
\end{proof}

Using the subroutines above, Algorithm~\ref{algo:search} visits each grid point of a square of side length  centered at the origin with probability .

 \begin{algorithm}
\caption{search(): Visit each grid point of a square of side length  centered at the origin with probability .}
\label{algo:search}
\If{if  shows heads}{
  walk(,up)
}
\Else{
  walk(,down)
}
\If{ shows heads}{
  walk(,right)
}
\Else{
  walk(,left)
}
\end{algorithm}

\begin{lemma}
\label{lem:uniformArea}
  If called at the origin, for each point , Algorithm~\ref{algo:search} visits point  with probability at least . It can be implemented using  bits of memory.
\end{lemma}
\begin{proof}
	Consider grid point . With probability  each, the algorithm decides to move up and right in the first and second call to Algorithm~\ref{algo:goToPoint}. By Lemma~\ref{lem:walk}, the first call will halt after exactly  moves with probability at least , and the second call will perform  moves with probability at least . Hence, the claimed lower bound on the probability to visit  follows. Analogously, the same holds for , , and . The memory requirements are  bits to memorize whether the direction of movement is currently up, down, left, or right, plus the  bits needed for the (sequential) calls to Algorithm~\ref{algo:goToPoint}.
\end{proof}


Finally, in Algorithm~\ref{algo:loop}, we use Algorithm \ref{algo:search} to efficiently search an area of  with  agents. Intuitively, the algorithm iterates through different values of the outer-loop parameter , which correspond to the different estimates of , increasing by approximately a factor of . For each such estimate, the algorithm needs to execute a number of calls to the search subroutine with parameter . 
However, since agents have limited memory and limited probability values, we can only count the number of such calls to the search routine approximately. We do so similarly to Algorithm \ref{algo:goToPoint}, by repeatedly tossing a biased coin and calling the search algorithm as long as the coin shows heads. 

\begin{algorithm}
\caption{Search Algorithm for  agents.  is a sufficiently large constant.}
\label{algo:loop}
\For{}{
  \While{coin()heads}{
    search()\\
    return to the origin
  }
}
\end{algorithm}

Throughout the proof of Algorithm \ref{algo:loop}, we refer to an iteration of the outer-most loop as a phase. 

\paragraph{Proof Overview.} First, in Lemma \ref{lem:exp_i}, we calculate the expected number of moves  for an agent to complete phase . Then, we apply the same reasoning as in Lemma \ref{lem:condition} (from Section \ref{sec:non-uniform}), to determine the expected number of  moves  for an agent  to complete phase  (past some initial number of  phases), conditioning on agent  finding the target in phase . Next, we move on to reasoning about all  agents, instead of a single agent. In Lemma \ref{lem:whileLoop}, we bound the probability that in each phase , at least  calls to the subroutine search are executed by all agents together. In Lemma \ref{lem:probFind}, we use that result to calculate the probability that at least one of the  agents finds the target in some phase . Finally, we use these intermediate results to prove the main result of this section, Theorem \ref{theorem:upper}, which shows that the expected number of moves for the first agent to find a target within distance  from the origin is .

Denote by  the expected number of moves until an agent completes phase . 

\begin{lemma}\label{lem:exp_i}
, where .
\end{lemma}
\begin{proof}
By linearity of expectation, we can calculate  as follows. In the expression below, index  counts the number of phases (up to ), index  counts the number of calls to the search subroutine, and index  counts the number of moves for an agent to complete each call to the search subroutine. In the sum indexed by , we just calculate the probability that exactly  calls to the search subroutine are executed and multiply that by the expected number of moves to complete one such call. In the sum indexed by , we calculate the probability that Algorithm \ref{algo:goToPoint} stops after  moves and multiply that by  because  each call to search() results in two calls to walk(). 

\end{proof}

Denote by  the expected number of moves until a fixed agent  finds the target, conditioning on the fact that agent  finds the target in phase , but no earlier phase.

\begin{corollary}
\label{coro:time}
	If , then it holds that .
\end{corollary}
\begin{proof}
In the last phase, agent  walks directly to the target, which takes at most  moves. For all previous phases, reasoning analogously to Lemma~\ref{lem:condition}, in terms of phases instead of iterations, we see that the expectation does not increase by more than a factor of  due to conditioning on not finding the target. The claim thus follows from Lemma~\ref{lem:exp_i} and the fact that .
\end{proof}

Denote by  the event that in total at least  calls to search are executed in phase .

\begin{lemma}
\label{lem:whileLoop}
	.
\end{lemma}
\begin{proof}
Abbreviate . By Lemma~\ref{lem:p_coin} and linearity of expectation, the expected number of calls to search() performed by all agents during phase  is


Since the coin flips are independent, we can apply Chernoff's bound (see Equation \eqref{eq:chernoff_lower} in the Appendix), showing that the probability that fewer than  searches are executed in total is at most . Hence, since  is a sufficiently large constant, the claim follows.
\end{proof}

Denote by  the event that the target is found by some agent in phase .

\begin{lemma}
\label{lem:probFind}
	 .
\end{lemma}
\begin{proof}
By Lemma~\ref{lem:whileLoop}, with probability at least , at least  iterations of the while loop are executed in total. Because , i.e., , Lemma~\ref{lem:uniformArea} shows that in each iteration, the probability to find the target is at least . Therefore, the probability to miss the target in all calls is at most

Because  is a sufficiently large constant, we may assume that this is at most . We conclude that

as claimed.
\end{proof}

Let event  denote the event that the target is found for the first time in phase .

\begin{theorem}\label{theorem:upper}
Let each of  agents execute a copy of Algorithm~\ref{algo:loop}. The minimum over all agents of the expected number of moves for an agent to find a target within distance  from the origin is .
\end{theorem}
\begin{proof}
Observe that because the probability to find the target in phase  is independent of all coin flips in earlier phases, we have that

For , by Lemma~\ref{lem:probFind} it follows that


Let random variable  denote the number of moves until the first agent finds the target.



We partition event  into disjoint events , where  denotes the event that agent  is the agent with the smallest id that finds the target in phase \footnote{We pick the agent with the smallest id just as a tie-breaker between agents; the ids of agents do not play an important role in the algorithm.}. By the definition of , we know that in any execution in , some agent finds the target in phase .

Next, we bound the value of . By the Law of Total Expectation applied to the partition of event , it follows:



Let random variable  denote the number of moves an agent  takes to complete iteration . Note that because we condition on event , we know that the expected number of rounds for some agent to find the target is at least the expected number of rounds for the fixed agent  to find the target and complete iteration . Therefore, it follows that: 



By the definition of , we know that  because the value of  is the same for each fixed agent , including the one with the smallest id. Therefore, we conclude that:


Finally, we sum over all phases to calculate the value of . Recall that we already calculated the value of . Using Corollary \ref{coro:time} for the value of , we conclude that:

\vspace*{-1.35cm}

\qedhere
\end{proof}


\section{Lower bound}
\label{sec:lower}

In this section, we present a lower bound showing that there is no algorithm that finds a target placed within distance  from the origin in  rounds with high probability (w.h.p.), such that the algorithm satisfies . 

Throughout this section, we say that some event occurs with high probability iff the probability of the event occurring is at least  for an arbitrary predefined constant  and some . We say that two probability distributions  and  are ``approximately equivalent'' iff  for an arbitrary predefined constant  and some . By  we denote the -norm on the respective space.

First, we state the main theorem of the section in terms of the performance metric  (the minimum over all agents of the number of steps for an agent to find the target). Note that, by the definition of a round, this is equivalent to counting the expected number of rounds until the first agent finds the target. At the end of the section, in Corollary \ref{cor:moves}, we generalize the main result to apply to metric  (the minimum over all agents of the number of moves for an agent to find the target). 

\begin{theorem}\label{thm:lower}
	Let  be an algorithm with  and  agents. There is a placement of the target within distance  from the origin such that w.h.p.\ no agent executing algorithm  finds it in fewer than  rounds.
	Moreover, the probability for some agent to find a target, placed uniformly at random in the square of side  centered at the origin, within  rounds is .
\end{theorem}

\subsection{Proof Overview}

Here we provide a high-level overview of our main proof argument. We fix an algorithm  and focus on executions of this algorithm of length  rounds. We prove that since agents have  states, they ``forget'' about past events too fast to behave substantially different from a biased random walk. 


More concretely, first we show, in Corollary \ref{cor:initial} that after  initial rounds each agent  is located in some recurrent class  of the Markov chain. We use this corollary to prove, in Corollary \ref{cor:origin}, that after the initial  rounds each agent  does not return to the origin (or it keeps returning every  rounds, so it does not explore much of the grid). Therefore, throughout the rest of the proof we can ignore the states labeled ``origin".  

Assume there is a unique stationary distribution of .\footnote{This holds only if the induced Markov chain on the recurrent class is aperiodic, but the reasoning is essentially the same for the general case. We handle this technicality in Section \ref{sec:draw}.} Since there are few states and non-zero transition probabilities are bounded from below, standard results on Markov chains imply that taking  steps from any state in the recurrent class will result in a distribution on the class's states that is (almost) indistinguishable from the stationary distribution (Corollary~\ref{cor:bound}); in other words, any information agents try to preserve in their state will be lost quickly with respect to .

The next step in the proof is a coupling argument. We split up the rounds in the execution into groups such that within each group, rounds are sufficiently far apart from one another for the above ``forgetting'' to take place. For each group, we show that drawing states independently from the stationary distribution introduces only a negligible error (Lemma~\ref{lem:pi_s} and Corollary~\ref{cor:coin}). Doing so, we can apply Chernoff's bound to each group, yielding that agents will not deviate substantially from the expected path they take when, in each round, they draw a state according to the stationary distribution and execute the corresponding move on the grid (Lemma~\ref{lem:upwards} and Corollary~\ref{cor:concentration}). Taking a union bound over all groups, it follows that, w.h.p., each agent will not deviate from a straight line (the expected path associated with the recurrent class it ends up in) by more than distance , where  is the number of states of the Markov chain. It is crucial here that the corresponding region in the grid, restricted to distance  from the origin, has size  and depends only on the component of the Markov chain the agent ends up in. Therefore, since there are no more than  components, taking a union bound over all agents shows that w.h.p.\ together they visit an area of .


\subsection{Proof}
We assume without loss of generality that values like  are integers; for the general case, one may simply round up. Moreover, since we are interested in asymptotics with respect to , we may always assume that  is larger than any given constant.

Fix any algorithm , some , and let . Consider the probability distribution of executions of  of length  rounds; we will fix the -term in the exponent later, in Lemma~\ref{lem:upwards}. 

We break the proof down into three main parts. First, in Section \ref{sec:initial}, we show that after a certain number of initial rounds each agent is in a recurrent class and, for simplicity, we can ignore the states labeled ``origin". Next, in Section \ref{sec:draw}, we show that if we break down the execution into large enough blocks of rounds, with high probability  we can assume that the steps associated with rounds in different blocks do not depend on each other. Finally, in Section \ref{sec:movement}, we focus on the movement of the agents in the grid, derived from these ``almost" independent steps, and we show that with high probability the agents will not explore any points outside of an area of size  around the origin.


\subsubsection{Initial steps in the Markov chain}
\label{sec:initial}

Let random variable  denote the recurrent class of the Markov chain in which agent  is located at the end of round ; if  is in a transient state at the end of round , we set . Also, by  we denote the smallest non-zero probability in the Markov chain. By assumption, we know that .

First we show that for any agent  and any state  of the Markov chain, if state  is always reachable by agent , then agent  visits state  within  rounds.

Let  where the constant  will be specified later.

\begin{lemma}
\label{lem:reach}
For any agent , any round , and any state , condition on the event that at the end of round  of the execution agent  never visits a state  such that  is not reachable from . Then, w.h.p.\ agent  visits state  at the end of round  such that .
\end{lemma}
\begin{proof}
	Since state  is reachable after each round, there must be a path of length at most  from the state in which agent  is located at the end of round  to state . Therefore, the probability that the agent visits state  within  rounds is bounded from below by the probability that a potentially biased random walk on a line of  nodes starting at the leftmost node reaches the rightmost node within  rounds. This probability in turn is bounded from below by

where the second last step uses fact that ; otherwise, each state in the Markov chain has only one outgoing transition, in which case the claim of the lemma is trivial.
Therefore, for an appropriate choice of , w.h.p.\ agent  visits state  within  rounds.
\end{proof}

In the following corollary we show that for any round , w.h.p.\ an agent is located in some recurrent class of the Markov chain.

\begin{corollary}
\label{cor:initial}
	For any agent  and any round , w.h.p.\ .
\end{corollary}
\begin{proof}
First, we derive a Markov chain from the original Markov chain as follows. We identify all recurrent states in the original Markov chain and we merge them all into a single recurrent state  of the derived Markov chain.

By definition of a recurrent class and because there is only one such class, for each state  in the derived Markov chain, the recurrent state  is always reachable from . Applying Lemma \ref{lem:reach} to agent , round  and state , it follows that w.h.p.\  visits  at the end of  or earlier. This implies that in the original Markov chain w.h.p.\ agent  visits \emph{some} recurrent state , where  is a recurrent class, at the end of round  or earlier.
Since recurrent classes cannot be left, it must be the case that  and  for each .
\end{proof}

In Corollary \ref{cor:initial}, we showed that at the end of each round  an agent  is located in some recurrent class  w.h.p.\ Since agent  does not leave that class in subsequent rounds, we will refer to it by . Next, we show that for any state  and any round , w.h.p.\ agent  visits state  at the end of round  or earlier.

\begin{corollary}
\label{cor:visits}
For each agent , each round , and each state , w.h.p.\ agent  visits state  at the end of round  such that .
\end{corollary}

\begin{proof}
	By Corollary \ref{cor:initial}, we know that w.h.p.\ at the end of round  agent  is in some recurrent class  that it never subsequently leaves. Therefore, each state  is reachable by  after each round . By Lemma \ref{lem:reach},  it is true that w.h.p.\ agent  visits state  within  rounds.
\end{proof}

Finally, we show that the recurrent class  in which agent  is located does not contain any states labeled ``origin'', or otherwise, the agent keeps returning to the origin too often and makes no progress exploring the grid. This result will let us, for convenience, ignore states labeled ``origin" through the rest of the proof.

\begin{corollary}
\label{cor:origin}
W.h.p., at least one of the following is true for any agent  in any round : (1) agent  never visits a point in the grid at distance more than  from the origin, or (2) agent  is located in a recurrent class in which none of the states are labeled ``origin".
\end{corollary}
\begin{proof}
	By Corollary \ref{cor:initial}, we know that w.h.p.\ at the end of round  agent  is in some recurrent class  that it never subsequently leaves. By Lemma \ref{cor:visits}, we know that for each state , labeled ``origin" and each round , w.h.p.\ agent  visits  at the end of round  or earlier. Therefore, if  contains a state labeled ``origin", then w.h.p.\ agent  never visits a point at distance more than  from the origin. Otherwise, (2) holds.
\end{proof}

Throughout the rest of the proof, we consider executions after round ; since,  and we consider executions of length , we can just ignore these initial rounds. Therefore, from Corollary \ref{cor:initial} and Corollary \ref{cor:origin}, we can assume for the rest of the proof that each agent  is in a recurrent class  and it does not return to the origin.


\subsubsection{Moves drawn from the stationary distribution}
\label{sec:draw}

Fix an agent  and consider . As , the Markov chain induced by  has a period of  (an aperiodic chain has period ). We apply Theorem~\ref{thm:feller} to  and denote by  the equivalence classes based on the period  whose existence is guaranteed by the theorem.

Consider blocks of rounds of size , where  is a constant that is determined by Corollary~\ref{cor:bound}. Without loss of generality, assume that  is a multiple of  (otherwise use  instead). We define group of rounds such that each group contains one round from each block. Formally, for  and , group  contains round numbers . Observe that this definition entails that at the end of all rounds from a given group, agent  is in some state from the same class  that is recurrent and closed under .

Let  denote the probability distribution on  of possible states agent  may be in at the end of round , conditional on its state being  at the end of round . Note that this distribution is, in fact, independent of . We obtain the following corollary of Lemma~\ref{lem:rosenthal} applied to the Markov chain induced by the matrix  restricted to class .

\begin{corollary}
\label{cor:bound}
	There is a unique stationary distribution  of the Markov chain on  induced by . For any state  and any round ,  and  are approximately equivalent.
\end{corollary}

\begin{proof}
	Since  is a multiple of , we can consider the probability matrix , which by Theorem~\ref{thm:feller} induces a Markov chain on . We apply Lemma~\ref{lem:rosenthal} to this chain with the following parameters: ,\footnote{Recall that we assume such values to be integer, otherwise rounding is required.}  (i.e.,  for all ), and . We need that  for each  and a suitable . Since  is recurrent, there is a path of at most  hops in the Markov chain between any pair of states . Because non-zero transition probabilities in  are at least ,  is a feasible choice.
	
	We conclude that Lemma~\ref{lem:rosenthal} yields that there is a unique stationary distribution  under  on  satisfying that 
	
\end{proof}

From this corollary, we infer the following coupling result.
\begin{lemma}
\label{lem:pi_s}
	Let  and . Then, for any state  and any constant , there exists a probability distribution  such that

where  is the unique stationary distribution of  under .
\end{lemma}
\begin{proof}
If , trivially  and we choose . Thus, assume that  in the following. For each ,

where the first inequality exploits that any state  must be reachable from any state  by a sequence of at most  state transitions. Also, note that  because by assumption we are guaranteed that . Since , this also implies that  for each .

	Now use the equation  to define , i.e.,
	
	We need to show that  indeed is a probability distribution. It holds that
	
	
	Hence it remains to show that for each , we have that . For , we bound
	
	Here we use that, by Corollary~\ref{cor:bound},  and  are approximately equivalent (using  as the constant in the exponent) in the third step, as well as that  is sufficiently large. Similarly,

This shows that Equation~\eqref{eq:def_pi_s} indeed ensures that  is a probability distribution, concluding the proof.
\end{proof}

We now show that within each class , approximating the random walk of an agent in the Markov chain by drawing its state for each round  \emph{independently} from the stationary distribution  does not introduce a substantial error. To this end, consider the following random experiment . For each round , , we toss an independent biased coin such that it shows head w.h.p. In this case, we draw the state at the end of round  independently from . Otherwise, we draw it from the distribution , where  is the state the agent was in  steps ago and  is the distribution given by Lemma~\ref{lem:pi_s}. Since each time the coin shows head w.h.p., a union bound shows that it holds that w.h.p.\ the coin shows head in \emph{all} rounds .

\begin{corollary}\label{cor:coin}
	If the experiment  described above is executed with probability of heads being , where  is the constant from Definition~\ref{def:whp}, w.h.p.\ no coin flip shows tail. In other words, for each round , the state of the agent at the end of round , , is drawn independently from the stationary distribution  (of  with respect to the chain induced by ).
\end{corollary}

\begin{proof}
For each round  as specified by the corollary, we apply Lemma~\ref{lem:pi_s} with parameter  and  being the state of the agent at the end of round . Hence, there exists a distribution  such that

where  is the state of the agent at the end of round . In other words, the following random experiment is equivalent to drawing from .
\begin{compactenum}
\item Flip a biased coin showing heads with probability .
\item If it shows heads, draw from .
\item Otherwise, draw from .
\end{compactenum}
Now consider all the coin flips during rounds , . By a union bound, the probability that no coin flip ever results in tails is bounded from below by .
\end{proof}

\subsubsection{Movement on the grid.}
\label{sec:movement}

Having established that an agent's state can essentially be understood as a (sufficiently small) collection of sets of independent random variables, we focus on the implications on the agents' movement in the grid. Let the random variable  have value  if the state of the agent at the end of round  is labeled up, and  otherwise. Note that these random variables depend only on the state transitions the agent performs in the Markov chain. Also let .

\begin{lemma}
\label{lem:upwards}
	Suppose agent  is initially in a state from the recurrent class . Then there is a , such that for each round  it holds that w.h.p.\ .
\end{lemma}

\begin{proof}
Throughout the entire proof, we condition on the agent being initially in a state from , but for simplicity largely omit this from the notation. Denote by  the conditional expectation given that agent  is initially in recurrent class . We will consider the special case  first. Since by assumption , it follow that , and so .
Therefore, it suffices to show that, for a suitable choice of , . Recall that  is the collection of step numbers  for . Observe that
	
	Denote for each  by  the event that experiment  results in all coin flips showing head. By Corollary~\ref{cor:coin}, this occurs w.h.p., and by a union bound  occurs w.h.p. We will show that for each , conditioned on , w.h.p.\ it holds that
	
	Conditioned on , we know that the considered variables  from  are independently and identically distributed: The state at the end of round  is drawn independently from some stationary distribution  that does not depend on , and the probability for the agent to move up in the grid equals the probability that this state is labeled ``up". Denote by  the probability for the agent to move up in such a round  (when its state is distributed according to ). By linearity of expectation,
	
	
	
	If , Chernoff's bound (Inequality~\eqref{eq:chernoff_upper} with ) implies that
	
	On the other hand, if , we choose  and apply the two-sided Chernoff bound (Inequality~\eqref{eq:chernoff_twosided}), yielding that
	
	We fix\footnote{As stated earlier, we deferred the choice of the -term in the exponent of , permitting to specify  in terms of  now.} . Thus, it holds that
	
	This shows that, conditioned on , the bound \eqref{eq:condition} holds w.h.p. To complete our line of reasoning, observe that
	
	and set .	By a union bound, we have that, w.h.p., both  occurs and bound~\eqref{eq:condition} holds for all . In this case, it follows that
	
	This proves the claim for the special case . For the general case, observe that decreasing  by an integer multiple of  will decrease the computed expectation by the same multiple of  (as long as  remains larger than ). Concerning the bound~\eqref{eq:condition}, observe that decreasing the number of steps will only decrease the probability of large deviations from the expectation of the random variable.	Since , the general statement hence follows analogously to the special case.
\end{proof}


Repeating these arguments for the other directions (right, down, and left), we see that overall, each agent behaves fairly predictably. Define  to be the random variable describing the sum of all moves the agent performs in the grid up to round , i.e., its position in the grid (in each dimension) at the end of round . For this random variable, the following statement holds.
\begin{corollary}\label{cor:concentration}
	Suppose agent  is initially in a state of the recurrent class . Then there is  depending only on  such that for each , it holds that  w.h.p.
\end{corollary}

\begin{proof}
Defining the random variables , , and  analogously to , we can apply the same reasoning as in Lemma~\ref{lem:upwards} to control their values. Hence, for each , , there is a  (depending only on ) such that  w.h.p. Observe that . Hence, with , a union bound shows that  w.h.p.
\end{proof}


We are now ready to resume the proof of Theorem \ref{thm:lower}.
\begin{proof}[Proof of Theorem~\ref{thm:lower}]
Denote by  the set of recurrent classes of the Markov chain describing an agent's state evolution. By Corollary~\ref{cor:initial}, it holds for each agent  that, after each round , w.h.p.\ the agent is located in recurrent class . Since Lemma~\ref{lem:upwards}, and therefore Corollary~\ref{cor:concentration}, do not depend on the initial state from  the agent is in, the same reasoning shows that, at the end of round , w.h.p.\ the position of  will not deviate by more than distance  from a straight line in the grid. By a union bound, this holds for all agents jointly w.h.p (recall that by assumption  is sub-exponential in ). Hence, w.h.p., it holds for each agent  and each round  that  never ventures further away from the origin than distance , or its position does not deviate by more than distance  from one of at most  straight lines or the origin. Since for any straight line only a segment of length  is in distance  from the origin, the union of all grid points that are (i) in distance at most  from the origin and (ii) in distance at most  from one of the straight lines has cardinality . Hence, there is a set  of  grid points that only depends on the algorithm , , and , with the following property: w.h.p., all grid points in distance  from the origin that are visited within the first  steps of an execution of  are in . Since there are  grid points in distance  from the origin, this implies that the target can be placed in such a way that w.h.p.\ no agent will find it within  rounds, and a uniformly placed target is found in this amount of time with probability .
\end{proof}

Finally, we need to show that Theorem \ref{thm:lower} also holds with respect to the metric . In the following corollary, we show that each move of an agent on the grid corresponds to at most  transitions in its Markov chain, or otherwise, the agent does not move on the grid after some point on. 

\begin{corollary}
\label{cor:moves}
	Let  be an algorithm with  and  agents. There is a placement of the target within distance  from the origin such that w.h.p.\ no agent executing algorithm  finds the target in fewer than  moves.
	Moreover, the probability for some agent to find a target, placed uniformly at random in the square of side  centered at the origin, within  moves is .
\end{corollary}
\begin{proof}
	First, we show that w.h.p., at least one of the following is true about any agent  in any round : (1) each move on the grid performed by  corresponds to at most  steps in its Markov chain, or (2)  is located in a recurrent class in which all states are labeled ``none".
	
	By Corollary \ref{cor:initial}, we know that at the end of round  w.h.p.\ agent  is in some recurrent class  that it never subsequently leaves. By Lemma \ref{cor:visits}, we know that for each state , w.h.p.\ it is visited within  steps. Therefore, if  contains a state labeled up/down/left/right, w.h.p.\ it is visited within  steps. Otherwise, (2) applies. 
	
	If part (2) of the statement applies, then an agent does not make any progress in the grid after it reaches its recurrent class, so it does not visit more than  grid points and, consequently, the corollary holds. If part (1) applies, since each move in the grid corresponds to at most  steps in the Markov chain,  moves correspond to  steps. In this case, we know Theorem \ref{thm:upper} guarantees that there is a placement of the target within distance  from the origin such that w.h.p.\ no agent finds it within  steps, and consequently  moves.
\end{proof}



\section{Discussion and Conclusion}
\label{sec:discussion}

We have presented an algorithm and a lower bound for the problem of  agents searching in a grid for a target placed at distance at most  from the origin. Our lower bound shows that for  sub-exponential in , the agent cannot find the target w.h.p.\ in fewer than  rounds if . We also present an algorithm that finds the target in  rounds in expectation for , proving our lower bound to be near tight.

Note that for the upper bound we get stronger results if we consider a fixed search area of distance  from the origin, as opposed to keeping track of a varying estimate of the search area. In the case of a fixed distance , and , suffices to find the target in  rounds in expectation.

Finally, we should mention that currently there is a gap of  between our upper and lower bounds. Also, for our lower bound we assume that  while our algorithm we assume  bits of memory, which constitutes a constant-factor gap. Closing either of these gaps is an open problem.


\bibliographystyle{plain}
\bibliography{ants}

\appendix

\section{Math Preliminaries}
\label{sec:math}

In this section, we briefly go over some mathematical definitions and results that will be used throughout the proofs. Throughout the paper, by  we will denote the -norm on the respective space.

\subsection{Markov Chains}

First, we state a basic result from \cite{feller68} about periodic Markov chains.

\begin{theorem}[Feller]
\label{thm:feller}
	In an irreducible Markov chain with period  the states can be divided into  mutually exclusive classes  such that it is true that (1) if  then the probability of being in state  in some round  is  unless  for some , and (2) a one-step transition always leads to a state in the right neighboring class (in particular from  to ). In the chain with matrix  each class  corresponds to an irreducible closed set.
\end{theorem}

The next theorem establishes a bound on the difference between the stationary distribution of a Markov chain and the distribution resulting after  steps.

\begin{lemma}[Rosenthal]
\label{lem:rosenthal}
	Let  be the transition probabilities for a time-homogeneous Markov chain on a general state space . Suppose that for some probability distribution  on , some positive integers  and , and some , 
			
where  represents the -step transition probabilities. 
Then for any initial distribution , the distribution  of the Markov chain after  steps satisfies

	 
where  is total variation distance and  is any stationary distribution. (In particular, the stationary distribution is unique.)
\end{lemma}


\subsection{Basic Probability Definitions and Results}



\begin{definition}
\label{def:whp}
	Let  be some probability distribution and let  be an arbitrary event in . For a given , we say that event  occurs ``with high probability in '' iff the probability of the event  occurring is at least  for an arbitrary predefined constant .
\end{definition}


Next, we provide a similar definition for the distance between two probability distributions.

\begin{definition}
	Let  and  be two probability distributions with the same range. For a given , we say that  and  are ``approximately equivalent with respect to '' iff  for an arbitrary predefined constant .
\end{definition}	
	Finally, we state the two versions of Chernoff's bound that we use throughout the proofs.
	
\begin{theorem}[Chernoff bound] Let  be independent random variables such that for , . Let  and let . Then, for any , it is true that:

	
	
	
\end{theorem}


\begin{theorem}[Two-sided Chernoff bound] Let  be independent random variables such that for , . Let  and let . Then, for any , it is true that:

	
	
\end{theorem}


\end{document}
