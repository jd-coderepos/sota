
\documentclass[dvipsnames]{article} 

\usepackage{array}
\usepackage{subcaption}
\usepackage{multirow,multicol}
\usepackage[flushleft]{threeparttable}
\usepackage{comment}
\usepackage{algorithmic}
\usepackage{booktabs}

\usepackage{amssymb}\usepackage{pifont}

\newcommand{\cmark}{\text{\ding{51}}}\newcommand{\xmark}{\text{\ding{55}}}

\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand{\km}[1]{{\color{red}[km: #1]}}
\newcommand{\rbg}[1]{{\color{blue}[rbg: #1]}}
\newcommand{\ppd}[1]{{\color{green}[ppd: #1]}}
\newcommand{\bd}[1]{\textbf{#1}}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcommand{\app}{\raise.17ex\hbox{}}
\newcommand{\ncdot}{{\mkern 0mu\cdot\mkern 0mu}}
\def\x{\times}
\newcommand{\dt}[1]{\fontsize{8pt}{.1em}\selectfont \emph{#1}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
  
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother
\def \hfillx {\hspace*{-\textwidth} \hfill}

\def\fig#1{Fig.~\ref{fig:#1}}
\def\imw#1#2{\includegraphics[clip,width=#2\linewidth]{#1}}
\def\imh#1#2{\includegraphics[clip,height=#2\textheight]{#1}}
\def\imwh#1#2#3{\includegraphics[clip,width=#2\linewidth,height=#3\textheight]{#1}}
\newcommand{\tb}[3]{\setlength{\tabcolsep}{#2mm}\begin{tabular}{#1}#3\end{tabular}}

\def\tablecite#1#{\def\pretablecite{#1}\tableciteaux}
\def\tableciteaux#1{\textsuperscript{\expandafter\originalcite\pretablecite{#1}}}

\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage[table]{xcolor}
\usepackage{soul}



\usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\title{Long-tailed Recognition by\\
Routing Diverse Distribution-Aware Experts}


\author{Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella X. Yu\\
UC Berkeley / ICSI, 
Nanyang Technological University\\
\texttt{\{xdwang,longlian,zhongqi.miao,stellayu\}@berkeley.edu}\\
\texttt{ziwei.liu@ntu.edu.sg}
\\ 
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\ZM}[1]{\textcolor{red}{ZM: #1}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}



Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on gaining performance on tail classes, often at the expense of losing performance on head classes and with increased classifier variance. 
The low tail performance manifests itself in large inter-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE), consisting of three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts.  With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5\% to 7\% on all the benchmarks including CIFAR100-LT, ImageNet-LT and iNaturalist 2018. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains. Our code is publicly available at: \href{https://github.com/frank-xwang/RIDE-LongTailRecognition}{https://github.com/frank-xwang/RIDE-LongTailRecognition}.

\end{abstract}

\iffalse
\begin{abstract}



Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on tail performance gain, often at the expense of head performance loss from increased classifier variance. The low tail performance manifests itself in large inter-class confusion and high classifier variance.  We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE). It has three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts.  With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5\% to 7\% on all the benchmarks including CIFAR100-LT, ImageNet-LT and iNaturalist. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains. 

\end{abstract}

\fi
 \def\figBiasVarTwo#1{
\begin{figure}[#1]
  \centering
  \begin{subfigure}{\linewidth}
  \centering
      \includegraphics[width=\linewidth]{figures/bias_variance_v3.pdf}\caption{Comparisons of the mean accuracy, per-class bias and variance of baseline methods and our RIDE method.  Better (worse) metrics than the distribution-unaware cross entropy (CE) reference are marked in green (red). }
      \label{fig:hardest}
  \end{subfigure}\
\text{Error}(x;h) = E[(h(x;D)-Y)^2]=\text{Bias}(h)^2 + \text{Variance}(h) + \text{irreducible error}.

  \label{eq:DiDi}
    \mathcal{L}_{\text{D-Diversify}}^i = - \frac{\lambda}{n-1}\sum_{j\neq i}^n\KL(\phi^i(\vec{x},\vec{T}), \phi^j(\vec{x},\vec{T}))

    {T_i} = \eta\psi_i + \eta(1-\max(\Psi));
    {\Psi} = \{\psi_1, ...,\psi_C\} = \{\gamma\cdot C\cdot\frac{n_i}{\sum_{k=1}^{C} n_k} + (1-\gamma)\}_{i=1}^{C}

        \mathcal{L}_{\text{Total}} = \frac{1}{nN}\sum_{i=1}^N \sum_{j=1}^n \mathcal{L}^j_{\text{Classify}}(\phi^j(\vec{x}), y) - \frac{\lambda}{n-1}\sum_{j \neq k}^n\KL(\phi^j(\vec{x}, \vec{T}), \phi^k(\vec{x}, \vec{T}))

    \mathcal{L}_{\text{Routing}} = -\omega_{\text{p}} y\log(\frac{1}{1+e^{-y_{\text{ea}}}}) - \omega_{\text{n}}(1 - y)\log(1-\frac{1}{1+e^{-y_{\text{ea}}}})

where the ground truth  is constructed as: if the current expert does not predict the sample correctly but one of the next experts gives correct prediction, the ground truth is set to 1 (considered as a positive sample), otherwise it is 0.  and  are the re-weighting factor (set to 100 for all positive samples and 1 for all negative samples). The expert assignment module is trained on a separate stage, with all other parameters frozen. Each expert, except the last one, has an independent expert assignment module with shared . Therefore, the number of expert assignment modules is . 

Since we use a binary cross entropy loss with logits input in training, the output of this module is the logits that decide whether we need to activate more experts, and the predicted probability can be obtained by appending a sigmoid function at test time. If the predicted probability is greater or equal to 0.5, which means that our predictor gives larger probability to having additional experts, we decide to proceed to the next expert. In case that one needs to manually adjust the proportion of samples passing through each expert, i.e. the ``easiness for each expert assignment module to be satisfied", one could adjust the positive weight of the binary cross entropy loss, , at training time. As observed in experiments, using a fixed positive weight () across all our experiments leads to a good trade-off of our EA module in accuracy and computational complexity.

\textbf{Self-distillation} step is optional but recommended if further improvements (about 0.4\%0.8\% for most experiments) are desired. Unlike previous methods with fixed number of experts (e.g. BBN, LFME), the total number of experts can be arbitrarily adjusted to balance accuracy and computation cost. This enables self-distillation from a model with more (6 in our setting) experts to the same model with fewer experts to obtain further improvements. We choose knowledge distillation \citep{hinton2015distilling} by default. The implementation details and comparison on various distillation algorithms such as CRD \citep{tian2019contrastive} are investigated in Appendix Section \ref{Knowledge_transfer}.
 \def\tabCifar#1{
\begin{table}[#1]
\caption{Top-1 accuracy comparison with state-of-the-arts on \textbf{CIFAR100-LT} with an imbalance ratio of 100. Compared with BBN \citep{zhou2020bbn} and LFME \citep{xiang2020learning}, which also contain multiple experts (or branches), RIDE (2 experts) outperforms them by a large margin with fewer GFlops. The relative computation cost (averaged on testing set) with respect to the baseline model and absolute improvements against SOTA (colored in green) are reported.
 denotes our reproduced results with released code.  denotes results copied from \citep{cao2019learning}.
}\vspace{-2mm}
\label{tab:cifar100}
\begin{center}
\setlength{\tabcolsep}{2.6mm}
\begin{tabular}{l||l|l|l|l|l}
\shline
Methods  & \multicolumn{1}{c|}{MFlops} & \multicolumn{1}{c|}{Acc. (\%)} & Many & Med & Few \\ 
\shline
Cross Entropy (CE)  & 69.5 (1.0x) & 38.3 & - & - & - \\ Cross Entropy (CE)  & 69.5 (1.0x) & 39.1 & \cellcolor{gray!25}66.1 & 37.3 & 10.6 \\ Focal Loss  \citep{lin2017focal} & 69.5 (1.0x) & 38.4 & - & - & - \\ OLTR  \citep{liu2019large} & - & 41.2 & 61.8 & 41.4 & 17.6 \\ LDAM + DRW \citep{cao2019learning} & 69.5 (1.0x) & 42.0 & - & - & - \\ LDAM + DRW  \citep{cao2019learning} & 69.5 (1.0x) & 42.0 & 61.5 & 41.7 & \cellcolor{gray!25}20.2 \\ BBN \citep{zhou2020bbn} & 74.3 (1.1x) & 42.6 & - & - & - \\ -norm  \citep{kang2019decoupling} & 69.5 (1.0x) & 43.2 & 65.7 & 43.6 & 17.3 \\ cRT  \citep{kang2019decoupling} & 69.5 (1.0x) & 43.3 & 64.0 & \cellcolor{gray!25}44.8 & 18.1 \\ M2m \citep{kim2020m2m} & - & 43.5 & - & - & - \\ LFME \citep{xiang2020learning} & - & \cellcolor{gray!25}43.8 & - & - & - \\ \hline
RIDE (2 experts) & \textbf{64.8 (0.9x)} & 47.0 {\small \textcolor{Green}{(+3.2)}} & 67.9 & 48.4 & 21.8 \\
RIDE (3 experts) & 77.8 (1.1x) & 48.0 {\small\textcolor{Green}{(+4.2)}} & 68.1 & 49.2 & 23.9\\
RIDE (4 experts) & 91.9 (1.3x) & \bf{49.1} {\small\bf\textcolor{Green}{(+5.3)}} & \bf{69.3} & \bf{49.3} & \bf{26.0} \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabImageNetResS#1{
\begin{table}[#1]
\caption{Top-1 accuracy comparison with state-of-the-art methods on \textbf{ImageNet-LT} \citep{liu2019large} with \textbf{ResNet-10}. Performance on Many-shot (100), Medum-shot (100 \& 20) and Few-shot (20) are also provided. Results marked with  are copied from \citep{liu2019large}. Results with  are from \citep{xiang2020learning}.}
\label{table:imagenet_lt_resnet10}
\begin{center}
\begin{tabular}{l||l|lx{30}x{30}|l}
\shline
Methods & \multicolumn{1}{c|}{GFlops} & Many & Medium & Few & \multicolumn{1}{c}{Overall} \\
\shline
Cross Entropy (CE)  &0.89 (1.0x) &40.9&10.7&0.4&20.9 \\ Focal Loss  \citep{lin2017focal} &0.89 (1.0x) &36.4&29.9&16.0&30.5 \\ Range Loss  \citep{zhang2017range} &0.89 (1.0x) &35.8&30.3&17.6&30.7 \\ Lifted Loss  \citep{oh2016deep} &0.89 (1.0x) &35.8&30.4&17.9&30.8 \\ OLTR \citep{liu2019large} &0.89 (1.0x) &43.2&35.1&18.5&35.6 \\ LFME \citep{xiang2020learning} &- &47.0&37.9&19.2&38.8 \\ \hline
Many-shot only  &- &59.3&&& \\ Medium-shot only  &- &&35.9&& \\ Few-shot only  &- &&&14.3& \\ \hline
RIDE (2 experts) &\bf{0.85 (1.0x)} &57.5&40.8&26.9&45.3  {\small\bf\textcolor{Green}{(+6.5)}} \\
RIDE (3 experts) &0.97 (1.1x) &57.6&41.7&\bf{28.0}&45.9 {\small\bf\textcolor{Green}{(+7.1)}} \\
RIDE (4 experts) &1.07 (1.2x) &\bf{58.5}&\bf{42.4}&27.7&\bf{46.6} {\small\bf\textcolor{Green}{(+7.8)}} \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabImageNetResNetLSplit#1{
\begin{table}[#1]
\caption{Top-1 accuracy comparison with state-of-the-art methods on \textbf{ImageNet-LT} \citep{liu2019large} with \textbf{ResNet-50}. Performance on Many-shot (100), Medum-shot (100 \& 20) and Few-shot (20) are also provided. Results marked with  are copied from \citep{kang2019decoupling}.}
\label{table:imagenet_lt_resnet50}
\begin{center}
\begin{tabular}{l||l|lx{30}x{30}|l}
\shline
Methods & \multicolumn{1}{c|}{GFlops} & Many & Medium & Few & \multicolumn{1}{c}{Overall} \\
\shline
Cross Entropy (CE)  &4.11 (1.0x) &64.0&33.8&5.8&41.6 \\ NCM \citep{kang2019decoupling} &- &53.1&42.3&26.5&44.3 \\ cRT \citep{kang2019decoupling} &4.11 (1.0x) &58.8&44.0&26.1&47.3 \\ -norm \citep{kang2019decoupling} &4.11 (1.0x) &56.6&44.2&27.4&  \\ LWS \citep{kang2019decoupling} &4.11 (1.0x) &57.1&45.2&29.3&47.7 \\ \hline
RIDE (2 experts) &\bf{3.71 (0.9x)} &65.8&51.0&34.6&54.4  {\small\bf\textcolor{Green}{(+6.7)}} \\
RIDE (3 experts) &4.36 (1.1x) &\bf{66.2}&51.7&34.9&54.9 {\small\bf\textcolor{Green}{(+7.2)}} \\
RIDE (4 experts) &5.15 (1.3x) &\bf{66.2}&\bf{52.3}&\bf{36.5}&\bf{55.4} {\small\bf\textcolor{Green}{(+7.7)}} \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabImageNetResNeXtLSplit#1{
\begin{table}[#1]
\caption{Top-1 accuracy comparison with state-of-the-art methods on \textbf{ImageNet-LT} \citep{liu2019large} with \textbf{ResNeXt-50}. Performance on Many-shot (100), Medum-shot (100 \& 20) and Few-shot (20) are also provided. Results marked with  are copied from \citep{kang2019decoupling}.}
\label{table:imagenet_lt_resnext50}
\begin{center}
\begin{tabular}{l||l|lx{30}x{30}|l}
\shline
Methods & \multicolumn{1}{c|}{GFlops} & Many & Medium & Few & \multicolumn{1}{c}{Overall} \\
\shline
Cross Entropy (CE)  &4.26 (1.0x) &65.9&37.5&7.7&44.4 \\ NCM \citep{kang2019decoupling} &- &56.6&45.3&28.1&47.3 \\ cRT \citep{kang2019decoupling} &4.26 (1.0x) &61.8&46.2&27.4&49.6 \\ -norm \citep{kang2019decoupling} &4.26 (1.0x) &59.1&46.9&30.7&  \\ LWS \citep{kang2019decoupling} &4.26 (1.0x) &60.2&47.2&30.3&49.9 \\ \hline
RIDE (2 experts) & \bf{3.92 (0.9x)} & 67.6 & 52.5 & 35.0 & 55.9  {\small\bf\textcolor{Green}{(+6.0)}} \\
RIDE (3 experts) & 4.69 (1.1x) &67.6&53.5&35.9&56.4 {\small\bf\textcolor{Green}{(+6.5)}} \\
RIDE (4 experts) & 5.19 (1.2x) &\bf{68.2}&\bf{53.8}&\bf{36.0}&\bf{56.8}  {\small\bf\textcolor{Green}{(+6.9)}} \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabImageNetResAll#1{
\begin{table}[#1]
\vspace{-16pt}
\caption{Top-1 accuracy comparison with state-of-the-art methods on  \textbf{ImageNet-LT} \citep{liu2019large} with ResNet-50 and ResNeXt-50. RIDE achieves consistent performance improvements on various backbones. Results marked with  are copied from \citep{kang2019decoupling}. We compare GFlops against the baseline model. Detailed results on each split are listed in appendix materials.}\vspace{-2mm}
\label{table:imagenet-lt}
\begin{center}
\setlength{\tabcolsep}{3.3mm}
\begin{tabular}{l||l|l|l|l}
\shline
\multirow{2}{*}{Methods}& \multicolumn{2}{c|}{ResNet-50} & \multicolumn{2}{c}{ResNeXt-50} \\ 
 & \multicolumn{1}{c}{GFlops} & \multicolumn{1}{c|}{Acc. (\%)} & \multicolumn{1}{c}{GFlops} & \multicolumn{1}{c}{Acc. (\%)} \\ [.1em]
\shline
Cross Entropy (CE)  & 4.11 (1.0x) & 41.6 & 4.26 (1.0x) & 44.4 \\ OLTR  \citep{liu2019large} & - & - & - & 46.3 \\ NCM \citep{kang2019decoupling} & 4.11 (1.0x) & 44.3 & 4.26 (1.0x) & 47.3 \\ -norm \citep{kang2019decoupling} & 4.11 (1.0x) & 46.7 & 4.26 (1.0x) & 49.4 \\ cRT \citep{kang2019decoupling} & 4.11 (1.0x) & 47.3 & 4.26 (1.0x) & 49.6 \\ LWS \citep{kang2019decoupling} & 4.11 (1.0x) & 47.7 & 4.26 (1.0x) & 49.9 \\ \hline
RIDE (2 experts) & \bf{3.71 (0.9x)} & 54.4 {\small\bf\textcolor{Green}{(+6.7)}} & \bf{3.92 (0.9x)} & 55.9 {\small\bf\textcolor{Green}{(+6.0)}} \\
RIDE (3 experts) & 4.36 (1.1x) & 54.9 {\small\bf\textcolor{Green}{(+7.2)}} & 4.69 (1.1x) & 56.4 {\small\bf\textcolor{Green}{(+6.5)}}\\
RIDE (4 experts) & 5.15 (1.3x) & \bf{55.4} {\small\bf\textcolor{Green}{(+7.7)}} & 5.19 (1.2x) & \bf{56.8} {\small\bf\textcolor{Green}{(+6.9)}} \\
\shline
\end{tabular}
\end{center}
\vspace{-16pt}
\end{table}
}



\def\tabiNaturalist#1{
\begin{table}[#1]
\caption{Comparison with state-of-the-art methods on  \textbf{iNaturalist} \citep{van2018inaturalist}. RIDE outperforms current SOTA BBN, which also contains multiple ``experts'', by a large margin on many-shot classes.
Results marked with  are from BBN \citep{zhou2020bbn} and Decouple \citep{kang2019decoupling}. 
BBN's results are from the released checkpoint. Relative improvements to SOTA result of each split (colored with gray) are also listed, with the largest boost from few-shot classes.
}\vspace{-2mm}
\label{table:inaturalist}
\begin{center}
\setlength{\tabcolsep}{2.8mm}
\begin{tabular}{l||l|llll}
\shline
Methods  & GFlops & All & Many & Medium & Few \\ 
\shline
CE  & 4.14 (1.0x) & 61.7 & 72.2 & 63.0 & 57.2 \\ \hline
CB-Focal  & 4.14 (1.0x) & 61.1 & - & - & - \\ OLTR & 4.14 (1.0x) & 63.9 & 59.0 & 64.1 & 64.9 \\ LDAM + DRW  & 4.14 (1.0x) & 64.6 & - & - & - \\ cRT & 4.14 (1.0x) & 65.2 & \cellcolor{gray!25}69.0 & 66.0 & 63.2 \\
-norm & 4.14 (1.0x) & 65.6 & 65.6 & 65.3 & \cellcolor{gray!25}65.9 \\
LWS & 4.14 (1.0x) & 65.9 & 65.0 & 66.3 & 65.5 \\
BBN & 4.36 (1.1x) & \cellcolor{gray!25}66.3 & 49.4 & \cellcolor{gray!25}70.8 & 65.3 \\
\hline RIDE (2 experts) & \bf{3.67 (0.9x)} & 71.4 {\small\bf\textcolor{Green}{(+5.1)}} & 70.2 {\small\bf\textcolor{Green}{(+1.2)}} & 71.3 {\small\bf\textcolor{Green}{(+0.5)}} & 71.7 {\small\bf\textcolor{Green}{(+5.8)}}\\
RIDE (3 experts) & 4.17 (1.0x) & 72.2 {\small\bf\textcolor{Green}{(+5.9)}} & 70.2 {\small\bf\textcolor{Green}{(+1.2)}} & 72.2 {\small\bf\textcolor{Green}{(+1.4)}} & 72.7 {\small\bf\textcolor{Green}{(+6.8)}}\\
RIDE (4 experts) & 4.51 (1.1x) & \bf{72.6} {\small\bf\textcolor{Green}{(+6.3)}} & \bf{70.9} {\small\bf\textcolor{Green}{(+1.9)}} & \bf{72.4} {\small\bf\textcolor{Green}{(+1.6)}} & \bf{73.1} {\small\bf\textcolor{Green}{(+7.2)}}\\
\shline
\end{tabular}
\end{center}
\end{table}
}


\def\tabManyMedFew#1{
\begin{table}[#1]
\caption{Comparison on many-shots, medium-shots and few-shots. The improvements of RIDE with more experts are mainly from few-shots and medium-shots.}\vspace{-4mm}
\label{sample-table}
\begin{center}
\begin{tabular}{l||llll}
\shline
Methods & Many-shots & Medium-shots & Few-shots & All \\ 
\shline
RIDE (2 experts) & 67.94 & 48.38 & 21.81 & 47.3 \\
RIDE (3 experts) & 68.09 (+0.2) & 49.18 (+0.8) & 23.94 (+2.1) & 48.1 (+0.8) \\
RIDE (4 experts) & 68.34 (+0.4) & 50.01 (+1.6) & 25.32 (+3.5) & 48.8 (+1.5) \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabAblation#1{
\begin{table}[#1]
\vspace{-8pt}
\caption{\textbf{Ablation studies on the effectiveness of each component} on CIFAR100-LT. LDAM \citep{cao2019learning} is used as our classification loss. The first 3 RIDE models only have architectural change without changes in training method. The performance without  checked indicates directly applying classification loss onto the final model output, which is the mean expert logits. This is referred to as collaborative loss above. In contrast, if  if checked, we apply individual loss to each individual expert. The difference between collaborative loss and individual loss is described above. 
Knowledge distillation step is optional if further improvements are desired.
Various knowledge distillation techniques are compared in the appendix.
}\vspace{-3mm}
\label{table:ablation}
\begin{center}
\iffalse \begin{tabular}{l|ccccc|ll}
\shline
Methods & \#expert &  &  & EA & distill & GFlops & Acc. (\%)\\ 
\shline
LDAM + DRW &&&&&&& 42.0 \\
\hline
\multirow{6}{*}{RIDE} & 2 & \checkmark &&&& 1.1x & 45.0 {\small\bf\textcolor{Green}{(+3.0)}}\\
& 2 && \checkmark &&& 1.1x & 46.6 {\small\bf\textcolor{Green}{(+4.6)}}\\
& 2 && \checkmark & & \checkmark  & 1.1x & 47.3 {\small\bf\textcolor{Green}{(+5.3)}}\\
& 2 && \checkmark & \checkmark & \checkmark & \bf{0.9x} & 47.0 {\small\bf\textcolor{Green}{(+5.0)}}\\
& 3 && \checkmark & \checkmark & \checkmark & 1.1x & 48.0 {\small\bf\textcolor{Green}{(+6.0)}}\\
& 4 && \checkmark & \checkmark & \checkmark & 1.3x & \bf{49.1} {\small\bf\textcolor{Green}{(+7.1)}}\\

\shline
\end{tabular}
\fi
\begin{tabular}{l|ccccc|ll}
\shline
Methods & \#expert &  &   & EA & distill & GFlops & Acc. (\%)\\ 
\shline
LDAM + DRW & 1 &&&&&& 42.0 \\
\hline
\multirow{6}{*}{RIDE} & 2 & &&&& 1.1x & 44.7 {\small\bf\textcolor{Green}{(+2.7)}}\\
& 3 & & & & & 1.5x & 46.1 {\small\bf\textcolor{Green}{(+4.1)}}\\
& 4 & & & & & 1.8x & 46.3 {\small\bf\textcolor{Green}{(+4.3)}}\\
& 4 & \checkmark & &&& 1.8x & 47.8 {\small\bf\textcolor{Green}{(+5.8)}}\\
& 4 & \checkmark & \checkmark &&& 1.8x & 48.7 {\small\bf\textcolor{Green}{(+6.7)}}\\
& 4 & \checkmark & \checkmark & & \checkmark & 1.8x & \bf{49.3} {\small\bf\textcolor{Green}{(+7.3)}}\\
& 4 & \checkmark & \checkmark & \checkmark & \checkmark & 1.3x & 49.1 {\small\bf\textcolor{Green}{(+7.1)}}\\

\shline
\end{tabular}
\end{center}
\end{table}
}


\def\figBarPlot#1{
\begin{figure}[#1]
\begin{tabular}{cc}
  \includegraphics[width=0.5\textwidth]{figures/iNaturalist.pdf}&
  \includegraphics[width=0.5\textwidth]{figures/Imagenet-LT.pdf}\\
\end{tabular}
\caption{The absolute accuracy difference of RIDE (blue) over \textit{iNaturalist}'s current state-of-the-art method BBN \citep{zhou2020bbn} ({\bf{left}}) and \textit{ImageNet-LT}'s current state-of-the-art method cRT \citep{kang2019decoupling} ({\bf{right}}). RIDE improves the performance of few- and medium-shots categories without sacrificing the accuracy on many-shots, and outperforms BBN on many-shots by a large margin (more than 20\% absolute increase).}
\label{fig:relative-improve}
\end{figure}
}

\def\figPieExpertsExperts#1{
\begin{figure}[#1]
    \centering
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/cifar100_lt_experts.pdf}\vspace{-2mm} \caption{\textbf{\# experts vs. top-1 accuracy for each split} (All, Many/Medium/Few) of CIFAR100-LT. Compared with the many-shot split, which is 3.8\% relatively improved by adding more experts, the few-shot split can get more benefits, that is, a relative improvement of 16.1\%. }
        \label{figure:expertsNum}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/perc_experts.pdf}\vspace{-2mm} \caption{\textbf{The proportion of the number of experts} allocated to each split of CIFAR100-LT. For RIDE with 3 or 4 experts, more than half of many-shot instances only require one expert. On the contrary, more than 76\% samples of few-shot classes require opinions from additional experts.}
        \label{fig:pieChart}
    \end{minipage}
\end{figure}
}

\def\figEntropy#1{
\begin{figure}[#1]
\includegraphics[width=\textwidth]{figures/entropy_imagenet_lt.pdf}
\caption{Entropy of baseline model and RIDE with two experts, each has the number of feature output dimension reduced by half and merged together \textbf{(left)} and their differences \textbf{(right)} on each class of ImageNet-LT. Class index is ranked by sample number.}
\label{fig:entropy_imagenet}
\end{figure}
}

\def\figMultiMethods#1{
\begin{figure}[#1]
\centering
\begin{minipage}[t]{0.42\textwidth}
    \vspace{0pt}
    \includegraphics[width=1\textwidth]{figures/bar_fat.pdf}
\end{minipage}\hfill
\begin{minipage}[t]{0.56\textwidth}
\caption{\textbf{Extend RIDE to various long-tailed recognition methods.} Consistent improvements can be observed on CIFAR100-LT, which illustrates that the proposed method can be applied to various training mechanisms, either methods that are end-to-end (e.g. LDAM) or require another stage of process (e.g. cRT and -norm). By using RIDE, cross-entropy loss (without any re-balancing strategies) can even outperforms current SOTA method LFME. Although higher accuracy can be obtained using distillation, we did not apply it here.}
    \label{fig:MultiMethods}
\end{minipage}\vspace{-14pt}
\end{figure}
}

\def\figHardestTsne#1{
\begin{figure}[#1]
    \centering
    \begin{subfigure}[t]{0.99\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/hardest_neg_all.pdf}\vspace{-6pt}
\caption{Histograms of each instance's hardest negative class confidence score for each split of CIFAR100-LT.}
        \label{fig:hardest_negative}
    \end{subfigure}
    \begin{subfigure}[t]{0.99\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/tsne_all.pdf}\vspace{-6pt}
\caption{t-SNE visualization of LDAM's and our model's embedding space of CIFAR100-LT.}
        \label{fig:tsne}
    \end{subfigure}\vspace{-6pt}
    \caption{{\bf{Why RIDE performs better?}} (a) Compared with head class, tail class is easier to confuse with nearest neighbor class. The integration of RIDE greatly reduces the confidence score of hardest negative class, which illustrates that our model are less confused with nearest negative class, especially for the tail category. (b) The feature embedding of RIDE is more compact for both head and tail classes and better separated.This behavior greatly reduces the difficulty for the classifier to distinguish head and tail categories.}
\end{figure}
}

\def\figHardest#1{
\begin{figure}[#1]
    \centering
    \includegraphics[width=1\textwidth]{figures/hardest_neg_all.pdf}\vspace{-4mm}
    \caption{\textbf{Histograms of confidence score of hardest negative class} for each split of CIFAR100-LT. RIDE is less confused with nearest negative class, especially for the tail category.}
    \label{fig:hardest_negative}
\end{figure}
}

\def\figTsne#1{
\begin{figure}[#1]
    \centering
        \includegraphics[width=1\textwidth]{figures/tsne_all.pdf}\vspace{-4pt}
\caption{t-SNE visualization of LDAM's and our model's embedding space of CIFAR100-LT. The feature embedding of RIDE is more compact for both head and tail classes and better separated. This behavior greatly reduces the difficulty for the classifier to distinguish the tail category.}
    \label{fig:tsne_cifar100}
\end{figure}
}


\section{Experiments}
\label{experiments}

\subsection{Datasets and Implementations}

We conduct experiments on three major long-tailed recognition benchmarks and different backbone networks to prove the effectiveness and universality of RIDE:

{\bf{CIFAR100-LT}} \citep{cao2019learning}: The long-tailed version of CIFAR100 follows an exponential decay in sample sizes across different classes. We conduct experiments on CIFAR100-LT with an imbalance factor of 100, with the ResNet-32 \citep{he2016deep} as a backbone network. 

{\bf{ImageNet-LT}} \citep{liu2019large}: RIDE with ResNet-50 and ResNeXt-50 \citep{xie2017aggregated} are experimented on ImageNet-LT. More details and experiments on other backbones are listed in appendix.

{\bf{iNaturalist}} \citep{van2018inaturalist}: We use ResNet-50 as the backbone network and apply the same training recipe as ImageNet-LT except with batch size 512 as  \cite{kang2019decoupling} does.

\tabCifar{!t}

\subsection{Experimental Results}

{\bf{CIFAR100-LT}}. Table \ref{tab:cifar100} shows that RIDE outperforms state-of-the-art methods by a large margin on CIFAR100-LT. The average computational cost is even about 10\% less than baseline models when we only apply 2 cascaded experts as in BBN. Compared with LFME \citep{xiang2020learning} and BBN \citep{zhou2020bbn}, which also apply multiple experts, RIDE significantly surpasses both by more than 5.3\% and 6.5\%, respectively. Since LDAM is end-to-end optimized, we choose it as the default training method for simplicity, unless otherwise noticed.

\figMultiMethods{!h}

{\bf{Integrating RIDE with various methods}}. \fig{MultiMethods} indicates that our method will likely be able to benefit from the future advancements to the loss function and training process. Consistent improvements can be observed. Cosine classifier is a baseline that we constructed in which we normalize the weights of the classifier and apply resample in the classifier retrain stage, similar to cRT. 
Since two-stage methods require an additional training stage, we use LDAM as our default choice.

\tabImageNetResAll{!t}

{\bf{ImageNet-LT}}. We further evaluate RIDE on ImageNet-LT with various backbones as in Table \ref{table:imagenet-lt}. Compared with current SOTA methods, LWS and cRT, RIDE achieves new state-of-the-art results and outperforms SOTA by more than 7.7\% with ResNet-50. ResNeXt-50 is based on group convolution \citep{xie2017aggregated}, which divides all filters into several groups and aggregates information from multiple groups. ResNeXt-50 generally performs better than ResNet-50 on multiple tasks. Using ResNeXt-50, we can see a performance improvement of 6.9\%.

\tabiNaturalist{!h}

{\bf{iNaturalist}}. iNaturalist 2018 is a naturally imbalanced fine grained dataset with 8,142 categories. Table \ref{table:inaturalist} shows that RIDE outperforms current SOTA by 6.3\%. Surprisingly, RIDE obtains very similar results in many-shots, medium-shots and few-shots, which is a very ideal result for the long tailed recognition task. Current SOTA method BBN also uses multiple experts. Compared with BBN, which significantly decreases the performance on many-shot classes by about 23\%, RIDE increases the accuracy on few-shot without sacrificing the performance on many-shot categories.

\tabAblation{!t}
{\bf{Contribution of each component of RIDE.}} 
RIDE is jointly trained with  and , we use LDAM as  by default. Table \ref{table:ablation} shows that the architectural change from the original ResNet-32 to the RIDE variant with 2  4 experts contributes 2.7\%  4.3\% accuracy improvements. The change from applying our classification loss naively to model output (collaborative loss) to applying to each expert as an individual loss brings 1.5\% improvements. Adding diversity further improves about 0.9\%. The computation cost is greatly reduced by adding the expert assignment module. Knowledge distillation from RIDE with 6 experts obtains another 0.6\% increase. All these coherent components of RIDE enable it to get 7.1\% increase compared with baseline LDAM.

{\bf{Influence of expert number.}} Fig. \ref{figure:expertsNum} shows the influence of expert number for each split. Compared with many-shot, which only obtains 3.8\% relative improvements by using 8 experts, the accuracy of few-shot classes is relative increased by more than 16\%, which indicates that few-shot classes can enjoy more benefits from using more experts. No distillation is applied in this comparison.

{\bf{Number of experts allocated to each split.}} \fig{pieChart} shows that: 
Most instances in the many-shot classes are assigned only 1 expert, whereas those in few-shot classes are often assigned more experts.  The low confidence in low-shot instances requires the model to seek a second (or a third, ...) opinion.

\figPieExpertsExperts{!t}
 \section{Summary}

We propose a novel multi-expert approach for long-tailed recognition.  It trains partially shared diverse distribution-aware experts and routes an instance to additional experts when necessary, with computational complexity comparable to a single expert. RIDE significantly outperforms SOTA methods by a large margin on all the benchmarks including CIFAR100-LT, ImageNet-LT, and iNaturalist. It can also be applied to various backbones and methods with consistent performance gains.  

{\bf Acknowledgments.}
This work was supported, in part, by Berkeley Deep Drive, US Government fund through Etegent Technologies on Low-Shot Detection and Semi-supervised Detection, and NTU NAP and A*STAR via  Industry Alignment Fund: Industry Collaboration Projects Grant.

\clearpage
\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\clearpage
\appendix
\def\tabKT#1{
\begin{table}[#1]
\caption{\textbf{Comparison of different distillation methods.}
 We transfer from a model based on ResNet-32 with 6 experts to a model of the same type, except with fewer experts. We use CIFAR100-LT for the following comparison. No expert assignment module is used in the following experiments. Following the procedure for CRD \citep{tian2019contrastive}, we also apply KD when we transfer from a teacher to students with other distillation methods.
}
\label{table:KT}
\begin{center}
\begin{tabular}{l|ccccc|ll}
\shline
Model Type & \#expert & Distillation Method & Accuracy (\%)\\ 
\shline
Teacher & 6 & & 49.7 \\
\hline
\multirow{15}{*}{Student} & 2 & No Distillation & 46.6 \\
& 2 & KD \citep{hinton2015distilling} & 47.3 \\
& 2 & CRD \citep{tian2019contrastive} & \textbf{47.5} \\
& 2 & PKT \citep{passalis2018learning} & 47.2 \\
& 2 & SP \citep{tung2019similaritypreserving} & 47.2 \\
\cline{2-4}
& 3 & No Distillation & 47.9 \\
& 3 & KD \citep{hinton2015distilling} & 48.4 \\
& 3 & CRD \citep{tian2019contrastive} & 48.5 \\
& 3 & PKT \citep{passalis2018learning} & 48.3 \\
& 3 & SP \citep{tung2019similaritypreserving} & \textbf{48.7} \\
\cline{2-4}
& 4 & No Distillation & 48.7 \\
& 4 & KD \citep{hinton2015distilling} & \textbf{49.3} \\
& 4 & CRD \citep{tian2019contrastive} & 49.0 \\
& 4 & PKT \citep{passalis2018learning} & 48.9 \\
& 4 & SP \citep{tung2019similaritypreserving} & 49.0 \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\figBranchesEnsembles#1{
\begin{figure}[#1]
    \begin{minipage}[t]{0.5\textwidth}
        \vspace{0pt}
        \includegraphics[width=1\textwidth]{figures/branches_ensembles.pdf}\vspace{-4pt}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.48\textwidth}
        \vspace{20pt}
        \caption{Comparison between our method and multiple LDAM models ensembled together. In the figure, ensembles of LDAM start from 1 ensemble (original LDAM) to 7 ensembles, and RIDE starts from 2 experts to 4 experts. Our method achieves higher accuracy with substantially less computational cost compared to ensemble method.}
        \label{fig:branches_ensembles}\vspace{-6pt}
    \end{minipage}\vspace{-14pt}
\end{figure}
}

\section{Appendix}
\subsection{Datasets and Implementations}

We conduct experiments on three major long-tailed recognition benchmarks and different backbone networks to prove the effectiveness and universality of RIDE:
\begin{enumerate}[leftmargin=0cm,labelwidth=\itemindent,labelsep=0cm,align=left]
\item {\bf{CIFAR100-LT}} \citep{Krizhevsky09learningmultiple,cao2019learning}: The original version of CIFAR-100 contains 50,000 images on training set and 10,000 images on validation set with 100 categories. The long-tailed version of CIFAR-100 follows an exponential decay in sample sizes across different categories. We conduct experiment on CIFAR100-LT with an imbalance factor of 100, i.e. the ratio between the most frequent class and the least frequent class.

To make fair comparison with previous works, we follow the training recipe of
\citep{cao2019learning} on CIFAR100-LT. We train the ResNet-32 \citep{he2016deep} backbone network by SGD optimizer with a momentum of 0.9. CIFAR100-LT is trained for 200 epochs with standard data augmentations \citep{he2016deep} and a batch size of 128 on one RTX 2080Ti GPU. The learning rate is initialized as 0.1 and decayed by 0.01 at epoch 120 and 160 respectively.
\item {\bf{ImageNet-LT}} \citep{deng2009imagenet,liu2019large}: ImageNet-LT is constructed by sampling a subset of ImageNet-2012 following the Pareto distribution with the power value  \citep{liu2019large}. ImageNet-LT consists of 115.8k images from 1,000 categories, with the largest and smallest categories containing 1,280 and 5 images, respectively.

Multiple backbone networks are experimented on ImageNet-LT, including ResNet-10, ResNet-50 and ResNeXt-50 \citep{xie2017aggregated}. All backbone networks are trained with a batch size of 256 on 8 RTX 2080Ti GPUs for 100 epochs using SGD with an initial learning rate of 0.1 decayed by 0.1 at 60 epochs and 80 epochs. We utilize standard data augmentations as in \citep{he2016deep}.

\item {\bf{iNaturalist}} \citep{van2018inaturalist}: The iNaturalist-2018 dataset is an imbalanced datasets with 437,513 training images from 8,142 classes with a balanced test set of 24,426 images. We use ResNet-50 as the backbone network and apply the same training recipe as ImageNet-LT, except that we use a batch size of 512.
\end{enumerate}

\subsection{Additional Experiments}

\figBarPlot{!h}

{\bf{Ablation study on distillation methods.}} 
\label{Knowledge_transfer}
Self-distillation step is optional but recommended if further improvements (0.4\%0.8\% for most experiments) are desired. We apply distillation from a more powerful model with more experts into a model with fewer experts. A simple way to transfer knowledge is knowledge distillation (KD) \citep{hinton2015distilling}, which applies KD loss 
()
to match the distribution of logits of a teacher and a student. We found that for teacher model with more experts using smaller distillation loss factor gives better performance. We hypothesize that since we distill from the same teachers, giving large distillation factor prevents the branches from becoming as diversified as it is able to. We also explored other distillation methods, such as CRD \citep{tian2019contrastive}, PKT \citep{passalis2018learning}, and SP \citep{tung2019similaritypreserving}, and compared the differences in Table \ref{table:KT}. Although adding other methods along with KD may boost performance, the difference is small. Therefore, we opt for simplicity and use KD only unless otherwise noticed.

\tabKT{!t}

{\bf{Detailed results for ImageNet-LT experiments}}. 
We list details of our ResNet-50 experiments in ImageNet-LT on Table \ref{table:imagenet_lt_resnet50}. With 2 experts, we are able to achieve about 7\% gain in accuracy with computational cost about 10\% less than baseline. In contrast to previous methods that sacrifice many-shot accuracy to get few-shot accuracy, we improve on all three splits on ImageNet-LT. From 3 experts to 4 experts, we keep the same many-shot accuracy while increasing the few-shot accuracy, indicating that we are using the additional computational power to improve on the hardest part of the data rather than uniformly applying to all samples. 

We also list our ResNet-10 and ResNeXt-50 experiments on Table \ref{table:imagenet_lt_resnet10} and \ref{table:imagenet_lt_resnext50}, respectively, to compare against other works evaluated on these backbones. Our method also achieves lower computational cost and higher performance when compared to other methods.  

As illustrated in \fig{relative-improve}, our approach provides a comprehensive treatment to all the many-shot, medium-shot and few-shot classes, achieving substantial improvements to current state-of-the-art on all aspects. Compared with cRT which reduces the performance on the many-shot classes, RIDE can achieves significantly better performance on the few-shot classes without impairing the many-shot classes. Similar observations can be obtained in the comparison with the state-of-the-art method BBN \citep{zhou2020bbn} on iNaturalist.

{\bf{Comparison with ensemble method.}}
Since our method requires the joint decision from several experts, which raw ensembles also do, we also compare against ensembles of LDAM in Fig.\ref{fig:branches_ensembles} on CIFAR100-LT. In the figure, even our method with 4 experts has less computational cost than the minimum computational cost for the ensemble of 2 LDAM models. This indicates that our model is much more efficient and powerful in terms of computational cost and accuracy than ensemble on long-tailed datasets.

\figBranchesEnsembles{t}
\figTsne{!h}

\textbf{t-SNE visualization}. We also provide the t-SNE visualization of embedding space on CIFAR100-LT as in \fig{tsne_cifar100}. Compared with the baseline method LDAM, the feature embedding of RIDE is more compact for both the head and tail classes and better separated from the neighboring classes. This greatly reduces the difficulty for the classifier to distinguish the tail category.

{\bf{What if we apply RIDE to balanced datasets?}} We also conducted experiments on CIFAR100 to check if our method can achieve similar performance gains on balanced datasets. However, we only obtained an improvement of about 1\%, which is much smaller than the improvements observed on the CIFAR100-LT. Compared with balanced datasets, long-tailed datasets can get more benefits from RIDE.

\tabImageNetResS{!ht}
\tabImageNetResNetLSplit{!t}
\tabImageNetResNeXtLSplit{!t} 
\end{document}
