\documentclass[11pt]{article}



\usepackage{fullpage}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{amsmath}




\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{obs}[thm]{Observation}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{claim}[thm]{Claim}
\newtheorem{rem}[thm]{Remark}
\newtheorem{defn}[thm]{Definition}

\def \OPT  {\mbox{\rm OPT}}






\begin{document}
\title{The Geometry of Scheduling}
\author{
Nikhil Bansal
\thanks{IBM T. J. Watson Research Center,
Yorktown Heights, NY 10598 USA.
E-mail: nikhil@us.ibm.com}
\and
 Kirk Pruhs 
 \thanks{
Computer Science Department, 
University of Pittsburgh, 
Pittsburgh, PA 15260 USA.
Email: kirk@cs.pitt.edu.
Supported in part by
NSF grants CNS-0325353,  IIS-0534531, and CCF-0830558, and
an IBM Faculty Award.} 
} 
\date{}
\maketitle

\begin{abstract}
We consider the following general scheduling problem: 
The input consists of  jobs, each with an arbitrary release time,
size, and a monotone function 
specifying the cost incurred when the job is completed at a particular time.
The objective is to find a preemptive schedule of minimum aggregate cost.
This problem formulation is general enough to include many natural scheduling 
objectives, such as weighted flow, weighted tardiness, and sum of flow squared.

The main contribution of this paper 
is a randomized polynomial-time algorithm with an approximation ratio 
,
where  is the maximum job size.
We also give an  approximation in the special case when all jobs have identical release times.
Initially, we show how to reduce this scheduling problem
to a particular geometric set-cover problem.
We then consider a natural linear programming formulation of this geometric set-cover problem, 
strengthened by adding knapsack cover inequalities,
and show that rounding the solution of this linear program can be reduced to 
other particular geometric set-cover problems.
We then develop algorithms for these sub-problems using the local ratio technique,
and Varadarajan's quasi-uniform sampling technique.

This general algorithmic approach improves the best known approximation ratios by at least an 
exponential factor (and much more in some cases) for essentially 
{\em all} of the nontrivial common special cases of this problem.
We believe that this geometric interpretation of scheduling 
is of independent interest.
\end{abstract}



\section{Introduction}




We consider the following general offline scheduling problem:

\medskip
{\em General Scheduling Problem (GSP):}
The input consists of a collection of  jobs, and for each job 
a positive integer release time ,
a positive integer size , 
and a cost or weight function  for each  
(we are purposely not precise about how these weight functions are represented
in the input). 
Jobs are to be scheduled preemptively on
one processor after their release times.  If job  completes at time , then a
cost of  is incurred. The scheduling objective is to minimize
the total cost, , where  is the completion
time of job .




This general problem generalizes several natural scheduling problems, for example:

\medskip
{\em Weighted Flow Time:}
If , where  is some fixed 
weight associated with job , then the objective is weighted flow time.

\smallskip

{\em Flow Time Squared:}
If , then the objective is the sum of the squares of the flow times. 

\smallskip

{\em Weighted Tardiness:}
If  for  not greater than some deadline ,
and  for  greater than ,
then the objective is weighted tardiness.
\medskip


In general, this problem formulation can model any cost objective function that is the 
sum of arbitrary cost functions for individual jobs, provided these cost functions are non-decreasing, i.e. it cannot hurt to finish a job earlier.

Flow time, which is the duration of time  
that a job is in the system,
is clearly the most natural and most commonly used quality of service measure for a job
in the computer systems literature.
Many commonly-used and commonly-studied scheduling objectives are based on combining
the flow times of the individual jobs.
However, flow time is also considered a rather difficult measure to work with mathematically. 
One reason for this is that even slight perturbations to the instance, can lead to
lead to large changes in the optimum value. 
Despite much interest, large gaps remain in our understanding for even basic 
flow time based scheduling objectives.
For example, for weighted flow time, the best known approximation ratios achievable by
polynomial-time algorithms are essentially no better than the poly-logarithmic 
competitive ratios achievable by online algorithms. 
For weighted tardiness, and flow time squared, no nontrivial approximation ratios were 
previously known
to be achievable.
While in contrast, for all of these three problems,
even the possibility of a polynomial time approximation scheme (PTAS) 
has not been ruled out.
We discuss the related previous work further in Section \ref{s:related}.




\subsection{Our Results}
The main contribution of this paper is the design and analysis of
a randomized -approximation
algorithm for GSP,  where  is the maximum job size. 
In the special case when all the release times are 0, we
obtain an  -approximation algorithm.
Let  be the maximum value attained by any weight function.
The running time of our algorithm is polynomial in ,  and ,
provided that we can in polynomial time determine the times when a weight
function doubles. 
This is polynomial in the input size if the input 
must contain an explicit representation of the largest possible weight.




The primary insight to obtain these results is to view the scheduling problem geometrically.
The initial step is to show that GSP can be
reduced 
(with only a constant factor loss in the approximation ratio)
to the following geometric set-cover problem that we call R2C:

\bigskip

{\em Definition of the R2C Problem:}
The input consists of a collection of  points in two dimensional space,
and for each point  an associated positive integer demand .
Each point  is specified by its coordinates .
Further the input contains a collection  of axis-parallel rectangles, each of them abutting 
the -axis. That is, each rectangle  has the form  . 
In addition, each rectangle  has an associated positive integer capacity  
and positive integer weight .
The goal is to find a minimum weight subset  of rectangles, 
such that for each point , the total capacity of rectangles covering  is at least ,
that is, .

\bigskip
As we shall see later, job sizes will be mapped to rectangle capacities in our reduction,
so we will also use  to denote the largest capacity of any rectangle.
Our algorithm for R2C starts with the natural linear programming (LP) 
relaxation of the problem, 
strengthened by adding the so-called knapsack cover inequalities.
To round this LP solution, our algorithm then proceeds in a way that is by now standard (see for example \cite{CGK10}) in the applications of knapsack cover inequalities. In the terminology of \cite{CGK10}, we reduce the problem to rounding an LP solution for the so-called {\em priority} set cover version of the problem and in addition
several set multi-cover problems. These resulting problems are simpler as they are uncapacitated.

In particular we proceed as follows.
The algorithm first picks rectangles that are selected by
the LP solution to a significant extent (i.e. , for some fixed constant ),
and then considers the {\em residual} solution.
The knapsack cover inequalities guarantee that remaining LP variables for a feasible solution 
to the residual instance. Since all variables  in this solution,
the capacities and demands can be rounded to powers of , and the variables can be scaled by
a constant factor, so that each point's demand is covered several times over. 


Points are then classified as heavy or light depending on whether or not
the optimal LP solution extensively covers the point with rectangles whose capacity
is larger than the demand of the point. 
We reduce the problem of covering the heavy points by rectangles with higher capacity to the
geometric cover problem R3U defined below. 
We show that the instances of R3U that we obtain 
have boundaries with low union complexity.
In particular, the boundary of the union of any  objects 
has a complexity of .
Using Varadarajan's quasi-uniform sampling technique~\cite{Varadarajan09} 
for approximating weighted set cover on geometric instances with low union 
complexity, one can obtain a covering that is  an
-approximation to fractional cover specified by the
LP solution.



\medskip

{\em Definition of the R3U Problem:}
The input consists of a collection of  points in three dimensional space.
Each point  is specified by its coordinates .
Further the input contains a collection  of axis-parallel right cuboids
each of them abutting the  and  coordinate planes. 
That is, each right cuboid  has the form  
. 
In addition, each right cuboid  has an associated positive 
integer weight .
The goal is to find a minimum weight subset  of cuboids
such that each point  is covered by at least one cuboid.


\medskip

We reduce the problem of covering the light points to 
different instances, one for each possible job size, 
of the weighted geometric multi-cover problem R2M defined below. 
We then show how to use the local ratio technique to obtain a solution
for each instance of R2M that is -approximate with the cost in the
optimal LP solution for jobs of this size. Combining these solutions for various sizes
implies a solution for covering all light points with cost  times the LP cost.

\medskip

{\em Definition of the R2M Problem:}
The input consists of a collection of  points in two dimensional space,
and for each point  an associated positive integer demand .
Each point  is specified by its coordinates .
Further the input contains a collection  of axis-parallel rectangles, 
each of them abutting the -axis. 
That is, each rectangle  has the form  . 
In addition, each rectangle  has an associated 
positive integer weight .
The goal is to find a minimum weight subset  of rectangles, 
such that for each point , the number of 
rectangles covering  is at least .




\subsection{ Identical Release Times}
In the instances of R2C that arise from our reduction from the 
general scheduling problem, in the special case of identical release times,
all the points lie on a line, and the rectangles are one-dimensional intervals.
This is precisely the generalized caching problem, for which a  polynomial-time
4-approximation algorithm is known~\cite{BBF} (see also~\cite{CGK10}, for a somewhat more systematic approach to it). 
Thus we conclude that there is a polynomial-time -approximation 
algorithm for GSP
when all release times are identical.


\subsection{ Related Results}
\label{s:related}
Let us first consider weighted flow time.
\cite{BansalD07}
gives an online algorithm that is 
-competitive, and a semi-online algorithm 
(which means that the parameters  and  must be known
a priori to the online algorithm) that 
is -competitive. \cite{ChekuriKZ01} gives a semi-online algorithm that is -competitive.
These online algorithms also give the best known approximation ratios 
for polynomial time algorithms.
\cite{ChekuriK02}
gives a -approximation algorithm that 
 has running time . 
Thus, this gives a quasi-polynomial time approximation
scheme (QPTAS) when both  and  are polynomially bounded in . 
Moreover, \cite{ChekuriK02} also gives a QPTAS for the case when only one of either  or
 is polynomially bounded in . 
In the special case that the weights are the reciprocal of the job sizes, and hence the objective is
average stretch/slow-down, then there is a polynomial time approximation
scheme~\cite{BenderMR04,ChekuriK02}.

It is also known that the algorithm highest density first is 
-speed -competitive for weighted flow~\cite{BecchettiLMP06}
and flow squared~\cite{BansalP03}. No other approximation guarantees
are known for flow squared.
An -approximation algorithm is known for weighted tardiness if
all jobs are released at the same time~\cite{Cheng2005}, and nothing seems to be known for arbitrary release dates.
PTAS's are known with the additional restriction that there are only a constant
number of deadlines~\cite{KarakostasKW09} or if jobs  have unit 
size~\cite{Lawler1982}. In general, there has been other extensive work on flow time related objectives and we refer the reader to  \cite{PST} for a survey.


The goal in geometric set cover problems is to improve the  set-cover bound using 
geometric structure. This is an active area of research and various different techniques have been developed.
However, until recently most of these techniques applied only to the {\em unweighted} case. 
A key idea is the connection between set covers and  
-nets  \cite{BronnimannG95}, where an -net is a sub-collection of sets that covers all the points that lie in at least an  fraction of the input sets. 
For any geometric problem, existence of 
 -nets of size at most  implies -approximate solution for unweighted set cover~\cite{BronnimannG95}.
Thus, proving better bounds on sizes of -nets  (an active research of research
is discrete geometry) directly gives improved guarantees for unweighted set-cover.
In a surprising result,~\cite{ClarksonV07} related the guarantee for unweighted set-cover
to the union complexity of sets.
If particular, if the sets have union complexity ,
which roughly means that the number of points on the boundary of
the union of any collection of  sets is ,
then one can obtain an  approximation~\cite{ClarksonV07}. This was subsequently improved 
to  \cite{Varadarajan09}.
In certain cases these results also extend to the  unweighted multi-cover case \cite{CCS}.
However, these techniques do not apply to weighted set cover problems: the problem is that
these techniques may sample some sets with much higher 
probability than that specified by the LP relaxation. 
In a recent breakthrough,  Varadarajan gave a new quasi-uniform sampling technique~\cite{Varadarajan10}
that obtains a  approximation for weighted geometric set cover problems
with union complexity . In fact his result gives an improved guarantee of  if  grows with  (even very mildly such as , where the  is iterated   times).

{\em Organization:}
The paper is organized as follows. In
section \ref{sec:reduce} the reduction from GSP to R2C is given.
In section \ref{sec:preliminaries} we give the LP formulation
of R2C and explain the initial preprocessing of the LP solution.
In section \ref{sec:heavy}
we explain how to reduce part of the problem
of rounding the LP solution to an instance of the R3U problem.
In section \ref{sec:light}
we explain how to reduce part of the problem
of rounding the LP solution to an instance of the R2M problem.


\section{The Reduction from GSP to R2C}
\label{sec:reduce} 
Our goal in this section is to prove Theorem \ref{red}.
We accomplish this by giving a reduction from GSP
to R2C, and then showing that this reduction increases the objective 
value of the optimal solution by at most a factor of four
(Lemma \ref{gc:s}), and that this reduction doesn't shrink the
objective value of the optimal solution (Lemma \ref{s:gc}).

\begin{thm}
\label{red}
A polynomial-time -approximation algorithm for R2C 
implies a polynomial-time  approximation algorithm for GSP.
\end{thm}


\medskip

{\em Definition of the Reduction from GSP to R2C:}
From an arbitrary instance  of GSP,
we explain how to create an instance  of R2C.
Considering ,
we say that a time  is of class  with respect to job  if 
the cost of finishing  at time  lies in , 
i.e.  . We say that  is of class , if the cost of finishing  at  is .
Let  denote the (possibly empty) time interval of class  times with respect to job .
Let  denote the set of all points that are endpoints of the intervals 
of the form  for some job  and class . 
For each time interval  of the form , 
where  and , we create a point  in 
with demand
, where  denotes the total size of jobs that are released during , 
i.e. .
For each job  in  
and , we create a rectangle 
in  with capacity  and weight .
We note that the rectangles  corresponding to the same job are pairwise disjoint.

Without loss of generality, we may assume that the time horizon is ,
otherwise the instance can be divided into disjoint non-interacting subsets.
Thus the maximum cost for any job can be , so . This implies that 
we can assume that  and that  , 
i.e. polynomial in the size of the input. 
Throughout the paper we will use  to denote the number of points in the R2C problem.
Clearly, . 



\begin{lemma}
\label{gc:s}
If there is a feasible solution  to  
with objective value , 
then there there is a feasible solution  to 
with objective value at most .
\end{lemma}
\begin{proof}
For job  in , 
let  denote the class during which  finishes in 
(i.e.  is the smallest integer such that the cost 
incurred by  in  is ).
Consider the solution  obtained by choosing for each job , 
the intervals .
Clearly, each job contributes at most ,  i.e. at most 4 times its contribution to , and hence the total cost of   
is at most  times the cost of .

It remains to show that   is feasible, i.e. for any point , the total capacity of rectangles covering  is at least . 
Suppose  corresponds to the time interval  from . 
Let  denote the jobs that arrive during . 
For  each job  that completes after , 
there is exactly one rectangle  that covers . 
Since  is a feasible schedule, the total size of jobs in  
that can complete during  itself cannot be more than 
. Thus the jobs in  that do not complete during  
must have a total size of at least ,
which is the covering requirement for .
\end{proof}

\begin{lemma}
\label{s:gc}
If there is a feasible solution  to  
with objective value , 
then there there is a feasible solution  to 
with objective value at most .
\end{lemma}
\begin{proof}
For each job , let  denote the largest index such that the rectangle  lies in .
Let us set a deadline  for  as the right end point of . 


We claim that there is a schedule  that completes each job
 by time . Consider the bipartite graph defined as follows: We have time slots  on the right. For each job , we have
 vertices on the left, each of which is connected to vertices  on the right.
By Hall's theorem, a feasible schedule exists if and only if for any time interval , the total size of jobs
that have both release times and deadlines in  is at most .
Moreover, it suffices to show such a result for intervals  of the form 
, for some jobs  and .
Equivalently, for any such time interval , 
the jobs  that are released during  and have  after the end of ,
have a total size of at least . 
 
Note that by the definition of , then there is a point
 in  that corresponds to the interval .
Then by the feasibility of , 
the total capacity of rectangles covering  in 
is at least . And as all of these rectangles 
correspond to different jobs in  (the rectangles corresponding to the same job are pairwise disjoint), we are done.


In  the cost of  is at most , since by the definition of the rectangle  the cost of finishing a job by deadline  is at most . Now, the cost incurred by  in  is at least  (since the rectangle  already has cost ). This implies that the cost of  is at most that of .
\end{proof}

\bigskip

{\em Identical Release times:}
Without loss generality, let  for all . In this case, the above reduction become simpler. In particular, the first dimension corresponding to release time becomes irrelevant and we obtain the following problem. For each job  and , there is an interval  corresponding to class  times with respect to  and has capacity  and weight .
All relevant intervals  are of the form 
 for  and have demand , where  is the total size of all the jobs. 
For each such , we introduce a point  with demand .
The goal is to find a minimum weight subcollection of intervals  such that covers the demand.
This is a special case of the following Generalized Caching Problem.

\medskip
{\em Generalized Caching Problem:} The input consists of a set of demands  at various time steps . In addition there is a collection of 
time intervals  , where each interval  has weight , size  and span
 with . The goal is to find a minimum weight subset of intervals that covers the demand. That is, find the minimum weight subset of intervals  such that 


A -approximation for this problem was obtained by Bar-Noy et al. \cite{BBF}, based
on the local-ratio technique. Their algorithm can equivalently be viewed as a primal dual algorithm
applied to a linear program with knapsack cover inequalities \cite{BR}.
This immediately implies a -approximation for GSP in the case of identical release times.


\section{The LP Formulation for R2C}
\label{sec:preliminaries}

The following is a natural integer programming formulation for
R2C. For each rectangle  there is an indicator variable
 specifying whether or not the rectangle  is selected.



It is easily seen 
that the natural relaxation of this linear program, where  
is replaced by , has a
large integrality gap. In particular, this is true even when  consist of a single point, in which case the problem is equivalent to the knapsack cover problem \cite{CarrFLP00}.
Thus, we strengthen this LP by adding knapsack cover inequalities introduced in \cite{CarrFLP00}
have proved 
to be a useful tool to address capacitated covering problems~\cite{gencaching, shmoys, sviri, BansalGK2010, CGK10}. 

This gives the  the following linear program:

Here  denotes the total capacity of rectangles in .
The constraints are valid for the following reason: For any subset ,  even if all the items in  are chosen, at least a demand of  must be covered by remaining rectangles. 
Moreover, truncating an item size to the residual capacity does not affect the feasibility of an integral solution. Even though there are exponentially many constraints per point, 
a feasible -approximate solution, for any constant , can be found using the Ellipsoid algorithm, see
\cite{CarrFLP00} for details. Further 
only the cost incurs the  factor loss, 
all the constraints are satisfied exactly.
We will refer the inequalities in line (\ref{c}) 
as the {\em knapsack cover inequalities}.



Let  be some -approximate feasible solution to the linear
program for R2C in lines (\ref{a})-(\ref{b}), 
and let  denote 's objective value. 


We now apply some relatively standard steps to simplify .
Let  be  a small constant,  suffices. 
Let  denote the set of rectangles for which . 
We pick all the rectangles in , i.e. set . Clearly, this cost of this set is at most  times the LP solution.

For each point , let  denote the set of rectangles in  that cover . Let us consider the residual instance, where the set of rectangles is restricted to  
and the demand of a point is . If , then  is already covered by  and we discard it.

Since the solution  satisfied all the knapsack cover inequalities for each point  and set , and hence in particular 
for every  and corresponding the set , we have
that 

Henceforth, this is the only fact we will use about the solution  (in particular, we do not care that 
satisfies  several other inequalities for each point ).
Let us scale the solution  restricted to  by  times. Call this solution . Note that since , it still holds that   .  Clearly,  satisfies

Let us define the new demand  of  as 
 rounded up to the nearest integer power of .
Similarly, defined a new capacity  of 
each rectangle  to be  rounded down to the nearest integer power of .
 still satisfies,


We call  a class  rectangle if . Similarly,  is a class  point if .
We call a point  {\em heavy} if is covered by rectangles with class at least as high as that of  in the LP solution, more precisely
if:

Equivalently,  is heavy if 
 
Otherwise we say that a point is {\em light}. Thus a light point satisfies:

We now have different algorithms for covering heavy and light points.

\section{Covering Heavy Points}
\label{sec:heavy}

In this section 
we show how reduce the problem of covering the heavy
points by larger class rectangles to R3U.
We then show that the resulting instances of R3U have low union complexity.
In particular any  cuboids in a resulting R3U instance
has union complexity .
By Varadarajan's quasi-uniform sampling technique~\cite{Varadarajan09}
this gives 
a solution that is an  approximation to the 
optimal fractional solution of this R3U instance. 
As  gives
a feasible fractional solution to this R3U instance, this means
that the cost of cuboids that the algorithm selects is  
approximate with .


\medskip

{\em The Problem of 
Covering the Heavy Points to R3U:} 
The reduction takes as inputs 
the instance  for heavy points obtained at the end of the previous section, and the LP solution  
and creates an instance  of R3U.
For each heavy point  with demand , 
there is a point  in .
For each rectangle  in  with capacity , 
we define a right cuboid 
of weight .


\medskip
It is clear that there is a one to one correspondence between
a covering of heavy points in  by rectangles of no smaller
class and a covering of the points in  by cuboids. 
Given a collection  of  geometric objects, the union complexity of  is
number of edges in the arrangement of the boundary of . For 3-dimensional objects, this is the 
total number of vertices, edges and faces on the boundary of . 
In Lemma \ref{2dunion} and Lemma \ref{3dunion}
we bound the union complexity of cuboids in .

\begin{lemma}
\label{2dunion}
For any collection of  rectangles of the type  , the union complexity  is .
\end{lemma}
\begin{proof}
For each rectangle of the form  has a side touching the -axis. Let us view of union of  such rectangles from . Consider the vertical faces on the boundary of the union.
For any two rectangles  and , the pattern  or  cannot appear. Thus the vertical faces from a Davenport Schinzel sequence of order 2, which has size at most  (see for example \cite{Mat}, chapter 7). Since the number of vertices is  times the number of faces, the result follows. 
\end{proof}

\begin{lemma}
\label{3dunion}
The union complexity of any  cuboids in  is .
\end{lemma}
\begin{proof}
This directly follows from lemma \ref{2dunion} and noting that the number of distinct heights is . In particular, since the heights of powers of 2, consider the  slice of the arrangement between  and . This corresponds to union of rectangles of the form . 
\end{proof}

\noindent
{\em Remark:} We remark that the bound in lemma \ref{3dunion} is tight for kind of cuboids we consider here.

The following result  is implicit in \cite{Varadarajan10}.
\begin{thm} [\cite{Varadarajan10}]
There is a randomized polynomial-time algorithm that,
given a weighted geometric set cover instance 
where the union complexity of any  objects is ,
produces an set cover of weight at most a factor of  times the 
optimal fractional set cover.

If the function  grows even very mildly with , say in particular that , where the  is iterated  times, then the approximation guarantee above is .
\end{thm}


Thus we can conclude that in polynomial time one find rectangles
in the R2C instance  that 
covers all the heavy points and that has weight at most 
 times .





\section{Covering Light Points}
\label{sec:light}

In this section 
we show how to decompose the problem of covering the light 
points to  instances of R2M, one instance  for 
each possible rectangle capacity class . The decomposition ensures that an  approximation for R2M implies an cover for light points in  with cost 
 times .
We then give an obtain an  approximation for an R2M instance on  points. 
To do this, we relate the multi-cover 
problem to the set cover problem (where all demands are ) and show that the set cover problem has
a 2-approximation with respect to the fractional solution. 
This implies that the cost of rectangles that the algorithm selects for  is 
approximate with .

\noindent
{\em Remark:} Better results for the R2M problem can be obtained by adapting Varadarajan's quasi-uniform
sampling technique to multi-cover instances. However, we follow the simpler approach here since it suffices for our purposes.

\medskip

{\em The Problem of 
Covering the Light Points to the instances  of R2M:} 
The reduction takes as inputs 
the instance  for R2C (restricted to light points), and the LP solution  
and for each  creates an instance  of R2M.
The points in  are the same as the points in .
The demand of a point  in  is defined as 
.
The rectangles in  are precisely the class  rectangles in , i.e. those of capacity  exactly .
The weight of the rectangles in  are the the same as in .
The goal is to cover each point  by  distinct rectangles. 



\begin{lemma}
Consider the union  of the rectangles picked in the solutions 
to the instances .
Then  satisfies the demand of  all the light points in .
\end{lemma}
\begin{proof} 
Consider a particular point  and suppose it lies in class  in , i.e. its demand . Then the extent to which  is
covered by  is at least
{\allowdisplaybreaks

}
where last inequality follows from (\ref{eq:med1}).
Since , it follows the each  is covered.
\end{proof} 

Henceforth we focus on a particular instance of R2M.
Let  be such an instance with  rectangles (sets)  and  points (elements) . Let  denote the covering requirement of . 
We are given  some fractional feasible solution , i.e. for each    and  for all .
The following lemma is standard.
\begin{lemma}
\label{lowdem}
For any multi-cover problem, 
at the loss of an  factor in approximation ratio, we can assume that the maximum demand  is . 
\end{lemma}
\begin{proof}
We pick each set  with probability . The expected cost of the sets picked is at most twice the LP cost.
By standard Chernoff bounds, for some large enough constant 
each element with demand  is covered 
with probability at least . In the residual instance, each uncovered element has demand  
and as  for each set, the LP solution restricted to the unpicked sets is a feasible solution to the residual instance.
\end{proof}

The following lemma shows how a rounding procedure for a set cover problem can be used for corresponding  multi-cover problem.
 
\begin{lemma}
\label{setmult}
An LP-based  approximation algorithm for a weighted set cover problem can be used to obtain an  approximation for any multi-cover variant of the problem where  is the maximum demand of any element. 
\end{lemma}
\begin{proof}
Let  be some feasible fractional solution to the multi-cover problem. Our algorithm proceeds in  rounds, and picking 
some sets in each round such that after  rounds, each  is covered by at least  distinct sets. 
Inductively, assume
that at beginning of round  each element has an uncovered demand of at most .
This is clearly true for .
For round , we proceed as follows. 
Consider the LP solution , restricted to the sets not chosen thus far in previous rounds.
Let  be the elements with (current) demand exactly .
We claim that  is a feasible fractional set cover solution for .
If  had requirement  initially, then it has been covered  times thus far. As each , the solution  restricted to sets not picked this far still covers  to extent  and hence  must cover  fractionally to extent at least . 

Let  denote the cover for  obtained by applying our set cover rounding procedure to .
We return the solution . In this solution, each element  is covered at least  times, and its cost is . 
\end{proof}


We now give a  approximation for R2M using local ratio. We refer the reader to \cite{BBF} for a general description of the technique. While we use local ratio below, our approximation can be easily made LP-based
using the equivalence between local ratio and the primal dual method \cite{BR}.

\begin{lemma}
\label{r2mlemma}
There is a 2-approximation for the R2M problem when all the demands are .
\end{lemma}
\begin{proof}
The algorithm is a straight-forward
application of local ratio rule. We adopt the notation from all the local ratio rule papers. Let  be
the original weight function. Consider the rightmost point  to be covered, that is the point  with
maximum  coordinate (if there are several, pick one arbitrarily). 
Let  be the minimum weight of a rectangle
covering . Define the weight function  for rectangles that cover , and 0 for the other rectangles. Let  be the residual weight function.
Recall that the local ratio rule tentatively picks all the sets  with  weight 0, removes the covered points and proceeds recursively on the residual instance with function .
Let  be the solution obtained recursively by the local ratio for the residual instance.
We then add all the rectangles in  and perform the greedy-delete step, i.e. remove them arbitrarily as
long as solution is feasible.

As  must be covered, any optimum solution must incur a  cost of .
It suffices to show that at most two rectangles with non-zero  weight can be picked by the algorithm.
Suppose more than two are left after the delete step. But as  is the rightmost point, any rectangle that covers  and is different from the one with the topmost edge or the one with the  bottommost edge will be redundant.
\end{proof}


\section*{ Acknowledgments} We greatly thank Alexander Souza, Cliff Stein, Lap-Kei Lee, Ho-Leung Chan,
and Pan Jiangwei for extensive discussions about this research.



\bibliographystyle{plain}
\bibliography{focs-appear}

\end{document}
