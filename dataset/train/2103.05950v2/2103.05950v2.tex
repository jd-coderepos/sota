

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}



\definecolor{mygray}{gray}{.9}
\definecolor{mypink}{rgb}{.99,.91,.95}
\definecolor{mycyan}{cmyk}{.3,0,0,0}
\newcolumntype{g}{>{\columncolor{mygray}}c}
\newcolumntype{w}{>{\columncolor{mygray}}l}

\parindent=0pt
\parskip=3ex


\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{4163} \def\confYear{CVPR 2021}



\begin{document}

\title{FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding}

\author{Bo Sun\affmark[1], Banghuai Li\affmark[2]\thanks{Corresponding author: libanghuai@megvii.com}, Shengcai Cai\affmark[2], Ye Yuan\affmark[2], and Chi Zhang\affmark[2]\\
\affaddr{\affmark[1]University of Southern California}\\
\affaddr{\affmark[2]MEGVII Technology}\\
\email{bos@usc.edu, \{libanghuai,caishengcai,yuanye,zhangchi\}@megvii.com}\\
}

\maketitle

\begin{abstract}
\vspace{-2.5mm}
\ \ \ \ Emerging interests have been brought to recognize previously unseen objects given very few training examples, known as few-shot object detection (FSOD). Recent researches demonstrate that good feature embedding is the key to reach favorable few-shot learning performance.  We observe object proposals with different Intersection-of-Union (IoU) scores are analogous to the intra-image augmentation used in contrastive approaches. And we exploit this analogy and incorporate supervised contrastive learning to achieve more robust objects representations in FSOD. We present \textbf{F}ew-\textbf{S}hot object detection via \textbf{C}ontrastive proposals \textbf{E}ncoding (\textbf{FSCE}), a simple yet effective approach to learning contrastive-aware object proposal encodings that facilitate the classification of detected objects. We notice the degradation of average precision (AP) for rare objects mainly comes from misclassifying novel instances as confusable classes. And we ease the misclassification issues by promoting instance level intra-class compactness and inter-class variance via our contrastive proposal encoding loss (CPE loss). Our design outperforms current state-of-the-art works in any shot and all data splits, with up to  on standard benchmark PASCAL VOC and  on challenging COCO benchmark. Code is available at: \url{https://github.com/MegviiDetection/FSCE}.
\end{abstract}




\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\linewidth]{figures/camera/Figure1}
\caption{Conceptualization of our contrastive object proposals encoding. We introduce a score function which measures the semantic similarity between region proposals. Positive proposals () refer to region proposals from the same category or the same object. Negative proposals () refer to proposals from different categories. We encourage the object encodings to have the property that , such that our contrastively learned object proposals have smaller intra-class variance and larger inter-class difference}
\vspace{-3mm}
\label{fig:figure1} \end{center}
\end{figure}





\begin{figure*}[t]
\includegraphics[width=1\textwidth]{figures/camera/ai_pw_cos_sim.png}
\centering
\caption{We find in fine-tuning based few-shot object detector, classification is more error-prone than localization. In the fine-tuning stage, RPN is able to make good enough foreground proposals for novel instances, hence novel objects are often accurately localized but mis-classified as confusable base classes. Here shows 20 top-scoring RPN proposals and example detection results from PASCAL VOC Split 1, wherein \textit{bird}, \textit{sofa} and \textit{cow} are novel categories. The left panel shows the pair-wise cosine similarity between the class prototypes learned in the bounding box classifier. For example, the similarity between \textit{bus} and \textit{bird} is -0.10, but the similarity between \textit{cow} and \textit{horse} is 0.39. Our goal is to decrease the instance-level similarity between \textbf{similar} objects that are from \textbf{different} categories.}\label{fig:pw-sim}\end{figure*}



\vspace{-4mm}
\section{Introduction}
Development of modern convolutional neural networks (CNNs) \cite{he_deep_2015,cai_cascade_2017,zhu_deformable_2018} give rise to great advances in general object detection \cite{ren_faster_2016,lin_focal_2018,tian_fcos_2019}. Deep detectors demand a large amount of annotated training data to saturate its performance~\cite{tan_efficientdet_2020,wang_cspnet_2019}. In few-shot learning scenarios, deep detectors suffer severer over-fitting and the gap between few-shot detection and general object detection is larger than the corresponding gap in few-shot image classification~\cite{wang_low-shot_2018,khodadadeh_unsupervised_2019,ren_meta-learning_2018}. On the contrary, a child can rapidly comprehend new visual concepts and recognize objects from a newly learned category given very few examples. Closing such gap is therefore an important step towards more successful machine perception \cite{funke_five_2020}. 

Precedented by few-shot image classification, earlier attempts in few-shot object detection utilize meta-learning strategy \cite{yan_meta_2020,wang_meta-learning_2019,xiao_few_shot_2020}. Meta-learners are trained with an episode of individual tasks, meta-task samples from common objects (base class) to pair with rare objects (novel class) to simulate few-shot detection tasks. Recently, the two-stage fine-tune based approach (TFA) reveals more potential in improving few-shot detection. Baseline TFA \cite{wang_frustratingly_2020} simply freeze all base class trained parameters and fine-tune only box classifier and box regressor with novel data, yet outperforms previous meta-learners. MPSR \cite{wu_multi-scale_2020} improves upon TFA by alleviating the scale bias inherent to few-shot dataset, but their positive refinement branch demands manual selection, which is somewhat less neat. In this work, we observe and address the essential weakness of the fine-tuning based approach~--~constantly mislabeling novel instances as confusable categories, and improve the few-shot detection performance to the new state-of-the-art (SOTA).

Object detection involves localization and classification of appeared objects. In few-shot detection, one might naturally conjecture the localization of novel objects is going to under-perform its base categories counterpart, with the concern that rare objects would be deemed as background~\cite{wang_meta-learning_2019,yan_meta_2020,fan2020few}. However, based on our experiments with Faster R-CNN~\cite{ren_faster_2016}, the commonly adopted detector in few-shot detection, class-agonistic region proposal network (RPN) is able to make foreground proposals for novel instances, and the final box regressor can localize novel instances quite accurately. In comparison, as demonstrated in Figure~\ref{fig:pw-sim}, misclassifying detected novel instances as confusable base classes is indeed the main source of error. We visualize the pairwise cosine similarity between class prototypes~\cite{snell_prototypical_2017,cos_face,deng_arcface_2019} of a Faster R-CNN box classifier trained with PASCAL VOC~\cite{voc07,voc12}. The cosine similarity between prototypes from resembled categories can be , whereas the similarity between objects and background is on average . In few-shot setting, the similarity between cluster centers can go as high as 0.59, \eg, between \textit{sheep} and \textit{cow}, \textit{bicycle} and \textit{motorbike}, making classification for similar objects error-prone.  We make a calculation upon baseline TFA, manually correcting misclassified yet accurately localized box predictions can increase novel class average precision (nAP) by over 20 points.

A common approach to learn well-separated decision boundary is to use a large margin classifier~\cite{large_margin}, but with our trials, category-level positive-margin based classifiers does not work in this data-hunger setting~~\cite{cos_face,deng2009imagenet}. To learn instance-level discriminative feature representations, contrastive learning~\cite{hadsell2006dimensionality,1467314} has demonstrated its effectiveness in tasks including recognition~\cite{schroff2015facenet}, identification~\cite{sun2014deep} and the recent successful self-supervised models~\cite{wu_unsupervised_2018,xie2020delving,he_momentum_2020,chen_simple_2020}. In supervised contrastive learning for image classification~\cite{supervised_contrastive_learning}, intra-image augmentations of images from the same class are used to enrich the positive example pairs. We think region proposals with different Intersection-over-Union (IoU) for an object are naturally analogous to the intra-image augmentation \textit{cropping}, as illustrated in Figure ~\ref{fig:figure1}. Therefore in this work, we explore to extend the supervised batch contrastive approach~\cite{supervised_contrastive_learning} to few-shot object detection. We believe the contrastively learned object representations aware of the intra-class compactness and the inter-class difference can ease the misclassification of unseen objects as similar categories.

We present \textbf{F}ew-\textbf{S}hot object detection via \textbf{C}ontrastive proposals \textbf{E}ncoding (\textbf{FSCE}), a simple yet effective fine-tune based approach for few-shot object detection. When transfer the base detector to few-shot novel data, we augment the primary Region-of-Interest (RoI) head with a contrastive branch, the contrastive branch measures the similarity between object proposal encodings. A supervised contrastive objective with specific considerations for detection will be optimized to reduce the variance of object proposal embeddings from the same category, while pushing different-category instances away from each other. The proposed contrastive objective, contrastive proposal encoding (CPE) loss, is employed to the original classification and localization objective in a multi-task fashion. The end-to-end training of our proposed method is identical to vanilla Faster R-CNN. 

To our best knowledge, we are the first to bring contrastive learning into few-shot object detection. Our simple design sets the new state-of-the-art in any shot (1, 2, 3, 5, 10, and 30), with up to  on the standard PASCAL VOC benchmark and  on the challenging COCO benchmark.


\section{Related Work}

\textbf{Few-shot learning.} Few-shot learning aims to recognize new concepts given limited labeled examples. Meta-learning approaches aim at training a meta-model on episodes of individual tasks such that it can adapt to new tasks with few samples \cite{finn_model-agnostic_2017,ren_meta-learning_2018,sun_meta-transfer_nodate,khodadadeh_unsupervised_2019,ravi_optimization_2017,nichol_reptile_nodate,rusu_meta-learning_2018}, known as ``learning-to-learn''. Deep metric-learning based approaches emphasize learning good feature representation embeddings that facilitate downstream tasks. The most intuitive metrics including cosine similarity~\cite{cos_face,cos_softmax,chen_closer_2020,deng_arcface_2019}, euclidean distance to class center~\cite{snell_prototypical_2017}, and graph distances~\cite{garcia_few-shot_2018}. Interestingly, hallucinator-based methods solve the data deficiency via learning to generate fake-data~\cite{wang_low-shot_2018}. Existing few-shot learners are mostly developed in the context of classification. In comparison, few-shot detection is more challenging as it involves both classification and localization, yet under-researched.

\textbf{Few-shot object detection.} There are two lines of work addressing the challenging few-shot object detection (FSOD) problem. First, meta-learning based approaches devise a stage-wise and periodic meta-training paradigm to train a meta-learner to help knowledge transfer from base classes. Meta R-CNN \cite{yan_meta_2020} meta-learns channel-wise attention layer for remodeling the RoI head. MetaDet \cite{wang_meta-learning_2019} applies a weight prediction meta-model to dynamically transfer category-specific parameters from the base detector. FSIW \cite{xiao_few_shot_2020} improves upon Meta R-CNN and FSRW \cite{kang_few-shot_2019} by more complex feature aggregation and meta-training on a balanced dataset. With the balanced dataset introduced in TFA \cite{wang_frustratingly_2020}, fine-tune based detectors are rowing over meta-learning based methods in performance, MPSR~\cite{wu_multi-scale_2020} sets the current state-of-the-art by mitigating the scale scarcity in few-shot datasets, but its generalizability is limited because the positive refinement branch contains manual decisions. RepMet~\cite{karlinsky_repmet_2018} attaches an embedding sub-net in RoI head to model a posterior class distribution. It utilizes advanced tricks including OHEM~\cite{shrivastava_training_2016} and SoftNMS~\cite{bodla_soft-nms_2017} but fails to catch up with current SOTA. We criticize complex algorithms as they can easily overfit and exhibit poor test results in FSOD. Instead, our insight here is that the degeneration of average precision (AP) for novel categories mainly comes from misclassifying novel instances as confusable categories, and we resort to contrastive learning to learn discriminative object proposal representations without complexing the model.


\textbf{Contrastive learning}
The recent success of self-supervised models can be attributed to the renewed interest in exploring contrastive learning. \cite{hjelm_learning_2018,wu_unsupervised_2018,oord_representation_2019,chuang_debiased_2020,he_momentum_2020,chen_improved_2020,chen_simple_2020,chen_big_2020}. Optimizing the contrastive objectives \cite{oord_representation_2019,cos_face,deng_arcface_2019,supervised_contrastive_learning} simultaneously maximize the agreement between similar instances defined as positive pairs and encourage the difference among dissimilar instances or negative pairs. With contrastive learning, the algorithm learns to build representations that do not concentrate on pixel-level details, but encoding high-level features effective enough to distinguish different images \cite{chen_simple_2020,he_momentum_2020,chen_improved_2020,chen_big_2020}. Supervised contrastive learning~\cite{supervised_contrastive_learning} extends the batch contrastive approach to supervised setting, but for image classification. 

To our best knowledge, this work is the first to integrate supervised contrastive learning~\cite{sun2014deep,supervised_contrastive_learning} into few-shot object detection. The state-of-the-art few-shot detection performance in any shot and all benchmarks demonstrate the effectiveness of our proposed method.




\begin{figure*}[t]
\includegraphics[width=1\textwidth]{figures/camera/ai_overview_resized.png}
\centering
\caption{Overview of our proposed FSCE. In our method, we jointly fine-tune the FPN pathway and RPN while fixing the backbone. We find this is effective in coordinating backbone feature maps to activate on novel objects yet still avoid the risk of overfitting. To learn contrastive object proposal encodings, we introduce a contrastive branch to guide the RoI features to learn contrastive-aware proposal embeddings. We design a contrastive objective to maximize the within-category agreement and cross-category disagreement.} 
\label{fig:overview}\end{figure*}




\section{Method}
Our proposed method FSCE involves a simple two-stage training. First, the standard Faster R-CNN detection model is trained with abundant base-class data (). Then, the base detector is transferred to novel data through fine-tuning on a balanced dataset~\cite{wang_cspnet_2019} with novel instances and randomly sampled base instances (). The backbone feature extractor is frozen during fine-tuning while the RoI feature extractor is supervised by a contrastive objective. We jointly optimize the contrastive proposal encoding (CPE) loss we proposed with the original classification and regression objectives in a multi-task fashion. Overview of our method is shown in Figure \ref{fig:overview}.

\subsection{Preliminary}

\textbf{Rethink the two-stage fine-tuning approach}. Original TFA~\cite{wang_frustratingly_2020} only fine-tunes the last two  layers--box classifier and box regressor--with novel data, the rest structures are frozen and taken as a fixed feature extractor. This could be viewed as an approach to counter the over-fitting of limited novel data. However it is counter-intuitive that Feature Pyramid Network (FPN~\cite{lin_feature_2017}), RPN, especially the RoI feature extractor which contain semantic information learned from base classes only, could be transferred directly to novel classes without any form of training.  In baseline TFA, unfreezing RPN and RoI feature extractor leads to degraded results for novel classes. However, we find this behavior is reversible and can  benefit novel detection results if trained properly. We propose a stronger baseline which adapts much better to novel data with jointly fine-tuned feature extractors and box predictors 

\textbf{Strong baseline.} We establish our strong baseline from the following observations. Initially, the detection performance for novel classes decreases as more network components are fine-tuned with novel shots. However, we notice a significant gap in the key RPN and RoI statistics between the data-abundant base training stage and the novel fine-tuning stage. As shown in Figure \ref{fig:rpn-roi}, the number proposals from positive anchors in novel fine-tuning is only  of its base training counterpart and the number of foreground proposals decreases consequently. We observe, especially at the beginning of fine-tuning, the positive anchors for novel objects receive comparatively low scores from RPN. Due to the low objectness scores, less positive anchors can pass non-max suppression (NMS) and become proposals that provide actual learning opportunities in RoI head for novel objects. Our insight is to rescue the low objectness positive anchors that are suppressed. Besides, re-balancing the foreground proposals fraction is also critical to prevent the diffusive yet easy backgrounds from dominating the gradient descent for novel instances in fine-tuning. 


\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{figures/camera/ai_rpn_2k_roi_256.png}
\end{center}
\vspace{-2.5mm}
\caption{Key detection statistics. Left shows the average number of positive anchors per image in RPN in base training and novel fine-tuning stage. Right shows the average number of foreground proposals per image during fine-tuning. In the left, orange line shows the original TFA setting, which use the same specs as base training. In the right, the blue line shows double the number of anchors kept after NMS in RPN, the gray line shows reducing RoI head batch size by half.} 
\label{fig:rpn-roi}
\end{figure}


\begin{table}[t]\footnotesize \vspace{1.5mm}
\begin{center}
\begin{tabular}{c|c|cc|cc}
\toprule
\multirow{2}{*}{Method} & Fine-tune & \multicolumn{2}{c|}{Refinement} & \multicolumn{2}{c}{Novel AP50}\\
 & FPN & RPN & ROI & ~~~~3 & 10 \\
\midrule
TFA w/ cos~\cite{wang_frustratingly_2020} & - & - & - & 44.7 & 56.0 \\
\midrule
\multirow{4}{*}{\shortstack{Strong baseline\
	logit_{\{i,j\}} = \alpha\frac{x_i^\top w_j}{||x_i||\cdot||w_j||}

\mathcal{L}_{CPE} = \frac1N\sum_{i=1}^Nf(u_i)\cdot\mathit{L}_{z_i}  \label{loss3}
\small
{\mathit{L}_{z_i}\! =\! \frac{-1}{N_{y_i}\!-1}\sum_{j=1,j\ne i}^{N}\mathbb{I}{\{y_i\!=\!y_j\}}\!\cdot\!\log\frac{\exp(\tilde{z_i}\!\cdot\! \tilde{z_j}/\tau)}{\sum_{k=1}^N\mathbb{I}_{k\ne i}\cdot\exp(\tilde{z_i}\!\cdot\! \tilde{z_k}/\tau)}}
    \label{loss4}

	f(u_i) = \mathbb{I}\{u_i\geqslant \phi\}\cdot g(u_i) \label{loss6}

    \mathbb{L} = \mathcal{L}_{rpn} + \mathcal{L}_{cls} + \mathcal{L}_{reg} + \lambda\mathcal{L}_{CPE}
Ours)}} & \cmark & \xmark & \xmark & \xmark & 47.2 & 56.9 & 59.8\\
& \cmark & \cmark & \xmark  & \xmark & 49.7 & 58.6 & 61.4 \\
& \cmark & \cmark & \cmark & \xmark & 50.6 & 60.7 & 62.7 \\
& \cmark & \cmark & \cmark & \cmark & \bf{51.4} & \bf{61.9} & \bf{63.4} \\
\bottomrule
\end{tabular}}
\end{center}
\caption{Ablation for key components proposed in FSCE.}
\end{table}





\textbf{Ablation for contrastive branch hyper-parameters.} Primary RoI feature vector contains post-ReLU activations truncated at zero, we therefore encode the RoI feature with a contrastive head to  such that similarity can be meaningfully measured. Based on our ablations, the few-shot detection performance is not sensitive to the contrastive head dimension. And among the commonly used temperature  used in contrastive objectives~\cite{supervised_contrastive_learning,he_momentum_2020,chen_improved_2020}, a medium temperature  works better than relatively small value 0.07 and large value 0.5.
\begin{table}[h]\small
\begin{center}
\begin{tabular}{c|ccc}
\toprule
\multirow{2}{*}{\shortstack{Contrast Head\\Dimension}} & \multicolumn{3}{c}{Temperature~()} \\
& 0.07 & 0.2 & 0.5 \\
\midrule
& 63.1 & \bf{63.4} & 62.9 \\
\midrule
& 62.4 & \bf{63.4} & 63.3\\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1.5mm}
\caption{Ablation for contrastive hyper-parameters, results from 10 shot of PASCAL VOC Split 1.}
\end{table}





\textbf{Ablation for Proposal Consistency Control.}
In equation (\ref{loss4}) and (\ref{loss6}), we propose a compound proposal consistency control mechanism, comprised of an indicator function with an IoU cut-off threshold , and a function  for re-weighting proposals with different level of IoU. Turns out a re-weighting is not necessary and a simple high-IoU cut-off works the best for 5 and 10 shots, but when number of shots is low, simply filtering out proposals with IoU less than  becomes less favorable as the data sparsity is too severe. In low-shot cases, keeping all proposals but down-weight low-IoU ones make more sense, and empirically, exponential decay (easy mining) does worse than a simple linear weighting.

\begin{table}[h]\normalsize
\label{ab-loss}
\begin{center}
\resizebox{\linewidth}{!}{\begin{tabular}{c|c|c|ccc}
\toprule
\multirow{2}{*}{Option} & \multirow{2}{*}{Threshold} & \multirow{2}{*}{\shortstack{Reweight\\function}} & \multicolumn{3}{c}{Novel AP} \\
& & & 3 & 5 & 10 \\
\midrule
\multirow{2}{*}{Hard Clip} &  &  & 50.5 & 60.7 & 62.1\\
&  &  & 50.8 & \bf{61.9} & \bf{63.4}\\
\midrule
\multirow{2}{*}{Weighting} &  & &\bf{51.4}& 59.7 & 61.1 \\
 &  &  & 50.8 & 59.6 & 61.6\\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-1.5mm}
\caption{Ablation for proposal consistency control in FSCE.}
\end{table}



\textbf{Visual inspections and analysis.} Figure \ref{fig:tsne} shows visual inspections of our proposed FSCE. We find in data-abundant general detection, the saturated performance of  classifier and  classifier are essentially equal.  layer can learn sophisticated decision boundary from enough data. Existing literature and we all confirm that  box classifier excels in few shot object detection, this can be attributed to the explicitly modeled similarity helps form tighter instances clusters on the projected unit hypersphere. The intuition to spacing different categories is trivial, but per our experiments well-established margin-based classifiers~\cite{cos_face,deng_arcface_2019} does not work in this data-hunger setting (-2 nAP compared to FSCE in 10 shots and worse in lower shots). Instead of adding a margin to classifier, FSCE models the instance-level intra-class similarity and inter-class via  loss and guide RoI head to learn contrastive-aware object proposal representations. t-SNE~\cite{t_sne} visualization of objects proposal embeddings affirms the effectiveness of our  loss in reducing intra-class variance and form more defined decision boundaries, this aligns well with our proposition. Figure \ref{fig:tsne} (c) shows example bad cases from TFA that are rescued by our FSCE including, missing detection for novel instances, low confidence scores for novel instances, and the pervasive misclassifications.

\vspace{-2mm}
\section{Conclusion}
\vspace{-2mm}
In this work, we propose a new perspective of solving FSOD via contrastive proposals encoding. Effectively saving accurately localized objects from being misclassified, our method achieves state-of-the-art results in any shot and both benchmarks, with up to +\% on PASCAL VOC and +\% on COCO. Our proposed contrastive proposal encoding head has a negligible cost and is generally applicable. It can be chipped into any two-stage detectors without interfering with the training pipeline. Also, we provide a strong baseline comparable to contemporary SOTA to facilitate future research in FSOD. 
For a broader impact, FSOD is of great worth considering the vast amount of objects in the real world. Our work proves the plausibility of incorporating contrastive learning into object detection frameworks. We hope our work can inspire more researches in contrastive visual embedding and few-shot object detection.

\textbf{Acknowledgement.} This work was supported by grants from the National Key R \& D Program of China. Grant number: 52019YFB1600500. 



{\small
\bibliographystyle{unsrt}
\bibliographystyle{ieee_fullname}
\bibliography{cvpr_final.bib}
}


\end{document}
