\label{sec:experiments}
\subsection{Datasets}
\noindent\textbf{Human3.6M (H36M)\cite{h36m_pami}} Following \cite{Ci_2019_ICCV}, we use the subjects S, S, S, S, and S for training, and S, S for testing. The Mean Per Joint Position Error (MPJPE) metric is computed under two protocols: Protocol \#1 computes MPJPE between the ground-truth (GT) and the estimated D poses after aligning their root (mid-hip) joints; Protocol \#2 reports MPJPE after the D estimate is aligned with the GT via a rigid transformation. Additionally, we present two new metrics to comprehensively measure the quality of the D pose estimates: (1) Mean Per Limb Length Error (MPLLE) computes the average limb length error between the GT and estimated poses over  limbs (\ie the purple edges in Figure \ref{fig:general_formula}), and (2) Mean Per Limb Angle Error (MPLAE) measures the average limb angle error between the GT and the estimated poses.\\


\noindent\textbf{MPI-INF-3DHP (3DHP) \cite{mehta2017monocular}} This dataset provides monocular videos of six subjects acting in three different scenes which include green screen indoor scenes, indoor scenes and outdoor scenes. This dataset is often used to evaluate the generalization performance of different models. Following the convention, we directly apply our model trained on the H36M dataset to this dataset without re-training. We report results using two metrics: Percentage of Correctly estimated Keypoints (PCK) \cite{andriluka14cvpr} and Area Under the Curve (AUC) \cite{mehta2017monocular}. 

\subsection{Implementation Details}
\label{sec:implementation}
We use the state-of-the-art D pose estimator \cite{Iskakov_2019_ICCV} as our baseline to estimate D poses. We insert ContextPose between the encoder and decoder networks as shown in Figure \ref{fig:overview}. To reduce GPU memory cost, we decrease the number of layers in the D network from five to two. The modification slightly improves the results of the baseline. For the ContextPose network,  is set to be . We jointly train the D and D networks for  epochs with the Adam \cite{kingma2015adam} optimizer. 
The learning rates are set to be   and  for the D and D networks, respectively. To prevent from over-fitting to the human appearance in the H36M dataset, we fix the D network and train the D network for  epochs before end-to-end training.




\begin{table*}[]
\center
\small
\setlength{\tabcolsep}{2pt}
\resizebox{6.7in}{!}{
\begin{tabular}{l c c c c c c c c c c c c c c c c}
\thickhline 
\textbf{Protocol \#1} & Dire. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
\hline 

Zhou \etal \cite{zhou2017towards} ICCV'17 & 54.8 & 60.7 & 58.2 & 71.4 & 62.0 & 65.5 & 53.8 & 55.6 & 75.2 & 111.6 & 64.2 & 66.1 & 51.4 & 63.2 & 55.3 & 64.9 \\
Martinez \etal (\textbf{FCN}) \cite{martinez2017simple} ICCV'17 & 51.8 & 56.2 & 58.1 & 59.0 & 69.5 & 78.4 & 55.2 & 58.1 & 74.0 & 94.6 & 62.3 & 59.1 & 65.1 & 49.5 & 52.4 & 62.9 \\
Pavlakos \etal \cite{pavlakos2018ordinal} CVPR'18 & 48.5 & 54.4 & 54.4 & 52.0 & 59.4 & 65.3 & 49.9 & 52.9 & 65.8 & 71.1 & 56.6 & 52.9 & 60.9 & 44.7 & 47.8 & 56.2 \\
Yang \etal \cite{Yang_2018_CVPR} CVPR'18 & 51.5 & 58.9 & 50.4 & 57.0 & 62.1 & 65.4 & 49.8 & 52.7 & 69.2 & 85.2 & 57.4 & 58.4 & 43.6 & 60.1 & 47.7 & 58.6 \\
Zhao \etal (\textbf{GNN}) \cite{zhao2019semantic} CVPR'19 & 47.3 & 60.7 & 51.4 & 60.5 & 61.1 & 49.9 & 47.3 & 68.1 & 86.2 & \textbf{55.0} & 67.8 & 61.0 & 42.1 & 60.6 & 45.3 & 57.6 \\
Qiu \etal (\textbf{PSM}) \cite{qiu2019cross} ICCV'19 & 223.1 & 231.8 & 273.0 & 237.3 & 248.1 & 243.9  & 209.0 & 279.7 & 280.9 & 296.3 & 241.9 & 234.0 & 230.8 & 217.8 & 220.4 & 244.8\\ 
Iskakov \etal \cite{Iskakov_2019_ICCV} ICCV'19 & 41.9 & 49.2 & 46.9 & 47.6 & 50.7 & 57.9 & 41.2 & 50.9 & 57.3 & 74.9 & 48.6 & 44.3 & \textbf{41.3} & 52.8 & 42.7 & 49.9 \\
Wang \etal \cite{wang2019not} ICCV'19 & 44.7 & 48.9 & 47.0 & 49.0 & 56.4 & 67.7 & 48.7 & 47.0 & 63.0 & 78.1 & 51.1 & 50.1 & 54.5 & 40.1 & 43.0 & 52.6 \\
Ci \etal (\textbf{LCN}) \cite{Ci_2019_ICCV} ICCV'19 & 46.8 & 52.3 & 44.7 & 50.4 & 52.9 & 68.9 & 49.6 & 46.4 & 60.2 & 78.9 & 51.2 & 50.0 & 54.8 & 40.4 & 43.3 & 52.7 \\
Pavllo* \etal \cite{pavllo:videopose3d:2019} CVPR'19 & 47.1 & 50.6 & 49.0 & 51.8 & 53.6 & 61.4 & 49.4 & 47.4 & 59.3 & 67.4 & 52.4 & 49.5 & 55.3 & 39.5 & 42.7 & 51.8* \\
Cai* \etal \cite{cai2019exploiting} ICCV'19 & 46.5 & 48.8 & 47.6 & 50.9 & 52.9 & 61.3 & 48.3 & 45.8 & 59.2 & 64.4 & 51.2 & 48.4 & 53.5 & 39.2 & 41.2 & 50.6* \\
Xu* \etal \cite{Xu_2020_CVPR} CVPR'20 & 40.6 & 47.1 & 45.7 & 46.6 & 50.7 & 63.1 & 45.0 & 47.7 & 56.3 & 63.9 & 49.4 & 46.5 & 51.9 & 38.1 & 42.3 & \underline{49.2}* \\
\rowcolor{mygray}


Ours & \textbf{36.3} & \textbf{42.8} & \textbf{39.5} & \textbf{40.0} & \textbf{43.9} & \textbf{48.8} & \textbf{36.7} & \textbf{44.0} & \textbf{51.0} & 63.1 & \textbf{44.3} & \textbf{40.6} & 44.4 & \textbf{34.9} & \textbf{36.7} & \textbf{43.4} \\




\thickhline
\textbf{Protocol \#2} & Dire. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
\hline 

Martinez \etal (\textbf{FCN}) \cite{martinez2017simple} ICCV'17 & 39.5 & 43.2 & 46.4 & 47.0 & 51.0 & 56.0 & 41.4 & 40.6 & 56.5 & 69.4 & 49.2 & 45.0 & 49.5 & 38.0 & 43.1 & 47.7 \\
Pavlakos \etal \cite{pavlakos2018ordinal} CVPR'18 & 34.7 & 39.8 & 41.8 & 38.6 & 42.5 & 47.5 & 38.0 & 36.6 & 50.7 & 56.8 & 42.6 & 39.6 & 43.9 & 32.1 & 36.5 & 41.8 \\
Yang \etal \cite{Yang_2018_CVPR} CVPR'18 & \textbf{26.9} & \textbf{30.9} & 36.3 & 39.9 & 43.9 & 47.4 & 28.8 & \textbf{29.4} & \textbf{36.9} & 58.4 & 41.5 & \textbf{30.5} & \textbf{29.5} & 42.5 & 32.2 & \underline{37.7} \\
Qiu \etal (\textbf{PSM}) \cite{qiu2019cross} ICCV'19 & 117.0 & 123.2 & 128.0 & 121.7 & 126.1 & 128.7 & 105.3 & 130.1 & 145.1 & 170.2 & 125.1 & 114.5 & 128.9 & 115.3 & 117.1 & 126.7 \\
Wang \etal \cite{wang2019not} ICCV'19 & 33.6 & 38.1 & 37.6 & 38.5 & 43.4 & 48.8 & 36.0 & 35.7 & 51.1 & 63.1 & 41.0 & 38.6 & 40.9 & 30.3 & 34.1 & 40.7 \\
Ci \etal (\textbf{LCN}) \cite{Ci_2019_ICCV} ICCV'19 & 36.9 & 41.6 & 38.0 & 41.0 & 41.9 & 51.1 & 38.2 & 37.6 & 49.1 & 62.1 & 43.1 & 39.9 & 43.5 & 32.2 & 37.0 & 42.2 \\
Pavllo* \etal \cite{pavllo:videopose3d:2019} CVPR'19 & 36.0 & 38.7 & 38.0 & 41.7 & 40.1 & 45.9 & 37.1 & 35.4 & 46.8 & 53.4 & 41.4 & 36.9 & 43.1 & 30.3 & 34.8 & 40.0* \\
Cai* \etal \cite{cai2019exploiting} ICCV'19 & 36.8 & 38.7 & 38.2 & 41.7 & 40.7 & 46.8 & 37.9 & 35.6 & 47.6 & \textbf{51.7} & 41.3 & 36.8 & 42.7 & 31.0 & 34.7 & 40.2* \\
Xu* \etal \cite{Xu_2020_CVPR} CVPR'20 & 33.6 & 37.4 & 37.0 & 37.6 & 39.2 & 46.4 & 34.3 & 35.4 & 45.1 & 52.1 & 40.1 & 35.5 & 42.1 & 29.8 & 35.3 & 38.9* \\
\rowcolor{mygray}
Ours & 30.5 & 34.9 & \textbf{32.0} & \textbf{32.2} & \textbf{35.0} & \textbf{37.8} & \textbf{28.6} & 32.6 & 40.8 & 52.0 & \textbf{35.0} & 31.9 & 35.6 & \textbf{26.6} & \textbf{28.5} & \textbf{34.6} \\


\thickhline 
\end{tabular}}
\caption{The MPJPE (mm) of the state-of-the-art methods on the H36M dataset under protocol \#1 and protocol \#2, respectively. \emph{*} means the method uses temporal information in videos.}
\label{tab:state_of_the_art_h36m}
\end{table*}

\subsection{Comparison to the State-of-the-arts}
\noindent\textbf{Results on the H36M Dataset} Table \ref{tab:state_of_the_art_h36m} shows the results of the state-of-the-art methods on the H36M dataset. Our approach outperforms the state-of-the-art methods by a notable margin under both protocols. This includes methods that explore temporal information in videos (labeled by * in the table). In particular, our method outperforms PSM \cite{qiu2019cross}, FCN \cite{martinez2017simple}, GNN \cite{zhao2019semantic}, and LCN \cite{Ci_2019_ICCV} by an even larger margin which validates the effectiveness of our context modeling strategy. We discover in our experiment that PSM \cite{qiu2019cross} gets very poor results in the monocular setting. To investigate the reasons, we project the estimated D poses back to D images and find that, for most cases, the projections perfectly match the D people although their D estimates are very different from the GT poses. We show an example in Figure \ref{fig:PSM_result}. This is mainly because PSM alone has limited capability to resolve  ambiguity. Note that a D pose estimate may be inaccurate even when its limb lengths are correct. In contrast, the deep learning-based methods such as GNN \cite{martinez2017simple,zhao2019semantic,Ci_2019_ICCV} have strong capability to reduce ambiguity because they can fit a large amount of data. We will discuss in more details on why our approach gets more accurate estimates than PSM and GNN in the subsequent ablative study.\\



\noindent\textbf{Results on the 3DHP Dataset}
Table \ref{tab:state_of_the_art_3dhp} shows the results of different methods on the 3DHP dataset. Our approach achieves significantly better PCK and AUC scores than other methods including FCN, LCN, and PSM for almost all scenes. The result suggests that ContextPose has strong generalization performance which we think is due to the leverage of limb length priors in deep networks. FCN \cite{martinez2017simple} gets a low accuracy because the dense connections degrade the generalization capability which has already been discussed in \cite{Ci_2019_ICCV}. LCN \cite{Ci_2019_ICCV} gets better results by fusing features of contextual joints but it is still worse than ours. The result validates the importance of combining deep networks and limb length priors.

\begin{figure}
	\centering
	\includegraphics[width=3.1in]{imgs/experiments/PSM_align.pdf}
	\caption{Visualization of a D pose estimated by PSM \cite{qiu2019cross}. The left figure shows the projection of the estimated D pose. The right figure shows the estimated (solid lines) and GT (dashed lines) D poses. The estimated D pose has correct D projection but it is very different from GT D pose. It means PSM suffers from severe ambiguity when it is used in the monocular setting.}
	\label{fig:PSM_result}
\end{figure}


\begin{table}
    \centering
    \resizebox{3.3in}{!}{
    \begin{tabular}{l|SMLMM} 
    \thickhline 
    Method & GS (PCK) & noGS (PCK) & Outdoor (PCK) & ALL (PCK)  & ALL (AUC)  \\
    \hline
    \multicolumn{6}{c}{Trained on: H36M+MPII \cite{andriluka14cvpr}} \\
    \hline
    \rowcolor{mygray}
    Zhou \etal \cite{zhou2017towards} & 71.1 & 64.7 & 72.7 & 69.2 & 32.5 \\
    \rowcolor{mylightergray}
    Yang \etal \cite{Yang_2018_CVPR} & - & - & - & 69.0 & 32.0 \\
    \rowcolor{mygray}
    Wang \etal \cite{wang2019not} & - & - & - & 71.9 & 35.8 \\
    \hline
    \multicolumn{6}{c}{Trained on: H36M+MPII+LSP \cite{Johnson10}} \\
    \hline
    \rowcolor{mygray}
    Pavlakos \etal \cite{pavlakos2018ordinal} & 76.5 & 63.1 & 77.5 & 71.9 & 35.3 \\
    \hline
    \multicolumn{6}{c}{Trained on: H36M} \\
    \hline
\rowcolor{mygray}
    Martinez \etal (\textbf{FCN}) \cite{martinez2017simple} & 49.8 & 42.5 & 31.2 & 42.5 & 17.0 \\
    \rowcolor{mylightergray}
    Qiu \etal (\textbf{PSM}) \cite{qiu2019cross} & 26.4 & 22.6 & 19.6 & 23.3 & 8.0 \\
    \rowcolor{mygray}
    Ci \etal (\textbf{LCN}) \cite{Ci_2019_ICCV} & 74.8 & 70.8 & \textbf{77.3} & \underline{74.0} & \underline{36.7} \\
    \rowcolor{mylightergray}
    Baseline & 75.2	& 73.3 & 62.2 & 71.3 & 35.0 \\
    \hline
\rowcolor{mygray}
    Ours & \textbf{82.6} & \textbf{80.5} & \textbf{77.3} & \textbf{80.5} & \textbf{42.7} \\
    \thickhline
    \end{tabular}}
    \caption{The results of the state-of-the-art methods on the 3DHP dataset. GS represents the green screen background scene. The results of \cite{martinez2017simple} are taken from \cite{luo2018orinet}.}
    \label{tab:state_of_the_art_3dhp}
\end{table}


\subsection{Ablation Study}

\begin{table}
    \centering
    \resizebox{3.3in}{!}{
    \begin{tabular}{l|c|c|c|c|c|c|c|c} 
    \thickhline 
    \multirow{2}{*}{Method} & \multirow{2}{*}{GA} & \multirow{2}{*}{PA} & \multicolumn{3}{c|}{S9} &  \multicolumn{3}{c}{S11} \\
    \cline{4-9}
    & & & MPJPE  & MPLLE  & MPLAE  & MPJPE  & MPLLE  & MPLAE  \\
    \hline
    Baseline & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & 54.38 & 15.03 & 0.1600 & 35.12 & 10.16 & 0.1250  \\	
    Ours w/o PA & \textcolor{mygreen}{\cmark} & \textcolor{red}{\xmark} & 52.00 & 14.58 & 0.1517 & 35.16 & 9.78 & 0.1240 \\
Ours w/o GA & \textcolor{red}{\xmark} & \textcolor{mygreen}{\cmark} &  52.46 & 14.16 & 0.1524 & 34.98 & 9.67 & 0.1224 \\
\cellcolor{mygray}{Ours} & \cellcolor{mygray}{\textcolor{mygreen}{\cmark}} & \cellcolor{mygray}{\textcolor{mygreen}{\cmark}} & \cellcolor{mygray}{\textbf{50.24}} & \cellcolor{mygray}{\textbf{14.13}} & \cellcolor{mygray}{\textbf{0.1509}} & \cellcolor{mygray}{\textbf{34.10}} & \cellcolor{mygray}{\textbf{9.50}} & \cellcolor{mygray}{\textbf{0.1217}} \\
\thickhline
    \end{tabular}}
    \caption{Ablative study on the global attention and pairwise attention in ContextPose. We show the MPJPE (mm), MPLLE (mm) and MPLAE (radian) on each test subject separately. ContextPose achieves large improvement on the more challenging subject of S.}
    \label{tab:ablation_effect}
\end{table}

\noindent\textbf{Effect of ContextPose}
\label{sec:ablation_effect}
We first compare our approach to the baseline w/o ContextPose. The results on the H36M dataset are shown in Table \ref{tab:ablation_effect}. We can see that ContextPose notably decreases MPJPE of the baseline from mm to mm on the challenging subject S. MPLLE decreases by nearly  meaning that the limb lengths of the estimated D poses are more accurate than the baseline. The improvement for S in terms of MPJPE is marginal because the baseline is already very accurate. However, we can see that there is still clear improvement in terms of limb lengths and angles. The result of the baseline is different from the number in Table \ref{tab:state_of_the_art_h36m} because we use a smaller D network in Table \ref{tab:ablation_effect} to reduce memory usage as stated in Section \ref{sec:implementation}. 

We plot the MPLLE of the baseline and our method for each sample in H36M dataset in Figure \ref{fig:delta_error}. We can see that ContextPose gets smaller errors than baseline for about  of the test data. In particular, the improvement is larger for hard cases where the baseline gets large errors (see the left side of the figure). It indicates that ContextPose reduces the chance of getting absurd poses by exploring context. There are few cases where ContextPose gets worse results. This usually happens when multiple body joints are occluded which makes estimating global attention a very challenging task. 


Table \ref{tab:state_of_the_art_3dhp} shows the results on the 3DHP dataset. We can see that using ContextPose significantly improves the PCK of the baseline from  to . The result represents that ContextPose is very important to improve the generalization performance of the D pose estimator. This is a big advantage for actual deployment. In fact, we can see that our approach even outperforms the methods which use even more training data. \\



\noindent\textbf{Effect of GA and PA}
We report results when we add one of the two modules (GA and PA) to the baseline in Table \ref{tab:ablation_effect}. Adding only the GA module makes little difference on the ultimate results measured by MPJPE, MPLLE, and MPLAE. In contrast, if we add the PA module, the results are improved by a notable margin which validates the importance of pairwise compatibility in context modeling. \\


\begin{figure}
	\centering
	\includegraphics[width=3.1in]{imgs/experiments/delta_limb_error.pdf}
	\caption{MPLLE (mm) of individual samples. The gray line shows the errors of the baseline. The blue line represents the error difference between ContextPose and baseline (below zero means our method gets smaller error).}
	\label{fig:delta_error}
\end{figure}












\begin{figure}
	\centering
	\includegraphics[width=3.3in]{imgs/experiments/quality_results.pdf}
	\caption{Example D pose estimates. The last four columns show the projected D poses and the weights in linear combination for some random joints (highlighted by small blue boxes). Row (d) and (e) show two failure cases.}
	\label{fig:quality_results}
\end{figure}



\subsection{Qualitative Results}
Figure \ref{fig:quality_results} shows some D poses estimated by ContextPose. The last four columns show the predicted weights (\ie the product of the GA and PA) for some random joints. In the first case of (a), the approach pays more attention to the features around the right knee when estimating the right ankle. Similarly, in the third case of (b), it focuses on features from right elbow when estimating right wrist. We show two failure cases in row (d) and (e). In particular, in (e) our estimate has correct limb lengths but inaccurate limb angles for the left leg. In addition, the projection of the D pose is also reasonable. This is a common error for monocular D pose estimation because it has severe ambiguity.
