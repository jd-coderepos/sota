\pdfoutput=1


\documentclass[11pt]{article}
\usepackage[table,xcdraw]{xcolor}
\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{array}
\usepackage{tikz}
\usepackage{bbm}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}



\usepackage{pgfplots}
\pgfplotsset{compat = newest}
\usetikzlibrary{positioning, arrows.meta}
\usepgfplotslibrary{fillbetween}


\usepackage{multirow}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amsxtra}
\usepackage{amsfonts}
\usepackage{multirow}




\clearpage{}







\newcommand{\co}[1]{\textcolor{blue}{#1}} \newcommand{\eat}[1]{} 

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\dgreen}[1]{\textcolor{green!70!black}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\yellow}[1]{\textcolor[rgb]{102, 77, 0}{#1}}

\newcommand{\sz}[1]{\lvert#1\rvert}   \newcommand{\card}[1]{\lvert#1\rvert} \newcommand{\Sz}[1]{\lVert#1\rVert}   \newcommand{\Card}[1]{\lVert#1\rVert} 

\newcommand{\eqdef}{\stackrel{\mathrm{def}}{=}} \newcommand{\eqdot}{\doteq} \newcommand{\eqdist}{\equiv} 

\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor} \newcommand{\floorn}[1]{\lfloor#1\rfloor} \newcommand{\ceil}[1]{\left\lceil#1\right\rceil} \newcommand{\ceiln}[1]{\lceil#1\rceil} 

\newcommand{\abs}[1]{\left\lvert#1\right\rvert} \newcommand{\absn}[1]{\lvert#1\rvert} 

\newcommand{\td}[2]{\if*#1\else^{#1}\fi\if*#2\else_{#2}\fi} 

\newcommand{\xpr}[2]{p\td{#1}{#2}} \newcommand{\uxpr}[2]{\tilde p\td{#1}{#2}} \newcommand{\pr}[1]{\xpr**\!\left(#1\right)} \newcommand{\prn}[1]{\xpr**\!(#1)} \newcommand{\prbig}[1]{\xpr**\!\bigl(#1\bigr)} \newcommand{\prBig}[1]{\xpr**\!\Bigl(#1\Bigr)} \newcommand{\prbigg}[1]{\xpr**\!\biggl(#1\biggr)} \newcommand{\prBigg}[1]{\xpr**\!\Biggl(#1\Biggr)} 

\newcommand{\prf}[3]{\xpr{#1}{#2}\!\left(#3\right)} \newcommand{\uprf}[3]{\uxpr{#1}{#2}\!\left(#3\right)} 

\newcommand{\prs}[1]{\xpr**\!\left(#1\right)} \newcommand{\prsn}[1]{\xpr**\!(#1)} 

\newcommand{\Pa}{\operatorname{Pa}}
\newcommand{\Nd}{\operatorname{NonDescendants}}

\newcommand\macrospace{}

\newcommand{\xev}[2]{\operatorname{E}\td{#1}{#2}} 

\newcommand{\evf}[3]{\xev{#1}{#2}{\left[\macrospace{}#3\macrospace{}\right]}} \newcommand{\evfn}[3]{\xev{#1}{#2}{[\macrospace{}#3\macrospace{}]}} \newcommand{\evfbig}[3]{\xev{#1}{#2}{\bigl[\macrospace{}#3\macrospace{}\bigr]}} \newcommand{\evfBig}[3]{\xev{#1}{#2}{\Bigl[\macrospace{}#3\macrospace{}\Bigr]}} \newcommand{\evfbigg}[3]{\xev{#1}{#2}{\biggl[\macrospace{}#3\macrospace{}\biggr]}} 

\newcommand{\ev}[1]{\evf**{#1}} \newcommand{\evn}[1]{\evfn**{#1}} \newcommand{\evbig}[1]{\evfbig**{#1}} \newcommand{\evBig}[1]{\evfBig**{#1}} \newcommand{\evbigg}[1]{\evfbigg**{#1}} 

\newcommand{\xbias}[2]{\operatorname{bias}\td{#1}{#2}} \newcommand{\bias}[1]{\xbias**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\biasn}[1]{\xbias**[\macrospace{}#1\macrospace{}]} 

\newcommand{\xvar}[2]{\operatorname{var}\td{#1}{#2}} \newcommand{\var}[1]{\xvar**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\varn}[1]{\xvar**[\macrospace{}#1\macrospace{}]} \newcommand{\varbig}[1]{\xvar**\bigl[\macrospace{}#1\macrospace{}\bigr]} \newcommand{\varBig}[1]{\xvar**\Bigl[\macrospace{}#1\macrospace{}\Bigr]} 

\newcommand{\xhatvar}[2]{\operatorname{\hat{v}ar}\td{#1}{#2}} \newcommand{\hatvar}[1]{\xhatvar**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\hatvarn}[1]{\xhatvar**[\macrospace{}#1\macrospace{}]} \newcommand{\hatvarBig}[1]{\xhatvar**\Bigl[\macrospace{}#1\macrospace{}\Bigr]} 

\newcommand{\varf}[3]{\xvar{#1}{#2}{\left[\macrospace{}#3\macrospace{}\right]}} \newcommand{\varfn}[3]{\xvar{#1}{#2}{[\macrospace{}#3\macrospace{}]}} \newcommand{\varfbig}[3]{\xvar{#1}{#2}{\bigl[\macrospace{}#3\macrospace{}\bigr]}} \newcommand{\varfBig}[3]{\xvar{#1}{#2}{\Bigl[\macrospace{}#3\macrospace{}\Bigr]}} \newcommand{\varfbigg}[3]{\xvar{#1}{#2}{\biggl[\macrospace{}#3\macrospace{}\biggr]}} 

\newcommand{\xcov}[2]{\operatorname{Cov}\td{#1}{#2}} \newcommand{\cov}[1]{\xcov**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\covn}[1]{\xcov**[\macrospace{}#1\macrospace{}]} \newcommand{\covBig}[1]{\xcov**\Bigl[\macrospace{}#1\macrospace{}\Bigr]} 

\newcommand{\xhatcov}[2]{\operatorname{\hat{C}ov}\td{#1}{#2}} \newcommand{\hatcov}[1]{\xhatcov**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\hatcovn}[1]{\xhatcov**[\macrospace{}#1\macrospace{}]} \newcommand{\hatcovBig}[1]{\xhatcov**\Bigl[\macrospace{}#1\macrospace{}\Bigr]} 



\newcommand{\xse}[2]{\operatorname{SE}\td{#1}{#2}} \newcommand{\se}[1]{\xse**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\sen}[1]{\xse**[\macrospace{}#1\macrospace{}]} 

\newcommand{\xhatse}[2]{\operatorname{\hat{S}E}\td{#1}{#2}} \newcommand{\hatse}[1]{\xhatse**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\hatsen}[1]{\xhatse**[\macrospace{}#1\macrospace{}]} 



\newcommand{\xsd}[2]{\operatorname{SD}\td{#1}{#2}} \newcommand{\sd}[1]{\xsd**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\sdn}[1]{\xsd**[\macrospace{}#1\macrospace{}]} 

\newcommand{\xhatsd}[2]{\operatorname{\hat{S}D}\td{#1}{#2}} \newcommand{\hatsd}[1]{\xhatsd**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\hatsed}[1]{\xhatsd**[\macrospace{}#1\macrospace{}]} 


\newcommand{\xcv}[2]{\operatorname{CV}\td{#1}{#2}} \newcommand{\cv}[1]{\xcv**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\cvn}[1]{\xcv**[\macrospace{}#1\macrospace{}]} \newcommand{\cvbig}[1]{\xcv**\bigl[\macrospace{}#1\macrospace{}\bigr]} \newcommand{\cvBig}[1]{\xcv**\Bigl[\macrospace{}#1\macrospace{}\Bigr]} 

\newcommand{\xmse}[2]{\operatorname{mse}\td{#1}{#2}} \newcommand{\mse}[1]{\xmse**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\msen}[1]{\xmse**[\macrospace{}#1\macrospace{}]} 

\newcommand{\xrmse}[2]{\operatorname{rmse}\td{#1}{#2}} \newcommand{\rmse}[1]{\xrmse**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\rmsen}[1]{\xrmse**[\macrospace{}#1\macrospace{}]} 

\newcommand{\xare}[2]{\operatorname{ARE}\td{#1}{#2}} \newcommand{\are}[1]{\xare**\left[\macrospace{}#1\macrospace{}\right]} \newcommand{\aren}[1]{\xare**[\macrospace{}#1\macrospace{}]} 



\newcommand{\pbinom}[3]{B\!\left(#1;\,#2,#3\right)} 

\newcommand{\phyper}[4]{H\!\left(#1;\,#2,#3,#4\right)} 



\renewcommand\:{\colon} \newcommand{\sset}[1]{\left\{\,#1\,\right\}} \newcommand{\ssets}[1]{\left\{#1\right\}} \newcommand{\ssetn}[1]{\{\,#1\,\}} 

\newcommand{\comp}[1]{\bar{#1}} 

\newcommand{\powerset}[1]{\cP\!\left(#1\right)} 

\newcommand\adots{{\ensuremath ..}} 

\newcommand{\Nat}{{\ensuremath \bN}}
\newcommand{\Bool}{{\ensuremath \{0,1\}}}

\newcommand\true{\text{true}} \newcommand\false{\text{false}} 

\newcommand\join\Join 

\DeclareSymbolFont{txsymbolsC}{U}{txsyc}{m}{n}
\SetSymbolFont{txsymbolsC}{bold}{U}{txsyc}{bx}{n}
\DeclareFontSubstitution{U}{txsyc}{m}{n}
\DeclareMathSymbol{\ljoin}{\mathrel}{txsymbolsC}{88}
\DeclareMathSymbol{\rjoin}{\mathrel}{txsymbolsC}{89}


\newcommand{\sql}[1]{\texttt{#1}}

\newcommand\mcup{\uplus}       \newcommand\bigmcup{\biguplus} \newcommand\mcap{\nplus}       \newcommand\bigmcap{\nbigplus} 

\newsavebox\setminusbox
\newlength\setminuslen
\newcommand\msetminus{{\savebox{\setminusbox}{}\settowidth{\setminuslen}{\usebox{\setminusbox}}\,\usebox{\setminusbox}\hspace{-.5\setminuslen}^+}} \newcommand\msetminusl{{\savebox{\setminusbox}{}\settowidth{\setminuslen}{\usebox{\setminusbox}}\setminus\hspace{-.5\setminuslen}^+}} 


\newcommand\xdiag{\operatorname{diag}}
\newcommand\diag[1]{\xdiag\left(#1\right)}    \newcommand\diagn[1]{\xdiag(#1)}    \newcommand\xrank{\operatorname{rank}}
\newcommand\rank[1]{\xrank\left(#1\right)}
\newcommand\rankn[1]{\xrank(#1)}



\newcommand\T{\top}
\newcommand\transpose[1]{#1^{\T}}
\newcommand\tr[1]{#1^{\textrm{T}}}
\newcommand\normn[1]{\lVert #1 \rVert}
\newcommand\fnorm[1]{\left\lVert #1 \right\rVert_\text{F}}
\newcommand\fnormn[1]{\lVert #1 \rVert_\text{F}}
\newcommand\rsim{\stackrel{\mathrm{row}}{\sim}}
\newcommand\csim{\stackrel{\mathrm{col}}{\sim}}




\newcolumntype{C}{>{}} \newcolumntype{L}{>{}} \newcolumntype{R}{>{}} \newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}} 

\newcommand{\mathls}{\addlinespace[5pt]}

\newcommand{\mlsideways}[1]{\begin{sideways}\centering\begin{tabular}{c}#1\end{tabular}\end{sideways}} 

\newcommand{\func}[1]{\textsc{#1}} 

\newcommand{\GQ}[2]{Q_{#1}(#2)} \newcommand{\GP}[2]{P_{#1}(#2)} \newcommand{\B}[3]{B\if*#1\else_{#1}\fi(#2,#3)} \newcommand{\I}[3]{I\if*#1\else_{#1}\fi(#2,#3)} \newcommand{\Digamma}[1]{\Psi(#1)} \newcommand{\harmonic}[1]{H_{#1}} 

\makeatletter
\def\imod#1{\allowbreak\mkern10mu({\operator@font mod}\,\,#1)}
\makeatother

\newcommand{\bigO}[1]{O\!\left(#1\right)} \newcommand{\bigOn}[1]{O(#1)} \newcommand{\bigObig}[1]{O\bigl(#1\bigr)} \newcommand{\bigOmega}[1]{\Omega\!\left(#1\right)} \newcommand{\bigOmegan}[1]{\Omega(#1)} \newcommand{\bigTheta}[1]{\Theta\!\left(#1\right)} \newcommand{\smallo}[1]{o\!\left(#1\right)} 

\newcommand{\toas}{\xrightarrow{\text{a.s.}}}

\newcommand{\dd}{\; \mathrm{d}} 




\newlength\hspaceoflen
\newcommand\hspaceof[1]{\settowidth\hspaceoflen{#1}\hspace\hspaceoflen}

\newcommand{\vcentered}[1]{\begingroup\setbox0=\hbox{#1}\parbox{\wd0}{\box0}\endgroup}

\newcommand\vect[1]{{\boldsymbol{#1}}}
\newcommand\va{\vect{a}}
\newcommand\vb{\vect{b}}
\newcommand\vc{\vect{c}}
\newcommand\vd{\vect{d}}
\newcommand\ve{\vect{e}}
\newcommand\vf{\vect{f}}
\newcommand\vg{\vect{g}}
\newcommand\vh{\vect{h}}
\newcommand\vi{\vect{i}}
\newcommand\vj{\vect{j}}
\newcommand\vk{\vect{k}}
\newcommand\vl{\vect{l}}
\newcommand\vm{\vect{m}}
\newcommand\vn{\vect{n}}
\newcommand\vo{\vect{o}}
\newcommand\vp{\vect{p}}
\newcommand\vq{\vect{q}}
\newcommand\vr{\vect{r}}
\newcommand\vs{\vect{s}}
\newcommand\vt{\vect{t}}
\newcommand\vu{\vect{u}}
\newcommand\vv{\vect{v}}
\newcommand\vw{\vect{w}}
\newcommand\vx{\vect{x}}
\newcommand\vy{\vect{y}}
\newcommand\vz{\vect{z}}
\newcommand\vzero{\vect{0}}
\newcommand\vone{\vect{1}}

\newcommand\valpha{\vect{\alpha}}
\newcommand\vbeta{\vect{\beta}}
\newcommand\veps{\vect{\epsilon}}
\newcommand\vdelta{\vect{\delta}}
\newcommand\veta{\vect{\eta}}
\newcommand\vgamma{\vect{\gamma}}
\newcommand\vtau{\vect{\tau}}
\newcommand\vtheta{\vect{\theta}}
\newcommand\vmu{\vect{\mu}}
\newcommand\vsigma{\vect{\sigma}}
\newcommand\vpi{\vect{\pi}}
\newcommand\vlambda{\vect{\lambda}}

\newcommand\mA{\vect{A}}
\newcommand\mB{\vect{B}}
\newcommand\mC{\vect{C}}
\newcommand\mD{\vect{D}}
\newcommand\mE{\vect{E}}
\newcommand\mF{\vect{F}}
\newcommand\mG{\vect{G}}
\newcommand\mH{\vect{H}}
\newcommand\mI{\vect{I}}
\newcommand\mJ{\vect{J}}
\newcommand\mK{\vect{K}}
\newcommand\mL{\vect{L}}
\newcommand\mM{\vect{M}}
\newcommand\mN{\vect{N}}
\newcommand\mO{\vect{O}}
\newcommand\mP{\vect{P}}
\newcommand\mQ{\vect{Q}}
\newcommand\mR{\vect{R}}
\newcommand\mS{\vect{S}}
\newcommand\mT{\vect{T}}
\newcommand\mU{\vect{U}}
\newcommand\mV{\vect{V}}
\newcommand\mW{\vect{W}}
\newcommand\mX{\vect{X}}
\newcommand\mY{\vect{Y}}
\newcommand\mZ{\vect{Z}}
\newcommand\mzero{\vect{0}}

\newcommand{\mEps}{{\ensuremath{\vect{\Epsilon}}}}
\newcommand{\mSigma}{{\ensuremath{\vect{\Sigma}}}}
\newcommand{\mLambda}{{\ensuremath{\vect{\Lambda}}}}
\newcommand{\mPsi}{{\ensuremath{\vect{\Psi}}}}

\newcommand\bA{\mathbb{A}}
\newcommand\bB{\mathbb{B}}
\newcommand\bC{\mathbb{C}} \newcommand\bD{\mathbb{D}}
\newcommand\bE{\mathbb{E}}
\newcommand\bF{\mathbb{F}}
\newcommand\bG{\mathbb{G}}
\newcommand\bH{\mathbb{H}}
\newcommand\bI{\mathbb{I}}
\newcommand\bJ{\mathbb{J}}
\newcommand\bK{\mathbb{K}}
\newcommand\bL{\mathbb{L}}
\newcommand\bM{\mathbb{M}}
\newcommand\bN{\mathbb{N}} \newcommand\bO{\mathbb{O}}
\newcommand\bP{\mathbb{P}}
\newcommand\bQ{\mathbb{Q}} \newcommand\bR{\mathbb{R}} \newcommand\bS{\mathbb{S}}
\newcommand\bT{\mathbb{T}}
\newcommand\bU{\mathbb{U}}
\newcommand\bV{\mathbb{V}}
\newcommand\bW{\mathbb{W}}
\newcommand\bX{\mathbb{X}}
\newcommand\bY{\mathbb{Y}}
\newcommand\bZ{\mathbb{Z}} 



\newcommand\xA{\mathscr{A}}
\newcommand\xB{\mathscr{B}}
\newcommand\xC{\mathscr{C}}
\newcommand\xD{\mathscr{D}}
\newcommand\xE{\mathscr{E}}
\newcommand\xF{\mathscr{F}}
\newcommand\xG{\mathscr{G}}
\newcommand\xH{\mathscr{H}}
\newcommand\xI{\mathscr{I}}
\newcommand\xJ{\mathscr{J}}
\newcommand\xK{\mathscr{K}}
\newcommand\xL{\mathscr{L}}
\newcommand\xM{\mathscr{M}}
\newcommand\xN{\mathscr{N}}
\newcommand\xO{\mathscr{O}}
\newcommand\xP{\mathscr{P}}
\newcommand\xQ{\mathscr{Q}}
\newcommand\xR{\mathscr{R}}
\newcommand\xS{\mathscr{S}}
\newcommand\xT{\mathscr{T}}
\newcommand\xU{\mathscr{U}}
\newcommand\xV{\mathscr{V}}
\newcommand\xW{\mathscr{W}}
\newcommand\xX{\mathscr{X}}
\newcommand\xY{\mathscr{Y}}
\newcommand\xZ{\mathscr{Z}}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\newcommand\cA{\mathcal{A}}
\newcommand\cB{\mathcal{B}}
\newcommand\cC{\mathcal{C}}
\newcommand\cD{\mathcal{D}}
\newcommand\cE{\mathcal{E}}
\newcommand\cF{\mathcal{F}}
\newcommand\cG{\mathcal{G}}
\newcommand\cH{\mathcal{H}}
\newcommand\cI{\mathcal{I}}
\newcommand\cJ{\mathcal{J}}
\newcommand\cK{\mathcal{K}}
\newcommand\cL{\mathcal{L}}
\newcommand\cM{\mathcal{M}}
\newcommand\cN{\mathcal{N}}
\newcommand\cO{\mathcal{O}}
\newcommand\cP{\mathcal{P}}
\newcommand\cQ{\mathcal{Q}}
\newcommand\cR{\mathcal{R}}
\newcommand\cS{\mathcal{S}}
\newcommand\cT{\mathcal{T}}
\newcommand\cU{\mathcal{U}}
\newcommand\cV{\mathcal{V}}
\newcommand\cW{\mathcal{W}}
\newcommand\cX{\mathcal{X}}
\newcommand\cY{\mathcal{Y}}
\newcommand\cZ{\mathcal{Z}}

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand\tA{\mathbfcal{A}}
\newcommand\tB{\mathbfcal{B}}
\newcommand\tC{\mathbfcal{C}}
\newcommand\tD{\mathbfcal{D}}
\newcommand\tE{\mathbfcal{E}}
\newcommand\tF{\mathbfcal{F}}
\newcommand\tG{\mathbfcal{G}}
\newcommand\tH{\mathbfcal{H}}
\newcommand\tI{\mathbfcal{I}}
\newcommand\tJ{\mathbfcal{J}}
\newcommand\tK{\mathbfcal{K}}
\newcommand\tL{\mathbfcal{L}}
\newcommand\tM{\mathbfcal{M}}
\newcommand\tN{\mathbfcal{N}}
\newcommand\tO{\mathbfcal{O}}
\newcommand\tP{\mathbfcal{P}}
\newcommand\tQ{\mathbfcal{Q}}
\newcommand\tR{\mathbfcal{R}}
\newcommand\tS{\mathbfcal{S}}
\newcommand\tT{\mathbfcal{T}}
\newcommand\tU{\mathbfcal{U}}
\newcommand\tV{\mathbfcal{V}}
\newcommand\tW{\mathbfcal{W}}
\newcommand\tX{\mathbfcal{X}}
\newcommand\tY{\mathbfcal{Y}}
\newcommand\tZ{\mathbfcal{Z}}







\newcommand\Ahat{{\hat A}}
\newcommand\Bhat{{\hat B}}
\newcommand\Chat{{\hat C}}
\newcommand\Dhat{{\hat D}}
\newcommand\Ehat{{\hat E}}
\newcommand\Fhat{{\hat F}}
\newcommand\Ghat{{\hat G}}
\newcommand\Hhat{{\hat H}}
\newcommand\Ihat{{\hat I}}
\newcommand\Jhat{{\hat J}}
\newcommand\Khat{{\hat K}}
\newcommand\Lhat{{\hat L}}
\newcommand\Mhat{{\hat M}}
\newcommand\Nhat{{\hat N}}
\newcommand\Ohat{{\hat O}}
\newcommand\Phat{{\hat P}}
\newcommand\Qhat{{\hat Q}}
\newcommand\Rhat{{\hat R}}
\newcommand\Shat{{\hat S}}
\newcommand\That{{\hat T}}
\newcommand\Uhat{{\hat U}}
\newcommand\Vhat{{\hat V}}
\newcommand\What{{\hat W}}
\newcommand\Xhat{{\hat X}}
\newcommand\Yhat{{\hat Y}}
\newcommand\Zhat{{\hat Z}}

\newcommand\ahat{{\hat a}}
\newcommand\bhat{{\hat b}}
\newcommand\chat{{\hat c}}
\newcommand\dhat{{\hat d}}
\newcommand\ehat{{\hat e}}
\newcommand\fhat{{\hat f}}
\newcommand\ghat{{\hat g}}
\newcommand\hhat{{\hat h}}
\newcommand\ihat{{\hat i}}
\newcommand\jhat{{\hat j}}
\newcommand\khat{{\hat k}}
\newcommand\lhat{{\hat l}}
\newcommand\mhat{{\hat m}}
\newcommand\nhat{{\hat n}}
\newcommand\ohat{{\hat o}}
\newcommand\phat{{\hat p}}
\newcommand\qhat{{\hat q}}
\newcommand\rhat{{\hat r}}
\newcommand\shat{{\hat s}}
\newcommand\that{{\hat t}}
\newcommand\uhat{{\hat u}}
\newcommand\vhat{{\hat v}}
\newcommand\what{{\hat w}}
\newcommand\xhat{{\hat x}}
\newcommand\yhat{{\hat y}}
\newcommand\zhat{{\hat z}}

\newcommand\rhohat{{\hat\rho}}



\accentedsymbol\Abar{{\bar A}}
\accentedsymbol\Bbar{{\bar B}}
\accentedsymbol\Cbar{{\bar C}}
\accentedsymbol\Dbar{{\bar D}}
\accentedsymbol\Ebar{{\bar E}}
\accentedsymbol\Fbar{{\bar F}}
\accentedsymbol\Gbar{{\bar G}}
\accentedsymbol\Hbar{{\bar H}}
\accentedsymbol\Ibar{{\bar I}}
\accentedsymbol\Jbar{{\bar J}}
\accentedsymbol\Kbar{{\bar K}}
\accentedsymbol\Lbar{{\bar L}}
\accentedsymbol\Mbar{{\bar M}}
\accentedsymbol\Nbar{{\bar N}}
\accentedsymbol\Obar{{\bar O}}
\accentedsymbol\Pbar{{\bar P}}
\accentedsymbol\Qbar{{\bar Q}}
\accentedsymbol\Rbar{{\bar R}}
\accentedsymbol\Sbar{{\bar S}}
\accentedsymbol\Tbar{{\bar T}}
\accentedsymbol\Ubar{{\bar U}}
\accentedsymbol\Vbar{{\bar V}}
\accentedsymbol\Wbar{{\bar W}}
\accentedsymbol\Xbar{{\bar X}}
\accentedsymbol\Ybar{{\bar Y}}
\accentedsymbol\Zbar{{\bar Z}}

\accentedsymbol\abar{{\bar a}}
\accentedsymbol\bbar{{\bar b}}
\accentedsymbol\cbar{{\bar c}}
\accentedsymbol\dbar{{\bar d}}
\accentedsymbol\ebar{{\bar e}}
\accentedsymbol\fbar{{\bar f}}
\accentedsymbol\gbar{{\bar g}}
\makeatletter
\@ifundefined{hbar}{}{
        \let\oldhbar\hbar \let\hbar\@undefined
}
\makeatother
\accentedsymbol\hbar{{\bar h}}
\accentedsymbol\ibar{{\bar i}}
\accentedsymbol\jbar{{\bar j}}
\accentedsymbol\kbar{{\bar k}}
\accentedsymbol\lbar{{\bar l}}
\accentedsymbol\mbar{{\bar m}}
\accentedsymbol\nbar{{\bar n}}
\makeatletter
\@ifundefined{obar}{}{
        \let\oldobar\obar \let\obar\@undefined
}
\makeatother
\accentedsymbol{\obar}{{\bar o}}
\accentedsymbol\pbar{{\bar p}}
\accentedsymbol\qbar{{\bar q}}
\accentedsymbol\rbar{{\bar r}}
\accentedsymbol\sbar{{\bar s}}
\accentedsymbol\tbar{{\bar t}}
\accentedsymbol\ubar{{\bar u}}
\accentedsymbol\vbar{{\bar v}}
\accentedsymbol\wbar{{\bar w}}
\accentedsymbol\xbar{{\bar x}}
\accentedsymbol\ybar{{\bar y}}
\accentedsymbol\zbar{{\bar z}}

\newcommand\eps{\epsilon}




\accentedsymbol\mAhat{{\hat\mA}}
\accentedsymbol\mBhat{{\hat\mB}}
\accentedsymbol\mChat{{\hat\mC}}
\accentedsymbol\mDhat{{\hat\mD}}
\accentedsymbol\mEhat{{\hat\mE}}
\accentedsymbol\mFhat{{\hat\mF}}
\accentedsymbol\mGhat{{\hat\mG}}
\accentedsymbol\mHhat{{\hat\mH}}
\accentedsymbol\mIhat{{\hat\mI}}
\accentedsymbol\mJhat{{\hat\mJ}}
\accentedsymbol\mKhat{{\hat\mK}}
\accentedsymbol\mLhat{{\hat\mL}}
\accentedsymbol\mMhat{{\hat\mM}}
\accentedsymbol\mNhat{{\hat\mN}}
\accentedsymbol\mOhat{{\hat\mO}}
\accentedsymbol\mPhat{{\hat\mP}}
\accentedsymbol\mQhat{{\hat\mQ}}
\accentedsymbol\mRhat{{\hat\mR}}
\accentedsymbol\mShat{{\hat\mS}}
\accentedsymbol\mThat{{\hat\mT}}
\accentedsymbol\mUhat{{\hat\mU}}
\accentedsymbol\mVhat{{\hat\mV}}
\accentedsymbol\mWhat{{\hat\mW}}
\accentedsymbol\mXhat{{\hat\mX}}
\accentedsymbol\mYhat{{\hat\mY}}
\accentedsymbol\mZhat{{\hat\mZ}}

\accentedsymbol\vahat{{\hat\va}}
\accentedsymbol\vbhat{{\hat\vb}}
\accentedsymbol\vchat{{\hat\vc}}
\accentedsymbol\vdhat{{\hat\vd}}
\accentedsymbol\vehat{{\hat\ve}}
\accentedsymbol\vfhat{{\hat\vf}}
\accentedsymbol\vghat{{\hat\vg}}
\accentedsymbol\vhhat{{\hat\vh}}
\accentedsymbol\vihat{{\hat\vi}}
\accentedsymbol\vjhat{{\hat\vj}}
\accentedsymbol\vkhat{{\hat\vk}}
\accentedsymbol\vlhat{{\hat\vl}}
\accentedsymbol\vmhat{{\hat\vm}}
\accentedsymbol\vnhat{{\hat\vn}}
\accentedsymbol\vohat{{\hat\vo}}
\accentedsymbol\vphat{{\hat\vp}}
\accentedsymbol\vqhat{{\hat\vq}}
\accentedsymbol\vrhat{{\hat\vr}}
\accentedsymbol\vshat{{\hat\vs}}
\accentedsymbol\vthat{{\hat\vt}}
\accentedsymbol\vuhat{{\hat\vu}}
\accentedsymbol\vvhat{{\hat\vv}}
\accentedsymbol\vwhat{{\hat\vw}}
\accentedsymbol\vxhat{{\hat\vx}}
\accentedsymbol\vyhat{{\hat\vy}}
\accentedsymbol\vzhat{{\hat\vz}}\clearpage{}
\newcommand\ak[1]{\red{\fbox{AK} #1}}
\newcommand\as[1]{\orange{\fbox{AS} #1}}


\aboverulesep = 0.15ex
\belowrulesep = 0.25ex

\RequirePackage[textsize=scriptsize,color=yellow!15,linecolor=red,disable]{todonotes}
\marginparsep 0pt \marginparwidth 24mm

\RequirePackage{enumitem}
\setlist[itemize]{nosep,leftmargin=*,labelwidth=0pt}
\setlist[enumerate]{nosep, leftmargin=*}
\setlist[description]{nosep,leftmargin=.8em}

\RequirePackage{amsmath,amsthm,bm,amssymb}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\secref}[1]{\S\ref{#1}}

\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother


\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\refalg}[1]{Algorithm \ref{#1}}
\newcommand{\refeqn}[1]{Equation \ref{#1}}
\newcommand{\reffig}[1]{Figure \ref{#1}}
\newcommand{\reftbl}[1]{Table \ref{#1}}
\newcommand{\refsec}[1]{Section \ref{#1}}




\newcommand{\m}[1]{\mathcal{#1}}
\newcommand{\bmm}[1]{\bm{\mathcal{#1}}}
\newcommand{\real}[1]{\mathbb{R}^{#1}}

\newcommand{\method}{\textsc{KGT5}}

\newcommand{\probP}{\text{I\kern-0.15em P}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}[theorem]{Claim}

\newcommand{\argmax}{arg\,max}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand{\note}[1]{\textcolor{blue}{#1}}

\newcommand*{\Scale}[2][4]{\scalebox{#1}{}}\newcommand*{\Resize}[2]{\resizebox{#1}{!}{}}


\makeatletter
\g@addto@macro{\normalsize}{\setlength{\abovedisplayskip}{3pt plus1pt}\setlength{\abovedisplayshortskip}{3pt plus1pt}\setlength{\belowdisplayskip}{3pt plus1pt}\setlength{\belowdisplayshortskip}{3pt plus1pt}}
\makeatother
 



\title{Sequence-to-Sequence Knowledge Graph Completion and Question Answering}





\author{Apoorv Saxena\\
  Indian Institute of Science \\
  Bangalore \\
  \small \texttt{apoorvsaxena@iisc.ac.in} \\
  \And
  Adrian Kochsiek \\
  University of Mannheim \\
  Germany \\
  \small \texttt{adrian@informatik.}\\      
  \small \texttt{uni-mannheim.de} \\
  \And Rainer Gemulla \\
University of Mannheim \\
  Germany \\
\small \texttt{rgemulla@uni-mannheim.de}
   \\ 
  }

\begin{document}
\maketitle
\begin{abstract}
Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors.
These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA).
KGEs typically create an embedding for each entity in the graph, which results in large model sizes on real-world graphs with millions of entities. 
For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline, limiting their utility.
We show that an off-the-shelf encoder-decoder Transformer model can serve as a scalable and versatile KGE model obtaining state-of-the-art results for KG link prediction and incomplete KG question answering.
We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding.
Such a simple but powerful method reduces the model size up to 98\% compared to conventional KGE models while keeping inference time tractable.
After finetuning this model on the task of KGQA over incomplete KGs, our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning.\footnote{\label{resources-note}Resources are available at \url{ https://github.com/apoorvumang/kgt5}}





\end{abstract}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth, trim={-1cm 0 -2.5cm 0},clip ]{kgt5_main3.pdf}
  \caption{Overview of our method \method{}. KGT5 is first trained on the link prediction task (predicting head/tail entities, given tail/head and relation). For question answering, the same model is further finetuned using QA pairs.
}
  \label{fig:kgt5-main}
\end{figure*}

\section{Introduction}
\label{sec:introduction}


A knowledge graph (KG) is a multi-relational graph where the nodes are entities from the real world (e.g. \textit{Barack Obama, United States}) and the named edges represent the relationships between them (e.g. \textit{Barack Obama - born in - United States}). 
KGs can be either domain-specific such as WikiMovies \cite{miller2016keyvalue} or public, cross-domain KGs encoding common knowledge such as Wikidata and DBpedia \cite{DBLP:journals/corr/abs-2003-00719}. These graph-structured databases play an important role in knowledge-intensive applications including web search, question answering and recommendation systems \cite{kg_survey_paper}. 


Most real-world knowledge graphs are incomplete.
However, some missing facts can be inferred using existing facts in the KG \cite{bordes2013translating}. 
This task termed knowledge graph completion (KGC)\footnote{We use the term KGC for the task of KG link prediction.} has become a popular area of research in recent years \cite{kge_survey} and is often approached using knowledge graph embedding (KGE) models.
KGE models represent each entity and relation of the KG by a dense vector embedding. 
Using these embeddings the model is trained to distinguish correct from incorrect facts.
One of the main downstream applications of KGEs is question answering over incomplete KGs (KGQA) \cite{kge_application_survey}. 

Taking into account the large size of real world KGs (Wikidata contains 90M entities) and the applicability to downstream tasks, KGE models
should fulfill the following desiderata: 
(i) \emph{scalability} -- i.e. have model size and inference time independent of the number of entities 
(ii) \emph{quality} -- reach good empirical performance
(iii) \emph{versatility} -- be applicable for multiple tasks such as KGC and QA, and 
(iv) \emph{simplicity} -- consist of a single module with a standard architecture and training pipeline.
Traditional KGE models fulfill quality and simplicity. 
They build upon a simple architecture and reach a high quality in terms of KGC.
However, as they create a unique embedding per entity/relation, they scale linearly with the number of entities in the graph, both in model size and inference time, and offer limited versatility.
Methods such as DKRL \cite{Xie_Liu_Jia_Luan_Sun_2016} and KEPLER \cite{wang2021KEPLER} attempt to tackle the scalability issue using compositional embeddings.
However, they fail to achieve quality comparable to conventional KGEs. 
KG-BERT \cite{kg-bert} utilizes pretrained BERT for link prediction and holds potential in terms of versatility as it is applicable to downstream NLP tasks. 
However, it is not scalable due to its underlying cross-encoder.\footnote{
    \citet{time-taken-kgbert} estimate it would take KG-BERT 3 days for an evaluation run on a KG with just 40k entities.
}
QA methods which leverage KGEs outperform traditional KGQA approaches on incomplete KGs, but combining KGEs with the QA pipeline is a non-trivial task;
models that attempt to do this often work on only limited query types (\citealt{huang2019knowledge}; 
\citealt{sun2021faithful}; \citealt{saxena2020improving}) or require multi-stage training and inference pipelines \cite{ren2021lego}.
Here, in order to achieve quality, these models have sacrificed versatility and simplicity.
A comparison of approaches in terms of desiderata is summarized in Tab.~\ref{tab:desiderata} in the appendix.

Our paper shows that all of these desiderata can be fulfilled by a simple sequence-to-sequence (seq2seq) model. To this end, we pose KG link prediction as a seq2seq task and train an encoder-decoder Transformer model \cite{vaswani2017attention} on this task. We then use this model pretrained for link prediction and further finetune it for question answering; while finetuning for QA, we regularize with the link prediction objective. 
This simple but powerful approach, which we call \method{}, is visualised in Fig. \ref{fig:kgt5-main}. 
With such a unified seq2seq approach we achieve 
(i) scalability -- by using compositional entity representations and autoregressive decoding (rather than scoring all entities) for inference
(ii) quality -- we obtain state-of-the-art performance on two tasks 
(iii) versatility -- the same model can be used for both KGC and KGQA on multiple datasets, and 
(iv) simplicity -- we obtain all results using an off-the-shelf model with no task or dataset-specific hyperparameter tuning.












\noindent In summary, we make the following contributions: 
\begin{itemize}
    \item We show that KG link prediction and question answering can be treated as sequence-to-sequence tasks and tackled successfully with a single encoder-decoder Transformer 
    (with the same architecture as T5-small \cite{raffel2020exploring}).
\item With this simple but powerful approach called \method{}, we reduce model size for KG link prediction up to 98\% while outperforming conventional KGEs on a dataset with 90M entities.
\item We show the versatility of this approach through the task of KGQA over incomplete graphs. By pretraining on KG link prediction and finetuning on QA, KGT5 performs similar to or better than much more complex methods on multiple large-scale KGQA benchmarks. 
\end{itemize}
















\section{Background \& Related Work}
\label{sec:related_work}
Given a set of entities  and a set of relations , a knowledge graph  is a collection of subject-predicate-object  triples.
Link prediction is the task of predicting missing triples in  by answering queries of the form of  and . This is typically accomplished using knowledge graph embedding (KGE) models.

Conventional KGEs assign an embedding vector to each entity and relation in the KG. They model the plausibility of  triples via model specific scoring functions  using the subject (), predicate () and object () specific embeddings.
Once trained, these embeddings are used for downstream tasks such as question answering.




Knowledge graph question answering (KGQA) is the task of answering a natural language question using a KG as source of knowledge. 
The questions can be either simple factual questions that require single fact retrieval 
(e.g. \textit{Which languages are spoken in India?}), 
or they can be complex questions that require reasoning over multiple facts in the KG 
(e.g. \textit{What are the genres of movies, in which Leonardo DiCaprio was leading actor?}).
KGEs can be utilized to perform KGQA when the background KGs are incomplete.


In the next few sections we will go into more detail about existing work on KGEs and KGQA.

\subsection{Knowledge Graph Embeddings}
\label{sec:kge}

\textbf{Atomic KGE models.} 
Multiple KGE models have been proposed in the literature, mainly differing in the form of their scoring function . A comprehensive survey of these models, their scoring functions, training regime and link prediction performance can be found in \citet{kge_survey} and \citet{ruffinelli2020you}.
It is important to note that although these models obtain superior performance in the link prediction task, they suffer from a linear scaling in model size with the number of entities in the KG, and applying them to question answering necessitates separate KGE and QA modules.




\noindent\textbf{Compositional KGE models.}
To combat the linear scaling of the model size with the number of entities in a KG, entity embeddings can be composed of token embeddings.
DKRL~\cite{xie2016representation} embeds entities by combining word embeddings of entity descriptions with a CNN encoder, followed by the TransE scoring function.
KEPLER~\cite{wang2021KEPLER} uses a Transformer-based encoder and combines the typical KGE training objective with a masked language modeling objective.
Both of these approaches encode entities and relations separately which limits the transferability of these models to downstream tasks such as question answering.
MLMLM~\cite{clouatre-etal-2021-mlmlm} encodes the whole query with a RoBERTa-based model and uses \texttt{[MASK]} tokens to generate predictions. However, it performs significantly worse than atomic KGE models on link prediction on large KGs, and is yet to be applied to downstream text-based tasks.



\begin{figure*}
  \centering
  \includegraphics[width=\textwidth, trim={0.2cm 0cm 0.3cm 0cm},clip ]{kgt5_inference9.pdf}
  \caption{Inference pipeline of (A) conventional KGE models versus (B) \method{} on the link prediction task. Given a query , we first verbalize it to a textual representation and then input it to the model. A fixed number of sequences are sampled from the model decoder and then mapped back to their entity IDs. This is in contrast to conventional KGEs, where each entity in the KG must be scored. Please see \secref{sec:lp_inference} for more details.
} 
  \label{fig:kgt5-inference}
\end{figure*}



\subsection{Knowledge Graph Question Answering}
\label{sec:kgqa}


Knowledge Graph Question Answering (KGQA) has been traditionally solved using semantic parsing  (\citealt{Berant2013SemanticPO}; \citealt{bast2015}; \citealt{das2021casebased}) where a natural language (NL) question is converted to a symbolic query over the KG. This is problematic for incomplete KGs, where a single missing link can cause the query to fail.
Recent work has focused on KGQA over incomplete KGs, which is also the focus of our work. These methods attempt to overcome KG incompleteness using KG embeddings (\citealt{huang2019knowledge}; \citealt{saxena2020improving}; \citealt{sun2021faithful}; \citealt{ren2021lego}). In order to use KGEs for KGQA, these methods first train a KGE model on the background KG, and then integrate the learned entity and relation embeddings into the QA pipeline. This fragmented approach brings several disadvantages;
for example \citet{huang2019knowledge}'s method only works for single fact question answering, while EmQL \cite{sun2021faithful} requires prior knowledge of the NL question's query structure. EmbedKGQA \cite{saxena2020improving} is capable of multi-hop question answering but is unable to deal with questions involving more than one entity. Hence, these methods are lacking in versatility. LEGO \cite{ren2021lego} can theoretically answer all first order logic based questions but requires multiple dataset dependent components including entity linking, relation pruning and branch pruning modules; here, to obtain versatility, LEGO has sacrificed simplicity.





\section{The \method{} Model}
\label{sec:model}




We pose both knowledge graph link prediction and question answering as sequence-to-sequence (seq2seq) tasks. We then train a simple encoder-decoder Transformer -- that has the same architecture as T5-small~\cite{raffel2020exploring} but without the pretrained weights -- on these tasks. While training for question answering, we regularize with the link prediction objective. This method, which we call \method{}, results in a scalable KG link prediction model with vastly fewer parameters than conventional KGE models for large KGs. 
This approach also confers simplicity and versatility to the model, whereby it can be easily adapted to KGQA on any dataset regardless of question complexity.

Posing KG link prediction as a seq2seq task requires textual representations of entities and relations, and a verbalization scheme to convert link prediction queries to textual queries; these are detailed in \secref{sec:query_verbalization}.
The link prediction training procedure is explained in \secref{sec:training} and inference in \secref{sec:lp_inference}. The KGQA finetuning and inference pipeline is explained in~\secref{sec:kgqa_training_and_inference}.














\subsection{Textual Representations \& Verbalization}
\label{sec:query_verbalization}
\textbf{Text mapping.} For link prediction we require a one-to-one mapping between an entity/relation and its textual representation.
For Wikidata-based KGs, we use canonical mentions of entities and relations as their textual representation, followed by a disambiguation scheme that appends descriptions and unique ids to the name.\footnote{Please see appendix \ref{sec:text_rep_appendix} for details on textual representations.}
For datasets used for QA only we do not enforce a one-to-one mapping as, in this case, unnecessary disambiguation can even harm model performance.\footnote{This is because QA systems consider surface forms during evaluation, not entity IDs. For example, it will be better to have the same mention for both the single and album version of a song rather than append a unique number to their mentions.}


\noindent\textbf{Verbalization.} We convert  query answering to a sequence-to-sequence task by \textit{verbalizing} the query  to a textual representation. This is similar to the verbalization performed by \citet{lama}, except there is no relation-specific template. For example, given a query \textit{(barack obama, born in, ?)}, we first obtain the textual mentions of the entity and relation and then verbalize it as 
\texttt{'predict tail: barack obama | born in'}. This sequence is input to the model, and output sequence is expected to be the answer to this query, \texttt{'united states'}, which is the unique mention of entity \textit{United States}. 














\subsection{Training \method{} for Link Prediction}
\label{sec:training}
To train KGT5, we need a set of (input, output) sequences. For each triple  in the training graph, we verbalize the queries  and  according to \secref{sec:query_verbalization} to obtain two input sequences. The corresponding output sequences are the text mentions of  and  respectively. KGT5 is trained with teacher forcing \cite{teacher_forcing} and cross entropy loss.\footnote{More details about training are available in Appendix \ref{sec:training_appendix}}

One thing to note is that unlike standard KGE models, we train \textit{without explicit negative sampling}. 
At each step of decoding, the model produces a probability distribution over possible next tokens. While training, this distribution is penalised for being different from the `true' distribution (i.e. a probability of 1 for the true next token, 0 for all other tokens) using cross entropy loss. Hence, this training procedure is most similar to the 1vsAll + CE loss in \citet{ruffinelli2020you}, except instead of scoring the true entity against all other entities, we are scoring the true token against all other tokens at each step, and the process is repeated as many times as the length of the tokenized true entity. This avoids the need for many negatives, and is independent of the number of entities.








\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lrrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{Dataset}} &
  \multicolumn{1}{c}{\textbf{Entities}} &
  \multicolumn{1}{c}{\textbf{Rels}} &
  \multicolumn{1}{c}{\textbf{Edges}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Token.\\ vocab\end{tabular}}} \\ \midrule
WikiKG90Mv2         & 91M  & 1,387 & 601M & 32k \\
Wikidata5M          & 4.8M & 828   & 21M  & 30k \\
MetaQA              & 43k  & 9     & 70k  & 10k \\
WQSP      & 158k & 816   & 376k & 32k \\
CWQ & 3.9M & 326   & 6.9M & 32k \\ \bottomrule
\end{tabular}}
\caption{Statistics of the KGs used.~We use subsets of FreeBase~\cite{freebase:datadumps}  for WebQuestionsSP (WQSP) and ComplexWebQuestions (CWQ).
}
\label{tab:kg-stats}
\end{table} 
\subsection{Link Prediction Inference}
\label{sec:lp_inference}
In conventional KGE models, we answer a query   by finding the score , where  is the model-specific scoring function. The entities  are then ranked according to the scores.

In our approach, given query , we first verbalize it (\secref{sec:query_verbalization}) before feeding it to KGT5.
We then \textit{sample} a fixed number of sequences from the decoder,\footnote{See Appendix \ref{sec:sampling_appendix} for additional details on sampling and our choice of decoding strategy.} which are then mapped to their entity ids.\footnote{
    The decoded sequence may or may not be an entity mention. We experimented with constrained decoding \cite{decao2020autoregressive} to force the decoder to output only entity mentions; however, we found this unnecessary since the model almost always outputs an entity mention, and increasing the number of samples was enough to solve the issue.
}
By using such a generative model, we are able to approximate (with high confidence) top- model predictions without having to score all entities in the KG, as is done by conventional KGE models.
For each decoded entity we assign a score equal to the (log) probability of decoding its sequence. This gives us a set of (entity, score) pairs. 
To calculate the final ranking metrics comparable to traditional KGE models, we assign a score of  for all entities not encountered during the sampling procedure. A comparison of inference strategy of conventional KGE models and \method{} is shown in Figure~\ref{fig:kgt5-inference}.







\subsection{KGQA Training and Inference}
\label{sec:kgqa_training_and_inference}
For KGQA, we pretrain the model on the background KG using the link prediction task (\secref{sec:training}). This pretraining strategy is analogous to `KGE module training' used in other KGQA works (\citealt{sun2021faithful}; \citealt{ren2021lego}). The same model is then finetuned for question answering.
Hereby, we employ the same strategy as \citet{roberts-etal-2020-much}: we concatenate a new task prefix (\texttt{predict answer:}) with the input question and define the mention string of the answer entity as output. 
This unified approach allows us to apply \method{} to any KGQA dataset regardless of question complexity, and without the need for sub-modules such as entity linking.

To combat overfitting during QA finetuning (especially on tasks with small KGs) we devise a regularisation scheme: we add link prediction sequences sampled randomly from the background KG to each batch such that a batch consists of an equal number of QA and link prediction sequences.
For inference, we use beam search followed by neighbourhood-based reranking (\secref{sec:setup}) to obtain the model's prediction which is a single answer.










\begin{table*}[ht!]
\centering

\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Model} & \textbf{MRR}   & \textbf{Hits@1} & \textbf{Hits@3} & \textbf{Hits@10} & \textbf{Params} \\ \midrule
TransE \cite{bordes2013translating}         & 0.253          & 0.170            & 0.311           & 0.392            & 2,400M            \\
DistMult \cite{yang2015embedding}       & 0.253          & 0.209           & 0.278           & 0.334            & 2,400M            \\
SimplE \cite{kazemi2018simple}          & 0.296          & 0.252  & 0.317           & 0.377            & 2,400M            \\
RotatE \cite{sun2018rotate}          & 0.290           & 0.234           & 0.322           & 0.390             & 2,400M            \\
QuatE \cite{zhang2019quaternion}          & 0.276          & 0.227           & 0.301           & 0.359            & 2,400M            \\
ComplEx \cite{trouillon2016complex} ^\ddagger^\ddagger^{\dagger\dagger}^{\dagger\dagger}^{\ddagger\ddagger}\dagger\ddagger\
\begin{aligned}
    score(a) &= p_a + \alpha \quad \mathrm{if} \  a \in \cN(e)  \\
     &= p_a \qquad \quad \mathrm{otherwise}
\end{aligned}

\begin{aligned}
    y_{t,c} &= \mathbbm{1}_{c = w_t} \\
    p_{t,c} &= \probP(v_c | input, w_1, w_2, ..., w_{t-1}) \\
    J_t &= - \sum_{c=1}^{M} y_{t,c}\log p_{t,c} \\
    Loss &= \frac{1}{T}\sum_{t=1}^T J_t
\end{aligned}

\begin{aligned}
\sum_{t=1}^T \log(\probP(w_t | input, w_1, w_2, ..., w_{t-1}))
\end{aligned}

where  is the model's output distribution.

Another way to obtain large number predictions could have been beam search \cite{graves_beam_search}. This would also have the advantage of being deterministic and guaranteed to produce as many predictions as we want. Although in theory wider beam sizes should give improved performance, it has been observed that for beam sizes larger than 5, performance of generative models suffers drastically \cite{yang_beam_search_curse} and sampling generally produces better results. We observe the same phenomenon in our work where beam size 50 produces far worse results than sampling 50 times (fig. \ref{fig:beam-sample-plot}). Modifying the stopping criteron \cite{murray_length_bias} or training method \cite{unlikelihood_training} might be helpful solutions that we hope to explore in future work.
\begin{table}[t!]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Dataset} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\ \midrule
MetaQA 1-hop     & 96,106         & 9,992               & 9,947         \\
MetaQA 2-hop     & 118,980        & 14,872              & 14,872        \\
MetaQA 3-hop     & 114,196        & 14,274              & 14,274        \\
WQSP             & 2,998          & 100                 & 1,639         \\
CWQ              & 27,639         & 3,519               & 3,531         \\ \bottomrule
\end{tabular}
\caption{Numbers of questions in the KGQA datasets used in our experiments.}
\label{tab:kgqa-dataset-stats}
\end{table} \begin{table}[t!]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lllll@{}}
\toprule
Dataset &
  \begin{tabular}[c]{@{}l@{}}Train \\ Questions\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Distinct \\ Qtypes\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Distinct \\ NL questions\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Train \\ QA pairs\end{tabular} \\ \midrule
1-hop & 96,106  & 11 & 161 & 184,884   \\
2-hop & 118,980 & 21 & 210 & 739,782   \\
3-hop & 114,196 & 15 & 150 & 1,521,495 \\ \bottomrule
\end{tabular}
}
\caption{Statistics for MetaQA QA datasets. Since it is a template-based dataset, there is very little linguistic variation - for each linguistic variation, there are more than 1,000 QA pairs on average in the 1-hop dataset. This is further amplified for 2-hop and 3-hop datasets since there are more correct answers on average per question.}
\label{tab:metaqa-traversal-stats}
\end{table} 


\begin{table*}[t!]
\centering
\begin{tabular}{c|llll|llll}
\hline
\multirow{3}{*}{Model}       & \multicolumn{4}{c|}{MRR}                                                                                                                                                                       & \multicolumn{4}{c}{Hits@1}                                                                                                                                                                    \\ \cline{2-9} 
                             & \multicolumn{3}{c|}{No. of entities to filter}                                                  & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}All \\ queries\end{tabular}}} & \multicolumn{3}{c|}{No. of entities to filter}                                                  & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}All \\ queries\end{tabular}}} \\ \cline{2-4} \cline{6-8}
                             & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1 to 10} & \multicolumn{1}{l|}{>10} & \multicolumn{1}{c|}{}                                                                        & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1 to 10} & \multicolumn{1}{l|}{>10} & \multicolumn{1}{c}{}                                                                        \\ \hline
\multicolumn{1}{l|}{ComplEx} & 0.534                   & \textbf{0.351}                             & \textbf{0.045}                     & 0.296                                                                               & 0.464                  & \textbf{0.233}                             & \textbf{0.027}                              & 0.241                                                                              \\
\multicolumn{1}{l|}{\method{}}    & \textbf{0.624}         & 0.215                    & 0.015                              & 0.300                                                                                        & \textbf{0.567}          & 0.164                    & 0.011                     & 0.267                                                                                        \\ \hline
\end{tabular}
\caption{For a test query , there can be multiple entities  such that  is in train set. These entities need to be `filtered' before evaluation. This table shows model performance on queries requiring different amounts of filtering. Dataset is Wikidata5M. The ComplEx checkpoint used in this analysis is slightly worse than the SOTA.}
\label{tab:kgc-filtering}
\end{table*} 
\begin{table*}[t!]
\centering
\begin{tabular}{@{}llllll@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Model(s)}} & \multicolumn{3}{c}{MetaQA} & \multirow{2}{*}{WQSP} & \multirow{2}{*}{CWQ} \\
\multicolumn{1}{c}{}                          & 1-hop   & 2-hop   & 3-hop  &                       &                      \\ \midrule
Baselines (LEGO, EmbedKGQA, EMQL, PullNet)    & 63.3    & 45.8    & 45.3   & 56.9                  & 25.2                 \\
Ours (\method{}, \method{} Ensemble)                   & 67.7    & 48.7    & 44.4   & 56.9                  & 24.5                 \\ \bottomrule
\end{tabular}
\caption{Percentage of questions answerable using ground truth query. For the baselines that we compare with, we do not have access to the exact same 50\% KG split used by them. This table lists the percentage of questions answerable using GT query, for the KGs used by the comparison models (LEGO, EmbedKGQA, EMQL, PullNet) as well as by our models (\method{}, \method{} + PathPred Ensemble). The GT query numbers for baselines were made available by \citealt{ren2021lego}.} 
\label{tab:kg-traversal-stats}
\end{table*} 

\section{Path Predictor on MetaQA}
\label{sec:kg_traversal_on_metaqa}

Being an artificially generated template-based dataset, MetaQA has far more questions than any other dataset that we compare with (Tab.~\ref{tab:kgqa-dataset-stats}). It also has very little variety in the forms of questions (Tab.~\ref{tab:metaqa-traversal-stats}). Hence we try to answer the following question: Can we create a simple model that maps a NL question to a relation path, and then does KG traversal with this path to answer questions? We achieve this by using distant supervision to get the question  path mapping data, which is then processed to get the final model. We call this model PathPred.
\textit{We do not use ground truth queries to create this data}.

A question in MetaQA consists of the question text , a topic entity  and a set of answers  (answers only in train set). Since the topic entity annotation is present for all questions (including test set), we can replace the entity in the question to get a base template .\footnote{As an example given a  `\textit{who are the co-actors of Brad Pitt}' and topic entity annotation `\textit{Brad Pitt}', we can get a base template  as `\textit{who are the co-actors of NE}' where \textit{NE} (named entity) is the substitution string.}

Given a training tuple of , we find all the k-hop relation paths  between  and  (k=1,2 or 3 depending on the dataset). We then aggregate these paths for each distinct , and take the most frequent path as the mapping from  to relation path. This mapping from question template  to a relation path  constitutes the PathPred model.

For a test question , we first get  from . We then use the aforementioned mapping to get a relation path using . This relation path is then used to traverse the KG starting from  to arrive at the answer(s).

In the \method{} + PathPred ensemble (\secref{sec:experiment_kgqa}, Tab.~\ref{tab:metaqa-main-results}), we first apply the PathPred technique; if the resulting answer set is empty -- which can happen due to KG incompleteness -- we apply \method{} to get the answer.








\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{\begin{minipage}{.5\linewidth}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Question type} & \multicolumn{1}{l}{\textbf{GTQ}} & \multicolumn{1}{l}{\textbf{KGT5}} & \multicolumn{1}{l}{\textbf{Gain}} \\ \midrule
actormovie    & 0.96 & 0.95 & \cellcolor[HTML]{E5F5ED}-0.01 \\
directormovie & 0.84 & 0.92 & \cellcolor[HTML]{95D5B6}0.08  \\
movieactor    & 0.79 & 0.77 & \cellcolor[HTML]{EEF8F3}-0.02 \\
moviedirector & 0.52 & 0.64 & \cellcolor[HTML]{72C69D}0.12  \\
moviegenre    & 0.48 & 0.63 & \cellcolor[HTML]{57BB8A}0.15  \\
movielanguage & 0.49 & 0.63 & \cellcolor[HTML]{60BF91}0.14  \\
movietags     & 0.72 & 0.7  & \cellcolor[HTML]{EEF8F3}-0.02 \\
moviewriter   & 0.66 & 0.8  & \cellcolor[HTML]{60BF91}0.14  \\
movieyear     & 0.46 & 0.45 & \cellcolor[HTML]{E5F5ED}-0.01 \\
tagmovie      & 1    & 0.96 & \cellcolor[HTML]{F7D7D5}-0.04 \\
writermovie   & 0.88 & 0.94 & \cellcolor[HTML]{A7DCC2}0.06  \\ \midrule
All                 & 0.678 & 0.732 & 0.054                           \\ \bottomrule
\end{tabular}\end{minipage}\begin{minipage}{.6\linewidth}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Question type} & \multicolumn{1}{l}{\textbf{GTQ}} & \multicolumn{1}{l}{\textbf{KGT5}} & \multicolumn{1}{l}{\textbf{Gain}} \\ \midrule
actormoviedirector    & 0.44 & 0.39 & \cellcolor[HTML]{FDF5F4}-0.05 \\
directormoviedirector & 0.34 & 0.62 & \cellcolor[HTML]{8AD0AE}0.28  \\
directormovielanguage & 0.37 & 0.77 & \cellcolor[HTML]{57BB8A}0.4   \\
writermoviewriter     & 0.39 & 0.39 & \cellcolor[HTML]{FFFFFF}0     \\
\rowcolor[HTML]{FFFFFF} 
actormoviegenre       & 0.48 & 0.55 & \cellcolor[HTML]{E2F4EB}0.07  \\
\rowcolor[HTML]{FFFFFF} 
directormoviegenre    & 0.46 & 0.7  & \cellcolor[HTML]{9BD7B9}0.24  \\
\rowcolor[HTML]{FFFFFF} 
actormovieactor       & 0.57 & 0.09 & \cellcolor[HTML]{EDA19A}-0.48 \\
\rowcolor[HTML]{FFFFFF} 
writermovieactor      & 0.51 & 0.31 & \cellcolor[HTML]{F7D7D5}-0.2  \\
\rowcolor[HTML]{FFFFFF} 
actormoviewriter      & 0.48 & 0.44 & \cellcolor[HTML]{FDF7F6}-0.04 \\
\rowcolor[HTML]{FFFFFF} 
moviedirectormovie    & 0.45 & 0.21 & \cellcolor[HTML]{F6D0CC}-0.24 \\
\rowcolor[HTML]{FFFFFF} 
actormovieyear        & 0.48 & 0.23 & \cellcolor[HTML]{F5CECA}-0.25 \\
\rowcolor[HTML]{FFFFFF} 
writermoviegenre      & 0.4  & 0.59 & \cellcolor[HTML]{B0DFC8}0.19  \\
\rowcolor[HTML]{FFFFFF} 
directormovieactor    & 0.51 & 0.5  & \cellcolor[HTML]{FEFDFC}-0.01 \\
\rowcolor[HTML]{FFFFFF} 
movieactormovie       & 0.73 & 0.06 & \cellcolor[HTML]{E67C73}-0.67 \\
\rowcolor[HTML]{FFFFFF} 
writermovieyear       & 0.37 & 0.35 & \cellcolor[HTML]{FEFBFA}-0.02 \\
\rowcolor[HTML]{FFFFFF} 
directormovieyear     & 0.45 & 0.51 & \cellcolor[HTML]{E6F5EE}0.06  \\
\rowcolor[HTML]{FFFFFF} 
directormoviewriter   & 0.47 & 0.44 & \cellcolor[HTML]{FDF9F8}-0.03 \\
\rowcolor[HTML]{FFFFFF} 
moviewritermovie      & 0.5  & 0.3  & \cellcolor[HTML]{F7D7D5}-0.2  \\
\rowcolor[HTML]{FFFFFF} 
writermoviedirector   & 0.33 & 0.31 & \cellcolor[HTML]{FEFBFA}-0.02 \\
writermovielanguage   & 0.32 & 0.66 & \cellcolor[HTML]{71C69C}0.34  \\
actormovielanguage    & 0.4  & 0.54 & \cellcolor[HTML]{C5E8D7}0.14  \\ \midrule
All                               & 0.471 & 0.363 & -0.108                      \\ \bottomrule
\end{tabular}

\end{minipage}}
\caption{Hits@1 performance on MetaQA 1-hop (left) and 2-hop (right) validation dataset, 50\% KG setting. GTQ refers to ground truth querying.}
\label{tab:metaqa_analysis_2hop}
\end{table*}








\begin{table*}[ht!]
\centering
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Question type} & \multicolumn{1}{l}{\textbf{GTQ}} & \multicolumn{1}{l}{\textbf{KGT5}} & \multicolumn{1}{l}{\textbf{Gain}} \\ \midrule
moviedirectormovielanguage & 0.17 & 0.85 & \cellcolor[HTML]{57BB8A}0.68  \\
moviedirectormovieactor    & 0.37 & 0.54 & \cellcolor[HTML]{D5EEE2}0.17  \\
movieactormovielanguage    & 0.29 & 0.8  & \cellcolor[HTML]{81CCA8}0.51  \\
moviewritermovieyear       & 0.31 & 0.47 & \cellcolor[HTML]{D8EFE4}0.16  \\
movieactormoviedirector    & 0.65 & 0.57 & \cellcolor[HTML]{F7D7D5}-0.08 \\
moviedirectormoviegenre    & 0.37 & 0.82 & \cellcolor[HTML]{90D3B2}0.45  \\
moviewritermoviedirector   & 0.4  & 0.52 & \cellcolor[HTML]{E2F3EB}0.12  \\
movieactormovieyear        & 0.63 & 0.72 & \cellcolor[HTML]{E9F7F0}0.09  \\
movieactormoviewriter      & 0.63 & 0.51 & \cellcolor[HTML]{F7D7D5}-0.12 \\
movieactormoviegenre       & 0.65 & 0.83 & \cellcolor[HTML]{D3EEE1}0.18  \\
moviedirectormoviewriter   & 0.39 & 0.55 & \cellcolor[HTML]{D8EFE4}0.16  \\
moviewritermoviegenre      & 0.42 & 0.75 & \cellcolor[HTML]{AEDEC7}0.33  \\
moviewritermovieactor      & 0.41 & 0.43 & \cellcolor[HTML]{FBFDFC}0.02  \\
moviedirectormovieyear     & 0.32 & 0.56 & \cellcolor[HTML]{C4E7D6}0.24  \\
moviewritermovielanguage   & 0.27 & 0.74 & \cellcolor[HTML]{8BD0AF}0.47  \\ \midrule
All                                          & 0.443 & 0.634 & 0.191                          \\ \bottomrule
\end{tabular}\caption{Hits@1 performance on MetaQA 3-hop validation dataset, 50\% KG setting. GTQ refers to ground truth querying.}
\label{tab:metaqa_analysis_3hop}
\end{table*} 



\end{document}
