\documentclass{amsart}
\pdfoutput=1
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{todonotes}

\newtheorem{Lem}{Lemma}
\newtheorem{Thm}{Theorem}



\begin{document}



\title []{Counting thin subgraphs via packings\\faster than meet-in-the-middle time}
\author []{Andreas Bj\"orklund and Petteri Kaski and \L ukasz Kowalik}

\begin{abstract}
Vassilevska and Williams~(STOC 2009) showed how to count simple paths on  vertices and matchings on  edges in an -vertex graph in time . In the same year, two different algorithms with the same runtime were given by Koutis and Williams~(ICALP 2009), and Bj\"orklund \emph{et al.}~(ESA 2009), via -time algorithms for counting -tuples of pairwise disjoint sets drawn from a given family of -sized subsets of an -element universe.
Shortly afterwards, Alon and Gutner~(TALG 2010) showed that these problems have  and  lower bounds when counting by color coding. 

Here we show that one can do better, namely, we show that the ``meet-in-the-middle'' exponent  can be beaten and give an algorithm that counts in time  for  a multiple of three. This implies algorithms for counting occurrences of a fixed subgraph on  vertices and pathwidth  in an -vertex graph in  time, improving on the three mentioned algorithms for paths and matchings, and circumventing the color-coding lower bound. 
We also give improved bounds for counting -tuples of disjoint -sets for . 

Our algorithms use fast matrix multiplication. We show an argument that this is necessary to go below the meet-in-the-middle barrier.
\end{abstract}

\maketitle




\section{Introduction}

Suppose we want to count the number of occurrences of a -element pattern 
in an -element universe. This setting is encountered, for example,
when  is a -vertex pattern graph,  is an -vertex host graph, and 
we want to count the number of subgraphs that are isomorphic to  in .
If  is a constant independent of , enumerating all the -element 
subsets or tuples of the -element universe can be done in time , 
which presents a trivial upper bound for counting small patterns. 

In this paper we are interested in patterns that are {\em thin}, such 
as pattern graphs that are paths or cycles, or more generally pattern graphs 
with {\em bounded pathwidth}. Characteristic to such patterns is 
that they can be split into two or more parts, such that the interface 
between the parts is easy to control. For example, a simple path on  
vertices can be split into two paths of half the length that have exactly 
one vertex in common; alternatively, one may split the path into 
two independent sets of vertices.

The possibility to split into two controllable parts immediately suggests 
that one should pursue an algorithm that runs in no worse 
time than ; such an algorithm was indeed 
discovered in 2009 by Vassilevska and Williams~\cite{VW09} for 
counting -vertex subgraphs that admit an independent set of size .
This result was accompanied, within the same year, of two publications 
presenting the same runtime restricted to counting paths and matchings. Koutis 
and Williams~\cite{KW09} and Bj\"orklund \emph{et al.}~\cite{BHKK09} 
describe different algorithms for the related problem of counting 
the number of -tuples of disjoint sets that can be formed from
a given family of -subsets of an -element universe in 
 time. Fomin~\emph{et al.}~\cite{FLRRS12} 
generalized the latter result into an algorithm that counts occurrences 
of a -vertex pattern graph with pathwidth  in  time.

Splitting into three parts enables faster listing of the parts in 
 time, but requires more elaborate control at the interface 
between parts. This strategy enables one to count also dense subgraphs
such as -cliques via an algorithm of Ne\v{s}et\v{r}il and 
Poljak \cite{NP85} (see also \cite{EG04,KKM00}) that uses fast
matrix multiplication to achieve a pairwise join of the three parts, 
resulting in running time , where 
 is the limiting exponent of square matrix 
multiplication \cite{LG14,VW12}.
Even in the case  this running time is, however, 
, which is inferior to ``meeting in the middle'' by 
splitting into two parts.

But is meet-in-the-middle really the best one can do?
For many problems it appears indeed that the worst-case running time given 
by meet-in-the-middle is difficult to beat. Among the most notorious examples 
in this regard is the Subset Sum problem, for which the 1974 
meet-in-the-middle algorithm of Horowitz and Sahni~\cite{HS74} remains to 
date the uncontested champion. Related problems such as the -Sum problem 
have an equally frustrating status, in fact to such an extent that the 
case  is regularly used as a source of hardness reductions in 
computational geometry~\cite{GO12}.

Against this background one could perhaps expect a barrier 
at the meet-in-the-middle time  for counting
thin subgraphs, and such a position would not be without some supporting 
evidence. Indeed, not only are the algorithms of Vassilevska and 
Williams~\cite{VW09}, Koutis and Williams~\cite{KW09}, and Bj\"orklund \emph{et al.}~\cite{BHKK09} fairly recent discoveries, but they all employ 
rather different techniques. Common to all three algorithms is however 
the need to consider the -element subsets of the -element vertex set, 
resulting in time . Yet further evidence towards a barrier 
was obtained by Alon and Gutner~\cite{AG09} who showed that 
color-coding based counting approaches relying on 
a perfectly -balanced family of hash functions face 
an unconditional  lower bound for the size 
of such a family. From a structural complexity perspective 
Flum and Grohe~\cite{FG04} have shown that counting -paths is \#W[1]-hard 
with respect to the parameter , and a very recent breakthrough of 
Curticapean~\cite{C13} establishes a similar barrier to counting -matchings.
This means that parameterized counting algorithms with running time  
for a function  independent of  are unlikely for these problems,
even if such a structural complexity approach does not pinpoint precise
lower bounds of the form  for some function . 

Contrary to the partial evidence above, however, our objective
in this paper is to show that 
{\em there is a crack in the meet-in-the-middle barrier}, 
albeit a modest one. In particular, we show that it is possible 
to count subgraphs on  vertices such as paths and matchings---and more 
generally any -vertex subgraphs with pathwidth ---within 
time  for .

Our strategy is to reduce the counting problem to the task of 
evaluating a particular trilinear form on weighted hypergraphs, and 
then show that this trilinear form admits an evaluation algorithm that breaks 
the meet-in-the-middle barrier. This latter algorithm is our main contribution,
which we now proceed to present in more detail.

\subsection{Weighted disjoint triples}

Let  be an -element set. For a nonnegative integer , let us write  for the set of all -element subsets of . 
Let  be three functions given 
as input. We are interested in computing the trilinear form

To ease the running time analysis, we make two assumptions. 
First,  is a constant independent of . 
Second, we assume that the values of the functions  are 
bounded in bit-length by a polynomial in , which will be the setup 
in our applications (Theorems~\ref{thm:subgraph}~and~\ref{thm:packings}).

Let us write  for the limiting exponent of square matrix 
multiplication,  \cite{LG14,VW12}.
Similarly, let us write  for the limiting 
exponent such that multiplying an  matrix with 
an  matrix takes  arithmetic operations, 
~\cite{LG12}. 

The next theorem is our main result; here the intuition is that we
take  in our applications, implying that we break the 
meet-in-the-middle exponent .

\begin{Thm}[Fast weighted disjoint triples]
\label{thm:main}
There exists an algorithm that evaluates  
in time 
for constants  and  independent of the constant ,
with  and 

\end{Thm}

\noindent
{\em Remark 1.} 
For  and  we obtain  and hence
 time.
For  we obtain  and hence
 time. Note that the latter case
occurs in the case  because then .

\medskip
\noindent
{\em Remark 2.} 
We observe that the trilinear form \eqref{eq:main} admits an evaluation 
algorithm analogous to the algorithm of 
Ne\v{s}et\v{r}il and Poljak \cite{NP85} discussed above. 
Indeed, \eqref{eq:main} can be split into a multiplication of two
 square matrices, which gives running time
. Even in the case  the running
time  is however inferior to Theorem~\ref{thm:main}.

\medskip
\noindent
{\em Remark 3.} 
Theorem~\ref{thm:main} can be stated in an alternative form that
counts the number of arithmetic operations (addition, subtraction, 
multiplication, and exact division of integers) performed by the 
algorithm on the inputs  to obtain . 
This form is obtained by simply removing the constant  from 
the bound in Theorem~\ref{thm:main}. 

Finally, we show that one can improve upon Theorem~\ref{thm:main}
via case by case analysis. Here our intent is to pursue only the 
cases  and leave the task of generalizing from here to
further work. 

When considering specific values of , it is convenient to 
measure efficiency using the number of arithmetic operations 
(addition, subtraction, multiplication, and exact division of 
integers) performed by an algorithm. 

\begin{Thm}
\label{thm:q234}
There exist algorithms that solve the weighted disjoint triples 
problem 
\begin{enumerate}
\item
for  in  arithmetic operations,
\item
for  in  arithmetic operations, and
\item
for  in  arithmetic operations.
\end{enumerate}
\end{Thm}

{\em Remark.} In the case  we observe that the three
algorithms in Theorem~\ref{thm:q234} all run in  
arithmetic operations, which is linear in the size of the input.

\subsection{Counting thin subgraphs and packings}

Once Theorem~\ref{thm:main} is available, the following theorem is an 
almost immediate corollary of techniques for counting injective 
homomorphisms of bounded-pathwidth graphs developed by 
Fomin~{\em et al.}~\cite{FLRRS12} 
(see also \S3 in Amini~{\em et al.}~\cite{AFS12}).
In what follows  is the constant in \eqref{eq:tau}. 

\begin{Thm}[Fast counting of thin subgraphs]
\label{thm:subgraph}
Let  be a fixed pattern graph with  vertices and pathwidth . 
Then, there exists an algorithm that takes as input an -vertex
host graph  and counts the number of subgraphs of  that are 
isomorphic to  in time 

where  is a constant independent of the constants .
\end{Thm}

\noindent
{\em Remark.} The running time in Theorem~\ref{thm:subgraph} 
simplifies to  if .

\medskip
Theorem~\ref{thm:main} gives also an immediate speedup for counting set 
packings. In this case we use standard dynamic programming to count, 
for each -subset  with , the number of -tuples of 
pairwise disjoint -subsets whose union is . 
We then use Theorem~\ref{thm:main} to 
assemble the number of -tuples of pairwise disjoint -subsets
from triples of such -subsets. This results in the following corollary.

\begin{Thm}[Fast counting of set packings]
\label{thm:packings}
There exists an algorithm that takes as input 
a family  of -element subsets of an -element 
set and an integer  that is divisible by , 
and counts the number of -tuples of pairwise disjoint subsets
from  in time 
where  is a constant independent of the constants .
\end{Thm}

\subsection{On the hardness of counting in disjoint parts}

We present two results that provide partial
justification why there was an apparent barrier 
at ``meet-in-the-middle time'' for counting in disjoint parts. 

First, in the case of two disjoint parts, the problem appears to
contain no algebraic dependency that one could expoit towards
faster algorithms beyond those already presented in 
Bj\"orklund {\em et al.}~\cite{BHKK08,BHKK09}. Indeed, we can
provide some support towards this intuition by showing that
the associated 2-tensor has full rank over the rationals, see Lemma~\ref{lem:disjmat}.
This observation is most likely not new but we were unable to find the right reference.

Second, recall that our algorithms mentioned in the previous section use fast matrix multiplication. 
We show an argument that this is necessary to go below the meet-in-the-middle barrier.
More precisely, we show that any {\em trilinear algorithm} (cf.~\cite[\S9]{Pan1984})
for  whose rank over the integers is below the 
meet-in-the-middle barrier implies a sub-cubic algorithm for matrix 
multiplication:

\begin{Thm}
\label{thm:omega-tau}
Suppose that for all constants  there exists
a trilinear algorithm for  with 
rank  over the integers,
where  and  are constants 
independent of  and . 
Then, .
\end{Thm}

\subsection{Overview of techniques and discussion}

The main idea underlying Theorem~\ref{thm:main} is to design
a system of linear equations whose solution contains the weighted 
disjoint triples \eqref{eq:main} as one indeterminate. The main obstacle
to such a design is of course that we must be able to construct 
and solve the system within the allocated time budget.

In our case the design will essentially be a balance between
two families of linear equations, the {\em basic} (first) family and
the {\em cheap} (second) family, for the same indeterminates. The basic 
equations alone suffice to solve 
the system in meet-in-the-middle time , whereas
the cheap equations solve directly for selected indeterminates 
{\em other than} \eqref{eq:main}. The virtue of the cheap equations 
is that their right-hand sides can be evaluated efficiently using 
fast (rectangular) matrix multiplication, which enables us to throw 
away the most expensive of the basic equations and still have sufficient 
equations to solve for \eqref{eq:main}, thereby breaking the 
meet-in-the-middle barrier. Alternatively one can view the 
extra indeterminates and linear equations as a tool to expand
the scope of our techniques beyond the extent of the apparent 
barrier so that it can be circumvented.

Before we proceed to outline the design in more detail, let us observe
that the general ingredients outlined above, namely fast matrix multiplication
and linear equations, are well-known techniques employed in a number of
earlier studies. In particular in the context of subgraph counting 
such techniques can be traced back at least to the triangle- and 
cycle-counting algorithms of Itai and Rodeh \cite{IR78}, with more recent 
uses including the algorithms of Kowaluk, Lingas, and Lundell~\cite{KLL}
that improve upon algorithms of 
Ne\v{s}et\v{r}il and Poljak \cite{NP85}
and 
Vassilevska and Williams \cite{VW09} 
for counting small dense subgraphs 
() with a maximum independent set of size . 
Also the counting-in-halves technique of 
Bj\"orklund {\em et al.}~\cite{BHKK09} can be seen to solve an 
(implicit) system of linear equations to recover weighted disjoint 
packings.

Let us now proceed to more detailed design considerations.
Here the main task is to relax \eqref{eq:main} 
into a collection of trilinear forms related by linear constraints.
A natural framework for relaxation is to parameterize the triples 
 so that the pairwise disjoint triples required by \eqref{eq:main} 
become an extremal case. 

A first attempt at such parameterization is to parameterize the triples 
 by the size of the union . In particular,
the triple  is pairwise disjoint if and only if .
With this parameterization we obtain  indeterminates, one for each 
value of . In this case {\em inclusion-sieving} 
(trimmed M\"obius inversion \cite{BHKK09,BHKK10}) 
on the subset lattice  enables a system of linear
equations on the indeterminates. This is in fact the approach underlying 
the counting-in-halves technique of Bj\"orklund {\em et al.}~\cite{BHKK09},
which generalizes also to M\"obius algebras of lattices with 
the set union (set intersection) replaced by the join (meet) operation of 
the lattice~\cite{BHKKNP12}. Unfortunately, it appears difficult to break 
the meet-in-the-middle barrier via this parameterization, in particular 
due to an apparent difficulty of arriving at a cheap system of equations 
to complement the basic equations arising from the inclusion sieve.

A second attempt at parameterization is to replace the set union  
with the {\em symmetric difference} 
 and parameterize
the triples  by the size of the symmetric difference 
. The set  is illustrated
in Fig.~\ref{fig:type}.

\begin{figure}[ht]
\begin{center}
\def\svgwidth{4cm} 
\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\ifx\svgwidth\undefined \setlength{\unitlength}{180.771875bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,0.82351743)\put(0,0){\includegraphics[width=\unitlength]{type.pdf}}\put(-0.00315487,0.73180796){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\put(0.84653482,0.73180796){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\put(0.66951614,0.01488227){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\end{picture}\endgroup  \end{center}
\vspace*{-4mm}
\caption{The set .}
\label{fig:type}
\end{figure}

Recalling that  and  coincide if and only if  and  are disjoint, we again 
recover the pairwise disjoint triples as the extremal case . 
With this parameterization we obtain  indeterminates,
one for each  such that .
In this case {\em parity-sieving} 
(trimmed ``parity-of-intersection transforms'', 
see~\S\ref{sect:parity-trans}) on the group algebra of the 
elementary Abelian group  enables a system of linear 
equations on the indeterminates. While this second parameterization via 
symmetric difference is {\em a priori} less natural than the 
first parameterization via set union, it turns out to be more successful 
in breaking the meet-in-the-middle barrier. In particular the basic 
equations (Lemma~\ref{lem:first}) on the  indeterminates
alone suffice to obtain an algorithm with running time ,
which is precisely at the meet-in-the-middle barrier.
The key insight then to break the barrier is that the indeterminates with 
{\em small} values of  can be solved directly (Lemma~\ref{lem:second}) via 
fast rectangular matrix multiplication. In particular this is because small 
 implies large overlap between the sets  and a 
``triangle-like'' structure that is amenable to matrix multiplication 
techniques. That is, from the perspective of the symmetric difference 
, it suffices to control the differences  and 
 (outer dimensions in matrix multiplication), whereas the 
overlap  (inner dimension) is free to range across sets disjoint 
from  (see~\S\ref{sect:sdp}). 

A further design constraint is that the basic equations (Lemma~\ref{lem:first})
must be mutually independent of the cheap equations (Lemma~\ref{lem:second})
to enable balancing the running time (see~\S\ref{sect:runtime}) 
while retaining invertibility of the system; here we have opted for an 
analytically convenient design where the basic equations are in general 
position (the coefficient matrix is a Vandermonde matrix) that enables easy 
combination with the cheap equations, even though this design may not be 
the most efficient possible from a computational perspective. 

From an efficiency perspective we can in fact do better than 
Theorem~\ref{thm:main} for small values of  by proceeding 
via case by case analysis (see~\S\ref{sect:q234}). 
We show that faster algorithms exist for at least  
(Theorem~\ref{thm:q234}). 

An open problem that thus remains is whether the upper bound 
 in Theorem~\ref{thm:main}
can be improved to the asymptotic form 
 for some constant 
independent of . In particular, such an improvement would
parallel the asymptotic running time  of the
counting-in-halves technique~\cite{BHKK09}. Furthermore, such an
improvement would be of considerable interest since it would, for example, 
lead to faster algorithms for computing the permanent of an 
integer matrix. Unfortunately, this also suggests that such an improvement
is unlikely, or at least difficult to obtain given the relatively 
modest progress in improved algorithms for the permament 
problem~\cite{Bjorklund12}. Some further evidence towards
the subtlety of counting in disjoint parts is that we can show
(Theorem~\ref{thm:omega-tau}) that to break the ``meet-in-the-middle'' 
barrier for the weighted disjoint triples problem with a trilinear 
algorithm, it is in fact {\em necessary} to use fast matrix multiplication 
(see~\S\ref{sect:lower-bounds}). Put otherwise, the proofs of
Theorems~\ref{thm:main} and \ref{thm:omega-tau} reveal that 
for constant  the structural tensors for 
weighted disjoint triples and matrix multiplication are
loosely rank-equivalent in terms of existence of low-rank decompositions.

\subsection{Organization}

The proof of Theorem~\ref{thm:main} is split into two parts.
First, in \S\ref{sect:system} we derive a linear system whose
solution contains . Then, in \S\ref{sect:solving}
we derive an algorithm that constructs and solves the system within the claimed
running time bound. We then proceed with the two highlighted
applications of Theorem~\ref{thm:main}: 
in \S\ref{sect:homomorphims-in-three-parts}
we give a proof of Theorem~\ref{thm:subgraph} by relying on techniques
of Fomin~{\em et al.}~\cite{FLRRS12}, and 
in \S\ref{sect:packings-in-three-parts} we prove Theorem~\ref{thm:packings}.
We conclude the paper in \S\ref{sect:lower-bounds} by connecting
fast trilinear algorithms for  to fast matrix multiplication.


\section{The linear system}
\label{sect:system}

We now proceed to derive a linear system whose solution contains
. Towards this end it is convenient to start by
recalling some elementary properties of the symmetric difference 
operator on sets.

For sets , let us write 
 for the symmetric 
difference of  and . We immediately observe that 

and hence

In particular, for any  we have 

Thus, the size  is always even if  is even 
and always odd if  is odd. In both cases 
may assume exactly  values in


We are now ready to define the  linear system. 
We start with the indeterminates of the system.

\subsection{The indeterminates}
For each , let 

In particular, since  are pairwise 
disjoint if and only if , 
we observe that . 
Thus, it suffices to solve for the indeterminate  
to recover \eqref{eq:main}. We proceed to formulate a linear
system towards this end. The system is based on two families
of equations. The first family will contribute  equations,
and the second family will contribute  equations. 

\subsection{A first family of equations}
Our first family of equations is based on a parity construction.
For now we will be content in simply defining the equations
and providing an illustration in Fig.~\ref{fig:triple-intersect}.
(The eventual algorithmic serendipity of this construction will be
revealed only later in \eqref{eq:xor-cap} and \eqref{eq:tpfgh}.)
Let  be an index for the equations,
let  denote parity, and let .
For all  let


\begin{figure}[ht]
\begin{center}
\def\svgwidth{6cm} 
\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\ifx\svgwidth\undefined \setlength{\unitlength}{263.4359375bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,0.61316631)\put(0,0){\includegraphics[width=\unitlength]{triple-intersect-bw.pdf}}\put(0.70985,0.22886132){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\put(0.58837834,0.01021234){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\put(0.89813107,0.53254046){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\put(0.13285963,0.22886132){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{
}}}\end{picture}\endgroup  \end{center} 
\vspace*{-4mm}
\caption{The set  (grey and dotted) from the definition of .}
\label{fig:triple-intersect}
\end{figure}

The right-hand sides of the first system are now defined by

Let us recall that the universe  has  elements.
For nonnegative integers  and , let us define the
Vandermonde matrix  by setting 

{\em Remark.} We recall from basic linear algebra that 
any  submatrix of a  Vandermonde matrix 
with entries  for  and  
has nonzero determinant if the values  are pairwise distinct. 
This makes a Vandermonde matrix particularly well-suited for 
building systems of independent equations from multiple families of equations.

\begin{Lem}[First family]
\label{lem:first}
For all  it holds that 

\end{Lem}
\begin{proof}
Let us fix a triple  with 
. From \eqref{eq:jq} we have .
Let us write  for the
number of -tuples  
such that 

From \eqref{eq:xj}, \eqref{eq:tp}, and \eqref{eq:yi} we observe 
that the lemma is implied by

Indeed,  and  are the coefficients before  in the LHS and RHS of~\eqref{eq:first-family}, respectively.
To prove~\eqref{eq:vm}, we proceed by induction on . The base case  is set up by
observing

For , let us study what happens if we extend
an arbitrary -tuple 
by a new element . We observe that we have exactly  
choices for the value  among the elements of  outside 
 and  choices inside . 
The parity \eqref{eq:u-parity} changes if and only if 
we choose an element inside .
Thus, for  we have

From \eqref{eq:m-base} and \eqref{eq:m-step} we thus have

Hence,  and from \eqref{eq:v} we conclude that the
lemma holds.

\end{proof}

\subsection{A second family of equations}

Our second family of equations is based on solving for the 
indeterminates \eqref{eq:xj} directly. We state the following
lemma in a general form, but for performance reasons we will 
in fact later use only the equations indexed by the  smallest 
values  in our linear system.

\begin{Lem}[Second family]
\label{lem:second}
For all  it holds that

\end{Lem}
\begin{proof}
We must show that the right-hand side of \eqref{eq:xj-direct} equals
\eqref{eq:xj}. Let us study a triple  
with . We observe that  
because otherwise taking the symmetric difference with  will either 
leave too many elements uncanceled or it cannot cancel enough of 
the elements in . Since  from \eqref{eq:xor-size} 
it follows that  is in fact always even. 
Furthermore, when  we observe that 

The lemma follows by solving for  and observing that
each triple  uniquely determines .
\end{proof}

\subsection{The linear system}
We are now ready to combine equations from the two families to a system 

of independent linear equations for the indeterminates 
. Recalling \eqref{eq:jq}, there are exactly
 indeterminates, and hence exactly 
independent equations are required.

Let us use a parameter  
in building the system. (The precise value of  will be 
determined later in \S\ref{sect:runtime}.)

We now select  equations 
from the first family (Lemma~\ref{lem:yi}), and  equations
from the second family (Lemma~\ref{lem:xj-direct}).
More precisely, we access the first family for  
equations indexed by , and the second family 
for the  equations indexed by the smallest  values . 
That is, if  is even, we use equations indexed by 
, and if  is odd, we use 
equations indexed by .
Thus, for all  we conclude that 

and that 


Let us now verify that the selected system consists of independent 
equations. To verify this it suffices to solve the system.
The equations from the second family (Lemma~\ref{lem:xj-direct}) 
by construction solve directly for  indeterminates. 
We are thus left with  equations from the first family
(Lemma~\ref{lem:yi}). Now observe that since we know
the values of  indeterminates, we can subtract their
contribution from both sides of the remaining equations, leaving
us  equations over  indeterminates. In fact (see the remark 
before Lemma~\ref{lem:yi}), the coefficient matrix of the remaining
system is a  submatrix of the original Vandermonde matrix,
and hence invertible. We conclude that the equations are independent.

It remains to argue that the system \eqref{eq:sys} can be constructed
and solved within the claimed running time.

\section{Efficient construction and solution}
\label{sect:solving}

This section proves Theorem~\ref{thm:main} by constructing and
solving the system derived in \S\ref{sect:system} within
the claimed running time. We start with some useful subroutines 
that enable us to efficiently construct the right-hand sides 
for \eqref{eq:yi} and \eqref{eq:xj-direct}.

\subsection{The intersection transform}

Let  and  be nonnegative integers.
For a function , define 
the {\em intersection transform}  of 
for all  by


The following lemma is an immediate corollary of a theorem of Bj\"orklund~{\em et~al.}~\cite[Theorem 1]{BHKK08}.

\begin{Lem}
\label{lem:fit}
There exists an algorithm that evaluates all the  values of the intersection transform
for all  in time  for a 
constant  independent of constants  and . 
\end{Lem}

\noindent
{\em Remark.} 
Lemma~\ref{lem:fit} can be stated in an alternative form that counts 
the number of arithmetic operations 
(addition, subtraction, multiplication, and exact division of integers) 
performed by the algorithm on the input  to obtain  for all 
. This form is obtained by simply removing the constant 
 from the bound in Lemma~\ref{lem:fit}.
(Indeed, we can use Bareiss's algorithm~\cite{B68} to solve the 
underlying linear system with exact divisions.)


\subsection{The parity transform}
\label{sect:parity-trans}

Let  be a nonnegative integer and let . 
For a function , define 
the {\em parity transform}  of 
for all  by


\begin{Lem}
\label{lem:fpt}
There exists an algorithm that evaluates the parity transform
for  in time  for a 
constant  independent of constants  and . 
\end{Lem}
\begin{proof}
We observe that 

and apply Lemma~\ref{lem:fit}.
\end{proof}

\subsection{Evaluating the right-hand side of the first family}
Let  be a nonnegative integer. Our objective is to evaluate 
the right-hand side of \eqref{eq:yi}.
Let us start by observing that it suffices to compute the values
\eqref{eq:tp} for all  with . 

The following lemma will be useful towards this end.
Denote by  the number of tuples  with 
 and .

\begin{Lem}
\label{lem:lnis}
We have
2.5mm]
(n-s+1)L_n(i-1,s-1)& \\
\quad\ +(s+1)L_n(i-1,s+1) & \text{if }.\\
\end{cases}

\label{eq:yi-tp}
y_i=\sum_{s=0}^i \tbinom{n}{s}^{-1}L_n(i,s)\sum_{Z\in\binom{U}{s}}T_0(Z)-T_1(Z)\,.

\label{eq:xor-cap}
\begin{split} 
\bigl|(A\oplus B\oplus C)\cap Z\bigr|
&= 
|(A \cap Z) \oplus(B \cap Z) \oplus(C \cap Z)| \\
&\equiv
|A\cap Z|
+|B\cap Z|
+|C\cap Z|\pmod 2\,,
\end{split}

\label{eq:tpfgh}
\begin{split}
T_p
=f\pi_p\cdot g\pi_p\cdot h\pi_p
+f\pi_{\bar p}\cdot g\pi_{\bar p}\cdot h\pi_p
+f\pi_{\bar p}\cdot g\pi_p\cdot h\pi_{\bar p}
+f\pi_p\cdot g\pi_{\bar p}\cdot h\pi_{\bar p}\,.
\end{split}

\label{eq:f-oplus-g}
(f\oplus g)(D)=
\!\!\sum_{\substack{A,B\in\binom{U}{q}\\A\oplus B=D}}\!\!f(A)g(B)\,.

\label{eq:F}
F(I,K)=
\begin{cases}
f(I\cup K) & \text{if };\\
0          & \text{otherwise}.
\end{cases}

\label{eq:G}
G(K,J)=
\begin{cases}
g(K\cup J) & \text{if };\\
0          & \text{otherwise}.
\end{cases}

\label{eq:from-fg}
(f\oplus g)(D)=\sum_{I\in\binom{D}{\ell/2}}FG\bigl(I,D\setminus I\bigr)\,.
\medskipamount]
(M2) &  arithmetic operations 
by decomposing the product into a sum of  products 
of  square matrices when .
\end{tabular}
\medskip

\noindent
{\em Remark.} The bounds above are not the best possible \cite{LG12}; 
however, to provide a clean exposition we will work with these 
somewhat sub-state-of-the-art bounds.

\begin{Lem}
\label{lem:fsdp}
There exists an algorithm that for 
evaluates the symmetric difference product \eqref{eq:f-oplus-g} 
for all even  in time 
 
for a constant  independent of constants  and . 
\end{Lem}
\begin{proof}
For convenience we may pad  and  with all-zero rows and columns 
so that  becomes an  matrix
and  becomes an  matrix.

For  by (M2) we can thus multiply  and  using
 arithmetic 
operations and hence in time . 

For  by (M1) we can thus multiply  and  using 

arithmetic operations.
For  the linear function 

has its maximum at  or at . 
Noting that , at  we
obtain the bound  for the running time.
At  we obtain the running time bound 
 
Again noting that , this bound simplifies to 
 
It remains to analyze the quantity . 
Towards this end, recall that  and 
, with  if and only 
if . The lemma now follows by observing that 

\end{proof}



\subsection{Evaluating the right-hand side of the second family}

Let  be a nonnegative integer. Our objective is to evaluate 
the right-hand side of \eqref{eq:xj-direct}. Let us start
by observing that \eqref{eq:xj-direct} can be stated using
the symmetric difference product \eqref{eq:f-oplus-g} 
and the intersection transform \eqref{eq:ft} in equivalent
form


\begin{Lem}
\label{lem:xj-direct}
There exists an algorithm that for 
evaluates the right-hand-side of \eqref{eq:xj-direct}
for all  in time 
 
for a constant  independent of constants  and . 
\end{Lem}
\begin{proof}
Because  we observe that
 in \eqref{eq:xj-direct-2}. 
Using Lemma~\ref{lem:fsdp} we can evaluate  for
all required  within the claimed time bound. 
Using Lemma~\ref{lem:fit} with 
 we can evaluate  for all 
 in  time which is within the claimed time bound. Finally,
the sums in \eqref{eq:xj-direct-2} can be computed
in the claimed time bound by using the evaluations 
and .
\end{proof}


\subsection{Running-time analysis}
\label{sect:runtime}

We now balance the running times from Lemma~\ref{lem:yi} 
and Lemma~\ref{lem:xj-direct} by selecting the value of 
. Disregarding the constant  which
is independent of  and , the contribution
of Lemma~\ref{lem:yi} is  
and the contribution of Lemma~\ref{lem:xj-direct} is 

In particular, we must minimize the maximum of the three contributions

We claim that if  then the maximum is controlled by

Let us select the value of  given by \eqref{eq:gamma-bal}.
Recalling that , we have

In \eqref{eq:gamma} we have  if and only if .
In particular, we have  if , implying

and thus \eqref{eq:gamma} and \eqref{eq:gamma-bal} determine the maximum
as claimed. In this case we can achieve running time

Conversely, if  then the maximum is controlled by

in which case we select  and achieve running time


Since the system \eqref{eq:sys} and its solution \eqref{eq:xj} are 
integer-valued and have bit-length bounded by a polynomial in 
that is independent of the constant , for example Bareiss's
algorithm~\cite{B68} solves the constructed system in 
the claimed running time.
This completes the proof of Theorem~\ref{thm:main}.


\subsection{Speedup for .}
\label{sect:q234}

In this section we prove Theorem~\ref{thm:q234}. We split the proof
into three parts.

\begin{proof}[Proof ().]
Let us study the first family of equations (Lemma~\ref{lem:first}). 
For  we have indeterminates  and
equations indexed by , where equation  can be 
constructed in  arithmetic operations;
cf.~Lemma~\ref{lem:fit} to Lemma~\ref{lem:yi}.
Thus, it suffices to replace the equation for  with an 
equation independent of the equations  to solve
for all the indeterminates, and in particular for , which
gives the weighted disjoint triples. Our strategy is to solve 
directly for the indeterminate . We observe that  requires
to sum over all triples  of -subsets such that
the -uniform hypergraph  has no vertices of 
odd degree. 
Up to isomorphism the only such hypergraph
for  is the triangle.
From now on we abuse the notation slightly and extend the domains of the functions ,  and  to sets of size {\em at most } so that they evaluate to  for sets of size strictly smaller than .
Accordingly, we have


where we can evaluate the inner sum simultaneously for all  
by multiplying two  matrices using 
arithmetic operations.
\end{proof}

\begin{proof}[Proof ().]
Let us imitate the proof for .
For  our indeterminates are , and the
equations are indexed by . Again it suffices to replace
the  equation. We will do this by solving directly for
the indeterminate . For  there are, up to isomorphism, 
exactly two -uniform hypergraphs  with a unique vertex 
of odd degree. 
In Type I hypergraphs  is of degree  and in Type II hypergraphs  is of degree . 
Let  and  denote the contribution to  of triples  corresponding to Type I and Type II hypergraphs, respectively.
Then .

Note that for every Type I hypergraph the hypergraph  is 2-uniform and has no odd vertices.
Hence the contribution  of Type I triples such that  can be computed in time  by applying the formula~\eqref{eq:2-uniform-x0} to functions , where ,  and . (Note that we use here the fact that ,  and  evaluate to  for sets with less than 3 elements.)
Since 
the value  can be computed in  time.

\begin{figure}[t]

\caption{\label{fig:3-uniform}Two nonisomorphic types of 3-uniform hypergraphs with one vertex of odd degree.
We display these hypergraphs below as incidence matrices
where the rows correspond to hyperedges and the columns correspond
to vertices, with a 1-entry indicating indidence and a 0-entry 
indicating non-indidence between a hyperedge and a vertex.
Vertical and horizontal lines to partition the vertices and 
the hyperedges to orbits with respect to the action of the automorphism 
group of the hypergraph.}
\end{figure}

Now consider Type II hypergraphs. 
Let us further partition Type II triples  according to which of the sets , ,  contains .
Let  denote the contribution to  of Type II triples where .
Note that then  and  are the contributions of Type II triples where  and , respectively, and hence 


Let us focus on , i.e.\ assume that  for some .
Since  and  are of degree , either both are in one of the remaining sets, say , or each of ,  is in exactly one of  and .
However we can assume the latter, because in the former  has at least two degree  vertices.
So let  be the vertex of  and let  be the vertex of .
Since the remaining vertices in  are of degree 2, there are exactly two of them, say,  and , and , see Fig~\ref{fig:3-uniform}. It follows that 


Note that it is sufficient to assume only ; indeed, since ,  and  evaluate to  for sets with less than 3 elements any choice of , , , ,  which satisfies this assumption but  produces a zero term in the sum.
Here the sum  can be evaluated with an  by 
 rectangular matrix multiplication in 
arithmetic operations; cf.~(M2). 
Hence it takes  time to compute , since we have shown that  can also be computed within this time bound.
\end{proof}

\begin{proof}[Proof ().]
Let us imitate the proof for .
For  our indeterminates are , 
and the equations are indexed by . 
It suffices to replace the  and  equations. 
We will do this by solving directly for
the indeterminates  and , that is, the cases  and 
for .

{\em The case .}
For  there is, up to isomorphism, a unique 
-uniform hypergraph  with no vertex of odd degree:

Accordingly, we have

where we can evaluate the inner sum simultaneously for all  
by multiplying two  matrices. This takes 
arithmetic operations.



{\em The case .}
For  we will show that there are, up to isomorphism, exactly four 
-uniform hypergraphs  with exactly two
vertices  of odd degree (see Fig~\ref{fig:4-uniform-j=2}).
For  let  denote the contribution to  of triples  such that the corresponding hypergraph is of type .

\begin{figure}[t]

\caption{\label{fig:4-uniform-j=2}Four nonisomorphic -uniform hypergraphs with exactly two vertices of odd degree.}
\end{figure}


In hypergraphs of Type I both odd degree vertices  and  are of degree 3.
Then  is 2-uniform and has no odd vertices.
Hence the contribution  of Type I triples such that both  and  are of degree  can be computed in time  by applying the formula~\eqref{eq:2-uniform-x0} to functions , where ,  and . 
Since 
the value  can be computed in  time.

In hypergraphs of Type II there is one vertex  of degree  and one vertex  of degree .
Then  is 3-uniform and has exactly one odd degree vertex, in fact a degree  vertex.
Hence the contribution  of Type II triples such that  is of degree  can be computed in time  by applying the formula~\eqref{eq:3-uniform-type-II} to functions , where ,  and . 
Since 
the value  can be computed in  time.
Note also that by the same reasoning the contribution  of Type II triples  such that the degree 1 vertex belongs to  is equal to , hence it also can be computed in  time. (We will use this quantity while computing  and )

In the remaining hypergraphs both  and  are of degree , but we have two nonisomorphic types of such hypergraphs.
In Type III hypergraphs both  and  are in the same hyperedge.
Let  denote the contribution to  of Type III triples where .
Note that then  and  are the contributions of Type III triples where  and , respectively, and hence 

We focus on , i.e.\ we assume  for some vertices  and  such that .
Then since the remaining vertices are all of degree , the remaining sets in the triple are  and  for some three different vertices , ,  outside . Then,


where  is an overcount which we specify in a moment.
Note that it is sufficient to assume only ; indeed, since ,  and  evaluate to  for sets with less than 4 elements any choice of , , , , , ,  which satisfies this assumption but  produces a zero term in the sum.
Observe also that the sum  can be evaluated for each  
with an  by  rectangular matrix multiplication
in  arithmetic operations; cf.~(M2).
Once these values are tabulated for every , the sum  can be evaluated in  time.
It remains to compute the value of overcount  efficiently.
This can be split as , where  and  denote the contribuitions of the terms where  and , respectively.

Let us focus on . In these triples either  or , but not both (recall that ).
Since , , and  are symmetric, we can assume that either  or .
In other words either ,  and  or ,  and .
Equivalently, we can drop the assumption  and just consider all triples ,  and . 
It follows that



Observe that  is equal to the contribution of Type II triples  such that the unique degree  vertex is in , i.e.\ 
, and we know how to compute this value in time .

Now consider . In these triples .
Since ,  and  are symmetric, we can assume .
It means that we count triples of the form , , and , for every five different vertices  and .
It follows that , which is computable in  time.

Finally we focus on Type IV triples, where there are two degree  vertices  and , the remaining vertices are of degree 2 and  and  are in different hyperedges. Let  denote the contribution to  of Type IV triples where .
Note that then  and  are the contributions of Type IV triples where  and , respectively, and hence 

We focus on , i.e.\ we can assume  and . 
Then  for some four different vertices , , ,  diffrent from  and . Since all vertices except for  and  are of degree 2 from the handshaking lemma the number of vertices  in the hypergraph satisfies , and hence , i.e.\ there is exactly one vertex more, call it .
Since  is of degree 2, . Since all vertices in  are of degree 2, two of them, say  are in , and the other two, say  in . Then,


where  is an overcount which we specify in a moment.
Note that by similar arguments as before, it is sufficient to assume only .
Observe also that the sum  can be evaluated for each  
by multiplying two  matrices in  arithmetic operations.
Moreover, for every  we compute and store the sum ; this takes time  . 
By noting that  it follows that once all the values of  and  are computed the sum  can be evaluated in  time.
It remains to compute the value of overcount  efficiently.

Observe that  is equal to the co ntribution of Type IV triples considered above where , i.e.\ the triples  such that ,  and . These are exactly the Type II triples such that the degree  vertex is in .
Hence, , and we know how to compute this value in time .
This finishes the proof of Theorem~\ref{thm:q234}.
\end{proof}



\section{Counting thin subgraphs in three parts}
\label{sect:homomorphims-in-three-parts}

This section proves Theorem~\ref{thm:subgraph} by relying on the techniques 
in \S4 of Fomin~{\em et al.}~\cite{FLRRS12} and invoking our 
Theorem~\ref{thm:main} as a subroutine that enables 
fast counting of injective homomorphisms in three parts.
Whereas Fomin~{\em et al.}~\cite{FLRRS12}
use the path decomposition to split  into two halves of size roughly 
 joined by a separator of size at most , we split  into 
a sequence of three parts of size roughly  joined by two separators 
of size at most . Accordingly, the following lemma is an immediate analog 
of Proposition 2 in Fomin~{\em et al.}~\cite{FLRRS12} (cf.~\cite{K92}).

\begin{Lem}
Let  be a graph with  vertices and pathwidth . Then, we can partition
the vertices of  into five pairwise disjoint sets  such that
(i) , (ii) , and (iii) every edge of 
joins vertices in one or two consecutive sets in the sequence . 
\end{Lem}

Imitating the design in \S4 of Fomin~{\em et al.}~\cite{FLRRS12},
we now iterate over all possible 
guesses  how an injective homomorphism from  to  can map 
the elements of the disjoint sets  and  to . For each such 
guess , we use the algorithm in Lemma 2 
of Fomin~{\em et al.}~\cite{FLRRS12} to compute 
for each  
with  the following three quantities:
(a) the number  of injective homomorphisms 
from  to  that extend , 
(b) the number  of injective homomorphisms 
from  to  that extend , 
and
(c) the number  of injective homomorphisms 
from  to  that extend .
This takes  time for a constant  independent
of the constants  and ; in particular the running-time bottleneck 
occurs with the functions  where we run an -time
algorithm of D{\'{\i}}az~{\em et al.}~\cite{DST02} to compute the number
of homomorphisms from  to 
that extend  for each of the  possibilities for 
.
 
Using the algorithm in Theorem~\ref{thm:main} for each guess , 
we obtain the number of injective homomorphisms from  to  as 
 in time
. Dividing by the number
of automorphisms of , we obtain the number of subgraphs isomorphic 
to  in  (cf.~\cite[Theorem~2]{FLRRS12}). This completes the proof of 
Theorem~\ref{thm:subgraph}. 



\section{Counting set packings in three parts}
\label{sect:packings-in-three-parts}

This section proves Theorem~\ref{thm:packings}. 
Let  be the -element universe and let 
 be a set of -element subsets of 
given as input. A further input is the integer .
Our task is to count the number of 
-tuples  that
are pairwise disjoint, that is,  holds for all 
.

The structure of the proof is to rely on standard dynamic programming 
techniques to execute the count for pairwise disjoint -tuples, and 
then invoke the weighted disjoint triples algorithm (Theorem~\ref{thm:main}) 
to arrive at the desired count. Let us now proceed with the details.

We start by defining a sequence of functions 
 that we will
then compute using dynamic programming.
For  and all , let
 be the number of -tuples 
 that (a) are pairwise disjoint,
and (b) satisfy .

To set up a base case for the dynamic programming, we observe 
that  is the indicator function 
for the subsets in . That is,  if and only if
, and  otherwise. Since 
 and  is a constant independent of , 
we have that  can be computed in time .
Next, suppose that we have computed  and want to
compute . For each , we use the 
following recurrence:

To see that the recurrence is correct, observe that 
for every 
that is pairwise disjoint with 
there is a unique  such that

is pairwise disjoint with 
, namely .
In particular, the left-hand side and the right-hand side of the 
recurrence count the same -tuples. 

To obtain the running time of the recurrence, observe that we
iterate over all  and then over all
, checking for each  whether .
Since both  and  are constants independent of , 
also  is a constant independent of , and the running 
time bound becomes .
In particular, to compute the function  using the recurrence 
thus takes  time.

We will now apply Theorem~\ref{thm:main}. Let us take
 and compute  using
the algorithm in Theorem~\ref{thm:main}. This will take
, which is exactly
the claimed running time. 

To complete the proof of Theorem~\ref{thm:packings}, we observe 
that 
is exactly the number of -tuples of pairwise disjoint subsets
from , multiplied by the multinomial coefficient
. Indeed, each -tuple 
 of pairwise disjoint sets 
is counted in  exactly 
 times, once for each possible way of 
partitioning the index set  into a three-tuple 
 with  such that 
, , 
and ; cf.~\eqref{eq:main}. 
Thus, 
is the count we want. This completes the proof of Theorem~\ref{thm:packings}.

\section{On the hardness of counting in disjoint parts}
\label{sect:lower-bounds}

This section presents two results that provide partial
justification why there was an apparent barrier 
at ``meet-in-the-middle time'' for counting in disjoint parts. 

First, in the case of two disjoint parts, the problem appears to
contain no algebraic dependency that one could expoit towards
faster algorithms beyond those already presented in 
Bj\"orklund {\em et al.}~\cite{BHKK08,BHKK09}. Indeed, we can
provide some support towards this intuition by recalling that
the associated 2-tensor has full rank over the rationals
(Lemma~\ref{lem:disjmat}).

Second, in the case of three disjoint parts, we have already
witnessed (in the proof of Theorem~\ref{thm:main}) that the
associated 3-tensor does not have full rank, in essence
because the 3-tensor for matrix multiplication does
not have full rank. This prompts the question whether it was
{\em necessary} to rely on fast matrix multiplication 
to break the barrier. We can provide some support towards an 
affirmative answer by showing that any {\em trilinear} algorithm 
for  that breaks the barrier implies a nontrivial algorithm 
for matrix multiplication (Theorem~\ref{thm:omega-tau}).

\subsection{Disjoint pairs}
It will be convenient to use Iverson's bracket notation;
for a logical proposition  we have  if  is true and  if
 is false. The -{\em disjointness matrix} is the 
 matrix with entries 
for all , .
The following lemma is well known.

\begin{Lem}
\label{lem:disjmat}
The -disjointness matrix has full rank over the rationals.
\end{Lem}

\begin{proof}
It suffices to show that the matrix is invertible. Observe that

holds for all  when the values  
for  are the solutions to the  
linear system with equations

The coefficient matrix of this system is a lower triangular matrix
with nonzero entries on the diagonal. Thus, the system is invertible.
\end{proof}
 

\subsection{Fast disjoint triples implies fast matrix multiplication}

Let us prove Theorem~\ref{thm:omega-tau}.
For every  
we have by assumption a trilinear algorithm of rank  for 
computing  for inputs 

over an -element universe .
That is, for every  and  there exist coefficients 
 
such that

with


Let us now derive from these trilinear algorithms
bilinear algorithms for matrix multiplication.
Let us recall from \eqref{eq:xj} the indeterminates  
with . In particular, a trilinear algorithm
for  is precisely a trilinear algorithm for .

Our proof strategy is to derive a bilinear algorithm for matrix
multiplication from a trilinear algorithm for , and then
derive a trilinear algorithm for  from the first family 
of equations (\S\ref{sect:system}) and the low-rank algorithm 
for . Finally, we use recursion on the bilinear algorithm
to conclude that .

Let  and  be matrices of size . 
Without loss of generality we may assume that  is even and 
that  is divisible by 3, with . 

Partition  into three disjoint sets  of size .
Define the function  for 
 and 
by setting , and let  vanish
elsewhere.
Similarly, define the function  for 
 and 
by setting , and let  vanish
elsewhere.

Assume that we have a trilinear algorithm of rank  over the integers 
that computes . We claim that we can transform this trilinear 
algorithm into a bilinear algorithm of rank  that multiplies  and . 
Indeed (cf.~\cite[\S9]{Pan1984}), we can fix  and  in 
the trilinear equations, and for each  and 
 solve for the indeterminate  with 
 to determine the -entry of the product 
matrix .

Recalling the first family of linear equations from \S\ref{sect:system},
from the proof of Lemma~\ref{lem:yi} and the structure of 
equations \eqref{eq:yi-tp} and \eqref{eq:tpfgh} we can observe 
that the right-hand side  of the first family has trilinear
rank at most 

whenever .
Thus, using the first family we can show that the trilinear rank 
of  is small by showing that the indeterminates  for 
large values of  have low trilinear rank.

Towards this end, 
let us solve for  with  using the trilinear algorithm 
for  as a subroutine. 
That is, we iterate over all possible choices
for the intersecting part of a triple  with ,
and for each such choice use  to sum over the disjoint parts 
to accumulate .
Let us now make this more precise.
For sets  let us abbreviate  and
. 
For , the {\em intersecting part}
of the triple  is the tuple
of disjoint sets 

The {\em size} of  is 

We have that  is large if and only if 
 is small. In more precise terms, from \eqref{eq:int-size}
we have

The {\em disjoint part} of  is the tuple 
of disjoint sets

It is immediate that  and  together
uniquely determine the triple .

Now consider an arbitrary triple  with 
. We know that there is a 
unique quadruple  of disjoint sets 
with . Furthermore, from \eqref{eq:j-int}
and  it follows that .
Thus, it suffices to iterate over at most  quadruples 
 to match the intersecting part of .

So suppose we have fixed  with 
, and let .
We can now capture each  with  
by means of the disjoint part . That is, there exists a unique 
disjoint triple  of subsets  
such that .
Furthermore, from \eqref{eq:int-def} and \eqref{eq:disj-def} it is 
immediate that


Since the sets  in general have size different from
, let us introduce the following padding to obtain a valid input
for . Let  be three sets that are disjoint from each
other and , with 
,
, and
.
Let  and .

We are now ready to construct an input for .
Define three functions  for all 
 by 

By construction, we now have that to every triple  with 
 there corresponds a unique disjoint triple 
 such that 

Taking the sum over all , we have

Thus, using the trilinear algorithm for  
for each choice of , we can compute 
 for all  with a trilinear algorithm of rank

Take  so that together with equations 
from the first family \eqref{eq:yi-trilinear} we have enough equations 
to solve for . It remains to select  so that 
\eqref{eq:yi-trilinear} and \eqref{eq:xj-trilinear} are balanced.
We have that \eqref{eq:yi-trilinear} and 
\eqref{eq:xj-trilinear} are balanced when

That is, when . We thus have a trilinear algorithm 
for  that has rank  for a 
constant  independent of  and .
That is, we have a bilinear algorithm of rank 
 to multiply two  matrices
with . For any constant  we can now
obtain, by selecting a large enough , a bilinear algorithm of
rank  to multiply two  matrices.
Taking a large enough  and using recursion 
(cf.~\cite[Theorem~2.1]{Pan1984}), we conclude that 
. Since  was arbitrary,
.




\section*{Acknowledgments}

A preliminary conference abstract of this work has appeared as A. Bj\"orklund, P. Kaski, and \L. Kowalik, ``Counting thin subgraphs via packings faster than meet-in-the-middle time,'' Proceedings of the 25th ACM-SIAM Symposium on Discrete Algorithms (SODA 2014, Portland, Oregon, January 5--7, 2014), SIAM, Philadelphia, PA, 2014, pp.~594--603.
This research was supported in part by the 
Swedish Research Council, Grant VR 2012-4730 (A.B.),
the Academy of Finland, Grants 252083, 256287, and 283437 (P.K.), and 
by t	he National Science Centre of Poland, Grants N206 567140 and 2013/09/B/ST6/03136 (\L.K.).




\bibliographystyle{abbrv}
\begin{thebibliography}{10}

\bibitem{AG09}
N.~Alon and S.~Gutner.
\newblock Balanced families of perfect hash functions and their applications.
\newblock {\em ACM Transactions on Algorithms}, 6(3), 2010.

\bibitem{AFS12}
O.~Amini, F.~V. Fomin, and S.~Saurabh.
\newblock Counting subgraphs via homomorphisms.
\newblock {\em SIAM J. Discrete Math.}, 26(2):695--717, 2012.

\bibitem{B68}
E.~H. Bareiss.
\newblock Sylvester's identity and multistep integer-preserving {G}aussian
  elimination.
\newblock {\em Math. Comp.}, 22:565--578, 1968.

\bibitem{Bjorklund12}
A.~Bj\"orklund.
\newblock Below all subsets for some permutational counting problems.
\newblock {\em CoRR}, abs/1211.0391, 2012.

\bibitem{BHKK08}
A.~Bj{\"o}rklund, T.~Husfeldt, P.~Kaski, and M.~Koivisto.
\newblock The fast intersection transform with applications to counting paths.
\newblock {\em CoRR}, abs/0809.2489, 2008.

\bibitem{BHKK09}
A.~Bj{\"o}rklund, T.~Husfeldt, P.~Kaski, and M.~Koivisto.
\newblock Counting paths and packings in halves.
\newblock In A.~Fiat and P.~Sanders, editors, {\em ESA}, volume 5757 of {\em
  Lecture Notes in Computer Science}, pages 578--586. Springer, 2009.

\bibitem{BHKK10}
A.~Bj{\"o}rklund, T.~Husfeldt, P.~Kaski, and M.~Koivisto.
\newblock Trimmed {M}oebius inversion and graphs of bounded degree.
\newblock {\em Theory Comput. Syst.}, 47(3):637--654, 2010.

\bibitem{BHKKNP12}
A.~Bj{\"o}rklund, T.~Husfeldt, P.~Kaski, M.~Koivisto, J.~Nederlof, and
  P.~Parviainen.
\newblock Fast zeta transforms for lattices with few irreducibles.
\newblock In Y.~Rabani, editor, {\em SODA}, pages 1436--1444. SIAM, 2012.

\bibitem{C13}
R.~Curticapean.
\newblock Counting matchings of size  is {\#}{W}[1]-hard.
\newblock In F.~V. Fomin, R.~Freivalds, M.~Kwiatkowska, and D.~Peleg, editors,
  {\em ICALP (1)}, volume 7965 of {\em Lecture Notes in Computer Science},
  pages 352--363. Springer, 2013.

\bibitem{DST02}
J.~D{\'{\i}}az, M.~Serna, and D.~M. Thilikos.
\newblock Counting {}-colorings of partial {}-trees.
\newblock {\em Theoret. Comput. Sci.}, 281(1-2):291--309, 2002.
\newblock Selected papers in honour of Maurice Nivat.

\bibitem{EG04}
F.~Eisenbrand and F.~Grandoni.
\newblock On the complexity of fixed parameter clique and dominating set.
\newblock {\em Theoret. Comput. Sci.}, 326(1-3):57--67, 2004.

\bibitem{FG04}
J.~Flum and M.~Grohe.
\newblock The parameterized complexity of counting problems.
\newblock {\em SIAM J. Comput.}, 33(4):892--922, 2004.

\bibitem{FLRRS12}
F.~V. Fomin, D.~Lokshtanov, V.~Raman, S.~Saurabh, and B.~V.~R. Rao.
\newblock Faster algorithms for finding and counting subgraphs.
\newblock {\em J. Comput. Syst. Sci.}, 78(3):698--706, 2012.

\bibitem{GO12}
A.~Gajentaan and M.~H. Overmars.
\newblock On a class of {} problems in computational geometry.
\newblock {\em Comput. Geom.}, 45(4):140--152, 2012.

\bibitem{HS74}
E.~Horowitz and S.~Sahni.
\newblock Computing partitions with applications to the knapsack problem.
\newblock {\em J. Assoc. Comput. Mach.}, 21:277--292, 1974.

\bibitem{IR78}
A.~Itai and M.~Rodeh.
\newblock Finding a minimum circuit in a graph.
\newblock {\em SIAM J. Comput.}, 7(4):413--423, 1978.

\bibitem{K92}
N.~G. Kinnersley.
\newblock The vertex separation number of a graph equals its path-width.
\newblock {\em Inform. Process. Lett.}, 42(6):345--350, 1992.

\bibitem{KKM00}
T.~Kloks, D.~Kratsch, and H.~M{\"u}ller.
\newblock Finding and counting small induced subgraphs efficiently.
\newblock {\em Inform. Process. Lett.}, 74(3-4):115--121, 2000.

\bibitem{KW09}
I.~Koutis and R.~Williams.
\newblock Limits and applications of group algebras for parameterized problems.
\newblock In S.~Albers, A.~Marchetti-Spaccamela, Y.~Matias, S.~E. Nikoletseas,
  and W.~Thomas, editors, {\em ICALP (1)}, volume 5555 of {\em Lecture Notes in
  Computer Science}, pages 653--664. Springer, 2009.

\bibitem{KLL}
M.~Kowaluk, A.~Lingas, and E.-M. Lundell.
\newblock Counting and detecting small subgraphs via equations.
\newblock {\em SIAM J. Discrete Math.}, 27(2):892--909, 2013.

\bibitem{LG12}
F.~{Le Gall}.
\newblock Faster algorithms for rectangular matrix multiplication.
\newblock In {\em FOCS}, pages 514--523. IEEE Computer Society, 2012.

\bibitem{LG14}
F.~{Le Gall}.
\newblock Powers of tensors and fast matrix multiplication.
\newblock arXiv:1401.7714, 2014.

\bibitem{LR83}
G.~Lotti and F.~Romani.
\newblock On the asymptotic complexity of rectangular matrix multiplication.
\newblock {\em Theoret. Comput. Sci.}, 23(2):171--185, 1983.

\bibitem{NP85}
J.~Ne{\v{s}}et{\v{r}}il and S.~Poljak.
\newblock On the complexity of the subgraph problem.
\newblock {\em Comment. Math. Univ. Carolin.}, 26(2):415--419, 1985.

\bibitem{Pan1984}
V.~Pan.
\newblock How can we speed up matrix multiplication?
\newblock {\em SIAM Rev.}, 26(3):393--415, 1984.

\bibitem{VW09}
V.~Vassilevska and R.~Williams.
\newblock Finding, minimizing, and counting weighted subgraphs.
\newblock In M.~Mitzenmacher, editor, {\em STOC}, pages 455--464. ACM, 2009.

\bibitem{VW12}
V.~{Vassilevska Williams}.
\newblock Multiplying matrices faster than {C}oppersmith-{W}inograd.
\newblock In H.~J. Karloff and T.~Pitassi, editors, {\em STOC}, pages 887--898.
  ACM, 2012.

\end{thebibliography}




\end{document}  
