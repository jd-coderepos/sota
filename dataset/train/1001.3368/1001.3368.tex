\documentclass{article}
\usepackage{xspace, macros}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{linlog}
\usepackage{prooftree}
\title{Linear Recursion}
\author{Sandra Alves, Maribel Fern\'andez, 
M\'{a}rio Florido, and Ian Mackie  \\\\
 University of Porto, Faculty of Science \& LIACC,\\ 
R. do Campo Alegre 1021/55, 4169-007, Porto, Portugal\\
 King's College London, Department of Informatics\\  
Strand, London WC2R~2LS, U.K. \\
 LIX, CNRS UMR 7161, \'Ecole Polytechnique\\ 
91128 Palaiseau Cedex, France
}
\date{}

\begin{document}
\maketitle
\bibliographystyle{abbrv}

\begin{abstract}
We define two extensions of the typed linear lambda-calculus that
yield minimal Turing-complete systems. The extensions are based on
unbounded recursion in one case, and bounded recursion with
minimisation in the other. We show that both approaches are compatible
with linearity and typeability constraints. Both extensions of the
typed linear lambda-calculus are minimal, in the sense that taking out
any of the components breaks the universality of the system.  We
discuss implementation techniques that exploit the linearity of the
calculi.  Finally, we apply the results to languages with fixpoint
operators: we give a compilation of the programming language PCF into
a linear lambda-calculus with linear unbounded recursion.
\end{abstract}

\section{Introduction}
Turing completeness is significant in computer science because it is a
standard measure of computational power: all general purpose
programming languages are Turing complete.  There are a number of
Turing-complete models of computation: Turing Machines, the
-calculus, term rewriting systems, partial recursive
functions, etc.  We refer to these as \emph{computation models} rather
than \emph{programming languages}, as the former can be seen as
abstract representations of computing devices, where the emphasis is
in the essential notions, whereas the latter include additional
features to make representing data and algorithms easier.

In this paper, we are interested in minimal models of computation that
are Turing complete (or universal).  In particular, we contribute to
the collection of universal systems based on the typed
-calculus, which is a paradigmatic  model of functional
computation.

There are several approaches to build a Turing complete system
starting from a typed -calculus. To obtain a minimal system,
our starting point is the typed linear -calculus, and we
add the least machinery needed to obtain a complete
system.

The linear -calculus~\cite{AbramskyS:comill} is a restriction
of the -calculus that models linear functions, defined by
syntactically linear terms where each variable occurs
exactly once~\cite{Kfoury}. The linear -calculus captures the essence of
functional computation, but it is computationally weak: all the
functions terminate in linear time. In fact, the linear
-calculus is operationally linear, that is, functions cannot
duplicate or erase arguments during evaluation (see
also~\cite{alves05tcs,klop07}). Operational linearity has great impact
when the management of resources (copying and erasing of arguments) is
important, as it can be used to efficiently implement garbage
collection, for instance. Note however, that checking if a system is
operationally linear relies on evaluation. On the other hand,
syntactical linearity is easy to check, and it is well-known that
compilers can make use of this information to optimise code. Syntactic
linearity is relevant in several program analysis techniques, for
instance, strictness analysis, pointer analysis, effects and resource
analysis (see, e.g.,
\cite{BoudolG:semlcr,EhrhardT:difflc,Wansbrough00,Wadler90,David_WalkerChapter,NoekerConcurrentClean,HofmannJ03,EggerMS09}). Linear
functions are also relevant in hardware compilation~\cite{Ghica07}:
circuits are static (i.e., they cannot be copied at run-time), so
linear computations are more naturally compiled into hardware.

Starting from the linear -calculus, we define two 
Turing-complete typed -calculi that are
universal and syntactically linear: one is based on bounded iteration
and minimisation, and the other uses unbounded recursion.

In the context of the simply typed -calculus, interesting
classes of programs can be captured by extensions of the linear
-calculus based on bounded iteration (see, e.g.,
\cite{G98,GirardJY:boull,AR02,BM04,H99,L04,T01}). In particular, a
linear version of G\"odel's \ST, which we call \LLCI, captures exactly
the class of primitive recursive functions (PR), if iterators use only
closed linear functions~\cite{Lago05}, whereas the same system with a
closed reduction strategy~\cite{fernandezM:clores} has all the
computation power of \ST~\cite{AlvesS:TCS}. The latter result shows
some redundancy regarding duplication in \ST, which can be achieved
through iteration or through non-linear occurrences of the bound
variable in the body of a function.

In recursion theory, Turing completeness can be achieved by adding a
minimisation operator to  a first-order linear system built
from a set of linear initial functions and a linear primitive
recursion scheme~\cite{DBLP:conf/birthday/AlvesFFM07}.
A similar result is shown in this paper for the linear -calculus:
an extension of \LLCI with a minimiser, which we call \LLCIm, is Turing-complete.
In \LLCIm,  both iteration and minimisation are needed to achieve completeness.

Alternatively, Turing completeness can be achieved by adding a
fixpoint operator to a typed -calculus (as it is done in
PCF~\cite{Plotkin77}). This approach has been used to extend linear
functional calculi (see, e.g.,
\cite{MackieIC:lilfpl,PittsAM:opeplp,paolini08ppdp,Brauner94}),
however, it relies on the existence of a non-linear conditional which
throws away a possibly infinite computation in one of the branches.

The question that arises is, what is the minimal extension of the
typed linear -calculus that yields a Turing complete system,
compatible with the notion of linear function?  We show how to obtain
a Turing-complete typed linear -calculus through the use of
an unbounded recursor with a built-in test on pairs, which allows the
encoding of both finite iteration and minimisation. More precisely, we
define \LLCIrec, a linear -calculus extended with numbers,
pairs and a linear unbounded recursor, with a closed-reduction
strategy. We show that  is Turing-complete and can be easily
implemented: we give an abstract machine whose configurations consist 
simply of a pair of term and a stack of terms. 

\LLCI, \LLCIrec and \LLCIm use a closed-reduction strategy in order to
preserve linearity and accommodate iteration or recursion. This
strategy is inspired by the closed cut-elimination strategy defined by
Girard~\cite{GirardGOI} for proof nets, which was adapted to the
-calculus in~\cite{fernandezM:clores}.  Closed cut
elimination is a simple and exceptionally efficient strategy in terms
of the number of cut elimination steps.  In the -calculus, it
avoids -conversion while allowing reductions inside
abstractions (in contrast with standard weak strategies), thus
achieving more sharing of computation.  An alternative approach to
preserve linearity of systems with iterators or recursors is to
consider a ``closed-at-construction'' discipline: the function used in
a bounded or unbounded recursor should be closed when the recursor is
built (rather than closed at the time of reduction).  In this paper,
we consider both approaches and analyse their computational
power. Although in the case of linear calculi with bounded recursion
closed reduction and closed construction capture different classes of
functions, we show that both disciplines yield Turing-complete systems
in calculi with unbounded recursion.



Summarising, this paper investigates the relationship between linearity 
and bounded/unbounded recursion in typed functional theories, aiming at
obtaining minimal Turing complete systems. The main contributions are:

\begin{itemize}
\item We define two extensions of the typed linear -calculus:
  , a linear calculus with numbers, pairs and an unbounded
  recursor, with a closed-reduction strategy; and , a linear
  -calculus extended with numbers, pairs, a bounded recursor
  and a minimisation operator, also with a closed-reduction
  strategy. We show some properties regarding reduction (such as
  subject-reduction and confluence), and prove Turing completeness of
  both systems by encoding the set of partial recursive functions in
   and . We also show that both systems are minimal, in
  the sense that taking out any of their components breaks the
  universality of the system.  relies only on unbounded
  recursion, whereas  needs both the iterator and the minimiser.

\item We explore some implementation issues for : we give
call-by-name and call-by-value evaluation strategies, and define a
simple abstract machine, exploiting its linearity.

\item We study the interplay between linearity and recursion based on
fixpoint combinators, and define an encoding of PCF into ,
which combined with the definition of an abstract machine for ,
gives a new implementation of PCF via a simple stack-based abstract
machine.

\item We study the interplay between linearity and
closed-reduction/closed-construction disciplines in systems with
bounded iteration and in systems with unbounded recursion.
\end{itemize}

\paragraph{Related Work}
Extensions of the linear -calculus based on bounded iteration
capture interesting classes of programs and have been used to
characterise complexity classes (see, e.g.,
\cite{G98,GirardJY:boull,AR02,BM04,H99,L04,T01}). However, in this
paper we are interested in Turing complete systems, so bounded
iteration is not sufficient.

Several approaches to obtain Turing complete system are described in
the literature, inspired by the work on linear
logic~\cite{GirardJY:linl}. In linear logic, linearity is the default,
and copying is obtained by the use of the \emph{``of course''}
exponential operator (!). To recover the full power of linear logic,
the linear calculi defined
in~\cite{AbramskyS:comill,MackieIC:lilfpl,HolmstromS:linfp} provide
explicit syntactical constructs for copying and erasing terms,
corresponding to the exponentials in linear logic.  However, adding
only copy and erase constructs to the typed linear -calculus
does not yield a universal system (see Section~\ref{sec:minimal}). In these works, some form of
unbounded recursion (using for instance fixpoint combinators and
conditionals) is also included.  Moreover, copy and erase constructs
are superfluous once recursion is added: a PCF-like language with
explicit resource management is not minimal (copy and erase constructs
are not needed).  Instead, copy and erase can be encoded through bounded or unbounded
recursion as shown in this paper (see also~\cite{MillerD:leagfp,AlvesS:phd,MackieIC:linearT}).


Several abstract machines for linear calculi are available in the
literature (see for
instance~\cite{MackieIC:geoim,David_WalkerChapter,LafontY:linam}). The
novelty here is that we implement a calculus that is syntactically
linear (in the sense that each variable is linear in  terms)
and therefore there is no need to include in the abstract machine an
environment (or store in the terminology
of~\cite{David_WalkerChapter}) to store bindings for variables.  As an
application, we give a compilation of the full PCF language into
, establishing a relation between unbounded recursion and
recursion through the use of fixpoint operators. 


 For , which
combines syntactical linearity with closed reduction, the fragment
without recursion is operationally linear; erasing and duplication can
only be done by the recursor (in linear logic~\cite{GirardJY:linl}
this is done by the use of exponentials, and in other linear
calculi~\cite{AbramskyS:comill,MackieIC:lilfpl,HolmstromS:linfp,David_WalkerChapter}
by explicit syntactical constructs). Moreover, only closed terms can
be erased or duplicated in .

There are several other domains where linearity plays a key role.  For
instance, in the area of quantum computation, the no-cloning theorem,
which states that qubits cannot be duplicated, is one of the most
important results in the area. This property is captured by a linear
calculus~\cite{Tonder2004}. In concurrent calculi, like the
-calculus~\cite{Milner1992}, a key aspect is the notion of name,
and the dual role that names play as communication channels and
variables. The linear -calculus~\cite{KobayashiPT96} has linear
(use-once) channels, which leads to clear gains in efficiency and on
program analysis avoiding several problems of channel sharing. Also,
inspired by the works by Kobayashi, Pierce and
Turner~\cite{KobayashiPT96} and the works by Honda~\cite{Honda93} on
session types, several type systems for the -calculus rely
directly on linearity to deal with resources, non-interference and
effects~\cite{GiuntiV10,YoshidaHB02}.  In this paper we focus on
functional computations, and aim at obtaining linear, universal models
of computation that can serve as a basis for the design of programming
languages. Our approach is to begin with the linear
-calculus, and achieve Turing-completeness in a controlled
way.  

This paper is an extended and revised version of~\cite{PPDP2011},
where  was first defined. Here, we provide proofs of Subject
Reduction, confluence and Turing completeness of , introduce
, analyse the power of iteration, minimisation, recursion and
fixpoint operators in linear calculi, and compare the closed-reduction
and closed-construction approaches.






























































































\section{Preliminaries: Linear Iteration}\label{sec:prelim}
In this section we recall the definition of \LLCI~\cite{AlvesS:TCS}, a
linear version of G\"odel's \ST (for details on the latter
see~\cite{GirardJY:prot}). We assume the reader is familiar with the
-calculus~\cite{BarendregtHP:lamcss}.

 \LLCI is an extension of the linear
-calculus~\cite{AbramskyS:comill} with numbers, pairs, and an
iterator. Linear -terms  are inductively defined
by: ,  if , and
 if .  Note that 
is used at least once in the body of the abstraction, and the
condition on the application ensures that all variables are used at
most once. Thus these conditions ensure syntactic linearity (variables
occur exactly once).  In \LLCI we also have numbers, generated by 
and , with an iterator:

and pairs:

Since  and  are binders, terms are defined modulo
-equivalence as usual. 

Note that, when projecting from a pair, we use both projections. A
simple example is the function that swaps the
components of a pair: 

In examples below we use tuples of any size, built from pairs. For example, 
 and

represents the term 



\LLCI uses a closed reduction strategy. The reduction rules for \LLCI are given in
Table~\ref{tab:closed-reduction}.  Substitution is a meta-operation
defined as usual, and reductions can take place in any context. 


\begin{table*}[ht]

\caption{Closed reduction in \LLCI}\label{tab:closed-reduction}
\end{table*}

Note that the \emph{Iter} rules are only triggered when the function
 is closed. Thanks to the use of a closed reduction strategy,
iterators on \emph{open} linear functions are accepted in \LLCI (since
these terms are syntactically linear), and reduction preserves
linearity. The  closedness conditions in rules \emph{Beta} and
\emph{Let} are not necessary to preserve linearity (since variables
are used linearly in abstractions and lets), but they ensure that all
the substitutions created during reduction are closed (thus, there is
no need to perform -conversions during reduction).  Normal
forms are not the same as in the -calculus (for example,
 is a normal form), but closed reduction is
still adequate for the evaluation of closed terms (if a closed term has a
weak head normal form, it will be reached~\cite{AlvesS:TCS}).  Closed
reduction can also be used to evaluate open terms, using the
``normalisation by evaluation'' technique~\cite{BergerU91} as shown
in~\cite{fernandezM:clores, FernandezM:aaecc05} (in the latter
director strings are used to implement closedness tests as local
checks on terms).



\LLCI is a typed calculus. Note that, although linear, some untyped terms are not strongly normalisable. For
instance,  where  reduces to itself. 
However, the linear type system defined in~\cite{AlvesS:TCS} ensures strong normalisation. We recall the type definitions for \LLCI below.

The syntax of terms in  does not include type annotations,
instead we will use a type assignment system based on \emph{linear
  types}.  The set of linear types is generated by the grammar:

where  is the type of numbers. A type environment  is a
list of type assumptions of the form  where  is a variable and
 a type, and each variable occurs at most once in .  We
write  to denote the set of variables that occur in
.

We write  if the term 
can be assigned the type  in the environment  using the
typing rules in Table~\ref{fig:types}.  Note that the only structural
rule is Exchange, we do not have Weakening and Contraction rules: we
are in a linear system.  For the same reason, the logical rules split
the context between the premises (i.e., the variable conditions in
Table~\ref{LrecTerms} are enforced by the typing rules).
\begin{table*}
{
{\bf Axiom} and {\bf Structural Rule}:

{\bf Logical Rules}:


{\bf Numbers:}

}
\caption{Type System for \LLCI}\label{fig:types}
\end{table*}

\LLCI has all the power of \ST; we refer
to~\cite{AlvesS:TCS} for more details and examples.

\section{Towards a Minimal Universal Type System}\label{sec:minimal}
In this section we will present two universal type systems which extend the linear -calculus:  and . While  is a linear calculus with an unbounded recursor,  is a linear calculus where recursion is obtained through iteration and minimisation. We show that both typed calculi are universal and \emph{minimal} (in the sense that all their constructors are necessary for the system to be universal).  

We avoid introducing superfluous operators and rules, such as copy and erase combinators. Indeed, these can be encoded using recursion, as we will show in this section. The reverse is not true (although in an untyped system, adding copy and erase combinators to the linear -calculus would produce a Turing-complete system). More precisely, the untyped linear -calculus extended with linear pairs and projections, and copy and erase combinators ( and ) with the following reduction rules:
 
has the computational power of the pure untyped -calculus, but the same does not follow if we consider typed terms. The typing rules for  and  are:

Since this system can be encoded in \LLCI (see~\cite{AlvesS:TCS}), which is not Turing complete (all typable terms are terminating), we conclude that the typed linear -calculus with pairs, projections and the combinators  and  is not universal.

Another way to obtain Turing completeness of typed -calculi is via fixpoint operators and conditionals, as done in PCF~\cite{Plotkin77}. In Section~\ref{sec:PCF} we discuss fixpoints in the presence of linearity and study the relation between  and PCF.
\subsection{Linear Unbounded Recursion}
\label{sec:LLCIrec}
In this section we define , an
extension of the linear -calculus~\cite{AbramskyS:comill}
with numbers, pairs, and a typed unbounded recursor with a closed
reduction strategy that preserves syntactic linearity. We prove that 
this system is Turing complete.


The syntax of \LLCIrec is similar to that of \LLCI (recalled in Section~\ref{sec:prelim}), except that
instead of a bounded iterator we have a recursor working on pairs of
natural numbers.  Table~\ref{tab:termsrec} summarises the syntax of
terms in .  We assume Barendregt's convention regarding names
of free and bound variables in terms (in particular, bound names are different from free names).

\begin{table*}

\caption{Terms in \LLCIrec}\label{LrecTerms}\label{tab:termsrec}
\end{table*}

The reduction rules for  are \emph{Beta} and \emph{Let}, 
given in Table~\ref{tab:closed-reduction}, together with
two rules for the recursor shown in Table~\ref{tab:closed-reduction2}.

\begin{table*}

\caption{Closed reduction for recursion}\label{tab:closed-reduction2}
\end{table*}


Note that the \emph{Rec} rules are only triggered when the closedness conditions
hold, thus linearity is preserved by reduction.  The
conditions on \emph{Beta} and \emph{Let} are orthogonal to the
linearity issues (as explained in the previous section, they simply
produce a more efficient strategy of reduction) and  do not
affect the technical results of the paper (we discuss the role of closed reduction in \LLCI and \LLCIrec in more detail in Section~\ref{sec:WRS11}).

The \emph{Rec} rules pattern-match on a pair of numbers (the usual
bounded recursor works on a single number). This is because we are
representing both bounded and unbounded recursion with the same
operator (as the examples below illustrate), which requires (for a
particular  and function ) being able to test the value of
, and access the value .  An alternative would be to have an
extra parameter of type  in the recursor.

\begin{example}\label{sec:Lrecex}
We illustrate the use of the recursor by encoding some standard functions in \LLCIrec.
\begin{itemize}
\item \textbf{Bounded iteration}
  Let  be the
identity function . \LLCI's iterator can be encoded in  using
the term  \text{``''} defined as follows:

We will show later that this term  has the same behaviour
as \LLCI's iterator.

\item  \textbf{Projections and duplication of natural numbers}
The first and second projection functions on pairs  of
natural numbers can be defined  by using the numbers in a recursor.

The following function  can be used to copy numbers:

Other
mechanisms to erase and copy numbers in  will be shown later.

\item  \textbf{Arithmetic functions}
We can now define some arithmetic functions that we will use in the paper.
\begin{itemize}
\item ;
\item 
\item \\
where ;
\item .
\end{itemize}
The correctness of these encodings can be easily proved by induction.

\item  \textbf{Minimisation}
\label{ex:min}
The examples above can also be defined in \LLCI, using bounded
recursion.  is a more powerful system:  it can encode the
minimisation operator  used to define partial recursive
functions.  Recall that if  is a total
function on natural numbers, 

Let  be a closed -term  in  
representing a total function  on natural numbers. The encoding of  is
 
where .  We 
prove the correctness of this encoding below (see Theorem~\ref{th:mincorrect}).
\end{itemize}
\end{example}




We use the same notation for typing judgements in \LLCI and \LLCIrec, since there will be no ambiguity.
We write  if the term 
can be assigned the type  in the environment  using the
typing rules in Table~\ref{fig:types}, where we replace the rule for the iterator by the following rule:



Note that all the terms given in the example above can be typed.







\begin{theorem}[Properties of  reductions in \LLCIrec]\
\label{th:propLrec}\ 
\begin{enumerate}
\item
If  then .
\item
Subject Reduction: Reductions preserve types.
\item
Church-Rosser:  \LLCIrec is confluent.
\item
Adequacy: If  in \LLCIrec, and   is a normal form, then:\\

\item
\LLCIrec is not strongly normalising, even for typeable terms.
\end{enumerate}
\end{theorem}

\begin{proof}\ 
\begin{enumerate}
\item
By induction on  type derivations.
\item
By induction on type derivations,
using a substitution lemma as usual. 
We show the case where the term has the form  (for the other
cases, the proof is the same as for \LLCI~\cite{AlvesS:TCS}). 

Assume 
.
If the reduction takes
place inside , , ,  or  the property follows directly by
induction.
If the reduction takes place at the root, there are two cases:
\begin{enumerate}
\item
 if
  . Then, by part 1, . The type
  derivation may end with , in which case the result
  is trivial, or with , in which case the derivation has
  conclusion  with premises:
  ,\ \ ,\ \ ,\ \ 
  . Therefore the property holds, directly from  .
\item
 if
  . Reasoning in a similar way, we note that when
  the type derivation ends with an application of the rule
  , it has conclusion  with
  premises , , , and . 
If  , then we can deduce  , therefore we have . Thus we can obtain . From these we deduce
 as
required.
\end{enumerate}
\item
Confluence can be proved directly, using
Martin-L\"of's technique (as it was done for \LLCI,
see~\cite{AlvesS:phd}) or can be obtained as a consequence of Klop's
theorem for orthogonal higher-order reduction systems~\cite{KlopJW:crs}.
\item
By induction on . 
If ,  or , then we are done. Otherwise:
\begin{itemize}
\item If , it follows by induction. 
\item If . Since  is in normal form, so are
the terms .  Since  is typable,  must be a term of type
, and by induction,  is a pair of numbers. But
then one of the recursor rules applies (contradiction).
\item
The cases of application and let are similar.
\end{itemize}
\item
The following term is typable but is not strongly normalisable:

Another non-terminating typable term will be given later, using the encoding of a fixpoint operator.
\end{enumerate}
\end{proof}

\paragraph*{The Computational Power of \LLCIrec}\label{sec:LLCIrecpow}\ \\\ \\
We now prove that \LLCIrec is Turing complete. Since  \LLCI can encode all the primitive recursive functions~\cite{AlvesS:phd,AlvesS:TCS}, it suffices to show that \LLCI is a subset of  (therefore  also encodes primitive recursion), and that one can encode minimisation.

First we show that the encoding of \LLCI's iterator, defined in Example \ref{sec:Lrecex},  behaves 
as expected. \LLCI is a sub-system of .
\begin{proposition}

\end{proposition}
\begin{proof}
\begin{itemize}
\item If :

\item If :

\end{itemize}
\end{proof}
If , , and , then ,
that is  is properly typed in \LLCIrec, as shown in
Figure~\ref{fig:typeiter}.

\begin{figure*}[t!]

\caption{Type derivation for }\label{fig:typeiter}
\end{figure*}

\begin{corollary}
\LLCIrec has all the computation power of \LLCI, thus, any function definable in \ST can be defined in .
\end{corollary}


We now show that the encoding of the minimiser given in Section~\ref{ex:min}
behaves as expected.

\begin{theorem}[Minimisation in \LLCIrec]
\label{th:mincorrect}
Let  be a closed -term in , encoding the 
total function  on natural numbers. Consider the term , with . The term  encodes .
\end{theorem}

\begin{proof}
Consider the non-empty sequence , such that
 is the first element in the sequence that is equal to
zero. Then  We proceed by induction on the length of .
\begin{itemize}
\item Basis: . Thus 

\item Induction: If , then , therefore  reduces to a term of the form . One easily notice that

\end{itemize}
Now, let , and consider
the sequence . One easily notices that . Note that, if there
exists no  such that , then  diverges, and so does the minimisation of .
\end{proof}
\begin{corollary}
 \LLCIrec is Turing complete.
\end{corollary}
\paragraph*{Erasing and Duplicating in }\ \\\ \\
There are various ways of encoding erasing and duplicating in .
First note that, although in the linear -calculus we are not able to discard
arguments of functions, terms are consumed by reduction. The idea of
erasing by consuming is related to the notion of Solvability
(see~\cite{BarendregtHP:lamcss}, Chapter 8)  as it relies on
reduction to the identity. Using this technique,
in~\cite{AlvesS:phd,AlvesS:TCS} it is shown that in \LLCI there is a
general form of erasing. In  this technique can be used to erase 
 terms of type , where  is a type generated by the grammar:
. In the definition of the erasing function  we use a function  to build a term of type  ( and  are mutually
recursive).

\begin{definition}[Erasing] If , then
   is defined as follows:

\end{definition}

\begin{theorem}
\label{thm:erase1}
\begin{enumerate}
\item
If  then , for any type .
\item  is  closed and typeable: . 
\item
For any type , . 
\item  is normalisable.
\end{enumerate}
\end{theorem}

\begin{proof}
The first two parts are proved by simultaneous induction on , as done for \LLCI\cite{AlvesS:TCS}.
The third part is proved by induction on .\begin{itemize}
\item If , then , and .
\item If , then , then 

Note that, by induction,  and .
\item  If   then , therefore\\

\end{itemize}
The last part is proved by induction on .
\end{proof}

, unlike \LLCI, is not normalising, and there are terms that
cannot be consumed using the technique described above. There are even
normalising terms that cannot be erased by reduction. For example,
consider the following term  which represents a fixpoint
operator (more details are given in Section~\ref{sec:PCF}):


This term is typable (it has type ) and
is a normal form (the recursor rules do not apply because  is a
variable).  However, the term 
 does
not have a normal form.  On the positive side, closed terms of type
, or tuples where the elements are terms of type , can
indeed be erased using this technique. Erasing ``by consuming''
reflects the work that needs to be done to effectively dispose of a
data structure (where each component is garbage collected).  For arrow
types, a different erasing mechanism will be defined in
Section~\ref{sec:PCF}.

\begin{theorem}
Let  be a type generated by the grammar:

If  and  has a normal form, then .
\end{theorem}
\begin{proof} 
By induction on . 
\begin{itemize}
\item If , then . Since  is normalising, , and by the Adequacy result (Theorem~\ref{th:propLrec}), , . Therefore .
\item  If 
 :  . Since  is
normalisable then,  by Adequacy
(Theorem~\ref{th:propLrec}), . Thus
. By induction hypothesis  and , therefore .
\end{itemize}
\end{proof}
There is also a mechanism to copy closed terms in :
\begin{definition}[Duplication]
\label{def:duplication-in-Lrec}
Define  as:

where .
\end{definition}

\begin{theorem}
\label{th:dupl}
If  then 
\end{theorem}
\begin{proof}
By the definition of .

\end{proof}



\subsection{\LLCIm:  Minimisation vs. Unbounded Recursion}
\label{sec:LLCIm}

There are two standard ways of extending the primitive recursive
functions so that all partial recursive functions are obtained. One is
unbounded minimisation, the other is unbounded recursion. For first-order
functions (i.e., functions of type level 1), both methods are
equivalent, see for instance~\cite{BergerU}.  
In this section we extend \LLCI with a minimisation operator -- we will refer to this extension as \LLCIm -- and establish its relation with \LLCIrec.
Starting from  \LLCI,  we add a minimiser with a typing rule

and two reduction rules:



\begin{theorem}[Properties of  reductions in \LLCIm]\ 
\begin{enumerate}
\item
If  then .
\item
Subject Reduction: If  and  then .
\item
\LLCIm is confluent: If  and  then there is some term  such that  and .
\end{enumerate}
\end{theorem}
\begin{proof}\ 
\begin{enumerate}
\item
By induction on the type derivation.
\item
Straightforward extension of the proof given for \LLCI in~\cite{AlvesS:TCS}, by induction on the type derivation . We show the case where the term  is  and there is a type derivation ending in:
{\small
}
If the reduction step takes place inside ,  or , the result follows directly by induction. If reduction takes place at the root, we have two cases:
\begin{enumerate}
\item , with . Note that  by part 1, and we have .  
\item , with . Then  , and we have:
{\small

}
Therefore:
{\small

}
\end{enumerate}
\item Using Tait-Martin-L\"of's method (see~\cite{BarendregtHP:lamcss} for more details).
\end{enumerate}
\end{proof}

Since \LLCI, and therefore \LLCIm, includes all the primitive recursive
functions, to show Turing completeness of \LLCIm it is sufficient to
show that unbounded minimisation can be encoded.  First, we recall the
following result from Kleene~\cite{klee:intr52}, which uses the well-known
minimisation operator  (already mentioned in Example~\ref{sec:Lrecex}).

\begin{theorem}[The Kleene normal form] 
Let  be a partial recursive function on . Then, a number
 and two primitive recursive functions ,  can be found such that

where  is the minimisation operator on the last argument of , that is,
.
\end{theorem} 

As a consequence of Kleene's theorem, we only have to prove that we
can encode minimisation of primitive recursive functions in order to
show Turing-completeness of , relying on the fact that primitive
recursive functions can be encoded in \LLCI. Below we give the
encoding of minimisation for functions of arity 1 (the extension to
functions of arity  is straightforward).

\begin{theorem}[Unbounded minimisation in \LLCIm] 
\label{th:encmin}
If  is
 a primitive recursive function and  is its encoding in \LLCIm, then

\end{theorem}

\begin{proof} 
Similar to the proof for \LLCIrec (Theorem~\ref{th:mincorrect}), considering the non-empty sequence , such that  is the first element in the sequence that is equal to
zero, and showing (by induction on the length of ) that: 
\end{proof}
\begin{corollary} 
\LLCIm is Turing complete.
\end{corollary}
We can also encode \LLCIrec into \LLCIm, simulating the recursor with iter
and . Consider the following term:
 where  is such that
. The function , given , will produce . Now consider , which will lead to the following sequence:
  
where  is the minimum number such that   
produces .
Now, one can encode  as:


Intuitively,  will iterate 
until  is equal to zero, and that  will
count the number of iterations that will actually be necessary, or
will go on forever if that never happens.

\LLCIm is a minimal universal system in the sense that the subsystems obtained by taking out  and , respectively, are not universal. Note that the subsystem without  corresponds to \LLCI and is therefore strongly normalising.
Also note that, the minimiser cannot replace bounded iteration, either in recursion theory or in the typed -calculus, as we now show.
\paragraph{Partial Recursive Functions without Bounded Iteration}
\begin{lemma}
For any function , , defined from the initial
functions (,  and projections) and composition,
without using the primitive recursive scheme, there is a constant  such that, 
  or
, for any given arguments .
\end{lemma}

\begin{proof}
Assume  is defined by the expression .
We proceed by induction on : The base cases (,  and ) are trivial. Let us consider the composition case.
If , where  and  are previously defined functions, then 
by induction hypothesis , where  or  for some constant . But then, by induction hypothesis  or , and the result follows.
\end{proof}

\begin{theorem}
Minimisation applied to functions in the previous class either returns 0 or is not defined.
\end{theorem}

\begin{proof}
By the previous lemma, when , then either it is
the constant function returning , or it returns  when the
argument . In the first case  returns , 
and in the second case either  and then  returns , or 
diverges.
\end{proof}

\paragraph{\LLCIm without Iteration}

\begin{lemma} If  is a term in \LLCIm without :

\end{lemma}
\begin{proof} First note that , and it is strongly normalisable\footnote{We are in a proper subset of \LLCI, for which the properties of Strong Normalisation and Adequacy hold~\cite{AlvesS:TCS}.}. Therefore, by Adequacy, , for some . Since  is linear, it cannot erase the  in its argument, therefore .  
\end{proof}
\begin{theorem} Let , ,  be terms in \LLCIm without . Then  either 
reduces to a reduct of , or diverges.
\end{theorem}
\begin{proof}
By Adequacy, , for some . If , then , using the first rule for .  If  then, using the second rule for , the computation diverges because, by the previous lemma,  with , will never reduce to .
\end{proof}
This theorem is stated for closed terms, but is valid also if  and  is an open term. Otherwise, if we have open terms the rule for  will not apply. 

 can be seen as a more compact version of \LLCIm where the
recursor can perform both bounded iteration or minimisation.
\section{Evaluation Strategies for \LLCIrec}\label{sec:strat}
In this section we define two evaluation strategies for \LLCIrec and 
derive a stack-based abstract machine.
\paragraph{Call-by-name}
The CBN evaluation relation for closed terms in 
\LLCIrec is defined in Table~\ref{fig:Lreceval}. The notation 
 means that the closed
term  evaluates in \LLCIrec to the value .  

\emph{Values} 
are terms of the form , ,  and
, i.e., \emph{weak head normal forms} (whnf). Note that
\LLCIrec does not evaluate under a  symbol, since  is
used as a constructor for natural numbers.
Also note that no closedness
conditions are needed in the evaluation rules for closed terms.
The rule \emph{Let} is given using application to simplify the
presentation (in this way, we will be able to reuse this rule when we
define the call-by-value evaluation relation below).


\begin{table*}


\caption{CBN evaluation for \LLCIrec}\label{fig:Lreceval}
\end{table*}


The evaluation relation  corresponds to
\emph{standard reduction} to weak head normal form. Recall that a
reduction is called standard if the contraction of redexes is made
from left-to-right (i.e., leftmost-outermost).  It is well known that
for the -calculus~\cite{BarendregtHP:lamcss}, the standard
reduction is normalising, that is, if a term has a normal form, then
it will be reached. A ``standardisation'' result holds for closed
terms in , as the following theorem shows.

\begin{theorem}(Standardisation)
\label{th:standard} 
If  (i.e.,  is a closed term in
  ) and  has a whnf, then , for some
  value .
\end{theorem}
\begin{proof}
We rely on Klop's result~\cite{Klop-thesis,terese2005}, which states
that leftmost-outermost reduction 
is normalising for left-normal orthogonal Combinatory
Reduction Systems (CRSs). A CRS is orthogonal if its rules are
left-linear (i.e., the left hand-sides of the rewrite rules contain no
duplicated variables) and non-overlapping (there are no critical
pairs). A CRS is left-normal if on the left hand-sides of the rewrite
rules, all the function symbols appear before the variables.  The
-calculus is an example of a  left-normal orthogonal CRS, as
is  \LLCIrec. Therefore, leftmost-outermost reduction is normalising for .
The result follows, since CBN performs leftmost-outermost reduction.
\end{proof}

For open terms, the set of weak head normal forms includes not only
values but also other kinds of terms, since, for instance, reduction
of an application is blocked if the argument is open.  However, an evaluation procedure can also be defined for open terms using closed reduction, if we
consider all the free variables as constants as shown
in~\cite{fernandezM:clores} (see also~\cite{BergerU91}).

\paragraph{Call-by-value}
A call-by-value evaluation relation for \LLCIrec can be obtained
from the CBN relation by changing the rule for application, as usual.





There is no change in the  \emph{Rec} and \emph{Let} rules, since they
rely on the \emph{App} rule.  Unlike CBN, the CBV strategy does not always 
reach a value, even if a closed term has one (Theorem~\ref{th:standard} does not hold for a CBV strategy).
For example, recall the term  in Section \ref{sec:LLCIrec},
and consider . This
term has a value under the CBN strategy, but not under CBV.
In fact, innermost strategies are normalising in an orthogonal system
if and only if the system is itself strongly normalising.

\subsection{Stack Machine for \LLCIrec} 
Intermediate languages that incorporate linearity  have well 
known implementation advantages whether in compilers, static analysis, 
or whenever resources are limited~\cite{LafontY:linam,MackieIC:lilfpl,PittsAM:opeplp,David_WalkerChapter}. Inspired by these previous works, we 
finish this section by illustrating how simply \LLCIrec can be 
implemented as a stack machine. We show a call-by-name version, but it
is straightforward to modify to other reduction strategies.

The basic principle of the machine is to find the next redex, using a
stack  to store future computations.  The elements of the
stack are terms in an extension of  that includes the
following additional kinds of terms: , ,
, where  are variables bound in   and  are 
terms.

The configurations of the machine are pairs consisting of a term and a
stack of extended terms.  Unlike Krivine's machine or its variants
(see for instance~\cite{HankinC, Curien91, FernandezM:newdem}) we do
not need to include an environment (sometimes called store, as
in~\cite{David_WalkerChapter}) in the configurations. Indeed, the
environment is used to store bindings for variables, but here as soon
as a binding of a variable to a term is known we can replace the
unique occurrence of that variable (the calculus is syntactically
linear). In other words, instead of building an environment, we use
``assignment'' and replace the occurrence of the variable by the term.

The transitions of the machine are given in Table
\ref{fig:Lrecmachine}. 
For a program (closed term ), the machine is
started with an empty stack: .  The machine stops when no rule
can apply.
\begin{table*}

\caption{Stack machine for \LLCIrec}\label{fig:Lrecmachine}
\end{table*}

The use of ``assignment'' means that there is no manipulation (no
copying, erasing, or even searching for bindings) in environments
usually associated to these kinds of implementations.

The correctness of the machine with respect to the CBN evaluation
relation is proved in the usual way: first we show that if a typeable
term has a value, the machine will find it (it cannot remain blocked)
and then we show that if the machine starting with a configuration 
 stops at a
value, then this value is a reduct of  in the calculus.


\begin{theorem} (Completeness)
If  and there is a value  such that 
, then .
\end{theorem}

\begin{proof}
  By induction on the evaluation relation, using Subject Reduction
  (Theorem~\ref{th:propLrec}) and the following property:

If   then .

This property is proved by induction on . Intuitively,
since only the top of the stack is used to select a transition, it is
clear that appending elements at the bottom of the stack does not
affect the computation.
\end{proof}


\begin{theorem}(Soundness)
If  and   then .
\end{theorem}
\begin{proof}
First, we define a readback function 
that converts a machine configuration 
into a term, by induction on  as follows:

Then, we show that 
a machine transition does not change the meaning of
the configuration:
If  then 
.
To prove this result we distinguish cases depending on the transition rule applied from Table~\ref{fig:Lrecmachine}.




If the transition  is an instance of the rules (app), (let), (rec) or (pair2), the result follows trivially since the readback is the same for both configurations: .  

If  the transition  is an instance of rule (abs), (pair1), (zero) or (succ) then  we can prove that  as follows. We observe that by definition of the readback function, in each of these cases there are terms   such that ,   and . Finally, by induction on the definition of the readback function, we show that if  then .

Having shown that a single transition  is sound, we derive the soundness of the machine by induction on the length of the transition sequence: If  then 
.
\end{proof}

\section{Applications:  Fixpoint Operators and PCF}
\label{sec:PCF}


We now study the relation between  and languages with 
fixpoint operators, in particular PCF.


\subsection{The Role of Conditionals}

Recursive function definitions based on fixpoint operators
rely on the use of a non-linear conditional that should
discard the branch corresponding to an infinite computation.  For
instance, the definition of factorial:

relies on the fact that  will return  when the input
number is , and discard the non-terminating ``else'' branch.
Enabling the occurrence of the (bound) variable, used to iterate the
function ( in the above definition), in only one branch of the
conditional is crucial for the definition of interesting recursive
programs. This is why denotational linear versions of
PCF~\cite{paolini08ppdp} allow stable variables to be used non-linearly
but not to be abstracted, since their only purpose is to obtain
fixpoints.

Fixpoint operators can be encoded in \LLCIrec:
recall the term  in Section~\ref{sec:LLCIrec}. More
generally,
 for any type  we define the term

where  represents the term
.
For every type ,  is well-typed in
\LLCIrec (see Figure \ref{fig:typefix}).
\begin{figure*}

\caption{Type derivation for }\label{fig:typefix}
\end{figure*}
Note that, for any closed term  of type , we have:

Although  behaves like a fixpoint operator, one cannot write
useful recursive programs using fixpoint operators alone (i.e. without
a conditional): if we apply  to a linear function , we obtain
a non-normalisable term (recall the example in
Section~\ref{sec:LLCIrec}).  Instead, in \LLCIrec, recursive
functions, such as factorial, can be easily encoded using
:

where   and  is
the duplicator term defined previously (see
Definition~\ref{def:duplication-in-Lrec}).  Note that, although
conditionals are not part of \LLCIrec syntax, reduction rules for
 use pattern-matching.  In the remainder of this
section we show how we can encode in \LLCIrec recursive functions
defined using fixpoints.


\subsection{Encoding PCF  in \LLCIrec}

PCF (Programming Language for Computable
Functions)~\cite{Plotkin77} can be seen as a minimalistic typed
 functional
programming language.
It is an extension of the simply typed -calculus with numbers, 
a fixpoint operator, and a conditional.
Let us first recall its syntax.  PCF  is a variant of the typed \lam,
with a basic type  for numbers and the following constants:
\begin{itemize}
\item , for 
\item 
\item , such that

\item for each type , ,
such that

\item for each type , , such that
.
\end{itemize}

\begin{definition}
PCF types and environments are translated into \LLCIrec types using :\\

\end{definition}
Since \LLCIrec is Turing complete, it can simulate any PCF program. Furthermore, it is possible to define an encoding in \LLCIrec for all the terms in PCF. We give a definition below, which is inspired by
the encoding of \ST~\cite{AlvesS:TCS}. For convenience, we make the
following abbreviations, where the variables  and  are assumed
fresh, and  is defined below:

\begin{definition}
Let  be a PCF term such that  and
.  The compilation into \LLCIrec, is
defined as: , where  is defined in Table
\ref{table:compPCF}, and for a term  and a variable , such that
,  is inductively defined in the following
way:
 

\begin{table*}
\caption{PCF compilation into }\label{table:compPCF}
\end{table*}
Notice that  is not defined for the entire syntax of \LLCIrec.
The reason for this is that, although other syntactic constructors (like
recursors or pairs) may appear in , they are the outcome of
 and therefore are closed terms, where  does not occur
free.
\end{definition}


Some observations about the encoding follow.

First, we remark that  is not encoded as 
, since 
does not evaluate under  or . We should not encode a
divergent PCF program into a terminating term in .
In particular, the translation of  is , which diverges (if we encode
 as , then we obtain , which is
not right).

Regarding abstractions or conditionals, the encoding is different from
the one used in for \ST in~\cite{AlvesS:TCS}. We cannot use the same
encoding as in \LLCI, where terms are erased by ``consuming them'',
because PCF, unlike \ST, is not strongly
normalising. The technique used here for erasing could have been used
for \LLCI, but erasing ``by consuming'' reflects the work needed to erase a data structure.

The second case in the encoding for abstractions (see
Table~\ref{table:compPCF}) uses a
recursor on zero to discard the argument, where the function parameter
is . The
reason for this is that one cannot use  directly as the function
parameter because that might make the term untypable, and just using
 would make the types work, but could encode
strongly normalisable terms into terms with infinite reduction
sequences (because  might not terminate). For
example, consider the encoding of .

The translation of a typable PCF term is also typable in \LLCIrec (this
is proved below).  In particular, for any type , the term
 is well-typed.
In Figure \ref{fig:typecond}, we show the type
derivation for the encoding of the conditional (we use  to represent
the term ).

\begin{figure*}[t!]

\caption{Type derivation for  }\label{fig:typecond}
\end{figure*}

The type derivation for  depends on the fact that, if , then for any type , we have  by Theorem~\ref{thm:erase1}. Note that the recursor on  in  discards the remaining recursion (corresponding to the branch of the
conditional that is not needed), returning .

\begin{table*}[t!]
{\bf Axiom} and {\bf Structural Rule}:


{\bf Logical Rules}:



{\bf Numbers}:



\caption{Typing rules for \LLCIrecX}\label{fig:typesX}
\end{table*}

We prove by induction that the encoding respects types. To make the
induction work, we need to define and intermediate system where certain
variables (not yet affected by the encoding) may occur non-linearly.
More precisely, we consider an extension to
\LLCIrec, which allows variables on a certain set  to appear
non-linearly in a term. We call the extended system \LLCIrecX; and it is
defined by the rules in Table \ref{fig:typesX}. Intuitively, if  is
the set of free-variables of , then  will be a \LLCIrec
term, except for the variables , which may occur non-linearly,
and , will be a typed \LLCIrec term.
We can prove the following results regarding \LLCIrecX.

\begin{lemma}\label{lem:samevarstype}
If , where  and
, then ,
where .
\end{lemma}
\begin{proof}
By induction on , using the fact that .
We show the cases for variable and application.
\begin{itemize}
\item . Then , and using the axiom we obtain both
    and .
\item , and  (the case where
  is similar).
Then  and .
Let  and . Then
 and  , where  and  can only
share variables in .
By induction hypothesis .
Also, since  and , we have
.
Therefore   .
\item , , and .
Let  and
 and
assume  is the type associated to  in . Then
 and
  .
By induction  hypothesis , and .
Thus , and
  .
Therefore   .
Also ,
therefore .
\end{itemize}
\end{proof}
\begin{lemma}\label{lem:typesystemLX}
If  is a PCF term of type , then  where the notation
 is used to denote the restriction of 
 to the variables in .
 \end{lemma}

\begin{proof}
By induction on the PCF type derivation for , as done for \ST in~\cite{AlvesS:TCS}.


\end{proof}
\begin{theorem}If  is a PCF term of type  under a set of
assumptions  for its free variables , then

\end{theorem}
\begin{proof}
By induction on the number of free variables of , using Lemmas~
\ref{lem:samevarstype} and \ref{lem:typesystemLX}.
\end{proof}

Using the encodings given above, it is possible to simulate the
evaluation of a PCF program in \LLCIrec.
More precisely, if  is a
closed PCF term of type , which evaluates to  under a CBN
semantics for PCF~\cite{Plotkin77}, then the encoding of  reduces
in \LLCIrec to the encoding of , and evaluates under a CBN semantics
to a value which is equal to the encoding of .
In Table~\ref{fig:eval} we recall the CBN rules for PCF: 
means that the closed term  evaluates to the value  (a value is
either a number, a -abstraction, a constant, or a partially
applied conditional).
\begin{table*}
{


}
\caption{CBN evaluation for PCF}\label{fig:eval}
\end{table*}

\begin{lemma}[Substitution]\label{lemsub} Let  be a term in \LLCIrec.
\begin{enumerate}
\item\label{one}  If , and , then

\item\label{two} If , then .
\end{enumerate}
\end{lemma}
\begin{proof}
By induction on .
\end{proof}

\begin{lemma}\label{lem:evalred}Let  be a closed PCF term. If , then .
\end{lemma}
\begin{proof}
By induction on the evaluation relation, using a technique
similar to the one used for \ST in \cite{AlvesS:TCS}.
Here we show the
main steps of reduction for  where , 
are closed terms by assumption.
\begin{itemize}
\item If : \\

\item If , let  be the term :\\

\end{itemize}
For application, we rely on the substitution lemmas above.
Note that for an application , where  is a constant, we rely on the correctness of the encodings for constants, which can be easily proved by induction. For example, in the case of  it is trivial to prove that, if  is a number
 in  (), then .
\end{proof}

\begin{theorem} Let  be a closed PCF term. If , then  such that , and .
\end{theorem}
\begin{proof}
By Lemma \ref{lem:evalred},  implies . 
By Theorem \ref{th:standard}, . Therefore, since
 and  the system is confluent (Theorem~\ref{th:propLrec}), 
. 
\end{proof}

\begin{lemma}\label{eqeval}If  and , then
 and .
\end{lemma}
\begin{proof} 
By transitivity of the equality relation.
\end{proof}
\begin{theorem} Let  be a closed PCF term.
If , then , such that, 
and .
\end{theorem}
\begin{proof}
By induction on the evaluation relation, using Lemma \ref{eqeval}. Note
that, if   is a value different from a partially applied conditional,
the result follows because  and  is also a value, i.e.
, therefore .
If  is an application  then ,
therefore  if  and  . If , then by I.H. , and . Note that  is a value of arrow type, which compilation equals
an abstraction, therefore  or . 
\begin{itemize}
\item If , we have two cases:
\begin{itemize}
\item : then , thus . Since
 and  then, by Lemma
\ref{lemsub}.\ref{two} , which, by Lemma \ref{lemsub}.\ref{one}, equals
, therefore (by Lemma \ref{eqeval})
, and . By I.H.,
 and , therefore  and
.
\item : let  represent the term . Then
, therefore
. Note that
 and
 if , then,
since , by I.H.,  and , therefore  and   as required.
\end{itemize}
\item : then , then . Then
 and  if , in
which case we have two possibilities:
\begin{itemize}
\item : then  if , in which case
. By I.H., , and ,
therefore  ( is the only value of type  that compiles
to ). Therefore  and  .
\item : then  if . By
I.H., , and , thus  ( is a number in PCF and it must different from , otherwise
its compilation would be ) and . Note that
, therefore, by Lemma \ref{eqeval},
. Now it suffices to notice that , and  as required.
\end{itemize}
\item For  and , the proof is similar to the case of
.
\item If : let  represent the term  . Then  , therefore
.
Then, since ,
 if
 (and
). Thus, by I.H.
 and  , therefore
 and   as required.
\item : let  represent the term . Then ,
therefore .
Then  and
. Note that
, because it is a
value, and .
\item : let  represent the term . Then , therefore\\
  . Then  and
. Note that
, because
it is a value, and .
\item : let  represent the term
. Then
, therefore . Then  and
 if , in which case
we have two possibilities:
\begin{itemize}
\item : then
 if . By I.H., , and , therefore  ( is the only value of type  that compiles to ). Also by I.H,  and , therefore , thus , and  as required.
\item : then
 if .  By I.H., , and , thus  ( is a number in PCF and it must different from , otherwise its compilation would be ). Also by I.H,  and , therefore  and  as required.
\end{itemize}
\end{itemize}
\end{proof}
This completes the proof of soundness and completeness of the encoding.

Note that the terms of the form  used
in the encoding of conditionals and -abstractions allow us to
discard terms without evaluating them.  This is a feature of the
encoding, otherwise terminating programs in PCF could be translated to
non-terminating programs in \LLCIrec. This differs from the definition
of erasing given in Section \ref{sec:LLCIrec}, where terms are
consumed and not discarded (in pure linear systems functions do not
discard their arguments). However, allowing terms to be discarded
without being evaluated, is crucial when defining recursion based on
fixpoints.

Once a PCF term is compiled into  it can be implemented using the
techniques in Section \ref{sec:strat}, thus we obtain a new stack
machine implementation of PCF.

\section{Closed Reduction vs Closed Construction in Calculi with Recursion}\label{sec:WRS11}
Both \LLCI and \LLCIrec use a closed reduction strategy that waits for
arguments to become closed before firing redexes. We now look in more
detail at the implications of using a closed reduction strategy,
instead of imposing functions used in iteration/recursion to be
closed-by-construction (a viable alternative in the presence of
linearity).

As mentioned in the Introduction, the closed reduction strategy for
the -calculus avoids -conversion while allowing
reductions inside abstractions, thus achieving more sharing of
computation.  When applied to \LLCI and \LLCIrec, it imposes certain
conditions on reduction rules; in particular iterated functions should
be closed. The intuition here is that we should only copy closed terms
because then all the resources are there. In linear logic words, we
can promote a term that is closed.



The closed reduction strategy waits, to reduce an iterator/recursor
term, until the iterated functions are closed. One can ask a stronger
constraint on the construction of terms, that is, to constrain
iterators/recursors to be closed on construction (i.e., we have a
syntactical constraint that only terms without free variables are used
in this context).  For \LLCI, to follow the closed-construction approach
one imposes an extra condition on the
iterated function , when defining iterators:

For \LLCIrec, one imposes an extra condition on  and :


In the rest of this section we compare the computation power of linear calculi
with closed reduction vs closed construction. We consider first calculi
 with bounded recursion (iterators) and then unbounded recursion.

\subsection{Closed Reduction/Closed Construction and Iteration}
Dal Lago~\cite{Lago05} defines a linear -calculus with
bounded iteration that encodes exactly the set of primitive recursive
functions following the closed construction approach. 
A similar system allowing iterators to be open at
construction, but imposing a closed condition on reduction, allows to
encode more than the primitive recursive functions, and in particular
allows the encoding of the Ackermann function, as shown
in~\cite{AlvesFFM07}.
Thus, imposing a closed-at-construction restriction on iterators clearly has
an impact in the presence of linearity. 


For G\"odel's \ST, the fact that we do not allow iterators to be open
at construction, does not affect the set of definable functions.  If
we define , then each iterator term
 in \ST, where  may be an open term, can be
translated into the typable term ,
where . It is easy to see that  and
 have the same normal form . It is worth remarking that we rely on a non-linear term  to
get this result. Indeed, iterating  is essentially equivalent to
constructing a Church numeral.

For a linear system with iteration such as \LLCI, although some
functions are naturally defined using an open function, for example:
 one
can encode them using a closed-at-construction iteration.  In general,
an iterator with an open function where the free variables are of type
 can be encoded using a closed-at-construction iterator, as
follows.  Consider , where  is open, for free
variables  of type . Then let 

Then we simulate
 using a closed iterator as follows:
.

This technique can also be applied to open functions where the free
variables are of type , for  generated by the following
grammar: .  More generally, open
functions where the free variables have base type can be encoded when
we consider iteration closed-at-construction.


\subsection{Closed Construction and Unbounded Recursion}

We now consider what happens when we use the closed-at-construction
approach in a linear system with unbounded recursion such as \LLCIrec.

Notice that the encoding of  in  given in Section~\ref{sec:LLCIrec}
is a term
closed-at-construction.  Since all the primitive recursive functions
are definable using closed-at-construction iterators, which are
trivially encoded using closed  recursors, we conclude that
imposing a closed-at-construction condition on \LLCIrec still gives a
Turing complete system.

Note however that, although \LLCIrec can encode all the computable
functions, that does not mean one can encode all the computational
behaviours. For example for any closed function , one can encode in
 a term , such that, . However, this relies
on the fact that one can copy any closed function , which can be
done both in \LLCI and \LLCIrec with closed reduction, but so far
there is no known encoding when one imposes a closed-at-construction
condition.


\section{Conclusions}\label{sec:conc}
This paper completes a line of work investigating
the power of linear functions, from the set of primitive recursive
functions to the full set of computable functions, with a strong focus
on Turing complete systems based on linear calculi. In
previous work, we  investigated linear primitive recursive functions, and a
linear version of G\"odel's \ST. Here, we extended these notions to general
recursion, using iteration and minimisation () and, alternatively,
unbounded recursion ().  \LLCIrec is a syntactically linear
calculus, but only the fragment without the recursor is operationally
linear.  The linear recursor allows us to encode duplicating and
erasing, thus playing a similar role to the exponentials in linear
logic. It encompasses bounded recursion (iteration) and minimisation
in just one operator.
Summarising, a typed linear -calculus
with bounded iteration (\LLCI) is not Turing complete, but replacing
the iterator with an unbounded recursor (), or adding a
minimiser (), yields a universal system.

Linear calculi have been successfully used to characterise complexity
classes, for instance, as a consequence of Dal Lago's
results~\cite{Lago05}, we know that a closed-by-construction
discipline in \LLCI gives exactly the set of PR functions, whereas
closed reduction recovers the power of \ST. Interestingly, a
closed-construction discipline does not weaken  (the encoding
of  is closed).

The encoding of PCF in  is type-respecting, and  seems a
potentially useful intermediate language for compilation. The meaning of the linear recursor will be
further analysed in a denotational setting in future work and the
pragmatical impact of these results is currently being investigated
within the language Lilac~\cite{MackieIC:lilfpl}. 








\bibliography{bibfile}
\label{sect:bib}
\end{document}
