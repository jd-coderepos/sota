\RequirePackage[OT1]{fontenc}
\documentclass[a4paper, 10pt, conference]{ieeeconf}      

\IEEEoverridecommandlockouts                  

\overrideIEEEmargins  

\usepackage[british]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{csquotes}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amstext} \usepackage{array}   \usepackage{makecell}
\usepackage{booktabs}

\RequirePackage[dvipsnames]{xcolor}       

\RequirePackage{tabularx}
\RequirePackage{booktabs}

\RequirePackage{graphicx} \RequirePackage{subcaption}   

\usepackage{tikz}
\usepackage{tikzscale}
\usepackage{tikzpeople}
\usepackage{tikz-3dplot}
\usepackage{pgfplots}
\usetikzlibrary{
  arrows,
  arrows.meta,
  backgrounds, 
  calc, 
  chains,
  decorations.pathreplacing,
  fit, 
  math,
  matrix,
  patterns, 
  positioning, 
  shadows.blur,
  shapes, 
  shapes.arrows,
  shapes.multipart
  }
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{polar}

\definecolor{color_street}{RGB}{124,124,124}
\definecolor{color_border}{RGB}{56,56,56}
\definecolor{color_sidewalk}{RGB}{180,180,180}
\definecolor{light_blue}{RGB}{179, 217, 255} 
\definecolor{light_red}{RGB}{255, 153, 128}
\definecolor{light_green}{RGB}{173, 235, 173}

\definecolor{coco_joint_color_1}{RGB}{192,192,192}
\definecolor{coco_joint_color_2}{RGB}{255,255,0}
\definecolor{coco_joint_color_3}{RGB}{255,0,255}
\definecolor{coco_joint_color_4}{RGB}{255,255,102}
\definecolor{coco_joint_color_5}{RGB}{255,102,255}
\definecolor{coco_joint_color_6}{RGB}{0,255,0}
\definecolor{coco_joint_color_7}{RGB}{255,0,0}
\definecolor{coco_joint_color_8}{RGB}{51,255,51}
\definecolor{coco_joint_color_9}{RGB}{255,51,51}
\definecolor{coco_joint_color_10}{RGB}{102,255,102}
\definecolor{coco_joint_color_11}{RGB}{255,102,102}
\definecolor{coco_joint_color_12}{RGB}{0,102,0}
\definecolor{coco_joint_color_13}{RGB}{102,0,0}
\definecolor{coco_joint_color_14}{RGB}{0,153,0}
\definecolor{coco_joint_color_15}{RGB}{153,0,0}
\definecolor{coco_joint_color_16}{RGB}{0,204,0}
\definecolor{coco_joint_color_17}{RGB}{204,0,0}

\definecolor{coco_limb_color_1}{RGB}{252, 157, 154}
\definecolor{coco_limb_color_2}{RGB}{200, 255, 0}
\definecolor{coco_limb_color_3}{RGB}{252, 157, 154}
\definecolor{coco_limb_color_4}{RGB}{252, 157, 154}
\definecolor{coco_limb_color_5}{RGB}{200, 255, 0}
\definecolor{coco_limb_color_6}{RGB}{200, 255, 0}
\definecolor{coco_limb_color_7}{RGB}{124, 244, 154}
\definecolor{coco_limb_color_8}{RGB}{250, 2, 60}
\definecolor{coco_limb_color_9}{RGB}{250, 2, 60}
\definecolor{coco_limb_color_10}{RGB}{250, 2, 60}
\definecolor{coco_limb_color_11}{RGB}{124, 244, 154}
\definecolor{coco_limb_color_12}{RGB}{124, 244, 154}
\definecolor{coco_limb_color_13}{RGB}{252, 157, 154}
\definecolor{coco_limb_color_14}{RGB}{252, 157, 154}
\definecolor{coco_limb_color_15}{RGB}{200, 255, 0}
\definecolor{coco_limb_color_16}{RGB}{200, 255, 0}

\definecolor{pedrec_joint_color_1}{RGB}{192,192,192}  \definecolor{pedrec_joint_color_2}{RGB}{255,255,0}    \definecolor{pedrec_joint_color_3}{RGB}{255,0,255}    \definecolor{pedrec_joint_color_4}{RGB}{255,255,102}  \definecolor{pedrec_joint_color_5}{RGB}{255,102,255}  \definecolor{pedrec_joint_color_6}{RGB}{0,255,0}      \definecolor{pedrec_joint_color_7}{RGB}{255,0,0}      \definecolor{pedrec_joint_color_8}{RGB}{51,255,51}    \definecolor{pedrec_joint_color_9}{RGB}{255,51,51}    \definecolor{pedrec_joint_color_10}{RGB}{102,255,102} \definecolor{pedrec_joint_color_11}{RGB}{255,102,102} \definecolor{pedrec_joint_color_12}{RGB}{0,102,0}     \definecolor{pedrec_joint_color_13}{RGB}{102,0,0}     \definecolor{pedrec_joint_color_14}{RGB}{0,153,0}     \definecolor{pedrec_joint_color_15}{RGB}{153,0,0}     \definecolor{pedrec_joint_color_16}{RGB}{0,204,0}     \definecolor{pedrec_joint_color_17}{RGB}{204,0,0}     \definecolor{pedrec_joint_color_18}{RGB}{0,0,0}       \definecolor{pedrec_joint_color_19}{RGB}{51,51,51}    \definecolor{pedrec_joint_color_20}{RGB}{101,101,101} \definecolor{pedrec_joint_color_21}{RGB}{148,148,148} \definecolor{pedrec_joint_color_22}{RGB}{224,224,224} \definecolor{pedrec_joint_color_23}{RGB}{0,255,0}     \definecolor{pedrec_joint_color_24}{RGB}{255,0,0}     \definecolor{pedrec_joint_color_25}{RGB}{152,255,152} \definecolor{pedrec_joint_color_26}{RGB}{255,152,152} 

\definecolor{pedrec_limb_color_1}{RGB}{252,157,154}  \definecolor{pedrec_limb_color_2}{RGB}{252,157,154}  \definecolor{pedrec_limb_color_3}{RGB}{200,255,0}    \definecolor{pedrec_limb_color_4}{RGB}{200,255,0}    \definecolor{pedrec_limb_color_5}{RGB}{250,2,60}     \definecolor{pedrec_limb_color_6}{RGB}{250,2,60}     \definecolor{pedrec_limb_color_7}{RGB}{124,244,154}  \definecolor{pedrec_limb_color_8}{RGB}{124,244,154}  \definecolor{pedrec_limb_color_9}{RGB}{252,157,154}  \definecolor{pedrec_limb_color_10}{RGB}{252,157,154} \definecolor{pedrec_limb_color_11}{RGB}{200,255,0}   \definecolor{pedrec_limb_color_12}{RGB}{200,255,0}   \definecolor{pedrec_limb_color_13}{RGB}{124,244,154} \definecolor{pedrec_limb_color_14}{RGB}{250,2,60}    \definecolor{pedrec_limb_color_15}{RGB}{124,124,124} \definecolor{pedrec_limb_color_16}{RGB}{124,124,124} \definecolor{pedrec_limb_color_17}{RGB}{124,124,124} \definecolor{pedrec_limb_color_18}{RGB}{124,124,124} \definecolor{pedrec_limb_color_19}{RGB}{200,255,0}   \definecolor{pedrec_limb_color_20}{RGB}{252,157,154} \definecolor{pedrec_limb_color_21}{RGB}{200,200,200} \definecolor{pedrec_limb_color_22}{RGB}{124,244,154} \definecolor{pedrec_limb_color_23}{RGB}{250,2,60}    \definecolor{pedrec_limb_color_24}{RGB}{200,255,0}   \definecolor{pedrec_limb_color_25}{RGB}{252,157,154} 

\definecolor{coco_skel_1}{RGB}{255, 0, 0}
\definecolor{coco_skel_2}{RGB}{0, 255, 0}

\pgfkeys{tikz/matrixgrid/.style={
  matrix of nodes,
  nodes in empty cells,
  column sep      = -\pgflinewidth,
  row sep         = -\pgflinewidth,
  nodes={inner sep=0mm,outer sep=0pt,
    minimum size=6mm,
    text height=\ht\strutbox,text depth=\dp\strutbox,
    draw
  }
}}

\pgfkeys{tikz/matrixbrace/.style={decorate, decoration={brace}, thick}}

\pgfkeys{tikz/matrix-area-marker/.style={
  fill,
  opacity=0.2,
  rounded corners,
  inner sep=-1pt,
  line width=1pt
}}

\pgfkeys{tikz/matrix-delimiter/.style={
  left delimiter=[,
  right delimiter=],
  inner sep=-2pt
}}

\pgfkeys{tikz/matrix-marker/.style={
  line cap= round,
  line width =15pt,
  opacity=0.2
}}

\pgfkeys{tikz/matrix-circle-marker/.style={
  fill,
  opacity=0.2,
  circle,
  inner sep=-1pt,
  line width=1pt
}}

\pgfdeclarelayer{bg}
\pgfdeclarelayer{layer1}
\pgfdeclarelayer{layer2}
\pgfdeclarelayer{layer3}
\pgfsetlayers{background,bg,main,layer1,layer2,layer3}



\newcommand*\matrixbraceleft[4][m]{
    \draw[matrixbrace] (#1.west|-#1-#3-1.south west) -- node[left=2pt] {#4} (#1.west|-#1-#2-1.north west);
}
\newcommand*\matrixbraceright[4][m]{
    \draw[matrixbrace] (#1.east|-#1-#2-1.north east) -- node[right=2pt] {#4} (#1.east|-#1-#3-1.south east);
}
\newcommand*\matrixbracetop[4][m]{
    \draw[matrixbrace] (#1.north-|#1-1-#2.north west) -- node[above=2pt] {#4} (#1.north-|#1-1-#3.north east);
}
\newcommand*\matrixbracebottom[4][m]{
    \draw[matrixbrace] (#1.south-|#1-1-#2.north east) -- node[below=2pt] {#4} (#1.south-|#1-1-#3.north west);
}

\newcommand\pgfmathsinandcos[3]{\pgfmathsetmacro#1{sin(#3)}\pgfmathsetmacro#2{cos(#3)}} 
\newcommand\LongitudePlane[3][current plane]{\pgfmathsinandcos\sinEl\cosEl{#2} \pgfmathsinandcos\sint\cost{#3} \tikzset{#1/.estyle={cm={\cost,\sint*\sinEl,0,\cosEl,(0,0)}}}
}
\newcommand\LatitudePlane[3][current plane]{\pgfmathsinandcos\sinEl\cosEl{#2} \pgfmathsinandcos\sint\cost{#3} \pgfmathsetmacro\yshift{\cosEl*\sint}
  \tikzset{#1/.estyle={cm={\cost,0,0,\cost*\sinEl,(0,\yshift)}}} }
\NewDocumentCommand{\DrawLongitudeCircle}{O{1} O{black} m}{
  \LongitudePlane{\angEl}{#3}
  \tikzset{current plane/.prefix style={scale=#1}}
\pgfmathsetmacro\angVis{atan(sin(#3)*cos(\angEl)/sin(\angEl))} \draw[current plane] (\angVis:1) arc (\angVis:\angVis+180:1); 
  \draw[current plane,dashed] (\angVis-180:1) arc (\angVis-180:\angVis:1); 
}
\NewDocumentCommand{\DrawLatitudeCircle}{ O{1} O{black} O{0.5} m }{
  \LatitudePlane{\angEl}{#4}
  \tikzset{current plane/.prefix style={scale=#1}}
  \pgfmathsetmacro\sinVis{sin(#4)/cos(#4)*sin(\angEl)/cos(\angEl)} 
\pgfmathsetmacro\angVis{asin(min(1,max(\sinVis,-1)))}  
  \draw[current plane,#2, line width=#3] (\angVis:1) arc (\angVis:-\angVis-180:1); 
  \draw[current plane,dashed, #2, line width=#3] (180-\angVis:1) arc (180-\angVis:\angVis:1);
}

\makeatletter \newcommand{\pgfplotsdrawaxis}{\pgfplots@draw@axis} \makeatother
\pgfplotsset{axis line on top/.style={
  axis line style=transparent,
  ticklabel style=transparent,
  tick style=transparent,
  axis on top=false,
  after end axis/.append code={
    \pgfplotsset{axis line style=opaque,
      ticklabel style=opaque,
      tick style=opaque,
      grid=none}
    \pgfplotsdrawaxis}
  }
}


\makeatletter
\long\def\ifnodedefined#1#2#3{\@ifundefined{pgf@sh@ns@#1}{#3}{#2}}
\makeatother

\newcolumntype{L}{>{}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg}
\graphicspath{{figures/}{pictures/}{images/}{./}} 


\title{\LARGE \bf
PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation
}

\author{Dennis Burgermeister and Crist\'obal Curio\\
	\scriptsize \\
\thanks{D. Burgermeister and C. Curio are with the Cognitive Systems Group, Computer Science Department, Reutlingen University, Germany.
{\tt\small \{Dennis.Burgermeister, Cristobal.Curio\}@Reutlingen-University.de}
}}


\begin{document}

\maketitle

\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
We present a multitask network that supports various deep neural network based pedestrian detection functions. Besides 2D and 3D human pose, it also supports body and head orientation estimation based on full body bounding box input. This eliminates the need for explicit face recognition. We show that the performance of 3D human pose estimation and orientation estimation is comparable to the state-of-the-art. Since very few data sets exist for 3D human pose and in particular body and head orientation estimation based on full body data, we further show the benefit of particular simulation data to train the network. The network architecture is relatively simple, yet powerful, and easily adaptable for further research and applications.
\end{abstract}

\section{Introduction}
The detection of pedestrians as well as their behavior remains a challenge in the field of autonomous driving. Previous work shows how to detect pedestrian actions using 2D pose recognition~\cite{ludlSimpleEfficientRealtime2019}. In this work, we present a new multitask network, PedRecNet, that can estimate 3D poses in addition to 2D poses. 3D poses bring in the benefit of multiple perspectives on the skeleton, enabling detection of movement changes which may not be visible in 2D projections~\cite{bulthoffTopdownInfluencesStereoscopic1998}. In addition to 3D pose data, orientation information about a person's body and head is also relevant for human recognition systems. Especially in pedestrian recognition, this information can be valuable to perform path planning or to detect if a pedestrian notices a vehicle or not. Since 3D pose recognition and body as well as head orientation estimation are related it seems beneficial to bring this tasks together in one versatile network. All tasks in the PedRecNet are based on the same input data, so all tasks should be implemented in the same network using a multitask approach. Since there are only a few real datasets available for 3D pose recognition and especially for body and head orientation estimation, we use simulation data to improve these parts of the PedRecNet and to enable training in the first place. The skeleton-based action recognition results presented by \cite{ludlSimpleEfficientRealtime2019} and \cite{ludlEnhancingDataDrivenAlgorithms2020} support the assumption that using abstract pose information rather than just visual information enables the transfer of simulated training data to real data. We will corroborate this work hypothesis in more detail in experiments using simulated training data. In the following, we describe the developed network, the datasets used, and evaluate 3D human pose recognition as well as the body orientation estimation on several real and simulated datasets.

The entire system and novel simulation data has been made public under the MIT license\footnote{\url{https://github.com/noboevbo/PedRec} (accessed on 2021-09-02)}.

Our contributions in this work are:

\begin{enumerate}
\item A multi-task network which supports 2D and 3D human pose estimation, as well as body and head orientation estimation on cropped full body input data.
\item An approach to 3D human pose estimation in which 3D human joint positions are encoded in the skeletal coordinate system. This makes the skeleton estimation independent of the camera parameters and can thus be better used in follow-up applications, such as action recognition which uses temporal data.
\item An integrated approach to body and head orientation estimation based on whole body bounding box input. This eliminates the need for face recognition to obtain a crop of the head bounding box.
\item Simulation data that provide, in particular, accurate head and body orientation data that are not available in standard data sets.
\end{enumerate}


\section{Related work}
Besides direct estimation of 3D human joint positions by a deep neural network~\cite{mehtaVNectRealtime3D2017,mehtaSingleShotMultiperson3D2018,mehtaXNectRealtimeMultiperson2020,luvizonMultitaskDeepLearning2020} alternative approaches exist that first estimate 2D poses followed by 3D pose regression~\cite{chen3DHumanPose2017,pavllo3DHumanPose2019,tekinLearningFuse2D2017,zhou3DHumanPose2017,kolotourosLearningReconstruct3D2019}. Some approaches show the application of model-based approaches~\cite{bogoKeepItSMPL2016,zhouDeepKinematicPose2016,mehtaVNectRealtime3D2017,mehtaXNectRealtimeMultiperson2020,kolotourosLearningReconstruct3D2019} to further improve a recognized 3D skeleton. The categorization in bottom-up~\cite{mehtaSingleShotMultiperson3D2018} and top-down approaches~\cite{luvizon2D3DPose2018,mehtaVNectRealtime3D2017,mehtaXNectRealtimeMultiperson2020} is also valid for 3D human pose estimation. Mehta~\textit{et al.} show an approach predicting three location-maps for the ,  and  position parameters per body joint \cite{mehtaVNectRealtime3D2017}. Those location maps encode the distance in , , or  direction from the coordinate root (hip center). The location of a joint on those location maps is retrieved from 2D pose heatmaps. The results are refined using a kinematic skeleton fitting method. They have also shown how to apply location maps in a bottom-up approach which also handles occlusion better by using redundancy in so-called occlusion-robust pose-maps by representing the decomposed body as torso, limbs, and heads \cite{mehtaSingleShotMultiperson3D2018}. Luvizon \textit{et al.} show a similar approach to encode depth information in a heatmap but use it only for the depth estimation~\cite{luvizonMultitaskDeepLearning2020}. It is also possible to directly regress 3D poses from 2D heatmaps \cite{tekinLearningFuse2D2017} or directly from 2D pose coordinates \cite{martinezSimpleEffectiveBaseline2017} which improves when using multiple frames as input \cite{pavllo3DHumanPose2019}. Another approach to retrieve 3D pose information from 2D poses is 3D catalog matching \cite{chen3DHumanPose2017}. Such approaches rely more heavily on the 2D human pose estimator's output than approaches that also use visual input. Kolotouros \textit{et al.} show how to reconstruct a volumetric model by estimating the parameters for the SMPL statistical body shape model~\cite{loperSMPLSkinnedMultiperson2015} and further improve the model by iteratively fitting on 2D joints~\cite{kolotourosLearningReconstruct3D2019}. We present an approach similar to \cite{luvizonMultitaskDeepLearning2020}, but with a straightforward and performant method to retrieve the pose and depth heatmaps in section \ref{sec:architecture}.

Body and head orientation estimation approaches are usually handled as separate problems. The estimation of body orientation often originate in pedestrian-related works. Classical approaches often used classification of body parts, for example, by using a part descriptor in a sliding window fashion to classify position, scale, and orientation of body parts \cite{andrilukaPictorialStructuresRevisited2009}. Another approach focuses on combining pedestrian orientation and classification by clustering pedestrians in the four categories front, back, left, and right and train classification networks on those clusters, which combined scores serve as the full pedestrian classification \cite{enzweilerIntegratedPedestrianClassification2010}. Another approach uses specific detectors for head and body orientation which are converted to a full continuous probability density function and stabilized over time by particle filtering \cite{flohrJointProbabilisticPedestrian2014,flohrProbabilisticFrameworkJoint2015}. The authors also discretized the orientation space to 45-degree bins and used a HOG/linSVM based classification system \cite{flohrJointProbabilisticPedestrian2014}. There is a lot of recent head orientation work using deep learning \cite{guptaNoseEyesEars2019,ruizFineGrainedHeadPose2018,panSelfPacedDeepRegression2020,huDeepConvolutionalNeural2021,valleMultitaskHeadPose2020,xiaEfficientMultitaskNeural2021}, which usually takes a head bounding box as input and thus is an additional step in the recognition pipeline. Such methods require a head bounding box with a reasonable resolution and thus high-resolution sensors or people not at distance from the camera sensor. Work on body orientation or combined body and head orientation estimation has not yet transitioned well to deep learning approaches. One possible reason could be the lack of appropriate datasets. There are not many standard datasets, and the existing ones are rather small and thus not suitable to train deep neural networks. Heo~\textit{et al.} try to overcome this issue on body orientation estimation by using a teacher-student learning framework in which they train a teacher network with labeled data and use this network to generate labels for an unlabeled dataset with which the student network is trained \cite{heoEstimationPedestrianPose2019}. They have also discretized the output orientation in 45-degree bins, turning the problem into a classification problem \cite{heoEstimationPedestrianPose2019}. Another work uses CNNs in a random forest that focuses on different body and head parts to recognize the human body and head orientation, with a focus on head orientation \cite{leeHeadBodyOrientation2019}. Steinhoff and Göhring propose the usage of IMUs to generate more labeled training data for body and head orientation tasks, but IMU-based approaches are usually hard to sync, suffer from error accumulation, and do not contain global reference points \cite{steinhoffPedestrianHeadBody2020}. Wu~\textit{et al.} propose the application of 3D human pose estimation approaches as a basis for body and head pose estimation using a 3D pose estimation network as a backbone for a classification header which classifies the input in  orientation bins \cite{wuMEBOWMonocularEstimation2020}. We propose a regression approach which is based on full human 3D pose information, described in section \ref{sec:architecture}. Regressed orientation estimation offer various benefits, for example in time-dependent fine-grained actions like the head movement during looking for traffic. We show how to train such a network with simulated data to overcome the deficient number of labeled data in this field. 

\section{Method}
\label{sec:architecture}
\begin{figure}[bth] 
  \centering    
  \resizebox{1\columnwidth}{!}{\begin{tikzpicture}[node distance=-0.025]
  \definecolor{fc}{HTML}{DB900B}

\definecolor{conv}{HTML}{0C65F2}
\definecolor{convt}{HTML}{0CDEF2}

\definecolor{pool}{HTML}{A5DFCB}
\definecolor{bn}{HTML}{DEAFC0}
\definecolor{dropout}{HTML}{91765D}

\definecolor{flatten}{HTML}{D9D9D9}

\definecolor{relu}{HTML}{FFBE99}
\definecolor{sigmoid}{HTML}{2B8F8E}

\definecolor{resnet}{HTML}{99A2FF}

\definecolor{softargmax}{HTML}{EB4C38}
\definecolor{softmax}{HTML}{EE8D81}

\tikzset{default/.style={black,draw=black,rectangle,minimum height=1cm, minimum width=0.2cm}}
\tikzset{fc/.style={default,fill=fc}}
\tikzset{conv/.style={default,fill=conv}}
\tikzset{convt/.style={default,fill=convt}}
\tikzset{pool/.style={default,fill=pool}}
\tikzset{bn/.style={default,fill=bn}}
\tikzset{dropout/.style={default,fill=dropout}}
\tikzset{flatten/.style={default,fill=flatten}}
\tikzset{relu/.style={default,fill=relu}}
\tikzset{sigmoid/.style={default,fill=sigmoid}}
\tikzset{softmax/.style={default,fill=softmax}}
\tikzset{softargmax/.style={default,fill=softargmax}}

  \node (x) at (0,0) {};
  \node[black,fill=resnet,rotate=90,draw=black,rectangle,minimum height=1cm, minimum width=1cm] (resnet) at (1,0) {\small ResNet50};
  \node (x_hat) at (2,0) {}; 
  \node[convt, right=of x_hat, xshift=0.25cm] (deconv_convt1) {};
  \node[bn, right=of deconv_convt1] (deconv_bn1) {};
  \node[relu, right=of deconv_bn1] (deconv_relu1) {};

  \node[convt, right=of deconv_relu1, xshift=0.25cm] (deconv_convt2) {};
  \node[bn, right=of deconv_convt2] (deconv_bn2) {};
  \node[relu, right=of deconv_bn2] (deconv_relu2) {};

  \coordinate[right=of deconv_relu2, xshift=0.25cm] (p2dp3dsplit) {};


\node[convt, right=of p2dp3dsplit, xshift=0.25cm, yshift=1.5cm] (pose2d_convt1) {};
  \node[bn, right=of pose2d_convt1] (pose2d_bn) {};
  \node[relu, right=of pose2d_bn] (pose2d_relu) {};


  \node[conv, right=of pose2d_relu, xshift=0.25cm] (pose2d_conv) {};
  \node[softmax, right=of pose2d_conv] (pose2d_softmax) {};
  \node[softargmax, right=of pose2d_softmax] (pose2d_softargmax) {};
  \node[right=of pose2d_softargmax, xshift=0.25cm] (loss_p2d) {};

\node[convt, right=of p2dp3dsplit, xshift=0.25cm, yshift=-1.5cm] (pose3d_convt1) {};
  \node[bn, right=of pose3d_convt1] (pose3d_bn) {};
  \node[relu, right=of pose3d_bn] (pose3d_relu) {};

  \node[conv, right=of pose3d_relu, xshift=0.25cm] (pose3d_conv) {};

  \node[conv, below=of pose3d_conv, yshift=-0.25cm] (pose3d_depth_conv) {};
  \node[sigmoid, right=of pose3d_depth_conv] (pose3d_depth_sigmoid) {};

  \node[draw=black, minimum height=0.7cm, circle,right=of pose3d_depth_sigmoid, xshift=0.25cm] (p3d_combine) {\tiny};

  \node[softmax] (pose3d_softmax) at (pose3d_conv -| p3d_combine) {};
  \node[softargmax, right=of pose3d_softmax] (pose3d_softargmax) {};

  \node[draw=black,circle, minimum height=0.7cm,right=of p3d_combine, xshift=0.25cm] (p3d_sum) {\tiny};

  \node[draw=black, minimum height=0.7cm,circle] (p3d_concat) at (pose3d_softargmax -| p3d_sum) {\tiny};


\node[right=of p3d_concat, xshift=0.25cm] (loss_p3d) {};



\node[draw=black, minimum height=0.7cm, circle,below=of pose2d_conv, yshift=0.34cm] (p2dp3d_concat) at (x -| pose3d_conv) {\tiny};
  \node[conv, right=of p2dp3d_concat, xshift=0.5cm] (conf_conv1)  {};
  \node[relu, right=of conf_conv1] (conf_relu1) {};
  \node[conv, right=of conf_relu1] (conf_conv2) {};
  \node[relu, right=of conf_conv2] (conf_relu2) {};
  \node[pool, right=of conf_relu2] (conf_pool1) {};
  \node[dropout, right=of conf_pool1, xshift=0.25cm] (conf_dropout1) {};
  \node[flatten, right=of conf_dropout1] (conf_flatten1) {};
  \node[fc, right=of conf_flatten1, xshift=0.25cm] (conf_fc1) {};
  \node[dropout, right=of conf_fc1] (conf_dropout2) {};
  \node[fc, right=of conf_dropout2] (conf_fc2) {};
  \node[sigmoid, right=of conf_fc2] (conf_sigmoid) {};
  \node[right=of conf_sigmoid, xshift=0.25cm] (loss_conf) {};

\node[convt, below=of pose3d_softargmax, yshift=-2cm] (orientation_convt1) {};
  \node[bn, right=of orientation_convt1] (orientation_bn1) {};
  \node[relu, right=of orientation_bn1] (orientation_relu1) {};
  \node[convt, right=of orientation_relu1, xshift=0.25cm] (orientation_convt2) {};
  \node[bn, right=of orientation_convt2] (orientation_bn2) {};
  \node[relu, right=of orientation_bn2] (orientation_relu2) {};
  \node[convt, right=of orientation_relu2, xshift=0.25cm] (orientation_convt3) {};
  \node[bn, right=of orientation_convt3] (orientation_bn3) {};
  \node[relu, right=of orientation_bn3] (orientation_relu3) {};

  \node[draw=black, minimum height=0.7cm,circle,right=of orientation_relu3,xshift=0.25cm] (orientation_concat) {\tiny};
  \coordinate[right=of orientation_concat,xshift=0.25cm] (orientation_split) {};

\node[fc, right=of orientation_split,xshift=0.25cm,yshift=2.25cm] (orientation_fc1) {};
  \node[softmax, right=of orientation_fc1] (orientation_softmax1) {};
  \node[softargmax, right=of orientation_softmax1] (orientation_softargmax1) {};

\node[fc, right=of orientation_split,xshift=0.25cm,yshift=0.75cm] (orientation_fc2) {};
  \node[softmax, right=of orientation_fc2] (orientation_softmax2) {};
  \node[softargmax, right=of orientation_softmax2] (orientation_softargmax2) {};

\node[fc, right=of orientation_split,xshift=0.25cm,yshift=-
  0.75cm] (orientation2_fc1) {};
  \node[softmax, right=of orientation2_fc1] (orientation2_softmax1) {};
  \node[softargmax, right=of orientation2_softmax1] (orientation2_softargmax1) {};

\node[fc, right=of orientation_split,xshift=0.25cm,yshift=-2.25cm] (orientation2_fc2) {};
  \node[softmax, right=of orientation2_fc2] (orientation2_softmax2) {};
  \node[softargmax, right=of orientation2_softmax2] (orientation2_softargmax2) {};

  \node[draw=black, minimum height=0.7cm,circle,right=of orientation_fc1,xshift=0.5cm,yshift=-0.75cm] (orientation_concat2) {\tiny};
  \node[draw=black, minimum height=0.7cm,circle,right=of orientation2_fc1,xshift=0.5cm,yshift=-0.75cm] (orientation2_concat3) {\tiny};
  \node[draw=black, minimum height=0.7cm,circle,right=of orientation_concat2,xshift=0.5cm,yshift=-1.5cm] (orientation_concat4) {\tiny};
  \node[right=of orientation_concat4, xshift=0.25cm] (loss_orientation) {};

  \draw[->] (x) -- (resnet);
  \draw[->] (resnet) -- (x_hat);
  \draw[->] (x_hat) -- (deconv_convt1);
  \draw[->] (deconv_relu1) -- (deconv_convt2);
  \draw[] (deconv_relu2) -- (p2dp3dsplit);

  \draw[->] (p2dp3dsplit) |- (pose2d_convt1);
  \draw[->] (p2dp3dsplit) |- (pose3d_convt1);

  \draw[->] (pose2d_relu) -- (pose2d_conv);
  \draw[->] (pose2d_softargmax) -- (loss_p2d);

  \draw[->] (pose3d_relu) -- (pose3d_conv);
  \draw[->] (pose3d_relu) |- (pose3d_depth_conv);

  \draw[->] (pose3d_conv) -- (pose3d_softmax);
  \draw[->] (pose3d_softmax) -- (p3d_combine);
  \draw[->] (pose3d_softargmax) -- (p3d_concat);
  \draw[->] (pose3d_depth_sigmoid) -- (p3d_combine);
  \draw[->] (p3d_combine) -- (p3d_sum);
  \draw[->] (p3d_sum) -- (p3d_concat);
  \draw[->] (p3d_concat) -- (loss_p3d);

  \draw[->, dotted] (pose2d_conv) -- (p2dp3d_concat);
  \draw[->, dotted] (pose3d_conv) -- (p2dp3d_concat);
\draw[->, dotted] (p2dp3d_concat) -- (conf_conv1);
\draw[->] (conf_pool1) -- (conf_dropout1);
  \draw[->] (conf_flatten1) -- (conf_fc1);
  \draw[->] (conf_sigmoid) -- (loss_conf);

  \coordinate[below=of orientation_convt1, yshift=-0.25cm] (below_orientation_convt) {};
  \coordinate (below_orientation_concat) at (below_orientation_convt -| orientation_concat) {};
  \draw[->] (x_hat.south) -- (below_orientation_convt) -- (below_orientation_concat) -- (orientation_concat);
  \draw[->, dotted] (pose3d_softargmax) -- (orientation_convt1);
\draw[->] (orientation_relu1) -- (orientation_convt2);
  \draw[->] (orientation_relu2) -- (orientation_convt3);
  \draw[->] (orientation_relu3) -- (orientation_concat);
  \draw[] (orientation_concat) -- (orientation_split);
  \draw[->] (orientation_split) |- (orientation_fc1);
  \draw[->] (orientation_split) |- (orientation_fc2);
  \draw[->] (orientation_split) |- (orientation2_fc1);
  \draw[->] (orientation_split) |- (orientation2_fc2);
  \draw[->] (orientation_softargmax1) -| (orientation_concat2);
  \draw[->] (orientation_softargmax2) -| (orientation_concat2);
  \draw[->] (orientation_concat2) -| (orientation_concat4);

\draw[->] (orientation2_softargmax1) -| (orientation2_concat3);
  \draw[->] (orientation2_softargmax2) -| (orientation2_concat3);
  \draw[->] (orientation2_concat3) -| (orientation_concat4);

  \draw[->] (orientation_concat4) -- (loss_orientation);

  \coordinate (resnet_to_orientation_coord) at (resnet |- loss_orientation) {};




  \node[conv, minimum height=0.2, below=of resnet_to_orientation_coord, yshift=-0.5cm, xshift=-1cm, label={right:conv}] (conv_label) {};
  \node[convt, minimum height=0.2, below=of conv_label, yshift=-0.15cm, label={right:conv transposed}] (convt_label) {};
  \node[fc, minimum height=0.2, below=of convt_label, yshift=-0.15cm, label={right:fully connected}] (fc_label) {};
  \node[softmax, minimum height=0.2, below=of fc_label, yshift=-0.15cm, label={right:Softmax}] (softmax_label) {};
  \node[softargmax, minimum height=0.2, below=of softmax_label, yshift=-0.15cm, label={right:Softargmax}] (softargmax_label) {};
  \node[flatten, minimum height=0.2, below=of softargmax_label, yshift=-0.15cm, label={right:flatten}] (flatten_label) {};
  \node[minimum height=0.2, minimum height=0.2, below=of flatten_label, yshift=-0.15cm, label={right:with gradient}] (wgradient_label) {};
  \draw (wgradient_label.west) -- (wgradient_label.east);
  \node[draw=black, minimum height=0.7cm, circle, below=of wgradient_label, yshift=-0.15cm, label={right:concatenation}] (concat_label) {\tiny};
  \node[draw=black, minimum height=0.7cm, circle, below=of concat_label, yshift=-0.15cm, label={right:product}] (prod_label) {\tiny};

  \node[pool, minimum height=0.2, right=of conv_label, xshift=3.2cm, label={right:pool}] (pool_label) {};
  \node[bn, minimum height=0.2, below=of pool_label, yshift=-0.15cm, label={right:batch norm}] (bn_label) {};
  \node[dropout, minimum height=0.2, below=of bn_label, yshift=-0.15cm, label={right:dropout}] (dropout_label) {};
  \node[relu, minimum height=0.2, below=of dropout_label, yshift=-0.15cm, label={right:relu}] (relu_label) {};
  \node[sigmoid, minimum height=0.2, below=of relu_label, yshift=-0.15cm, label={right:sigmoid}] (sigmoid_label) {};
  \node[minimum height=0.2, minimum height=0.2, below=of sigmoid_label, yshift=-0.15cm, label={right:without gradient}] (wogradient_label) {};
  \draw[dotted] (wogradient_label.west) -- (wogradient_label.east);
  \node[below=of wogradient_label, label={right:feature vector}] (x_hat_label) {};
  \node[draw=black, minimum height=0.7cm, circle, below=of wogradient_label, yshift=-0.6cm, label={right:sum}] (sum_label) {\tiny};

\end{tikzpicture} }
  \caption[Simplified PedRecNet architecture]{Simplified PedRecNet architecture. The input  is an RGB cropped bounding box image of a human. The dotted connection lines indicate connections without gradient flow in the backward pass.}
  \label{fig:pedrecnet}
\end{figure}

The overall network architecture is shown in Figure \ref{fig:pedrecnet}. The PedRecNet expands the 2D human pose estimation approach proposed by Xiao~\textit{et al.}~\cite{xiaoSimpleBaselinesHuman2018}. The PedRecNet architecture is based on a ResNet50 backbone for feature extraction but other backbones could be used as well. The ResNet50 architecture was chosen as a compromise between accuracy and performance. The inputs  are always images cropped to the size of certain bounding boxes. First features  are extracted using the feature extraction part of the network. Next, the 2D human pose estimation part is based on three transpose convolution blocks with which joint heatmaps are generated from the extracted features~\cite{xiaoSimpleBaselinesHuman2018}. 

In PedRecNet, the 2D human pose estimation architecture was extended to include 3D human pose estimation. For this purpose, two transpose convolution blocks are used as a common basis and then split into a 2D and a 3D path. These have basically the same structure. The output in the 2D path corresponds to 2D image coordinates. The 3D path, leading to , corresponds to the estimation of the  and  coordinates of a joint relative to the hip and additional depth estimation of the  coordinate using a sigmoid map. Another change from the previous 2D pose estimation approach is the post-processing of the heatmaps. In the approach shown in \cite{ludlSimpleEfficientRealtime2019}, the heatmaps were output from the network, and a non-maximum suppression (NMS) was used to determine the coordinates. This has the disadvantage that the subpixel coordinate values are not available in the network. The NMS approach also implies that artificial heatmaps have to be generated to be used as training data labels. The PedRecNet applies a softargmax layer, which determines the coordinates from the heatmaps in the network inside the network. This allows to use estimated joint coordinates as inputs into other parts of the network. For example, the 3D joint positions are used as input into the orientation estimation part of the network, which is visualized as the path to . The 3D joint position coordinates are scaled up into a feature space via 1D transpose convolution blocks. These are concatenated with the visual features  such that the orientation estimation can use direct information from the image in addition to the, noisy and potentially erroneous, 3D pose. This concatenated feature vector is input into a fully connected layer which generates a one-dimensional heatmap for each, the polar angle  and azimuthal angle  (see Figure~\ref{fig:camera_sphere}) of the body as well as the head. From these one-dimensional orientation heatmaps, the corresponding normalized angle is extracted using 1D softargmax. To classify the visibility of labels, we added a standard classification head to the network leading to the path to .

\begin{figure}[bth] 
  \centering    
  \resizebox{0.8\columnwidth}{!}{\begin{tikzpicture} \tikzset{>=latex, inner sep=0pt,outer sep=2pt,mark coordinate/.style={inner sep=0pt,outer sep=0pt,minimum size=3pt,fill=black,circle}}


\def\radius{3} \def\angEl{20} \def\angAz{0} \def\angPhiOne{220} \def\angThetaOne{20} 



\pgfmathsetmacro\H{\radius*cos(\angEl)} \LongitudePlane[xzplane]{\angEl}{\angAz}
\LongitudePlane[pzplane]{\angEl}{\angPhiOne}
\LatitudePlane[equator]{\angEl}{0}



\draw (-3,0) arc(180:0:3);

\DrawLatitudeCircle[\radius]{0} 

\coordinate[mark coordinate] (origin) at (0,0);
\path[xzplane] (\radius,0) coordinate[mark coordinate] (PHI_0) node[right=0 of PHI_0] () {};
\path[xzplane] (0,\radius) coordinate[mark coordinate] (THETA_0) node[above=0.25 of THETA_0] () {};
\draw[dotted] (origin) -- (PHI_0) node[midway, above] (MID_PHI_0) {};
\draw[dotted] (origin) -- (THETA_0) coordinate[midway] (MID_THETA_0) coordinate[pos=0.55] (NEAR_START_THETA_0);

\path[pzplane] (\angThetaOne:\radius) coordinate[mark coordinate, RedOrange] (C1);
\path[pzplane] (\radius, 0) coordinate (C1_equator);
\draw[dotted, RedOrange, thick] (origin) -- (C1_equator);
\draw[->, RedOrange, thick] (origin) -- (C1) coordinate[midway] (MID_C1);
\draw[->,thin, RedOrange, thick] (MID_THETA_0) to[bend right=15] node[midway,above] {} (MID_C1);
\draw[equator,->, RedOrange, very thick] (\angPhiOne - 20:\radius) arc (\angPhiOne - 20:\angPhiOne:\radius) node[left=1.5ex] {};






\end{tikzpicture}  }
  \caption{Visualization of the orientation estimation for each, the body and the head. The orange dot shows an example point on a 3D sphere, visualizing an orientation. We use the standard notation from ISO 80000-2:2019\cite{internationalorganizationforstandardizationISO80000220192019} for spherical coordinates. As we only need the polar angle  and azimuthal angle  we use a unit sphere and with .}
  \label{fig:camera_sphere}
\end{figure}

The PedRecNet outputs 2D human joint positions as pixel coordinates in the bounding box's coordinate system, normalized between zero and one. The 3D human joint positions are output as 3D coordinates relative to the hip center position, and normalized between zero and one. Orientations are output as angles between  for  and  for , and also normalized between zero and one.

\subsection{Datasets}
\label{sec:pedrec:datasets}
The training and validation datasets of the PedRec network are composed of the COCO~\cite{linMicrosoftCOCOCommon2014}, the H36M~\cite{ionescuHuman36MLarge2014}, and self-generated simulated datasets. The datasets support different labels, especially orientation estimation labels are only available in the simulation data (cf. Table~\ref{tab:pedrec_dataset_labels}).


\begin{table}[!htbp]
  \centering
  \begin{tabular}{l l l l} \toprule
      Dataset & Pose2D & Pose3D & Orientation \\ \midrule
      COCO & \checkmark & \text{\sffamily X} & \text{\sffamily O} \\
      H36M & \checkmark & \checkmark & \text{\sffamily X} \\
      TUD~\cite{andrilukaMonocular3DPose2010} & \text{\sffamily X} & \text{\sffamily X} & \text{\sffamily O}\\
      SIM-ROM & \checkmark & \checkmark & \checkmark \\
      SIM-Circle & \checkmark & \checkmark & \checkmark \\
  \end{tabular}
  \caption[Dataset Label Support]{Overview of used datasets and the supported labels. COCO (MEBOW~\cite{wuMEBOWMonocularEstimation2020}) and TUD~\cite{andrilukaMonocular3DPose2010} orientation annotations provide only body  labels. The 2D and 3D pose estimation labels differ in the available labels as COCO, H36M and SIM use different skeleton structures. Legend: \checkmark data is available, \text{\sffamily X} data is not available, {\sffamily O} data is partially available.}
  \label{tab:pedrec_dataset_labels}
\end{table}

Whole-body data, including the  and  angle of the body and head orientations, are only available in the simulated datasets. Therefore, the validation of the orientation estimation on real data can be performed with these datasets only for the azimuthal angle  of the body. All simulated datasets are created using motions, captured in a motion capture laboratory.

\subsubsection{SIM-ROM}
In the SIM-ROM dataset, a person was recorded performing an extended range of motions that should include as many poses as possible. The idea behind this dataset is to provide data from various performable body poses for 3D pose recognition.

\subsubsection{SIM-C01}
The SIM-C01 dataset is a large scale pedestrian action dataset containing actions ranging from simple walking, to hitchhiking, to tripping and falling. This dataset is used in this work for validation (SIM-C01V) only.

\subsubsection{SIM-CIRCLE}
The SIM-Circle dataset resulted from an analysis of the other datasets. As highlighted in Figure~\ref{fig:datasets_body_orientations}, there is a substantial difference in the distribution of the azimuthal angle  of the body pose.

\begin{figure}[!htbp]
  \centering
  \subcaptionbox{MEBOW~\cite{wuMEBOWMonocularEstimation2020}}[.32\columnwidth]{\resizebox{0.32\columnwidth}{!}{\begin{tikzpicture}
  \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
  
  \begin{polaraxis}[
ylabel={Samples},
  ylabel style={yshift=-5pt,anchor=east},
  axis line on top,
  yticklabel style={color=black,
  ytick align=outside,
  anchor=north,
        yshift=-2*\pgfkeysvalueof{/pgfplots/major tick length}}
  ]
  \addplot [no markers, very thin, fill=SkyBlue] table [col sep=comma] {code/coco_mebow_body_orientations.csv};
  \end{polaraxis}

\end{tikzpicture} }
  }\hfill
  \subcaptionbox{TUD~\cite{andrilukaMonocular3DPose2010}}[.32\columnwidth]{\resizebox{0.32\columnwidth}{!}{\begin{tikzpicture}

  \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
  
  \begin{polaraxis}[
  width=0.5\textwidth,
  ylabel={Samples},
  ylabel style={yshift=-5pt,anchor=east},
  axis line on top,
  yticklabel style={color=black,
  ytick align=outside,
  anchor=north,
        yshift=-2*\pgfkeysvalueof{/pgfplots/major tick length}}
  ]
  \addplot [no markers, very thin, fill=SkyBlue] table [col sep=comma] {code/tud_body_orientations.csv};
  \end{polaraxis}
  
\end{tikzpicture} }
  }\hfill
  \subcaptionbox{SIM-ROM}[.32\columnwidth]{\resizebox{0.32\columnwidth}{!}{\begin{tikzpicture}

  \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
  
  \begin{polaraxis}[
  width=0.5\textwidth,
  ylabel={Samples},
  ylabel style={yshift=-5pt,anchor=east},
  axis line on top,
  yticklabel style={color=black,
  ytick align=outside,
  anchor=north,
        yshift=-2*\pgfkeysvalueof{/pgfplots/major tick length}}
  ]
  \addplot [no markers, very thin, fill=SkyBlue] table [col sep=comma] {code/rtsim_rom_body_orientations.csv};
  \end{polaraxis}
  
\end{tikzpicture} }
  } 
  \subcaptionbox{SIM-CIRCLE\label{fig:datasets_body_orientations:circle}}[.32\columnwidth]{\resizebox{0.32\columnwidth}{!}{\begin{tikzpicture}

  \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
  
  \begin{polaraxis}[
  width=0.5\textwidth,
  ylabel={Samples},
  ylabel style={yshift=-5pt,anchor=east},
  axis line on top,
  yticklabel style={color=black,
  ytick align=outside,
  anchor=north,
        yshift=-2*\pgfkeysvalueof{/pgfplots/major tick length}}
  ]
  \addplot [no markers, very thin, fill=SkyBlue] table [col sep=comma] {code/circle01_body_orientations.csv};
  \end{polaraxis}
  
\end{tikzpicture} }
  }\hfill
  \subcaptionbox{SIM-C01 (Train)}[.32\columnwidth]{\resizebox{0.32\columnwidth}{!}{\begin{tikzpicture}

  \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
  
  \begin{polaraxis}[
  width=0.5\textwidth,
  ylabel={Samples},
  ylabel style={yshift=-5pt,anchor=east},
  axis line on top,
  yticklabel style={color=black,
  ytick align=outside,
  anchor=north,
        yshift=-2*\pgfkeysvalueof{/pgfplots/major tick length}}
  ]
  \addplot [no markers, very thin, fill=SkyBlue] table [col sep=comma] {code/conti01_body_orientations.csv};
  \end{polaraxis}
  
\end{tikzpicture} }
  }
  \subcaptionbox{SIM-C01 (Val)}[.32\columnwidth]{\resizebox{0.32\columnwidth}{!}{\begin{tikzpicture}

  \definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
  
  \begin{polaraxis}[
  width=0.5\textwidth,
  ylabel={Samples},
  ylabel style={yshift=-5pt,anchor=east},
  axis line on top,
  yticklabel style={color=black,
  ytick align=outside,
  anchor=north,
        yshift=-2*\pgfkeysvalueof{/pgfplots/major tick length}}
  ]
  \addplot [no markers, very thin, fill=SkyBlue] table [col sep=comma] {code/conti01_val_body_orientations.csv};
  \end{polaraxis}
  
\end{tikzpicture} }
  }
  \caption[Distribution of body  orientations]{Distribution of body  orientations [] in the datasets used in this work. The plots show the distribution of the samples in a polar plot. The small peaks in SIM-CIRCLE are due to overlapping start and end frames.}
  \label{fig:datasets_body_orientations}
\end{figure}

The same could be observed for the distribution of the azimuthal angle  of the head poses. In order to generate further data with a uniform distribution, the SIM-Circle dataset was created, in which 3D models walk clock- and counterclockwise in a circle at a uniform speed (see Figure~\ref{fig:datasets_body_orientations:circle}). Table~\ref{tab:dataset_statistics} provides an overview over the datasets used.

\begin{table}[!htbp]
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l l l l l l} \toprule
    Data & COCO~\cite{linMicrosoftCOCOCommon2014} & H36M~\cite{ionescuHuman36MLarge2014} & TUD~\cite{andrilukaMonocular3DPose2010} & SIM-ROM & SIM-CIRCLE \\ 
    \midrule
     &  &  &  &  &  \\
     &  &  &  &  &  \\
     &  &  &  &  &  \\
     &  &  &  &  &  \\
     &  &  &  &  &  \\
     &  &  &  &  &  \\
     &  &  &  &  & \\
     &  &  & &  & \\
     &  &  & &  &  \\
     &  &  & &  &  \\
     &  &  & &  & \\
     &  &  & &  &  \\
     &  &  & &  & \\
  \end{tabular}}
  \caption[Dataset - Statistics]{Statistics of datasets used in the PedRec experiments.  represents the 2D or 3D bounding box size and  the distance to the camera. The number of samples is notated as , the mean of bounding boxes and the distances to the camera as , and the standard deviation values are notated with .}
  \label{tab:dataset_statistics}
\end{table}

This overview shows that the H36M dataset was recorded in a lab with limited space, which is why the bounding boxes are always relatively large. Comparing the 2D bounding box diameters of COCO, TUD, and SIM-CIRCLE, the distribution is similar, as each of the datasets contains individuals at various distances.

\subsection{Training procedure}
\label{sec:pedrec:training}
The network was trained step by step in the following order:

\begin{enumerate}
  \item 2D human pose estimation
  \item 3D human pose estimation
  \item Joint visibility
  \item Head and body orientation
\end{enumerate}

The training was performed on real data (COCO+H36M) and then on simulation data in the same order. 

\paragraph{Loss Functions}
\label{sec:pedrec:loss_functions}
We used the  loss function for the 2D and 3D human joint coordinate regression losses  and . The joint visibility loss  is the standard binary cross-entropy loss. In the orientation regression task, we represent circular data in a one-dimensional map. Thus we cannot use the standard  or  loss; for example, a prediction of  with a ground truth of  results in an error of . This applies only for the azimuthal angle , the polar angle  is defined between  and , and thus the standard distance metrics can be used. As such, we applied the following loss functions:



where  is the number of samples and  and  are the softargmax outputs for the azimuthal angle  and the polar angle  normalized between zero and one. As shown in equation~\ref{eq:loss_o_phi} and \ref{eq:loss_o_theta} the  loss is applied. For training data which only provides labels for the azimuthal angle  only equation~\ref{eq:loss_o_phi} is used. 

In our experiments,  may be a subset of the entire training set, as various datasets with different supported labels (see table~\ref{tab:pedrec_dataset_labels}) were combined. As such, each loss function contains a sample selection step before the actual calculation of the loss.

To weight the loss functions, we applied uncertainty loss, described by Kendall~\textit{et al.}~\cite{kendallMultitaskLearningUsing2018}, to balance the different loss outputs. The final loss function is:



where  are learnable parameters. We use  instead  to ensure a positive loss value.

\paragraph{Optimizer}
For optimization we used the AdamW optimizer~\cite{loshchilovDecoupledWeightDecay2019} which is an slightly modified variant of the Adam optimizer~\cite{kingmaAdamMethodStochastic2015}. We applied the learning rate range test~\cite{smithCyclicalLearningRates2017a} to get an initial learning rate of . We used a standard weight decay of . We also applied the 1cycle policy~\cite{smithDisciplinedApproachNeural2018} with which the learning rate is updated during the training process from a minimum learning rate of  to the maximum of  and afterward back to a minimum using cosine annealing. Smith showed that this approach results in faster convergence and usually better results~\cite{smithCyclicalLearningRates2017a}. The network was trained with a training cycle of  epochs, from which  epochs were trained with frozen weights in the feature extractor. For the last five epochs with the feature extractor unfrozen, we reduce the learning rate for the feature extraction to  and for the other layers to . The training cycle was repeated five times, which improved the performance slightly.

\paragraph{Datasets}
We used the training datasets of COCO~\cite{linMicrosoftCOCOCommon2014}, H36M~\cite{ionescuHuman36MLarge2014}, SIM-ROM, and SIM-Circle to train the PedRecNet. The validation is done on the validation parts of COCO, H36M, and the SIM-C01 dataset. We subsampled the H36M training dataset by ten, all samples from the other datasets were used. For the orientation estimation part, we used only labels from SIM-ROM and SIM-Circle during the initial training. COCO labels were used in an additional training step during orientation experiments (see section \ref{sec:pedrec:results:orientation}). The dataset names are abbreviated in the results as follows:  stands for COCO,  for COCO (MEBOW~\cite{wuMEBOWMonocularEstimation2020}),  for H36M and  for SIM-Circle and SIM-ROM combined. COCO is always the base dataset of PedRecNet and is therefore used as one of the training dataset in every experiment.

\paragraph{Augmentations} We augmented the data by scaling the input by up to . We rotate the input by up to  in  of the cases, but only when no orientation labels were used. The image is flipped in  of the cases.

\section{Experiments}
\paragraph{3D pose estimation}
\begin{table*}[!htp]
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{l l l l l l l l l l l l l l l l l l}
    \toprule
    Method & Prop. & Dir. &  Disc. & Eat & Greet & Phone & Photo &  Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg \\\midrule
    Chen~\textit{et al.} '17~\cite{chen3DHumanPose2017} & g & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - &  \\
    Martinez~\textit{et al.} '17~\cite{martinezSimpleEffectiveBaseline2017} & g &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    Luvizon ~\textit{et al.} '17~\cite{luvizonHumanPoseRegression2017} & gs &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    Yang~\textit{et al.} '18~\cite{yang3DHumanPose2018a} & - &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    Pavllo~\textit{et al.} '18~\cite{pavllo3DHumanPose2019} & agt &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    Pavllo~\textit{et al.} '18~\cite{pavllo3DHumanPose2019} & ag &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    Luvizon~\textit{et al.} '20~\cite{luvizonMultitaskDeepLearning2020} & gs &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ 
    Shan~\textit{et al.} '21~\cite{shanImprovingRobustnessAccuracy2021} & at &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    Gong~\textit{et al.} '21~\cite{gongPoseAugDifferentiablePose2021} & ag & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - &  \\
    \midrule
    ours ++ & ag &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    ours +++ & ag &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    \midrule
    ours ++ & g &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    ours +++ & g &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
  \end{tabular}
  }
  \caption[H36M Results]{Results on the H36M dataset reported as mean per joint position error (MPJPE). Legend of properties (Prop.), influencing the results: (a) used test-time augmentation (in ours only flip test is applied), (g) used ground truth bounding boxes, (s) sampled every 64th frame of validation set, (t) used temporal information}
  \label{tab:h36m_eval_results}
\end{table*}

For 3D human pose estimation, we first consider the performance compared to other methods on the H36M validation dataset. The results of 3D pose estimation are shown in Table~\ref{tab:h36m_eval_results}. It summarizes further approaches, which differ in the methods used, input data, and test methods. Some approaches use test-time augmentations like flip-testing. Others use temporal information to improve the results. Our novel PedRecNet is based on single-frame estimation. For better comparability, we also show results using flip-testing in addition to the results without any form of test-time augmentation. The performance of our method with an average MPJPE of  is comparable to current SOTA approaches such as Gong~\textit{et al.}~\cite{gongPoseAugDifferentiablePose2021} with  and Luvizon~\textit{et al.}~\cite{luvizonMultitaskDeepLearning2020} with . The use of temporal information leads to noticeable better results in this benchmark. This becomes clear when comparing the two results of Pavllo~\textit{et al.}~\cite{pavllo3DHumanPose2019}. An improvement of  could be achieved by using temporal information. The performance from PedRecNet, depending on the datasets used, is very similar but decreases slightly when simulation data is added. However, this may also be because the simulation data is partly very different from the H36M data in terms of distances and body size of the persons.

\begin{figure*}[htp]
  \centering
  \subcaptionbox{\label{fig:example_3d_skeletons:a}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {-0.59/7.44/-0.47/1.0/1.0,-0.14/7.76/-0.61/1.0/1.0,-0.33/7.76/-0.06/1.0/1.0,0.52/7.65/-0.75/1.0/1.0,0.03/7.61/0.58/1.0/1.0,2.35/5.44/-0.27/1.0/1.0,-1.16/6.00/0.71/1.0/1.0,2.67/2.52/-0.63/1.0/1.0,-2.17/3.59/1.36/1.0/1.0,2.37/0.44/-2.22/1.0/1.0,-1.91/2.34/-0.49/1.0/1.0,1.01/-0.10/-0.26/1.0/1.0,-1.01/0.08/0.26/1.0/1.0,0.63/-4.43/-1.20/1.0/1.0,-1.24/-4.41/-0.85/1.0/1.0,1.36/-7.12/1.46/1.0/1.0,-0.92/-8.27/-1.04/1.0/1.0,0.00/-0.00/-0.00/1.0/1.0,0.30/2.89/0.07/1.0/1.0,0.57/5.81/-0.10/1.0/1.0,0.42/6.67/-0.17/1.0/1.0,0.66/8.94/0.03/1.0/1.0,1.26/-9.28/0.95/1.0/1.0,-1.53/-9.33/-2.91/1.0/1.0,1.71/-0.51/-2.77/1.0/1.0,-1.15/1.69/-1.27/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.9] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
\end{scope}
\node[above=of skeleton, yshift=-0.09cm, inner sep=0pt]
{\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_1.png}};

\end{tikzpicture}   }
  \subcaptionbox{\label{fig:example_3d_skeletons:b}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {0.21/5.14/-0.87/1.0/1.0,0.58/5.49/-0.64/1.0/1.0,0.14/5.48/-0.47/1.0/1.0,1.29/5.53/-0.25/1.0/1.0,0.16/5.40/0.15/1.0/1.0,2.36/3.98/-0.47/1.0/1.0,-0.09/4.48/1.45/1.0/1.0,2.05/2.35/-1.81/1.0/1.0,-1.36/3.01/1.46/1.0/1.0,-0.01/2.18/-2.54/1.0/1.0,-1.95/2.35/0.42/1.0/1.0,0.72/-0.20/-0.50/1.0/1.0,-0.87/0.25/0.45/1.0/1.0,-1.44/-0.65/-2.00/1.0/1.0,-3.21/-0.43/-0.49/1.0/1.0,-2.30/-2.98/-1.78/1.0/1.0,-3.52/-2.83/-0.45/1.0/1.0,0.00/0.00/-0.00/1.0/1.0,0.75/2.04/0.53/1.0/1.0,0.92/4.42/0.45/1.0/1.0,0.79/4.87/0.20/1.0/1.0,0.77/6.71/0.08/1.0/1.0,-3.37/-3.57/-3.25/1.0/1.0,-4.63/-3.32/-1.70/1.0/1.0,-1.01/1.88/-2.64/1.0/1.0,-2.32/2.15/-0.41/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.9] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
\end{scope}
\node[above=of skeleton, yshift=-0.09cm, inner sep=0pt]
{\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_2.png}};

\end{tikzpicture}   }
  \subcaptionbox{\label{fig:example_3d_skeletons:c}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {2.53/7.30/-0.33/1.0/1.0,2.25/7.58/0.08/1.0/1.0,2.07/7.69/-0.51/1.0/1.0,1.74/7.46/0.75/1.0/1.0,1.25/7.67/-0.68/1.0/1.0,2.27/5.50/1.15/1.0/1.0,-0.57/5.73/-1.06/1.0/1.0,3.63/2.94/1.26/1.0/1.0,-2.04/3.27/-0.77/1.0/1.0,4.92/1.30/-0.13/1.0/1.0,-2.45/0.92/-1.41/1.0/1.0,0.49/-0.03/0.91/1.0/1.0,-0.54/0.01/-0.87/1.0/1.0,-0.48/-4.21/1.51/1.0/1.0,1.31/-3.78/-1.83/1.0/1.0,-3.35/-6.99/2.09/1.0/1.0,2.08/-7.79/-1.88/1.0/1.0,0.00/0.00/-0.00/1.0/1.0,0.29/2.89/0.06/1.0/1.0,0.86/5.76/-0.01/1.0/1.0,1.26/6.60/-0.06/1.0/1.0,1.40/8.89/0.22/1.0/1.0,-2.27/-8.99/1.80/1.0/1.0,4.01/-8.46/-3.04/1.0/1.0,5.61/0.38/-1.53/1.0/1.0,-2.37/-0.53/-1.11/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.5cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_4.png}};
\end{tikzpicture}   }
  \subcaptionbox{\label{fig:example_3d_skeletons:d}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {1.71/4.13/-3.34/1.0/1.0,1.98/4.42/-3.30/1.0/1.0,1.77/4.66/-3.16/1.0/1.0,2.48/4.38/-2.92/1.0/1.0,2.02/4.96/-2.42/1.0/1.0,3.30/2.58/-2.64/1.0/1.0,1.45/5.01/-0.59/1.0/1.0,3.44/-0.26/-2.19/1.0/1.0,0.22/5.15/0.40/1.0/1.0,3.47/-2.77/-1.70/1.0/1.0,-0.29/4.61/0.87/1.0/1.0,0.52/-0.42/-0.63/1.0/1.0,-0.58/0.38/0.62/1.0/1.0,-1.88/1.26/-2.87/1.0/1.0,-3.96/1.69/-0.31/1.0/1.0,-1.79/-1.91/-2.70/1.0/1.0,-2.76/-1.80/0.56/1.0/1.0,0.00/-0.00/-0.00/1.0/1.0,1.62/1.76/-0.27/1.0/1.0,2.25/3.80/-1.67/1.0/1.0,2.16/4.09/-2.37/1.0/1.0,2.53/5.77/-3.26/1.0/1.0,-3.17/-3.03/-3.53/1.0/1.0,-4.89/-2.92/0.54/1.0/1.0,3.37/-3.84/-1.78/1.0/1.0,-0.52/3.43/1.44/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.5cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_3.png}};
\end{tikzpicture} }
\subcaptionbox{\label{fig:example_3d_skeletons:e}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {-1.12/7.35/0.63/1.0/1.0,-0.76/7.70/0.54/1.0/1.0,-1.05/7.66/0.90/1.0/1.0,-0.09/7.52/0.62/1.0/1.0,-0.78/7.42/1.48/1.0/1.0,1.10/6.12/0.76/1.0/1.0,-1.66/5.55/0.92/1.0/1.0,0.25/6.59/-1.53/1.0/1.0,-2.28/4.19/0.14/1.0/1.0,-0.96/8.36/-2.10/1.0/1.0,-2.95/3.80/-1.29/1.0/1.0,0.77/0.09/0.31/1.0/1.0,-0.79/-0.08/-0.28/1.0/1.0,0.46/-4.07/0.34/1.0/1.0,-0.50/-4.15/-1.21/1.0/1.0,-0.67/-7.44/0.66/1.0/1.0,-1.41/-7.97/-1.10/1.0/1.0,0.00/-0.00/0.00/1.0/1.0,-0.08/3.02/0.42/1.0/1.0,-0.21/5.87/0.96/1.0/1.0,-0.29/6.75/0.84/1.0/1.0,-0.32/8.78/1.43/1.0/1.0,-0.15/-8.71/-0.48/1.0/1.0,-0.73/-9.11/-2.30/1.0/1.0,-1.78/9.30/-2.47/1.0/1.0,-3.29/3.41/-2.03/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.09cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_5.png}};
\end{tikzpicture} }
\subcaptionbox{\label{fig:example_3d_skeletons:f}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {-3.03/3.09/-4.29/1.0/1.0,-3.17/3.44/-4.26/1.0/1.0,-3.51/3.31/-4.33/1.0/1.0,-2.94/3.59/-3.82/1.0/1.0,-4.01/3.23/-3.82/1.0/1.0,-2.05/3.59/-2.91/1.0/1.0,-4.42/1.93/-2.53/1.0/1.0,-1.00/1.55/-2.36/1.0/1.0,-4.63/-0.32/-1.38/1.0/1.0,-1.64/-0.36/-2.80/1.0/1.0,-3.93/-1.62/-1.10/1.0/1.0,0.45/0.72/0.11/1.0/1.0,-0.33/-0.65/-0.08/1.0/1.0,3.94/0.88/0.28/1.0/1.0,3.04/-0.89/-1.07/1.0/1.0,6.11/-0.66/2.92/1.0/1.0,4.58/-1.06/1.47/1.0/1.0,-0.00/-0.00/0.00/1.0/1.0,-2.03/1.38/-0.96/1.0/1.0,-3.24/2.72/-2.75/1.0/1.0,-3.32/2.85/-3.61/1.0/1.0,-3.97/4.29/-4.58/1.0/1.0,8.65/-0.99/2.77/1.0/1.0,5.65/-1.67/0.85/1.0/1.0,-1.98/-1.51/-2.23/1.0/1.0,-3.78/-2.04/-1.29/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.5cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_6.png}};
\end{tikzpicture} }
  \caption[3D Human pose estimation in the wild: PedRecNet examples]{3D Human pose estimation \enquote{in the wild}: PedRecNet examples. Top: Cropped image of the person inputed in the network. Bottom: Predicted 3D human pose.}
  \label{fig:example_3d_skeletons}
\end{figure*}

Figure~\ref{fig:example_3d_skeletons} shows some examples on \enquote{in the wild} real data. The examples are from various sources and include different cameras, focal lengths, exposures and perspectives. Examples \ref{fig:example_3d_skeletons:d}-\ref{fig:example_3d_skeletons:f} show that even in challenging situations a good 3D pose can be predicted. In contrast, we have reported some error cases in Figure \ref{fig:example_3d_skeleton_false_dets}. Figure \ref{fig:example_3d_skeleton_false_dets:b} shows an extreme corner case where the pose detection fails completely. Note that the corner case dataset from our work~\cite{ludlUsingSimulationImprove2018} was not used during training. In Figure \ref{fig:example_3d_skeleton_false_dets:e} the pose is correct in principle, but the outstretched left arm is not correctly recognized. From the pose only, it is not clear that the person is just operating a traffic light switch. Example \ref{fig:example_3d_skeleton_false_dets:f} shows a false recognition due to self occlusion. The body occludes the left arm, but the 3D pose recognition estimates it to be hidden behind the right arm and displays it stretched out accordingly. These false detections by occlusion could possibly be improved by further training data or the use of temporal context.

\begin{figure}[htp]
  \centering
  \resizebox*{0.8\columnwidth}{!}{\subcaptionbox{\label{fig:example_3d_skeleton_false_dets:b}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {-3.17/1.71/-3.47/1.0/1.0,-2.92/1.88/-3.72/1.0/1.0,-3.26/2.02/-3.41/1.0/1.0,-2.09/2.10/-3.74/1.0/1.0,-2.83/2.32/-2.69/1.0/1.0,-0.95/1.63/-3.46/1.0/1.0,-2.53/2.22/-1.18/1.0/1.0,-0.50/-0.47/-3.53/1.0/1.0,-2.09/0.28/0.07/1.0/1.0,-1.11/-1.58/-3.52/1.0/1.0,-1.80/-1.21/0.41/1.0/1.0,0.45/-0.07/-0.67/1.0/1.0,-0.45/0.08/0.75/1.0/1.0,-0.14/-1.91/-1.39/1.0/1.0,-1.21/-1.71/0.58/1.0/1.0,0.09/-3.93/-0.44/1.0/1.0,0.16/-4.37/1.08/1.0/1.0,0.00/-0.00/-0.00/1.0/1.0,-0.44/1.72/-0.89/1.0/1.0,-1.67/1.92/-2.24/1.0/1.0,-2.26/1.85/-2.97/1.0/1.0,-2.98/3.04/-3.81/1.0/1.0,-0.31/-4.87/-0.91/1.0/1.0,-0.79/-5.07/1.24/1.0/1.0,-1.74/-2.47/-3.48/1.0/1.0,-1.62/-1.60/0.95/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.5cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_wrong_2.png}};
\end{tikzpicture}   }
\subcaptionbox{\label{fig:example_3d_skeleton_false_dets:e}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {0.88/7.22/0.48/1.0/1.0,0.49/7.52/0.66/1.0/1.0,0.60/7.60/0.11/1.0/1.0,-0.21/7.27/0.92/1.0/1.0,-0.01/7.54/-0.44/1.0/1.0,-0.31/5.52/1.87/1.0/1.0,-0.66/5.68/-1.48/1.0/1.0,0.63/3.53/2.71/1.0/1.0,-1.08/3.00/-2.62/1.0/1.0,2.33/2.74/1.65/1.0/1.0,0.68/2.27/-3.42/1.0/1.0,0.04/-0.06/1.00/1.0/1.0,-0.08/0.04/-0.99/1.0/1.0,0.54/-4.19/0.98/1.0/1.0,-0.69/-4.17/-0.97/1.0/1.0,1.10/-8.07/1.15/1.0/1.0,-1.69/-8.13/-0.80/1.0/1.0,0.00/0.00/-0.00/1.0/1.0,-0.21/2.90/0.06/1.0/1.0,-0.31/5.70/0.05/1.0/1.0,-0.15/6.62/0.15/1.0/1.0,-0.44/8.74/0.20/1.0/1.0,3.26/-8.78/1.19/1.0/1.0,0.22/-9.41/-1.17/1.0/1.0,3.00/2.48/0.89/1.0/1.0,1.48/1.69/-3.45/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.09cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_wrong_6.png}};
\end{tikzpicture} }
\subcaptionbox{\label{fig:example_3d_skeleton_false_dets:f}}[.15\textwidth]{\begin{tikzpicture}
    \begin{scope}[scale=0.12, local bounding box=skeleton]
  \tdplotsetmaincoords{70}{45}
  \tikzset{joint/.style = {circle, fill, minimum size=2pt,
  inner sep=0pt, outer sep=0pt},
  gridstyle/.style={
  gray!60!white, thick
  },
  tickstyle/.style={
  ultra thick
  }}
  \tikzset{joint/.default = 2pt, tdplot_main_coords}
  \tikzmath{
    \gridsize = 5;
    \gridxmin = -7;
    \gridxmax = 7;
    \gridymin = -7;
    \gridymax = 7;
    \gridzmin = -7;
    \gridzmax = 7;
      \ticklen = 0.15;
      \axisxmin = -15;
      \axisxmax = 15;
      \axisymin = -15;
      \axisymax = 15;
      \axiszmin = -15;
      \axiszmax = 15;
      \tickdist = 5;
      \axistickxmin = -15;
      \axistickxmax = \axisxmax - 5;
      \axistickymin = -15;
      \axistickymax = \axisymax - 5;
      \axistickzmin = -15;
      \axistickzmax = \axiszmax - 5;
    \gridxrange = \gridxmin + \gridsize;
    \gridyrange = \gridymin + \gridsize;
    \gridzrange = \gridzmin + \gridsize;
      \axistickxrange = \axistickxmin + \tickdist;
      \axistickyrange = \axistickymin + \tickdist;
      \axistickzrange = \axistickzmin + \tickdist;
      }


\foreach \x in {\gridxmin, \gridxrange,...,\gridxmax}
  {
      \draw [gridstyle] (\x,\gridymin,\gridzmin) -- (\x,\gridymax,\gridzmin);
      \draw [gridstyle] (\x,\gridymax,\gridzmin) -- (\x,\gridymax,\gridzmax);
  }
\foreach \y in {\gridymin, \gridyrange,..., \gridymax}
  {
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmin,\y,\gridzmax);
      \draw [gridstyle] (\gridxmin,\y,\gridzmin) -- (\gridxmax,\y,\gridzmin);
  }
\foreach \z in {\gridzmin, \gridzrange,...,\gridzmax}
  {
      \draw [gridstyle] (\gridxmin,\gridymin,\z) -- (\gridxmin,\gridymax,\z);
      \draw [gridstyle] (\gridxmin,\gridymax,\z) -- (\gridxmax,\gridymax,\z);
  }







\foreach \x/\y/\z/\score/\visible [count=\i] in {1.93/6.29/0.25/1.0/1.0,1.64/6.60/0.47/1.0/1.0,1.76/6.72/-0.06/1.0/1.0,0.86/6.51/0.70/1.0/1.0,1.20/6.81/-0.46/1.0/1.0,1.06/5.20/1.42/1.0/1.0,0.18/5.57/-1.61/1.0/1.0,1.69/3.61/2.29/1.0/1.0,1.69/3.93/-2.39/1.0/1.0,3.10/2.83/1.82/1.0/1.0,3.87/3.70/-1.90/1.0/1.0,0.44/-0.11/0.75/1.0/1.0,-0.49/0.05/-0.75/1.0/1.0,0.84/-3.07/0.22/1.0/1.0,-0.15/-3.17/-0.73/1.0/1.0,0.32/-5.78/0.47/1.0/1.0,0.09/-5.90/-0.28/1.0/1.0,-0.00/0.00/0.00/1.0/1.0,0.29/2.79/0.10/1.0/1.0,0.57/5.37/-0.04/1.0/1.0,0.85/6.13/0.02/1.0/1.0,1.02/8.11/0.23/1.0/1.0,1.80/-6.39/-0.20/1.0/1.0,0.64/-6.76/-1.21/1.0/1.0,4.02/2.77/1.59/1.0/1.0,4.89/3.50/-1.53/1.0/1.0
  } {
      \newdimen \vis
      \vis = \visible pt
      \ifdim \vis > 0.0pt
          \node[anchor=north, joint, pedrec_joint_color_\i, opacity=1] (joint-\i) at (\x, \z, \y)  {};
      \fi
  }

\foreach \a/\b [count=\i] in {7/9,9/11,6/8,8/10,13/15,15/17,12/14,14/16,1/3,3/5,1/2,2/4,18/12,18/13,18/19,19/20,20/21,21/22,20/6,20/7,21/1,16/23,17/24,10/25,11/26} {
      \ifnodedefined{joint-\a}{
          \ifnodedefined{joint-\b}{
              \draw[pedrec_limb_color_\i, line width=2.5pt, opacity=0.95] (joint-\a) -- (joint-\b);
          }{}
      }{}
  }
    \end{scope}
    \node[above=of skeleton, yshift=-0.09cm, inner sep=0pt]
    {\includegraphics[height=90px]{gfx/pedrec/example_3d_skeleton_wrong_3.png}};
\end{tikzpicture} }}
\caption[3D Human pose estimation in the wild: PedRecNet false predictions]{3D Human pose estimation in the wild: PedRecNet false predictions. Top: Cropped image of the person inputed in the network. Bottom: Predicted 3D human pose.}
  \label{fig:example_3d_skeleton_false_dets}
\end{figure}

\paragraph{Body orientation estimation}
\label{sec:pedrec:results:orientation}
As described in the related work section, there are only few datasets in the area of body and head orientation estimation for full-body inputs. For a state-of-the-art comparison, we found the relatively new dataset ~\cite{wuMEBOWMonocularEstimation2020} suitable, which provides body orientation labels for the COCO dataset. However, it only contains the azimuthal angle  and labels for the head pose are not included. We also used the TUD~\cite{andrilukaMonocular3DPose2010} dataset in the analysis, although it only contains  samples in the validation dataset. Accordingly, the significance of the results here is relatively low. Table~\ref{tab:pedrec_body_orientation_estimation_results} gives an overview on the results on these datasets. It is to notice that with the PedRecNet we already achieve an  of  on the TUD~\cite{andrilukaMonocular3DPose2010} dataset and  on the MEBOW dataset. For , which is often sufficient for real-world applications, we even achieve  on the TUD~\cite{andrilukaMonocular3DPose2010} dataset and  on the MEBOW dataset. These are surprising results for not using any training data from the corresponding training datasets. Especially when compared to the earlier approaches of Hara~\textit{et al.}~\cite{haraDesigningDeepConvolutional2017} and Yu~\textit{et al.}~\cite{yuContinuousPedestrianOrientation2019}, the PedRecNet gives better results without ever having seen any data from the TUD~\cite{andrilukaMonocular3DPose2010} dataset. We are accordingly able to provide a solid baseline here purely with simulated data. It should be noted, however, that 3D pose data is also used for orientation estimation and the training for this has included real data from the H36M dataset. When the MEBOW training data are used in addition to the simulation data, the  and  improve by  and , respectively, and are  and  worse than the results reported by Wu~\textit{et al.}. In total,  body orientations were predicted with an error above . We analyzed these misclassifications further and detected erroneous ground truth labels for  images, some of which are shown in Figure~\ref{fig:mebow_wrong_labels}. 

\begin{table*}[!htbp]
  \centering

  \begin{tabular}{l l l l l l} \toprule
      Network & Trainset & Testset &  &  & MAE() \\ 
      \midrule
      Wu~\textit{et al.}\cite{wuMEBOWMonocularEstimation2020} (2020) & MEBOW & MEBOW &  &  &  \\
      ours & MEBOW & MEBOW &  &  &  \\
      ours & SIM+MEBOW & MEBOW &  &  &  \\
      ours & SIM & MEBOW &  &  &  \\\midrule
      Hara~\textit{et al.}\cite{haraDesigningDeepConvolutional2017} (2017) & TUD & TUD &  &  &  \\
      Yu~\textit{et al.}\cite{yuContinuousPedestrianOrientation2019} (2019) & TUD & TUD &  &  &  \\
      Wu~\textit{et al.}\cite{wuMEBOWMonocularEstimation2020} (2020) & MEBOW & TUD &  &  &  \\
      ours & MEBOW & TUD &  &  &  \\
      ours & SIM+MEBOW & TUD &  &  &  \\
      ours & SIM & TUD &  &  &  \\
      \midrule
      ours & MEBOW & SIM-C01 &  &  &  \\
      ours & SIM+MEBOW & SIM-C01 &  &  &  \\
      ours & SIM & SIM-C01 &  &  &  \\
  \end{tabular}
  \caption[Human body orientation () results]{Human body orientation () test results on the MEBOW, TUD~\cite{andrilukaMonocular3DPose2010} and SIM-C01V datasets. The column trainset specifies the training dataset(s) used to train the specific networks. Testset specifies on which testsets the results are reported on. In addition to the accuracy in  and  intervals we report the mean average error (MAE).}
  \label{tab:pedrec_body_orientation_estimation_results}
\end{table*}

\begin{figure}
  \centering
  \resizebox*{0.8\columnwidth}{!}{\subcaptionbox{\label{fig:mebow_wrong_labels:a}}[.15\textwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (96:1);
      \draw[ultra thick, red, dashed, -latex] (0,0) -- (285:1);
  \end{scope}
  \node[above=of body, yshift=-0.5cm, inner sep=0pt]
  {\includegraphics[width=.15\textwidth]{data/COCOMEBOW_FALSEDETS/wrong label/000000201072-285-96.jpg}};
\end{tikzpicture}   }
  \subcaptionbox{\label{fig:mebow_crowded_unclears:a}}[.15\textwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (264:1);
      \draw[ultra thick, red, dashed, -latex] (0,0) -- (84:1);
  \end{scope}
  \node[above=of body, yshift=-0.5cm, inner sep=0pt]
  {\includegraphics[width=.15\textwidth]{data/COCOMEBOW_FALSEDETS/crowd unclear which/000000240250-84-264.jpg}};
\end{tikzpicture}   }
  \subcaptionbox{\label{fig:mebow_wrong_predicted:c}}[.15\textwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (158:1);
      \draw[ultra thick, red, dashed, -latex] (0,0) -- (30:1);
  \end{scope}
  \node[above=of body, yshift=-0.5cm, inner sep=0pt]
  {\includegraphics[width=.15\textwidth]{data/COCOMEBOW_FALSEDETS/false predicted/000000099054-30-158.jpg}};
\end{tikzpicture}   }}
  \caption[MEBOW validation dataset: Wrong labels]{MEBOW validation dataset: Examples of misclassifications. The red dotted arrow shows the annotated ground truth, the black arrow shows the prediction of PedRecNet. The misclassifications are caused by: (a) false ground truth label, (b) occlusion of the labeled person (the one in the back of the woman) and (c) PedRecNet misclassification.}
  \label{fig:mebow_wrong_labels}
\end{figure}

\paragraph{Head orientation estimation}
For the head orientation () estimation, we use the SIM-C01V dataset. We consider only the  estimate at this point because  is underrepresented in the SIM-C01 dataset; the head orientations are relatively horizontal in the pedestrian actions in almost all cases. Accordingly, for the estimation of , further targeted experiments and new data recordings are needed in the future. The results for the estimation of the head  orientation are shown in Table~\ref{tab:pedrec_head_orientation_estimation_results}.

\begin{table}[!htbp]
  \centering
  \resizebox{1\columnwidth}{!}{\begin{tabular}{l l l l l l} \toprule
      Network & Trainset & Testset &  &  &  \\ \midrule
      PedRec & SIM+MEBOW & SIM-C01V &  &  &  \\
      PedRec & SIM & SIM-C01V &  &  &  \\
  \end{tabular}}
  \caption[PedRecNet: Head orientation results]{PedRecNet: Head orientation test results for .}
  \label{tab:pedrec_head_orientation_estimation_results}
\end{table}

The results are slightly inferior to the body orientation estimation by  and  for  and , respectively. In general, however, performance on the body and head orientation estimates is relatively similar. The somewhat inferior performance can be explained by the head region's smaller image area than the body region. 
We are not currently aware of a larger and publicly dataset that includes head orientation images in addition to full-body images. Therefore, most approaches to head orientation estimation work with datasets that only contain cropped faces. In productive applications, face recognition can then be performed first, followed by a crop of the face, and orientation estimation can be performed based on this cropped face bounding box. In addition, most datasets only contain faces, which means that a side view or the back of the head cannot usually be used for orientation estimation. In our approach, the entire body is always considered, which enables head orientation estimation even for a side and back view of a person. However, based on subjective observation of \enquote{in-the-wild} examples, we think that we can achieve similar performance on real data for head pose recognition as for body pose recognition when trained on simulation data only. We show \enquote{in-the-wild} examples in Figure~\ref{fig:orientation_inthewild_examples}.

\begin{figure}
  \centering
  \resizebox*{0.8\columnwidth}{!}{\subcaptionbox{\label{fig:orientation_inthewild_examples:a}}[.23\columnwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (209:1);
\end{scope}
  \node[above=of body, yshift=-1cm] {Head};
  \begin{scope}[yshift=-3cm, local bounding box=head]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (252:1);
\end{scope}
  \node[above=of head, yshift=-1cm] (head_label) {Body};
  \node[above=of head_label, yshift=2.25cm, inner sep=0pt]
  {\includegraphics[height=90px]{gfx/pedrec/orientation_example_1.png}};
\end{tikzpicture} }
  \subcaptionbox{\label{fig:orientation_inthewild_examples:b}}[.23\columnwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (247:1);
\end{scope}
  \node[above=of body, yshift=-1cm] {Head};
  \begin{scope}[yshift=-3cm, local bounding box=head]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (231:1);
\end{scope}
  \node[above=of head, yshift=-1cm] (head_label) {Body};
  \node[above=of head_label, yshift=2.25cm, inner sep=0pt]
  {\includegraphics[height=90px]{gfx/pedrec/orientation_example_2.png}};
\end{tikzpicture}   }
  \subcaptionbox{\label{fig:orientation_inthewild_examples:c}}[.23\columnwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (125:1);
\end{scope}
  \node[above=of body, yshift=-1cm] {Head};
  \begin{scope}[yshift=-3cm, local bounding box=head]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (102:1);
\end{scope}
  \node[above=of head, yshift=-1cm] (head_label) {Body};
  \node[above=of head_label, yshift=2.25cm, inner sep=0pt]
  {\includegraphics[height=90px]{gfx/pedrec/orientation_example_3.png}};
\end{tikzpicture}   }
  \subcaptionbox{\label{fig:orientation_inthewild_examples:d}}[.23\columnwidth]{\begin{tikzpicture}
\begin{scope}[local bounding box=body]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (50:1);
\end{scope}
  \node[above=of body, yshift=-1cm] {Head};
  \begin{scope}[yshift=-3cm, local bounding box=head]
      \draw[thick] (0,0) circle(1);
      \draw[dotted] (-1,0) -- (1,0);
      \draw[dotted] (0,-1) -- (0,1);
      \draw[ultra thick, -latex] (0,0) -- (59:1);
\end{scope}
  \node[above=of head, yshift=-1cm] (head_label) {Body};
  \node[above=of head_label, yshift=2.25cm, inner sep=0pt]
  {\includegraphics[height=90px]{gfx/pedrec/orientation_example_4.png}};
\end{tikzpicture}   }}
  \caption[Head and body orientation estimation \enquote{in the wild}: Examples]{Head and body orientation estimation \enquote{in the wild}: Examples. Top: Cropped image of the person processed by the network. Bottom two rows: Predicted head and body orientation .}
  \label{fig:orientation_inthewild_examples}
\end{figure}

Figure~\ref{fig:orientation_inthewild_examples:a} shows a typical example, where one can nicely depict the different estimates of head versus body orientation. Example~\ref{fig:orientation_inthewild_examples:b} shows a boy in a stroller, which shows that the orientation estimation gives good results even in non-upright positions. Figure \ref{fig:orientation_inthewild_examples:c} shows a person who was photographed from behind. Especially the correct head pose estimation is interesting, although the person wears a hood and only a small part of the nose is visible. Another interesting example is demonstrated in Figure \ref{fig:orientation_inthewild_examples:d}, where the orientation estimation is based on input data of a person shot from behind and only visible in a low-resolution image section of about . 

\section{Conclusion}
With PedRecNet, we presented a simple yet efficient architecture that performs multiple tasks simultaneously and can run on consumer hardware at over 15FPS even with multiple people. The network achieves performance that is comparable to current SOTA methods for 2D and 3D pose detection and orientation estimation. Our model combines all these tasks in a simple and extensible architecture which is straight forward to train. Thus, the introduced model is also well suited as a baseline for further research. We have further shown that we can train the orientation estimation purely with simulation data and achieve high accuracy on real data without requiring real sensor data for training.

\section*{ACKNOWLEDGMENT}
This project has been supported by the Continental AG as part of a research cooperation.




\bibliography{literature.bib}
\bibliographystyle{IEEEtran}

\end{document}