\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{multirow}
\usepackage{array}\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{natbib}  \usepackage[hidelinks]{hyperref}

\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2021.1)
} 



\setcounter{secnumdepth}{2} 







\title{Extreme Cross Network: Representing Dense and Sparse Feature Interactions \\for Recommender Systems}
\author{

Anonymous Author(s)
}
\affiliations{


}
\iffalse
\title{My Publication Title --- Single Author}
\author {
Author Name \\
}

\affiliations{
    Affiliation \\
    Affiliation Line 2 \\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {


        First Author Name,\textsuperscript{\rm 1}
        Second Author Name, \textsuperscript{\rm 2}
        Third Author Name \textsuperscript{\rm 1} \\
}
\affiliations {
\textsuperscript{\rm 1} Affiliation 1 \\
    \textsuperscript{\rm 2} Affiliation 2 \\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi
\begin{document}

\maketitle

\begin{abstract}
	\vspace{-0.1cm}
Click-Through Rate (CTR) prediction is a core task in nowadays commercial recommender systems. Feature crossing, as the mainline of research on CTR prediction, has shown a promising way to enhance the predictive performance.
However, previous methods are either low performance or retain implicit components for crossing features. Moreover, they mainly focus on the modeling of cross sparse features and neglect to represent cross dense features. 
Motivated by this, we propose a novel Extreme Cross Network (XCrossNet), which aims at learning dense and sparse feature interactions in an explicit manner. In detail, we separately design a cross layer for crossing dense features and a product layer for crossing sparse features, then feed these cross features into an attention mechanism and an MLP for combination and selection.
XCrossNet as a feature structure-oriented model leads to a more expressive representation and a more precise CTR prediction, which is not only explicit and interpretable, but also time-efficient and easy to implement.
Extensive experiments on three large-scale public datasets show significant improvement of XCrossNet over state-of-the-art models on effectiveness and efficiency.
\vspace{-0.4cm}
\end{abstract}




\section{Introduction}



Accurate targeting of commercial recommender systems is of great importance, in which Click-Through Rate (CTR) prediction plays a key role. CTR prediction aims to estimate the ratio of clicks to the impression of a recommended item for a user \shortcite{feng2019deep,shi2020deep}. 
We show a common commercial recommender system for online display advertising in Figure~\ref{fig1}. Advertisers expect lower costs to achieve a higher Return On Investment (ROI). Generally, advertisers have two bid strategies, one is based on Cost Per Click (CPC), and the other is based on Cost Per Action (CPA). 
From the publishersâ€™ perspective, traffic monetizing hinges on Cost Per  Mille Impressions (CPM). The AD Exchange platforms usually trade with advertisers and publishers according to the Generalized Second Price (GSP) of the maximum effective Cost Per Mille (eCPM). 
Demand Side Platforms (DSP) help plenty of advertisers manage display ads campaigns through AD Exchange platforms.
Therefore, campaign performance directly depends on predicted Click-Through Rate~(pCTR) or predicted ConVersion Rate (pCVR).
If CTR is overestimated, advertisers could waste campaign budgets on the useless impression; On the other hand, if CTR is underestimated, advertisers would lose some valuable impressions and the campaigns may under deliver. With multi-billion dollar business on commercial recommendation today \shortcite{zhou2019deep, lyu2020deep}, CTR prediction has received growing interest from communities of both academia and industry. 


\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth,height=4.49cm]{CTR05} 

\tiny
{
	ROI = Return On Investment, GMV = Gross Merchandise Volume, CPC = Cost Per Click, CPA = Cost Per Action, CPM = Cost Per Mille Impressions, pCTR = predicted Click-Through Rate, pCVR = predicted ConVersion Rate, eCPM = effective Cost Per Mille, GSP = Generalized Second Price.
}
\vspace{-0.2cm}
	\caption{Recommender systems for display advertising.}
	\vspace{-0.55cm}
	\label{fig1}
\end{figure}

In web-scale commercial recommender systems, the inputs of users' characteristics are in two kinds of structures. The first kind of structure is described by numerical or dense parameters, e.g., ``\texttt{Age\_years=22, Height\_cm=165}''. Each of such characteristics is formalized as a value associated with a numerical field, while the values are named as dense features. The second kind of structure is described by categorical or sparse parameters, e.g.,``\texttt{Gender=Female, Relationship=In love}''. Each of such characteristics is formalized as a vector of one-hot encoding associated with a categorical field, while the vectors are named as sparse features. Data scientists usually spend much time on interactions of raw features to generate better predictive models \cite{lian2018xdeepfm,DBLP:conf/ijcai/OuyangZRQLD19}. 
Among these feature interactions, cross features, previously focused more on cross-product of sparse features,  show a promising way to enhance the performance of prediction \shortcite{chapelle2014simple,cheng2016wide,luo2019autocross,lu2020dual}.
Owing to the fact that correct cross features are mostly task-specific and difficult to identify a priori, the crucial challenge is in automatically extracting sophisticated cross features hidden in high-dimensional data. 

Research on feature crossing as the mainline of CTR prediction has attracted widespread attention in recent years. Shallow models like Factorization Machine \cite{DBLP:conf/icdm/Rendle10} are simple, interpretable, and easy to scale, but limited in expressive ability. In contrast, deep learning has shown powerful expressive capabilities, nevertheless, as shown in \cite{DBLP:conf/wsdm/BeutelCJXLGC18}, Deep Neural Networks (DNNs) require many more parameters than tensor factorization to approximate high-order cross features.
Besides, almost all deep models leverage multilayer perceptron (MLP) to learn high-order feature interactions \shortcite{wang2017deep, DBLP:conf/ijcai/GuoTYLH17, lian2018xdeepfm}, however, whether plain DNNs indeed effectively represent right functions of cross features remains an open question \shortcite{wang2017deep,lian2018xdeepfm}. 
Moreover, most methods mainly focus on the representation of cross sparse features and neglect to represent cross dense features or the combination of dense and sparse features. Previous work always directly concatenates dense features with the embeddings of sparse features, which could cause an important feature dimensionality imbalance problem.

Based on all these observations, we propose a novel \textbf{Extreme Cross Network (XCrossNet)}, to represent both dense and sparse feature interactions. Modeling with XCrossNet consists of three stages: the \textbf{Feature Crossing}, the \textbf{Feature Concatenation}, and the \textbf{Feature Selection}. In the Feature Crossing stage, we separately design a cross layer for crossing dense features and a product layer for crossing sparse features. In the Feature Concatenation stage, cross dense features and cross sparse features interact through an attention mechanism for combination. Lastly, in the Feature Selection stage, we employ an MLP for capturing non-linear interactions and their relative importance.
Experimental results on three large-scale public datasets demonstrate the superior performance of XCrossNet over the state-of-the-art baselines.
The main contributions are highlighted as follows:
\begin{itemize}
	\vspace{-0.1cm}
	\item  Our cross layer and product layer explicitly model cross features to specific orders, which is different from the previous ideas of implicitly learning feature interactions.
	\vspace{-0.1cm}
	\item We propose XCrossNet, which efficiently learns dense and sparse feature interactions. It is feasible to balance the dimension of dense features up to the dimension of sparse features for solving the feature dimensionality imbalance.\vspace{-0.1cm}
	\item Extensive experiments on three large-scale public datasets show significant improvement of XCrossNet over state-of-the-art models on effectiveness and efficiency.
\end{itemize}

\vspace{-0.4cm}
\section{Related Work}

Studies on CTR prediction can be categorized into five classes which will be respectively introduced below. 

(1) {\em Generalized linear models.} Logistic Regression (LR) models such as FTRL are widely used in CTR prediction for their simplicity and efficiency \shortcite{DBLP:conf/www/RichardsonDR07,DBLP:conf/kdd/LeeODL12,DBLP:conf/kdd/McMahanHSYEGNPDGCLWHBK13}. \citeauthor{DBLP:conf/icml/YanLXH14} argue that LR cannot capture nonlinear feature interactions and propose Coupled Group Lasso (CGL) to solve it. Human efforts are usually needed for LR models. 
Gradient boosting decision tree (GBDT) is a method to automatically do feature engineering and search interactions \cite{friedman2001greedy}, then the transformed feature interactions can be fed into LR. In practice, tree-based models are more suitable for dense features but not for sparse features.

(2) {\em Quadratic polynomial mappings and Factorization Machines.} Poly2 enumerates all pairwise feature interactions to avoid feature engineering which works well on dense features \shortcite{DBLP:journals/jmlr/ChangHCRL10}. For sparse features, Factorization Machine~(FM) and its variants project each feature into a low-dimensional vector and models cross features by inner product \cite{DBLP:conf/icdm/Rendle10}. 
FFM enables each feature to have multiple latent vectors to interact with features from different fields \shortcite{juan2016field}.
As both FM and FFM can only model order-2nd cross features. An efficient algorithm Higher-Order FM (HOFM) for training arbitrary-order cross features was proposed by introducing the ANOVA kernel \shortcite{blondel2016higher}. As shown in \shortcite{DBLP:conf/ijcai/XiaoY0ZWC17}, HOFM achieves marginal improvement over FM whereas using many more parameters and only its low-order (usually less than 5) form can be practically used.

(3) {\em Implicit deep learning models.} As deep learning has shown promising representation capabilities in recommender systems, several models use MLP to improve FM. Attention FM (AFM) considered the importance of different order-2nd cross features \shortcite{DBLP:conf/ijcai/XiaoY0ZWC17}. Neural FM (NFM) stacked deep neural networks on top of the output of the order-2nd cross features to model higher-order cross features \shortcite{he2017neural}.
FNN uses FM to pre-train feature embeddings and then feeds them into an MLP \shortcite{zhang2016deep}.
Moreover, CCPM uses convolutional layers to explore local-global dependencies of cross features \shortcite{liu2015convolutional}.
IPNN (also known as PNN) feeds the interaction result of the FM layer and feature embeddings into an MLP \shortcite{qu2016product}. PIN introduces a micro-network for each pair of fields to model pairwise cross features \shortcite{qu2018product}. 
FGCNN combines a CNN and MLP to generate new features for feature augmentation~\shortcite{liu2019feature}.
 However, all these approaches learn the high-order cross features in an implicit manner, therefore lack good model explainability.
 
(4) {\em Wide\&Deep based models.}  \citeauthor{lian2018xdeepfm} argue that implicit deep learning models focus more on high-order cross features but capture little low-order cross features. The Wide\&Deep model overcomes this problem by introducing a hybrid architecture, which contains a shallow component and a deep component with the purpose of learning both memorization and generalization~\shortcite{cheng2016wide}.
DeepFM uses an FM layer to replace the wide component in Wide\&Deep~\shortcite{DBLP:conf/ijcai/GuoTYLH17}. DeepFM and Wide\&Deep cannot explicitly model higher-order feature interactions, which could further improve model performance. To learn cross features in an explicit fashion, Deep\&Cross~\shortcite{wang2017deep} and xDeepFM~\shortcite{lian2018xdeepfm} take outer product of features at the bit- and vector-wise level respectively. However, it uses so many parameters that great challenges are posed to identify important cross features in the huge combination space.

(5) {\em AutoML based models.} There exist some approaches using AutoML techniques to deal with cross features in recommender systems. AutoCross is proposed to search over subsets of candidate features to identify effective interactions \shortcite{luo2019autocross}. This requires training the whole model to evaluate the selected feature interactions, but the candidate sets are incredibly many. AutoInt leverages attention and residual networks to form an interacting layer to determine the relevance of cross features~\shortcite{song2019autoint}. AutoGroup treats the selection process of high-order feature interactions as a structural optimization problem, and solves it with Neural Architecture Search \shortcite{DBLP:conf/sigir/LiuXGTZHL20}. It achieves state-of-the-art performance on various datasets, but is too complex to be applied in industrial applications.




\vspace{-0.2cm}
\section{Extreme Cross Network}
\vspace{-0.1cm}
In this section, we describe the details of Extreme Cross Network (XCrossNet) in the following three steps: Feature Crossing, Feature Concatenation, and Feature Selection. 

We firstly formalize CTR prediction problem.
 Suppose the dataset for training consists of  instances , where  indicates dense features including  numerical fields, and  indicates sparse features including  categorical fields, and  indicates the user's click behaviors ( means the user clicked the item, and  otherwise). The task of CTR prediction is to build a prediction model  to estimate the ratio of clicks to impressions of a given feature context.






\begin{figure*}[!t]
	\centering
	\includegraphics[width=1\textwidth]{XCrossNet03} \caption{The structure of XCrossNet.}
	\label{fig2}
	\vspace{-0.4cm}
\end{figure*}


\vspace{-0.2cm}
\subsection{Feature Crossing}
\vspace{-0.1cm}
\subsubsection{{\em Definition. }}

In CTR predictions, a feature pair  of two dense features  and  is defined as a cross feature if and only if , but  as , and  as ; If  or (and)  is a sparse feature, the case is  as , or (and)  as .  


Some dense features and sparse features are more or less related to the prediction results. Thus, we can quantify the impacts of features through the weights of individual features. However, as for cross features, there are individual features less related to the prediction, but their combinations lead to a key impact. From a dynamic perspective, inspired by manual feature engineering on cross features, we observe a cross feature like that, its value increases significantly as the values of the individual features increase jointly, but changes gently as one of the individual features approaches to zero or zero vector. 

Based on the definition, cross features can be generalized to high-order cases. If we consider individual features as order-st features, an order-th cross feature combines  individual features, where some of them could be less related to the prediction, but their feature combination has a significant impact. In this way, we make the following~assumption.

\vspace{-0.2cm}
\subsubsection{{\em Assumption. }}

For any individual features, their order-th cross feature can be expressed by a multivariate polynomial of degree .

When cross dense features are expressed by polynomials, we can simply treat as successive multiplications of the associated feature values. As for cross sparse features, we need the inner product of two feature vectors, since sparse features are represented as vectors of fields.
However, it is unrealistic to enumerate all cross features to high orders, because it not only requires huge storage and computation, but also causes the model to be unable to generalize to unseen feature combinations. 
Therefore, the explicit structures of modeling cross features based on our assumption will be introduced, and is expressive, while possessing both generalization and high~efficiency.

\subsubsection{Cross layers on dense features.}
Here we introduce a novel cross layer for crossing dense features. Cross layers have the following formula: 
\begin{small}

 \end{small}where  indicates the input dense features, and  is a column vector denoting the order-th cross features. Later we prove how  expresses multivariate polynomials of degree  after weighted mapping.  are the weight and bias parameters respectively, and  denote the outputs from the -th and the -th cross layers.

We denote . If the proposed cross layer expresses any cross features of order-th, it could approximate to any multivariate polynomials of degree , denoted as : 
\begin{small}
\end{small}where . For simplicity, here we use  to denote the original subscript of . We study the coefficient  given by  from cross layers, since it constitutes the output  from the -th cross layer. Moreover, the following derivations do not include bias terms. Then: 
\begin{small}

\end{small}Afterwards, let  denotes the multi-index vectors of orders , and  denotes the order of field . Clearly  from cross layers approaches the coefficient ~as: 
\begin{small}

\end{small}With  approximate to multivariate polynomials of degree , the output  from the -th cross layer that includes all cross features to order-th could approximate polynomials in the following class: 
\begin{small}

\end{small}

Therefore, cross layer is extremely expressive to cross dense features up to explicit orders. Besides, by polynomial approximation, vectors of orders cross different fields, which results in cross features relate to interdependent weighted parameters. In other words, cross layers have each cross feature learned some independent parameters from other features, while the weights of a cross term could share corresponding parameters, which enables the model to generalize to rare or never co-occurred feature combinations.


\subsubsection{{\em Efficiency.}} The number of parameters involved in  cross layers is . The time complexity of cross layers is , increasing linearly with the dimensions of the dense features. Therefore, cross layers introduce negligible complexity compared to plain neural networks. Noted that, the efficiency benefits from the rank-one property of , which allows the generation of cross features without storing the entire matrix.


\subsubsection{Embedding and Product Layer on Sparse Features.}
As sparse features  are represented as vectors of one-hot encoding of high-dimensional spaces, we employ an embedding layer to transform these one-hot encoding vectors into dense vectors  as:
\begin{small}

\end{small}where  indicates the input sparse feature of field ,  is the embedding weights,  denotes the embedding size, and  denotes the feature embedding of field~.

Afterwards, we can propose a product layer for cross sparse features. First, we donate order-2nd cross sparse features as , and order-1st sparse features as , thus the output of product layer is .

The cross feature of two sparse features of field  and field  equals the inner product of two embedding vectors as . As for represention of vectors, we concatenate the weighted sums of inner products to formulate order-2nd cross features as: 
\begin{small}

\end{small}where  is the size of the product layer, and  is a  dimensional vector, of each dimension  denotes a weighted sum of inner products of two sparse features. Thus, we have . We assume that the weighted parameter  for reduction, so  can be given as: 
\begin{small}

\end{small}The feature vector of order-1st features has a similar formula as follows: 
\begin{small}

\end{small}where  is a  dimensional vector, of each dimension  denotes a weighted sum of sparse features. The weighted feature can be expressed as inner product . Thus, we have . 

\vspace{-0.1cm}
\subsubsection{{\em Efficiency.}} The training time complexity of embedding layer is . As for product layer, the time complexity of training order-1st features is , through Eq. \ref{eq1} we reduce the time of training order-2nd cross features from  to . So the overall time complexity of product layer becomes .  Moreover, inner product operations are easily accelerated through parallelization by GPUs. Therefore, the total time complexity of embedding layer and product layer on sparse features can be accelerated as  in practice.

\vspace{-0.1cm}
\subsection{Feature Concatenation}
For the Feature Concatenation stage, in order to learn feature interactions of different structures, cross dense features  and cross sparse features  are fed into an attention mechanism to adaptively learn the weights for feature combinations. Formally, the attention network is defined as:
\begin{small}
	
\end{small}where  is a feature vector of cross dense features  (Noted ),  is a feature vector of cross sparse features , and , , ,  are learning model parameters.  is the hidden layer size of the attention network, denoted as attention factor.  is the normalized weight. We use the  as the activation function, which empirically shows good performance. The output of the attention-based weighted sum pooling is formulated as follows:
\begin{small}
	
\end{small}where  is an  dimensional vector, which compresses all hybrid structure feature interactions by distinguishing their importance. 
\subsubsection{{\em Efficiency.}} The training time complexity of the Feature Concatenation stage is mainly  determined by Eq. \ref{eq2} as , which can be accelerated through parallelization as  in practice.

\vspace{-0.1cm}
\subsection{Feature Selection}

In the Feature Selection stage, we employ an MLP to capture non-linear interactions and the relative importance of cross features. The deep layers and the output layer respectively have the following formula: 
\begin{small}
	
\end{small}where  are hidden layers,  and  are activation functions,  are weights, and  are biases, and  is the output result. For CTR prediction, the loss function is the log loss as follows:
where  is the total number of training instances. The optimization process is to minimize the following objective function: 
where  denotes the regularization term and  denotes the set of parameters, including cross layers, embedding layer, product layer, attention layer, deep layers and output layer.

\vspace{-0.1cm}
\section{Experiments}

In this section, extensive experiments are conducted to answer the following research questions:

\begin{description}
	\vspace{-0.1cm}
	\item[RQ1:] How does XCrossNet perform compared with the state-of-the-art CTR prediction models?
	\vspace{-0.1cm}
	\item[RQ2:] How does the feature dimensionality imbalance impact CTR prediction?
	\vspace{-0.1cm}
	\item[RQ3:] How do different hyper-parameter settings impact the performance of XCrossNet?
	\vspace{-0.1cm}
	\item[RQ4:] How effective is each component of XCrossNet?

\end{description}


\begin{table}[h]
	\vspace{-0.2cm}
	\centering
	\fontsize{8pt}{11pt} \selectfont{
\begin{tabular}{p{1cm} <{\centering}  p{1.2cm}<{\centering}  p{1.4cm}<{\centering}   p{0.8cm}<{\centering}  p{1.7cm}<{\centering} }
			\hline
			Dataset & \#instances & \#dimension & \#fields & positive ratio \\
			\hline
			Criteo &  &  &  &  \\
			Avazu &  &  &  &  \\
			iPinYou &  &  &  &  \\
			
			\hline
		\end{tabular}
	}
	\caption{Statistics of Experimental Datasets.}
	\label{table1}
	\vspace{-0.3cm}
\end{table}


\vspace{-0.3cm}
\subsection{Experimental Setup}

\subsubsection{Datasets.} Experiments are conducted on the following three public datasets. \textbf{Criteo} dataset contains one month of ad click logs. We select 7 consecutive days of samples as the training set while the next one day for evaluation. To counter label imbalance, negative down-sampling is applied to keep the positive ratio roughly at 50\%. \textbf{Avazu} dataset was released in the CTR prediction contest on Kaggle in 2014. 80\% of randomly shuffled data is allotted to training and validation with 20\% for testing. \textbf{iPinYou} dataset was published in the iPinYou RTB Bidding Algorithm Competition in 2013. We utilize seasons 2 and 3 as our dataset. To make a fair comparison, we process the data in the three datasets exactly the same as in \shortcite{qu2018product,liu2019feature,DBLP:conf/sigir/LiuXGTZHL20}. Table \ref{table1} summarizes the characteristics of the three datasets.

\begin{table}[!t]
	\centering
	\fontsize{7pt}{8.8pt} \selectfont{
\begin{tabular}{|p{1.3cm} <{\centering} | p{1.99cm} | p{1.79cm} | p{1.89cm}| }
			\hline
			Params & Criteo & Avazu & iPinYou \\
			\hline
			Genaral & \begin{tabular}[c]{@{}l@{}}bs=2000; lr=1e-3 \\ opt=Adam \end{tabular}
			& \begin{tabular}[c]{@{}l@{}}bs=2000; lr=1e-3 \\ opt=Adam \end{tabular} & \begin{tabular}[c]{@{}l@{}}bs=2000; lr=1e-3\\ opt=Adam  \\ l2\_e=1e-6 \end{tabular} \\
			\hline
			
			LR & -- & -- & -- \\
			\hline
			\multirow{2}{*}{GBDT} & depth=25  & depth=18 & depth=6 \\
			& \#tree=1300 & \#tree=1000 & \#tree=600 \\
			\hline
			\multirow{3}{*}{FM, AFM} & K=20; t=0.01; R=32 & K=40; t=1;R=256 & K=20; t=1;R=256 \\ 
			& l2\_a=0.1 & l2\_a=0.1 & l2\_a=0.1 \\
			& sub-net=[40,1] & sub-net=[80,1] & sub-net=[40,1] \\			 
			\hline
			FFM & K=4 & K=4 & K=4 \\
			\hline
			\multirow{3}{*}{CCPM} & K=20 & K=40 & K=20 \\
			& kernel=[7256] & kernel=[7128]  & kernel=[7128] \\
			& net=[2563,1]  & net=[1283,1]  & net=[1283,1] \\
			\hline
			\multirow{2}{*}{Wide\&Deep} & K=20 & K=40 & K=20 \\
			& net=[4003,1] & net=[7005,1] & net=[3003,1] \\
			\hline
			\multirow{3}{*}{Deep\&Cross} & K=20 & K=40 & K=20 \\
			& net=[4003,1] & net=[7005,1] & net=[3003,1] \\
			& Cross=[2003] & Cross=[2002] & Cross=[2003] \\
			\hline
			\multirow{3}{1.6cm}{FNN, DeepFM IPNN} & K=20 & K=40 & K=20 \\ 
			& net=[7005,1] & net=[5005,1] & net=[3003,1]  \\
			& LN=true & LN=true & LN=true \\			 
			\hline
			\multirow{4}{*}{PIN} & K=20 & K=40 & K=20 \\ 
			& net=[7005,1] & net=[5005,1] & net=[3003,1]  \\ 
			& sub-net=[40,5] & sub-net=[40,5] & sub-net=[40,5] \\
			& LN=true & LN=true & LN=true \\			 
			\hline
			xDeepFM & \begin{tabular}[c]{@{}l@{}}K=20\\ net=[4003,1] \\ CIN=[1004]\end{tabular}
			& \begin{tabular}[c]{@{}l@{}}K=40\\net=[7005,1] \\CIN=[1002]\end{tabular} & \begin{tabular}[c]{@{}l@{}}K=20\\ net=[3003,1] \\ CIN=[1004] \\ LN=true\end{tabular} \\
			\hline
			FGCNN & \begin{tabular}[c]{@{}l@{}}K=20 \\ conv=9*1 \\  kernel=[38,40,42,44] \\
				new=[3,3,3,3] \\ BN=true \\ net=[4096,2048,1]
			\end{tabular}
			& \begin{tabular}[c]{@{}l@{}}K=40 \\ conv=7*1 \\ \tiny{kernel=[14,16,18,20]} \\ new=[3,3,3,3] \\
				BN=true \\ net=[4096,2048,\\ 1024,512,1]
			\end{tabular} & 
			\begin{tabular}[c]{@{}l@{}}K=20 \\ conv=3*1 \\ kernel=[4,6,8] \\ new=[1,1,1] \\ BN=true \\ net=[6003,1]
			\end{tabular} \\
			\hline
			AutoGroup & \begin{tabular}[c]{@{}l@{}}K=20 \\ lr\_h=10 \\  =0.1 \\
				net=[1024,512,256,1] \\ n\_p=[35,390,300,\\500,450,150] \\ q=6
			\end{tabular}
			& \begin{tabular}[c]{@{}l@{}}K=40 \\ lr\_h=1e4 \\  =0.01 \\
				\tiny{net=[1024,512,256,1]} \\ n\_p=[15,130,170,\\210,250,290] \\ q=6
			\end{tabular} & 
			\begin{tabular}[c]{@{}l@{}}K=20 \\ lr\_h=1e3 \\  =0.01 \\
				net=[3003,1] \\ n\_p=[14,120,160,\\200,240] \\ q=5
			\end{tabular} \\
			\hline
			\multirow{4}{*}{XCrossNet} & K=20; T=350 & K=40; T=250 & K=20; T=150 \\ 
			& R=256 & R=256 & R=256 \\
			& n\_c=6 & n\_c=4 & n\_c=4 \\ 
			& net=[4003] & net=[7003] & net=[3003] \\	
			\hline
		\end{tabular}
	}
	\tiny
	{Note: bs=batch size, opt=optimizer, lr=learning rate, l2\_e= regularisation on embedding layer, t=softmax temperature, l2\_a= regularisation on attention network, R=size of attention factor, K=embedding size, net=MLP structure, sub-net=micro network, LN=layer normalization, BN=batch normalization, lr\_h=learning rate for structural parameters, =Gumbel-softmax temperature, q=maximum order for explicit interactions, n\_p=number of feature sets in each order, T=size of product layers, n\_c=number of cross layers on the depth.}
	\vspace{-0.1cm}
	\caption{Hyper-parameter settings.}
	\label{table2}
	\vspace{-0.5cm}
\end{table}

\vspace{-0.1cm}
\subsubsection{Evaluation Metrics.} \textbf{AUC} (Area Under ROC) and \textbf{Logloss} (cross entropy) are selected as our evaluation metrics. Note that \textbf{an improvement of 0.001-level in AUC or Logloss is usually regarded as being significant for CTR prediction}, because it will lead to a large increase in company's revenue as a large user base, which has been pointed in many existing work \shortcite{cheng2016wide,DBLP:conf/ijcai/GuoTYLH17,wang2017deep,lu2020dual}.

\vspace{-0.1cm}
\subsubsection{Baselines.} As aforementioned, we use following highly related state-of-the-art models as baselines: \textbf{LR} \shortcite{DBLP:conf/kdd/LeeODL12}, \textbf{GBDT} \shortcite{friedman2001greedy}, \textbf{FM} \shortcite{DBLP:conf/icdm/Rendle10}, \textbf{AFM} \shortcite{DBLP:conf/ijcai/XiaoY0ZWC17}, \textbf{FFM} \shortcite{juan2016field}, \textbf{CCPM} \shortcite{liu2015convolutional}, \textbf{Wide\&Deep} \shortcite{cheng2016wide}, \textbf{Deep\&Cross} \shortcite{wang2017deep} and its shallow part \textbf{Cross Network}, \textbf{FNN} \shortcite{zhang2016deep}, \textbf{DeepFM} \shortcite{DBLP:conf/ijcai/GuoTYLH17}, \textbf{IPNN} \shortcite{qu2016product}, \textbf{PIN} \shortcite{qu2018product}, \textbf{xDeepFM} \shortcite{lian2018xdeepfm} and its shallow part \textbf{CIN}, \textbf{FGCNN} \shortcite{liu2019feature}, and \textbf{AutoGroup} \shortcite{DBLP:conf/sigir/LiuXGTZHL20}. 

\vspace{-0.1cm}
\subsubsection{Hyper-parameter settings.} Table \ref{table2} summarizes the hyper-parameters for each model. To be fair and achieve the best performance, hyper-parameters of each model are tuned on validation set by grid-seaching carefully \footnote{All the source code will be released after acceptance.}.

\vspace{-0.1cm}
\subsection{Overall Performance (RQ1)}
The experiments for XCrossNet and the best baseline model are repeated 10 times by changing the random seeds. Table~\ref{table3} summarizes the performance of all compared methods on three large-scale public datasets, while the training time on Tesla K80 GPUs is shown in Figure \ref{fig3} for comparison of efficiency. We have the following key observations:


\begin{table}[t]
	\centering

	\fontsize{8pt}{13pt} \selectfont{
	\begin{tabular}{p{1.25cm} <{\centering}|p{0.75cm} <{\centering} p{0.8cm} <{\centering}|p{0.75cm} <{\centering} p{0.83cm} <{\centering}|p{0.7cm} <{\centering} p{0.9cm} <{\centering}}
		\hline
		\hline
		\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Criteo}                  & \multicolumn{2}{c|}{Avazu}                   & \multicolumn{2}{c}{iPinYou}                 \\
		\cline{2-7}
		& AUC(\%)  & Logloss & AUC(\%)  & Logloss   & AUC(\%)   & Logloss   \\
		\hline
		LR                      & 78.00                    & 0.5631                    & 76.76                    & 0.3868                   &  76.38                  & 0.005691                 \\


		GBDT                      & 78.62                  & 0.5560                &  77.53                    & 0.3824                  & 76.90                & 0.005578                  \\
FM                      & 79.09                  & 0.5500                  & 77.93                   & 0.3805              &  77.17                  & 0.005595                    \\
AFM                      & 79.13                    & 0.5517                    & 78.06                   & 0.3794                    & 77.71                    & 0.005562                   \\
FFM                      & 79.80                    & 0.5438                   & 78.31                    & 0.3781                    & 76.18                    & 0.005695                    \\
CCPM                      & 79.55                   & 0.5469                   & 78.12                    & 0.3800                    & 77.65                    & 0.005593                    \\
Wide\&Deep                      & 79.77                    & 0.5446                    & 78.10                    & 0.3803                    & 77.86                    & 0.005571                    \\
Cross                      & 78.70                    & 0.5550                  & 77.62                    & 0.3820                    & 76.70                    & 0.005603                   \\
		Deep\&Cross                      & 79.76                   & 0.5445                    & 78.11                    & 0.3801                 & 77.88                    & 0.005569                    \\
		FNN                      & 79.87                    & 0.5428                    & 78.30                    & 0.3778                   & 77.82                    & 0.005573                   \\
DeepFM                    & 79.91                    & 0.5423                   & 78.36                    & 0.3777                    & 77.92                    & 0.005588                   \\
IPNN                      & 80.13                    & 0.5399                   & 78.68                   & 0.3757                    & 78.17                    & 0.005549                   \\
PIN                      & 80.18                    & 0.5394                    & 78.72                    & 0.3755                  & 78.22                    & 0.005547                    \\
CIN                     & 78.81                   & 0.5538                    & 78.02                    & 0.3797                    & 77.30                    & 0.005592                    \\
xDeepFM                      & 80.06                    & 0.5408                    & 78.55                   & 0.3766                    & 78.04                    & 0.005555                    \\
FGCNN                      & 80.22                    & 0.5389                    & 78.82                   & 0.3747                   & 77.85                    & 0.005612                   \\
AutoGroup                      & 80.28                    & 0.5384                    & 79.15                    & 0.3729                    & 78.59                   & 0.005528                    \\
	\hline
		\multirow{2}{*}{XCrossNet}                     & \textbf{80.67}                    & \textbf{0.5339}                   & \textbf{79.54}                    & \textbf{0.3698}                    & \textbf{78.72}                    & \textbf{0.005507}                    \\
		& \textbf{0.01} & \textbf{0.0001} & \textbf{0.03} & \textbf{0.0002} & \textbf{0.03} &\textbf{ 4E-6} \\
	\hline
	\hline
	
	\end{tabular}
}

Note:  and  repsent siginicant level -value  and -value  \\ of comparing XCrossNet with the best baseline.
	\vspace{-0.2cm}
	\caption{Performance comparison of different models.}
	\label{table3}
	\vspace{-0.6cm}
\end{table}

Firstly, most neural network models outperform linear models (i.e., LR), tree-based models (i.e., GBDT), and FM variants (i.e., FM, FFM, AFM), which indicates neural network can learn non-linear feature interactions and endow better expressive ability. Meanwhile, comparing IPNN and PIN with FNN and Wide\&Deep based models, we find that explicitly modeling low-order feature interactions can simplify the training of neural networks and boost the performance. AutoGroup and FGCNN are overall the best baselines on three datasets, mainly because some high-order feature interactions are captured via CNN or feature grouping, and contribute in final predictions.

Secondly, XCrossNet consistently achieves the best performance on all datasets. In detail, XCrossNet significantly outperforms the best baseline in terms of AUC and Logloss on -value  level on Criteo and Avazu datasets, while -value  on iPinYou dataset. From training time comparison, we can observe XCrossNet is more efficient than field-aware models, mainly because they further allow each feature to learn several vectors where each vector is associated with a field, which leads to huge parameter consuming and time consuming.


\begin{figure}[!t]
	\centering
	\includegraphics[width=8.5cm]{times12} \vspace{-0.6cm}
	\caption{Training time comparison of different models.}
	\label{fig3}
	\vspace{-0.2cm}
\end{figure}

\begin{figure}[!t]
	\centering
	\includegraphics[width=7.5cm]{imbalance17} \vspace{-0.3cm}
	\caption{Impact of feature dimensionality imbalance.}
	\label{fig4}
	\vspace{-0.5cm}
\end{figure}


\vspace{-0.1cm}
\subsection{\fontsize{10.4pt}{13pt} \selectfont{Feature Dimensionality Imbalance Study (RQ2)}}
\vspace{-0.1cm}
In XCrossNet, we denote  as the balance index of dimensions of dense and sparse features.
Noted that, the dimension of cross dense features  equals , increasing with the depth of cross layers. As for Criteo dataset,  and , we set the depth of cross layers from 1 to 8, while the corresponding dimension of cross dense features is from 13 to 104. Experimental results are shown in Figure~\ref{fig4} in terms of AUC. We can observe that increasing the depth of cross layers benefit XCrossNet to achieve stable improvements on AUC performance, mainly because the higher dimensions of cross dense features are able to boost the balance index, which results in relatively balanced impacts of dense and sparse features on prediction.







\begin{figure*}[!t]
	\centering
	\subfigure[Embedding size]{
		\begin{minipage}[t]{4.6cm}
			\centering
			\includegraphics[width=4.6cm,height=3.111cm]{embedding-AUC01}
\end{minipage}}\subfigure[Size of attention factor]{
		\begin{minipage}[t]{4.4cm}
			\centering
			\includegraphics[width=4.4cm,height=3.111cm]{attention-AUC01}
\end{minipage}}\subfigure[Activation functions]{
		\begin{minipage}[t]{4.4cm}
			\centering
			\includegraphics[width=4.4cm,height=3.111cm]{Activation-AUC01}
\end{minipage}
	}\subfigure[Number of deep layers]{
		\begin{minipage}[t]{4.4cm}
			\centering
			\includegraphics[width=4.4cm,height=3.111cm]{deep-AUC01}
\end{minipage}
	}\centering
	\vspace{-0.4cm}
	\caption{Impact of network hyper-parameters on AUC performance.}
	\label{fig5}
	\vspace{-0.4cm}
\end{figure*}

\begin{figure*}[!t]
	\centering
	\subfigure[Embedding size]{
		\begin{minipage}[t]{4.6cm}
			\centering
			\includegraphics[width=4.6cm,height=3.111cm]{embedding-Logloss01}
\end{minipage}}\subfigure[Size of attention factor]{
		\begin{minipage}[t]{4.4cm}
			\centering
			\includegraphics[width=4.4cm,height=3.111cm]{attention-Logloss01}
\end{minipage}}\subfigure[Activation functions]{
		\begin{minipage}[t]{4.4cm}
			\centering
			\includegraphics[width=4.4cm,height=3.111cm]{Activation-Logloss01}
\end{minipage}
	}\subfigure[Number of deep layers]{
		\begin{minipage}[t]{4.4cm}
			\centering
			\includegraphics[width=4.4cm,height=3.111cm]{deep-Logloss01}
\end{minipage}
	}\centering
	\vspace{-0.4cm}
	\caption{ Impact of network hyper-parameters on Logloss performance.}
	\label{fig6}
	\vspace{-0.4cm}
\end{figure*}

\vspace{-0.1cm}
\subsection{Hyper-parameter Study (RQ3)}

We study the impact of hyper-parameters of XCrossNet, including (1) embedding size; (2) size of attention factor; (3)~activation functions; (4) number of deep layers.
\vspace{-0.1cm}
\subsubsection{Embedding size.} Figures 5a and 6a demonstrate the impact of embedding size. We can observe that model performance on Criteo and Avazu datasets boosts steadily when the embedding size increase from 4 to 20. Even with very low embedding sizes, XCrossNet still has comparable performance to some popular Wide\&Deep based models with high embedding size. Specifically, on Criteo dataset, XCrossNet achieves AUC0.800 and Logloss0.541 with embedding size set as 10, which is better than DeepFM with embedding size set as 20.




\begin{table}[t]
	\centering
	
	\fontsize{8pt}{11pt} \selectfont{
		\begin{tabular}{p{1.8cm} <{\centering}|p{1.1cm} <{\centering} p{1.1cm} <{\centering}|p{1.1cm} <{\centering} p{1.1cm} <{\centering}}
			\hline
			\hline
			\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Criteo}                  & \multicolumn{2}{c}{Avazu}                                  \\
			\cline{2-5}
			& AUC(\%)  & Logloss & AUC(\%)  & Logloss     \\
			\hline
			\multirow{2}{*}{XCrossNet-CL} & 79.55 & 0.5471 & 79.16 & 0.3722   \\
			&  1.4\% &  2.5\% &  0.49\% &  0.68\% \\
			\hline
			\multirow{2}{*}{XCrossNet-PL} & 79.20 & 0.5524 & 79.01 & 0.3734   \\
			&  1.8\% &  3.5\% &  0.68\% &  1.0\% \\
			\hline
			\multirow{2}{*}{XCrossNet-AL} & 80.16 & 0.5402 & 79.15 & 0.3724   \\
			&  0.64\% &  1.2\% &  0.50\% &  0.73\% \\
			\hline
\multirow{2}{*}{XCrossNet-DL} & 79.90 & 0.5426 & 78.83 & 0.3742   \\
			&  0.97\% &  1.6\% &  0.91\% &  1.2\% \\
			\hline
			\hline
			
		\end{tabular}
	}
\vspace{-0.2cm}
	\caption{Performance of each component of XCrossNet.}
	\label{table5}
	\vspace{-0.5cm}
\end{table}		

\vspace{-0.13cm}
\subsubsection{Size of attention factor.} As shown in Figures 5b and 6b, the model performance on Criteo dataset boosts while size of the attention factor increasing from 64 to 320, while on Avazu dataset, 256 is a more suitable setting for the size of attention factor to avoid the model being to over-complicated.

\vspace{-0.13cm}
\subsubsection{Activation functions.} As shown in Figures 5c and 6c, ReLU is indeed more appropiate for hidden neurons of deep layers compared with different activation functions.

\vspace{-0.13cm}
\subsubsection{Number of deep layers.} Figures 5d and 6d demonstrate the impact of the number of deep layers. The model performance boosts with depth of MLP at the beginning on Criteo dataset. However, it starts to degrade when the depth of MLP is set to gteater than 3, mainly because of overfitting evidenced by the observation that the training error still keeps decreasing. For Avazu dataset, the model performance also degrades when the depth of MLP is set to greater than~3.


\subsection{Ablation Study (RQ4)}

We conduct an ablation study to isolate the relative importance of each component of XCrossNet. We downgrade the current model as 4 models: XCrossNet-CL removes the cross layers on dense features; XCrossNet-PL removes the product layer on sparse features; XCrossNet-AL removes the attention layer in the Feature Concatenation stage; XCrossNet-DL removes the deep layers in the Feature Selection stage. From Table \ref{table5}, we can observe that cross layers and product layer are beneficial to the model, which implies XCrossNet can represent cross dense feature and cross sparse features effectively. Meanwhile, it is necessary and effective to interact dense and sparse features by the attention layer for feature combination. Lastly, the model performance degrades apparently when we remove deep layers, because deep layers help to capture non-linear feature interactions and their relative importance from extensive cross features for prediction. 


\section{Conclusion}

In summary, this paper presents a novel feature representation model, namely Extreme Cross Network (XCrossNet), for improving CTR prediction in commercial recommender systems.  
We separately design a cross layer for crossing dense features and a product layer for crossing sparse features, then the cross features interact through an attention mechanism and an MLP for combination and selection.
The main contribution of our approach is to represent both dense and sparse feature interactions in an explicit and efficient way. Moreover, it is feasible to balance the dimension of dense features up to the dimension of sparse features for solving the feature dimensionality imbalance problem.
Empirical studies verified the effectiveness of our model on three large-scale public datasets.
The XCrossNet framework of three stages is a new hybrid feature structure-oriented model. We encourage more feature crossing network layers to be proposed with our framework.



\appendix
\bibliographystyle{aaai}
\bibliography{ref.bib}


\end{document}
