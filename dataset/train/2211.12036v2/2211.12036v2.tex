\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{hyperref}
\hypersetup{
colorlinks = true, 
urlcolor = magenta, 
linkcolor = red, 
citecolor = blue}\usepackage{enumitem}
\usepackage{romannum}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{cuted}
\usepackage{capt-of}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.9}
\usepackage{cite}
\usepackage{pifont}
\usepackage{multirow}




\iccvfinalcopy 

\def\iccvPaperID{5932} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}



\title{Dual Prototype Attention for Unsupervised Video Object Segmentation}
\author{Suhwan Cho\quad Minhyeok Lee\quad Seunghoon Lee\quad Dogyoon Lee\quad Sangyoun Lee\vspace{0.5cm}\\
~~Yonsei University\\
~~Korea Institute of Science and Technology (KIST)}


\maketitle
\def\thefootnote{*}\footnotetext{These authors contribute equally to this work.}
\pagenumbering{gobble}  




\begin{abstract}
Unsupervised video object segmentation (VOS) aims to detect and segment the most salient object in videos. The primary techniques used in unsupervised VOS are 1) the collaboration of appearance and motion information and 2) temporal fusion between different frames. This paper proposes two novel prototype-based attention mechanisms, inter-modality attention (IMA) and inter-frame attention (IFA), to incorporate these techniques via dense propagation across different modalities and frames. IMA densely integrates context information from different modalities based on a mutual refinement. IFA injects global context of a video to the query frame, enabling a full utilization of useful properties from multiple frames. Experimental results on public benchmark datasets demonstrate that our proposed approach outperforms all existing methods by a substantial margin. The proposed two components are also thoroughly validated via ablative study. Code and models are available at \url{https://github.com/Hydragon516/DPA}.
\end{abstract}



\section{Introduction}
Video object segmentation (VOS) is a fundamental task in computer vision. Given a video sequence as input, the objective is to segment objects for entire frames. It can be divided into several categories depending on how the objects to be detected are defined. In this study, we deal with the unsupervised setting, i.e., detecting and segmenting the most salient object in a video sequence without any external guidance such as target mask or reference text.



\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figure/intro}
\caption{Visualized feature maps of various embedding stages.}
\label{figure1}
\end{figure}



In unsupervised VOS, collaboration of different modalities and different frames is widely adopted. As salient objects usually show distinctive movements over the background, existing approaches including MATNet~\cite{MATNet}, FSNet~\cite{FSNet}, and HFAN~\cite{HFAN} leverage motion cues in addition to appearance cues. For each video frame, an RGB image and an optical flow map generated by pre-trained optical flow estimation models are used as input to exploit appearance and motion information simultaneously. In order to fuse the multi-modal cues, they mainly focus on designing a reciprocity framework that blends the embedded features obtained from each modality. On the other hand, there are also studies, such as COSNet~\cite{COSNet}, AGNN~\cite{AGNN}, and AD-Net~\cite{AD-Net}, that focus on exploiting temporal coherence of a video. They transfer information of the initial frame to each query frame or iteratively refines each frame via temporally connecting different frames. 


However, existing modality fusion methods and temporal aggregation methods have significant limitations. First, conventional multi-modality solutions are not carefully designed to be robust against various situations. As they fuse multi-modal cues through a simple summation, concatenation, or modulating channel weights, respective cues can act as noise if their quality is not reliable. Second, existing temporal fusion methods do not fully consider global context of a video or require high computational cost. They only consider the initial frame as an anchor frame that provides external guidance or perform iterative refinement between entire frames, which severely degrades their efficacy.



In this paper, we propose two novel modules to overcome the aforementioned limitations of existing solutions. \textbf{First}, we introduce an inter-modality attention (IMA) to refine cues of respective modalities by densely integrating context information of both modalities. For each modality, useful cues are first extracted and refined to provide valuable supervision to each other. Then, instead of a naive fusion, the features of each modality is adaptively allocated to other modality based on mutual feature propagation. \textbf{Second}, we introduce inter-frame attention (IFA) to leverage global context of a video without requiring heavy computational cost. Once a video sequence is given as input, a designated number of frames are first sampled from the entire video sequence and features from those frames are stored in an external memory bank. When predicting each frame, the stored features are adaptively propagated to query frames to provide overall properties of a video. \textbf{Finally}, we extend the proposed two modules by incorporating a prototype framework. Through converting pixel-level information to prototype-level information, more reliable and comprehensive cues can be leveraged, as each prototype is constructed as having spatial structure knowledge of the scenes. 



We evaluate our proposed approach on three popular benchmark datasets for unsupervised VOS, DAVIS~2016~\cite{DAVIS} validation set, FBMS~\cite{FBMS} test set, and YouTube-Objects~\cite{YTOBJ} dataset. On all of them, our method surpasses all existing methods by a substantial margin. Extensive experiments are also conducted to demonstrate the effectiveness of the proposed components. 


Our main contributions can be summarized as follows:
\begin{itemize}[leftmargin=0.2in]
\item We propose dual attention modules, IMA and IFA, to effectively leverage multi-modality fusion and temporal aggregation for unsupervised VOS.

\item We incorporate a prototype framework into the proposed attention mechanisms to further improve their efficacy by refining the source information. 

\item On all public benchmark datasets, our approach sets a new state-of-the-art performance, while avoiding high computational complexity.
\end{itemize} 



\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figure/framework}
\caption{Architecture of our proposed network. Based on a two-stream encoder--decoder architecture that simultaneously leverages RGB image and optical flow map, IMA and IFA modules are adopted. For simplicity, skip connections between encoding blocks and decoding blocks are omitted in the illustration.}
\label{figure2}
\end{figure*}




\section{Related Work}
\noindent\textbf{Multi-modality fusion.} In unsupervised VOS, two-stream architectures that jointly leverage appearance cues and motion cues have been attracting extensive attention. MATNet~\cite{MATNet} designs a two-stream encoder that utilizes an RGB image and an optical flow map to enhance spatio-temporal object representation. RTNet~\cite{RTNet} proposes a reciprocal transformation network to identify and segment primary objects in videos. FSNet~\cite{FSNet} proposes a full-duplex strategy to effectively fuse RGB images and optical flow maps; specifically, a bidirectional interaction module is used to ensure the mutual restraint between appearance and motion cues. AMC-Net~\cite{AMC-Net} proposes a co-attention gate that modulates the impacts of appearance and motion cues. Based on the learned weights, appearance and motion information can be leveraged adaptively. TransportNet~\cite{TransportNet} establishes the correspondence between appearance and motion cues while suppressing the distractions via optimal structural matching. HFAN~\cite{HFAN} proposes a hierarchical feature alignment network that aligns the object positions using the appearance and motion features. The cross-modal mismatch can be mitigated by adapting the aligned features. PMN~\cite{PMN} stores prototypes of appearance cues as well as and motion cues to fully leverage multiple modalities. TMO~\cite{TMO} optionally employs a motion stream on top of an appearance stream to adaptively leverage motion information. However, the performance can be further improved as the modality fusion is based on a simple summation or concatenation.


\vspace{1mm}
\noindent\textbf{Temporal aggregation.} Unlike existing two-stream methods, some studies focus on fully exploiting the temporal coherence of a video. COSNet~\cite{COSNet} employs the co-attention layers to capture global correlations and scene context by propagating semantic information in the reference frames to the query frame. AGNN~\cite{AGNN} builds fully connected graphs to represent frames as nodes and relations between those frames as edges. Rich relations between arbitrary frames can be obtained through parametric message passing. AD-Net~\cite{AD-Net} and F2Net~\cite{F2Net} regards the initial frame of a video as a reference frame, and leverages the reference frame information for query frame prediction. IMP~\cite{IMP} iteratively propagates the segmentation mask of an easy reference frame to other frames by using a pre-trained semi-supervised VOS algorithm. These methods can capture temporal coherence in a video, but still suffer from certain problems such as the global context of a video not being completely leveraged and requiring heavy computational complexity owing to the iterative inferring process. 



\section{Approach}
\subsection{Problem Formulation}
The goal of an unsupervised VOS algorithm is to identify the most salient object for all frames of a video. Following common protocol in the VOS community, we collaboratively use RGB images and optical flow maps as the input of our network. The network output is binary segmentation masks that have the same resolution as the input information. RGB images, optical flow maps, and output segmentation masks are denoted as , , and , respectively, where  is the number of total frames.


\subsection{Network Architecture}
Following existing two-stream approaches for unsupervised VOS, such as MATNet~\cite{MATNet}, FSNet~\cite{FSNet}, HFAN~\cite{HFAN}, PMN~\cite{PMN}, and TMO~\cite{TMO}, our network is designed based on a simple two-stream encoder--decoder architecture. As both RGB image and optical flow map are given as input, two separate encoders are adopted. The features obtained from those encoders are decoded using a decoder that outputs a binary segmentation mask. In the middle of the encoding and decoding process, the proposed IMA and IFA are adopted for mutual modality fusion and temporal cue aggregation, respectively. The visualized pipeline of our network can be found at Figure~\ref{figure2}.



\subsection{Inter-Modality Attention (IMA)}
\label{IMA}
Existing two-stream approaches, including the aforementioned methods, focus on fusing the multi-modal cues. However, the fusion is implemented by a simple summation or concatenation, which causes unstable cue generation for difficult scenarios. To improve multi-modality fusion for unsupervised VOS, we propose IMA to densely and thoroughly exchange information between appearance and motion cues based on prototype attention mechanism. The proposed IMA consists of three parts: prototype generation, self-correlation calculation, and mutual feature refinement. In Figure~\ref{figure3}, we visualize the architecture of IMA.


\vspace{1mm}
\noindent\textbf{Prototype generation.} Inspired by OCR~\cite{OCR}, we first generate prototypes based on learnable object regions. For each input feature map , soft object region  is calculated by applying a simple channel-wise softmax operation as

Considering the backbone encoder is learned with large-scale ImageNet~\cite{imagenet}, each channel in  already contains the clustering ability that helps separate the input feature map into semantic parts spatially. Then, using  and , prototypes  are obtained as

where  indicates matrix multiplication. In ,  prototypes with a channel size of  are contained. Note that  is equal to , but is adopted for better clarification. 



\vspace{1mm}
\noindent\textbf{Self-correlation calculation.} In order to incorporate the information of the constructed prototypes  into the input features , we first calculate self-correlation map  for each modality as

where  indicates channel L2 normalization. The generated  represents the cosine similarities between each prototype and each input feature. Considering  is used as a source information to embed key and value features of each modality, it can also be directly extracted from input features . To validate the incorporation of a prototype framework helps better fusion of two modalities, we conduct an ablation study regarding the  key--value extraction (from normal embedding vs. prototype-based self-correlation calculation) in Section~\ref{analysis}.



\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figure/IMA}
\caption{Visualized pipeline of IMA.}
\label{figure3}
\end{figure}




\vspace{1mm}
\noindent\textbf{Mutual feature refinement.} After generating the self-correlation maps for each modality, the information of each modality can now be effectively transferred to each other using a cross attention mechanism. From , the key features  and value features  are first calculated as

where  indicates a pixel-wise (HW-wise) fully-connected layer. Next, the correspondence map  of the key features of each modality, i.e.,  and , are computed as

As each channel in  contains certain properties of prototypes, relations between every constructed prototype are contained in . Based on ,  from one modality are propagated to the other modality as

where  indicates the transferred features. Finally, the input features are concatenated with the transferred features, and the refined features  are obtained as

where  indicates channel concatenation and  is a convolutional layer for feature refinement. Through this mutual refinement process, each modality can appropriately reflect semantic information from another modality.




\subsection{Inter-Frame Attention (IFA)}
As much as aligning the feature domains of multiple modalities is important, exploiting temporal coherence of a video is also an effective strategy for unsupervised VOS. Existing approaches, such as COSNet~\cite{COSNet}, AD-Net~\cite{AD-Net}, F2Net~\cite{F2Net}, and IMP~\cite{IMP}, design their network architectures to utilize this temporal coherence. However, they are either time-consuming owing to their iterative workflows or not fully leveraging the global context of a video. To overcome these limitations, we propose IFA to efficiently leverage the temporal coherence of a video. The proposed IFA consists of three parts: reference frame sampling, prototype generation, and temporal propagation. In Figure~\ref{figure4}, we visualize the architecture of IFA.



\vspace{1mm}
\noindent\textbf{Reference frame sampling.} The objective of IFA is to collect and store the global context of a video and propagate it to each query frame. To efficiently obtain the global context, we sample the frames in a video and selectively store the sampled frames rather than storing all frames. As the sampling method, we adopt the uniform sampling strategy, i.e., the frames are sampled while keeping the intervals identical. For example, if we want to sample  frames from the -length video, the sampled frames are defined as  where




\vspace{1mm}
\noindent\textbf{Prototype generation.} Before transferring semantic context of the reference frames to the query frame, we first transform the input features  to prototypes  similar to \textit{prototype generation} in Section~\ref{IMA}. Note that the prototype construction is implemented for each frame separately. The generated prototypes are then used as key, query, and value features for attention mechanism where value features of reference frames are transferred to query frame based on correspondence scores. As in IMA,  can be directly used instead of  for attention embedding, but we use  to obtain better feature representations. The effects of this "feature--prototype conversion" process is also analyzed in Section~\ref{analysis}.



\vspace{1mm}
\noindent\textbf{Temporal context propagation.} After constructing prototypes of the selected reference frames and the query frame, we extract key features  and value features  from the reference frames and query features  from the query frame. Here, key all embedding processes are implemented for each frame separately. Then, the correspondence map  is generated as

Based on , the context of the reference frames are adaptively read and stored in read features  as

The generated  has  prototypes with feature size of , which contain information of the sampled frames. As it does not have spatial information related to the query frame, it cannot be directly leveraged for the feature fusion process. Therefore, we calculate the correlation scores  between  and  to force the temporally transferred information to have the same spatial size as the input features, as follows:

As  and  have the same spatial size, feature fusion between them can now be easily achieved. The refined features  is defined as

By employing the proposed IFA, the semantic context of the reference frames is propagated to the query frame for reliable cue generation. In particular, when reliable information cannot be obtained from a single frame owing to difficulties such as occlusions, the use of IFA can lead to stable functioning of a system. 



\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figure/IFA}
\caption{Visualized pipeline of IFA.}
\label{figure4}
\end{figure}


\subsection{Implementation Details}
\noindent\textbf{Optical flow map.} Following a common protocol for two-stream approaches in unsupervised VOS, we generate optical flow maps from a pre-trained optical flow estimation model. The generated two-channel motion flow maps are converted to three-channel RGB flow maps and then saved in advance. As an optical flow estimation model, we adopt RAFT~\cite{RAFT} pre-trained on the Sintel~\cite{Sintel} dataset. 


\vspace{1mm}
\noindent\textbf{Network design.} We adopt VGG-16~\cite{vgg} as our backbone encoder for the appearance branch and motion branch. The encoded features are refined using IMA and IFA, where IMA is adopted for the fourth and fifth encoding blocks and IFA is adopted at the fifth encoding block. Note that IMA and IFA are separately adopted in the fifth encoding block, that is, they are employed in a parallel manner and then fused later. After refining the encoded features using IMA and IFA, an ASPP module~\cite{ASPP} is applied to obtain stronger feature representations. The decoder takes the features from the ASPP module as its input, and gradually refines those features using lower-level features from the encoder.



\vspace{1mm}
\noindent\textbf{Two-stage network training.} Following previous methods, such as F2Net~\cite{F2Net}, RTNet~\cite{RTNet}, FSNet~\cite{FSNet}, and PMN~\cite{PMN}, we train our network using multiple steps. As the first step, a salient object detection dataset DUTS~\cite{DUTS} is adopted to pre-train the model on large-scale data. Both DUTS training set and test set are used as our training dataset. As it is an image-level dataset, only RGB images are available. Therefore, we only train the appearance branch and copy the learned parameters to the motion branch after the pre-training is done. Then, the entire model is trained on the DAVIS 2016~\cite{DAVIS} training set and YouTube-VOS 2018~\cite{YTVOS} training set with appearance as well as motion branches turned on. If a video sequence contains multiple objects, we regard them as a single object to obtain binary ground truth masks. Training snippets are randomly sampled from the DAVIS 2016 training set and YouTube-VOS 2018 training set with the same probabilities.


\vspace{1mm}
\noindent\textbf{Training details.} For network optimization, we use cross-entropy loss and the Adam optimizer~\cite{adam}. The learning rate is decayed from 1e-4 to 1e-5 using the cosine annealing scheduler~\cite{cosine}, and the batch size is set to 16. For network training, two GeForce RTX 3090 GPUs are used. 


\section{Experiments}
In this section, the datasets used in this study are first introduced in Section~\ref{datasets}. Each proposed component is analyzed in Section~\ref{analysis}. Quantitative and qualitative comparison with other methods can be found at Section~\ref{quanti} and Section~\ref{quali}, respectively. Note that our method is abbreviated as DPA. 


\subsection{Datasets}
\label{datasets}
In this study, we use three datasets for network training (DUTS~\cite{DUTS}, DAVIS 2016~\cite{DAVIS}, and YouTube-VOS~\cite{YTVOS}) and three datasets for network testing (DAVIS 2016, FBMS~\cite{FBMS}, and YouTube-Objects~\cite{YTOBJ}). The most popular dataset is DAVIS 2016, which consists of 30 training videos and 30 validation videos. The performance of an unsupervised VOS network is mainly evaluated on the DAVIS 2016 validation set. FBMS and YouTube-Objects are less important than DAVIS 2016, but are also widely used datasets for validating the performance of VOS models. 



\subsection{Evaluation Metrics}
We employ three evaluation metrics in this study: region similarity , boundary accuracy , and their average .  and  can be calculated as follows:


For DAVIS 2016~\cite{DAVIS} validation set, , , and  are used for network evaluation, while only  is used for evaluating the segmentation performance on the FBMS~\cite{FBMS} test set and YouTube-Objects~\cite{YTOBJ} dataset.



\subsection{Analysis}
\label{analysis}
To verify the effectiveness of the proposed components, we perform an ablation study on them, the results of which are present in Table~\ref{Table:ablation}. Note that the model versions for ablation study are trained and tested with 352352-resolution videos and evaluated on the DAVIS 2016 validation set.


\noindent\textbf{Use of IMA and IFA.} To compare the model performance with and without IMA and IFA, we compare model version \Romannum{1}, \Romannum{2}, \Romannum{3}, and \Romannum{4}. As presented in the table, IMA and IFA both bring significant performance improvements to the baseline model. When IMA is employed alone, 2.5\% improvements on  are obtained, which implies that identifying inter-relationships between RGB images and optical flow maps via cross attention is effective for blending two distinct streams. IFA also brings meaningful improvements,  score of 2.0\%, backing up the need for global observation for specifying the primary objects. If IMA and IFA are used together, outstanding performance is obtained,  of 86.9\%. This indicates that IMA and IFA can constructively compensate each other as they focus on separate problems lying in two-stream unsupervised VOS. The qualitative effects of adopting IMA and IFA can also be found at Figure~\ref{figure1}. In the figure, feature maps of various embedding stages are visualized. We compare four feature maps, i.e., feature maps before IMA or IFA, feature maps after IMA, feature maps after IFA, and feature maps after IMA and IFA. It can be seen that IMA and IFA are each effective for capturing and specifying the salient objects (clearer edges and higher confidence for object regions). The joint use of IMA and IFA is even better than using IMA or IFA alone, validating the compatibility of the proposed modules.  


\vspace{1mm}
\noindent\textbf{Number of reference frames.} As described before, IFA can take an arbitrary number of frames as reference frames given that it is based on an attention mechanism. To determine the optimal number of frames, we compare model versions that use various numbers of reference frames. As shown in model version \Romannum{5}, \Romannum{6}, \Romannum{7}, and \Romannum{8}, employing more reference frames generally leads to a higher segmentation performance. This proves that as the amount of information from a video increases, the model becomes more generalized and robust against occlusions caused by external stuffs. However, if the number of reference frames is larger than three, the performance gain is not satisfactory considering additional computations. In other words, providing semantic cues in three representative frames of a video is enough for specifying the objects.


\vspace{1mm}
\noindent\textbf{Prototype embedding.} In IMA and IFA, we adopt prototypes as input of attention mechanisms instead of the original input features. In Table~\ref{Table:ablation}, we quantitatively compare the model versions with and without the prototype embedding. For both IMA and IFA, employing prototype embedding as a pre-processing step brings meaningful improvements, demonstrating its effectiveness as a feature refinement tool. We also compare the value feature maps in IMA at Figure~\ref{figure5}. As can be seen from the figure, value features generated from self-correlation maps instead of the original input features makes features of different modalities to focus on similar regions. This backups the need for prototype embedding, as it helps IMA to use more refined features as source information for detecting the most salient object.



\begin{table}[t!]
\centering 
\caption{Ablation study on the proposed components.  and  indicate the use of prototype embedding and the number of reference frames used in IFA.}
\vspace{1mm}
\small
\begin{tabular}{c|P{1cm}|P{1cm}|P{0.6cm}|P{0.6cm}P{0.6cm}P{0.6cm}}
\toprule
Version &IMA &IFA & & & &\\
\midrule
\Romannum{1} & & &- &83.4 &83.2 &83.5\\
\Romannum{2} &w/  & &- &85.9 &85.4 &86.3\\
\Romannum{3} & &w/  &4 &85.4 &85.0 &85.8\\
\Romannum{4} &w/  &w/  &4 &86.9 &86.3 &87.4\\
\midrule
\Romannum{5} &w/  &w/  &1 &86.3 &85.8 &86.9\\
\Romannum{6} &w/  &w/  &2 &86.5 &86.0 &87.1\\
\Romannum{7} &w/  &w/  &3 &86.8 &86.2 &87.5\\
\Romannum{8} &w/  &w/  &5 &86.9 &86.3 &87.5\\
\midrule
\Romannum{9} &w/o  &w/  &4 &86.1 &85.5 &86.6\\
\Romannum{10} &w/  &w/o  &4 &86.3 &85.7 &87.1\\
\Romannum{11} &w/o  &w/o  &4 &85.3 &84.6 &86.0\\
\bottomrule
\end{tabular}
\label{Table:ablation}
\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figure/corr}
\caption{Visualized activation map of different IMA versions.}
\label{figure5}
\end{figure}







\begin{table*}
\centering 
\caption{Quantitative evaluation on the DAVIS 2016 validation set and FBMS test set. OF and PP indicate the use of optical flow estimation models and post-processing techniques, respectively.}
\vspace{1mm}
\small
\begin{tabular}{p{2.3cm}P{2cm}P{1.5cm}P{0.5cm}P{0.5cm}P{1cm}P{1cm}P{1cm}P{1cm}P{1cm}}
\toprule
\multicolumn{6}{c}{} &\multicolumn{3}{c}{DAVIS 2016} &\multicolumn{1}{c}{FBMS}\\
\cline{7-10}
Method &Publication &Resolution &OF &PP &fps & & & &\\
\midrule
PDB~\cite{PDB} &ECCV'18 &473473 & &\checkmark &20.0 &75.9 &77.2 &74.5 &74.0\\
MOTAdapt~\cite{MOTAdapt} &ICRA'19 &- & &\checkmark &- &77.3 &77.2 &77.4 &-\\
AGS~\cite{AGS} &CVPR'19 &473473 & &\checkmark &10.0 &78.6 &79.7 &77.4 &-\\
COSNet~\cite{COSNet} &CVPR'19 &473473 & &\checkmark &- &80.0 &80.5 &79.4 &75.6\\
AD-Net~\cite{AD-Net} &ICCV'19 &480854 & &\checkmark &4.00 &81.1 &81.7 &80.5 &-\\
AGNN~\cite{AGNN} &ICCV'19 &473473 & &\checkmark &3.57 &79.9 &80.7 &79.1 &-\\
MATNet~\cite{MATNet} &AAAI'20 &473473 &\checkmark &\checkmark &20.0 &81.6 &82.4 &80.7 &76.1\\
WCS-Net~\cite{WCS-Net} &ECCV'20 &320320 & & &\underline{33.3} &81.5 &82.2 &80.7 &-\\
DFNet~\cite{DFNet} &ECCV'20 &- & &\checkmark &3.57 &82.6 &83.4 &81.8 &-\\
3DC-Seg~\cite{3DC-Seg} &BMVC'20 &480854 & &\checkmark &4.55 &84.5 &84.3 &84.7 &-\\
F2Net~\cite{F2Net} &AAAI'21 &473473 & & &10.0 &83.7 &83.1 &84.4 &77.5\\
RTNet~\cite{RTNet} &CVPR'21 &384672 &\checkmark &\checkmark &- &85.2 &85.6 &84.7 &-\\
FSNet~\cite{FSNet} &ICCV'21 &352352 &\checkmark &\checkmark &12.5 &83.3 &83.4 &83.1 &-\\
TransportNet~\cite{TransportNet} &ICCV'21 &512512 &\checkmark & &12.5 &84.8 &84.5 &85.0 &78.7\\
AMC-Net~\cite{AMC-Net} &ICCV'21 &384384 &\checkmark &\checkmark &17.5 &84.6 &84.5 &84.6 &76.5\\
DConv3D~\cite{D^2Conv3D} &WACV'22 &480854 & & &- &86.0 &85.5 &86.5 &-\\
IMP~\cite{IMP} &AAAI'22 &- & & &1.79 &85.6 &84.5 &86.7 &77.5\\
HFAN~\cite{HFAN} &ECCV'22 &512512 &\checkmark & &12.4 &\underline{87.0} &\underline{86.6} &87.3 &-\\
PMN~\cite{PMN} &WACV'23 &352352 &\checkmark & &- &85.9 &85.4 &86.4 &77.7\\
TMO~\cite{TMO} &WACV'23 &384384 &\checkmark & &\textbf{43.2} &86.1 &85.6 &86.6 &\underline{79.9}\\
\midrule
\textbf{DPA} & &352352 &\checkmark & &25.4 &86.9 &86.3 &\underline{87.4} &78.9\\
\textbf{DPA} & &512512 &\checkmark & &20.2 &\textbf{87.6} &\textbf{87.1} &\textbf{88.2} &\textbf{81.0}\\
\bottomrule
\end{tabular}
\label{Table:DAVIS,FBMS}
\end{table*}



\begin{table*}
\centering 
\caption{Quantitative evaluation on the YouTube-Objects dataset. Performance is reported using the  mean.}
\vspace{1mm}
\small
\begin{tabular}{p{1.9cm}P{1.2cm}P{0.9cm}P{0.9cm}P{0.9cm}P{0.9cm}P{0.9cm}P{0.9cm}P{0.9cm}P{1.2cm}P{0.9cm}}
\toprule
Method &Aeroplane &Bird &Boat &Car &Cat &Cow &Dog &Horse &Motorbike &Train\\
\midrule
PDB~\cite{PDB} &78.0 &80.0 &58.9 &76.5 &63.0 &64.1 &70.1 &67.6 &58.4 &35.3\\
AGS~\cite{AGS} &\textbf{87.7} &76.7 &\textbf{72.2} &78.6 &69.2 &64.6 &73.3 &64.4 &62.1 &48.2\\
COSNet~\cite{COSNet} &81.1 &75.7 &71.3 &77.6 &66.5 &69.8 &76.8 &67.4 &67.7 &46.8\\
AGNN~\cite{AGNN} &71.1 &75.9 &70.7 &78.1 &67.9 &69.7 &\underline{77.4} &67.3 &\underline{68.3} &47.8\\
MATNet~\cite{MATNet} &72.9 &77.5 &66.9 &79.0 &73.7 &67.4 &75.9 &63.2 &62.6 &51.0\\
WCS-Net~\cite{WCS-Net} &81.8 &\underline{81.1} &67.7 &79.2 &64.7 &65.8 &73.4 &68.6 &\textbf{69.7} &49.2\\
RTNet~\cite{RTNet} &84.1 &80.2 &70.1 &\underline{79.5} &71.8 &70.1 &71.3 &65.1 &64.6 &53.3\\
AMC-Net~\cite{AMC-Net} &78.9 &80.9 &67.4 &\textbf{82.0} &69.0 &69.6 &75.8 &63.0 &63.4 &57.8\\
HFAN~\cite{HFAN} &84.7 &80.0 &\underline{72.0} &76.1 &\underline{76.0} &\textbf{71.2} &76.9 &\textbf{71.0} &64.3 &\underline{61.4}\\
TMO~\cite{TMO} &85.7 &80.0 &70.1 &78.0 &73.6 &\underline{70.3} &76.8 &66.2 &58.6 &47.0\\
\midrule
\textbf{DPA} &\underline{85.9} &\textbf{83.6} &68.4 &78.4 &\textbf{77.2} &68.3 &\textbf{78.0} &\underline{70.0} &59.4 &\textbf{64.3}\\
\bottomrule
\end{tabular}
\label{Table:YTOBJ}
\end{table*}



\subsection{Quantitative Results}
\label{quanti}
In Table~\ref{Table:DAVIS,FBMS} and Table~\ref{Table:YTOBJ}, we present quantitative comparison between our proposed method and existing state-of-the-art methods on DAVIS 2016~\cite{DAVIS} validation set, FBMS~\cite{FBMS} test set, and YouTube-Objects~\cite{YTOBJ} dataset. Our model is tested on a single GeForce RTX 3090 GPU.

\vspace{1mm}
\noindent\textbf{DAVIS 2016.} On the DAVIS 2016 validation set, the methods using predicted optical flow maps show notable performance. Specifically, PMN~\cite{PMN} and TMO~\cite{TMO} achieve  scores of 85.9\% and 86.1\%, respectively. The best performance among the existing methods is obtained by HFAN~\cite{HFAN}, which shows 87.0\% on , 86.6\% on , and 87.3\% on . However, it should be considered that HFAN uses higher input resolution compared to other methods. Our proposed DPA outperforms all other methods on the same condition, i.e., input resolution. With 352352 resolution, it achieves a  score of 86.9\%, whereas if a higher resolution of 512512 is adopted, a  score of 87.6\% is obtained. It is also notable that while showing such satisfactory performance, the inference speed of the system is as efficient as other fast methods. 

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figure/result}
\caption{Qualitative comparison between DPA and other state-of-the-art methods.}
\label{figure6}
\end{figure*}



\vspace{1mm}
\noindent\textbf{FBMS.} Unlike the DAVIS 2016 validation set, the FBMS test set contains multi-object scenarios as well as single-object scenarios. Even under this condition, DPA outperforms all other existing approaches by a reasonable margin with a  score of 1.1\%. This demonstrates that DPA is robust for videos containing multiple objects as well.


\vspace{1mm}
\noindent\textbf{YouTube-Objects.} Compared to the DAVIS 2016 validation set and FBMS test set, YouTube-Objects dataset is a much more challenging dataset. DPA surpasses other methods in these challenging cases as well. Among ten classes, DPA achieves the first place on four classes. 



\subsection{Qualitative Results}
\label{quali}
We compared DPA to COSNet~\cite{COSNet} and MATNet~\cite{MATNet} on DAVIS 2016~\cite{DAVIS} validation set in Figure~\ref{figure5}. In the first sequence, DPA keeps tracking the dancer stably, while other methods are distracted by background distractions. In the second and the third sequence, the target objects are rapidly moving and severely occluded by obstacles. Even under such conditions, DPA detects different primary objects compared to other susceptible trackers. 



\section{Conclusion}
In unsupervised VOS, multi-modality fusion and temporal aggregation are considered as essential components, but they have limitations such as lack of thorough information exchange or substantial time consumption. To further improve them, we propose two novel attention modules, IMA and IFA, by incorporating a prototype framework. By leveraging IMA and IFA in a collaborative manner, we achieve outstanding performance on all public benchmark datasets.






{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}