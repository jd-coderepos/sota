\chapter{The linear combination method}\label{ch:LCmethod}
In this chapter we introduce the linear combination method, or the LC method for short. We present in \SCref{LCprelim} some preliminary lemmas. Then we describe in \SCref{convLC} how to perform a conversion step. In \SCref{equivalent} we give definitions and results about equivalence of DAEs and address how equivalence is related to the LC method.

For simplicity, throughout this report, we consider only the second type of SA's failures described in \SCref{idfailure}: ``singular'' means identically singular but not structurally singular.
Based on this assumption, symbolic cancellations are not considered an issue that makes the \Sigmeth fail. 


\section{Preliminary lemmas}\label{sc:LCprelim}


\begin{lemma}\label{le:griewank}
{\bf\em (Griewank's Lemma)}\cite[Lemma 5.1]{nedialkov2007solving}
Let $v$ be a function of $t$, $x_j$'s and derivatives of them $(j=\oneton)$. Denote $v^{(p)}=\text{d}^pv/\text{d}t^p$, where $p>0$. If $\hoder{x_j}{v}\le q$,
then
\begin{equation*}\pp{v}{x_j^{(q)}}=\pp{v'}{x_j^{(q+1)}}.
\end{equation*}
Hence
\begin{equation}\label{eq:griewank2}
\pp{v}{x_j^{(q)}} = \pp{v'}{x_j^{(q+1)}} \cdots = \pp{v^{(p)}}{x_j^{(q+p)}}.
\end{equation}
\end{lemma}



\begin{lemma}\label{le:sigred0}
Let $\Sigma$ and $\newSig$ be $n\times n$ signature matrices. 
Assume $\val{\Sigma}$ is finite,  $\~c,\~d$ are valid offsets for $\Sigma$, 
and $\newsij{i}{j} \le d_j-c_i$ for all $i,j=\oneton$. 
If a HVT in $\newSig$ contains a position $(i,j)$ where $\newsij{i}{j} < d_j-c_i$, then $\val{\newSig} < \val{\Sigma}$.
\end{lemma}
\begin{proof}
Let $T$ be a HVT in $\newSig$. Then
\begin{align}\label{eq:lcstrictless}
\val{\newSig} &= \sum_{(i,j)\in T} \newsij{i}{j} < \sum_{j=1}^n d_j - \sum_{i=1}^n c_i = \val{\Sigma}.\qed
\end{align}
\renewcommand{\qed}{}
\end{proof}

\begin{corollary}\label{co:sigred}
For a row index $\indxk$, let
\begin{align*}\tgnstretch
\Biggl\{
\begin{array}{ll}
\newsij{i}{j}=\sij{i}{j} &\qquad\text{for all $i\neq \indxk$ and all $j$, and}\\
\newsij{\indxk}{j}< d_j-c_\indxk &\qquad\text{for all $j$.}
\end{array}
\end{align*}
Then $\val{\newSig} < \val{\Sigma}$.
\end{corollary}

\begin{proof}
Since $\newsij{\indxk}{j} < d_j-c_\indxk$ for all $j$, the intersection of a HVT in $\newSig$ with positions in row $\indxk$ is a position $(\indxk,r)$ with $\newsij{\indxk}{r} < d_r-c_\indxk$. By \LEref{sigred0},  
$\val{\newSig} < \val{\Sigma}$.
\end{proof}

This lemma shows that, if we replace a row $\indxk$ in $\Sigma$ with a row with entries less than $d_j-c_\indxk$ for each column $j$, then the value of this signature matrix decreases.

\section{Conversion step}\label{sc:convLC}









Given a SWP DAE of the form \rf{maineq}, assume that we apply the $\Sigma$-method and obtain a singular system Jacobian $\Jac$. We seek a reformulation of this DAE so that the system Jacobian $\newJ$ of the new DAE may be generically nonsingular. We denote by $\Sigma$ and $\newSig$ the signature matrices of the original DAE and this new DAE, respectively. Denote by $\~c,\~d$ the valid offsets for $\Sigma$.

We describe below how to perform a conversion step using a linear combination (LC) of equations.  We call this conversion technique the {\em LC conversion method}, or simply the {\em LC method}.
The main result from this conversion is that, under certain conditions, we can obtain an equation in a row, say $\indxk$, such that $x_j$ occurs in this row of order $<d_j-c_\indxk$ for all $j$. Hence by \COref{sigred}, $\val{\newSig}< \val{\Sigma$}.

We assume $n\ge 2$. Let $u$ be a nonzero vector function from the null space of $\Jac^T$.
Here, $\Jac$ and $u$ are considered as functions of $t$, $x_j$'s and appropriate derivatives of them.

Denote by $\eqsetI(u)$ the set of indices for which the $i$th component of $u$ is not identically zero
\begin{align}\label{eq:idef}
\eqsetI(u) = \{ \, i \mid  u_i\neq 0 \,\},
\end{align}
and let
\begin{align}\label{eq:thdef}
\theta(u) = \min_{i \in \eqsetI(u)} c_i. 
\end{align}
Since $u$ is nonzero and $\Jac$ is identically singular, $\eqsetI(u)$ has at least two elements. Otherwise $\Jac$ has a row of identical zeros and is structurally singular.

\begin{remark}
We consider $u$ in its simplest form in the sense that its elements do not have a common factor comprising $t$, $x_j$'s, or/and derivatives of them. For instance, in \EXref{lenaintro}, we do not use $u=(0,0,x'_1,-x'_1)^T$ though $\Jac^Tu=\~0$, but use $u=(0,0,1,-1)^T$.

Also, we do not consider $u$ with any fractions. For example, we use $u=(0,0,x'_1, x_1x_2)^T$ instead of 
$(0,0,x_1^{-1},x_2(x'_1)^{-1})^T$. 
\end{remark}

The {\em sufficient condition} for applying the LC method is the following: for a nonzero $u\in \ker(\Jac^T)$,
\begin{equation}\label{eq:LCcond}
\boxed{\hoder{x_j}{u}< d_j-\theta(u) \qquad \text{for all $j=\oneton$}}
\end{equation}
 If this condition is satisfied, then we can perform a conversion step. We explain this ``sufficiency" in \RMref{suffLC}.

Denote by $\nzset(u)\subseteq \eqsetI(u)$ the set of indices $\indxk$ such that the $\indxk$th component of $u$ is not an identical zero and $c_\indxk=\theta(u)=\min_{i \in \eqsetI(u)} c_i$:
\begin{align}\label{eq:LCsetK}
\nzset(u) = \setbg{\,\indxk\in \eqsetI(u) \mid\, c_\indxk=\theta (u)}.
\end{align}
From \rf{thdef}, there exists at least one $\indxk\in \eqsetI(u)$ such that $c_\indxk=\theta(u)$, so $\nzset(u)\neq\emptyset$.

We choose an $\indxk\in \nzset(u)$ and replace $f_\indxk$ by 
\begin{align}\label{eq:replacing}
\newf_\indxk = \sum_{i\in \eqsetI(u)} u_i f_i^{\left(c_i-\theta(u)\right)}.
\end{align}
We refer to \rf{replacing} as a {\em conversion step} using the LC method
and to the resulting DAE as a {\em converted} DAE. Critical for the success of the LC method is the following lemma.
\begin{theorem}\label{le:LC}
For a SWP DAE with identically singular $\Jac$, let $u$ be a nonzero $n$-vector such that $\Jac^Tu=\~0$.
If 
\begin{equation*}\hoder{x_j}{u}< d_j-\theta(u) \qquad \text{for all $j=\oneton$}
\end{equation*}
and we replace $f_\indxk$ by $\newf_\indxk$ in \rf{replacing}, 
then the converted DAE has $\newSig$ with $\val{\newSig} < \val{\Sigma}$.
\end{theorem}

First, we illustrate with an example how to perform a conversion step, and then we prove this lemma. Since $u$ is fixed during a conversion step, for brevity we write $\eqsetI(u)$, $\theta(u)$, and $\nzset(u)$ as $\eqsetI$, $\theta$, and $\nzset$, respectively\footnote{This set $L$ is not to be confused with the constant $L$ in the pendulum-related DAEs.}.

\begin{example}\label{ex:FGxy}
Consider
\begin{equation}\label{eq:FGxy}
\begin{aligned}
0= f_1 &= -x'_1 + x_3\\
0= f_2 &= -x'_2+ x_4\\
0= f_3 &= F(x_1,x_2) \\
0= f_4 &= x_3 F_{x_1}(x_1,x_2) + x_4 F_{x_2}(x_1,x_2) + G(x_1,x_2).
\end{aligned}
\end{equation}
Here $F_{x_1}(x_1,x_2)=\partial F(x_1,x_2)/\partial x_1$, and similarly we write $F_{x_2}(x_1,x_2)$, $G_{x_1}(x_1,x_2)$, and $G_{x_2}(x_1,x_2)$. 


\begin{align*}\renewcommand{\arraystretch}{1.5}
\Sigma=
\begin{blockarray}{rcccc ll}
&  x_1 &   x_2 &  x_3 &  x_4 & \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[cccc]ll}
f_1 & 1^\bullet  &-  & 0& -    &\s0  \\
f_2 & -  &1  & -& 0^\bullet    &\s0  \\
f_3 & 0  &0^\bullet  & -& -    &\s1  \\
f_4 & \lgo &\lgo  & 0^\bullet & 0    &\s0  \\
\end{block}
 \s{d_j}& \s1 &\s1&\s0  &\s0 & \valSig{\Sigma}{1}\\
 \end{blockarray}
\Jac = 
\begin{blockarray}{rcccc cc}
&  x_1 &   x_2 &  x_3 &  x_4 \\
\begin{block}{r @{\hspace{10pt}}[cccc]cc}
f_1 & -1  &0  & 1& 0      \\
f_2 & 0  &-1  & 0& 1      \\
f_3 & F_{x_1}  & F_{x_2}  &0 &0\\
f_4 & 0  &0  & F_{x_1} & F_{x_2}     \\
\end{block}
& \mcdetJac{5}{\detJac{\Jac}{0}}
\end{blockarray}
\end{align*}
Because of singular $\Jac$, the SA fails. It reports structural index 2, but the differentiation index is 3. If we take $u = \bigl( F_{x_1},\, F_{x_2},\, 1,\, -1\bigr)^T$, then $\Jac^T u=\~0$.

We illustrate \rrf{idef}{LCsetK}:
\begin{align*}
\eqsetI &= \setbg{\,i \mid\, u_i\neq 0\,} = \setbg{1,2,3,4}, \\
\theta &= \min_{i\in \eqsetI}c_i=0, \\
\nzset &= \setbg{\,\indxk\in \eqsetI \mid\, c_\indxk=\theta=0} = \setbg{1,2,4}.
\end{align*}
Then we check if condition \rf{LCcond} holds:
\begin{align*}
\hoder{x_1}{u} &= \;\;\, 0 \;\;< 1 = d_1-\theta, \\
\hoder{x_2}{u} &= \;\;\, 0 \;\;< 1 = d_2-\theta, \\
\hoder{x_3}{u} &= -\infty < 0 = d_3-\theta,\quad\text{and}\quad \\
\hoder{x_4}{u} &= -\infty < 0 = d_4-\theta. \qquad
\end{align*}
Hence $\hoder{x_j}{u}<d_j-\theta$ for all $j$.

Using \rf{replacing} gives
\begin{align*}
\newf &= \sum_{i\in \eqsetI} u_i f_i^{(c_i-\theta)} 
= \sum_{i\in \eqsetI} u_i f_i^{(c_i)} 
\\&= F_{x_1} f_1 + F_{x_2} f_2 + f'_3 - f_4 \\
&= F_{x_1} (-x'_1+x_3) + F_{x_2} (-x'_2+x_4) + (F(x_1,x_2))' - (x_3 F_{x_1}+ x_4 F_{x_2}+G) \\
&= - x'_1F_{x_1} + x_3F_{x_1} -  x'_2F_{x_2} + x_4F_{x_2} + x'_1F_{x_1} + x'_2F_{x_2} - x_3F_{x_1} - x_4F_{x_2} -G \\
&= -G.
\end{align*}
Now, with $\theta=0$,
\begin{align*}
\hoder{x_1}{\newf} &= \;\;\, 0 \;\; < 1 = d_1-\theta, \\
\hoder{x_2}{\newf} &= \;\;\, 0 \;\; < 1 = d_2-\theta, \\
\hoder{x_3}{\newf} &= -\infty < 0 = d_3-\theta, \quad\text{and}\quad\\
\hoder{x_4}{\newf} &= -\infty < 0 = d_4-\theta. \qquad
\end{align*}
That is, $\hoder{x_j}{\newf}<d_j-\theta$ for all $j$. 


For each $\indxk\in \nzset=\setbg{1,2,4}$, assuming $u_\indxk\neq 0$, we can replace $f_\indxk$ by $\newf_\indxk=\newf$. We show in the following the three possible converted DAEs, each with $\val{\newSig}=0$ and generically nonsingular $\newJ$.

\begin{itemize}
\item $\indxk=1$:
\begin{equation}\label{eq:FGxyk=1}
\begin{aligned}
0= \newf_1 &= -G(x_1,x_2)\\
0= f_2 &= -x'_2+ x_4\\
0= f_3 &= F(x_1,x_2) \\
0= f_4 &= x_3 F_{x_1}(x_1,x_2) + x_4 F_{x_2}(x_1,x_2) + G(x_1,x_2)
\end{aligned}
\end{equation}
\begin{align*}\tgnstretch
\newSig=
\begin{blockarray}{rcccc ll}
&  x_1 &   x_2 &  x_3 &  x_4 & \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[cccc]ll}
\newf_1 & 0^\bullet  &0  & -& -    &\s1  \\
f_2 & -  &1  & -& 0^\bullet    &\s0  \\
f_3 & 0  &0^\bullet  & -& -    &\s1  \\
f_4 & \lgo  &\lgo  & 0^\bullet & 0    &\s0  \\
\end{block}
 \s{d_j}& \s1 &\s1&\s0  &\s0 &\valSig{\newSig}{0}\\
 \end{blockarray}
\newJ = 
\begin{blockarray}{rcccc cc}
&  x_1 &   x_2 &  x_3 &  x_4 \\
\begin{block}{r @{\hspace{10pt}}[cccc]cc}
\newf_1 & -G_{x_1}  & -G_{x_2}  & 0 & 0      \\
f_2 & 0  &-1  & 0& 1      \\
f_3 & F_{x_1}  & F_{x_2}  &0 &0\\
f_4 & 0  &0  & F_{x_1} & F_{x_2}     \\
\end{block}
\mcdetJac{6}{\detJac{\newJ}{F_{x_1}(F_{x_1}G_{x_2} - F_{x_2}G_{x_1})}}
\end{blockarray}
\end{align*}
When $u_1 = F_{x_1}\neq 0$ and $F_{x_1}G_{x_2} \neq F_{x_2}G_{x_1}$, the determinant is nonzero and the SA succeeds.

\item $\indxk=2$:
\begin{equation}\label{eq:FGxyk=2}
\begin{aligned}
0= f_1 &= -x'_1+ x_3\\
0= \newf_2 &= -G(x_1,x_2)\\
0= f_3 &= F(x_1,x_2)\\
0= f_4 &= x_3 F_{x_1}(x_1,x_2) + x_4 F_{x_2}(x_1,x_2) + G(x_1,x_2)
\end{aligned}
\end{equation}
\begin{align*}\tgnstretch
\newSig=
\begin{blockarray}{rcccc ll}
&  x_1 &   x_2 &  x_3 &  x_4 & \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[cccc]ll}
f_1 & 1  &-  & 0^\bullet& -    &\s0  \\
\newf_2 & 0^\bullet  &0  & -& -    &\s1  \\
f_3 & 0  &0^\bullet  & -& -    &\s1  \\
f_4 & \lgo  &\lgo  & 0 & 0^\bullet    &\s0  \\
\end{block}
 \s{d_j}& \s1 &\s1&\s0  &\s0 &\valSig{\newSig}{0}\\
 \end{blockarray}
\newJ = 
\begin{blockarray}{rcccc cc}
&  x_1 &   x_2 &  x_3 &  x_4  \\
\begin{block}{r @{\hspace{10pt}}[cccc]cc}
f_1 & -1 & 0 & 1 & 0 \\
\newf_2 & -G_{x_1}  & -G_{x_2}  & 0 & 0      \\
f_3 & F_{x_1}  & F_{x_2}  &0 &0\\
f_4 & 0  &0  & F_{x_1} & F_{x_2}     \\
\end{block}
\mcdetJac{6}{\detJac{\newJ}{F_{x_2}(F_{x_1}G_{x_2} - F_{x_2}G_{x_1})}}
\end{blockarray}
\end{align*}
Similarly, the SA succeeds when $u_2=F_{x_2}\neq 0$ and $F_{x_1} G_{x_2}\neq F_{x_2} G_{x_1}$.

\item $\indxk=4$:
\begin{equation}\label{eq:FGxyk=4}
\begin{aligned}
0= f_1 &= -x'_1+ x_3\\
0= f_2 &= -x'_2 + x_4\\
0= f_3 &= F(x_1,x_2) \\
0= \newf_4 &= -G(x_1,x_2)
\end{aligned}
\end{equation}
\begin{align*}\tgnstretch
\newSig=
\begin{blockarray}{rcccc ll}
&  x_1 &   x_2 &  x_3 &  x_4 & \s{c_i}  \\
\begin{block}{r @{\hspace{10pt}}[cccc]ll}
f_1 & 1  &-  & 0^\bullet& -    &\s0  \\
f_2 & -  &1  & -& 0^\bullet    &\s0  \\
f_3 & 0  &0^\bullet  & -& -    &\s1  \\
\newf_4 & 0^\bullet  &0  & -& -    &\s1  \\
\end{block}
\s{d_j}& \s1 &\s1&\s0  &\s0 &\valSig{\newSig}{0}\\
\end{blockarray}
\newJ = 
\begin{blockarray}{rcccc cc}
&  x_1 &   x_2 &  x_3 &  x_4  \\
\begin{block}{r @{\hspace{10pt}}[cccc]cc}
f_1 & -1  & 0 & 1& 0    \\
f_2 & 0  &-1  & 0& 1    \\
f_3 & F_{x_1} & F_{x_2} & 0& 0    \\
\newf_4 & -G_{x_1}  &-G_{x_2}  & 0& 0    \\
\end{block}
\mcdetJac{6}{\detJac{\newJ}{-F_{x_1}G_{x_2} + F_{x_2}G_{x_1}}}
\end{blockarray}
\end{align*}
In this case, SA's success requires only $F_{x_1}G_{x_2} \neq F_{x_2}G_{x_1}$.
\end{itemize}

\end{example}

Using the LC method, we obtain three converted DAEs from \rf{FGxy}: \rf{FGxyk=1}, \rf{FGxyk=2}, and \rf{FGxyk=4}. 
However, it is not guaranteed that all converted DAEs and the original DAE have exactly the same solution sets.
We will cover this equivalence issue in \SCref{equivalent}.

Now we prove \LEref{LC}.
\begin{proof}
We show first that 
\begin{align*}
\newsij{\indxk}{j} = \hoder{x_j}{\newf_\indxk} < d_j-c_\indxk \qquad \text{for all $j=\oneton$.}
\end{align*}

Since $\theta=\min_{i\in \eqsetI} c_i$, $c_i-\theta \ge 0$ for $i\in \eqsetI$. By \rf{cidj}, $\hoder{x_j}{f_i}=\sij{i}{j}\le d_j-c_i$. Applying Griewank's Lemma \rf{griewank2} to \rf{sysjac}, with $q=c_i-\theta$, gives
\begin{align}\label{eq:gl}
\Jac_{ij} 
= \frac{\partial f_i}{\partial x_j^{(d_j-c_i)}} 
= \frac{\partial f_i^{(c_i-\theta)}}{\partial x_j^{(d_j-c_i+c_i-\theta)}} 
= \frac{\partial f_i^{(c_i-\theta)}}{\partial x_j^{(d_j-\theta)}} \qquad \text{for $i\in \eqsetI$}.
\end{align}
Then for all $j=\oneton$,
\begin{alignat}{3}
0 &= (\Jac^Tu)_j = \sum_{i=1}^n u_i(\Jac^T)_{ji}= 
\sum_{i \in \eqsetI}^n u_i \Jac_{ij}  &\hspace{10mm}&\text{using $\Jac^Tu=\~0$} \nonumber\\
&=\sum_{i \in \eqsetI} u_i \frac{\partial f_i}{\partial x_j^{(d_j-c_i)}} 
=\sum_{i \in \eqsetI} u_i \frac{\partial f_i^{(c_i-\theta)}}{\partial x_j^{(d_j-\theta)}} 
&&\text{using \rf{gl}}
\nonumber\\
&= \frac{\partial\left(
\sum_{i \in \eqsetI} u_i  f_i^{(c_i-\theta)}\right)}
{\partial x_j^{(d_j-\theta)}} 
 &&\text{using $\hoder{x_j}{u}<d_j-\theta$ for all $j$}\hspace{-5mm}
\label{eq:notdepend}\\
&= \frac{\partial \newf_\indxk}{\partial x_j^{(d_j-\theta)}}
 &&\text{using \rf{replacing}} \nonumber.
\end{alignat}
This shows that $\newf_\indxk$ does not truly depend on $x_j^{(d_j-\theta)}$, that is,
\begin{align*}
\newsij{\indxk}{j}=\hoder{x_j}{\newf_\indxk} < d_j-\theta = d_j-c_\indxk\qquad\text{for all $j=\oneton$}.\label{newseq}
\end{align*}
By \COref{sigred}, $\val{\newSig}< \val{\Sigma$}.
\end{proof}

\begin{remark}\label{rm:nameLC}
We name this method ``LC'' because of the following considerations. The vector $u$ from the null space of $\Jac^T$ that satisfies \rf{LCcond} does not comprise the leading derivatives $x_j^{(d_j-\theta)}$ for all $j$ in equation $f_i^{\left(c_i-\theta\right)}$ for all $i\in \eqsetI$. We consider here each $u_i$ as a ``constant'' in 
\[
\newf_\indxk = \sum_{i\in \eqsetI} u_i f_i^{\left(c_i-\theta\right)},
\]
and $\newf_\indxk$ as a ``linear combination'' of equations $f_i^{\left(c_i-\theta\right)}$.
\end{remark}

\begin{remark}\label{rm:suffLC}
If $\hoder{x_j}{u}= d_j-\theta$ for some $j$, then $\val{\newSig}$ is not guaranteed $< \val{\Sigma}$. In this case, we cannot swap the sum and the differentiation operator in \rf{notdepend}. Therefore, we cannot prove $\partial\newf_\indxk / \partial x_j^{(d_j-\theta)}=0$. Then, in $\newSig$, it can happen that
\[
\newsij{\indxk}{j} = \hoder{x_j}{\newf_\indxk}=d_j-\theta = d_j-c_\indxk \qquad \text{for some $j$},
\]
giving $\le$ instead of strictly $<$ in \rf{lcstrictless}.

However, if $\hoder{x_j}{u}<d_j-\theta$ holds for $j$ from a particular set, we can still achieve $\val{\newSig}<\val{\Sigma}$. We leave this investigation for future work, consider the condition \rf{LCcond} sufficient for now, and require it to be satisfied for the LC method.
\end{remark}


If $u$ is a constant vector, then $\hoder{x_j}{u}=-\infty$ for every $x_j$, and the condition \rf{LCcond} is automatically satisfied. In this case we do not need to check it. We illustrate this in the next example.
\begin{example}\label{ex:xyzt}
Consider
\begin{equation*}\begin{aligned}
0 = f_1 &= x_1 + tx_2 + t^2x_3 + g_1(t)\\
0 = f_2 &= x'_1 + tx'_2 + t^2x'_3 + g_2(t)\\
0 = f_3 &= x''_1 + tx''_2 + 2t^2x''_3 + g_3(t).
\end{aligned}
\end{equation*}
\begin{align*}\tgnstretch
\Sigma=
\begin{blockarray}{rccc ll}
& x_1 &  x_2 &  x_3 &  \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[ccc]ll}
f_1 & 0^\bullet  & 0 &0   &   \s2 \\
f_2 & 1 &1^\bullet  &1 &   \s1  \\ 
f_3 & 2  &2 & 2^\bullet & \s0  \\
\end{block}
 \s{d_j}& \s2 &\s2 &\s2 &\valSig{\Sigma}{3}
 \end{blockarray}
\Jac = \begin{blockarray}{rccc cc}
& x_1 & x_2 & x_3 \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
f_1 & 1  &t    &t^2      \\
f_2 & 1  &t    &t^2      \\
f_3 & 1  &t    &2t^2    \\  
\end{block}
&\mcdetJac{3}{\detJac{\Jac}{0}}
\end{blockarray}
\end{align*}
For $u=(-1,1,0)^T$, $\Jac^Tu=0$. Using \rrf{idef}{LCsetK} gives
\[
\eqsetI=\setbg{1,2},\quad \theta=c_2=1,\quad \text{and}\quad \nzset=\setbg{2}.
\]
Since $u$ is a constant vector, condition \rf{LCcond} is satisfied. We replace $f_2$ by
\begin{align*}
\newf_2 &= u_1 f^{(2-1)}_1 + u_2 f^{(1-1)}_2 \\
&= -f'_1 + f_2 \\
&= -\bigl( x_1 + tx_2 + t^2x_3 - g_1 \bigr)'+ (x'_1 + tx'_2 + t^2x'_3 + g_2) \\
&= - x_2 - 2tx_3 - g'_1+g_2.
\end{align*}

The converted DAE is
\begin{alignat*}{3}
0 &= f_1 &&= x_1 + tx_2 + t^2x_3 + g_1\\
0 &= \newf_2 &&= -x_2 - 2tx_3 - g'_1+g_2\\
0 &= f_3 &&= x''_1 + tx''_2 + 2t^2x''_3 + g_3.
\end{alignat*}
\begin{align*}\tgnstretch
\newSig=
\begin{blockarray}{rccc ll}
& x_1 &  x_2 &  x_3 &  \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[ccc]ll}
f_1 & 0^\bullet  & 0 &0   &   \s2 \\
\newf_2 &  - &0^\bullet  &0 &   \s2  \\ 
f_3 & 2  &2 & 2^\bullet & \s0  \\
\end{block}
 \s{d_j}& \s2 &\s2 &\s2  &\valSig{\newSig}{2} \\
 \end{blockarray}
\newJ = \begin{blockarray}{rccc cc}
& x_1 &  x_2 & x_3 \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
f_1 & 1  &t    &t^2      \\
\newf_2 & 0  &-1  &-2t     \\
f_3 & 1  &t    &2t^2      \\
\end{block}
&\mcdetJac{3}{\detJac{\Jac}{-t^2}}
\end{blockarray}
\end{align*}
If $t>\sqrt{\epsilon}$ for a suitable $\epsilon$ depending on the machine precision, then $\Jac$ is computably nonsingular.
\end{example}

\section{Equivalent DAEs}\label{sc:equivalent}
First, we give a definition for equivalent DAEs.
\begin{definition}\label{df:equivalent}
Let $\daeF$ and $\newdaeF$ denote two DAEs. They are {\em equivalent} (on some interval for $t$)
if a solution of $\daeF$ is a solution to $\newdaeF$
and vice versa.
\end{definition}

In the following context, we denote by $\daeF$ the original DAE with equations $f_i$, $i=\oneton$, and singular Jacobian $\Jac$. After a conversion step using the LC method, we obtain a (converted) DAE, denoted by $\newdaeF$, with equations $\newf_i$, $i=\oneton$, and Jacobian $\newJ$, which may be singular.

\begin{theorem}\label{th:equivLC}
After a conversion step using the LC method, DAEs $\daeF$ and $\newdaeF$ are equivalent on some real time interval $\intvI$ for $t$, if $u_\indxk\neq 0$ for all $t\in\intvI$.
\end{theorem}

\begin{proof}
Let a solution of $\daeF$, over some interval $\intvI\subset \bbR$, be a vector-valued function 
\[
\~x(t)=\bigl(x_1(t),\ldots,x_n(t)\bigr)^T
\]
that satisfies \rf{maineq} for all $t\in \intvI$.

We denote the vector used in the LC method by $u = (u_1,\ldots, u_n)$, where its $i$th component is of the form
\begin{equation*}u_i = u_i
\left(t;\, x_1,x'_1,\ldots,x_1^{(d_1-\theta-1)};\, \ldots ;\,x_n,x'_n,\ldots,\,x_n^{(d_n-\theta-1)}
\right).
\end{equation*}

If $u$ is defined at $(t,\~x(t))$ for all $t\in \intvI$, then
\[
\newf_\indxk  =  \sum_{i\in \eqsetI} u_i f_i^{(c_i-\theta)} \quad\text{and}\quad \newf_i = f_i \,\text{ for }\, i\neq l
\]
 vanish at $ (t,\~x(t))$, and thus  $\~x(t)$ is a solution to $\newdaeF$.

Conversely, assume that $\newx(t)$ is a solution of $\newdaeF$ on $\intvI$. If $u$ is defined at $(t,\newx(t))$ for all $t\in \intvI$ and
$u_l\neq 0$, then 
\begin{equation}\label{eq:LCsolution2}
f_\indxk =\frac{1}{u_\indxk}\biggl( \newf_\indxk- \sum_{i\in \eqsetI\backslash \{\indxk\}} u_i \newf_i^{(c_i-\theta)}\biggr)
 \quad\text{and}\quad f_i=\newf_i \,\text{ for }\, i\neq l
\end{equation}
vanish at $(t,\newx(t))$, and thus $\newx(t)$ is a solution to $\daeF$.

By \DFref{equivalent}, $\daeF$ and $\newdaeF$ are equivalent.
\end{proof}

\begin{remark}\label{rm:goodchoiceLC}
We can see from \rf{LCsolution2} that, if we have a choice for $l$, it is desirable to choose it such that $u_l$ is identically nonzero, e.g., a nonzero constant,  $x_1^2+1$, or $2+\cos^2 x_3$. In this case, $\daeF$ and $\newdaeF$ are {\em always} equivalent---we do not need to check the {\em equivalence condition} $u_l\neq 0$ when we solve $\newdaeF$.
\end{remark}

\begin{example}
In \EXref{FGxy}, case $\indxk=1$ [resp. $\indxk=2$]  requires $F_{x_1}\neq 0$ [resp. $F_{x_2}\neq 0$] to recover the original DAE \rf{FGxy} from \rf{FGxyk=1} [resp. from \rf{FGxyk=2}].  However, for case $\indxk=4$,  $u_4=1$ is a nonzero constant for any $t$. Therefore this choice is more desirable than the other two.
\end{example}



Below we define an ill-posed DAE using the structural posedness defined in the \daesa papers \cite{NedialkovPryce2012a,NedialkovPryce2012b}.
\begin{definition}\label{df:illposed}
A DAE is ill posed if it has an equivalent DAE that is structurally ill-posed (SIP); otherwise it is well posed.
\end{definition}

\begin{example}
Consider problem \rf{pendmess1}. Using $0=f_3=x^2+y^2-\pendL^2$, we reduce $f_1$ to the trivial $0=\newf_1=0$. This is just performing a simple substitution, and is not applying the LC method.
The signature matrix
\begin{align}\label{eq:SigIllposed}
\tgnstretch
\newSig = 
\begin{blockarray}{rccc cc}
&  x &   y & \lam  \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
\newf_1 & -  & -  & -   \\
f_2 & -  & 2  & 0  \\
f_3 & 0  & 0  & -   \\
\end{block}
 \end{blockarray}
 \end{align}
 does not have a finite HVT, so the resulting DAE is SIP. Hence, by \DFref{illposed} , the original SWP DAE \rf{pendmess1}  is ill posed.
\end{example}

\begin{corollary}\label{co:illposed}
If a structurally well-posed DAE can be converted, by the LC method, to an equivalent DAE that is structurally ill-posed, then the original DAE is ill posed.
\end{corollary}

\begin{proof}
This follows from \THref{equivLC} and \DFref{illposed}.
\end{proof}

\begin{example}\label{ex:pendmess2}
Consider the following SWP DAE
\begin{equation}\label{eq:pendmess2}
\begin{aligned}
0 = f_1 &= y'''+y'\lam+y\lam' \\
0 = f_2 &= y''+y\lam -\pendG\\
0 = f_3 &= x^2+y^2-\pendL^2.
\end{aligned}
\end{equation}
\begin{align*}\tgnstretch
\Sigma = 
\begin{blockarray}{rccc ll}
&  x &   y & \lam  & \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[ccc]ll}
f_1 & -  & 3  & 1^\bullet & \s0 \\
f_2 & -  & 2^\bullet  & 0 & \s1 \\
f_3 & 0^\bullet  & 0  & - & \s0  \\
\end{block}
 \s{d_j}& \s0 &\s3 &\s1  &\valSig{\Sigma}{3} \\
 \end{blockarray}
\Jac = \begin{blockarray}{rccc cc}
& x &  y & \lam \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
f_1 & 0  &1    &y  \\
f_2 & 0  &1    &y  \\
f_3 & 2x  &0    &0  \\
\end{block}
&\mcdetJac{3}{\detJac{\Jac}{0}}
\end{blockarray}
\end{align*}

For $u=(1,-1,0)^T$, $\Jac^Tu=0$. Using \rrf{idef}{LCsetK} gives
\[
\eqsetI=\setbg{1,2},\quad \theta=c_1=0,\quad \text{and}\quad \nzset=\setbg{1}.
\]
Since $u$ is a constant vector, condition \rf{LCcond} is satisfied. We replace $f_1$ by
\begin{align*}
\newf_1 &= f_1-f'_2 = (y'''+y'\lam+y\lam') - (y''+y\lam-g)' = 0.
\end{align*}
The signature matrix of the resulting problem is exactly \rf{SigIllposed}. Hence, by \COref{illposed}, \rf{pendmess2} is ill posed.
\end{example}



If the Jacobian of the converted DAE is still singular, we may be able to apply the LC method iteratively, provided condition \rf{LCcond} is satisfied on each iteration. Since after each conversion step we reduce the value of the signature matrix by at least 1, the number of iterations does not exceed $\val{\Sigma}$, where $\Sigma$ is for the original DAE. We use \EXref{messpend} to show how we can iterate with the LC method.
\begin{example}\label{ex:messpend}
We construct the following (artificial) \modpenda DAE from \pend \rf{pend}:
\begin{equation}\label{eq:messpendLC}
\begin{aligned}
0 = A &= f_3+f_1' = x^2+y^2-\pendL^2 + (x''+x\lam)'\\
0 = B &= f_1+A'' = x''+x\lam + \bigl(x^2+y^2-\pendL^2 + (x''+x\lam)'\bigr)''\\
0 = C &= f_2+A''' = y''+y\lam-\pendG + \bigl(x^2+y^2-\pendL^2 + (x''+x\lam)'\bigr)'''.
\end{aligned}
\end{equation}
\begin{align*}\tgnstretch
\Siter{0} = 
\begin{blockarray}{rccc ll}
&  x &   y & \lam  & \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[ccc]ll}
A & 3^\bullet  & 0  & 1 & \s3 \\
B & 5  & 2^\bullet  & 3 & \s1 \\
C & 6  & 3  & 4^\bullet & \s0  \\
\end{block}
 \s{d_j}& \s6 &\s3 &\s4  &\valSig{\Siter{0}}{9} \\
 \end{blockarray}
\Jiter{0} = \begin{blockarray}{rccc cc}
& x &  y & \lam \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
A & 1  &2y    &x  \\
B & 1  &2y    &x  \\
C & 1  &2y    &x  \\
\end{block}
&\mcdetJac{3}{\detJac{\Jiter{0}}{0}}
\end{blockarray}
\end{align*}
Here, a superscript denotes an iteration number, not a power. We show how to recover the simple pendulum problem.

We find a vector in $\ker(\JiterT{0})$: $\uiter{0}=(-1,1,0)^T$. Then 
\[
\Iiter{0}=\setbg{1,2},\quad \thiter{0}=1,\quad \text{and} \quad \Kiter{0}=\setbg{2}.
\]
We replace the second equation $B$ by
\begin{align*}
-A^{(3-1)}+B = -A''+(A''+f_1) = f_1 = x''+x\lam.
\end{align*}
The converted DAE is
\begin{alignat*}{3}
0 &= A &&= x^2+y^2-\pendL^2 + (x''+x\lam)'\\
0 &= f_1 &&= x''+x\lam\\
0 &= C &&=  y''+y\lam-\pendG + \bigl(x^2+y^2-\pendL^2 + (x''+x\lam)'\bigr)'''.
\end{alignat*}
\begin{align*}\tgnstretch
\Siter{1} = 
\begin{blockarray}{rccc ll}
&  x &   y & \lam  & \s{c_i}  \\
\begin{block}{r @{\hspace{10pt}}[ccc]ll}
A & 3^\bullet  & 0  & 1 & \s3 \\
f_1 & 2  & -  & 0^\bullet & \s4 \\
C & 6  & 3^\bullet  & 4 & \s0  \\
\end{block}
 \s{d_j}& \s6 &\s3 &\s4  &\valSig{\Siter{1}}{6} \\
 \end{blockarray}
\Jiter{1} = \begin{blockarray}{rccc cc}
& x &  y & \lam \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
A & 1  &2y    &x  \\
f_1 & 1  &0    &x  \\
C & 1  &2y    &x  \\
\end{block}
&\mcdetJac{3}{\detJac{\Jiter{1}}{0}}
\end{blockarray}
\end{align*}

Although $\val{\Siter{1}}=6<9=\val{\Siter{0}$}, matrix $\Jiter{1}$ is still singular. If $\uiter{1}=(-1,0,1)^T$, then $\JiterT{1} \uiter{1}=\~0$. This gives 
\[
\Iiter{1}=\setbg{1,3},\quad \thiter{1}=0,\quad \text{and} \quad \Kiter{1}=\setbg{3}. 
\]
We replace the third equation $C$ by
\begin{align*}
-A^{(3-0)}+C = -A'''+(f_2+A''') = f_2 = y''+y\lam-\pendG.
\end{align*}
The converted DAE is
\begin{alignat*}{3}
0 &= A &&= x^2+y^2-\pendL^2 + (x''+x\lam)'\\
0 &= f_1 &&= x''+x\lam\\
0 &= f_2 &&= y''+y\lam -\pendG.
\end{alignat*}
\begin{align*}\tgnstretch
\Siter{2} = 
\begin{blockarray}{rccc ll}
&  x &   y & \lam  & \s{c_i} \\
\begin{block}{r @{\hspace{10pt}}[ccc]ll}
A & 3^\bullet  & \lgo  & 1 & \s0  \\
f_1 & 2  & -  & 0^\bullet    & \s1  \\
f_2 & -  & 2^\bullet  & \lgo & \s0 \\
\end{block}
\s{d_j}& \s3 &\s2 &\s1 &\valSig{\Siter{2}}{5} \\
\end{blockarray}
\Jiter{2} = \begin{blockarray}{rccc cc}
& x &  y & \lam \\
\begin{block}{r @{\hspace{10pt}}[ccc]cc}
A & 1     &0   &x    \\
f_1 & 1     &0   &x    \\ 
f_2 & 0     &1   &0    \\
\end{block}
&\mcdetJac{3}{\detJac{\Jiter{2}}{0}}
\end{blockarray}.
\end{align*}

We have $\val{\Siter{2}}=5<6=\val{\Siter{1}}$, but $\Jiter{2}$ is still singular. We find $u=(1,-1,0)^T$ such that $\JiterT{2}\uiter{2}=\~0$. Then 
\[
\Iiter{2}=\{1,2\},\quad \thiter{2}=0,\quad \text{and}\quad \Kiter{2}=\setbg{1}. 
\]
Replacing the first equation $A$ by
\begin{align*}
A-f_1' = (f_3+f_1')-f_1' = f_3 = x^2+y^2-\pendL^2,
\end{align*}
we recover $f_1,f_2,f_3$ from \rf{messpendLC}. This is exactly the DAE \pend\rf{pend}, with $\val{\Sigma}=2$ and $\det(\Jac)=-2\pendL^2$; cf. \EXref{simplepend}.

Since each $u$ in every conversion iteration is a constant vector, each $u_\indxk$ we pick is a nonzero constant. By \RMref{goodchoiceLC}, the original DAE \rf{messpendLC} and \pend are {\em always} equivalent. Hence, we can solve \rf{messpendLC} by simply solving \pend.
\end{example}

%
