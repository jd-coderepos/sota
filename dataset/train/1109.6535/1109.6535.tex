\documentclass[10pt,twocolumn]{article} \usepackage{amsmath,epsf,amssymb,cite,pifont,amsthm, mathrsfs,epsfig,  bbm, amsthm,  setspace}
\input xy
\xyoption{all}
\graphicspath{ {Figures/} }
\topmargin-1in \textheight9.9in \textwidth6.8in \pagestyle{plain}
\oddsidemargin -0.2in \evensidemargin -0.2in

\newtheorem{thm}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{defn}{Definition}







\renewcommand{\AA}{\mathcal A}
\renewcommand{\aa}{\mathfrak{a}}
\newcommand{\C}{\mathbb C}
\newcommand{\CC}{\mathcal C}
\newcommand{\DD}{\mathcal D}
\newcommand{\E}{\mathbb E}
\newcommand{\F}{\mathbb F}
\newcommand{\FF}{\mathcal F}
\newcommand{\HQ}{\mathbb H}
\newcommand{\M}{\mathbb M}
\newcommand{\MM}{\mathcal M}
\newcommand{\N}{\mathbb N}
\renewcommand{\O}{\mathcal O}
\renewcommand{\P}{\mathbb P}
\newcommand{\PP}{\mathcal P}
\newcommand{\p}{\mathfrak p}
\newcommand{\R}{\mathbb R}
\newcommand{\RR}{\mathcal R}
\renewcommand{\SS}{\mathcal S}
\newcommand{\UU}{\mathcal U}
\newcommand{\U}{\mathcal U}
\newcommand{\Q}{\mathbb Q}
\newcommand{\X}{\mathbb X}
\newcommand{\Y}{\mathbb Y}
\newcommand{\Z}{\mathbb Z}







\newcommand{\cF}{\Scr F}
\newcommand{\fix}{}
\newcommand{\iso}{\cong}
\newcommand{\homotopic}{\simeq}
\newcommand{\Aut}{\textrm{Aut}}
\newcommand{\Hom}{\textrm{Hom}}
\newcommand{\open}{\mathcal{O}}
\newcommand{\Perm}{\textrm{Perm}}
\newcommand{\im}{\textrm{Im }}
\newcommand{\Ker}{\textrm{Ker }}
\newcommand{\inv}{^{-1}}
 \newcommand{\e}{\varepsilon}
\renewcommand{\1}{\mathbbm{1}}
\newcommand{\homeo}{\approx}
\renewcommand{\phi}{\varphi}
\newcommand{\Moebius}{M\"{o}bius }
\newcommand{\Poincare}{Poincar\'{e} }
\newcommand{\Cech}{\v{C}ech }
\renewcommand{\th}{ }
\newcommand{\interior}[1]{\mathring{#1}} 
\newcommand{\CP}{\C \textrm{P}}
\newcommand{\RP}{\R \textrm{P}}
\newcommand{\HP}{\HP \textrm{P}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\Cl}{\mathcal{C}\ell}
\newcommand{\GL}{\textrm{GL}}
\newcommand{\SL}{\textrm{SL}}
\newcommand{\To}{\longrightarrow}
\newcommand{\Mapsto}{\longmapsto}
\def\labto#1{\mathrel{\mathop\to^{#1}}} \newcommand{\normal}{\trianglelefteq}
\newcommand{\ideal}[1]{\langle #1 \rangle}
\newcommand{\emptyword}{(\,\,)}
\newcommand{\Syl}{\textrm{Syl}}
\newcommand{\subgroup}{\leqslant}
\newcommand{\lcm}{\textrm{lcm}}
\newcommand{\Ext}{\textrm{Ext}}
\newcommand{\rank}{\textrm{rank}}
\newcommand{\Gal}{\textrm{Gal}}

\newcommand{\st}{}
\newcommand{\nd}{}
\newcommand{\low}{\textrm{low}}
\newcommand{\Lk}{\textrm{Lk}}
\newcommand{\St}{\textrm{St}}

\newcommand{\RF}{\mathcal{R},\mathcal{F}}
\newcommand{\Ch}{\textrm{Ch}}

\newcommand{\norm}[1]{\| #1 \|}





\newcommand{\mindeath}{d}
\newcommand{\twosimps}{\ell}
\newcommand{\edges}{m}
\newcommand{\death}{r}
\newcommand{\sizeset}{k}
\newcommand{\chiint}{n}





\title{Failure Filtrations for Fenced Sensor Networks
\footnote{Research is partially supported by NSF under grants 
NSF-DMS-09-43760 and
NSF-DMS-10-45153, 
AFOSR  under grant  FA9550-10-1-0436, 
and
NIH under grant K25-AI079404.
}}
\author{Elizabeth Munch\footnote{Department of Mathematics, Duke University, Durham, NC},  Michael Shapiro\footnote{Department of Pathology, Tufts University, Boston, MA}, John Harer\footnote{Departments of Mathematics, Computer Science, and Electrical and Computer Engineering, Program in Computational Biology and Bioinformatics, Duke University, Durham, NC}}
\date{\today}






\begin{document}
\maketitle

\begin{abstract}

In this paper we consider the question of sensor network coverage for  a 2-dimensional domain.
We seek to compute the probability  that a set of sensors fails to cover given only
non-metric, local (who is talking to whom) information
and a probability distribution of failure of each node.
This builds on the work of de Silva and Ghrist who analyzed this
problem in the deterministic situation.
We first show that a it is part of a slightly larger class of problems which is \#P-complete, and thus fast algorithms likely do not exist unless PNP.
We then give a deterministic algorithm which is feasible in the case of a small set of sensors,
and give a dynamic algorithm for an arbitrary set of sensors failing over time which utilizes a new criterion for coverage based on the one proposed by de Silva and Ghrist.
These algorithms build on the theory of topological persistence
\cite{Edelsbrunner2010}.

\end{abstract}



\section{Introduction}


The newly emerged field of Computational Topology \cite{Edelsbrunner2010} continues to find ever
increasing areas of application.
Perhaps its most significant application so far has been in the use of topological data analysis (TDA) on a wide variety
of datasets   \cite{Edelsbrunner2010} \cite{Carlsson2009} \cite{Chazal2009},
and has also been used effectively to find
structure in images \cite{Carlsson2008}\cite{Edelsbrunner2009},
shape in proteins and protein complexes \cite{Agarwal2006}\cite{Ban2004}\cite{Headd2007}
and in many other areas.
Recently, it was applied to sensor networks in \cite{Ghrist2005}, \cite{DeSilva2006}, \cite{DeSilva2007},\cite{Tahbaz-Salehi2010}
and the current paper is an extension of \cite{DeSilva2006}. 

Topology enters the study of sensor network when we consider questions like coverage.
When does a set of sensors effectively monitor a region and when are there gaps?
Phrasing this geometrically,
we start with a set of sensors  in a domain 
where each can detect objects in a circular region of fixed radius ,
and we ask if the union of these discs covers all of .
This problem has been studied quite a bit,
but previous to \cite{DeSilva2006}, most work fell into one of two groups -
approaches that utilized geometric analysis to obtain an exact answer
and those that sought a non-deterministic approximation but assumed
significant capabilities of the sensors.
For a survey of the literature, see \cite{Yick2008}.

The former approach
requires a great deal of prior knowledge about the geometry of the domain and the exact location of the sensors.
The latter,
does not require this exactness, but often requires a uniform distribution of nodes or a high level
of intelligence in the
sensors.
The main contribution of  \cite{DeSilva2006} was a criterion for
coverage that  requires none of these things.


In the current paper, we take a middle ground and address  the question of computing  the probability of failure of
the criterion of \cite{DeSilva2006} given the probability of failure of each sensor.
We  show that a computing the probability of failure for a generalized set of complexes is   NP-hard, 
but we give an algorithm which can be used to solve small instances of the problem,
and an alternative, dynamic algorithm to give an early warning of potential failure.



\subsubsection*{Outline. }
The paper is organized as follows.
Section \ref{S: Persistent Homology} gives an introduction to Rips complexes and persistent homology,
both of which will be used extensively in this paper.
In Section \ref{S: Coverage Criterion}, we  summarize the problem and results of \cite{DeSilva2006}.
Section \ref{S: Sensor Failure} adds the assumption that sensors have a probability of failure, and Section
\ref{S: Complexity} discusses the complexity issues of determining the probability that there is coverage of
the domain.
Section \ref{S: Deterministic Algorithm} presents a deterministic algorithm for those times when
the set of sensors is small enough, and Section \ref{S: Monitored System} gives a
dynamic algorithm for use when the set of sensors is too large.


\section{Rips Complexes and Persistent Homology}\label{S: Persistent Homology}

Let  be a set of points in  and suppose that  is given.
We are interested in the topology of , the union of balls of radius  about the points
of .
One can build a variety of complexes with vertices  that capture this topology,
the simplest and most intuitive is the \Cech complex which has a simplex
 whenever the balls of radius  about the  have a non-trivial
intersection.
The nerve lemma tells us that \Cech does indeed capture the topology of ,
but it can be difficult to compute.
In particular, it requires one to know the exact location of the points of , a luxury
that we do not have in this case.
We must therefore use an approximation known as the Rips complex for the problem
at hand.

The Rips complex   has a -dimensional simplex 
whenever  for every pair of vertices .
Unfortunately, the Rips complex does not retain the homotopy type of ,
but what is lost in topological data is made up for in ease of computation.
Furthermore, the only  information necessary
to build the Rips complex is the set of pairs of points whose distance is below a the prescribed threshold .


Since we will be considering filtrations of our simplicial complex and looking at how the topology
changes with the change of the simplicial complex, a brief review of persistent homology is in order \cite{Edelsbrunner2010}.

We begin with a filtration of a simplicial complex , given by a series of inclusions
\begin{center}
 \includegraphics[scale=.85]{HomologyFiltration3}
\end{center}
which induces maps on homology

\begin{center}
 \includegraphics[scale=.85]{HomologyFiltration4}
\end{center}
We will use homology with  coefficients for the entirety of the paper.
Consider how the sequence of homology groups changes as the simplicial complex changes.
A class  is {\em born} at  if it is not in the image of the map
.
This class {\em dies} entering  if once there it merges with an older class,
i.e. lies in the image of the map .
See Figure \ref{F:BirthDeathClass}.


\begin{figure}
 \begin{center}
\includegraphics[scale = .8]{BirthDeathClass}
 \end{center}
 \caption{A visual representation of births and deaths of homology classes.
 The class  is born at  because it is not in the image of the map from .
 It dies entering  because it was still not in the image of  at ,
 but has merged with an older class upon entering .}
\label{F:BirthDeathClass}
\end{figure}

To determine when classes are born and die in the filtration, we build the boundary matrix .
This is a square matrix with a row and column for each simplex in ,
ordered with respect to the filtration (which  ensures that a simplex comes after all of its faces).
 is a - matrix which has a  in location  if and only if the th simplex is a
face of the th.
Applying the persistence algorithm as in \cite{Edelsbrunner2010} yields a reduced matrix ,
where  is the boundary matrix and  is an elementary matrix storing the column operations
performed on  during the persistence algorithm.
Here, a reduced matrix is one in which every column is either completely zero, or the lowest 
in the column is not the same as the lowest 1 in any other column.
We write  if the lowest 1 in column  of matrix  is in row .

To read the births and deaths from the matrix, note that a -dimensional class is born with addition of a
dimension  simplex  if the column corresponding to  is completely zero.
A representative for the class that is born is stored in the corresponding column of .
The class born at  dies with the addition of  if the lowest one of the column corresponding to
 is in the row corresponding to ,
in which case  and  are paired.
If the addition of a simplex  gives birth to a class, it is called a \textit{positive simplex}.
Similarly, if the addition of a simplex  gives death to a class, it is called a \textit{negative simplex}.

The de Silva-Ghrist criterion requires us to work with persistent \textit{relative} homology.
In this case, we take a pair , where  is a subcomplex of , and a filtration
\begin{center}
 \includegraphics[scale=.85]{HomologyFiltration2}
\end{center}
inducing maps on relative homology
\begin{center}
 \includegraphics[scale=.85]{HomologyFiltration}
\end{center}
and consider births and deaths in the usual way.
This requires a slight modification of the boundary matrix by reordering the rows so that those
simplices which are in the subspace  are moved to the top of the matrix prior to performing the persistence algorithm.
Then the addition of  gives birth to a class if its column is either zero,
or the lowest one in its column corresponds to a simplex in .
Simplex  gives death to a class if its lowest one corresponds to a simplex which is not in .
Other than these distinctions, computing persistent homology in the relative case is the same as in the absolute case.


\section{The Coverage Criterion}\label{S: Coverage Criterion}

Working in a simply-connected domain in the plane,
suppose that we have a set of sensors with a fixed radius of coverage.
Our goal is check that these sensors cover the whole domain.
What makes this a challenging problem is that
{\em we do not assume that we know the locations of the sensors}.
This means that standard geometric techniques are not applicable.
Instead we turn to topology to answer the coverage question by building
the {\em Rips complex} on the set of sensors,
thought of as points in the plane.
We can then use homology to check for holes in the coverage.

Let    be the set of points corresponding to the location of the set of sensors in a compact connected domain 
which has a piecewise linear boundary.
Suppose that each sensor has a fixed coverage radius .
The question is whether every point in  lies within distance  of some sensor in .
We do not use the distance  to build the Rips complex, instead we add an additional capability to each sensor.
Let  be fixed, with  for technical reasons.
Each sensor is given a unique identification number to broadcast.
If another node is within distance , it can hear the signal and identify the ID
number, but it has no information about the location of the broadcaster.
In particular, it does not know its direction or its exact distance,  only that that distance
is less than .
Whenever two sensors can hear each others' identification number, an edge is
placed in the Rips complex.
Higher dimensional simplices are then added when all of their faces are already there.

The boundary of the domain  is taken to be
piecewise linear with a sensor at each of its vertices.
The boundary is called the
 \textbf{fence}, and
each  node in the fence knows the identification number of its two
fence neighbors, both of which are  within distance .

Summarizing, following \cite{DeSilva2006},
 the assumptions are:
\begin{enumerate}
	\item Nodes  broadcast their unique ID numbers.
	          Each node can detect the identity of any node within broadcast radius .
	\item Nodes have radially symmetric covering domains of cover radius .
	\item Nodes lie in a compact connected domain  whose boundary
	          is connected and piecewise-linear with vertices marked fence
	         nodes .
		  Non-fence nodes are called interior nodes,  and denoted .
	\item Fence nodes  are ordered cyclically and each  knows the
	        identities of its two neighbors on .
	        These neighbors both lie within distance  of .
\end{enumerate}
Based on this information, build the Rips complex  with the fence  as a subcomplex.
With this setup, de Silva and Ghrist in \cite{DeSilva2006}
give their controlled boundary criterion for coverage:
\begin{thm}[de Silva/Ghrist Criterion (dS-G)] \label{Criterion}
   If there is a nontrivial element of the relative homology group
    which maps to a nonzero class under the
   connecting homomorphism ,
   then the union of the disks of radius  about the nodes contains all of .
\end{thm}

The class  is \textbf{fundamental} if it satisfies the criterion of
Theorem \ref{Criterion},
but we stress that when there is such an element it is not necessarily unique.
The term \textbf{absolute cycle} will be used for a class in  that comes
from , which is equivalent to saying that it maps to 
under the connecting homomorphism.

The assumption that  is required to compensate for the fact that the Rips complex
does not accurately reflect the topology of the cover.
While this bound promises that holes in the cover appear also as holes in the Rips complex,
we can still create examples where phantom holes appear in the Rips complex even though no hole exists in the cover itself.
In a perfect world, this theory would be built on \Cech complexes,
however the lack of location data for the nodes makes this method impossible.







\section{Sensor Failure} \label{S: Sensor Failure}
Over time, sensors have a likelihood of failure which increases the longer the system is in place,
caused perhaps by malicious actions, environmental conditions or mechanical failure.
As nodes fail, there are two possible effects on the coverage:
either the death of a subset of nodes
creates a hole in the Rips complex,
or the removal of the nodes does not affect the existence of a fundamental class.
Once again, we emphasize
 that we are specifically not looking for the probability of failure of the \textit{cover}
over time, just the failure of the dS-G criterion.
For this reason, we also assume that only interior nodes can fail.
The loss of a fence node causes instant failure of the dS-G criterion,
so there is nothing to check in this case.

Let us start with a Rips complex pair  built from a set of nodes .
At time , we assume we have a fundamental class .
If a set of interior sensors  fails, any simplex in 
that has a vertex in the set  is lost.
Therefore this subcomplex, , can be thought of as the largest subcomplex of  that has
  as its vertices.
We could then determine whether  fails the dS-G criterion by looking for a fundamental class in
, but this is a very narrow view of the problem.
Much more information is available in a filtration that we will now construct.
Note that it will  contain  as one of its subcomplexes.

Let .
Order the nodes so that
 and .
Let  so that ,
and let  be the maximal subcomplex of  with vertices .
Then the filtration
\begin{center}
 \includegraphics[scale=.85]{HomologyFiltration5}
\end{center}
induces maps on relative homology
\begin{center}
 \includegraphics[scale=.85]{HomologyFiltration6}
\end{center}
An example of this filtration is illustrated in Figure \ref{fig: rips complex example}.
\begin{figure}[ht]
\begin{center}
\includegraphics{deathsetexampleripsB10}
\end{center}
\caption{ A small scale example of the Rips complex  built from a set of points in the plane.
The outer ring of nodes labeled with letters is the fence .} \label{fig: rips complex example}
\end{figure}
Intuitively, we expect that discovering a fundamental element at any point in this sequence implies
that there is a fundamental element in any subsequent group.

 \begin{lemma}\label{L:map}
Let  be subsets of .  Then if  is fundamental,  its image under the map

is also  fundamental.
\end{lemma}

  \begin{proof}
 Consider the commutative diagram
 
 where the horizontal maps are induced by the inclusion  and the vertical maps are the boundary maps.
Since  is fundamental,   in .
Since the diagram commutes,  and hence is nonzero.
This also implies that  is nonzero, so it is a fundamental element of .
  \end{proof}

This lemma shows that if   passes the dS-G criterion,
then  passes the dS-G criterion for all .
But it also shows that if  fails the dS-G criterion, then  fails the dS-G  criterion for all
.
Thus if   is a death set, and ,
Lemma \ref{L:map} implies that  is also a death set.
This leads us to make the following definitions:

\begin{defn}
A set  whose removal causes failure of the dS-G criterion is called a
\textbf{death set}.
A death set  is a \textbf{minimal death set} if no subset of  is itself a death set.
\end{defn}

\begin{defn}
If the removal of  does not cause failure, we call  a \textbf{cake set}.
A cake set  is a \textbf{maximal cake set} if no superset of  is also a cake set.
\end{defn}
As we will show in section \ref{S: Probability of Failure}, minimal death sets are directly related to the failure of the dS-G criterion.
However, we first look at the issues arising from the complexity of the problem.




\section{Complexity Issues}\label{S: Complexity}

The first issue to address is whether this problem is computationally complex.
In this section, we will show that  in fact it is part of a slightly larger group of problems which are  NP-hard, more specifically \#P-complete.

\subsection{Use of the 2-skeleton} \label{S: 2-skeleton}
A simplifying step is to work with the 2-skeleton  of  rather than the full complex.
To justify this, notice that passing to the 2-skeleton does not affect our observance of the dS-G criterion:

\begin{lemma}
The dS-G criterion is satisfied for  if and only if it is satisfied for ,
where  is the 2-skeleton.
\label{L: 2-skeleton}
\end{lemma}
\begin{proof}
Consider the following diagram built from the long exact sequences for the pairs 
and  and the maps induced by the inclusion :

If the dS-G criterion is satisfied for , then there is an 
such that  is nonzero.
Clearly  also satisfies the dS-G criterion since .

Now assume that the dS-G criterion is satisfied for .
Then there is a  such that  is nonzero in .
Here it is important to note that since  is the 2-skeleton of ,
.
As the top and bottom rows are exact with the last two groups equal,
and since  maps to zero in ,
it follows that it also maps to zero in .
Because the top row is exact, there is an  which maps to ,
and hence satisfies the dS-G criterion.
\end{proof}

This lemma implies that the sets of death sets, minimal death sets, cake sets,  and maximal cake sets are equivalent
to their counterparts when computed in the 2-skeleton.
It also implies that the probability of failure of the dS-G criterion in the full Rips complex
and the probability of failure of the dS-G criterion in the 2-skeleton are the same.
And lastly, in  there is exactly one cycle representing each homology class ().
For these reasons we will simplify notation and write 
for the 2-skeleton of the Rips complex for the remainder of the paper.






\subsection{\#P-Complete}\label{S: SharpP-Complete}

The class of problems defined as \#P-complete was introduced by Valiant in \cite{Valiant};
they form a specific class of NP-hard problems.
Typically, \#P-complete problems are concerned with counting \textit{how many} of something
exists  whereas general NP problems just ask \textit{if} something exists.
Problems which are \#P-complete likely do not have polynomial time algorithms.

To show that a problem is \#P-complete, we reduce one difficult problem to another.
Reducing problem  to problem  means that we take any instance of problem ,
use it to create an instance of problem , and conclude that the answer to solving problem 
gives an answer to problem .
To prove NP-completeness or \#P-completeness,
both turning an instance of problem  into an instance of problem 
and returning the answer to problem  given the solution to problem  must
be done in polynomial time.
If we can reduce  to  in polynomial time,  we write .

A reduction from  to  is called \textit{parsimonious} if the number of solutions for 
is in one-to-one correspondence to solutions for .
This is an important property for proving that problems are \#P-complete since we need to be
able to count the number of solutions of  based on the number of solutions of .

In order to show that our sensor network problem is \#P-complete,
we need to find a polynomial time, parsimonious reduction from a \#P-complete problem to our problem.


It has been known for several decades that the computer science problem of {\bf network reliability}
is \#P-complete \cite{Garey1979,Colbourn1987}.
We will specifically work with the two terminal network reliability problem as defined in \cite{Garey1979}.
An instance of the problem is a graph  with marked vertices ,
a rational failure probability ,  for each edge ,
and a positive rational number .
Then, assuming edge failures are independent of one another,
we ask whether the probability that  and  have a path with no failed edge
is greater than or equal to .
The fact that this problem is hard in the class of counting problems comes from needing to count the possible paths from  to  when determining the probability of failure.

This problem has striking similarities to ours, and the closeness is even more pronounced
when we look at it in the following way.
Considering the graph  as a one-dimensional simplicial complex, a path in  with
endpoints at  and  is a fundamental class in ,
where fundamental means that the boundary of the class is homologous to  in
.


Our goal is to reduce network reliability to our problem, which we will therefore call
{\em 2-dimensional network reliability}.
An instance of the problem is a simplicial complex  with a subcomplex
 that is homeomorphic to .
We also have rational probabilities of failure , , on the  vertices not in ,
and a value .
We ask the following question:
{\em Given the fact that failures of vertices are independent of each other, is the probability
that we have a fundamental class  at least ?}

Notice that our definition of the problem takes no account of the geometry inherent in the
originally defined problem
as we are ignoring the fact that we obtained this simplicial complex from a set of points in , and thus we are proving a larger class of problems to be \#P-complete.
To prove that 2-dimensional network reliability is \#P-complete, we must take an instance of the
1-dimensional network reliability problem, turn it into an instance of the 2-dimensional case in
polynomial time, take the solution given there and turn it into an answer to the 1-dimensional
case in polynomial time.


\begin{thm}
2-dimensional network reliability is \#P-complete.
\end{thm}

\begin{proof}
Consider a finite graph  with vertex set  and  probability of failure
 given on each edge .
We will construct a 2-dimensional simplicial complex  with a subcomplex 
 so that there is a one-to-one correspondence between paths from  to  in 
  and fundamental classes of .
This correspondence will also preserve the probability of failure of the class,
so this will imply that the probability of failure in the 1-dimensional case can be computed by
determining the probability of failure in the 2-dimensional case.

Suppose that  has  vertices.
Order these vertices by choosing a map
 that sends each vertex to a distinct integer in
, with  and .
Extend  to all of  by linear interpolation over each edge,
and subdivide  by adding vertices at all points of .
Call the result , the map  is now piecewise linear.
If an edge  of  is subdivided into  subedges in ,
we set the probability of failure for one of the subedges equal to  and the rest equal to 0.  
See Figure [\ref{fig:graphexample}] for an example of building this graph.

Form the complex

where  iff  and either  or .
Note that this collapses the top and  bottom graphs each onto a separate copy of the interval
.
To make this a true simplicial complex, divide each rectangle of the form 
into triangles by placing a vertex at the barycenter and adding the obvious four new edges
and four new triangles.
The resulting complex will be called .
Then define  to be the subcomplex
 together with the two edges  and ;
 is homeomorphic to  by construction.
(In Figure [\ref{fig:graphexample}] we
have not subdivided the rectangles to keep the picture uncluttered.)


\begin{figure}[h]
\begin{center}
\includegraphics[scale=.5]{graphexample2}
\end{center}
\caption{ is the instance of the 1-dimensional Network Reliability problem with an ordering  placed on the vertices.
This map can be extended by linear interpolation over each edge.
We then subdivide the edges at all points of .
Finally, we define the complex  where  iff 
and either  or .  }
\label{fig:graphexample}
\end{figure}

Set the probability of
failure of vertices that were added to the centers of the rectangles
equal to the probability of failure of the edge of  from which they arose.
Notice that failure of one of these vertices
leads to removal of the interior of the corresponding rectangle.

It is obvious that each path in  gives rise to a fundamental class in .
To prove the opposite, first recall that since  has no -simplices, each fundamental
class in  has a unique representative cycle ().
Furthermore, since we are using homology with  coefficients,
each class is simply a subset of the set of -simplices in .
A relative cycle  has the added property that an even number of
 simplices in  contain any edge of 
and a fundamental class must have  equal to the sum of all of
the simplices of .
Notice also that if  contains any  2-simplex from a rectangle, it must contain all
four 2-simplices from that rectangle,
so it is equivalent to think of  as a set of rectangles from before the subdivision.

Our conclusion now follows easily.
Every rectangle in  determines a unique edge of .
Since  is covered exactly once, no two edges of  that
are equivalent under  can occur as edges of rectangles in .
Since  is a relative cycle, each vertical edge must lie on either
 or  rectangles, so the edges patch together to give a path from
 to .
Hence there is a one to one correspondence between the two sets.

Since we set up each rectangle to have an equal probability to that of its corresponding edge,
the probability that a fundamental class is still in  is equal to the probability that
the corresponding path in  is still functioning.
Thus, if we could compute the probability of failure in  in a reasonable time frame, the solution would
give the probability of failure in  in a reasonable time frame.
Since the latter problem is \#P-complete, our 2-dimensional version is also \#P-complete.

\end{proof}


\section{A Deterministic Algorithm}\label{S: Deterministic Algorithm}

Now that we know that the general problem is \#P-complete,
we strive to find ways to work around the computational complexity issues.
We will first show that, given a set of sensors which is relatively small
or at the very least relatively sparse in the domain, we can write a deterministic
algorithm to compute the probability of failure of the system.
In section \ref{S: Monitored System}, we will  consider the modified
problem of predicting failure as sensors in the system fail.


\subsection{The Hasse Diagram}\label{S: Hasse}

Consider a set of sensors  in .
Recall that edges are added when  sensors are within  of each other,
and 2-simplices are added wherever all three edges have already been included.

Consider all possible subsets  and
as before construct the Rips complex ,
the largest subcomplex of  which does not utilize the nodes in .
The collection of  these Rips complexes forms a poset under inclusion,
where  gives the reverse inclusion .
Arrange all of these Rips complexes into a Hasse diagram,
as shown in Figure \ref{fig: Inclusion Diagram} for the example in
Figure \ref{fig: rips complex example}.
Here we place  in the row indexed by the number of elements in ,
and we have shaded all the complexes  which fail.
A line is drawn between  and  if  is obtained from 
by removing a vertex.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=.9]{deathsetsexamplecolor}
\end{center}
\caption{Hasse Diagram.  A Rips complex  is placed on a row according to the size of  and lines are drawn to show inclusion between complex is neighboring rows.}\label{fig: Inclusion Diagram}
\end{figure}

In Figure \ref{fig: Inclusion Diagram} notice that, if  fails the dS-G criterion, the Rips
complexes for all its supersets of  do as well, so all its successors are also shaded.
This means that when searching for failures we do not have to check every possible subset.
Using breadth first search from , we only need check complexes where all the predecessors are cake sets.
From this search pattern, if it is necessary to check the set in the first place
then it is not just a death set but a minimal death set.
This means that there is no post processing needed to determine the list of minimal death sets.

Given this setup, we now consider the probability of failure of the dS-G criterion.


\subsection{Probability of Failure} \label{S: Probability of Failure}

Let  be a random variable which gives the time of death of node .
In many cases,  will be an exponential random variable,
but this has no effect on our result so we make no such assumption.
We do however assume that  and  are independent for .

Let  be the random variable which gives the first time at which all nodes in the set have
failed,
clearly .
Because the failures of the nodes are independent events, we have

Next, let  be the collection of all death sets, not necessarily minimal.
Let  be the random variable which gives the time of failure of the dS-G criterion for the system.
The value of  gives the first time that all of the nodes in one of the  have failed,
i.e. .
Hence the probability that the system has failed the dS-G criterion by time  is given by

Unfortunately, these events are not independent since many of the death sets have non-trivial intersections.
On the other hand, we can organize  using the concept of death chains.

A \textbf{death chain} is a sequence
 of death sets .
A \textbf{maximal death chain} is a death chain which cannot be increased in length, either by inserting any
intermediate set between elements of the chain, or adding any sets to either end of the chain.
Note that such a chain starts with a minimal death set,
ends with , and the number of elements increases by exactly one going from
 to .
If we read off the Hasse diagram as in Figure \ref{fig: Inclusion Diagram},
we see that a maximal chain is a path which goes from a minimal death set to the bottom of the diagram.




We will call a maximal death chain with  as the minimal element an \textbf{-chain}.
This allows consideration of all possible maximal chains from the complex ,
grouped by first element: in our case -chains through -chains.
Using this organization of the death sets gives the following theorem:


\begin{thm}
Let  be the set of minimal death sets for the Rips complex .
Then the probability that the complex has failed by time  is equal to

	
	
\end{thm}

\begin{proof}
If  is the set of all death sets, some set  in  must have failed in order to cause failure of the dS-G criterion.
Every set in  contains one of the minimal death sets ,
so every set in  is in at least one -chain.

It is obvious that given any -chain , where the time at which any of the death sets in the chain have failed is given by , satisfies

From this we can also conclude that any set of -chains , where each respective time of failure is given by , satisfies


Therefore, instead of asking for failure of the dS-G criterion, we can then ask for the time when at least one of the
sets in at least one of the chains has failed.
Hence
	

\end{proof}








This means that the probability of failure of the system can be computed given the minimal death sets.
Notice that there is still work to be done since the minimal death sets may have intersections.
However, since we assume that we have a small number of sensors that are well distributed
and are not extremely dense in the domain,
the number of intersections will be small and therefore this later computation is feasible.
Thus, we seek an algorithm to compute the minimal death sets
although from our knowledge that the problem is \#P-complete,
we expect that this algorithm will be exponential in the worst case.


\subsection{Death Sets Algorithm}\label{S: Death Sets Algorithm}

When constructing an algorithm to determine the set of minimal death sets, the search space is the Hasse diagram described in Section \ref{S: Hasse}.
Since this has size , we expect this is the source of our complexity issues.

Search the Hasse diagram using breadth first search.
This will exploit the property that if  is a death set and  then  is also a death set.
Hence, to find the minimal death sets, we must only check complexes where every predecessor is still a cake set.
If we are forced to check all of the nodes of the Hasse diagram, then we will need to check  complexes.
However, with our assumption that we do not have a very dense set of sensors, it should take removal of a small set of sensors in order to break the dS-G criterion.
This means the size of minimal death sets will be relatively small, and thus they will be close to the top of the Hasse diagram.
More importantly, it means there will be relatively few of them so out output size will not be too large.

Given this method to work through the sets, we need an efficient way
to check the dS-G criterion.
Consider the subcomplex , thought of as the point in a filtration of
 where all simplices have been added except those which have vertices in .
Order the simplices so that those in the fence come first in the ordering.
Initial intuition says that in order to talk about failure of the dS-G criterion when the nodes in  are removed,
we should filter  so that all nodes, edges,
and triangles which have any vertex in  are last in the ordering.
We could then construct the boundary matrix for this ordering,
cut off the final columns corresponding to simplices which would be gone if the vertices in  failed,
and reduce the resulting matrix in order to read off the homology of .

However, this turns out to be much more work than is needed.
If we add all degree  and degree 0 simplices from the start,
even if they have interior nodes which we assume
to have failed,  the failure of the dS-G criterion is not affected.
The following expresses this and is elementary to prove.
\begin{lemma}
Let  be a 1-dimensional simplicial complex,
 a 2-dimensional simplicial complex whose vertex set may intersect nontrivially with , and , then
.
	\label{L: Extra Frame Stuff}
\end{lemma}


This implies that the time in the filtration when we add the 1-simplices is irrelevant to the homology group we are interested in,
namely .
Thus, we can order our filtration so that all the 2-simplices are at the end
and consider the failure of a node as the failure only of the 2-simplices which contain it as a face.


Given this filtration, we reduce the matrix  via the persistence algorithm (see, e.g., \cite{Edelsbrunner2010}),
and consider the rightmost columns, which correspond to 2-simplices.
If the row for a simplex has no lowest one in a row below those corresponding
to the fence simplices, the addition of that 2-simplex creates a new class in .
If this column is not entirely 0 and has a lowest 1 in a row corresponding to a fence simplex,
that class has a boundary which is nonzero in .
Thus, our dS-G criterion reduces to looking for a column corresponding to a 2-simplex which has a
lowest one in a row corresponding to a fence simplex.


This shows that we can quickly determine  whether a complex satisfies the dS-G criterion  once we have
determined the correct filtration for  and have reduced the matrix .
We would like to not have to rewrite and re-reduce the matrix  for each complex  to be checked.
So, let us determine an efficient way to swap all the 2-simplices that
have a vertex in our failure set  to the end.
For this, we turn to \cite{Cohen-Steiner2006}, which gives an algorithm to quickly update and
maintain the properties of  and , where , as we swap columns (Notice that  from the earlier discussion in Section \ref{S: Persistent Homology}).
This means that we do not have to  rerun the persistence algorithm each time to reduce the matrix .

Let  be an -decomposition.
That is,  is the reduced matrix, and  is upper triangular.
Let  be the matrix which swaps rows  and , so that  is the boundary matrix with simplices
 and  switched.
This can be written as , so we need to determine when  is not reduced and
 is not upper triangular.
Notice that  is not reduced if and only if there are columns  and  with
, , and .  is not upper triangular if and only if .



In \cite{Cohen-Steiner2006}, cases are split into whether  and  are positive or negative.
The one case that will not occur for us is the possibility that  and  are positive,
and there are rows  and  with  and .
This would imply that  and  are 3-simplices whose additions kill the classes
born by the addition of  and .
As we are assuming  is a 2-dimensional simplicial complex, this case is impossible, hence we can disregard it.
With respect to the other cases, we can either swap rows and columns in  and ,
hence replace them with  and  with no issues, or we must replace them with 
and , where  is the matrix which adds column  to column  in .
We replace  and  with  and  if
\begin{list}{}{}
\item  and  are both positive simplices,
\item  and  are both negative simplices and 
\item  is negative,  is positive, and \item  is positive and  is negative.
\end{list}
We instead replace  with  and  with  if
\begin{list}{}{}
\item  and  are both negative and  \item  is negative,  is positive, and 
\end{list}
Hence,  the columns of  2-simplices which correspond to failure of specific vertices can be quickly swapped to the end of our matrix.
Now that matrix  corresponding to the filtration placing the simplices in  at the end has been reduced, we can check whether our complex  passes the dS-G criterion.
In the language of persistence homology, the dS-G criterion is looking for a column which represents a positive simplex, and therefore a simplex which adds a new class to  .
Additionally, the boundary of this new class is nonzero in .
In terms of the matrices, we need to find a column  in the reduced matrix  which is   has a lowest 1 corresponding to a simplex in .
A representation of this column is in Figure \ref{F: matrixRfundamental}.
\begin{figure}
\begin{center}
\includegraphics{matrixRfundamental}
\end{center}
\caption{The column representing the birth of a fundamental class. It corresponds to a 2-simplex and its lowest
 corresponds to a simplex in .} \label{F: matrixRfundamental}
\end{figure}
Our algorithm is as follows.


\begin{tabbing}
\\
\textbf{CakeOrDeath}()\\
\\
Given: \= The boundary matrix  with filtration \\
\> Fence, Remaining 1-simplices, 2-simplices  \\
Reduce \\
\textbf{for} \=\=,  in the order of BFS in the Hasse diagram:\\
\>	Swap all columns corresponding to 2-simplices with\\
\>\>		a vertex in  to end of matrix , and maintain\\
\>\>		 and .\\
\>	\textbf{if} there is not a column as in Figure \ref{F: matrixRfundamental}:\\
\>\>		mark  as `Minimal Death.'\\
\>	\textbf{else}:\\
\>\>		mark  as `Cake.'\\
\>	\textbf{endif}\\
\textbf{endfor}
\end{tabbing}


\subsection{Complexity of Algorithm} \label{S: Complexity of Algorithm}
As a beginning aside, we point out why we chose breadth instead of depth first search.
BFS has the property that we will only ever check sets which are cake or minimal death.
On the other hand, DFS would require post-processing to determine which of the death sets  found were minimal death sets.
The perk of DFS, however, is that it requires less matrix swaps since multiple sets  can be labeled as cake or death by reading off of one matrix.
As we wish to have less post-processing, we choose to use BFS for our algorithm.

Assume that the matrix  is stored as a sparse matrix.
This is done with a linear array of lists  where  is the total number of simplices
in the 2-dimensional complex .
Also, assume the ordering of the simplices is
.
Each entry  in this array stores a linked list denoting the locations of the codimension-1 faces of , or equivalently, the 1s in column  of the full matrix .  This not only speeds up the operations required on the matrix, but reduces the storage size of  to .

There are four major parts to the algorithm.
The first is to reduce .
Using the persistence algorithm, this takes time at most .
See \cite{Edelsbrunner2010} for details.


For each complex to be checked,  all necessary columns must be swapped to the right side of matrix .
If there are  2-simplices, at worst there are  swaps to be performed.
While each swap has at worst an  running time, from \cite{Cohen-Steiner2006}
there is an amortized time proportional to the number of 1s in the affected rows and columns, so this is .
This step therefore has an amortized cost of .

Next, we check for a fundamental class.
If this is done in  complexes,  then .
(Recall that because we assume that the dense set of sensors is not dense,  will likely be much smaller than .)

If  a vector \texttt{Low} giving the location of the lowest 1 in each column is maintained throughout the process of swapping,  easily done via the cases in \cite{Cohen-Steiner2006}, it only takes  time to check for a column which fits our requirements.
Hence for each death set, the amortized running time is , and so the overall running time is .

To make this running time feasible, one needs to keep  under control.
The easiest way to do this is to have a sparse set of sensors.
For example, suppose the area of the domain  is  and we have  sensors.
Each sensor covers an area of , so the sensors cover a total area (double counting overlap) of .
This means the expected number of sensors covering any point is .
Therefore the death sets should be of size approximately , so it is only necessary
to check complexes through about the  row.
Thus .

In conclusion, computing the probability exactly will be easier if the set of sensors is sparse,
but what is gained in exactness of the computation is lost  in the robustness of the network.





\section{A Dynamic Algorithm for a Monitored System}\label{S: Monitored System}


Suppose we are in a situation where the deterministic algorithm is not feasible.
Computing the probability of failure exactly is an NP-hard problem,
and thus an exact computation is essentially impossible when the set of sensors is large.
Instead, assume that a central monitoring station receives information as to whether or not each sensor has failed.
In this case, a more practical question is to ask when  the system is getting close to failure and so we seek a dynamic algorithm to predict which nodes would cause failure of the criterion should they fail soon.
To do this, we will create a new criterion built from the old which will give an early warning for failure.
It essentially gives a flag on each interior vertex warning that its failure would probably cause failure of the dS-G criterion.

For technical reasons, we will assume  in this section  that the domain is convex.
A domain that is not convex can have a radius where the Rips complex has a nontrivial class in  even though  passes the dS-G criterion.
This assumption is much stronger than is necessary since all we really need
is  that  whenever  passes the dS-G criterion.



The main idea for the new criterion comes from the following theorem.
\begin{thm}\label{Thm: New Criterion}
 Assume that the pair of simplicial complexes  passes the dS-G criterion, , and  is a vertex in .
Then  passes the dS-G criterion if and only if .
\end{thm}

\begin{proof}
Assume that  passes the dS-G criterion and that the domain is convex, which
implies that  is trivial.
Mayer-Vietoris  for  gives the exact sequence

Thus, since ,  and  are all  trivial,  .

Assume on the other hand that .
Note that if  passes the dS-G criterion, .
Using the long exact sequence of the pair , we have

hence the middle map is an isomorphism.



By excision, .
Using the knowledge that  has the homotopy type of a point for the first isomorphism and the long exact sequence of the pair  for the second, we have

Thus, .

Consider the diagram

Since  passes the dS-G criterion, there is a fundamental class  in .
By definition, it maps to  which is nonzero in , and since the two rows are exact,  maps to 0 in .
The last two vertical maps are isomorphisms, so there must be a nonzero  in  which maps to  under the top left horizontal map, and therefore  passes the dS-G criterion.


\end{proof}

Notice that the assumption that  is only used  for one direction of the theorem: if  passes, then we have a link with trivial first homology.
Despite being counterintuitive, it is possible for a set of points in the plane to have a non-trivial second homology group \cite{Chambers2009}.
The expectation is that this event will not be frequent, but it must be kept in mind as we create a new criterion in this monitored set up.


\subsection{The new criterion and complexity}

Given Theorem \ref{Thm: New Criterion}, we propose a new criterion to complement the de Silva - Ghrist criterion:
\begin{defn}[Link Condition]
 If an interior vertex  has , we say it is flagged.  Otherwise, we say it is not flagged.
\end{defn}
The idea is that if a vertex is flagged, there is a chance its removal will cause failure of the dS-G criterion.  If it is not flagged, then its removal can do no harm.

With this definition in mind, we give a dynamic algorithm to follow as nodes fail:

\begin{tabbing}
\\
\textbf{MonitoredSystemFailure}()\\
\\
Given: Simplicial complex pair \\
\\
Check \= dS-G criterion 
 (We assume that this initial \\ \>check will always pass)\\
Compute link of each vertex  and \\
\textbf{if} \={}:\\
\>	Mark  as flagged.\\
\textbf{endif}\\
\\
\textbf{if} \=ver\=tex \= fails:\\
\>		\textbf{if} { is flagged}:\\
\>\>			Update matrix  to remove dead simplices\\
\>\>			Check dS-G criterion\\
\>\>			 \textbf{if}  fails the dS-G criterion:\\
\>\>\>				 Break\\
\>\>			\textbf{endif} \\
\>		\textbf{endif} \\
\>		Update links of vertices\\
\>		Compute  for  whose link has \\ \>\>changed\\
\>		Mark or unmark  as flagged according to  \\ \>\>computation.\\
\textbf{endif} \\		
\\
\end{tabbing}


This algorithm turns out to be polynomial in the number of simplices in the worst case.
We will split the complexity computation into two parts: the initialization step, done before any vertex has failed, and the time taken for the algorithm for each failed vertex.

\subsubsection*{Initialization}

Let  be the number of two simplices of dimension .  In section \ref{S: 2-skeleton}, we showed that this is the highest dimension simplex needed for the dS-G criterion, and since the link condition only looks at the first dimension of the complex, nothing above the second dimension is required.

The complexity of computing the link of  is directly related to the number of simplices containing  as a vertex.
In fact, given a list of all simplices in  which include vertex , print the simplex obtained by removing vertex  from the simplex.
This is the link, so given a listing of the simplices with vertex , the link of  can be computed in time  where  is the number of adjacent simplices.
Since  is obviously less than , the time to initially compute all the links is .

The time to compute  is , again with  equal to the number of simplices in the link of .  Since, , the time taken for this initial step is .
Thus, the entirety of the initialization step takes time .

\subsubsection*{Failed Vertex}

In the worst case, every failed vertex is flagged and so the dS-G criterion must be recomputed each time.
As seen in section \ref{S: Complexity of Algorithm}, updating the matrix  and checking the dS-G criterion takes time  where  is the number of two simplices.


What is interesting about the link condition is that it is easy to maintain the links of all the interior vertices.
  Let  be the link of  in the simplicial complex  and let  be the largest subcomplex of  without the vertex .

\begin{lemma}
For any vertices  in a simplicial complex ,

\end{lemma}
\begin{proof}
 If  then obviously .
 Also, this implies that the simplex .
 Since  and , we must have that , so .

 Let . Since it is in , the simplex, .
 As  is also in , , so .  Therefore, , and equation \ref{E: Link Condition} follows.
\end{proof}

This lemma implies that the only  update needed after the failure of a vertex is to delete any simplices in the link which were also deleted in the simplicial complex.
In the worst case, the link of every vertex must be updated, the first homology recomputed, and the flag remarked as needed.
Since the size of each link is at most , this step takes .

If this sequence of events happens for every , this second part of the algorithm takes time 
Combining this with the initializing step, the whole algorithm takes at worst time , so is polynomial in the number of simplices.





\section{Conclusions and Possible Extensions}\label{S: Conclusion}

In this paper, we have extended the problem posed by de Silva and Ghrist in \cite{DeSilva2006}
by assuming that  sensors have a probability of failure, and asking for the probability of failure of the dS-G criterion for
coverage.
We determined that the generalized version of the problem is \#P-complete, and thus it is unlikely that there is an
 algorithm to answer this question in general which runs in a reasonable amount of time.
Finally, we provided a deterministic algorithm which does work in the case of a small set of sensors,
and a method to predict failure when the system is larger but is being monitored.

The obvious immediate extension of our work is to determine whether the the version of the problem
posed by de Silva and Ghrist in \cite{DeSilva2007}, which allows for higher dimensions and looser
boundary conditions, is also amenable to an application of probability of failure.
We conjecture that this extended problem will also be NP-hard.

In the long term, we would like to see more applications of computational topology to the
design and analysis of sensor networks.
Since we can make such strong conclusions with such weak assumptions on the capabilities of the sensors,
we expect that such applications are abundant.



\bibliographystyle{authordate1}	\bibliography{FailureFiltrationLibrary}





\end{document}
