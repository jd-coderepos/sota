\documentclass{journal}
\pdfoutput=1

\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,epstopdf}
\usepackage{url,hyperref}\urlstyle{same}
\usepackage{color}
\usepackage{microtype}
\usepackage{plaatjes}
\usepackage{fullpage}
\usepackage[left,pagewise]{lineno}
\usepackage{xspace}
\usepackage{euscript}
\usepackage[section]{algorithm}
\usepackage[noend]{algorithmic}

\definecolor {infocolor} {rgb} {0.6,0.6,0.6}
\renewcommand\linenumberfont{\normalfont\normalsize\textcolor{infocolor}}


\definecolor {remarkcolor} {rgb} {0.1,0.7,0.2}
\definecolor {namecolor} {rgb} {0.2,0.1,0.7}
\newcommand{\marrow}{\marginpar[\hfill]{}}
\newcommand{\personalremark}[3]{\textcolor{namecolor}{\textsc{#1 #2:} \marrow\textcolor{remarkcolor}{\textsf{#3}}}}
\newcommand{\allan}[2][says]{\personalremark{Allan}{#1}{#2}}
\newcommand{\jeff}[2][says]{\personalremark{Jeff}{#1}{#2}}
\newcommand{\maarten}[2][says]{\personalremark{Maarten}{#1}{#2}}

\setlength{\fboxsep}{.5pt}
\renewcommand\thefootnote{\tiny\protect\framebox{\arabic{footnote}}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\i}[1]{\ensuremath{\mathit{#1}}}
\newcommand{\Eu}[1]{\ensuremath{\EuScript{#1}}}
\renewcommand{\c}[1]{\ensuremath{\EuScript{#1}}}
\renewcommand{\b}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\mbb}[1]{\ensuremath{\mathbb{#1}}}
\renewcommand{\sf}[1]{\textsf{#1}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\B}[1]{\ensuremath{\textbf{#1}}}
\renewcommand{\P}{\ensuremath{\B{Pr}}}
\newcommand{\Pv}{\ensuremath{\textsf{P}_v}}
\newcommand{\Pout}{\ensuremath{\tau_{\textrm{out}}}}
\newcommand{\diameter}{\ensuremath{\textsf{diam}}}
\newcommand{\seb}{\ensuremath{\textsf{seb}_2}}
\newcommand{\dwid}{\ensuremath{\textsf{dwid}}}
\newcommand{\MEB}[1]{\ensuremath{{#1}\textsf{-MEBr}}}
\newcommand{\F}{\ensuremath{\textsf{F}}}
\newcommand{\mY}{\ensuremath{\textsf{Min}}}
\newcommand{\erf}{\ensuremath{\textsf{erf}}}
\newcommand{\sip}{\textsf{sip\xspace}}
\newcommand{\IP}[2]{\ensuremath{ \langle #1 , #2 \rangle}}
\newcommand{\fsg} [1] {\ensuremath {G \langle #1 \rangle}}
\newcommand{\wid}{\omega}
\newcommand{\etal}{et. al.}
\newcommand{\pset}{support\xspace}
\newcommand{\psets}{supports\xspace}
\newcommand{\DIAM}{\xspace}
\newcommand{\PDIAM}{\xspace}
\newcommand{\DIAMd}{\xspace}
\newcommand{\PDIAMd}{\xspace}
\newcommand{\threeSAT}{\xspace}
\newcommand{\tCtS}{\xspace}
\newcommand{\twoSAT}{\xspace}

\newtheorem {theorem}{Theorem}[section]
\newtheorem {problem}[theorem]{Problem}
\newtheorem {lemma}[theorem]{Lemma}
\newtheorem {observation}[theorem]{Observation}
\newtheorem {corollary}[theorem]{Corollary}


\title{Geometric Computations\\ on Indecisive and Uncertain Points 
}

\author{
  Allan J{\o}rgensen\thanks
  { formerly: MADALGO, Deptartment of Computer Science, University of Aarhus, Denmark.
    \texttt{jallan(at)madalgo.au.dk}
  }
  \and
  Maarten L\"offler\thanks
  {
    formerly: Computer Science Deptartment, University of California, Irvine, USA.
    \texttt{mloffler(at)uci.edu}
  }
  \and
  Jeff M. Phillips\thanks
  {
    School of Computing, University of Utah, USA.
    \texttt{jeffp(at)cs.utah.edu}
  }
}
\date{ }


















\begin{document}

\maketitle

\begin{abstract}
We study computing geometric problems on \emph {uncertain} points. An uncertain point is a point that does not have a fixed location, but rather is described by a probability distribution.  When these probability distributions are restricted to a finite number of locations, the points are called \emph{indecisive} points.  In particular, we focus on geometric shape-fitting problems and on building compact distributions to describe how the solutions to these problems vary with respect to the uncertainty in the points.  Our main results are:
(1) a simple and efficient randomized approximation algorithm for calculating the distribution of any statistic on uncertain data sets;
(2) a polynomial, deterministic and exact algorithm for computing the distribution of answers for any LP-type problem on an indecisive point set; and
(3) the development of \emph{shape inclusion probability} (SIP) functions which captures the ambient distribution of shapes fit to uncertain or indecisive point sets and are admissible to the two algorithmic constructions.  
\end{abstract}








\section {Introduction}

In gathering data there is a trade-off between quantity and accuracy.  The drop in the price of hard drives and other storage costs has shifted this balance towards gathering enormous quantities of data, yet with noticeable and sometimes intentionally tolerated increased error rates.  However, often as a benefit from the large data sets, models are developed to describe the pattern of the data error.



Let us take as an example Light Detection and Ranging (LIDAR) data gathered for Geographic Information Systems (GIS)~\cite{LKC04}, specifically height values at millions of locations on a terrain.
Each data point  has an -value (longitude), a -value (latitude), and a -value (height).
This data set is gathered by a small plane flying over a terrain with a laser aimed at the ground measuring the distance from the plane to the ground.  Error can occur due to inaccurate estimation of the plane's altitude and position or artifacts on the ground distorting the laser's distance reading.
But these errors are well-studied and can be modeled by replacing each data point with a probability distribution of its actual position.  Greatly simplifying, we could represent each data point as a -variate normal distribution centered at its recorded value; in practice, more detailed uncertainty models are built.

Similarly, large data sets are gathered and maintained for many other applications.
In robotic mapping~\cite{Thr02,EP04} error models are provided for data points gathered by laser range finders and other sources.
In data mining~\cite{AY08,AS00} original data (such as published medical data) are often perturbed by a known model to preserve anonymity.
In spatial databases~\cite{GS05,SC01,CKP03} large data sets may be summarized as probability distributions to store them more compactly.
Data sets gathered by crawling the web have many false positives, and allow for models of these rates.  
Sensor networks~\cite{DGMHH04} stream in large data sets collected by cheap and thus inaccurate sensors.
In protein structure determination~\cite{PYCDB06} every atom's position is imprecise due to inaccuracies in reconstruction techniques and the inherent flexibility in the protein.
In summary, there are many large data sets with modeled errors and this uncertainty should be dealt with explicitly.  








\subsection {The Input: Geometric Error Models}

The input for a typical computational geometry problem is a set  of  points in , or more generally .  In this paper we consider extensions of this model where each point is also given a model of its uncertainty.  This model describes for each point a distribution or bounds on the point's location, if it exists at all.  

\begin{itemize}
\item 
Most generally, we describe these data as \emph{uncertain points} .  Here each point's location is described by a probability distribution  (for instance by a Gaussian distribution).  This general model can be seen to encompass the forthcoming models, but is often not worked with directly because of the computational difficulties arisen from its generality.  For instance in tracking uncertain objects a particle filter uses a discrete set of locations to model uncertainty~\cite{MDFW00} while a Kalman filter restricts the uncertainty model to a Gaussian distribution~\cite{Kal60}.  

\item 
A more restrictive model we also study in this paper are \emph{indecisive points} where each point can take one of a finite number of locations.  To simplify the model (purely for making results easier to state) we let each point have exactly  possible locations, forming the domain of a probability distribution.  That is each uncertain point  is at one of .  Unless further specified, each location is equally likely with probability , but we can also assign each location a weight  as the probability that  is at  where  for all .  

\tweeplaatjes {ex-input} {ex-sample} {(a) An example input consisting of  sets of  points each. (b) One of the  possible samples of  points.}

Indecisive points appear naturally in many applications. They play an important role in databases~\cite{DS04,ABSHNSW06,CM08,CG09,TCXNKP05,ACTY09,CLY09}, machine learning~\cite{BZ04}, and sensor networks~\cite{ZC04} where a limited number of probes from a certain data set are gathered, each potentially representing the true location of a data point.  Alternatively, data points may be obtained using imprecise measurements or are the result of inexact earlier computations.
However, the results with detailed algorithmic analysis generally focus on one-dimensional data; furthermore, they often only return the expected value or the most likely answer instead of calculating a full distribution.  




\item 
An \emph{imprecise point} is one where its location is not known precisely, but it is restricted to a range.  In one-dimension these ranges are modeled as uncertainty intervals, but in 2 or higher dimensions they become geometric regions. 
An early model to quantify imprecision in geometric data, motivated by finite precision of coordinates, is {\it -geometry}, introduced by Guibas \etal~\cite {gss-egbra-89}, where each point was only known to be somewhere within an -radius ball of its guessed location. 
The simplicity of this model has provided many uses in geometry.
Guibas \etal~\cite {gss-cscah-93} define \emph {strongly convex} polygons: polygons that are guaranteed to stay convex, even when the vertices are perturbed by . 
Bandyopadhyay and Snoeyink~\cite{bs-ads-04} compute the set of all potential simplices in  and  that could belong to the Delaunay triangulation.
Held and Mitchell~\cite {hm-ticpps-08} and L\"offler and Snoeyink~\cite {ls-dtip-08} study the problem of preprocessing a set of imprecise points under this model, so that when the true points are specified later some computation can be done faster.

A more involved model for imprecision can be obtained by not specifying a single  for all the points, but allowing a different radius for each point, or even other shapes of imprecision regions. This allows for modeling imprecision that comes from different sources, independent imprecision in different dimensions of the input, etc. This extra freedom in modeling comes at the price of more involved algorithmic solutions, but still many results are available.
Nagai and Tokura~\cite {nt-teb-00} compute the union and intersection of all possible convex hulls to obtain bounds on any possible solution, as does Ostrovsky-Berman and Joskowicz~\cite {obj-ue-05} in a setting allowing some dependence between points.
Van Kreveld and L\"offler~\cite {kl-bgmips-06} study the problem of computing the smallest and largest possible values of several geometric extent measures, such as the diameter or the radius of the smallest enclosing ball, where the points are restricted to lie in given regions in the plane.
Kruger~\cite {k-bmips-08} extends some of these results to higher dimensions.

Although imprecise points do not traditionally have an associated probability distribution associated to them, we argue that they can still be considered a special case of our uncertain points, since we can impose e.g. a uniform distribution on the regions, and then ask question about the smallest or largest non-zero probability values of some function, which would correspond to bounds in the classical model.



\item 
A \emph{stochastic point}  has a fixed location, but which only exists with a probability .  These points arise naturally in many database scenarios~\cite{ABSHNSW06,CLY09} where gathered data has many false positives. Recently in geometry Kamousi, Chan, and Suri~\cite{KCS11a,KCS11b} considered geometric problems on stochastic points and geometric graphs with stochastic edges.  
These stochastic data sets can be interpreted as uncertain point sets as well by allowing the probability distribution governing uncertain points to have a certain probability of not existing, or rather the integral of the distribution is  instead of always .  

\end{itemize}


\subsection {The Output: Distributional Representations}

This paper studies how to compute distributions of statistics over uncertain data.  
These distributions can take several forms.  In the simplest case, a distribution of a single value has a one-dimensional domain.  
The technical definition yields a simpler exposition when the distribution is represented as a cumulative density function, which we refer to as a \emph{quantization}.  
This notion can be extended to a multi-dimensional cumulative density function (a \emph{-variate quantization}) as we measure multiple variables simultaneously.  Finally, we also describe distributions over shapes defined on uncertain points (e.g. minimum enclosing ball).  As the domains of these shape distributions are a bit abstract and difficult to work with, we convey this information as a \emph{shape inclusion probability} or \emph{SIP}; for any point in the domain of the input point sets we describe the probability the point is contained in the shape.  

This model of uncertain data has been studied in the database community but for different types of problems on usually one-dimensional data, such as indexing~\cite{ACTY09,TCXNKP05,KMMH06}, ranking~\cite{CLY09}, nearest neighbors~\cite{CCMC08} and creating histograms~\cite{CG09}.


\subsection{Contributions}

For each type of distributional representation of output we study, the goal is a function from some domain to a range of .  
For the general case of uncertain points, we provide simple and efficient, randomized approximation algorithms that results in a function that \emph{everywhere} has error at most .  Each variation of the algorithm runs in  time and produces an output of size  where  describes the complexity of the output shape (i.e. VC-dimension),  is the probability of failure, and  is the time it takes to compute the geometric question on certain points.  These results are quite practical as experimental results demonstrate that the constant for the big-Oh notation is approximately .  Furthermore, for one-dimensional output distributions (quantizations) the size can be reduced to , and for -dimensional distributions to .  
We also extend these approaches to allow for geometric approximations based on -kernels~\cite{AHV04,AHV07}.  

For the case of indecisive points, we provide deterministic and exact, polynomial-time algorithms for all LP-type problems with constant combinatorial dimension (e.g. minimum enclosing ball).  We also provide evidence that for problems outside this domain, deterministic exact algorithms are not available, in particular showing that diameter is \#P-hard despite having an output distribution of polynomial size.  
Finally, we consider deterministic algorithms for uncertain point sets with continuous distributions describing the location of each point.  We describe a non-trivial range space on these input distributions from which an -sample creates a set of indecisive points, from which this algorithm can be performed to deterministically create an approximation to the output distribution.  
































\section {Preliminaries}\label{sec:prelim}

This section provides formal definitions for existing approximation schemes related to our work as well as the for our output distributions.  

\subsection{Approximation Schemes: -Samples and -Kernels}

This work allows for three types of approximations.  The most natural in this setting is controlled by a parameter  which denotes the error tolerance for probability.  That is an -approximation for any function with range in  measuring probability can return a value off by at most an additive .  
The second type of error is a parameter  which denotes the chance of failure of a randomized algorithm.  That is a -approximate randomized algorithm will be correct with probability at least .  
Finally, in specific contexts we allow a geometric error parameter .  In our context, an -approximate geometric algorithm can allow a relative -error in the width of any object.  This is explained more formally below.  
It should be noted that these three types of error cannot be combined into a single term, and each needs to be considered separately.  However, the  and  parameters have a well-defined trade-off.  

\paragraph{-Samples.}
For a set  let  be a set of subsets of .  In our context usually  will be a point set and the subsets in  could be induced by containment in a shape from some family of geometric shapes.
For example,  describes one-sided intervals of the form .
The pair  is called a \emph{range space}.  We say that  is an \emph{-sample} of  if

where  takes the absolute value and  returns the measure of a point set.  In the discrete case  returns the cardinality of .  We say  \emph{shatters} a set  if every subset of  is equal to  for some .  The cardinality of the largest discrete set  that  can shatter is the \emph{VC-dimension} of .




When  has constant VC-dimension , we can create an -sample  of , with probability , by uniformly sampling  points from ~\cite{VC71,LLS01}.  There exist deterministic techniques to create -samples~\cite{Mat91,CM96} of size  in time .
A recent result of Bansal~\cite{Ban10} (also see this simplification~\cite{LM12}) can slightly improve this bound to , following an older existence proof~\cite{MWW93}, in time polynomial in  and .  
When  is a point set in  and the family of ranges  is determined by inclusion in axis-aligned boxes, then an -sample for  of size  can be constructed in  time~\cite{Phi08}.

For a range space  the \emph{dual range space} is defined  where  is all subsets  defined for an element  such that .  If  has VC-dimension , then  has VC-dimension .  Thus, if the VC-dimension of  is constant, then the VC-dimension of  is also constant \cite{Mat99}.  


When we have a distribution , such that , we can think of this as the set  of all points in , where the weight  of a point  is .  
To simplify notation, we write  as a range space where the ground set is this set  weighted by the distribution .


\paragraph{-Kernels.}
Given a point set  of size  and a direction , let , where  is the inner product operator.
Let  describe the width of  in direction .
We say that  is an \emph{-kernel} of  if for all 

-kernels of size ~\cite{AHV04} can be calculated in time ~\cite{Cha06,YAPV04}.  Computing many extent related problems such as diameter and smallest enclosing ball on  approximates the problem on ~\cite{AHV04,AHV07,Cha06}.


\subsection{Problem Statement}  
Let  describe the probability distribution of an uncertain point  where the integral .
We say that a set  of  points is a \emph {\pset} from  if it contains exactly one point from each set , that is, if  with .
In this case we also write .
Let  describe the distribution of supports  under the joint probability over each .
For brevity we write the space  as .
For this paper we will assume , so the distribution for each point is independent, although  this restriction can be easily removed for all randomized algorithms.



\paragraph{Quantizations and their approximations.}
Let  be a function on a fixed point set.  Examples include the radius of the minimum enclosing ball where  and the width of the minimum enclosing axis-aligned rectangle along the -axis and -axis where .
Define the ``dominates'' binary operator  so that  is true if for every coordinate .
Let .
For a query value  define,

Then  is the cumulative density function of the distribution of possible values that  can take\footnote{For a function  and a distribution of point sets , we will always represent the cumulative density function of  over  by .
}.  We call  a \emph{quantization} of  over .  

\drieplaatjes {uq-true} {uq-points} {uq-approx}
{ \label{fig:1e-quants}
  (a) The true form of a monotonically increasing function from .
  (b) The -quantization  as a point set in .  
  (c) The inferred curve  in .
}

Ideally, we would return the function  so we could quickly answer any query exactly, however, for the most general case we consider, it is not clear how to calculate  exactly for even a single query value .
Rather, we introduce a data structure, which we call an -quantization, to answer any such query approximately and efficiently, illustrated in Figure \ref{fig:1e-quants} for .  An \emph{-quantization} is a point set  which induces a function  where  describes the fraction of points in  that  dominates.  Let .  Then .
For an isotonic (monotonically increasing in each coordinate) function  and any value , an -quantization, , guarantees that

More generally (and, for brevity, usually only when ), we say  is a -variate -quantization.  An example of a -variate -quantization is shown in Figure \ref{fig:ke-quants}.
The space required to store the data structure for  is dependent only on  and , not on  or .

\vierplaatjes [height=100pt] {mq-true} {mq-points} {mq-approx} {mq-both}
{ \label{fig:ke-quants}
  (a) The true form of a -monotone -variate function.
  (b) The -quantization  as a point set in .
  (c) The inferred surface  in .
  (d) Overlay of the two images.
}



\paragraph{-Kernels.}
Rather than compute a new data structure for each measure we are interested in, we can also compute a single data structure (a coreset) that allows us to answer many types of questions.
For an isotonic function , an \emph{-quantization} data structure  describes a function  so for any , there is an  such that
(1)  and
(2) .
An \emph{-kernel} is a data structure that can produce an -quantization, with probability at least , for  where  measures the width in any direction and whose size depends only on , , and .
The notion of -quantizations is generalizes to a -variate version, as do -kernels. 


\paragraph{Shape inclusion probabilities.}
A \emph{summarizing shape} of a point set  is a Lebesgue-measureable subset of  that is determined by .  I.e. given a class of shapes , the summarizing shape  is the shape that optimizes some aspect with respect to .
Examples include the smallest enclosing ball and the minimum-volume axis-aligned bounding box.
For a family  we can study the \emph{shape inclusion probability function}  (or \sip\ function), where  describes the probability that a query point  is included in the summarizing shape\footnote{For technical reasons, if there are (degenerately) multiple optimal summarizing shapes, we say each is equally likely to be the summarizing shape of the point set.}.
For the more general types of uncertain points, there does not seem to be a closed form for many of these functions.
In these cases we can calculate an -\sip function  such that

The space required to store an -\sip function depends only on  and the complexity of the summarizing shape.

































\section{Randomized Algorithm for -Quantizations}
\label{sec:rand-eQ}

We develop several algorithms with the following basic structure (as outlined in Algorithm \ref{alg:rand-draw}):
(1) sample one point from each distribution to get a random point set;
(2) construct the summarizing shape of the random point set;
(3) repeat the first two steps  times and calculate a summary data structure.
This algorithm only assumes that we can draw a random point from  for each  in constant time; if the time depends on some other parameters, the time complexity of the algorithms can be easily adjusted.

\begin{algorithm}[h!!t]
\caption{Approximate  w.r.t. a family of shapes  or function 
\label{alg:rand-draw}}
\begin{algorithmic}[1]
\FOR { \textbf{to} }
  \FOR {\textbf{all} }
    \STATE Sample  from .
  \ENDFOR
  \STATE Set .
\ENDFOR
\STATE Reduce or Simplify the set .
\end{algorithmic}
\end{algorithm}




\paragraph{Algorithm for -quantizations.}
\label{sec:algQ}
For a function  on a point set  of size , it takes  time to evaluate .
We construct an approximation to  
as follows.  First draw a sample point  from each  for , then evaluate .  The fraction of trials of this process that produces a value dominated by  is the estimate of .
In the univariate case we can reduce the size of  by returning  evenly spaced points according to the sorted order.

\begin{theorem}
For a distribution  of  points, with success probability at least ,
there exists an -quantization of size  for , and it can be constructed in  time.
\label{thm:eq-main}
\end{theorem}
\begin{proof}
Because  is an isotonic function, there exists another function  such that  where .  Thus  is a probability distribution of the values of  given inputs drawn from .
This implies that an -sample of  is an -quantization of , since both estimate within  the fraction of points in any range of the form .  This last fact can also be seen through a result by Dvoretzky, Kiefer, and Wolfowitz~\cite{DKW56}.  

By drawing a random sample  from each  for , we are drawing a random point set  from .  Thus  is a random sample from .  Hence, using the standard randomized construction for -samples,  such samples will generate an -sample for , and hence an -quantization for , with probability at least .

Since in an -quantization  every value  is different from  by at most , then we can take an -quantization of the function described by  and still have an -quantization of .
Thus, we can reduce this to an -quantization of size  by taking a subset of  points spaced evenly according to their sorted order.
\end{proof}

\paragraph{Multivariate -quantizations.}
We can construct -variate -quantizations similarly using the same basic procedure as in Algorithm \ref{alg:rand-draw}.
The output  of  is now -variate and thus results in a -dimensional point.
As a result, the reduction of the final size of the point set requires more advanced procedures.

\begin{theorem}
Given a distribution  of  points, with success probability at least , we can construct a -variate -quantization for 
of size  and in time . \label{thm:k-var-q}
\end{theorem}
\begin{proof}
Let  describe the family of ranges where a range .
In the -variate case there exists a function  such that  where .  Thus  describes the probability distribution of the values of , given inputs drawn randomly from .  Hence a random point set  from , evaluated as , is still a random sample from the -variate distribution described by .  Thus, with probability at least , a set of  such samples is an -sample of , which has VC-dimension , and the samples are also a -variate -quantization of .  Again, this specific VC-dimension sampling result can also be achieved through a result of Kiefer and Wolfowitz~\cite{KW58}.  
\end{proof}

We can then reduce the size of the -quantization  to  in  time \cite{Phi08} or to  in  time \cite{CM96}, since the VC-dimension is  and each data point requires  storage.


Also on -variate statistics, we can query the resulting -dimensional distribution using other shapes with bounded VC-dimension , and if the sample size is , then all queries have at most -error with probability at least .  In contrast to the two above results, this statement seems to require the VC-dimension view, as opposed to appealing to the Kiefer-Wolfowitz line of work~\cite{DKW56,KW58}.  





\subsection{-Kernels}
\label{sec:ea-kernel}

The above construction works for a fixed family of summarizing shapes.  In this section, we show how to build a single data structure, an -kernel, for a distribution  in  that can be used to construct -quantizations for several families of summarizing shapes. 
This added generality does come at an increased cost in construction.
In particular, an -kernel of  is a data structure such that in any query direction , with probability at least , we can create an -quantization for the cumulative density function of , the width in direction .

We follow the randomized framework described above as follows.
The desired -kernel  consists of a set of  -kernels,  , where each  is an -kernel of a point set  drawn randomly from .  Given , with probability at least , we can create an -quantization for the cumulative density function of width over  in any direction .
Specifically, let .

\begin{lemma}
With probability at least ,
 is an -quantization for the cumulative density function of the width of  in direction .
\end{lemma}
\begin{proof}
The width  of a random point set  drawn from  is a random sample from the distribution over widths of  in direction .  Thus, with probability at least ,  such random samples would create an -quantization.  Using the width of the -kernels  instead of  induces an error on each random sample of at most .
Then for a query width , say there are  point sets  that have width at most  and  -kernels  with width at most ; see Figure \ref{fig:ea-quant}.
Note that .
Let . For each point set  that has width greater than  but the corresponding -kernel  has width at most ,
it follows that  has width greater than .
Thus the number of -kernels  that have width at most  is at most , and thus there is a width  between  and  such that the number of -kernels at most  is exactly .
\end{proof}

\begin{figure}[htb!]
\begin{center}
\includegraphics{ea-quant.pdf}
\end{center}
\caption[Error bounds in -quantization]{\label{fig:ea-quant} -quantization  (white circles) and -quantization  (black circles) given a query width .}
\end{figure}



Since each  can be computed in  time, we obtain:


\begin{theorem}
We can construct an -kernel for  on  points in  of size  in  time.
\end{theorem}



The notion of -quantizations and -kernels can be extended to -dimensional queries or for a series of up to  queries which all have approximation guarantees with probability .


\paragraph{Other coresets.}
In a similar fashion, coresets of a point set distribution  can be formed using coresets for other problems on discrete point sets.  For instance, sample  points sets  each from  and then store -samples  of each.
When we use random sampling in the second set, then not all distributions  need to be sampled for each  in the first round.  
This results in an -sample of , and can, for example, be used to construct (with probability ) an -quantization for the fraction of points expected to fall in a query disk.
Similar constructions can be done for other coresets, such as -nets~\cite{HW87}, -center~\cite{APV02}, or smallest enclosing ball~\cite{BC03}.




\subsection{Measuring the Error}

We have established asymptotic bounds of  random samples for constructing -quantizations.  Now we empirically demonstrate that the constant hidden by the big-O notation is approximately , indicating that these algorithms are indeed quite practical.

As a data set, we consider a set of  sample points in  chosen randomly from the boundary of a cylinder piece of length  and radius .  We let each point represent the center of 3-variate Gaussian distribution with standard deviation  to represent the probability distribution of an uncertain point.  This set of distributions describes an uncertain point set .

We want to estimate three statistics on :
, the width of the points set in a direction that makes an angle of  with the cylinder axis;
, the diameter of the point set;
and , the radius of the smallest enclosing ball (using code from Bernd G\"{a}rtner~\cite{Gar99}).
We can create -quantizations with  samples from , where the value of  is from the set .

We would like to evaluate the -quantizations versus the ground truth function ; however, it is not clear how to evaluate .  Instead, we create another -quantization  with  samples from , and treat this as if it were the ground truth.
To evaluate each sample -quantization  versus  we find the maximum deviation (i.e. ) with  defined with respect to  or .  This can be done by for each value  evaluating  and  and returning the maximum of both values over all .  

Given a fixed ``ground truth'' quantization  we repeat this process for  trials of , each returning a  value.  The set of these  maximum deviations values results in another quantization  for each of  and , plotted in Figure \ref{fig:maxerr-dwr}.
Intuitively, the maximum deviation quantization  describes the sample probability that  will be less than some query value.

\begin{figure}[tbh]
\begin{center}
\includegraphics[width=\linewidth]{maxerror-wr-small.pdf}
\end{center}
\caption{\label{fig:maxerr-dwr}
Shows quantizations of  trials for  where  and  measure  and .  The size of each  is  (from right to left) and the ``ground truth'' quantization  has size .  Smooth, thick curves are  where .  }
\end{figure}

Note that the maximum deviation quantizations  are similar for both statistics (and others we tried), and thus we can use these plots to estimate , the sample probability that , given a value .  We can fit this function as approximately  with  and .  Thus solving for  in terms of , , and  reveals: .
This indicates the big-O notation for the asymptotic bound of  \cite{LLS01} for -samples only hides a constant of approximately .

We also ran these experiments to -variate quantizations by considering the width in  different directions.  As expected, the quantizations for maximum deviation can be fit with an equation  with , so .  For , this bound for  becomes too conservative; even fewer samples were needed.


































\section {Deterministic Computations on Indecisive Point Sets}
\label {sec:pointsetsets}
  
In this section, we take as input a set of  indecisive points, and describe deterministic exact algorithms for creating quantizations of classes of functions on this input.  We characterize problems when these deterministic algorithms can or can not be made efficient.  

\subsection {Polynomial Time Algorithms}
\label {sec:pta}

We are interested in the distribution of the value  for each \pset .  Since there are  possible \psets, in general we cannot hope to do anything faster than that without making additional assumptions about .
Define  as the fraction (measured by weight) of \psets of  for which  gives a value smaller than or equal to .
In this version, for simplicity, we assume general position and that  can be described by  words, (handled otherwise in Appendix \ref{sec:bignum}).  
First, we will let  denote the radius of the smallest enclosing disk of  in the plane, and show how to solve the decision problem in polynomial time in that case. We then show how to generalize the ideas to other classes of measures.

\paragraph{Smallest enclosing disk.}
Consider the problem where  measures the radius of the smallest enclosing disk of a \pset and let all weights be uniform so  for all  and .  
Evaluating  in time polynomial in  and  is not completely trivial since there are  possible \psets.  However, we can make use of the fact that each smallest enclosing disk is in fact defined by a set of at most  points that lie on the boundary of the disk. 
For each \pset  we define  to be this set of at most  points, which we call the \emph {basis} for .
Bases have the property that .

Now, to avoid having to test an exponential number of \psets, we define a \emph{potential basis} to be a set of at most  points in  such that each point is from a different . Clearly, there are at most  possible potential bases, and each \pset  has one as its basis.
Now, we only need to count for each potential basis the number of \psets it represents.
Counting the number of samples that have a certain basis is easy for the smallest enclosing circle. Given a basis , we count for each indecisive point  that does not contribute a point to  itself how many of its members lie inside the smallest enclosing circle of , and then we multiply these numbers.
\tweeplaatjes {sec-basis} {sec-basis-count} {(a) The smallest enclosing circle of a set of points is defined by two or three points on the boundary. (b) This circle contains one purple (dark) point, four blue (medium) points, and two yellow (light) points. Hence there are  samples that have this basis.}
Figure~\ref {fig:sec-example+sec-example-smallest+sec-example-largest+sec-example-result} illustrates the idea.

\vierplaatjes {sec-example} {sec-example-smallest} {sec-example-largest} {sec-example-result}
{
  (a) Example input with  and . 
  (b) One possible basis, consisting of  points. This basis has one support: the basis itself. 
  (c) Another possible basis, consisting of  points. This basis has three \psets. 
  (d) The graph showing for each diameter  how many \psets do not exceed that diameter. This corresponds to the cumulative distribution of the radius of the smallest enclosing disk of these points.
}


Now, for each potential basis  we have two values: the number of \psets that have  as their basis, and the value . We can sort these  pairs on the value of , and the result provides us with the required distribution.
We spend  time per potential basis for counting the points inside and  time for multiplying these values,
so combined with  potential bases this gives  total time.

\begin {theorem}
Let  be a set of  sets of  points.
In  time, we can compute a data structure of  size that can tell us in  time for any value  how many \psets of  satisfy .
\end {theorem}



\paragraph{LP-type problems.}

The approach described above also works for measures  other than the smallest enclosing disk.  In particular, it works for LP-type problems~\cite{SW92} that have constant combinatorial dimension.
An \emph{LP-type problem} provides a set of constraints  and a function  with the following two properties:

\begin{tabular} {rl} \textsc{Monotonicity:} & For any , .\\
\textsc{Locality:} & For any  with \\ &and an  such that \\ &implies that .\\
\end{tabular}

A \emph{basis} for an LP-type problem is a subset  such that  for all proper subsets  of .  And we say that  is a basis for a subset  if ,  and  is a basis.  A constraint  \emph{violates} a basis  if .
The radius of the smallest enclosing ball is an LP-type problem (where the points are the constraints and ) as are linear programming and many other geometric problems.
Let the maximum cardinality of any basis be the \emph{combinatorial dimension} of a problem.  

For our algorithm to run efficiently, we assume that our LP-type problem has available the following algorithmic primitive, which is often assumed for LP-type problems with constant combinatorial dimension~\cite{SW92}.  For a subset  where  is known to be the basis of  and a constraint , a \emph{violation test} determines in  time if ; i.e., if  \emph{violates} .  More specifically, given an efficient violation test, we can ensure a stronger algorithmic primitive.  A \emph{full violation test} is given a subset  with known basis  and a constraint  and determines in  time if .  This follows because we can test in  time if ; \textsc{monotonicity} implies that  only if , and \textsc{locality} implies that  only if .
Thus we can test if  violates  by considering just  and , but if either \textsc{monotonicity} or \textsc{locality} fail for our problem we cannot.

We now adapt our algorithm to LP-type problems where elements of each  are potential constraints and the ranking function is .
When the combinatorial dimension is a constant , we need to consider only  bases, which will describe all possible supports.

The full violation test implies that given a basis , we can measure the sum of probabilities of all \psets of  that have  as their basis in  time.  For each indecisive point  such that , we sum the probabilities of all elements of  that do not violate .  The product of these probabilities times the product of the probabilities of the elements in the basis, gives the probability of  being the true basis.  See Algorithm \ref{alg:count-prob} where the indicator function applied  returns  if  does not violate  and  otherwise.
It runs in  time.

\begin{algorithm}[h!!t]
\caption{\label{alg:count-prob}Construct Probability Distribution for .}
\begin{algorithmic}[1]
\FOR {all potential bases }
  \FOR { \textbf{to} }
    \IF {there is a  such that }
      \STATE Set .
    \ELSE
      \STATE Set .
    \ENDIF
  \ENDFOR
  \STATE Store a point with value  and weight .
\ENDFOR
\end{algorithmic}
\end{algorithm}

As with the special case of smallest enclosing disk, we can create a distribution over the values of  given an indecisive point set .  For each basis  we calculate , the summed probability of all \psets that have basis , and . We can then sort these pairs according to the value as  again.
For any query value , we can retrieve  in  time and it takes  time to describe (because of its long length).

\begin{theorem}
Given a set  of  indecisive point sets of size  each, and given an LP-type problem  with combinatorial dimension , we can create the distribution of  over  in  time.  The size of the distribution is .
\label{thm:gen-comb-dist}
\end{theorem}

If we assume general position of  relative to , then we can often slightly improve the runtime needed to calculate  using range searching data structures.  However, to deal with degeneracies, we may need to spend  time per basis, regardless.

If we are content with an approximation of the distribution rather than an exact representation, then it is often possible to drastically reduce the storage and runtime following techniques discussed in Section \ref{sec:algQ}.  

Measures that fit in this framework for points in  include smallest enclosing axis-aligned rectangle (measured either by area or perimeter) (), smallest enclosing ball in the , , or  metric (), directional width of a set of points (), and, after dualizing, linear programming ().
These approaches also carry over naturally to deterministically create polynomial-sized -variate quantizations.  


\subsection {Hardness Results}

In this section, we examine some extent measures that do not fit in the above framework.
First, diameter does not satisfy the \textsc{locality} property, and hence we cannot efficiently perform the full violation test.  
We show that a decision variant of diameter is \#P-Hard, even in the plane, and thus (under the assumption that \#P  P), there is no polynomial time solution.  This result holds despite the fact that diameter has a combinatorial dimension of , implying that the associated quantization has at most  steps.  
Second, the area of the convex hull does not have a constant combinatorial dimension, thus we can show the resulting distribution may have exponential size.  


\paragraph{Diameter.}
The \emph {diameter} of a set of points in the plane is the largest distance between any two points. We will show that the counting problem of computing  is \#P-hard when  denotes the diameter.


        \begin{problem}
        \PDIAM: 
        Given a parameter  and a set  of  sets, each consisting of  points in the plane, how many \psets  have ?
        \label{prb:count-diam}
        \end{problem}

We will now prove that Problem \ref{prb:count-diam} is \#P-hard.
Our proof has three steps. We first show a special version of \twoSAT has a polynomial reduction from Monotone \twoSAT, which is \#P-complete~\cite{Val79}.
Then, given an instance of this special version of \twoSAT, we construct a graph with weighted edges on which the diameter problem is equivalent to this \twoSAT instance.  Finally, we show the graph can be embedded as a straight-line graph in the plane as an instance of \PDIAM.

Let 3CLAUSE-\twoSAT be the problem of counting the number of solutions to a 2SAT formula, where each variable occurs in at most three clauses, and each variable is either in exactly one clause or is negated in exactly one clause.  Thus, each distinct literal appears in at most two clauses.  

\begin {lemma}
\label{lem:3CLAUSE-2SAT}
Monotone \twoSAT has a polynomial reduction to 3CLAUSE-\twoSAT.
\end {lemma}
\begin{proof}
The Monotone \twoSAT problem counts the number satisfying assignments to a \twoSAT instance where each clause has at most two variables and no variables are negated.  
Let  be the set of  clauses which contain variable  in a Monotone \twoSAT instance.  We replace  with  variables  and we replace  with the following  clauses  and  .
The first set of clauses preserves the relation with other original variables and the second set of clauses ensures that all of the new variables have the same value (i.e. \textsf{TRUE} or \textsf{FALSE}).
This procedure is repeated for each original variable that is in more than  clause.
\end{proof}

We convert this problem into a graph problem by, for each variable , creating a set  of two points.  Let .  Truth assignments of variables correspond to a \pset as follows.  If  is set \textsf{TRUE}, then the \pset includes , otherwise the \pset includes .  We define a distance function  between points, so that the distance is greater than  (long) if the corresponding literals are in a clause, and less than  (short) otherwise.  If we consider the graph formed by only long edges, we make two observations.
First, the maximum degree is , since each literal is in at most two clauses.
Second, there are no cycles since a literal is only in two clauses if in one clause the other variable is negated, and negated variables are in only one clause.
These two properties imply we can use the following construction to show that the \PDIAM problem is as hard as counting Monotone \twoSAT solutions, which is \#P-complete.

\begin{lemma}
\label{lem:embed-DIAM}
An instance of \PDIAM reduced from 3CLAUSE-\twoSAT can be embedded so .
\end{lemma}
\begin{proof}

Consider an instance of 3CLAUSE-\twoSAT where there are  variables, and thus the corresponding graph has  sets .  We construct a sequence  of  points.
It contains all points from  and a set of at most as many dummy points.
First organize a sequence  so if two points  and  have a long edge, then they are consecutive.  Now for any pair of consecutive points in  which do not have a long edge, insert a dummy point between them to form the sequence .  Also place a dummy point at the end of .

We place all points on a circle  of diameter , see Figure \ref{fig:embed-diam}.
We first place all points on a semicircle of  according to the order of , so each consecutive points are  radians apart.  Then for every other point (i.e. the points with an even index in the ordering ) we replace it with its antipodal point on , so no two points are within  radians of each other.  Finally we remove all dummy points.
This completes the embedding of , we now need to show that only points with long edges are further than  apart.

\eenplaatje {embed-diam}
{Embedded points are solid, at center of circles of radius .  Dummy points hollow. Long edges are drawn between points at distance greater than .}

We can now argue that only vertices which were consecutive in  are further than  apart, the remainder are closer than .  Consider a vertex  and a circle  of radius  centered at .  Let  be the antipodal point of  on .   intersects  at two points, at  radians in either direction from .  Thus only points within  radians of  are further than a distance  from .  This set includes only those points which are adjacent to  in , which can only include points which should have a long edge, by construction.  
\end{proof}

Combining Lemmas~\ref {lem:3CLAUSE-2SAT} and \ref {lem:embed-DIAM}: 
\begin{theorem}
\PDIAM is \#P-hard.
\end{theorem}


\paragraph{Convex hull.}
Our LP-type framework also does not work for any properties of the convex hull (e.g. area or perimeter) because it does not have constant combinatorial dimension; a basis could have size .   In fact, the complexity of the distribution describing the convex hull may be , since if all points in  lie on or near a circle, then every \pset  may be its own basis of size , and have a different value .  



































\section {Deterministic Algorithms for Approximate Computations on Uncertain Points}
\label {sec:distributions}

In this section we show how to approximately answer questions about most representations of independent uncertain points; in particular, we handle representations that have almost all ( fraction) of their mass with bounded support in  and is described in a compact manner (see Appendix \ref{sec:eps-dist}).  
Specifically, in this section, we are given a set  of  independent random variables over the universe , together with a set  of  probability distributions that govern the variables, that is, .
Again, we call a set of points  a \pset of , and because of the independence we have probability .

    The main strategy will be to replace each distribution  by a discrete point set , such that the uniform distribution over  is ``not too far'' from  ( is not the most obvious -sample of ). Then we apply the algorithms from Section~\ref {sec:pointsetsets} to the resulting set of point sets. Finally, we argue that the result is in fact an -quantization of the distribution we are interested in.   Using results from Section \ref{sec:rand-eQ} we can simplify the output in order to decrease the space complexity for the data structure, without increasing the approximation factor too much.

\paragraph{General approach.}
Given a distribution  describing uncertain point  and a function  of bounded combinatorial dimension  defined on a \pset of , we can describe a straightforward range space , where  is the set of ranges corresponding to the bases of  (e.g., when  measures the radius of the smallest enclosing ball,  would be the set of all balls).
More formally,  is the set of subsets of  defined as follows: for every set of  points which define a basis  for ,  contains a range  that contains all points  such that .
However, taking -samples from each  is \emph{not} sufficient to create sets  such that  so for all  we have .

 is a complicated joint probability depending on the  distributions and , and the  straightforward -samples do not contain enough information to decompose this joint probability.  
The required -sample of each  should model  in relation to  and any instantiated point  representing  for .  
The following crucial definition allows for the range space to depend on any  points, including the possible locations of each uncertain point.  

Let  describe a family of Lebesgue-measurable sets defined by  points  and a value .  Specifically,  is the set of points .
We describe examples of  in detail shortly, but first we state the key theorem using this definition.  Its proof, delayed until after examples of , will make clear how  exactly encapsulates the right guarantees to approximate , and thus why  does not.  


\begin{theorem} \label{thm:n-apx}
Let  be a set of uncertain points where each .  
For a function , let  be an -sample of  and let .
Then for any , 

\end{theorem}


\paragraph{Smallest axis-aligned bounding box by perimeter.}
Given a set of points , let  represent the perimeter of the smallest axis-aligned box that contains .  Let each  be a bivariate normal distribution with constant variance.  Solving  is an LP-type problem with combinatorial dimension , and as such, we can describe the basis  of a set  as the points with minimum and maximum - and -coordinates.  Given any additional point , the perimeter of size  can only be increased to a value  by expanding the range of -coordinates, -coordinates, or both.  As such, the region of  described by a range  is defined with respect to the bounding box of  from an edge increasing the -width or -width by , or from a corner extending so the sum of the  and  deviation is .  
See Figure \ref{fig:shape-aabbp}.

Since any such shape defining a range  can be described as the intersection of  slabs along fixed axis (at , , , and ), we can construct an -sample  of  of size  in  time~\cite{Phi08}.  From Theorem \ref{thm:n-apx}, it follows that for  and any  we have .

We can then apply Theorem \ref{thm:gen-comb-dist} to build an -quantization of  in  time.  
The size can be reduced to  within that time bound. 

\begin{corollary}
Let  be a set of indecisive points where each  is bivariate normal with constant variance.  Let  measure the perimeter of the smallest enclosing axis-aligned bounding box.
We can create an -quantization of  in  time  of size .
\label{cor:n-apx-aabbp}
\end{corollary}


\drieplaatjes {shape-aabbp} {shape-sebl2} {shape-sebl2-wedges}
{
  (a) A shape from  for axis-aligned bounding box, measured by perimeter.  
  (b) A shape from  for smallest enclosing ball using the  metric in . The curves are circular arcs of two different radii. 
  (c) The same shape divided into wedges from .
}

\paragraph{Smallest enclosing disk.}
Given a set of points , let  represent the radius of the smallest enclosing disk of .  Let each  be a bivariate normal distribution with constant variance.  Solving  is an LP-type problem with combinatorial dimension , and the basis  of  generically consists of either  points which lie on the boundary of the smallest enclosing disk, or  points which are antipodal on the smallest enclosing disk.  However, given an additional point , the new basis  is either  or it is  along with  or  points which lie on the convex hull of .

We can start by examining all pairs of points  and the two disks of radius  whose boundary circles pass through them.  If one such disk  contains , then .  For this to hold,  and  must lie on the convex hull of  and no point that lies between them on the convex hull can contribute to such a disk.  Thus there are  such disks.
We also need to examine the disks created where  and one other point  are antipodal.  The boundary of the union of all such disks which contain  is described by part of a circle of radius  centered at some .  Again, for such a disk  to describe a part of the boundary of , the point  must lie on the convex hull of .  The circular arc defining this boundary will only connect two disks  and  because it will intersect with the boundary of  and  within these disks, respectively.  An example of  is shown in Figure \ref{fig:shape-sebl2}.

Unfortunately, the range space  has VC-dimension ; it has  circular boundary arcs.  So, creating an -sample of  would take time exponential in .  However, we can decompose any range  into at most  ``wedges.''  We choose one point  inside the convex hull of .  For each circular arc on the boundary of  we create a wedge by coning that boundary arc to .  Let  describe all wedge shaped ranges.  Then  has VC-dimension  at most  since it is the intersection of  ranges (two halfspaces and one disk) that can each have VC-dimension .
We can then create , an -sample of , of size  in  time, via Corollary \ref{cor:norm-es} (Appendix \ref{sec:eps-dist}).
It follows that  is an -sample of , since any range  can be decomposed into at most  wedges, each of which has counting error at most , thus the total counting error is at most .

Invoking Theorem \ref{thm:n-apx}, it follows that , for any  we have .   We can then apply Theorem \ref{thm:gen-comb-dist} to build an -quantization of  in  time.   This is dominated by the time for creating the  -samples, even though we only need to build one and then translate and scale to the rest.  
Again, the size can be reduced to  within that time bound.  

\begin{corollary}
Let  be a set of indecisive points where each  is bivariate normal with constant variance.  Let  measure the radius of the smallest enclosing disk.
We can create an -quantization of  in  time  of size .
\label{cor:n-apx-seb2}
\end{corollary}


Now that we have seen two concrete examples, we prove Theorem \ref{thm:n-apx}. More examples can be found in Appendix \ref {app:moreexamples}.

\paragraph{Proof of Theorem \ref{thm:n-apx}.}


When each  is drawn from a distribution , then we can write  as the probability that  as follows.  Let  be the indicator function, i.e., it is  when the condition is true and  otherwise.

Consider the inner most integral

where  are fixed.  The indicator function  is true 
when

and hence  is contained in a shape .
Thus if we have an -sample  for , then we can guarantee that

We can then move the  outside and change the order of the integrals to write:

Repeating this procedure  times we get:

where .

Similarly we can achieve a symmetric lower bound for .  \qed
























\section {Shape Inclusion Probabilities}

So far, we have been concerned only with computing probability distributions of single-valued functions on a set of points. However, many geometric algorithms produce more than just a single value. In this section, we consider dealing with uncertainty when computing a two-dimensional shape (such as the convex hull, or MEB) of a set of points directly.


\subsection{Randomized Algorithms}
We can also use a variation of Algorithm \ref{alg:rand-draw} to construct -shape inclusion probability functions.
For a point set , let the summarizing shape  be from some geometric family  so  has bounded VC-dimension .
We randomly sample  point sets  each from  and then find the summarizing shape  (e.g. minimum enclosing ball) of each .  Let this set of shapes be .  If there are multiple shapes from  which are equally optimal, choose one of these shapes at random.
For a set of shapes , let  be the subset of shapes that contain .
We store  and evaluate a query point  by counting what fraction of the shapes the point is contained in, specifically returning  in  time.  In some cases, this evaluation can be sped up with point location data structures.

To state the main theorem most cleanly, for a range space , denote its dual range space as  where  is all subsets , for any point , such that .  Recall that the VC-dimension  of  is at most  where  is the VC-dimension of , but is typically much smaller.  

\begin{theorem}
Consider a family of summarizing shapes  with dual range space  with VC-dimension , and where it takes  time to determine the summarizing shape  for any point set  of size .
For a distribution  of a point set of size , with probability at least ,
we can construct an -\sip function of size 
and in time .
\label{thm:rand-sip}
\end{theorem}
\begin{proof}
Using the above algorithm, sample  point sets  from  and generate the  summarizing shapes .
Each shape is a random sample from  according to , and thus  is an -sample of .

Let , for , be the probability that  is the summarizing shape of a point set  drawn randomly from .
For any , let  be the probability that some shape from the subset  is the summarizing shape of  drawn from .

We approximate the \sip function at  by returning the fraction .
The true answer to the \sip function at  is .
Since  is an -sample of , then with probability at least 

\end{proof}



\paragraph{Representing -\sip functions by isolines.}

\vierplaatjes [height=120pt]
{sip-uniform-seb-5000-half} {sip-normal-seb-5000-half.pdf} 
{sip-uniform-aabb-25000-half.pdf} {sip-normal-aabb-25000-half.pdf}
{ \label {fig:sip-seb} 
  The \sip for the smallest enclosing ball (a,b) or smallest enclosing 
  axis-aligned rectangle (c,d), for uniformly (a,c) or normally (b,d) 
  distributed points. Isolines are drawn for .
}

Shape inclusion probability functions are density functions.  A convenient way of visually representing a density function in  is by drawing the isolines.  A \emph{-isoline} is a collection of closed curves bounding the regions of the plane where the density function is greater than .

In each part of Figure \ref{fig:sip-seb} a set of 5 circles correspond to points with a probability distribution.   In part (a,c) the probability distribution is uniform over the inside of the circles.  In part (b,d) it is drawn from a normal distribution with standard deviation given by the radius.  We generate -\sip functions for the smallest enclosing ball in Figure \ref{fig:sip-seb}(a,b) and for the smallest axis-aligned rectangle in Figure \ref{fig:sip-seb}(c,d).

In all figures we draw approximations of -isolines.
These drawing are generated by randomly selecting  (Figure \ref{fig:sip-seb}(a,b)) or  (Figure \ref{fig:sip-seb}(c,d)) shapes, counting the number of inclusions at different points in the plane and interpolating to get the isolines.
  The innermost and darkest region has probability , the next one probability , etc., the outermost region has probability .





\subsection {Deterministic Algorithms}

We can also adapt the deterministic algorithms presented in Section \ref{sec:pta} to deterministically create SIP functions for indecisive (or -SIPs for many uncertain) points.  
We again restrict our attention to a class of LP-type problems, specifically, problems where the output is a (often minimal) summarizing shape  of a data set  where the boundaries of the two shapes  and  intersect in at most a constant number of locations.  An example is the smallest enclosing disk in the plane, where the circles on the boundaries of any two disks intersect at most twice.  

As in Section \ref{sec:pta}, this problem has a constant combinatorial dimension  and possible locations of indecisive points can be labeled inside or outside  using a full violation test.  This implies, following Algorithm \ref{alg:count-prob}, that we can enumerate all  potential bases, and for each determine its weight towards the SIP function using the full violation test.  This procedure generates a set of  weighted shapes in  time.  Finally, a query to the SIP function can be evaluated by counting the weighted fraction of shapes that it is contained in.  

We can also build a data structure to speed up the query time.  Since the boundary of each pair of shapes intersects a constant number of times, then all  pairs intersect at most  times in total.  In , the arrangement of these shapes forms a planar subdivision with as many regions as intersection points.  We can precompute the weighted fraction of shapes overlapping on each region.  Textbook techniques can be used to build a query structure of size  that allows for stabbing queries in time  to determine which region the query lies, and hence what the associated weighted fraction of points is, and what the SIP value is.  
We summarize these results in the following theorem.  

\begin{theorem}
Consider a set  of  indecisive point sets of size  each, and an LP-type problem  with combinatorial dimension  that finds a summarizing shape for which every pair intersects a constant number of times.  
We can create a data structure of size  in time  that answers SIP queries exactly in  time, 
and another structure of size  in time  that answers SIP queries exactly in  time.  
\end{theorem}


Furthermore, through specific invocations of Theorem \ref{thm:n-apx}, we can extend these polynomial deterministic approaches to create -SIP data structures for many natural classes of uncertain input point sets.  

































\section {Conclusions}

In this paper, we studied the computation and representation of complete probability distributions on the output of single- and multi-valued geometric functions, when the input points are uncertain. 
We considered randomized and deterministic, exact and approximate approaches for indecisive and probabilistic uncertain points, and presented polynomial-time algorithms as well as hardness results.
These results extend to when the output distribution is over the family of low-description-complexity summarizing shapes. 

We draw two main conclusions.
Firstly, we observe that the tractability of exact computations on indecisive points really depends on the problem at hand. On the one hand, the output distribution of LP-type problems can be represented concisely and computed efficiently. On the other hand, even computing a single value of the output distribution of the diameter problem is already \#P-hard.
Secondly, we showed that computing approximate quantizations deterministically is often possible in polynomial time. However, the polynomials in question are of rather high degree, and while it is conceivable that these degrees can be reduced further, this will require some new ideas. In the mean time, the randomized alternatives remain more practical.

We believe that the problem of representing and approximating distributions of more complicated objects (especially when they are somehow representing uncertain points), is an important direction for further study.  










\section*{Acknowledgments}
The authors would like to thank Joachim Gudmundsson and Pankaj Agarwal for helpful discussions in early phases of this work, Sariel Har-Peled for discussions about wedges, and Suresh Venkatasubramanian for organizational tips.  

This research was partially supported by the Netherlands Organisation for
Scientific Research (NWO) through the project GOGO and under grant 639.021.123,
by the Office of Naval Research under MURI grant N00014-08-1-1015,
and by a subaward to the University of Utah under NSF award 0937060 to CRA.

\newpage
\bibliographystyle{abuser}
\bibliography{uncert,refs}












\newpage
\appendix



\section{-Samples of Distributions}
\label{sec:eps-dist}
In this section we explore conditions for continuous distributions such that they can be approximated with bounded error by discrete distributions (point sets).  We state specific results for multi-variate normal distributions.  

We say a subset  is \emph{polygonal approximable} if there exists a polygonal shape  with  facets such that  for any .  Usually,  is dependent on , for instance for a d-variate normal distribution ~\cite{Phi08,Phi09}.  In turn, such a polygonal shape  describes a continuous point set where  can be given an -sample  using  points if  has bounded VC-dimension \cite{Mat99} or using  points if  is defined by a constant  number of directions \cite{Phi08}.  For instance, where  is the set of all balls then the first case applies, and when  is the set of all axis-aligned rectangles then either case applies.

A shape  may describe a distribution .
For instance for a range space , then the range space of the associated shape  is  where  describes balls in  for the first  coordinates and any points in the th coordinate.

The general scheme to create an -sample for , where  is a polygonal shape, is to use a lattice  of points.
A \emph{lattice}  in  is an infinite set of points defined such that for  vectors  that form a basis, for any point ,  and  are also in  for any .
We first create a discrete -sample  of   and then create an -sample  of  using standard techniques~\cite{CM96,Phi08}.  Then  is an -sample of .
For a shape  with  -faces on its boundary, any subset  that is described by a subset from  is an intersection  for some .  Since  has  -dimensional faces, we can bound the VC-dimension of  as  where  is the VC-dimension of .
Finally the set  is determined by choosing an arbitrary initial origin point in  and then uniformly scaling all vectors  until ~\cite{Mat99}.  This construction follows a less general but smaller construction in Phillips~\cite{Phi08}.

It follows that we can create such an -sample  of  of size  in time  by starting with a scaling of the lattice so a constant number of points are in  and then doubling the scale until we get to within a factor of  of .  If there are  points inside , it takes  time to count them.  We can then take another -sample of  of size

 in time
 .

\begin{theorem}
For a polygonal shape  with  (constant size) facets, we can construct an -sample for  of size

in time
,
where  has VC-dimension  and
.

This can be reduced to size

in time
.
\label{lem:mu-eps}
\end{theorem}

We can consider the specific case of when  is a -variate normal distribution .  Then  and .

\begin{corollary}
Let  be a -variate normal distribution with constant standard deviation.
We can construct an -sample of  with VC-dimension  (and where ) of size  in time
.
\label{cor:norm-es}
\end{corollary}

For convenience, we restate a tighter, but less general theorem from Phillips, here slightly generalized.
\begin{theorem}[\cite{Phi08,Phi09}]
Let  be a -variate normal distribution with constant standard deviation.
Let  be a range space where the ranges are defined as the intersection of  slabs with fixed normal directions.
We can construct an -sample of  of size  in time .
\label{thm:Phi08}
\end{theorem}


\subsection{Avoiding Degeneracy}
An important part of the above construction is the arbitrary choice of the origin points of the lattice .  This allows us to arbitrarily shift the lattice defining  and thus the set .  In Section \ref{sec:pta} we need to construct  -samples  for  range spaces .
In Algorithm \ref{alg:count-prob} we examine sets of  points, each from separate -samples that define a minimal shape .  It is important that we do not have two such (possibly not disjoint) sets of  points that define the same minimal shape .  (Note, this does not include cases where say two points are antipodal on a disk and any other point in the disk added to a set of  points forms such a set; it refers to cases where say four points lie (degenerately) on the boundary of a disc.)
We can guarantee this by enforcing a property on all pairs of origin points  and  for  and .  For the purpose of construction, it is easiest to consider only the th coordinates  and  for any pair of origin points or lattice vectors (where the same lattice vectors are used for each lattice).  We enforce a specific property on every such pair  and , for all  and all distributions and lattice vectors.

First, consider the case where  describes axis-aligned bounding boxes.  It is easy to see that if for all pairs  and  that  is irrational, then we cannot have  points on the boundary of an axis-aligned bounding box, hence the desired property is satisfied.

Now consider the more complicated case where  describes smallest enclosing balls.  There is a polynomial of degree  that describes the boundary of the ball, so we can enforce that for all pairs  and  that  is of the form  where  and  are rational coefficients and  and  are distinct integers that are not multiple of cubes.    Now if  such points satisfy (and in fact define) the equation of the boundary of a ball, then no th point which has this property with respect to the first  can also satisfy this equation.

More generally, if  can be described with a polynomial of degree  with  variables, then enforce that every pair of coordinates are the sum of -roots.  This ensures that no  points can satisfy the equation, and the undesired situation cannot occur.



\section {Computing Other Measures on Uncertain Points}
\label {app:moreexamples}
We generalize this machinery to other LP-type problems  defined on a set of points in  and with constant combinatorial dimension.  Although, in some cases (like smallest axis-aligned bounding box by perimeter) we are able to show that  has constant VC-dimension, for other cases (like radius of the smallest enclosing disk) we cannot and need to first decompose each range  into a set of disjoint ``wedges'' from a family of ranges .

To simplify the already large polynomial runtimes below we replace the runtime bound in Theorem \ref{thm:gen-comb-dist-bignum} (below, which extends Theorem \ref{thm:gen-comb-dist} to deal with large input sizes) with .


\begin{lemma}
If the disjoint union of  shapes from  can form any shape from , then an -sample of  is an -sample of .
\end{lemma}
\begin{proof}
For any shape  we can create a set of  shapes  whose disjoint union is .  Since each range of  may have error , their union has error at most .
\end{proof}

We study several example cases for which we can deterministically compute -quantizations.  For each case we show an example element of  on an example of  points.

To facilitate the analysis, we define the notion of shatter dimension, which is similar to VC-dimension.  The shatter function  of a range space  is the maximum number of sets in  where .  The \emph{shatter dimension}  of a range space  is the minimum value such that .  If a range  is defined by  points, then .  It can be shown \cite{HP} that  and .
And, in general, the basis size of the related LP-type problem is bounded .





\paragraph{Directional Width.}
We first consider the problem of finding the width along a particular direction  (\textsf{dwid}).  Given a point set ,  is the width of the minimum slab containing , as in Figure \ref{fig:shape-dwid}.    This can be thought of as a one-dimensional problem by projecting all points  using the operation .  The directional width is then just the difference between the largest point and the smallest point.  As such, the VC-dimension of  is .  Furthermore,  in this case, so  also has VC-dimension .  For , we can then create an -sample  of  of size  in  time given basic knowledge of the distribution .
We can then apply Theorem \ref{thm:gen-comb-dist} to build an -quantization in time  for the \textsf{dwid} case.

We can actually evaluate  for all values of  faster using a series of sweep lines.  Each of the  potential bases are defined by a left and right end point.  Each of the  points in  could be a left or right end point.
We only need to find the  widths (defined by pairs of end points) that wind up in the final -quantization.
Using a Frederickson and Johnson approach~\cite{FJ84}, we can search for each width in  steps.  At each step we are given a width  and need to decide what fraction of \psets have width at most .  We can scan from each of the possible left end points and count the number of \psets that have width at most .  For each , this can be performed in  time with a pair of simultaneous sweep lines.
The total runtime is .

\begin{theorem}
We can create an -quantization of size  for the \textsf{dwid} problem in  time.
\end{theorem}

\tweeplaatjes {shape-dwid} {shape-aabba}
{(a) Directional (vertical) width. (b) Axis-aligned bounding box, measured by area. The curves are hyperbola parts.}


\paragraph{Axis-aligned bounding box.}
We now consider the set of problems related to axis-aligned bounding boxes in .  For a point set , we minimize , which either represents the -dimensional volume of  (the \textsf{aabbv} case --- minimizes the area in ) or the -dimensional volume of the boundary of  (the \textsf{aabbp} case --- minimizes the perimeter in ).
      Figures~\ref{fig:shape-aabbp} and~\ref {fig:shape-aabba} show two examples of elements of  for the \textsf{aabbp} case and the \textsf{aabbv} case in .  For both  has a shatter dimension of  because the shape is determined by the -coordinates of  points and the -coordinates of  points.  This generalizes to a shatter dimension of  for , and hence a VC-dimension of .  The smaller VC-dimension in the \textsf{aabbp} case discussed in detail above can be extended to higher dimensions.



Hence, for ,  for both cases we can create an -sample  of , each of size  in total time  via Corollary \ref{cor:norm-es}.
In , we can construct the -quantization in
 time via Theorem \ref{thm:gen-comb-dist}.

\begin{theorem}
We can create an -quantization of size  for the \textsf{aabbp} or \textsf{aabbv} problem on  -variate normal distributions in  time.
\end{theorem}


\paragraph{Smallest enclosing ball.}
Figure \ref{fig:shape-seblinfty+shape-sebl1} shows example elements of  for smallest enclosing ball, for metrics  (the \textsf{seb} case) and  (the \textsf{seb} case) in .  An example element of  for smallest enclosing ball for the  metric (the \textsf{seb} case) was shown in Figure \ref{fig:shape-sebl2}.  For  and ,  has VC-dimension  because the shapes are defined by the intersection of halfspaces from  predefined normal directions.
For  and , we can create  -samples  of each  of size  in total time  via Theorem \ref{thm:Phi08}.
We can then create an -quantization in  time via Theorem \ref{thm:gen-comb-dist}.

\tweeplaatjes {shape-seblinfty} {shape-sebl1}
{(a) Smallest enclosing ball,  metric. (b) Smallest enclosing ball,  metric.}

\begin{theorem}
We can create an -quantization of size  for the \textsf{seb} or \textsf{seb} problem on  -variate normal distributions in  time.
\end{theorem}


For the  case in ,
 has infinite VC-dimension, but  has VC-dimension at most  because it is the intersection of  halfspaces and one disc.
Any shape  can be formed from the disjoint union of  wedges.  Choosing a point in the convex hull of  as the vertex of the wedges will ensure that each wedge is completely inside the ball that defines part of its boundary.  Thus, in  the  -samples of each  are of size  and can all be calculated in total time .  And then
the -quantization can be calculated in  time by assuming general position of all  and then using range counting data structures.
We conjecture this technique can be extended to , but we cannot figure how to decompose a shape  into a polynomial number of wedges with constant VC-dimension.

       \begin{table}[h!!t]
      \caption{-Samples for Summarizing Shape Family .}
      \small
      \centering
      \begin{tabular}{|l|c|c|c|c|c|}
      \hline
      case &  &  &  &  & runtime
      \\ \hline \hline
      \textsf{dwid} &  &  &  &  & 
      \\ \hline
      \textsf{aabbp} &  &  &  &  & 
      \\ \hline
      \textsf{aabbv} &  &  &  &  & 
      \\ \hline
      \textsf{seb} &  &  &  &  & 
      \\ \hline
      \textsf{seb} &  &  &  &  & 
      \\ \hline
      \textsf{seb}  &  &  & \hspace{-.06in} &  & 
      \\ \hline
      \textsf{diam}  &   &  &  &  & 
      \\ \hline
     \end{tabular}
      \label{tbl:Afn-size}
      \\   ignores poly-logarithmic factors .
      \end{table}


\section{Algorithm \ref{alg:count-prob} in RAM model}
\label{sec:bignum}
In this section we will analyze Algorithm \ref{alg:count-prob} without the assumption that the integer  can be stored in  words.  
To simplify the results, we assume a RAM model where a word size contains  bits, each weight  can be stored in one word, and the weight of any basis , the product of   word weights, can thus be stored in  words.
The following lemma describes the main result we will need relating to large numbers.
\begin{lemma}
In the RAM model where a word size has  bits we can calculate the product of  numbers where each is described by  bits in  time.
\end{lemma}
\begin{proof}
Using F\"urer's recent result~\cite{Fur09} we can multiply two -bit numbers in  bit operations.  The product of  -bit numbers has  bits, and can be accomplished with  pairwise multiplications.

We can calculate this product efficiently from the bottom up, starting with  multiplications of two  bit numbers.  Then we perform  multiplications of two  bit numbers, and so on.  Since each operation on  bits takes  time in our model, F\"urer's result clearly upper bounds the RAM result.
The total cost of this can be written as

Since we assume  we can simplify this bound to .
\end{proof}

This implies that Algorithm \ref{alg:count-prob} takes  time where each  can be described in  bits.  This dominates the single division and all other operations.  
Now we can rewrite Theorem \ref{thm:gen-comb-dist} without the restriction that  can be stored in  words.  

\begin{theorem}
Given a set  of  indecisive point sets of size  each, and given an LP-type problem  with combinatorial dimension , we can create the distribution of  over  in  time.  The size of the distribution is .
\label{thm:gen-comb-dist-bignum}
\end{theorem}


\end{document}
