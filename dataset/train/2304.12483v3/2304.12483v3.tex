\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subcaption}


\usepackage{booktabs}
\usepackage{float}
\usepackage{appendix}
\usepackage{lipsum}
\usepackage{afterpage}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\usepackage[normalem]{ulem}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\rai}[1]{\textcolor{blue}{[Aashish: #1]}}
\newcommand{\fer}[1]{\textcolor{red}{[Fernando: #1]}}
\newcommand{\fran}[1]{\textcolor{red}{[Francisco: #1]}}
\iccvfinalcopy

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}





\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\def\iccvPaperID{2893} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{ Towards Realistic Generative 3D Face Models }




\author{
Aashish Rai
\and
Hiresh Gupta
\and
Ayush Pandey
\and
Francisco Vicente Carrasco
\and
Shingo Jason Takagi
\and
Amaury Aubel
\and
Daeil Kim
\and
Aayush Prakash
\and
Fernando De la Torre
\vspace{0.05in}
\and
\centerline{Carnegie Mellon University \hspace{0.2in} Meta Reality Labs}
\and
{\tt\small \url{https://aashishrai3799.github.io/Towards-Realistic-Generative-3D-Face-Models}}
}

\vspace{-0.2in}



\twocolumn[{
\renewcommand\twocolumn[1][]{#1}\newcommand{\dalle}[0]{DALLE\xspace}
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
\includegraphics[width=0.95\linewidth]{figures/main_figure_v6.png}
    \vspace{-0.075in}
    \caption{3D generative face model. a) High-resolution 3D shape and albedo recovered from a StyleGAN2 generated image. Novel views can be rendered using the estimated face model. b) Editing of 3D faces with text. This method allows for 3D expression manipulation through guidance with the CLIP model.}\label{fig:fig1}
\end{center}
}]
\maketitle



\begin{abstract}
\vspace{-0.1in}
\blfootnote{* equal contribution}
In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face  model to produce high-quality albedo and precise 3D shape leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. 
The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision.
Moreover, this approach outperforms most state-of-the-art (SOTA) methods in the well-known NoW and REALY benchmarks for 3D face shape reconstruction. It also outperforms the SOTA reconstruction models in recovering rendered faces' identities across novel poses by an average of . Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces.


\end{abstract}

\vspace{-0.2in}
\section{Introduction}
\vspace{-0.1in}




The success of language models like GPT-3~\cite{radford2019language}, and more recently, the release of text-to-image models like  GLIDE~\cite{Nichol2021GLIDETP}, DALLE-2~\cite{ramesh2022hierarchical}, or Imagen \cite{Saharia2022PhotorealisticTD} have all contributed to the enormous popularity of generative AI. Besides generating images with unprecedented visual quality, these models also show remarkable generalization ability to novel texts with complex compositions of concepts, making them generalists for image synthesis. In the context of faces, StyleGAN2~\cite{karras2020analyzing} has the capability of generating powerful face images that are frequently indistinguishable from reality. While these 2D generative models create high-quality faces for many applications of interest, such as facial animation~\cite{ichim2015dynamic, weise2011realtime}, expression transfer~\cite{kim2018deep, thies2016face2face, olivier2021facetunegan} virtual avatars~\cite{lombardi2018deep}, 
these 2D models often encounter difficulties when it comes to effectively  disentangle facial attributes like pose, expression, and illumination.  As a result, their capacity to edit such attributes is limited. Moreover, 
a  3D representation (shape, texture) is crucial to many entertainment industries—including games, animation, and visual effects— that are demanding 3D content at increasingly enormous scales to create immersive virtual worlds.  Recall that many applications of interest require 3D assets that are consumable by a graphics engine (e.g., Unity \cite{unity}, Unreal \cite{unrealengine}).


To address this demand, recently, researchers have proposed generative models to generate 3D faces~\cite{abrevaya2019decoupled, taherkhani2022controllable, gecer2020synthesizing}. Even though these algorithms perform well, the lack of diverse and high-quality 3D training data has limited the generalization of these algorithms and their use in real-world applications~\cite{toshpulatov2021generative}. Another line of research involves using parametric models like 3DMM\cite{3dmm}, BFM\cite{bfm}, FLAME\cite{3dmm}, and derived methods \cite{deca, occlusion_robust_mofa, deep3dface_recon, tewari, ringnet} to approximate the 3D geometry and texture of a 2D face image. While these 3D face reconstruction techniques can reasonably recover low-frequency details, they typically do not recover high-frequency details. Also, predicting high-resolution texture maps that capture details remains an unaddressed problem. Most of the works focusing in this direction either emphasize mesh or texture. However, a generative 3D face model that can generate both high-quality texture and a detailed mesh with the same quality as 2D models is still missing.  

This paper proposes a 3D generative model for faces using a self-supervised approach that can generate high-resolution texture and capture high-frequency details in the geometry. The method leverages a pretrained StyleGAN2 to generate high-quality 2D faces (see Fig.~\ref{fig:fig1}~a). 
We propose a network, AlbedoGAN, that  generates light-independent albedo directly from StyleGAN2’s latent space. For the shape component, the FLAME model~\cite{flame} is combined with per-vertex displacement maps guided by StyleGAN's latent space, resulting in a higher-resolution mesh. The two networks for albedo and shape are trained in alternating descent fashion. The proposed method outperforms SOTA methods in shape estimation, such as DECA~\cite{deca} and MICA~\cite{MICA:ECCV2022}, by  and , respectively. It's worth noting that MICA only generates a neutral and frontal smooth mesh, while the proposed algorithm can generate any expression. Fig.~\ref{fig:fig1}(a) shows how an image generated by StyleGAN2 can be uplifted to 3D with a detailed shape and albedo, being able to render realistic 3D faces.  Finally, given the 3D face asset, our algorithm can edit the face in 3D. For example, Fig.~\ref{fig:fig1}(b) illustrates expression manipulation through text-based editing guided by the CLIP model~\cite{radford2021learning}. Briefly stated, our main contributions are:



1. A self-supervised method to leverage StyleGAN2 into a 3D generative model producing high-quality albedo and a detailed mesh.  We introduce AlbedoGAN, a single-pass albedo prediction network that generates high-resolution albedo and decouples illumination using shading maps. 

2. We show that our model outperforms existing methods in capturing high-frequency facial details in a mesh.  Moreover, the proposed method reconstructs 3D faces that recover identity better than SOTA methods. 

3. We propose a displacement map generator capable of decoding per-vertex displacements directly from StyleGAN's latent space using detailed normals of the mesh.

4. Since our entire architecture can generate 3D faces from StyleGAN2’s latent space, we can perform face editing directly in the 3D domain using the latent codes or text. 



\vspace{-2mm}
\section{Related Work}
\vspace{-1mm}

\label{sec:formatting}


Reconstructing a 3D Face from a single 2D image is an ill-posed problem that has intrigued researchers for decades. Blanz et al. \cite{3dmm} took the first significant step in this direction when they introduced 3D Morphable Models (3DMM) \cite{3dmm} in 1999 as general face representation. Their work inspired decades of work in estimating parameters for 3DMM to find a textured mesh that best fits an input 2D face. While estimating parameters for parametric models like 3DMM and its advanced versions like FLAME \cite{flame} has been the bulk of the focus for researchers, there has also been a good amount of work done in learning volumetric representations (e.g., NeRF) for a face. In particular, the focus has been on using Neural Implicit Representation \cite{neural-head-avatars, nerface, imavatar, lolnerf, pi_gan, mofa_nerf} to learn density and radiance to represent a face. However, it has been widely noticed that they are prone to generating artifacts and consume a lot of time to render these detailed representations. Furthermore, these methods do not generate a topologically uniform mesh, and therefore do not directly serve applications in graphic engines, face animation, avatar creation, etc. Due to the above-mentioned reasons, we do not consider implicit representation in our research. In the following sections, we describe work that is related to individual components of our framework.

\begin{figure*}[!ht]
\centering
\includegraphics[width=5.in]{figures/albedo_gan_v6.png} 
\caption{AlbedoGAN. Pose-invariant albedo, , obtained by texture extraction and synthesis \ref{sec:tex_extraction}, is used to train StyleGAN2 generator, , for a given latent code . We use a 3DMM fitting, image blending, and Spherical Harmonics to extract .  and  are introduced to generate realistic images and identity consistent albedo, respectively.}
\label{fig:tex_prep}
\vspace{-2mm}
\end{figure*}



\vspace{-1mm}
\subsection{Texture Generation for 3D Mesh}

Most of the work done in synthesizing texture for a mesh can be broadly divided into two parts: using a parametric texture model like 3DMM or Basel Face Model (BFM) \cite{deca, deep3dface_recon, occlusion_robust_mofa, tewari} or a GAN-based approach \cite{Gecer_2021_CVPR, gecer2020tbgan, Gecer_2019_CVPR, fastganfit, textureGeneration, texture-completion-iccv, gecer2019ganfit, uvgan} to generate texture. 
Using a parametric model BFM \cite{bfm}, works like \cite{deca, deep3dface_recon, occlusion_robust_mofa, tewari} learn an encoder to predict the parameters that generate a texture that best fits the visible part of the face. 
Since the model uses representation in low dimensional PCA-based space, they generate an approximate texture that often lacks high-frequency details, which correspond to low variation direction in the projected latent space and do not lead to photo-realistic rendering. Recently, with the advent of GANs, there are works that leverage them to extract texture \cite{Gecer_2021_CVPR, gecer2020tbgan, texture-completion-iccv}. TBGAN \cite{gecer2020tbgan} trains a Progressive GAN to generate a high-quality texture with a differentiable renderer in a supervised setting. On the other hand, OSTeC \cite{Gecer_2021_CVPR} uses 3DMM as initialization to generate a textured mesh and then uses GAN inversion with StyleGAN2 to generate multiple views of this mesh and extract texture. This is an extremely time-consuming and takes several minutes per image. We propose AlbedoGAN, which is able to generate high-quality texture in a single pass in a time-efficient manner. AlbedoGAN generates textures that maintain identity over multiple poses - where most previous methods struggle.










\subsection{3D Shape prediction from 2D Image}



 Similar to texture, Blanz et al.'s seminal work \cite{3dmm} can represent a face mesh in a low-dimensional PCA-based space. This led to the development of a huge corpus of work \cite{deep3dface_recon, tewari, ringnet, occlusion_robust_mofa} in 3D face reconstruction focused on learning an encoder that could predict parameters for generating shapes using 3DMM, given a 2D image. The encoder could be learned in a self-supervised way with 2D image losses \cite{deca, occlusion_robust_mofa, deep3dface_recon, tewari, ringnet}, or with 3D supervision, \cite{MICA:ECCV2022}. After 3DMM\cite{3dmm}, there have been newer parametric models BFM \cite{bfm} and FLAME \cite{flame}, which have been learned from more subjects and encode the structure of the face better. The explosive development of 3D Face Reconstruction led to the creation of NoW benchmark \cite{ringnet} as a common ground for comparison across 3D Face reconstruction methods. Currently, DECA \cite{deca}, and MICA \cite{MICA:ECCV2022} show best reconstruction on NoW Benchmark. DECA's architecture is inspired by RingNet \cite{ringnet}. However, instead of 3DMM, DECA uses FLAME \cite{flame}, and it adds an encoder-decoder network that learns to generate displacement maps to learn animatable details in UV space. MICA \cite{MICA:ECCV2022}, the current state-of-art model, leverages ArcFace backbone \cite{deng2018arcface} to learn the actual shape of the face by regressing it on a high-quality 3D face scan dataset\cite{LYHM, stirling, facewarehouse}.


































\section{AlbedoGAN Training}




Albedo constitutes one of the crucial parts of a 3D face model, since face appearance is largely dictated by it. To generate high quality 3d models, we need to generate albedo that generalize over pose, age, and ethnicity. However, training such a diverse albedo generative model requires a massive database of 3D scans, which is neither cost nor time effective. An efficient way of extracting textures from existing 2D images is fitting a 3DMM and capturing a UV mapped texture. However, this "pseudo" texture does not generalize well over poses nor disentangle shadows. In this paper, we leverage 3DMM fitting, image blending, and Spherical Harmonics lighting to capture high-quality  resolution albedo that generalizes well over different poses and tackles shading variations.

This section describes albedo training, refer Fig.~\ref{fig:tex_prep} for an overview.  The first step includes texture extraction and correction, \ref{sec:tex_extraction}, followed  by the use of a spherical harmonics model to extract albedo from texture (Section~\ref{sec:sh_model}). Section~\ref{sec:pretraining_albedo} explains training AlbedoGAN - a StyleGAN2 model to generate albedo corresponding to the given latent code . 












\subsection{Texture Extraction and Correction}
\label{sec:tex_extraction}



First, we establish a correspondence between the input 2D image  and the UV domain by taking orthogonal projection of a mesh fitted on the given image using a 3DMM \cite{flame}. Using this correspondence, we get the RGB values for the UV texture and perform barycentric interpolation to fill out the missing pixels. 


Now, texture correction is performed to fill the occluded areas by leveraging pose information. This happens by projecting the flipped input image and fitted mesh, and collecting the pixels corresponding to the missing parts. These pixels are blended to the original texture to get a complete pose-invariant texture.














\subsection{Albedo from Texture}
\label{sec:sh_model}


As shown in Fig. \ref{fig:tex_prep}, the next step includes obtaining an albedo and shading map from the unevenly illuminated texture. Following previous works \cite{deca, deep3dface_recon},  we made the following assumptions: (1) The illumination model is Spherical Harmonics (SH), (2) light source is distant and monochromatic, and (3) surface reflectance is Lambertian. The shading map can thus be calculated as 

\vspace{-2mm}

    

where,  are the SH basis function,  are SH coefficients, and  are surface normals. The relation between albedo , texture , and shading map  can then be defined as  
, where,  is the Schur product.





\begin{figure*}[!ht]
\centering
\includegraphics[width=7. in]{figures/Architecture_v7.png}
\vspace{1mm}
\caption{Overview of our generative model. The AlbedoGAN generator, , is used to synthesize albedo  corresponding to a latent code .  generates a 2D image, , given to the shape model, , to get a detailed mesh, \textbf{M}'.
Finally, a differentiable renderer (DR) is used to synthesize  from the mesh , albedo , lighting , and pose .
Losses between  and  are used to train the shape model and the AlbedoGAN via Alternating Descent.}
\vspace{-1mm}
\label{fig:architecture}
\vspace{-1mm}
\end{figure*}

\subsection{Training}  
\label{sec:pretraining_albedo}

In this section, we address the AlbedoGAN model training procedure. Later, the resulting model will be fine-tuned during the shape and displacement map training process, taking into account geometry and more sophisticated Phong illumination model.








Our approach, AlbedoGAN, is built upon a generative model that can synthesize face images corresponding to a latent vector, to this end we selected StyleGAN2. 
This model is best suited to our requirements as it uses a mapping network  that maps an input noise vector  to an intermediate latent vector . This mapping, , adds the ability for manipulation and better projection. Consequently, we use  as latent space for AlbedoGAN.

Hence, face images are generated by randomly sampling  using a pretrained StyleGAN2. The generated images act as input, , see Fig.\ref{fig:tex_prep}. The same latent codes, ,  are used in the AlbedoGAN generator, to produce . As shown in Fig.\ref{fig:tex_prep},  is passed through the texture extraction, \ref{sec:tex_extraction} and albedo extraction,  \ref{sec:sh_model}, steps to obtain . This albedo is used as a ground truth for the training of the real/fake discriminator, . Additionally, we constraint AlbedoGAN to generate identity consistent albedos by introducing an identity discriminator, . To this intent, we use the features of a pretrained face recognition model \cite{he2016deep} . Our identity loss is defined as cosine distance between the identity features of predicted albedo  and  as: 



\vspace{-2mm}




\section{Alternating Descent in Albedo and Shape}

In this section, we describe our regression method to 3D shape given a face image and the Differentiable Rendering, DR, based approach to fine-tune AlbedoGAN. This fine-tuning process for AlbedoGAN takes into account expression, camera pose, and the Phong illumination model. 

Unfortunately, jointly optimizing all the components (shape, albedo, illumination, etc.) that produce the best rendered face that is consistent with the input image is computationally expensive. Thus, we propose using Alternating Descent for optimization. First, we optimize the shape for a few iterations while freezing AlbedoGAN. Next, AlbedoGAN is fine-tuned using the updated 3D shape, with more detailed normals of the shape. This alternating optimization cycle is repeated throughout the course of the training process. Next, we first describe albedo optimization, \ref{sec:albedo_optimization}, followed by shape optimization,  \ref{sec:shape_optimization}.

\subsection{Albedo optimization}
\label{sec:albedo_optimization}
To fine-tune AlbedoGAN using the information of the 3D shape and illumination model, we first assume we have an estimate of a detailed 3D shape . As shown in Fig.~\ref{fig:architecture}, given an estimated mesh , predicted albedo , pose , and light , we can generate a detail rendered image  using DR,  as:
  



The overall loss function  is defined as a sum of the following terms: 

\vspace{-4mm}



Where each loss is defined as follows:





{\bf Symmetric Reconstruction Loss, :} A simple supervision function that encourages low-level similarity in the predicted image and the corresponding ground truth and symmetry in the estimated albedo. We use the Mean Squared Error (MSE) to calculate reconstruction error.

\label{eq:l_rec}

\vspace{-2mm}

Where  is the flipped ground truth obtained through StyleGAN2, and  is the rendered estimated image through flipping , pose , and light . 

{\bf Identity Loss, :} This loss term is introduced with the intent of making the AlbedoGAN generator learn to match the identity of the rendered face, , with the ground truth, . As in section \ref{sec:pretraining_albedo}, we use a pretrained face recognition model \cite{he2016deep} for feature extraction. The cosine distance, eq.~\ref{eq:l_id}, is used to calculate the identity loss while fine-tuning the model.


{\bf Perceptual Loss, :} This perceptual loss is introduced with the goal of  forcing AlbedoGAN to generate a  that matches the visual appearance of . Motivated by the existing research, we selected a VGG16 based feature extractor \cite{schroff2015facenet}, a pretrained face recognition model. We use the output of  as the image features. The loss is calculated by the L2 distance between the feature vectors from  and .  



{\bf Landmark Loss, :} AlbedoGAN is also fine tuned-using using 68 facial landmarks detected on the ground truth and the rendered image to avoid misaligned generations. We used a SOTA face landmark detection~\cite{yin2020fan} to predict 68 landmarks on  and . The loss is calculated using MSE between the two set of landmarks. 




\subsection{Shape Model and Optimization}
\label{sec:shape_optimization}
We proceed with the shape optimization, while freezing AlbedoGAN. We sample a latent vector  and use a pretrained StyleGAN2 model, , to generate a 2D face image, , and AlbedoGAN generator, , to generate albedo, . Figure~\ref{fig:architecture} describes the detailed architecture of our shape model, . We leverage ArcFace backbone \cite{deng2019arcface} to predict the face shape (), expression (), lighting (), and camera pose () parameters for the given image . 
The shape  embedding vector , pose , and expression  parameters are fed into a parametric face model that gives us a coarse mesh representation () as described below: 


where  represents the generated coarse mesh synthesized by a 3DMM decoder. Specifically, we use FLAME \cite{flame} as our mesh decoder, which generates a coarse mesh with  vertices. This coarse mesh is computed by using a template mesh, , representing a mean human face and different principal components  and  corresponding to the shape and expression terms respectively.


To capture high-frequency details in meshes, we learn a displacement generator  to augment the coarse mesh, , with a detailed UV displacement map . Recent research \cite{latent_space2, latent_space1} has shown that StyleGAN's latent space contains information about high-frequency details of a face. Using this insight, we predict displacement maps to capture expression and pose dependent per vertex offsets. The latent code  is the same as used in AlbedoGAN and the StyleGAN2 model. Finally, we combine the displacement map along the vertex normals of the mesh  to get a detailed mesh  by adding them in the UV domain. 



We use the detailed mesh, , along with the predicted pose , light  parameters, and the synthesized albedo  to render an image  as described below:



We apply multiple 2D image-based losses, including identity loss, perceptual loss, and landmark loss between  and  to optimize the mesh representation in a self-supervised fashion. 
In addition to the losses, we also calculate a shape center loss eq.~\ref{eq:shape_consistency} on images belonging to identity . In particular, eq.~\ref{eq:shape_consistency} tries to reduce the distance between shape vector for all the images and their corresponding mean .
Besides this, we also perform L2 regularization, eq.~\ref{eq:l_reg}, on predicted shape , expression , and displacement maps  that enforce a prior distribution towards the mean face.




The overall loss function  is defined as a weighted sum:















\section{Experiments}

This section describes the implementation details, quantitative, and qualitative evaluation of the shape and texture reconstruction models, along with SOTA comparison. 











\subsection{Dataset}

We randomly sampled , -dimensional random vectors  from a Gaussian distribution and generated the corresponding  from the StyleGANs mapping network as . These intermediate latent vectors  are used to generate  images  from a pretrained StyleGAN2 \cite{stylegan2} generator. To ensure diversity in the generated images across ethnicity, expression, age, and pose; we followed the work in \cite{rai2021improved}. The texture-preprocessing step (as described in Sec. \ref{sec:tex_extraction}) is used to get the complete GT-albedo corresponding to all the samples in the dataset. 

To train our shape model with 2D images, we chose  of the previously sampled  vectors. Then we perform latent space editing in the  space to generate a total of  images (belonging to different expressions and poses) per identity using StyleGAN2 implementation \cite{rai2021improved} of InterFaceGAN \cite{shen2020interfacegan}. We estimate  landmarks on all the GT images using the FAN~\cite{yin2020fan} landmark detection.   

\subsection{Implementation Details}

{\bf Albedo Generation:} PyTorch \cite{paszke2019pytorch} is used as the implementation framework on CUDA enabled system with NVIDIA RTX  GPUs. We use the PyTorch implementation of StyleGAN2 \footnote{https://github.com/rosinality/stylegan2-pytorch} for albedo and image generation. To generate face images from StyleGAN2, we use the official pretrained weights trained on the FFHQ dataset for  resolution. We use Adam optimizer to train the AlbedoGAN with learning rate  and . The generator was regularized after every  iteration, while the discriminator after every  training iterations. 

In the first step, we train the AlbedoGAN from GT-albedo and use the GAN loss and ID loss eq.\ref{eq:l_id} for supervision. Similar to StyleGAN2, for the GAN loss, we use the element-wise Softplus, which can be defined as .  The  is calculated between the predicted albedo from the generator  and the GT-albedo . The  was set to  during this training. We trained the model on  GPUs, and it took around  hours for complete training (batch size ).

This gave us a robust albedo generator capable of generating a pose-invariant albedo corresponding to a given  or . 

\begin{figure}[t]
\centering
\includegraphics[width=0.42\textwidth]
{figures/texture-samples-new.png}

\caption{(a) Randomly generated albedo from AlbedoGAN. (b) Patches of randomly generated albedo. AlbedoGAN can generate high-quality albedo of  resolution.}
\label{fig:tex_samples}
\vspace{-3mm}
\end{figure}

{\bf Optimizing Shape:} We train our shape model, , on the synthetically generated images by StyleGAN2 capturing multiple images of the same identity across varying expression \& pose. We run a face detector \cite{guo2021sample} on the input images and scale the face crops to a resolution of  before passing them to our shape model. The shape model consist of an ArcFace backbone that is initialized to the weights learned by \cite{MICA:ECCV2022} and a convolution-styled decoder () respectively. The whole pipeline is optimized using Adam Optimizer with a learning rate of . The final loss is calculated between rendered images  and GT , where  is set to , and  , ,  and   are set to  and  respectively. 

\begin{figure}[t]
\centering
\includegraphics[width=0.40\textwidth]{figures/random-new3.png}
\vspace{-1mm}
\caption{Randomly generated coarse mesh, detailed mesh and rendered faces from our model, for input 2D faces.}
\label{fig:stylegan3d_samples}
\vspace{-4mm}
\end{figure}

{\bf Fine-tuning AlbedoGAN using DR:} Once we have a pretrained AlbedoGAN and a good shape estimator, we now fine-tune the AlbedoGAN. This makes the albedo generator learn to capture more details from the GT-face. PyTorch3D is used as the differentiable renderer in all our experiments under this section. We kept using  from previous AlbedoGAN training but gave more gravity to the rendering losses. The rendering loss is calculated between  and  along with a symmetric reconstruction loss between  and  to maintain low-level consistency in the fine-tuned albedo.  and  were set to  for albedo based losses. , ,  and  were set to  for rendering losses, respectively. We fine-tuned the model for another  hours, keeping the batch size of . The results of randomly generated textures from our AlbedoGAN are shown in Fig. \ref{fig:tex_samples}. 

Once we fine-tune the AlbedoGAN, we again train the shape model with updated albedo weights and repeat this step multiple times until we get a final model that can synthesize realistic-looking 3D faces corresponding to the given 2D images.

Fig. \ref{fig:fig1}(a), \ref{fig:stylegan3d_samples}, \ref{fig:mesh_comparison} shows the reconstructed 3D faces generated from our model for multiple poses. It is interesting to see how our model generalizes well over different poses and generates realistic-looking 3D faces. Section B.3 in supplementary demonstrates lighting and pose control in rendered faces using corresponding parameters and shading maps. Some additional results on testing our pipeline on real-world images using GAN inversion can be seen in supplementary section C.
















\subsection{Evaluation of Shape and Texture}

\subsubsection{NoW Benchmark - shape reconstruction}

\vspace{-1mm}

NoW benchmark \cite{ringnet} is a standard benchmark to evaluate the accuracy of 3D meshes estimated from 2D images. It consists of  images for  test subjects across different expressions, poses, and occlusions, split across two sets for validation (20 subjects) and test (80 subjects). NoW provides 3D ground truth meshes for each test subject, and the predicted mesh is rigidly aligned with the ground truth mesh using 3D face landmarks. The per-vertex error is then used for all the subjects to compute the mean, median, and standard deviation of the errors. Table \ref{table:recons_error} depicts the comparison of our model with the current SOTA methods, including DECA and MICA.
Fig.~\ref{fig:mesh_comparison} illustrates the visual comparison among these methods.
Our model outperforms the DECA model by achieving a \textbf{23\%} better median error in coarse mesh and a \textbf{20\%} better median error in the detailed mesh.
Our approach can reconstruct realistic-looking rendered faces, and model accurate head shapes, especially for faces with big heads. 
We also observe an improvement over the MICA model that was trained on 3D face scans \cite{LYHM, stirling, facewarehouse} with 2300 subjects on the NoW validation set. As illustrated in \ref{fig:mesh_comparison}, our method produces a more detailed mesh, capturing wrinkles, expression, pose and head shape correctly by only training on synthetic images. 


\begin{table}[h]
\centering
\caption{Reconstruction error on the NoW Benchmark.}
\vspace{-2mm}
\fontsize{8}{7}
\small
\scalebox{0.85}{\begin{tabular}{c|c|c|c}
Method            & Median (mm) & Mean (mm) & Std (mm)  \\
\hline
\textbf{Validation Set} & & & \\

Deep3D \cite{deep3dface_recon} & 1.286       & 1.864     & 2.361     \\
DECA \cite{deca} & 1.178       & 1.464     & 1.253     \\
MICA \cite{MICA:ECCV2022}   & 0.913       & 1.130      & \textbf{0.948}     \\
Ours   & \textbf{0.903}       & \textbf{1.122}     & 0.957     \\
\hline


\textbf{Test Set} & & & \\

Deep3D \cite{deep3dface_recon} & 1.11       & 1.41     & 1.21     \\
DECA \cite{deca} & 1.09       & 1.38     & 1.18     \\
Ours   & 0.97       & 1.21     & 1.02     \\
MICA \cite{MICA:ECCV2022} & \textbf{0.90}  & \textbf{1.11}  & \textbf{0.92}     \\
\hline


\label{table:recons_error} 
\end{tabular}}
\end{table}


\subsection{REALY 3D Benchmark}

We also evaluated our method on the most recent REALLY benchmark~\cite{REALY} for single-image 3D face reconstruction from frontal and side view images.  Our results, as presented in Tables~\ref{table:realy_front}, demonstrate a significant improvement over DECA by  and MICA by . Our method also stands in the \textbf{top 3} (out of 18 methods) on the REALLY benchmark challenge outperforming most of the existing methods. The only two methods better than ours are HRN \cite{HRN} and Deep3D \cite{deep3d_rebuttal}.  However, they both generate only frontal mesh, while our method generates a complete head model and is trained in an {\bf unsupervised} setting.


\begin{table}[h]
      \caption{Single image reconstruction error on REALY Benchmark for Frontal and Side-view images (lower is better).}
      \label{table:realy_front}
    \fontsize{8}{7}
    \small
    \scalebox{0.85}{\begin{tabular}{|l|l|l|l|l|l|}
    \hline
       Method  & @nose & @mouth & @forehead & @cheek & @all \\ \hline\hline

       \textbf{Front View} & & & & & \\

        HRN & 1.722 & 1.357 & 1.995 & 1.072 & 1.537 \\ 
        Deep3D & 1.719 & 1.368 & 2.015 & 1.528 & 1.657 \\  
        \textbf{Ours} & 1.656 & 2.087 & 2.102 & 1.141 & \textbf{1.746} \\  
        GANFit & 1.928 & 1.812 & 2.402 & 1.329 & 1.868 \\ 
        DECA & 1.697 & 2.516 & 2.394 & 1.479 & 2.010 \\ 
        PRNet & 1.923 & 1.838 & 2.429 & 1.863 & 2.013 \\ 
        EMOCA & 1.868 & 2.679 & 2.426 & 1.438 & 2.103 \\ 
        MICA & 1.585 & 3.478 & 2.374 & 1.099 & 2.134 \\ 
        RingNet & 1.934 & 2.074 & 2.995 & 2.028 & 2.258 \\ 
        
        \hline

        \textbf{Side View} & & & & & \\
        
        HRN & 1.642 & 1.285 & 1.906 & 1.038 & 1.468 \\ 
        Deep3D & 1.749 & 1.411 & 2.074 & 1.528 & 1.691 \\  
        \textbf{Our} & 1.576 & 2.218 & 2.142 & 1.112 & \textbf{1.762} \\  
PRNet & 1.868 & 1.856 & 2.445 & 1.960 & 2.032 \\ 
        DECA & 1.903 & 2.472 & 2.423 & 1.630 & 2.107 \\ 
        EMOCA & 1.867 & 2.636 & 2.448 & 1.548 & 2.125 \\ 
        MICA & 1.525 & 2.636 & 2.448 & 1.548 & 2.125 \\ 
        RingNet & 1.921 & 1.994 & 3.081 & 2.027 & 2.256 \\ 
        
        \hline
        
    \end{tabular}}
    \vspace{-3mm}
    \label{comparision}
\end{table}




\vspace{-4mm}


\begin{table}[th]
\centering
\caption{Diversity values of randomly generated meshes for various methods. Higher is better. }
\vspace{-2mm}
\scalebox{0.85}{\begin{tabular}{c|c|c|c|c}
 -        &  Deep3D \cite{deep3dface_recon}   &   DECA \cite{deca}  &  MICA \cite{MICA:ECCV2022}  & Ours  \\
\hline
DIV &  0.18  &  1.13  & 0.21   &   \textbf{1.67}  
\label{table:div_spec} 
\end{tabular}}
\vspace{-2mm}
\end{table}




\subsubsection{Diversity metrics}

One of the important features of a good 3D face model is how diverse it's synthesized meshes are. Similar to prior works that generate 3D meshes~\cite{abrevaya2019decoupled, taherkhani2022controllable}, we measure global diversity as the mean vertex distance over all possible pairs of  meshes. Table \ref{table:div_spec} reports the diversity values for  meshes synthesized by MICA\cite{MICA:ECCV2022}, DECA\cite{deca} and Deep3D\cite{deep3dface_recon}.

As illustrated in Fig.~\ref{fig:mesh_comparison}, our model captures head shapes better than DECA model which produces similar-looking head shapes. We observe a significant improvement in diversity statistics over MICA that only predicts a smooth and neutral mesh and was trained on limited 3D data.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{figures/compare-new2.png}
\caption{3D Face Reconstruction comparison with DECA\cite{deca} \& MICA \cite{MICA:ECCV2022}. DECA is unable to model head shape accurately and provides a smooth texture. MICA only outputs a smooth, neutral and frontal mesh. Our method produces a more detailed mesh with photo-realistic texture, capturing wrinkles, expressions and head shape accurately.}
\label{fig:mesh_comparison}
\vspace{-8mm}
\end{figure}


\subsubsection{Evaluation of Rendered Faces}


We evaluate the quality of texture by comparing qualitatively to a recently proposed method, OSTeC \cite{Gecer_2021_CVPR}. We also quantitatively compare texture and rendered faces by rendering it on a mesh with other methods, including LiftedGAN~\cite{shi2021lifting}, DECA~\cite{deca}, and OSTeC~\cite{Gecer_2021_CVPR}.

Fig. \ref{fig:OSTeC_vs_ours} shows the visual comparison between OSTeC \cite{Gecer_2021_CVPR} and our model.  OSTeC uses latent optimization based GAN inversion which adds random artifacts in the generated image like the beard appearing on some patches of the face. Furthermore, this leads to inconsistency while stitching textures across different poses.




For quantitative evaluation of our texture and rendered faces, we randomly sampled 1K images from a pretrained StyleGAN2 generator. We use the same set of  to generate rendered 3D faces using our method. The corresponding 2D face images are used to perform 3D reconstruction using other methods\cite{deca, Gecer_2021_CVPR, shi2021lifting}. To compare how well the texture preserves identity, we measure identity similarity between the input image and the corresponding 3D faces rendered at multiple poses, including the original pose, \textdegree, \textdegree, \textdegree, and \textdegree.  The observations can be found in Table \ref{table:id_similarity}. As inferred from the table, our method not only performs better in capturing the identity for the front poses but also for a large number of side poses. More evaluations can be found in supplementary section A.








\begin{table}[h]
\centering

\vspace{-1mm}
\scalebox{0.85}{\begin{tabular}{c|c|c|c|c|c}
 Method & same pose & \textdegree & \textdegree & \textdegree & \textdegree \\
 \hline
 LiftedGAN \cite{shi2021lifting} & 0.90 & 0.88 & 0.854 & 0.83 & 0.804 \\
 DECA \cite{deca} & 0.869 & 0.799 & 0.76 & 0.69 & 0.63 \\
 OSTeC \cite{Gecer_2021_CVPR} & 0.952 & 0.939 & 0.921 & 0.906 & 0.88 \\
 Ours & \textbf{0.999} & \textbf{0.995} & \textbf{0.988} & \textbf{0.972} & \textbf{0.941} 
\vspace{-1mm}
\label{table:id_similarity} 
\end{tabular}}
\caption{ID similarity comparison between the input image and the corresponding 3D face rendered at various poses.}
\vspace{-4mm}
\end{table}


\begin{figure}[h]
\centering
\includegraphics[width=0.40\textwidth]{figures/ostec-comp2.png}
\caption{Qualitative comparison of synthesized texture and rendered results with OSTeC \cite{Gecer_2021_CVPR}. 
OSTeC produces a smooth texture by stitching multiple images, often leading to artifacts.
Our approach can synthesize a better textured mesh outperforming OSTeC in preserving the details. }
\label{fig:OSTeC_vs_ours}
\vspace{-2mm}
\end{figure}




\begin{figure}[h]
\centering
\includegraphics[width=0.40\textwidth]{figures/exp-edit.png}
\caption{Generating different expressions by manipulating . The first column in each row is the neutral expression corresponding to randomly sampled . The subsequent columns show different expressions for the same identity by varying . }
\label{fig:expressions}
\vspace{-4mm}
\end{figure}


\subsection{3D face manipulation}

 Once our end-to-end pipeline is trained and is able to generate albedo and mesh corresponding to a  or , it opens up an avenue for latent space manipulations in 3D rendered faces. This section shows examples of editing 3D faces directly from the latent space or text.

{\bf Latent space manipulation:} Given a latent code , we can get the modified latent code  by , where  is a vector orthogonal to the semantic boundary and  is a constant. More details on latent space manipulations using semantic boundaries can be found in \cite{shen2020interfacegan, rai2021improved}. Fig. \ref{fig:expressions} shows the result of latent space manipulations for generating different expressions of the same identity. 

{\bf Text based 3D Face Editing:} Similar to latent space manipulations, we can perform text-based 3D face editing using Contrastive Language-Image Pretraining
(CLIP) models~\cite{radford2021learning}. We used StyleCLIP \cite{patashnik2021styleclip} model to enable text-based editing. The text-guided latent optimization and latent residual mapper strategies of StyleCLIP are implemented in  space and make it easy for us to plug into our pipeline. This paper shows results for the text-guided latent optimization approach, which can be trained for random text queries. Given a  for a face, StyleCLIP can produce an updated latent code  corresponding to a given text query and input . Fig. \ref{fig:fig1}(c) illustrates some sample results generated for the text-based 3D face editing.  Refer to supplementary Section B.5 for more results on text-based editing.






\section{Conclusion and Future Work}




In this paper, we attempt to develop a high-quality 3D face generation pipeline. We propose AlbedoGAN that synthesizes albedo and generalizes well over multiple poses capturing intrinsic details of the face. Our approach generates meshes that capture high-frequency details like face wrinkles. Comprehensive experiments demonstrate superiority of our method over others in predicting detailed mesh and preserving the identity in reconstructed 3D faces. 
As a consequence of using StyleGAN2 based pipeline, we bring style editing, semantic face manipulations, and text-based editing in 3D faces.



While our pipeline can generate high-quality 3D faces from StyleGAN2's latent space, some issues still need to be addressed. Mesh-based representations are unable to model details like hair. We foresee exploiting topologically uniform mesh, and a NeRF-based approach should be able to capture such facial features. We'll extend our work to incorporate more complex illumination models.






{\small
\bibliographystyle{ieee_fullname}
\bibliography{iccv_paper_review}
}






\end{document}
