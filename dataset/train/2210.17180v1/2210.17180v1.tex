\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{bbm}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{xspace}
\newcommand{\sexyname}{ASE-NAS\xspace}
\newcommand{\sexynameplus}{ASE-NAS+\xspace}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{bbding}

\usepackage{graphicx}
\usepackage{caption}



\usepackage{booktabs} \newcommand{\topline}{\toprule [0.1em]}
\newcommand{\midline}{\midrule [0.05em]}
\newcommand{\bottomline}{\bottomrule [0.1em]}

\clearpage{}\def\support{\mbox{support}}
\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\etal{{\em et al.}}
\def\card{{\mbox{Card}}}

\def\balpha{\mbox{{\boldmath }}}
\def\bbeta{\mbox{{\boldmath }}}
\def\bzeta{\mbox{{\boldmath }}}
\def\bgamma{\mbox{{\boldmath }}}
\def\bdelta{\mbox{{\boldmath }}}
\def\bmu{\mbox{{\boldmath }}}
\def\bftau{\mbox{{\boldmath }}}
\def\beps{\mbox{{\boldmath }}}
\def\blambda{\mbox{{\boldmath }}}
\def\bLambda{\mbox{{\boldmath }}}
\def\bnu{\mbox{{\boldmath }}}
\def\bomega{\mbox{{\boldmath }}}
\def\bfeta{\mbox{{\boldmath }}}
\def\bsigma{\mbox{{\boldmath }}}
\def\bzeta{\mbox{{\boldmath }}}
\def\bphi{\mbox{{\boldmath }}}
\def\bxi{\mbox{{\boldmath }}}
\def\bvphi{\mbox{{\boldmath }}}
\def\bdelta{\mbox{{\boldmath }}}
\def\bvarpi{\mbox{{\boldmath }}}
\def\bvarsigma{\mbox{{\boldmath }}}
\def\bXi{\mbox{{\boldmath }}}
\def\bmW{\mbox{{\boldmath }}}
\def\bmY{\mbox{{\boldmath }}}

\def\bPi{\mbox{{\boldmath }}}

\def\bOmega{\mbox{{\boldmath }}}
\def\bDelta{\mbox{{\boldmath }}}
\def\bPi{\mbox{{\boldmath }}}
\def\bPsi{\mbox{{\boldmath }}}
\def\bSigma{\mbox{{\boldmath }}}
\def\bUpsilon{\mbox{{\boldmath }}}

\def\mA{{\mathcal A}}
\def\mB{{\mathcal B}}
\def\mC{{\mathcal C}}
\def\mD{{\mathcal D}}
\def\mE{{\mathcal E}}
\def\mF{{\mathcal F}}
\def\mG{{\mathcal G}}
\def\mH{{\mathcal H}}
\def\mI{{\mathcal I}}
\def\mJ{{\mathcal J}}
\def\mK{{\mathcal K}}
\def\mL{{\mathcal L}}
\def\mM{{\mathcal M}}
\def\mN{{\mathcal N}}
\def\mO{{\mathcal O}}
\def\mP{{\mathcal P}}
\def\mQ{{\mathcal Q}}
\def\mR{{\mathcal R}}
\def\mS{{\mathcal S}}
\def\mT{{\mathcal T}}
\def\mU{{\mathcal U}}
\def\mV{{\mathcal V}}
\def\mW{{\mathcal W}}
\def\mX{{\mathcal X}}
\def\mY{{\mathcal Y}}
\def\mZ{{\mathcal{Z}}}



\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\def\bmA{{\mathbfcal A}}
\def\bmB{{\mathbfcal B}}
\def\bmC{{\mathbfcal C}}
\def\bmD{{\mathbfcal D}}
\def\bmE{{\mathbfcal E}}
\def\bmF{{\mathbfcal F}}
\def\bmG{{\mathbfcal G}}
\def\bmH{{\mathbfcal H}}
\def\bmI{{\mathbfcal I}}
\def\bmJ{{\mathbfcal J}}
\def\bmK{{\mathbfcal K}}
\def\bmL{{\mathbfcal L}}
\def\bmM{{\mathbfcal M}}
\def\bmN{{\mathbfcal N}}
\def\bmO{{\mathbfcal O}}
\def\bmP{{\mathbfcal P}}
\def\bmQ{{\mathbfcal Q}}
\def\bmR{{\mathbfcal R}}
\def\bmS{{\mathbfcal S}}
\def\bmT{{\mathbfcal T}}
\def\bmU{{\mathbfcal U}}
\def\bmV{{\mathbfcal V}}
\def\bmW{{\mathbfcal W}}
\def\bmX{{\mathbfcal X}}
\def\bmY{{\mathbfcal Y}}
\def\bmZ{{\mathbfcal Z}}



\def\0{{\bf 0}}
\def\1{{\bf 1}}

\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf{Z}}}


\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bff{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

\def\hy{\hat{y}}
\def\hby{\hat{{\bf y}}}


\def\mmE{{\mathbb E}}
\def\mmP{{\mathrm P}}
\def\mmB{{\mathrm B}}
\def\mmR{{\mathbb R}}
\def\mmV{{\mathbb V}}
\def\mmN{{\mathbb N}}
\def\mmZ{{\mathbb Z}}
\def\mMLr{{\mM_{\leq k}}}

\def\tC{\tilde{C}}
\def\tk{\tilde{r}}
\def\tJ{\tilde{J}}
\def\tbx{\tilde{\bx}}
\def\tbK{\tilde{\bK}}
\def\tL{\tilde{L}}
\def\tbPi{\mbox{{\boldmath }}}
\def\tw{{\bf \tilde{w}}}



\def\barx{\bar{\bx}}

\def\pd{{\succ\0}}
\def\psd{{\succeq\0}}
\def\vphi{\varphi}
\def\trsp{{\sf T}}

\def\mRMD{{\mathrm{D}}}
\def \DKL{{D_{KL}}}
\def\st{{\mathrm{s.t.}}}
\def\nth{{\mathrm{th}}}


\def\bx{{\bf x}}
\def\bX{{\bf X}}
\def\by{{\bf y}}
\def\bY{{\bf Y}}
\def\bw{{\bf w}}
\def\bW{{\bf W}}
\def\balpha{{\bm \alpha}}
\def\bbeta{{\bm \beta}}
\def\boldeta{{\bm \eta}}
\def\boldEta{{\bm \Eta}}
\def\bGamma{{\bm \Gamma}}
\def\bmu{{\bm \mu}}

\def\bK{{\bf K}}
\def\bb{{\bf b}}
\def\bg{{\bf g}}
\def\bp{{\bf p}}
\def\bP{{\bf P}}
\def\bh{{\bf h}}
\def\bc{{\bf c}}
\def\bz{{\bf z}}

\def\st{{\mathrm{s.t.}}}
\def\tr{\mathrm{tr}}
\def\grad{{\mathrm{grad}}}

\newtheorem{coll}{Corollary}
\newtheorem{deftn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{ass}{Assumption}


\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{c.f.}} \def\Cf{\emph{C.f.}}
\def\etc{\emph{etc.}} \def\vs{\emph{vs.}}
\def\wrt{{w.r.t.~}} \def\dof{d.o.f}
\def\etal{\emph{et al.}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}\clearpage{}

\begin{document}

\title{Automatic Subspace Evoking for Efficient \\ Neural Architecture Search}

\author{
Yaofo Chen, Yong Guo, Daihai Liao, Fanbing Lv,  Hengjie Song, and Mingkui Tan
\thanks{
	Y. Chen, Y. Guo, H. Song and M. Tan
	are with the School of Software Engineering,
	South China University of Technology,
	Guangzhou 510641,
	China
	(e-mail:
	chenyaofo@gmail.com;
	guoyongcs@gmail.com;
	sehjsong@scut.edu.cn;
	mingkuitan@scut.edu.cn)
}\thanks{
	D. Liao and F. Lv are with Changsha Hisense Intelligent System Research Institute Co., Ltd, Changsha 410006, China
	(e-mail:
	liaodaihai@hisense.com;
	lvfanbing@hisense.com)
}
\thanks{
This work was partially supported by Key-Area Research and Development Program of Guangdong  Province (2019B010155002, 2019B010155001), National Natural Science Foundation of China (NSFC) 61836003 (key project), National Natural Science Foundation of China (NSFC) 62072190, National Key R\&D Program of China (No.2020AAA0106900), Key Realm R\&D Program of Guangzhou 202007030007, Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183.
}
\thanks{Y. Chen and Y. Guo contributed equally to this paper.}
\thanks{H. Song and M. Tan are the corresponding authors.}
}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}


\maketitle

\begin{abstract}
Neural Architecture Search (NAS) aims to automatically find effective architectures from a predefined search space.
However, the search space is often extremely large.
As a result, directly searching in such a large search space is non-trivial and also very time-consuming.
To address the above issues, in each search step, we seek to limit the search space to a small but effective subspace to boost both the search performance and search efficiency.
To this end, we propose a novel Neural Architecture Search method via Automatic Subspace Evoking (\sexyname) that finds promising architectures in automatically evoked subspaces.
Specifically, we first perform a global search, \ie, automatic subspace evoking, to evoke/find a good subspace from a set of candidates.
Then, we perform a local search within the evoked subspace to find effective architectures.
More critically, we further boost search performance by taking well-designed/searched architectures as the initial candidate subspaces.
Experimental results demonstrate that our method not only greatly reduces the search cost but also finds better architectures than state-of-the-art methods in various benchmark search spaces.
\end{abstract}

\begin{IEEEkeywords}
Neural Architecture Search, Search Space Evoking, Subspace Graph, Global Search and Local Search, Search Efficiency, Convolutional Neural Networks
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{D}{eep} neural networks (DNNs) have been the workhorse of many challenging tasks, including image classification~\cite{resnet,alexey2021vit,liu2021Swin,liu2021swinv2}, action recognition~\cite{duan2020omni,feichtenhofer2019slowfast,tu2019action,liu2022apsnet} and neural language processing~\cite{radford2018improving,brown2020language}.
The success of DNNs is largely attributed to the innovation of effective neural architectures.
However, designing effective architectures often greatly depends on expert knowledge and human efforts.
Thus, it is non-trivial to design architectures to satisfy the requirements manually.
To address this, neural architecture search (NAS)~\cite{zoph2016neural} is developed to automate the process of the architecture design.

Existing NAS methods search for effective architectures in a predefined search space~\cite{zoph2016neural,liu2018darts,tan2019mnasnet}.
To cover as many good architectures as possible, the search space is often designed to be extremely large (\eg,  in ENAS~\cite{pham2018efficient} and  in DARTS~\cite{liu2018darts}).
Directly searching in such a large space is very difficult and time-consuming in practice.
Specifically, to explore the large search space, we have to sample and evaluate plenty of architectures, which is very computationally expensive and time-consuming.
Moreover, we can only access a small proportion of architectures in the search space due to the limitation of the computational resources in practice.
In other words, regarding a very large search space, we can only obtain limited information to guide the architecture search.


\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{imgs/pipeline.pdf}
\caption{An illustration of the search process.
We find promising architectures in a two-step search manner:
1) we perform a global search to evoke/find a promising subspace from a set of candidates;
2) we move the focus on the subspace and conduct a local search for effective architectures within it.
Then, we update the candidate subspace with the better searched architecture.
}
\label{fig:graph_space}
\end{figure*}


To overcome the above difficulties brought by the large search space, PNAS~\cite{liu2018progressive} and CNAS~\cite{guo2020breaking} propose to start from a very small search space to perform an architecture search and then gradually enlarge the search space by adding nodes or operations.
Recently, AlphaX~\cite{wang2019alphax} partitions the search space into existing good subspaces and unexplored subspaces and adopts the Monte Carlo Tree Search (MCTS) method to encourage exploring the good ones.
However, these methods suffer from two limitations.
\textbf{First}, these methods still find architectures from a very large space at each search step, which may not only result in unnecessary explorations but also affect the search results.
\textbf{Second}, the small search spaces partitioned by these methods are fixed during the search phase, which may not be optimal to find good architectures.
Thus, how to find/design a small but effective search space that covers as many good architectures as possible is an important problem.

To achieve this goal, an underlying hypothesis is that the neighborhood around an effective architecture is usually a good subspace for further exploration.
In this case, once we find the small but effective search space around a promising architecture, it is more likely to find a better architecture within the subspace rather than the entire search space.
We empirically verify this hypothesis that similar architectures tend to have close performance (See the results of Figures~\ref{fig:acc_vs_dist}, as well as the observations in~\cite{ying2019bench,guo2020breaking}).
Inspired by this, instead of searching in the whole search space, we seek to limit the search space within a reduced one in each search step by recognizing and evoking a small but effective subspace.
In this sense, we may boost the search performance and search efficiency as well since we only focus on an evoked effective subspace, in which it is easier to find good architectures than directly exploring the whole search space.

In this paper, we propose an efficient Neural Architecture Search method via Automatic Subspace Evoking (called \sexyname). 
The key idea is to find/evoke a small but effective subspace from the whole search space in each step of the architecture search.
To this end, we first construct a set of candidate subspaces and then build a subspace graph above them, in which edges denote the relationships/information among different subspaces.
As shown in Figure~\ref{fig:graph_space}, 
we first perform a \textit{global search} to evoke/find a promising subspace from the subspace graph.
Then, we focus on the searched subspace and conduct a \textit{local search} to obtain the resultant architectures.
It is worth noting that, once we find a better architecture, we would also update the subspace graph according to it. In this way, it becomes possible to gradually find better architectures during the search process.
Moreover, we are able to further boost our \sexyname by taking existing well-designed architectures (\eg, OFA architecture~\cite{Cai2020Once}) to construct candidate subspaces.
Extensive experiments in two benchmark search spaces show the effectiveness of the proposed method.

Our contributions are summarized as follows:
\begin{itemize}
    \item 
    Instead of searching in the entire search space, we seek to find/evoke a small but effective subspace for each step of the architecture search.
    With the help of the automatically evoked subspaces, we are able to improve search performance and efficiency simultaneously.
    
    \item We propose a novel Automatic Subspace Evoking algorithm to enhance the performance of neural architecture search. 
    Specifically, we first perform a global search to find promising subspaces and then perform a local search to obtain the resultant architectures.
    
    \item Extensive experiments demonstrate the superiority of the proposed method over existing NAS methods. More critically, the searched subspaces also exhibit promising transferability to a new dataset.
\end{itemize}

\section{Related Work}

\subsection{Neural Architecture Search}

In recent years, NAS has drawn great attention for the effective architectures designing.
As the pioneering work, Zoph and Le~\cite{zoph2016neural} use reinforcement learning (RL) to discover the optimal configuration of each neural network layer.
After that, many RL-based methods~\cite{baker2016designing,guo2021towards,chen2021contrastive,tang2021autopedestrian} enhance the performance of searched architectures of the above method.
Besides, evolutionary based methods~\cite{lu2020neural,chen2021oneshot,chen2021autoformer,dai2021fbnetv3,liu2022block} search for promising architectures by gradually evolving a population.
Moreover, gradient based methods~\cite{liu2018darts,chen2021drnas,xia2020revealing,nguyen2020constrained,yang2022video} find effective architectures by relaxing the search space to be continuous.

However, traditional NAS methods~\cite{baker2016designing,real2019regularized} often require great computational resources and thus result in unaffordable time costs.
To improve search efficiency, plenty of efforts have been made to reduce the computational cost during search.
Weight-sharing based NAS methods~\cite{pham2018efficient,Cai2020Once,chen2021contrastive} estimate the performance of candidate architectures by inheriting weights from a trained supernet, which greatly lowers the computational cost of architecture evaluation.
In addition, EcoNAS~\cite{zhou2020econas} carefully designs a proxy training strategy to evaluate candidate architectures on small proxy datasets.
Unlike these methods, our method achieves high efficiency from a different perspective.
To be specific, we reduce the number of architecture evaluations by searching in small and effective subspaces instead of the whole large space.

\subsection{Search Space Design of NAS Methods}

NAS methods often find promising architectures in a predefined large search space, such as NASNet~\cite{zoph2018learning}, DARTS~\cite{liu2018darts} and MobileNet-like~\cite{tan2019mnasnet} search spaces.
Most existing methods directly perform searching in these large search spaces, which may not only result in inefficient sampling but also hamper the search performance.
To address this issue, local search methods~\cite{white2020exploring} search in an iterative manner: travel all architectures in the neighborhood of an architecture and update it with the best-found architecture, which is computationally expensive since they require plenty of architectures evaluations.
Recent works~\cite{liu2018progressive, guo2020breaking,li2020improving} seek to search from a small search space and enlarge the space by progressively adding the nodes or operations.
In addition, AlphaX~\cite{wang2019alphax} and LaNAS\cite{wang2022sample} build a Monte Carlo Search Tree to partition the search space into different subspaces according to their performance and encourage to explore the promising subspaces.
However, these methods still need to generate architecture from the overall search space and subspaces designed by them are fixed and may not be optimal.
Unlike these methods, our global and local search strategy is able to recognize and evoke the small but effective subspaces from candidates dynamically during the search process and find effective architectures in it.

\begin{figure*}[t]
\centering
\includegraphics[width=.95\linewidth]{imgs/overview.pdf}
\caption{An overview of our \sexyname. We build a set of subspaces  centered on randomly sampled candidate architectures  and construct a subspace graph  to model the relationships among these subspaces.
By taking  as the input, the controller first evokes a promising subspace  via global search policy and then predicts an architecture modification  via local search policy.
Next, we update the candidate architecture  with the resultant architecture  if  has better performance than  (\ie, ).
}
\label{fig:overview}
\end{figure*}

\section{Architecture Search via Subspace Evoking}

In this paper, we propose the Automatic Subspace Evoking (\sexyname) method to boost both the search performance and efficiency of NAS.
In Section~\ref{sec:overall}, we first discuss the motivation and the overview of \sexyname.
Then, we depict the details of two key steps of our method, \ie, the global search and local search in Sections~\ref{sec:global_search} and ~\ref{sec:local_search}, respectively.


\subsection{Motivation and Method Overview}
\label{sec:overall}
Existing NAS methods often consider an extremely large search space  to find good architectures~\cite{pham2018efficient,liu2018darts, Cai2020Once}. 
However, directly performing architecture search in such a large search space is non-trivial and often very expensive.
Regarding these issues, instead of using the whole search space, it should be possible to limit each search step within a reduced search space to boost the search performance and search efficiency as well.
To achieve this goal, an underlying hypothesis is that the neighborhood around an effective architecture is usually a good subspace for further exploration to find better ones.
In this case, we are more likely to find effective architectures in the neighborhood/subspace than the whole search space under the same budget of search cost.
 

Inspired by this, we propose a new search algorithm by evoking/identifying effective subspaces, in which 
it is easier to find good architectures than directly exploring the whole space.
To this end, we define a subspace  based on a center architecture  within it (See more details in Section~\ref{sec:global_search}).
As shown in Figure~\ref{fig:overview} and Algorithm~\ref{alg:training}, we first learn a global search policy  to search for a promising subspace  based on a center architecture .
Then, we further learn a local search policy  to produce the resultant architectures within the subspace.
Specifically, the global search policy  takes a set of candidate subspaces  (as well as their relationship) as inputs and evokes/finds a promising subspace .
Based on the evoked/searched subspace, the local policy  further generates a modification  (\ie, modifying some operations of some layers in ) to explore the subspace.
Finally, we combine  and  to obtain the resultant architecture by

Here,  denotes the combination operation, as will be depicted in Section~\ref{sec:local_search}.
Note that  is devised to constrain the architecture after modifications still in the subspace.

It is worth noting that, if we directly maximize the performance of the resultant architecture , the search algorithm may always select the same subspace with the best center architecture at the current step and thus easily get stuck in a local optimum (See results in supplementary). To avoid this, we encourage the exploration ability by maximizing the performance improvement between  and the center architecture  in , \ie, ,
where  denotes some performance metric (\eg, validation accuracy) and  denotes the optimal parameters of  trained on some dataset.
This can be thought of as finding the subspace with the largest potential to find better architectures, instead of the one containing the best architecture found previously.
Formally, we seek to solve the following optimization problem:

Since we would update the searched subspace with the resultant architecture  (See Figure~\ref{fig:overview}), the performance improvement of the previously searched subspace may not always be the largest one and our method is able to explore other subspaces in the subsequent iterations.

\subsection{Global Search with Automatic Subspace Evoking}
\label{sec:global_search}

As the first step of \sexyname,
we seek to find/evoke effective subspaces via a global search process. Specifically, we first discuss the construction of candidate subspaces. Then, we depict the details of our global search algorithm. 

\begin{algorithm}[t]
	\caption{\small{Training method for \sexyname}.}
    	\begin{algorithmic}[1]\small
            \REQUIRE The search space , the global policy  and the local policy .
            \STATE Train the parameters of the supernet.
            \STATE Randomly sample architectures  from  to build the subspaces  using Eqn.~(\ref{eq:subspace}).
            \STATE Construct the subspace graph  based on .
            \WHILE{not convergent}
                \STATE \emph{// Perform a global search to evoke a promising subspace }
                \STATE Sample a subspace  centered on .
                \STATE \emph{// Perform a local search in the evoked subspace }
                \STATE Sample modifications .
                \STATE Build a resultant architecture .
                \STATE Compute reward  using the weights inherited from the supernet. 
                \STATE \emph{// Update subspaces with the locally searched architecture }
                \IF{}
                    \STATE Replace the candidate subspace  with .
                    \STATE Update the edges connected to  in .
                \ENDIF
                \STATE Update the parameters  and  by optimizing Eqn.~(\ref{eq:multiple_anchor_objective}) using policy gradient~\cite{williams1992simple}.
            \ENDWHILE
    	\end{algorithmic}
		\label{alg:training}
\end{algorithm}

\textbf{{Subspace Construction}}.
At the beginning of our search method, we seek to construct candidate subspaces centered on a set of architectures for search.
To this end, we randomly collect a set of discrete architectures  from the search space and build the candidate subspaces  around them. 
Let  be a function to measure the \textit{Architecture Distance} between two architectures  and .
Specifically, we take an architecture  as the center of the subspace  and constrain that all the architectures  in  have distances less than a specific threshold  from the center architecture , \ie, .
Formally, we construct the subspace  \wrt a center architecture  as follows:


Note that architectures in different subspaces may have different operations/topologies and different performances as well.
To exploit the relationship/information among different subspaces, as shown in Figure~\ref{fig:overview}, we build a subspace graph  to guide the search. 
Here,  is a set of nodes and each node denotes a specific subspace . 
 is a set of directed edges from a weak subspace (with a worse center architecture) to a better subspace (whose center architecture has higher accuracy). 
Note that, instead of randomly initializing the center architectures, we may further improve our \sexyname by taking existing well-designed/searched architectures to construct subspace (See results in Tables~\ref{tab:nasbench_search} and~\ref{tab:imagenet}).



\textbf{Searching for Effective Subspaces}.
In each search step, we conduct a global search to find/evoke a promising subspace  from all candidate subspaces . As mentioned before, we seek to find  that has the potential to achieve a large performance improvement . 
(as aforementioned in Eqn.~(\ref{eq:multiple_anchor_objective})). 
Here,  is the center architecture of  and  is another architecture in this subspace. To this end,  
we devise a controller model which contains a two-layer graph neural network (GNN)~\cite{you2020handling} and an LSTM network.
\footnote{More details about GNN are put in Appendix.}
Specifically, to exploit the information of the subspace graph ,
we first employ the GNN model to extract features from . Then, we feed the extracted features into the LSTM network that samples a candidate subspace  via a softmax classifier. 

Note that the search performance greatly depends on the initial subspaces, if we fix all candidate subspaces in  during search. In this sense, the controller may get stuck in a local optimum due to the very limited search space
(See results in Figure~\ref{fig:all_comparisons_subspace_updating}).
To address this issue, we propose a simple strategy to gradually update/improve the candidate subspaces  using the newly searched architectures.
Specifically, for a selected subspace  (See Figure~\ref{fig:overview}), we replace its center architecture  with the locally searched architecture  if  yields better performance than .
This follows that the subspace around better architecture may be more likely to contain promising architectures.
Once the center architecture updates, the corresponding subspace also gets updated.
Then, we will also update the subspace graph  by updating all the edges that are connected to . In this way, the candidate subspaces constantly improve, which helps to explore more and more promising spaces. After search, we select the best center architecture in the subspace graph as the inferred architecture according to the validation performance.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{imgs/search_distance.pdf}
\caption{An illustration of the architecture representation method and calculation of the architecture distance.
We represent an architecture as a string, in which each item denotes an operation (\eg, convolution).
For example, `3', `5' and `7' denote ,  and  convolution, respectively.
}
\label{fig:arch_representation}
\end{figure}

\subsection{Local Search within the Evoked Subspace}
\label{sec:local_search}

Given a searched subspace ,
we then perform a local search to find effective architectures.
To guarantee that the search process is limited within the subspace,
we first discuss how to measure the distance between architectures. 
Then, we depict the details of our local search algorithm.


Before defining the \emph{Architecture Distance} between two architectures, we first revisit the representation method of architectures, making it easier to be understood.
Following~\cite{Cai2020Once,pham2018efficient}, one can represent an architecture as a -dimensional string , where  denotes the number of components in the architecture and each item  denotes some operation (\eg, convolution).
We propose to compute the \emph{Architecture Distance}  by counting the number of different components between two architectures (See Figure~\ref{fig:arch_representation}).
Let  be an indicator function.
Given two architectures  and , the distance between them is computed by



Here, we justify the proposed architecture distance by empirically investigating the relationship between accuracy and architecture distance (See results in Figure~\ref{fig:acc_vs_dist}).
We observe that the architectures with smaller distances tend to have similar performance, which is also observed in popular NAS methods~\cite{ying2019bench,guo2020breaking}. 
In this sense, once we find a promising subspace centered on a good architecture, it would be much easier to find better architectures via a local search.

To ensure that the locally searched architecture  belongs to , 
we devise a local policy  that modifies the center architecture  by  times. Specifically, for each time modification, the local policy  determines which layer to be modified and which kind of operation to be applied to this layer. 
Such decision process will be repeated  times to obtain the complete modification . Then, by applying  to , we obtain the modified/searched architecture  in the subspace. Note that if  selects the same operation as the original one in some layer, it will produce no modification to the architecture. Thus, after  times single-layer modification, the resultant architecture  still belongs to , \ie, satisfying the constraint .

\textbf{Analysis on the size of local search space.}
Let  be the number of components in an architecture and  be the number of candidate operations for each component.
The size of the whole search space is .
Given a local search distance , the size of the subspace becomes , where  denotes the combination function. 
When we consider a very small , the subspace would be much smaller than the whole space. In this case, the union of these subspaces constructed with a small  may not cover the entire search space.
Nevertheless, it is exactly our key idea that we seek to focus on some promising subspaces instead of the entire search space to enhance both the search performance and search efficiency.
In the extreme case, when we consider , the subspace is exactly the whole space and our method will be reduced to the standard NAS.
Thus, we investigate the effect of  on the performance of \sexyname in Section~\ref{sec:ablation_on_M}.


\begin{table*}[t]
	\centering
	\caption{Comparisons with existing methods in NAS-Bench-201~\cite{dong2020nasbench201}.
	``ImageNet-16-120'" denotes a subset of ImageNet dataset with 120 classes and 1616 resolution.
	''Search Cost" denotes the time cost in the search phase (measured by second).  denotes we implement the baselines with the official code under our same settings. Our \sexyname achieves higher accuracy than state-of-the-art methods with less search cost, which verify the search performance and efficiency of our method.}
    {
    \resizebox{0.75\linewidth}{!}{
	\begin{tabular}{c|c|ccc}
		\topline
		Method & Search Cost (s) & CIFAR-10 & CIFAR-100  & ImageNet-16-120 \\
		\midline
		Random Search & 25k & 93.88\scriptsize0.27 & 71.54\scriptsize1.04 & 45.19\scriptsize1.06 \\
		REINFORCE~\cite{williams1992simple} & 25k & 93.85\scriptsize0.37 & 71.71\scriptsize1.09 & 45.24\scriptsize1.18 \\
		PNAS~\cite{liu2018progressive} & 25k & 93.71\scriptsize0.29 & 70.89\scriptsize0.99 & 44.75\scriptsize0.80\\
		CNAS~\cite{guo2020breaking} & 25k & 93.95\scriptsize0.28 & 71.73\scriptsize1.05 & 45.46\scriptsize0.97 \\
		ENAS~\cite{pham2018efficient} & -- & 53.89\scriptsize0.58 & 13.96\scriptsize2.33 & 14.84\scriptsize2.10 \\
		DARTS~\cite{liu2018darts} & 30k & 54.30\scriptsize0.00 & 15.61\scriptsize0.00 & 16.32\scriptsize0.00 \\
		SETN~\cite{dong2019oneshot} & 34k & 87.64\scriptsize0.00 & 59.05\scriptsize0.24 & 32.52\scriptsize0.21 \\
		GDAS~\cite{dong2019searching} & 32k & 93.61\scriptsize0.09 & 70.70\scriptsize0.30 & 41.71\scriptsize0.98 \\
		DSNAS~\cite{hu2020dsnas} & -- & 93.08\scriptsize0.13 & 31.01\scriptsize16.38 & 41.07\scriptsize0.09 \\
		PC-DARTS~\cite{xu2020pcdarts} & -- & 93.41\scriptsize0.30 & 67.48\scriptsize0.89 & 41.31\scriptsize0.22 \\
\midline
		\sexyname (Ours) & 25k & \textbf{94.23\scriptsize0.22} & 72.76\scriptsize0.80 & 46.13\scriptsize0.67 \\
		\sexynameplus (Ours) & 25k & -- & \textbf{73.12\scriptsize0.61} & \textbf{46.66\scriptsize0.52} \\
		\bottomline
	\end{tabular}
	}
	}
	\label{tab:nasbench_search}
\end{table*}


\section{Experiments}

The experiments are organized as follows.
We first perform experiments in NAS-Bench-201~\cite{dong2020nasbench201} search space to demonstrate the effectiveness of our \sexyname.
Then, we evaluate our \sexyname in MobileNet-like~\cite{howard2019searching} search space and compare the performance of our method with state-of-the-art methods on ImageNet~\cite{deng2009imagenet}.

\subsection{Performance Comparisons on NAS-Bench-201}

\textbf{Search Space.}
We apply our \sexyname to a cell-based NAS-Bench-201 search space~\cite{dong2020nasbench201}.
Each cell is a directed acyclic graph with 4 nodes and 6 edges.
Each edge is associated with an operation, which has 5 different candidates, including \textit{zeroize}, \textit{skip connection}, \textit{11 convolution}, \textit{33 convolution} and \textit{33 average pooling}.
Since we search for the candidate operation for each edge, there are  candidate architectures in total.
For each architecture, NAS-Bench-201 provides precomputed training, validation, and test accuracies on three different datasets, namely CIFAR-10, CIFAR-100 and ImageNet-16-120. Note that ImageNet-16-120 is a subset of ImageNet~\cite{deng2009imagenet} dataset with 120 classes and 1616 image resolution.

\textbf{Implementation Details.}
Following the settings in NAS-Bench-201~\cite{dong2020nasbench201}, we use the validation accuracy in epoch 12 as the reward and report the test accuracy in epoch 200 to compare with other baseline methods.
For a fair comparison, we consider the evaluation time of candidate architectures when computing the search cost (limited to 25k seconds).
Following the setting in ~\cite{pham2018efficient}, we train our \sexyname with a batch size of 1 and set the strength of the entropy regularizer to .
We use an Adam optimizer with a learning rate of .
We set the number of candidate subspaces  to 4 and the search distance  to 4.
In practice, we combine the global policy  and the local policy  into a single policy to make decisions to predict the promising subspace  and the modification  in succession.
In other words, we seek to learn a joint policy that first selects a candidate subspace and then finds a promising architecture modification within the selected subspace.
Note that combining the global and local policies together is equivalent to treating them individually.

\begin{table*}[t]
	\centering
	\caption{Comparisons of the architectures searched/designed by different methods on ImageNet.
	``--'' means unavailable results.
	``\#Queries``, a widely used metric~\cite{luo2020seminas,yan2020does}, denotes the number of architecture-accuracy pairs queried from supernet or performance predictor during the search. A smaller ``\#Queries`` means the search algorithm is more efficient. Our \sexyname outperforms than most human designed and automatically searched architectures with less search cost and fewer search queries.}
	{
	\resizebox{\textwidth}{!}
	{
    \begin{tabular}{c|ccccccc}
    \topline
    \multicolumn{1}{c|}{\multirow{2}[0]{*}{Search Space}} &
    \multicolumn{1}{c}{\multirow{2}[0]{*}{Architecture}} &
    \multicolumn{2}{c}{Test Accuracy (\%)} &
    \multicolumn{1}{c}{\multirow{2}[0]{*}{\#MAdds (M)}} &
    \multicolumn{1}{c}{\multirow{2}[0]{*}{\#Queries (k)}} &
    \multicolumn{1}{c}{Search Time} \\
    \cline{3-4} &  & \multicolumn{1}{c}{Top-1} & \multicolumn{1}{c}{Top-5} &  & & (GPU days) \\
    \midline
    \multirow{4}*{--} & ResNet-18~\cite{resnet} & 69.8 & 89.1 & 1,814 &  -- & -- \\
& MobileNetV2 ()~\cite{sandler2018mobilenetv2} & 74.7 & --  & 585 & -- & --  \\
     & ShuffleNetV2 ()~\cite{ma2018shufflenet} & 73.7 & -- & 524 & -- & --  \\
    \midline
    \multirow{2}*{NASNet} & NASNet-A~\cite{zoph2018learning} & 74.0 & 91.6 & 564 & 20  & 1800\\
     & AmoebaNet-A~\cite{real2019regularized} & 74.5 & 92.0 & 555 & 20  & 3150\\
\midline
    \multirow{4}*{DARTS} & DARTS~\cite{liu2018darts} & 73.1 & 91.0 & 595 &19.5  & 4\\
     & P-DARTS~\cite{chen2019progressive} & 75.6 & 92.6 & 577 &11.7  & 0.3\\
& CNAS~\cite{guo2020breaking} & 75.4 & 92.6 & 576 & 100 & 0.3\\
     & AlphaX~\cite{wang2019alphax} & 75.5 & 92.2 & 579 & -- & 12\\
    \midline
    \multirow{13}*{MobileNet-like} & MobileNetV3-Large~\cite{howard2019searching} & 75.2 & -- & 219  & -- & --\\
     & FBNet-C~\cite{wu2019fbnet} & 74.9 & --  & 375  &11.5 & 9\\
     & AtomNAS~\cite{Mei2020AtomNAS} & 75.9 & 92.0  & 367 & 78  & --\\
& ProxylessNAS~\cite{cai2018proxylessnas} & 75.1 & 92.3  & 465 & -- & 8.3\\
& FairNAS~\cite{chu2021fairnas} & 77.5 & 93.7 & 392 & 11.2 & 12 \\
    & DNA~\cite{li2020block}& 78.4 & 94.0 & 611 & -- & 32.6 \\
    & FBNetV2~\cite{FBNETV2} & 77.2 & --  & 325 & 11.5 & 25\\
& EfficientNet-B1~\cite{EfficientNet}& 79.2 & 94.5 & 734 & -- & --\\
    & OFA-Large~\cite{Cai2020Once} & 80.0 & 94.9  & 595 & 20 & 51.7\\
    & Cream-L~\cite{peng2020cream} & 80.0 & 94.7 & 604 & -- & 12\\
    & NEAS-L~\cite{chen2021oneshot} & 80.0 & 94.8 & 574 & -- & 13 \\
    & \sexyname (Ours) & 79.9 & 94.8 & 597 & \textbf{10} & \textbf{0.8}\\
    & \sexynameplus (Ours) & \textbf{80.2} & \textbf{94.9} & 582 & \textbf{10} & 51.7+0.8\\
    \bottomline
    \end{tabular}
    }
    }
	\label{tab:imagenet}
\end{table*}

\textbf{Comparisons with State-of-the-art Methods.}
We compare our method with two baselines, namely \textit{Random Search} and \textit{REINFORCE}~\cite{williams1992simple}. 
Random Search baseline randomly samples architectures and selects one with the highest accuracy among them as the final derived architecture.
REINFORCE baseline performs searching by directly maximizing the expectation of the performance of sampling architectures with reinforcement learning.
We report the average test accuracy on three datasets over 500 runs with different seeds.
In this experiment, we initialize the subspace graph with randomly sampled centered architectures.
We also conduct experiments using the subspace graph initialized with searched well-designed architectures, which achieves better search performance.

From Table~\ref{tab:nasbench_search}, compared with the baselines, our \sexyname achieves the highest average accuracy on three datasets, \ie, CIFAR-10, CIFAR-100 and ImageNet-16-120.
Compared with REINFORCE~\cite{williams1992simple} baseline, the proposed \sexyname yields better search accuracy and lower variance (\eg, 72.760.80\% \vs 71.711.09\% on CIFAR-100).
The reason is our \sexyname searches by focusing on the small but effective subspace, which reduces the search difficulty resulting from the large search space.
During the search, our \sexyname updates the candidate subspaces with the locally searched architectures and finds better architectures in the constantly improved subspaces.
Besides the superior performance, we also highlight that the search time cost of our \sexyname is 25k seconds, which is much more efficient than most state-of-the-art NAS methods, such as DARTS~\cite{liu2018darts}, SETN~\cite{dong2019oneshot} and GDAS~\cite{dong2019searching}.
These results demonstrate the superiority of our proposed \sexyname over the considered methods.

\textbf{Transferability of Subspace Graph to New Datasets.}
When we search on a new target dataset, we often have to search from scratch (\ie, initializing candidate subspaces with randomly sampled architectures).
However, we may have found some promising architectures on existing datasets.
In this case, we can introduce a new variant of our method, called \sexynameplus, which may enhance the search performance on the target dataset by transferring the subspaces of the subspace graph found in the existing datasets.
To verify this, we conduct experiments on NAS-Bench-201 by considering CIFAR-10 as the existing dataset and CIFAR-100 as well as ImageNet-16-120 as two new target datasets.

From the results in Table~\ref{tab:nasbench_search}, the \sexynameplus with the subspaces transferred from CIFAR-10 achieves higher accuracy and lower variance than the \sexyname without that (73.12{0.61} \vs~72.76{0.80} on CIFAR-100, 46.66{0.52} \vs~46.13{0.67} on ImageNet-16-120).
The reason is that the well-designed architectures on CIFAR-10 may also have high performance on other datasets (\eg, CIFAR-100 and ImageNet-16-120).
In this sense, subspaces searched in CIFAR-10 provide a good initialization when searching in a new dataset.
These results show the transferability of the candidate subspaces between two different datasets and the potential of our method when applied to new datasets.


\subsection{Performance Comparisons on ImageNet}

\textbf{Search Space}.
In this part, we further evaluate our method on another benchmark search space, \ie, MobileNet-like search space~\cite{howard2019searching}.
The candidate architecture consists of 5 different units and each of them has consecutive layers.
We search for MBConv in each layer with kernel sizes selected from , expansion rates selected from  and the number of layers in each unit selected from .
Following~\cite{wu2019fbnet}, we randomly choose 10\% classes from the original dataset as the training set to train the supernet.
We measure the validation accuracy of sub-networks on 1000 validation images sampled from the training set.
We train the supernet with a progressive shrinking strategy~\cite{Cai2020Once} for 90 epochs.
To compute the performance improvement  with the validation accuracy, we train a predictor to predict the validation accuracy following~\cite{Cai2020Once}. 



\textbf{Implementation Details.}
Following~\cite{pham2018efficient}, we train the \sexyname for 10k iterations with a batch size of 1.
We use an Adam optimizer with a learning rate of .
To encourage the exploration of the \sexyname, we add an entropy regularizer to the reward weighted by .
We set the number of candidates subspaces  to 10 and the local search distance  to 3.
We report the search cost based on NVIDIA Tesla V100 GPU.
Following the \textit{mobile setting}~\cite{liu2018darts}, we constraint the number of multiply-adds (\#MAdds) of the searched architecture to be less than 600M.
To achieve this, we update the candidate subspace (See line 12 in Algorithm~1) with the architecture which has \#MAdds less than 600M.
To accelerate model evaluation, following~\cite{Cai2020Once,lu2020neural}, we first obtain the parameters from the full network of OFA and then finetune them for 75 epochs. 
We perform data augmentations including horizontally flipping, random crops, color jittering, and AutoAugment.
We use an SGD optimizer with a learning rate of 0.012.
The learning rate decay follows the cosine annealing strategy with a minimum of 0.001.




\textbf{Comparisons with State-of-the-art Methods.}
To investigate the effectiveness of the proposed method, we apply our method to MobileNet-like search space as two variants:
1) \textbf{\sexyname} searches based on the subspace graph initialized with a set of randomly sampled architectures, which is suitable in the scenario without any available well-designed architectures.
2) \textbf{\sexynameplus} adopts the subspace graph which is initialized with a set of existing well-designed architectures searched by OFA~\cite{Cai2020Once}.

As shown in Table~\ref{tab:imagenet}, under the mobile setting, the architecture searched by \sexyname reaches 79.9\% top-1 accuracy and 94.8\% top-5 accuracy, which outperforms not only the manual designed architectures but also most automatic searched ones.
Specifically, \sexyname outperforms the best manually designed architecture (\ie, MobileNetV2) by 5.2\% (\ie, 79.9\% \vs~74.7\%).
Compared with the state-of-the-art NAS method (\eg, OFA and Cream-L), \sexyname also achieves competitive performance (\ie, 79.9\% \vs~80.0\%) with less number of queries and search cost (only 0.8 GPU days).
Here, the lower costs mainly benefit from two aspects: 1) accelerating the search policy learning by reducing the number of queries (ours 10k \vs~OFA 20k); 2) accelerating the supernet training by using a proxy dataset (ImageNet-100) and early stop (training 90 epochs).
These results show the effectiveness and efficiency of the proposed \sexyname.

Compared with \sexyname, our \sexynameplus further improves the top-1 accuracy on ImageNet from 79.9\% to 80.2\%.
Note that our \sexynameplus outperform all of the considered manually-designed and automatically searched architectures.
The reason is that \sexyname searches for the promising subspace from randomly initialized candidates and needs to improve the candidate subspaces gradually.
Instead, \sexynameplus directly employs the promising subspaces centered on a set of well-designed architectures at the beginning of the search.
In this case, the architectures in such initialized subspaces have better performance than that in randomly initialized subspaces, which accelerates the search process by providing good subspace initialization centered on the good architectures.
The results demonstrate that we are able to apply our method to existing designed architectures/subspaces to further enhance the search performance.

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{imgs/ASLNAS.pdf}
\caption{
    The architecture searched by \sexyname in MobileNet-like search space.
}
\label{fig:arch_visualization}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{imgs/ASLNAS-plus.pdf}
\caption{
    The architecture searched by \sexynameplus in MobileNet-like search space.
}
\label{fig:arch_visualization_plus}
\end{figure*}

\textbf{Visualization of Searched Architectures}.
We show the visualization results of \sexyname (top-1 accuracy 79.9\%) and \sexynameplus (top-1 accuracy 80.2\%) in MobileNet-like search space in Figure~\ref{fig:arch_visualization} and Figure~\ref{fig:arch_visualization_plus}, respectively.
From the visualization results, we found that
the number of conv  and conv  is much larger than the number of conv .
The reason may have two aspects. 
First, conv  and conv  have a larger reception field and larger model capacity with more parameters than conv ~\cite{tan2019mixconv}. Second, larger conv operations may help the network preserve more information when doing downsampling (mentioned by ProxylessNAS~\cite{cai2018proxylessnas}).
In this case, architectures with conv  and conv  may often be beneficial to achieve better performance than that with conv .
Similar results of "large convs are dominated" can also be found in \cite{Cai2020Once,cai2018proxylessnas}.



\section{More Discussions and Ablations}

In this section, we perform more experiments to  demonstrate the effectiveness of the proposed architecture distance (Section~\ref{sec:ablation_arch_distance}), the proposed subspace updating scheme (Section~\ref{sec:ablation_subspace_updating}), the proposed performance improvement reward (Section~\ref{sec:ablation_perf_imp_reward}), and the subspace graph (Section~\ref{sec:ablation_subspace_graph}).
In addition, we conduct ablations to investigate the effect of the number of candidate subspaces  and the local search spaces in Sections~\ref{sec:ablation_on_K} and~\ref{sec:ablation_on_M}, respectively.

\subsection{More Discussions on Architecture Distance}\label{sec:ablation_arch_distance}

As mentioned before, our method is built upon an underlying hypothesis that the neighborhood around a good architecture is usually a promising subspace.
In other words, the architectures in the neighborhood/subspace are more likely to have good performance.
In this sense, once we find a promising subspace centered on a good architecture, it would be much easier to find better architectures via a local search.
To build such subspace around a given architecture, we devise an \textit{Architecture Distance}  to measure the distance between two architectures.
In the following, We provide empirical results in  NAS-Bench-201 and MobileNet-like search spaces to demonstrate that the devised distance function is able to support the hypothesis.
We conduct experiments in NAS-Bench-201 and MobileNet-like search space by computing the performance difference (measured in accuracy) between two architectures with different distances.

We show the results over 100 different trials in Figure~\ref{fig:acc_vs_dist}.
From the results, the average accuracy difference between two architectures becomes larger when their distance increases.
For example, in NAS-Bench-201 space, the accuracy difference increases from 4.49 to 6.69 when the distance grows from 1 to 2.
Meanwhile, the variance of the performance difference also increases as the corresponding distance grows (\eg, increasing from 6.1 to 9.5 when the distance grows from 1 to 2).
The results demonstrate that the rationality and effectiveness of the designed architecture distance, \ie, the architectures with smaller distances (in the same subspace) tend to have similar performance.
Thus, we are able to find promising architectures in a subspace around existing good architectures more easily than the overall search space.
Similar observations are also found in NAS-Bench-101~\cite{ying2019bench} search space.


\begin{figure*}[!t]
\centering
\subfloat[Comparisons in NAS-Bench-201 search space.]{\includegraphics[width=0.9\columnwidth]{imgs/acc_vs_dist_nasbench.pdf}\label{fig:acc_vs_dist_nasbench}}
\hfil
\subfloat[Comparisons in MobileNet-like search space.]{\includegraphics[width=0.9\columnwidth]{imgs/acc_vs_dist_mobilenet.pdf}\label{fig:acc_vs_dist_mobilenet}}
\caption{The performance difference that is measured by accuracy (\%) \vs~the architecture distance in NAS-Bench-201 search space (a) and MobileNet-like search space (b).}
\label{fig:acc_vs_dist}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{imgs/all_comparisons_subspace_updating.pdf}
\caption{
Comparisons of the search performance with/without subspace updating on NAS-Bench-201.
}
\label{fig:all_comparisons_subspace_updating}
\end{figure*}


\subsection{Effect of Subspace Updating Scheme}
\label{sec:ablation_subspace_updating}

In the global search, if we keep all candidate subspaces fixed, the controller may get stuck in a local optimum due to the very limited search spaces, which would lead to poor search performance.
To address this issue, we propose a simple strategy to gradually update the candidate subspaces with the locally searched architectures.
To be specific, we replace the center architecture  in the evoked subspace with locally searched architecture  if  has better performance than .
This simple updating strategy ensures the candidate subspaces would become more and more promising, which helps to find good architecture within the gradually improved subspaces during the local search.

To verify the effectiveness of the proposed subspace updating scheme, we conduct more experiments on NAS-Bench-201 and compare our \sexyname with three baselines and variants, namely \textit{Random Search in Overall Search Space}, \textit{Random Search in Evoked Search Space} and \textit{\sexyname without Subspace Update}.
The first two baselines conduct a random search in the entire search space and the evoked subspace in the final search step of our method, respectively.
The variant \textit{\sexyname without Subspace Update} uses the same settings as our method but performs a search without updating the candidate subspaces.

From the results in Figure~\ref{fig:all_comparisons_subspace_updating}, random searching in evoked subspace (orange bar) has higher average accuracy and lower variances than that in overall search space (purple bar), \ie, 91.891.53 \vs~87.379.71, which demonstrate the effectiveness of the proposed subspace updating scheme.
Moreover, our \sexyname with subspace (red bar) updating outperforms \sexyname without that (blue bar) on three considered datasets. The results indicate that our \sexyname constantly finds promising subspaces during the search, which also shows the effectiveness of the subspace updating scheme.
In addition, compared with the baseline that searches randomly in the evoked subspace, our \sexyname method consistently achieves better searched accuracy. The results show that the local search scheme is able to further enhance the search performance by finding promising architectures within the evoked subspace.


\subsection{Effect of the Performance Improvement Reward}\label{sec:ablation_perf_imp_reward}

In the policy learning, we use the performance improvement between the resultant  and the center architecture  in  as the reward.
Besides this, one can learn the policy by considering the performance of  as the reward.
However, the policy may easily get stuck in a local optimum and always select the same subspace with the highest performing center architecture.
To verify this, we conduct more experiments in the MobileNet-like search space on ImageNet to compare the search performance with these two kinds of reward functions.

We show the averaged validation accuracy (obtained by the supernet) over 10 different runs in Figure~\ref{fig:comparisons_with_rewards_subgraph}(a).
The variant using the performance improvement (red line) achieves higher search performance than that using the performance of resultant architecture (blue line) (67.810.18 \vs~67.630.31).
In addition, maximizing the performance improvement finds more new subspaces than using the absolute performance during the search process (319 \vs~284).
The reason is that if we directly maximize the performance of the resultant architecture , the search algorithm may always select the same subspace with the best architecture.
In this case, the remaining subspaces are ignored, which results in poor explorations during search.
In contrast, using the performance improvement encourages explorations among different subspaces.


\subsection{Effect of Subspace Graph}\label{sec:ablation_subspace_graph}

We build the subspace graph with a set of candidate subspaces.
In the graph, nodes denote candidate subspaces and direct edges denote the relationships among them.
In practice, we represent direct edges as a modification vector that how to modify the center architecture in the weak subspace to that in the better subspace.
These edges take helpful information for the local search since they are good examples to represent how to modify an architecture to a better one for the local policy.
In addition, the architectures in different subspaces may have different computational operations/topologies and different performances.
In this case, the graph structure in the subspace graph may convey beneficial information to select a promising subspace in the global search.

We investigate the effect subspace graph by performing more experiments in MobileNet-like search space with/without the subspace graph structure.
Specifically, for the variant without the subspace graph structure, we treat the candidate subspaces as separate points and extract the features from them using two fully-connected layers instead of the two-layer graph neural network.
We show the results in Figure~\ref{fig:comparisons_with_rewards_subgraph}(b).
We report the averaged validation accuracy (obtained by the supernet) over 10 different runs.
From the results, \sexyname with subspace graph (red line) has not only higher validation accuracy but also lower variance than \sexyname without that (blue line) (67.810.18 \vs~67.120.46).
The results demonstrate the effectiveness of the subspace graph.

\begin{figure*}[!t]
\centering
\subfloat[Comparisons with different reward functions.]{\includegraphics[width=0.9\columnwidth]{imgs/comparisons_with_rewards.pdf}\label{fig:comparisons_with_rewards}}
\hfil
\subfloat[Comparisons with/without subspace graph.]{\includegraphics[width=0.9\columnwidth]{imgs/comparisons_with_subgraph.pdf}\label{fig:comparisons_with_subgraph}}
\caption{Comparisons of the search performance with different reward functions(a) and with/without subspace graph(b) in MobileNet-like search space on ImageNet.}
\label{fig:comparisons_with_rewards_subgraph}
\end{figure*}

\begin{figure*}[!t]
\centering
\subfloat[Ablation on \#candidate subspaces .]{\includegraphics[width=0.9\columnwidth]{imgs/comparisons_hyperparameter_K.pdf}\label{fig:comparisons_hyperparameter_K}}
\hfil
\subfloat[Ablation on the local search distance .]{\includegraphics[width=0.9\columnwidth]{imgs/comparisons_hyperparameter_M.pdf}\label{fig:comparisons_hyperparameter_M}}
\caption{Comparisons of the search performance with the different numbers of candidate subspaces  (a) and the different local search distance  (b) on ImageNet.}
\label{fig:comparisons_hyperparameter}
\end{figure*}


\subsection{Effect of the Number of Candidate Subspaces }\label{sec:ablation_on_K}

We build the subspace graph with  architectures and thus have  candidate subspaces .
When we consider a small , the information carried by the subspaces would be limited, resulting in poor search performance.
In contrast, a larger  means more explorations in candidate subspaces.
Nevertheless, too many candidate subspaces would introduce a heavy computational burden since the computational cost of the GNN increase squarely as the  becomes larger.
To investigate the effect of , we conduct an ablation study with different  on ImageNet.
For fair comparisons, we set the number of each subspace updates in the graph to the same.

In Figure~\ref{fig:comparisons_hyperparameter}(a), our \sexyname achieves the worst validation accuracy when  since it only explores a single subspace during the search process, which greatly depends on the initialized architecture.
As  becomes larger, our \sexyname achieves better validation accuracy.
The reason is that more architectures benefit the search by exploring more diverse promising subspaces.
In this case, our \sexyname has a larger probability to find promising architectures.
Besides, when  is larger than 10, our \sexyname yields very similar search performance.
These results demonstrate that using 10 different subspaces is sufficient to achieve competitive performance.
Thus, we set  to 10 on ImageNet.


\subsection{Effect of the Local Search Distance }\label{sec:ablation_on_M}

When performing the local search in , we use a hyper-parameter  to restrict the size of the local search subspace.
A smaller search distance  means that we perform the local search in a smaller subspace.
Note that searching in small but effective subspaces is exactly our core idea to enhance both search performance and efficiency.
In contrast, a larger  enables \sexyname to explore more architectures in the search space but makes it harder to explore the whole search space.
Note that the maximum of  equals to the number of components  in the architecture, \ie, .
To investigate the effect of , we conduct experiments with more different search distance  in MobileNet-like search space ( in this space). 


In Figure~\ref{fig:comparisons_hyperparameter}(b), our \sexyname achieves the best validation accuracy when  and the worst validation accuracy when . When  is too small (\eg, ), it is easy to fall into the local optimum and hard to find better architectures in the subspace, resulting in poor search results.
When  becomes larger (\eg, ), the large search space lowers the search efficiency and makes it difficult to find good architectures.
In this case, the search performance drops greatly (\eg, only 63.84\% when ).
Thus, we set the search distance  to 3 in the MobileNet-like search space.


\section{Conclusion}

In this paper, we have proposed a Neural Architecture Search method via Automatic Subspace Evoking (\sexyname), which focuses on automatically searched small and effective subspaces and conduct search in them.
Specifically, we first perform global search for a promising subspace from the candidate subspaces.
Then, we perform local search for effective architectures in the globally searched subspace instead of the original large one.
Finally, we update the candidate subspace with the locally searched architecture.
Moreover, our \sexyname is able to further enhance search performance by taking well designed/searched architectures as the prior knowledge.
Extensive experiments demonstrate the superiority of our method over the considered methods.

{
\appendix[More Details on Graph Neural Network]
In the global search, we employ a two-layer graph neural network (GNN)~\cite{you2020handling} to extract the features of the subspace graph.
Specifically, we first provide more details about how to represent the subspace graph as a set of node embeddings and edge embeddings.
Since the node/subspace  is uniquely determined by its centered architecture , we use the embedding of the architecture  to represent the subspace .
Specifically, for the architecture , we adopt the concatenation of the learnable vector of each item  in it to represent the embedding .
For each edge, we represent its embedding  by .
Since the edge in the subspace graph implies how to modify an architecture to obtain another, \eg, replacing convolution with max pooling in some layer.
Given two center architectures, if the components are the same in some positions, the corresponding part of edge features will be zero.
In this case, the edges explicitly capture the information on how to modify one to another.

To extract the features from the subspace graph, we extend GraphSAGE~\cite{hamilton2017graphsage} by introducing \emph{edge embeddings} following~\cite{you2020handling}.
Specifically, at each GNN layer , the message passing function takes the concatenation of the node embedding and the edge embeddings in the previous layer as the input:

where  is the mean pooling function for aggregation,  is the non-linearity function (\eg, the Rectified Linear Unit (ReLU)~\cite{nair2010rectified} ),  is the trainable weight,  is the node neighborhood function.
Note that the edge embedding would be a zero vector if the corresponding edge does not exist.
Then, we update the node embedding  by:

where  is the trainable weight.
In addition, we update the edge embedding  by:

where  is the trainable weight.
With the extracted node features , we feed them into the LSTM model that samples decisions via softmax classifiers to perform a global search for the promising subspace .
 }

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,egbib}
	
\end{document}
