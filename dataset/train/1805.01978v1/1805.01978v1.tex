

\section{Experiments}
\label{exp}

We conduct 4 sets of experiments to evaluate our  approach.
The first set is on CIFAR-10 to compare 
our non-parametric softmax with parametric softmax.
The second set is on ImageNet to compare
our method with other unsupervised learning methods.
The last two sets of experiments investigate two different tasks, semi-supervised
learning and object detection, to show the generalization ability of our learned feature representation.

\begin{table}[t]
	\setlength{\tabcolsep}{1.8pt}
	\centering
	\begin{tabular}{c|c|c}
		\Xhline{3\arrayrulewidth}
		Training / Testing & Linear SVM & Nearest Neighbor \\
		\Xhline{3\arrayrulewidth}
		Param Softmax & 60.3 & 63.0 \\
		\Xhline{3\arrayrulewidth}
Non-Param Softmax & 75.4 & \textbf{80.8} \\
		\Xhline{3\arrayrulewidth}
		NCE  & 44.3 & 42.5\\ \hline
		NCE  & 60.2 & 63.4\\ \hline
		NCE  & 64.3   & 78.4\\ \hline
		NCE  & 70.2  & \textbf{80.4}\\
		\Xhline{3\arrayrulewidth}
	\end{tabular}
	\caption{\small
	Top-1 accuracy on CIFAR10, by applying 
	linear SVM or kNN classifiers
		on the learned features.
	Our non-parametric softmax outperforms parametric softmax, and NCE provides close approximation as  increases.
	}
\label{exp:ablative}
\vspace{-3pt}
\end{table}

\subsection{Parametric vs. Non-parametric Softmax}
\label{p:np}


A key novelty of our approach is the non-parametric softmax function.
Compared to the conventional parametric softmax,
our softmax allows a non-parametric metric to transfer to supervised tasks.

We compare both parametric and non-parametric formulations on
CIFAR-10~\cite{krizhevsky2009learning}, a dataset with
 training instances in  classes. This size allows us to compute the non-parametric softmax in Eq.\eqref{eqn:np_softmax} without any
approximation. We use ResNet18 as the backbone network and its output features mapped into -dimensional vectors.

We evaluate the classification effectiveness based on the learned feature representation.
A common practice~\cite{zhang2017split, doersch2015unsupervised, pathak2016context}
is to train an SVM on the learned feature over the training set, and 
to then classify test instances based on the feature extracted from the trained network.
In addition, we also use nearest neighbor classifiers to
assess the learned feature. The latter directly relies on the feature metric and
may better reflect the quality of the representation.

Table~\ref{exp:ablative} shows top-1 classification
accuracy on CIFAR10. On the features learned with parametric softmax, we obtain
accuracy of  and  with linear SVM and kNN classifiers respectively.  On the features learned with
non-parametric softmax, the accuracy 
rises to   and  for the linear and nearest neighbour classifiers, a remarkable
 boost for the latter.

We also study the quality of NCE approximating non-parametric softmax (Sec.~\ref{sub:nce}).
The approximation is controlled by ,
the number of negatives drawn for each instance.
With , the accuracy with kNN
drops significantly to . As  increases, the performance improves
steadily. When , the accuracy approaches that at  -- full form evaluation without any approximation.  This result provides assurance that NCE is an efficient approximation.


\subsection{Image Classification}

We learn a feature representation on ImageNet ILSVRC~\cite{russakovsky2015imagenet},
and compare our method with representative unsupervised learning methods.

\vspace{-9pt}
\paragraph{Experimental Settings.}
We choose design parameters via empirical validation.
In particular, we set temperature  and use NCE with  to balance performance and computing cost.
The model is trained for  epochs using
SGD with momentum. The batch size is 256.
The learning rate is initialized to ,
scaled down with coefficient  every 40 epochs after the first  epochs.
Our code is available at: \url{http://github.com/zhirongw/lemniscate.pytorch}.


\vspace{-9pt}
\paragraph{Comparisons.}
We compare our method
with a randomly initialized network (as a lower bound) and
various unsupervised learning methods, including
self-supervised learning~\cite{doersch2015unsupervised,zhang2016colorful,noroozi2016unsupervised,zhang2017split},
adversarial learning~\cite{donahue2016adversarial}, and
Exemplar CNN~\cite{doersch2017multi}.
The split-brain autoencoder~\cite{zhang2017split} serves a strong baseline that represents the state of the art.
The results of these methods are reported with
AlexNet architecture ~\cite{krizhevsky2012imagenet} in their original papers,
except for exemplar CNN~\cite{dosovitskiy2014discriminative},
whose results are reported with ResNet-101~\cite{doersch2017multi}.
As the network architecture has a big impact on the performance,
we consider a few typical architectures:
AlexNet~\cite{krizhevsky2012imagenet},
VGG16~\cite{simonyan2014very},
ResNet-18,
and ResNet-50~\cite{he2015deep}.



\begin{table}[tp]
\setlength{\tabcolsep}{1.1pt}
\centering
\begin{tabular}{c|ccccc|c|c}
\Xhline{2\arrayrulewidth}
  \multicolumn{8}{c}{Image Classification Accuracy on ImageNet} \\
\Xhline{2\arrayrulewidth}
method  & conv1 & conv2 & conv3 & conv4 & conv5 & kNN & \#dim \\
\hline
Random & 11.6 & 17.1 & 16.9 & 16.3 & 14.1 & 3.5 & 10K \\
Data-Init~\cite{krahenbuhl2015data} & 17.5 & 23.0 & 24.5 & 23.2 & 20.6 & - & 10K\\
Context~\cite{doersch2015unsupervised} & 16.2 & 23.3 & 30.2 & 31.7 & 29.6 & - & 10K\\
Adversarial~\cite{donahue2016adversarial} & 17.7 & 24.5 & 31.0 & 29.9 & 28.0 & - & 10K\\
Color~\cite{zhang2016colorful} & 13.1 & 24.8 & 31.0 & 32.6 & 31.8 & - & 10K\\
Jigsaw~\cite{noroozi2016unsupervised} & 19.2 & 30.1 & 34.7 & 33.9 & 28.3 & - & 10K\\
Count~\cite{noroozi2017representation} & 18.0 & 30.6 & 34.3 & 32.5 & 25.7 & - & 10K \\
SplitBrain~\cite{zhang2017split} & 17.7 & 29.3 & 35.4 & 35.2 & 32.8 & 11.8 & 10K\\
\Xhline{2\arrayrulewidth}
Exemplar\cite{doersch2017multi} & & & 31.5 & & & - & 4.5K \\
\Xhline{2\arrayrulewidth}
Ours Alexnet & 16.8 & 26.5 & 31.8 & 34.1 & \textbf{35.6} & 31.3 & 128 \\
Ours VGG16 & 16.5 & 21.4 & 27.6 & 35.1 & \textbf{39.2} & 33.9 & 128 \\
Ours Resnet18 & 16.0 & 19.9 & 29.8 & 39.0 & \textbf{44.5} & \textbf{41.0} & 128\\
Ours Resnet50 & 15.3 & 18.8 & 24.9 & 40.6 & \textbf{54.0} & \textbf{46.5} & 128\\
\Xhline{2\arrayrulewidth}
\end{tabular}
\caption{\small
    Top-1 classification accuracy on ImageNet.
}
\label{exp:cls_imagenet}
\end{table}

\begin{table}[t]
	\setlength{\tabcolsep}{1.1pt}
	\centering
	\begin{tabular}{c|ccccc|c|c}
		\Xhline{2\arrayrulewidth}
		\multicolumn{8}{c}{Image Classification Accuracy on Places} \\
		\Xhline{2\arrayrulewidth}
		method & conv1 & conv2 & conv3 & conv4 & conv5 & kNN & \#dim  \\
		\hline
Random &  15.7 & 20.3 & 19.8 & 19.1 & 17.5 & 3.9 & 10K \\
Data-Init~\cite{krahenbuhl2015data} &  21.4 & 26.2 & 27.1 & 26.1 & 24.0 & - & 10K \\
Context~\cite{doersch2015unsupervised} & 19.7 & 26.7 & 31.9 & 32.7 & 30.9 & - & 10K \\
		Adversarial~\cite{donahue2016adversarial} &  17.7 & 24.5 & 31.0 & 29.9 & 28.0 & - & 10K \\
		Video~\cite{wang2015unsupervised} & 20.1 & 28.5 & 29.9 & 29.7 & 27.9 & - & 10K \\
		Color~\cite{zhang2016colorful} & 22.0 & 28.7 & 31.8 & 31.3 & 29.7 & - & 10K \\
		Jigsaw~\cite{noroozi2016unsupervised} & 23.0 & 32.1 & 35.5 & 34.8 & 31.3 & - & 10K\\
		SplitBrain~\cite{zhang2017split} & 21.3 & 30.7 & 34.0 & 34.1 & 32.5 & 10.8 & 10K \\
		\Xhline{2\arrayrulewidth}
		Ours Alexnet & 18.8 & 24.3 & 31.9 & \textbf{34.5} & 33.6 & 30.1 & 128 \\
		Ours VGG16 & 17.6 & 23.1 &  29.5 & 33.8 & \textbf{36.3} & 32.8 & 128 \\
		Ours Resnet18 & 17.8 & 23.0 & 30.1 & 37.0 & \textbf{38.1} & \textbf{38.6} & 128\\
		Ours Resnet50 & 18.1 & 22.3 & 29.7 & 42.1 & \textbf{45.5} & \textbf{41.6} & 128\\
		\Xhline{2\arrayrulewidth}
	\end{tabular}
	\caption{\small
        Top-1 classification accuracy on Places,  based directly on features learned on ImageNet, without any fine-tuning.
    }
\label{exp:cls_places}
\end{table}
 


We evaluate the performance with two different protocols:
(1) Perform linear SVM on the intermediate features from
\texttt{conv1} to \texttt{conv5}. Note that 
there are also corresponding layers in VGG16 and ResNet~\cite{simonyan2014very,he2015deep}.
(2) Perform kNN on the output features.
Table~\ref{exp:cls_imagenet} shows that:

\vspace{-7pt}
\begin{enumerate}[leftmargin=*,labelindent=0pt,itemsep=0pt]
\item With AlexNet and linear classification on intermediate features,
our method achieves an accuracy of ,
outperforming all baselines, including the state-of-the-art.
Our method can readily scale up to deeper networks. As we move from
AlexNet to ResNet-50, our accuracy is raised to ,
whereas the accuracy with
exemplar CNN~\cite{doersch2017multi} 
is only  even with ResNet-101.

\begin{figure}[t]
	\centering
	\includegraphics[height=140pt]{figures/consistency_new.pdf}
	\caption{\small
		Our kNN testing accuracy on ImageNet continues to improve as the training loss decreases, demonstrating 
		that our unsupervised learning objective captures apparent similarity which aligns well with the semantic annotation of the data.
	}
	\label{fig:consistency}
\vspace{-6pt}
\end{figure}

\item Using nearest neighbor classification on the final 128 dimensional features, our method achieves , ,  and  accuracies
with AlexNet, VGG16, ResNet-18 and ResNet-50,
not much lower than the linear classification results, demonstrating that our learned feature induces a reasonably good metric.
As a comparison, for Split-brain, the accuracy drops to  with nearest neighbor classification on \texttt{conv3} features,
and to  after projecting the features to 128 dimensions.

\item With our method, the performance gradually increases
as we examine the learned feature representation from earlier to later layers, which is generally desirable.
With all other methods,
the performance decreases beyond \texttt{conv3} or \texttt{conv4}.

\item It is important to note that the features from
intermediate convolutional layers can be over  dimensions.
Hence, for other methods, using the features from the \emph{best-performing}
layers can incur significant storage and computation costs.
Our method produces a -dimensional representation at the last layer,
which is very efficient to work with. The encoded features of all 
images in ImageNet only take about  MB of storage.
Exhaustive nearest neighbor search over this dataset only takes  ms per image on a Titan X GPU.
\end{enumerate}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\linewidth]{figures/retrieval_new.pdf}
	\caption{\small
		Retrieval results for example queries.
		The left column are queries from the validation set, while
		the right columns show the  closest instances from the training set.
		The upper half shows the best cases. The lower half shows the worst cases.
	}
	\label{fig:retrieval}
\vspace{-3pt}
\end{figure*}

\vspace{-18pt}
\paragraph{Feature generalization.}
We also study how the learned feature representations can
generalize to other datasets. With the same settings, we conduct another
large-scale experiment on \emph{Places}~\cite{zhou2014learning}, a large
dataset for scene classification, which contains  training images
in  categories.
In this experiment, we directly use the feature extraction networks trained
on ImageNet \emph{without finetuning}. Table~\ref{exp:cls_places} compares
the results obtained with different methods and under different evaluation policies. Again, with linear classifier on \texttt{conv5} features,
our method achieves competitive performance of
top-1 accuracy  with AlexNet, and 
  with ResNet-50.
With nearest neighbors on the last layer which is much smaller than intermediate layers, we achieve an accuracy of  with ResNet-50. These results show
remarkable generalization ability of the representations learned using our method.

\vspace{-9pt}
\paragraph{Consistency of training and testing objectives.}
Unsupervised feature learning is difficult because the training objective is agnostic about the testing objective. 
A good training objective should be reflected in consistent improvement in the 
testing performance.  We investigate the relation between the training loss
and the testing accuracy across iterations.
Fig.~\ref{fig:consistency} shows that our testing accuracy continues to improve 
as training proceeds, with no sign of overfitting.
It also suggests that better optimization of the training objective
may further improve our testing accuracy.

\vspace{-9pt}
\paragraph{The embedding feature size.}
We study how the performance changes as  we vary the embedding size from 32 to 256.
Table~\ref{table:feat_size} shows that the performance increases from 32, plateaus at 128, and appears to saturate towards 256.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c|c|c}
\Xhline{2\arrayrulewidth}
embedding size &  &  &  &  \\
\Xhline{2\arrayrulewidth}
top-1 accuracy & 34.0 & 38.8 & 41.0 & 40.1\\
\Xhline{2\arrayrulewidth}
\end{tabular}
\caption{\small
Classification performance on ImageNet with ResNet18 for different embedding feature sizes.
}
\label{table:feat_size}
\vspace{-9pt}
\end{table}

\vspace{-9pt}
\paragraph{Training set size.}
To study how our method scales with the data size,
we train different representations with various proportions of ImageNet data,
and evaluate the classification performance on
the full labeled set using nearest neighbors.
Table~\ref{table:data_scale} shows that our feature learning method benefits from larger
training sets, and the testing accuracy improves as the training set grows.
This property is crucial for successful unsupervised learning, as there is no shortage of unlabeled data in the wild.

\begin{table}[t]
	\centering
	\begin{tabular}{c|c |c|c|c|c}
		\Xhline{2\arrayrulewidth}
		training set size &  &  &  &  &  \\
		\Xhline{2\arrayrulewidth}
		accuracy & 3.9 & 10.7 & 23.1 &  31.7 & 41.0 \\
		\Xhline{2\arrayrulewidth}
	\end{tabular}
	\caption{\small
		Classification performances
		trained on different amount of training set
with ResNet-18.
	}
	\label{table:data_scale}
\vspace{-9pt}
\end{table}

\vspace{-11pt}
\paragraph{Qualitative case study.}
To illustrate the learned features,
Figure~\ref{fig:retrieval} shows the results of image retrieval using the learned
features.
The upper four rows show the best cases where all top 10 results
are in the same categories as the queries.
The lower four rows show the worst cases where none of the top 10
are in the same categories. However, even for the failure cases, the retrieved images are still visually similar to the queries, a testament to the power of our unsupervised learning objective.

\subsection{Semi-supervised Learning}

We now study how the learned feature extraction network can benefit other tasks, and whether it can provide a good basis for transfer learning to other tasks.
A common scenario that can benefit from unsupervised learning is when we have a large amount of data of which only a small fraction are labeled.
A natural semi-supervised learning approach is to first learn from the big unlabeled data and then fine-tune the model
on the small labeled data.

We randomly choose a subset of ImageNet as
labeled and treat others as unlabeled.
We perform the above semi-supervised learning and measure the classification accuracy on the validation set.
In order to compare with \cite{larsson2017colorization}, we report the top-5 accuracy here.

We compare our method with three baselines:
(1) \emph{Scratch}, \ie~fully supervised training on the small labeled subsets,
(2) \emph{Split-brain}~\cite{zhang2017split} for pre-training,
and (3) \emph{Colorization}~\cite{larsson2017colorization} for pre-training.
Finetuning on the labeled subset takes  epochs with initial learning
rate  and a decay rate of 10 every 30 epochs.
We vary the proportion of labeled subset 
from  to  of the entire dataset.

Fig.~\ref{fig:semi} shows
that our method significantly outperforms all other approaches, and ours is the only one outperforming supervised learning from limited labeled data.
When only  of data is labeled, we outperform by a large  margin,
demonstrating that our feature learned
from unlabeled data is effective for task adaptation.



\begin{figure}[t]
	\centering
	\includegraphics[height=180pt]{figures/semi_accuracy.pdf}
	\caption{\small
		Semi-supervised learning results on ImageNet with an increasing fraction of labeled data ( axis).  Ours are consistently and significantly better.
		Note that the results for colorization-based pretraining are from a deeper ResNet-152 network~\cite{larsson2017colorization}.
}
	\label{fig:semi}
\vspace{-6pt}
\end{figure}


\subsection{Object Detection}


To further assess the generalization capacity of the learned features, we
transfer the learned networks to the new task of object detection on PASCAL VOC 2007~\cite{everingham2010pascal}.
Training object detection model from scratch is often difficult, and a prevalent practice is to pretrain the underlying CNN on ImageNet and
fine-tune it for the detection task.

We experiment with
Fast R-CNN~\cite{girshick2015fast} with AlexNet and VGG16 architectures,
and Faster R-CNN~\cite{ren2015faster} with ResNet-50.
When fine-tuning Fast R-CNN, the learning rate is initialized to  and
scaled down by  times after every  iterations.
When fine-tuning AlexNet and VGG16, we follow the standard practice, fixing
the conv1 model weights.
When fine-tuning Faster R-CNN, we fix
the model weights below the 3rd type of residual blocks,
only updating the layers above and freezing all batch normalization layers.
We follow the standard pipeline for finetuning and do not use the rescaling method proposed in~\cite{doersch2015unsupervised}.
We use the standard trainval set in VOC 2007 for training and  testing.

We compare three settings:
1) directly training from scratch (lower bound),
2) pretraining on ImageNet in a supervised way (upper bound), and
3) pretraining on ImageNet or other data using various unsupervised methods.

Table~\ref{exp:det} lists detection performance in terms of mean average precision (mAP).
With AlexNet and VGG16, our method achieves an mAP of  and ,
on par with the state-of-the-art unsupervised methods.
With Resnet-50, our method achieves an mAP of ,
surpassing all existing unsupervised learning approaches.
It also shows that our method scales well as the network gets deeper.
There remains a significant gap of  to be narrowed towards mAP  from supervised pretraining.

\begin{table}[t]
\setlength{\tabcolsep}{1.8pt}
~~~~~~~
\begin{minipage}{0.3\linewidth}
\begin{tabular}[c]{c|c}
\Xhline{2\arrayrulewidth}
\multicolumn{1}{c|}{Method}                                          & mAP    \\ \hline
\multicolumn{1}{c|}{AlexNet Labels}                         &  \\
\multicolumn{1}{c|}{Gaussian}                                        &  \\
\multicolumn{1}{c|}{Data-Init~\cite{krahenbuhl2015data}}     &  \\
\multicolumn{1}{c|}{Context~\cite{doersch2015unsupervised}}   &  \\
\multicolumn{1}{c|}{Adversarial~\cite{donahue2016adversarial}}    &  \\
\multicolumn{1}{c|}{Color~\cite{zhang2016colorful}}           &  \\
\multicolumn{1}{c|}{Video~\cite{wang2015unsupervised}}         &  \\
\multicolumn{1}{c|}{Ours Alexnet}                             &    \\
\Xhline{2\arrayrulewidth}
\end{tabular}
\end{minipage}
~~~~~~~~~
\begin{minipage}{0.3\linewidth}
\begin{tabular}[c]{c|c}
\Xhline{2\arrayrulewidth}
\multicolumn{1}{c|}{Method}                                          & mAP    \\ \hline
\multicolumn{1}{c|}{VGG Labels}                         &  \\
\multicolumn{1}{c|}{Gaussian}                                        &  \\
\multicolumn{1}{c|}{Video~\cite{wang2015unsupervised}}     &  \\
\multicolumn{1}{c|}{Context~\cite{doersch2015unsupervised}}   &  \\
\multicolumn{1}{c|}{Transitivity~\cite{wang2017transitive}}   &  \\
\multicolumn{1}{c|}{Ours VGG}                                      &  \\
\hline
\hline
\multicolumn{1}{c|}{ResNet Labels} &  \\
\multicolumn{1}{c|}{Ours ResNet}                                            &  \\
\Xhline{2\arrayrulewidth}
\end{tabular}
\end{minipage}
\caption{Object detection performance on PASCAL VOC 2007 test,
	 in terms of mean average precision (mAP), for 
	supervised pretraining methods (marked by ),
 existing unsupervised methods, and our method.
}\label{exp:det}
\vspace{-9pt}
\end{table}
