\documentclass{article}
\usepackage[nonatbib,final]{nips_2016}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{bm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\bs}[1]{\boldsymbol{\mathbf{#1}}}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{color}
\usepackage{wrapfig}
\graphicspath{{gfx/},{gfx/supp/}}



\usepackage{tikz}
\newcommand{\dittotikz}{\tikz{
        \draw [line width=0.12ex] (-0.2ex,0) -- +(0,0.8ex)
            (0.2ex,0) -- +(0,0.8ex);
        \draw [line width=0.08ex] (-0.6ex,0.4ex) -- +(-1.5em,0)
            (0.6ex,0.4ex) -- +(1.5em,0);
    }}

\newcommand{\secc}[1]{\autoref{sec:#1}}
\renewcommand{\sectionautorefname}{Section~}
\renewcommand{\subsectionautorefname}{Section~}
\renewcommand{\subsubsectionautorefname}{Section~}
\renewcommand{\figureautorefname}{Figure}
\renewcommand{\tableautorefname}{Table~}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\etal}{~et al.~}

\title{Domain Separation Networks}
\author{
  Konstantinos Bousmalis\thanks{Authors contributed equally.} \\
  Google Brain\\
  Mountain View, CA \\
  \texttt{konstantinos@google.com} \\
\And
  George Trigeorgis\footnotemark[1] \hspace{1mm}\thanks{This work was completed while George Trigeorgis was at Google Brain in Mountain View, CA.} \\
  Imperial College London \\
  London, UK\\
  \texttt{g.trigeorgis@imperial.ac.uk} \\
  \And
  Nathan Silberman \\
  Google Research \\
  New York, NY \\
  \texttt{nsilberman@google.com} \\
  \And
  Dilip Krishnan \\
  Google Research \\
  Cambridge, MA \\
   \texttt{dilipkay@google.com} \\
   \And
  Dumitru Erhan \\
  Google Brain \\
  Mountain View, CA \\
   \texttt{dumitru@google.com} \\
}
\begin{document}

\maketitle
\vspace{-4mm}
\begin{abstract}
\vspace{-4mm}
The cost of large scale data collection and annotation often makes the
application of machine learning algorithms to new tasks or datasets
prohibitively expensive. One approach circumventing this cost is training
models on synthetic data where annotations are provided automatically.
Despite their appeal, such models often fail to
generalize from synthetic to real images, necessitating
domain adaptation algorithms to manipulate these models before they can
be successfully applied. Existing approaches focus either on mapping
representations from one domain to the other, or on learning to extract features
that are invariant to the domain from which they were extracted.
However, by focusing only on creating a mapping or shared representation
between the two domains, they ignore the individual characteristics of
each domain. We suggest that explicitly modeling what is unique to each domain
can improve a model's ability to extract domain--invariant features.
Inspired by work on private--shared component analysis, we explicitly
learn to extract image representations that are partitioned into two
subspaces: one component which is private to each domain and one which
is shared across domains. Our model is trained not only to perform the
task we care about in the source domain, but
also to use the partitioned representation to reconstruct the images
from both domains. Our novel architecture results in a model that outperforms
the state--of--the--art on a range of unsupervised domain
adaptation scenarios and additionally produces visualizations of the private
and shared representations enabling interpretation of the domain adaptation
process.
\vspace{-4mm}
\end{abstract}
\section{Introduction}
\vspace{-3mm}

The recent success of supervised learning algorithms has been partially
attributed to the large-scale datasets \cite{lin2014microsoft, ILSVRC15} 
on which they are trained. Unfortunately, collecting, annotating, and curating
such datasets is an extremely expensive and time-consuming process. An alternative
would be creating large-scale
datasets in non--realistic but inexpensive settings, such as
computer generated scenes. While such approaches offer the promise
of effectively unlimited amounts of labeled data, models trained in such
settings do not generalize well to realistic domains. 
Motivated by this, we examine the problem of learning
representations that are domainâ€“invariant in scenarios where the data
distributions during training and testing are different. In this setting,
the source data is labeled for a particular task and we would like to
transfer knowledge from the source to the target domain for which we have no ground truth labels.





In this work, we focus on the tasks of object classification and pose estimation, where the object of
interest is in the foreground of a given image, for both source and target domains.
The source and target pixel distributions can differ in a number of ways. We
define ``low-level'' differences in the distributions as those arising due to noise,
resolution, illumination and color. ``High-level'' differences relate
to the number of classes, the types of objects, and geometric variations, such as
3D position and pose. We assume that our source and target domains differ mainly in terms of the
distribution of low level image statistics and that they have high level parameters with similar distributions and the same label space. 

We propose a novel method, the Domain Separation Networks (DSN), for learning domain--invariant representations. Previous work attempts to either find a mapping from representations of the source domain to those of the target~\cite{sun2015return}, or find representations that are shared between the two domains~\cite{ganin2016domain,tzeng2015simultaneous,long2015learning}. While this, in principle, is a good idea, it leaves the shared representations vulnerable to contamination by noise that is correlated with the underlying shared distribution~\cite{salzmann2010factorized}. Our model, in contrast, introduces the notion of a private subspace for each domain, which captures domain specific properties, such as background and low level image statistics. A shared subspace, enforced through the use of autoencoders and explicit loss functions, captures representations shared by the domains. By finding a shared subspace that is orthogonal to the subspaces that are private, our model is able to separate the information that is unique to each domain, and in the process produce representations that are more more meaningful for the task at hand. 
Our method outperforms the state--of--the--art domain adaptation techniques on a range of datasets for object classification and pose estimation, while having an interpretability advantage by allowing the visualization of these private and shared representations. In \secc{related_work}, we survey related work and introduce relevant terminology. Our architecture, loss functions and learning regime are presented in \secc{model}. Experimental results and discussion are given in \secc{experiments}. Finally, conclusions and directions for future work are in \secc{conclusion}. \vspace{-4mm}
\section{Related Work}
\vspace{-4mm}
\label{sec:related_work}

Learning to perform unsupervised domain adaptation is an open theoretical and practical problem. While much prior art exists, our literature 
review focuses primarily on Convolutional Neural Network (CNN) based methods due
to their empirical superiority on this problem~\cite{ganin2016domain, long2015learning, sun2015return, tzeng2015ddc}. Ben-David \etal \cite{ben2010theory} provide upper bounds on a domain-adapted
classifier in the target domain. They introduce the idea of training a binary 
classifier trained to distinguish source and target domains. The error that this
``domain incoherence'' classifier provides (along with the error of a source domain specific classifier) combine to give the overall bounds. Mansour \etal \cite{mansour2009domain} extend the theory of \cite{ben2010theory} to handle the case of multiple source domains. 

Ganin \etal ~\cite{ganin2014unsupervised,ganin2016domain} and Ajakan
\etal ~\cite{Ajakan2014} use adversarial training to find
domain--invariant representations
in-network. Their Domain--Adversarial Neural Networks (DANN) exhibit an 
architecture whose first few feature extraction layers are shared by two
classifiers trained simultaneously. The first is trained to correctly
predict task-specific class labels on the source data  while the second is
trained to predict the domain of each input. DANN minimizes the domain
classification loss with respect to parameters specific to the domain
classifier, while maximizing it with respect to the parameters that are
common to both classifiers. This minimax optimization becomes possible via
the use of a  gradient reversal layer (GRL). 





Tzeng \etal \cite{tzeng2015ddc} and Long \etal
\cite{long2015learning} proposed versions of this model where
the maximization of the domain classification loss is replaced by the minimization
of the Maximum Mean Discrepancy (MMD) metric \cite{gretton2012mmd}. The MMD
metric is computed between features extracted from sets of samples from each
domain. The Deep Domain Confusion Network by Tzeng \etal \cite{tzeng2015ddc} has an 
MMD loss at one layer in the CNN architecture while Long \etal
\cite{long2015learning} proposed the Deep Adaptation Network that has MMD losses at
multiple layers. 



Other related techniques involve learning a transformation from one domain to the other. In this setup, the feature extraction pipeline is fixed during the domain adaptation optimization. 
This has been applied in various non-CNN based approaches
\cite{gong2012geodesic,caseiro2015beyond,gopalan2011domain} as well as the recent
CNN-based Correlation Alignment (CORAL) \cite{sun2015return} algorithm which
``recolors'' whitened source features with the covariance of features from the
target domain. 









%
 



\section{Method}
\vspace{-4mm}
\label{sec:model}
\begin{figure}[tb]
     \centering
     \includegraphics[width=1\linewidth]{gfx/model}
     \caption{Training of our Domain Separation Networks. A shared-weight encoder  learns to capture representation components for a given input sample that are shared among domains. A private encoder  (one for each domain) learns to capture domain--specific components of the representation. A shared decoder learns to reconstruct the input sample by using both the private and source representations. The private and shared representation components are pushed apart with soft subspace orthogonality constraints , whereas the shared representation components are kept similar with a similarity loss . See text for more information.}
     \label{fig:model}
\end{figure}
While the Domain Separation Networks (DSNs) could in principle be applicable to other learning tasks, without loss of generalization, we mainly use
image classification as the cross-domain task. Given a labeled dataset in a source domain and
an unlabeled dataset in a target domain, our goal is to train a classifier on data
from the source domain that generalizes to the target domain. Like previous efforts~\cite{ganin2014unsupervised,ganin2016domain},
our model is trained such that the representations of images from the source
domain are similar to those from the target domain. This allows
a classifier trained on images from the source domain to generalize
as the inputs to the classifier are in theory invariant to the domain of origin. However, these representations might trivially include noise that is highly correlated with the shared
representation, as shown by Salzmann \etal ~\cite{salzmann2010factorized}.

Our main novelty is that, inspired by recent
work \cite{jia2010factorized,salzmann2010factorized,virtanen2011bayesian} on 
shared--space component analysis, DSNs explicitly and jointly model both private and shared components of the domain representations. The private component
of the representation is specific to a single domain and the shared component
of the representation is shared by both domains. To induce the model to 
produce such split representations, we add a loss function that encourages
independence of these parts. Finally, to ensure that the private
representations are still useful (avoiding trivial solutions) and to add
generalizability, we also add a reconstruction loss.
The combination of these objectives is a model that produces a shared
representation that is similar for both domains and a private representation that
is different. By partitioning the space in such a manner, the classifier trained on
the shared representation is better able to generalize across domains as its inputs
are uncontaminated with aspects of the representation that are unique to each domain.



More specifically, 
let  represent a labeled dataset
of  samples from the source domain where  and let
 represent an unlabeled dataset of 
samples from the target domain where . Let
 be a function parameterized by  which maps
an image  to a hidden representation  representing features 
that are common or
\textit{shared} across domains. Let  be
an analogous function which maps an image  to a hidden representation
 representing features that are \textit{private} to each domain.
Let  be a decoding function mapping a hidden representation
 to an image reconstruction . Finally, 
represents a task-specific function, parameterized by  that maps from
hidden representations  to the task-specific predictions . The resulting Domain Separation Network (DSN) model is depicted in \autoref{fig:model}.

\subsection{Learning}
\label{sec:learning}
Inference in a DSN model is given by 
and 
where  is the reconstruction of the input  and  is the
task-specific prediction. 
The goal of training is to minimize the following loss with respect to parameters :

 where  are weights that control the interaction of the loss terms.
The classification loss  trains the model
to predict the output labels we are ultimately interested in. Because we assume
the target domain is unlabeled, the loss is applied only to the source domain.
We want to minimize the negative log--likelihood of the ground truth class for each source domain sample:

where  is the one--hot encoding of the class label for source input 
and  are the softmax predictions of the model: .
We use a scale--invariant mean squared error term~\cite{eigen2014depth} for the
reconstruction loss  which is applied to
both domains:

where   is the number of pixels in input ,  is a vector of ones of
length ; and  is the squared -norm. While a mean squared error
loss is traditionally used for reconstruction tasks, it penalizes predictions that
are correct up to a scaling term. Conversely, the
scale-invariant mean squared error penalizes differences between \textit{pairs}
of pixels. This allows the model to learn to reproduce the overall shape of the objects
being modeled without expending modeling power on the absolute color or intensity
of the inputs. We validated that this reconstruction loss was indeed the correct choice experimentally in Section~\ref{sec:discussion} by training a version of our best DSN model with the traditional mean squared error loss instead of the one in \autoref{eq:reconstruction_loss}.

The difference loss is also applied to both domains and encourages the
shared and private encoders to encode different aspects of the inputs. We define
the loss via a soft subspace orthogonality constraint between the
private and shared representation of each domain. Let  and  
be matrices whose rows are the hidden \textit{shared} representations
 and  from samples of source
and target data respectively. Similarly, let  and  be matrices
whose rows are the \textsl{private} representation  and
 from samples of source and target data respectively. The
difference loss encourages orthogonality between the shared and the private
representations of each domain:

where  is the squared Frobenius norm.
Finally, the similarity loss encourages the hidden
representations  and  from the shared encoder to be as
similar as possible irrespective of the domain. We experimented with two similarity losses, which we discuss in detail.



\subsection{Similarity Losses}
\label{sec:similarity_loss}
The domain adversarial similarity loss \cite{ganin2014unsupervised,ganin2016domain}
is used to train a model to produce representations such that a classifier cannot
reliably predict the domain of the encoded representation.
Maximizing such ``confusion'' is achieved via a Gradient Reversal Layer (GRL)
and a \textit{domain classifier} trained to predict the domain producing the
hidden representation.
The GRL has the same output as the identity function,
but reverses the gradient direction. Formally, for some function , the GRL
is defined as  with a gradient .
The domain classifier 
parameterized by  maps a shared representation vector
 to a prediction of the label  of the
input sample . Learning with a GRL is adversarial in that  is
optimized to increase 's ability to discriminate between encodings of images from
the source or target domains, while the reversal of the gradient results in the model
parameters  learning representations from which domain classification
accuracy is reduced.
Essentially, we \textsl{maximize} the binomial cross--entropy for the domain prediction task with respect to , while \textsl{minimizing} it with respect to : 


where  is the ground truth domain label for sample .

The Maximum Mean Discrepancy (MMD) loss \cite{gretton2012mmd} is a kernel-based
distance function between pairs of samples.
We use a biased statistic for the squared population MMD between shared encodings of the source samples  and the shared encodings of the target samples :

where  is a PSD kernel function. In our experiments we used a linear combination of multiple RBF kernels: , where  is the standard deviation and  is the weight for our  RBF kernel. Any additional kernels we include in the multi--RBF kernel are additive and guarantee that their linear combination remains characteristic. Therefore, having a large range of kernels is beneficial since the distributions of the shared features change during learning, and different components of the multi--RBF kernel might be responsible at different times for making sure we reject a false null hypothesis, i.e. that the loss is sufficiently high when the distributions are not similar~\cite{long2015learning}. The advantage of using an RBF kernel with the MMD distance is that the Taylor expansion of the Gaussian function allows us to match all the moments of the two populations. The caveat is that it requires finding optimal kernel bandwidths .







 \section{Evaluation}
\label{sec:experiments}
\vspace{-4mm}






We are motivated by the problem of learning models on a clean, synthetic dataset and testing on
noisy, realâ€“world dataset. To this end, we evaluate on object classification datasets used in previous work\footnote{The most commonly used dataset for visual domain adaptation in the context of object classification is Office \cite{saenko2010adapting}. However, this dataset exhibits significant variations in both low-level and high-level parameter distributions. Low-level variations are due to the different cameras and background textures in the images (e.g. Amazon versus DSLR). However, there are significant high-level variations due to object identity: e.g. the motorcycle class contains non-motorcycle objects; the backpack class contains a laptop; some domains contain the object in only one pose. Other commonly used datasets such as Caltech-256 suffer from similar problems. We therefore exclude these datasets from our evaluation. For more information, see our Supplementary Material.}
including MNIST and {MNIST-M} \cite{ganin2016domain}, the German Traffic Signs
Recognition Benchmark (GTSRB) \cite{stallkamp2012gtsrb}, and the Streetview
House Numbers (SVHN) \cite{netzer2011reading}. We also evaluate on the cropped
LINEMOD dataset, a standard for object instance
recognition and 3D pose 
estimation~\cite{hinterstoisser2012accv,wohlhart2015learning}, for which we have
synthetic and real data\footnote{\url{https://cvarlab.icg.tugraz.at/projects/3d_object_detection/}}.
We tested the following unsupervised domain adaptation scenarios: \textsl{(a)} from MNIST to MNIST-M; \textsl{(b)} from SVHN to MNIST; \textsl{(c)} from synthetic traffic signs to real ones with GTSRB; \textsl{(d)} from synthetic LINEMOD object instances rendered on a black background to the same object instances in the real world.

We evaluate the efficacy of our method with each of the two similarity losses outlined in \autoref{sec:similarity_loss} by comparing against the prevailing visual domain adaptation techniques for neural networks: Correlation Alignment (CORAL)~\cite{sun2015return},  Domain--Adversarial Neural Networks (DANN)~\cite{ganin2014unsupervised,ganin2016domain}, and MMD regularization~\cite{tzeng2015ddc,long2015learning}. 
For each scenario we provide two additional baselines: the performance on the target domain of the respective model with no domain adaptation and trained \textsl{(a)} on the source domain (``Source--only'' in \autoref{tab:results}) and \textsl{(b)} on the target domain (``Target--only''), as an empirical lower and upper bound respectively. 

We have not found a universally applicable way to optimize hyperparameters for unsupervised domain adaptation.  Previous work~\cite{ganin2016domain} suggests the use of reverse validation. We implemented this (see Supplementary Material for details) but found that that the reverse validation accuracy did not always align well with test accuracy. Ideally we would like to avoid using labels from the target domain, as it can be argued that if ones does have target domain labels, they should be used during training. However, there are applications where a labeled target domain set cannot be used for training. An example is the labeling of a dataset with the use of AprilTags~\cite{olson2011apriltag}, 2D barcodes that can be used to label the pose of an object, provided that a camera is calibrated and the physical dimensions of the barcode are known. These images should not be used when learning features from pixels, because the model might be able to decipher the tags. However, they can be part of a test set that is not available during training, and an equivalent dataset without the tags could be used for unsupervised domain adaptation.
 We thus chose to use a small set of labeled target domain data as a validation set for the hyperparameters of all the methods we compare. All methods were evaluated using the same protocol, so comparison numbers are fair and meaningful. The performance on this validation set can serve as an \textsl{upper bound} of a satisfactory validation metric for unsupervised domain adaptation, which to our knowledge is still an open research question, and out of the scope of this work. 











\begin{table}[t]
\centering
\caption{Mean classification accuracy (\%) for the unsupervised domain adaptation scenarios we evaluated all the methods on. We have replicated the experiments from Ganin \etal~\protect\cite{ganin2016domain} and in parentheses we show the results reported in their paper. The ``Source--only'' and ``Target--only'' rows are the results on the target domain when  using no domain adaptation and training only on the source or the target domain respectively. The results that perform best in each domain adaptation task are in bold font.}
\vspace{2mm}
\label{tab:results}
\begin{tabular}{ | l | l | l | l | l | }
\hline
\bf Model   & \bf MNIST to & \bf Synth Digits to &\bf SVHN to  &\bf Synth Signs to \\
 &\bf MNIST-M  &\bf SVHN        &\bf MNIST &\bf GTSRB     \\ \hline \hline
Source-only  & 56.6 (52.2) & 86.7 (86.7)      & 59.2 (54.9) & 85.1  (79.0)    \\ \hline \hline
CORAL \cite{sun2015return} & 57.7 & 85.2       &  63.1     & 86.9        \\ \hline
MMD  \cite{tzeng2015ddc,long2015learning}  & 76.9 & 88.0 & 71.1 & 91.1 \\ \hline 
DANN \cite{ganin2016domain}  & 77.4 (76.6)  &  90.3 (91.0)    & 70.7 (73.8)  &     92.9 (88.6) \\ \hline
DSN w/ MMD (ours)  & 80.5 & 88.5  & 72.2   & 92.6 \\ \hline
DSN w/ DANN (ours) & \textbf{83.2} & \textbf{91.2} & \textbf{82.7} & \textbf{93.1} \\ \hline\hline
Target-only  & 98.7 & 92.4  & 99.5 & 99.8  \\ \hline
\end{tabular}
\end{table}



\subsection{Datasets and Adaptation Scenarios}
\textbf{MNIST to MNIST-M.} In this domain adaptation scenario we use the popular MNIST~\cite{lecun1998gradient} dataset of handwritten digits as the source domain, and MNIST-M, a variation of MNIST proposed for unsupervised domain adaptation by ~\cite{ganin2016domain}. MNIST-M was created by using each MNIST digit as a binary mask and inverting with it the colors of a background image. The background images are random crops uniformly sampled from the Berkeley Segmentation Data Set~(BSDS500)~\cite{arbelaez2011contour}.  In all our experiments, following the experimental protocol by~\cite{ganin2016domain}. Out of the  MNIST-M training examples, we used the labels for  of them to find optimal hyperparameters for our models. This scenario, like all three digit adaptation scenarios, has 10 class labels.



\textbf{Synthetic Digits to SVHN.}  In this scenario we aim to learn a classifier for the Street-View House Number data set (SVHN)~\cite{netzer2011reading}, our target domain, from a dataset of purely synthesized digits, our source domain. The synthetic digits~\cite{ganin2016domain} dataset was created by rasterizing bitmap fonts in a sequence~(one, two, and three digits) with the ground truth label being the digit in the center of the image, just like in SVHN. The source domain samples are further augmented by variations in scale, translation, background colors, stroke colors, and Gaussian blurring. We use  Synthetic Digits for our source domain training set,  unlabeled SVHN samples for domain adaptation, and  SVHN samples for testing.  Similarly to above, we used the labels of  SVHN training examples to find optimal hyperparameters for our models. 

\textbf{SVHN to MNIST.}  Although the SVHN dataset contains significant variations (in scale, background clutter, blurring, embossing, slanting, contrast, rotation, sequences to name a few) there is not a lot of variation in the actual digits shapes. This makes it quite distinct from a dataset of handwritten digits, like MNIST, where there are a lot of elastic distortions in the shapes, variations in thickness, and noise on the digits themselves. Since the ground truth digits in both datasets are centered, this is a well--posed and rather difficult domain adaptation scenario. As above, we used the labels of  MNIST training examples for validation.

\textbf{Synthetic Signs to GTSRB.} We also perform an experiment using a dataset of synthetic traffic signs from \cite{Moiseev2013} to real world dataset of traffic signs (GTSRB)~\cite{stallkamp2012gtsrb}. While the three digit adaptation scenarios have 10 class labels, this scenario has 43 different traffic signs. The synthetic signs were obtained by taking relevant pictograms and adding various types of variations, including random backgrounds, brightness, saturation, 3D rotations, Gaussian and motion blur. We use  synthetic signs for training,  random GTSRB real--world signs for domain adaptation and validation, and the remaining  GTSRB real signs as the test set.

\textbf{Synthetic Objects to LineMod.} The LineMod dataset \cite{wohlhart2015learning} consists of CAD models of objects in a cluttered environment and a high variance of 3D poses for each object. We use the 11 non--symmetric objects from the cropped version of the dataset, where the images are cropped with the object in the center, for the task of object instance recognition and 3D pose estimation. We train our models on  images for these objects rendered on a black background without additional noise. We use a target domain training set of  real--world images for domain adaptation and validation, and a target domain test set of  for testing. For this scenario our task is both classification and pose estimation; our task loss is therefore ,
where  is the positive unit quaternion vector representing the ground truth 3D pose, and  is the equivalent prediction. The first term is the classification loss, similar to the rest of the experiments, the second term is the log of a 3D rotation metric for quaternions~\cite{huynh2009metrics}, and  is the weight for the pose loss. Quaternions are a convenient angle--axis representation for 3D rotations. In \autoref{tab:pose_results} we report the mean angle the object would need to be rotated (on a fixed 3D axis) to move from the predicted pose to the ground truth \cite{hinterstoisser2012accv}.



\begin{table}[]
\centering
\caption{Mean classification accuracy and pose error for the ``Synth Objects to LINEMOD'' scenario. }
\label{tab:pose_results}
\begin{tabular}{@{}|l|c|c|@{}}
\hline
\multicolumn{1}{|c|}{\textbf{Method}} & \multicolumn{1}{c|}{\textbf{Classification Accuracy}} & \multicolumn{1}{c|}{\textbf{Mean Angle Error}} \\ \hline \hline
Source-only                           & 47.33\%                                   & \\\hline  \hline
MMD                                   & 72.35\%                                        &                               \\ \hline
DANN                                  & 99.90\%                                               &                 \\ \hline\hline
DSN w/ MMD (ours)                     & 99.72\%                                            &\\ \hline
DSN w/ DANN (ours)                    & \textbf{100.00}\%                                           & \\ \hline\hline
Target-only                           & 100.00\%                                              &                                  \\ \hline
\end{tabular}
\end{table}
\subsection{Implementation Details}
\begin{figure}[t]
    \centering
\begin{subfigure}[b]{.23\linewidth}
        \centering
        \includegraphics[height=\linewidth]{mnist_m_source}
        \caption{MNIST (source)}
        \label{fig:mnist_to_mnist-m_source}
    \end{subfigure}          \hfill
    \begin{subfigure}[b]{.23\linewidth}
        \centering
    \includegraphics[height=\linewidth]{mnist_m_recon_left}
        \caption{MNIST-M (target)}
        \label{fig:mnist_to_mnist-m_target}
    \end{subfigure}          \hfill
    \begin{subfigure}[b]{.23\linewidth}
        \centering
        \includegraphics[height=\linewidth]{pose_figs/pose_synth}
        \caption{Synth Objects (source)}
        \label{fig:mnist_to_mnist-m_target}
    \end{subfigure}          \hfill
    \begin{subfigure}[b]{.23\linewidth}
        \centering
        \includegraphics[height=\linewidth]{pose_figs/pose_real}
        \caption{LINEMOD (target)}
        \label{fig:mnist_to_mnist-m_target}
    \end{subfigure}          \hfill    
    \caption{Reconstructions for the representations of the two domains for ``MNIST to MNIST-M'' and for ``Synth Objects to LINEMOD''. In each block from left to right: the original image ; reconstructed image ; shared only reconstruction  ;  private only reconstruction  .}
    \label{fig:mnist_m_visualizations}
\end{figure}
All the models were implemented using TensorFlow\footnote{Our code will be open--sourced under \url{https://github.com/tensorflow/models/} before the NIPS 2016 meeting.}~\cite{abadi2016tensorflow} and were trained with Stochastic Gradient Descent plus momentum~\cite{sutskever2013momentum}. Our initial learning rate was multiplied by  every  steps (mini-batches). We used batches of 32 samples from each domain for a total of 64 and
the input images were mean-centered and rescaled to .
In order to avoid distractions for the main classification task during the early stages of the training procedure, we activate any additional domain adaptation loss after  steps of training. 
For all our experiments our CNN topologies are based on the ones used in ~\cite{ganin2016domain}, to be comparable to previous work in unsupervised domain adaptation. The exact architectures for all models are shown in our Supplementary Material. 

 In our framework, CORAL \cite{sun2015return} would be equivalent to fixing our shared representation matrices  and , normalizing them and then minimizing  with respect to a weight matrix  that aligns the two correlation matrices. For the {CORAL} experiments, we follow the suggestions of~\cite{sun2015return}, and extract features for both source and target domains from the penultimate layer of each network. Once the correlation matrices for each domain are aligned, we evaluate on the target test data the performance of a linear support vector machine (SVM) classifier trained on the source training data. The SVM penalty parameter was optimized based on the target domain validation set for each of our domain adaptation scenarios. 
For MMD regularization, we used a linear combination of 19 RBF kernels\footnote{The Supplementary Material has details on all the parameters.}. We applied MMD on \textsl{fc3} on all our model architectures and minimized
 with respect to .
Preliminary experiments with having MMD applied on more than one layers did not show any performance improvement for our experiments and architectures. 
For DANN regularization, we applied the GRL and the domain classifier as prescribed in \cite{ganin2016domain} for each scenario. We optimized
 by minimizing it with respect to  and maximizing it with respect to the domain classifier parameters .

For our Domain Separation Network experiments, our similarity losses are always applied at the first fully connected layer of each network after a number of convolutional and max pooling layers. For each private space encoder network we use a simple convolutional and max pooling structure followed by a fully-connected layer with a number of nodes equal to the number of nodes at the final layer  of the equivalent shared encoder . The output of the shared and private encoders gets added before being fed to the shared decoder .
For the latter we use a deconvolutional architecture \cite{zeiler2010deconvolutional} which consists of a fully connected layer with 300 nodes, a resizing layer to , two  convolutional layers, one upsampling layer to , another  convolutional layer, followed by the reconstruction output.













\subsection{Discussion}
\label{sec:discussion}
 The DSN with DANN model outperforms all the other methods we experimented with for all our unsupervised domain adaptation scenarios (see \autoref{tab:results} and \ref{tab:pose_results}). Our unsupervised domain separation networks are able to improve both upon MMD regularization and DANN.  Using DANN as a similarity loss (\autoref{eq:dann}) worked better than using MMD (\autoref{eq:mmd}) as a similarity loss, which is consistent with results obtained for domain adaptation using MMD regularization and DANN alone.


In order to examine the effect of the soft orthogonality constraints (), we took our best model, our DSN model with the DANN loss, and removed these constraints by setting the  coefficient to . Without them, the model performed consistently worse in all scenarios.
We also validated our choice of our scale--invariant mean squared error reconstruction loss as opposed to the more popular mean squared error loss by running our best model with . With this variation we also get worse classification results consistently, as shown in experiments from \autoref{tab:ablation_results}.
\begin{table}[t]
\centering
\caption{Effect of our difference and reconstruction losses on our best model. The first row is replicated from \autoref{tab:results}. In the second row, we remove the soft orthogonality constraint. In the third row, we replace the scale--invariant MSE with regular MSE.}
\vspace{2mm}
\label{tab:ablation_results}
\begin{tabular}{ | l | l | l | l | l | }
\hline
\bf Model   & \bf MNIST to & \bf Synth. Digits to & \bf SVHN to  & \bf Synth. Signs to \\
 & \bf MNIST-M  & \bf SVHN        & \bf MNIST & \bf GTSRB\\ \hline \hline
All terms  & \textbf{83.23} & \textbf{91.22} & \textbf{82.78} & \textbf{93.01} \\ \hline 
No  & 80.26 & 89.21 &  80.54 & 91.89\\ \hline
With  & 80.42 & 88.98 &  79.45 & 92.11\\ \hline
\end{tabular}
\end{table}

The shared and private representations of each domain are combined for the reconstruction of samples. Individually decoding the shared and private representations gives us reconstructions that serve as useful depictions of our domain adaptation process. In \autoref{fig:mnist_m_visualizations} we use the ``MNIST to MNIST-M''  and the ``Synth. Objects to LINEMOD'' scenarios   for such visualizations.  In the former scenario, the model clearly separates the foreground from the background and produces a shared space that is very similar to the source domain. This is expected since the target is a transformation of the source. In the latter scenario, the model is able to produce visualizations of the shared representation that look very similar between source and target domains, which are useful for classification and pose estimation, as shown in \autoref{tab:pose_results}.
 \section{Conclusion}
\label{sec:conclusion}
\vspace{-4mm}
We present in this work a deep learning model that improves upon existing unsupervised domain adaptation techniques. The model does so by explicitly separating representations private to each domain and shared between source and target domains. By using existing domain adaptation techniques to make the shared representations similar, and soft subspace orthogonality constraints to make private and shared representations dissimilar, our method outperforms all existing unsupervised domain adaptation methods in a number of adaptation scenarios that focus on the synthetic--to--real paradigm. %
 
\subsubsection*{Acknowledgments}

We would like to thank Samy Bengio, Kevin Murphy, and Vincent Vanhoucke for valuable comments on this work. We would also like to thank Yaroslav Ganin and Paul Wohlhart for providing some of the datasets we used.

\clearpage
{
\small
\bibliographystyle{ieee}
\bibliography{domain_separation}
}

\clearpage
\section*{Supplementary Material}
\appendix 



\section{Correlation Regularization}
\label{sec:correg}

Correlation Alignment (CORAL)~\cite{sun2015return} aims to find a mapping from the representations of the source domain to the representations of the target domain by matching only the second--order statistics. In our framework, this would be equivalent to fixing our common representation  matrices  and  after normalizing them and then finding a weight matrix  that aligns the two correlation matrices. Although this has the advantage that the optimization
is convex and can be solved in closed form, all convolutional features remain
fixed during the process, which might not be optimal for the task at hand. Also, because of this we are not able to use it as a similarity loss for our DSNs. 
Motivated by this shortcoming, we propose here a new domain adaptation method, Correlation Regularization (CorReg). We show in \autoref{tab:results} that our new domain adaptation method, which is theoretically as powerful as an MMD loss with a second--order polynomial kernel, outperforms CORAL in all our datasets. Adapting a feature hierarchy to be domain--invariant is more powerful than learning a mapping from the representations of one domain to those of another. Moreover, we use it as yet another similarity loss for our Domain Separation Networks:

Our DNS with CorReg performs better than both CORAL and CorReg, which is consistent with the rest of our results.

\begin{table}[h]
\centering
\caption{Our main results from the paper with two additional lines for CorReg and DSN with CorReg.}
\label{tab:results}
\begin{tabular}{ | l | l | l | l | l | }
\hline
\bf Model   & \bf MNIST to & \bf Synth Digits to &\bf SVHN to  &\bf Synth Signs to \\
 &\bf MNIST-M  &\bf SVHN        &\bf MNIST &\bf GTSRB     \\ \hline \hline
Source-only  & 56.6 (52.2) & 86.7 (86.7)      & 59.2 (54.9) & 85.1  (79.0)    \\ \hline \hline
CORAL \cite{sun2015return} & 57.7 & 85.2       &  63.1     & 86.9        \\ \hline
CorReg (Ours)  & 62.06 & 87.33       & 69.20 & 90.75     \\ \hline\hline
MMD  \cite{tzeng2015ddc,long2015learning}  & 76.9 & 88.0 & 71.1 & 91.1 \\ \hline 
DANN \cite{ganin2016domain}  & 77.4 (76.6)  &  90.3 (91.0)    & 70.7 (73.8)  &     92.9 (88.6) \\ \hline
DSN w/ MMD (ours)  & 80.5 & 88.5  & 72.2   & 92.6 \\ \hline
DSN w/ DANN (ours) & \textbf{83.2} & \textbf{91.2} & \textbf{82.7} & \textbf{93.1} \\ \hline\hline
Target-only  & 98.7 & 92.4  & 99.5 & 99.8  \\ \hline
\end{tabular}
\end{table}

\section{Office Dataset Criticism}
The most commonly used dataset for visual domain adaptation in the context of object classification is Office \cite{saenko2010adapting}, sometimes combined with the Caltech--256 dataset \cite{griffin2007caltech} as an additional domain. However, these datasets exhibit significant variations in both low-level and high-level parameter distributions. Low-level variations are due to the different cameras and background textures in the images (e.g. Amazon versus DSLR), which is welcome. However, there are significant high-level variations due to elements like label pollution: e.g. the motorcycle class contains non-motorcycle objects; the backpack class contains 2 laptops; some classes contain the object in only one pose. Other commonly used datasets such as Caltech-256 suffer from similar problems. We illustrate some of these issues for the `back\_pack' class for its 92 Amazon samples, its 12 DSLR samples, its 29 Webcam samples, and its 151 Caltech samples in \autoref{fig:office_pollution2}. Other classes exhibit similar problems. For these reasons some works, eg \cite{sun2015return}, pretrain their models on Imagenet before performing the domain adaptation in these scenarios. This essentially involves another source domain (Imagenet) in the transfer.
\begin{figure}[th]
    \centering
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{laptopbag}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{problematic}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{manybackpacks}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{laptop}
    \end{subfigure}   \hfill
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{laptop2}
    \end{subfigure}  \\    
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{backpack1}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{backpack2}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{backpack3}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{backpack4}
    \end{subfigure}   \hfill
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{backpack5}
    \end{subfigure}  \\
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{wbackpack1}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{wbackpack2}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{wbackpack3}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{wbackpack4}
    \end{subfigure}   
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{wbackpack5}
    \end{subfigure}  \\
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{caltech_backpack1}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{caltech_backpack2}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{caltech_backpack3}
    \end{subfigure}    \hfill
        \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{caltech_backpack4}
    \end{subfigure}   
    \begin{subfigure}[b]{0.18\linewidth}
        \centering
        \includegraphics[width=\linewidth]{caltech_backpack5}
    \end{subfigure}      
    \caption{Examples of the `back\_pack' class in the different domains in Office and Caltech--256. \textbf{First Row:} 5 of the 92 images in the Amazon domain. \textbf{Second Row:} The DSLR domain contains 4 images for the rightmost image from different frontal angles, 2 images for the other 4 backpacks for a total of 12 images for this class. \textbf{Third Row:} The webcam domain contains the exact same backpacks with DSLR with similar poses for a total of 29 images for this class. \textbf{Fourth Row:} Some of the 151 backpack samples Caltech domain.}
    \label{fig:office_pollution2}
\end{figure}


\section{Domain Separation}
We visualize in~\autoref{fig:sdads} reconstructions for both source and target domains of each domain adaptation scenario. Although the visualizations are not as clear as with the ``MNIST to MNIST-M'' scenario, where the target domain was a direct transformation of the source domain, it is interesting to note the similarities of the visualizations of the shared representations, and the exclusion of some shared information in the private domains.

\newcommand{\subfigscale}{0.24}

\begin{figure}[ht]
    \centering
    
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{synth_svhn/svhn_8}
    \end{subfigure}
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{svhn_mnist/reconstruction_svhn_mnist_svhn_5}
    \end{subfigure}
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{gtsrb/real_1}
    \end{subfigure}
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pose_figs/duck_real}
    \end{subfigure} \\
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{synth_svhn/syn_svhn_8}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{svhn_mnist/reconstruction_svhn_mnist_mnist_5}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{gtsrb/synth_1}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\subfigscale\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pose_figs/source_duck_2}
        \caption{}
    \end{subfigure}
    \caption{
    Reconstructions for the representations of the two domains for \textit{a) Synthetic Digits to SVHN, \textit{b)} SVHN to MNIST, \textit{c)} Synthetic Signs to GTSRB, \textit{d)} Synthetic Objects to LineMOD. In each block from left to right: the original image ; reconstructed image ; shared only reconstruction  ;  private only reconstruction  .Reconstructions of target \textit{(top row)} and source \textit{(bottom row)} domains. }.
    }
    \label{fig:sdads}
\end{figure}






\section{Network Topologies and Optimal Parameters}
Since we used different network topologies for our domain adaptation scenarios, there was not enough space to include these in the main paper. We present the exact topologies used in Figures \ref{fig:mnist_arch}--\ref{fig:mnist_arch4}. 

Similarly, we list here all hyperparameters that are important for total reproducibility of all our results.
For CORAL, the SVM
penalty parameter that was optimized based on the validation set for each of our domain adaptation scenarios:  for ``MNIST to MNIST-M'', ``Synth Digits to SVHN'', ``Synth Signs to GTSRB'', and   for ``SVHN to MNIST''.
For MMD we use 19 RBF kernels with the following standard deviation parameters:

and equal  weights. We use learning rate between  and . 
For DANN we use learning rate between  and .
For DSN w/ DANN and DSN w/ MMD we use a constant initial learning rate of   use the hyperparameters in the range of: , whereas for DNS w/ CorReg we use . For the GTSRB experiment we use . In all cases we use an exponential decay of  on the learning rate every  iterations. For the LINEMOD experiments we use .




\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{mnist_m}
    \caption{The network topology for ``MNIST to MNIST-M''}
    \label{fig:mnist_arch}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{svhn}
    \caption{The network topology for ``Synth SVHN to SVHN'' and ``SVHN to MNIST'' experiments.}
    \label{fig:mnist_arch2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{gtsrb}
    \caption{The network topology for ``Synth Signs to GTSRB''}
    \label{fig:mnist_arch3}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{pose_mini}
    \caption{The network topology for ``Synthetic Objects to Linemod''}
    \label{fig:mnist_arch4}
\end{figure}

\clearpage



 

\end{document}