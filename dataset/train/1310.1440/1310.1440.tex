\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{a4paper,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}

\usepackage{cite}

\usepackage{paralist}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{url}

\usepackage{color}

\usepackage{amsmath}
\usepackage{amssymb}

\allowdisplaybreaks[1]

\usepackage{amsthm}
\renewcommand{\qedsymbol}{\rule{1.2ex}{1.2ex}}
\renewcommand{\proofname}{\upshape\bfseries Proof}
\newcommand{\Qed}{\qedhere}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}

\makeatletter
\def\@endtheorem{\endtrivlist}
\makeatother

\newcommand{\substr}[3]{#1[#2..#3]}

\newcommand{\scheme}{\mathcal{S}}
\newcommand{\schemelam}{\mathcal{S}_{\text{Lam}}}

\newcommand{\partition}{X}
\newcommand{\numstrings}{\mathit{strings}}
\newcommand{\numstringsp}{\numstrings'}
\newcommand{\strings}{\mathcal{A}}
\newcommand{\numnodes}[1]{\mathit{nodes}_{#1}}
\newcommand{\numnodesd}[2]{\mathit{nodes}_{#1,#2}}
\newcommand{\prob}[2]{\mathit{prob}_{#1,#2,\sigma}}
\newcommand{\Psearch}{P^*}

\newcommand{\numstringsedit}{\numstrings_{\mathrm{edit}}}
\newcommand{\numnodesp}[2]{\mathit{nodes}_{#1}(#2)}
\newcommand{\numnodesqp}[3]{\mathit{nodes}_{#1,#2}(#3)}
\newcommand{\numnodesqpb}[3]{\mathit{nodes}_{#1,#2,#3}}

\newcommand{\automaton}[1]{\mathcal{A}_{#1}}
\newcommand{\transition}[1]{\hat{\delta}_{#1}}
\newcommand{\states}[1]{\mathrm{Q}_{#1}}

\newcommand{\numstringspp}{\numstrings''}
\newcommand{\logn}{N}

\newcommand{\lognc}{N}
\newcommand{\ppartition}{Y}
\newcommand{\spartition}{Z}

\newcommand{\plen}[1]{m_{#1}}
\newcommand{\plast}[1]{l_{#1}}
\newcommand{\pnumparts}[1]{p_{#1}}
\newcommand{\prank}[1]{r_{#1}}
\newcommand{\pprefix}[1]{\mathit{prefix}(#1)}
\newcommand{\optv}[3]{v(#1,#2,#3)}

\newcommand{\seq}[2]{[#1,#2]}
\newcommand{\seqr}[2]{\overline{[#1,#2]}}



\newif\iffull
\fulltrue
\newif\ifextra
\extrafalse

\begin{document}

\title{Approximate String Matching using a Bidirectional Index}
\author{
Gregory Kucherov\thanks{CNRS/LIGM, Universit\'e Paris-Est Marne-la-Vall\'ee, France}
\and
Kamil Salikhov\thanks{Mechanics and Mathematics Department, Lomonosov Moscow State University, Russia}
\and
Dekel Tsur\thanks{Department of Computer Science, Ben-Gurion University of
the Negev, Israel}
}
\date{}
\maketitle

\begin{abstract}
We study strategies of approximate pattern matching that exploit
bidirectional text indexes, extending and generalizing ideas of
\cite{LamLTWWY09}. We introduce a formalism, called search schemes, to specify search strategies of
this type, then develop a probabilistic measure for the efficiency of
a search scheme, prove several combinatorial results on efficient
search schemes, and finally, provide experimental computations
supporting the superiority of our strategies. 
\end{abstract}

\section{Introduction}

Approximate
string matching has numerous practical applications and has long been a subject of extensive studies by 
algorithmic researchers~\cite{Navarro:2001:GTA:375360.375365}. If errors are allowed in a
match between a pattern string and a text string, most of
fundamental ideas behind exact string search algorithms become
inapplicable.

The problem of approximate string matching comes in
different variants. In this paper, we are concerned with the
\emph{indexed} variant, when a static text is available for
pre-processing and storing in a data structure (index), before any matching
query is made. 
The 
challenge of indexed approximate matching is to construct a small-size
index supporting quick search for approximate 
pattern occurrences, within a worst-case time weakly dependent on the text
length. 
From the theoretical perspective, 
even the case of one allowed error turned out to be
highly nontrivial and gave rise to a series of works (see
\cite{LamSW05} and references therein). In the case of  errors,
existing solutions generally have time or space complexity that is exponential
in , see~\cite{SungEncyclopedia08} for a survey.

The quest for efficient approximate string matching algorithms has
been boosted by a new generation of DNA sequencing
technologies, capable to produce huge quantities of short 
DNA sequences, called \emph{reads}. 
Then, an important task is to \emph{map} 
those reads to a given reference genomic sequence,
which requires very fast and accurate approximate string matching
algorithms. 
This motivation resulted in a very large number of read mapping algorithms and
associated software programs, 
we refer to~\cite{Li01092010} for a survey.

Broadly speaking, 
read mapping algorithms follow one of two main approaches, or sometimes
a combination of those. 
The \emph{filtration} approach proceeds in two
steps: it first identifies (with or without using a full-text index)
locations of the text where the pattern can \emph{potentially} occur,
and then verifies these locations for actual matches. Different filtration schemes have been proposed
\cite{NavarroRaffinot-book02,FarachLST-conf,KucherovNoeRoytbergJCBB05,KarkkainenN07}. Filtration
algorithms usually don't offer interesting worst-case time and space bounds but are
often efficient on average and are widely used in practice. 
Another approach, usually called \emph{backtracking}, 
extends exact matching algorithms to the approximate case by some
enumeration of possible errors and by simulating exact search of all
possible variants of the pattern. It is this approach that we follow in
the present work. Backtracking and filtration techniques can be
combined in a \emph{hybrid} approach~\cite{NavarroB00}. 

Some approximate matching algorithms use standard text indexes, such as
suffix tree or suffix arrays. However, for large
datasets occurring in modern applications, these indexes are
known to take too much memory. Suffix arrays and suffix trees
typically require at least 4 or 10 \emph{bytes} per character respectively. 
The last years saw the development of \emph{succinct} or \emph{compressed full-text indexes}
that occupy virtually as much memory as the sequence itself and yet
provide very powerful functionalities~\cite{NavarroM07}.
For example, the FM-index~\cite{FerraginaM00},
based on the Burrows-Wheeler Transform~\cite{BurrowW94}, may
occupy 2--4 \emph{bits} of memory {per character} for DNA texts. FM-index has now been used in
many practical bioinformatics software programs,
e.g.~\cite{LangmeadTPS09,LiD09,Simpson01032012}.
Even if succinct indexes are primarily
designed for exact string search, using them for approximate matching
naturally became an attractive opportunity. This direction has been
taken in several papers, see~\cite{RussoEtAlAlgorithms09}, as well as
in practical implementations~\cite{Simpson01032012}. 

Interestingly, succinct indexes can provide even more functionalities
than classical ones. In particular, succinct indexes can be
made \emph{bidirectional}, i.e.\ can perform pattern search in both
directions \cite{LamLTWWY09,RussoEtAlAlgorithms09,SchnattingerOG12,BelazzouguiCKM13}.
Lam et al.~\cite{LamLTWWY09} showed how a
bidirectional FM-index can be used to efficiently search for strings
up to a small number (one or two) errors. The idea is to partition the pattern into 
equal parts, where  is the number of errors,
and then perform multiple searches on the FM-index, where
each search assumes a different distribution of mismatches among the
pattern parts. It has been shown experimentally
in~\cite{LamLTWWY09} that this improvement leads to a
faster search compared to the best existing read alignment
software.
Related algorithmic ideas appear also in~\cite{RussoEtAlAlgorithms09}.

In this paper, we extend the search strategy of~\cite{LamLTWWY09} 
in two
main directions. We consider the case of arbitrary  and propose to
partition the pattern into more than  parts that can be of
\emph{unequal} size. To demonstrate the benefit of both ideas, we first introduce a general formal framework for this
kind of algorithm, called \emph{search scheme}, that allows us to easily specify them and to
reason about them (Section~\ref{sec:bidirectional}). 
Then, in Section~\ref{sec:analysis} we perform a probabilistic analysis
that provides us with a quantitative measure of performance of a search
scheme, and give an efficient algorithm for obtaining the optimal
pattern partition for a given scheme.
Furthermore, we prove
several combinatorial results on the design of efficient search
schemes (Section~\ref{sec:design}).
Finally, Section~\ref{sec:experiments} contains comparative analytical
estimations, based on our probabilistic analysis, that demonstrate the
superiority of our search strategies for many practical parameter
ranges. 
We further report on large-scale experiments on genomic data
supporting this analysis. 

\section{Bidirectional search}\label{sec:bidirectional}

In the framework of text indexing, pattern search is usually done by
scanning the pattern online and recomputing \emph{index points}
referring to the occurrences of the scanned part of the pattern. 
With classical text indexes, such as suffix trees or
suffix arrays, the pattern is scanned left-to-right (\emph{forward
search}). However, some compact indexes such as FM-index provide a
search algorithm that scans the pattern right-to-left (\emph{backward
search}). 

Consider now approximate string matching. For ease of presentation, we
present most of our ideas for the
case of Hamming distance
(recall that the Hamming distance between two strings  and  of equal
lengths is the number of indices  for which ),
although our algorithms extend to the edit
distance as well. Section~\ref{sec:estimation-edit} below will
specifically deal with the edit distance.

Assume that  letter mismatches are allowed between a pattern  and
a substring of length  of a text .
Both forward and backward search can be extended to
approximate search in a straightforward way, by exploring all
possible mismatches along the search, as long as their number does
not exceed  and the current pattern still occurs in the text.
For the forward search, for example, the algorithm enumerates all
substrings of  with
Hamming distance at most  to a \emph{prefix} of .
Starting with the empty string, the enumeration 
is done by extending the current string with the corresponding letter
of , and with all other letters provided that the number of accumulated
mismatches has not yet reached . For each extension,
its positions in  are computed using the index.
Note that the set of enumerated strings is closed under prefixes and
therefore can be represented by the nodes of a trie.
Similar to forward search, \emph{backward search}
enumerates all substrings of 
with Hamming distance at most  to a \emph{suffix} of .

Clearly, backward and forward search are symmetric and, once we have
an implementation of one, the other can be implemented similarly by constructing the index
for the reversed text. 
However, combining both forward and backward search within one algorithm
results in a more efficient search.
To illustrate this, consider the case .
Partition  into two equal length parts .
The idea is to perform two complementary searches: forward search for
occurrences of  with a mismatch in  and backward search for
occurrences with a mismatch in . In both searches, branching is
performed only after  characters are matched. 
Then,
the number of strings enumerated by the
two searches is much less than the number of strings enumerated by a single
standard forward search, even though two searches are performed instead
of one.

A \emph{bidirectional index} of a text allows one to extend the current
string  both left and right, that is, compute the positions of either
 or  from the positions of .
Note that a bidirectional
index allows forward and backward searches to alternate, which will be
crucial for our purposes. 
Lam et al.~\cite{LamLTWWY09} showed how the FM-index can be made
bidirectional. Other succinct bidirectional indexes were given
in~\cite{RussoEtAlAlgorithms09,SchnattingerOG12,BelazzouguiCKM13}.
Using a bidirectional index, such as FM-index, forward and backward searches can be
performed in time linear in the number of enumerated strings.
Therefore, our main goal is to organize the search so that the number
of enumerated strings is minimized.

Lam et al.~\cite{LamLTWWY09} gave a new search algorithm, called
\emph{bidirectional search}, that utilizes the bidirectional property
of the index.
Consider the case , studied in~\cite{LamLTWWY09}.
In this case, the pattern is partitioned into three equal length parts,
.
There are now 6 cases to consider according to the placement of mismatches
within the parts:
011 (i.e.\ one mismatch in  and one mismatch in ),
101, 110, 002, 020, and 200.
The algorithm of Lam et al.~\cite{LamLTWWY09} performs three searches
(illustrated in Figure~\ref{fig:tries}):
\begin{enumerate}
\item A forward search that allows no mismatches when processing
characters of , and 0 to 2 accumulated mismatches when processing 
characters of  and .
This search handles the cases 011, 002, and 020 above.
\item
A backward search that allows no mismatches when processing characters
of ,
0 to 1 accumulated mismatches when processing characters of , and
0 to 2 accumulated mismatches when processing characters of .
This search handles the cases 110 and 200 above.
\item
The remaining case is 101.
This case is handled using a \emph{bidirectional search}.
It starts with a forward search on string  that
allows no mismatches when processing characters of , and
0 to 1 accumulated mismatches when processing the characters of .
For each string  of length  enumerated by the forward search whose
Hamming distance from  is exactly 1, a backward search for  is performed
by extending  to the left, 
allowing one additional mismatch.
In other words, the search allows 1 to 2 accumulated mismatches when processing the
characters of .
\end{enumerate}
\begin{figure}
\centering
\subfigure[Forward search]{\includegraphics[scale=0.5]{trie-Sf}}
\subfigure[Backward search]{\includegraphics[scale=0.5]{trie-Sb}}
\subfigure[Bidirectional search\label{fig:trie-Sbd}]
 {\includegraphics[scale=0.5]{trie-Sbd}}
\caption{The tries representing the searches of Lam et al.\ for binary
alphabet , search string ,
and number of errors .
Each trie represents one search and assumes that all the enumerated substrings exist in the text .
In an actual search on a specific , each trie contains of a subset of
the nodes, depending on whether the strings of the nodes in the trie appear
in .
A vertical edge represents a match, and a diagonal edge represents a mismatch.
\label{fig:tries}}
\end{figure}

We now give a formal definition for the above.
Suppose that the pattern  is partitioned into  parts.
A \emph{search} is a triplet of strings 
where  is a permutation string of length  over ,
and  are strings of length  over .
The string  indicates the order in which the parts of  are processed,
and thus it must satisfy the following \emph{connectivity property}:
For every ,
 is either  or .
The strings  and  give upper and lower bounds on the number of
mismatches:
When the -th part is processed, the number of accumulated mismatches
between the active strings and the corresponding substring of  must be between
 and .
Formally, for a string  over integers,
the \emph{weight} of  is .
A search  \emph{covers} a string  if
 for all 
(assuming ).
A \emph{-mismatch search scheme}  is a collection of
searches such that for every string  of weight ,
there is a search in  that covers .
For example, the 2-mismatch scheme of Lam et al.\ consists of
searches , ,
and . We denote this scheme by .

In this work, we introduce two types of improvements over the search scheme of Lam et al.
\paragraph{Uneven partition.}
In , search  enumerates more strings
than the other two searches, as it allows 2 mismatches on the second
processed part
of , while the other two searches allow only one mismatch.
If we increase the length of  in the partition of , the
number of strings enumerated by  will decrease, while
the number of strings enumerated by the two other searches will increase.
We show that for some typical parameters of the problem, the decrease in the
former number is larger than the increase of the latter number,
leading to a more efficient search. 

\paragraph{More parts.}
Another improvement can be achieved using partitions with  or
more parts, rather than  parts.
\iffull
We explain in Section~\ref{sec:uneven} why such partitions can
reduce the number of enumerated strings.
\fi

\section{Analysis of search schemes}\label{sec:analysis}
In this section we show how to estimate the performance of a given search scheme
. Using this technique, we \iffull first explain why an
uneven partition can lead to a better performance, and then \fi present a
dynamic programming algorithm for designing an
optimal partition of a pattern. 

\subsection{Estimating the efficiency of a search scheme}\label{sec:estimation}
To measure the efficiency of a search scheme, we estimate the number of strings
enumerated by all the searches of .
We assume that performing single steps of
forward, backward, or bidirectional searches takes the same amount of time.
It is fairly straightforward to extend the method of this section to the
case when these times are not equal.
Note that the bidirectional index of Lam et al.~\cite{LamLTWWY09}
reportedly spends slightly more time (order of 10\%) on forward search
than on backward search. 

For the analysis, we assume that characters
of  and  are randomly drawn uniformly and independently
from the alphabet.
We note that it is possible to extend the method of this section to a
non-uniform distribution.
For more complex distributions,
a Monte Carlo simulation can be applied
which, however, 
requires much more time than
the method of this section.

\subsubsection{Hamming distance}\label{sec:hamming-estimation}

Our approach to the analysis is as follows.
Consider a fixed search , and the trie representing this search 
(see Figure~\ref{fig:tries}).
The search enumerates the largest number of strings when
the text contains all strings of length  as substrings.
In this case, every string that occurs in the trie is enumerated.
For other texts, the set of enumerated strings is a subset of the set of strings
that occurs in trie.
The expected number of strings enumerated by  on random  and  is equal
to the sum over all nodes  of the trie of the probability
that the corresponding string appears in .
We will first show that this probability depends only on the depth of 
(Lemmas~\ref{lem:substring-probability} and~\ref{lem:Ali} below).
Then, we will show how to count the number of nodes in each level of the
trie.

Let  denote the probability that a random string of length 
is a substring of a random string of length , where the characters
of both strings are randomly chosen uniformly and independently
from an alphabet of size .
The following lemma gives an approximation for  with
a bound on the approximation error.
\begin{lemma}\label{lem:substring-probability}
.
\end{lemma}
\begin{proof}
Let  and  be random strings of length  and , respectively.
Let  be the event that  appears in  at position .
The event  is independent of the events
, where
.
By the Chen-Stein method~\cite{Chen75,Barbour92},

where .
Clearly, .
It is also easy to verify that .
Therefore,
.
The lemma follows since  
for all .
\end{proof}
The bound in Lemma~\ref{lem:substring-probability} on the error of the
approximation of  is large if  is small,
say .
In this case, we can get a better bound by observing that
, where .
Since ,
we obtain that .

Let  denote the expected number of strings
enumerated when performing a search  on a random
text of length  and random pattern of length , where
 is a partition of the pattern and
 is the alphabet size 
(note that  is not a parameter for  since the value
of  is implied from ).
For a search scheme , .

Fix , , , and .
Let  be the set of enumerated strings of length 
when performing search  on a random pattern of length ,
partitioned by ,
and a text  containing all strings of length at most  as
substrings.
Let  be the -th element of 
(an order on  will be defined in the proof of the next lemma).
Let , namely, the number of nodes
at depth  in the trie that represents the search .
Let  be the string containing the characters of  according to the
order they are read by the search.
In other words,  is the character such that 
every node at depth  of the trie has an edge to a child
with label .

\begin{lemma}\label{lem:Ali}
For every  and , the string  is a random string with uniform
distribution.
\end{lemma}
\begin{proof}
Assume that the alphabet is .
Consider the trie that represents the search .
We define an order on the children of each node of the trie as follows:
Let  be a node in the trie with depth .
The label on the edge between  and its leftmost child is 
.
If  has more than one child, the labels on the edges to the rest of the
children of , from left to right,
are .
We now order the set  according to the nodes of depth
 in the trie. Namely, let 
be the nodes of depth  in the trie, from left to right.
Then,  is the string that corresponds to .
We have that  for
,
where  is the rank of the node of depth  on the path from the
root to  among its siblings.
Now, since each letter of  is randomly chosen uniformly and independently
from the alphabet, it follows that each letter of  has uniform
distribution and the letters of  are independent.
\end{proof}

By the linearity of the expectation,

By Lemma~\ref{lem:Ali} and Lemma~\ref{lem:substring-probability},

We note that the bounds on the approximation errors of 
are small, therefore even when these bounds are multiplied by 
and summed over all , the resulting bound on the error is small.

In order to compute the values of , we give some definitions.
Let  be the number of strings in 
of length  with Hamming distance  to the prefix of 
of length .
For example, consider search  and partition of 
a pattern of length 6 into 3 parts of length 2,
as shown in Figure~\ref{fig:trie-Sbd}.
Then, , ,
 (strings baabb and babab),
and  (strings baaba and babaa).

Let  be a string obtained from  by replacing each
character  of  by a run of  of length
,
where  is the length of the -th part in the partition
.
Similarly,  is a string obtained from
 by replacing each character   by a run of  of length
, and  is defined analogously.
In other words, values 
give lower and upper bounds on the number of allowed mismatches for an
enumerated string of length .
For example, for  and the partition  defined above,
, ,
and .

Values  are given by the following recurrence.

For a specific search, a closed formula can be given for .
If a search scheme  contains two or more searches with the same
-strings, these searches can be merged in order to eliminate the
enumeration of the same string twice or more.
It is straightforward to modify the computation of  to account for this optimization.


Consider equation~(\ref{eq:a}).
The value of the term  is very close to 1
for .
When , the value of this term decreases
exponentially.
Note that  increases exponentially, but the base of the exponent
of  is  whereas the base of  is
.
We can then approximate  with function
 defined by

where  is a constant chosen so that 
is sufficiently small.

From the above formulas we have that the time complexities for
computing  and
 are
 and , respectively.


\subsubsection{Edit distance}\label{sec:estimation-edit}
We now show how to estimate the efficiency of a search scheme for the edit distance.


We define  analogously to 
in the previous section, except that edit distance errors are allowed.
Fix a search  and a partition .
We assume without loss of generality that  is the identity permutation.
Similarly to the Hamming distance case,
define  to be the set of enumerated strings of length 
when performing the search  on a random pattern of length ,
partitioned by ,
and a text  containing all the strings of length at most  as
substrings.
Unlike the case of Hamming distance, here the strings of  are not
distributed uniformly.
Thus, we do not have the equality
.
We will use  as an approximation
for , but we do not have an estimation
on the error of this approximation.
Note that in the Hamming distance case, the sizes of the sets  are
the same for every choice of the pattern, whereas this is not true for
edit distance.
We therefore define  to be the number of enumerated strings
of length  when performing the search  on a pattern  of length ,
partitioned by , and a text .
We also define  to be the expectation of ,
where  is chosen randomly.

We next show how to compute values .
We begin by giving an algorithm for computing  for
some fixed .
Build a non-deterministic automaton  that recognizes the
set of strings that are within edit distance at most  to , and the
locations of the errors satisfy the requirements of the
search~\cite{MihovSchulz,KarkkainenN07}
(see Figure~\ref{fig:NFA} for an example).
For a state  and a string , denote by  the set of all
states  for which there is a path in  from  to 
such that the concatenation of the labels on the path is equal to .
For a set of states  and a string ,
.
Clearly,  is equal to the number of strings  of length 
for which ,
where  is the initial state.
Let  be the number of strings  of length  for which
.
The values of  can be computed using dynamic programming
and the following recurrence.

The values  gives the values of
, since by definition,

where the summation is done over all non-empty sets of states .

Note that for a string  of length , set 
is a subset of a set of  states that depends on . This set,
denoted , includes the -th state in the first row of the
automaton, states  on the second row,
states  on the third row, and so on (see Figure~\ref{fig:NFA}).
The size of  is .
Therefore, the number of sets  for which  is at most
.
If  is small enough, a state can be encoded in one machine word,
and the computation of  can be done in constant time using
precomputed tables.
Thus, the time for computing all values of  is
.

\begin{figure}
\centering
\includegraphics[scale=0.75]{NFA}
\caption{Non-deterministic automaton corresponding
to the search  and pattern  over the alphabet
.
A path from the initial state  to the state in the -th row and
-column of the automaton correspond to a string with edit distance
 to .
The nodes of the set  are marked by gray.\label{fig:NFA}}
\end{figure}

Now consider the problem of computing the values of .
Observe that for , the value of
 depends on the characters of ,
and does not depend on the rest of the characters of .
Our algorithm is based on this observation.
For an integer , a set , and
a string  of length , define

Then,

where , and
 is a string satisfying 
(the rest of the characters of  can be chosen arbitrarily).

From the above, the time complexity for computing
 is
.
Therefore, our approach is practical only for small values of .

\iffull
\subsection{Uneven partitions}\label{sec:uneven}
In Section~\ref{sec:bidirectional}, we provided an informal explanation why
partitioning the pattern into unequal parts may be beneficial. 
We now provide a formal justification for this. 
To this end, we replace (\ref{eq:a2}) by an even simpler estimator of
:


As an example, consider scheme .
Denote by , ,  the lengths of the parts
in a partition  of  into 3 parts.
\ifextra
It is straightforward to give closed form formulas for the values of 
 for the searches of the scheme.
For example, for  we have

(if  then )
and therefore


For the backward search ,

(if  then 
)
and therefore

Therefore
\else
It is straightforward to give closed formulas for  for each search of .
For example,
\fi

where
,
,
, and
.
\ifextra
Similarly,

where
,
,
,
,
,
, and
.
\else
Similar formulas can be given for  and .
\fi
If , , and  are close to  and  then
 and an
equal sized partition is optimal in this case.
However, if , then

It is now clear why the equal sized partition is not optimal in this case.
The degree of  in the above polynomial is 3, while
the degrees of  and  are 2.
Thus, if , decreasing  and  by, say 1,
while increasing  by 2 reduces the value of the polynomial.
\fi

\subsection{Computing an optimal partition}
In this Section, we show how to find an optimal partition for a given
search scheme  and a given number of parts .
An optimal partition can be naively found by enumerating
all  possible partitions, and for each partition , computing
.
We now describe a more efficient dynamic programming algorithm.

We define an optimal partition to be a partition that maximizes
.
Let .
If , then any partition in which all parts are of size at least
 is an optimal partition.
Therefore, assume for the rest of this section that .
We say that a partition  is \emph{bounded} if the sizes of the parts
of  are at most .
If  is not bounded, we can transform it into a bounded
partition by decreasing the sizes of parts which are larger than 
and increasing the sizes of parts which are smaller that .
This transformation 
can only decrease the value of
.
Therefore, there exists an optimal partition which is bounded.
Throughout this section we will consider only bounded partitions.
For brevity, we will use the term partition instead of bounded partition.

Our algorithm takes advantage of the fact that the value of
 does not depend on the entire
partition , but only on the partition of a substring of 
of length  induced by .
More precisely, consider a fixed .
By definition,
 depends on
the values  (the number of nodes in
levels  in the trie that correspond to the search ).
From Section~\ref{sec:estimation}, these values depend on
the strings  and  which are fixed, and on the string
.
The latter string depends on ,
where  is the minimum index such that

and on the values 
.

The algorithm works by going over the prefixes of  in increasing length
order. For each prefix , it computes a set of partitions of 
such that at least one partition in this set can be extended to an optimal
partition of .
In order to reduce the time complexity, the algorithm needs to identify
partitions of  that cannot be extended into an optimal partition of .
Consider the following example.
Suppose that , ,  and ,
where the -strings of  are , ,
and , respectively.
Consider a prefix  of , and let
 be two partitions of ,
where the parts in  are of sizes 3,3,2,
and the parts in  are of sizes 4,2,2.
Note that  and  have the same number of parts,
and they induce the same partition on .
We claim that one of these two partitions is always at least as good
as the other for every extension of both partitions to a partition of .
To see this, let  denote a partition of  into
two parts, and consider the three searches of .

\begin{enumerate}
\item
For search  we have that 

for every , and 

for every .
It follows that the value of
 is the same
for every , and the value of 
 is the same
for every .
These two values can be equal or different.

\item
For the search  we have that 
.
It follows that

for all  and this common value does not depend on .

\item
For the search  we have that 

for every .
For example, if  is a partition of 
into parts of sizes 2,2 then 
.
It follows that

for every .
This common value depends on .

\end{enumerate}
We conclude that either 

for every ,
or 
for every .


We now give a formal description of the algorithm.
We start with some definitions.
For a partition  of a substring  of 
pattern , we define the following quantities:
 is the length of ,
 is the length of the last part of ,
 is the number of parts in ,
and  is the left-to-right rank of the part of
 containing .
Let  be the partition of 
 of  that is
composed from the first  parts of .
For the example above,
,
,
,
, and
 is a partition of  with
parts sizes .

For a partition  of a prefix  of ,
 is a set containing every search 
such that  appears before
 in the -string of .
If the length of  is less than  we define 
, and if  we define
.
For the example above, .

Let  be a partition of a substring
 of ,
and  be a partition of a substring .
We say that  and  are \emph{compatible}
if these partitions induce the same partition on the common substring
.
For example, the partition of  into parts of sizes 
is compatible with the partition of  into parts of sizes
.

\begin{lemma}\label{lem:optimal-1}
Let  be a partition of a prefix of  of length at least .
Let  be a search.
The value  is the same for every
partition  of  whose first  parts
match .
\end{lemma}
\begin{proof}
Let  be the index such that .
Since  appears before
 in string ,
from the connectivity property of  we have that
(1) Every value in  that appears before 
is at most .
In other words,  for every .
(2)  appear before
 in .
By the definition of ,
.
Therefore,  and 
.
Thus, string  and values

are the same for every partition  that satisfies the requirement of
the lemma.
\end{proof}

For a partition  of a prefix of  of length at least ,
define  to be 
,
where  is an arbitrary partition of  whose first
 parts match 
(the choice of  does not matter due to Lemma~\ref{lem:optimal-1}).
For a partition  of a prefix of  of length less than ,
.
Define


\begin{lemma}\label{lem:optimal-2}
Let  be a partition of a substring 
such that  and
.
Let  be an integer.
The value of  is the same for
every partition  of  with  parts
that is compatible with .
\end{lemma}
\begin{proof}
We assume  (the case
 is similar).
Since ,
the set  is the
same for every partition  of  with  parts
that is compatible with .
For a search  in this set,  appears before
 in ,
and  appears before .
Let , 
where  is an arbitrary partition of  whose first
 parts are the parts of .
We obtain that 
,
and the lemma follows.
\end{proof}

For  that satisfy the requirements of
Lemma~\ref{lem:optimal-2}, let  denote the value of
, where  is an arbitrary partition of
 with  parts that is compatible with .

For , , and a partition  of
 with at most  parts,
let  be the minimum value of ,
where  is a partition of  into  parts
that is compatible with .
\begin{lemma}\label{lem:optimal-3}
For , , and a partition  of
 with at most  parts,

where the minimum is taken over all partitions 
of a substring  of  that satisfy the following:
(1)  is compatible with ,
(2) ,
(3) ,
(4)  if .
\end{lemma}
An algorithm for computing the optimal partition follows from
Lemma~\ref{lem:optimal-3}.
The time complexity of the algorithm is
, where
 is time for
computing  values, and  is time
for computing  values. 

\section{Properties of optimal search schemes}\label{sec:design}

Designing an efficient search scheme for a given set of parameters consists
of
\begin{inparaenum}[(1)]
\item choosing a number of parts,
\item choosing searches, \item choosing a partition of the pattern.
\end{inparaenum}
While it is possible to enumerate all possible choices, and 
evaluate the efficiency of the resulting scheme using
Section~\ref{sec:estimation}, this is generally infeasible due to a large
number of possibilities.
It is therefore desirable to have a combinatorial characterization of
optimal search schemes.


The \emph{critical string} of a search scheme  is the
lexicographically maximal -string of a search in .
A search of  is \emph{critical} if its -string is equal to the
critical string of .
For example, the critical string of  is , and 
 is the critical search. 
For typical parameters, critical searches of a search scheme constitute
the bottleneck.
Consider a search scheme , and assume that the -strings
of all searches contain only zeros.
Assume further that the pattern is partitioned into 
equal-size parts.
Let  be the maximum index such that for every search 
and every ,  of  is no larger than
the number in position  in the critical string of .
From Section~\ref{sec:analysis}, the number of strings enumerated by a search
 depends mostly
on the prefix of the -string of 
of length .
Thus, if ,
a critical search enumerates an equal or greater number of strings than
a non-critical search.

We now consider the problem of designing a search scheme whose critical string
is minimal.
Let  denote the lexicographically minimal critical string of a
-mismatch search scheme that partitions the pattern into  parts.
The next theorems give the values of  and .
\iffull
We need the following definition.
A string over the alphabet of integers is called \emph{simple}
if it contains a substring of the form  for .
\begin{lemma}\label{lem:simple}
\begin{itemize}
\item[(i)] Every string  of weight  and length at least  is
  simple.
\item[(ii)] If  is a non-simple string of weight  and length  then
, , and  for all .
Moreover, there are no two consecutive 's in .
\end{itemize}
\end{lemma}
\begin{proof}
\emph{(i)}
The proof is by induction on .
It is easy to verify that the lemma holds for .
Suppose we proved the lemma for .
Let  be a string of weight  and length .
If  then by the induction hypothesis  is simple,
and therefore  is simple.
Suppose that .
Let  be the minimum index such that 
( must exist due to the assumption that ).
If  then we are done.
Otherwise, we can use the induction hypothesis on 
and obtain that  is simple.


\emph{(ii)}
Let  be a non-simple string of weight  and length .
If  then  has weight  and
length , and thus by
\emph{(i)} we obtain that 
is simple, contradicting the assumption that  is non-simple.
Similarly,  cannot be greater than .
For , if  then either  or
 satisfies the condition of 
\emph{(i)}.
Similarly, if  then either  or
 satisfies the condition of 
\emph{(i)}.
\end{proof}
\else
We omit the proofs due to space limitations.
\fi

We use the following notation.
For two integers  and ,  denotes the string
 if , and the empty string if .
Moreover,  denotes the string  if ,
and the empty string if .

\begin{theorem}\label{thm:alpha}
 for every odd , and
 for every even .
\end{theorem}
\iffull
\begin{proof}
We first give an upper bound on  for odd .
We build a search scheme as follows.
The scheme contains searches 

for all  and , which cover all simple strings
of weight  and length .
In order to cover the non-simple strings, the scheme contains the following
searches.
\begin{enumerate}
\item  for every odd 
(for , the -string is ).
\item  for every odd 
(for , the -string is ).
\end{enumerate}

Let  be a non-simple string of weight  and length .
By Lemma~\ref{lem:simple},  where each of  and 
is either string  or empty string, and each  is either
, , , or .
A string  is called a \emph{block} of type~1, 2, or~3 if  is
equal to , , or , respectively.
Let  be the blocks of type~1 and type~2, from left to right.

We consider several cases.
The first case is when  and  are empty strings, and  is of type~1.
Since the weight of  is odd, it follows that  is odd.
If  has no other blocks,  is covered by search
, where  is the index in  in which  starts.
Otherwise, if  is of type~1, then  is covered by search
, where  is the index in  in which  starts,
and  is the index in which the first block to the right of  starts
(this block is either , or a block of type~3).
Now suppose that  is of type~2.
If  is of type~2, then  is covered by search
, where  is the index in  in which  ends,
and  is the index in which the first block to the left of  ends.
By repeating these arguments, we obtain that  is covered unless
the types of  alternate between type~1 and type~2.
However, since  is odd,  is of type~1, and in this case
 is covered by ,
where  is the index in  in which  starts,
and  is the index in which the first block to the left of  ends.

Now, if  is empty string and , define a string .
By the above,  is covered by some search .
Then,  is covered by either  or .
The same argument holds for the case when .
The proof for the case when  is of type~2 is analogous and thus omitted.

The lower bound on  for odd  is obtained by considering
the string . The -string of a search that covers 
must be at least .

We next give an upper bound on  for even .
We define -mismatch search schemes  recursively.
For ,  consists of a single search .
For ,  consists of the following searches.
\begin{enumerate}
\item For every search  in ,
 contains a search
.
\item A search
.
\item A search
.
\end{enumerate}
Note that the critical string of  is 
corresponding to item 1 above.
We now claim that all number strings of length  and weight at most 
are covered by the searches of .
The proof is by induction on .
The base  is trivial.
Suppose the claim holds for .
Let  be a number string of length  and weight .
If , then  is covered by either  or .
Otherwise, the weight of  is at most
.
By induction,
 is covered by some search
. Then search  covers .

To prove that  for even ,
consider the string .
It is easy to verify that the -string of a search that covers 
must be at least .
\end{proof}
\fi
\begin{theorem}\label{thm:alpha2}
 for every .
\label{theorem2}
\end{theorem}
\iffull
\begin{proof}
We first give an upper bound on .
We build a -mismatch search scheme  that contains
searches

for all  and .
Let  be a string of weight  and length .
By Lemma~\ref{lem:simple} there are indices  and  such that
, and therefore  is covered by
.

The lower bound is obtained from the string .
It is easy to verify that the -string of a search that covers 
must be at least .
\end{proof}
\fi

An important consequence of Theorems~\ref{thm:alpha} and
\ref{theorem2} is that for some typical cases, partitioning the pattern
into  parts brings an advantage over  parts. 
For , for example, we have  while .
Since the second element of  is smaller than that
of , a 4-part search scheme potentially enumerates
less strings than a 3-part scheme.
On the other hand, the average length of a part is smaller when using 4 parts,
and therefore the branching occurs earlier in the searches of a 4-part scheme.
The next section shows that for some parameters, -part schemes
outperform -part schemes,
while for other parameters the inverse occurs.

\section{Case studies}\label{sec:experiments}
In this Section, we provide results of several computational
experiments we have performed to analyse practical applicability
of our techniques. 

We designed search schemes for 2, 3 and 4 errors (given in Appendix) using a
greedy algorithm. The algorithm iteratively adds searches to a search
scheme. 
At each step, the algorithm considers the uncovered string  of weight 
such that the lexicographically minimal -string that covers 
is maximal.
Among the searches that cover  with minimal -string, a search that
covers the maximum number of uncovered strings of weight  is chosen.
The -string of the search is chosen to be lexicographically maximal among
all possible -string that do not decrease the number of uncovered strings.
For each search scheme and each choice of parameters, we computed an
optimal partition. 

\subsection{Numerical comparison of search schemes}
\label{subsec:experimenttheor}
We first performed a comparative estimation of the efficiency of
search schemes using the method of Section~\ref{sec:hamming-estimation}
(case of Hamming distance). More precisely, for a given search scheme ,
we estimated the number of strings
 enumerated during the search. 

Results for  mismatches are given in Table~\ref{tab:sigma4} and
Table~\ref{tab:sigma30} for -letter and -letter alphabets
respectively.
Table~\ref{tab:nonuniform} contains estimations for nonuniform letter
distribution.
Table~\ref{tab:k3} contains estimations for 
mismatches for -letter alphabet.


We first observe that our method provides an advantage
only on a limited range of pattern lengths. This conforms to our analysis 
\iffull
(see Section~\ref{sec:uneven})
\else
(details omitted due to lack of space)
\fi
that implies that our schemes can bring an improvement when  is smaller than
 approximately. 
When  is small, Tables~\ref{tab:sigma4}--\ref{tab:k3} suggest that
using more parts of unequal size can
bring a significant improvement. 
For big alphabets (Table~\ref{tab:sigma30}),
we observe a larger gain in efficiency,
due to the fact that values 
 (see equation~(\ref{eqn:nl})) grow faster when the
alphabet is large, and thus a change in the size of parts can
have a bigger influence on these values.
Moreover, if the probability distribution of letters in both the text and
the pattern is nonuniform,
then we obtain an even larger gain (Table~\ref{tab:nonuniform}), since in this case, the
strings enumerated during the search have a larger probability to
appear in the text than for the uniform distribution. 


For  mismatches and  letters (Table~\ref{tab:k3}), 
we observe a smaller gain, and even a loss for pattern lengths  and
 when shifting from  to  parts.
This is explained by Theorem~\ref{thm:alpha} showing the
difference of critical strings between odd and even numbers of
errors. 
Thus, for  mismatches and  parts, the
critical string is  
while for  parts it is . 
When patterns are not too small, the latter does not lead to an improvement strong enough
to compensate for the decrease of part length. 
Note that the situation is different for even number of errors, where incrementing the number of parts from
 to  leads to transforming the critical strings from
 to . 

Another interesting observation is that with  parts, obtained
optimal partitions have equal-size parts, as the -strings
of all searches of the 4-part scheme are all the same (see Appendix). 



These estimations suggest that our techniques can 
bring a
significant gain in efficiency for some parameter ranges, however the
design of a search scheme should be done carefully for each specific
set of parameters.

\begin{table}[!tb]
\caption{Values of  for -mismatch
search schemes, for different pattern lengths . 
Second column corresponds to 
search scheme  with three equal-size parts, 
the other columns show results for unequal partitions and/or more
parts. The partition used is shown in the second sub-column. 
\label{tab:sigma4}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
 & 3 equal & \multicolumn{2}{|c|}{3 unequal} &
\multicolumn{2}{|c|}{4 unequal} & \multicolumn{2}{|c|}{5 unequal} \\
\hline
24 & 1197 &  1077 & 9,7,8    &  959 & 7,4,4,9   &  939 & 7,1,6,1,9 \\
36 &  241 &   165 & 15,10,11 &  140 & 12,5,7,12 &  165 & 11,1,9,1,14 \\
48 &   53 &    53 & 16,16,16 &   51 & 16,7,9,16 &   53 & 16,1,15,1,15 \\
\hline
\end{tabular}
\end{table}
\begin{table}[!tb]
\caption{Values of  for -mismatch
search schemes.\label{tab:sigma30}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
 & 3 equal & \multicolumn{2}{|c|}{3 unequal} &
\multicolumn{2}{|c|}{4 unequal} & \multicolumn{2}{|c|}{5 unequal} \\
\hline
15 &  846 &  286 & 6,4,5 &  231 & 5,2,3,5 &  286 & 5,1,3,1,5 \\
18 &  112 &  111 & 7,6,5 &   81 & 6,2,4,6 &  111 & 6,1,4,1,6 \\
21 &   24 &   24 & 7,7,7 &   23 & 7,3,4,7 &   24 & 7,1,6,1,6 \\
\hline
\end{tabular}
\end{table}
\begin{table}[!tb]
\caption{Values of  for -mismatch
search schemes, using a non-uniform letter distribution (one letter with
probability  and the rest with probability 
each).\label{tab:nonuniform}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
 & 3 equal & \multicolumn{2}{|c|}{3 unequal} &
\multicolumn{2}{|c|}{4 unequal} & \multicolumn{2}{|c|}{5 unequal} \\
\hline
24 & 3997 &  3541 & 10,8,6   & 3592 & 6,7,1,10  & 3541 & 6,1,7,1,9 \\
36 &  946 &   481 & 16,10,10 &  450 & 11,6,6,13 &  481 & 10,1,9,1,15 \\
48 &  203 &   157 & 18,15,15 &  137 & 16,7,9,16 &  157 & 15,1,14,1,17\\
\hline
\end{tabular}
\end{table}
\begin{table}[!tb]
\caption{Values of  for -mismatch
search schemes. Best partitions obtained for 4 parts are equal.\label{tab:k3}}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & \multicolumn{2}{|c|}{4 equal/unequal} & \multicolumn{2}{|c|}{5 unequal} \\
\hline
24 & 11222 & 6,6,6,6 &  8039 & 4,6,5,1,8 \\
36 &   416 & 9,9,9,9 &   549 & 6,11,5,1,13 \\
48 &   185 & 12,12,12,12 &   213 & 11,11,11,1,14 \\
\hline
\end{tabular}
\end{table}

\subsection{Experiments on genomic data}\label{subsec:experimentpract}

To perform large-scale experiments on genomic sequences, we
implemented our method using the {\sc 2BWT} library
provided by~\cite{LamLTWWY09}
(\url{http://i.cs.hku.hk/2bwt-tools/}). We then experimentally compared
different search schemes, both in terms of running time and average
number of enumerated substrings. Below we only report 
running time, as in all cases, the number of enumerated substrings
produced very similar results. 

\begin{sloppypar}
The experiments were done on the sequence of human chromosome
14 (\emph{hr14}). The sequence is  long, with nucleotide distribution
29\%, 21\%, 21\%, 29\%.
Searched patterns were generated as i.i.d.\ sequences. For every search
scheme and pattern length, we ran  pattern searches for Hamming distance
and  searches for the edit distance.
\end{sloppypar}

\subsubsection{Hamming distance}

For the case of 2 mismatches, we implemented the 3-part and 4-part schemes
(see Appendix), as well as their equal-size-part
versions for comparison. For each pattern length, we computed an
optimal partition, taking into account a non-uniform distribution of
nucleotides. Results are presented in Table~\ref{tab:times2}.

Using unequal parts for 3-part schemes yields a notable time decrease 
for patterns of length  and  (respectively, by 24\% and 16\%). 
Furthermore, we observe that
using unequal part lengths for 4-part schemes is beneficial as
well. For pattern lengths  and , we obtain a speed-up by
27\% and 28\% respectively.
Overall, the experimental results are consistent with numerical
estimations of Section~\ref{subsec:experimenttheor}. 

For the case of 3 mismatches, we implemented 4-part and 5-part schemes
from Appendix, as well as their equal part
versions for comparison. Results (running time) are presented in
Table~\ref{tab:times3}. In accordance with estimations of
Section~\ref{subsec:experimenttheor}, here we observe a clear
improvement only for pattern length  and not for longer
patterns.

\begin{table}[!tb]
\caption{Total time (in sec) of search for  patterns
in \emph{hr14}, up to 2 mismatches.
2nd column contains time obtained on partition into three equal-size parts. 
The 3rd (respectively 4th and 5th) column shows the running time
respectively for the -unequal-parts, 
-equal-parts and
-unequal-parts searches, together with their ratio (\%) to the
corresponding -equal-parts value.
\label{tab:times2}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & 3 equal & \multicolumn{2}{|c|}{3 unequal} & 4 equal & \multicolumn{2}{|c|}{4 unequal} \\
\hline
15 & 24.8 & 25.4 (102\%) & 6,6,3 & 25.3 (102\%) & 25.3 (102\%) & 3,5,1,6 \\
24 & 5.5 & 4.2 (76\%)  & 10,7,7 & 5.2 (95\%) & 4.0 (73\%) & 7,4,4,9 \\
33 & 1.73  & 1.45 (84\%)   & 13,10,10 & 2.07 (120\%)  & 1.25 (72\%) & 11,5,6,11 \\
42 & 0.71  & 0.71 (100\%)  & 14,14,14 & 1.24 (175\%)  & 0.82 (115\%) & 14,6,8,14 \\
\hline
\end{tabular}
\end{table}



\begin{table}[!tb]
\caption{Total time (in sec) of search for  patterns
  in \emph{hr14}, up to 3 mismatches.
\label{tab:times3}}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
m & 4 equal & 5 equal & \multicolumn{2}{|c|}{5 unequal} \\
\hline
15 & 241 & 211 (86\%) & 206 (85\%) & 2,3,5,1,4 \\
24 & 19.7 & 26.7 (136\%) & 19.6 (99\%) & 2,9,3,1,9 \\
33 & 4.3 & 6.9 (160\%) & 4.7 (109\%) & 6,9,6,1,11 \\
42 & 1.85 & 2.52 (136\%) & 2.05 (111\%) & 10,10,9,1,12 \\
51 & 1.07 & 1.57 (147\%) & 1.06 (99\%) & 12,13,12,1,13 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Edit distance}

In the case of edit distance, along with the search schemes for 2 and
3 errors from the previous section, we also implemented search schemes
for 4 errors (see Appendix). 
Results are shown in Table~\ref{tab:times2edit} (2 errors),
Table~\ref{tab:times3edit} (3 errors) and
Table~\ref{tab:times4edit} (4 errors). 


\begin{table}[!tb]
\caption{Total time (in sec) of search for  patterns
in \emph{hr14}, up to 2 errors (edit distance).
\label{tab:times2edit}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & 3 equal & \multicolumn{2}{|c|}{3 unequal} & 4 equal & \multicolumn{2}{|c|}{4 unequal} \\
\hline
15 & 11.5 & 11.4 (99\%) & 6,6,3 & 10.9 (95\%) & 11.1 (97\%) & 3,5,1,6 \\
24 & 2.1 & 1.3 (62\%)  & 11,5,8 & 1.5 (71\%) & 1.0 (48\%) & 7,4,4,9 \\
33 & 0.34  & 0.22 (65\%)   & 13,10,10 & 0.35 (103\%)  & 0.19 (56\%) & 11,5,6,11 \\
42 & 0.08  & 0.08 (100\%)  & 14,14,14 & 0.18 (225\%)  & 0.08 (100\%) & 14,6,8,14 \\
\hline
\end{tabular}
\end{table}



\begin{table}[!tb]
\caption{Total time (in sec) of search for  patterns
  in \emph{hr14}, up to 3 errors (edit distance).
\label{tab:times3edit}}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
m & 4 equal & 5 equal & \multicolumn{2}{|c|}{5 unequal} \\
\hline
15 & 233 & 174 (75\%) & 168 (72\%) & 2,2,6,1,4 \\
24 & 13.5 & 13.2 (98\%) & 10.8 (80\%) & 3,8,3,1,9 \\
33 & 0.74 & 1.81 (245\%) & 1.07 (145\%) & 5,10,5,1,12 \\
42 & 0.28 & 0.45 (161\%) & 0.37 (132\%) & 9,10,9,1,13 \\
51 & 0.13 & 0.24 (185\%) & 0.14 (108\%) & 12,12,12,1,14 \\
\hline
\end{tabular}
\end{table}

\begin{table}[!tb]
\caption{Total time (in sec) of search for  patterns
in \emph{hr14}, up to 4 errors (edit distance).
\label{tab:times4edit}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & 5 equal & \multicolumn{2}{|c|}{5 unequal} & 6 equal & \multicolumn{2}{|c|}{6 unequal} \\
\hline
15 & 4212 & 3222 (76\%)  & 3,1,8,1,2 & 4028 (96\%) & 3401 (81\%) & 2,2,1,7,1,2 \\
24 & 145 & 133 (92\%)  & 7,3,5,1,8 & 131 (90\%) & 113 (78\%) & 2,7,3,4,5,3 \\
33 & 6.5  & 5.8 (89\%)   & 8,7,5,8,5 & 6.6 (102\%)  & 5.1 (78\%) & 4,8,6,3,5,7 \\
42 & 1.66  & 1.16 (70\%)  & 12,8,7,8,7 & 1.51 (91\%)  & 1.17 (70\%) & 7,8,8,5,2,12 \\
51 & 0.60  & 0.49 (82\%)  & 13,11,9,9,9 & 0.74 (123\%)  & 0.54 (90\%) & 9,10,9,9,1,13 \\
60 & 0.28  & 0.24 (86\%)  & 14,13,11,11,11 & 0.44 (157\%)  & 0.28 (117\%) & 11,12,11,11,1,14 \\
\hline
\end{tabular}
\end{table}

For  errors, we observe up to two-fold speed-up for pattern lengths
,  and . For the case of 
errors, the improvement is achieved for pattern lengths  and 
(respectively 28\% and 20\%). Finally, for  errors, we obtain a significant speed-up (18\%
to 30\%) for pattern lengths between  and .




\subsubsection{Experiments on simulated 
genomic
  reads}\label{subsubsec:experimentgenomic}

\begin{table}[!tb]
\caption{Total time (in sec) 
of search for  reads in \emph{hr14}, up to 4 errors.
First row corresponds to read set with constant error rate .
Second row corresponds to read set with error rate increasing from  to .
\label{tab:expgenomic}}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & 5 equal & 6 equal & \multicolumn{2}{|c|}{6 unequal} \\
\hline
100 & 247  & 250 (101\%)  & 283 (115\%) & 20,20,20,19,1,20 \\
100 & 415  & 367 (88\%)  & 350 (84\%) & 20,20,20,19,1,20 \\
\hline
\end{tabular}
\end{table}


Experiments of Section~\ref{subsec:experimentpract} have been made with
random patterns. In order to make experiments closer to 
the practical bioinformatic setting 
occurring in mapping genomic reads to their
reference sequence, we also experimented with patterns simulating
reads issued from genomic sequencers. For that, 
we generated realistic single-end reads of length  (typical length of {\sc
  Illumina} reads) from \emph{hr14} 
using {\sc dwgsim} read simulator
(\url{https://github.com/nh13/DWGSIM}). Two sets of reads were
generated using two different error rate
values (parameter \texttt{-e} of {\sc dwgsim}): \texttt{0.03} for the
first dataset and \texttt{0.0-0.03} for the second one. This means
that in the first set, error probability is uniform over the read
length, while in the second set, this probability gradually increases from 
to  towards the right end of the read. The latter simulates
the real-life situation occurring with current sequencing
technologies including {\sc Illumina}. 


The results are shown in Table~\ref{tab:expgenomic}. As expected, due
to a large pattern length, our schemes did not produce a speed-up for
the case of constant error rate. 
Interestingly however,
for the case of non-uniform distribution of errors, our schemes showed
a clear advantage. 
This illustrates another possible benefit of our techniques: they are
better adapted to a search for patterns with non-uniform
distribution of errors, which often occurs in practical situations
such as mapping genomic reads. 

\section{Conclusions} This paper can be seen as the first step towards an automated design
of efficient search schemes for approximate string matching, based on
bidirectional indexes. More research has to be done in order to allow
an automated design of optimal search schemes. 
It would be very interesting to study an approach when a search scheme
is designed simultaneously with the partition, rather than
independently as it was done in our work. 

We expect that search schemes similar to those studied in this paper
can be applied to hybrid approaches to approximate matching (see Introduction), as well
as possibly to other search strategies. 





\paragraph{Acknowledgements.}
GK has been supported by the ABS2NGS grant of the French government
(program \emph{Investissement d'Avenir}) as well as by a EU Marie-Curie
Intra-European Fellowship for Carrier Development.
KS has been supported by the \emph{co-tutelle} PhD fellowship of the French
government.
DT has been supported by ISF grant 981/11.
\bibliographystyle{plain}
\bibliography{string-index,string-read-alignment}

\newpage
\section*{Appendix}
The following search schemes were used in experiments described in
Section~\ref{sec:experiments}. 

\noindent
For 2 mismatches or errors:
\begin{enumerate}
\item Slightly modified scheme .
The searches are:
, ,
and .
Note that the -string of  is  and not  as in .
While   and  have the same efficiency for equal-size
partitions, this in not the case for unequally sized parts.
\item -part scheme with searches
, , \linebreak[4], and
.
\end{enumerate}

\vspace{5mm}
\noindent
For 3 mismatches or errors:
\begin{enumerate}
\item -part scheme with searches 
, , \linebreak[4], and
.
\item -part scheme with searches
, , \linebreak[4], and
.
\end{enumerate}

\vspace{5mm}
\noindent
For 4 mismatches or errors:
\begin{enumerate}
\item -part scheme with searches
, , 
\linebreak[4], , 
, \linebreak[4], 
 and .

\item -part scheme with searches
, , 
, , 
, , 
, ,
 and .
\end{enumerate}


\end{document}
