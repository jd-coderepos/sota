\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ruled]{algorithm2e}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\urlstyle{same}





\iccvfinalcopy 

\def\iccvPaperID{2640} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{SPICE: Semantic Pseudo-labeling for Image Clustering}

\author{Chuang Niu \ \ and \ \  Ge Wang\\
Rensselaer Polytechnic Institute\\
110 8th Street, Troy, New York 12180, USA\\
{\tt\small \{niuc, wangg6\}@rpi.edu}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
    This paper presents \textbf{SPICE}, a \textbf{S}emantic \textbf{P}seudo-labeling framework for \textbf{I}mage \textbf{C}lust\textbf{E}ring.
    Instead of using indirect loss functions required by the recently proposed methods, SPICE generates pseudo-labels via self-learning and directly uses the pseudo-label-based classification loss to train a deep clustering network.
    The basic idea of SPICE is to synergize the discrepancy among semantic clusters, the similarity among instance samples, and the semantic consistency of local samples in an embedding space to optimize the clustering network in a semantically-driven paradigm.
    Specifically, a semantic-similarity-based pseudo-labeling algorithm is first proposed to train a clustering network through unsupervised representation learning. Given the initial clustering results, a local semantic consistency principle is used to select a set of reliably labeled samples, and a semi-pseudo-labeling algorithm is adapted for performance boosting.
    Extensive experiments demonstrate that SPICE clearly outperforms the state-of-the-art methods on six common benchmark datasets including STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet. On average, our SPICE method improves the current best results by about \textbf{10\%} in terms of  adjusted rand index, normalized mutual information, and clustering accuracy. Codes are available \href{https://github.com/niuchuangnn/SPICE}{online}.

\end{abstract}

\section{Introduction}

Image clustering aims to group images into different clusters without human annotations, and is an essential task in unsupervised learning especially for computer vision. The characterization of similarity and discrepancy is at the core of all clustering methods. Particularly, the similarity measurement of images depends on the representation features that are semantically and contextually dependent. Given an imperfect representation, how to effectively cluster samples with high-level semantics is another challenge, especially for clustering images of complicated contents.

\begin{figure}[bt!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/idea.png}
    \caption{Semantic relevance in the embedding space. (a) Global semantic similarity v.s. local instance similarity, where each point denotes a sample in the embedding space, stars denote the cluster centers, different colors denote different clusters, and black circles and lines indicate similar samples; (b) local semantic consistency to determine reliable and unreliable samples; and (c) neighboring samples of different semantics, where the first image is the query image and the other five images are nearest images with closest embedding features provided by SCAN \cite{scan}.}
    \label{fig_general}
\end{figure}

By combining deep learning techniques with traditional clustering algorithms, some deep clustering methods were proposed to learn representation features and perform clustering simultaneously and alternatively \cite{Xie2016, LI2018161, DCN2016, DeepCluster2017, Zhang_2019_CVPR, DEPICT2017, VaDE2017, GMVAE, DASC2018}. However, these encoder-decoder-based methods hardly capture discriminitative features of complex images.
Thus, a number of methods were proposed to learn discriminitative label features under various constraints \cite{DAIC2017,IIC2019,Wu_2019_ICCV,gatcluster,gatcluster, cc}. However, these methods have limited performance when directly using the label features to measure the similarity among samples. This is because the category-level features lose too much instance-level information to accurately measure the relations of instances.
Recently, SCAN \cite{scan} was proposed to leverage the embedding features of a representation learning model to search for similar samples across the whole dataset, and then encourage the model to output the same labels for similar instances, achieving significantly better results.
However, the local nearest samples in the embedding space do not always have the same semantics especially when the samples lie around the borderlines of different clusters as shown in Fig. \ref{fig_general}-(a) and (c), which may compromise the performance.
Essentially, these label-feature-based methods aim to train a classification model with an objective function of multiple indirect losses mainly based on the instance similarity.

In this paper, we propose a Semantic Pseudo-labeling Image ClustEring (SPICE) framework that explicitly leverages both the discrepancy among semantic clusters and the similarity among instance samples to adaptively label training samples in batch-wise.
In contrast to the instance similarity among individual samples, we call the similarity between the cluster prototype and instance samples as the semantic similarity, as shown in Fig. \ref{fig_general}-(a).
Thus, constraining semantically similar samples to have the same label can reduce the semantic inconsistency of borderline samples when the semantic clusters are well captured.
To this end, we propose a semantic pseudo-labeling method that assigns the same cluster label to semantically similar samples.
Specifically, for each batch of samples, we use the most confident samples predicted by the classification model to compute the prototype of each cluster, and then spread the semantic labels of prototypes to their neighbors based on the semantic similarity.
Given these pseudo labels, the classification model can be optimized using the classification loss directly, which is then used to compute prototypes in the next iteration.
On the other hand, the semantic inconsistency may appear  around borderlines and other places. 
To reduce the inconsistency of similar samples, we design a local consistency principle to select a set of reliably labeled images from the clustering results as in Fig. \ref{fig_general}-(b), and then reformulate the unsupervised task into a semi-supervised pseudo-labeling process for performance boosting.

Our main contributions are as follows:
\emph{First}, a novel SPICE framework is proposed for image clustering, which directly uses the classification loss to train the classification network through the pseudo-labeling processes that synergize both the discrepancy among semantic clusters and the similarity among instance samples.
\emph{Second}, a pseudo-labeling method is designed to utilize the semantic-similarity measurement during training and reduce the semantic inconsistency of the samples around borderlines. In practice, it is operated batch-wise to optimize a light-weight classifier such that it is scalable to large datasets.
\emph{Third}, a double softmatx cross-entropy loss function is designed to enforce the model making confident predictions, which effectively boosts the clustering performance.
\emph{Fourth}, the designed local consistency principle effectively reduces the semantic inconsistency, and boosts the clustering performance by transforming the original clustering problem into a semi-supervised learning paradigm.
\emph{Fifth}, SPICE clearly outperforms the state-of-the-art approaches on six image clustering benchmarks, and codes have been made publicly available at \href{https://github.com/niuchuangnn/SPICE}{https://github.com/niuchuangnn/SPICE}.

\section{Related work}

\noindent \textbf{Unsupervised Deep Clustering} methods have shown significant superiority over traditional clustering algorithms, especially in computer vision. In a data-driven fashion, deep clustering can effectively utilize the representation ability of deep neural networks. Initially, several methods were proposed to combine the stacked auto-encoders (SAE) with the traditional clustering algorithms \cite{Xie2016,LI2018161,DCN2016,DeepCluster2017,Zhang_2019_CVPR,DEPICT2017,VaDE2017,GMVAE,DASC2018}, such as k-means \cite{DEN2014, DMC2017} and spectral clustering \cite{DSCN2017}. However, since the pixel-wise reconstruction loss of SAE tends to over-emphasize  low-level features, these methods have an inferior performance when clustering images of complex contents due to lack of object-level semantics. 
Instead of using SAE, JULE \cite{Yang2016Joint} and DeepClustering \cite{vf2018} alternately perform the traditional clustering algorithms, such as agglomerative clustering and k-means respectively, for clustering, and then train the convolutional neural network (CNN) with the cluster indices for representation learning. However, the performance of these methods can be compromised by the errors accumulated during the alternation, and their successes in online scenarios are limited as they need to perform clustering on the entire dataset.

Recently, novel methods emerged that directly learn to map images into label features, which are used as the representation features during training and as the one-hot encoded cluster indices during testing \cite{DAIC2017, Wu_2019_ICCV, IIC2019, Huang_2020_CVPR, gatcluster, scan, cc}. Actually, these methods aim to train the classification model in the unsupervised setting while using multiple indirect loss functions, such as sample relations \cite{DAIC2017}, invariant information \cite{IIC2019, cc}, mutual information \cite{Wu_2019_ICCV}, partition confidence maximisation \cite{Huang_2020_CVPR}, attention \cite{gatcluster}, and entropy \cite{gatcluster, Huang_2020_CVPR, scan, cc}. However, the performance of these methods may be sub-optimal when using such label features to compute the similarity and discrepancy between samples, as the category-level label features can hardly reflect the relations of instance-level samples accurately.
Therefore, SCAN \cite{scan} was proposed to use embedding features of the representation learning model for computing the instance similarity, based on which the label features are learned by encouraging nearest samples to have the same label. However, since the embedding features are not perfect, similar instances do not always have the same semantics especially when the samples lie near the borderlines of different clusters, which may degrade the clustering performance.

\begin{figure*}[bt!]
    \centering
    \includegraphics[width=0.94\textwidth]{figures/framework.png}
    \caption{Illustration of the SPICE framework. (a) SPICE-Self trains a classification model via pseudo labeling, where CNN is fixed after pretraining through representation learning. (b) SPICE-Semi retrains the classification model via semi-pseudo-labeling, where reliable labels are selected from the results from SPICE-Self according to local consistency of neighboring samples. (c) A toy example of pseudo labeling, where red, green, and blue denote different clusters.}
    \label{fig_framework}
\end{figure*}

\noindent \textbf{Unsupervised Representation Learning} maps samples/images into semantically meaningful features without human annotations, which facilitates various down-stream tasks, such as object detection and classification. Previously, various pretext tasks were heuristically designed for this purpose, such as colorization \cite{colorization2016}, rotation \cite{rotation2018}, jigsaw \cite{jigsaw2016}, etc.
Recently, contrast learning methods combined with data augmentation strategies achieved great successes, such as SimCLR \cite{simclr}, MOCO \cite{He_2020_CVPR}, and BYOL \cite{byol}, just to name a few.

\noindent \textbf{Semi-supervised Classification} methods reduce the requirement of labeled data for training a classification model by providing a means of leveraging unlabeled data. In this category, remarkable results were obtained with consistency regularization \cite{NIPS2016_30ef30b6, DBLP} that constrains the model to output the same prediction for different transformations of the same image, pseudo-labeling \cite{pseudo} that uses confident predictions of the model as the labels to guide training processes, and entropy minimization \cite{NIPS2004_96f2b50b, pseudo} that steers the model to output high-confidence predictions. MixMatch \cite{mixmatch} algorithm combines these principles in a unified scheme and achieves an excellent performance, which is further improved by ReMixMatch \cite{remixmatch} along this direction. Recently, FixMatch \cite{fixmatch} proposed a simplified framework that uses the confident prediction of a weakly transformed image as the pseudo label when the model is fed a strong transformation of the same image, delivering superior results.

\section{Method}

\subsection{Framework of SPICE}
The proposed SPICE framework consists of three training stages. We first pretrain an unsupervised representation learning model adapted from that used in SCAN \cite{scan}, and then the CNN backbone of the pretrained model is frozen to extract embedding features in the following two stages: SPICE-Self and SPICE-Semi, as shown in Fig. \ref{fig_framework}.
Specifically, SPICE-Self aims to train a classification model based on the embedding features in the unsupervised setting.
SPICE-Self has three branches: the first branch takes original images as inputs and outputs the embedding features, the second branch takes the weakly transformed images as inputs and outputs the semantic labels, and the third branch takes the strongly transformed images as inputs and predicts the cluster labels.
Given the outcomes of the first two branches, a pseudo-labeling algorithm based on the semantic-similarity generates the pseudo-labels to supervise the third branch.
In practice, SPICE-Self only needs to train the light-weight classification head in the third branch, as analyzed in Subsection \ref{sec_abl}.
To further improve the performance, SPICE-Semi first determines a set of reliably labeled images based on a local semantic consistency principle from the clustering results of SPICE-Self, and then the clustering task is reformulated as a semi-supervised learning problem to retrain the classification model.
Finally, the trained classification model is able to predict the cluster labels of both the training images and the unseen images beyond training. In the following two sub-sections, we will introduce the details of SPICE-Self and SPICE-Semi respectively.

\begin{algorithm}
\scriptsize
\label{alg_1}
\caption{SPICE-Self Algorithm.}
\LinesNumbered
\KwIn{Dataset , CNN parameters , , , , , , , , }
\KwOut{Cluster label  of }
Set parameters of CNN to , , and randomly initialize  \;
\While{ E}{

    \For{}{
            \textbf{\emph{Step-1}:} \\
            Select  samples as  from  \;
            \For{}{
                  Select  samples as  from \;
                  Compute embedding features  \;
                  Weakly transform samples  \;
                  Predict probabilities  \;
             }
            Concatenate all embedding features as  \;
            Concatenate all predicted probabilities as  \;
            
            \textbf{\emph{Step-2}:} \\
            Calculate cluster centers based on  and Eqs. (\ref{eq_conf}) and (\ref{eq_center}) \;
            Select samples  from  and assign labels  with Eq. (\ref{eq_label}) \;
            
            \textbf{\emph{Step-3}:} \\
            Strongly transform samples  \;
            \For{}{
                Select  samples with pseudo labels from  \;
                Optimize  by minimizing the DS-CE loss \;
            }

    }
}

Select the best CLSHead with minimum loss as  \;

\ForEach{}{
         \;
        \;
}
\end{algorithm}


\subsection{SPICE-Self}
\label{sec_proto}
Given an image dataset  and the parameters  of the pre-trained CNN, SPICE-Self aims to group these images into the predefined  clusters by training a classification model; i.e., , where  is the predicted probability over  clusters,  denotes the function of the neural network, and  denotes the parameters of a multilayer perceptron (MLP), which is referred to as CLSHead.
However, in the unsupervised setting we do not have the ground truth to train the model. Instead, we design a semantic-similarity based pseudo-labeling algorithm that dynamically estimates the pseudo labels for batch-wise samples in the training process, as in Algorithm \ref{alg_1}.

Specifically, SPICE-Self consists of three steps in each training iteration.
First, we compute the embedding features, , of a large batch samples, , and the corresponding semantic predictions, , for the weakly transformed samples, , where
 is batch size,
 is the dimension of the feature vector, and
 is the weak transformation.
Due to the limited memory, we divide the large batch into mini-batches to compute these results, where each mini-batch has  samples.

Second, we use the top confident samples to estimate the prototype for each cluster based on  and , and then the indices of the cluster centers are assigned to their nearest neighbors as the pseudo labels.
Formally, the top confident samples for each cluster are defined as

where  denotes the descendingly sorted probabilities of cluster  over  samples,
 is the number of top confident samples,  is the confident ratio,  means the balance assignment of  samples to  clusters,  denotes the -\emph{th} largest probability, and  denotes the indices of the top confident samples.
Naturally, the cluster centers  in the embedding space are computed as

Based on the the cosine similarity between embedding features and the cluster center , we select  nearest samples, denoted by , for each cluster, and assign the pseudo label of  to these samples, which can be formally described as

where  denotes the pseudo labels corresponding to the selected samples  of all clusters.
Without loss of generality, we assume that the training set has a balanced number of samples over clusters and set .

A toy example of this pseudo-labeling process is shown in Fig. \ref{fig_framework}(c), where there is a batch of 10 samples and 3 clusters, 2 confident samples for each cluster are selected to calculate the prototypes based on the semantic predictions and embedding features, and then 3 nearest samples to each cluster are selected and labeled. Note that there may exist overlapped samples between different clusters, so there are two options to handle these labels: one is the overlap assignment that one sample may have more than one cluster labels as indicated by the yellow and red circles in Fig. \ref{fig_framework}(c), and the other is non-overlap assignment that all samples have only one cluster label as indicated by the dished red circle. We found that the overlap assignment is better as analyzed in Subsection \ref{sec_abl}.

Third, we update the parameters of CLSHead using the labeled samples.
Specifically, the selected images  are first transformed with a strong augmentation operator , i.e., . Then, given the labeled dataset , a cross-entropy loss function with a double softmax activation function is used to optimize the classification model in terms of the loss function Eq. (\ref{eq_loss}). The basic idea behind this loss function is to encourage the model to output confident predictions, which helps learn from unlabeled data \cite{NIPS2004_96f2b50b}.

where  denotes the indicator function, and  is the mini-batch size. Note that  is already the output of the softmax function, and  is the output of two cascaded softmax functions, and thus we name  as double softmax cross-entropy (DS-CE).
As the probability , we have . Hence, the value of DS-CE is consistently larger than that of CE, e.g., even when the prediction  is ideal one-hot encoding  is still not zero, which will enforce  to be confident.

In this stage, we fix the parameters of CNN from the representation learning, and only optimize the light-weight CLSHead. Thus, the computational burden is significantly reduced so that we can train multiple CLSHeads simultaneously and independently. By doing so, the instability of clustering from the initialization can be effectively alleviated through selecting best CLSHeads. Specifically, the best head can be selected for the minimum loss value of  over the whole dataset; i.e, we set  and follow Steps 1, 2, and 3 in Algorithm \ref{alg_1} to compute the loss value.
During testing, the trained model with the best CLSHead is used to classify the input images into different clusters.

\subsection{SPICE-Semi}
\label{sec_semi}
Given the clustering results and embedding features, , from SPICE-Self, here we aim to further improve the clustering performance by retraining a classification model with a semi-supervised learning paradigm. As shown in Fig. \ref{fig_general}(b), we first select a set of reliably labeled images to alleviate the local semantic inconsistency. Specifically, for each sample , we can select its  nearest samples according to the cosine similarity of embedding features, and the corresponding labels of these nearest samples are denoted by . Then, the local consistency  of the sample  is defined as

If , then the sample  is regarded as the reliably predicted and used as the labeled images in SPICE-Semi. Given the partially labeled images, we adopt a simple semi-supervised learning framework, i.e., FixMatch \cite{fixmatch}, to retrain the classification model, as shown in Fig. \ref{fig_framework}(b).
During training, the reliable labels of local consistent samples keep fixed, and the pseudo labels of unlabeled images are adaptively generated by thresholding the confident predictions.
Formally, the loss function of SPICE-Semi is

where  and  denote the number of labeled and unlabeled images in each training batch,  is the cross-entropy loss function,  denotes the classification network with the parameter of ,  is the label of weakly-transformed image ,  is the prediction of the strongly-augmented image ,  denotes an unlabeled image,  is the prediction of a weakly-transformed image,  is the pseudo label, and  is threshold confidence. As in FixMatch \cite{fixmatch}, all images including the labeled samples are used as unlabeled images. Thus, the first term in Eq. (\ref{eq_loss_semi}) is to learn cluster semantics from the semantic consistent predictions of SPICE-Self, and the second term constrains all samples to have the consistency predictions for different transformations.

\section{Experiments and Results}

\subsection{Benchmark datasets and evaluation metrics}

\begin{table}[ht]
\footnotesize
\caption{Specifications and partitions of selected datasets.}
\label{table_data}
\centering
\begin{tabular}{|c|cccc|}
\hline
Dataset & Image size & \# Training & \# Testing & \# Classes \\
\hline\hline
STL10         &    & 5000      & 8000   &  10   \\
Cifar10       &    & 50000     & 10000  &  10   \\
Cifar100-20   &    & 50000     & 10000  &  20   \\
ImageNet-10   &   & 13000     & N/A    &  10   \\
ImageNet-Dog  &   & 19500     & N/A    &  15   \\
Tiny-ImageNet &    & 100000    & 10000  &  200  \\
\hline

\end{tabular}
\end{table}

We evaluated the performance of SPICE on six commonly used image clustering datasets, including
STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet.
The key details on each dataset are summarized in Table \ref{table_data}, where the datasets reflect a diversity of image sizes, the number of images, and the number of clusters.
We used three popular metrics to evaluate clustering results, including Adjusted Rand Index (ARI) \cite{hubert1985comparing}, Normalized Mutual Information (NMI) \cite{strehl2002clusterensembles} and clustering Accuracy (ACC) \cite{LiD06}.


\begin{table*}[t]
\footnotesize
\renewcommand\tabcolsep{3.1pt}
\caption{ Comparison with competing methods. '*' indicates that the methods were trained and evaluated on two split datasests (training and testing images are mutually exclusive), and the remaining methods were trained and evaluated on the whole dataset  (training and testing datasets are identical). The best results are highlighted in \textbf{bold}.}
\label{table_results_compare}
\centering
\begin{tabular}{|c|ccc|ccc|ccc|ccc|ccc|ccc|}
\hline

\multirow{2}{*}{Method}             & \multicolumn{3}{c|}{STL10}&\multicolumn{3}{c|}{ImageNet-10}&\multicolumn{3}{c|}{ImageNet-Dog-15}&\multicolumn{3}{c|}{Cifar10}&\multicolumn{3}{c|}{Cifar100-20}&\multicolumn{3}{c|}{Tiny-ImageNet-200}\\
                                                                        \cline{2-19}
                                                                        & ACC  & NMI  & ARI& ACC&NMI&ARI   & ACC&NMI&ARI  &ACC&NMI&ARI   &ACC&NMI&ARI  &ACC&NMI&ARI\\
\hline\hline
JULE \cite{Yang2016Joint}                & 0.277  & 0.182 & 0.164   & 0.300 &0.175&0.138    & 0.138 &0.054&0.028   & 0.272 &0.192&0.138   & 0.137 &0.103&0.033   & 0.033 &0.102&0.006\\
DEC \cite{Xie2016}                       & 0.359  & 0.276 & 0.186   & 0.381 &0.282&0.203    & 0.195 &0.122&0.079   & 0.301 &0.257&0.161   & 0.185 &0.136&0.050   & 0.037 &0.115&0.007\\
DAC \cite{DAIC2017}                      & 0.470  & 0.366 & 0.257   & 0.527 &0.394&0.302    & 0.275 &0.219&0.111   & 0.522 &0.396&0.306   & 0.238 &0.185&0.088   & 0.066 &0.190&0.017\\
DeepCluster \cite{vf2018}                & 0.334  & N/A   &  N/A    & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.374& N/A   &  N/A  & 0.189 & N/A &  N/A   & N/A &N/A&N/A\\
DDC \cite{chang2019deep}                 & 0.489  & 0.371 & 0.267   & 0.577 &0.433&0.345    & N/A &N/A&N/A         & 0.524 &0.424&0.329   & N/A &N/A&N/A         & N/A &N/A&N/A\\
IIC  \cite{IIC2019}                      & 0.610  & N/A   &  N/A    & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.617& N/A & N/A     & 0.257& N/A & N/A     & N/A &N/A&N/A\\
DCCM \cite{Wu_2019_ICCV}                 & 0.482  & 0.376 & 0.262   & 0.710 &0.608 &0.555   & 0.383&0.321 &0.182   & 0.623& 0.496&0.408   & 0.327 &0.285&0.173   & 0.108 &0.224&0.038\\
GATCluster \cite{gatcluster}             & 0.583  & 0.446 & 0.363   & 0.762 &0.609 &0.572   & 0.333&0.322 & 0.200  & 0.610&0.475 &0.402   & 0.281 &0.215&0.116   & N/A &N/A&N/A\\
PICA \cite{Huang_2020_CVPR}              & 0.713  & 0.611 & 0.531   & 0.870 &0.802 &0.761   & 0.352&0.352 & 0.201  & 0.696&0.591 &0.512   & 0.337 &0.310&0.171   & 0.098 &0.277&0.040\\
CC \cite{cc}                             & 0.850  & 0.746 & 0.726   & 0.893 &0.859 &0.822   & 0.429&0.445 & 0.274  & 0.790&0.705 &0.637   & 0.429 &0.431&0.266   & 0.140 &0.340&0.071\\


\textbf{SPICE-Self}                  & 0.908  & 0.817 & 0.812   & \textbf{0.969} &\textbf{0.927} &\textbf{0.933} & 0.546&0.498&\textbf{0.362}   & 0.838&0.734  &0.705    & 0.468 &0.448&0.294      & \textbf{0.305} &\textbf{0.449}&\textbf{0.161}\\
\textbf{SPICE}                        & \textbf{0.938}&\textbf{0.872}&\textbf{0.870}      & 0.967 & 0.917 & 0.929 & \textbf{0.554} & \textbf{0.504} & 0.343            & \textbf{0.926}&\textbf{0.865}&\textbf{0.852}   &\textbf{0.538}& \textbf{0.567}&\textbf{0.387}     & N/A &N/A&N/A\\
\hline
ADC* \cite{ADC2019}                       & 0.530  &N/A&N/A          & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.325 & N/A& N/A     &0.160  &N/A&N/A       & N/A &N/A&N/A\\
SCAN* \cite{scan}                                    & 0.755  & 0.654 & 0.590   & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.818&0.712 &0.665   & 0.422 &0.441&0.267   & N/A &N/A&N/A\\
SCAN-SL* \cite{scan}                                 & 0.809  & 0.698 & 0.646   & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.883&0.797 &0.772   & 0.507 &0.486&0.333   & N/A &N/A&N/A\\
\textbf{SPICE-Self*}                 & 0.899  & 0.809 & 0.797   & N/A &N/A&N/A           & N/A &N/A&N/A        & 0.820&0.694 &0.666   & 0.467 & 0.442 & 0.290                  & 0.288 &0.523&0.142\\
\textbf{SPICE}*                       & \textbf{0.929}  & \textbf{0.860} & \textbf{0.853}   & N/A &N/A&N/A      & N/A &N/A&N/A         & \textbf{0.917} & \textbf{0.858} & \textbf{0.836}  & \textbf{0.584} & \textbf{0.583} & \textbf{0.422}              & N/A &N/A&N/A\\

\hline

\end{tabular}
\end{table*}

\subsection{Implementation details}
For fair comparison, we adopted the same backbone network used in \cite{IIC2019, Huang_2020_CVPR} for representation learning and SPICE-Self.
The CLSHead in SPICE-Self consists of two fully-connected layers; i.e., , where  and  are the feature dimension and the number of clusters respectively.
For ImageNet-10 and ImageNet-Dog, we used the pretrained model on ImageNet in MoCo-v2 \cite{He_2020_CVPR}.
We directly used the original images without resizing for all datasets.
In SPICE-Semi, we adopted the same networks used in FixMatch \cite{fixmatch}. Specifically, we used WideResNet-28-2 for Cifar10, WRN-28-8 for CIFAR-100, and WRN-37-2 for STL-10, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet.
For representation learning, we used MoCo-v2 \cite{He_2020_CVPR} in all our experiments, which was also used in SCAN \cite{scan}.
For weak augmentation, a standard flip-and-shift augmentation strategy was implemented as in FixMatch.
For strong augmentation, we adopted the same strategies in SCAN \cite{scan}. Specifically, the images were strongly augmented by composing Cutout \cite{cutout} and four randomly selected transformations from RandAugment \cite{randaugent}.
We empirically set M to 1,000 for STL10, Cifar10, and ImageNet-10 that contain 10 clusters, 1,500 for ImageNet-Dog with 15 clusters, 2,000 for Cifar100-20 with 20 clusters, and 5,000 for TinyImageNet with 200 clusters. We set  to 256 for TinyImageNet, and 128 for the remaining datasets. The hyper-parameter  can be any value according to the device memory and will not affect the performance.
In SPICE-Self, the model consists of 10 CLSHeads, and the best head with the minimum loss was automatically selected as the final head in each trial.
We set cofident ratio  to 0.5.
To select the reliably labeled images in SPICE-Semi, we empirically set  and .
We set  and  that are the same as those in FixMatch \cite{fixmatch}.

\subsection{Main results}

We evaluated SPICE on six image clustering benchmarks on both the whole dataset and split datasets, and compared it with the most recent deep clustering methods shown in Table \ref{table_results_compare}.
For clustering on the whole dataset, all training and testing images were combined to train and evaluate models. In this task, SPICE improved ACC, NMI, and ARI by 13.8\%, 12.8\%, and 14.4\% respectively relative to the best results recently reported by CC \cite{cc}. Similarly, our proposed method also improved ACC, NMI, and ARI by 10+\% on ImageNet-Dog-15, Cifar-10, Cifar100-20, Tiny-ImageNet-200, and by 7.6\%, 6.8\%, and 11.1\% on ImageNet-10. Without the boosting stage, SPICE-Self is still better than the exiting deep clustering methods on all datasets. In addition, SPICE-Semi cannot  boost the performance significantly for ImageNet-10 and ImageNet-Dog-15. The reason may be that the representation features are well pre-trained on the whole ImageNet, and thus hard to be further improved with sub-datasets.

For clustering on split datasets, the images in the training and testing datasets were used to train and test the models separately, i.e., the testing images were not used for  training  including representation learning. In this task, SPICE improved ACC, NMI, and ARI by 12.0\%, 16.2\%, and 20.7\% on STL10, by 3.4\%, 6.1\%, and 6.4\% on Cifar10, and by 7.7\%, 9.7\%, and 8.9\% on Cifar100-20, respectively.

In clustering the images in Tiny-ImageNet-200, although our results are significantly better than the existing results, there were still some weaknesses. This is mainly due to the class hierarchies; i.e., some classes share the same supper class, as analyzed in \cite{scan}. Due to the low performance, some clusters cannot be reliably labeled based on the local consistency principle so that SPICE-Semi cannot be applied for further boosting, which is a limitation of SPICE-Semi that needs to be addressed in the future.

Overall, our  comparative results systematically demonstrate the superiority of the proposed SPICE method on both the whole and split datasets.

\subsection{Comparison with semi-supervised learning}

\begin{table}[ht]
\footnotesize
\renewcommand\tabcolsep{2.5pt}
\caption{ Comparison results with semi-supervised learning on STL10 and Cifar10.}
\label{table_results_semi}
\centering
\begin{tabular}{|c|cccc|cc|}

\hline

Method                          & MixMatch&UDA&ReMixMatch&FixMatch& SCAN & SSLabel\\

\hline\hline
Cifar10                         & 0.890 & 0.912 & 0.947 & 0.949 & 0.883 & 0.917   \\
\hline
stl10                           & 0.897 & 0.923 & 0.948  & 0.920 & 0.809 & 0.929   \\
\hline
\end{tabular}
\end{table}

In this subsection, we further compared SPICE with the recently proposed semi-supervised learning methods including MixMatch \cite{mixmatch}, UDA \cite{UDA}, ReMinMatch \cite{remixmatch}, and FixMatch \cite{fixmatch}, as shown in Table \ref{table_results_semi}. Here the semi-supervised learning methods used 250 and 1,000 samples with ground truth labels on Cifar10 and STL10 respectively. It is found in our experiments that SPICE is comparable to and even better than these state-of-the-art semi-supervised learning methods. Actually, these results demonstrate that SPICE-Self with local consistency selection can reliably label a set of images with different semantics without human interaction.

\subsection{Empirical analysis}

In this subsection, we empirically analyze the effectiveness of different components and options in the proposed SPICE framework. 

\subsubsection{Visualization of cluster semantics}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/proto.png}
    \caption{Visualization of learned semantic clusters.}
    \label{fig_proto}
\end{figure}

We visualized the semantic clusters learned by SPICE-Self in terms of the prototype samples and the discriminative regions, as shown in Fig. \ref{fig_proto}.
Specifically, in each column the top three nearest samples to cluster centers represent the cluster prototypes, and in each image with an overlapped heat map the discriminative region is highlighted. The heat map is computed by computing the cosine similarity between the cluster center (with Eq. (\ref{eq_conf}) and (\ref{eq_center})) of the whole dataset and the convolutional feature map of the individual image, and then resized and normalized into [0, 1]. It shows that the top three samples exactly match the human annotations, and the discriminative regions focus on the semantic objects. For example, the cluster with label '3' captures the 'cat' class, and its most discriminative region covers the cat head. The visual results indicate that semantically meaningful clusters are learned, and the cluster center vectors can define the discriminative regions.


\subsubsection{Ablation study}
\label{sec_abl}


\begin{table}[ht]
\footnotesize
\renewcommand\tabcolsep{6.8pt}
\caption{ Ablation studies of SPICE on the whole STL10 dataset.}
\label{table_results_abl}
\centering
\begin{tabular}{|c|ccc|ccc|ccc|}

\hline

Method                          & ACC&NMI&ARI\\

\hline\hline
k-means                                          & 0.797  0.046 & 0.768  0.021 & 0.624  0.041     \\
CE                                               & 0.875  0.031 & 0.784  0.017 & 0.764  0.033     \\
TCE                                              & 0.895  0.005 & 0.794  0.010 & 0.787  0.010     \\
Non-overlap                                      & 0.885  0.002 & 0.788  0.003 & 0.771  0.003     \\
Conv-SH                                          & 0.622  0.061 & 0.513  0.037 & 0.437  0.053     \\
Conv-MH                                          & 0.687  0.037 & 0.577  0.029 & 0.512  0.033     \\
Entropy                                          & 0.907  0.001 & 0.817  0.003 & 0.810  0.003     \\
\hline
SPICE-Self                                       & \textbf{0.908  0.001} & \textbf{0.817  0.002} & \textbf{0.812  0.002}    \\
SPICE                                         & 0.937  0.001 & 0.871  0.001 & 0.870  0.001   \\
\hline
\end{tabular}
\end{table}

We evaluated the effectiveness of different components of SPICE in an ablation study, as shown in Table \ref{table_results_abl}. In each experiment, we replaced one component of SPICE-Self with another option, and five trials were conducted to report the mean and standard deviation for each metric. Specifically, when we directly applied the k-means algorithm on the representation features, the clustering results are significantly degraded, indicating that the clustering performance depends on not only the representation features but also a better clustering algorithm. Then, we replaced DS-CE with CE or the CE with a temperature parameter (TCE), which was set to 0.2 for enforcing the model to output confident predictions. The results show that TCE is better than CE, which is consistent with the literature. On the other hand, the results of TCE are inferior to that with  DS-CE. In addition, we do not need to set the hyper-parameter for DS-CE as defined in Eq. (\ref{eq_loss}). The results also show that the overlap assignment is preferred over the non-overlap assignment, as introduced in Subsection \ref{sec_proto}, which may be explained by the fact that the non-overlap assignment may introduce extra local inconsistency when assigning the label to a sample far away from the cluster center, as shown in Fig. \ref{fig_framework} (dashed red circle).
Next, we made the CNN backbone trainable and trained the model with a single head (Conv-SH) or multiple heads (Conv-MH) end-to-end, the corresponding results became significantly worse. Actually, the quality of pseudo labels not only depends on the similarity measurement but also the predictions of the classifier. When tuning all parameters during training, the model tends to output incorrect predictions in the initial stage, which will harm the pseudo-labeling quality and get trapped in a bad cycle.
Finally, we added another entropy loss during training. The results were not changed, showing that our pseudo-labeling process alone is sufficient to prevent trivial solutions. However, when clustering a large number of clusters, e.g., 200 clusters in Tiny-ImageNet, the entropy loss was found necessary to avoid the empty clusters.

\subsubsection{Model selection}

\begin{figure}[bt!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/acc.png}
    \caption{Model Selection. Each curve represents the changing process of ACC v.s. epoch of a specific CLS head. The blue squares mark the selected best CLS head for each epoch, and the red stars represent the corresponding best CLS head evaluated with the ground truth. The blue circle denotes the finally selected model. The red circle is the ground truth best model.}
    \label{fig_select}
\end{figure}

In unsupervised learning, the training process is hard to converge to the best state without the ground truth supervision. Thus, how to estimate the performance of models in the unsupervised training process is very important. As introduced in Subsection \ref{sec_proto}, we use the classification loss defined in Eq. (\ref{eq_loss}) on the whole test dataset to approximate the model performance; i.e., the smaller loss, the better model. The model selection process is shown in Fig. \ref{fig_select}, it can be seen that the performance of the selected model is very close to that of the ground truth selection, proving the effectiveness of this loss metric.



\subsubsection{Effect of local consistency}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.43\textwidth]{figures/lc.png}
    \caption{Local consistency on Cifar10 dataset. Each point denotes a sample with various colors for different clusters. (a) All samples and (b) the selected semantic consistency samples.}
    \label{fig_lc}
\end{figure}

In Subsection \ref{sec_semi}, we introduced a local semantic consistency principle to select the reliably labeled images. Here we show the effectiveness of this principle in Fig. \ref{fig_lc}, where t-SNE was used to map the representation features of images in Cifar10 to 2D vectors for visualization. In Fig. \ref{fig_lc}(a), some obvious inconsistent semantic samples are evident, and the predicted ACC of SPICE-Self on all samples is 83.8\%. Using these samples for training SPICE-Semi, the ACC is not boosted. In Fig. \ref{fig_lc}(b), the selected samples are shown, where the ratio of local inconsistency samples is significantly decreased, and the ACC is increased to 95.9\% correspondingly. Using the selected samples for training, the ACC of SPICE-Semi is significantly boosted (92.6\% v.s. 83.8\%) compared with that of SPICE-Self.



\subsubsection{Effect of data augmentation}

\begin{table}[ht]
\footnotesize
\renewcommand\tabcolsep{5.3pt}
\caption{ Ablation studies of SPICE-Self on STL10.}
\label{table_results_abl}
\centering
\begin{tabular}{|cc|ccc|ccc|ccc|}

\hline

Aug1 & Aug2                          & ACC&NMI&ARI\\

\hline\hline
Weak   & Weak                         & 0.905  0.002 & 0.815  0.003 & 0.808  0.003     \\
Strong & Weak                         & 0.883  0.029 & 0.799  0.019 & 0.781  0.031     \\
Strong & Strong                       & 0.902  0.008 & 0.812  0.009 & 0.803  0.013     \\
Weak   & strong                       & 0.908  0.001 & 0.817  0.002 & 0.812  0.002     \\
\hline
\end{tabular}
\end{table}

We evaluated the effects of different data augmentations to SPICE-Self, as shown in Table \ref{table_data}, where Aug1 and Aug2 correspond to data augmentations of the second and the third branches in Fig. \ref{fig_framework}(a). The results show that when the labeling branch used the weak augmentation and the training branch used the strong augmentation, the model achieved the best performance. Moreover, the model had relatively worse performance when the labeling process used the strong augmentation, which is due to that the labeling branch aims to generate reliable pseudo labels that will be compromised by the strong augmentation.
The model performed better when the training branch used the strong augmentation, as it will drive the model to output consistent predictions of different transformations.
Overall, the data augmentation have a small impact on the results, because the pre-trained CNN had been already equipped with the transformation invariance ability.

\section{Conclusion}
We have presented a semantic framework for clustering, with the acronym ``SPICE''. SPICE  synergizes the discrepancy between semantic clusters, the similarity among instance samples and the inconsistency of neighboring samples in the embedding space.
To capitalize this synergy, SPICE consists of two components for (1) self-learning-based pseudo-labeling (SPICE-Self) and (2) semi-pseudo-label-based classification (SPICE-Semi).
Extensive experiments have demonstrated the superiority of SPICE over the competing methods on six public datasets with an average performance boost of 10\% in terms of adjusted rand index, normalized mutual information, and clustering accuracy.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
