\def\submit{n} \def\yes{y}
\def\no{n}

\if\submit\yes
\documentclass[oribibl]{llncs}
\else
\documentclass[a4paper,11pt]{article}
\usepackage{fullpage}
\fi
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{times}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}


\usepackage[nodayofweek]{datetime}
\newdateformat{simple}{\THEDAY\ \monthname[\THEMONTH]\ \THEYEAR}
\simple

\DeclareMathOperator*{\argmax}{arg\,max}
\allowdisplaybreaks[2]

\if\submit\no
\usepackage{amsthm} \newtheoremstyle{plain-boldhead}{\topsep}{\topsep}{\itshape}{}{\bfseries}{.}{ }{\thmname{#1}\thmnumber{ #2}\thmnote{ (\bfseries #3)}}\newtheoremstyle{definition-boldhead}{\topsep}{\topsep}{\normalfont}{}{\bfseries}{.}{ }{\thmname{#1}\thmnumber{ #2}\thmnote{ (\bfseries #3)}}\theoremstyle{plain-boldhead}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}[theorem]
\theoremstyle{definition-boldhead}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\fi


\usepackage{floatmodif}
\floatstyle{ruled}

\newfloat{alg}{htbp}{alg}
\floatname{alg}{Algorithm}

\newfloat{module}{htbp}{module}
\floatname{module}{Module}




\newcommand{\str}[1]{\textsc{#1}}
\newcommand{\var}[1]{\textit{#1}}
\newcommand{\op}[1]{\textsl{#1}}
\newcommand{\msg}[2]{\ensuremath{[$\str{#1},~{#2}$]}}
\newcommand{\msgnp}[1]{$[$\str{#1}$]$}
\newcommand{\newe}{{\bf invoke}\xspace}
\newcommand{\event}[2]{$\langle$~{\var{#1}}-\textsl{#2}~$\rangle$}
\newcommand{\eventt}[3]{$\langle$~{\var{#1}}-\textsl{#2}~$\mid$~{#3}~$\rangle$}
\newcommand{\instance}[1]{\textbf{instance}~\var{#1}}
\newcommand{\becomes}{\ensuremath{\leftarrow}}
\newcommand{\false}{\textsc{false}}
\newcommand{\true}{\textsc{true}}

\newcommand{\replicaset}{\ensuremath{\mathcal{D}}\xspace}
\newcommand{\clientset}{\ensuremath{\mathcal{C}}\xspace}
\newcommand{\values}{\ensuremath{\mathcal{V}}\xspace}
\newcommand{\fragments}{\ensuremath{\mathcal{F}}\xspace}
\newcommand{\valv}{\ensuremath{\var{v}}\xspace}
\newcommand{\strings}{\ensuremath{\Sigma^{*}}}
\newcommand{\dir}{\var{dir}\xspace}

\newcommand{\token}[1]{\textsc{#1}}
\newcommand{\fixme}{\textbf{FIXME}}

\newcommand{\NAME}{AWE\xspace}
\newcommand{\node}{node\xspace}
\newcommand{\nodes}{nodes\xspace}

\hyphenation{time-stamp}
\hyphenation{time-stamps}

\providecommand{\note}[1]{}
\renewcommand{\note}[1]{[[\textsf{\bf #1}]]}


\begin{document}

\title{\bf Erasure-Coded Byzantine Storage with Separate Metadata}


\if\submit\no

\author{Elli Androulaki\footnotemark[1]
  \and Christian Cachin\footnotemark[1]
  \and Dan Dobre\footnotemark[2]
  \and Marko Vukoli\'c\footnotemark[3]
  }

\date{\today}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{IBM Research - Zurich,
    R\"uschlikon, Switzerland, \texttt{\{lli,cca\}@zurich.ibm.com}.}
\footnotetext[2]{NEC Labs Europe, Germany, \texttt{dan.dobre@neclab.eu}.}
\footnotetext[3]{Eur{\'e}com, Sophia Antipolis, France,
  \texttt{vukolic@eurecom.fr}.}
\renewcommand{\thefootnote}{\arabic{footnote}}


\else


\author{Elli Androulaki\inst{1},
  Christian Cachin\inst{1},
  Dan Dobre\inst{2},
  Marko Vukoli\'c\inst{3}}

\institute{IBM Research - Zurich, \email{\{lli,cca\}@zurich.ibm.com} \and
  NEC Labs Europe, Germany, \email{dan.dobre@neclab.eu} \and
  Eur{\'e}com, \email{vukolic@eurecom.fr}}

\fi


\maketitle

\pagestyle{plain}
\thispagestyle{plain}

\begin{abstract}\noindent
  Although many distributed storage protocols have been introduced, a
  solution that combines the strongest properties in terms of
  availability, consistency, fault-tolerance, storage complexity and
  the supported level of concurrency, has been elusive for a long
  time. Combining these properties is difficult, especially if the
  resulting solution is required to be efficient and incur low cost.

  We present \NAME, the first \emph{erasure-coded} distributed
  implementation of a multi-writer multi-reader read/write storage
  object that is, at the same time: (1) asynchronous, (2) wait-free,
  (3) atomic, (4) amnesic, (i.e., with data \nodes storing a bounded
  number of values) and (5) Byzantine fault-tolerant (BFT) using the
  optimal number of \nodes. Furthermore, \NAME is efficient since it
  does not use public-key cryptography and requires data \nodes that
  support only reads and writes, further reducing the cost of
  deployment and ownership of a distributed storage solution. Notably,
  \NAME stores metadata separately from $k$-out-of-$n$ erasure-coded
  fragments. This enables \NAME to be the first BFT protocol that uses
  as few as $2t+k$ data \nodes to tolerate $t$ Byzantine \nodes, for
  any $k\ge 1$.
\end{abstract}


\section{Introduction}

\paragraph{Background.}
\emph{Erasure coding} is a key technology that saves space and retains
robustness against faults in distributed storage systems. In short, an
erasure code splits a large data object into $n$ fragments such that
from any $k$ of them the input value can be reconstructed. The utility
of erasure coding is demonstrated by large-scale erasure-coding
storage systems that have been deployed
today~\cite{hsxocg12,cleversafect13}. These distributed storage
systems offer large capacity, high throughput, and resilience to
faults.

Whereas the storage systems in production use today only tolerate
component crashes or outages, storage systems in the \emph{Byzantine
  failure model} survive also more severe faults, ranging from
arbitrary state corruption to malicious attacks on components. In this
paper, we consider a model where \emph{clients} directly access a
storage service provided by distributed servers, called \emph{\nodes}
--- a fraction of the \nodes may be Byzantine, whereas clients may
fail as well, but only by crashing.

Although Byzantine-fault tolerant (BFT) erasure-coded distributed
storage systems have received some attention in the
literature~\cite{gwgr04,cactes06,hegare07b,dugule08,bcqas11}, our
understanding of their properties lies behind that of replicated
storage. In fact, most existing BFT erasure-coded storage approaches have drawbacks that
prevented their wide-spread use.  For example, they relied on the
\nodes storing an unbounded number of values~\cite{gwgr04}, required
the \nodes to communicate with each other~\cite{cactes06}, used
public-key cryptography~\cite{cactes06,hegare07b}, or might have
blocked clients due to concurrent operations of other
clients~\cite{hegare07b}.

We consider an abstract \emph{wait-free} storage register with
\emph{atomic} semantics~\cite{herwin90}, accessed concurrently by
multiple readers and writers (MRMW).  Wait-free termination means that
any client operation terminates irrespective of the behavior of the
Byzantine \nodes and of other clients.  This is not easy to achieve
with Byzantine \nodes~\cite{ackm06} even in systems that replicate
the data.  Therefore, previous works have often used a weaker notion
of liveness called \emph{finite-write (FW) termination}, which ensures
that read operations progress only in executions with a finite number
of writes.

\paragraph{Contribution.}
This paper introduces \NAME, the \emph{first} asynchronous, wait-free
distributed BFT erasure-coded storage protocol with optimal
resilience.   As in
previous work, we assume there are $n$ \nodes and up to $t$ of them
may exhibit non-responsive (NR-)arbitrary faults, that is, Byzantine
corruptions.  The best resilience that has been achieved so far is $n
> 3t$, which is optimal for Byzantine storage~\cite{maalda02}.
However, our protocol features a separation of metadata and erasure coded fragments; with this approach our protocol may reduce the
number of \emph{data \nodes}, i.e., those that store a fragment, to
lower values than $n$ for $k \leq t$.  In particular, our protocol takes only
$2t+k$ data \nodes; this idea saves resources, as in the separation
of agreement and execution for BFT services~\cite{ymvad03}.  For
implementing the metadata service, $n > 3t$ \nodes are still
needed.

Our protocol
employs simple, passive data \nodes; they cannot execute code and they
only support read and write operations, such as the key-value stores
(KVS) provided by popular cloud storage services.  The metadata
service itself is an atomic snapshot object, which has only weak
semantics and may be implemented in a replicated asynchronous system
from simple read/write registers~\cite{aadgms93}.  The protocol is
also \emph{amnesic}~\cite{chguke07}, i.e., the \nodes store a
bounded number of values and may erase obsolete data.
The protocol uses only simple cryptographic
hash functions but no (expensive) public-key operations.

In summary, protocol \NAME, introduced in Section~\ref{sec:protocol}, is the first
erasure-coded distributed implementation of a MRMW storage object that
is, at the same time: (1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (5) tolerates the optimal number of Byzantine \nodes, and (6) does not use public-key
cryptography.  Furthermore, \NAME can be
implemented from non-programmable \nodes (KVS) that only support reads and
writes (in the vein of Disk Paxos~\cite{ackm06}). In practice, the KVS interface is offered by commodity cloud storage services, which could be used as \NAME data \nodes
to reduce the cost of \NAME deployment and ownership.
While some of these desirable properties have been achieved in different combinations so far, they have never been achieved together with erasure-coded storage, as explained later.  Combining these properties has been a longstanding open problem~\cite{gwgr04}.




\paragraph{Related work.}

We provide a brief overview of the most relevant literature on the
subject.  Table~\ref{tab:comparison} summarizes this section.

Earlier designs for erasure-coded distributed storage have suffered
from potential aborts due to contention~\cite{fmssv04} or from
the need to maintain an unbounded number of fragments at data
\nodes~\cite{gwgr04}.
In the crash-failure model, ORCAS~\cite{dugule08} and
CASGC~\cite{clmm13} achieve optimal resilience $n > 2t$ and low
communication overhead, combined with wait-free (ORCAS)
and FW-termination (CASGC), respectively.

In the model with Byzantine \nodes, Cachin and Tessaro
(CT)~\cite{cactes06} introduced the first wait-free protocol with
atomic semantics and optimal resilience $n > 3t$.  CT uses a
verifiable information dispersal protocol but needs \node-to-\node
communication, which lies outside our model. Hendricks et al.~(HGR)~\cite{hegare07b} present an optimally resilient
protocol that comes closest to our protocol among the existing
solutions.  It offers many desirable features, that is, it has as
low communication cost, works asynchronously, achieves optimal
resilience, atomicity, and is amnesic.  Compared to our work, it uses
public-key cryptography, achieves only FW-termination instead of
wait-freedom, and requires \emph{processing} by the \nodes, i.e., the
ability to execute complex operations beyond simple reads and writes.

To be fair, much of the (cryptographic) overhead inherent in the CT
and HGR protocols defends against poisonous writes from Byzantine
clients, i.e., malicious client behavior that leaves the \nodes in
an inconsistent state.  We do not consider Byzantine clients in this
work, since permitting arbitrary client behavior is problematic.  Such
a client might write garbage to the storage system at any time and
wipe out the stored value.
\if\submit\no
Furthermore, the standard formal
correctness notions such as linearizability fail when clients
misbehave (apart from crashing).  Hendricks~\cite{hendri09} discusses
correctness notions in the presence of Byzantine clients.
\fi
However,
even without the steps that protect against poisonous writes, HGR still
requires processing by the \nodes and is not wait-free.

The M-PoWerStore protocol~\cite{dklmsv13} employs a cryptographic
``proof of writing'' for wait-free atomic erasure-coded distributed
storage.  It is the first wait-free BFT solution without
\node-to-\node communication.  Similar to other protocols,
M-PoWerStore uses \nodes with processing capabilities and is not
amnesic.

Several systems have recently addressed how to store erasure-coded
data on multiple redundant cloud services but only few of them focus
on wait-free concurrent access.  HAIL~\cite{bojuop09}, for instance,
uses Byzantine-tolerant erasure coding and provides data integrity
through proofs of retrievability; however, it does not address concurrent operations by different clients.  DepSky~\cite{bcqas11}
achieves regular semantics and uses lock-based concurrency control;
therefore, one client may block operations of other clients.

A key aspect of \NAME lies in the differentiation of (small) metadata
from (large) bulk data: this enables a modular protocol design and an
architectural separation for implementations.
The FARSITE system~\cite{abcccd02} first introduced such a separation
for replicated storage; their data \nodes and their metadata
abstractions require processing, however, in contrast to~\NAME. Non-explicit ways of separating metadata from data can already be
found in several previous erasure coding-based protocols.  For
instance, the cross checksum, a vector with the hashes of all $n$
fragments, has been replicated on the data nodes to ensure
consistency~\cite{gwgr04,cactes06}.

Finally, a recent protocol called MDStore~\cite{cadovu13} has shown
that separating metadata from bulk data permits to reduce the number
of data \nodes in asynchronous wait-free BFT distributed storage
implementations to only~$2t+1$.  When protocol~\NAME is reduced to use
replication with the trivial erasure code ($k=1$), it uses as few
\nodes as MDStore to achieve the same wait-free atomic semantics;
unlike \NAME, however, MDStore is not amnesic and uses processing
\nodes.


\newcommand{\Y}{\ensuremath{\checkmark}}
\newcommand{\T}{\ensuremath{^*}}

\begin{table}
  \centering
  \begin{tabular}{|l||c|c|c|c|c|c|}
    \hline
    {\bf Protocol} & {\bf BFT} & {\bf Liveness} & {\bf Data \nodes} &
      {\bf Type} & {\bf Amnesic} & {\bf Cryptogr.} \\ \hline
    ORCAS~\cite{dugule08} & --- & Wait-free & $2t+1$ & Proc. &
      ---  & N/A \\ \hline
    CASGC~\cite{clmm13}   & --- & FW-term.  & $2t+1$ & Proc. &
      \Y \T   & N/A \\ \hline
    CT~\cite{cactes06}    & \Y \T & Wait-free \T & $3t+1$ & Msg. &
      ---  & Public-key \\ \hline
    HGR~\cite{hegare07b}  & \Y \T & FW-term.  & $2t+k$, for $k > t$ & Proc. &
      \Y \T & Public-key \\ \hline
    M-PoWerStore~\cite{dklmsv13}  & \Y \T & Wait-free \T & $3t+1$ & Proc. &
      ---  & Hash func. \T \\ \hline
    DepSky~\cite{bcqas11} & \Y \T & Obstr.-free  & $3t+1$ & R/W \T &
      ---  & Public-key \\ \hline
    \NAME (Sec.~\ref{sec:protocol})
                          & \Y \T & Wait-free \T & $2t+k$, for $k \geq 1$ \T &
      R/W \T & \Y \T & Hash func. \T \\ \hline
  \end{tabular}
  \caption{Comparison of erasure-coded distributed storage solutions.
    An asterisk (\T) denotes optimal properties. The column labeled
    \emph{Type} states the computation requirements on \nodes:
    \emph{Proc.}\ denotes processing;
\emph{Msg.}\ means sending messages to other
    \nodes, in addition to processing;
    \emph{R/W} means a register object supporting only read and write.}
  \label{tab:comparison}
\if\submit\yes
\vspace*{-4mm}
\fi
\end{table}


\paragraph{Structure.}

The paper continues with the model in Section~\ref{sec:def} and
presents Protocol~\NAME in Section~\ref{sec:protocol}.  The
communication and storage complexities of \NAME are compared
to those of existing protocols in Section~\ref{sec:complexity}.
\if\submit\no
Section~\ref{sec:proof} contains a formal proof for the properties
of~\NAME.
\else
For lack of space, proof of \NAME correctness is postponed to Appendix~\ref{app}.
\fi


\section{Definitions}
\label{sec:def}

\paragraph{System model.}
We consider an asynchronous distributed system of components (or
processes) that communicate with each other.  The components contain a
set \clientset of $m$ \emph{clients}, a set $\replicaset$ of $n$
\emph{data \nodes} $d_1, \dots, d_n$, and further process
abstractions.  The components interact asynchronously via exchanging
events.  A protocol specifies a collection of programs with
instructions for all components.


A component may fail by crashing or by exhibiting \emph{Byzantine}
faults; the latter means they may deviate arbitrarily from their
specification.  We assume that clients can only crash;
on the other hand, up to~$t$ data \nodes can
be Byzantine and behave adversarially (NR-arbitrary faults).  A
component that does not fail is called \emph{correct}.





\paragraph{Notation.}
Protocols are presented in a modular way using an event-based
notation~\cite{CachinGR11}.  A component is specified through its
\emph{interface}, containing the events that it exposes to other
components that may call it, and its \emph{properties}, which define
its behavior.  A component may react to a received event by doing
computation and triggering further events.
Every component is named by an identifier.  Events are qualified by
the component identifier to which the event belongs and may take
parameters.  An event \op{Sample} of a component~\var{m} with a
parameter~$x$ is denoted by \eventt{m}{Sample}{$x$}.

\if\submit\no
Components interact asynchronously with others through exchanging
events.  We assume that all events communicated from one component to
another are delivered in FIFO-order.
There are two kinds of events in a component's interface: \emph{input
  events} that it receives from other components, typically to invoke
its services, and \emph{output events}, through which the component
delivers information or signals a condition to another component.  The
behavior of a component is typically stated through a number of
properties or through a sequential implementation.
\fi

\paragraph{Objects and histories.}
An \emph{object} is a special type of component for which every input
event (called an \emph{invocation} in this context) triggers exactly
one output event (called a \emph{response}).  Every such pair of
invocation and response define an \emph{operation} of the object.  An
operation \emph{completes} when its response occurs.

A \emph{history}~$\sigma$ of an execution of an object~$O$ consists of
the sequence of invocations and responses of $O$ occurring in
$\sigma$.  An operation is called \emph{complete} in a history if it
has a matching response.
An operation~$o$ \emph{precedes} another operation~$o'$ in a sequence
of events~$\sigma$, denoted $o <_\sigma o'$, whenever $o$ completes
before $o'$ is invoked in $\sigma$. If $o$ precedes $o'$ then $o'$
\emph{follows}~$o$.  A sequence of events~$\pi$ \emph{preserves the
  real-time order} of a history~$\sigma$ if for every two operations
$o$ and $o'$ in $\pi$, if $o <_\sigma o'$ then $o<_\pi o'$.  Two
operations are \emph{concurrent} if neither one of them precedes the
other.  A sequence of events is \emph{sequential} if it does not
contain concurrent operations.
We often simplify the terminology by exploiting that every
\emph{sequential} sequence of events corresponds naturally to a
sequence of operations.


An execution is \emph{well-formed} if the events at every object are
alternating invocations and matching responses, starting with an
invocation.  An execution is \emph{fair}, informally, if it does not
halt prematurely when there are still steps to be taken or triggered
events to be consumed (see the standard literature for a formal
definition~\cite{Lynch96}).


\paragraph{Registers.}
A \emph{read/write register}~\var{r} is an object that stores a value from
a domain \values and supports exactly two operations, for writing and
reading the value.  More precisely:
\if\submit\yes
(i) a \emph{Write} operation to~\var{r} is triggered by an invocation \eventt{r}{Write}{$v$} that takes a value $v \in \values$ as parameter and terminates by generating a response \event{r}{WriteAck} with no parameter; and (ii) a \emph{Read} operation from~\var{r} is triggered by an invocation \event{r}{Read} with no parameter; the register signals that the read operation completes by triggering a response \eventt{r}{ReadResp}{$v$}, which contains a parameter $v \in \values$.
\else
\begin{itemize}
\item A \emph{Write} operation to~\var{r} is triggered by an
  invocation \eventt{r}{Write}{$v$} that takes a value $v \in \values$
  as parameter and terminates by generating a response
  \event{r}{WriteAck} with no parameter.
\item A \emph{Read} operation from~\var{r} is triggered by an invocation
  \event{r}{Read} with no parameter; the register signals that the
  read operation completes by triggering a response
  \eventt{r}{ReadResp}{$v$}, which contains a parameter $v \in
  \values$.
\end{itemize}
\fi
The behavior of a register is given through its sequential
specification, which requires that every \var{r}-\op{Read} operation
returns the value written by the last preceding \var{r}-\op{Write}
operation in the execution, or the special symbol $\bot \not\in
\values$ if no such operation exists. For simplicity, we will assume
that every distinct value is written only once.

In this work, any client may invoke the operations of the emulated
register object; such registers are also called \emph{multi-reader
  multi-writer (MRMW) registers}.  Furthermore, we assume that all
clients invoke a well-formed sequence of operations.


\paragraph{Consistency and availability.}
Recall that clients interact with an object~$O$ through its
operations, defined in terms of an invocation and a response event
of~$O$.  We say that a client~$c$ \emph{executes} an operation between
the corresponding invocation and response events.  When accessed
concurrently by multiple processes, executions of objects considered
in this work are \emph{linearizable}, that is, the object appears to
execute all operations \emph{atomically}.

\if\submit\yes
 More formally, a sequence of events~$\pi$ is called a \emph{view} of a history $\sigma$ at a client~$c$ w.r.t. an object~$O$ if: (i) $\pi$ is a sequential permutation of some subsequence of complete operations in $\sigma$; (ii) all complete operations executed by $c$ appear in $\pi$; and (iii) $\pi$ satisfies the sequential specification of $O$. A history $\sigma$ is \emph{linearizable}~\cite{herwin90} w.r.t. an object~$O$ if
  there exists a sequence of events $\pi$ such that: (i) $\pi$ is a view of $\sigma$ at all clients w.r.t. $O$; and (ii) $\pi$ preserves the real-time order of $\sigma$. If every history of a protocol is  linearizable, the protocol is itself called linearizable, or \emph{atomic}. Finally, a protocol is called \emph{wait-free}~\cite{herlih91} if every operation invoked by a correct client eventually completes.
\else

\begin{definition}[View]
  A sequence of events~$\pi$ is called a \emph{view} of a history
  $\sigma$ at a client~$c$ w.r.t. an object~$O$ whenever:
\begin{enumerate}
\item $\pi$ is a sequential permutation of some subsequence
of complete operations in $\sigma$;
\item all complete operations executed by $c$ appear in $\pi$; and
\item $\pi$ satisfies the sequential specification of~$O$.
\end{enumerate}
\end{definition}

\begin{definition}[Linearizability~\cite{herwin90}]
  A history $\sigma$ is linearizable w.r.t. an object~$O$ if
  there exists a sequence of events $\pi$ such that:
\begin{enumerate}
\item $\pi$ is a view of $\sigma$ at all clients w.r.t. $O$; and
\item $\pi$ preserves the real-time order of $\sigma$.
\end{enumerate}
\end{definition}

The goal of this work is to describe a protocol that emulates a
linearizable register abstraction among the clients; such a register
is also called \emph{atomic}.  Some of the clients may crash and some
\nodes may be Byzantine, but every client operation should terminate
in all cases, irrespective of how other clients and \nodes behave.

\begin{definition}[Wait-freedom~\cite{herlih91}]
  A protocol is called \emph{wait-free} if every operation invoked by
  a correct client eventually completes.
\end{definition}
\fi


\paragraph{Cryptography.}
We make use of cryptographic hash functions.
One can imagine that the cryptographic schemes are implemented by a
distributed oracle accessible to all components~\cite{CachinGR11}.  A
hash function $H$ maps a bit string~$x$ of arbitrary length to a
short, unique representation of fixed length.  We use a
\emph{collision-free} hash function; this property means that no
process, not even a Byzantine component, can find two distinct values
$x$ and $x'$ such that $H(x) = H(x')$.


\section{Protocol \NAME}
\label{sec:protocol}

\if\submit\no
This section introduces the \emph{asynchronous wait-free erasure-coded
Byzantine distributed storage protocol (AWE)}.



\subsection{Abstractions}
\fi

\if\submit\yes
We first overview the key components used in protocol \NAME.
\fi

\paragraph{Erasure code.}
An \emph{$(n, k)$-erasure code (EC)} with domain \values is given by
an encoding algorithm, denoted \op{Encode}, and a reconstruction
algorithm, called~\op{Reconstruct}.
Given a (large) value $v \in \values$, algorithm
$\op{Encode}_{k,n}(v)$ produces a vector $[f_1, \dots, f_n]$ of $n$
\emph{fragments}, which are from a domain~\fragments.  A fragment is
typically much smaller than the input, and any $k$ fragments contain
all information of $v$, that is, $|\values| \approx k |\fragments|$.

For an $n$-vector $F \in \big(\fragments \cup \{\bot\}\big)^n$, whose
entries are either fragments or the symbol~$\bot$, algorithm
$\op{Reconstruct}_{k,n}(F)$ outputs a value~$v \in \values$ or~$\bot$.
An output value of~$\bot$ means that the reconstruction failed.  The
\emph{completeness} property of an erasure code requires that an encoded value
can be reconstructed from any $k$ fragments.
In other words, for every $v \in \values$, when one computes $F
\becomes \op{Encode}_{k,n}(v)$ and then erases up to $n-k$ entries in
$F$ by setting them to~$\bot$, algorithm $\op{Reconstruct}_{k,n}(F)$
outputs~$v$.
\if\submit\no
More details are available in the
literature~\cite{rabin89,plank05}.
\fi


\paragraph{Metadata service.}
The metadata service is implemented by a standard \emph{atomic
  snapshot object}~\cite{aadgms93}, called \dir, that serves as a
\emph{directory}.  A snapshot object extends the simple storage
function of a register to a service that maintains one value for each
client and allows for better coordination.  Like an array of
multi-reader single-writer (MRSW) registers, it allows every client to
\emph{update} its value individually; for reading it supports a
\emph{scan} operation that returns the vector of the stored values,
one for every client.  More precisely, the operations of~\dir are:
\begin{itemize}
\item An \emph{Update} operation to~\dir is triggered by an invocation
  \eventt{dir}{Update}{$c, v$} by client~$c$ that takes a value $v \in
  \values$ as parameter and terminates by generating a response
  \event{r}{UpdateAck} with no parameter.
\item A \emph{Scan} operation on~\dir is triggered by an invocation
  \event{dir}{Scan} with no parameter; the snapshot object returns a
  vector~$V$ of $m = |\clientset|$ values to~$c$ as the parameter in
  the response \eventt{r}{ScanResp}{$V$}, with $V[c] \in \values$ for
  $c \in \clientset$.
\end{itemize}
The sequential specification of the snapshot object follows directly
from the specification of an array of $m$~MRSW registers
(hence, the snapshot initially stores the special symbol $\bot \not\in
\values$ in every entry).  When accessed concurrently from multiple
clients, its operations appear to take place atomically, i.e., they
are linearizable.  Snapshot objects are weak~--- they can be
implemented from read/write registers~\cite{aadgms93}, which, in turn,
can be implemented from a set of a distributed processes subject to
Byzantine faults.  Wait-free amnesic implementations of registers with
the optimal number of $n > 3t$ processes are possible using existing
constructions~\cite{gulevu06,domasu08}.


\if\submit\yes
\paragraph{Data \nodes.}

Data \nodes in \NAME export a subset of key-value store API. We model the state of data nodes as an array $\var{data}[\var{ts}] \in \strings$, initially $\bot$, for $\var{ts} \in \var{Timestamps}$.  Here, as
in standard implementations of multi-writer distributed
storage~\cite{CachinGR11}, every value is associated to a timestamp,
which consists of a sequence number~\var{sn} and the identifier~$c$ of the writing client, i.e., $\var{ts} = (\var{sn}, c) \in
\var{Timestamps} = N_0 \times (\clientset \cup \{\bot\})$; timestamps
are initialized to $T_0 = (0, \bot)$.

Data \node $d_i$ exports three operations:
\begin{itemize}
\item \eventt{$d_i$}{Write}{$\var{ts}, v$}, which stores $\var{data}[\var{ts}] \becomes v$ and returns \eventt{$d_i$}{WriteAck}{$\var{ts}$};
\item  \eventt{$d_i$}{Read}{$\var{ts}$}, which returns  \eventt{$d_i$}{ReadResp}{$\var{ts}, \var{data}[\var{ts}]$}; and
\item  \eventt{$d_i$}{Free}{$\var{TS}$}, which stores $\var{data}[\var{ts}] \becomes \bot$ for all $\var{ts} \in \var{TS}$, and returns \eventt{$d_i$}{FreeAck}{$\var{TS}$}.
\end{itemize}

\fi

\subsection{Protocol overview}

\if\submit\no
The high-level architecture of \NAME uses the metadata directory~\dir
to maintain pointers to the fragments stored at the data \nodes.
As in standard implementations of multi-writer distributed
storage~\cite{CachinGR11}, every value is associated to a timestamp,
which consists of a sequence number~\var{sn} and the identifier~$c$ of
the writing client, i.e., $\var{ts} = (\var{sn}, c) \in
\var{Timestamps} = N_0 \times (\clientset \cup \{\bot\})$; timestamps
are initialized to $T_0 = (0, \bot)$.  The metadata contains the
timestamp of the most recently written value for every client, and
readers determine the value to read by retrieving all timestamps,
determining their maximum, and accessing the fragments associated to
the highest timestamp.  Comparisons among timestamps use the standard
ordering, where $\var{ts}_1 > \var{ts}_2$ for $\var{ts}_1 =
(\var{sn}_1, c_1)$ and $\var{ts}_2 = (\var{sn}_2, c_2)$ if and only if
$\var{sn}_1 > \var{sn}_2 \lor (\var{sn}_1 = \var{sn}_2 \land c_1 >
c_2)$.
\fi

\if\submit\yes
\NAME uses the metadata directory~\dir
to maintain pointers to the fragments stored at the data \nodes.
\fi
The directory stores an entry for every writer; it contains the
timestamp of its most recently written value, the identities of those
\nodes that have acknowledged to store a fragment of it, a vector
with the hashes of the fragments for ensuring data integrity, and
additional metadata to support concurrent reads and writes.  The linearizable
semantics of protocol~\NAME are obtained from the atomicity of the
metadata directory.

At a high level, the writer first invokes \dir-\op{Scan} on the
metadata to read the highest stored timestamp, increments it, and uses
this as the timestamp of the value to be written.  Then it encodes the
value to $n$ fragments and sends one fragment to each data \node.
The data \nodes store it and acknowledge the write.  After the
writer has received acknowledgments from $t+k$ data \nodes, it
writes their identities (together with the timestamp and the hashes of
the fragments) to the metadata through \dir-\op{Update}.  The reader
proceeds accordingly: it first invokes \dir-\op{Scan} to obtain the
entries of all writers; it determines the highest timestamp among them
and extracts the fragment hashes and the identities of the data
\nodes; finally, it contacts the data \nodes and reconstructs the
value after obtaining $k$ fragments that match the hashes in the
metadata.

Although this simplified algorithm achieves atomic semantics, it does
not address timely garbage-collection of obsolete fragments, the main
problem to be solved for amnesic erasure-code distributed storage.
It is easy to see that overwriting the fragments during the next write
operation may cause a reader to stall.




Protocol \NAME uses two mechanisms to address this: first, the writer
\emph{retains} those values that may be accessed concurrently and
exempts them from garbage collection so that their fragments remain
intact for concurrent readers, which gives the reader enough time to
retrieve its fragments. Secondly, some of the retained values may also
be \emph{frozen} in response to concurrent reads; this forces a
concurrent read to retrieve a value that is guaranteed to exist at the
data \nodes rather than simply the newest value, thereby effectively
limiting the amount of stored values. A similar freezing method has
been used for wait-free atomic storage with replicated
data~\cite{gulevu06,domasu08}, but it must be changed for
erasure-coded storage with separated metadata.  The retention
technique together with the separation of metadata appears novel.

For the two mechanisms, every reader maintains a \emph{reader index}, both in its
local variable \var{readindex} and in its metadata. The reader index serves for
coordination between the reader and the writers. The reader increments its index
whenever it starts a new \var{r}-\op{Read} and immediately writes it to \dir, thereby
announcing its intent to read. Writers access the reader indices after updating
the metadata for a write and before (potentially) erasing obsolete fragments.
Every writer $w$ maintains a table \var{frozenindex} with its most
recent recollection of all reader indices. When the newly obtained index of a
reader $c$ has changed, then $w$ detects that $c$ has started a new operation
at some time after the last write of~$w$.

When $w$ detects a new operation of $c$, it does not know whether $c$ has
retrieved the timestamp from \dir before or after the \dir-\op{Update} of the
current write. The reader may access either value; the writer therefore
\emph{retains} both the current and the preceding value for
$c$ by storing a pointer to them in \var{frozenptrlist} and in
\var{reservedptrlist}.
Clearly, both values have to be excluded from garbage collection by~$w$
in order to guarantee that the reader completes.


However, the operation of the reader~$c$ may access \dir after the
\dir-\op{Update} of one or more subsequent write operation by~$w$,
which means that the \nodes would have to retain every value
subsequently written by $w$ as well.  To prevent this from happening
and to limit the number of stored values, $w$ \emph{freezes} the
currently written timestamp (as well as the value) and forces $c$ to
read this timestamp when it accesses \dir within the same operation.
In particular, the writer stores the current timestamp in
\var{frozenptrlist} at index~$c$ and updates the reader index of $c$
in \var{frozenindex}; then, the writer pushes both tables,
\var{frozenindex} and \var{frozenptrlist}, to the metadata service
during its next \var{r}-\op{Write}. The values designated by
\var{frozenptrlist} (they are called \emph{frozen}) and
\var{reservedptrlist} (they are called \emph{reserved}) are retained
and excluded from garbage collection until~$w$ detects the
next read of $c$, i.e., the reader index of $c$ increases.  Thus, the
current read may span many concurrent writes of $w$ and the fragments
remain available until $c$ finishes reading.



On the other hand, a reader must consider frozen values.  When
a slow read operation spans multiple concurrent writes, the reader~$c$
learns that it should retrieve the frozen value through its entry in
the \var{frozenindex} table of the writer.  More precisely, when $c$
retrieves the metadata from \dir and finds that writer $w$'s
$\var{frozenindex}[c]$ entry equals its $\var{readindex}$ variable,
then $w$ has frozen the value designated by $\var{frozenptrlist}[c]$
for~$c$.

The protocol is amnesic because each writer retains at most two values
per reader, a frozen value and a reserved value.  Every data \node
therefore stores at most two fragments for every reader-writer pair
plus the fragment from the currently written value.  The combination
of freezing and retentions ensures that readers never wait.

\subsection{Details}
\label{sec:details}

\paragraph{Data structures.}
We use abstract data structures for compactness.  In particular, given
a timestamp $\var{ts} = (\var{sn}, c)$, its two fields can be accessed
as $\var{ts}.\var{sn}$ and $\var{ts}.c$.  A data type $\var{Pointers}$
denotes a set of tuples of the form $(\var{ts}, \var{set},
\var{hash})$ with $\var{ts} \in \var{Timestamps}$, $\var{set}
\subseteq [1,n]$, and $\var{hash}[i] \in \strings$ for $i \in [1,n]$.
Their initialization value is $\var{Nullptr} = ((0, \bot), \emptyset,
[\bot, \dots, \bot])$.

A \var{Pointers} structure contains the relevant information about one
stored value.  For example, the writer locally maintains
$\var{writeptr} \in \var{Pointers}$ designating to the most recently
written value.  More specifically, $\var{writeptr}.\var{ts}$ contains
the timestamp of the written value, $\var{writeptr}.\var{set}$
contains the identities of the \nodes that have confirmed to have
stored the written value, and $\var{writeptr}.\var{hash}$ contains the
cross checksum, the list of hash values of the data fragments, of
the written value.


The metadata directory~\var{dir} contains a vector~$M$ with a tuple
for every client~$p \in \clientset$ of the form
\[
  M[p] \ = \
  \bigl(
    \var{writeptr}, \var{frozenptrlist}, \var{frozenindex}, \var{readindex}
  \bigr),
\]
where the field $\var{writeptr} \in \var{Pointers}$ represents the
\emph{written value}, the field \var{frozenptrlist} is an array
indexed by $c \in \clientset$ such that $\var{frozenptrlist}[c] \in
\var{Pointers}$ denotes a value \emph{frozen by $p$ for reader~$c$},
and the integer \var{readindex} denotes the reader-index of~$p$.

For preventing that concurrently accessed fragments are
garbage-collected, the writer maintains two tables,
\var{frozenptrlist}, and \var{reservedptrlist}, each containing one
\var{Pointers} entry for every reader in \clientset.  The second one,
\var{reservedptrlist}, is stored only locally, together with the
\var{frozenindex} table, which denotes the writer's most recently
obtained copy of the reader indices.  For the operations of the
reader, only the local \var{readindex} counter is needed.

Every client maintains the following variables between operations:
\var{writeptr}, \var{frozenptrlist}, \var{frozenindex}, and
\var{reservedptrlist} implement freezing, reservations, and retentions
for writers as mentioned, and \var{readindex} counts the reader
operations.

When clients access \dir, they may not be interested to retrieve all
fields or to update all fields.  For clarity we replace the fields to
be ignored by $\ast$ in those \dir-\op{Scan} and \dir-\op{Update}
operations.


\paragraph{Operations.}
At the start of a write operation, the writer~$w$ saves the current
value of \var{writeptr} in \var{prevptr}, to be used later during its
operation, if $w$ should reserve and retain that value.  Then $w$
determines the timestamp of the current operation, which is stored in
$\var{writeptr}.\var{ts}$.  After computing the fragments of~$v$,
sending them to the data \nodes, and obtaining $t+k$ acknowledgements,
the writer updates its metadata entry.  It writes \var{writeptr},
pointing to~$v$, together with \var{frozenptrlist} and
\var{frozenindex}, as they resulted after the previous write to \dir.  Then
$w$ invokes \dir-\op{Scan} and acquires the current metadata~$M$,
which it uses to determine values to freeze and to retain.  It
compares the acquired reader indices with the ones obtained during its
last write (as stored in \var{frozenindex}).  When $w$ detects a read
operation by~$c$ because $M[c].\var{readindex} >
\var{frozenindex}[c]$, it freezes the current value (by setting
$\var{frozenptrlist}[p]$ to \var{writeptr})
and reserves the previously written value (by setting
$\var{reservedptrlist}[p]$ to \var{prevptr}).  Finally, the writer
deletes all fragments at the data \nodes except for those of the
currently written and the retained values.























To determine the timestamps for retrieving fragments, the reader
uses the following two functions:
\begin{minipage}{0.4\textwidth}
\begin{tabbing}\small
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \> \textbf{function} $\op{readfrom}(M, c, p, \var{index})$ \textbf{is} \\
  \> \> \textbf{if} $\var{index} > M[p].\var{frozenindex}[c]$ \textbf{then} \\
  \> \> \> \textbf{return} $M[p].\var{writeptr}$ \\
  \> \> \textbf{else} \quad // $\var{index} = M[p].\var{frozenindex}[c]$ \\
  \> \> \> \textbf{return} $M[p].\var{frozenptrlist}[c]$ \end{tabbing}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\vspace{9mm}
\begin{tabbing}\small
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \> \textbf{function} $\op{highestread}(M, c, \var{index})$ \textbf{is} \\
  \> \> $\var{max} \becomes \var{Nullptr}$ \\
  \> \> \textbf{forall} $p \in \clientset$ \textbf{do} \\
  \> \> \> $\var{ptr} \becomes \op{readfrom}(M, c, p, \var{index})$ \\
  \> \> \> \textbf{if} $\var{ptr}.\var{ts} > \var{max}.\var{ts}$
              \textbf{then} \\
  \> \> \> \> \var{max} \becomes \var{ptr} \\
  \> \> \textbf{return} $\var{max}$
\end{tabbing}
\end{minipage}

\newpage



\noindent
Upon retrieving the array $M$ from \dir, the reader sets
$\var{readptr} \becomes \op{highestread}(M, c, \var{readindex})$,
which implements the logic of accessing frozen timestamps.
\if\submit\no
The two functions above ensure that
\begin{tabbing}\small
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \> $\op{readfrom}(M, c, p, \var{index}) \: = \:$ \\
  \> \> $\bigl( \var{ptr} \in \var{Pointers} \; : \; \bigr.$ \\
  \> \> \> $(\var{ptr} = M[p].\var{writeptr} \land
             \var{index} > M[p].\var{frozenindex}[c])$ \\
  \> \> \> $\bigl. \mbox{} \lor (\var{ptr} = M[p].\var{frozenptrlist}[c] \land
            \var{index} = M[p].\var{frozenindex}[c]) \bigr) $ \\
\\
  \> $\op{highestread}(M, c, \var{index}) \: = \:$ \\
  \> \> $\arg\max_{\var{ptr} \in \var{Readset}}
         \bigl\{ \var{ptr}.\var{ts} \bigr\}$,
         where $\var{Readset} = \{ \op{readfrom}(M, c, p, \var{index}) \;|\;
         p \in \clientset \}$
\end{tabbing}
\noindent
\fi
The details of protocol~\NAME appear in
\if\submit\yes
Algorithms~\ref{alg:client-1}--\ref{alg:client-2}.
\else
Algorithms~\ref{alg:client-1}--\ref{alg:datareplica}.
\fi



\begin{alg}\small
\begin{tabbing}
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
\if\submit\no
  \textbf{Uses} \\
  \> Atomic snapshot object, \instance{dir} \\
  \> Data \nodes, \textbf{instances} $d_1, \dots, d_n$ \\
  \\
\fi
  \textbf{State} \\
  \> // State maintained across write and read operations \\

\> $\var{writeptr} \in \var{Pointers}$, initially $\var{Nullptr}$
     \` // Metadata of the currently written value \\
  \> $\var{frozenptrlist}[p] \in \var{Pointers}$, initially $\var{Nullptr}$,
     for $p \in \clientset$
     \` // Value frozen and retained for reader $p$ \\
  \> $\var{reservedptrlist}[p] \in \var{Pointers}$, initially $\var{Nullptr}$,
     for $p \in \clientset$
     \` // Value reserved and retained for reader~$p$ \\
  \> $\var{frozenindex}[p] \in N_0$, initially 0, for $p \in \clientset$
     \` // Last known reader index of~$p$ \\

\> $\var{readindex} \in N_0$, initially 0
  \` // Reader index of~$c$ \\

  \> // Temporary state during operations \\
  \> $\var{prevptr} \in \var{Pointers}$, initially $\var{Nullptr}$
     \` // Metadata of the value written by $c$ prior to current write \\
  \> $\var{readptr} \in \var{Pointers}$, initially $\var{Nullptr}$
     \` // Metadata of the value to be read by $c$ \\
  \> $\var{readlist}[i] \in \strings$, initially $\bot$, for $i \in [1,n]$
     \` // List of \nodes that have responded during read \\
  \\
  \textbf{upon} \eventt{r}{Write}{$v$} \textbf{do} \\
  \> $\var{prevptr} \becomes \var{writeptr}$ \\
\> \newe \event{dir}{Scan};
     \textbf{wait for} \eventt{dir}{ScanResp}{$M$} \\
  \> $(\var{wsn}, \ast) \becomes
        \max \{ M[p].\var{writeptr}.\var{ts} \;|\; p \in \clientset \}$
     \` // Highest timestamp field~\var{ts} in a \var{writeptr} in $M$ \\
  \> $\var{writeptr}.\var{ts} \becomes (\var{wsn} + 1, c)$
     \` \> // Construct metadata of the currently written value \\
  \> $\var{writeptr}.\var{set} \leftarrow \emptyset$ \\
  \> $[v_1, \ldots, v_n] \leftarrow \var{Encode}_{k,n}(v)$ \\
  \> \textbf{forall} $i \in [1,n]$ \textbf{do} \\
  \> \> $\var{writeptr}.\var{hash}[i] \becomes H(v_i)$ \\
  \> \> \newe \eventt{$d_i$}{Write}{$\var{writeptr}.\var{ts}, v_i$} \\
  \\
  \textbf{upon} \eventt{$d_i$}{WriteAck}{\var{ats}} \textbf{such that}
  	$\var{ats} = \var{writeptr}.\var{ts} \land
         |\var{writeptr}.\var{set}| < t+k$ \textbf{do} \\
  \> $\var{writeptr}.\var{set} \becomes \var{writeptr}.\var{set} \cup \{i\}$ \\
  \> \textbf{if} $|\var{writeptr}.\var{set}| = t+k$ \textbf{then} \\
\> \> // Update metadata at \dir with currently written value
           and with frozen values from previous write \\
  \> \> \newe \eventt{dir}{Update}{$c, (\var{writeptr},
           \var{frozenptrlist}, \var{frozenindex}, \ast)$};
        \textbf{wait for} \event{dir}{UpdateAck} \\
\> \> // Obtain current reader indices \\
  \> \> \newe \event{dir}{Scan};
        \textbf{wait for} \eventt{dir}{ScanResp}{$M$} \\
  \> \> $\var{freets} \becomes \{ \var{prevptr}.\var{ts} \}$ \\
  \> \> \textbf{forall} $p \in \clientset \setminus \{c\}$ \textbf{do} \\
  \> \> \> $(\ast, \ast, \ast, \var{index}) \becomes M[p]$ \\
  \> \> \> \textbf{if} $\var{index} > \var{frozenindex}[p]$ \textbf{then}
           \` // Client~$p$ may be concurrently reading \var{prevptr}
                 or \var{writeptr} \\
  \> \> \> \> $\var{freets} \becomes \var{freets}
                 \cup \{ \var{frozenptrlist}[p].\var{ts},
                         \var{reservedptrlist}[p].\var{ts} \}$ \\
  \> \> \> \> $\var{frozenptrlist}[p] \becomes \var{writeptr}$;
              $\var{frozenindex}[p] \becomes \var{index}$ \\
  \> \> \> \> $\var{reservedptrlist}[p] \becomes \var{prevptr}$ \\
  \> \> $\var{freets} \becomes \var{freets} \setminus
           \bigcup_{p \in \clientset} \{ \var{frozenptrlist}[p].\var{ts},
                                         \var{reservedptrlist}[p].\var{ts} \}$ \\
  \> \> \textbf{forall} $j \in [1,n]$ \textbf{do}
        \` // Clean up all fragments except for current, frozen, and reserved values \\
  \> \> \> \newe \eventt{$d_j$}{Free}{\var{freets}}\\
\> \> \newe \event{r}{WriteAck}
\end{tabbing}
\caption{Protocol~\NAME, atomic register instance~\var{r}
  for client~$c$ (part~1).}
\label{alg:client-1}
\end{alg}


\begin{alg}\small
\begin{tabbing}
  xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
\textbf{upon} \event{r}{Read} \textbf{do} \\
  \> \textbf{forall} $i \in [1,n]$ \textbf{do}
        $\var{readlist}[i] \becomes \bot$ \\
  \> $\var{readindex} \becomes \var{readindex} + 1$ \\
  \> \newe \eventt{dir}{Update}{$c,
        (\ast, \ast, \ast, \var{readindex})$};
     \textbf{wait for} \event{dir}{UpdateAck} \\
  \> // Parse the content of \dir and extract the
      highest timestamp, potentially frozen for~$c$ \\
  \> \newe \event{dir}{Scan};
     \textbf{wait for} \eventt{dir}{ScanResp}{$M$} \\
  \> $\var{readptr} \becomes \op{highestread}(M, c, \var{readindex})$ \\
  \> \textbf{if} $\var{readptr}.\var{ts} = (0,\bot)$ \textbf{then} \\
  \> \> \newe \eventt{r}{ReadResp}{$\bot$} \\
  \> \textbf{else} // Contact the data \nodes to obtain the data fragments\\
  \> \> \textbf{forall} $i \in \var{readptr}.\var{set}$ \textbf{do} \\
  \> \> \> \newe \eventt{$d_i$}{Read}{$\var{readptr}.\var{ts}$} \\
  \\
  \textbf{upon} \eventt{$d_i$}{ReadResp}{$\var{vts}, v$}
	\textbf{such that} $\var{vts} = \var{readptr}.\var{ts} \land
	\var{readlist}[i] = \bot$ \textbf{do} \\
  \> \textbf{if} $v \neq \bot \land H(v) = \var{readptr}.\var{hash}[i]$
        \textbf{then} \\
  \> \> $\var{readlist}[i] \becomes v$ \\
  \> \> \textbf{if} $\bigl|\{j| \var{readlist}[j]\neq \bot\}\bigr| = k$
     \textbf{then} \\
  \> \> \> $\var{readptr} \becomes \var{Nullptr}$ \\
  \> \> \> \var{retval} \becomes $\var{Reconstruct}_{k,n}(\var{readlist})$ \\
  \> \> \> \newe \eventt{r}{ReadResp}{\var{retval}}
\end{tabbing}
\caption{Protocol~\NAME, atomic register instance~\var{r}
  for client~$c$ (part~2).}
\label{alg:client-2}
\end{alg}

\if\submit\no

\begin{alg}\small
\begin{tabbing}
  xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \textbf{State} \\
  \> $\var{data}[\var{ts}] \in \strings$, initially $\bot$,
     for $\var{ts} \in \var{Timestamps}$
     \` // Stored data values indexed by timestamp \\
  \\
  \textbf{upon} \eventt{$d_i$}{Write}{$\var{ts}, v$} \textbf{do} \\
  \> $\var{data}[\var{ts}] \becomes v$\\
  \> \newe \eventt{$d_i$}{WriteAck}{$\var{ts}$}\\
  \\
  \textbf{upon} \eventt{$d_i$}{Read}{$\var{ts}$} \textbf{do} \\
  \> \newe \eventt{$d_i$}{ReadResp}{$\var{ts}, \var{data}[\var{ts}]$} \\
  \\
  \textbf{upon} \eventt{$d_i$}{Free}{$\var{freets}$} \textbf{do}\\
  \> \textbf{forall} $\var{ts} \in \var{freets}$ \textbf{do} \\
  \> \> $\var{data}[\var{ts}] \becomes \bot$ \\
  \> \newe \eventt{$d_i$}{FreeAck}{$\var{ts}$}
\end{tabbing}
\caption{Protocol~\NAME, implementation of data \node~$d_i$.}
\label{alg:datareplica}
\end{alg}

\fi




\if\submit\no

\paragraph{Remarks.}

Note that \NAME does not need a majority of correct data \nodes and
neither refers to quorum systems for correctness; these aspects are
all encapsulated in the directory service.  For liveness, though, the
protocol needs to obtain responses from $t+k$ data \nodes during write
operations, which is only possible if $n \geq 2t+k$.

In the current formulation of \NAME, every writer retains exactly two
values for each reader, regardless of whether the reader has completed
its operation.  In fact, a value continues to be retained
for a reader~$c$ until $c$ invokes a subsequent \var{r}-\op{Read} (and
concurrently or later, the writer invokes another \var{r}-\op{Write}).
In order to avoid retaining unnecessary values, one could introduce an
additional field in the metadata for each reader, through which the
reader can signal when it completes a read operation.  The writer
would periodically check this and remove the values no longer needed.

The data \nodes can be implemented from a key-value store (KVS)
abstraction that has become a prominent interface for cloud-storage
systems.  A KVS can be implemented from read/write registers, as shown
by Cachin et al.~\cite{cajuso12}, though their implementation does
not preserve the space complexity.

\fi

\begin{comment}
\note{Readers may be Byzantine.}

In current formulation and the complexity analysis presented in
Section~\ref{} we implicitely assume that there are no Byzantine clients.
Clearly, given that a Byzantine writer could update \dir with timestamps that
do not correspond to values stored to data \nodes, readers could be
stalled from completing.
However, \NAME algorithms could resist such poisonous writer attacks with a
slight modification on the reader side. In particular, we could modify the
\var{highestread} operation such that it returns the highest $t+1$ timestamps
from the timestamps returned from the \var{readfrom} invocations within.
Then the reader could issue read requests for the data fragments stored
in each data \node for all $t+1$ values. Given that at most $t$ entries
are false, the reader is guaranteed to reconstruct at least one value.
Clearly, such an extension would increase the communication overhead of
\op{Read} into $(t+1)(t+k)\ell_v/k$ bits.

\note{The optimistic scheme of ORCAS~\cite{dugule08} can be an
  extension of this, if synchrony is assumed.  (ORCAS-A or -B??}

\note{In current formulation all data \nodes store the fragment, but
this is not needed; change d-WriteAck handler so that
additional parameter gcts denotes which data can be erased (all up
to and including gcts); for \nodes in wset, gcts=ts-1, for the
others gcts==ts, ie., they may erase the fragment again.}

\note{Address consistency amplification aspect.}
\note{Garbage collection could be disconnected from register-write operations.}

\end{comment}




\section{Complexity comparison}
\label{sec:complexity}

This section compares the communication and storage complexities of
\NAME to existing erasure-coded distributed storage solutions, in a
setting with $n$ data \nodes and $m$~clients.  We denote the size of
each stored value $v\in \values$ by $\ell = \lceil \log_2 |\values|
\rceil$.  In line with the intended deployment scenarios, we assume
that $\ell$ is much larger (by several orders of magnitude) than $n^2$
and $m^2$ , i.e., $\ell \gg n^2$ and $\ell \gg m^2$.

We examine the worst-case communication and storage costs incurred by
a client in the protocol and distinguish metadata operations (on \dir)
from operations on the data \nodes with data (i.e., erasure-coded
fragments of data values).

For protocol~\NAME, the metadata of one value written to \dir consists
of a pointer, containing the cross checksum with $n$ hash values, the
$t+k$ identities of the data \nodes that store a data fragment, and a
timestamp.  Moreover, the metadata entry of one writer contains also
the list of $m$ pointers to frozen values, the $m$ indices relating to
the frozen values, and the writer's reader index.  Assuming a
collision-resistant hash function with output size~$\lambda$ bits and
timestamps no larger than $\lambda$ bits, the total size of the
metadata is $O(m^2 n \lambda)$.  (Note that a $2^\lambda$-bit counter
suffices for all protocol executions where the hash function is
secure, as collisions in hash functions can be found with about
$2^{\lambda/2}$ operations.)  In the remainder of this section, the
size of the metadata is considered to be negligible and is ignored,
though it would incur in practice.

According to the above assumption, the complexity of \NAME is
dominated by the data itself.  When writing a value $v \in \values$,
the writer sends a fragment of size $\ell/k$ and a timestamp of size
$\lambda$ to each of the $n$ data \nodes.  Assuming further that $\ell
\gg \lambda$, the total storage space occupied by $v$ at the data
\nodes amounts to $n \ell / k$ bits.  Similarly, a read operation
incurs a communication cost of $(t+k) k / \ell$ bits.

With respect to storage complexity, protocol \NAME freezes and
reserves two timestamps and their fragments for each writer-reader
pair, and additionally stores the fragments of the last written value
for each writer.  This means that the storage cost is at most $2 m^2 n
\ell / k$ bits in total.  The improvement described in a remark of
Section~\ref{sec:details} reduces this to $2 m n \ell / k$ in the best
case.




\begin{table}
 \centering
\begin{tabular}{|l||cc|c|}
 \hline
 {{\bf Protocol}}
 & \multicolumn{2}{|c|}{{\bf Communication cost} }
 & \multicolumn{1}{|c|}{{\bf Storage cost}}\\

 & \multicolumn{1}{|c}{\op{Write}}
 & \multicolumn{1}{c|}{\op{Read}}
 &
\\\hline

 ORCAS-A~\cite{dugule08}
 &  $(1+m) n\ell$
 &  $ 2n \ell$
 &  $ n \ell$\\ \hline

 ORCAS-B~\cite{dugule08}
 & $(1+m) n \ell/k$
 & $2 n \ell/k$
 & $m n \ell/k$\\ \hline

 CASGC~\cite{clmm13}
 & $n \ell/k$ \T
 & $\infty$
 & $m n \ell/k$ \\ \hline

 CT~\cite{cactes06}
 & $(n+m) n \ell/(k+t)$
 & $\ell$ \T
 & $n\ell/(k+t)$ \T \\ \hline

 HGR~\cite{hegare07b}
 & $n \ell/k$ \T
 & $\infty$
 & $m n \ell/k$\\  \hline

 M-PoWerStore~\cite{dklmsv13}
 & $n \ell/k$ \T
 & $n \ell/k$
 & $\infty$ \\ \hline

 DepSky~\cite{bcqas11}
 & $n \ell/k$ \T
 & $n \ell/k$
 & $\infty$ \\ \hline

 \NAME (Sec.~\ref{sec:protocol})
 & $n \ell/k$ \T
 & $(t+k) \ell/k$
 & $2m^2n \ell/k$\\\hline
\end{tabular}
\caption{Comparison of the communication and space complexities of
  erasure-coded distributed storage solutions.  There are $m$ clients, $n$
  data \nodes, the erasure code parameter is $k=n-2t$, and the data values
  are of size $\ell$ bits. An asterisk (\T) denotes optimal properties.}
\label{tab:complexity}
\if\submit\yes
\vspace*{-4mm}
\fi
\end{table}

Table~\ref{tab:complexity} shows the communication and storage costs
of protocol \NAME and the related protocols.
We use the wait-free semantics achieved by \NAME and others as the
base case; in CASGC~\cite{clmm13} and HGR~\cite{hegare07b}, a read
operation concurrent with an unbounded number of writes may not
terminate, hence we state their cost as~$\infty$.  In contrast to
\NAME, DepSky~\cite{bcqas11} is neither wait-free nor amnesic and
M-PoWerStore~\cite{dklmsv13} is not amnesic.  It is easy to see that
\NAME performs better than most storage solutions in terms
communication complexity.


\if\submit\no
\section{Analysis}
\label{sec:proof}
\fi

\newcommand{\opr}{\ensuremath{o}\xspace}
\newcommand{\popr}{\ensuremath{o'}\xspace}
\newcommand{\opread}{\ensuremath{\op{Read}}\xspace}
\newcommand{\opwrite}{\ensuremath{\op{Write}}\xspace}
\newcommand{\tsopr}{\ensuremath{\var{ts}}\xspace}
\newcommand{\tspopr}{\ensuremath{\var{ts}'}\xspace}

\if\submit\no
In this section we prove that protocol \NAME, given by
Algorithms~\ref{alg:client-1}--\ref{alg:datareplica}, emulates an
atomic read/write register and is wait-free.

Whenever the metadata directory \var{dir} contains an entry $\var{ts}
= M[c].\var{frozenptrlist}[p].\var{ts}$ we say that timestamp~\var{ts}
is \emph{frozen by $c$ for $p$}.  If \var{ts} is frozen by some $c$
for any~$p$, then \var{ts} is simply \emph{frozen}.  Furthermore,
considering the state of writer~$c$, a timestamp~\var{ts} is said to
be \emph{retained by $c$ for $p$} when either
$\var{frozenptrlist}[p].\var{ts} = \var{ts}$ (this includes that
\var{ts} is frozen by $c$ for $p$) or when
$\var{reservedptrlist}[p].\var{ts} = \var{ts}$ (which means that
\var{ts} is reserved by $c$ for $p$).  A timestamp is \emph{retained}
by $c$ when it is retained by $c$ for some~$p$.  We call the timestamp
$M[c].\var{writeptr}.\var{ts}$ the \emph{written} timestamp of $c$.

\begin{lemma}[Frozen timestamps]\label{lem:timestamps}
  At any time the timestamps that a client has frozen
  are no larger than its written timestamp. More precisely, for all
  $c, p \in \clientset$,
  \[
    M[c].\var{writeptr}.\var{ts}
      \ > \ M[c].\var{frozenptrlist}[p].\var{ts}.
  \]
  Moreover, during any \dir-\op{Update} operation of~$c$, the
  timestamp $M[c].\var{writeptr}.\var{ts}$ and all timestamps
  $M[c].\var{frozenptrlist}[p].\var{ts}$ may only increase.
\end{lemma}

\begin{proof}
  From Algorithm~\ref{alg:client-1} it follows that for any client $c$, the
  timestamps stored in $M[c].\var{writeptr}.\var{ts}$ in successive
  \var{r}-\op{Write} operations of $c$ increase.
  From the same algorithm, it is clear that
  $M[c].\var{frozenptrlist}[p].\var{ts}$ is only updated through a
  \var{r}-\op{Write} operation of $c$, and is set to the written timestamp
  of the preceding \var{r}-\op{Write} operation of $c$, which is
  strictly smaller than the written timestamp stored in
  $M[c].\var{writptr}.\var{ts}$.  The second inequality
  follows analogously.  Thus, the values stored in
  $M[c].\var{frozenptrlist}[p].\var{ts}$ only increase.
\end{proof}

We define the \emph{timestamp of a register operation \opr} as
follows: (i) for an \var{r}-\op{Write} operation, the timestamp of
\opr is the value assigned to variable~\var{writeptr}.\var{ts}
during~\opr; (ii) when \opr is an \var{r}-\op{Read} operation, then
its timestamp is the value assigned to variable~\var{readptr}.\var{ts}
by \op{highestread}.  Note that the timestamp of an \var{r}-\op{Read}
operation is $(0, \bot)$ if and only if \opr returns~$\bot$.
Furthermore, we say that a value $v$ is \emph{associated} to a
timestamp \var{ts} whenever the timestamp of the register operation
that writes $v$ is \var{ts}.

According to \op{highestread}, the timestamp in the returned pointer
may be frozen (taken from the \var{frozenptrlist} field of $M$) or written
(taken from the \var{writeptr} field of $M$), but not both.

\begin{lemma}[Read frozen timestamp]\label{lem:readfrozen}
  If the timestamp \var{ts} of a \var{r}-\op{Read} operation~$o_r$ by
  client~$c$ has been frozen for $c$ by a client~$w$, then $w$
  executes two \var{r}-\op{Write} operations concurrently to~$o_r$,
  where the \dir-\op{Scan} operation of the former \var{r}-\op{Write}
  operation~$o_{w,1}$ and the \dir-\op{Update} operation of the latter
  \var{r}-\op{Write} operation~$o_{w,2}$ occur between
  \dir-\op{Update} and \dir-\op{Scan} operations of~$o_r$.  Moreover,
  the timestamp of the \var{r}-\op{Read} operation~$o_r$ is \var{ts},
  the one associated with the value written by~$o_{w,1}$.
\end{lemma}

\begin{proof}
  From Algorithm~\ref{alg:client-2} it follows that for
  \op{highestread} within $o_r$ to return a frozen timestamp, then, if
  $M$ is the metadata snapshot returned by the \dir-\op{Scan}
  operation during~$o_r$, it holds $M[w].\var{frozenindex}[c] =
  \var{readindex}$. This means that $w$ invoked \dir-\op{Update} with
  the most recent value of \var{readindex} before the \dir-\op{Scan}
  during~$o_r$. To do that, $w$ must have detected the change of the
  \var{readindex} entry in $M[c]$ caused by $o_r$ through the
  \dir-\op{Scan} operation invoked during~$o_{w,1}$. From
  Algorithm~\ref{alg:client-1}, this can only be the operation through
  which $w$ wrote the value associated to~\var{ts}.
\end{proof}


\begin{lemma}[Partial order]\label{lem:partorder}
  Let \opr and \popr be two distinct operations on register~\var{r}
  with timestamps~\tsopr and \tspopr, respectively, such that \opr
  precedes \popr. Then $\tsopr \leq \tspopr$.  Furthermore, if \popr
  is of type \var{r}-\op{Write}, then $\tsopr < \tspopr$.
\end{lemma}

\begin{proof}
We distinguish between two cases, depending on the type of~\opr.
\begin{description}
\item[Case 1:]
If \opr is of type \var{r}-\op{Write}, the claim follows directly from
  Lemma~\ref{lem:timestamps} and from the algorithm of the writer.
  In particular, if \popr is of type \var{r}-\op{Read}, then, if there is
  no concurrent \var{r}-\op{Write} operation of the same client $w$ as \opr,
  \tsopr{} is returned as written timestamp by the \var{readfrom} function when
  called for $w$ and reader of \popr.
  In addition, if \popr runs concurrently with a
  \var{r}-\op{Write} of $w$, then one of the two hold:
  (i) \tsopr{} (or a higher timestamp if many \var{r}-\op{Write} operations
  have intervened) is frozen for \popr{} and
  is returned by the \var{readfrom} operation invoked by \var{highestread}
  in \popr for $w$, (ii) \tsopr (or a higher timestamp if many
  \var{r}-\op{Write} operations have intervened) has not yet been frozen
  by $w$, in which case a written timestamp greater or equal to \tsopr
  (by Lemma~\ref{lem:timestamps}) is returned by the \var{readfrom} operation
  invoked by \var{highestread} in \popr for $w$.

\item[Case~2:] If \opr is of type \var{r}-\op{Read}, then let
  $\var{ts}^*$ be the maximum value of the timestamp field~\var{ts} in
  a \var{writeptr} at the time when the \dir-\op{Scan} operation
  invoked by \opr returns.  Note that \op{highestread} obtains \tsopr
  as this maximum or as a frozen timestamp.
  Lemma~\ref{lem:timestamps} implies now that $\tsopr \leq
  \var{ts}^*$.

  We now show that $\var{ts} \leq \tspopr$ by distinguishing two
  cases.  First, if \popr is of type \var{r}-\op{Write}, the writer
  calls \dir-\op{Scan} after $o$ completes and determines the maximum
  value of the~\var{ts} field in any \var{writeptr}.  Then it
  increments that timestamp to obtain \tspopr.  This ensures that
  $\tspopr > \var{ts}^* \geq \tsopr$, as claimed.

  Second, if \popr is of type \var{r}-\op{Read}, then \tspopr may
  either have been a written timestamp or a frozen timestamp (at the
  time when the client obtained the response of its \dir-\op{Scan}).
  If \tspopr has been written, then \tspopr is the maximum value of
  the~\var{ts} field in any \var{writeptr}, which is at least as large
  as $\var{ts}^*$ by Lemma~\ref{lem:timestamps} and by the atomicity
  of~\dir.

  Alternatively, if \tspopr  has been frozen by writer~$w$, then
  Lemma~\ref{lem:readfrozen} applies and shows that there exist two
  \var{r}-\op{Write} operations by $w$ that are  concurrent
  to \popr, of which the first writes the value associated to \tspopr.
As such, if $ts_w$ is the timestamp returned by
  the \var{readfrom} function invoked by any \var{r}-\op{Read} operation \opr
  that precedes \popr{} and for writer $w$, then $\tsopr_w \leq \tspopr$.
  Since this can be extended to all writers, it holds that $\tsopr \leq \tspopr$.
\end{description}
\end{proof}


\begin{lemma}[Unique writes]\label{lem:unqwrites}
  If \opr and \popr are two distinct operations of type
  \var{r}-\op{Write} with timestamps \tsopr and \tspopr, respectively,
  then $\tsopr \neq \tspopr$.
\end{lemma}

\begin{proof}
  If \opr and \popr are executed by different clients, then the two
  timestamps differ in their second component. If \opr and \popr are
  executed by the same client, then the client executed them
  sequentially. By Lemma~\ref{lem:partorder}, it holds $\tsopr \neq
  \tspopr$.
\end{proof}





\begin{lemma}[Integrity]\label{lem:integr}
  Let $o_r$ be an operation of type \var{r}-\op{Read} with timestamp
  $\var{ts}_r$ that returns a value $v \neq \bot$.  Then there is a
  unique operation $o_w$ of type \var{r}-\op{Write} that writes $v$
  with timestamp~$\var{ts}_w = \var{ts}_r$.
\end{lemma}

\begin{proof}
  Operation~$o_r$ by client~$c$ returns~$v$ and is, thus, complete.
  This means that the client has processed $k$ events of type
  $d_i$-\op{ReadResp} from distinct \nodes in a set $\replicaset_k$;
  according to the protocol, the client has verified that the response
  from every $d_i \in \replicaset_k$ contains a
  timestamp~$\var{vts}_i$ and a fragment~$v_i$ such that $\var{vts}_i
  = \var{ts}_r$ and $H(v_i) = \var{readptr.hash}[i]$.



  According to the code, the value \var{readptr} is computed from a
  \var{writeptr} or a $\var{frozenptr}[c]$ entry stored in the metadata
  directory~\dir.  This pointer must have been computed during the
  write operation with timestamp~$\var{ts}_w$ and was later stored in
  \dir by the same client.  Note that by Lemma~\ref{lem:unqwrites}, no
  other write has timestamp $\var{ts}_w$.  From the algorithm of the
  writer, it follows that the entries in \var{readhash} were generated
  as hash values of the fragments, i.e., $\var{readhash}[i] =
  H(\bar{v}_i)$, where $\bar{v}_i$ for $i=1,\dots,n$ represent the
  erasure-coded fragments of some value~$\bar{v}$.

  Based on the check by the reader and the security property of the
  hash function, this means that $v_i = \bar{v}_i$ for all $i \in
  \replicaset_k$. The completeness of the erasure code now implies
  that the reconstruction yields $\bar{v} = v$, the value associated
  to~$\var{ts}_w$ and written by $o_w$.
\end{proof}



\newcommand{\opwi}[1]{\ensuremath{o_{w,#1}}\xspace}
\newcommand{\updatewi}[1]{\dir-\ensuremath{\var{Update}_{w,#1}}\xspace}
\newcommand{\scanwi}[1]{\dir-\ensuremath{\var{Scan}_{w, #1}}\xspace}
\newcommand{\updaterd}{\dir-\ensuremath{\var{Update}_r}\xspace}
\newcommand{\scanrd}{\dir-\ensuremath{\var{Scan}_r}\xspace}

\begin{lemma}[Read concurrent with multiple writes]\label{lem:concurrent}
  Consider an operation~$o_r$ of type \var{r}-\op{Read} invoked by a
  reader~$c$, with timestamp $\var{ts}_r$.  At the time when $c$
  determines~$\var{ts}_r$ (by \op{highestread}), there are at least
  $k$ distinct correct data \nodes that store a data fragment
  (different from~$\bot$) under timestamp~$\var{ts}_r$ and they do not
  free this fragment before $c$ completes~$o_r$.
\end{lemma}

\begin{proof}
  Suppose that $\var{ts}_r = (\var{sn}, w)$ and the writer is
  client~$w$.  Consider a sequence $\opwi{1}, \ldots, \opwi{m}$ of
  \var{r}-\op{Write} operations executed by $w$ with respective
  timestamps $\var{ts}_{w,1}, \ldots, \var{ts}_{w,m}$, of which some
  are concurrent to~$o_r$.  Now consider the linearization of \dir and
  let \opwi{i} be the last one among these \var{r}-\op{Write}
  operations whose \dir-\op{Update} (denoted by \updatewi{i}) precedes
  the \dir-\op{Update} operation of the reader during~$o_r$ (denoted
  by \updaterd).  Let \var{readindex} denote the reader's index
at the time when~$c$ invokes \updaterd.

  W.l.o.g.\ suppose that \updaterd follows at least one
  \dir-\op{Update} operation that is triggered by an
  \var{r}-\op{Write} operation of~$w$; furthermore, suppose that $w$
  executes at least one more \var{r}-\op{Write} operation
  \updatewi{i+1} after \updatewi{i}.

  We claim that $\var{ts}_r = \var{ts}_{w,i} \lor \var{ts}_r =
  \var{ts}_{w,i+1}$.  To show this, we distinguish four cases,
  considering the linearization of operations on~\dir.  Let \scanwi{i}
  denote the second invocation of \dir-\op{Scan} during \opwi{i}, the
  one from which the writer takes~\var{readindex}.\begin{description}
  \item[Case~1:] Suppose that \updaterd precedes \scanwi{i}; this
    means that $w$ detects the concurrent read~$o_r$ during \opwi{i},
    in the sense that~$w$ updates its variable $\var{frozenindex}[c]$ to
    \var{readindex}.

    (Case~1.a) If the \dir-\op{Scan} operation of the reader~$c$
    during~$o_r$, denoted by \scanrd, precedes \updatewi{i+1}, then
    $c$ obtains $\var{ts}_r = \var{ts}_{w,i}$ as the highest timestamp
    stored in~$M$ by the algorithm.

    (Case~1.b) Otherwise, \scanrd follows \updatewi{i+1}; then the
    reader~$c$ obtains $M$ such that $M[w].\var{frozenindex}[c]$ is equal
    to~\var{readindex} and $\var{ts}_r =
    M[w].\var{frozenptrlist}[c].\var{ts} = \var{ts}_{w,i}$, according to
    \op{readfrom} in the protocol and because $M[w].\var{frozenindex}[c]$
    is equal to~\var{readindex}.


  \item[Case~2:] Suppose that \updaterd follows \scanwi{i}.  This
    means that \updaterd{} takes place between \scanwi{i} and
    \updatewi{i+1} and $w$ detects the concurrent read~$o_r$ only
    during \opwi{i+1}, after executing \scanwi{i+1}.  The same two
    sub-cases may occur now.

    (Case~2.a) If \scanrd precedes \updatewi{i+1}, then $\var{ts}_r =
    \var{ts}_{w,i}$, analogous to Case~1.a.

    (Case~2.b) Otherwise, \scanrd follows \updatewi{i+1} and the
    reader obtains $\var{ts}_r = \var{ts}_{w,i+1}$.  To see this,
    suppose that (Case~2.b.i) \scanrd precedes the \updatewi{i+2} in
    the subsequent \var{r}-\op{Write} operation of~$w$ or there is no
    such \var{r}-\op{Write}; then, the value \var{readindex} of $c$
    remains greater than $M[w].\var{frozenindex}[c]$ and thus $c$ sets
    $\var{ts}_r = \var{ts}_{w,i+1}$.  Alternatively (Case~2.b.ii),
    suppose that \scanrd follows \updatewi{i+2}; then, according to
    the protocol, the writer has already set $M[w].\var{frozenindex}[c] =
    \var{readindex}$ during \updatewi{i+2} and $c$ sets $\var{ts}_r =
    \var{ts}_{w,i+1}$ analogous to Case~1.b.
  \end{description}

  Suppose the reader determines that $\var{readptr}.\var{ts} =
  \var{ts}_r$; then the correct \nodes in $\var{readptr}.\var{set}$
  store a fragment of the associated value because at least $t + k$
  \nodes in \var{readptr}.\var{set} have sent a
  \var{$d_i$}-\op{WriteAck} for~$\var{ts}_r$ to the writer.
  Accounting for the up to $t$ faulty \nodes, at least $k$ correct
  \nodes have once stored a fragment in $\var{data}[\var{ts}_r]$.
It remains to argue why these \nodes do not free this fragment
  before $c$ completes~$o_r$.

  In Case~1.a, the writer detects the concurrent read during \opwi{i}
  and therefore excludes the data fragments associated to $\var{ts}_r$
  from garbage collection for~$c$, by setting
   $\var{frozenptrlist}[r].\var{ts}$ to $\var{ts}_r$ in its state.
  According to the logic of the protocol, $\var{ts}_r$ remains frozen
  and the corresponding fragments are retained at least until $c$ invokes
  a subsequent read operation.


  In Case~2.a, almost the same happens during \opwi{i+1}, when the
  writer detects the concurrent read.  The writer sets
  $\var{reservedptrlist}[r].\var{ts}$ to $\var{ts}_r$ in its state.
  Again according to the protocol, $\var{ts}_r$ remains reserved and
  the writer retains the corresponding fragments at least until $c$
  invokes a subsequent read.

Intuitively, Cases~1.a and 2.a demonstrate why $w$ retains two
  values during a write (the one being written and the one written
  before): $w$ does not know which one of the two the reader is about
  to access.

  In Case~2.b.i, if the writer detects the concurrent read during
  \opwi{i+2}, then it reserves and retains~$\var{ts}_r$ and the claim
  follows analogously to Case~2.a.

  In Cases~1.b and 2.b.ii, the reader accesses a frozen value.  Again,
  according to the protocol, $\var{ts}_r$ remains frozen and is retained
  at least until $c$ invokes a subsequent read operation.  The lemma follows.
\end{proof}
\fi

\if\submit\no
\begin{theorem}[Atomicity]\label{thm:atomic}
  Given a atomic snapshot object \dir, protocol \NAME emulates an
  atomic MRMW register~\var{r}.
\end{theorem}


\begin{proof}
  We show that every execution $\sigma$ of the protocol is
  linearizable with respect to an MRMW register.  By
  Lemma~\ref{lem:integr}, the timestamp of a \var{r}-\op{Read} either
  has been written by some \var{r}-\op{Write} operation or \var{r}-\opread
  returns~$\bot$.

  We first construct an execution~$\tau$ from $\sigma$ by completing
  all operations of type \var{r}-\op{Write} for those values~$v$ that
  have been returned by some \var{r}-\op{Read} operation. Then we
  obtain a sequential permutation~$\pi$ from $\tau$ as follows: (1)
  order all operations according to their timestamps; (2) among the
  operations with the same timestamp, place the \var{r}-\op{Read}
  operations immediately after the unique \var{r}-\op{Write} with this
  timestamp; and (3) arrange all non-concurrent operations in the same
  order as in~$\tau$.  Note that concurrent \var{r}-\op{Read}
  operations with the same timestamp may appear in arbitrary order.

  For proving that $\pi$ is a view of $\tau$ at a client~$c$ w.r.t.\ a
  register, we must show that every \var{r}-\op{Read} operation
  returns the value written by the latest preceding \var{r}-\op{Write}
  that appears before in $\pi$ or $\bot$ if there is no such
  operation.

  Let $o_r$ be an operation of type \var{r}-\opread with
  timestamp~$\var{ts}_r$ that returns a value~$v$. If $v = \bot$, then
  by construction $o_r$ is ordered before any write operation
  in~$\pi$.  Otherwise, it holds $v \neq \bot$ and according to
  Lemma~\ref{lem:integr}, there exists an \var{r}-\op{Write} operation
  $o_w$ that writes $v$ with the same timestamp.  In this case, $o_w$
  is placed in $\pi$ before $o_r$ by construction.  No other
  \var{r}-\op{Write} operation appears between $o_w$ and $o_r$ because
  all other write operations have a different timestamp and therefore
  appear in $\pi$ either before~$o_w$ or after~$o_r$.

  It remains to show that $\pi$ preserves the real-time order
  of~$\sigma$.  Consider two operations~\opr and \popr in $\tau$ with
  timestamps $\var{ts}_\opr$ and $\var{ts}_\popr$, respectively, such
  that \opr precedes~\popr.  From Lemma~\ref{lem:partorder}, we have
  $\tspopr \geq \tsopr$. If $\tspopr > \tsopr$ then \popr appears
  after \opr in $\pi$ by construction.  Otherwise $\tspopr = \tsopr$
  and \popr must be an operation of type \var{r}-\op{Read}.  If \opr
  is of type \var{r}-\op{Write}, then \popr appears after \opr since
  we placed each \var{r}-\op{Read} after the \var{r}-\op{Write} with
  the same timestamp.  Otherwise, \opr is a \var{r}-\op{Read} and the
  two \var{r}-\op{Read} operations appear in $\pi$ in the same order
  as in~$\tau$ by construction.
\end{proof}
\fi

\if 0

\begin{lemma}[Read progress]
  Consider an r-read operation by client~$c$ with timestamp $ts_r$ /
  pointer $ptr_r$; then at least distinct $t+k$ data \nodes that
  store a data fragment such that $ptr_r.hash$ etc. matches \dots\ and
  they do not ``free'' these before $c$ invokes its subsequent r-read
  op.
\end{lemma}

\begin{proof}
  adapt from below
\end{proof}


\begin{lemma}[Concurrent Read/Write (single writer)]
\label{lem:concwr}
Let $o_r$ be an operation of type \var{r}-\op{Read} with timestamp $\var{ts}_r$
invoked by a reader $c_r$, and $\opwi{1}, \ldots, \opwi{n_w}$ be operations
of type \var{r}-\op{Write} invoked by the only writer $c_w$,
and $\var{ts}_{w,1}, \ldots, \var{ts}_{w,n_w}$ their respective timestamps.
We further assume that some of the \var{r}-\op{Write} operations are
concurrent with $o_r$. Now, let \opwi{i} be the most recent
\var{r}-\op{Write} operation
whose \dir-\op{Update} (we call the latter \updatewi{i}) completes before the
\dir-\op{Update} of the $o_r$ (we call the latter \updaterd).
Then, $\var{ts}_r = \var{ts}_{w,i}$ or $\var{ts}_r = \var{ts}_{w,i+1}$. \\
\note{Corner cases:} (1) If there is no \updatewi{i} call that has completed
before \updaterd, then $\var{ts}_r = (0, \bot)$ or $\var{ts}_r = \var{ts}_{w, 1}$.
(2) If $i=n_w$, then $\var{ts}_r = \var{ts}_{w, n_w}$.\\
Furthermore, these two values are \emph{frozen}, i.e., excluded from garbage
collection by $c_w$ until $o_r${} **completes**.

\end{lemma}

\begin{proof}
We distinguish between the two cases:
\begin{itemize}
\item[(i)] \updaterd{} precedes \scanwi{i}:
Here, the writer $c_w$ detects the ongoing $o_r$, and updates its local
\var{frozenindex} variable accordingly. If \scanrd{} precedes
\updatewi{i+1}, then $\var{ts}_r = \var{ts}_{w,i}$. Otherwise, if
\scanrd{} is invoked at any point after \updatewi{i+1}, $o_r$
forces $\var{ts}_r = \var{ts}_{w,i}$; the latter is because from this point onwards
\var{readindex} within $o_r$ is equal to $M[c_w].\var{frozenindex}[c_r]$.
\item[(ii)] \updaterd{} does not precede \scanwi{i}:
In this case \updaterd{} is invoked between \scanwi{i} and \updatewi{i+1},
and the writer $c_w$ detects the ongoing $o_r$ via \scanwi{i+1}. As before,
if \scanrd is invoked before \updatewi{i+1}, $\var{ts}_r = \var{ts}_{w,i}$.
If \scanrd{} is invoked at any point after \updatewi{i+1}, then
$\var{ts}_r = \var{ts}_{w,i+1}$. In particular, if \scanrd  occurs before
\updatewi{i+2} (assuming that $i+2 \leq n_w$), \var{readindex} within
$o_r$ remains greater than $M[c_w].\var{frozenindex}[c_r]$, and thus reads
sets $\var{ts}_r = \var{ts}_{w,i+1}$. Otherwise, \var{readindex} when \scanrd occurs
equals to $M[c_w].\var{frozenindex}[c_r]$, and $o_r$ forces
$\var{ts}_r = \var{ts}_{w,i+1}$. Clearly, if $i+1 = n_w$, $o_r$ sets $\var{ts}_r = \var{ts}_{n_w}$.
\end{itemize}
\note{Corner cases}. It is easy to see, that if $i = n_w$, then $o_r$ sets
$\var{ts}_r = \var{ts}_{n_w}$. Similarly, if $i = 0$, i.e., \updaterd{} takes place before
\updatewi{1}, then if \scanrd is invoked before \updatewi{1}, $o_r$ sets
$ts = (0,\bot)$, while if \scanrd is invoked after \updatewi{1}, $o_r$ sets
$ts = \var{ts}_{w, 1}$.

In addition, it is easy to see that in both cases, $\var{ts}_r$ is frozen by
$c_w$. \note{E: Need to add more things here.}
\end{proof}


\begin{lemma}[Concurrent read/write (multiple writers)]\label{lem:concmwr}
Let $o_r$ be an operation of type \var{r}-\op{Read} with timestamp $\var{ts}_r$
invoked by a reader $c_r$, and $\opwi{1}, \ldots, \opwi{n_{w}}$ be
operations of type \var{r}-\op{Write} invoked by each
writer $c_w \in \clientset$, and $\var{ts}_{w,1}, \ldots, \var{ts}_{w,n_w}$ their
respective timestamps.
We further assume that some of the \var{r}-\op{Write} operations are
concurrent with $o_r$, while \var{r}-\op{Write} operations of different
clients can also be concurrent with each other and/or with $o_r$.
Now, assume that for each writer $c_w \in \clientset$,
$\opwi{i_w}$ denotes the the most recent \var{r}-\op{Write} operation of
$c_w$ whose \dir-\op{Update} (we call the latter $\updatewi{i_w}$) completes
before the \dir-\op{Update} of the $o_r$ (we call the latter \updaterd).
Then, \[
\var{ts}_r \in \bigcup\limits_{\forall c_w \in\clientset} \{\var{ts}_{w,i_w} \cup \var{ts}_{w,i_w+1}\}.\]
\end{lemma}

\begin{proof}
It derives directly from Lemma~\ref{lem:concwr}.\\
\note{E: Is there a simple way to do this?}\\
\end{proof}

\fi



\if\submit\no
\begin{theorem}[Wait-freedom]\label{thm:waitfree}
  Given an atomic snapshot object \dir and assuming that $n \geq
  2t+k$, protocol \NAME is wait-free.
\end{theorem}

\begin{proof}
  As the atomic snapshot \dir operates correctly, all its operations
  eventually complete independently of other processes. It remains to
  show that no \var{r}-\op{Write} and no \var{r}-\op{Read} operation
  blocks.

  For a \var{r}-\op{Write} operation, the client needs to receive $t +
  k$ \var{$d_i$}-\op{WriteAck} events from distinct data \nodes
  before completing.  As there are $n$ \nodes and up to $t$ may be
  faulty, the assumption $n \geq 2t + k$ implies this.

  During a \var{r}-\op{Read} operation, the reader needs to obtain $k$
  valid fragments, i.e., fragments that pass the verification of their
  hash value.  According to Lemma~\ref{lem:concurrent}, there are at
  least $k$ correct data \nodes designated by
  \var{readptr}.\var{set} that store a fragment under
  timestamp~$\var{ts}_r$ until the operation completes.  As the reader
  contacts these \nodes and waits for $k$ fragments, these fragments
  eventually arrive and can be reconstructed to the value written by
  the writer by the completeness of the erasure code.
\end{proof}
\fi





\if\submit\no
\section{Conclusion}

This paper has presented \NAME, the first \emph{erasure-coded}
distributed implementation of a multi-writer multi-reader read/write
storage object that is, at the same time: (1) asynchronous, (2)
wait-free, (3) atomic, (4) amnesic, (i.e., with data \nodes storing a
bounded number of values) and (5) Byzantine fault-tolerant (BFT) using
the optimal number of \nodes. \NAME is efficient since it does not use
public-key cryptography and requires data \nodes that support only
reads and writes, further reducing the cost of deployment and
ownership of a distributed storage solution. Notably, \NAME stores
metadata separately from $k$-out-of-$n$ erasure-coded fragments. This
enables \NAME to be the first BFT protocol that uses as few as $2t+k$
data \nodes to tolerate $t$ Byzantine \nodes, for any $k\ge 1$.

Future work should address how to optimize protocol \NAME and to
reduce the storage consumption for practical systems; this could be
done at the cost of increasing its conceptual complexity and losing
some of its ideal properties.  For instance, when the metadata service
is moved from a storage abstraction to a service with processing, it
is conceivable that fewer values have to be retained at the \nodes.


\section*{Acknowledgment}

We thank Alessandro Sorniotti, Nikola Kne\v{z}evi\'{c}, and Radu
Banabic for inspiring discussions during the early stages of this
work. This work is supported in part by the EU CLOUDSPACES (FP7-317555)
and SECCRIT (FP7-312758) projects.

\fi




\begin{thebibliography}{10}

\bibitem{ackm06}
I.~Abraham, G.~Chockler, I.~Keidar, and D.~Malkhi.
\newblock Byzantine disk {Paxos}: Optimal resilience with {Byzantine} shared
  memory.
\newblock {\em Distributed Computing}, 18(5):387--408, 2006.

\bibitem{abcccd02}
A.~Adya, W.~J. Bolosky, M.~Castro, G.~Cermak, R.~Chaiken, J.~R. Douceur,
  J.~Howell, J.~R. Lorch, M.~Theimer, and R.~P. Wattenhofer.
\newblock {FARSITE}: Federated, available, and reliable storage for an
  incompletely trusted environment.
\newblock In {\em Proc.\ 5th Symp.\ Operating Systems Design and Implementation
  (OSDI)}, 2002.

\bibitem{aadgms93}
Y.~Afek, H.~Attiya, D.~Dolev, E.~Gafni, M.~Merritt, and N.~Shavit.
\newblock Atomic snapshots of shared memory.
\newblock {\em Journal of the ACM}, 40(4):873--890, 1993.

\bibitem{bcqas11}
A.~Bessani, M.~Correia, B.~Quaresma, F.~Andr\'{e}, and P.~Sousa.
\newblock {DepSky}: Dependable and secure storage in a cloud-of-clouds.
\newblock In {\em Proc.\ 6th European Conference on Computer Systems
  (EuroSys)}, pages 31--46, 2011.

\bibitem{bojuop09}
K.~D. Bowers, A.~Juels, and A.~Oprea.
\newblock {HAIL}: A high-availability and integrity layer for cloud storage.
\newblock In {\em Proc.\ 16th ACM Conference on Computer and Communications
  Security (CCS)}, pages 187--198, 2009.

\bibitem{cadovu13}
C.~Cachin, D.~Dobre, and M.~Vukoli{\'c}.
\newblock {BFT} storage with $2t+1$ data replicas.
\newblock Report arXiv:1305.4868, CoRR, 2013.

\bibitem{CachinGR11}
C.~Cachin, R.~Guerraoui, and L.~Rodrigues.
\newblock {\em Introduction to Reliable and Secure Distributed Programming
  ({Second Edition})}.
\newblock Springer, 2011.

\bibitem{cajuso12}
C.~Cachin, B.~Junker, and A.~Sorniotti.
\newblock On limitations of using cloud storage for data replication.
\newblock Proc.\ WRAITS, 2012.

\bibitem{cactes06}
C.~Cachin and S.~Tessaro.
\newblock Optimal resilience for erasure-coded {Byzantine} distributed storage.
\newblock In {\em Proc.\ International Conference on Dependable Systems and
  Networks (DSN-DCCS)}, pages 115--124, 2006.

\bibitem{clmm13}
V.~R. Cadambe, N.~Lynch, M.~Medard, and P.~Musial.
\newblock Coded atomic shared memory emulation for message passing
  architectures.
\newblock CSAIL Technical Report MIT-CSAIL-TR-2013-016, MIT, 2013.

\bibitem{chguke07}
G.~Chockler, R.~Guerraoui, and I.~Keidar.
\newblock Amnesic distributed storage.
\newblock In G.~Taubenfeld, editor, {\em Proc.\ 21th International Conference
  on Distributed Computing (DISC)}, volume 4731 of {\em Lecture Notes in
  Computer Science}, pages 139--151. Springer, 2007.

\bibitem{dklmsv13}
D.~Dobre, G.~Karame, W.~Li, M.~Majuntke, N.~Suri, and M.~Vukoli\'{c}.
\newblock {PoWerStore}: Proofs of writing for efficient and robust storage.
\newblock In {\em Proc.\ ACM Conference on Computer and Communications Security
  (CCS)}, 2013.

\bibitem{domasu08}
D.~Dobre, M.~Majuntke, and N.~Suri.
\newblock On the time-complexity of robust and amnesic storage.
\newblock In T.~P. Baker, A.~Bui, and S.~Tixeuil, editors, {\em Proc.\ 12th
  Conference on Principles of Distributed Systems (OPODIS)}, volume 5401 of
  {\em Lecture Notes in Computer Science}, pages 197--216. Springer, 2008.

\bibitem{dugule08}
P.~Dutta, R.~Guerraoui, and R.~R. Levy.
\newblock Optimistic erasure-coded distributed storage.
\newblock In G.~Taubenfeld, editor, {\em Proc.\ 22th International Conference
  on Distributed Computing (DISC)}, volume 5218 of {\em Lecture Notes in
  Computer Science}, pages 182--196. Springer, 2008.

\bibitem{fmssv04}
S.~{Fr\o{}lund}, A.~Merchant, Y.~Saito, S.~Spence, and A.~Veitch.
\newblock A decentralized algorithm for erasure-coded virtual disks.
\newblock In {\em Proc.\ International Conference on Dependable Systems and
  Networks (DSN-DCCS)}, pages 125--134, 2004.

\bibitem{gwgr04}
G.~R. Goodson, J.~J. Wylie, G.~R. Ganger, and M.~K. Reiter.
\newblock Efficient {Byzantine}-tolerant erasure-coded storage.
\newblock In {\em Proc.\ International Conference on Dependable Systems and
  Networks (DSN-DCCS)}, pages 135--144, 2004.

\bibitem{gulevu06}
R.~Guerraoui, R.~R. Levy, and M.~Vukoli\'{c}.
\newblock Lucky read/write access to robust atomic storage.
\newblock In {\em Proc.\ International Conference on Dependable Systems and
  Networks (DSN-DCCS)}, pages 125--136, 2006.

\bibitem{hendri09}
J.~Hendricks.
\newblock {\em Efficient Byzantine Fault Tolerance for Scalable Storage and
  Services}.
\newblock PhD thesis, School of Computer Science, Carnegie Mellon University,
  July 2009.

\bibitem{hegare07b}
J.~Hendricks, G.~R. Ganger, and M.~K. Reiter.
\newblock Low-overhead {Byzantine} fault-tolerant storage.
\newblock In {\em Proc.\ 21st ACM Symposium on Operating Systems Principles
  (SOSP)}, 2007.

\bibitem{herlih91}
M.~Herlihy.
\newblock Wait-free synchronization.
\newblock {\em ACM Transactions on Programming Languages and Systems},
  11(1):124--149, Jan. 1991.

\bibitem{herwin90}
M.~P. Herlihy and J.~M. Wing.
\newblock Linearizability: A correctness condition for concurrent objects.
\newblock {\em ACM Transactions on Programming Languages and Systems},
  12(3):463--492, July 1990.

\bibitem{hsxocg12}
C.~Huang, H.~Simitci, Y.~Xu, A.~Ogus, B.~Calder, P.~Gopalan, et~al.
\newblock Erasure coding in {Windows} {Azure} {Storage}.
\newblock In {\em Proc.\ {USENIX} Annual Technical Conference}, 2012.

\bibitem{Lynch96}
N.~A. Lynch.
\newblock {\em Distributed Algorithms}.
\newblock Morgan Kaufmann, San Francisco, 1996.

\bibitem{maalda02}
J.-P. Martin, L.~Alvisi, and M.~Dahlin.
\newblock Minimal {Byzantine} storage.
\newblock In D.~Malkhi, editor, {\em Proc.\ 16th International Conference on
  Distributed Computing (DISC)}, volume 2508 of {\em Lecture Notes in Computer
  Science}, pages 311--325. Springer, 2002.

\bibitem{plank05}
J.~S. Plank.
\newblock Erasure codes for storage applications.
\newblock Tutorial, presented at the Usenix Conference on File and Storage
  Technologies (FAST), 2005.

\bibitem{rabin89}
M.~O. Rabin.
\newblock Efficient dispersal of information for security, load balancing, and
  fault tolerance.
\newblock {\em Journal of the ACM}, 36(2):335--348, 1989.

\bibitem{cleversafect13}
W.~Wong.
\newblock Cleversafe grows along with customers' data storage needs.
\newblock Chicago Tribune, Nov. 2013.

\bibitem{ymvad03}
J.~Yin, J.-P. Martin, A.~V.~L. Alvisi, and M.~Dahlin.
\newblock Separating agreement from execution in {Byzantine} fault-tolerant
  services.
\newblock In {\em Proc.\ 19th ACM Symposium on Operating Systems Principles
  (SOSP)}, pages 253--268, 2003.

\end{thebibliography}



\if\submit\yes
\newpage
\appendix
\section{\NAME Correctness}
\label{app}
\input{appendix}
\fi



\end{document}
