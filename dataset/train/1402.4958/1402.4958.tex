\def\submit{n} \def\yes{y}
\def\no{n}

\if\submit\yes
\documentclass[oribibl]{llncs}
\else
\documentclass[a4paper,11pt]{article}
\usepackage{fullpage}
\fi
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{times}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}


\usepackage[nodayofweek]{datetime}
\newdateformat{simple}{\THEDAY\ \monthname[\THEMONTH]\ \THEYEAR}
\simple

\DeclareMathOperator*{\argmax}{arg\,max}
\allowdisplaybreaks[2]

\if\submit\no
\usepackage{amsthm} \newtheoremstyle{plain-boldhead}{\topsep}{\topsep}{\itshape}{}{\bfseries}{.}{ }{\thmname{#1}\thmnumber{ #2}\thmnote{ (\bfseries #3)}}\newtheoremstyle{definition-boldhead}{\topsep}{\topsep}{\normalfont}{}{\bfseries}{.}{ }{\thmname{#1}\thmnumber{ #2}\thmnote{ (\bfseries #3)}}\theoremstyle{plain-boldhead}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}[theorem]
\theoremstyle{definition-boldhead}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\fi


\usepackage{floatmodif}
\floatstyle{ruled}

\newfloat{alg}{htbp}{alg}
\floatname{alg}{Algorithm}

\newfloat{module}{htbp}{module}
\floatname{module}{Module}




\newcommand{\str}[1]{\textsc{#1}}
\newcommand{\var}[1]{\textit{#1}}
\newcommand{\op}[1]{\textsl{#1}}
\newcommand{\msg}[2]{\ensuremath{[]}}
\newcommand{\msgnp}[1]{\str{#1}}
\newcommand{\newe}{{\bf invoke}\xspace}
\newcommand{\event}[2]{荟狎１荇屮趔禧２
\newcommand{\eventt}[3]{荟狎１荇屮趔禧２３
\newcommand{\instance}[1]{\textbf{instance}\var{#1}}
\newcommand{\becomes}{\ensuremath{\leftarrow}}
\newcommand{\false}{\textsc{false}}
\newcommand{\true}{\textsc{true}}

\newcommand{\replicaset}{\ensuremath{\mathcal{D}}\xspace}
\newcommand{\clientset}{\ensuremath{\mathcal{C}}\xspace}
\newcommand{\values}{\ensuremath{\mathcal{V}}\xspace}
\newcommand{\fragments}{\ensuremath{\mathcal{F}}\xspace}
\newcommand{\valv}{\ensuremath{\var{v}}\xspace}
\newcommand{\strings}{\ensuremath{\Sigma^{*}}}
\newcommand{\dir}{\var{dir}\xspace}

\newcommand{\token}[1]{\textsc{#1}}
\newcommand{\fixme}{\textbf{FIXME}}

\newcommand{\NAME}{AWE\xspace}
\newcommand{\node}{node\xspace}
\newcommand{\nodes}{nodes\xspace}

\hyphenation{time-stamp}
\hyphenation{time-stamps}

\providecommand{\note}[1]{}
\renewcommand{\note}[1]{[[\textsf{\bf #1}]]}


\begin{document}

\title{\bf Erasure-Coded Byzantine Storage with Separate Metadata}


\if\submit\no

\author{Elli Androulaki\footnotemark[1]
  \and Christian Cachin\footnotemark[1]
  \and Dan Dobre\footnotemark[2]
  \and Marko Vukoli\'c\footnotemark[3]
  }

\date{\today}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{IBM Research - Zurich,
    R\"uschlikon, Switzerland, \texttt{\{lli,cca\}@zurich.ibm.com}.}
\footnotetext[2]{NEC Labs Europe, Germany, \texttt{dan.dobre@neclab.eu}.}
\footnotetext[3]{Eur{\'e}com, Sophia Antipolis, France,
  \texttt{vukolic@eurecom.fr}.}
\renewcommand{\thefootnote}{\arabic{footnote}}


\else


\author{Elli Androulaki\inst{1},
  Christian Cachin\inst{1},
  Dan Dobre\inst{2},
  Marko Vukoli\'c\inst{3}}

\institute{IBM Research - Zurich, \email{\{lli,cca\}@zurich.ibm.com} \and
  NEC Labs Europe, Germany, \email{dan.dobre@neclab.eu} \and
  Eur{\'e}com, \email{vukolic@eurecom.fr}}

\fi


\maketitle

\pagestyle{plain}
\thispagestyle{plain}

\begin{abstract}\noindent
  Although many distributed storage protocols have been introduced, a
  solution that combines the strongest properties in terms of
  availability, consistency, fault-tolerance, storage complexity and
  the supported level of concurrency, has been elusive for a long
  time. Combining these properties is difficult, especially if the
  resulting solution is required to be efficient and incur low cost.

  We present \NAME, the first \emph{erasure-coded} distributed
  implementation of a multi-writer multi-reader read/write storage
  object that is, at the same time: (1) asynchronous, (2) wait-free,
  (3) atomic, (4) amnesic, (i.e., with data \nodes storing a bounded
  number of values) and (5) Byzantine fault-tolerant (BFT) using the
  optimal number of \nodes. Furthermore, \NAME is efficient since it
  does not use public-key cryptography and requires data \nodes that
  support only reads and writes, further reducing the cost of
  deployment and ownership of a distributed storage solution. Notably,
  \NAME stores metadata separately from -out-of- erasure-coded
  fragments. This enables \NAME to be the first BFT protocol that uses
  as few as  data \nodes to tolerate  Byzantine \nodes, for
  any .
\end{abstract}


\section{Introduction}

\paragraph{Background.}
\emph{Erasure coding} is a key technology that saves space and retains
robustness against faults in distributed storage systems. In short, an
erasure code splits a large data object into  fragments such that
from any  of them the input value can be reconstructed. The utility
of erasure coding is demonstrated by large-scale erasure-coding
storage systems that have been deployed
today\cite{hsxocg12,cleversafect13}. These distributed storage
systems offer large capacity, high throughput, and resilience to
faults.

Whereas the storage systems in production use today only tolerate
component crashes or outages, storage systems in the \emph{Byzantine
  failure model} survive also more severe faults, ranging from
arbitrary state corruption to malicious attacks on components. In this
paper, we consider a model where \emph{clients} directly access a
storage service provided by distributed servers, called \emph{\nodes}
--- a fraction of the \nodes may be Byzantine, whereas clients may
fail as well, but only by crashing.

Although Byzantine-fault tolerant (BFT) erasure-coded distributed
storage systems have received some attention in the
literature\cite{gwgr04,cactes06,hegare07b,dugule08,bcqas11}, our
understanding of their properties lies behind that of replicated
storage. In fact, most existing BFT erasure-coded storage approaches have drawbacks that
prevented their wide-spread use.  For example, they relied on the
\nodes storing an unbounded number of values\cite{gwgr04}, required
the \nodes to communicate with each other\cite{cactes06}, used
public-key cryptography\cite{cactes06,hegare07b}, or might have
blocked clients due to concurrent operations of other
clients\cite{hegare07b}.

We consider an abstract \emph{wait-free} storage register with
\emph{atomic} semantics\cite{herwin90}, accessed concurrently by
multiple readers and writers (MRMW).  Wait-free termination means that
any client operation terminates irrespective of the behavior of the
Byzantine \nodes and of other clients.  This is not easy to achieve
with Byzantine \nodes\cite{ackm06} even in systems that replicate
the data.  Therefore, previous works have often used a weaker notion
of liveness called \emph{finite-write (FW) termination}, which ensures
that read operations progress only in executions with a finite number
of writes.

\paragraph{Contribution.}
This paper introduces \NAME, the \emph{first} asynchronous, wait-free
distributed BFT erasure-coded storage protocol with optimal
resilience.   As in
previous work, we assume there are  \nodes and up to  of them
may exhibit non-responsive (NR-)arbitrary faults, that is, Byzantine
corruptions.  The best resilience that has been achieved so far is , which is optimal for Byzantine storage\cite{maalda02}.
However, our protocol features a separation of metadata and erasure coded fragments; with this approach our protocol may reduce the
number of \emph{data \nodes}, i.e., those that store a fragment, to
lower values than  for .  In particular, our protocol takes only
 data \nodes; this idea saves resources, as in the separation
of agreement and execution for BFT services\cite{ymvad03}.  For
implementing the metadata service,  \nodes are still
needed.

Our protocol
employs simple, passive data \nodes; they cannot execute code and they
only support read and write operations, such as the key-value stores
(KVS) provided by popular cloud storage services.  The metadata
service itself is an atomic snapshot object, which has only weak
semantics and may be implemented in a replicated asynchronous system
from simple read/write registers\cite{aadgms93}.  The protocol is
also \emph{amnesic}\cite{chguke07}, i.e., the \nodes store a
bounded number of values and may erase obsolete data.
The protocol uses only simple cryptographic
hash functions but no (expensive) public-key operations.

In summary, protocol \NAME, introduced in Section\ref{sec:protocol}, is the first
erasure-coded distributed implementation of a MRMW storage object that
is, at the same time: (1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (5) tolerates the optimal number of Byzantine \nodes, and (6) does not use public-key
cryptography.  Furthermore, \NAME can be
implemented from non-programmable \nodes (KVS) that only support reads and
writes (in the vein of Disk Paxos\cite{ackm06}). In practice, the KVS interface is offered by commodity cloud storage services, which could be used as \NAME data \nodes
to reduce the cost of \NAME deployment and ownership.
While some of these desirable properties have been achieved in different combinations so far, they have never been achieved together with erasure-coded storage, as explained later.  Combining these properties has been a longstanding open problem\cite{gwgr04}.




\paragraph{Related work.}

We provide a brief overview of the most relevant literature on the
subject.  Table\ref{tab:comparison} summarizes this section.

Earlier designs for erasure-coded distributed storage have suffered
from potential aborts due to contention\cite{fmssv04} or from
the need to maintain an unbounded number of fragments at data
\nodes\cite{gwgr04}.
In the crash-failure model, ORCAS\cite{dugule08} and
CASGC\cite{clmm13} achieve optimal resilience  and low
communication overhead, combined with wait-free (ORCAS)
and FW-termination (CASGC), respectively.

In the model with Byzantine \nodes, Cachin and Tessaro
(CT)\cite{cactes06} introduced the first wait-free protocol with
atomic semantics and optimal resilience .  CT uses a
verifiable information dispersal protocol but needs \node-to-\node
communication, which lies outside our model. Hendricks et al.(HGR)\cite{hegare07b} present an optimally resilient
protocol that comes closest to our protocol among the existing
solutions.  It offers many desirable features, that is, it has as
low communication cost, works asynchronously, achieves optimal
resilience, atomicity, and is amnesic.  Compared to our work, it uses
public-key cryptography, achieves only FW-termination instead of
wait-freedom, and requires \emph{processing} by the \nodes, i.e., the
ability to execute complex operations beyond simple reads and writes.

To be fair, much of the (cryptographic) overhead inherent in the CT
and HGR protocols defends against poisonous writes from Byzantine
clients, i.e., malicious client behavior that leaves the \nodes in
an inconsistent state.  We do not consider Byzantine clients in this
work, since permitting arbitrary client behavior is problematic.  Such
a client might write garbage to the storage system at any time and
wipe out the stored value.
\if\submit\no
Furthermore, the standard formal
correctness notions such as linearizability fail when clients
misbehave (apart from crashing).  Hendricks\cite{hendri09} discusses
correctness notions in the presence of Byzantine clients.
\fi
However,
even without the steps that protect against poisonous writes, HGR still
requires processing by the \nodes and is not wait-free.

The M-PoWerStore protocol\cite{dklmsv13} employs a cryptographic
``proof of writing'' for wait-free atomic erasure-coded distributed
storage.  It is the first wait-free BFT solution without
\node-to-\node communication.  Similar to other protocols,
M-PoWerStore uses \nodes with processing capabilities and is not
amnesic.

Several systems have recently addressed how to store erasure-coded
data on multiple redundant cloud services but only few of them focus
on wait-free concurrent access.  HAIL\cite{bojuop09}, for instance,
uses Byzantine-tolerant erasure coding and provides data integrity
through proofs of retrievability; however, it does not address concurrent operations by different clients.  DepSky\cite{bcqas11}
achieves regular semantics and uses lock-based concurrency control;
therefore, one client may block operations of other clients.

A key aspect of \NAME lies in the differentiation of (small) metadata
from (large) bulk data: this enables a modular protocol design and an
architectural separation for implementations.
The FARSITE system\cite{abcccd02} first introduced such a separation
for replicated storage; their data \nodes and their metadata
abstractions require processing, however, in contrast to\NAME. Non-explicit ways of separating metadata from data can already be
found in several previous erasure coding-based protocols.  For
instance, the cross checksum, a vector with the hashes of all 
fragments, has been replicated on the data nodes to ensure
consistency\cite{gwgr04,cactes06}.

Finally, a recent protocol called MDStore\cite{cadovu13} has shown
that separating metadata from bulk data permits to reduce the number
of data \nodes in asynchronous wait-free BFT distributed storage
implementations to only.  When protocol\NAME is reduced to use
replication with the trivial erasure code (), it uses as few
\nodes as MDStore to achieve the same wait-free atomic semantics;
unlike \NAME, however, MDStore is not amnesic and uses processing
\nodes.


\newcommand{\Y}{\ensuremath{\checkmark}}
\newcommand{\T}{\ensuremath{^*}}

\begin{table}
  \centering
  \begin{tabular}{|l||c|c|c|c|c|c|}
    \hline
    {\bf Protocol} & {\bf BFT} & {\bf Liveness} & {\bf Data \nodes} &
      {\bf Type} & {\bf Amnesic} & {\bf Cryptogr.} \\ \hline
    ORCAS\cite{dugule08} & --- & Wait-free &  & Proc. &
      ---  & N/A \\ \hline
    CASGC\cite{clmm13}   & --- & FW-term.  &  & Proc. &
      \Y \T   & N/A \\ \hline
    CT\cite{cactes06}    & \Y \T & Wait-free \T &  & Msg. &
      ---  & Public-key \\ \hline
    HGR\cite{hegare07b}  & \Y \T & FW-term.  & , for  & Proc. &
      \Y \T & Public-key \\ \hline
    M-PoWerStore\cite{dklmsv13}  & \Y \T & Wait-free \T &  & Proc. &
      ---  & Hash func. \T \\ \hline
    DepSky\cite{bcqas11} & \Y \T & Obstr.-free  &  & R/W \T &
      ---  & Public-key \\ \hline
    \NAME (Sec.\ref{sec:protocol})
                          & \Y \T & Wait-free \T & , for  \T &
      R/W \T & \Y \T & Hash func. \T \\ \hline
  \end{tabular}
  \caption{Comparison of erasure-coded distributed storage solutions.
    An asterisk (\T) denotes optimal properties. The column labeled
    \emph{Type} states the computation requirements on \nodes:
    \emph{Proc.}\ denotes processing;
\emph{Msg.}\ means sending messages to other
    \nodes, in addition to processing;
    \emph{R/W} means a register object supporting only read and write.}
  \label{tab:comparison}
\if\submit\yes
\vspace*{-4mm}
\fi
\end{table}


\paragraph{Structure.}

The paper continues with the model in Section\ref{sec:def} and
presents Protocol\NAME in Section\ref{sec:protocol}.  The
communication and storage complexities of \NAME are compared
to those of existing protocols in Section\ref{sec:complexity}.
\if\submit\no
Section\ref{sec:proof} contains a formal proof for the properties
of\NAME.
\else
For lack of space, proof of \NAME correctness is postponed to Appendix\ref{app}.
\fi


\section{Definitions}
\label{sec:def}

\paragraph{System model.}
We consider an asynchronous distributed system of components (or
processes) that communicate with each other.  The components contain a
set \clientset of  \emph{clients}, a set  of 
\emph{data \nodes} , and further process
abstractions.  The components interact asynchronously via exchanging
events.  A protocol specifies a collection of programs with
instructions for all components.


A component may fail by crashing or by exhibiting \emph{Byzantine}
faults; the latter means they may deviate arbitrarily from their
specification.  We assume that clients can only crash;
on the other hand, up to data \nodes can
be Byzantine and behave adversarially (NR-arbitrary faults).  A
component that does not fail is called \emph{correct}.





\paragraph{Notation.}
Protocols are presented in a modular way using an event-based
notation\cite{CachinGR11}.  A component is specified through its
\emph{interface}, containing the events that it exposes to other
components that may call it, and its \emph{properties}, which define
its behavior.  A component may react to a received event by doing
computation and triggering further events.
Every component is named by an identifier.  Events are qualified by
the component identifier to which the event belongs and may take
parameters.  An event \op{Sample} of a component\var{m} with a
parameter is denoted by \eventt{m}{Sample}{}.

\if\submit\no
Components interact asynchronously with others through exchanging
events.  We assume that all events communicated from one component to
another are delivered in FIFO-order.
There are two kinds of events in a component's interface: \emph{input
  events} that it receives from other components, typically to invoke
its services, and \emph{output events}, through which the component
delivers information or signals a condition to another component.  The
behavior of a component is typically stated through a number of
properties or through a sequential implementation.
\fi

\paragraph{Objects and histories.}
An \emph{object} is a special type of component for which every input
event (called an \emph{invocation} in this context) triggers exactly
one output event (called a \emph{response}).  Every such pair of
invocation and response define an \emph{operation} of the object.  An
operation \emph{completes} when its response occurs.

A \emph{history} of an execution of an object consists of
the sequence of invocations and responses of  occurring in
.  An operation is called \emph{complete} in a history if it
has a matching response.
An operation \emph{precedes} another operation in a sequence
of events, denoted , whenever  completes
before  is invoked in . If  precedes  then 
\emph{follows}.  A sequence of events \emph{preserves the
  real-time order} of a history if for every two operations
 and  in , if  then .  Two
operations are \emph{concurrent} if neither one of them precedes the
other.  A sequence of events is \emph{sequential} if it does not
contain concurrent operations.
We often simplify the terminology by exploiting that every
\emph{sequential} sequence of events corresponds naturally to a
sequence of operations.


An execution is \emph{well-formed} if the events at every object are
alternating invocations and matching responses, starting with an
invocation.  An execution is \emph{fair}, informally, if it does not
halt prematurely when there are still steps to be taken or triggered
events to be consumed (see the standard literature for a formal
definition\cite{Lynch96}).


\paragraph{Registers.}
A \emph{read/write register}\var{r} is an object that stores a value from
a domain \values and supports exactly two operations, for writing and
reading the value.  More precisely:
\if\submit\yes
(i) a \emph{Write} operation to\var{r} is triggered by an invocation \eventt{r}{Write}{} that takes a value  as parameter and terminates by generating a response \event{r}{WriteAck} with no parameter; and (ii) a \emph{Read} operation from\var{r} is triggered by an invocation \event{r}{Read} with no parameter; the register signals that the read operation completes by triggering a response \eventt{r}{ReadResp}{}, which contains a parameter .
\else
\begin{itemize}
\item A \emph{Write} operation to\var{r} is triggered by an
  invocation \eventt{r}{Write}{} that takes a value 
  as parameter and terminates by generating a response
  \event{r}{WriteAck} with no parameter.
\item A \emph{Read} operation from\var{r} is triggered by an invocation
  \event{r}{Read} with no parameter; the register signals that the
  read operation completes by triggering a response
  \eventt{r}{ReadResp}{}, which contains a parameter .
\end{itemize}
\fi
The behavior of a register is given through its sequential
specification, which requires that every \var{r}-\op{Read} operation
returns the value written by the last preceding \var{r}-\op{Write}
operation in the execution, or the special symbol  if no such operation exists. For simplicity, we will assume
that every distinct value is written only once.

In this work, any client may invoke the operations of the emulated
register object; such registers are also called \emph{multi-reader
  multi-writer (MRMW) registers}.  Furthermore, we assume that all
clients invoke a well-formed sequence of operations.


\paragraph{Consistency and availability.}
Recall that clients interact with an object through its
operations, defined in terms of an invocation and a response event
of.  We say that a client \emph{executes} an operation between
the corresponding invocation and response events.  When accessed
concurrently by multiple processes, executions of objects considered
in this work are \emph{linearizable}, that is, the object appears to
execute all operations \emph{atomically}.

\if\submit\yes
 More formally, a sequence of events is called a \emph{view} of a history  at a client w.r.t. an object if: (i)  is a sequential permutation of some subsequence of complete operations in ; (ii) all complete operations executed by  appear in ; and (iii)  satisfies the sequential specification of . A history  is \emph{linearizable}\cite{herwin90} w.r.t. an object if
  there exists a sequence of events  such that: (i)  is a view of  at all clients w.r.t. ; and (ii)  preserves the real-time order of . If every history of a protocol is  linearizable, the protocol is itself called linearizable, or \emph{atomic}. Finally, a protocol is called \emph{wait-free}\cite{herlih91} if every operation invoked by a correct client eventually completes.
\else

\begin{definition}[View]
  A sequence of events is called a \emph{view} of a history
   at a client w.r.t. an object whenever:
\begin{enumerate}
\item  is a sequential permutation of some subsequence
of complete operations in ;
\item all complete operations executed by  appear in ; and
\item  satisfies the sequential specification of.
\end{enumerate}
\end{definition}

\begin{definition}[Linearizability\cite{herwin90}]
  A history  is linearizable w.r.t. an object if
  there exists a sequence of events  such that:
\begin{enumerate}
\item  is a view of  at all clients w.r.t. ; and
\item  preserves the real-time order of .
\end{enumerate}
\end{definition}

The goal of this work is to describe a protocol that emulates a
linearizable register abstraction among the clients; such a register
is also called \emph{atomic}.  Some of the clients may crash and some
\nodes may be Byzantine, but every client operation should terminate
in all cases, irrespective of how other clients and \nodes behave.

\begin{definition}[Wait-freedom\cite{herlih91}]
  A protocol is called \emph{wait-free} if every operation invoked by
  a correct client eventually completes.
\end{definition}
\fi


\paragraph{Cryptography.}
We make use of cryptographic hash functions.
One can imagine that the cryptographic schemes are implemented by a
distributed oracle accessible to all components\cite{CachinGR11}.  A
hash function  maps a bit string of arbitrary length to a
short, unique representation of fixed length.  We use a
\emph{collision-free} hash function; this property means that no
process, not even a Byzantine component, can find two distinct values
 and  such that .


\section{Protocol \NAME}
\label{sec:protocol}

\if\submit\no
This section introduces the \emph{asynchronous wait-free erasure-coded
Byzantine distributed storage protocol (AWE)}.



\subsection{Abstractions}
\fi

\if\submit\yes
We first overview the key components used in protocol \NAME.
\fi

\paragraph{Erasure code.}
An \emph{-erasure code (EC)} with domain \values is given by
an encoding algorithm, denoted \op{Encode}, and a reconstruction
algorithm, called\op{Reconstruct}.
Given a (large) value , algorithm
 produces a vector  of 
\emph{fragments}, which are from a domain\fragments.  A fragment is
typically much smaller than the input, and any  fragments contain
all information of , that is, .

For an -vector , whose
entries are either fragments or the symbol, algorithm
 outputs a value or.
An output value of means that the reconstruction failed.  The
\emph{completeness} property of an erasure code requires that an encoded value
can be reconstructed from any  fragments.
In other words, for every , when one computes  and then erases up to  entries in
 by setting them to, algorithm 
outputs.
\if\submit\no
More details are available in the
literature\cite{rabin89,plank05}.
\fi


\paragraph{Metadata service.}
The metadata service is implemented by a standard \emph{atomic
  snapshot object}\cite{aadgms93}, called \dir, that serves as a
\emph{directory}.  A snapshot object extends the simple storage
function of a register to a service that maintains one value for each
client and allows for better coordination.  Like an array of
multi-reader single-writer (MRSW) registers, it allows every client to
\emph{update} its value individually; for reading it supports a
\emph{scan} operation that returns the vector of the stored values,
one for every client.  More precisely, the operations of\dir are:
\begin{itemize}
\item An \emph{Update} operation to\dir is triggered by an invocation
  \eventt{dir}{Update}{} by client that takes a value  as parameter and terminates by generating a response
  \event{r}{UpdateAck} with no parameter.
\item A \emph{Scan} operation on\dir is triggered by an invocation
  \event{dir}{Scan} with no parameter; the snapshot object returns a
  vector of  values to as the parameter in
  the response \eventt{r}{ScanResp}{}, with  for
  .
\end{itemize}
The sequential specification of the snapshot object follows directly
from the specification of an array of MRSW registers
(hence, the snapshot initially stores the special symbol  in every entry).  When accessed concurrently from multiple
clients, its operations appear to take place atomically, i.e., they
are linearizable.  Snapshot objects are weak--- they can be
implemented from read/write registers\cite{aadgms93}, which, in turn,
can be implemented from a set of a distributed processes subject to
Byzantine faults.  Wait-free amnesic implementations of registers with
the optimal number of  processes are possible using existing
constructions\cite{gulevu06,domasu08}.


\if\submit\yes
\paragraph{Data \nodes.}

Data \nodes in \NAME export a subset of key-value store API. We model the state of data nodes as an array , initially , for .  Here, as
in standard implementations of multi-writer distributed
storage\cite{CachinGR11}, every value is associated to a timestamp,
which consists of a sequence number\var{sn} and the identifier of the writing client, i.e., ; timestamps
are initialized to .

Data \node  exports three operations:
\begin{itemize}
\item \eventt{}{Write}{}, which stores  and returns \eventt{}{WriteAck}{};
\item  \eventt{}{Read}{}, which returns  \eventt{}{ReadResp}{}; and
\item  \eventt{}{Free}{}, which stores  for all , and returns \eventt{}{FreeAck}{}.
\end{itemize}

\fi

\subsection{Protocol overview}

\if\submit\no
The high-level architecture of \NAME uses the metadata directory\dir
to maintain pointers to the fragments stored at the data \nodes.
As in standard implementations of multi-writer distributed
storage\cite{CachinGR11}, every value is associated to a timestamp,
which consists of a sequence number\var{sn} and the identifier of
the writing client, i.e., ; timestamps
are initialized to .  The metadata contains the
timestamp of the most recently written value for every client, and
readers determine the value to read by retrieving all timestamps,
determining their maximum, and accessing the fragments associated to
the highest timestamp.  Comparisons among timestamps use the standard
ordering, where  for  and  if and only if
.
\fi

\if\submit\yes
\NAME uses the metadata directory\dir
to maintain pointers to the fragments stored at the data \nodes.
\fi
The directory stores an entry for every writer; it contains the
timestamp of its most recently written value, the identities of those
\nodes that have acknowledged to store a fragment of it, a vector
with the hashes of the fragments for ensuring data integrity, and
additional metadata to support concurrent reads and writes.  The linearizable
semantics of protocol\NAME are obtained from the atomicity of the
metadata directory.

At a high level, the writer first invokes \dir-\op{Scan} on the
metadata to read the highest stored timestamp, increments it, and uses
this as the timestamp of the value to be written.  Then it encodes the
value to  fragments and sends one fragment to each data \node.
The data \nodes store it and acknowledge the write.  After the
writer has received acknowledgments from  data \nodes, it
writes their identities (together with the timestamp and the hashes of
the fragments) to the metadata through \dir-\op{Update}.  The reader
proceeds accordingly: it first invokes \dir-\op{Scan} to obtain the
entries of all writers; it determines the highest timestamp among them
and extracts the fragment hashes and the identities of the data
\nodes; finally, it contacts the data \nodes and reconstructs the
value after obtaining  fragments that match the hashes in the
metadata.

Although this simplified algorithm achieves atomic semantics, it does
not address timely garbage-collection of obsolete fragments, the main
problem to be solved for amnesic erasure-code distributed storage.
It is easy to see that overwriting the fragments during the next write
operation may cause a reader to stall.




Protocol \NAME uses two mechanisms to address this: first, the writer
\emph{retains} those values that may be accessed concurrently and
exempts them from garbage collection so that their fragments remain
intact for concurrent readers, which gives the reader enough time to
retrieve its fragments. Secondly, some of the retained values may also
be \emph{frozen} in response to concurrent reads; this forces a
concurrent read to retrieve a value that is guaranteed to exist at the
data \nodes rather than simply the newest value, thereby effectively
limiting the amount of stored values. A similar freezing method has
been used for wait-free atomic storage with replicated
data\cite{gulevu06,domasu08}, but it must be changed for
erasure-coded storage with separated metadata.  The retention
technique together with the separation of metadata appears novel.

For the two mechanisms, every reader maintains a \emph{reader index}, both in its
local variable \var{readindex} and in its metadata. The reader index serves for
coordination between the reader and the writers. The reader increments its index
whenever it starts a new \var{r}-\op{Read} and immediately writes it to \dir, thereby
announcing its intent to read. Writers access the reader indices after updating
the metadata for a write and before (potentially) erasing obsolete fragments.
Every writer  maintains a table \var{frozenindex} with its most
recent recollection of all reader indices. When the newly obtained index of a
reader  has changed, then  detects that  has started a new operation
at some time after the last write of.

When  detects a new operation of , it does not know whether  has
retrieved the timestamp from \dir before or after the \dir-\op{Update} of the
current write. The reader may access either value; the writer therefore
\emph{retains} both the current and the preceding value for
 by storing a pointer to them in \var{frozenptrlist} and in
\var{reservedptrlist}.
Clearly, both values have to be excluded from garbage collection byin order to guarantee that the reader completes.


However, the operation of the reader may access \dir after the
\dir-\op{Update} of one or more subsequent write operation by,
which means that the \nodes would have to retain every value
subsequently written by  as well.  To prevent this from happening
and to limit the number of stored values,  \emph{freezes} the
currently written timestamp (as well as the value) and forces  to
read this timestamp when it accesses \dir within the same operation.
In particular, the writer stores the current timestamp in
\var{frozenptrlist} at index and updates the reader index of 
in \var{frozenindex}; then, the writer pushes both tables,
\var{frozenindex} and \var{frozenptrlist}, to the metadata service
during its next \var{r}-\op{Write}. The values designated by
\var{frozenptrlist} (they are called \emph{frozen}) and
\var{reservedptrlist} (they are called \emph{reserved}) are retained
and excluded from garbage collection until detects the
next read of , i.e., the reader index of  increases.  Thus, the
current read may span many concurrent writes of  and the fragments
remain available until  finishes reading.



On the other hand, a reader must consider frozen values.  When
a slow read operation spans multiple concurrent writes, the readerlearns that it should retrieve the frozen value through its entry in
the \var{frozenindex} table of the writer.  More precisely, when 
retrieves the metadata from \dir and finds that writer 's
 entry equals its  variable,
then  has frozen the value designated by 
for.

The protocol is amnesic because each writer retains at most two values
per reader, a frozen value and a reserved value.  Every data \node
therefore stores at most two fragments for every reader-writer pair
plus the fragment from the currently written value.  The combination
of freezing and retentions ensures that readers never wait.

\subsection{Details}
\label{sec:details}

\paragraph{Data structures.}
We use abstract data structures for compactness.  In particular, given
a timestamp , its two fields can be accessed
as  and .  A data type 
denotes a set of tuples of the form  with , , and  for .
Their initialization value is .

A \var{Pointers} structure contains the relevant information about one
stored value.  For example, the writer locally maintains
 designating to the most recently
written value.  More specifically,  contains
the timestamp of the written value, 
contains the identities of the \nodes that have confirmed to have
stored the written value, and  contains the
cross checksum, the list of hash values of the data fragments, of
the written value.


The metadata directory\var{dir} contains a vector with a tuple
for every client of the form

where the field  represents the
\emph{written value}, the field \var{frozenptrlist} is an array
indexed by  such that  denotes a value \emph{frozen by  for reader},
and the integer \var{readindex} denotes the reader-index of.

For preventing that concurrently accessed fragments are
garbage-collected, the writer maintains two tables,
\var{frozenptrlist}, and \var{reservedptrlist}, each containing one
\var{Pointers} entry for every reader in \clientset.  The second one,
\var{reservedptrlist}, is stored only locally, together with the
\var{frozenindex} table, which denotes the writer's most recently
obtained copy of the reader indices.  For the operations of the
reader, only the local \var{readindex} counter is needed.

Every client maintains the following variables between operations:
\var{writeptr}, \var{frozenptrlist}, \var{frozenindex}, and
\var{reservedptrlist} implement freezing, reservations, and retentions
for writers as mentioned, and \var{readindex} counts the reader
operations.

When clients access \dir, they may not be interested to retrieve all
fields or to update all fields.  For clarity we replace the fields to
be ignored by  in those \dir-\op{Scan} and \dir-\op{Update}
operations.


\paragraph{Operations.}
At the start of a write operation, the writer saves the current
value of \var{writeptr} in \var{prevptr}, to be used later during its
operation, if  should reserve and retain that value.  Then 
determines the timestamp of the current operation, which is stored in
.  After computing the fragments of,
sending them to the data \nodes, and obtaining  acknowledgements,
the writer updates its metadata entry.  It writes \var{writeptr},
pointing to, together with \var{frozenptrlist} and
\var{frozenindex}, as they resulted after the previous write to \dir.  Then
 invokes \dir-\op{Scan} and acquires the current metadata,
which it uses to determine values to freeze and to retain.  It
compares the acquired reader indices with the ones obtained during its
last write (as stored in \var{frozenindex}).  When  detects a read
operation by because , it freezes the current value (by setting
 to \var{writeptr})
and reserves the previously written value (by setting
 to \var{prevptr}).  Finally, the writer
deletes all fragments at the data \nodes except for those of the
currently written and the retained values.























To determine the timestamps for retrieving fragments, the reader
uses the following two functions:
\begin{minipage}{0.4\textwidth}
\begin{tabbing}\small
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \> \textbf{function}  \textbf{is} \\
  \> \> \textbf{if}  \textbf{then} \\
  \> \> \> \textbf{return}  \\
  \> \> \textbf{else} \quad //  \\
  \> \> \> \textbf{return}  \end{tabbing}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\vspace{9mm}
\begin{tabbing}\small
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \> \textbf{function}  \textbf{is} \\
  \> \>  \\
  \> \> \textbf{forall}  \textbf{do} \\
  \> \> \>  \\
  \> \> \> \textbf{if} 
              \textbf{then} \\
  \> \> \> \> \var{max} \becomes \var{ptr} \\
  \> \> \textbf{return} 
\end{tabbing}
\end{minipage}

\newpage



\noindent
Upon retrieving the array  from \dir, the reader sets
,
which implements the logic of accessing frozen timestamps.
\if\submit\no
The two functions above ensure that
\begin{tabbing}\small
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \>  \\
  \> \>  \\
  \> \> \>  \\
  \> \> \>  \\
\\
  \>  \\
  \> \> ,
         where 
\end{tabbing}
\noindent
\fi
The details of protocol\NAME appear in
\if\submit\yes
Algorithms\ref{alg:client-1}--\ref{alg:client-2}.
\else
Algorithms\ref{alg:client-1}--\ref{alg:datareplica}.
\fi



\begin{alg}\small
\begin{tabbing}
xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
\if\submit\no
  \textbf{Uses} \\
  \> Atomic snapshot object, \instance{dir} \\
  \> Data \nodes, \textbf{instances}  \\
  \\
\fi
  \textbf{State} \\
  \> // State maintained across write and read operations \\

\> , initially 
     \` // Metadata of the currently written value \\
  \> , initially ,
     for 
     \` // Value frozen and retained for reader  \\
  \> , initially ,
     for 
     \` // Value reserved and retained for reader \\
  \> , initially 0, for 
     \` // Last known reader index of \\

\> , initially 0
  \` // Reader index of \\

  \> // Temporary state during operations \\
  \> , initially 
     \` // Metadata of the value written by  prior to current write \\
  \> , initially 
     \` // Metadata of the value to be read by  \\
  \> , initially , for 
     \` // List of \nodes that have responded during read \\
  \\
  \textbf{upon} \eventt{r}{Write}{} \textbf{do} \\
  \>  \\
\> \newe \event{dir}{Scan};
     \textbf{wait for} \eventt{dir}{ScanResp}{} \\
  \> 
     \` // Highest timestamp field\var{ts} in a \var{writeptr} in  \\
  \> 
     \` \> // Construct metadata of the currently written value \\
  \>  \\
  \>  \\
  \> \textbf{forall}  \textbf{do} \\
  \> \>  \\
  \> \> \newe \eventt{}{Write}{} \\
  \\
  \textbf{upon} \eventt{}{WriteAck}{\var{ats}} \textbf{such that}
  	 \textbf{do} \\
  \>  \\
  \> \textbf{if}  \textbf{then} \\
\> \> // Update metadata at \dir with currently written value
           and with frozen values from previous write \\
  \> \> \newe \eventt{dir}{Update}{};
        \textbf{wait for} \event{dir}{UpdateAck} \\
\> \> // Obtain current reader indices \\
  \> \> \newe \event{dir}{Scan};
        \textbf{wait for} \eventt{dir}{ScanResp}{} \\
  \> \>  \\
  \> \> \textbf{forall}  \textbf{do} \\
  \> \> \>  \\
  \> \> \> \textbf{if}  \textbf{then}
           \` // Client may be concurrently reading \var{prevptr}
                 or \var{writeptr} \\
  \> \> \> \>  \\
  \> \> \> \> ;
               \\
  \> \> \> \>  \\
  \> \>  \\
  \> \> \textbf{forall}  \textbf{do}
        \` // Clean up all fragments except for current, frozen, and reserved values \\
  \> \> \> \newe \eventt{}{Free}{\var{freets}}\\
\> \> \newe \event{r}{WriteAck}
\end{tabbing}
\caption{Protocol\NAME, atomic register instance\var{r}
  for client (part1).}
\label{alg:client-1}
\end{alg}


\begin{alg}\small
\begin{tabbing}
  xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
\textbf{upon} \event{r}{Read} \textbf{do} \\
  \> \textbf{forall}  \textbf{do}
         \\
  \>  \\
  \> \newe \eventt{dir}{Update}{};
     \textbf{wait for} \event{dir}{UpdateAck} \\
  \> // Parse the content of \dir and extract the
      highest timestamp, potentially frozen for \\
  \> \newe \event{dir}{Scan};
     \textbf{wait for} \eventt{dir}{ScanResp}{} \\
  \>  \\
  \> \textbf{if}  \textbf{then} \\
  \> \> \newe \eventt{r}{ReadResp}{} \\
  \> \textbf{else} // Contact the data \nodes to obtain the data fragments\\
  \> \> \textbf{forall}  \textbf{do} \\
  \> \> \> \newe \eventt{}{Read}{} \\
  \\
  \textbf{upon} \eventt{}{ReadResp}{}
	\textbf{such that}  \textbf{do} \\
  \> \textbf{if} 
        \textbf{then} \\
  \> \>  \\
  \> \> \textbf{if} 
     \textbf{then} \\
  \> \> \>  \\
  \> \> \> \var{retval} \becomes  \\
  \> \> \> \newe \eventt{r}{ReadResp}{\var{retval}}
\end{tabbing}
\caption{Protocol\NAME, atomic register instance\var{r}
  for client (part2).}
\label{alg:client-2}
\end{alg}

\if\submit\no

\begin{alg}\small
\begin{tabbing}
  xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxx\kill
  \textbf{State} \\
  \> , initially ,
     for 
     \` // Stored data values indexed by timestamp \\
  \\
  \textbf{upon} \eventt{}{Write}{} \textbf{do} \\
  \> \\
  \> \newe \eventt{}{WriteAck}{}\\
  \\
  \textbf{upon} \eventt{}{Read}{} \textbf{do} \\
  \> \newe \eventt{}{ReadResp}{} \\
  \\
  \textbf{upon} \eventt{}{Free}{} \textbf{do}\\
  \> \textbf{forall}  \textbf{do} \\
  \> \>  \\
  \> \newe \eventt{}{FreeAck}{}
\end{tabbing}
\caption{Protocol\NAME, implementation of data \node.}
\label{alg:datareplica}
\end{alg}

\fi




\if\submit\no

\paragraph{Remarks.}

Note that \NAME does not need a majority of correct data \nodes and
neither refers to quorum systems for correctness; these aspects are
all encapsulated in the directory service.  For liveness, though, the
protocol needs to obtain responses from  data \nodes during write
operations, which is only possible if .

In the current formulation of \NAME, every writer retains exactly two
values for each reader, regardless of whether the reader has completed
its operation.  In fact, a value continues to be retained
for a reader until  invokes a subsequent \var{r}-\op{Read} (and
concurrently or later, the writer invokes another \var{r}-\op{Write}).
In order to avoid retaining unnecessary values, one could introduce an
additional field in the metadata for each reader, through which the
reader can signal when it completes a read operation.  The writer
would periodically check this and remove the values no longer needed.

The data \nodes can be implemented from a key-value store (KVS)
abstraction that has become a prominent interface for cloud-storage
systems.  A KVS can be implemented from read/write registers, as shown
by Cachin et al.\cite{cajuso12}, though their implementation does
not preserve the space complexity.

\fi

\begin{comment}
\note{Readers may be Byzantine.}

In current formulation and the complexity analysis presented in
Section\ref{} we implicitely assume that there are no Byzantine clients.
Clearly, given that a Byzantine writer could update \dir with timestamps that
do not correspond to values stored to data \nodes, readers could be
stalled from completing.
However, \NAME algorithms could resist such poisonous writer attacks with a
slight modification on the reader side. In particular, we could modify the
\var{highestread} operation such that it returns the highest  timestamps
from the timestamps returned from the \var{readfrom} invocations within.
Then the reader could issue read requests for the data fragments stored
in each data \node for all  values. Given that at most  entries
are false, the reader is guaranteed to reconstruct at least one value.
Clearly, such an extension would increase the communication overhead of
\op{Read} into  bits.

\note{The optimistic scheme of ORCAS\cite{dugule08} can be an
  extension of this, if synchrony is assumed.  (ORCAS-A or -B??}

\note{In current formulation all data \nodes store the fragment, but
this is not needed; change d-WriteAck handler so that
additional parameter gcts denotes which data can be erased (all up
to and including gcts); for \nodes in wset, gcts=ts-1, for the
others gcts==ts, ie., they may erase the fragment again.}

\note{Address consistency amplification aspect.}
\note{Garbage collection could be disconnected from register-write operations.}

\end{comment}




\section{Complexity comparison}
\label{sec:complexity}

This section compares the communication and storage complexities of
\NAME to existing erasure-coded distributed storage solutions, in a
setting with  data \nodes and clients.  We denote the size of
each stored value  by .  In line with the intended deployment scenarios, we assume
that  is much larger (by several orders of magnitude) than 
and  , i.e.,  and .

We examine the worst-case communication and storage costs incurred by
a client in the protocol and distinguish metadata operations (on \dir)
from operations on the data \nodes with data (i.e., erasure-coded
fragments of data values).

For protocol\NAME, the metadata of one value written to \dir consists
of a pointer, containing the cross checksum with  hash values, the
 identities of the data \nodes that store a data fragment, and a
timestamp.  Moreover, the metadata entry of one writer contains also
the list of  pointers to frozen values, the  indices relating to
the frozen values, and the writer's reader index.  Assuming a
collision-resistant hash function with output size bits and
timestamps no larger than  bits, the total size of the
metadata is .  (Note that a -bit counter
suffices for all protocol executions where the hash function is
secure, as collisions in hash functions can be found with about
 operations.)  In the remainder of this section, the
size of the metadata is considered to be negligible and is ignored,
though it would incur in practice.

According to the above assumption, the complexity of \NAME is
dominated by the data itself.  When writing a value ,
the writer sends a fragment of size  and a timestamp of size
 to each of the  data \nodes.  Assuming further that , the total storage space occupied by  at the data
\nodes amounts to  bits.  Similarly, a read operation
incurs a communication cost of  bits.

With respect to storage complexity, protocol \NAME freezes and
reserves two timestamps and their fragments for each writer-reader
pair, and additionally stores the fragments of the last written value
for each writer.  This means that the storage cost is at most  bits in total.  The improvement described in a remark of
Section\ref{sec:details} reduces this to  in the best
case.




\begin{table}
 \centering
\begin{tabular}{|l||cc|c|}
 \hline
 {{\bf Protocol}}
 & \multicolumn{2}{|c|}{{\bf Communication cost} }
 & \multicolumn{1}{|c|}{{\bf Storage cost}}\\

 & \multicolumn{1}{|c}{\op{Write}}
 & \multicolumn{1}{c|}{\op{Read}}
 &
\\\hline

 ORCAS-A\cite{dugule08}
 &  
 &  
 &  \\ \hline

 ORCAS-B\cite{dugule08}
 & 
 & 
 & \\ \hline

 CASGC\cite{clmm13}
 &  \T
 & 
 &  \\ \hline

 CT\cite{cactes06}
 & 
 &  \T
 &  \T \\ \hline

 HGR\cite{hegare07b}
 &  \T
 & 
 & \\  \hline

 M-PoWerStore\cite{dklmsv13}
 &  \T
 & 
 &  \\ \hline

 DepSky\cite{bcqas11}
 &  \T
 & 
 &  \\ \hline

 \NAME (Sec.\ref{sec:protocol})
 &  \T
 & 
 & \\\hline
\end{tabular}
\caption{Comparison of the communication and space complexities of
  erasure-coded distributed storage solutions.  There are  clients, 
  data \nodes, the erasure code parameter is , and the data values
  are of size  bits. An asterisk (\T) denotes optimal properties.}
\label{tab:complexity}
\if\submit\yes
\vspace*{-4mm}
\fi
\end{table}

Table\ref{tab:complexity} shows the communication and storage costs
of protocol \NAME and the related protocols.
We use the wait-free semantics achieved by \NAME and others as the
base case; in CASGC\cite{clmm13} and HGR\cite{hegare07b}, a read
operation concurrent with an unbounded number of writes may not
terminate, hence we state their cost as.  In contrast to
\NAME, DepSky\cite{bcqas11} is neither wait-free nor amnesic and
M-PoWerStore\cite{dklmsv13} is not amnesic.  It is easy to see that
\NAME performs better than most storage solutions in terms
communication complexity.


\if\submit\no
\section{Analysis}
\label{sec:proof}
\fi

\newcommand{\opr}{\ensuremath{o}\xspace}
\newcommand{\popr}{\ensuremath{o'}\xspace}
\newcommand{\opread}{\ensuremath{\op{Read}}\xspace}
\newcommand{\opwrite}{\ensuremath{\op{Write}}\xspace}
\newcommand{\tsopr}{\ensuremath{\var{ts}}\xspace}
\newcommand{\tspopr}{\ensuremath{\var{ts}'}\xspace}

\if\submit\no
In this section we prove that protocol \NAME, given by
Algorithms\ref{alg:client-1}--\ref{alg:datareplica}, emulates an
atomic read/write register and is wait-free.

Whenever the metadata directory \var{dir} contains an entry  we say that timestamp\var{ts}
is \emph{frozen by  for }.  If \var{ts} is frozen by some 
for any, then \var{ts} is simply \emph{frozen}.  Furthermore,
considering the state of writer, a timestamp\var{ts} is said to
be \emph{retained by  for } when either
 (this includes that
\var{ts} is frozen by  for ) or when
 (which means that
\var{ts} is reserved by  for ).  A timestamp is \emph{retained}
by  when it is retained by  for some.  We call the timestamp
 the \emph{written} timestamp of .

\begin{lemma}[Frozen timestamps]\label{lem:timestamps}
  At any time the timestamps that a client has frozen
  are no larger than its written timestamp. More precisely, for all
  ,
  
  Moreover, during any \dir-\op{Update} operation of, the
  timestamp  and all timestamps
   may only increase.
\end{lemma}

\begin{proof}
  From Algorithm\ref{alg:client-1} it follows that for any client , the
  timestamps stored in  in successive
  \var{r}-\op{Write} operations of  increase.
  From the same algorithm, it is clear that
   is only updated through a
  \var{r}-\op{Write} operation of , and is set to the written timestamp
  of the preceding \var{r}-\op{Write} operation of , which is
  strictly smaller than the written timestamp stored in
  .  The second inequality
  follows analogously.  Thus, the values stored in
   only increase.
\end{proof}

We define the \emph{timestamp of a register operation \opr} as
follows: (i) for an \var{r}-\op{Write} operation, the timestamp of
\opr is the value assigned to variable\var{writeptr}.\var{ts}
during\opr; (ii) when \opr is an \var{r}-\op{Read} operation, then
its timestamp is the value assigned to variable\var{readptr}.\var{ts}
by \op{highestread}.  Note that the timestamp of an \var{r}-\op{Read}
operation is  if and only if \opr returns.
Furthermore, we say that a value  is \emph{associated} to a
timestamp \var{ts} whenever the timestamp of the register operation
that writes  is \var{ts}.

According to \op{highestread}, the timestamp in the returned pointer
may be frozen (taken from the \var{frozenptrlist} field of ) or written
(taken from the \var{writeptr} field of ), but not both.

\begin{lemma}[Read frozen timestamp]\label{lem:readfrozen}
  If the timestamp \var{ts} of a \var{r}-\op{Read} operation by
  client has been frozen for  by a client, then 
  executes two \var{r}-\op{Write} operations concurrently to,
  where the \dir-\op{Scan} operation of the former \var{r}-\op{Write}
  operation and the \dir-\op{Update} operation of the latter
  \var{r}-\op{Write} operation occur between
  \dir-\op{Update} and \dir-\op{Scan} operations of.  Moreover,
  the timestamp of the \var{r}-\op{Read} operation is \var{ts},
  the one associated with the value written by.
\end{lemma}

\begin{proof}
  From Algorithm\ref{alg:client-2} it follows that for
  \op{highestread} within  to return a frozen timestamp, then, if
   is the metadata snapshot returned by the \dir-\op{Scan}
  operation during, it holds . This means that  invoked \dir-\op{Update} with
  the most recent value of \var{readindex} before the \dir-\op{Scan}
  during. To do that,  must have detected the change of the
  \var{readindex} entry in  caused by  through the
  \dir-\op{Scan} operation invoked during. From
  Algorithm\ref{alg:client-1}, this can only be the operation through
  which  wrote the value associated to\var{ts}.
\end{proof}


\begin{lemma}[Partial order]\label{lem:partorder}
  Let \opr and \popr be two distinct operations on register\var{r}
  with timestamps\tsopr and \tspopr, respectively, such that \opr
  precedes \popr. Then .  Furthermore, if \popr
  is of type \var{r}-\op{Write}, then .
\end{lemma}

\begin{proof}
We distinguish between two cases, depending on the type of\opr.
\begin{description}
\item[Case 1:]
If \opr is of type \var{r}-\op{Write}, the claim follows directly from
  Lemma\ref{lem:timestamps} and from the algorithm of the writer.
  In particular, if \popr is of type \var{r}-\op{Read}, then, if there is
  no concurrent \var{r}-\op{Write} operation of the same client  as \opr,
  \tsopr{} is returned as written timestamp by the \var{readfrom} function when
  called for  and reader of \popr.
  In addition, if \popr runs concurrently with a
  \var{r}-\op{Write} of , then one of the two hold:
  (i) \tsopr{} (or a higher timestamp if many \var{r}-\op{Write} operations
  have intervened) is frozen for \popr{} and
  is returned by the \var{readfrom} operation invoked by \var{highestread}
  in \popr for , (ii) \tsopr (or a higher timestamp if many
  \var{r}-\op{Write} operations have intervened) has not yet been frozen
  by , in which case a written timestamp greater or equal to \tsopr
  (by Lemma\ref{lem:timestamps}) is returned by the \var{readfrom} operation
  invoked by \var{highestread} in \popr for .

\item[Case2:] If \opr is of type \var{r}-\op{Read}, then let
   be the maximum value of the timestamp field\var{ts} in
  a \var{writeptr} at the time when the \dir-\op{Scan} operation
  invoked by \opr returns.  Note that \op{highestread} obtains \tsopr
  as this maximum or as a frozen timestamp.
  Lemma\ref{lem:timestamps} implies now that .

  We now show that  by distinguishing two
  cases.  First, if \popr is of type \var{r}-\op{Write}, the writer
  calls \dir-\op{Scan} after  completes and determines the maximum
  value of the\var{ts} field in any \var{writeptr}.  Then it
  increments that timestamp to obtain \tspopr.  This ensures that
  , as claimed.

  Second, if \popr is of type \var{r}-\op{Read}, then \tspopr may
  either have been a written timestamp or a frozen timestamp (at the
  time when the client obtained the response of its \dir-\op{Scan}).
  If \tspopr has been written, then \tspopr is the maximum value of
  the\var{ts} field in any \var{writeptr}, which is at least as large
  as  by Lemma\ref{lem:timestamps} and by the atomicity
  of\dir.

  Alternatively, if \tspopr  has been frozen by writer, then
  Lemma\ref{lem:readfrozen} applies and shows that there exist two
  \var{r}-\op{Write} operations by  that are  concurrent
  to \popr, of which the first writes the value associated to \tspopr.
As such, if  is the timestamp returned by
  the \var{readfrom} function invoked by any \var{r}-\op{Read} operation \opr
  that precedes \popr{} and for writer , then .
  Since this can be extended to all writers, it holds that .
\end{description}
\end{proof}


\begin{lemma}[Unique writes]\label{lem:unqwrites}
  If \opr and \popr are two distinct operations of type
  \var{r}-\op{Write} with timestamps \tsopr and \tspopr, respectively,
  then .
\end{lemma}

\begin{proof}
  If \opr and \popr are executed by different clients, then the two
  timestamps differ in their second component. If \opr and \popr are
  executed by the same client, then the client executed them
  sequentially. By Lemma\ref{lem:partorder}, it holds .
\end{proof}





\begin{lemma}[Integrity]\label{lem:integr}
  Let  be an operation of type \var{r}-\op{Read} with timestamp
   that returns a value .  Then there is a
  unique operation  of type \var{r}-\op{Write} that writes 
  with timestamp.
\end{lemma}

\begin{proof}
  Operation by client returns and is, thus, complete.
  This means that the client has processed  events of type
  -\op{ReadResp} from distinct \nodes in a set ;
  according to the protocol, the client has verified that the response
  from every  contains a
  timestamp and a fragment such that  and .



  According to the code, the value \var{readptr} is computed from a
  \var{writeptr} or a  entry stored in the metadata
  directory\dir.  This pointer must have been computed during the
  write operation with timestamp and was later stored in
  \dir by the same client.  Note that by Lemma\ref{lem:unqwrites}, no
  other write has timestamp .  From the algorithm of the
  writer, it follows that the entries in \var{readhash} were generated
  as hash values of the fragments, i.e., , where  for  represent the
  erasure-coded fragments of some value.

  Based on the check by the reader and the security property of the
  hash function, this means that  for all . The completeness of the erasure code now implies
  that the reconstruction yields , the value associated
  to and written by .
\end{proof}



\newcommand{\opwi}[1]{\ensuremath{o_{w,#1}}\xspace}
\newcommand{\updatewi}[1]{\dir-\ensuremath{\var{Update}_{w,#1}}\xspace}
\newcommand{\scanwi}[1]{\dir-\ensuremath{\var{Scan}_{w, #1}}\xspace}
\newcommand{\updaterd}{\dir-\ensuremath{\var{Update}_r}\xspace}
\newcommand{\scanrd}{\dir-\ensuremath{\var{Scan}_r}\xspace}

\begin{lemma}[Read concurrent with multiple writes]\label{lem:concurrent}
  Consider an operation of type \var{r}-\op{Read} invoked by a
  reader, with timestamp .  At the time when 
  determines (by \op{highestread}), there are at least
   distinct correct data \nodes that store a data fragment
  (different from) under timestamp and they do not
  free this fragment before  completes.
\end{lemma}

\begin{proof}
  Suppose that  and the writer is
  client.  Consider a sequence  of
  \var{r}-\op{Write} operations executed by  with respective
  timestamps , of which some
  are concurrent to.  Now consider the linearization of \dir and
  let \opwi{i} be the last one among these \var{r}-\op{Write}
  operations whose \dir-\op{Update} (denoted by \updatewi{i}) precedes
  the \dir-\op{Update} operation of the reader during (denoted
  by \updaterd).  Let \var{readindex} denote the reader's index
at the time when invokes \updaterd.

  W.l.o.g.\ suppose that \updaterd follows at least one
  \dir-\op{Update} operation that is triggered by an
  \var{r}-\op{Write} operation of; furthermore, suppose that 
  executes at least one more \var{r}-\op{Write} operation
  \updatewi{i+1} after \updatewi{i}.

  We claim that .  To show this, we distinguish four cases,
  considering the linearization of operations on\dir.  Let \scanwi{i}
  denote the second invocation of \dir-\op{Scan} during \opwi{i}, the
  one from which the writer takes\var{readindex}.\begin{description}
  \item[Case1:] Suppose that \updaterd precedes \scanwi{i}; this
    means that  detects the concurrent read during \opwi{i},
    in the sense that updates its variable  to
    \var{readindex}.

    (Case1.a) If the \dir-\op{Scan} operation of the reader    during, denoted by \scanrd, precedes \updatewi{i+1}, then
     obtains  as the highest timestamp
    stored in by the algorithm.

    (Case1.b) Otherwise, \scanrd follows \updatewi{i+1}; then the
    reader obtains  such that  is equal
    to\var{readindex} and , according to
    \op{readfrom} in the protocol and because 
    is equal to\var{readindex}.


  \item[Case2:] Suppose that \updaterd follows \scanwi{i}.  This
    means that \updaterd{} takes place between \scanwi{i} and
    \updatewi{i+1} and  detects the concurrent read only
    during \opwi{i+1}, after executing \scanwi{i+1}.  The same two
    sub-cases may occur now.

    (Case2.a) If \scanrd precedes \updatewi{i+1}, then , analogous to Case1.a.

    (Case2.b) Otherwise, \scanrd follows \updatewi{i+1} and the
    reader obtains .  To see this,
    suppose that (Case2.b.i) \scanrd precedes the \updatewi{i+2} in
    the subsequent \var{r}-\op{Write} operation of or there is no
    such \var{r}-\op{Write}; then, the value \var{readindex} of 
    remains greater than  and thus  sets
    .  Alternatively (Case2.b.ii),
    suppose that \scanrd follows \updatewi{i+2}; then, according to
    the protocol, the writer has already set  during \updatewi{i+2} and  sets  analogous to Case1.b.
  \end{description}

  Suppose the reader determines that ; then the correct \nodes in 
  store a fragment of the associated value because at least 
  \nodes in \var{readptr}.\var{set} have sent a
  \var{}-\op{WriteAck} for to the writer.
  Accounting for the up to  faulty \nodes, at least  correct
  \nodes have once stored a fragment in .
It remains to argue why these \nodes do not free this fragment
  before  completes.

  In Case1.a, the writer detects the concurrent read during \opwi{i}
  and therefore excludes the data fragments associated to 
  from garbage collection for, by setting
    to  in its state.
  According to the logic of the protocol,  remains frozen
  and the corresponding fragments are retained at least until  invokes
  a subsequent read operation.


  In Case2.a, almost the same happens during \opwi{i+1}, when the
  writer detects the concurrent read.  The writer sets
   to  in its state.
  Again according to the protocol,  remains reserved and
  the writer retains the corresponding fragments at least until 
  invokes a subsequent read.

Intuitively, Cases1.a and 2.a demonstrate why  retains two
  values during a write (the one being written and the one written
  before):  does not know which one of the two the reader is about
  to access.

  In Case2.b.i, if the writer detects the concurrent read during
  \opwi{i+2}, then it reserves and retains and the claim
  follows analogously to Case2.a.

  In Cases1.b and 2.b.ii, the reader accesses a frozen value.  Again,
  according to the protocol,  remains frozen and is retained
  at least until  invokes a subsequent read operation.  The lemma follows.
\end{proof}
\fi

\if\submit\no
\begin{theorem}[Atomicity]\label{thm:atomic}
  Given a atomic snapshot object \dir, protocol \NAME emulates an
  atomic MRMW register\var{r}.
\end{theorem}


\begin{proof}
  We show that every execution  of the protocol is
  linearizable with respect to an MRMW register.  By
  Lemma\ref{lem:integr}, the timestamp of a \var{r}-\op{Read} either
  has been written by some \var{r}-\op{Write} operation or \var{r}-\opread
  returns.

  We first construct an execution from  by completing
  all operations of type \var{r}-\op{Write} for those values that
  have been returned by some \var{r}-\op{Read} operation. Then we
  obtain a sequential permutation from  as follows: (1)
  order all operations according to their timestamps; (2) among the
  operations with the same timestamp, place the \var{r}-\op{Read}
  operations immediately after the unique \var{r}-\op{Write} with this
  timestamp; and (3) arrange all non-concurrent operations in the same
  order as in.  Note that concurrent \var{r}-\op{Read}
  operations with the same timestamp may appear in arbitrary order.

  For proving that  is a view of  at a client w.r.t.\ a
  register, we must show that every \var{r}-\op{Read} operation
  returns the value written by the latest preceding \var{r}-\op{Write}
  that appears before in  or  if there is no such
  operation.

  Let  be an operation of type \var{r}-\opread with
  timestamp that returns a value. If , then
  by construction  is ordered before any write operation
  in.  Otherwise, it holds  and according to
  Lemma\ref{lem:integr}, there exists an \var{r}-\op{Write} operation
   that writes  with the same timestamp.  In this case, 
  is placed in  before  by construction.  No other
  \var{r}-\op{Write} operation appears between  and  because
  all other write operations have a different timestamp and therefore
  appear in  either before or after.

  It remains to show that  preserves the real-time order
  of.  Consider two operations\opr and \popr in  with
  timestamps  and , respectively, such
  that \opr precedes\popr.  From Lemma\ref{lem:partorder}, we have
  . If  then \popr appears
  after \opr in  by construction.  Otherwise 
  and \popr must be an operation of type \var{r}-\op{Read}.  If \opr
  is of type \var{r}-\op{Write}, then \popr appears after \opr since
  we placed each \var{r}-\op{Read} after the \var{r}-\op{Write} with
  the same timestamp.  Otherwise, \opr is a \var{r}-\op{Read} and the
  two \var{r}-\op{Read} operations appear in  in the same order
  as in by construction.
\end{proof}
\fi

\if 0

\begin{lemma}[Read progress]
  Consider an r-read operation by client with timestamp  /
  pointer ; then at least distinct  data \nodes that
  store a data fragment such that  etc. matches \dots\ and
  they do not ``free'' these before  invokes its subsequent r-read
  op.
\end{lemma}

\begin{proof}
  adapt from below
\end{proof}


\begin{lemma}[Concurrent Read/Write (single writer)]
\label{lem:concwr}
Let  be an operation of type \var{r}-\op{Read} with timestamp 
invoked by a reader , and  be operations
of type \var{r}-\op{Write} invoked by the only writer ,
and  their respective timestamps.
We further assume that some of the \var{r}-\op{Write} operations are
concurrent with . Now, let \opwi{i} be the most recent
\var{r}-\op{Write} operation
whose \dir-\op{Update} (we call the latter \updatewi{i}) completes before the
\dir-\op{Update} of the  (we call the latter \updaterd).
Then,  or . \\
\note{Corner cases:} (1) If there is no \updatewi{i} call that has completed
before \updaterd, then  or .
(2) If , then .\\
Furthermore, these two values are \emph{frozen}, i.e., excluded from garbage
collection by  until {} **completes**.

\end{lemma}

\begin{proof}
We distinguish between the two cases:
\begin{itemize}
\item[(i)] \updaterd{} precedes \scanwi{i}:
Here, the writer  detects the ongoing , and updates its local
\var{frozenindex} variable accordingly. If \scanrd{} precedes
\updatewi{i+1}, then . Otherwise, if
\scanrd{} is invoked at any point after \updatewi{i+1}, 
forces ; the latter is because from this point onwards
\var{readindex} within  is equal to .
\item[(ii)] \updaterd{} does not precede \scanwi{i}:
In this case \updaterd{} is invoked between \scanwi{i} and \updatewi{i+1},
and the writer  detects the ongoing  via \scanwi{i+1}. As before,
if \scanrd is invoked before \updatewi{i+1}, .
If \scanrd{} is invoked at any point after \updatewi{i+1}, then
. In particular, if \scanrd  occurs before
\updatewi{i+2} (assuming that ), \var{readindex} within
 remains greater than , and thus reads
sets . Otherwise, \var{readindex} when \scanrd occurs
equals to , and  forces
. Clearly, if ,  sets .
\end{itemize}
\note{Corner cases}. It is easy to see, that if , then  sets
. Similarly, if , i.e., \updaterd{} takes place before
\updatewi{1}, then if \scanrd is invoked before \updatewi{1},  sets
, while if \scanrd is invoked after \updatewi{1},  sets
.

In addition, it is easy to see that in both cases,  is frozen by
. \note{E: Need to add more things here.}
\end{proof}


\begin{lemma}[Concurrent read/write (multiple writers)]\label{lem:concmwr}
Let  be an operation of type \var{r}-\op{Read} with timestamp 
invoked by a reader , and  be
operations of type \var{r}-\op{Write} invoked by each
writer , and  their
respective timestamps.
We further assume that some of the \var{r}-\op{Write} operations are
concurrent with , while \var{r}-\op{Write} operations of different
clients can also be concurrent with each other and/or with .
Now, assume that for each writer ,
 denotes the the most recent \var{r}-\op{Write} operation of
 whose \dir-\op{Update} (we call the latter ) completes
before the \dir-\op{Update} of the  (we call the latter \updaterd).
Then, 
\end{lemma}

\begin{proof}
It derives directly from Lemma\ref{lem:concwr}.\\
\note{E: Is there a simple way to do this?}\\
\end{proof}

\fi



\if\submit\no
\begin{theorem}[Wait-freedom]\label{thm:waitfree}
  Given an atomic snapshot object \dir and assuming that , protocol \NAME is wait-free.
\end{theorem}

\begin{proof}
  As the atomic snapshot \dir operates correctly, all its operations
  eventually complete independently of other processes. It remains to
  show that no \var{r}-\op{Write} and no \var{r}-\op{Read} operation
  blocks.

  For a \var{r}-\op{Write} operation, the client needs to receive  \var{}-\op{WriteAck} events from distinct data \nodes
  before completing.  As there are  \nodes and up to  may be
  faulty, the assumption  implies this.

  During a \var{r}-\op{Read} operation, the reader needs to obtain 
  valid fragments, i.e., fragments that pass the verification of their
  hash value.  According to Lemma\ref{lem:concurrent}, there are at
  least  correct data \nodes designated by
  \var{readptr}.\var{set} that store a fragment under
  timestamp until the operation completes.  As the reader
  contacts these \nodes and waits for  fragments, these fragments
  eventually arrive and can be reconstructed to the value written by
  the writer by the completeness of the erasure code.
\end{proof}
\fi





\if\submit\no
\section{Conclusion}

This paper has presented \NAME, the first \emph{erasure-coded}
distributed implementation of a multi-writer multi-reader read/write
storage object that is, at the same time: (1) asynchronous, (2)
wait-free, (3) atomic, (4) amnesic, (i.e., with data \nodes storing a
bounded number of values) and (5) Byzantine fault-tolerant (BFT) using
the optimal number of \nodes. \NAME is efficient since it does not use
public-key cryptography and requires data \nodes that support only
reads and writes, further reducing the cost of deployment and
ownership of a distributed storage solution. Notably, \NAME stores
metadata separately from -out-of- erasure-coded fragments. This
enables \NAME to be the first BFT protocol that uses as few as 
data \nodes to tolerate  Byzantine \nodes, for any .

Future work should address how to optimize protocol \NAME and to
reduce the storage consumption for practical systems; this could be
done at the cost of increasing its conceptual complexity and losing
some of its ideal properties.  For instance, when the metadata service
is moved from a storage abstraction to a service with processing, it
is conceivable that fewer values have to be retained at the \nodes.


\section*{Acknowledgment}

We thank Alessandro Sorniotti, Nikola Kne\v{z}evi\'{c}, and Radu
Banabic for inspiring discussions during the early stages of this
work. This work is supported in part by the EU CLOUDSPACES (FP7-317555)
and SECCRIT (FP7-312758) projects.

\fi




\begin{thebibliography}{10}

\bibitem{ackm06}
I.Abraham, G.Chockler, I.Keidar, and D.Malkhi.
\newblock Byzantine disk {Paxos}: Optimal resilience with {Byzantine} shared
  memory.
\newblock {\em Distributed Computing}, 18(5):387--408, 2006.

\bibitem{abcccd02}
A.Adya, W.J. Bolosky, M.Castro, G.Cermak, R.Chaiken, J.R. Douceur,
  J.Howell, J.R. Lorch, M.Theimer, and R.P. Wattenhofer.
\newblock {FARSITE}: Federated, available, and reliable storage for an
  incompletely trusted environment.
\newblock In {\em Proc.\ 5th Symp.\ Operating Systems Design and Implementation
  (OSDI)}, 2002.

\bibitem{aadgms93}
Y.Afek, H.Attiya, D.Dolev, E.Gafni, M.Merritt, and N.Shavit.
\newblock Atomic snapshots of shared memory.
\newblock {\em Journal of the ACM}, 40(4):873--890, 1993.

\bibitem{bcqas11}
A.Bessani, M.Correia, B.Quaresma, F.Andr\'{e}, and P.Sousa.
\newblock {DepSky}: Dependable and secure storage in a cloud-of-clouds.
\newblock In {\em Proc.\ 6th European Conference on Computer Systems
  (EuroSys)}, pages 31--46, 2011.

\bibitem{bojuop09}
K.D. Bowers, A.Juels, and A.Oprea.
\newblock {HAIL}: A high-availability and integrity layer for cloud storage.
\newblock In {\em Proc.\ 16th ACM Conference on Computer and Communications
  Security (CCS)}, pages 187--198, 2009.

\bibitem{cadovu13}
C.Cachin, D.Dobre, and M.Vukoli{\'c}.
\newblock {BFT} storage with  data replicas.
\newblock Report arXiv:1305.4868, CoRR, 2013.

\bibitem{CachinGR11}
C.Cachin, R.Guerraoui, and L.Rodrigues.
\newblock {\em Introduction to Reliable and Secure Distributed Programming
  ({Second Edition})}.
\newblock Springer, 2011.

\bibitem{cajuso12}
C.Cachin, B.Junker, and A.Sorniotti.
\newblock On limitations of using cloud storage for data replication.
\newblock Proc.\ WRAITS, 2012.

\bibitem{cactes06}
C.Cachin and S.Tessaro.
\newblock Optimal resilience for erasure-coded {Byzantine} distributed storage.
\newblock In {\em Proc.\ International Conference on Dependable Systems and
  Networks (DSN-DCCS)}, pages 115--124, 2006.

\bibitem{clmm13}
V.R. Cadambe, N.Lynch, M.Medard, and P.Musial.
\newblock Coded atomic shared memory emulation for message passing
  architectures.
\newblock CSAIL Technical Report MIT-CSAIL-TR-2013-016, MIT, 2013.

\bibitem{chguke07}
G.Chockler, R.Guerraoui, and I.Keidar.
\newblock Amnesic distributed storage.
\newblock In G.Taubenfeld, editor, {\em Proc.\ 21th International Conference
  on Distributed Computing (DISC)}, volume 4731 of {\em Lecture Notes in
  Computer Science}, pages 139--151. Springer, 2007.

\bibitem{dklmsv13}
D.Dobre, G.Karame, W.Li, M.Majuntke, N.Suri, and M.Vukoli\'{c}.
\newblock {PoWerStore}: Proofs of writing for efficient and robust storage.
\newblock In {\em Proc.\ ACM Conference on Computer and Communications Security
  (CCS)}, 2013.

\bibitem{domasu08}
D.Dobre, M.Majuntke, and N.Suri.
\newblock On the time-complexity of robust and amnesic storage.
\newblock In T.P. Baker, A.Bui, and S.Tixeuil, editors, {\em Proc.\ 12th
  Conference on Principles of Distributed Systems (OPODIS)}, volume 5401 of
  {\em Lecture Notes in Computer Science}, pages 197--216. Springer, 2008.

\bibitem{dugule08}
P.Dutta, R.Guerraoui, and R.R. Levy.
\newblock Optimistic erasure-coded distributed storage.
\newblock In G.Taubenfeld, editor, {\em Proc.\ 22th International Conference
  on Distributed Computing (DISC)}, volume 5218 of {\em Lecture Notes in
  Computer Science}, pages 182--196. Springer, 2008.

\bibitem{fmssv04}
S.乞茱祯钿廉湾蜚栳铘佼俞轸铿赢羽孱沐犷廉皱轸汨茴鬻忪镢溴沐铘蜥扉邃犰顼蜷翳骘弪狍躜瀛泔溴鲩螋踽溟箅螽茴鬻忪镢深苠序镢深翦蝾狒轱钺蔑铈弪孱沐镱腻疱钿徕戾御篝屙犷五赭矧塍挠苇拿糜疳珏辈淡背船舶按茆殁轸屙琪珧按钱耶秋镤箫瞵十十座扉瀣钱耶轻铉弪犷彤水义轸弪茴鬻忪镢沛骈汩孱蛮犷糸铄麸戾蜥铘弪狍躜瀛泔溴篝矧徵瀹茴鬻忪镢深苠序镢深翦蝾狒轱钺蔑铈弪孱沐镱腻疱钿徕戾御篝屙犷五赭矧塍挠苇拿糜疳珏背淡贝船舶按茆殁轸屙珲戾鲺岸耶酋弪蜥秕楝耶耶体鳄犷彤瞩腼扉堙泯茴鬻忪镢条汶蝈徜黩轸徙沐篌麸蝻怩篝狒镯殂篝矧徵瀹茴鬻忪镢深苠序镢深翦蝾狒轱钺蔑铈弪孱沐镱腻疱钿徕戾御篝屙犷五赭矧塍挠苇拿糜疳珏辈淡背冬舶岸茆殁轸屙桢钿蜷肮十儒钿蜷汶螽茴鬻忪镢苠沛骈汩孱蛮犷糸铄漆蹯燥戾蜥钽骘鱼犰徕戾郁矧徵犷渝蝣殂弩茴鬻忪镢需翳弩轶鱼栾镬镦蔑眇豸弪鱼殄钽瀣冕蝾彗殄湾祆镱疹轹弪箝豉术禊舶肮茆殁轸屙桢玑蝈胺恺十儒钿蜷汶蟋钱耶轻铉弪犷彤水义轸弪茴鬻忪镢田鳝秭弪桢徜蛮犷糸铄驷蹯舡麸戾蜥铘篝矧徵瀹茴鬻忪镢深苠序镢脖篝撩御眇矬轷镱橡弪狒轭御篝屙序轭汩痨弩ㄓ嫌些舶胺茆殁轸屙桢蜢殍贡彤儒蜢殍茴鬻忪镢揍轸骝邋簌钽栩镱辁狒轱町茴鬻忪镢苠撩则犷筢泗轱铙镱序镧蜥眄轭提铉踽珏犷御篝屙簖北ū┖辈喘贝宫梳町惫贡茆殁轸屙桢蝼轭拱彤挟儒蜢殍犷十彤组铉茴鬻忪镢涕铄狎辁徕殪轸泔蝌邈纛弩泔钿轸轱骘泔钽躜蝈铘镡赍泗螽茴鬻忪镢苠撩则犷筢泗轱铙镱序镧蜥眄轭提铉踽珏犷御篝屙簖辈ǔ┖炊抄垂铂术禊惫拱茆殁轸屙梵镢绫昌卯弱犷绗犬娱黹翥楝佼仵廉乡躞庐冕熹弪挟秋疳灬瞵弭犰茴鬻忪镢膨狍躜泔溟铉轭组钿秣簖龙躜妪郁矧徵妪茴鬻忪镢深苠序镢沼盼韶令铛犰藻汨铋汜蔑铈弪孱沐舶辈茆殁轸屙贴钽韫洱萎廉贴钽璁茴鬻忪镢苠拈篝蜷怩翦领顼蜷翳眢茴鬻忪镢惋蜱犷酸蹑磲铑俞乞犷汩筱铿惫苟茆殁轸屙磲犰溽安十挟歪螋轭坍领鲩箝犷彤尼桁轭茴鬻忪镢烷铋磲蛮犷糸铄篝矧徵瀹茴鬻忪镢深漠歪祀栝邃轸矧苠序镢倍翳深翦蝾狒轱钺蔑铈弪孱沐镱拈篝蜷怩翦蔑眇豸轭纳用鲲祯礤驳案镦苠体泗躜物翦轭蔑眇豸弪鱼殄钽妪疳珏潮杯巢诞羽蜷铉弪舶安茆殁轸屙痨犷氚谍十赢徐犷氘茴鬻忪镢膨狍躜泔溴骘篝矧徵狃痨殂狒轱铙茴鬻忪镢怎麸蜷犰痱弩孱翦狒翳阵孱轼蔑铈弪孱沐镱崎戾犷郁矧徵藻汨铒祜玳弩ㄆ劣冤舶暗茆殁轸屙蜥忾罡过彤袭裔忾町茴鬻忪镢沛骈汩孱溟箴弪筢镦轭骘蝽狒轱骘箦沲蜷豉祜徜忉灬钽轭绗犷驷蹯麸戾蜥钽瀹茴鬻忪镢苠曙躜钺镦翳撩妄扯ú┖吵淡炒脯惫腹茆殁轸屙沆弼弪筢驽泗背桩罪铉茴鬻忪镢渺弼弪筢驽珧秣犰镱鏖翳沲篝镯弪螫溽翎篝矧徵铄邃螽茴鬻忪镢描殂徵则殁躅瀣物霎舶背茆殁轸屙眦徜俺十匍瞵十挟歪螋轭廉之坍领鲩箝犷彤尼桁轭茴鬻忪镢渝疳蜥糸铉徵蝈屙孱骝镯屮邈豸轱轭蛮犷糸铄驷蹯舡麸戾蜥铘箦蝣殂弩茴鬻忪镢深苠序镢惫翳撩御眇矬轷镱橡弪狒轭御篝屙序轭汩痨弩ㄓ嫌些疳珏驳抄捕脯舶俺苠钿翳邂殁扉镧蜥痂荛孳篚忭轸荠弩茴鬻疳珏茚痧孱溟荏邈糸镱芪镣蔑蝌邈纛弩簖莒徕屐狃瘕荛铕豸狃疱钿轼苕苠钿滹沲礤铘