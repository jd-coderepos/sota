\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbm}
\usepackage{float}
\usepackage{framed}
\usepackage{enumerate}

\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[colorlinks=true,
            linkcolor=red,
            urlcolor=black,
            citecolor=gray]{hyperref}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\mean}{mean}
\DeclareMathOperator*{\rank}{rank}
\DeclareMathOperator*{\tr}{tr}
\let\ker\relax
\DeclareMathOperator*{\ker}{ker}

\newcommand{\wh}{\widehat}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\eqdef}{\mathbin{\stackrel{\rm def}{=}}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\argmin}{\arg\!\min}

\newcommand{\refine}{\texttt{RefineSparsifier}}
\newcommand{\maintain}{\texttt{MaintainSketches}}
\newcommand{\maintainma}{\texttt{MaintainMatrixSketches}}
\newcommand{\sample}{\texttt{RowSampleMatrix}}

\newcommand{\va}{\vec{a}}
\newcommand{\lap}{\mvar{{\mathcal L}}}
\newcommand{\nnz}{\mathrm{nnz}}
\newcommand{\plog}{\mathop\mathrm{polylog}}
\newcommand{\poly}{\mathop\mathrm{poly}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\mvar}[1]{\bv{#1}}
\newcommand{\ma}{\mvar{A}}
\newcommand{\mb}{\mvar{B}}
\newcommand{\md}{\mvar{D}}
\newcommand{\mf}{\mvar{F}}
\newcommand{\mk}{\mvar{K}}
\newcommand{\mj}{\mvar{J}}
\newcommand{\ms}{\mvar{S}}
\newcommand{\mw}{\mvar{W}}
\newcommand{\mpi}{\mvar{\Pi}}
\newcommand{\mproj}{\mvar{P}}
\newcommand{\mzero}{\mvar{0}}
\newcommand{\mone}{\mvar{1}}
\newcommand{\iMatrix}{\mvar{I}}
\newcommand{\otilde}{\tilde{\mathcal O}}
\newcommand{\onesVec}{\vec{1}}
\newcommand{\setVec}[1]{\onesVec_{#1}}
\newcommand{\indicVec}[1]{\onesVec_{#1}}

\newcommand{\dist}{\mathcal{D}}

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{\newenvironment{rep#1}[1]{\def\rep@title{#2 \ref{##1}}\begin{rep@theorem}}{\end{rep@theorem}}}
\makeatother


\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newreptheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newreptheorem{lemma}{Lemma}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{note}[lemma]{Note}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{definition}[theorem]{Definition}

\usepackage{nth}
  \usepackage{intcalc}

  \newcommand{\cSTOC}[1]{\nth{\intcalcSub{#1}{1968}}\ Annual\ ACM\ Symposium\ on\ Theory\ of\ Computing\ (STOC)}
  \newcommand{\cFSTTCS}[1]{\nth{\intcalcSub{#1}{1980}}\ International\ Conference\ on\ Foundations\ of\ Software\ Technology\ and\ Theoretical\ Computer\ Science\ (FSTTCS)}
  \newcommand{\cCCC}[1]{\nth{\intcalcSub{#1}{1985}}\ Annual\ IEEE\ Conference\ on\ Computational\ Complexity\ (CCC)}
  \newcommand{\cFOCS}[1]{\nth{\intcalcSub{#1}{1959}}\ Annual\ IEEE\ Symposium\ on\ Foundations\ of\ Computer\ Science\ (FOCS)}
  \newcommand{\cRANDOM}[1]{\nth{\intcalcSub{#1}{1996}}\ International\ Workshop\ on\ Randomization\ and\ Computation\ (RANDOM)}
  \newcommand{\cISSAC}[1]{#1\ International\ Symposium\ on\ Symbolic\ and\ Algebraic\ Computation\ (ISSAC)}
  \newcommand{\cICALP}[1]{\nth{\intcalcSub{#1}{1973}}\ International\ Colloquium\ on\ Automata,\ Languages and\ Programming\ (ICALP)}
  \newcommand{\cCOLT}[1]{\nth{\intcalcSub{#1}{1987}}\ Annual\ Conference\ on\ Computational\ Learning\ Theory\ (COLT)}
  \newcommand{\cCSR}[1]{\nth{\intcalcSub{#1}{2005}}\ International\ Computer\ Science\ Symposium\ in\ Russia\ (CSR)}
  \newcommand{\cMFCS}[1]{\nth{\intcalcSub{#1}{1975}}\ International\ Symposium\ on\ the\ Mathematical\ Foundations\ of\ Computer\ Science\ (MFCS)}
  \newcommand{\cPODS}[1]{\nth{\intcalcSub{#1}{1981}}\ Symposium\ on\ Principles\ of\ Database\ Systems\ (PODS)}
  \newcommand{\cSODA}[1]{\nth{\intcalcSub{#1}{1989}}\ Annual\ ACM-SIAM\ Symposium\ on\ Discrete\ Algorithms\ (SODA)}
  \newcommand{\cNIPS}[1]{Advances\ in\ Neural\ Information\ Processing\ Systems\ \intcalcSub{#1}{1987} (NIPS)}
  \newcommand{\cWALCOM}[1]{\nth{\intcalcSub{#1}{2006}}\ International\ Workshop\ on\ Algorithms\ and\ Computation\ (WALCOM)}
  \newcommand{\cSoCG}[1]{\nth{\intcalcSub{#1}{1984}}\ Annual\ Symposium\ on\ Computational\ Geometry\ (SCG)}
  \newcommand{\cKDD}[1]{\nth{\intcalcSub{#1}{1994}}\ ACM\ SIGKDD\ International\ Conference\ on\ Knowledge\ Discovery\ and\ Data\ Mining\ (KDD)}
  \newcommand{\cICML}[1]{\nth{\intcalcSub{#1}{1983}}\ International\ Conference\ on\ Machine\ Learning\ (ICML)}
  \newcommand{\cAISTATS}[1]{\nth{\intcalcSub{#1}{1997}}\ International\ Conference\ on\ Artificial\ Intelligence\ and\ Statistics\ (AISTATS)}
  \newcommand{\cITCS}[1]{\nth{\intcalcSub{#1}{2009}}\ Conference\ on\ Innovations\ in\ Theoretical\ Computer\ Science\ (ITCS)}
  \newcommand{\cPODC}[1]{{#1}\ ACM\ Symposium\ on\ Principles\ of\ Distributed\ Computing\ (PODC)}
  \newcommand{\cAPPROX}[1]{\nth{\intcalcSub{#1}{1997}}\ International\ Workshop\ on\ Approximation\ Algorithms\ for\  Combinatorial\ Optimization\ Problems\ (APPROX)}
  \newcommand{\cSTACS}[1]{\nth{\intcalcSub{#1}{1983}}\ International\ Symposium\ on\ Theoretical\ Aspects\ of\  Computer\ Science\ (STACS)}

  \newcommand{\pSTOC}[1]{Preliminary\ version\ in\ the\ \cSTOC{#1}}
  \newcommand{\pFSTTCS}[1]{Preliminary\ version\ in\ the\ \cFSTTCS{#1}}
  \newcommand{\pCCC}[1]{Preliminary\ version\ in\ the\ \cCCC{#1}}
  \newcommand{\pFOCS}[1]{Preliminary\ version\ in\ the\ \cFOCS{#1}}
  \newcommand{\pRANDOM}[1]{Preliminary\ version\ in\ the\ \cRANDOM{#1}}
  \newcommand{\pISSAC}[1]{Preliminary\ version\ in\ the\ \cISSAC{#1}}
  \newcommand{\pICALP}[1]{Preliminary\ version\ in\ the\ \cICALP{#1}}
  \newcommand{\pCOLT}[1]{Preliminary\ version\ in\ the\ \cCOLT{#1}}
  \newcommand{\pCSR}[1]{Preliminary\ version\ in\ the\ \cCSR{#1}}
  \newcommand{\pMFCS}[1]{Preliminary\ version\ in\ the\ \cMFCS{#1}}
  \newcommand{\pPODS}[1]{Preliminary\ version\ in\ the\ \cPODS{#1}}
  \newcommand{\pSODA}[1]{Preliminary\ version\ in\ the\ \cSODA{#1}}
  \newcommand{\pNIPS}[1]{Preliminary\ version\ in\ \cNIPS{#1}}
  \newcommand{\pWALCOM}[1]{Preliminary\ version\ in\ the\ \cWALCOM{#1}}
  \newcommand{\pSoCG}[1]{Preliminary\ version\ in\ the\ \cSoCG{#1}}
  \newcommand{\pKDD}[1]{Preliminary\ version\ in\ the\ \cKDD{#1}}
  \newcommand{\pICML}[1]{Preliminary\ version\ in\ the\ \cICML{#1}}
  \newcommand{\pAISTATS}[1]{Preliminary\ version\ in\ the\ \cAISTATS{#1}}
  \newcommand{\pITCS}[1]{Preliminary\ version\ in\ the\ \cITCS{#1}}
  \newcommand{\pPODC}[1]{Preliminary\ version\ in\ the\ \cPODC{#1}}
  \newcommand{\pAPPROX}[1]{Preliminary\ version\ in\ the\ \cAPPROX{#1}}
  \newcommand{\pSTACS}[1]{Preliminary\ version\ in\ the\ \cSTACS{#1}}


  \newcommand{\STOC}[1]{Proceedings\ of\ the\ \cSTOC{#1}}
  \newcommand{\FSTTCS}[1]{Proceedings\ of\ the\ \cFSTTCS{#1}}
  \newcommand{\CCC}[1]{Proceedings\ of\ the\ \cCCC{#1}}
  \newcommand{\FOCS}[1]{Proceedings\ of\ the\ \cFOCS{#1}}
  \newcommand{\RANDOM}[1]{Proceedings\ of\ the\ \cRANDOM{#1}}
  \newcommand{\ISSAC}[1]{Proceedings\ of\ the\ \cISSAC{#1}}
  \newcommand{\ICALP}[1]{Proceedings\ of\ the\ \cICALP{#1}}
  \newcommand{\COLT}[1]{Proceedings\ of\ the\ \cCOLT{#1}}
  \newcommand{\CSR}[1]{Proceedings\ of\ the\ \cCSR{#1}}
  \newcommand{\MFCS}[1]{Proceedings\ of\ the\ \cMFCS{#1}}
  \newcommand{\PODS}[1]{Proceedings\ of\ the\ \cPODS{#1}}
  \newcommand{\SODA}[1]{Proceedings\ of\ the\ \cSODA{#1}}
  \newcommand{\NIPS}[1]{\cNIPS{#1}}
  \newcommand{\WALCOM}[1]{Proceedings\ of\ the\ \cWALCOM{#1}}
  \newcommand{\SoCG}[1]{Proceedings\ of\ the\ \cSoCG{#1}}
  \newcommand{\KDD}[1]{Proceedings\ of\ the\ \cKDD{#1}}
  \newcommand{\ICML}[1]{Proceedings\ of\ the\ \cICML{#1}}
  \newcommand{\AISTATS}[1]{Proceedings\ of\ the\ \cAISTATS{#1}}
  \newcommand{\ITCS}[1]{Proceedings\ of\ the\ \cITCS{#1}}
  \newcommand{\PODC}[1]{Proceedings\ of\ the\ \cPODC{#1}}
  \newcommand{\APPROX}[1]{Proceedings\ of\ the\ \cAPPROX{#1}}
  \newcommand{\STACS}[1]{Proceedings\ of\ the\ \cSTACS{#1}}


  \newcommand{\arXiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
  \newcommand{\farXiv}[1]{Full\ version\ at\ \arXiv{#1}}
  \newcommand{\parXiv}[1]{Preliminary\ version\ at\ \arXiv{#1}}
  \newcommand{\CoRR}{Computing\ Research\ Repository\ (CoRR)}

  \newcommand{\cECCC}[2]{\href{http://eccc.hpi-web.de/report/20#1/#2/}{Electronic\ Colloquium\ on\ Computational\ Complexity\ (ECCC),\ Technical\ Report\ TR#1-#2}}
  \newcommand{\ECCC}{Electronic\ Colloquium\ on\ Computational\ Complexity\ (ECCC)}
  \newcommand{\fECCC}[2]{Full\ version\ in\ the\ \cECCC{#1}{#2}}
  \newcommand{\pECCC}[2]{Preliminary\ version\ in\ the\ \cECCC{#1}{#2}}
 
\title{Single Pass Spectral Sparsification in Dynamic Streams}

\author{\and\and
Michael Kapralov\\MIT\\ {kapralov@mit.edu}
\and
Yin Tat Lee\\MIT\\ {yintat@mit.edu}
\and
Cameron Musco\\MIT\\ {cnmusco@mit.edu}
\and\and
Christopher Musco\\MIT\\ {cpmusco@mit.edu}
\and
Aaron Sidford\\MIT\\ {sidford@mit.edu}
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
We present the first single pass algorithm for computing spectral sparsifiers of graphs in the dynamic semi-streaming model. Given a single pass over a stream containing insertions and deletions of edges to a graph , our algorithm maintains a randomized linear sketch of the incidence matrix of  into dimension .
Using this sketch, at any point, the algorithm can output a  spectral sparsifier for  with high probability. 

While  space algorithms are known for computing \emph{cut sparsifiers} in dynamic streams [AGM12b, GKP12] and spectral sparsifiers in \emph{insertion-only} streams [KL11], prior to our work, the best known single pass algorithm for maintaining spectral sparsifiers in dynamic streams required sketches  of dimension  [AGM13].

To achieve our result, we show that, using a coarse sparsifier of   and a linear sketch of 's incidence matrix, it is possible to sample edges by effective resistance,  obtaining a spectral sparsifier of arbitrary precision. Sampling from the sketch requires a novel application of  sparse recovery, a natural extension of the  methods used for cut sparsifiers in [AGM12b]. Recent work of [MP12] on row sampling for matrix approximation gives a recursive approach for obtaining the required coarse sparsifiers. 

Under certain restrictions, our approach also extends to the problem of maintaining a spectral approximation for a general matrix  given a stream of updates to rows in .

\end{abstract}
\thispagestyle{empty}


\clearpage
\setcounter{page}{1}

\section{Introduction}
\subsection{The Dynamic Semi-Streaming Model}
When processing massive graph datasets arising from social networks, web topologies, or interaction graphs, computation may be as limited by space as it is by runtime. To cope with this issue, one might hope to apply techniques
from the streaming model of computation, which restricts algorithms to few passes over the input and space polylogarithmic in the input size. Streaming algorithms
have been studied extensively in various application domains -- see \cite{muthukrishnan2005data} for an overview. However, the model has proven too restrictive for even the
simplest graph algorithms. For example, testing - connectivity requires
 space \cite{henz:lb}. 

The less restrictive semi-streaming model, in which the
algorithm is allowed  space, is more suited for graph
algorithms~\cite{feigenbaum2005graph}, and has received significant attention in recent years. In this model, a processor receives a stream of edges over a fixed set of  nodes.  Ideally, the processor should only have to perform a single pass (or few passes) over the edge stream, and the processing time per edge, as well as the time required to output the final answer, should be small.

In the \emph{dynamic semi-streaming model}, the graph stream may include both edge insertions and deletions \cite{linearMeasurement}. This extension captures the fact that large graphs are unlikely to be static. Dynamic semi-streaming algorithms allow us to quickly process general updates in the form of edge insertions and deletions to maintain a small-space representation of the graph from which we can later compute a result. Sometimes the dynamic model is referred to as the \emph{insertion-deletion model}, in contrast to the more restrictive \emph{insertion-only model}.

Work on semi-streaming algorithms in both the dynamic and insertion-only settings is extensive. Researchers have tackled connectivity, bipartiteness, minimum spanning trees, maximal matchings, and spanners among other problems \cite{feigenbaum2005graph, epstein2011improved, elkin2011streaming, linearMeasurement,gssss}. In \cite{gregorSurvey},  McGregor surveys much of this progress and provides a more complete list of citations.

\subsection{Streaming Sparsification}
There has also been a focus on computing \emph{general purpose} graph compressions in the streaming setting. The goal is to find a subgraph of an input graph  that has significantly fewer edges than , but still maintains important properties of the graph. Hopefully, this \emph{sparsified graph} can be used to approximately answer a variety of questions about  with reduced space and time complexity. Typically, the goal is to find a subgraph with just  edges in comparison to the possible  edges in . 

First introduced by Bencz{\'u}r and Karger \cite{benczur1996approximating}, a \emph{cut sparsifier} of a graph  is a weighted subgraph with only  edges that preserves the total edge weight over every cut in  to within a  multiplicative factor. Cut sparsifiers can be used to compute approximations for minimum cut, sparsest cut, maximum flow, and a variety of other problems over . In \cite{spielmanTengSpectralSparse}, Spielman and Teng introduce the stronger \emph{spectral sparsifier}, a weighted subgraph whose Laplacian spectrally approximates the Laplacian of . In addition to maintaining the cut approximation of Bencz{\'u}r and Karger, spectral sparsifiers can be used to approximately solve linear systems over the Laplacian of , and to approximate effective resistances, spectral clusterings, random walk properties, and a variety of other computations. 

\if Both cut and spectral sparsifiers are powerful primitives for the semi-streaming model since they are rich compressions of dense graphs that can be stored in  space. Streaming algorithms that allow us to extract a sparsifier from a graph immediately yield streaming algorithms for the many problems that sparsifiers allow us to approximate. 
\fi

The problem of computing graph sparsifiers in the semi-streaming model has received a lot of attention. Given just  space, the hope is to compute a sparsifier using barely more space than required to store the sparsifier, which will typically have  edges.  Ahn and Guha give the first single pass, insertion-only algorithm for cut sparsifiers \cite{ahnStreamingSparsification}. Kelner and Levin give a single pass, insertion-only algorithm for spectral sparsifiers \cite{kelner2011spectral}. Both algorithms store a sparse graph: edges are added as they are streamed in and, when the graph grows too large, it is resparsified. The construction is very clean, but inherently does not extend to the dynamic model since, to handle edge deletions, we need more information than just a sparsifier itself. Edges eliminated to create an intermediate sparsifier may become critically important later if other edges are deleted, so we need to maintain information that allows recovery of such edges.

Ahn, Guha, and McGregor make a very important insight in \cite{linearMeasurement}, demonstrating the power of linear graph sketches in the dynamic model. They present the first dynamic algorithm for cut sparsifiers, which initially required  space and  passes over the graph stream. However, the result was later improved to a single pass and  space \cite{gssss,goel2012single}. Our algorithm extends the sketching and sampling approaches from these papers to the spectral problem.

In \cite{ahn2013spectral}, the authors show that linear graph sketches that capture connectivity information can be used to coarsely approximate spectral properties and they obtain spectral sparsifiers using  space in the dynamic setting. However, they also show that their coarse approximations are tight, so a new approach is required to obtain spectral sparsifiers using just  space. They conjecture that a dynamic algorithm for doing so exists. The development of such an algorithm is also posed as an open question in \cite{gregorSurvey}. A two-pass algorithm for constructing a spectral sparsifier in the dynamic streaming model using  space is presented in~\cite{KW14}. The approach is very different from ours: it leverages a reduction from spanner constructions to spectral sparsification presented in~\cite{KP12}. It is not known if this approach extends to a space efficient single pass algorithm.

\subsection{Our Contribution}

Our main result is an algorithm for maintaining a small graph sketch from which we can recover a spectral sparsifier. For simplicity, we present the algorithm in the case of unweighted graphs. However, in Section \ref{weighted}, we show that it is easily extended to weighted graphs. This model matches what is standard for dynamic cut sparsifiers  \cite{gssss,goel2012single}. 

\begin{theorem}[Main Result]
\label{main_sparsification_theorem} There exists an algorithm that, for any , processes a  list of edge insertions and deletions for an unweighted graph  in a single pass and maintains a set of linear sketches of this input in  space. From these sketches, it is possible to recover, with high probability, a weighted subgraph  with  edges such that  is a  spectral sparsifier of . The algorithm 
recovers  in  time.
\end{theorem}

It is well known that independently sampling edges from a graph  according to their \emph{effective resistances} (i.e. leverage scores) gives a  spectral sparsifier of  with  edges \cite{graphSparsificationEffectiveResistance}. We can `refine' any coarse sparsifier for  by using it to approximate effective resistances and then resample edges according to these approximate resistances. We show how to perform this refinement in the streaming setting, extending graph sketching techniques initially used for cut sparsifiers (\cite{gssss,goel2012single}) and introducing a new sampling technique based on an  heavy hitters algorithm. Our refinement procedure is combined with a clever recursive method for obtaining a coarse sparsifier introduced by Miller and Peng in a recent paper on iterative row sampling for matrix approximation \cite{pengV1}.

\if 0
An interesting open question is whether or not cut and spectral sparsifiers can be computed in the turnstile model, which allows arbitrary increments and decrements to edge weights. This goal is complicated by the fact that such updates are nonlinear with respect to the edge-vertex incidence matrix of a graph, which contains \emph{square roots} of edge weights. Thus, they are unnatural for any sketching algorithm that, like our approach, compresses this incidence matrix (as opposed to working directly with the graph Laplacian).
\fi

The fact that our algorithm maintains a linear sketch of the streamed graph allows for the 
simple handling of edge deletions, which are treated as negative edge insertions. Additionally,  due to their linearity, our sketches are composable -- sketches of subgraphs can simply be added to produce a sketch of the full graph. Thus, our techniques are directly applicable in distributed settings where separate processors hold different subgraphs or each processes different edge substreams. 

Our application of linear sketching also gives a nice information theoretic result on graph compression. A spectral sparsifier is a powerful compression for a graph. It maintains, up to an  factor, all spectral information about the Laplacian using just  space. At first glance, it may seem that such a compression requires careful analysis of the input graph to determine what information to keep and what to discard. However, the non-adaptive linear sketches used in our algorithm are completely \emph{oblivious}: at each edge insertion or deletion, we do not need to examine the current compression at all to make the appropriate update. As in sparse recovery or dimensionality reduction, we essentially just multiply the vertex edge incidence matrix by a random projection matrix, decreasing its height drastically in the process. Nevertheless, the oblivious compression obtained holds as much information as a spectral sparsifier -- in fact, we show how to extract a spectral sparsifier from it! Furthermore, the compression is only larger than  by log factors. Our result is the first of this kind in the spectral domain. The only other streaming algorithm for spectral sparsification that uses  space is distinctly non-oblivious \cite{kelner2011spectral} and oblivious subspace embeddings for compressing general matrices inherently require  space, even when the matrix is sparse (as in the case of an edge vertex incidence matrix) \cite{sarlos2006improved,clarkson2013low, meng2013, osnap}.

Finally, it can be noted that our proofs rely very little on the fact that our data stream represents a graph. We show that, with a few modifications, given a stream of row updates for a general structured matrix , it is possible to maintain a  sized sketch from which a spectral approximation to  can be recovered. By structured, we mean any matrix whose rows are selected from some fixed dictionary of size . Spectral graph sparsification is a special case of this problem: set  to be the vertex edge incidence matrix of our graph. The dictionary is the set of all possible  edge rows that may appear in  and  is the graph Laplacian.  






\subsection{Road Map}
\begin{description}
\item[Section \ref{notation}] Lay out notation, build linear algebraic foundations for spectral sparsification, and present  lemmas for graph sampling and sparse recovery required by our algorithm.
\item[Section \ref{algorithm_overview}] Give an overview of our central algorithm, providing intuition and motivation.
\item[Section \ref{recursive_algorithm}] Present an algorithm of Miller and Peng (\cite{pengV1}) for building a chain of coarse sparsifiers and prove our main result, assuming a primitive for sampling edges by effective resistance in the streaming model.
\item[Section \ref{streaming_row_sampling}] Develop this sampling primitive, our main technical contribution.
\item[Section \ref{weighted}] Show how to extend the algorithm to weighted graphs.
\item[Section \ref{structured}] Show how to extend the algorithm to general structured matrices.
\item[Section \ref{pseudorandomness}] Remove our assumption of fully independent hash functions, using a pseudorandom number generator to achieve a final small space algorithm.
\end{description}
 \section{Notation and Preliminaries}\label{notation}

\subsection{Graph Notation}
Let  be the vertex edge incidence matrix of the undirected, unweighted complete graph over  vertices. , the row corresponding to edge  contains a  in column , a  in column , and 's elsewhere. 

We write the vertex edge incidence matrix of an unweighted, undirected graph  as  where  is an  diagonal matrix with ones at positions corresponding to edges contained in  and zeros elsewhere.\footnote{Typically rows of  that are all  are removed, but we find this formulation more convenient for our purposes.}
The  Laplacian matrix of  is given by  . 



\subsection{Spectral Sparsification}
For any matrix ,  is a  spectral sparsifier of  if, , . This condition can also be written as  where  indicates that  is positive semidefinite. More succinctly,  denotes the same condition. 
We also use the slightly weaker notation  to indicate that  for all  in the \emph{row span} of . If  has the same row span as  this notation is equivalent to the initial notion of spectral sparsification.

While these definitions apply to general matrices, for our purposes,  is typically the vertex edge incidence matrix of a graph  and  is a graph Laplacian. We do not always require our approximation  to be the graph Laplacian of a weighted subgraph, which is a standard assumption. For this reason, we avoid the standard  notation for the Laplacian.  For our purposes,  is always be a sparse symmetric diagonally dominant matrix with no more than  non-zero entries. In fact, it will always be the Laplacian of a sparse subgraph, but possibly with weight added to its diagonal entries. Furthermore, the final approximation returned by our streaming algorithm will be a bonafide spectral graph sparsifier -- i.e. the Laplacian matrix of a weighted subgraph of .

\subsection{Leverage Scores and Row Sampling}
\label{Leverage Scores and Row Sampling}
For any  with rank , consider the reduced singular value decomposition, .  and  have orthonormal columns and  is diagonal and contains the non-zero singular values of . Then, . We let  denote the Moore-Penrose pseudoinverse of :

 The leverage score, , for a row  in  is defined as

The last inequality follows from the fact that every row in a matrix with orthonormal columns has norm less than 1. In a graph, , where  is the \emph{effective resistance} of edge  and  is the edge's weight. Furthermore,




It is well known that by sampling the rows of  according to their leverage scores it is possible to obtain a matrix  such that  with high probability. Furthermore, if obtaining exact leverage scores is computationally difficult, it suffices to sample by upper bounds on the scores. Typically, rows are sampled \emph{with replacement} with probability proportional to their leverage score \cite{graphSparsificationEffectiveResistance,pengV2}. We require an alternative procedure for sampling edges \emph{independently}.



\begin{lemma}[Spectral Approximation via Leverage Score Sampling]\label{sparsifier_sampling}
Let  be a vector of leverage score overestimates for 's rows such that  for all . For  and fixed constant , define the sampling probability for row  to be . Define a diagonal sampling matrix  with  with probability  and  otherwise. With high probability,

Furthermore,  has  non-zeros with high probability. \end{lemma}
A proof of Lemma \ref{sparsifier_sampling} based on a matrix concentration result from \cite{tropp2012user} can be found in \cite{uniformSampling} (Lemma 4). Note that, when applied to the vertex edge incidence matrix of a graph, leverage score sampling is equivalent to effective resistance sampling, as introduced in \cite{graphSparsificationEffectiveResistance} for graph sparsification.

\subsection{Sparse Recovery}
\label{sparse recovery}
While we cannot sample by leverage score directly in the streaming model, we can use a sparse recovery primitive to sample edges from a set of linear sketches. We use an  heavy hitters algorithm that, for any vector , lets us recover from a small linear sketch , the index  and the approximate value of  for all  such that .

\begin{lemma}[ Heavy Hitters]
\label{sparse_recovery_primitive}
For any , there is a decoding algorithm  and a distribution on matrices  in  such that, for any , given , the algorithm  returns a vector  such that
 has  non-zeros and satisfies

with probability  over the choice of .
The sketch  can be maintained and decoded in  space. 
\end{lemma}
This procedure allows us to distinguish from a sketch whether or not a specified entry in  is equal to 0 or has value . 
We give a proof of Lemma \ref{sparse_recovery_primitive} in Appendix \ref{sparse_recovery_appendix}
 
\section{Algorithm Overview}\label{algorithm_overview}
Before formally presenting a proof of our main result, Theorem \ref{main_sparsification_theorem}, we give an informal overview of the algorithm to provide intuition. 

\subsection{Effective Resistances}
As explained in Section \ref{Leverage Scores and Row Sampling}, spectral sparsifiers can be generated by sampling edges, i.e. rows of the vertex edge incidence matrix. For an unweighted graph , each edge  is sampled independently with probability proportional to its leverage score, . After sampling, we reweight and combine any sampled edges. The result is a subgraph of  containing, with high probability,  edges and spectrally approximating .

If we view  as an electrical circuit, with each edge representing a unit resistor, the leverage score of an edge  is equivalent  to its effective resistance. This value can be computed by forcing  unit of current out of vertex  and  unit of current into vertex . The resulting voltage difference between the two vertices is the effective resistance of . Qualitatively, if the voltage drop is low, there are many low resistance (i.e. short) paths between  and . Thus, maintaining a direct connection between these vertices is less critical in approximating , so  is less likely to be sampled.
Effective resistance can be computed as:



Note that  can be computed for any pair of vertices, , or in other words, for any possible edge in . We can evaluate   even if  is not present in the graph. Thus, we can reframe our sampling procedure. Instead of just sampling edges actually in , imagine we run a sampling procedure for \emph{every possible} . When recombining edges to form a spectral sparsifier, we separately check whether each edge  is in  and only insert into the sparsifier if it is. 
\subsection{Sampling in the Streaming Model}
With this procedure in mind, a sampling method that works in the streaming setting requires two components. First, we need to obtain a constant factor approximation to  for any . Known sampling algorithms, including our Lemma \ref{sparsifier_sampling}, are robust to this level of estimation. Second, we need to compress our edge insertions and deletions in such a way that, during post-processing of our sketch, we can determine whether or not a sampled edge  actually exists in . 

The first requirement is achieved through the recursive procedure given in \cite{pengV1}. We will give the overview shortly but, for now, assume that we have access to a coarse sparsifier, . Computing  gives a 2 factor multiplicative  approximation of  for each . Furthermore, as long as  has sparsity , the computation can be done in small space using an iterative system solver (e.g. conjugate gradient) or a nearly linear time solver for symmetric diagonally dominant matrices (e.g. \cite{koutis2011nearly}).

Solving part two (determining which edges are actually in ) is a bit more involved.
As a first step, consider writing

Referring to Section \ref{notation}, recall that  is exactly the same as a standard vertex edge incidence matrix except that rows in  corresponding to nonexistent edges are zeroed out instead of removed. Denote . Each nonzero entry in  contains the voltage difference across some edge (resistor) in  when one unit of current is forced from  to . 

When  is not in , then the  entry of ,   is . If  is in , . Furthermore, . 
Given a space allowance of , the sparse recovery algorithm from Lemma \ref{sparse_recovery_primitive} allows us to recover an entry if it accounts for at least an  fraction of the total  norm. Currently, , which could be much smaller than . However, suppose we had a sketch of  with all but a  fraction of edges randomly sampled out. Then, we would expect  and
thus,  and sparse recovery would successfully indicate whether or not . What's more, randomly zeroing out entries of  can serve as our main sampling routine for edge . This process will set  with probability , exactly what we wanted to sample by in the first place!

However, how do we go about sketching every appropriately sampled ? Well, consider subsampling our graph at geometrically decreasing rates,  for . Maintain linear sketches  of the vertex edge incidence matrix for every subsampled graph using the  sparse recovery sketch distribution from Lemma \ref{sparse_recovery_primitive}.
When asked to output a spectral sparsifier, for every possible edge , we compute using  a rate  that approximates . 

Since each sketch is linear, 
we can just multiply  on the right by  to compute

where  is  sampled at rate . 
Then, as explained, we can use our sparse recovery routine to determine whether or not  is present. If it is, we have obtained a sample for our spectral sparsifier!

\subsection{A Chain of Coarse Sparsifiers}
The final required component is access to some sparse . This coarse sparsifier is obtained recursively by constructing a chain of matrices,  each weakly approximating the next. Specifically, imagine producing  by adding a fairly light identity matrix to . As long as the identity's weight is small compared to 's spectrum,  approximates . Add even more weight to the diagonal to form . Again, as long as the increase is small,  approximates . We continue down the chain until , which will actually have a heavy diagonal after all the incremental increases. Thus,  can be approximated by an appropriately scaled identity matrix, which is clearly sparse. Miller and Peng show that parameters can be chosen such that  \cite{pengV1}.

Putting everything together, we maintain  sketches for . We first use a weighted identity matrix as a coarse approximation for , which allows us to recover a good approximation to  from our sketch. This approximation will in turn be a coarse approximation for , so we can recover a good sparsifier of . Continuing up the chain, we eventually recover a good sparsifier for our final matrix, . 











\section{Recursive Sparsifier Construction}\label{recursive_algorithm}
In this section, we formalize a recursive procedure for obtaining a chain of coarse sparsifiers that was introduced by Miller and Peng  -- ``Introduction and Removal of Artificial Bases'' \cite{pengV1}. We prove Theorem \ref{main_sparsification_theorem} by combining this technique with the sampling algorithm developed in Section \ref{streaming_row_sampling}.



\begin{theorem}[Recursive Sparsification -- \cite{pengV1}, Section 4]
\label{miller_peng_chain}
Consider any PSD matrix  with maximum eigenvalue bounded from above by  and minimum non-zero eigenvalue bounded from below by . Let .  For , define

So,  and . Then the chain of PSD matrices,  with

satisfies the following relations:
\begin{enumerate}
  \item ,
  \item  for all ,
  \item .
\end{enumerate}
When  is the Laplacian of an unweighted graph, its largest eigenvalue  and its smallest non-zero eigenvalue . Thus the length of our chain, , is .
\end{theorem}

For completeness, we include a proof of Theorem \ref{miller_peng_chain} in Appendix \ref{miller_peng_appendix}.  Now, to prove our main result, we need to state the sampling primitive for streams that we develop in Section \ref{streaming_row_sampling}. This procedure maintains a linear sketch of a vertex edge incidence matrix , and using a coarse sparsifier of , performs independent edge sampling as required by Lemma \ref{sparsifier_sampling}, to obtain a better sparsifier of .










\begin{theorem}\label{refinement}
Let  be the vertex edge incidence matrix of an unweighted graph , specified by an insertion-deletion graph stream. Let  be a fixed parameter and consider . For any , there exists a sketching procedure  that outputs an  sized sketch . There exists a corresponding recovery algorithm \texttt{RefineSparsifier} running in  space, such that,
if  is a spectral approximation to  with  non-zeros and  for some constant  then:\\

  returns, with high probability, , where , and  contains only  reweighted rows of  with high probability.
 runs in  time.
\end{theorem}

Using this sampling procedure, we can initially set  and use it obtain a sparsifier for  from a linear sketch of . This sparsifier is then used on a second sketch of  to obtain a sparsifier for , and so on. Working up the chain, we eventually obtain a sparsifier for our original . While sparsifier recovery proceeds in several levels, we construct all required sketches in a \emph{single pass} over edge insertions and deletions. Recovery is performed in post-processing.

\begin{proof}[Proof of Theorem \ref{main_sparsification_theorem}]
Let  be the Laplacian of our graph . Process all edge insertions and deletions, using  to produce a sketch,  for each .
We then use Theorem \ref{refinement} to recover an  approximation, , for any   given an  approximation for . First, consider the base case, . Let:

By Theorem \ref{miller_peng_chain}, Relation 3:

Thus, with high probability,  and  contains  entries.

Now, consider the inductive case. Suppose we have some  such that . Let:

By Theorem \ref{miller_peng_chain}, Relation 2:

Furthermore, by assumption we have the inequalities:

Thus:

So, with high probability  returns  such that  and  contains just  nonzero elements. It is important to note that there is no ``compounding of error'' in this process. Every  is an  approximation for . Error from using  instead of  is absorbed by a constant factor increase in the number of rows sampled from . The corresponding increase in sparsity for  does not compound -- in fact Theorem \ref{refinement} is completely agnostic to the sparsity of the coarse approximation  used.

Finally, to obtain a bonafide graph sparsifier (a weighted subgraph of our streamed graph), let:

As in the inductive case,

Thus, it follows that, with high probability,  has sparsity  and . Since we set  to 0 for this final step,  simply equals  for some  that contains reweighted rows of . Any vector in the kernel of  is in the kernel of , and thus any vector in the kernel of  is in the kernel of . Thus, we can strengthen our approximation to:

We conclude that  is the Laplacian of some graph  containing  reweighted edges and approximating  spectrally to precision . Finally, note that we require  recovery steps, each running in  time. Thus, our total recovery time is .
\end{proof}

\section{Streaming Row Sampling}\label{streaming_row_sampling}



In this section, we develop the sparsifier refinement routine required for Theorem \ref{main_sparsification_theorem}.

\begin{proof}[Proof of Theorem \ref{refinement}]

Outside of the streaming model, given full access to  rather than just a sketch  it is easy to implement  via leverage score sampling. Letting  denote appending the rows of one matrix to another, we can define , so . Since  and , for any row of  we have


Let  be the leverage score of  approximated using . Let  be the vector of approximate leverage scores, with the leverage scores of the  rows corresponding to  rounded up to . While not strictly necessary, including rows of the identity with probability  will simplify our analysis in the streaming setting. Using this  in Lemma \ref{sparsifier_sampling}, we can obtain  with high probability. Since , we can write  , where  contains  reweighted rows of  with high probability.

The challenge in the semi-streaming setting is actually sampling edges given only a sketch of . The general idea is explained in Section \ref{algorithm_overview}, with detailed pseudocode included below. 
\begin{framed}{\noindent\bfseries Streaming Sparsifier Refinement}


\paragraph{:}
\begin{enumerate}

\item For  let  be a uniform hash function. Let  be  with all rows except those with  zeroed out. So  is  with rows sampled independently at rate .  is simply .  

\item Maintain sketchs  where  are drawn from the distribution from Lemma \ref{sparse_recovery_primitive} with .
\item Output all of these sketches stacked: .

\end{enumerate}

\paragraph{:}
\begin{enumerate}
\item Compute  for each .

\item For every edge  in the set of  possible edges:
\begin{enumerate}
\item Compute  and , where  is the oversampling constant from Lemma \ref{sparsifier_sampling}. Choose  such that .

\item Compute  and run the heavy hitters algorithm of Lemma \ref{sparse_recovery_primitive}. Determine whether or not  or  by checking whether the returned . 


\item If it is determined that  set .

\end{enumerate}

\item Output .
\end{enumerate}
\end{framed}

We show that every required computation can be performed in the dynamic semi-streaming model and then prove the correctness of the sampling procedure. 
\subsubsection*{Implementation in the Semi-Streaming Model.}
Assuming access to uniform hash functions,  requires  space in total and can be implemented in the dynamic streaming model. When an edge insertion comes in, use  to compute which 's should contain the inserted edge, and update the corresponding sketches. For an edge deletion, simply update the sketches to add  to each appropriate .

Unfortunately, storing  uniform hash functions over  requires  space, and is thus impossible in the semi-streaming setting. If Section \ref{pseudorandomness} we show how to cope with this issue by using a small-seed pseudorandom number generator.


Step 1 of  can also be implemented in  space. Since  has  non-zeros and  has  rows, computing  requires  linear system solves in . We can use an iterative algorithm or a nearly linear time solver for symmetric diagonally dominant matrices to find solutions in  space total.

For step 2(a), the  chosen to guarantee  could in theory be larger than the index of the last sketch  maintained. However, if we take  samplings, our last will be empty with high probability. Accordingly, all samplings for higher values of  can be considered empty as well and we can just skip steps 2(b) and 2(c) for such values of . Thus,  sampling levels are sufficient.


Finally, by our requirement that  is able to compute  factor leverage score approximations, with high probability, Step 2 samples at most  edges in total (in addition to selecting  identity edges). Thus, the procedure's output can be stored in small space.

\subsubsection*{Correctness}

To apply our sampling lemma, we need to show that, with high probability,  independently samples each row of  with probability  where . Since the algorithm samples the rows of  with probability , and since  for all , by Lemma \ref{sparsifier_sampling}, with high probability,  is a  spectral sparsifier for . Furthermore,  contains  reweighted rows of .

In , an edge is only included in  if it is included in the  where


The probability that  is included in the sampled matrix  is simply , and sampling is done independently using uniform hash functions. So, we just need to show that, with high probability, any  included in its respective  is recovered by Step 2(b).

Let  and . 
As explained in Section \ref{algorithm_overview},
 
Furthermore, we can compute:
 
Now, writing , we \emph{expect}  to equal . We want to argue that the norm falls close to this value with high probability. This follows from claiming that no entry in  is too large.
For any edge  define:


\begin{lemma}\label{leverage_score_bound}\end{lemma}.
\begin{proof}



Consider . 
Let  and . If we have  then


which implies  as desired.


Now,  is a weighted graph Laplacian added to a weighted identity matrix. Thus it is full rank and diagonally dominant. Since it has full rank, . Since  is diagonally dominant and since  is zero everywhere except at  and , it must be that   is the maximum value of  and  is the minimum value. So  and .

\end{proof}
From Lemma \ref{leverage_score_bound}, the vector  has all entries (and thus all squared entries) in  so we can apply a Chernoff/Hoeffding bound to show concentration for . Specifically, we use the standard multiplicative bound \cite{Hoeffding:1963}:

Since

we can set  and conclude that
 
Accordingly,  with high probability for some constant  and .

Now, if , then our sparse recovery routine must return an estimated value for  that is . We set , so with high probability, the returned value is . On the other hand, if  is non-zero, it equals , so our sparse recovery sketch must return a value greater than . Therefore, as long as we set  high enough, we can distinguish between both cases by simply checking whether or not the return value is , as described for Step 2.

Thus, as long as  concentrates as described, our procedure recovers  if and only if  is included in . As explained, this ensures that our process is exactly equivalent to independent sampling. Since concentration holds with probability , we can adjust constants and union bound over all  possible edges to claim that our algorithm returns the desired  with high probability.

\end{proof}





\section{Sparsification of Weighted Graphs}\label{weighted}

We can use a standard technique to extend our result to streams of weighted graphs in which an edge's weight is specified at deletion, matching what is known for cut sparsifiers in the dynamic streaming model \cite{gssss,goel2012single}. Assume that all edge weights and the desired approximation factor  are polynomial in , then we can consider the binary representation of each edge's weight out to  bits. For each bit of precision, we maintain a separate unweighted graph . We add each edge to the graphs corresponding to bits with value one in its binary representation. When an edge is deleted, its weight is specified, so we can delete it from these same graphs. Since G = , given a  sparsifier  for each  we have:

So  is a spectral sparsifier for , the Laplacian of the weighted graph .

\section{Sparsification of Structured Matrices}\label{structured}

Next, we extend our algorithm to sparsify certain general quadratic forms in addition to graph Laplacians. There were only three places in our analysis where we used that  was not an arbitrary matrix. First, we needed that , where  is the vertex edge incidence matrix of the unweighted complete graph on  vertices. In other words, we assumed that we had some dictionary matrix  whose rows encompass every possible row that could arrive in the data stream. In addition to this dictionary assumption, we needed  to be sparse and to have a bounded condition number in order to achieve our small space results. These conditions allow our compression to avoid an  lower bound for approximately solving regression on general  matrices in the streaming model \cite{clarkson2009numerical}.

As such, to handle the general `structured matrix' case, we assume that we have some dictionary  containing  rows . We assume that . In the dynamic streaming model we receive insertions and deletions of rows from  resulting in a matrix  where  is a diagonal matrix such that  for all . Our goal is to recover from an  space compression a diagonal matrix  with at most  nonzero entries such that . Formally, we prove the following:

\begin{theorem}[Streaming Structured Matrix Sparsification] \label{main_structured_theorem}
Given a row dictionary  containing all possible rows of the matrix , there
exists an algorithm that, for any , processes a stream
of row insertions and deletions for  in a single
pass and maintains a set of linear sketches of this input in 
space where  is an upper bound on the condition number of . From these
sketches, it is possible to recover, with high probability, a matrix
 such that 
contains only  reweighted rows of 
and  is a  spectral
sparsifier of . The algorithm recovers  in  time.
\end{theorem}
Note that, when , the sketch space is .
To prove Theorem \ref{main_structured_theorem}, we need to introduce a more complicated sampling procedure than what was used for the graph case. In Lemma \ref{leverage_score_bound}, for the correctness proof of  in Section \ref{streaming_row_sampling}, we relied on the structure of our graph Laplacian and vertex edge incidence matrix to show that . This allowed us to show that the norm of a sampled  concentrates around its mean. Thus, we could recover edge  with high probability if it was in fact included in the sampling . Unfortunately, when processing general matrices,  is not necessarily the largest element  and the concentration argument fails.

We overcome this problem by modifying our algorithm to compute more sketches. Rather than computing a single , for every sampling rate , we compute  sketches of different samplings of  at rate . Each sampling is fully independent from the \emph{all} others, including those at the same and different rates. This differs from the graph case, where  was always a subsampling of  (for ease of exposition). Our modified set up lets us show that, with high probability, the norm of  is close to its expectation for at least a  fraction of the independent samplings for rate . We  can recover row  if it is present in one of the `good' samplings. 

Ultimately, we argue, in a similar manner to \cite{KP12}, that we can sample rows according to some distribution that is close to the distribution obtained by independently sampling rows according to leverage score. Using this primitive, we can proceed as in the previous sections to prove Theorem~\ref{main_structured_theorem}. In Section~\ref{sec:sub:gen:row}, we provide the row sampling subroutine and in Section~\ref{sec:sub:gen:sparse}, we show how to use this sampling routine to prove Theorem~\ref{main_structured_theorem}. 


\subsection{Generalized Row Sampling}
\label{sec:sub:gen:row}
Our leverage score sampling algorithm for the streaming model is as follows:


\newcommand{\strlev}{\tilde{\tau}}
\newcommand{\vxst}{\bv{x}_s^{(t)}}
\newcommand{\vxsit}{\bv{x}_{s_i}^{(t)}}
\newcommand{\vxsiti}{\bv{x}_{s_i}^{(t_i)}}
\newcommand{\xsit}{\bv{x}_{s_i}^{(t)}}
\newcommand{\xsiti}{\bv{x}_{s_i}^{(t_i)}}

\begin{framed}{\noindent\bfseries Streaming Row Sampling Algorithm} 

\paragraph{:}
\begin{enumerate}

\item Let , , and for all  and  let  be a diagonal matrix with  independently with probability  and is  otherwise.\footnotemark
\item For all  and  maintain sketch  where each  is drawn independently from the distribution in Lemma~\ref{sparse_recovery_primitive} with  and .
\item Add rows of , independently sampled at rate  , to each sketch.

\end{enumerate}

\paragraph{:}
\begin{enumerate}
\item For all  and  let  and compute .

\item For every :
\begin{enumerate}[(a)]
\item Compute  and , where  is the oversampling constant from Lemma \ref{sparsifier_sampling}. Choose  such that .

\item Pick  uniformly at random and use Lemma~\ref{sparse_recovery_primitive} to check if .

\item If  is recovered, add row  to the set of sampled edges with weight .
\end{enumerate}

\end{enumerate}

\end{framed}\footnotetext{Throughout this section, for  we let }


We claim that, with high probability, the set of edges returned by the above algorithm is a random variable that is stochastically dominated by the two random variables obtained by sampling edges independently at rates  and , respectively.

The following property of PSD matrices is used in our proof of correctness:
\begin{lemma}
\label{lem:gen:psd_fact}
For any symmetric PSD matrix  and indices  we have

\end{lemma}

\begin{proof}
Let  be the vector with a  at position  and s else where. For all  by the fact that  is PSD we have that

Expanding, we have that:

yielding the result.
\end{proof}

We can now proceed to prove that our sampling procedure approximates sampling the rows of  by their leverage scores.
\begin{lemma}\label{matrix_dominance} Consider an execution of  where
\begin{itemize}
  \item  for , and
  \item .
\end{itemize}
Let  be a random variable for the indices returned by . Let  denote the indices of the nonzero rows of  and let  and  be random variables for the subset of  obtained by including each  independently with probability

With high probability, i.e. except for a  fraction of the probability space,  is stochastically dominated by  and  stochastically dominates  with respect to set inclusion.
\end{lemma}

\begin{proof}
By definition,  and  are always subsets of  and  is a subset of  with high probability (it is a subset as long as the algorithm of Lemma~\ref{sparse_recovery_primitive} succeeds). Thus it remains to show that, with high probability for each  ,

    Furthermore, by definition, with high probability,  outputs  if and only if  and consequently

As shown in Equation \ref{entry_is_lev_score}, when proving our graph sampling Lemma, for all ,

Consequently, by the definition of  we can rewrite \eqref{eq:gen:sample:1} as:

From \eqref{eq:gen:sample:2} and the independence of  we obtain the following trivial upper bound on ,

and consequently  is stochastically dominated by  as desired.

As shown in Equation \ref{real_eqn}, when proving the graph sampling case, for all  and 

Recalling that , combining \eqref{eq:gen:sample:2} and \eqref{eq:gen:sample:3} yields:

where .

To bound the probability that  we break the contribution to  for each  into two parts. For all  we let , i.e. the set of all rows  which we attempt to recover at the same sampling rate as . For any , we let  and  . Using this notation and  we obtain the following lower bound

For all , the rows that we attempt to recover at the same rate as row , we know that . By Lemma~\ref{lem:gen:psd_fact} we know that for all  with  and 


Now recall that . If  and therefore  then  and setting constants high enough and considering , we see that row  is output with high probability. On the other hand if , then by \eqref{eq:gen:sample:6} and Chernoff bound choosing a sufficiently large constant we can ensure that with high probability  for all  and . 

Furthermore, by \eqref{eq:gen:sample:3} and Markov bound we know that . Therefore, by Chernoff bound, with high probability for each  with  for at least a  fraction of the values of  we have . However, note that by construction all the  are mutually independent of the  and the values of  for . So,  is simply picking each row  with probability  (failing with only a  probability) or not being able to recover each edge independently with some probability at most  -- the probability that  is too large. Consequently,  except for a negligible fraction of the probability space we have that

and we have the desired result.
\end{proof}

\subsection{Generalized Recursive Sparsification}
\label{sec:sub:gen:sparse}

Next we show how to construct a spectral sparsifier in the streaming model for a general
structured matrix using the row sampling subroutine, .
In the graph case, Theorem \ref{main_sparsification_theorem} shows
that, if we can find a sparsifier to a graph  using a coarse sparsifier,
then we can use the chain of spectrally similar graphs provided
in Theorem \ref{miller_peng_chain} to find a final  sparsifier for our input graph.

The proof of Theorem \ref{main_sparsification_theorem} includes our third reliance on the fact that we are sparsifying graphs -- we claim that the condition number of an unweighted
graph is polynomial in . This fact does not hold in the general matrix case since the condition number can be exponentially large even for bounded integer matrices. Therefore,
our result for general matrix depends on the condition number of . 


\begin{theorem}\label{refinement_unweighted_matrix} Given a row
dictionary . Let 
be the matrix specified by an insertion-deletion stream where 
is a diagonal matrix such that  for all . Let  be a given upper bound on the possible condition number of any .
Let  be a fixed parameter and consider .
For any , there exists a sketching procedure  that outputs an  sized sketch . There exists a corresponding recovery algorithm \texttt{RefineMatrixSparsifier} such that if  for some  then:\\



returns, with high probability, ,
where ,
and  contains only 
reweighted rows of  with high probability. \end{theorem}

\begin{proof} As in the graph case, we can think of the identity  as a set of rows that we sample with probability
. Hence, we have .

Lemma \ref{matrix_dominance} shows that 
returns a random set of indices of  such that the generated random variable is
dominated by  and is stochastically dominates . Recall that   and  are random variables for the subset
of  obtained by including each  independently
with probability 

Since  is a constant factor approximation of
leverages score, Lemma \ref{sparsifier_sampling} shows that sampling and reweighing the rows according
to  gives a spectral
sparsifier of  with the guarantee required. Similarly, sampling
according to  gives a sparsifier. Since the indices
returned by \textbf{} are sandwiched
between two processes which each give spectral sparsifiers, sampling according
to  gives
the required spectral sparsifier \cite{KP12}.
\end{proof}

Using \texttt{RefineMatrixSparsifier}, the arguments in Theorem \ref{main_sparsification_theorem}
yield Theorem \ref{main_structured_theorem}. Our sketch size needs to be based on  for two reasons -- we must subsample the matrix at  different rates as our leverage scores will be lower bounded by some . Further the chain of recursive sparsifiers presented in Theorem \ref{miller_peng_chain} will have length . Recovery will run in time . Space usage will depend on the sparsity of the rows in  as we will need enough space to solve linear systems in . In the worst case, this will require  space, however, if the row of  are sparse, and hence  is sparse, recovery will take less space, specifically  with constant row sparsity. 
\section{Using a Pseudorandom Number Generator}\label{pseudorandomness}
In the proof of our sketching algorithm, Theorem \ref{refinement}, we assume that  has access to  uniform random hash functions,  mapping every edge to . These functions are used to subsample our vertex edge incidence matrix, , at geometrically decreasing rates. Storing the functions as described would require  space - we need  random bits for each possible edge. 

To achieve  space, we need to compress the hash functions using Nisan's pseudorandom number generator. Our approach follows an argument in \cite{gssss} (Section 3.4) that was originally introduced in \cite{indyk2000stable} (Section 3.3). First, we summarize the pseudorandom number generator from \cite{nisan1992pseudorandom}

\begin{theorem}[Corollary 1 in \cite{nisan1992pseudorandom}]\label{pseudorandomPrimitive}
Any randomized algorithm running in  and using  random bits may be converted to one that uses only  random bits (and runs in space ).
\end{theorem}
\cite{nisan1992pseudorandom} gives this conversion explicitly by describing a method for generating  pseudorandom bits from  truly random bits. For any algorithm running in , the pseudorandom bits are ``good enough'' in that the output distribution of the algorithm under pseudorandom bits is very close to the output distribution under truly random bits. In particular, the total variation distance between the distributions is at worst  (see Lemma 3 in \cite{nisan1992pseudorandom}). It follows that using pseudorandom bits increases the failure probability of any randomized algorithm by just  in the worst case.

As described, our algorithm runs in  space and it is not immediately obvious how to use Theorem \ref{pseudorandomPrimitive} to reduce this requirement.
However, consider the following: suppose our algorithm is used on a sorted edge stream where all insertions and deletions for a single edge come in consecutively. In this case, at any given time, we only need to store one random bit for each hash function, which requires just  space. The random bits can be discarded after moving on to the next edge. Thus, the entire algorithm can run in  space. Then, we can apply Theorem \ref{pseudorandomPrimitive}, using the pseudorandom generator to get all of our required random bits by expanding just  truly random bits. 
Since our failure probability increases by at most , we still only fail with probability inverse polynomial in . 

Now notice that, since our algorithm is sketch based, edge updates simply require an addition to or subtraction from a sketch matrix. These operations commute, so our output will not differ if we reorder of the insertion/deletion stream. Thus, we can run our algorithm on a general edge stream, using the pseudorandom number generator to generate any of the required  bits as they are needed and operating in only  space.

Each time an edge is streamed in, we need to generate  random bits from the pseudorandom generator. This can be done in  time \cite{indyk2000stable}, which dominates the runtime required to process each streaming update.

Finally, Section \ref{structured} uses a slightly different sampling scheme for general structured matrices. Instead of building a sequence of subsampled matrices, the row dictionary is sampled independently at each level. In total, the required number of random bits is , where  is the number of rows in the dictionary . We require that , in which case the arguments above apply unmodified for the general matrix case.







\section{Acknowledgements}
We would like to thank Richard Peng for pointing us to the recursive row sampling algorithm contained in \cite{pengV1}, which became a critical component of our streaming algorithm. 
We would also like to thank Jonathan Kelner for useful discussions and Jelani Nelson for a helpful initial conversation on oblivious graph compression.

This work was partially supported by NSF awards 0843915, 1111109, and 0835652, CCF-1065125, CCF-AF-0937274,  CCF-0939370,  and CCF-1217506, NSF Graduate Research Fellowship grant 1122374, Hong Kong RGC grant 2150701, AFOSR grants FA9550-13-1-0042 and FA9550-12-1-0411, MADALGO center, Simons Foundation, and the Defense Advanced Research Projects Agency (DARPA).

\bibliography{streaming_main}{}
\bibliographystyle{alpha}

\appendix


\section{Sparse Recovery}\label{sparse_recovery_appendix}
In this section we give a proof of the  heavy hitters algorithm given in Lemma \ref{sparse_recovery_primitive}. It is known that  heavy hitters is equivalent to the  sparse recovery problem \cite{gilbert2010sparse}. Some sparse recovery algorithms are in fact based on algorithms for solving heavy hitters problem. However, we were not able to find a suitable reference for an  heavy hitters algorithm so we show the reduction here - namely, how to find  heavy hitters using a sparse recovery algorithm. 

We follow the terminology of~\cite{gilbert-stoc2010}. An approximate sparse recovery system consists of parameters , an  measurement matrix , and a decoding algorithm . For any vector  the decoding algorithm  can be used to recover an approximation  to  from the {\em linear sketch} . In this paper  we will use a sparse recovery algorithm that achieves the  sparse recovery guarantee:
where  is the best -term approximation to  and .  Our main sparse recovery primitive is the following result of~\cite{gilbert-stoc2010}:
\begin{theorem}[Theorem~1 in~\cite{gilbert-stoc2010}]\label{thm:l2l2}
For each  and , there is an algorithm and a distribution  over matrices in  satisfying that for any , given , the algorithm returns  such that  has  non-zeros and 

with probability at least .
The decoding algorithm runs in time .
\end{theorem}

Using this primitive, we can prove Lemma \ref{sparse_recovery_appendix}.

\begin{replemma}{sparse_recovery_primitive}[ Heavy Hitters]
For any , there is a decoding algorithm  and a distribution on matrices  in  such that, for any , given , the algorithm  returns a vector  such that
 has  non-zeros and satisfies

with probability  over the choice of . The sketch  can be maintained and decoded in  space. 
\end{replemma}



\begin{proof}
Let  be a random hash function (pairwise independence suffices), and  for  let  if  and  o.w. 
For a vector  we write  to denote  with the -th component zeroed out. 

By Markov's inequality we have

Note that since we are only using Markov's inequality, it is sufficient to have  be pairwise independent. Such a function  can be represented in small space.
Now invoke the result of Theorem~\ref{thm:l2l2} on  with , , and let  be the output. We have 

Hence, we have


This shows that applying sketches from Theorem~\ref{thm:l2l2} to vectors , for  and outputting the vector  with  allows us to recover all  with  additive error with probability at least .

Performing  repetitions and taking the median value of  yields the result. 
Note that our scheme uses  space and decoding time, and is linear in , as desired.
\end{proof}

\section{Recursive Sparsification}\label{miller_peng_appendix}
For completeness, we give a short proof of Theorem \ref{miller_peng_chain}:
\begin{reptheorem}{miller_peng_chain}[Recursive Sparsification -- \cite{pengV1}, Section 4]
Consider any PSD matrix  with maximum eigenvalue bounded from above by  and minimum nonzero eigenvalue bounded from below by . Let .  For , define:

So,  and . Then the chain of PSD matrices,  with:

satisfies the following relations:
\begin{enumerate}
  \item 
  \item  for all 
  \item 
\end{enumerate}
When  is the Laplacian of an unweighted graph,  and  (where here  is the smallest \emph{nonzero} eigenvalue). Thus the length of our chain, , is .
\end{reptheorem}

\begin{proof}
Relation 1 follows trivially from the fact that  is smaller than the smallest nonzero eigenvalue of . For any :
  
The other direction follows from . Using the same argument, relation 3 follows from the fact that . For relation 2:

Again, the other direction just follows from  . 

Finally, we need to prove the required eigenvalue bounds. For an unweighted graph,  follows from fact that  is the maximum eigenvalue of the Laplacian of the complete graph on  vertices.  by Lemma 6.1 of \cite{spielmanTengSolver}.
Note that this argument extends to weighted graphs when the ratio between the heaviest and lightest edge is bounded by a polynomial in .
\end{proof}



 
\end{document}