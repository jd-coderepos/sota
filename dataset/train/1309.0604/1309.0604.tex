\subsection{General cost measures}

Observe that the random linear coding strategy depends on the client to find a sufficient number of linearly independent combinations in caches that are not too far away. If $q$ is small there is a significant probability that the information of the next cache is not linearly independent. Hence, one might argue that for small values of $q$ there exists performance measures for which the partitioning strategy outperforms random linear coding. Our first result demonstrates that this is not true, \ie that random linear coding outperforms the partitioning strategy for any value of $q$ and any cost function.
\begin{theorem}  \label{th:codingbetter}
Consider an arbitrary placement of caches, a bounded increasing function $G$ and any $q$. Then
\begin{equation*}
\EE\left[ G\left(D(\II^c)\right) \right] \leq \EE\left[ G\left(D(\II^p)\right) \right],
\end{equation*}
\ie coding always outperforms partitioning.
\end{theorem}
The proof of the above theorem is given in Appendix~\ref{app:codingbetter}.
Note, that the above result in particular implies that coding is better than uncoded strategy
for any realization in case of placement according to a spatial Poisson process.



Since less than $k$ caches can never provide the complete data, it is clear that the minimum cost that one can hope to achieve is given by the cost of contacting the nearest $k$ caches. For notation convenience, denote the minimum expected cost as
\begin{equation}
\Gmin= \EE\left[ G\left(D(1), \dots, D(k)\right) \right].
\end{equation}


Our next result provides a bound on the deviation of the cost of the coded strategy from $\Gmin$.
\begin{theorem} \label{th:upper}
The expected cost of the coded strategy is upper bounded as
\begin{equation}
\EE\left[ G\left(D(\II^c)\right) \right] \leq \Gmin + \Gdeff, \end{equation}
where $\Gdeff=\Gmax(1-(1-1/q)^k)$.
\end{theorem}
\begin{IEEEproof} Let $A$ denote the event that the first $k$ caches provide a full rank system. Then
\begin{multline}
\EE\left[ G\left(D(\II^c)\right) \right]  = \EE\left[ G\left(D(\II^c)\right) | A \right]P(A) \\ + \EE\left[ G\left(D(\II^c)\right) | \bar A \right] P(\bar A)
\end{multline}
and the result follows from the assumption that $G$ is bounded by $\Gmax$ and $P(\bar A)\leq 1-(1-1/q)^k$, \cf~\cite{ho2006random} or~\cite{ncfundamentals}.
\end{IEEEproof}
We will illustrate the above result for a specific performance measure below. In particular, we will show that the coded strategy is close to optimal.

\subsection{Waiting time}

Next, we consider specific instances of the cost. First we consider $W$ as defined in Section~\ref{ssec:w}. Before giving the results we will provide some definitions and results on the Gamma function. Let $\Gamma(k,x)$ denote the upper incomplete Gamma function, \ie
\begin{equation}
 \Gamma(k,x) = \int_x^\infty z^{k-1}e^{-z}dz. \end{equation}
Furthermore, we define $\Gamma(k)=\Gamma(k,0)$ and $\gamma(k,x) = \Gamma(k) - \Gamma(k,x)$. The use of $\Gamma(k,x)$ for Poisson processes stems from the fact that for a Poisson random variable $X$ with mean $\mu$ we have
\begin{equation}
P(X<n) = e^{-\mu}\sum_{i=0}^{n-1}\frac{\mu^i}{\Gamma(i+1)} = \frac{\Gamma(n,\mu)}{\Gamma(n)}.
\end{equation}
Therefore, the probability that $D_n$, the distance to the $n$-th nearest neigbor in an HPP of density $\lambda$ is at most $\delta$ is
\begin{equation}
P\left(D_n\leq \delta\right) = 1-\frac{\Gamma(n,\lambda\pi\delta^2)}{\Gamma(n)}.
\end{equation}


We are now ready to present our first result on $W$. The proof of the next theorem is given in Appendix~\ref{app:wmain}.
\begin{theorem} \label{th:wmain}
Let the caches be distributed in the plane as HPP with density $\lambda$.
Then, the expected costs of the partitioning and coded strategies are
\begin{align*}
W^p =&\ k\left(\frac{k}{\lambda\pi}\right)^{b-1}\gamma\left(b,\frac{\dd}{k}\right)
 + k\left(\frac{d}{\lambda\pi}\right)^{b-1}\Gamma\left(1,\frac{\dd}{k}\right), \\
W^c_{\mathrm{min}} =&\ \frac{\gamma(k+b,d) + bd^{b-1}\Gamma(k+1,d)-(b-1)d^{b}\Gamma(k,d)}{(\lambda\pi)^{b-1}b\Gamma(k)},
\end{align*}
where $b=a/2+1$ and $d=\lambda\pi\dmax^2$.
\end{theorem}

To get an idea about the nature of the above involved equations, a reader may check Figure~\ref{fig:WHPPSimTheo}
where a numerical example is presented.





From the observation that
\begin{equation}
\lim_{d\to\infty} d^c\Gamma(k,d) = 0,
\end{equation}
for any $c$,
we immediately obtain the following corollary which provides limiting expressions for $W^p$ and $W^c_{\mathrm{min}}$ for $\dmax\to\infty$.
\begin{corollary} \label{cor:w}
Let $\bar W^p = \lim_{\dmax\to\infty} W^p$ and $\bar W^c_{\mathrm{min}} = \lim_{\dmax\to\infty} W^c_{\mathrm{min}}$. We have
\begin{align}
\bar W^p =& k\left(\frac{k}{\lambda\pi}\right)^{a/2}\Gamma\left(1+\frac{a}{2}\right), \\
\bar W^c_{\mathrm{min}} &=  \frac{\Gamma(k+1+\frac{a}{2})}{(1+\frac{a}{2})(\lambda\pi)^{a/2}\Gamma(k)}.
\end{align}
\end{corollary}

Next we turn our attention to the cost benefit of coding over partitioning, defined as the ratio $\bar W^p/\bar W^c_{\mathrm{min}}$.
Clearly, it is an optimistic prediction. However, in the numerical examples section we shall demonstrate that the actual gain is
typically close to this optimistic prediction. It follows readily from Corollary~\ref{cor:w} that
\begin{equation}
\frac{\bar W^p}{\bar W^c_{\mathrm{min}}}  = \left(1+\frac{a}{2}\right) k^{1+a/2}B\left(k,1+\frac{a}{2}\right),
\end{equation}
where $B(k,1+\frac{a}{2})$ denotes the Beta function.


\begin{theorem} \label{th:benefit}
The benefit of coding over partitioning, $\bar W^p/\bar W^c_{\mathrm{min}}$, is increasing in $k$. Moreover
\begin{equation}
\lim_{k\to\infty} \frac{\bar W^p}{\bar W^c_{\mathrm{min}}} = \left(\frac{a}{2}+1\right)\Gamma\left(\frac{a}{2}+1\right).
\end{equation}
Hence
\begin{equation}
1 \leq \frac{W^p}{W^c} \leq \left(\frac{a}{2}+1\right)\Gamma\left(\frac{a}{2}+1\right),
\end{equation}
with equality on the LHS iff $k=1$.
\end{theorem}
\begin{IEEEproof}
We start with proving that $\bar W^p/\bar W^c_{\mathrm{min}}$, is increasing in $k$.
Let $b=1+\frac{a}{2}$. We have
\begin{equation}
 \frac{\partial}{\partial k}\frac{\bar W^p}{\bar W^c_{\mathrm{min}}} = b k^b B(k,b)\left[ \frac{b}{k} + \psi(k) - \psi(k+b)\right],
\end{equation}
where $\psi(x) = \int_0^\infty \left( \frac{e^{-t}}{t} - \frac{e^{-xt}}{1-e^{-t}}\right)dt$ is the digamma function~\cite{abramowitz1974handbook}. We need to show that
\begin{equation}
 \int_0^\infty e^{-kt} \frac{1-e^{-bt}}{1-e^{-t}}dt \leq \frac{b}{k},
\end{equation}
but this follows directly from the observation that $\frac{1-e^{-bt}}{1-e^{-t}}\leq b$.

The limiting expression follows from an application of Stirlings approximation.
\end{IEEEproof}
Note, that for $a=1,2,3$, the upper bound in Theorem~\ref{th:benefit} reduces to $\frac{3\sqrt{\pi}}{4}\approx 1.3$, $2$ and $\frac{15\sqrt{\pi}}{8}\approx 3.3$ respectively.


\subsection{Cache miss probability}
Next let us consider the cache miss probability.
\begin{theorem}\label{thm:outage}
The cache miss probability of the partitioning and coded strategy are
\begin{align}
F^p &= 1 - \left( 1 - e^{-\lambda\pi r^2/k} \right)^k, \\
F^c_\mathrm{min} &= \frac{\Gamma(k,\lambda\pi r^2)}{\Gamma(k)}.
\end{align}
\end{theorem}
\begin{IEEEproof}
For the coded strategy we have
\begin{equation}\label{eq:Fcmin}
F^c_\mathrm{min} = P(D(k)>r) = \frac{\Gamma(k,\lambda\pi r^2)}{\Gamma(k)},
\end{equation}
since the $k$-th nearest cache needs to be within distance $r$.


For the uncoded strategy we have
\begin{align}
F^p
&= 1 - P_{\lambda/k}(D_1\leq r)^k \\
&= 1 - \left( 1 - e^{-\lambda\pi r^2/k} \right)^k, \end{align}
which follows from the fact that each of the parts is found in the first neighbour in a (thinned) Poisson process of intensity $\lambda/k$.
\end{IEEEproof}

We note that the following asymptotics for equations in the statement of Theorem~\ref{thm:outage} take place
$$
F^p \approx 1 - \frac{(\lambda\pi r^2)^k}{k^k},
$$
$$
F^c_\mathrm{min} \approx 1- e^{-\lambda\pi r^2} \frac{(\lambda\pi r^2)^k}{k!},
$$
for large values of $k$. This indicates that when increasing $k$, the cache miss probability increases much faster for the uncoded
strategy than for the coded strategy. We shall illustrate this phenomenon with a specific numerical example in the next section.
