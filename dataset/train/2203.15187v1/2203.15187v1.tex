\appendix
\section*{Appendix}

Sec.~\ref{sec:extra} reports additional experiments and analysis.
Sec.~\ref{sec:proposal} elaborates on the procedure of action proposal generation.
Sec.~\ref{sec:experiment_details} provides more dataset-specific implementation details and hyper-parameters for training and testing.
We also provide more qualitative results in Sec.~\ref{sec:qualitative_more}.
We discuss the limitation and broader impact of our work in Sec.~\ref{sec:limitation} and Sec.~\ref{sec:impact}.


\section{Additional Experiments and Analysis}
\label{sec:extra}

\paragraph{Error analysis}
To analyze the effectiveness of our \system, we conduct a DETAD~\cite{alwassel_2018_detad} false positive analysis of the base model without any action-aware segment modeling modules and our \system. We present the results in Figure~\ref{fig:error}. It shows a detailed categorization of false positive errors and summarizes the distribution of these errors.
 represents the number of ground truth segments in the THUMOS-14 dataset. 
We can observe that \system generates more true positive predictions with high confidence scores and produces less localization error and confusion error (at the top 1 scoring predictions).
It verifies that \system improves the detection results by predicting more accurate action boundaries with our action-aware segment modeling modules.

\paragraph{Ablation on the increased receptive field}
To further demonstrate that the effectiveness of our intra- and inter-segment attention modules is due to the segment-centric design instead of the increased receptive field, we replace our intra- and inter-segment attention modules with convolutional layers and compare the experimental results. From Table~\ref{tab:receptive} we can see that by replacing the attention modules with convolutional layers, the performances drop by 
at least , and even fall below the base model. We hypothesize that increasing the kernel size of the convolutional layers may lead to confusion between foreground and background snippets especially near the action boundaries.
In contrast, our segment-centric attention design can model temporal structures within and across action segments and localize actions more precisely.
The results verify that the segment-centric design is the key to our intra- and inter-segment attention modules.


\begin{algorithm*}[t!]
    \caption{Action Proposal Generation}
    \label{alg:generation}
    \KwIn{Predicted Action Segments , selection ratio , segment extension parameter }
    \KwOut{Action Proposals }
    \SetAlgoLined
    \For{ground-truth class }
    {
         SORT \COMMENT{sort segments by scores of class }\\
         \COMMENT{sum confidence scores for all segments} \\
        Select , s.t.  \COMMENT{select top- segments from }\\
         \COMMENT{extend selected segments on both sides}\\
    }
\end{algorithm*}



\begin{table}[t]
\centering
\caption{Ablation on the increased receptive field.}
\resizebox{0.99\linewidth}{!}{
    \begin{tabular}{@{}cccccccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Modeling}} & \multirow{2}{*}{\textbf{\tabincell{c}{Kernel\\Size}}} & \multicolumn{8}{c}{\textbf{mAP@IoU (\%)}} \\
    \cmidrule(l{4pt}r{1pt}){3-10}
    & & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & AVG \\
    \midrule
    Base & - & 67.8 & 60.7 & 51.8 & 41.3 & 30.7 & 19.9 & 10.1 & 40.3 \\
    \midrule
     & 3 & 66.2 & 59.3 & 50.5 & 39.9 & 29.9 & 19.2 & 9.1 & 39.2 \\
    \multirow{2}{*}{Conv} & 5 & 66.5 & 58.9 & 51.0 & 40.0 & 29.7 & 19.3 & 9.8 & 39.3 \\
     & 9 & 67.1 & 59.8 & 50.4 & 40.1 & 29.1 & 19.2 & 10.2 & 39.4 \\
     \midrule
    Attention & - & 68.9 & 63.1 & 54.9 & 44.5 & 34 & 22.0 & 11.9 & \bf 42.7 \\
    \bottomrule
    \end{tabular}
}
\label{tab:receptive}
\end{table}

\begin{figure}[t]
    \centering
    \vspace{-0.1in}
    \adjincludegraphics[width=\linewidth, trim={{0.05\width} {0.1\height} {0.05\width} {0.1\height}},clip]{fig/error.pdf}
    \vspace{-0.4in}
    \caption{Diagnosing detection results. We present DETAD~\cite{alwassel_2018_detad} false positive profiles of the base model and our \system.}
    \label{fig:error}
\end{figure}



\section{Action Proposal Generation}
\label{sec:proposal}
\begin{table}[t]
\centering
\caption{Ablation on different action proposal selection methods.}
\resizebox{0.99\linewidth}{!}{
    \begin{tabular}{@{}ccccccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{8}{c}{\textbf{mAP@IoU (\%)}} \\
    \cmidrule(l{4pt}r{1pt}){2-9}
    & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & AVG \\
    \midrule
    (a) & 69.9 & 63.8 & 56 & 45.8 & 36.6 & 25.0 & 13.5 & 44.4 \\
    (b) & 70.5 & 64.6 & 57.3 & 46.8 & 35.7 & 24.3 & 14.2 & 44.8 \\
    (c) & 71.2 & 65.5 & 57.1 & 46.8 & 36.6 & 25.2 & 13.4 & \bf 45.1 \\
    \bottomrule
    \end{tabular}
}
\vspace{-0.1in}
\label{tab:selection}
\end{table}

In Alg.~\ref{alg:generation}, we present the details of how to generate action proposals  from the action localization results (\ie, action segments) .
Specifically, we first sort all the segment scores across the set  for each ground-truth class . 
Then we sum the confidence scores of all the action segments and output , and pick the top- action segments with their confidence scores summation equal to  to form the action proposals.
Note that the number of the action proposals is video-adaptive and content dependent, despite  is shared for all videos.
Finally, following the common practice in temporal action localization~\cite{shou2017cdc,lin2018bsn,zeng2019graph,lin2020fast}, we extend each proposal on both ends by  of the proposal length to get an extended proposal with a longer temporal duration which can take more context-related snippets into consideration.

To verify the effectiveness of our proposal generation design, we compare three different settings of the segment selection procedure:
\textbf{(a)} Fixed number of selected action segments where  is a fixed value for each class, which is not video-adaptive and content dependent; 
\textbf{(b)}  proportional to the number of predicted action segments in , where ;
\textbf{(c)} our design.
In Table~\ref{tab:selection}, we can see that our design achieves the best results among the three designs.


\section{Experiment Details}
\label{sec:experiment_details}
For the hyper-parameters, we set  for THUMOS-14 and  for ActivityNet-v1.3.

Following~\cite{zhang2021cola,qu2021acm}, during inference, we use a set of thresholds to obtain the predicted action instances, then perform non-maximum suppression to remove overlapping segments. Specifically, for THUMOS-14, we set the foreground attention threshold from 0.1 to 0.9 with step 0.025, and perform NMS with a t-IOU threshold of 0.45. For ActivityNet-v1.3, we set the foreground-attention threshold from 0.005 to 0.02 with step 0.005, and apply NMS with a t-IoU threshold of 0.9.

We implement our method in PyTorch~\cite{paszke2019pytorch} and train it on a single NVIDIA RTX1080Ti gpu.


\begin{figure}[t]
    \centering
    \vspace{-0.2in}
    \adjincludegraphics[width=\linewidth, trim={{0.03\width} {0.1\height} {0.02\width} {0.1\height}},clip]{fig/qualitative_more.pdf}
    \vspace{-0.2in}
    \caption{Visualization of ground-truth, predictions and action proposals. Top-2 predictions with the highest confidence scores are selected for the base model and our \system. Transparent frames represent background frames.}
    \label{fig:qualitative_more}
    \vspace{-0.1in}
\end{figure}

\section{More Qualitative Results}
\label{sec:qualitative_more}
We provide more qualitative results in Figure~\ref{fig:qualitative_more}. 
The first example of action ``\textit{HammerThrow}" shows the missed detection of short actions and over-completeness error.
The second and third example of action ``\textit{Shotput}" and action ``\textit{CleanAndJerk}" shows the incompleteness error.
It clearly shows that our \system can help address these errors with more accurate action boundary predictions.


\section{Limitation}
\label{sec:limitation}
The main limitation of our \system is that the performance of our action-aware segment modeling modules depends on the generated action proposals. When the action proposals are largely misaligned with the ground-truth action segments, our \system is not able to fix the error and generate correct predictions, as shown in Figure~\textcolor{red}{3}.


\section{Broader Impacts}
\label{sec:impact}
As the most popular media format nowadays, most information is spread in the format of videos. 
The temporal action localization task aims at finding the temporal boundaries and classifying category labels of actions of interest in untrimmed videos. 
Unlike supervised learning based approach that requires dense segment-level annotations, our proposed weakly-supervised temporal action localization model \system only requires video-level labels. 
Therefore, WTAL is much more valuable in the real-world applications such as popular video-sharing social-network services, where billions of videos have only video-level user-generated tags.
Besides, WTAL has broad applications in various fields, \eg event detection, video summarization, highlight generation and video surveillance.

