\appendix\label{appendixA}
\subsection{Proof of Lemma \ref{lem:The-spatial-spectrum}}\label{proof1}
For the ease of exposition, we first define $\rho_{i}\triangleq\log(1-p_{i})$,
$\xi_{m,d}^{i}\triangleq\log(\theta_{m}B_{m,d}^{i}p_{i})$, and\[
\Phi_{i}^{m}(\boldsymbol{d},\boldsymbol{a})=-\rho_{i}\left(\frac{1}{2}\sum_{j\in\mathcal{N}_{i}^{m}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}+\xi_{m,d_{i}}^{i}\right)I_{\{a_{i}=m\}}.\]
Thus, we have $\Phi(\boldsymbol{d},\boldsymbol{a})=\sum_{i=1}^{N}\sum_{m=1}^{M}\Phi_{i}^{m}(\boldsymbol{d},\boldsymbol{a})$.

Now suppose that a user $k$ unilaterally changes its strategy $a_{k}$
to $a_{k}^{'}$. Let $\boldsymbol{a}'=(a_{1},...,a_{k-1},a_{k}^{'},a_{k+1},...,a_{N})$
be the new strategy profile. Thus, the change in potential $\Phi$
from $\boldsymbol{a}$ to $\boldsymbol{a}^{'}$ is given by\begin{align}
 & \Phi(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi(\boldsymbol{d},\boldsymbol{a})= \sum_{i=1}^{N}\sum_{m=1}^{M}\Phi_{i}^{m}(\boldsymbol{d},\boldsymbol{a}^{'})-\sum_{i=1}^{N}\sum_{m=1}^{M}\Phi_{i}^{m}(\boldsymbol{d},\boldsymbol{a})\nonumber \\
= & \sum_{m=1}^{M}\Phi_{k}^{m}(\boldsymbol{d},\boldsymbol{a}^{'})-\sum_{m=1}^{M}\Phi_{k}^{m}(\boldsymbol{d},\boldsymbol{a})+\sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}\sum_{m=1}^{M}\Phi_{i}^{m}(\boldsymbol{d},\boldsymbol{a}^{'})-\sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}\sum_{m=1}^{M}\Phi_{i}^{m}(\boldsymbol{d},\boldsymbol{a})\nonumber \\
= & \left(\sum_{m=1}^{M}\Phi_{k}^{m}(\boldsymbol{d},\boldsymbol{a}^{'})-\sum_{m=1}^{M}\Phi_{k}^{m}(\boldsymbol{d},\boldsymbol{a})\right)+\sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}\left(\Phi_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a})\right)+\sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}\left(\Phi_{i}^{a_{k}}(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi_{i}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})\right).\label{eq:P1}\end{align}


Equation (\ref{eq:P1}) consists of three parts. Next we analyze each part separately. For the first part, we have\begin{align}
 & \sum_{m=1}^{M}\Phi_{k}^{m}(\boldsymbol{d},\boldsymbol{a}^{'})-\sum_{m=1}^{M}\Phi_{k}^{m}(\boldsymbol{d},\boldsymbol{a})\nonumber \\
= & \Phi_{k}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a}) = -\rho_{k}\left(\frac{1}{2}\sum_{j\in\mathcal{N}_{k}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})}\rho_{j}+\xi_{a_{k}^{'},d_{k}}^{k}\right)+\rho_{k}\left(\frac{1}{2}\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}+\xi_{a_{k},d_{k}}^{k}\right).\label{eq:P2}\end{align}


For the second part in (\ref{eq:P1}), \begin{align*}
 & \Phi_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a})\\
= & -\rho_{i}\left(\frac{1}{2}\sum_{j\in\mathcal{N}_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})}\rho_{j}+\xi_{a_{k}^{'},d_{i}}^{i}\right)I_{\{a_{i}=a_{k}^{'}\}}+\rho_{i}\left(\frac{1}{2}\sum_{j\in\mathcal{N}_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}+\xi_{a_{k}^{'},d_{i}}^{i}\right)I_{\{a_{i}=a_{k}^{'}\}}\\
= & -\frac{1}{2}\rho_{i}\left(\sum_{j\in\mathcal{N}_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})}\rho_{j}-\sum_{j\in\mathcal{N}_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right)I_{\{a_{i}=a_{k}^{'}\}}= -\frac{1}{2}\rho_{i}\rho_{k}I_{\{a_{i}=a_{k}^{'}\}}.\end{align*}
This means \begin{align}
 \sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}\left(\Phi_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi_{i}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a})\right)=\sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}-\frac{1}{2}\rho_{i}\rho_{k}I_{\{a_{i}=a_{k}^{'}\}}=-\frac{1}{2}\rho_{k}\sum_{i\in\mathcal{N}_{k}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})}\rho_{i}.\label{eq:P4}\end{align}
For the third term in (\ref{eq:P1}), we can similarly get\begin{align}
 & \sum_{i\in\mathcal{N}_{k}(\boldsymbol{d})}\left(\Phi_{i}^{a_{k}}(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi_{i}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})\right)=\frac{1}{2}\rho_{k}\sum_{i\in\mathcal{N}_{k}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a})}\rho_{i}.\label{eq:P5-1}\end{align}
Substituting (\ref{eq:P2}), (\ref{eq:P4}), and (\ref{eq:P5-1}) into
(\ref{eq:P1}), we obtain\begin{align}
 & \Phi(\boldsymbol{d},\boldsymbol{a}^{'})-\Phi(\boldsymbol{d},\boldsymbol{a})\nonumber \\
= & -\rho_{k}\left(\sum_{j\in\mathcal{N}_{k}^{a_{k}^{'}}(\boldsymbol{d},\boldsymbol{a}^{'})}\rho_{j}+\xi_{a_{k}^{'},d_{k}}^{k}-\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}-\xi_{a_{k},d_{k}}^{k}\right)= -\rho_{k}\left(U_{k}(\boldsymbol{d},\boldsymbol{a}^{'})-U_{k}(\boldsymbol{d},\boldsymbol{a})\right).\label{eq:P6}\end{align}

Since $0<p_{k}<1$ and hence $-\log(1-p_{k})>0$, we can conclude
that $\Phi(\boldsymbol{d},\boldsymbol{a})$ defining in (\ref{eq:potential1}) is a weighted
potential function with the weight $-\log(1-p_{k})$.
\qed

\subsection{Proof of Lemma \ref{lll}}\label{proof1-2}
We complete the proof by checking the assumptions of Theorem 2.1 in
\cite{key-8}(pp.127).

(a) Since $0<\theta_{m},p_{n}<1$ and $b_{m,d}^{n}$ is bounded, then
$U_{n}(T)$ must be also bounded. It follows
that $|U_{n}(T)(I_{\{a_{n}(T)=m\}}-\sigma_{m}^{n}(T))|<\infty$.
Thus, $\sup_{T}E[|U_{n}(T)(I_{\{a_{n}(T)=m\}}-\sigma_{m}^{n}(T))|^{2}]<\infty$.

(b) First, we can obtain from (\ref{eq:ddynamc0}) that \begin{eqnarray}
\frac{d\sigma_{m}^{n}(T)}{dT} & = & \lim_{\mu_{T}\rightarrow0}\frac{\sigma_{m}^{n}(T+1)-\sigma_{m}^{n}(T)}{\mu_{T}}=U_{n}(T)(I_{\{a_{n}(T)=m\}}-\sigma_{m}^{n}(T)).\label{eq:mdynamics1}\end{eqnarray}
By taking the expectation of the RHS of (\ref{eq:mdynamics1}) with
respective to $\boldsymbol{\sigma}(T)$, we have\begin{align*}
 E[U_{n}(T)(I_{\{a_{n}(T)=m\}}-\sigma_{m}^{n}(T))|\boldsymbol{\sigma}(T)] = & \sigma_{m}^{n}(T)(1-\sigma_{m}^{n}(T))V_{m}^{n}(\boldsymbol{\sigma}(T))+\sum_{i\neq m}\sigma_{i}^{n}(T)(1-\sigma_{m}^{n}(T))V_{i}^{n}(\boldsymbol{\sigma}(T))\\
  = & \sigma_{m}^{n}(T)\sum_{i=1}^{M}\sigma_{i}^{n}(T)(V_{m}^{n}(\boldsymbol{\sigma}(T))-V_{i}^{n}(\boldsymbol{\sigma}(T))).\end{align*}


(c) First, $V_{m}^{n}(\boldsymbol{\sigma}(T))=E[U_{n}(T)|\boldsymbol{\sigma}(T),a_{n}(T)=m]$
is an expectation function, and hence is differentiable. It then follows
that $\sigma_{m}^{n}(T)\sum_{i=1}^{M}\sigma_{i}^{n}(T)(V_{m}^{n}(\boldsymbol{\sigma}(T))-V_{i}^{n}(\boldsymbol{\sigma}(T)))$
is also differentiable because the sum of differentiable functions
is also differentiable. Thus $\sigma_{m}^{n}(T)\sum_{i=1}^{M}\sigma_{i}^{n}(T)(V_{m}^{n}(\boldsymbol{\sigma}(T))-V_{i}^{n}(\boldsymbol{\sigma}(T)))$
is continuous.

(d) We have $\sum_{T}\mu_{T}=\infty$ and $\sum_{T}\mu_{T}^{2}<\infty$
by assumption.

(e) Since the sample average estimation is unbiased, the noise term
is hence the martingale difference noise. Then the expected biased
error $\beta_{T}=0$. It follows that $\sum_{T}\mu_{T}|\beta_{T}|<\infty$
with probability one.\qed


\subsection{Proof of Lemma \ref{lem4.3}}\label{proof1-4}
Let $\boldsymbol{a}=(i, a_{-n})$ and $\boldsymbol{a}^{'}=(j, a_{-n})$. By the definition of $V_{n}^{n}(\boldsymbol{\sigma}(T))$, we first have that\begin{align}
 & V_{i}^{n}(\boldsymbol{\sigma}(T))-V_{j}^{n}(\boldsymbol{\sigma}(T))\nonumber \\
 = & E\left[\log\left(\theta_{a_{n}}B_{a_{n},d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{a_{n}}(\boldsymbol{d},a_{n},a_{-n})}(1-p_{n'})\right)|a_{n}=i,\boldsymbol{\sigma}(T)\right]\nonumber \\
 & - E\left[\log\left(\theta_{a_{n}}B_{a_{n},d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{a_{n}}(\boldsymbol{d},a_{n},a_{-n})}(1-p_{n'})\right)|a_{n}=j,\boldsymbol{\sigma}(T)\right]\nonumber \\
= & E\left[\log\left(\theta_{i}B_{i,d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{i}(\boldsymbol{d},\boldsymbol{a})}(1-p_{n'})\right)-\log\left(\theta_{j}B_{j,d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{j}(\boldsymbol{d},\boldsymbol{a}^{'})}(1-p_{n'})\right)|\boldsymbol{\sigma}_{-n}(T)\right]\nonumber \\
= & \sum_{a_{-n}}\left(\log\theta_{i}B_{i,d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{i}(\boldsymbol{d},\boldsymbol{a})}(1-p_{n'})-\log\theta_{j}B_{j,d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{j}(\boldsymbol{d},\boldsymbol{a}^{'})}(1-p_{n'})\right)Pr\{a_{-n}|\boldsymbol{\sigma}_{-n}(T)\}\label{eq:l1}\end{align}
According to (\ref{eq:P6}), we have

\begin{align}
 & \log\left(\theta_{i}B_{i,d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{i}(\boldsymbol{d},\boldsymbol{a})}(1-p_{n'})\right)-\log\left(\theta_{j}B_{j,d_{n}}^{n}p_{n}\prod_{n'\in\mathcal{N}_{n}^{j}(\boldsymbol{d},\boldsymbol{a}^{'})}(1-p_{n'})\right)\nonumber \\
= & \frac{1}{-\log(1-p_{n})}\sum_{k=1}^{N}-\log(1-p_{k})\left(\frac{1}{2}\sum_{n'\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\log(1-p_{n'})+\log\theta_{a_{k}}B_{a_{k},d_{k}}^{k}p_{k}\right)\nonumber \\
 & -\frac{1}{-\log(1-p_{n})}\sum_{k=1}^{N}-\log(1-p_{k})\left(\frac{1}{2}\sum_{n'\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a}^{'})}\log(1-p_{n'})+\log\theta_{a_{k}}B_{a_{k},d_{k}}^{k}p_{k}\right).\label{eq:l2}\end{align}
By (\ref{eq:l1}) and (\ref{eq:l2}), it follows that\begin{align*}
 & -\log(1-p_{n})\left(V_{i}^{n}(\boldsymbol{\sigma}(T))-V_{j}^{n}(\boldsymbol{\sigma}(T))\right)\\
= & \sum_{a_{-n}}Pr\{a_{-n}|\boldsymbol{\sigma}_{-n}(T)\}\left(\sum_{k=1}^{N}-\log(1-p_{k})\right.\left(\frac{1}{2}\sum_{n'\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\log(1-p_{n'})+\log\theta_{a_{k}}B_{a_{k},d_{k}}^{k}p_{k}\right)\\
 & -\sum_{k=1}^{N}-\log(1-p_{k})\left.\left(\frac{1}{2}\sum_{n'\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a}^{'})}\log(1-p_{n'})+\log\theta_{a_{k}}B_{a_{k},d_{k}}^{k}p_{k}\right)\right)\\
= & L_{i}^{n}(\boldsymbol{\sigma}(T))-L_{j}^{n}(\boldsymbol{\sigma}(T)),\end{align*}
which completes the proof. \qed

\subsection{Proof of Theorem \ref{thm:For-the-distributed}}\label{proof1-3}
We first consider the variation of $L(\boldsymbol{\sigma}(T))$ along
the trajectories of ODE in (\ref{eq:mdynamics0}), i.e., differentiating
$L(\boldsymbol{\sigma}(T))$ with respective to time $T$, \begin{align}
    & \frac{dL(\boldsymbol{\sigma}(T))}{dT}= \sum_{j=1}^{M}\frac{dL(\boldsymbol{\sigma}(T))}{d\sigma_{j}^{n}(T)}\frac{d\sigma_{j}^{n}(T)}{dT}\nonumber \\
  = & \sum_{j=1}^{M}L_{j}^{n}(\boldsymbol{\sigma}(T))\sigma_{j}^{n}(T)\sum_{i=1}^{M}\sigma_{i}^{n}(T)\left(V_{j}^{n}(\boldsymbol{\sigma}(T))-V_{i}^{n}(\boldsymbol{\sigma}(T))\right)\nonumber \\
  = & \frac{1}{2}\sum_{j=1}^{M}\sum_{i=1}^{M}\sigma_{j}^{n}(T)\sigma_{i}^{n}(T)\left(V_{j}^{n}(\boldsymbol{\sigma}(T))-V_{i}^{n}(\boldsymbol{\sigma}(T))\right)\left(L_{j}^{n}(\boldsymbol{\sigma}(T))-L_{i}^{n}(\boldsymbol{\sigma}(T))\right).\label{eq:pr4}\end{align}
According to Lemma \ref{lem4.3}, we have $\frac{dL(\boldsymbol{\sigma}(T))}{dT}\geq0.$
Hence $L(\boldsymbol{\sigma}(T))$ is non-decreasing along the trajectories
of the ODE (\ref{eq:mdynamics0}). According to \cite{key-99}, the
learning mechanism converges to a stationary point
$\boldsymbol{\sigma}^{*}$ such that $\frac{dL(\boldsymbol{\sigma}^{*})}{dT}=0,$
i.e., ($\forall i,j\in\mathcal{M},n\in\mathcal{N}$)\begin{align}
\sigma_{j}^{n*}\sigma_{i}^{n*}(V_{j}^{n}(\boldsymbol{\sigma}^{*})-V_{i}^{n}(\boldsymbol{\sigma}^{*}))(L_{j}^{n}(\boldsymbol{\sigma}^{*})-L_{i}^{n}(\boldsymbol{\sigma}^{*}))= \sigma_{j}^{n*}\sigma_{i}^{n*}(V_{j}^{n}(\boldsymbol{\sigma}^{*})-V_{i}^{n}(\boldsymbol{\sigma}^{*}))^{2}=0.\label{eq:pr8}\end{align}
According to (\ref{eq:mdynamics0}) and (\ref{eq:pr8}), we have $\frac{d\sigma_{m}^{n*}}{dT}=0,\forall m\in\mathcal{M},n\in\mathcal{N}.$


If $\boldsymbol{\sigma}^{*}$ is a Nash equilibrium, it must satisfy
that\begin{equation}
V_{i}^{n}(\boldsymbol{\sigma}^{*})\leq\sum_{j=1}^{M}\sigma_{j}^{n*}V_{j}^{n}(\boldsymbol{\sigma}^{*}),\forall n\in\mathcal{N},i\in\mathcal{M}.\label{eq:pr7}\end{equation}
If $\boldsymbol{\sigma}^{*}$ is not a Nash equilibrium, we must have
that there is some $n$ and $i$ such that $V_{i}^{n}(\boldsymbol{\sigma}^{*})>\sum_{j=1}^{M}\sigma_{j}^{n*}V_{j}^{n}(\boldsymbol{\sigma}^{*}).$
Due to the continuity of the expectation function $V_{i}^{n}$, the
inequality will still hold in a small open neighborhood around $\boldsymbol{\sigma}^{*}$.
Then it follows from (\ref{eq:mdynamics0}) that, for all points $\hat{\boldsymbol{\sigma}}$
in this neighborhood of $\boldsymbol{\sigma}^{*}$ that satisfy $\hat{\sigma}_{i}^{n}\neq0$,
we have \begin{align*}
\frac{d\hat{\sigma}_{i}^{n}}{dT}  = \hat{\sigma}_{i}^{n}(V_{i}^{n}(\hat{\boldsymbol{\sigma}})-\sum_{j=1}^{M}\hat{\sigma}_{j}^{n}V_{j}^{n}(\hat{\boldsymbol{\sigma}})) > 0.\end{align*}
Hence in all sufficiently small neighborhoods of $\boldsymbol{\sigma}^{*}$,
there will be infinitely many points starting from which $\hat{\boldsymbol{\sigma}}$
will eventually leave the neighborhood. Thus, the learning mechanism must asymptotically converge to a stable stationary
point $\boldsymbol{\sigma}^{*}$ that satisfies (\ref{eq:pr7}), which is a Nash equilibrium. Moreover, according to the Sard's theorem \cite{schoen1994lectures}, when the ODE (\ref{eq:mdynamics0}) asymptotically converges, the converging equilibrium is not contained in the interior of the mixed strategy polytope \cite{kleinberg2009multiplicative}. That is, for each user $n$, there exists only one channel selection $a_{n}^{*}\in\mathcal{M}$ such that $\sigma_{m}^{n*}=1$ if $m=a_{n}^{*}$ and $\sigma_{m}^{n*}=0$ otherwise.  The learning mechanism hence converges to a Nash equilibrium with pure strategy profile. \qed

\subsection{Proof of Lemma \ref{lem:hh}}\label{proof2}
Suppose that a user $k$ changes its location $d_{k}$ to the location
$d_{k}^{'}$. Let $\boldsymbol{d}^{'}=(d_{1},..,d_{k-1},d_{k}^{'},d_{k+1},...,d_{N})$.
Recall that $\rho_{i}=\log(1-p_{i})$ and $\xi_{m,d}^{i}=\log(\theta_{m}B_{m,d}^{i}p_{i})$  as defined in Appendix \ref{proof1}.
Then the change in potential $\Phi$ from $\boldsymbol{d}$ to $\boldsymbol{d}^{'}$ is given
by\begin{align}
\Phi(\boldsymbol{d}^{'},\boldsymbol{a})-\Phi(\boldsymbol{d},\boldsymbol{a})= -\rho_{k}(\xi_{a_{k},d_{k}^{'}}^{k}-\xi_{a_{k},d_{k}}^{k})+\sum_{i=1}^{N}-\frac{1}{2}\rho_{i}\left(\sum_{j\in\mathcal{N}_{i}^{a_{i}}(\boldsymbol{d}^{'},\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{i}^{a_{i}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right).\label{eq:sm0}\end{align}


For the last term, we have \begin{align}
 & \sum_{i=1}^{N}-\frac{1}{2}\rho_{i}\left(\sum_{j\in\mathcal{N}_{i}^{a_{i}}(\boldsymbol{d}^{'},\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{i}^{a_{i}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right)\nonumber \\
= & -\frac{1}{2}\rho_{k}\left(\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d}^{'},\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right)-\frac{1}{2}\sum_{n\in\mathcal{N}}I_{\{n\in\mathcal{N}_{k}(\boldsymbol{d}^{'},\boldsymbol{a})\cup\mathcal{N}_{k}(\boldsymbol{d},\boldsymbol{a})\}}\rho_{n}\left(\sum_{j\in\mathcal{N}_{n}^{a_{n}}(\boldsymbol{d}',\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{n}^{a_{n}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right)\nonumber \\
= & -\frac{1}{2}\rho_{k}\left(\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d}^{'},\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right)-\frac{1}{2}\sum_{n\in\mathcal{N}}I_{\{n\in\mathcal{N}_{k}(\boldsymbol{d}^{'},\boldsymbol{a})\cup\mathcal{N}_{k}(\boldsymbol{d},\boldsymbol{a})\}}\rho_{n}\rho_{k}\left(I_{\{n\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d}',\boldsymbol{a})\}}-I_{\{n\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})\}}\right)\nonumber \\
= & -\frac{1}{2}\rho_{k}\left(\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d}^{'},\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right) -\frac{1}{2}\rho_{k}\sum_{n\in\mathcal{N}}I_{\{n\in\mathcal{N}_{k}(\boldsymbol{d}^{'},\boldsymbol{a})\cup\mathcal{N}_{k}(\boldsymbol{d},\boldsymbol{a})\}}I_{\{n\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d}',\boldsymbol{a})\}}\rho_{n}\nonumber \\
& +\frac{1}{2}\rho_{k}\sum_{n\in\mathcal{N}}I_{\{n\in\mathcal{N}_{k}(\boldsymbol{d}^{'},\boldsymbol{a})\cup\mathcal{N}_{k}(\boldsymbol{d},\boldsymbol{a})\}}I_{\{n\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})\}}\rho_{n}\nonumber \\
= & -\rho_{k}\left(\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d}^{'},\boldsymbol{a})}\rho_{j}-\sum_{j\in\mathcal{N}_{k}^{a_{k}}(\boldsymbol{d},\boldsymbol{a})}\rho_{j}\right).\label{eq:sm1}\end{align}
Combing (\ref{eq:sm0}) and (\ref{eq:sm1}), we have $\Phi(\boldsymbol{d}^{'},\boldsymbol{a})-\Phi(\boldsymbol{d},\boldsymbol{a})=-\log(1-p_{k})\left(U_{k}(\boldsymbol{d}^{'},\boldsymbol{a})-U_{k}(\boldsymbol{d},\boldsymbol{a})\right).$ \qed


\subsection{Proof of Lemma \ref{thm:The-distributed-strategic}} \label{proof3}
As mentioned, the system state of the distributed strategic mobility Markov chain is defined as the location profile
$\boldsymbol{d}\in\Theta$ of all users. Since distance measure is symmetry, we
have that if $d^{'}\in\mathcal{\triangle}_{d}^{n}$ then $d\in\mathcal{\triangle}_{d^{'}}^{n}$.
Further, since all locations on the spatial domain $\Delta$ are connected,
all system states $\boldsymbol{d}$ hence can reach each other within a finite
number of transitions, and the resulting finite Markov chain is irreducible
and aperiodic. The process is thus ergodic and has a unique stationary
distribution.

We then show the Markov chain is time reversible by checking the following
detailed balance equations are satisfied:\begin{equation}
Pr(\boldsymbol{d},\boldsymbol{a})q_{\boldsymbol{d},\boldsymbol{d}'}=Pr(\boldsymbol{d}^{'},\boldsymbol{a})q_{\boldsymbol{d}^{'},\boldsymbol{d}},\forall \boldsymbol{d},\boldsymbol{d}^{'}\in\Theta,\label{eq:SD1}\end{equation}
where $q_{\boldsymbol{d},\boldsymbol{d}'}$ is the transition rate from state $\boldsymbol{d}=(d_{1},...,d_{N})$
to state $\boldsymbol{d}^{'}=(d_{1}^{'},...,d_{N}^{'})$. According to the algorithm,
we know that the set of states that is directed connected to the state
$\boldsymbol{d}$ are the one where $\boldsymbol{d}$ and $\boldsymbol{d}^{'}$ differ by exactly one user,
say user $n$, such that $d_{i}=d_{i}^{'}, \forall i\neq n$ and $d_{n}\neq d_{n}^{'}$.

Since user $n$ revises its location by the timer mechanism, according the system state transition rate in (\ref{eq:SD2}), we have that\begin{align}
   Pr(\boldsymbol{d},\boldsymbol{a})q_{\boldsymbol{d},\boldsymbol{d}'} = \tau_{n}\frac{e^{\gamma\Phi(\boldsymbol{d},\boldsymbol{a})}}{\sum_{\tilde{\boldsymbol{d}}\in\mathcal{\boldsymbol{d}}^{N}}e^{\gamma\Phi(\tilde{\boldsymbol{d}},\boldsymbol{a})}}\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a})}}.\label{eq:SD3}\end{align}
Similarly, we obtain that\begin{align}
   Pr(\boldsymbol{d},\boldsymbol{a})q_{\boldsymbol{d},\boldsymbol{d}'} = \tau_{n}\frac{e^{\gamma\Phi(\boldsymbol{d}^{'},\boldsymbol{a})}}{\sum_{\tilde{\boldsymbol{d}}\in\mathcal{\boldsymbol{d}}^{N}}e^{\gamma\Phi(\tilde{\boldsymbol{d}},\boldsymbol{a})}}\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a})}}.\label{eq:SD4}\end{align}

Since the strategic mobility game is a potential game, we have\begin{equation}
\Phi(\boldsymbol{d}^{'},\boldsymbol{a})-\Phi(\boldsymbol{d},\boldsymbol{a})=-\log(1-p_{n})\left(U_{n}(\boldsymbol{d}^{'},\boldsymbol{a})-U_{n}(\boldsymbol{d},\boldsymbol{a})\right).\label{eq:SD5}\end{equation}

Combing (\ref{eq:SD3}), (\ref{eq:SD4}) and (\ref{eq:SD5}), we have
detailed balance equation (\ref{eq:SD1}) hold. The Markov chain
is hence time-reversible and has the stationary distribution given
in (\ref{eq:SD}). \qed

\subsection{Proof of Theorem \ref{thm:joint}}\label{proof4}
According to Theorem \ref{thm:For-the-distributed}, we know the distributed
learning algorithm can converge to the Nash equilibrium of the spatial channel
selection game. Let $\boldsymbol{a}_{\boldsymbol{d}}^{*}$ be the Nash equilibrium by the distributed
learning algorithm when the location profile of all users are $\boldsymbol{d}$.
Since the Nash equilibrium of the potential game is also a maximum
point to the potential function, we have $\boldsymbol{a}_{\boldsymbol{d}}^{*}=\arg\max_{\boldsymbol{a}}\Phi(\boldsymbol{d},\boldsymbol{a})$.




Similarly as the analysis of the distributed strategic
mobility algorithm in Lemma \ref{thm:The-distributed-strategic},
we define the system state of the joint channel selection and strategic mobility Markov chain as the location profile
$\boldsymbol{d}\in\Theta$ of all users. Since distance measure is symmetry, we
have that if $d^{'}\in\mathcal{\triangle}_{d}^{n}$ then $d\in\mathcal{\triangle}_{d^{'}}^{n}$.
Further, since all locations on the spatial domain $\Delta$ are connected,
all system states $\boldsymbol{d}$ hence can reach each other within a finite
number of transitions, and the resulting finite Markov chain is irreducible
and aperiodic.

We then show the Markov chain is time reversible by checking the following
detailed balance equations are satisfied:\begin{equation}
Pr(\boldsymbol{d})q_{\boldsymbol{d},\boldsymbol{d}'}=Pr(\boldsymbol{d}^{'})q_{\boldsymbol{d}^{'},\boldsymbol{d}},\forall \boldsymbol{d},\boldsymbol{d}^{'}\in\Theta,\label{eq:sd11}\end{equation}
where $q_{\boldsymbol{d},\boldsymbol{d}'}$ is the transition rate from state $\boldsymbol{d}=(d_{1},...,d_{N})$
to state $\boldsymbol{d}^{'}=(d_{1}^{'},...,d_{N}^{'})$. According to the algorithm,
we know that the set of states that is directed connected to the state
$\boldsymbol{d}$ are the one where $\boldsymbol{d}$ and $\boldsymbol{d}^{'}$ differ by exactly one user,
say user $n$, such that $d_{i}=d_{i}^{'}\forall i\neq n$ and $d_{n}\neq d_{n}^{'}$.
Since the user $n$ revise its location by the timer mechanism, we
know that the rate of revision is equal to $\tau_{n}|\mathcal{\triangle}_{d_{n}}^{n}|$.
Since user $n$ will randomly choose a new location $d_{n}^{'}$ and
stays there with probability $\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}$,
the probability from state $\boldsymbol{d}$ to $\boldsymbol{d}^{'}$ is then given as $\frac{1}{|\mathcal{\triangle}_{d_{n}}^{n}|}\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}.$
Thus ,the transition rate from state $\boldsymbol{d}$ to $\boldsymbol{d}^{'}$is give as \begin{equation}
q_{\boldsymbol{d},\boldsymbol{d}'}=\tau_{n}\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}.\label{eq:sd12}\end{equation}
It follows that\begin{align}
 Pr(\boldsymbol{d})q_{\boldsymbol{d},\boldsymbol{d}'}= \tau_{n}\frac{e^{\gamma\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}}{\sum_{\tilde{\boldsymbol{d}}\in\mathcal{\boldsymbol{d}}^{N}}e^{\gamma\Phi(\tilde{\boldsymbol{d}},\boldsymbol{a}_{\tilde{\boldsymbol{d}}}^{*})}}\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}.\label{eq:sd13} \end{align}

Similarly, we obtain that\begin{align}
  Pr(\boldsymbol{d}^{'})q_{\boldsymbol{d}^{'},\boldsymbol{d}} = \tau_{n}\frac{e^{\gamma\Phi(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}{\sum_{\tilde{\boldsymbol{d}}\in\mathcal{\boldsymbol{d}}^{N}}e^{\gamma\Phi(\tilde{\boldsymbol{d}},\boldsymbol{a}_{\tilde{\boldsymbol{d}}}^{*})}}\frac{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}{e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}+e^{-\log(1-p_{n})\gamma U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})}}.\label{eq:14} \end{align}

Since the joint channel selection and strategic mobility game is a potential game, we have\begin{align}
 \Phi(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})-\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})=-\log(1-p_{n})\left(U_{n}(\boldsymbol{d}^{'},\boldsymbol{a}_{\boldsymbol{d}^{'}}^{*})-U_{n}(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})\right).\label{eq:15}\end{align}

Combing (\ref{eq:sd13}), (\ref{eq:14}) and (\ref{eq:15}), we have
detailed balance equation (\ref{eq:sd11}) holds. The Markov chain
is hence time-reversible and has the unique stationary distribution given as $
Pr(\boldsymbol{d})=\frac{e^{\gamma\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})}}{\sum_{\tilde{\boldsymbol{d}}\in\mathcal{\boldsymbol{d}}^{N}}e^{\gamma\Phi(\tilde{\boldsymbol{d}},\boldsymbol{a}_{\tilde{\boldsymbol{d}}}^{*})}},\boldsymbol{d}\in\Theta$.

With the similar proof as in Theorem \ref{thmmm}, we can hence show that, as $\gamma\rightarrow\infty$, the algorithm approaches
the equilibrium such that $\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})$ is maximized in term of decision variable $\boldsymbol{d}$, i.e., $\max_{\boldsymbol{d}}\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})$. Furthermore, we can show that $\max_{\boldsymbol{d}}\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})=\max_{\boldsymbol{d},\boldsymbol{a}}\Phi(\boldsymbol{d},\boldsymbol{a})$ by contradiction. Let $\boldsymbol{d}^{*}=\arg\max_{\boldsymbol{d}}\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})$ and $(\bar{\boldsymbol{d}},\bar{\boldsymbol{a}})=\arg\max_{\boldsymbol{d},\boldsymbol{a}}\Phi(\boldsymbol{d},\boldsymbol{a})$. Suppose that $\Phi(\boldsymbol{d}^{*},\boldsymbol{a}_{\boldsymbol{d}^{*}}^{*})<\Phi(\bar{\boldsymbol{d}},\bar{\boldsymbol{a}})$.
Since the learning algorithm maximizes the potential $\Phi(\boldsymbol{d},\boldsymbol{a})$
given a location profile $\boldsymbol{d}$, we have that $\Phi(\bar{\boldsymbol{d}},\boldsymbol{a}_{\bar{\boldsymbol{d}}}^{*})=\max_{\boldsymbol{a}}\Phi(\bar{\boldsymbol{d}},\boldsymbol{a})\geq\Phi(\bar{\boldsymbol{d}},\bar{\boldsymbol{a}})$.
It follows that $\Phi(\bar{\boldsymbol{d}},\boldsymbol{a}_{\bar{\boldsymbol{d}}}^{*})\geq\Phi(\bar{\boldsymbol{d}},\bar{\boldsymbol{a}})>\Phi(\boldsymbol{d}^{*},\boldsymbol{a}_{\boldsymbol{d}^{*}}^{*})$,
which contradicts with that $\Phi(\boldsymbol{d}^{*},\boldsymbol{a}_{\boldsymbol{d}^{*}}^{*})=\max_{\boldsymbol{d}}\Phi(\boldsymbol{d},\boldsymbol{a}_{\boldsymbol{d}}^{*})\geq\Phi(\bar{\boldsymbol{d}},\boldsymbol{a}_{\bar{\boldsymbol{d}}}^{*})$.
Since the joint spatial channel selection and mobility game is a
potential game, we know that the maximum point $\Phi(\boldsymbol{d}^{*},\boldsymbol{a}_{\boldsymbol{d}^{*}}^{*})$ of the potential function must be a Nash equilibrium. \qed

\subsection{Proof of Theorem \ref{thm:For-the-spatialPoA}}\label{proofPoA}
For the ease of exposition, we first define that $E_{n}(\boldsymbol{d})=\max_{m\in\mathcal{M}}\left\{ \log\left(\theta_{m}B_{m,d_{n}}^{n}p_{n}\right)\right\} $
and hence we have $E(\boldsymbol{d})=\min_{n\in\mathcal{N}}\{E_{n}(\boldsymbol{d})\}$.
Since $0<p_{n}<1$, we have $\log(1-p_{n})<0$. It follows from (\ref{eq:payoff2})
that\begin{equation}
U_{n}(\boldsymbol{d},\boldsymbol{a})\leq E_{n}(\boldsymbol{d}).\label{eq:poa1}\end{equation}
Thus,\begin{equation}
\max_{\boldsymbol{a}}\sum_{n\in\mathcal{N}}U_{n}(\boldsymbol{d},\boldsymbol{a})\leq\sum_{n\in\mathcal{N}}E_{n}(\boldsymbol{d}).\label{eq:poa2}\end{equation}
Suppose that $\tilde{\boldsymbol{a}}\in\Xi$ is an arbitrary Nash
equilibrium of the spatial channel selection game. Then at Nash equilibrium,
we must have that\begin{equation}
U_{n}(\boldsymbol{d},\tilde{\boldsymbol{a}})\geq E_{n}(\boldsymbol{d})+\sum_{i\in\mathcal{N}_{n}(\boldsymbol{d})}\log(1-p_{i}).\label{eq:poa3}\end{equation}
Otherwise, the user $n$ always can improve its payoff by choosing
the channel that maximizes $\log\left(\theta_{m}B_{m,d_{n}}^{n}p_{n}\right)$.

According to (\ref{eq:poa2}) and (\ref{eq:poa3}), we then obtain\begin{eqnarray}
\mbox{PoA} & \geq & \frac{\sum_{n\in\mathcal{N}}U_{n}(\boldsymbol{d},\tilde{\boldsymbol{a}})}{\max_{\boldsymbol{a}}\sum_{n\in\mathcal{N}}U_{n}(\boldsymbol{d},\boldsymbol{a})}\nonumber \\
 & \geq & \frac{\sum_{n\in\mathcal{N}}\left(E_{n}(\boldsymbol{d})+\sum_{i\in\mathcal{N}_{n}(\boldsymbol{d})}\log(1-p_{i})\right)}{\sum_{n\in\mathcal{N}}E_{n}(\boldsymbol{d})}\nonumber \\
 & = & 1+\frac{\sum_{n\in\mathcal{N}}\sum_{i\in\mathcal{N}_{n}(\boldsymbol{d})}\log(1-p_{i})}{\sum_{n\in\mathcal{N}}E_{n}(\boldsymbol{d})}\nonumber \\
 & \geq & 1-\frac{\sum_{n\in\mathcal{N}}\sum_{i\in\mathcal{N}_{n}(\boldsymbol{d})}\varpi}{\sum_{n\in\mathcal{N}}E_{n}(\boldsymbol{d})}\nonumber \\
 & \geq & 1-\frac{\sum_{n\in\mathcal{N}}K(\boldsymbol{d})\varpi}{NE(\boldsymbol{d})}\nonumber \\
 & \geq & 1-\frac{K(\boldsymbol{d})\varpi}{E(\boldsymbol{d})}.\label{eq:PoA5}\end{eqnarray}
\qed
