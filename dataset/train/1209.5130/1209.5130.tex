\appendix\label{appendixA}
\subsection{Proof of Lemma \ref{lem:The-spatial-spectrum}}\label{proof1}
For the ease of exposition, we first define ,
, and
Thus, we have .

Now suppose that a user  unilaterally changes its strategy 
to . Let 
be the new strategy profile. Thus, the change in potential 
from  to  is given by


Equation (\ref{eq:P1}) consists of three parts. Next we analyze each part separately. For the first part, we have


For the second part in (\ref{eq:P1}), 
This means 
For the third term in (\ref{eq:P1}), we can similarly get
Substituting (\ref{eq:P2}), (\ref{eq:P4}), and (\ref{eq:P5-1}) into
(\ref{eq:P1}), we obtain

Since  and hence , we can conclude
that  defining in (\ref{eq:potential1}) is a weighted
potential function with the weight .
\qed

\subsection{Proof of Lemma \ref{lll}}\label{proof1-2}
We complete the proof by checking the assumptions of Theorem 2.1 in
\cite{key-8}(pp.127).

(a) Since  and  is bounded, then
 must be also bounded. It follows
that .
Thus, .

(b) First, we can obtain from (\ref{eq:ddynamc0}) that 
By taking the expectation of the RHS of (\ref{eq:mdynamics1}) with
respective to , we have


(c) First, 
is an expectation function, and hence is differentiable. It then follows
that 
is also differentiable because the sum of differentiable functions
is also differentiable. Thus 
is continuous.

(d) We have  and 
by assumption.

(e) Since the sample average estimation is unbiased, the noise term
is hence the martingale difference noise. Then the expected biased
error . It follows that 
with probability one.\qed


\subsection{Proof of Lemma \ref{lem4.3}}\label{proof1-4}
Let  and . By the definition of , we first have that
According to (\ref{eq:P6}), we have


By (\ref{eq:l1}) and (\ref{eq:l2}), it follows that
which completes the proof. \qed

\subsection{Proof of Theorem \ref{thm:For-the-distributed}}\label{proof1-3}
We first consider the variation of  along
the trajectories of ODE in (\ref{eq:mdynamics0}), i.e., differentiating
 with respective to time , 
According to Lemma \ref{lem4.3}, we have 
Hence  is non-decreasing along the trajectories
of the ODE (\ref{eq:mdynamics0}). According to \cite{key-99}, the
learning mechanism converges to a stationary point
 such that 
i.e., ()
According to (\ref{eq:mdynamics0}) and (\ref{eq:pr8}), we have 


If  is a Nash equilibrium, it must satisfy
that
If  is not a Nash equilibrium, we must have
that there is some  and  such that 
Due to the continuity of the expectation function , the
inequality will still hold in a small open neighborhood around .
Then it follows from (\ref{eq:mdynamics0}) that, for all points 
in this neighborhood of  that satisfy ,
we have 
Hence in all sufficiently small neighborhoods of ,
there will be infinitely many points starting from which 
will eventually leave the neighborhood. Thus, the learning mechanism must asymptotically converge to a stable stationary
point  that satisfies (\ref{eq:pr7}), which is a Nash equilibrium. Moreover, according to the Sard's theorem \cite{schoen1994lectures}, when the ODE (\ref{eq:mdynamics0}) asymptotically converges, the converging equilibrium is not contained in the interior of the mixed strategy polytope \cite{kleinberg2009multiplicative}. That is, for each user , there exists only one channel selection  such that  if  and  otherwise.  The learning mechanism hence converges to a Nash equilibrium with pure strategy profile. \qed

\subsection{Proof of Lemma \ref{lem:hh}}\label{proof2}
Suppose that a user  changes its location  to the location
. Let .
Recall that  and   as defined in Appendix \ref{proof1}.
Then the change in potential  from  to  is given
by


For the last term, we have 
Combing (\ref{eq:sm0}) and (\ref{eq:sm1}), we have  \qed


\subsection{Proof of Lemma \ref{thm:The-distributed-strategic}} \label{proof3}
As mentioned, the system state of the distributed strategic mobility Markov chain is defined as the location profile
 of all users. Since distance measure is symmetry, we
have that if  then .
Further, since all locations on the spatial domain  are connected,
all system states  hence can reach each other within a finite
number of transitions, and the resulting finite Markov chain is irreducible
and aperiodic. The process is thus ergodic and has a unique stationary
distribution.

We then show the Markov chain is time reversible by checking the following
detailed balance equations are satisfied:
where  is the transition rate from state 
to state . According to the algorithm,
we know that the set of states that is directed connected to the state
 are the one where  and  differ by exactly one user,
say user , such that  and .

Since user  revises its location by the timer mechanism, according the system state transition rate in (\ref{eq:SD2}), we have that
Similarly, we obtain that

Since the strategic mobility game is a potential game, we have

Combing (\ref{eq:SD3}), (\ref{eq:SD4}) and (\ref{eq:SD5}), we have
detailed balance equation (\ref{eq:SD1}) hold. The Markov chain
is hence time-reversible and has the stationary distribution given
in (\ref{eq:SD}). \qed

\subsection{Proof of Theorem \ref{thm:joint}}\label{proof4}
According to Theorem \ref{thm:For-the-distributed}, we know the distributed
learning algorithm can converge to the Nash equilibrium of the spatial channel
selection game. Let  be the Nash equilibrium by the distributed
learning algorithm when the location profile of all users are .
Since the Nash equilibrium of the potential game is also a maximum
point to the potential function, we have .




Similarly as the analysis of the distributed strategic
mobility algorithm in Lemma \ref{thm:The-distributed-strategic},
we define the system state of the joint channel selection and strategic mobility Markov chain as the location profile
 of all users. Since distance measure is symmetry, we
have that if  then .
Further, since all locations on the spatial domain  are connected,
all system states  hence can reach each other within a finite
number of transitions, and the resulting finite Markov chain is irreducible
and aperiodic.

We then show the Markov chain is time reversible by checking the following
detailed balance equations are satisfied:
where  is the transition rate from state 
to state . According to the algorithm,
we know that the set of states that is directed connected to the state
 are the one where  and  differ by exactly one user,
say user , such that  and .
Since the user  revise its location by the timer mechanism, we
know that the rate of revision is equal to .
Since user  will randomly choose a new location  and
stays there with probability ,
the probability from state  to  is then given as 
Thus ,the transition rate from state  to is give as 
It follows that

Similarly, we obtain that

Since the joint channel selection and strategic mobility game is a potential game, we have

Combing (\ref{eq:sd13}), (\ref{eq:14}) and (\ref{eq:15}), we have
detailed balance equation (\ref{eq:sd11}) holds. The Markov chain
is hence time-reversible and has the unique stationary distribution given as .

With the similar proof as in Theorem \ref{thmmm}, we can hence show that, as , the algorithm approaches
the equilibrium such that  is maximized in term of decision variable , i.e., . Furthermore, we can show that  by contradiction. Let  and . Suppose that .
Since the learning algorithm maximizes the potential 
given a location profile , we have that .
It follows that ,
which contradicts with that .
Since the joint spatial channel selection and mobility game is a
potential game, we know that the maximum point  of the potential function must be a Nash equilibrium. \qed

\subsection{Proof of Theorem \ref{thm:For-the-spatialPoA}}\label{proofPoA}
For the ease of exposition, we first define that 
and hence we have .
Since , we have . It follows from (\ref{eq:payoff2})
that
Thus,
Suppose that  is an arbitrary Nash
equilibrium of the spatial channel selection game. Then at Nash equilibrium,
we must have that
Otherwise, the user  always can improve its payoff by choosing
the channel that maximizes .

According to (\ref{eq:poa2}) and (\ref{eq:poa3}), we then obtain
\qed
