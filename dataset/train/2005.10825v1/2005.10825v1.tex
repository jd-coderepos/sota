\begin{table*}[t!]
    \centering
    \caption{\textbf{Quantitative comparison at the full-image level}. The methods in the first block are trained using the ImageNet dataset. The symbol  denotes the methods that are finetuned on the COCO-Stuff training set.}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{@{}lclllclllclll@{}}
        \toprule
        \multirow{2}{*}{Method} & \phantom{abc} & 
        \multicolumn{3}{c}{Imagenet ctest10k} & \phantom{abc} & 
        \multicolumn{3}{c}{COCOStuff validation split} &\phantom{abc} & 
        \multicolumn{3}{c}{Places205 validation split}\\
        \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-13}
        &&  &  &  &&  &  &  &&  &  & \\
        \midrule
        lizuka~\etal~\cite{Iizuka-SIGGRAPH-2016} && 0.200 & 23.636 & 0.917 && 0.185 & 23.863 & 0.922 && 0.146 & 25.581 & 0.950 \\
        Larsson~\etal~\cite{Larsson-ECCV-2016} && 0.188 & 25.107 & 0.927 && 0.183 & 25.061 & 0.930 && 0.161 & 25.722 & 0.951 \\
        Zhang~\etal~\cite{Zhang-ECCV-2016} && 0.238 & 21.791 & 0.892 && 0.234 & 21.838 & 0.895 && 0.205 & 22.581 & 0.921 \\
        Zhang~\etal~\cite{Zhang-SIGGRAPH-2017} && 0.145 & 26.166 & 0.932 && 0.138 & 26.823 & 0.937 && 0.149 & 25.823 & 0.948 \\
        Deoldify~\etal~\cite{Deoldify} && 0.187 & 23.537 & 0.914 && 0.180 & 23.692 & 0.920 && 0.161 & 23.983 & 0.939 \\
        Lei~\etal~\cite{Lei-CVPR-2019} && 0.202 & 24.522 & 0.917 && 0.191 & 24.588 & 0.922 && 0.175 & 25.072 & 0.942 \\
        Ours && \textbf{0.134} & \textbf{26.980} & \textbf{0.933} && \textbf{0.125} & \textbf{27.777} & \textbf{0.940} && \textbf{0.130} & \textbf{27.167} & \textbf{0.954}\\
        \midrule
        Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}* && 0.140 & 26.482 & 0.932 && 0.128 & 27.251 & 0.938 && 0.153 & 25.720 & 0.947 \\
        Ours* && \textbf{0.125} & \textbf{27.562} & \textbf{0.937} && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} && \textbf{0.120} & \textbf{27.800} & \textbf{0.957}\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:comparision}
\end{table*} 
\section{Experiments}
\label{sec:results}

In this section, we present extensive experimental results to validate the proposed instance-aware colorization algorithm. 
We start by describing the datasets used in our experiments, performance evaluation metrics, and implementation details (\secref{setting}).
We then report the quantitative evaluation of three large-scale datasets and compare our results with the state-of-the-art colorization methods (\secref{quantitative}).
We show sample colorization results on several challenging images (\secref{visual}).
We carry out three ablation studies to validate our design choices (\secref{ablation}).
Beyond standard performance benchmarking, we demonstrate the application of colorizing legacy black and white photographs (\secref{legacy}).
We conclude the section with examples where our method fails (\secref{failure}).
Please refer to the project webpage for the dataset, source code, and additional visual comparison.

\subsection{Experimental setting}
\label{sec:setting}

\heading{Datasets.}
We use three datasets for training and evaluation.

\emph{ImageNet}~\cite{ILSVRC15}: ImageNet dataset has been used by many existing colorization methods as a benchmark for performance evaluation. We use the original training split (~1.3 million images) for training all the models and use the testing split (ctest10k) provided by~\cite{Larsson-ECCV-2016} with 10,000 images for evaluation.

\emph{COCO-Stuff}~\cite{caesar-CVPR-2018}: In contrast to the \emph{object-centric} images in the ImageNet dataset, the COCO-Stuff dataset contains a wide variety of natural scenes with multiple objects present in the image. 
There are 118K images (each image is associated with a bounding box, instance segmentation, and semantic segmentation annotations). 
We use the 5,000 images in the original validation set for evaluation.

\emph{Places205}~\cite{zhou-CVPR-2014}: To investigate how well a colorization method performs on images from a different dataset, we use the 20,500 testing images (from 205 categories) from the Places205 for evaluation. 
Note that we use the Place205 dataset only for evaluating the transferability. 
We do not use its training set and the scene category labels for training.

\heading{Evaluation metrics.}
Following the experimental protocol by existing colorization methods, we report the PSNR and SSIM to quantify the colorization quality.
To compute the SSIM on color images, we average the SSIM values computed from individual channels.
We further use the recently proposed perceptual metric LPIPS by Zhang~\etal~\cite{zhang-CVPR-2018} (version 0.1; with VGG backbone).

\heading{Training details.}
We adopt a three-step training process on the ImageNet dataset as follows.

(1) \emph{Full-image colorization network}: We initialize the network with the pre-trained weight provided by \cite{Zhang-SIGGRAPH-2017}. We train the network for two epochs with a learning rate of 1e-5. 
(2) \emph{Instance colorization network}: We start with the pre-trained weight from the trained full-image colorization network above and finetune the model for five epochs with a learning rate of 5e-5 on the extracted instances from the dataset. 
(3) \emph{Fusion module}: Once both the full-image and instance network have been trained (\ie, warmed-up), we integrate them with the proposed fusion module. 
We finetune all the trainable parameters for 2 epochs with a learning rate of 2e-5. 
In our implementation, the numbers of channels of full-image feature, instance feature and fused feature in all 13 layers are 64, 128, 256, 512, 512, 512, 512, 256, 256, 128, 128, 128 and 128.

In all the training processes, we use the ADAM optimizer~\cite{kingma2014adam} with  and . 
For training, we resize all the images to a resolution of . 
Training the model on the ImageNet takes about three days on a desktop machine with one single RTX 2080Ti GPU.


\begin{figure*}[!t]
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_gray.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_gray.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_gray.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_gray.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_gray.png}
       \caption{Input}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_Iizuka.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_Iizuka.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_Iizuka.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_Iizuka.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_Iizuka.png}
       \caption{Iizuka~\etal~\cite{Iizuka-SIGGRAPH-2016}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_larrson.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_larrson.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_larrson.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_larrson.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_larrson.png}
       \caption{Larrson~\etal~\cite{larsson2017colorization}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_deoldify.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_deoldify.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_deoldify.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_deoldify.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_deoldify.png}
       \caption{Deoldify~\cite{Deoldify}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_UG.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_UG.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_UG.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_UG.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_UG.png}
       \caption{Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_ours.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_ours.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_ours.png}
      \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_ours.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_ours.png}
       \caption{Ours}
    \end{subfigure}
    \caption{\tb{Visual Comparisons with the state-of-the-arts.} Our method predicts visually pleasing colors from complex scenes with multiple object instances.}
    \label{fig:comparison}
\end{figure*} 
\subsection{Quantitative comparisons}\label{sec:quantitative}
\heading{Comparisons with the state-of-the-arts.}
We report the quantitative comparisons on three datasets in \tabref{comparision}. 
The first block of the results shows models trained on the ImageNet dataset.
Our instance-aware model performs favorably against several recent methods~\cite{Iizuka-SIGGRAPH-2016, larsson2017colorization, Zhang-ECCV-2016, Zhang-SIGGRAPH-2017, Deoldify, Lei-CVPR-2019} on all three datasets, highlighting the effectiveness of our approach.
Note that we adopted the automatic version of Zhang~\etal~\cite{Zhang-SIGGRAPH-2017} (\ie, without using any color guidance) in all the experiments.
In the second block, we show the results using our model finetuned on the COCO-Stuff training set (denoted by the ``*").
As the COCO-Stuff dataset contains more diverse and challenging scenes, our results show that finetuning on the COCO-Stuff dataset further improves the performance on the \emph{other two datasets} as well.
To highlight the effectiveness of the proposed instance-aware colorization module, we also report the results of Zhang~\etal~\cite{Zhang-SIGGRAPH-2017} finetuned on the same dataset as a strong baseline for a fair comparison.
For evaluating the performance at the \emph{instance-level}, we take the full-image ground-truth/prediction and crop the instances using the ground-truth bounding boxes to form instance-level ground-truth/predictions.
\tabref{instance_comp} summarizes the performance computed by averaging over all the instances on the COCO-Stuff dataset.
The results present a significant performance boost gained by our method in all metrics, which further highlights the contribution of instance-aware colorization to the improved performance.

\begin{table}[!t]
    \centering
    \caption{\textbf{Quantitative comparison at the instance level.} The methods in the first block are trained using the ImageNet dataset. The symbol  denotes the methods that are finetuned on the COCO-Stuff training set.}
    \resizebox{.45\textwidth}{!}{
        \begin{tabular}{@{}lclll@{}}
            \toprule
            \multirow{2}{*}{Method} & \phantom{abc} & 
            \multicolumn{3}{c}{COCOStuff validation split} \\
            \cmidrule{3-5}
            &&  &  &  \\
            \midrule
            lizuka~\etal~\cite{Iizuka-SIGGRAPH-2016} && 0.192 & 23.444 & 0.900 \\
            Larsson~\etal~\cite{Larsson-ECCV-2016} && 0.179 & 25.249 & 0.914 \\
            Zhang~\etal~\cite{Zhang-ECCV-2016} && 0.219 & 22.213 & 0.877 \\
            Zhang~\etal~\cite{Zhang-SIGGRAPH-2017} && 0.154 & 26.447 & 0.918 \\
            Deoldify~\etal~\cite{Deoldify} && 0.174 & 23.923 & 0.904 \\
            Lei~\etal~\cite{Lei-CVPR-2019} && 0.177 & 24.914 & 0.908 \\
            Ours && \textbf{0.115} & \textbf{28.339} & \textbf{0.929} \\
            \midrule
            Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}* && 0.149 & 26.675 & 0.919 \\
            Ours* && \textbf{0.095} & \textbf{29.522} & \textbf{0.938} \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:instance_comp}
\end{table} 


\heading{User study.} 
We conducted a user study to quantify the user-preference on the colorization results generated by our method and another two strong baselines, Zhang~\etal~\cite{Zhang-SIGGRAPHASIA-2018} (finetuned on the COCO-Stuff dataset) and a popular online colorization method DeOldify~\cite{Deoldify}.
We randomly select 100 images from the \emph{COCO-Stuff} validation dataset.
For each participant, we show him/her a pair of colorized results and ask for the preference (forced-choice comparison).
In total, we have 24 participants casting 2400 votes in total.
The results show that on average our method is preferred when compared with Zhang~\etal~\cite{Zhang-SIGGRAPHASIA-2018} ( v.s. ) and DeOldify~\cite{Deoldify} ( v.s. ). 
Interestingly, while DeOldify does not produce accurate colorization evaluated in the benchmark experiment, the saturated colorized results are sometimes more preferred by the users.


\subsection{Visual results}
\label{sec:visual}


\heading{Comparisons with the state-of-the-art.}
\figref{comparison} shows sample comparisons with other competing baseline methods on \emph{COCO-Stuff}. 
In general, we observe a consistent improvement in visual quality, particularly for scenes with multiple instances.

\begin{figure}[!t]
    \begin{subfigure}[!t]{.39\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/gray.png}\\
       {Input}
    \end{subfigure}
    \begin{subfigure}[!t]{.19\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/u3.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/b3.png}
       {Layer3}
    \end{subfigure}
    \begin{subfigure}[!t]{.19\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/u7.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/b7.png}\\
       {Layer7}
    \end{subfigure}
    \begin{subfigure}[!t]{.19\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/u10.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/b10.png}\\
       {Layer10}
    \end{subfigure}
    \\
    \begin{subfigure}[!t]{0.49\linewidth}\centering
        \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/mask_fig_UG_small.png}
        {Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}}
    \end{subfigure}
    \begin{subfigure}[!t]{0.49\linewidth}\centering
        \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/mask_fig_ours_small.png}
        {Our results}
    \end{subfigure}
    \vspace{\figcapmargin}
    \caption{\tb{Visualizing the fusion network.} The visualized weighted mask in layer3, layer7 and layer10 show that our model learns to \emph{adaptively} blend the features across different layers. Fusing instance-level features help improve colorization.}
    \label{fig:vismask}
\end{figure} 
\heading{Visualizing the fusion network.} \figref{vismask} visualizes the learned masks for fusing instance-level and full-image level features at multiple levels. 
We show that the proposed instance-aware processing leads to improved visual quality for complex scenarios. 


\begin{table*}[t!]
\caption{\tb{Ablations.} We validate our design choices by comparing with several alternative options.}
\label{tab:ablation}
    \begin{subtable}[t]{0.325\textwidth}
        \centering
        \caption{Different Fusion Part}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}ccclll@{}}
                \toprule
                \multicolumn{2}{c}{Fusion Part} & \phantom{abc} & 
                \multicolumn{3}{c}{COCOStuff validation split}\\
                \cmidrule{1-2} \cmidrule{4-6}
                Encoder & Decoder &&  &  &  \\ \midrule
                 &  && 0.128 & 27.251 & 0.938 \\
                \checkmark &  && 0.120 & 28.146 & 0.942 \\
                 & \checkmark && 0.117 & 27.959 & 0.941 \\
                \checkmark & \checkmark && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:ablation_fusion}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.325\textwidth}
        \centering
        \caption{Different Bounding Box Selection}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}lclll@{}}
                \toprule
                \multirow{2}{*}{Box Selection} & \phantom{abc} & 
                \multicolumn{3}{c}{COCOStuff validation split} \\
                \cmidrule{3-5}
                &&  &  &  \\
                \midrule
                Select top 8 && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} \\
                Random select 8 && 0.113 & 28.386 & 0.943 \\
                Select by threshold && 0.117 & 28.139 & 0.942 \\
                G.T. bounding box && 0.111 & 28.470 & \textbf{0.944} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:ablation_box}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.325\textwidth}
        \centering
        \caption{Different Weighted Sum}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}lclll@{}}
                \toprule
                \multirow{2}{*}{Weighted Sum} & \phantom{abc} & 
                \multicolumn{3}{c}{COCOStuff validation split} \\
                \cmidrule{3-5}
                &&  &  &  \\
                \midrule
                Box mask && 0.140 & 26.456 & 0.932 \\
                G.T. mask && 0.199 & 24.243 & 0.921 \\
                Fusion module && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:ablation_mask}
    \end{subtable}
\end{table*} 
\subsection{Ablation study}
\label{sec:ablation}
Here, we conduct ablation study to validate several important design choices in our model in \tabref{ablation}.
In all ablation study experiments, we use the COCO-Stuff validation dataset. 
First, we show that fusing features extracted from the instance network with the full-image network improve the performance.
Fusing features for both encoder and decoder perform the best. 
Second, we explore different strategies of selecting object bounding boxes as inputs for our instance network.
The results indicate that our default setting of choosing the top eight bounding boxes in terms of confidence score returned by object detector performs best and is slightly better than using the ground-truth bounding box.  
Third, we experiment with two alternative approaches (using the detected box as a mask or using the ground-truth instance mask provided in the COCO-Stuff dataset) for fusing features from multiple potentially overlapping object instances and the features from the full-image network.
Using our fusion module obtains a notable performance boost than the other two options.
This shows the capability of our fusion module to tackle more challenging scenarios with multiple overlapping objects.

\subsection{Runtime analysis}
Our colorization network involves two steps: 
(1) colorizing the individual instances and outputting the instance features; and 
(2) fusing the instance features into the full-image feature and producing a full-image colorization.
Using a machine with Intel i9-7900X 3.30GHz CPU, 32GB memory, and NVIDIA RTX 2080ti GPU, our average inference time over all the experiments is 0.187s for an image of resolution .
Each of two steps takes approximately  of the running time, while the complexity of step 1 is proportional to the number of input instances and ranges from 0.013s (one instance) to 0.1s (eight instances).

\begin{figure}[!b]
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/gray/colorized-old-photos-9_gray.png}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/expert/colorized-old-photos-9.jpg}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/ours/colorized-old-photos-9.png}}
    \\
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/gray/colorized-old-photos-17_gray.png}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/expert/colorized-old-photos-17.jpg}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/ours/colorized-old-photos-17.png}}
    \\
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/gray/colorized-old-photos-37_gray.png}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/expert/colorized-old-photos-37.jpg}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/ours/colorized-old-photos-37.png}}
    \\
    \mpage{0.31}{Input} \hfill
    \mpage{0.31}{Expert} \hfill
    \mpage{0.31}{Our results}
    
    \caption{\tb{Colorizing legacy photographs.} 
    The middle column shows the manually colorized results by the experts.}
    \label{fig:legacy}
\end{figure} 
\subsection{Colorizing legacy black and white photos}
\label{sec:legacy}
We apply our colorization model to colorize legacy black and white photographs. 
\figref{legacy} shows sample results along with manual colorization results by human expert\footnote{\url{bit.ly/color_history_photos}}.




\begin{figure}[!t]
    \begin{subfigure}[!t]{.34\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/fail/000000434230_graybox.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/fail/000000434230_.png}
       \caption{{Missing detections}}
    \end{subfigure}
    \begin{subfigure}[!t]{.65\linewidth}
       \includegraphics[width=.48\linewidth,keepaspectratio]{figure/images/fail/000000559707_graybox.png}
       \includegraphics[width=.48\linewidth,keepaspectratio]{figure/images/fail/000000559707_.png}
       \caption{{Superimposed detections}}
    \end{subfigure}
    \caption{\tb{Failure cases.} (\emph{Left}) our model reverts back to the full-image colorization when a lot of vases are missing in the detection. (\emph{Right}) the fusion module may get confused when there are many superimposed object bounding boxes.
    }
    \label{fig:failure}
\end{figure} 
\subsection{Failure modes}
\label{sec:failure}

We show 2 examples of failure cases in \figref{failure}. 
When the instances were not detected, our model reverts back to the full-image colorization network.
As a result, our method may produce visible artifacts such as washed-out colors or bleeding across object boundaries.

\ignorethis{
\begin{itemize}
    \item Imagenet~\cite{ILSVRC15}, as the reason describe in \secref{intro}, althogh a lot of existing work use Imagenet as train/test dataset, but it lacks of the instance diversity. 
    \begin{itemize}
        \item Training split: original train split, about 1.3 millon images
        \item Testing split: Follow the testing split proposed by~\cite{Larsson-ECCV-2016} ctest10k, Number of Images: 10000.
        \item All the method in the \tabref{comparision} are trained on Imagenet only.
        \item Quantitative comparisons (first column of \tabref{comparision}).
    \end{itemize}
    \item COCOStuff~\cite{caesar-CVPR-2018}, I think I should also put some figure here to show that COCOStuff do have more instance.
    \begin{itemize}
        \item Training images: 118k
        \item Label Data: bounding box, instance mask, and segmentation
        \item Thing (Instance) class: 80 classes
        \item More instance in single images.
        \item Testing Split: original validation set. 5000 images.
        \item Quantitative comparisons (second column of \tabref{comparision}).
    \end{itemize}
    \item Places205~\cite{zhou-CVPR-2014}, We also use this dataset to examinate every method's cross dataset generalization ability.
    \begin{itemize}
        \item Training images: 2.5 million images, 205 scene categories.
        \item Testing Images: 20500 images, 205 scene categories.
        \item Quantitative comparisons (third column of \tabref{comparision}).
    \end{itemize}
\end{itemize}

\heading{Evaluation metrics}
\begin{itemize}
    \item PSNR
    \item SSIM
    \item LPIPS Zhang~\etal~\cite{zhang-CVPR-2018}
    \begin{itemize}
        \item greate perceptual similarity
        \item Backbone: VGG as backbone
        \item Version: 0.1 \end{itemize}
\end{itemize}


\heading{Training details}
Fill up the traingin detailes
\begin{itemize}
    \item machine specs:
    \begin{itemize}
        \item GPU: RTX 2080ti x 1
    \end{itemize}
    \item Based on the pre-trained weight provided by~\cite{Zhang-SIGGRAPH-2017}, so it don't need heavily training.
    \item Resolution: For all the input image, we resize to 256x256
    \item optimizer: Adam optimizer
    \begin{itemize}
        \item beta 1: 0.9
        \item beta 2: 0.999
    \end{itemize}
    \item 3 stage with different optimizer setting
    \begin{enumerate}
        \item train full image backbone
        \begin{itemize}
            \item epoch: 2
            \item learning rate: 0.00001
        \end{itemize}
        \item train instance image backbone
        \begin{itemize}
            \item epoch: 5
            \item learning rate:  0.00005
        \end{itemize}
        \item train fusion module
        \begin{itemize}
            \item epoch: 2
            \item learning rate: 0.00002
        \end{itemize}
    \end{enumerate}
    \item First train full image backbone, then use this weight and train instance image backbone then it could converge faster.
\end{itemize}


\subsection{Quantitative comparisons}\label{sec:quantitative}
\heading{Comparisons with baselines}
The quantitative comparison with the state-of-the-art is shown in \tabref{comparision}.
Provide two training weight:
\begin{itemize}
    \item Training on Imagenet: first six method in \tabref{comparision}
    \item Training on COCOStuff: the last two method with * sign in \tabref{comparision}
\end{itemize}

\heading{User study}
\begin{itemize}
    \item As shown in \figref{userstudy}, we evaluate our visual quality on 100 images with a forced-choice pairwise comparison.
    \item We will provide all the images used in user study in supplementary.
    \item Show some of our good case in user study and some bad case in user study.
\end{itemize}

\input{figure/fig_userstudy}

\subsection{Visual results}
\label{sec:visual}

\heading{Comparisons with the state-of-the-art}
We also show some of our cases and compare with other baselines (as shown in \figref{comparison}).

\begin{figure*}[!t]
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_gray.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_gray.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_gray.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_gray.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_gray.png}
       \caption{Input}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_Iizuka.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_Iizuka.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_Iizuka.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_Iizuka.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_Iizuka.png}
       \caption{Iizuka~\etal~\cite{Iizuka-SIGGRAPH-2016}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_larrson.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_larrson.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_larrson.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_larrson.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_larrson.png}
       \caption{Larrson~\etal~\cite{larsson2017colorization}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_deoldify.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_deoldify.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_deoldify.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_deoldify.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_deoldify.png}
       \caption{Deoldify~\cite{Deoldify}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_UG.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_UG.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_UG.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_UG.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_UG.png}
       \caption{Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.16\linewidth}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000172856_ours.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000179265_ours.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000496954_ours.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000578792_ours.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/visual_compare/000000158660_ours.png}
       \caption{Ours}
    \end{subfigure}
    \caption{\tb{Visual Comparisons with the state-of-the-arts.} Our method predicts visually pleasing colors from complex scenes with multiple object instances.}
    \label{fig:comparison}
\end{figure*} 
\heading{Visualizing the fusion network.}

We also visualize the the weighted mask in our different fusion module (as shown in \figref{vismask}).
\begin{figure}[!t]
    \begin{subfigure}[!t]{.39\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/gray.png}\\
       {Input}
    \end{subfigure}
    \begin{subfigure}[!t]{.19\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/u3.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/b3.png}
       {Layer3}
    \end{subfigure}
    \begin{subfigure}[!t]{.19\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/u7.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/b7.png}\\
       {Layer7}
    \end{subfigure}
    \begin{subfigure}[!t]{.19\linewidth}\centering
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/u10.png}
       \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/b10.png}\\
       {Layer10}
    \end{subfigure}
    \\
    \begin{subfigure}[!t]{0.49\linewidth}\centering
        \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/mask_fig_UG.png}
        {Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}}
    \end{subfigure}
    \begin{subfigure}[!t]{0.49\linewidth}\centering
        \includegraphics[width=\linewidth,keepaspectratio]{figure/images/vismask/mask_fig_ours.png}
        {Our results}
    \end{subfigure}
    \caption{\tb{Visualizing the fusion network.} The visualized weighted mask in layer3, layer7 and layer10 show that our model learns to \emph{adaptively} blend the features across different layers. Fusing instance-level features help improve colorization.}
    \label{fig:vismask}
\end{figure} 
\begin{table*}[t!]
    \centering
    \caption{\textbf{Quantitative comparison at the full-image level}. The methods in the first block are trained using the ImageNet dataset. The symbol  denotes the methods that are finetuned on the COCO-Stuff training set.}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{@{}lclllclllclll@{}}
        \toprule
        \multirow{2}{*}{Method} & \phantom{abc} & 
        \multicolumn{3}{c}{Imagenet ctest10k} & \phantom{abc} & 
        \multicolumn{3}{c}{COCOStuff validation split} &\phantom{abc} & 
        \multicolumn{3}{c}{Places205 validation split}\\
        \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-13}
        &&  &  &  &&  &  &  &&  &  & \\
        \midrule
        lizuka~\etal~\cite{Iizuka-SIGGRAPH-2016} && 0.200 & 23.636 & 0.917 && 0.185 & 23.863 & 0.922 && 0.146 & 25.581 & 0.950 \\
        Larsson~\etal~\cite{Larsson-ECCV-2016} && 0.188 & 25.107 & 0.927 && 0.183 & 25.061 & 0.930 && 0.161 & 25.722 & 0.951 \\
        Zhang~\etal~\cite{Zhang-ECCV-2016} && 0.238 & 21.791 & 0.892 && 0.234 & 21.838 & 0.895 && 0.205 & 22.581 & 0.921 \\
        Zhang~\etal~\cite{Zhang-SIGGRAPH-2017} && 0.145 & 26.166 & 0.932 && 0.138 & 26.823 & 0.937 && 0.149 & 25.823 & 0.948 \\
        Deoldify~\etal~\cite{Deoldify} && 0.187 & 23.537 & 0.914 && 0.180 & 23.692 & 0.920 && 0.161 & 23.983 & 0.939 \\
        Lei~\etal~\cite{Lei-CVPR-2019} && 0.202 & 24.522 & 0.917 && 0.191 & 24.588 & 0.922 && 0.175 & 25.072 & 0.942 \\
        Ours && \textbf{0.134} & \textbf{26.980} & \textbf{0.933} && \textbf{0.125} & \textbf{27.777} & \textbf{0.940} && \textbf{0.130} & \textbf{27.167} & \textbf{0.954}\\
        \midrule
        Zhang~\etal~\cite{Zhang-SIGGRAPH-2017}* && 0.140 & 26.482 & 0.932 && 0.128 & 27.251 & 0.938 && 0.153 & 25.720 & 0.947 \\
        Ours* && \textbf{0.125} & \textbf{27.562} & \textbf{0.937} && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} && \textbf{0.120} & \textbf{27.800} & \textbf{0.957}\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:comparision}
\end{table*} 
\subsection{Ablation study}
\label{sec:ablation}

\begin{itemize}
    \item To show that fusing all the layers instance network with full image network would be the best, we evaluate different fusion parts in our network and shown in \tabref{ablation}.
    \item To show that as the number of the bounding boxes or the accuracy of the boxes' position increase, we could get better results and shown in \tabref{ablation_box}.
    \item To show that our weight mask is the good fusing strategy, we do some evaluate on different fusing method and shown in \tabref{ablation_mask}.
\end{itemize}

\begin{table*}[t!]
\caption{\tb{Ablations.} We validate our design choices by comparing with several alternative options.}
\label{tab:ablation}
    \begin{subtable}[t]{0.325\textwidth}
        \centering
        \caption{Different Fusion Part}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}ccclll@{}}
                \toprule
                \multicolumn{2}{c}{Fusion Part} & \phantom{abc} & 
                \multicolumn{3}{c}{COCOStuff validation split}\\
                \cmidrule{1-2} \cmidrule{4-6}
                Encoder & Decoder &&  &  &  \\ \midrule
                 &  && 0.128 & 27.251 & 0.938 \\
                \checkmark &  && 0.120 & 28.146 & 0.942 \\
                 & \checkmark && 0.117 & 27.959 & 0.941 \\
                \checkmark & \checkmark && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:ablation_fusion}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.325\textwidth}
        \centering
        \caption{Different Bounding Box Selection}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}lclll@{}}
                \toprule
                \multirow{2}{*}{Box Selection} & \phantom{abc} & 
                \multicolumn{3}{c}{COCOStuff validation split} \\
                \cmidrule{3-5}
                &&  &  &  \\
                \midrule
                Select top 8 && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} \\
                Random select 8 && 0.113 & 28.386 & 0.943 \\
                Select by threshold && 0.117 & 28.139 & 0.942 \\
                G.T. bounding box && 0.111 & 28.470 & \textbf{0.944} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:ablation_box}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.325\textwidth}
        \centering
        \caption{Different Weighted Sum}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{@{}lclll@{}}
                \toprule
                \multirow{2}{*}{Weighted Sum} & \phantom{abc} & 
                \multicolumn{3}{c}{COCOStuff validation split} \\
                \cmidrule{3-5}
                &&  &  &  \\
                \midrule
                Box mask && 0.140 & 26.456 & 0.932 \\
                G.T. mask && 0.199 & 24.243 & 0.921 \\
                Fusion module && \textbf{0.110} & \textbf{28.592} & \textbf{0.944} \\
                \bottomrule
            \end{tabular}
        }
        \label{tab:ablation_mask}
    \end{subtable}
\end{table*} \begin{figure}[!b]
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/gray/colorized-old-photos-9_gray.png}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/expert/colorized-old-photos-9.jpg}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/ours/colorized-old-photos-9.png}}
    \\
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/gray/colorized-old-photos-17_gray.png}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/expert/colorized-old-photos-17.jpg}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/ours/colorized-old-photos-17.png}}
    \\
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/gray/colorized-old-photos-37_gray.png}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/expert/colorized-old-photos-37.jpg}} \hfill
    \mpage{0.31}{\includegraphics[width=\linewidth]{figure/images/teaser/ours/colorized-old-photos-37.png}}
    \\
    \mpage{0.31}{Input} \hfill
    \mpage{0.31}{Expert} \hfill
    \mpage{0.31}{Our results} \\
    \caption{\tb{Colorizing legacy photographs.} 
    The middle column shows the manually colorized results by the experts.}
    \label{fig:legacy}
\end{figure} 
\subsection{Colorizing legacy black and white photos}
\label{sec:legacy}
For the real world legacy images, we can still have some good results on instance part (as shown in \figref{legacy}).


\subsection{Diverse colorization}
\label{sec:diverseColorization}
\begin{itemize}
    \item Showing that our architecture could be easily applied to other existing models and produce the reasonable results.
    \item Show some results with static background color, but diverse color on instance part.
\end{itemize}


\subsection{Failure modes}
\label{sec:failure}
\begin{itemize}
    \item failure from object detection failed on some object
    \item failed because of some other reason
\end{itemize}
}



