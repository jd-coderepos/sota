

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{booktabs}
\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}. }
\newcommand{\eg}{\textit{e}.\textit{g}. }
\definecolor{demphcolor}{RGB}{144,144,144}
\newcommand{\demph}[1]{\textcolor{demphcolor}{#1}}

\newcommand\blfootnote[1]{\begingroup
\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}


\usepackage[accsupp]{axessibility}  




\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{6672}  

\title{Pose for Everything: Towards Category-Agnostic Pose Estimation} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Pose for Everything}
\author{Lumin Xu\inst{1,2*} \and Sheng Jin\inst{3,4*} \and Wang Zeng\inst{1,2} \and	Wentao Liu\inst{4,5} \and	Chen Qian\inst{4} \\ Wanli Ouyang\inst{5,6} \and Ping Luo\inst{3} \and Xiaogang Wang\inst{1} \
\begin{aligned}
    \hat{\mathcal{F}_{S}^{j}} = AvgPool(Upsample(\mathcal{F}_{S}) \otimes {H}_{S}^{*j}), \quad j=1, 2, ..., J
\end{aligned}
\label{eq:F_S}

\begin{aligned}
    {H}_{Q}^{j} = \mathrm{\Theta}_{M}(Expand(\bar{\mathcal{F}_{S}^j}) \oplus \mathcal{F}_{Q}), \quad j = 1, 2, ..., J.
\end{aligned}
\label{eq:H_Q}

    \mathcal{L}_{MSE} = \dfrac{1}{JHW} \sum_{j=1}^J \sum_{\mathrm{p}} \lVert H_Q^j(\mathrm{p}) - H_Q^{*j}(\mathrm{p}) \rVert_2^2,
\label{eq:mse}

    \mathrm{PCK} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1} \left( \frac{ \lVert p_i - {p}_i^* \rVert_2}{d} \leq \sigma \right),

where  and  are predicted and ground-truth keypoint locations respectively.  is the indicator function.  is the longest side of the ground-truth bounding box, which is used as the normalization term. The correct ratio of the overall  keypoints is calculated. 
In the experiments, we report the average PCK@0.2 ( = 0.2) of all the categories in each split. In order to minimize the category bias, the mean PCK result of all the 5 splits is also reported. 


\subsection{Benchmark Results on MP-100 Dataset}

Class-Agnostic Pose Estimation (CAPE) is a new task that has not been tackled before. We tailor the existing few-shot learning baseline approaches, including Prototypical Networks~\cite{snell2017prototypical}, Finetune~\cite{nakamura2019revisiting}, and MAML~\cite{finn2017model} to address this new task. For fair comparisons, all baselines employ the same backbone network architecture (ResNet-50~\cite{he2016deep}) as ours.

\textbf{Prototypical Networks (ProtoNet)}~\cite{snell2017prototypical}. 
ProtoNet is a popular few-shot image classification approach, which constructs a prototype for each class and the query example is assigned to the class whose prototype is the ``nearest". To solve CAPE, we adapt ProtoNet to construct a prototype for each keypoint and find the location whose features are closest to the prototype in the query image. Unlike image classification, both the receptive fields and the spatial resolution of the features are critical for pose estimation. We empirically find that the features from Stage-3 achieve a good balance of these two factors among all the 4 stages.


\textbf{Fine-tune}~\cite{nakamura2019revisiting}. 
The model is first pre-trained using a combination of all base categories on the train set. During testing, the model is fine-tuned on the support images of the novel category before estimating the pose of the query images.
To handle the problem of various number of keypoints, the model is designed to output the maximum number of keypoints among all the categories, \ie 68 on MP-100 dataset, and only the few valid keypoints are supervised for each particular category during training and fine-tuning. 


\textbf{Model-Agnostic Meta-Learning (MAML)}~\cite{finn2017model}.
Through meta training, the MAML model is explicitly trained to search for a good initialization such that its parameters can quickly adapt to the given category by fine-tuning on several support images. Similar to Fine-tune~\cite{nakamura2019revisiting}, the number of keypoints of the model is set as 68. During meta testing, the model can rapidly adapt to the novel categories given a few support images.


\begin{table*}[t]
    \setlength\tabcolsep{6pt}
	\caption{Comparisons with the baseline methods on MP-100 dataset under both 5-shot and 1-shot settings. POMNet significantly outperforms other approaches.}
	\label{tab:comparison}
	\begin{center}
	    \scalebox{0.95}{
		\begin{tabular}{c|ccccc|c}
		    \hline
			5-Shot & Split1 & Split2 & Split3 & Split4 & Split5 & Mean (PCK) \\ \hline
            ProtoNet~\cite{snell2017prototypical} & 60.31 & 53.51 & 61.92 & 58.44 & 58.61 & 58.56 \\
            MAML~\cite{finn2017model} & 70.03 & 55.98 & 63.21 & 64.79 & 58.47 & 62.50 \\
            Fine-tune~\cite{nakamura2019revisiting} & 71.67 & 57.84 & 66.76 & 66.53 & 60.24 & 64.61 \\
            POMNet (Ours) & \textbf{84.72} & \textbf{79.61} & \textbf{78.00} & \textbf{80.38} & \textbf{80.85} & \textbf{80.71} \\
			\hline
			\hline
			1-Shot & Split1 & Split2 & Split3 & Split4 & Split5 & Mean (PCK)\\ \hline
            ProtoNet~\cite{snell2017prototypical} & 46.05 & 40.84 & 49.13 & 43.34 & 44.54 & 44.78 \\
            MAML~\cite{finn2017model} & 68.14 & 54.72 & 64.19 & 63.24 & 57.20 & 61.50 \\
            Fine-tune~\cite{nakamura2019revisiting} & 70.60 & 57.04 & 66.06 & 65.00 & 59.20 & 63.58 \\
            POMNet (Ours) & \textbf{84.23} & \textbf{78.25} & \textbf{78.17} & \textbf{78.68} & \textbf{79.17} & \textbf{79.70} \\
			\hline
		\end{tabular}
		}
	\end{center}
\end{table*}

As shown in Table~\ref{tab:comparison}, our proposed POMNet shows superiority over the existing few-shot learning based approaches on the task of Class-Agnostic Pose Estimation (CAPE). We first conduct experimental comparisons under the 5-shot setting. 
We observe that ProtoNet~\cite{snell2017prototypical} mostly relies on low-level appearance features and encounters difficulties in constructing a reliable prototype using only 5 samples for all the keypoints. It processes each type of keypoint individually and does not utilize the structural information, restricting its upper bound performance.
MAML~\cite{finn2017model} and Fine-tune~\cite{nakamura2019revisiting} adapt to the novel object category by fine-tuning on a few samples during testing. However, the limited number of samples makes it hard for the model to achieve good performance on the novel categories due to severe over-fitting or under-fitting problems.
Our proposed POMNet considers the CAPE task as a matching problem, decoupling the model from the object category and the number of keypoints. In the meanwhile, KIM explicitly captures the relationship among keypoints and the structure of the object of interest. As a result, POMNet achieves 80.71 PCK on the novel categories under 5-shot settings, and outperforms the baseline methods by a large margin (over 25\% improvement). 

When the number of sample images decreases to one, the degeneration of our POMNet is only 1.3\% (79.70 vs 80.71 PCK). This is presumably because POMNet captures the relationship among semantic keypoints and is more robust to occlusion and visual ambiguity. In comparison, ProtoNet requires building the prototype based on a single keypoint, thus is more sensitive to the appearance variation, resulting in a larger performance drop (44.78 vs 58.46 PCK). 


\subsection{Cross Super-Category Pose Estimation}

\begin{table*}[t]
    \setlength\tabcolsep{6pt}
	\caption{Cross super-category evaluation (PCK). POMNet outperforms other methods. But there is still large room for improvement on the rare categories.}
	\label{tab:cross-category}
	\begin{center}
	    \scalebox{0.95}{
		\begin{tabular}{c|cccc}
			\hline
			Method & Human Body & Human Face & Vehicle & Furniture \\ \hline
            ProtoNet~\cite{snell2017prototypical} & 37.61 & 57.80 & 28.35 & 42.64 \\
            MAML~\cite{finn2017model} & 51.93 & 25.72 & 17.68 & 20.09 \\
            Fine-tune~\cite{nakamura2019revisiting} & 52.11 & 25.53 & 17.46 & 20.76 \\
            POMNet (Ours) & \textbf{73.82} & \textbf{79.63} & \textbf{34.92} & \textbf{47.27} \\
			\hline
		\end{tabular}
		}
	\end{center}
\end{table*}


In order to further evaluate the generalization ability, we conduct the cross super-category pose estimation evaluation with the ``Leave-One-Out'' strategy.
That is, we train the model on all but one super-categories on MP-100 dataset, and evaluate the performance on the remaining one super-category. The super-categories to be evaluated include human body, human face, vehicle, and furniture.

As shown in Table~\ref{tab:cross-category}, our proposed POMNet outperforms the baseline methods on all the super-categories, demonstrating stronger generalization ability. However, super-category generalization is challenging and there is still a large room for improvement. We notice that all the methods perform poorly on the super-categories of vehicle and furniture. This is possibly because these categories are very different from the training ones and the extracted features are not discriminative enough. There are a great number of invisible keypoints for vehicle, and the intra-class variation between images is large for furniture, making these two super-categories more challenging. Solving CAPE requires to handle occlusion and intra-class appearance variation, and extract more discriminative features for unseen categories. We will explore these directions in the future.


\subsection{Ablation Study}

\begin{table*}[t]
    \setlength\tabcolsep{6pt}
	\caption{Ablation study of proposed components on MP-100 Split1 under 1-shot setting. KIM and MH significantly improve the model performance.}
	\label{tab:ablation}
	\begin{center}
	    \scalebox{0.95}{
		\begin{tabular}{l|ccc|c}
			\hline
			& Self-Atten. & Cross-Atten. & Matching Head & PCK \\ \hline
			\#1 &  &  &  & 74.40 \\
		    \#2	&  &  &  & 79.19 \\
			\#3 &  &  &  & 80.76 \\
			\#4 &  &  &  & 82.92 \\
		    \#5 &  &  &  & 84.23 \\
			\hline
		\end{tabular}
		}
	\end{center}
\end{table*}

\textbf{Effect of model components.} Table~\ref{tab:ablation} shows the effect of Keypoint Interaction Module (KIM) and Matching Head (MH). Comparing \#1 and \#5, we find that KIM significantly improves the CAPE model performance (13.2\% improvement). \#3 and \#4 show the effect of self-attention and cross-attention design, respectively. Especially, the 11.5\% gain from \#1 to \#4 shows that message passing among keypoints by self-attention greatly benefits keypoint localization. Comparison between \#2 and \#5 verifies the necessity of MH. \#2 replaces MH by matrix multiplication between support keypoint features and query image features. It collapses the channel dimension to 1 for each keypoint, causing undesirable information loss required for precise localization. 


\begin{table}[htb]
    \setlength\tabcolsep{6pt}
	\begin{center}
	\caption{\textbf{Left:} Effect of training category number (``\#Train'') under 1-shot setting. Evaluation is conducted on a novel category (``human body").
	\textbf{Right:} Both training and testing are on ``human body" only.}
		\begin{tabular}{ccccc||cc}
			\hline
			\#Train & 1 & 9 & 49 & 99 & Oracle & SBL-Res50 [60] \\\hline
			PCK & 39.32 & 55.74 & 70.46 & 73.82 & 89.79 & 89.76 \\ \hline
		\end{tabular}
	\label{tab:num_class}
	\end{center}
\end{table}

\textbf{Effect of training category number.} As shown in Table~\ref{tab:num_class} \textbf{Left}, more training categories leads to better generalizability to the novel category, which validates the necessity of MP-100 dataset and the rationality of our experiments.

\textbf{Sanity check.} We perform traditional one class pose estimation as a sanity check. In Table~\ref{tab:num_class} \textbf{Right}, ``Oracle" means POMNet trained and tested on the same category (``human body") only. It performs comparable with SBL-Res50~\cite{he2016deep}, which demonstrates the correctness of our design choices.


\subsection{Qualitative Results}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.88\textwidth]{images/viz.pdf}
	\caption{Qualitative results of POMNet on unseen categories. The first column shows the manually annotated support samples, and the others are the predicted query samples. The last column shows some failure cases (in \textcolor{red}{RED} circles).
	}
	\label{fig:viz}
\end{figure}

In Fig.~\ref{fig:viz}, we qualitatively evaluate the generalization ability of POMNet to the novel categories on MP-100 test sets. Our method is robust to perspective variation and appearance diversity. Typical failure cases include appearance ambiguity (the first two examples) and severe occlusion (the 3rd example).

\section{Conclusions and Limitations}

This paper introduces a novel task, termed Category-Agnostic Pose Estimation (CAPE). The idea of CAPE can benefit a wide range of application scenarios. It would not only promote the development of pose estimation (\eg pseudo-labeling for novel categories), but also enable the researchers in the other fields to detect keypoints of objects they are interested in (\eg plants). Besides, it may also make broader positive impacts for other computer vision tasks. For example, CAPE models can be developed for keypoint-based object tracking, contour-based instance segmentation, and graph matching. To achieve this goal, we propose the first CAPE framework, POse Matching Network (POMNet), and the first dataset for CAPE task, Multi-category Pose (MP-100). Experiments show that POMNet significantly outperforms the other approaches on MP-100 dataset. 
However, there are still many remaining challenges, \eg the generalization performance on rare categories, intra-class appearance variation, self-occlusion, and appearance ambiguity.
In conclusion, CAPE, as an important yet challenging task, is worth more research attention and further exploration.

~\\
\textbf{Acknowledgement.} 
This work is supported in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants (Nos. 14202217, 14203118, 14208619), in part by Research Impact Fund Grant No. R5001-18.
Ping Luo is supported by the General Research Fund of HK No.27208720, No.17212120, and No.17200622.
Wanli Ouyang is supported by the Australian Research Council Grant DP200103223, Australian Medical Research Future Fund MRFAI000085, CRC-P Smart Material Recovery Facility (SMRF) – Curby Soft Plastics, and CRC-P ARIA - Bionic Visual-Spatial Prosthesis for the Blind.

\clearpage
\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}
