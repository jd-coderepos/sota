\begin{sidewaysfigure}
    \includegraphics[width=\columnwidth]{figures_v2/figure_1_metaworld.pdf}
    \caption{Enlarged image of Figure \ref{fig:ml45_teaser}.}
    \label{app:ML45_figure}
\end{sidewaysfigure}
\clearpage
\begin{sidewaysfigure}[p]
    \includegraphics[width=\columnwidth]{figures_v2/figure_3_metaworld.pdf}
    \caption{\footnotesize Enlarged image of Figure~\ref{fig:evaluation}.
    }
\end{sidewaysfigure}
\clearpage

\section{Task Descriptions}
\label{app:tasks}

In Table~\ref{tbl:tasks}, we include a description of each of the 50 Meta-World tasks.


\begin{table}[h]
\footnotesize
    \centering
    \begin{tabular}{ll}
\toprule
 Task & Description  \\
\midrule
turn on faucet & Rotate the faucet counter-clockwise. Randomize faucet positions \\
sweep & Sweep a puck off the table. Randomize puck positions\\
assemble nut & Pick up a nut and place it onto a peg. Randomize nut and peg positions\\
turn off faucet & Rotate the faucet clockwise. Randomize faucet positions\\
push & Push the puck to a goal. Randomize puck and goal positions\\
pull lever & Pull a lever down $90$ degrees. Randomize lever positions\\
turn dial & Rotate a dial $180$ degrees. Randomize dial positions\\
push with stick & Grasp a stick and push a box using the stick. Randomize stick positions.\\
get coffee & Push a button on the coffee machine. Randomize the position of the coffee machine\\
pull handle side & Pull a handle up sideways. Randomize the handle positions\\
basketball & Dunk the basketball into the basket. Randomize basketball and basket positions\\
pull with stick & Grasp a stick and pull a box with the stick. Randomize stick positions\\
sweep into hole & Sweep a puck into a hole. Randomize puck positions\\
disassemble nut & pick a nut out of the a peg. Randomize the nut positions\\
place onto shelf & pick and place a puck onto a shelf. Randomize puck and shelf positions\\
push mug & Push a mug under a coffee machine. Randomize the mug and the machine positions\\
press handle side & Press a handle down sideways. Randomize the handle positions\\
hammer & Hammer a screw on the wall. Randomize the hammer and the screw positions\\
slide plate & Slide a plate into a cabinet. Randomize the plate and cabinet positions\\
slide plate side & Slide a plate into a cabinet sideways. Randomize the plate and cabinet positions\\
press button wall & Bypass a wall and press a button. Randomize the button positions\\
press handle & Press a handle down. Randomize the handle positions\\
pull handle & Pull a handle up. Randomize the handle positions\\
soccer & Kick a soccer into the goal. Randomize the soccer and goal positions\\
retrieve plate side & Get a plate from the cabinet sideways. Randomize plate and cabinet positions\\
retrieve plate & Get a plate from the cabinet. Randomize plate and cabinet positions\\
close drawer & Push and close a drawer. Randomize the drawer positions\\
press button top & Press a button from the top. Randomize button positions\\
reach & reach a goal position. Randomize the goal positions\\
press button top wall & Bypass a wall and press a button from the top. Randomize button positions\\
reach with wall & Bypass a wall and reach a goal. Randomize goal positions\\
insert peg side & Insert a peg sideways. Randomize peg and goal positions\\
pull & Pull a puck to a goal. Randomize puck and goal positions\\
push with wall & Bypass a wall and push a puck to a goal. Randomize puck and goal positions\\
pick out of hole & Pick up a puck from a hole. Randomize puck and goal positions\\
pick\&place w/ wall & Pick a puck, bypass a wall and place the puck. Randomize puck and goal positions\\
press button & Press a button. Randomize button positions\\
pick\&place & Pick and place a puck to a goal. Randomize puck and goal positions\\
pull mug & Pull a mug from a coffee machine. Randomize the mug and the machine positions\\
unplug peg & Unplug a peg sideways. Randomize peg positions\\
close window & Push and close a window. Randomize window positions\\
open window & Push and open a window. Randomize window positions\\
open door & Open a door with a revolving joint. Randomize door positions\\
close door & Close a door with a revolving joint. Randomize door positions\\
open drawer & Open a drawer. Randomize drawer positions\\
insert hand & Insert the gripper into a hole.\\
close box & Grasp the cover and close the box with it. Randomize the cover and box positions\\
lock door & Lock the door by rotating the lock clockwise. Randomize door positions\\
unlock door & Unlock the door by rotating the lock counter-clockwise. Randomize door positions\\
pick bin & Grasp the puck from one bin and place it into another bin. Randomize puck positions\\
\bottomrule
\end{tabular}
\vspace{0.2cm}
    \caption{A list of all of the Meta-World tasks and a description of each task.}
    \label{tbl:tasks}
\end{table}




\section{Benchmark Verification with Single-Task Learning}
\label{app:singletask}


In this section, we aim to verify that each of the benchmark tasks are individually solvable provided enough data. To do so, we consider two state-of-the-art single task reinforcement learning methods, proximal policy optimization (PPO)~\cite{schulman2017proximal} and soft actor-critic (SAC)~\cite{haarnoja2018soft}.
This evaluation is purely for validation of the tasks, and not an official evaluation protocol of the benchmark. Details of the hyperparameters are provided in Appendix~\ref{app:hyperparameters}.
The results of this experiment are illustrated in Figure~\ref{fig:single_task_results}. We indeed find that SAC can learn to perform all of the $50$ tasks to some degree, while PPO can solve a large majority of the tasks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth, left]{figures_v2/single_task_success_rates.pdf}
    \vspace{-1cm}
    \caption{Performance of independent policies trained on individual tasks using soft actor-critic (SAC) and proximal policy optimization (PPO) on 3 seeds. We verify that SAC can solve all of the tasks and PPO can also solve most of the tasks.}
    \vspace{-0.5cm}
    \label{fig:single_task_results}
\end{figure}

\section{Learning curves}
\label{app:curves}

In evaluating meta-learning algorithms, we care not just about performance but also about efficiency, i.e. the amount of data required by the meta-training process. While the adaptation process for all algorithms is extremely efficient, requiring only a few trajectories, the meta-learning process can be very inefficient.
In Figure~\ref{fig:babyresults}, we show full learning curves of the three meta-learning methods on ML1. In Figure~\ref{fig:learningcurves}, we show full learning curves of MT10, ML10, MT50 and ML45.
The MT10 and MT50 learning curves show the efficiency of multi-task learning, a critical evaluation metric, since sample efficiency gains are a primary motivation for using multi-task learning. 
Unsurprisingly, we find that off-policy algorithms such as soft actor-critic are able to learn with substantially less data than on-policy algorithms.


\begin{figure}[H]
    \centering
\includegraphics[width=0.9\columnwidth]{figures_v2/Seed_Sensitivity_ML1_reach-v2.pdf}
    \caption{Comparison of PEARL, MAML, and RL$^2$ learning curves on ML-1 reach.}
    \label{fig:babyresults}
\end{figure}
\begin{figure}[H]
\centering
    \includegraphics[width=0.9\columnwidth]{figures_v2/Seed_Sensitivity_ML1_push-v2.pdf}
    \caption{Comparison of PEARL, MAML, and RL$^2$ learning curves on ML-1 push.}
    \label{fig:ml1-push}
\end{figure}
\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures_v2/Seed_Sensitivity_ML1_pick-place-v2.pdf}
    \label{fig:ml1-pick-place}
    \caption{Comparison of PEARL, MAML, and RL$^2$ learning curves on the simplest evaluation, ML-1, where the methods need to adapt quickly to new object and goal positions within the one meta-training task.}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures_v2/Seed_Sensitivity_MT10.pdf}
    \caption{Comparison of MTRL algorithms on MT-10. MT-SAC vastly outperforms is on-policy counterparts in performance and sample efficiency.}
    \label{fig:learningcurves}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures_v2/Seed_Sensitivity_MT50.pdf}
    \vspace{-0.5cm}
    \caption{Comparison of MTRL algorithms on MT-50. MT-SAC vastly outperforms is on-policy counterparts in sample efficiency. Its performance tapers off, and with more training, MT-PPO outperforms it.}
    \label{fig:mt50-curve}
\end{figure}
\begin{figure}[H]
    \vspace{-2.5cm}
    \centering
    \includegraphics[width=\columnwidth]{figures_v2/Seed_Sensitivity_Ml10.pdf}
    \caption{Performance of meta-RL algorithms on ML-10. RL$^2$ significantly outperforms other methods in terms of sample efficiency and performance on test tasks. MAML has better test performance early on, RL$^2$ outperforms it with more training.}
    \label{fig:ml10-curve}
\end{figure}
\begin{figure}[H]
\centering
    \includegraphics[width=\columnwidth]{figures_v2/Seed_Sensitivity_Ml45.pdf}
    \caption{Learning curves of all methods on the ML-45 benchmark. Y-axis represents success rate averaged over tasks in percentage (\%). The dashed lines represent asymptotic performances. PEARL underperforms MAML and RL$^2$. RL$^2$ significantly outperforms other methods in terms of sample efficiency and performance on train tasks. RL$^2$ and MAML have similar performance on test tasks.}
    \label{fig:ml45-curve}
\end{figure}
\pagebreak{}

\section{Hyperparameter Details}
\label{app:hyperparameters}

In this section, we provide hyperparameter values for each of the methods in our experimental evaluation.

\subsection{Single Task SAC}

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.25in} X X X l }
    \toprule
    \textbf{Description} & \textbf{value} & \texttt{variable\_name} \\
    \midrule
    \multicolumn{3}{l}{Normal Hyperparameters} \\
    \midrule
    Batch size & $500$ & \texttt{batch\_size} \\
    Number of epochs & $500$ & \texttt{n\_epochs} \\
    Path length per roll-out & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & \texttt{discount} \\
    \midrule
    \multicolumn{3}{l}{Algorithm-Specific Hyperparameters} \\
    \midrule
    Policy hidden sizes & {\scriptsize $(256, 256)$}& \texttt{hidden\_sizes} \\
    Activation function of hidden layers & ReLU & \texttt{hidden\_nonlinearity} \\
    Policy learning rate & \num{3e-4} & \texttt{policy\_lr} \\
    Q-function learning rate & \num{3e-4} & \texttt{qf\_lr} \\
    Policy minimum standard deviation & \(e^{-20}\) & \texttt{min\_std} \\
    Policy maximum standard deviation & \(e^{2}\) & \texttt{max\_std} \\
    Gradient steps per epoch & 500 & \texttt{gradient\_steps\_per\_itr} \\
    Number of epoch cycles & 40 & \texttt{epoch\_cycles} \\
    Soft target interpolation parameter & \num{5e-3} & \texttt{target\_update\_tau} \\
    Use automatic entropy Tuning & True & \texttt{use\_automatic\_entropy\_tuning} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with Single Task SAC}
\label{tab:garage_sac_hparams}
\end{table}
\clearpage
\subsection{Single Task PPO}

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.5in} X X X l }
    \toprule
    \textbf{Description} & \textbf{value} & \texttt{variable\_name} \\
    \midrule
    \multicolumn{3}{l}{Normal Hyperparameters} \\
    \midrule
    Batch size & $5{,}000$ & \texttt{batch\_size} \\
    Number of epochs & $4{,}000$ & \texttt{n\_epochs} \\
    Path length per roll-out & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & \texttt{discount} \\
    \midrule
    \multicolumn{3}{l}{Algorithm-Specific Hyperparameters} \\
    \midrule
    Policy mean hidden sizes & $(128, 128)$ & \texttt{hidden\_sizes} \\
    Policy minimum standard deviation & $0.5$ & \texttt{min\_std} \\
    Policy maximum standard deviation & $1.5$ & \texttt{max\_std} \\
    Policy share standard deviation and mean network & True & \texttt{std\_share\_network} \\
    Activation function of mean hidden layers & tanh & \texttt{hidden\_nonlinearity} \\
    Optimizer learning rate & \num{5e-4} & \texttt{learning\_rate} \\ 
    Likelihood ratio clip range & $0.2$ & \texttt{lr\_clip\_range} \\
    Advantage estimation $\lambda$ & $0.95$ & \texttt{gae\_lambda} \\
    Use layer normalization & False & \texttt{layer\_normalization} \\
    Entropy method &  \detokenize{max} & \texttt{entropy\_method} \\
    Loss function &  \detokenize{surrogate_clip} & \texttt{pg\_loss} \\
    Maximum number of epochs for update & $256$ & \texttt{max\_epochs} \\
    Minibatch size for optimization & $32$ & \texttt{batch\_size} \\
    \midrule
    \multicolumn{3}{l}{Value Function Hyperparameters} \\
    \midrule
    Policy hidden sizes & $(128, 128)$ & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & tanh & \texttt{hidden\_nonlinearity} \\
    Initial value for standard deviation & $1$ & \texttt{init\_std} \\
    Use trust region constraint & False & \texttt{use\_trust\_region} \\
    Normalize inputs & True & \texttt{normalize\_inputs} \\
    Normalize outputs & True & \texttt{normalize\_outputs} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with Single Task PPO}
\label{tab:garage_st_ppo_hparams}
\end{table}



Below we summarize in as much detail as possible the hyperparameters used for each experiment in this chapter.
Seed values were individually chosen at random for each experiment.

\clearpage
\subsection{MT-PPO}

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.7in} X X X l }
    \toprule
    \textbf{Description} & \textbf{MT10} & \textbf{MT50} & \texttt{variable\_name} \\
    \midrule
    \multicolumn{4}{l}{Normal Hyperparameters} \\
    \midrule
    Batch size & $100{,}000$ & $500{,}000$ & \texttt{batch\_size} \\
    Number of epochs & $10{,}000$ & $10{,}000$ & \texttt{n\_epochs} \\
    Path length per roll-out & $500$ & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & $0.99$ & \texttt{discount} \\
    \midrule
    \multicolumn{4}{l}{Algorithm-Specific Hyperparameters} \\
    \midrule
    Policy mean hidden sizes & $(512, 512)$ & \texttt{hidden\_sizes} \\
    Policy minimum standard deviation & $0.5$ & $0.5$ & \texttt{min\_std} \\
    Policy maximum standard deviation & $1.5$ & $1.5$ & \texttt{max\_std} \\
    Policy share standard deviation and mean network & True & True & \texttt{std\_share\_network} \\
    Activation function of hidden layers & tanh & tanh & \texttt{hidden\_nonlinearity} \\
    Optimizer learning rate & \num{5e-4} & \num{5e-4} & \texttt{learning\_rate} \\ 
    Likelihood ratio clip range & $0.2$ & $0.2$ & \texttt{lr\_clip\_range} \\
    Advantage estimation $\lambda$ & $0.97$ & $0.97$ & \texttt{gae\_lambda} \\
    Use layer normalization & False & False & \texttt{layer\_normalization} \\
    Use trust region constraint & False & False & \texttt{use\_trust\_region} \\
    Entropy method &  \detokenize{max} & \detokenize{max} & \texttt{entropy\_method} \\
    Policy entropy coefficient & $5e-3$ & $5e-3$ & \texttt{policy\_ent\_coeff} \\
    Loss function &  \detokenize{surrogate_clip} & \detokenize{surrogate_clip} & \texttt{pg\_loss} \\
    Maximum number of epochs for update & $16$ & $16$ & \texttt{max\_epochs} \\
    Minibatch size for optimization & $32$ & $32$ & \texttt{batch\_size} \\
    \midrule
    \multicolumn{4}{l}{Value Function Hyperparameters} \\
    \midrule
    Value Function hidden sizes & $(512, 512)$ & $(512, 512)$ & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & tanh & tanh & \texttt{hidden\_nonlinearity} \\
    Trainable standard deviation  & True & True & \texttt{learn\_std} \\
    Initial value for standard deviation & $1$ & $1$ & \texttt{init\_std} \\
    Use layer normalization & False & False & \texttt{layer\_normalization} \\
    Use trust region constraint & False & False & \texttt{use\_trust\_region} \\
    Normalize inputs & True & True & \texttt{normalize\_inputs} \\
    Normalize outputs & True & True & \texttt{normalize\_outputs} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with Multi-Task PPO}
\label{tab:garage_ppo_hparams}
\end{table}

\FloatBarrier


\clearpage
\subsection{MT-TRPO}

\FloatBarrier

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{2in} X X X l }
    \toprule
    \textbf{Description} & \textbf{MT10} & \textbf{MT50} & \texttt{variable\_name} \\
    \midrule
    \multicolumn{4}{l}{Normal Hyperparameters} \\
    \midrule
    Batch size & $100{,}000$ & $500{,}000$ & \texttt{batch\_size} \\
    Number of epochs & $10{,}000$ & $10{,}000$ & \texttt{n\_epochs} \\
    Path length per roll-out & $500$ & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & $0.99$ & \texttt{discount} \\
    \midrule
    \multicolumn{4}{l}{Algorithm-Specific Hyperparameters} \\
    \midrule
    Policy mean hidden sizes & $(512, 512)$ & \texttt{hidden\_sizes} \\
    Policy minimum standard deviation & $0.5$ & $0.5$ & \texttt{min\_std} \\
    Policy maximum standard deviation & $1.5$ & $1.5$ & \texttt{max\_std} \\
    Policy share standard deviation and mean network & True & True & \texttt{std\_share\_network} \\
    Activation function of hidden layers & tanh & tanh & \texttt{hidden\_nonlinearity} \\
    Advantage estimation $\lambda$ & $0.95$ & $0.95$ & \texttt{gae\_lambda} \\
    Maximum KL divergence & \num{1e-2} & \num{1e-2} & \texttt{max\_kl\_step} \\
    Number of CG iterations & $10$ & $10$ & \texttt{cg\_iters} \\
    Regularization coefficient & \num{1e-5} & \num{1e-5} & \texttt{reg\_coeff} \\
    Use layer normalization & False & False & \texttt{layer\_normalization} \\
    Use trust region constraint & False & False & \texttt{use\_trust\_region} \\
    Entropy method & \detokenize{no_entropy} & \detokenize{no_entropy} & \texttt{entropy\_method} \\
    Loss function & surrogate & surrogate & \texttt{pg\_loss} \\
    \midrule
    \multicolumn{4}{l}{Value Function Hyperparameters} \\
    \midrule
    Hidden sizes & $(512, 512)$ & $(512, 512)$ & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & tanh & tanh & \texttt{hidden\_nonlinearity} \\
    Trainable standard deviation & True & True & \texttt{learn\_std} \\
    Initial value for standard deviation & $1$ & $1$ & \texttt{init\_std} \\
    Use layer normalization & False & False & \texttt{layer\_normalization} \\
    Use trust region constraint & True & True & \texttt{use\_trust\_region} \\
    Normalize inputs & True & True & \texttt{normalize\_inputs} \\
    Normalize outputs & True & True & \texttt{normalize\_outputs} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with Multi-Task TRPO}
\label{tab:garage_trpo_hparams}
\end{table}

\FloatBarrier


\clearpage
\subsection{MT-SAC}

\FloatBarrier

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.5in} X X X l }
    \toprule
    \textbf{Description} & \textbf{MT10} & \textbf{MT50} & \texttt{variable\_name} \\
    \midrule
    \multicolumn{4}{l}{General Hyperparameters} \\
    \midrule
    Batch size & $5{,}000$ & $25{,}000$ & \texttt{batch\_size} \\
    Number of epochs & $500$ & $500$ & \texttt{epochs} \\
    Path length per roll-out & $500$ & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & $0.99$ & \texttt{discount} \\
    \midrule
    \multicolumn{4}{l}{Algorithm-Specific Hyperparameters} \\
    \midrule
    Policy hidden sizes & {\scriptsize $(400, 400)$} & {\scriptsize $(400, 400)$} & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & ReLU & ReLU & \texttt{hidden\_nonlinearity} \\
    Policy learning rate & \num{3e-4} & \num{3e-4} & \texttt{policy\_lr} \\
    Q-function learning rate & \num{3e-4} & \num{3e-4} & \texttt{qf\_lr} \\
    Policy minimum standard deviation & \(e^{-20}\) & \(e^{-20}\) & \texttt{min\_std} \\
    Policy maximum standard deviation & \(e^{2}\) & \(e^{2}\) & \texttt{max\_std} \\
    Gradient steps per epoch & 500 & 500 & \texttt{gradient\_steps\_per\_itr} \\
    Number of epoch cycles & 200 & 40 & \texttt{epoch\_cycles} \\
    Soft target interpolation parameter & \num{5e-3} & \num{5e-3} & \texttt{target\_update\_tau} \\
    Use automatic entropy Tuning & True & True & \texttt{use\_automatic\_entropy\_tuning} \\
    Minimum Buffer Batch Size & 1500 & 7500 & \texttt{min\_buffer\_size} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with Multi-Task SAC}
\label{tab:garage_mtsac_hparams}
\end{table}

\FloatBarrier


\clearpage
\subsection{TE-PPO}

\FloatBarrier

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ l X X X l }
    \toprule
    \textbf{Description} & \textbf{MT10} & \textbf{MT50} & \texttt{argument\_name} \\
    \midrule
    General Hyperparameters \\
    \midrule
    Batch size & $50{,}000$ & $250{,}000$ & \texttt{batch\_size} \\
    Number of epochs & $4{,}000$ & $2{,}000$ & \texttt{n\_epochs} \\
    \midrule
    Algorithm-Specific Hyperparameters \\
    \midrule
    Policy hidden sizes & $(32, 16)$ & $(32, 16)$ & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & tanh & tanh & \texttt{hidden\_nonlinearity} \\
    Likelihood ratio clip range & $0.2$ & $0.2$ & \texttt{lr\_clip\_range} \\
    Latent dimension & $4$ & $4$ & \texttt{latent\_length} \\
    Inference window length & $6$ & $6$ & \texttt{inference\_window} \\ 
    Embedding maximum standard deviation & $0.2$ & $0.2$ & \texttt{embedding\_max\_std} \\
    Policy entropy coefficient & $2e-2$ & $2e-2$ & \texttt{policy\_ent\_coeff} \\
    Value function & \multicolumn{2}{L{4.2cm}}{Gaussian MLP fit with observations, latent variables and returns} & \texttt{baseline} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with Task Embeddings PPO}
\label{tab:garage_te_hparams}
\end{table}

\FloatBarrier

\clearpage

\subsection{MAML}

\FloatBarrier

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.5in} X X X l }
    \toprule
    \textbf{Description} & \textbf{ML1} & \textbf{ML10} & \textbf{ML45} & \texttt{argument\_name} \\
    \midrule
    \multicolumn{5}{l}{Meta-/Multi-Task Hyperparameters} \\
    \midrule
    Meta-batch size & $20$ & $20$ & $45$ & \texttt{meta\_batch\_size} \\
    Roll-outs per task & $10$ & $10$ & $20$ & \texttt{rollouts\_per\_task} \\
    \midrule
    General Hyperparameters \\
    \midrule
    Path length per roll-out & $500$ & $500$ & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & $0.99$ & $0.99$ & \texttt{discount} \\
    \midrule
    Algorithm-specific Hyperparameters \\
    \midrule
    Policy hidden sizes & $(128, 128)$ & $(128, 128)$ & $(128, 128)$ & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & tanh & tanh & tanh &  \texttt{hidden\_nonlinearity} \\
    Activation function of output layer & tanh & tanh & tanh &  \texttt{output\_nonlinearity} \\
    Inner algorithm learning rate & \num{1e-4} & \num{1e-4} & \num{1e-4} & \texttt{inner\_lr} \\
    Optimizer learning rate & \num{1e-3} & \num{1e-3} & \num{1e-3} & \texttt{outer\_lr} \\ 
    Maximum KL divergence & \num{1e-2} & \num{1e-2} & \num{1e-2} & \texttt{max\_kl\_step} \\
    Number of inner gradient updates & $1$ & $1$ & $1$ & \texttt{num\_grad\_update} \\
    Policy entropy coefficient & \num{5e-5} & \num{5e-5} & \num{5e-5} & \texttt{policy\_ent\_coeff} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with MAML}
\label{tab:garage_maml_hparams}
\end{table}

\FloatBarrier


\clearpage
\subsection{\RLsq}

\FloatBarrier

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.5in} X X X l }
    \toprule
    \textbf{Description} & \textbf{ML1} & \textbf{ML10} & \textbf{ML45} & \texttt{argument\_name} \\
    \midrule
    \multicolumn{5}{l}{Meta-/Multi-Task Hyperparameters} \\
    \midrule
    Meta-batch size & $25$ & $10$ & $25$ & \texttt{meta\_batch\_size} \\
    Roll-outs per task & $10$ & $10$ & $10$ & \texttt{rollouts\_per\_task} \\    
    \midrule
    \multicolumn{5}{l}{General Hyperparameters} \\
    \midrule
    Path length per roll-out & $500$ & $500$ & $500$ & \texttt{max\_path\_length} \\
    Discount factor & $0.99$ & $0.99$ & $0.99$ & \texttt{discount} \\
    \midrule
    \multicolumn{5}{l}{Algorithm-Specific Hyperparameters} \\
    \midrule
    Policy hidden sizes & {\scriptsize $(256, )$} & {\scriptsize $(256, )$} & {\scriptsize $(256, )$} & \texttt{hidden\_sizes} \\
    Activation function of hidden layers & tanh & tanh & tanh & \texttt{hidden\_nonlinearity} \\
    Activation function of recurrent layers & sigmoid & sigmoid & sigmoid & \texttt{recurrent\_nonlinearity} \\
    Optimizer learning rate & \num{5e-4} & \num{5e-4} & \num{5e-4} & \texttt{optimizer\_lr} \\
    Likelihood ratio clip range & $0.2$ & $0.2$ & $0.2$ & \texttt{lr\_clip\_range} \\
    Advantage estimation $\lambda$ & $0.95$ & $0.95$ & $0.95$ & \texttt{gae\_lambda} \\
    Optimizer maximum epochs & $10$ & $10$ & $10$ & \texttt{optimizer\_max\_epochs} \\
    RNN cell type used in Policy & GRU & GRU & GRU & \texttt{cell\_type} \\
    Value function & \multicolumn{3}{C{5cm}}{Linear feature baseline} & \texttt{baseline} \\
    Policy entropy coefficient & \num{5e-6} & \num{5e-6} & \num{5e-6} & \texttt{policy\_ent\_coeff} \\
    Minimum policy standard deviation & $0.5$ & $0.5$ & $0.5$ & \texttt{min\_std} \\
    Maximum policy standard deviation & $0.5$ & $0.5$ & $0.5$ & \texttt{max\_std} \\ 
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with \RLsq}
\label{tab:garage_rl2_hparams}
\end{table}

\FloatBarrier


\clearpage
\subsection{PEARL}

\FloatBarrier

\begin{table}[h!]
\begin{tabularx}{\linewidth}{ L{1.5in} X X X l }
    \toprule
    \textbf{Description} & \textbf{ML1} & \textbf{ML10} & \textbf{ML45} & \texttt{argument\_name} \\
    \midrule
    \multicolumn{5}{l}{Meta-/Multi-Task Hyperparameters} \\
    \midrule
    Meta-batch size & $16$ & $10$ &  $45$ & \texttt{meta\_batch\_size} \\
    Tasks sampled per epoch & $15$ & $10$ &  $45$ & \texttt{num\_tasks\_sample} \\
    Number of independent evaluations & \multicolumn{3}{C{3cm}}{$5$} & \texttt{num\_evals} \\
    Steps sampled per evaluation & $450$ & $1{,}650$ & $1{,}650$ & \texttt{num\_steps\_per\_eval} \\
    \midrule
    General Hyperparameters \\
    \midrule
    Batch size & $500$ & $1{,}000$ & $1{,}000$ &\texttt{batch\_size} \\
    Path length per roll-out & \multicolumn{3}{C{3cm}}{$500$} & \texttt{max\_path\_length} \\
    Reward scale & \multicolumn{3}{C{3cm}}{$10{,}000$} & \texttt{reward\_scale} \\
    Discount factor & \multicolumn{3}{C{3cm}}{$0.99$} & \texttt{discount} \\
    \midrule
    Algorithm-Specific Hyperparameters \\
    \midrule
    Policy hidden sizes & \multicolumn{3}{C{3cm}}{$(300, 300, 300)$} & \texttt{net\_size} \\
    Activation function of hidden layers & \multicolumn{3}{C{3cm}}{ReLU} & \texttt{hidden\_nonlinearity} \\
    Policy learning rate & \multicolumn{3}{C{3cm}}{ \num{3e-4}} & \texttt{policy\_lr} \\
    Q-function learning rate & \multicolumn{3}{C{3cm}}{ \num{3e-4}} & \texttt{qf\_lr} \\
    Value function learning rate & \multicolumn{3}{C{3cm}}{ \num{3e-4}} & \texttt{vf\_lr} \\
    Context learning rate & \multicolumn{3}{C{3cm}}{ \num{3e-4}} & \texttt{context\_lr} \\
    Latent dimension & \multicolumn{3}{C{3cm}}{$7$} & \texttt{latent\_dimension} \\
    Policy mean regularization coefficient & \multicolumn{3}{C{3cm}}{ \num{1e-3}} & \texttt{policy\_mean\_reg\_coeff} \\
    Policy standard deviation regularization coefficient & \multicolumn{3}{C{3cm}}{ \num{1e-3}} & \texttt{policy\_std\_reg\_coeff} \\
    Soft target interpolation parameter & \multicolumn{3}{C{3cm}}{ \num{5e-3}} & \texttt{soft\_target\_tau} \\
    KL $\lambda$ & \multicolumn{3}{C{3cm}}{$0.01$} & \texttt{KL\_lambda} \\
    Use information bottleneck & \multicolumn{3}{C{3cm}}{True} & \texttt{use\_information\_bottleneck} \\
    Use next observation in context & \multicolumn{3}{C{3cm}}{False} & \texttt{use\_next\_observation\_in\_context} \\
    Gradient steps per epoch & $1$ & $60$ & $14$ & \texttt{num\_steps\_per\_epoch} \\
    Steps sampled in the initial epoch & $1{,}000$ & $5{,}000$ & $22{,}500$ & \texttt{num\_initial\_steps} \\
    Prior steps sampled per epoch  & \multicolumn{3}{C{3cm}}{$2500$} & \texttt{num\_steps\_prior} \\
    Posterior steps sampled per epoch  & \multicolumn{3}{C{3cm}}{$2500$} & \texttt{num\_steps\_posterior} \\
    Extra posterior steps sampled per epoch  & \multicolumn{3}{C{3cm}}{$2500$} & \texttt{num\_extra\_steps\_posterior} \\
    Embedding batch size & \multicolumn{3}{C{3cm}}{$250$} & \texttt{embedding\_batch\_size} \\
    Embedding minibatch size & \multicolumn{3}{C{3cm}}{$250$} & \texttt{embedding\_mini\_batch\_size} \\
    \bottomrule
\end{tabularx}
\caption{Hyperparameters used for Garage experiments with PEARL}
\label{tab:garage_pearl_hparams}
\vspace{0.75in}
\end{table}

\FloatBarrier


\setttsize{}

\pagebreak

\section{Reward Functions and Single-Task Results}
\label{app:rewardfns}

{\setlength{\mathindent}{0cm}\subsection{Reward Functions}
The variables that will be discussed are the following:
\begin{flalign*}
O \in \mathbb{R}^3 &: \text{object position} \\
h \in \mathbb{R}^3 &: \text{hand/gripper position} \\
t \in \mathbb{R}^3 &: \text{target/goal position} \\
h_l \in \mathbb{R}^3 &: \text{position of the left hand/gripper pad} \\
h_r \in \mathbb{R}^3 & : \text{position of the right hand/gripper pad} \\
O_i \in \mathbb{R}^3 &: \text{initial position of the object} \\
h_i \in \mathbb{R}^3 &: \text{initial position of the hand/gripper} \\
g \in \mathbb{R} & : \text{gripper closed/open amount} &&
\end{flalign*}
The following tolerance function is used frequently:

\[   L(x,b_{min},b_{max},m)=\left\{
\begin{array}{ll}
      1 & b_{min}\leq x\leq b_{max} \\
      S\left(\frac{b_{min}-x}{m}, 0.1\right) & x<b_{min} \\
      S\left(\frac{x-b_{max}}{m}, 0.1\right) & x\geq b_{max}
\end{array} \right. \]

Where  S is defined to be a long-tail sigmoid:

\[S(a_1, a_2)=\left(\left(\frac{1}{a_2 - 1} - 1\right)a_1^2 + 1\right)^{-1}\]

With these basics in place, we define a caging tensor that describes behaviour in an axis which intersects the gripper's actuated fingers (in code, the Y axis):

\[C_{LR}(c_1, c_2) = L\left(
\left| \begin{bmatrix} h_{L,(y)} \\ h_{R,(y)} \end{bmatrix} - o_{(y)}\right|,
c_1,
c_2,
\left| \left| \begin{bmatrix} h_{L,(y)} \\ h_{R,(y)} \end{bmatrix} - o_{i,(y)}\right| - c_2\right|
\right)\]

A similar caging value describes behaviour in the other two axes (in code, X and Z axes):

\[C_P(c_3) = L\left(
\lVert o_{(xz)} - h_{(xz)} \rVert,
0,
c_3,
\lVert o_{i,(xz)} - h_{i,(xz)} \rVert - c_3
\right)\]

These get lumped together as follows ($T_{H_0}$ is the Hamacher product):

\[C(c_1, c_2, c_3)=T_{H_0}(T_{H_0}(C_{LR,(0)}, C_{LR,(1)}), C_P(c_3))\]


The caging reward has two modes: medium density and high density. The arguments $c_1, c_2, c_3$ are passed to $C$

\[R_{cage,dense}(c_1, c_2, c_3)=\left\{
\begin{array}{ll}
      0.5(C + T_{H_0}(C, g)) & C > 0.97 \\
      0.5C & otherwise
\end{array} \right. \]

\[R_{cage}(c_1, c_2, c_3, c_4)=\left\{
\begin{array}{ll}
      0.5(L(\lVert o - h \rVert, 0, c_4, \lVert o - h_i \rVert) + T_{H_0}(C, g)) & C > 0.97 \\
      0.5L(\lVert o - h \rVert, 0, c_4, \lVert o - h_i \rVert) & otherwise
\end{array} \right. \]

In each set of expressions given below, the arguments passed to $R_{cage}$ or $R_{cage,dense}$ correspond to $[c_1,c_2,c_3...]$. The caging reward also considers $[h,h_i,o,o_i]$ as described on the previous page, but these arguments are omitted for brevity.

If computation involves a parameter $A$, understand that $A$ is non-zero $iff$ the Sawyer successfully grasps the object. As such, $A$ serves as a post-grasp guidance term.

Common patterns include $A+T_{H_0}(R_{cage}, L(t-o,...))$, $T_{H_0}(1-g, L(o-h,...))$, and $L(t-o,...)+L(o-h,...)$. As a general rule, rewards for simple tasks consist of summed tolerances, while more difficult tasks add complexity in the form of Hamacher Products. The Hamacher Products combine tolerances, grip effort, and/or $R_{cage}$ to produce a smooth, dense reward.

\pagebreak


\subsubsection{Basketball}
\begin{align*}
A=\mathbb{I}_{\lVert o - h \rVert < 0.035 \And g > 0 \And o_{(z)} - o_{i(z)} > 0.01} \cdot
    (1 + L(\lVert \langle1,1,2\rangle \cdot (t - o) \rVert, 0, 0.08, \lVert \langle1,1,2\rangle \cdot (t - o_i) \rVert))
\end{align*}
\[
R=\left\{
  \begin{array}{ll}
\begin{aligned}
  A + T_{H_0}(& R_{cage,dense}(0.025,0.06,0.005), \\
              & L(\lVert \langle1,1,2\rangle \cdot (t - o) \rVert, 0, 0.08, \lVert \langle1,1,2\rangle \cdot (t - o_i) \rVert))
\end{aligned} & \lVert \langle1,1,2\rangle \cdot (t - o) \rVert\geq 0.08  \\
10 & otherwise \\
  \end{array}\right.
\]

\subsubsection{Button Press Top Down}
\[R=\left\{
\begin{array}{ll}
      5T_{H_0}(
        1-g,
        L(\lVert o - h \rVert, 0, 0.01, \lVert o - h_i \rVert))
        & \lVert o - h \rVert > 0.03 \\
      5T_{H_0}(
        1-g,
        L(\lVert o - h \rVert, 0, 0.01, \lVert o - h_i \rVert)) +
        5L(| t_{(z)} - o_{(z)} |, 0, 0.005, | t_{(z)} - o_{i,(z)} |))
        & otherwise
\end{array} \right. \]

\subsubsection{Button Press Top Down Wall}
\[R=\left\{
\begin{array}{ll}
      5T_{H_0}(
        1-g,
        L(\lVert o - h \rVert, 0, 0.01, \lVert o - h_i \rVert))
        & \lVert o - h \rVert > 0.03 \\
      5T_{H_0}(
        1-g,
        L(\lVert o - h \rVert, 0, 0.01, \lVert o - h_i \rVert)) +
        5L(| t_{(z)} - o_{(z)} |, 0, 0.005, | t_{(z)} - o_{i,(z)} |))
        & otherwise
\end{array} \right. \]

\subsubsection{Button Press}
\[R=\left\{
\begin{array}{ll}
      2T_{H_0}(
        g,
        L(\lVert o - h \rVert, 0, 0.05, \lVert o - h_i \rVert))
        & \lVert o - h \rVert > 0.05 \\
      2T_{H_0}(
        g,
        L(\lVert o - h \rVert, 0, 0.05, \lVert o - h_i \rVert)) +
        8L(| t_{(y)} - o_{(y)} |, 0, 0.005, | t_{(y)} - o_{i,(y)} |))
        & otherwise
\end{array} \right. \]

\subsubsection{Button Press Wall}
\[R=\left\{
\begin{array}{ll}
      2T_{H_0}(
        1-g,
        L(\lVert o - h \rVert, 0, 0.01, \lVert o - h_i \rVert))
        & \lVert o - h \rVert > 0.07 \\
      4 + 2g + 4(L(| t_{(y)} - o_{(y)} |, 0, 0.005, | t_{(y)} - o_{i,(y)} |)^2))
        & otherwise
\end{array} \right. \]

\subsubsection{Coffee Button}
\[R=\left\{
\begin{array}{ll}
      2T_{H_0}(
        g,
        L(\lVert o - h \rVert, 0, 0.05, \lVert o - h_i \rVert))
        & \lVert o - h \rVert > 0.05 \\
      2T_{H_0}(
        g,
        L(\lVert o - h \rVert, 0, 0.05, \lVert o - h_i \rVert)) +
        8L(| t_{(y)} - o_{(y)} |, 0, 0.005, | t_{(y)} - o_{i,(y)} |))
        & otherwise
\end{array} \right. \]

\subsubsection{Coffee Pull}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.04 \And g > 0} \cdot
    (1 + 5L(\lVert \langle2,2,1\rangle \cdot (t - o) \rVert, 0, 0.05, \lVert \langle2,2,1\rangle \cdot (t - o_i) \rVert))
\]
\[R=\left\{
\begin{array}{ll}
    \begin{aligned}
      A + T_{H_0}(
        & R_{cage}(0.02,0.05,0.05,0.04), \\
        & L(\lVert \langle2,2,1\rangle \cdot (t - o) \rVert, 0, 0.05, \lVert \langle2,2,1\rangle \cdot (t - o_i) \rVert))
    \end{aligned}
        & \lVert \langle2,2,1\rangle \cdot (t - o) \rVert\geq 0.05 \\
      10 & otherwise
\end{array} \right. \]

\subsubsection{Coffee Push}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.04 \And g > 0} \cdot
    (1 + 5L(\lVert \langle2,2,1\rangle \cdot (t - o) \rVert, 0, 0.05, \lVert \langle2,2,1\rangle \cdot (t - o_i) \rVert))
\]
\[R=\left\{
\begin{array}{ll}
    \begin{aligned}
      A + T_{H_0}(
        & R_{cage}(0.02,0.05,0.05,0.04), \\
        & L(\lVert \langle2,2,1\rangle \cdot (t - o) \rVert, 0, 0.05, \lVert \langle2,2,1\rangle \cdot (t - o_i) \rVert))
        \end{aligned}
        & \lVert \langle2,2,1\rangle \cdot (t - o) \rVert\geq 0.05 \\
      10 & otherwise
\end{array} \right. \]

\subsubsection{Door Close}
\[R=\left\{
\begin{array}{ll}
    6L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)) +
    3L(\lVert t - h \rVert, 0, 0.012, 0.1 + \lVert h_i - o \rVert)
    & \lVert t - o \rVert\geq 0.05 \\
      10 & otherwise
\end{array} \right. \]

\subsubsection{Door Lock}
\[
R=2T_{H_0}(
    g,
    L(\lVert \langle1,4,2\rangle \cdot (o - h) \rVert, 0, 0.01, \lVert \langle1,4,2\rangle \cdot (o - h_i) \rVert))
+ 8L(|t_{(z)} - o_{i,(z)}|, 0, 0.005, 0.1)
\]

\subsubsection{Door Unlock}
\[R=
\begin{aligned}
2L(&\lVert \langle1,4,2\rangle \cdot (o - h + \langle0, 0.055, 0.07\rangle) \rVert, \\ 
&\;0, \\ 
&\;0.02, \\ 
&\lVert \langle1,4,2\rangle \cdot (o_i - h_i + \langle0, 0.055, 0.07\rangle) \rVert)) + 
8L(|t_{(x)} - o_{i,(x)}|, 0, 0.005, 0.1)
\end{aligned}
\]

\subsubsection{Door Open}
\[
alt = \mathbb{I}_{\lVert h_{(xy)} - o_{(xy)} \lVert > 0.12} \cdot \left(0.4 + 0.04\log\left(\lVert h_{(xy)} - o_{(xy)} \lVert-0.12\right) \right)
\]

\[ready=\left\{
\begin{array}{ll}
T_{H_0}\left(
    L(\lVert h-o-\langle0.05,0.03,-0.01\rangle \rVert, 0, 0.06, 0.5), 
    L(alt-h_{(z)}, 0, 0.01, \frac{alt}{2}),
\right)
& h_{(z)} < alt \\

L(\lVert h-o-\langle0.05,0.03,-0.01\rangle \rVert, 0, 0.06, 0.5)
& otherwise
\end{array} \right. \]

\[R=\left\{
\begin{array}{ll}
2T_{H_0}\left(g, ready\right) +
8\left( 0.2\mathbb{I}_{o_{(\theta)} < 0.03} + 0.8L(o_{(\theta)} + \frac{2\pi}{3}, 0, 0.5, \frac{\pi}{3}) \right)
& |t_{(x)} - o_{(x)}| > 0.08 \\

10 & otherwise
\end{array} \right. \]

\subsubsection{Box Close}
\[
alt = \mathbb{I}_{\lVert h_{(xy)} - o_{(xy)} \lVert > 0.02} \cdot \left(0.4 + 0.04\log\left(\lVert h_{(xy)} - o_{(xy)} \lVert-0.02\right) \right)
\]

\[ready=\left\{
\begin{array}{ll}
T_{H_0}\left(
    L(\lVert h-o \rVert, 0, 0.02, 0.5), 
    L(alt-h_{(z)}, 0, 0.01, \frac{alt}{2}),
\right)
& h_{(z)} < alt \\

L(\lVert h-o \rVert, 0, 0.02, 0.5)
& otherwise
\end{array} \right. \]

\[R=\left\{
\begin{array}{ll}
2T_{H_0}\left(\frac{g+1}{2}, ready\right) +
8\left( 0.2\mathbb{I}_{o_{(z)} > 0.04} + 0.8L(\langle 1, 1, 3 \rangle \lVert t - o \rVert, 0, 0.05, 0.25) \right)
& |t - o| \geq 0.08 \\

10 & otherwise
\end{array} \right. \]

\subsubsection{Drawer Open}
\[
R=5\left(
    L\left(\lVert t - o \rVert, 0, 0.02, 0.2\right) +
    L\left(\lVert (o - h) \cdot \langle3, 3, 1\rangle \rVert, 0, 0.01, \lVert (o_i - h_i) \cdot \langle3, 3, 1\rangle \rVert \right)
    \right)
\]

\subsubsection{Drawer Close}
\[
R=\left\{
    \begin{array}{ll}
        T_{H_0}\left(
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert - 0.05),
            T_{H_0}\left(
                g,
                L(\lVert o - h \rVert, 0, 0.005, \lVert o_i - h_i \rVert - 0.005)
            \right)
        \right)
            & \lVert t - o \rVert > 0.065 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Faucet Close}

\[
R=\left\{
    \begin{array}{ll}
        4L(\lVert o - h \rVert, 0, 0.01, \lVert o_i - h_i \rVert - 0.01) +
        6L(\lVert t - o \rVert, 0, 0.07, \lVert t - o_i \rVert - 0.07)
            & \lVert t - o \rVert > 0.07 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Faucet Open}

\[
R=\left\{
    \begin{array}{ll}
        \begin{aligned}
         (& 4L(\lVert o - h + \langle-.04,0,.03\rangle \rVert, 0, 0.01, \lVert o_i - h_i \rVert - 0.01)  \\
         & + 6L(\lVert t - o + \langle-.04,0,.03\rangle \rVert, 0, 0.07, \lVert t - o_i \rVert - 0.07) )
         \end{aligned}
        & \lVert t - o + \langle-.04,0,.03\rangle \rVert > 0.07 \\
        
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Hand Insert}

\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.02 \And g > 0} \cdot
    (1 + 7L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.015,0.05,0.005),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Pick Place}

\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.02 \And g > 0 \And o_{(z)} > 0.01} \cdot
    (1 + 5L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.015,0.05,0.005),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Pick Out Of Hole}
A funnel-shaped surface guides the gripper as it seeks to grab and lift the object; this prevents the gripper from running into the side of the hole in the table. The height (or "altitude") of this surface is given by $alt$ since the variables $h$ and $z$ are already used.
\[
alt = \mathbb{I}_{\lVert h_{(xy)} - o_{i,(xy)} \lVert > 0.03} \cdot
    \left(0.15 + 0.015\log\left(\lVert h_{(xy)} - o_{i,(xy)} \lVert - 0.03\right)\right)
\]
\[
\begin{aligned}
A=&\mathbb{I}_{\lVert o - h \rVert < 0.04 \And g < 0.33 \And o_{(z)} - o_{i,(z)} > 0.02} \\ 
& \bullet
    \left(1 + 5T_{H_0}\left(
        \begin{aligned}
        L(&\lVert t - o \rVert, 0, 0.02, \lVert t - o_i \rVert), \\
         max(&\mathbb{I}_{h_{(z)} > alt}, L(alt - h_{(z)}, 0, 0.01, 0.02))\\
        \end{aligned}
    \right)\right)
\end{aligned}
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.015,0.05,0.005),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Plate Slide Back Side}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.07 \And h_{(z)} \leq 0.03}
\]
\[
R=\left\{
    \begin{array}{ll}
        A \cdot
        (2 + 7L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)) + 
        (1-A) \cdot
        1.5L(\lVert o - h \rVert, 0, 0.05, \lVert o_i - h_i \rVert - 0.05)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Plate Slide Back}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.07 \And h_{(z)} \leq 0.03}
\]
\[
R=\left\{
    \begin{array}{ll}
        A \cdot
        (2 + 7L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)) + 
        (1-A) \cdot
        1.5L(\lVert o - h \rVert, 0, 0.05, \lVert o_i - h_i \rVert - 0.05)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Plate Slide Side}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.07 \And h_{(z)} \leq 0.03}
\]
\[
R=\left\{
    \begin{array}{ll}
        A \cdot
        (2 + 7L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)) + 
        (1-A) \cdot
        1.5L(\lVert o - h \rVert, 0, 0.05, \lVert o_i - h_i \rVert - 0.05)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Plate Slide}
\[
R=\left\{
    \begin{array}{ll}
    8T_{H_0}(
        L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert),
        L(\lVert o - h \rVert, 0, 0.05, \lVert o_i - h_i \rVert)
    ) & \lVert t - o \rVert \geq 0.05 \\
    10 & otherwise
    \end{array}
\right.
\]

\subsubsection{Handle Press Side}
\[
R=\left\{
    \begin{array}{ll}
    10T_{H_0}(
        L(| t_{(z)} - o_{(z)} |, 0, 0.05, | t_{(z)} - o_{i,(z)} |),
        L(\lVert o - h \rVert, 0, 0.02, \lVert o_i - h_i \rVert - 0.02)
    ) & \lVert t - o \rVert > 0.05 \\
    10 & otherwise
    \end{array}
\right.
\]

\subsubsection{Handle Press}
\[
R=\left\{
    \begin{array}{ll}
    10T_{H_0}(
        L(| t_{(z)} - o_{(z)} |, 0, 0.05, | t_{(z)} - o_{i,(z)} |),
        L(\lVert o - h \rVert, 0, 0.02, \lVert o_i - h_i \rVert - 0.02)
    ) & \lVert t - o \rVert > 0.05 \\
    10 & otherwise
    \end{array}
\right.
\]

\subsubsection{Handle Pull}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.035 \And g > 0 \And o_{(z)} - o_{i,(z)} > 0.01} \cdot
    (1 + 5L(| t_{(z)} - o_{(z)} |, 0, 0.05, | t_{(z)} - o_{i,(z)} |))
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.022,0.05,0.01),
            L(| t_{(z)} - o_{(z)} |, 0, 0.05, | t_{(z)} - o_{i,(z)} |)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Handle Pull Side}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.035 \And g > 0 \And o_{(z)} - o_{i,(z)} > 0.01} \cdot
    (1 + 5L(| t_{(z)} - o_{(z)} |, 0, 0.05, | t_{(z)} - o_{i,(z)} |))
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.032,0.06,0.01),
            L(| t_{(z)} - o_{(z)} |, 0, 0.05, | t_{(z)} - o_{i,(z)} |)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Reach}
\[
R=10L(\lVert t - h \rVert, 0, 0.05, \lVert t - h_i \rVert)
\]

\subsubsection{Reach Wall}
\[
R=10L(\lVert t - h \rVert, 0, 0.05, \lVert t - h_i \rVert)
\]

\subsubsection{Push}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.02 \And g > 0}
\]
\[
R=\left\{
    \begin{array}{ll}
    (A+1) \cdot R_{cage,dense}(0.015,0.05,0.005) +
    A \cdot (1 + 5L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
        & \lVert t - o \rVert > 0.05 \\
    10
        & otherwise
    \end{array}
\right.
\]

\subsubsection{Sweep Into Goal}
Note: This technically uses a $R_{cage,dense}$ function with slightly different margin parameters than the one described above (they are constant rather than dynamic), but the behaviour is mostly the same.
\[
R=\left\{
    \begin{array}{ll}
    \begin{aligned}
        & (2R_{cage,dense}(0.02,0.05,0.01) \\
        & + 2T_{H_0}\left(
            R_{cage,dense}(0.02,0.05,0.01),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right))
    \end{aligned}
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Sweep}
Note: This technically uses a $R_{cage,dense}$ function with slightly different margin parameters than the one described above (they are constant rather than dynamic), but the behaviour is mostly the same.
\[
R=\left\{
    \begin{array}{ll}
        \begin{aligned}
        & (2R_{cage,dense}(0.02,0.05,0.01) \\
        & + 2T_{H_0}\left(
            R_{cage,dense}(0.02,0.05,0.01),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right))
        \end{aligned}
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Push Back}
Note: This technically uses a $R_{cage,dense}$ function with slightly different margin parameters than the one described above (they are constant rather than dynamic), but the behaviour is mostly the same.
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.01 \And 0 < g < 0.55 \And \lVert t - o_i \rVert - \lVert t - o \rVert > 0.01} \cdot
    (1 + 5L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.01,0.05,0.01),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Window Open}
\[
R=10T_{H_0}(
    L(| t_{(x)} - o_{(x)} |, 0, 0.05, | t_{(x)} - o_{i,(x)} |),
    L(\lVert o - h \rVert, 0, 0.02, \lVert o_i - h_i \rVert - 0.02)
)
\]

\subsubsection{Window Close}
\[
R=10T_{H_0}(
    L(| t_{(x)} - o_{(x)} |, 0, 0.05, | t_{(x)} - o_{i,(x)} |),
    L(\lVert o - h \rVert, 0, 0.02, \lVert o_i - h_i \rVert - 0.02)
)
\]

\subsubsection{Dial Turn}
\[
\begin{aligned}
R=10T_{H_0}(
    & L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert - 0.05) , \\
    & \begin{aligned}
        T_{H_0}(& g, \\
        L(&\lVert o - h + \langle0.05,0.02,0.09\rangle \rVert , \\ 
        & 0, \\
        &0.005, \\
        &\lVert o_i - h_i + \langle0.05,0.02,0.09\rangle \rVert - 0.005)))
    \end{aligned}
\end{aligned}
\]

\subsubsection{Bin Picking}
Two funnel-shaped surfaces guide the gripper as it seeks to carry the object between the two bins; this prevents the gripper from running into the side of the bins. The height (or "altitude") of this surface is given by $alt$ since the variables $h$ and $z$ are already used.
\[
\begin{aligned}
alt = min (&\mathbb{I}_{\lVert h_{(xy)} - o_{i,(xy)} \lVert > 0.03} \cdot
    \left(0.2 + 0.02\log \left(\lVert h_{(xy)} - o_{i,(xy)} \lVert - 0.03\right)\right), \\
    & \mathbb{I}_{\lVert h_{(xy)} - t_{(xy)} \lVert > 0.03} \cdot
    \left(0.2 + 0.02\log\left(\lVert h_{(xy)} - t_{(xy)} \lVert - 0.03\right)\right))
\end{aligned}
\]
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.04 \And g < 0.43 \And o_{(z)} - o_{i,(z)} > 0.02} \cdot
    \left(1 + 5T_{H_0}\left(
        \begin{aligned}
        &L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert), \\
        & max(\mathbb{I}_{h_{(z)} > alt}, L(alt - h_{(z)}, 0, 0.01, 0.05))\\
        \end{aligned}
    \right)\right)
\]
\[
R=\left\{
    \begin{array}{ll}
        A + 
        T_{H_0}\left(
            R_{cage,dense}(0.015,0.05,0.01),
            L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert)
        \right)
            & \lVert t - o \rVert > 0.05 \\
        10
            & otherwise
    \end{array}
\right.
\]

\subsubsection{Assembly}
In addition to the components described below, the assembly reward is weighted by how level the object is (tilted object quaternions are penalized).

\[
alt = \mathbb{I}_{\lVert t_{(xy)} - o_{(xy)} \lVert > 0.02} \cdot \left(0.4 + 0.04\log\left(\lVert t_{(xy)} - o_{(xy)} \lVert-0.02\right) \right)
\]

\[
A=0.1\mathbb{I}_{o_{(z)} > 0.02 or \lVert t_{(xy)} - o_{(xy)} \lVert < 0.02} + 
    0.9L(\langle 1, 1, 3 \rangle \langle t_{(x)} - o_{(x)}, t_{(y)} - o_{(y)}, alt - o_{(z)} \rangle, 0, 0.02, 0.4)
\]

\[R=\left\{
\begin{array}{ll}
2R_{cage,dense}(0.015, 0.02, 0.01) + 8A
& |t_{(x)} - o_{(x)}| > 0.02 \\

10 & otherwise
\end{array} \right. 
\]

\subsubsection{Disassemble}
In addition to the components described below, the disassemble reward is weighted by how level the object is (tilted object quaternions are penalized).

\[R=\left\{
\begin{array}{ll}
2R_{cage,dense}(0.015, 0.02, 0.01) +
6 \left( 0.1\mathbb{I}_{o_{(z)} > 0.02} + 0.9L(\lVert t - o \rVert, 0, 0.02, 0.2) \right)
& o_{(z)} > t_{(z)} \\

10 & otherwise
\end{array} \right. 
\]

\subsubsection{Hammer}
In addition to the components described below, the hammer reward is weighted by how level the object is (tilted object quaternions are penalized).

\[R=\left\{
\begin{array}{ll}
2R_{cage,dense}(0.015, 0.02, 0.01) +
6 \left( 0.1\mathbb{I}_{o_{(z)} > 0.02} + 0.9L(\lVert t - o \rVert, 0, 0.02, 0.2) \right)
& |o_{(y)} - o_{i,(y)}| > 0.09 \\

10 & otherwise
\end{array} \right. 
\]

\subsubsection{Lever Pull}

\[
\begin{aligned}
R=10T_{H_0}(
    &L\left(\lVert t - o \rVert, 0, 0.04, \lVert t - o_i \rVert\right), \\
    &L\left(
        \langle4,1,4\rangle \cdot (h - o + \langle0,0.055,0.07\rangle),
        0,
        0.02,
        \langle4,1,4\rangle \cdot (h_i - o_i + \langle0,0.055,0.07\rangle)
    \right)
)\end{aligned}
\]

\subsubsection{Stick Push}
Note: \textit{a} is the second object in the environment, which in this case is a thermos.
\[
R=\left\{
\begin{array}{ll}
\begin{aligned}
& 2 + 5L(\lVert t-o \rVert, 0, 0.12, \lVert t-o_i \rVert - 0.12) \\
& + 3L(\lVert t-a \rVert, 0, 0.12, \lVert t-a_i \rVert - 0.12)\\
\end{aligned}
& \lVert h-o \rVert < 0.02, g>0, o_{(z)}-o_{i,(z)}>0.01, \lVert t-a \rVert > 0.12 \\

10
& \lVert h-o \rVert < 0.02, g>0, o_{(z)}-o_{i,(z)}>0.01, \lVert t-a \rVert \leq 0.12 \\

R_{cage,dense}(0.04,0.05,0.01)
& otherwise
\end{array} \right. \]

\subsubsection{Stick Pull}
Note: \textit{a} is the second object in the environment, which in this case is a thermos.
\textit{in} is a condition involving lots of vector offsets from the object observations. It indicates whether the stick is inserted into the thermos' handle or not. The variable \textit{stick\_in\_place}, and \textit{stick\_grabbed} have also been defined so that the reward function fits on one page.

$\textit{stick\_in\_place} = L(\lVert (o-a) \cdot \langle 1,1,2 \rangle \rVert, 0, 0.12, \lVert (o_i-a_i) \cdot \langle 1,1,2 \rangle \rVert)$

$\textit{stick\_grabbed} = \lVert h-o \rVert < 0.02, g>0, o_{(z)}-o_{i,(z)}>0.01$

\begin{align*}
R=\left\{\begin{array}{ll} 1 + 6 \cdot stick\_in\_place &  stick\_grabbed, \lnot in, \lVert t-a \rVert > 0.12 \\
\begin{aligned}
& 6
+ stick\_in\_place
+ 2L(\lVert t-o \rVert, 0, 0.12, \lVert t-o_i \rVert) \\ 
& + L(\lVert t-a \rVert, 0, 0.12, \lVert t-a_i \rVert)
\end{aligned}
& stick\_grabbed, in, \lVert t-a \rVert > 0.12 \\
10
& stick\_grabbed, in, \lVert t-a \rVert \leq 0.12 \\
T_{H_0}(R_{cage,dense}(0.014,0.05,0.01), stick\_in\_place)
& otherwise
\end{array} \right.
\end{align*}

\subsubsection{Shelf Place}
In addition to the components described below, the shelf-place reward includes negative components that help avoid collision with the shelf.
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.025 \And g > 0 \And o_{(z)}>0.01} \cdot
    (1 + 5L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
\]
\[R=\left\{
\begin{array}{ll}
      A + T_{H_0}(
        R_{cage}(0.01,0.02,0.05,0.01),
        L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
        & \lVert t - o \rVert\geq 0.05 \\
      10 & otherwise
\end{array} \right. \]

\subsubsection{Peg Insert}
In addition to the components described below, the peg-insert reward includes negative components that help avoid collision with the hole/box into which the peg gets inserted.
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.08 \And g > 0 \And o_{(z)}>0.01} \cdot
    (1 + 5L(\lVert \langle2,2,1\rangle \cdot (t - o) \rVert, 0, 0.05, \lVert \langle2,2,1\rangle \cdot (t - o_i) \rVert))
\]
\[R=\left\{
\begin{array}{ll}
      \begin{aligned}
      A + T_{H_0}(&R_{cage}(0.0075,0.01,0.03,0.005),\\
      
        &L(\lVert \langle2,2,1\rangle \cdot (t - o) \rVert, 0, 0.05, \lVert \langle2,2,1\rangle \cdot (t - o_i) \rVert))
        \end{aligned}
        & \lVert \langle2,2,1\rangle \cdot (t - o) \rVert\geq 0.07 \\
      10 & otherwise
      
\end{array} \right. \]

\subsubsection{Peg Unplug}
\[
A=\mathbb{I}_{\lVert o - h \rVert < 0.035 \And g > 0.5 \And o_{(x)}-o_{i,(x)}>0.015} \cdot
    (1 + 5L(\lVert t - o \rVert, 0, 0.05, \lVert t - o_i \rVert))
\]
\[R=\left\{
\begin{array}{ll}
      A + 2R_{cage}(0.01,0.025,0.05,0.005)
        & \lVert t - o \rVert\geq 0.07 \\
      10 & otherwise
\end{array} \right. \]

\subsubsection{Soccer}
In addition to the components described below, the soccer reward function includes parameters to fine-tune movements near the goal line.
\[R=\left\{
\begin{array}{ll}
      \begin{aligned}
      &3R_{cage}(0.013,0.023,0.05,0.005) \\
      &+ 6.5L(\lVert \langle3,1,1\rangle(t - o) \rVert, 0, 0.07, \lVert
      \langle3,1,1\rangle(t - o_i) \rVert)
      \end{aligned}
        & \lVert \langle3,1,1\rangle(t - o) \rVert\geq 0.07 \\
      10 & otherwise
\end{array} \right. \]

\subsubsection{Pick Place Wall}
The pick-place-wall reward is essentially two pick-place rewards stacked on top of one another. The first pick-place reward incentivizes movement to a neutral midpoint above the wall (to avoid running into it). The second pick-place reward incentivizes movement to the target position. The math is such that there is no discontinuity between the two reward components.

\subsubsection{Push Wall}
The push-wall reward is the same as the pick-place-wall reward, but without incentives to pick up the object. Additionally, the midpoint is configured to be next to the wall (so that policies push the object around the wall) rather than above the wall.
}


\begin{table}[h]
    \centering
    \begin{tabular}{lc}
\toprule
\footnotesize Task & Success Metric  \\
\midrule
faucet-open &  $\1_{\|o - t\|_2 < 0.07}$\\
sweep & $\1_{\|o - t\|_2 < 0.05}$\\
pick-out-of-hole & $\1_{\|o - t\|_2 < 0.07}$\\
faucet-close & $\1_{\|o - t\|_2 < 0.07}$\\
push & $\1_{\|o - t\|_2 < 0.05}$\\
stick-push & $\1_{\|o - t\|_2 < 0.12} \;and\; \1_{grasped(o)}$\\
coffee-button & $\1_{\|o - t\|_2 < 0.02}$\\
handle-pull-side & $\1_{\|o - t\|_2 < 0.08}$\\
basketball & $\1_{\|o - t\|_2 < 0.08}$\\
stick-pull & $\1_{\|o - t\|_2 < 0.12} \;and\; \1_{grasped(o)}$\\
sweep-into & $\1_{\|o - t\|_2 < 0.05}$\\
disassemble & $\1_{o_z - o_z initial \;>\; 0.15}$\\
assembly & $\1_{\|o - t\|_2 < 0.02} \;and\; \1_{g_z-o_z > 0}$\\
shelf-place & $\1_{\|o - t\|_2 < 0.07}$\\
coffee-push & $\1_{\|o - t\|_2 < 0.07}$\\
handle-press-side & $\1_{\|o - t\|_2 < 0.02}$\\
hammer & $\1_{nail \;travels\; >\ 0.09 \;into \;wood \;block}$\\
plate-slide & $\1_{\|o - t\|_2 < 0.07}$\\
plate-slide-side & $\1_{\|o - t\|_2 < 0.07}$\\
button-press-wall & $\1_{\|o - t\|_2 < 0.03}$\\
handle-press & $\1_{\|o - t\|_2 < 0.02}$\\
handle-pull & $\1_{\|o - t\|_2 < 0.05}$\\
soccer & $\1_{\|o - t\|_2 < 0.07}$\\
plate-slide-back-side & $\1_{\|o - t\|_2 < 0.07}$\\
plate-slide-back & $\1_{\|o - t\|_2 < 0.07}$\\
drawer-close & $\1_{\|o - t\|_2 < 0.055}$\\
reach & $\1_{\|o - t\|_2 < 0.05}$\\
button-press-topdown-wall & $\1_{\|o - t\|_2 < 0.02}$\\
reach-wall & $\1_{\|o - t\|_2 < 0.05}$\\
peg-insert-side & $\1_{\|o - t\|_2 < 0.07}$\\
push-wall & $\1_{\|o - t\|_2 < 0.07}$\\
pick-place-wall & $\1_{\|o - t\|_2 < 0.07}$\\
button-press & $\1_{\|o - t\|_2 < 0.02}$\\
button-press-topdown & $\1_{\|o - t\|_2 < 0.02}$\\
pick-place & $\1_{\|o - t\|_2 < 0.07}$\\
push-back & $\1_{\|o - t\|_2 < 0.07}$\\
coffee-pull & $\1_{\|o - t\|_2 < 0.07}$\\
peg-unplug-side & $\1_{\|o - t\|_2 < 0.07}$\\
dial-turn & $\1_{\|o - t\|_2 < 0.07}$\\
lever-pull & $\1_{rad(o) - rad(t) < \pi / 24}$\\
window-close & $\1_{\|o - t\|_2 < 0.05}$\\
window-open & $\1_{\|o - t\|_2 < 0.05}$\\
door-open & $\1_{\|o - t\|_2 < 0.08}$\\
door-close & $\1_{\|o - t\|_2 < 0.08}$\\
drawer-open & $\1_{\|o - t\|_2 < 0.03}$\\
hand-insert & $\1_{\|o - t\|_2 < 0.05}$\\
box-close & $\1_{\|o - t\|_2 < 0.08}$\\
door-lock & $\1_{\|o - t\|_2 < 0.02}$\\
door-unlock & $\1_{\|o - t\|_2 < 0.02}$\\
bin-picking & $\1_{\|o - t\|_2 < 0.05}$\\
\bottomrule
\end{tabular}
\vspace{0.2cm}
    \caption{A list of success metrics used for each of the Meta-World tasks. All units are in meters.}
    \label{tbl:task_metrics}
\end{table}


