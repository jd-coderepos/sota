

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{amsmath}

\usepackage{graphicx}
\usepackage{bm, upgreek}
\usepackage{amssymb}
\usepackage{array, booktabs}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{color}
\usepackage{soul}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{pifont}\usepackage{comment}
\usepackage{cleveref}
\usepackage[ruled,vlined]{algorithm2e}



\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{blue-green}{rgb}{0.0, 0.87, 0.87}
\definecolor{celestialblue}{rgb}{0.29, 0.59, 0.82}
\definecolor{cerulean}{rgb}{0.0, 0.48, 0.65}
\definecolor{ceruleanblue}{rgb}{0.16, 0.32, 0.75}

\newcommand{\Ours}{DensePhrases}
\newcommand{\ours}{DensePhrases}
\newcommand{\sect}[1]{Section \ref{#1}}
\newcommand{\conc}{\mathbin{\|}}
\newcommand{\squad}{SQuAD}
\newcommand{\retrievalonly}{retriever-only}
\newcommand{\readeronly}{reader-only}

\newcommand{\documentset}{\mathcal{D}}
\newcommand{\phraseset}{\mathcal{S}(\mathcal{D})}
\newcommand{\phraseinp}{\mathcal{S}(p)}
\newcommand{\wordset}{\mathcal{W}(\mathcal{D})}
\newcommand{\phrasedump}{\mf{H}}
\newcommand{\traincorpus}{\mathcal{C}}
\newcommand{\lm}{\mathcal{M}}

\newcommand\sys[1]{\textsc{#1}}
\newcommand\ti[1]{\textit{#1}}
\newcommand\ts[1]{\textsc{#1}}
\newcommand\tf[1]{\textbf{#1}}
\newcommand\ttt[1]{\texttt{#1}}
\newcommand\mf[1]{\mathbf{#1}}

\newcommand{\eu}{\mathrm{e}}
\newcommand{\su}{\mathrm{s}}

\newcommand{\am}{{\bm a}}
\newcommand{\dm}{{\bm d}}
\newcommand{\xm}{{\bm x}}
\newcommand{\sm}{{\bm s}}
\newcommand{\wm}{{\bm w}}
\newcommand{\qm}{{\bm q}}

\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Db}{\mathbf{D}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\ab}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\cb}{\mathbf{c}}
\newcommand{\db}{\mathbf{d}}
\newcommand{\eb}{\mathbf{e}}
\newcommand{\fb}{\mathbf{f}}
\newcommand{\kb}{\mathbf{k}}
\newcommand{\lb}{\mathbf{l}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\ob}{\mathbf{o}}
\newcommand{\ib}{\mathbf{i}}
\newcommand{\pb}{\mathbf{p}}
\newcommand{\qb}{\mathbf{q}}
\newcommand{\rb}{\mathbf{r}}
\newcommand{\sbo}{\mathbf{s}}
\newcommand{\wb}{\mathbf{w}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Kb}{\mathbf{K}}
\newcommand{\Lb}{\mathbf{L}}
\newcommand{\Sb}{\mathbf{S}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\alphab}{\mathbf{\alpha}}
\newcommand{\betab}{\mathbf{\beta}}
\newcommand{\psib}{\mathbf{\psi}}
\newcommand{\phib}{\mathbf{\phi}}
\newcommand{\gammab}{\mathbf{\gamma}}
\newcommand{\thetab}{\mathbf{\theta}}
\newcommand{\Thetab}{\mathbf{\Theta}}
\newcommand{\kappab}{\mathbf{\kappa}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\II}{\mathbb{I}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\gpu}[1]{\textcolor{brickred}{#1}}
\newcommand{\cpu}[1]{\textcolor{ceruleanblue}{#1}}
\newcommand{\smalltext}[1]{{\scriptstyle \text{TEXT}}(#1)}
\newcommand{\tinytext}[1]{{\scriptscriptstyle \text{TEXT}}(#1)}

\newcommand{\dist}{\mathbf{\phi}}

\newcommand{\draftonly}[1]{#1}
\renewcommand{\draftonly}[1]{}

\newcommand{\draftcomment}[3]{\draftonly{\textcolor{#2}{{{[#1: #3]}}}}}
\newcommand{\todo}[1]{\draftcomment{TODO}{red}{#1}}
\newcommand{\jinhyuk}[1]{\draftcomment{Jinhyuk}{blue}{#1}}
\newcommand{\mujeen}[1]{\draftcomment{Mujeen}{brown}{#1}}
\newcommand{\danqi}[1]{\draftcomment{Danqi}{teal}{#1}}

 
\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Learning Dense Representations of Phrases at Scale}


\author{
  Jinhyuk Lee\quad Mujeen Sung \quad Jaewoo Kang \quad Danqi Chen\\
  Korea University\quad Princeton University\\
  \texttt{\{jinhyuk\_lee,mujeensung,kangj\}@korea.ac.kr} \\
  \texttt{danqic@cs.princeton.edu} \\}

\date{}

\begin{document}
\maketitle



\begin{abstract}











Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference~\citep{seo2019real}.
However, current phrase retrieval models heavily depend on their sparse representations while still underperforming retriever-reader approaches.
In this work, we show for the first time that we can learn \ti{dense phrase representations} alone that achieve much stronger performance in open-domain QA. Our approach includes (1) learning query-agnostic phrase representations via question generation and distillation; (2) novel negative-sampling methods for global normalization; (3) query-side fine-tuning for transfer learning.
On five popular QA datasets, our model \ti{DensePhrases} improves previous phrase retrieval models by -- absolute accuracy and matches the performance of state-of-the-art retriever-reader models.
Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs.
Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing {DensePhrases} as a dense knowledge base for downstream tasks.\footnote{Our code is available at \url{https://github.com/jhyuklee/DensePhrases}.}








\end{abstract}
 

\section{Introduction}
\label{sec:intro}









Open-domain question answering (QA) aims to provide answers to natural-language questions using a large text corpus~\citep{voorhees1999trec,ferrucci2010building,chen2020open}.
While a dominating approach is a two-stage retriever-reader approach~\citep{chen2017reading,lee2019latent,guu2020realm,karpukhin2020dense},
we focus on a recent new paradigm solely based on \textit{phrase retrieval}~\citep{seo2018phrase,seo2019real,lee2020contextualized}.
Phrase retrieval highlights the use of phrase representations and finds answers purely based on the similarity search in the vector space of phrases.
Without relying on an expensive reader model (e.g., a 12-layer BERT model~\cite{devlin2019bert}) for processing text passages, it has demonstrated great runtime efficiency at inference time.
Table~\ref{tab:category} compares the two approaches in detail.









Despite great promise, it remains a formidable challenge to build effective dense representations for each single phrase in a large corpus (e.g., Wikipedia). First, since phrase representations need to be decomposable from question representations, they are often less expressive than query-dependent representations---this challenge brings the \textit{decomposability gap} as stated in~\citep{seo2018phrase,seo2019real}.
Second, it requires retrieving answers correctly out of {ten billions} of phrases, which are more than four orders of magnitude larger than the number of documents in Wikipedia.
Consequently, this approach heavily relies on sparse representations for locating relevant documents and paragraphs while still falling behind retriever-reader models~\citep{seo2019real,lee2020contextualized}.





\begin{table*}[t]
\label{table:open_qa_results}
\begin{center}
\centering
\resizebox{2.0\columnwidth}{!}{\begin{tabular}{llccccc}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Sparse?}} & {\textbf{Storage}} & {\textbf{\#Q/sec}} & \textbf{NQ}  & \textbf{SQuAD} \\
& & & (GB) & (\gpu{GPU}, \cpu{CPU}) & (Acc) & (Acc) \\
\midrule
\multirow{5}{*}{Retriever-Reader} & DrQA~\citep{chen2017reading} & \cmark & 26 & \gpu{1.8}, \cpu{0.6} & - & 29.8 \\
& BERTSerini~\citep{yang2019end} & \cmark & 21 & \gpu{2.0}, \cpu{0.4} & - & 38.6 \\
& ORQA~\citep{lee2019latent} & \xmark & 18 & \gpu{8.6}, \cpu{1.2} & 33.3 & 20.2 \\
& REALM~\citep{guu2020realm} & \xmark & 18 & \gpu{8.4}, \cpu{1.2} & 40.4 & -\\
& DPR-multi~\citep{karpukhin2020dense}& \xmark & 76 & \gpu{0.9}, \cpu{0.04} & 41.5 & 24.1 \\
\midrule
\multirow{3}{*}{Phrase Retrieval} & DenSPI~\citep{seo2019real} & \cmark & 1,200 & \gpu{2.9}, \cpu{2.4} & 8.1 & 36.2\\
& DenSPI + Sparc~\citep{lee2020contextualized} & \cmark & 1,547 & \gpu{2.1}, \cpu{1.7} & 14.5 & 40.7 \\
& \textbf{\ours~(Ours)} & \xmark & 320 & \gpu{20.6},	\cpu{13.6} & 40.9 & 38.0 \\
\bottomrule
\end{tabular}
}
\end{center}\vspace{-0.1cm}
\caption{Retriever-reader and phrase retrieval approaches for open-domain QA. The \textit{retriever-reader} approach retrieves a small number of relevant documents from which the answers are extracted.
The \textit{phrase retrieval} approach retrieves an answer out of ten billions of phrase representations pre-indexed from the entire Wikipedia.\footnotemark~NQ, SQuAD: the accuracy is measured on the test sets of Natural Questions~\cite{kwiatkowski2019natural} and SQuAD~\cite{rajpurkar2016squad} in the open-domain setting.
}\vspace{-0.3cm}
\label{tab:category}
\end{table*}







 
In this work, we investigate whether we can build fully dense phrase representations at scale for open-domain QA.
First, we attribute the cause of the decomposability gap to the sparsity of training data.
We close this gap by generating questions for every answer phrase, as well as distilling knowledge from query-dependent models (\Cref{sec:piqa}). {Second}, we use negative sampling strategies such as in-batch negatives~\citep{henderson2017efficient,karpukhin2020dense}, to approximate global normalization.
We also propose a novel method called \ti{pre-batch negatives}, which leverages preceding mini-batches as negative examples to compensate the need of large-batch training (\Cref{sec:normalization}). Lastly, for task-specific adaptation of our model, we propose query-side fine-tuning that drastically improves the performance of phrase retrieval without re-building billions of phrase representations (\Cref{sec:qsft}).












Consequently, all these improvements enable us to learn a much stronger phrase retrieval model, without the use of \ti{any} sparse representations. We evaluate our final model, \ti{DensePhrases}, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models~\citep{seo2019real,lee2020contextualized}---a 15\% to 25\% absolute improvement in most datasets. Our model also matches performance of state-of-the-art retriever-reader models~\citep{guu2020realm,karpukhin2020dense}.
Because of the removal of sparse representations and careful design choices,  we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as improving the throughput by processing more than 10 questions per second on CPUs.







Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. As such, it can be integrated into other knowledge-intensive NLP tasks beyond question answering.
To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without the need for re-building billions of phrase representations.
With fine-tuning on a small number of subject-relation-object triples on the query representations alone, we achieve state-of-the-art performance on two slot filling tasks~\citep{petroni2020kilt}, using only 5\% of the training data.


\footnotetext[1]{Since we benchmark each model with a batch size of  as detailed in~\Cref{apdx:server}, the speedup in previous phrase retrieval models~\cite{seo2019real,lee2020contextualized}  is smaller than previously claimed, although they still performs much faster on CPUs.}








 

\section{Background}
\label{sec:background}







\begin{comment}
\danqi{
\begin{itemize}
\end{itemize}
}
\end{comment}

\subsection{Open-domain QA}

We first formulate the task of open-domain question answering for a set of  documents .
We follow the recent work~\cite{chen2017reading,lee2019latent} and treat all of English Wikipeida as , hence .
However, most approaches---including ours---are generic and could be applied to other collections of documents.

The task aims to provide an answer  for the input question  based on .
In this work, we focus on the extractive QA setting, where each answer is a segment of text, or a \ti{phrase}, that can be found in .
Denote the set of phrases in  as  and each phrase  consists of contiguous words  in its document .
In practice, we consider all the phrases up to  words in  and  comprises a large number of  phrases.
An extractive QA system returns a phrase  and  is a scoring function. The system finally maps  to an answer string : . Evaluation is typically done by comparing the predicted answer  with a gold answer  (after normalization).



















\paragraph{Retriever-reader}
A dominating paradigm in open-domain QA is the retriever-reader approach~\cite{chen2017reading,lee2019latent,karpukhin2020dense}, which leverages a first-stage document retriever () and only reads top  () documents for finding the answer with a reader model (). The scoring function  can be decomposed as:

where  and if , the score will be 0.
The model can be easily adapted from documents to passages and has been studied extensively~\cite{yang2019end,wang2019multi}.
However, this approach suffers from error propagation when incorrect documents are retrieved and can be slow as it usually requires running an expensive reader model on every retrieved document or passage at inference time.




\begin{comment}
\paragraph{Reader Only}
A second paradigm, the reader only approach~\citep{roberts2020much} (also known as a closed-book models\jinhyuk{add EaE?} merely depends on parameters of language models () to store all the factual knowledge:

This approach requires gigantic language models (e.g., T5 models with 11G parameters) to achieve competitive performance. It is also known that these models perform more question memorization than generalization~\cite{lewis2020question}.
\end{comment}



\paragraph{Phrase retrieval}
\citet{seo2019real} introduce the phrase retrieval approach that encodes phrase and question representations \textit{independently} and performs similarity search over the phrase representations to find an answer.
Their scoring function  is computed as follows:
\vspace{-0.5em}

where  and  denote the phrase encoder and the question encoder respectively.
As  and  representations are decomposable, it can support maximum inner product search (MIPS) and improve the efficiency of open-domain QA models.
Previous approaches~\cite{seo2019real,lee2020contextualized} leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: \footnote{\newcite{seo2019real} use sparse representations of both paragraphs and documents  and \newcite{lee2020contextualized} use contextualized sparse representations conditioned on the phrase.}
However, since the sparse vectors are difficult to parallelize with dense vectors, their method is not purely implemented as MIPS in practice while requiring more storage.
The goal of this work is to only use the dense representations, i.e., , which can model  solely with MIPS, as well as close the gap in performance.


Although we focus on the extractive QA setting, recent works propose to use a generative model as the reader~\citep{lewis2020retrieval,izacard2020leveraging}, or learn a closed-book QA model~\citep{roberts2020much}, which predicts an answer directly without using an external knowledge source. The extractive setting provides two advantages: first, the model directly locates the source of the answer, which is more interpretable, and second, phrase-level knowledge retrieval can be uniquely adapted to other NLP tasks as we show in Section~\ref{sec:slot_filling}.











\begin{figure*}[t]
\begin{center}
\includegraphics[height=6.5cm]{figures/overview_figure.pdf}
\end{center}\vspace{-0.2cm}
\caption{An overview of \ours.  (a) We learn a query-agnostic reading comprehension model (\Cref{sec:piqa}), as well as normalize phrase representations with an extensive use of negative examples (\Cref{sec:normalization}). (b) With the trained phrase encoder, we build a MIPS index for the phrase representations of the entire text corpus  of  tokens in total (\Cref{sec:optimization}). (c) Our query-side fine-tuning further optimizes the question encoder and can also adapt our model to new types of questions such as  (\Cref{sec:qsft}). (d) During inference, we search for start and end positions with MIPS (\Cref{sec:optimization}).
\todo{Single-Passage  Single-passage, Query-Side Fine-Tuning  Query-side Fine-tuning}
}\vspace{-0.2cm}\label{fig:sample}
\end{figure*}


 
\subsection{Technical Challenges}


Despite an appealing approach, phrase retrieval poses several key technical challenges.
The first challenge is the decomposition constraint between question and phrase encoders as stated in Equation~\eqref{eqn:decomp}, which brings a significant degradation of performance.
While a similar problem is observed in learning dual encoders for passage representations~\citep{humeau2019poly,khattab2020colbert}, phrase representations are even more difficult to learn due to the fine-grained representations.
The second challenge arises from the scale.
Compared to 5-million documents, or 21-million text blocks as used in previous work, each phrase representation has to be properly normalized over 60 billions of dense representations.
This normalization problem is also implied in Equation~\eqref{eqn:decomp} that  is defined over all the phrases in .
Lastly, since it is computationally expensive to build billions of phrase representations at the Wikipedia scale, it is prohibitive to update the phrase representations once they are obtained.
As a result, current phrase retrieval models often rely on their zero-shot ability~\citep{lee2020contextualized}. 

























In this work, we introduce {\ours} to tackle these technical challenges, as illustrated in Figure~\ref{fig:sample}. In the following, we first describe our query-agnostic model in a single-passage setting and address the decomposability gap (\Cref{sec:piqa}). Then we propose several normalization techniques for scaling phrase representations to the full collection of documents  (\Cref{sec:normalization}).
Finally, we detail how we adapt our model for transfer learning~(\Cref{sec:qsft}), without the need for re-building of phrase representations at scale.
 

\section{Phrase-Indexed Question Answering}\vspace{-0.1cm}
\label{sec:piqa}

In this section, we start by learning query-agnostic phrase representations in a reading comprehension setting, in which a gold passage  is given for a question-answer pair (). Our goal is to build a strong reading comprehension model while enforcing the decomposability of query and phrase representations. This problem was first formulated by \newcite{seo2018phrase} and dubbed the phrase-indexed question answering (PIQA) task. In the following, we first describe our base model (Section~\ref{sec:base_model}) and propose two new solutions to close the decomposability gap: tackling data sparsity via question generation (Section~\ref{sec:data_sparsity}) and distillation from query-dependent models (Section~\ref{sec:distillation}).



















\subsection{Base Model}\label{sec:base_model}




We first describe our base architecture, which consists of a phrase encoder  and a question encoder . Given a passage  of  tokens , we consider phrases of up to  tokens and the full set of phrases is denoted by . Each phrase  has start and end indicies  and  and the gold phrase is \footnote{In some reading comprehension datasets (e.g., SQuAD, Natural Questions), the gold phrase  is given. While in some other datasets, only the gold answer  is provided and we need to find a phrase  that matches . }.
Following previous work on phrase, or span representations~\citep{lee2017learning,seo2018phrase}, we first use a pre-trained language model  to obtain contextualized word representations for each passage token: . Then, we can represent each phrase  as the concatenation of corresponding start and end vectors:
\vspace{-0.5em}

Using contextualized word representations to construct phrase representations has another great advantage that we can eventually reduce the storage of phrase representations to word representations. Therefore, we only need to save  (the total number of tokens in ) vectors, which is at least one magnitude order smaller than .

Similarly, we need to learn a question encoder  that maps a question of  tokens  to a vector of the same dimension as . To do so, we use another two different pre-trained LMs  and  applied on  and finally obtain representations  and  pooled from the \texttt{[CLS]} token representations of  and  respectively. Finally,  simply takes their concatenation:
\vspace{-1em}

\vspace{-1.5em}










In summary, we have three different LMs in total, which are initialized from the same pre-trained LM.
Since the start and end representations of phrases are produced by the same language model, we need two question representations  and  to differentiate the start and end positions.
In our pilot experiments, we found that SpanBERT~\citep{joshi2020spanbert} leads to superior performance compared to BERT~\cite{devlin2019bert}.
SpanBERT is designed to predict the information in the entire span from its two endpoints, therefore it is well suited for our phrase representations.
In our final model, we use SpanBERT-base-cased as our base LMs for  and , and hence .










\paragraph{Single-passage training}
For the reading comprehension setting, we maximize the log-likelihood of the start and end positions of the gold phrase :

We define  and  in a similar way. The final loss is calculated as:


\paragraph{Differences from DenSPI}
We deviate from DenSPI in the following ways: (1) Previous models split a hidden vector from a pre-trained LM into four vectors (start \& end vectors and two vectors for calculating a coherency score).
We don't do any splitting of vectors and remove the use of coherency scalars. We find that it is beneficial to keep the output dimension of pre-trained LMs for fully utilizing their representational capacity; (2) Previous models use a shared encoder for phrases and questions.
However, we use two different language models for representing questions. (3) We use SpanBERT instead of BERT. See Table~\ref{tab:piqa-ablation} for an ablation study.













\subsection{Tackling Data Sparsity}\label{sec:data_sparsity}
The performance of query-agnostic models is always inferior to query-dependent models with cross-attention~\citep{devlin2019bert}. We hypothesize that one key reason is most reading comprehension datasets only provide a few annotated questions in each passage, compared to the set of possible answer phrases. For instance, each passage in the training set of Natural Questions~\cite{kwiatkowski2019natural} mostly has only one annotated question.  Suppose that we are given a passage with the following question-answer pair in the training set:












\begin{comment}
\begin{quote}
\small
 \textit{\textbf{Peter Gardner Ostrum} (... ; born November 1957) is (...) former child actor whose only film role was as Charlie Bucket (...)}\\
 \textit{who played charlie bucket in the original charlie and the chocolate factory} \\
 \textit{\textbf{Peter Gardner Ostrum}}
\end{quote}
\end{comment}

\begin{quote}
\small
 \textit{Queen Elizabeth II is the sovereign, and her heir apparent is her eldest son, \textbf{Charles, Prince of Wales}. (...) Third in line is Prince George, the eldest child of the Duke of Cambridge (...)} \\
 \textit{who is next in line to be the monarch of england} \\
 \textit{\textbf{Charles, Prince of Wales}}
\end{quote}
\vspace{-0.5em}
\noindent While cross-attention models only need to represent the passage focusing on ``who is Queen Elizabeth II's heir apparent'', our phrase encoder should take all the other phrases into account, (e.g.,  \textit{Prince George}), because their representations will be re-used for other questions (e.g.,  \textit{who is the eldest child of the duke of cambridge}).











Following this intuition, we propose to use a simple \ti{question generation} model to generate questions for each training passage.
We generate questions from a question generation (QG) model built upon a T5-large model~\citep{raffel2020exploring}.
The input of the model is a passage  with the gold answer  highlighted by inserting surrounding special tags and the model is trained to maximize the log-likelihood of the question words of . Since we want to cover many possible answer phrases during inference, we extract all the entities in each training passage as candidate answers to extract named entities. to generate questions.
We keep the question-answer pairs only when a strong query-dependent (QD) reading comprehension model (SpanBERT-large, 88.2 EM on SQuAD) gives correct prediction on the generated pair.
The remaining generated QA pairs  are directly augmented to the original training set. Generated QA pairs can help learn phrase representations aligned with those of corresponding questions, instead of biased to few annotated questions.








\subsection{Distillation}
\label{sec:distillation}
As query-dependent (QD) models with cross-attention are considered stronger models, we also propose improving our query-agnostic model by distilling knowledge from a cross-attention model.
We minimize the Kullback–Leibler (KL) divergence between the probability distribution from our phrase encoder and that from a query-dependent model~\citep{hinton2015distilling}. We use a SpanBERT-base QA model as the QD model.
The distillation loss is computed as follows:

where  (and ) is defined in Equation~\eqref{eqn:pstart} and  and  denote the probability distributions, which are used to predict the start and end indices in the query-dependent model.


\section{Phrase Representations at Scale}
\label{sec:normalization}


Eventually, we need to build phrase representations for billions of phrases, therefore a bigger challenge is to incorporate more phrases during training so the representations can be better normalized.
While \citet{seo2019real} simply sample two negative questions from other passages based on question similarity, we propose two effective negative-sampling strategies, which are efficient to compute and highly useful in practice.


\subsection{In-batch Negatives}
We use in-batch negatives for our dense phrase representations, which also has been shown to be effective in learning dense passage representations before~\cite{karpukhin2020dense}. Specifically, for the -th example in a mini-batch of size , we denote the hidden representations of the gold start and end positions ( and ) as  and , as well as the question representation as . Let  be the  matrices and each row corresponds to  respectively. Basically, we can treat all the gold phrases from other passages  in the same mini-batch as negative examples.  We compute  and  and the -th row of  and  return  scores each, including a positive score and  negative scores:  and . Similar to Equation~\eqref{eqn:pstart}, we can compute the loss function for the -th example as:

Finally, the loss is summed over  examples in the mini-batch.
We also attempted using other non-gold phrases from other passages as negatives but didn't find a meaningful improvement.









\subsection{Pre-batch Negatives}


The in-batch negatives are highly effective and they usually benefit from a large batch size~\citep{karpukhin2020dense}.
However, batch sizes are bounded by GPU memory and it is challenging to further increase them.
In this section, we propose a novel negative sampling method called \ti{pre-batch negatives}, which can effectively utilize the representations from the preceding  mini-batches.
In each iteration, we maintain a FIFO queue of  mini-batches to cache phrase representations (, ).  The cached phrase representations are then used as negative samples for the next iteration, providing  additional negative samples in total. This approach is inspired by the momentum contrast idea recently proposed in unsupervised visual representation learning~\cite{he2020momentum}. However, our approach differs from theirs in that we have independent encoders for phrases and questions and back-propagate to both during training, without a momentum update.\footnote{In their approach, the query and key encoders are shared. Only the query encoder is back-propagated and the key encoder is updated as a moving average. Interested readers are referred to their paper for details.}






These pre-batch negatives are used together with in-batch negatives and the training loss is the same as Equation~\eqref{eqn:inbatch}, except that the gradients are \ti{not} back-propagated to the cached pre-batch negatives.
In practice, we found that pre-batch negatives work well, once the phrase encoder is warmed up with in-batch negatives. After the warm-up stage, we simply shift from in-batch negatives ( negatives) to in-batch and pre-batch negatives (hence a total number of  negatives).
For simplicity, we use  to denote both in-batch negatives and pre-batch negatives during training.
Since we do not retain the computational graph for forward and backward propagation, the memory consumption of pre-batch negatives is much more manageable while allowing increasing the number of negative samples.
Empirically, we found that using a large number of pre-batch negatives does not always help since the phrase representations can get easily outdated.





\subsection{Optimization, Indexing and Search}\label{sec:optimization}
With our loss terms defined previously, we finally minimize the following loss function on a question answering dataset, together with the generated questions (Section~\ref{sec:data_sparsity}):

\noindent where  determine the importance of each loss term.
We found that setting , , and  works well in practice.
In our experiments (\Cref{sec:rc_result}), we use reading comprehension datasets (a gold passage  is provided)  such as SQuAD~\cite{rajpurkar2016squad} and Natural Questions~\cite{kwiatkowski2019natural} to train the phrase and question encoders.




\paragraph{Indexing}
After training the phrase encoder , we need to encode all the phrases  in the entire English Wikipedia  and store an index of the phrase dump.
We segment each document  into a set of natural paragraphs, from which we obtain token representations for each paragraph using .
Then, we build a phrase dump  by stacking the token representations from all the paragraphs in .
Note that this process is computationally expensive and takes about hundreds of GPU hours with a large disk footprint.
To reduce the size of phrase dump, we follow and modify several techniques introduced in~\citet{seo2019real} (see \Cref{apdx:storage}).
After indexing, we can use two rows  and  of  to represent a dense phrase representation .  We use \texttt{faiss}~\citep{johnson2017billion} for building a MIPS index of .\footnote{We use \texttt{IVFSQ4} with 1M clusters and set n-probe to 256.}

\paragraph{Search}
During inference, for a given question , we can find the answer  as follows (Figure~\ref{fig:sample}):
\vspace{-0.5em}

\noindent where  denotes a phrase with start and end indices as  and  in the index .
We can compute the  of  (or ) efficiently by performing MIPS over  with  (or ).
In practice, we search for the top  start and top  end positions separately and perform a constrained search over the start and end positions respectively such that .
Since we share  for the start and end representations,  and  are also batched for MIPS to benefit from multi-threading.
To avoid producing redundant answers, we only keep the best scoring phrases when the two phrases that have the same normalized string are retrieved from the same paragraph.




\section{Query-side Fine-tuning}
\label{sec:qsft}

So far, we have created a phrase dump  as well as a question encoder  that can be directly used for question answering.
In this section, we propose a novel method called \ti{query-side fine-tuning}, which can facilitate {transfer learning} on a new dataset, by training the question encoder  to correctly retrieve a desired answer  for a question  given .
There are several advantages for doing this: (1) It can help our model quickly adapt to new QA tasks \ti{without} re-building billions of phrase representations.\footnote{Previous work~\cite{lee2020contextualized} used the question encoder  directly for additional QA tasks such as TREC and it is called a \ti{zero-shot} prediction. } (2) Even for the question-answering datasets used to build  (SQuAD and NQ in our experiments), we also find that query-side fine-tuning can further improve performance because it can reduce the discrepancy between training and inference. (3) It also creates a possibility to adapt our DensePhrases to non-QA tasks when the query is written in a different format. In Section~\ref{sec:slot_filling}, we show the possibility of directly using \ours~ for slot filling tasks by using a query such as \ti{(Michael Jackson, is a singer of, x)}.




In this regard, we can view our model as a knowledge base that can be accessed by many different types of queries and it is able to return phrase-level knowledge efficiently.
Formally speaking, we maximize the marginal log-likelihood of the gold answer  for a question , which resembles the weakly-supervised setting in the open-domain QA~\citep{lee2019latent,min2019discrete}.
The loss for query-side fine-tuning is computed as follows:

where  is the score of the phrase  (Equation~\eqref{eqn:decomp}) and  denotes the top  phrases for  (Equation \eqref{eqn:formula}).
In practice, we use  for the query-side fine-tuning.
Note that only the parameters of the question encoder  are updated.


\begin{comment}
It is important to note that for adapting retriever-reader models (such as DPR~\citep{karpukhin2020dense}) to new types of questions, we need to re-train the reader model as well as the question (and possibly the passage) encoder of the retriever model.
However, when the size of the training set is relatively small, it would be cumbersome to train a reader model using only a small amount of data.\footnote{For instance, the performance of DPR-single on the small sized dataset (TREC, WebQuestions) is relatively low and the reader model in DPR-multi is pre-trained on larger datasets before training on TREC or WebQuestions.}
On the other hand, with the query-side fine-tuning, we can easily adapt our model to new questions since we only have to update the question encoder, which can also was also trained on large QA datasets.
\end{comment}











 

\section{Experiments}
\label{sec:experiments}










\subsection{Setup}
\paragraph{Datasets}
We use two \ti{reading comprehension} datasets: SQuAD~\citep{rajpurkar2016squad} and Natural Questions~\citep{kwiatkowski2019natural}, in which a gold passage is provided.
For Natural Questions, we use the short answer as a ground truth answer  and its long answer as a gold passage  (Appendix~\ref{apdx:prepro}). We train our phrase representations on these two datasets, and also report the performance of our query-agnostic models in the reading comprehension setting.


We evaluate our approach on five popular \ti{open-domain QA} datasets: Natural Questions~\citep{kwiatkowski2019natural}, WebQuestions~\citep{berant2013semantic}, CuratedTREC~\citep{baudivs2015modeling}, TriviaQA~\citep{joshi2017triviaqa}, and SQuAD~\citep{rajpurkar2016squad} and the data statistics are provided in Table~\ref{table:openqa-data}.
Although many questions in SQuAD are context-dependent, we evaluate our model on SQuAD for the comparison with previous phrase retrieval models~\citep{seo2019real,lee2020contextualized}, which were mainly trained and evaluated on SQuAD.

Finally, we also evaluate our model on two \ti{slot filling} tasks, to show how to adapt our {\ours} for other knowledge-intensive NLP tasks.
We focus on using two slot filling datasets from the KILT benchmark~\citep{petroni2020kilt}: T-REx~\citep{elsahar2018t} and zero-shot relation extraction~\citep{levy2017zero}.
Each query is provided in the form of ``\{subject entity\} [SEP] \{relation\}", where the answer is the object entity.












\begin{table*}[t]
    \centering
    \resizebox{2.0\columnwidth}{!}{\begin{tabular}{llccccc}
        \toprule
        \textbf{Model} & &\textbf{NQ} & \textbf{WQ} & \textbf{TREC} & \textbf{TQA} &  \textbf{SQuAD}\\
         \midrule

        \textit{Retriever-reader} & : (Pre-)Training \\ \midrule
        DrQA~\citep{chen2017reading} & - & - & 20.7 & 25.4 & - & 29.8 \\ BERT + BM25~\citep{lee2019latent}  & - & 26.5 & 17.7 & 21.3 & 47.1 & \tf{33.2} \\ ORQA~\citep{lee2019latent} & \{Wiki.\} & 33.3 & 36.4 & 30.1 & 45.0 & 20.2 \\ REALM~\citep{guu2020realm} & \{Wiki., CC-News\}& 40.4 & 40.7 & 42.9 & - & - \\
        DPR-multi~\citep{karpukhin2020dense}  & \{NQ, WQ, TREC, TQA\} & \tf{41.5} & \tf{42.4} & \tf{49.4} & \tf{56.8} & 24.1 \\
\midrule



        \textit{Phrase retrieval} & : Training \\\midrule
        DenSPI~\citep{seo2019real} & \{SQuAD\} & 8.1 & 11.1 & 31.6 & 30.7 & 36.2 \\ DenSPI + Sparc~\citep{lee2020contextualized} & \{SQuAD\} & 14.5 & 17.3 & 35.7 & 34.4 & \textbf{40.7}  \\ DenSPI + Sparc~\citep{lee2020contextualized} & \{NQ, SQuAD\} & 16.5 & - & - & - & -  \\ \ours~(ours) & \{SQuAD\}& 31.2 & 36.3 & 50.3 & \textbf{53.6} & 39.4 \\
\ours~(ours) & \{NQ, SQuAD\}&  \textbf{40.9} & \textbf{37.5} & \textbf{51.0} & 50.7 & 38.0 \\
\bottomrule
    \end{tabular}
    }\caption{Open-domain QA results. We report exact match (EM) on the testing sets. We also show the additional (pre-)training datasets for learning the retriever models () and creating the phrase dump (). : no supervision using target training data (zero-shot). : unlabeled data used for extra pre-training.
}\vspace{-0.2cm}\label{tab:od-qa}
\end{table*}
 

\begin{table}[t]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{SQuAD}} & \multicolumn{2}{c}{\textbf{NQ (Long)}} \\\cmidrule{2-3} \cmidrule{4-5}
        & EM & F1 & EM & F1 \\
        \midrule

        \multicolumn{3}{l}{\textit{Query-Dependent}}\\\midrule
BERT-base & 80.8 & 88.5 & 69.9 & 78.2 \\
SpanBERT-base & 85.7 & 92.2 & 73.2 & 81.0 \\
\midrule

        \multicolumn{3}{l}{\textit{Query-Agnostic}} \\\midrule
DilBERT~\citep{siblini2020delaying} & \underline{63.0} & \underline{72.0} & - & - \\
        DeFormer~\citep{cao2020deformer} & - & \underline{72.1} & - & - \\
        DenSPI & 73.6 & 81.7 & 68.2 & 76.1 \\
DenSPI + Sparc & 76.4 & 84.8 & - & - \\
\ours~(ours) & \textbf{78.3} & \textbf{86.3} & \textbf{71.9} & \textbf{79.6} \\




        \bottomrule
    \end{tabular}
    }
    \caption{Reading comprehension results, evaluated on the development sets of SQuAD and Natural Questions. Underlined numbers: estimated from the figures from the original papers. : BERT-large model. }\label{tab:pi-qa}\vspace{-0.2cm}
\end{table}

%
 
\paragraph{Implementation details}
Our phrase and question encoders are trained on each training set with Equation~\eqref{eqn:aggregate}.
We use SQuAD to train our QG model.\footnote{The quality of generated questions from a QG model trained on Natural Questions is worse due to the ambiguity of information-seeking questions.}
We denote the training datasets used for reading comprehension (Equation~\eqref{eqn:aggregate}) as .
For using \ours~in open-domain QA, we train two versions of phrase encoders, each of which are trained on  and , respectively.
Note that both DenSPI and DenSPI + Sparc are trained on .
We build the phrase dump  for the 2018-12-20 Wikipedia snapshot which contains about 5.6 million documents and 3 billion words.
Then, we perform query-side fine-tuning our model on each dataset using Equation~\eqref{eqn:qsft}.
While we use a single 48GB GPU (Quadro RTX 8000) for training the phrase encoders with Equation~\eqref{eqn:aggregate}, query-side fine-tuning is relatively cheap and uses a single 12GB GPU (TITAN Xp).
For slot filling, we use the same set of hyperparameters used for query-side fine-tuning our models on open-domain QA datasets and we use the phrase dump obtained from \ours~(NQ + SQuAD).
To see how rapidly our model adapts to the new query types, we train our models on randomly sampled 5k or 10k training samples.
See~\Cref{apdx:hyper} for more details on the hyperparameters for each task.

\subsection{Experiments: Question Answering}\label{sec:openqa_result}
\paragraph{Baselines}
For reading comprehension, we report scores of query-agnostic models including DenSPI~\citep{seo2019real}, DenSPI + Sparc~\citep{lee2020contextualized} as well as Deformer~\citep{cao2020deformer} and DilBERT~\citep{siblini2020delaying}, where they only allow a late interaction (cross-attention) in the last few layers of BERT.
We report their results based on the interaction in the last layer, which mostly resembles the fully query-agnostic models.

For open-domain QA, we report the scores of extractive open-domain QA models including DrQA~\citep{chen2017reading}, BERT + BM25~\citep{lee2019latent}, ORQA~\citep{lee2019latent}, REALM~\citep{guu2020realm}, and DPR-Multi~\citep{karpukhin2020dense}.
We also show the performance of previous phrase retrieval models: DenSPI and DenSPI + Sparc.
In~\Cref{apdx:complexity}, we provide a thorough analysis on the computational complexity of each open-domain QA model.

\paragraph{Reading comprehension}\label{sec:rc_result}
Experimental results on reading comprehension datasets are shown in Table~\ref{tab:pi-qa}.
Among query-agnostic models, our models achieve the best performance of 78.3 EM on SQuAD by improving the previous dense phrase retrieval model (DenSPI) by . Despite it is still behind query-dependent models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.





\begin{comment}
\begin{table*}[t]
    \centering
    \resizebox{2.0\columnwidth}{!}{\begin{tabular}{lccccccccccccc}
        \toprule
        \multirow{2}{1.2cm}{\textbf{Model}}& \multicolumn{6}{c}{\textbf{T-REx}} && \multicolumn{6}{c}{\textbf{ZsRE}} \\\cmidrule{2-7} \cmidrule{9-14}
        & R-Prec & Recall@5 & Accuracy & F1 & KILT-AC & KILT-F1 & & R-Prec & Recall@5 & Accuracy & F1 & KILT-AC & KILT-F1  \\
        \midrule

        BART & 0.00 & 0.00 & 45.06 & 49.24 & 0.00& 0.00 & & 0.00 & 0.00 & 9.14 & 12.21 & 0.00& 0.00 \\
        T5 &  0.00 & 0.00 & 43.56 & 50.61& 0.00 & 0.00 & & 0.00 & 0.00 & 9.02 & 13.52 & 0.00& 0.00\\
        DPR + BERT & - & - & - & - & - & - & & 40.11 & 40.11 & 6.93 & 37.28 & 4.47 & 27.09 \\
        BART + DPR & 13.26 & 17.04 & 59.16 & 62.76& 11.12 & 11.41 & & 28.90 & 39.21 & 30.43 & 34.47 & 18.91 & 20.32 \\
        RAG & 28.68 & 33.04 & \textbf{59.20} & \textbf{62.96} & 23.12 & 23.94 & & 53.73 & 59.52 & 44.74 & 49.95 & 36.83 & 39.91\\
        \midrule
        \ours~(+5K) & 34.74 & 37.07 & 52.84 & 59.99 & 25.24 & 29.48 & & \textbf{56.79} & \textbf{59.57} & \textbf{45.75} & \textbf{53.68} & \textbf{40.27} & \textbf{46.31} \\
        \ours~(+10K) & \textbf{35.68} & \textbf{38.08} & 55.16 & 61.44 & \textbf{27.60} & \textbf{30.95} & & 53.20 & 55.78 & 43.80 & 52.75 & 38.40 & 45.15 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Slot filling task results on the T-REx and Zero shot RE (ZsRE) test sets.
    \label{tab:re}}
\end{table*}
\end{comment}





\begin{comment}
\begin{table}[t]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
        \multirow{2}{1.2cm}{\textbf{Model}}& \multicolumn{2}{c}{\textbf{T-REx}} & \multicolumn{2}{c}{\textbf{ZsRE}} \\\cmidrule{2-3} \cmidrule{4-5}
        & Acc & F1 & Acc & F1 \\
        \midrule

        BART & 45.06 & 49.24 & 9.14 & 12.21 \\
        T5 & 43.56 & 50.61 & 9.02 & 13.52 \\
        DPR + BERT & - & - & 6.93 & 37.28 \\
        BART + DPR & 59.16 & 62.76 & 30.43 & 34.47 \\
        RAG & \textbf{59.20} & \textbf{62.96} & 44.74 & 49.95 \\
        \midrule
\ours~(5K) & 51.82 & 59.38 & 46.13 & 53.65 \\
        \ours~(10K) & 53.90 & 61.74 & \textbf{47.42} & \textbf{54.75} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Slot filling task results on the T-REx and Zero shot RE (ZsRE) test sets.\jinhyuk{KILT-Acc is SOTA for both T-REx and ZsRE. Why don't we report it?}
    \label{tab:slot_filling}}
\end{table}
\end{comment}

\begin{table}[t]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c}{\textbf{T-REx}} & \multicolumn{2}{c}{\textbf{ZsRE}} \\\cmidrule{2-3} \cmidrule{4-5}
        & Acc & F1 & Acc & F1 \\
        \midrule

DPR + BERT & - & - & 4.47 & 27.09 \\
        DPR + BART & 11.12 & 11.41 & 18.91 & 20.32 \\
        RAG & 23.12 & 23.94 & 36.83 & 39.91 \\
        \midrule
\ours & 25.32 & 29.76 & 40.39 & 45.89 \\
        \ours & \textbf{27.84} & \textbf{32.34} & \textbf{41.34} & \textbf{46.79} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Slot filling results on the test sets of T-REx and Zero shot RE (ZsRE) in the KILT benchmark~\cite{petroni2020kilt}. We report KILT-AC and KILT-F1 (denoted as \ti{Acc} and \ti{F1} in the table), which consider both span-level accuracy and correct retrieval of evidence documents.
    We consider two settings, in which we use 5K and 10K training examples respectively.
    }\vspace{-0.2cm}\label{tab:slot_filling}
\end{table}
 
\paragraph{Open-domain QA}
Experimental results on open-domain QA are summarized in Table~\ref{tab:od-qa}.
Without using any sparse representations, \ours~outperforms previous phrase retrieval models by a large margin of a -- absolute improvement on all the datasets except SQuAD. As previous models only used SQuAD to train the phrase model and perform zero-shot prediction on other datasets, we add one more experiment training the model of \newcite{lee2020contextualized} on  for a fair comparison. However, it only increases the result from 14.5\% to 16.5\% on Natural Questions, demonstrating that it is not enough to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models~\cite{karpukhin2020dense,guu2020realm}, while running much faster during inference (Table~\ref{tab:category}). We can also consider using distantly-supervised examples of TriviaQA, WebQuestions and TREC for training our phrase representations, as the DPR-multi model did, and we leave it to future work. Finally, we find that using  or  doesn't make much difference after query-side fine-tuning in most datasets, except that including NQ in  brings a large gain of 9.7\% on the NQ evaluation.









\subsection{Experiments: Slot Filling}\label{sec:slot_filling}

Table~\ref{tab:slot_filling} summarizes the results of our model on the two slot filling datasets, along with the scores of baseline models provided by~\citet{petroni2020kilt}.
The only extractive baseline is DPR + BERT which performs poorly in zero-shot relation extraction.
On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training samples (less than 5\% of the training data).
This showcases how \ours~can be easily leveraged for knowledge-intensive NLP tasks.

 

\section{Analysis}
\label{sec:analysis}



\begin{table}[t]
    \centering
    \resizebox{0.95\columnwidth}{!}{\begin{tabular}{lcccccc}
        \toprule
         \textbf{Model} &  &
         {{Share}} & {{Split}} & {{QG}} &  {{Distill}} &  {{EM}} \\


        \midrule
        DenSPI & Bb. & \cmark & \cmark & \xmark & \xmark & 70.2 \\
        & Sb. & \cmark & \cmark & \xmark & \xmark & 68.5 \\
        & Bl. & \cmark & \cmark & \xmark & \xmark & 73.6 \\
        \midrule
        Dense & Bb. & \cmark & \xmark  & \xmark & \xmark & 70.2 \\
        Phrases & Bb. & \xmark & \xmark  & \xmark & \xmark & 71.9 \\
        & Sb. & \xmark & \xmark  & \xmark & \xmark & 73.2 \\
        & Sb. & \xmark & \xmark  & \cmark & \xmark & 76.3 \\
        & Sb. & \xmark & \xmark  & \cmark & \cmark & \textbf{78.3} \\


        \bottomrule
    \end{tabular}
    }
    \caption{Ablation of \ours~on the development set of SQuAD. Bb: BERT-base, Sb: SpanBERT-base, Bl: BERT-large. Share: whether question and phrase encoders are shared or not. Split: whether the full hidden vectors are kept or split into start and end vectors. DenSPI~\cite{seo2019real} also included a coherency scalar and see their paper for more details.}\label{tab:piqa-ablation}\vspace{-0.2cm}
\end{table}
 
\subsection{Ablation of Phrase Representations}
Table~\ref{tab:piqa-ablation} shows the ablation result of our model on SQuAD.
We observe that not sharing phrase and question encoders ({Share} = \xmark) and using the full output dimension ({Split} = \xmark) together improves the performance by 2\%.
Using a stronger pre-trained LM SpanBERT leads to another 1.3\% improvement.
Augmenting training set with generated questions ({QG} = \cmark) and performing distillation from query-dependent models ({Distill} = \cmark) further improves performance up to EM = 78.3.
We also attempt adding the generated questions to the training of the query-dependent model and only find a 0.3\% improvement (SpanBERT-base), which validates our hypothesis that data sparsity is a bottleneck for query-agnostic models.








\subsection{Effect of Batch Negatives}\label{sec:semi_od}
We further evaluate the effectiveness of various negative sampling methods introduced in Section~\ref{sec:normalization}.
Since it is computationally expensive to test each setting at the full Wikipedia scale, we use a smaller text corpus  of all the gold passages in the development sets of Natural Questions, for the ablation study.
We also report performance in the reading comprehension setting so  only consists of a gold passage. Empirically, we find that results are generally well correlated when we gradually increase the size of  and we encourage interested readers to experiment with these settings for model development. The results are summarized in Table~\ref{tab:sod-qa}. While using a larger batch size () is beneficial for in-batch negatives, the number of preceding batches in pre-batch negatives is optimal when .
Somewhat surprisingly, the pre-batch negatives also improve the performance when .






\begin{table}[t]
    \centering
    \resizebox{0.90\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
        {Type} &  &  &
         &  \\
        \midrule
        None & 48 & - & 70.4 & 35.3 \\
        \midrule
        + In-batch & 48 & - & 70.5 & 52.4 \\ & 84 & - & 70.3 & 54.2 \\ \midrule
        + Pre-batch & 84 & 1 & 71.6 & 59.8 \\ & 84 & 2 & \textbf{71.9} & \textbf{60.4} \\ & 84 & 4 & 71.2 & 59.8 \\ \bottomrule
    \end{tabular}
    }
    \caption{Effect of in-batch negatives and pre-batch negatives on the development set of Natural Questions. : batch size, : number of preceding mini-batches used in pre-batch negatives. We report EM of our model with smaller sets such as  (all the gold passages in the development set of NQ) and  (single passage).
}\label{tab:sod-qa}\vspace{-0.3cm}
\end{table}


\begin{comment}
\begin{table*}[t]
    \centering
    \resizebox{2.0\columnwidth}{!}{\begin{tabular}{clcccccccccc}
        \toprule
        & & & & & & \multicolumn{5}{c}{EM for Corpus Size (\% of Wikipedia)} \\ \cmidrule{7-11}
        & \textbf{Model} & Training Data & Sparse & In-Batch & Momentum & Gold & 0.001\% & 1\% & 10\% & 100\% \\
        \midrule

        \multirow{8}{1.5cm}{SQuAD} & DenSPI (w/o sparse) & SQuAD & \xmark & \xmark & \xmark & 73.6 & - & - & - & 11.2 \\
        & DenSPI & SQuAD & \cmark & \xmark & \xmark & 73.6 & - & - & - & 36.2 \\
        & DenSPI + Sparc & SQuAD & \cmark & \xmark & \xmark & 76.4 & 67.0 & \textbf{62.1} & 54.5 & 40.7 \\
        & \ours & SQuAD & \xmark & \xmark & \xmark & \textbf{78.3} & 67.1 & - & - \\
        & \ours & SQuAD & \xmark &  & \xmark & 77.9 & 68.9 & 48.7 & - & -\\
        & \ours & SQuAD & \xmark &  & \xmark & 76.4 & \textbf{73.6} & 60.5 & - & -\\
        & \ours & SQuAD & \xmark &  &  & - & - & - & - \\
        & \ours & SQuAD + NQ & \xmark &  &  & 74.9 & - & - & - \\
        \midrule

        & & & & & & Gold & 0.05\% & 1\% & 10\% & 100\% \\ \midrule
        \multirow{8}{1.5cm}{Natural Questions} & DenSPI (w/o sparse) & NQ & \xmark & \xmark & \xmark & 68.2 & 35.3 & - & - & - \\
        & DenSPI (w/o sparse) & NQ & \xmark &  & \xmark & 68.2 & 54.3 & 35.3 & 31.6 & - \\
        & DenSPI & NQ & \cmark &  & \xmark & 68.2 & 58.0 & 44.1 & 38.5 & 26.3 \\
        & \ours & NQ & \xmark & \xmark & \xmark & 70.3 & - & - & - \\
        & \ours & NQ & \xmark &  & \xmark & 70.2 & 34.7 & - & - & - \\
        & \ours & NQ & \xmark &  & \xmark & 70.5 & 54.9 & - & - \\
        & \ours & NQ & \xmark &  &  & \textbf{71.7} & \textbf{61.2} &  \textbf{44.2} &  \textbf{38.6} \\
        & \ours & SQuAD + NQ & \xmark &  &  & 69.9 & - & - & - \\
        \bottomrule
    \end{tabular}
    }
    \caption{Semi open-domain setting results on the SQuAD and Natural Questions development set. For a better comparison with previous phrase models, we test on SQuAD as well. \jinhyuk{I have to tune in-batch negative size and momentum batch negative size.}\jinhyuk{Numbers for DenSPI are from very old experiment sheets and are overestimated on (slightly easier) sampled questions.}
    \label{tab:sod-qa}}
\end{table*}

\end{comment}
 


\subsection{Effect of Query-side Fine-tuning}~\label{sec:qsft-ablation}
The query-side fine-tuning further trains the question encoder  with the phrase dump  and enables us to fine-tune \ours~on different types of questions.
We use three different phrase encoders, each of which is trained on a different training dataset .
We summarize the results in Table~\ref{tab:qsft-ablation}.
For the datasets that were not used for training the phrase encoders (TQA, WQ, TREC), we observe a 15\% to 20\% improvement after query-side fine-tuning, compared to zero-shot prediction. Even for the datasets that have been used (NQ, SQuAD), it leads to significant improvements (e.g., +8.3\% improvement on NQ for  = \{NQ\}) and it clearly demonstrates it can effectively reduce the discrepancy between training and inference.



\begin{table}[t]
    \centering
    \resizebox{0.9\columnwidth}{!}{\begin{tabular}{ccccccc}
        \toprule
         {QS} &
        {NQ} & {WQ} & {TREC} & {TQA} & {SQuAD} \\
        \midrule
        \multicolumn{6}{c}{{} = \{SQuAD\}} \\
        \midrule
        \xmark & 12.3 & 11.8 & 36.9 & 34.6  & 35.5 \\
        \cmark & 31.2 & 36.3 & 50.3 & \textbf{53.6} & \textbf{39.4} \\
        \midrule
        \multicolumn{6}{c}{{} = \{NQ\}} \\
        \midrule
        \xmark & 32.6 & 21.1 & 32.3 & 32.4 & 20.7 \\
        \cmark & \tf{40.9} & 37.1 & 49.7 & 49.2 & 25.7 \\
        \midrule
        \multicolumn{6}{c}{{} = \{NQ, SQuAD\}} \\
        \midrule
         \xmark & 28.9 & 18.9 & 34.9 & 31.9 & 33.2 \\
          \cmark & \textbf{40.9} & \textbf{37.5} & \textbf{51.0} & 50.7 & 38.0 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Effect of query-side fine-tuning in \ours~on each test set. We report EM of each model before ({QS} = \xmark) and after ({QS} = \cmark) the query-side fine-tuning.}\label{tab:qsft-ablation}\vspace{-0.3cm}
\end{table}
 






 

\section{Conclusion}
\label{sec:conclusion}

In this study, we show that we can learn dense representations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks.
We tackle the decomposability gap by mitigating data sparsity and introduce two batch-negative techniques for normalizing billions of phrase representations.
We also introduce query-side fine-tuning that easily adapts our model to any type of query with a single 12GB GPU.
As a result, we achieve much stronger performance on five popular open-domain QA datasets compared to previous phrase retrieval models, while reducing the storage footprint and improving latency significantly.
We also achieve strong performance on two slot filling datasets using only a small number of training examples, showing the possibility of utilizing our {\ours} as a dense knowledge base.
 
\section*{Acknowledgments}
We thank Sewon Min, Hyunjae Kim, Gyuwan Kim, Jungsoo Park, Zexuan Zhong, Dan Friedman, Chris Sciavolino for providing valuable comments and feedback.
This research was supported by a grant of the Korea Health Technology R\&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health \& Welfare, Republic of Korea (grant number: HR20C0021). This research was also supported by National Research Foundation of Korea (NRF-2020R1A2C3010638).


\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{\Alph{section}.\arabic{table}}

\section{Complexity Analysis}~\label{apdx:complexity}
We describe the resources and time spent during inference (Table~\ref{tab:category} and \ref{tab:complexity}) and indexing (Table~\ref{tab:complexity}).
With our limited GPU resources (24GB  4), it takes about 20 hours for indexing the entire phrase representations.
We also largely reduced the storage from 1,547GB to 320GB by (1) removing sparse representations and (2) using our sharing and split strategy.
See Appendix~\ref{apdx:storage} for the details on the reduction of storage footprint and Appendix~\ref{apdx:server} for the specification of our server for the benchmark.



\begin{table}[h]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{llll}
        \toprule



        \textbf{Indexing} & Resources & Storage & Time \\ \midrule
        DPR & 32GB GPU  8 & 76GB & 17h \\
        DenSPI + Sparc & 24GB GPU  4 & 1,547GB & 85h \\\ours & 24GB GPU  4 & 320GB & 20h \\ \midrule


        \textbf{Inference} & \multicolumn{1}{l}{RAM / GPU} & \multicolumn{2}{l}{\#Q/sec (\gpu{GPU}, \cpu{CPU})} \\ \midrule
        DPR & 86GB / 17GB & \multicolumn{2}{c}{\gpu{0.9}, \cpu{0.04}} \\
        DenSPI + Sparc & 27GB / 2GB & \multicolumn{2}{c}{\gpu{2.1}, \cpu{1.7}} \\
        \ours & 12GB / 2GB & \multicolumn{2}{c}{\gpu{20.6}, \cpu{13.6}} \\
\bottomrule
    \end{tabular}
    }
    \caption{Complexity analysis of three open-domain QA models during indexing and inference. For inference, we also report the minimum requirement of RAM and GPU memory for running each model with \gpu{GPU}.
    For computing \#Q/s for \cpu{CPU}, we do not use GPUs but load all models on the RAM.
}\vspace{-0.2cm}
    \label{tab:complexity}
\end{table}
 
\section{Reducing Storage Footprint}\label{apdx:storage}
As shown in Table~\ref{tab:category}, we have reduced the storage footprint from 1,547GB~\citep{lee2020contextualized} to 320GB.
We detail how we can reduce the storage footprint in addition to the several techniques introduced by~\citet{seo2019real}.

First, following~\citet{seo2019real}, we apply a linear transformation on the passage token representations to obtain a set of filter logits, which can be used to filter many token representations from .
This filter layer is supervised by applying the binary cross entropy with the gold start/end positions (trained together with Equation~\eqref{eqn:aggregate}).
We tune the threshold for the filter logits on the reading comprehension development set to the point where the performance does not drop significantly while maximally filtering tokens.

Second, in our architecture, we use a base model (SpanBERT-base) for a smaller dimension of token representations () and does not use any sparse representations including tf-idf or contextualized sparse representations~\citep{lee2020contextualized}.
We also use the scalar quantization for storing \ttt{float32} vectors as \ttt{int8} during indexing.

Lastly, since the inference in Equation~\eqref{eqn:formula} is purely based on MIPS, we do not have to keep the original start and end vectors which takes about 500GB.
However, when we perform query-side fine-tuning, we need the original start and end vectors for reconstructing them to compute Equation~\eqref{eqn:qsft} since MIPS index only returns the top- scores and their indices, but not the vectors.

\section{Server Specifications for Benchmark}\label{apdx:server}
To compare the complexity of open-domain QA models, we install all models in Table~\ref{tab:category} on the same server using their public open-source code.
Our server has the following specifications:

\begin{table}[h]
    \centering
    \resizebox{0.8\columnwidth}{!}{\begin{tabular}{llll}
        \toprule
        Hardware \\ \midrule
        Intel Xeon CPU E5-2630 v4 @ 2.20GHz \\
        128GB RAM \\
        12GB GPU (TITAN Xp)  2\\
        2TB 970 EVO Plus NVMe M.2 SSD  1\\
        \bottomrule
    \end{tabular}
    }
    \caption{Server specification for the benchmark
    }
    \label{tab:server}
\end{table}
 For DPR, due to its large memory consumption, we use a similar server with a 24GB GPU (TITAN RTX).
For all models, we use 1,000 randomly sampled questions from the Natural Questions development set for the speed benchmark and measure \#Q/sec.
We set the batch size to 64 for all models except BERTSerini, ORQA and REALM, which do not support a batch size of more than 1 in their open-sources.
\#Q/sec for DPR includes retrieving passages and running a reader model and the batch size for the reader model is set to 8 to fit in the 24GB GPU (retriever batch size is still 64).
For other hyperparameters, we use the default settings of each model.
We also exclude the time and the number of questions in the first five iterations for warming up each model.
Note that despite our effort to match the environment of each model, their latency can be affected by various different settings in their implementations such as the choice of library (PyTorch vs. Tensorflow).



\section{Pre-processing for Single-Passage Training}\label{apdx:prepro}
We use two reading comprehension datasets (SQuAD and Natural Questions) for training our model on Equation~\eqref{eqn:aggregate}.
For SQuAD, we use the original dataset provided by the authors~\citep{rajpurkar2016squad}.
For Natural Questions~\citep{kwiatkowski2019natural}, we use the pre-processed version provided by~\citet{asai2019learning}.\footnote{\url{https://github.com/AkariAsai/learning\_to\_retrieve\_reasoning\_paths}}
We also match the gold passages in Natural Questions to the paragraphs in Wikipedia whenever possible.
Since we want to check the performance changes of our model with the growing number of tokens, we follow the same split (train/dev/test) used in Natural Questions-Open for the reading comprehension setting as well.



\begin{table}[t]
\label{table:dataset}
\begin{center}
\centering
\resizebox{0.9\columnwidth}{!}{\begin{tabular}{lrrr}
\toprule
\multicolumn{1}{l}{\bf Dataset}  & {\bf Train} & {\bf Dev} & {\bf Test}\\
\midrule
Natural Questions & 79,168 & 8,757 & 3,610 \\
WebQuestions & 3,417 & 361 & 2,032 \\
CuratedTrec & 1,353 & 133 & 694 \\
TriviaQA & 78,785 & 8,837 & 11,313 \\
SQuAD &  78,713 & 8,886 & 10,570 \\
\midrule

T-REx & 2,284,168 & 5,000 & 5,000 \\
Zero-Shot RE & 147,909 & 3,724 & 4,966 \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Statistics of five open-domain QA datasets and two slot filling datasets. We follow the same splits in open-domain QA for the two reading comprehension datasets (SQuAD and Natural Questions).}\vspace{-0.3cm}\label{table:openqa-data}
\end{table}
 
\section{Hyperparameters}\label{apdx:hyper}
We use the Adam optimizer~\citep{kingma2014adam} in all our experiments.
For training our phrase and question encoders with Equation~\eqref{eqn:aggregate}, we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1.
We use a batch size of 84 and train each model for 4 epochs for all datasets where the loss of pre-batch negatives is applied in the last two epochs.
We use spaCy\footnote{\url{https://spacy.io/}} for extracting named entities in each training passage, which are used to generate questions.
The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.
The number of preceding batches  is set to 2.
For the query-side fine-tuning with Equation~\eqref{eqn:qsft}, we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1.
We use a batch size of 12 and train each model for 10 epochs for all datasets.
The top  for the Equation~\eqref{eqn:qsft} is set to 100.
Using the development set, we select the best performing model for each dataset, which are then evaluated on each test set.
Since SpanBERT only supports cased models, we also truecase the questions~\citep{lita2003truecasing} that are originally provided in the lowercase (Natural Questions and WebQuestions). 
\end{document}
