\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx, fleqn, tabularx, wrapfig}
\usepackage{textcomp}
\usepackage[letterpaper, margin=1.2in]{geometry}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage[makeroom]{cancel}
\usepackage[table]{xcolor}
\usepackage{enumerate}
\usepackage{mathpazo}
\usepackage{tgpagella}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{nicefrac,xfrac}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{stackengine}
\usepackage{xspace}
\usepackage{contour}
\usepackage{bbm}
\usepackage{bbding}



\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{Y}{>{\centering\arraybackslash}X}


\makeatletter
\newcommand\notsotiny{\@setfontsize\notsotiny{7}{8}}
\makeatother

\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{dateplot}
\usetikzlibrary{arrows.meta}

\begin{document}

\title{Prismer: A Vision-Language Model with  \\ An Ensemble of Experts}

\author{
  Shikun Liu\thanks{Corresponding Author: shikun.liu17@imperial.ac.uk. Work done during an internship at NVIDIA.}
  \qquad Linxi Fan \qquad Edward Johns \\ \medskip
Zhiding Yu \qquad  Chaowei Xiao \qquad  Anima Anandkumar \\ \medskip
 \small Imperial College London \qquad NVIDIA \qquad  ASU \qquad Caltech
}

\date{}

\maketitle

\begin{abstract}
  Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.  Code is available at \url{https://github.com/NVlabs/prismer}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{images/intro2.pdf}
  \caption{{\bf Prismer model overview.} Prismer is a data-efficient vision-language model that leverages diverse pre-trained experts through its predicted multi-modal signals. It can perform vision-language reasoning tasks such as image captioning and visual question answering. The analogy is with an optical prism: Prismer splits a single reasoning task into diverse domain-specific reasoning.}
  \label{fig:expert}  
\end{figure}

Large pre-trained models have demonstrated exceptional generalisation capabilities across a wide range of tasks. However, these capabilities come at a hefty cost in terms of computational resources required for training and inference, as well as the need for large amounts of training data. In the language domain, models with hundreds of billions of learnable parameters typically require a compute budget on the yottaFLOP scale \cite{chowdhery2022palm,brown2020gpt3,black2022gptneox,rae2021gopher}. 

The problems in vision-language learning are arguably more challenging. This domain is a strict super-set of language processing, whilst also requiring extra skills unique to visual and multi-modal reasoning.   
For example, many image captioning and visual question answering problems require the model to be capable of fine-grained object recognition, detection, counting, and 3D perception \cite{antol2015vqa,chen2015coco_caption}. A typical solution is to use a massive amount of image-text data to train one giant, monolithic model that learns to develop these modality-specific skills from scratch, simultaneously, and within the same generic architecture.

Instead, we investigate an alternative approach to learn these skills and domain knowledge via {\it distinct and separate sub-networks}, referred to as ``experts". As such, each expert can be optimised independently for a specific task, allowing for the use of domain-specific data and architectures that would not be feasible with a single large network. This leads to improved training efficiency, as the model can focus on {\it integrating} specialised skills and domain knowledge, rather than trying to learn everything at once, making it an effective way to {\it scale down} multi-modal learning.

To achieve this, we propose Prismer\footnote{The model name ``Prismer" draws from the analogy to an optical prism which breaks a white light into a spectrum of colours, and here we break down a single reasoning task into diverse domain-specific reasoning.}, a visually conditioned autoregressive text generation model, trained to {\it better use diverse pre-trained domain experts} for open-ended vision-language reasoning tasks. Prismer's key design elements include i) powerful vision-only and language-only models for {\it web-scale knowledge} to construct our core network backbones, and ii) modality-specific vision experts encoding multiple types of visual information, including {\it low-level vision signals} such as depth, and {\it high-level vision signals} such as instance and semantic labels, as a form of {\it auxiliary knowledge}, directly from their corresponding network outputs. All expert models are individually pre-trained and frozen, and are connected through some lightweight trainable components which contribute to roughly 20\% of the total network parameters. 

Despite Prismer being trained on only 13M publicly available image/alt-text data examples, it shows strong multi-modal reasoning performance in tasks such as image captioning, image classification, and visual question answering, competitive with many state-of-the-art vision-language models \cite{alayrac2022flamingo,wang2022git,wang2021simvlm}, that were trained with one or two orders of magnitude more data. Finally, we conduct an in-depth analysis of Prismer's learning behaviours and observe some encouraging properties. For example, i) Prismer exhibits {\it strong robustness against the inclusion of noisy experts}, and ii) the learning performance also scales favourably with increases in both the \textit{quantity} or \textit{quality} of experts.



\section{Related Work}
\label{sec:related}

\paragraph{Vision-Language Models (VLMs)}  Inspired by the breakthrough of transformers in the language domain \cite{vaswani2017transformer,devlin2019bert}, early works aimed to model the vision-language relationship using a shared network based on transformers in a {\it single-stream} design \cite{li2020visualbert,chen2020uniter,li2020oscar,su2020vlbert}. These works usually leverage a pre-trained object detector, encoding images as sequences of {\it visual words}, parameterised by object- or region-level features. Prismer takes a slightly different approach by using pre-trained models to provide their output predictions as auxiliary signals, whilst still relying on the original images to encode visual features.

Another line of works encodes vision and language features in separate networks in a {\it dual-stream} design, where the vision-only and language-only features are aligned through contrastive learning \cite{radford2021clip,zhai2022lit,jia2021align,li2021albef}. These works typically focus on close-ended multi-modal alignment tasks such as image-text classification and retrieval. In contrast, Prismer's vision encoder also aligns its vision features with the language embedding through pre-training with contrastive learning, but with a greater emphasis on multi-modal generation tasks. 

Both single- and dual-steam VLMs in the past years have often been pre-trained with a combination of multiple objectives, such as masked language modelling, masked region modelling, word-region alignment, visual grounding and more \cite{li2020visualbert,cho2021vlt5,li2022blip,li2021albef,lu2019vilbert}. These multiple objectives can make the training process more complex and require careful balancing of the different losses. Prismer adopts a different approach, aligning with recent developments in VLMs that focus on language generation, and only require a single autoregressive training objective  \cite{wang2022git,wang2021simvlm,hu2022lemon}. Despite the reduced complexity, training these large-scale VLMs is data intensive and computationally demanding, often requiring billions of training data. To overcome these challenges, Prismer leverages powerful pre-trained domain expert models for data-efficient training. Unlike another set of works that prioritise in-context capability by conditioning on a large frozen language model with no task-specific fine-tuning \cite{eichenberg2021magma,tsimpoukelli2021frozen,alayrac2022flamingo}, Prismer focuses on fine-tuned performance with an emphasis on parameter efficiency, using smaller but diverse pre-trained models.

\paragraph{Multi-task and Auxiliary Learning} Multi-task learning and auxiliary learning aim to train models to predict multiple modalities (such as semantic segmentation, object detection, and depth estimation) from a single input, thereby improving the performance across one or multiple tasks. This is often achieved through the design of effective multi-task networks that balance task-shared and task-specific features \cite{shikun2019mtan,misra2016cross_stitch,sun2019adashare,xu2018padnet}, or through the explicit modelling of task relationships \cite{shikun2019maxl,shikun2022auto_lambda,navon2021auxilearn,zamir2018taskonomy,fifty2021tag}.  Prismer also employs multiple modalities, similar to these methods, but only uses them solely as input, serving as auxiliary knowledge. Prismer is more related to works such as \cite{bachmann2022multimae,ghiasi2021must}, which utilise pre-trained experts to create pseudo labels for multi-task self-training. However, whilst those methods focus on learning task-agnostic features through multi-task supervision, Prismer focuses purely on multi-modal reasoning with a single-task objective.

\paragraph{Unifying Pre-trained Experts} The utilisation of diverse pre-trained domain experts for multi-modal reasoning has been investigated in previous studies. Socratic models \cite{zeng2022socratic} use language as a one-way communication interface to connect different pre-trained experts. However, this design is limited to multi-modal reasoning within the domains on which the pre-trained experts were trained, and errors predicted by previous experts can be carried forward to future experts. On the other hand, PIC \cite{li2022pic} addresses this issue by using a group of pre-trained experts to evaluate each expert's prediction and reach a consensus through an iterative closed-loop communication.  Whilst both methods perform multi-modal reasoning in a zero-shot manner without any training, Prismer utilises a unified architecture design to enhance the integration and sharing of information between the pre-trained experts.

Finally, we would like to note the difference between ``Mixture of Experts (MoE)" \cite{riquelme2021scaling,nguyen2018practical,masoudnia2014mixture} and ``Ensemble of Experts'' defined in Prismer. In MoE, the ``experts" are sub-modules in a single network, interconnected through their corresponding gating networks, encoding {\it implicit knowledge} guided by a shared training objective. Conversely, in Prismer, the ``experts" are independently pre-trained models, encoding {\it explicit knowledge} based on their pre-trained tasks or domains.



\section{Prismer: Open-ended Reasoning with Multi-modal Knowledge}
In this section, we introduce the Prismer model, a type of vision-language generative model that takes multi-modal signals as input, and outputs free-form text.


\subsection{Model Overview}
The design of the Prismer model is illustrated in Fig.~\ref{fig:prismer}.  Prismer is an encoder-decoder transformer model \cite{vaswani2017transformer} that leverages a library of existing pre-trained experts. It consists of a vision encoder and an auto-regressive language decoder. The vision encoder takes an RGB image and its corresponding multi-modal labels as input ({\it e.g.} depth, surface normal, segmentation labels, predicted from the frozen pre-trained experts), and outputs a sequence of RGB and multi-modal features. The language decoder is then conditioned on these multi-modal features via cross attention, and produces a sequence of text tokens.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{images/prismer.pdf}
  \caption{{\bf Prismer architecture design overview.} Prismer has two main trainable components: the Experts Resampler that converts variable multi-modal signals to a fixed number of outputs, and the Adaptor that enhances the model's expressivity for vision-language reasoning. To ensure that the model takes advantage of the rich domain-specific knowledge encoded in the pre-trained experts, the majority of network weights are frozen during training, as represented by  {\footnotesize \SnowflakeChevron}. }
  \label{fig:prismer}  
\end{figure*}


One of the key advantages of the Prismer model is its exceptional data efficiency during training. This is achieved by leveraging {\it a combined power of strong domain-specific experts}, resulting in a significant reduction in the number of GPU hours required to achieve comparable performance to other state-of-the-art vision-language models.  Prismer is built on top of existing pre-trained vision-only and language-only backbone models --- this allows us to tap into the vast amount of  {\it web-scale knowledge} already stored in these pre-trained parameters. Additionally, we also extend the vision encoder to accept multi-modal signals --- this enables it to better capture semantics and information about the input image through the help of the {\it generated multi-modal auxiliary knowledge}. For example, we expect ``text-reading" problems can be easily solved by leveraging an OCR detection expert; and ``object-recognition" problems can be easily solved by leveraging an object detection expert. A visualisation of all expert labels we included in Prismer is shown in Fig.~\ref{fig:expert} and is further explained in Sec.~\ref{sec:experts}. 

Prismer is designed to fully leverage pre-trained experts whilst keeping the number of trainable parameters to a minimum. To do this, the majority of the network weights of the pre-trained experts are frozen to maintain the {\it integrity of their learned knowledge} and prevent catastrophic forgetting   \cite{kemker2018measuring,kirkpatrick2017overcoming_forgetting}. To link the multi-modal labels as well as the vision and language parts of Prismer, we insert two types of parameter-efficient trainable components: {\it Experts Resampler} and {\it Adaptor}. The Experts Resampler is used in the vision encoder to map a variable length of multi-modal signals to a sequence of multi-modal features with a {\it fixed length}. The Adaptors are inserted in each transformer layer of the vision and language parts of the model to better adapt the pre-trained experts to new tasks and modalities.

Prismer is a {\it generative} model, and we re-formulate all vision-language reasoning tasks as a {\it language modelling or prefix language modelling} problem. For example, given the multi-modal tokens (encoded from an input RGB image and its multi-modal labels) and a question as the prefix, the model generates the answer for the visual question answering task; given the multi-modal tokens, the model generates its caption for the image captioning task. Once we have a prefix prompt, we may either sample the output text in an autoregressive manner, as in an {\it open-ended} setting; or we may rank the log-likelihood from a fixed set of completions, as in a {\it closed-ended} setting.


\subsection{Pre-trained Experts}
\label{sec:experts}
In Prismer, we include two types of pre-trained experts:

\paragraph{Backbone Experts} The vision-only and language-only pre-trained models, which are responsible for encoding images and texts into a meaningful sequence of tokens. Both models are required to be based on the transformer architecture \cite{vaswani2017transformer}, so we that can easily connect them with a few trainable components of similar designs. To preserve their rich domain-specific knowledge encoded in the network parameters, the majority of the weights are frozen during pre-training.
  
\paragraph{Modality Experts} The models that can produce task-specific labels depending on their training datasets. In Prismer, we include up to 6 modality experts all from the vision domain, encoding three {\it low-level} vision signals: depth, surface normals, and edge; and three {\it high-level} vision signals: object labels, segmentation labels, and text labels. These modality experts are treated as {\it black-box predictors}, and their predicted labels are used as input for the Prismer model. As a result, all network weights of the modality experts are frozen, and they can have {\it any design}.

We apply modality-specific post-processing on these predicted labels, transforming them to a  tensor ({here  represent image height, width and channels respectively. {\it e.g.}  for depth and edge label, and  for surface normals label). For all expert labels encoding high-level semantic signals, we tile each pixel with its corresponding text embedding from a pre-trained CLIP text model \cite{radford2021clip}, and then we further apply PCA to down-sample the dimensionality to  for efficient training. The detailed descriptions of all modality experts, including their pre-trained datasets and the architecture design, are listed in Table~\ref{tab:experts}.

\begin{table}[ht!]
    \setlength{\tabcolsep}{0.2em}
    \centering
    \scriptsize
      \begin{tabular}{C{0.15\linewidth}C{0.18\linewidth}C{0.15\linewidth}C{0.1\linewidth}L{0.37\linewidth}}
      \toprule
       Task & Dataset & Model & Params. & Post-Processing \\
      \midrule 
       Semantic Segmentation & COCO-Stuff \cite{caesar2018coco-stuff} & Mask2Former \cite{cheng2022mask2former}  & 215M & Tile each pixel with its corresponding label parametrised by CLIP text embedding. \\
       \midrule
       Object Detection &  \makecell[b]{COCO \cite{lin2014coco} \\ + Objects365 \cite{shao2019objects365}\\ + OpenImages \cite{kuznetsova2020openimages}\\ + Mapillary \cite{neuhold2017mapillary}} & UniDet \cite{zhou2022unidet} & 120M & Tile each pixel with its corresponding label parametrised by CLIP text embedding. The labels for the overlapping pixels are further determined by the depth expert.  \\
      \midrule
      Text Detection & ICDAR 2015 \cite{karatzas2015icdar} & CharNet \cite{liu2018charnet} & 89M & Tile each pixel with its corresponding text parametrised by CLIP text embedding. \\
      \midrule
       Depth Estimation  &  MIX-6 \cite{ranftl2021dpt} & DPT \cite{ranftl2021dpt} & 123M & Re-normalised to .\\
      \midrule      
       Surface Normal & ScanNet \cite{dai2017scannet}  & NLL-AngMF \cite{bae2021nll} & 72M & Re-normalised to . \\
      \midrule
       Edge Detection  &  BIPED \cite{poma2020dexined} & DexiNed \cite{poma2020dexined} & 35M & Re-normalised to . \\
      \bottomrule
      \end{tabular}\caption{{\bf The detailed description of modality experts}. We provide a detailed description of each modality expert including its pre-trained dataset, parameter size, model name and type and post-processing strategy. }
    \label{tab:experts}
\end{table}


\subsection{Key Architectural Components}
\label{sec:arch}

\paragraph{Modality-Specific Convolutional Stem}  All expert labels are first processed with randomly initialised convolution layers to map them to the same dimensionality. Specifically, we apply 5 convolutional layers and each is composed of a small  kernel, which is shown to perform better than a single convolutional layer but with a larger kernel in the original Vision Transformer design \cite{dosovitskiy2020vit}, consistent with the finding in \cite{xiao2021early}. The convolutional stem is designed to be modality-specific, which we have found to yield superior performance in comparison to a shared design in a multi-task learning setting \cite{shikun2019mtan,misra2016cross_stitch}. 

For high-level semantic labels such as those in object detection, semantic segmentation, and OCR detection, we down-sample the resolution by a factor of 4 to conserve running memory. Furthermore, for each object instance, we add a trainable and randomly sampled embedding to distinguish among different object instances. The size of this instance embedding is set to 128, which corresponds to the maximum possible number of object instances to be present in a single image. For RGB images, we simply process with the pre-trained convolutional stem defined by the original vision backbone. All modality expert embeddings, including RGB, are then added with a pre-trained positional embedding before being further processed by transformer layers.


\paragraph{Experts Resampler} The computational complexity of self-attention is {\it quadratically proportional} to the number of input patches.  And therefore, the vision encoder can easily require tremendous memory when including a large number of modality experts. To address this issue, we propose {\it Experts Resampler}, which takes a {\it variable} number of expert labels as input and outputs a {\it fixed} number of embeddings, illustrated in Fig.~\ref{fig:resampler_adaptor} Left. Such design produces a {\it constant} memory for the self-attention computation in the vision encoder, as well as the vision-text cross attention in the language decoder (shown in Fig.~\ref{fig:prismer}), independent of the inclusion of a different number of experts. Inspired by the design in the Perceiver \cite{jaegle2021perceiver} and the Flamingo model \cite{alayrac2022flamingo}, the Experts Resampler learns a pre-defined number of latent input queries, to cross-attend a flattened embedding concatenated from all multi-modal features. The Resampler then compresses the multi-modal features into a much smaller number of tokens equal to the number of learned latent queries, as a form of {\it auxiliary knowledge distillation}. We design keys and values to be a concatenation for both multi-modal features and the learned latent queries, which is shown to be more effective, consistent with the design in the Flamingo model \cite{alayrac2022flamingo}.


\paragraph{Lightweight Adaptor} We insert one lightweight adaptor into each transformer layer of both vision and language backbones in order to improve Prismer's expressivity and conditioning on multi-modal features, illustrated in Fig.~\ref{fig:resampler_adaptor}~Right. The adaptor has an encoder-decoder design, which has proven to be successful for efficient transfer learning in the NLP domain \cite{houlsby2019parameter,pfeiffer2020adapterhub}. It first down-projects the input features into a smaller dimension, applies a non-linearity, and then up-projects the features back to the original input dimension. We choose the non-linearity function to be squared ReLU  \cite{so2021primer} -- a simple and parameter-free function that delivers strong training stability. With the residual connection, we initialise all adaptors with near-zero weights to approximate the identity function. Combined with a standard cross attention block in the language decoder, the model is able to smoothly transition from the domain-specific vision-only and language-only backbones to a vision-language model during pre-training with paired image-text data.

The model performance, memory usage and time complexity for other design choices are systematically evaluated and ablated in Sec.~\ref{subsec:ablative}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{images/resampler_adaptor.pdf}
  \caption{{\bf Design details in Experts Resampler and Adaptor.} Left: The Experts Resampler takes multi-modal features with variable length as input, and outputs a fixed number of tokens via cross attention. Right: The Adaptor has a residual connection to the input and two fully-connected layers, that down-projects the input features to a smaller bottleneck dimension and then up-projects back to the original dimension.}
  \label{fig:resampler_adaptor}  
\end{figure}

\subsection{Training Objective}
For simplicity, we train Prismer with a {\it single} objective --- to predict the next text token autoregressively. Following the standard encoder-decoder architecture, the vision encoder predicts the multi-modal features , and the language decoder learns to maximise the conditional likelihood of the paired text caption  with its length  under the forward autoregressive factorisation: 

In practice, our {\it one-time} pre-processing step of collecting multi-modal expert labels is computationally cheap and fast with data parallelism. The single generative objective then only requires one forward pass to compute gradients, which is significantly more efficient and streamlined than many other VLMs that may require a multi-stage and/or multi-step pre-training \cite{li2022blip,li2021albef,wang2022ofa,dou2022meter,chen2020uniter}, with multiple objectives and data sources. However, because our model only focuses on multi-modal language generation, it is less suitable for multi-modal discriminative tasks such as image-text retrieval and visual entailment, which are the focus of other types of VLMs  \cite{gan2020villa,chen2020uniter,jia2021align}.



\section{Experiments}

\subsection{Prismer Model Variants}
In addition to Prismer, we also introduce a model variant named PrismerZ, which solely relies on the power of strong backbone experts and is trained with {\it zero} modality experts. PrismerZ has the same architectural design as the original Prismer but without the Experts Resampler. PrismerZ simplifies the data inference process as it only requires RGB images, making it more efficient and applicable to a wider range of applications. Prismer is less efficient in data inference due to the need for data processing on expert labels, but as we will show, it has better predictive performance.

Both Prismer and PrismerZ utilise ViT \cite{dosovitskiy2020vit} pre-trained by CLIP \cite{radford2021clip} as the frozen vision encoder, and RoBERTa \cite{liu2019roberta} as the frozen language decoder. We have alternatively tried using two other popular open-sourced decoder-only autoregressive language models: OPT \cite{zhang2022opt} and BLOOM \cite{scao2022bloom}, but early experiments showed that they did not perform as well.

We experiment with two model sizes, {\tt BASE} and {\tt LARGE}. The {\tt BASE} model is built on top of {\tt ViT-B/16} and {\tt RoBERTa}, and the {\tt LARGE} model is built on top of {\tt ViT-L/14} and {\tt RoBERTa}. In Prismer, we apply the same Experts Resampler with roughly 50M parameters in both model sizes. The detailed architecture details are summarised in Table~\ref{tab:arch}.


\begin{table}[ht!]
  \setlength{\tabcolsep}{0.2em}
  \centering
  \scriptsize
    \begin{tabular}{L{0.12\linewidth}C{0.06\linewidth}C{0.06\linewidth}C{0.12\linewidth}C{0.06\linewidth}C{0.06\linewidth}C{0.12\linewidth}C{0.06\linewidth}C{0.06\linewidth}C{0.08\linewidth}C{0.08\linewidth}}
    \toprule
      &\multicolumn{2}{c}{Resampler} & \multicolumn{3}{c}{Vision Encoder} & \multicolumn{3}{c}{Language Decoder} & \multirow{2}{*}[-2pt]{\makecell[c]{Trainable \\ Params. }} & \multirow{2}{*}[-2pt]{\makecell[c]{Total \\ Params. }}\\
      \cmidrule(lr){2-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9}
      & Layers & Width & Backbone & Layers & Width & Backbone & Layers & Width \\
      \midrule
      Prismer  & 4 & 768 & {\tt ViT-B/16}  & 12 & 768 & {\tt RoBERTa} & 12 & 768 & 160M & 980M  \\
      Prismer & 4 & 1024 & {\tt ViT-L/14} & 24 & 1024  & {\tt RoBERTa} & 24 & 1024 & 360M & 1.6B  \\
      \midrule
      PrismerZ  & - & - & {\tt ViT-B/16}  & 12 & 768 & {\tt RoBERTa} & 12 & 768 & 105M & 275M  \\
      PrismerZ & - & - & {\tt ViT-L/14} & 24 & 1024  & {\tt RoBERTa} & 24 & 1024 & 270M & 870M  \\
    \bottomrule
    \end{tabular}\caption{{\bf Prismer and PrismerZ architecture details.} We report the backbone we choose for each architecture size, along with its corresponding number of layers and width. We also report the number of trainable parameters and total parameters for each architecture. We count the total parameters required for data inference, which include the additional 6 modality experts with a combined parameter size of 654M parameters in our Prismer model.}
  \label{tab:arch}
\end{table}


\subsection{Training and Evaluation Details}
\paragraph{Pre-training Datasets} We construct our pre-training data from the following datasets: two in-domain datasets: COCO \cite{lin2014coco} and Visual Genome \cite{krishna17visualgenome}; and three web datasets: Conceptual Captions \cite{sharma2018cc3m}, SBU captions \cite{ordonez2011sbu_caption}, and a much noisier Conceptual 12M \cite{changpinyo2021cc12m}. The web datasets are pre-filtered and re-captioned by a pre-trained image captioner \cite{li2022blip}. The pre-training datasets include 11M unique images or 12.7M image/alt-text pairs.\footnote{This is slightly less than the theoretical number which should be 14M unique images. It is because some image URLs in the web datasets are not valid during the time we downloaded the datasets.} All datasets are available publicly and have been widely used for pre-training many VLMs \cite{li2021albef,li2022blip,chen2020uniter}. 


\paragraph{Optimisation and Implementation} All our models are trained with AdamW optimiser \cite{loshchilov2018adamw} with a weight decay of 0.05. Since only a small proportion of the model parameters are trainable, model sharding is only applied during fine-tuning on large-resolution images. Specifically, we employ ZeRO Stage 2 technique \cite{rajbhandari2020zero}, which enables the sharding of optimiser states and parameter gradients across all GPU instances. Additionally, we also apply Automatic Mixed Precision (AMP) with {\tt fp16} precision to further reduce training time. For more details on our data processing techniques and hyper-parameter choices, please refer to Appendix \ref{appendix:hyper}. An analysis of training costs compared to other vision-language models can be found in Appendix \ref{appendix:cost}.

\paragraph{Evaluation Setting} We evaluate the performance of our models through language modelling, which is a more challenging task than discriminative learning (particularly in VQA tasks), and aligns with that used in other vision-language generative models \cite{li2022blip,alayrac2022flamingo,wang2022git,chen2022pali}. For example, the model must accurately generate all text tokens for a question (which is on average 2.2 tokens per question in the VQAv2 dataset \cite{antol2015vqa} as reported in \cite{wang2022git}), rather than just one correct prediction as required in discriminative models. 


Specifically, we evaluate image captioning tasks in an open-ended setting, and we apply beam search with a beam size of 3 for text generation. A prefix prompt of ``A picture of" is added to the input text for fined-tuned image captioning tasks, similar to previous studies such as in \cite{wang2021simvlm,li2022blip,radford2021clip}, which was shown to improve the quality of image captions. We evaluate both VQA and image classification tasks in a close-ended setting, by ranking the per-token log-likelihood from a pre-defined answer list.


\subsection{Results on Vision-Language Benchmarks} 
\begin{table}[t!]
  \setlength{\tabcolsep}{0.07em}
  \centering
  \scriptsize
    \begin{tabular}{L{0.15\linewidth}C{0.1\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}C{0.07\linewidth}}
    \toprule
     &  \multirow{2}{*}[-2pt]{\makecell[c]{Pre-train \\ (\# Pairs)}}  & \multicolumn{4}{c}{COCO Caption} & \multicolumn{4}{c}{NoCaps} & \multicolumn{2}{c}{VQAv2}\\
     \cmidrule{3-6} \cmidrule(rl){7-10}\cmidrule(l){11-12}
     & & B~@~4 & M & C & S & In & Near & Out & Overall & test-dev & test-std\\
     \midrule
      OSCAR \cite{li2020oscar}  & 6.5M & 36.5 & 30.3 & 123.7 & 23.1 & 83.4 & 81.6 & 77.6 & 81.1 & 73.2 & 73.4 \\
      VinVL \cite{zhang2021vinvl} & 8.9M &  38.2 & 30.3 & 129.3 &23.6  &103.7 & 95.6 & 83.8 & 94.3 & 76.0 & 76.1\\
      GIT \cite{wang2022git} & 10M &  {\bf 40.4} & 30.0 & 131.4 & 23.0 &100.7 &97.7 & 89.6 & 96.6 & 72.7 & -\\
      BLIP \cite{li2022blip} & 129M & 39.7 & - & 133.3 & - & {\bf 111.8} & {\bf 108.6} &  111.5 & {\bf 109.6}  & {\bf 78.3} & {\bf 78.3} \\
      LEMON \cite{hu2022lemon} & 200M &  40.3 & 30.2 & 133.3 &23.3 & 107.7 & 106.2 & 107.9 & 106.8 & - & - \\
      PrismerZ & 12.7M & 39.7 & 31.1 & 133.7 & 24.1 & 108.7 & 107.8 & 105.8 & 107.5 & 76.6 & -\\
      Prismer & 12.7M &  40.1 & {\bf 31.1} & {\bf 135.1} & {\bf 24.1} & 108.8 & 108.3 & {\bf 111.7} & 109.1 & 76.8 & 77.0 \\
      \midrule
      OSCAR \cite{li2020oscar} & 6.5M & 37.4 & 30.7 & 127.8 & 23.5 & 85.4 & 84.0 & 80.3 & 83.4 & 73.4 & 73.8 \\
      VinVL \cite{zhang2021vinvl} & 8.9M &   38.5 & 30.4 & 130.8 & 23.4 &- & - & - & - & 76.5 & 76.6 \\
      GIT \cite{wang2022git} & 20M & {\bf 42.0} & 30.8 & {\bf 138.5} & 23.8 &107.7 & 107.8 &102.5 & 106.9 & 75.5 & -\\
      BLIP \cite{li2022blip} & 129M & 40.4 & - & 136.7 & - & 114.9 & 112.1 & {\bf 115.3} & 113.2  & - & -\\
      LEMON \cite{hu2022lemon} & 200M & 40.6 & 30.4 & 135.7 & 23.5  & {\bf 116.9} & {\bf 113.3} & 111.3 & {\bf 113.4} & - & - \\
      PrismerZ & 12.7M & 40.0 & 31.2 & 135.7 &24.2  & 112.3& 111.2 & 112.8 & 111.8 & 77.5 & - \\
      Prismer & 12.7M  & 40.4 & {\bf 31.4} & 136.5 & {\bf 24.4} & 114.2 & 112.5 & 113.5 & 112.9 &  {\bf 78.4} & {\bf 78.5} \\
      \midrule
      LEMON \cite{hu2022lemon} & 200M & 41.5 & 30.8 & 139.1 & 24.1  & 118.0 & 116.3 & 120.2 & 117.3 & - & - \\
      SimVLM \cite{wang2021simvlm} & 1.8B& 40.6 & 33.7 & 143.3 & 25.4 & 113.7 & 110.9 & 115.2 & 112.2 & 80.0 & 80.3 \\
      GIT \cite{wang2022git} & 0.8B & 44.1 & 31.5 & 144.8 & 24.7 & 129.8 & 124.1 & 127.1 &  125.5 & 78.6 & 78.8 \\
      GIT-2 \cite{wang2022git} & 12.9B &  44.1 & 31.4 & 145.0 & 24.8  & 126.9 & 125.8 & 130.6 & 126.9 & 81.7 & 81.9 \\
      CoCa \cite{yu2022coca} & 4.8B &  40.9& 33.9 & 143.6& 24.7 & - & - & - & 122.4&  82.3 & 82.3\\
      PaLI \cite{chen2022pali} & 1.6B & - & - & 149.1 & - & - & - & - &127.0 & 84.3 & 84.3  \\
    \bottomrule
    \end{tabular}\caption{{\bf Fine-tuned performance on COCO Caption (Karpathy split), NoCaps (validation set) and VQAv2.} Both Prismer and PrismerZ achieve superior performance in all three datasets compared to other VLMs with similar model sizes. Prismer can achieve competitive performance on par with VLMs that are trained with orders of magnitude more data. \{B@4, M, C, S\} refer to BLEU@4, METEOR, CIDEr, SPICE respectively.  \{In, Near, Out\} refer to in-domain, near-domain and out-of-domain respectively.}
  \label{tab:fine_tuned_benchmark}
  \vspace{-0.2cm}
\end{table}

\paragraph{Fine-tuned Performance on COCO Caption, NoCaps and VQAv2} We fine-tune our models on COCO Caption dataset \cite{chen2015coco_caption} on a widely adopted Karpathy split \cite{karpathy2015coco_karpathy}, with the standard cross-entropy loss, and without metric-specific optimisation \cite{vedantam2015cider}. We evaluate the fine-tuned models on the COCO Caption Karpathy test split and NoCaps \cite{agrawal2019nocaps} validation set. We also evaluate our models on the VQAv2 dataset \cite{antol2015vqa}, with additional training samples from Visual Genome \cite{krishna17visualgenome} following \cite{li2022blip}. We compare our models with prior state-of-the-art VLMs that are mostly pre-trained on image-text data for a fair comparison. We sort all VLMs by their model sizes and report the results in Table \ref{tab:fine_tuned_benchmark}. 

The results show that both Prismer and PrismerZ achieve superior performance considering their model sizes, which suggests that the strong backbone experts are primarily responsible for good generalisation. However, the modality experts provide an additional boost in performance, particularly in image captioning tasks (such as a 6 CIDEr score increase in the NoCaps out-of-domain set in the {\tt BASE} model) and in the {\tt LARGE} model variant (such as a 1 VQAv2 accuracy increase in the {\tt LARGE} model). Both Prismer and Prismer achieve comparable image captioning performance to BLIP \cite{li2022blip} and LEMON \cite{hu2022lemon}, despite being trained on 10 and 20 times less data, respectively. Additionally, the Prismer model has achieved VQAv2 accuracy comparable to GIT \cite{wang2022git}, despite being trained on 60 times less data. Whilst we acknowledge a noticeable performance gap between Prismer and the current state-of-the-art VLMs (such as CoCa \cite{yu2022coca}, GIT-2 \cite{wang2022git} and PaLI \cite{chen2022pali}), these models require substantially higher training costs and access to large-scale private training data.


\paragraph{Zero-shot Performance on Image Captioning} Our generative pre-training approach allows for zero-shot generalisation, where the models can be directly applied to image captioning tasks without additional fine-tuning. In Fig.~\ref{fig:zero} Left, we show that Prismer achieves improved performance on the NoCaps dataset compared to SimVLM \cite{wang2021simvlm} by a large margin, whilst using 140 times less training data. Additionally, we notice that the zero-shot performance of Prismer models even surpasses the fine-tuned performance of certain VLMs such as OSCAR \cite{li2020oscar} and VinVL \cite{zhang2021vinvl}, as shown in Table \ref{tab:fine_tuned_benchmark}. 

\begin{figure}[t!]
  \begin{minipage}[t]{.61\linewidth}
  \centering
  \scriptsize
  \vspace{0.5cm}
    \setlength{\tabcolsep}{0.05em}
      \begin{tabular}{L{0.2\linewidth}C{0.08\linewidth}C{0.08\linewidth}C{0.08\linewidth}C{0.08\linewidth}}
        \toprule
        & \multicolumn{4}{c}{COCO Caption} \\
        \cmidrule(lr){2-5}
         & B~@~4 & M & C & S  \\
        \midrule
          ZeroCap \cite{tewel2022zerocap} & 2.6 &  11.5 & 14.6 & 5.5 \\
          MetaLM \cite{hao2022metalm} & 24.5  & 22.5 & 82.2 & 15.7 \\
          VLKD \cite{dai2022vlkd} & 25.8  & 23.1 & 85.1  & 16.9 \\
          Flamingo \cite{alayrac2022flamingo} &  - & - & 84.3 & - \\
          CapDec \cite{nukrai2022capdec} & 26.4 &  25.1 & 91.8 & - \\
          \midrule
          Prismer & 36.1 & 29.3 & 122.6 & 22.9 \\
          Prismer & 39.5 &  30.4 &  129.7 &  23.8\\
        \bottomrule
      \end{tabular}\hfill \begin{tabular}{L{0.25\linewidth}C{0.09\linewidth}C{0.09\linewidth}}
        \toprule
        & \multicolumn{2}{c}{NoCaps} \\
        \cmidrule(lr){2-3}
         &  C & S  \\
        \midrule
        FewVLM \cite{jin2022fewvlm} & 47.7 & 9.1 \\
          MetaLM \cite{hao2022metalm}  & 58.7 & 8.6 \\
          VLKD \cite{dai2022vlkd} & 63.6 & 12.8 \\
          SimVLM \cite{wang2021simvlm}  & 96.6 & - \\
          SimVLM \cite{wang2021simvlm} & 101.4 & - \\
          \midrule
          Prismer & 87.5 & 13.0 \\
          Prismer &  107.9 & 14.8\\
        \bottomrule
        \end{tabular}\end{minipage}\hfill
  \begin{minipage}[t]{.38\linewidth}
    \vspace{0pt}
    \centering
    \small
    \scalebox{0.74}{\begin{tikzpicture}

\definecolor{color0}{RGB}{125, 111, 190}
\definecolor{color1}{RGB}{211, 82, 66}
\definecolor{color2}{RGB}{63, 159, 204}
\definecolor{color3}{RGB}{38, 192, 159}

\begin{axis}[
every axis y label/.style={at={(current axis.north west)},above=2mm},
every axis x label/.style={at={(current axis.south)},yshift=-8mm},
width=8.5cm, height=6.5cm,
legend cell align={left},
legend style={fill opacity=1.0, draw opacity=1, text opacity=1, draw=none, column sep=2pt, very thick, anchor=south east, at={(0.98,0.02)}, legend columns=2, font=\footnotesize},
tick align=outside,
tick pos=left,
xlabel={shots/class},
xmin=0, xmax=17,
xtick={1,2,4,8,16},
ytick={30,40,50,60,70,80},
ylabel={Acc. (\%)},
ymin=25, ymax=85,
]
\addlegendimage{empty legend}
\addlegendentry{}

\addplot [ultra thick, color0, mark=*, mark size=1.5, mark options={solid}]
table {1 52.50
2 56.93
4 61.45
8 65.34
16 68.32
};
\addlegendentry{Prismer}

\addlegendimage{empty legend}
\addlegendentry{}

\addplot [ultra thick, color1, mark=*, mark size=1.5, mark options={solid}]
table {1 62.35
2 66.11
4 70.19
8 74.45
16 78.3
};
\addlegendentry{Prismer}

\addplot [ultra thick, color2, mark=*, mark size=1.5, mark options={solid}]
table {1 71.7
5 77.3
};
\addlegendentry{Flamingo}

\addplot [ultra thick, color0, dashed, mark=*, mark size=1.5, mark options={solid}, opacity=0.5]
table {1 33.84
2 45.636
4 55.774
8 63.416
16 67.78
};
\addlegendentry{ViT-B/16}

\addplot [ultra thick, color3, mark=*, mark size=1.5, mark options={solid}]
table {1 72.5
5 81
};
\addlegendentry{GIT}

\addplot [ultra thick, color1, dashed, mark=*, mark size=1.5, mark options={solid}, opacity=0.5]
table {1 42.326
2 54.854
4 65.35
8 71.69
16 75.668
};
\addlegendentry{ViT-L/14}

\end{axis}

\end{tikzpicture}
 }
    \vspace{-0.2cm}
  \end{minipage}
  \caption{ {\bf Results on zero-shot image captioning and few-shot ImageNet classification.} Left: Prismer achieves state-of-the-art zero-shot image-captioning results on COCO Caption (Karpathy test) and NoCaps (validation set), outperforms SimVLM by a large margin, despite being trained on 140 times less data. Right: Prismer significantly improves few-shot performance compared to its corresponding vision backbone. However, Prismer still underperforms GIT and Flamingo which are trained on significantly more data. }
  \label{fig:zero}
  \end{figure}

We present a list of example captions generated by Prismer in Table \ref{tab:zero_shot_nocaps}. The results show that both Prismer and Prismer are capable of generating captions that are semantically coherent and aligned with the visual content of the images. Notably, Prismer generates captions of higher quality compared to  Prismer, exhibiting a deep understanding of fine-grained object semantics such as brand recognition ({\it e.g.} Mercedes, CK One), and cultural concepts  ({\it e.g.} vintage drawing, tango), indistinguishable to human-written captions.


\begin{table}[ht!]
  \setlength{\tabcolsep}{0.2em}
  \centering
  \scriptsize
    \begin{tabular}{L{0.2\linewidth}C{0.28\linewidth}C{0.24\linewidth}C{0.24\linewidth}}
      \toprule
       &  Ground-Truth & Prismer  & Prismer   \\
      \midrule
      \includegraphics*[trim={0 5cm 0 4cm}, clip,width=\linewidth]{images/caption/1.jpg} &  {\it 1. A clear bottle of CK cologne is full of liquid.}\medskip \newline {\it 2. The bottle of perfume is made by Calvin Klein.} & {\it A bottle of alcohol sitting next to a computer keyboard.} & {\it A bottle of ck one next to a computer keyboard.} \\
\includegraphics*[width=\linewidth]{images/caption/3.jpg} & {\it 1. A statue has a large purple headdress on it.}\medskip\newline {\it 2. A woman decorated in fashioned clothing and relics. } & {\it The woman is wearing a black dress.} & {\it A mannequin dressed in a black dress with feathers on her head.} \\
      \includegraphics*[width=\linewidth]{images/caption/4.jpg} & {\it 1. A new white car with the door open is in a showroom full of people.}\medskip\newline {\it 2. A shiny white mercedes car is on display. } & {\it A white car on display at a car show.} & {\it A white mercedes car on display at an auto show.} \\
      \includegraphics*[width=\linewidth]{images/caption/5.jpg} & {\it 1. Large piece of meat with slices of pineapple with cherries being held on with toothpicks on blue and white plate.}\medskip\newline {\it 2. A cake has several slices of pineapple and cheries in them.} & {\it Pineapples on a plate.} & {\it Pineapple upside down cake on a blue and white plate.} \\
      \includegraphics*[trim={0 2cm 0 2cm}, clip,width=\linewidth]{images/caption/6.jpg} & {\it 1. A man and woman is dancing as a crowd watches them in the distance.}\medskip\newline {\it 2. A woman in a red dress dancing with a bald man wearing black. } & {\it A couple of people that are standing in the dirt.} & {\it A couple dancing tango in front of a crowd.} \\
      \includegraphics*[trim={0 5.5cm 0 0.5cm}, clip,width=\linewidth]{images/caption/7.jpg} & {\it 1. Two illustrations of lobster colors are shown as Fig. 21 and Fig. 22. }\medskip\newline {\it 2. A drawing of a lobster and a lobster. } & {\it Colored drawing of two lobsters on pink paper.} & {\it A vintage illustration of lobsters from the 19th century.} \\
      \includegraphics*[width=\linewidth]{images/caption/8.jpg} & {\it 1. Man in skydiving gear giving two thumbs up with skydivers in the sky behind him.}\medskip\newline {\it 2. Person giving double thumbs up sign while others are parachuting in the background. } & {\it Man wearing a blue and purple jacket.} & {\it A man wearing a helmet and goggles with parachutes in the background.} \\
    \bottomrule
    \end{tabular}\caption{{\bf Visualisation of zero-shot image captioning on NoCaps.}  Prismer produces more detailed and semantically coherent captions than Prismer, showing an understanding of fine-grained object recognition and abstractions. Results are \textit{not} cherry-picked.  }  
    \label{tab:zero_shot_nocaps}
\end{table}


\paragraph{Few-shot Performance on ImageNet Classification} Finally, we fine-tune and evaluate Prismer on ImageNet dataset \cite{deng2009imagenet} in a few-shot setting. Following the approach outlined in \cite{radford2021clip}, we convert the classification task into a language modelling problem by mapping each unique category to a template caption: ``A photo of a [CLASS NAME]", and we then score all captions using the log-likelihood estimated by our model. Unlike Flamingo \cite{alayrac2022flamingo} which performs few-shot classification via in-context examples without gradient updates, we perform few-shot classification via lightweight fine-tuning following \cite{wang2022git}. This is more similar to the standard linear probe setting, by considering the entire language decoder as an image classifier. Accordingly, we also compare with the few-shot linear probe performance of Prismer's original vision backbones ViT-B/16 and ViT-L/14 \cite{dosovitskiy2020vit}, as reported in \cite{schuhmann2022laion5b,radford2021clip}.


From the results shown in Fig.~\ref{fig:zero}~Right, we observe that Prismer underperforms GIT \cite{wang2022git} and Flamingo \cite{alayrac2022flamingo}, which both have stronger vision backbones and are pre-trained on significantly more data. However, Prismer still outperforms its original vision backbones ViT-B and ViT-L by a large margin, especially in a very few-shot setting. This suggests that Prismer's generalisation abilities are enhanced by the multi-modal training data and expert labels, and its performance can likely be improved further by using an even stronger vision backbone.

\section{Additional Analysis}
We conduct experiments to probe Prismer carefully and discover some interesting abilities in Sec.~\ref{sec:intriguing}. And we also ablate various architectural components and training strategies in Sec.~\ref{subsec:ablative}. To speed up training, all experiments are conducted with the {\tt BASE} model on a combined dataset of the Conceptual Captions and SBU, consisting of a total of 3M data. All experiments are evaluated on the VQAv2 {\tt test-dev} split in a smaller  resolution.

\subsection{Intriguing Properties of Prismer}
\label{sec:intriguing}
\paragraph{More Experts, Better Performance} We observe that the performance of Prismer improves with the addition of more modality experts, as shown in Fig.~\ref{fig:ablative_more}. This is because more experts provide a greater diversity of domain knowledge to the model. However, we also note that the performance of the model eventually plateaus, which suggests that additional modality experts beyond a certain number do not provide any extra gains.

\paragraph{Better Experts, Better Performance} To evaluate the impact of expert quality on Prismer's performance, we construct a \textit{corrupted} depth expert by replacing a certain number of predicted depth labels with random noise sampled from a Uniform Distribution. As shown in Fig.~\ref{fig:ablative_better}, Prismer's performance improves as the quality of the depth expert improves. This is intuitive as better experts provide more accurate domain knowledge, allowing the model to perceive more accurately.

\paragraph{Robustness to Noisy Experts} Our results also show that Prismer maintains performance even when including experts that predict noise, as shown in Fig.~\ref{fig:ablative_noise}. Interestingly, adding noise can even result in a non-trivial improvement compared to training on RGB images alone, which can be considered as a form of implicit regularisation. This property allows the model to safely include many experts {\it without degrading the performance}, even when the expert is {\it not necessarily informative}. Therefore, Prismer presents a more effective learning strategy than the standard multi-task or auxiliary learning methods, which either require exploring task relationships \cite{shikun2022auto_lambda,fifty2021tag,zamir2018taskonomy} or designing more advanced optimisation procedures \cite{shikun2019maxl,navon2021auxilearn}. 


\begin{figure*}[t]
  \centering
  \captionsetup[subfigure]{font=footnotesize}
  \begin{subfigure}[t]{0.33\linewidth}
    \centering
    \resizebox{\linewidth}{!}{{\begin{tikzpicture}

\definecolor{color1}{RGB}{218, 223, 225}
\definecolor{color2}{RGB}{166, 189, 218}

\begin{axis}[
every axis y label/.style={at={(current axis.north west)},above=2mm},
width=7.5cm, height=4.5cm,
tick align=outside,
tick pos=left,
xmin=-0.4, xmax=3.4,
xticklabel style={align=center},
xtick={0,1,2,3},
xticklabels={{RGB\\\phantom{abc}},{+2 Exps},{+4 Exps},{+6 Exps}},
ylabel={Acc. (\%)},
ymin=72, ymax=73.0,
ytick={72, 72.5, 73},
yticklabels={72.0, 72.5, 73.0}, 
ticklabel style = {font=\small}
]
\draw[draw=none,fill=color1] (axis cs:-0.325,0) rectangle (axis cs:0.325,72.17);
\draw[draw=none,fill=color2] (axis cs:0.675,0) rectangle (axis cs:1.325,72.64);
\draw[draw=none,fill=color2] (axis cs:1.675,0) rectangle (axis cs:2.325,72.79);
\draw[draw=none,fill=color2] (axis cs:2.675,0) rectangle (axis cs:3.325,72.82);
\node[above] at (0, 72.17) {\footnotesize 72.17};
\node[above] at (1, 72.64) {\footnotesize 72.64};
\node[above] at (2, 72.79) {\footnotesize 72.79};
\node[above] at (3, 72.82) {\footnotesize 72.82};
\end{axis}


\end{tikzpicture}
 }} 
    \caption{{\footnotesize  with More Experts}}
    \label{fig:ablative_more}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.33\linewidth}
    \centering
    \resizebox{\linewidth}{!}{{\begin{tikzpicture}

\definecolor{color1}{RGB}{218, 223, 225}
\definecolor{color2}{RGB}{166, 189, 218}

\begin{axis}[
every axis y label/.style={at={(current axis.north west)},above=2mm},
width=7.5cm, height=4.5cm,
tick align=outside,
tick pos=left,
xmin=-0.4, xmax=3.4,
xticklabel style={align=center},
xtick={0,1,2,3},
xticklabels={RGB,{+Depth\\25\% N. },{+Depth\\10\% N.},{+Depth\\ No N.}},
ylabel={Acc. (\%)},
ymin=72, ymax=73.0,
ytick={72, 72.5, 73},
yticklabels={72.0, 72.5, 73.0}, 
ticklabel style = {font=\small}
]
\draw[draw=none,fill=color1] (axis cs:-0.325,0) rectangle (axis cs:0.325,72.17);
\draw[draw=none,fill=color2] (axis cs:0.675,0) rectangle (axis cs:1.325,72.44);
\draw[draw=none,fill=color2] (axis cs:1.675,0) rectangle (axis cs:2.325,72.52);
\draw[draw=none,fill=color2] (axis cs:2.675,0) rectangle (axis cs:3.325,72.59);
\node[above] at (0, 72.17) {\footnotesize 72.17};
\node[above] at (1, 72.44) {\footnotesize 72.44};
\node[above] at (2, 72.52) {\footnotesize 72.52};
\node[above] at (3, 72.59) {\footnotesize 72.59};
\end{axis}


\end{tikzpicture}
 }} 
    \caption{{\footnotesize  with Better Experts}} 
    \label{fig:ablative_better}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.33\linewidth}
    \centering
    \resizebox{\linewidth}{!}{{\begin{tikzpicture}

\definecolor{color1}{RGB}{218, 223, 225}
\definecolor{color2}{RGB}{166, 189, 218}

\begin{axis}[
every axis y label/.style={at={(current axis.north west)},above=1mm},
width=7.5cm, height=4.5cm,
tick align=outside,
tick pos=left,
xmin=-0.4, xmax=3.4,
xticklabel style={align=center},
xtick={0,1,2,3},
xticklabels={{RGB},{+Noise},{+Depth},{+Depth\\ \& Noise}},
ylabel={Acc. (\%)},
ymin=72, ymax=73.0,
ytick={72, 72.5, 73},
yticklabels={72.0, 72.5, 73.0}, 
ticklabel style = {font=\small}
]
\draw[draw=none,fill=color1] (axis cs:-0.325,0) rectangle (axis cs:0.325,72.17);
\draw[draw=none,fill=color1] (axis cs:0.675,0) rectangle (axis cs:1.325,72.26);
\draw[draw=none,fill=color2] (axis cs:1.675,0) rectangle (axis cs:2.325,72.59);
\draw[draw=none,fill=color2] (axis cs:2.675,0) rectangle (axis cs:3.325,72.58);
\node[above] at (0, 72.17) {\footnotesize 72.17};
\node[above] at (1, 72.26) {\footnotesize 72.26};
\node[above] at (2, 72.59) {\footnotesize 72.59};
\node[above] at (3, 72.58) {\footnotesize 72.58};
\end{axis}


\end{tikzpicture}
 }} 
    \caption{{\footnotesize with Noisy Experts}}   
    \label{fig:ablative_noise}
  \end{subfigure}
  \caption{{\bf Prismer's VQAv2 accuracy with different types and the number of experts.} Prismer has shown that its performance improves with an increase in the number and quality of modality experts. Additionally, Prismer also demonstrates its strong robustness to noisy experts, making it a practical and effective multi-modal learning strategy.}
\end{figure*}

\subsection{Architecture Design and Training Details}
\label{subsec:ablative}

\begin{table}[t!]
  \setlength{\tabcolsep}{0.1em}
  \centering
  \scriptsize
    \begin{tabular}{L{0.24\linewidth}L{0.22\linewidth}L{0.21\linewidth}C{0.1\linewidth}C{0.1\linewidth}C{0.1\linewidth}}
    \toprule
     Ablated Component & Our Setting & Changed Setting & \makecell[b]{Params. \\ (Rel.)} & Step Time (Rel.) & VQAv2 (Acc.) \\
     \cmidrule(r){1-3} \cmidrule(lr){4-5} \cmidrule(l){6-6}
     \multicolumn{3}{c}{\bf Prismer (our setting with reduced training)} & {\bf 1.00} & {\bf 1.00} &  {\bf  72.79} \\
     \midrule
     \multirow{2}{*}{\makecell[l]{(i) Adapter  Design }} & \multirow{2}{*}{\makecell[l]{Residual MLP}} & Residual MLP  & 1.04  & 1.02 &  72.36 \\
     &   & Gated Residual MLP &  1.03 & 1.03 &  70.54  \\
      \midrule
     \multirow{2}{*}{\makecell[l]{(ii) Adapter  Bottleneck Dim. }} & \multirow{2}{*}{\makecell[l]{1}} &  1/2 & 0.95 & 0.96 & 72.52\\
     &   & 1/4  & 0.93 & 0.93 & 71.66 \\
     \midrule
     \multirow{3}{*}{\makecell[l]{(iii) Resampler Design}} & \multirow{3}{*}{\makecell[l]{Experts Perceiver}} &  Random Sampling & 0.91 & 0.96 &72.24 \\
     &   & Full Perceiver & 1.00  & 0.90 & 65.05 \\
     &   & Dual Perceiver & 1.08 & 1.02 & 71.56 \\
     \midrule
     \multirow{3}{*}{\makecell[l]{(iv) Resampler Layers}} & \multirow{3}{*}{\makecell[l]{4}} &  1 & 0.94 & 0.93 & 70.61 \\
     &   & 2 & 0.96& 0.96 &72.39 \\
     &   & 6 & 1.04 & 1.01 &72.78 \\
     \midrule
     \multirow{3}{*}{\makecell[l]{(v) Resampler Latents}} & \multirow{3}{*}{\makecell[l]{64}} &  32 & 1.00 & 0.95 &72.44\\
     &   & 128 & 1.00 &1.01 &70.28\\
     &   & 256 & 1.00 &1.06 & 68.07\\
     \midrule
     \multirow{3}{*}{\makecell[l]{(vi) Pre-training}} & \multirow{3}{*}{\makecell[l]{Freeze Vision and Lang.}} &  Freeze Vision Only & 1.00  & 1.07 & 70.49\\
     &   & Freeze Lang. Only & 1.00 & 1.05  &67.77\\
     &   & All Parameters & 1.00 & 1.15  &68.13\\
     \midrule
     \multirow{3}{*}{\makecell[l]{(vii) Fine-tuning}} & \multirow{3}{*}{\makecell[l]{Freeze Vision}} & Freeze Vision and Lang. &  1.00 & 1.00 & 71.36 \\
     &   & Freeze Lang. Only & 1.00 & 1.00 &70.37\\
     &   & All Parameters & 1.00 & 1.00 &68.69\\
    \bottomrule
    \end{tabular}\caption{{\bf Ablation studies for architecture components and training strategies.} We perform ablation studies to evaluate the impact of different architectural components and training strategies on the VQAv2 {\tt test-dev} performance. We compare the performance of our default setting to other design and training options. The number of parameters and pre-training step time of the changed setting relative to the default setting are reported. To ensure a fair comparison, all experiments are evaluated using a reduced amount of training data and 3 modality experts: depth, normal and segmentation.}
  \label{tab:ablative}
\end{table}

\paragraph{Adaptor Design and Size} In our ablation study of adaptor designs, as shown in row (i) and (ii) of Table~\ref{tab:ablative}, we find that the most straightforward adaptor design, consisting of a standard residual connection and an encoder-decoder structure, performs the best. We have experimented with more intricate designs, such as adding an additional adaptor at the end of each transformer layer or incorporating a learnable gating mechanism akin to the one shown in \cite{liu2021gatedmlp}, but both have resulted in inferior performance. Furthermore, we observe that a larger bottleneck hidden size for the single adaptor has led to improved performance.

\paragraph{Resampler Design and Multi-modal Sampling Strategy} In our ablation study of Experts Resampler designs and various strategies for encoding multi-modal signals, as shown in row (iii) - (v) of Table~\ref{tab:ablative}, we find that using lightweight designs for the resampler layers and latent variables is crucial for stable training.  Our experiments also show that using a non-learnable random sampling approach resulted in a  slightly lower performance compared to using a learnable resampler. We have also attempted to optimise the resampler by receiving all input signals, including RGB information, but this approach has also resulted in a significant decline in performance. Finally, incorporating an extra resampler at the end of the vision encoder is not beneficial, though it may help in reducing and keeping a constant memory usage independent of the  image resolutions, it ultimately leads to a decrease in performance.


\paragraph{The Effect of Frozen Backbones}  In our experiments on pre-training and fine-tuning whilst freezing different parts of the model, as shown in row (vi) and (vii) of Table~\ref{tab:ablative}, we find that freezing pre-trained parameters is essential for achieving strong performance and avoiding over-fitting and catastrophic forgetting of the learned knowledge.\footnote{We assume the size of our pre-training multi-modal data is significantly smaller than the original pre-training data used to train the backbone experts.} Freezing these parameters also saves a significant amount of GPU memory. Even when fine-tuning on different downstream tasks, we find that freezing the vision encoder is beneficial (while allowing the resampler and adaptors to be trainable). This observation is consistent with the findings in \cite{zhai2022lit}, which demonstrates that fine-tuning only the language model with a frozen vision model can produce a much stronger zero-shot vision-language retrieval performance.



\section{Conclusions, Limitations and Discussion}
In this paper, we have introduced Prismer, a vision-language model designed for reasoning tasks. Prismer is parameter-efficient and utilises a small number of trainable components to connect an ensemble of diverse, pre-trained experts. By leveraging these experts, Prismer achieves competitive performance in image captioning, VQA, and image classification benchmarks, comparable to models trained on up to two orders of magnitude more data.

For full transparency, we now discuss some limitations of Prismer during our implementation and explore potential future directions for this work.

\paragraph{Multi-modal In-context Learning} Zero-shot in-context generalisation is an emergent property that only exists in very large language models \cite{brown2020gpt3,wei2022emergent}. In this work, we build Prismer on top of a small-scale language model with the main focus on parameter-efficient learning. Therefore, it does not have the ability to perform few-shot in-context prompting by design.

\paragraph{Zero-shot Adaptation on New Experts} We experiment with inference on a pre-trained Prismer with a different segmentation expert pre-trained on a different dataset. Although we apply the same language model to encode semantic labels, Prismer shows limited adaptability to a different expert with a different set of semantic information, which leads to a notable performance drop.

\paragraph{Free-form Inference on Partial Experts} Similarly, we discover that Prismer entangles its multi-modal features from all experts we include during pre-training. Therefore, only having a partial number of experts during inference will lead to a notable performance drop. We attempt to use a different training objective such as masked auto-encoding \cite{bachmann2022multimae}, to design Prismer to reason on an arbitrary number of experts, but it eventually leads to a degraded fine-tuned performance.

\paragraph{Representation of Expert Knowledge} In our current design of Prismer, we convert all expert labels into an image-like 3-dimensional tensor via modality-specific post-processing for simplicity. There are other efficient methods to represent expert knowledge, such as converting object detection labels into a sequence of text tokens \cite{chen2021pix2seq,chen2022pix2seqv2}. This may potentially lead to a stronger reasoning performance and a more stable training landscape in future works.

\newpage
\bibliography{../../references}
\bibliographystyle{plain}

\newpage
\appendix

\section{Detailed Training Strategy and Hyper-parameters}
\label{appendix:hyper}

All models are pre-trained with  image resolution, and we evaluate the models on three types of vision-language reasoning tasks: image captioning, visual question answering (VQA), and image classification. We fine-tune the models with a larger resolution  on image captioning and VQA tasks, and  on image classification tasks. Automated data augmentation \cite{cubuk2020randaugment} is applied for both pre-training and fine-tuning. A list of the hyper-parameters used in the experiments can be found in Table \ref{tab:hyper}.


\begin{table}[ht!]
  \setlength{\tabcolsep}{0.2em}
  \centering
  \footnotesize
    \begin{tabular}{L{0.15\linewidth}C{0.2\linewidth}C{0.2\linewidth}C{0.2\linewidth}C{0.2\linewidth}}
    \toprule
      & Pre-training  & COCO / NoCaps & VQAv2 & ImageNet \\
      \midrule
    Optimiser & \multicolumn{4}{c}{AdamW} \\
    LR Schedule & \multicolumn{4}{c}{Cosine annealling to zero} \\
    Weight Decay & \multicolumn{4}{c}{0.05} \\
    Warmup Steps & 2000 & 0 & 0 & 0\\
    Initial LR &  (B / L) &   &  & \\
    Resolution & 224 & 480 & 480 & 384  \\
    Epochs & 20 & 3 & 10 & 20\\
    Batch Size & 1024 & 256 & 512 & 64  \\
    \bottomrule
    \end{tabular}\caption{{\bf The detailed list of hyper-parameters and training strategy.} To ensure reproducibility, we have included a list of all hyper-parameters used in our experiments. These same hyper-parameters are applied to both the {\tt BASE} and {\tt LARGE} model variants. }
  \label{tab:hyper}
\end{table}

\section{Comparison of Training Cost}
\label{appendix:cost}

Prismer is highly efficient in terms of the training cost. The largest model variant, Prismer, only requires 8 days of training on 32 NVIDIA V100 GPUs. This is significantly more efficient than previous state-of-the-art VLMs such as SimVLM \cite{wang2021simvlm} which requires 5 days of training on 2048 TPUv3, GIT-2 \cite{wang2022git} which requires 1.5 months of training on 1500 NVIDIA A100s, and Flamingo \cite{alayrac2022flamingo} which requires 2 weeks of training on 1536 TPUv4. A detailed breakdown of the pre-training cost can be found in Table \ref{tab:cost}.

\begin{table}[ht!]
  \setlength{\tabcolsep}{0.2em}
  \centering
  \footnotesize
    \begin{tabular}{L{0.15\linewidth}C{0.2\linewidth}C{0.2\linewidth}C{0.2\linewidth}C{0.15\linewidth}}
    \toprule
      & Model Params. &  \makecell{Pre-training Data \\ (\# Image-Text Pairs)} & \makecell{Pre-training Cost\\  (\# PFlops Days)} \\
      \midrule
      BLIP  & 583M & 129M  & 22.2 \\
      SimVLM & 1.4B & 1.8B & 66.9 \\
      GIT & 681M & 0.8B & 45.8 \\
      PaLI & 17B & 2.3B & 450\\
      Flamingo & 80B & 2.3B & 1.4K \\
      GIT-2 & 5.1B & 12.9B & 5.5K \\
      \midrule
      Prismer & 980M & 12.7M & 0.66 \\
      Prismer & 1.6B & 12.7M & 1.9\\
    \bottomrule
    \end{tabular}\caption{{\bf Training cost of vision-language models.} We compare the training cost of Prismer with several other vision-language models using the approximation method from \cite{brown2020gpt3}. The symbol  represents the training cost approximated by \cite{chen2022pali}, and  represents the training cost approximated by us.}
  \label{tab:cost}
\end{table}



\end{document}
