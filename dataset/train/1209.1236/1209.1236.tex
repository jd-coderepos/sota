\documentclass[10pt,conference,letterpaper]{IEEEtran}
\makeatletter
\def\ps@headings{\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}\def\@oddfoot{}\def\@evenfoot{}}
\makeatother
\pagestyle{headings}
\usepackage{richard_config}




\begin{document} 

\title{Coordination of autonomic functionalities in communications networks}

\author{
\IEEEauthorblockN{Richard Combes\IEEEauthorrefmark{1}, Zwi Altman\IEEEauthorrefmark{1} and Eitan Altman\IEEEauthorrefmark{2}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Orange Labs\\
38/40 rue du G\'en\'eral Leclerc,92794 Issy-les-Moulineaux\\
Email:\{richard.combes,zwi.altman\}@orange.com}

\IEEEauthorblockA{\IEEEauthorrefmark{2}INRIA Sophia Antipolis\\
06902 Sophia Antipolis, France\\
Email:Eitan.Altman@sophia.inria.fr}
}

\date{\today}
\maketitle

\begin{abstract}
	Future communication networks are expected to feature autonomic (or self-organizing) mechanisms to ease deployment (self-configuration), tune parameters automatically (self-optimization) and repair the network (self-healing). Self-organizing mechanisms have been designed as stand-alone entities, even though multiple mechanisms will run in parallel in operational networks. An efficient coordination mechanism will be the major enabler for large scale deployment of self-organizing networks. We model self-organizing mechanisms as control loops, and study the conditions for stability when running control loops in parallel. Based on control theory and Lyapunov stability, we propose a coordination mechanism to stabilize the system, which can be implemented in a distributed fashion. The mechanism remains valid in the presence of measurement noise via stochastic approximation. Instability and coordination in the context of wireless networks are illustrated with two examples and the influence of network geometry is investigated. We are essentially concerned with linear systems, and the applicability of our results for non-linear systems is discussed.
\end{abstract}
\begin{IEEEkeywords}
Self-Organizing Networks;Self-Organization;Self-Optimization;Self-configuration;Stochastic approximation;Stability;Coordination;Dynamical Systems;      
\end{IEEEkeywords}
\section{Introduction}
Deployment, optimization and maintenance of communication networks are complex tasks which occupy thousands of network engineers everyday. In order to ease this burden and reduce costs of operations, researchers and industrials have proposed to include autonomic functionalities in future networks. Networks with autonomic entities are often called \ac{SON}. Self-organization should enable at least partial automation of the configuration of newly deployed network node (self-configuration), of parameter tuning (self-optimization) and of reparation of faulty network nodes (self-healing).

In wireless networks, SON functionalities such as \ac{ICIC}, load balancing, management of \ac{BS} sleep mode, mobility management and drive test automation have been identified as important use cases (\cite{3gpp.36.902}). Future standards for wireless networks such as \ac{LTE} feature \ac{SON} functionalities. In previous contributions, \ac{SON} functionalities have been designed as stand-alone entities (\cite{StolyarInfocom2009,CombesPeva2011,CombesInfocom2012,ElayoubiSakerinfocom2011,CombesCNSM2011}). Such contributions did not take into account the interaction between different \ac{SON} functionalities operating simultaneously. Since \ac{SON} functionalities are numerous, a large number of \ac{SON} entities might operate simultaneously in the network. This poses a fundamental stability issue, if we cannot guarantee stable interaction of multiple \ac{SON} entities. It is fair to say that a robust coordination mechanism will be the main enabler for large-scale deployment of \ac{SON}.

\emph{Our contribution:} The main contribution of this paper is to propose a generic mathematical model for the interaction of multiple \ac{SON} mechanisms running in parallel, along with a practically implementable coordination mechanism. We model \ac{SON} mechanisms as control loops, so that the system can be described by an \ac{ODE}. Stability is studied using the Lyapunov approach used by control theorists. In particular, stability conditions can be stated in terms of \emph{linear matrix inequalities} (\cite{LMIControlBoyd}). When stability does not hold naturally, we propose a \emph{coordination} mechanism which forces stability. The design of a coordination mechanism corresponds, in the context of control theory, to the concepts of \emph{controllability} and \emph{state-feedback synthesis}. We propose a generic coordination mechanism which can be implemented in a distributed way. When the network is controlled using measurements corrupted by additive noise, the results remain valid using stochastic approximation theorems (\cite{Kushner}). 

	To the best of our knowledge, no previous contribution had studied the problem of \ac{SON} coordination using this control theory/stochastic approximation-based framework and provided a generic coordination mechanism which is practically implementable (distributed and possibly asynchronous).

The rest of the paper is organized as follows: in Section~\ref{sec:parallel_control_loops} we state the proposed model for interaction of \ac{SON} mechanisms running in parallel and the coordination problem to be solved. In Section~\ref{sec:linear_case} we examine the case where performance indicators are linear functions of the parameters, and propose a practically implementable coordination mechanism. In Section~\ref{sec:application} we illustrate the application of our model to traffic management in wireless networks with two examples. Section~\ref{sec:conclusion} concludes the paper. In appendices~\ref{app:odes} and~\ref{app:linear_ode} we recall the basic notions of stability for \acp{ODE} and linear \acp{ODE} respectively.
\section{SON coordination as parallel control loops}\label{sec:parallel_control_loops}
\subsection{The model}
	A \ac{SON} mechanism is an entity which monitors a given performance indicator and controls a scalar parameter. The current value of the performance indicator is observed, and the parameter is modified accordingly to attain some objective. We consider  \ac{SON} mechanisms operating simultaneously. We define  the parameter controlled by the -th \ac{SON} mechanism and  the vector of parameters. The -th \ac{SON} mechanism monitors a performance indicator and updates its parameter  proportionally to .  is the direction of update of .

	We say that the -th \ac{SON} mechanism operates in \emph{stand-alone} mode if all parameters but  are kept constant. Namely all the other mechanisms are shut down. The -th \ac{SON} mechanism operating in stand-alone is described by the \ac{ODE}:

	We say that the \ac{SON} mechanisms operate in \emph{parallel} mode if all parameters are modified simultaneously, which is described by the \ac{ODE}:

 	We say that the -th \ac{SON} mechanism is stable in stand-alone mode if there exists  which is \emph{asymptotically stable} for~\eqref{eq:son_standalone}. The definition of asymptotic stability is recalled in appendix~\ref{app:odes}. It is noted that  can depend on , . We say that the \ac{SON} mechanisms are stable in parallel mode if there exists  which is asymptotically stable for~\eqref{eq:son_simultaneous}. Typically, the \ac{SON} mechanisms are designed and studied in a stand-alone manner, so that stand-alone stability is verified. 
	
	However, stand-alone stability does not imply parallel stability. First consider a case where  does not depend on , for all pairs  , . Then~\eqref{eq:son_simultaneous} is a set of  parallel independent \acp{ODE}, so that stand-alone stability implies parallel stability. On the other hand, if there exists  such that  depends on , then the situation is not so clear-cut. We say that \ac{SON}  and  \emph{interact}. Namely, interaction potentially introduces \emph{instablity}.
	
	In the remainder of this article we will be concerned with conditions for parallel stability, and designing coordination mechanisms to force stability whenever possible.
\subsection{Examples of parallel mechanisms}
	Two particular cases of parallel mechanisms will be of interest. The first case is what we will call \emph{zero-finding} algorithms. Each \ac{SON} mechanism monitors the value of a performance indicator and tries to achieve a fixed target value for this performance indicator. Namely:

	where  is the performance indicator monitored by \ac{SON}  and  - a target level for this performance indicator. The goal of the -th \ac{SON} mechanism is to find  such that . If  is strictly decreasing  then stand-alone stability is assured. 

	Another case of interest is maximization algorithms. Each \ac{SON} mechanism tries to maximize a given performance indicator. There exists a continuously differentiable function  such that:

	In stand-alone mode, \ac{SON}  indeed converges to a local maximum of . If we restrict  to a closed, convex and bounded set and if  is concave  , we fall within the framework of \emph{concave games} considered in \cite{RosenConvexGame}. An important result of \cite{RosenConvexGame} is that if we add an assumption called \emph{diagonal strict convexity}, then parallel stability occurs. However, diagonal strict convexity is fairly restrictive, and without it there is no guarantee that parallel stability occurs, and coordination is needed.
\subsection{Discrete time algorithms}
	Our model is based on \acp{ODE}, which are both \emph{deterministic} and \emph{continuous-time} objects. In a practical system, the parameters evolve in discrete time. Furthermore, the parameters evolve stochastically because they are updated based on noisy feedback, due to measurement noise. Denote by  the value of parameters at time . At each parameter update, performance indicators are estimated based on measurements so that \ac{SON}  obtains the quantity , with . The additive noise  is introduced by the measurements. The parameters are updated using the noisy feedback:

where  is a small constant.	Stochastic approximation (\cite{Kushner,Borkar}) gives a strong link between the noisy, discrete-time~\eqref{eq:son_stoch_approx} and the \ac{ODE} studied in our model~\eqref{eq:son_simultaneous}. In particular it can be shown that (under some technical conditions)  converges to \emph{asymptotically stable} sets of the \ac{ODE} when . The main point is that our model based on \acp{ODE} is sufficient to study the stability of the discrete-time, noisy algorithms used in practical systems. 

\section{Coordination for linear systems}\label{sec:linear_case}
\subsection{Linear systems}
		In most of the remainder of this paper, we will study the case where  is affine:

with  a vector of size  and  a matrix of size . We assume that  is invertible and we define . The \ac{SON} mechanisms running in parallel are described by the linear \ac{ODE}:

	It is noted that in the linear case, we always fall within the scope of zero-finding algorithms described previously, by defining:

	In particular, stand-alone stability occurs iif  , , i.e all the diagonal terms of  are strictly negative. Basic results on linear \acp{ODE} are recalled in appendix~\ref{app:linear_ode}. Namely, parallel stability holds iif all the eigenvalues of  have strictly negative real part.

\subsection{Coordination}
\subsubsection{Coordination mechanism}
	If  has at least one eigenvalue with positive or null real part, convergence to  does not occur, and a coordination mechanism is needed. We consider a \emph{linear} coordination mechanism, where  is replaced by  with  a  real matrix. The \ac{ODE} for the coordinated system is:

	 Define  as:

	The coordination mechanism can be interpreted as transforming the performance indicator monitored by \ac{SON}  from  to a linear combination of the performance indicators monitored by all the \ac{SON} mechanisms . As explained in appendix~\ref{app:linear_ode}, stability is achieved if there exists a symmetric matrix   such that:

where  denotes positive definitiveness for symmetric matrices. In particular, 
 
acts as a Lyapunov function.
\subsubsection{Distributed implementation}	
	It is noted that the choice for the coordination matrix  is not unique. For instance  ensures stability. For the coordination mechanism to be scalable with respect to the number of \acp{SON},  should be chosen to allow \emph{distributed} implementation. We say that \ac{SON}  is a neighbor of \ac{SON}  if . We define  the set of neighbors of . The coordination mechanism is distributed if each \ac{SON} needs only to exchange information with its neighbors.
	
	We give an example of a coordination mechanism which can always be distributed. The mechanism is based on a \emph{separable} Lyapunov function. Define the weighted square error:

with  strictly positive weights and  the diagonal matrix with diagonal elements .
	Coordination is achieved by following the gradient of  so that  is a Lyapunov function:

	Namely, we choose . We can verify that the mechanism is distributed:

	Indeed, the update of  only requires knowledge of  and , for , and this information is available from the \emph{neighbors} of .
	
	It is also noted that such a mechanism can be implemented in an \emph{asynchronous manner}, i.e the components of  are updated in a round-robin fashion, or at random instants, and the average frequency of update is the same for all components. The reader can refer to \cite{Bertsekas}[chapters 6-8] for the round-robin updates  and \cite{Kushner}[chapter 12] for the random updates. Asynchronous implementation is important in practice because if the \acp{SON} are not co-located, maintaining clock synchronization among the \acp{SON} would generate a considerable amount of overhead.
\subsection{Applicability of the linear model and parameter estimation}
	For practical systems, performance indicators  are not linear functions of . However, as long as they are smooth, they can be approximated by linear functions using a Taylor expansion. Consider a point  with . If the values of  are restricted to a small neigborhood of :

with  the Jacobian of  evaluated at . The Hartman-Grossman theorem (\cite{HartmanGrossman}) states that on a neigbourhood of , stability of the system with linear approximation implies stability of the original, non-linear system. Hence implementing the proposed coordination mechanism where  is replaced by  ensures stability if we constrain  to a small neighborhood of .
	
	The parameters  and  might be unknown, and we can only access to noisy values of  for different values of . The crudest approach is to estimate  and  through finite differences:
	
	with  the -th unit vector. The results are averaged over several successive measurements and additive noise is omitted for notation clarity. In general, the measurements of  are obtained by calculating the time average of some output of the network during a relatively long time, so that a form of the central limit theorem applies and the additive noise is Gaussian. In this case, a better method is to employ \emph{least-squares regression}. Least-squares regression is a well studied topic  with very efficient numerical methods (\cite{LeastSquaresBjorck}) even for large data sets so that estimation of  and  is not computationally difficult.
	
	Finally, since practical systems do not remain stationary for an infinite amount of time, a database with values of  and  for each set of operating conditions must be maintained. In the context of wireless networks, the relationship between parameters and performance indicators changes when the traffic intensity changes because of daily traffic patterns. For instance, during the night traffic is very low, and traffic peaks are observed at the end of the day. A database with estimated values of  and  at (for instance) each hour of the day should be constructed. 
	
\section{Application to wireless networks}\label{sec:application}
	In this section we illustrate instability and coordination in the context of wireless networks using two examples. We show that instability occurs even in simple models with as few as two \acp{SON} in parallel.
\subsection{Admission control and resource allocation}
\subsubsection{Model}
	We consider a \ac{BS} in downlink, serving elastic traffic. Users enter the network according to a Poisson process with arrival rate , to download a file of exponential size , with . The \ac{BS} has  parallel resources available, and we write  the amount of resources used. We ignore the granularity of resources, either assuming that there are a large number of resources or using time sharing, using each resource a proportion  of the time. Depending on the access technology, resources can be: codes in \ac{CDMA}, time slots in \ac{TDMA}, time-frequency blocks in \ac{OFDMA} etc. When a user is alone in the system, his data rate is . Users are served in a processor sharing manner (for instance through Round Robin scheduling): if there are  active users, each user has a throughput of . Admission control applies. We define  a blocking threshold and the probability of accepting a new user when  users are already present in the system is  with  a smooth, strictly decreasing function and . We choose  as a logistic function for numerical calculations:

	
	Define  the number of active users in the system at time , then  is a continuous time Markov chain.  is ergodic because the probability of accepting a new user goes to  as . We define the load:

We write  the stationary distribution of .  is reversible, and  can be derived from the detailed balance conditions:

Using Little's law, the mean file transfer time is given by the average number of active users divided by the arrival rate:

Let  a minimal data rate required to ensure good \ac{QoS}. We say that there is an outage in a state of the system if users have a throughput lower than . When there are  active users in the system, outage occurs if and only if:

The outage probability is then:

In this model,  is not smooth, which is why we introduce the smoothed outage : 

with  a smooth function approximating  . We also choose  as a logistic function for numerical calculations.

 This queuing system is controlled by two mechanisms, and that control occurs on a time scale much slower than the arrivals and departures of the users, so that the mean file transfer time and outage probability are relevant performance metrics, and can be estimated from (noisy) measurements. The mechanisms are:
\begin{itemize}
	\item \emph{Resource allocation:} a mechanism adjusts the amount of used resources to reach a target outage rate. Such mechanisms have been considered in green networking when a \ac{BS} can switch off part of its resources in order to save energy. 
		
	Another application is interference coordination: using more resources will create inter-cell interference in neighboring \acp{BS} and degrade their \ac{QoS}. Hence \acp{BS} should use as little resources as possible, as long as their target \ac{QoS} is met.
	\item \emph{Admission control: } another mechanism adjusts the admission control threshold to reach a target file transfer time. In particular, it is noted that without admission control, the mean file transfer time becomes infinite in overload.
\end{itemize}
It is noted that  is strictly decreasing and \newline  is strictly increasing. Using the notations of Section \ref{sec:parallel_control_loops}, we have  control loops, , , , . Consider  an operating point. The stability in the neighborhood of  can be calculated. The system will fail to converge to the desired operating point as long as the Jacobian matrix has a negative determinant, so that there are two eigenvalues of opposite sign:
	
\subsubsection{Results}
	We now evaluate the stability region of the system numerically by checking condition~\eqref{eq:instability_condition} for various operating points. We choose the following parameter values: , , , , . Figure~\ref{fig:stability_region} presents the results. In the white region the system is stable, and in the gray region it is unstable. Even in such a simple setting with  \ac{BS} and  \ac{SON} mechanisms, instability occurs for a large set of operating points.
	\begin{figure}[htbp]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/stability_region.eps}
		\end{center}
		\caption{Stability region of the system}
		\label{fig:stability_region}
	\end{figure}
\subsection{Distributed interference control}
\subsubsection{Model}
	We turn to a case in which the \acp{SON} are not co-located, and distributed coordination is required. We study a model in which each \ac{BS} can choose its transmitted power in order to control the \ac{QoS} in the network. We consider a dense network, and the objective of each \ac{BS} is to make sure that their neighboring \acp{BS} achieve a target coverage probability, by not transmitting at too high power. We define the coverage probability as the proportion of users whom achieve a given minimum data rate. This is relevant to current wireless networks, where \acp{BS} are linked with an interface (called X2 interface in the \ac{LTE} standard), so that a \ac{BS} experiencing low data rates or congestion can send an alarm its neighbors to force them to transmit at lower power to reduce inter-cell interference, or to offload some of its traffic to reduce congestion.
	
	We consider  \acp{BS} serving a bounded area . The zone served by   is written . Interference from neighboring cells is treated as Gaussian noise. We denote by  the signal attenuation between \ac{BS}   and location .  includes both path-loss and shadowing. For numerical calculations, we use the classical model where the signal attenuation (in dB) is a linear function of the distance, and shadowing is (in dB) a centered Gaussian random variable. Fast-fading is ignored. \ac{BS}  transmits at power . The \ac{SINR} at location  is calculated by:

with  the thermal noise.
	We denote by  the data rate at location  when no other users are being served by \ac{BS} . The data rate  is calculated using the Shannon formula:

with  the system bandwidth. Consider a target data rate . We define  the coverage probability for \ac{BS} , which is the probability that a user arrives in  at a location  such that . The coverage probability can be calculated by:

	Consider users arriving in the network according to a space-time Poisson process of intensity , i.e the number of users arriving during time  in a region of size  centered at location  is . Then the number of users arriving during time interval  in  which have a data rate superior to , divided by  is an unbiased estimate of . We define  the set of neighbors of \ac{BS} . The neigbouring relation can either come from geographical proximity, or from propagation conditions, so that neighboring \acp{BS} are \acp{BS} which strongly interfere each other.
 	
 We define  the coverage probability of the neighbours of \ac{BS}  by:
 
	We study the case where each \ac{BS} adjusts its transmitted power  to avoid degrading the network \ac{QoS} and make sure that its neighbors reach a target coverage probability . \ac{BS}  transmitting at higher power decreases the data rate of users in  , , so that  is strictly decreasing. It is noted that we work with  in dB (not in linear scale). Using the notations of Section \ref{sec:parallel_control_loops}, we have  \acp{SON}, with , and .
	
	\subsubsection{Results}
	For numerical calculations, the signal attenuation (in dB) at distance  (in km) is . The shadowing standard deviation is . Thermal noise power spectral density is , the system bandwidth  is . The minimal data rate  is , corresponding to a minimal \ac{SINR} of . 
	
	We first consider an hexagonal network with  \acp{BS} and inter-site distance of  m, using a wrap-around to avoid border effects and ensure that all \acp{BS} are symmetric.  Namely the \acp{BS} are placed on a torus. We are interested in stability of the operating point  ,  , . The corresponding target value for  is , . Without coordination, this operating point is unstable as shown by calculating the Jacobian of  at  using finite differences and calculating its eigenvalues. Another illustration of instability is given by plotting trajectories of the corresponding (non-linear) \ac{ODE} starting in a neighbourhood of . On figures~\ref{fig:hexa_network_powers} and~\ref{fig:hexa_network_coverage} we represent the transmitted powers and coverage probabilities respectively as a function of time obtained by discretization of the \ac{ODE}. Only \acp{BS} ,  and  are represented for clarity , where \acp{BS}  and  are neighbors of \ac{BS} . Clearly,  is not stable and the plotted solution does not remain close to . On figures~\ref{fig:hexa_network_powers_coord} and~\ref{fig:hexa_network_coverage_coord} we show the same quantities when the coordination mechanism is applied with  as explained in the previous section. Indeed, the solution stays close to  at all times. 

	Practical networks do not exactly follow an hexagonal model due to non-uniformity of traffic and scarcity of sites with good propagation characteristics. A popular model to take into account this irregularity is to assume that \acp{BS} locations follow a Poisson point process on the plane. Based on measurements from operational networks, results in \cite{AndrewsBaccelliGeometry} suggest that from the point of view of coverage, the Poisson model is pessimistic while the hexagonal model is optimist and reality lies somewhere in-between. We show that, like in the hexagonal case, instability occurs with a non-negligible probability. Hence instability is not an artifact of the hexagonal model. 
	
	We use the following procedure. For each snapshot we generate \ac{BS} locations according to a Poisson process on a square area of  , and we find a point  at which all \acp{BS} have the same coverage probability. To be consistent with the hexagonal model, we choose the neighbors of a \ac{BS} to be the  closest \acp{BS}. We calculate the Jacobian matrix of  at  to assess its stability. We simulate  snapshots to estimate the probability of observing an unstable network. Figure~\ref{fig:poisson_network_proba} shows the probability of instability for different values of the number of \acp{BS} per unit of surface. There is a non-negligible probability of instability, and this probability rapidly goes to  when the network becomes denser. An intuitive explanation is that when the network gets denser, \ac{BS} become closer to each other, so that the coupling between the corresponding \ac{SON} mechanisms becomes stronger and causes instability. 
	\begin{figure}[htbp]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/hexa_network_powers.eps}
		\end{center}
		\caption{Hexagonal network, no coordination, transmitted powers as a function of time}
		\label{fig:hexa_network_powers}
	\end{figure}	
	\begin{figure}[htbp]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/hexa_network_coverage.eps}
		\end{center}
		\caption{Hexagonal network, no coordination, coverage probability as a function of time}
		\label{fig:hexa_network_coverage}
	\end{figure}
	\begin{figure}[htbp]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/hexa_network_powers_coord.eps}
		\end{center}
		\caption{Hexagonal network with coordination, transmitted powers as a function of time}
		\label{fig:hexa_network_powers_coord}
	\end{figure}
	\begin{figure}[htbp]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/hexa_network_coverage_coord.eps}
		\end{center}
		\caption{Hexagonal network with coordination, coverage probability as a function of time}
		\label{fig:hexa_network_coverage_coord}
	\end{figure}
	\begin{figure}[htbp]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/poisson_network_proba.eps}
		\end{center}
		\caption{Probability of observing an unstable network as a function of the density of BSs}
		\label{fig:poisson_network_proba}
	\end{figure}


\section{Conclusion}\label{sec:conclusion}
	In this paper we have studied the problem of coordinating multiple \ac{SON} entities operating in parallel.  Using tools from control theory and Lyapunov stability, we have proposed a coordination mechanism to stabilize the system. The mechanism can be implemented in a distributed fashion so it is scalable with respect to the number of \acp{SON}. We have shown that the mechanism remains valid in the presence of measurement noise, using stochastic approximation. Instability and coordination in the context of wireless networks have been illustrated with two examples. We have shown that even for two control loops, instability can occur, and the influence of network geometry has been investigated. An interesting continuation of this work would be to investigate coordination without linearity/linearization. Coordination for non-linear systems is more challenging because we cannot rely on local analysis and Lyapunov functions which are quadratic forms.
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main}
\appendices
\section{Asymptotic behavior of ODEs}\label{app:odes}
	Consider the \ac{ODE}:

	which we assume to have a unique solution for each initial condition defined on . We write  the value at  of the solution for initial condition . We denote by  the distance to set . We say that  is \emph{invarient} if  implies  , . We say that  is \emph{Lyapunov stable} if for all  there exists  such that  implies  , . A compact invariant set  is an \emph{attractor} if there is an open invariant set  such that  implies  .  is called the \emph{basin of attraction}.  A compact invariant set  is \emph{locally asymptotically stable} if it is both Lyapunov stable and an attractor. If its basin of attraction  is equal to the whole space then  is \emph{globally asymptotically stable}. Asymptotic stability is often characterized using \emph{Lyapunov functions}. A positive, differentiable function , is said to be a Lyapunov function if   is decreasing, and strictly decreasing whenever . Then the set of zeros of  is locally asymptotically stable. If we add the condition , then we have global asymptotic stability.

\section{Linear ODEs}\label{app:linear_ode}
	Consider the \ac{ODE}:

Its solution has the form:

	We denote by  positive negativity for symmetric matrices.  is asymptotically stable iif all the eigenvalues of  have a strictly negative real part. Alternatively, asymptotic stability applies iif there exists  such that . In this case,  is a Lyapunov function for the \ac{ODE}. The reader can refer to \cite{LMIControlBoyd} for the linear matrix inequality approach to stability.
\end{document}