\documentclass{article}

\PassOptionsToPackage{numbers}{natbib}






\usepackage[final]{neurips_2020}



\usepackage{caption}
\usepackage{subfigure}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{array}
\usepackage{enumitem} 


\usepackage{color}
\usepackage{algorithmicx,algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{threeparttable}

\newcommand{\diag}{\textbf{diag\,}}
\newcommand{\re}{\textbf{Re\,}}
\newcommand{\methodname}{MineRAL}
\newcommand{\fixme}[1]{\textcolor{red}{#1}}
\newcommand{\tfont}[1]{{\fontfamily{lmtt}\selectfont \textbf{#1}}}
\newcommand{\kgfont}[1]{\textit{#1}}
\newcommand{\wrong}[1]{\color{red}\kgfont{#1}}
\newcommand{\udfunderscore}{\hspace{-0.5mm}\_}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\title{Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion\\
\vspace{3mm}
Supplementary Material}




\author{Zhanqiu Zhang \qquad
  Jianyu Cai \qquad
  Jie Wang \thanks{Corresponding author.} \vspace{1mm}\\
University of Science and Technology of China\\
\texttt{\{zzq96,jycai\}@mail.ustc.edu.cn,jiewangx@ustc.edu.cn}
}


\begin{document}

\maketitle

\section{Proof for Theorem 1}
\begin{theorem}\label{thm:main}
Suppose that  for , where  are real matrices and  is diagonal. Then, the following equation holds

The equation holds if and only if

and

for all ,
where , , and  are the -th columns of , , and , respectively.
\end{theorem}

\begin{proof}
We have that

The equality holds if and only if , i.e., .

For all CP decomposition ,  we can always let ,  and  such that

and meanwhile ensure that . Therefore, we know that

In the same manner, we know that

The equality holds if and only if .

Therefore, the conclusion holds if and only if 
and

.
\end{proof}

Therefore, for DURA, we know that 

which completes the proof.




\begin{table}[ht]
    \centering
    \caption{Comparison to Reg\_p1. ``R'': RESCAL. ``C'': ComplEx. }    \label{table:cmp_results}
        \vskip 0.1in
        \begin{tabular}{l  c c c  c c c }
            \toprule
              &\multicolumn{3}{c}{\textbf{WN18RR}}&  \multicolumn{3}{c}{\textbf{FB15k-237}} \\
             \cmidrule(lr){2-4}
             \cmidrule(lr){5-7}
             & MRR & H@1 & H@10 & MRR & H@1 & H@10 \\
            \midrule
            R-Reg\_p1   &.281 &.220 &.394 &.310 &.228 &.338\\
            C-Reg\_p1   &.409 &.393 &.439 &.316 &.229 &.487\\
            \midrule
            R-DURA    &\textbf{.498} &\textbf{.455} &.577 &.368 &\textbf{.276} &.550 \\
            C-DURA &.491 &.449 &.571 &\textbf{.371} &\textbf{.276} &\textbf{.560} \\
            \bottomrule
        \end{tabular}
\end{table}

\section{The optimal value of p}
In DB models, the commonly used  is either  or . When , DURA takes the form as the one in Equation (8) in the main text. If , we cannot expand the squared score function of the associated DB models as in Equation (4).
Thus, the induced regularizer takes the form of . The above regularizer with  (Reg\_p1) does not gives an upper bound on the tensor nuclear-2 norm as in Theorem 1. Table \ref{table:cmp_results} shows that, DURA significantly outperforms Reg\_p1 on WN18RR and FB15k-237. Therefore, we choose .

\section{Computational Complexity}
Suppose that  is the number of triplets known to be true in the knowledge graph,  is the embedding dimension of entities. Then, for CP and ComplEx, the complexity of DURA is ; for RESCAL, the complexity of DURA is . That is to say, the computational complexity of weighted DURA is the same as the weighted squared Frobenius norm regularizer. 

\section{More Details About Experiments}
In this section, we introduce the training protocol and the evaluation protocol.
\begin{table}[t]
    \centering
    \caption{Hyperparameters found by grid search.  is the embedding size,  is the batch size,  is the regularization coefficients, and  and  are weights for different parts of the regularizer.}
    \label{table:hp}
    \vskip 0.1in
    \setlength{\tabcolsep}{1.25mm}{
    \begin{tabular}{l  c c c c c  c c c c c  c c c c c }
    \toprule
        &\multicolumn{5}{c}{\textbf{WN18RR}}&  \multicolumn{5}{c}{\textbf{FB15k-237}} & \multicolumn{5}{c}{\textbf{YAGO3-10}}\\
         \cmidrule(lr){2-6}
         \cmidrule(lr){7-11}
         \cmidrule(lr){12-16}
         &  &  &  &  & &  &  &  &  & &  &  &  &  & \\
        \midrule
        CP & 2000 & 100 & 1e-1 & 0.5 & 1.5 & 2000 & 100 & 5e-2 & 0.5 & 1.5 & 1000 & 1000 & 5e-3 & 0.5 & 1.5\\
ComplEx & 2000 & 100 & 1e-1 & 0.5  &1.5 & 2000 & 100 & 5e-2 & 0.5 & 1.5 & 1000 & 1000 & 5e-2 & 0.5 & 1.5\\
        RESCAL & 512 & 1024 &1e-1 & 1.0 & 1.0 & 512 & 512 &1e-1 & 2.0 & 1.5 & 512 & 1024 &5e-2 & 1.0 & 1.0\\
        \bottomrule
    \end{tabular}
    }
   
\end{table}
\subsection{Training Protocol} 
We adopt the cross entropy loss function for all considered models as suggested in \citet{old_dog}. We adopt the ``reciprocal'' setting that creates a new triplet  for each triplet   \cite{n3,simple}. We use Adagrad \citep{adagrad} as the optimizer, and use grid search to find the best hyperparameters based on the performance on the validation datasets. Specifically, we search learning rates in , regularization coefficients in . On WN18RR and FB15k-237, we search batch sizes in  and embedding sizes in . On YAGO3-10, we search batch sizes in  and embedding sizes in . 
We search both  and  in .


We implement DURA in PyTorch and run on all experiments with a single NVIDIA GeForce RTX 2080Ti graphics card.

As we regard the link prediction as a multi-class classification problem and adopt the cross entropy loss, we can assign different weights for different classes (i.e., tail entities) based on their frequency of occurrence in the training dataset. Specifically, suppose that the loss of a given query  is , where  is the true tail entity, then the weighted loss is 

where 

 is a fixed number,  denotes the frequency of occurrence in the training set of the entity . For all models on WN18RR and RESCAL on YAGO3-10, we choose  and for all the other cases we choose .

We choose a learning rate of  after grid search. Table \ref{table:hp} shows the other best hyperparameters for DURA found by grid search. Please refer to the Experiments part in the main text for the search range of the hyperparameters.

\subsection{Evaluation Protocol} 
Following \citet{transe}, we use entity ranking as the evaluation task. For each triplet  in the test dataset, the model is asked to answer  and . To do this, we fill the positions of missing entities with candidate entities to create a set of candidate triplets, and then rank the triplets in descending order by their scores. Following the ``Filtered'' setting in \citet{transe}, we then filter out all existing triplets known to be true at ranking. We choose Mean Reciprocal Rank (MRR) and Hits at N (H@N) as the evaluation metrics. Higher MRR or H@N indicates better performance. Detailed definitions are as follows.
\begin{itemize}[itemindent=-1em]
    \item The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:
    
    \item The Hits@N is the ratio of ranks that no more than :
    
    where  if  or otherwise .
\end{itemize}

\subsection{The queries in T-SNE visualization}
In Table \ref{table:query_name}, we list the ten queries used in the T-SNE visualization (Section 5.4 in the main text). Note that a query is represented as , where  denotes the head entity and  denotes the relation.
\begin{table}[h]
    \centering
    \caption{The queries in T-SNE visualizations.}
    \label{table:query_name}
    \vskip 0.1in
    \resizebox{1 \columnwidth}!{
    \begin{tabular}{c l}
    \toprule
        \textbf{Index} & \textbf{Query} \\
        \midrule
        1 & (political drama, \text{/media\_common/netflix\_genre/titles}, ?)\\
        2 & (Academy Award for Best Original Song, \text{/award/award\_category/winners./award/award\_honor/ceremony},?)\\
        3 & (Germany, \text{/location/location/contains},?) \\
        4 & (doctoral degree ,  \text{/education/educational\_degree/people\_with\_this\_degree./education/education/major\_field\_of\_study},?)\\
        5 & (broccoli, \text{/food/food/nutrients./food/nutrition\_fact/nutrient},?)\\
        6 & (shooting sport, \text{/olympics/olympic\_sport/athletes./olympics/olympic\_athlete\_affiliation/country},?) \\
        7 & (synthpop, \text{/music/genre/artists}, ?)\\
        8 & (Italian American, \text{/people/ethnicity/people},?)\\
        9 & (organ, \text{/music/performance\_role/track\_performances./music/track\_contribution/role}, ?)\\
        10 & (funk, \text{/music/genre/artists}, ?)\\
        \bottomrule
    \end{tabular}
    }
\end{table}


\bibliography{neurips_2020}
\bibliographystyle{neurips_2020}
\end{document}
