\documentclass{article}

\PassOptionsToPackage{numbers}{natbib}






\usepackage[final]{neurips_2020}



\usepackage{caption}
\usepackage{subfigure}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{array}
\usepackage{enumitem} 


\usepackage{color}
\usepackage{algorithmicx,algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{threeparttable}

\newcommand{\diag}{\textbf{diag\,}}
\newcommand{\re}{\textbf{Re\,}}
\newcommand{\methodname}{MineRAL}
\newcommand{\fixme}[1]{\textcolor{red}{#1}}
\newcommand{\tfont}[1]{{\fontfamily{lmtt}\selectfont \textbf{#1}}}
\newcommand{\kgfont}[1]{\textit{#1}}
\newcommand{\wrong}[1]{\color{red}\kgfont{#1}}
\newcommand{\udfunderscore}{\hspace{-0.5mm}\_}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\title{Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion\\
\vspace{3mm}
Supplementary Material}




\author{Zhanqiu Zhang \qquad
  Jianyu Cai \qquad
  Jie Wang \thanks{Corresponding author.} \vspace{1mm}\\
University of Science and Technology of China\\
\texttt{\{zzq96,jycai\}@mail.ustc.edu.cn,jiewangx@ustc.edu.cn}
}


\begin{document}

\maketitle

\section{Proof for Theorem 1}
\begin{theorem}\label{thm:main}
Suppose that $\hat{\mathcal{X}}_j=\textbf{H}\textbf{R}_j\textbf{T}^\top$ for $j=1,2,\dots,|\mathcal{R}|$, where $\textbf{H},\textbf{T},\textbf{R}_j$ are real matrices and $\textbf{R}_j$ is diagonal. Then, the following equation holds
\begin{align*}
    \min_{\substack{\hat{\mathcal{X}}_j=\textbf{H} \textbf{R}_j\textbf{T}^\top\\  }}&\frac{1}{\sqrt{|\mathcal{R}}|}\sum_{j=1}^{|\mathcal{R}|}(\|\textbf{H}\textbf{R}_j\|_F^2+\|\textbf{T}\|_F^2+\|\textbf{T}\textbf{R}_j^\top\|_F^2+\|\textbf{H}\|_F^2)=\|\hat{\mathcal{X}}\|_*.
\end{align*}
The equation holds if and only if
$
  \|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{t}_{:d}\|_2
$
and
$
  \|\textbf{t}_{:d}\|_2\|\textbf{r}_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{h}_{:d}\|_2,
$
for all $\,d\in\{1,2,\ldots, D\}$,
where $\textbf{h}_{:d}$, $\textbf{r}_{:d}$, and $\textbf{t}_{:d}$ are the $d$-th columns of $\textbf{H}$, $\widetilde{\textbf{R}}$, and $\textbf{T}$, respectively.
\end{theorem}

\begin{proof}
We have that
\begin{align*}
    &\sum_{j=1}^{|\mathcal{R}|}\left(\|\textbf{H}\textbf{R}_j\|_F^2+\|\textbf{T}\|_F^2\right)\\
    =&\sum_{j=1}^{|\mathcal{R}|}\left(\sum_{i=1}^I\|\textbf{h}_{i}\circ \textbf{r}_{j}\|_F^2+\sum_{d=1}^D\|\textbf{t}_{:d}\|_F^2\right)\\
    =&\sum_{j=1}^{|\mathcal{R}|}\left(\sum_{d=1}^D\|\textbf{t}_{:d}\|_F^2+\sum_{i=1}^I\sum_{d=1}^D\textbf{h}_{id}^2\textbf{r}_{jd}^2\right)\\
=&\sum_{j=1}^{|\mathcal{R}|}\sum_{d=1}^D\|\textbf{t}_{:d}\|_2^2+\sum_{d=1}^D\|\textbf{h}_{:d}\|_2^2\|\textbf{r}_{:d}\|_2^2\\
    =&\sum_{d=1}^D(\|\textbf{h}_{:d}\|_2^2\|\textbf{r}_{:d}\|_2^2+|\mathcal{R}|\|\textbf{t}_{:d}\|_2^2)\\
    \ge& \sum_{d=1}^D2\sqrt{|\mathcal{R}|}\|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2\|\textbf{t}_{:d}\|_2\\
    =&2\sqrt{|\mathcal{R}|}\sum_{d=1}^D\|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2\|\textbf{t}_{:d}\|_2.
\end{align*}
The equality holds if and only if $\|\textbf{h}_{:d}\|_2^2\|\textbf{r}_{:d}\|_2^2=|\mathcal{R}|\|\textbf{t}_{:d}\|_2^2$, i.e., $\|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{t}_{:d}\|_2$.

For all CP decomposition $\hat{\mathcal{X}}=\sum_{d=1}^D \textbf{h}_{:d} \otimes \textbf{r}_{:d} \otimes \textbf{t}_{:d}$,  we can always let $\textbf{h}'_{:d}=\textbf{h}_{:d}$, $\textbf{r}'_{:d}=\sqrt{\frac{\|\textbf{t}_d\|_2\sqrt{|\mathcal{R}|}}{\|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2}}\textbf{r}_{:d}$ and $\textbf{t}'_{:d}= \sqrt{\frac{\|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2}{\|\textbf{t}_{:d}\|_2\sqrt{|\mathcal{R}|}}}\textbf{t}_{:d}$ such that
$$\|\textbf{h}'_{:d}\|_2\|\textbf{r}'_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{t}'_{:d}\|_2,$$
and meanwhile ensure that $\hat{\mathcal{X}}=\sum_{d=1}^D \textbf{h}'_{:d} \otimes \textbf{r}'_{:d} \otimes \textbf{t}'_{:d}$. Therefore, we know that
\begin{align*}
    \frac{1}{\sqrt{|\mathcal{R}|}}\sum_{j=1}^{|\mathcal{R}|}\|\hat{\mathcal{X}}_j\|_*&=\frac{1}{2\sqrt{|\mathcal{R}|}}\sum_{j=1}^{|\mathcal{R}|}\min_{\hat{\mathcal{X}}_j=\textbf{H}\textbf{R}_j\textbf{T}^\top}(\|\textbf{H}\textbf{R}_j\|_F^2+\|\textbf{T}\|_F^2)\\
    &\le\frac{1}{2\sqrt{|\mathcal{R}|}}\min_{\hat{\mathcal{X}}_j=\textbf{H}\textbf{R}_j\textbf{T}^\top}\sum_{j=1}^{|\mathcal{R}|}(\|\textbf{H}\textbf{R}_j\|_F^2+\|\textbf{T}\|_F^2)\\
    &= \min_{\hat{\mathcal{X}}=\sum_{d=1}^D \textbf{h}_{:d}\otimes \textbf{r}_{:d}\otimes \textbf{t}_{:d}}\sum_{d=1}^D\|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2\|\textbf{t}_{:d}\|_2\\
    &=\|\hat{\mathcal{X}}\|_*.
\end{align*}
In the same manner, we know that
\begin{align*}
    \frac{1}{2\sqrt{|\mathcal{R}|}}\min_{\hat{\mathcal{X}}_j=\textbf{H}\textbf{R}_j\textbf{T}^\top}\sum_{j=1}^{|\mathcal{R}|}(\|\textbf{T}\textbf{R}_j^\top\|_F^2+\|\textbf{H}\|_F^2)=\|\hat{\mathcal{X}}\|_*.
\end{align*}
The equality holds if and only if $ \|\textbf{t}_{:d}\|_2\|\textbf{r}_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{h}_{:d}\|_2$.

Therefore, the conclusion holds if and only if $
  \|\textbf{h}_{:d}\|_2\|\textbf{r}_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{t}_{:d}\|_2
$
and
$
  \|\textbf{t}_{:d}\|_2\|\textbf{r}_{:d}\|_2=\sqrt{|\mathcal{R}|}\|\textbf{h}_{:d}\|_2,
$
$\,\forall\,d\in\{1,2,\ldots, D\}$.
\end{proof}

Therefore, for DURA, we know that 
\begin{align*}
     \min_{\substack{\hat{\mathcal{X}}_j=\textbf{H} \textbf{R}_j\textbf{T}^\top\\  }}\frac{1}{\sqrt{|\mathcal{R}|}}g(\hat{\mathcal{X}})=\|\hat{\mathcal{X}}\|_*,
\end{align*}
which completes the proof.




\begin{table}[ht]
    \centering
    \caption{Comparison to Reg\_p1. ``R'': RESCAL. ``C'': ComplEx. }    \label{table:cmp_results}
        \vskip 0.1in
        \begin{tabular}{l  c c c  c c c }
            \toprule
              &\multicolumn{3}{c}{\textbf{WN18RR}}&  \multicolumn{3}{c}{\textbf{FB15k-237}} \\
             \cmidrule(lr){2-4}
             \cmidrule(lr){5-7}
             & MRR & H@1 & H@10 & MRR & H@1 & H@10 \\
            \midrule
            R-Reg\_p1   &.281 &.220 &.394 &.310 &.228 &.338\\
            C-Reg\_p1   &.409 &.393 &.439 &.316 &.229 &.487\\
            \midrule
            R-DURA    &\textbf{.498} &\textbf{.455} &.577 &.368 &\textbf{.276} &.550 \\
            C-DURA &.491 &.449 &.571 &\textbf{.371} &\textbf{.276} &\textbf{.560} \\
            \bottomrule
        \end{tabular}
\end{table}

\section{The optimal value of p}
In DB models, the commonly used $p$ is either $1$ or $2$. When $p=2$, DURA takes the form as the one in Equation (8) in the main text. If $p=1$, we cannot expand the squared score function of the associated DB models as in Equation (4).
Thus, the induced regularizer takes the form of $\sum_{(h_i,r_j,t_k)\in\mathcal{S}}\|\textbf{h}_i\bar{\textbf{R}}_j-\textbf{t}_k\|_1+\|\textbf{t}_k\textbf{R}_j^\top-\textbf{h}_i\|_1$. The above regularizer with $p=1$ (Reg\_p1) does not gives an upper bound on the tensor nuclear-2 norm as in Theorem 1. Table \ref{table:cmp_results} shows that, DURA significantly outperforms Reg\_p1 on WN18RR and FB15k-237. Therefore, we choose $p=2$.

\section{Computational Complexity}
Suppose that $k$ is the number of triplets known to be true in the knowledge graph, $n$ is the embedding dimension of entities. Then, for CP and ComplEx, the complexity of DURA is $O(kn)$; for RESCAL, the complexity of DURA is $O(kn^2)$. That is to say, the computational complexity of weighted DURA is the same as the weighted squared Frobenius norm regularizer. 

\section{More Details About Experiments}
In this section, we introduce the training protocol and the evaluation protocol.
\begin{table}[t]
    \centering
    \caption{Hyperparameters found by grid search. $k$ is the embedding size, $b$ is the batch size, $\lambda$ is the regularization coefficients, and $\lambda_1$ and $\lambda_2$ are weights for different parts of the regularizer.}
    \label{table:hp}
    \vskip 0.1in
    \setlength{\tabcolsep}{1.25mm}{
    \begin{tabular}{l  c c c c c  c c c c c  c c c c c }
    \toprule
        &\multicolumn{5}{c}{\textbf{WN18RR}}&  \multicolumn{5}{c}{\textbf{FB15k-237}} & \multicolumn{5}{c}{\textbf{YAGO3-10}}\\
         \cmidrule(lr){2-6}
         \cmidrule(lr){7-11}
         \cmidrule(lr){12-16}
         & $k$ & $b$ & $\lambda$ & $\lambda_1$ &$\lambda_2$ & $k$ & $b$ & $\lambda$ & $\lambda_1$ &$\lambda_2$ & $k$ & $b$ & $\lambda$ & $\lambda_1$ &$\lambda_2$ \\
        \midrule
        CP & 2000 & 100 & 1e-1 & 0.5 & 1.5 & 2000 & 100 & 5e-2 & 0.5 & 1.5 & 1000 & 1000 & 5e-3 & 0.5 & 1.5\\
ComplEx & 2000 & 100 & 1e-1 & 0.5  &1.5 & 2000 & 100 & 5e-2 & 0.5 & 1.5 & 1000 & 1000 & 5e-2 & 0.5 & 1.5\\
        RESCAL & 512 & 1024 &1e-1 & 1.0 & 1.0 & 512 & 512 &1e-1 & 2.0 & 1.5 & 512 & 1024 &5e-2 & 1.0 & 1.0\\
        \bottomrule
    \end{tabular}
    }
   
\end{table}
\subsection{Training Protocol} 
We adopt the cross entropy loss function for all considered models as suggested in \citet{old_dog}. We adopt the ``reciprocal'' setting that creates a new triplet $(e_k,r_j^{-1},e_i)$ for each triplet  $(e_i,r_j,e_k)$ \cite{n3,simple}. We use Adagrad \citep{adagrad} as the optimizer, and use grid search to find the best hyperparameters based on the performance on the validation datasets. Specifically, we search learning rates in $\{0.1, 0.01\}$, regularization coefficients in $\{0, 1\times 10^{-3}, 5\times 10^{-3}, 1\times 10^{-2}, 5\times 10^{-2}, 1\times 10^{-1}, 5\times 10^{-1}\}$. On WN18RR and FB15k-237, we search batch sizes in $\{100,500,1000\}$ and embedding sizes in $\{500,1000,2000\}$. On YAGO3-10, we search batch sizes in $\{256,512,1024\}$ and embedding sizes in $\{500,1000\}$. 
We search both $\lambda_1$ and $\lambda_2$ in $\{0.5, 1.0, 1.5, 2.0\}$.


We implement DURA in PyTorch and run on all experiments with a single NVIDIA GeForce RTX 2080Ti graphics card.

As we regard the link prediction as a multi-class classification problem and adopt the cross entropy loss, we can assign different weights for different classes (i.e., tail entities) based on their frequency of occurrence in the training dataset. Specifically, suppose that the loss of a given query $(h,r,?)$ is $\ell((h,r,?),t)$, where $t$ is the true tail entity, then the weighted loss is 
\begin{align*}
    w(t)\ell((h,r,?),t),
\end{align*}
where 
\begin{align*}
    w(t)=w_0\frac{\#t}{\max\{\#t_i:t_i \in \text{training set}\}} + (1-w_0),
\end{align*}
$w_0$ is a fixed number, $\#t$ denotes the frequency of occurrence in the training set of the entity $t$. For all models on WN18RR and RESCAL on YAGO3-10, we choose $w_0=0.1$ and for all the other cases we choose $w_0=0$.

We choose a learning rate of $0.1$ after grid search. Table \ref{table:hp} shows the other best hyperparameters for DURA found by grid search. Please refer to the Experiments part in the main text for the search range of the hyperparameters.

\subsection{Evaluation Protocol} 
Following \citet{transe}, we use entity ranking as the evaluation task. For each triplet $(h_i,r_j,t_k)$ in the test dataset, the model is asked to answer $(h_i, r_j, ?)$ and $(t_k, r_j^{-1}, ?)$. To do this, we fill the positions of missing entities with candidate entities to create a set of candidate triplets, and then rank the triplets in descending order by their scores. Following the ``Filtered'' setting in \citet{transe}, we then filter out all existing triplets known to be true at ranking. We choose Mean Reciprocal Rank (MRR) and Hits at N (H@N) as the evaluation metrics. Higher MRR or H@N indicates better performance. Detailed definitions are as follows.
\begin{itemize}[itemindent=-1em]
    \item The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:
    \begin{align*}
        \text{MRR}=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{\text{rank}_i}.
    \end{align*}
    \item The Hits@N is the ratio of ranks that no more than $N$:
    \begin{align*}
        \text{Hits@N}=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\mathbbm{1}_{x\le N}(\text{rank}_i),
    \end{align*}
    where $\mathbbm{1}_{x\le N}(\text{rank}_i)=1$ if $\text{rank}_i\le N$ or otherwise $\mathbbm{1}_{x\le N}(\text{rank}_i)=0$.
\end{itemize}

\subsection{The queries in T-SNE visualization}
In Table \ref{table:query_name}, we list the ten queries used in the T-SNE visualization (Section 5.4 in the main text). Note that a query is represented as $(h, r, ?)$, where $h$ denotes the head entity and $r$ denotes the relation.
\begin{table}[h]
    \centering
    \caption{The queries in T-SNE visualizations.}
    \label{table:query_name}
    \vskip 0.1in
    \resizebox{1 \columnwidth}!{
    \begin{tabular}{c l}
    \toprule
        \textbf{Index} & \textbf{Query} \\
        \midrule
        1 & (political drama, \text{/media\_common/netflix\_genre/titles}, ?)\\
        2 & (Academy Award for Best Original Song, \text{/award/award\_category/winners./award/award\_honor/ceremony},?)\\
        3 & (Germany, \text{/location/location/contains},?) \\
        4 & (doctoral degree ,  \text{/education/educational\_degree/people\_with\_this\_degree./education/education/major\_field\_of\_study},?)\\
        5 & (broccoli, \text{/food/food/nutrients./food/nutrition\_fact/nutrient},?)\\
        6 & (shooting sport, \text{/olympics/olympic\_sport/athletes./olympics/olympic\_athlete\_affiliation/country},?) \\
        7 & (synthpop, \text{/music/genre/artists}, ?)\\
        8 & (Italian American, \text{/people/ethnicity/people},?)\\
        9 & (organ, \text{/music/performance\_role/track\_performances./music/track\_contribution/role}, ?)\\
        10 & (funk, \text{/music/genre/artists}, ?)\\
        \bottomrule
    \end{tabular}
    }
\end{table}


\bibliography{neurips_2020}
\bibliographystyle{neurips_2020}
\end{document}
