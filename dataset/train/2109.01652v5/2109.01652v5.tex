    
\documentclass{article} \usepackage{iclr2022_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{soul}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\definecolor{battleshipgrey}{rgb}{0.3, 0.3, 0.3}
\definecolor{brilliantrose}{rgb}{1.0, 0.33, 0.64}
\usepackage{cleveref}
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefname{table}{Table}{Tables}
\crefname{figure}{Figure}{Figures}
\crefname{algorithm}{Algorithm}{}
\crefname{equation}{eq.}{}
\crefname{appendix}{Appendix}{}
\crefformat{section}{\S#2#1#3}

\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage[export]{adjustbox}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amssymb}
\usepackage{calc}
\usepackage{wrapfig}


\definecolor{americanrose}{rgb}{1.0, 0.01, 0.24}
\definecolor{jweigreen}{rgb}{0,0.45,0.24}
\definecolor{bluegray}{rgb}{0.1, 0.1, 0.4}
\newcommand{\battleshipgrey}[1]{{\color{battleshipgrey}{#1}}}
\newcommand{\todo}{{\color{brilliantrose}{\textbf{To-do}}}}
\newcommand{\jason}[1]{{\color{purple}{\textbf{\{Jason:} #1\textbf{\}}}}}
\newcommand{\vzhao}[1]{{\color{blue}{\textbf{\{vzhao:} #1\textbf{\}}}}}
\newcommand{\bosma}[1]{{\color{blue}{\textbf{\{bosma:} #1\textbf{\}}}}}
\newcommand{\prelim}[1]{{\color{brilliantrose}{\textbf{#1}}}}
\newcommand{\highlight}[1]{{\color{violet}{\textbf{#1}}}}
\newcommand{\textttsmall}[1]{\texttt{{\small#1}}}
\newcommand{\flan}{FLAN}
\newcommand{\samplingexplanation}{Multiple \flan{} outputs are generated via random sampling with a temperature of 0.9 and top  of 40.}
\newcommand{\baselm}{LaMDA-PT}
\newcommand{\checkme}[1]{{\color{blue}{#1\textbf}}}

\title{Finetuned Language Models Are Zero-Shot Learners \raggedright}



\author{\hspace{-1.7mm}
{ 
Jason Wei\thanks{Lead contributors. Author contributions \hyperref[sec:contributions]{\color{bluegray}{listed at end of paper}}.
} \hspace{0.5mm},
Maarten Bosma,
Vincent Y.~Zhao,
Kelvin Guu,
Adams Wei Yu,} \\
\textbf{Brian Lester, Nan Du, Andrew M.~Dai, and Quoc V.~Le} \vspace{1.3mm} \\
Google Research \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle
\vspace{-2mm}
\begin{abstract}

This paper explores a simple method for improving the zero-shot learning abilities of language models. 
We show that \textit{instruction tuning}---finetuning language models on a collection of datasets described via instructions---substantially improves zero-shot performance on unseen tasks.\vspace{1mm}

We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates.
We evaluate this instruction-tuned model, which we call \flan{}, on unseen task types.
\flan{} substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.
\flan{} even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.
Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.

\end{abstract}

\vspace{-3mm}
\begin{figure}[h]
    \centering
    \includegraphics[height=82mm]{figs/figure-pull-example.pdf}
    \vspace{-2mm}
    \caption{
    Top: overview of instruction tuning and \flan. Instruction tuning finetunes a pretrained language model on a mixture of tasks phrased as instructions.
    At inference time, we evaluate on an unseen task type; for instance, we could evaluate the model on natural language inference (NLI) when no NLI tasks were seen during instruction tuning.
    Bottom: performance of zero-shot FLAN, compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning improved performance substantially out of ten we evaluate.
    NLI datasets: ANLI R1--R3, CB, RTE. Reading comprehension datasets: BoolQ, MultiRC, OBQA. Closed-book QA datasets: ARC-easy, ARC-challenge, NQ, TriviaQA.
    }
    \label{fig:flan-pull}
\end{figure}

\clearpage

\section{Introduction}

Language models (LMs) at scale, such as GPT-3~\citep{brown2020language}, have been shown to perform few-shot learning remarkably well. 
They are less successful at zero-shot learning, however.
For example, GPT-3's zero-shot performance is much worse than few-shot performance on tasks such as reading comprehension, question answering, and natural language inference. 
One potential reason is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not similar to the format of the pretraining data.

In this paper, we explore a simple method to improve the zero-shot performance of large language models, which would expand their reach to a broader audience.
We leverage the intuition that NLP tasks can be described via natural language instructions, such as ``\textit{Is the sentiment of this movie review positive or negative?}'' or ``\textit{Translate `how are you' into Chinese.}''
We take a pretrained language model of 137B parameters and perform \textit{instruction tuning}---finetuning the model on a mixture of more than 60 NLP datasets expressed via natural language instructions.
We refer to this resulting model as \flan, for \underline{F}inetuned \underline{La}nguage \underline{N}et.

To evaluate the zero-shot performance of \flan\ on unseen tasks, we group NLP datasets into clusters based on their task types and hold out each cluster for evaluation while instruction tuning \flan{} on all other clusters.
For example, as shown in \cref{fig:flan-pull}, to evaluate \flan's ability to perform natural language inference, we instruction tune the model on a range of other NLP tasks such as commonsense reasoning, translation, and sentiment analysis.
As this setup ensures that \flan{} has not seen any natural language inference tasks in instruction tuning, we then evaluate its ability to perform zero-shot natural language inference.

Our evaluations show that \flan{} substantially improves the zero-shot performance of the base 137B-parameter model.
\flan's zero-shot also outperforms 175B-parameter GPT-3's zero-shot on 20 of 25 datasets that we evaluate, and even outperforms GPT-3's few-shot by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. 
In ablation studies, we find that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the benefits of instruction tuning emerge only with sufficient model scale. 

Instruction tuning is a simple method that, as depicted in \cref{fig:flan-paradigm}, combines appealing aspects of both the pretrain--finetune and prompting paradigms by using supervision via finetuning to improve language model's responses to inference-time text interactions.
Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions.
Source code for loading the instruction tuning dataset used for \flan{} is publicly available at \url{https://github.com/google-research/flan}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.93\linewidth]{figs/figure-general.pdf}
    \vspace{-1mm}
    \caption{Comparing instruction tuning with pretrain--finetune and prompting.}
    \label{fig:flan-paradigm}
\end{figure}



\section{\flan: Instruction Tuning Improves Zero-Shot Learning}
The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions.
The idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks. 
To evaluate performance on unseen tasks, we group datasets into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters.

\subsection{Tasks \& Templates}\label{subsec:tasks_and_templates}
As creating an instruction tuning dataset with many tasks from scratch would be resource-intensive, we transform existing datasets from the research community into an instructional format. 
We aggregate 62 text datasets that are publicly available on Tensorflow Datasets, including both language understanding and language generation tasks, into a single mixture.
\cref{fig:flan-clusters} shows these datasets---each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are of the same task type. 
Descriptions, sizes, and examples of each dataset are shown in \cref{task_details}.
\vspace{-1mm}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/all-tasks.pdf}
    \vspace{-7mm}
    \caption{Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).}
    \vspace{-2mm}
    \label{fig:flan-clusters}
\end{figure}

For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset.
While most of the ten templates describe the original task, to increase diversity, for each dataset we also include up to three templates that ``turned the task around,'' (e.g., for sentiment classification we include templates asking to generate a movie review).
We then instruction tune a pretrained language model on the mixture of all datasets, with examples in each dataset formatted via a randomly selected instruction template for that dataset.
\cref{fig:flan-template-example} shows multiple instruction templates for a natural language inference dataset.
\vspace{-1mm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.93\linewidth]{figs/template-example.pdf}
    \vspace{-3mm}
    \caption{Multiple instruction templates describing a natural language inference task.}
    \vspace{-1mm}
    \label{fig:flan-template-example}
\end{figure}

\subsection{Evaluation Splits}\label{subsec:eval_splits}
We are interested in how \flan{} performs on tasks not seen in instruction tuning, and so it is crucial to define what counts as an unseen task.
Whereas some prior work defines unseen tasks by disallowing the same dataset to appear in training, we use a more conservative definition that leverages the task clusters from \cref{fig:flan-clusters}.
In this work, we only consider dataset  unseen at evaluation time if no datasets from any task clusters that  belongs to were seen during instruction tuning.
For instance, if  is an entailment task, then no entailment datasets appeared in instruction tuning, and we instruction-tuned on all other clusters.\footnote{When evaluating on the read.\ comp.\ with commonsense cluster, both read.\ comp.\ and commonsense reasoning were dropped from instruction tuning.
Conversely, the read.\ comp.\ with commonsense cluster was not used for instruction tuning when evaluating on read.\ comp.\ or commonsense reasoning.
We also drop the paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa.}
Hence, to evaluate zero-shot \flan{} on  task clusters, we instruction tune  models, where each model holds out a different task cluster for evaluation.

\subsection{Classification with Options}\label{subsec:options}
\vspace{-1mm}
The output space for a given task is either one of several classes (classification) or free text (generation). 
As \flan{} is an instruction-tuned version of a decoder-only language model, it naturally responds in free text, and so no further modifications are needed for generation tasks.

For classification tasks, prior work \citep{brown2020language} used a \textit{rank classification} approach where, for example, only two outputs (``\textit{yes}'' and ``\textit{no}'') are considered and the higher probability one is taken as the model's prediction.
Though this procedure is logically sound, it is imperfect in that the probability mass for answers may have an undesired distribution among ways of saying each answer (e.g., a large number of alternative ways of saying ``\textit{yes}'' may lower the probability mass assigned to ``\textit{yes}'').
Therefore, we include an \textit{options} suffix, in which we append the token \textttsmall{OPTIONS} to the end of a classification task along with a list of the output classes for that task.
This makes the model aware of which choices are desired when responding to classification tasks.
Example use of options is shown in the NLI and commonsense examples in \cref{fig:flan-pull}.

\vspace{-1mm}
\subsection{Training Details}
\vspace{-1mm}

\textbf{Model architecture and pretraining.} 
In our experiments, we use \baselm{}, a dense left-to-right, decoder-only transformer language model of 137B parameters \citep{thoppilan2022lamda}.
This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece library \citep{sentencepiece}. 
Around 10\% of the pretraining data was non-English.
Note that \baselm{} only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).

\textbf{Instruction tuning procedure.}
\flan{} is the instruction-tuned version of \baselm. 
Our instruction tuning pipeline mixes all datasets and randomly samples from each dataset.
To balance the different sizes of datasets, we limit the number of training examples per dataset to 30k and follow the examples-proportional mixing scheme \citep{raffel2019exploring} with a mixing rate maximum of 3k.\footnote{In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional sampling weight for examples in excess of 3,000.}
We finetune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer \citep{shazeer2018adafactor} with a learning rate of 3e-5. 
The input and target sequence lengths used in finetuning are 1024 and 256, respectively. 
We use packing \citep{raffel2019exploring} to combine multiple training examples into a single sequence, separating inputs from targets using a special EOS token.
This instruction tuning takes around 60 hours on a TPUv3 with 128 cores.
For all evaluations, we report results on the final checkpoint trained for 30k steps.

\vspace{-1mm}
\section{Results}\label{sec:results}
\vspace{-2mm}
We evaluate \flan{} on natural language inference, reading comprehension, closed-book QA, translation, commonsense reasoning, coreference resolution, and struct-to-text. 
As described in \cref{subsec:eval_splits}, we evaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for evaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses a different checkpoint).
For each dataset, we evaluate the mean of performance on all templates, which proxies the expected performance given a typical natural language instruction.
As a dev set is sometimes available for manual prompt engineering \citep{brown2020language}, for each dataset we also obtain the test set performance using the template with the best dev set performance.

For comparison, we report zero and few-shot results for \baselm\ using the same prompts as GPT-3 (as \baselm\ is not suitable for natural instructions without instruction tuning).
This baseline provides the most direct ablation of how much instruction tuning helps.
Instruction tuning significantly improves \baselm\ on most datasets.

We also show the zero-shot performances of GPT-3 175B \citep{brown2020language} and GLaM 64B/64E \citep{du2021glam}, as reported in their respective papers. 
With the best dev template, zero-shot \flan{} outperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3's few-shot performance on 10 datasets. 
With the best dev-template, zero-shot \flan{} outperforms zero-shot GLaM on 13 of 19 available datasets and one-shot GLaM on 11 of 19 datasets.

Overall, we observe that instruction tuning is very effective on tasks naturally verbalized as instructions (e.g., NLI, QA, translation, struct-to-text) and is less effective on tasks directly formulated as language modeling, where instructions would be largely redundant (e.g., commonsense reasoning and coreference resolution tasks that are formatted as finishing an incomplete sentence or paragraph).
Results on natural language inference, reading comprehension, closed-book QA, and translation are summarized in \cref{fig:flan-results-summary} and described below.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/summary-results.pdf}
    \vspace{-6mm}
    \caption{Zero-shot performance of \flan{} compared to \baselm{} 137B, GPT-3 175B, and GLaM 64B/64E on natural language inference, reading comprehension, closed-book QA, and translation. 
    Performance of \flan{} is the mean of up to 10 instructional templates per task.
    Supervised models were either T5, BERT, or translation models (specified in \cref{tab:nlu_table} and \cref{tab:nlg_table} in the Appendix).
    }
    \vspace{-1mm}
    \label{fig:flan-results-summary}
\end{figure}


\textbf{Natural language inference (NLI).} 
On five NLI datasets, where a model must determine whether a hypothesis is true given some premise, FLAN outperforms all baselines by a large margin. As noted by \citet{brown2020language}, perhaps one reason why GPT-3 struggles with NLI is that NLI examples are unlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased as a continuation of a sentence. 
For \flan{}, we phrase NLI as the more natural question ``\texttt{\small Does <premise> mean that <hypothesis>?}'', achieving much higher performance.

\textbf{Reading comprehension.} 
On reading comprehension, where models are asked to answer a question about a provided passage, FLAN outperforms baselines for MultiRC \citep{khashabi-etal-2018-looking} and OBQA \citep{mihaylov-etal-2018-suit}.
On BoolQ \citep{clark-etal-2019-boolq}, \flan{} outperforms GPT-3 by a large margin, though \baselm{} already achieves high performance on BoolQ.

\textbf{Closed-book QA.}
For closed-book QA, which asks models to answer questions about the world without access to specific information containing the answer, \flan{} outperforms GPT-3 on all four datasets.
Compared to GLaM, \flan{} has better performance on ARC-e and ARC-c \citep{clark2018think}, and slightly lower performance on NQ \citep{orqa,kwiatkowski2019natural} and TQA \citep{JoshiTriviaQA2017}.

\textbf{Translation.}
Similar to GPT-3, the training data for \baselm\ is around 90\% English and includes some text in other languages that was not specifically used to train the model to perform machine translation. 
We also evaluate \flan's performance on machine translation for the three datasets evaluated in the GPT-3 paper: French--English from WMT'14 \citep{wmt14}, and German--English and Romanian--English from WMT'16 \citep{wmt16}. 
Compared with GPT-3, \flan{} outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in most cases.
Similar to GPT-3, \flan{} shows strong results for translating into English and compares favorably against supervised translation baselines.
Translating from English into other languages, however, was relatively weaker, as might be expected given that \flan{} uses an English sentencepiece tokenizer and that the majority of pretraining data is English.
 

\newcommand{\flanvalspaced}[4]{
\makecell[l]{\hspace{#4mm}#1\vspace{-1.5mm}\\
{\hspace{#4mm}\battleshipgrey{\tiny std=#2}}\vspace{0.3mm}\\
\hspace{#4mm}{#3}}}
\newcommand{\flanval}[3]{\flanvalspaced{#1}{#2}{#3}{0}}

\newcommand{\gptvalspaced}[4]{\makecell[l]{\hspace{#4mm}#1\vspace{-0.4mm}\\{\hspace{#4mm}\battleshipgrey{\scriptsize #3}}}}
\newcommand{\gptval}[3]{\gptvalspaced{#1}{#2}{#3}{0}}

\newcommand{\gptname}[0]{\makecell[l]{GPT-3 175B zero-shot\vspace{-0.6mm}\\{\scriptsize \hspace{3mm} few-shot}}}
\newcommand{\glmname}[0]{\makecell[l]{\baselm{} 137B zero-shot \vspace{-0.6mm}\\{\scriptsize\hspace{3mm} few-shot}}}
\newcommand{\gptvalna}[0]{\gptval{--}{--}{--}}
\newcommand{\tasktype}[1]{\multicolumn{6}{l}{\textsc{\underline{\textbf{#1}}}}}
\newcommand{\tfiveval}[1]{#1}
\newcommand{\bertlargeval}[1]{#1}
\newcommand{\flanname}[0]{\makecell[l]{- average template{\tiny \vspace{2mm}} \\ {- best dev template}}}
\newcommand{\datasetacc}[1]{\makecell{#1 \vspace{-0.7mm}\\{\footnotesize acc.}\vspace{-0.7mm}}}
\newcommand{\datasetem}[1]{\makecell{#1 \vspace{-0.7mm}\\{\scriptsize EM}\vspace{-0.7mm}}}
\newcommand{\datasetbleu}[1]{\makecell{#1 \vspace{-0.7mm}\\{\scriptsize BLEU}\vspace{-0.7mm}}}
\newcommand{\datasetfone}[1]{\makecell{#1 \vspace{-0.7mm}\\{\scriptsize F1}\vspace{-0.7mm}}}
\newcommand{\datasetcustom}[2]{\makecell{#1 \vspace{-0.7mm}\\{\scriptsize #2}\vspace{-0.7mm}}}
\newcommand{\wewin}[1]{\textcolor{jweigreen}{\hspace{0.3mm}{\scriptsize}{\scriptsize#1}}\hspace{1mm}}
\newcommand{\welose}[1]{\textcolor{americanrose}{\hspace{0.7mm}{\scriptsize -}{\scriptsize#1}}\hspace{1mm}}
\newcommand{\wekindawin}[1]{\textcolor{jweigreen}{\scriptsize{\hspace{0.6mm}\hspace{0.3mm}#1}\hspace{1mm}}}
\newcommand{\explainflan}[0]{For \flan, we report both the average of up to ten templates, as well as the best dev template.}
\newcommand{\explainwewin}[0]{The triangle \textcolor{jweigreen}{\scriptsize} indicates improvement over few-shot GPT-3.}
\newcommand{\explainwekindawin}[0]{The up-arrow \textcolor{jweigreen}{\scriptsize} indicates improvement only over zero-shot GPT-3.}
\newcommand{\gptvala}[3]{& #1 & #2 {\tiny #3}}
\newcommand{\baselmvala}[3]{& #1 & #2 {\tiny [#3]}}
\newcommand{\flanvala}[3]{& #1{\tiny #2} & #3}
\newcommand{\na}[0]{\makecell[c]{--}}
\newcommand{\explainkt}[0]{{\scriptsize } indicates the number of few-shot exemplars. {\scriptsize \#} indicates the number of templates that FLAN is evaluated on.}
\newcommand{\fewk}[1]{{\tiny [#1]}}
 
 








\textbf{Additional tasks.}
Although we see strong results for the above task clusters, one limitation with instruction tuning is that it does not improve performance for many language modeling tasks (e.g., commonsense reasoning or coreference resolution tasks formulated as sentence completions).
For seven commonsense reasoning and coreference resolution tasks (see \cref{tab:nlu_table} in the Appendix), \flan{} only outperforms \baselm{} on three of the seven tasks.
This negative result indicates that when the downstream task is the same as the original language modeling pre-training objective (i.e., in cases where instructions are largely redundant), instruction tuning is not useful.
Finally, we report results for sentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which GPT-3 results are not available, in \cref{tab:nlu_table} and \cref{tab:nlg_table} in the Appendix.
Generally, zero-shot \flan{} outperforms zero-shot \baselm{} and is comparable with or better than few-shot \baselm{}.

\section{Ablation Studies \& Further Analysis}

\subsection{Number of instruction tuning clusters}\label{subsec:finetuning_clusters}
As the core question of our paper asks how instruction tuning improves a model's zero-shot performance on unseen tasks, in this first ablation we examine how performance is affected by the number of clusters and tasks used in instruction tuning.
For this setup, we hold out NLI, closed-book QA, and commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction tuning.\footnote{We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning in this ablation because they are too similar to NLI and commmonsense reasoning, respectively.}
We show results for one to seven instruction tuning clusters, where clusters are added in decreasing order of number of tasks per cluster.

\cref{fig:flan-ablation-numtasks} shows these results.
As expected, we observe that average performance across the three held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the exception of the sentiment analysis cluster), confirming the benefits of our proposed instruction tuning approach on zero-shot performance on novel tasks.
It is further interesting to see that, for the seven clusters we test, the performance does not appear to saturate, implying that performance may further improve with even more clusters added to instruction tuning.
Of note, this ablation does not allow us to draw conclusions about which instruction tuning cluster contributes the most to each evaluation cluster, although we see minimal added value from the sentiment analysis cluster.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figs/ablation-1-figure.pdf}
    \vspace{-2mm}
    \caption{Adding additional task clusters to instruction tuning improves zero-shot performance on held-out task clusters. 
    The evaluation tasks are the following. 
    Commonsense: CoPA, HellaSwag, PiQA, and StoryCloze. NLI: ANLI R1--R3, QNLI, RTE, SNLI, and WNLI. Closed-book QA: ARC easy, ARC challenge, Natural Questions, and TriviaQA.}
    \label{fig:flan-ablation-numtasks}
\end{figure}
\vspace{-2mm}


\begin{wrapfigure}{r}{0.53\textwidth}
    \centering
    \vspace{-13mm}
    \includegraphics[width=\linewidth]{figs/scale-ablation-held-out-only.pdf}
    \vspace{-7mm}
    \caption{
    Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, potentially because all model capacity is used to learn the mixture of instruction tuning tasks. 
    }
    \vspace{-6mm}
    \label{fig:scale-ablation}
\end{wrapfigure}

\subsection{Scaling laws}\label{subsec:scaling_laws}

As \citet{brown2020language} shows that zero and few-shot capabilities of language models substantially improve for larger models, we next explore how the benefits of instruction tuning are affected by model scale. 
Using the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters. 

\cref{fig:scale-ablation} shows these results. 
We see that for the two models on the order of 100B parameters, instruction tuning substantially improves performance on held-out tasks, as is expected given the prior results in our paper.
The behavior on held-out tasks for the 8B and smaller models, however, is thought-provoking---instruction tuning actually hurts performance on held-out tasks. 
One potential explanation for this result could be that for small-scale models, learning the 40 tasks used during instruction tuning fills the entire model capacity, causing these models to perform worse on new tasks. 
Under this potential explanation, for the larger scale models, instruction tuning fills up some model capacity but also teaches these models how to follow instructions, allowing them to generalize to new tasks with the remaining capacity. 

\begin{wrapfigure}{r}{0.43\textwidth}
    \centering
    \vspace{-6mm}
    \includegraphics[width=\linewidth]{figs/instruction-ablation.pdf}
    \vspace{-7mm}
    \caption{
    Ablation study result using models with instructions removed from finetuning (FT). 
    }
    \vspace{-3mm}
    \label{fig:role-of-insructions}
\end{wrapfigure}

\subsection{Role of instructions}\label{subsec:role-of-instructions}
In a final ablation study, we explore the role of instructions during finetuning, as one possibility is that performance gains come entirely from multi-task finetuning and the model could perform just as well without instructions.
We hence consider two finetuning setups without instructions.
In a \textit{no template} setup, only inputs and outputs were given to the model (e.g., for translation the input would be ``\textit{The dog runs.}'' and the output would be ``\textit{Le chien court.}'').
In a \textit{dataset name} setup, each input is prepended with the name of the task and dataset (e.g., for translation to French, the input would be ``\textit{[Translation: WMT'14 to French] The dog runs.}'').

We compare these two ablations to \flan{}'s finetuning procedure, which used natural instructions (e.g., ``\textit{Please translate this sentence to French: `The dog runs.'}'').
We perform evaluations for four held-out clusters from \cref{fig:flan-results-summary}.
For the no template setup, we used the \flan{} instructions during zero-shot inference (because if we used no template, the model would not know what task to perform).
For models finetuned on dataset name only, we report zero-shot performance for \flan{} instructions as well as using the dataset name.
\cref{fig:role-of-insructions} shows the results---both ablation configurations performed substantially worse than \flan{}, indicating that training with instructions is crucial for zero-shot performance on unseen tasks.

\subsection{Instructions with Few-Shot Exemplars}\label{subsec:finetune}

\newcommand{\instruct}{\textrm{instruct}}
So far, we have focused on instruction tuning in the zero-shot setting. 
Here, we study how instruction tuning can be used when few-shot exemplars are available at inference time.
The format for the few-shot setting builds on the zero-shot format. 
For some input  and output , let  denote the zero-shot instructions. 
Then, given  few-shot exemplars  and a new input , the instruction format for the few-shot setting is ``'', where  denotes string concatenation with a delimiter token inserted in between.
At both training and inference time, exemplars are randomly drawn from the training set, and the number of exemplars is capped at 16 and such that the total sequence length is less than 960 tokens. 
Our experiment uses the same task splits and evaluation procedure as \cref{sec:results}, such that few-shot exemplars for an unseen task are only used at inference time.

As shown in \cref{fig:few-shot}, few-shot exemplars improve the performance on all task clusters, compared with zero-shot FLAN. 
Exemplars are especially effective for tasks with large/complex output spaces, such as struct to text, translation, and closed-book QA, potentially because exemplars help the model better understand the output format. 
In addition, for all task clusters, standard deviation among templates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/few-shot.pdf}
    \vspace{-4mm}
    \caption{
    Adding few-shot exemplars to FLAN is a complementary method for improving the performance of instruction-tuned models.
    The orange bars indicate standard deviation among templates, averaged at the dataset level for each task cluster.
    }
    \label{fig:few-shot}
\end{figure}


\subsection{Instruction Tuning Facilitates Prompt Tuning}\label{subsec:prompt_tuning}

\begin{wrapfigure}{r}{0.33\textwidth}
    \centering
    \vspace{-10mm}
    \includegraphics[width=\linewidth]{figs/prompt-tuning-figure.pdf}
    \vspace{-7mm}
    \caption{
    Instruction-tuned models respond better to continuous inputs from prompt tuning.
    When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning.
    Performance shown is the average on the SuperGLUE dev set.
    }
    \vspace{-3mm}
    \label{fig:prompt_tuning}
\end{wrapfigure}

As we've seen that instruction tuning improves the ability of a model to respond to instructions, it follows that, if \flan{} is indeed more amenable to performing NLP tasks, then it should also achieve better performance when performing inference using soft prompts, represented by prepended continuous variables optimized via prompt tuning \citep{li-liang-2021-prefix,lester-prompt-tuning}.
As further analysis, we train continuous prompts for each of the SuperGLUE \citep{wang2019superglue} tasks in accordance with the cluster splits from \cref{subsec:eval_splits} such that when prompt-tuning on task , no tasks in the same cluster as  were seen during instruction tuning.
Our prompt tuning setup follows the procedure of \citet{lester-prompt-tuning} except that we use a prompt length of 10, weight decay of 1e-4, and did not use dropout on the attention scores; we found in preliminary experiments that these changes improved the performance of \baselm. 

\cref{fig:prompt_tuning} shows the results of these prompt tuning experiments for both using a fully-supervised training set and in a low-resource setting with only 32 training examples.
We see that in all scenarios, prompt tuning works better with \flan{} than \baselm.
In many cases, especially for the low-resource setting, prompt tuning on \flan{} even achieves more than 10\% improvement over prompt tuning on the \baselm. 
This result exemplifies in another way how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks.





\section{Related Work}
Our work relates to several broad research areas including zero-shot learning, prompting, multi-task learning, and language models for NLP applications \citep[][\textit{inter alia}]{radford2019language,raffel2019exploring,brown2020language,efrat2020turking,aghajanyan2021muppet,li-liang-2021-prefix}.
We describe prior work for these broad areas in an extended related work section (\cref{sec:extended_related_work}), and here we describe two subareas narrower in scope that perhaps relate most closely to our work.

The way we ask a model to respond to instructions is similar to QA-based task formulation \citep{kumar2016ask,mccann2018natural}, which aims to unify NLP tasks by casting them as QA over a context.
Though these methods are very similar to ours, they mostly focus on multi-task learning instead of zero-shot learning, and---as noted by \citet{liu2021survey}---they are generally not motivated by using existing knowledge in pretrained LMs.
Moreover, our work supercedes recent work such as \citet{chai2020description} and \citet{zhong2021meta} in terms of both model scale and scope of tasks.

The success of language models has led to nascent research on the ability of models to follow instructions. 
Most recently, \citet{mishra2021natural} finetune 140M parameter BART on instructions with few-shot exemplars, and evaluate its few-shot abilities on unseen tasks---this is similar to our few-shot instruction tuning result from \cref{subsec:finetune}.
This promising result (as well as one from \citet{ye2021crossfit}, which does not emphasize instructions as much) suggests that finetuning on a collection of tasks improves few-shot performance on unseen tasks, even at a smaller model scale.
\citet{sanh2021multitask} finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a model of 11B parameters.
At a model scale similar to ours, OpenAI's InstructGPT models are trained via both finetuning and reinforcement learning to produce outputs that are more preferred by human raters \citep{ouyang2022instructgpt}.

\section{Discussion}

Our paper has explored a simple question in zero-shot prompting: does finetuning a model on a collection of tasks phrased as instructions improve its performance on unseen tasks?
We operationalize this question via instruction tuning, a simple method that combines appealing aspects of both the pretrain--finetune and prompting paradigms.
Our instruction-tuned model, \flan{}, improves performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that we evaluate on.
Ablation studies reveal that performance on unseen tasks improves with the number of instruction tuning task clusters, and, interestingly, that performance improvements from instruction tuning emerge only with sufficient model scale.
Moreover, instruction tuning can be combined with other prompting methods such as few-shot prompting and prompt tuning.

The diverse capabilities of language models at scale have drawn attention to the tradeoffs between specialist models (one model per task) and generalist models \citep[one model for many tasks;][]{arivazhagan2019massively,pratap2020massively}, for which our study has potential implications. 
Although one might expect labeled data to have the most natural role in improving specialist models, instruction tuning demonstrates how labeled data can be used to help large language models perform many, unseen tasks. 
In other words, the positive effect of instruction tuning on cross-task generalization shows that task-specific training is complementary to general language modeling and motivates further research on generalist models. 

As for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though we try to use accepted categorizations in the literature), and we only explore the use of relatively short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers). 
A limitation for our evaluation is that individual examples might have appeared in the modelsâ€™ pretraining data, which includes web documents, though in post-hoc analysis (\cref{sec:data_contamination}) we do not find any evidence that data overlap substantially impacted the results.
Finally, the scale of \flan{} 137B makes it costly to serve.
Future work on instruction tuning could include gathering/generating even more task clusters for finetuning, 
cross-lingual experiments, 
using \flan{} to generate data for training downstream classifiers, 
and using finetuning to improve model behavior with respect to bias and fairness \citep{solaiman2021process}. 

\section{Conclusions}
This paper has explored a simple method for improving the ability of language models at scale to perform zero-shot tasks based purely on instructions.
Our instruction-tuned model, \flan{}, compares favorably against GPT-3 and signals the potential ability for language models at scale to follow instructions.
We hope that our paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models. 









\clearpage

\section*{Ethical Considerations}
This work uses language models, for which the risks and potential harms are discussed in \citet{bender-koller-2020-climbing}, \citet{brown2020language}, \citet{10.1145/3442188.3445922}, Patterson et al., (2021), and others. As our contribution in this paper is not a pretrained language model itself but rather an empirical study of how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we additionally highlight two relevant ethical considerations. First, labeled datasets such as those we use for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot applications of the model on downstream tasks.  And second, instruction-tuned models can potentially require less data and expertise to use; such lower barriers to access could increase both the benefits and associated risks of such models.

\section*{Environmental Considerations}
We use the same pretrained language models as \citet{Austin2021ProgramSW}. The energy cost and carbon footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. 
The additional instruction tuning gradient-steps for finetuning \flan{} is less than 2\% of the number of pretraining steps, and so the estimated additional energy cost is comparatively smaller.

\section*{Author Contributions}\label{sec:contributions}
Maarten Bosma conceived the original idea and implemented the first version of \flan. 
Vincent Zhao prototyped the training and evaluation pipelines, as well as rank classification. 
Kelvin Guu proposed and implemented the idea of task clusters and evaluation using inter-cluster splits. 
Jason Wei, Maarten Bosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. 
Jason Wei, Vincent Zhao, and Adams Wei Yu conducted and managed most of the experiments. 
Jason Wei designed and ran the ablation studies. 
Jason Wei, Maarten Bosma, and Quoc V. Le wrote most of the paper. 
Jason Wei, Maarten Bosma, and Nan Du obtained the zero and few-shot baselines. 
Vincent Zhao and Kelvin Guu designed, implemented, and conducted the few-shot FLAN experiments.
Maarten Bosma and Jason Wei ran the data contamination analysis.
Brian Lester ran the prompt tuning experiments. 
Quoc V.~Le and Andrew M.~Dai advised, provided high-level guidance, and helped edit the paper.

\section*{Acknowledgements}
We thank Ed Chi, Slav Petrov, Dan Garrette, Ruibo Liu, and Clara Meister for providing feedback on our manuscript. 
We thank Adam Roberts, Liam Fedus, Hyung Won Chung, and Noam Shazeer for helping debug some of our models. 
We thank Ellie Pavlick for feedback on the study design during the middle stages of the project.  
We thank Daniel De Freitas Adiwardana for helping initiate the project, large language model advising, and giving us access to some computational resources.
Finally, we thank the team involved in pretraining \baselm{}: Daniel De Freitas Adiwardana, Noam Shazeer, Yanping Huang, Dmitry Lepikhin, Dehao Chen, Yuanzhong Xu and Zhifeng Chen.


\clearpage 
\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\newpage 
\appendix

\newcommand{\trainsize}[0]{30,000}
\newcommand{\devsize}[0]{200}

\section{Additional Results}\label{sec:supp_all_results}




This section shows the full results for all datasets we evaluate.
Results for translation and struct to text are shown in \cref{tab:nlg_table}, and the results for eight NLU task clusters are shown in \cref{tab:nlu_table}.

We show FLAN's performance using the best of up to ten instruction templates as well as the template with the best performance on the dev set. 
For \baselm{}, we use the templates from \citet{brown2020language}, which were optimized for GPT-3, without performing any prompt engineering to optimize them on our model. 
For simplicity, we use greedy search for all generative tasks (compared with beam search used in \citet{brown2020language}). 
Unlike GPT-3, which chooses the number of few-shot exemplars  via best dev set performance, for few-shot \baselm{} we choose the highest  that fits in the context length of 1024 tokens, from . 

For DROP \citep{Dua2019DROP} and SQuADv2 \citep{rajpurkar-etal-2018-know}, based on email correspondence with \citet{brown2020language}, their definition of zero-shot differs from ours in that they actually use exemplars, but only from the same passage as the inference question (each passage has more than one question). 
Hence, GPT-3 zero-shot results are not directly comparable with ours for DROP and SQuADv2.
We mark these results using the  symbol.
Moreover, it is unclear how to parse the end of an answer for these two datasets, and so we use curly bracket delimiters \texttt{\{} and \texttt{\}}, where we expect \texttt{\}} to indicate the end of the answer.

For struct to text, reported T5/mT5 results are from the GEM benchmark paper \citep{gehrmann2021gem}, though we do not report their results for DART (through correspondence with authors, we confirmed that their results for DART were incorrect). 
Though we use a summarization task cluster during instruction tuning, we leave evaluation of summarization for future work, as the mean input of most summarization datasets exceeds \flan's input length of 1024 tokens.

\begingroup
\setlength{\tabcolsep}{1.5pt}
\newcommand{\centerme}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\centermewithrightbar}[1]{\multicolumn{1}{c|}{#1}}
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l lc cl cl rc rc lc}
    \toprule
     & & & & & & & \multicolumn{6}{c}{FLAN 137B} \\
     \cmidrule(lr){8-13}
     & & & \multicolumn{2}{c}{\baselm{}} &  \multicolumn{2}{c}{GPT-3 175B} & \multicolumn{2}{c}{zero-shot} & \multicolumn{3}{c}{few-shot}\\
     \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-12}
     & Metric & \makecell[c]{\scriptsize Supervised\vspace{-0.6mm}\\\scriptsize Model} & \makecell[c]{zero-\vspace{-0.6mm}\\ shot} & \makecell[c]{few-\vspace{-0.6mm}\\ shot {\tiny []}} & \makecell[c]{zero-\vspace{-0.6mm}\\ shot} & \makecell[c]{few-\vspace{-0.6mm}\\ shot {\tiny []}} & \makecell[c]{\scriptsize average \vspace{-0.6mm}\\ \scriptsize template}  & \makecell[c]{\scriptsize best dev \vspace{-0.6mm}\\ \scriptsize template} &  \makecell[c]{\scriptsize average \vspace{-0.6mm}\\ \scriptsize template}  & \makecell[c]{\scriptsize best dev \vspace{-0.6mm}\\ \scriptsize template} & {\tiny []} & {\scriptsize \#} \\
    \midrule
    \tasktype{Translation} \\
    WMT '14 EnFr & BLEU & 35.0     \baselmvala{11.2}{31.5}{5} \gptvala{25.2}{32.6}{[64]} \flanvala{32.9}{1.1}{33.9} \flanvala{33.9}{0.2}{33.8} & \fewk{9} & \tiny{5} \\
    WMT '14 FrEn & BLEU & 45.6     \baselmvala{7.2}{34.7}{5} \gptvala{21.2}{39.2}{[64]} \flanvala{35.5}{1.3}{35.9} \flanvala{38.0}{0.1}{37.9} & \fewk{9} & \tiny{3} \\
    WMT '16 EnDe & BLEU & 38.6     \baselmvala{7.7}{26.7}{5} \gptvala{24.6}{29.7}{[64]} \flanvala{25.4}{1.8}{27.0} \flanvala{26.8}{0.4}{26.1} & \fewk{11} & \tiny{5} \\
    WMT '16 DeEn & BLEU & 41.2     \baselmvala{20.8}{36.8}{5} \gptvala{27.2}{40.6}{[64]} \flanvala{38.9}{0.3}{38.9} \flanvala{40.6}{0.1}{40.7} & \fewk{11} & \tiny{3} \\
    WMT '16 EnRo & BLEU & 39.9     \baselmvala{3.5}{22.9}{5} \gptvala{14.1}{21.0}{[64]} \flanvala{16.7}{1.6}{18.9} \flanvala{20.5}{0.1}{20.5} & \fewk{9} & \tiny{5} \\
    WMT '16 RoEn & BLEU & 38.5     \baselmvala{9.7}{37.5}{5} \gptvala{19.9}{39.5}{[64]} \flanvala{36.8}{0.5}{37.3} \flanvala{38.2}{0.1}{38.1} & \fewk{9} & \tiny{3} \\
    \midrule
    \tasktype{Struct to Text} \\
    CommonGen & Rouge-1 & \tfiveval{64.0}   \baselmvala{3.9}{56.7}{3} \gptvala{\na}{\na}{} \flanvala{54.6}{2.3}{56.3} \flanvala{56.6}{0.3}{56.4} & \fewk{16} & \tiny{6} \\
     & Rouge-2 & \tfiveval{29.4}            \baselmvala{1.5}{29.6}{3} \gptvala{\na}{\na}{} \flanvala{28.8}{2.4}{27.6} \flanvala{30.9}{0.7}{29.9} & \fewk{16} & \tiny{6} \\
     & Rouge-L & \tfiveval{54.5}            \baselmvala{3.2}{48.5}{3} \gptvala{\na}{\na}{} \flanvala{48.4}{1.9}{48.7} \flanvala{50.7}{0.2}{51.0} & \fewk{16} & \tiny{6} \\
    DART & Rouge-1 & \na                    \baselmvala{11.3}{56.0}{3} \gptvala{\na}{\na}{} \flanvala{45.5}{4.2}{48.9} \flanvala{57.9}{1.6}{59.2} & \fewk{11} & \tiny{7} \\
     & Rouge-2 & \na                        \baselmvala{1.5}{29.6}{3} \gptvala{\na}{\na}{} \flanvala{25.0}{3.7}{30.0} \flanvala{35.8}{1.0}{36.2} & \fewk{11} & \tiny{7} \\
     & Rouge-L & \na                        \baselmvala{3.2}{48.5}{3} \gptvala{\na}{\na}{} \flanvala{38.4}{3.8}{43.4} \flanvala{48.5}{0.9}{48.2} & \fewk{11} & \tiny{7} \\
    E2ENLG & Rouge-1 & \tfiveval{72.6}      \baselmvala{6.2}{56.7}{3} \gptvala{\na}{\na}{} \flanvala{44.8}{3.9}{51.4} \flanvala{59.1}{1.3}{59.7} & \fewk{12} & \tiny{9} \\
     & Rouge-2 & \tfiveval{47.5}            \baselmvala{2.5}{31.4}{3} \gptvala{\na}{\na}{} \flanvala{24.2}{3.6}{30.1} \flanvala{33.2}{1.1}{33.6} & \fewk{12} & \tiny{9} \\
     & Rouge-L & \tfiveval{56.4}            \baselmvala{4.9}{41.1}{3} \gptvala{\na}{\na}{} \flanvala{37.0}{3.5}{42.4} \flanvala{44.9}{0.8}{45.1} & \fewk{12} & \tiny{9} \\
    WebNLG & Rouge-1 & \tfiveval{83.5}      \baselmvala{13.9}{68.3}{3} \gptvala{\na}{\na}{} \flanvala{50.6}{4.7}{57.7} \flanvala{68.5}{2.2}{71.2} & \fewk{10} & \tiny{8} \\
     & Rouge-2 & \tfiveval{63.6}            \baselmvala{6.9}{46.0}{3} \gptvala{\na}{\na}{} \flanvala{29.8}{4.2}{35.4} \flanvala{48.0}{1.5}{49.8} & \fewk{10} & \tiny{8} \\
     & Rouge-L & \tfiveval{71.0}            \baselmvala{11.8}{56.5}{3} \gptvala{\na}{\na}{} \flanvala{43.4}{4.5}{49.7} \flanvala{58.8}{1.1}{60.2} & \fewk{10} & \tiny{8} \\
    \bottomrule
    \end{tabular}
    \caption{
    Results for translation and struct-to-text tasks.
    \explainkt
    T5-11B,
    \citet{edunov-etal-2018-understanding},
    \citet{durrani-etal-2014-edinburghs},
    \citet{wang2019multi},
    \citet{sennrich-etal-2016-edinburgh},
    \citet{liu-etal-2020-multilingual-denoising}.
    }
    \label{tab:nlg_table}
\end{table}
\endgroup 
\begingroup
\setlength{\tabcolsep}{0.8pt}
\newcommand{\centerme}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\centermewithrightbar}[1]{\multicolumn{1}{c|}{#1}}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l cc ll cl cl lc lcl r}
    \toprule
     & & & & & & & & & \multicolumn{6}{c}{FLAN 137B} \\
     \cmidrule(lr){10-15}
     & & & \multicolumn{2}{c}{GLaM} & \multicolumn{2}{c}{\baselm{}} & \multicolumn{2}{c}{GPT-3 175B} & \multicolumn{2}{c}{zero-shot} & \multicolumn{3}{c}{few-shot} \\
     \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-14}
     & \makecell[c]{\scriptsize Random\vspace{-0.6mm}\\ \scriptsize Guess} & \makecell[c]{\scriptsize Supervised\vspace{-0.6mm}\\\scriptsize Model} & \makecell[c]{zero-\vspace{-0.6mm}\\ shot} & \makecell[c]{one-\vspace{-0.6mm}\\ shot} & \makecell[c]{zero-\vspace{-0.6mm}\\ shot} & \makecell[c]{few-\vspace{-0.6mm}\\ shot {\tiny []}} & \makecell[c]{zero-\vspace{-0.6mm}\\ shot} & \makecell[c]{few-\vspace{-0.6mm}\\ shot {\tiny []}} & \makecell[c]{\scriptsize average \vspace{-0.6mm}\\ \scriptsize template}  & \makecell[c]{\scriptsize best dev \vspace{-0.6mm}\\ \scriptsize template} &  \makecell[c]{\scriptsize average \vspace{-0.6mm}\\ \scriptsize template}  & \makecell[c]{\scriptsize best dev \vspace{-0.6mm}\\ \scriptsize template} & {\tiny []} & {\scriptsize \#} \\
    \midrule
    \tasktype{NLI} \\
    ANLI R1 & 33.3 & \bertlargeval{57.4} & 40.9 & 42.4       \baselmvala{39.6}{39.0}{5} \gptvala{34.6}{36.8}{[50]} \flanvala{47.7}{1.4}{46.4} \flanvala{44.2}{2.3}{47.9} & \fewk{6} & \tiny{8} \\
    ANLI R2 & 33.3 & \bertlargeval{48.3} & 38.2 & 40.0       \baselmvala{39.9}{37.5}{5} \gptvala{35.4}{34.0}{[50]} \flanvala{43.9}{1.3}{44.0} \flanvala{41.6}{1.4}{41.1} & \fewk{6} & \tiny{8} \\
    ANLI R3 & 33.3 & \bertlargeval{43.5} & 40.9 & 40.8       \baselmvala{39.3}{40.7}{5} \gptvala{34.5}{40.2}{[50]} \flanvala{47.0}{1.3}{48.5} \flanvala{42.8}{2.2}{46.8} & \fewk{6} & \tiny{8} \\
    CB & 33.3 & \tfiveval{93.6} & 33.9 & 73.2                 \baselmvala{42.9}{34.4}{5} \gptvala{46.4}{82.1}{[32]} \flanvala{64.1}{14.7}{83.9} \flanvala{82.6}{4.4}{82.1} & \fewk{7} & \tiny{10} \\
    MNLI-m & 33.3 & \tfiveval{92.2} & \na & \na             \baselmvala{35.7}{43.7}{5} \gptvala{\na}{\na}{} \flanvala{51.1}{6.2}{61.2} \flanvala{60.8}{3.7}{63.5} & \fewk{10} & \tiny{10} \\
    MNLI-mm & 33.3 & \tfiveval{91.9} & \na & \na           \baselmvala{37.0}{43.8}{5} \gptvala{\na}{\na}{} \flanvala{51.0}{6.5}{62.4} \flanvala{61.0}{3.5}{63.5} & \fewk{10} & \tiny{10} \\
    QNLI & 50.0 & \tfiveval{96.9} & \na & \na              \baselmvala{50.6}{55.7}{5} \gptvala{\na}{\na}{} \flanvala{59.6}{4.9}{66.4} \flanvala{62.0}{1.7}{63.3} & \fewk{12} & \tiny{9} \\
    RTE & 50.0 & \tfiveval{92.5} & 68.8 & 71.5              \baselmvala{73.3}{70.8}{5} \gptvala{63.5}{72.9}{[32]} \flanvala{78.3}{7.9}{84.1} \flanvala{79.9}{6.9}{84.5} & \fewk{8} & \tiny{10} \\
    SNLI & 33.3 & \bertlargeval{91.3} & \na & \na          \baselmvala{33.3}{54.7}{5} \gptvala{\na}{\na}{} \flanvala{43.0}{7.4}{53.4} \flanvala{62.3}{2.4}{65.6} & \fewk{15} & \tiny{9} \\
    WNLI & 50.0 & \tfiveval{94.5} & \na & \na              \baselmvala{56.3}{64.8}{5} \gptvala{\na}{\na}{} \flanvala{61.0}{10.6}{74.6} \flanvala{55.4}{11.0}{70.4} & \fewk{14} & \tiny{10} \\
    \midrule
    \tasktype{Reading Comp.} \\
    BoolQ & 50.0 & \tfiveval{91.2} & 83.0 & 82.8             \baselmvala{81.0}{80.0}{1} \gptvala{60.5}{77.5}{[32]} \flanvala{80.2}{3.1}{82.9} \flanvala{83.6}{0.8}{84.6} & \fewk{4} & \tiny{9} \\
    DROP & \na & \bertlargeval{80.5} & 54.9 & 55.2      \baselmvala{3.8}{10.3}{1} \gptvala{23.6}{36.5}{[20]} \flanvala{21.9}{0.9}{22.7} \flanvala{22.3}{1.1}{23.9} & \fewk{2} & \tiny{7} \\
    MultiRC & \na & \tfiveval{88.1} & 45.1 & 62.0        \baselmvala{60.0}{59.6}{5} \gptvala{72.9}{74.8}{[32]} \flanvala{74.5}{3.7}{77.5} \flanvala{69.2}{3.2}{72.1} & \fewk{1} & \tiny{8} \\
    OBQA & 25.0 & \tfiveval{85.4} & 53.0 & 55.2              \baselmvala{41.8}{50.6}{10} \gptvala{57.6}{65.4}{[100]} \flanvala{77.4}{1.3}{78.4} \flanvala{77.2}{1.3}{78.2} & \fewk{16} & \tiny{7} \\
    SQuADv1 & \na & \tfiveval{96.2} & \na & \na        \baselmvala{22.7}{50.2}{3} \gptvala{\na}{\na}{} \flanvala{79.5}{1.6}{80.1} \flanvala{82.1}{0.5}{82.7} & \fewk{4} & \tiny{8} \\
    SQuADv2 & \na & \bertlargeval{83.4} & 68.3 & 70.0   \baselmvala{11.1}{34.9}{3} \gptvala{59.5}{69.8}{[16]} \flanvala{40.9}{1.8}{44.2} \flanvala{40.8}{0.9}{43.1} & \fewk{3} & \tiny{10} \\
    \midrule 
    \tasktype{Closed-Book QA} \\
    ARC-c & 25.0 & \tfiveval{81.1} & 48.2 & 50.3     \baselmvala{42.0}{49.4}{10} \gptvala{51.4}{51.5}{[50]} \flanvala{61.7}{1.4}{63.1} \flanvala{63.7}{0.6}{63.8} & \fewk{13} & \tiny{7} \\
    ARC-e & 25.0 & \tfiveval{92.6} & 71.9 & 76.6           \baselmvala{76.4}{80.9}{10} \gptvala{68.8}{70.1}{[50]} \flanvala{79.5}{0.8}{79.6} \flanvala{80.5}{0.5}{80.7} & \fewk{14} & \tiny{7} \\
    NQ & \na & \tfiveval{36.6} & 21.5 & 23.9            \baselmvala{3.2}{22.1}{5} \gptvala{14.6}{29.9}{[64]} \flanvala{18.6}{2.7}{20.7} \flanvala{27.2}{0.5}{27.6} & \fewk{16} & \tiny{10} \\
    TQA {\tiny (wiki)} & \na & \tfiveval{60.5} & 68.8 & 71.5      \baselmvala{21.9}{63.3}{10} \gptvala{64.3}{71.2}{[64]} \flanvala{66.5}{2.6}{68.1} \flanvala{66.5}{1.0}{67.3} & \fewk{16} & \tiny{10} \\
    TQA {\tiny (tfds-dev)} & \na & \tfiveval{51.0} & \na & \na      \baselmvala{18.4}{55.1}{10} \gptvala{\na}{\na}{\na} \flanvala{55.0}{2.3}{56.7} \flanvala{57.2}{0.6}{57.8} & \fewk{16} & \tiny{10} \\
    \midrule 
    \tasktype{Commonsense} \\
    COPA & 50.0 & \tfiveval{94.8} & 90.0 & 92.0              \baselmvala{90.0}{89.0}{10} \gptvala{91.0}{92.0}{[32]} \flanvala{90.6}{2.0}{91.0} \flanvala{88.5}{3.8}{87.0} & \fewk{16} & \tiny{8} \\
    HellaSwag & 25.0 & \bertlargeval{47.3} & 77.1 & 76.8     \baselmvala{57.0}{58.8}{10} \gptvala{78.9}{79.3}{[20]} \flanvala{56.4}{0.5}{56.7} \flanvala{59.4}{0.2}{59.2} & \fewk{3} & \tiny{8} \\
    PIQA & 50.0 & \bertlargeval{66.8} & 80.4 & 81.4          \baselmvala{80.3}{80.2}{10} \gptvala{81.0}{82.3}{[50]} \flanvala{80.9}{0.8}{80.5} \flanvala{82.1}{0.3}{81.7} & \fewk{10} & \tiny{8} \\
    StoryCloze & 50.0 & \bertlargeval{89.2} & 82.5 & 84.0    \baselmvala{79.5}{83.7}{10} \gptvala{83.2}{87.7}{[70]} \flanvala{92.2}{1.3}{93.4} \flanvala{93.3}{0.9}{94.7} & \fewk{10} & \tiny{8} \\
    \midrule 
    \tasktype{Sentiment} \\
    IMDB & 50.0 & \bertlargeval{95.5} & \na & \na          \baselmvala{76.9}{83.3}{1} \gptvala{\na}{\na}{} \flanvala{94.1}{0.4}{94.3} \flanvala{94.8}{0.3}{95.0} & \fewk{2} & \tiny{7} \\
    Sent140 & 50.0 & \bertlargeval{87.0} & \na & \na   \baselmvala{41.4}{63.3}{5} \gptvala{\na}{\na}{} \flanvala{69.9}{2.5}{73.5} \flanvala{68.7}{1.2}{69.3} & \fewk{16} & \tiny{6} \\
    SST-2 & 50.0 & \tfiveval{97.5} & \na & \na              \baselmvala{51.0}{92.3}{5} \gptvala{71.6}{95.6}{[8]} \flanvala{92.6}{1.7}{94.6} \flanvala{94.4}{0.8}{94.6} & \fewk{16} & \tiny{8} \\
    Yelp & 50.0 & \bertlargeval{98.1} & \na & \na           \baselmvala{84.7}{89.6}{3} \gptvala{\na}{\na}{} \flanvala{97.8}{0.2}{98.1} \flanvala{97.9}{0.1}{98.0} & \fewk{4} & \tiny{7} \\
    \midrule 
    \tasktype{Paraphrase} \\
    MRPC & 50.0 & \tfiveval{90.4} & \na & \na               \baselmvala{53.7}{64.0}{5} \gptvala{\na}{\na}{} \flanvala{69.1}{1.3}{69.1} \flanvala{67.5}{1.7}{67.2} & \fewk{10} & \tiny{10} \\
    QQP & 50.0 & \tfiveval{90.6} & \na & \na                \baselmvala{34.9}{58.9}{3} \gptvala{\na}{\na}{} \flanvala{72.1}{6.8}{75.9} \flanvala{73.5}{2.9}{75.9} & \fewk{16} & \tiny{7} \\
    PAWS Wiki & 50.0 & \tfiveval{91.9} & \na & \na          \baselmvala{45.5}{53.5}{5} \gptvala{\na}{\na}{} \flanvala{61.5}{6.5}{69.4} \flanvala{66.2}{0.9}{70.2} & \fewk{10} & \tiny{10} \\
    \midrule 
    \tasktype{Coreference} \\
    DPR & 50.0 & \bertlargeval{84.8} & \na & \na            \baselmvala{54.6}{57.3}{5} \gptvala{\na}{\na}{} \flanvala{60.3}{3.5}{66.8} \flanvala{62.4}{1.6}{63.3} & \fewk{16} & \tiny{10} \\
    Winogrande & 50.0 & \bertlargeval{65.8} & 73.4 & 73.0     \baselmvala{68.3}{68.4}{10} \gptvala{70.2}{77.7}{[50]} \flanvala{67.3}{2.5}{71.2} \flanvala{72.3}{0.9}{72.8} & \fewk{16} & \tiny{10} \\
    WSC273 & 50.0 & \bertlargeval{70.0} & 86.8 & 83.9        \baselmvala{81.0}{61.5}{5} \gptvala{88.3}{88.5}{[32]} \flanvala{80.8}{3.7}{\na} \flanvala{\na}{\na}{\na} & \fewk{\na} & \tiny{10} \\
    \midrule 
    \tasktype{Read. Comp. w/ Commonsense} \\
    CosmosQA & 25.0 & \bertlargeval{67.1} & \na & \na      \baselmvala{34.1}{33.8}{5} \gptvala{\na}{\na}{} \flanvala{58.4}{1.3}{60.6} \flanvala{56.7}{1.3}{56.0} & \fewk{5} & \tiny{8} \\
    ReCoRD & \na & \tfiveval{93.4} & 90.3 & 90.3              \baselmvala{87.8}{87.6}{1} \gptvala{90.2}{89.0}{[32]} \flanvala{67.8}{3.0}{72.5} \flanvala{77.0}{2.0}{79.0} & \fewk{1} & \tiny{10} \\
    
    
    \bottomrule
    \end{tabular}
    \caption{
    Results for eight NLU task clusters. 
    All values shown are for accuracy (or exact match) except DROP, MultiRC, and SQuAD v1 and v2, which are F1.
    \explainkt
    T5-11B,
    BERT-large.
    see data contamination (\cref{sec:data_contamination}).
    WSC273 \citep{levesque2012winograd} does not have training or validation sets, and so we do not compute few-shot results for FLAN.
    For Trivia QA (TQA), we report exact match (EM) on both the wikipedia subset of the dev set to compare with GPT-3, as well as the full TFDS dev set.
    }
    \label{tab:nlu_table}
\end{table}
\endgroup 

\clearpage
\section{Further Ablation Studies and Analysis}
\subsection{Datasets per Task Cluster \& Templates per Dataset}
Our primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on unseen tasks.
\cref{subsec:finetuning_clusters} showed that adding more task clusters improves performance; here, we further explore whether adding additional datasets improves performance when the number of task clusters is held constant.
We use the same split as in \cref{subsec:finetuning_clusters}, where the NLI, commonsense reasoning, and closed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning.
For these seven task clusters, we instruction tune models using just one dataset per task cluster and using four datasets per task cluster (for task clusters that did not have four tasks, we just used all available tasks). 
In addition, we simultaneously explore the role of the number of instruction templates per dataset; as mentioned in \cref{subsec:tasks_and_templates}, for each dataset we manually composed ten instructional templates for instruction tuning. 
Here, we instruction tune models using 1, 4, and 10 templates per dataset.

\cref{fig:ablation-templates} shows these results.
Using more datasets per cluster improved performance by almost 10\% on average across the three held-out clusters. 
Using more templates per dataset, however, had a comparatively negligible effect on performance when there was one task per cluster, which disappeared when there were four tasks per cluster.
The small effect of templates is striking given our original motivation that composing ten templates per task would mitigate overfitting to any particular template. 
This results serves to underscore, however, the unpredictability of finetuning large language models, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\linewidth]{figs/templates-per-dataset.png}
    \vspace{-2mm}
    \caption{Effect of datasets per task cluster and templates per dataset on performance on three held-out clusters: NLI, commonsense reasoning, and closed-book QA.
    Adding more datasets per task cluster substantially improves performance.
    Using more templates per dataset, however, only had a very small effect on performance, which disappeared when there were sufficient dataset per task cluster.
    }
    \label{fig:ablation-templates}
\end{figure}

\subsection{Role of instructions during finetuning}\label{subsec:role_instructions}
The per-cluster results for the ablation study from \cref{subsec:role-of-instructions} are shown in \cref{tab:no_instructions}.





\begingroup
\setlength{\tabcolsep}{4.5pt}
\begin{table}[h]
    \centering
    \begin{tabular}{ll cccc c}
    \toprule
     & & \multicolumn{5}{c}{Zero-shot performance on unseen task cluster} \\
     \cmidrule(lr){3-7} 
     
    \makecell[l]{Finetuning prompt} & \makecell[l]{Inference prompt} & \makecell[c]{NLI} & \makecell[c]{\footnotesize{Read.}\vspace{-0.5mm}\\\footnotesize{Comp.}} &  \makecell[c]{\footnotesize{Closed-}\vspace{-0.5mm}\\\footnotesize{Book QA}} & \makecell[c]{Translation} & \makecell[c]{\footnotesize{\underline{Four-Task}}\vspace{-0.2mm}\\\footnotesize{\underline{Average}}}  \\
     \midrule
    \makecell[l]{Natural instructions \CNN) -- At first glance, "The Flat" might seem like an episode of "Hoarders," Israeli-style. The documentary film opens after an elderly woman dies in Tel Aviv. Her grandchildren assemble to clean out her apartment, packed with dusty books, vintage clothing (dozens of pairs of fancy gloves, for instance), enough purses to stock a department store, jewelry, mementoes and closets full of knickknacks. But buried among the detritus they chance upon something remarkable -- mysterious papers linking the grandparents to an important Nazi figure. How could such ardent Zionists, who left their native Germany in the early 1930s, have been involved with an SS official like Leopold von Mildenstein?\\\\What I found out was this journey, the Nazi (\\\\OPTIONS:\\- Arnon Goldfinger) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- CNN) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- Germany) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- Israeli) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- Leopold von Mildenstein) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- Nazi) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- SS) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- Tel Aviv) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- The Flat) and his wife were accompanied by my grandparents," Goldfinger told CNN.\\- Zionists) and his wife were accompanied by my grandparents," Goldfinger told CNN.}
{Leopold von Mildenstein) and his wife were accompanied by my grandparents," Goldfinger told CNN.}
{
    \taskdescription
    {Reading Comprehension with Commonsense Reasoning} {ReCoRD} {\citep{DBLP:journals/corr/abs-1810-12885}} {asks for the answer to a cloze-style question where an entity is masked out} {the training set of 100,730 examples} {30,000} {200} {the TFDS validation set of 10,000 examples} }





 
\subsection{Translation (7 languages)}\label{appen:data-to-text}

\taskio
{Here the largest town of the district is located: Nordenham , lying opposite to Bremerhaven at the Weser mouth.\\\\Translate to German}
{An der B 211 befindet sich in Loyermoor der so genannte ``Geest-Abbruch'', der eine HÃ¶hendifferenz von gut 30 Meter Ã¼berbrÃ¼ckt.}
{
Example input and output for translation. This example is from WMT'16 English--German; all languages use the same translation templates.
}






 
\end{document}
