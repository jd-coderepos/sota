[{'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2016 Romanian-English', 'Metric': 'BLEU score', 'Score': '36.7'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2016 English-Romanian', 'Metric': 'BLEU score', 'Score': '18.4'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2014 French-English', 'Metric': 'BLEU score', 'Score': '36.5'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2016 English-German', 'Metric': 'BLEU score', 'Score': '27.0'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2014 English-French', 'Metric': 'BLEU score', 'Score': '34'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2016 German-English', 'Metric': 'BLEU score', 'Score': '39.8'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '77.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '80.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '82.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'ARC-c', 'Metric': 'Accuracy', 'Score': '63.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '56.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'NaturalQA', 'Metric': 'EM', 'Score': '20.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'ARC-e', 'Metric': 'Accuracy', 'Score': '79.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Story Cloze', 'Metric': 'Accuracy', 'Score': '93.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '91.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OBQA', 'Metric': 'Accuracy', 'Score': '78.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WSC273', 'Metric': 'Accuracy', 'Score': '80.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '72.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '84.1%'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '56.7'}}]
