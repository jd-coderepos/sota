

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} \input{ushyphex}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[accsupp]{axessibility}
\usepackage{listings}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{mystyle}{
backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}



\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\def\cvprPaperID{10929} \def\confName{CVPR}
\def\confYear{2023}

\title{MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset}


\newtheorem{theorem}{\textsc{Theorem}}

\begin{document}

\author{Chen Feng, Ioannis Patras\\
Queen Mary University of London, UK\\
{\tt\small \{chen.feng, i.patras\}@qmul.ac.uk}
}
\maketitle

\begin{abstract}
Deep learning has achieved great success in recent years with the aid of advanced neural network structures and large-scale human-annotated datasets. However, it is often costly and difficult to accurately and efficiently annotate large-scale datasets, especially for some specialized domains where fine-grained labels are required. In this setting, coarse labels are much easier to acquire as they do not require expert knowledge. In this work, we propose a contrastive learning method, called \textbf{mask}ed \textbf{con}trastive learning~(\textbf{MaskCon}) to address the under-explored problem setting, where we learn with a coarse-labelled dataset in order to address a finer labelling problem. More specifically, within the contrastive learning framework, for each sample our method generates soft-labels  with the aid of coarse labels against other samples and another augmented view of the sample in question. 
By contrast to self-supervised contrastive learning where only the sample's augmentations are considered hard positives, and in supervised contrastive learning where only samples with the same coarse labels are considered hard positives, we propose soft labels based on sample distances, that are masked by the coarse labels. This allows us to utilize both inter-sample relations and coarse labels. 
We demonstrate that our method can obtain as special cases many existing state-of-the-art works and that it provides tighter bounds on the generalization error. 
Experimentally, our method achieves significant improvement over the current state-of-the-art in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online Products and Stanford Cars196 datasets. Code and annotations are available at \url{https://github.com/MrChenFeng/MaskCon_CVPR2023}.
\end{abstract}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figures/workflow2.pdf}
\caption{Contrastive learning sample relations using MaskCon (ours) and other learning paradigms when only coarse labels are available. MaskCon are closer to the fine ones.}
    \label{fig:intro_mechanism_toy}
\end{figure}

\section{Introduction}
Supervised learning with deep neural networks has achieved great success in various computer vision tasks such as image classification, action detection and object localization. However, the success of supervised learning relies on large-scale and high-quality human-annotated datasets, whose annotations are time-consuming and labour-intensive to produce. To avoid such reliance, various learning frameworks have been proposed and investigated: Self-supervised learning aims to learn meaningful representations with heuristic proxy visual tasks, such as rotation prediction~\cite{gidaris2018rotpredict} and the more prevalent instance discrimination task, the latter, being widely applied in self-supervised contrastive learning framework; semi-supervised learning usually considers a dataset for which only a small part is annotated -- within this setting, pseudo labelling methods~\cite{pseudolabelling} and consistency regularization techniques~\cite{mixmatch, fixmatch} are typically used; Moreover, learning using more accessible but noisy data, such as web-crawled data, has also received increasing attention~\cite{ssr, dividemix}.

In this work, we consider an under-explored problem setting aiming at reducing the annotation effort -- learning fine-grained representations with a coarsely-labelled dataset. Specifically, we learn with a dataset that is fully labeled, albeit at a coarser granularity than we are interested in (i.e., that of the test set). Compared to fine-grained labels, coarse labels are often significantly easier to obtain, especially in some of the more specialized domains, such as the recognition and classification of medical pathology images. As a simple example, for the task of differentiation between different pets, we need a knowledgeable cat lover to distinguish between `British short' and `Siamese', but even a child annotator may help to discriminate between `cat' and `non-cat'~(\cref{fig:intro_mechanism_toy}). 
Unfortunately, learning with a coarse labelled dataset has been less investigated compared to other weakly supervised learning paradigms. 
Recently, Bukchi et al.~\cite{coarse_fewshot} investigate on learning with coarse labels in the few-shot setting. More closely related to us, Grafit~\cite{touvron2021grafit} proposes a multi-task framework by a weighted combination of self-supervised contrastive learning and supervised contrastive learning cost; Similarly, CoIns~\cite{xu2021coins} uses both a self-supervised contrastive learning cost and a supervised learning cross-entropy loss. Both works combine a fully supervised learning cost (cross entropy or contrastive) with a self-supervised contrastive loss -- these works are the main ones with which we compare. 

Differently than them, instead of using self-supervised contrastive learning as an auxiliary task, we propose a novel learning scheme, namely \textbf{Mask}ed \textbf{Con}trastive Learning~(\textbf{MaskCon}).  Our method aims to learn by considering inter-sample relations of each sample with other samples in the dataset. Specifically, we always consider the relation to oneself as confidently positive. To estimate the relations to other samples, we derive soft labels by contrasting an augmented view of the sample in question with other samples, and further improve it by utilizing the \textit{mask} generated based on the coarse labels.
Our approach generates soft inter-sample relations that can more accurately estimate fine inter-sample relations compared to the baseline methods~(\cref{fig:intro_mechanism_toy}). Efficiently and effectively, our method achieves significant improvements over the state-of-the-art in various datasets, including CIFARtoy, CIFAR100, ImageNet-1K and more challenging fine-grained datasets Stanford Online Products and Stanford Cars196. 

\section{Related works}
In this section, we first briefly review the contrastive learning works which are the basis of our method and then, we briefly introduce the state-of-the-art works in related problem settings. 



\paragraph{Self-supervised contrastive learning}
\label{sec:contrastive_learning_rw}
Contrastive learning has recently emerged as a powerful method for representation learning without labels. In the context of self-supervision, contrastive learning has been used for instance discrimination, producing models whose performance rivals that of fully supervised training. The objective of contrastive instance discrimination is to minimize the distance between transformed views of the same sample in the feature space, while maximizing their distance from views of other samples. Notable methods in this area are SimCLR~\cite{simclr} and MoCo~\cite{moco}, as well as other variants including SwAV~\cite{swav}, which leverages clustering to identify prototypes for the contrastive learning objective and~\cite{wang2021solving}, that reformulates the instance discrimination contrastive objective in the form of a triplet loss. These methods have been found to produce models that, requiring only limited amounts of annotated data for fine-tuning on a particular task or domain, achieve performance that reaches or even surpasses that of supervised models~\cite{simclrv2, ericsson2021well}. Furthermore, self-supervised training has been found to compare positively with supervised training in other respects, such as producing models that perform better in the context of continual learning~\cite{gallardo2021self}.

However, self-supervised contrastive learning based on instance discrimination task usually suffers from the `false negative' problem -- that samples from the same class should not be considered as negative. For example, pictures of different cats should not be considered completely negative to each other. To this end, heuristically modifying the inter-sample relations have been widely applied in self-supervised learning~\cite{dwibedi2021nnclr, meanshift, CLID, ascl}, intuitively similar to us. Many of the state-of-the-art methods can be considered as special cases of our method by the adjustment of hyperparameters~(\cref{method:relation_to_otherworks}).

\paragraph{Combing contrastive learning with supervised learning}
\label{sec:combination_rw}
The success of self-supervised contrastive learning has led to increasing attention on how to integrate contrastive learning into existing supervised learning paradigms. One approach is supervised contrastive learning, which adapts contrastive learning to the fully supervised setting. Supervised contrastive learning was first introduced in \cite{nca, cns} under the name of neighbourhood components analysis (NCA). Recently, supervised contrastive learning has regained interest due to the significant progress made in \cite{supcon} by employing more advanced network structures and image augmentations. Compared to standard supervised learning using cross-entropy loss, supervised contrastive learning exhibits superior performance in hard sample mining~\cite{supcon}.

Other works have attempted to introduce self-supervised contrastive learning as an auxiliary task, especially for weakly supervised learning problems. Gidaris et al.~\cite{gidaris2019boosting} jointly train a supervised classifier and a rotation-prediction objective. Komodakis and Gidaris~\cite{komodakis2018unsupervised} seeking to improve model performance on few-shot learning. Wei et al.~\cite{wei2020can} propose a label-filtered instance discrimination objective for the pre-training of models to be used for transfer learning in other datasets. S4~\cite{zhai2019s4l} uses rotation and exemplar~\cite{dosovitskiy2014discriminative} prediction to improve performance in semi-supervised learning tasks. Feng et al.~\cite{ssr} apply a self-supervised contrastive loss to safely learn meaningful representations in the presence of noisy labels.

\paragraph{Hierarchical image classification \& Unsupervised clustering}
Hierarchical image classification methods aim to learn better representations with datasets having hierarchically structured labels. To utilize information from different levels of labels,  Zhang et al.~\cite{zhang2022hierarchical_alllabels} propose a multi-losses contrastive learning framework with each loss considering a specific level of labels.
Unsupervised clustering methods often have the same problem setting but different goals than self-supervised learning, that is, to recover and identify the ground-truth semantic labels of each sample~\cite{caron2018deepclustering, van2020scan}. DeepCluster~\cite{caron2018deepclustering} simultaneously learns the clustering assignments and a parametric classifier, by feeding the clustering assignments as pseudo labels to the classifier. SCAN~\cite{van2020scan} inherits the idea of DeepClustering while augmenting extra self-labelling and neighbor mining techniques.

\section{Methodology}\label{method}
We consider the problem of learning when the labels of the training data and the labels of the test data are inconsistent in granularity, and in particular when the labels of the training set are coarser. More specifically, let us denote with  an i.i.d sampled train dataset with annotated \textit{coarse labels}  subject to . Here,  denotes the number of samples and  denotes the number of coarse classes. Let us also define a finely labelled set  subject to , where  denotes the number of fine classes. Typically, . 

\subsection{Preliminaries}\label{method_pre}
We first quickly review the common learning paradigms utilized in the baseline methods and our method.
\subsubsection{Supervised learning}\label{method_pre_supervised}
In most supervised learning frameworks, the objective is the minimization of the below empirical risk:

where  denotes the feature encoder, and  denotes the classifier head~(usually a single fully-connected layer).

For brevity, we denote ,  and  as the \textit{feature}, \textit{logit} and \textit{prediction} of a sample view  respectively. Here,

is the cross-entropy loss function -- the default loss function for classification problems.

\subsubsection{Contrastive learning}
Unlike the common supervised learning model above where a parametric classifier  is learned based on the semantic labels  of the samples , we can also perform contrastive learning based on the inter-sample relations , with each entry  depicting the inter-sample relation between  and . 
Intuitively,  means that sample  and  generate a strong positive pair. Since each sample may form multiple positive sample pairs, for brevity, we abuse the notation here with  denoting also the sample-wise normalized inter-sample relations.
To learn such inter-sample relations, instead of a parametric classifier , the  encoder is usually followed by a projector , which is often implemented as an MLP and learned by regularizing the inter-sample relations ~(\cref{eq:selfconrisk}). 

More specifically, let us denote by  the \textit{projection}. We first calculate the cosine similarity  between a sample  and the dataset :

Let us further define , where  is the temperature hyperparameter. Then the following empirical risk will be optimized:

where the contrastive loss  is defined as follows:




\noindent\textbf{Self-supervised contrastive learning}
\label{method_pre_selfcontratsive}
We first introduce the most prevalent form of contrastive learning currently --- self-supervised contrastive learning for learning without labels. Since there are no annotated labels, we usually set  by considering each sample having only itself as positive:

Typically, we aim at maximizing the relations between different augmented views of the same sample while minimizing the relations between augmented views of different samples. This is also widely known as instance discrimination task. We denote such self-supervised contrastive loss as .

\noindent\textbf{Supervised contrastive learning}
\label{method_pre_supervisedcontrastivelearning}
We can easily transition from self-supervised contrastive learning to supervised contrastive learning, by simply changing the inter-sample relations  for \cref{eq:selfconrisk} and \cref{eq:conloss}. More specifically, as in \cite{nca, cns, supcon}, samples from the same semantic classes are considered positive pairs, 

in addition to the sample itself (and its augmented views) as in self-supervised contrastive learning. We denote the supervised contrastive loss as .

\begin{mdframed}[linecolor=lightgray!20,backgroundcolor=lightgray!20,everyline=true,linewidth=2pt,roundcorner=0pt,innertopmargin=0pt,
    innerbottommargin=0pt,
    innerrightmargin=0pt,
    innerleftmargin=0pt,
        leftmargin = 0pt,
        rightmargin = 0pt
        ]\textit{\textbf{Memory bank} For consistency with the supervised cross-entropy loss and for clarity, we present all the essential formulas above in terms of the entire dataset . However, due to computation and memory constraints, it is often unrealistic to contrast a specific sample  with the whole dataset.  In the actual implementation in this work, following MoCo\cite{moco}, we use a dynamic \textit{FIFO} memory bank  consisting of cached projections of  samples. For each sample , we generate two random augmented views as ~(query) and ~(key) with their corresponding projections denoted by  and , respectively. Then, we calculate the cosine similarity between  on the one hand, and  and each projection in the memory bank on the other: 

For more details, please refer to the MoCo paper~\cite{moco}.}
\end{mdframed}

\subsubsection{Baseline methods}
Clearly, with respect to the finer target question labels, each sample , is a trustworthy positive sample in relation to itself (i.e., they have the same fine-grained label). However, considering the samples with the same coarse label as positives may lead to under-clustering issues~(\cref{fig:tsne}) (they may have different fine-grained labels). 
Recent works attempt to address such under-clustering by combining an additional self-supervised contrastive loss with a specific supervised loss that utilizes coarse labels. Grafit~\cite{touvron2021grafit} combine it with the supervised contrastive loss and CoIns~\cite{xu2021coins} with the the supervised cross-entropy loss. 
In both cases, as a result, the self-supervised contrastive loss that considers each sample as a class by itself, aims to mitigate the tendency for under-clustering. 
Formally, given our definitions of the previously mentioned loss functions in \cref{method_pre} (,  and ) it turns out that the losses used by Grafit~\cite{touvron2021grafit} and CoIns~\cite{xu2021coins} can be expressed as follows: 

where  controls the relative weight of each loss.

\subsection{MaskCon: Masked Contrastive learning}\label{method_maskcon}
Instead of equally weighing all the samples within the same coarse classes, we aim to emphasize the samples with the same fine labels and reduce the importance of the others. To achieve this, we introduce a novel contrastive learning method, namely \textbf{Mask}ed \textbf{Con}trastive learning~(\textbf{MaskCon}), within the framework of contrastive learning that utilizes inter-sample relations directly.


More specifically, for sample , we estimate its inter-sample relations  to other samples utilizing the key view projection  and the whole dataset  excluding itself~(since it will always be considered as a trustworthy positive), as below:

where the similarity  is given by

Please note the use of the mask~() that excludes from the  that estimates inter-sample relationships, the samples  that have a different coarse label with the sample  (and sets their  to 0). While it is risky to consider all samples from the same coarse class as positive, we can confidently identify those samples that do not have the same coarse class as negative. This reduces the noise in .
Finally, we re-scale the  with its maximum

to make the closest neighbour as positive as the sample itself and arrive at:

Compared to , we thus reweight the samples of the same coarse label according to the similarities in the feature space.

We denote the masked contrastive loss as  and, similarly to Grafit and CoIns, we also consider a weighted loss as the final objective:



\paragraph{Relations to SOTA works}\label{method:relation_to_otherworks}
By adjusting  and , our method can obtain various existing SOTA methods as special cases. More specifically, by setting  to , our method degenerates to Grafit~\cite{touvron2021grafit}. Ignoring the mask, i.e., treating all samples as having the same coarse label,
our method can also generalize to existing SOTAs in self-supervised learning. For example, NNCLR~\cite{dwibedi2021nnclr} instead of considering only each sample itself as positive it considers as positive the nearest neighbors as well. Formally, by setting  and , MaskCon degenerates to NNCLR; ASCL~\cite{ascl} proposes to introduce extra positive samples by calculating soft inter-sample relations, adaptively weighted by its own normalized entropy. By setting , MaskCon degenerates to ASCL. 

\subsection{Theoretical justification of MaskCon}
The fundamental goal of contrastive learning is to identify  that suits the problem granularity and accurately represents the relationship between samples. Assuming that there exists an optimal hidden , we can define the expected population risk in terms of  as shown in~\cref{eq:populationrisk}. 

Moreover, let us define the empirical risk with respect to  as shown in~\cref{eq:optimalrisk}.


\begin{theorem}\label{theorem1}
The generalization error bounds of contrastive learning depends on the difference between the inter-sample relations  and the optimal hidden inter-sample relations , i.e.,

\end{theorem}
\begin{proof}
Let us assume  and is -Lipschitz continuous w.r.t . Let  and  be the covering numbers that correspond to the finite hypothesis space of  and .
We have:

with probability at least .
\end{proof}

Based on \cref{theorem1}, it can be concluded that our method performs better than Grafit with high probability, because an appropriate temperature  allows  to more accurately estimate the optimal ~(\cref{fig:intro_mechanism_toy}) in comparison to assigning equal weights to all coarse neighbours.

\section{Experiments}
In this section, extensive experiments are conducted to validate the findings and effectiveness of the proposed Masked Contrastive Learning (MaskCon) framework. The impact of hyperparameters is thoroughly investigated in \cref{exp:ablations}.
In~\cref{exp:cifartoy}, \cref{exp:cifar100} and \cref{exp:in1k}, results are provided for experiments on CIFAR datasets and ImageNet-1K dataset with coarse labels. In~\cref{exp:fine-grained}, we apply our method on more challenging fine-grained datasets, including Stanford Online Products~(SOP) dataset~\cite{sopdataset} and Stanford Cars 196 dataset~\cite{cars196}. 

To ensure the robustness of the proposed method and exclude the impact of model capacity, the model settings are kept consistent throughout all the experiments, except for the two hyperparameters,  and , which are ablated in \cref{exp:ablations}.
ResNet18~\cite{resnet} is employed in all experiments, with the initial convolutional layer modified to have a kernel size of 33 and a stride of 1, and the initial max-pooling operator removed for CIFAR experiments, to account for the smaller image size~(3232) as suggested in prior works \cite{moco}. More implementation and dataset details can be found in \cref{appendixa}.




We compare our method with two competing methods: \textbf{Grafit} and \textbf{CoIns}. For a fair comparison, we exhaust the weight  choices for both methods and report the best achievable results in all experiments. Note that when , Grafit and CoIns degenerate to self-supervised contrastive learning denoted as \textbf{SelfCon}; Conversely, when , Grafit degenerates to supervised contrastive learning~\cite{supcon} denoted as \textbf{SupCon}, while CoIns degenerates to conventional supervised cross-entropy learning denoted as \textbf{SupCE}. 
For reference, we also show the results when training with fine labels -- this is denoted as \textbf{SupFINE}. 
\paragraph{Evaluation protocol}
To evaluate the different methods on the test set with fine labels, we use the recall@K~\cite{oh2016recallretrieval} metric widely used in the image retrieval task. Each test image first retrieves top-K nearest neighbours from the test set and receives 1 if there exists at least one image from the same fine class among the top-K nearest neighbours, otherwise 0. Recall@K averages this score over all the test images.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/hyperparameters.png}
    \caption{Recall@1 w.r.t  and  on CIFAR100 dataset}
    \label{fig:hyperparameters}
\end{figure}

\begin{table*}[htbp]
\centering
\resizebox{0.82\linewidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{CIFARtoy-goodsplit}                           & \multicolumn{4}{c}{CIFARtoy-badsplit}                             \\ \cmidrule(l){2-5} \cmidrule(l){6-9}
                        & Recall@1       & Recall@2       & Recall@5       & Recall@10      & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SelfCon                 & 84.83          & 91.55          & 96.35          & 98.16          & 84.83          & 91.55          & 96.35          & 98.16          \\
Grafit                  & 86.61          & 92.33          & 97.01          & 98.38          & 89.96          & 94.36          & 97.61          & 98.10   \\
SupCon                  & 73.84          & 84.25          & 92.14          & 95.46          & 84.66          & 90.93          & 95.15          & 96.71          \\
CoIns                   & 86.15          & 92.76          & 97.21          & 98.46          & 90.55          & 94.94          & 97.73          & 98.71          \\
SupCE                   & 76.30          & 85.26          & 94.65          & 97.46          & 87.15          & 92.85          & 96.78          & 98.34          \\ \midrule
\textcolor{gray}{SupFINE}        & \textcolor{gray}{94.11}          & \textcolor{gray}{96.53}          & \textcolor{gray}{98.25}          & \textcolor{gray}{98.96}   & \textcolor{gray}{94.11}          & \textcolor{gray}{96.53}          & \textcolor{gray}{98.25}          & \textcolor{gray}{98.96}   \\ \midrule
MaskCon~(Ours)          & \textbf{90.28}~\footnotesize{(\textcolor{green}{13.98})} & \textbf{94.04} & \textbf{97.33} & \textbf{98.53} & \textbf{91.56}~\footnotesize{(\textcolor{green}{4.41})} & \textbf{95.23} & \textbf{97.70} & \textbf{98.70}          \\\bottomrule
\end{tabular}
}
\caption{Results on CIFARtoy dataset.}
\label{tab:cifartoy}
\end{table*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tsne.pdf}
    \caption{t-SNE visualization of learned representation on CIFARtoy dataset.}
    \label{fig:tsne}
\end{figure*}

\subsection{Effect of  and }\label{exp:ablations}
In this section, we extensively ablate the effect of hyperparameters of MaskCon -- weight  and temperature . Specifically, we show the recall@1 results on CIFAR100 dataset in \cref{fig:hyperparameters}, with  and . 





The results demonstrate a significant improvement in performance upon the inclusion of additional inter-sample relationships () in comparison to self-supervised learning alone () on the CIFAR dataset. Furthermore, a suitable temperature , such as 0.05 or 0.1, consistently yields superior results compared to the simple weighted combination in Grafit (). In general, we recommend initiating the hyperparameter search with  and , where  denotes the temperature for the projection head. Additional results concerning various hyperparameters on other datasets can be found in \cref{appdix b}.



\subsection{Experiments on CIFARtoy dataset} \label{exp:cifartoy}
To simulate the coarse labelling process, we manually generate toy datasets based on the CIFAR10 dataset. Following the problem motivation, a subset of 8 classes is selected from the original 10 classes. Specifically, the classes 'airplane', 'truck', 'automobile', and 'ship' are designated as Class A, while 'horse', 'dog', 'bird', and 'cat' are assigned to Class B. Note that Class A comprises non-organic objects, while Class B comprises animals. Moreover, a bad split of the aforementioned dataset is defined, wherein 'airplane', 'automobile', 'bird', and 'cat' are Class A, and 'horse', 'dog', 'ship', and 'truck' are Class B.


As shown in \cref{tab:cifartoy}, our method achieves the best performance in both splits. Moreover, the improvement over the supervised learning with coarse labels~(SupCE) on good split~(13.98\%) is much higher than the bad split~(4.41\%), which shows the substantial potential of MaskCon in dealing with more realistic coarse-labelled datasets~\footnote{
It is worth noting that the better performance on the bad split may seem counter-intuitive at first glance.
However, note that a bad split actually makes the labels more informative, as it explicitly helps to discriminate visually similar classes, e.g., 'cat' and 'dog' in different coarse classes in our setting.}.

In \cref{fig:tsne} we visualize the learned \textit{features} of all test samples in the good split. Clearly, the learned representations with supervised contrastive learning~(SupCon) and self-supervised contrastive learning~(SelfCon) tend to under-cluster and over-cluster the samples, respectively. By contrast, our method gets more compact and clear clusters, in line with the results when trained with fine labels~(SupFINE).


\subsection{Experiments on CIFAR100 dataset}\label{exp:cifar100}
The common CIFAR100 dataset has 20 classes of coarse labels in addition to the 100 classes of fine labels, with each coarse class containing five fine-grained classes~(500 samples). The results in \cref{tab:cifar100} show that our method achieves significant improvements over the SOTAs. In particular, it improves the top-1 retrieval precision from 47.25\% to 65.52\%, approaching the results by the model learned with fine labels~(71.13\%). 
\begin{table}[htbp]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Method         & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SelfCon        & 40.50          & 51.83          & 66.23          & 76.66          \\
Grafit         & 60.57          & 71.13          & 82.32          & 89.21          \\
SupCon         & 58.65          & 70.04          & 82.18          & 89.09          \\
CoIns          & 60.10          & 70.89          & 83.14          & \textbf{89.52} \\
SupCE          & 47.25          & 61.24          & 77.78          & 87.01          \\ \midrule
\textcolor{gray}{SupFINE}        & \textcolor{gray}{71.13}          & \textcolor{gray}{80.03}  & \textcolor{gray}{87.61}          & \textcolor{gray}{91.59}          \\ \midrule
MaskCon~(Ours) & \textbf{65.52}~\footnotesize{(\textcolor{green}{18.17})} & \textbf{74.46} & \textbf{83.64} & 89.25          \\ \bottomrule
\end{tabular}
}
\caption{Results on CIFAR100 dataset.}
\label{tab:cifar100}
\end{table}



\subsection{Experiments on ImageNet-1K dataset}\label{exp:in1k}
In this section, we evaluate our method on the large-scale ImageNet-1K dataset. For efficiency, we experiment with the downsampled version of ImageNet-1K dataset~\cite{chrabaszcz2017downsampled}, where each sample was resized to 3232. 
Since no official coarse labels exist for ImageNet, we introduce coarse labels based on the WordNet~\cite{miller1998wordnet} hierarchy so as to artificially group the whole dataset into 12 coarse classes: `0:~Invertebrate', `1:~Domestic animal', `2:~Bird', `3:~Mammal', `4:~Reptile/Aquatic vertebrate', `5:~Device', `6:~Vehicle', `7:~Container', `8:~Instrument', `9:~Artifact', `10:~Clothing' and `11:~Others'. In \cref{tab:imagenet}, we show the results on downsampled ImageNet-1K datasets. 

\begin{table}[htbp]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Method                    & Recall@1                & Recall@2                & Recall@5                & Recall@10               \\ \midrule
SelfCon                   & 10.28                   & 14.15                   & 22.36                   & 30.34                   \\
Grafit                    & 18.13                   & 25.46                   & 37.19                   & 46.64                   \\
SupCon                    & 13.36                   & 19.40                   & 29.77                   & 39.38                   \\
CoIns                     & 18.36                   & 25.54                   & 37.09                   & 46.89                   \\ 
SupCE                     & 12.23                   & 14.15                   & 27.76                   & 37.03                   \\
\midrule
\textcolor{gray}{SupFINE} & \textcolor{gray}{33.97} & \textcolor{gray}{44.55} & \textcolor{gray}{57.23} & \textcolor{gray}{65.77} \\ \midrule
MaskCon~(Ours)                   & \textbf{19.08}~\footnotesize{(\textcolor{green}{6.86})}          & \textbf{26.21}          & \textbf{38.17}          & \textbf{47.96}          \\ \bottomrule
\end{tabular}
}
\caption{Results on ImageNet-1K dataset.}
\label{tab:imagenet}
\end{table}

We can again validate that with temperature-controlled soft relations, our method surpasses SupCE, Grafit and CoIns consistently, especially for top-10 retrieval results.

\subsection{Experiments on fine-grained datasets}\label{exp:fine-grained}
In this section, we conduct experiments in a more challenging scenario -- fine-grained datasets with only coarse labels. Please note, that in this work, we do not aim to compare with the state-of-the-art works in fine-grained classification, which usually involve more specialized techniques, such as object localization and local feature extraction.

\subsubsection{Stanford Online Products~(SOP)}
The Stanford Online Products~(SOP) dataset~\cite{sopdataset} consists of 22,634 products, with each product having between two and twelve photos from different perspectives, for a total of 120,053 images. In addition, there are 12 coarse classes based on the semantic categories of the products, such as 'bicycle' and 'kettle'. For the common image retrieval task, the category of the test set is possibly unknown, so we firstly extract a subset with an unknown test category based on the SOP dataset. We then selected the categories with eight or more images. We then split almost equally this subset to obtain the training set and the test set with 25,368 and 25,278 images, consisting of 2,517 and 2,518 classes respectively, denoted as SOP-split1. Moreover, we select all products with twelve images, and then randomly select ten train images and two test images. Thus, we have 1,498 classes with a total of 17,976 images~(2,996 test images and 14,980 train images), denoted as SOP-split2. 

\begin{table}[htbp]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Method         & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SelfCon        & 70.36          & 75.57          & 81.53          & 85.13          \\
Grafit         & 74.02          & 78.82          & 84.13          & 87.91          \\
SupCon         & 53.69          & 59.55          & 67.12          & 72.78          \\
CoIns          & 70.84          & 76.01          & 82.2           & 86.08          \\
SupCE          & 36.35          & 42.39          & 50.30          & 56.52          \\ \midrule
\textcolor{gray}{SupFINE}        & \textcolor{gray}{83.94}          & \textcolor{gray}{88.04}          & \textcolor{gray}{91.95}          & \textcolor{gray}{94.00}          \\ \midrule
MaskCon~(Ours) & \textbf{74.05}~\footnotesize{(\textcolor{green}{37.7})} & \textbf{78.97} & \textbf{84.48} & \textbf{87.96} \\ \bottomrule
\end{tabular}
}
\caption{Results on SOP-split1 dataset.}
\label{tab:sopsplit1}
\end{table}



\begin{table}[htbp]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Method         & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SelfCon        & 35.85          & 41.46          & 49.77          & 56.11          \\
Grafit         & 39.12          & 44.66          & 53.10          & 59.65          \\
SupCon         & 25.07          & 29.24          & 35.85          & 41.59          \\
CoIns          & 38.22          & 45.19          & 54.37          & 61.18          \\
SupCE          & 22.56          & 26.34          & 33.28          & 38.95          \\ \midrule
\textcolor{gray}{SupFINE}        & \textcolor{gray}{69.56}          & \textcolor{gray}{75.70}          & \textcolor{gray}{83.24}          & \textcolor{gray}{87.88}          \\ \midrule
MaskCon~(Ours) & \textbf{45.36}~\footnotesize{(\textcolor{green}{22.8})} & \textbf{51.07} & \textbf{58.91} & \textbf{65.52} \\ \bottomrule
\end{tabular}
}
\caption{Results on SOP-split2 dataset.}
\label{tab:sopsplit2}
\end{table}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.86\textwidth]{figures/retrieval.pdf}
    \caption{Examples of Top-10 image retrieval results on Cars196 dataset.}
    \label{fig:cars_retrieval}
\end{figure*}


In \cref{tab:sopsplit1} and \cref{tab:sopsplit2} we show results on SOP-split1 and SOP-split2, respectively. Similarly, regardless of whether the test class is known or not, our method still achieves significant improvements over state-of-the-art methods. 
An interesting phenomenon here is, that on the SOP dataset, SelfCon alone is significantly better than its SupCE/SupCon counterpart. We conjecture that this is due to the fact that each fine class of the SOP dataset in fact consists of different views of the same product, and the number of images per fine class is quite small~(2 to 12). For such a dataset, the classification of fine classes is closer to instance discrimination rather than the coarse classes classification.


\subsubsection{Stanford Cars196}
Stanford Cars196 is another widely used fine-grained image classification benchmark. As there are no official annotated coarse labels, we manually group 196 car models into 8 coarse classes based on the common types of cars: `0:~Cab', `1:~Sedan', `2:~SUV', `3:~Convertible', `4:~Coupe', `5:~Hatchback', `6:~Wagon' and `7:~Van'.


In \cref{tab:cars196}, our method again has the best performance. In addition, unlike in the SOP datasets, SupCon and SupCE have an overwhelming advantage over SelfCon. In this dataset, this is not surprising because for each fine class, there are more images~(ranging from 24 images to 68 images) with different colours/backgrounds in the cars dataset, leading to a much higher intra-class variance. Some image retrieval examples can be found in ~\cref{fig:cars_retrieval}.
\begin{table}[htbp]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Method         & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SelfCon        & 20.97          & 28.75          & 41.44          & 52.61          \\
Grafit         & 42.30          & 54.79          & 71.1           & 81.74          \\
SupCon         & 42.30          & 54.79          & 71.1           & 81.74          \\
CoIns          & 42.77          & 55.60          & 72.29          & 82.53          \\
SupCE          & 42.77          & 55.60          & 72.29          & 82.53          \\ \midrule
\textcolor{gray}{SupFINE}        & \textcolor{gray}{78.09}          & \textcolor{gray}{82.97}          & \textcolor{gray}{86.51}          & \textcolor{gray}{88.04}          \\ \midrule
MaskCon~(Ours) & \textbf{45.53}~\footnotesize{(\textcolor{green}{2.76})} & \textbf{58.56} & \textbf{74.36} & \textbf{84.36} \\ \bottomrule
\end{tabular}
}
\caption{Results on Stanford Cars196 dataset.}
\label{tab:cars196}
\end{table}
\vspace{-5mm}


\subsubsection{Additional discussion}
Compared to the state-of-the-art, our method achieves the best results in all experiments. We would like to note here some of our insights from the results on the CIFAR and the fine-grained ones~(including ImageNet-1K). More specifically, we note that on the CIFAR datasets the performance of our method approaches that of the supervised one with fine labels~(SupFINE), while on the fine-grained datasets there is a larger gap.
We note that the critical question is to consider whether the instance discrimination task is useful for fine-grained classification in different scenarios, since coarse labelling usually does not lead to worse results.
Towards that, recent works have shown that the core idea of self-supervised contrastive learning --- augmentation invariance may be destructive for fine-grained tasks~\cite{xiao2020whatdoesnt_finegrained1, cole2022whendoes_finegrained2}. For example, the commonly applied colour distortion augmentation will promote the model to be non-sensitive to colour information. However, colour may be the key to discriminate between different breeds of birds. How to adapt the instance discrimination task to the fine-grained coarse-labelled dataset is a future direction we wish to pursue.






\section{Conclusion}
In this work, we propose a \textbf{Mask}ed \textbf{Con}trastive learning framework~(\textbf{MaskCon}) for learning fine-grained information with coarse-labelled datasets. On the basis of two baseline methods, we utilize coarse labels and the instance discrimination task to better estimate inter-sample relations. We show theoretically that our method can reduce the optimization error bound. Extensive experiments with various hyperparameter settings on multiple benchmarks, including the CIFAR datasets and the more challenging fine-grained classification datasets  show that our method achieves consistent and large improvement over the baselines. 

{\setlength{\parindent}{0.0cm}
\textbf{Acknowledgments:} This work was supported by the EU H2020 AI4Media No. 951911 project.
} 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\onecolumn
\appendix

\section{Implementation and dataset details}\label{appendixa}
\subsection{Implementation details}

\paragraph{Hyperparameter settings}
For datasets except ImageNet-1K, we use the SGD optimizer with a momentum of 0.9, and train for 200 epochs with a batch size of 128, a learning rate of 0.02 and cosine annealing lr scheduler, and weight decay as 0.0005. For ImageNet-1K dataset, we train for 100 epochs with a batch size of 256. For contrastive learning, we use a memory bank with 8,192 elements, an MLP projector with a hidden dimension of 512, an output dimension of 128, and a temperature for the projection head as . Moreover, we show the MaskCon-specific hyperparameters  and  corresponding to the reported numbers in the main paper below~(\cref{tab:hyperparameters}). This can be further validated in \cref{appendixb}.
\begin{table}[htbp]
\centering
\resizebox{0.4\linewidth}{!}{
\begin{tabular}{@{}l|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{SOP-split1} & \multicolumn{2}{c|}{SOP-split2} & \multicolumn{2}{c}{Cars196} \\ \cmidrule(l){2-7} 
                         &            &           &            &           &          &         \\ \midrule
Grafit                   & 0.8           & /               & 0.8           & /               & 1           & /             \\
CoIns                    & 0.5           & /               & 0.2           & /               & 1           & /             \\
Ours                     & 0.8           & 0.1             & 0.5           & 0.05            & 1           & 0.1           \\ \bottomrule
\end{tabular}}
\caption{Hyperparameters in the main paper.}
\label{tab:hyperparameters}
\end{table}

\paragraph{Augmentation strategies}
The augmentation strategy is critical for contrastive learning, especially for self-supervised contrastive learning. Following ~\cite{ascl, zheng2021ressl}, we apply strong and weak augmentations for the query and key view respectively. As discussed in the main paper, the strong augmentation may be destructive for fine-grained image classification. In particular, considering that each class in the SOP dataset consists of different views of the same product, we, therefore, use weak image augmentations for both query and key view. We list the applied data augmentations for each dataset as below in the \textsc{pytorch} format, the complete repo will be released upon acceptance.

\noindent\textit{\textbf{Strong augmentation}}
\begin{lstlisting}[language=Python]
if dataset == 'cars196':
    strong_transform = transforms.Compose([
    transforms.RandomResizedCrop(size, (0.2, 1)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomPerspective(0.5, 0.5),
    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),
    transforms.RandomGrayscale(p=0.2),
    transforms.RandomApply(
    [GaussianBlur([.1, 2.])], p=0.5),
    transforms.ToTensor(),
    normalize,
    ])
elif dataset == 'sop_split1' or dataset == 'sop_split2':
    strong_transform = transforms.Compose([
    transforms.RandomResizedCrop(size, (0.2, 1)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomPerspective(0.5, 0.5),
    transforms.ToTensor(),
    normalize,
    ])
else:
    strong_transform = transforms.Compose([
    transforms.RandomResizedCrop(size, (0.2, 1)),
    transforms.RandomHorizontalFlip(),
    CIFAR10Policy(),
    transforms.ToTensor(),
    normalize,
    ])
\end{lstlisting}
\noindent\textit{\textbf{Weak augmentation}}
\begin{lstlisting}[language=Python]
if dataset == 'cars196' or dataset == 'sop_split1' or dataset == 'sop_split2':
    weak_transform = transforms.Compose([
    transforms.RandomResizedCrop(size, (0.2, 1)),
    transforms.RandomPerspective(0.5, 0.5),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    normalize
    ])
else:
    weak_transform = transforms.Compose([
    transforms.RandomResizedCrop(size, (0.2, 1)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    normalize,
    ])
\end{lstlisting}


\noindent\textit{\textbf{Test augmentation}}
\begin{lstlisting}[language=Python]
test_transform = transforms.Compose([
    transforms.Resize(size),
    transforms.ToTensor(),
    normalize,
    ])
\end{lstlisting}




\subsection{Dataset details}
\paragraph{Stanford Online Products~(SOP)}
For Stanford Online Products~(SOP) datasets, we artificially generate two different splits. There are very limited images for each product in the original SOP datasets, ranging from 2 to 12~(\cref{fig:sop_statistics}). Considering an extreme case, that is, for each product there is only one view image, which will lead us to self-supervised learning based on instance discrimination. To avoid this, we thus select those classes with more images. More specifically, for SOP-split1, we select the classes with eight or more images and obtain a subset consisting of 5,035 classes. We evenly split~(2,517 train classes and 2,518 test classes) this subset to obtain the training set and the test set with 25,368 and 25,278 images, respectively. For SOP-split2, we select all products with twelve images, and then randomly select ten train images and two test images. Thus, we have 1,498 classes with a total of 17,976 images~(2,996 test images and 14,980 train images), respectively.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/num_sop.jpg}
    \caption{Statistics on SOP dataset.}
    \label{fig:sop_statistics}
\end{figure}




\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/coarse_mapping.jpg}
    \caption{Coarse labels for each of the fine classes---each image captioned as \{fine label\}/\{coarse label\}.}
    \label{fig:coarse_cars196}
\end{figure*}
\paragraph{Stanford Cars196}
For the Cars196 dataset, similarly, there are no official coarse labels. For this reason, we manually group 196 car models into 8 coarse classes based on the common types of cars: `0:~Cab', `1:~Sedan', `2:~SUV', `3:~Convertible', `4:~Coupe', `5:~Hatchback', `6:~Wagon' and `7:~Van'.
\begin{table}[htbp]
\centering
\begin{tabular}{@{}cccccccc@{}}
\toprule
0  & 1  & 2  & 3  & 4  & 5  & 6 & 7  \\ \midrule
18 & 50 & 36 & 30 & 29 & 14 & 7 & 12 \\ \bottomrule
\end{tabular}
\caption{Number of fine classes in each coarse class on Cars196.}
\label{tab:carssplit}
\end{table}
In \cref{fig:coarse_cars196}, we provide some example images from original fine classes to coarse classes for the Stanford Cars196 dataset. 



\paragraph{ImageNet-1K}
In this section, we evaluate our method on the large-scale ImageNet-1K dataset. For efficiency, we experiment with the downsampled version of ImageNet-1K dataset~\cite{chrabaszcz2017downsampled}, with each sample resized to 3232. 
\begin{table}[htbp]
\centering
\begin{tabular}{@{}cccccccccccc@{}}
\toprule
0  & 1   & 2  & 3  & 4  & 5   & 6  & 7  & 8   & 9  & 10 & 11 \\ \midrule
61 & 123 & 59 & 95 & 60 & 130 & 70 & 53 & 105 & 71 & 93 & 80 \\ \bottomrule
\end{tabular}
\caption{Number of fine classes in each coarse class.}
\label{tab:imagenetsplit}
\end{table}
Since there are no officially coarse labels for imagenet, we here introduce coarse labels based on the WordNet~\cite{miller1998wordnet} hierarchy so as to artificially group the whole dataset into 12 coarse classes: `0:~Invertebrate', `1:~Domestic animal', `2:~Bird', `3:~Mammal', `4:~Reptile/Aquatic vertebrate', `5:~Device', `6:~Vehicle', `7:~Container', `8:~Instrument', `9:~Artifact', `10:~Clothing' and `11:~Others'. 


\paragraph{Class imbalance}
We note that the number of fine classes in each coarse class is highly imbalanced~(\cref{tab:imagenetsplit}, \cref{tab:carssplit}). In this work, we do not explicitly deal with it~(for e.g., dataset resampling), as we believe, that such class imbalance can be common in the targeted problem setting, and we aim to test our method in such more realistic setting. 

\section{Additional experiments and results}\label{appendixb}

As mentioned in the main content, we perform a search for the hyperparameters for both Grafit and CoIns and report the best results. Here, in~\cref{tab:cifar20},~\cref{tab:sop-split1},~\cref{tab:sop-split2},~\cref{tab:cifartoy} and \cref{tab:cars196}, we report the results with so additional different hyperparameter settings.

Please note that we do not investigate fewer values for the hyperparameter  in comparison to Grafit and CoIns. From the tables we can see that with appropriate weights, both Grafit and CoIns get considerable improvements over the supervised and self-supervised only baselines. Our method, achieves even higher improvements with an appropriate .
\begin{table}[htbp]
\centering
\begin{tabular}{@{}l|c|c|cccc@{}}
\toprule
Method                  &  &  & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SupFINE                 & /   & /      & 71.13          & 80.03          & 87.61          & 91.59          \\ \midrule
\multirow{4}{*}{Grafit} & 0.2 & /      & 47.80          & 59.41          & 73.29          & 82.17          \\
                        & 0.5 & /      & 57.78          & 68.24          & 80.01          & 87.05          \\
                        & 0.8 & /      & \textbf{60.57} & \textbf{71.13} & \textbf{82.32} & \textbf{89.21} \\
                        & 1.0 & /      & 58.65          & 70.04          & 82.18          & 89.09          \\ \midrule
\multirow{4}{*}{CoIns}  & 0.2 & /      & 56.93          & 68.51          & 80.90          & 87.88          \\
                        & 0.5 & /      & \textbf{60.10} & \textbf{70.89} & \textbf{83.14} & \textbf{89.52} \\
                        & 0.8 & /      & 59.35          & 70.40          & 83.19          & 89.82          \\
                        & 1.0 & /      & 47.25          & 61.24          & 77.78          & 87.00          \\ \midrule
MaskCon                 & 0.5 & 0.01   & 54.22          & 65.31          & 77.45          & 85.12          \\
                        & 0.5 & 0.05   & 58.89          & 68.91          & 80.02          & 86.89          \\
                        & 0.5 & 0.1    & 60.18          & 70.41          & 81.81          & 88.25          \\
                        & 0.5 & 0.5    & 57.46          & 68.27          & 79.86          & 87.05          \\
                        & 1   & 0.01   & 61.41          & 71.22          & 81.44          & 87.70          \\
                        & 1   & 0.05   & \textbf{65.52} & \textbf{74.46} & \textbf{83.64} & \textbf{89.25} \\
                        & 1   & 0.1    & 62.52          & 72.51          & 83.27          & 89.18          \\
                        & 1   & 0.5    & 59.76          & 70.28          & 81.97          & 88.58          \\ \midrule
SelfCon                 & /   & /      & 40.50          & 51.83          & 66.23          & 76.66          \\ \bottomrule
\end{tabular}
\caption{Extra results on CIFAR100 dataset.}
\label{tab:cifar20}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{@{}l|c|c|cccc@{}}
\toprule
Method                  &  &  & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SupFINE                 & /   & /      & 83.94          & 88.04          & 91.95          & 94.00          \\ \midrule
\multirow{4}{*}{Grafit} & 0.2 & /      & 71.04          & 76.17          & 81.85          & 85.51          \\
                        & 0.5 & /      & 72.66          & 77.43          & 82.89          & 86.40          \\
                        & 0.8 & /      & \textbf{74.02} & \textbf{78.82} & \textbf{84.13} & \textbf{87.91} \\
                        & 1.0 & /      & 53.69          & 59.55          & 67.12          & 72.78          \\ \midrule
\multirow{4}{*}{CoIns}  & 0.2 & /      & 70.75          & 76.32          & 82.24          & 86.16          \\
                        & 0.5 & /      & \textbf{70.84} & \textbf{76.01} & \textbf{82.20} & \textbf{86.08} \\
                        & 0.8 & /      & 67.68          & 73.26          & 80.04          & 84.18          \\
                        & 1.0 & /      & 36.35          & 42.39          & 50.30          & 56.52          \\ \midrule
MaskCon                 & 0.2 & 0.1    & 72.35          & 77.34          & 82.92          & 86.46          \\
                        & 0.5 & 0.05   & 70.87          & 76.17          & 82.11          & 85.92          \\
                        & 0.5 & 0.1    & 73.22          & 78.34          & 83.98          & 87.35          \\
                        & 0.8 & 0.05   & 68.53          & 73.72          & 79.86          & 84.22          \\
                        & 0.8 & 0.1    & \textbf{74.05} & \textbf{78.97} & \textbf{84.48} & \textbf{87.96} \\ \midrule
SelfCon                 & /   & /      & 70.36          & 75.57          & 81.53          & 85.13          \\ \bottomrule
\end{tabular}
\caption{Extra results on SOP-split1 dataset.}
\label{tab:sop-split1}
\end{table}


\begin{table*}[htbp]
\centering
\begin{tabular}{@{}l|c|c|cccc|cccc@{}}
\toprule
\multirow{2}{*}{Method}  & \multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{4}{c|}{CIFARtoy~(good split)}                        & \multicolumn{4}{c}{CIFARtoy~(bad split)}                          \\ \cmidrule(l){4-11} 
                         &                      &                         & Recall@1       & Recall@2       & Recall@5       & Recall@10      & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SupFINE                  & /                    & /                       & 94.14          & 96.61          & 98.03          & 98.65          & 94.11          & 96.53          & 98.45          & 98.96          \\ \midrule
\multirow{4}{*}{Grafit}  & 0.2                  & /                       & 85.34          & 91.38          & 96.05          & 97.85          & 86.33          & 92.31          & 96.63          & 98.18          \\
                         & 0.5                  & /                       & \textbf{86.61} & \textbf{92.33} & \textbf{97.01} & \textbf{98.38} & 87.94          & 93.63          & 97.59          & 98.76          \\
                         & 0.8                  & /                       & 86.23          & 92.56          & 96.69          & 98.50          & \textbf{89.96} & \textbf{94.36} & \textbf{97.71} & \textbf{98.90} \\
                         & 1.0                  & /                       & 73.84          & 84.25          & 92.14          & 95.46          & 84.66          & 90.93          & 95.15          & 96.71          \\ \midrule
\multirow{4}{*}{CoIns}   & 0.2                  & /                       & 85.74          & 91.88          & 96.41          & 98.60          & 89.80          & 94.13          & 97.18          & 98.30          \\
                         & 0.5                  & /                       & 86.00          & 92.25          & 96.91          & 98.60          & 90.14          & 94.11          & 97.78          & 98.70          \\
                         & 0.8                  & /                       & \textbf{86.15} & \textbf{92.76} & \textbf{97.21} & \textbf{98.76} & \textbf{90.55} & \textbf{94.94} & \textbf{97.73} & \textbf{98.71} \\
                         & 1.0                  & /                       & 76.30          & 8.26           & 94.65          & 97.46          & 87.15          & 92.85          & 96.78          & 98.34          \\ \midrule
\multirow{8}{*}{MaskCon} & 0.5                  & 0.01                    & 87.44          & 93.01          & 96.95          & 98.38          & 88.89          & 93.84          & 97.20          & 98.38          \\
                         & 0.5                  & 0.05                    & 88.09          & 93.18          & 96.99          & 98.51          & 90.21          & 94.41          & 97.41          & 98.56          \\
                         & 0.5                  & 0.1                     & 87.13          & 92.95          & 96.93          & 98.44          & 88.29          & 93.61          & 97.25          & 98.54          \\
                         & 0.5                  & 0.5                     & 86.31          & 92.49          & 96.74          & 98.53          & 88.46          & 93.43          & 97.63          & 98.70          \\
                         & 1                    & 0.01                    & 88.24          & 93.44          & 97.01          & 98.26          & 90.53          & 94.23          & 97.51          & 98.55          \\
                         & 1                    & 0.05                    & \textbf{90.28} & \textbf{94.04} & \textbf{97.33} & \textbf{98.53} & \textbf{91.56} & \textbf{95.23} & \textbf{97.70} & \textbf{98.70} \\
                         & 1                    & 0.1                     & 78.25          & 87.10          & 94.26          & 96.85          & 88.44          & 93.65          & 97.10          & 98.34          \\
                         & 1                    & 0.5                     & 79.58          & 88.10          & 94.58          & 96.99          & 88.25          & 93.33          & 96.99          & 98.19          \\ \midrule
SelfCon                  & /                    & /                       & 84.84          & 91.73          & 96.14          & 97.94          & 84.83          & 91.55          & 96.35          & 98.16          \\ \bottomrule
\end{tabular}
\caption{Extra results on CIFARtoy dataset.}
\label{tab:cifartoy}
\end{table*}




\begin{table}[htbp]
\centering
\begin{tabular}{@{}l|c|c|cccc@{}}
\toprule
Method                  &  &  & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SupFINE                 & /   & /      & 69.56          & 75.70          & 83.24          & 87.88          \\ \midrule
\multirow{4}{*}{Grafit} & 0.2 & /      & 36.85          & 42.52          & 50.23          & 57.04          \\
                        & 0.5 & /      & 37.98          & 43.69          & 51.67          & 57.98          \\
                        & 0.8 & /      & \textbf{39.12} & \textbf{44.66} & \textbf{53.10} & \textbf{59.65} \\
                        & 1.0 & /      & 25.07          & 29.24          & 35.85          & 41.59          \\ \midrule
\multirow{4}{*}{CoIns}  & 0.2 & /      & \textbf{38.22} & \textbf{45.19} & \textbf{54.37} & \textbf{61.18} \\
                        & 0.5 & /      & 38.18          & 45.06          & 54.17          & 61.28          \\
                        & 0.8 & /      & 36.72          & 42.49          & 51.23          & 58.61          \\
                        & 1.0 & /      & 22.56          & 26.34          & 33.28          & 38.95          \\ \midrule
MaskCon                 & 0.2 & 0.05   & 39.69          & 45.69          & 53.70          & 60.05          \\
                        & 0.5 & 0.05   & \textbf{45.36} & \textbf{51.07} & \textbf{58.91} & \textbf{65.52} \\
                        & 0.8 & 0.05   & 43.84          & 50.03          & 58.18          & 64.19          \\
                        & 1.0 & 0.05   & 41.05          & 46.70          & 54.74          & 60.98          \\ \midrule
SelfCon                 & /   & /      & 35.85          & 41.46          & 49.77          & 56.11          \\ \bottomrule
\end{tabular}
\caption{Extra results on SOP-split2 dataset.}
\label{tab:sop-split2}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{@{}l|c|c|cccc@{}}
\toprule
Method                  &  &  & Recall@1       & Recall@2       & Recall@5       & Recall@10      \\ \midrule
SupFINE                 & /   & /      & 78.09          & 82.97          & 86.51          & 88.04          \\ \midrule
\multirow{4}{*}{Grafit} & 0.2 & /      & 23.64          & 32.69          & 46.35          & 57.98          \\
                        & 0.5 & /      & 28.83          & 38.96          & 53.99          & 65.08          \\
                        & 0.8 & /      & 35.58          & 47.97          & 64.47          & 75.28          \\
                        & 1.0 & /      & \textbf{42.30} & \textbf{54.79} & \textbf{71.10} & \textbf{81.74} \\ \midrule
\multirow{4}{*}{CoIns}  & 0.2 & /      & 32.30          & 43.19          & 59.11          & 71.10          \\
                        & 0.5 & /      & 36.71          & 48.80          & 65.33          & 76.16          \\
                        & 0.8 & /      & 38.71          & 49.04          & 67.28          & 79.23          \\
                        & 1.0 & /      & \textbf{42.77} & \textbf{55.60} & \textbf{72.29} & \textbf{82.53} \\ \midrule
MaskCon                 & 1   & 0.01   & 27.87          & 38.19          & 52.80          & 64.13          \\
                        & 1   & 0.05   & 33.04          & 43.69          & 59.10          & 70.59          \\
                        & 1   & 0.1    & \textbf{45.53} & \textbf{58.56} & \textbf{74.36} & \textbf{84.36} \\
                        & 1   & 0.5    & 42.84          & 55.23          & 71.57          & 81.72          \\ \midrule
SelfCon                 & /   & /      & 20.97          & 28.75          & 41.44          & 52.61          \\ \bottomrule
\end{tabular}
\caption{Extra results on Stanford Cars196 dataset.}
\label{tab:cars196}
\end{table}






\end{document}