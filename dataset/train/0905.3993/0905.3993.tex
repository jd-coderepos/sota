\documentclass{sig-alternate}

\usepackage{amssymb,amsfonts,amsmath,xspace}
\usepackage[naturalnames]{hyperref} 
\usepackage{algorithmic} 
\usepackage[english, algosection, algoruled, noline]{algorithm2e}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary} 
\newtheorem{definition}[theorem]{Definition}\newtheorem{dxample}[theorem]{Example}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\def\newblock{\hskip .11em plus .33em minus .07em}
\newenvironment{bc}{\noindent{\bf Bivariate case:}}{\hfill\medskip}
\newenvironment{oneshot}[1]{\noindent\begin{trivlist}\item[\hskip\labelsep{\sc#1}]}{\hfill\smallskip\end{trivlist}\vspace*{-.6cm}}
\newenvironment{cs}{\noindent{\bf Case Study:}}{\hfill\medskip}
\def\bm{\boldsymbol}
\def\axel{{\sc Axel}}
\def\mathemagix{{\sc Mathemagix}}
\newcommand{\ds}{\displaystyle}
\newcommand{\dott}{{..}}
\newcommand{\hide}[1]{{}}
\def\Maple{{\sc Maple}}  \def\Axel{{\sc Axel}}
\def\tP{{\tilde P}}
\def\CC{{\mathbb C}} \def\NN{{\mathbb N}} \def\PP{{\mathbb P}}
\def\QQ{{\mathbb Q}} \def\RR{{\mathbb R}} \def\ZZ{{\mathbb Z}}
\def\KK{{\mathbb K}}

\def\QQ{\mathbb{Q}}
\def\kk{\mathbb{K}}
\def\RR{\mathbb{R}}
\def\Bc{\mathcal{B}}
\def\Cc{\mathcal{C}}
\def\Oc{\mathcal{O}}
\def\Pc{\mathcal{P}}
\def\Sc{\mathcal{S}}
\def\Lc{\mathcal{L}}
\def\R{\mathbf{R}}

\def\Cf{\mathfrak{C}}
\def\xb{\mathbf{x}}

\def\sign{\mathrm{sign}}
\def\flower{\mathrm{low}}
\def\fupper{\mathrm{upp}}
\def\Input{\textsc{Input}}
\def\Output{\textsc{Output}}
\newcommand {\vb} {\textbf{b}}
\newcommand {\vf} {\textbf{f}}
\newcommand {\vx} {\textbf{x}}
\newcommand {\vd} {\textbf{d}}
\newcommand {\vt} {\textbf{t}}
\newcommand {\vi} {\textbf{i}}
\newcommand {\vj} {\textbf{j}}
\newcommand {\vk} {\textbf{k}}
\newcommand {\ve} {\textbf{e}}
\newcommand {\vv} {\textbf{v}}
\newcommand {\dK} {\mathds{K}}
\newcommand {\dR} {\mathds{R}}

\newcommand {\interior}[1] {\stackrel{\circ}{#1}}
\newcommand {\pd}[2] {{\partial_{#2}{#1}}} \newcommand {\grad}[1] {\bigtriangledown {#1}}
\newcommand {\crprod} {\wedge}
\newcommand {\transpose} [1] {{#1}^{\top}}
\newcommand {\Jac}{\mathrm{Jac}}


\newcommand{\OO}{\ensuremath{\mathcal{O}}\xspace}
\newcommand{\sO}{\ensuremath{\widetilde{\mathcal{O}}}\xspace}
\newcommand{\OB}{\ensuremath{\mathcal{O}_B}\xspace}
\newcommand{\sOB}{\ensuremath{\widetilde{\mathcal{O}}_B}\xspace}
\newcommand{\oo}{\ensuremath{{\scriptscriptstyle\OO}}\xspace}

\newcommand{\res}{\ensuremath{\mathsf{Res}}}
\newcommand{\lc}{\ensuremath{\mathsf{lc}_t}}

\newcommand{\uvec}[1]{\underline{#1}}
\newcommand{\bvec}[1]{\ensuremath{\mathbf{#1}}}


\newcommand{\dg}{\ensuremath{\mathsf{deg}}\xspace}
\newcommand{\bitsize}[1]{\ensuremath{\mathcal{L}\left( #1 \right)}\xspace}
\newcommand{\Multiply}[1]{\ensuremath{\mathsf{M}\left( #1 \right)}\xspace}


\pdfpagewidth=8.5in
\pdfpageheight=11in

\title{
Continued Fraction Expansion of Real Roots of Polynomial Systems
}

\numberofauthors{1}

\author{
\alignauthor
Angelos Mantzaflaris, Bernard Mourrain, Elias Tsigaridas\\
\affaddr{GALAAD, INRIA Sophia Antipolis}\\
\email{[FirstName.LastName]@sophia.inria.fr}
}


\begin{document}

\conferenceinfo{SNC'09,} {August 3--5, 2009, Kyoto, Japan.} 
\CopyrightYear{2009}
\crdata{978-1-60558-664-9/09/08} 

\maketitle

\begin{abstract}
  We present a new algorithm for isolating the real roots of a system of
  multivariate polynomials, given in the monomial basis. It is inspired by
  existing subdivision methods in the Bernstein basis; it can be seen as
  generalization of the univariate continued fraction algorithm or
  alternatively as a fully analog of Bernstein subdivision in the monomial basis.
  The representation of the subdivided domains is done through homographies,
  which allows us to use only integer arithmetic and to treat efficiently
  unbounded regions.  We use univariate bounding functions, projection and
  preconditionning techniques to reduce the domain of search. The resulting
  boxes have optimized rational coordinates, corresponding to the first terms
  of the continued fraction expansion of the real roots.  An extension of
  Vincent's theorem to multivariate polynomials is proved and used for the
  termination of the algorithm. New complexity bounds are provided for a
  simplified version of the algorithm. Examples computed with 
  a preliminary C++ implementation illustrate the approach.
\end{abstract}

\vspace{-.3cm}
\category{I.1.2}{Computing Methodologies}{Symbolic and Algebraic
  Manipulation}[algebraic algorithms]
\vspace{-.3cm}
\terms{Algorithms, Theory}
\vspace{-.3cm}
\keywords{subdivision algorithm, homography, tensor monomial basis, continued fractions, C++ implementation}




\section{Introduction} \label{intro}


The problem of computing roots of univariate polynomials has a long
mathematical history \cite{Pan97rev}.  Recently, some new
investigations focused on subdivision methods, where root localization
is based on simple tests such as \emph{Descartes' Rule of Signs} and
its variant in the Bernstein basis \cite{MOURRAIN:2005, emt-lncs-2006,
  MR2289103}.  Complexity analysis was developed for univariate
integer polynomial taking into account the bitsize of the
coefficients, and providing a good understanding of their behavior
from a theoretical and practical point of view.  Approximation and
bounding techniques have been developed \cite{jue:07e} to improve the
local speed of convergence to the roots.

Even more recently a new attention has been given to continued
fraction algorithms (CF), see
e.g. \cite{sharma-tcs-2008,et-tcs-2007} and references
therein. They differ from previous subdivision-based algorithms in
that instead of bisecting a given initial interval and thus producing
a binary expansion of the real roots, they compute continued fraction
expansions of these roots. The algorithm relies heavily on
computations of lower bounds of the positive real roots, and different
ways of computing such bounds lead to different variants of the
algorithm.  The best known worst-case complexity of CF is  \cite{sharma-tcs-2008}, while its average complexity is
, thus being the only complexity result that matches,
even in the average the complexity bounds of numerical algorithms
\cite{Pan02jsc}.  Moreover, the algorithm seems to be the most
efficient in practice \cite{ACS-TR-363602-02,et-tcs-2007}.


Subdivision methods for the approximation of isolated roots of
multivariate systems are also investigated but their analysis is much
less advanced.  In \cite{sp-csnps-93}, the authors used tensor product
representation in Bernstein basis and domain reduction techniques
based on the convex hull property to speed up the convergence and
reduce the number of subdivisions. In \cite{ELBER:2001}, the emphasis
is put on the subdivision process, and stopping criterion based on the
normal cone to the surface patch.  In \cite{mp:smspe-05}, this
approach has been improved by introducing pre-conditioning and
univariate-solver steps. The complexity of the method is also analyzed
in terms of intrinsic differential invariants.

This work is in the spirit of \cite{mp:smspe-05}.  The novelty of our
approach is the presentation of a tensor-monomial basis algorithm that
generalizes the univariate continued fraction algorithm and does not
assume generic position. We apply a subdivision approach also
exploiting certain properties of the Bernstein polynomial
representation, even though no basis conversion takes place.



Our contributions are as follows.
We propose a new adaptive algorithm for polynomial system real solving
that acts in monomial basis, and exploits the continued fraction
expansion of (the coordinates of) the real roots.  This yields the
best rational approximation of the real roots.  All computations are
performed with integers, thus this is a division-free algorithm.
We propose a first step towards the generalization of Vincent's
theorem to the multivariate case (Th.~\ref{vincentxyz})
We perform a (bit) complexity analysis of the algorithm, when oracles
for lower bounds and counting the real roots are available
(Prop.~\ref{prop:mcf-complexity}) and we propose non-trivial
improvements for reducing the total complexity even more
(Sec.~\ref{sec:complexity-improvements}).  In all cases the bounds
that we derive for the multivariate case, match the best known ones
for the univariate case, if we restrict ourselves to .


\subsection{Notation}

For a polynomial  ,  denotes its
total degree, while  denotes its degree w.r.t.~.
Let  with , . 
If not specified, we denote .

We are interested in isolating the real roots of a system of polynomials 
, in a
box , . 
We denote by  the solution
set in   of the equation , where  is  or .

In what follows \OB, resp. \OO, means bit, resp. arithmetic,
complexity and the \sOB, resp. \sO, notation means that we are
ignoring logarithmic factors.
For ,  is the maximum bit size of the
numerator and the denominator.  For a polynomial  , we denote by \bitsize{f} the maximum of the bitsize of
its coefficients (including a bit for the sign).  In the following, we
will consider classes of polynomials such that .



Also, to simplify the notation we introduce multi-indices, for the
variable vector , , the sum , and .
The tensor Bernstein basis polynomials of multidegree degree 
of a box  are denoted   where
.


\subsection{The general scheme}   

In this section, we describe the family of algorithms that we consider.
The main ingredients are 
\begin{itemize}
 \item  a suitable representation of the equations in a given (usually
rectangular) domain, for instance a representation in the Bernstein
basis or in the monomial basis;
 \item an algorithm to split the
representation into smaller sub-domains;
 \item a reduction procedure to
shrink the domain.
\end{itemize}
Different choices for each of these ingredients lead to algorithms
with different practical behaviors. The general process is summarized
in Alg.~\ref{algo:subdivision}.

\begin{algorithm} \label{algo:subdivision}
\caption{Subdivision scheme}
\KwIn{A set of equations  represented over a domain .} 
\KwOut{A list of disjoint domains, each containing one and only one real root of .}

Initialize a stack  and add  on top of it\;

While  is not empty do 
\begin{enumerate}
\item[a)] Pop a system  and:
\item[b)] Perform a precondition process and/or a reduction process to
  refine the domain.
\item[c)] Apply an exclusion test to identify if the domain contains
  no roots.
\item[d)] Apply an inclusion test to identify if the domain contains a
  single root. In this case output .
\item[e)] If both tests fail split the representation into a number of
  sub-domains and push them to .
\end{enumerate}
\end{algorithm}
The instance of this general scheme that we obtain generalizes the
continued fraction method for univariate polynomials; the realization of the
main steps (b-e) can be summarized as follows: 
\begin{enumerate}
\item[b)] Perform a precondition process and compute a lower bound on
  the roots of the system, in order to reduce the domain.
\item[c)] Apply interval analysis or sign inspection to identify if some
   has constant sign in the domain, i.e. if the domain contains
  no roots.
\item[d)] Apply Miranda test to identify if the domain contains a
  single root. In this case output .
\item[e)] If both tests fail, split the representation at 
  and continue.
\end{enumerate}

In the following sections, we are going to describe more precisely the
specific steps and analyze their complexity.
In Sec.~\ref{homography}, we describe the representation of domains via
homographies and the connection with the Bernstein basis representation.
Subdivision, based on shifts of univariate polynomials, reduction and
preconditionning are analyzed in Sec.~\ref{subdivision-reduction}.
Exclusion and inclusion tests as well as a generalization of Vincent's
theorem to multivariate polynomials, are presented in
Sec.~\ref{criteria}.
In Sec. \ref{sec:complexity}, we recall the main properties of
Continued Fraction expansion of real numbers and use them to analyze
the complexity of a subdivision algorithm following this generic
scheme.
We conclude with examples produced by our C++ implementation in
Sect.~\ref{sec:impl}.

\if 0
\begin{bc}
The algorithm in dimension two:\\
Input:  and  defined over a box . \\
Output: approximations of  the real roots of . 
\begin{enumerate}
 \item Initialize a stack  and add  on top of it.
 \item While  is not empty repeat 3-4:

\item Check for common solutions:
\begin{enumerate}
 \item Pop a pair  and compute lower bounds  on the common solutions in .
 \item Translate the polynomials by .
 \item Repeat (a)-(b) until for some pair  it is .
 \item If no roots and for some  we have  or  ELSE SUBDIVIDE.
\end{enumerate}
\item Subdivision step:\\
Subdivide  into , ,  and :\\
In step  of subdivision: for every sub-domain apply the transformation  to  and  to obtain 
\end{enumerate}
\end{bc}
\fi 

\section{Representation: Homographies} \label{homography}


A widely used representation of a polynomial  over a rectangular
domain is the tensor-Bernstein representation. De Casteljau's
algorithm provides an efficient way to split this representation to
smaller domains. A disadvantage is that converting integer polynomials
to Bernstein form results in rational or, if one uses machine numbers,
approximate Bernstein coefficients. We follow an alternative approach
that does not require basis conversion since it applies to monomial
basis: We introduce a tensor-monomial representation, i.e. a
representation in the monomial basis over  and provide an algorithm to subdivide this representation
analogously to the Bernstein case.

\if 0
An important fact of Bernstein basis is the quadratic convergence property(cf.~\cite[Theorem~2.1]{reif-2000}):

\begin{proposition} \label{quad_convergence}
Consider the piecewise linear function  defined by the Bernstein coefficients of  in ; then for all :

where .  \end{proposition}
\fi

In a tensor-monomial representation a polynomial is represented as a
tensor (higher dimensional matrix) of coefficients in the natural
monomial basis, that is,

for every equation  of the system.  Splitting this
representation is done using homographies. The main operation in this
computation is the Taylor shift.

\begin{definition}
  A homography (or Mobius transformation) is a bijective projective
  transformation ,
  defined over  as
 
with , , . 
\end{definition}
Using simple calculations, we can see that the inverse

is also a homography, hence the set of homographies is a group under
composition.  Also, notice that if  then, taking
proper limits when needed, we can write

hence , 

is a polynomial defined
over  that corresponds to the (possibly unbounded) box 

of the initial system, in the sense that the zeros of the initial
system in  are in one-to-one correspondence with the positive
zeros of .

We focus on the computation of . 
We use the basic homographies 
(translation by ) or simply  if ,  (contraction by ) and  (reciprocal polynomial).
These notations are naturally extended to variable vectors; for instance
, .
Complexity results for these computations appear in the following sections.
We can see that they suffice to compute any homography:

\if 0
Now we set  so that .
Notice that the inverse transformation  which is applied here to the domain maps the interval
 to ,
whereas  takes  to .
\fi

\begin{lemma}
The group of homographies with coefficients in  is generated by , , .
\end{lemma}
\begin{proof}
It can be verified that a  with arbitrary coefficients  is constructed as

where the product denotes composition.  We abbreviate 
 
and 
, ,
e.g.  and .


\end{proof}

Representation via homography is in an interesting correspondence to
the Bernstein representation:

\begin{lemma} \label{lem:bernsteincoefs}
Let  the Bernstein expansion of  in the box  yielded by a homography . If 

then

\end{lemma}
\begin{proof}
Let . For a tensor-Bernstein polynomial  we compute

as needed.
\end{proof}

\begin{corollary}\label{cor:bernsteincoefs}
The Bernstein expansion of  in  is

That is, the coefficients of  coincide with the Bernstein
coefficients up to contraction and binomial factors.
\end{corollary}

Thus tensor-Bernstein coefficients and tensor-mo\-no\-mial
coefficients in a sub-domain of  differ only by
multiplication by positive constant. In particular they are of the
same sign. Hence this corollary allows us to take advantage of sign
properties (eg. the variation diminishing property) of the Bernstein
basis without computing it.



The resulting representation of the system consists of  the transformed
polynomials , represented as tensors of coefficients as
well as  integers,  for 
from which we can recover the endpoints of the domain, using~(\ref{Hbox}).


\section{Subdivision and reduction} \label{subdivision-reduction}

\subsection{The subdivision step}

We describe the subdivision step using the homography representation. This is
done at a point . It
consists in computing up to  new sub-domains (depending on the
number of nonzero 's), each one having  as a vertex.

Given  that represent the initial system at some
domain, we consider the partition of  defined by the
hyperplanes , . These intersect at 
hence we call this \emph{partition at }. Subdividing at  is equivalent to subdividing the
initial domain into boxes that share the common vertex  and have faces either parallel or perpendicular to those of
the initial domain.

We need to compute a homography representation for every domain in
this partition. The computation is done coordinate wise; observe that
for any domain in this partition we have, for all , either
 or . It suffices to apply a
transformation that takes these domains to . In the former case,
we apply  to the current polynomials and in the latter
case we shift them by , i.e. we apply . The integers
 that keep track of the current
domain can be easily updated to correspond to the new subdomain.

We can make this process explicit in general dimension: every computed subdomain
corresponds to a binary number of length , where the th bit is
 if  is applied or  if  is applied. 

In our continued fraction algorithm the subdivision is performed at .

\noindent{}{\bf Illustration.} Let us illustrate this process in dimension
two. The system  is defined over . We subdivide this
domain into , ,  and
. Equivalently, we compute four new pairs of
polynomials, as illustrated in Fig.~\ref{fig:subdiv} (we abbreviate
).
\begin{figure}[h]
  \centering
  \includegraphics[scale=.6]{figs/subdiv}
  \caption{Subdividing the domain of .}
  \label{fig:subdiv}
\end{figure}




\noindent{}{\bf Complexity of subdivision step.}
The transformation of a polynomial into two sub-domains,
i.e. splitting w.r.t. one direction, consists of performing 
univariate shifts, one for every coefficient  of
.

If the subdivision is performed in every direction, each
transformation consists of  univariate shifts for every
variable, i.e.  shifts. There are  sub-domains to
compute, hence a total of  shifts have to be
performed in a single subdivision step. We must also take into account
that every time a univariate shift is performed, the coefficient
bitsize increases.

The operations 

are essentially of the same complexity, except that the second
requires one to exchange the coefficient of 
with  before translation, i.e. an
additional  cost. Hence we only need to consider the case of
shifts for the complexity.

The continued fraction algorithm subdivides a domain using unit shifts
and inversion.  Successive operations of this kind increase the bitsize
equivalently to a big shift by the sum of these units. Thus it
suffices to consider the general computation of 
to estimate the complexity of the subdivision step.

\begin{lemma}[{Shift complexity}] \label{shiftComplexity} The
  computation of  with  and
   can be performed in .
\end{lemma}
\begin{proof}
  We use known facts for the computation of  for
  univariate polynomials.  If  and  is
  univariate, this operation is performed in
  ; the resulting coefficients are of
  bitsize  \cite{gg-shift-1997}. 
  Hence  is
  computed in .

  Suppose we have computed 
  for some . The coefficients are of bitsize . The computation of shift w.r.t. th
  variable  results in a
  polynomial of bitsize  and
  consists of 
  operations. That is, we perform  univariate polynomial
  shifts, one for every coefficient of  in
  .

  This gives a total cost for computing  of

The latter sum implies that it is faster to apply the shifts with
increasing order, starting with the smallest number . Since
 for all , and we must shift a system of
 polynomials we obtain the stated result.
\end{proof}

Let us present an alternative way to compute a sub-domain using
contraction, preferable when the bitsize of  is big.  The
idea behind this is the fact that  and  compute the same
sub-domain, in two different ways. 

\begin{lemma} \label{contractComplexity} If , ,
  then the coefficients of , , can be computed in  .
\end{lemma}
\begin{proof}
  The operation, i.e. computing the new coefficients  can be done with  multiplications: Since
  , 
  if these powers are computed
  successively then every coefficient is computed using two
  multiplications. Moreover, it suffices to keep in memory the 
  powers ,
   in order to compute any . Geometrically this can be understood as a stencil of 
  points that sweeps the coefficient tensor and updates every element
  using one neighbor at each time. The bitsize of the multiplied
  numbers is  hence the result follows.
\end{proof}

Now if we consider a contraction followed by a shift by 
w.r.t.  for  polynomials we obtain  operations for the computation of the
domain. The disadvantage is that the resulting coefficients are of
bitsize  instead of  with the
use of shifts. Also note that this operation would compute a expansion
of the real root which differs from  continued fraction expansion.



\subsection{Reduction: Bounds on the range of {}}  \label{subsec:reduction}

In this section we define univariate polynomials whose graph in
 bounds the graph of . For every direction , we provide
two polynomials bounding the values of  in  from
below and above respectively.

Define


\begin{lemma} \label{u_bounds}
  For any ,  and any , we have

\end{lemma}
\begin{proof}
For , we can directly write

The product of power sums is greater than 1; divide both sides by it. Analogously for .
\end{proof}

\begin{corollary} \label{cbounds}
Given , if  with ,
where 
 
then . Consequently, all positive roots of  lie in
. Also, for  with ,
 
it is , i.e. all pos. roots are in .

Combining both bounds we deduce that 
 
is a bounding box for .
\end{corollary}
\begin{proof}
  The denominator in~(\ref{univbounds}) is always positive in
  . Let  with . If
   then also  and it follows . Similarly . The same arguments hold for ,
  , ,
  and , since lower bounds on the zeros of  yield upper
  bounds on the zeros of .
\end{proof}

Thus lower and upper bounds on the th coordinates of the roots of
 are given by 

respectively, i.e. the intersection of these bounding boxes.

We would like to remain in the ring of integers all along the process,
thus integer lower or upper bounds will be used.These can be the
floor or ceil of the above roots of univariate polynomials, or even
known bounds for them, e.g. Cauchy's bound.

If the minimum and maximum are taken with the ordering of coefficients
defined as  then different  polynomials are obtained. By Cor.~\ref{cor:bernsteincoefs}
their control polygon is the lower and upper hull respectively of the
projections of the tensor-Bernstein coefficients to the th
direction and are known to converge quadratically to simple roots when
preconditioning (described in the following paragraph)
is utilized~\cite[Cor.~5.3]{mp:smspe-05}.\\


\if 0
We can further improve the upper bound, by setting
  
thus


Clearly, for ,  and .
\fi


\noindent{}{\bf Complexity analysis.} The analysis of the
subdivision step in Sect.~\ref{subsec:reduction} applies as well to the reduction step,
since reducing the domain means computing a new subdomain and ignoring the
remaining part.

If a lower bound  is known, with , then
the reduction step is 
performed in . This is an
instance of Lem.~\ref{shiftComplexity}. 

The projections of Lem.~\ref{u_bounds} are computed using  comparisons. The computation of  costs  in average, for solving these projections using univariate CF algorithm. Another option would to compute well known lower bounds on their roots, for instance Cauchy's bound in .
 
\noindent{}{\bf Illustration.} Consider a bi-quadratic , namely,  with coefficients . Suppose that  for . We compute

thus . Fig.~\ref{fig:env} shows how these univariate quadratics bound the graph of  in .
\begin{figure}[h]
  \centering
  \includegraphics[trim= 0mm 20mm 0mm 30mm, clip, scale=.3]{figs/env_functions.eps}
  \caption{The enveloping polynomials  in domain  for a bi-quadratic polynomial .}
  \label{fig:env}
\end{figure}

\subsection{Preconditioning}

To improve the reduction step, we use preconditioning. 
The aim of a preconditioner is to tune the system so that it can be
tackled more efficiently; in our case we aim at improving the bounds
of~Cor.~\ref{cbounds}.

A preconditioning matrix  is an invertible  matrix that
transforms a system  into the equivalent one
. 
This transformation does not alter the roots of the system, since the
computed equations generate the same ideal. The bounds obtained on the
resulting system can be used directly to reduce the domain of the
equations before preconditioning.
Preconditioning can be performed to a subset of the equations.

Since we use a reduction process using Cor.~\ref{cbounds} we
want to have among our equations  of them whose zero locus
 is orthogonal to the th direction, for all
.

Assuming a square system, we precondition  to
obtain a locally orthogonal to the axis system; an ideal
preconditioner would be the Jacobian of the system evaluated at a
common root; instead, we evaluate  at the image of the
center  of the initial domain ,
. Thus we must compute the inverse of the Jacobian matrix
 evaluated
at .

\if 0
The computation of  can be done
efficiently; we only need to compute the Jacobian of the initial
system, since

One can see that  so

\fi

\noindent{}{\bf Precondition step complexity.} Computing  is done with cost 
and evaluating at  has cost . We also need
 for inversion and  for multiplying
polynomials times scalar as well as summing polynomials. This
gives a precondition cost of order .


\section{Exclusion -- Inclusion criteria} \label{criteria}

A subdivision scheme is able to work when two tests are
available: one that identifies empty domains (exclusion test) and one
that identifies domains with exactly one zero (inclusion test). If
these two tests are negative, a domain cannot be neither included nor
excluded so we need to apply further reduction/subdivision steps to it.
The certification is the following: if the result of the test is affirmative,
then this is undoubtedly true.

\noindent{\bf Exclusion test.}
The bounding functions defined in the previous section provide a fast
filter to exclude empty domains. 
Define  and .

\begin{corollary}
If for some  and for some  it is
 or  then the system has no
solutions.  Also, if  
  then there can be no
solution to the system.
\end{corollary}
\begin{proof}
  For the former statement observe that  has no real positive
  roots, thus the system has no roots. The latter statement means that
  the reduced domains of each  do not intersect, thus
  there are no solutions.
\end{proof}

We can use interval arithmetic to identify additional empty domains; if
the sign of some initial  is constant in  then this domain is discarded. We can also simply inspect the
coefficients of each ; if there are no sign changes then there
corresponding box contains no solution.

The accuracy of these criteria greatly affects the performance of the
algorithm. In particular, the sooner an empty domain is rejected the
less subdivisions will take place and the process will terminate
faster. We justify that the exclusion criteria will eventually succeed
on an empty domain by proving a generalization of Vincent's theorem to
the tensor multivariate case.

\begin{theorem} \label{vincentxyz} Let  be a
  polynomial with real coefficients, such that it has no (complex)
  solution with  for . Then all its
  coefficients  are of the same sign.
\end{theorem}
\begin{proof}
  We prove the result by induction on , the number of
  variables. For , this is the classical Vincent's
  theorem~{\cite{Vincent}}.
  
  Consider now a polynomial 
   in two variables with no
  (complex) solution such that  for . We
  prove the result for , by induction on the degree . The property is obvious for polynomials of degree . Let
  us assume it for polynomials of degree less than .
  
  By hypothesis, for any  with ,
  the univariate polynomial  has no root with .  According to Lucas theorem~\cite{Marden1966}, the complex
  roots of  are in the convex hull of the
  complex roots of . Thus, there is no root of
   with  and . By induction hypothesis, the coefficients of
   are of the same sign. We decompose 
  as
   where  with  of the same sign, say
  positive. By Vincent theorem in one variable, as  has no
  root with , the coefficients  of  are also of the same sign. If this sign is different from
  the sign of  for  (ie. negative
  here), then  has one sign variation in its coefficients
  list. By Descartes rule, it has one real positive root, which
  contradicts the hypothesis on . Thus, all the coefficients have
  the same sign.
  
  Assume that the property has been proved for polynomials in 
  variables and let us consider a polynomial \  in  variables with \ no (complex) solution
  such that  for . For any  with , for , \ the polynomial 
  and  has no root
  with . By Lucas theorem and induction hypothesis on
  the degree,  has coefficients of the same
  sign. We also have  with coefficients
  of the same sign, by induction hypothesis on the number of
  variables. If the two signs are different, then  has one sign variation in its coefficients and thus one real
  positive root, say , which cannot be the case, since
   would yield a real root of . We deduce
  that all the coefficients of  are of the same sign.
  
  This completes the induction proof of the theorem.
\end{proof}

This implies that empty regions will be eventually excluded by sign inspection.

\begin{corollary} \label{corxyz}
  Let 
  be the representation of  through  in a box . If
  there is no toot  of  such that  
   then all the coefficients  are of the same sign.

That is, if ,
where  is the center of , then  is excluded by sign conditions. 
\end{corollary}
\begin{proof}
  The interval  is transformed by  into
   and the disk  is transformed into the half complex plane
  .  We deduce that  has no root with , .  By Thm.~\ref{vincentxyz}, the
  coefficients of  are of the same sign.
\end{proof}

We deduce that if a domain is far enough from the zero locus of
some  then it will be excluded, hence redundant empty domains
concentrate only in a neighborhood of .

\begin{definition}
The tubular neighborhood of size  of  is the set 

\end{definition}

We bound the number of boxes that are not excluded at each level of the subdivision tree.
\begin{lemma} Assume that for ,
   is bounded.
Then the number of boxes of size  kept by the algorithm
is less than  , where  is such that
   st. ,

\end{lemma}
\begin{proof}
  Consider a subdivision of a domain  into boxes of size
  . We will bound the number  of boxes in this
  subdivision that are not rejected by the algorithm. By
  Cor.~\ref{corxyz} if a box is not rejected, then we have for all
   , where  is the center of the box.
  Thus all the points of this box are at distance  to 
that is in .

To bound , it suffices to estimate the dimensional volume , since we have:

 
When  tends to , this volume becomes equivalent to a
constant times . For a square system with single
roots in , it becomes equivalent to the sum for all real roots
 in  of the volumes of parallelotopes in  dimensions
of height  and unitary edges proportional to the
gradients of the polynomials evaluated at the common root; It is thus
bounded by . We deduce that there
exists a constant  such that
. For
overdetermined systems, the volume is bounded by a similar
expression. Since  has a limit
when  tends to , we deduce the existence of the finite
constant  and the bound of the lemma on the number of kept boxes of
size .
\end{proof}

\noindent{\bf Inclusion test.}
We present a test that discovers common solutions, in a box, or
equivalently in , through homography. To simplify the
statements we assume that the system is square, i.e. .

\begin{definition}
  The \emph{lower face} polynomial of  w.r.t. direction  is
  . The \emph{upper face} polynomial of 
  w.r.t.  is .
\end{definition}



\begin{lemma}[Miranda Theorem~\cite{vrahatis1989}]
  If for some permutation ,
   and  are constant and opposite for all , then
  the equations  have at least one root in .
\end{lemma}

The implementation of the Miranda test can be done efficiently if we
compute a  matrix with th entry  iff  and  are opposite. Then,
Miranda test is satisfied iff there is no zero row and no zero
column. To see this observe that the matrix is the sum of a
permutation matrix and a  matrix iff this permutation satisfies
Miranda's test.

Combined with the following simple fact, we have a test that
identifies boxes with a single root.
\begin{lemma} 
  If  has constant sign in a box , then there
  is at most one root of  in .
\end{lemma} 
\begin{proof}
  Suppose  are two distinct roots; by the mean value theorem
  there is a point  on the line segment , and thus
  in , s.t.  hence .
\end{proof}


\if 0
In order to identify boxes that contain solutions we use the topological degree and the Jacobian.

Another criterion that can be used is the Newton test.
\begin{lemma}
If the Newton iteration is a contraction over a box  then there exists a unique root of the system inside .
\end{lemma}
\begin {proof}
Fixed point theorem for existence, mean value theorem for unicity.
\end{proof}

\subsection{Multiple roots}
Consider the system
 which has a solution  iff  is a simple
root of , so then .

The Jacobian of this system


\fi
\noindent{}{\bf Complexity of the inclusion criteria.}  Miranda test can be
decided with  evaluations on interval (cf.~\cite{Garloff00}) as well
as one evaluation of , overall  operations.  The
cost of the inclusion test is dominated by the cost of evaluating
 polynomials of size  on an interval,
i.e.  operations suffice.


\begin{proposition} 
If the real roots of the square system in the initial domain  are simple, then
Alg.~\ref{algo:subdivision} stops with boxes isolating the
real roots in .
\end{proposition}
\begin{proof}
  If the real roots of  in  are simple, in a small neighborhood of
  them the Jacobian of  has a constant sign. By the inclusion
  test, any box included in this neighborhood will be output if and
  only if it contains a single root and has no real roots of the
  jacobian. Otherwise, it will be further subdivided or
  rejected. Suppose that the subdivision algorithm does not
  terminate. Then the size of the boxes kept at each step tends to
  zero. By Cor.~\ref{corxyz}, these boxes are in the intersection of
  the tubular neighborhoods  for
   the maximal size of the kept boxes. If 
  is small enough, these boxes are in a neighborhood of a root in
  which the Jacobian has a constant size, hence the inclusion test
  will succeed.  By the exclusion criteria, a box domain is not
  subdivided indefinitely, but is eventually rejected when the
  coefficients become positive.  Thus the algorithm either outputs
  isolating boxes that contains a real root of the system or rejects
  empty boxes. This shows, by contradiction, the termination of the
  subdivision algorithm.
\end{proof} 



\section{The complexity of  mCF} 
\label{sec:complexity}

In this section we compute a bound on the complexity of the algorithm
that exploits the continued fraction expansion of the real roots of
the system.  Hereafter, we call this algorithm MCF (Multivariate
Continued Fractions).  Since the analysis of the reduction steps of
Sec. \ref{subdivision-reduction} and the Exclusion-Inclusion test of
Sec.~\ref{criteria} would require much more developments, we simplify
the situation and analyze a variant of this algorithm. We assume that
two oracles are available. One that computes, exactly, the partial
quotients of the positive real roots of the system, and one that
counts exactly the number of real roots of the system inside a
hypercube in the open positive orthant, namely  . In what
follows, we will assume the cost of the first oracle is bounded by
, while the cost of the second is bounded by
, and we derive the total complexity of the algorithm
with respect to these parameters.  In any case the number of reduction
or subdivision steps that we derive is a lower bound on the number of
steps that every variant of the algorithm will perform. The next
section presents some preliminaries on continued fractions, and then
we detail the complexity analysis.


\subsection{About continued fractions}

Our presentation follows closely \cite{emt-lncs-2006}.
For additional details we refer the reader to, e.g., \cite{Yap2000,BomPoo:contfrac:95,Poorten:intro}.
In general a {\em simple (regular) continued fraction} 
is a (possibly infinite)  expression of the form 

where the numbers  are called {\em partial quotients}, 
 and  for .  
Notice that  may have any sign, however, in our real root isolation
algorithm , without loss of generality.
By considering the recurrent relations

it can be shown by induction that ,
for .


If  then 
 
and since this is a series of decreasing alternating terms it converges to some real
number .
A finite section 
is called the th {\em convergent} (or {\em approximant}) of 
and the tails   are known as its
{\em complete quotients}. 
That is  
for .
There is an one to one correspondence between the real numbers and the continued
fractions, where evidently the finite continued fractions correspond
to rational numbers.

It is known that  
and that , 
where  is the th Fibonacci number 
and  is the golden ratio.
Continued fractions are the best rational approximation(for a given denominator size).
This is as follows:

Let  be the continued fraction expansion of a real
number. The Gauss-Kuzmin distribution
\cite{BomPoo:contfrac:95}
states that for almost all real numbers  (meaning that the set
of exceptions has Lebesgue measure zero) the probability for a
positive integer  to appear as an element  in the
continued fraction expansion of  is  

The Gauss-Kuzmin law induces that we can not bound the mean value 
of the partial quotients 
or in other words that the expected value (arithmetic mean) of the partial
quotients is diverging, i.e. 


Surprisingly enough the geometric (and the harmonic) mean is not only
asymptotically bounded, but is bounded by a constant, for almost all .
For the geometric mean this is the famous Khintchine's constant
\cite{Khintchine:97}, i.e.

It is not known if  is a transcendental number.
The expected value of the bit size of the partial quotients is a constant
for almost all real numbers, 
when  or  sufficiently big
 \cite{Khintchine:97}.
Notice that in (\ref{eq:Gauss-Kuzmin}), ,
thus  is uniformly distributed in . 
Let , then











\subsection{Complexity results}

Let  be an upper bound on the bitsize of the partial quotient that
appear during the execution of the algorithm.

\begin{lemma}
  \label{lem:mcf-steps}
  The number of reduction and subdivision steps 
  that the algorithm performs is .
\end{lemma}
\begin{proof}
  Let  be a real root of the system.
  It suffices to consider the number of steps needed to isolate
  the  coordinate of .

  Recall, that we assume, working in the positive orthant, 
  we can compute exactly the next partial quotient in each coordinate;
  in other words a vector , where each , 
  , is the partial quotient of a coordinate of a positive 
  real\footnote{Actually the analysis holds even in the
  case where each  is the partial quotient of the positive imaginary part of
  a coordinate of a solution of the system.} solution of the system.

  Let  be the number of steps needed to isolate the
   coordinate of the real root . 
  The analysis is similar to the univariate case.
  The successive approximations of
   by the lower bound ,
  yield  the -th approximant,
   of ,
  which using (\ref{eq:cf-approx}) satisfies
  

  In order to isolate , it suffices to have
  
  where  is the local separation bound of ,
  that is the smallest distance between  and all the other
  -coordinates of the positive real solutions of the system.

  Combining the last two equations, we deduce that
  to achieve the desired approximation, we should have 
  ,
  or  
  .
  That is to isolate the  coordinate it suffices to perform 
   steps.
  To compute the total number of steps, we need to sum over all positive real
  roots and multiply by , which is the number of coordinates,
  that is 
  
  where  is the number of positive real roots.


  To bound the logarithm of the product, we use  \cite{emt-dmm-2009},
  i.e. aggregate separation bounds for multivariate, zero-di\-men\-sion\-al polynomial
  systems. It holds 
  
  Taking into account that  we conclude that the number of steps 
  is .
\end{proof}


\begin{proposition}
  \label{prop:mcf-complexity}
  The total complexity of the algorithm is 
  .  
\end{proposition}
\begin{proof}
  At each -th step of algorithm, 
  if there are more than one roots of the corresponding system in the positive orthant
  (the cost of estimating this is ,  
  we compute the corresponding partial quotients ,
  where 
  (the cost of estimating this is 
  Then, for each polynomial of the system, , we perform the shift operation
  ,
  and then we split to  subdomains.
  Let us estimate the cost of the last two operations.

  A shift operation on a polynomial of degree , by a
  number of bitsize , increases the bitsize of the polynomial by an
  additive factor .
At the  step of the algorithm, the polynomials of the corresponding system
  are of bitsize , and we need to
  perform a shift operation to all the variables, with number of bitsize
  .
  The cost of this operation is ,
  and since we have  polynomials the costs becomes 
  ,
  The resulting polynomial has bitsize .
  
  To compute the cost of splitting the domain, we proceed as follows.
  The cost is bounded by the cost of performing  operations ,
  which in turn is  
  .
  So the total cost becomes
  .

  
  It remains to bound .
  If  is a bound on the bitsize of all the partial quotients that appear
  during and execution of the algorithm, then 
  .

Moreover,  (lem.~\ref{lem:mcf-steps}),
  and so the cost of each step is
.
  
  Finally, multiplying by the number of steps (lem.~\ref{lem:mcf-steps})
  we get a bound of .  

  To derive the total complexity we have to take into account that at each step
  we compute some partial quotients and  and we count the number of real root of
  the system in the positive orthant.
  Hence the total complexity of the algorithm is 
  .  
\end{proof}

In the univariate case (),
if we assume that (\ref{eq:exp_b}) holds for real algebraic numbers, 
then the cost of  and 
is dominated by that of the other steps, that is the splitting operations, 
and the (average) complexity becomes 
and matches the one derived in \cite{et-tcs-2007}
(without scaling). 


\subsection{Further improvements}
\label{sec:complexity-improvements}

We can reduce the number of steps that the algorithm performs, and thus improve
the total complexity bound of the algorithm, using the same trick as in
\cite{et-tcs-2007}.
The main idea is that the continued fraction expansion of a real root of a
polynomial does not depend on the initial computed interval that contains all
the roots.
Thus, we spread away the roots by scaling the variables of the polynomials of the
system by a carefully chosen value.

If we apply the map 
, to the initial
polynomials of the system, then the real roots are multiply by , and
thus their distance increase.
The key observation is that the continued fraction expansion of the real roots
does not depend on their integer part.
Let  be the roots of the system, and , be the roots after the
scaling. It holds .
From \cite{emt-dmm-2009} it holds that 

and thus

If we choose 
and assume that  which is the worst case, then 
.
Thus, following the proof of Lem.~\ref{lem:mcf-steps},
the number of steps that the algorithm is .

The bitsize of the scaled polynomials becomes
.
The total complexity of algorithm is now

where  the maximum bitsize of the partial quotient
appear during the execution of the algorithm.
If we assume that (\ref{eq:exp_b}) holds for real algebraic numbers, 
then . Notice that in
this case, when , the bound becomes , which agrees with
the one proved in \cite{et-tcs-2007}.

The discussion above combined with Prop.~\ref{prop:mcf-complexity} lead us to:

\begin{theorem}
  \label{th:improved-mcf-complexity}
  The total complexity of the algorithm is 
  .
\end{theorem}

\section{Implementation and Examples}\label{sec:impl}
\begin{figure}[t]
  \centering
  \includegraphics[scale=.27]{figs/s1}
  \caption{Isolating boxes of the real roots of .}
  \label{fig:sys-1}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[scale=.23]{figs/s2}\hfill
  \includegraphics[scale=.23]{figs/s3}\hfill
  \includegraphics[scale=.23]{figs/s4}
  \caption{Isolating boxes of the real roots of the system 
    Left: , Middle: , Right: .}
  \label{fig:sys-2}
  \label{fig:sys-3}
  \label{fig:sys-4}
\end{figure*}

We have implemented the algorithm in the C++ library \texttt{realroot} of  
\mathemagix\footnote{\texttt{http://www.mathemagix.org/}},
which is an open source effort that provides fundamental algebraic operations 
such as algebraic number manipulation tools, different types of univariate 
and multivariate polynomial root solvers, resultant and GCD computations, etc.

The polynomials are internally represented as a vector of coefficients
along with some additional data, such as a variable dictionary and the
degree of the polynomial in every variable. This allows us to map the
tensor of coefficients to the one-dimensional memory. The univariate
solver that is used is the continued fraction solver; this is
essentially the same algorithm with a different inclusion criterion,
the Descartes rule. The same data structures is used to store the
univariate polynomials, and the same shift/contraction routines. The
univariate solver outputs the roots in increasing order, as a result
of a breadth-first traverse of the subdivision tree. In fact, we only
compute an isolation box for the smallest positive root of univariate
polynomials and stop the solver as soon as the first root is
found. Our code is templated and is efficiently used with GMP
arithmetic, since long integers appear as the box size decreases.

The following four examples demonstrate the output of our implementation, 
which we visualize using \axel\footnote{\texttt{http://axel.inria.fr}}.

First, we consider the system  (), 
where , and .
We are looking for the real solutions in the domain 
, which is mapped to , by an initial
transformation. The isolating boxes of the real roots can be seen in
Fig.~\ref{fig:sys-1}. 

In systems , We multiply  and  by
quadratic components, hence we obtain

and 

The isolating boxes of this system could be seen in
Fig.~\ref{fig:sys-2}.  Notice, that
size of the isolation boxes that are returned in this case is
considerably smaller. 


Consider the system , which 
consists of  and , which is a polynomial of bidegree .
The output of the algorithm, that is the isolating boxes of the real roots can be seen in 
Fig.~\ref{fig:sys-4}.
One important observation is the fact the isolating boxes {\em are not} squares,
which verifies the adaptive nature of the proposed algorithm.

We provide execution details on
these experiments in
Table~\ref{tab:exec}. Several
optimizations can be applied to our
code, but the results already
indicate that our approach competes
well with the Bernstein case.

\begin{table} \begin{center}\begin{tabular}{|c|c|c|c|c|c|} \hline System     & Domain   &Iters.& Subdivs. & Sols. & Excluded \\ \hline \hline
 &&53    &26        & 4     & 25   \\ \hline
 &&263   &131       &12     & 126  \\ \hline
 &&335   &167       &8      &160   \\ \hline
 && 1097 & 548      & 16    &533   \\ \hline
\end{tabular}\end{center}
\caption{Execution data for , , , .}
\label{tab:exec}
\end{table}

\vspace{.1cm}

\noindent{\bf Acknowledgements}\\
The first and second author were supported by Marie-Curie Initial
Training Network SAGA, [FP7/2007-2013], grant
[PITN-GA-2008-214584]. The third author was supported by contract
[ANR-06-BLAN-0074] ``Decotes''.

\bibliographystyle{abbrv}
\begin{thebibliography}{10}



\bibitem{Vincent}
A.~Alesina and M.~Galuzzi.
\newblock New proof of {V}incent's thm.
\newblock {\em Enseignement Math{\'e}matique}, 44:219--256, 1998.

\bibitem{jue:07e}
M.~Barto\v{n} and B.~J\"uttler.
\newblock Computing roots of polynomials by quadratic clipping.
\newblock {\em Comp.\ Aided Geom. Design}, 24:125--141, 2007.

\bibitem{BomPoo:contfrac:95}
E.~Bombieri and A.~van~der Poorten.
\newblock Continued fractions of algebraic numbers.
\newblock In {\em Computational algebra and number theory (Sydney, 1992)},
  pp. 137--152. Kluwer Acad. Publ., Dordrecht, 1995.

\bibitem{MR2289103}
A.~Eigenwillig, V.~Sharma, and C.~K. Yap.
\newblock Almost tight recursion tree bounds for the {D}escartes method.
\newblock In {\em I{SSAC} 2006}, pp. 71--78. ACM, New York, 2006.

\bibitem{ELBER:2001}
G.~Elber and M.-S. Kim.
\newblock Geometric constraint solver using multivariate rational spline
  functions.
\newblock In {\em Proc. of 6th ACM Symposium on Solid Modelling and
  Applications}, pp. 1--10. ACM Press, 2001.

\bibitem{ACS-TR-363602-02}
I.~Emiris, M.~Hemmer, M.~Karavelas, S.~Limbach, B.~Mourrain, E.~P. Tsigaridas,
  and Z.~Zafeirakopoulos.
\newblock Cross-benchmarks of univariate algebraic kernels.
\newblock  ACS-TR-363602-02, INRIA, MPI and NUA, 2008.

\bibitem{emt-lncs-2006}
I.~Z. Emiris, B.~Mourrain, and E.~P. Tsigaridas.
\newblock {Real Algebraic Numbers: Complexity Analysis and Experimentation}.
\newblock In P.~Hertling, C.~Hoffmann, W.~Luther, and N.~Revol, ed., {\em
  {Reliable Implementa- tions of Real Number Algorithms: Theory and Practi- ce}},
  {\em LNCS} vol. 5045, pp. 57--82. Springer Verlag, 2008.

\bibitem{emt-dmm-2009}
I.~Z. Emiris, B.~Mourrain, and E.~P. Tsigaridas.
\newblock The dmm bound: multivariate (aggregate) separation bounds.
\newblock Technical report, INRIA, March 2009.

\bibitem{Garloff00}
J.~Garloff and A.~P. Smith.
\newblock Investigation of a subdivision based algorithm for solving systems of
  polynomial equations.
\newblock {\em Journal of Nonlinear Analysis}, 47(1):167--178, 2001.

\bibitem{Khintchine:97}
A.~Khintchine.
\newblock {\em {Continued Fractions}}.
\newblock University of Chicago Press, Chicago, 1964.



\bibitem{Marden1966}
M.~Marden.
\newblock {\em Geometry of Polynomials}.
\newblock American Mathematical Society, Providence, RI, 1966.

\bibitem{mp:smspe-05}
B.~Mourrain and J.~Pavone.
\newblock Subdivision methods for solving polynomial equations.
\newblock Special issue in honor of Daniel Lazard.
\newblock {\em JSC} 44(3):292 -- 306, 2009.

\bibitem{MOURRAIN:2005}
B.~Mourrain, F.~Rouillier, and M.-F. Roy.
\newblock {\em Bernstein's basis and real root isolation}, pp. 459--478.
\newblock MSRI Publications. Cambridge University Press, 2005.

\bibitem{Pan97rev}
V.~Pan.
\newblock Solving a polynomial equation: Some history and recent progress.
\newblock {\em SIAM Rev.}, 39(2):187--220, 1997.

\bibitem{Pan02jsc}
V.~Pan.
\newblock Univariate polynomials: Nearly optimal algorithms for numerical
  factorization and rootfinding.
\newblock {\em JSC}, 33(5):701--733, 2002.



\bibitem{sharma-tcs-2008}
V.~Sharma.
\newblock Complexity of real root isolation using continued fractions.
\newblock {\em Theor. Comput. Sci.}, 409(2):292--310, 2008.

\bibitem{sp-csnps-93}
E.~C. Sherbrooke and N.~M. Patrikalakis.
\newblock Computa- tion of the solutions of nonlinear polynomial systems.
\newblock {\em Comput. Aided Geom. Design}, 10(5):379--405, 1993.

\bibitem{et-tcs-2007}
E.~P. Tsigaridas and I.~Z. Emiris.
\newblock {On the complexity of real root isolation using Continued Fractions}.
\newblock {\em Theoretical Computer Science}, 392:158--173, 2008.

\bibitem{Poorten:intro}
A.~van~der Poorten.
\newblock An introduction to continued fractions.
\newblock In {\em Diophantine analysis}, pp. 99--138. {Cambridge University
  Press}, 1986.

\bibitem{gg-shift-1997}
J.~von~zur Gathen and J.~Gerhard.
\newblock {Fast Algorithms for Taylor Shifts and Certain Difference Equations.}
\newblock In {\em Proc.\ Annual ACM ISSAC}, pp. 40--47, 1997.

\bibitem{vrahatis1989}
M.~N. Vrahatis.
\newblock A short proof and a generalization of {M}iranda's existence theorem.
\newblock {\em Proceedings of the American Mathematical Society},
  107(3):701--703, 1989.

\bibitem{Yap2000}
C.~Yap.
\newblock {\em Fundamental Problems of Algorithmic Algebra}.
\newblock Oxford University Press, New York, 2000.

\end{thebibliography}

\end{document}
