\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{xparse}
\usepackage{xspace}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{colortbl}
\usepackage{textcomp}
\usepackage{subfig}

\usepackage{algorithm-mine}
\usepackage{algorithmic-mine}



\newcommand{\eqnname}{Eq.}
\newcommand{\sectionname}{Section}
\newcommand{\algname}{Listing}
\newcommand{\cf}{cf.\ }

\newcommand{\ThreeD}{\mbox{3-D}}
\newcommand{\TwoD}{\mbox{2-D}}
\newcommand{\SomeD}[1]{\mbox{#1-D}}

\newcommand{\note}[1]{\fbox{\vspace{0.5cm}
        \begin{minipage}{\linewidth}
            #1
        \end{minipage}
        \vspace{0.5cm}
    } }
 \DeclareDocumentCommand\vec{m}{\boldsymbol{#1}}
\DeclareDocumentCommand\mat{m}{\boldsymbol{#1}}
\DeclareDocumentCommand\setvar{m}{\mathcal{#1}}
\DeclareDocumentCommand\t{m}{{#1}^{\rm T}}
\DeclareDocumentCommand\inv{m}{{#1}^{-1}}
\DeclareDocumentCommand\|{}{\mid} \DeclareDocumentCommand\DiracDelta{m m}{\delta(#1 - #2)}

\DeclareDocumentCommand\MapVar{m}{#1^{*}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareDocumentCommand\IndexedVar{m g g}{#1\IfNoValueF{#2}{_{#2}}\IfNoValueF{#2}{^{(#3)}}
}
\DeclareDocumentCommand\Indicator{m}{\mathbbm{1}[#1]}

\DeclareDocumentCommand\d{}{\,{\rm d}}
\DeclareDocumentCommand\E{o m}{\operatorname{E}\IfNoValueF{#1}{_#1}\left[#2\right]}
\DeclareDocumentCommand\N{o m m}{\mathcal{N} (\IfNoValueF{#1}{#1;} #2, #3)}

\DeclareDocumentCommand\RFS{}{RFS\xspace}
\DeclareDocumentCommand\PHD{}{PHD\xspace}
\DeclareDocumentCommand\GMPHD{}{GM-PHD\xspace}
\DeclareDocumentCommand\SMCPHD{}{SMC-PHD\xspace}
\DeclareDocumentCommand\PGFL{}{PGFL\xspace}
\DeclareDocumentCommand\MTState{o}{X\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\MTMeasurement{o}{Z\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\Clutter{o}{K\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\NumTargetsSet{o}{M\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\NumTargets{O{i}}{m_{#1}}
\DeclareDocumentCommand\STState{o}{\vec{x}\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\STMeasurement{o}{\vec{z}\IfNoValueF{#1}{_{#1}}}

\DeclareDocumentCommand\Psurvival{o}{p_{\rm S}\IfNoValueF{#1}{(#1)}}
\DeclareDocumentCommand\Pdetection{o}{p_{\rm D}\IfNoValueF{#1}{(#1)}}
\DeclareDocumentCommand\STMotionModel{o}{f\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\STMeasurementModel{o}{g\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\STMotionProbability{o m m}{p\IfNoValueF{#1}{_{#1} (#2 \| #3)}}
\DeclareDocumentCommand\STMeasurementProbability{o m m}{p\IfNoValueF{#1}{_{#1}} (#2 \| #3)}

\DeclareDocumentCommand\v{o}{v\IfNoValueF{#1}{_{#1}}}       \DeclareDocumentCommand\b{o}{b\IfNoValueF{#1}{_{#1}}}       \DeclareDocumentCommand\c{o}{c\IfNoValueF{#1}{_{#1}}}  \DeclareDocumentCommand\u{o}{u\IfNoValueF{#1}{_{#1}}}       

\DeclareDocumentCommand\Trajectory{o}{T\IfNoValueF{#1}{_{#1}}}
\DeclareDocumentCommand\TrackDistribution{o o}{\pi\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\TrackR{o o}{r\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\TrackP{o o}{p\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\TrackV{o o}{v\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\TrackS{o}{q\IfNoValueF{#1}{_{#1}}}

\DeclareDocumentCommand\NumParticles{m}{N_{#1}}
\DeclareDocumentCommand\ParticleSample{o o}{\vec{x}\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\ParticleWeightV{o o}{w\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\ParticleSet{o o}{Q\IfNoValueF{#1}{_{#1}}\IfNoValueF{#2}{^{(#2)}}}

\DeclareDocumentCommand\BirthSample{o o}{\vec{x}_{\textrm{b},\IfNoValueF{#1}{#1}}\IfNoValueF{#2}{^{(#2)}}}
\DeclareDocumentCommand\BirthWeightV{o o}{w_{\textrm{b},\IfNoValueF{#1}{#1}}^{(\IfNoValueF{#2}{#2})}}
\DeclareDocumentCommand\BirthSet{o}{B\IfNoValueF{#1}{_{#1}}}


\DeclareDocumentCommand\Pose{}{\vec{x}}
\DeclareDocumentCommand\Measurement{}{\vec{z}}
 
\newcommand{\etal}{et al.}

\DeclareDocumentCommand\LossVar{}{\mathcal{L}}

\DeclareDocumentCommand{\Dataset}{}{\mathcal{D}}
\DeclareDocumentCommand{\DatasetSize}{}{N}

\DeclareDocumentCommand\Input{}{\vec{x}}
\DeclareDocumentCommand\Label{}{y}
\DeclareDocumentCommand\Latent{}{\vec{r}}

\DeclareDocumentCommand\Encode{o m}{f(#2\IfNoValueF{#1}{; #1})}
\DeclareDocumentCommand\ParamSet{}{\Theta}
\DeclareDocumentCommand\EncoderParams{}{\Theta}

\DeclareDocumentCommand\Predict{o o m}{g\IfNoValueF{#1}{_{#1}}(#3\IfNoValueF{#2}{; #2})}

\DeclareDocumentCommand\RunningLabel{}{k}
\DeclareDocumentCommand\NumLabels{}{L}
\DeclareDocumentCommand\NumSamples{}{M}

\DeclareDocumentCommand\ClassMean{}{\vec{\mu}}
\DeclareDocumentCommand\Stddev{}{\sigma}
\DeclareDocumentCommand\Variance{}{\Stddev^2}
\DeclareDocumentCommand\SampleWeight{}{w}

\DeclareDocumentCommand\IdentityMatrix{}{\mat{I}}

\DeclareDocumentCommand\setv{m}{#1}



\title{Simple Online and Realtime Tracking with a Deep Association Metric}
\name{Nicolai Wojke\textsuperscript{\textdagger},
    Alex Bewley\textsuperscript{},
    Dietrich Paulus\textsuperscript{\textdagger}}
\address{University of Koblenz-Landau\textsuperscript{\textdagger},
    Queensland University of Technology\textsuperscript{}}



\begin{document}
\maketitle
\begin{abstract}
Simple Online and Realtime Tracking (SORT) is a pragmatic approach to
multiple object tracking with a focus on simple, effective algorithms.
In this paper, we integrate appearance information to improve the performance
of SORT{}.
Due to this extension we are able to track objects through longer periods of
occlusions, effectively reducing the number of identity switches.
In spirit of the original framework we place much of the computational
complexity into an offline pre-training stage where we learn a deep
association metric on a large-scale person re-identification dataset.
During online application, we establish measurement-to-track associations
using nearest neighbor queries in visual appearance space.
Experimental evaluation shows that our extensions reduce the number of identity
switches by 45\%, achieving overall competitive performance at high
frame rates.
\end{abstract}
\begin{keywords}
    Computer Vision,
    Multiple Object Tracking,
Data Association
\end{keywords}
\section{INTRODUCTION}
\label{sec:intro}

Due to recent progress in object detection, tracking-by-detection has become
the leading paradigm in multiple object tracking.
Within this paradigm, object trajectories are usually found in a global
optimization problem that processes entire video batches at once.
For example, flow network formulations~\cite{ZhangLN08, PirsiavashRF11, DBLP:journals/pami/BerclazFTF11}
and probabilistic graphical models~\cite{DBLP:conf/cvpr/YangN12a, DBLP:conf/cvpr/YangN12,
DBLP:conf/cvpr/AndriyenkoSR12, DBLP:conf/cvpr/MilanSR13}
have become popular frameworks of this type.
However, due to batch processing, these methods are not applicable in
online scenarios where a target identity must be available at each time step.
More traditional methods are Multiple Hypothesis Tracking~(MHT)~\cite{Reid79analgorithm} and the Joint Probabilistic Data Association
Filter~(JPDAF)~\cite{fortmann1983sonar}. These methods perform data association
on a frame-by-frame basis.
In the JPDAF, a single state hypothesis is generated by weighting individual
measurements by their association likelihoods. In MHT, all possible
hypotheses are tracked, but pruning schemes must be applied for computational
tractability.
Both methods have recently been revisited in a tracking-by-detection
scenario~\cite{kim2015multiple, hamid2015joint} and shown promising results.
However, the performance of these methods comes at increased computational
and implementation complexity.

Simple online and realtime tracking (SORT)~\cite{Bewley2016_sort} is a much
simpler framework that performs Kalman filtering in image space and
frame-by-frame data association using the Hungarian method
with an association metric that measures bounding box overlap.
This simple approach achieves favorable performance at high frame rates.
On the MOT challenge dataset~\cite{MOTChallenge2015},
SORT with a state-of-the-art people detector~\cite{renNIPS15fasterrcnn}
ranks on average higher than MHT on standard detections.
This not only underlines the influence of object detector performance on
overall tracking results, but is also an important insight from a practitioners
point of view.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{images/occlusion1_2_smaller}
\caption{Exemplary output of our method on the MOT challenge
    dataset~\cite{milan2016mot16} in a common tracking situation with
    frequent occlusion.}
\label{fig:example-output}
\vspace{-0.4cm}
\end{figure}

While achieving overall good performance in terms of tracking precision and
accuracy, SORT returns a relatively high number of identity switches.
This is, because the employed association metric is only accurate when state estimation uncertainty is low.
Therefore, SORT has a deficiency in tracking through occlusions as they
typically appear in frontal-view camera scenes.
We overcome this issue by replacing the association metric with
a more informed metric that combines motion and appearance information.
In particular, we apply a convolutional neural network (CNN) that has been
trained to discriminate pedestrians on a large-scale person re-identification
dataset.
Through integration of this network we increase robustness against misses
and occlusions while keeping the system easy to implement, efficient, and
applicable to online scenarios.
Our code and a pre-trained CNN model are made publicly available to
facilitate research experimentation and practical application development.



\section{SORT WITH DEEP ASSOCIATION METRIC}
\label{sec:tracking}

We adopt a conventional single hypothesis tracking methodology with recursive
Kalman filtering and frame-by-frame data association.
In the following section we describe the core components of this system in
greater detail.

\subsection{Track Handling and State Estimation}

The track handling and Kalman filtering framework is mostly identical to the
original formulation in~\cite{Bewley2016_sort}.
We assume a very general tracking scenario where the camera is uncalibrated
and where we have no ego-motion information available.
While these circumstances pose a challenge to the filtering framework,
it is the most common setup considered in recent multiple object tracking
benchmarks~\cite{milan2016mot16}.
Therefore, our tracking scenario is defined on the eight dimensional
state space~
that contains the bounding box center position , aspect ratio ,
height , and their respective velocities in image coordinates.
We use a standard Kalman filter with constant velocity motion and linear
observation model, where we take the bounding
coordinates~ as direct observations of the
object state.

For each track  we count the number of frames since the last successful
measurement association . This counter is incremented during Kalman filter
prediction and reset to~0 when the track has been associated with a measurement.
Tracks that exceed a predefined maximum age~ are considered to
have left the scene and are deleted from the track set.
New track hypotheses are initiated for each detection that cannot be associated
to an existing track.
These new tracks are classified as tentative during their first three frames.
During this time, we expect a successful measurement association at each time
step.  Tracks that are not successfully associated to a measurement within their first
three frames are deleted.

\subsection{Assignment Problem}

A conventional way to solve the association between the
predicted Kalman states and newly arrived measurements is to build an
assignment problem that can be solved using the Hungarian algorithm.
Into this problem formulation we integrate motion and appearance information
through combination of two appropriate metrics.

To incorporate motion information we use the (squared) Mahalanobis distance
between predicted Kalman states and newly arrived measurements:

where we denote the projection of the -th track distribution into
measurement space by~ and the -th bounding box
detection by .
The Mahalanobis distance takes state estimation uncertainty into account by
measuring how many standard deviations the detection is away from the mean
track location.
Further, using this metric it is possible to exclude unlikely associations by
thresholding the Mahalanobis distance at a  confidence interval computed
from the inverse  distribution.
We denote this decision with an indicator

that evaluates to  if the association between the -th track and -th
detection is admissible. For our four dimensional measurement space the
corresponding Mahalanobis threshold is~.

While the Mahalanobis distance is a suitable association metric when motion
uncertainty is low, in our image-space problem formulation the predicted
state distribution obtained from the Kalman filtering framework provides only
a rough estimate of the object location.
In particular, unaccounted camera motion can introduce rapid displacements in
the image plane, making the Mahalanobis distance a rather uninformed metric for
tracking through occlusions.
Therefore, we integrate a second metric into the assignment problem.
For each bounding box detection  we compute an
appearance descriptor  with .
Further, we keep a gallery~
of the last  associated appearance descriptors for each track .
Then, our second metric measures the smallest cosine distance between
the~-th track and~-th detection in appearance space:

Again, we introduce a binary variable to indicate if an association is admissible
according to this metric

and we find a suitable threshold for this indicator on a separate training
dataset.
In practice, we apply a pre-trained CNN to compute bounding box appearance
descriptors. The architecture of this network
is described in \sectionname~\ref{subsec:metric-learning}.

In combination, both metrics complement each other by serving different aspects
of the assignment problem. On the one hand, the Mahalanobis distance provides
information about possible object locations based on motion that
are particularly useful for short-term predictions.
On the other hand, the cosine distance considers appearance information that
are particularly useful to recover identities after long-term occlusions, when
motion is less discriminative.
To build the association problem we combine both metrics using a weighted sum

where we call an association admissible if it is within the gating region of
both metrics:

The influence of each metric on the combined association cost can be controlled
through hyperparameter .
During our experiments we found that setting  is a reasonable
choice when there is substantial camera motion.
In this setting, only appearance information are used in the association cost
term. However, the Mahalanobis gate is still used to disregarded infeasible
assignments based on possible object locations inferred by the Kalman filter.

\subsection{Matching Cascade}
\label{subseq:matching-cascade}

\begin{algorithm}[tb]
\caption{Matching Cascade}
\algsetup{linenosize=\small}
\small
\begin{algorithmic}[1]
    \INPUT{Track indices ,
        Detection indices ,
        Maximum age~
    }
    \STATE{Compute cost matrix  using \eqnname~\ref{eq:total_cost_elem}} \label{lst:cascade_cost_matrix}
    \STATE{Compute gate matrix  using \eqnname~\ref{eq:gate_elem}} \label{lst:cascade_gate_matrix}
    \STATE{Initialize set of matches }
    \STATE{Initialize set of unmatched detections }
    \FOR{}
        \STATE{Select tracks by age } \label{lst:cascade_select} \label{lst:cascade_track_selection}
        \STATE{} \label{lst:cascade_solve}
        \STATE{} \label{lst:cascade_aggregate_matches}
        \STATE{} \label{lst:cascade_update_unmatched}
    \ENDFOR{}
    \RETURN  \label{lst:cascade_return}
\end{algorithmic}
\label{lst:matching_cascade}
\end{algorithm}

Instead of solving for measurement-to-track associations in a global
assignment problem, we introduce a cascade that solves a series of subproblems.
To motivate this approach, consider the following situation:
When an object is occluded for a longer period of time, subsequent Kalman
filter predictions increase the uncertainty associated with the object
location. Consequently, probability mass spreads out in state space and the
observation likelihood becomes less peaked.
Intuitively, the association metric should account for this spread of
probability mass by increasing the measurement-to-track distance.
Counterintuitively, when two tracks compete for the same detection, the
Mahalanobis distance favors larger uncertainty, because it effectively reduces
the distance in standard deviations of any detection towards the projected
track mean.
This is an undesired behavior as it can lead to increased track fragmentations
and unstable tracks.
Therefore, we introduce a matching cascade that gives priority to more
frequently seen objects to encode our notion of probability spread in
the association likelihood.

Listing~\ref{lst:matching_cascade} outlines our matching algorithm.
As input we provide the set of track~ and detection~
indices as well as the maximum age .
In lines~\ref{lst:cascade_cost_matrix} and~\ref{lst:cascade_gate_matrix} we
compute the association cost matrix and the matrix of admissible associations.
We then iterate over track age  to solve a linear assignment problem
for tracks of increasing age.
In line~\ref{lst:cascade_track_selection} we select the subset of
tracks~ that have not been associated with a detection in the
last~ frames.
In line~\ref{lst:cascade_solve} we solve the linear assignment between tracks
in~ and unmatched detections~.
In lines~\ref{lst:cascade_aggregate_matches}
and~\ref{lst:cascade_update_unmatched} we update the set of matches and
unmatched detections,
which we return after completion in line~\ref{lst:cascade_return}.
Note that this matching cascade gives priority to tracks of smaller age, i.e.,
tracks that have been seen more recently.

In a final matching stage, we run intersection over union association
as proposed in the original SORT algorithm~\cite{Bewley2016_sort}
on the set of unconfirmed and unmatched tracks of age . This helps to
to account for sudden appearance changes, e.g., due to partial occlusion with
static scene geometry, and to increase robustness against erroneous
initialization.

\subsection{Deep Appearance Descriptor}
\label{subsec:metric-learning}

\begin{table}[t!]
    \begin{tabular}{p{0.28\linewidth}cc}
        \toprule
        \textbf{Name} & \textbf{Patch Size/Stride} & \textbf{Output Size} \\
        \midrule
        Conv 1 & / &  \\
        Conv 2 & / &  \\
        Max Pool 3 & / &  \\
        Residual 4 & / &  \\
        Residual 5 & / &  \\
        Residual 6 & / &  \\
        Residual 7 & / &  \\
        Residual 8 & / &  \\
        Residual 9 & / &  \\
        Dense 10 & &  \\
        \multicolumn{2}{l}{Batch and  normalization} &  \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the CNN architecture. The final batch and 
    normalization projects features onto the unit hypersphere.}
\label{tab:network-architecture}
\end{table}

\begin{table*}[t!]
    \centering
    \begin{tabular}{@{}lccccccccc@{\hskip7pt}c@{}}
\toprule
 & \multicolumn{1}{l}{} & \textbf{MOTA}  & \textbf{MOTP}  & \textbf{MT}  & \textbf{ML}  & \textbf{ID}  & \textbf{FM}  & \textbf{FP}  & \textbf{FN}  & \textbf{Runtime}  \\
\midrule
KDNT~\cite{yu2016poi}\textsuperscript{} & BATCH & 68.2 & 79.4 & 41.0\% & 19.0\% & 933 & 1093 & 11479 & 45605 & 0.7\,Hz \\
LMP\_p~\cite{keuper2016multi}\textsuperscript{} & BATCH & \textbf{71.0} & \textbf{80.2} & \textbf{46.9\%} & 21.9\% & 434 & \textbf{587} & 7880 & \textbf{44564} & 0.5\,Hz \\
MCMOT\_HDM~\cite{lee2016multi} & BATCH & 62.4 & 78.3 & 31.5\% & 24.2\% & 1394 & 1318 & 9855 & 57257 & 35\,Hz \\
NOMTwSDP16~\cite{choi2015near} & BATCH & 62.2 & 79.6 & 32.5\% & 31.1\% & \textbf{406} & 642 & \textbf{5119} & 63352 & 3\,Hz \\
\midrule
EAMTT~\cite{sanchez2016online} & \textbf{ONLINE} & 52.5 & 78.8 & 19.0\% & 34.9\% & 910 & \textbf{1321} & \textbf{4407} & 81223 & 12\,Hz \\
POI~\cite{yu2016poi}\textsuperscript{} & \textbf{ONLINE} & \textbf{66.1} & 79.5 & \textbf{34.0\%} & 20.8\% & 805 & 3093 & 5061 & \textbf{55914} & 10\,Hz \\
        SORT~\cite{Bewley2016_sort}\textsuperscript{} & \textbf{ONLINE} & 59.8 & \textbf{79.6} & 25.4\% & 22.7\% & 1423 & 1835 & 8698 & 63245 & \textbf{60\,Hz} \\
Deep SORT (Ours)\textsuperscript{} & \textbf{ONLINE} & 61.4 & 79.1 & 32.8\% & \textbf{18.2\%} & \textbf{781} & 2008 & 12852 & 56668 & 40\,Hz \\
\bottomrule
\end{tabular}
    \caption{Tracking results on the MOT16~\cite{milan2016mot16} challenge.
    We compare to other published methods with non-standard detections.
    The full table of results can be found on the challenge website.
    Methods marked with \textsuperscript{} use detections provided by~\cite{yu2016poi}.}
\label{tab:mot-results}
\end{table*}

By using simple nearest neighbor queries without additional metric learning,
successful application of our method requires a well-discriminating feature
embedding to be trained offline, before the actual online tracking application.
To this end, we employ a CNN that has been
trained on a large-scale person re-identification
dataset~\cite{Zheng2016} that contains over~1,100,000 images of~1,261
pedestrians, making it well suited for deep metric learning in a people
tracking context.

The CNN architecture of our network is shown in
\tablename~\ref{tab:network-architecture}.
In summary, we employ a wide residual network~\cite{Zagoruyko2016}
with two convolutional layers followed by six residual blocks. The global
feauture map of dimensionality  is computed in dense layer 10.
A final batch and~ normalization projects features
onto the unit hypersphere to be compatible with our cosine appearance
metric.
In total, the network has 2,800,864 parameters and one forward pass
of 32 bounding boxes takes approximately~ on an Nvidia GeForce
GTX 1050 mobile GPU{}. Thus, this network is well suited for online tracking,
provided that a modern GPU is available.
While the details of our training procedure are out of the scope of this paper,
we provide a pre-trained model in our GitHub
repository~\footnote{\url{https://github.com/nwojke/deep_sort}}
along with a script that can be used to generate features.


\if0

At each time step of the online tracking scenario, we are given the
mean and covariance of  predicted Kalman states.
In addition, for each track we are given a
gallery~
that contains the last  associated appearance descriptors.
These have been generated from the bounding
boxes of associated detections by propagating the corresponding image patch
into representation space of the CNN described in
\sectionname~\ref{sec:metric_learning}.
Further, we are given a set of 
detections~ that contain
the bounding box coordinates  and their corresponding appearance
descriptor~.

Given a suitable association metric, the detection-to-track association can be
solved as a linear assignment problem, e.g., using the Hungarian
algorithm~\cite{Munkres1957}.
In the Kalman filtering framework, the usual choice for this metric is the
Mahalanobis distance

between observation  and projected Kalman state
.
However, this metric has limited discriminative capabilities in our image-space
filtering framework where unaccounted camera motion can lead to rapid
displacements between successive frames.
Therefore, we introduce a second metric based on the cosine distance between
the appearance descriptor of each detection and its nearest neighbor in the
gallery of each track:

We combine both metrics using a weighted sum and define an indicator that
evalutes to  if, given some pre-defined threshold , the association between
the -th track and -th detection is admissible and  otherwise:

Therefore, an association is admissible only if the detection is within a
specific gating region according to each of the two metrics.
We compute the threshold for the Mahalanobis gate from the inverse 
distribution at a  confidence interval and compute a suitable threshold
for the cosine distance on the MOT16~\cite{milan2016mot16} challenge training
set.
\fi

\section{EXPERIMENTS}
\label{sec:experiments}







\if0
\begin{table}[t!]
    \centering
    \begin{tabular}{p{1.2cm}ccccc}
        \toprule
        & \textbf{Precision} & \textbf{Recall} & \textbf{MOTA} & \textbf{MOTP} & \textbf{ID} \\
        \midrule
        \dots & & & & & \\
        SORT & & & & & \\
        \textbf{Ours} & & & & & \\
        \bottomrule
    \end{tabular}
    \caption{Tracking results on the MOT16~\cite{milan2016mot16} challenge. We limit our
    comparison to related online methods. The full table of results can be found on the challenge
    website.}
    \label{tab:mot-results}
\end{table}
\fi

We assess the performance of our tracker on the MOT16
benchmark~\cite{milan2016mot16}.
This benchmark evaluates tracking performance on seven challenging test
sequences, including frontal-view scenes with moving camera as well as top-down
surveillance setups.
As input to our tracker we rely on detections provided
by~Yu~\etal~\cite{yu2016poi}. They have trained a Faster RCNN on a collection
of public and private datasets to provide excellent performance.
For a fair comparison, we have re-run SORT on the same detections.

Evaluation on test sequences were carried out using~
and~ frames.
As in~\cite{yu2016poi}, detections have been thresholded at a confidence score
of~.
The remaining parameters of our method have been found on separate training
sequences which are provided by the benchmark.
Evaluation is carried out according to the following metrics:
\begin{itemize}[leftmargin=*, noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt]
\item Multi-object tracking accuracy (MOTA): Summary of overall tracking
    accuracy in terms of false positives, false negatives and identity
    switches~\cite{DBLP:journals/ejivp/BernardinS08}.
\item Multi-object tracking precision (MOTP): Summary of overall tracking
    precision in terms of bounding box overlap between ground-truth and
    reported location~\cite{DBLP:journals/ejivp/BernardinS08}.
\item Mostly tracked (MT): Percentage of ground-truth tracks that have the same
    label for at least 80\% of their life span.
\item Mostly lost(ML): Percentage of ground-truth tracks that are tracked for
    at most 20\% of their life span.
\item Identity switches (ID): Number of times the reported identity of a
    ground-truth track changes.
\item Fragmentation (FM): Number of times a track is interrupted by a missing
    detection.
\end{itemize}
The results of our evaluation are shown in \tablename~\ref{tab:mot-results}.
Our adaptions successfully reduce the number of identity switches.
In comparison to SORT, ID switches reduce from 1423 to 781.
This is a decrease of approximately 45\%.
At the same time, track fragmentation increase slightly due to maintaining
object identities through occlusions and misses.
We also see a significant increase in number of mostly tracked objects and
a decrease of mostly lost objects.
Overall, due to integration of appearance information we successfully maintain
identities through longer occlusions.
This can also be seen by qualitative analysis of the tracking output that
we provide in the supplementary material.
An exemplary output of our tracker is shown in
\figurename~\ref{fig:example-output}.

Our method is also a strong competitor to other online tracking frameworks.
In particular, our approach returns the fewest number of identity switches of
all online methods while maintaining competitive MOTA scores, track
fragmentations, and false negatives.
The reported tracking accuracy is mostly impaired by a larger number of false
positives.
Given their overall impact on the MOTA score, applying a larger confidence
threshold to the detections can potentially increase the reported performance
of our algorithm by a large margin.
However, visual inspection of the tracking output shows that these false
positives are mostly generated from sporadic detector responses at static scene
geometry.
Due to our relatively large
maximum allowed track age, these are more commonly joined to object
trajectories. At the same time, we did not observe tracks jumping between
false alarms frequently. Instead, the tracker commonly generated relatively
stable, stationary tracks at the reported object location.



Our implementation runs at approximately 20\,Hz with roughly half of the time
spent on feature generation.
Therefore, given a modern GPU, the system remains computationally efficient
and operates at real time.

\section{CONCLUSION}
\label{sec:conclusion}

We have presented an extension to SORT that incorporates appearance information
through a pre-trained association metric.
Due to this extension, we are able to track through longer periods of
occlusion, making SORT a strong competitor to state-of-the-art online
tracking algorithms. Yet, the algorithm remains simple to implement and
runs in real time.

\clearpage

\bibliographystyle{IEEEbib}
\bibliography{root}

\if 0
\newpage



\section{METRIC LEARNING}
\label{sec:metric_learning}
In a people tracking context, the goal of metric learning is to embed features
in a space such that images of the same identity are in close proximity while
distant from others.
Given enough training data, convolutional neural networks (CNN) are well-suited
to learn such an embedding in an end-to-end fashion.
Popular deep metric learning methods for this task are
contrastive~\cite{Chopra2005} and triplet~\cite{weinberger2009distance}
loss siamese networks. In addition,
Rippel~\etal~\cite{Rippel2016a} have proposed the multi-modal
magnet loss to overcome some learning inefficiencies of siamese
networks.
Others approach metric learning in a classification context by placing a
softmax classifier on top of the encoder network and taking the activations of
the last fully connected layer as representation space.
While there is no guarantee that this space behaves accoding to some predefined
metric, this approach has been applied successfully to various
applications~\cite{Taigman2014, Xiao2016, zheng2016person}.
Further, it has been argued that learning in a classification context can be
more efficient than direct metric learning
approaches~\cite{Rippel2016a, Taigman2014}.


\subsection{Dataset}

We use a large scale person re-identification dataset to learn our association
metric.
The MARS~\cite{Zheng2016} dataset contains~1,261 identities and
over~1,100,000 images taken from 6 cameras.
The data has been generated using a multi-target tracker that generates
tracklets, i.e., short-term track fragments, which have then been manually
annotated to consistent identities.
Consequently,
this dataset contains significant bounding box misalignment and inaccurate
labelling.
Since we can expect similar data characteristics in our online tracking
scenario, this dataset is well suited to learn an association metric for online
people tracking.

\subsection{Network Architecture}

An overview of our network architecture is given in
\tablename~\ref{tab:network-architecture}.
We mostly follow the ideas in~\cite{Zagoruyko2016} and build a small
residual network with only  convolutions.
Input images are scaled to  and presented to the network in
RGB color space.
We first apply two convolutional layers before max pooling.
The following 6 layers are pre-activation residual
blocks where we double the dimensionality and reduce the grid size in
layers~6 and~8.
We then use a fully connected layer with dimensionality  to extract
a global feature map.
Finally, we project all features onto the unit hypersphere using~
normalization.
We use batch normalization before the ELU activation function~\cite{Clevert2015}
in all layers.
During training, we randomly drop units from the fully connected layer with
probability~ and the same dropout rate is applied inside the residual
blocks to regularize the model.
During training, we place a softmax classifier on top of the encoder network
that is trained to identify the individuals of in the training set.
At test time, we use the cosine similarity to perform nearest neighbor queries.
The total number of parameters of this network is 2,800,864. One forward pass
of 32 images on an Nvidia Quadro M6000 GPU takes
approximately~.


\begin{table}[t!]
    \begin{tabular}{p{0.28\linewidth}cc}
        \toprule
        \textbf{Name} & \textbf{Patch Size/Stride} & \textbf{Output Size} \\
        \midrule
        Conv 1 & / &  \\
        Conv 2 & / &  \\
        Max Pool 3 & / &  \\
        Residual 4 & / &  \\
        Residual 5 & / &  \\
        Residual 6 & / &  \\
        Residual 7 & / &  \\
        Residual 8 & / &  \\
        Residual 9 & / &  \\
        Dense 10 & &  \\
        \multicolumn{2}{l}{ normalization} &  \\
        \bottomrule
    \end{tabular}
\caption{Network architecture with two convolutional layers followed by
    six residual blocks and a fully connected layer.}
\label{tab:network-architecture}
\end{table}

\subsection{Training}

The network was trained on the provided data splits. In addition, we split off
approximately 10\% of the training data for validation.
We further increased variation of input data by randomly flipping images and
changing brightness, contrast, and saturation.
The network was trained for 200,000 iterations using a batch size of 128 images.
For optimization we used Adam~\cite{Kingma2014} with learning rate 
and weight decay~.

In this section we describe our pre-trained feature space that is used to
match detections against to existing tracks.
This feature space is trained on a large-scale person re-identification
dataset.


\subsection{Learning Objective}

Given a dataset

of input images and associated class labels, the goal of metric learning is to
train a parametrized encoder function  that
transforms inputs into a representation space where samples of the same
identity appear in close proximity while distant from others.
This encoder function is modeled by a CNN and optimized using stochastic
gradient descent.
As loss function we chose the recently proposed magnet loss~\cite{Rippel2016a}.
This loss has been formulated to overcome shortcomings of the well-established
contrastive~\cite{Chopra2005} and triplet~\cite{weinberger2009distance}
loss functions and has shown promising performance on a fine-grained
classification task.
In its original formulation, the loss is formulated as a mixture model to cope
with multi-modal phenomena. We restrict ourself to a unimodal variant that can
be computed efficiently on the GPU.

Therefore, during each training iteration
we sample a fixed number of  classes and  images per
class.
Then, we keep an explicit representation of the distribution of each class in
feature space.
More specifically, we model the distribution of class~ as
spherical Gaussian 
with mean~ and
covariance~.
The parameters of this distribution are computed on the GPU for each batch
individually:
Let~ denote a set of classes that are present in the current data batch.
Further, let 
denote a set of indices that refers to the  samples of
class~ in the current batch.
Then,

is the mean of class  and the variance of all samples away
from their respective means is

Given these class distributions, we compute the magnet loss~\cite{Rippel2016a}
as follows:

where  is the hinge function and  is a
margin parameter.
This loss quantifies class separation in feature space and converges to 
if the gap between all classes approaches  standard deviations.

\subsection{Network Architecture}

\begin{table}[t!]
    \begin{tabular}{p{0.28\linewidth}cc}
        \toprule
        \textbf{Name} & \textbf{Patch Size/Stride} & \textbf{Output Size} \\
        \midrule
        Conv 1 & / &  \\
        Conv 2 & / &  \\
        Max Pool 3 & / &  \\
        Residual 4 & / &  \\
        Residual 5 & / &  \\
        Residual 6 & / &  \\
        Residual 7 & / &  \\
        Residual 8 & / &  \\
        Residual 9 & / &  \\
        Dense 10 & &  \\
        \multicolumn{2}{l}{L2 normalization} &  \\
        \bottomrule
    \end{tabular}
    \caption{Our network architecture consists of two convolutions followed by
    three residual blocks with two modules each and a fully connected
    layer.}
\label{tab:network-architecture}
\end{table}

An overview of our network architecture is given in
\tablename~\ref{tab:network-architecture}.
We mostly follow the ideas in~\cite{Zagoruyko2016} and build a small
residual network with only  convolutions.
Input images are scaled to  and presented to the network in
RGB color space.
We first apply two convolutional layers before max pooling.
The following 6 layers are pre-activation residual
blocks where we double the dimensionality and reduce the grid size in
layers~6 and~8.
We then use a fully connected layer with dimensionality  to extract
a global feature map.
We use the ELU activation function~\cite{Clevert2015} in all layers.
During training, we randomly drop units from the fully connected layer with
probability~ and the same dropout rate is applied inside the residual
blocks to regularise the model.


\subsection{Training}

The network was trained on the MARS~\cite{Zheng2016} dataset using the provided
data split. MARS contains over half a million training images taken from six
cameras and is, therefore, well suited for deep learning.
We further increased variation of input data by randomly flipping images and
changing brightness, contrast, and saturation.
The network was trained for 50,000 iterations using a batch size of 128 images
using  classes per batch and  images per class.
For optimization we used Adam~\cite{Kingma2014} with learning rate 
and weight decay .
\fi

\if0
\subsection{Person Re-Identification}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/mars-cmc-curve}
    \caption{CMC curves on the MARS~\cite{Zheng2016} dataset. We compare
    ourselfs to the baseline methods of~\cite{Zheng2016} with pre-trained
    ImageNet models.}
    \label{fig:results-mars}
\end{figure}

\begin{table}[t!]
    \newcommand{\RankN}[1]{#1}
    \newcommand{\C}[1]{\multicolumn{1}{c}{#1}}
    \definecolor{SeqColor}{rgb}{0.9,0.9,0.9}
    \centering
    \begin{tabular}{>{\quad}
            p{0.36\linewidth}
            >{\RaggedLeft\arraybackslash}p{0.09\linewidth}
            >{\RaggedLeft\arraybackslash}p{0.09\linewidth}
            >{\RaggedLeft\arraybackslash}p{0.09\linewidth}
            >{\RaggedLeft\arraybackslash}p{0.09\linewidth}
            }
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Rank}} & \textbf{mAP} \\
        & \C{\textbf{\RankN{1}}} & \C{\textbf{\RankN{5}}} & \C{\textbf{\RankN{20}}} & \\
        \midrule
        \rowcolor{SeqColor}
            \multicolumn{5}{l}{IDE~\cite{Zheng2016}} \\
        + XQDA~\cite{Liao2015} & 65.5 & \textbf{81.7} & 90.1 & 46.9 \\
        + KISSME~\cite{Kostinger2012} & 64.3 & 80.2 & 88.6 & 47.5 \\
        Euclidean & 58.0 & 76.8 & 86.9 & 39.5 \\
        \rowcolor{SeqColor}
            \hspace{-1em}\textbf{Ours} & \textbf{67.3} & 81.5 & \textbf{91.2} & \textbf{49.6} \\  \bottomrule
    \end{tabular}
    \caption{Performance comparison on the MARS~\cite{Zheng2016} datasets using
    CMC and mAP scores.}
    \label{tab:results-mars}
\end{table}

In order to assess the re-identification capabilities of our deep association
metric, we evaluate our network on the MARS dataset~\cite{Zheng2016}.
Following the standard evaluation protocol, we evaluate in a
\textit{video to video} mode where each tracklet is represented by a single
feature descriptor.
Then, probe to gallery rankings are established at tracklet level.
Given a set of images from the same tracklet, we use the mean direction of
their corresponding locations in representation space as a feature vector.
Then, we establish the ranking at tracklet level using cosine similarity.
Evaluation is carried out using the  provided evaluation software and we report
cumulative matching characteristics~(CMC) curves and mean average
precision~(mAP) on the provided data splits.
\figurename~\ref{fig:results-mars} and \tablename~\ref{tab:results-mars}
show the results of our evaluation\footnote{We have re-generated all numbers using the provided evaluation software and
pre-computed features. We obtain slightly different results than in
the original publication~\cite{Zheng2016}.}.
Despite using a much smaller network and no additional metric learning, our
method provides slightly better recognition rate than the best method
of Zheng~\etal~\cite{Zheng2016}.
They train an AlexNet with over 60 million parameters and apply additional
metric learning on the representation space.
With less than 3 million parameters, our network surpasses the best
mAP score by a margin of 2.7.
Further, note that in contrast to~\cite{Zheng2016}, we did not pre-train our
model on ImageNet.
They report between  and  recognition rate is due to this
initialisation.
Therefore, we may expect further improvements by using a pre-initialized model.

\subsection{Tracking}
\fi


\end{document}
