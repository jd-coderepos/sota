\documentclass{article}

\PassOptionsToPackage{numbers,square}{natbib}


\usepackage[final]{neurips_2021}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{color}
\usepackage{csquotes}
\usepackage{soul}
\usepackage{enumerate}
\usepackage[normalem]{ulem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{array}
\usepackage{rotating}
\usepackage{arydshln}
\usepackage{ragged2e}
\usepackage{wrapfig}
\newcommand\eg{\emph{e.g.}} \newcommand\Eg{\emph{E.g.}}
\newcommand\ie{\emph{i.e.}} \newcommand\Ie{\emph{I.e.}}
\newcommand\cf{\emph{c.f.}} \newcommand\Cf{\emph{C.f.}}
\newcommand\etc{\emph{etc.}}
\newcommand\wrt{w.r.t.} \newcommand\dof{d.o.f.}
\newcommand\etal{\emph{et al.}}
\usepackage{algorithm}
\usepackage{algorithmicx}

\def\Jing#1{{\color{magenta}{\bf [Jing:} {\it{#1}}{\bf ]}}}

\def\Rev#1{\textcolor{blue}{#1}}


\usepackage[colorlinks=true,citecolor=blue]{hyperref}

\makeatletter
\newcommand{\settitle}{\@maketitle}
\makeatother

\title{
Learning Generative Vision Transformer with Energy-Based Latent Space for Saliency Prediction
}

\author{Jing Zhang$^{1}$,
  Jianwen Xie$^{1}$,
  Nick Barnes$^{2}$,
  Ping Li$^{1}$\\
$^{1}$ Cognitive Computing Lab, 
Baidu Research\\ $^{2}$ The Australian National University \\
\ \{zjnwpu,\ jianwen.kenny,\ pingli98\}@gmail.com,  nick.barnes@anu.edu.au 
}

\begin{document}

\maketitle

\begin{abstract}
Vision transformer networks have shown superiority in many computer vision tasks. In this paper, we take a step further by proposing a novel generative vision transformer with latent variables following an informative energy-based prior for salient object detection. Both the vision transformer network and the energy-based prior model are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. Further, with the generative vision transformer, we can easily obtain a pixel-wise uncertainty map from an image, which indicates the model confidence in predicting saliency from the image. Different from the existing generative models which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive to capture the latent space of the data. We apply the proposed framework to both RGB and RGB-D salient object detection tasks.
Extensive experimental results show that our framework can achieve not only accurate saliency predictions but also meaningful uncertainty maps that are consistent with the human perception. 
\end{abstract}

\section{Introduction}\label{intro_sec}

In the field of computer vision, salient object detection~\cite{wei2020f3net,wei2020label,fan2020bbs,Fu2020JLDCF,chen2018progressively,jing2020weakly} (SOD) or visual saliency prediction, which aims at highlighting objects more attentive than the surrounding areas in images, has achieved significant performance improvement with the deep convolutional neural network revolution.
Given a set of training images along with their saliency annotations, the conventional SOD models seek to learn a deterministic one-to-one mapping from image domain to saliency domain.

Two main issues exist in the above conventional deep saliency prediction framework: (i) the convolution operation based on sliding window makes the deep saliency prediction model less effective in modeling the global contrast of the image, which is essential for salient object detection~\cite{global_contrast}; (ii) the one-to-one deterministic mapping mechanism makes the current framework not only impossible to represent the pixel-wise uncertainty in predicting salient objects, but also hard to handle incomplete data in a weakly supervised scenario~\cite{jing2020weakly}. 
Besides, given an image, the saliency output of a human is subjective, therefore, a stochastic generative model is more natural than a deterministic model for representing saliency prediction. Although~\cite{ucnet_sal} introduces a conditional variational autoencoder (CVAE)~\cite{structure_output} for RGB-D salient object detection, the potential posterior collapse problem~\cite{Lagging_Inference_Networks} makes the stochastic predictions less effective in generating meaningful uncertainty estimation.

 Motivated by the above two issues, we propose a novel framework, the generative vision transformer, for salient object detection, where a vision transformer structure~\cite{liu2021swin} is used as a backbone and latent variables are introduced in designing our generative framework. 
On the one hand, transformers~\cite{transformer_nips} have proven to be very effective in long-range dependency modeling, and are capable of modeling various scopes of object context information with the multi-head self-attention module. With such an architecture,
we can achieve global context modeling for effective salient object detection. On the other hand, the latent variables account for randomness and uncertainty in modeling the mapping from image domain to saliency domain, and also enable the model to produce stochastic saliency predictions for uncertainty estimation. Therefore, the proposed model is a latent variable transformer. 

Nowadays, there are two types of generative models that have been widely used, namely the variational autoencoder (VAE)~\cite{vae_bayes_kumar} and the generative adversarial network (GAN)~\cite{GAN_nips},  which correspond to two different generative learning strategies to train latent variable models. To train a top-down latent variable generator, the VAE introduces an extra encoder to approximate the intractable posterior distribution of the latent variables, and trains the generator via a perturbation of maximum likelihood; while the GAN introduces a discriminator to distinguish between generated samples and real data, and trains the generator to fool the discriminator.~\cite{abp,xie2019learning} present the third learning strategy, namely alternating back-propagation (ABP), to train the generator 
with latent variables being directly sampled from the true posterior distribution by using a gradient-based Markov chain Monte Carlo (MCMC)~\cite{liu2008monte}, e.g., Langevin dynamics~\cite{neal2011mcmc,WellingT11,DubeyRWPSX16}. All the three generative models define the prior distribution of the latent variables as a simple non-informative isotropic Gaussian distribution, which is less expressive in capturing meaningful latent representation of the data.

In this paper, we investigate generative modeling and learning of the vision transformer. We construct a generative model for salient object detection in the form of a top-down conditional latent variable model. Specifically, we propose a generative vision transformer by adding latent variables into the traditional deterministic vision transformer, and assume the latent variables follow an informative trainable energy-based prior distribution~\cite{ebm_prior,PangW21}. Following~\cite{xie2016theory}, we parameterize the energy function of the energy-based model (EBM) by a deep net. Instead of using variational learning or adversarial learning, we jointly train the parameters of the EBM prior and the transformer network by maximum likelihood estimation (MLE). The MLE algorithm relies on MCMC sampling to evaluate the intractable prior and posterior distributions of the latent variables.   

Experimental results on RGB and RGB-D salient object detections~\cite{wei2020f3net,scrn_sal,ucnet_sal,fan2020bbs} show that the generative framework equipped with the EBM prior and the transformer-based non-linear mapping is powerful in representing the conditional distribution of object saliency given an image, leading to more reasonable uncertainty estimation as shown in Figure~\ref{fig:framework_outputs}, where stochastic saliency prediction is provided by a learned model and the visualization of the pixel-wise uncertainty is presented.   



\begin{figure*}[h]
   \begin{center}
\includegraphics[width=1\linewidth]{imgs/prediction_gen_uncertainty.png}
   \end{center}
   \vspace{-0.1in}
   \caption{An illustration of the stochastic saliency prediction obtained by the proposed generative vision transformer with an EBM prior, as well as the corresponding pixel-wise uncertainty map.}
\label{fig:framework_outputs}
\end{figure*}

We summarize our main contributions and novelties as follows: (i) we propose a novel top-down generative vision transformer network with an energy-based prior distribution defined on latent space for salient object detection; (ii) we jointly train the vision transformer network and the energy-based prior model by an MCMC-based maximum likelihood estimation, without relying on any extra assisting network for adversarial learning or variational learning; (iii) we achieve new benchmark results for both RGB and RGB-D salient object detections, and obtain meaningful uncertainty maps that are highly consistent with human perception for saliency prediction.














\section{Related Work}


\noindent\textbf{Salient object detection:}
The main goal of the existing deep fully-supervised RGB image-based salient object detection models~\cite{cpd_sal,nldf_sal,scrn_sal,wei2020f3net,wang2020progressive,zhang2020learning_eccv,wei2020label} is to achieve structure preserving saliency prediction, by either 
sophisticated feature aggregation~\cite{cpd_sal,wang2020progressive}, auxiliary edge detection~\cite{scrn_sal,qin2019basnet,wei2020label}, or resorting to structure-aware loss functions~\cite{nldf_sal,wei2020f3net}.
With extra depth information, RGB-D salient object detection models \cite{qu2017rgbd,ucnet_sal,chen2018progressively,ji2021cal,zhao2019Contrast,ssf_rgbd,fan2020bbs,ji2020accurate,piao2019depth,zhang2020bilateral,Sun_2021_CVPR_DSA2F,Zhang_2021_ICCV_RGBD} mainly focus on effective multi-modal modeling. Our paper solves the same problems, i.e., RGB and RGB-D salient object detection, by developing a new generative transformer-based framework.  




\noindent\textbf{Vision transformers:}
The breakthroughs of the Transformer networks~\cite{transformer_nips} in natural language processing (NLP) domain have sparked the interest of the computer vision community in developing vision transformers for different computer vision tasks, such as image classification \cite{dosovitskiy_ViT_ICLR_2021,liu2021swin}, object detection~\cite{carion_DETR_ECCV_2020, wang_PVT_2021,mvt_multi_view_transformer,liu2021swin}, image segmentation~\cite{zheng_SETR_2020, dpt_transformer, wang_PVT_2021, liu2021swin}, object tracking~\cite{xu_TransCenterTracking_2021, yan_SpatialTemporalTransformerTrackingv2_2021}, pose estimation~\cite{mao_TFPose_2021,stoffl_InstancePose_2021}, etc.
Among them, DPT~\cite{dpt_transformer} adopts a U-shape structure and uses ViT~\cite{dosovitskiy_ViT_ICLR_2021} as an encoder to perform semantic segmentation and monocular depth estimation.
Swin~\cite{liu2021swin} presents a hierarchical transformer with a shifted windowing scheme to achieve an efficient transformer network with high resolution images as input. Different from the above vision transformers that mainly focus on discriminative modeling and learning, our paper emphasizes generative modeling  and learning of the vision transformer by involving latent variables and MCMC inference.


\noindent\textbf{Dense prediction with generative models:}
VAEs have been successfully applied to image segmentation \cite{PHiSeg2019, probabilistic_unet}. For saliency prediction,~\cite{SuperVAE_AAAI19} adopts a VAE to model the image background, and separates salient objects from the background through reconstruction residuals.~\cite{ucnet_sal,jing2020uncertainty} design CVAEs to model the subjective nature of saliency. GAN-based methods can be divided into two categories, namely fully-supervised and semi-supervised settings. The former~\cite{groenendijk2020benefit,gan_maskerrcnn} uses the discriminator to distinguish model predictions from ground truths, while the latter~\cite{gan_semi_seg,hung2018adversarial} uses the GAN to explore the contribution of unlabeled data. \cite{zhang2021energy} uses a cooperative learning framework \cite{xie2018cooperative,xie2021cooperative} for generative saliency prediction. ~\cite{jing2020uncertainty} trains a single top-down generator in the ABP framework for RGB-D saliency prediction. Our model generalizes \cite{jing2020uncertainty} by replacing the simple Gaussian prior by a learnable EBM prior and adopting a vision transformer-based generator for salient object prediction. 

\noindent\textbf{Energy-based models:} Recent works have shown strong performance of data space EBMs \cite{xie2016theory,nijkamp2019learning} in modeling high-dimensional complex dependencies, such as images \cite{ZhengXL21,ZhaoXL21,GaoLZZW18,DuM19,GaoSPWK21}, videos \cite{XieZW17,XieZW21}, 3D shapes \cite{XieZGWZW18,xie2020generative}, and point clouds \cite{XieXZZW21}, and also demonstrated the effectiveness of latent space EBMs \cite{ebm_prior} in improving the model expressivity for text \cite{PangW21}, image \cite{ebm_prior}, and trajectory \cite{PangZ0W21} generation. Our paper also learns a latent space EBM as the prior model but builds the EBM on top of a vision transformer generator for image-conditioned saliency map prediction.




\section{Generative Vision Transformer with Energy-Based Latent Space}


\subsection{Model}


We formulate the supervised saliency prediction problem as a conditional generative learning problem. Let $\textbf{I} \in \mathbb{R}^{h \times w \times 3}$ be an observed RGB image, $s \in \mathbb{R}^{h \times w \times 1}$ be the saliency map, and $z \in \mathbb{R}^{1 \times 1 \times d}$ be the $d$-dimensional vector of latent variables, where $h \times w  \gg d$. Consider the following generative model to predict a saliency map $s$ from an image $\textbf{I}$,
\begin{equation}
     s=T_{\theta}(\textbf{I}, z)+ \epsilon, \hspace{3mm} z \sim p_\alpha(z), \hspace{3mm} \epsilon \sim \mathcal{N}(0,\sigma_\epsilon^2 I),   \label{eq:G_Transformer}
\end{equation}
where $T_\theta$ is the non-linear mapping process from $[z,\textbf{I}]$ to $s$ with parameters $\theta$, $p_\alpha(z)$ is the prior distribution with parameters $\alpha$, and $\epsilon \sim \mathcal{N}(0,\sigma_\epsilon^2 I)$ is the observation residual of saliency with $\sigma_\epsilon$ being given. Due to the stochasticity of the latent variables $z$, given an image $\textbf{I}$, its saliency map is also stochastic. Such a probabilistic model is in accord with the uncertainty of the image saliency.  

Following~\cite{ebm_prior}, the prior $p_{\alpha}(z)$ is not assumed to be a simple isotropic Gaussian distribution as GAN~\cite{GAN_nips}, VAE~\cite{vae_bayes_kumar, structure_output} or ABP~\cite{abp}. 
Specifically, it is in the form of the energy-based correction or exponential tilting~\cite{xie2016theory} of an isotropic Gaussian reference distribution $p_0(z)=\mathcal{N}(0,\sigma_z^2I)$, i.e., 
\begin{equation}
     p_\alpha(z)= \frac{1}{Z(\alpha)} \exp \left[-U_{\alpha}(z) \right] p_{0}(z) \propto \exp \left[-U_{\alpha}(z)-\frac{1}{2\sigma_z^2}||z||^2 \right], \label{eq:ebm_prior}
\end{equation}
where $\mathcal{E}_{\alpha}(z)=U_{\alpha}(z)+\frac{1}{2\sigma_{z}^2}||z||^2$ is the energy function that maps the latent variables $z$ to a scalar, and $U_{\alpha}(z)$ is parameterized by a multi-layer perceptron (MLP) with trainable parameters $\alpha$. The standard deviation $\sigma_z$ is a hyperparameter. $Z(\alpha)=\int \exp[-U_{\alpha}(z)]p_0(z)dz$ is the intractable normalizing constant that resolves the requirement for a probability distribution to have a total probability equal to one. $p_{\alpha}(z)$ is an informative prior distribution in our model and its parameters $\alpha$ need to be estimated along with the non-linear mapping function $T_{\theta}$ from the training data.

The mapping function $T_{\theta}$ is parameterized by a vision  transformer~\cite{liu2021swin} with self-attention mechanism, which encodes the input image $\textbf{I}$ and then decodes it along with the vector of latent variables $z$ to the saliency map $s$, thus, $p_{\theta}(s|\textbf{I},z)=\mathcal{N}(T_{\theta}(\textbf{I},z),\sigma_{\epsilon}^2 I)$. The resulting generative model is a conditional directed graphical model that combines the EBM prior~\cite{ebm_prior} and the vision transformer~\cite{liu2021swin}.  

\subsection{Learning}



The generative transformer with an energy-based prior, which is presented in Eq.~(\ref{eq:G_Transformer}), can be trained via maximum likelihood estimation. For notation simplicity, let $\beta=(\theta, \alpha)$. For the training examples $\{(\textbf{I}_i, s_i),i=1,...,n\}$, the observed-data log-likelihood function is defined as
\begin{equation}
    L(\beta)=\sum_{i=1}^{n} \log p_{\beta}(s_i|\textbf{I}_i)=\sum_{i=1}^{n} \log \left[\int p_{\beta}(s_i,z_i|\textbf{I}_i)dz_i \right]=\sum_{i=1}^{n} \log \left[\int p_{\alpha}(z_i)p_{\theta}(s_i|\textbf{I}_i,z_i)dz_i \right]. \nonumber \end{equation}
Maximizing $L(\beta)$ is equivalent to minimizing the Kullback-Leibler (KL) divergence between the model $p_{\beta}(s|\textbf{I})$ and the data distribution $p_{\text{data}}(s|\textbf{I})$. The gradient of $L(\beta)$ can be computed based on
\begin{equation}
    \nabla_{\beta} \log p_{\beta}(s|\textbf{I})= \text{E}_{p_{\beta}(z|s,\textbf{I})} \left[\nabla_{\beta} \log p_{\beta}(s,z|\textbf{I}) \right]=\text{E}_{p_{\beta}(z|s,\textbf{I})}[\nabla_{\beta} (\log p_{\alpha}(z)+  \log p_{\theta}(s|\textbf{I},z))], \label{eq:gradient}
\end{equation}
where the posterior distribution $p_{\beta}(z|s,\textbf{I})=p_{\beta}(s,z|\textbf{I})/p_{\beta}(s|\textbf{I})=p_{\alpha}(z)p_{\theta}(s|\textbf{I},z)/p_{\beta}(s|\textbf{I})$. 



The learning gradient in Eq.~(\ref{eq:gradient}) can be decomposed into two parts, i.e., the gradient for the energy-based model $\alpha$
\begin{equation}
    \text{E}_{p_{\beta}(z|s,\textbf{I})}[\nabla_{\alpha} \log p_{\alpha}(z)]= \text{E}_{p_{\alpha}(z)}[\nabla_{\alpha} U_{\alpha}(z)]-\text{E}_{p_{\beta}(z|s,\textbf{I})}[\nabla_{\alpha} U_{\alpha}(z)], \label{eq:prior_gradient}
\end{equation}
and the gradient for the transformer $\theta$
\begin{equation}
    \text{E}_{p_{\beta}(z|s,\textbf{I})}[\nabla_{\theta} \log p_{\theta}(s|\textbf{I},z)]=\text{E}_{p_{\beta}(z|s,\textbf{I})}\left[\frac{1}{\sigma_{\epsilon}^2}(s-T_{\theta}(\textbf{I},z)) \nabla_{\theta} T_{\theta}(\textbf{I},z)\right]. \label{eq:transformer_gradient}
\end{equation}
$\nabla_{\alpha} U_{\alpha}(z)$ in Eq.~(\ref{eq:prior_gradient}) and
$\nabla_{\theta} T_{\theta}(\textbf{I},z)$ in Eq.~(\ref{eq:transformer_gradient}) can be efficiently computed via back-propagation. Both Eq.~(\ref{eq:prior_gradient}) and Eq.~(\ref{eq:transformer_gradient}) include intractable expectation terms $\text{E}_{p}(\cdot)$, which can be approximated by MCMC samples. To be specific, we can use a gradient-based MCMC, e.g., Langevin dynamics, which is initialized with a Gaussian noise distribution $p_0$, to draw samples from the energy-based prior model $p_{\alpha}(z) \propto \exp \left[-\mathcal{E}_{\alpha}(z)\right]$ by iterating
\begin{equation}
    z_{t+1} = z_{t} - \delta \nabla_z \mathcal{E}_{\alpha}(z_t) + \sqrt{2\delta} e_t, \hspace{2mm} z_0\sim p_{0}(z), \hspace{2mm} e_t \sim \mathcal{N}(0,I),
    \label{eq:MCMC_prior}
\end{equation}
and draw samples from the posterior distribution $p_{\beta}(z|s, \textbf{I})$ by iterating
\begin{equation}
    z_{t+1} = z_{t} - \delta  \left[\nabla_{z} \mathcal{E}_{\alpha}(z_t)-\frac{1}{\sigma_{\epsilon}^2}(s-T_{\theta}(\textbf{I},z_t)) \nabla_{z} T_{\theta}(\textbf{I},z_t) \right] + \sqrt{2\delta} e_t, \hspace{2mm} z_0\sim p_{0}(z), \hspace{2mm} e_t \sim \mathcal{N}(0,I).
    \label{eq:MCMC_post}
\end{equation}
$\delta$ is the Langevin step size and can be specified independently in Eq.~(\ref{eq:MCMC_prior}) and Eq.~(\ref{eq:MCMC_post}). We use $\{z_i^{+}\}$ and $\{z_i^{-}\}$ to denote, respectively, the samples from the posterior distribution $p_{\beta}(z|s,\textbf{I})$ and the prior distribution $p_{\alpha}(z)$. The gradients of $\alpha$ and $\theta$ can be computed with $\{(\textbf{I}_i, s_i)\}$, $\{z_i^{+}\}$ and $\{z_i^{-}\}$ by
\begin{align}
    \nabla{\alpha}&= \frac{1}{n}\sum_{i=1}^{n}[\nabla_{\alpha} U_{\alpha}(z_i^{-})]- \frac{1}{n}\sum_{i=1}^{n} \left[\nabla_{\alpha} U_{\alpha}(z_{i}^{+})\right],  \label{eq:prior_gradient_MCMC}
\\
    \nabla{\theta}&= \frac{1}{n}\sum_{i=1}^{n}\left[\frac{1}{\sigma_{\epsilon}^2}(s_i-T_{\theta}(\textbf{I}_i,z_i^{+})) \nabla_{\theta} T_{\theta}(\textbf{I}_i,z_i^{+})\right] ,\label{eq:posterior_gradient_MCMC} \end{align}
We can update the parameters with $\nabla{\alpha}$ and $\nabla{\theta}$ via the Adam optimizer~\cite{kingma2014adam}. We present the full learning and sampling algorithm of our model in Algorithm~\ref{alg1}.


\begin{algorithm}[H]
\small
\caption{Maximum likelihood learning algorithm for generative vision transformer with energy-based latent space for saliency prediction}
\textbf{Input}: (1) Training images $\{\textbf{I}_i\}_{i}^{n}$ with associated saliency maps $\{s_i\}_{i}^{n}$;
(2) Number of learning iterations $M$; (3) Numbers of Langevin steps for prior and posterior $\{K^{-},K^{+}\}$; (4) Langevin step sizes for prior and posterior $\{\delta^{-},\delta^{+}\}$; (5) Learning rates for energy-based prior and transformer $\{\xi_\alpha,\xi_\theta\}$; (6) batch size $n'$.\\
\textbf{Output}: 
Parameters $\theta$ for the transformer and $\alpha$ for the energy-based prior model
\begin{algorithmic}[1]
\State Initialize $\theta$ and $\alpha$ 
\For{$m \leftarrow  1$ to $M$}
\State Sample observed image-saliency pairs $\{(\textbf{I}_i,s_i)\}_i^{n'}$
\State For each $(\textbf{I}_i,s_i)$, sample the prior $z_i^{-} \sim p_{\alpha_m}(z)$ using $K^{-}$ Langevin steps in Eq.~(\ref{eq:MCMC_prior}) with a step size $\delta^{-}$. 
\State For each $(\textbf{I}_i,s_i)$, sample the posterior $z_i^{+} \sim p_{\beta_m}(z|s_i,\textbf{I}_i)$ using $K^{+}$ Langevin steps in  Eq.~(\ref{eq:MCMC_post}) with a step size $\delta^{+}$. 
\State Update energy-based prior by Adam with the gradient $\nabla{\alpha}$ computed in Eq.~(\ref{eq:prior_gradient_MCMC}) and a learning rate $\xi_\alpha$.
\State Update transformer by Adam with the gradient $\nabla{\theta}$ computed in Eq.~(\ref{eq:posterior_gradient_MCMC}) and a learning rate $\xi_\theta$.
\EndFor
\end{algorithmic} \label{alg1}
\end{algorithm}




\subsection{Network}\label{network_sec}


\noindent\textbf{Generative vision transformer:}
We design the generative vision transformer using the Swin transformer~\cite{liu2021swin} backbone as shown in Figure~\ref{fig:generative_transformer}, which takes a three-channel image $\textbf{I}$ and the latent variables $z$ as input and outputs a one-channel saliency map $T_\theta(\textbf{I},z)$. Two main modules are included in our generative vision transformer, including a \enquote{Transformer Encoder} module and a \enquote{Feature Aggregation} module. The former takes $\textbf{I}$ as input and produces a set of feature maps $\{f_l\}_{l=1}^5$ of channel sizes 128, 256, 512, 1024 and 1024, respectively, while the latter takes $\{f_l\}_{l=1}^5$ and the vector of latent variables $z$ as input to generate the saliency prediction $s$.
\begin{wrapfigure}{r}{8.2cm}
\centering
\includegraphics[width=1.0\linewidth]{imgs/generative_transformer.png}
\caption{Generative latent variable vision transformer}
\label{fig:generative_transformer}
\end{wrapfigure}
Specifically, we first feed each $f_l$ to a $3\times3$ convolutional layer to reduce the channel dimension to $32$, and obtain a new set of feature maps $\{f'_l\}_{l=1}^5$ after channel reduction. Then, we replicate the vector $z$ spatially and perform a channel-wise concatenation with
$f'_5$, followed by a $3\times3$ convolutional layer that seeks to produce a feature map $F_5$ with same number of channels as that of $f'_5$.
Finally, we sequentially concatenate feature maps from  high level to low level via feature aggregation, i.e., from $l=4$ to 1, we compute $F_l=\text{Conv}_{3 \times 3}(\text{M}(\text{Concat}(f'_l,F_{l+1},...,F_5)))$,  where $\text{Conv}_{3 \times 3}(\cdot)$ is a $3\times3$ convolutional layer that reduces the channel dimension to $32$, $\text{M}(\cdot)$ is the channel attention module~\cite{rca_eccv}, and $\text{Concat}(\cdot)$ is the channel-wise concatenation operation. Note that, we upsample the higher level feature map to the same spatial size as that of the lower level feature map before the concatenation operation. We feed $F_1$ to a $3\times3$ convolutional layer to obtain
the one-channel saliency map $T_\theta(\textbf{I},z)$.




\noindent\textbf{Energy-based prior model:}
We design an energy-based model for the latent variables $z$ by parameterizing the function $U_{\alpha}(z)$ via a multilayer perceptron (MLP), which uses three fully connected layers to map the latent variables $z$ to a scalar. The sizes of the feature maps of different layers of the MLP are $C_e,C_e$ and $1$, respectively. We will simply use $C_e$ to represent the size of the EBM prior and set $C_e=60$ in our experiment. GELU~\cite{hendrycks2016gaussian} activation is used after each layer except the last one. 

\subsection{Analysis}


\noindent\textbf{Convergence:} Theoretically, when the Adam optimizer of $\beta=(\theta,\alpha)$ in the learning algorithm converges to a local minimum, it solves the following estimating equations
\begin{align}
\nabla{\alpha}=0  \hspace{2mm} &\Rightarrow  \hspace{2mm} \frac{1}{n}\sum_{i=1}^{n}\text{E}_{p_{\beta}(z_i|s_i,\textbf{I}_i)}[\nabla_{\alpha} U_{\alpha}(z_i)]-\text{E}_{p_{\alpha}(z)}[\nabla_{\alpha} U_{\alpha}(z)]=0, \label{eq:eq2} \\
\nabla{\theta}=0 \hspace{2mm} &\Rightarrow  \hspace{2mm} \frac{1}{n}\sum_{i=1}^{n}\text{E}_{p_{\beta}(z_i|s_i,\textbf{I}_i)}\left[\frac{1}{\sigma_{\epsilon}^2}(s_i-T_{\theta}(\textbf{I}_i,z_i)) \nabla_{\theta} T_{\theta}(\textbf{I}_i,z_i)\right]=0, \label{eq:eq1} 
\end{align}
which are the maximum likelihood estimating equations. However, in practise, the Langevin dynamics in Eq.~(\ref{eq:prior_gradient_MCMC}) and Eq.~(\ref{eq:posterior_gradient_MCMC}) might not converge to the target distributions due to the use of a small number of Langevin steps (i.e., short-run MCMC), the estimating equations in Eq.~(\ref{eq:eq2}) and Eq.~(\ref{eq:eq1}) will correspond to a perturbation of the MLE estimating equation according to~\cite{nijkamp2019learning,nijkamp2020learning,ebm_prior}. The learning algorithm can be justified as a Robbins-Monro~\cite{robbins1951stochastic} algorithm, whose convergence is theoretically sound. Our model can also be trained with an extra encoder as an amortized inference network~\cite{vae_bayes_kumar} for $p_{\beta}(z|s,\textbf{I})$  and an extra generator as an amortized sampling network~\cite{XieLGW18,xie2018cooperative,XieZL21} for $p_{\alpha}(z)$ . In this work, we prefer to keep our training algorithm simple in order to avoid extra efforts for the design of the auxiliary network architectures. We will study the joint training strategy in our future work.

 \begin{figure*}[t]
   \begin{center}
   \includegraphics[width=0.90\linewidth]{imgs/results_show.png}
   \end{center}
   \caption{Visual comparison of our  model and the state-of-the-art saliency prediction model, the BBSNet~\cite{fan2020bbs}. From top to bottom: images, ground truth saliency maps, results of the BBSNet~\cite{fan2020bbs} and results obtained by our model.
   }\label{fig:visual_comparison}
\end{figure*}

\noindent\textbf{Accuracy:} Compared with the GAN-based generative framework, our model is a likelihood-based generative framework that will not suffer from mode collapse~\cite{AroraRZ18}. In comparison with VAE-based generative framework, whose training is also based on likelihood, our MCMC-based maximum likelihood learning algorithm will not encounter the posterior collapse issue that is caused by amortized inference. On the other hand, the variational inference typically relies on an extra inference network for efficient inference of the latent variables given an image and saliency pair, however, the approximate inference might not be able to take the full place of the posterior distribution in practise. To be specific, we use $q_{\phi}(z|s,\textbf{I})$ to denote the tractable approximate inference network with parameters $\phi$. The variational inference seeks to optimize
$\min_{\beta} \min_{\phi} \text{KL}(p_{\text{data}}(s|\textbf{I})q_{\phi}(z|s,\textbf{I})||p_{\beta}(z,s|\textbf{I}))$, which can be further decomposed into $\min_{\beta} \min_{\phi} \text{KL}(p_{\text{data}}(s|\textbf{I})|| p_{\beta}(s|\textbf{I})) + \text{KL}( q_{\phi}(z|s,\textbf{I})  || p_{\beta}(z|s,\textbf{I}))$. That is, the variational inference maximizes the conditional data likelihood plus a KL-divergence between the approximate inference network and the posterior distribution. Only when the $\text{KL}( q_{\phi}(z|s,\textbf{I}) || p_{\beta}(z|s,\textbf{I})) \rightarrow 0$, the variational inference will lead to the MLE solution, which is exactly the objective of our model. However, there might exist a gap between them in practise due to the improper design of the inference network. Our learning algorithm is much simpler and more accurate than amortized inference.       

\noindent\textbf{Computational and memory costs analysis:} From the learning perspective, due to the iterative Langevin sampling for the posterior and prior distributions of latent variables, our model is more time-consuming for training than generative models with amortized inference, such as VAE, which is
roughly 2.4 times faster than ours on RGB image-based salient object detection. However, for VAE, the inference model is parameterized by another set of parameters, which need to be updated by back-propagation. In our model, the Langevin dynamics is not treated as a model because once the top-down generator is updated in each iteration, the posterior distribution can be derived from the generator. With the posterior distribution, the Langevin sampling is just an optimization-like process to find fair samples in the latent space defined by the posterior distribution. Without relying on an extra inference network, our framework is efficient in memory and friendly for network design. 
















\section{Experimental Results}


\subsection{Setup}
\label{setup_sec}


\noindent\textbf{Datasets:} 
For RGB SOD, we train models on DUTS training  set~\cite{imagesaliency},
and test them on five benchmark datasets, including DUTS testing set,  ECSSD~\cite{yan2013hierarchical}, DUT~\cite{Manifold-Ranking:CVPR-2013}, HKU-IS~\cite{li2015visual} and PASCAL-S~\cite{pascal_s_dataset}.
For RGB-D SOD, we follow the conventional training setting, in which the training set is a combination of 1,485 images from NJU2K
dataset~\cite{NJU2000} and 700 images from NLPR dataset~\cite{peng2014rgbd}. We test the trained models on NJU2K testing set, NLPR testing set,
DES~\cite{cheng2014depth}, SSB~\cite{niu2012leveraging} and SIP~\cite{sip_dataset} testing set.



\noindent\textbf{Evaluation Metrics:} We adopt four evaluation metrics to measure the performance, including Mean Absolute Error $\mathcal{M}$, Mean F-measure ($F_{\beta}$), Mean E-measure ($E_{\xi}$)~\cite{fan2018enhanced} and S-measure ($S_{\alpha}$)~\cite{fan2017structure}.



\noindent\textbf{Implementation Details:}
Our generative vision transformer is built upon the Swin transformer~\cite{liu2021swin}  and uses it as the backbone of the encoder in our framework. The Swin backbone can be initialized with the parameters pretrained on the ImageNet-1K~\cite{imagenet_1k} dataset for image classification, and the other parameters of the newly added components, including the decoder part and the MLP of the energy based prior model, will be randomly initialized from the Gaussian distribution $\mathcal{N}(0,0.01)$.
Empirically we set the number of dimensions $d$ of the latent variables $z$ as $d=32$. We set $\sigma_{\epsilon}=1$ in Eq.~(\ref{eq:G_Transformer})  and $\sigma_{z}=1$ in Eq.~(\ref{eq:ebm_prior}). 
We resize all the images and the saliency maps to the resolution of $384\times384$ pixels to fit the Swin transformer. The maximum epoch is 50. The initial learning rates are $2.5 \times 10^{-5}$. The whole training takes 9 hours with a batch size $n'=10$ on one NVIDIA GTX 2080Ti GPU for each model.
During testing, our model can process 15 images per second.














\subsection{Performance Comparison}


We compare our framework with the state-of-the-art RGB SOD models and RGB-D SOD models, and show a comparison of performance in Table~\ref{tab:benchmark_rgb_sod} and Table~\ref{tab:benchmark_rgbd_sod} respectively, where VST \cite{Liu_2021_ICCV_VST} is the only transformer-based saliency detection model.
We observe consistently better performance of the proposed frameworks. Especially for PASCAL-S dataset~\cite{pascal_s_dataset}, which contains more than 40\% examples with large-sized salient objects, the significant performance gap between our method and the existing solutions demonstrates the effectiveness of our generative framework for saliency prediction.
We further show a qualitative comparison between our RGB-D saliency prediction model and the BBSNet~\cite{fan2020bbs} in Figure~\ref{fig:visual_comparison}. As we can see, our transformer-based framework, with an effective global context modeling, is superior to its competitor in detecting various sizes of salient objects. 




\begin{table*}[h!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.3mm}
  \caption{Performance comparison with benchmark RGB salient object detection models.}
  \begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{DUTS~\cite{imagesaliency}}&\multicolumn{4}{c|}{ECSSD~\cite{yan2013hierarchical}}&\multicolumn{4}{c|}{DUT~\cite{Manifold-Ranking:CVPR-2013}}&\multicolumn{4}{c|}{HKU-IS~\cite{li2015visual}}&\multicolumn{4}{c}{PASCAL-S~\cite{pascal_s_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$ \\ \hline
   CPD~\cite{cpd_sal} & .869 & .821 & .898 & .043 & .913 & .909 & .937 & .040 & .825 & .742 & .847 & .056 & .906 & .892 & .938 & .034 & .848 & .819 & .882 & .071  \\
   SCRN~\cite{scrn_sal} & .885 & .833 & .900 & .040 & .920 & .910 & .933 & .041 & .837 & .749 & .847 & .056 & .916 & .894 & .935 & .034 & .869 & .833 & .892 & .063 \\ 
   PoolNet~\cite{Liu19PoolNet} & .887 & .840 & .910 & .037 & .919 & .913 & .938 & .038 & .831 & .748 & .848 & .054 & .919 & .903 & .945 & .030 & .865 & .835 & .896 & .065  \\ 
    BASNet~\cite{qin2019basnet} & .876 & .823 & .896 & .048 & .910 & .913 & .938 & .040 & .836 & .767 & .865 & .057 & .909 & .903 & .943 & .032 & .838 & .818 & .879 & .076 \\ 
   EGNet~\cite{zhao2019EGNet} & .878 & .824 & .898 & .043 & .914 & .906 & .933 & .043 & .840 & .755 & .855 & .054 & .917 & .900 & .943 & .031 & .852 & .823 & .881 & .074  \\
   F3Net~\cite{wei2020f3net} & .888 & .852 & .920 & .035 & .919 & .921 & .943 & .036 & .839 &  .766 & .864 & .053 & .917 & .910 & .952 & .028 & .861 & .835 & .898 & .062 \\
   ITSD~\cite{zhou2020interactive} & .886 & .841 & .917 & .039 & .920 & .916 & .943 & .037 & .842 & .767 & .867 & .056 & .921 & .906 & .950 & .030 & .860 & .830 & .894 & .066 \\
  SCNet \cite{zhang2021energy} & .902 & .870 & .936 & .032 & .928 & .930 & .955 & .030 & .847 & .778 & .879 & .053& .927 & .917 & .960 & .026 & .873 & .846 & .909 & .058 \\
   LDF~\cite{wei2020label} & .892 & .861 & .925 & .034 & .919 & .923& .943 & .036 & .839 & .770 & .865 & .052 & .920 & .913 & .953 & .028 & .860 & .856 & .901 & .063 \\ 
VST~\cite{Liu_2021_ICCV_VST} &.896 &.842 &.918 &.037 &.932 &.911 &.943 &.034 &.850 &.771 &.869 &.058 &.928 &.903 &.950 &.030 &.873 &.832 &.900 &.067  \\ \hline
\textbf{Ours} &\textbf{.908} &\textbf{.875} &\textbf{.942} &\textbf{.029} &\textbf{.935} &\textbf{.935} &\textbf{.962} &\textbf{.026} &\textbf{.858} &\textbf{.797} &\textbf{.892} &\textbf{.051} &\textbf{.930} &\textbf{.922} &\textbf{.964} &\textbf{.023} &\textbf{.877} &\textbf{.855} &\textbf{.915} &\textbf{.054}  \\ \hline
  \end{tabular}
\label{tab:benchmark_rgb_sod}
\end{table*}

\begin{table*}[h!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.3mm}
  \caption{Performance comparison with benchmark RGB-D salient object detection models.}
  \begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{NJU2K~\cite{NJU2000}}&\multicolumn{4}{c|}{SSB~\cite{niu2012leveraging}}&\multicolumn{4}{c|}{DES~\cite{cheng2014depth}}&\multicolumn{4}{c|}{NLPR~\cite{peng2014rgbd}}&\multicolumn{4}{c}{SIP~\cite{sip_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$ \\ \hline
   BBSNet~\cite{fan2020bbs}  &.921 &.902 &.938 &.035  &.908 &.883 &.928 &.041 &.933 &.910 &.949 &.021 &.930 &.896 &.950 &.023 &.879 &.868 &.906 &.055 \\
   BiaNet~\cite{zhang2020bilateral}  &.915 &.903 &.934 &.039  &.904 &.879 &.926 &.043 &.931 &.910 &.948 &.021 &.925 &.894 &.948 &.024 &.883 &.873 &.913 &.052 \\
   CoNet~\cite{ji2020accurate}  &.911 &.903 &.944 &.036  &.896 &.877 &.939 &.040 &.906 &.880 &.939 &.026 &.900 &.859 &.937 &.030  &.868 &.855 &.915 &.054 \\
   UCNet~\cite{ucnet_sal} &.897 &.886 &.930 &.043 &.903 &.884 &.938 &.039 &.934 &.919 &.967 &.019 &.920 &.891 &.951 &.025 &.875 &.867 &.914 &.051 \\
   JLDCF~\cite{Fu2020JLDCF} &.902 &.885 &.935 &.041  &.903 &.873 &.936 &.040 &.931 &.907 &.959 &.021 &.925 &.894 &.955 &.022  &.880 &.873 &.918 &.049 \\ 
   DSA2F~\cite{Sun_2021_CVPR_DSA2F} &.903 &.901 &.923 &.039  &.904 &.898 &.933 &.036 &.920 &.896 &.962 &.021 &.918 &.897 &.950 &.024  &- &- &- &- \\ 
VST~\cite{Liu_2021_ICCV_VST} &.922 &.898 &.939 &.035 &.913 &.879 &.937 &.038 &.943 &.920 &.965 &.017 &.932 &.897 &.951 &.024  &.904 &.894 &.933 &.040 \\ \hline
\textbf{Ours} &\textbf{.929} &\textbf{.924} &\textbf{.956} &\textbf{.028} &\textbf{.916} &\textbf{.898} &\textbf{.950} &\textbf{.032} &\textbf{.945} &\textbf{.928} &\textbf{.971} &\textbf{.016} &\textbf{.938} &\textbf{.921} &\textbf{.966} &\textbf{.018}  &\textbf{.906} &\textbf{.908} &\textbf{.940} &\textbf{.037}  \\
   \hline 
  \end{tabular}
\label{tab:benchmark_rgbd_sod}
\end{table*}


















\subsection{Backbone Analysis}
\label{sec:backbone_analysis}
To test the performance of the transformer structure proposed in Section~\ref{network_sec}, we compare it with a conventional convolutional backbone for salient object detection. We create the baseline by replacing the Swin encoder in our transformer structure by the ResNet50~\cite{he2016deep_resnet} encoder and keeping the decoder part (i.e., the ``Feature Aggregation'' module shown in Figure~\ref{fig:generative_transformer}) unchanged for a fair comparison. 

\begin{table*}[b!]
\vspace{-0.15in}
\centering
\scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.35mm}
  \caption{Analysis of different backbones without involving latent variables.}
{
\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{NJU2K~\cite{NJU2000}}&\multicolumn{4}{c|}{SSB~\cite{niu2012leveraging}}&\multicolumn{4}{c|}{DES~\cite{cheng2014depth}}&\multicolumn{4}{c|}{NLPR~\cite{peng2014rgbd}}&\multicolumn{4}{c}{SIP~\cite{sip_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$\\ \hline


Ours-Res50-D &.908 &.896 &.931 &.036 &.906 &.889 &.927 &.037 &.918 &.907 &.948 &.022 &.921 &.893 &.951 &.024  &.874 &.856 &.917 &.049   \\  
  Ours-Swin-D &.919 &.923 &.947 &.032 &.914 &.897 &.943 &.033 &.931 &.919 &.959 &.022 &.933 &.912 &.951 &.022  &.897 &.899 &.931 &.041  \\  
  \hline
\end{tabular}
}
\label{tab:backbone_models}
\end{table*}

We first test them without involving latent variables, which means that we need to train them in a discriminative manner.  We use ``Ours-Swin-D'' to denote the model using the Swin encoder and ``Ours-Res50-D'' to denote the one using the ResNet50 encoder.
Model performance in the task of RGB-D saliency detection are shown in Table~\ref{tab:backbone_models}.  We observe that ``Ours-Swin-D'' outperforms ``Ours-Res50-D'', which clearly indicates the effectiveness of  transformer backbone for salient~object~detection. 







We further study the influence of different encoder backbones in the context of the proposed generative saliency prediction framework with an EBM prior. Table~\ref{tab:generarive_backbone_rgb_sod} and Table~\ref{tab:generarive_backbone_rgbd_sod}, respectively, depict the performance comparisons of our frameworks using different backbones in the tasks of RGB and RGB-D salient object detections. We compare the ResNet50 encoder backbone and the Swin transformer encoder backbone. We also modify the DPT~\cite{dpt_transformer} backbone such that it can adapt to the tasks of RGB and RGB-D salient object detections with our framework. The DPT built on the ViT transformer~\cite{dosovitskiy_ViT_ICLR_2021} is originally designed for semantic segmentation and depth estimation. The comparison results verify the effectiveness of the vision transformer used in our generative framework. In particular, our current solution with the Swin transformer encoder backbone can achieve the best performance for saliency prediction. Further, the performance gap between ``Ours-Swin-D'' in Table~\ref{tab:backbone_models} and ``Ours-Swin'' in Table~\ref{tab:generarive_backbone_rgbd_sod} indicates the effectiveness of the generative learning with an expressive latent space.






\begin{table*}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.3mm}
  \caption{Performance of different backbones within our model for RGB saliency prediction.}
  \begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{DUTS~\cite{imagesaliency}}&\multicolumn{4}{c|}{ECSSD~\cite{yan2013hierarchical}}&\multicolumn{4}{c|}{DUT~\cite{Manifold-Ranking:CVPR-2013}}&\multicolumn{4}{c|}{HKU-IS~\cite{li2015visual}}&\multicolumn{4}{c}{PASCAL-S~\cite{pascal_s_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$ \\ \hline
Ours-Res50  & .890 & .850 & .927 & .035 & .918 & .914 & .944 & .036 & .837 & .762 & .867 & .053 & .917 & .906 & .952 & .029 & .859 & .830 & .896 & .063  \\
      Ours-DPT  &.899 &.874 &.940 &.031 &.924 &.933 &.956 &.031 &.854 &.792 &.890 &.054 &.922 &.920 &.960 &.026 &.870 &.854 &.911 &.055  \\
\textbf{Ours-Swin} &\textbf{.908} &\textbf{.875} &\textbf{.942} &\textbf{.029} &\textbf{.935} &\textbf{.935} &\textbf{.962} &\textbf{.026} &\textbf{.858} &\textbf{.797} &\textbf{.892} &\textbf{.051} &\textbf{.930} &\textbf{.922} &\textbf{.964} &\textbf{.023} &\textbf{.877} &\textbf{.855} &\textbf{.915} &\textbf{.054}  \\ \hline
  \end{tabular}
\label{tab:generarive_backbone_rgb_sod}
\end{table*}

\begin{table*}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.2mm}
  \caption{Performance of different backbones within our model for RGB-D saliency prediction.}
  \begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{NJU2K~\cite{NJU2000}}&\multicolumn{4}{c|}{SSB~\cite{niu2012leveraging}}&\multicolumn{4}{c|}{DES~\cite{cheng2014depth}}&\multicolumn{4}{c|}{NLPR~\cite{peng2014rgbd}}&\multicolumn{4}{c}{SIP~\cite{sip_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$ \\ \hline
Ours-Res50  & .919 & .909 & .946 & .033 & .906 & .882 & .937 & .038 & .937 & .925 & .974 & .017 & .920 & .892 & .949 & .025 & .882 & .872 & .918 & .049  \\
  Ours-DPT &.924 &.913 &.950 &.031 &.915 &.892 &.946 &.034 &.941 &.921 &.968 &.017 &.935 &.913 &.964 &.019  &.901 &.903 &.933 &.038 \\
\textbf{Ours-Swin} &\textbf{.929} &\textbf{.924} &\textbf{.956} &\textbf{.028} &\textbf{.916} &\textbf{.898} &\textbf{.950} &\textbf{.032} &\textbf{.945} &\textbf{.928} &\textbf{.971} &\textbf{.016} &\textbf{.938} &\textbf{.921} &\textbf{.966} &\textbf{.018}  &\textbf{.906} &\textbf{.908} &\textbf{.940} &\textbf{.037}  \\ \hline
  \end{tabular}
\label{tab:generarive_backbone_rgbd_sod}
\end{table*}

\subsection{Generative Learning Analysis}\label{sec:generative_learning} 


We compare our framework with other alternative generative solutions in Table~\ref{tab:generative_model_analysis}. The results are reported in the task of RGB-D saliency detection. For fair comparison, we implement an ABP-based model~\cite{abp}, a GAN-based model~\cite{GAN_nips}, and a VAE-based model~\cite{vae_bayes_kumar,structure_output} using the same transformer-based generator as ours. The latent variables in these models are still assumed to follow the isotropic Gaussian distribution, as in their original algorithms. 
To be specific, for the ABP-based model, we use MCMC-based inference while training, and sample the latent variables directly from the Gaussian distribution during testing.
For the GAN-based alternative, we design a fully convolutional discriminator~\cite{hung2018adversarial} that consists of five $3\times3$ convolutional layers with a stride of 2 in each layer. The discriminator takes the concatenation of an image and a saliency map as input, and is trained to distinguish between the predicted saliency map and the ground truth given an image. The numbers of the output channels of the discriminator are $64,64,64,64$ and $1$. For the VAE-based alternative, we introduce an extra encoder as an approximate Gaussian inference model via the reparameterization trick. The encoder consists of four $4\times4$ convolutional layers with a stride of 2 in each layer and maps the concatenation of an image and a saliency map to feature maps of channel sizes $64,64,64$ and $64$ sequentially. After that, two fully connected layers are adopted to produce the mean and the standard deviation of the inference model.
For all the three alternative generative models, we set the numbers of the dimension of the latent space the same as that in our model, which is $d=32$. 


\begin{table*}[h!]
\centering
  \scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.35mm}
  \caption{Performance of different generative models with transformer backbones for SOD.}
{
\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{NJU2K~\cite{NJU2000}}&\multicolumn{4}{c|}{SSB~\cite{niu2012leveraging}}&\multicolumn{4}{c|}{DES~\cite{cheng2014depth}}&\multicolumn{4}{c|}{NLPR~\cite{peng2014rgbd}}&\multicolumn{4}{c}{SIP~\cite{sip_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$ \\ \hline
   ABP &.920 &.915 &.951 &.030 &.910 &.890 &.942 &.035 &.935 &.920 &.962 &.018 &.930 &.914 &.962 &.020 &.900 &.898 &.935 &.039  \\ 
   GAN &.928 &.922 &.954 &.030 &.913 &.892 &.941 &.033 &.940 &.924 &.969 &.018 &.934 &.915 &.961 &.021 &.901 &.904 &.937 &.039  \\
   VAE &.928 &.921 &.955 &.029 &.914 &.894 &.947 &.033 &.942 &.922 &.970 &.017 &.934 &.914 &.961 &.020 &.904 &.906 &.935 &.038  \\
   \hline
   \textbf{Ours} &\textbf{.929} &\textbf{.924} &\textbf{.956} &\textbf{.028} &\textbf{.916} &\textbf{.898} &\textbf{.950} &\textbf{.032} &\textbf{.945} &\textbf{.928} &\textbf{.971} &\textbf{.016} &\textbf{.938} &\textbf{.921} &\textbf{.966} &\textbf{.018}  &\textbf{.906} &\textbf{.908} &\textbf{.940} &\textbf{.037}  \\
   \hline 
  \end{tabular}
}
  \label{tab:generative_model_analysis}
\end{table*}

As shown in Table~\ref{tab:generative_model_analysis}, 
compared with the deterministic baseline \enquote{Ours-Swin-D} (in Table~\ref{tab:backbone_models}), the three generative frameworks in  achieve better or comparable performance. Especially in the DES dataset~\cite{cheng2014depth}, they achieve large performance improvements. Our model outperforms all these alternative generative solutions, showing the effectiveness of the informative EBM prior distribution used in our model.




\subsection{Hyperparameter Analysis}


The main hyperparameters in our framework include
the number of Langevin steps $K$, the Langevin step size $\delta$, the number of dimensions of the latent space $d$, and the size of EBM prior $C_e$. We have two sets of hyperparameters $\{\delta^-, K^-\}$ and $\{\delta^+, K^+\}$ of the Langevin dynamics for sampling from the prior distribution and the posterior distribution, respectively.  
As to the Langevin step size, we find stable model performance with $\delta^{-}\in [0.2,0.6]$ and $\delta^{+}\in [0.05,0.3]$, and we set $\delta^{-}=0.4$ and $\delta^{+}=0.1$ in our paper. For the number of Langevin steps, we empirically set $K^-=K^+=5$ to achieve a trade-off between the training efficiency and the model performance, as more Langevin steps will lead to longer training time but more convergent inference results. Additionally, we investigate the influence of the number of latent dimensions by varying  $d=\{8,16,32,64\}$, and observe comparable performance among different choices of $d$. We set $d=32$ in our paper. We also investigate the influence of the EBM size by varying $C_e=\{20,60,100\}$, and show the results in Table~\ref{tab:hyperparameter_analysis}, in which we find $C_e=60$ can provide optimal saliency prediction performance.


\begin{table*}[h!]
\scriptsize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{0.25mm}
  \caption{Influence of the size of the EBM prior model}
{
\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
  \hline
&\multicolumn{4}{c|}{NJU2K~\cite{NJU2000}}&\multicolumn{4}{c|}{SSB~\cite{niu2012leveraging}}&\multicolumn{4}{c|}{DES~\cite{cheng2014depth}}&\multicolumn{4}{c|}{NLPR~\cite{peng2014rgbd}}&\multicolumn{4}{c}{SIP~\cite{sip_dataset}} \\
    Method & $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$& $S_{\alpha}\uparrow$&$F_{\beta}\uparrow$&$E_{\xi}\uparrow$&$\mathcal{M}\downarrow$ \\ \hline
    $C_e=20$ &\textbf{.930} &.913 &.957 &\textbf{.026} &\textbf{.917} &\textbf{.898} &.950 &\textbf{.030} &\textbf{.951} &.921 &.970 &\textbf{.016} &.937 &.913 &.950 &.022 &.901 &.892 &.931 &.037  \\ 
    $C_e=100$ &.926 &.904 &.950 &.031 &.916 &.891 &\textbf{.952} &.031 &.942 &\textbf{.930} &.970 &\textbf{.016} &\textbf{.939} &.913 &.960 &.021 &.903 &.895 &.934 &\textbf{.036}  \\ \hline
    \textbf{Ours ($C_e=60$)} &.929 &\textbf{.924} &\textbf{.956} &.028 &.916 &\textbf{.898} &.950 &.032 &.945 &.928 &\textbf{.971} &\textbf{.016} &.938 &\textbf{.921} &\textbf{.966} &\textbf{.018}  &\textbf{.906} &\textbf{.908} &\textbf{.940} &.037  \\
   \hline 
  \end{tabular}
 }
  \label{tab:hyperparameter_analysis}
\end{table*}

\subsection{Explainability Analysis}














As a generative model, our framework is capable of obtaining a meaningful uncertainty map that summarizes the stochastic behavior of the model in performing saliency prediction. The uncertainty in our paper refers to the stochastic property of generating the saliency prediction from an input image, which is captured by the probabilistic model $p_{\beta}(s|\textbf{I})$.
In general, a generative saliency prediction method can provide not only accurate predictions but also reasonable uncertainty maps that  represent the “subjective nature” of the human visual saliency. In this section, we propose to use the uncertainty map to help qualitatively evaluate different generative saliency prediction frameworks. Specifically, we propose to compute the uncertainty map as the variance of multiple saliency predictions produced from the learned probabilistic model. In the experiment, for each input image, we first output ten saliency maps by using the Langevin sampling from the learned conditional distribution, and then compute the variance map (uncertainty) based on the generated saliency predictions.


To quantify the complexity of a color image, we calculate a complexity score by the following way: (i) we first over-segment the image with the SLIC~\cite{slic} to obtain $200$ superpixels, (ii) then we compute the similarity of each superpixel with the others using handcrafted features from~\cite{rbd_sal}, which gives us a $200$-dimensional contrast vector, representing an overall contrast of the image, (iii) we define the complexity of the image as the mean entropy of the contrast vector. The higher the score, the more complicated the image. In general, a reasonable generative saliency prediction model should have more confident predictions on images with simpler backgrounds (i.e., lower complexity scores), and less confident predictions on images with complicated backgrounds (i.e., higher complexity scores). 


In Figure~\ref{fig:generative_model_uncertainty_maps}, we compare the uncertainty maps of different generative models that we have presented in Section~\ref{sec:generative_learning}. For each row of Figure~\ref{fig:generative_model_uncertainty_maps}, we display an input image, which is tagged with an image complexity score, and the associated ground truth saliency map, followed by the predicted saliency maps and the uncertainty maps obtained by the ABP-based model, the GAN-based model, the VAE-based model, and our model, respectively. As shown in Figure~\ref{fig:generative_model_uncertainty_maps}, the complexity score of each image is reasonable in the sense that it is consistent with the human perception. We observe that the uncertainty maps obtained by the other generative models fail to reflect the difficulties of the images for saliency prediction. For example, the image shown in the first row of Figure \ref{fig:generative_model_uncertainty_maps} has a structured foreground (i.e., a temple) and a textured background (i.e., a forest), which leads to a large ambiguity of the boundary between the foreground and the background. The uncertainty map obtained by our model indicates a big variance around the boundary region, which demonstrates the explainability and the reasonability of our model. Note that only generative saliency prediction frameworks can provide such an explainability analysis based on the uncertainty maps. This might motivate us to develop generative frameworks for explainable saliency prediction in the future.        





  









\begin{figure*}[h!]
\scriptsize
\begin{center}
   \begin{tabular}{c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }}
   \rotatebox{90}{\footnotesize{0.20}}&{\includegraphics[width=0.085\linewidth]{sample_show/image_left30_0.20149.jpg}}& 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_gt.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_abp.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_vabp.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_gan.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_vgan.png}}& 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_vae.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_vvae.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_ebm.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left30_vebm.png}}\\
   \rotatebox{90}{\footnotesize{0.17}}&{\includegraphics[width=0.085\linewidth]{sample_show/image_left48_0.17271.jpg}}&
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_gt.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_abp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_vabp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_gan.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_vgan.png}}& 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_vae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_vvae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_ebm.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left48_vebm.png}}\\
   \rotatebox{90}{\footnotesize{0.14}}&{\includegraphics[width=0.085\linewidth]{sample_show/image_left75_0.13511.jpg}}& 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_gt.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_abp.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_vabp.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_gan.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_vgan.png}}& 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_vae.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_vvae.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_ebm.png}}
   & 
   {\includegraphics[width=0.085\linewidth]{sample_show/image_left75_vebm.png}}\\
   \rotatebox{90}{\footnotesize{0.07}}&{\includegraphics[width=0.085\linewidth]{sample_show/image_left939_0.070278.jpg}}&
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_gt.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_abp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_vabp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_gan.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_vgan.png}}& 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_vae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_vvae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_ebm.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left939_vebm.png}}\\
  \rotatebox{90}{\footnotesize{0.04}}&{\includegraphics[width=0.085\linewidth]{sample_show/image_left586_0.039472.jpg}}&
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_gt.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_abp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_vabp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_gan.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_vgan.png}}& 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_vae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_vvae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_ebm.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left586_vebm.png}}\\
  \rotatebox{90}{\footnotesize{0.02}}&{\includegraphics[width=0.085\linewidth]{sample_show/image_left54_0.017564.jpg}}&
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_gt.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_abp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_vabp.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_gan.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_vgan.png}}& 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_vae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_vvae.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_ebm.png}}
  & 
  {\includegraphics[width=0.085\linewidth]{sample_show/image_left54_vebm.png}}\\
&\footnotesize{Image}&\footnotesize{GT}&\multicolumn{2}{c}{\footnotesize{ABP}}&\multicolumn{2}{c}{\footnotesize{GAN}}&\multicolumn{2}{c}{\footnotesize{VAE}}&\multicolumn{2}{c}{\footnotesize{Ours}}\\
   \end{tabular}
   \end{center}
\caption{A comparison of uncertainty maps obtained by different generative saliency prediction frameworks for explainability analysis. Each row represents one example, in which we display an image tagged with a complexity score, the corresponding ground truth saliency map, as well as the predicted saliency maps and the uncertainty maps obtained by different generative frameworks.  
   }
\label{fig:generative_model_uncertainty_maps}
\end{figure*}








\section{Conclusion and Discussion}



In this paper, we study the generative modeling and learning of vision transformer in the context of RGB and RGB-D salient object detections. We start from defining a conditional probability distribution of saliency map given an input image by a top-down latent variable generative framework, in which the non-linear mapping from image domain to saliency domain is parameterized by a proposed vision transformer network and the prior distribution of the low-dimensional latent space is represented by a trainable energy-based model. Instead of using amortized inference and sampling strategies, we learn the model by the MCMC-based maximum likelihood, where the Langevin sampling is used to evaluate the intractable posterior and prior distributions of the latent variables for calculating the learning gradients of the model parameters. With the informative energy-based prior and the expressive top-down vision transformer network, our model can achieve both accurate predictions and meaningful uncertainty maps that are consistent with the human perception. 

From the machine learning perspective, our model is a likelihood-based top-down deep conditional generative model, which is neither CGAN-based nor CVAE-based frameworks, and it is trained without relying on any assisting network. The learning algorithm derived from the proposed model is based on MCMC inference for the posterior and MCMC sampling for the prior, which makes our framework more natural, principled, and statistically rigorous than others. The MCMC-based inference is immediately available in the sense that there is nothing to worry about the non-trivial design and training of a separate inference model as in VAEs. Such a framework is not only useful for saliency prediction (Though this is what we target in this paper) but also applicable to a vast of conditional learning scenarios, such as semantic segmentation, image-to-image translation, etc. Thus, the proposed generative model and the learning algorithm are generic and universal.

From the computer vision perspective, our model with a special network design to handle saliency prediction is a new member of the family of saliency prediction methods. In comparison with the traditional discriminative saliency prediction methods, our generative method is  natural and reasonable because it models the saliency prediction as a conditional probability distribution. Moreover, it demonstrates impressive performance over all RGB and RGB-D SOD benchmarks. As we know, the computer vision community has started to develop vision transformer networks for various computer vision tasks, such as image classification, segmentation, detection, generation, etc. Our paper is the first one to present a generative vision transformer for both RGB and RGB-D saliency predictions. Thus, the proposed framework is significantly important for the computer vision community.







\bibliographystyle{plain}
\bibliography{SOD_Reference}






\end{document}
