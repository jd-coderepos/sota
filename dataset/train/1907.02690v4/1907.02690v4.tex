
















\newpage
\begin{center}
    \Large\textbf{Supplementary Materials for ``Twin Auxiliary Classifiers GAN''}\\
\end{center}
{\large This supplementary material provides the
 proofs and more experimental details which are omitted in the submitted paper. The equation
 numbers in this material are consistent with those in the paper.}
 










\section*{S1. Proof of Theorem 1}
\begin{proof}
The optimal  is obtained by the following optimization problem:

The optimization problem in (\ref{Eq:opt_theo1}) is equivalent to minimizing the objective point-wisely for each , i.e., 

which is a linear programming (LP) problem. The optimal solution must lie in the extreme points of the feasible set, which are those points with posterior probability 1 for one class and 0 for the other classes. By evaluating the objective values of these extreme points, the optimal solution is  (5) with objective value , where .
\end{proof}

\section*{S2. Proof of Theorem 2}
\begin{proof}
The minimax game (7) can be written as

where the constraint is because  is forced to have probability outputs that sum to one. In the following proposition, we will give the optimal  for any given , or equivalently . 
\begin{Proposition}
Let for a fixed generator , the optimal prediction probabilities  of  are 

\end{Proposition}
\begin{proof}
For a fixed , (\ref{Eq:minmax_sup}) reduces to maximize the value function  w.r.t. :

By maximizing the value function pointwisely and applying Lagrange multipliers, we obtain the following problem:

Setting the derivative of (\ref{Eq:lagrange}) w.r.t.  to zeros, we obtain

We can solve for the Lagrange multiplier  by substituting (\ref{Eq:lagrange_sol}) into the constraint  to give . Thus we obtain the optimal solution

\end{proof}

Now we are ready the prove the theorem.
If we add  to , we can obtain:

By using the definition of JSD, we have

Since the Jensen-Shannon divergence among multiple distributions is always non-negative, and zero if they are equal, we have shown that  is the global minimum of  and that the only solution is .
\end{proof}

\section*{S3. Proof of Theorem 3}
According to the triangle inequality of total variation (TV) distance, we have

Using the definition of TV distance, we have

where  and  are densities,  is a (-finite) measure,  is an upper bound of  , and (a) follows from the H{\"o}lder inequality.

Similarly, we have 

where  is an upper bound of  . Combining (\ref{Eq:tv_dist}), (\ref{Eq:bound_x}), and (\ref{Eq:bound_ygx}), we have 

According to he Pinsker inequality  \cite{Tsybakov2008}, and the relation between TV and JSD, \ie~  \cite{NIPS2018_8229}, we can rewrite (\ref{Eq:tv_final}) as


\section*{S4. 1D MoG synthetic Data}
\subsection*{S4.1. Experimental Setup}
For all the networks in AC-GAN, Projection cGAN, and our TAC-GAN, we adopt the three layer Multi-Layer Perceptron (MLP) with hidden dimension 10 and Relu \cite{relu} activation function. The only difference is the number of input and output nodes. We choose Adam \cite{adam} as the optimizer and set the learning rate as 2e-4 and the hyperparameter of Adam as . We train 10 steps for , , and  and 1 step for  in every iteration. The batch size is set to 256.

\subsection*{S4.2. More Results}

\begin{figure}[H]
\centering\includegraphics[width=13cm]{figure/1_D_result/all_1_D_visualize.pdf}
  \caption{Change distance between the means of adjacent 1-D Gaussian Components, in this figure, all models adopt cross entropy loss.}
  \label{all_1_D}
\end{figure}

\begin{figure}[H]
\centering\includegraphics[width=13cm]{figure/1_D_result/all_1_D_visualize_M.pdf}
  \caption{Change distance between the means of adjacent 1-D Gaussian Components, in this figure, all models adopt hinge loss.}
  \label{all_1_D_M}
\end{figure}
\section*{S5. 2D MoG Synthetic Data}

\begin{figure}[H]
\centering\includegraphics[width=13cm]{figure/2_D_result/all_2_D_visualize.pdf}
  \caption{Change distance between the means of adjacent 2-D Gaussian Components in x-axis, in this figure, all models adopt cross entropy loss.}
  \label{all_1_D}
\end{figure}



\section*{S6. Overlapping MNIST}
\subsection*{S6.1. Experimental Setup}
For the network settings, the  network consists of three layers of Res-Block and relies on Conditional Batch Normalization (CBN) \cite{CBN} to plug in label information. The network structure of  mirrors  network without CBN. To stabilize training,  share the the convolutional layers and differ in the fully-connected layers. The chosen dimension of latent  is 128 and optimizer is Adam with learning rate =2e-4 and  for both  and  networks.  Each iteration contains 2 steps of  training and 1 step of  training. The batch size is set to 100.

\subsection*{S6.2. More Results}
In this experiment, we fix the training data and change the weight of classifier from  to  with step 0.5 for our model TAC-GAN and AC-GAN. For AC-GAN, when the value of  becomes larger, the proportion of the generated digit `0', which is the overlapping digit, goes smaller. However, our model is still able to replicate the true distribution.

\begin{figure}[h]
\includegraphics[width=13cm]{figure/overlap_MNIST/all_overlap_MNIST.pdf}
  \caption{More generated results for the overlapping MNIST dataset.}
  \label{all_MNIST}
\end{figure}

\section*{S7. CIFAR100 }
\subsection*{S7.1. Experimental Setup}
Due to the complexity and diversity of this dataset, we apply the latest SN-GAN \cite{miyato2018spectral} as our base model, the implementation is based on Pytorch implementation of Big-GAN \cite{brock2018large} and SN layer is added to both  and  networks \cite{Han18}. On this dataset, there is no need to add Self-Attention layer \cite{Han18} and only three Res-Blocks layers are applied due to the low resolution as . As done by SN-GAN \cite{miyato2018spectral}, we replace the loss term {\textcircled{\small a}} by the hinge loss in order to stabilize the GAN training part. For all evaluated methods, the batch size is 100 and total number of training iterations is 60K. The optimizer parameters are identical to those used in the overlapping MNIST experiment. 
\subsection*{S7.2. PAC-GAN Improvement}
PacGAN is a method that significantly increases the diversity of GANs. Here we combine pacGAN with both AC-GAN and our TAC-GAN and the results are shown in Table \ref{tab:my_label}.  The results indicates that pacGAN can increase the performance of both AC-GAN and TAC-GAN but the drawbacks in AC-GAN loss cannot be fully addressed by pacGAN. 
\begin{table}[H]
    \centering
 \begin{tabular}{l|c c c c }
  \toprule
   & TAC-GAN & pacGAN4+TAC-GAN & AC-GAN & pacGAN4+AC-GAN \\
  IS & 9.34  0.077 & \textbf{9.85  0.116 } & 5.37  0.064 & 8.54  0.143 \\
  FID & 7.22 & \textbf{6.79} & 82.45 & 20.94 \\
  \bottomrule
 \end{tabular}
    \caption{IS and FID scores}
    \label{tab:my_label}
\end{table}
\subsection*{S7.3. More Results}
We show the generated samples for all classes in Figure~\ref{all_CIFAR100} and report the FID and LPIPS scores for each class in Figure~\ref{cifar100_fid_each_class} and Figure~\ref{cifar100_lpips_each_class}, respectively.
\begin{figure}[H]
\centering\includegraphics[width=8.5cm]{figure/all_cifar100.pdf}
  \caption{100 classes of CIFAR100 generated samples, we choose the classifier weight  for AC-GAN model.}
  \label{all_CIFAR100}
\end{figure}

\begin{figure}[H]
\vspace{-4cm}\hspace{-3cm}\includegraphics[width=20cm]{figure/cifar100_fid_each_class.pdf}
\vspace{-4cm}\caption{The FID score is reported for each class on CIFAR100 generated data, lower is better. The y axis denotes class label and x axis denotes FID score.}
  \label{cifar100_fid_each_class}
\end{figure}

\begin{figure}[H]
\vspace{-4cm}\hspace{-3cm}\includegraphics[width=20cm]{figure/cifar100_lpips_each_class.pdf}
\vspace{-4cm}\caption{The LPIPS score is reported for each class on CIFAR100 generated data, larger values means better variance inner class. The y axis denotes class label and x axis denotes LPIPS score.}
  \label{cifar100_lpips_each_class}
\end{figure}


\section*{S8. ImageNet1000}
\subsection*{S8.1. Experimental Setup}
We adopt the full version of Big-GAN model architecture as the base network for AC-GAN, Projection cGAN, and TAC-GAN. In this experiment, we apply the shared class embedding for each CBN layer in  model, and feed noise  to multiple layers of  by concatenating with class embedding vector. We use orthogonal initialization for network parameters \cite{brock2018large}. In addition, following \cite{brock2018large}, we add Self-Attention layer with the resolution of 64 for ImageNet. Due to limited computational resources, we fix the batch size to 256. To boost the training speed, we only train one step for  network and one step for  network.

\subsection*{S8.2. More Results}
\begin{figure}[H]
\centering\includegraphics[width=10cm]{figure/imagenet/part_of_imagenet.pdf}
  \caption{In this figure, we randomly select some generated samples from 1000 classes. It contains birds, snakes, bug, dog, food, scene, etc. Our model shows a very competitive fidelity and diversity. Generative models are all trained on ImageNet1000 and the image resolution is .}
  \label{part_imagenet}
\end{figure}

\begin{figure}[H]
\vspace{-4cm}\hspace{-3cm}\includegraphics[width=19cm]{figure/imagenet_fid_each_class.pdf}
\vspace{-3.8cm}
  \caption{The FID score is reported for each class on ImageNet1000 generated data, we randomly select 100 classes from our generated samples for comparison between our model TAC-GAN and Projection cGAN. The method with a lower FID score is better. The y axis denotes class label and x axis denotes FID score.}
  \label{imagenet_fid_each_class}
\end{figure}

\begin{figure}[H]
\vspace{-4cm}\hspace{-3cm}\includegraphics[width=19cm]{figure/imagenet_lpips_each_class.pdf}
\vspace{-3.8cm}
  \caption{The LPIPS score is reported for each class on ImageNet1000 generated data. we randomly select 100 classes from our generated samples for comparison between our model TAC-GAN and Projection cGAN, higher LPIPS socre means larger intra-class variance. The y axis denotes class label and x axis denotes LPIPS score. }
  \label{imagenet_lpips_each_class}
\end{figure}


\section*{S9. VGGFace200}


\subsection*{S9.1. Experimental Setup}
We adopt the full version of Big-GAN model architecture as the base network for AC-GAN, Projection cGAN, and TAC-GAN. In this experiment, we apply the shared class embedding for each CBN layer in  model, and feed noise  to multiple layers of  by concatenating with class embedding vector. We use orthogonal initialization for network parameters \cite{brock2018large}. In addition, following \cite{brock2018large}, we add Self-Attention layer with the resolution of 32 for VGGFace. Due to limited computational resources, we fix the batch size to 256. In this setting, we train two steps for  network and two steps for  network. The only difference of the networks applied on ImageNet and VGGFace is that the network on ImageNet has one additional up-sampling block and one more down-sampling block added to  and  Networks to accommodate higher resolution.

\subsection*{S9.2. More Results}

\begin{figure}[H]
\centering\includegraphics[width=9.5cm]{figure/vgg_face/part_of_vggface.pdf}
  \caption{In this figure, we randomly select some generated samples for illustration. All the generative models are trained on 200 classes on the randomly sampled 200 classes from the VGGFace2 dataset.}
  \label{part_imagenet}
\end{figure}

\begin{figure}[H]
\vspace{-5cm}\hspace{-3cm}\includegraphics[width=20cm]{figure/vggface_lpips_each_class.pdf}
\vspace{-3.8cm}
  \caption{The LPIPS score is reported for each class on VGGFace200 generated data. we randomly select 100 classes from our generated samples for comparison between our model TAC-GAN and Projection cGAN, higher LPIPS score means larger intra-class variance. The y axis denotes class label and x axis denotes LPIPS score.}
  \label{vggface_lpips_each_class}
\end{figure}
%
