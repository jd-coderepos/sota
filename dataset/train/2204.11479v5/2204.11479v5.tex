In this section we will provide the experiments conducted on public classification benchmarks, such as ESC-50 \cite{piczak2015esc}, AudioSet \cite{gemmeke2017audio}  UrbanSound8K \cite{salamon2014dataset}. In addition to ESC scenarios, we examined the system on SpeechCommands \cite{warden2018speech} dataset, containing spoken words in English, to show some robustness to audio signal type. During the training process, we used AdamW \cite{loshchilov2017decoupled} optimizer with maximal learning rate of  and one cycle strategy \cite{smith2018disciplined}. In addition, we use weight decay with , EMA \cite{tarvainen2017mean, izmailov2018averaging} with decay rate of 0.995 and SKD \cite{zhang2019your}. The loss is label-smoothing with a noise parameter set to  for single-label classification tasks, and binary cross-entropy for the multi-label classification case. When applying mixing augmentations we use multi-label objective and use binary cross-entropy, as suggested by \cite{wightman2021resnet}. To handle distinct sample lengths across datasets we adjust the parameter controlling the downsample of the network. The set of augmentations used is detailed \ref{tab:label-augs}, with one noise type and one mixing strategy being randomly selected in each iteration.

\subsection{ESC-50}
The \textit{ESC-50} set \cite{piczak2015esc} consists of 2000 samples of environmental sounds for 50 classes. Each sample has a length of  seconds and is sampled at 44.1KHz. The set has an official split into  folds. We resampled the samples to 22.05KHz for being compliant with the majority of other works, and followed the standard -fold cross-validation to evaluate our model. Each experiment repeated three times and averaged to final score.

\begin{table}[H]
\centering
  \caption{ESC-50, accuracy with model size and inference time measured on P-100 machine.}
  \label{tab:esc50-scrach}
  \begin{tabular}{ccccccl}
    \toprule
    Model & e2e & Pretrained & Accuracy & Parameters & time\\
    \midrule
    ESResNet-Att \cite{guzhov2021esresnet} & \xmark & none & 83.15 & 32.6 & 11.3\\
ERANN-1-3 \cite{verbitskiy2021eranns} & \xmark & none & 89.2 & 13.6 & -\\
EnvNet-v2 \cite{tokozume2017learning} & \cmark & none & 84.9 & 101 & 2.7\\
    AemNet WM \cite{lopez2021efficient} & \cmark & none & 81.5 & \textbf{5} & -\\
    EAT-S & \cmark & none & \textbf{92.15} & \textbf{5.3} & 8.3\\
    \midrule
    PANN \cite{kong2020panns} & \xmark & AudioSet & 94.7 & 81 & - \\
ERANN-2-5 \cite{verbitskiy2021eranns} & \xmark & AudioSet & \textbf{96.1} & 37.9 & -\\
    AemNet-DW WM \cite{lopez2021efficient} & \cmark & AudioSet & 92.32 & \textbf{1.2} & -\\
    EAT-S & \cmark & AudioSet & 95.25 & 5.3 & 8.3\\
    EAT-M & \cmark & AudioSet & \textbf{96.3} & 25.5 & 9.6\\
\midrule
    AST \cite{gong2021ast} & \xmark & ImageNet+AudioSet & 95.6 & 88.1 & 26.7\\
PaSST-S \cite{koutini2021efficient} & \xmark & ImageNet+AudioSet & \textbf{96.8} & 85.4 & 25.4\\
    HTS-AT \cmmnt{\tablefootnote{During inference the sample repeated twice rather zero padding to match the pretrained sequence length of 10 seconds}}\cite{chen2022hts} & \xmark & ImageNet+AudioSet & \textbf{97} & \textbf{31} & -\\
    \bottomrule
\end{tabular}
\end{table}

\noindent It is evident from the results that our method is more effective than others under the same settings. In absence of external data, the next in line in accuracy \cite{verbitskiy2021eranns} possess  more parameters\cmmnt{and presumably longer inference time}, while similar model size network \cite{lopez2021efficient} has a   gap in accuracy. In the AudioSet fine-tuned case, we manage achieve SotA while being  lighter than \cite{verbitskiy2021eranns}.

\subsection{UrbanSound8K}
\textit{UrbanSound8K} is an audio dataset containing  labeled sound samples, belonging to  class labels, split to  folds. The samples last up to  seconds and the sampling rate varies from KHz-KHz. The classes are drawn from the urban sound taxonomy and all excerpts are taken from field recordings \footnote{Can be found at \url{www.freesound.org}}. The experiment was conducted on the official 10 fold split, with samples resampled to 22.05KHz and zero-padding the short samples to 4 seconds. 

\begin{table}[htb!]
\centering
  \caption{UrbanSound8K, accuracy with model size and inference time measured on P-100 machine}
  \label{tab:urban8k}
  \begin{tabular}{ccccccl}
    \toprule
    Model & e2e & Pretrained & Accuracy & Parameters & time\\
    \midrule
ESResnet-Att \cite{guzhov2021esresnet} & \xmark & none &  &  & \\
    AemNet WM \cite{lopez2021efficient} & \cmark & none & 81.5 & \textbf{5} & -\\
    ERANN-1-4 \cite{verbitskiy2021eranns} & \xmark & none & 83.5 & 24.1 & -\\
    EAT-S & \cmark & none & \textbf{85.5} & \textbf{5.3} & 8.5\\
    \midrule
    ERANN-2-6 \cite{verbitskiy2021eranns} & \xmark & AudioSet & \textbf{90.8} & 54.5 & -\\
    EAT-S & \cmark & AudioSet & 88.1 & \textbf{5.3} & 8.5\\
    EAT-M & \cmark & AudioSet & \textbf{90} & 25.5 & 9.6\\
    \midrule
    ESResNeXt-fbsp \cite{guzhov2021esresnextfbsp} & \xmark & ImageNet+AudioSet & 89.14 & 25 & \\
  \bottomrule
\end{tabular}
\end{table}

\noindent The results on the UrbanSound8K dataset, detailed in Table \ref{tab:urban8k}, follow the same pattern as on the ESC-50, Table \ref{tab:esc50-scrach}, by outperforming previous approaches in the limited data scenario, while being competitive in the fine-tune mode.

\subsection{SpeechCommands}
\textit {Speech Commands V2} \cite{warden2018speech} is a dataset consisting of  recordings for 35 words with a 1-second duration, with a sampling rate equal to 16KHz. The set has an official train, validation, and test split with  , , and  samples, respectively. Our experiment involves the 35-class classification task. 

\begin{table}[htb!]
\centering
  \caption{Speech Commands V2 (35 classes), accuracy with model size and inference time measured on P-100 machine}
  \label{tab:speechcommands}
  \begin{tabular}{cccccc}
    \toprule
    Model & e2e & Pretrained type & Result & Parameters & time\\
    \midrule
    AST \cite{gong2021ast} & \xmark & ImageNet & 98.11 & 87.3 & 11\\
    HTS-AT \cite{chen2022hts} & \xmark & AudioSet & 98.0 & 31.0 & -\\
EAT-S & \cmark & none & \textbf{98.15} & \textbf{5.3} & 7.5\\
  \bottomrule
\end{tabular}
\end{table}

\noindent In Table \ref{tab:speechcommands}, we see that our approach achieves SotA results even without using external data, while being at least  lighter than other methods. Furthermore, demonstrating that our method is robust to additional content, such as speech. 


\subsection{AudioSet}
\textit{AudioSet} \cite{gemmeke2017audio} is a collection of over  million -second audio clips excised from YouTube videos with a class ontology of  labels covering a wide range of everyday sounds, from human and animal sounds, to natural and environmental sounds, to musical and miscellaneous sounds. The set consist of two subsets, named \textit{balanced} with  samples and \textit{unbalanced} training with  samples, with evaluation set with  samples. The noise augmentations were excluded during training due to the presence of noise, and pink noise in class labels.

\begin{table}[htb]
\centering
  \caption{Audioset, mAP with model size and inference time measured on P-100, w/o external data}
  \label{tab:audest}
  \begin{tabular}{ccccccl}
    \toprule
    Model & e2e & Pretrained & mAP & Parameters & time\\
    \midrule
    AST \cite{gong2021ast} & \xmark & none  &  &  & \\
    ERANN-1-6 \cite{verbitskiy2021eranns} & \xmark & none  & \textbf{45.6} & 54.5 & 14\tablefootnote{Measured on V-100 machine \cite{verbitskiy2021eranns}}\\
    AemNet WM \cite{lopez2021efficient} & \cmark & none  & 33.16 & \textbf{5} & -\\
    HTS-AT \cite{chen2022hts} & \xmark & none & \textbf{45.3} & 31 & -\\
    EAT-S & \cmark & none  &  & \textbf{5.3} & 8.4\\
    EAT-M & \cmark & none & 42.6 & 25.5 & 14.6\\
\bottomrule
\end{tabular}
\end{table}

\noindent Table \ref{tab:audest}, evidently demonstrates the advantage of training a large model, for fitting large sets. Yet, our method can be a considered a good balance in terms of accuracy vs efficiency, without apparent affect on downstream tasks.
\subsection{Efficiency and edge deployment}
In this section the focus will be on model complexity. Complexity translates to model size and inference time, which can induce costs and inflexibility for platforms and applications. Our EAT-S model has 5.3M parameters, which resembles the proportion of MobileNet-V2 architecture \cite{sandler2018mobilenetv2} in both size and inference time, as detailed in Appendix \ref{app:time}. This makes EAT-S a candidate for deploying audio classification capabilities in low-memory edge devices.

\subsection{Ablation study}
In this section we explore the impact of our suggestions for augmentations and architecture. The experiments were conducted on the ESC-50 dataset.

\begin{table}[htbp]
\caption{Ablations - Classification results conducted on ESC-50 (incremental improvements over baselines)}
\label{tab:ablations}
\centering
\subfloat[Baseline  (without any mix)]{\begin{tabular}{cc}
     \toprule
    Model & Relative accuracy \\ & to baseline\\
    \midrule
    +mixup  &  \\
    +cutmix & \\
    +freqmix & \\
    +phasemix  & \\
    \bottomrule
\end{tabular}}\qquad\qquad \subfloat[Baseline  (without architecture modification)]{\begin{tabular}{cc}
    \toprule
    Block & Relative accuracy \\ & to baseline\\
    \midrule
    +modified residual blocks &  \\
    +dilated residual blocks & \\
    +transformer & \\
    \bottomrule
\end{tabular}}
\end{table}

\noindent For Tables \ref{tab:ablations}a and \ref{tab:ablations}b , in each experiment, we incrementally add to the baseline and report the relative result. 
For the augmentation ablation study, Table \ref{tab:ablations}a, the baseline refers to not applying any mixing augmentations, while in the architecture ablation study, Table \ref{tab:ablations}b, the baseline refers to our architecture without the suggested modifications - (a) modified residual block and dilated residual blocks vs. common residual block and (b) transformer vs. convolution layer with global average pooling.
We can see from Tables \ref{tab:ablations}a and \ref{tab:ablations}b that in both cases, the increments significantly improve the accuracy.