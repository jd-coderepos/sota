


\begin{figure*}[ht]
    \centering
\includegraphics[width=1.0\linewidth]{framework.pdf}
\caption{
(a) Overview of the proposed Proposal-based Multiple Instance Learning framework, which consists of candidate proposal generation, proposal feature extraction, proposal classification and refinement.
(b) The Surrounding Contrastive Feature Extraction (SCFE) module extends the boundaries of the candidate proposals and then calculates the outer-inner contrastive features of the candidate proposals.
    (c) The Proposal Completeness Evaluation (PCE) module generates the completeness pseudo label by computing the IoU with the selected pseudo instances.
    (d) The Instance-level Rank Consistency (IRC) loss constrains the normalized relative classification scores within the cluster between RGB and FLOW modalities to be consistent.
    }
    \label{fig:framework}
\end{figure*}

In this section, we elaborate on the proposed Proposal-based Multiple Instance Learning framework (P-MIL) for Weakly-supervised Temporal Action Localization (WTAL),
as illustrated in Figure~\ref{fig:framework}.
Given a video , the goal of WTAL is to predict a set of action instances ,
where  and  denote the start time and end time of the -th action,
 and  represent the action category and the confidence score, respectively.
During training, each video  has its ground-truth video-level category label ,
where  represents the number of action classes.
 if the -th action presents in the video and  otherwise.
The proposed P-MIL framework consists of three steps,
including candidate proposal generation (Sec.~\ref{sec:proposal-generation}),
proposal feature extraction and classification (Sec.~\ref{sec:proposal-based}),
proposal refinement (Sec.~\ref{sec:instance-level}).
The details are as follows.







\subsection{Candidate Proposal Generation}
\label{sec:proposal-generation}
In order to generate the candidate proposals,
an S-MIL model~\cite{mm2021CO2Net} is trained.
We first divide each untrimmed video into non-overlapping 16-frame segments,
and then apply the pretrained feature extractor (\emph{e.g.} I3D~\cite{cvpr2017i3d}) to extract segment-wise features  for both RGB and FLOW modalities,
where  indicates the number of segments in the video and  is the feature dimension.
Following the typical two-branch architecture,
a category-agnostic attention branch is utilized to calculate the attention sequence 
and a classification branch is used to predict the base Class Activation Sequence (CAS) ,
where the -th class indicates the background~\cite{aaai2020background,iccv2019backgroundmodel}.
By multiplying  with  in the temporal dimension,
we can obtain the background suppressed CAS .
After that, the predicted video-level classification scores 
are derived by applying a temporal top- aggregation strategy~\cite{aaai2020background,aaai2021HAMNet,cvpr2022asmloc} to  and , respectively,
followed by a softmax operation.




Guided by the video-level category label , the classification loss is formulated as

where  and , based on the assumption that background is present in all videos but filtered out by the attention sequence .
Furthermore, a sparsity loss~\cite{cvpr2018stpn} 
is also employed on the attention sequence  to focus on the key foreground segments.
Overall, the training objectives are as follows:

where  is a balancing factor.


With the trained S-MIL model,
we apply multiple thresholds  on the attention sequence  to generate the candidate action proposals .
To enable our P-MIL model to learn better foreground-background separation in the training stage,
we also apply extra thresholds  to generate the background proposals ,
where the attention sequence  is below .
Thus, the final candidate proposals for training are formulated as

where  indicates the total number of the candidate proposals.
Note that we only use the action proposals  for inference.



\subsection{Proposal Feature Extraction and Classification}
\label{sec:proposal-based}
Given the candidate proposals ,
previous S-MIL methods use the CAS to calculate the confidence score (\emph{e.g.} Outer-Inner Score~\cite{eccv2018autoloc}) of each proposal.
However, these indirect scoring approaches can lead to suboptimal results.
To address this problem, we propose to directly classify the candidate proposals
and aggregate them into video-level classification scores,
which are supervised by video-level category labels.



{\bf Surrounding Contrastive Feature Extraction}.
For the given candidate proposals , we first extract corresponding proposal features .
Since the training stage is mainly guided by the video-level classification, the classifier tends to focus on discriminative short proposals to minimize the classification loss.
To address this issue, we propose a Surrounding Contrastive Feature Extraction (SCFE) module.
Specifically, given a candidate proposal , we first extend the boundary by  of its length on both the left and right sides, yielding three regions: \emph{left}, \emph{inner}, and \emph{right}.
For each region, we then employ RoIAlign~\cite{iccv2017maskrcnn} followed by max-pooling on the segment features  to extract an associated -dimensional feature vector, indicated by ,  and , respectively.
An intuitive way to obtain the proposal feature is to directly concatenate the three feature vectors and feed them into a fully connected layer.
However, inspired by AutoLoc~\cite{eccv2018autoloc}, we take a more effective approach that calculates the outer-inner contrastive features of the proposal followed by a fully connected layer, which is written as:

where  denotes the concatenate operation.
By taking the surrounding contrastive information into consideration, those discriminative short proposals can be effectively suppressed.


{\bf Classification Head}.
Similar to the pipeline of the S-MIL framework, given the proposal features , a category-agnostic attention branch is then used to predict the attention weights , which indicate the foreground probability of each proposal.
Meanwhile, a classification branch is used to predict the base classification scores  of the proposals.
By multiplying  with , we obtain the background suppressed classification scores .
Finally, the predicted video-level classification scores  are derived by applying a top- pooling followed by softmax to  and , respectively, which are supervised by the video-level category labels.




\subsection{Proposal Refinement}
\label{sec:instance-level}
{\bf Proposal Completeness Evaluation}.
The candidate proposals generated by the S-MIL method may be over-complete, 
which include irrelevant background segments.
In this regard, we present a Proposal Completeness Evaluation (PCE) module.
Given the candidate proposals, we use the attention weights to select the high-confidence proposals as \emph{pseudo instances},
and then obtain the completeness pseudo label of each proposal by computing the Intersection over Union (IoU) with these pseudo instances.
Formally, we first apply a threshold  ( is set to  in our case) to the attention weights  of proposals to select a set of high-confidence proposals .
Then, following the Non-Maximum Suppression (NMS) process,
we select the proposal with the highest attention weight as the pseudo instance,
remove the proposals that overlap with it from , and repeat the process until  is empty.
After that, we acquire a set of pseudo instances .
By computing the IoU between the candidate proposals  and the pseudo instances ,
we can obtain an IoU matrix of  dimensions.
We assign the pseudo instance with the largest IoU to each proposal via taking maximum for the  dimension,
and then we obtain the completeness pseudo labels  for the candidate proposals.
Under the guidance of ,
a completeness branch is introduced to predict the completeness scores  in parallel with the attention branch and the classification branch,
which can help to inhibit the activation of low-quality proposals.


{\bf Instance-level Rank Consistency}.
Due to the NMS process in the testing stage,
the relative scores of the candidate proposals belonging to the same action instance have a significant impact on the detection results.
In order to learn robust relative scores,
we design an Instance-level Rank Consistency (IRC) loss by leveraging the complementarity of RGB and FLOW modalities.
In detail, 
we first apply a threshold  to the attention sequence  to eliminate the low-confidence proposals,
and the remaining proposals are denoted as .
For each proposal  in ,
those candidate proposals that overlap with it are considered as a cluster , where .
The classification scores  corresponding to this cluster are indexed from the RGB and FLOW modalities,
given as  and , respectively, where  indicates one of the ground truth categories.
Then the normalized relative scores within the cluster are formulated as

The Kullback-Leibler (KL) divergence is used to constrain the consistency between RGB and FLOW modalities, defined as:


With the IRC loss, we can achieve reliable detection by discarding proposals with low relative scores in the NMS process.




\subsection{Network Training and Inference}
\label{sec:network}
{\bf Network Training}.
In the training stage, guided by the video-level category label , the main classification loss is formulated as

where  and .
Moreover, with the PCE module, a completeness loss is defined as the Mean Square Error (MSE) between the completeness pseudo labels  and the predicted completeness scores :

Overall, the training objective of our model is

where  and  are balancing hyper-parameters.


{\bf Inference}.
In the testing stage, we first apply the threshold  to the video-level classification scores  and neglect those categories below .
For each remaining category , we score the -th candidate proposal as

Finally, the class-wise soft-NMS~\cite{iccv2017softnms} is employed to remove the duplicate proposals.




\subsection{Discussions}
\label{sec:discussion}
In this section, we discuss the differences between the proposed method and several relevant methods, including AutoLoc~\cite{eccv2018autoloc} and CleanNet~\cite{iccv2019cleannet}.
To deal with the inconsistency between the \emph{localization} objective of the testing stage and the \emph{classification} objective of the training stage,
AutoLoc and CleanNet propose to directly predict the temporal boundaries of action instances, 
with the supervision provided by the Outer-Inner-Contrastive loss and the temporal contrast loss, respectively.
Different from these approaches,
we concentrate on another inconsistency in the S-MIL framework
about what to score between the training and testing stages.
The candidate \emph{proposals} need to be scored in the testing stage,
while the S-MIL classifier is trained to score the \emph{segments} during training.
To resolve this inconsistency,
we propose a novel Proposal-based Multiple Instance Learning framework
that directly classifies the candidate proposals in both the training and testing stages.


\begin{table*}[t]
    \vspace{-1em}
	\footnotesize
	\centering
	\caption{Detection performance comparison with state-of-the-art methods on the THUMOS14 test set.
TSN, UNT and I3D represent TSN~\cite{eccv2016tsn}, UntrimmedNet~\cite{cvpr2017untrimmednets} and I3D~\cite{cvpr2017i3d} features, respectively.
		 means fusing the detection results of the S-MIL and our P-MIL model.
	}
    \vspace{-0.5em}
	\label{tab:thumos14}
	\begin{tabular}{c|c|c|ccccccc|ccc}
    \toprule
    \multirow{2.5}{*}{\bf Supervision} & \multirow{2.5}{*}{\bf Methods}   & \multirow{2.5}{*}{\bf Feature} & \multicolumn{7}{c}{\bf mAP@IoU (\%)}               & \multicolumn{3}{c}{\bf AVG mAP (\%)} \\
                                                                                               \cmidrule{4-13}
                                   &                              &                            & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & 0.1:0.5 & 0.3:0.7 & 0.1:0.7 \\
    \midrule
    \multirow{5}{*}{Fully}         
& TAL-Net~\cite{cvpr2018tal-net}, CVPR2018          & I3D                      & 59.8          & 57.1          & 53.2          & 48.5          & 42.8          & 33.8          & 20.8          & 52.3          & 39.8          & 45.1          \\
                                   & BMN~\cite{iccv2019bmn}, ICCV2019                  & TSN                      & -             & -             & 56.0          & 47.4          & 38.8          & 29.7          & 20.5          & -             & 38.5          & -             \\
& GTAD~\cite{cvpr2020gtad}, CVPR2020                & TSN                      & -             & -             & 54.5          & 47.6          & 40.3          & 30.8          & 23.4          & -             & 39.8          & -             \\
                                   & ContextLoc~\cite{iccv2021contextloc}, ICCV2021    & I3D                      & -             & -             & 68.3          & 63.8          & 54.3          & 41.8          & 26.2          & -             & 50.9          & -             \\
                                   & RefactorNet~\cite{cvpr2022RefactorNet}, CVPR2022  & I3D                      & -             & -             & 70.7          & 65.4          & 58.6          & 47.0          & 32.1          & -             & 54.8          & -             \\
    \midrule
\multirow{22}{*}{Weakly}       
& AutoLoc~\cite{eccv2018autoloc}, ECCV2018                   & UNT                      & -             & -             & 35.8          & 29.0          & 21.2          & 13.4          & 5.8           & -             & 21.0          & -             \\
								   & CleanNet~\cite{iccv2019cleannet}, ICCV2019                 & UNT                      & -             & -             & 37.0          & 30.9          & 23.9          & 13.9          & 7.1           & -             & 22.6          & -             \\
								   \cmidrule{2-13}
								   & STPN~\cite{cvpr2018stpn}, CVPR2018                         & I3D                      & 52.0          & 44.7          & 35.5          & 25.8          & 16.9          & 9.9           & 4.3           & 35.0          & 18.5          & 27.0          \\
								   & WTALC~\cite{eccv2018wtalc}, ECCV2018                       & I3D                      & 55.2          & 49.6          & 40.1          & 31.1          & 22.8          & -             & 7.6           & 39.8          & -             & -             \\
								   & CMCS~\cite{cvpr2019cmcs}, CVPR2019                         & I3D                      & 57.4          & 50.8          & 41.2          & 32.1          & 23.1          & 15.0          & 7.0           & 40.9          & 23.7          & 32.4          \\
								   & WSAL-BM~\cite{iccv2019backgroundmodel}, ICCV2019           & I3D                      & 60.4          & 56.0          & 46.6          & 37.5          & 26.8          & 19.6          & 9.0           & 45.5          & 27.9          & 36.6          \\
& DGAM~\cite{cvpr2020DGAM}, CVPR2020                         & I3D                      & 60.0          & 54.2          & 46.8          & 38.2          & 28.8          & 19.8          & 11.4          & 45.6          & 29.0          & 37.0          \\
								   & EM-MIL~\cite{eccv2020EM-MIL}, ECCV2020                     & I3D                      & 59.1          & 52.7          & 45.5          & 36.8          & 30.5          & 22.7          & \textbf{16.4} & 44.9          & 30.4          & 37.7          \\
& TSCN~\cite{eccv2020TSCN}, ECCV2020                         & I3D                      & 63.4          & 57.6          & 47.8          & 37.7          & 28.7          & 19.4          & 10.2          & 47.0          & 28.8          & 37.8          \\
& CoLA~\cite{cvpr2021cola}, CVPR2021                         & I3D                      & 66.2          & 59.5          & 51.5          & 41.9          & 32.2          & 22.0          & 13.1          & 50.3          & 32.1          & 40.9          \\
								   & AUMN~\cite{cvpr2021aumn}, CVPR2021                         & I3D                      & 66.2          & 61.9          & 54.9          & 44.4          & 33.3          & 20.5          & 9.0           & 52.1          & 32.4          & 41.5          \\
								   & UGCT~\cite{cvpr2021ugct}, CVPR2021                         & I3D                      & 69.2          & 62.9          & 55.5          & 46.5          & 35.9          & 23.8          & 11.4          & 54.0          & 34.6          & 43.6          \\
								   & CO-Net~\cite{mm2021CO2Net}, MM2021                     & I3D                      & 70.1          & 63.6          & 54.5          & 45.7          & 38.3          & 26.4          & 13.4          & 54.4          & 35.7          & 44.6          \\
								   & D2-Net~\cite{iccv2021d2net}, ICCV2021                      & I3D                      & 65.7          & 60.2          & 52.3          & 43.4          & 36.0          & -             & -             & 51.5          & -             & -             \\
								   & FAC-Net~\cite{iccv2021facnet}, ICCV2021                    & I3D                      & 67.6          & 62.1          & 52.6          & 44.3          & 33.4          & 22.5          & 12.7          & 52.0          & 33.1          & 42.2          \\
                                   & FTCL~\cite{cvpr2022ftcl}, CVPR2022                         & I3D                      & 69.6          & 63.4          & 55.2          & 45.2          & 35.6          & 23.7          & 12.2          & 53.8          & 34.4          & 43.6          \\
                                   & RSKP~\cite{cvpr2022rskp}, CVPR2022                         & I3D                      & 71.3          & 65.3          & 55.8          & 47.5          & 38.2          & 25.4          & 12.5          & 55.6          & 35.9          & 45.1          \\
                                   & ASM-Loc~\cite{cvpr2022asmloc}, CVPR2022                    & I3D                      & 71.2          & 65.5          & 57.1          & 46.8          & 36.6          & 25.2          & 13.4          & 55.4          & 35.8          & 45.1          \\
                                   & DCC~\cite{cvpr2022dcc}, CVPR2022                           & I3D                      & 69.0          & 63.8          & 55.9          & 45.9          & 35.7          & 24.3          & 13.7          & 54.1          & 35.1          & 44.0          \\
                                   \cmidrule{2-13}
& \textbf{ours}                                   			& I3D                      & 70.9          & 66.6          & 57.8          & 48.6          & 39.8          & 27.1          & 14.4          & 56.8          & 37.5          & 46.5          \\
                                   & \textbf{ours}                            				& I3D                      & \textbf{71.8} & \textbf{67.5} & \textbf{58.9} & \textbf{49.0} & \textbf{40.0} & \textbf{27.1} & 15.1          & \textbf{57.4} & \textbf{38.0} & \textbf{47.0} \\
    \bottomrule
	\end{tabular}
    \vspace{-0.5em}
\end{table*}



