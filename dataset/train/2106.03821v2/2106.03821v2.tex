\pdfoutput=1
\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}

\usepackage{subcaption}
\usepackage{float}
\usepackage{tablefootnote}
\usepackage{mathrsfs}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}\newcommand{\cmark}{\ding{51}}\usepackage{setspace}
\usepackage{xcolor} \usepackage[hang,flushmargin]{footmisc}

\newcommand\cev[1]{\overleftarrow{#1}}


\title{Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion}
\name{Baptiste Pouthier, Laurent Pilati, Leela K. Gudupudi, Charles Bouveyron, Frederic Precioso}
\address{
  NXP Semiconductors, France\\
  Universit\'e C\^ote d'Azur, Inria, CNRS, LJAD, I3S, Maasai, France}
\email{\{baptiste.pouthier, laurent.pilati, leela.k.gudupudi\}@nxp.com \\ \{charles.bouveyron, frederic.precioso\}@univ-cotedazur.fr}


\hyphenpenalty=10000
\hbadness=10000
\linespread{0.89}

\begin{document}

\maketitle
\begin{abstract}
It is now well established from a variety of studies that there is a significant benefit from combining video and audio data in detecting active speakers. However, either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. This paper outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We finally show that the proposed method significantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset.
\end{abstract}
\noindent\textbf{Index Terms}: active speaker detection, audiovisual, multimodal fusion, multi-objective

\section{Introduction}

Active Speaker Detection (ASD) contemplates on identifying active speakers in a video by analyzing both visual and audio features. Hence, ASD is inherently multimodal in nature, where video and audio data are essential attributes. In recent years there has been considerable interest in the ASD methods based upon audio-visual cues \cite{871073, whos_speaking?_Chakravarty1, Chakravarty2, audio_visualcotraining_Chakravarty3, syncnet}. Despite this interest, the lack of large-scale in-the-wild datasets impeded scientific progress in the field. Unfortunately, most of the prior works are challenged by skewed results owing to the poor quality of the considered datasets.

The recent AVA-ActiveSpeaker dataset \cite{AVA} can potentially overcome these limitations and reshape the ASD field of study. Together with the large in-the-wild dataset, the authors presented a baseline model based on a two-stream network that merges video and audio modalities in an end-to-end fashion. Within the annual AVA-ActiveSpeaker challenge \cite{activitynet_challenge}, Chung \cite{naver} and Zhang et al. \cite{Zhang2019MultiTask_3Dconv} improved this baseline using hybrid 3D-2D CNNs pre-trained on large-scale multimodal datasets \cite{syncnet_embedding}. Unfortunately, this approach encounters two major challenges in practice, as illustrated in Fig.\ref{fig:mainPage}: (1) multi-speaker scenario where multiple persons in a video frame are speaking, and (2)~low-resolution and/or indiscernible faces in video frames. In \cite{Alcazar_2020_CVPR}, Alc\`{a}zar et al. addressed the multi-speakers scenario by learning long-term relationships between speakers, and Huang et al. \cite{optical_flow} handled the uncertainty in video modality by adding optical flow to raw pixel representation to strengthen face characterisation. Nevertheless, these studies focused on ad-hoc objectives with limited scope and little attention has been paid to the aggregated approaches which exploit correlation between both the challenges. In this paper, we propose to learn this correlation using a cross-modal fusion that involves simultaneous learning of the uncertainty in both the modalities using a multi-objective learning scheme as described in Section~\ref{problem_formulation}.

\begin{figure}[t]
\centering
\includegraphics[trim=0mm 4mm 0mm 0mm, clip, width=1\linewidth]{images/main_page_corrected.pdf}\-4ex]
\label{fig:1}
\label{fig:mainPage}
\end{figure}

Traditionally, in multi-task learning, uncertainty is handled by learning adaptive weighting between each task's loss functions \cite{happy, kendall2017multi}. In the field of Automatic Video Description, researchers have investigated attention-based \cite{dot_product_self_attention} fusion mechanisms that capture the importance of each of the modalities. Hore et al. \cite{fusion_before_melissa} proposed a model that selectively uses features from different modalities. This approach was later improved using hierarchical attention fusion in \cite{melissa}. Despite the growing interest of these fusion mechanisms in other disciplines, to the best of our knowledge, no studies have been conducted on ASD problem. The present paper aims to investigate different fusion schemes to solve the ASD problem. We also propose a novel audiovisual fusion mechanism as a first attempt to enrich the self-attention model with uncertainty information to disentangle the practical challenges. 


The proposed method consists of learning a self-attention \cite{dot_product_self_attention} and uncertainty-based fusion mechanism that weights video and audio embeddings in an end-to-end fashion. We use the typical two-stream DNN architecture from the literature \cite{AVA, Alcazar_2020_CVPR}, with a stream per each modality, to encode both embeddings. By characterizing uncertainty of each modality, we aim at disentangling the problem presented in Fig.\ref{fig:mainPage}. Indeed, the fusion scheme we propose determines the viability of each modality at any given time to learn a comprehensive understanding of every situation towards ASD disambiguation. Our solution significantly outperforms state-of-the-art in the AVA-ActiveSpeaker dataset by 4.8 and 1.7 on validation and test sets, respectively.



\begin{figure}[t]
\centering
\includegraphics[trim=2mm 3mm 6mm 3mm, clip, width=1\linewidth]{images/network_global_update.pdf}
\label{network}
\-4ex]
\label{fig:network}
\end{figure}


\section{Active Speaker Detection} \label{ASD}

Fig.\ref{fig:mainPage} demonstrates video and audio modalities may conflict, with non-talking faces affiliated with speech-labeled audio. Conflicting video and audio information lead to a compromised audio-visual learning. Most recent ASD studies \cite{AVA, naver, Zhang2019MultiTask_3Dconv, Alcazar_2020_CVPR, optical_flow} defined a trade-off optimizing both video and audio streams towards video label, sometimes using auxiliary classifiers to increase the  discriminative power of the individual streams towards the unique objective \cite{AVA, Alcazar_2020_CVPR}. However, this approach undermines the learning of the audio stream. In this section, we detail the multi-objective optimization of the video and audio streams towards their respective ground-truth labels to learn unbiased and accurate representations of video and audio modalities. Then, we introduce a novel audiovisual fusion mechanism that uses self-attention and uncertainty indicators to better disentangle the ambiguous scenarios.





\subsection{Multi-Objective Learning} \label{problem_formulation}

Let  be a series of consecutive frames, where  denotes the  face detected in the  frame, and  is the corresponding audio segment.
Each audiovisual sequence is noted . During training, each multimodal sample  is associated with a video-based ground-truth label  where   if the face  is speaking and  otherwise. To cater multi-objective learning, we also define audio-based ground-truth by aggregating video-based labels of each frame using Eq.\ref{eq_audio_label}.

\indent We train a standard two-stream architecture, presented in Fig.\ref{fig:network}, that leverages auxiliary classifiers (Aux) to encourage video and audio networks to minimize their own loss function, thus optimizing both modalities intermediate feature representations. 
The outputs of the Video, Audio and Multimodal networks are denoted as ,, and  respectively; we formulate these quantities as , , and  where , , and  are the weights of the respective networks. To train these weights, we define the loss function  as a cross-entropy loss function . The final multi-objective loss-function  is formulated as , where , , and  respectively. 







\begin{figure}[t]
\centering
\includegraphics[trim=6mm 5mm 6mm 6mm,clip, width=1\linewidth]{images/embedding_i4.pdf}
    \-4.5ex]
\label{fig:embeddings}
\end{figure}

\subsection{Cross-Modality Fusion}\label{fusion}

This section describes a multimodal fusion that focuses on disentangling ambiguous scenarios in ASD problem with self-attention and uncertainty estimation. The key strategy here is to consider dynamically varying weights for each modality at any given time by computing an estimate of their relative uncertainty. Fig.\ref{fig:fusion} illustrates the concept of our fusion mechanism, inspired by the method presented in \cite{melissa}, that 
fuses the information from both modalities in a hierarchical fashion. First, we feed each video  and audio  embedded sequences to separated 128-dim BiGRU layers with hidden state . 
Then, we compute the multimodal embedding  using weighted concatenation as given by Eq.\ref{concatenation}:

where the weights  and  characterise, at each time , the uncertainty of the video and audio modalities. Finally, we use post-fusion 100-dim BiGRU layers with hidden state , a \mbox{2-dimensional} Fully Connected (FC) layer, and a softmax.

In \cite{fusion_before_melissa, melissa}, the  weights are learnt using perceptrons that fully-connect, at any given time, the hidden representation of each modality.
This paper presents a novel approach to compute the weights of the modalities using: (1) an estimation of the uncertainty of both video and audio embedded representations, and (2) self-attention to measure the importance of video and audio modalities in their local temporal context.\\

\noindent{\bf High-Level Attributes:} To characterize the uncertainty, we first rely on a high-level assessment of the quality of the video and audio data. We associate each multimodal training sample  with two attributes  and  that characterize the number of detected faces in frame  and the resolution of the targeted face , respectively. Indeed, higher the number of potential speakers lower the chance for the audio to be discriminative. Similarly, low-resolution face coincides with prediction uncertainty as it is more difficult to interpret facial cues and lips movements on a smaller face thumbnail. We denote  and  the number of pixels in the face thumbnail  before any resizing. To scale the two quantities  and , we first use data binning then target encoding \cite{target_encoding} to replace each value with a blend of posterior and prior probabilities of target over the entire training data. Five bins are used to pre-process the number of detected faces (four bins for  plus a bin handling ). Face-size values are discretized within eight bins with first and last bins being left-open and right-open intervals, respectively.\\

\begin{figure}[t]
\centering
\includegraphics[trim=21.3mm 13.8mm 20mm 29mm,clip, width=1\linewidth]{images/attentionAndCoAndNetwork_reduced.pdf}
   \caption{Multimodal fusion scheme where video and audio embedded representations are merged. FC represents a 2-dim Fully Connected layer with a softmax function.}~\
H^{\{V,A\}} = [h_1^{\{V,A\}}, ..., h_T^{\{V,A\}}]
\label{H_matrix}

S^{\{V,A\}} = softmax(H^{\{V,A\}}H^{{\{V,A\}}^T}+B)
\label{self_attention_weights}

\lambda_t^{\{V,A\}} = W^{\{V,A\}} v_t + b^{\{V,A\}}
\label{lambda_calculation}
-32.75ex]
\subfloat[\label{fig5:embedding_overall} Embedding models overall performance]{\hspace{.5\linewidth}}\hspace*{0mm}
\subfloat[\label{fig5:multimodal_overall} Multimodal models overall performance]{\hspace{.5\linewidth}}\hspace*{0mm}\-4.5ex]
\label{fig5}
\end{figure*}


\begin{table}[t!]
\begin{center}
\caption{Performance comparison of the proposed method with the state of the art. Results are reported on both validation and ActivityNet Challenge hidden test sets.}
\label{tab:SOTA}
\begin{tabular}{lcc}
\Xhline{2\arrayrulewidth}
Method & mAP & AUC\\
\hline
\textit{Validation subset} \\
\bf{Our method} & \bf{91.9} & \bf{96.3}\\
Active Speakers Context \cite{Alcazar_2020_CVPR} & 87.1 & - \\
Huang et al. \cite{optical_flow} & - & 93.2\\
Google Baseline \cite{AVA} & 86.3 & 92.0\\
Naver Corp. (Temporal Convolutions) \cite{naver} & 85.5 & - \\
Zhang et al. \cite{Zhang2019MultiTask_3Dconv} & 84.0 & - \\
\hline
\textit{ActivityNet Challenge Leaderboard} \\
\bf{Our method} & \bf{89.5} & -\\
Naver Corp. \cite{naver} & 87.8 & -\\
Active Speakers Context \cite{Alcazar_2020_CVPR} & 86.7 & -\\
Zhang et al. \cite{Zhang2019MultiTask_3Dconv} & 83.5 & - \\
Google Baseline \cite{AVA} & 82.1 & -\\
\Xhline{2\arrayrulewidth}
\end{tabular}
\end{center}
\vspace{-8mm}
\end{table}

\section{Evaluation and Analysis}
\label{evaluation}
\subsection{Experimental Setup}
\label{evaluation:setup}

\noindent{\bf AVA-ActiveSpeaker Dataset} \cite{AVA}{\bf:} It is the most comprehensive, largest, and challenging publicly available dataset for audiovisual ASD problem. It consists of 262 movies divided into training (120), validation (33) and test (109) sets.
In total, 5,498K faces are labeled with normalized bounding boxes.
For training and validation sets, ground-truths on whether someone is speaking are also provided. The ground-truths for the test set are withheld for the annual ActivityNet Challenge \cite{activitynet_challenge}.\\





\noindent{\bf Training Strategy:} 
The ADAGRAD optimizer \cite{adagrad} is used with a learning rate of 0.015 to train the network in end-to-end fashion for 20 epochs with mini-batches of 16 sequences and without any pre-training. 
Roth et al. \cite{AVA} demonstrated that stacking few consecutive frames as input of the first 2D convolutional layer is beneficial to learn short temporal motion. Therefore, the input to the visual network (Fig.\ref{embeddings:video}) is a stack of 3 consecutive grayscale face thumbnails. The faces are extracted using the provided bounding boxes and resized to 224x224.  We feed the audio embedding network (Fig.\ref{embeddings:audio})
with 13 MFCC features extracted from the preceding 0.5s of audio with a 25ms window and a 10ms step.
Our BiGRUs are trained with 1.12s long segments (28 frames) to capture most of the different speech patterns within the AVA-Active-Speaker dataset, the average continuous speech duration being 1.11s \cite{AVA}.\\












\noindent{\bf Metrics:} For ease of comparison with previous studies, we evaluate the proposed method using the official ActivityNet Challenge evaluation script \cite{AVA} that computes the mean Average Precision (mAP) score. When available, Area Under Receiver Operating Characteristic Curve (AUC) score is also provided in Table~\ref{tab:SOTA}.
As the AVA-ActiveSpeaker test set ground-truths are kept private for the official challenge, most of our performance analysis is conducted on the validation set.

\subsection{Comparison with State Of The Art}
\label{evaluation:SOTA}

Experimental results in Table~\ref{tab:SOTA} show that the proposed method outperforms all existing approaches on both test and validation sets, in terms of mAP and AUC scores. It is worth highlighting that the proposed method outperforms one of the best approaches in \cite{Alcazar_2020_CVPR} by a significant margin of 4.8\% and 2.8\% in mAP score on the validation and test sets respectively. Furthermore, our approach surpasses Naver's method \cite{naver} by 1.7\% and ranks first in the AVA-ActiveSpeaker Leaderboard.





Table \ref{tab:numberOfParameters} compares the number of parameters of the top-ranked models on the ActivityNet Challenge Leaderboard.
Our results are very favorable since our approach has significantly fewer parameters compared to the state of the art and does not necessitates any pre-training and/or ensemble-models. 



\begin{table}[h]
\begin{center}
\caption{Comparison of the gross number of parameters.}
\label{tab:numberOfParameters}
\begin{tabular}{lcc}
\Xhline{2\arrayrulewidth}
Method & \#params & pre-training\\
\hline
\bf{Our method} & \bf{2M} & \xmark \\
Naver Corp. \cite{naver} & 13M & \cmark  \\ 
Active Speakers Context \cite{Alcazar_2020_CVPR} & 22M & \cmark  \\ 
Zhang et al. \cite{Zhang2019MultiTask_3Dconv} & 22M & \cmark  \\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
\end{center}
\vspace{-8mm}
\end{table}


\subsection{Performance Breakdown} \label{evaluation:fusion_eval}

Multi-objective learning, as formulated in Section~\ref{problem_formulation}, aims at (1) learning accurate representations of each modality, and (2) allowing unbiased estimation of the uncertainty of video and audio intermediate features. We therefore evaluate the discriminative power of video and audio embedded representations. Figures \ref{fig5:embedding_overall} and \ref{fig5:embedding_breakdown} detail the performance of the embedding networks detailed in Fig.\ref{fig:embeddings} when used alone. The presented results are with 128-dim BiGRU added on top of each embedding network.
We compare the performance of the audio embedding network optimized towards either video or audio ground-truth labels as discussed in Section \ref{problem_formulation}.
As expected, the performance of the audio embedding network trained with video ground-truths strongly degrades on the number of speakers. It suffers in the ambiguous scenario presented in Fig.\ref{fig:mainPage} where multiple persons share the same audio track.
On the contrary, the performance of the audio network trained with audio labels is almost constant while increasing the number of speakers. We also observed a major mAP score increase of 26.9\% compared with using video labels. Thus,
the multi-objective approach using an independent audio-based labels allows the reliability on the audio network in difficult/ambiguous scenarios.


Figures \ref{fig5:multimodal_overall} and \ref{fig5:multimodal_breakdown} compare the performance
of our fusion method with the Naïve and Hierarchical \cite{melissa} Fusion schemes in different scenarios. Our multi-objective model is first evaluated using a naïve concatenation fusion (NF) to combine video and audio modalities. Here it is crucial to note that the method improves Active Speakers Context \cite{Alcazar_2020_CVPR} mAP by 3.1\% on validation subset (Table \ref{tab:SOTA}). This result highlights the effectiveness of our end-to-end multi-objective learning. Hierarchical Fusion (HF) refers the fusion scheme presented in \cite{melissa}. Note that our adaptation implies a slight modification of the initial method to match our "many-to-many" architecture.
As shown in Fig.\ref{fig5:multimodal_overall}, the proposed fusion scheme clearly has an advantage over NF and HF.
Fig.\ref{fig5:multimodal_breakdown} presents additional comparative analysis results by varying the number of faces detected (left) and the size of the face thumbnails (right). The proposed method clearly outperforms both NF and HF schemes, especially in challenging scenarios.































\section{Conclusion}

In this paper, we proposed a self-attention and uncertainty-based fusion mechanism that learns a comprehensive understanding of every situation towards ASD disambiguation. Our approach catered multi-objective optimization to encourage the learning of unbiased multimodal features. 
Experimental results on the challenging AVA-ActiveSpeaker dataset demonstrate
that the proposed method achieves superior performance than existing methods.
Besides, the proposed method outperformed the state-of-the-art on both validation and test datasets and ranked first in the ActivityNet Challenge, despite having fewer parameters and without any pre-training. 

\section{Acknowledgements}

This work has been supported by the French government, through the 3IA C\^ote d'Azur Investment in the Future project managed by the National Research Agency (ANR) with the reference numbers ANR-19-P3IA-0002.






\bibliographystyle{IEEEtran}

\bibliography{mybib}



\end{document}
