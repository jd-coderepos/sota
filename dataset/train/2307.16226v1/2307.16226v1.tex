\documentclass[sigconf,natbib=false]{acmart}
\DeclareUnicodeCharacter{0301}{}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{url}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{mygray}{gray}{.9}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}


\acmSubmissionID{1615}






\RequirePackage[
  datamodel=acmdatamodel,
  style=acmnumeric,
  ]{biblatex}

\addbibresource{sample-base.bib}
\begin{document}

\title{ScribbleVC: Scribble-supervised Medical Image Segmentation with Vision-Class Embedding}

\begin{comment}
\author[1,2]{Zihan Li}
\author[1]{C. Coauthor}
\affil[1]{Department of Computer Science and Engineering}
\affil[2]{Department of Electrical Engineering}




\institution{Xiamen University, Xiamen, China\\ \email{hongqq@xmu.edu.cn} \and Univeristy of Washington, Seattle, USA
\and Peking Union Medical College Hospital, Beijing, China \and School of Biomedical Engineering, ShanghaiTech University, Shanghai, China \and Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China \email{dgshen@shanghaitech.edu.cn} \and City University of Hong Kong, Hong Kong, China \and Hong Kong Centre for Cerebro-Cardiovascular Health Engineering (COCHE), Hong Kong, China}
\end{comment}

\author{Zihan Li}
\email{zhanli@uw.edu}
\affiliation{\institution{Xiamen University, \\University of Washington}
  \city{Seattle}
  \country{USA}
  \postcode{98101}}

\author{Yuan Zheng}
\email{zhengyuan@stu.xmu.edu.cn}
\affiliation{\institution{Xiamen University}
  \city{Xiamen}
  \country{China}
  \postcode{361005}}

\author{Xiangde Luo}
\email{xiangde.luo@std.uestc.edu.cn}
\affiliation{\institution{University of Electronic Science and Technology of China}
  \city{Chengdu}
  \country{China}
  \postcode{611731}}

\author{Dandan Shan}
\email{shandd@stu.xmu.edu.cn}
\affiliation{\institution{Xiamen University}
  \city{Xiamen}
  \country{China}
  \postcode{361005}}

\author{Qingqi Hong}
\email{hongqq@xmu.edu.cn}
\affiliation{\institution{Xiamen University, COCHE}
  \city{Xiamen}
  \country{China}
  \postcode{361005}}

\renewcommand{\shortauthors}{Li, et al.}

\begin{abstract}
  Medical image segmentation plays a critical role in clinical decision-making, treatment planning, and disease monitoring. However, accurate segmentation of medical images is challenging due to several factors, such as the lack of high-quality annotation, imaging noise, and anatomical differences across patients. In addition, there is still a considerable gap in performance between the existing label-efficient methods and fully-supervised methods. To address the above challenges, we propose ScribbleVC, a novel framework for scribble-supervised medical image segmentation that leverages vision and class embeddings via the multimodal information enhancement mechanism. In addition, ScribbleVC uniformly utilizes the CNN features and Transformer features to achieve better visual feature extraction. The proposed method combines a scribble-based approach with a segmentation network and a class-embedding module to produce accurate segmentation masks. We evaluate ScribbleVC on three benchmark datasets and compare it with state-of-the-art methods. The experimental results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency. The datasets and code are released on GitHub.\blfootnote{Corresponding author: Qingqi Hong. Li, Z and Zheng, Y have the equal contribution.}\footnote{\href{https://github.com/HUANGLIZI/ScribbleVC}{https://github.com/HUANGLIZI/ScribbleVC}}
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Image segmentation}
\ccsdesc[300]{Applied computing~Life and medical sciences}

\keywords{Scribble-supervised learning, Medical image segmentation, Vision-Language embedding}

\maketitle
\vspace{-4mm}
\section{Introduction}
Medical image segmentation plays a crucial role in medical image analysis, particularly in clinical practice where accurate segmentation is necessary for diagnosis and treatment planning. However, achieving accurate segmentation results for complex organs with intricate organizational structures remains a challenge, often requiring manual or semi-automatic methods. Recent studies have demonstrated the potential of deep learning for automatic medical image segmentation. However, creating high-quality medical image datasets is hampered by two issues: the high cost of expert annotation and the difficulty in obtaining high-quality medical images. These challenges limit the practical application of medical image segmentation models. To address these issues, researchers have started exploring label-efficient methods such as using scribble annotations for training. This approach shows promise in improving the performance of medical image segmentation models while reducing the need for expensive and time-consuming expert segmentation annotations and insufficient image annotations. Valvano et al. \cite{MAAG} proposed a scribble-supervised segmentation model based on multi-scale GAN and attention gates by introducing an unpaired segmentation mask, which requires additional annotation masks for model training. Meanwhile, Luo et al. \cite{luo2022scribble} proposed a scribble-supervised segmentation model by training a dual branch network and dynamically mixing pseudo-label supervision. In addition, Cyclemix \cite{Zhang_2022_CycleMix} is used to generate mixed images and regularization the model using circular consistency to perform medical image segmentation based on scribble supervision. While using scribble annotations for training can reduce the need for expensive and time-consuming expert segmentation annotations and insufficient image annotations, the imprecise nature of these labels can limit the accuracy of the resulting segmentation models. The limited supervised signals provided by scribble annotations can hinder the model's ability to learn the necessary visual features required for accurate medical image segmentation. Moreover, medical images often suffer from various quality defects that can adversely affect performance compared to fully supervised methods.

\begin{figure}[ht]
\setlength{\abovecaptionskip}{1mm}
  \centering
  \includegraphics[width=\linewidth]{image//explain_transformer.pdf}
  \caption{Performance comparison of segmentation results produced by different methods: (a) the input images, masks, and scribble annotations, (b) fully-supervised UNet++ \cite{unet++},  (c) scribble-supervised UNet++, and (d) ScribbleVC (ours).}
  \label{comparison}
  \vspace{-6mm}
\end{figure}
To effectively address these above issues, we propose the scribble-supervised model ScribbleVC that utilizes scribble-annotated images and visual class embedding features for training. To enhance the visual features of medical images, our model learns additional class embedding from category information. To address the issue of insufficient expert notes, we adopt scribble-supervised learning, which enables the model to extract features from scribble annotations while reducing the reliance on costly expert annotations. To better extract visual features, we design a CNN-Transformer encoder that unifies global and local features of the image. Our model incorporates two separate decoders, which extract CNN-style and Transformer-style features respectively, to fully utilize the information provided by scribble annotations. These decoders are supervised by the scribble annotations to ensure consistency between the two feature types. We incorporate class embedding features into our model to address the issue of low-quality medical images. Since category information is already present in the scribble annotations, our approach explicitly utilizes the categories, which helps segmentation even in the presence of quality defects. We obtain class embedding features through encoding rules rather than additional encoders, which reduces the number of parameters in the model. The multimodal information enhancement mechanism utilizes visual features and category information and improves pseudo labels through visual-class multimodal features. Overall, the main contributions of this paper are as follows:
 
\begin{itemize}
    \item We propose a brand new model (ScribleVC) for medical image segmentation with visual class embedding. To our knowledge, it is the first exploration of scribble-supervised models for visual-class embedding.
    \item We propose a multimodal information enhancement mechanism to introduce category feature information into visual features. In addition, we uniformly utilize CNN and Transformer features to achieve better visual feature extraction.
    \item To evaluate the performance of ScribbleVC, our study conducts experiments on the ACDC, MSCMRseg, and NCI-ISBI datasets. The results show that ScribbleVC has superior segmentation performance than other state-of-the-art methods, achieving a Dice score of 88.4\%, 86.8\%, and 79.8\% respectively.
\end{itemize}

\section{Related work}

\subsection{Medical Image Segmentation}
In the research and application of medical image segmentation technology\cite{hong2023distance}\cite{shan2023coarse}\cite{xu2022mrdff}, deep learning-based medical image segmentation technology is one of the current research hotspots. Deep learning algorithms are more adaptable to new pathological changes and different image qualities and can handle complex medical image segmentation tasks. Secondly, deep learning algorithms \cite{ronneberger2015u}\cite{milletari2016v}\cite{isensee2021nnu}\cite{luo2022semi} can process medical images that contain common problems such as noise, artifacts, and motion artifacts. Meanwhile, with the continuous development of computer technology, the real-time performance and accuracy of medical image segmentation algorithms based on deep learning have also been greatly improved.  
Typical network structures include U-Net\cite{ronneberger2015u}, etc. U-Net is a classic medical image segmentation model based on convolutional neural networks. The advantage of fully supervised medical image segmentation is that it can achieve high-precision segmentation results, especially in the case of a large amount of annotated data \cite{kirillov2023segment}\cite{qiu2023cor}. However, fully supervised methods typically require a large amount of annotated data for training, which is a bottleneck in the field of medical image segmentation \cite{wang2023SwinMM}. Due to the complexity and diversity of medical images, manually annotating data requires a significant amount of time and effort from professional doctors. In addition, annotated data may be very limited or difficult to obtain. How to train high-performance medical image segmentation models with as little annotated data as possible has also become an important challenge. Luo et al.\cite{luo2021urpc} utilized a pyramid prediction network and multi-scale uncertainty correction to learn from unlabeled data. 
\vspace{-4mm}
\subsection{Scribble-supervised Image Segmentation}
The weakly supervised learning method is another method to solve the problem of insufficient annotation data. The weakly supervised learning method is a training method using partially labeled data or weak supervised signals, which can improve the performance of the model in the case of limited labeled data. Weakly supervised learning methods can be divided into many types, such as tag noise-based methods, image-level annotation-based methods, and scribble-based methods. Scribble annotation refers to users manually drawing simple lines or scribbling to annotate the position of objects in an image. In the field of medical image segmentation, manual annotation data is usually provided in the form of points, lines, or regions. Ji et al. \cite{ji2019scribble} proposed a scribble-based hierarchical weakly supervised learning method for brain tumor segmentation, which combined weakly annotations for model training, including scribbles on the whole tumor and healthy brain tissue and the global labels for each Substructure. Valvano et al. \cite{MAAG} proposed a scribble-supervised segmentation model based on multi-scale GAN and attention gates by introducing an unpaired segmentation mask. These methods typically require additional dense annotations for model training. Therefore, we explored the impact of mask and scribble ratios on performance in our study.

\begin{figure*}[!h]
  \setlength{\abovecaptionskip}{-1mm}
  \centering
    \includegraphics[width=0.86\textwidth]{image/ScribbleVL.pdf}
  \caption{Overview of ScribbleVC, consisting of hybrid encoder-decoder and multimodal information enhancement module.}
  \label{fig:vlnetwork}
  \vspace{-4mm}
\end{figure*}
\vspace{-2mm}
\subsection{Multimodal learning}
Real-world information is often conveyed through multiple modalities, including images, videos, text, speech, and others \cite{Yang2023NeRCo}\cite{li2023chatdoctor}. Multimodal learning aims to identify the optimal feature representations from these diverse sources of information. In the area of natural image processing, the integration of images and other modal data has been widely used in semantic interpretation tasks such as Media Captioning 
\cite{wang2018video}\cite{lu2018neural}, Visual Question Answering 
\cite{hu2020iterative}\cite{marino2019ok}, Text-Image Retrieval \cite{wang2016learning} , and Text-to-Image Generation \cite{rombach2022high}\cite{rombach2022high}. In medical image analysis, different modalities often refer to imaging data acquired from various devices, such as positron emission tomography (PET), magnetic resonance imaging (MRI), and computed tomography (CT). Different modalities can represent complementary image features and information of the same object, and their synergistic cooperation can provide more comprehensive diagnostic information. Xue et al. \cite{xue2021multi} fed PET and CT images into a shared downsampling block to eliminate misleading features. Fu et al. \cite{fu2021multimodal} proposed a multimodal spatial attention module to emphasize the tumor-related regions in PET-CT images and suppress the irrelevant areas.
CLIP \cite{radford2021learning}, which predicts image categories by computing the similarity between images and texts, has gained widespread popularity among researchers. Therefore, recent works \cite{huang2021gloria}\cite{li2023lvit} in the field of medical image analysis have also attempted to incorporate textual information to improve performance in relevant tasks. For instance, GLoRIA \cite{huang2021gloria} learned global and local representations by comparing subregions of images and words in radiology reports. Zhou et al. \cite{zhou2022generalized} performed generalized radiograph representation learning by cross-supervising between medical images and radiology reports. However, different from the above methods, ScribbleVC does not require additional text annotations but can achieve multimodal interaction by extracting category information from the image.
\vspace{-4mm}
\section{Our approach}
Due to the limitations of annotation information in scribble annotations, we incorporate category feature information to assist in the extraction of image features. We design a Scribble-supervised model, ScribbleVC,  which is a multimodal model that consists of two main components: a hybrid encoding and decoding structure and a multimodal information enhancement module, as shown in Figure \ref{fig:vlnetwork}. 
Our proposed method utilizes a hybrid encoder that combines a convolutional neural network and Transformer to encode the input medical image. This encoder generates two types of feature representations: CNN image features and Transformer image features. To enhance the segmentation accuracy, our method incorporates known category information into these feature representations. It is achieved by extracting class embedding features and adding them to the corresponding features. This operation results in the formation of CNN multimodal features and Transformer multimodal features. Two separate decoders are designed to handle the differences in feature representations between the CNN and Transformer. To further improve visual information transmission, we introduce residual connections between the encoding and decoding modules of the CNN. Our method utilizes scribble supervision, pseudo-label supervision, and category supervision on different branches to generate a final segmentation result.
\begin{figure*}[!ht]
\setlength{\abovecaptionskip}{-1mm}
  \centering
  \includegraphics[width=0.86\textwidth]{image/Multimodal.pdf}
  \caption{Overview of Multimodal Information Enhancement.}
  \vspace{-4mm}
\end{figure*}
\vspace{-2mm}
\subsection{Hybrid Encoder-Decoder}
\subsubsection{CNN-Trans Encoder}
Visual descriptors can be categorized into local features and global representations. Local features are compact vector representations of the image's local neighborhood, while global representations include shape descriptors, contour representations, and distant object types. In deep learning, convolutional neural networks leverage convolutional operations to construct multi-layer networks and collect local features, preserving them as feature maps. On the other hand, Transformer aggregates global representations by compressing patch embeddings via cascaded self-attention modules. To leverage the advantages of both local features and global representations, we propose a hybrid encoder that combines convolutional neural networks and Transformers. By exploiting the complementary nature of the two styles of features, the hybrid encoder inputs local context from the CNN branch into the feature map, enhancing the local perception ability of the Transformer branch. Similarly, the global features from the Transformer branch are gradually fed back to patch embedding, enriching the global representation of the CNN branch. This process also enables interaction between convolutional neural network feature information and Transformer feature information.

where  denotes the output of the i-th Trans layer, whose inputs are  and . And  will be the input of the i-th Conv layer. The output of the i-th Conv layer is .

\subsubsection{Classification Head}
It is worth noting that to achieve the automatic generation of category feature information, two classification heads are designed at the tail of the encoder, which is respectively used to process CNN image features and Transformer image features. The classification head can automatically generate the category information contained in images, thereby achieving multimodal information enhancement.

where  consists of one LayerNorm layer and one linear layer. And  consists of one Conv2d layer and one AvgPool layer. The inputs of  are  and .

\subsubsection{Segmentation Decoder}
In the decoder section, we design the CNN decoder and Transformer decoder for processing different types of multimodal features. Both decoders utilize deconvolution to perform upsampling operations to ensure the reproducibility of model performance. The difference is that the encoder and decoder parts of the CNN have added residual connections to ensure that local features of the image can be better learned by the model. Due to the global nature of the category features imposed by the multimodal information enhancement mechanism, no additional residual connections were added to the Transformer encoder and decoder. Finally, the outputs of the CNN-branch decoder and the Transformer-branch decoder form a mixed prediction result, which is used as a pseudo label to realize the weakly supervised learning.
\vspace{-2mm}
\subsection{Multimodal Information Enhancement}
To fully utilize the category information in the Scribble labels, we propose a multimodal information enhancement mechanism. First, we extract the feature vectors of the category. Next, the hybrid encoder undergoes feature interaction and outputs image feature vectors with both global and local information. These image feature vectors are then predicted by the classification head to obtain prediction probabilities. We then use category embedding to multiply the predicted probability of each category by the image feature vector, resulting in the characteristic features of each sample in the batch corresponding to the category. Finally, we calculate the mean of the characteristic features of the batch to obtain the category feature vector of the corresponding category in the -th batch.
To update the historical category feature vector, the second step involves averaging the category feature vector with the corresponding vector from the previous batch, followed by prediction using the classification header. If the new category feature vector outperforms the previous vector in the prediction results, it is updated as the new historical category feature vector; otherwise, the previous vector remains unchanged.
In the third step, we combine class embeddings with image feature vectors to obtain multimodal fusion feature vectors. For a sample, if the predicted class probability of its image feature vector is greater than 0.5, we consider its expected prediction value for that class to be 1, indicating that the sample can introduce class feature vectors. If all the predicted expected values of the categories for the sample are 1, and they meet the conditions described in the second step, we use the prediction probability of the sample for the category as the weight of the category feature vector. After the weighted sum, we add the image feature vector to obtain the mixed feature. However, if a category with a predicted expected value of 1 for the sample does not meet the conditions described in the second step, the fusion feature vector will not be updated, and the feature vector output by the encoder will still be used as input to the decoder. The design aims to highlight all categories together when enhancing image feature vectors, as adding only one may lead to imbalanced category features.

During training, the model retains historical category feature vectors, which are then used in the testing phase to replace the category feature vectors. In the testing phase, the model does not extract category feature vectors from each image feature vector in the test set, and it does not perform the second step of detecting category feature vectors and updating historical category feature vectors. Instead, it only calculates the category prediction probability for each image feature vector in the test set.
To generate predictions, the model multiplies the historical category feature vector corresponding to the predicted category by the predicted probability value of the image feature vector in that category. It then weights and sums all historical category feature vectors that meet the conditions, and adds the resulting vector to the image feature vector as input to the decoder.
\vspace{-2mm}
\subsection{Training Strategy and Loss Function}
The overall training strategy is divided into four parts. The first part is the supervision of scribble annotations, the second part is the supervision of unlabeled pixels by using the threshold-based mechanism, the third part is the loss of gating condition random field, and the fourth part is the classification supervision of category.

In the first part, the segmentation results of the convolutional neural network and Transformer are supervised by partial cross entropy function  with scribble annotation. Among them, is the prediction result of the convolutional neural network branch, and is the predicted result of the Transformer branch. is a partial cross entropy function, which is defined as:

where  is the set of categories in the image, and is the set of labeled pixels in the scribble ;  and  are the probability that the -th pixel in the scribble belongs to the -th class, and the probability that the -th pixel in the prediction results belongs to the -th class, respectively.

In the second part, we employ a threshold-based pseudo-labeling mechanism to supervise unlabeled pixels. The dual-branch network is utilized to generate two sets of predictions with different attentional focuses, namely local information and center position offset. The mixed prediction is used to supervise both branches. The pseudo label  is generated using a threshold-based approach to reduce errors. It is achieved by combining the predicted probabilities from both branches according to the following formula:

To include dynamic prediction results for a given pixel, the prediction probabilities of both branches at that pixel must exceed the threshold . This criterion ensures that unreliable predicted pixels are excluded from the dynamic prediction results. In this study, the threshold was set to 0.5. Additionally, a random number  is generated for each batch, with a range of (0,1). This strategy allows the convolutional neural network branches and Transformer branches to learn from each other through pseudo-labels, and dynamic mixing improves the diversity of the pseudo-labels. The threshold setting helps prevent prediction errors from misleading the model through the use of pseudo-labels.

To balance the local and global information provided by CNN and Transformers, we proposes a strategy to limit the gradient flow between the two branches while avoiding consistency learning. It maintains the independence of the two branches and allows the supervised signal to propagate to all unlabeled pixels. The mixed prediction results are then used to supervise both branches during training. The dynamic prediction  supervision approach amplifies the supervised signal from limited annotated pixels to the entire image. The formula for dynamic prediction supervision is:

The third part introduces the gated conditional random field loss. Gated conditional random field loss is commonly used in the training of weakly supervised image segmentation methods. It helps to eliminate the influence of irrelevant pixels on the classification of the current pixel. Furthermore, it places more emphasis on the semantic boundary rather than the semantic relationship between regions. This simplifies the process of combining a conditional Random field and CNN. Additionally, it does rely on high-dimensional filters. The gated conditional random field loss is defined as:

where  is the number of pixels, and  is the gating function to mask unexpected pixel positions.  The similarity between the pixels  and  is measured by the function . Additionally,  and  are the prediction probability value of pixels  and , respectively.

The fourth part improves the accuracy of category features by applying a classification loss to the encoded features. The category loss is defined as:

where , and  represent the prediction probability of convolutional neural networks and Transformer networks, respectively, while  represents the actual category of input images. Because an input image may correspond to multiple categories, the predicted probability and actual category adopt a binary loss of :

where  represents the total number of samples. To ensure probability value can predict multiple categories simultaneously, the prediction probability  is obtained through the sigmoid function instead of the softmax function.
Finally, the total loss function is:

where  are the weights of each part of the loss to balance different supervised losses.


\section{Experiments}
\begin{figure*}[htbp]
\setlength{\abovecaptionskip}{0mm}
\centering 
\includegraphics[width=0.92\textwidth]{image//result.pdf}
\caption{Qualitative comparison between our method (ScribbleVC) and other state-of-the-art methods on ACDC and MSCMRseg datasets. Subscripts \textit{F} and \textit{S} indicate segmentation models are trained with dense annotations or scribble annotations.}
\label{results}
\vspace{-5mm}
\end{figure*}
\subsection{Setup}
\subsubsection{Datasets}
\textbf{ACDC dataset} \cite{acdc} includes 100 cine-MRI scans with manual scribble annotations for RV, LV, and MYO supplied by \cite{MAAG}. The scans are divided into sets of 70, 15, and 15 for training, validation, and testing. We split the training set into two halves, 35 images with scribble labels and 35 masks with full annotations. It is worth noting that the corresponding masks are not used in training. \textbf{MSCMRseg dataset} \cite{mscmr1, mscmr2} includes Late Gadolinium Enhancement MRI scans from 45 cardiomyopathy patients, each with scribble annotations of LV, MYO, and RV provided by \cite{Zhang_2022_CycleMix}. The 45 scans are randomly partitioned into three sets: 25 for training, 5 for validation, and 15 for testing. \textbf{NCI-ISBI dataset} \cite{Clark2013Cancer} is from ISBI 2013 Prostate Magnetic Resonance Imaging Challenge. There are 80 volumes in the NCI-ISBI dataset, which are divided into a training set and a test set of 3:1. All the labels in the training set are scribble annotations, and the category information is provided by the scribble labels. Category information is only used as a supervisory signal during training and is not provided during testing.

\subsubsection{Implementation details}
The model was implemented using Pytorch and trained on one NVIDIA RTX 3090. To expand the training set, we applied random rotation, flipping, and noise to the images. The learning rate is fixed at 1e-4, and the weight decay is set to 0.0005. Our model is trained with AdamW optimizer for 300 epochs in the experiments. We empirically set the weights  to (1, 0.5, 0.1, 0.1) in Eqn. \ref{eq:text_total}. For all datasets, the Dice coefficient (Dice) is used as an evaluation metric.
\vspace{-2mm}
\begin{table}[!ht]\footnotesize
\setlength{\abovecaptionskip}{0mm}
  \caption{Performance comparison between our method (ScribbleVC) and other state-of-the-art methods on ACDC. Bold denotes the best performance.}
    \resizebox{0.98\columnwidth}{!}{\begin{tabular}{l|c|ccc|c}
    \toprule[1pt]
    Methods & Data  & LV & MYO & RV & Avg \\ \hline
    \multicolumn{6}{l}{\textbf{35 scribbles}} \\ \hline
    UNETR\cite{Hatamizadeh_2022_WACV} & scribbles & .688 & .330 & .180 & .399 \\
    SwinUNETR\cite{swinunetr} & scribbles & .768 & .683 & .640 & .697 \\
    SwinUNet\cite{swinunet} & scribbles & .862 & .768 & .735 & .788 \\ 
    TransUNet\cite{chen2021transunet} & scribbles & .599 & .475 & .428 & .501 \\ 
    TFCNs\cite{li2022tfcns} & scribbles & .703 & .614 & .619 & .645 \\ 
    UNet\cite{lin2016scribblesup} & scribbles & .624 & .537 & .526 & .562 \\
    UNet\cite{MAAG} & scribbles & .784 & .675 & .563 & .674 \\
    UNet\cite{liu2022weakly} & scribbles & .605 & .599 & .655 & .620 \\
    UNet\cite{kim2019mumford} & scribbles & .873 & .812 & .833 & .839 \\
    UNet\cite{EM} & scribbles & .789 & .761 & .788 & .779 \\
    UNet\cite{crf} & scribbles & .766 & .661 & .590 & .672 \\
    UNet\cite{unet+} & scribbles & .785 & .725 & .746 & .752 \\
    UNet\cite{unet++} & scribbles & .846 & .787 & .652 & .761 \\
    MixUp\cite{mixup} & scribbles & .803 & .753 & .767 & .774 \\
    Cutout\cite{cutout} & scribbles & .832 & .754 & .812 & .800 \\
    CutMix\cite{cutmix} & scribbles & .641 & .734 & .740 & .705 \\
    Puzzle Mix\cite{puzzlemix} & scribbles & .663 & .650 & .559 & .624 \\
    Co-mixup\cite{comixup} & scribbles & .622 & .621 & .702 & .648 \\
    CycleMix\cite{Zhang_2022_CycleMix} & scribbles & .883 & .798 & .863 & .848 \\
    \rowcolor{mygray}
    \textbf{ScribbleVC} & scribbles & \textbf{.914} & \textbf{.866} & \textbf{.870} & \textbf{.884} \\ \hline
    \multicolumn{6}{l}{\textbf{35 scribbles + 35 unpaired masks}} \\ \hline
    UNet\cite{MAAG} & mixed & .404 & .597 & .753 & .585 \\
    PostDAE\cite{postdae} & mixed & .806 & .667 & .556 & .676 \\
    ACCL\cite{accl}  & mixed & .878 & .797 & .735 & .803 \\
    MAAG\cite{MAAG}  & mixed & .879 & .817 & .752 & .816 \\ \hline
    \multicolumn{6}{l}{\textbf{35 masks}} \\ \hline
    UNet\cite{unet} & masks & .892 & .830 & .789 & .837 \\
    UNet\cite{unet+} & masks & .849 & .792 & .817 & .820 \\
    UNet\cite{unet++} & masks & .875 & .798 & .771 & .815 \\
    Puzzle Mix\cite{puzzlemix} & masks &.849 & .807 & .865 & .840 \\
    VT-UNet & masks & .895 & .807 & .804 & .836 \\
    UNETR\cite{Hatamizadeh_2022_WACV} & masks & .926 & .844 & .845 & .872 \\
    SwinUNet\cite{swinunet} & masks & .900 & .812 & .818 & .843 \\
    \bottomrule[1pt]
    \end{tabular}}
  \label{ACDCresult}\vspace{-6mm}
\end{table}
\vspace{-4mm}
\subsection{Performance Comparison with Other State-Of-The-Art Methods}
To demonstrate the comprehensive segmentation performance of our method, we compare ScribleVC with different SOTA methods.

1) Transformer-based fully-supervised segmentation methods, including UNEt TRansformers (UNETR) \cite{Hatamizadeh_2022_WACV}, Swin UNEt TRansformers (SwinUNETR) \cite{swinunetr}, SwinUNet \cite{swinunet}, TransUNet \cite{chen2021transunet} and TFCNs \cite{li2022tfcns} which are the medical image segmentation models utilizing a combination of convolutional layers and Transformers.

2) Different scribble-supervised strategies on UNet: partial cross-entropy loss (pce) \cite{lin2016scribblesup}, 
weighted partial cross-entropy loss (wpce) \cite{MAAG}, uncertainty self-ensembling and transformation-consistent regularization (ustr) \cite{liu2022weakly}, mumford–shah Loss (mloss) \cite{kim2019mumford}, 
entropy minimization (em) \cite{EM}, and conditional random field (crf) \cite{crf}.

3) Different scribble-supervised frameworks with the same loss: the partial cross-entropy loss on different variants of UNet \cite{lin2016scribblesup}, including UNet \cite{unet+} 
and UNet \cite{unet++}.

4) Different data augmentation: MixUp \cite{mixup}, Cutout \cite{cutout}, CutMix \cite{cutmix}, Puzzle Mix \cite{puzzlemix}, Co-mixup \cite{comixup}, CycleMix \cite{Zhang_2022_CycleMix}.
Second, we also compare some adversarial learning methods, including UNet \cite{MAAG}, PostDAE \cite{postdae}, ACCL \cite{accl}, and MAAG \cite{MAAG}.

Finally, we investigate the fully-supervised methods: UNet \cite{unet}, UNet \cite{unet+},  UNet \cite{unet++}, Puzzle Mix \cite{puzzlemix} and CycleMix \cite{Zhang_2022_CycleMix}. 
\vspace{-4mm}
\begin{table}[ht!]\footnotesize
\setlength{\abovecaptionskip}{0mm}
  \centering
  \caption{Performance comparison between our method (ScribbleVC) and other state-of-the-art methods on MSCMRseg. Bold denotes the best performance.}
  \resizebox{0.98\columnwidth}{!}{\begin{tabular}{l|c|ccc|c}
    \toprule[1pt]
    Methods & Data  & LV & MYO & RV & Avg \\ \hline
    \multicolumn{6}{l}{\textbf{25 scribbles}} \\  \hline
    UNet\cite{unet+} & scribbles & .494 & .583 & .057 & .378 \\
    UNet\cite{unet++} & scribbles & .497 & .506 & .472 & .492 \\
    MixUp\cite{mixup} & scribbles & .610 & .463 & .378 & .484 \\
    Cutout\cite{cutout} & scribbles & .459 & .641 & .697 & .599 \\
    CutMix\cite{cutmix} & scribbles & .578 & .622 & .761 & .654 \\
    Puzzle Mix\cite{puzzlemix} & scribbles & .061 & .634 & .028 & .241 \\
    Co-mixup\cite{comixup} & scribbles & .356 & .343 & .053 & .251 \\
    DMPLS\cite{luo2022scribble} & scribbles & .881 & .644 & \textbf{.863} & .796 \\
    CycleMix\cite{Zhang_2022_CycleMix} & scribbles & .870 & .739 & .791 & .800 \\
    \rowcolor{mygray}
    \textbf{ScribbleVC} & scribbles & \textbf{.921} & \textbf{.830} & .852 & \textbf{.868} \\  \hline
    \multicolumn{6}{l}{\textbf{25 masks}} \\  \hline
    UNet\cite{unet} & masks & .850 & .721 & .738 &  .770 \\
    UNet\cite{unet+} & masks & .857 & .720 & .689 & .755 \\
    UNet\cite{unet++} & masks & .866 & .745 & .731 & .774 \\
    Puzzle Mix\cite{puzzlemix} & masks & .867 & .742 & .759 & .789 \\
    CycleMix\cite{Zhang_2022_CycleMix} & masks & .864 & .785 & .781 & .810 \\ \bottomrule[1pt]
    \end{tabular}}
  \label{MSCMRresult}\vspace{-3mm}
\end{table}

As shown in Table \ref{ACDCresult} and Table \ref{MSCMRresult}, our ScribbleVC model outperforms a number of training strategies, model architectures, and data augmentation techniques based on UNet in scribble supervision. In particular, it outperforms the SOTA method CycleMix by a margin of 3.6\% (88.4\% vs 84.8\%) and 6.8\% (86.8\% vs 80.0\%) on ACDC and MSCMRseg, respectively, which demonstrates the effectiveness of incorporating Transformer global context to CNN local features in scribble-supervised semantic segmentation. Meanwhile, we found that the full-annotation-designed Transformer-based medical image segmentation models only achieved average performance on scribble data. In contrast, our ScribbleVC model can achieve superior performance by jointly leveraging local detailed information and global context. The ACDC results in the second section (scribbles + unpaired masks) of Table~\ref{ACDCresult} demonstrate significant performance improvements of ScribbleVC compared to other weakly-supervised methods. It can be observed that the Dice scores of all three categories of LV, MYO, and RV achieved by ScribbleVC have exceeded the previous best method (MAAG \cite{MAAG}). We believe those methods with additional unpaired masks could only learn limited shape priors due to the vague of segmentation boundaries.

On the other hand, ScribbleVC can overcome this limitation by utilizing the self-attention mechanism of Transformers to learn global shapes without additional fully-annotated masks. 
In the last section of Table~\ref{ACDCresult} and Table~\ref{MSCMRresult}, we also compared the proposed ScribbleVC with several fully-supervised learning methods on ACDC and MSCMR, including CycleMix with fully-supervised learning. As shown in the tables, the results with fully-supervised learning are better than those with scribble annotations plus additional unpaired masks because of the acquisition of pixel-wise relationships.
However, our ScribbleVC outperforms the fully-supervised methods at a lower annotation cost, demonstrating the great potential in medical image segmentation.
\vspace{-4mm}
\subsection{Comparison with Pseudo-label Generating Methods on the ACDC dataset}

To compare our ScribbleVC with other pseudo-label generating methods, we employed UNet with only partial cross-entropy loss (pce) \cite{lin2016scribblesup} as the base segmentation network architecture plus: 1) using pseudo label generated by Random Walker (rw) \cite{grady2006random}, 2) incorporating pseudo-labeling plus label filtering named Scribble2Label (s2l) \cite{S2L}, 3) with dual-branch using dynamically mixed pseudo labels supervision (DMPLS) \cite{luo2022scribble}.
Additionally,  we also compare with TS-UNet \cite{can2018learning}, a variant of UNet+ with a combination of the random walker, dense CRF, and uncertain estimation.
\vspace{-2mm}
\begin{table}[!htbp]\footnotesize
\setlength{\abovecaptionskip}{0mm}
  \centering
  \caption{Comparison with pseudo-label generating methods on the ACDC dataset.}
  \resizebox{0.98\columnwidth}{!}{\begin{tabular}{l|c|ccc|c}
    \toprule[1pt]
    Methods & Data  & LV & MYO & RV & Avg \\
    \midrule
    TS-UNet\cite{can2018learning} & scribbles & .479 & .408 & .272 & .386 \\
    UNet\cite{lin2016scribblesup} & scribbles & .624 & .537 & .526 & .562 \\
    UNet\cite{grady2006random} & scribbles & .840 & .730 & .802 & .791 \\
    UNet\cite{S2L} & scribbles & .767 & .715 & .765 & .820 \\
DMPLS\cite{luo2022scribble} & scribbles & .875 & \textbf{.903} & .852 & .870 \\
    \rowcolor{mygray}
    \textbf{ScribbleVC} & scribbles & \textbf{.914} & .866 & \textbf{.870} & \textbf{.884} \\
    \bottomrule[1pt]
    \end{tabular}}
  \label{pseudo}\vspace{-2mm}
\end{table}\vspace{-2mm}

As shown in Table~\ref{pseudo}, some pseudo-label-based methods with scribble annotations can achieve reasonably good performance, with both S2L and DMPLS achieving 80\% or higher. Nevertheless, our method outperforms these methods by a significant margin, confirming the enhancement of pseudo-label generating of the CNN-Transformer synergy in our network.
\vspace{-2mm}
\subsection{Comparison with Scribble-supervised Methods on the NCI-ISBI dataset}
In this section, we compared our method with scribble-supervised segmentation methods in NCI-ISBI scribble-annotated medical images. Specifically, we employed UNet as the base segmentation network architecture with
partial cross-entropy loss (Scribblesup) \cite{lin2016scribblesup}, 
utilizing uncertainty-aware self-ensembling and transformation-consistent regularization (USTM) \cite{liu2022weakly}, 
using entropy minimization (SSEM) regularization \cite{EM}, incorporating pseudo-labeling plus label filtering named Scribble2Label (S2L) \cite{S2L}, 3D-UNet \cite{cciccek20163d}, SegNet \cite{badrinarayanan2017segnet} and CRF-RNN\cite{monteiro2018conditional}.
All baseline models are trained only on the labeled pixels of the scribble data. 
The results are reported in Table~\ref{tab:ISBIresult}. We found that 
our ScribbleVC model can achieve superior performance to other scribble-supervised and even fully-supervised methods by jointly leveraging local detailed information and global context, which demonstrates the effectiveness of incorporating Transformer global context to CNN local features in scribble-supervised semantic segmentation.
\vspace{-2mm}
\begin{table}[!ht]\tiny
\setlength{\abovecaptionskip}{0mm}
    \centering
    \caption{Comparison with scribble-supervised methods on the Prostate (NCI-ISBI)  dataset.} \label{tab:ISBIresult}
    \resizebox{0.98\columnwidth}{!}{\begin{tabular}{l|c|cc|c}
    \toprule
        Methods & Data  & PZ & CG & Avg  \\ 
        \midrule
        Scribblesup\cite{lin2016scribblesup} & scribbles & .271 & .369 & .320  \\ 
USTM\cite{liu2022weakly} & scribbles & .401 & .209 & .305 \\ 
        SSEM\cite{EM} & scribbles & .501 & .393 & .447 \\
S2L\cite{S2L} & scribbles & .674 & .650 & .662 \\
        3D-UNet\cite{cciccek20163d} & scribbles &	.670 & .829 &.750\\
        SegNet\cite{badrinarayanan2017segnet}& scribbles &.720 & .837 &.778\\
        CRF-RNN\cite{monteiro2018conditional} & scribbles & .698 & \textbf{.863} &.781\\
        \rowcolor{mygray}
        \textbf{ScribbleVC} & scribbles & \textbf{.743} & .854 & \textbf{.798} \\ \midrule
        UNet & masks & .723 & .832 & .778  \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-4mm}
\end{table}
\vspace{-2mm}
\subsection{Ablation Experiments}
The section studies the effectiveness of different components of the proposed ScribbleVC, including CNN, Transformer, and CLS modules. Table~\ref{tab:ablation} reports the results.
Compared with \#1 with only convolutional neural network branches and \#2 with only Transformer branches, \#3 with both convolutional neural network and Transformer branches has better performance, indicating that the synergistic effect of convolutional neural network and Transformer has a promoting effect on the model. Compared to \#3, \#4 with multimodal information enhancement mechanism exhibits better performance, confirming the effectiveness of this mechanism.
\vspace{-2mm}
\begin{table}[ht!]\footnotesize
\setlength{\abovecaptionskip}{0mm}
  \centering
  \caption{Ablation study: ScribbleVC for image segmentation with different settings, including the CNN branch, the Transformer branch, and CLS module.} \label{tab:ablation}
   \resizebox{0.98\columnwidth}{!}{\begin{tabular}{c|ccc|cc|c}
    \toprule[1pt]
   Models & CNN & Transformer & CLS & PZ & CG & Avg \\
   \midrule
    \#1  & \checkmark &    &    & .666 & .167 & .416 \\
    \#2  &    & \checkmark &    & .433 & .633 & .533 \\
    \#3  & \checkmark & \checkmark &    & .708 & .843 & .775 \\
    \#4  & \checkmark & \checkmark & \checkmark & \textbf{.743} & \textbf{.854} & \textbf{.798} \\
    \bottomrule[1pt]
  \end{tabular}}
  \vspace{-2mm}
\end{table}
\vspace{-4mm}
\subsection{Data Sensitivity Experiments}
The data sensitivity study investigates the performance of ScribbleVC with different numbers of scribble annotations during training. 
As shown in Table~\ref{sensitivity}, the performance of ScribbleVC has been boosted gradually as the number of scribble-annotated samples increases. Even with just 20 training samples with scribbles, our model can reach 75.1\%, which confirms that ScribbleVC is able to achieve satisfactory segmentation results with a relatively small amount of scribble annotations. The overall performance of ScribbleVC stabilized when the number of scribble annotations reached 40 (67\% of 60 scribbles). The best performance can be achieved by using all 60 scribble annotations, resulting in an accuracy of 79.8\%. 
\vspace{-6mm}
\begin{table}[!ht]\tiny
\setlength{\abovecaptionskip}{0mm}
  \centering
  \caption{Data sensitivity study: the performance of ScribbleVC with the different numbers of scribbles for training.}
  \resizebox{0.98\columnwidth}{!}{{\begin{tabular}{c|c|cc|c}
    \toprule
    Method & Scribble Data & PZ & CG & Avg \\
    \midrule
    ScribbleVC & 20 scribbles & .668 & .833 & .751 \\
    ScribbleVC & 30 scribbles & .713 & .834 & .773 \\ 
    ScribbleVC & 40 scribbles & .728 & .846 & .787 \\
    ScribbleVC & 50 scribbles & .726 & .859 & .792 \\
    ScribbleVC & 60 scribbles & \textbf{.743} & \textbf{.854} & \textbf{.798} \\
\bottomrule
    \end{tabular}}}
  \label{sensitivity}\vspace{-2mm}
\end{table}\vspace{-4mm}
\section{Conclusion}
In this paper, we present ScribleVC, a novel model for medical image segmentation using scribble supervision. By leveraging category information from scribble labels, ScribleVC enhances the effectiveness of this annotation method. Our approach employs a multimodal information enhancement mechanism to incorporate category feature information into visual features. Additionally, we achieve improved visual feature extraction by leveraging both CNN and Transformer features. As the first exploration of scribble-supervised models for visual-class embedding, ScribleVC is a simple yet effective model that delivers high-quality pixel-level segmentation results. Experimental results show that our ScribleVC outperforms state-of-the-art methods on the ACDC, MSCMRseg, and NCI-ISBI datasets.\\
\noindent\textbf{Acknowledgments}: This work was supported in part by the Natural Science Foundation of Fujian Province of China (No. 2020J01006), the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No. VRLAB2022AC04), and ITC-InnoHK Projects at Hong Kong Centre for Cerebro Cardiovascular Health Engineering (COCHE).


\printbibliography
\end{document}
