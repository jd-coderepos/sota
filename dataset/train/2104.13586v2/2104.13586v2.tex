\section{Experiments}

\subsection{Dataset Preparation}
We use six datasets in our experiments: FineGYM~\cite{shao2020finegym}, NTURGB+D~\cite{Shahroudy_2016_NTURGBD, Liu_2019_NTURGBD120}, 
Kinetics400~\cite{carreira2017quo, yan2018spatial}, UCF101~\cite{soomro2012ucf101}, HMDB51~\cite{kuehne2011hmdb} 
and Volleyball~\cite{ibrahim2016hierarchical}. 
Unless otherwise specified, we use the Top-Down approach for pose extraction:
the detector is Faster-RCNN~\cite{ren2015faster} with the ResNet50 backbone, the pose estimator is 
HRNet~\cite{sun2019deep} pre-trained on COCO-keypoint~\cite{lin2014microsoft}.
For all datasets except FineGYM, 2D poses are obtained by directly applying Top-Down pose estimators to RGB inputs.
We report the \textbf{Mean Top-1} accuracy for FineGYM and \textbf{Top-1} accuracy for other datasets.
We adopt the 3D ConvNets implemented in MMAction2~\cite{2020mmaction2} in experiments. 

\noindent
\textbf{FineGYM. } 
FineGYM is a fine-grained action recognition dataset with 29K videos of 99 fine-grained gymnastic action classes. 
During pose extraction, we compare three different kinds of person bounding boxes: 
1. Person bounding boxes predicted by the detector (\textbf{Detection});
2. GT bounding boxes for the athlete in the first frame, tracking boxes for the rest frames (\textbf{Tracking}).
3. GT bounding boxes for the athlete in all frames (\textbf{GT}).
In experiments, we use human poses extracted with the third kind of bounding boxes unless otherwise noted.

\noindent
\textbf{NTURGB+D. } 
NTURGB+D is a large-scale human action recognition dataset collected in the lab.
It has two versions, namely NTU-60 and NTU-120 (a superset of NTU-60): NTU-60 contains 57K videos of 60 human actions, while NTU-120 contains 114K videos of 120 human actions.
The datasets are split in three ways: 
Cross-subject (\textbf{X-Sub}), Cross-view (\textbf{X-View}, for NTU-60), Cross-setup (\textbf{X-Set}, for NTU-120), 
for which action subjects, camera views, camera setups are different in training and validation.
The 3D skeletons collected by sensors are available for this dataset. 
Unless otherwise specified, we conduct experiments on the \textbf{X-sub} splits for NTU-60 and NTU-120.

\noindent
\textbf{Kinetics400, UCF101, and HMDB51. } 
The three datasets are general action recognition datasets collected from the web.
Kinetics400 is a large-scale video dataset with 300K videos from 400 action classes.
UCF101 and HMDB51 are smaller, contain 13K videos from 101 classes and 6.7K videos from 51 classes, respectively.
We conduct experiments using 2D-pose annotations extracted with our Top-Down pipeline. 

\noindent
\textbf{Volleyball. }
Volleyball is a group activity recognition dataset with 4830 videos of 8 group activity classes.
Each frame contains approximately 12 persons, while only the center frame has annotations for GT person boxes.
We use tracking boxes from~\cite{SendoMVA2019Heatmapping} for pose extraction.

\begin{table*}[t]
	\vspace{2mm}
	\begin{minipage}{.58\textwidth}
		\captionsetup{font=small}
		\centering
		\captionof{table}{ \textbf{PoseConv3D \textit{v.s.} GCN. } 
			We compare the performance of PoseConv3D and GCN on several datasets. 
			For PoseConv3D, we report the results of 1/10-clip testing. 
			We exclude parameters and FLOPs of the FC layer, since it depends on the number of classes.}
		\label{tab-gcnvscnn}
		\vspace{-2mm}
		\resizebox{\columnwidth}{!}{
		\tablestyle{7pt}{1.1}
			\begin{tabular}{c|ccc|cccc}
				\shline
				& \multicolumn{3}{c|}{MS-G3D} & \multicolumn{4}{c}{Pose-SlowOnly} \\ 
				\shline
				Dataset & Acc & Params & FLOPs & 1-clip & 10-clip & Params & FLOPs \\ 
				\shline
				FineGYM & 92.0  & 2.8M & 24.7G  & \textbf{92.4} & \textbf{93.2} & \multirow{4}{*}{\textbf{2.0M}} & \multirow{4}{*}{\textbf{15.9G}} \\ 
				NTU-60 & 91.9  & 2.8M &  16.7G  & \textbf{93.1} & \textbf{93.7} & & \\
				NTU-120 & 84.8  & 2.8M & 16.7G  & \textbf{85.1} & \textbf{86.0} & & \\
				Kinetics400 & \textbf{44.9}  & 2.8M & 17.5G  & 44.8 & \textbf{46.0} & & \\
				\shline
		\end{tabular}}
		\vspace{-2mm}
		\centering
		\captionof{table}{ \textbf{Recognition performance w. different dropping KP probabilities.} PoseConv3D is more robust to input perturbations. }
		\label{tab-robustness}
		\resizebox{.8\columnwidth}{!}{
			\tablestyle{8pt}{1.1}
			\begin{tabular}{c|ccccc} \shline
				Method /    & 0    & 1/8  & 1/4  & 1/2  & 1    \\ 
				\shline
				MS-G3D & 92.0 & 91.0 & 90.2 & 86.5 & 77.7 \\
				+ robust training	& 90.9	& 91.0	& 91.0	& 91.0	& 90.6 \\ 
				\shline
				Pose-SlowOnly & \textbf{92.4} & \textbf{92.4} & \textbf{92.3} & \textbf{92.1} & \textbf{91.5} \\ 
				\shline
			\end{tabular}}
	\end{minipage}
	\hfill
	\begin{minipage}{.40\textwidth}
		\centering 
		\captionsetup{font=small}
		\captionof{table}{ 
				\textbf{Train/Test w. different pose annotations. } 
				PoseConv3D shows great generalization capability in the cross-PoseAnno setting
				(LQ for low-quality; HQ for high-quality). }
			\label{tab-generalization}
		\captionsetup[subfloat]{font=footnotesize,position=bottom}
\subfloat[Train/Test w. Pose from different estimators. \label{tab-robustxanno}]{
			\resizebox{\columnwidth}{!}{
			\tablestyle{4pt}{1.2}
			\begin{tabular}{c|ccc}
				\shline
					   & \multicolumn{3}{c}{Train  Test} \\ \cline{2-4}
					   & HQ  LQ   & LQ  HQ  & LQ  LQ  \\ \shline
				MS-G3D    & 79.3  & 87.9 & 89.0  \\ \hline
				PoseConv3D & \textbf{86.5} & \textbf{91.6} & \textbf{90.7} \\ \shline
			\end{tabular}}}
		\vspace{2mm}
		\subfloat[Train/Test w. Pose extracted with different boxes. \label{tab-robustxbox}]{
			\resizebox{\columnwidth}{!}{
			\tablestyle{4pt}{1.2}
			\begin{tabular}{c|ccc}
				\shline
					   & \multicolumn{3}{c}{Train  Test} \\ \cline{2-4}
					   & HQ  LQ   & LQ  HQ  & LQ  LQ  \\ \shline
				MS-G3D    & 78.5  & 89.1 & 82.9  \\ \hline
				PoseConv3D & \textbf{82.1} & \textbf{90.6} & \textbf{85.4} \\ \shline
			\end{tabular}}}
			\vspace{-2mm}
	\end{minipage}
	\vspace{-2mm}
\end{table*}


\subsection{Good properties of PoseConv3D}
To elaborate on the good properties of 3D convolutional networks over graph networks, we compare Pose-SlowOnly with MS-G3D~\cite{liu2020disentangling}, a representative GCN-based approach in multiple dimensions.
Two models take exactly the \textbf{same} input (coordinate-triplets for GCN, heatmaps generated from coordinate-triplets for PoseConv3D). 

\noindent\textbf{Performance \& Efficiency.}
In performance comparison between PoseConv3D and GCN, 
we adopt the input shape 48\x 56\x 56 for PoseConv3D.
Table~\ref{tab-gcnvscnn} shows that under such configuration,
PoseConv3D is lighter than the GCN counterpart, both in the number of parameters and FLOPs.
Though being light-weighted, PoseConv3D achieves competitive performance on different datasets.
The 1-clip testing result is better than or comparable with a state-of-the-art GCN while requiring much less computation.
With 10-clip testing, PoseConv3D consistently outperforms the state-of-the-art GCN.
Only PoseConv3D can take advantage of multi-view testing since it subsamples the entire heatmap volumes to form each input.
Besides, PoseConv3D uses the same architecture and hyperparameters for different datasets, 
while GCN relies on heavy tuning of architectures and hyperparameters on different datasets~\cite{liu2020disentangling}.

\noindent
\textbf{Robustness.} 
To test the robustness of both models, we can drop a proportion of keypoints in the input and see how such perturbation will affect the final accuracy.
Since limb keypoints\footnote{There are eight limb keypoints: bow, wrist, knee, ankle (left/right).} 
are more critical for gymnastics than the torso or face keypoints, 
we test both models by randomly dropping one limb keypoint in each frame with probability .
In Table~\ref{tab-robustness}, we see that PoseConv3D is highly robust to input perturbations:
dropping one limb keypoint per frame leads to a moderate drop (less than 1\%) in Mean-Top1,
while for GCN, it's 14.3\%. 
Someone would argue that we can train GCN with the noisy input, 
similar to the dropout operation~\cite{srivastava2014dropout}.
However, even under this setting, the Mean-Top1 accuracy of GCN still drops by 1.4\% for the case .
Besides, with robust training, there will be an additional 1.1\% drop for the case .
The experiment results show that PoseConv3D significantly outperforms GCN in terms of robustness for pose recognition.

\noindent
\textbf{Generalization.}
To compare the generalization of GCN and 3D-CNN, we design a cross-model check on FineGYM.
Specifically, we use two models,~\ie, HRNet (Higher-Quality, or HQ for short) and MobileNet (Lower-Quality, LQ) for pose estimation and train two PoseConv3D on top, respectively.
During testing, we feed LQ input into the model trained with HQ one and vice versa.
From Table~\ref{tab-robustxanno}, we see that
the accuracy drops less when using lower-quality poses for both training \& testing with PoseConv3D compared to GCN.
Similarly, we can also vary the source of person boxes, using either \textbf{GT} boxes (HQ) or \textbf{tracking} results (LQ) for training and testing.
The results are shown in Table~\ref{tab-robustxbox}.
The performance drop of PoseConv3D is also much smaller than GCN.

\noindent
\textbf{Scalability.}
The computation of GCN scales linearly with the increasing number of persons in the video,
making it less efficient for group activity recognition. 
We use an experiment on the Volleyball dataset~\cite{ibrahim2016hierarchical} to prove that. 
Each video in the dataset contains 13 persons and 20 frames.
For GCN, the corresponding input shape will be 13\x 20\x 17\x 3, \textbf{13} times larger than the input for one person. 
Under such configuration, the number of parameters and FLOPs for GCN is 2.8M and 7.2G (13\x).
For PoseConv3D, we can use one \textbf{single} heatmap volume (with shape 17\x 12\x 56\x 56) to represent all 13 persons\footnote{
In experiments, we find that using a single heatmap volume to represent all people is the best practice (compared to using one heatmap volume for each person). 
Please refer to Sec~\ref{sec-groupractice} for more details. }. 
The base channel-width of Pose-SlowOnly is set to 16, leading to only 0.52M parameters and 1.6 GFLOPs.
Despite the much smaller parameters and FLOPs, PoseConv3D achieves 91.3\% Top-1 accuracy on Volleyball-validation, 2.1\% higher than the GCN-based approach.


\begin{table}[t]
	\captionsetup{font=small, position=top}
	\caption{\textbf{The design of RGBPose-Conv3D. } 
		Bi-directional lateral connections outperform uni-directional ones in the early stage feature fusion.}
	\label{tab-bi-directional}
	\vspace{-2mm}
	\centering
	\resizebox{\linewidth}{!}{
	\tablestyle{4pt}{1.2}
	\begin{tabular}{c|cccc}
		\shline
		& late fusion & RGB  Pose & Pose  RGB & RGB  Pose \\ \shline
		1-clip  & 92.6       & 93.0    & 93.4    & \textbf{93.6}  \\ \hline
		10-clip & 93.4       & 93.7    & 93.8    & \textbf{94.1}  \\ \shline
		\end{tabular}}
		\vspace{-2mm}
\end{table}

\begin{table}[t]
	\captionsetup{font=small, position=top}
	\caption{ \textbf{The universality of RGBPose-Conv3D. } 
		The \textbf{early+late} fusion strategy works both on RGB-dominant NTU-60 and Pose-dominant FineGYM. }
	\label{tab-universality}
	\vspace{-2mm}
	\centering
	\resizebox{\linewidth}{!}{
	\tablestyle{4pt}{1.2}
	\begin{tabular}{c|cccc}
		\shline
		& RGB         & Pose        & late fusion  & early+late fusion  \\ \shline
		FineGYM & 87.2 / 88.5 & 91.0 / 92.0 & 92.6 / 93.4 & \textbf{93.6 / 94.1} \\ \hline
		NTU-60  & 94.1 / 94.9 & 92.8 / 93.2 & 95.5 / 96.0 & \textbf{96.2 / 96.5} \\ \shline
	\end{tabular}}
	\vspace{-4mm}
\end{table}


\begin{table*}[t]
	\captionsetup{font=small, position=top}
	\vspace{-3mm}
	\centering 
	\caption{ \textbf{PoseConv3D is better or comparable to previous state-of-the-arts. } 
		With estimated high-quality 2D skeletons and the great capacity of 3D-CNN to learn spatiotemporal features, 
		PoseConv3D achieves superior performance across \textbf{5 out of 6} benchmarks.
		 means using joint/limb-based heatmaps.
		++ denotes using the same human skeletons as ours.
		Numbers with * are reported by \cite{shao2020finegym}.
	} 
	\label{tab-posesota}
	\vspace{-2mm}
	\resizebox{.95\linewidth}{!}{
	\tablestyle{6pt}{1.2}
	\begin{tabular}{c|cccccc}
	\shline
	Method       						& NTU60-XSub & NTU60-XView & NTU120-XSub & NTU120-XSet & Kinetics & FineGYM \\ \shline
	ST-GCN~\cite{yan2018spatial} 		& 81.5       & 88.3        & 70.7        & 73.2        & 30.7     & 25.2*   \\ 
	AS-GCN~\cite{li2019actional}   	& 86.8       & 94.2        & 78.3        & 79.8        & 34.8     & -       \\ 
	RA-GCN~\cite{song2020richly} 		& 87.3       & 93.6        & 81.1        & 82.7        & -        & -       \\ 
	AGCN~\cite{shi2019two}         	& 88.5       & 95.1        & -           & -           & 36.1     & -       \\ 
	DGNN~\cite{shi2019skeleton} 	  	& 89.9       & 96.1        & -           & -           & 36.9     & -       \\ 
	FGCN~\cite{yang2020feedback}       & 90.2       & 96.3        & 85.4        & 87.4        & -        & -       \\ 
	Shift-GCN~\cite{cheng2020skeleton}	& 90.7       & 96.5        & 85.9        & 87.6        & -        & -       \\ 
	DSTA-Net~\cite{shi2020decoupled}   & 91.5       & 96.4        & 86.6        & 89.0        & -        & -       \\ 
	MS-G3D~\cite{liu2020disentangling} & 91.5       & 96.2        & 86.9        & 88.4        & 38.0     & -       \\ \shline
	MS-G3D ++							& 92.2       & 96.6        & \textbf{87.2}  & 89.0     & 45.1     & 92.6    \\ \shline
	PoseConv3D ()      			& \textbf{93.7}   & \textbf{96.6}  & 86.0   & \textbf{89.6} & \textbf{46.0} & \textbf{93.2}	\\ \shline
	PoseConv3D ()    			& \textbf{94.1}   & \textbf{97.1}  & 86.9   & \textbf{90.3} & \textbf{47.7} & \textbf{94.3} \\ \shline
	\end{tabular}}
	\vspace{-2mm}
\end{table*}

\begin{table*}[t]
	\captionsetup{font=small}
	\centering 
	\caption{ \textbf{Comparison to the state-of-the-art of Multi-Modality Action Recognition. } 
			 Strong recognition performance is achieved on multiple benchmarks with multi-modality fusion. 
			 R, F, P indicate RGB, Flow, Pose. } 
	\label{tab-rgbposesota}
	\vspace{-2mm}
	\captionsetup[subfloat]{font=small}
	\captionsetup[subffloat]{justification=centering}
\resizebox{\linewidth}{!}{
	\subfloat[\label{tab-rgbpose-conv3d} Mulit-modality action recognition with \textit{RGBPose-Conv3D}. ]{
		\tablestyle{6pt}{1.3}
		\begin{tabular}{c|c|c}
			\shline
			Dataset	& Previous state-of-the-art	& \textbf{Ours}  \\ \shline
			FineGYM-99        		& \begin{tabular}[c]{@{}c@{}} 87.7 (R) \cite{kwon2021learning} \end{tabular} & \textbf{95.6} (R + P) \\ \hline
			NTU60 (X-Sub / X-View)  & \begin{tabular}[c]{@{}c@{}} 95.7 / 98.9 (R + P) \cite{davoodikakhki2020hierarchical} \end{tabular} & \textbf{97.0 / 99.6} (R + P) \\ \hline
			NTU120 (X-Sub / X-Set)  & \begin{tabular}[c]{@{}c@{}} 90.7 / 92.5 (R + P) \cite{das2021vpn++} \end{tabular} & \textbf{95.3 / 96.4} (R + P) \\ \shline
		\end{tabular}}
	\hspace{1mm}
	\subfloat[\label{tab-rgbpose-latefusion} Mulit-modality action recognition with \textit{LateFusion}.\footnotemark ]{
		\tablestyle{6pt}{1.3}
		\begin{tabular}{c|c|c|c}
			\shline
			Dataset  & Previous state-of-the-art & \textbf{Ours (Pose)} & \textbf{Ours (Fused)}  \\ \shline
			Kinetics400	& \begin{tabular}[c]{@{}c@{}} 84.9 (R) \cite{liu2021video} \end{tabular} & 47.7 & \textbf{85.5 } (R + P)\\ \hline
			UCF101 & \begin{tabular}[c]{@{}c@{}} 98.6 (R + F) \cite{duan2020omni} \end{tabular} & 87.0 & \textbf{98.8} (R + F + P) \\ \hline
			HMDB51 	& \begin{tabular}[c]{@{}c@{}} 83.8 (R + F)	\cite{duan2020omni} \end{tabular} & 69.3 & \textbf{85.0} (R + F + P) \\ \shline
		\end{tabular}}}
	\vspace{-4mm}
\end{table*}

\subsection{Multi-Modality Fusion with RGBPose-Conv3D}
The 3D-CNN architecture of PoseConv3D makes it more flexible to fuse pose with other modalities via some early fusion strategies.
For example, in \emph{RGBPose}-Conv3D, lateral connections between the \emph{RGB}-pathway and \emph{Pose}-pathway are exploited for cross-modality feature fusion in the early stage.
In practice, we first train two models for RGB and Pose modalities separately and use them to initialize the \emph{RGBPose}-Conv3D.
We continue to finetune the network for several epochs to train the lateral connections. 
The final prediction is achieved by late fusing the prediction scores from both pathways.
\emph{RGBPose}-Conv3D can achieve better fusing results with \textbf{early+late} fusion.

We first compare uni-directional lateral connections and bi-directional lateral connections in Table~\ref{tab-bi-directional}.
The result shows that bi-directional feature fusion is better than uni-directional ones for RGB and Pose. 
With bi-directional feature fusion in the early stage, the \textbf{early+late} fusion with 1-clip testing can outperform the \textbf{late} fusion with 10-clip testing. 
Besides, \emph{RGBPose}-Conv3D also works in situations when the importance of two modalities is different.
The pose modality is more important in FineGYM and vice versa in NTU-60. 
Yet we observe performance improvement by \textbf{early+late} fusion on both of them in Table~\ref{tab-universality}. 
We demonstrate the detailed instantiation of \emph{RGBPose}-Conv3D we used in Sec~\ref{sec-rgbpose2stream}. 

\subsection{Comparisons with the state-of-the-art}

\noindent\textbf{Skeleton-based Action Recognition. }
In Table~\ref{tab-posesota}, we compare PoseConv3D with prior works for skeleton-based action recognition. 
Prior works (Table~\ref{tab-posesota} upper) use 3D skeletons collected with Kinect for NTURGB+D, 
2D skeletons extracted with OpenPose for Kinetics 
(details for FineGYM skeleton data are unknown).
PoseConv3D adopts 2D skeletons extracted with good practices introduced in Sec~\ref{sec-pose-extraction}, which have better quality. 
We instantiate PoseConv3D with the SlowOnly backbone, feed 3D heatmap volumes of shape 48\x 56\x 56 as inputs, and report the accuracy obtained by 10-clip testing. 
For a fair comparison, we also evaluate the state-of-the-art MS-G3D with our 2D human skeletons (\emph{MS-G3D++}):
\emph{MS-G3D++} directly takes the extracted coordinate-triplets  as inputs, while \emph{PoseConv3D} takes pseudo heatmaps generated from the coordinate-triplets as inputs. 
With high quality 2D human skeletons, \emph{MS-G3D++} and \emph{PoseConv3D} both achieve far better performance than previous state-of-the-arts, 
demonstrating the \textbf{importance} of the proposed practices for pose extraction in skeleton-based action recognition. 
When both take high-quality 2D poses as inputs, PoseConv3D outperforms the state-of-the-art MS-G3D across \textbf{5 of 6} benchmarks, 
showing its great spatiotemporal feature learning capability. 
PoseConv3D achieves by far the best results on \textbf{3 of 4} NTURGB+D benchmarks. 
On Kinetics, PoseConv3D surpasses MS-G3D++ by a noticeable margin, significantly outperforming all previous methods. 
Except for the baseline reported in \cite{shao2020finegym}, no work aims at skeleton-based action recognition on FineGYM before,
while our work first improves the performance to a decent level.

\footnotetext{For K400, we fuse PoseConv3D Pose predictions (Top1 acc 47.7\%) with VideoSwin~\cite{liu2021video} RGB predictions.
For UCF101 and HMDB51, we fuse PoseConv3D Pose predictions (with K400 PoseConv3D pre-training, 
87\% Top1 acc on UCF101, 69.3\% Top1 acc on HMDB51, details in Sec~\ref{sec-mmar-ucfhmdb}) 
with OmniSource~\cite{duan2020omni} RGB+Flow predictions.}

\noindent
\textbf{Multi-modality Fusion. }
As a powerful representation itself, skeletons are also complementary to other modalities, like RGB appearance. 
With multi-modality fusion (\emph{RGBPose-Conv3D} or \emph{LateFusion}),
we achieve state-of-the-art results across \textbf{8} different video recognition benchmarks. 
We apply the proposed \emph{RGBPose-Conv3D} to FineGYM and 4 NTURGB+D benchmarks, 
using R50 as the backbone; 16, 48 as the temporal length for \emph{RGB}/\emph{Pose}-Pathway. 
Table~\ref{tab-rgbpose-conv3d} shows that our \textbf{early+late} fusion achieves excellent performance across various benchmarks.
We also try to fuse the predictions of PoseConv3D directly with other modalities with \emph{LateFusion}. 
Table~\ref{tab-rgbpose-latefusion} shows that late fusion with the Pose modality can push the recognition precision to a new level.
We achieve the new state-of-the-art on three action recognition benchmarks: Kinetics400, UCF101, and HMDB51.
On the challenging Kinetics400 benchmark, 
fusing with PoseConv3D predictions increases the recognition accuracy by 0.6\% beyond the state-of-the-art~\cite{liu2021video},
which is strong evidence for the complementarity of the Pose modality.



\subsection{Ablation on Heatmap Processing}

\noindent\textbf{Subjects-Centered Cropping. }
Since the sizes and locations of persons can vary a lot in a dataset, 
focusing on the action subjects is the key to reserving as much information as possible with a relatively small  budget.
To validate this, we conduct a pair of experiments on FineGYM with input size 32\x 56\x 56, with or without subjects-centered cropping.
We find that subjects-centered cropping is helpful in data preprocessing, which improves the Mean-Top1 by 1.0\%, from 91.7\% to 92.7\%.

\noindent
\textbf{Uniform Sampling. }
The input sampled from a small temporal window may not capture the entire dynamic of the human action. 
To validate this, we conduct experiments on FineGYM and NTU-60.
For fixed stride sampling, which samples from a fixed temporal window, we try to sample 32 frames with the temporal stride 2, 3, 4; 
for uniform sampling, we sample 32 frames uniformly from the entire clip.
In testing, we adopt a fixed random seed when sampling frames from each clip to make sure the test results are reproducible. 
From Figure~\ref{fig-unisample}, we see that uniform sampling consistently outperforms sampling with fixed temporal strides.
With uniform sampling, 1-clip testing can even achieve better results than fixed stride sampling with 10-clip testing.
Note that the video length can vary a lot in NTU-60 and FineGYM.
In a more detailed analysis, we find that uniform sampling mainly improves the recognition performance 
for longer videos in the dataset (Figure~\ref{fig-unideeper}).
Besides, uniform sampling also outperforms fixed stride sampling on RGB-based recognition on the two datasets\footnote{
	Please refer to Sec~\ref{sec-uniformrgb} for details and discussions. }.

\noindent
\textbf{Pseudo Heatmaps for Joints and Limbs. }
GCN approaches for skeleton-based action recognition usually ensemble results of multiple streams (joint, bone, \textit{etc.}) 
to obtain better recognition performance~\cite{shi2019two}. 
The practice is also feasible for PoseConv3D.
Based on the coordinates  we saved, 
we can generate pseudo heatmaps for joints and limbs. 
In general, we find that both joint heatmaps and limb heatmaps are good inputs for 3D-CNNs. 
Ensembling the results from joint-PoseConv3D and limb-PoseConv3D (namely PoseConv3D ()) can lead to noticeable and consistent performance improvement.

\noindent
\textbf{3D Heatmap Volumes \emph{v.s} 2D Heatmap Aggregations. }
The 3D heatmap volume is a more `lossless' 2D-pose representation,
compared to 2D pseudo images aggregating heatmaps
with colorization or temporal convolutions.
PoTion~\cite{choutas2018potion} and PA3D~\cite{yan2019pa3d} are not evaluated on popular benchmarks for skeleton-based action recognition, 
and there are no public implementations.  
In the preliminary study, we find that the accuracy of PoTion is much inferior () to GCN or PoseConv3D (all ).
For an apple-to-apple comparison, we also re-implement PoTion, PA3D 
(with higher accuracy than reported) 
and evaluate them on UCF101, HMDB51, and NTURGB+D. 
PoseConv3D achieves much better recognition results with 3D heatmap volumes, 
than 2D-CNNs with 2D heatmap aggregations as inputs. 
With the lightweight X3D, PoseConv3D significantly outperforms 2D-CNNs, 
with comparable FLOPs and far fewer parameters (Table~\ref{tab-volumevsaggr}).

\begin{figure}[t]
	\captionsetup{font=small}
	\includegraphics[width=\linewidth]{figs/univsfix} 
	\vspace{-8mm}
	\caption{ \textbf{Uniform Sampling outperforms Fix-Stride Sampling. } All results are for 10-clip testing, except Uni-32[1c], which uses 1-clip testing. }
	\label{fig-unisample}
	\vspace{-3mm}
\end{figure}

\begin{figure}[t]
	\captionsetup{font=small}
	\centering
	\includegraphics[width=\linewidth]{figs/unisample} 
	\vspace{-5mm}
	\caption{ \textbf{Uniform Sampling helps in modeling longer videos. }  
		L: The length distribution of NTU60-XSub val videos.
		R: Uniform Sampling improves the recognition accuracy of longer videos.}
	\label{fig-unideeper}
	\vspace{-3mm}
\end{figure}

\begin{table}[t]
	\captionsetup{font=small, position=top}
	\centering 
	\caption{ \textbf{An apple-to-apple comparison between 3D heatmap volumes and 2D heatmap aggregations. }}
	\label{tab-volumevsaggr}
	\vspace{-2mm}
	\resizebox{\linewidth}{!}{
	\tablestyle{6pt}{1.3}
	\begin{tabular}{c|ccc|cc}
	\shline
	Method		 	 & HMDB51 & UCF101 & NTU60-XSub & FLOPs & Params \\ 
	\shline
	PoTion~\cite{choutas2018potion}   & 51.7   & 67.2   & 87.8   & 0.60G & 4.75M  \\
	PA3D~\cite{yan2019pa3d} & 53.5   & 69.1   & 88.6   & 0.65G  & 4.81M  \\ 
	\shline
	Pose-SlowOnly (Ours) & \textbf{58.6}   & \textbf{79.1}   & \textbf{93.7}   & 15.9G  & \textbf{2.0M} \\
	Pose-X3D-s (Ours)  & \textbf{55.6}   & \textbf{76.7}  & \textbf{92.3}   & \textbf{0.60G}  & \textbf{0.24M} \\ 
	\shline
	\end{tabular}}
	\vspace{-5mm}
\end{table}