\renewcommand\thefigure{A-\arabic{figure}}
\renewcommand\thesection{A-\arabic{section}}
\renewcommand\thetable{A-\arabic{table}}
\renewcommand\theequation{A-\arabic{equation}}
\renewcommand\thealgorithm{A-\arabic{algorithm}} 
\setcounter{equation}{0}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{algorithm}{0}
\setcounter{table}{0}

\begin{figure*}[ht!]
\centering
\begin{tabular}{@{}ccc@{}}

\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/interactive_tool/position.jpg}
  \caption{Position}
  \label{fig:tool:rgb}
\end{subfigure}
&
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/interactive_tool/labels.jpg}
  \caption{Ground Truth}
  \label{fig:tool:gt}
\end{subfigure}
 & 
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/interactive_tool/features.jpg}
  \caption{\textcolor{red}{Linearity}, \textcolor{green}{Planarity} \& \textcolor{blue}{Verticality}}
  \label{fig:tool:feat}
\end{subfigure}
\\

\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/interactive_tool/rgb.jpg}
  \caption{RGB}
  \label{fig:tool:rgb}
\end{subfigure}
&
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/interactive_tool/predictions_with_errors.jpg}
  \caption{Predictions \& Errors}
  \label{fig:tool:pred}
\end{subfigure}
&
\begin{subfigure}[b]{0.35\textwidth}
  \includegraphics[width=\linewidth]{images/interactive_tool/level_2.jpg}
  \caption{Level-2}
  \label{fig:tool:level2}
\end{subfigure}
\\

\end{tabular}

\caption{{\bf Interactive Visualization.} Our interactive viewing tool allows for the manipulation and visualization of sample point clouds colorized according to their position (a), semantic labels (b), selected geometric features (c), radiometry (d), and to visualize our network's prediction (e) and partitions (f).}

\label{fig:visu}
\end{figure*} 
In this document, we introduce our interactive visualization tool (\secref{sec:visu}), share our source code (\secref{sec:code}), discuss limitations of our approach (\secref{sec:limitations}), provide a description (\secref{sec:hf}) and an analysis (\secref{sec:hfablation}) of all handcrafted features used by our method, detail the construction of the superpoint-graphs (\secref{sec:graphs}) and the partition process (\secref{sec:pcp}), and provide guidelines on how to choose the partition's hyperparameters (\secref{sec:hyper}). Finally, we clarify our architecture parameters (\secref{sec:implem}), explore our model's salability (\secref{sec:scalingablation}) and supervision (\secref{sec:lossablation}), detail the class-wise performance of our approach on each dataset (\secref{sec:classwise}), and the color maps used in the illustrations of the main paper (\figref{fig:colormaps}).

\section{Interactive Visualization}
\label{sec:visu}

We release for this project an interactive plotly visualization tool that produces HTML files compatible with any browser. As shown in \figref{fig:visu}, we can visualize samples from S3DIS, KITTI-360, and DALES with different point attributes and from any angle. These visualizations were instrumental in designing and validating our model and we hope that they will facilitate the reader's understanding as well.

\section{Source Code}
\label{sec:code}


We make our source code publicly available at \GITHUB.
The code provides all necessary instructions for installing and navigating the project, simple commands to reproduce our main results on all datasets, ready-to-use pretrained models, and ready-to-use notebooks. 

Our method is developed in PyTorch and relies on PyTorch Geometric, PyTorch Lightning, and Hydra. 

\section{Limitations}
\label{sec:limitations}
Our model provides significant advantages in terms of speed and compacity but  also comes with its own set of limitations.

\paragraph{Overfitting and Scaling.} The superpoint approach drastically simplifies and compresses the training sets: the $274$m 3D points of S3DIS are captured by a geometry-driven multilevel graph structure with fewer than $1.25$m nodes.
While this simplification favors the compacity and speed of the training of the model, this can lead to overfitting when using \SHORTHAND configurations with more parameters, as shown in \secref{sec:scalingablation}.
Scaling our model to millions of parameters may only yield better results for training sets that are sufficiently large, diverse, and complex.

\paragraph{Errors in the Partition.} Object boundaries lacking obvious discontinuities, such as curbs vs. roads or whiteboards vs. walls, are not well recovered by our partition. As partition errors cannot be corrected with our approach, this may lead to classification errors.
To improve this, we could replace our handcrafted point descriptors (\secref{sec:hf}) with features directly learned for partitioning \cite{landrieu2019point,hui2021superpoint}.
However, such methods significantly increase the preprocessing time, contradicting our current focus on efficiency.
In line with \cite{hsu2020incorporating,ran2022surface}, we use easy-to-compute yet expressive handcrafted features. 
Our model \SHORTHANDNANO without point encoder relies purely on such features and reaches $70.8$ mIoU on S3DIS 6-Fold with only $27$k param, illustrating this expressivity.

\paragraph{Learning Through the Partition.} The idea of learning point and adjacency features directly end-to-end is a promising research direction to improve our model. However, this implies efficiently backpropagating through superpoint hard assignments, which remains an open problem. Furthermore, such a method would consider individual 3D points during training, which would necessitate to perform the partitioning step multiple times during training time, which may negate the efficiency of our method

\paragraph{Predictions.}
Finally, our method predicts labels at the superpoint level $P_{1}$ and not individual 3D points. 
Since this may limit the maximum performance achievable by our approach, we could consider adding an upsampling layer to make point-level predictions. However, this does not appear to us as the most profitable research direction. Indeed, this may negate some of the efficiency of our method. Furthermore, as shown in the ablation study \textcolor{red}{4.3 d)} of the main paper, the ``oracle'' model outperforms ours by a large margin. This may indicate that performance improvements should primarily be searched in superpoint classification rather than in improving the partition.

Our model also learns features for superpoints and not individual 3D points. This may limit downstream tasks requiring 3D point features, such as surface reconstruction or panoptic segmentation.
However, we argue that specific adaptations could be explored to perform these tasks at the superpoint level.

\section{Handcrafted Features}
\label{sec:hf}

\begin{figure*}[ht!]
\centering
\begin{tabular}{@{}ccc@{}}

\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/geof/point_geof_rgb.jpg}
  \caption{Input}
  \label{fig:geof:rgb}
\end{subfigure}
&
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/geof/point_geof_linearity.jpg}
  \caption{Linearity}
  \label{fig:geof:linearity}
\end{subfigure}
 & 
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/geof/point_geof_planarity.jpg}
  \caption{Planarity}
  \label{fig:geof:planarity}
\end{subfigure}
\\

\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/geof/point_geof_scattering.jpg}
  \caption{Scattering}
  \label{fig:geof:scattering}
\end{subfigure}
&
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/geof/point_geof_verticality.jpg}
  \caption{Verticality}
  \label{fig:geof:verticality}
\end{subfigure}
&
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\linewidth]{images/geof/point_geof_elevation.jpg}
  \caption{Elevation}
  \label{fig:geof:elevation}
\end{subfigure}
\\

\end{tabular}

\caption{{\bf Point Geometric Features.} Given an input cloud (a), the computed PCA-based geometric features (b, c, d, e) and distance to the ground (f) offer a simple characterization of the local geometry around each point.}

\label{fig:geof}
\end{figure*} 
Our method relies on simple handcrafted features to build the hierarchical partition and learn meaningful {points and adjacency relationships.}
In this section, we provide further details on the definition of these features and how to compute them.
It is important to note that these features are only computed once during preprocessing, and thanks to our optimized implementation, this step only takes a few minutes.



\paragraph{Point Features.} We can associate each 3D point with a set of 
{$8$ easy-to-compute handcrafted features, described below.}

\begin{itemize}
    \item \textit{Radiometric features} (3 or 1): RGB colors are available for S3DIS and KITTI-360, and intensity values for DALES. These radiometric features are normalized to $[0, 1]$ at preprocessing time. 
    For KITTI-360, we find that using the HSV color model yields better results.

    \item \textit{Geometric features} (5): We use PCA-based features: \textit{linearity}, \textit{planarity}, \textit{scattering}, \cite{demantke2011dimensionality} and \textit{verticality} \cite{guinard2017weakly}, computed on the set of $50$-nearest neighbors of each point. This neighbor search is only computed once during preprocessing and is also necessary to build the graph $\cG$.
    We also define \textit{elevation} as the distance between a point and the ground below it. Since the ground is neither necessarily flat nor horizontal, we use the RANSAC algorithm~\cite{fischler1981random} on a coarse subsampling of the scene to find a ground plane. We normalize the elevation by dividing it by $4$ for S3DIS and $20$ for DALES and KITTI-360.

\end{itemize}

At preprocessing time, we only use radiometric and geometric features to compute the hierarchical partition. At training time, \SHORTHAND computes point embeddings by mapping all available point features, along with the normalized point position to a vector of size $D_\text{point}$ with a dedicated MLP $\phi^0_\text{enc}$.

We provide an illustration of the geometric point features in \figref{fig:geof}, to help the reader apprehend these simple geometric descriptors. 



\paragraph{Adjacency Features.}~The relationship between adjacent superpoints provides crucial information to leverage their context. For each edge of the superpoint-graph, we compute the $18$ following features: \\
\begin{itemize}
\item \textit{Interface features (7)}: 
All adjacent superpoints share an \emph{interface}, \ie pairs of points from each superpoint that are close and share a line of sight. SuperpointGraph \cite{landrieu2018large} uses the Delaunay triangulation of the entire point cloud to compute such interfaces, while we propose a faster heuristic approach in \secref{sec:graphs} called the \emph{Approximate Superpoint Gap algorithm}. Each pair of points of an interface defines an offset, \ie a vector pointing from one superpoint to its neighbor. We compute the mean offset (dim 3), the mean offset length (dim 1), and the standard deviation of the offset in each canonical direction (dim 3).

\item \textit{Ratio features (4)}: 
As defined in \cite{landrieu2018large}, we characterize each pair of adjacent superpoints with the ratio of their \textit{lengths}, \textit{surfaces}, \textit{volumes}, and \textit{point counts}.

\item \textit{Pose features (7)}: For each superpoint, we define a normal vector as its principal component with the smallest eigenvalue. We then characterize the relative position between two superpoints with the cosine of the angle between the superpoint normal vectors (dim: 1) and between each of the two superpoints' normal and the mean offset direction (dim: 2). Additionally, the offset between the centroids of the superpoints is used to compute the centroid distance (dim: 1) and the unit-normalized centroid offset direction (dim: 3).

\end{itemize}

Note that the mean offset and the ratio features are not symmetric and imply that the edges of the superpoint-graphs are oriented.
As mentioned in Section \textcolor{red}{$3.3$}, a network $\phi_\text{adj}^i$ maps these handcrafted features to a vector of size $D_\text{key}+D_\text{que}+D_\text{val}$, for each level $i\geq1$ of the encoder and the decoder.



\section{Influence of Handcrafted Features}
\label{sec:hfablation}

\begin{table}
\caption{\textbf{Ablation on Handcrafted Features.} Impact of handcrafted features on the mIoU for all tested datasets.}
\label{tab:hfablation}
\centering
\small{
\begin{tabular}{@{}lccc@{}}
    \toprule
    Experiment & S3DIS & KITTI & DALES \\
     & 6-Fold & 360 Val & \\
    \midrule
    Best Model           & 76.0 & 63.5 & 79.6 \\
    \\
    \multicolumn{4}{c}{\textit{a)  Point Features}} \\
    \midrule
    No radiometric feat. & -2.7 & -4.0 & -1.2 \\
    No geometric feat.   & -0.7 & -4.1 & -1.4 \\
    \\
    \multicolumn{4}{c}{\textit{b) Adjacency Features}} \\
    \midrule 
    No interface feat.   & -0.2 & -0.6 & -0.7 \\
    No ratio feat.       & -1.1 & -2.2 & -0.4 \\
    No pose feat.        & -5.5 & -1.2 & -0.8 \\
    \multicolumn{4}{c}{\textit{c) Room Features}} \\
    \midrule
    Room-level samples   & -3.8 & - & - \\
    Normalized Room pos.        & -0.7 & - & - \\   
    \bottomrule
\end{tabular}}
\end{table} 
In \tabref{tab:hfablation}, we quantify the impact of the handcrafted features detailed in \secref{sec:hf} on performance. To this end, we retrain \SHORTHAND without each feature group and evaluate the prediction on S3DIS Area~5. 

\paragraph{a) Point Features.} Our experiments show that removing radiometric features has a strong impact on performance, with a drop 
of $2.7$ to $4.0$ mIoU.
In contrast, removing geometric features 
results in a performance drop 
of $0.7$ on S3DIS, but $4.1$ on KITTI-360.

We observe that both outdoor datasets strongly benefit from local geometric features, which we hypothesize is due to their lower resolution and noise level.
These results indicate that radiometric features play an important role for all datasets and that geometric features may facilitate learning on noisy or subsampled datasets.




\paragraph{b) Adjacency Features.} The analysis of the impact of adjacency features on our model's performance indicates that they play a crucial role in leveraging contextual information from superpoints: removing all adjacency features leads to a significant drop of 
$3.0$ to $6.3$ mIoU points on the datasets, as shown in \textcolor{red}{4.3 b)} of the main paper. 
Among the different types of adjacency features, 
pose features appear particularly useful in characterizing the adjacency relationships between superpoints of S3DIS, while interface features have a smaller impact.
These results suggest that the relative pose 
of objects in the scene may have more influence on the 3D semantic analysis performed by our model than the precise characterization of their interface.
On the other hand, interface and ratio features seem to have more impact on outdoor datasets, while the pose information seems to be less informative in the semantic understanding of the scene.

\paragraph{c)  S3DIS Room Partition.} The S3DIS dataset is divided into individual rooms aligned along the $x$ and $y$ axes.
This setup simplifies the classification of classes such as walls, doors, or windows as they are consistently located at the edge of the room samples. Some methods also add normalized room coordinates to each points.
However, we argue that this partition may not generalize well to other environments, such as open offices, industrial facilities, or mobile mapping acquisitions, which cannot naturally be split into rooms.

To address this limitation, we use the absolute room positions to reconstruct the entire floor of each S3DIS area \cite{thomas2019kpconv,chaton2020torch}. This enables our model to consider large multi-room samples, resulting in a performance increase of $3.8$ points. This highlights the advantage of capturing long-range contextual information.
Additionally, we remark that \SHORTHAND performs better without using room-normalized coordinates, which may lead to overfitting and poor performance on layouts that deviate from the room-based structure of the S3DIS dataset such as large amphitheaters.



\section{Superpoint-Graphs Computation}
\label{sec:graphs}

The Superpoint Graph method by Landrieu and Simonovsky \cite{landrieu2018large} builds a graph from a point cloud using Delaunay triangulation, which can take a long time for large point clouds. In contrast, our approach connects two superpoints in $\cP_{i}$, where $i\geq1$ if their closest points are within a distance gap $\epsilon_i>0$. However, computing pairwise distances for all points is computationally expensive. We propose a heuristic to approximately find the closest pair of points for two superpoints, see Algorithm \ref{alg:pointnn}. We also accelerate the computation of adjacent superpoints by approximating only for superpoints with centroids closer than the sum of their radii plus the gap distance. This approximation helps to reduce the number of computations required for adjacency computation, which leads to faster processing times. All steps involved in the computation of our superpoint-graph are implemented on the GPU to further enhance computational efficiency.


\begin{algorithm}
\caption{Approximate Superpoint Gap}\label{alg:pointnn}
\begin{algorithmic}
\State{\textbf{Input:} superpoints $p_1$ and $p_2$, $\text{num\_steps}$}
    \State{$c_1 \gets  \text{centroid}(p_1)$ }
    \State{$c_2 \gets  \text{centroid}(p_2)$ }

    \For{$s \in \text{num\_steps}$}
        \State{$c_2 \gets \argmin_{p \in p2} \Vert c_1-p\Vert$}
        \State{$c_1 \gets \argmin_{p \in p1} \Vert c_2-p\Vert$}
    \EndFor
    
\Return{$\Vert c_1-c_2\Vert$}

\end{algorithmic}
\end{algorithm}

 \if 1 0

\begin{algorithm}
\caption{Superpoint-Graph Computation}\label{alg:segmentnn}
\begin{algorithmic}
\State{\textbf{Input:} $\cP_{i}$ partition, $\cP_{0}$ points, $\epsilon_i$ gap}

    \# compute the centroid and distance of superpoints
    \State{$\text{diam} \gets$ diameters of all superpoints of $\cP_{i}$ } 
    \State{$\text{center} \gets$ centroids of all superpoints of $\cP_{i}$}

    \State{$\text{max\_diam} \gets \text{max}(\text{diam})$}

    \# 
    \State{$\text{neigh}, dist \gets \text{radius\_nn}(center, max\_diam)$}
    
    \For{$(p_1, p_2), (d) \in zip(neigh, dist)$}
        \If{$\epsilon_i + (diam[p_1] + diam[p_2]) / 2 < d$}
            \State{$neigh, dist \gets$ trim out too-far $p_1, p_2$ pair }
        \EndIf
    \EndFor
    
    \For{$p_1, p_2 \in neigh$}
        \State{$d \gets \text{iterative\_smallest\_distance}(p_1, p_2, \cP_{0})$}
        \If{$\epsilon_i < d$}
            \State{$neigh, dist \gets$ trim out too-far $p_1, p_2$ pair }
        \EndIf
    \EndFor

\Return{neigh}

\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Superpoint-Graph Computation}\label{alg:segmentnn}
\begin{algorithmic}
\State{\textbf{Input:} $\cP_{i}$ partition, $\cP_{0}$ points, $\epsilon_i$ gap}

    \State{$diam \gets$ array of size $i$ } 
    \State{$center \gets$ array of size $i$ }
    \For{$p \in \cP_{i}$}
        \State{$diam[p] \gets  \text{compute\_diameter}(p)$ }
        \State{$center[p] \gets  \text{compute\_centroid}(p)$ }
    \EndFor
    
    \State{$max\_diam \gets \text{max}(diam)$}
    
    \State{$neigh, dist \gets \text{radius\_nn}(center, max\_diam)$}
    
    \For{$(p_1, p_2), (d) \in zip(neigh, dist)$}
        \If{$\epsilon_i + (diam[p_1] + diam[p_2]) / 2 < d$}
            \State{$neigh, dist \gets$ trim out too-far $p_1, p_2$ pair }
        \EndIf
    \EndFor
    
    \For{$p_1, p_2 \in neigh$}
        \State{$d \gets \text{iterative\_smallest\_distance}(p_1, p_2, \cP_{0})$}
        \If{$\epsilon_i < d$}
            \State{$neigh, dist \gets$ trim out too-far $p_1, p_2$ pair }
        \EndIf
    \EndFor
\Return{neigh}

\end{algorithmic}
\end{algorithm}
\fi

Recovering the interface between two adjacent superpoints as evoked in \secref{sec:hf} involves a notion of visibility: we connect points from each superpoint which are \emph{facing} each other. This can be a challenging and ambiguous problem, which SuperPoint Graph \cite{landrieu2016cut} tackles using a Delaunay triangulation of the points. However, this method is impractical for large point clouds. To address this issue, we propose a heuristic approach with the following steps: (i) first, we use the Approximate Superpoint Gap algorithm to compute the approximate nearest points for each superpoint. Then, we restrict the search to only consider points within a certain distance of the nearest points. Finally, we match the points by sorting them along the principal component of the selected points.

\section{Details on Hierarchical Partitions}
\label{sec:pcp}
We present here a more detailed explanation of the hierarchical partition process. We define for each point $c$ of $\cC$ a feature $f_c$ of dimension $D$, and $G\eqdef(\cC,\cE,w)$ is the k-nn adjacency between the points, with $w \in \bR_+^\cE$ a nonnegative proximity value. Our goal is to compute a hierarchical multilevel partition of the point cloud into superpoints homogeneous with respect to $f$ at increasing coarseness. 

\paragraph{Piecewise Constant Approximation on a Graph.}
We first explain how to compute a single-level partition of the point cloud.
We consider the pointwise features $f_c$ as a $D$-dimensional signal $f \in \bR^{D \times \vert \cC \vert}$ defined on the nodes of the weighted graph $G\eqdef(\cC,\cE,w)$. 
We first define an energy $\cJ(e; f, \cG, \lambda)$ measuring the fidelity between a vertex-valued signal $e \in \bR^{D \times \vert \cC \vert}$ and the length of its contours, defined as the weight of the cut between its constant components \cite{landrieu2016cut}:
\begin{align}\label{eq:gmpp}
\cJ(e; f, \cG, \lambda)
\eqdef
\Vert e - {f}\Vert^2
+ \lambda\!\!\!
\sum_{(u,v) \in \cE}\!\!\!
w_{u,v}
\left[
e_u \neq e_v
\right]~,
\end{align}
with $\lambda \in \bR_+$ a regularization strength and $[a \neq b]$ the function equals to $0$ if $a=b$ and $1$ otherwise. Minimizers of $\cJ$ are approximations of $f$ that are piecewise constant with respect to a partition with simple contours in $\cG$.

We can characterize such signal $e \in \bR^{D \times \vert \cC \vert}$ by the coarsest partition $\cP^e$ of $\cP$ and its associated variable $f^e \in \bR^{D\times \vert \cP^e \vert}$ such that $e$ is constant within each segment $p$ of $\cP^e$ with value $f^e_p$. 
The partition $\cP^e$ also induces a graph $\hat{\cG}^e\eqdef(\cP^e,\cE^e,w^e)$ with $\cE^e$ linking the component of $\cP^e$ adjacent in $\cG$ and $w^e$ the weight of the cut between adjacent elements of $P^e$: 
\begin{align}
   &\cE^e \eqdef \{(U,V) \mid U,V \in \cP^e, (U \times V) \cap \cE \neq \varnothing \}\\
  &\text{For}\; (U,V) \in \cE^e,\; w^e_{U,V}\eqdef\sum_{(u,v) \in U \times V \cap \cE} w_{u,v} 
\end{align}

We denote by $\parti{e}$ the function mapping $e$ to these uniquely defined variables:  
\begin{align}
f^e, \cP^e, \hat{\cG}^e \eqdef \parti{e}~.
\end{align}


\paragraph{Point Cloud Hierarchical Partition.}
A set of partitions $\cP\eqdef[\cP_0, \cdots, \cP_i]$ defines a hierarchical partition of $\cC$ with $I$ levels if
$\cP_0=\cC$ and $P_{i+1}$ is a partition of $P_{i}$ for $i \in [0,I-1]$. 
We propose to use the formulations above to define a hierarchical partition of the point cloud $\cC$ characterized by a list $\lambda_1, \cdots, \lambda_{I}$ of nonnegative regularization strengths defining the coarseness of the successive partitions. In particular, We chose $\lambda_1$ such that $\vert \cP_1 \vert / \vert \cP_0 \sim 30$ in our experiments.
 
We first define $\hat{\cG}_0$ as the point-level adjacency graph $\hat{\cG}$ and $f_0$ as $f$. We can now define the levels of a hierarchical partition $\cP_i$ for $i \in [1,I]$:
\begin{align}\label{eq:nested}
    f_{i}, \cP_{i}, \hat{\cG}_{i}
    &\eqdef \partition (
        \argmin_{e \in \bR^{D \times \vert \cP_{i-1} \vert}}
        \cJ
        \left(
            e; f_{i-1}, \hat{\cG}_{i-1}, \lambda_{i-1}
        \right)
    ).
\end{align}
Given that the optimization problems defined in \eqref{eq:nested} for $i>1$ operate on the component graphs $\hat{\cG}_i$, which are smaller than $\hat{\cG}_0$, the first partition is the most demanding in terms of computation.

Note that we used the hat notation $\hat{\cG}_i$, because these graphs are only used for computing the hierarchical partitions $\cP_i$, and should be distinguished from the the superpoint graphs $\cG_i$ on which is based our self-attention mechanism, constructed from $\cP_i$ as explained in \secref{sec:graphs}.


\section{Parameterizing the Partition}
\label{sec:hyper}
We define $\cG$ as the $k=10$-nearest neighbor adjacency graph and set all edge weights $w$ to $1$. The point features $f_p$ whose piecewise constant approximation yields the partition are of three types: geometric, radiometric, and spatial.

Geometric features ensure that the superpoints are geometrically homogeneous and with simple shapes. We use the normalized dimensionality-based method described in \secref{sec:hf}. Radiometric features encourage the border of superpoints to follow the color contrast of the scene and are either RGB or intensity values; they must be normalized to fall in the [0,1] range. Lastly, we can add to each point their spatial coordinates with a normalization factor $\mu$ in $m^{-1}$ to limit the size of the superpoints. We recommend setting $\mu$ as the inverse of the maximum radius expected for a superpoint: the largest sought object (facade, wall, roof) or an application-dependent constraint.

The coarseness of the partitions depends on the regularization strength $\lambda$ as defined in \secref{eq:pcp}.
Finer partitions should generally lead to better results but to an increase in training time and memory requirement. We chose a ratio $\mid \cP_0 \mid / \mid \cP_1\mid \sim 30$ across all datasets as it proved to be a good compromise between efficiency and precision. Depending on the desired trade-off, different ratios can be chosen by trying other values of $\lambda$.

 
\if 1 0
\section{Influence of hyperparameters on partition}
This section must be \textbf{reassuring} to the reader: sure we are introducing new hyperparameters you are not familiar with but:
\begin{itemize}
    \item see how robust the model is to (most of) these
    \item here are fat-and-easy-to-reproduce experiments for choosing them and studying their importance on your dataset
\end{itemize}

The influence of hyperparameters is drawn by varying the regularization lambda for fixed values of a parameter, to obtain roughly ($10^2, 10^3, 10^4, 10^5, 10^6$) superpoints each time. Comparing Oracle mIoU, OA, compute speed 
\begin{itemize}
    \item voxel
    \item koutlier
    \item kfeat (20-50) (number of neighbors considered for pointwise features)
    \item radius knn (maximum radius for the neighbor search) 
    \item point feats (linearity, planarity, scattering, verticality, rgb, xyz\/sigma, ...)
    \item lambda xyz (supmat pour justif serieux handcrafted)
    \item lambda reg (0.05, 0.1, 0.4 viser 1 point par ordre de gdeur)
    \item split\_dampening\_ratio
    \item kadj (if it saturates fast, important message)
    \item cutoff 
    \item iterations
\end{itemize}

Remember that we want:
\begin{itemize}
    \item maximum semantic purity
    \item minimum preprocessing time
    \item minimum training time (ie minimum number of superpoints)
\end{itemize}
\fi 

\if 1 0
\section{Superpoint Dropout}
\label{sec:spdropout}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{images/sampling/idx.jpg}
\caption{{\bf Superpoint Dropout.} We can use the hierarchical partitions to guide augmentations such as masking geometrically-consistent parts of the considered scene. We represent in black the masked superpoints.}
\label{fig:dropout}
\end{figure} 

We show the effect of our superpoint dropout on an S3DIS scene in \figref{fig:dropout}. As confirmed in \textcolor{red}{$4.3$}, removing geometrically-consistent parts of the scene offers a powerful augmentation for learning on relatively small datasets like S3DIS.

\fi

\section{Implementation Details}
\label{sec:implem}

\begin{table}
\caption{\textbf{Model Configuration.} We provide the detailed architecture of the
SPT-X architecture. In this paper, we use $X=64$ and $X=128$.
}
\label{tab:implementation}
\centering
\small{
\begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
     {\it Handcrafted features} \\
    $D_\text{point}^{\text{hf}}$ & $D_\text{point}^{\text{radio}} + D_\text{point}^{\text{geof}}$ \\
    $D_\text{adj}^{\text{hf}}$  & 18 \\
    \midrule
    {\it Embeddings sizes} \\
    $D_\text{point}$         & 128 \\
    $D_\text{adj}$           & 32 \\
    \midrule
    {\it Transformer blocks} \\
    $D_\text{val}$           & \underline{\textbf{X}} \\
    $D_\text{key}$           & 4 \\
    \# blocks encoder        & 3 \\
    \# blocks decoder        & 1 \\
    \# heads                 & 16 \\
    \midrule
    {\it MLPs} \\
    $\phi_\text{adj}^i$      & \footnotesize $[D_\text{adj}^{\text{hf}}, D_\text{adj}, D_\text{adj}, 3 D_\text{adj}]$  \\
    $\phi^0_\text{enc}$      & \footnotesize $[D_\text{point}^{\text{hf}} + D_\text{point}^{\text{pos}}, 32, 64, D_\text{point}]$ \\
    $\phi^1_\text{enc}$      & \footnotesize $[D_\text{point} + D_\text{point}^{\text{pos}}, D_\text{val}, D_\text{val}]$ \\
    $\phi^2_\text{enc}$      & \footnotesize $[D_\text{val} + D_\text{point}^{\text{pos}}, D_\text{val}, D_\text{val}]$ \\
    $\phi^1_\text{dec}$      & \footnotesize $[D_\text{val} + D_\text{val} + D_\text{point}^{\text{pos}}, D_\text{val}, D_\text{val}]$ \\

    \bottomrule
\end{tabular}}
\end{table} 
\if 1 0
\begin{table*}
\caption{\textbf{Model Configuration.} Detailed description of the \SHORTHAND architecture parameters for each dataset.}
\label{tab:implementation}
\centering
\small{
\begin{tabular}{@{}lccc@{}}
    \toprule
     & S3DIS & KITTI-360 & DALES \\
 
    \midrule
    Activation & LeakyReLU & LeakyReLU & LeakyReLU \\
    Normalization & GraphNorm & GraphNorm & GraphNorm \\
    $D_\text{point}$         & 128 & 128 & 128 \\
    $D_\text{superpoint}$    & 64 & 32 & 32 \\
    $D_\text{adj}$           & 64 & 32 & 32 \\
    $D_\text{val}$           & 64 & 128 & 64 \\
    $D_\text{key}$           & 4 & 4 & 4 \\
    $\MLP_\text{point}$      & \footnotesize $[14,32,64,D_\text{point}]$      & \footnotesize $[11,32,64,D_\text{point}]$          & \footnotesize $[9,32,64,D_\text{point}]$ \\
    $\MLP_\text{superpoint}$ & \footnotesize $[20,D_\text{superpoint},D_\text{superpoint}]$          & \footnotesize $[17,D_\text{superpoint},D_\text{superpoint}]$              & \footnotesize $[15,D_\text{superpoint}, D_\text{superpoint}]$ \\
    $\MLP_\text{adj}$        & \footnotesize $[18,D_\text{adj},D_\text{adj}]$          & \footnotesize $[18,D_\text{adj},D_\text{adj}]$              & \footnotesize $[18,D_\text{adj},D_\text{adj}]$ \\
    $\phi^1_\text{enc}$      & \footnotesize $[D_\text{point}+D_\text{superpoint}+3,64,64]$                 & \footnotesize $[D_\text{point}+D_\text{superpoint}+3,128,128]$      & \footnotesize $[D_\text{point}+D_\text{superpoint}+3,64,64]$ \\
    $\phi^2_\text{enc}$      & \footnotesize $[64+D_\text{superpoint}+3,64,64]$                  & \footnotesize $[128+D_\text{superpoint}+3,128,128]$      & \footnotesize $[64+D_\text{superpoint}+3,64,64]$ \\
    $\phi^1_\text{dec}$      & \footnotesize $[64+64+D_\text{superpoint}+3,64,64]$               & \footnotesize $[128+128+D_\text{superpoint}+3,128,128]$  & \footnotesize $[64+64+D_\text{superpoint}+3,64,64]$ \\

    \# blocks encoder        & 3 & 3 & 3 \\
    \# blocks decoder        & 1 & 1 & 1 \\
    \# heads                 & 16 & 16 & 16 \\
    
    \bottomrule
\end{tabular}}
\end{table*} \fi

We provide the exact parameterization of the \SHORTHAND architecture used for our experiments.   
All MLPs in the architecture use LeakyReLU activations and GraphNorm \cite{cai2021graphnorm} normalization. For simplicity, we represent an MLP by the list of its layer widths: $[\text{in\_channels}, \text{hidden\_channels}, \text{out\_channels}]$. 

\paragraph{Point Input Features.} {
We refer here to the dimension of point positions, radiometry, and geometric features as $D_\text{point}^{\text{pos}}=3$, $D_\text{point}^{\text{radio}}$, and $D_\text{point}^{\text{geof}}=4$ respectively. 
As seen in \secref{sec:hf}, S3DIS and KITTI-360 use $D_\text{point}^{\text{radio}}=3$, while DALES uses $D_\text{point}^{\text{radio}}=1$.
}

\paragraph{Model Architecture.}{
The exact architecture SPT-64 used for S3DIS and DALES is detailed in \tabref{tab:implementation}.
The other models evaluated are SPT-16, SPT-32, SPT-128 (used for KITTI-360), and SPT-256, which use the same parameters except for $D_\text{val}$. 

\paragraph{SPT-nano.} For SPT-nano, we use and $D_\text{val}=16$, $D_\text{adj}=16$, and $D_\text{key}=2$. As SPT-nano does not compute point embedding, it does not use $\phi^0$, and we set up $\phi^1_\text{enc}$ as $[D_\text{point}^{\text{hf}} + D_\text{point}^{\text{pos}}, D_\text{val}, D_\text{val}]$.
}

\section{Model Scalability}
\label{sec:scalingablation}

We study the scalability of SPT by comparing models with different parameter counts on each dataset. 
It is important to note that the superpoint approach drastically compresses the training set, which can lead to overfitting, see \secref{sec:limitations}. 
For example, as illustrated in \tabref{tab:scalingablation}, SPT-128 with $D_\text{val}={128}$ (777k param.) performs $1.4$ points below $D_\text{val}=64$ on S3DIS. 

We report a similar behavior for other hyperparameters: in \tabref{tab:qkdimablation}, $D_\text{key}=8$ instead of $4$ incurs a drop of $1.0$, while in \tabref{tab:headsablation}, $N_\text{heads}=32$ instead of $16$ a drop of $0.1$ point. For the larger KITTI-360 dataset ($13$m nodes), $D_\text{val}={128}$ performs $1.9$ points above $D_\text{val}=64$, but $5.4$ points above $D_\text{val}=256$ (2.7m param.).

\begin{table}[H]
\caption{\textbf{Impact of Model Scaling.} Impact of model size for each dataset.}
\label{tab:scalingablation}
\centering
\small{
\begin{tabular}{@{}lcccc@{}}
    \toprule
    Model & Size & S3DIS & KITTI & DALES \\
     & $\times10^6$ & 6-Fold & 360 Val & \\
    \midrule
    SPT-32  & 0.14 & 74.5 & 60.6 & 78.7 \\
    SPT-64  & 0.21 & \bf 76.0 & 61.6 & \bf 79.6 \\
    SPT-128 & 0.77 & 74.6 & \bf 63.5 & 78.8 \\
    SPT-256 & 1.80 & 74.0 & 58.1 & 77.6 \\
    \bottomrule
\end{tabular}}
\end{table} \begin{table}[H]
\caption{\textbf{Impact of Query-Key Dimension.} Impact of $D_\text{key}$ on S3DIS 6-Fold.}
\label{tab:qkdimablation}
\centering
\small{
\begin{tabular}{@{}ccccc@{}}
    \toprule
    $D_\text{key}$  & 2 & 4 & 8 & 16 \\
    \midrule
    SPT-64 & 75.6 & \bf 76.0 & 75.0 & 74.7 \\
    \bottomrule
\end{tabular}}
\end{table} \begin{table}[H]
\caption{\textbf{Impact of Heads Count.} Impact of the number of heads $N_\text{head}$ on the S3DIS 6-Fold performance.}
\label{tab:headsablation}
\centering
\small{
\begin{tabular}{@{}ccccc@{}}
    \toprule
    $N_\text{head}$ & 4 & 8 & 16 & 32 \\
    \midrule
   SPT-64 &  74.3 & 75.2 & \bf 76.0 & 75.9 \\
    \bottomrule
\end{tabular}}
\end{table} 
\section{Hierarchical Supervision}
\label{sec:lossablation}

We explore, in \tabref{tab:lossablation}, alternatives to our hierarchical supervision introduced in Section \textcolor{red}{$3.3$} : predicting the most frequent label for $\cP_1$ and the distribution for $\cP_2$.
We use ``\text{freq}-$\cP_i$'' to refer to the prediction of the most frequent label applied  the $\cP_i$ partition. Similarly, ``\text{dist}-$\cP_i$" denotes the prediction of the distribution of labels within each superpoint of the partition $\cP_i$. 

We observe a consistent improvement across all datasets by adding the \text{dist}-$\cP_i$ supervision. 
This illustrates the benefits of supervising higher-level partitions, despite their lower purity.
Moreover, supervising $\cP_1$ with the distribution rather than the most frequent label leads to a further performance drop.
This validates our choice to consider $\cP_1$ superpoints as sufficiently pure to be supervised using their dominant label.

\begin{table}[H]
\caption{\textbf{Ablation on Supervision.} Impact of our hierarchical supervision for each dataset.}
\label{tab:lossablation}
\centering
\small{
\begin{tabular}{@{}lcccc@{}}
    \toprule
    Loss & S3DIS & KITTI & DALES \\
     & 6-Fold & 360 Val & \\
    \midrule
    \text{freq}-$\cP_i$-$\cP_1$ \text{dist}-$\cP_i$-$\cP_2$ & \bf 76.0 & \bf 63.5 & \bf 79.6 \\
    \midrule
    \text{freq}-$\cP_1$    & -0.2 & -0.8 & -0.8 \\
    \text{dist}-$\cP_i$-$\cP_1$    & -0.8 & -1.3 & -0.8 \\
    \bottomrule
\end{tabular}}
\end{table} 
\section{Detailed Results}
\label{sec:classwise}

\begin{table*}[t]
\caption{{\bf Class-wise Performance.} Class-wise mIoU across all datasets for our \METHOD. }
\label{tab:classwise} 
\begin{center}
\footnotesize{

    \begin{tabular}{lc*{14}{c}}
        
        \multicolumn{15}{c}{S3DIS Area~5}\\
        Method & mIoU & ceiling & floor & wall & beam & column & window & door & chair & table & bookcase & sofa & board & clutter\\
        \midrule
        PointNet \cite{qi2017pointnet} & 41.1 & 88.8 & 97.3 & 69.8 & \bf 0.1 & 3.9 & 46.3 & 10.8 & 52.6 & 58.9 & 40.3 & 5.9 & 26.4 & 33.2 \\
        SPG \cite{landrieu2018large} & 58.4 & 89.4 & 96.9 & 78.1 & 0.0 & 42.8 & 48.9 & 61.6 & 84.7 & 75.4 & 69.8 & 52.6 & 2.1 & 52.2 \\
        MinkowskiNet \cite{choy20194d} & 65.4 & 91.8 & \bf 98.7 & 86.2 & 0.0 & 34.1 & 48.9 & 62.4 & 81.6 & \bf 89.8 & 47.2 & 74.9 & 74.4 & 58.6 \\
        {SPG + SSP \cite{landrieu2019point}} & 61.7 & 91.9 & 96.7 & 80.8 & 0.0 & 28.8 & 60.3 & 57.2 & 85.5 & 76.4 & 70.5 & 49.1 & 51.6 & 53.3 \\
        KPConv \cite{thomas2019kpconv} & 67.1 & 92.8 & 97.3 & 82.4 & 0.0 & 23.9 & 58.0 & 69.0 & 91.0 & 81.5 & 75.3 & 75.4 & 66.7 & 58.9 \\
        PointTrans.\cite{zhao2021point} & 70.4 & 94.0 & 98.5 & \bf 86.3 & 0.0 & 38.0 & 63.4 & 74.3 & 89.1 & 82.4 & 74.3 & 80.2 & 76.0 & 59.3 \\
        DeepViewAgg \cite{robert2022learning} & 67.2 & 87.2 & 97.3 & 84.3 & 0.0 & 23.4 & \bf 67.6 & 72.6 & 87.8 & 81.0 & 76.4 & 54.9 & \bf 82.4 & 58.7 \\
        Stratified PT \cite{lai2022stratified} & \bf 72.0 & \bf 96.2 & \bf 98.7 & 85.6 & 0.0 & \bf 46.1 & 60.0 & \bf 76.8 & \bf 92.6 & 84.5 & \bf 77.8 & 75.2 & 78.1 & \bf 64.0\\
        \midrule
        \SHORTHAND &  68.9 & 92.6 & 97.7 & 83.5 & \bf 0.2 & 42.0 & 60.6 & 67.1 & 88.8 & 81.0 & 73.2 & \bf 86.0 & 63.1 & 60.0 \\
        \SHORTHANDNANO & 64.9 & 92.4 & 97.1 & 81.6 &  0.0 & 38.2 & 56.4 & 58.6 & 86.3 & 77.3 & 69.6 & 82.5 & 50.5 & 53.4 \\
        \midrule~\\

        \multicolumn{15}{c}{S3DIS 6-FOLD}\\
        \midrule~\\
        PointNet \cite{qi2017pointnet} & 47.6 & 88.0 & 88.7 & 69.3 & 42.4 & 23.1 & 47.5 & 51.6 & 42.0 & 54.1 & 38.2 & 9.6 & 29.4 & 35.2 \\
        SPG \cite{landrieu2018large} & 62.1 & 89.9 & 95.1 & 76.4 & 62.8 & 47.1 & 55.3 & 68.4 & 73.5 & 69.2 & 63.2 & 45.9 & 8.7 & 52.9 \\
        ConvPoint \cite{boulch2020convpoint} & 68.2 & \bf 95.0 & \bf 97.3 & 81.7 & 47.1 & 34.6 & 63.2 & 73.2 & 75.3 & 71.8 & 64.9 & 59.2 & 57.6 & 65.0 \\
        MinkowskiNet \cite{choy20194d,robert2022learning} & 69.5 & 91.2 & 90.6 & 83.0 & 59.8 & 52.3 & 63.2 & 75.7 & 63.2 & 64.0 & 69.0 & 72.1 & 60.1 & 59.2 \\
        {SPG + SSP \cite{landrieu2019point}} & 68.4 & 91.7 & 95.5 & 80.8 & 62.2 & 54.9 & 58.8 & 68.4 & 78.4 & 69.2 & 64.3 & 52.0 & 54.2 & 59.2 \\
        KPConv \cite{thomas2019kpconv} & 70.6 & 93.6 & 92.4 & 83.1 & 63.9 & 54.3 & 66.1 & 76.6 & 57.8 & 64.0 & 69.3 & 74.9 & 61.3 & 60.3 \\
        DeepViewAgg \cite{robert2022learning} & 74.7 & 90.0 & 96.1 & \bf 85.1 & 66.9 & 56.3 & \bf 71.9 & \bf 78.9 & 79.7 & 73.9 & \bf 69.4 & 61.1 & \bf 75.0 & \bf 65.9 \\
        \midrule
        \SHORTHAND & \bf 76.0 & 93.9 & 96.3 & 84.3 & \bf 71.4 & \bf 61.3 & 70.1 & 78.2 & \bf 84.6 & \bf 74.1 & 67.8 & \bf 77.1 & 63.6 & 65.0 \\
        \SHORTHANDNANO & 70.8 & 93.1 & 96.0 & 80.9 & 68.4 & 54.0 & 62.2 & 71.3 & 76.3 & 70.8 & 63.3 & 74.3 & 51.9 & 57.6 \\
        \midrule
    \end{tabular}\\~\\~\\

    \begin{tabular}{lc*{16}{c}}
        \multicolumn{17}{c}{KITTI-360 Val}\\
        Method & mIoU & \rotatebox{90}{road} & \rotatebox{90}{sidewalk} & \rotatebox{90}{building} & \rotatebox{90}{wall} & \rotatebox{90}{fence} & \rotatebox{90}{pole} & \rotatebox{90}{traffic lig.} & \rotatebox{90}{traffic sig.} & \rotatebox{90}{vegetation} & \rotatebox{90}{terrain} & \rotatebox{90}{person} & \rotatebox{90}{car} & \rotatebox{90}{truck} & \rotatebox{90}{motorcycle} & \rotatebox{90}{bicycle}\\
        \midrule
        MinkowskiNet \cite{choy20194d,robert2022learning} & 54.2 & 90.6 & 74.4 & 84.5 & 45.3 & 42.9 & 52.7 & 0.5 & 38.6 & 87.6 & 70.3 & 26.9 & 87.3 & 66.0 & 28.2 & 17.2 \\
        DeepViewAgg \cite{robert2022learning} & 57.8 & \bf 93.5 & 77.5 & 89.3 & 53.5 & \bf 47.1 & \bf 55.6 & 18.0 & 44.5 & \bf 91.8 & 71.8 & 40.2 & 87.8 & 30.8 & 39.6 & \bf 26.1 \\
        \midrule
        \SHORTHAND  & \bf 63.5 & 93.3 & \bf 79.3 & \bf 90.8 & \bf 56.2 & 45.7 & 52.8 & \bf 20.4 & \bf 51.4 & 89.8 & \bf 73.6 & \bf 61.6 & \bf 95.1 & \bf 79.0 & \bf 53.1 & 10.9 \\
        \SHORTHANDNANO & 57.2 & 91.7 & 74.7 & 87.8 & 49.3 & 38.8 & 49.0 & 12.2 & 39.2 & 88.0 & 69.5 & 39.9 & 94.2 & 80.1 & 33.7 & 10.4 \\
        \midrule
    \end{tabular}\\~\\~\\
    
    \begin{tabular}{lc*{10}{c}}
        \multicolumn{11}{c}{DALES}\\
        Method & mIoU & ground & vegetation & car & truck & power line & fence & pole & building \\
        \midrule
        PointNet++ \cite{qi2017pointnetpp}   & 68.3 & 94.1 & 91.2 & 75.4 & 30.3 & 79.9 & 46.2 & 40.0 & 89.1 \\
        ConvPoint \cite{boulch2020convpoint} & 67.4 & 96.9 & 91.9 & 75.5 & 21.7 & 86.7 & 29.6 & 40.3 & 96.3 \\
        SPG \cite{landrieu2018large}         & 60.6 & 94.7 & 87.9 & 62.9 & 18.7 & 65.2 & 33.6 & 28.5 & 93.4 \\
        PointCNN \cite{li2018pointcnn}       & 58.4 & \bf 97.5 & 91.7 & 40.6 & 40.8 & 26.7 & 52.6 & 57.6 & 95.7 \\
        KPConv \cite{thomas2019kpconv}       & \bf 81.1 & 97.1 & \bf 94.1 & 85.3 & 41.9 & \bf 95.5 & \bf 63.5 & \bf 75.0 & 96.6 \\
        \midrule
        \SHORTHAND & 79.6 & 96.7 & 93.1 & \bf 86.1 & \bf 52.4 & 94.0 & 52.7 & 65.3 & \bf 96.7 \\
        \SHORTHANDNANO & 75.2 & 96.5 & 92.6 & 78.1 & 35.8 & 92.1 & 50.8 & 59.9 & 96.0 \\
        \midrule
    \end{tabular}\\
    
}\end{center}
\end{table*} 
We report in \tabref{tab:classwise} the class-wise performance across all datasets for \SHORTHAND and other methods for which this information was available. As previously stated, \SHORTHAND performs close to state-of-the-art methods on all datasets, while being significantly smaller and faster to train. 
By design, superpoint-based methods can capture long-range interactions and their predictions are more spatially regular than point-based approaches. This may explain the performance of \SHORTHAND on S3DIS, which encompasses large, geometrically homogeneous objects or whose identification requires long-range context understanding, such as ceiling, floor, columns, and windows.
For all datasets, results show that some progress could be made in analyzing smaller objects with intricate geometries. This suggests that a more powerful point-level encoding may be beneficial.

\balance

\begin{figure*}[!b]
    \centering
    \begin{tabular}{c}
        \toprule
        \large{S3DIS}
        \\\midrule
        \begin{tabular}{@{}rlrlrlrlrl@{}}
            \definecolor{tempcolor}{rgb}{0.91,0.90,0.41}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{ceiling} 
            &
            \definecolor{tempcolor}{rgb}{.37,0.61,0.77}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{floor}
            &
            \definecolor{tempcolor}{rgb}{0.70,0.45,0.31}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{wall}
            &
            \definecolor{tempcolor}{rgb}{0.95,.58,0.51}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{beam} &
            \definecolor{tempcolor}{rgb}{0.31,0.63,.58}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{column} 
            \\
            \definecolor{tempcolor}{rgb}{0.30,0.68,.32}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{window}
            &
            \definecolor{tempcolor}{rgb}{.42,0.52,0.29}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{door}
            &
            \definecolor{tempcolor}{rgb}{.16,0.19,0.39}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{chair}
            &
            \definecolor{tempcolor}{rgb}{.30,0.30,0.30}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{table} 
            &
            \definecolor{tempcolor}{rgb}{.88,0.20,0.20}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{bookcase}
            \\
            \definecolor{tempcolor}{rgb}{0.35,.18,0.37}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{sofa}
            &
            \definecolor{tempcolor}{rgb}{.32,0.43,.45}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{board}
            &
            \definecolor{tempcolor}{rgb}{0.91,0.91,.90}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{clutter} 
            &
            \definecolor{tempcolor}{rgb}{0.,0.,0}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{unlabeled} 
        \end{tabular}
        \\
         \toprule
        \large{KITTI-360}
        \\\midrule
        \begin{tabular}{rlrlrlrlrl}
            \definecolor{tempcolor}{rgb}{0.50, 0.25, 0.50}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{road} 
            &
            \definecolor{tempcolor}{rgb}{0.95, 0.13, 0.90}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{sidewalk} 
            &
            \definecolor{tempcolor}{rgb}{0.27, 0.27, 0.27}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{building} 
            &
            \definecolor{tempcolor}{rgb}{0.4 , 0.4 , 0.61}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{wall} 
            &
            \definecolor{tempcolor}{rgb}{0.74, 0.6 , 0.6 }
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{fence} 
            \\
            \definecolor{tempcolor}{rgb}{0.6 , 0.6 , 0.6 }
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{pole} 
            &
            \definecolor{tempcolor}{rgb}{0.98, 0.66, 0.11}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{traffic light} 
            &
            \definecolor{tempcolor}{rgb}{0.86, 0.86, 0.  }
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{traffic sign} 
            &
            \definecolor{tempcolor}{rgb}{0.41, 0.55, 0.13}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{vegetation} 
            &
            \definecolor{tempcolor}{rgb}{0.59, 0.98, 0.59}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{terrain} 
            \\
            \definecolor{tempcolor}{rgb}{0.86, 0.07, 0.23}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{person} 
            &
            \definecolor{tempcolor}{rgb}{0.  , 0.  , 0.55}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{car} 
            &
            \definecolor{tempcolor}{rgb}{0.  , 0.  , 0.27}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{truck} 
            &
            \definecolor{tempcolor}{rgb}{0.  , 0.  , 0.90}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{motorcycle} 
            &
            \definecolor{tempcolor}{rgb}{0.46, 0.04, 0.12}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{bicycle} 
            \\
            \definecolor{tempcolor}{rgb}{0.  , 0.  , 0.  }
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{ignored} 
            &
        \end{tabular}\\
         \toprule
            \large{DALES}
        \\\midrule
        \begin{tabular}{rlrlrlrlrl}
            \definecolor{tempcolor}{rgb}{0.95, 0.84, 0.67}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{ground} 
            &
            \definecolor{tempcolor}{rgb}{0.27, 0.45, 0.26}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{vegetation}
            &
            \definecolor{tempcolor}{rgb}{0.91, 0.20, 0.94}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{car}
            &
            \definecolor{tempcolor}{rgb}{0.95, 0.93, 0.}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{truck} &
            \definecolor{tempcolor}{rgb}{0.75, 0.6,0.6  }
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{power line} 
            \\
            \definecolor{tempcolor}{rgb}{0., 0.91,  0.04}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{fence}
            &
            \definecolor{tempcolor}{rgb}{0.94,  0.45, 0.}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2); 
            & \small{pole}
            &
            \definecolor{tempcolor}{rgb}{0.84, 0.26, 0.21}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{building}
            &
            \definecolor{tempcolor}{rgb}{0., 0.03, 0.45}
            \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0, 0) rectangle (1.2, 1.2);
            & \small{unknown}
            
        \end{tabular}\\
        \midrule~\\
    \end{tabular}
    \caption{{\bf Colormaps.} }
    \label{fig:colormaps}
\end{figure*} 
