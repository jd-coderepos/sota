\subsection{Main Results}\label{results:main}

\begin{table}[]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{ccc}
    \toprule
        Transition system & Avg. \#actions & Oracle \textsc{Smatch}  \\
    \midrule
         \citet{naseem2019rewarding} & 73.6 & 93.3 \\
         \citet{astudillo2020transition} & 76.2 & 98.0 \\
         Ours & 41.6 & 98.9 \\
    \bottomrule
    \end{tabular}
    } \caption{Average number of actions and oracle \textsc{Smatch} on AMR 2.0 training data. The average source length is 18.9.
     from author correspondence.}
\label{tab:oracle_smatch}
\end{table}


\paragraph{Oracle Actions}
Table~\ref{tab:oracle_smatch} compares the oracle data \textsc{Smatch} and average action sequence length on the AMR 2.0 training set among recent transition systems.
Our approach yields much shorter action sequences due to the target-side pointing mechanism. It has also the best coverage on training AMR graphs, due to the flexibility of our transitions that can capture the majority of graph components. We chose not to tackle a number of small corner cases, such as disconnected subgraphs for a token, that account for the missing oracle performance.

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{cll}
        \toprule
        Corpus & Model & \textsc{Smatch} (\%) \\
        \midrule
        \multirow{14}{*}{\begin{tabular}{@{}c@{}} AMR \\ 1.0 \end{tabular}} &
         \citet{pust2015parsing} & 67.1 \\
        & \citet{flanigan2016cmu} & 66.0 \\
& \citet{wang2017getting} & 68.1 \\
        & \citet{guo2018better} & 68.3 \small{0.4} \\
        & \citet{zhang2019amr} & 70.2 \small{0.1} \\
        & \citet{zhang2019broad} & 71.3 \small{0.1} \\
        & \citet{cai2020amr} & 74.0 \\
        & \citet{cai2020amr} & 75.4 \\
        & \citet{astudillo2020transition} & \textbf{76.9 \small{0.1}} \\
        \cmidrule(lr){2-3}
        & \citet{lee2020pushing} (85K silver) & \textbf{78.2 \small{0.1}} \\
        \cmidrule(lr){2-3}
        & APT small & 78.2 / 78.2 \small{0.0} \\
        & APT base & \textbf{78.5 / 78.3 \small{0.1}} \\
        \cmidrule(lr){2-3}
        & APT small p.e. & 79.7 \\
        & APT base p.e. & \textbf{79.8} \\
        \midrule
        \multirow{21}{*}{\begin{tabular}{@{}c@{}} AMR \\ 2.0 \end{tabular}} & 
         \citet{van2017neural} & 71.0 \\
        & \citet{groschwitz2018amr} & 71.0 \\
        & \citet{lyu2018amr} & 74.4 \small{0.2} \\
        & \citet{cai2019core} & 73.2 \\
        & \citet{lindemann2019compositional} & 75.3 \small{0.1} \\
        & \citet{naseem2019rewarding} & 75.5 \\
        & \citet{zhang2019amr} & 76.3 \small{0.1} \\
        & \citet{zhang2019broad} & 77.0 \small{0.1} \\
        & \citet{cai2020amr} & 78.7 \\
        & \citet{cai2020amr} & 80.2 \\
        & \citet{astudillo2020transition} & 80.2 \small{0.0}\\
        & \citet{bevilacqua2021one} & \textbf{83.8} \\
        \cmidrule(lr){2-3}
        & \citet{xu2020improving} (4M silver) & 80.2 \\
        & \citet{lee2020pushing} (85K silver) & 81.3 \small{0.0} \\
        & \citet{bevilacqua2021one} (200K silver) & \textbf{84.3} \\
        \cmidrule(lr){2-3}
        & APT small & 81.7 / 81.5 \small{0.2} \\
        & APT base  & \textbf{81.8 / 81.7 \small{0.1}} \\
        \cmidrule(lr){2-3}
        & APT small p.e.  & 82.5 \\
        & APT base p.e.  & 82.8 \\
& APT base (70K Silver)  &  82.8 / 82.6 \small{}\\
        & APT base (70K Silver) p.e.  & \textbf{83.4} \\
        \midrule
        \multirow{4}{*}{\begin{tabular}{@{}c@{}} AMR \\ 3.0 \end{tabular}} & 
         \citet{lyu2020differentiable} & 75.8 \\
         & \citet{bevilacqua2021one} & \textbf{83.0} \\
        \cmidrule(lr){2-3}
        & APT base & 80.4 / 80.3 \small{0.1} \\
        & APT base p.e. & \textbf{81.2} \\
        
        \bottomrule
    \end{tabular}
    } \caption{\textsc{Smatch} scores on AMR 1.0, 2.0, and 3.0 test sets.
APT is our model.
     or  indicates pre-trained BERT or RoBERTa embeddings,  use of graph re-categorization,
     improved results reported in \citet{lee2020pushing}.
     denotes concurrent work based on fine-tuning pre-trained BART large models.
    We report the best/average score  standard deviation over 3 seeds.
    p.e. is partial ensemble decoding with 3 seed models.}
    \label{tab:main_amr2.0+1.0}
\end{table}








\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{cccc}
    \toprule
        \multirow{2}{*}{Model} &
        \multirow{2}{*}{\begin{tabular}{@{}c@{}} Fixed Extra \\ Features \end{tabular}} &
        \multirow{2}{*}{\begin{tabular}{@{}c@{}} Trained \\ Param. \end{tabular}} &
        \multirow{2}{*}{\begin{tabular}{@{}c@{}} \textsc{Smatch} \\ AMR 2.0 \end{tabular}} \\
        \\
    \midrule
        \citet{zhang2019amr} & BERT & 66.1M  & 76.3 \\ \citet{cai2020amr} & BERT & 27.1M & 78.7 \\  \citet{cai2020amr} & BERT & 26.1M & 80.2 \\  \citet{astudillo2020transition} & RoBERTa & 21.7M & 80.2 \\ \citet{xu2020improving} (4M silver) & - & 239.1M & 80.2 \\ \citet{bevilacqua2021one} & - & 411.8M & 83.8 \\  \midrule
        APT small & RoBERTa & 17.5M  & 81.7 \\ APT base & RoBERTa &  21.4M &  81.8 \\ \midrule
        APT small p.e. & RoBERTa & 52.5M & 82.5 \\
        APT base p.e. & RoBERTa & 64.3M  & 82.8 \\

    \bottomrule
    \end{tabular}
    } \caption{Comparison of model parametrization sizes and \textsc{Smatch} scores on AMR 2.0 test set.
    Model sizes of previous works are obtained from their officially released pre-trained models.
     is an estimate by removing BERT parameters in the released model, where a BERT base model is trained together which is different from the paper description.
     denotes concurrent work based on fine-tuning pre-trained BART large models.
    }
    \label{tab:main_param}
\end{table}







\paragraph{Parsing Performance}
We compare our action-pointer transition/Transformer (APT) model with existing approaches in Table~\ref{tab:main_amr2.0+1.0}\footnotemark\footnotetext{We exclude \citet{xu2020improving} AMR 1.0 numbers since they report 16833 train sentences, not 10312.}.
We indicate the use of pre-trained BERT or RoBERTa embeddings (from large models) with  or , and graph re-categorization with . Graph re-categorization \cite{lyu2018amr,zhang2019amr, cai2020amr,bevilacqua2021one} removes node senses and groups certain nodes together such as named entities in pre-processing. It reverts these back in post-processing with the help of a name entity recognizer.
We report results over 3 runs for each model with different random seeds. Given that we use fixed pre-trained embeddings, it becomes computationally cheap to build a partial ensemble that uses the average probability of 3 models from different seeds which we denote as p.e.

With the exception of the recent BART-based model \citet{bevilacqua2021one},
we outperform all previously published approaches, both with our small and base models. Our best single-model parsing scores are 81.8 on AMR 2.0 and 78.5 on AMR 1.0, which improves  points over the previous best model trained only with gold data.
Our small model only trails the base model by a small margin and 
we achieve high performance on small AMR 1.0 dataset,
indicating that our approach benefits from having good inductive bias towards the problem so that the learning is efficient.
More remarkably, we even surpass the scores reported in \citet{lee2020pushing} combining various self-learning techniques and utilizing 85K extra sentences for self-annotation (silver data). 
For the most recent AMR 3.0 dataset, we report our results for future reference.

Additionally, the partial ensemble decoding proves to be simple and effective in boosting the model performance, which consistently brings more than  point gain for AMR 1.0 and 2.0. It should be noted that the ensemble decoding is only \% slower than a single model. 

We thus use this ensemble to annotate the K sentence set used in \cite{lee2020pushing}. After removing parses with detached nodes we obtained 70K model-annotated silver data sentences. Adding these for training regularly, we achieve our best score of 83.4 with ensemble on AMR 2.0.

\paragraph{Model Size}
In Table~\ref{tab:main_param}, we compare parameter sizes of recently published models alongside their parsing performances on AMR 2.0.
Similar to our approach, most models use large pre-trained models to extract contextualized embeddings as fixed features, with the exception of \citet{xu2020improving}, which is a seq-to-seq pre-training approach on large amount of data, and \citet{bevilacqua2021one}, which directly fine-tunes a seq-to-seq BART large \citep{lewis2019bart} model.\footnote{Here we focus on trainable parameters for learning efficiency. For deployment the total number of parameters should be considered, where all the models relying on BERT/RoBERTa features would be on the similar level.}
Except the large BART model, our APT small (3 layers) has the least number of trained parameters yet already surpasses all the previous models. This justifies our method is highly efficient in learning for AMR parsing.
Moreover, with the small parameter size, the partial ensemble is an appealing way to improve parsing quality with minor decoding overhead. Although more performant, direct fine-tuning of pre-trained seq-to-seq models such as BART would require prohibitively large numbers to perform an ensemble.

\paragraph{Fine-grained Results}
Table~\ref{tab:finegrained_amr2.0} shows the fine-grained AMR 2.0 evaluation  \citep{damonte2016incremental} of APT and previous models with comparable trainable parameter sizes. Our model achieves the best scores among all sub-tasks except negations and wikification, handled by post-processing on the best performing approach.
We obtain large improvement on edge related sub-tasks including SRL ( arcs) and Reentrancies, proving the effectiveness of our target-side pointer mechanism.





\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{cccccccccc}
        \toprule
        Model & \textsc{Smatch} & Unlabeled & No WSD & Concepts & Named Ent. & Negations & Wikification & Reentrancies & SRL \\
        \midrule
        \citet{van2017neural}     & 71.0 & 74   & 72   & 82   & 79   & 62   & 65   & 52   & 66 \\
        \citet{groschwitz2018amr} & 71.0 & 74   & 72   & 84   & 78   & 57   & 71   & 49   & 64 \\
        \citet{lyu2018amr}        & 74.4 & 77.1 & 75.5 & 85.9 & 86.0 & 58.4 & 75.7 & 52.3 & 69.8 \\
        \citet{cai2019core} & 73.2 & 77.0 & 74.2 & 84.4 & 82.0 & 62.9 & 73.2 & 55.3 & 66.7 \\
        \citet{naseem2019rewarding} & 75.5 & 80 & 76 & 86 & 83 & 67 & 80 & 56 & 72 \\
        \citet{zhang2019amr} & 76.3 & 79.0 & 76.8 & 84.8 & 77.9 & 75.2 & 85.8 & 60.0 & 69.7 \\
        \citet{zhang2019broad} & 77.0 & 80 & 78 & 86 & 79 & 77 & 86 & 61 & 71 \\
        \citet{cai2020amr} & 80.2 & 82.8 & 80.8 & 88.1 & 81.1 & \textbf{78.9} & \textbf{86.3} & 64.6 & 74.2 \\
        \citet{astudillo2020transition} & 80.2 & 84.2 & 80.7 & 88.1 & 87.5 & 64.5 & 78.8 & 70.3 & 78.2 \\
        \midrule
        APT small & 81.7 & 85.4 & 82.2 & \textbf{88.9} & \textbf{88.9} & 67.5 & 78.7 & 70.6 & 80.7 \\
        APT base & \textbf{81.8} & \textbf{85.5} & \textbf{82.3} & 88.7 & 88.5 & 69.7 & 78.8 & \textbf{71.1} & \textbf{80.8} \\
        \bottomrule
    \end{tabular}
    } \caption{Fine-grained F1 scores on the AMR 2.0 test set.
/ and  marks uses of pre-trained BERT/RoBERTa embeddings and graph re-categorization processing.
     We cite improved results reported in \citet{lee2020pushing}.
    We report results with our single best model for fair comparison.}
    \label{tab:finegrained_amr2.0}
\end{table*}

\subsection{Analysis}

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{cccc}
    \toprule
        \multicolumn{2}{c}{Model Configuration} & \multicolumn{2}{c}{
        \textsc{Smatch} (\%)} \\
    \midrule
         \begin{tabular}{@{}c@{}} Mono. \\ Alignment \end{tabular} &  \begin{tabular}{@{}c@{}} Graph \\ embedding \end{tabular} & \begin{tabular}{@{}c@{}} AMR \\ 1.0 \end{tabular} & \begin{tabular}{@{}c@{}} AMR \\ 2.0 \end{tabular}  \\
    \midrule
         & & 72.2 \small{0.4} & 77.5 \small{0.2} \\
         \ding{51} & &  78.0 \small{0.1} & 81.5 \small{0.1} \\
\ding{51} & \ding{51}  & 78.3 \small{0.1} & 81.7 \small{0.1} \\
    \midrule
        \multicolumn{2}{c}{No subspace restriction} & 78.0 \small{0.1} & 80.9 \small{0.1} \\
        \multicolumn{2}{c}{RoBERTa base embeddings} & 78.0 \small{0.1} & 81.3 \small{0.1} \\
        \multicolumn{2}{c}{BERT large embeddings} & 77.7 \small{0.1} & 81.4 \small{0.1} \\
    \bottomrule
    \end{tabular}
    } \caption{Ablation study of model components. The analysis is with our base model size.}
    \label{tab:model_components_2}
\end{table}

\paragraph{Ablation of Model Components}
We evaluate the contribution of different components in our model in Table~\ref{tab:model_components_2}.
The top part of the table shows effects of 2 major components that utilize parser state information and the graph structural information in the Transformer decoder. The baseline model is a free Transformer model with pointers (row 1), which is greatly increased by including the  monotonic action-source alignment via hard attention (row 2) on both AMR 1.0 and AMR 2.0 corpus, and combining it with the graph embedding (row 3) gives further improvements of 0.3 and 0.2 for AMR 1.0 and AMR 2.0.
This highlights that injecting hard encoded structural information in the Transformer decoder greatly helps our problem.


The bottom part of Table~\ref{tab:model_components_2} evaluates the contribution of output space restriction for target and input pre-trained embeddings for source, respectively. Removing the restriction for target output space i.e. the valid actions, hurts the model performance, as the model may not be able to learn the underlying rules that govern the target sequence restrictions. Switching the RoBERTa large embeddings to RoBERTa base or BERT large also hurts the performance (although score drops are only ), indicating that the contextual embeddings from large and better pre-trained models better equip the parser to capture semantic relations in the source sentence.

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{ccc}
    \toprule
        \multirow{2}{*}{\begin{tabular}{@{}c@{}} Data oracle \\ variation \end{tabular}} &
        \multicolumn{2}{c}{\textsc{Smatch} (\%)} \\
        & Train oracle & Model test \\
    \midrule
        None & 98.9 & 81.7 \small{0.1} \\
        \midrule
        No subgraph breakdown & 97.8 &  80.6 \small{0.1} \\
        Create farther edges first & 98.9 &  81.4 \small{0.2} \\
        Post-order subgraph traversal   & 98.9 &  81.8 \small{0.1} \\
    \bottomrule
    \end{tabular}
    } \caption{Results of model performance with different data oracles on AMR 2.0 corpus.}
    \label{tab:transition_oblation}
\end{table}


\paragraph{Effect of Oracle Setup}

As our model directly learns from the oracle actions, we study how the upstream transition system affects the model performance by varying transition setups in Table~\ref{tab:transition_oblation}.
We try three variations of the oracle. In the first setup, we measure the impact of breaking down \textsc{subgraph} action into individual node generation and attachment actions. We do this by using the \textsc{subgraph} for all cases of multi-node alignments. This degrades the parser performance and oracle \textsc{Smatch} considerably, dropping by absolute 1.1 points. This is expected, since \textsc{subgraph} action makes internal nodes of the subgraph unattachable. In the second setup, we vary the order of edge creation actions. We reverse it so that the edges connecting farther nodes are built first. Although this does not affect the oracle score, we observe that the model performance on this oracle drops by 0.3. The reason might be that the easy close-range edge building actions become harder when pushed farther, also making easy decisions first is less prone to error propagation. Finally, we also change the order in which the various nodes connected to a token are created. Instead of generating the nodes from the root downwards, we perform a post-order traversal, where leaves are generated before parents. This also does not affect oracle score, however it gave a minor gain in parser performance. 





\paragraph{Effect of Beam Size}
Figure~\ref{fig:beam} shows performance for different beam sizes. Ideally, if the model is more certain and accurate in making right predictions at different steps, the decoding performance should be less impacted by beam size. The results show that performance improves with beam size, but the gains saturate at beam size 3. This indicates that a smaller beam size can be considered for application scenarios with time constraints.
































\begin{figure}[!t]
    \centering
\includegraphics[width=\columnwidth]{figures/beam_smatch_6x6_3x3.pdf}
    \caption{Effect of decoding beam size for \textsc{Smatch}, with our best single models on AMR 2.0 test set.}
    \label{fig:beam}
\end{figure}
