\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{xspace}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[numbers]{natbib}
\usepackage{xcolor}
\usepackage{balance}


\newif\ifarxiv
\newif\ifnotarxiv

\arxivtrue
\notarxivfalse



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\date{}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\def \pzo {\phantom{0}} 
\def \dzo {\phantom{00}}
\def \tzo {\phantom{000}}
\def \qzo {\phantom{0000}}

\newcommand{\stdminus}[1]{\scalebox{0.65}{}}
\newcommand{\rv}[1]{{\color{red}#1}}
\newcommand{\hugo}[1]{{\color{blue!20!red}[\textbf{Hugo}:#1]}}
\newcommand{\matthijs}[1]{{\color{blue}[\textbf{Matthijs}:#1]}}
\newcommand{\ben}[1]{{\color{orange}[\textbf{Benjamin}:#1]}}
\newcommand{\alaa}[1]{{\color{green!40!red}[\textbf{Alaa}:#1]}}
\newcommand{\pierre}[1]{{\color{green!20!red}[\textbf{Pierre}:#1]}}
\newcommand{\aj}[1]{{\color{green!20!red}[\textbf{Armand}:#1]}}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{1.0ex \@plus 1ex \@minus .2ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother

\usepackage{enumitem}
\setlist[itemize]{topsep=5pt,
labelsep=5pt,labelindent=0.4\parindent,itemindent=0pt,leftmargin=*,itemsep=-1pt 
}





\begin{document}

\title{LeViT: a Vision Transformer in ConvNet's Clothing \\ for Faster Inference}

\author{
Benjamin Graham \and
Alaaeldin El-Nouby \and
Hugo Touvron \and
Pierre Stock  \and 
Armand Joulin \and
Herv\'e J\'egou \and
Matthijs Douze
}

\maketitle



\begin{abstract}
We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. 
Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. 
We also introduce the attention bias, a new way to integrate positional information in vision transformers.

As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. 
We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. 
Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff.
For example, at 80\% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at \url{https://github.com/facebookresearch/LeViT}.
\end{abstract}



\vspace{-10pt}
\section{Introduction}
\label{sec:introduction}

Transformer neural networks were initially introduced for Natural Language Processing applications~\cite{Vaswani2017AttentionIA}. They now dominate in most applications of this field. 
They manipulate variable-size sequences of token embeddings that are fed to a residual architecture. 
The model comprises two sorts for residual blocks: Multi-Layer Perceptron (MLP) and an original type of layer: the self-attention, which allows all pairs of tokens in the input to be combined via a bilinear function. 
This is in contrast to 1D convolutional approaches that are limited to a fixed-size neighborhood.




\begin{figure}
\hspace*{-7mm}\includegraphics[width=1.2\linewidth]{figs/tradeoffs_fp16.pdf}
\caption{\label{fig:mainspeedaccuracyplot}
Speed-accuracy operating points for convolutional and visual transformers. 
	\emph{Left} plots: on 1 CPU core, \emph{Right:} on 1 GPU.
	 LeViT is a stack of transformer blocks, with pooling steps to reduce the resolution of the activation maps as in classical convolutional architectures. 
}
\end{figure}


Recently, the vision transformer (ViT) architecture~\cite{dosovitskiy2020image} obtained state-of-the-art results for image classification in the speed-accuracy tradeoff with pre-training on large scale dataset. The Data-efficient Image Transformer \cite{Touvron2021DeiT} obtains competitive performance when training the ViT models only on ImageNet \cite{deng2009imagenet}. It also introduces smaller models adapted for high-throughput  inference.


In this paper, we explore the design space to offer even better trade-offs than ViT/DeiT models in the regime of small and medium-sized architectures. We are especially interested in optimizing the performance--accuracy trade-off, such as the throughput (images/second) performance depicted in Figure~\ref{fig:mainspeedaccuracyplot} for Imagenet-1k-val~\cite{Russakovsky2015ImageNet12}. 

While many works \cite{Han2015DeepCC,courbariaux2016binaryconnect,zhou2018dorefanet,wang2019haq,stock2020bit} aim at reducing the memory footprint of classifiers and feature extractors, inference speed is equally important, with high throughput corresponding to better energy efficiency.
In this work, our goal is to develop a Vision Transformer-based family of models with better inference speed on both highly-parallel architectures like GPU, regular Intel CPUs, and ARM hardware commonly found in mobile devices. 
Our solution re-introduces convolutional components in place of transformer components that learn convolutional-like features. 
In particular, we replace the uniform structure of a Transformer by a pyramid with pooling, similar to the LeNet~\cite{lecun1989backpropagation} architecture. Hence we call it LeViT. 

There are compelling reasons why transformers are faster than convolutional architectures for a given computational complexity. 
Most hardware accelerators (GPUs, TPUs) are optimized to perform large matrix multiplications.
In transformers, attention and MLP blocks rely mainly on these operations. 
Convolutions, in contrast, require complex data access patterns, so their operation is often IO-bound.
These considerations are important for our exploration of the speed/accuracy tradeoff.  













\paragraph{The contributions}
of this paper are techniques that allow ViT models to be shrunk down, both in terms of the width and spatial resolution:

\begin{itemize}
\item
	A multi-stage transformer architecture using attention as a downsampling mechanism;
\item
    A computationally efficient patch descriptor that shrinks the number of features in the first layers;
\item
    A learnt, per-head translation-invariant attention bias that replaces ViT's positional embeddding;
\item
    A redesigned Attention-MLP block that improves the network capacity for a given compute time.
\end{itemize}


% 



\section{Related work}
\label{sec:related}





The convolutional networks descended from LeNet~\cite{lecun1989backpropagation} have evolved substantially over time~\cite{Krizhevsky2012AlexNet,Simonyan2015VGG,He2016ResNet,Szegedy2016InceptionResNetV2,Xie2017AggregatedRT,tan2019efficientnet}.
The most recent families of architectures focus on finding a good trade-off between efficiency and performance~\cite{Radosavovic2020RegNet,tan2019efficientnet,Howard2019SearchingFM}.
For instance, the EfficientNet~\cite{tan2019efficientnet} family was discovered by carefully designing individual components followed by hyper-parameters search under a FLOPs constraint.







\paragraph{Transformers.} 
The transformer architecture was first introduced by Vaswani \etal~\cite{Vaswani2017AttentionIA} for machine translation. Transformer encoders primarily rely on 
the self-attention operation in conjunction with feed-forward layers, providing a strong and explicit method for learning long range dependencies. Transformers have been subsequently adopted for NLP tasks providing state-of-the-art performance on various benchmarks \cite{devlin2018bert, Radford2018improving}. 
There have been many attempts at adapting the transformer architecture to images~\cite{parmar2018image,child2019generating}, first by applying them on pixels.
Due to the quadratic computational complexity and number of parameters involved by attention mechanisms, most authors~\cite{child2019generating,Cordonnier2020OnTR} initially considered images of small sizes like in CIFAR or Imagenet64~\cite{chrabaszcz2017downsampled}.
Mixed text and image embeddings already use transformers with detection bounding boxes as input~\cite{li2020oscar}, \ie the bulk of the image processing is done in the convolutional domain.

\paragraph{The vision transformer (ViT)~\cite{dosovitskiy2020image}.} 
Interestingly, this transformer architecture is very close to the initial NLP version, devoid of explicit convolutions (just fixed-size image patch linearized into a vector), yet it competes with the state of the art for image classification. 
ViT achieves strong performance when pre-trained on a large labelled dataset such as the JFT300M   (non-public, although training on Imagenet-21k also produces competitive results).  

The need for this pre-training, in addition to strong data augmentation, can be attributed to the fact that transformers have less built-in structure than convolutions, in particular they do not have an inductive bias to focus on nearby image elements. The authors  hypothesized that a large and  varied dataset is needed to regularize the training.

In DeiT~\cite{Touvron2021DeiT}, the need for the large pre-training dataset is replaced with a student-teacher setup and stronger data augmentation and regularization, such as stochastic depth~\cite{Huang2016DeepNW} or repeated augmentation~\cite{berman2019multigrain,hoffer2020augment}. 
The teacher is a convolutional neural network that ``helps'' its student network to acquire an inductive bias for convolutions. 
The vision transformer has been thereafter successfully adopted by a wider range of computer vision tasks including object detection \cite{beal2020toward}, semantic segmentation \cite{zheng2020rethinking} and image retrieval \cite{el2021training}.

\paragraph{Positional encoding.} Transformers take a set as input, and hence are invariant to the order of the input. However, in language as well as in images, the inputs come from a structure where the order is important. The original Transformer~\cite{Vaswani2017AttentionIA} incorporates absolute non-parametric positional encoding with the input. 
Other works has replaced them with parametric encoding~\cite{ghering2017convseq2seq} or adopt Fourier-based kernelized versions~\cite{parmar2018image}. 
Absolute position encoding enforce a fixed size for the set of inputs, but some works use relative position encoding~\cite{shaw2018self} that encode the relative position between tokens. 
In our work, we replace these explicit positional encoding by positional biases that implicitly encode the spatial information.  



\paragraph{Attention along other mechanisms.}  
Several works have included attention mechanisms in neural network architectures designed for vision~\cite{wang2017residual,woo2018cbam,li2019selective,bello2019attention}.
The  mechanism is used channel-wise to capture cross-feature information that complements convolutional layers~\cite{Wang2018NonlocalNN,chen2020dynamic,zhao2020exploring}, select paths in different branch of a network~\cite{szegedy2015going}, or combine both~\cite{zhang2020resnest}. 
For instance, the squeeze-and-excite network of Hu \etal \cite{Hu2017SENet} has an attention-like module to model the channel-wise relationships between the features of a layer. 
Li \etal \cite{li2019selective} use the attention mechanism between branches of the network to adapt the receptive field of neurons.

Recently, the emergence of transformers led to hybrid architectures that benefit from other modules.
Bello~\cite{Bello2021LambdaNetworksML} proposes an approximated content attention with a positional attention component. 
Child~\etal~\cite{child2019generating} observe that many early layers in the network learn locally connected patterns, which resemble convolutions. 
This suggests that hybrid architectures inspired both by transformers and convnets are a compelling design choice. A few recent works explore this avenue for different tasks ~\cite{Srinivas2021BottleneckTF,wu2020visual}. 
In image classification, a recent work that comes out in parallel with ours is the Pyramid Vision Transformer (PVT)~\cite{wang2021pyramid}, whose design is heavily inspired by ResNet. It is principally intended to address object and instance segmentation tasks.   

Also concurrently with our work, \citet{yuan2021tokens} propose the Tokens-to-Tokens ViT (T2T-ViT) model. Similar to PVT, its design relies on re-tokenization of the output after each layer by aggregating the neighboring tokens such number of tokens are progressively reduced. 
Additionally, \citet{yuan2021tokens} investigate the integration of architecture design choices from CNNs \cite{Hu2017SENet, zagoruyko2016wide, huang2017densely} that can improve the performance and efficiency of vision transformers. 
As we will see, these recent methods are not as much focused as our work on the trade-off between accuracy and inference time. They are not competitive with respect to that compromise. 

 


\section{Motivation}
\label{sec:motivation}

In this section we discuss the seemingly convolutional behavior of the  transformer patch projection layer. We then carry out ``grafting experiments'' of a transformer (DeiT-S) on a standard convolutional architecture (ResNet-50). The conclusions drawn by this analysis will motivate our subsequent design choices in Section~\ref{sec:model}.  


\begin{figure}[t]
\newcommand{\igconvmask}[1]{\includegraphics[width=0.45\linewidth,trim=15 15 460 0,clip]{figs/deit_conv_mask/#1.png}}
\hspace*{-1mm}
\begin{tabular}{@{}c@{\ \ \ }cc@{}}
& head 3 & head 8 \\
\raisebox{3ex}{K} & 
\igconvmask{K_3} & 
\igconvmask{K_8} \\
\hline
\raisebox{3ex}{Q} & 
\igconvmask{Q_3} &
\igconvmask{Q_8} \\
\hline
\raisebox{3ex}{V} & 
\igconvmask{V_3} & 
\igconvmask{V_8} \\
\end{tabular}

\caption{\label{fig:deitfilters}
	Patch-based convolutional masks in the pre-trained DeiT-base model~\cite{Touvron2021DeiT}.
	The figure shows 12 of the 64 filters per head.
	Note that the K and Q filters are very similar, this is because the weights are entangled in the  multiplication. 
}
\end{figure}


\subsection{Convolutions in the ViT architecture}

ViT's patch extractor is a 16x16 convolution with stride 16. Moreover, the output of the patch extractor is multiplied by learnt weights to form the first self-attention layer's  and  embeddings, so we may consider these to also be convolutional functions of the input. This is also the case for variants like DeiT~\cite{Touvron2021DeiT} and PVT~\cite{wang2021pyramid}. In Figure~\ref{fig:deitfilters} we visualize the first layer of DeiT's attention weights, broken down by attention head.
This is a more direct representation than the principal components depicted by Dosovitskiy \etal~\cite{dosovitskiy2020image}. One can observe the typical patterns inherent to convolutional architectures: attention heads specialize in specific patterns (low-frequency colors / high frequency graylelvels), and the patterns are similar to Gabor filters.

In convolutions where the convolutional masks overlap significantly, the spatial smoothness of the masks comes from the overlap: nearby pixel receive approximately the same gradient.
For ViT convolutions there is no overlap. The smoothness mask is likely caused by the data augmentation: when an image is presented twice, slightly translated, the same gradient goes through each filter, so it learns this spatial smoothness.

Therefore, in spite of the absence of ``inductive bias'' in transformer architectures, the training \emph{does} produce filters that are similar to traditional convolutional layers. 


\begin{figure}[t]
\includegraphics[trim=35 0 45 15,clip, width=1.04\linewidth]{figs/res50_deit.pdf}
\caption{
	\label{fig:hybrid_convergence} 
	Models with convolutional layers show a faster convergence  in the early stages
 compared to their DeiT counterpart.
}
\end{figure}

\subsection{Preliminary experiment: grafting}

The authors of the ViT image classifier~\cite{dosovitskiy2020image} experimented with stacking the transformer layers above a traditional ResNet-50. 
In that case, the ResNet acts as a feature extractor for the transformer layers and the gradients can be propagated back through the two networks.
However, in their experiments, the number of transformer layers was fixed (e.g. 12 layers for ViT-Base). 

In this subsection, we investigate the potential of mixing transformers with convolutional network \emph{under a similar computational budget}: 
We explore trade-offs obtained when varying the number of convolutional stages
and transformer layers. Our objective is to evaluate  variations of convolutional and transformer hybrids while controlling for the runtime. 






\paragraph{Grafting.}

The grafting combines a ResNet-50 and a DeiT-Small.
The two networks have similar runtimes.

We crop the upper stages of the ResNet-50 and likewise reduce the number of DeiT layers (while keeping the same number of transformer and MLP blocks). 
Since a cropped ResNet produces larger activation maps than the  activations consumed by DeiT, we introduce a pooling layer between them. 
In preliminary experiments we found average pooling to perform best.
The positional embedding and classification token are introduced at the interface between the convolutional and transformer layer stack. For the ResNet-50 stages, we use
ReLU activation units~\cite{nair2010rectified} and batch normalization~\cite{ioffe15batchnorm}.

\paragraph{Results.}
Table~\ref{tab:grafted} summarizes the results. 
The grafted architecture produces better results than both DeiT and ResNet-50 alone.
The smallest number of parameters and best accuracy are with two stages of ResNet-50, because this excludes the convnet's large third stage. 
Note that in this experiment, the training process is similar to DeiT: 300 epochs, we measure the top-1 validation accuracy on ImageNet, and the speed as the number of images that one GPU can process per second. 

One interesting observation that we show Figure~\ref{fig:hybrid_convergence} is that the convergence of grafted models during training seems to be similar to a convnet during the early epochs and then switch to a convergence rate similar to DeiT-S. 
A hypothesis is that the convolutional layers have the ability to learn representations of the low-level information in the earlier layers more efficiently due to their strong inductive biases, noticeably their translation invariance. 
They rely rapidly on meaningful patch embeddings, which can explain the faster convergence during the first epochs.

\paragraph{Discussion.}

It appears that in a runtime controlled regime it is beneficial to insert convolutional stages below a transformer.
Most of the processing is still done in the transformer stack for the most accurate variants of the grafted architecture. 
Thus, the priority in the next sections will be to reduce the computational cost of the transformers.
For this, instead of just grafting, the transformer architecture needs to be merged more closely with the convolutional stages.






\begin{table}[t]
\scalebox{0.8}{
\begin{tabular}{@{\ }cc|c|ccc|c@{\ }}
\toprule
\#ResNet & \#DeiT-S & nb. of  &  \multicolumn{2}{c}{FLOPs (M)} & Speed & IMNET \\
\cline{4-5} 
stages       & \ layers  & Params &  conv  & transformer  & im/s  & top-1 \\              
\midrule
0     &    12 & 22.0M & \dzo57   &  4519 & \pzo966  & 79.9 \\
1     & \pzo9 & 17.1M & \pzo820  &  3389 & \pzo995  & 80.6 \\
2     & \pzo6 & 13.1M & 1876 &  2260 & 1048     & 80.9  \\
3     & \pzo3 & 15.1M & 3385 &  1130 & 1054     & 80.1  \\
4     & \pzo0 & 25.5M & 4119 &  \tzo0  & 1254     & 78.4 \\ 
\bottomrule
\addlinespace[2mm]
\end{tabular}}
\caption{\label{tab:grafted}
	DeiT architecture grafted on top of a truncated ResNet-50 convolutional architecture.}
	\vspace{-3pt}
\end{table}


 


\section{Model}
\label{sec:model}

In this section we describe the design process of the LeViT architecture and what tradeoffs were taken. 
The architecture is summarized in Figure~\ref{fig:blockdiagram}.


\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{figs/block_diagram.pdf}
\vspace{-15pt}
\caption{\label{fig:blockdiagram}
	Block diagram of the LeViT-256 architecture. 
	The two bars on the right indicate the relative resource consumption of each layer, measured in FLOPs, and the number of parameters.
}
\vspace{-10pt}
\end{figure}













\subsection{Design principles of LeViT}

LeViT builds upon the ViT~\cite{dosovitskiy2020image} architecture and DeiT~\cite{Touvron2021DeiT} training method. We incorporate components that were proven useful for convolutional architectures. 
The first step is to get a compatible representation. 
Discounting the role of the classification embedding, ViT is a stack of layers that processes activation maps.
Indeed, the intermediate ``token'' embeddings can be seen as the traditional  activation maps in FCN architectures ( format). 
Therefore, operations that apply to activation maps (pooling, convolutions) can be applied to the intermediate representation of DeiT. 

In this work we optimize the architecture for compute, not necessarily to minimize the number of parameters. 
One of the design decisions that makes the ResNet~\cite{He2016ResNet} family more efficient than the VGG network~\cite{Simonyan2015VGG} is to apply strong resolution reductions with a relatively small computation budget in its first two stages. 
By the time the activation map reaches the big third stage of ResNet, its resolution has already shrunk enough that the convolutions are applied to small activation maps, which reduces the computational cost. 

\subsection{LeViT components}

\paragraph{Patch embedding.}

The preliminary analysis in Section~\ref{sec:motivation} showed that the accuracy can be improved when a small convnet is applied on input to the transformer stack.
In LeViT we chose to apply 4 layers of   convolutions (stride 2) to the input to perform the resolution reduction. The number of channels goes .
This reduces the activation map input to the lower layers of the transformer 
without losing salient information.
The patch extractor for LeViT-256 transforms the image shape  into  with 184 MFLOPs.
For comparison, the first 10 layers of a ResNet-18 perform the same dimensionality reduction with 1042 MFLOPs.

\paragraph{No classification token.}

To use the  tensor format, we remove the classification token. Similar to convolutional networks, we replace it by average pooling on the last activation map, which produces an embedding used in the classifier. 
For distillation during training, we train separate heads for the classification and distillation tasks. 
At test time, we average the output from the two heads.
In practice, LeViT can be implemented using either  or  tensor format, whichever is more efficient.


\paragraph{Normalization layers and activations.}

The FC layers in the ViT architecture are equivalent to  convolutions.
The ViT uses layer normalization before each attention and MLP unit. 
For LeViT, each convolution is followed by a batch normalization.
Following~\cite{Goyal2017AccurateLM}, each batch normalization weight parameter that joins up with a residual connection is initialized to zero.
The batch normalization can be merged with the preceding convolution for inference, which is a runtime advantage over layer normalization (for example, on EfficientNet B0, this fusion speeds up inference on GPU by a factor 2).
Whereas DeiT uses the GELU function, all of LeViT's non-linear activations are Hardswish~\cite{Howard2019SearchingFM}.

\paragraph{Multi-resolution pyramid.}

Convolutional architectures are built as pyramids, where the resolution of the activation maps decreases as their number of channels increases during processing. 
In Section~\ref{sec:motivation} we used the ResNet-50 stages to pre-process the transformer stack.


LeViT integrates the ResNet stages within the transformer architecture. 
Inside the stages, the architecture is similar to a visual transformer: a residual structure with alternated MLP and activation blocks. 
In the following we review the modifications of the attention blocks (Figure~\ref{fig:attentionblock}) compared to the classical setup~\cite{Vaswani2017AttentionIA}. 

\paragraph{Downsampling.}



Between the LeViT stages, a \emph{shrinking attention block} reduces the size of the activation map:
a subsampling is applied before the Q transformation, which then propagates to the output of the soft activation. This maps an input tensor of size  to an output tensor of size  with . Due to the change in scale, this attention block is used without a residual connection. To prevent loss of information, we take the number of attention heads to be .


\paragraph{Attention bias instead of a positional embedding.}

The positional embedding in transformer architectures is a location-dependent trainable parameter vector that is added to the token embeddings prior to inputting them to the transformer blocks.
If it was not there, the transformer output would be independent to permutations of the input tokens.
Ablations of the positional embedding result in a sharp drop of the classification accuracy~\cite{Chu2021PositionEncoding}. 

However positional embeddings are included only on input to the sequence of attention blocks. 
Therefore, since the positional encoding is important for higher layers as well, it is likely that it remains in the intermediate representations and needlessly uses representation capacity. 


Therefore, our goal is to provide positional information within each attention block, and to explicitly inject relative position information in the attention mechanism: 
we simply add an \emph{attention bias} to the attention maps.
The scalar attention value between two pixels  and  for one head  is calculated as

The first term is the classical attention. 
The second is the translation-invariant attention bias. Each head has  parameters corresponding to different pixel offsets. Symmetrizing the differences  and  encourages the model to train with flip invariance.

\begin{figure}
\centering
\includegraphics[width=0.43\linewidth]{figs/attn_regular.pdf} \hfill
\raisebox{0pt}{\includegraphics[width=0.54\linewidth]{figs/attn_shrink.pdf}}
\caption{\label{fig:attentionblock}
	The LeViT attention blocks, using similar notations to~\cite{Wang2018NonlocalNN}.
	Left: regular version, 
	Right: with 1/2 reduction of the activation map.
	The input activation map is of size . 
	 is the number of heads, the multiplication operations are performed independently per head.
}
\end{figure}

\paragraph{Smaller keys.} The bias term reduces the pressure on the keys to encode location information, so we reduce the size of the keys matrices relative to the  matrix. If the keys have size ,  will have  channels. Restricting the size of the keys reduces the time needed to calculate the key product .

For downsampling layers, where there is no residual connection, we set the dimension of  to  to prevent loss of information. 

\paragraph{Attention activation.}

We apply a Hardswish activation to the product   before the regular linear projection is used to combine the output of the different heads. 
This is akin to a ResNet bottleneck residual block, in the sense that  is the output of a   convolution,  corresponds to a spatial convolution, and the projection is another  convolution.

\paragraph{Reducing the MLP blocks.}

The MLP residual block in ViT is a linear layer that increases the embedding dimension by a factor 4, applies a non-linearity and reduces it back with another non-linearity to the original embedding's dimension.
For vision architectures, the MLP is usually more expensive in terms of runtime and parameters than the attention block.
For LeViT, the ``MLP'' is a  convolution, followed by the usual batch normalization. 
To reduce the computational cost of that phase, we reduce the expansion factor of the convolution from 4 to 2.
One design objective is that attention and MLP blocks  consume approximately the same number of FLOPs.



\subsection{The LeViT family of models}

The LeViT models can spawn a range of speed-accuracy tradeoffs by varying the size of the computation stages. 
We identify them by the number of channels input to the first transformer, \eg LeViT-256 has 256 channels on input of the transformer stage. 
Table~\ref{tab:levitvariants} shows how the stages are designed for the models that we evaluate in this paper.


\def \mysp {\hspace{9pt}}
\begin{table*}
\newcommand{\ntimes}[3]{#1}
\newcommand{\subsamrow}[1]{\phantom{4}}
\newcommand{\subsamrowd}[1]{\phantom{4\ }}
\ifarxiv
\centering
\else
\scalebox{0.8}{
\fi
\begin{tabular}{|@{\mysp}l@{}|@{\mysp}c@{\mysp}|@{\mysp}c@{\mysp}|@{\mysp}c@{\mysp}|@{\mysp}c@{\mysp}|@{\mysp}c|@{\ }}
\toprule 
Model & LeViT-128S &  LeViT-128 & LeViT-192 & LeViT-256 & LeViT-384 \tabularnewline
 & () &  () & () & () & ()\tabularnewline
\midrule
\begin{minipage}{1.2cm} Stage 1:\\  \end{minipage} &
  \ntimes{2}{128}{4} & 
  \ntimes{4}{128}{4} & 
  \ntimes{4}{192}{3} & 
  \ntimes{4}{256}{4} & 
  \ntimes{4}{384}{6} \15pt]
\begin{minipage}{1.2cm} Stage 2:  \end{minipage} &
  \ntimes{3}{256}{6} & 
  \ntimes{4}{256}{8} & 
  \ntimes{4}{288}{5} & 
  \ntimes{4}{384}{6} & 
  \ntimes{4}{512}{9} \18pt]
\begin{minipage}{1.2cm} Stage 3:   \end{minipage} &
  \ntimes{4}{384}{8} & 
  \ntimes{4}{384}{12} & 
  \ntimes{4}{384}{6} & 
  \ntimes{4}{512}{8} & 
  \ntimes{4}{768}{12} \5pt]
    \showblocks{0} \5pt]
    Stage 1, attention block 4 \8pt]
    Shrinking attention between stages 1 and 2 \8pt]
    Stage 2, attention block 1 \5pt]
     \5pt]
    \showblocks{16} \\
     \5pt]
    \showblocks{26} \5pt]
    \end{tabular}}}
    \caption{
        Visualization of the attention bias for several blocks of a trained LeViT-256 model.
        The center for which the attention is computed is the upper left pixel of the map (with a square).
        Higher bias values are in yellow, lower values in dark blue (values range from -20 to 7).
    \label{fig:attentionbias}}
\end{figure*}




\ifarxiv 


\else


\begingroup
\setlength{\bibsep}{5pt}
  \bibliographystyle{IEEEtranN_fullname}
  \bibliography{egbib}
\endgroup



\end{document}
 
\end{document}
