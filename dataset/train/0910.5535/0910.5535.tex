\documentclass[11pt]{article}
\usepackage{amsfonts}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{theorem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[left=1in,right=1in,top=1in,bottom=1in,nohead]{geometry}
\usepackage[active]{srcltx}
\parindent 0in
\parskip .1in
\allowdisplaybreaks[2]
\def\mstar{\mnote{****}}
\newcommand{\ElseIf}{\ElsIf}
\newcommand{\TO}{\tilde{O}}
\newcommand{\TT}{\tilde{\Theta}}
\newcommand{\TOm}{\tilde{\Omega}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\dist}{\mathop{\mathrm{dist}}}
\newcommand{\Bin}{\mathop{\mathrm{Bin}}}
\newcommand{\card}[1]{\left|#1\right|}
\newenvironment{proof}{{\bf Proof:}}{\hfill\mbox{}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\mbox{}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}

\newtheorem{fact}{Fact}[section]
\def\dw{W}
\def\dww{W_2}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\F{\Phi}
\def\tc{\tilde{c}}
\def\cA{{\cal A}}
\def\cM{{\cal M}}
\def\cP{{\cal P}}
\def\cE{{\cal E}}
\def\bx{{\bf x}}
\def\a{\alpha}
\def\b{\beta}
\def\d{\delta}
\def\D{\Delta}
\def\e{\varepsilon}
\def\f{\phi}
\def\g{\gamma}
\def\G{\Gamma}
\def\k{\kappa}
\def\la{\lambda}
\def\K{\Kappa}
\def\z{\zeta}
\def\th{\theta}
\def\TH{\Theta}
\def\l{\lambda}
\def\L{\Lambda}
\def\m{\mu}
\def\n{\nu}
\def\p{\pi}
\def\P{\Pi}
\def\r{\rho}
\def\R{\Rho}
\def\s{\sigma}
\def\S{\Sigma}
\def\t{\tau}
\def\om{\omega}
\def\OM{\Omega}

\def\cV{{\cal V}}
\def\cF{{\cal F}}
\def\para{\vspace{.1in}}
\newcommand{\gap}[1]{\hspace{#1in}}
\def\whp{{\bf whp}}
\def\Whp{{\bf Whp}}
\newcommand{\rdup}[1]{\left\lceil #1 \right\rceil }
\newcommand{\rdown}[1]{\left\lfloor #1 \right\rfloor }
\newcommand{\mnote}[1]{\marginpar{\footnotesize\raggedright#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\def\vol{{\bf vol}_N}
\def\diam{{\bf diam}}
\newcommand{\proofstart}{{\bf Proof\hspace{2em}}}
\newcommand{\proofend}{\hspace*{\fill}\mbox{}}
\def\eps{\varepsilon}
\def\cX{{\cal X}}
\def\R{\mathbb{R}}
\def\E{{\sf E}}
\def\Pr{{\sf P}}
\def\ba{{\bf a}}
\newcommand{\raw}[1]{\rightarrow_{#1}}
\newcommand{\ang}[1]{\langle #1\rangle}
\def\cC{{\cal C}}
\def\cB{{\cal B}}
\def\cD{{\cal D}}
\def\cG{{\cal G}}
\newcommand{\ignore}[1]{}
\def\tO{\tilde{O}}
\def\Bdnm{\cB_{d,\n,\m}}
\def\Gv{\cX_{\vv}}
\def\Poi{\mathop{\mathrm{Poi}}}
\def\vv{\vec{v}}
\def\yy{\vec{y}}
\def\cQ{{\cal Q}}
\newtheorem{theorem}{Theorem}
\newtheorem{example}[theorem]{Example}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}



\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\bfrac}[2]{\brac{\frac{#1}{#2}}}
\newcommand{\bink}[2]{{#1 \choose #2}}
\newcommand{\beq}[1]{}
\newcommand{\blem}[1]{\begin{lemma}\label{#1}}
\newcommand{\elem}{\end{lemma}}
\newcommand{\bth}[1]{\begin{theorem}\label{#1}}
\newcommand{\enth}{\end{theorem}}
\newcommand{\brem}[1]{\begin{remark}\label{#1}}
\newcommand{\erem}{\end{remark}}

\def\c{{3/4}}
\def\go{{3/5}}



\author{Alan Frieze\thanks{Supported in part by NSF Grant DMS0753472.}, P\'all Melsted\\
Department of Mathematical Sciences\\Carnegie Mellon University\\Pittsburgh PA 15213\\
U.S.A.}
\title{Maximum Matchings in Random Bipartite Graphs and the Space Utilization of Cuckoo Hashtables}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We study the the following question in Random Graphs. 
We are given two disjoint sets  with  and . We construct a random graph  by allowing each
 to choose  random neighbours in . The question discussed is as to the size  of the largest matching in .
When considered in the context of Cuckoo Hashing, one key question is as to when is  \whp? 
We answer this question exactly when 
is at least three. We also establish a precise threshold for when Phase 1 of the Karp-Sipser Greedy matching algorithm
suffices to compute a maximum matching \whp.
\end{abstract}


\section{Introduction}
For a graph  we let  denote the size of the maximum matching in .
In essence this paper provides an analysis of  
in the following model of a random bipartite graph. We have two disjoint
sets  where  where . Each  independently chooses  random vertices of  as neighbours. 
Our assumptions are that  are fixed and . One motivation for this study comes from
Cuckoo Hashing.

Briefly each one of  items  has
 possible locations , where  is
typically a small constant and the  are hash functions, typically
assumed to behave as independent fully random hash functions.  (See
\cite{MVad} for some justification of this assumption.)  
We are thus led to consider the bipartite graph  which has vertex set  and edge set . 
Under the assumption that the hash functions are completely random we see that  has the 
same distribution as the random graph defined in the previous paragraph. 

We assume
each location can hold only one item.  When an item  is inserted
into the table, it can be placed immediately if one of its  locations is
currently empty.  If not, one of the items in its  locations must
be displaced and moved to another of its  choices to make room
for .  This item in turn may need to displace another item out of one
its  locations.  Inserting an item may require a sequence of moves,
each maintaining the invariant that each item remains in one of its
 potential locations, until no further evictions are needed. 
Thus having inserted  items, we have constructed a matching  of size  in . Adding a 'th
item is tantamount to constructing an augmenting path with repsect to .
All  items will be insertable
in this way iff  contains a matching of size .

The case of  choices is notably different from that for other values
of  and the theory for the case where there are  
bucket choices for each item is well understood at this point
\cite{DM,kutz,cuckoo}. We will therefore assume that .

We will now revert to the abstract question posed in first paragraph of the paper.
\section{Definitions and Results}
This question was studied to some extent by Fotakis, Pagh, Sanders and Spirakis \cite{fotakis}.
They show in the course of their analysis of Cuckoo hashing that the following holds:
\begin{lemma}\label{fot}
Suppose that  and . Suppose also that . Then
\whp\  contains a matching of size  i.e. a matching of  into .
\end{lemma}
\proofend

In particular, if  and  then Lemma \ref{fot} shows that there is a matching of  into  \whp.

This lemma is not tight and recently  Mitzenmacher et al \cite{DMP} observed a connection with a result of
Dubois and Mandler on Random 3-XORSAT \cite{DuMa} that enables one to essentially answer the question as to when
 for the case . More recently, Fountoulakis and Panagiotou \cite{FP} have established 
thresholds for when there is a matching of  into  \whp, for all .

We begin with a simple observation that is the basis of the Karp-Sipser Algorithm
\cite{KS,AFP}. If  is a vertex of degree one in  and  is its unique incident edge,
then there exists a maximum matching of  that includes . Karp and Sipser exploited this via a simple 
greedy algorithm:
 \begin{algorithm}
\caption{Karp-Sipser Algorithm}\label{KS}
\begin{algorithmic}[1]
\Procedure{KSGreedy}{}
\State 
\While{}
  \If { has vertices of degree one}
    \State Select a vertex  uniformly at random from the set of vertices of degree one
    \State Let  be the edge incident to 
  \Else
    \State Select an edge  uniformly at random\label{phase2}
  \EndIf
  \State 
  \State  
\EndWhile
\State \textbf{return} M
\EndProcedure
\end{algorithmic}
\end{algorithm}

Phase 1 of the Karp-Sipser Algorithm ends and Phase 2 begins when the graph remaining has minimum degree at least two. 
So if  denotes the graph  remaining at the end of
Phase 1 and  is the number of iterations involved in Phase 1 then
\beq{simple}
\m(G)=\t_1+\m(\G_1).
\eeq
Our approach to estimating  is to (i) obtain an asymptotic expression for  that holds \whp\ and then (ii) show that 
\whp\  has a (near) perfect matching and then apply \eqref{simple}.

We summarise our results as follows: Let  satisfy 
\beq{zaa}
z_1=\frac{e^{z_1}-1}{d-1}
\eeq
and let
\beq{azaa}
\a_1=\frac{z_1}{d(1-e^{-z_1})^{d-1}}.
\eeq
\begin{theorem}\label{th1}\ 
If  then \whp\ .
\end{theorem}
Thus \whp\ Phase 1 of the Karp-Sipser Algorithm finds a (near) maximum matching if .
In particular, if  then  and  and thus  is enough for a
matching of  into .

Andrea Montanari has pointed out that our proof of Theorem \ref{th1} via the differential equations method is not
new and already appears in Luby, Mitzenmacher, Shokrollahi and Spielman \cite{LMSS} and also in Dembo and Montanari \cite{DeMo}.
We will prune this from the final version of the paper, but leave it in here for now.

Now consider larger . Let  be the largest non-negative solution to 

\begin{theorem}\label{th2}
If  then \whp
\begin{description}
\item[(a)] .
\item[(b)] .
\item[(c)] If  then
\beq{match}
\m(\G_1)=\min\set{|L_1|,\,|R_1|}=\min\set{n-\t_1,(1-(1+z^*)e^{-z^*})m+o(m)}.
\eeq
Here  are the two sides of the bipartition of , after deleting any isolated vertices from the 
-side.
\end{description}
\end{theorem}
\section{Structure of the paper}
We first prove Theorem \ref{th1}. This involves studying Phase 1 of the Karp-Sipser Algorithm. For this we first describe 
the distribution of the graph . This is done in Section \ref{probsec}. The distribution of  is determined
by a few parameters and these evolve as a Markov chain. To study this chain, we introduce and solve a set of differential
equations. This is done in Section \ref{diffeqs}. We show that the chains trajectory and the solution to the equations are close.
By analysing the equations we can tell when Phase 1 is sufficient to solve the problem. This is done in Section \ref{ap1}.
If Phase 1 is not sufficient then the graph  that remains has degree  on the -side and minmum degree at least two
on the -side. We show that \whp\  has a matching of size equal to the minimum set size of the partition. 
\cite{DMP} and \cite{DM} and \cite{FP}. 
\section{Probability Model for Phase 1}\label{probsec}
We will represent  and more generally  by a random sequence . 
A sequence  is to be viewed as  subsequences  where 
or . 
The 's represent edges that have been deleted by the Karp-Sipser algorithm.
For  we 
define the bipartite (multi-)graph  as follows: Its vertex set consists of a bipartition
 (the {\em left side}) and  (the {\em right side}). The edges incident with  are 

i.e. we read the sequence  from left to write and add edges to  in blocks of size . Each block being
assigned to a unique vertex of . 

We should be clear now that our probability space is  with uniform measure and not 
.

Given a graph
 we let  where  is the set of vertices in 
that have degree . We let  where .

For the graph  we choose  uniformly at random from  and put . Next
let  and let  be the graph  that we have after  
steps of Phase 1 of the Karp-Sipser algorithm. The sequence  is defined as follows: Observe first that
the vertex  of degree one is always in . Suppose that it is incident to the unique edge . Then
we simply replace  in  by  to obtain . We should thus think of the Karp-Sipser Algorithm as acting
on sequences  and not on graphs. We write  to mean that  can be obtained from  by a single 
Phase 1 step of the Karp-Sipser Algorithm. 

Let  where  is the number of vertices on the left
side of the bipartition of . Assuming that we have only run the Karp-Sipser algorithm up to the
end of Phase 1, we have . Also 
\beq{begin}
\vv(0)\sim (n,\a d e^{-\a d}m,(1-e^{-\a d}-\a d e^{-\a d})m)\ \ \whp. 
\eeq
We will omit the parameter
 from  when it is clear from the context. 
Let  be
the set of all  with parameters .

\begin{lemma}\label{lem2}
Suppose  is a random member of . Then given , 
is a random member of  for all .
\end{lemma}
\begin{proof}
We prove this by induction on . It is true for  by assumption and so assume it is true for some .
Let  and now fix a triple  as a possible value for . Fix .
We first compute the
number of  such that . Let  be the number of vertices in . Some of these
will be in  and some will be in . So we choose non-negative integers  such that 
. Next let  be the number of vertices in . 
We can choose a vertex  so that  in \mstar\  
ways and now let us enumerate the ways of choosing .
Our choices for  are (i) distinctly from  (i.e.  is distinct from rest of the ), (ii) 
non-distinctly from  (i.e.  is chosen more than once in the construction), (iii) from  and 
(iv) from . We must exercise choice (i) exactly  times, choice (ii) at least twice for each of  distinct values,
choice (iii) at least once for each of  distinct values and choice (iv) the remaining times.

\ignore{
Next let 
be the number of distinct neighbours of  in \bx. We will use the notation
 for a vector . 
The number of choices for \bx\
is then 
\beq{indy}
\sum_{\substack{0\leq c\leq d-a-b\\0\leq b_1\leq b}}\ 
\sum_{\substack{\br\in [d]^{b_1},\bs\in [d]^{b_2},\bt\in [d]^c\\|r|+|s|=d-a-c}}(n-t)
\binom{m-v'-v_1'}{a}\binom{v_1'}{b}\binom{v'}{d-a-b}(v')^{d-a-b}\binom{d}{c}\binom{d-c}{r_1,\ldots,s_b}.
\eeq 
(i)  choices for , (ii)  choices for 
the set of vertices of degree one in \bx\ that become of degree zero in \by, (iii)  choices for 
the set of vertices of degree at least two in  
that become of degree one in , (iv)  choices for the set of vertices of degree at least two in \bx\
that are incident with  and have degree at least two in \by, (v)  ways of filling in . 
}
The number of choices for  depends only on  and ,
i.e. for each  we have that 
 
is independent of \by, given  and
. 

Similarly given  there is a unique , which when removed determines
. Thus  is fixed given .  Thus if  is a random member of  then 

which is independent of  given  and so  is a random member of .
\end{proof}

\begin{lemma}
The random sequence  is a Markov chain.
\end{lemma}
\begin{proof}
As in \cite{AFP},

which depends only on .
\end{proof}


\begin{lemma}\label{trunp}
Conditional on  if  is selected uniformly at random from
 then each vertex  has degree  where , a Poisson random variable
conditioned to take a value at least two, and  satisifes

where .

The  are also conditioned to satisfy
. 
\end{lemma}
\begin{proof}
Suppose we first fix the edges incident with vertices of degree one in \bx. Then we randomly fill in the remaining
 non- positions in \bx\ with values from some fixed -subset  of , subject to each of these  vertices
having degree at least two. The degrees  of these vertices will have the description described in the lemma (for a proof 
see Lemma 4 of \cite{AFP}). 
\end{proof}

From \cite{AFP} we can use the following lemma
\begin{lemma} {\bf \cite{AFP}}\\
{\bf (a)} Assume that . For every  and ,
\beq{eq0}
\Pr(Y_j = k | \vv) = \frac{z^k}{k!f(z)}\brac{1+O\bfrac{k^2+1}{vz}}
\eeq
{\bf (b)} For all 

\end{lemma}
\proofend
\section{Differential Equations}\label{diffeqs}
Let  be the current parameter tuple and  be the tuple after one step of the Karp-Sipser algorithm. The following
lemma gives  for each step of Phase 1 of the Karp-Sipser algorithm.
\begin{lemma}\label{change}
Assuming  and  we have

\end{lemma}
\begin{proof}
  First note that , one vertex  with  will
  be picked and  and its neighbor  will be removed from
  . This implies that  decreases by  and the number of
  edges removed is , i.e. all edges incident to . Let  be
  the number of multiple edges incident to . Then we have

where  has distribution \eqref{eq0}.

{\bf Explanation:}
The  choices of neighbours for the remaining vertices in  form a list with  unique names and  non-unique
names and where the number of times a vertex appears among the  has distribution \eqref{eq0}. Also, if we construct this 
list vertex by vertex, it will appear in a random order. So the probability that  appears in two of the choices for 
 is bounded by  and this justifies \eqref{eq1}.

The change in  comes from  being removed, minus the number of 
other degree one vertices adjacent to  and plus the number of vertices
adjacent to  of degree exactly two. Any change from vertices of degree three
or more is absorbed by the  term for multiple edges.

The expected change is then


Similarly for , the change is only due to vertices adjacent to  of degree exactly two, modulo multiple edges. Thus

\end{proof}
\\\
Lemma \ref{change} suggests that we consider the following pair of differential equations

where  and  satisfies 
\beq{13}
\frac{\z(e^\z-1)}{f(\z)}=\frac{dw-y_1}{y}.
\eeq
The boundary conditions are (see \eqref{begin})
\beq{bound}
\z(0)=\a d,\;y_1(0)= m \a de^{-\a d},\;y(0)= m(1-(1+\a d)e^{-\a d}).
\eeq
The  are of course the deterministic counterparts of  respectively.

\begin{lemma}\label{lemde}
The solution to \eqref{dv1}, \eqref{dv} and \eqref{bound} is

\end{lemma}
\begin{proof}
We take the derivative of \eqref{zeqn} with respect to .
The RHS becomes, using \eqref{dv1} and \eqref{dv}
 

On the other hand, on differentiating the LHS of \eqref{zeqn} (with  replaced by ) we get


Comparing \eqref{a} and \eqref{b} we see that


Integrating yields


Plugging in  we see that 

This verifies \eqref{zw} and \eqref{zw1}.

Going back to \eqref{dv} and \eqref{zweqn} we have 

So 
 
and integrating yields

Taking  gives 
and we see that \eqref{v-form} holds.

{\large
{\bf
\ignore{
Using \eqref{v-form} in \eqref{dv1} we get

Dividing through by  we get

and dividing by  and collecting  on LHS gives

Integrating with respect to  and pluggin in  gives

Do we need to derive \eqref{v1-form}? It is not used for the next derivation.
}
}
}

We now solve for  in terms of  as a function of . It follows from \eqref{13} and 
\eqref{v-form} that

\end{proof}

At this point we wish to show that \whp\ the sequence  closely follows the trajectory

described in Lemma \ref{lemde}. One possibility is to use Theorem 5.1 of Wormald \cite{Wormald}, but there
is a problem with an ``unbounded'' Lipschitz coefficient. 
One can allow for this in \cite{Wormald}, but it is unsatisfactory to ask the reader to 
check this. We have decided to use an approach suggested in Bohman \cite{r3t}. 

Next let  be a large positive constant and let .
Then let

and

Then define the event

where 
\beq{t1}
t_1=\min\set{t>0:\;y_1(t)=0}.
\eeq
Now define four sequences of random variables: 

Because  is convex we have 

for  and . So,
\beq{gt}
g((t+1)/n)-g(t/n)\geq \frac{K}{n-t}\,g(t/n).
\eeq

Suppose that  holds. We write

(For this we need . But this follows from \eqref{13} and the fact that  is decreasing -- see \eqref{zweqn}).

Putting  
we have (see \eqref{b})  and since
\beq{diff}
(e^x-1)^2-x^2e^x=\sum_{k=4}^\infty(2^k-2-k(k-1))\frac{x^k}{k!}
\eeq
we see that  in any bounded interval . 

Hence from \eqref{rat} we have

or
\beq{zzz}
|\z-z|=O\bfrac{Err(t)}{v}.
\eeq
Using \eqref{zzz} we obtain

for some .

For the second term we use 
 
This implies that  for .

Now with ,

If  holds then, where ,


This shows that  is a sub-martingale. Also,



It follows from the Azuma-Hoeffding inequality that we can write

By almost identical arguments we have

It follows that \whp, when  holds, we have

Now by construction,  will fail at some time . It follows from \eqref{tite1}, \eqref{tite2} that \whp\ 
it will fail either because (i)  or . We claim the latter.
Observe that if  then \eqref{zw}--\eqref{sad} imply  and
. Together with \eqref{tite1}, \eqref{tite2}, this implies that .

In summary then, \whp\, the process satisfies
\beq{u1}
|\vv(t)-\yy(t)|_\infty<2Err(t_2)=O\bfrac{n^{2/3}}{(1-t_2/n)^K}=O(n^{2/3+Kd\g/(d-1)}) \qquad\qquad for\ 1\leq t\leq t_2
\eeq
and
\beq{u2}
v_1(t_2)=O(n^{1-\g})\text{ where }t_2=t_1+O(n^{1-\g}).
\eeq
We use \eqref{zw} for \eqref{u1} and \eqref{sad}, \eqref{zzz} for \eqref{u2}.


\ignore{
\section{Growth of }
When  is small, we see that \eqref{dv1} becomes

after substituting for  using \eqref{actual}.

This is positive for small . This is because our differential equations are only good for Phase 1.
In Phase 2, vertices on the left lose edges. I think we observed this already. To handle Karp-Sipser in
Phase 2 we need to keep track of the number of vertices on the left with degree  for . So we
need  variables  and  equations where the extra two equations are needed to keep track of the growth of
 and .
}
\section{Analysis of Phase 1}\label{ap1}
We will first argue that \whp\ Phase 1 is sufficient to find a matching from  to  when there is no solution  to 
\beq{p1e}
\bfrac{\z}{\a d}^{\frac{1}{d-1}} + e^{-\z}-1=0.
\eeq
It follows from \eqref{sad}, \eqref{t1} and \eqref{u2} that in this case Phase 1 ends with there being at most
 vertices of  left unmatched, \whp. Furthermore at time  we will have

where .
\begin{lemma}\label{lemP1}
Suppose that . Then \whp\ at time ,  is a forest.
\end{lemma}
\begin{proof}
Let  denote the set of vertices of degree at least two in
the -side of . 
Let  denote  where  are truncated Poisson conditioned
only to sum to . For large  we use the bound 
\beq{b1}
\Pr(X_1=d_1,\ldots,X_k=d_k)\leq O(n^{1/2})\prod_{i=1}^k\frac{z^{d_i}}{d_i!f(z)}.
\eeq
For  and  we write
\beq{b2}
\Pr(X_1=d_1,\ldots,X_k=d_k)=\prod_{i=1}^k\Pr(X_i=d_i\mid X_j=d_j,j<i)=(1+o(1))\prod_{i=1}^k\frac{z^{d_i}}{d_i!f(z)}.
\eeq
It is equation \eqref{eq0} that alows us to write the final equality in \eqref{b2}. The extra conditioning
 only changes the required sum.

Thus let  denote  for  and  otherwise.
The expected number of cycles can be bounded by  plus

\end{proof}

{\bf Explanation of \eqref{cycles}:} We condition on the degree sequence. 
Having fixed the degree sequence,
we swap to the configuration model \cite{BoCo}. Having chosen  and  vertices  in  and their
degrees, we can work within tnis model. We then choose a cycle through these vertices in  ways.
We then choose the configuration points associated with our -cycle in 
 ways. We then multiply by the probability 
of choosing the pairings associated with the edges of the cycle.
\begin{corollary}\label{cor1}
Suppose that . Then \whp\ at time ,  contains a matching from  into .
Furthemore, such a matching will be constructed in Phase 1.
\end{corollary}
\begin{proof}
We can assume from Lemma \ref{lemP1} that  is a forest. Each vertex of  has degree
 and so Hall's theorem will show that the required matching exists. (Any Hall witness would induce a cycle).
Finally note that Phase 1 of the Karp-Sipser algorithm is exact on a forest.
\end{proof}
\subsection{Threshold for Phase 1 to be sufficient}
Put  and  so that \eqref{p1e} can be written as
\beq{za}
A\z^B + e^{-\z}-1=0.
\eeq
Assume  is fixed. We find a threshold for  in terms of  for there to be no positive solution to \eqref{za}. 
We find the place  where the curve
 touches the curve  i.e. where

In which case
\beq{mm0}
\frac{\z^*}{B}=e^{\z^*}-1\text{ and }A^*=\frac{1-e^{-\z^*}}{(\z^*)^B}
\eeq
or
\beq{zaaz}
\z^*=\frac{e^{\z^*}-1}{d-1}\text{ and }\a^*=\frac{\z^*}{d(1-e^{-\z^*})^{d-1}}.
\eeq
In general, keeping  fixed let 

We must show that if  then the only solution to  is .

Observe that  and . Also, if  then  and if  then 
and there must be a positive solution to .

Observe that  implies 
(i)  and that (ii)  for . So if  has no positive solution
then neither has . We argue that  has at most 2 solutions, which implies that  has at most two 
positive solutions. As we increase  to  these solutions must converge, by (ii).

Now 

But the function  is convex for any . Indeed

and so  has at most two solutions for any .
\subsection{Finishing the proof of Theorem \ref{th1}}
We now have to relate the above results to the actual process. We know from our analysis of the differential equations that
for some ,

When , Lemma \ref{lemP1} and Corollary \ref{cor1} imply Theorem \ref{th1}.

So assume that . Thus  and .
We argue that if  is the solution to \eqref{za} then  decreases monotonically with .
Indeed, if  then  for .
Now  is strictly monotone decreasing with  and so 
\beq{mm1}
\frac{(d-1)z_A}{e^{z_A}-1}<\frac{(d-1)z_{A^*}}{e^{z_{A^*}}-1}=1.
\eeq
The second equation in \eqref{mm1} is the first equation in \eqref{mm0}.

At time  we will have  and . For the next  steps
we have from \eqref{v1cc} that
\beq{mm2}
\E(v_1'-v_1\mid \vv)=-1+(1+o(1))\frac{d-1}{dw}\frac{z^2}{f}v=-1+(1+o(1))\frac{(d-1)z_A}{e^{z_A}-1}\leq -\e
\eeq
for some small positive . In which case, \whp,  will become zero in  steps.
Indeed \eqref{mm2} implies that the sequence

is a supermartingale that cannot change by
more than  in any step. The Azuma-Hoeffding inequality implies that for  we have

 
I.e. \whp\  and .
\section{Proof of Theorem \ref{th2}}\label{pot2}
Let us summarize what we have to prove.
We have a random bipartite graph  with partition  and .
Each vertex in  has degree  and each vertex in  has degree at least 2.
At this point it is convenient to drop the suffix 1. So from now on,  etc. refer to the graph left at the end of Phase 1.

The degrees of  satisfy,  for . The degrees of vertices in  are distributed as the 
box occupancies  in the following experiment. We throw  balls randomly into  boxes and condition that 
each box gets
at least two balls.  In these circumstance the 's are independent truncated Poisson, subject to the 
condition that , see Lemma \ref{trunp} with . 
Thus for any  and any set of positive integers  we have 
 
for 
 where  satisfies 

The  term accounts for the conditioning 
We will prove
\begin{theorem}\label{min}
Let  be a bipartite graph chosen uniformly from the sets of graphs with bipartition , 
such that each vertex of  has degree  and each vertex of  has degree at least two. Then \whp

\end{theorem}
\subsection{Useful Lemmas}
Define the function  to be the unique solution to 

Let  be defined by

Observe that
\beq{gz}
\frac{f(\z(x))}{\z(x)^x}=\frac{g(x)}{x^x}.
\eeq
\begin{lemma}\label{use1}
The function  is log-concave as a function of 
\end{lemma}
\begin{proof}
We will write  for  and  for  throughout this proof. Now  from
which we get 
\beq{dzx}
\frac{d\z}{dx} = \frac{f^2}{(e^\z-1)^2-\z^2e^\z}
\eeq
and note that  for . Taking the derivative of  we get

Now  so 

Thus we have
\beq{gdash}
\frac{d}{dx}\log(g(x)) = \log\bfrac{e^\z-1}{f}+1
\eeq

Taking the second derivative we get

and since  is strictly negative for  we get that  is log-concave
\end{proof}
\begin{lemma}\label{use2}
 is concave as a function of .
\end{lemma}
\begin{proof}
We begin with \eqref{dzx}. We note from \eqref{diff} that the denominator

Then we have

Now let

where

We check that  and  which implies that . One can finish the argument by checking that
 
for . This is simply a matter of checking for small values until the  term dominates.
\end{proof}
Next let 

\begin{lemma}\label{use3}
 is convex as a function of .
\end{lemma}
\begin{proof}

Let

Direct computation gives  and for  

One can then check that  for . Thus 
implying that  is convex.
\end{proof}
\subsection{The case }\label{m=n}
We will first prove Theorem \ref{min} under the assumption that  and then in Sections \ref{gre} and \ref{le} we will extend
the result to arbitrary . We will as usual prove that Hall's Condition holds \whp. We will therefore estimate the 
probability of the existence of sets  where  and  such that . Here  
is the set of neighbours of  in . We call such a pair of sets, a {\em witness} to the non-existence of a perfect matching.
There are two possibilities to consider: (i)  and  or
(ii)  and . We deal with both cases in order to help extend the results to . We observe
that if there exist a pair  then there exist a minimal pair and in this case each  has at least two neighbours in .
We deal first with the existence probability for a witness in Case (i) and leave Case (ii) until Section \ref{AinR}. We then 
combine these results to finish the case  in Section \ref{combine}. We will deal computationally with {\em minimal witnesses} 
where each vertex in  has at least 2 neighbours in . If  has a unique neighbour  in  then 
 is also a witness. 
\subsubsection{Case 1}\label{Case1}
We estimate


{\bf Explanation of \eqref{expl0}:}
Choose sets  in   ways. Choose degrees  with probability 
 such that  for some .
Choose the degrees  in the sub-graph induced by . Having fixed the degree sequence,
we swap to the configuration model. Choose the configuration points associated
the  in  ways. Assign these  choices of points points associated with  in  ways.
Then multiply by the probability  of a given pairing of points in .

{\bf Explanation of \eqref{3u} to \eqref{4u}:} If  where  for  
then  for any positive  and  is minimised at  satisfying 
.



For the remainder of Section \ref{m=n} we assume that 
\beq{46a}
n\leq m\leq n+o(n^{7/8}).
\eeq
In which case we have


Thus\eqref{1aa} becomes
\beq{1a}
\p_L(k,\ell,D)=O\bfrac{1}{n^{1/2}}e^{o(n^{7/8}a)}h(a)^{(d-2)n}
\brac{\frac{z^d}{f(z)}\frac{f(\z_1)}{\z_1^{d-x}}
\bfrac{e\frac{a}{1-a}}{x}^x}^{n-k}.
\eeq
{\bf Case 1.1:} .

Observe (see \eqref{gz}) that

where  is as defined in Lemma \ref{use1}.

It follows from \eqref{gdash} that
\beq{int}
-\log\bfrac{g(d-x)}{g(d)} = \int_{d-x}^d\frac{d}{dt}\log(g(t))dt \ge \int_{d-x}^d(\log(1+\z e^{-\z})+1)dt.
\eeq
Now  which implies that . Also,

And so . Thus

This implies that  where  and . 
Note that  and so  in the range of interest.
Plugging this into the last parenthesis of \eqref{1a} gives
\beq{1b}
\p_L(k,\ell,D)=O\bfrac{1}{n^{1/2}}e^{o(n^{7/8}a)}
\psi(x)^n\brac{h(a)^{d-2}\brac{d^d\frac{1}{(d-x)^{d-x}}\bfrac{\frac{a}{1-a}}{x}^x}^{1-a}}^n
\eeq
This immediately yields
\beq{A0}
A_{\ref{A0}}=\sum_{\ell<k=\e_Ln}^{n(1-2/d)}\sum_{D=dk}^{dk+n^{1/10}}
\p_L(k,\ell,D)\leq\sum_{\ell<k=\e_Ln}^{n(1-2/d)}\sum_{D=dk}^{dk+n^{1/10}}
O\bfrac{\log n}{n^{1/2}}h(a)^{(d-2)n}e^{o(n)}=o(1).
\eeq
We use the notation  so that the reader can easily refer back to the equation giving its definition.

We will work with  because it is easy to show that \whp\ the maximum degree in  is .
The bound for  comes 
from \eqref{1b}, using the fact that  is bounded away from 1 and  in this summation.
 is the first of several sums that 
together show the unlikelihood chance of a witness. We will display them as they
become available and use them in Sections \ref{combine}, \ref{gre} and \ref{le}.

The main term  
in \eqref{1b} is maximized when , provided  or . This in turn gives


The function  is at most 1 and is log-convex in  on . 
Indeed, if  then

We have  and . It follows that for every  there exists a constant  such that 
\beq{43}
\r_d(a)\leq e^{-Ka}\qquad for\ a\leq \e_L(K).
\eeq 
We let .

We can immediately write
\beq{A1}
A_{\ref{A1}}=\sum_{\ell<k=2}^{n^{1/10}}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)=
\sum_{\ell<k=2}^{n^{1/10}}\sum_{D=dk}^{\log n}O\bfrac{\log n}{n^{1/2}}e^{o(kn^{-1/8})}=o(1).
\eeq
The bound for  is derived from \eqref{42} using . 

Along the same lines we have
\beq{A2}
A_{\ref{A2}}=\sum_{\ell<k=n^{1/10}}^{\e_Ln}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)=
\sum_{\ell<k=n^{1/10}}^{\e_Ln}\sum_{D=dk}^{k\log n}O\bfrac{\log n}{n^{1/2}}
e^{-k(1-o(n^{-1/8})}=o(1).
\eeq
The bound for 
comes from \eqref{42} and \eqref{43}. 

Now
 decreases in  and is  for .
So if  then
\beq{A3}
A_{\ref{A3}}=\sum_{\ell<k=\e_Ln}^{n(1-2/d)}\sum_{D=dk+n^{1/10}}^{\log n}\p_L(k,\ell,D)=
\sum_{\ell<k=\e_Ln}^{n(1-2/d)}\sum_{D=dk+n^{1/10}}^{k\log n}
O\bfrac{\log n}{n^{1/2}}e^{o(n^{7/8}a)}\psi(n^{-4/5})^n\r_d(a)^n=o(1).
\eeq
The bound for  comes from \eqref{42} using the fact that  and  in this summation. 

When  we need some extra calculations. First note that  and so arguing as above we have
\beq{A33}
A_{\ref{A33}}=\sum_{\ell<k=\e_Ln}^{.15n}\sum_{D=3k+n^{1/10}}^{k\log n}\p_L(k,\ell,D)=
\sum_{\ell<k=\e_Ln}^{.15n}\sum_{D=3k+n^{1/10}}^{\log n}O\bfrac{\log n}{n^{1/2}}e^{o(n^{7/8}a)}\psi(n^{-4/5})^n\r_3(a)^n=o(1).
\eeq
Because we can choose any value for  in the bound \eqref{1a} we can simplify matters by choosing  
independent of  to get
\beq{1a1a}
\p_L(k,D)=O\bfrac{1}{n^{1/2}}e^{o(n^{7/8}a)}h(a)^{n}
\brac{\frac{z^3}{f(z)}\frac{f(\xi)}{\xi^{3}}
\bfrac{e\xi a}{x(1-a)}^x}^{n-k}.
\eeq
Now 
\beq{xxq}
\bfrac{\xi ea}{(1-a)x}^x\leq \exp\set{\frac{\xi a}{1-a}}
\eeq
and so
\beq{are}
\p_L(k,D)\leq O\bfrac{k}{n^{1/2}}\brac{h(a)e^{\xi a}e^{o(n^{-1/8}a)}\brac{\frac{z^3}{f(z)}\frac{f(\xi)}{\xi^3}}^{1-a}}^n.
\eeq
Now the function  is log-convex. Our choice of
 will be 1.5 and we note that with this choice
 and so
\beq{A333}
A_{\ref{A333}}=\sum_{\ell<k=.15n}^{2n/5}\sum_{D=3k+n^{1/10}}^{k\log n}\p_L(k,\ell,D)\leq
\sum_{\ell<k=.15n}^{2n/5}\sum_{D=3k+n^{1/10}}^{k\log n}O\bfrac{\log n}{n^{1/2}}e^{o(n^{7/8}a)}(.98)^n=o(1).
\eeq
We have gone slightly beyond  to . It is convenient to repeat this idea for a couple of ranges. Putting 
we get  from which we deuce that
\beq{A3333}
A_{\ref{A3333}}=\sum_{\ell<k=2n/5}^{.74n}\sum_{D=3k+n^{1/10}}^{k\log n}\p_L(k,\ell,D)\leq
\sum_{\ell<k=2n/5}^{.74n}\sum_{D=3k+n^{1/10}}^{k\log n}O\bfrac{\log n}{n^{1/2}}e^{o(n^{7/8}a)}(.995)^n=o(1).
\eeq
Putting 
we get  from which we deuce that
\beq{bbx}
A_{\ref{bbx}}=\sum_{\ell<k=.74}^{.87n}\sum_{D=3k+n^{1/10}}^{k\log n}\p_L(k,\ell,D)\leq
\sum_{\ell<k=.74}^{.87n}\sum_{D=3k+n^{1/10}}^{k\log n}O\bfrac{\log n}{n^{1/2}}e^{o(n^{7/8}a)}(.995)^n=o(1).
\eeq
{\bf Case 1.2.1:} .

For  the maximising value for  in \eqref{1b} is 
at  (recall that , so plugging into \eqref{1b} gives



Let . Then

Assume for now that .
Then the derivative with respect to , for of the last expression is

so it takes a minimum at  with a value

Now  and so for  and  we have
\beq{A4}
A_{\ref{A4}}=\sum_{\ell<k=n(1-2/d)}^{n-n^{7/8}}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)
\leq \sum_{\ell<k=n(1-2/d)}^{n-n^{7/8}}\sum_{D=dk}^{k\log n}
O\bfrac{\log n}{n^{1/2}}e^{-.18(n-k)+o(n^{-1/8}k)}=o(1).
\eeq

For  we use the following
\begin{claim}\label{claim:fz}
For  we have .
\end{claim}

Substituting this into \eqref{1a} gives
\beq{2b}
\p_L(k,\ell,D)=O\bfrac{1}{n^{1/2}}\brac{h(a)^{d-2}
\brac{e^{o(n^{-1/8}a)}\frac{z^d}{f(z)}\frac{3}{4}\bfrac{e\frac{a}{1-a}}{x}^x}^{1-a}}^n
\eeq

The maximum of  
is taken when either  for  or at  
for .

So for  we get

The expression  is log-convex on  
and  at both ends of the interval for both .
We can therefore write
\beq{A4'}
A_{\ref{A4'}}=\sum_{\ell<k=n(1-2/d)}^{n(1-1/(d-1))}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)
\leq \sum_{\ell<k=n(1-2/d)}^{n(1-1/(d-1))}\sum_{D=dk}^{k\log n}
O\bfrac{\log n}{n^{1/2}}e^{o(n^{7/8}a)}(.97)^n=o(1).
\eeq
for .

{\bf Case 1.2.2:} .

For  from \eqref{2b} we get

The expression  
is log-concave on .  
The derivative of  at  is at least 1/100 for both . Consequently for 
\beq{A4''}
A_{\ref{A4''}}=\sum_{\ell<k=n(1-1/(d-1))}^{n-n^{7/8}}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)\leq \sum_{\ell<k=n(1-2/d)}^{n-n^{7/8}}\sum_{D=dk}^{k\log n}
O\bfrac{\log n}{n^{1/2}}e^{-(n-k)/100+o(n^{-1/8}k)}=o(1).
\eeq
for .

For  we go back to \eqref{1a1a} and \eqref{xxq} and put  giving
\beq{less1}
\p_L(k,D)\leq O\bfrac{1}{n^{1/2}}\brac{e^{o(n^{-1/8}a)}h(a)\brac{\frac{z^3 }{f(z)}\frac{ea^3f((1-a)/a)}{(1-a)^3}}^{1-a}}^n.
\eeq
Now for  we have

Plugging this into \eqref{less1} for  and replacing  we have
\beq{less2}
\p_L(k,\ell,D)\leq O\bfrac{1}{n^{1/2}}\brac{e^{o(n^{-1/8}a)}a^a\brac{\frac{ez^3}{2f(z)}}^{1-a}}^n.
\eeq
The function  is log-convex and  and . 
Also, . It follows that
\beq{A4'''}
A_{\ref{A4'''}}=\sum_{\ell<k=.84n}^{n-n^{7/8}}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)\leq \sum_{\ell<k=.84n}^{n-n^{7/8}}\sum_{D=dk}^{k\log n}
O\bfrac{k\log n}{n^{1/2}}e^{-\min\set{(n-k)/20,n/1000}+o(n^{-1/8}k)}=o(1).
\eeq
\begin{proofof}{Claim \ref{claim:fz}}
Recall from Lemma \ref{use2} that  is concave and thus 
is decreasing. Since  we have

Thus we have that  for all . For  we can upper bound

For  we have that  and so we can bound

Taking the logarithm of this expression and substituting  we get
\beq{2bb}
\log f(u)-\brac{\frac{u}{3}+2}\log u = u - \frac{u}{3}\log(u) + \brac{\log f(u)-u-2\log(u)}=u - \frac{u}{3}\log(u) + H(u)
\eeq
where  is from Lemma \ref{use3}.

Since  is convex we have

Pluggin this into \eqref{2bb} we get

which is concave in  and takes a maximum value of  when

Pluggin this back in we see that for , which is  we have

\end{proofof}


\subsubsection{Case 2}\label{AinR}
Now let us estimate the probability of a violation of Hall's condition with . We once again begin with arbitrary .
Let

where ,  and .

{\bf Explanation of \eqref{expl1}:}
Choose sets  in   ways. Choose degrees  with probability 
 such that  for some .
Choose the degrees  in the sub-graph induced by . Having fixed the degree sequence,
swap to the configuration model \cite{BoCo}. Choose the configuration points associated with
the  in  ways. 
Then multiply by the probability  of a given pairing of points in .

We assume that \eqref{46a} holds
for the remainder of the section. In which case we have

Thus,
\eqref{3ayxw} becomes
\beq{3ayx}
\p_R(k,\ell,D)\leq O\bfrac{1}{n^{1/2}}\brac{e^{o(n^{-1/8}a)}
\frac{h(\th a/d)^d}{h(a)^2h(\th/d)^{ad}}\frac{z^d}{f(z)}\frac{f(\z_1)^a}{\z_1^{\th a}}
\frac{f(\z_2)^{1-a}}{\z_2^{d-\th a}}}^n
\eeq

\ignore{{\bf {\Large Mathematica says that  works for ,
provided .\\
Note that
.\\
For a fixed  the function  is log-concave for  and log-convex for
. The function  for .
}}}

It follows from Lemma \ref{use1} that we can upper bound



Plugging this into \eqref{3ayx} gives

\beq{4x}
\p_R(k,\ell,D)\leq O\bfrac{1}{n^{1/2}}\bfrac{e^{o(n^{-1/8}a)}a^{a\th}(1-a)^{d-a\th}}{h(a)^2h\bfrac{\th}{d}^{ad}}^n
\eeq
Now let . Then



Thus  is concave and is maximized when . Because  we can only use this for 
. 

{\bf Case 2.1:} .


where the function  is defined following \eqref{42}.

We find that 
\beq{d=5}
\r_d(1-2/d)=\brac{\frac{d^4}{16}\brac{1-\frac{2}{d}}^{(d-2)^2}}^{1/d}\leq .9\text{ for }d\geq 5.
\eeq
Now  for  and . So, with the aid of \eqref{43},

We will treat  and  under Case 2.2.

{\bf Case 2.2:} .

In this case the expression in \eqref{4x} (ignoring error terms) is maximized at . Then

Let . Then


Thus  is strict concave and its maximum is taken at  and  for all .
Furthermore,  for . It follows that if  then

For  we use a better bound on  in \eqref{colbert}. 

{\bf Case 2.2a: .}


Replacing the  factor in \eqref{4x} which comes from  gives, for d=5,


Let  for any , note that if  then

and so  is maximized when  or  and the maximum value is 

Thus from \eqref{6b} we get 

Let . Then


So  is log-convex on . We have  and 
and . It follows that

{\bf Case 2.2b: .}



Now if  then

Thus  is log-convex on . We have  and 
and . It follows from this and \eqref{43} that

{\bf Case 2.2c: .}



Now if  then

{\bf Case 2.2c(i):} .\\
Thus  is log-concave on  and log-convex on . 
We have , 
and  and  and . It follows that

Now let us consider . 

{\bf Case 2.2c(ii):} .\\
(a)  and .\\
We go back to \eqref{3ayx} and make the choice  and replace
 by  and 
consider the function 

so that . Let . Then

It follows from \eqref{daa} that 
\beq{isconvex}
G_1(\th,a)\text{ is a convex function of }a\text{ for }0\leq a\leq a_\th=\frac{3\th-6}{\th},\,\text{for  fixed,\,}
\eeq
and 
\beq{isconcave}
G_1(\th,a)\text{ is a concave function of }a\text{ for },\,\text{for  fixed,\,}. 
\eeq
It follows from \eqref{partial} that
\beq{isisconcave}
G_1(\th,a)\text{ is a concave function of  on  for  fixed, }. 
\eeq
A calculation shows that if  then

Furthermore, if  then
\beq{short}
g_2(\th)=\log\bfrac{12}{3^\th(3-\th)}.
\eeq
(a)  and .\\
For  we have . So,
\beq{smalla}
F_1(\th,a)\leq e^{-2a}\text{ for }0\leq a\leq e^{-10000},\,2.0005\leq \th\leq 3.
\eeq

(b)  and .\\
Now  for  and so \eqref{isconvex} implies that 
for  and .
Now \eqref{smalla} implies
that  and \eqref{mmm} implies that  
and so \eqref{isisconcave} implies that  for . Also,
by direct calculation, we have
 and  and so
 for . Thus,


(c)  and .\\
We take  and  in \eqref{3ayx} and let

where 


Let . 
and 
and so \eqref{isconvex},\eqref{isconcave} and \eqref{isisconcave} hold with  replaced by . 
Putting  we see that ,
using \eqref{long} ().
Thus  is convex on . Furthermore  and so  for 
and therefore  when  and .
Next let . We have  
for , using \eqref{short} (). So,
 for  when . Thus 


Now suppose that we repeat the idea of the previous paragraph, but this time we take  and  in \eqref{3ayx}
and use the same notation. Putting  we see that ,
using \eqref{long} ().
Thus  is convex on . Furthermore  and so  for 
and therefore  when  and .
Next let . We have  
for , using \eqref{short} (). So,
 for  when . Thus 


(d)  and .\\ 
For this we simplify our estimate of  by removing some terms involving  from \eqref{3ayxw}.

Now let
 
We take  and then 
 
for . 
Keeping some slack, we define 

and .
Now let . We have  
and  and we find from
\eqref{long} that  is concave on . Furthermore  and 
using \eqref{loong} we see that  and so  for .
So  for .
Next let . We see from \eqref{short} that  for  and thus 
 for  when . 
Replacing  by  in the definition of  we get 
for  when
.
So, for some small constant ,
\beq{B4y}
B_{\ref{B4y}}=\sum_{\ell<k=2}^{.51n}\sum_{D=2k}^{3k}\p_R(k,\ell,D)\leq
\sum_{\ell<k=2}^{.51n}\sum_{D=2k}^{3k}e^{-ck}=o(1).
\eeq
\subsubsection{Finishing the case }\label{combine}
We repeat our observation that the maximum degree  in  is  \whp. Therefore

{\bf Case 1:} .

where the  term accounts for .
We use  to account for witnesses  with . 
This is because if  and  then  and  and 
and there will be a minimal witness  with .

{\bf Case 2:} .

We point out for use in the next section that our computations allow us 
to claim that we have
\beq{next}
\sum_{\substack{k=n^\c\\ \ell\leq\min\set{k-1,m/2}}}^{n-n^\c}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)=O(e^{-\Omega(n^\c)}).
\eeq

Our computations also allow us to claim that
\beq{next1}
\sum_{\substack{k=n^\c\\ \ell\leq\min\set{k-1,n/2}}}^{n-n^\c}\sum_{D=2k}^{dk}\p_R(k,\ell,D)=O(e^{-\Omega(n^\c)}).
\eeq

\subsection{The case }\label{gre}
Let  denote the set of bipartite graphs with  that are -regular on  and 
degree at least 2 on . Here . In fact suppose first that  where  is a constant.
Suppose that  is chosen uniformly at random from .

If there is no matching from  to , then let a minimal witness  be {\em small} if  
and {\em large} if  and {\em medium} otherwise. 
\subsubsection{Small/Large Witnesses}\label{smallw}
We go back to \eqref{1aa}. We see that  implies that the term 
 is maximised over  when . Next let  then  and 
. Thus  is log-convex in  and so 
\beq{hab}
h(a/\b)^\b\geq \exp\set{H(1)+H'(1)(\b-1)}=h(a)(1-a)^{\b-1.}
\eeq
Going back to \eqref{42} we see that now we have
\beq{71}
\p_L(k,\ell,D)\leq O\bfrac{1}{n^{1/2}}\bfrac{\r_d(a)}{(1-a)^{\b-1}}^n.
\eeq
By taking  in place of  we can take  in \eqref{43} and plugging this into \eqref{71}
we see that
\beq{noo5}
\sum_{\ell<k=2}^{n^\c}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)\leq O\bfrac{1}{n^{1/2}}\bfrac{e^{-\b a}}{(1-a)^{\b-1}}^n=o(1).
\eeq
To deal with  we treat this as  in Section \ref{AinR}. Indeed, if there is such 
a witness , let  and . Then  and  and so we can 
find a witness  with  and .


We use \eqref{3axxx} for this calculation. Now

So from \eqref{3axxx} we can write
\beq{noo1}
\p_R(k,\ell,D)\leq O\bfrac{1}{m^{1/2}}\brac{\bfrac{az}{\z_1}^{\th-2}\exp\set{1+\frac{1}{\b}-(d-\th)\log(1-\th/d)-\th+O(a)}
\frac{f(\z_1)}{\z_1^2} \frac{z^2}{f(z)}}^{a n}.
\eeq
Now we claim that 
\beq{zeta1}
\z_1^{\th-2}\geq \frac12\text{ and that }f(x)x^{-2}\text{ is monotone increasing in }x.
\eeq

First notice that 
which is clearly monotone increasing. Second note that 
 and since  we have

and since  is concave 
we have . This, along with , implies that . We can then lower bound


Using this we see from
\eqref{noo1} that if

then

In which case we have
\beq{ps1}
\sum_{\ell<k=2}^{n^{\c}}\sum_{\th\geq \th_0}\p_R(k,\ell,D)\leq O\bfrac{1}{m^{1/2}}e^{-k}=o(1).
\eeq
When  we have . Therefore
\beq{bor}
\p_R(k,\ell,D)\leq O\bfrac{1}{m^{1/2}}\bfrac{z^2e^{-(d-2)\log(1-2/d)+o(1)}}{2f(z)}^k.
\eeq
Now for  we have
\beq{noo2}
\frac{z^2e^{-(d-2)\log(1-2/d)+o(1)}}{2f(z)}\leq \frac{9}{10}
\eeq
and so
\beq{noo3}
\sum_{k=1}^{n^{\c}}\sum_{\th\leq \th_0}\p_R(k,\ell,D)\leq O\bfrac{1}{m^{1/2}}\bfrac{9}{10}^k=o(1).
\eeq
When , the expression on the LHS of \eqref{noo2} is at most 1.26. So in this case we go back to 
\eqref{3axxx} and replace  by . After this
\eqref{11} is replaced by

And then \eqref{bor} is replaced by

and so
\beq{noo4}
\sum_{k=1}^{n^{\c}}\sum_{\th\leq \th_0}\p_R(k,\ell,D)\leq O\bfrac{1}{m^{1/2}}\frac{1}{2^k}=o(1).
\eeq
\subsubsection{Medium Witnesses}
Let  denote the number of -vertices of degree  in  and let .

We define three events: 
We argue next that if  then
\beq{sud}
\Pr(\cA(n,m)\cup \cB(n,m))=e^{-\Omega(\log^2n)}.
\eeq

For any  we have

where .

We will now use the following bounds (see for example \cite{AS})

If  then we can use \eqref{cher1} with  to deal with  and also with .
If  then . We can therefore use \eqref{cher2} with  to deal with 
. This concludes the proof of \eqref{sud}.

Now consider a set of pairs . We place  into  if  is obtained from 
in the following manner: Choose a vertex  of degree at least four in . 
Suppose that its neighbours are 
in any order. 
To create  we (i) replace  by two vertices  and  and then (ii) 
let the neighbours of  in  be  and let the neighbours of  be .

For  let

and for  let

We note that if
 then
\begin{itemize}
 \item  implies that .
\item  for all .
\item  implies that .
\item  for all .
\end{itemize}
We then note that
\ignore{

}

Now let  be properties such that if  and  then . Let 
be chosen uniformly from  and let  denote probabilities computed w.r.t. this choice. Then

and

So,

So,

So, if  is a property of  for , 
\beq{x1}
\frac{|\cP_m|}{\cG(n,m)}\leq (1+O(n^{-2/5}))^{m-n}\frac{|\cP_n|}{\cG(n,n+n^{4/5})}.
\eeq
We use \eqref{x1} in the following way: First let  be the property that  
contains a minimal witness  with .
If  and  then . 
Indeed  is a witness in .
Applying \eqref{x1} and \eqref{next} we see that 
\whp\  fails to occur. 
Now let  be the property that  contains a minimal
witness  with . 
If  and  has a 
witness  with  and  
then . Indeed  is 
also a witness in . Now if  with a witness
 then  is a witness in  and so contains 
a minimal witness  where 
 i.e. .
Applying \eqref{x1} and \eqref{next1} we see that 
\whp\  fails to occur. 
This deals with medium witnesses.


It only remains to consider  close to  i.e. where  defined at the beginning of this section 
is close 1/2. Observe first that the number of edges incident
with vertices of degree greater than two is at most .
If there are  vertices of degree  then  and  which implies that .
So the number of edges incident with vertices of degree greater than two is at most .

Now consider a witness  where . We must have
 which implies that  which can be made arbitrarily small.
Now the estimate in \eqref{71} will suffice up to  and so we only need to make  close enough to 1/2 so that
 (which depends only on  and not ). 
\subsection{The case }\label{le}
We once again consider medium witnesses separately from small or large witnesses.
\subsubsection{Small/Large Witnesses}\label{smallww}
We first go back to \eqref{3axxx} and deal with  for  as we did in Section \ref{gre}. 
For  we deal with  for . We will go back to \eqref{1aa} and write 

Now  implies that 

Also,  implies that . Therefore,

Arguing as for \eqref{71} we get

Taking  as in \eqref{A2} and noting that  here we get
Thus
\beq{noo7}
\sum_{k=1}^{n^\c}\sum_{D=dk}^{k\log n}\p_L(k,\ell,D)\leq \sum_{k=1}^{n^\c}\sum_{D=dk}^{k\log n}
O\bfrac{1}{n^{1/2}}\brac{e^{-a+O(a^2\log^2n)}}^n=o(1).
\eeq
\subsubsection{Medium Witnesses}
Now consider a set of pairs . We place  into  if  is obtained from 
in the following manner:  Choose . Replace edges  by  for all  and all .
Add vertex  and  edges .

Note that if  and  has a matching of  into  then so does .

For  let now

and for  let

Let
 
and for  let

Let 

Let 

We note that
\begin{itemize}
\item  implies that .
\item  implies that .
\item  for all .
\end{itemize}
We then note that


Now let  be properties such that if  and  then . Let 
be chosen uniformly from  and let  denote probabilities computed with respect to this choice. Then

and

Arguing as in Section \ref{gre} we see that if
 is a property of  for , 
\beq{y1}
\frac{|\cP_m|}{\cG(n,m)}\leq (1+O(n^{-2/5}))^{n-m}\frac{|\cQ|}{\cG(m+n^{4/5},m)}.
\eeq
First let  be 
the property that  contains a minimal witness  with .
If  and  then . 
Indeed  is a witness in .
Applying \eqref{y1} and \eqref{next1} we see that 
\whp\  fails to occur. 
Now let  be the property that  contains a minimal
witness  with .
If  and  has a 
witness  with  and  
then . Indeed  is 
also a witness in . Now if  with a witness
 then  is a witness in  and so contains 
a minimal witness  where 
 i.e. .
Applying \eqref{y1} and \eqref{next} we see that 
\whp\  fails to occur. 
This deals with medium witnesses.
\begin{thebibliography}{100}

\bibitem{AS} N. Alon and J. Spencer, The Probabilistic Method, Wiley-Interscience, 2008.

\bibitem{AFP} J. Aronson, A. Frieze, B.G. Pittel. Maximum matchings in
  sparse random graphs: Karp-Sipser re-visited. \emph{Random
    Structures and Algorithms}, 12(2):111-178, 1998.

\bibitem{ABKU} Y. Azar, A. Broder, A. Karlin, and E. Upfal. Balanced
    Allocations. \emph{SIAM Journal on Computing}, 29(1):180-200,
    1999.

\bibitem{r3t} T. Bohman, The Triangle-Free Process , {\em Advances in Mathematics}, to appear.

\bibitem{BoCo} B. Bollob\'as, A probabilistic proof of an asymptotic formula for the number of labelled regualr graphs,
{\em European Journal of Combinatorics} 1 (1980) 311-316.

\bibitem{BK} A. Broder and A. Karlin. Multilevel Adaptive Hashing. In
    \emph{Proceedings of the 1st ACM-SIAM Symposium on Discrete
    Algorithms} (SODA), pp. 43-53, 1990.

\bibitem{BM} A. Broder and M. Mitzenmacher. Using Multiple Hash
    Functions to Improve IP Lookups. \emph{Proceedings of the 20th
    IEEE International Conference on Computer Communications}
    (INFOCOM), pp. 1454-1463, 2001.

\bibitem{DeMo} A. Dembo and A. Montanari, Finite size scaling for the core of large random hypergraphs,
{\em Annals of Applied probability} 18 (2008) 1993-2040.

\bibitem{DM} L. Devroye and P. Morin.  Cuckoo Hashing: Further
    Analysis. \emph{Information Processing Letters}, 86(4):215-219,
    2003.

\bibitem{DuMa} O. Dubois and J. Mandler, The 3-XORSAT Threshold, Proceedings of the 43rd IEEE Symposium
on Foundations of Computing (2002) 779-788.

\bibitem{FP} N. Fountoulakis and K. Panagioutou, Sharp Thresholds for Cuckoo Hashing, arXiv:0910.5147v1 [cs.DS].

\bibitem{DMP} M. Mitzenmacher, Private Communication.

\bibitem{LMSS} M. Luby, M. Mitzenmacher, M. Shokrollahi and D. Spielman, 
Efficient Erasure Correcting Codes, {\em IEEE Transactions on Information Theory} 47 (2001) 569-584.

\bibitem{DW} M. Dietzfelbinder and C. Weidling.  Balanced Allocation
    and Dictionaries with Tightly Packed Constant Size Bins.
    \emph{Theoretical Computer Science}, 380(1-2):47-68, 2007.

\bibitem{fotakis} D. Fotakis, R. Pagh, P. Sanders, and P. Spirakis.
    Space Efficient Hash Tables With Worst Case Constant Access Time.
    \emph{Theory of Computing Systems}, 38(2):229-248, 2005.

\bibitem{KS} R.M. Karp and M. Sipser, Maximum Matchings in Sparse Random Graphs,
{\em Proceedings of the 22nd Annual IEEE Symposium on Foundations of Computer Science} (1981) 364-375.


\bibitem{cuckoo-q} A. Kirsch and M. Mitzenmacher. Using a Queue to
    De-amortize Cuckoo Hashing in Hardware. In \emph{Proceedings of the
    Forty-Fifth Annual Allerton Conference on Communication, Control, and
    Computing}, 2007.

\bibitem{stash} A. Kirsch, M. Mitzenmacher, and U. Wieder.  More
    Robust Hashing: Cuckoo Hashing with a Stash.  In
    \emph{Proceedings of the 16th Annual European Symposium on
    Algorithms}, pp. 611-622, 2008.

\bibitem{one-move} A. Kirsch and M. Mitzenmacher. The Power of One
    Move: Hashing Schemes for Hardware. In \emph{Proceedings of the
    27th IEEE International Conference on Computer Communications}
    (INFOCOM), pp. 565-573, 2008.

\bibitem{kutz} R. Kutzelnigg.  Bipartite Random Graphs and Cuckoo
    Hashing. In \emph{Proceedings of the Fourth Colloquium on
    Mathematics and Computer Science}, 2006.

\bibitem{MVad} M. Mitzenmacher and S. Vadhan. Why Simple Hash
    Functions Work: Exploiting the Entropy in a Data Stream. In
    \emph{Proceedings of the Nineteenth Annual ACM-SIAM Symposium on
    Discrete Algorithms} (SODA), pp. 746-755, 2008.

\bibitem{cuckoo} R. Pagh and F. Rodler. Cuckoo Hashing. \emph{Journal
    of Algorithms}, 51(2):122-144, 2004.

\bibitem{Vo} B. V\"{o}cking.  How Asymmetry Helps Load Balancing.
    \emph{Journal of the ACM}, 50(4):568-589, 2003.

\bibitem{Wormald} N.C. Wormald, The differential equation method for random graph processes and greedy algorithms,
in  {\em Lectures on Approximation and Randomized Algorithms} (M. Karonski and H.J. Proemel, eds), PWN, Warsaw, (1999) 73-155.

\end{thebibliography}
\end{document}
