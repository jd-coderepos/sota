\documentclass{article} \usepackage[dvipsnames]{xcolor}
\usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}
\newcommand{\newmaml}{MAML++}
\newcommand{\cut}[1]{}
\makeatletter

\newcommand{\mypm}{\mathbin{\mathpalette\@mypm\relax}}
\newcommand{\@mypm}[2]{\ooalign{\raisebox{.1\height}{}\cr
  \smash{\raisebox{-.6\height}{}}\cr}}
\makeatother
\title{How to train your MAML}

\author{Antreas Antoniou \\
University of Edinburgh\\
\texttt{\{a.antoniou\}@sms.ed.ac.uk} \\
\And
Harrison Edwards\\
OpenAI,
University of Edinburgh\\
\texttt{\{h.l.edwards\}@sms.ed.ac.uk} \\
\AND
Amos Storkey \\
University of Edinburgh\\
\texttt{\{a.storkey\}@ed.ac.uk}}





\newif\ifincludecomment
\includecommenttrue \ifincludecomment
  \newenvironment{guidance}{\color{ForestGreen}\it}{\color{black}\rm}
\else
  \NewEnviron{guidance}{}{}
\fi

\setlength{\marginparwidth}{16mm}
\usepackage[colorinlistoftodos,textsize=tiny, textwidth=18mm]{todonotes}
\ifincludecomment
\newcommand{\maybecomment}[1]{\todo[color=olive!40]{#1}} 
\newcommand{\maybetohere}[1]{\todo[color=red!40]{#1}} 
\newcommand{\maybedelete}[1]{\todo[color=blue!40]{#1}} 
\newcommand{\maybeinsert}[1]{\color{ForestGreen} #1}
\else
  \newcommand{\maybecomment}[1]{}
  \newcommand{\maybeinsert}[1]{#1}
\newcommand{\maybedelete}[1]{} 
\fi
\newcommand{\amostohere}[1]{{\color{black}\maybetohere{AMOS HERE}}}

\newcommand{\antreas}[1]{{\color{purple}\maybecomment{CS: #1}}}
\newcommand{\harri}[1]{{\color{ForestGreen}\maybecomment{RK: #1}}}
\newcommand{\amos}[1]{{\color{blue}\maybecomment{AJS: #1}}}


\newcommand{\amosed}[1]{{\maybeinsert{#1}}}
\newcommand{\harried}[1]{{\maybeinsert{#1}}}
\newcommand{\antreased}[1]{{\maybeinsert{#1}}}


\newcommand{\amosdel}[1]{{\maybedelete{#1}}}
\newcommand{\harridel}[1]{{\maybedelete{#1}}}
\newcommand{\antreasdel}[1]{{\maybedelete{#1}}}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem. \emph{Model Agnostic Meta Learning} or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call \emph{\newmaml}.
















\end{abstract}


\section{Introduction}
The human capacity to learn new concepts using only a handful of samples is immense. In stark contrast, modern deep neural networks need, at a minimum, thousands of samples before they begin to learn representations that can generalize well to unseen data-points \citep{krizhevsky2012imagenet,huang2017densely}, and mostly fail when the data available is scarce. The fact that standard deep neural networks fail in the small data regime can provide hints about some of their potential shortcomings. Solving those shortcomings has the potential to open the door to understanding intelligence and advancing Artificial Intelligence. Few-shot learning encapsulates a family of methods that can learn new concepts with only a handful of data-points (usually 1-5 samples per concept). This possibility is attractive for a number of reasons. First, few-shot learning would reduce the need for data collection and labelling, thus reducing the time and resources needed to build robust machine learning models. Second, it would potentially reduce training and fine-tuning times for adapting systems to newly acquired data. Third, in many real-world problems there are only a few samples available per class and the collection of additional data is either remarkably time-consuming and costly or altogether impossible, thus necessitating the need to learn from the available few samples. 

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.81\textwidth]{maml_instability_v2.pdf}
    \caption{Stabilizing MAML: This figure illustrates 3 seeds of the original strided MAML vs strided MAML++. One can see that 2 out of 3 seeds with the original strided MAML seem to become unstable and erratic, whereas all 3 of the strided MAML++ models seem to consistently converge very fast, to much higher generalization accuracy without any stability issues.}
	\label{figure:maml-stability}
\end{figure}

The nature of few-shot learning makes it a very hard problem if no prior knowledge exists about the task at hand. For a model to be able to learn a robust model from a few samples, knowledge transfer \citep[see e.g.\ ][]{caruana1995learning} from other similar tasks is key. However, manual knowledge transfer from one task to another for the purpose of fine-tuning on a new task can be a time consuming and ultimately inefficient process. \emph{Meta-learning} \citep{schmidhuber1987evolutionary,vilalta2002perspective}, or \emph{learning to learn} \citep{thrun1998learning}, can instead be used to automatically learn across-task knowledge usually referred to as \emph{across-task} (or sometimes slow) knowledge such that our model can, at inference time, quickly acquire \emph{task-specific} (or fast) knowledge from new tasks using only a few samples. Meta-learning can be broadly defined as a class of machine learning models that become more proficient at learning with more experience, thus learning \emph{how} to learn. More specifically meta-learning involves learning at two levels. At the \emph{task-level}, where the \emph{base-model} is required to acquire task-specific (fast) knowledge rapidly, and at the \emph{meta-level}, where the \emph{meta-model} is required to slowly learn \emph{across-task} (slow) knowledge. 
Recent work in meta-learning has produced state of the art results in a variety of settings \citep{wangmlrl2016,ba2016using,zoph2016neural,andrychowicz2016learning, vinyals2016matching, li2016learning,li2017meta, antoniou2017data,brock2017smash,munkhdalai2017meta}. The application of meta-learning in the few-shot learning setting has enabled the overwhelming majority of the current state of the art few-shot learning methods \citep{vinyals2016matching,ravi2016optimization,edwards2016towards,finn2017model,li2017meta,munkhdalai2017meta}. One such method, known for its simplicity and state of the art performance, is \emph{Model Agnostic Meta-Learning} (\emph{MAML}) \citep{finn2017model}. In MAML, the authors propose learning an initialization for a base-model such that after applying a very small number of gradient steps with respect to a training set on the base-model, the adapted model can achieve strong generalization performance on a validation set (the validation set consists of new samples from the same classes as the training set). Relating back to the definitions of meta-model and base-model, in MAML the meta-model is effectively the initialization parameters. These parameters are used to initialize the base-model, which is then used for task-specific learning on a support set, which is then evaluated on a target set. MAML is a simple yet elegant meta-learning framework that has achieved state of the art results in a number of settings. However, MAML suffers from a variety of problems which: 1) cause instability during training, 2) restrict the model's generalization performance, 3) reduce the framework's flexibility, 4) increase the system's computational overhead and 5) require that the model goes through a costly (in terms of time and computation needed) hyperparameter tuning before it can work robustly on a new task. 



















In this paper we propose \newmaml, an improved variant of the MAML framework that offers the flexibility of MAML along with many improvements, such as robust and stable training, automatic learning of the inner loop hyperparameters, greatly improved computational efficiency both during inference and training and significantly improved generalization performance. \newmaml\ is evaluated in the few-shot learning setting where the system is able to set a new state of the art across all established few-shot learning tasks on both Omniglot and Mini-Imagenet, performing as well as or better than all established meta learning methods on both tasks.



















\section{Related Work}

The \emph{set-to-set} few-shot learning setting \citep{vinyals2016matching}, neatly casts few-shot learning as a meta-learning problem. In set-to-set few-shot learning we have a number of tasks, each task is composed by a \emph{support set} which is used for task-level learning, and a \emph{target set} which is used for evaluating the base-model on a certain task after it has acquired task-specific (or fast) knowledge. Furthermore, all available tasks are split into 3 sets, the \emph{meta-training} set, the \emph{meta-validation} set and the \emph{meta-test} set, used for training, validating and testing our meta-learning model respectively. 



Once meta-learning was shown to be an effective framework for few-shot learning and the set to set approach was introduced, further developments in few-shot learning were made in quick succession. One contribution was \emph{Matching Networks} \citep{vinyals2016matching}. Matching networks achieve few-shot learning by learning to \emph{match} target set items to support set items. More specifically, a matching network learns to match the target set items to the support set items using cosine distance and a fully differentiable embedding function.  First, the support set embedding function , parameterized as a deep neural network, embeds the support set items into embedding vectors, then the target set embedding function  embeds the target set items. Once all data-item embeddings are available, cosine distance is computed for all target set embeddings when compared to all support set embeddings. As a result, for each target set item, a vector of cosine distances with respect to all support set items will be generated (with each distance's column tied to the respective support set class). Then, the softmax function is applied on the generated distance vectors, to convert them into probability distributions over the support set classes. 

Another notable advancement was the gradient-conditional \emph{meta-learner} LSTM \citep{ravi2016optimization} that learns how to update a \emph{base-learner} model. At inference time, the meta-learner model applies a single update on the base-learner given gradients with respect to the support set. The fully updated base-model then computes predictions on the target set. The target set predictions are then used to compute a task loss. Furthermore they jointly learn the meta-learner's parameters as well as the base-learners initializations such that after a small number of steps it can do very well on a given task. The authors ran experiments on Mini-Imagenet where they exceed the performance of Matching Networks.

In \emph{Model Agnostic meta-learning} (MAML) \citep{finn2017model} the authors proposed increasing the gradient update steps on the base-model and replacing the meta-learner LSTM with Batch Stochastic Gradient Descent \citep{krizhevsky2012imagenet}, which as a result speeds up the process of learning and interestingly improves generalization performance and achieves state of the art performance in both Omniglot and Mini-Imagenet.

In \emph{Meta-SGD} \citep{li2017meta} the authors proposed learning a static learning rate and an update direction for each parameter in the base-model, in addition to learning the initialization parameters of the base-model. Meta-SGD showcases significantly improved generalization performance (when compared to MAML) across all few-shot learning tasks, whilst only requiring a single inner loop update step. However this practice effectively doubles the model parameters and computational overheads of the system.











\section{Model Agnostic Meta Learning}
\label{maml-definition}
\emph{Model Agnostic Meta-Learning} (MAML) \citep{finn2017model} is a meta-learning framework for few-shot learning. MAML is elegantly simple yet can produce state of the art results in few-shot regression/classification and reinforcement learning problems. In a sentence, MAML learns good initialization parameters for a network, such that after a few steps of standard training on a few-shot dataset, the network will perform well on that few shot task.


More formally, we define the \emph{base model} to be a neural network  with meta-parameters . We want to learn an initial  that, after a small number  of gradient update steps on data from a support set  to obtain , the network performs well on that task's target set . Here  is the index of a particular support set task in a batch of support set tasks.  This set of  updates steps is called the \emph{inner-loop update process}.


The updated base-network parameters after  steps on data from the support task  can be expressed as:

where  is the learning rate,  are the base-network weights after  steps towards task ,  is the loss on the support set of task  after  (i.e. the previous step) update steps.
Assuming \cut{our model takes  steps towards the support set loss before computing the target set loss, and }that our \emph{task batch size} is  we can define a \emph{meta-objective}, which can be expressed as:

where we have explicitly denoted the dependence of  on , given by unrolling (\ref{maml_inner_update}). The objective (\ref{maml_outer_objective}) measures the quality of an initialization  in terms of the total loss of using that initialization across all tasks. This meta objective is now minimized to optimize the initial parameter value . It is this initial  that contains the across-task knowledge. The optimization of this meta-objective is called the \emph{outer-loop update process}.

The resulting update for the meta-parameters  can be expressed as:

where  is a learning rate and  denotes the loss on the target set for task .

In this paper we use the cross-entropy \citep{de2005tutorial,rubinstein1999cross} loss throughout.

\subsection{Model Agnostic meta-learning Problems}\label{section:maml-issues}
The simplicity, elegance and high performance of MAML make it a very powerful framework for meta-learning. However, MAML has also many issues that make it problematic to use. 

\textbf{Training Instability:} Depending on the neural network architecture and the overall hyperparameter setup, MAML can be very unstable during training as illustrated in Figure \ref{figure:maml-stability}. Optimizing the outer loop involved backpropagating derivatives through an unfolded inner loop consisting of the same network multiple times. This alone could be cause for gradient issues. However, the gradient issues are further compounded by the model architecture, which is a standard 4-layer convolutional network without skip-connections. The lack of any skip-connections means that every gradient must be passed through each convolutional layer many times; effectively the gradients will be multiplied by the same sets of parameters multiple times. After multiple back-propagation passes, the large depth structure of the unfolded network and lack of skip connections can cause gradient explosions and diminishing gradient problems respectively.

\textbf{Second Order Derivative Cost:} Optimization through gradient update steps requires the computation of second order gradients which are very expensive to compute. The authors of MAML proposed using first-order approximations to speed up the process by a factor of three, however using these approximations can have a negative impact on the final generalization error. Further attempts at using first order methods have been attempted in Reptile \citep{DBLP:journals/corr/abs-1803-02999} where the authors apply standard SGD on a base-model and then take a step from their initialization parameters towards the parameters of the base-model after  steps. The results of Reptile vary, in some cases exceeding MAML, and in others producing results inferior to MAML. Approaches to reduce computation time while not sacrificing generalization performance have yet to be proposed.

\textbf{Absence of Batch Normalization Statistic Accumulation:} A further issue that affects the generalization performance is the way that batch normalization is used in the experiments in the original MAML paper. Instead of accumulating running statistics, the statistics of the current batch were used for batch normalization. This results in batch normalization being less effective, since the biases learned have to accommodate for a variety of different means and standard deviations instead of a single mean and standard deviation. On the other hand, if batch normalization uses accumulated running statistics it will eventually converge to some global mean and standard deviation. This leaves only a single mean and standard deviation to learn biases for. Using running statistics instead of batch statistics, can greatly increase convergence speed, stability and generalization performance as the normalized features will result in smoother optimization landscape \citep{santurkar2018does}. 





\textbf{Shared (across step) Batch Normalization Bias:} An additional problem with batch normalization in MAML stems from the fact that batch normalization biases are not updated in the inner-loop; instead the same biases are used throughout all iterations of base-models. Doing this implicitly assumes that all base-models are the same throughout the inner loop updates and hence have the same distribution of features passing through them. This is a false assumption to make, since, with each inner loop update, a new base-model is instantiated that is different enough from the previous one to be considered a new model from a bias estimation point of view. Thus learning a single set of biases for all iterations of the base-model can restrict performance.

\textbf{Shared Inner Loop (across step and across parameter) Learning Rate:} One issue that affects both generalization and convergence speed (in terms of training iterations) is the issue of using a shared learning rate for all parameters and all update-steps. Doing so introduces two major problems. Having a fixed learning rate requires doing multiple hyperparameter searches to find the correct learning rate for a specific dataset; this process can be very computationally costly, depending on how search is done. 



The authors in \citep{li2017meta} propose to learn a learning rate and update direction for each parameter of the network. Doing so solves the issue of manually having to search for the right learning rate, and also allows individual parameters to have smaller or larger learning rates. However this approach brings its own problems. Learning a learning rate for each network parameter means increased computational effort and increased memory usage since the network contains between 40K and 50K parameters depending on the dimensionality of the data-points. 



\textbf{Fixed Outer Loop Learning Rate:} In MAML the authors use Adam with a fixed learning rate to optimize the meta-objective. Annealing the learning rate using either step or cosine functions has proven crucial to achieving state of the art generalization performance in a multitude of settings \citep{loshchilov2016sgdr,he2016deep,larsson2016fractalnet,huang2017densely}. Thus, we theorize that using a static learning rate reduces MAML's generalization performance and might also be a reason for slower optimization. Furthermore, having a fixed learning rate might mean that one has to spend more (computational) time tuning the learning rate.



\section{Stable, automated and improved MAML}\label{section:maml-fix}
In this section we propose methods for solving the issues with the MAML framework, described in Section \ref{section:maml-issues}. Each solution has a reference identical to the reference of the issue it is attempting to solve.

\textbf{Gradient Instability  Multi-Step Loss Optimization (MSL)}: MAML works by minimizing the target set loss computed by the base-network after it has completed \textbf{all} of its inner-loop updates towards a support set task. Instead we propose minimizing the target set loss computed by the base-network after \textbf{every} step towards a support set task. More specifically, we propose that the loss minimized is a weighted sum of the target set losses after every support set loss update. More formally:

Where  is a learning rate,  denotes the target set loss of task  when using the base-network weights after  steps towards minimizing the support set task and  denotes the importance weight of the target set loss at step , which is used to compute the weighted sum.

By using the \emph{multi-step} loss proposed above we improve gradient propagation, since now the base-network weights at every step receive gradients both directly (for the current step loss) and indirectly (from losses coming from subsequent steps). With the original methodology described in Section~\ref{maml-definition} the base-network weights at every step except the last one were optimized implicitly as a result of backpropagation, which caused many of the instability issues MAML had. However using the multi-step loss alleviates this issue as illustrated in Figure \ref{figure:maml-stability}. Furthermore, we employ an annealed weighting for the per step losses. Initially all losses have equal contributions towards the loss, but as iterations increase, we decrease the contributions from earlier steps and slowly increase the contribution of later steps. This is done to ensure that as training progresses the final step loss receives more attention from the optimizer thus ensuring it reaches the lowest possible loss. If the annealing is not used, we found that the final loss might be higher than with the original formulation.

\textbf{Second Order Derivative Cost  Derivative-Order Annealing (DA):} One way of making MAML more computationally efficient is reducing the number of inner-loop updates needed, which can be achieved with some of the methods described in subsequent sections of this report. However, in this paragraph, we propose a method that reduces the per-step computational overhead directly. The authors of MAML proposed the usage of first-order approximations of the gradient derivatives. However they applied the first-order approximation throughout the whole of the training phase. Instead, we propose to anneal the derivative-order as training progresses. More specifically, we propose to use first-order gradients for the first 50 epochs of the training phase, and to then switch to second-order gradients for the remainder of the training phase. We empirically demonstrate that doing so greatly speeds up the first 50 epochs, while allowing the second-order training needed to achieve the strong generalization performance the second-order gradients provide to the model. An additional interesting observation is that derivative-order annealing experiments showed no incidents of exploding or diminishing gradients, contrary to second-order only experiments which were more unstable. Using first-order before starting to use second-order derivatives can be used as a strong \emph{pretraining} method that learns parameters less likely to produce gradient explosion/diminishment issues.

\textbf{Absence of Batch Normalization Statistic Accumulation  Per-Step Batch Normalization Running Statistics (BNRS):} In the original implementation of MAML \cite{finn2017model} the authors used only the current batch statistics as the batch normalization statistics. This, we argue, caused a variety of undesirable effects described in Section~\ref{section:maml-issues}. To alleviate the issues we propose using running batch statistics for batch normalization. A naive implementation of batch normalization in the context of MAML would require sharing running batch statistics across all update steps of the inner-loop fast-knowledge acquisition process. However doing so would cause the undesirable consequence that the statistics stored be shared across all inner loop updates of the network. This would cause optimization issues and potentially slow down or altogether halt optimization, due to the increasing complexity of learning parameters that can work across various updates of the network parameters. A better alternative would be to collect statistics in a per-step regime. To collect running statistics per-step, one needs to instantiate  (where  is the total number of inner-loop update steps) sets of running mean and running standard deviation for each batch normalization layer in the network and update the running statistics respectively with the steps being taken during the optimization. The per-step batch normalization methodology should speed up optimization of MAML whilst potentially improving generalization performance.

\textbf{Shared (across step) Batch Normalization Bias  Per-Step Batch Normalization Weights and Biases (BNWB):} In the MAML paper the authors trained their model to learn a \textbf{single} set of biases for each layer. Doing so assumes that the distributions of features passing through the network are similar. However, this is a false assumption since the base-model is updated for a number of times, thus making the feature distributions increasingly dissimilar from each other. To fix this problem we propose learning a set of biases \textbf{per-step} within the inner-loop update process. Doing so, means that batch normalization will learn biases specific to the feature distributions seen at each set, which should increase convergence speed, stability and generalization performance.

\textbf{Shared Inner Loop Learning Rate (across step and across parameter)  Learning Per-Layer Per-Step Learning Rates and Gradient Directions (LSLR):} Previous work in \cite{li2017meta} demonstrated that learning a learning rate and gradient direction for each parameter in the base-network improved the generalization performance of the system. However, that had the consequence of increased number of parameters and increased computational overhead. So instead, we propose, learning a learning rate and direction for each layer in the network as well as learning different learning rates for each adaptation of the base-network as it takes steps. Learning a learning rate and direction for each layer instead for each parameter should reduce memory and computation needed whilst providing additional flexibility in the update steps. Furthermore, for each learning rate learned, there will be  instances of that learning rate, one for each step to be taken. By doing this, the parameters are free to learn to decrease the learning rates at each step which may help alleviate overfitting.

\textbf{Fixed Outer Loop Learning Rate  Cosine Annealing of Meta-Optimizer Learning Rate (CA):} In MAML the authors use a static learning rate for the optimizer of the meta-model. Annealing the learning rate, either by using step-functions \citep{he2016deep} or cosine functions \citep{loshchilov2016sgdr} has proved vital in learning models with higher generalization power. The cosine annealing scheduling has been especially effective in producing state of the art results whilst removing the need for any hyper-parameter searching on the learning rate space. Thus, we propose applying the cosine annealing scheduling on the meta-model's optimizer (i.e. the \emph{meta-optimizer}). Annealing the learning rate allows the model to fit the training set more effectively and as a result might produce higher generalization performance.


\subsection{Datasets}\label{datasets-maml}
The datasets used to evaluate our methods were the Omniglot \citep{lake2015human} and Mini-Imagenet \citep{vinyals2016matching,ravi2016optimization} datasets. Each dataset is split into 3 sets, a training, validation and test set. The Omniglot dataset is composed of 1623 characters classes from various alphabets. There exist 20 instances of each class in the dataset. For Omniglot we shuffle all character classes and randomly select 1150 for the training set and from the remaining classes we use 50 for validation and 423 for testing. In most few-shot learning papers the first 1200 classes are used for training and the remaining for testing. However, having a small validation set to choose the best model is crucial, so we choose to use a small set of 50 classes as validation set. For each class we use all available 20 samples in the sets. Furthermore for the Omniglot dataset, data augmentation is used on the images in the form of rotations of 90 degree increments. Class samples that are rotated are considered new classes, e.g. a 180 degree rotated character  is considered a different class from a non rotated , thus effectively having 1623 x 4 classes in total. However the rotated classes are generated dynamically after the character classes have been split into the sets such that rotated samples from a class reside in the same set (i.e. the training, validation or test set).
The Mini-Imagenet dataset was proposed in \cite{ravi2016optimization}, it consists of 600 instances of 100 classes from the ImageNet dataset, scaled down to 84x84. We use the split proposed in \cite{ravi2016optimization}, which consists of 64 classes for training, 12 classes for validation and 24 classes for testing.

\subsection{Experiments}
To evaluate our methods we adopted a hierarchical hyperparameter search methodology. First we began with the baseline MAML experiments, which were ran on the 5/20-way and 1/5-shot settings on the Omniglot dataset and the 5-way 1/5-shot setting on the Mini-Imagenet dataset. Then we added each one of our 6 methodologies on top of the default MAML and ran experiments for each one separately. Once this stage was completed we combined the approaches that showed improvements in either generalization performance or convergence speed (both in terms of number of epochs and clock-time) and ran a final experiment to establish any potential gains from the combination of the techniques.  


An experiment consisted of training for 150 epochs, each epoch consisting of 500 iterations. At the end of each epoch, we evaluated the performance of the model on the validation set. Upon completion of all epochs, an ensemble of the top 3 performing per-epoch-models on the validation set were applied on the test set, thus producing the final test performance of the model. An evaluation ran consisted of inference on 600 unique tasks. A distinction between the training and evaluation tasks, was that the training tasks were generated dynamically continually without repeating previously sampled tasks, whilst the 600 evaluation tasks generated were identical across epochs. Thus ensuring that the comparison between models was fair, from an evaluation set viewpoint. Every experiment was repeated for 3 independent runs.

The models were trained using the Adam optimizer with a learning rate of ,  and . Furthermore, all Omniglot experiments used a task batch size of 16, whereas for the Mini-Imagenet experiments we used a task batch size of 4 and 2 for the 5-way 1-shot and 5-way 5-shot experiments respectively.

\begin{table}[tbh]
\centering
\caption{\newmaml\ Omniglot 20-way Few-Shot Results: Our reproduction of MAML appears to be replicating all the results except the 20-way 1-shot results. Other authors have come across this problem as well \cite{jamal2018task}. We report our own base-lines to provide better relative intuition on how each method impacted the test accuracy of the model. We showcase how our proposed improvements individually improve on the MAML performance. Our method improves on the existing state of the art.}
\begin{tabular}{|l|c|c}
\hline
\multicolumn{3}{|c|}{\textbf{Omniglot 20-way Few-Shot Classification}}                  \\ \hline
                    & \multicolumn{2}{c|}{\textbf{Accuracy}}                               \\ \hline
\textbf{Approach}   & \textbf{1-shot}             & \multicolumn{1}{c|}{\textbf{5-shot}}    \\ \hline
Siamese Nets        & 88.2\%                      & \multicolumn{1}{c|}{97.0\%}             \\ \hline
Matching Nets       & 93.8\%                      & \multicolumn{1}{c|}{98.5\%}             \\ \hline
Neural Statistician & 93.2\%                      & \multicolumn{1}{c|}{98.1\%}             \\ \hline
Memory Mod.         & 95.0\%                      & \multicolumn{1}{c|}{98.6\%}             \\ \hline
Meta-SGD            & 95.930.38\%          & \multicolumn{1}{c|}{98.970.19\%} \\ \hline
Meta-Networks       & 97.00\%                     & \multicolumn{1}{c|}{}                \\ \hline
MAML (original)     & 95.80.3\%            & \multicolumn{1}{c|}{98.90.2\%}   \\ \hline
MAML (local replication)         & 91.271.07\%          & \multicolumn{1}{c|}{98.78\%}            \\ \hline
\newmaml            & \textbf{97.650.05\%} & \multicolumn{1}{c|}{\textbf{99.330.03\%}}   \\ \hline
MAML + MSL          & 91.530.69\%          & \multicolumn{1}{c|}{-}                  \\ \hline
MAML + LSLR         & 95.770.38\%          & \multicolumn{1}{c|}{-}                  \\ \hline
MAML + BNWB + BNRS  & 95.350.23\%          & \multicolumn{1}{c|}{-}                  \\ \hline
MAML + CA           & 93.030.44\%          & \multicolumn{1}{c|}{-}                  \\ \hline
MAML + DA           & 92.30.55\%           & \multicolumn{1}{c|}{-}                  \\ \hline
\end{tabular}
	\label{table:omniglot_maml++}
\end{table}








\begin{table}[tbh]
\centering
\caption{\newmaml\ Mini-Imagenet Results. \emph{\newmaml\ } indicates MAML + all the proposed fixes. Our reproduction of MAML appears to be replicating all the results of the original. Our approach sets a new state of the art across all tasks. It is also worth noting, that our approach, with only 1 inner loop step can already exceed all other methods. Additional steps allow for even better performance.}
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{4}{|c|}{\textbf{Mini-Imagenet 5-way Few-Shot Classification}}                                                       \\ \hline
\textbf{\textbf{}}        & \textbf{\textbf{Inner Steps}} & \multicolumn{1}{c|}{\textbf{Accuracy}} &                             \\ \hline
\textbf{Mini-Imagenet}    &                               & \multicolumn{1}{c|}{\textbf{1-shot}}                        & \multicolumn{1}{c|}{\textbf{5-shot}}             \\ \hline
Matching Nets             & \multicolumn{1}{c|}{-}                             & 43.56\%                                & 55.31\%                     \\ \hline
Meta-SGD                  & \multicolumn{1}{c|}{1}                             & 50.471.87\%                     & 64.030.94\%          \\ \hline
Meta-Networks             & \multicolumn{1}{c|}{-}                             & 49.21\%                                & \multicolumn{1}{c|}{-}                         \\ \hline
MAML (original paper)     & \multicolumn{1}{c|}{5}                             & 48.701.84\%                     & 63.110.92\%          \\ \hline
MAML (local reproduction) & \multicolumn{1}{c|}{5}                             & 48.250.62\%                     & 64.390.31\%                     \\ \hline
\newmaml                  & \multicolumn{1}{c|}{1}                             & 51.050.31\%                     & \multicolumn{1}{c|}{-}                           \\ \hline
\newmaml                  & \multicolumn{1}{c|}{2}                             & 51.490.25\%                     & \multicolumn{1}{c|}{-}                           \\ \hline
\newmaml                  & \multicolumn{1}{c|}{3}                             & 51.110.11\%                     & \multicolumn{1}{c|}{-}                           \\ \hline
\newmaml                  & \multicolumn{1}{c|}{4}                             & 51.650.34\%                     & \multicolumn{1}{c|}{-}                           \\ \hline
\newmaml                  & \multicolumn{1}{c|}{5}                             & \textbf{52.150.26\%}            & \textbf{68.320.44\%} \\ \hline
\end{tabular}

	\label{table:mini-imagenet_maml++}
\end{table}





\subsection{Results}
Our proposed methodologies are empirically shown to improve the original MAML framework. In Table \ref{table:omniglot_maml++} one can see how our proposed approach performs on Omniglot. Each proposed methodology can individually outperform MAML, however, the most notable improvements come from the learned per-step per-layer learning rates and the per-step batch normalization methodology. In the 5-way 1-shot tasks it achieves 99.47\% and in the 20-way Omniglot tasks \newmaml\ achieves 97.76\% and 99.33\% in the 1-shot and 5-shot tasks respectively. \newmaml\ also showcases improved convergence speed in terms of training iterations required to reach the best validation performance. Furthermore, the multi-step loss optimization technique substantially improves the training stability of the model as illustrated in Figure \ref{figure:maml-stability}. In Table \ref{table:omniglot_maml++} we also include the results of our own implementation of MAML, which reproduces all results except the 20-way 1-shot Omniglot case. Difficulty in replicating the specific result has also been noted before in \cite{jamal2018task}. We base our conclusions on the relative performance between our own MAML implementation and the proposed methodologies.

Table \ref{table:mini-imagenet_maml++} showcases \newmaml on Mini-Imagenet tasks, where \newmaml\ sets a new state of the art in both the 5-way 1-shot and 5-shot cases where the method achieves 52.15\% and 68.32\% respectively. More notably, \newmaml\ can achieve very strong 1-shot results of 51.05\% with only a single inner loop step required. Not only is \newmaml\ cheaper due to the usage of derivative order annealing, but also because of the much reduced inner loop steps. Another notable observation is that \newmaml converges to its best generalization performance much faster (in terms of iterations required) when compared to MAML as shown in Figure \ref{figure:maml-stability}.

\section{Conclusion}
In this paper we delve deep into what makes or breaks the MAML framework and propose multiple ways to reduce the inner loop hyperparameter sensitivity, improve the generalization error, stabilize and speed up MAML. The resulting approach, called \newmaml sets a new state of the art across all few-shot tasks, across Omniglot and Mini-Imagenet. The results of the approach indicate that learning per-step learning rates, batch normalization parameters and optimizing on per-step target losses appears to be key for fast, highly automatic and strongly generalizable few-shot learning. 

\section{Acknowledgements}
This work was supported in by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical
Sciences Research Council and the University of Edinburgh as well as by the European Unions Horizon 2020 research and innovation programme under grant agreement No 732204 (Bonseyes) and by the Swiss State Secretariat for Education Research and Innovation (SERI) under contract number 16.0159. The opinions expressed and arguments employed herein do not necessarily reflect the official views of these funding bodies.




\bibliography{iclr2019_conference,datasets,deeplearning,general,metalearning}
\bibliographystyle{iclr2019_conference}
\appendix


\section{Additional Results}

\begin{table}[tbh]
\centering
\caption{Mini-Imagenet Training Iteration Timing table. In this table, one can see per training iteration wall-clock timings for MAML vs MAML++. We provide timings for variants of the model spanning 1 to 5 inner loop steps. It can be observed that MAML++ needs less time per training iteration, even though it needs more parameters and has more computations needed. We can see that more steps require more computation in return for better generalization performance as evidenced in Table~\ref{table:mini-imagenet_maml++}.
	\label{table:mini-imagenet_timings}}
	
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Inner Loop Steps}  & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\ \hline
\textbf{MAML ++ (ms/iter)} & 275.319    & 433.8172   & 579.314    & 786.3278   & 947.0376   \\ \hline
\textbf{MAML (ms/iter)}    & 294.373    & 475.1218   & 658.4436   & 859.1158   & 1028.1656  \\ \hline
\end{tabular}
\end{table}


\begin{table}[tbh]
\centering
\caption{\newmaml\ Omniglot 5-way Results: \emph{\newmaml} indicates MAML + all the proposed fixes. We report our own base-lines on MAML to provide better relative intuition on how each method impacted the test accuracy of the model. We can see that \newmaml matches or improves on MAML across all cases. \newmaml also has performance very close to Meta-SGD which uses double the amount of parameters that MAML requires (about 40K extra parameters), whilst only using 1 extra parameter, per layer, per step for LSLR and  (where  is the number of filters at layers preceding batch normalization,  is number of layers and  is number of inner loop step) when using BNWB. Having less parameters also means smaller training and testing times.}
	\label{table:omniglot2_maml++}
\begin{tabular}{|l|c|c|}
\hline
\multicolumn{3}{|c|}{\textbf{Omniglot 5-way Few-Shot Classification}}                          \\ \hline
                    & \multicolumn{2}{c|}{\textbf{Accuracy}}  \\ \hline
\textbf{Omniglot}   & \textbf{1-shot}    & \textbf{5-shot}    \\ \hline
Siamese Nets        & 97.3\%             & 98.4\%             \\ \hline
Matching Nets       & 98.1\%             & 98.9\%             \\ \hline
Neural Statistician & 98.1\%             & 99.5\%             \\ \hline
Memory Mod.         & 98.4\%             & 99.6\%             \\ \hline
Meta-SGD            & \textbf{99.530.26\%} & \textbf{99.930.09\%} \\ \hline
Meta-Networks       & 98.95\%            & \multicolumn{1}{c|}{-}                \\ \hline
MAML (original)     & 98.700.4\%  & 99.90.1\%   \\ \hline
MAML (local replication)         & 98.57\%            & 99.82\%            \\ \hline
\newmaml            & 99.47\%   & 99.93\%   \\ \hline
\end{tabular}

\end{table}
\end{document}
