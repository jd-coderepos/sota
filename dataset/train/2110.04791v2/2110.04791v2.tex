To evaluate the effectiveness of our proposed \emph{SRSSN}, we conduct three sets of experiments: 1) speech separation in the clean (noise-free) setting \maRevise{involving 2 and 3 speakers respectively}, 2) speech separation between 2 speakers in noisy and reverberant settings, and 3) speech recognition on separated speech signals decoded by methods for speech separation to evaluate the performance of speech separation indirectly. We also perform ablation study on the task of speech separation in the clean setting to investigate the effect of each proposed functional technique in our proposed \emph{SRSSN}. In each set of experiments, we evaluate the performance of our \emph{SRSSN} adopting the separator structure of DPRNN-TASNET~\cite{DPRNN} and DPTNET-TASNET~\cite{DPTNet} respectively to evaluate the robustness of our model utilizing different classical separator structures. These two versions of our model are denoted as DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} respectively.

\subsection{Experimental Setup}
\noindent\textbf{Evaluation Metrics.} 
For speech separation, we employ two standard metrics for evaluation, namely scale-invariant signal-to-noise ratio improvement ($\Delta$SI-SNR)~\cite{SISNR} and signal-to-distortion ratio improvement ($\Delta$SDR)~\cite{SDR}. Higher value of $\Delta$SI-SNR or $\Delta$SDR indicates higher quality of the separated results. For speech recognition, we employ Word Error Rate (WER) of the predicted transcripts relative to the reference transcripts for evaluation. Lower value of WER implies better recognition result and higher quality of the separated speech. 

\noindent\textbf{Implementation Details}
Our model is implemented in Pytorch framework~\cite{pytorch}. It is trained using Adam~\cite{adam} optimizer with a learning rate of $10^{-3}$ and a weight decay of $10^{-5}$ on 2-second temporal segments for 200 epochs. We clip all gradients to lie in the interval $[-5, 5]$ to avoid potential gradient explosion.
For the encoder $\mathcal{E}^c$ in the coarse phase, the number of filters $N^c$, kernel size $K^c$, and stride size are set to be 256, 16, and 8, respectively.
For the encoder $\mathcal{E}^r$ in the refining phase, the number of filters $N^r$, kernel size $K^r$, stride size, and number of groups $P$ are tuned to be 256, 2, 1, and 4, respectively. For both separators $\mathcal{S}^c$ and $\mathcal{S}^r$, the numbers of core blocks $R$ are set to 6 except for the experiment on ablation study (Section~\ref{sssec:ablation}), and the length of chunks $L$ is set to 100.
In the DPRNN blocks, each Bi-LSTM layer is equipped with 128 hidden units in each direction. 
In the DPTNET blocks, each improved Transformer layer~\cite{DPTNet} consists of a 4-head self-attention layer with total embedding dimension of 64, a Bi-LSTM layer with 128 hidden units in each direction, and a linear layer with 64 hidden units.



\subsection{Speech Separation in Clean Setting}
\label{ssec:sep_clean}
We first conduct experiments on speech separation in clean setting, i.e., no noise is contained in the mixture of speech signals except the signals of involved speakers to be separated. We first perform ablation study to investigate the effectiveness of the coarse-to-fine separation framework and the proposed Fine-grained Encoding Mechanism. Then we compare our \emph{SRSSN} to the state-of-the-art methods for speech separation in this experimental setting.

\noindent\textbf{Dataset.} 
We perform experiments on WSJ0-2mix and \maRevise{ WSJ0-3mix~\cite{DPCL}} in the clean setting, which is the reference mixed speech datasets for single-channel speech separation. WSJ0-2mix is generated from Wall Street Journal (WSJ) dataset~\cite{wsj0}, and consists of mixed speech utterances from two different speakers with random signal-to-noise ratio between 0 dB and 5dB. The data in WSJ0-2mix is split into three sets with duration of 30 hours, 10 hours and 5 hours for training, validation and test, respectively. \maRevise{WSJ0-3mix, containing three-speaker mixtures, is generated in a similar way as in WSJ0-2mix.}
We use the \textit{min} version of speech data with sampling rate of 8kHz, the benchmark for speaker separation, in which the longer utterance is trimmed to align the shorter utterance.

\begin{comment}
\begin{figure}[b]
\begin{minipage}[b]{0.43\linewidth}
    \centering
    \centerline{\includegraphics[width=\textwidth]{bar_SI-SNR}}
    \centerline{(a) Performance in SI-SNRi.}\medskip
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[b]{0.43\linewidth}
    \centering
    \centerline{\includegraphics[width=\textwidth]{bar_SDR}}
    \centerline{(b) Performance in SDRi.}\medskip
  \end{minipage}
\caption{Performance of four variants of our SRSSN in terms of SI-SNRi and SDRi for ablation study. $R$ is the number of DPRNN blocks in both coarse and refining separators.}
\label{fig:ablation}
\end{figure}
\end{comment}







\begin{figure*}
  \centering
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{dprnn_bar_revised}
    \centerline{(a) Performance of DPRNN-\emph{SRSSN}}\medskip
  \end{minipage}
  \hspace{0.01\linewidth}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{dpt_bar_revised}
    \centerline{(b) Performance of DPTNET-\emph{SRSSN}}\medskip
  \end{minipage}
  \caption{\maRevise{Performance of nine variants of our \emph{SRSSN} in terms of $\Delta$SI-SNR and $\Delta$SDR for ablation study, using DPRNN and DPTNET as separator respectively.}}
  \label{fig:ablation_groups}
\end{figure*}

\begin{comment}
\begin{table*}
\centering
  \caption{Performance of eight variants of our SRSSN in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) for ablation study. }
\label{tab:ablation}
  \setlength{\tabcolsep}{1.2pt} \begin{tabular}
  {l p{7em}<{\centering} p{10em}<{\centering} p{6em}<{\centering} p{5em}<{\centering} p{5em}<{\centering} p{5em}<{\centering} p{5em}<{\centering}}
  \toprule
  \multirow{2}{*}{\centering Method} & Space order & Parameters of encoder & \multirow{2}{*}{\centering Model size} & \multicolumn{2}{c}{DPRNN} & \multicolumn{2}{c}{DPTNET} \\
  & ($\mathcal{H}^c$ \& $\mathcal{H}^r$) & ($\mathcal{E}^c$ \& $\mathcal{E}^r$) & & $\Delta$SI-SNR & $\Delta$SDR & $\Delta$SI-SNR & $\Delta$SDR\\
  \midrule
  \midrule
  \textbf{Base model} & low \& - & 4k \& - & - & 17.8 & 18.1 & 18.3 & 18.6 \\
  \midrule
  \textbf{Base model-expanded} & low \& - & 16k \& - & - & 17.8 & 18.1 & 18.2 & 18.5 \\
  \midrule
  \textbf{Iterative} & low \& low & 4k \& 4k & - & 18.3 & 18.5 & 19.2 & 19.5 \\ 
  \midrule
  \textbf{High-order} & high \& - & - \& - & - & 19.1 & 19.3 & 19.0 & 19.2 \\
  \midrule
  \textbf{SRSSN-1D} & low \& low & 4k \& 131k & - & 19.3 & 19.5 & 19.7 & 19.9 \\
  \midrule
  \textbf{SRSSN-1D-expanded} & low \& low & 4k \& 524k & - & 19.5 & 19.7 & 20.0 & 20.2 \\
  \midrule
  \textbf{SRSSN-$\mathcal{L}^r$} & high \& high & 4k \& 32k  & - & 18.1 & 18.3 & 18.7 & 18.9 \\ 
  \midrule
  \textbf{SRSSN} & high \& high & 4k \& 32k & - & 20.0 & 20.2 & 20.4 & 20.6 \\ 
  \midrule
  \end{tabular} 
\end{table*}
\end{comment}

\begin{comment}
\begin{table*}
\centering
  \caption{Performance of eight variants of our SRSSN in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) for ablation study. }
\label{tab:ablation}
  \setlength{\tabcolsep}{1.2pt} \begin{tabular}
  {l p{7em}<{\centering} p{10em}<{\centering} p{6em}<{\centering} p{5em}<{\centering} p{5em}<{\centering} p{5em}<{\centering} p{5em}<{\centering}}
  \toprule
  \multirow{2}{*}{\centering Method} & Basis functions & Parameters of encoders & \multirow{2}{*}{\centering Model size} & \multicolumn{2}{c}{DPRNN} & \multicolumn{2}{c}{DPTNET} \\
  & ($\mathcal{H}^c$ \& $\mathcal{H}^r$) & ($\mathcal{E}^c$ \& $\mathcal{E}^c$) & & $\Delta$SI-SNR & $\Delta$SDR & $\Delta$SI-SNR & $\Delta$SDR\\
  \midrule
  \midrule
  \textbf{Base model} & 256 \& - & 4k \& - & - & 17.8 & 18.1 & 18.3 & 18.6 \\
  \midrule
  \multirow{3}{*}{\textbf{Base model-expanded}} & 512 \& - & 8k \& - & - & 17.9 & 18.2 & 18.0 & 18.3 \\ 
  & 1024 \& - & 16k \& - & - & 17.8 & 18.1 & 18.2 & 18.5 \\ 
  & 2048 \& - & 32k \& - & - & 17.7 & 18.0 & 18.3 & 18.5 \\ 
  \midrule
  \textbf{High-order} & - \& - & - \& - & - & 19.1 & 19.3 & 19.0 & 19.2 \\
  \midrule
  \textbf{Iterative} & 256 \& 256 & 4k \& 4k & - & 18.3 & 18.5 & 19.2 & 19.5 \\ 
  \midrule
  \textbf{SRSSN-1D} & 256 \& 256 & 4k \& 131k & - & 19.3 & 19.5 & 19.7 & 19.9 \\ 
  \midrule
  \multirow{3}{*} {\textbf{SRSSN-1D-expanded}} & 256 \& 512 & 4k \& 262k & - & 19.4 & 19.7 & 20.1 & 20.3 \\ 
  & 256 \& 1024 & 4k \& 524k & - & 19.5 & 19.7 & 20.0 & 20.2 \\ 
  & 256 \& 2048 & 4k \& 1048k & - & 19.6 & 19.9 & 19.7 & 19.9 \\ 
  \midrule
  \textbf{SRSSN-$\mathcal{L}^r$} & 256 \& 256 & 4k \& 4k & - & 18.1 & 18.3 & 18.7 & 18.9 \\ 
  \midrule
  \multirow{3}{*} {\textbf{SRSSN}} & 256 \& 256 $\times$ 2 & 4k \& 65k & - & 19.6 & 19.9 & 20.2 & 20.4 \\ 
  & 256 \& 256 $\times$ 4 & 4k \& 32k & - & 20.0 & 20.2 & 20.4 & 20.6 \\ 
  & 256 \& 256 $\times$ 8 & 4k \& 16k & - & 20.0 & 20.2 & - & - \\ 
  \midrule
  \end{tabular} 
\end{table*}
\end{comment}

\subsubsection{Ablation Study}
\label{sssec:ablation}
We perform ablation experiments on \maRevise{nine} variants of our \emph{SRSSN} for both DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} \maRevise{on WSJ0-2mix~\cite{DPCL}}:
\begin{itemize}\setlength{\itemsep}{-0cm}
    \item \textbf{Base model}, which has only coarse phase and thus no Fine-grained Encoding Mechanism is used. As a result, the base model is equivalent to DPRNN-TASNET or DPTNET-TASNET, depending on the separator structure (based on DPRNN or DPTNET blocks). The stride size for \textbf{Base model} in both cases are set to be 8, which is consistent with all other models in ablation study. \item \textbf{Base-expanded}, which is similar to \textbf{Base model} with one difference: the encoding space in the coarse phase is expanded by scaling up the number of basis functions in the latent domain to exploit the limit of sufficiently large encoding space, namely using much more CNN filters ($N^c=1024$) for the encoder. 
\maRevise{
    \item \textbf{Base-deeper}, which has deeper encoder and decoder than \textbf{Base model}, to investigate the effect of the convolutional depth of both the encoder and decoder. In this variant, we deepen the encoder and the decoder with 2, 3 and 4 convolutional layers (1-D) respectively, and select the best performance as the optimal results w.r.t. the convolutional depth.  }
    \maRevise{
    \item \textbf{Base-high-order}, which directly encodes the mixed speech into high-order embedding space and perform speech separation in only one separation phase. Compared to our \textbf{\emph{SRSSN}}, no rough separation is performed in the 1-order embedding space. Hence, this variant is proposed to validate the effectiveness of coarse-to-fine separation scheme. Specifically, the encoder is modeled by cascading the coarse encoder $\mathcal{E}^c$ and refining encoder $\mathcal{E}^r$, to encode the speech signal into the fine-grained embedding space. The refining separator $\mathcal{S}^r$ and the refining decoder $\mathcal{D}^r$ are applied subsequently. 
    }
    \item \textbf{Iterative}, which adopts the iterative scheme used in iTDCN++~\cite{universal, improving}. Specifically, the separation procedure in \textbf{Base model} is repeated twice, where the mixed speech and the initial estimates in the first phase are fed into the second phase. Two phases are cascaded into an end-to-end model.
    \item \textbf{\emph{SRSSN}-1D}, which has both the coarse and refining phases but no Fine-grained Encoding Mechanism is used in the refining phase. The refining phase has the same model structure as the coarse phase. Thus both two phases encode features in a 1-order latent domain. Note that \textbf{\emph{SRSSN}-1D} is different from \textbf{Iterative} in that the refining phase of \textbf{\emph{SRSSN}-1D} accepts the coarsely separated latent representations as input whilst the second phase of \textbf{Iterative} takes the decoded estimated signals as the input.
    \item \textbf{\emph{SRSSN}-1D-expanded}, which is similar to \textbf{\emph{SRSSN}-1D} with one difference: the encoding space in the refining phase is expanded by scaling up the number of basis functions in the latent domain. Similar to \textbf{Base-expanded}, much larger number of CNN filters ($N^r=1024$) are used for the refining encoder.
\item \textbf{\emph{SRSSN}-$\mathcal{L}^r$}, which is the same as the proposed $\emph{SRSSN}$, except that the model is trained with only loss function $\mathcal{L}^r$ in the refining phase, discarding the loss function $\mathcal{L}^c$ in the coarse phase. 
    \item \textbf{\emph{SRSSN}}, which is our intact model: the Coarse-to-fine framework is applied and the Fine-grained Encoding Mechanism is leveraged to construct a fine-grained encoding space defined by a learned high-order latent domain, which enables fine-grained separation.
\end{itemize}
Since the separator accounts for most of model parameters, the total number of core blocks (DPRNN or DPTNET) in the separator for each variant is kept consistent for a fair comparison. Specifically, for the variants with only one separation phase including \textbf{Base model}, \textbf{Base-expanded}, \maRevise{\textbf{Base-deeper}, and \textbf{Base-high-order},} the number of core blocks $R$ is set to 6. For the variants with two separation phases, including \textbf{Iterative}, \textbf{\emph{SRSSN}-1D}, \textbf{\emph{SRSSN}-1D-expanded}, \textbf{\emph{SRSSN}-$\mathcal{L}^r$}, and \textbf{\emph{SRSSN}}, the number of blocks $R$ in both phases is set to 3. Figure~\ref{fig:ablation_groups} presents the experimental results of these \maRevise{nine} variants of our model with two different separator structures (DPRNN-based and DPTNET-based) for ablation study.

\maRevise{It should be noted that the convolutional sampling rate (on the input signal) in the refining phase of our model are actually equal to the coarse phase when setting the stride size in the refining phase to be 1, because the refining phase is performed on the output of the coarse phase. In all our implementation, the stride size in the coarse phase and refining phase are set to be 8 and 1 respectively, thus the overall stride size (on the input signal) of our \textbf{\emph{SRSSN}} after two phases is 8, which is equal to other methods in the ablation study. In such experimental settings, the comparisons between our model and other models in ablation study are fair.}



\begin{comment}
We first conduct experiments to investigate the effectiveness of the proposed Fine-grained Encoding Mechanism and Coarse-to-fine separation framework. To this end, we perform ablation study on four variants of our \emph{SRSSN}:\\
    1) \textbf{Base model}, which has only the coarse phase and thus no Fine-grained Encoding Mechanism is used. It is the same as DPRNN-TASNET~\cite{DPRNN} and we implement it using Asteroid toolkit~\cite{asteroid}.\\2) \textbf{SRSSN-1D}, which has both the coarse and refining phases but no Fine-grained Encoding Mechanism is used. The refining phase has exactly the same model structure as the coarse phase. Thus both two phases encode features in a 1-order latent domain.\\
    3) \textbf{SRSSN-1D-expanded}, which is similar to \textbf{SRSSN-1D} with one difference: the encoding space in the refining phase is expanded by scaling up the number of basis functions in the latent domain.\\
    4) \textbf{SRSSN}, which is our intact model.Figure~\ref{fig:ablation} presents the experimental results of these four variants of our model in two settings with different model complexities.\end{comment}

\begin{figure*}[t]
  \centering
  \begin{minipage}[b]{0.44\linewidth}
    \centering
    \includegraphics[width=\textwidth]{dprnn_bar_diff_data}
    \centerline{(a) Performance of using DPRNN as separator}\medskip
  \end{minipage}
  \hspace{0.03\linewidth}
  \begin{minipage}[b]{0.44\linewidth}
    \centering
    \includegraphics[width=\textwidth]{dpt_bar_diff_data}
    \centerline{(b) Performance of using DPTNET as separator}\medskip
  \end{minipage}
  \caption{\maRevise{Performance of our \textbf{\emph{SRSSN}} and \textbf{Base model} in terms of $\Delta$ SI-SNR and $\Delta$ SDR on different test subsets with different level of similarity between involved speakers in the mixed speech.}}
  \label{fig:diff_data}
\end{figure*}

\smallskip\noindent\textbf{Effect of Coarse-to-fine framework.}
For both DPRNN-based and DPTNET-based separator structures, the performance is improved significantly in both metrics from \textbf{Base model} to \textbf{\emph{SRSSN}-1D}, which manifests the remarkable advantages of our proposed Coarse-to-fine separation framework. Although the coarse phase and the refining phase of \textbf{\emph{SRSSN}-1D} have the same model structure especially with the same encoding scheme, the learned encoding features of two phases are able to adapt to different separation stages under the guidance of the loss functions during training. The performance comparison between \textbf{Iterative}~\cite{universal, improving} and \textbf{\emph{SRSSN}-1D} demonstrates the benefit of our proposed Coarse-to-fine framework. The strategy of progressive separation through multiple phases is also adopted in \textbf{Iterative}, where the initial estimates from the first phase serve as prior speaker information to improve the separation performance in the second phase. However, it needs to learn a new encoding space for separation from scratch. In our proposed Coarse-to-Fine method, the separated representations from the first phase are further separated in the second phase, where the more thorough encoding space is constructed based on the existing coarse encoding space.
The method \textbf{\emph{SRSSN}-$\mathcal{L}^r$} trained with only loss function in the refining phase obtained much lower performance than the final version. The reason is that the performance of the coarse phase degenerates notably without direct supervision by the loss function on it.

\maRevise{
The performance gain from \textbf{High-order} to \textbf{\emph{SRSSN}} also indicates the advantages of the progressive separation strategy through multiple phases defined in our proposed Coarse-to-fine framework.}




\smallskip\noindent\textbf{Effect of Fine-grained Encoding Mechanism.}
Our intact \textbf{\emph{SRSSN}} outperforms \textbf{\emph{SRSSN}-1D} substantially in both DPRNN-based and DPTNET-based cases, which indicates the effectiveness of the proposed Fine-grained Encoding Mechanism. Compared to the 1-order latent domain in \textbf{\emph{SRSSN}-1D}, the constructed high-order latent domain space in \textbf{\emph{SRSSN}} enables the model to perform separation in more fine-grained encoding space and achieve more precise separation results. 
To further explore the performance limit of 1-order latent domain and investigate the essential difference between 1-order and high-order latent domain space, we scale up the number of basis functions in the latent domain of \textbf{Base model} and \textbf{\emph{SRSSN}-1D} and compare our \textbf{\emph{SRSSN}} with the expanded models \textbf{Base-expanded} and \textbf{\emph{SRSSN}-1D-expanded}.  Figure~\ref{fig:ablation_groups} shows similar results in \textbf{Base-expanded} and \textbf{\emph{SRSSN}-1D-expanded}. Increasing the number of basis functions in 1-order latent space slightly improves the separation performance in the case of \textbf{\emph{SRSSN}-1D-expanded} at the cost of proportionally increasing the parameters of encoder. In the case of DPTNET-\emph{SRSSN}, the performance is even degenerated from \textbf{Base model} to \textbf{Base-expanded} due to potential overfitting in 1-order latent domain. Our model \textbf{\emph{SRSSN}} performs distinctly better than these expanded models. 

\maRevise{
As shown in Figure~\ref{fig:ablation_groups}, \textbf{Base-deeper}, which has deeper encoder and decoder than \textbf{Base model},  slightly improves the separation performance than \textbf{Base model}. However, it performs worse than \textbf{Base-high-order}, which reveals that the high-order encoding space modeled by our model is not equivalent to (but more powerful than) the deeper encoding space by \textbf{Base-deeper}. 
}
The high-order latent space constructed by the fine-grained encoding mechanism significantly improves the feature representation power with even less encoder parameters due to the constructed mechanism of high-order domain described in \ref{ssec:FGEM}.

\begin{comment}
\begin{table}[t]
  \centering
  \caption{\maRevise{Performance of different methods for speech separation in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) with different encoder configurations of kernel size and stride size.}}
  \label{tab:enc_kernel_stride}
\renewcommand{\arraystretch}{1.4}
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}
  {lcccc}
  \toprule
  Method & (Kernel, Stride) & Temporal resolution & $\Delta$SI-SNR $\uparrow$ & $\Delta$SDR $\uparrow$\\
  \midrule
  \multirow{4}{*}{DPRNN-TASNET} & (16, 8) & $T/8$ & 17.8 & 18.1 \\
  & (8, 4) & $T/4$ &  &  \\
  & (4, 2) & $T/2$ &  &  \\
  & (2, 1) & $T$ &  &  \\
  \midrule
  \multirow{2}{*}{DPRNN-\emph{SRSSN}} & (16, 8) - (2, 1) & $T/8$ & 20.0 & 20.2 \\
  & (8, 4) - (4, 2)  & $T/8$ & 20.3 & 20.5 \\
  \midrule
  \multirow{4}{*}{DPTNET-TASNET} & (16, 8) & $T/8$ & 18.3 & 18.6 \\
  & (8, 4) & $T/4$ &  &  \\
  & (4, 2) & $T/2$ &  &  \\
  & (2, 1) & $T$ & 20.2 & 20.6 \\
  \midrule
  \multirow{2}{*}{DPTNET-\emph{SRSSN}} & (16, 8) - (2, 1) & $T/8$ & 20.4 & 20.6 \\
  & (8, 4) - (4, 2)  & $T/8$ & 20.8 & 21.0 \\
  \bottomrule
  \end{tabular} }
\end{table}
\begin{figure*}
  \centering
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{memory_dprnn}
    \centerline{(a) Memory usage of DPRNN-TASNET and DPRNN-\emph{SRSSN}}\medskip
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{memory_dpt}
    \centerline{(b) Memory usage of DPTNET-TASNET and DPTNET-\emph{SRSSN}}\medskip
  \end{minipage}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{time_dprnn}
    \centerline{(c) Inference time of DPRNN-TASNET and DPRNN-\emph{SRSSN}}\medskip
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{time_dpt}
    \centerline{(d) Inference time of DPTNET-TASNET and DPTNET-\emph{SRSSN}}\medskip
  \end{minipage}
  \caption{}
  \label{fig:mem_time}
\end{figure*}
\maRevise{
\smallskip\noindent\textbf{Effect of encoder kernel size and stride size.} Table~\ref{tab:enc_kernel_stride} presents the performance of our models (\emph{SRSSN}-based) and the base models (TASNET-based) with different encoder configurations on kernel size and stride size. For \textbf{base model}, smaller stride size leads to higher performance, which is in line with the reported result in~\cite{DPRNN}. The rationale behind this result is that smaller stride size results in larger temporal resolution, which provides the potential for more precise separation. For both DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN}, larger temporal resolution in coarse phase allows more precise separation in coarse phase, which also benefits the final performance. 
We also compare the memory consumption and inference time between two versions of our model (DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN})  with their counterparts (DPRNN-TASNET and DPTNET-TASNET) using the same separator structure. 
Figure~\ref{fig:mem_time} presents the comparison results in inference mode on the same GPU (a single NVIDIA RTX 3090) of different models when separating mixed speech of different speech duration (in second). For the TASNET-based models, small stride size results in longer feature sequence, which leads to higher memory usage and more inference time. The stride size of is tuned to be 1 in DPRNN-TASNET and DPTNET-TASNET for optimal performance, consuming more memory and inference time than other models. Our method achieves much better performance with a larger stride than TASNET-based methods, which is favorable for devices with limited memory and computational efficiency.
}
\end{comment}

\begin{figure*}[!t]
\centering
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_legend}
  \end{minipage}
  \hspace{0.05\linewidth}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_bar}
  \end{minipage}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_dprnn_21593}
    \centerline{(a) Sample 1 in DPRNN-\emph{SRSSN}}\medskip
  \end{minipage}
  \hspace{0.05\linewidth}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_dptnet_21593}
    \centerline{(b) Sample 1 in DPTNET-\emph{SRSSN}}\medskip
  \end{minipage}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_dprnn_10023}
    \centerline{(c) Sample 2 in DPRNN-\emph{SRSSN}}\medskip
  \end{minipage}
  \hspace{0.05\linewidth}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_dptnet_10023}
    \centerline{(d) Sample 2 in DPTNET-\emph{SRSSN}}\medskip
  \end{minipage}
  \caption{Visualization of STFT power spectrums of our DPRNN-\emph{SRSSN} (left) and DPTNET-\emph{SRSSN} (right) in both coarse and refining phases on two randomly selected samples from test set. The contrasting regions are highlighted in green and blue boxes.}
  \label{fig:real_examples}
\end{figure*}

\begin{figure*}[!t]
\centering
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_diff_legend}
  \end{minipage}
  \hspace{0.05\linewidth}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_bar}
  \end{minipage}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_diff_p1_dprnn}
    \centerline{(a) Sample 1 in case of DPRNN}\medskip
  \end{minipage}
  \hspace{0.05\linewidth}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_diff_p1_dptnet}
    \centerline{(b) Sample 1 in case of DPTNET}\medskip
  \end{minipage}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_diff_p2_dprnn}
    \centerline{(c) Sample 2 in case of DPRNN}\medskip
  \end{minipage}
  \hspace{0.05\linewidth}
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \includegraphics[width=\textwidth]{spec_diff_p2_dptnet}
    \centerline{(d) Sample 2 in case of DPTNET}\medskip
  \end{minipage}
  \caption{\maRevise{Visualization of STFT power spectrums of our \textbf{\emph{SRSSN}} and \textbf{Base model} in case of DPRNN and DPTNET on two randomly selected samples from the subset of \textit{High similarity} in test set. The contrasting regions are highlighted in green and blue boxes.}}
  \label{fig:real_examples_diff}
\end{figure*}

\smallskip\noindent\textbf{\maRevise{Robustness of the proposed \textbf{\emph{SRSSN}}}.}
\maRevise{To evaluate the robustness of our \textbf{\emph{SRSSN}}, we divide the test data into separate subsets according to the similarity between involved two speakers and compare the performance between our \textbf{\emph{SRSSN}} and \textbf{Base model}. Typically, mixed speech with more similarities between speakers is harder to be separated. 
Specifically, we utilize a pre-trained speaker encoder model\footnote[1]{\maRevise{https://github.com/resemble-ai/Resemblyzer}}~\cite{speaker-enc} to extract the embedding vector of each involved speaker and calculate their cosine similarity for each sample. Higher cosine similarity indicates higher similarity of speech characteristics of involved speakers. We sort the samples in test set according to the similarity, and divide them into three subsets with equal number of samples, namely \textit{Low similarity}, \textit{Medium similarity} and \textit{High similarity}. Figure~\ref{fig:diff_data} presents the results on these three subsets and all samples. As the speaker similarity increases, the performance decreases for both models, which is reasonable. Our \textbf{\emph{SRSSN}} consistently outperforms \textbf{Base model} on all subsets in both metrics, which manifests the robustness of our \textbf{\emph{SRSSN}}.
}

\begin{table}[t]
  \centering
  \caption{\maRevise{Performance of different number of phases of stepwise separation in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) using DPRNN as separator.}}
  \label{tab:comparison_stages}
\renewcommand{\arraystretch}{1.4}
  \resizebox{0.99\linewidth}{!}{
  \begin{tabular}
  {lccccc}
  \toprule
  Method & Model size & GPU Memory usage & $\Delta$SI-SNR $\uparrow$ & $\Delta$SDR $\uparrow$ \\
  \midrule
  1-phase & 2.5M & 1.76Gib & 17.5 & 17.7 \\ 
  2-phase & 2.7M & 2.07Gib & 19.0 & 19.3 \\ 
  3-phase & 3.0M & 3.86Gib & 19.1 & 19.3 \\ 
  \bottomrule 
  \end{tabular} }
\end{table}

\begin{comment}
\begin{figure*}
  \centering
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{memory_stage}
    \centerline{(a) Comparison of GPU memory usage.}\medskip
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{time_stage}
    \centerline{(b) Comparison of inference time.}\medskip
  \end{minipage}
  \caption{\maRevise{Comparison of GPU memory usage and inference time as a function of input speech length at 8kHz sampling rate among methods of different number of phases using DPRNN as separator. The results are reported in inference mode on a single NVIDIA RTX 3090.}}
  \label{fig:memory_time_stage}
\end{figure*}
\end{comment}

\smallskip\noindent\textbf{\maRevise{Exploration of number of phases for stepwise separation.}}
\maRevise{Theoretically, our proposed Fined-grained Encoding Mechanism can be iteratively employed without limitation to constructed higher-order encoding space and perform more fine-grained speech separation. However, more times of iterations inevitably lead to the increase of the model complexity and computational cost but diminishing marginal performance gain. We conduct experiments to investigate the scalability of our \textbf{\emph{SRSSN}} with increasing separation phases: 1) \textbf{1-phase}, which is the same as \textbf{Base model}; 2) \textbf{2-phase}, which is the same as current version of our \textbf{\emph{SRSSN}}; 3) \textbf{3-phase}, which performs stepwise separation through 3 phases sequentially in the 1-order, 2-order, and 3-order encoding space, respectively. Due to the memory limit, we only perform experiments in the case of DPRNN separator and use fewer DPRNN blocks. For \textbf{1-phase}, we use 4 blocks. For \textbf{2-phase}, we use 2 blocks for both phases. For \textbf{3-phase}, we use 2 blocks for the first phase and 1 phase for the later two phases. Similar to the construction of the 2-order embedding space, the 3-order embedding space is constructed by the decomposition of the 2-order embedding space using our proposed Fined-grained Encoding Mechanism.}

\maRevise{Table~\ref{tab:comparison_stages} presents the separation performance of these versions of \textbf{\emph{SRSSN}}. The performance is improved significantly from \textbf{1-phase} to \textbf{2-phase} in terms of both metrics, whilst the performance gain is negligible from \textbf{2-phase} to \textbf{3-phase} in terms of $\Delta$SI-SNR. Such results implie that the 2-order latent domain constructed in \textbf{2-phase} suffices to provide a separable encoding space. Besides, the model size and GPU memory usage (on a single NVIDIA RTX 3090 when separating the 4-second speech) increases as the increases of the iterating times. These results are consistent with above theoretical analysis.}









\smallskip\noindent\textbf{Qualitative Evaluation.} To gain more insight into the effect of speech separation, we perform two sets of qualitative evaluation: 1) qualitative results by the coarse phase and refining phase and \maRevise{2) qualitative comparison between our \textbf{\emph{SRSSN}} and the \textbf{base model.}} In the first set of experiments, we randomly select two samples from the test set for DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN}, and employ the Librosa analysis toolkit~\cite{librosa} to visualize the STFT power spectrums of the estimated speech sources in both phases in Figure~\ref{fig:real_examples}. Comparing between the visualization of the groundtruth and estimates for each involved speaker, we observe that the separated results for one speaker in the coarse phase still contain residual ingredients from the other speaker, particularly in the regions indicated by bounding boxes. 
In contrast, the separated results in the refining phase is much better than that in the coarse phase: most of the incorrect residual ingredients are successfully removed. 
The estimates in the refining phase show more similar spectrum patterns as their groundtruth counterparts than that in the coarse phase. It indicates that the fine-grained embedding space defined by the high-order latent domain in refining phase enables a more precise separation.

\maRevise{In the second  set of qualitative experiments, we randomly select two samples from the subset of \textit{High similarity} (the most challenging subset) and compare between our \textbf{\emph{SRSSN}} and \textbf{Base model} qualitatively. Figure~\ref{fig:real_examples_diff} visualizes the STFT power spectrums of the estimated speech sources. It is clearly shown that our \textbf{\emph{SRSSN}} is able to perform a more precise speech separation than the base model.}






\begin{table}[!t]
\centering
  \caption{Performance of different methods for speech separation on WSJ0-2mix in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) in the clean setting.}
  \label{tab:comparison}
\setlength{\tabcolsep}{2.5pt} \renewcommand{\arraystretch}{1.4}
  \resizebox{1\linewidth}{!}{
  \begin{tabular}
{llccc}
  \toprule
  & Method & Model size & $\Delta$SI-SNR $\uparrow$ & $\Delta$SDR $\uparrow$ \\
  \midrule
  \multirow{4}{*}{\shortstack{Frequency \\domain-based}} & DPCL++~\cite{DPCL2}  & 13.6M & 10.8 & $-$ \\
  & UPIT-Bi-LSTM-ST~\cite{UPIT} & 92.7M & $-$ & 10.0 \\
  & Chimera++~\cite{Chimera++} & 32.9M & 11.5 & 12.0 \\
  & Deep CASA~\cite{CASA} & 12.8M & 17.7 & 18.0 \\
  \midrule
  \multirow{12}{*}{\shortstack{Learnable latent \\domain-based}} & Bi-LSTM-TASNET~\cite{BLSTM-TasNet} & 23.6M & 13.2 & 13.6 \\ 
  & Conv-TASNET~\cite{Conv-tasnet}  & 5.1M & 15.3 & 15.6 \\
  & E2EPF~\cite{post} & $-$ & 16.9 & 17.3 \\
  & FurcaNeXt~\cite{furcanext} & 51.4M & $-$ & 18.4 \\
  & DPRNN-TASNET~\cite{DPRNN} & 2.6M & 18.8 & 19.0 \\
  & SuDoRM-RF~\cite{sudo} & 2.6M & 18.9 & $-$ \\
  & Nachmani et al.~\cite{MULCAT} & 7.5M & 20.1 & $-$ \\
  & DPTNET-TASNET~\cite{DPTNet} & 2.7 M & 20.2 & 20.6 \\
  & SepFormer~\cite{SepFormer} & 26M & 20.4 & 20.5 \\
  & Wavesplit~\cite{wavesplit} & 29M & 21.0 & 21.2 \\
\cmidrule[0.5pt]{2-5}
  & DPRNN-\emph{SRSSN} (ours) & 7.5M & 20.5 & 20.7 \\
  & DPTNET-\emph{SRSSN} (ours) & 5.7M & \textbf{21.2} & \textbf{21.4} \\
  \midrule
& \maRevise{Wavesplit $+$ \textit{Data Augment}~\cite{wavesplit}} & 29M & 22.2 & 22.3 \\
  & \maRevise{SepFormer $+$ \textit{Data Augment}~\cite{SepFormer}} & 26M & 22.3 & 22.4 \\
  \bottomrule
  \end{tabular} }
\end{table}

\textit{2) Comparison with State-of-the-art Methods on WSJ0-2mix (involving 2 speakers):}
Next we conduct experiments to compare our model with state-of-the-art methods for speech separation on WSJ0-2mix dataset\cite{DPCL}. In particular, we compare our model with 2 types of methods: 1) methods performing separation in the frequency domain, including DPCL++~\cite{DPCL2}, UPIT-Bi-LSTM-ST~\cite{UPIT}, Chimera++~\cite{Chimera++} and Deep CASA~\cite{CASA}; 2) methods performing separation in a learnable latent domain 
in an end-to-end way, including Bi-LSTM-TASNET~\cite{BLSTM-TasNet}, Conv-TASNET~\cite{Conv-tasnet}, E2EPF~\cite{post},  FurcaNeXt~\cite{furcanext}, DRPNN-TASNET~\cite{DPRNN}, SuDoRM-RF~\cite{sudo}, Nachmani et al.~\cite{MULCAT}, DPTNET-TASNET~\cite{DPTNet}, SepFormer\cite{SepFormer} and Wavesplit~\cite{wavesplit}. We evaluate the performance of two versions of our \emph{SRSSN}: DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN}. The number of blocks in both coarse separator and refining separator $R$ is set to 6.

Table~\ref{tab:comparison} presents the experimental results of different models for speech separation on WSJ0-2mix dataset~\cite{wsj0} in terms of both $\Delta$SI-SNR and $\Delta$SDR. 
\begin{comment}
It should be noted that we report all results without using data augmentation for a fair comparison. 
\end{comment}
\maRevise{For the results without using data augmentation with dynamic mixing~\cite{wavesplit},} our DPRNN-\emph{SRSSN} outperforms all other methods except Wavesplit~\cite{wavesplit} in terms of both metrics while DPTNET-\emph{SRSSN} performs better than all other methods, which demonstrates advantages of our model. The methods which learn a separable encoding space defined by a latent domain, generally perform better than the other type of methods separating speech in frequency domain explicitly, which implies that the frequency domain is not necessarily the best separation space for speech as described in \cite{Conv-tasnet}. It is worth noting that our DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} outperform the original methods DPRNN-TASNET and DPTNET-TASNET by a large margin, respectively.

\begin{comment}
\begin{figure}[!t]
   \centering
   \includegraphics[width=0.4\textwidth]{memory}
   \caption{Comparison of GPU memory usage as a function of input speech length at 8Khz sampling rate. The results are reported in inference mode on a single NVIDIA RTX 3090.}
   \label{fig:memory}
\end{figure}
\end{comment}

\begin{figure*}
  \centering
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{memory}
    \centerline{(a) Comparison of GPU memory usage.}\medskip
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{time}
    \centerline{(b) \maRevise{Comparison of inference time.}}\medskip
  \end{minipage}
  \caption{Comparison of GPU memory usage \maRevise{and inference time} as a function of input speech length at 8kHz sampling rate. The results are reported in inference mode on a single NVIDIA RTX 3090.}
  \label{fig:memory_time}
\end{figure*}

SepFormer~\cite{SepFormer}, which is extended over DPTNET~\cite{DPTNet} by using deeper and wider Transformer layers, achieves better performance than DPTNET at the cost of much larger model size. Nachmani et al.~\cite{MULCAT} conducts an additional task to minimize the distance of the learned speaker embeddings between the estimates and the groundtruth, which is extracted by a speaker recognition model. Such extra supervision further improves the separation performance whilst it relies on extra datasets to train the speaker recognition model. Thus it utilizes extra data for training than our model. Wavesplit~\cite{wavesplit} learns speaker-discriminative vectors at each time step by leveraging the information of speaker identities in datasets to boost performance in the training process. Compare to Nachmani et al.~\cite{MULCAT}, SepFormer\cite{SepFormer}, and Wavesplit~\cite{wavesplit}, our method DPTNET-\emph{SRSSN} achieves more superior performance without using additional information in a lightweight way. The techniques used in these models can be readily integrated into our \emph{SRSSN}, leading to a more powerful speech separation system. 

We also compare the memory consumption \maRevise{and inference time} between two versions of our model (DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN}) with their TASNET-based counterparts (DPRNN-TASNET and DPTNET-TASNET) using the same separator structure. Figure~\ref{fig:memory_time} presents the comparison results in inference mode on the same GPU (a single NVIDIA RTX 3090) of four models when separating mixed speech of different speech duration (in second).
DPRNN-\emph{SRSSN} consumes slightly less memory than DPRNN-TASNET~\cite{DPRNN}, while DPTNET-\emph{SRSSN} reduces much more memory usage compared to DPTNET-TASNET~\cite{DPTNet}, as the length of input speech increases.
The major decrease of memory consumption lies in the larger stride size of encoder in our models than the TASNET-based methods. As reported in \cite{DPRNN}, smaller stride size of encoder leads to better performance. 
However, smaller stride size results in longer feature sequence, which requires more floating-point operations and memory usage. The stride size of encoder is tuned to be 1 in DPRNN-TASNET~\cite{DPRNN} and DPTNET-TASNET\cite{DPTNet} for optimal performance, while it is set to 8 in our coarse encoder. Our method achieves much better performance with a larger stride than TASNET-based methods, which is favorable for devices with limited memory.

\maRevise{Regarding the inference time, DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} both require slightly more inference time than their TASNET-based counterparts due to two separation phases in our \emph{SRSSN}}.







\begin{table}[!t]
\centering
  \caption{\maRevise{Performance of different methods for speech separation on WSJ0-3mix in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) in the clean setting.}}
  \label{tab:comparison_3spk}
\setlength{\tabcolsep}{2.5pt} \renewcommand{\arraystretch}{1.4}
  \resizebox{1\linewidth}{!}{
  \begin{tabular}
{llccc}
  \toprule
  & Method & Model size & $\Delta$SI-SNR $\uparrow$ & $\Delta$SDR $\uparrow$ \\
  \midrule
  \multirow{2}{*}{\shortstack{Frequency \\domain-based}} & DPCL++~\cite{DPCL2}  & 13.6M & 7.1 & $-$ \\
  & UPIT-Bi-LSTM-ST~\cite{UPIT} & 92.7M & $-$ & 7.7 \\
  \midrule
  \multirow{11}{*}{\shortstack{Learnable latent \\domain-based}} & E2EPF~\cite{post} & $-$ & 12.5 & 13.0 \\
  & Conv-TASNET~\cite{Conv-tasnet} & 5.1M & 12.7 & 13.1 \\
  & DPRNN-TASNET~\cite{DPRNN} & 2.6M & 15.7 & 16.0 \\
  & DPTNET-TASNET~\cite{DPTNet} & 2.7 M & 16.2 & 16.5 \\
  & Nachmani et al.~\cite{MULCAT} & 7.5M & 16.9 & $-$ \\
  & Wavesplit~\cite{wavesplit} & 29M & 17.3 & 17.6 \\
  & SepFormer~\cite{SepFormer} & 26M & 17.6 & 17.9 \\
  \cmidrule[0.5pt]{2-5}
  & DPRNN-\emph{SRSSN} (ours) & 7.5M & 18.8 & 19.0 \\
  & DPTNET-\emph{SRSSN} (ours) & 5.7M & \textbf{19.4} & \textbf{19.6} \\
  \midrule
& Wavesplit $+$ \textit{Data Augment}~\cite{wavesplit} & 29M & 17.8 & 18.1 \\
  & SepFormer $+$ \textit{Data Augment}~\cite{SepFormer} & 26M & 19.5 & 19.7 \\
  \bottomrule
  \end{tabular} }
\end{table}

\maRevise{
\textit{3) Comparison with State-of-the-art Methods on WSJ0-3mix (involving 3 speakers):}
Next we conduct experiments to compare our model with state-of-the-art methods for speech separation on WSJ0-3mix dataset\cite{DPCL} involving 3 speakers, which is more challenging than the scenario with 2 speakers. In particular, we compare our model with 2 types of methods: 1) methods performing separation in the frequency domain, including DPCL++~\cite{DPCL2}, UPIT-Bi-LSTM-ST~\cite{UPIT}; 2) methods performing separation in a learnable latent domain, including E2EPF~\cite{post}, Conv-TASNET~\cite{Conv-tasnet}, DRPNN-TASNET~\cite{DPRNN}, DPTNET-TASNET~\cite{DPTNet}, Nachmani et al.~\cite{MULCAT}, Wavesplit~\cite{wavesplit}, and SepFormer\cite{SepFormer}. We evaluate the performance of two versions of our \emph{SRSSN}: DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN}. The number of blocks in both coarse separator and refining separator $R$ is set to 6. 
}

\maRevise{
Table~\ref{tab:comparison_3spk} presents the experimental results of different models for speech separation on WSJ0-3mix dataset~\cite{DPCL} in terms of both $\Delta$SI-SNR and $\Delta$SDR. Our DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} both outperform all other methods without using data augmentation by a large margin. DPTNET-\emph{SRSSN} performs slightly worse than SepFormer using data augmentation~\cite{SepFormer}, which manifests the significant advantages of our \emph{SRSSN}. It is worth noting that our DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} both outperform their original methods DPRNN-TASNET and DPTNET-TASNET significantly.
}

\begin{table}[!t]
  \centering
  \caption{Performance of Speech Separation by different methods in terms of $\Delta$SI-SNR (dB) and $\Delta$SDR (dB) on WHAM! and WHAMR! in noisy and reverberant settings.}
  \label{tab:comparison_noisy}
  \setlength{\tabcolsep}{2pt} \renewcommand{\arraystretch}{1.4}
  \resizebox{1\linewidth}{!}{
  \begin{tabular}
{lcccc}
  \toprule
  \multirow{2}{*}{Method} & \multicolumn{2}{c}{WHAM!} & \multicolumn{2}{c}{WHAMR!} \\
  & $\Delta$SI-SNR $\uparrow$ & $\Delta$SDR $\uparrow$ & $\Delta$SI-SNR $\uparrow$ & $\Delta$SDR $\uparrow$ \\
  \midrule
Chimera++~\cite{WHAM} & 9.9 & $-$ & $-$ & $-$ \\
  Bi-LSTM-TASNET~\cite{WHAMR} & 12.0 & $-$ & 9.2 & $-$ \\
  Conv-TASNET~\cite{filterbank, WHAMR} & 12.7 & $-$ & 8.3 & $-$ \\
  Learnable fbank~\cite{filterbank} & 12.9 & $-$ & $-$ & $-$ \\
  Cascaded-Bi-LSTM-TASNET~\cite{WHAMR} &  12.9 & $-$ & 10.8 & $-$ \\
  DPRNN-TASNET~\cite{MULCAT} & 13.9 & $-$ & 10.3 & $-$ \\
  DPTNET-TASNET & 14.9 & 15.3 & 12.1 & 11.1 \\
  Nachmani et al.~\cite{MULCAT} & 15.2 & $-$ & 12.2 & $-$ \\
  Wavesplit~\cite{wavesplit} & 15.4 & 15.8 & 12.0 & 11.1 \\
\midrule
  DPRNN-\emph{SRSSN} (ours) & 15.7 & 16.1 & \textbf{12.3} & \textbf{11.4} \\
  DPTNET-\emph{SRSSN} (ours) & \textbf{16.1} & \textbf{16.5} & \textbf{12.3} & 11.3 \\
  \midrule
   \maRevise{Wavesplit $+$ \textit{Data Augment}~\cite{wavesplit}} & 16.0 & 16.5 & 13.2 & 12.2 \\
  \bottomrule
  \end{tabular} }
\end{table}

\subsection{Speech separation in Noisy and Reverberant Settings}

In this set of experiments, we conduct experiments in noisy and reverberant settings to validate the robustness of our proposed \emph{SRSSN}.

\smallskip\noindent\textbf{Datasets.}
We perform experiments on WSJ0 Hipster Ambient Mixtures (WHAM!) dataset~\cite{WHAM} and WHAMR! dataset~\cite{WHAMR}, which are constructed based on WSJ0-2mix dataset~\cite{DPCL}. In WHAM!, each two-speaker utterance from WSJ0-2mix dataset is mixed with a unique noise sample, which is recorded in non-stationary ambient environments such as coffee shops, restaurants and bars. The random SNR value between the first (louder) speaker and the noise is sampled from a uniform distribution between $-$6 dB and $+$3 dB. To separate the clean signals for involved speakers from such noisy speech data, the models are required to perform not only speech separation but also denoising. WHAMR!~\cite{WHAMR} is an reverberant extension of WHAM!, in which synthetic reverberation noise is further fused into the input speech data. Thus dereverberation is also required for this data to perform thorough speech separation.

We compare our proposed DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} with state-of-the-art methods for speech separation in noisy and reverberant settings: Chimera++~\cite{WHAM}, Bi-LSTM-TASNET~\cite{WHAMR},   Conv-TASNET~\cite{filterbank, WHAMR}, 
Learnable fbank~\cite{filterbank}, 
Cascaded-Bi-LSTM-TASNET~\cite{WHAMR},
DPRNN-TASNET~\cite{MULCAT}, 
DPTNET-TASNET~\cite{DPTNet},
Nachmani et al.~\cite{MULCAT} and Wavesplit~\cite{wavesplit}.
Note that Cascaded-Bi-LSTM-TASNET~\cite{WHAMR} is specifically designed to adapt to the noisy and reverberant conditions. 

Table~\ref{tab:comparison_noisy} shows that our DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} outperform other methods without using data augmentation by a large margin under noisy and reverberant conditions.
In particular, our method performs distinctly better than the cascaded model Cascaded-Bi-LSTM-TASNET~\cite{WHAMR}, which is equipped with the denoising and dereverberation functions. It manifests our model is generalized well to the noisy and reverberant conditions without specific design for adaptation.

\begin{table}[t]
  \centering
  \caption{Performance of speech recognition as well as speech separation on Libri2Mix dataset.}
  \label{tab:comparison_libri}
\renewcommand{\arraystretch}{1.4}
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}
  {lccc}
  \toprule
  Method & $\Delta$SI-SNR (dB) $\uparrow$ & $\Delta$SDR (dB) $\uparrow$ & WER (\%) $\downarrow$\\
  \midrule
  Bi-LSTM-TASNET & 13.5 & 13.9 & 30.8 \\ 
  Conv-TASNET  & 14.4 & 14.7 & 27.4 \\
  DPRNN-TASNET & 16.1 & 16.6 & 23.8\\
  DPTNET-TASNET & 16.7 & 17.1 & 22.4 \\
  \midrule
  DPRNN-\emph{SRSSN (ours)} & 17.3 & 17.7 & 22.1 \\
  DPTNET-\emph{SRSSN} (ours) & \textbf{18.3} & \textbf{18.6} & \textbf{20.6} \\
  \midrule
\textit{Target signal} & $-$ & $-$ & 15.6 \\
  \textit{Mixed signal} & $-$ & $-$ & 95.7 \\
  \bottomrule
  \end{tabular} }
\end{table}

\subsection{Speech Recognition on Separated speech}

We further conduct experiments of speech recognition on separated speech signals decoded by methods for speech separation to evaluate the performance of speech separation indirectly.  To be specific, we first perform speech separation on a mixture of speech dataset, then we perform speech recognition using a standard Automatic Speech Recognition (ASR) model on the separated speech signals by different speech separation models, respectively. The achieved performance of speech recognition on the separation results can be considered as an indirect evaluation measurement for the corresponding model for speech separation.

\noindent\textbf{Dataset} 
We conduct experiments on a recently released and fully open-source dataset Libri2Mix~\cite{librimix} for speech recognition. Libri2Mix is generated based on the ASR dataset LibriSpeech~\cite{librispeech} by mixing randomly selected speech utterances from different speakers. 
We use the speech data in the clean condition with sampling rate of 8kHz. It consists of two modes \textit{min} and \textit{max}. In the \textit{min} mode, the longer utterance is trimmed to align the shorter utterance. In the \textit{max} mode, the shorter utterance is padded with zeros to align the longer utterance. We train the models for speech separation in the \textit{min} mode of the \textit{train-100} subset, and perform test of speech separation in the \textit{max} mode. The separated signals in the test phase are further used for performing experiments of speech recognition.

We use the standard hybrid DNN-HMM framework~\cite{DNN-HMM} as the ASR model to perform speech recognition, implemented based on Kaldi open-source toolkit~\cite{kaldi}. The DNN acoustic model with p-norm non-linearities~\cite{pnorm} is trained on the top of fMLLR features and the forced alignment of the training data is produced by a GMM-HMM model~\cite{compact}. A 4-gram language model is utilized for rescoring. The ASR model is trained on the subset \textit{train-clean-100} of LirbiSpeech~\cite{librispeech}, following the official kaldi implementation \footnote[1]{https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech}. We compare our proposed DPRNN-\emph{SRSSN} and DPTNET-\emph{SRSSN} with following state-of-the-art models for speech separation: Bi-LSTM-TASNET~\cite{BLSTM-TasNet}, Conv-TASNET~\cite{Conv-tasnet}, DPRNN-TASNET~\cite{DPRNN}, DPTNET-TASNET~\cite{DPTNet}. 

Table~\ref{tab:comparison_libri} presents the performance of speech recognition. Besides, we also the report the experimental results of speech separation. \textit{Target signal} and \textit{Mixed signal} denote the ASR results on the target signals and original mixed (unseparated) signals respectively, which can be viewed as the upper bound and the lower bound for the speech recognition by the same ASR model. 
Our model achieves the best performance in both speech recognition and speech separation. Besides, it is shown that the performance of speech recognition is consistent with the performance of speech separation: better speech separated results lead to higher performance of speech recognition, which reveals the effectiveness of such indirect evaluation way, namely performing speech recognition on the separated speech signals.




\begin{comment}
\begin{table}[t]
  \centering
  \caption{Separation and recognition performance on Libri2Mix}
  \label{tab:comparison_libri}
\renewcommand{\arraystretch}{1.4}
  \resizebox{1\linewidth}{!}{
  \begin{tabular}
{lcccc}
  \toprule


  Method & $\Delta$SI-SNR & $\Delta$SDR & WER \\
  
\hline
  Bi-LSTM-TASNET & 13.0 & 13.4 & 13.5 & 13.9 & -0.641 \\ 
  Conv-TASNET & 13.7 & 14.1 & 14.4 & 14.7 & -0.674 \\
  DPRNN-TASNET  & 15.8 & 16.3 & 16.1 & 16.6 & -0.715\\
  DPTNET-TASNET & 15.9 & 16.3 & 16.7 & 17.1 & -0.725 \\
  IBM (oracle) & - & - & 13.2 & 13.5 & - \\
  IRM (oracle) & - & - & 13.7 & 13.9 & - \\
\hline
  DPRNN-SRSSN & 16.8 & 17.3 & 16.8 & 17.2 &  \\
  DPTNET-SRSSN & 17.5 & 18.1 & 18.3 & 18.6 & -0.745 \\
  \hline
  \hline
\end{tabular} }
\end{table}
\end{comment}
\begin{comment}
\begin{table}[t]
  \centering
  \caption{Enhancement on WHAM! and WHAMR!}
  \label{tab:comparison}
  \setlength{\tabcolsep}{1.2pt} \resizebox{1\linewidth}{!}{
  \begin{tabular}
  {l p{5em}<{\centering} p{5em}<{\centering} p{5em}<{\centering}       
  p{5em}<{\centering}}
  \toprule
  Method & SI-SNR & $\Delta$SI-SNR & SDR & $\Delta$SDR \\
  \midrule
  Conv-TASNET~\cite{Conv-tasnet}  & - & - & - & - \\
  DPRNN-TASNET~\cite{DPRNN}  & - & - & - & - \\
  DPTNET-TASNET~\cite{DPTNet}  & - & - & - & - \\
  \midrule
  DPRNN-SRSSN & - & - & - & - \\
  DPTNET-SRSSN & - & - & - & - \\
  \bottomrule
  \end{tabular} }
\end{table}
\end{comment}
    
\begin{comment}
In this study, we focus on two-speaker speech separation.
The dataset WSJ0-2mix \cite{DPCL} sampled at 8 kHz is used to evaluate our proposed separation algorithm, which is widely used in two-speaker mixture separation experiments.
The WSJ0-2mix consists of 30 hours of training data, 10 hours of validation data, and 5 hours of test data, generated using the Wall Street Journal (WSJ) dataset \cite{wsj0}.
The two-speaker mixtures are generated by mixing the utterances from two different speakers at random signal-to-noise ratio (SNR) between $-$5 dB and 5 dB.
The speakers in the test set are different from the training set and validation set.

\subsection{Training and Evaluation Details}
\label{ssec:details}

Our models are implemented based on the Asteroid \cite{asteroid}, a PyTorch-based audio source separation toolkit.
All models are trained for 200 epochs on 2-second long segments with a batch size of 6.
We use Adam optimizer ~\cite{adam} with a learning rate of $10^{-3}$ and a weight decay of $10^{-5}$.
We clip gradients with $L_2$-norm above 5 to avoid exploding gradients.
For the encoder $\mathcal{E}^c$ in the coarse phase, the number of filters $N^c$, kernel size $K^c$, and stride size are set to 64, 16, and 8, respectively.
For the encoder $\mathcal{E}^r$ in the refining phase, the number of filters $N^r$, kernel size $K^r$, stride size, and number of groups $m$ are set to 64, 2, 1, and 8, respectively.
In both separators $\mathcal{S}^c$ and $\mathcal{S}^r$ implemented by stacking $R$ blocks of DPRNN~\cite{DPRNN}, each Bi-LSTM layer contains 128 hidden units in each direction and the chunk size is set to 100 frames.
The performance of all models is evaluated using the SI-SNR~\cite{SISNR} improvement (SI-SNRi) and signal-to-distortion ratio (SDR)~\cite{SDR} improvement (SDRi).
They are both the metrics calculated using the estimate minus the metrics calculated using the mixture.

\subsection{Results}
\label{ssec:results}










In order to understand the specific contribution of the components in our proposed method, the following ablation studies are conducted.
1) Baseline: we reproduce the DPRNN-TASNET \cite{DPRNN} implemented with the Asteroid toolkit \cite{asteroid}.
2) Iterative: we implement an iterative version of DPRNN. As described in the study\cite{universal}, the initial estimates of the first iteration and the mixture serve as input for the second iteration to obtain the final estimates.
3) PMDSN (2D): we train the PMDSN network where the refining separation phase is performed in a 2-D space $T\text{-}F'$, in which the original learnable dimension $F$ in the coarse phase is replaced with a new learnable dimension $F'$.
Specifically, the number of groups $G$ described in Section \ref{ssec:separation_3d} is set to 1. 
Then the separation sapce in the refining degenerates into the 2-D space, in which each time step is represented in one dimension $F'$. 
The same separation and merging operations are performed as described \ref{ssec:separation_3d}, except for the sizes of representations.
4) PMDSN (3D): we train the PMDSN network where the refining separation phase is performed in a 3-D place $T\text{-}F\text{-}H$ with different groups $G=2, 4, 8$ as described in Section \ref{ssec:separation_3d}.

Figure \ref{fig:ablation_groups} shows the performance of each model with different number of DPRNN blocks: $R=3, 6$.
As can be seen, compared with baseline, the iterative and coarse-to-fine strategies have both obtained higher performance, which indicates the importance of the secondary separation.
The coarse-to-fine framework has achieved a more significant improvement in results.
This indicates the superiority of the coarse-to-fine method over the iterative method, although they are both twice as deep as a single-stage method.
In the refining stage, the ways of separation in 3-D space perform better than in 2-D space. 
The optimal performance is achieved by the PMDSN model using the configuration of $R=6, G=8$.
Compared with the method in the 2-D space, the method in the 3-D space has an extra dimension, which allows the neural network to distinguish between speakers by one more latent dimension in the hidden space without increasing the model parameters.




We compare our proposed SRSSN with the two groups of methods. 
The difference between them lies in the separation space. 
The former performs separation in the frequency domain, and the latter performs separation in a learnable latent space.
1) The first group includes:
DPCL++~\cite{DPCL2} which learns speaker-discriminative embeddings for each time-frequency (t-f) bin and performs the clustering to obtain an assignment of each t-f bin to one of the speakers;
UPIT-Bi-LSTM-ST~\cite{UPIT} which uses bidirectional long short-term memory (Bi-LSTM) to estimate masks to separate the representations in frequency domain and directly minimizes the prediction error with the utterance-level PIT technique;
ADANet~\cite{ADANet} which learns centroids for each speakers in the embedding space and forces t-f embeddings to be close to their corresponding speaker centroid;
Chimera++~\cite{Chimera++} which integrates the deep clustering network and the mask inference network into a multi-task framework;
Deep CASA~\cite{CASA} which first performs frame-level spectra separation and then sequentially groups the frame-level separated spectra to different speakers through clustering.
2) The second group includes:
TASNET-based methods following the encoder-separator-decoder framework consisting of Bi-LSTM-TasNet~\cite{BLSTM-TasNet}, Conv-TASNET~\cite{Conv-tasnet}, FurcaNeXt~\cite{furcanext}, and DPRNN-TASNET~\cite{DPRNN};
the method by Nachmani et al.~\cite{MULCAT} which directly estimates the speech sources after each gated-DPRNN block without estimating masks and further conduct a task to minimize the distance between the speaker embeddings of the estimates and targets extracted by a pretrained speaker recognition model;
Iterative DPRNN-TASNET: we implement an iterative version~\cite{universal, improving} of DPRNN-TASNET. As described in~\cite{universal, improving}, the initial estimates of the first iteration and the mixture serve as input for the second iteration to obtain the final estimates. Specifically, for each iteration, the network hyperparameters are set as same as in the coarse phase of our SRSSN.

Table \ref{table_2} shows the SI-SNRi and SDRi as well as their numbers of parameters.
SRSSN with the configuration of $R=3$ performs slightly better compared with the Iterative DPRNN-TASNET, and performs slightly worse compared with the state-of-the-art method by Nachmani et al.~\cite{MULCAT}, through the model size of SRSSN ($R=3$) is almost half of theirs.
SRSSN ($R=6$) outperforms all the previous methods.
\end{comment}