\documentclass{eptcs}
\providecommand{\event}{COS 2013} \usepackage{breakurl}             \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{bussproofs}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}

\newcommand{\axc}[1]{\AxiomC{#1}}
\newcommand{\uic}[2]{\RightLabel{\small{#2}}\UnaryInfC{#1}}
\newcommand{\bic}[2]{\RightLabel{\small{#2}}\BinaryInfC{#1}}
\newcommand{\tic}[2]{\RightLabel{\small{#2}}\TrinaryInfC{#1}}

\newcommand{\hyp}{\mathsf{hyp}}
\newcommand{\wkn}[1]{\mathsf{wkn}{(#1)}}
\newcommand{\inl}[1]{\mathsf{inl}{(#1)}}
\newcommand{\inr}[1]{\mathsf{inr}{(#1)}}
\newcommand{\lam}[1]{\mathsf{lam}{(#1)}}
\newcommand{\reset}[1]{\mathsf{reset}{(#1)}}
\newcommand{\shift}[1]{\mathsf{shift}{(#1)}}
\newcommand{\casemy}[3]{\mathsf{case}({#1},{#2},{#3})}
\newcommand{\app}[2]{\mathsf{app}({#1},{#2})}

\DeclareMathOperator{\Nil}{nil}
\DeclareMathOperator{\Cons}{cons}
\DeclareMathOperator{\Unit}{Unit}
\DeclareMathOperator{\one}{tt}
\DeclareMathOperator{\Normal}{Normal}
\DeclareMathOperator{\Neutral}{Neutral}
\DeclareMathOperator{\Type}{Type}
\DeclareMathOperator{\Bool}{Bool}
\DeclareMathOperator{\Formula}{Formula}
\DeclareMathOperator{\List}{List}
\DeclareMathOperator{\Syn}{Syntax}
\DeclareMathOperator{\Val}{Value}
\DeclareMathOperator{\Ans}{Answer}
\DeclareMathOperator{\Fst}{fst}
\DeclareMathOperator{\Snd}{snd}
\DeclareMathOperator{\FV}{FV}
\newcommand{\normal}{{\!\!\text{nf}}}
\newcommand{\neutral}{{\!\!\text{ne}}}
\newcommand{\forces}[3]{{#1}\Vdash_{#2}{#3}}
\newcommand{\sforces}[3]{{#1}\Vdash^{\text{s}}_{#2}{#3}}
\newcommand{\run}[1]{\text{run}{(#1)}}
\newcommand{\ret}[1]{\text{return}{(#1)}}
\newcommand{\bind}[2]{\text{bind}{(#1,#2)}}
\newcommand{\nil}{{\Nil}}
\newcommand{\cons}[2]{{\Cons{(#1,#2)}}}
\newcommand{\Universal}{\mathcal{U}}
\newcommand{\lsub}[1]{{{_#1}}\!\!\!}
\newcommand{\lsup}[1]{{{^#1}}\!\!\!}
\newcommand{\reify}[4]{\lsup{#1}\downarrow_{#2}^{\!{#3}}({#4})}
\newcommand{\reflect}[4]{\lsup{#1}\uparrow_{#2}^{\!{#3}}({#4})}
\newcommand{\eval}[2]{\llbracket{#1}\rrbracket_{#2}}
\newcommand{\Gammareflect}[2]{\text{reflect}({#1},{#2})}
\newcommand{\ureset}[1]{\langle{#1}\rangle}
\newcommand{\ushift}[2]{\mathcal{S}{#1}.{#2}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{definition*}{Definition}
\theoremstyle{plain}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{corollary*}{Corollary}
\theoremstyle{remark}
\newtheorem{remark}[definition]{Remark}
\newtheorem{example}[definition]{Example}
\newtheorem{fact}[definition]{Fact}
\newtheorem*{remark*}{Remark}

 
\title{Type Directed Partial Evaluation for Level-1 Shift and Reset} 
\author{Danko Ilik\thanks{This work is covered by a Kurt G\"odel Research Prize Fellowship 2011}
  \institute{Laboratory for Complex Systems and Networks\\Macedonian Academy of Sciences and Arts\\Skopje, Macedonia}
  \email{danko.ilik@gmail.com}
}

\def\titlerunning{Type Directed Partial Evaluation for Level-1 Shift and Reset}
\def\authorrunning{Danko Ilik}
\begin{document}
\maketitle

\begin{abstract} 
  We present an implementation in the Coq proof assistant of type
  directed partial evaluation (TDPE) algorithms for call-by-name and
  call-by-value versions of shift and reset delimited control
  operators, and in presence of strong sum types. We prove that the
  algorithm transforms well-typed programs to ones in normal
  form. These normal forms can not always be arrived at using the so
  far known equational theories. The typing system does not allow
  answer-type modification for function types and allows delimiters to
  be set on at most one atomic type. The semantic domain for
  evaluation is expressed in Constructive Type Theory as a dependently
  typed monadic structure combining Kripke models and continuation
  passing style translations.  
\end{abstract}


\section{Introduction}

Type directed partial evaluation (TDPE) is a technique that partially evaluates a program by first compiling it, and pre-computing known (``static'') input data on the fly, and then decompiling it to normal form in an efficient process driven by the program's type. It was discovered by Danvy \cite{Danvy1999} in Programming Languages Theory, although the exact same algorithm had been isolated at about the same time also in the study of typed lambda calculi and in Logic: Berger and Schwichtenberg \cite{BergerS1991} found it while looking for an efficient procedure for reducing \emph{open} lambda terms and called it Normalization by Evaluation (NBE); Catarina Coquand \cite{CCoquand1993} realized that it is the procedure behind the proof of completeness of minimal intuitionistic logic (without  and ) with respect to Kripke models.

However, when one moves from simply typed lambda calculus towards richer programming languages, to extend the TDPE method to cope with the new constructs does not appear to be straightforward. Already adding strong sum types seems to require one to implement TDPE using delimited control operators -- indeed, this is one of the more important applications of Danvy and Filinski's operators shift and reset \cite{DanvyF1990}. In turn, when considering TDPE for a language extended with the delimited control operators themselves, there has only been preliminary work on the subject, for the call-by-value case, by Tsushima and Asai \cite{TsushimaAsai2009}.

In this paper, we consider TDPE for the first level of the shift and reset hierarchy. Using their simpler non-extended CPS semantics, we build a type-theoretic framework that acts as a specification for TDPE algorithms (Section~\ref{sec:model}). The algorithms themselves, for both call-by-value and call-by-name, are given in Section~\ref{sec:algorithm}, where we also look at specific examples and compare their partial evaluations to the ones predicted by the known equational theories. In the concluding Section~\ref{sec:conclusion}, we give further explanation about our implementation and about the related works.

The Coq implementation of the algorithms can be found at the address \href{http://dankoi.github.com/metamath/}{dankoi.github.com/metamath}. Originally, this work was conceived as an alternative normalization proof for the core logical system from \cite{Ilik2010}, a proper constructive extension of intuitionistic logic with delimited control operators.

\section{Type-theoretic Model}\label{sec:model}

The programming language that we want to partially evaluate, our \emph{object language}, will be the lambda calculus with function and sum types and the shift/reset delimited control operators, described in Table~\ref{tab:typing}. We do not work with the most general known typing system for shift and reset in which implication is a quaternary connective \cite{DanvyF1989} and we allow a delimiter () to be set only at an atomic type ().

\begin{table}
  \centering
  \begin{tabular}{ m{7cm} m{7cm} }
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
    &
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
    \\
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
    &
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
    \\
    \multicolumn{2}{ m{14cm} }{
      \begin{prooftree}
        \axc{}
        \axc{}
        \axc{}
        \tic{}{~}
      \end{prooftree}
    }
    \\
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
    &
    \begin{prooftree}
      \axc{}
      \axc{}
      \bic{}{~}
    \end{prooftree}
    \\
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
    &
    \begin{prooftree}
      \axc{}
      \uic{}{~}
    \end{prooftree}
  \end{tabular}
  
  \caption{A typing system for lambda calculus with sum types and shift and reset, where variable binding is handled using deBruijn indices ( and )}
  \label{tab:typing}
\end{table}

For expressing variable binding, we rely on deBruijn indices in the form of  and  rules, where  can be thought of as zero and  as the successor. Lambda abstraction () and control () are therefore unary.

The turnstile ``'' is annotated by a Boolean  of value 0 or 1, value 1 meaning that a delimiting  has been previously applied in the lambda term (typing tree derivation). All rules, except for  and  ignore this annotation . The rule  sets it to 1, and  can only be used if the annotation has been previously set to 1 i.e. in a delimited sub-term.

The idea behind every TDPE algorithm is the following: we want to transform a program written in the object language to a meta-level ``bytecode'' version of it, ``run'' this bytecode  (this is called the \emph{evaluation} phase), and then, based on the program's type, recover a program in the object language that is already in normal form (the \emph{reification} phase) and -equal to the starting one. In other words, one relies on normalization at the meta-level, to produce an object-level normal form. This becomes non-trivial if the meta-level and the object level are not essentially the same, like in our case where the meta-level has no control feature while the object-level does.

The essential choice to make is what to choose for the ``semantic'' meta-level structure that evaluation will take place in. CPS semantics imposes itself, because it is the orthodox and simplest way to specify shift and reset  \cite{DanvyF1992}. The TDPE will thus be of the form of the two-phase transformation,

where  denotes the type of programs of the object language, and  and  are ``values'' and ``answers'' of a ``continuation'' in the usual terminology \cite{Danvy1999}. If we also want the transformation to account for \emph{open} terms (which allows to do normalization below a binder) and to guarantee that the input and output programs are actually programs of the same type, we need to enrich the  semantic domain (bytecode) by a pre-order, keeping track of a context  denoting open variables, and a parameter  corresponding to the type of the transformed program. 
We obtain the statement

where  denotes the semantic domain that is the target of evaluation (Subsection~\ref{subsec:soundness}), and source of reification (Subsection~\ref{subsec:completeness}). 

\subsection{Evaluating into the Models}\label{subsec:soundness}

We will use a combination of Kripke-CPS models for classical logic (used previously with Lee and Herbelin for proving NBE for the classical sequent calculus LK \cite{IlikLH2010}) and those for intuitionistic logic (used for proving NBE for intuitionistic natural deduction with  and  \cite{Ilik2011}). We give the mathematical definitions, trying to be precise but as informal as possible -- the interested reader may find the fully formal version in the Coq implementation -- keeping also in mind that, while we do use dependent types, the dependencies are rather weak (-types over the small set of formulas, booleans, and the type ).

\begin{definition}A \emph{Kripke CPS structure} is given by a type , a relation  that is a preorder, i.e. both of

hold, and a relation

with the properties:


 is the type of booleans with inhabitants 0 (false) and 1 (true), and  is the order on booleans defined by the relation of less-than-or-equal of their numerical values. ``'' denotes the type universe of the meta-language, while  is the type of types of the object language i.e. those built from , , atomic types, and the special fixed type  that reset can be set on -- we do not make the usual assumption that  denotes the empty type, it is simply a notation for a chosen atomic type.

Inhabitants  of type  are called \emph{worlds}, and when we have  we say that the world  is \emph{exploding} for the formula  with annotation . This terminology (``exploding'' or ``fallible'') comes from classic use of Kripke models when interpreting absurdity in a constructive way \cite{TroelstraVD1}. The relation , the answer type of the continuations, will later be instantiated with the set of typable terms in normal form, that is, it will be used to pass on the output of the TDPE between different sub-phases of the algorithm in the process of building the final normal form.
\end{definition}

\begin{definition}\label{def:forcing}Given , , , , the dependently typed continuations ``monad'', , defined by
w_2:K)(w_1\le w_2 \to F w_2 0 A \to X w_2 0 C) \to X w_1 0 C)

\forces{w}{1}{A} := (w_1:K)(w\le w_1 \to \
is called \emph{forcing}. That is, we read  as ``the world  forces the type  with annotation ''. 
\end{definition}

\begin{remark}
  We have put the word ``monad'' in quotes because we have not sought to prove the usual categorical or the functional programming laws for monads hold. Yet, the fact that we can define the monadic unit, bind, and run, will be quite convenient for structuring the computation/proofs later on.
\end{remark}

The following two definitions present two alternatives that can be used to instantiate  from Definition~\ref{def:forcing}; when the (non-strong) forcing relation is used in the definitions, it is implicitly instantiated with the strong forcing relation being defined. Note that, type theoretically, (non-strong) forcing and strong forcing need not be defined simultaneously, Definition~\ref{def:forcing} comes first.

\begin{definition}[Strong forcing, call-by-value variant] The \emph{strong forcing} relation  is defined by recursion on the type , by the following clauses:

\end{definition}

\begin{definition}[Strong forcing, call-by-name variant] The \emph{strong forcing} relation  is defined by recursion on the type , by the following clauses:

\end{definition}

Although a different strong forcing relation  determines a different forcing relation , the important properties that hold of the latter are nonetheless the same regardless of which strong forcing was chosen.

\begin{lemma} The following properties hold of strong and ordinary forcing:
  
\end{lemma}
\begin{proof} The proofs of monotonicity of strong forcing with respect to  and  are done by induction on the formula  using monotonicity of . Monotonicity of (non-strong) forcing requires no induction. The proofs of , and  follow the structure given on Figure~\ref{fig:glue}.
\end{proof}

We will use the same turnstile symbols to denote forcing and strong forcing of \emph{finite lists} of formulas, , defined by,

where  is the product type (i.e. logical conjunction, when used as a predicate) and  is the singleton type. Naturally, the monotonicity properties from the previous lemma extend to forcing and strong forcing for lists.

\begin{theorem}[Evaluation for call-by-name]\label{thm:eval:cbn} If , then for any  and any  such that  we have that from the finite product  we can construct .
\end{theorem}
\begin{theorem}[Evaluation for call-by-value]\label{thm:eval:cbv} If , then for any  and any  such that  we have that from the finite product  we can construct .
\end{theorem}
\begin{proof}The proofs of both theorems are done in continuation-passing style, by using induction on the derivation of . The program skeletons that corresponds to the proofs can be seen on figures~\ref{fig:evalcbn} and~\ref{fig:evalcbv}, 
 and the full proofs are available in the Coq formalization.\end{proof}

\subsection{Reifying from the Models}\label{subsec:completeness}

While the evaluation theorems from the previous subsection can be used for any concrete structure that implements the Kripke-CPS models axiomatization, in this section we build one such model, , the \emph{universal model}, from syntactic elements. It gets its name from the fact that if something is forced in  then it is also forced in any other possible model.

To obtain a finer grained characterization of the TDPE procedure, we will separate the lambda terms into a level of \emph{normal terms} and a level of \emph{neutral terms} using the following inductive definition.

This definition concerns \emph{typed} lambda terms (i.e. typing tree derivations), although typing information has been suppressed.

The separation into normal versus neutral terms is standard in the NBE literature, but what is new here is that, in order to obtain the Disjunction Property at the end of this section,  has to be neutral.

\begin{definition}[The model ] The universal Kripke-CPS model  is built when the set of worlds is the set of contexts ,

and the predicate  is defined by recursion on the structure of types of the object language,

as the set of terms in normal or neutral form of the given type.

The pre-order  is defined as the prefix relation on lists. It is not hard to see that reflexivity and transitivity of  hold, and that -monotonicity and -monotonicity hold by the weakening properties of the typing system (formal lemmas \texttt{proof\_nf\_mon}, \texttt{proof\_ne\_mon}, \texttt{proof\_nf\_mon2}, and \texttt{proof\_ne\_mon2}). The property  is provided by the syntactic  rule (formal lemma \texttt{X\_reset}).
\end{definition}

We can now prove that for any meta-level evaluation there exists a term in the object language (\emph{reification} part). Due to contravariance of implication (function types), we need a simultaneous map in the other direction (\emph{reflection} part) \footnote{Note that, while reflection and evaluation (theorems~\ref{thm:eval:cbn} and~\ref{thm:eval:cbv}), have the same typing, the first just does eta-expansions by recursion on the object-language type, while the latter is more informative being defined by recursion on the object-language \emph{term}. }.

\begin{theorem}[Reification () and reflection ()] Given ,  and , the following two statements hold:
  
\end{theorem}
\begin{proof} The two statements are proved simultaneously, by induction on the type . The program skeleton corresponding to the proof can be seen on figures~\ref{fig:reifycbn} and~\ref{fig:reifycbv}. The full proof is done in continuation passing style and is available in the Coq formalization.
\end{proof}

Let  denote the fold-left of the list  for the reflection function applied to a variable (), using the unit type constructor  in the base case. For example, for , we have


We can now obtain the main result of the paper by composing the Evaluation theorems with the Reification theorem, all of which have constructive proofs. In other words, we take a term , apply a meta-CPS translation  on it, in an initial environment built from the context  by the reflect function, and then reconstruct a term in normal form based on the type  using the reification function .

\begin{corollary}[TDPE for call-by-name] Given , we have that .
\end{corollary}

\begin{corollary}[TDPE for call-by-value] Given , we have that .
\end{corollary}

\begin{remark}
The difference in formulation between the two corollaries is due to the fact that the Evaluation theorem for call-by-name (Theorem~\ref{thm:eval:cbn}) uses ordinary forcing for the context , while the corresponding Theorem~\ref{thm:eval:cbv} for call-by-value uses strong forcing. TDPE for CBN can therefore be run on open terms directly, while for CBV we have to have a closed term as input, although TDPE for CBV does normalize below lambda abstractions.
\end{remark}

The following property shows that the calculus from Table~\ref{tab:typing} can be considered a constructive logical system, despite the fact that it contains control operators which are usually connected with classical logic. (Classical logic does not have this property)
\begin{proposition}[Disjunction Property] If  then from  one can get  such that either  or .
\end{proposition}
\begin{proof}
We can use TDPE to transform  to a term in normal form . Now, from the syntax of normal and neutral forms, one can see that the only possibilities for  are that it is either a  or a  --  can not be any of the neutral forms because it does not have a free variable (the context is ) -- and  cannot be a  because of the annotation 0 on the turnstile.
\end{proof}

\section{Algorithm}\label{sec:algorithm}

In this section we show the algorithmic core of the TDPE procedure. While the exact program in a dependently typed language can be seen with all its gory details in the Coq formalization, our intention here is to give a human readable account of the procedure that we extracted by hand from the Coq formalization. This extraction consists in deleting the dependently typed information which is mostly connected to handling worlds (members of the preorder ) and the associated monotonicity proofs.

We will use two levels of lambda calculus: on one level we will have the ``dynamic'' lambda terms from Table~\ref{tab:typing}, and on the other ``static'' level we will use ordinary mathematical function notation: ``'' for abstraction, ``'' for application,  for injection-left,  for injection-right, and the usual big-open-curly-bracket for definition by cases. Small Greek letters  are used for static variables; there are no explicit dynamic variables since we use deBruijn indices. The equality symbol ``:='' denotes definitional equality.

The monadic glue functions are defined on Figure~\ref{fig:glue}. Parameters corresponding to dependent types for world-handling have been left out (worlds are marked with bars ``'').
\begin{figure*}
\centering
  
  \caption{Monadic glue functions}
  \label{fig:glue}
\end{figure*}

The evaluation algorithms corresponding to theorems~\ref{thm:eval:cbv} and~\ref{thm:eval:cbn} are given on figures~\ref{fig:evalcbn} and \ref{fig:evalcbv}.

\begin{figure*}
\centering
  
  \caption{Evaluation for call-by-name}
  \label{fig:evalcbn}
\end{figure*}

\begin{figure*}
\centering
    
  \caption{Evaluation for call-by-value}
  \label{fig:evalcbv}
\end{figure*}

The reification algorithms are defined by mutual recursion with reflection algorithms on figures~\ref{fig:reifycbn} and~\ref{fig:reifycbv}. For facilitating comparison, the places where call-by-value and call-by-name versions differ are marked with boxes.

\begin{figure*}
\centering
  
  \caption{Reification and reflection for call-by-name}
  \label{fig:reifycbn}
\end{figure*}

\begin{figure*}
\centering
    
  \caption{Reification and reflection for call-by-value}
  \label{fig:reifycbv}
\end{figure*}


\subsection{Known Equational Theories}

Before considering computational tests, we recall the available equational theories for shift and reset. 

The equational theory for call-by-value shift and reset, for the full hierarchy, has been proven sound and complete with respect to the extended CPS translation \cite{DanvyF1990} by Kameyama \cite{Kameyama2007}. Considering the first level of the hierarchy which is of interest here, the equations are expressed using the classes of \emph{values} () and \emph{pure evaluation contexts} (),

as follows:


The equational theory for call-by-name shift and reset, for the first level of the hierarchy, has been studied by Kameyama and Tanaka \cite{KameyamaTanaka2010}. For the purpose of proving soundness and completeness with respect to Biernacka and Biernacki's \cite{BiernackaB2009} call-by-name CPS semantics for shift and reset, Kameyama and Tanaka distinguish between two kinds of term applications, the usual one, and the one to continuation variables (); and two kinds of substitutions, for normal variables (), and for continuation variables (). The classes of values and pure evaluation contexts are restrictions of the call-by-name ones, given by:\footnote{Kameyama and Tanaka also consider constants  among the call-by-name values, however no variables are allowed. Since we do not have constants in our minimal object-language of study, we did not include them as an option of .}

The equational theory is as follows,

where the substitution  is defined by recursive descent on the term  and affects only the subterms of the form  by:


We have used conventional syntax, writing  as , and  as , and will continue to do so in the next subsection.

\subsection{Example Runs of the Algorithm}\label{examples}

Let us now consider some test-runs of our TDPE procedure. Each example consists of an input term, marked with a number to refer to, and two outputs: using TDPE for call-by-value (CBV) and for call-by-name (CBN).

We begin with simple examples where the continuation variable of shift is not used (exceptions effect).


The CBN normal forms are not perfect as the resets are systematically duplicated at top level. This duplication is not related to the number of reset as input, as can be seen from Example (\ref{ex2}), but to a ``bug'' in the Coq formalization. Namely, the lemma \texttt{Kont\_sforces\_mon2'}, proving monotonicity of non-strong forcing with respect to the Boolean order, uses a reset in the proof, and, since this lemma is not used in the CBV case, the problem does not appear there.\footnote{The solution might be to make the non-strong forcing monad monotone also for the  relation and not only for  on worlds, and is the subject of future work.}

The CBV equational theory can derive the TDPE output for examples (\ref{ex1}) and (\ref{ex2}). The CBN equational theory derives  but not . However, our TDPE for CBN identifies the two, because it also normalizes  to .

The next example does not use a control operator, but has a delimiter.

The CBV and CBN equational theories do not transform Example (\ref{ex3}) further, because the subterm  is not a value. TDPE for CBV removes one delimiter, as if  were a value, and TDPE for CBN delimits the inside variable y, as if it were taking into account that variables are not values according to the CBN equational theory.

Let us consider an example that uses the continuation inside a shift.

Starting from Example~(\ref{ex4}), the CBV equational theory can obtain the term
,
and then also the term

however, no further rewriting is possible using that theory, because neither is  a pure evaluation context nor is  a value. As for the CBN equational theory, it can not rewrite the starting term (\ref{ex4}), because there are nested applications to the continuation variable  and, unlike in the CBV case,  is not a pure evaluation context in CBN. Note that: 1) there is no  missing in the output of CBN TDPE; 2) the term

obtained by CBV equations is normalized by the CBV TDPE procedure to the same thing as term (\ref{ex4}) (see \texttt{nbe\_tests.v} from the implementation), meaning that TDPE knows how to further reduce those ``blocked'' terms.

The following example is very similar to the previous one, hence we will not comment on it much, but its purpose is to show that CBN TDPE can duplicate the variable  if needed.


The next example contains an evaluation context and a form of shift that can be used with the equational theory for CBN.

The CBV equational theory rewrites (\ref{ex6}) to , and the CBN one rewrites (\ref{ex6}) to . If we apply TDPE on these results of rewriting, we get the same output as the TDPE for (\ref{ex6}).


In the following example, two shifts interact inside the same reset.

The CBV equational theory can transform (\ref{ex7}) to  which can in turn be transformed by CBV TDPE to the same result as for (\ref{ex7}). The CBN equational theory can rewrite the left occurrence of shift and obtain , but no further rewriting is possible because  is not a pure evaluation context in CBN; nevertheless,  can further be transformed by CBN TDPE to the same output as for (\ref{ex7}).

We consider an example that involves sum types, but briefly, since we do not have a ready made equational theory to compare the output to.

We see that not only the reset is pushed from the front of the case-expression into its branches. 

\section{Discussion and Related Work}\label{sec:conclusion}

The CPS translation that we use for CBV TDPE is exactly\footnote{There are additional typing annotations concerning worlds attached to the continuations at the type theoretic level. Another subtle point is that, when evaluating reset (figures \ref{fig:evalcbn} and \ref{fig:evalcbv}), the type theoretic model predicates when to insert a syntactic reset between the return and the run. One may insert a reset in the other cases as well, if one wants to obtain normal forms with more resets, but we prefer to not do it since it is not mandated by the model.} the standard (non-extended) CBV CPS translation of Danvy and Filinski \cite{DanvyF1990}, known also as 1-CPS. Terms in 1-CPS arising from shift are not evaluation-order independent when executed in regular functional programming languages, and that is why, to fix the semantics of shift and reset regardless of the target language of CPS, an additional CBV CPS translation of the CPS result is usually performed, and this composition of two CPS translations is known as the extended CPS, or 2-CPS. It is with respect to this 2-CPS that Kameyama \cite{Kameyama2007} proved the equational theory for CBV to be sound and complete.

The CPS translation used for CBN TDPE is \emph{not} the available 1-CPS of Biernacka and Biernacki \cite{BiernackaB2009}. The difference is in the shift rule (see Figure~\ref{fig:evalcbn}), as Biernacka and Biernacki's

would not type check in our type theoretic model. The standard 2-CPS translation, that Kameyama and Tanaka \cite{KameyamaTanaka2010} proved their equational theory for CBN sound and complete for, is obtained by performing a \emph{CBV} translation of the 1-CPS CBN translation.

We profit from our implementation language having strong reduction\footnote{Constructive type theory is strongly normalizing and there is a simple and efficient implementation of a virtual machine for strong reduction \cite{GregoireL2002}.} in that we do not have to apply two passes of CPS. That is, 1-CPS is sufficient because our evaluation (CPS translation) of a term at the meta-level is a typed and closed term which reduces to the same normal form regardless of the reduction strategy. The typed CPS-s used by Kameyama and Tanaka need recursive types, while we do not. On the other hand, we do not know if it possible at all to account for constants defined by general recursion in our model \footnote{We have however, in separate work, extended the model to higher type primitive recursion (Godel's System T) plus shift and reset on numeric types.}. The question, therefore, of whether our TDPE could be useful in practice is open. We certainly find it useful when ``practice'' concerns lambda calculi for Logic and proof assistants.

We saw in Subsection~\ref{examples} that some terms that cannot be further rewritten by the equational theories, can be further normalized by the TDPE. On the other hand, the equational theories have been proven to be sound and complete with respect to the CPS translation, that is, an equation holds between two terms if and only if the two terms have --equal CPS translations. That means that the extra ``rewriting'' done by the TDPE somehow extends the equality of CPS translations -- indeed, the outputs of TDPE for the examples of Subsection~\ref{examples} do \emph{not} have CPS translations --equal with the ones of the inputs. Nevertheless, at least for all the examples that we have tested, the TDPE identifies the original terms with the ``intermediary'' results arrived to by the equational theories.

As for the typing system we use, we note that it is Filinski's system \cite{FilinskiThesis}, which is sufficient for representing monadic effects by delimited control. A difference with that system is that there is an annotation  on the turnstile () whose purpose is to not allow shifts appearing outside the delimited. This could have also been guaranteed by an external syntactic criteria on whole terms, but the calculus is easier to model if all information is already present in the typing system. The more general typing system for shift and reset, with answer type modification, can type check more programs, but with the price of a function being able to modify its own answer type that we are not ready to pay. In particular, the modified meening of implication would not immediately correspond to something well known on the side of Logic.

Our work was developed independently of the results of the previous work on TDPE for CBV shift and reset of Tsushima and Asai \cite{TsushimaAsai2009}, which seems to derive from their preceding works on traditional offline and online partial evaluation for shift and reset \cite{Asai2002,Asai2004}. The difference between theirs and our results seems to be that: 1) they treat a more general typing system (function types with answer type modification); 2) we aim to produce normal forms that eliminate as many shifts and resets as possible: for example, during reification for function types, Tsushima and Asai's TDPE constructs a  immediately after the first , whereas we postpone the construction of  to some cases of reification that will subsequently called.

\subsection*{Acknowledgements}
I thank the anonymous referees for pointing out problematic parts that led to improvement of the paper. 

\bibliographystyle{eptcs}
\bibliography{nbe-shift-1}

\end{document}
