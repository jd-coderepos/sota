\documentclass[11pt]{myclass}
\usepackage[margin=1in]{geometry}
\usepackage{mathptmx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{float,ctable}
\usepackage{algorithm, algorithmic}
\usepackage{wrapfig}
\newcommand{\breg}{\ensuremath{D_\phi}}
\newcommand{\sbreg}{\ensuremath{D_{s\phi}}}
\newcommand{\subin}[1]{\ensuremath{#1_{\text{in}}}}
\newcommand{\subout}[1]{\ensuremath{#1_{\text{out}}}}
\newcommand{\Bout}{\subout{B}}
\newcommand{\Bin}{\subin{B}}
\newcommand{\tI}{type-I\xspq-\bar{x}ace}
\newcommand{\tII}{type-II\xspace}
\newcommand{\eps}{\varepsilon}
\newcommand{\etal}{\emph{et al}\xspace}

\title{Approximate Bregman near neighbors in sublinear time: Beyond the triangle inequality}
\author{Amirali Abdullah\\University of Utah \and John Moeller\\University of Utah \and Suresh Venkatasubramanian\\University of Utah}
\date{}
\begin{document}
\begin{titlepage}
\maketitle
\thispagestyle{empty}
\begin{abstract}
\emph{Bregman divergences} are  important distance measures that are used extensively in data-driven applications such as computer vision, text mining, and speech processing, and are a key focus of interest in machine learning. Answering \emph{nearest neighbor} (NN) queries under these measures is very important in these applications and has been the subject of extensive study, but is problematic because these distance measures  lack metric properties like symmetry and the triangle inequality.
In this paper, we present the first provably  \emph{approximate nearest-neighbor} (ANN)  algorithms for a broad sub-class of Bregman divergences under some assumptions. Specifically, we examine Bregman divergences which can be decomposed along each dimension and our bounds also depend on restricting the size of our allowed domain. We obtain bounds for both the regular asymmetric Bregman divergences as well as their symmetrized versions. 
To do so, we develop two geometric properties vital to our analysis: a \emph{reverse triangle inequality} 
(RTI) and a relaxed triangle inequality called \emph{-defectiveness} where  is a domain-dependent value. 
Bregman divergences  satisfy the RTI but \emph{not} -defectiveness. However, we show that the square root of a 
Bregman divergence does satisfy -defectiveness. This allows us to then utilize both properties in
 an efficient search data structure that follows the general two-stage paradigm of a ring-tree 
decomposition followed by a quad tree search used in previous near-neighbor algorithms for Euclidean space and spaces of bounded doubling dimension. 


Our first algorithm resolves a query for a -dimensional -ANN in  time and  space and holds for generic -defective distance measures satisfying a RTI. 
Our second algorithm is more specific in analysis to the Bregman divergences and uses a further structural parameter, the maximum ratio of second derivatives over each dimension of our allowed domain (). This allows us to locate a -ANN in  time and  space, where there is a further  factor in the big-Oh for the query time.

\end{abstract}  
\end{titlepage}

\section{Introduction}
\label{Introduction}
The nearest neighbor problem is one of the most extensively studied problems in data analysis. The past 20 years has seen tremendous research into the problem of computing near neighbors efficiently as well as approximately in different kinds of metric spaces.

An important application of the nearest-neighbor problem is in querying content databases (images, text, and audio databases, for example). In these applications, the notion of similarity is  based on a distance metric that arises from information-theoretic or other considerations. Popular examples include the Kullback-Leibler divergence~\cite{kullback}, the Itakura-Saito distance~\cite{itakura} and the Mahalanobis distance~\cite{mahalanobis36}. These distance measures are examples of a general class of divergences called the \emph{Bregman divergences}~\cite{bregman}, and this class has received much attention in the realm of machine learning, computer vision and other application domains. 


Bregman divergences possess a rich geometric structure but are not metrics in general, and are not even symmetric in most cases! While the  geometry of Bregman divergences has been studied from a combinatorial perspective and for clustering, there have been no algorithms with provable guarantees for the fundamental  problem of nearest-neighbor search. This is in contrast with extensive \emph{empirical} study of Bregman-based near-neighbor search\cite{caytonpaper,vptrees,tailoredbregmannn,spellmanvemuri,bregsearch}. 

In this paper we present the first provably approximate nearest-neighbor (ANN) algorithms for a broad sub-class of Bregman divergences, with an assumption of restricted domain. Our first algorithm processes queries in  time using  space and only uses general properties of the underlying distance function (which includes Bregman divergences as a special case). The second algorithm processes queries in  time using  space and exploits structural constants associated specifically with Bregman divergences. An interesting feature of our algorithms is that they extend the ``ring-tree + quad-tree'' paradigm for ANN searching beyond Euclidean distances and metrics of bounded doubling dimension to distances that might not even be symmetric or satisfy a triangle inequality. 

\subsection{Overview of Techniques}
\label{ssec:intro-key-ideas}

At a high level\cite{snotes}, low-dimensional Euclidean approximate near-neighbor search  works as follows. The algorithm builds a quad-tree-like data structure to search the space efficiently at query time. Cells reduce exponentially in size, and so a careful application of the triangle inequality and some packing bounds allows us to bound the number of cells explored in terms of the ``spread'' of the point set (the ratio of the maximum to minimum distance). Next, 
terms involving the spread are eliminated by finding an initial crude approximation to the nearest neighbor. 
Since the resulting depth to explore is bounded by the logarithm of the ratio of the cell sizes, any -approximation of the nearest neighbor results in a depth of . A standard data structure that yields such a crude bound is the \emph{ring tree}~\cite{blackbox}.

Unfortunately, these methods (which work also for doubling metrics~\cite{bounded,blackbox}) 
require two key properties: the existence of the triangle inequality, as well as packing bounds for fitting small-radius balls into large-radius balls. Bregman divergences in general are not symmetric and do not even satisfy a directed triangle inequality! We note in passing that such problems do not occur for the \emph{exact} nearest neighbor problem in constant dimension: this problem reduces to point location in a Voronoi diagram, and Bregman Voronoi diagrams possess the same combinatorial structure as Euclidean Voronoi diagrams~\cite{bvd}. The complexity of a Voronoi diagram of  points is well known to be , and as such of prohibitive space complexity.

\paragraph*{Reverse Triangle Inequality}
The first observation we make is that while Bregman divergences do not satisfy a triangle inequality, they satisfy a weak \emph{reverse triangle inequality}: 
along a line, the sum of lengths of two contiguous intervals is always \emph{less} than the length of the union. 
This immediately yields a packing bound: intuitively, we cannot pack too many disjoint intervals in a larger interval because their sum would then be too 
large, violating the reverse triangle inequality. 

\paragraph{-defectiveness}

The second idea is to allow for a \emph{relaxed} triangle inequality. We do so by defining a distance measure to be 
\emph{-defective} w.r.t a given domain if there exists a fixed  such that for all triples of points  , we have that
. This notion was first employed by Farago et.al \cite{firstdefect} for an algorithm based on optimizing average case
complexity. 

A different natural way to relax the triangle inequality would be to show there exists a fixed  such that 
for all triples , the inequality . In fact, this is the notion of \emph{-similarity} used by 
Ackermann \etal~\cite{musimilarcoresets}  to \emph{cluster} data under a Bregman divergence. However, this version of a relaxed triangle inequality 
is too weak for the nearest-neighbor problem, as we see in Figure\ref{counter}. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.4]{mucounter}
  \end{center}
  \caption{The ratio , no matter how small  is}
  \label{counter}
\end{figure}

Let  be a query point,  be a point from  such that  is known and  
be the actual nearest neighbor to . The principle of grid related machinery is that for  and  sufficiently 
large, and  sufficiently small, we can verify that  is a  nearest neighbor, 
i.e we can short-circuit our grid. 

The figure \ref{counter} illustrates a case where this short-circuit may not be valid for -similarity. Note that -similarity is satisfied 
here for any . Yet the ANN quality of , i.e, , need not be better than  even 
for arbitrarily close  and ! This demonstrates the difficulty of naturally 
adapting the Ackermann notion of -similarity to finding a  nearest neighbor.

In fact, the relevant relaxation of the triangle inequality that we require is slightly different. Rearranging terms, we instead require that there exist 
a parameter  such that for all triples , . We call such a distance \emph{-defective}. 
It is fairly straightforward to see that a -defective distance measure is also -similar, but the converse does not hold, 
as the example above shows.

Without loss of generality, assume that .  Then  and , 
so . Since  is the greatest of the three distances, this inequality is the strongest and implies 
the corresponding -similarity inequalities for the other two distances.

Unfortunately, Bregman divergences do not satisfy -defectiveness for any size domain or value of ! One of our
technical contributions is demonstrating in Section \ref{sec:prop-sqrtd_s-phi} that surprisingly, the square root of Bregman divergences
does satisfy this property over restrictions of our domain with  depending on the boundedness of this subdomain we consider and the choice of divergence.

\paragraph{A Generic Approximate Near-Neighbor Algorithm}

After establishing that Bregman divergences satisfy the reverse triangle inequality and -defectiveness (Section~\ref{sec:prop-sqrtd_s-phi}),
 we first show (Section~\ref{sec:ringsec}) that \emph{any} distance measure satisfying 
the reverse triangle inequality, -defectiveness, and some mild technical conditions admits a ring-tree-based construction 
to obtain a weak near neighbor. However, applying it to a quad-tree construction creates a problem. 
The -defectiveness of a distance measure means that if we take a unit length interval and divide it into two parts, 
all we can expect is that each part has length between  and . This implies that while we may have 
to go down to level  to guarantee that all cells have side length ), some cells 
might have side length as little as , weakening packing bounds considerably. 

We deal with this problem in two ways. For Bregman divergences, we can exploit geometric properties of the 
associated convex function  (see Section~\ref{sec:defn}) to ensure that cells at a fixed level have bounded
size (Section~\ref{sec:condition}); this is achieved by reexamining the second derivative . 

For more general abstract distances that satisfy the reverse triangle inequality and -defectiveness,
 we instead construct a portion of the quad tree ``on the fly'' for each query (Section~\ref{sec:finalized-algorithm}).
 While this is expensive, it still yields polylog() bounds for the overall query time in fixed dimensions. 
Both of these algorithms rely on packing/covering bounds that we prove in Section \ref{covering}. 

An important technical point is that for exposition and simplicity, we initially work with 
the \emph{symmetrized} Bregman divergences (of the form ), 
and then  extend these results to general Bregman divergences (Section~\ref{sec:generalizations}).
 We note that the results for symmetrized Bregman divergences might be interesting in their own right, 
as they have also been used in applications~\cite{vptrees,tailoredbregmannn,symmetrizedcentroids,moresymmetrizedcentroids}.



 

\section{Related Work}
\label{sec:related}


Approximate nearest-neighbor algorithms come in two flavors: the high dimensional variety, where all bounds must be polynomial in the dimension , and the constant-dimensional variety, where terms exponential in the dimension are permitted, but query times must be sublinear in . In this paper, we focus on the constant-dimensional setting. The idea of using ring-trees appears in many works ~\cite{indykmotwani,blackbox,peledmendel}, and a good exposition of the general method can be found in Har-Peled's textbook ~\cite[Chapter 11]{snotes}. 

The Bregman distances were first introduced by Bregman\cite{bregman}. They are the unique divergences that satisfy certain axiom systems for distance measures ~\cite{csiszar}, and are key players in the theory of information geometry ~\cite{amari}. Bregman distances are used extensively in machine learning, where they have been used to unify boosting with different loss functions\cite{collins2002logistic} and unify different mixture-model density estimation problems ~\cite{dhillon}. A first study of the algorithmic geometry of Bregman divergences was performed by Nielsen, Nock and Boissonnat ~\cite{bvd}. This was followed by a series of papers analyzing the behavior of clustering algorithms under Bregman divergences ~\cite{musimilarcoresets,ackermann2, ackermann3,roglin1,mcgregor}. 

Many heuristics have also been proposed for spaces endowed with Bregman divergences. Nielsen and Nock ~\cite{nsmallestdisk} developed a Frank-Wolfe-like iterative scheme for finding minimum enclosing balls under Bregman divergences. Cayton ~\cite{caytonpaper} proposed the first nearest-neighbor search strategy for Bregman divergences, based on a clever primal-dual branch and bound strategy. Zhang \etal ~\cite{bregsearch} developed another prune-and-search strategy that they argue is more scalable and uses operations better suited to use within a standard database system. 
For good broad reviews of near neighbor search in theory and practice, the reader is referred to the books by Har-Peled\cite{snotes}, Samet ~\cite{samet} and Shakhnarovich \etal ~\cite{nnbook}.


 
\section{Definitions}
\label{sec:defn}


In this paper we study the approximate nearest neighbor problem for distance functions :  Given a point set , a query point , and an error parameter , find a point  such that . 
We start by defining general properties that we will require of our distance measures. In what follows, we will assume that the distance measure  is \emph{reflexive}:  if and only if  and otherwise . 

\begin{defn}[Monotonicity]\label{monotonedefn}
Let ,  be a distance function. We say that  is \emph{monotonic} if and only if for all 
we have that  and .
\end{defn}


For a general distance function , where , we say that  is monotonic if it is monotonic when restricted to any subset of  parallel to a coordinate axis. 

\begin{defn}[Reverse Triangle Inequality(RTI)]
Let  be a subset of .  We say that a monotone distance measure  satisfies a \emph{reverse triangle inequality} or RTI if for any three elements , 

\end{defn}

\begin{defn}[-defectiveness] \label{musimdefn}
Let  be a symmetric monotone distance measure satisfying the reverse triangle inequality. We say that 
 is \emph{-defective} with respect to domain  if for all ,


For an asymmetric distance measure , we define left and right sided -defectiveness respectively as 





Note that by interchanging  and  and using the symmetry of the modulus sign, we can also rewrite left and right sided 
-defectiveness respectively as   and .
\end{defn}

\paragraph{Two technical notes.} The distance functions under consideration are typically defined over . We will assume in this paper that the distance  is \emph{decomposable}: roughly, that  can be written as , where  and  are monotone. This captures all the Bregman divergences that are typically used (with the exception of the Mahalanobis distance and matrix distances). See Table~\ref{tab:breg-examples}.

\ctable[
    caption = Commonly used Bregman divergences ,
    label = tab:breg-examples,
    pos = htbp
]{c|c|c|c}{
\tnote{The Mahalanobis distance is technically not decomposable, but is a linear transformation of a decomposable distance}
\tnote[b]{( denotes the cone of positive definite matrices)}
}{
    Name & Domain &  & \breg(x,y)\\ \hline
     &  &   &  \\
    Mahalanobis\tmark &  & & \\
    Kullback-Leibler & & & \\
    Itakura-Saito & & & \\
    Exponential & & & \\
    Bit entropy &  &  & \\
    Log-det & & & \\
    von Neumann entropy & & & \\ \hline
}

  
 We will also need to compute the diameter of an axis parallel box of side-length . Our results hold as long as the diameter of such a box is : note that this captures standard distances like those induced by norms, as well as decomposable Bregman divergences. In what follows, we will mostly make use of the \emph{square root} of a Bregman divergence, for which the diameter of a box is  or , and so without loss of generality we will use this in our bounds. 

\paragraph{Bregman Divergences.}

Let  be a \emph{strictly convex} function that is differentiable in the relative interior of . Strict convexity implies that the second derivative is never  and will be a convenient technical assumption.
The \emph{Bregman divergence}  is defined as 


In general,  is asymmetric. A \emph{symmetrized} Bregman divergence can be defined by averaging: 



An important subclass of Bregman divergences are the \emph{decomposable} Bregman divergences. Suppose  has domain  and can be written as , where  is 
also strictly convex and differentiable in relint(). Then 
is a \emph{decomposable} Bregman divergence. 

 The majority of commonly used Bregman divergences are decomposable:~\cite[Chapter 3]{cayton-thesis} illustrates some of the commonly used ones, including the Euclidean distance, the KL-divergence, and the Itakura-Saito distance.
 In this paper we will hence limit ourselves to considering decomposable distance measures. 
We note that due to the primal-dual relationship of   and  , 
for our results on the asymmetric Bregman divergence we need only consider right-sided nearest neighbors and left-sided results follow symmetrically.

\paragraph{Some notes on terminology and computation model.}
We note now that whenever we refer to ``bisecting" an interval  under a distance measure  satisfying an RTI, we shall precisely mean finding  s.t . The RTI now implies that 
and that repeated bisection quickly reduces the length of subintervals. Computing
such a bisecting point of an interval exactly, or even placing a point at a specified distance from a given point  is not
trivial. However we argue in Section \ref{sec:numerical} that both tasks can be approximately done by numerical procedures
without significantly affecting our asymptotic bounds. For the remainder of the paper we shall take an idealized context and assume any such computations
can be done to the desired accuracy quickly. 

We also stipulate that the ``diameter" of any subset of our domain  under distance measure  shall be .
Where the choice of distance measure  may appear ambiguous from the context, we shall explicitly refer to the -diameter.

\section{Properties of Bregman Divergences}\label{sec:properties}
\label{sec:prop-sqrtd_s-phi}

The previous section defined key properties that we desire of a distance function . The Bregman divergences (or modifications thereof) 
satisfy the following properties, as can be shown by direct computation. 

\begin{lemma}\label{lefttr}
Any one-dimensional Bregman divergence is monotonic.
\end{lemma}
\begin{lemma}\label{cover}
Any one-dimensional Bregman divergence satisfies the reverse triangle inequality. 
Let  be three points in the domain of . Then it holds that:



\end{lemma}

\begin{proof}
We prove the first case, the second follows almost identically.



But since  for all , by convexity of  we have that . This allows us
to make the substitution.



\end{proof}

Note that this lemma can be extended similarly by induction to any series of  points between  and .
 Further, using the relationship between  and the ``dual'' distance  , we can show that the reverse triangle inequality holds going ``left'' as well:  
. These two separate reverse triangle inequalities together yield the result for  . We also
get a similar result for  by algebraic manipulations.



\begin{lemma}\label{Aklreverse}
 satisfies the reverse triangle inequality.
\end{lemma}
\begin{proof}

Fix , and assume that the reverse triangle inequality does not hold:


Squaring both sides, we get:

which is a contradiction, since the LHS is a perfect square.
\end{proof}

 While the Bregman divergences satisfy both monotonicity and the reverse triangle inequality,
 they are not -defective with respect to \emph{any} domain! An easy example of this is , which is also a Bregman divergence.
A surprising fact however is that  and  do satisfy -defectiveness (with  depending on the bounded size of our domain). While we were unable to show precise bounds for  in terms of the domain, the values are small.  For example, for the symmetrized KL-divergence on the simplex where each coordinate is bounded between  and ,  is . If each coordinate is between  and ,then  is . We discuss the
empirical values of  in greater detail in Appendix \ref{sec:muranges}. The proofs showing  is bounded are somewhat tedious and not highly insightful, so we place those in the Appendix \ref{sec:bounded} for the interested reader.


\begin{lemma}\label{Arootmu}
Given any interval  on the real line, there exists a finite  such that   is -defective with respect to . We require 
all order derivatives of  to be defined and bounded over the closure of , and  to be bounded away from zero.
\end{lemma}

\begin{proof}
Refer to \ref{app:1} in Appendix.
\end{proof}

We note that the result for  is proven by establishing the
 following relationship between  and  over a bounded interval , and with some
further computation. 
\begin{lemma}\label{firstTosecond}
Given a Bregman divergence  and a bounded interval ,  is 
bounded by a parameter    where  depends on the choice of divergence and interval. We also require 
the derivatives of  to be defined and bounded over the closure of , and  to be bounded away from zero.
\end{lemma}
\begin{proof}
By continuity, compactness and the strict convexity of , we have that over a finite interval   is bounded.
 Now by using the Lagrange form of , we get that 
\end{proof}

\begin{lemma}\label{Arootmubreg}
Given any interval  on the real line, there exists a finite  such that   is right-sided -defective with respect to .We require all order 
derivatives of  to be defined and bounded over the closure of , and  to be bounded away from zero. 
\end{lemma}

\begin{proof}
Refer to \ref{app:2} in Appendix.
\end{proof}

We extend our results to  dimensions naturally now by showing that if  is a domain such that  and  are -defective with respect to the projection of  onto each coordinate axis, then  and  are  -defective with respect to all of .  

\begin{lemma}\label{AallDmusim}
Consider three points, , ,  such that . Then 



Similarly, if . Then 



\end{lemma}

\begin{proof}

The last inequality is what we need to prove for -defectiveness with respect to  .
By assumption we already have -defectiveness w.r.t each , for every : 

So to complete our proof we need only show:

But notice the following:

So inequality \ref{sec:prop-sqrts-skl} is simply a form of the Cauchy-Schwarz inequality, which states that for two vectors  and  in , that , or that


The second part of the proposition can be derived by an essentially identical argument.
\end{proof}





\section{Packing and Covering Bounds}
\label{covering}
The aforementioned key properties (monotonicity, the reverse triangle inequality, decomposability, and -defectiveness) can be 
used to prove packing and covering bounds for a distance measure . We now present some of these bounds. 

\subsection{Covering bounds in  dimension}
\begin{lemma}[Interval packing]\label{1dintersect}
Consider a monotone distance measure  satisfying the reverse triangle inequality, an interval  such that  and a
 collection of disjoint intervals intersecting , where . Then  .
\end{lemma}

\begin{proof}
 Let  be the intervals of  that are totally contained in . 
 The combined length under  of all intervals in  is at least , but by the reverse triangle inequality their total length cannot exceed , so . 
 There can be only two members of  not in , so  .
 \end{proof}  

A simple greedy approach yields a constructive version of this lemma. 
\begin{corollary}\label{1dcover}
Given any two points,  on the line s.t , we can construct a packing of  by  intervals ,  such that ,  and . Here  is a monotone distance measure satisfying the reverse triangle inequality.
\end{corollary}

We recall here that ,  and  satisfy the conditions of Lemma \ref{1dintersect} and corollary \ref{1dcover} as they satisfy
an RTI and are decomposable.  However, since   may not satisfy the reverse triangle inequality, we instead prove a weaker 
packing bound on  by using .

\begin{lemma}[Weak interval packing]\label{1dsqrtbregint}
 Given distance measure  and an interval  such that  and a collection of disjoint intervals intersecting 
where . Then  . Such a set of intervals can be explicitly constructed.
\end{lemma}

\begin{proof}
 We note that here , and . The result then follows trivially from lemma \ref{1dintersect},
since  satisfies the conditions of lemma \ref{1dintersect}.
\end{proof}


\subsection{Properties of cubes and their coverings}
 The one dimensional bounds can be generalized to higher dimensions to provide packing bounds for 
balls and cubes (which we define below) with respect to a monotone, decomposable distance measure. 


\begin{defn} \label{cube}
 Given a collection of  intervals  and distance measure , s.t  where , the \emph{cube} in  dimensions is defined as  and is said to have \emph{side-length} . We shall specify the choice of  by referring to the cube as either a -cube, -cube, -cube, -cube or a
 -cube. Where we make an argument that holds for more than one of these types of cubes, we shall refer to simply a
 -cube where the possible values of  will be specified. We follow the same convention for balls.
 \end{defn}

  We add that for a given distance measure , a \emph{box}  can be defined similarly to
a cube, except that the side lengths need not necessarily be equal. In this case we let 
and let the th side-length be . Again where the choice of distance measure  appears at all ambiguous we shall refer to the  side-length. 

We pause here to note that for an asymmetric decomposable measure  in  dimensions, every -box has an implied associated ordering on each of the  composing intervals. For a -box defined as prod  and bisected by a collection of 
such that , there will be  subboxes produced such that their th composing interval will
be either  or .

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.7]{bisection.pdf}
  \end{center}
  \caption{A cube of directed side length  subdivided into cubes of side length  }
  \label{fig:bisection}
\end{figure}

 In what can be viewed as a generalization of bisection to splitting each side of a -cube into multiple sub-intervals,
 we show the following:
 
\begin{lemma}\label{scubeCover}
Given a  dimensional -cube  of side-length  under distance measure , 
we can cover it with at most  -cubes of side-length \emph{exactly}  under the same measure , where  may be either ,  and . 
\end{lemma}

\begin{proof}
Note that ,  and  satisfy conditions of corollary \ref{1dcover}. 
Hence we can employ the packing of at most  points in each dimension spaced  apart. 
We then take a product over all  dimensions, and the lemma follows trivially. 
\end{proof}
 
 Weaker packing bounds for  as noted in lemma \ref{1dsqrtbregint} yield us a weaker version of lemma \ref{scubeCover}.
\begin{lemma}\label{acubeCover}
Given a  dimensional -cube  of side-length , 
we can cover it with at most  -cubes of side-length \emph{exactly} .
\end{lemma}
\begin{proof}
Identical to the proof of lemma \ref{scubeCover} and using lemma \ref{1dsqrtbregint} to obtain packing bounds.
\end{proof}
We note that this subdivision of -cubes corresponds to placing an equal number of points (the vertices of the cubes),
and this is what we shall refer to more loosely as \emph{gridding} in the remainder of our paper.



\subsection{Covering with balls in higher dimensions}
Covering a -ball with a number of smaller -balls is a key ingredient in our results. Our approach is to divide a -ball into 
orthants, then to show each orthant can be covered by a certain number of smaller -cubes, and then finally that each
such -cube
can be covered by a -ball of a certain radius. 

We show now results for ,
,  and . We present first the easier cases for the two symmetric measures,  and
. 

\begin{lemma}\label{scube}
A -cube in  dimensions of side-length  can be covered by a -ball of radius . 
Similarly, a -cube in  dimensions of side-length  can be covered by a -ball
of radius . 
\end{lemma}

\begin{proof}
Recall that a -cube is defined as  s.t  (where  is
induced by restricting  to the th dimension). Let the vertex space of the -cube be , where . Now pick an arbitrary vertex , and consider
the -ball  of radius  with center . By decomposability and monotonicity, for any , we have:
 
Hence an -cube of side-length  can be covered by an -ball of radius .  The second result follows
by noting that an -cube of side-length  is an -cube of side-length . Hence this can be covered
by an -ball of radius , which is simply an  ball of radius .
\end{proof}


\begin{lemma}\label{acube}
A -cube in  dimensions of side-length  can be covered by a -ball of radius . 
Similarly, a -cube in  dimensions of side-length  can be covered by a -ball
of radius . 
\end{lemma}

\begin{proof}
Similar to lemma \ref{scube}, we begin by recalling that a -cube is defined as  s.t  (where  is
induced by restricting  to the th dimension). We again let the vertex space of the -cube be , where . Now we have to be somewhat more careful in our choice of center for the -ball  of
radius  than we were in lemma \ref{scube}. We choose the ``lowest" point of the -cube, which is  (see figure \ref{fig:acube})
and term this as a \emph{canonical corner}.We note that our definition does not require that . Now for any other  we have:
 
The argument for  follows analogously to that for  in lemma \ref{scube}. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.4]{acube.pdf}
  \end{center}
  \caption{ is within  distance under  of every other point of the cube.}
  \label{fig:acube}
\end{figure}
\end{proof}


We will also find the following relation of the diameter of a -cubes to the  side-length useful later in this paper.

\begin{lemma}\label{diamtolen}
The diameter of an -cube of side-length  is bounded by . 
\end{lemma}
\begin{proof}
Consider any two points  and  in the -cube of -side-length  
and defined as . Note that since  we have that .
Hence  and .
\end{proof}
\begin{corollary}
For any -box of maximum -side length , the diameter of the box is bounded by .
\end{corollary}



We now proceed to showing covering bounds for  and  using the geometry we have developed thus far.

\begin{lemma}\label{sballcover}
Consider a -ball  of radius  and center .
Then in the case of ,  can be covered with  -balls of radius . In the case of ,  can be covered with 
 -balls of radius . 
\end{lemma}
\begin{proof}
We divide the -ball into  orthants around the center . Each orthant can be covered by a -cube of size . 
For both  and , by lemma \ref{scubeCover} each such -cube can be broken down 
into  sub -cubes of side-length .

By lemma \ref{scube}, we can cover each such -cube by a -ball of radius  placed at any corner. Similarly, for , we can cover each
sub -cube by a -ball of radius  placed at any corner. 


Since there are  sub -cubes to each of the  orthants whether  or
 respectively, the lemma now follows by covering each sub -cube with a -ball of the required radius.
\end{proof}



\begin{lemma}\label{aballcover}
Consider a -ball  of radius  and center  with respect to distance measure .
Then in the case of ,  can be covered with  -balls of radius . And for  ,  can be 
covered by  -balls of radius . 
\end{lemma}

\begin{proof}
We divide the -ball into  orthants around the center . Each orthant can be covered by a -cube of size . We now consider each case separately.
For ,  by lemma \ref{scubeCover} each such -cube can be broken down 
into  -cubes of side-length . For , by lemma \ref{acubeCover} we can break down each -cube into 
 sub -cubes of side-length .

By lemma \ref{acube}, we can cover each such -cube by a -ball of radius  placed at a canonical corner. Similarly for , by lemma \ref{acube} we can cover each
sub -cube by a -ball of radius  placed at a canonical corner. 
Since there are  and  sub -cubes to each of the  orthants for  and
 respectively, the lemma now follows by covering each sub -cube with a -ball of the required radius.
\end{proof}
\section{Computing a rough approximation}\label{sec:ringsec}
To illustrate our techniques, we will focus on finding approximate nearest neighbors under  over the next
two sections. When we define our notation more generally - e.g, of a ring separator - we may use a more generic distance measure
.



Later we will show how our results can be extended to the asymmetric case with mild modifications and careful attention to
directionality.
We now describe how to compute a  
rough approximate nearest-neighbor under  on our point set , 
which we will use in the next section to find the -approximate nearest neighbor. 
The technique we use is based on ring separators. Ring separators are a fairly old concept in geometry, 
notable appearances of which include the landmark paper by Indyk and Motwani~\cite{indykmotwani}. 
Our approach here is heavily influenced by Har-Peled and Mendel~\cite{peledmendel}, 
and by Krauthgamer and Lee~\cite{blackbox}, 
and our presentation is along the template of the textbook by Har-Peled~\cite[Chapter 11]{snotes}.
 
We note here that the constant of  which appears in our final bounds for storage and query time
 is specific to . However, an argument on the same lines will yield a constant of 
for any generic -defective, symmetric RTI-satisfying decomposable distance measure  such that 
the -diameter of a cube of side-length  is bounded by . 

Let  denote a -ball of radius  centered at , and let  denote the complement
 (or exterior) of . A \emph{ring}  is the difference of two concentric -balls: 
. We will often refer to the larger -ball  
as  and the smaller -ball as . We use  to denote the set
 , and use  as , where
 we may drop the reference to  when the context is obvious. 
A \emph{-ring separator}  on a point set  is a ring such that 
, , 
 and  is empty of points of  (see figure \ref{eRing}). A -ring tree is a binary tree obtained
by repeated dispartition of our point set  using a -ring separator. (We shall make the choice of distance measure 
explicit whenever using a -ring separator.)
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.4]{eRing.pdf}
  \end{center}
  \caption{The points of  are split into  and }
  \label{eRing}
\end{figure}


Note that later on in this section, we will abuse this notation slightly by using ring-separators where the annulus
is not actually empty, but we will bound the added space complexity and tree depth introduced. Finally,  denote the minimum sized -ball containing at least  points of  by ; 
its radius is denoted by . 

We demonstrate that for any point set  a ring separator exists under  and secondly, it can always be computed efficiently.  
Applying this ``separator'' recursively on our point structure yields a ring-tree structure for searching our point set.  Before we proceed further, we need to establish some properties of disks under a -defective distance.
Lemma \ref{circle} is immediate from the definition of -defectiveness, Lemma \ref{randommufraction}
is similar to one obtained by Har-Peled and Mazumdar~\cite{smallestdisk} and the idea of 
repeating points in both children of a ring-separator derives from a result by Har-Peled and 
Mendel~\cite{peledmendel}. 

\begin{lemma} \label{circle}
Let  be a -defective distance, and let  be a -ball. Then for any two points , .
\end{lemma}
\begin{proof}
 Follows from the definition of  -defectiveness.
  
 \end{proof}
 
 \begin{corollary}
 For any -ball  and two points ,
 . 
 \end{corollary}
\begin{proof}
Since  is -defective over a prespecified restricted domain.
\end{proof}

\begin{lemma} \label{randommufraction}
 Given a parameter , we can compute in  expected time a  approximation to the smallest  
radius -ball containing  points by the algorithm \ref{smallest}. 
\end{lemma}
\begin{proof}
As described by Har-Peled and Mazumdar (\cite{smallestdisk}) we let  be a random sample from , generated by choosing every point of  with probability . Next, compute for every , the
smallest -ball centered at  containing  points of . By median selection, this can be done in  time and since , this gives us
the expected running time of . Now, let  be the minimum radius computed.
Note that by lemma \ref{circle}, if  then we have that .
 But since  contains  points, we can upper bound the probability of
 failure as the probability that we do not select any of the  points in  in our
sample. Hence:



Note that one can obtain a similar approximation deterministically by brute force search, but this would incur a prohibitive
 running time.
\end{proof}

\begin{algorithm}
  \caption{ApproxSmallestBall}
  \begin{algorithmic}
  	\STATE  
    \STATE Choose  by picking every  with probability 
    \STATE  
    \STATE 
    \FORALL {}     	
    	\STATE Compute smallest -ball  with center  that contains  points of .
    	\IF{ }
    	  \STATE 
    	  \STATE 
    	\ENDIF 
    \ENDFOR 
    \RETURN 
  \end{algorithmic}
  \label{smallest}
\end{algorithm}

We can now use Lemma \ref{randommufraction} and the corresponding algorithm \ref{construct} to construct our ring-separator.

\begin{algorithm}
  \caption{MakeRing}
  \begin{algorithmic}
  \STATE \COMMENT{Here , and the thickness of the separating ring is }
  	\STATE  
  	\STATE 
  	\STATE 
    \STATE 
    \STATE  
    \STATE  
    \STATE 
    \STATE Divide  into  rings of equal thickness, such that  is the -th ring.
    \STATE 
    \STATE 
    \FORALL {}
    	\IF{  }
    		\STATE 
    		\STATE 
    	\ENDIF
    \ENDFOR
    \FORALL {}
    	\IF{}
    		\STATE Add  to 
    	\ELSIF{}
    		\STATE Add  to 
    	\ELSE
    		\STATE Add  to  and 
    	\ENDIF
    \ENDFOR
    \STATE  number of points in 
    \STATE  number of points in 
    \IF {{} \OR {}} 
    	\RETURN {MakeRing } 
    	
    \COMMENT{This checks implictly that our earlier call to the randomized 
    returned our desired approximation. If not, we try our procedure again as standard for Las Vegas algorithms.}
    \ELSE	
    	\RETURN { and }
    \ENDIF
  \end{algorithmic}
  \label{construct}
\end{algorithm}

\begin{lemma} \label{improvedRing}
For arbitrary  s.t  and  in a -defective domain, we can construct a -ring separator  under  in  expected time on a point set  by repeating points. See algorithm \ref{construct}.
\end{lemma}
\begin{proof}


Using Lemma \ref{randommufraction}, we compute a -ball  (where ) 
containing  points such that  where  is a parameter to be set. 
Consider the -ball . We shall argue that there 
must be  points of  in the complement of , , for careful choices of .
As described in Lemma \ref{sballcover},  can be covered by  hypercubes of side-length 
, the union of which we shall refer to as . Set .  Imagine a partition of  
into a grid, where each cell is of -side-length  and hence of diameter at 
most  (by lemma \ref{diamtolen}). 
A -ball of radius  on any corner of a cell will contain the entire cell, and so it 
will contain at most  points, by the definition of . 

 By Lemma \ref{scubeCover} the grid on  has at most  cells.
 Set .  Then we have that  
contains at most  points.  
Since the inner -ball  contains at least  points, and the outer -ball  
contains at most  points, hence the annulus  contains at most 
 points. Now, divide  into  rings of equal width, 
and by the pigeonhole principle at least one of these rings must contain at most  
points of . Now let the inner -ball corresponding to this ring be  and the outer 
-ball 
be . Let , . Add any remaining points of  to \emph{both}  and (see figure \ref{aRing}),i.e,
consider that these points are duplicated and are in both sets. 


\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.6]{aRing.pdf}
  \end{center}
  \caption{The points are split into  and  with some point duplication}
  \label{aRing}
\end{figure}

Assign  and  to two nodes  and  respectively.  Even for ,  each node contains at most 
 points. Also, the thickness of the ring is bounded by , i.e it 
is a  ring separator.
Finally, we can check in  time if the 
randomized process of Lemma \ref{randommufraction} succeeded simply by verifying the number of points
in the inner and outer ring is bounded by the values just computed.
\end{proof}


\begin{lemma}\label{ringsep}
Given any point set  under  in a -defective domain, we can construct a  ring-separator tree  of depth  by algorithm \ref{makeTree}.
\end{lemma}

\begin{proof}
Repeatedly partition  by lemma \ref{randommufraction} into  and  where  is the parent node. Store only the single point  in node , the center of the -ball . 
We continue this partitioning until we have nodes with only a single point contained in them.
Since each child contains at least  points (by proof of Lemma \ref{improvedRing}), each subset reduces by a factor of at least  at each step, 
and hence the depth of the tree is logarithmic. We calculate the depth more exactly, 
noting that in Lemma \ref{improvedRing}, .
Hence the depth  can be bounded as:

\end{proof}

\begin{algorithm}
  \caption{MakeTreeP}
  \begin{algorithmic}
  \STATE \COMMENT{Here  is the thickness of the ring w.r.t radius of the inner ball.}
  	\STATE Add  to 
    \STATE ,   MakeRing 
	\STATE Set  as a child of 
	\STATE Set  as a child of 
	\STATE MakeTree( , , )
	\STATE MakeTree( , , )
  \end{algorithmic}
  \label{makeTree}
\end{algorithm}


Finally, we verify that the storage space require is not excessive.
\begin{lemma}\label{ringstorage}
To construct a  ring-separator tree under  in a -defective domain requires  storage and  time.
\end{lemma}

\begin{proof}
By Lemma \ref{ringsep} the depth bounds still hold upon repeating points.  For storage, we have to bound
the total number of points in our data structure after repetition, let us say . 
Since each node corresponds to a splitting of ,there may be only  nodes and total storage. We aim to show . We begin by noting that in the proof of Lemma \ref{improvedRing}, for a node containing  points, at most an additional  may be duplicated in the two children.

To bound this over each level of our tree, we sum across each node to obtain that the number of points  in our structure at the -th level, as: 
 
Note also by Lemma \ref{ringsep}, the tree depth is  
or bounded by  where  is a constant. Hence we only need to bound the storage at the level . We solve the recurrence, noting that  (no points have been duplicated yet) and  for all  and hence 
. Thus the recurrence works out to:


Where the main algebraic step is that . 
This proves that the number of points, and hence our storage complexity is . Multiplying the depth by  for computing the smallest under -ball across nodes on each level, gives us the time complexity of .
We note that other tradeoffs are available for different values of approximation quality () and construction time / query time.
\end{proof}



\paragraph{Algorithm and Quality Analysis}
\begin{algorithm}
  \caption{FindRoughNNP}
  \begin{algorithmic}
  	\STATE 
  	\STATE 
    \STATE 
	
	\WHILE { has children}
	\STATE 
	\STATE  is the inner ball associated with .
	\IF {}
		\STATE 
		\STATE 
	\ENDIF
	\IF {}
		\STATE 
	\ELSE
		\STATE 
	\ENDIF
	\ENDWHILE
	\RETURN 
  \end{algorithmic}
  \label{roughNN}
\end{algorithm}


Let  be the best candidate for nearest neighbor to  
found so far and . Let  
be the exact nearest neighbor to  from point set  and  
be the exact nearest neighbor distance. Finally, let  be the tree node currently
 being examined by our algorithm, and  be a representative 
point  of . By convention  represents the radius of the \emph{inner} -ball associated with a node ,
 and within each node  we store , which 
is the center of . The node associated with the inner -ball  
is denoted by  and the node associated with  is denoted by 
. 




\begin{lemma}\label{ringsearch}
Given a -ring tree  for a point set with respect to  in a -defective domain, where  
and query point  we can find a  nearest neighbor to  in  time.
\end{lemma}
\begin{proof}
Our search algorithm is a binary tree search. Whenever we reach node , 
if  set  and  
as our current nearest neighbor and nearest neighbor distance respectively.
Our branching criterion is that if  ,
 we continue search in , else we continue the  search in .
 Since the depth of the tree is  by Lemma \ref{ringsep}, this process will take  time.

Turning now to quality, let  be the first node such that  but we 
searched in , or vice-versa.  After examining ,  and  can only decrease at each step.
 An upper bound on  yields a bound on the quality of the approximate nearest neighbor produced.
In the first case, suppose , but we searched in .
Then  and  .
Now -defectiveness implies that 

And for the upper bound on , we again apply -defectiveness to conclude that , which yields 

.


We now consider the other case. Suppose  and we search 
in  instead.  By construction we must have
  and .
Again, -defectiveness yields . Now we can simply take the ratios of the two:
. Taking an upper bound of the approximation provided by each case, the ring tree 
provides us a  approximation. The space/running time bound follows from Lemma \ref{ringstorage}, and noting that taking a thinner ring
()  in the proof there only decreases the depth of the tree due to lesser duplication of points.
\end{proof}


\begin{corollary}
Setting , given a point set with respect to  in a -defective domain we can find a  approximate nearest neighbor to a query point 
 in  time, using a  ring separator tree constructed in
  expected time.
\end{corollary}

\begin{proof}
 The query time is bounded by the depth of the tree, which is bounded in Lemma \ref{ringsep} . That we can construct
 a ring of our desired thickness at each step in reasonable expected time is guaranteed by \ref{improvedRing} . The space guarantee comes from Lemma \ref{ringstorage} and 
the quality of nearest neighbor obtained from our ring tree analyzed by Lemma \ref{ringsearch} .
Note that we are slightly abusing notation in Lemma \ref{improvedRing}, in that the separating ring
obtained there and which we use is not empty of points of  as originally stipulated. However remember that if  is in the ring, then  repeats in \emph{both} children 
and cannot fall off the search path. Hence we can ``pretend'' the ring is empty  as in our analysis in 
Lemma \ref{ringsearch}.
\end{proof}

\section{Computing a  approximation.}
\label{sec:finalized-algorithm}
We give now our overall algorithm for obtaining a  nearest neighbor in  query time under . We note that although our bounds are for , similar bounds follow in the same manner for any decomposable symmetric distance measure , which satisfies an RTI and for which the ratio of diameter to side
length of a cube is bounded by . 

\subsection{Preprocessing}
We first construct an improved ring-tree  on our point set  in  time as 
described in Lemma \ref{ringstorage}, with ring thickness . 
We then compute an efficient orthogonal range reporting data structure on  in  time,
 such as that described in~\cite{rangesearching} by Afshani \etal. We note the main result we need: 

\begin{lemma}\label{rangesearch}
We can compute a data structure from  with  storage (and same construction time), such that given an arbitrary axis parallel box  we can determine in  query time a point  if 
\end{lemma} 

\subsection{Query handling}
Given a query point , we use  to obtain a point  in  time such that . 
Given , we can use Lemma \ref{sballcover} to find a 
family  of  -cubes of side-length exactly  such that
 they cover the -ball . We use our range reporting structure to find a 
point  for all non-empty cubes in  in a total of  time.
 These points act as representatives of the -cubes for what follows. Note that  must 
necessarily be in one of these -cubes, and hence there must be a ()-nearest 
neighbor  in some . To locate this , we 
construct a quadtree~\cite[Chapter 11]{snotes}~\cite{skipquadtrees} for repeated bisection and 
search on each .
 
Algorithm~\ref{algo} describes the overall procedure. 
We call the collection of all cells produced during the procedure a \emph{quadtree}.
We borrow the presentation in Har-Peled's book~\cite{snotes} with the important qualifier 
that we construct our quadtree at runtime. The terminology here is as introduced earlier in 
Section \ref{sec:ringsec}.

\begin{algorithm}
  \caption{QueryApproxNN}
  \begin{algorithmic}
    \STATE Instantiate a queue  containing all cells from  along with their representatives and 
           enqueue \textbf{root}.
    \STATE Let ,
    
    \REPEAT 
    \STATE Pull off the head of the queue and place it in .
    \IF{}
      \STATE Let , 
             
      \STATE Bisect  according to procedure of Lemma   
             \ref{bisectionProcedure}; denote the result as .
      \FORALL{}
        \STATE As described in \ref{bisectionProcedure}, check if  is non-empty by passing it to our range reporting structure, which will also return us some   if  is not empty. 
        \STATE Also check if   may contain a point closer than  to . (This may be done in  time for each cell, given the coordinates of the corners.) 
        \IF{ is non-empty AND has a close enough point to }
          \STATE Let 
          \STATE Enqueue 
        \ENDIF
      \ENDFOR
    \ENDIF
    \UNTIL  is empty
    \STATE Return 
  \end{algorithmic}
  \label{algo}
\end{algorithm}
\begin{lemma}\label{correctness}
Algorithm~\ref{algo} will always return a -approximate nearest neighbor. 
\end{lemma}
\begin{proof}
Let  be the point returned by the algorithm at the end of execution. By the method of the algorithm, for all points  for which the distance is directly evaluated, we have that
. 
The terminology here is as in Section \ref{sec:ringsec}.
We look at points  which are \emph{not} evaluated during the running of the algorithm, 
i.e. we did not expand their containing cells. But by the criterion of the algorithm for not expanding a cell, it must be that . For , this means that  for any , including . So  is indeed a  approximate nearest neighbor.
\end{proof}

We must analyze the time complexity of a single iteration of our algorithm, namely the complexity of a subdivision of a -box  and determining which of the  -subcells of  are non-empty.
\begin{lemma}\label{bisectionProcedure}
Let  be a -box with maximum -side-length  and  its subcells produced by bisecting along each side of  under . For all non-empty -subcubes  of , we can find  in  total time complexity, and the maximum -side-length of any  is at most .
\end{lemma}

\begin{proof}
 Note that  is defined as a product of  intervals. For each interval, we can find an approximate bisecting point under
 in  time and by the RTI each subinterval is of length at most  under . This leads to an  cost to find a bisection point for all intervals, which define  -subboxes or children of .

We pass each -subbox of  to our range reporting structure. By lemma \ref{rangesearch}, this takes  time to check emptiness or return a point  contained in the child, if non-empty. Since there are  non-empty children of , this implies a cost of  time incurred.

 Checking each of the non-empty subboxes  to see if it may contain a point closer than  to  takes a further  time per cell or  time. 
\end{proof}

We now bound the number of cells that will be added to our search queue. We do so indirectly, by placing a lower bound on the maximum -side-length of all such cells.


\begin{lemma}\label{depthCube}
Algorithm \ref{algo} will not add the children of node   to our search queue 
if the maximum side-length of  is less than .
\end{lemma}

\begin{proof}

Let  represent the -diameter of cell . By construction, 
we can expand  only if some subcell of  contains a point  
such that . Note that since  is 
examined, we have . 
Now assuming we expand , then we must have:


So .
First note . Also, by definition,  . And  by lemma \ref{diamtolen} where  is the maximum side-length of . 
Making the appropriate substitutions yields us our required bound.
\end{proof}

Given the bound on quadtree depth (Lemma \ref{depthCube}), and using the fact that at most  nodes are expanded at level , we have:
\begin{lemma}\label{timeFinal}
Given a -cube  of -side-length , we can compute a -nearest neighbor to  in  time.
\end{lemma}

\begin{proof}
Consider a quadtree search from  on a -cube  of -side-length . By lemma \ref{depthCube}, our algorithm will not 
expand cells with all -side-lengths smaller than  . But since the -side-length reduces by at least half in each dimension upon each split, all -side-lengths are less than this value after 
 repeated bisections of our root cube.

Noting that  time is spent at each node by lemma \ref{bisectionProcedure}, and that at the th level the 
number of nodes expanded is , we get a final time complexity bound of 
. 
\qedhere
\end{proof}


Substituting  in Lemma \ref{timeFinal} gives us a bound 
of . 
 This time is per -cube of  that covers . Noting 
that there are  such -cubes gives us a final time complexity of .
For the space complexity of our run-time queue, observe that the number of nodes in our queue 
increases only if a node has more than one non-empty child, i.e, there is a split of our  points. 
Since our point set may only split  times, this gives us a bound of  on the space complexity 
of our queue. 


\section{Logarithmic bounds, with further assumptions.}
\label{sec:condition}
For a given , let  
over our bounded subset of the domain ( may be infinity over the unrestricted domain, or on a subset over whose closure  tends to infinity or zero).  is susceptible to the choice of bounded subset of the domain and in general grows as we expand our allowed subset.
We conjecture that  although we cannot prove it. In particular, we show that if we assume a bounded  (in addition to ), 
we can obtain a  nearest neighbor in time  time for . We do so by constructing a \emph{Euclidean} quadtree  on our set
in preproccessing and using  and  to express the bounds obtained in terms of .

We will refer to the Euclidean distance  as  and note first the following key relation between  and , where  serves to relate the two measures by some constant factor. Nock \etal~\cite{mixed} use a comparable measure to  as do Sra \etal~\cite{tensorclust}, for similar purposes of establishing a constant factor approximation 
to the Euclidean distance.

\begin{lemma}\label{EucBregBisect}
Suppose we are given a interval  s.t. , , and . 
Suppose we divide  into  subintervals of equal length with endpoints , where  and , .
Then .
\end{lemma}

\begin{proof}
We can relate  to  via the Taylor expansion of :
  for some . Combining this with  yields 



and  


\end{proof}

\begin{corollary}\label{repbis}
If we recursively bisect an interval  s.t.  and  
into  equal subintervals (under ), then 
 for any of the
subintervals  so obtained. Hence after  subdivisions, all intervals will be of length 
at most  under .
Also, given a cube of initial side-length , after  repeated bisections (under ) 
the diameter will be at most   under .
\end{corollary}

We find the smallest enclosing -cube  that bounds our point set, 
and then construct our compressed Euclidean quadtree in preprocessing on this cube.  Say 
is of side-length .
Corollary \ref{repbis} gives us that for cells formed at the -th level of decomposition, 
the side-length under  is between  and . Refer to these cells formed at the -th level as .

\begin{lemma}\label{packingTheTree}
Given a -ball  of radius , let . 
Then  and the side-length of each cell in  is between  and  under . 
We can also explicitly retrieve the quadtree cells corresponding to  in  time.
\end{lemma}

\begin{proof}
Note that for cells in , we have side-lengths under  between  and  by Corollary \ref{repbis}.
 Substituting ,  these cells have side-length between  and  under . 
By the reverse triangle inequality and Lemma \ref{1dintersect}, we get our required bound for .
In preconstruction of our quadtree  we maintain for each dimension the corresponding interval quadtree , . 
Observe this incurs at most  storage, with  in the big-Oh.  For retrieving the actual cells , we first
find the  intervals from level  in each  that may intersect . Taking a product of these, we get  cells 
which are a superset of the canonical cells . Each cell may be looked up in  time from the compressed
 quadtree \cite{snotes} so our overall retrieval time is .
\end{proof}


 Given query point , we first obtain in  time with our ring-tree a rough  ANN  
under  s.t. \\ .
Note that we can actually obtain a -ANN instead, using the results of Section \ref{ringsep}. But a coarser approximation
of -ANN suffices here for our bound. The tree depth (and implicitly the storage and running time) is still bounded
by the  of Lemma \ref{ringsep}, since in using thinner rings we have less point duplication
and the same proportional reduction in number of points in each node at each level.


Now Lemma \ref{packingTheTree}, we have  quadtree cells intersecting  .

 Let us call this collection of cells .  We then carry out a quadtree search on each element of . Note that
 we expand only cells which may contain a point nearer to query point  than the current best candidate.  
We bound the depth of our search using -defectiveness similar to Lemma~\ref{depthCube}:

\begin{lemma}\label{modDepth}
We will not expand cells of -diameter less than \\ or cells whose all side-lengths w.r.t.  are less than .
\end{lemma}

For what follows, refer to our \emph{spread} as .

\begin{lemma}\label{treeDepth}
We will only expand our tree to a depth of \\.
\end{lemma}
\begin{proof}
 Using Lemma \ref{modDepth} and Corollary \ref{repbis}, each cell of  will be expanded only to a depth of 
. 
This gives us a depth of .
\end{proof}



\begin{lemma}\label{breathnum}
The number of cells examined at the -th level is  .
\end{lemma}

\begin{proof}
Recalling that the cells of  start with side-length at most  under , at the -th level the -diameter of cells is at most  , by Corollary~\ref{repbis}. 
Hence by -defectiveness, there must be some point examined by our algorithm at -distance at most
. Note that our algorithm will
only expand cells within this distance of .

The  side-length of a cell  at this level is at least . Applying the
packing bounds from Lemma \ref{scubeCover}, and the fact that , the number of cells expanded is at most

\end{proof}
Finally we add the 
 to get the total number of nodes explored: 

Recalling that , substituting back and ignoring lower order terms, the time complexity is 


Accounting for the  cells in  that we need to search, this adds a further  multiplicative factor.
This time complexity of this quadtree phase(number of cells explored) of our algorithm dominates the time complexity of the ring-tree 
search phase of our algorithm, and hence is our overall time
complexity for finding a  ANN to .
For space and pre-construction time, we note that compressed Euclidean quadtrees can be built in  time and require  space~\cite{snotes},
which matches our bound for the ring-tree construction phase of our algorithm requiring  time and  space. 

\section{The General Case: Asymmetric Divergences}\label{sec:generalizations}

Without loss of generality we will focus on the \emph{right-sided} nearest neighbor: given a point set , query point  and , find  that approximates  to within a factor of .  Since a Bregman divergence is not in general -defective, we will consider instead : by monotonicity and with an appropriate choice of , the result will carry over to . 

We list  three issues that have to be resolved to complete the algorithm. Firstly, because of asymmetry, we cannot bound the diameter of a 
quadtree cell  of side-length  by . However, as the proof of Lemma \ref{acube} shows, we can 
choose a \emph{canonical corner} of a cell such that a (directed) ball of radius  centered at that corner covers the cell. 
By -defectiveness, we can now conclude (see lemma \ref{adiamtolength}) that the diameter of  is at most  (note that this incurs an extra 
factor of  in all expressions). Secondly, since while  satisfies -defectiveness (unlike ) the opposite is 
true for the reverse triangle inequality, which is satisfied by  but not . This requires the use of a weaker packing 
bound based on Lemma \ref{1dsqrtbregint}, introducing dependence in  instead of . And thirdly, the lack of symmetry means
we have to be careful of the use of directionality when proving our bounds. Perhaps surprisingly, the major part of the arguments
carry through simply by being consistent in the choice of directionality.



 Note that for this section  we are referring to .
 With some small adjustments, similar bounds can be obtained for
more generic asymmetric, monotone, decomposable and -defective distance  measures satisfying packing bounds.
The left-sided asymmetric nearest neighbor can be determined analogously.

Finally, given a bounded domain , we have that   is left-sided -defective 
for some  and right sided -defective for some  (see Lemma \ref{Arootmubreg} for detailed proof). 
For what follows, let   and describe  as simply -defective. 


Most of the proofs here mirror their counterparts in Sections \ref{sec:ringsec} and \ref{sec:finalized-algorithm}. 

\subsection{Asymmetric ring-trees}
\label{subsec:ringextension} 
Since we focus on \emph{right}-near-neighbors, all balls and ring separators referred to will use \emph{left-balls}
 i.e balls . As in Section \ref{sec:ringsec}, we will design a ring-separator algorithm and 
use that to build a ring-separator tree.


\begin{lemma} \label{leftcircle}
Let  be a -defective distance, and let  be a left-ball with respect to . Then for any two points , .
\end{lemma}

 \begin{proof}
 Follows from the definition of right sided -defectiveness.
  
 \end{proof}
 
 \begin{corollary}
 For any -left-ball  and two points ,
 . 
 \end{corollary}
\begin{proof}
Since  is -defective over a prespecified restricted domain.
\end{proof}

As in Lemma \ref{randommufraction} we can construct  (in  expected time) a -approximate -left-ball 
enclosing  points. This in turn yields a ring-separator construction, 
and from it a ring tree with an extra  factor in depth as compared to
symmetric ring-trees, due to the weaker packing bounds for .

 We note that the asymptotic bounds for ring-tree storage and construction time follow from purely combinatorial arguments
 and hence are unchanged for . 
Once we have the ring- tree, we can use it as before to identify a rough near-neighbor for a query ;
 once again, exploiting -defectiveness gives us the desired approximation guarantee for the result.

 \begin{lemma} \label{randommufractionassym}
 Given any parameter , we can compute in   randomized time a -left-ball  such that  and .
\end{lemma}  

\begin{proof}
 Follows identically to the proof of Lemma \ref{randommufraction}.
\end{proof}


\begin{lemma}\label{ringassym}
There exists a parameter  (which depends only on  and ), such that for any -dimensional point set  and any -defective , we can find a  left-ring separator 
in  expected time.
\end{lemma}
 
\begin{proof}
First, using our randomized construction, we compute a -left-ball  (where ) containing  
points such that , where  is a parameter to be set. 
Consider the -left-ball .  As described in Lemma \ref{aballcover},  can be covered by  
-hypercubes of side-length ,
 the union of which we shall refer to as . Set . 
 Imagine a partition of  into a grid, where each cell is of side-length . 
Each cell in this grid can be covered by a -ball of radius 
centered on it's lowest corner.  This implies any cell will contain at most  points, 
by the definition of . 

 By Lemma \ref{acubeCover} the grid on  has at most  cells.
 Each cell may contain at most  points. In particular, set . Then we have that  may contain 
at most  points,  or since ,  contains at most  points 
and  contains at least  points.  The rest of the proof goes through as in Lemma \ref{improvedRing}
\end{proof}


We proceed now to the construction of our ring-tree using the basic ring-separator structure of Lemma \ref{ringassym}.

\begin{lemma}\label{assymringsep}
Given any point set , we can construct a  
left ring-separator tree  under  of depth  .
\end{lemma}

\begin{proof}
Repeatedly partition  by Lemma \ref{ringassym} into  and  where  is the parent node. Store only the single point  in node , the center of the ball . We continue this partitioning until we have nodes with only a single point contained in them.

Since each child contains at least  points, each subset reduces by a factor of at least  at each step, 
and hence the depth of the tree is logarithmic. 
We calculate the depth more exactly, noting that in Lemma \ref{ringassym}, .
Hence the depth  can be bounded as:

\end{proof}

Note that Lemma \ref{assymringsep} also serves to bound the query time of our data structure. We need only now bound the approximation quality.
The derivation is similar to Lemma \ref{ringsearch}, but with some care about directionality. 


\begin{lemma}\label{ringsearchassym}
Given a -ring tree  for a point set with respect to a -defective  , where , and query point  we can find a  nearest neighbor  to query point  in  time.
\end{lemma}

\begin{proof}
Our search algorithm is a binary tree search. Whenever we reach node , if  
set  and  as our current 
nearest neighbor and nearest neighbor distance respectively.  
Our branching criterion is that if  , we
 continue search in , else we continue the  
search in . Since the depth of the tree is  by Lemma \ref{assymringsep}, 
this process will take  time.
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.4]{case1.pdf}
  \end{center}
  \caption{ is outside  so we search , but }
  \label{case1}
\end{figure}
 Let  be the first node such that  but we searched in , or vice-versa. 
The analysis goes by cases. In the first case as seen in figure \ref{case1}, suppose , but we searched in .
Then


Now left-sided -defectiveness implies a lower bound on the value of :


And for the upper bound on . First by right-sided -defectiveness:


We now consider the other case. Suppose  and we search in  instead. The analysis is almost identical. By construction we must have:


Again, left-sided -defectiveness yields:


We can simply take the ratios of the two:


Taking an upper bound of the approximation quality provided by each case, we get that the ring separator provides us a  rough approximation.
Substitute   and the time bound follows from the bound of the depth of the tree in Lemma \ref{assymringsep}.
\end{proof}

\begin{corollary}
 We can find a  nearest neighbor to query point  in  time  using a  ring-tree constructed in
  expected time.
\end{corollary}

\begin{proof}
 Set , using Lemma \ref{assymringsep}. The construction time for the ring tree follows by combining Lemmas \ref{assymringsep}
and \ref{ringassym}.
\end{proof}



\subsection{Asymmetric quadtree decomposition}
\label{subsec:quadextension}

As in Section \ref{sec:finalized-algorithm}, we use the approximate near-neighbor returned by the ring-separator-tree query to progressively expand cells, using a subdivide-and-search procedure similar to Algorithm \ref{algo} albeit with 
 replaced with . A key difference is the procedure used to bisect a cell. 



\begin{lemma}\label{bisectionasymmetric}
Let  be a -box with maximum -side-length  and  its subcells 
produced by partitioning each side of  into two equal intervals under . For all non-empty subboxes  of , we can find  in  total time complexity, and the maximum -side-length of any  is at most .
\end{lemma}
\begin{proof}
 Note that  is defined as a product of  intervals. For each interval, we can find an approximate bisecting point 
under  in  time. Here the bisection point  of interval  is such that 
. By resorting to the RTI for , we get that  and hence  which implies . The rest of our proof follows as in Lemma \ref{bisectionProcedure}.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale = 0.7]{bisection.pdf}
  \end{center}
  \caption{Illustrating bisection of a box, in this case of equal side lengths . The arrows show directionality. }
\end{figure}
\end{proof}


We now bound the number of cells that will be added to our search queue. 
We do so indirectly, by placing a lower bound on the maximum -side-length of all such cells, and note that for the
asymmetric case we get an additional factor of .


\begin{lemma}\label{adiamtolength}
The -diameter of an -cube  of -side-length  is bounded by . 
\end{lemma}
\begin{proof}
Since the cube may be covered by a -left-ball of radius  placed at a suitably chosen corner (by lemma \ref{acube}), lemma \ref{leftcircle} bounding the diameter of such a ball gives the required bound on the diameter
of the cube.
\end{proof}

\begin{lemma}\label{depthAsymmetricCube}
Algorithm \ref{algo} (with  replaced by ) will not add the children of node   to our search queue 
if the maximum -side-length of  is less than .
\end{lemma}
\begin{proof}
Let  represent the diameter or maximum distance between any two points of cell .

By construction, we can expand  only if some subcell of  contains a point  
such that . Note that since  is examined, 
we have .
 Now assuming we expand , then we must have:

Note that we substitute  and that by the definition of  
as our candidate nearest neighbor distance,  . 
Our main modification from the symmetric case is that here 
 by lemma \ref{adiamtolength},  where  is the maximum side-length of , 
as opposed to . 
\end{proof}

The main difference between this lemma and Lemma~\ref{depthCube} is the extra factor of  that we incur 
(as discussed) because of asymmetry. We only need do a little more work to obtain our final buonds:
\begin{lemma}\label{timeFinalAsymmetric}
Given a -cube  of -side-length , and letting   we can compute a - right sided nearest neighbor to  in  
in  time.
\end{lemma}

\begin{proof}
Consider a quadtree search from  on a -cube  of -side-length . By lemma \ref{depthAsymmetricCube},
 our algorithm will not expand cells with all -side-lengths smaller than  .
 But since the -side-length reduces by at least a factor of  in each dimension upon each split, 
all -side-lengths are less than this value after   repeated bisections of our root cube. Observe now that  time is spent at each node by Lemma \ref{bisectionasymmetric}, that at the -th level the 
number of nodes expanded is , and that .
We then get a final time complexity bound of 
. 
\qedhere
\end{proof}

Substituting  in Lemma \ref{timeFinalAsymmetric} gives us a  bound of .  This time is per cube of  that covers right-ball . Noting that there are  such cubes gives us a final time complexity of . The space bound follows as in Section~\ref{sec:finalized-algorithm}. 


\paragraph{Logarithmic bounds for Asymmetric Bregman divergences}
\label{subsec:assymcondition}

We now extend our logarithmic bounds from Section \ref{sec:condition} 
to asymmetric Bregman divergence .
First note that the following lemma goes through by identical argument 
to lemma~\ref{EucBregBisect}.
\begin{lemma}\label{EucABregBisect}
Given an interval  s.t. ,  and , suppose we divide  into  subintervals of equal length under  with endpoints  where , for all . Then .
\end{lemma}
\begin{corollary}\label{repabis}
If we recursively bisect an interval  
s.t.  and  into  equal subintervals (under ), then  for any of the subintervals  so obtained.
Hence after  subdivisions, all intervals will be of length at most  under .
\end{corollary}

We now construct a compressed Euclidean quad tree as before, 
modifying the Section \ref{sec:condition} analysis slightly 
to account for the weaker packing bounds for  and the extra  factor on the diameter of a cell. 

\begin{theorem}
  Given an asymmetric decomposable Bregman 
divergence  that is -defective over a domain  with associated 
 as in Section \ref{sec:condition}, we can compute a -approximate right-near-neighbor in 
time .
\end{theorem}



We note our first new Lemma, a slightly modified packing bound due to  not having a direct RTI.

\begin{lemma}\label{cpack}
 Given an interval  s.t. , and 
intervals with endpoints , s.t. for all , , 
at most  such intervals intersect .
\end{lemma}

\begin{proof}
By the Lagrange form, 

or we can say that .
The RTI for  then gives us the required result.
\end{proof}

\begin{corollary}\label{cpackd}
 Given a ball  of radius  under , there can be at most
 disjoint -cubes that can intersect  where each cube has 
side-length at least  under .
\end{corollary}


 As before, we  find the smallest enclosing Bregman cube of side-length 
that encloses our point set, and then construct a compressed Euclidean quad-tree in
pre-processing. Let  denote the cells at the -th level. 

 \begin{lemma}\label{apackingTheTree}
 Given a  right-ball  of radius  under , let . 
Then  and the side-lengths of each cell in  are
between  and  under . We can also explicitly retrieve 
the quadtree cells corresponding to  in  time.
 \end{lemma}

 \begin{proof}
 Note that for cells in , we have -side-lengths between 
 and  by Corollary~\ref{repabis}.
  Substituting ,  these cells have side-length 
between  and  under .
 Now, we look in each dimension at the number of disjoint intervals of length 
at least  that can intersect . By Lemma~\ref{cpack}, this is at most .
 The rest of the proof follows as in Lemma~\ref{packingTheTree}. 
 \end{proof}

 We first obtain in  time with our asymmetric ring-tree an  
ANN  to query point , such that
.
 We then use Lemma \ref{apackingTheTree} to get  cells of our quadtree that 
intersect right-ball . 

Let us call this collection of cells as . We then carry out a quadtree search on each element
of . Note that we expand only cells which may contain a point nearer to query point 
than the current best candidate. We bound the depth of our search using -defectiveness
similar to Lemma~\ref{treeDepth}.


\begin{lemma}\label{modADepth}
 We need only expand cells of -diameter greater than 

\end{lemma}

\begin{proof}
 By -defectiveness, similar to Lemma~\ref{depthCube}.
\end{proof}

\begin{corollary}\label{asidelen}
 We will not expand cells where the length of each side is less than 
 under .
\end{corollary}

 \begin{proof}
Note that a quadtree cell  where every side-length is less than  can be covered by a ball of radius  under 
  with appropriately chosen corner as center of ball, 
as explained in proof of Lemma \ref{acube}. Now by Lemma~\ref{leftcircle}, 
, . 
Substituting for  from Lemma \ref{modADepth}, the -diameter of  is at 
most .
 \end{proof}

Let the spread be .
\begin{lemma}\label{AfinalTreeDepth}
We will only expand our tree to a depth of 
.
\end{lemma}

\begin{proof}
 Note first that . Then 
by Lemma \ref{apackingTheTree}, each of the cells of our corresponding quadtree is of -side-length at most
.  Using \ref{asidelen} to lower bound the minimum -side-length of any quadtree cell expanded, and \ref{repabis} to bound number of bisections needed to guarantee all -side lengths are within this  gives us out bound.
\end{proof}


\begin{lemma}\label{atreebreadth}
 The number of cells expanded at the -th level is 
.
\end{lemma}
 
\begin{proof}
Recalling that the cells of  start with all -side-lengths at most ,
at the -th level the side-length of a cell  is at most  under  by Corollary~\ref{repabis}. 
And using Lemma~\ref{leftcircle},  .
Hence by -defectiveness there must be a point at distance at most 
.

The -side-length of a cell  at this level is at least , so the number of cells expanded is at most , 
by Corollary \ref{cpackd}.
Using the fact that , we get 
.
\end{proof}

Simply summing up all , the total number of nodes explored is

or

after substituting back for  and ignoring smaller terms.
Recalling that there are  cells in  adds a further  multiplicative factor.
This time complexity of this quadtree phase(number of cells explored) of our algorithm dominates the time complexity of the ring-tree 
search phase of our algorithm, and hence is our overall time
complexity for finding a  ANN to .
For space and pre-construction time, we note that compressed Euclidean quadtrees can be built in  time and require  space~\cite{snotes},
which matches our bound for the ring-tree construction phase of our algorithm requiring  time and  space. 

\section{Numerical arguments for bisection}\label{sec:numerical}
In our algorithms, we are required to \emph{bisect} a given interval with respect to the distance measure , as well as construct points that lie a fixed distance away from a given point. We note that in both these operations, we do not need exact answers: a constant factor approximation suffices to preserve all asymptotic bounds.  
In particular, our algorithms assume two procedures: 
\begin{enumerate}
\item{Given interval , find  such that }

\item{ Given  and distance , find  s.t } 
\end{enumerate}

Cayton presents a similar bisection procedure~\cite{caytonpaper} as ours for the second task above, although our analysis of
the convergence time is more explicit in our parameters of  and . For a given  and precision parameter , we describe a procedure that 
yields an  approximation in  steps for both problems, where  implicitly depends on the domain of convex function :




Note that this implies linear convergence. While more involved numerical methods such as Newton's method may yield better results, 
our approximation algorithm serve as proof-of-concept that the numerical precision is not problematic. 

A careful adjustment of our NN-analysis now gives  
a  time complexity to compute a -ANN to query point .


We now describe some useful properties of .


\begin{lemma}\label{ratio}
Consider   such that . Then for any two intervals  , 



\begin{proof}
The lemma follows by the definition of  and by direct computation from the Lagrange form of , i.e, , for some 
.
\end{proof}

\end{lemma}

\begin{lemma}\label{approxnum}
Given a point ,  distance , precision parameter  and a -defective , we can locate a  point  such that  in   time.
\end{lemma}

\begin{proof} 
Let  be the point such that . We outline an iterative process,  \ref{finalalgo}, with -th iterate  that converges to .
\begin{algorithm}
  \caption{QueryApproxDist}
  \begin{algorithmic}
    \STATE Let  be such that 
	\STATE Let 

    \REPEAT 
    	\IF{}
			\STATE	
		\ELSE 
			\STATE 		
    	\ENDIF
		
		\STATE 
    \UNTIL {}
    \STATE Return 
  \end{algorithmic}
  \label{finalalgo}
\end{algorithm}
First note that  and 
. It immediately follows that .

By construction, . Hence by Lemma \ref{ratio}, .  We now use -defectiveness to upper bound our error  at the -th iteration:

 

Choosing  such that  implies that .\qedhere
\end{proof}
An almost identical procedure can locate an  approximate bisection point of interval  in  time, and similar techniques can be applied for . We omit the details here.


\section{Further work}
A major open question is whether bounds independent of -defectiveness can be obtained for the complexity of ANN-search under Bregman divergences. As we have seen, traditional grid based methods rely heavily on the triangle inequality and packing bounds, and there are technical difficulties in adapting other method such as cone decompositions~\cite{chanNN} or approximate Voronoi diagrams~\cite{plebs}. We expect that we will need to exploit geometry of Bregman divergences more substantially. 
\section{Acknowledgements}
We thank Sariel Har-Peled and anonymous reviewers for helpful comments.


\appendix
\section{Proofs from Section \ref{sec:properties}}
\label{sec:bounded}
\begin{lemma}\label{app:1}
Given any interval  on the real line, there exists a finite  such that   is -defective with respect to . We require all order derivatives of  to be defined and bounded over the closure of ,
and  to be bounded away from zero.
\end{lemma}

\begin{proof}
Consider three points . 

Due to symmetry of the cases and conditions, there are three cases to consider: ,  and .
\begin{description}
\item[\textbf{Case 1:}]
Here . The following is trivially true by the monotonicity of .



\item[\textbf{Cases 2 and 3:}]
For the remaining symmetric cases,  and , note that since  and  are both bounded, continuous functions on a compact domain (the interval ), we need only show that the following limit exists:


First consider , and we assume . For ease of computation, we replace  by , to be restored 
at the last step. We will use the following Taylor expansions repeatedly in our derivation: , ,  and .  Here  denotes a tail of a Taylor expansion
around  (or equivalently a Maclaurin expansion in ) where the lowest order term is . Since we will be handling
multiple Taylor expansions in what follows, we will use subscripts of the form , , etc. to distinguish the tails
of different series.  


Computing the denominator, using the expansion that , we get:

Where in the third last step we set .

We now address the numerator, and begin by taking the same Taylor expansion.


We now take the McLaurin expansion of the square roots and note for the second such expansion we gain more higher order terms of  which we merge with
 to obtain a . 


Where the  is again obtained in the second last step from merging products involving one of  or  , as well as of the two terms involving  with each other. And in the last step, we set .

Now combine numerator and denominator back in equation \ref{mucomp} and observe a factor of  cancels out. 


Note now that since  is strictly convex, neither the numerator nor denominator of this expression approach  as 
 (or equivalently, ). So we can safely drop the higher order terms in the limit to obtain:


Substituting back  for , we see that limit \ref{mu-def-limit} exists, provided  is strictly convex:



\end{description}
The analysis follows symmetrically for case 3, where .\qedhere
\end{proof}


\begin{lemma}\label{app:2}
Given any interval  on the real line, there exists a finite  such that   is right-sided -defective with respect to .We require all order derivatives of  to be defined and bounded over the closure of ,
and  to be bounded away from zero. 
\end{lemma}

\begin{proof}
Consider any three points . We will prove that there exists finite  such that:



 Here there are now six cases to consider: , , , , , and  .

\begin{description}
\item[\textbf{Case 1 and 2:}]
Here .  By monotonicity we have that:


But by lemma \ref{firstTosecond}, we have that  for some parameter  defined over .
This implies that , i.e, it is bounded over . A similar analysis works for Case 2 where .

\item[\textbf{Cases 3 and 4:}]
For these two cases,  and , note that since  and  are both bounded, continuous functions on a compact domain (the interval ), we need only show that the following limit exists:


First consider , and we assume .  For ease of computation, we replace  by , to be restored 
at the last step. We will use the following Taylor expansions repeatedly in our derivation: , ,  and .  Here  denotes a tail of a Taylor expansion where the lowest order term is . Since we will be handling
multiple Taylor expansions in what follows, we will use subscripts of the form , , etc. to distinguish the tails
of different series.  


Computing the denominator by replacing  with  and taking the Taylor expansion of :

Where in the last step, we let .
We now address the numerator:

Where in the last step we took the Taylor Expansion of . Collecting terms of  and continuing, we obtain:

Where we note in the above that the new error term of  was produced by combining  with the error term
produced by taking the Maclaurin expansion of the square root.


Now combine numerator and denominator back in equation \ref{mucom} and cancel a factor of  accordingly, we get:


Now if we take  or equivalent , neither the numerator nor denominator of this
new expression become  and indeed we may drop the higher order terms of  safely.  Noting that , for some . 


Substituting back  for , we see that limit \ref{mubreg-def-limit} exists, provided  is strictly convex:



The analysis follows symmetrically for case 4, by noting that  and that , i.e we may suitably interchange  and .

\item[\textbf{Cases 5 and 6:}]
Here  or . Looking more carefully at the analysis for cases 3 and 4, note that the ordering  vs  does not affect the magnitude of the expression for limit  \ref{mubreg-def-limit}, only the sign. Hence we can use the same analysis to prove -defectiveness for cases 5 and 6.
\qedhere
\end{description}
\end{proof}

\begin{corollary}
 Given any interval  on the real line, there exists a finite 
 such that   is left-sided -defective with respect to  
\end{corollary}

\begin{proof}
Follows from similar computation. 
\end{proof}


\section{Discussion of empirical values of .}
\label{sec:muranges}
We calculate now the values of  observed for a selection of Bregman divergences points spread over a range of intervals; namely ,  and
.   Note that each of the values below is for the square root of the relevant divergence and that for the
Itakura Saito, Kullback-Liebler and Symmetrized Kullback-Liebler,  is a boundary point where distances approach infinity. 
Interestingly, lemma \ref{AallDmusim} implies that whatever bounds for  hold for points spread on an interval  also hold for points in the box . We observe that for reasonable spreads of points,
while  is not necessarily always small, it is also not a galactic constant as well.

\begin{center}
  \begin{tabular}{| l | c | r |}
    \hline
    Name & Interval Range & Value of  \\ \hline
     & [0.1 0.9] & 2.35 \\ 
     Itakura-Saito & [0.01 0.99] & 7.17 \\ 
     & [0.001 0.999] & 22.42 \\   
     \hline
     & [0.1 0.9] & 1.65 \\ 
     Kullback-Liebler & [0.01 0.99] & 3.67 \\ 
     & [0.001 0.999] & 9.18 \\   
     \hline
     & [0.1 0.9] & 1.22 \\ 
     Symmetrized Kullback-Liebler & [0.01 0.99] & 2.42 \\ 
     & [0.001 0.999] & 6.05 \\   
     \hline
     & [0.1 0.9] & 1.14 \\ 
     Exponential  & [0.001 0.999] & 1.18 \\ 
     & [0.001 100] & 9.95 \\   
     \hline
  \end{tabular}
\end{center}
\newpage
\bibliography{nn}
\bibliographystyle{acm}
\end{document}
