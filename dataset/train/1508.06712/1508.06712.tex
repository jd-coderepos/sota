\documentclass[copyright,creativecommons]{eptcs}
\providecommand{\event}{EXPRESS/SOS 2015}

\usepackage[ngerman,english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{extarrows}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{calc,positioning}

\bibliographystyle{eptcs}

\usepackage{count1to}
\usepackage{semantic}
\usepackage{url}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{cite}
\usepackage{bigdelim}
\usepackage{etoolbox}
\usepackage{hyperref}
\usepackage{cleveref}


\usepackage{macros}


\newcommand{\M}[1]{
  \textcolor{blue}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{blue}{\bfseries\sffamily M}}
}
\newcommand{\K}[1]{
  \textcolor{green}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{green}{\bfseries\sffamily K}}
}
\newcommand{\C}[1]{
  \textcolor{violet}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{violet}{\bfseries\sffamily C}}
}
\newcommand{\U}[1]{
  \textcolor{red}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{red}{\bfseries\sffamily U}}
}


\title{Encoding CSP into CCS
	\thanks{Supported by the DFG via project ``Synchronous and Asynchronous Interaction in Distributed Systems".}}

\author{Meike Hatzel 
	\institute{TU Berlin}
	\and Christoph Wagner
	\institute{TU Berlin}
	\and Kirstin Peters\thanks{Supported by the German Federal and State Governments via the Excellence Initiative (Institutional Strategy).}
	\institute{TU Dresden}
	\and Uwe Nestmann
	\institute{TU Berlin}
}
\def\titlerunning{Encoding CSP into CCS}
\def\authorrunning{M.\ Hatzel, C.\ Wagner, K.\ Peters, U.\ Nestmann}
\def\copyrightholders{Hatzel, Wagner, Peters \& Nestmann}


\begin{document}


\maketitle

\begin{abstract}
	We study encodings from CSP into asynchronous CCS with name passing and matching, so in fact, the asynchronous -calculus. By doing so, we discuss two different ways to map the multi-way synchronisation mechanism of CSP into the two-way synchronisation mechanism of CCS. Both encodings satisfy the criteria of Gorla except for compositionality, as both use an additional top-level context. Following the work of Parrow and Sj√∂din, the first encoding uses a centralised coordinator and establishes a variant of weak bisimilarity between source terms and their translations. The second encoding is decentralised, and thus more efficient, but ensures only a form of coupled similarity between source terms and their translations.
\end{abstract}


\section{Introduction}

In the context of a scientific meeting on Expressiveness in Concurrency and Structural Operational Semantics (SOS), likely very little needs to be said about the process algebras (or process calculi) CSP and CCS. Too many papers have been written since their advent in the 70's to be mentioned in our own paper; it is instructive, though, and recommended to appreciate Jos Baeten's historical overview \cite{Baeten:2005:BHP:1085667.1085669}, which also places CSP and CCS in the context of other process algebras like ACP and the many extensions by probabilities, time, mobility, etc. Here, we just select references that help to understand our motivation.

\vspace{0.3em}
\noindent
\textbf{Differences.}\;
From the beginning, although CSP \cite{hoare:78csp} and CCS \cite{CCS} were intended to capture, describe and analyse reactive and interactive concurrent systems, they were designed following rather different philosophies. Tony Hoare described this nicely in his position paper \cite{Hoare2006209} as follows: ``A primary goal in the original design of CCS was to discover and codify a minimal set of basic primitive agents and operators \dots and a wide range of useful operators which have been studied subsequently are all definable in terms of CCS primitives." and ``CSP was more interested in this broader range of useful operators, independent of which of them might be selected as primitive." So, at their heart, the two calculi use two different synchronisation mechanisms, one (CCS) using binary, \ie two-way, handshake via matching actions and co-actions, the other (CSP) using multiway synchronisation governed by explicit synchronisation sets that are typically attached to parallel composition. Another difference is the focus on Structural Operational Semantics in CCS, and the definition of behavioural equivalences on top of this, while CSP emphasised a trace-based denotational model, enhanced with failures, and the question on how to design models such that they satisfy a given set of laws of equivalence.

\vspace{0.3em}
\noindent
\textbf{Comparisons.}\;
From the early days, researchers were interested in more or less formal comparisons between CSP and CCS. This was carried out by both Hoare \cite{Hoare2006209} and Milner \cite{DBLP:conf/ifip/Milner86} themselves, where they concentrate on the differences in the underlying design principles. But also other researchers joined the game, but with different analysis tools and comparison criteria. 

For example, Brookes \cite{DBLP:conf/icalp/Brookes83} contributed a deep study on the relation between the underlying abstract models, synchronisation trees for CCS and the failures model of CSP. Quite differently, Lanese and Montanari \cite{Lanese200655} used the power to transform graphs as a measure for the expressiveness of the two calculi. 

Yet completely differently, Parrow and Sj{\"o}din \cite{sjodin:phd,parrowCoupled92} tried to find an algorithm to implement|best in a fully distributed fashion|the multiway synchronisation operator of CSP (and its variant LOTOS \cite{DBLP:conf/pstv/Brinksma85}) using the supposedly simpler two-way synchronisation of CCS. They came up with two candidates|a reasonably simple centralised synchroniser, and a considerably less simple distributed synchroniser\footnote{Recently \cite{7092761}, a slight variant of the protocol behind this algorithm was used to implement the distributed compiler DLC for a substantial subset of LNT (successor of LOTOS New Technology) that yields reasonably efficient C code.}|and proved that the two are not weakly bisimilar, but rather coupled similar, which is only slightly weaker. Coupled simulation is a notion that Parrow and Sj\"odin invented for just this purpose, but it has proved afterwards to be often just the right tool when analysing the correctness of distribution- and divergence-sensitive encodings that involve partial commitments (whose only effect is to gradually perform internal choices) \cite{nestmannPierce00}.

The probably most recent comparison between CSP and CCS was provided by van Glabbeek~\cite{DBLP:journals/corr/abs-1208-2750}. As an example for his general framework to analyse the relative expressive power of calculi, he studied the existence of syntactical translations from CSP into CCS, for which a common semantical domain is provided via labelled transition systems (LTS) derived from respective sets of SOS rules. The comparison is here carried out by checking whether a CSP term and its translation into CCS are distinguishable with respect to a number of equivalences defined on top of the LTS. The concrete results are: (1)~there is a translation that is correct up to trace equivalence (and contains deadlocks), and (2)~there is no translation that is correct up to weak bisimilarity equivalence that also takes divergence into account.

\vspace{0.3em}
\noindent
\textbf{Contribution.}\;
Given van Glabbeek's negative result, and given Parrow and Sj\"odin's algorithm, we set out to check whether we can define a syntactical encoding from CSP into CCS|using Parrow and Sj\"odin's ideas|that is correct up to coupled similarity.\footnote{The idea and a first draft of the encoding were developed by Nestmann and van Glabbeek during a stay at NICTA, Sydney.} We almost managed. In this paper, we report on our current results along these lines: 
(1)~Our encoding target is an asynchronous variant of CCS, but enhanced with name-passing and matching, so it is in fact an asynchronous -calculus; we kept mentioning CCS in the title of this paper, as it clearly emphasises the origin and motivation of this work. But, we could \emph{not} do without name-passing.
(2)~We exhibit one encoding that is not distributability-preserving (so, it represents a centralised solution), but is correct up to weak bisimilarity and does not introduce divergence. This does not contradict van Glabbeek's results, but suggests that van Glabbeek's framework implies some form of distributability-preservation.
(3)~We exhibit another encoding that \emph{is} distributability-preserving and divergence-reflecting, but is only correct up to coupled similarity.

\vspace{0.3em}
\noindent
\textbf{Overview.}\;
We introduce the considered variants of CSP and CCS in \S~\ref{sec:techPrel}. There we also introduce the criteria---that are (variants of) the criteria in \cite{gorla10} and \cite{petersNestmannGoltz13}---modulo which we prove the quality of the considered encodings. In \S~\ref{sec:innerPart} we introduce the inner layer of our two encodings. It provides the main machinery to encode synchronisations of CSP. We complete this encoding with an outer layer that is either a centralised (\S~\ref{sec:central}) or a decentralised coordinator (\S~\ref{sec:decentral}). In \S~\ref{sec:conclusion} we discuss the two encodings. Missing proofs and some additional informations can be found in \cite{hatzelTechRep15}.



\section{Technical Preliminaries}
\label{sec:techPrel}

A process calculus  consists of a set  of processes (syntax) and a reduction relation  (semantics).
Let  be the countably-infinite set of names.
 denotes an internal unobservable action.
We use  to range over names and  to range over processes.
We use  to range over .
 denotes a sequence of names.
Let  and   denote the sets of free names and bound names occurring in , respectively.
Their definitions are completely standard.
We use  to range over substitutions.
A substitution is a mapping  from names to names.
The application of a substitution on a term  is defined as the result of simultaneously replacing all free occurrences of  by  for .
For all names in  the substitution behaves as the identity mapping.
The relation  as defined in the semantics below defines the reduction steps processes can perform. We write  if  and call  a \emph{derivative} of .
Let  denote the reflexive and transitive closure of .
 is \emph{divergent} if it has an infinite sequence of steps .
We use \emph{barbs} or \emph{observables} to distinguish between processes with different behaviours. We write  if  has a barb , where the predicate  can be defined differently for each calculus. Moreover  has a weak barb , if  may reach a process with this barb, \ie .

As source calculus we use the following variant of CSP \cite{hoare:78csp}.

\begin{definition}\label{CSPSyntax}
The processes  are given by

	where  is a process variable, , and  is a finite index set.
\end{definition}

 is the parallel composition of  and , where  and  can proceed independently except for actions , on which they have to synchronise.
 describes \emph{divergence}.
 denotes \emph{inaction}.
\emph{Internal choice}  reduces to either  or  within a single internal step.
\emph{Concealment}  hides an action  and masks it as .
\emph{Renaming}  for some  extended by  behaves as , where  is replaced by  for all .
\emph{Recursion}  describes a process behaving like  with every occurrence of  being replaced by .
\emph{External choice}  offers a selection of one of the \emph{action prefixes}  followed by the corresponding continuation , so it may perform any  and then behave like . Note that we enforce action prefixes to be syntactically part of an external choice construct.
 As usual, we use  to denote binary external choice.

The CSP semantics is given by the following rules, using labelled steps  to define :
\vspace{-0.75em}

\vspace{-1.0em}\\
A barb of CSP is the possibility of a term, to perform an action, \ie .
Following the definition of distributability in \cite{petersNestmannGoltz13} a CSP term  is distributable into  if  are unguarded subterms of  such that every action prefix in  occurs in exactly one of the , where different but equally-named action prefixes are distinguished and unguarded occurrences of  may result in several copies of  within the .

As target calculus we use an asynchronous variant of CCS\cite{CCS} with name-passing and matching.

\begin{definition}\label{def:ccs_syntax}
  The processes  are given by
  
\end{definition}

 is the parallel composition of  and , where  and  can either proceed independently or synchronise on matching channels names.
 restricts the visibility of actions using names in~ to~.
 denotes input on channel .
 is output on channel .
Since there is no continuation, we interpret this calculus as asynchronous.
We use  to denote \emph{replicated input} on channel  with the continuation .
 is the matching operator, if  then  is enabled.
 denotes inaction.

The CCS semantics is given by following transition rules:
\vspace{-0.6em}

\vspace{-1em}\\
where  denotes structural congruence given by the rules: ,
, , ,  if , and .
As discussed in \cite{petersNestmannGoltz13}, a CCS term  is distributable into  if .

\vspace{0.5em}
\noindent
\textbf{Simulation Relations.}\;
The semantics of a process is usually considered modulo some behavioural equivalence.
For many calculi, \emph{the} standard reference equivalence is some form of weak bisimilarity.
In the context of encodings, the source and target language often differ in their relevant obervables, \ie barbs. In this case, it is advantageous to use a variant of reduction bisimilarity.
With Gorla \cite{gorla10}, we add a \emph{success} operator  to the syntax of both CSP and CCS. Since  cannot be further reduced, the semantics is left unchanged in both cases. The test for the reachability of success is standard in both languages, \ie .
To obtain a non-trivial equivalence, we require that the bisimulation respects success and the reachability of barbs.
We use the standard definition of barbs in CSP, \ie action prefixes.
Our encoding function will translate all source terms into closed terms, thus the standard definition of CCS barbs would not provide any information.
Instead we use a notion of translated barb () that reflects how the encoding function translates source term barbs. Its definition is given in Section~\ref{sec:innerPart}.

\begin{definition}[Bisimulation]
	A relation  is a \emph{(success-sensitive, [translated-]barb-respecting, weak, reduction) bisimulation} if, whenever , then:
	\begin{compactitem}
		\item  implies 
		\item  implies 
		\item  iff 
		\item  and  reach the same (translated) barbs, where we use  for CSP and  for CCS
	\end{compactitem}
	Two terms  are \emph{bisimilar}, denoted as , if there exists a bisimulation that relates  and .
\end{definition}

\noindent
We use the symbol  to denote either bisimilarity on our target language CCS or on the disjoint union of CSP and CCS that allows us to describe the relationship between source terms and their translations. In the same way we define a corresponding variant of coupled similarity.

\begin{definition}[Coupled Simulation]
	A relation  is a \emph{(success-sensitive, [translated-]barb-respecting, weak, reduction) coupled simulation} if, whenever , then:
	\begin{compactitem}
		\item  implies  and 
		\item  iff 
		\item  and  reach the same (translated) barbs, where we use  for CSP and  for CCS
	\end{compactitem}
	Two terms  are \emph{coupled similar}, denoted as , if there exists a coupled simulation that relates  and  in both directions.
\end{definition}

\vspace{0.3em}
\noindent
\textbf{Encodings and Quality Criteria.}\;
We consider two different translations from (the above-defined variant of) CSP into (the above-defined variant of) CCS with name passing and matching. 
In this context, we refer to CSP terms as \emph{source terms}  and to CCS terms as \emph{target terms} . 
Encodings often translate single source steps into a sequence or pomset of target steps. We call such a sequence or pomset a \emph{simulation} of the corresponding source term step.
Moreover, we assume for each encoding the existence of a so-called renaming policy , \ie a mapping of names from the source into vectors of target term names.

To analyse the quality of encodings and to rule out trivial or meaningless encodings, Gorla \cite{gorla10} provide a general framework comprising five quality criteria, which have afterwards been used in many papers.
In addition to our above-mentioned definition of process calculus, whough, Gorla requires the target calculus to be equipped with a notion of behavioural equivalence  on target terms. 
Its purpose is to describe the `abstract' behaviour of a target process, where `abstract' refers to an observer at the source level. 
In \cite{gorla10}, the equivalence  is often defined as a barbed equivalence (cf.~\cite{milner.sangiorgi:barbed-bisimulation}) or can be derived directly from the reduction semantics, and it typically is a congruence, at least with respect to parallel composition. 
Bisimilarity and coupled similarity are such relations on CCS terms.
The criteria are:
\begin{compactenum}[(1)]
	\item \emph{Compositionality}: The translation of an operator  is the same for all occurrences of that operator in a term, \ie it can be captured by a context  such that  for .
	\item \emph{Name Invariance}: The encoding does not depend on particular names, \ie for every  and , it holds that  if  is injective and  otherwise, where  is such that  for every .
	\item \emph{Operational Correspondence}: Every computation of a source term can be simulated by its translation, \ie  implies  (completeness), and every computation of a target term corresponds to some computation of the corresponding source term (soundness, compare to Section~\ref{sec:decentral}). 
	\item \emph{Divergence Reflection}: The encoding does not introduce divergence, \ie  implies .
	\item \emph{Success Sensitiveness}: A source term and its encoding answer the tests for success in exactly the same way, \ie  iff .
\end{compactenum}

Our encodings will satisfy all of these criteria except for compositionality, because both encodings consists of two layers.
\cite{petersNestmannGoltz13} shows that the above criteria do not ensure that an encoding preserves distribution and proposes an additional criterion for the preservation of distributability.

\begin{definition}[Preservation of Distributability]
	\label{def:distributabilityPreservation}
	
	An encoding  \emph{preserves distributability} if for every  and for all terms  that are distributable within  there are some  that are distributable within  such that  for all .
\end{definition}

\noindent
Here, because of the choice of the source and the target language, an encoding preserves distributability if for each sequence of distributable source term steps their simulations are pairwise distributable. 
In both languages two alternative steps of a term are in \emph{conflict} with each other if---for CSP---they reduce the same action-prefix or---for CCS---they either reduce the same input using two outputs or they reduce the same output using two [replicated] inputs. 
Two alternative steps that are not in conflict are \emph{distributable}.

\section{Translating the CSP Synchronisation Mechanism}
\label{sec:innerPart}

CSP and CCS---or the -calculus---differ fundamentally in their communication and synchronisation mechanisms.
In CSP there is only a single kind of action , where  is a name. 
Synchronisation is implemented by the parallel operator  that in CSP is augmented with a set of names  containing the names that need to be synchronised at this point. 
By nesting parallel operators arbitrarily many actions on the same name can be synchronised.
In CCS there are two different kinds of actions: inputs  and outputs . Again synchronisation is implemented by the parallel operator, but in CCS only a single input and a single matching output can ever be synchronised within one step.

To encode the CSP communication and synchronisation mechanisms in CCS with name passing we make use of a technique already used in \cite{petersNestmann12, peters12} to translate between different variants of the -calculus. CSP actions are translated into action announcements augmented with a lock indicating, whether the respective action was already used in the simulation of a step. The other operators of CSP are then translated into handlers for these announcements and locks.
The translation of sum combines several actions under the same lock and thus ensures that only one term of the sum can ever be used.
The translation of the parallel operator combines announcements of actions that need to be synchronised into a single announcement under a fresh lock, whose value is determined by the combination of the respective underlying locks at its left and right side. Announcements of actions that do not need to be synchronised are simply forwarded.
A second layer---containing either a centralised or a decentralised coordinator---then triggers and coordinates the simulation of source term steps.

Action announcements are of the form :  is the translation of the source term action.  is used to trigger the computation of the Boolean value of . The lock  evaluates to  as long as the respective translated action was not successfully used in the simulation of a step.  is used to guard the encoded continuation of the respective source term action. In the case of a successful simulation attempt involving this announcement, an output  allows to unguard the encoded source term continuation and ensures that all following evaluations of  return . The message  indicates an aborted simulation attempt and allows to restore  for later simulation attempts. Once a lock becomes , all request for its computation return .

\vspace{0.3em}
\noindent
\textbf{Abbreviations.}\;
We introduce some abbreviations to simplify the presentation of the encodings. We use
\vspace{-1.9em}

\vspace{-2em}\\
to test, whether an action belongs to the set of synchronised actions in the encoding of the parallel operator.
As already done in \cite{nestmann96, nestmannPierce00} we use Boolean-valued locks to ensure that every translation of an action is only used once to simulate a step.
\emph{Boolean locks} are channels on which only the Boolean values  (true) or  (false) are transmitted. 
An unguarded output over a Boolean lock with value  represents a positive instantiation of the respective lock, whereas an unguarded output sending  represents a negative instantiation. 
At the receiving end of such a channel, the Boolean value can be used to make a binary decision, which is done here within an \emph{-construct}.
This construct and according instantiations of locks are implemented as in \cite{nestmann96, nestmannPierce00} using restriction and the order of transmitted values.
\vspace{-0.5em}

\vspace{-2em}\\
We observe that the Boolean values  and  are realised by a pair of links without parameters. Both cases of the -construct operate as guard for its subterms  and . The renaming policy  reserves the names  and  to implement the Boolean values  and .

\vspace{0.3em}
\noindent
\textbf{The Algorithm.}\;
The encoding functions introduce some fresh names, that are reserved for special purposes. In Table~\ref{tab:resNam} we list the reserved names  and provide a hint on their purpose.
\begin{table}[t]
	\begin{tabular}{|c|c|}
		\hline
		reserved names & purpose\\
		\hline
		,  & announce the ability to perform an action\\
		, , ,  & (translated) source term channel, channel from the left/right of a parallel operator\\
		, ,  & lock, lock from the left/right of a parallel operator\\
		 & re-instantiate a positive sum lock\\
		, ,  & request the computation of the value of a lock\\
		, , ,  & simulate a source term step and unguard the corresponding continuations\\
		 & order left announcements for the same channel that need to be synchronised\\
		,  & distribute right announcements that need to be synchronised\\
		 & Boolean value ( or )\\
		 & fresh name used to announce -steps that result from concealment\\
		 & used by the centralised encoding to avoid overlapping simulation attempts\\
		 & fresh names used to encode internal choice\\
		 & fresh names used to encode divergence\\
		 & used to encode Boolean values\\
		\hline
	\end{tabular}
	\caption{Reserved Names.}
	\label{tab:resNam}
\end{table}
Moreover we reserve the names  and assume an injective mapping  that maps process variables of CSP to distinct names.
The renaming policy  for our encodings is then a function that reserves the names in  and translates every source term name into three target term names. More precisely, choose  such that:
\begin{compactenum}
	\item No name is mapped onto a reserved name, \ie  for all .
	\item No two different names are mapped to overlapping sets of names, \ie  for all  with .
\end{compactenum}
We naturally extend the renaming policy to sets of names, \ie  if .
Let  denote the projection of a -tuple to its th element, if . Moreover  for a set  of -tuples and .

\begin{figure}[htp]
	
	where  is short for ,  is short for , and  is short for .
	\caption{An encoding from CSP into CCS with value passing (inner part).}
	\label{fig:innerEncoding}
\end{figure}
The inner part of our two encodings is presented in Figure~\ref{fig:innerEncoding}. The most complex case is the translation of the parallel operator  that is based on the following four steps:
\begin{description}
	\item[Step 1:] Action announcements for channels \\
		In the case of actions on channels ---that do not need to be synchronised here---the encoding of the parallel operator acts like a forwarder and transfers action announcements of both its subtrees further up in the parallel tree.
		Two different restrictions of the channel for action announcements  from the left side  and the right side , allow to trace action announcements back to their origin as it is necessary in the following case.
		In the present case we use  to bridge the action announcement over the restrictions on .
	\item[Step 2:] Action announcements for channels \\
		Actions  need to be synchronised, \ie can be performed only if both sides of the parallel operator cooperate on this action. Simulating this kind of synchronisation is the main purpose of the encoding of the parallel operator.
		The renaming policy  translates each source term name into three target term names. The first target term name is used as reference to the original source term name and transferred in announcements. The other two names are used to simulate the synchronisation of the parallel operator in CSP. Announcements from the left are translated to outputs on the respective second name and announcements from the right to the respective third name. Restriction ensures that these outputs can only be computed by the current parallel operator encoding. The translations of the announcements into different outputs for different source term names allows us to treat announcements of different names concurrently using the term , where  is a source term name.
	\item[Step 3:] The term \\
		In  all announcements for the same source term name  from the left are ordered in order to combine each left and each right announcement on the same name. Several such announcements may result from underlying parallel operators, sums with similar summands, and junk left over from already simulated source term steps. For each left announcement a fresh instance of  is generated and restricted. The names  and  are used to transfer right announcements to the respective next left announcement, where  is used to bridge over the restriction on . This way each right announcement will eventually be transferred to each left announcement on the same name. Note that this kind of forwarding is not done concurrently but in the source language a term  also cannot perform two steps on the same name  concurrently. After combining a left and a right announcement on the same source term name a fresh set of auxiliary variables  is generated and a corresponding announcement is transmitted. The term  reacts to requests regarding this announcement and is used to simulate a step on the synchronised action.
	\item[Step 4:] The term \\
		If a request reaches  it starts questioning the left and the right side. First the left side is requested to compute the current value of the lock of the action. Only if  is returned, the right side is requested to compute its lock as well. This avoids deadlocks that would result from blindly requesting the computation of locks in the decentralised encoding. If the locks of both sides are still valid the fresh lock  returns  else  is returned. For each case  ensures that subsequently requests will obtain an answer by looping with  or returning  to all requests, respectively. The messages  and  cause the respective underlying subterms on the left and the right side to do the same, whereas  and  cause the unguarding of encoded continuations as result of a successful simulation of a source term synchronisation step.
\end{description}

\vspace{0.3em}
\noindent
\textbf{Basic Properties and Translated Observables.}\;
The protocol introduced by the encoding function in Figure~\ref{fig:innerEncoding} (and its outer parts introduced later) simulates a single source term step by a sequence of target term steps. Most of these steps are merely pre- and post-processing steps, \ie they do not participate in decisions regarding the simulation of conflicting source term steps but only prepare and complete simulations. Accordingly we distinguish between \emph{auxiliary steps}---that are pre- and post-processing steps---and \emph{simulation steps}---that mark a point of no return by deciding which source term step is simulated. Note that the points of no return and thus the definition of auxiliary and simulation steps is different in the two variants of our encoding.

Auxiliary steps do not influence the choice of source terms steps that are simulated. Moreover they operate on restricted channels, \ie are unobservable. Accordingly they do not change the state of the target term modulo the considered reference relations  and . We introduce some auxiliary lemmata to support this claim.

The encoding  translates source term barbs  into free announcements with  as first value and a lock  as third value that computes to . The two coordinators, \ie outer encodings, we introduce later, restrict the free -channel of .

\begin{definition}[Translated Barbs]
	Let  such that , , or .
	 has a translated barb , denoted by , if
	\begin{compactitem}
		\item there is an unguarded output ---on a free channel  in the case of  or the outermost variant of  in the case of the later introduced encodings  and ---in  or
		\item such an announcement was consumed to unguard an -construct testing  and this construct is still not resolved in 
	\end{compactitem}
	such that all locks that are necessary to instantiate  are positively instantiated.
\end{definition}

Analysing the encoding function in Figure~\ref{fig:innerEncoding} we observe that an encoded source term has a translated barb iff the corresponding source term has the corresponding source term barb.

\begin{obs}
	For all , it holds  iff .
	\label{obs:transBarbs}
\end{obs}

All instances of success in the translation result from success in the source. More precisely the only way to obtain  in the translation is by .

\begin{obs}
	For all , it holds  iff .
	\label{obs:success}
\end{obs}

The encoding propagates announcements through the translated parallel structure. In the translation of parallel operators it combines all left and right announcements \wrt to the same channel name, if this channel needs to be synchronised. Therefore we copy announcements.
We use locks carrying a Boolean value to indicate whether an announcement was already used to simulate a source term step. These locks carry  in the beginning and are swapped to  as soon as the announcement was used. In each state there is at most one positive instantiation of each lock and as soon as a lock is instantiated negatively it never becomes positive again.

\begin{lemma}
	Let  such that . Then for each variant  of the names 
	\begin{compactenum}
		\item there is at most one positive instantiation of  in ,
		\item if there is a positive instantiation of  in  then there is no other instantiation of  in ,
		\item if there is a negative instantiation of  in  then no derivative of  contains a positive instantiation of .
	\end{compactenum}
	\label{lem:sumLocks}
\end{lemma}

\section{The Centralised Encoding}
\label{sec:central}

Figure~\ref{fig:innerEncoding} describes how to translate CSP actions into announcements augmented with locks and how the other operators are translated to either forward or combine these announcements and locks. With that  provides the basic machinery of our encoding from CSP into CCS with name passing and matching. However it does not allow to simulate any source term step. Therefore we need a second (outer) layer that triggers and coordinates the simulation of source term steps. We consider two ways to implement this coordinator: a centralised and a decentralised coordinator. The centralised coordinator is depicted in Figure~\ref{fig:centralised}.

\begin{figure}
	
	\caption{A \textbf{centralised} encoding from CSP into CCS with value passing.}
	\label{fig:centralised}
\end{figure}

The channel  is used to ensure that simulation attempts of different source term steps cannot overlap each other. For each simulation attempt exactly one announcement is consumed. The coordinator then triggers the computation of the respective lock that was transmitted in the announcement. This request for the computation of the lock is propagated along the parallel structure induced by the translations of parallel operators until---in the leafs---encodings of sums are reached. There the request for the computation yields the transmission of the current value of the respective lock. While being transmitted back to the top of the tree, different locks that refer to synchronisation in the source terms are combined. If the computation of the lock results with  at the top of the tree, the respective source term step is simulated. Else the encoding aborts the simulation attempt and restores the consumed informations about the values of the respective locks. In both cases a new instance of  allows to start the next simulation attempt. Accordingly only some post-processing steps can overlap with a new simulation attempt.

As we prove below, the points of no return in the centralised encoding can result from the consumption of action announcements by the outer encoding in Figure~\ref{fig:centralised} if the corresponding lock computes to . Moreover the encoding of internal choice and divergence introduces simulation steps, namely all steps on variants of the channels , , and . All remaining steps of the centralised encoding are auxiliary.

\begin{definition}[Auxiliary and Simulation Steps]
	A step  such that  is called a \emph{simulation step}, denoted by , if  is a step on the outermost channel  and the computation of the value of the received lock  will return  or it is a step on a variant of , , or .
	
	Else the step  is called an \emph{auxiliary step}, denoted by .
	\label{def:auxStepsCentral}
\end{definition}

\noindent
Let  denote the reflexive and transitive closure of  and let .
Auxiliary steps do not change the state modulo .

\begin{lemma}
	 implies  for all target terms .
	\label{lem:auxStepsCentral}
\end{lemma}

By distinguishing auxiliary and simulation steps, we can prove a condition stronger than operational correspondence, namely that each source term step is simulated by exactly one simulation step.

\begin{lemma}
	For all , it holds  iff .
	\label{lem:sourceVsSimStep}
\end{lemma}

\noindent
This direct correspondence between source term steps and the points of no return of their translation allows us to prove a variant of operational correspondence that is significantly stricter than the variant proposed in \cite{gorla10}.

\begin{definition}[Operational Correspondence]
	\\
	An encoding  is \emph{operationally corresponding} \wrt  if it is:
	\begin{compactitem}
		\item[\; Complete:]  implies 
		\item[\; Sound:]  implies 
	\end{compactitem}
\end{definition}

\noindent
The `if'-part of Lemma~\ref{lem:sourceVsSimStep} implies operational completeness \wrt  and the `only-if'-part contains the main argument for operational soundness \wrt . Hence  is operationally corresponding \wrt to .

\begin{theorem}
	The encoding  is operationally corresponding \wrt to .
	\label{thm:operationalCorrespondenceCentral}
\end{theorem}

To obtain divergence reflection we show that there is no infinite sequence of only auxiliary steps.
Then divergence reflection follows from the combination of this fact and Lemma~\ref{lem:sourceVsSimStep}.

\begin{theorem}
	The encoding  reflects divergence.
	\label{thm:divergenceReflectionCentral}
\end{theorem}

The encoding function ensures that  has an unguarded occurrence of  iff  has such an unguarded occurrence. Operational correspondence ensures that  and  also answer the question for the reachability of  in the same way.

\begin{theorem}
	The encoding  is success sensitive.
	\label{thm:successSensitivenessCentral}
\end{theorem}

In a similar way we can prove that a source term reaches a barb iff its translation reaches the respective translated barb.

\begin{theorem}
	For all , it holds  iff .
	\label{thm:respectsBarbsCentral}
\end{theorem}

As proved in \cite{petersGlabbeek15}, Theorem~\ref{thm:operationalCorrespondenceCentral}, the fact that  is success sensitive and respects (translated) barbs, Theorem~\ref{thm:successSensitivenessCentral}, and Theorem~\ref{thm:respectsBarbsCentral} imply that for all  it holds  and  are (success sensitive, (translated) barb respecting, weak, reduction) bisimilar, \ie .
Bisimilarity is a strong relation between source terms and their translation. On the other hand, because of efficiency, distributability preserving encodings are more interesting.
Because of  the encoding  obviously does not preserve distributability. As discussed in \cite{parrowCoupled92} bisimulation often forbids distributed encodings. Instead they propose coupled simulation as a relation that still provides a strong connection between source terms and their translations but is more flexible. Following the approach in \cite{parrowCoupled92} we consider a decentralised coordinator next.

\section{The Decentralised Encoding}
\label{sec:decentral}

\begin{figure}
	
	\caption{A \textbf{decentralised} encoding from CSP into CCS with value passing.}
	\label{fig:decentralised}
\end{figure}

Figure~\ref{fig:decentralised} presents a decentralised variant of the coordinator in Figure~\ref{fig:centralised}.
The only difference between the centralised and the decentralised version of the coordinator is that the latter can request to check different locks concurrently. Technically  and  differ only by the use of . As a consequence the steps of different simulation attempts can overlap and even (pre-processing) steps of simulations of conflicting source term steps can interleave to a certain degree. Because of this effect,  does not satisfy the version of operational correspondence used above for , but  satisfies weak operational correspondence that was proposed in \cite{gorla10} as part of a set of quality criteria.

Since several announcements can be processed concurrently by the decentralised coordinator, here all consumptions of announcements are auxiliary steps. Instead the consumption of positive instantiations of locks can mark a point of no return. In contrast to  not every point of no return in  unambiguously marks a simulation of a single source term step, because in contrast to  the encoding  introduces \emph{partial commitments} \cite{peters12,petersNestmann12}.

Consider the example .

\noindent
  \begin{minipage}[c]{0.3\textwidth-2pt}
      \begin{tikzpicture}[auto,node distance=1.2cm]
        \node (E)                        {};
        \node (T)    [right of=E]        {};
        
        \node (T2)    [right=1cm of T]        {};
        \node (T1)    [above of=T2]        {};
        \node (T3)    [below of=T2]        {};

        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (E) -- (T);
        \draw[double] (E) -- (T);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T1.west) node [near end, below, rotate=40, scale = 0.7] {};
        \draw[double] (T) -- (T1.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T2.west) node [at end, below, scale = 0.7] {};
        \draw[double] (T) -- (T2.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T3.west) node [at end, below, rotate=-40, scale = 0.7] {};
        \draw[double] (T) -- (T3.west);
      \end{tikzpicture}
  \end{minipage}
  \begin{minipage}[c]{0.3\textwidth-2pt}
    \begin{center}
      \begin{tikzpicture}[auto,node distance=1.2cm]
        \node (T)                        {};
        \node (d1)    [right of=T]        {};
        \node (T2)    [right of=d1]        {};
        \node (T1)    [above of=T2]        {};
        \node (T3)    [below of=T2]        {};

        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T1.west) node [at end, below, rotate=40, scale = 0.7] {};
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T2.west) node [at end, below, scale = 0.7] {};
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T3.west) node [at end, below,rotate=-40, scale = 0.7] {};
      \end{tikzpicture}
      \end{center}
  \end{minipage}
  \begin{minipage}[c]{0.4\textwidth-2pt}
      \begin{tikzpicture}[auto,node distance=1.2cm]
        \node (E)                        {};
        \node (T)    [right of=E]        {};
        \node (P2)    [right=1cm of T]        {};
        \node (P1)    [above of=P2]        {};
        \node (T12)    [right=1cm of P2]        {};
        \node (T11)    [above of=T12]        {};
        \node (T3)    [below of=P2]        {PC_{1}\barbBisim \EncDO{P_{3}}};

        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (E) -- (T);
        \draw[double] (E) -- (T);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (P1);
        \draw[double] (T) -- (P1);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (P2);
        \draw[double] (T) -- (P2);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (P1) -- (T11.west) node [near end, below, scale = 0.7] {};
        \draw[double] (P1) -- (T11.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (P1) -- (T12.west) node [at end, below, rotate=-40, scale = 0.7] {};
        \draw[double] (P1) -- (T12.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T3.west) node [at end, below, rotate=-43, scale = 0.7] {};
        \draw[double] (T) -- (T3.west);
      \end{tikzpicture}
  \end{minipage}

  In the example, two sides of a parallel operator have to synchronise on either action , or action , or action  happens without synchronisation.
  In the centralised encoding  the use of  ensures that different simulation attempts cannot overlap. Thus, only after finishing the simulation of a source term step, the simulation of another source term step can be invoked. As a consequence each state reachable from encoded source terms can unambiguously be mapped to a single state of the source term. This allows us to use a stronger version of operational correspondence and, thus, to prove that source terms and their translations are bisimilar. The corresponding 1-to-1 correspondence between source terms and their translations is visualised by the first two graphs above, where .

  The decentralised encoding  introduces partial commitments.
  Assume the translation of a source term that offers several alternative ways to be reduced. Then some encodings---as our decentralised one---do not always decide on which of the source term steps should be simulated next. More precisely a partial commitment refers to a state reachable from the translation of a source term in that already some possible simulations of source term steps are ruled out, but there is still more than a single possibility left.
  
  In the decentralised encoding announcements can be processed concurrently and parts of different simulation attempts can interleave. The only blocking part of the decentralised encoding are conflicting attempts to consume the same positive instantiation of a lock.
  In the presented example above there are two locks; one for each side of the parallel operator. The simulations of the step on  and  need both of these locks, whereas to simulate the step on  only a positive instantiation of the right lock needs to be consumed.
  By consuming the positive instantiation of the left lock in an attempt to simulate the step on , the simulation of the step on  is ruled out, but the simulation of the step on  is still possible. Since either the simulation of the step on  or the simulation of the step on  succeeds, the simulation of the step on  is not only blocked but ruled out. But the consumption of the instantiation of the left lock does not unambiguously decide between the remaining two simulations. The intermediate state that results from consuming the instantiation of the left lock and represents a partial commitment is visualised in the right graph above by the state .
  
  Partial commitments forbid a 1-to-1 mapping between the states of  a source term and its translations by a bisimulation. But, as shown in \cite{parrowCoupled92}, partial commitments do not forbid to relate source terms and their translations by coupled similarity.

Whether the consumption of a positive instantiation of a lock is an auxiliary step---does not change the state of the term modulo ---, is a partial commitment, or unambiguously marks a simulation of a single source term step depends on the surrounding term, \ie cannot be determined without the context. For simplicity we consider all steps that reduce a positive instantiation of a lock as simulation steps.
Also steps on variants of the channels , , and  are simulation steps, because they unambiguously mark a simulation of a single source term step. All remaining steps of the decentralised encoding are auxiliary.

\begin{definition}[Auxiliary and Simulation Steps]
	A step  such that  is called a \emph{simulation step}, denoted by , if  reduces a positive instantiation of a lock or is a step on a variant of , , or .
	
	Else the step  is called an \emph{auxiliary step}, denoted by .
	\label{def:auxStepsDecentral}
\end{definition}

\noindent
Again let  denote the reflexive and transitive closure of  and let .
Since auxiliary steps do not introduce partial commitments, they do not change the state modulo . The proof of this lemma is very similar to the centralised case.

\begin{lemma}
	 implies  for all target terms .
	\label{lem:auxStepsDecentral}
\end{lemma}

In contrast to the centralised encoding, the simulation of a source term step in the decentralised encoding can require more than a single simulation step and a single simulation step not unambiguously refers to the simulation of a particular source term step. The partial commitments described above forbid operational correspondence, but the weaker variant proposed in \cite{gorla10} is satisfied. We call this variant weak operational correspondence.

\begin{definition}[Weak Operational Correspondence]
	\\
	An encoding  is \emph{weakly operationally corresponding} \wrt  if it is:
	\begin{compactitem}
		\item[\; Complete:]  implies 
		\item[\; Weakly Sound:]  implies 
	\end{compactitem}
\end{definition}

The only difference to operational correspondence is the weaker variant of soundness that allows for  to be an intermediate state that does not need to be related to a source term directly. Instead there has to be a way from  to some  such that  is related to a source term.

\begin{theorem}
	The encoding  is weakly operational corresponding \wrt to .
	\label{thm:operationalCorrespondenceDecentral}
\end{theorem}

As in the encoding , there is no infinite sequence of only auxiliary steps in .
Moreover each simulation of a source term requires only finitely many simulation steps (to consume the respective positive instantiations of locks). Thus  reflects divergence.

\begin{theorem}
	The encoding  reflects divergence.
	\label{thm:divergenceReflectionDecentral}
\end{theorem}

The encoding function ensures that  has an unguarded occurrence of  iff  has such an unguarded occurrence. Operational correspondence again ensures that  and  also answer the question for the reachability of  in the same way.

\begin{theorem}
	The encoding  is success sensitive.
	\label{thm:successSensitivenessDecentral}
\end{theorem}

Similarly, a source term reaches a barb iff its translation reaches the respective translated barb.

\begin{theorem}
	For all , it holds  iff .
	\label{thm:respectsBarbsDecentral}
\end{theorem}

Weak operational correspondence does not suffice to establish a bisimulation between source terms and their translations.
But, as proved in \cite{petersGlabbeek15}, Theorem~\ref{thm:operationalCorrespondenceDecentral}, the fact that  is success sensitive and respects (translated) observables, Theorem~\ref{thm:successSensitivenessDecentral}, and Theorem~\ref{thm:respectsBarbsDecentral} imply that  and  are (success sensitive, (translated) barbs respecting, weak, reduction) coupled similar, \ie .

It remains to show, that  indeed preserves distributability. Therefore we prove that all blocking parts of the encoding  refer to simulations of conflicting source term steps.

\begin{theorem}
	The encoding  preserves distributability.
	\label{thm:distributability}
\end{theorem}


\section{Conclusions}
\label{sec:conclusion}
 
We introduced two encodings from CSP into asynchronous CCS with name passing and matching.
As in \cite{parrowCoupled92} we had to encode the multiway synchronisation mechanism of CSP into binary communications and, similarly to \cite{parrowCoupled92}, we did so first using a centralised controller that was then modified into a decentralised controller.
By doing so we were able to transfer the observations of \cite{parrowCoupled92} to the present case:
\begin{compactenum}
	\item The centralised solution allows to prove a stronger connection between source terms and their translations, namely by bisimilarity. Our decentralised solution does not relate source terms and their translations that strongly and we doubt that any decentralised solution can do so.
	\item Nonetheless, decentralised solutions are possible as presented by the second encoding and they still relate source terms and their translations in an interesting way, namely by coupled similarity.
\end{compactenum}
Thus as in \cite{parrowCoupled92} we observed a trade-off between \emph{centralised} but \emph{bisimilar} solutions on the one-hand side and \emph{decentralised} but only \emph{coupled similar} solutions on the other side.

More technically we showed here instead a trade-off between centralised but \emph{operationally corresponding} solutions on the one-hand side and \emph{weakly operationally corresponding} but decentralised solutions on the other side.
The mutual connection between operational correspondence and bisimilarity as well as between weak operational correspondence and coupled similarity is proved in \cite{petersGlabbeek15}.

Both encodings make strict use of the renaming policy and translate into closed terms.
Hence the criterion \emph{name invariance} is trivially satisfied in both cases.
Moreover we showed that both encodings are \emph{success-sensitive}, \emph{reflect divergence}, and even \emph{respect barbs} \wrt to the standard source term (CSP) barbs and a notion of translated barbs on the target.
The centralised encoding  additionally satisfies a variant of \emph{operational correspondence} that is stricter than the variant proposed in \cite{gorla10}.
The decentralised encoding  satisfies \emph{weak operational correspondence} as proposed in \cite{gorla10} and \emph{distributability preservation} as proposed in \cite{petersNestmannGoltz13}.
Thus both encodings satisfy all of the criteria proposed in \cite{gorla10} except for compositionality.
However in both cases the inner part is obviously compositional and the outer part only adds a fixed context.

\providecommand{\thisvolume}[2][]{this volume of EPTCS}
\def\opa{}
\bibliography{cspToCcs}

\end{document}
