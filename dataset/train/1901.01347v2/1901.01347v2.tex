
\subsection{Derivation on the bound inequality in linear dynamic system\label{subsec:Derivation-on-theRNN}}

The linear dynamic system hidden state is described by the following
recursive equation:


By induction,


where  is some constant with respect to . In this case,
. By applying norm
sub-multiplicativity\footnote{If not explicitly stated otherwise, norm refers to any consistent
matrix norm which satisfies sub-multiplicativity. }, 


That is, .

\subsection{Derivation on the bound inequality in standard RNN\label{subsec:Derivation-on-theRNN-1}}

The standard RNN hidden state is described by the following recursive
equation:


From ,
by induction, 


where  and  converts
a vector into a diagonal matrix. As ,

is bounded by some value . By applying norm sub-multiplicativity, 


That is, .

\subsection{Derivation on the bound inequality in LSTM\label{subsec:Derivation-on-theLSTM}}

For the case of LSTM, the recursive equation reads:


Taking derivatives,


where  denotes the value in the output gate at -th timestep
(similar notations are used for input gate (), forget gate
() and cell value ()) and ``non-matrix'' terms actually
represent diagonal matrices corresponding to these terms. Under the
assumption that =0, we then make use of the results in \cite{miller2018recurrent}
stating that  is bounded for
all . By applying -norm sub-multiplicativity and
triangle inequality, we can show that



with



By applying -norm sub-multiplicativity and triangle inequality,



As LSTM is -contractive with  in the -norm
(readers are recommended to refer to \cite{miller2018recurrent} for
proof), which implies ,

as . For , under the assumption
that , we can always
find some value  such that . For
, . That is,
.

\subsection{Proof of theorem \ref{thm:The-average-amount}\label{subsec:Proof-of-theorem-1}}
\begin{proof}
Given that  with some ,
we can use  as the upper bound on 
with , respectively. Therefore,


where  is
continuous on . According to intermediate value theorem,
there exists  such that .
\end{proof}

\subsection{Proof of theorem \ref{thm:Under-the-assumption}\label{subsec:Proof-of-theorem-2}}
\begin{proof}
According to Theorem \ref{thm:The-average-amount}, there exists some
such that the summation of contribution
stored between  and  can be quantified as 
(after ignoring contributions before -th timestep for simplicity).
Let denote ,
we have .
Therefore, .
Let  and ,
the average contribution stored in a MANN has a lower bound quantified
as , where .
\end{proof}

\subsection{Proof of theorem \ref{thm:Given-the-number}\label{subsec:Proof-of-theorem}}
\begin{proof}
The second-order derivative of  reads:



We have  with 
and , so  is a concave function.
Thus, we can apply Jensen inequality as follows: 

Equality holds if and only if .
We refer to this as\textit{ Uniform Writing} strategy. By plugging
the optimal values of , we can derive the maximized average
contribution as  follows:


When , .
This is true for all writing strategies. Thus, Uniform Writing is
optimal for .
\end{proof}
We can show that this solution is also optimal for the case .
As  with ,
 is a convex function and Eq. (\ref{eq:jensen})
flips the inequality sign. Thus,  reaches its minimum
with Uniform Writing. For , minimizing 
is desirable to prevent the system from diverging. 

We can derive some properties of function . Let .
We have 
with , so 
is an increasing function if we fix  and let  vary. That explains
why having more memory slots helps improve memorization capacity.
If ,  becomes E.q (\ref{eq:d0}).
In this case, MANNs memorization capacity converges to that of recurrent
networks.

\subsection{Summary of synthetic discrete task format\label{subsec:Summary-of-synthetic}}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
Task & Input & Output\tabularnewline
\hline 
\hline 
Double &  & \tabularnewline
\hline 
Copy &  & \tabularnewline
\hline 
Reverse &  & \tabularnewline
\hline 
Add &  & \tabularnewline
\hline 
Max &  & \tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Synthetic discrete task's input-output formats.  is the sequence
length.}
\end{table}

\subsection{UW performance on bigger memory\label{subsec:UW-performance-on}}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
Model &  & Copy (L=500)\tabularnewline
\hline 
\hline 
DNC & 128 & 24.19\%\tabularnewline
\hline 
DNC+UW & 128 & 81.45\%\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Test accuracy (\%) on synthetic copy task. MANNs have 50 memory slots.
Both models are trained with 100,000 mini-batches of size 32.}

\end{table}

\subsection{Memory operating behaviors on synthetic tasks\label{subsec:Memory-writing-behaviours}}

In this section, we pick three models (DNC, DNC+UW and DNC+CUW) to
analyze their memory operating behaviors. Fig. \ref{fig:Memory-operations-on}
visualizes the values of the write weights and read weights for the
copy task during encoding input and decoding output sequence, respectively.
In the copy task, as the sequence length is 50 while the memory size
is 4, one memory slot should contain the accumulation of multiple
timesteps. This principle is reflected in the decoding process in
three models, in which one memory slot is read repeatedly across several
timesteps. Notably, the number of timesteps consecutively spent for
one slot is close to -the optimal interval, even for DNC ( Fig.
\ref{fig:Memory-operations-on}(a)), which implies that the ultimate
rule would be the uniform rule. As UW and CUW are equipped with uniform
writing, their writing patterns follow the rule perfectly. Interestingly,
UW chooses the first written location for the final write (corresponding
to the \textless eos\textgreater{} token) while CUW picks the last
written location. As indicated in Figs. \ref{fig:Memory-operations-on}(b)
and (c), both of them can learn the corresponding reading pattern
for decoding process, which leads to good performances. On the other
hand, regular DNC fails to learn a perfect writing strategy. Except
for the timesteps at the end of the sequence, the timesteps are distributed
to several memory slots while the reading phase attends to one memory
slot repeatedly. This explains why regular DNC cannot compete with
the other two proposed methods in this task. 

For the max task, Fig. \ref{fig:Memory-operations-on-1} displays
similar visualization with an addition of write gate during encoding
phase. The write gate indicates how much the model should write the
input at some timestep to the memory. A zero write gate means there
is no writing. For this task, a good model should discriminate between
timesteps and prefer writing the greater ones. As clearly seen in
Fig. \ref{fig:Memory-operations-on-1}(a), DNC suffers the same problem
as in copy task, unable to synchronize encoding writing with decoding
reading. Also, DNC's write gate pattern does not show reasonable discrimination.
For UW (Fig. \ref{fig:Memory-operations-on-1}(b)), it tends to write
every timestep and relies on uniform writing principle to achieve
write/read accordance and thus better results than DNC. Amongst all,
CUW is able to ignore irrelevant timesteps and follows uniform writing
at the same time (see Fig. \ref{fig:Memory-operations-on-1}(c)).

\begin{figure}
\begin{centering}
\includegraphics[width=1\textwidth]{fig/copy_w}
\par\end{centering}
\caption{Memory operations on copy task in DNC (a), DNC+UW (b) and DNC+CUW(c).
Each row is a timestep and each column is a memory slot.\label{fig:Memory-operations-on}}

\end{figure}
\begin{figure}
\begin{centering}
\includegraphics[width=1\textwidth]{fig/max_w}
\par\end{centering}
\caption{Memory operations on max task in DNC (a), DNC+UW (b) and DNC+CUW(c).
Each row is a timestep and each column is a memory slot.\label{fig:Memory-operations-on-1}}
\end{figure}

\subsection{Visualizations of model performance on sinusoidal regression tasks\label{subsec:Visualizations-of-model}}

We pick randomly 3 input sequences and plot the output sequences produced
by DNC, UW and CUW in Figs. \ref{fig:Sinusoid} (clean) and \ref{fig:Sinusoid-noisy}
(noisy). In each plot, the first and last 100 timesteps correspond
to the given input and generated output, respectively. The ground
truth sequence is plotted in red while the predicted in blue. We also
visualize the values of MANN write gates through time in the bottom
of each plots. In irregular writing encoding phase, the write gate
is computed even when there is no write as it reflects how much weight
the controller puts on the timesteps. In decoding, we let MANNs write
to memory at every timestep to allow instant update of memory during
inference. 

Under clean condition, all models seem to attend more to late timesteps
during encoding, which makes sense as focusing on late periods of
sine wave is enough for later reconstruction. However, this pattern
is not clear in DNC and UW as in CUW. During decoding, the write gates
tend to oscillate in the shape of sine wave, which is also a good
strategy as this directly reflects the amplitude of generation target.
In this case, both UW and CUW demonstrate this behavior clearer than
DNC. 

Under noisy condition, DNC and CUW try to follow sine-shape writing
strategy. However, only CUW can learn the pattern and assign write
values in accordance with the signal period, which helps CUW decoding
achieve highest accuracy. On the other hand, UW choose to assign write
value equally and relies only on its maximization of timestep contribution.
Although it achieves better results than DNC, it underperforms CUW. 

\begin{figure}
\begin{centering}
\noindent\begin{minipage}[t]{1\columnwidth}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dnc1}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dnc4}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dnc5}\end{minipage}
\par\end{centering}
\begin{centering}
\noindent\begin{minipage}[t]{1\columnwidth}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dncuw1}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dncuw4}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dncuw5}\end{minipage}
\par\end{centering}
\begin{centering}
\noindent\begin{minipage}[t]{1\columnwidth}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dnccuw1}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dnccuw4}\includegraphics[width=0.3\linewidth]{fig/sin/gen/sine-dnccuw5}\end{minipage}
\par\end{centering}
\caption{Sinusoidal generation with clean input sequence for DNC, UW and CUW
in top-down order. \label{fig:Sinusoid}}
\end{figure}
\begin{figure}
\begin{centering}
\noindent\begin{minipage}[t]{1\columnwidth}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dnc1}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dnc2}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dnc3}\end{minipage}
\par\end{centering}
\begin{centering}
\noindent\begin{minipage}[t]{1\columnwidth}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dncuw1}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dncuw2}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dncuw3}\end{minipage}
\par\end{centering}
\begin{centering}
\noindent\begin{minipage}[t]{1\columnwidth}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dnccuw1}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dnccuw2}\includegraphics[width=0.3\linewidth]{fig/sin/gen/nsine-dnccuw3}\end{minipage}
\par\end{centering}
\caption{Sinusoidal generation with noisy input sequence for DNC, UW and CUW
in top-down order. \label{fig:Sinusoid-noisy}}
\end{figure}

\subsection{Comparison with non-recurrent methods in flatten image classification
task\label{subsec:Comparsion-with-non-recurrent}}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|l|c|c|}
\hline 
Model & MNIST & pMNIST\tabularnewline
\hline 
\hline 
DNC+CUW & 99.1 & 96.3\tabularnewline
\hline 
The Transformer & 98.9 & 97.9\tabularnewline
\hline 
Dilated CNN & 98.3 & 96.7\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Test accuracy (\%) on MNIST, pMNIST. Previously reported results are
from \cite{vaswani2017attention} and \cite{chang2017dilated}.
\label{tab:mnist-1}}
\end{table}

\subsection{Details on document classification datasets\label{subsec:Details-on-document}}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|>{\centering}p{0.1\linewidth}|>{\centering}p{0.1\linewidth}|>{\centering}p{0.1\linewidth}|>{\centering}p{0.1\linewidth}|}
\hline 
Dataset & Classes & Average lengths & Max lengths & Train samples & Test samples\tabularnewline
\hline 
\hline 
IMDb & 2 & 282 & 2,783 & 25,000 & 25,000\tabularnewline
\hline 
Yelp Review Polarity (Yelp P.) & 2 & 156 & 1,381 & 560,000 & 38,000\tabularnewline
\hline 
Yelp Review Full (Yelp F.) & 5 & 158 & 1,381 & 650,000 & 50,000\tabularnewline
\hline 
AG's News (AG) & 4 & 44 & 221 & 120,000 & 7,600\tabularnewline
\hline 
DBPedia (DBP) & 14 & 55 & 1,602 & 560,000 & 70,000\tabularnewline
\hline 
Yahoo! Answers (Yah. A.) & 10 & 112 & 4,392 & 1,400,000 & 60,000\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Statistics on several big document classification datasets}

\end{table}

\subsection{Document classification detailed records\label{subsec:Document-classification-detailed}}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{Model} & AG & IMDb & Yelp P. & Yelp F.\tabularnewline
\hline 
\hline 
\multirow{4}{*}{UW} & 1 & 93.42 & \textbf{91.39} & \textbf{96.39} & 64.89\tabularnewline
\cline{2-6} 
 & 2 & 93.52 & 91.30 & 96.31 & 64.97\tabularnewline
\cline{2-6} 
 & 3 & \textbf{93.69} & 91.25 & 96.39 & \textbf{65.26}\tabularnewline
\cline{2-6} 
 & Mean/Std & 93.540.08 & 91.320.04 & 96.360.03 & 65.040.11\tabularnewline
\hline 
\multirow{4}{*}{CUW} & 1 & 93.61 & 91.26 & \textbf{96.42} & \textbf{65.63}\tabularnewline
\cline{2-6} 
 & 2 & \textbf{93.87} & 91.18 & 96.29 & 65.05\tabularnewline
\cline{2-6} 
 & 3 & 93.70 & \textbf{91.32} & 96.36 & 64.80\tabularnewline
\cline{2-6} 
 & Mean/Std & 93.730.08 & 91.250.04 & 96.360.04 & 65.160.24\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Document classification accuracy (\%) on several datasets reported
for 3 different runs. Bold denotes the best records.}

\end{table}

