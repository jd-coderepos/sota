

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}



\title{Classification and understanding of cloud structures via satellite images with EfficientUNet\\
}

\author{\IEEEauthorblockN{
Tashin Ahmed}
Dhaka, Bangladesh \\
tashinahmed@aol.com
\and
\IEEEauthorblockN{
Noor Hossain Nuri Sabab}
Dhaka, Bangladesh \\
nsabab@aol.com
}

\maketitle

\begin{abstract}

Climate change has been a common interest and the forefront of crucial political discussion and decision-making for many years. Shallow clouds play a significant role in understanding the Earth's climate, but they are challenging to interpret and represent in a climate model. By classifying these cloud structures, there is a better possibility of understanding the physical structures of the clouds, which would improve the climate model generation, resulting in a better prediction of climate change or forecasting weather update. Clouds organise in many forms, which makes it challenging to build traditional rule-based algorithms to separate cloud features. In this paper, classification of cloud organization patterns was performed using a new scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet as the encoder and UNet as decoder where they worked as feature extractor and reconstructor of fine grained feature map and was used as a classifier, which will help experts to understand how clouds will shape the future climate. By using a segmentation model in a classification task, it was shown that with a good encoder alongside UNet, it is possible to obtain good performance from this dataset. Dice coefficient has been used for the final evaluation metric, which gave the score of 66.26\% and 66.02\% for public and private leaderboard on Kaggle competition respectively.

\end{abstract}

\begin{IEEEkeywords}
EfficientNet, UNet, clouds, Dice coefficient
\end{IEEEkeywords}














\section{Introduction}

Clouds play an important role in climate by controlling the amount of solar energy that reaches the surface of the Earth and the amount of the Earth's energy that is radiated back into space. The more energy that is trapped inside the planet, the warmer the atmosphere becomes, giving rise to sea level via meltdown of polar ice caps and contributing to global warming. The less energy that is trapped, the colder the temperature becomes. Understanding the structure of the clouds gives a better insight into the planet's weather. Hence it is crucial to climatologists \cite{turner2007thin}. \textbf{Albedo} is a measure of how much energy is reflected without being absorbed \cite{twomey1974pollution}. White surfaces reflect the most energy; hence it has a high albedo, while dark surfaces absorb most energy, indicating a low albedo. Earth's albedo is 0.3, which indicate the warming of the climate \cite{palle2004changes}. Interpreting cloud structures provide useful insight into the abuse of Earth's climate and risks associated with it. Satellite images of clouds give a broader picture of the atmosphere, and interpreting the images provide information on the current situation of the planet.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{dataset_image.png}
    \caption{Masked Images of four different class samples.}
    \label{Masked}
\end{figure}



It is assumed that as the overall temperature of Earth increases, it will evaporate more water from the oceans, resulting in more clouds with different structures and variations \cite{leconte2013increased}. The general effect of clouds on climate change depends on which cloud types change, and whether they become more or less abundant, thicker or thinner, and higher or lower in altitude. There are several types of cloud structures. \textbf{Cirrus} clouds are the most abundant of all top-level clouds. Cirrus means a "curl of hair" \cite{dowling1990summary}. These feathery clouds are composed of ice and consist of long, thin streamers that are also known as mare's tails. A few scattered cirrus clouds is a good sign of good weather. However, a gradually increasing cover of web-like cirrus clouds is a sign of more humid air mass and storm is approaching. \textbf{Cirrostratus} clouds look like thin scattered lines that spread themselves across the sky. When these icy fragments cover the sky, they give the air a subtle, white appearance. These clouds can indicate the approach of rain. They are translucent so that the sun and moon can be readily seen through them. Cirrostratus clouds are usually visible 12 to 24 hours before a period of rain or snow \cite{gadsden1989noctilucent}. \textbf{Cirrocumulus} is another type of high cloud, which tend to be broad groupings of white streaks that are sometimes seemingly neatly aligned. During the summertime in the tropics, they could indicate an incoming of a hurricane \cite{mclean1957cloud}. There are many more forms of clouds which determine the weather and provides an indication of any natural disaster, which can be handled in a low-risk manner if correctly detected beforehand.



Formation of clouds and detecting their structures and patterns beforehand allows a lot of high-risk activities to be avoided previously. Aircraft flights are a high-risk activity that carries thousands of passengers at a given interval of time, and flying through clouds is similar to driving a car through a thick fog - it is difficult to see what is ahead, making it a challenging maneuver to accomplish, as a conequence learning about cloud formations and their potential dangers when flying is a vital part of pilot training in some countries \cite{zhang2011impact}. Cloud structures like \textbf{cumulonimbus} are a direct threat to aircraft \cite{mason2006ice}. Cloud-borne updrafts and downdrafts result in rapid and unpredictable changes to the lift force on aircraft wings. These changes cause the plane to lurch and jump about during flight, known as turbulence, which sometimes makes less experienced pilots lose control of the craft, and the result is often fatal, with high casualties. Cargo ships, on the other hand, rely a lot on the weather of the sea, which is often unpredictable and ever-changing. Cargoes usually have a tight schedule and delay causes a loss of hundreds of thousands of dollars in fuel consumption. Storms also delay shipping which contributes to the loss of millions of dollars. An early prediction of storms or change in weather saves many lives and millions of dollars.



Interpretation of satellite images of cloud structures requires the expertise of a well-trained meteorologist, although it is not feasible and not always readily available. An intelligent automated system to interpret the satellite images, therefore, becomes a promising alternative with ease of access and hence becomes quite desirable for the understanding of cloud structures. In this paper, a model has been developed to classify the clouds into four categories using satellite images, with classification architecture like EfficientNet and segmentation architecture UNet \cite{tan2019efficientnet} \cite{ronneberger2015u}.












\section{Dataset \label{dataset}}

The dataset consists of satellite images, courtesy of \href{https://worldview.earthdata.nasa.gov/}{ \textbf{NASA Worldview}}, gathered from Kaggle competition \color{blue}\href{ https://www.kaggle.com/c/understanding_cloud_organization/data}{\textit{"Understanding Clouds from satellite images"}}\color{black}. The images contain clouds of four classes namely: \textbf{Fish}, \textbf{Flower}, \textbf{Sugar} and \textbf{Gravel}. The images were taken from three regions, spanning 21 longitude and 14 latitude. The true-color images were taken from two polar-orbiting satellites, \href{https://en.wikipedia.org/wiki/Terra_(satellite)}{Terra} and \href{https://en.wikipedia.org/wiki/Aqua_(satellite)}{Aqua}. An image might be attached from two orbits, due to the small footprint of Moderate Resolution Imaging Spectroradiometer \href{https://modis.gsfc.nasa.gov/}{\textbf{(MODIS)}} onboard these satellites. The remaining area, which has not been covered by two succeeding orbits, is marked black, as shown in \textbf{Figure \ref{Masked}}.


The dataset was split into train and validation of 80:20. The training sample was perfectly balanced with 22184 images, which consists of  images, where each class has 5546 images. All images are of the same size, which is  pixels. 68 scientists labelled images in the train set, and 3 individual ones labelled each image. Several augmentation techniques were applied on the images, which is a process used to artificially expand the size of a training dataset by creating modified versions of images in the dataset, which improves the performance and ability of the model to generalise. Albumentations library has been used for augmentation \cite{buslaev2020albumentations}. This library efficiently implements an abundant variety of image transform operations that are optimized for performance. The images were augmented into four types: horizontal flip, vertical flip, random rotation 20 and grid distortion. Also some adjustment have been done in image size to feed into efficentUNet architecture mentioned in \textbf{Section \ref{effunet}}. 








\begin{figure*}[ht!]
 \center
  \includegraphics[width=\textwidth]{clouds_effUnetB0.pdf}
  \caption{Architecture of EfficientUNet with EfficientNet-B0 framework for semantic segmentation. Blocks of EfficientNet-B0 as encoder has been shown in \textbf{Figure \ref{mbconv}}}
  \label{efficientunet}
\end{figure*}





\section{Materials}\label{evaluation}



\subsection{Public and Private Leaderboard Score}
Since the dataset was gathered from a Kaggle competition, there were two sets of scores that competed among others. Public LB\footnote{LB: Leaderboard} scores are those which are shown while the competition is ongoing. It shows the outcome from a subset of the test dataset. Private LB scores get generated after the competition is over, that provides scores on the remaining test dataset. As a result, public LB scores are usually better than the private. For this particular competition, private score was calculated on 75\% of the test data. 


Pixel encoding technique was followed to participate in the submission of the competition since the image sizes were too large for Kaggle system  \cite{richards1993method}. As a result, instead of submitting an exhaustive list of indices for segmentation, pairs of values were submitted, which contained the start position and the run length of the image pixels. For example, a pair value of (1, 3) indicates that the pixel starts at 1 and run 3 pixels. The competition also required a space-delimited list of pairs. The predicted encodings were scaled by 0.25 per side, which scaled down the images of size  pixels in both train and test set to  pixels, hence allowing the scope to achieve reasonable submission evaluation times.






\subsection{Dice coefficient (DSC)}\label{dsc}

In this paper, the evaluation metric used is the Dice coefficient to measure the quality of the model, as instructed by Kaggle competition. It was used to compare the pixel-wise agreement between a predicted segmentation and corresponding ground truth, using the following equation:

\begin{center}

\end{center}

Here X is the predicted set of pixels, and Y is the ground truth. The dice coefficient is defined to be 1 when both X and Y are empty. The LB score is the mean of the Dice coefficients for each (Image, Label) pair in the test set.

Dice coefficients are slightly different from the more popular evaluation metric: accuracy of a model. They are used to quantify the performance of image segmentation methods. Some ground truth regions are annotated in the images, and then an automated algorithm is allowed to do it. The algorithm is validated by calculating the dice score, which is a measure of how similar the objects are and is calculated by the overlap of the two segmentations divided by the total size of the two objects \cite{dice1945measures}.












\subsection{Optimizer}  
Rectified Adam (RAdam) was used instead of Adam as an optimizer for high accuracy and fewer epochs \cite{liu2019variance}.   


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{clouds_effnetb0_mbconv.pdf}
    \caption{Architecture of EfficientNet-B0 with MBConv as Basic building blocks.}
    \label{mbconv}
\end{figure}




\subsection{Loss Function}
Categorical Cross-entropy (CCE) has been applied as a loss function as it is a multi-label classification task. It is designed to quantify the difference between two probability distributions.

Here  is the -th scalar value in the model output,  denotes the corresponding target value, and output size gives the number of scalar values in the model output.

This function is important to measure and distinguish two discrete probability distribution.  specifies probability that event  occurred and sum of all  is 1, indicating that precisely one event occurred. The negative sign ensures that the loss gets smaller when the distributions get closer to each other.

\textbf{Softmax} activation function is recommended with categorical crossentropy, which rescales the model output to ensure it has the right properties, as positive outputs are desirable so that logarithm of every output value  exists. The main appeal of this loss function is to compare two probability distributions. For the classification part loss score was calculated from the CCE and for the segmentation part it was calculated as .
DSC is a measure of overlap between corresponding pixel values of prediction and ground truth respectively. Range of DSC is between 0 and 1 as it is understandable from \textbf{Subsection \ref{dsc}} and the larger the better. So, Dice Loss (DICE) try to maximize the overlap between the above mentioned two sets (predcitions and ground truth pixel values) \cite{sudre2017generalised}.







\section{Description of applied architectures}
\subsection{EfficientNet}

EfficientNet presented by \href{https://ai.google/research/}{Google AI research} is considered as a group of CNN models, but with subtle improvements, it works better than its predecessors \cite{tan2019efficientnet}. It consists of 8 models from B0 to B7, where each subsequent model number refers to variants with more parameters and higher accuracy. EfficientNet works in three ways:
\begin{itemize}
    \item \textbf{Depthwise + Pointwise Convolution}: Depthwise convolution performs independently over each channel of input. This is a spatial convolution. Pointwise convolution projects the channel's output by the depthwise convolution onto a new channel space. This is a  convolution.
    \item \textbf{Inverse Res}: ResNet blocks consist of 
    \begin{itemize} 
        \item a layer that squeezes the channels
        \item a layer that extends the channels. 
    \end{itemize}
    In this way, it links skip connections to rich channel layers \cite{he2016deep}.
    \item \textbf{Linear Bottlneck}: In each block, it uses linear activation in the last layer to prevent loss of information from ReLU \cite{agarap2018deep}. 
\end{itemize}

Among the 8 models of EfficientNet, 6 models, namely from B0 to B5, were explored in this paper. Due to the rise in complexity, the remaining models were ignored as they produced underappreciated results with poor performance while absorbing precious runtime.

As mentioned earlier, EfficientNet has 8 models, B0 - B7, among which, first 6 models have been explored in this paper. The layers in each of the models (B0 - B7) can be created by using 5 standard modules shown in \textbf{Figure \ref{Common_blocks}}. 

\begin{itemize}
    \item \textbf{Module 1} acts as the starting block for the sub-blocks.
    \item \textbf{Module 2} acts as the initializing point for the first sub-block of all the 7 main blocks except the 1st block.
    \item \textbf{Module 3} is used as a skip connection block for all the sub-blocks.
    \item \textbf{Module 4} combines the skip connections that occurred in the first sub-blocks.
    \item \textbf{Module 5} combines each sub-block that is connected to its previous sub-block in a skip connection.
\end{itemize}


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{modules_1_to_5.pdf}
    \caption{Common modules used to implement layers of all 8 models of EfficientNet}
    \label{Common_blocks}
\end{figure}


The individual modules are further used in various order to create sub-blocks, as shown in \textbf{Figure \ref{submodules}}. It’s easy to observe the difference among the models, with a gradual increase in the number of sub-blocks. The main building block for EfficientNet is MBConv layer which is an inverted residual block originally applied in MobileNetV2 \cite{sandler2018mobilenetv2}. Basic building block of EfficientNet-B0 with respect to MBConv layers have been shown in \textbf{Figure \ref{mbconv}}. The 8 models of EfficientNet (B0 - B7) share the common blocks with subtle complexities in their architectures. 






EfficientNet is a scaled-up neural network architecture, where the models scale all dimensions with a compound coefficient, which is a newly proposed method known as \textbf{compound scaling} \cite{lee2020compounding}. Here scaling up is defined as a systematic, principled scaling of three factors, which are depth, width and resolution.

\begin{samepage}
\begin{itemize}
    \item \textbf{Width scale} adds more feature maps in each layer.
    \item \textbf{Depth scale} adds more layers to the network.
    \item \textbf{Resoultion scale} increase resolution of input images.
\end{itemize}
\end{samepage}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{submodules.pdf}
    \caption{Sub-blocks using individual modules presented in \textbf{Figure \ref{Common_blocks}}}
    \label{submodules}
\end{figure}





Every architecture has similarity with its earlier versions. The only difference is the different feature maps that increase the number of parameters. All the models have the same architecture as its previous one, except for the multiplied block (x2) that expands and covers more blocks. This gives a lot of parameters to be used in a calculation, making it a very robust model.
It’s not difficult to observe the changes among all the models, and they gradually increased the number of sub-blocks \cite{tan2019efficientnet}. Starting from EfficientNet-B0, compound scaling method was used to scale up with two steps:
\begin{itemize}
    \item \textbf{Step 1} coefficient was fixed to 1 assuming twice more resources to be available, and it enforces a small grid search for the networks depth, width and resolution constants.
    \item \textbf{Step 2} The constants then get fixed and scaled up the baseline network with different coefficient to obtain the successive versions from B1 to B7.
\end{itemize}



EfficientNet was prioritized in this paper due to limitations of Kaggle notebook as well. It was also the core reason why the remaining EfficientNet models were avoided, since they calculate a substantial amount of parameters, which takes a lot of processing power and time, producing a disappointing outcome.


\subsection{U-Net}
U-Net is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to produce more precise segmentation. The idea here is to enhance a contracting layer by successive layers, where pooling operations are replaced by upsampling operators and these layers increase the resolution of the output. A successive convolutional layer then learn to assemble a precise output based on this information \cite{ronneberger2015u}.

U-Net have a large number of feature channels in the upsampling part and it allows the network to propagate context information to higher resolution layers. As a result, the expansive path is more or less symmetric to the contracting part, and produces a u-shaped architecture \cite{long2015fully}. 

The network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path \cite{ronneberger2015u}.


\section{EfficientUnet}\label{effunet}
In conventional UNet, expansion path is nearly symmetric to the contracting path. In this work, EfficientNet was used as an encoder in contracting path instead
of conventional set of convolution layers. The decoder module is similar to the original UNet. Details of the proposed architecture are illustrated in \textbf{Figure \ref{efficientunet}}. The original input image size is  then resized the images to  for further processing. The number of levels, resolution and number of channels of each feature map is also shown in the \textbf{Figure \ref{efficientunet}}. First, the feature map of last logit of the encoder was upsampled bi-linearly by a factor of two and then concatenated with the feature map from encoder having same spatial resolution. It was followed by 3×3 convolution layers before again
upsampling by the factor of two. The process was repeated till the segmentation map of size equal to input image was reconstructed. The proposed architecture is asymmetric unlike the original UNet. Here, the contracting path is deeper than the expansion path. Inclusion of powerful CNN like EfficientNet as encoder improved overall performance of the algorithm \cite{baheti2020eff}.
















\section{Methods, Results and Discussion}
Experimentation has been undertaken by applying six different versions of EfficentNet architectures mentioned previously.
Mean Validation Accuracy (mVA) has been measured for all EfficentNet architectures for both baseline and fine-tuned versions.
The outcome of this part of the inspection has been presented in \textbf{Table \ref{mva}}.

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|l|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}EfficientNet\\ Version\end{tabular}} & \multicolumn{2}{l|}{mean validation accuracy} \\ \cline{2-3} 
   & Baseline & Fine Tuned \\ \hline
B0 & 0.670    & 0.836      \\ \hline
B1 & 0.656    & 0.829      \\ \hline
B2 & 0.657    & 0.827      \\ \hline
B3 & 0.659    & 0.825      \\ \hline
B4 & 0.640    & 0.798      \\ \hline
B5 & 0.601    & 0.732      \\ \hline
\end{tabular}
\caption{Mean Validation Accuracy Scores}
\label{mva}
\end{table}

Cross validation result along with the public and private LB score (DSC) has been presented in \textbf{Table \ref{lb}}.  


\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}EfficientNet\\ Version\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Cross\\ Validation\end{tabular}} &
  \multicolumn{2}{l|}{LB score} \\ \cline{3-4} 
   &         & Private & Public  \\ \hline
B0 & 0.6389  & 0.64595 & 0.65936 \\ \hline
B1 & 0.6423  & 0.64640 & 0.65849 \\ \hline
B2 & 0.6351 & 0.64551 & 0.65801 \\ \hline
B3 & 0.6311  & 0.64585 & 0.65820 \\ \hline
B4 & 0.6294  & 0.63911 & 0.65563 \\ \hline
B5 & 0.6255  & 0.64059 & 0.65325 \\ \hline
\end{tabular}
\caption{Cross Validation, Private and Public DSC LB Scores of EfficientNet Architecture for Classification}
\label{lb}
\end{table}



The mean validation accuracy (mVA) score was best for the B0 model of EfficientNet architecture, as shown in \textbf{Table \ref{mva}}, although the other versions (B1-B5) came close on both private and public leaderboard scores, as shown in \textbf{Table \ref{lb}}. The cross validation scores weren't stable since the image segmentation wasn't reliable, with fluctuations in the outcome, having only 63.89\% in B0 while the other models had a higher value, as a result it was important to use an efficient segmentation approach to reach a stable and accurate result, as shown in \textbf{Table \ref{ulb}}. It became visible that an improved segmentation of images stabilized the scores, with a gradual trend in the scores and model B0 showing the best response in both cross validation and LB values, with 66.54\% in cross validation, 66.02\% in private and 66.26\% in public leaderboard. The other models werent far behind either, and the scores decreased in a descending order with B1 being the second-best model with 66.01\% on cross validation, 65.53\% on private and 65.98\% on public leaderboard.








\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}EfficientNet\\ Version\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Cross\\ Validation\end{tabular}} &
  \multicolumn{2}{l|}{LB score} \\ \cline{3-4} 
   &        & Private & Public \\ \hline
B0 & 0.6654 & 0.6602  & 0.6626 \\ \hline
B1 & 0.6601 & 0.6553  & 0.6598 \\ \hline
B2 & 0.6589 & 0.6570  & 0.6578 \\ \hline
B3 & 0.6588 & 0.6500  & 0.6582 \\ \hline
B4 & 0.6425 & 0.6417  & 0.6421 \\ \hline
B5 & 0.6322 & 0.6319  & 0.6333 \\ \hline
\end{tabular}
\caption{Cross Validation, Private and Public DSC LB Scores of EfficientNetUnet for Classification}
\label{ulb}
\end{table}













\section {Conclusion} 


This paper implements classification of satellite images of cloud structures into four different classes: Sugar, Gravel, Fish and Flower. 6 versions of EfficientNet from B0 to B5 were used as encoder and UNet as decoder has been applied. Dice coefficient was used as the evaluation metric. The scores used were compared with both public and private LB scores of Kaggle competition. By using a segmentation model like UNet in a classification problem, it was proven that with a good encoder it is possible to achieve good performance from the dataset. Although EfficientNet was used in this paper, it could be replaced with a different model as well but was not tested in this research. Also, a good segmentation of the images boost the output of the classification drastically, which was also proven in this paper. In future, estimation of the distribution of classes and adjustment of the validation set could be implemented accordingly, although it would only be ideal for the competition. As the complex architectures of EfficientNet gave less appreciating results because of default coefficients, altering these hyperparameters according to the dataset will hopefully improve the outcome. Exploring gradient weighted class activation mapping to generate a baseline, which is a class explainability technique, could be achieved.






\section*{Acknowledgment}
We thank \href{https://www.kaggle.com/}{Kaggle} for hosting such a great contest and for their kernel. We also thank \href{https://www.mpimet.mpg.de/en/mpimet-homepage/}{Max Planck Institute for Meteorology} for providing the dataset. We also admire \href{https://github.com/qubvel}{Pavel Yakubovskiy} for sharing his contribution in GitHub repository from where we managed to access the pretrained imagenet weights.

\bibliography{bibs} 
\bibliographystyle{ieeetr}

\newpage
\clearpage
\onecolumn
\appendix 





\section{}{\textbf{PR Curves}}

Precision-Recall(PR) curve demonstrates the relationship between precision (positive predictive value) and recall (sensitivity) of a machine learning model. Precision gives the percentage of the relevant outcome, while recall indicates the total consistent result. PR curve is a vital evaluation metric as it provides a more informative picture of an algorithm's performance. X-axis shows recall while the Y-axis shows precision. 

The precision-recall curve shows the trade-off between precision and recall for different threshold. A high area under the curve shows both high recall and high precision, where high precision relates to a low false-positive rate, and high recall refers to a flat false-negative rate. Since cloud structures are complex, it was essential to understand whether the implemented model was correctly detecting the shapes and evaluating true positive, true negative scores appropriately, hence PR curve was a crucial evaluation metric to understand the learning rate of the model.

Precision-Recall curves for all four classes are shown below. Highest precision (0.74) corresponding to recall threshold was obtained for class: Sugar and lowest for class: Fish which was 0.55, while the maximum and minimum recall for class: Flower was 0.49 and class: Fish was 0.22.







\begin{figure*}[h!]
 \center
  \includegraphics[width=\textwidth]{clouds_four_pr_auc.pdf}
  \caption{PR-Curves for All Four Classes}
  \label{AAA}
\end{figure*}

\newpage




\section{}{\textbf{PR-AUC; EfficientNet-B0}}

It takes nearly 25 epochs to train the model to reach the best result while mean PR AUC still increases even after 30 epochs for both train and validation set, showed in \textbf{Figure \ref{mean}}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{mean.jpg}
    \caption{Training and Validation PR-AUC}
    \label{mean}
\end{figure}




\section{}{\textbf{Loss Graph; EfficientNet-B0}}

Train and validation loss graph of EfficentNet-B0 which clearly shows that EfficientNet architecture is not enough to train this data as the loss is not decreasing.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{loss.jpg}
    \caption{Training and Validation Loss (CCE) for EfficentNet-B0}
    \label{loss}
\end{figure}

\newpage
\section{}{\textbf{Loss Graph; EfficientUNet-B0}}

Applying EfficientNet as encoder gave a huge success than only EfficientNet as segmentation model, since UNet was able to do the segmentation way better than a regular classification model.  

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{effunetb0.png}
    \caption{Training and Validation Loss (DICE) for EfficentUNet-B0}
\end{figure}
\section{}{\textbf{Segmentation Results}}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg0.png}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg1.png}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg2.png}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg3.png}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg4.png}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg5.png}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{seg6.png}
\end{figure}



\end{document}
