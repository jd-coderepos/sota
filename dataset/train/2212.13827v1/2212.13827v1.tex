\documentclass{article}









\usepackage[numbers]{natbib}
\usepackage[final,nonatbib]{neurips_2022}



\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}   
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=darkpastelgreen,bookmarks=false]{hyperref}
\usepackage{soul}
\usepackage{multirow}
\usepackage{preamble}
\usepackage{bbm}
\def\sS{{\mathbb{S}}}
\def\sB{{\mathbb{B}}}
\usepackage{bbm}
\usepackage{preamble}
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\newcommand{\Cross}{}\title{Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data}
\bibliographystyle{plainnat}




\newcommand{\etal}{\textit{et al}.} 
\newcommand{\ie}{\textit{i}.\textit{e}. }
\newcommand{\eg}{\textit{e}.\textit{g}. }
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         \newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   \usepackage{adjustbox} \newcommand{\hrnote} [1]
{\textcolor{blue}{[HR: #1]}}
\usepackage{}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig} \usepackage{multirow} \definecolor{mygray}{gray}{0.9} \usepackage[table]{colortbl}
\usepackage{algorithm}
\newcolumntype{?}{!{\vrule width 1pt}}
\author{Harsh Rangwani\thanks{Equal Contribution} \qquad Sumukh K Aithal\footnotemark[1]
  \qquad Mayank Mishra \qquad R. Venkatesh Babu \\
  Video Analytics Lab, Indian Institute of Science, Bengaluru, India \\ 
  \texttt{ \{harshr@iisc.ac.in, sumukhaithal6@gmail.com, }\\
  { \texttt{mayankmishra@iisc.ac.in, venky@iisc.ac.in\}}}
}

\begin{document}


\maketitle
    

\begin{abstract}




Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin-based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converge to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2\% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4\% across imbalanced datasets. The code is available at \href{https://github.com/val-iisc/Saddle-LongTail}{https://github.com/val-iisc/Saddle-LongTail}.





 \end{abstract}

\section{Introduction}
In recent years, there has been a lot of progress in visual recognition thanks to the availability of well-curated datasets~\cite{krizhevsky2009learning, russakovsky2015imagenet}, which are artificially balanced in terms of the frequency of samples across classes. However, modern real-world datasets are often imbalanced (\ie long-tailed etc.)~\cite{krishna2017visual, thomee2016yfcc100m, van2017devil} and suffer from various kinds of distribution shifts.  Overparameterized models like deep neural networks usually overfit classes with a high frequency of samples ignoring the minority (tail) ones~\cite{buda2018systematic, van2017devil}. In such  scenarios, when evaluated for metrics that focus on performance on minority data, these models perform poorly. These metrics are an essential and practical criterion for evaluating models in various domains like fairness~\cite{cotter2019training}, medical imaging~\cite{zhang2019medical} etc.
 
 Many approaches designed for improving the generalization performance of models trained on imbalanced data, are based on the re-weighting of loss~\cite{cui2019class}. The relative weights for samples of each class are determined, such that the expected loss closely approximates the testing criterion objective~\cite{cao2019learning}. In recent years, re-weighting techniques such as Deferred Re-Weighting (DRW)~\cite{cao2019learning}, and Vector Scaling (VS) Loss~\cite{kini2021label} have been introduced, which improve over the classical re-weighting method of weighting the loss of each class sample proportionally to the inverse of class frequency. However, even these improved re-weighting techniques lead to overfitting on the samples of tail classes. Also, it has been shown that use of re-weighted loss for training deep networks converges to final solutions similar to the un-weighted loss case, rendering it to be ineffective~\cite{byrd2019effect}.


This work looks at the loss landscape in weight space around final converged solutions for networks trained with re-weighted loss. We find that the generic Hessian-based analysis of the average loss used in prior works~\cite{pmlr-v97-ghorbani19b, foret2021sharpnessaware}, does not uncover any interesting insights about the sub optimal generalization on tail classes (Sec. \ref{saddle_analysis}). As the frequency of samples is different for each class due to imbalance, we analyze the Hessian of the loss for each class. This proposed way of analysis finds that re-weighting cannot prevent convergence to saddle points in the region of high negative curvature for tail classes, which eventually leads to poor generalization~\cite{dauphin2014identifying}. Whereas for head classes, the solutions converge to a minima with almost no significant presence of negative curvature, similar to networks trained on balanced data. This problem of converging to saddle points has not received much traction in recent times, as the negative eigenvalues disappear when trained on balanced datasets, indicating convergence to local minima~\cite{chaudhari2019entropy, pmlr-v97-ghorbani19b}. However, surprisingly our analysis shows that convergence to saddle points is still a practical problem for neural networks when they are trained on imbalanced (long-tailed) data (Fig. \ref{fig:overview}).


\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{Figures/overview_7.pdf}
  \caption{
With class-wise Hessian analysis of loss, we observe that when deep neural networks are trained on class-imbalanced datasets, the final solution for tail classes reach a region of large negative curvature indicating convergence to saddle point (bottom), whereas the head classes converge to a minima (top). The properties of the loss landscape (saddle points or minima) can be observed by analyzing eigen spectral density (centre). \protect\footnotemark{} }
  \label{fig:overview}
\vspace{-1em}
\end{figure*}

A pletheora of optimization methods in literature have been designed to be able to escape saddle points efficiently~\cite{ge2015escaping, jin2017escape, Jin2019StochasticGD}, some of which involve adding a component of isotropic noise to gradients. However, these methods have not been able to improve the performance of deep networks in practice, as the implicit noise of SGD in itself mitigates the issue of saddle points when trained on balanced data~\cite{daneshmand2018escaping, Jin2019StochasticGD}. However in the case of imbalanced datasets, we find that the component of SGD along negative curvature (i.e., implicit noise) is insufficient to escape saddle points for minority classes. Thus, learning on imbalanced data can be serve as a practical benchmark for optimization algorithms that can escape saddle points. 

We further demonstrate that Sharpness-Aware Minimization (SAM)~\cite{foret2021sharpnessaware} a recent optimization technique, with re-weighting can effectively enhance the gradient component along the negative curvature, allowing effective escape from saddle points which leads to improved generalization performance. We find that SAM can significantly improve the performance across various re-weighting and margin enhancing methods designed for long-tailed and class-imbalanced learning. The significant improvements are also observed on large-scale datasets of ImageNet-LT and iNaturalist 2018, demonstrating our resutls' applicability at scale. 
We summarize our contributions below:

\begin{itemize}
    \item We propose class-wise Hessian analysis of loss which reveals convergence to saddle points in the loss landscape for minority classes. We find that even loss re-weighting solutions converge to saddle point, leading to sub-optimal generalization on the minority classes. 
    \item We theoretically demostrate that SAM with re-weighting and high regularization factor significantly enhances the component of stochastic gradient along the direction of negative curvature , that results in effective escape from saddle points.
    \item We find that SAM can successfully enhance the performance of even state-of-the-art techniques for learning on imbalanced datasets which have a re-weighting component (\eg VS Loss and LDAM) across various datasets and degrees of imbalance.
\end{itemize}











\footnotetext{Figures for the minima and saddle point are from \protect\citep{arora2020theory} and used for illustration purposes only.}
\section{Related Work \& Background}
In this work, we use  to denote the output of a model,   to denote the gradients with respect to parameters,  and  denote the data and labels, respectively. 
We review the re-weighting methods used for training on imbalanced data with distribution shifts, followed by optimization techniques related to our work.

\subsection{Long-Tailed Learning}
Re-sampling \cite{buda2018systematic} and Re-weighting \cite{5128907} are the most commonly used methods to train on class-imbalanced datasets. Oversampling the minority classes \cite{chawla2002smote} and undersampling the majority classes \cite{buda2018systematic} are two approaches to re-sampling. Oversampling leads to overfitting on the tail classes, and undersampling discards a large amount of data, which inevitably results in poor generalization. 
~\citet{Kang2020Decoupling} proposed to decouple representation learning and classifier training to improve performance with the same. Mixup Shifted Label-Aware Smoothing model (MiSLAS) \cite{zhong2021improving} aims to improve the calibration of models trained on long-tailed datasets by mixup and label-aware smoothing and thereby improve performance. RIDE \cite{wang2021longtailed} and TADE \cite{zhang2021test} are ensemble-based methods that achieve state-of-the-art on the long-tailed visual recognition.  \citet{Samuel_2021_ICCV} introduces a new loss, DRO-LT, based on distributionally robust optimization for learning balanced feature representations. We explore the problem of training class-imbalanced datasets through the lens of optimization and loss landscape. We will now describe some representative recent effective methods in detail, which we will use as baselines. Additional discussion on long-tailed learning methods is present in App. \ref{app:related_work}.

\textbf{LDAM \cite{cao2019learning}}: LDAM introduces optimal margins for each class based on reducing the error through a generalization bound. It results in the following loss function where  is the margin for each class:

The core idea of LDAM is to regularize the classes with low frequency (low \ie ) more, in comparison to the head classes with high frequency.

\textbf{DRW \cite{cao2019learning}}: Deferred Re- Weighting refers to training the model with average loss till certain epochs (K), then introducing weight  proportional to   to loss term specific to each class  at a later stage. This way of re-weighting has been shown to be effective for improving generalization performance when combined with various losses such as Cross Entropy (CE), LDAM etc. We will be using CE+DRW method as a representative re-weighting method for our analysis. We define CE+DRW loss below for completeness:



\textbf{VS\cite{kini2021label}}: Vector Scaling loss is a recently proposed loss function which unifies the idea of multiplicative shift (CDT shift \cite{ye2020identifying}), additive shift (i.e Logit Adjustment~\cite{menon2020long}) and loss re-weighting. The final loss has the following form:

Here the  and  are the multiplicative and additive logit hyperparameters, respectively.     
\subsection{Loss Landscape}
\textbf{Saddle Points}: Saddle points are regions in loss landscape that usually depict a plateau region with some negative curvature. In the non-convex setting, it has been shown that there is an existence of an exponential number of saddle points in loss landscape and convergence to these points demonstrate poor generalization~\cite{dauphin2014identifying}. There has been a lot of effort in developing methods for effectively escaping saddle points which involve the addition of noise (e.g., Perturbed Gradient Descent (PGD)~\cite{ge2015escaping, jin2017escape, Jin2019StochasticGD}). However, these algorithms have not received much attention in the deep learning community as it has been shown that the implicit noise in SGD can escape saddles easily and converge to local minima~\cite{daneshmand2018escaping}. Also, it has been empirically shown that negative eigenvalues from the Hessian spectrum disappear after a few steps of training, indicating escape from saddle points when neural networks are trained on balanced datasets~\cite{alain2019negative, chaudhari2019entropy, sagun2017empirical}. However, contrary to this, we demonstrate that convergence to saddle points is prevalent in minority class loss landscapes and is a practical problem that can serve as a practical benchmark for the development of algorithms that escape saddle points.  


\textbf{Flat Minima based Optimization methods}: Empirically, it has been shown that converging to a flat minima in loss landscape for a deep network leads to improved generalization in comparison to sharp minima ~\cite{hochreiter1997flat, keskar2016large}. Recent works have tried to exploit this connection between the geometry of the loss landscape and generalization to achieve lower generalization error. Sharpness-Aware Minimization (SAM) \cite{foret2021sharpnessaware} is one such algorithm that aims to simultaneously minimize the loss value and sharpness of the loss surface. SAM has shown impressive generalization abilities across various tasks including Natural Language processing \cite{bahri2021sharpness}, meta-learning \cite{abbas2022sharp} and domain adaptation \cite{rangwani2022closer}.
Low-Pass Filtering SGD (LPF-SGD) \cite{bisla2022low} is another recently proposed optimization algorithm that aims to recover flat minima from the optimization landscape. LPF-SGD convolves the loss function with a Gaussian Kernel with variance proportional to the norm of the parameters of each filter in the network. In this work, we aim to explore the effectiveness of such algorithms for the task of escaping saddle points, which is a new direction for these algorithms.


\begin{figure}[t]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/average_sgd_caption.pdf}  

  \label{fig:sub-first2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/class_0_sgd_caption.pdf}  

  \label{fig:sub-second1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/class_8_sgd_caption.pdf}  

  \label{fig:sub-second2}
\end{subfigure}
\newline

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/average_sam_caption.pdf}  

  \label{fig:sub-third1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/class_0_sam_caption.pdf}  

  \label{fig:sub-fourth3}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/class_8_sam_caption.pdf}  

  \label{fig:sub-second3}
\end{subfigure}
\caption{Eigen Spectral Density (Class-wise) on the head class (Class 0) and tail class (Class 8) with SGD and SAM. It can be observed that with the head classes, the validation accuracy with SGD (B) and SAM (E) are similar and the density of negative eigenvalues is not significant. On the tail class, SAM (F) escapes the saddle points (large density of negative eigenvalues) in SGD (C), leading to 20\% increase in validation accuracy. A and D show the overall spectral density calculated across all samples in the dataset. Overall spectral density does not indicate the presence of saddles.}
\label{fig:head-tail-esd}
\vspace{-1em}
\end{figure}


\section{Convergence to Saddle Points in Tail Class Loss Landscape}
\label{saddle_analysis}
This section analyzes the dynamics of the loss landscape of neural networks trained on imbalanced datasets. We use the Cross Entropy (CE) loss  to denote the average cross entropy loss for each class. For fine-grained analysis, we focus on average loss on each class . We visualize the loss landscape of the head and tail classes through the computation of Hessian Eigenvalue Density \cite{pmlr-v97-ghorbani19b}. The Hessian of the train loss for each class  contains important properties about the curvature of the classwise loss landscape. The Hessian Eigenvalue Density provides all suitable information regarding the eigenvalues of . In this work, we focus on (max eigenvalue) and (min eigenvalue), which depict the extent of positive and negative curvature present. We use the Lanczos algorithm as introduced in \citet{pmlr-v97-ghorbani19b} to compute the Hessian Eigenvalue Density (spectral density) tractably. We further calculate the validation accuracy of a particular class  and its eigen spectral density for analysis. We provide more details for these experiments in the App. \ref{app:eigen_spectral_density_plots}.

\textbf{Does the proposed class-wise analysis of loss landscape offer any additional insights?} In prior works~\cite{pmlr-v97-ghorbani19b,gilmer2021loss, li2020hessian}, the Hessian of the average loss is used to characterize the nature of the converged point in the loss landscape. However, we find that when particularly trained on imbalanced datasets like CIFAR-10 LT, the eigen spectral density of the Hessian of average loss (Fig. \ref{fig:head-tail-esd}\textcolor{red}{A}) does not differ from that of head class loss (Fig. \ref{fig:head-tail-esd}\textcolor{red}{B}), indicating convergence to a local minima. However, explicitly analyzing the Hessian for the tail class loss (Fig. \ref{fig:head-tail-esd}\textcolor{red}{C}) gives the correct indication of the presence of negative eigenvalues (i.e., curvature), which is in contrast to average loss. Hence, our proposed class-wise analysis of Hessian is essential for characterizing the nature of the converged solution when the training data is imbalanced.   


\textbf{What happens when you train a neural network with CE-DRW method on CIFAR-10 LT?}
Fig. \ref{fig:head-tail-esd} shows the spectral density on samples from the head class (Class 0 with 5000 samples) and tail class (Class 8 with 83 samples) at the checkpoint with the best validation accuracy. The spectral density of the head class contains few negative eigenvalues. Most of the eigenvalues are centered around zero, as also observed when training on a balanced dataset \cite{pmlr-v97-ghorbani19b}. On the other hand, for the tail class, there exists a large number of both negative and positive eigenvalues, indicating convergence to a saddle point. We find that at this point, the  is low along with the norm of gradient, which indicates a stationary saddle point. We also observe that the spectral density of the tail class contains many outlier eigenvalues, and  is much larger compared to the head class indicating sharp curvature. These evidences show that \emph{the tail class solution converges to a saddle point instead of a local minimum}. \citet{merkulov2019empirical} indicated the existence of stationary points with low error but poor generalization in the loss landscape of neural networks. Also, the existence of saddle points being associated with poor generalization has been observed for small networks~\cite{dauphin2014identifying}. However, in this work, we show that convergence to saddle points can specifically occur in the loss landscape of tail classes even for the popular ResNet \cite{He_2016_CVPR} family of networks, which is an important and novel observation to the best of our knowledge. 


\textbf{Dynamics of training on Long-Tailed Datasets}: We analyze the  for the head, mid and tail classes at various epochs (10, 50, 160, 170, 190, 200) across training to understand the dynamics of optimization with CE+DRW on long-tailed data (Fig. \ref{fig:dynamics}\textcolor{red}{A}).  is a measure of non-convexity of the loss landscape \cite{li2018visualizing}, where a high value of  conveys non-convexity indicating convergence to points with significant negative curvature. The network converges to non-convex regions with negative curvature for tail classes, showing convergence to the saddle point. Also, we find that for the certain tail (Class 7, 8) and mid classes (Class 5), the network starts converging towards regions with negative curvature after applying loss re-weighting (DRW at 160th epoch). This indicates that DRW leads to convergence to a saddle point rather than preventing it. 









\begin{figure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{Figures/convexity.pdf}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{Figures/convexity_sam_2.pdf}  
  \label{fig:app_dynamics}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{Figures/correlation_rho_lmin_2.pdf}  
  \label{fig:rhovslam}
\end{subfigure}
\vspace{-1em}
\caption{A) In CE+DRW, the tail class loss landscapes show significant non-convexity as indicated by the large value of  whereas head classes (0,2) converge to convex landscapes.
B) When CE+DRW is trained with SAM, it avoids convergence to non-convex regions throughout the training, as indicated by the low value of .
C) With high , the  increases, and the model converges to a point with low negative curvature (approx. minima).}
\label{fig:dynamics}
\end{figure}
\section{Escaping Saddle Points for Improved Generalization}
\label{sec:sam_analysis}
In this section, we analyze the Sharpness-Aware Minimization technique for escaping from saddle points in tail class loss landscape. In existing works~\cite{andriushchenko2022understanding, liu2022towards, zhuang2022surrogate}, the effectiveness of SAM in escaping saddle points has not been explored to the best of our knowledge.

\textbf{Sharpness-Aware Minimization (SAM)}: 
Sharpness-Aware Minimization is a recent technique which aims to flatten the loss landscape by first finding a sharp maximal point  in the neighborhood of current weight . It then minimizes the loss at the sharp point ().    

Here,  is any objective (eg. CE or LDAM loss function) and  is the hyperparameter that controls the extent of neighborhood. A high value of  leads to convergence to much flat loss landscape. The inner optimization in above objective is first approximated using a first order solution:\vspace{-1em}


After finding , the network weights are updated using the gradient . In recent work~\cite{andriushchenko2022understanding}, it has been shown that the normalization of the norm of the gradient for  calculation above leads to oscillation which implies non-convergence theoretically. Also, it has been empirically shown that the unnormalized version of the gradient with adjusted  performs better than the normalized version. Hence, we use the approximation i.e.  for our theoretical results. As we will be using the stochastic version of the gradient, we use  as the stochasticity parameter and denote the gradient as . With this, we now define the gradient with respect to  that is associated with SAM:

As we are using the same batch for obtaining the gradient to calculate the  and loss, we can use the same  as the argument. We now analyze the component of the SAM gradient in the direction of negative curvature, which is required for escaping saddle points~\cite{daneshmand2018escaping}. 

\subsection{Analysis of SAM for Escaping Saddle Points}
\label{sam_saddle}
\definecolor{darkgreen}{RGB}{30,160,30}

Our analysis is based on the Correlated Negative Curvature (CNC) assumption \citep{daneshmand2018escaping} that states that stochastic gradients have components along the direction of negative curvature, which helps them escape from saddle points. This assumption has been shown to be theoretically valid for the problem of learning half-spaces and also has been empirically verified for a large number of neural networks of different sizes \cite{daneshmand2018escaping}. We now formally state the assumption below:
\begin{assumption}[Correlated Negative Curvature \cite{daneshmand2018escaping}]
\label{ass:cnc}
Let  be the minimum eigenvector corresponding to the minimum eigenvalue of the Hessian matrix . The stochastic gradient  satisfies the CNC assumption if the second moment of the projection along the direction  is uniformly bounded away from zero, i.e.


\end{assumption}

\vspace{-0.5em}
It has also been emphasized that the value of  is shown to correlate with the magnitude of . This shows that with a high negative eigenvalue, there is a large component of gradient along the negative curvature along . This allows the SGD algorithms to escape the saddle points. However, we find that in the case of class imbalanced learning (Fig. \ref{fig:overview}) even stochastic gradients may have an insufficient component in the direction of negative curvature to escape the saddle points. We now show that SAM technique, which aims to reach a flat minima, further amplifies the gradient component along negative curvature and can be effectively used to escape the saddle point. We now formally state our theorem based on the CNC assumption below:
\begin{theorem}
\label{th:sam_rho}
Let  be the minimum eigenvector corresponding to the minimum eigenvalue  of the Hessian matrix . The  satisfies that it's second moment of projection in  is atleast  times the original (component of ): 

\end{theorem}




\begin{remark}
The above theorem adds the factor  to increase the component in direction of negative curvature () when . Due to this increase, the model will be able to escape from directions with high negative curvature, leading to an increased . Also, as the factor  is inversely proportional to , the high value of  aids in effectively increasing the minimum negative eigenvalue. To empirically verify this, we evaluate the Hessian spectrum for the CIFAR-10 LT dataset using CE-DRW method for different values of  (Fig. \ref{fig:dynamics}\textcolor{red}{C}). We find that, as expected from the theorem, in practice, the high values of  lead to less negative values of . This indicates escaping the saddle points effectively, hence avoiding convergence to regions having negative curvature in loss landscape. The proof of the above theorem and additional details is provided in Appendix \ref{app:proof_theorem}.

\vspace{1mm} \noindent    We also want to convey that theoretically, techniques like Perturbed Gradient Descent (PGD), and LPF-SGD (Low-Pass Filter SGD), which add Gaussian noise into gradient to escape saddle points can also be used for mitigating negative curvature. Also it has been found that SGD~\cite{daneshmand2018escaping} can also escape the saddle points and converges to solutions with a flat loss landscape. Also, theoretically according to Theorem 2 in \citet{daneshmand2018escaping} the SGD algorithm convergence to a second-order stationary point depends on the  as  under some assumptions on . 
As we find that as SAM with high  enhances the component of SGD in direction of negative curvature () by , it is reasonable to expect that SAM is able to escape saddle points effectively and converge to solutions with significant less negative curvature quickly implying better generalization. We provide empirical evidence for this in Fig. \ref{fig:dynamics}\textcolor{red}{B} and Sec. \ref{subsec:ablation}.
\end{remark}
 \textbf{What happens when you train a neural network with SAM + DRW?}
With SAM (high ), the large negative eigenvalues present in the loss landscape of the tail class get suppressed (Fig. \ref{fig:head-tail-esd}\textcolor{red}{F}). In the spectral density for the tail class, it can be seen that  is much closer to zero for SAM compared to its counterpart with SGD. This aligns with the hypothesis that SAM escapes regions of negative curvature, leading to improved accuracy on the tail classes. However, the spectral density of the head class does not change significantly compared to that of Empirical Risk Minimization (ERM), although the  is much lower for SAM, indicating a flatter minima for the head class.  

We also analyze the  across multiple steps of training with SAM (Fig. \ref{fig:dynamics}\textcolor{red}{B}), where
 is a measure of non-convexity of the loss surface. We observe that SAM does not allow the tail classes to reach a region of high non-convexity. The values of   is much lower for SAM compared to SGD (Fig. \ref{fig:dynamics}\textcolor{red}{A}) throughout training, indicating minimal negative eigenvalues (\ie more convexity) in the loss landscape, especially for the tail and medium classes. This clearly shows that SAM avoids regions of substantial negative curvature in the search of flat minima. Further, we note that once the re-weighting begins, SAM is able to avoid convergence to a saddle point (non-convexity decreases), which is contrary to what we observe with CE+DRW (with SGD). Theorem \ref{th:sam_rho} states that SAM consists of larger component in the direction of negative curvature which allows to reach a solution with minimal negative curvature. Empirically, Fig. \ref{fig:dynamics}\textcolor{red}{B} also supports the Theorem \ref{th:sam_rho} as we observe that SAM reaches a minima (high convexity) for all the classes.  






\begin{table}
  \caption{Results on CIFAR-10 LT and CIFAR-100 LT with =100. SAM with re-weighting is able to avoid the regions of negative curvature, leading to major gain in performance on the mid and tail classes with CE, LDAM and VS.}
  \label{tab:cifar_lt}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|llll | llll}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{4}{c}{CIFAR-10 LT}& \multicolumn{4}{c}{CIFAR-100 LT}\\
    
    \cmidrule(r){1-9}
     &Acc & Head & Mid & Tail & Acc & Head  & Mid & Tail  \\
    \midrule
    CE & 71.7   & 90.8  & 71.9  & 52.3 & 38.5 & 64.5 & 36.8 & 8.2\\
    \rowcolor{mygray} CE + SAM & 73.1  & 93.3 & 74.1 & 51.7 & 39.6 & 66.5 & 38.1 & 8.0  \Bstrut{}\\
\hline
    CE + DRW \cite{cao2019learning} & 75.5 & 91.6  & 74.1  & 61.4 & 41.0 & 61.3 & 41.7 & 14.7 \Tstrut{}\\
    \rowcolor{mygray}
    \rowcolor{mygray} CE + DRW + SAM & 80.6 & 91.4 & 78.0 & 73.1  & 44.6 & 61.2 & 47.5 & 20.7  \Bstrut{}\\
    \hline
    LDAM + DRW \cite{cao2019learning} & 77.5 & 91.1 & 75.7 & 66.4 & 42.7 & 61.8 & 42.2 & 19.4 \Tstrut{}\\
    \rowcolor{mygray} LDAM + DRW + SAM & 81.9 & 91.0 & 79.2 & 76.4  & 45.4 & 64.4 & 46.2 & 20.8   \Bstrut{}\\
    \hline
    VS \cite{kini2021label} & 78.6 & 90.6 & 75.8  & 70.3 & 41.7 & 54.4 & 41.1 & 26.8  \Tstrut{}\\
    \rowcolor{mygray} VS + SAM & 82.4 & 90.7 & 79.6 & 78.0 & 46.6 & 56.4  & 48.8  & 31.7 \Bstrut{}\\

    
    \bottomrule
  \end{tabular}

   \end{adjustbox}
\end{table}

    

    



\section{Experiments}
\label{sec:experiments}
\subsection{Class-Imbalanced Learning}
 \textbf{Datasets}: We report our results on four long-tailed datasets: CIFAR-10 LT \cite{cao2019learning}, CIFAR-100 LT \cite{cao2019learning}, ImageNet-LT \cite{liu2019large}, and iNaturalist 2018 \cite{van2018inaturalist}. 
\textbf{a) CIFAR-10 LT and CIFAR-100 LT}: The original CIFAR-10 and CIFAR-100 datasets consist of 50,000 training images and 10,000 validation images, spread across 10 and 100 classes, respectively. We use two imbalance versions, i.e., long-tail imbalance and step imbalance, as followed in \citet{cao2019learning}. The imbalance factor, , denotes the ratio between the number of samples in the most frequent () and least frequent class (). For both the imbalanced versions, we analyze the results with  = 100.  \textbf{b) ImageNet-LT and iNaturalist 2018}: We use the ImageNet-LT version as proposed by \cite{liu2019large}, which is an class-imbalanced version of the large-scale ImageNet dataset \cite{russakovsky2015imagenet}. It consists of 115.8K images from 1000 classes, with 1280 images in the most frequent class and 5 images in the least. iNaturalist 2018 \cite{van2018inaturalist} is a real-world long-tailed dataset that contains 437.5K images from 8,142 categories. In the case of long-tail imbalance, we segregate the classes of all the datasets into \textit{Head} (Many), \textit{Mid} (Medium), and \textit{Tail} (Few) subcategories, as defined in \cite{zhong2021improving}. For step imbalance experiments on CIFAR datasets, we split the classes into \textit{Head} (Frequent) and \textit{Tail} (Minority), as done in \cite{cao2019learning}. \\
\\
\textbf{Experimental Details}:
We follow the hyperparameters and setup as in \citet{cao2019learning} for CIFAR-10 LT and CIFAR-100 LT datasets. We train a ResNet-32 architecture as the backbone and SGD with a momentum of 0.9 as the base optimizer for 200 epochs. A multi-step learning rate schedule is used, which drops the learning rate by 0.01 and 0.0001 at the 160th and 180th epoch, respectively. For training with SAM, we set a constant  value of either 0.5 or 0.8 for most methods. For ImageNet-LT and iNaturalist 2018 datasets, we use the ResNet-50 backbone similar to \cite{zhong2021improving}. An initial learning rate of 0.1 and 0.2 is set for iNaturalist 2018 and ImageNet-LT, respectively, followed by a cosine learning rate schedule. We initialize the  value with 0.05 and utilize a step schedule to increase the  value during the course of training for SAM experiments. We run every experiment on long-tailed CIFAR datasets with three seeds and report the mean and standard deviation. Additional implementation details are provided in the App. \ref{app:experimental_details}. Algorithm for DRW+SAM is defined in App. \ref{app:algorithm}.
\\
\\
\textbf{Baselines}: 
\textbf{a) Cross-Entropy (CE)}: CE minimizes the average loss across all samples, and thus, the performance of tail classes is much lower than that of head classes. 
\textbf{b) CE + Deferred Re-Weighting (DRW) \cite{cao2019learning}}: The re-weighting of CE loss inversely by class frequency is done in the later stage of training.
\textbf{c) LDAM + DRW \cite{cao2019learning}}: Label-Distribution-Aware Margin (LDAM) proposes a margin-based loss that encourages larger margins for less-frequent classes.
\textbf{d) Vector Scaling (VS) Loss~\cite{kini2021label}}: VS loss incorporates both additive and multiplicative logit adjustments to modify inter-class margins. 

\begin{table}[t]
	\begin{minipage}{0.6\linewidth}
  \caption{Results on CIFAR-10 and CIFAR-100 with Step Imbalance ( = 100). SAM generalizes well across datasets with varied type of imbalance, resulting in substantial gain in tail accuracy in all settings.}
  \label{tab:cifar_lt_step}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|lll | lll}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{3}{c}{CIFAR-10}& \multicolumn{3}{c}{CIFAR-100}\\
    
    \cmidrule(r){1-7}
     &Acc & Head  & Tail & Acc & Head   & Tail  \\
    \midrule
    CE & 65.1 & 88.6 & 41.7 & 38.6 & 76.3 & 00.9 \\
    \rowcolor{mygray}CE + SAM & 66.1 & 92.9 & 39.4 & 39.3 & 78.6 & 00.0 \Bstrut{} \\
\hline
    CE + DRW \cite{cao2019learning} & 72.2 & 93.1 & 51.2 & 45.8 & 73.9 & 17.8  \Tstrut{}\\
    \rowcolor{mygray}CE + DRW + SAM & 79.3 & 92.7 & 65.8 & 48.3 & 73.1  & 23.4   \Bstrut{}\\
\hline
    LDAM + DRW \cite{cao2019learning} & 77.6 & 89.2 & 66.0 & 45.3 & 70.3 & 20.4 \Tstrut{}\\
    \rowcolor{mygray}LDAM + DRW + SAM & 81.0 & 90.5 & 71.5 & 49.2 & 74.0 & 24.4 \Bstrut{}\\
    \hline
    VS \cite{kini2021label} & 77.0 & 91.7 & 62.3 & 46.5 & 69.0 & 24.1 \Tstrut{}\\
    \rowcolor{mygray}VS + SAM & 82.0 & 91.7 & 72.3 & 48.3 & 70.4 & 26.2 \Bstrut{}\\

    
    \bottomrule
  \end{tabular}
   \end{adjustbox}
	\end{minipage}\hfill
	\begin{minipage}{0.35\linewidth}
      \centering
      \includegraphics[width=1.0\textwidth]{Figures/rho_acc.pdf}
      \captionof{figure}{Impact of  (regularization factor) on Overall Accuracy and Tail Accuracy (CIFAR-10 LT).}
      \label{fig:rhovsacc}

	\end{minipage}
\end{table}


\textbf{Results}:
Table \ref{tab:cifar_lt} summarizes our results on CIFAR-10 LT and CIFAR-100 LT with  of 100. It can be observed that SAM with re-weighting significantly improves the accuracy on mid and tail classes while preserving the accuracy on head classes. SAM improves upon the overall performance of CE+DRW by 5.1\% on CIFAR-10 LT and 3.6\% on CIFAR-100 LT datasets, with the tail class accuracy increasing by 11.7\% and 7.7\% respectively. These results empirically show that escaping saddle points with SAM leads to a notable increase in overall accuracy primarily due to the major gain in the accuracy on the tail classes. The addition of SAM to recently proposed long-tail learning methods like LDAM and VS loss leads to a significant increase in performance, which indicates that the role of SAM is orthogonal to the margin-based methods. On the other hand, SAM without re-weighting (CE+SAM) improves accuracy on the head and mid classes rather than the tail class. This can be attributed to the fact that standard ERM minimizes the average loss across all the samples without re-weighting such that the weightage of tail class samples in the overall loss is minimal. This shows that naive application of SAM is ineffective in improving tail class performance, in comparison to proposed combination of re-weighting methods with SAM. We also show improved results with various imbalance factors () in App. \ref{app:different_if}.



We also show results with step imbalance ( = 100) on CIFAR-10 and CIFAR-100 datasets (Table \ref{tab:cifar_lt_step}). With step imbalance on CIFAR-10, the first five classes have 5000 samples each, and the remaining classes have 50 samples each. The addition of SAM improves the overall performance of CE+DRW on CIFAR-10 by 7.1\%, with the tail class accuracy increasing by 14.6\%. We observe that on most tail classes, the density of negative eigenvalues in the spectral density is much lower with SAM. This indicates that despite multiple classes with few samples, SAM with DRW can avoid the saddle points. SAM systematically improves performance with LDAM and VS loss leading to state-of-the-art performance on both CIFAR-10 and CIFAR-100 in the step imbalance setting. 



\textbf{Do these observations scale to large-scale datasets?}
We report the results on ImageNet-LT dataset in Table \ref{tab:imagenet_lt}. We also compare with recent long-tail learning methods:  cRT \cite{Kang2020Decoupling}, MisLAS \cite{zhong2021improving}, DisAlign \cite{zhang2021distribution} and DRO-LT \cite{Samuel_2021_ICCV}. The observations on CIFAR-10 LT and CIFAR-100 LT hold good even on ImageNet-LT. For example, the accuracy on tail classes increases by 6.5\% with the introduction of SAM on CE + DRW, which is similar to the gain observed in CIFAR-100 LT with CE + DRW. We observe that LDAM+DRW+SAM surpasses the performance of two-stage training methods including MisLAS, cRT, LWS, and DisAlign. Compared to these two-stage methods, our method is a single stage method and outperforms these two-stage methods. These observations point out that the problem of saddle points also exists in large datasets and convey that SAM is easily generalizable to large-scale imbalanced datasets without making any significant changes. On iNaturalist 2018 \cite{van2018inaturalist} too, the accuracy on tail classes gets boosted by more than 3\% with SAM (Table \ref{tab:imagenet_lt}). 

\begin{table}
  \caption{Results on iNaturalist 2018 and ImageNet-LT datasets with LDAM+DRW and comparison with other methods. The numbers for methods marked with  are taken from \cite{zhong2021improving}.}
  \label{tab:imagenet_lt}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|c|llll|llll}
    \toprule

    \multicolumn{2}{c}{Method}  & \multicolumn{4}{c}{iNaturalist 2018} &
    \multicolumn{4}{c}{ImageNet-LT}\\
    \hline
     &Two stage & Acc & Head & Mid & Tail & Acc & Head & Mid & Tail  \Tstrut{}\\
    \midrule
    CE &\Cross& 60.3 & 72.8 & 62.7 & 54.8 & 42.7 & 62.5 & 36.6 & 12.5  \\
    \hline
    cRT \cite{Kang2020Decoupling} &\checkmark &68.2 & \underline{73.2} & 68.8 & 66.1 &50.3 & \underline{62.5} & 47.4 & 29.5 \Tstrut{}\\
    LWS \cite{Kang2020Decoupling} &\checkmark &69.5 & 71.0 & 69.8 & 68.8 & 51.2 & 61.8 & 48.6 & 33.5\\
    MiSLAS \cite{zhong2021improving} &\checkmark &\textbf{71.6} & \underline{73.2} & \textbf{72.4} & \underline{70.4} & 52.7 &61.7 & 51.3 & \textbf{35.8} \\
    DisAlign \cite{zhang2021distribution} &\checkmark &69.5 & 61.6 & \underline{70.8} & 69.9 & 52.9 & 61.3 & \textbf{52.2} & 31.4\\
    DRO-LT \cite{Samuel_2021_ICCV} &\Cross &69.7 & \textbf{73.9} & 70.6 & 68.9 &\textbf{53.5} & \textbf{64.0} & 49.8 & 33.1\\

\hline
    CE + DRW & \Cross & 63.0 & 59.8 & 64.4 & 62.3 & 44.9 & 57.9 & 42.2 &21.6 \Tstrut{} \\
    \rowcolor{mygray} CE + DRW + SAM & \Cross& 65.3 & 60.5 & 66.2 & 65.5 & 47.1 & 56.6 & 45.8 & 28.1  \\
\hline
    LDAM + DRW & \Cross&67.5 & 63.0 & 68.3 & 67.8 &49.9 & 61.1 & 48.2 & 28.3 \Tstrut{} \\
    \rowcolor{mygray} LDAM + DRW + SAM & \Cross&\underline{70.1} & 64.1 & 70.5 & \textbf{71.2} & \underline{53.1} & 62.0 & \underline{52.1} & \underline{34.8}\\
    \bottomrule
  \end{tabular}

  \end{adjustbox}

\end{table}



\textbf{Comparison with SOTA}: VS loss \cite{kini2021label} is a recently proposed margin-based method that achieves state-of-the-art performance on class-imbalanced datasets with single-stage training without strong augmentations \cite{zhong2021improving}, ensembles \cite{zhang2021test} or self-supervision \cite{yang2020rethinking}. SAM significantly improves upon the performance of VS on both CIFAR-10 LT and CIFAR-100 LT. For the practitioners, we suggest \emph{using high  SAM with re-weighting or margin based methods} for effective learning on long-tailed data. We also integrate SAM with more recent IB-Loss \cite{Park_2021_ICCV} and Parametric Contrastive Learning (PaCo) \cite{cui2021parametric} methods and report the results in App. \ref{app:additional_results}. We find that SAM is also effectively able to improve performance of these recent methods.  
    
    



    


    






\subsection{Ablation Studies}
\label{subsec:ablation}
\textbf{A note on  value}:
We observe that as we increase the smoothness parameter () in SAM, the accuracy on the tail classes increases significantly (Fig. \ref{fig:rhovsacc}). The accuracy on tail classes increases from 63\% for  = 0.05 to 73\% for  = 0.8 on CIFAR-10 LT with CE+DRW. This can be ascribed to the correlation between  and  as discussed in Sec. \ref{sam_saddle}. As the  increases, the negative curvature in the tail classes disappears because SAM aims to find a flat minima with a large neighborhood with a low loss value. A very large  (0.8) leads to a drop in the head accuracy because it restricts the solution space of the head class, resulting in a drop in the overall accuracy. This also emphasizes that a high  is necessary for escaping saddle points and achieving the best results.



\textbf{Other methods to escape saddle points}: In Table \ref{tab:lpf_sgd}, we show that other methods developed to escape saddle points, such as PGD, can be used for improving generalization on tail classes. LPF-SGD, an algorithm promoting convergence to flat landscape, inherently adds Gaussian noise to the network parameters and could be considered similar to PGD. We can see that the addition of PGD and LPF-SGD to CE+DRW leads to a substantial gain in the performance of tail classes on CIFAR-10 LT and CIFAR-100 LT. It can also be observed that CE+DRW+SAM outperforms both PGD and LPF-SGD by 2\% on average. This further highlights that various methods in literature developed to escape saddle points efficiently can be directly used to improve the performance of minority classes when training on class-imbalanced datasets.
\begin{table}
  \caption{Results on CIFAR-10 LT and CIFAR-100 LT with various methods that escape saddle points.}
  \label{tab:lpf_sgd}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|llll | llll}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{4}{c}{CIFAR-10 LT}& \multicolumn{4}{c}{CIFAR-100 LT}\\
    
    \cmidrule(r){1-9}
     &Acc & Head & Mid & Tail & Acc & Head  & Mid & Tail  \\
    \midrule

    CE + DRW & 75.5&91.6&74.1&61.4&41.0&61.3&41.7&14.7 \Tstrut{}\\
    CE + DRW + PGD \cite{jin2017escape} & 77.2&92.0&75.2&65.0&42.2&63.0&41.6&17.0\\
    CE + DRW + LPF-SGD \cite{bisla2022low} & 78.5&90.8&77.7&67.2&42.9&64.0&43.7&15.8\\
    CE + DRW + SAM & 80.6  & 91.4 & 78.0 & 73.1  & 44.6& 61.2 & 47.5 & 20.7
    \Bstrut{}\\

    
    \bottomrule
  \end{tabular}

   \end{adjustbox}
\end{table}

    


    




    






\section{Conclusion}
In this work, we show that training on imbalanced datasets can lead to convergence to points with sufficiently large negative curvature in the loss landscape for minority classes. We find that this is quite common when neural networks are trained with loss functions that are re-weighted or modified to enhance the focus on minority classes. Due to the occurrence of saddle points, we observe that the network suffers from poor generalization on minority classes. We propose to use Sharpness-Aware Minimization (SAM) with a high regularization factor  as an effective method to escape regions of negative curvature and enhance the generalization performance. We theoretically and empirically demonstrate that SAM with high  is able to escape saddle points faster than SGD and converge to better solutions, which is a novel observation to the best of our knowledge. We show that combining SAM with state-of-the-art techniques for learning with imbalanced data leads to significant gains in performance on minority classes. We hope that our work leads to further research in studying the effect of negative curvature in generalization as we show they are a practical issue for class-imbalanced learning using deep neural networks.


\textbf{Acknowledgements}: This work was supported in part by  SERB-STAR Project (Project:STR/2020/000128), Govt. of India. Harsh Rangwani is supported by Prime Minister's Research Fellowship (PMRF). We are thankful for their support.


\clearpage
\appendix

\textbf{\Large Appendix}

\section{Limitations of Our Work} 
We would like to highlight that our theoretical results are based on \citet{daneshmand2018escaping} which verified CNC condition for small scale neural networks, verifying the CNC condition for large networks and exactly characterizing the saddle point solutions obtained by SAM for minority classes, are good directions for future work. 

Also empirically, we propose to use Sharpness-Aware Minimization with high  for tail classes to escape from saddle points. Although the general guideline is to use a higher  value like 0.5 or 0.8 to achieve the best result, we do find that  as a hyperparameter still requires tuning to obtain the best results. We believe making SAM hyper-parameter free is an interesting direction to pursue in the future.

\section{Proof of Theorem}
\label{app:proof_theorem}
In this section, we re-state Theorem \ref{th:sam_rho} and provide it's proof. The theorem analyzes the variance of stochastic gradient for SAM along the direction of negative curvature and shows that SAM amplifies the variance by a factor, which signals that it has a stronger component in direction of negative curvature under certain conditions. Hence, SAM can be used for effectively escaping saddle points in the loss landscape. This is based on Correlated Negative Curvature (CNC) Assumption for stochastic gradients (Assumption \ref{ass:cnc}). The 
 whereas the Hessian denoted by  where  is the number of parameters in the model.
\begin{theorem}
Let  be the minimum eigenvector corresponding to the minimum eigenvalue  of the Hessian matrix . The  satisfies that it's second moment of projection in  is atleast  times the original (component of ): 



\end{theorem}
\begin{proof}

Using the first-order approximation of a vector valued function through Taylor series:

here  is the jacobian operator. After considering  to be small we have the following approximation for the SAM gradient:

Here, we have used the following property that   is the Hessian matrix  (also written as ). Also, as we now want to work with stochastic gradients, we replace gradient  with it's stochastic version  and introduce an expectation expression.
Now, we analyze the second-moment of the SAM gradient along the direction of most negative curvature :

Here, we use the matrix notation for dot product . Using the property of the eigen vector: , we substitute the value below:

The last step follows from the CNC Assumption \ref{ass:cnc}. This completes the proof.
\end{proof}
\section{Experimental Details}
\label{app:experimental_details}

\textbf{Imbalanced CIFAR-10 and CIFAR-100}:
For the long-tailed imbalance (CIFAR-10 LT and CIFAR-100 LT), the sample size across classes decays exponentially with  = 100. CIFAR-10 LT holds 5000 samples in the most frequent class and 50 in the least, whereas CIFAR-100 LT decays from 500 samples in the most frequent class to 5 in the least. The classes are divided into three subcategories: \textit{Head }(Many),\textit{ Mid} (Medium), and \textit{Tail} (Few). For CIFAR-10 LT, the first 3 classes (> 1500 images each) fall into the head classes, following 4 classes (> 250 images each) into the mid classes, and the final 3 classes (< 250 images each) into the tail classes. Whereas for CIFAR-100 LT, head classes consist of the initial 36 classes, mid classes contain the following 35 classes, and the tail classes consist of the remaining 29 classes.

In the step imbalance setting, both CIFAR-10 and CIFAR-100 are split into two classes, i.e., \textit{Head} (Frequent) and \textit{Tail} (Minority), with  = 100. The first 5 (Head) classes of CIFAR-10 contain 5000 samples each, along with 50 samples each in the remaining 5 (Tail) classes. On the other hand, the top first 50 (Head) classes of CIFAR-100 contain 500 samples each, and the remaining 50 (Tail) classes consist of 5 samples each. 

All the experiments on imbalanced CIFAR-10 and CIFAR-100 are run with ResNet-32 backbone and SGD with momentum 0.9 as the base optimizer. All the methods train on imbalanced CIFAR-10 and CIFAR-100 with a batch size of 128 for 200 epochs, except for VS Loss, which runs for 300 epochs. We follow the learning rate schedule mentioned in \citet{cao2019learning}. In the initial 5 epochs, we linearly increase the learning rate to reach 0.1. Following that, a multi-step learning rate schedule decays the learning rate by scaling it with 0.001 and 0.0001 at 160th and 180th epoch, respectively. For LDAM runs on imbalanced CIFAR, the value of  is tuned so that  is normalised to set maximum margin of 0.5 (refer to Equation. \ref{eq:ldam_loss} in main text). In the case of VS Loss, we use  as 0.05 and  as 0.75 for imbalanced CIFAR-10 and CIFAR-100 datasets (refer to Equation. \ref{eq:vs_loss} in main text).

\vspace{1mm}\noindent \textbf{ImageNet-LT and iNaturalist 2018}:
The classes in ImageNet-LT and iNaturalist 2018 datasets are also divided into three subcategories, i.e., \textit{Head }(Many),\textit{ Mid} (Medium), and \textit{Tail} (Few). For ImageNet-LT, the head classes consist of the first 390 classes, mid classes contain the subsequent 445 classes, and the tail classes hold the remaining 165 classes. Whereas for iNaturalist 2018, first 842 classes fall into the head classes, subsequent 3701 classes into the mid classes, and the remaining 3599 into the tail classes. 

For ImageNet-LT and iNaturalist 2018, all the models are trained for 90 epochs with a batch size of 256. We use ResNet-50 architecture as the backbone and SGD with momentum 0.9 as the base optimizer. A cosine learning rate schedule is deployed with an initial learning rate of 0.1 and 0.2 for iNaturalist 2018 and ImageNet-LT, respectively. For LDAM runs on ImageNet-LT and iNaturalist 2018, the value of  is tuned so that  is normalised to set maximum margin of 0.3 (refer to Equation. \ref{eq:ldam_loss} in main text).  
\begin{table}
  \caption{ value for used for reporting the results with SAM.}
  \label{tab:app_rho}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l?c|c ? c|c}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{2}{c}{CIFAR-10}& \multicolumn{2}{c}{CIFAR-100}\\
    
    \cmidrule(r){1-5}
     &LT ( = 100) & Step ( = 100) & LT ( = 100) & Step ( = 100)  \\
    \midrule
    CE + SAM & 0.1 & 0.1 & 0.2 & 0.5  \Tstrut{} \Bstrut{}\\
\hline


 CE + DRW + SAM & 0.5  & 0.2 & 0.8 & 0.2 \Tstrut{} \Bstrut{}\\
    \hline

LDAM + DRW + SAM & 0.8 & 0.1 & 0.8 & 0.5 \Tstrut{}  \Bstrut{}\\
    \hline

VS + SAM & 0.5 & 0.2 & 0.8 & 0.2 \Tstrut{} \Bstrut{}\\

    
    \bottomrule
  \end{tabular}

   \end{adjustbox}

\end{table}

\vspace{1mm}\noindent \textbf{Optimum \textbf{} value}: Table \ref{tab:app_rho} compiles the  value used by SAM across various methods on imbalanced CIFAR-10 and CIFAR-100 datasets. The  value in these runs is kept constant throughout the duration of training. 
We adopt a common step  schedule for the SAM runs on both ImageNet-LT and iNaturalist 2018. We initialise the  with 0.05 for the initial 5 epochs and change it to 0.1 till the 60th epoch. Following that, we increase the  value to 0.5 for the final 30 epochs. 

\vspace{1mm}\noindent \textbf{How to select \textbf{} ?}  is an hyperparameter in the SAM algorithm and it is important to choose the right value of  for best performance on long-tailed learning. We observe that default value of  (0.05) as suggested in \citet{foret2021sharpnessaware} does not lead to significant gain in accuracy (Refer Fig. \ref{fig:rhovsacc} in main paper), as it is not able to escape the region of negative curvature. On long-tail CIFAR-10 and CIFAR-100 setting with re-weighting (DRW), a large value of  (0.5 or 0.8) seems to work best instead, as in this work our objective to escape saddle points instead of improving generalization. This can be intuitively understood as large regularization () is required for highly imbalanced datasets to escape saddle points as suggested by Theorem \ref{th:sam_rho}. In Table \ref{tab:app_rho}, we have reported the  value used in every experiment. For the large scale datasets like ImageNet-LT and iNaturalist 18, we found that progressively increasing the  value gives the best results. This is based on the idea that, as the training progresses, more flatter regions can be recovered from the loss landscape \cite{bisla2022low}. In our experiments on ImageNet-LT, we use a large  of 0.5 in the last 30 epochs of training and we observe that the tail accuracy significantly increases at this stage of training. For using the proposed method on a new imbalanced dataset, we suggest starting with  = 0.05 and increasing  till the overall accuracy starts to decrease.

\vspace{1mm}\noindent \textbf{LPF-SGD and PGD}:
We use the official implementation of LPF-SGD \cite{bisla2022low} \footnote{https://github.com/devansh20la/LPF-SGD} to report the results on CIFAR-10 LT and CIFAR-100 LT. For LPF-SGD, we use Monte Carlo iterations () = 8 and a constant filter radius () of 0.001 (as defined in Algorithm 4.1 in \citet{bisla2022low}). We implement the stochastic PGD method \cite{jin2017escape, Jin2019StochasticGD} on our own since there is no official PyTorch implementation available. We sample the perturbation (noise) from a Gaussian distribution with zero mean and () standard deviation. We use a  of 0.0001 for CIFAR-10 and CIFAR-100 LT experiments.

\vspace{1mm}\noindent \textbf{Hessian Experiments}: For calculating the Eigen Spectral Density, we use the PyHessian library \cite{yao2020pyhessian}. PyHessian uses Lanczos algorithm for fast and efficient computation of the complete Hessian eigenvalue density. The Hessian is calculated on the average loss of the training samples as done in \cite{pmlr-v97-ghorbani19b, yao2020pyhessian}.  and  are extracted from the complete Hessian eigenvalue density. It has been shown that the estimated spectral density calculated with the Lanczos algorithm can be used as an approximate to the exact spectral density \cite{pmlr-v97-ghorbani19b}. Several works \cite{foret2021sharpnessaware, pmlr-v97-ghorbani19b, gilmer2021loss, yao2020pyhessian} have used the same method to calculate spectral density and analyze the loss landscape of neural networks. 

\begin{table}
  \caption{Results on ImageNet-LT (ResNet-50) with LDAM+DRW and comparison with other methods. The numbers for methods marked with  are taken from \cite{zhong2021improving}.}
  \label{tab:app_imagenetlt}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|c|llll}
    \toprule
     &Two stage &Acc & Head & Mid & Tail  \\
    \midrule
    CE &\Cross & 42.7 & 62.5 & 36.6 & 12.5 \\
    \hline
    cRT \cite{Kang2020Decoupling} &\checkmark &50.3 & \underline{62.5} & 47.4 & 29.5 \Tstrut{}\\
    LWS \cite{Kang2020Decoupling}  &\checkmark &51.2 & 61.8 & 48.6 & 33.5\\

    MisLAS \cite{zhong2021improving} &\checkmark &52.7 &61.7 & 51.3 & \textbf{35.8}\\
    DisAlign \cite{zhang2021distribution}& \checkmark&52.9 & 61.3 & \textbf{52.2} & 31.4\\
    DRO-LT* \cite{Samuel_2021_ICCV} & \Cross&\textbf{53.5} & \textbf{64.0} & 49.8 & 33.1 \\
\hline
    LDAM + DRW &\Cross &49.9 & 61.1 & 48.2 & 28.3 \Tstrut{} \\
    \rowcolor{mygray} LDAM + DRW + SAM &\Cross& \underline{53.1} & 62.0 & \underline{52.1} & \underline{34.8}  \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

\vspace{1mm}\noindent All of our implementations are based on PyTorch \citep{NEURIPS2019_9015}. For experiments pertaining to imbalanced CIFAR, we use NVIDIA GeForce RTX 2080 Ti, whereas for the large scale ImageNet-LT and iNaturalist 2018, we use NVIDIA A100 GPUs. We log all our experiments with Wandb \cite{wandb}.

\begin{figure}[t]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+None_New/_class_0_sgd_ce_None_2.pdf}  

  \label{fig:sub-first3}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+None_New/_class_7_sgd_ce_None_2.pdf}  

  \label{fig:sub-second4}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+None_New/_class_9_sgd_ce_None_2.pdf}  

  \label{fig:sub-second5}
\end{subfigure}

\caption{Eigen Spectral Density of Head (Class 0) and Tail (Class 7 and Class 9) with standard CE (without re-weighting). Since CE minimizes the average loss, it can be seen that the loss on the tail class samples (B and C) is quite high. On the head class (A), the loss is low and  is close to 0.}
\label{fig:app_ce_none}

\end{figure}




    








\section{Additional Eigen Spectral Density Plots}
\label{app:eigen_spectral_density_plots}
We find that the spectral density of a class is representative of the other classes in same category (Head, Mid or Tail), hence for brevity we only display the eigen spectrum of one class per category for analysis.

\begin{figure}[t]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+DRW/_class_2_sgd_ce_drw_2.pdf}  

  \label{fig:sub-first4}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+DRW/_class_5_sgd_ce_drw_2.pdf}  

  \label{fig:sub-second6}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+DRW/_class_7_sgd_ce_drw_2.pdf}  

  \label{fig:sub-second7}
\end{subfigure}
\newline

\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+DRW/_class_2_sam_ce_drw_2.pdf}  

  \label{fig:sub-third2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/CE+DRW/_class_5_sam_ce_drw_2.pdf}  

  \label{fig:sub-fourth1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{    Figures/CE+DRW/_class_7_sam_ce_drw_2.pdf}  

  \label{fig:sub-second9}
\end{subfigure}
\caption{Eigen Spectral Density of the Head (Class 2), Mid (Class 5) and Tail classes (Class 7) with CE+DRW and CE+DRW+SAM.}
\label{fig:app_head-tail-esd}
\vspace{-1.5em}
\end{figure}

\textbf{CE}: The spectral density on the standard CE loss (without re-weighting) can be seen in Fig. \ref{fig:app_ce_none}. We notice that the density and magnitude of negative eigenvalues is much larger for the tail class (Class 7 and Class 9 in Fig. \ref{fig:app_ce_none}\textcolor{red}{B} and \ref{fig:app_ce_none}\textcolor{red}{C}) compared to the head classes (Fig. \ref{fig:app_ce_none}\textcolor{red}{A}). On the other hand, the spectral density of the head class (Class 0) is very different from that of the tail class, with  of the head class very close to 0 indicating convergence to minima.  



It must be noted that without re-weighting, the loss on the tail class samples is high because CE minimizes the average loss. Hence, the solution may not converge for tail class loss. However, in CE+DRW after re-weighting, we observe that the loss on tail class samples is very low, which indicates convergence to a stationary point. Thus, in CE+DRW, we can evidently conclude that the presence of large negative curvature indicates convergence to a saddle point. In summary, we find that just using CE converges to a point with significant negative curvature in tail class loss landscape. Further, though DRW is able to decrease the loss on tail classes, it still does converge to a point with significant negative curvature. This indicates that it converges to a saddle point instead of a minima. Hence, both CE and CE+DRW do not converge to local minima in tail class loss landscape. 




\vspace{1mm}\noindent \textbf{CE+DRW}: We show additional class wise Eigen Spectral Density plots with CE+DRW and CE+DRW with SAM in Fig. \ref{fig:app_head-tail-esd}. We analyze the spectral density plots on Head (Class 2), Mid (Class 5) and Tail (Class 7). It can be seen that the magnitude of  and  is much lower with SAM in all the classes (Fig. \ref{fig:app_head-tail-esd} \textcolor{red}{D}, \textcolor{red}{E}, \textcolor{red}{F}). This indicates that SAM reaches a flatter local minima with no significant presence of negative eigenvalues, escaping saddle points. 

\vspace{1mm}\noindent \textbf{LDAM}: We also show Spectral density plots of Class 0 (Fig. \ref{fig:app_ldam} \textcolor{red}{A}, \textcolor{red}{C}) and Class 9 (Fig. \ref{fig:app_ldam} \textcolor{red}{B}, \textcolor{red}{D}) with LDAM+DRW method (SGD and SAM) in Fig. \ref{fig:app_ldam}. The existence of negative eigenvalues in the tail class spectral density (Fig. \ref{fig:app_ldam}\textcolor{red}{B}) indicates that even for LDAM loss (a regularized margin based loss), the solutions do converge to a saddle point. This also indicates that observations with CE+DRW hold good for long-tailed learning methods like LDAM which use margins instead of re-weighting directly. Hence, this gives evidence of the reason why SAM can be combined easily with  LDAM, VS Loss etc. to effectively improve performance. 



The spectral density of the tail class of LDAM with SAM (Fig. \ref{fig:app_ldam}\textcolor{red}{D}) contains fewer negative eigenvalues compared to SGD (Fig. \ref{fig:app_ldam}\textcolor{red}{B}). This indicates convergence to local minima and clearly explains why SAM improves the performance of LDAM by 12.7\%.
\\







\begin{figure}[t]
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/LDAM+DRW/class_0_sgd_ldam_drw.pdf}  

  \label{fig:sub-first1}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/LDAM+DRW/class_9_sgd_ldam_drw.pdf}  

  \label{fig:sub-second10}
\end{subfigure}

\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/LDAM+DRW/class_0_sam_ldam_drw_new.pdf}  

  \label{fig:sub-third3}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/LDAM+DRW/class_9_sam_ldam_drw.pdf}  

  \label{fig:sub-fourth5}
\end{subfigure}

\caption{Eigen Spectral Density of the Head (Class 0) and Tail (Class 9) class trained with LDAM. Even with LDAM, we observe existence of negative eigenvalues in the loss landscape for the tail class, which reduce in magnitude when LDAM is used with SAM.}
\label{fig:app_ldam}

\end{figure}




\section{Additional Results}
\label{app:additional_results}

For further establishing the generality of our method, we choose two recent orthogonal method Influence-Balanced Loss \cite{Park_2021_ICCV} (IB-Loss) and Parametric Contrastive Learning (PaCo) \cite{cui2021parametric} and apply proposed high  SAM over them. We use the open-source implementations of IB-Loss \footnote{https://github.com/pseulki/IB-Loss} and PaCo \footnote{https://github.com/dvlab-research/Parametric-Contrastive-Learning} to reproduce the results and add our proposed method (high  SAM) to that setup to obtain the results reported in the table below. We show results on CIFAR-100 LT with an imbalance factor () of 100 and 200. We observe that SAM with high  significantly improves overall performance along with the performance on tail classes with both IB-Loss and PaCo method (Table \ref{tab:ib_cf100}). Despite PaCo baseline achieving close to state-of-the-art performance, the addition of high  SAM is able to further improve the accuracy. This indicates the generality and applicability of proposed method across various long-tailed learning algorithms.

We show additional results on the large scale ImageNet-LT (Table \ref{tab:app_imagenetlt}) and iNaturalist 2018 (Table \ref{tab:imagenet_lt}) dataset with LDAM-DRW. We also compare with recent long-tail learning methods:  cRT \cite{Kang2020Decoupling}, MisLAS \cite{zhong2021improving}, DisAlign \cite{zhang2021distribution} and DRO-LT \cite{Samuel_2021_ICCV}. On ImageNet-LT, LDAM+DRW with SAM leads to a 3.2\% gain in overall accuracy with 6.5\% increase in tail class accuracy. It can be seen that LDAM+DRW+SAM outperforms most other methods, including MisLAS which uses mixup. Also, it is important to note that MisLAS is trained for 180 epochs unlike LDAM+DRW which is trained only for 90 epochs. We observe that LDAM+DRW+SAM surpasses the performance of two-stage training methods including MisLAS, cRT, LWS, and DisAlign. Compared to these two-stage methods, our method is a single stage method and outperforms these two-stage methods. We want to add that we were not able to reproduce the numbers reported in DRO-LT* \cite{Samuel_2021_ICCV} when we were trying to incorporate SAM with DRO-LT.

With LDAM+DRW, the addition of SAM results in an increase in Head, Mid and Tail categories on iNaturalist 2018 (Table \ref{tab:imagenet_lt}). Specifically, LDAM+DRW+SAM outperforms all other methods in the tail class accuracy. 

This further emphasizes that our analysis is applicable to large scale imbalanced datasets like ImageNet-LT and iNaturalist 2018. We also want to highlight that our analysis shows that high  SAM with re-weighting can be used as a \emph{strong baseline} in long tailed visual recognition problem. We also find that SAM is highly compatible with different loss-based methods (like LDAM, VS) for tackling imbalance and can be used to achieve significantly better performance.
\begin{table}
  \caption{Results on CIFAR-10 LT with different Imbalance Factor ().}
  \label{tab:cifar10_imbalance_lt}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|llll | llll}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{4}{c}{ = 10}& \multicolumn{4}{c}{ = 50}\\
    
    \cmidrule(r){1-9}
     &Acc & Head & Mid & Tail & Acc & Head  & Mid & Tail  \\
    \midrule
    CE + DRW \cite{cao2019learning} & 88.3 & 93.6  & 85.3 & 86.9 & 79.9 & 92.2 & 76.5 & 72.0 \Tstrut{}\\
    \rowcolor{mygray}
    \rowcolor{mygray} CE + DRW + SAM & 89.7 & 93.4 & 86.1  & 90.8 & 83.8 & 91.3 & 80.5 & 80.8  \Bstrut{}\\
    \hline
    LDAM + DRW \cite{cao2019learning} & 87.8 & 91.9  & 85.0 & 87.5 & 82.0 & 90.9 & 78.7 & 77.5 \Tstrut{}\\
    \rowcolor{mygray} LDAM + DRW + SAM & 89.4 & 93.4  & 86.2 & 89.8 & 84.8 & 92.8 & 82.1 & 80.4  \Bstrut{}\\
    \hline
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{4}{c}{ = 100}& \multicolumn{4}{c}{ = 200}\\
    
    \cmidrule(r){1-9}
     &Acc & Head & Mid & Tail & Acc & Head  & Mid & Tail  \\
    \midrule
    CE + DRW \cite{cao2019learning} & 75.5 & 91.6  & 74.1 & 61.4 & 69.9 & 91.1 & 70.0 & 48.4 \Tstrut{}\\
    \rowcolor{mygray}
    \rowcolor{mygray} CE + DRW + SAM & 80.6 & 91.4 & 78.0  & 73.1 & 76.6 & 91.5 & 74.9 & 64.0 \Bstrut{}\\
    \hline
    LDAM + DRW \cite{cao2019learning} & 77.5 & 91.1  & 75.7 & 66.4 & 72.5 & 90.2 & 72.3 & 54.9 \Tstrut{}\\
    \rowcolor{mygray} LDAM + DRW + SAM & 81.9 & 91.0  & 79.2 & 76.4 & 78.1 & 91.2 & 75.6 & 68.4  \Bstrut{}\\
    \bottomrule
  \end{tabular}

   \end{adjustbox}

\end{table}
\section{Additional Results with Varying Imbalance Factor}
\label{app:different_if}
We show the results with different imbalance factors ( = 10, 50, 100 and 200) on CIFAR-10 LT (Table \ref{tab:cifar10_imbalance_lt}) and CIFAR-100 LT (Table \ref{tab:cifar100_imbalance_lt}) datasets with two methods. It can be seen that the observations in Table \textcolor{red}{1} are applicable with different degrees of imbalance. SAM with re-weighting improves upon the performance of CE and LDAM losses in all the experiments with varied imbalance factor. We observe an average increase of 3.9\% and 3.2\% on CIFAR-10 LT and CIFAR-100 LT datasets, respectively. This gain in performance is primarily due to the improvement in the tail accuracy, which increases by 8.6\% on CIFAR-10 LT and 3.9\% on CIFAR-100 LT.

As the dataset becomes more imbalanced ( increases), the gain in accuracy with SAM on the tail classes improves significantly. For instance, on CIFAR-10 LT with  = 10 (Table \ref{tab:cifar10_imbalance_lt}), CE+DRW+SAM improves upon CE+DRW by 1.2\% with a 3.9\% increase in tail class accuracy. However, with a more imbalanced dataset (\ie CIFAR-10 LT  = 200), SAM leads to a 6.7\% boost in overall accuracy with a massive 15.6\% increase in the tail class performance. 


\begin{table}[]
  \caption{Results on CIFAR-100 LT with different Imbalance Factor ().}
  \label{tab:cifar100_imbalance_lt}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|llll | llll}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{4}{c}{ = 10}& \multicolumn{4}{c}{ = 50}\\
    
    \cmidrule(r){1-9}
     &Acc & Head & Mid & Tail & Acc & Head  & Mid & Tail  \\
    \midrule
    CE + DRW \cite{cao2019learning} & 58.1 & 65.6 & 58.5 & 48.2 & 46.5 & 63.3 & 47.5 & 24.4 \Tstrut{}\\
    \rowcolor{mygray}
    \rowcolor{mygray} CE + DRW + SAM & 60.7 & 66.0 & 60.5 & 54.4 & 50.0 & 61.9 & 50.9 & 33.7 \Bstrut{}\\
    \hline
    LDAM + DRW \cite{cao2019learning} & 57.8 & 67.5 & 58.9 & 44.5 & 47.1 & 62.9 & 48.2 & 26.1\Tstrut{}\\
    \rowcolor{mygray} LDAM + DRW + SAM & 60.1 & 70.2 & 61.3 & 46.1 & 49.4 & 66.1 & 50.2 & 27.8  \Bstrut{}\\
    \hline
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{4}{c}{ = 100}& \multicolumn{4}{c}{ = 200}\\
    
    \cmidrule(r){1-9}
     &Acc & Head & Mid & Tail & Acc & Head  & Mid & Tail  \\
    \midrule
    CE + DRW \cite{cao2019learning} & 41.0 & 61.3 & 41.7 & 14.7 & 36.9 & 59.7 & 36.1 & 9.6 \Tstrut{}\\
    \rowcolor{mygray}
    \rowcolor{mygray} CE + DRW + SAM & 44.6 & 61.2 & 47.5 & 20.7 & 41.7 & 63.4 & 43.0 & 13.1 \Bstrut{}\\
    \hline
    LDAM + DRW \cite{cao2019learning} & 42.7 & 61.8 & 42.2 & 19.4 & 38.3 & 58.8 & 36.3 & 15.1 \Tstrut{}\\
    \rowcolor{mygray} LDAM + DRW + SAM & 45.4 & 64.4 & 46.2 & 20.8 & 42.0 & 63.0 & 41.4 & 16.6 \Bstrut{}\\
    \bottomrule
  \end{tabular}

   \end{adjustbox}

\end{table}

\begin{table}[t]
  \caption{Results on CIFAR-100 LT with IB-Loss and PaCo.}
  \label{tab:ib_cf100}
  \centering
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{l|ll|ll}
    \toprule
    \multicolumn{1}{c}{}  & \multicolumn{2}{c}{ = 100}& \multicolumn{2}{c}{ = 200}\\
    
     & Acc & Tail & Acc & Tail  \\


    \hline

    IB \cite{cao2019learning} &   40.4  & 14.9 & 36.7 & 10.3\Tstrut{}\\

    \cellcolor{mygray}{IB + SAM} & \cellcolor{mygray}42.8 &  \cellcolor{mygray}25.0 &\cellcolor{mygray}37.7  &\cellcolor{mygray}17.8 \Bstrut{}\\
    \hline
    PaCo \cite{cui2021parametric} & 51.5 & 33.9 & 47.0 & 26.9\Tstrut{}\\
    \cellcolor{mygray}{PaCo + SAM} & \cellcolor{mygray}53.0 & \cellcolor{mygray}36.0 &\cellcolor{mygray}48.0  &\cellcolor{mygray}27.8 \Bstrut{}\\

    \bottomrule
  \end{tabular}

   \end{adjustbox}

\end{table}
\section{Algorithm}
\label{app:algorithm}
We describe our method in detail in Algorithm \ref{algo:DRW_SAM}.  On the large scale ImageNet-LT and iNaturalist-18 dataset, we use  > . For CIFAR-10 LT and CIFAR-100 LT, we find that  =  works well.
\begin{algorithm}[t]
\caption{DRW + SAM}
\label{algo:DRW_SAM}
\begin{algorithmic}[1]
\Require  Network  with parameters ; Training set ; Batch size ; Learning rate ; Neighborhood size , Neighborhood size for re-weighted loss ; Total Number of Iterations ; Deferred Reweighting Threshold ; Number of samples in class : ; Loss Function  (Cross-Entropy, LDAM).

	\For{ to  }
    \State Sample a mini-batch   with size . \If{ < }
      \State Compute Loss  
      \State Compute  \Comment{Compute Sharp-Maximal Point}
      \State Compute Loss at  
      \State Calculate gradient : 
    \Else \Comment{Deferred Re-Weighting (DRW)}
    \State Compute re-weighted Loss  
      \State Compute 
      \State Compute re-weighted Loss at  
      \State Calculate gradient : 
    \EndIf
    
       \State Update weights 
    \EndFor
\end{algorithmic}
\end{algorithm}




    








\section{Related Work: Long-tailed Learning}
\label{app:related_work}
In this section, we discuss some recent approaches in long-tailed learning. Equalization loss is proposed in \citet{tan2020equalization} based on the proposition that the gradients of negative samples overpower the gradient of positive samples for minority classes. Influence-Balanced Loss \cite{Park_2021_ICCV} is a sample-level re-weighting method that reweights each sample by the inverse of the norm of the gradient of each sample. The gradient of each sample estimates the influence of that sample in determining the decision boundary. Distill the Virtual Examples (DiVE) \cite{He_2021_ICCV} addresses the problem of class-imbalanced learning from the lens of knowledge distillation. It is shown that the teacher models' predictions (virtual examples) can be distilled into the student model by making use of cross-category interactions. This leads to an improvement in the accuracy of the minority class samples. 

\vspace{1mm}\noindent Self-Supervised Learning methods have been shown to learn generalizable representations \cite{chen2020simple} which are useful for a wide variety of downstream tasks. Self-Supervised pre-training (SSP) has been shown to improve the performance of class-imbalanced learning \cite{NEURIPS2020_e025b627}. Parametric Contrastive Learning (PaCo) \cite{cui2021parametric} introduces parametric class-wise learnable centers into the Supervised Contrastive Learning \cite{khosla2020supervised} framework to improve the performance on imbalanced datasets. PaCo achieves close to state-of-the-art performance on most of the long-tailed learning benchmarks. Self Supervised to Distillation (SSD) \cite{li2021self} is a multi-stage training framework for long-tailed recognition with a total of four stages of training. The first two stages involve self-supervised training followed by the generation of soft labels. The final two stages include joint training with distillation and classifier fine-tuning. Balanced Contrastive Learning (BCL) \cite{Zhu_2022_CVPR} adapts the Supervised Contrastive framework \cite{khosla2020supervised} by proposing a Balanced Contrastive loss which ensures that the feature space is balanced when training with an imbalanced dataset.
\section{Code and License Details}
Our codebase is derived from the official implementation of LDAM-DRW\cite{cao2019learning}\footnote{https://github.com/kaidic/LDAM-DRW}, VS-Loss \cite{kini2021label}\footnote{https://github.com/orparask/VS-Loss} and SAM\cite{foret2021sharpnessaware}\footnote{https://github.com/davda54/sam} which have been released under the \texttt{MIT} license. We have included the code and the pretrained weights of the CE+DRW model trained of CIFAR-10 LT in the supplementary material. The code to reproduce the experiments is available at \href{https://github.com/val-iisc/Saddle-LongTail}{https://github.com/val-iisc/Saddle-LongTail}.

\bibliography{neuripsbib}

\end{document}