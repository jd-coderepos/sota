

\noindent\textbf{Datasets}. We perform evaluations on two standard benchmarks widely-used in the community, namely Udacity~\cite{udacity} and Comma.ai~\cite{santana2016learning} for evaluation.
They are the largest steering angle prediction datasets by far. Note that the Berkeley Deep Drive (BDD) dataset~\cite{yu2018bdd100k} provides vehicle turning directions (i.e., go straight, stop, turn left / right) instead of steering wheel angles. Nonetheless, we conducted experiments on this dataset and provide the results. We show that heterogeneous feature mimicking still helps even in a much larger dataset (BDD dataset provides more than 7 M video frames).

The Udacity dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic.
The images of Comma.ai dataset are mainly captured from highway and urban roads. It contains 11 video clips within 7.5 hours driving. Busy traffic conditions make this dataset challenging.
Note that there is no official and publicly available partition setting for this dataset. For fair comparisons, we benchmark our method and variants using a common setting. Specifically, we use 5\% of each of the 11 clips for validation and testing, chosen randomly as a continuous chunk. The remaining frames are used for training. As pre-processing, we downsample the clips by a factor of two in time and remove video frames whose speed is less than 15 m/s to discard erroneous steering readings. As a result, we obtain a total number of 341,663 frames for training and 23,642 frames for testing. We release the data partitions on our project page\footnote{Project page: \emph{https://cardwing.github.io/projects/FM-Net}}.
Some typical frames of these two datasets are shown in Fig.~\ref{fig:qualitative_results}.



\vspace{0.1cm}
\noindent\textbf{Implementation details}. To facilitate the training of our network, the pixel values of input video frames are normalized to lie in [-1, 1]. Frames in Udacity are resized to 160~~160.
We follow the common practice~\cite{udacity} to use video clips of 10 frames each as inputs (\ie, ).
The balancing parameters  is set as 1.0 for both the speed and torque prediction tasks, while  is set as 0.2 for all auxiliary networks.
In Udacity, three vehicle states, \ie, steering angle, torque and speed are used as targets, while we only use steering angle and speed in Comma.ai, since it does not provide steering torque.
A training batch for our network contains 16 video clips. The learning rate is set as  in first 30 training episodes and reduced to  thereafter.











\vspace{0.1cm}
\noindent\textbf{Evaluation metrics}. We follow existing studies and use mean absolute error (MAE) and root mean square error (RMSE) as metrics.



\subsection{Comparative Evaluations}
\label{exp:comparison}


We compare the proposed method with state-of-the-art approaches on two publicly available datsets, \ie, Udacity and Comma.ai. The results are summarized in Table~\ref{tab:comparison}.

\noindent
\textbf{Udacity}. We compare with~\cite{kim2017interpretable} based on the results reported in their paper. We obtain the results of~\cite{udacity} by using its codes shared by the top team on official Udacity GitHub\footnote{https://github.com/udacity/self-driving-car/.}. All baselines use the same train/test partition. The baseline 3D CNN + LSTM~\cite{udacity} is the best existing method on this dataset.
It is evident from Table~\ref{tab:comparison} that a deeper model (our 50-layer ResNet) is advantageous than 3D CNN. In particular, the MAE is reduced from 2.5598 to 1.9167, a relative improvement of 25\%.
Adding LSTM to model the temporal information further improves the result of 3D ResNet from MAE of 1.9167 to 1.7147.
The best result is yielded by the proposed FM-Net, which is based on 3D ResNet + LSTM but further enhanced with heterogeneous feature mimicking.

\noindent
\textbf{Comma.ai}. Making comparisons on this dataset is more challenging as there are no official or publicly available train/test partition settings. Owing to this reason, we did not include the results reported by Kim \etal~\cite{kim2017interpretable} in Table~\ref{tab:comparison} to avoid unfair comparison. Based on our own partition setting, we run the code of the best baseline in Udacity, \ie, 3D CNN + LSTM~\cite{udacity}, on this data and report its results. Again, 3D ResNet and 3D ResNet + LSTM outperform the shallower baselines, and FM-Net with heterogeneous feature mimicking achieves the best performance. Noticeably, with feature mimicking, we bring down the MAE by 12\% (from 0.7989 to 0.7048), which is significant.

\noindent
\textbf{BDD100K}. As can be seen from Table~\ref{tab:comparison}, it is apparent that feature mimicking can bring considerable performance gains to 3D ResNet + LSTM and outperforms all previous algorithms. The results validate the effectiveness of our proposed feature mimicking method.

\noindent
\textbf{Qualitative results}.
We show qualitative results of FM-Net with and without feature mimicking in Fig.~\ref{fig:qualitative_results}.
It can be observed that FM-Net with heterogeneous feature mimicking yields much stable manoeuvre in driving albeit challenging road conditions including curved roads and extremely dark scenes. The observations are consistent on both Udacity and Comma.ai datasets. 


\begin{table}[t]
\centering
\caption{Comparison with state-of-the-art methods on Udacity, Comma.ai and BDD100K datasets.  indicates the results are copied from~\cite{kim2017interpretable}. Note that the proposed FM-Net is based on 3D ResNet+LSTM but further enhanced with heterogeneous feature mimicking.}
\label{tab:comparison}
\footnotesize{
\begin{tabular}{l|c|c}
\hline
\multirow{2}*{Method} & \multicolumn{2}{|c}{Udacity}  \\
\cline{2-3}
~ & MAE & RMSE  \\
\hline \hline
CNN + FCN~\cite{bojarski2016end}& 4.1200 & 4.8300  \\
\hline
CNN + LSTM~\cite{kim2017interpretable}& 4.1500 & 4.9300 \\
\hline
CNN + Attention~\cite{kim2017interpretable} & 4.1500 & 4.9300 \\
\hline
3D CNN~\cite{udacity} & 2.5598 & 3.6646 \\
\hline
3D CNN+LSTM~\cite{udacity} & 1.8612 & 2.7167  \\
\hline \hline
3D ResNet~(ours) & 1.9167 & 2.8532 \\
\hline
3D ResNet+LSTM~(ours) & 1.7147 & 2.4899  \\
\hline
\textbf{\algorithmname~(ours)} & \textbf{1.6236} & \textbf{2.3549} \\
\hline
\end{tabular}\vspace{3ex}
\begin{tabular}{l|c|c}
\hline
\multirow{2}*{Method} & \multicolumn{2}{|c}{Comma.ai}  \\
\cline{2-3}
~ & MAE & RMSE  \\
\hline \hline
3D CNN~\cite{udacity} & 1.7539 & 2.7316 \\
\hline
3D CNN + LSTM~\cite{udacity} & 1.4716 & 1.8397  \\
\hline \hline
3D ResNet~(ours) & 1.5427 & 2.4288 \\
\hline
3D ResNet+LSTM~(ours) & 0.7989 & 1.1519  \\
\hline
\textbf{\algorithmname~(ours)} & \textbf{0.7048} & \textbf{0.9831} \\
\hline
\end{tabular}\vspace{3ex}
\begin{tabular}{l|c}
\hline
Method & Accuracy on BDD100K \\
\hline \hline 
FCN + LSTM~\cite{xu2017end} & 82.03\% \\
\hline
CNN + LSTM~\cite{xu2017end} & 81.23\% \\
\hline
3D CNN + LSTM~\cite{udacity} & 82.94\% \\
\hline \hline
3D ResNet + LSTM (ours) & 83.69\% \\
\hline
\textbf{\algorithmname~(ours)} & \textbf{85.03\%} \\
\hline
\end{tabular}
}
\end{table}


\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{./prediction_comparison_new.pdf}
\vskip -0.3cm
  \caption{Qualitative results of FM-Net with and without heterogeneous feature mimicking (FM) on (a) Udacity and (b) Comma.ai test sets. The top row shows steering angles over time (x axis denotes frame and y axis represents steering angle). Ground-truth (gt) is represented in blue line. We selected a few representative examples to highlight the advantages of using feature mimicking. The bar charts in the second row provide closer observations on the predictions and errors (represented by the error bar).}
  \label{fig:qualitative_results}
\end{figure*}


\subsection{Ablation Study}
\label{exp:ablation_study}



\begin{table}
\centering
\caption{Performance comparison of mimicking features from different layers of auxiliary networks.  We use ``P'' and ``F'' to denote PSPNet and FlowNet, respectively. The abbreviation is used along with ``L'' and ``M'' and ``H'' to represent \tuple{low}{middle}{high}. For instance, PSPNet feature mimicking (high-level) is abbreviated as ``PH''. Full feature mimicking means ``PH + PM + PL + FH + FM + FL''.}
\label{tab:fm_ablation}
\footnotesize{
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}*{Method} & \multicolumn{2}{|c|}{Udacity} & \multicolumn{2}{|c}{Comma.ai} \\
\cline{2-5}
~ & MAE & RMSE & MAE & RMSE \\
\hline \hline
Without feat. mimick & 1.7147 & 2.4899 & 0.7989 & 1.1519 \\
\hline
PH + FH & 1.6826 & 2.4013 & 0.7514 & 1.0836 \\
\hline
PM + FM & 1.6928 & 2.4659 & 0.7749 & 1.0836 \\
\hline
PL + FL & 1.6869 & 2.4521 & 0.7627 & 1.1204 \\
\hline
PH + PM + PL & 1.6653 & 2.3847 & 0.7315 & 1.0574 \\
\hline
FH + FM + FL & 1.6573 & 2.3746 & 0.7259 & 1.0215 \\
\hline
With full feat. mimick & \textbf{1.6236} & \textbf{2.3549} & \textbf{0.7048} & \textbf{0.9831} \\
\hline
\end{tabular}
}
\end{table}

\vspace{0.15cm}
\noindent\textbf{The effectiveness of heterogeneous feature mimicking}.
We summarize the performance of mimicking features from different layers of auxiliary networks in Table.~\ref{tab:fm_ablation}. We have a few observations.
(1) Feature mimicking is beneficial since the one without feature mimicking results in the lowest performance.
(2) High-level feature mimicking (PH+FH) brings slightly more benefits than mid- (PM+FM) and low-level (PL+FL) feature mimicking judging from RMSE. This may be explained from the observation that high-level features contain more semantical meanings than those of mid- and low-levels, as supported by the feature embeddings shown in Fig.~\ref{fig:overview}(b). Another reason could be that it is more fruitful to regularize the high-level features of FM-Net rather than the mid- and low-level features.
(3) Comparing PH + PM + PL and FH + FM + FL, we see no obvious difference between using either PSPNet or FlowNet2 as the sole auxiliary network although the results obtained from using FlowNet2 is marginally better.
(4) The best performance is achieved when we use both auxiliary networks and activate the \tuple{low}{middle}{high} mimicking paths.
Mimicking features at different levels from different networks help FM-Net to capture more diverse contextual information, \eg~object motion and scene structure, at different feature resolutions.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{./different_level_mimicking.pdf}
  \vskip -0.3cm
  \caption{Comparative results of activating a single mimicking path chosen from \tuple{low}{middle}{high} of an auxiliary network.}
\label{fig:different_level_mimicking}
\end{figure}

Table~\ref{tab:fm_ablation} studies some representative combinations of mimicking paths. Next, we further examine the performance of FM-Net when we only allow a single mimicking path chosen from \tuple{low}{middle}{high} of an auxiliary network. The results are shown in Fig.~\ref{fig:different_level_mimicking}.
It is observed that high-level mimicking paths are generally superior to the mid- and low-level paths. It is interesting to learn that regularizing low-level features with mimicking brings more benefits than mid-level features do. This is an intriguing observation that worths further investigations.




\vspace{0.15cm}
\noindent\textbf{Feature mimicking v.s. pre-training}.
Pre-training is an alternative approach to introduce a side task indirectly without performing annotations on the target set -- we can pre-train the FM-Net using the same image segmentation task on Cityscape dataset~\cite{cordts2016cityscapes} as in the PSPNet and subsequently fine-tune FM-Net on the steering angle prediction task. In this way, we wish to observe if the network could still benefit from the segmentation task.
We compare feature mimicking with this approach and report the results in Table~\ref{tab:compare_with_pretraining}. We include FM-Net without both Cityscape pre-training\footnote{The mIOUs of FM-Net with Cityscape pre-training only (ResNet-50, with Large FOV, without data augmentation, ASPP and CRF) in the validation and testing set of Cityscape are 67.2 and 66.4, respectively, which are comparable with the state-of-the-art method~\cite{chen2018deeplab} (ResNet-101, with data augmentation, Large FOV, ASPP and CRF, validation: 71.4, testing: 70.4).} and feature mimicking as a baseline. In this comparison, the FM-Net variant with feature mimicking only mimics features from PSPNet (\ie, PH+PM+PL in Table~\ref{tab:fm_ablation}). All three methods in Table~\ref{tab:fm_ablation} used ImageNet initialization.
As can be observed, pre-training only yields very marginal improvement. By contrast, feature mimicking brings a higher gain to FM-Net. The results suggest that this na\"{i}ve pre-training scheme may not be the most effective way in our problem context: (1) the side task pre-training employs Cityscape, which introduces a domain gap when we applied the pre-trained network on Udacity and Comma.ai; (2) the network structure of FM-Net is not optimal for direct learning from the image segmentation task.
Feature mimicking alleviates the two aforementioned issues as it approximates PSPNet's features by using target data as input. And it focuses to approximate \tuple{low}{middle}{high} features well rather do well on the segmentation tasks itself, thus network architecture becomes a less crucial issue.


\begin{table}[t]
\centering
\caption{Comparing heterogeneous feature mimicking and network pre-training. Variants (A) without both Cityscape pre-training and feature mimicking, (B) with Cityscape pre-training only, and (C) with feature mimicking only.}
\label{tab:compare_with_pretraining}
\footnotesize{
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}*{FM-Net} & \multicolumn{2}{|c|}{Udacity} & \multicolumn{2}{|c}{Comma.ai} \\
\cline{2-5}
~ & MAE & RMSE & MAE & RMSE \\
\hline \hline
Variant A & 1.7147 & 2.4899 & 0.7989 & 1.1519 \\
\hline
Variant B & 1.7125 & 2.4614 & 0.7842 & 1.0908 \\
\hline
Variant C & 1.6653 & 2.3847 & 0.7315 & 1.0574 \\
\hline
\end{tabular}
}
\end{table} 