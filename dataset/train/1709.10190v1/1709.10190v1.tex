\section{Experiments} \label{sec-Experiments}



\begin{table*}[t]
\caption{\textbf{Office dataset.} Classification accuracy for domain adaptation over the 31 categories of the Office dataset. $\mathcal{A}$, $\mathcal{W}$, and $\mathcal{D}$ stand for Amazon, Webcam, and DSLR domain. {\tt Lower Bound} is our base model without adaptation.}
\label{tab-Office-all-Classes}
\centering
\resizebox{1.5\columnwidth}{!}{\begin{tabular}{ | l | c | c | c | c | c | c | c |}
\cline{3-8}
   \multicolumn{2}{c}{\tt{}} & \multicolumn{3}{|c|}{\tt{Unsupervised}} & \multicolumn{3}{|c|}{\tt{Supervised}}\\
\hline
            & {\tt Lower Bound} & {\tt ~\cite{tzeng2014deep}} & {\tt ~\cite{long2015learning}} & {\tt ~\cite{ghifary2016deep}} &  {\tt ~\cite{tzengHDS15iccv}} &{\tt ~\cite{koniusz2016domain}} & {\tt \modelDA}  \\
\hline
{\sl $\mathcal{A} \rightarrow \mathcal{W}$}  & 61.2 $\pm$ 0.9 & 61.8 $\pm$ 0.4 & 68.5 $\pm$ 0.4 & 68.7 $\pm$ 0.3   &  82.7 $\pm$ 0.8 & 84.5 $\pm$ 1.7  &  {\bf 88.2 $\pm$ 1.0}   \\
{\sl $\mathcal{A} \rightarrow \mathcal{D}$}  & 62.3 $\pm$ 0.8 & 64.4 $\pm$ 0.3 & 67.0 $\pm$ 0.4 & 67.1 $\pm$ 0.3   & 86.1 $\pm$ 1.2 & 86.3 $\pm$ 0.8  & {\bf 89.0 $\pm$ 1.2}    \\
{\sl $\mathcal{W} \rightarrow \mathcal{A}$}  & 51.6 $\pm$ 0.9 & 52.2 $\pm$ 0.4 & 53.1 $\pm$ 0.3 & 54.09 $\pm$ 0.5  & 65.0 $\pm$ 0.5 & 65.7 $\pm$ 1.7 &  {\bf 72.1 $\pm$ 1.0}   \\
{\sl $\mathcal{W} \rightarrow \mathcal{D}$}  & 95.6 $\pm$ 0.7  & 98.5 $\pm$ 0.4 & {\bf 99.0 $\pm$ 0.2} & {\bf 99.0 $\pm$ 0.2}   & 97.6 $\pm$ 0.2 & 97.5 $\pm$ 0.7  &  97.6 $\pm$ 0.4   \\
{\sl $\mathcal{D} \rightarrow \mathcal{A}$}  & 58.5 $\pm$ 0.8  & 52.1 $\pm$ 0.8 & 54.0 $\pm$ 0.4 & 56.0 $\pm$ 0.5 &  66.2 $\pm$ 0.3   & 66.5 $\pm$ 1.0 &  {\bf 71.8 $\pm$ 0.5}   \\
{\sl $\mathcal{D} \rightarrow \mathcal{W}$}  & 80.1 $\pm$ 0.6 & 95.0 $\pm$ 0.5 & 96.0 $\pm$ 0.3 & {\bf 96.4 $\pm$ 0.3}   & 95.7 $\pm$ 0.5 & 95.5 $\pm$ 0.6 & {\bf 96.4 $\pm$ 0.8}  \\
\hline
{\sl Average}  & 68.2 & 70.6 & 72.9 & 73.6   & 82.21 & 82.68 & {\bf 85.8}  \\
\hline
    \end{tabular}}
\end{table*}


\begin{table}[t]
\caption{\textbf{Office dataset.} Classification accuracy for domain adaptation over the Office dataset when only the labeled target samples of 15 classes are available during training. Testing is done on all 31 classes. $\mathcal{A}$, $\mathcal{W}$, and $\mathcal{D}$ stand for Amazon, Webcam, and DSLR domain. {\tt Lower Bound} is our base model without adaptation.}
\label{tab-Office-missing-Classes}
\centering
\resizebox{.7\columnwidth}{!}{\begin{tabular}{ | l | c | c | c |}
\hline
            & {\tt Lower Bound} & {\tt ~\cite{tzengHDS15iccv}} & {\tt \modelDA}  \\
\hline
{\sl $\mathcal{A} \rightarrow \mathcal{W}$}  & 52.1 $\pm$ 0.6 & 59.3 $\pm$ 0.6 & {\bf 63.3 $\pm$ 0.9} \\
{\sl $\mathcal{A} \rightarrow \mathcal{D}$}  & 61.6 $\pm$ 0.8 & 68.0 $\pm$ 0.5 & {\bf 70.5 $\pm$ 0.6} \\
{\sl $\mathcal{W} \rightarrow \mathcal{A}$}  & 34.5 $\pm$ 0.9 & 40.5 $\pm$ 0.2 & {\bf 43.6 $\pm$ 1.0}  \\
{\sl $\mathcal{W} \rightarrow \mathcal{D}$}  & 95.1 $\pm$ 0.2 & {\bf 97.5 $\pm$ 0.1} & 96.2 $\pm$ 0.3   \\
{\sl $\mathcal{D} \rightarrow \mathcal{A}$}  & 40.1 $\pm$ 0.3 & {\bf 43.1 $\pm$ 0.2} & 42.6 $\pm$ 0.6  \\
{\sl $\mathcal{D} \rightarrow \mathcal{W}$}  & 89.7 $\pm$ 0.8 & {\bf 90.0 $\pm$ 0.2} & {\bf 90.0 $\pm$ 0.2}  \\
\hline
{\sl Average}  & 62.26 & 66.4 & 67.83 \\
\hline
    \end{tabular}}
\end{table}





\begin{table*}[t]
\caption{\textbf{Office dataset.} Classification accuracy for domain adaptation over the 10 categories of the Office dataset. $\mathcal{A}$, $\mathcal{W}$, and $\mathcal{D}$ stand for Amazon, Webcam, and DSLR domain. {\tt Lower Bound} is our base model with no adaptation.}
\label{tab-Office-10-Classes}
\centering
\resizebox{1.4\columnwidth}{!}{\begin{tabular}{| l | c | c | c | c | c | c|}
\cline{2-7}
         \multicolumn{1}{c|}{\tt{}}   & {\tt Lower Bound} & {\tt GFK~\cite{gong2012geodesic}} & {\tt mSDA~\cite{chen2012marginalized}} & {\tt CDML~\cite{wang2014cross}} &  {\tt RTML~\cite{ding2017robust}} & {\tt \modelDA}  \\
\cline{2-7}
 \multicolumn{1}{c|}{\tt{}} & \multicolumn{6}{|c|}{\bf{SURF}}\\
\hline
{\sl $\mathcal{A} \rightarrow \mathcal{W}$} & 26.5 $\pm$ 3.1  & 39.9 $\pm$ 0.9 & 35.5 $\pm$ 0.5 & 37.3 $\pm$ 0.7 & 43.4 $\pm$ 0.9   &  {\bf 71.2 $\pm$ 1.3}    \\
{\sl $\mathcal{A} \rightarrow \mathcal{D}$} & 17.5 $\pm$ 1.2  & 36.2 $\pm$ 0.7 & 29.7 $\pm$ 0.7 & 35.3 $\pm$ 0.5 & 43.3 $\pm$ 0.6   &  {\bf 74.2 $\pm$ 1.3}    \\
{\sl $\mathcal{W} \rightarrow \mathcal{A}$} & 25.9 $\pm$ 1.0  & 29.8 $\pm$ 0.6 & 32.1 $\pm$ 0.8 & 32.4 $\pm$ 0.5 & 37.5 $\pm$ 0.7   &  {\bf 42.9 $\pm$ 0.9}    \\
{\sl $\mathcal{W} \rightarrow \mathcal{D}$} & 46.9 $\pm$ 1.1  & 80.9 $\pm$ 0.4 & 56.6 $\pm$ 0.4 & 77.9 $\pm$ 0.9 & {\bf 91.7 $\pm$ 1.1}   &  85.1 $\pm$ 1.0    \\
{\sl $\mathcal{D} \rightarrow \mathcal{A}$} & 19.3 $\pm$ 1.9  & 33.2 $\pm$ 0.6 & 33.6 $\pm$ 0.8 & 29.4 $\pm$ 0.8 & {\bf 36.3 $\pm$ 0.3}   &  28.9 $\pm$ 1.3    \\
{\sl $\mathcal{D} \rightarrow \mathcal{W}$} & 48.0 $\pm$ 2.1  & 79.4 $\pm$ 0.6 & 68.6 $\pm$ 0.7 & 79.4 $\pm$ 0.6 & {\bf 90.5 $\pm$ 0.7}   &  77.3 $\pm$ 1.6    \\
\hline
{\sl Average}  & 30.6 & 43.5 & 38.4 & 43.5 & 49.8   & {\bf 63.2}   \\
\hline
 \multicolumn{1}{c|}{\tt{}} & \multicolumn{6}{|c|}{\bf{DeCaF-fc6}}\\
\hline
{\sl $\mathcal{A} \rightarrow \mathcal{W}$} & 78.9 $\pm$ 1.8  & 73.1 $\pm$ 2.8 & 64.6 $\pm$ 4.2 & 75.9 $\pm$ 2.1 & 79.5 $\pm$ 2.6   &  {\bf 94.5 $\pm$ 1.9}    \\
{\sl $\mathcal{A} \rightarrow \mathcal{D}$} & 79.2 $\pm$ 2.1 & 82.6 $\pm$ 2.1 & 72.6 $\pm$ 3.5 & 81.4 $\pm$ 2.6 & 83.8 $\pm$ 1.7    & {\bf 97.2 $\pm$ 1.0}   \\
{\sl $\mathcal{W} \rightarrow \mathcal{A}$} & 77.3 $\pm$ 1.1 & 82.6 $\pm$ 1.3 & 71.4 $\pm$ 1.7 & 86.3 $\pm$ 1.6 & 90.8 $\pm$ 1.6   & {\bf 91.2 $\pm$ 0.8}   \\
{\sl $\mathcal{W} \rightarrow \mathcal{D}$} & 96.6 $\pm$ 1.0 & 98.8 $\pm$ 0.9  & 99.5 $\pm$ 0.6 & 99.4 $\pm$ 0.4 & {\bf 100 $\pm$ 0.0}   & 99.6 $\pm$ 0.5   \\
{\sl $\mathcal{D} \rightarrow \mathcal{A}$} & 84.0 $\pm$ 1.3 & 85.4 $\pm$ 0.7  & 78.8 $\pm$ 0.5 & 88.4 $\pm$ 0.5 & 90.6 $\pm$ 0.5  & {\bf 91.7 $\pm$ 1.0}    \\
{\sl $\mathcal{D} \rightarrow \mathcal{W}$} & 96.7 $\pm$ 0.9  & 91.3 $\pm$ 0.4 & 97.5 $\pm$ 0.4 & 95.1 $\pm$ 0.5 & 98.6 $\pm$ 0.3  & {\bf 98.7 $\pm$ 0.6}   \\
\hline
{\sl Average}  & 85.4 & 85.63 & 80.73 & 87.75 & 90.55   & {\bf 95.4}   \\
\hline
    \end{tabular}}
\end{table*}



\begin{table}[t]
\caption{\textbf{MNIST-USPS datasets.} Classification accuracy for domain adaptation over the MNIST and USPS datasets. $\mathcal{M}$ and $\mathcal{U}$ stand for MNIST and USPS domain. {\tt Lower Bound} is our base model without adaptation. {\tt \modelDA- $n$} stands for our method when we use $n$ labeled target samples per category in training.}
\label{tab-MNIST-USPS}
\centering
\resizebox{.65\columnwidth}{!}{\begin{tabular}{ | l | c | c | c |}
\hline
    Method        & $\mathcal{M} \rightarrow \mathcal{U}$ & $\mathcal{U} \rightarrow \mathcal{M}$ & {\sl Average} \\
\hline
{\tt ADDA~\cite{Tzeng_2017_CVPR}}   & 89.4  & 90.1 & 89.7 \\
{\tt CoGAN~\cite{liu2016coupled}}   & 91.2  & 89.1 & 90.1 \\
\hline
{\tt Lower Bound}                & 65.4  & 58.6 & 62.0 \\
{\tt CCSA-1}                     & 85.0  & 78.4 & 81.7 \\
{\tt CCSA-2}                     & 89.0  & 82.0 & 85.5 \\
{\tt CCSA-3}                     & 90.1  & 85.8 & 87.9 \\
{\tt CCSA-4}                     & 91.4  & 86.1 & 88.7 \\
{\tt CCSA-5}                     & 92.4  & 88.8 & 90.1 \\
{\tt CCSA-6}                     & 93.0  & 89.6 & 91.3 \\
{\tt CCSA-7}                     & 92.9  & 89.4 & 91.1 \\
{\tt CCSA-8}                     & 92.8  & 90.0 & 91.4 \\
\hline
    \end{tabular}}
\end{table}



We divide the experiments into two parts, domain adaptation and domain generalization. In both sections, we use
benchmark datasets and compare our domain adaptation
model and our domain generalization model, both indicated
as CCSA, with the state-of-the-art.


\subsection{Domain Adaptation}



\begin{table}[t]
\caption{\textbf{VLCS dataset.} Classification accuracy for domain generalization over the 5 categories of the VLCS dataset. {\tt LB} (Lower Bound) is our base model trained without the contrastive semantic alignment loss. {\tt 1NN} stands for first nearest neighbor. }
\label{tab-VLCS}
\centering
\resizebox{0.92\columnwidth}{!}{\begin{tabular}{ | l | c | c | c | c | c | c | c |}
\cline{2-8}
   \multicolumn{1}{c}{\tt{}} & \multicolumn{3}{|c|}{\tt{Lower Bound}} & \multicolumn{4}{|c|}{\tt{Domain Generalization}}\\
\hline
            & {\tt 1NN} & {\tt SVM} & {\tt LB} & {\tt UML~\cite{FXRQ_iccv13}} &  {\tt {\small LRE-SVM~\cite{xuLNX14eccv}}} &{\tt SCA~\cite{ghifary2016scatter}} & {\tt \modelGA}  \\
\hline
{\sl $\mathcal{L},\mathcal{C},\mathcal{S} \rightarrow \mathcal{V}$}  
& 57.2 & 58.4 & 59.1 & 56.2 &  60.5  & 64.3  & {\bf 67.1}   \\
{\sl $\mathcal{V},\mathcal{C},\mathcal{S} \rightarrow \mathcal{L}$}
& 52.4 & 55.2 & 55.6 & 58.5 &  59.7  & 59.6  & {\bf 62.1}   \\
{\sl $\mathcal{V},\mathcal{L},\mathcal{S} \rightarrow \mathcal{C}$}
& 90.5 & 85.1 & 86.1 & 91.1 &  88.1  & 88.9  & {\bf 92.3}   \\
{\sl $\mathcal{V},\mathcal{L},\mathcal{C} \rightarrow \mathcal{S}$}
& 56.9 & 55.2 & 54.6 & 58.4 &  54.8  & {\bf 59.2}  & 59.1   \\
{\sl $\mathcal{C},\mathcal{S} \rightarrow \mathcal{V},\mathcal{L}$}
& 55.0 & 55.5 & 55.3 & 56.4 &  55.0  & {\bf 59.5}  & 59.3   \\
{\sl $\mathcal{C},\mathcal{L} \rightarrow \mathcal{V},\mathcal{S}$}
& 52.6 & 51.8 & 50.9 & {\bf 57.4} &  52.8  & 55.9  & 56.5   \\
{\sl $\mathcal{V},\mathcal{C} \rightarrow \mathcal{L}, \mathcal{S}$}
& 56.6 & 59.9 & 60.1 & 55.4 &  58.8  & {\bf 60.7}  & 60.2   \\
\hline
{\sl Average}  
& 60.1 & 60.1 & 60.2 & 61.5 &  61.4  & 64.0 & {\bf 65.0}  \\
\hline
    \end{tabular}}
\end{table}



We present results using the Office dataset~\cite{saenkoKFD2010eccv}, the MNIST dataset~\cite{lecun1998gradient}, and the USPS dataset~\cite{hull1994database}.

\subsubsection{Office Dataset} 

The office dataset is a standard benchmark dataset for visual domain adaptation. It contains 31 object classes for three domains: Amazon, Webcam, and DSLR, indicated as $\mathcal{A}$, $\mathcal{W}$, and $\mathcal{D}$, for a total of 4,652 images. We consider six domain shifts using the three domains ($\mathcal{A} \rightarrow \mathcal{W}$, $\mathcal{A} \rightarrow \mathcal{D}$, $\mathcal{W} \rightarrow \mathcal{A}$, $\mathcal{W} \rightarrow \mathcal{D}$, $\mathcal{D} \rightarrow \mathcal{A}$, and $\mathcal{D} \rightarrow \mathcal{W}$). We performed different experiments using this dataset.

\noindent \textbf{First experiment.} We followed the setting described in~\cite{tzengHDS15iccv}. All classes of the office dataset and 5 train-test splits are considered. For the source domain, 20 examples per category for the Amazon domain, and 8 examples per category for the DSLR and Webcam domains are randomly selected for training for each split. Also, 3 labeled examples are randomly selected for each category in the
target domain for training for each split. The rest of the target samples are used for testing. Note that we used the same splits generated by~\cite{tzengHDS15iccv}. We also report the classification results of the SDA algorithm presented in~\cite{long2015learning} and~\cite{koniusz2016domain}. In addition to the SDA algorithms, we report the results of some recent UDA algorithms. They follow a different experimental protocol compared to the SDA algorithms, and use all samples of the target domain in training as unlabeled data together with all samples of the source domain. 

\begin{comment}
So, we cannot make an exact comparison between results. However, since UDA algorithms use all samples of the target domain in training and we use only very few of them (3 per class), we think it is still worth looking at how they differ.
\end{comment}

For the embedding function $g$, we used the convolutional layers of the VGG-16 architecture~\cite{Simonyan14c} followed by 2 fully connected layers with output size of 1024 and 128, respectively. For the prediction function $h$, we used a fully connected layer with softmax activation. Similar to~\cite{tzengHDS15iccv}, we used the weights pre-trained on the ImageNet dataset~\cite{imagenet2015} for the convolutional layers, and initialized the fully connected layers using all the source domain data. We then fine-tuned all the weights using the train-test splits.

Table~\ref{tab-Office-all-Classes} reports the classification accuracy over $31$ classes for the Office dataset and shows that \modelDA has better performance compared to~\cite{tzengHDS15iccv}. Since the difference between $\mathcal{W}$ domain and $\mathcal{D}$ domain is not considerable, unsupervised algorithms work well on $\mathcal{D} \rightarrow \mathcal{W}$ and $\mathcal{W} \rightarrow \mathcal{D}$.   However, in the cases when target and source domains are very different ($\mathcal{A} \rightarrow \mathcal{W}$, $\mathcal{W} \rightarrow \mathcal{A}$, $\mathcal{A} \rightarrow \mathcal{D}$, and $\mathcal{D} \rightarrow \mathcal{A}$), \modelDA shows larger margins compared to the second best. This suggests that \modelDA will provide greater alignment gains when there are bigger domain shifts. Figure~\ref{fig-plots}(a) instead, shows how much improvement can be obtained with respect to the base model. This is simply obtained by training $g$ and $h$ with only the classification loss and source training data, so no adaptation is performed.


\noindent \textbf{Second experiment.}  We followed the setting described in~\cite{tzengHDS15iccv} when only 10 target labeled samples of 15 classes of the Office dataset are available during training. Similar to~\cite{tzengHDS15iccv}, we compute the accuracy on the remaining 16 categories for which no target data was available during training. We used the same network structure as in the first experiment and the same splits generated by~\cite{tzengHDS15iccv}.

Table~\ref{tab-Office-missing-Classes} shows that \modelDA is effective at transferring information from the labeled classes to the unlabeled target classes. Similar to the first experiment, \modelDA works well when shifts between domains are larger.

\noindent \textbf{Third experiment.} We used the original train-test splits of the Office dataset~\cite{saenkoKFD2010eccv}. The splits are generated in a similar manner to the first experiment but here instead, only 10 classes are considered (backpack, bike, calculator, headphones, keyboard, laptop-computer, monitor, mouse, mug, and projector). In order to compare our results with the state-of-the-art, we used DeCaF-fc6 features~\cite{Donahue13decaf} and 800-dimension SURF features as input.  For DeCaF-fc6 features (SURF features) we used 2 fully connected layers with output size of 1024 (512) and 128 (32) with ReLU activation as the embedding function, and one fully connected layer with softmax activation as the prediction function.
The features and splits are available on the Office dataset webpage~\footnote{\url{https://cs.stanford.edu/~jhoffman/domainadapt/}}.

We compared our results with three UDA (GFK~\cite{gong2012geodesic}, mSDA~\cite{chen2012marginalized}, and RTML~\cite{ding2017robust}) and one SDA (CDML~\cite{wang2014cross}) algorithms under the same settings. Table~\ref{tab-Office-10-Classes} shows that \modelDA provides an improved accuracy with respect to the others. Again, greater domain shifts are better compensated by \modelDA. Figure~\ref{fig-plots}(b) shows the improvement of \modelDA over the base model using DeCaF-fc6 features.

\begin{comment}
\modelDA did not win 3 out of 6 cases. That is most likely attributed to the fact that the network used here is more suitable for sparse features like DeCaF-fc6 than non-sparse features like SURF. Indeed, \modelDA is more suitable for CNN networks followed by fully connected layers.
\end{comment}


\subsubsection{MNIST-USPS Datasets} \label{sec-MNIST-USPS}

The MNIST ($\mathcal{M}$) and USPS ($\mathcal{U}$) datasets have recently been used for domain adaptation~\cite{fernandoTT15prl,rozantsev2016beyond}. They contain images of digits from 0 to 9. We considered two cross-domain tasks, $\mathcal{M} \rightarrow \mathcal{U}$ and $\mathcal{U} \rightarrow \mathcal{M}$, and followed the experimental setting in~\cite{fernandoTT15prl,rozantsev2016beyond}, which involves randomly selecting 2000 images from MNIST and 1800 images from USPS. Here, we randomly selected $n$ labeled samples per class from target domain data and used them in training. We evaluated our approach for $n$ ranging from 1 to 8 and repeated each experiment $10$ times (we only show the mean of the accuracies because the standard deviation is very small).

\begin{comment}
Since the images of the USPS dataset have $16\times16$ pixels, we resized the images of the MNIST dataset to $16\times16$ pixels. 

 where {\tt CCSA-$n$} stands for our method when we use $n$ labeled target samples per category in training.
\end{comment}

Similar to~\cite{lecun1998gradient}, we used $2$ convolutional layers with 6 and 16 filters of $5\times5$ kernels followed by max-pooling layers and $2$ fully connected layers with size $120$ and $84$ as the embedding function $g$, and one fully connected layer with softmax activation as the prediction function $h$. We compare our method with $2$ recent UDA methods. Those methods use all target samples in their training stage, while we only use very few labeled target samples per category in training.

Table~\ref{tab-MNIST-USPS} shows the average classification accuracy of the MNIST-USPS datasets. \modelDA works well even when only one target sample per category ($n=1$) is available in training. Also, we can see that by increasing $n$, the accuracy quickly converges to the top.

\noindent \textbf{Ablation study.} We consider three baselines to compare with \modelDA  for $\mathcal{M} \rightarrow \mathcal{U}$ task. First, we train the network with source data and then fine-tune it with available target data. Second, we train the network using the classification and semantic
alignment losses ($\mathcal{L}_{CSA} (f) = \mathcal{L}_{C} (h\circ g) + \mathcal{L}_{SA} (g)$). Third, we train the network using the classification and separation losses ($\mathcal{L}_{CS} (f) = \mathcal{L}_{C} (h\circ g) + \mathcal{L}_{S} (g)$). Figure~\ref{fig-plots}(d) shows the average accuracies over $10$ repetitions. It shows that CSA and CS improve the accuracy over fine-tuning. Using the semantic alignment loss together with separation loss (CCSA) shows the best performance.


\noindent \textbf{Visualization.} We show how samples lie on the embedding space using \modelDA. First, we considered the row images of the MNIST and USPS datasets and plotted 2D visualization of them using t-SNE algorithm~\cite{maaten2008visualizing}. As Figure~\ref{fig-domain_embeded}(Left) shows the row images of the same class and different domains lie far away from each other in the 2D subspace. For example, the samples of the class zero of the USPS dataset ($0\; U$) are far from the class zero of the MNIST dataset ($0\; M$). Second, we trained our base model with no adaptation on the MNIST dataset. We then plotted the 2D visualization of the MNIST and USPS samples in the embedding space (output of $g$, the last fully connected layer). As Figure~\ref{fig-domain_embeded}(Middle) shows, the samples from the same class and different domains still lie far away from each other in the 2D subspace. Finally, we trained our SDA model on the MNIST dataset and $3$ labeled samples per class of the USPS dataset. We then plotted the 2D visualization of the MNIST and USPS samples in the embedding space (output of $g$). As Figure~\ref{fig-domain_embeded}(Right) shows, the samples from the same class and different domains now lie very close to each other in the 2D subspace. Note however, that this is only a 2D visualization of high-dimensional data, and Figure~\ref{fig-domain_embeded}(Right) may not perfectly reflect how close is the data from the same class, and how classes are separated.

\noindent \textbf{Weight sharing}: There is no restriction whether $g_t$ and $g_s$ should share the weights or not. Not sharing weights will likely lead to overfitting, given the reduced amount of target training data, and weight-sharing can be seen as a regularizer. We repeated the experiment for $\mathcal{M} \rightarrow \mathcal{U}$ task when $n=4$. Not sharing weights provides the average accuracy $88.6$ over $10$ repetitions  which is less than the average accuracy with weight-sharing (see Table~\ref{tab-MNIST-USPS}). A similar behavior can be seen for other experiments.

\subsection{Domain Generalization}

We evaluate \modelGA on different datasets. The goal is to show that \modelGA is able to learn a domain invariant embedding subspace for visual recognition tasks.

\subsection{VLCS Dataset}
In this section, we use images of $5$ shared object categories (bird, car, chair, dog, and person), of the PASCAL VOC2007 ($\mathcal{V}$)~\cite{everingham2010pascal}, LabelMe ($\mathcal{L}$)~\cite{russell2008labelme},
Caltech-101 ($\mathcal{C}$)~\cite{fei2007learning}, and SUN09 ($\mathcal{S}$)~\cite{choi2010exploiting} datasets, which is known as VLCS dataset~\cite{FXRQ_iccv13}.

\cite{ghifary2015domain,ghifary2016scatter} have shown that there are covariate shifts between the above $4$ domains and have developed a DG method to minimize them. We followed their experimental setting, and randomly divided each domain into a training set ($70\%$) and a test set ($30\%$) and conducted a {\tt leave-one-domain-out} evaluation ($4$ cross-domain cases) and a {\tt leave-two-domain-out} evaluation ($3$ cross-domain cases). In order to compare our results with the state-of-the-art, we used DeCaF-fc6 features which are publicly available~\footnote{\url{http://www.cs.dartmouth.edu/~chenfang/proj_page/FXR_iccv13/index.php}}, and repeated each cross-domain case $20$ times and reported the average classification accuracy.

We used $2$ fully connected layers with output size of $1024$ and $128$ with ReLU activation as the embedding function $g$, and one fully connected layer with softmax activation as the prediction function $h$.
To create positive and negative pairs for training our network, for each sample of a source domain we randomly selected 5 samples from each remaining source domain, and help in this way to avoid overfitting. However, to train a deeper network together with convolutional layers, it is enough to create a large amount of positive and negative pairs.


We report comparative results in Table~\ref{tab-VLCS}, where all DG methods work better than the base model, emphasizing the need for domain generalization. Our DG method has higher average performance. Also, note that in order to compare with the state-of-the-art DG methods, we only used 2 fully connected layers for our network and precomputed features as input. However, when using convolutional layers on row images, we expect our DG model to provide better performance. Figure~\ref{fig-plots}(c) shows the improvement of our DG model over the base model using DeCaF-fc6 features.

\subsection{MNIST Dataset}
We followed the setting in~\cite{ghifary2015domain}, and randomly selected a set $M$ of $100$ images per category from the MNIST dataset ($1000$ in total). We then rotated each image in $M$ five times with $15$ degrees intervals, creating five new domains $M_{15^{\circ}}$, $M_{30^{\circ}}$, $M_{45^{\circ}}$, $M_{60^{\circ}}$, and $M_{75^{\circ}}$. We conducted a {\tt leave-one-domain-out} evaluation ($6$ cross-domain cases in total). We used the same network of Section~\ref{sec-MNIST-USPS}, and we repeated the experiments $10$ times. To create positive and
negative pairs for training our network, for each sample of a
source domain we randomly selected 2 samples from each
remaining source domain. We report comparative average accuracies for \modelGA and others in Table~\ref{tab-MNIST}, showing again a performance improvement.


\begin{table}[t]
\caption{\textbf{MNIST dataset.} Classification accuracy for domain generalization over the MNIST dataset and its rotated domains. }
\label{tab-MNIST}
\centering
\resizebox{.9\columnwidth}{!}{\begin{tabular}{ | c | c | c | c |}
\hline
         & {\sl CAE~\cite{rifai2011contractive}} & {\sl MTAE~\cite{ghifary2015domain}} & {\sl \modelGA} \\
\hline
$M_{15^{\circ}},M_{30^{\circ}},M_{45^{\circ}},M_{60^{\circ}},M_{75^{\circ}}  \rightarrow M $   
& 72.1  & 82.5 & {\bf 84.6} \\
$M,M_{30^{\circ}},M_{45^{\circ}},M_{60^{\circ}},M_{75^{\circ}}  \rightarrow M_{15^{\circ}} $           
& 95.3  & {\bf 96.3} & 95.6 \\
$M,M_{15^{\circ}},M_{45^{\circ}},M_{60^{\circ}},M_{75^{\circ}}  \rightarrow M_{30^{\circ}} $                 
& 92.6  & 93.4 & {\bf 94.6} \\
$M,M_{15^{\circ}},M_{30^{\circ}},M_{60^{\circ}},M_{75^{\circ}}  \rightarrow M_{45^{\circ}} $                      
& 81.5  & 78.6 & {\bf 82.9} \\
$M,M_{15^{\circ}},M_{30^{\circ}},M_{45^{\circ}},M_{75^{\circ}}  \rightarrow M_{60^{\circ}} $                      
& 92.7  & 94.2 & {\bf 94.8} \\
$M,M_{15^{\circ}},M_{30^{\circ}},M_{45^{\circ}},M_{60^{\circ}}  \rightarrow M_{75^{\circ}}  $                      
& 79.3  & 80.5 & {\bf 82.1} \\
\hline
Average 
& 85.5  & 87.5 & {\bf 89.1} \\
\hline
    \end{tabular}}
    
\end{table}


