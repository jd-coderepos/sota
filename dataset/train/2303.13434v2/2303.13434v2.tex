

\documentclass[10pt,twocolumn,letterpaper, ]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{overpic}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{overpic}
\usepackage{bm}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\usepackage[dvipsnames]{xcolor}
\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
\definecolor{yellow}{rgb}{0.85, 0.85, 0.31}
\definecolor{blue_light}{rgb}{0.34, 0.95, 0.95}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{colorlinks,linkcolor={red},citecolor={hollywoodcerise},urlcolor={red}}  
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{12593} \def\confName{CVPR}
\def\confYear{2023}
\usepackage[accsupp]{axessibility}
\begin{document}
\title{Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective}

\author{Jinjing Zhu$^{1}$\thanks{These authors contributed equally to this work. }
\quad
Haotian Bai$^{1}$\footnotemark[1]
\quad
Lin Wang$^{1,2}$\thanks{Corresponding Author.}
\and
\affmark[1] AI Thrust, HKUST(GZ)\quad
\affmark[2] Dept. of CSE, HKUST\\
\quad
{\tt\small zhujinjing.hkust@gmail.com, haotianwhite@outlook.com, linwang@ust.hk}
}

\maketitle
\begin{abstract}
\vspace{-10pt}
Endeavors have been recently made to leverage the vision transformer (ViT) for the challenging unsupervised domain adaptation (UDA) task. They typically adopt the cross-attention in ViT for direct domain alignment. 
However, as the performance of cross-attention highly relies on the quality of pseudo labels for targeted samples, it becomes less effective when the domain gap becomes large. 
We solve this problem from a game theory's perspective with the proposed model dubbed as \textbf{PMTrans}, which bridges source and target domains with an intermediate domain. 
Specifically, we propose a novel ViT-based module called \textbf{PatchMix} that effectively builds up the intermediate domain,~\ie, probability distribution, by learning to sample patches from both domains based on the game-theoretical models.
This way, it learns to mix the patches from the source and target domains to maximize the cross entropy (CE), while exploiting two semi-supervised mixup losses in the feature and label spaces to minimize it.
As such, we interpret the process of UDA as a min-max CE game with three players, including the feature extractor, classifier, and PatchMix, to find the Nash Equilibria. 
Moreover, we leverage attention maps from ViT to re-weight the label of each patch by its importance, making it possible to obtain more domain-discriminative feature representations. We conduct extensive experiments on \textbf{four} benchmark datasets, and the results show that PMTrans significantly surpasses the ViT-based and CNN-based SoTA methods by \textbf{+3.6\%} on Office-Home, \textbf{+1.4\%} on Office-31, and \textbf{+7.2\%} on DomainNet, respectively. \url{https://vlis2022.github.io/cvpr23/PMTrans}
\end{abstract}
\vspace{-18pt}
\section{Introduction}
\vspace{-5pt}
\begin{figure}[t]
    \centering
    \captionsetup{font=small}
    \includegraphics[width=0.85\linewidth]{result2.pdf}
    \vspace{-8pt}
    \caption{The classification accuracy of our PMTrans surpasses the SoTA methods by \textbf{+7.2\%} on the most challenging DomainNet dataset. Note that the target tasks treat one domain of DomainNet as the target domain and the others as the source domains.}
    \label{fig:result_cover}
    \vspace{-20pt}
\end{figure}
Convolutional neural networks (CNNs) have achieved tremendous success on numerous vision tasks; however, they still suffer from the limited generalization capability to a new domain due to the domain shift problem~\cite{ZhangDTZJ22}. Unsupervised domain adaptation (UDA) tackles this issue by transferring knowledge from a labeled source domain to an unlabeled target domain~\cite{PanY10}. A significant line of solutions reduces the domain gap based on the category-level alignment which produces pseudo labels for the target samples, such as metric learning~\cite{Kang0YH19, ZhuZWKCBXH21}, adversarial training~\cite{SaitoWUH18, Du0S0021, LiL0Y21}, and optimal transport~\cite{XuLWC020}. Furthermore, several works ~\cite{Dosovitskiy2021AnII,abs-2204-07683} explore the potential of ViT for the non-trivial UDA task. Recently, CDTrans \cite{abs-2109-06165} exploits the cross-attention in ViT for direct domain alignment, buttressed by the crafted pseudo labels for target samples. 
However, CDTrans has a distinct limitation: as the performance of cross-attention highly depends on the quality of pseudo labels, it becomes less effective when the domain gap becomes large. 
As shown in Fig.~\ref{fig:result_cover}, due to the significant gap between
the domain \textit{qdr} and the other domains, aligning distributions directly between them performs poorly. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{introduction.pdf}
    \captionsetup{font=small}
    \vspace{-8pt}
    \caption{PMTrans builds up the intermediate domain (\textcolor{green(pigment)}{green patches}) via a novel PatchMix module by learning to sample patches from the source (\textcolor{blue}{blue patches}) and target (\textcolor{pink}{pink patches}) domains. PatchMix tries to maximize the CE (\textcolor{green(pigment)}{$\uparrow$}) between the intermediate domain and source/target domain, while the feature extractor and classifier try to minimize it (\textcolor{red}{$\downarrow$}) for aligning domains.}
    \label{fig:cover_mix}
    \vspace{-15pt}
\end{figure}
In this paper, we probe a new problem for UDA: \textbf{\textit{how to smoothly bridge the source and target domains by constructing an intermediate domain with an effective ViT-based solution?}}
The intuition behind this is that, compared to direct aligning domains,  
alleviating the domain gap between the intermediate and source/target domain can facilitate domain alignment. 
Accordingly, we propose a novel and effective method, called \textbf{PMTrans} (PatchMix Transformer) to construct the intermediate representations. 
Overall, PMTrans interprets the process of domain alignment as a min-max cross entropy (CE) game with three players, \ie, the feature extractor, a classifier, and a \textbf{PatchMix} module, to find the Nash Equilibria. Importantly, the PatchMix module is proposed to effectively build up the intermediate domain,~\ie, probability distribution, by learning to sample patches from both domains with weights generated from a learnable Beta distribution based on the game-theoretical models~\cite{abs-2202-05352, bacsar1998dynamic, MazumdarRS20}, as shown in Fig.~\ref{fig:cover_mix}. That is, we aim to learn to mix patches from two domains to maximize the CE between the intermediate domain and source/target domain. Moreover, two semi-supervised mixup losses in the feature and label spaces are proposed to minimize the CE. Interestingly, \textbf{we conclude that the source and target domains are aligned if mixing the patch representations from two domains is equivalent to mixing the corresponding labels}. Therefore, the domain discrepancy can be measured based on the CE between the mixed patches and mixed labels. Eventually, the three players have no incentive to change their parameters to disturb CE, meaning the source and target domains are well aligned. Unlike existing mixup methods~\cite{ZhangCDL18, YunHCOYC19, UddinMSCB21}, our proposed PatchMix subtly learns to combine the element-wise global and local mixture by mixing patches from the source and target domains for ViT-based UDA. Moreover, we leverage the class activation mapping (CAM) from ViT to allocate the semantic information to re-weight the label of each patch, thus enabling us to obtain more domain-discriminative features. 

We conduct experiments on \textbf{four} benchmark datasets, including Office-31 \cite{SaenkoKFD10}, Office-Home \cite{VenkateswaraECP17}, VisDA-2017 \cite{abs-1710-06924}, and DomainNet \cite{PengBXHSW19}.
The results show that the performance of PMTrans significantly surpasses that of the ViT-based \cite{abs-2204-07683,abs-2109-06165,abs-2108-05988} and CNN-based SoTA methods~\cite{NaJCH21,LiXGLWL21,SharmaKC21} by \textbf{+3.6\%} on Office-Home, \textbf{+1.4\%} on Office-31, and \textbf{+7.2\%} on DomainNet (See Fig.~\ref{fig:result_cover}), respectively. 

Our main contributions are four-fold: (\textbf{I}) We propose a novel ViT-based UDA framework, PMTrans, to effectively bridge source and target domains by constructing the intermediate domain. (\textbf{II}) We propose PatchMix, a novel module to build up the intermediate domain via the game-theoretical models. (\textbf{III}) We propose two semi-supervised mixup losses in the feature and label spaces to reduce CE in the min-max CE game. (\textbf{IV}) Our PMTrans surpasses the prior methods by a large margin on three benchmark datasets. 



\vspace{-8pt}
\section{Related Work}
\vspace{-3pt}
\noindent \textbf{Unsupervised Domain Adaptation.}
The prevailing UDA methods focus on domain alignment and learning discriminative domain-invariant features via metric learning, domain adversarial training, and optimal transport. Firstly, the metric learning-based methods aim to reduce the domain discrepancy by learning the domain-invariant feature representations using various metrics. For instance, some methods~\cite{LongC0J15, LongZ0J17, ZhuZW19, Kang0YH19} use the maximum mean discrepancy (MMD) loss to measure the divergence between different domains. In addition, the central moment discrepancy (CMD) loss~\cite{ZellingerGLNS17} and maximum density divergence (MDD) loss~\cite{LiCDZLS21} are also proposed to align the feature distributions. Secondly, the domain adversarial training methods learn the domain-invariant representations to encourage samples from different domains to be non-discriminative with respect to the domain labels via an adversarial loss~\cite{GaninUAGLLML16, XuZNLWTZ20, WuIE20}.
The third type of approach aims to minimize the cost transported from the source to the target distribution by finding an optimal coupling cost to mitigate the domain shift~\cite{CourtyFTR17, CourtyFHR17}. Unfortunately, these methods are not robust enough for the noisy pseudo target labels for accurate domain alignment.\textit{ Different from these mainstream UDA methods and ~\cite{AcunaLZF22}, we interpret the process of UDA as a min-max CE game and find the Nash Equilibria for domain alignment with an intermediate domain and a pure ViT-based solution.}

\noindent\textbf{Mixup.}
It is an effective data augmentation technique to prevent models from over-fitting by linearly interpolating two input data. Mixup types can be categorized into global mixup (e.g., Mixup \cite{ZhangCDL18} and Manifold-Mixup \cite{VermaLBNMLB19}) and local mixup (CutMix \cite{YunHCOYC19}, saliency-CutMix \cite{UddinMSCB21}, TransMix \cite{DBLP:journals/corr/abs-2111-09833}, and Tokenmix \cite{LiuLZ0022}). In CNN-based UDA tasks, several works \cite{XuZNLWTZ20, WuIE20, NaJCH21, Dai00T0D21} also use the mixup technique by linearly mixing the source and target domain data. \textit{In comparison, we unify the global and local mixup in our PMTrans framework by learning to form a mixed patch from the source/target patch as the input to ViT. We learn the hyperparameters of the mixup ratio for each patch, which is the \textbf{first} attempt to interpolate patches based on the distribution estimation. Accordingly, we propose PatchMix which effectively builds up the intermediate domain by sampling patches from both domains based on the game-theoretical models.}

\noindent\textbf{Transformer.} Vision Transformer(ViT) \cite{VaswaniSPUJGKP17} has recently been introduced to tackle the challenges in various vision tasks~\cite{CaronTMJMBJ21, LiuL00W0LG21}. Several works have leveraged ViT for the non-trivial UDA task. TVT~\cite{abs-2108-05988} proposes an adaptation module to capture domain data's transferable and discriminative features. SSRT \cite{abs-2204-07683} proposes a framework with a transformer backbone and a safe self-refinement strategy to handle the issue in case of a large domain gap. More recently, CDTrans \cite{abs-2109-06165} proposes a two-step framework that utilizes the cross-attention in ViT for direct feature alignment and pre-generated pseudo labels for the target samples. \textit{Differently, we probe to construct an intermediate domain to bridge the source and target domains for better domain alignment. Our PMTrans effectively interprets the process of domain alignment as
a min-max CE game, leading to a significant UDA performance enhancement}. 


\begin{figure*}[t!]
    \centering
    
    \includegraphics[width=.85\textwidth]{framework.pdf}
    \captionsetup{font=small}
    \vspace{-10pt}
    \caption{Overview of the proposed PMTrans framework. It consists of three players:
the PatchMix module empowered by a patch embedding (\textbf{Emb}) layer and a learnable Beta distribution (\textbf{Beta}), ViT encoder, and classifier. 
    }
    \label{fig:framework}
    \vspace{-15pt}
\end{figure*}

\vspace{-4pt}
\section{Methodology}\label{Methodology}
\vspace{-4pt}
In UDA, denote a labeled source set $\mathcal{D}_{s}=\left\{\left(\boldsymbol{x}^{s}_{i}, \boldsymbol{y}^{s}_{i}\right)\right\}_{i=1}^{n_{s}}$ with $i$-th sample $\boldsymbol{x}_{i}^{s} $ and its corresponding one-hot label $\boldsymbol{y}_{i}^{s}$ and an unlabeled target set $\mathcal{D}_{t}=$ $\left\{\boldsymbol{x}^{t}_{j}\right\}_{j=1}^{n_{t}}$ with $j$-th sample $\boldsymbol{x}_{j}^{t}$, $n_{s}$ and $n_{t}$ as the size of samples in the source and target domains, respectively. Note that the data in two domains are sampled from two different distributions, and we assume that the two domains share the same label space. Our goal is to address the significant domain divergence issue and smoothly transfer the knowledge from the source domain to the target domain. Firstly, we define and introduce PatchMix, and interpret the process of UDA as a min-max CE game. Secondly, we describe the proposed PMTrans which smoothly aligns the source and target domains by constructing an intermediate domain via a three-player game. 
\subsection{PMTrans: Theoretical Analysis}
\label{sub:PMTrans}
\subsubsection{PatchMix}




\begin{definition}
\label{Def: Mixup}
(PatchMix): Let $\mathcal{P}_{\lambda}$ be a linear interpolation operation on two pairs of randomly drawn samples $(\boldsymbol{x}^{s}, \boldsymbol{y}^{s})$ and $(\boldsymbol{x}^{t}, \boldsymbol{y}^{t})$. Then with $\lambda_k \sim \operatorname{Beta}(\beta, \gamma)$, it interpolates the $k$-th source patch $\boldsymbol{x}_k^{s}$ and target patch $\boldsymbol{x}_k^{t}$ to reconstruct a mixed representation with $n$ patches.
\begin{small}
\begin{equation}
\begin{aligned}
\label{eq:mixup_eq}
   &\boldsymbol{x}^{i}=\mathcal{P}_\lambda(\boldsymbol{x}^s, \boldsymbol{x}^t), ~\boldsymbol{x}^{i}_{k}=\lambda_{k} \odot \boldsymbol{x}_k^s + (1-\lambda_k)\odot\boldsymbol{x}_k^t,  \\ 
   &\boldsymbol{y}^{i}=\mathcal{P}_\lambda(\boldsymbol{y}^s, \boldsymbol{y}^t) = \frac{(\sum_{k=1}^{n}\lambda_{k})\boldsymbol{y}^s + (\sum_{k=1}^{n}(1-\lambda_{k}))\boldsymbol{y}^t}{n}. 
\end{aligned}
\end{equation}
\end{small}
\end{definition}



where $\boldsymbol{x}^{i}_{k}$ is the $k$-th patch of $\boldsymbol{x}^{i}$, and $\odot$ denotes multiplication. In Definition.~\ref{Def: Mixup}, each image $\boldsymbol{x}^{i}$ of the intermediate domain composes the sampled patches $\boldsymbol{x}_{k}$ from the source/target domain. Here, $\lambda_k \in [0,1]$ is the random mixing proportion that denotes the patch-level sampling weights. Furthermore, we calculate the image-level importance by aggregating patch weights $\sum_{k=1}^{n}(1-\lambda_{k})$, which is utilized to interpolate their labels. As a result, we mix both samples $(\boldsymbol{x}^s, \boldsymbol{y}^s)$ and $(\boldsymbol{x}^t, \boldsymbol{y}^t)$ to construct a new intermediate domain $\mathcal D_{i}=\left\{\left(\boldsymbol{x}^{i}_{l}, \boldsymbol{y}^{i}_{l}\right)\right\}_{l=1}^{n_{i}}$. 
To align the source and target domains, we need to evaluate the gap numerically. In detail, let $P_S$ and $P_T$ be the empirical distributions defined by $\mathcal{D}_{s}$ and $\mathcal{D}_{t}$, respectively. The domain divergence between source and target domains can be measured as 
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:eq1}
\begin{aligned}
 D(P_S,P_T)
 =&\inf _{f \in \mathcal{F}, c \in \mathcal{C}, \mathcal{P}_{\lambda} \in \mathcal{P}} \underset{(\boldsymbol{x}^s, \boldsymbol{y}^s),\left(\boldsymbol{x}^{t}, \boldsymbol{y}^{t}\right)} {\mathbb{E}}\\ &\ell\left(c\left(\mathcal{P}_{\lambda}\left(f(\boldsymbol{x}^{s}), f\left(\boldsymbol{x}^{t}\right)\right)\right), \mathcal{P}_{\lambda}\left(\boldsymbol{y}^{s}, \boldsymbol{y}^{t}\right)\right),
\end{aligned}
\end{equation}}
where $\mathcal{F}$ denotes a set of encoding functions \ie, the feature extractor and $\mathcal{C}$ denotes a set of decoding functions \ie the classifier. Let $\mathcal{P}$ be the set of functions to generate the mixup ratio for building the intermediate domain. Then we can reformulate Eq. \ref{eq:eq1} as
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{small}
\begin{equation}
\begin{aligned}
\label{eq:D}
  D\left(P_{S},P_{T}\right)
  =& \inf_{\boldsymbol{h}_{1}^{s}, \ldots, \boldsymbol{h}_{n_{s}}^{s} \in \mathcal{H}^{s},\boldsymbol{h}_{1}^{t}, \ldots, \boldsymbol{h}_{n_{t}}^{t} \in \mathcal{H}^{t}}\frac{1}{n_{s} \times n_{t}} \sum_{i}^{n_{s}}\sum_{j}^{n_{t}}\\
  &\left\{\inf _{c \in \mathcal{C}} \int_{0}^{1} \ell\left(f\left(\mathcal{P}_{\lambda}\left(\boldsymbol{h}_{i}^{s}, \boldsymbol{h}_{j}^{t}\right)\right), \mathcal{P}_{\lambda}\left(\boldsymbol{y}_{i}^{s}, \boldsymbol{y}_{j}^{t}\right)\right) p(\lambda) \mathrm{d} \lambda\right\},
\end{aligned}
\end{equation}
\end{small}}
where $\ell$ is CE loss, $\boldsymbol{h}_{i}^{s} = f(\boldsymbol{s}^{s}_{i})$ and $\boldsymbol{h}_{j}^{t} = f(\boldsymbol{x}^{t}_{j})$. Note $\mathcal{H}^{s}$ and $\mathcal{H}^{t}$ denote the representation spaces with dimensionality $\operatorname{dim}(\mathcal{H})$ for source and target domains, respectively. $\text {Let} f^{\star} \in \mathcal{F}, c^{\star} \in \mathcal{C}, \text{ and } {\mathcal{P}_{\lambda}}^{\star} \in \mathcal{P}$ be minimizers of Eq.~\ref{eq:eq1}.
\begin{theorem}
\label{theorem:Distribution estimation1}
(Domain Distribution Alignment with PatchMix): 
Let $d \in \mathbb{N}$ to represent the number of classes contained in three sets $\mathcal D_{s}$, $\mathcal D_{t}$, and $\mathcal D_{i}$. If $\operatorname{dim}(\mathcal{H}) \geq d-1$, ${\mathcal{P}_{\lambda}}’ \ell(c^{\star}(f^{\star}(\boldsymbol{x}_{i})),\boldsymbol{y}^{s})+ (1-{\mathcal{P}_{\lambda}}')\ell(c^{\star}(f^{\star}(\boldsymbol{x}_{i})),\boldsymbol{y}^{t})=0$, then $D\left(P_{S},P_{T}\right)=0$ and the corresponding minimizer $c^{\star}$ is a linear function from $\mathcal{H}$ to $\mathbb{R}^{d}$. Denote the scaled mixup ratio sampled from a learnable Beta distribution as $\mathcal{P}_{\lambda}’$.
\end{theorem}
Theorem.~\ref{theorem:Distribution estimation1} indicates that \textbf{the source and target domains are aligned if mixing the patches from two domains is equivalent to mixing the corresponding labels}. Therefore, minimizing the CE between the mixed patches and mixed labels can effectively facilitate domain alignment. \textit{For the proof of Theorem.~\ref{theorem:Distribution estimation1}, refer to the suppl. material.}


\vspace{-10pt}
\subsubsection{A Min-Max CE Game}
We interpret UDA as a min-max CE game among three players, namely the feature extractor ($\mathcal{F}$), classifier ($\mathcal{C}$), and PatchMix module ($\mathcal{P}$), as shown in Fig. \ref{fig:framework}. 
To specify each player's role, we define $\boldsymbol{\omega}_{\mathcal{F}} \in \Omega_{\mathcal{F}}$, $\boldsymbol{\omega}_{\mathcal{C}} \in \Omega_{\mathcal{C}}$, and $\boldsymbol{\omega}_{\mathcal{P}} \in \Omega_{\mathcal{P}}$ as the parameters of $\mathcal F$, $\mathcal C$, and $\mathcal P$, respectively. The joint domain is defined as $\Omega = \Omega_{\mathcal{F}} \times \Omega_{\mathcal{C}} \times \Omega_{\mathcal{P}}$ and their joint parameter set is defined as $\boldsymbol{\omega}=\{\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}}, \boldsymbol{\omega}_{\mathcal{P}}\}$. 
Then we use the subscript $_{-m}$ to denote all other parameters/players except $m$, \eg, $\boldsymbol{\omega}_{-{\mathcal{C}}} = \{\boldsymbol{\omega}_{\mathcal{F}},\boldsymbol{\omega}_{\mathcal{P}}\}$. In our game, $m$-th player is endowed with a cost function $J_{m}$ and strives to reduce its cost, which contributes to the change of CE. Each player' cost function $J_{m}$ is represented as
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\small
\begin{split}\label{eq:objective1}
J_{{\mathcal{F}}}\left(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{-{\mathcal{F}}}\right) &:=\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha {\text{CE}}_{s,i, t}(\boldsymbol{\omega}),\\ 
J_{{\mathcal{C}}}\left(\boldsymbol{\omega}_{\mathcal{C}}, \boldsymbol{\omega}_{-{\mathcal{C}}}\right) &:=\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha {\text{CE}}_{s,i, t}(\boldsymbol{\omega}),\\ 
J_{{\mathcal{P}}}\left(\boldsymbol{\omega}_{{\mathcal{P}}}, \boldsymbol{\omega}_{-{\mathcal{P}}}\right) &:=-\alpha {\text{CE}}_{s,i, t}(\boldsymbol{\omega}),    
\end{split}
\end{equation}}
where $\alpha$ is the trade-off parameter, $\ell$ is the supervised classification loss for the source domain, and $\text{CE}_{s,i,t}(\boldsymbol{\omega})$ is the discrepancy between the intermediate domain and the source/target domain. The definitions of $\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})$ and $\text{CE}_{s,i,t}(\boldsymbol{\omega})$ are shown in Sec.~\ref{PMTransframework}. As illustrated in Eq.~\ref{eq:objective1}, the game is essentially a min-max process,~\ie, a competition for the player $\mathcal P$ against both players $\mathcal F$ and $\mathcal C$. Specifically, as depicted in Fig.~\ref{fig:framework}, $\mathcal P$ strives to diverge while $\mathcal{F}$ and $\mathcal C$ try to align domain distributions, which is a min-max process on CE. In this min-max CE game, each player behaves selfishly to reduce its cost function, and this competition will possibly end with a situation where no one has anything to gain by changing only one's strategy. This situation is called Nash Equilibrium (NE) in game theory. 



\begin{definition}
\label{Def:NE}
(Nash Equilibrium): The equilibrium states each player's strategy is the best response to other players. And a point ${\omega}^*\in \Omega$ is Nash Equilibrium if 
\begin{small}
\begin{equation}
    \begin{aligned}
    \forall \boldsymbol{\omega}_{m} \in {\Omega}_{i}, \forall m \in \{\mathcal{F}, \mathcal{C}, \mathcal{P}\}, s.t. J_m(\boldsymbol{\omega}_m^*, \boldsymbol{\omega}_{-m}^*) \leq J_m(\boldsymbol{\omega}_m, \boldsymbol{\omega}_{-m}^*). \nonumber
    \end{aligned}
\end{equation}
\end{small}
\end{definition}
Intuitively, in our case, NE means that no player has the incentive to change its own parameters, as there is no additional pay-off.








\subsection{The Proposed Framework}
\label{PMTransframework}
\noindent \textbf{Overview.}
Fig.~\ref{fig:framework} illustrates the framework of our proposed PMTrans, which consists of a ViT encoder, a classifier, and a PatchMix module. PatchMix module is utilized to maximize the CE between the intermediate domain and source/target domain, conversely, two semi-supervised mixup losses in the feature and label spaces are proposed to minimize CE. Finally, a three-player game containing feature extractor, classifier, and PatchMix module, minimizes and maximizes the CE for aligning distributions between the source and target domains.  

\noindent \textbf{PatchMix}.
As shown in Fig.~\ref{fig:framework}, the PatchMix module is proposed to construct the intermediate domain, buttressed by Definition.~\ref{Def: Mixup}. In detail, the patch embedding layer in PatchMix transforms input images from source/target domains into patches. And the ViT encoder aims to extract features from the patch sequences. The classifier maps the outputs of ViT encoder to make predictions, each of which is exploited to select the feature map to re-weight the patch sequences. The PatchMix with a learnable Beta distribution aims to maximize the CE between the intermediate and source/target domain, and is presented as follows.

When exploiting PatchMix to construct the intermediate domain, it is worth noting that not all patches have equal contributions for the label assignment. As Chen \etal\cite{DBLP:journals/corr/abs-2111-09833} observed, the mixed image has no valid objects due to the random process while there is still a response in the label space. 
To remedy this issue, we re-weight $\mathcal{P}_\lambda(\boldsymbol{y}^s, \boldsymbol{y}^t)$ in Definition. \ref{Def: Mixup} with the normalized attention score $a_{k}$. For the implementation details of attention scores, refer to \textit{the suppl. material}. The re-scaled $\mathcal{P}_\lambda(\boldsymbol{y}^s, \boldsymbol{y}^t)$ is defined as 
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
    \begin{aligned}
    \mathcal{P}_\lambda(\boldsymbol{y}^s, \boldsymbol{y}^t) = \lambda^s\boldsymbol{y}^s + \lambda^t\boldsymbol{y}^t, \nonumber
    \end{aligned}
\end{equation} }
where
{\setlength\abovedisplayskip{5pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
\small
    \begin{aligned}
\lambda^s &= \frac{\sum_{k=1}^{n} \lambda_{k} {a}_{k}^{s}}{\sum_{k=1}^{n} \lambda_{k} {a}_{k}^{s}+\sum_{k=1}^{n}(1-\lambda_{k}){a}_{k}^{t}}
, \\
\lambda^t &= \frac{\sum_{k=1}^{n}(1-\lambda_{k}) {a}_{k}^{t}}{\sum_{k=1}^{n} \lambda_{k} {a}_{k}^{s}+\sum_{k=1}^{n}(1-\lambda_{k}){a}_{k}^{t}}.\nonumber
    \end{aligned}
\end{equation}}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{y_labels.pdf}
    \captionsetup{font=small}
    \vspace{-15pt}
    \caption{(a) The illustration of two proposed semi-supervised losses. (b) Label similarity $\boldsymbol{y}^{is}$ and $\boldsymbol{y}^{it}$. Better viewed in color.}
    \label{fig:align-feature}
    \vspace{-15pt}
\end{figure}
\noindent\textbf{Semi-supervised mixup loss}. 
As PatchMix tries to maximize the CE between the intermediate domain and source/target domain, we now need to find a way to minimize the CE in the game. Intuitively, we propose two semi-supervised mixup losses in the feature and label spaces to minimize the discrepancy between features of mixing patches and corresponding mixing labels based on Theorem.~\ref{theorem:Distribution estimation1}. The objective of the proposed PMTrans is show in Fig. \ref{fig:align-feature} \textcolor{red}{(a)} and consists of a classification loss of source data and two semi-supervised mixup losses.

\textbf{ 1) Label space:} As introduced in Theorem.~\ref{theorem:Distribution estimation1}, we apply a supervised mixup loss in the label space to measure the domain divergence based on the CE loss between the mixing logits and corresponding mixing labels (See \textcolor{green(pigment)}{green arrow} in Fig. \ref{fig:align-feature} \textcolor{red}{(a)}). 
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
\small
    \begin{aligned}
 \mathcal{L}_{l}^{I,S}(\boldsymbol{\omega}) &= \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{s} \ell\left(\mathcal{C}\left(\mathcal {F}\left(\boldsymbol{x}^{i}\right)\right), \boldsymbol{y}^{s}\right),\\
\mathcal{L}_{l}^{I,T}(\boldsymbol{\omega}) &= \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{t} \ell\left(\mathcal{C}\left(\mathcal {F}\left(\boldsymbol{x}^{i}\right)\right), \hat{\boldsymbol{y}^{t}}\right), \nonumber
    \end{aligned}
\end{equation}}

where $\hat{\boldsymbol{y}^{t}}$ is the pseudo label for target data. For convenience, we utilize the method, commonly used in~\cite{LiangHF20, LiangHF21}, to generate pseudo labels $ \hat{\boldsymbol{y}^{t}}$ for samples via $k$-means cluster. 

\textbf{2) Feature space:} Nonetheless, the supervised loss alone in the label space is
not sufficient to diminish the domain divergence due to the less reliable pseudo labels of the target data. 
Therefore, we further  \textbf{propose to minimize the discrepancy between the similarity of the features and the similarity of labels in the feature space} for aligning the intermediate and source/target domain without the supervised information of the target domain. The experimental results in Tab.~\ref{tab:semi1} validate its effectiveness.

Specifically, we first compute the cosine similarity between the intermediate domain and source/target domain in the feature space. The feature similarity is defined as
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
    d(\boldsymbol{x}^{i}, \boldsymbol{x}^{s}) = cos(\mathcal {F}\left(\boldsymbol{x}^{i}\right), \mathcal {F}\left(\boldsymbol{x}^{s}\right)), \nonumber
\end{equation}}
where $cos$ denotes the cosine similarity. 
As shown in Fig.~\ref{fig:align-feature}\textcolor{red}{(b)}, for the source domain, we exploit the ground-truth to calculate the label similarity, $y^{is} =y^s({y^s})^{\intercal}$, as a binary matrix to represent whether samples share the same labels. For example, the intermediate image is constructed by sampling patches from the source image,~\eg, \textit{clock}; therefore, the label similarity is set as 1 if it is calculated between the intermediate image and the source class \textit{`clock'}, otherwise, it is set as 0.
Then, we utilize the CE to measure the domain discrepancy based on the difference between the feature similarity and label similarity. The supervised mixup loss in the feature space (See \textcolor{red}{red arrow} in Fig. \ref{fig:align-feature} \textcolor{red}{(a)}) is formulated as
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{small}
\begin{equation}
    \begin{aligned}
    \mathcal{L}_{f}^{I,S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}}) = \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{s}\ell\left(d(\boldsymbol{x}^{i},\boldsymbol{x}^{s}), \boldsymbol{y}^{is}\right). \nonumber
    \end{aligned}
\end{equation}
\end{small}
}
Moreover, for the intermediate and target domains, due to lack of supervision, we utilize identity matrix $y^{it}$ as the label similarity. For example, in Fig.~\ref{fig:align-feature} \textcolor{red}{(b)}, as the intermediate image is built by sampling patches from the target image, \eg, bottle; therefore, the label similarity between the intermediate image and the corresponding target image is set as 1 and vice versa. 
To measure the divergence between the intermediate and target domains in the feature space, we propose an unsupervised mixup loss as
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{small}
\begin{equation}
\small
    \begin{aligned}
   \mathcal{L}_{f}^{I,T}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}}) = \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{t}\ell\left(d(\boldsymbol{x}^{i}, \boldsymbol{x}^{t}), \boldsymbol{y}^{it}\right), \nonumber
    \end{aligned}
\end{equation}
\end{small}}

Finally, the two semi-supervised mixup losses in the feature and label spaces are formulated as
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
\small
    \begin{aligned}
 \mathcal{L}_{f}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}}) &= \mathcal{L}_{f}^{I,S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{{\mathcal{P}}}) + \mathcal{L}_{f}^{I,T}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}}),\\
\mathcal{L}_{l}(\boldsymbol{\omega}) &= \mathcal{L}_{l}^{I,S}(\boldsymbol{\omega}) + \mathcal{L}_{l}^{I,T}(\boldsymbol{\omega}). \nonumber
    \end{aligned}
\end{equation}}
Moreover, the classification loss is applied to the labeled source domain data (See \textcolor{blue_light}{blue arrow} in Fig. \ref{fig:align-feature} \textcolor{red}{(a)}) and is formulated as 
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
\small
    \begin{aligned}
\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})=\mathbb{E}_{\left(\boldsymbol{x}^{s}, \boldsymbol{y}^{s}\right) \sim D^{s}} \ell\left(\mathcal{C}\left(\mathcal{F}\left(\boldsymbol{x}^{s}\right)\right), \boldsymbol{y}^{s}\right).\nonumber
    \end{aligned}
\end{equation}}
\begin{table*}[t]
\centering
\resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{c|l|llllllllllllll}
\toprule
Method &&A$\to$ C& A$\to$ P & A $\to$ R & C $\to$ A &C $\to$ P &C $\to$ R &P$\to$ A& P$\to$ C & P$ \to$ R & R$ \to$ A &R$ \to$ C &R$ \to$ P& Avg     \\
\hline
\hline
ResNet-50 &\multirow{5}{*}{\rotatebox{90}{ResNet}}& 44.9& 66.3 &74.3 &51.8& 61.9 &63.6& 52.4 &39.1& 71.2& 63.8& 45.9& 77.2 &\colorbox{lightgray}{59.4}   \\
MCD &&48.9& 68.3& 74.6& 61.3& 67.6& 68.8& 57.0& 47.1 &75.1& 69.1& 52.2& 79.6& \colorbox{lightgray}{64.1}\\
MDD &&54.9& 73.7& 77.8& 60.0& 71.4& 71.8& 61.2& 53.6& 78.1& 72.5& 60.2& 82.3& \colorbox{lightgray}{68.1}\\
BNM&&56.7 &77.5& 81.0& 67.3& 76.3& 77.1& 65.3& 55.1& 82.0 &73.6 &57.0& 84.3& \colorbox{lightgray}{71.1}\\
FixBi && 58.1& 77.3& 80.4 &67.7& 79.5& 78.1& 65.8& 57.9& 81.7& 76.4& 62.9& 86.7& \colorbox{lightgray}{72.7}  \\
\midrule
TVT & \multirow{7}{*}{\rotatebox{90}{ViT}}&   74.9&86.8&89.5&82.8&88.0&88.3&79.8&71.9&90.1&85.5&74.6&90.6&\colorbox{lightgray}{83.6}\\
Deit-based&&61.8 &79.5& 84.3& 75.4 &78.8& 81.2& 72.8 &55.7& 84.4& 78.3& 59.3& 86.0 &\colorbox{lightgray}{74.8} \\
CDTrans-Deit&  & 68.8&85.0&86.9&81.5&87.1&87.3&79.6&63.3&88.2&82.0&66.0&90.6&\colorbox{lightgray}{80.5}   \\
\textbf{PMTrans-Deit} &&71.8& 87.3 &88.3 &83.0 &87.7 & 87.8 &78.5 &67.4 &89.3 &81.7 &70.7 & 92.0 &\colorbox{lightgray}{82.1}\\
ViT-based &&67.0 &85.7 &88.1 &80.1 &84.1 &86.7 &79.5 &67.0 &89.4 &83.6 &70.2 &91.2 & \colorbox{lightgray}{81.1}\\
SSRT-ViT&&75.2& 89.0& 91.1& 85.1 &88.3& 89.9& 85.0& 74.2 &91.2& 85.7& 78.6 &91.8& \colorbox{lightgray}{85.4}\\
\textbf{PMTrans-ViT} &&81.2&91.6&92.4&\textbf{88.9}&91.6&93.0&\textbf{88.5}&80.0&\textbf{93.4}&\textbf{89.5}&\textbf{82.4}&94.5&\colorbox{lightgray}{88.9}\\
\midrule
Swin-based&\multirow{2}{*}{\rotatebox{90}{Swin}}&72.7   &87.1   &90.6   &84.3   &87.3   &89.3   &80.6 &68.6 &90.3&84.8&69.4&91.3& \colorbox{lightgray}{83.6}  \\
\textbf{PMTrans-Swin} && \textbf{81.3}&\textbf{92.9}&\textbf{92.8}&88.4&\textbf{93.4}&\textbf{93.2}&87.9&\textbf{80.4}&93.0&89.0&80.9&\textbf{94.8}&\colorbox{lightgray}{\textbf{89.0}}\\
\bottomrule
\end{tabular}}
\vspace{-8pt}
\caption{Comparison with SoTA methods on Office-Home. The best performance is marked as \textbf{bold}.}
\captionsetup{font=small}
\label{tab:officeHome_Res}
\vspace{-10pt}
\end{table*}
\begin{table}[t]
\centering
\setlength{\tabcolsep}{1.5mm}
\captionsetup{font=small}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|l|lllllll}
\toprule
Method  &&A $\to$ W & D$\to$ W & W$\to$ D & A$\to$ D &D$\to$ A &W$\to$ A & \colorbox{lightgray}{Avg}  \\
\hline
\hline
ResNet-50 &\multirow{5}{*}{\rotatebox{90}{ResNet}} &  68.9& 68.4 &62.5& 96.7& 60.7& 99.3& \colorbox{lightgray}{76.1}   \\
BNM &&91.5&98.5&\textbf{100.0}&90.3&70.9&71.6&\colorbox{lightgray}{87.1}\\
MDD&&94.5& 98.4& \textbf{100.0}& 93.5& 74.6 &72.2& \colorbox{lightgray}{88.9}\\
SCDA&&94.2 &98.7& 99.8& 95.2& 75.7& 76.2& \colorbox{lightgray}{90.0}\\
FixBi &  &  96.1& 99.3& \textbf{100.0}& 95.0&78.7& 79.4& \colorbox{lightgray}{91.4}  \\ 
\midrule
TVT &\multirow{7}{*}{\rotatebox{90}{ViT}}&96.4& 99.4& \textbf{100.0}& 96.4& 84.9& 86.0&\colorbox{lightgray}{93.9}   \\
Deit-based &     &89.2& 98.9& \textbf{100.0}& 88.7& 80.1& 79.8& \colorbox{lightgray}{89.5}\\
CDTrans-Deit& & 96.7 &99.0& \textbf{100.0}& 97.0& 81.1& 81.9 &\colorbox{lightgray}{92.6}  \\
\textbf{PMTrans-Deit}&&99.0&99.4&\textbf{100.0}&96.5&81.4&82.1&\colorbox{lightgray}{93.1}\\
ViT-based       &&91.2& 99.2& \textbf{100.0}& 90.4& 81.1& 80.6& \colorbox{lightgray}{91.1}  \\
SSRT-ViT&&97.7& 99.2 &\textbf{100.0} &98.6& 83.5& 82.2& \colorbox{lightgray}{93.5}\\
\textbf{PMTrans-ViT}&& 99.1&\textbf{99.6}&\textbf{100.0}&99.4&85.7&86.3&\colorbox{lightgray}{95.0}\\
\midrule
Swin-based &\multirow{2}{*}{\rotatebox{90}{Swin}}&97.0   &99.2   &\textbf{100.0}   &95.8   &82.4   &81.8   &\colorbox{lightgray}{92.7}   \\
\textbf{PMTrans-Swin} & &\textbf{99.5}   &99.4  &\textbf{100.0}   &\textbf{99.8}   &\textbf{86.7}   &\textbf{86.5}   &\colorbox{lightgray}{\textbf{95.3}}  \\ 
\bottomrule
\end{tabular}}
\vspace{-8pt}
\caption{Comparison with SoTA methods on Office-31. The best performance is marked as \textbf{bold}.}
\label{tab:office31_res}
\vspace{-10pt}
\end{table}
\noindent\textbf{A Three-Player Game}.
Finally, the min-max CE game aims to align distributions in the feature and label spaces. The total CE between the intermediate domain and source/target domain is 
{\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{equation}
{\text{CE}}_{s,i,t}(\boldsymbol{\omega}) =  \mathcal{L}_{f}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}})+\mathcal{L}_{l}(\boldsymbol{\omega}).  \nonumber  
\end{equation}}

We adopt the \textit{random} mixup-ratio from a learnable Beta distribution in our PatchMix module to maximize the CE between the intermediate domain and source/target domain. Moreover, the feature extractor and classifier have the same objective to minimize the CE between the intermediate domain and source/target domain.
Therefore, the total objective of PMTrans is achieved by reformulating Eq.~\ref{eq:objective1} as
\[
J\left(\boldsymbol{\omega}\right) :=\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha {\text{CE}}_{s,i,t}(\boldsymbol{\omega}),
\]
where $\alpha$ is trade-off parameter. 
After optimizing the objective, the PatchMix module with the ideal Beta distribution
will not maximize the CE anymore. Meanwhile, the feature extractor and classifier have no incentive to change their parameters to minimize the CE. Finally, the discrepancy between the intermediate domain and  source/target domain is nearly zero, further indicating that the source and target domains are well aligned.
\section{Experiments}
\subsection{Datasets and implementation}
\noindent\textbf{Datasets.} To evaluate the proposed method, we conduct experiments on four popular UDA benchmarks, including Office-Home \cite{VenkateswaraECP17}, Office-31 \cite{SaenkoKFD10}, VisDA-2017 \cite{abs-1710-06924}, and DomainNet \cite{PengBXHSW19}.
\textit{The details of the datasets and transfer tasks on these datasets can be found in the suppl. material}.

\noindent\textbf{Implementation.} In all experiments, we use the Swin-based transformer~\cite{Liu2021SwinTH}  pre-trained on ImageNet \cite{DengDSLL009} as the backbone for our PMTrans. The base learning rate is $5e^{-6}$ with a batch size of 32, and we train models by 50 epochs. For VisDA-2017, we use a lower learning rate $1e^{-6}$. We adopt AdamW \cite{LoshchilovH19} with a momentum of $0.9$, and a weight decay of $0.05$ as the optimizer. Furthermore, for fine-tuning purposes, we set the classifier (MLP) with a higher learning rate $1e^{-5}$ for our main tasks and learn the trade-off parameter adaptively. For a fair comparison with prior works, we also conduct experiments with the same backbone \textbf{Deit-based} \cite{DBLP:journals/corr/abs-2012-12877} as CDTrans \cite{abs-2109-06165}, and \textbf{ViT-based} \cite{Dosovitskiy2021AnII} as SSRT \cite{abs-2204-07683} on Office-31, Office-Home, and VisDA-2017. These two studies are trained for 60 and 100 epochs separately. 




\subsection{Results}

We compare PMTrans with the SoTA methods, including ResNet-based and ViT-based methods. The ResNet-based methods are FixBi \cite{NaJCH21}, MCD \cite{SaitoWUH18}, SWD \cite{LeeBBU19}, SCDA \cite{0008XLLLQL21}, BNM \cite{CuiWZLH020}, and MDD \cite{0002LLJ19}. The ViT-based methods are SSRT \cite{abs-2204-07683}, CDTrans \cite{abs-2109-06165}, and TVT \cite{abs-2108-05988}. 

 For the ResNet-based methods, we utilize ResNet-50 as the backbone for the Office-Home, Office-31, and DomainNet datasets, and we adopt ResNet-101 for VisDA-2017 dataset. Note that each backbone is trained with the source data only and then tested with the target data. 


\begin{table*}[t]
\centering
\captionsetup{font=small}
\resizebox{0.85\linewidth}{!}{ 
\begin{tabular}{c|l|lllllllllllll}
\toprule
Method &&plane& bcycl& bus &car& horse& knife& mcycl& person& plant &sktbrd &train& truck& Avg     \\
\hline
\hline
ResNet-50&\multirow{7}{*}{\rotatebox{90}{ResNet}}  & 55.1 &53.3& 61.9& 59.1& 80.6 &17.9 &79.7& 31.2& 81.0 &26.5 &73.5& 8.5& \colorbox{lightgray}{52.4} \\
BNM &&89.6 &61.5 &76.9 &55.0& 89.3& 69.1 &81.3 &65.5& 90.0 &47.3& 89.1 &30.1& \colorbox{lightgray}{70.4}\\
MCD& &  87.0 &60.9 &83.7& 64.0& 88.9& 79.6& 84.7& 76.9& 88.6& 40.3& 83.0& 25.8 &\colorbox{lightgray}{71.9}\\
SWD&&90.8 &82.5 &81.7 &70.5& 91.7 &69.5& 86.3& 77.5 &87.4& 63.6& 85.6 &29.2 &\colorbox{lightgray}{76.4}\\
FixBi && 96.1 &87.8& 90.5& 90.3 &96.8& 95.3 &92.8& 88.7& 97.2 &94.2& 90.9 &25.7& \colorbox{lightgray}{87.2}  \\ 
\midrule
TVT&\multirow{7}{*}{\rotatebox{90}{ViT}}& 82.9 &85.6 &77.5 &60.5& 93.6& 98.2& 89.4& 76.4& 93.6& 92.0& 91.7& 55.7&\colorbox{lightgray}{83.1}\\
Deit-based & &  98.2 &73.0 &82.5& 62.0 &97.3& 63.5& 96.5& 29.8&68.7 &86.7 &96.7&23.6&\colorbox{lightgray}{73.2} \\
CDTrans-Deit&&97.1& 90.5& 82.4& 77.5& 96.6&96.1 &93.6 &\textbf{88.6}&\textbf{97.9}&86.9 &90.3&\textbf{62.8}&\colorbox{lightgray}{88.4}\\
\textbf{PMTrans-Deit}&&98.2& 92.2 &88.1 &77.0 &97.4&95.8 &94.0&72.1 &97.1 &95.2 &94.6&51.0 &\colorbox{lightgray}{87.7}\\
ViT-based & &99.1& 60.7& 70.1& 82.7& 96.5 &73.1& 97.1 &19.7 &64.5& 94.7 &97.2& 15.4&\colorbox{lightgray}{72.6}\\
SSRT-ViT&&98.9& 87.6 &\textbf{89.1}&\textbf{84.8}& 98.3& \textbf{98.7}& 96.3&81.1& 94.8 &97.9& 94.5 &43.1& \colorbox{lightgray}{\textbf{88.8}}\\
\textbf{PMTrans-ViT}&& 98.9 & \textbf{93.7} &84.5 &73.3 &\textbf{99.0} &98.0 &96.2 &67.8 &94.2 &\textbf{98.4} &96.6 &49.0 & \colorbox{lightgray}{87.5}\\
\midrule
Swin-based &\multirow{2}{*}{\rotatebox{90}{Swin}}        &99.3   &  63.4 &  85.9 & 68.9  &  95.1 & 79.6  &\textbf{ 97.1}&29.0&81.4&94.2&\textbf{97.7}&29.6&\colorbox{lightgray}{76.8}  \\
\textbf{PMTrans-Swin}&& \textbf{99.4}                        & 88.3  & 88.1  &78.9   & 98.8  &98.3 &95.8&70.3   &94.6&98.3&96.3&48.5& \colorbox{lightgray}{88.0}  \\ 
\bottomrule
\end{tabular}}
\vspace{-8pt}
\caption{Comparison with SoTA methods on VisDA-2017. The best performance is marked as \textbf{bold}.}
\label{tab:VisDA1}
\end{table*}
\begin{table*}
\centering
\vspace{-5pt}
\captionsetup{font=small}
\resizebox{\linewidth}{!}{ 
\begin{tabular}{c|lllllll||c|lllllll||c|llllllll}
\toprule
MCD                 & clp& inf& pnt& qdr& rel& skt& Avg &SWD & clp& inf& pnt& qdr& rel& skt& Avg&BNM & clp& inf& pnt& qdr& rel& skt& Avg  \\
\hline
\hline
clp                      &  - &15.4 &25.5& 3.3& 44.6& 31.2 &24.0  &clp&  -& 14.7& 31.9& 10.1& 45.3& 36.5& 27.7&clp                      &  -& 12.1 &33.1& 6.2& 50.8 &40.2& 28.5  \\
inf                     &  24.1& -& 24.0 &1.6 &35.2& 19.7& 20.9 &inf &   22.9& -& 24.2& 2.5& 33.2& 21.3& 20.0&inf                     &   26.6 &- &28.5& 2.4 &38.5& 18.1& 22.8\\
pnt                      & 31.1& 14.8& -& 1.7& 48.1& 22.8& 23.7& pnt &  33.6 &15.3 &- &4.4 &46.1& 30.7 &26.0&pnt                      &  39.9& 12.2& - &3.4& 54.5& 36.2 &29.2\\
qdr                      &  8.5& 2.1 &4.6& -& 7.9& 7.1& 6.0& qdr &  15.5 &2.2& 6.4& -& 11.1& 10.2 &9.1&qdr                      &  17.8 &1.0 &3.6& - &9.2& 8.3& 8.0\\
rel                      &  39.4& 17.8& 41.2 &1.5& -& 25.2& 25.0&real& 41.2& 18.1& 44.2& 4.6& -& 31.6& 27.9 &rel                      & 48.6 &13.2& 49.7& 3.6& - &33.9& 29.8 \\
skt                     &   37.3& 12.6& 27.2 &4.1 &34.5& - &23.1& skt & 44.2& 15.2&37.3 &10.3& 44.7& -& 30.3 &skt                     &   54.9 &12.8 &42.3 &5.4 &51.3& - &33.3  \\ 
Avg                      &  28.1 &12.5& 24.5& 2.4& 34.1& 21.2 &\colorbox{lightgray}{20.5}& Avg & 31.5& 13.1& 28.8 &6.4& 36.1& 26.1& \colorbox{lightgray}{23.6}&Avg                      & 37.6& 10.3& 31.4& 4.2& 40.9& 27.3 & \colorbox{lightgray}{25.3} \\
\hline
\hline
CGDM & clp& inf& pnt& qdr& rel& skt& Avg&MDD& clp& inf& pnt& qdr& rel& skt& Avg&SCDA & clp& inf& pnt& qdr& rel& skt& Avg  \\
\hline
\hline
clp& -& 16.9& 35.3& 10.8& 53.5 &36.9& 30.7 & clp& -& 20.5& 40.7& 6.2& 52.5& 42.1& 32.4&clp& -& 18.6& 39.3& 5.1& 55.0& 44.1& 32.4\\
inf &27.8 &- &28.2& 4.4 &48.2& 22.5& 26.2& inf& 33.0& - &33.8& 2.6 &46.2 &24.5 &28.0&inf &29.6& - &34.0 &1.4 &46.3& 25.4& 27.3 \\ 
pnt& 37.7& 14.5& - &4.6& 59.4& 33.5& 30.0  &pnt &43.7& 20.4& - &2.8& 51.2 &41.7& 32.0&pnt& 44.1& 19.0& -& 2.6& 56.2& 42.0& 32.8 \\
qdr& 14.9 &1.5& 6.2& - &10.9& 10.2& 8.7  &qdr& 18.4& 3.0& 8.1 &- &12.9 &11.8& 10.8&qdr& 30.0 &4.9 &15.0 &- &25.4 &19.8 &19.0\\
rel &49.4& 20.8& 47.2& 4.8& - &38.2& 32.0 & rel&52.8& 21.6& 47.8& 4.2 &-& 41.2& 33.5&rel &54.0 &22.5& 51.9 &2.3& - &42.5& 34.6\\
skt& 50.1&16.5& 43.7& 11.1& 55.6& - &35.4 & skt& 54.3 &17.5 &43.1 &5.7 &54.2& - &35.0&skt& 55.6 &18.5 &44.7 &6.4 &53.2 &- &35.7\\ 
Avg& 36.0& 14.0& 32.1& 7.1 &45.5& 28.3& \colorbox{lightgray}{27.2} &Avg& 40.4 &16.6& 34.7& 4.3& 43.4 &32.3& \colorbox{lightgray}{28.6}&Avg& 42.6& 16.7& 37.0& 3.6 &47.2& 34.8& \colorbox{lightgray}{30.3}\\
\hline
\hline
CDTrans& clp& inf& pnt& qdr& rel& skt& Avg&SSRT& clp& inf& pnt& qdr& rel& skt& Avg&\textbf{PMTrans}& clp& inf& pnt& qdr& rel& skt& Avg\\
\hline
\hline
clp  &  - &29.4& 57.2& 26.0& 72.6& 58.1 &48.7&clp&-& 33.8 &60.2&19.4& 75.8 &59.8 &49.8&    clp & - & 34.2 &62.7 &32.5 &79.3 &63.7 & 54.5  \\
inf  &   57.0 &- &54.4 &12.8& 69.5& 48.4 &48.4&inf&55.5 &- &54.0 &9.0 &68.2 &44.7& 46.3    &inf& 67.4&- &61.1 & 22.2 & 78.0 & 57.6&57.3 \\ 
pnt  &  62.9& 27.4& -& 15.8& 72.1& 53.9& 46.4&pnt&61.7& 28.5& -& 8.4& 71.4& 55.2& 45.0&     pnt&69.7&33.5&-&23.9&79.8&61.2&53.6 \\
qdr &  44.6& 8.9 &29.0 &- &42.6 &28.5 &30.7&qdr&42.5 &8.8 &24.2& - &37.6 &33.6 &29.3 &      qdr&54.6&17.4&38.9&-&49.5  &41.0&40.3\\
rel &  66.2& 31.0& 61.5& 16.2& -& 52.9& 45.6&rel&69.9 &37.1& 66.0& 10.1& - &58.9& 48.4&       rel&74.1&35.3&70.0&25.4&-&61.1&53.2 \\
skt  &  69.0& 29.6 &59.0& 27.2 &72.5& -& 51.5&skt&70.6 &32.8 &62.2 &21.7 &73.2 &- &52.1&       skt&73.8&33.0&62.6&30.9&77.5&-& 55.6\\ 
Avg  & 59.9 &25.3 &52.2& 19.6& 65.9 &48.4& \colorbox{lightgray}{45.2}&Avg&60.0& 28.2& 53.3& 13.7& 65.3 &50.4& \colorbox{lightgray}{45.2}&     Avg&67.9&30.7&59.1&27.0&72.8&56.9&\colorbox{lightgray}{\textbf{52.4}}  \\
\bottomrule
\end{tabular}}
\vspace{-8pt}
\caption{Comparison with SoTA methods on DomainNet. The best performance is marked as \textbf{bold}.}
\vspace{-10pt}
\label{tab:domainnet}
\end{table*}



\noindent\textbf{Results on Office-Home.} Tab.~\ref{tab:officeHome_Res} shows the quantitative results of methods using different backbones. As expected, our PMTrans framework achieves noticeable performance gains and surpasses TVT, SSRT, and CDTrans by a large margin. Importantly, our PMTrans achieves an improvement more than \textbf{5.4\%} accuracy over the Swin backbone and yields \textbf{89.0\%} accuracy. Interestingly, our proposed PMTrans can decrease the domain divergence effectively with Deit-based and ViT-based backbones. The results indicate that our method can obtain more robust transferable representations than the CNN-based and ViT-based methods.

\noindent\textbf{Results on Office-31.}  Tab.~\ref{tab:office31_res} shows the quantitative comparison with the CNN-based and ViT-based methods.
Overall, our PMTrans achieves the best performance on each task with \textbf{95.3\%} accuracy and outperforms the SoTA methods with identical backbones. Numerically, PMTrans noticeably surpasses the SoTA methods with an increase of \textbf{+1.4\%} accuracy over TVT, \textbf{+2.7\%} accuracy over CDTrans, and \textbf{+1.8\%} accuracy over SSRT, respectively. 

\noindent\textbf{Results on VisDA-2017.} 
As shown in Tab.~\ref{tab:VisDA1}, our PMTrans achieves \textbf{88.0\%} accuracy and outperforms the baseline by \textbf{11.2\%}. In particular, for the `hard' categories, such as "person", our method consistently achieves a much higher performance boost from \textbf{29.0\%} to \textbf{70.3\%}. These improvements indicate that our method shows an excellent generalization capability and achieves comparable performance (\textbf{88.0\%}) with the SoTA methods (\textbf{88.7\%}). PMTrans also surpasses the SoTA methods on several sub-categories, such as "horse" and "sktbrd". 
In particular, it is shown that the SoTA methods,~\eg, CDTrans and SSRT, achieve better results on this dataset. The reason is that CDTrans and SSRT are trained with a batch size of 64 while PMTrans's batch size is 32. It indicates that when the input size is much bigger, the input can represent the data distributions better. \textit{A detailed ablation study for this issue can be found in the suppl. material.}. 


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{feature_visual.pdf}
    \vspace{-8pt}
    \caption{t-SNE visualizations for task A$\rightarrow$C on the Office-Home dataset. Source and target instances are shown in blue and red, respectively.}
    \label{fig:visual}
    \vspace{-5pt}
\end{figure*}
\begin{table*}[t]
\centering
\resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{ccc|ccllllllllllllll}
\toprule
$\mathcal{L}_{cls}^{S}$&$\mathcal{L}_{f}$&$\mathcal{L}_{l}$&A$\to$ C& A$\to$ P & A $\to$ R & C $\to$ A &C $\to$ P &C $\to$ R &P$\to$ A& P$\to$ C & P$ \to$ R & R$ \to$ A &R$ \to$ C &R$ \to$ P& Avg     \\
\hline
\hline
\checkmark&&&72.7   &87.1   &90.6   &84.3   &87.3   &89.3   &80.6 &68.6 &90.3&84.8&69.4&91.3& \colorbox{lightgray}{83.6} \\
\checkmark& \checkmark&&73.9& 87.5& 91.0& 85.3 &87.9 &89.9& 82.8 &72.1& 91.2 &86.3& 74.1 &92.4&\colorbox{lightgray}{84.6}\\
\checkmark& &\checkmark&79.2&91.8&92.3&88.0&92.6&\textbf{93.0}&87.1&77.8&92.5&88.2&78.4&93.9&\colorbox{lightgray}{87.9}\\
\hline
 \checkmark & \checkmark& \checkmark&\textbf{81.3}&\textbf{92.9}&\textbf{92.8}&\textbf{88.4}&\textbf{93.4}&\textbf{93.2}&\textbf{87.9}&\textbf{80.4}&\textbf{93.0}&\textbf{89.0}&\textbf{80.9}&\textbf{94.8}&\colorbox{lightgray}{\textbf{89.0}}\\
\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Effect of semi-supervised loss. The best performance is marked as \textbf{bold}.}
\label{tab:semi1}
\end{table*}
\begin{table*}[t!]
\centering
\resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{c|clllllllllllll}
\toprule
Method  &A$\to$ C& A$\to$ P & A $\to$ R & C $\to$ A &C $\to$ P &C $\to$ R &P$\to$ A& P$\to$ C & P$ \to$ R & R$ \to$ A &R$ \to$ C &R$ \to$ P& \colorbox{lightgray}{Avg}     \\
\hline
\hline
Beta(1,1)&79.9&92.0&92.3&88.6&92.6&92.4&86.9&79.0&92.4&88.2&79.3&94.0&\colorbox{lightgray}{88.1}\\
Beta(2,2)&79.9&92.1&92.7&88.4&92.4&92.7&86.9&79.5&92.1&88.1&79.6&94.3&\colorbox{lightgray}{88.2}\\
\hline
Learning   &\textbf{81.3}&\textbf{92.9}&\textbf{92.8}&\textbf{88.4}&\textbf{93.4}&\textbf{93.2}&\textbf{87.9}&\textbf{80.4}&\textbf{93.0}&\textbf{89.0}&\textbf{80.9}&\textbf{94.8}&\colorbox{lightgray}{\textbf{89.0}}\\

\bottomrule
\end{tabular}}
\vspace{-6pt}
\caption{Effect of learning parameters. The best performance is marked as \textbf{bold}.}
\label{tab:Mixup}
\end{table*}
\vspace{-2pt}
\begin{table*}[t!]
\centering
\resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{c|clllllllllllll}
\toprule
Method &A$\to$ C& A$\to$ P & A $\to$ R & C $\to$ A &C $\to$ P &C $\to$ R &P$\to$ A& P$\to$ C & P$ \to$ R & R$ \to$ A &R$ \to$ C &R$ \to$ P& \colorbox{lightgray}{Avg}     \\
\hline
\hline
Mixup &79.4&92.4&92.6&87.5&92.8&92.4&86.8&\textbf{80.3}&92.5&88.2&79.7&\textbf{95.4}&\colorbox{lightgray}{88.3}\\
CutMix&79.2&91.2&92.2&87.6&91.8&91.8&86.0&77.8&92.6&88.2&78.4&94.1&\colorbox{lightgray}{87.6}\\
\hline
PatchMix &\textbf{81.3}&\textbf{92.9}&\textbf{92.8}&\textbf{88.4}&\textbf{93.4}&\textbf{93.2}&\textbf{87.9}&\textbf{80.4}&\textbf{93.0}&\textbf{89.0}&\textbf{80.9}&94.8&\colorbox{lightgray}{\textbf{89.0}} \\
\bottomrule
\end{tabular}}
\vspace{-8pt}
\caption{Effect of PatchMix. The best performance is marked as \textbf{bold}.}
\vspace{-13pt}
\label{tab:Patch_Mixup}
\end{table*}
\noindent\textbf{Results on DomainNet.} PMTrans achieves a very high average accuracy on the most challenging DomainNet dataset, as shown in Tab. \ref{tab:domainnet}. Overall, our proposed PMTrans outperforms the SoTA methods by \textbf{+7.2\%} accuracy. Incredibly, PMTrans surpasses the SoTA methods in all the \textbf{30 sub-tasks}, which demonstrates the strong ability of PMTrans to alleviate the large domain gap. Moreover, transferring knowledge is much more difficult when the domain gap becomes significant. 
\textit{When taking more challenging $qdr$ as the target domain while others as the source domain, our PMTrans achieves an average accuracy of \textbf{27.0\%}, while ViT-based SSRT and CDTrans only achieve an average accuracy of \textbf{13.7\%} and \textbf{19.6\%}, respectively.} The comparisons on DomainNet dataset demonstrate that our PMTrans yields the best generalization ability for the challenging UDA problem.
\subsection{Ablation Study}



\noindent\textbf{Semi-supervised mixup loss.}
As shown in Tab. \ref{tab:semi1}, Swin with the semi-supervised mixup loss in the feature and label spaces outperforms the counterpart built on Swin with only source training by \textbf{+1.0\%} and \textbf{+4.3\%} on Office-Home dataset, respectively. The results indicate the effectiveness of the semi-supervised mixup loss for diminishing the domain discrepancy. Moreover, we observe that the CE loss yields better performance on the label space than that on the feature space. The reason is that the CE loss on the label space utilizing the class information performs better than on the feature space without the class information.

\noindent\textbf{Learning hyperparameters of mixup.}
Tab. \ref{tab:Mixup} shows the ablation results for the effects of learning hyperparameters of the Beta distribution on the Office-Home. We compare the learning hyperparameters of mixup with fixed parameters, such as Beta(1,1) and Beta(2,2). The proposed method achieves \textbf{+0.9\%} and \textbf{+0.8\%} accuracy increment compared with that based on Beta(1,1) and Beta(2,2). The results demonstrate that learning to estimate the distribution to build up the intermediate domain facilitates domain alignment.

\noindent\textbf{PatchMix.} Comparisons of PMTrans with Mixup \cite{ZhangCDL18} and CutMix \cite{YunHCOYC19} are shown in Tab. \ref{tab:Patch_Mixup}. PMTrans outperforms Mixup and CutMix by \textbf{+0.7\%} and \textbf{+1.4\%} accuracy on the Office-Home dataset, demonstrating that PatchMix can capture the global and local mixture information better than the global mixture Mixup and local mixture CutMix methods.

\noindent\textbf{Visualization.}
In Fig.~\ref{fig:visual}, we visualize the features learned by Swin-based, PMTrans-Swin, PMTrans-ViT, and PMTrans-Deit on task A $\rightarrow$ C from the Office-Home dataset via the t-SNE~\cite{DonahueJVHZTD14}. Compared with Swin-based and PMTrans-Swin, our PMTrans model can better align the two domains by constructing the intermediate domain to bridge them. Moreover, comparisons between PMTrans with different transformer backbones reveal that PMTrans works successfully with different backbones on UDA tasks.
\textit{Due to the page limit, more experiments and analyses can be found in the suppl. material.}
\vspace{-8pt}
\section{Conclusion and Future Work}
\vspace{-4pt}
In this paper, we proposed a novel method, PMTrans, an optimization solution for UDA from a game perspective. Specifically, we first proposed a novel ViT-based module called PatchMix that effectively built up the intermediate domain to learn discriminative domain-invariant representations for domains. And the two semi-supervised mixup losses were proposed to assist in finding the Nash Equilibria. Moreover, we leveraged attention maps from ViT to re-weight the label of each patch by its significance. PMTrans achieved the SoTA results on four benchmark UDA datasets, outperforming the SoTA methods by a large margin. In the near future, we plan to implement our PatchMix and the two semi-supervised mixup losses to solve self-supervised and semi-supervised learning problems. We will also exploit our method to tackle the challenging downstream tasks, \eg, semantic segmentation and object detection. 

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
