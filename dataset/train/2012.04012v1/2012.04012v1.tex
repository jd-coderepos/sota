\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{acronym} 

\usepackage{caption}
\usepackage{subcaption}
\usepackage{balance}

\usepackage[toc,page]{appendix}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\todo}[1]{\PackageWarning{}{Unprocessed todo}{\footnotesize \textcolor{red}{XXX \textbf{#1} XXX}}}
\newcommand{\qheading}[1]{\noindent\textbf{#1}}
\newcommand{\modelname}{DECA\xspace}
\newcommand{\supmat}{Sup.~Mat.\xspace}
\newcommand{\imsize}{0.48}
\newcommand{\varhspace}{0.059}

\acrodef{PCA}{Principal Component Analysis}
\acrodef{GAN}{generative adversarial network}
\acrodef{MDL}{minimum description length}
\acrodef{LBS}{linear blend skinning}
\acrodef{SfS}{shape from shading}


\begin{document}

\title{Learning an Animatable Detailed 3D Face Model from In-The-Wild Images}

\author{Yao Feng\textsuperscript{1,2}\thanks{equal contribution}  \qquad Haiwen Feng\textsuperscript{1} \qquad Michael J. Black\textsuperscript{1} \qquad Timo Bolkart\textsuperscript{1}  \\
\textsuperscript{1}Max Planck Institute for Intelligent Systems, T{\"u}bingen, Germany \\
\textsuperscript{2}Max Planck ETH Center for Learning System \\
{\tt\small \{yfeng, hfeng, black, tbolkart\}@tuebingen.mpg.de}
}

\maketitle


\begin{abstract}
While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations.
Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression.
Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images.
We present the first approach to jointly learn a model with animatable detail and a detailed 3D face regressor from in-the-wild images that recovers shape details as well as their relationship to facial expressions. 
Our \modelname (Detailed Expression Capture and Animation) model is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. 
We introduce a novel detail-consistency loss to disentangle person-specific details and expression-dependent wrinkles.
This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. 
\modelname achieves state-of-the-art shape reconstruction accuracy on two benchmarks. 
Qualitative results on in-the-wild data demonstrate \modelname's robustness and its ability to disentangle identity and expression dependent details enabling animation of reconstructed faces.
The model and code are publicly available at \url{https://github.com/YadiraF/DECA}.
\end{abstract}
 
\newcommand{\vect}[1]{\mathbf{#1}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\shapecoeff}{\boldsymbol{\beta}}
\newcommand{\shapedim}{{\left| \shapecoeff \right|}}
\newcommand{\shapespace}{\mathcal{S}}
\newcommand{\posecoeff}{\boldsymbol{\theta}}
\newcommand{\posedim}{{\left| \posecoeff \right|}}
\newcommand{\posespace}{\mathcal{P}}
\newcommand{\expcoeff}{\boldsymbol{\psi}}
\newcommand{\expdim}{{\left| \expcoeff \right|}}
\newcommand{\expspace}{\mathcal{E}}
\newcommand{\numverts}{n}
\newcommand{\template}{\textbf{T}}

\newcommand{\numjoints}{k}
\newcommand{\joints}{\textbf{J}}
\newcommand{\jointregressor}{\mathcal{J}}
\newcommand{\blendweights}{\mathcal{W}}
\newcommand{\blendweightsdim}{\left| \mathcal{W} \right|}

\newcommand{\landmark}{\textbf{k}}



\newcommand{\lighting}{\textbf{l}}
\newcommand{\cam}{\textbf{c}}


\newcommand{\albedo}{A}
\newcommand{\albedocoeffs}{\boldsymbol{\alpha}}
\newcommand{\albedodim}{\left| \albedocoeffs \right|}
\newcommand{\normalcoeffs}{\boldsymbol{\nu}}
\newcommand{\normaldim}{\left| \normalcoeffs \right|}

\newcommand{\uvsize}{d}
\newcommand{\image}{I}

\newcommand{\flamev}{M}
\newcommand{\normals}{N}

\newcommand{\displacements}{D}
\newcommand{\detailgeom}{G}


\newcommand{\coarseencoder}{E_c}
\newcommand{\detailencoder}{E_d}
\newcommand{\edm}{F_d}

\newcommand{\zcode}{\boldsymbol{\delta}}
\newcommand{\cons}{constraint} 




 \section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.165\columnwidth]{images/teaser/columns/alfw1_CFD-BF-001-021-HC_animation_lighting4.jpg}\includegraphics[width=0.165\columnwidth]{images/teaser/columns/alfw2_celeb12_2_animation_lighting1.jpg}\includegraphics[width=0.165\columnwidth]{images/teaser/columns/image00302_IMG_0406_inputs_animation_lighting0.jpg}\includegraphics[width=0.165\columnwidth]{images/teaser/columns/image03786_CFD-BF-001-021-HC_animation_lighting1.jpg}\includegraphics[width=0.165\columnwidth]{images/teaser/columns/image02673_IMG_0406_inputs_animation_lighting4.jpg}\includegraphics[width=0.165\columnwidth]{images/teaser/columns/IMG_0392_inputs_celeb12_2_animation_lighting0.jpg}
	\caption{{\bf \modelname.}  Example images (row 1), the regressed coarse shape (row 2), detail shape (row 3) and reposed coarse shape (row 4), and reposed with person-specific details (row 5).
	\modelname is robust to occlusion and captures person-specific details as well as expression wrinkles that appear in regions like forehead and mouth.    
	Our novelty is that this detail shape can be reposed (animated) such that the wrinkles are specific to the source shape and expression.
	}
    \label{fig:teaser}
\end{figure}

Two decades have passed since the seminal work of Vetter and Blanz~\cite{VetterBlanz1998} that first showed how to reconstruct 3D facial geometry from a single image.
Since then, 3D face reconstruction methods have rapidly advanced (for a comprehensive overview see~\cite{Zollhoefer2018}) enabling applications such as 3D avatar creation for VR/AR~\cite{Hu2017}, video editing~\cite{Thies2016}, face recognition~\cite{Blanz2002,Romdhani2002}, virtual make-up~\cite{Scherbaum2011}, or speech-driven facial animation~\cite{VOCA2019}.
To make the problem tractable, most existing methods incorporate prior knowledge about geometry or appearance by leveraging pre-computed 3D face models~\cite{Brunton2014,Egger2020}. 
These models reconstruct the coarse face shape but are unable to capture geometric details such as expression-dependent wrinkles, which are essential for realism and for analysing human emotion.

Several methods recover detailed facial geometry~\cite{Abrevaya2020,Cao2015,Chen2019,Guo2018,Richardson2017,AnhTran2018,LuanTran2019}, however, they require high-quality training scans~\cite{Cao2015,Chen2019} or lack robustness to occlusions~\cite{Abrevaya2020,Guo2018,Richardson2017}. 
None of these works explore how the recovered wrinkles change with varying expressions. 
Previous methods that learn expression-dependent detail models~\cite{chaudhuri2020personalized,yang2020facescape} either use detailed 3D scans as training data and, hence, do not generalize to unconstrained images~\cite{yang2020facescape}, or model expression-dependent details as part of the appearance map rather than the geometry~\cite{chaudhuri2020personalized},  preventing realistic mesh relighting. 

We introduce \modelname (Detailed Expression Capture and Animation), which learns an {\em animatable} displacement model from in-the-wild images without 2D-to-3D supervision. 
In contrast to prior work, these {\em animatable expression-dependent wrinkles are specific to an individual} and are regressed from an image.
Specifically, \modelname jointly learns
1) a geometric detail model that generates a UV displacement map from a low-dimensional representation that consists of subject-specific detail parameters and expression parameters, and
2) a regressor that predicts subject-specific detail, albedo, shape, expression, pose, and lighting parameters from an image. 
The detail model builds upon FLAME's~\cite{FLAME2017} coarse geometry, and we formulate the displacements as a function of subject-specific detail parameters and FLAME's jaw pose and expression parameters.


To gain control over expression-dependent wrinkles of the reconstructed face, while preserving person-specific details (i.e.~moles, pores, eyebrows, and expression-independent wrinkles), the person-specific details and expression-dependent wrinkles must be disentangled. 
Our key contribution is a novel {\em detail consistency loss} that enforces this disentanglement.
Given two images of the same person with different expressions, we observe that their 3D face shape and their person-specific details are the same in both images, but the expression and the intensity of the wrinkles differ with expression.
During training, this observation is exploited by swapping the detail codes between different images of the same identity and enforcing the newly rendered results to look similar to the original input images. 
Once trained, \modelname reconstructs a detailed 3D face from a single image (Fig.~\ref{fig:teaser} third row) in real time (about 120fps on a Nvidia Quadro RTX 5000), and is able to animate the reconstruction with realistic adaptive expression wrinkles (Fig.~\ref{fig:teaser} bottom). 

In summary, our main contributions are:
1) The first approach to learn an animatable displacement model from in-the-wild images that can synthesize plausible geometric details by varying expression parameters. 
2) A novel detail consistency loss that disentangles identity-dependent and expression-dependent facial details. 
3) Reconstruction of geometric details that is, unlike most competing methods, robust to occlusions, poses, and illumination variation.
This is enabled by our low-dimensional detail representation, the detail disentanglement, and training from a large dataset of in-the-wild images.
4) State-of-the-art shape reconstruction accuracy on two different benchmarks. 
5) Code and model will be made publicly available for research purposes. \section{Related work}

The reconstruction of 3D faces from visual input has received significant attention over the last decades after the pioneering work of Parke~\cite{Parke1974}, the first method to reconstruct 3D faces from multi-view images.
While a large body of related work aims to reconstruct 3D faces from various input modalities such as multi-view images~\cite{Beeler2010,Cao2018,Pighin1998}, video data~\cite{Garrido2016,Ichim2015,Jeni2015,Shi2014,Suwajanakorn2014}, RGB-D data~\cite{Li2013,Thies2015,Weise2011} or subject-specific image collections~\cite{KemelmacherSeitz2011,Roth2016}, our main focus is on methods that use only a single RGB image.
For a more comprehensive overview, see Zollh{\"o}fer et al.~\cite{Zollhoefer2018}.

\qheading{Coarse reconstruction:}
Many monocular 3D face reconstruction methods follow Vetter and Blanz~\cite{VetterBlanz1998} by estimating coefficients of pre-computed statistical models in an analysis-by-synthesis fashion. 
Such methods can be categorized into optimization-based~\cite{AldrianSmith2013,Bas2017fitting,Blanz2002,BlanzVetter1999,Gerig2018,RomdhaniVetter2005,Thies2016}, or learning-based methods~\cite{Chang2018,Deng2019,Genova2018,Kim2018,Richardson2016,Sanyal2019,Tewari2017,AnhTran2017,Tu2019}.
These methods estimate parameters of a statistical face model with a fixed linear shape space, which captures only low-frequency shape information.
This results in overly-smooth reconstructions. 

Several works are model-free and directly regress 3D faces (i.e. voxels~\cite{Jackson2017} or meshes~\cite{Dou2017,Feng2018,Guler2017,Wei2019}) and hence could capture more variation than the model-based methods. 
However, all these methods require explicit 3D supervision, which is provided either by an optimization-based model fitting~\cite{Feng2018,Guler2017,Jackson2017,Wei2019} or by synthetic data generated by sampling a statistical face model~\cite{Dou2017} and therefore also only capture coarse shape variations. 

Instead of capturing high-frequency geometric details, some methods reconstruct coarse facial geometry along with high-fidelity textures~\cite{Gecer2019,Saito2017,Slossberg2018,Yamaguchi2018}. 
As this ``bakes" shading details into the texture, lighting changes do not affect these details.  
To enable animation and relighting, \modelname captures these details as part of the geometry.


\qheading{Detail reconstruction:}
Another body of work aims to reconstruct faces with ``mid-frequency" details.
Common optimization-based methods fit a statistical face model to images to obtain a coarse shape estimate, followed by a \ac{SfS} method to reconstruct facial details from monocular images~\cite{Jiang2018,Li2018} or videos~\cite{Garrido2016,Suwajanakorn2014}. 
Unlike \modelname, these approaches are slow, the results lack robustness to occlusions, and the coarse model fitting step requires facial landmarks, making them error-prone for large viewing angles and occlusions.

Most regression-based approaches~\cite{Cao2015,Chen2019,Guo2018,Richardson2017,AnhTran2018} follow a similar approach by first reconstructing the parameters of a statistical face model to obtain a coarse shape, followed by a refinement step to capture localized details.
Chen et al.~\cite{Chen2019} and Cao et al.~\cite{Cao2015}  compute local wrinkle statistics from high-resolution scans and leverage these to constrain the fine-scale detail reconstruction from images~\cite{Chen2019} or videos~\cite{Cao2015}.
Guo et al.~\cite{Guo2018} and Richardson et al.~\cite{Richardson2017} directly regress per-pixel displacement maps.
All these methods only reconstruct fine-scale details in non-occluded regions, causing visible artifacts in the presence of occlusions. 
Tran et al.~\cite{AnhTran2018} gain robustness to occlusions by applying some face segmentation method~\cite{Nirkin2018} to determine occluded regions, and employ an example-based hole filling of the occluded regions. 
Further, model-free methods exist that directly reconstruct detailed meshes~\cite{Sela2017,Zeng2019} or surface normals that add detail to coarse reconstructions~\cite{Abrevaya2020,Sengupta2018}.

Tran et al.~\cite{LuanTran2019} and Tewari et al.~\cite{Tewari2019,Tewari2018} jointly learn a statistical face model and reconstruct 3D faces from images. 
While offering more flexibility than fixed statistical models, these methods capture limited geometric details compared to other detail reconstruction methods. 

Unlike \modelname, none of these detail reconstruction methods offer animatable details after reconstruction.

\qheading{Animatable detail reconstruction:}
Most relevant to \modelname are methods that reconstruct detailed faces while allowing animation of the result.
Golovinski et al.~\cite{Golovinskiy2006}, Shin et al.~\cite{Shin2014} and FaceScape~\cite{yang2020facescape} learn correlations between wrinkles and factors like age and gender~\cite{Golovinskiy2006} or expression~\cite{Shin2014,yang2020facescape} from high-quality face scans. 
In contrast, \modelname learns an animatable detail model solely from in-the-wild images without paired 3D training data.
While FaceScape~\cite{yang2020facescape} predicts an animatable 3D face from a single image, the method is not robust to occlusions.
This is due to a two step reconstruction process: first optimize the coarse shape, then predict a displacement map from the texture map extracted with the coarse reconstruction.

Chaudhuri et al.~\cite{chaudhuri2020personalized} learn identity and expression corrective blendshapes with dynamic (expression-dependent) albedo maps~\cite{Nagano2018}. 
They model geometric details as part of the albedo map, and therefore, the shading of these details does not adapt with varying lighting. 
This results in unrealistic renderings.
In contrast, \modelname models details as geometric displacements, which look natural when re-lit.  \section{Preliminaries}
\label{FLAMEDECODER}

\qheading{Geometry prior:}
FLAME~\cite{FLAME2017} is a statistical 3D head model that combines separate linear identity shape and expression spaces with \ac{LBS} and pose-dependent corrective blendshapes to articulate the neck, jaw, and eyeballs.
Given parameters of facial identity , pose  (with  joints for neck, jaw, and eyeballs), and expression , FLAME outputs a mesh with  vertices. 
The model is defined as

with the blend skinning function  that rotates the vertices in  around joints , linearly smoothed by blendweights . The joint locations  are defined as a function of the identity . Further,

denotes the mean template  in ``zero pose'' with added shape blendshapes , pose correctives , and expression blendshapes , with the learned identity, pose, and expression bases  and . See~\cite{FLAME2017} for details.

\qheading{Appearance model:}
FLAME does not have an appearance model, hence we convert Basel Face Model's PCA albedo space \cite{Paysan2009} into the FLAME UV layout to make it compatible with FLAME.
The appearance model outputs a UV albedo map  for albedo parameters .

\qheading{Camera model:}
Photographs in existing in-the-wild face datasets are often taken from a distance.
We, therefore, use an orthographic camera model  to project the 3D mesh into image space. 
Face vertices are projected into the image as , where  is a vertex in ,  is the orthographic 3D-2D projection matrix, and  and  denote isotropic scale and 2D translation, respectively.
The parameters , and  are summarized as .

\qheading{Illumination model:}
For face reconstruction, the most frequently-employed illumination model is based on Spherical Harmonics (SH)~\cite{M1966Spherical}. 
By assuming that the light source is distant and the face's surface reflectance is Lambertian, the shaded face image is computed as:

where the albedo, , surface normals, , and shaded texture, , are represented in UV coordinates and where , , and  denote pixel  in the UV coordinate system. The SH basis and coefficients are defined as  and , with , and  denotes the Hadamard product. 

\qheading{Texture rendering:}
Once we have the geometry parameters (), albedo (), lighting () and camera information , we can recover the 2D image  by rendering as , where  denotes the rendering function.
 
FLAME is able to generate a face geometry with various poses, shapes and expressions from a low-dimensional latent space. 
However, the representational power of the model is limited by the low mesh resolution and therefore mid-frequency details are mostly missing in FLAME's surface.
The next section introduces our expression-dependent displacement model that augments FLAME with mid-frequency details, and it demonstrates how to reconstruct detailed geometry from a single image and animate it.

 \section{Proposed method}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\textwidth, trim={0cm, 0cm, 0cm, 0cm}]{images/overview/model_overview_v3.png} 
	\caption{\modelname training and animation. 
	During training, \modelname estimates parameters to reconstruct face shape for each image and, at the same time, learns an expression-conditioned displacement model by leveraging the shape and detail consistency information from multiple images of the same individual (see Sec.~\ref{sec: disentanglement} for details, the yellow box region is further illustrated in Fig.~\ref{fig:detail_consistency}).
	Once trained, \modelname animates a face (right) by combining the reconstructed source identity's shape, head pose, and detail code, with the reconstructed source expression's jaw pose and expression parameters to obtain an animated coarse shape and an animated displacement map.
	Finally, \modelname outputs an animated detail shape.}
	\label{fig:overview}
\end{figure*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\columnwidth, trim={2cm, 1cm, 2cm, 1cm}]{images/overview/detail_consistency_v5.png}  
	\caption{Detail consistency loss. See Sec.~\ref{sec: disentanglement} for details.}
	\label{fig:detail_consistency}
\end{figure}

The goal of \modelname is to learn a parameterized face model with geometric detail solely from in-the-wild images (Fig.~\ref{fig:overview} left).
Once trained, \modelname reconstructs the 3D head with detailed face geometry from a single face image . 
The learned parametrization of the reconstructed details enables us then to animate the detail reconstruction by controlling FLAME's expression and jaw pose parameters (Fig.~\ref{fig:overview} right).
This synthesizes new wrinkles while keeping person-specific details unchanged. 

\qheading{Key idea:}
The key idea of \modelname is grounded in the observation that an individual's face shows different details (i.e.~wrinkles), depending on their facial expressions but that other properties of their shape remain unchanged.
Consequently, facial details should be  separated into static person-specific details and dynamic expression-dependent details such as wrinkles~\cite{Li2009RobustSG}. 
However, disentangling static and dynamic facial details is a non-trivial task.
Static facial details are different across people, whereas dynamic expression dependent facial details even vary for the same person.
Thus, \modelname learns an expression-conditioned detail model to infer facial details from both the person-specific detail latent space and the expression space. 

The main difficulty of learning a detail displacement model is the lack of training data. 
Prior work uses specialized camera systems to scan people in a controlled environment to obtain detailed facial geometry. 
However, this approach is expensive and impractical for capturing large numbers of identities with varying expressions and diversity in ethnicity and age. 
Therefore we propose an approach to learn detail geometry from in-the-wild images.

\subsection{Coarse reconstruction}
We first learn a coarse reconstruction (i.e.~in FLAME's model space) in an analysis-by-synthesis way: given a 2D image  as input, we encode the image to a latent code, decode this to synthesize a 2D image , and minimize the difference between the synthesized image and the input.
As shown in Fig.~\ref{fig:overview}, we train an encoder , which consists of a ResNet50~\cite{He2015DeepRL} network followed by a fully connected layer, to regress a low-dimensional latent code.
This latent code consists of FLAME parameters , ,  (i.e.~representing the coarse geometry), albedo coefficients , camera , and lighting parameters . 
More specifically, the coarse geometry uses the first 100 FLAME shape parameters (), 50 expression parameters (), and 50 albedo parameters (). 
In total,  predicts a  dimensional latent code.

Given a dataset of  face images  with multiple images per subject, corresponding identity labels , and   keypoints  per image, the coarse reconstruction branch is trained by minimizing

with landmark loss , eye closure loss , photometric loss , identity loss , shape consistency loss  and regularization .

\qheading{Landmark re-projection loss:}
The landmark loss measures the difference between ground-truth  face landmarks  and the corresponding landmarks in the FLAME's surface , projected into the image by the estimated camera model. 
The landmark loss is defined as


\qheading{Eye closure loss:}
The eye closure loss computes the relative offset of landmarks  and  on the upper and lower eyelid, and measures the difference to the offset of the corresponding landmarks in the FLAME's surface  and  projected into the image.
Formally, the loss is given as

where  is the set of upper/lower eyelid landmark pairs.

\qheading{Photometric loss:}
The photometric loss computes the error between the input image  and the rendering  as .
Here,  is a face mask with value  in the face skin region, and value  elsewhere obtained by an existing face segmentation method~\cite{Nirkin2018}, and  denotes the Hadamard product.
Computing the error in the face region only provides robustness to common occlusions by e.g. hair, clothes, sunglasses, etc.

\qheading{Identity loss:}
Recent 3D face reconstruction methods demonstrate the effectiveness of utilizing an identity loss to produce more realistic face shapes~\cite{Deng2019,Gecer2019}. 
Motivated by this, we also use a pretrained face recognition network~\cite{Cao2018_VGGFace2}, to employ an identity loss during training. 

The face recognition network  outputs feature embeddings of the rendered images and the input image, and the identity loss then measures the cosine similarity between the two embeddings.
Formally, the loss is defined as


\qheading{Shape consistency loss:}
Given two images  and  of the same subject (i.e.~), the coarse encoder  should output the same shape parameters (i.e.~).
Previous work encourages shape consistency by enforcing the distance between  and  to be smaller by a margin than the distance to the shape coefficients corresponding of a different subject~\cite{Sanyal2019}. 
However, choosing this fixed margin is challenging in practice. 
Instead, we propose a different strategy by replacing  with  while keeping all other parameters unchanged.
Given that  and  represent the same subject, this new set of parameters must reconstruct  well. 
Formally, we minimize


\qheading{Regularization:}
 regularizes shape , expression , and albedo .



\subsection{Detail reconstruction} 

The detail reconstruction aims at augmenting the coarse FLAME geometry with a detailed UV displacement map  (see Fig.~\ref{fig:overview}).
Similar to the coarse reconstruction, we train an encoder  (with the same architecture as ) to encode  to a -dimensional latent code , representing subject-specific details.
The latent code  is then concatenated with FLAME's expression  and jaw pose parameters , and decoded by  to . 

\qheading{Detail decoder: }
The detail decoder is defined as

where the detail code  controls the static person-specific details.
We leverage the expression  and jaw pose parameters  from the coarse reconstruction branch to capture the dynamic expression wrinkle details.
For rendering,  is converted to a normal map. 

\qheading{Detail rendering:}
The detail displacement model allows us to generate images with fine-scale surface details. 
To reconstruct the detailed geometry , we convert  and its surface normals  to UV space, denoted as  and , and combine them with  as

By calculating normal  from , we obtain the detail rendering  by rendering  with applied normal map as


The detail reconstruction is trained by minimizing

with photometric detail loss , ID-MRF loss , soft symmetry loss , and detail regularization .

\qheading{Detail photometric losses:}
With the applied detail displacement map, the rendered images  contain some geometric details.
Equivalent to the coarse rendering, we use a photometric loss , where, recall,  is a mask representing the visible skin pixels.

\qheading{ID-MRF loss:}
We add an Implicit Diversified Markov Random Fields (ID-MRF) loss~\cite{Wang2018} to reconstruct geometric details.
Given two images of the same person, the ID-MRF loss extracts feature patches from different layers of a pre-trained network, and then minimizes the difference between corresponding nearest neighbor feature patches from both images. 
Following Wang et al.~\cite{Wang2018}, the loss is computed on layers  and  of VGG19~\cite{Simonyan2014VeryDC} as

where  denotes the ID-MRF loss which is employed on the feature patches extracted from  and  with layer  of VGG19.
As for the photometric losses, we compute  only for the face skin region in UV space.

\qheading{Soft symmetry loss:}
To add robustness to occlusions, we add a soft symmetry loss to regularize non-visible face parts.
Specifically, we minimize

where  denotes the face skin mask in UV space, and  is the horizontal flip operation. 

\qheading{Detail regularization:} 
The detail displacements are regularized by  to reduce noise.



\subsection{Detail disentanglement} 
\label{sec: disentanglement}
Optimizing  enables us to reconstruct faces with mid-frequency details.
Making these detail reconstructions animatable however requires us to disentangle person specific details (i.e.~moles, pores, eyebrows, and expression-independent wrinkles) controlled by  from expression-dependent wrinkles (i.e.~wrinkles that change for varying facial expression) controlled by FLAME's expression and jaw pose parameters,  and .
Our key observation is that the same person in two images should have both similar coarse geometry {\em and} personalized details. 
So for the rendered detail image, {\em exchanging the detail codes between two images of the same subject should have no effect on the rendered image.}

\qheading{Detail consistency loss:}
Given two images  and  of the same subject (i.e.~), the loss is defined as

where , , , , , , and  are the parameters of , while  is the detail code of  (see Fig.~\ref{fig:detail_consistency}).
We show the necessity and effectiveness of  in Sec.~\ref{sec:ablation}.





 \section{Implementation Details}


\qheading{Data:}
We train \modelname on three publicly available datasets: \mbox{VGGFace2}~\cite{Cao2018_VGGFace2}, BUPT-Balancedface~\cite{Wang_2019_ICCV} and VoxCeleb2~\cite{Chung18b}.
VGGFace2~\cite{Cao2018_VGGFace2} contains images of over  subjects, with an average of more than  images per subject. 
BUPT-Balancedface~\cite{Wang_2019_ICCV} offers  subjects per ethnicity (i.e.~Caucasian, Indian, Asian and African), and VoxCeleb2~\cite{Chung18b} contains  videos of  subjects.
In total, \modelname is trained on 2 Million images. 

All datasets provide an identity label for each image.
We use FAN~\cite{Bulat2017} to predict  2D landmarks  on each face. 
To improve the robustness of the predicted landmarks, we run FAN for each image twice with different face crops, and discard all images with non-matching landmarks.
See \supmat~for details on data selection and data cleaning.

\qheading{Implementation details:}
\modelname is implemented in PyTorch~\cite{Paszke2019PyTorchAI}, using the differentiable rasterizer from Pytorch3D~\cite{ravi2020pytorch3d} for rendering.
We use Adam~\cite{Kingma2015AdamAM} as optimizer with a learning rate of .  
The input image size is  and UV space size . 
See \supmat~for details.
 \section{Evaluation}

\subsection{Qualitative evaluation}
\label{sec:qual_eval}

\qheading{Reconstruction:}
Given a single face image, \modelname reconstructs the 3D face shape with mid-frequency geometry details.
The second row of Fig.~\ref{fig:teaser} shows that the coarse shape (i.e.~in FLAME space) well represents the overall face shape, and the learned \modelname detail model reconstructs subject-specific details and wrinkles of the input identity (Fig.~\ref{fig:teaser} row three), while being robust to partial occlusions.

Figure \ref{fig:qualitative1} qualitatively compares \modelname results with state-of-the-art coarse face reconstruction methods, namely PRNet~\cite{Feng2018}, RingNet~\cite{Sanyal2019}, Deng et al.~\cite{Deng2019}, FML~\cite{Tewari2019} and 3DDFA-V2~\cite{guo2020towards}.
Compared to these methods, \modelname better reconstructs the overall face shape with details like the nasolabial fold (rows 1, 2, 3, 4, and 6) and forehead wrinkles (row 3).
\modelname better reconstructs the mouth shape and the eye region than all other methods.
\modelname further reconstructs a full head while PRNet~\cite{Feng2018}, Deng et al.~\cite{Deng2019}, FML~\cite{Tewari2019} and 3DDFA-V2~\cite{guo2020towards} reconstruct tightly cropped faces. 
While RingNet~\cite{Sanyal2019}, like \modelname, is based on FLAME~\cite{FLAME2017}, \modelname better reconstructs the face shape and the facial expression. 

Figure \ref{fig:qualitative2} compares \modelname visually to existing detail face reconstruction methods, namely Extreme3D~\cite{AnhTran2018}, Cross-modal~\cite{Abrevaya2020}, and FaceScape~\cite{yang2020facescape}.
Extreme3D~\cite{AnhTran2018} and Cross-modal~\cite{Abrevaya2020} reconstruct more details than \modelname but at the cost of being less robust to occlusions (rows 1, 2, 3).
Unlike \modelname, Extreme3D and Cross-modal only reconstruct static details. 
However, using static details instead of \modelname's animatable details leads to visible artifacts when animating the face (see Fig.~\ref{fig:animation2}).
While FaceScape~\cite{yang2020facescape} provides animatable details, unlike \modelname, the method is trained on high-resolution scans while \modelname is solely trained on in-the-wild images.
Also, with occlusion, FaceScape produces artifacts (rows 1, 2) or effectively fails (row 3).

In summary, \modelname produces high-quality reconstructions, outperforming previous work in terms of robustness, while enabling animation of the detailed reconstruction.
To demonstrate the quality of \modelname and the robustness to variations in head pose, expression, occlusions, image resolution, lighting conditions, etc., we show results for 200 randomly selected ALFW2000~\cite{Zhu2015} images in the \supmat~along with more qualitative coarse and detail reconstruction comparisons to the state-of-the-art.

\begin{figure}[t]
    \offinterlineskip
    \centering
    \includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_0.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_1.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_2.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_3.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_4.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_5.jpg}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r1/0_6.png}\\  
    \includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_0.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_1.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_2.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_3.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_4.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_5.jpg}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r2/0_6.png}\\  
    \includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_0.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_1.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_2.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_3.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_4.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_5.jpg}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r3/0_6.png}\\  
    \includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_0.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_1.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_2.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_3.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_4.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_5.jpg}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r4/0_6.png}\\  
    \includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_0.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_1.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_2.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_3.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_4.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_5.jpg}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r5/0_6.png}\\  
    \includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_0.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_1.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_2.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_3.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_4.png}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_5.jpg}\includegraphics[width=0.142\columnwidth]{images/qualitative/coarse_comparison/r6/0_6.png}  
    \caption{Comparison to other coarse reconstruction methods, from left to right: PRNet~\cite{Feng2018}, RingNet~\cite{Sanyal2019} Deng et al.~\cite{Deng2019}, FML~\cite{Tewari2019}, 3DDFA-V2~\cite{guo2020towards}, \modelname (ours).}
    \label{fig:qualitative1}
\end{figure}

\begin{figure}[t]
    \offinterlineskip
        \includegraphics[width=0.99\columnwidth]{images/qualitative/detail_comparison_no_bg/aflw_67.png}\\ 
        \includegraphics[width=0.99\columnwidth]{images/qualitative/detail_comparison_no_bg/aflw_246.png}\\ 
        \includegraphics[width=0.99\columnwidth]{images/qualitative/detail_comparison_no_bg/alfw3.png}\\ 
        \includegraphics[width=0.99\columnwidth]{images/qualitative/detail_comparison_no_bg/image00199.png}\\ 
        \includegraphics[width=0.99\columnwidth]{images/qualitative/detail_comparison_no_bg/n000658_0093_01.png}\\ 
        \includegraphics[width=0.99\columnwidth]{images/qualitative/detail_comparison_no_bg/n004007_0119_01.png} 
    \caption{Comparison to other detail reconstruction methods, from left to right: Extreme3D~\cite{AnhTran2018}, FaceScape~\cite{yang2020facescape}, Cross-modal~\cite{Abrevaya2020}, \modelname (ours).}
    \label{fig:qualitative2}
\end{figure}

\qheading{Detail animation:}
\modelname models detail displacements as a function of subject-specific detail parameters  and FLAME's jaw pose  and expression parameters .
This formulation allows us to animate detailed facial geometry such that wrinkles are specific to the source shape and expression as shown in Fig.~\ref{fig:teaser}.
Using static details instead of \modelname's animatable details (i.e.~by using the reconstructed details as a static displacement map) and animating only the coarse shape by changing the FLAME parameters results in visible artifacts as shown in Fig.~\ref{fig:animation2} (top), while animatable details (middle) look similar to the reference shape (bottom) of the same identity. 
The \supmat~shows more comparisons of animatable and static details. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\columnwidth, trim={2cm, 1cm, 2cm, 1cm}]{images/qualitative/qualitative_animation1.png}  
	\caption{Effect of \modelname's animatable details. Given images of source identity I and source expression E (left), \modelname reconstructs the detail shapes (middle) and animates the detail shape of I with the expression of E (right, middle). This synthesized \modelname expression appears identical to the reconstructed same subject's reference detail shape (right, bottom). Using the reconstructed details of I instead (i.e. static details) and animating the coarse shape only, results in visible artifacts (right, top). See Sec.~\ref{sec:qual_eval} for details.}
	\label{fig:animation2}
\end{figure}

\subsection{Quantitative evaluation}
We compare \modelname with publicly available methods, namely 3DDFA-V2~\cite{guo2020towards}, Deng et al.~\cite{Deng2019}, RingNet~\cite{Sanyal2019}, PRNet~\cite{Feng2018}, 3DMM-CNN~\cite{AnhTran2017} and Extreme3D~\cite{AnhTran2018}.

\qheading{NoW benchmark:} 
The NoW challenge~\cite{Sanyal2019} consists of 2054 face images of 100 subjects, split into a validation set (20 subjects) and a test set (80 subjects), with a reference 3D face scan per subject. 
The images consist of indoor and outdoor images, neutral expression and expressive face images, partially occluded faces, and varying viewing angles ranging from frontal view to profile view, and selfie images.
The challenge provides a standard evaluation protocol that measures the distance from all reference scan vertices to the closest point in the reconstructed mesh surface, after rigidly aligning scans and reconstructions. 
For details, see~\cite{NoW2019}.

We found that the tightly cropped face meshes predicted by Deng et al.~\cite{Deng2019} are smaller than the NoW reference scans, which would result in a high reconstruction error in the missing region.
For a fair comparison to the method of Deng et al.~\cite{Deng2019}, we use the Basel Face Model (BFM)~\cite{Paysan2009} parameters they output, reconstruct the complete BFM mesh, and get the NoW evaluation for these complete meshes. 
As shown in Tab.~\ref{tab:NoW_table} and the cumulative error plot in the \supmat, \modelname gives state-of-the-art results on NoW, providing the reconstruction error with the lowest mean, median, and standard deviation.

\begin{table}[]
    \centering
{\footnotesize 
	\begin{tabular}{|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Method}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Median (mm)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Mean (mm)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Std (mm)\end{tabular}}} \\ \hline
		3DMM-CNN~\cite{AnhTran2017}   & 1.84   & 2.33  & 2.05    \\ \hline
		PRNet~\cite{Feng2018}         & 1.50   & 1.98  & 1.88    \\ \hline
		Deng et al.19~\cite{Deng2019}      & 1.23   & 1.54  & 1.29    \\ \hline
		RingNet~\cite{Sanyal2019}  & 1.21   & 1.54  & 1.31    \\ \hline
		3DDFA-V2~\cite{guo2020towards}  & 1.23   & 1.57  & 1.39    \\ \hline
		\modelname (ours)          & \textbf{1.09}   & \textbf{1.38}    & \textbf{1.18}  \\ \hline
	\end{tabular}
}	
	\caption{Reconstruction error on the NoW~\cite{Sanyal2019} benchmark.}
    \label{tab:NoW_table}
\end{table}

\qheading{Feng et al. benchmark:}
The Feng et al.~challenge~\cite{Feng2018evaluation} contains 2000 face images of 135 subject, and a reference 3D face scan for each subject. 
The benchmark consists of 1344 low-quality (LQ) images extracted from videos, and 656 high-quality (HQ) images taken in controlled scenarios.
A  protocol similar to NoW is used for evaluation that measures the distance between all reference scan vertices to the closest points on the reconstructed mesh surface, after rigidly aligning scan and reconstruction. 
As shown in Tab.~\ref{tab:benchmarkdtirling} and the cumulative error plot in the \supmat, \modelname provides state-of-the-art performance. 

\begin{table}[]
    \centering
{\footnotesize 
	\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Method}} & 
		\multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Median (mm)\end{tabular}}} & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Mean (mm)\end{tabular}}} & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Std (mm)\end{tabular}}} \\ \cline{2-7} 
		& \textbf{LQ}  & \textbf{HQ}  & \textbf{LQ}  & \textbf{HQ}  & \textbf{LQ}  & \textbf{HQ}  \\ \hline
		3DMM-CNN~\cite{AnhTran2017}  & 1.88  & 1.85  & 2.32   & 2.29   & 1.89  & 1.88  \\ \hline
		Extreme3D~\cite{AnhTran2018} & 2.40  & 2.37  & 3.49   & 3.58   & 6.15  & 6.75  \\ \hline
		PRNet~\cite{Feng2018}        & 1.79  & 1.60  & 2.38  & 2.06  & 2.19  & 1.79 \\ \hline
		RingNet~\cite{Sanyal2019}  & 1.63  & 1.58  & 2.08  & 2.02  & 1.79  & 1.69  \\ \hline
		3DDFA-V2~\cite{guo2020towards}  & 1.62  & 1.49  & 2.10  & 1.91  & 1.87  & \textbf{1.64}  \\ \hline
		\modelname (ours)          & \textbf{1.48}   & \textbf{1.44}    & \textbf{1.91}  & \textbf{1.89}   & \textbf{1.68}    & 1.66 \\ \hline
	\end{tabular}
}
	\caption{Feng et al.~\cite{Feng2018evaluation} benchmark performance.}
	\label{tab:benchmarkdtirling}
\end{table}



\subsection{Ablation experiment}
\label{sec:ablation}

\begin{figure}[t]
    \centerline{
		\includegraphics[width=1\columnwidth, trim={0cm, 1.0cm, 0cm, 0cm}]{images/ablation/ablation.png}        
    }
    \caption{Ablation experiments. Left: Effects of  on the animation of the source identity with the source expression visualized on a neutral expression template mesh. Without , no wrinkles appear in the forehead due to the surprise source expression. Right: Effect of  on the detail reconstruction. Without , less details are reconstructed.}
	\label{fig:abl}
\end{figure}

\qheading{Detail consistency loss:}
To evaluate the importance of our novel detail consistency loss  (Eq.~\ref{loss:dc}), we train \modelname with and without .
Figure~\ref{fig:abl} (left) shows the \modelname details for detail code  from the source identity, and expression  and jaw pose parameters  from the source expression. 
For \modelname trained with  (top), wrinkles appear in the forehead as a result of the raised eyebrows of the source expression, while for \modelname trained without  (bottom), no such wrinkles appear. 
This indicates that without , person-specific details and expression-dependent wrinkles are not well disentangled. 
See \supmat~for more disentanglement results. 

\qheading{ID-MRF loss:}
Figure~\ref{fig:abl} (right) shows the effect of  on the detail reconstruction. 
Without  (middle), wrinkle details (e.g. in the forehead) are not reconstructed, resulting in an overly smooth result.
With  (right), \modelname captures the details. 











































	











































%
 \section{Conclusion and discussion}

We have presented \modelname, which enables detailed expression capture and animation from single images by learning an animatable detail model from in-the-wild images. 
In total, \modelname is trained from about 2M in-the-wild face images without 2D-to-3D supervision.
\modelname reaches state-of-the-art shape reconstruction performance enabled by a shape consistency loss. 
A novel detail consistency loss helps \modelname to disentangle expression-dependent wrinkles from person-specific details. 
The low-dimensional detail latent space makes the fine-scale reconstruction robust to noise and occlusions, and the novel loss leads to disentanglement of identity and expression-dependent wrinkle details. 
This enables applications like animation, shape change, wrinkle transfer, etc.
\modelname is publicly available for research purposes. 
Due to the reconstruction accuracy, the reliability, and the speed, \modelname is useful for applications like face reenactment or virtual avatar creation.

\modelname opens the door for future work.
First, our albedo model is dependent on the Basel face model, which lacks ethnic diversity and facial hair.  
This pushes skin tone into the lighting model and causes facial hair to be explained by shape deformations. 
We believe that we can learn a more diverse albedo model from in-the-wild images using our system.
Second, we want to extend the model over time, both for tracking and to learn more personalized models of individuals from video where we could enforce continuity of intrinsic wrinkles over time.
Third, while robust, our method can still fail due to extreme head pose and lighting. 
This suggests the need for more diverse training data.

 \section{Acknowledgements}
We thank S. Sanyal for providing us the RingNet PyTorch implementation, support with paper writing, and fruitful discussions, and M. Kocabas, N. Athanasiou, and V. Fernández Abrevaya for the helpful suggestions. 
We further thank all Perceiving Systems department members for the feedback. This work was partially supported by the Max Planck ETH Center for Learning Systems.

\qheading{Disclosure:}
MJB has received research gift funds from Intel, Nvidia, Adobe, Facebook, and Amazon. While MJB is a
part-time employee of Amazon, his research was performed solely at, and funded solely by, MPI. 
MJB has financial interests in Amazon and Meshcapade GmbH.
 
{\small
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{Abrevaya2020}
Victoria~Fern{\'a}ndez Abrevaya, Adnane Boukhayma, Philip~HS Torr, and Edmond
  Boyer.
\newblock Cross-modal deep face normals with deactivable skip connections.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 4979--4989, 2020.

\bibitem{AldrianSmith2013}
Oswald Aldrian and William~AP Smith.
\newblock Inverse rendering of faces with a {3D} morphable model.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence
  (PAMI)}, 35(5):1080--1093, 2013.

\bibitem{Bas2017fitting}
Anil Bas, William A.~P. Smith, Timo Bolkart, and Stefanie Wuhrer.
\newblock Fitting a {3D} morphable model to edges: A comparison between hard
  and soft correspondences.
\newblock In {\em Asian Conference on Computer Vision Workshops}, pages
  377--391, 2017.

\bibitem{Beeler2010}
Thabo Beeler, Bernd Bickel, Paul Beardsley, Bob Sumner, and Markus Gross.
\newblock High-quality single-shot capture of facial geometry.
\newblock {\em ACM Transactions on Graphics (TOG)}, 29(4):40, 2010.

\bibitem{Blanz2002}
Volker Blanz, Sami Romdhani, and Thomas Vetter.
\newblock Face identification across different poses and illuminations with a
  {3D} morphable model.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, pages 202--207, 2002.

\bibitem{BlanzVetter1999}
Volker Blanz and Thomas Vetter.
\newblock A morphable model for the synthesis of {3D} faces.
\newblock In {\em ACM Transactions on Graphics, (Proc. SIGGRAPH)}, pages
  187--194, 1999.

\bibitem{Brunton2014}
Alan Brunton, Augusto Salazar, Timo Bolkart, and Stefanie Wuhrer.
\newblock Review of statistical shape spaces for {3D} data with comparative
  analysis for human faces.
\newblock {\em Computer Vision and Image Understanding (CVIU)}, 128(0):1--17,
  2014.

\bibitem{Bulat2017}
Adrian Bulat and Georgios Tzimiropoulos.
\newblock How far are we from solving the {2D} \& {3D} face alignment problem?
  (and a dataset of 230,000 {3D} facial landmarks).
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 1021--1030, 2017.

\bibitem{Cao2015}
Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler.
\newblock Real-time high-fidelity facial performance capture.
\newblock {\em ACM Transactions on Graphics (TOG)}, 34(4):1--9, 2015.

\bibitem{Cao2018_VGGFace2}
Qiong Cao, Li Shen, Weidi Xie, Omkar~M Parkhi, and Andrew Zisserman.
\newblock {VGGFace2}: A dataset for recognising faces across pose and age.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, pages 67--74, 2018.

\bibitem{Cao2018}
Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Shiying Li, and Jingyi Yu.
\newblock Sparse photometric {3D} face reconstruction guided by morphable
  models.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 4635--4644, 2018.

\bibitem{NoW2019}
NoW challenge.
\newblock \url{https://ringnet.is.tue.mpg.de/challenge}, 2019.

\bibitem{Chang2018}
Feng-Ju Chang, Anh~Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, and Gerard
  Medioni.
\newblock Expnet: Landmark-free, deep, {3D} facial expressions.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, pages 122--129, 2018.

\bibitem{chaudhuri2020personalized}
Bindita Chaudhuri, Noranart Vesdapunt, Linda Shapiro, and Baoyuan Wang.
\newblock Personalized face modeling for improved face reconstruction and
  motion retargeting.
\newblock {\em arXiv preprint arXiv:2007.06759}, 2020.

\bibitem{Chen2019}
Anpei Chen, Zhang Chen, Guli Zhang, Kenny Mitchell, and Jingyi Yu.
\newblock Photo-realistic facial details synthesis from single image.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 9429--9439, 2019.

\bibitem{Chung18b}
J.~S. Chung, A. Nagrani, and A. Zisserman.
\newblock Voxceleb2: Deep speaker recognition.
\newblock In {\em INTERSPEECH}, 2018.

\bibitem{VOCA2019}
Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael
  Black.
\newblock Capture, learning, and synthesis of {3D} speaking styles.
\newblock {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 10101--10111, 2019.

\bibitem{Deng2019}
Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong.
\newblock Accurate {3D} face reconstruction with weakly-supervised learning:
  From single image to image set.
\newblock In {\em Computer Vision and Pattern Recognition Workshops}, 2019.

\bibitem{Dou2017}
Pengfei Dou, Shishir~K Shah, and Ioannis~A Kakadiaris.
\newblock End-to-end {3D} face reconstruction with deep neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 5908--5917, 2017.

\bibitem{Egger2020}
Bernhard Egger, William A.~P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael
  Zollh{\"o}fer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski,
  Sami Romdhani, Christian Theobalt, Volker Blanz, and Thomas Vetter.
\newblock {3D} morphable face models - past, present and future.
\newblock {\em ACM Transactions on Graphics (TOG)}, 2020.

\bibitem{Feng2018}
Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi Zhou.
\newblock Joint {3D} face reconstruction and dense alignment with position map
  regression network.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages
  534--551, 2018.

\bibitem{Feng2018evaluation}
Zhen-Hua Feng, Patrik Huber, Josef Kittler, Peter Hancock, Xiao-Jun Wu, Qijun
  Zhao, Paul Koppen, and Matthias R{\"a}tsch.
\newblock Evaluation of dense {3D} reconstruction from {2D} face images in the
  wild.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, 2018.

\bibitem{Garrido2016}
Pablo Garrido, Michael Zollh{\"o}fer, Dan Casas, Levi Valgaerts, Kiran
  Varanasi, Patrick P{\'e}rez, and Christian Theobalt.
\newblock Reconstruction of personalized {3D} face rigs from monocular video.
\newblock {\em ACM Transactions on Graphics (TOG)}, 35(3):28, 2016.

\bibitem{Gecer2019}
Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou.
\newblock {GANFIT:} generative adversarial network fitting for high fidelity
  {3D} face reconstruction.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 1155--1164, 2019.

\bibitem{Genova2018}
Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, and
  William~T. Freeman.
\newblock Unsupervised training for {3D} morphable model regression.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 8377--8386, 2018.

\bibitem{Gerig2018}
Thomas Gerig, Andreas Morel-Forster, Clemens Blumer, Bernhard Egger, Marcel
  Luthi, Sandro Sch{\"o}nborn, and Thomas Vetter.
\newblock Morphable face models-an open framework.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, pages 75--82, 2018.

\bibitem{Golovinskiy2006}
Aleksey Golovinskiy, Wojciech Matusik, Hanspeter Pfister, Szymon Rusinkiewicz,
  and Thomas~A. Funkhouser.
\newblock A statistical model for synthesis of detailed facial geometry.
\newblock {\em ACM Transactions on Graphics (TOG)}, 25(3):1025--1034, 2006.

\bibitem{Guler2017}
Riza~Alp G{\"u}ler, George Trigeorgis, Epameinondas Antonakos, Patrick Snape,
  Stefanos Zafeiriou, and Iasonas Kokkinos.
\newblock {DenseReg}: Fully convolutional dense shape regression in-the-wild.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 6799--6808, 2017.

\bibitem{guo2020towards}
Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan~Z Li.
\newblock Towards fast, accurate and stable {3D} dense face alignment.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2020.

\bibitem{Guo2018}
Yudong Guo, Jianfei Cai, Boyi Jiang, Jianmin Zheng, et~al.
\newblock {CNN}-based real-time dense face reconstruction with inverse-rendered
  photo-realistic face images.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence
  (PAMI)}, 41(6):1294--1307, 2018.

\bibitem{He2015DeepRL}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 770--778, 2016.

\bibitem{Hu2017}
Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jaewoo Seo, Jens Fursund,
  Iman Sadeghi, Carrie Sun, Yen-Chun Chen, and Hao Li.
\newblock Avatar digitization from a single image for real-time rendering.
\newblock {\em ACM Transactions on Graphics (TOG)}, 36(6):195:1--195:14, 2017.

\bibitem{Ichim2015}
Alexandru~Eugen Ichim, Sofien Bouaziz, and Mark Pauly.
\newblock Dynamic {3D} avatar creation from hand-held video input.
\newblock {\em ACM Transactions on Graphics (TOG)}, 34(4):45, 2015.

\bibitem{Jackson2017}
Aaron~S Jackson, Adrian Bulat, Vasileios Argyriou, and Georgios Tzimiropoulos.
\newblock Large pose {3D} face reconstruction from a single image via direct
  volumetric {CNN} regression.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 1031--1039, 2017.

\bibitem{Jeni2015}
L{\'a}szl{\'o}~A Jeni, Jeffrey~F Cohn, and Takeo Kanade.
\newblock Dense {3D} face alignment from {2D} videos in real-time.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, volume~1, pages 1--8, 2015.

\bibitem{Jiang2018}
Luo Jiang, Juyong Zhang, Bailin Deng, Hao Li, and Ligang Liu.
\newblock {3D} face reconstruction with geometry details from a single image.
\newblock {\em Transactions on Image Processing}, 27(10):4756--4770, 2018.

\bibitem{KemelmacherSeitz2011}
Ira Kemelmacher-Shlizerman and Steven~M Seitz.
\newblock Face reconstruction in the wild.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 1746--1753, 2011.

\bibitem{Kim2018}
Hyeongwoo Kim, Michael Zollh{\"o}fer, Ayush Tewari, Justus Thies, Christian
  Richardt, and Christian Theobalt.
\newblock {InverseFaceNet:} deep monocular inverse face rendering.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 4625--4634, 2018.

\bibitem{Kingma2015AdamAM}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2015.

\bibitem{Li2009RobustSG}
Hao Li, Bart Adams, Leonidas~J. Guibas, and Mark Pauly.
\newblock Robust single-view geometry and motion reconstruction.
\newblock {\em ACM Transactions on Graphics (TOG)}, 28:175, 2009.

\bibitem{Li2013}
Hao Li, Jihun Yu, Yuting Ye, and Chris Bregler.
\newblock Realtime facial animation with on-the-fly correctives.
\newblock {\em ACM Transactions on Graphics (TOG)}, 32(4):42--1, 2013.

\bibitem{FLAME2017}
Tianye Li, Timo Bolkart, Michael.~J. Black, Hao Li, and Javier Romero.
\newblock Learning a model of facial shape and expression from {4D} scans.
\newblock {\em ACM Transactions on Graphics, (Proc. SIGGRAPH Asia)}, 36(6),
  2017.

\bibitem{Li2018}
Yue Li, Liqian Ma, Haoqiang Fan, and Kenny Mitchell.
\newblock Feature-preserving detailed {3D} face reconstruction from a single
  image.
\newblock In {\em European Conference on Visual Media Production}, pages 1--9,
  2018.

\bibitem{M1966Spherical}
Claus Müller.
\newblock {\em Spherical Harmonics}.
\newblock Springer Berlin Heidelberg, 1966.

\bibitem{Nagano2018}
Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral
  Agarwal, Jens Fursund, and Hao Li.
\newblock {paGAN}: real-time avatars using dynamic textures.
\newblock {\em ACM Transactions on Graphics (TOG)}, 37(6):258:1--258:12, 2018.

\bibitem{Nirkin2018}
Yuval Nirkin, Iacopo Masi, Anh~Tran Tuan, Tal Hassner, and Gerard Medioni.
\newblock On face segmentation, face swapping, and face perception.
\newblock In {\em International Conference on Automatic Face \& Gesture
  Recognition (FG)}, pages 98--105, 2018.

\bibitem{Parke1974}
Frederick~Ira Parke.
\newblock A parametric model for human faces.
\newblock Technical report, University of Utah, 1974.

\bibitem{Paszke2019PyTorchAI}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas K{\"o}pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{Paysan2009}
Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter.
\newblock A {3D} face model for pose and illumination invariant face
  recognition.
\newblock In {\em International Conference on Advanced Video and Signal Based
  Surveillance}, pages 296--301, 2009.

\bibitem{Pighin1998}
Fr{\'e}d{\'e}ric Pighin, Jamie Hecker, Dani Lischinski, Richard Szeliski, and
  David~H. Salesin.
\newblock Synthesizing realistic facial expressions from photographs.
\newblock In {\em SIGGRAPH}, pages 75--84, 1998.

\bibitem{ravi2020pytorch3d}
Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo,
  Justin Johnson, and Georgia Gkioxari.
\newblock Pytorch3d.
\newblock \url{https://github.com/facebookresearch/pytorch3d}, 2020.

\bibitem{Richardson2016}
E. Richardson, M. Sela, and R. Kimmel.
\newblock {3D} face reconstruction by learning from synthetic data.
\newblock In {\em {3D} Vision}, pages 460--469, 2016.

\bibitem{Richardson2017}
Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel.
\newblock Learning detailed face reconstruction from a single image.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 1259--1268, 2017.

\bibitem{Romdhani2002}
Sami Romdhani, Volker Blanz, and Thomas Vetter.
\newblock Face identification by fitting a {3D} morphable model using linear
  shape and texture error functions.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 3--19,
  2002.

\bibitem{RomdhaniVetter2005}
S. Romdhani and T. Vetter.
\newblock Estimating {3D} shape and texture using pixel intensity, edges,
  specular highlights, texture constraints and a prior.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, volume~2, pages 986--993, 2005.

\bibitem{Roth2016}
Joseph Roth, Yiying Tong, and Xiaoming Liu.
\newblock Adaptive {3D} face reconstruction from unconstrained photo
  collections.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 4197--4206, 2016.

\bibitem{Saito2017}
Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and Hao Li.
\newblock Photorealistic facial texture inference using deep neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 5144--5153, 2017.

\bibitem{Sanyal2019}
Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael Black.
\newblock Learning to regress {3D} face shape and expression from an image
  without {3D} supervision.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 7763--7772, 2019.

\bibitem{Scherbaum2011}
Kristina Scherbaum, Tobias Ritschel, Matthias Hullin, Thorsten Thorm{\"a}hlen,
  Volker Blanz, and Hans-Peter Seidel.
\newblock Computer-suggested facial makeup.
\newblock {\em Computer Graphics Forum}, 30(2):485--492, 2011.

\bibitem{Sela2017}
Matan Sela, Elad Richardson, and Ron Kimmel.
\newblock Unrestricted facial geometry reconstruction using image-to-image
  translation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 1576--1585, 2017.

\bibitem{Sengupta2018}
Soumyadip Sengupta, Angjoo Kanazawa, Carlos~D. Castillo, and David~W. Jacobs.
\newblock {SfSNet:} learning shape, reflectance and illuminance of faces in the
  wild.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 6296--6305, 2018.

\bibitem{Shi2014}
Fuhao Shi, Hsiang-Tao Wu, Xin Tong, and Jinxiang Chai.
\newblock Automatic acquisition of high-fidelity facial performances using
  monocular videos.
\newblock {\em ACM Transactions on Graphics (TOG)}, 33(6):222, 2014.

\bibitem{Shin2014}
Il-Kyu Shin, A~Cengiz {\"O}ztireli, Hyeon-Joong Kim, Thabo Beeler, Markus
  Gross, and Soo-Mi Choi.
\newblock Extraction and transfer of facial expression wrinkles for facial
  performance enhancement.
\newblock In {\em Pacific Conference on Computer Graphics and Applications},
  pages 113--118, 2014.

\bibitem{Simonyan2014VeryDC}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em CoRR}, abs/1409.1556, 2014.

\bibitem{Slossberg2018}
Ron Slossberg, Gil Shamai, and Ron Kimmel.
\newblock High quality facial surface and texture synthesis via generative
  adversarial networks.
\newblock In {\em European Conference on Computer Vision Workshops (ECCV-W)},
  2018.

\bibitem{Suwajanakorn2014}
Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and Steven~M Seitz.
\newblock Total moving face reconstruction.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages
  796--812, 2014.

\bibitem{Tewari2019}
Ayush Tewari, Florian Bernard, Pablo Garrido, Gaurav Bharaj, Mohamed Elgharib,
  Hans-Peter Seidel, Patrick P{\'e}rez, Michael Zollh{\"o}fer, and Christian
  Theobalt.
\newblock {{FML:} Face Model Learning from Videos}.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 10812--10822, 2019.

\bibitem{Tewari2018}
Ayush Tewari, Michael Zollh{\"o}fer, Pablo Garrido, Florian Bernard, Hyeongwoo
  Kim, Patrick P{\'e}rez, and Christian Theobalt.
\newblock Self-supervised multi-level face model learning for monocular
  reconstruction at over 250 {Hz}.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 2549--2559, 2018.

\bibitem{Tewari2017}
Ayush Tewari, Michael Zollh{\"o}fer, Hyeongwoo Kim, Pablo Garrido, Florian
  Bernard, Patrick Perez, and Christian Theobalt.
\newblock {MoFA:} model-based deep convolutional face autoencoder for
  unsupervised monocular reconstruction.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 1274--1283, 2017.

\bibitem{Thies2015}
Justus Thies, Michael Zollh{\"o}fer, Matthias Nie{\ss}ner, Levi Valgaerts, Marc
  Stamminger, and Christian Theobalt.
\newblock Real-time expression transfer for facial reenactment.
\newblock {\em ACM Transactions on Graphics (TOG)}, 34(6):183--1, 2015.

\bibitem{Thies2016}
Justus Thies, Michael Zollh{\"o}fer, Marc Stamminger, Christian Theobalt, and
  Matthias Nie{\ss}ner.
\newblock {Face2Face}: Real-time face capture and reenactment of {RGB} videos.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 2387--2395, 2016.

\bibitem{AnhTran2017}
Anh~Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard Medioni.
\newblock Regressing robust and discriminative {3D} morphable models with a
  very deep neural network.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 1599--1608, 2017.

\bibitem{AnhTran2018}
Anh~Tuan Tran, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, and G{\'e}rard
  Medioni.
\newblock Extreme {3D} face reconstruction: Seeing through occlusions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 3935--3944, 2018.

\bibitem{LuanTran2019}
Luan Tran, Feng Liu, and Xiaoming Liu.
\newblock Towards high-fidelity nonlinear {3D} face morphable model.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 1126--1135, 2019.

\bibitem{Tu2019}
Xiaoguang Tu, Jian Zhao, Zihang Jiang, Yao Luo, Mei Xie, Yang Zhao, Linxiao He,
  Zheng Ma, and Jiashi Feng.
\newblock Joint {3D} face reconstruction and dense face alignment from a single
  image with {2D}-assisted self-supervised learning.
\newblock {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2019.

\bibitem{VetterBlanz1998}
Thomas Vetter and Volker Blanz.
\newblock Estimating coloured {3D} face models from single images: An example
  based approach.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages
  499--513, 1998.

\bibitem{Wang_2019_ICCV}
Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang.
\newblock Racial faces in the wild: Reducing racial bias by information
  maximization adaptation network.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, October 2019.

\bibitem{Wang2018}
Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia.
\newblock Image inpainting via generative multi-column convolutional neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 331--340, 2018.

\bibitem{Wei2019}
Huawei Wei, Shuang Liang, and Yichen Wei.
\newblock {3D} dense face alignment via graph convolution networks.
\newblock {\em arXiv preprint arXiv:1904.05562}, 2019.

\bibitem{Weise2011}
Thibaut Weise, Sofien Bouaziz, Hao Li, and Mark Pauly.
\newblock Realtime performance-based facial animation.
\newblock {\em ACM Transactions on Graphics, (Proc. SIGGRAPH)}, 30(4):77, 2011.

\bibitem{Yamaguchi2018}
Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie Zhao, Weikai Chen, Kyle
  Olszewski, Shigeo Morishima, and Hao Li.
\newblock High-fidelity facial reflectance and geometry inference from an
  unconstrained image.
\newblock {\em ACM Transactions on Graphics (TOG)}, 37(4):162:1--162:14, 2018.

\bibitem{yang2020facescape}
Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and
  Xun Cao.
\newblock {FaceScape}: a large-scale high quality {3D} face dataset and
  detailed riggable {3D} face prediction.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 601--610, 2020.

\bibitem{Zeng2019}
Xiaoxing Zeng, Xiaojiang Peng, and Yu Qiao.
\newblock {DF2Net}: A dense-fine-finer network for detailed {3D} face
  reconstruction.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2019.

\bibitem{Zhu2015}
Xiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan~Z Li.
\newblock High-fidelity pose and expression normalization for face recognition
  in the wild.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 787--796, 2015.

\bibitem{Zollhoefer2018}
M. {Zollh{\"o}fer}, J. {Thies}, P. {Garrido}, D. {Bradley}, T. {Beeler}, P.
  P{\'e}rez, M. {Stamminger}, M. Nie{\ss}ner, and C. {Theobalt}.
\newblock State of the art on monocular {3D} face reconstruction, tracking, and
  applications.
\newblock {\em Computer Graphics Forum (Eurographics State of the Art Reports
  2018)}, 37(2), 2018.

\end{thebibliography}
 \balance
}

\newpage


\begin{appendices}
\section{Implementation Details}

\qheading{Data:}
\modelname is trained on 2 Million images from \mbox{VGGFace2}~\cite{Cao2018_VGGFace2} and BUPT-Balancedface~\cite{Wang_2019_ICCV} and VoxCeleb2~\cite{Chung18b}.
From VGGFace2~\cite{Cao2018_VGGFace2}, we randomly select  images such that  images are of resolution higher than , and  are of lower resolution.
From BUPT-Balancedface~\cite{Wang_2019_ICCV} we randomly sample  with Asian or African ethnicity labels to reduce the ethnicity bias of VGGFace2. 
From VoxCeleb2~\cite{Chung18b} we choose  frames, with multiple samples from the same video clip per subject to obtain data with variation only in the facial expression and head pose.
We also sample  images from the VGGFace2~\cite{Cao2018_VGGFace2} test set for validation. 

\qheading{Data cleaning:}
We generate a different crop for the face image by shifting the provided bounding box by  to the bottom right (i.e.~shift by , where  and  denote the bounding box width and height).
Then we expand the original and the shifted bounding boxes by  to the top, and by  to the left, right, and bottom.
We run FAN~\cite{Bulat2017}, providing the expanded bounding boxes as input and discard all images with , where  and  are the th landmarks for the original and the shifted bounding box, respectively, and  denote the normalization matrix .

\qheading{Training details:}
We pre-train the coarse model (i.e.~) for two epochs with a batch size of  with , , , and .
Then, we train the coarse model for  epochs with a batch size of , with  images per subject with , , , , ,  , and .
The landmark loss uses different weights for individual landmarks, the mouth corners and the nose tip landmarks are weighted by a factor of , other mouth and nose landmarks with a factor of , and all remaining landmarks have a weight of .
This is followed by training the detail model (i.e.~ and ) on VGGFace2 and VoxCeleb2 with a batch size of , with  images per subject, and parameters , , , , and .
The coarse model is fixed while training the detail model. 



\section{Evaluation}



\subsection{Detail animation}

\begin{figure}[ht]
	\centering
        \includegraphics[width=0.99\columnwidth]{images/qualitative/qualitative_animation_v3_1.png}
	\caption{Effect of \modelname's animatable details. Given a single image (top), \modelname reconstructs a detailed mesh (second row). Using static details and animating the coarse FLAME shape only (third row) results in visible artifacts as highlighted by the red boxes. Instead, reposing with \modelname's animatable details (bottom) results in a more realistic mesh with geometric details.}
	\label{fig:animation1}
\end{figure}

As described in Section 6.1 and shown in Figure 6 of the main paper, using a static displacement map to model geometric details instead of \modelname's animatable details results in visible artifacts.
Figure~\ref{fig:animation1} shows more examples where using static details results in artifacts in the mouth corner or the forehead region, while  \modelname's animated results look plausible. 



\subsection{Quantitative evaluation}

\begin{figure*}[t]
    \begin{tabular}{c@{}c@{}c}
        \includegraphics[width=0.33\textwidth]{images/DECA_CVPR_NoW_test.pdf} &  
        \includegraphics[width=0.33\textwidth]{images/DECA_CVPR_Stirling_LQ.pdf} & 
        \includegraphics[width=0.33\textwidth]{images/DECA_CVPR_Stirling_HQ.pdf} \\
         NoW~\cite{Sanyal2019} &
         Feng et al.~\cite{Feng2018evaluation} LQ &
         Feng et al.~\cite{Feng2018evaluation} HQ
    \end{tabular}
	\caption{Quantitative comparison to state-of-the-art on two 3D face reconstruction benchmarks, namely the NoW~\cite{Sanyal2019} challenge (left) and the Feng et al.~\cite{Feng2018evaluation} benchmark for low-quality (middle) and high-quality (right) images.}
	\label{fig:cumulative_error}
\end{figure*}

As described in Section 6.2 of the main paper, we quantitatively compare \modelname with publicly available methods, namely 3DDFA-V2~\cite{guo2020towards}, Deng et al.~\cite{Deng2019}, RingNet~\cite{Sanyal2019}, PRNet~\cite{Feng2018}, 3DMM-CNN~\cite{AnhTran2017} and Extreme3D~\cite{AnhTran2018} on two existing 3D face reconstruction benchmarks, the NoW challenge~\cite{Sanyal2019} and the Feng et al.~\cite{Feng2018evaluation} benchmark.
The left of Figure~\ref{fig:cumulative_error} shows the cumulative errors for Table 1 of the main paper, the middle and right of Figure~\ref{fig:cumulative_error} show the cumulative errors for Table 2 of the main paper. 
Note that in all cases, the \modelname curve in dark blue is aboth that of the other methods.
This demonstrates that \modelname gives state-of-the-art reconstruction performance for both benchmarks.



\subsection{Qualitative comparisons}
\begin{figure*}[t]
    \offinterlineskip
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01198.jpg}\\ 
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01210.jpg}\\ 
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01238.jpg}\\ 
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01247.jpg}\\ 
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01262.jpg}\\ 
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01282.jpg}\\ 
    \includegraphics[width=0.99\textwidth]{images/all_methods_comparison/image01283.jpg} 
    \begin{tabular}{ccccccccc}
      \hspace{0.025\textwidth}  (a) \hspace{\varhspace\textwidth} & (b) \hspace{\varhspace\textwidth} & (c) \hspace{\varhspace\textwidth} & (d) \hspace{\varhspace\textwidth} & (e) \hspace{\varhspace\textwidth} & (f) \hspace{\varhspace\textwidth} & (g) \hspace{\varhspace\textwidth} & (h) \hspace{\varhspace\textwidth} & (i) \\
    \end{tabular}
    \caption{Comparison to previous work, from left to right: (a) Input image, (b) 3DDFA-V2~\cite{guo2020towards}, (c) FaceScape~\cite{yang2020facescape}, (d) Extreme3D~\cite{AnhTran2018}, (e) PRNet~\cite{Feng2018}, (f) Deng et al.~\cite{Deng2019}, (g) Cross-modal~\cite{Abrevaya2020}, (h) \modelname detail reconstruction, and (i) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh.}
    \label{fig:qualitative_all}
\end{figure*}

Figure~\ref{fig:qualitative_all} shows additional qualitative comparisons to existing coarse and detail reconstruction methods.
\modelname better reconstructs the overall face shape than all existing methods, it reconstructs more details than existing coarse reconstruction methods (e.g. (b), (e), (f)), and it is more robust to occlusions compared to existing detail reconstruction methods (e.g. (c), (d), (g)). 

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_01.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_02.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh.}
    \label{fig:ALFW1}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_03.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_04.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh.}
    \label{fig:ALFW2}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_05.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_06.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. }
    \label{fig:ALFW3}    
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_07.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_08.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. }
    \label{fig:ALFW4}    
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_09.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_10.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. }
    \label{fig:ALFW5}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_11.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_12.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. }
    \label{fig:ALFW6}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_13.jpg}
    \includegraphics[width=\imsize\textwidth]{images/supmat/ALFW_fused_14.jpg}
    \caption{Qualitative comparisons on random ALFW2000~\cite{Zhu2015} samples. a) Input image, b) 3DDFA-V2~\cite{guo2020towards}, c) FaceScape~\cite{yang2020facescape}, d) Extreme3D~\cite{AnhTran2018}, e) \modelname detail reconstruction, and f) reposing (animation) of \modelname's detail reconstruction to a common expression. The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. }
    \label{fig:ALFW7}
\end{figure*}

As promised in the main paper (e.g.~Section 6.1), we show results for more than 200 randomly selected ALFW2000~\cite{Zhu2015} samples in Figures~\ref{fig:ALFW1}, \ref{fig:ALFW2}, \ref{fig:ALFW3}, \ref{fig:ALFW4}, \ref{fig:ALFW5}, \ref{fig:ALFW6}, and \ref{fig:ALFW7}.
For each sample, we compare the \modelname's detail reconstruction (e) with the state-of-the-art coarse reconstruction method 3DDFA-V2~\cite{guo2020towards} (see (b)) and existing detail reconstruction methods, namely FaceScape~\cite{yang2020facescape} (see (c)), and Extreme3D~\cite{AnhTran2018} (see (e)).
 In total, \modelname reconstructs more details then 3DDFA-V2, and it is more robust to occlusions than FaceScape and Extreme3D. 
 Further, the \modelname retargeting results appear realistic.

\end{appendices} 
\end{document}
