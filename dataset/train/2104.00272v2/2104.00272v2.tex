

\begin{figure*}
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w10_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w10_004.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w10_007.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w15_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w15_004.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w15_007.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w20_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w20_004.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/b_w20_007.jpg_graphormer_prediction.jpg}
\vspace{-2mm}
\setlength{\tabcolsep}{30pt}
\begin{tabular}{cccccc}
Input & Output &  Input & Output &  Input & Output
\end{tabular}
\caption{
Qualitative results of our method. There is a hand with an orange. We demonstrate the robustness of our model by adding artificial occlusions including black vertical stripes to the images. We can see that Graphormer reconstructs plausible hand mesh under the occlusion scenarios. Please see \href{https://github.com/microsoft/MeshGraphormer/blob/main/docs/Fig1.gif}{\texttt{Fig1.gif}} for more detailed video results.
} 
\label{fig:vis_hand_1}
\end{center}
\end{figure*} 



\begin{figure*}
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w10_000.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w10_006.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w10_008.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w15_000.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w15_006.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w15_008.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w20_000.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w20_006.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/d_w20_008.jpg_graphormer_prediction.jpg}
\vspace{-2mm}
\setlength{\tabcolsep}{30pt}
\begin{tabular}{cccccc}
Input & Output &  Input & Output &  Input & Output
\end{tabular}
\caption{
Qualitative results of our method. There is a hand holding a banana. We do not have any banana training images. However, Graphormer generalizes to the novel object, and creates the hand mesh with the correct pose. Please see \href{https://github.com/microsoft/MeshGraphormer/blob/main/docs/Fig2.gif}{\texttt{Fig2.gif}} for more detailed video results.
} 
\label{fig:vis_hand_2}
\end{center}
\end{figure*} 

\begin{figure*}
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w10_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w10_004.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w10_007.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w15_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w15_004.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w15_007.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w20_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w20_004.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/a_w20_007.jpg_graphormer_prediction.jpg}
\vspace{-2mm}
\setlength{\tabcolsep}{30pt}
\begin{tabular}{cccccc}
Input & Output &  Input & Output &  Input & Output
\end{tabular}
\caption{
Qualitative results of our method. There is a hand holding an ice cream cone. The ice cream cone is a novel object unseen in training, and the hand pose is also object-specific. We can see that Graphormer works reasonably well for the test images. Please see  \href{https://github.com/microsoft/MeshGraphormer/blob/main/docs/Fig3.gif}{\texttt{Fig3.gif}} for more detailed video results.
} 
\label{fig:vis_hand_3}
\vspace{-2mm}
\end{center}
\end{figure*} 

\begin{table*}
\centering
\begin{tabular}{lccc}
    \toprule
	Method  & Para (M) &  Para (M) & PAMPJPE  \\
	\midrule
	Graphormer - GRB &  &  & \\
	Graphormer - GRB + MLP1 &  &  & \\
    Graphormer - GRB + MLP2 &  &  & \\
	\midrule
	Graphormer &  &  & \\
	\bottomrule
\end{tabular}
\caption{Comparison between the use of MLP and GRB.}
\label{tbl:mlp}
\vspace{-2mm}
\end{table*}


\begin{figure*}
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w10_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w10_006.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w10_009.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w15_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w15_006.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w15_009.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w20_001.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w20_006.jpg_graphormer_prediction.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.33\textwidth]{supp-figs/e_w20_009.jpg_graphormer_prediction.jpg}
\vspace{-2mm}
\setlength{\tabcolsep}{30pt}
\begin{tabular}{cccccc}
Input & Output &  Input & Output &  Input & Output
\end{tabular}
\caption{
Qualitative results of our method. It is a hand holding half an orange. Graphormer is able to reconstruct a reasonable hand mesh even though most of the fingers are invisible. Please see \href{https://github.com/microsoft/MeshGraphormer/blob/main/docs/Fig4.gif}{\texttt{Fig4.gif}} for more detailed video results.
} 
\label{fig:vis_hand_4}
\end{center}
\end{figure*}

\section{\camready{Qualitative Comparison}}

\camready{Figure~\ref{fig:occlusions} shows qualitative results of Graphormer compared with METRO~\cite{lin2020end} in the scenario of heavy occlusions. We can see that both methods are quite robust against occlusions, but Graphormer generates better head and body poses. At the top right of Figure~\ref{fig:occlusions}, almost half of the subject is occluded. Graphormer reconstructs a human mesh with more accurate head/body pose compared to METRO. At the bottom right of Figure~\ref{fig:occlusions}, the subject is occluded by the car door. We see Graphormer reconstructs a more reasonable body shape. At the bottom left, the subject is standing behind the fence. Our method reconstructs a human mesh with the two legs better aligned with the image. The results demonstrate the effectiveness of the proposed method.}

\section{Additional Qualitative Results}

Further, to demonstrate the robustness and generalization capability of our model to challenging scenarios, we test our model on the hand images that are collected from the Internet. The images have severe occlusions with different objects. 

To make the task even more difficult, we create artificial occlusions including black vertical stripes to cover one or two fingers, or part of the palm of the hand in the test images. Please note that the artificial occlusions are only used in the inference stage. We do not use any artificial occlusions in training.

In Figure~\ref{fig:vis_hand_1}, Figure~\ref{fig:vis_hand_2}, Figure~\ref{fig:vis_hand_3}, and Figure~\ref{fig:vis_hand_4}, we show the input images and our reconstructed hand meshes. For each figure, the top row shows the occlusion scenario with narrow black stripes. From the second row to the bottom, we gradually increase the width of the stripes to occlude more fingers or more parts of the hand. 

Figure~\ref{fig:vis_hand_1} shows a hand with an orange. Our model is able to reconstruct a reasonable hand mesh, even if the hand is severely obscured by the vertical stripes. The results show that Graphormer is to some extent robust to the artificial occlusion patterns.

In Figure~\ref{fig:vis_hand_2}, there is a hand grasping a banana. Although the banana is a novel object unseen in training and a large portion of the fingers are occluded by the banana, Graphormer successfully reconstructs the hand mesh under various occlusion scenarios. This demonstrates the generalization ability of our proposed Graphormer.

Figure~\ref{fig:vis_hand_3} shows a hand with an ice cream cone. Please note that the ice cream cone is unseen during training, and the interaction between the hand and the ice cream cone is complex. However, our model generalizes well to the test images. Even though the occlusions by the ice cream cone are severe and sometimes most of the fingers are occluded by the black vertical stripes, Graphormer still reconstructs a reasonable hand shape with object-specific grasp. 

Figure~\ref{fig:vis_hand_4} shows a hand with half an orange. Most of the fingers are invisible in this image. However, our model creates a hand mesh with the correct hand pose.

We further present the video results of Figure~\ref{fig:vis_hand_1}, Figure~\ref{fig:vis_hand_2}, Figure~\ref{fig:vis_hand_3}, and Figure~\ref{fig:vis_hand_4}. Please find the video results in the attached GIF files. 



\section{\camready{Comparison between MLP and GCN}}

\camready{In this section, we replace graph residual block with MLPs, and study the performance of the use of MLPs with a similar or larger model size.}

\camready{In Table~\ref{tbl:mlp}, the first row corresponds to the baseline transformer that uses image grid features. In the second and the third rows, we gradually increase the hidden size of the MLP module in the transformer, but we do not achieve any gain in performance. The bottom row of Table~\ref{tbl:mlp} shows the results of our Graphormer. As can be seen, adding graph convolutions has a slight increase of 0.04M parameters, but it improves performance significantly from 35.9 to 34.5 PA-MPJPE.} 

\begin{figure}
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.9\columnwidth]{./rebuttal_figs/attention_wo_scale_label.jpg}
\caption{
Attention map without color normalization. Graphormer pays more attentions to the lower left leg compared to METRO.} 
\label{fig:att}
\vspace{-2mm}
\end{center}
\end{figure}

\begin{figure*}[t]
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.8\textwidth]{rebuttal_figs/different_design_choices.png}
\caption{
Three design options we have studied for building our proposed Graphormer Encoder. The designs are inspired by language and speech literature~\cite{gulati2020conformer,wu2020lite}.} 
\label{fig:different_choices}
\vspace{-2mm}
\end{center}
\end{figure*}

\section{\camready{Design Options of Graphormer Encoder}}

\camready{In Figure~\ref{fig:different_choices}, we graphically illustrate three design options of the Graphormer encoder we have studied in the paper. Please refer to Table 6 in our main paper for the performance comparisons between the design options. We observe that placing graph convolutions after MHSA works better than other design options for the reconstruction of human mesh.}


\section{\camready{Discussion of Attention Map}}
\camready{Please note that the attention colors in the paper's diagrams are normalized based on the maximum attention value. Because the maximum attention value for Graphormer is smaller, so the overall colors are lighter. We attach the two diagrams without color normalization in Figure~\ref{fig:att}. We see that both methods pay similar attention on the left arm and right foot, while Graphormer also attends to the left lower leg.} 

\section{\camready{Discussion of Camera Parameters}}

\camready{We learn camera parameters for a weak perspective camera model. Following~\cite{kolotouros2019convolutional}, we predict a scaling factor  and a 2D translation vector . Please note that the model prediction is already in the camera coordinate system, thus we don't have to compute global camera rotation. The camera parameters are learned via 2D pose re-projection optimization. It doesn't require any GT camera parameters.}

\begin{table}
\centering
\begin{tabular}{lccc}
    \toprule
	 & HRNet & Transformer & Graphormer\\
	\midrule
	
	\# Parameters (M) &  &  &  \\ 
	GFLOPs &  &  &  \\ 
	\bottomrule
\end{tabular}
\caption{Number of parameters and computational complexity in terms of GFLOPs.}
\label{tbl:complexity}
\end{table}



\section{\camready{Training Time}}
\camready{We conducted experiments on a machine with 8 NVIDIA V100 GPUs. We use a batch size of 32. For each epoch, our training takes about 35 minutes. We train the proposed model for 200 epochs. The overall training takes 5 days.}

\section{Computational Costs}
Since we inject graph convolutions into the transformer, one may wonder about the computational costs of the proposed Graphormer. To answer the question, we report the number of parameters and the computational complexity in terms of GFLOPs. 

Table~\ref{tbl:complexity} shows the comparison between the conventional transformer and the proposed Graphormer. We also report the computational cost of the HRNet CNN backbone for reference. As we can see, adding graph convolutions has a slight increase of M parameters and  GFLOPs compared to the conventional transformer. The results suggest that little complexity has been added to the transformer architecture. However, Graphormer significantly improves the state-of-the-art performance across multiple benchmarks. This verifies the effectiveness of the proposed method.

\camready{Please note that the total parameters of our end-to-end pipeline is the sum of HRNet and Graphormer.}

\section{\camready{Limitation}}
\camready{We observed that our method may not work well if the reconstruction target is out of the view. For example, as shown in Figure~\ref{fig:limitation}(a), when the majority of the human body is not in the input image, our method fails to estimate a correct human mesh. This is probably due to the lack of out-of-the-view 3D training data in our training set. In Figure~\ref{fig:limitation}(b), only two hands are visible and the rest of the human body is out of the view. Our method does not work well in this case. We plan to address this issue in our future work.
}




\begin{figure}
\begin{center}
(a) Example1
\includegraphics[width=1.\columnwidth]{./supp-figs/graphormer_fail2.jpg}\\
\setlength{\tabcolsep}{45.0pt}
\begin{tabular}{cc}
Input & Output\\
\end{tabular}
(b) Example2\\
\includegraphics[width=1.\columnwidth]{./supp-figs/graphormer_fail0.jpg}\\
\setlength{\tabcolsep}{45.0pt}
\begin{tabular}{cc}
Input & Output\\
\end{tabular}
\caption{
Failure cases. Mesh Graphormer may not work well if the reconstruction target is out of the view.
} 
\vspace{-0mm}
\label{fig:limitation}
\end{center}
\end{figure}