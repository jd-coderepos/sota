\documentclass[11pt]{amsart}
\usepackage[text={6.5in,9in},centering]{geometry}
 \geometry{letterpaper}                   \usepackage{graphicx}
\usepackage{times}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{epstopdf}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage}
\usepackage[foot]{amsaddr}
\usepackage{tikz}

\usepackage{verbatim}
\usetikzlibrary{arrows,shapes}

\newcounter{foo}
\newtheorem{theorem}[foo]{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{conjecture}[lemma]{Conjecture}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}[foo]{Proposition}
\newtheorem{claim}[lemma]{Claim}


\newcommand{\remark}{{\bf Remark\hspace{2em}}}
\newcommand{\proofstart}{{\bf Proof\hspace{2em}}}
\newcommand{\proofend}{\hspace*{\fill}\mbox{}}

\addtolength{\textheight}{0.05cm}



\usepackage{url}

\def\calL{{\mathcal{L}}}
\def\calI{{\mathcal{L}}}
\def\calD{{\mathcal{D}}}
\def\calE{{\mathcal{E}}}
\def\calU{{\mathcal{U}}}
\def\calM{{\mathcal{M}}}
\def\calT{{\mathcal{T}}}
\def\calP{{\mathcal{P}}}
\def\calL{{\mathcal{L}}}
\def\calQ{{\mathcal{Q}}}
\def\calS{{\mathcal{S}}}
\def\calX{{\mathcal{X}}}

\newcommand{\reals}{\mathbb{R}}

\newcommand{\prob}[1]{\textsc{#1}}
\newcommand{\Capacity}{\prob{Capacity}}
\newcommand{\capacity}{\Capacity}
\newcommand{\Scheduling}{\prob{Scheduling}}
\newcommand{\scheduling}{\Scheduling}
\newcommand{\conn}{\prob{Connectivity}}
\newcommand{\connectivity}{\conn}
\newcommand{\Connectivity}{\conn}
\newcommand{\alg}[1]{\textbf{#1}}

\newcommand{\geomodel}{\textsc{geo-SINR}}
\newcommand{\decaymodel}{\textsc{mb-SINR}}

\newcommand{\define}[1]{\emph{#1}} \newcommand{\degree}{^\circ}

\newcommand{\mypara}[1]{\smallskip\noindent\textbf{#1.}}  \newcommand{\tightpara}[1]{\noindent\textbf{#1.}}  \newcommand{\inddim}{D}  

\newcommand{\todo}[1]{\textsf{TODO: #1}}
\newcommand{\authorcomment}[1]{}


\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\urldef{\ourmail}\url{{mbodlaender,magnusmh}@gmail.com}

\title{Beyond Geometry : Towards Fully Realistic Wireless Models
\thanks{ICE-TCS, School of Computer Science, Reykjavik University, 101 Reykjavik, Iceland. \url{
\{mbodlaender,magnusmh\}@gmail.com}. tel: +354-825-6384.
Supported by Icelandic Research Fund grant-of-excellence no.~120032011.}}
\author{Marijke H.L. Bodlaender \and Magn\'us M. Halld\'orsson}

\begin{document}

\begin{titlepage}

\date{\today}
\thispagestyle{empty}

\begin{abstract}
Signal-strength models of wireless communications capture the gradual
fading of signals and the additivity of interference. As such, they
are closer to reality than other models. However, nearly all theoretic
work in the SINR model depends on the assumption of smooth geometric
decay, one that is true in free space but is far off in actual
environments. The challenge is to model realistic environments,
including walls, obstacles, reflections and anisotropic antennas,
without making the models algorithmically impractical or analytically
intractable.

We present a simple solution that allows the modeling of arbitrary
static situations by moving from geometry to arbitrary \emph{decay
  spaces}.  The complexity of a setting is captured by a
\emph{metricity} parameter  that indicates how far the decay
space is from satisfying the triangular inequality.  All results that
hold in the SINR model in general metrics carry over to decay spaces,
with the resulting time complexity and approximation depending on
 in the same way that the original results depends on the path
loss term .  For distributed algorithms, that to date have
appeared to necessarily depend on the planarity, we indicate how they
can be adapted to arbitrary decay spaces at a cost in time complexity
that depends on a \emph{fading} parameter of the decay space. In
particular, for decay spaces that are \emph{doubling}, the parameter
is constant-bounded.

Finally, we explore the dependence on  in the approximability
of core problems. In particular, we observe that the capacity maximization problem 
has exponential upper and lower bounds in terms of  in general
decay spaces. In Euclidean metrics and related growth-bounded decay
spaces, the performance depends on the exact metricity definition, 
with a polynomial upper bound in terms of , but an exponential
lower bound in terms of a variant parameter .
On the plane, the upper bound result actually yields the first approximation of 
a capacity-type SINR problem that is subexponential in .
\end{abstract}

\maketitle

\vspace{4em}



\end{titlepage}

\section{Introduction}
\label{sec:intro}

Signal-strength models of wireless communications capture the gradual
fading of signals and the additivity of interference. As such, they
are closer to reality than other models. In spite of the apparent
great complexity of such models, various fundamental problems have
been resolved analytically in recent years. These also seem essential for
studying certain properties of wireless networks, such as capacity
\cite{kumar00}, or connectivity and aggregation, which can be achieved
in logarithmic rounds in worst case \cite{MoWa06,SODA12}.

Nearly all theoretic work in signal-strength models have been done in
the ``SINR model'' that assumes that signals decay as a smooth
polynomial function of distance. We shall refer to this as the
{\geomodel} model. This assumption about decay (or \emph{path loss})
is true in free space, but turns out to be far off in actual
environments, as shown by a long history of experimental studies
(e.g., \cite{kotz2004experimental}). Quoting a recent meta-study,
\cite{baccour2012radio}, ``link quality is not correlated with distance.'' 
Experimental studies have long ago jettisoned the geometric path loss
assumption. 
This questions the wisdom of studying ``SINR models'' analytically, given the added
effort and complexity.

One hope might be that results in the ``basic SINR model'' could
eventually carry some insights that would be of use in more detailed
models that capture more of reality. Yet, there are no proposed
intermediate models, and real environments consist of 
assortments of walls, ceilings and obstacles,
as well as complex interactions involving reflections, shadowing, 
multi-path signals, and anisotropic (or even directional) antennas.
It might seem near impossible to capture this all without 
making the resulting models hopelessly impractical for algorithm
design and/or analytically intractable. 

\iffalse
\textbf{Intro}
\begin{itemize}
\item Signal-strength (SS) wireless models: Recent results; theoretically allow efficient scheduling/capacity/aggregation; more realistic (capture additivity and fading). 
\item Geometric decay: The Achilles heel of SS-models. Known to be problematic; theoreticians hope that the simplified models yield intrinsic understanding. 
\item Capturing realistic environments: Walls, indoor, outdoor,
  antennas, fading, anything. Directional antennas. 
Experimental: Moved away from prescriptive models
The other parts of the SINR model match well experimentally.
\item Challenge to theory: Extend the models without complicating them; 
 generality, avoid reinventing; 
Good models: faithful, general, analyzable, algorithmic.
(Chasm between experimental and analytic wireless research)
\end{itemize}
\fi

\tightpara{Our contributions}
We present a simple solution that allows the modeling of arbitrary static
situations by moving from geometry to arbitrary \emph{decay spaces}.
The decay between two ordered nodes is the reduction in the strength
of a signal sent from the first node to the second.
By signal-strength measurements, that almost any cheap node can
perform today, these decays capture the \emph{truth on the ground}.
The complexity of a setting is captured by a
\emph{metricity} parameter  that indicates how far the decay
space is from satisfying the triangular inequality.

\emph{All} results that hold in the SINR model in general metrics carry over
to decay spaces, with the resulting time complexity and approximation
depending on  in the same way that the original results depends
on the path loss term .


For distributed algorithms, that to date have appeared to necessarily depend on
the planarity, we introduce a \emph{fading} parameter of the decay space
and indicate they can be adapted to arbitrary decay spaces
at a cost in time complexity that depends on a \emph{fading} parameter
of the decay space. In particular, for decay spaces that are
\emph{doubling}, the parameter is constant-bounded.

Finally, we explore the dependence on  in the approximability
of core problems. In particular, we observe that the {\capacity} problem 
has exponential upper and lower bounds in terms of  in general
decay spaces. In Euclidean metrics and related growth-bounded decay
spaces, the performance depends on the exact metricity definition, 
with a polynomial upper bound in terms of , but an exponential
lower bound in terms of a variant parameter .

One may ask if we are being led to yet another model that will later been shown unrealistic. Fortunately, numerous experimental studies have verified the remaining key assumptions in wide range of situations and technology \cite{son2006,MaheshwariJD2008,chen2010,sevani2012sir,us:ICDCS14}: additivity of interference, SINR capture effectiveness (the near-thresholding relationship between SINR level and packet reception rate), and invariability of wireless conditions in static environments. Thus, we may finally be reaching a wireless model that is a close approximation of reality, yet usable algorithmically and analytically.
That said, one should not discount the value of abstractions or the potentially value of simple models. Also, modeling dynamic and mobile situations, which is outside the scope of our work, remains a highly important (and largely open) issue.

\iffalse On the other hand, allowing for \emph{arbitrary} path loss results in
 computational problems with extremely strong approximation hardness
 results \cite{GHWW09,Hastad}.  This places algorithmic research in the
 quandary of choosing between intractability or lack of fidelity.

\textbf{Future considerations}
\begin{itemize}
\item Temporality: Most likely built on top of a static component 
\item Randomization: Adapting algorithmic models to (the remaining) unpredictability
\medskip

\item Dynamic environments: Only static here; dynamic part is beyond
  scope, it will typically involve static components; hold when environment is not changing.
\end{itemize}
\fi

\tightpara{Related work}
The ``abstract SINR'' model captures, like decay spaces, arbitrary pairwise path loss. Some positive results hold in that model, e.g., 
distributed power assignment of feasible sets \cite{LotkerPPP11},
reductions involving Rayleigh fading \cite{Dams2012}, and 
special cases of capacity maximization \cite{us:algosensors11}.
However, for most problems of interest, extremely strong inapproximability results hold \cite{GHWW09,khot2006better}. 
Thus, it is essential to use near-metric properties of the decay space.

The introduction of general metrics (apparently first in
\cite{FKV09,FKRV09}) was a significant step in extending SINR theory
beyond geometric assumptions.
Fading metrics \cite{us:talg12} were identified to capture the main
property required from the planar setting.
The concept of \emph{inductive independence} \cite{KV10,hoeferspaa}
has heralded a more systematic approach to SINR analysis, and can by
itself be seen as parameter of the decay space. 
Same holds for \emph{-independence} \cite{infocom11,dams2013sleeping}
in the case of uniform power.

In a sibling paper \cite{us:ICDCS14}, we introduced decay spaces and
metricity with a focus on experimental validation.
The experimental results align with previous results (e.g., 
\cite{son2006,MaheshwariJD2008,chen2010,sevani2012sir}) that
whereas geometric decay is far off, other factors of the ``SINR
model'' closely approximate reality.
In the current paper, for comparison, we substantiate our claims 
of theory transfer, 
treat the fading necessary for distributed algorithms, give lower
bound results in terms of metricity parameters, and show that capacity
approximation in the plane depends only polynomially on the path loss 
term .

\iffalse

\begin{quote}
\emph{To do: Expand discussion on the validity of these assumptions}
Summary: geometric is basically always way off; additivity and
thresholding are close approximations. Cite ICDCS and others.
\end{quote}
\fi

\mypara{Outline of the rest of the paper}
In the next section, we introduce decay spaces (formal definitions, the metricity parameter and how these spaces can be populated),
and indicate how previous results in metric spaces carry over.
In Sec.~\ref{sec:fading}, we address the core requirement of \emph{fading} for distributed algorithms, introduce a parameter that extends their reach to arbitrary spaces, and prove constant upper bounds in spaces with bounded doubling dimension.
The impact of metricity parameters on approximability is treated in Sec.~\ref{sec:dependence}.

\section{Decay Spaces}
\label{sec:models}

\subsection{Signal-strength models}
\label{sec:ss}

The \emph{abstract SINR} model has two key properties:
\textbf{(i)} signal decays as it travels from a sender to a receiver,
and \textbf{(ii)} interference -- signals from other than the intended transmitter -- accumulates.
Transmission succeeds if and only if the interference is below a given threshold.

Formally, a \emph{link}  is given by a pair of
nodes, sender  and a receiver .  The \emph{channel gain}  denotes the multiplicative
decay in the signal of  as received at .  The
\emph{interference}  of sender  (of link ) on 
the receiver  (of link ) is ,
where  is the power used by .
When , we refer to  as the \emph{signal strength} of link .
If a set  of
links transmits simultaneously, then the \emph{signal to noise and
  interference ratio} (SINR) at  is

where  is the ambient noise.

We refer to the standard signal-strength model as the {\geomodel} model, which 
adds to the SINR formula the assumption of \emph{geometric path loss}:
that signal decays proportional to a fixed polynomial of the distance, 
\emph{i.e.}, 
,
 where the \emph{path loss term}  is assumed to be an
 arbitrary but fixed constant between 1 and 6.  
This assumption is valid in free space, with  in perfect vacuum.  

The last assumption made in theoretical models is \emph{thresholding}:
the transmission of  is \emph{successful} iff , where  is a hardware-dependent constant.  We shall
also make this assumption. It's been shown by Dams, Kesselheim and
Hoefer \cite{Dams2012} that certain models that include a randomized
filter in this decision can be efficiently simulated by thresholding
algorithms.

\subsection{Metrics and Decay Spaces}
\label{sec:decayspaces}

We seek to model arbitrary path loss that is independent of distance.
We capture this by a \emph{decay} function  of pairs of points (or
nodes) so that .

We shall formulate signal decay as \emph{decay spaces}.  Decays
between distinct points are always positive.  Exactly what happens at
a given point (i.e., the value of ) is immaterial to our
consideration, since we may assume that all nodes are distinct.  



\begin{definition}
  A \emph{decay space} is a pair , where  is a
  discrete set of nodes (or points) and  is a mapping (or matrix)  that associates values (\emph{decays}) with
  ordered pairs of nodes.  The decays satisfy: i) 
  (non-negativity), and ii)  if and only if  (the
  identity of indiscernibles).
\end{definition}

Decay spaces need not be symmetric nor obey the triangular inequality.
Such spaces are known as \emph{pre-metrics}.
As shorthand, we write .



Decay space can either represent the truth-on-the-ground, or
its representation/approximation as data. They are relatively easily obtained by measurements, which even the cheapest gadgets today provide. 
They can also be inferred by packet reception rates, or predicted by heuristic  or environmental models \cite{Goldsmith}.

\subsubsection*{Metricity}
We introduced in \cite{us:ICDCS14} a parameter that represents how close the decay matrix is to a distance metric. 
\begin{definition}
The \emph{metricity}  of a decay space  is
the smallest number such that, for every triplet ,
  
\end{definition}
Note that  is well-defined since  satisfies (\ref{eq:zeta}).
In the case of geometric path loss, , since
. 

We define \emph{quasi-distances} between nodes in a decay space 
by . Let  for short.
These quasi-distances induce a \emph{quasi-metric} 
, i.e., a metric except for the possible lack of symmetry.
In the Euclidean setting, quasi-distances are simply the Euclidean distances.



\subsection{Theory transfer}
\label{sec:theorytransfer}

The lion share of the theoretic literature on signal-strength models
can be converted to decay spaces with limited effort.
We aim here to clarify and substantiate that observation.
Our objective is for the non-specialist to be able to determine with
limited effort which results do hold in the decay model and which don't and additionally, when the question arises, which properties of metric
and/or decay spaces are necessary for correct functioning.

In this section, we focus on what is needed for results to hold in
arbitrary decay spaces. In the following section, we deal with results that require special space properties, particularly in the context of distributed algorithms.
By a \emph{result}, we mean a combination of an algorithm or a protocol and its
analysis.

The complexity of a result can be a function of the metric/space.
Here, complexity refers to measures like time and message count, 
but also performance
measures like approximability. In particular, these measures have
nearly always been functions of the metric parameters, such as the
path loss term , but this dependence is often hidden in big-oh notation.

We make the following sweeping assertion (stated without substantiation
in the sibling paper \cite{us:ICDCS14}):

\begin{proposition}
  If a {\geomodel} result only requires metric properties (symmetry, triangular
  inequality), then it holds equally well in arbitrary decay spaces.  Symmetry
  is required of the decay space only if it was required in the
  original setting.  The relevant complexity measure (time,
  approximation) grows with  in the same manner as for the
  original result in terms of .
\label{prop:metric}
\end{proposition}

\begin{proof}
The quasi-distances  of a decay space 
form a quasi-metric , which becomes a metric 
iff  satisfies symmetry. 
Applying the original result to the metric  with path
loss constant  gives an equivalent solution to the problem
on the decay space .
\end{proof}

Specifically, the following results on the following problems carry over without change: 
capacity maximization \cite{SODA11,KesselheimESA12}, 
scheduling \cite{FKRV09,FKV09},
weighted capacity \cite{us:talg12,us:Infocom12}, 
spectrum auctions \cite{hoeferspaa,HoeferK12},
relationship between power control regimes \cite{tonoyan2011a,us:SODA13},
dynamic packet scheduling \cite{CISS12,sirocco12,kesselheimStability,us:SODA13}, 
distributed scheduling \cite{KV10,icalp11}, and
distributed capacity maximization with regret-minimization \cite{infocom11}
(extended for jamming \cite{dams2013jamming}, 
online requests with stochastic assumptions \cite{GHKSV13},
and changing spectrum availability \cite{dams2013sleeping}).

We can also make an immediate observation regarding methods that
hold for restricted metrics.
\begin{observation}
If a result holds in {\geomodel} for a given class  of metrics,
then it holds equally in those decay spaces whose induced quasi-metric
is contained in .
\label{asst:metric}
\end{observation}



\iffalse \mypara{Use of geometric properties}

The use of geometry in results in {\geomodel}
can potentially involve the whole spectrum of its millennium old study.
In practice today, only select range of properties have been used to
derive analytic results. We focus on the primary ones.

\begin{description}
\item[Positions] Positions have no counterpart in non-Euclidean
  metrics\authorcomment{That have been considered in this context, at
    least; non-Euclidean geometries with coordinate structures do
    exist.}.  Thus, if essential, their use disqualifies them from
  being extended to decay spaces.

In practice, positions have been used as means toward two objectives:
a) obtain distances, which can equally be represented as decays.
In this case, positions are no hindrance.
b) sparsification of the instance, so make
simultaneous communication non-conflicting.
We address this issue in Sec.~\ref{sec:fading}. 

\item[Angles] Again angles have no place in general metrics.
As such, their use is problematic. Sometimes, however, they are means
to prove packing bounds.
\item[Packings] A frequently used property of Euclidean metrics, that
  appears in different guises, is that
the number of disjoint balls of unit radius that fit inside a ball of
radius  is polynomially bounded (with its degree depending on the
dimension of the metric). This property does not hold for all metrics,
but it does hold for a large subclass, in particular those known as
\emph{doubling metrics}. We show in Sec.~\ref{sec:fading} how to
extend such results to a wider class of decay spaces.
\end{description}
\fi

\mypara{Results that do not carry over to decay spaces}
There remains a large amount of work in {\geomodel} that depends on
\emph{positions} (or distributions thereof). 
Such results are necessarily tied to geometry, although with some work
it may be possible to extend them to other decay spaces.

A common use of positional information is by partitioning the
plane, so as to make simultaneous communication non-conflicting. This
is particularly an issue for deterministic distributed algorithms.
Examples of this include deterministic distributed broadcast
\cite{JurdzinskiKRS13, JurdzinskiKS13FCT} and local broadcast
\cite{JurdzinskiK12, FuchsW13}.  Also, some centralized approximation
algorithms and heuristics for {\capacity} and {\scheduling} of
\cite{gouss2007, DBLP:journals/corr/abs-1208-0627}.  
Occasionally, angles are used, e.g.\cite{GHWW09}, which does not carry over
(but see Sec.~\ref{sec:polya}).

There is also a large literature on average case analysis, typically
assuming a uniform distribution of points in the plane, starting with
an influential paper of Gupta and Kumar \cite{kumar00} that first
introduced {\geomodel}.

Finally, SINR diagrams \cite{AvinEKLPR12} (and follow-up work of
subsets of the authors) uses intrinsically topological properties of Euclidean
metrics. 


\iffalse The algorithms/analysis must be adaptable to different values than
hold only in the plane, e.g., the path loss term .
We know of no result to date for which this cannot be 
attained. 

\textbf{Issues in doing this formally:}
\begin{itemize}
  \item Allowed: Distances and triangular inequality
  \item Disallowed: positions, angles, packings. (see Fading section)
  \item May need: assume symmetry; knowledge of parameter upper bounds
  \item Parameters must adapt to the fading space:  -->
    , with inequality rather than equality.
  \item Algorithms modified to use decay (instead of distances), 
    but can do so indirectly through the distance function . Thus, no change.
  \item Idea: In the end, the essential matters is whether a
    transmission is successful or not. This can be computed equally
    well depending on decays.
\end{itemize}
\fi

\iffalse \subsection{Realizing decay spaces}
\label{sec:realizing}

Decay space can either represent the truth-on-the-ground, or
its representation/approximation as data.


Measurements are the most common form of obtaining the decay representation.
Even the more inexpensive wireless gadgets today have a mechanism
to measure \emph{received signal strength} (RSS). Most commonly, the
precision is within 1dB, or about 26\%.
The accuracy is often higher, and by adjusting power settings,
higher precision values can be obtained.

In simulations, where decays must be generated, 
heuristic models are frequently used,
including two-ray and multi-ray models,
and Hata and Okumura models \cite{Goldsmith}.
Most common, perhaps, is to add probabilistic components.
Particularly, \emph{log-normal path loss} consists of 
the geometric decay multiplied by an exponential random variable with unit expectation, independently. 



It is also often possible to \emph{infer} decay from the received transmissions.
Even in the absence of measurement capability, the fact that a packet
was received and decoded gives an indication of its signal strength.
This holds even if both the transmission and the reception have
probabilistic components.
Using basic concentration results, 
precise estimates of signal strength
can be obtained by the \emph{packet reception ratio} (PRR)
by trying different transmission power level.
\fi

\subsection{Additional definitions: Power, affectance, separability}


We will work with a total order  on the links, where  implies that . 
A power assignment  is \emph{monotone} if both 
and  hold whenever .
\footnote{This corresponds to \emph{length monotone} and
  \emph{sub-linear} power assignments in {\geomodel}.} This captures
the main power strategies, including uniform and linear power.


We modify the notion of \emph{affectance} \cite{GHWW09,HW09,KV10}:
The affectance  of link  on link  under power assignment   is the interference of  on  normalized to the signal strength (power received) of , or

where  is a constant depending only on universal constants and the signal strength  of , indicating the extent to which the ambient noise affects the transmission. 
We drop  when clear from context.
Furthermore let . For a set  of links and link , let  be the \emph{out-affectance} of  on  and  be the \emph{in-affectance}.
Assuming  contains at least two links we can rewrite Eqn.~\ref{eqn:sinr} as  and this is the form we will use.
A set  of links is \emph{feasible} if  
and more generally \emph{-feasible} if .






Define  as the (quasi-)distance between two links  and .
Let .
A link  is said to be \emph{-separated} from a set  of links, for
parameter , if  for every .  A set  is -separated if each link in  is
-separated from the rest of the set.

Let  refer to the base of the natural logarithm and recall that , for any value .



\section{Fading Properties and Distributed Algorithms}
\label{sec:fading}

\iffalse
\begin{quote}
\textbf{Summary of section:} Necessity of fading for the proper working of
distributed algorithms (potential interference-only signals from
far-away are one key difference in SS-models wrt.\ graphs).
We identify a new parameter, , the \emph{fading parameter} of
a decay space, and show that it is in particular bounded in 
a natural extension of doubling metrics.
We discuss to what extent that assumption is necessary.

Also: Want to claim that if algorithms hold in the plane, without
using geometry directly, but only the fading property, they work also
in decay spaces of bounded fading.
\end{quote}
\fi

In the study of distributed algorithms in  in the plane, the
standard assumption is that the path loss constant  is
strictly larger than 2. The reason for this is that when , nodes
that are spatially well separated will not affect each other by too
much, a property that does not hold when .
This property is generalized to doubling metrics whose doubling dimension is strictly smaller than the path loss constant , dubbed \emph{fading metrics} \cite{us:talg12}.
We call this property, that the sum of affectances from spatially separated transmitting nodes 
converges, the \emph{fading} property. For the most common type of distributed algorithm to work, this has to be bounded.


\iffalse One candidate approach for extending the fading property to decay
spaces would be to apply the same argument to the corresponding
quasi-distance metric.  If the result (quasi-)metric is indeed a
fading metric, everything is fine and well.  The problem is that in
the process of turning decays into quasi-distances, we may have
introduced a non-fading behavior.  For the purpose of metricity, we
want decays to be small, but for fading purposes, they need to be
sufficiently large.  \authorcomment{This is actually not true. It is indeed
  sufficient to check if the induced quasi-distance metric is a fading
  metric w.r.t\ . Which means we could omit the fading
  lemma.... But, it should be useful to have it argued in full
  generality directly in the decay space. }
\fi

We define a parameter  that captures the fading effect.
Let  be the space of all -separated subsets in .
\begin{definition}
The \emph{fading value}  of a node 
relative to a separation term  is 

The \emph{fading parameter}  of a decay space 
is the maximum fading value of a node in the space,
,
relative to a given separation term .
\label{defn:fading}
\end{definition}



That is, the total interference  experienced by a node  from an -separated set  (of senders) using uniform power  is at most . Thus, if the intended signal comes from an -neighborhood (in decay space), then the resulting affectance is bounded by .

Until now,  has been expected to be an absolute constant.
However, we can now simply treat it as a parameter and thus
handle arbitrary decay spaces by distributed algorithms.
Thus, we can achieve significantly more generality than before.
This would necessarily come at the cost of extra time complexity.

\subsection{Fading spaces}

We identify a large class of decay spaces for which the fading parameter is small.
These are generalizations of fading metrics.


First, some additional notation.  The \emph{-ball}  centered at  with radius 
contains all points  for which decay to  is less than .  A
set  is a \emph{-packing} if , for any
.  Thus,  is a -packing iff the set
 of balls are disjoint.  The \emph{-packing
  number}  is the size of the largest
-packing into the body .  


Intuitively, a space is \emph{doubling} if the number of mutually unit-separated points 
within a given distance from a center increases by at most a polynomial of the distance.

\begin{definition}
Let  be a decay space.
Define , as the size of the densest -packing in .
The \define{Assouad dimension  of  with parameter } is
given by
  
\end{definition}
 is in effect the minimum degree  for which sizes of
-packings can be bounded by , for all .
Note that that  \cite{Heinonen}.


\begin{definition}
A \emph{fading space} is a decay space  with Assouad dimension
strictly smaller than 1, , w.r.t.\ some absolute constant .
\end{definition}

\subsection{Annulus argument}
Most randomized algorithms (e.g. in \cite{PODC13} and \cite{Yu12}) ensure that in any given neighborhood
(defined as the set of nodes to which a given node can communicate directly),
the expected number of transmissions in a slot is bounded above by a certain constant.
This ensures that the total expected affectance from other nodes transmitting
is also bounded by a (different) constant.
By adjusting the constants appropriately, one can focus only on the local behavior.
Some deterministic algorithms similarly ensure a spatial separation of sending (and thus possibly interfering) nodes and use this property to bound the total affectance from these nodes.

All proofs of the discussed sort use a common approach.  They define
some type of separation between interfering nodes which can be a
(probabilistic) constant density, a hard minimum distance between
nodes or links or similar.  Then the interference at a node  is
bounded, either directly or, if the node is receiver of a
predefined link, as the (possibly probabilistic) affectance on the
node.  To do this we draw concentric circles around , cutting the
space around  up into annuli.  Using the separation of the
interferers, we argue that the number of interferers that can be
packed in the annulus at distance  is bounded by a polynomial
depending on  and the Assouad dimension of the
space.  

We argue that a general version of this `annulus argument' still holds
when directly used in fading decay spaces, after which we indicate
how other different variations carry over.

Recall the Riemann -function,  , which is known to converge for .
We build on a similar result in \cite{us:talg12} for metric spaces.



\begin{theorem}\label{annulusargument}
The fading parameter of a decay space   with Assouad
dimension  and related constant  is bounded by . 
\end{theorem}

\iffalse \begin{theorem}\label{annulusargument}
Let  be a set of nodes 
inducing decay space   with Assouad dimension .
If  is -separated, with ,
then the total interference on any node  in  is at most a constant  with respect to uniform power .
\end{theorem}
\fi

\begin{proof}
Let .
Since  is -separated, the nodes in  form an -packing.
Since  is doubling, there is a constant  such that  for any , 
the maximal size of an -packing in a ball of radius  centered around a point  is, 


We bound the received signal  at a listening node .
Let  be a number.
Let  and let .
Then  since  is -spaced.

We first note that since  and ,



Since each sender  is of distance at least  from 
the received signal from  on  is bounded by

Then,

By the doubling property of , the size of  is 
  
Thus, using that , since ,
  
Continuing,
  
using the definitions of  and .
\end{proof}



\subsection{Common usage of the annulus argument}
We list some common types of lemmas in which the annulus argument is used and show how to use Theorem \ref{annulusargument} in the proofs for these lemmas.

A common usage of the annulus argument is to prove the following: if
 is a set of links, using a uniform power assignment , with
senders of a minimal mutual distance  and with the longest link of
length at most a given constant times , then  forms a -feasible set. 
For sets as described in Theorem \ref{annulusargument}, where all
\emph{nodes} are -separated and a maximum link decay  at most
constant , the transition is straightforward.
By the definition of affectance and Theorem \ref{annulusargument}, 
the affectance of  on link  with maximum decay  is at most 

where .
To obtain a -feasible set, we simply set .


However, if only a separation on senders is defined (e.g. in \cite{us:talg12}), 
we use the triangular inequality to bound the interference at  in terms of interference at .
Requiring , we obtain , since for any sender  by the triangle inequality

using that .
And thus , so the argument holds as
before by adjusting  with an extra  factor.
When , the overhead factor is correspondingly smaller.


Examples of problems with centralized algorithms that use this form of annulus argument:
connectivity \cite{MoWa06,moscibroda06b,Moscibroda07,SODA12},
scheduling \cite{chafekar07,tonoyan2012},
flow-based throughput \cite{CKMPS08},
online capacity maximization \cite{fanghanel2013online},
and bounds on the utility of conflict graphs \cite{tonoyan2013,tonoyan2013a}.

For randomized algorithms, the annulus argument is used in 
a similar way to bound expected interference.  The expected
interference in a disk is bounded by arguments specific to the
analyzed algorithm.
 These arguments may or may not translate to the
decay space as discussed in Sec.~\ref{sec:theorytransfer}.
Instead of adjust the separation term , thy typically adjust the transmission probabilities.
Once the expected interference in a disk is bounded, however, the
argumentation for bounding the total expected interference at a node
,  follows Theorem \ref{annulusargument}.  





The probabilistic version of the annulus argument forms the core of
the analysis for many randomized distributed algorithms which often carry over without any significant further adjustments.
Example include (distributed) coloring \cite{YWHLa11}, local broadcast
\cite{Goussevskaia2008Local, Yu11, Yu12, FOMC12}, broadcast
\cite{DaumGKN13} and multiple-message broadcast \cite{YuHWTL12,
  YuHWYL13}, capacity \cite{pei2013distributed}, 
dominating set \cite{ScheidelerRS08} and (multihop) connectivity  \cite{PODC12, PODC13}, and dynamic packet scheduling \cite{pei2012low}.







\subsection{Beyond fading spaces}
Fading spaces do not completely characterize spaces with a
bounded fading parameter. One reason is that the definition of
doubling metrics is scale-invariant in that the packing
constraint holds for balls of any size, whereas we are often only
interested in balls of a fixed size (or in a limited range of sizes).

Consider, for instance, the metric space formed by a star centered at
node  with  leaves  at distance 
 and one leaf  at distance .  Suppose the
decay  equals the distance (so ).  The doubling
dimension of this space is , so unbounded.  Suppose also we are
interested in the separation term , i.e., how well we can transmit
from  to  in the presence of transmissions from the other
nodes. If , we find that the total interference at node
 is , which is asymptotically
smaller than the signal received from .



\section{Dependence on the Metricity in Approximations}
\label{sec:dependence}

With the pinpointing of the metricity parameter  as a key 
indicator of a decay space, the question
arises how it affects the complexity of fundamental problems. 
This differs from {\geomodel} where the path loss term
 has traditionally been viewed as a constant.



We explore here the approximability of the {\capacity} problem as a
function of innate properties of the decay space in question.
Given a set  of links, the {\capacity} problem asks for 
maximum cardinality subset
of  that is feasible.
The {\capacity} problem is fundamental, not only because it addresses
the basic question of how much wireless communication
can coexist, but also because it has been the underlying core routine
in other problems, including scheduling \cite{GHWW09}, throughput maximization (via
flow) \cite{wanwireless}, spectrum auctions \cite{hoeferspaa}, spectrum sharing \cite{us:Infocom12}, and connectivity and aggregation \cite{SODA12,PODC12}.

Our generic statement, Prop.~\ref{prop:metric}, along with known
approximation results \cite{SODA11,KesselheimESA12} in general metrics, 
implies that {\capacity} in decay spaces can be approximated
within a function of . Specifically, the
approximation of \cite{SODA11} (for monotone power) is exponential in , 
which was refined to  in \cite{us:ICDCS14}.

We can also observe that the known hardness construction for
``abstract SINR'' \cite{GHWW09} (see also \cite{SODA11}) implies that
-approximation for {\capacity} is hard.  We include
the argument in the appendix for completeness.

\begin{theorem}
 \prob{Capacity} of equi-decay links is hard to approximate within  factor.
This holds even if the algorithm is allowed 
arbitrary power control against an adversary that uses uniform power.
\label{thm:cap-hardness}
\end{theorem}

This leaves the question whether better results are possible in the
Euclidean metric and comparable decay spaces. 
Surprisingly, the answer depends on the exact definition of the
metricity parameter.
Specifically, {\capacity} with uniform power is then
approximable within a polynomial of , while for
a natural variant of the -parameter, exponential dependence
is still necessary.

\subsection{Improved Approximations in Bounded Growth Decay Spaces}
\label{sec:polya}

We show here that {\capacity} with uniform power can be approximated
within polynomial factors of  in Euclidean metrics.  More
generally, this holds for decay spaces of bounded growth, as we shall
define shortly.  Interestingly, it does not rely on the fading
behavior of the plane (i.e., that ). This appears to be
the first instance in the signal-strength literature where better
results are shown to be obtainable in the plane independent of
 than for general metrics.

The intuitive reason why uniform power in the plane proves to be easier
is two-fold.  The main cause for exponential dependence on 
comes from the use of the triangular inequality. If one can ensure
that one angle is highly acute, the overhead of the inequality goes
down accordingly. In particular, the overhead in switching the
reference from a receiver to a sender of a link goes down if the
length of the link relative to the other distances is small.

We shall show that links with uniform power in bounded-growth decay spaces
satisfy a useful structural property that allows for improved
approximation for numerous problems.

\mypara{Bounded Growth Decay Spaces}
We shall consider decay spaces that have upper bounds on two measures
that restrict growth: 
the doubling dimension (from Sec.~\ref{sec:fading}), 
and the independence dimension, defined in \cite{GHWW09} for metrics
and adapted as follows to decay spaces.


\begin{definition}[\cite{GHWW09}]
A set  of points in a decay space  is \emph{independent} w.r.t.\ a point  if  for each .
The \emph{independence dimension} of  is the size of the largest independent point set.
\end{definition}

Spaces of bounded independence dimension  have the following
useful property: for any point , there is a set  of at most  points that \emph{guard}  in the following
sense: , for any point . A node  \emph{guards} node  \emph{from} node  if
.

Welzl \cite{Welzl08} has made a number of useful observations of
metrics of bounded independence dimension. He showed that the number of guards needed in a
metric is indeed exactly its independence dimension. In a Euclidean
space , it equals the maximum number of unit vectors
that form pairwise angles of more than .
Therefore, the independence is at most the so-called kissing number,
the maximum number of disjoint open balls of radius 1 that can touch
the unit ball. This number grows exponentially in the dimensions but
its exact value is not known for most dimensions.

As a simple example, let us see how six guards suffice in the plane.
Given a point , divide the plane into six  sectors around 
and partition  accordingly into sets .
Let  consist of the nearest point to  in each of the six sectors.
The guarding property follows from the fact that the angle  is at least , for each point  and guard .

We define a decay space to be \emph{bounded-growth} if it has bounded
independence dimension and its quasi-distance metric has a bounded
doubling dimension.  (The dimension of a decay space and its
quasi-distance metric is the same.)

The doubling and independence dimensions are actually incomparable.
The uniform metric, where all decays equal 1, is of independence
dimension 1 but unbounded doubling dimension.
The following curious construction of Welzl \cite{Welzl08} gives a
metric of doubling dimension 1 whose independence dimension is unbounded:
Let  with
, for , and , for . We leave it to the curious reader to
verify that any ball (only those of radius  or 
matter) can be covered with two balls of half the radius and that  are independent with respect to .

\mypara{Amicability}
The following definition originates in \cite{infocom11} and 
was formally stated in \cite{dams2013sleeping} as \emph{-independent} conflict graphs.

\begin{definition}
A set  of links is \define{-amicable} if there is a constant  such that, for
any feasible subset , there is a subset  with  such that for any vertex ,  (using uniform power).
\end{definition}

It is known that sets in {\geomodel} in metric spaces are -amicable \cite{infocom11}.


Various decentralized capacity-type problems with uniform power have been treated with no-regret minimization techniques, relying only on the amicability property of the instances. This started with a distributed constant approximation for {\capacity} \cite{Dinitz2010,infocom11}, and was extended to deal with jamming \cite{dams2013jamming}, online requests against stochastic adversaries \cite{GHKSV13}, and changing spectrum availability \cite{dams2013sleeping}.
Our -bound on amicability improves these results in 
the bounded-growth metrics.

We show that growth-bounded instances are -amicable,
thus obtaining improved approximations for the above problems (as
functions of ).

\mypara{Capacity approximation via bounds on amicability}

To bound amicability, we first show how to turn feasible
sets in doubling spaces into well separated sets at 
limited cost. The proof is deferred to the appendix.

\begin{lemma}
  Let  be a feasible set of links in a decay space whose
  quasi-distance metric has doubling dimension .  Then,  can be
  partitioned into  sets, all of which are
  -separated.
\label{lem:planar_separation}
\end{lemma}

We are now ready to prove the structural result of this section.

\begin{theorem}
  Let  be a set of links in a decay space of independence dimension
   and whose quasi-distance metric has doubling dimension
  .  Then,  is -amicable.
\label{thm:indep}
\end{theorem}

\begin{proof}
Let  be any feasible subset of .
By Lemma \ref{lem:planar_separation}, there is a subset
 of size  that is 
-separated.
Let  be the subset of links in
 with low out-affectance.
Note that , by feasibility,
so the average
out-affectance of links in  is at most 1, and at least half
the links will have at most double the out-affectance.
Thus, 



Consider any link .  Let 
be the indices of senders in  that guard the sender  of
, where . Partition  into sets 
, where  is contained in  and guards
 from the senders of other links in .
Consider any set  and let  be a link in .
Since  guards  from , .
Then, additionally using the triangular inequality and that  is -separated,

So, 
.
In a similar way, we obtain that
  ,
so 
 
Combining, we get that 
.
We can then bound the out-affectance of  on  by

using the definition of  in the last inequality.
Then, .
Then,  satisfies the definition of amicability with  and .
\end{proof}

We arrive at the main result of this section, whose proof is given in the appendix. Algorithm \ref{alg:capfixtri} combines the characteristics of the capacity algorithms of \cite{GHWW09} and \cite{SODA11}.


\begin{algorithm}[h]
\caption{Capacity for uniform power in bounded-growth decay spaces.}\label{alg:capfixtri}
\begin{algorithmic}
\STATE Let  be a set of links using uniform power and let 
\FOR { in order of increasing  value}
\IF { is -separated from  and } \label{alg:tri1/2}
\STATE 
\ENDIF
\ENDFOR
\STATE Return 
\end{algorithmic}
\end{algorithm}

\begin{theorem}
  Uniform power {\capacity} -approximable in
  bounded-growth decay spaces (by Algorithm \ref{alg:capfixtri}).
  In particular, it is -approximable on the plane, for any .
\label{thm:cap-poly-bndgwth}
\end{theorem}

This is actually the first SINR approximation result (for capacity or related problems) that is sub-exponential in .

\subsection{Inapproximability results for a variant of metricity}
\label{sec:expon-phi}

\mypara{Metricity variant  } 
Alternative measures of the metric-like behavior of a space
 can
be concocted. A particularly natural one is the 
parameter  that bounds the \emph{multiplicative} factor within which 
 satisfies a relaxed triangular inequality:

So,  is the smallest value such that , for every .
For comparison with , we define .

Examining the proofs of the various results for {\capacity} 
and \emph{inductive independence} \cite{hoeferspaa}, we find that the triangular
inequality is applied to compare lengths that are within constant
factor of each other, in which case the overhead is comparable to the case of . Thus, the results hold also in terms of .

\begin{observation} {\capacity}, both with monotone power
  \cite{SODA11,us:ICDCS14} and arbitrary power control
  \cite{KesselheimSODA11}, is approximable within .
  Other results with effective (exponential) approximations in terms
  of similar bounds hold for inductive independence
  \cite{hoeferspaa,us:SODA13} and relationships between power control
  and monotone power \cite{us:SODA13}.
\end{observation}

Bounds on inductive independence also have numerous implications,
including connectivity and aggregation \cite{SODA12,PODC12}, spectrum
auctions \cite{hoeferspaa,HoeferK12}, dynamic packet scheduling
\cite{sirocco12,kesselheimStability}, and distributed scheduling
\cite{KV10,icalp11}.

We can observe that . Namely, for any nodes ,
, 
using the definition of .
Thus, .
Hence, lower bounds in terms of 
carry over to lower bounds in terms of ,
so exponential approximations in terms of  are best
possible in general metrics.

A converse relation between  and  does not exist, however.
Consider the instance on three points  with 
,  and . 
Then, one can verify that , while 
, which is unbounded.


We find that {\capacity} in
bounded-growth spaces is still exponentially hard in terms of .
We give a construction that is embedded on a pair of lines,
that holds for arbitrary values of a parameter .
For decays within the lines, it uses the usual distance function raised to power ,
while between the lines, it uses two fixed decays:  and .
It then also shows that strong hardness holds even when none of the decay
functions are particularly fast growing. 
The proof is deferred to the appendix.

\begin{theorem}[\cite{GHWW09}]
\prob{Capacity} of equi-decay links in bounded-growth decay spaces
is hard to approximate within -factor.
This holds even if the algorithm is allowed 
arbitrary power control against an adversary that uses uniform power.
\label{thm:hardness}
\end{theorem}

We note that the decays used in the construction were all 
in the range  and  between pairs of distance .
This result thus shows that huge decays (or, path loss) are not
needed \emph{per se} to get large approximation hardness. Rather, it is the
differences in decay among spatially related points that is the
cause. 


\iffalse --- Not clear if this is of any value
Another corollary of the construction regards fixed-parameter tractability.

\begin{corollary}
\prob{Arbitrary Power Capacity} and \prob{Monotone Power Capacity}
are both W[1]-hard.
\label{thm:w1hardness}
\end{corollary}

(Proof to be added)
\fi



\iffalse We have proposed a model that captures spatial variability of wireless signals.
The approach is generic enough that most algorithmic techniques for the geometric SINR model carry over with minimal adaptation. 

There are three immediate open questions that our results raise:
Can we further reduce the gap in the approximation in terms of the smoothness parameter? Can we extend our approach to the case, commonly required by distributed algorithms, where we need a \emph{lower} bound on decay and smoothness?
And, can we extend the graph-based approach we applied to connectivity to Steiner-problems or directed graphs?

It is also important to point out that our model does not attempt to cover all the challenging aspects of wireless networks. For one, it is \emph{static} and does not capture dynamicity or temporal variability; this is a very challenging but important aspect. And second, it is \emph{deterministic}; success or failure in wireless networks does have a probabilistic aspect. The work of \cite{Dams2012} suggests that deterministic approximation algorithms perform well if probabilities are independent on a wide scale; the challenge is in modeling the likely dependence, both spatially and temporally. Handling these issues is an interesting and challenging open research direction. 
\fi

\newpage

\bibliographystyle{abbrv}
\bibliography{references}

\appendix

\section{Missing proof from Section \ref{sec:dependence}}

\noindent \textbf{Theorem \ref{thm:cap-hardness}.} \emph{
\prob{Capacity} of equi-decay links is hard to approximate within  factor.
This holds even if the algorithm is allowed 
arbitrary power control against an adversary that uses uniform power.
}

\begin{proof}
Given a graph , form a set  of links of unit-decay with a
link  for each node  and with the (bi-directional) decay of
 as 2 if  and  if .

If  is a feasible set of links in , then it contains no two
links  and  that form an edge in , no matter what
power they assume.  Similarly, if  is an independent set in ,
then if  is the corresponding set of links, the affectance of any
given link  in  when using uniform power is at most ; thus,  is feasible.  Hence, there is a one-one
correspondence between independent sets in  and feasible sets in
, as well as between sets that are feasible and those that are feasible
under uniform power.

Now, observe that , as  is the maximum ratio
between decays, and the bound is actually tight.  The
-approximation hardness of \prob{Max Independent Set}
\cite{khot2006better} then translates to -approximation hardness for {\capacity}.
\end{proof}


\section{Missing proofs from Section \ref{sec:polya}}

We shall make use of the following technique.

\begin{lemma}[Signal-strengthening \cite{HW09}]
There is a polynomial-time algorithm that, for any given , 
partitions any -feasible set into 
 sets, all -feasible.
\label{lem:signal-strength}
\end{lemma}

We first argue that feasible sets under uniform power must be somewhat separated (or, -separated), independent of metric.

\begin{lemma}
Let  be an -feasible set of links under uniform power and assume
.
Then,  is -separated.
\label{lem:onezetasep}
\end{lemma}

\begin{proof}
Suppose otherwise. Then, there are two links  in 
 that are not -separated.
There are three cases, depending on which pairwise distance bound is violated.

Consider first the case when .  Since the two links are feasible
simultaneously, the signal received by  from  is at least as
strong as that from the other sender  (since ).
So, , implying that 
.
Then, by the triangular inequality and these bounds, 

Thus, .
It follows that
 
This contradicts the assumption that  and  coexist in the same -feasible set.

Consider next the case when .
Without loss of generality, assume  .
By the triangular inequality,
, 
implying that ,
leading to a contradiction as before.
Finally, the case when  is symmetric to the
previous one when swapping senders and receivers.
Hence, the claim.
\end{proof}

We next show that in doubling metrics, the separation factor can be
expanded by a polynomial factor at the cost of a polynomial factor.

\begin{lemma}
Let  and  be positive parameters, .
Let  be a -separated set of links 
in a decay space whose quasi-distance metric has doubling dimension .
Then,  can be partitioned into  sets each of which 
is -separated.
\label{lem:sep-strength}
\end{lemma}

\begin{proof}
Consider a link  in . Let  be the set
of links in  whose receivers are within distance  from . Then, we have a set of 
disjoint balls of radius  that are properly contained
in a ball of radius of  (around ).
By the definition of the Assouad dimension, 


We now form the graph , where  and 
 iff  or .
Let .
Form a total order  on the nodes by non-increasing link length.
By (\ref{eq:rv}), each node has at most  neighbors that follow it in the ordering (because if  then ). That is,  is a 
\emph{-inductive} (or, \emph{-degenerate}) ordering of .
Coloring the graph first-fit according to  then uses at most  colors. 
To complete the proof, we observe that a set of links is -separated
if and only if the corresponding set of vertices in the graph is independent 
(graph-theoretically). 
\end{proof}

Put together, we obtain a sparsity-strengthening lemma in doubling spaces.

\noindent \textbf{Lemma \ref{lem:planar_separation}.} \emph{
Let  be a feasible set of links in a decay space whose
  quasi-distance metric has doubling dimension .  Then,  can be
  partitioned into  sets, all of which are
  -separated.
}

\begin{proof}
Recall that by the signal strengthening Lemma
\ref{lem:signal-strength},  can be partitioned into at most
 sets each of which is -feasible.  Let
 be such a set.  By Lemma \ref{lem:onezetasep},  is
-separated, so by Lemma \ref{lem:sep-strength},  can be
partitioned into  sets, each of which is
-separated.
\end{proof}


\noindent \textbf{Theorem \ref{thm:cap-poly-bndgwth}.} \emph{
Uniform power {\capacity} is -approximable in
  bounded-growth decay spaces.
}

\begin{proof}
We use Algorithm \ref{alg:capfixtri}.


Let  be a set of links and  and  be the sets computed by the
algorithm on input . 
Let  denote the order in which the algorithm processes the links.
Note that by rearrangement and the construction of
, .  Thus, the average
in-affectance of a node is at most , and by Markov's inequality

Let  be a maximum capacity subset of . Let  be the subset of  promised by Thm.~\ref{thm:indep} that has
cardinality  and satisfies , for every .  Observe that the proof of
Thm.~\ref{thm:indep} actually ensures that  is
-separated.

Let .  Partition  into  and , where
links in  failed the requirement of -separability from
, while those in  passed the separability requirement but failed
the affectance test. We proceed to bound  and  in terms of .

First, observe that for each link  in , at most one link in
 can fail to be -separated from , as otherwise
 would not be -separated.  That implies that .

Now, let  be a link in  and let  be the links in  that precede  in the decay
order.  Let  be a link in .
Then, ,  and ,
since .
Using the triangular inequality, the fact that ,
and that  is -separated from , we get that

Thus, .
Hence, since  and , 

Thus, .
By definition of , .  
Combining the last two inequalities, we get that 
.
Summing this inequality over links in ,

On the other hand, by amicability,

Combining (\ref{eq:sum-axw}) and (\ref{eq:sum-amic}),
we obtain that

Thus, ,
and

using (\ref{eq:s-x}). Hence, 
,
as claimed.
\end{proof}

\section{Missing proof from Section \ref{sec:expon-phi}}



\noindent\textbf{Theorem \ref{thm:hardness}}(\cite{GHWW09}).\emph{
\prob{Capacity} of equi-decay links in bounded-growth decay spaces
is hard to approximate within -factor.
This holds even if the algorithm is allowed 
arbitrary power control against an adversary that uses uniform power.
} 

\begin{proof}
By reduction from the maximum independent set problem in graphs.
Let  be arbitrary value satisfying , 
denoting the maximum path loss term and let .
Assume for simplicity that  and .
Let  refers to the standard Euclidean distance.

Given graph , form a set  of links
with link  for each vertex
 located in the plane.
The senders are located on the vertical line segment 
and the receivers on the segment :
 at point  and  at point .

Decays between points on the same line (both senders or both receivers)
are set to their distance to the power of .
For decays between points on different lines, 
we use two fixed decays:  and .

Formally, for links  and , let
 
where .
Also, let .

With uniform power ,
we have that for each ,

Hence, a set  of links is feasible iff  is an independent set.

For the case of power control, consider a pair of links 
and let  be any power assignment on the links.
If , then 
, which implies that

So, at least one of  and  must be
greater than one, implying that no power assignment allows
 and  to be simultaneously feasible.
Hence, any feasible set  must correspond to an independent set in ,
and we know that any independent set in  can be made feasible in
 using uniform power. Solutions to {\capacity} on  are therefore
in one-one correspondence with solutions to \prob{Max Independent Set}
on , preserving solution size.

Regarding , observe that .
Then, we can verify that for any triplet  of points used in ,

Thus, .
Hence, if \prob{Capacity} is approximable within  factor,
then \prob{Max Independent Set} is approximable within  factor.
In particular, the -computational hardness of \prob{Max Independent
Set} \cite{khot2006better} implies equivalent -hardness for \prob{Capacity}.

Finally, we examine the bounded-growth properties of the space. 
A -ball with  contains either only senders or
only receivers, and such sets can be covered by two balls of half the radius.
However, any subset of nodes can be covered with four balls of radius
at least , two on each line. Thus, the decay
space is doubling (with ).
As for independence, all nodes on a line are closer to each other than
they are to any node on the other line. Thus, an independent set with
respect to a point  contains at most two points from the same line
as  and at most one point from the other line, for an independence
dimension of 3.
\end{proof}

\end{document}  
