\documentclass[10pt,a4paper]{article}
\usepackage{fullpage,nopageno}
\usepackage{graphicx,amssymb,amsmath,xspace,amsfonts,tabularx}
\usepackage{clrscode}

\renewcommand\topfraction{.90}
\renewcommand\textfraction{.10}
\renewcommand\floatpagefraction{.75}
\advance\textheight 3\baselineskip

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newenvironment{proof}{Proof:}{\qed}
\newenvironment{denseitems}{\list{}{\itemsep0pt\parsep0pt}}{\endlist}

\def\squareforqed{\hbox{\rlap{}}}
\def\qed{\ifmmode\squareforqed\else{\unskip\nobreak\hfil
\penalty50\hskip1em\null\nobreak\hfil\squareforqed
\parfillskip=0pt\finalhyphendemerits=0\endgraf}\fi}

\def\tin{\textsc{tin}\xspace}
\def\tins{\textsc{tin}s\xspace}
\def\dem{\textsc{dem}\xspace}
\def\dems{\textsc{dem}s\xspace}
\def\gis{\textsc{gis}\xspace}
\def\lidar{\textsc{lidar}\xspace}
\def\io{\textsc{i/o}\xspace}
\def\ios{\textsc{i/o}'s\xspace}
\def\tsm{\textsc{TerraStream}\xspace}
\def\tfl{\textsc{TerraFlow}\xspace}

\newcommand{\Reals}{{\mathbb R}}
\newcommand{\graph}{{\cal G}}
\newcommand{\etal}{et al.\xspace}
\newcommand{\eps}{\varepsilon}

\def\sort{\mathit{Sort}}
\def\Sort(#1){({#1}/B) \log_{M/B} ({#1}/B)}
\def\scan{\mathit{Scan}}

\title{Simple I/O-efficient flow accumulation on grid terrains}

\author{Herman~Haverkort\thanks{TU Eindhoven, the Netherlands, cs.herman@haverkort.net}
\hspace{.3cm}
Jeffrey Janssen\thanks{Realworld Systems, Culemborg, the Netherlands, twilight@nmotion.nl}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
The \emph{flow accumulation} problem for grid terrains takes as input a matrix of flow directions, that specifies for each cell of the grid to which of its eight neighbours any incoming water would flow. The problem is to compute, for each cell , from how many cells of the terrain water would reach . We show that this problem can be solved in  \ios for a terrain of  cells. Taking constant factors in the \io-efficiency into account, our algorithm may be an order of magnitude faster than the previously known algorithm that is based on time-forward processing and needs  \ios.
\end{abstract}

\section{Introduction}
Current remote-sensing technology provides high-resolution terrain data for geographic information systems (\gis). In particular, with \lidar technology one can get detailed elevation models of the earth surface. These are usually made available in the form of a triangulated irregular network (\tin) or a grid: a matrix with elevation values for points in a regular grid on the surface of the earth. One important application of such models are hydrological studies: analysing the flow of water on a terrain, for example to study the effects of possible human intervention or to estimate risks of erosion.

The grid models may be as large as several dozen gigabytes so that they do not fit in the main memory of a computer at once. Therefore hydrological analysis requires efficient algorithms that scale well and are designed to minimise the swapping of data between main memory and disk. Throughout the current decade Arge et al.\ have designed and published such algorithms for the following pipeline of computations on terrain data:\begin{denseitems}
\item constructing an elevation model from raw elevation samples~\cite{cloud,terrastream};
\item (partial) \emph{flooding:} eliminating spurious depressions from the model, so that streams do not appear to be interrupted by virtual dams that result from sampling errors~\cite{unionfind,terraflow,terrastream};
\item \emph{flow routing:} determining in what direction water could flow on each point of the terrain, in such a way that from every point of the terrain water would follow a non-ascending path to the lowest point of the watershed that contains that point~\cite{terraflow,terrastream};
\item \emph{flow accumulation:} computing for every point of the terrain the size of the region from which water flows to that point~\cite{terraflow,gridproblems,terrastream};
\item \emph{hierarchical watershed labelling:} giving each point of the terrain a label that indicates its position in the watershed hierarchy~\cite{pfafstetter,terrastream}.
\end{denseitems}
Arge et al.\ implemented these algorithms in the form of a system called \tsm~\cite{terrastream}.
While the published algorithms have been shown to be very effective, in this paper we will show that for some of these algorithms alternatives exist that may process grid models (but not \tins) up to an order of magnitude faster.

\paragraph*{Analysing I/O-efficiency}
In this paper we analyse the efficiency of algorithms with the standard model that was defined by Aggarwal and Vitter~\cite{iomodel}. In this model, a computer has a memory of size  and a disk of unbounded size. The disk is divided into blocks of size . Data is transferred between memory and disk by transferring complete blocks: transferring one block is called an ``\io''. Algorithms can only operate on data that is currently in main memory; to access the data in any block that is not in main memory, it first has to be copied from disk. If data in the block is modified, it has to be copied back to disk later, at the latest when it is evicted from memory to make room for another block. In this paper we will be concerned with the \io-efficiency of algorithms: the number of \ios they need as a function of the input size , the memory size , and the block size . As a point of reference, scanning  consecutive records from disk takes  \ios; sorting takes  \ios in the worst case~\cite{iomodel}. It is sometimes assumed that .

We distinguish \emph{cache-aware} algorithms and \emph{cache-oblivious} algorithms. Cache-aware algorithms may use knowledge of  and  (and to some extent even control ) and they may use this to control which blocks are kept in memory and which blocks are evicted. Cache-oblivious algorithms do not know  and  and cannot control which blocks are kept in memory: the caching policy is left to the hardware and the operating system. Nevertheless cache-oblivious algorithms can often be designed and proven to be \io-efficient~\cite{flpr-coa-99}. The idea is to design the algorithm's pattern of access to locations in input and output files such that effective caching is achieved by any reasonable general-purpose caching policy (such as least-recently-used replacement) for any values of  and . As a result, any bounds that can be proven on the \io-efficiency of a cache-oblivious algorithm do not only apply to the transfer of data between disk and main memory, but also to the transfer of data between main memory and the various levels of smaller caches. However, in practice cache-oblivious algorithms cannot always match the performance of cache-aware algorithms that are tuned to specific values of  and .

\paragraph*{Our results}
We present and analyse several algorithms for flow accumulation on grid terrains:\begin{denseitems}
\item an ``\io-na\"ive'' algorithm that processes the grid row by row;
\item a variant of the above that processes the row-by-row data in so-called Z-order;
\item a variant of the above that processes data that is stored in Z-order;
\item a cache-aware \io-efficient algorithm based on separators;
\item a variant of the above that processes data that is stored in Z-order;
\item a cache-oblivious variant of the above;
\item and for comparison: the algorithm based on time-forward processing from Arge et al.~\cite{terraflow,gridproblems,terrastream}
\end{denseitems}
Our results are summarised in Table~\ref{tab:results}.
We find that the last algorithm induces a significant overhead from sorting the input cells into topological order. This is not only because the sorting itself takes \io, but also because sorting the cells makes it necessary to store the coordinates in the grid for each cell: the coordinates are needed to sort the cells back into row-by-row order when the computation is complete. Our new algorithms avoid this overhead: they do not sort. The best of our algorithms are asymptotically more efficient than time-forward processing (under mild assumptions), and because the coordinates of a grid cell can always be deduced from its location in the file, there is no overhead from storing coordinates. In practice the difference in performance may be up to an order of magnitude, and preliminary experimental results indicate that our algorithms are fast indeed. For comparison: the authors of \tsm reported 455 minutes for flow accumulation by time-forward processing, working on a grid of similar size and hardware that appeared to be slightly faster than ours.
\begin{table}
\def\arraystretch{1.25}
\begin{tabularx}{\hsize}{|Xr|lr|r|r|}
\hline
algorithm & files & \multicolumn{2}{c|}{asymptotic number of \ios} & \multicolumn{1}{c|}{\io-volume /} & running time\\
          & & worst case & realistic & \multicolumn{1}{c|}{(in+output)} & \multicolumn{1}{c|}{}\\
\hline\hline
\multicolumn{6}{|l|}{flow accumulation algorithms}\\
\hline
``na\"ive'' row-by-row scan & \textsf{R}                  &                            & { } &         & 111 min.\hphantom{*}\\
``na\"ive'' Z-order-scan & \textsf{R}     &                           & { } &         &     \\
``na\"ive'' Z-order-scan & \textsf{Z}        &                    & { } &         &  41 min.\hphantom{*}\\
cache-aware separators & \textsf{R}                   & \multicolumn{2}{c|}{{ }} & 2.0--6.6{ } &  39 min.\hphantom{*}\\
cache-aware separators & \textsf{Z}   & \multicolumn{2}{c|}{{ }} &     1.1{ }  & \\
cache-oblivious separators & \textsf{R}              & \multicolumn{2}{c|}{{ }}  & & \\
cache-oblivious separators & \textsf{Z}              & \multicolumn{2}{c|}{{ }}  & & 118 min.\hphantom{*}\\
\hline
time-forward proc.~\cite{terraflow,gridproblems,terrastream} & \textsf{RZ} & \multicolumn{2}{c|}{{ }}                    &   7.8--32.0{ } & \\
\hline\hline
\multicolumn{6}{|l|}{conversion to/from Z-order}\\
\hline
\multicolumn{2}{|l|}{Z-order scan} & \multicolumn{2}{c|}{{ }} &  & 88 min.\hphantom{*}\\
\multicolumn{2}{|l|}{row-by-row scan} & \multicolumn{2}{c|}{{ }} & 2.0\hphantom{ } & \\
\multicolumn{2}{|l|}{by sorting} & \multicolumn{2}{c|}{{ }} & 4.0--6.0{ } & \\
\hline
\end{tabularx}
\caption{\textsf{R}: input/output files in row-by-row order;\quad
\textsf{Z}: input/output files in Z-order;\quad
\textsc{RZ}: results hold for both formats.\quad
\textsf{A}: requires knowledge of  and/or knowledge or tuning of~;\quad
\textsf{C}: requires the \emph{confluence assumption} (see Section~\ref{sec:confluence});\quad
\textsf{S}: requires that  rows of the input fit in memory;\quad
\textsf{T}: requires tall-cache assumption ( for some constant~).\hfill\break
The \io-volume is the number of bytes transferred relative to the size of the input and output files, for a range of reasonable values for , , and  (see Section~\ref{sec:parameters}).}
\label{tab:results}
\end{table}
This suggests that, while the time-forward processing algorithms are more flexible (they are easier to adapt to triangulated terrains and to so-called multiple-flow-direction models), their flexibility comes at a price when processing grid terrains. Simple alternatives such as our algorithms based on Z-order or our cache-aware algorithm are therefore worth to be considered for practical applications.

In the following section we discuss some preliminaries: we introduce the \emph{confluence assumption} which is helpful when analysing the \io-efficiency of algorithms on realistic inputs, and we discuss how to traverse a grid in Z-order and how to convert a grid file from row-by-row order to Z-order and back. In the next section we present and analyse our flow accumulation algorithms and experimental results.
In Section~\ref{sec:otherstages} we discuss possible applications of our ideas to other stages of the hydrological analysis pipeline.

\section{Preliminaries}

\subsection{The parameters of the I/O-model}
\label{sec:parameters}
Sometimes we will require the \emph{tall-cache assumption} ( for some constant ); when this is the case we will indicate this explicitly.
When estimating \io-volumes, we will explicitly consider the scenarios  bytes,  bytes, and  bytes,  bytes, in both cases with an input grid of size  cells (this is roughly the size of the largest grid we experimented with).

\subsection{The confluence assumption}\label{sec:confluence}
In the next section we will see that some algorithms would perform very poorly when given a grid of  cells in which a river flows from the right to the left, making  meanders with an amplitude of  cells (see Figure~\ref{fig:meanders}, left), for some constants  and . However, it does not seem to make all that much sense to take such cases into account in asymptotic analysis: that would suggest that with growing resolution and size of our grids, rivers would get more and larger meanders. But the major features of real drainage networks would never change just because our grids get bigger. To capture this intuition, we pose the \emph{confluence assumption}, defined as follows.

For any square  of  cells, let  be the square of  cells centered on .
Let the \emph{first far cells} of  be the cells  on the boundary of  such that from at least one cell  of  water reaches  before leaving . The \emph{confluence assumption} states that there is a constant , independent of  and , such that any square  has at most  different first far cells (see Figure~\ref{fig:confluence}, middle). We call  the \emph{confluence constant}.
Informally, the confluence assumption says that the water that flows from any square region will collect into a small number of river valleys before it gets very far.

We will use the confluence assumption in the analysis of some of our algorithms.
\begin{figure}
\centering\includegraphics[width=\hsize]{rivers.pdf}
\caption{Left: an unrealistic grid with a river with  meanders with an amplitude of .
Middle: a realistic grid. The white encircled cells are the first far cells of ; it has only few first far cells.
Right: Z-order.}
\label{fig:meanders}\label{fig:confluence}\label{fig:zorder}
\end{figure}

\subsection{Storing grids in Z-order}\label{sec:Zorder}
Some of our algorithms will benefit from processing or storing the grid cells in Z-order. This order is defined as follows.
Say our input is a grid  of height  and width . This grid is contained in a matrix  of  cells, where  is the smallest power of 2 such that  and . Let the first cell of  (in the upper left corner) also be the first cell of . The matrix  has four quadrants of  cells. When storing  in Z-order, we first store the part of  in the upper left quadrant of , then the part of  in the upper right quadrant of , then the part of  in the lower left quadrant of , and finally the part of  in the lower right quadrant of . Within each quadrant, the cells of  are ordered recursively in the same way (see Figure~\ref{fig:zorder}, right).

\paragraph{Computing Z-order coordinates from row/column coordinates and vice versa}

Assume, for now, that , and consider the cell  in row  and column  of the grid. Let  and  be the binary representations of  and  (where ). Then the position of  in a row-by-row file is , and its position in a Z-order file is . It is therefore quite easy to obtain, for any cell, its position in a row-by-row file from its position in a Z-order file and vice versa.

When the input grid is not square or when its height and width are not integral powers of two, the conversion is slightly more difficult, but in practice it can still be done efficiently as follows. Let  be a grid of size  stored in Z-order. Then  has size , where . The sequence of cells of the matrix  in Z-order can now be divided into  subsequences , such that each sequence  consists of cells in  and each sequence  consists of cells outside~. We can now construct two arrays  and , where  and . Thus  and  store, for each segment , its offset in the file (which only stores the cells in ) and its offset in the Z-order traversal of , respectively.

An offset  in the Z-order file can now be converted to row and column coordinates as follows: find the highest  such that , compute , and extract the row and column coordinates  and . Row and column coordinates can be converted to an offset in the Z-order file in a symmetric way.

Note that  and  can be computed without reading the input grid. For all reasonable grid sizes ~and  easily fit in main memory. When this is not the case, the number of \ios incurred by swapping blocks of  and  is always dominated by the number of \ios incurred by accessing the cells whose offsets or coordinates are being computed. The costs of converting coordinates can therefore be ignored in the \io-efficiency analysis of the algorithms in this paper.

From a practical point of view one should also consider the effort involved in the bit manipulations that are needed to extract  and  from , and to find the Z-order offsets of the neighbours of a given cell . This can also be done efficiently with a method called \emph{dilated arithmetic} and another set of look-up tables of size ~\cite{bitmanipulation}.

\paragraph{Converting grids from/to Z-order efficiently}
To convert a grid from row-by-row order to Z-order, we consider two very simple cache-oblivious algorithms: (A) read each cell from the row-by-row file and write it to the Z-order file, going through the grid in Z-order, and (B) read each cell from the row-by-row file and write it to the Z-order file, going through the grid row by row.

Algorithm (A), the Z-order scan, is efficient when we assume a tall cache. Then there is a  such that  is a power of two, a square of  cells occupies  blocks of the row-by-row file on disk, and these blocks fit in memory.
The algorithm processes roughly  such squares that each form a contiguous section of the Z-order file. For each such section the necessary blocks from the row-by-row file can be read and kept in memory until the section is completely processed. The total number of \ios for the conversion thus becomes at most  for reading plus  for writing, making a total of .

If  rows of the grid fit in memory, where  is the number of bytes per cell, then algorithm (B) is even more efficient. Algorithm (B), the row-by-row scan, would read every input block (from the row-by-row file) once and it would be able to keep every output block (which is  rows high) in memory until all of its cells have been read. Thus every output-block only needs to be written to disk once. In practice the assumption that  rows of the grid fit in memory seems reasonable; therefore we assume that the \io-volume of the conversion from row-by-row to Z-order is twice the input size. When  rows do not fit in memory, algorithm (B) may cause every output block to be reloaded  times, so that the algorithm uses  \ios.

Alternatively, one may adapt merge sort to convert a row-by-row file to Z-order in  \ios, making two or three read/write passes over the input in practice.

To convert a grid from Z-order to row-by-row order, the same algorithms can be used while substituting reading for writing and vice versa.

\section{Algorithms for flow accumulation}

We assume that the input to the flow accumulation problem consists of a file  that contains one byte for each cell, stored row-by-row. The byte for cell  indicates the \emph{flow direction} of : to which of the eight neighbours of  (if any) any water that arrives at  will flow. This neighbour is called the \emph{out-neighbour} of . The required output is a file  with eight bytes per cell, stored row-by-row, that specify each cell's \emph{flow accumulation}. The flow accumulation of a cell  is the number of cells that lie upstream of , including  itself. We can picture the flow accumulation of  as the total number of units of rain that arrive at or pass through  when each cell receives one unit of rain from the sky, which then flows down over the surface from cell to cell, following the flow directions, until it arrives at a cell that does not have an out-neighbour.

In the following we describe several algorithms for the flow accumulation problem and also discuss their \io-volume (number of \ios times block size) divided by the total size of the input and the output files. With input and output as defined above, the input+output size is 9 bytes times the number of cells in the grid.

\subsection{An I/O-na\"ive flow accumulation algorithm}\label{sec:naiveacc}

Our first algorithm is just clever enough to run in linear time, but utterly na\"ive from an \io point of view.
In this algorithm, let  be the index (offset in the input file) of a cell in , and let  be the index of the neighbour of cell  to which the incoming water of  flows. Note that  can be computed from  and the flow direction stored for  in .

\begin{codebox}
\Procname{}
\li Create a file  with flow value (initially 1) and marking bit (initially not set) for each cell
\li \For each cell  (in row-by-row order)
\li \Do  \If  is not marked
\li      \Then  
\li             \While all in-neighbours of  are marked and  has an out-neighbour 
\li             \Do    mark 
\li                    
\li                    
                \End
         \End
    \End
\end{codebox}

The algorithm goes through all cells and gives each of them one unit of water; this water is then forwarded downstream until a cell is reached that is still waiting for incoming water from some of its neighbours. The accumulated flow then waits there until the cell has received the incoming flow from all neighbours, so that for each cell, all incoming flow is forwarded downstream in a single operation. Thus the algorithm runs in linear time.

However, the \io-behaviour of this simple algorithm can be quite bad: consider processing the terrain shown in Figure~\ref{fig:meanders} (left) in row-by-row order. None of the cells on the meandering river can have their flow forwarded downstream until the scan reaches the last cell in the lower right corner. At that point the while-loop of lines 5 to~8 will follow the whole river back to the upper left corner, possibly requiring one \io almost every step of the way. The worst-case \io-efficiency is therefore . Fortunately, under the confluence assumption the situation does not look so grim:

\begin{theorem}\label{NaiveAccumulationRonR}
Under the confluence assumption algorithm \textsc{NaiveAccumulation} uses  I/O's.
\end{theorem}
\begin{proof}
Consider the grid to consist of square subgrids of  cells.
While running the algorithm, we maintain that at line 3, the subgrid that contains  and the eight subgrids around it are in memory. When in line 5--8 we access a cell  that is currently not in memory, we load the subgrid that contains  and the subgrids around it into memory; upon termination of this loop we reload the subgrids that were in memory before entering the loop.

Ignoring the loop in line 5--8, each disk block of the row-by-row files is loaded into memory at most  times, causing  \ios in total. In line 5--8, the nine subgrids currently in memory are replaced (and possibly reloaded afterwards) only when we reach and finish a cell on the boundary of the nine subgrids after following a path that started from the subgrid in the middle. By the confluence assumption this happens at most a constant number of times for each group of  subgrids. Since there are  such groups (one centered on each subgrid) and each is stored across at most  blocks on disk, the algorithm takes  \ios in total.
\end{proof}

\subsection{Flow accumulation in Z-order with row-by-row files}

We can make algorithm \textsc{NaiveAccumulation} slightly smarter by changing the loop in line 2 to go through all cells in Z-order. This does not change the worst-case \io-efficiency of the algorithm (extreme rivers could still cause  \ios). However, we would expect significantly better performance in practice:

\begin{theorem}\label{NaiveAccumulationZonRrealistic}
Under the confluence assumption and with a tall cache of size , algorithm \textsc{NaiveAccumulation}, running the loop on line 2 in Z-order, needs only  I/O's.
\end{theorem}
\begin{proof}
By the tall-cache assumption, there is a  such that  is a power of two and the blocks containing a section of  cells fit in memory. Now consider the grid to consist of square subgrids of  cells. Thus each subgrid contains  cells and is stored in  blocks in the row-by-row files. The idea is again to keep the subgrid of the current cell in memory, together with the eight subgrids around it.

Ignoring the loop in line 5--8, each group of  subgrids is loaded into memory once: each group is loaded when the Z-order scan of line 2 enters the subgrid  in the middle of the group, and the Z-order scan then traverses  completely before proceeding to the next subgrid. Thus each group is loaded at most once and each subgrid is loaded at most nine times, causing  \ios. Following the same analysis as before, lines 5--8 cause each group of  subgrids to be evicted from memory at most a constant number of times, causing  \ios as well.
\end{proof}

\subsection{Flow accumulation in Z-order with Z-order files}

Algorithm NaiveAccumulation can be improved further by not only processing the cells in Z-order, but also using input in Z-order and producing output in Z-order. We now get the following:

\begin{theorem}
Algorithm \textsc{NaiveAccumulation}, running the loop on line 2 in Z-order and working on files in Z-order, needs only  I/O's in the worst case. Under the confluence assumption, the required number of I/O's is .
\end{theorem}
\begin{proof}
As in the proof of Theorem~\ref{NaiveAccumulationRonR}, we consider the grid to be divided into subgrids of size . In Z-order files, these occupy only a constant number of blocks each. Such a subgrid  is loaded into memory when we access a cell on its boundary, or when it is reloaded after completing the loop in lines 5--8 of the algorithm. In the latter case we can charge the reloading of block  to the access to the boundary cell of  that was last accessed before  was evicted from memory. Since each subgrid has only  cells on its boundary and each cell of the grid is accessed only a constant number of times, each of the  subgrids is loaded only  times, causing  \ios in total.

For the \io-efficiency under the confluence assumption, see the proof of Theorem~\ref{NaiveAccumulationZonRrealistic}.
\end{proof}


\subsection{Cache-aware flow accumulation using separators}\label{sec:cacheawareaccumulation}

We will now describe a slightly more involved algorithm that achieves an \io-efficiency of  even in the worst case. Unlike the previous algorithms, this algorithm is cache-aware: it needs to know  and~. The algorithm is based on separators much in the same way as the single-source shortest paths algorithm of Arge et al.~\cite{gridproblems} (but with a much smaller ``reduced'' graph). We present a cache-oblivious variant of our algorithm in Section~\ref{sec:cacheobliviousaccumulation}.

Let  be chosen such that a subgrid of size  fits in memory (while leaving a bit of space to store additional information about its boundary cells). Our cache-aware algorithm processes the grid in subgrids of size , where the boundary cells of each subgrid are shared with the neighbouring subgrids (except on the outer boundary of the grid). Let  be the set of boundary cells of all subgrids. For any such subgrid , let  be the set of cells of  that do not lie on the boundary of , and let  be the union of all subgrid interiors, that is, all cells except those in . Given a subgrid  and a cell  on its boundary, we define its local destination  as follows: if the out-neighbour of  lies outside , then  is undefined, otherwise it is the first boundary cell of  that lies downstream of  (if it exists). The algorithm now proceeds in three phases.

In the first phase, the rain that falls on each cell of  is forwarded downstream to the first cell of  that is reached (if any). The rain is only stored at that cell of , not at the cells on the way. In the second phase, the rain that arrived at each cell  of , together with the rain that falls on , is forwarded to all cells of  downstream of . In the third phase, the rain that falls on each cell of  and the rain that arrived at each cell of  is forwarded to all cells of  that lie downstream of it, until another cell of  is reached (or a cell without an out-neighbour). The result of these three phases is that the rain that falls on any grid cell  is forwarded to all cells downstream of  (see Figure~\ref{fig:cacheawarephases} for an illustration, Figure~\ref{fig:cacheawarecode} for pseudocode).

\begin{figure}
\centering
\includegraphics[width=\hsize]{phases.pdf}
\caption{Flow accumulation in three phases. The shaded cells are the cells of . From left to right: input; contents of \id{SNeighbours} and \id{SFlowAcc} after the first phase; contents of \id{SFlowAcc} after the second phase; contents of \id{FlowAcc} after the third phase.}
\label{fig:cacheawarephases}
\end{figure}

\begin{figure}
\begin{codebox}
\Procname{}
\li Create file  with flow value (initially 1) and marking bit (initially not set) for each cell
\li Create file  with out-neighbour (initially undefined) for each cell of 
\li Create file  with flow value (initially 1) and marking bit (not set) for each cell of 
\li \Comment phase one: push flow from subgrid interiors to 
\li \For each subgrid 
\li \Do  run \proc{NaiveAccumulation} on  in memory,
\li      \hbox{}\quad\quad\quad\quad\quad\quad starting with zero flow on the cells of , and
\li      \hbox{}\quad\quad\quad\quad\quad\quad leaving  undefined for each cell of 
\li      \For each cell  on the boundary of 
\li      \Do  store flow accumulation of  in 
\li           \hbox{}\quad\quad\quad\quad\quad\quad (adding it to any previously stored value)
\li           \If  is defined
\li           \Then 
              \End
         \End
    \End
\li \Comment phase two: compute flow accumulation for 
\li Run line 2--8 of \proc{NaiveAccumulation} on , using neighbour relations in 
\li \Comment phase three: recompute flow in subgrid interiors, now including flow that comes in from 
\li \For each subgrid 
\li \Do  \For each cell  on the boundary of 
\li      \Do 
\li          \If  lies in 
\li          \Then 
             \End
         \End
\li      run line 2--8 of \proc{NaiveAccumulation} on , using the file ,
\li      \hbox{}\quad\quad\quad\quad\quad\quad leaving  undefined whenever  lies on the boundary of 
    \End
\end{codebox}
\caption{Pseudocode for cache-aware separator-based flow accumulation.}
\label{fig:cacheawarecode}
\end{figure}

\begin{theorem}\label{CacheAwareAccumulation}
With a tall cache of size , algorithm \textsc{CacheAwareAccumulation} needs only  I/O's.
\end{theorem}
\begin{proof}
In the first and the third phase, we process  blocks that each take  \ios to read, can be processed completely in main memory, and---in the third phase---may take another  \ios to write. The second phase runs a linear-time algorithm on a file of size , of which each record is accessed a constant number of times in the first and the third phase. The total number of \ios is therefore . By the tall-cache assumption, this is .
\end{proof}

\paragraph*{I/O-volume estimate for realistic values of ,  and }
\textsc{CacheAwareAccumulation} does not depend on the interplay between ,  and  as much as the previous algorithms. Therefore it is possible to give a good estimate of the \io-volume. The bottleneck is the third phase. Assume we set up \id{SFlowAcc} and \id{SNeighbours} such that they first store, row by row, all rows that completely consist of cells of , and then, column by column, the remaining cells of . Then we need to choose  such that the blocks containing  rows of  cells from \id{FlowAcc} and \id{FlowDir} fit in memory, plus 4 rows of  cells from \id{SFlowAcc} and \id{SNeighbours}. In \id{FlowDir} we use 1 byte per cell, in the other files we use 8 bytes per cell. So  must satisfy:
A sufficient (but not necessary) condition is that  is at most roughly . This means that  will typically be several thousands:
for  and  we would get ; for  and  we would get .

Taking the number of subgrids times the number of rows times the number of blocks per row times the block size times two (for reading and writing), we find that the \io-volume for accessing \id{FlowAcc} in the third phase is now roughly
.
From this we may subtract , since the blocks of \id{FlowAcc} do not need to be read from disk the first time they are accessed.
For accessing  in the first and third phase (reading only) we get roughly
.

The remaining \io is neglectible: for accessing  in the first phase we get roughly
, which is roughly three orders of magnitude smaller than the amount of \io for accessing \id{FlowAcc}. The same is true for accessing \id{SFlowAcc} in the first and the third phase, and for accessing these files in the second phase: in practice  will be small enough that the second phase can be done in main memory.

The \io-volume thus adds up to roughly , dividing this by  (the input+output size) we get an ``overhead'' of factor . For the example values of  and  just mentioned, the result ranges from 2.0 to 6.6.

\subsection{Cache-aware flow accumulation using separators with Z-order files}\label{sec:cacheawareaccumulationZorder}

The algorithm explained above depends on the tall-cache assumption. This is because the boundary of a square of  cells may cross  blocks on disk, and without the tall-cache assumption, there is no guarantee that these will fit in memory for any . The need for the tall-cache assumption can be eliminated by using files in Z-order. Then any square  of  cells is contained in at most four squares of size , where , that are each stored contiguously on disk. The square  is thus stored in  blocks, which will always fit for some . Thus we get:

\begin{theorem}\label{CacheAwareAccumulation}
Algorithm \textsc{CacheAwareAccumulation} on files in Z-order needs only  I/O's.
\end{theorem}

\paragraph*{I/O-volume estimate for realistic values of ,  and }
To get a good \io-volume in practice, we will now make sure that each subgrid  is stored consecutively in the Z-order file. To achieve this, we do not let neighbouring subgrids share rows and columns anymore, but make sure the subgrids are disjoint and their height is a power of two. Furthermore, we let the loops of line 5 and line 16 go through the subgrids in Z-order.

As a result, the first phase only scans each block of the \id{FlowDir} file once, and so does the third phase; in addition the third phase writes each block of \id{FlowAcc} once (reading is not necessary). There may be up to eight times more cells in  (two times more because subgrids do not share boundary cells anymore, and four times more because we need to round the subgrid width down to a power of two). However, as explained above, the accesses to \id{SNeighbour} and \id{SFlowAcc} are so few that they will still be neglectible. Thus the total \io-volume becomes roughly N + N + 8N bytes, which is 1.1 times the input+output size.

\subsection{Cache-oblivious flow accumulation using separators}\label{sec:cacheobliviousaccumulation}

We can make a cache-oblivious version of the separator-based algorithm by using a hierarchy of subgrids and separators as follows. We consider  levels, numbered 0 to , where  is the smallest integer such that the input grid is contained in a  grid. On level , we consider subgrids of size , that each share their boundary cells with the neighbouring subgrid. Let  be the set of boundary cells of all subgrids on level , and let  be the union of all subgrid interiors on level . Note that ,  is the full grid, and  is the boundary of the full grid. We can think of the subgrids as being organised in a tree~, where the leaves correspond to subgrids of size , and each internal node on level  corresponds to a subgrid  on level  with four children, namely the subgrids on level  that lie inside . The root of~ corresponds to the full grid.

The algorithm now proceeds in two phases. In the first phase we do a post-order traversal of~. When processing a subgrid  at level , we forward the rain that falls on each cell  of  to the first cell of  downstream of , that is, to the boundary of . In the second phase we traverse  in reverse order (which is therefore a pre-order traversal). When processing a subgrid  at level , we forward the rain that arrived at each cell  on the boundary of  to all cells of  that lie on the path that leads downstream from  until it reaches another cell on the boundary of  (or a cell without an out-neighbour). These two phases together result in all rain that falls on any grid cell  to be forwarded to all cells downstream of~.

To implement this approach efficiently, we use the post-order traversal to create a stack  of pointers from boundary cells to internal cells that can be retrieved during the reverse traversal. More precisely, when processing a subgrid  at level  in the post-order traversal, we store on  the coordinates of the first cell of  that lies downstream of , for each cell  on the boundary of  that has an out-neighbour inside .

\begin{theorem}\label{CacheAwareAccumulation}
With a tall cache of size , separator-based cache-oblivious flow accumulation needs only  I/O's.
\end{theorem}
\begin{proof}
For simplicity we assume that the input grid is square.
We first analyse the number of pointers  that are stored on  for a subtree of  rooted at a subgrid  of size  (where a cell on the boundary of two/four subgrids counts for 1/2 or 1/4 to each of them). Since pointers are stored only for cells on the boundary of , we get , which solves to .

Next we analyse the number of \ios  for the post-order traversal of a subtree of  rooted at a subgrid  of size . For a sufficiently small constant , we have , since we would simply load  into memory, do all the necessary processing in memory, and then write  back to disk while  pushing  pointers for  and its subgrids on the stack. By the tall-cache assumption, we thus have . For subgrids  of size , we need to make the recursive calls, read the flow values for  cells and the  pointers stored on  for the four subgrids of , and then use this information to forward flow to  cells, compute  pointers for  and push these onto . Forwarding the flow and computing the pointers can easily be done in  time and \ios, so that we get: .
With base case , this solves to .
The analysis of the reverse traversal is similar to the analysis of the post-order traversal, so that we get  \ios in total.
\end{proof}

Although the cache-oblivious algorithms needs only  \ios, it is not as efficient as the cache-aware algorithm of the previous subsection. The cache-oblivious algorithm as just described is expensive in two ways. First, the number of pointers over all levels sums up to approximately . Except a small number on the highest levels, all of these pointers are written to and read from disk at least once. If a pointer is stored in 16 bytes (coordinates of source and destination), this amounts to an \io-volume of almost 10 times the input+output size. To alleviate this problem, an efficient implementation should use larger subgrids as the base case (for example  instead of ). Second, we need to maintain flow accumulation values for boundary cells. Doing so directly in the output file is expensive, because on a vertical subgrid boundary, every cell will be in a different block. Although in the asymptotic analysis this works out, it causes a significant constant-factor overhead. Storing the grid in Z-order helps to some extent (as confirmed by our experiments, see Section~\ref{sec:evaluation}). Alternatively one could use the stack with pointers to store intermediate flow accumulation values (similar to  in \proc{CacheAwareAccumulation}), but this would further increase the \io-volume for stack operations.

\subsection{Previous work: flow accumulation with time-forward processing}\label{sec:tfpaccumulation}

In \tfl and \tsm~\cite{terraflow,terrastream}, the following algorithm is used to compute flow accumulation. We first create a stream of cells in topological order. In principle it would suffice to sort the cells by decreasing elevation, but this would not work for flat areas. Therefore we will assume that after flooding, a flow routing phase has produced an additional file with a topological number for each cell. We now create a stream of cells that lists for each cell its location, its topological number and the topological number of its out-neighbour. We sort this stream by the first topological number.

We now apply a technique called time-forward processing: we maintain a priority queue in which flow values are stored with the topological number of the cell to which the flow is going. The algorithm processes all cells one by one in topological order; when processing a cell, it extracts its incoming flow from the priority queue, adds one unit of rain, writes the resulting total to an output stream together with the coordinates of the cell, and finally forwards the total flow to the out-neighbour by entering it in the priority queue with the out-neighbour's topological number as key. The algorithm finishes by sorting the flow accumulation values from the output stream into a row-by-row grid.

Using an \io-efficient priority queue, the algorithm clearly runs in  \ios~\cite{terraflow,gridproblems}

\paragraph*{I/O-volume estimate for realistic values of ,  and }
For an estimate of the \io-volume in a practical setting with a grid of size  and a memory size of , we consider two scenarios. In the optimistic scenario, we have block size , only 1/3 of the cells contain data, and the priority queue is completely maintained in memory at all times. In the pessimistic (but nevertheless quite realistic) scenario, we have block size , all cells contain data, and the records that pass through the priority queue are written to and read from disk once on average.

In the optimistic scenario, we start with scanning the input file with flow directions and the file with topological numbers, scanning the latter with a window of  cells to be able to retrieve topological numbers of neighbours. Assuming three rows of the grid fit in memory, this amounts to scanning 9 bytes per cell. While doing so, we generate a stream of cells with location, topological number, and topological number of the out-neighbour, which is fed to the first pass of a sorting algorithm. The sorting algorithm writes the partially sorted stream to disk (24 bytes per actual data cell = 8 bytes per grid cell). Since , the sorting algorithm needs only two passes; the second pass results in 16 bytes of \io per cell (24 for reading, 24 for writing, for 1/3 of the cells). The time-forward processing phase reads the sorted stream (24 bytes  1/3 of the cells) and eventually outputs locations and flow accumulations for all data cells (16 bytes  1/3 of the cells). The output then needs to be sorted: the first pass results in  bytes of \io per cell (32 bytes per data cell). The second pass reads  bytes per cell and writes 8 bytes per grid cell (flow accumulations only, but also for non-data cells). In total, we transfer  bytes per grid cell. For a fair comparison, we do not count the input file with topological numbers towards the size of the input and output---after all, all other algorithms presented in this paper do not even need such a file. So  bytes per grid cell amounts to 7.8 times the input+output size.

In the pessimistic scenario, we need three sorting passes, and the priority queue needs the disk. Filling in the details of the computation above, we get an \io-volume of 289 bytes per cell, which is 32 times the input+output size.

\subsection{A brief comparison of flow-accumulation algorithms}\label{sec:evaluation}
We implemented some of our algorithms and tested them on elevation models of the Netherlands and of the Neuse watershed in North Carolina, using a Dell Optiplex GX260 computer, equipped with a 3 GHz Pentium 4 processor and 1 GB of RAM, Ubuntu 7.04 (kernel 2.60.20-16) (installed on a 80 GB Samsung HD080HJ hard disk), and a 250 GB Seagate ST320506AS hard disk. We report the results for the largest data set: a grid of  cells modelling the Neuse basin; 35\% of the grid cells contain data.

Our results are shown in Table~\ref{tab:runningtimes}. For comparison, the authors of \tsm reported 455 minutes for flow accumulation of a grid of similar size, on hardware that appeared to be faster than ours~\cite{terrastream}.
Our cache-aware flow accumulation algorithm thus seems to be an order of magnitude faster. Note that the \io-volume estimates given in Section~\ref{sec:cacheawareaccumulation} and Section~\ref{sec:tfpaccumulation} would predict a difference of a factor four in the optimistic setting, where we assume that \tsm's time-forward processing would manage to do with sorting in two passes and keeping the priority queue in main memory. Although we have not run direct comparisons with the latest version of \tsm, our analysis indicates that the performance difference must remain significant because it is inherent to the characteristics of the different algorithms.

\begin{table}
\centering\def\arraystretch{1.1}
\begin{tabular}{|l|rr|rr|}
\hline
algorithm                      & \multicolumn{2}{c|}{using Seagate disk} & \multicolumn{2}{c|}{using both disks} \\
&  \multicolumn{1}{l}{time} & CPU usage                & \multicolumn{1}{l}{time} & CPU usage\\

\hline
``na\"ive'' row-by-row scan of row-by-row file & 111 min. & 22\% & & \\
``na\"ive'' Z-order scan of Z-order file    & 41 min. & 26\% & 34 min. & 32\% \\
cache-aware separator alg. on row-by-row file & 39 min. & 18\% & 25 min. & 26\% \\
\hline
\end{tabular}
\caption{Running times in minutes and CPU usage for flow accumulation of a grid of  cells.}
\label{tab:runningtimes}
\end{table}

Our results also indicate that our \io-na\"ive algorithms perform surprisingly well, especially when working on data in Z-order. At first sight our theoretical analysis seems to explain this: under the confluence assumption the \io-na\"ive algorithm on Z-order files needs only  \ios. However, if we try to estimate the \io-volume by filling in the constant factors in the computation of the asymptotic bound, then we end up with an \io-volume bound of dozens (using Z-order) or thousands (using row-by-row order) times the input+output size. It seems that the surprisingly good performance in practice must be due to the fact that for modest values of  we can fit  rows of the grid in memory. Thus the main scan can keep blocks in memory long enough so that each block only needs to be loaded once, and apparently the inner loop of the algorithm does not cause as many disruptions as the theoretical analysis might suggest. All things considered this means that these na\"ive algorithms are surprisingly fast, but we cannot rule out that their efficiency may depend on the ratio of  and  and on the characteristics of the terrain.

In contrast, the efficiency of our cache-aware algorithm is supported firmly by our theoretical analysis. Note that our theoretical analysis also indicates that the algorithm should be much faster still when working on Z-order files, requiring little more than two scans of the input file and a single scan of the output file (further experiments should confirm this). However, converting to Z-order takes time too (converting a grid of this size with 8 bytes per cell took 88 minutes). Whether it pays off to use files in Z-order may therefore depend on the context. Storing temporary files in the pipeline in Z-order seems to be a good idea; files that need to be processed by other software may better be kept in row-by-row order.

We also implemented the cache-oblivious algorithm, but until now this was significantly slower than the cache-aware algorithm (even when the subgrid size of the latter was not tuned optimally) so the cache-oblivious implementation would need to be optimised further to be competitive.

\section{Applying grid-based techniques to other parts of the pipeline}
\label{sec:otherstages}

\subsection{Flooding}
The flooding problem takes as input a file  that stores the elevation of each cell. A path in the grid is a sequence of cells such that each pair of consecutive cells on the path are neighbours of each other (each cell that is not on the boundary has eight neighbours). The flooding problem in its basic form is to compute to what elevation each cell should be raised, so that from each cell there is a non-ascending path to a cell on the boundary of the grid. If we define the height of a path as the elevation of the highest cell on the path, the problem is equivalent to determining, for each cell , the height of the lowest path from  to the boundary. The required output is a file  that stores these heights for each cell .

\paragraph{Time-forward processing and I/O-na\"ive flooding}
In 2003 Arge et al.\ described two algorithms for the flooding problem~\cite{terraflow}. The first algorithm proceeds in three phases.
First we follow the path from each cell downhill until a sink, a cell without an out-neighbour, is reached. Each cell  is labelled with a pointer  to the sink at the end of the path that goes downhill from . This labelling constitutes a decomposition of the terrain into watersheds and is used to build a \emph{watershed graph} . The watershed graph has one node for every sink, and for every pair of adjacent watersheds with sinks  and , the graph contains an edge  with height equal to the lowest pass between  and . In other words, the elevation of  is the minimum of  over all pairs of neighbouring cells  such that  and .
In the second phase an algorithm is run on  to determine to what height each watershed should be flooded. In the third phase, we scan the complete terrain and replace each cell's elevation by the maximum of its original elevation and its watershed's flood height.

Arge et al.\ use time-forward processing for the first phase, so that the complete algorithm needs  \ios, assuming  can be kept in main memory. Instead we could consider using an \io-na\"ive algorithm similar to the one described in Section~\ref{sec:naiveacc} for the first phase, and get  \ios under the confluence assumption. We found that in practice, this did not work well with row-by-row-ordered files. Using files in Z-order we could flood the Neuse grid mentioned before in 435 minutes: a running time of roughly the same order of magnitude as Terrastream's~\cite{terrastream}.

\paragraph{Separator-based algorithms}
The second algorithm sketched by Arge et al.\ seems quite similar to our cache-aware separator-based flow accumulation algorithm. As in Section~\ref{sec:cacheawareaccumulation}, the idea is to first process subgrids of  cells that fit in memory. For each subgrid  we compute a `substitute' graph that encodes the lowest-path heights between each pair of cells on the boundary of . In the second phase of the algorithm we would combine the substitute graphs for all subgrids into one graph, which is used to compute the lowest-path heights from each cell of  to the boundary of the grid, where  is the set of boundary cells of all subgrids. Finally, in the third phase each subgrid  is processed again, now to compute the lowest-path height from each cell of  to the boundary of the grid, using the previously computed lowest-path heights for the cells on the boundary of .

The key to success is the size of the substitute graphs. The algorithm by Arge et al.~\cite{gridproblems} for single-source \emph{shortest} paths works in a similar way, but needs substitute graphs of size  to encode the shortest-path distances between all pairs of cells on the boundary of a  subgrid. However, for \emph{lowest} paths substitute graphs of size  suffice. Such a graph can be created from the elevations and neighbour relations in a subgrid  as follows. Consider the subgrid  as a graph, whose edges represent the neighbour relations in the grid, where each edge has elevation equal to its highest vertex. Compute the lowest paths to the boundary of  for all cells in . Next, contract all directed edges  of those lowest paths one by one, replacing each undirected edge  of the graph with an edge  with elevation . Whenever there are multiple edges between the same pair of vertices, keep only the edge with the lowest elevation. This results in a substitute graph whose vertices are the boundary vertices of  and which preserves the lowest-path heights between each pair of vertices. Since the substitute graph thus constructed is planar, it has size .

We implemented a separator-based flooding algorithm as described above, and found that it could process the Neuse grid in 146 minutes, using row-by-row-ordered files on one disk, or in 132 minutes on two disks. As with our flow accumulation algorithm, we believe further efficiency gains could be achieved by using files in Z-order.

\paragraph{Partial flooding?}
It should be noted that a direct comparison between our algorithms and \textsc{Terra-Stream} cannot be made. \tsm offers the functionality of \emph{partial flooding}: eliminating only insignificant depressions while not flooding major depressions. Our algorithms do not do this. A major open question is therefore how the grid structure could be exploited to design an algorithm that can do very fast \emph{partial} flooding.

\subsection{Other parts of the pipeline}
\tsm's hierarchical watershed labelling algorithm~\cite{pfafstetter,terrastream} uses time-forward processing to pass labels upwards into the river network. This is not very different from how time-forward processing is used for flooding or for flow accumulation, and one may expect that grid-based algorithms (\io-na\"ive or cache-aware) may help here too. Open questions include whether grid-based algorithms, maybe together with assumptions on realistic terrains, could help to simplify and speed up flow routing on flat areas and to do flow accumulation with multiple-flow directions, an approach where each cell sends water to all of its lower neighbours instead of just one of them.

\section{Conclusions and remaining work}
We have shown that certain hydrological computations on terrain data may be sped up by an order of magnitude by exploiting the grid structure of the data and/or by storing grids in Z-order rather than row-by-row order. A striking result is that one of the algorithms with the best performance is actually so simple that it is almost na\"ive. Some of the most prominent questions that remain unanswered at this point are the following.

\emph{What would be typical values for the confluence constant?}
It would be interesting to design an algorithm that can compute the confluence constant for any given grid terrain. Then we may investigate to what extent the confluence constant is indeed independent of the sampling density of a terrain, and what are typical values for the confluence constant for different types of terrains.

\emph{How would the cache-aware separator-based algorithm perform on files in Z-order?}
Unfortunately we did not have time to implement and run these tests yet, and we hope to be able to do so some time in the future.

\emph{Can we exploit the grid structure to design equally efficient flow routing algorithms?}
If yes, then the complete part of the pipeline from elevation model to hierarchical watershed labels could probably be sped up tremendously. In that case, even if files in row-by-row order would be desired at the input and output end of the pipeline, it could pay off to convert them to Z-order and to use files in Z-order for the intermediate stages. A disadvantage of our current algorithm is that multiple-flow direction models cannot be handled, but for hierarchical watershed labelling such models cannot be used and single-flow direction models---those handled by our algorithms---are exactly what is needed.

\paragraph*{Acknowledgements}
The authors thank Andrew Danner for providing test data and assistance with \tsm.
We thank Laura Toma and the students of the 2006 class of \io-efficient algorithms
at the TU Eindhoven for inspiring discussions.

\small
\bibliographystyle{abbrv}

\begin{thebibliography}{10}
\bibitem{cloud}
P. K. Agarwal, L. Arge, and A. Danner.
From point cloud to grid DEM: a scalable approach.
In {\em Proc. 12th Int. Symp. Spatial Data Handling} (SDH 2006), pages 771--788, Springer, 2006.

\bibitem{unionfind}
P. K. Agarwal, L. Arge, and K. Yi.
I/O-efficient batched union-find and its applications to terrain analysis.
In {\em Proc. 22nd Ann. Symp. Computational Geometry} (SoCG 2006), pages 167--176, 2006.

\bibitem{iomodel}
A. Aggarwal and J. S. Vitter.
The input/output complexity of sorting and related problems.
{\em Communications of the ACM}, 31(9):1116--1127, 1988.

\bibitem{terraflow}
L.~Arge, J.~Chase, P.~Halpin, L.~Toma, D.~Urban, J.~S.~Vitter, and R.~Wickremesinghe.
Flow computation on massive grid terrains.
{\em GeoInformatica}, 7(4):283--313, 2003.

\bibitem{pfafstetter}
L. Arge, A. Danner, H. Haverkort, and N. Zeh.
I/O-efficient hierarchical watershed decomposition of grid terrain models.
In {\em Proc. 12th Int. Symp. Spatial Data Handling} (SDH 2006), pages 825--844, Springer, 2006.

\bibitem{gridproblems}
L. Arge, L. Toma, and J. S. Vitter.
I/O-efficient algorithms for problems on grid-based terrains.
{\em ACM J. Experimental Algorithmics}, 6(1), 2001.

\bibitem{terrastream}
A.~Danner, T.~M\o lhave, K.~Yi, P.~K.~Agarwal, L.~Arge, and H.~Mitasova.
\textsc{TerraStream:} from elevation data to watershed hierarchies.
In {\em Proc. 15th Ann. ACM Symp. Advances in Geographic Information Systems.} (GIS 2007).

\bibitem{flpr-coa-99}
M.~Frigo, C.~E. Leiserson, H.~Prokop, and S.~Ramachandran.
Cache-oblivious algorithms.
In {\em Proc. 40th Annu. IEEE Symp. Found. Comput. Sci.}, pages
  285--298, 1999.

\bibitem{bitmanipulation}
J.~Thiyagalingam and P. Kelly.
Is Morton layout competitive for large two-dimensional arrays?
In {\em Proc. 8th Int. Euro-Par Conf. Parallel Processing, 2002,} LNCS 2400:137--157.

\end{thebibliography}
\end{document}
