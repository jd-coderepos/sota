\documentclass{article} \clearpage{}\usepackage{amsmath,amssymb,amsthm,bbm}
\usepackage[hmargin=2cm,vmargin=2cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{tikz-qtree}
\usepackage{endnotes}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{endnotes}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage{forest}
\usepackage{mdframed}

\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\newcommand{\halfopen}[2]{\ensuremath{[#1,#2)}}
\newcommand{\doubbr}[1]{\left[\left[#1\right]\right]}
\newcommand{\singbr}[1]{\left[#1\right]}
\newcommand{\myset}[1]{\left\{#1\right\}}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\expect}[1]{\E\singbr{#1}}
\newcommand{\expected}[2]{\underset{#1}{\E}\singbr{#2}}
\newcommand{\hatexpected}[2]{\underset{#1}{\wh{\E}}\singbr{#2}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\floor}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\prob}[1]{\ensuremath{\Pr \paren{#1}}}
\newcommand{\Prob}[2]{\ensuremath{\underset{#1}{\Pr}\paren{#2}}}
\newcommand{\Span}[1]{\mathrm{span} \paren{#1}}
\newcommand{\dimm}[1]{\mathrm{dim} \paren{#1}}
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\corr}[2]{\mathrm{cor}\paren{#1, #2}}
\newcommand{\var}[1]{\mathrm{Var}\paren{#1}}
\newcommand{\cov}[2]{\mathrm{Cov}\paren{#1, #2}}
\newcommand{\covar}[3]{\underset{#1}{\mathrm{Cov}}\paren{#2, #3}}
\newcommand{\vari}[2]{\underset{#1}{\mathrm{Var}}\paren{#2}}
\newcommand{\stdev}[1]{\mathrm{stdev}\paren{#1}}
\newcommand{\tr}[1]{\mathrm{tr}\paren{#1}}
\newcommand{\diag}[1]{\mathrm{diag}\paren{#1}}
\newcommand{\col}[1]{\mathrm{col}\paren{#1}}
\newcommand{\range}[1]{\mathrm{range}\paren{#1}}
\newcommand{\row}[1]{\mathrm{row}\paren{#1}}
\newcommand{\nul}[1]{\mathrm{null}(#1)}
\newcommand{\rank}[1]{\mathrm{rank}\paren{#1}}
\newcommand{\nullity}[1]{\mathrm{nullity}\paren{#1}}
\newcommand{\wh}[1]{\ensuremath{\widehat{#1}}}
\newcommand{\wt}[1]{\ensuremath{\widetilde{#1}}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}
\newcommand*\wb[1]{\ensuremath{\widebar{#1}}}

\newcommand{\ep}{\ensuremath{\epsilon}}
\newcommand{\de}{\ensuremath{\delta}}
\newcommand{\la}{\ensuremath{\lambda}}
\newcommand{\si}{\ensuremath{\sigma}}
\newcommand{\ga}{\ensuremath{\gamma}}
\newcommand{\al}{\ensuremath{\alpha}}
\newcommand{\ka}{\ensuremath{\kappa}}
\newcommand{\be}{\ensuremath{\beta}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\Co}{\ensuremath{\mathbb{C}}}
\newcommand{\Zi}{\ensuremath{\mathbb{Z}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\E}{\ensuremath{\mathbf{E}}}
\newcommand{\x}{\ensuremath{\boldsymbol{x}}}
\newcommand{\y}{\ensuremath{\boldsymbol{y}}}
\newcommand{\z}{\ensuremath{\boldsymbol{z}}}
\newcommand{\X}{\ensuremath{\boldsymbol{X}}}
\newcommand{\Y}{\ensuremath{\boldsymbol{Y}}}
\newcommand{\Z}{\ensuremath{\boldsymbol{Z}}}
\newcommand{\C}{\ensuremath{\boldsymbol{C}}}

\newcommand{\Ra}{\ensuremath{\Rightarrow}}
\newcommand{\ra}{\ensuremath{\rightarrow}}
\newcommand{\ul}{\underline}
\newcommand{\ub}{\ensuremath{\underbrace}}
\usepackage{mathtools}
\newcommand{\ubk}{\ensuremath{\underbracket}}
\newcommand{\ob}{\ensuremath{\overbrace}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\by}{\times}

\newcommand*{\red}{\textcolor{Red}}
\newcommand*{\blue}{\textcolor{blue}}
\newcommand*{\green}{\textcolor{green}}
\newcommand{\dgreen}{\textcolor{ForestGreen}}
\newcommand{\black}{\textcolor{black}}
\newcommand{\purple}{\textcolor{purple}}
\definecolor{light-gray}{gray}{0.80}
\newcommand{\gray}{\textcolor{light-gray}}
\newcommand{\dgray}{\textcolor{gray}}
\definecolor{darkred}{rgb}{0.64, 0.0, 0.0}
\newcommand{\dred}{\textcolor{darkred}}
\newcommand{\skyblue}{\color{SkyBlue}}
\newcommand{\orange}{\textcolor{orange}}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}{Proposition}[section]
\newtheorem{asm}{Assumption}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{ex}{Example}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{fact}[thm]{Fact}
\newtheorem{defn}{Definition}[section]
\newtheorem{remark}[thm]{Remark}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\arginf}{arg\,inf}






\newcommand{\homework}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 533:~Natural Language Processing \hfill {\small (#2)}} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in {  {\it Instructor: {\rm #3}} \hfill}
      \vspace{2mm}}
   }
   \end{center}
   \vspace*{4mm}
}

\newcommand{\problem}[3]{
  \vspace{2mm}
  \begin{center}
     \vbox{
       \hbox to \textwidth { \framebox{{\bf Problem #1: #2}} \hfill \hfill {\small (#3)} }
     }
  \end{center}
}

\newtheorem{innercustomlem}{Lemma}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomlem{#1}\innercustomlem}
  {\endinnercustomlem}

\newenvironment{itemizesquish}{\begin{list}{\labelitemi}{\setlength{\itemsep}{-0.2em}\setlength{\labelwidth}{0.5em}\setlength{\leftmargin}{\labelwidth}
\addtolength{\leftmargin}{\labelsep}}}{\end{list}}

\def\ci{\perp\!\!\!\perp}


\newcommand{\pop}{\ensuremath{\mathbf{pop}}}
\newcommand{\dat}{\ensuremath{\mathbf{dat}}}
\newcommand{\Unk}{\ensuremath{\mathbf{Unk}}}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\clearpage{}
\usepackage{iclr2022_conference,times}
\usepackage{makecell}
\usepackage{adjustbox}

\title{EntQA: Entity Linking as Question Answering}

\author{Wenzheng Zhang, Wenyue Hua, Karl Stratos \\
Department of Computer Science\\
Rutgers University\\
\texttt{\{wenzheng.zhang,wenyue.hua,karl.stratos\}@rutgers.edu}
}
\iclrfinalcopy

\begin{document}
\maketitle


\begin{abstract}
  A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base.
  A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult.
  We present a new model that does not suffer from this limitation called \textbf{EntQA}, which stands for \textbf{Ent}ity linking as \textbf{Q}uestion \textbf{A}nswering.
  EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module.
  Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on
  pretrained models for dense entity retrieval and reading comprehension.
  Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision.
  EntQA achieves strong results on the GERBIL benchmarking platform.
\end{abstract}


\section{Introduction}
\label{sec:intro}

We consider the most general form of entity linking (EL) in which a system, given a document, must both extract entity mentions and link the mentions to
their corresponding entries in a knowledge base (KB).
EL is a foundational building block in automatic text understanding with applications to question answering (QA) \citep{ferrucci2012introduction},
information retrieval \citep{xiong2017word,hasibi2016exploiting,balog2013multi,reinanda2015mining}, and commercial recommendation systems \citep{yang2018collective,slawski2015}.

The output space in EL is intractably large. Any subset of all possible spans in the document linked to any KB entries (typically in the order of millions) can be a system output.
To get around the intractability, existing methods decompose EL into mention detection (MD) and entity disambiguation (ED) and tackle them with varying degrees of independence.
In all cases, however, the order of these two subproblems is MD followed by ED: first the system identifies potential entity mentions,
and then the mentions are resolved to KB entries.
Previous works either assume that mentions are given \citep{gupta2017entity},
run an off-the-shelf named-entity recognition (NER) system to extract mentions and resolve them by ED (MDED pipeline) \citep{hoffart2011robust,ling2015design,van2020rel},
or train an end-to-end model that jointly performs MDED by beam search \citep{kolitsas2018end,cao2021autoregressive}.

A limitation of performing MD before ED is that it requires finding mentions without knowing the corresponding entities.
By definition, a mention needs an entity (i.e., a mention of what?).
Existing methods suffer from the dilemma of having to predict mentions before what they refer to, which is unnatural and difficult.
For example, the MDED pipeline heuristically extracts mentions from spans of named entities found by a third-party NER system,
and the performance bottleneck is often errors in MD propagated to ED.
End-to-end models alleviate the problem of error propagation, but the search is only approximate and the dilemma,
albeit to a lesser degree, remains.

In this work, we propose flipping the order of the two subproblems and solving ED before MD.
We first find candidate entities that might be mentioned in the given document, then for each candidate find its mentions if possible.
Our key observation is that while finding mentions is difficult without the knowledge of relevant entities,
finding relevant entities is easy without the knowledge of their specific mentions.
This simple change fundamentally solves the dilemma above since identifying mentions of a particular entity is well defined.

We cast the problem as \emph{inverted} open-domain QA.
Specifically, given a document, we use a dual encoder retriever to efficiently retrieve top- candidate entities from the KB as ``questions''.
Then we apply a deep cross-attention reader on the document for each candidate to identify mentions of the candidate in the document as ``answer spans''.
Unlike in standard QA, the model must predict an unknown number of questions and answers.
We present a simple and effective solution based on thresholding.
We call our model \textbf{EntQA}, standing for \textbf{Ent}ity linking as \textbf{Q}uestion \textbf{A}nswering.

Beyond conceptual novelty, EntQA also offers many practical advantages.
First, EntQA allows us to piggyback on recent progress in dense entity retrieval and open-domain QA.
For instance, we warm start EntQA with the BLINK entity retriever \citep{wu2020scalable} and ELECTRA finetuned on a QA dataset \citep{clark2019electra} to obtain an easy improvement.
Second, EntQA has no dependence on a hardcoded mention-candidates dictionary which is used in previous works to reduce the search space and bias the model
\citep{ganea2017deep,kolitsas2018end,cao2021autoregressive}.
The dictionary is typically constructed using a large KB-specific labeled corpus (e.g., Wikipedia hyperlinks),
thus having no dependence on it makes our approach more broadly applicable to KBs without such resources.
Third, training EntQA is data efficient and can be done with an academic budget,
in contrast with GENRE \citep{cao2021autoregressive} which requires industry-scale pretraining by weak supervision.

EntQA achieves strong performance on the GERBIL benchmarking platform \citep{roder2018gerbil}.
The in-domain F score on the test portion of the AIDA-CoNLL dataset is 85.8 (2.1 absolute improvement).
The macro-averaged F score across 8 evaluation datasets is 60.5 (2.3 absolute improvement).\footnote{Code available at: \url{https://github.com/WenzhengZhang/EntQA}}
We analyze EntQA and find that its retrieval performance is extremely strong (over 98 top-100 recall on the validation set of AIDA),
verifying our hypothesis that finding relevant entities without knowing their mentions is easy.
We also find that the reader makes reasonable errors such as accurately predicting missing hyperlinks or linking a mention to a correct entity that is more specific than the gold label.


\section{Model}
\label{sec:model}

Let  denote the set of entities in a KB associated with a text title and description.
Let  denote the vocabulary and  the set of all documents up to length .
EL is the task of mapping  to  where
 is the set of all possible linked spans in  and  is the power set.
The size of the output space is  where  is typically very large (e.g., around 6 million in Wikipedia)
and  can also be large (e.g.,  in AIDA), ruling out any naive exhaustive search as a feasible approach.

EntQA decomposes EL into two subproblems: entity retrieval and question answering.
More specifically, given a document ,
\begin{enumerate}
\item The \textbf{retriever} module retrieves top- candidate entities that might be mentioned in .
\item The \textbf{reader} module extracts mentions of each candidate entity in  (or rejects it), then returns a subset of globally reranked labeled mentions as the final prediction.
\end{enumerate}
Our approach bears superficial similarities to a standard framework in open-domain QA that pipelines retrieval and span finding \citep[\textit{inter alia}]{karpukhin2020dense},
but it has the following important differences.
First, instead of retrieving passages given a question, it retrieves questions (i.e., candidate entities) given a passage.
Second, even when considering a single question, there can be multiple answer spans (i.e., mentions) instead of one.
Both the number of gold entities present in a document and the number of mentions of each gold entity are unknown,
making this setting more challenging than standard QA in which we only need to find a single answer span for a single question on a passage.

\paragraph{Input representation.}
Both the retriever and the reader work with text representations of documents and entities, thus applicable to a zero-shot setting (e.g., linking to a new KB at test time by reading entity descriptions).
We use the title  and the description  to represent an entity .
Since a document  is generally too long to encode with a Transformer encoder which has a quadratic dependency on the input length, we break it down in  overlapping passages
 of length  with stride  (e.g.,  and ) and operate at the passage-level similarly as in QA \citep{alberti2019bert}.
When a document is long, individual passages may lose global information.
For long documents, we find it beneficial to carry a document-level topical text  across passages in that document (e.g., first sentence).
We emphasize that we do \emph{not} use any extra information outside the document.
In our experiments we simply set  (i.e., the first token in the document).

\paragraph{Notation.}
We write  to denote a Transformer encoder that maps any token sequence to the same-length sequence of corresponding contextual embeddings;
the symbol  is used to distinguish different encoders.
We assume the usual special tokens in the input popularized by BERT \citep{devlin2019bert}:  to represent the whole input and  to indicate an input boundary.
We write  to denote the text concatenation; we insert an unused token type in the vocabulary in between two texts being concatenated.
We write  to denote the -th column of matrix .

\subsection{Retriever}
\label{subsec:retriever}

Given a passage  in document  and an entity , the retriever computes

At inference time, we precompute  for each  and use Faiss \citep{johnson2019billion} for fast top- retrieval.

\paragraph{Training.}
We train the retriever by a multi-label variant of noise contrastive estimation (NCE).
Given a passage  in document , we have a set of multiple gold entities  that are mentioned in the passage and optimize the per-example objective

where  is a set of negative examples that excludes \emph{all} gold entities .
The objective effectively constructs  independent NCE instances, each of which treats a gold entity as the only
correct answer while ensuring that other gold entities are not included in negative examples.
We obtain 90\% of  by sampling entities uniformly at random from 
and 10\% by hard negative mining (i.e., using highest-scoring incorrect entities under the model),
which is well known to be beneficial in entity retrieval \citep{gillick-etal-2019-learning,wu2020scalable,zhang2021understanding}.


\subsection{Reader}
\label{subsec:reader}

Let  denote  candidate entities for a passage  in document .
For each , the reader computes a joint encoding of  by

then defines a conditional distribution over mention spans of  in  by

where  are additional parameters. The reader also multitasks reranking:
it uses  to define a conditional distribution over candidate entities by


\paragraph{Training.}
We obtain candidates   from a fully trained retrieval module to make training consistent with test time.
During training, we always include all gold entities as candidates (i.e., ).
Let  denote the set of gold mention spans of  in ;
if  is not present in , we define .
We optimize the per-example objective

where  is the indicator function equal to one if  is true and zero otherwise.
Note that the reader is trained to predict the \texttt{[CLS]} span for incorrect entities.


\subsection{Inference}
\label{subsec:inference}

At test time, we process a new document  in passages  independently as follows:
\begin{enumerate}
\item Retrieve top- highest scoring entities  under .
\item For each candidate , extract top- most likely mention spans  under 
  while discarding any span less probable than .
\item Return a subset of the surviving labeled mentions  with  as the final prediction.
\end{enumerate}
We do not apply any further processing to combine passage-level predictions other than merging duplicate labeled spans  in the overlapping sections.
This inference scheme is simple yet effective.
For each candidate entity, the reader scrutinizes the passage with deep cross-attention to see if there are any mentions of the entity
and has a chance to reject it by predicting .
The reader delays its final decision until it has processed all candidates to globally reconsider labeled mentions with ranking probabilities.
Figure~\ref{fig:entqa} shows a successful prediction on a passage from the validation portion of AIDA.

\begin{figure}[t]
  \begin{center}
    \begin{minipage}{.48\textwidth}
      \begin{center}\textbf{Passage}\\ \vspace{1mm}
        \setlength{\fboxsep}{1em}
        \noindent\fbox{
          \parbox{\textwidth}{ \large
            After bowling \dgray{[}\textcolor{green!60!black}{Somerset}\dgray{]} out for 83 on the opening morning at \dgray{[}\textcolor{cyan!80!black}{\textbf{Grace Road}}\dgray{]}, \dgray{[}\textcolor{red!80!black}{\textbf{Leicestershire}}\dgray{]} extended their first innings by 94 runs before being bowled out for 296 with \dgray{[}\textcolor{orange!80!black}{\textbf{England}}\dgray{]}
          }
        }
      \end{center}
    \end{minipage}\hspace{5mm}
    \begin{minipage}{.47\textwidth}
      \begin{center}\textbf{Top- candidate entities}\\ \vspace{1mm}
        \scalebox{0.8}{
          \adjustbox{minipage=\textwidth,margin=1em,width=\textwidth,set height=3.6cm,set depth=3.3cm,frame,left}{
            \phantom{\xmark} \hspace{1.2mm} \phantom{0}1.~\textcolor{red!80!black}{\textbf{Leicestershire County Cricket Club}} \\
            \phantom{\xmark} \hspace{1.2mm} \phantom{0}2.~\textcolor{cyan!80!black}{\textbf{Grace Road}} \\
            \phantom{\xmark} \hspace{1.2mm} \phantom{0}3.~\textcolor{green!60!black}{\textbf{Somerset County Cricket Club}} \\
            \xmark \hspace{2mm} \phantom{0}4.~Durham County Cricket Club  \\
            \xmark \hspace{2mm} \phantom{0}5.~Nottinghamshire County Cricket Club \\
            \xmark \hspace{2mm} \phantom{0}6.~Derbyshire County Cricket Club \\
            \xmark \hspace{2mm} \phantom{0}7.~Warwickshire County Cricket Club \\
            \xmark \hspace{2mm} \phantom{0}8.~Leicestershire \\
            \xmark \hspace{2mm} \phantom{0}9.~Worcestershire County Cricket Club \\
            \xmark \hspace{2mm} 10.~Yorkshire County Cricket Club \\
            \phantom{\xmark} \hspace{1.2mm} 11.~\textcolor{orange!80!black}{\textbf{England cricket team}} \\
            \xmark \hspace{2mm} 12.~Marylebone Cricket Club \\
            \xmark \hspace{2mm} 13.~Sussex County Cricket Club \\
            \xmark \hspace{2mm} 14.~Kent County Cricket Club \\
            \xmark \hspace{2mm} 15.~Leicester \\
            \xmark \hspace{2mm} 16.~Aylestone Road \\
            \xmark \hspace{2mm} 17.~County Cricket Ground, Derby \\
            \phantom{aaaaaaaaaaa} \\
          }
        }
      \end{center}
    \end{minipage}

  \end{center}
  \caption{Example prediction by EntQA taken from AIDA-A.
    Given a passage, the retriever module ranks  candidate entities,
    then the reader module finds mentions of each entity or rejects it (marked by ).
    Both modules use entity descriptions (not shown).
    In this example, it predicts the span ``England'' for the 11th candidate \texttt{England cricket team} but rejects the 35th candidate \texttt{England} (the country).
  }
  \label{fig:entqa}
\end{figure}


\section{Experiments}
\label{sec:experiments}

We evaluate EntQA on the GERBIL benchmarking platform \citep{roder2018gerbil}, which offers reliable comparison with state-of-the-art EL methods on numerous public datasets.

\subsection{Setting}

\paragraph{Datasets.}
We follow the established practice and report the InKB Micro F score on the in-domain and out-of-domain datasets used in \cite{cao2021autoregressive}.
Specifically, we use the AIDA-CoNLL dataset \citep{hoffart2011robust} as the in-domain dataset:
we train EntQA on the training portion of AIDA, use the validation portion (AIDA-A) for development, and reserve the test portion (AIDA-B) for in-domain test performance.
We use seven out-of-domain test sets: MSNBC, Derczynski (Der) \citep{derczynski2015analysis}, KORE 50 (K50) \citep{hoffart2012kore}, N3-Reuters-128 (R128),
N3-RSS-500 (R500) \citep{roder2014n3}, and OKE challenge 2015 and 2016 (OKE15 and OKE16) \citep{nuzzolese2015open}.
We refer to Table~6 in \cite{kolitsas2018end} for the datasets' statistics.
For the KB, we use the 2019 Wikipedia dump provided in the KILT benchmark \citep{petroni-etal-2021-kilt}, which contains 5.9 million entities.

\paragraph{Model details.}
We initialize the passage encoder  and the entity encoder  in the retriever module
with independent BLINK retrievers pretrained on Wikipedia hyperlinks \citep{wu2020scalable}
and optimize the NCE objective \eqref{eq:obj-retriever} with hard negative mining.
We initialize the joint encoder  in the reader module with ELECTRA-large \citep{clark2019electra} finetuned on SQuAD 2.0 \citep{rajpurkar2018know}
and optimize the reader objective \eqref{eq:obj-reader}.
We break up each document  into overlapping passages of length  with stride  under WordPiece tokenization.
For each passage in , we concatenate the input with the first token of the document , which corresponds to the topic in AIDA but not in other datasets.
We use 64 candidate entities in training for both the retriever and the reader; we use 100 candidates at test time.
We predict up to  mention spans for each candidate entity.
We use  as the threshold in all experiments, chosen after trying values 0.01, 0.1, and 0.05 on the validation set.
Additional experiments on automatically tuning  are discussed in Appendix~\ref{app:threshold}.
For optimization, we use Adam \citep{adam14} with learning rate 2e-6 for the retriever and 1e-5 for the reader;
we use a linear learning rate decay schedule with warmup proportion 0.06 over 4 epochs for both modules.
The batch size is 4 for the retriever and 2 for the reader.
The retriever is trained on 4 GPUs (A100) for 9 hours; the reader is trained on 2 GPUs for 6 hours.

\paragraph{Baselines.}
We compare with state-of-the-art EL systems that represent a diverse array of approaches.
\cite{hoffart2011robust} and \cite{van2020rel} use the MDED pipeline; despite the limitation of pipelining MD with ED,
the latter achieve excellent performance by solving MD with a strong NER system \citep{akbik2018contextual}.
\cite{kolitsas2018end} use an end-to-end model that sequentially performs MD and ED; to make the problem tractable,
they drastically prune the search space with a mention-candidates dictionary and the model score.
\cite{cao2021autoregressive} propose GENRE, a sequence-to-sequence model for EL.
The model conditions on the given document and autoregressively generates a labeled version of the document by at each position
either copying a token, starting or ending a mention span, or, if the previous generation was the end of a mention ,
generating the entity title associated with  token by token.
At inference time, GENRE critically relies on a prefix tree (aka. trie) derived from Wikipedia to constrain
the beam search so that it produces a valid entity title in the KB.
Since each beam element must first predict a mention before predicting an entity, unless the beam size is unbounded so that every labeled span is considered,
GENRE will suffer from MD errors propagating to ED.

\begin{table}[t]
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{InKB Micro F on the in-domain and out-of-domain test sets on the GERBIL benchmarking platform.
    For each dataset, {\bf bold} indicates the best model and \ul{underline} indicates the second best.
  }
  \label{tab:main}
  \small
  \begin{center}
    \begin{tabular}{lcccccccc|c}
      \Xhline{2\arrayrulewidth}
      \rule{0pt}{1.1em}& In-domain & \multicolumn{7}{c}{Out-of-domain}  & \\
           \rule{0pt}{1.1em}{\bf Method} & {\bf AIDA} & {\bf MSNBC} & {\bf Der} & {\bf K50} & {\bf R128} & {\bf R500} & {\bf OKE15} & {\bf OKE16} & {\bf Avg} \\
           \hline\-1em]
      \hline  \-1em]
      \Xhline{2\arrayrulewidth}
    \end{tabular}
  \end{center}
\end{table}


\subsection{Results}
\label{subsec:results}


Table~\ref{tab:main} shows the main results.
EntQA achieves the best in-domain test F score for AIDA (+2.1) and is also performant on out-of-domain datasets
(+3.8 on KORE 50 and +7.4 on N3-Reuters-128, close second-best on Derczynski and N3-RSS-500).
The performance is lower on OKE15 and OKE16 for the same reason pointed out by \cite{cao2021autoregressive}:
these datasets are annotated with coreference (i.e., they contain pronouns and common nouns linked to entities)
which our model is not trained for, while many other systems have a component in their pipelines to handle these cases.
We hypothesize that the performance on MSNBC is lagging because it has long documents (544 words per document on average)
which are processed in relatively short passages under EntQA due to our computational constraints.
Overall, EntQA achieves the best macro-averaged F score across the 8 evaluation datasets (+2.3).

The inference runtime of EntQA is clearly linear in the number of candidate entities .
To get a sense of speed, we compared the runtime of EntQA with that of GENRE on the AIDA validation set using 1 GPU on the same machine.
GENRE took 1 hour and 10 minutes, excluding 31 minutes to first build a prefix tree.
EntQA took 20 minutes with , 10 minutes with , and 4 minutes with , excluding 1 hour to first index entity embeddings, yielding F scores , , and .
Interestingly, we can obtain a significant speedup at a minor cost in performance by decreasing .
We believe this can be a useful feature of the model in controlling the speed-performance tradeoff.


We note that there is an issue of using different editions of Wikipedia between the systems.
For instance, \cite{hoffart2011robust} use the 2010 dump, \cite{van2020rel} and we use the 2019 dump,
whereas \cite{kolitsas2018end} and \cite{cao2021autoregressive} use the 2014 dump (even though the latter use the 2019 dump for pretraining).
Thus there is a concern that differences in performance are due to different snapshots of Wikipedia.
While we consider it out of scope in our work to fully address this concern, we find that using different editions of Wikipedia does not fundamentally change
the performance of EntQA, which is consistent with GERBIL's intent of being KB-agnostic.
For instance, we obtained the same validation F on AIDA with our model trained on either the 2014 or 2019 dump.
We use the KILT edition of Wikipedia mainly for convenience.




\subsubsection{Other Practical Highlights}

\paragraph{No dictionary.}
EntQA has no dependence on a mention-candidates dictionary.
All previous works rely on a dictionary 
that maps a mention string  to a small set of candidate entities  associated with empirical
conditional probabilities  \citep[\textit{inter alia}]{hoffart2011robust}.
For instance, it is an essential component of the search procedure in the end-to-end model of \cite{kolitsas2018end}.
While not mentioned in the paper or on the GitHub repository, GENRE \citep{cao2021autoregressive} also uses the dictionary from \cite{kolitsas2018end}
in their prefix tree to constrain the beam search (personal communication with one of the authors of the paper).
Constructing such a dictionary typically assumes the existence of a large KB-specific labeled corpus (e.g., internal links in Wikipedia).
EntQA is thus more broadly applicable to KBs without such resources (e.g., for small domain-specific KBs).

\paragraph{No model-specific pretraining.}
EntQA does not require model-specific pretraining; it only uses standard pretrained Transformers for initialization and is directly finetuned on AIDA.
This is in contrast with GENRE which requires industry-scale pretraining by weak supervision.
Specifically, GENRE is trained by finetuning BART \citep{lewis2020bart} on autoregressive EL training examples constructed from all Wikipedia abstract sections on 64 GPUs for 30 hours,
followed by finetuning on AIDA.
Thus training GENRE from scratch is beyond the means of most academic researchers, making it difficult to make substantial changes to the model.
EntQA can be trained with academic resources and outperforms GENRE.


\subsection{Ablation Studies}

The final form of EntQA in Section~\ref{subsec:results} is the result of empirically exploring various modeling and optimization choices during development.
We present an ablation study to illustrate the impact of these choices.

\paragraph{Retriever}
Table~\ref{tab:retriever} shows an ablation study for the retriever module. We report top-100 recall (R@100) on the validation set of AIDA.
The baseline retriever is initialized with BLINK \citep{wu2020scalable},
uses the passage representation , and is trained by optimizing the multi-label variant of NCE \eqref{eq:obj-retriever}
that considers one gold entity at a time by excluding others in the normalization term.
We see that the baseline retriever has an extremely high recall (98.2),
confirming our hypothesis that it is possible to accurately infer relevant entities in a passage without knowing where they are mentioned.
We also see that it is very important to use the proposed multi-label variant of NCE instead of naive NCE that normalizes over all gold entities, which results in a massive decrease in recall (82.7).
We consider optimizing the marginal log-likelihood (i.e., the log of the sum of the probabilities of gold entities, rather than the sum of the log),
but it yields much worse performance (83.8).
It is helpful to initialize with BLINK rather than BERT-large, use hard negatives in NCE, and append  to input passages.
Table~\ref{tab:retriever} additionally shows the BM25 recall, which is quite poor (36.6).
Upon inspection, we find that BM25 fails to retrieve diverse entities.
For instance, a passage on cricket may have diverse gold entities such as an organization (\texttt{Leicestershire County Cricket Club}), location (\texttt{London}),
and person (\texttt{Phil Simmons}), but the top entities under BM25 are dominated by person entities (\texttt{Alan Shipman}, \texttt{Dominique Lewis}, etc.).
This shows the necessity of explicitly training a retriever to prioritize diversity in our problem.



\begin{table}[t]
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{Ablation study for the retriever module. Each line makes a single change from the baseline retriever used in Table~\ref{tab:main}.
    We also compare with BM25.}
  \label{tab:retriever}
  \small
  \begin{center}
    \begin{tabular}{lc}
      \Xhline{2\arrayrulewidth}
      Retriever & Val R@100 \\
      \hline
      Baseline & 98.2 \\
      -- Omit excluding other gold entities in the normalization term of NCE &  82.7 \\
      -- Train by optimizing the marginal log-likelihood  &  83.8 \\
      -- Initialize with BERT-large & 94.4 \\
      -- Omit hard negatives in NCE (i.e., negative examples are all random) & 94.4 \\
      -- Omit the document-level information  in the passage representation  &  96.6 \\
      \hline
      BM25 & 36.6 \\
      \Xhline{2\arrayrulewidth}
    \end{tabular}
  \end{center}
\end{table}


\begin{table}[t]
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{Ablation study for the reader module. Each line makes a single change from the baseline reader used in Table~\ref{tab:main}.
    Candidate entities are obtained from the baseline retriever in Table~\ref{tab:retriever} (except the oracle experiment).}
  \label{tab:reader}
  \small
  \begin{center}
    \begin{tabular}{lc}
      \Xhline{2\arrayrulewidth}
      Reader & Val F  \\
      \hline
      Baseline & 87.3 \\
      -- Initialize with BERT-large & 85.6 \\
      -- Train by optimizing the marginal log-likelihood  &  86.9 \\
      -- Initialize with ELECTRA-large (not finetuned on SQuAD 2.0) & 88.4 \\
      -- Omit the reranking probabilities  (i.e., only use span probabilities) &  87.9 \\
      -- Omit the document-level information  in the input passage representation  &  87.5 \\
      \hline  \-1em]
      \Xhline{2\arrayrulewidth}
    \end{tabular}
  \end{center}
\end{table}



\appendix
\section{Threshold Optimization}
\label{app:threshold}

To see if it is possible to improve over the static threshold value , we tried automatically calibrating  based on the AIDA validation performance
by considering every effective threshold obtained from a sorted list of probabilities of labeled mentions.
The best threshold was . The validation F1 score improved from 87.32 to 87.75,
and the GERBIL test score improved from 60.46 to 60.55.
Thus threshold optimization can yield a minor improvement, but overall we find that EntQA is robust to choices of threshold in a reasonable range.

\section{Document-Level Information}
\label{app:doc-level}

We explored various ways of injecting document-level information in paragraphs.
We tried the first token, the first sentence, and a continuous topic embedding (obtained by averaging all token embeddings in the document).
We settled on the first-token version because it gave the best performance.
For many of the GERBIL datasets, however, we obtain almost the same performance with or without the topic information.
As it is somewhat dataset-specific (e.g., the first word in AIDA is always the topic word), we leave it as an option in our model for the user to decide.
Table~\ref{tab:no-topic} shows the GERBIL performance without any topic information vs with the first token in the document.




\end{document}
