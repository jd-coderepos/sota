\section{Experiment}\label{sec:Experiment}
The implementation details are described in supplementary materials. In this section, we demonstrate our 
approach by evaluating our results (Section ~\ref{sec:our_result}) 
and analyzing our methods (Section ~\ref{sec:discuss}).
























\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
\caption{
	Musical style classification comparision on testing data (Accuracy).
}
\label{table:music_attribute}
\begin{tabular}{l|c}
\hline
Method & Accuracy (\%)  \\
\hline
Convolution RNN\cite{choi2017convolutional} & 89.6 \\
Baseline (FC + Bi-LSTM) & 88.0   \\
\textbf{Ours (Temproal Conv + Bi-LSTM)} & \textbf{92.1} \\
\hline	
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}





\begin{figure*}[t]
    \centering
    \subfigure[Example of generated modern dance.]
    {\includegraphics[width=12cm, angle=0]{fig/example2_nolstm.pdf}}
    \subfigure[Example of generated curtilage dance.]
    {\includegraphics[width=12cm, angle=0]{fig/example1_nolstm.pdf}}
    \caption{\textbf{Dance motion generated by our method}. 
    We rendered the generated dance motion with meshes and textures. Our method can obtain 
    realistic, diverse and music-consistent dance motion, and the dance motion includes finger motion (blue arrow). 
    See the video in supplementary materials.}
    \label{fig:qualitative_result}
    \vspace{-2mm}
\end{figure*}























\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
\caption{
\textbf{Comparison of realism}(FID, lower is better), \textbf{diversity}(higher is better), \textbf{rhythm-consistent}(rhythm hit rate, higher is better). 
}
\label{table:comparison_result}
\begin{tabular}{l c c c| c c c}
\toprule
  \multirow{2}{*}{Method} & \multicolumn{3}{c}{Morden Dance} &  \multicolumn{3}{c}{Curtilage Dance}\\ 
   \cmidrule(lr){2-4}\cmidrule(lr){5-7}
   & FID & Diversity &   Rhythm Hit & FID & Diversity &   Rhythm Hit \\
Real Dances & 6.2 & 56.1 & 61.3\% & 5.3 & 48.7 & 70.6\% \\
\midrule
LSTM-autoencoder\cite{tang2018dance} & 82.1 & 18.3 & 11.2\% & 76.4 & 13.8 & 12.9\%\\
Temporal Conv\cite{ginosar2019learning} & 36.7 & 33.4 & 39.8\% & 33.2 & 37.6 & 50.3\%\\
LSTM\cite{lee2018interactive} & 27.8 & 46.5 & 50.8\% & 26.3 & 40.2 & 58.9\%\\
\midrule
Ours(no GMM loss) & 32.3 & 43.1 & 42.8\% & 22.4 & 38.6 & 52.7\%\\
Ours & \textbf{12.5} & \textbf{52.5} & \textbf{58.7\%} & \textbf{8.7} & \textbf{46.9} & \textbf{69.2}\%\\
\bottomrule	
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}


\begin{figure}[t] 
  \begin{minipage}[t]{0.45\textwidth} 
  \centering 
    \includegraphics[width=6cm]{fig/user_study1.pdf} 
    \figcaption{\textbf{Result of user study}. This figure shows the scores given by 25 users, including the means and standrand deviations for dance motion generated by SOTA LSTM\cite{lee2018interactive}(GMM loss) and our method. Our generated dance motions are better than the LSTM.}
    \label{fig:user_study} 
  \end{minipage}\begin{minipage}{0.03\textwidth}
  \
  \end{minipage}
  \begin{minipage}[t]{0.52\textwidth} 
  \centering 
    \includegraphics[width=6.5cm]{fig/pca_musicmap_2d.pdf} 
    \figcaption{\textbf{The comparison of with/without musical context-aware encoder}. PCA is used to reduce the dimension of end-effector position features, so that we can compare the diversity of generated dance motion. Higher diversity can be obtained when using musical context-aware encoder. }
    \label{fig:pca_musicmap} 
  \end{minipage}\end{figure}


\subsection{Result}\label{sec:our_result}
Style-consistency mainly depends on the musical style classification, so we need to make a simple evaluation of our musical style classification model, and then evaluate the performance of the generated dance motion.

\textbf{Musical style classification.}
To verify the effectiveness of our approach, we compared with the Convolution RNN\cite{choi2017convolutional}, and built an FC+Bi-LSTM model as the baseline. The baseline
replaces the temporal convolution layer in our musical style classifier with fully connected layer. The training data 
and strategies are exactly the same as our model.
We use the prediction accuracy to evaluate the final results and our method can obtain excellent results than the convolution RNN and baseline, as shown in Table ~\ref{table:music_attribute}.

\textbf{Evaluation of dance performance.}
We compare against several current SOTA motion modeling methods. 
LSTM-autoencoder\cite{tang2018dance} generates 3D dance motion directly from music. Temporal Convolution(Temporal Conv)\cite{ginosar2019learning} models body gestures from speech. LSTM\cite{lee2018interactive} is an autoregressive model, and the generated motion is controlled by control signals. We found that LSTM was difficult to model the dance using MSE loss, so replaced with GMM loss.
20 dance motions(10 modern dance motions and 10 curtilage dance motions) are generated by our model and the above methods, respectively.
We compare them by the realism, diversity, style-consistency, rhythm-consistency, melody-consistency. 

\textbf{1)Realism and style-consistency.}
We evaluate the dance motion realism and style-consistency by Fr\'echet Inception Distance(FID)\cite{heusel2017gans}, similar to \cite{yan2019convolutional}. Because FID requires an action classifier to extract dance features, we train an action classifier based on temporal convolution and Bi-LSTM on our dataset as the feature extractor.
The realism is reflected in that the generated dance needs to be close to the real dance, and the style-consistency is reflected in the classifier is classified according to the dance type. As shown in Table~\ref{table:comparison_result}, the FID of our generated dance motions is lower, which means our results is closer to the real dances and more style-consistent.
\textbf{2)Diversity.} We evaluate the dance motion diversity by 
the average feature distance among different dance motions, similar to \cite{lee2019dancing}. Our method can generate more diverse dance motions, as shown in Table~\ref{table:comparison_result}. 
\textbf{3)Rhythm-consistency.} In Sec~\ref{sec:music_feature_extraction}, we analyze rhythm-consistency, so we use the rhythm hit rate as the evaluation method of rhythm-consistency(Rhythm hit needs to meet the error within 0.25s), similar to \cite{lee2019dancing}. The comparison shows our method can obtain higher rhythm hit rate in Table~\ref{table:comparison_result}.

Our method is superior to other methods by the above quantitative evaluation. Compared against the LSTM-autoencoder\cite{tang2018dance}, generates 3D dance motion directly from music, our method is completely superior to it. More importantly, we can generate different dance types with the same model, but they can not. Our DanceNet is based on temporal convolution, and we combine dilated temporal convolution with the gated activation unit to obtain better performance than general temporal convolution\cite{ginosar2019learning}. We compared against the autoregressive model, LSTM(GMM loss)\cite{lee2018interactive}, and our results are better than theirs. Our explanation is that the autoregressive model based on dilated temporal convolution is better and more robust than LSTM-based for modeling dance  motion.





\textbf{User study.} Because some indicators are difficult to quantify, e.g., melody-consistency, and each of the above evaluation methods is based on one characteristic, it is difficult to fully evaluate result. Therefore, we use user study to comprehensively evaluate the generated dance motions. Evaluating all results would take a lot of time for the users, so we only evaluate our results with LSTM\cite{lee2018interactive}(better than other methods).
We asked 25 users to score these dance motions. The score basis consists of the realism, diversity, music-consistency. More scoring indicators are described in supplementary materials. 
We report the mean scores and standrand deviations for the dance motions generated by
our model and the LSTM, as shown in Figure ~\ref{fig:user_study}. The result of user study shows 
that our generated dance motions are obviously superior to the LSTM.
Our mean score reaches 8.452 (modern dance), 8.196 (curtilage dance), and the standrand deviations
are significantly smaller than the LSTM, especially modern dance.
In our motion data, modern dance is more diverse than curtilage dance, which is a very important reason that the modern dance generated by the LSTM is worse than curtilage dance(lower mean score, larger standard deviation). Our approach can generate diverse modern dance, and the score is slightly higher than curtilage dance. It means that our method is more robust to diverse(complex) dance motion. In addition, we rendered the generated dance motions with meshes and textures. We show two examples in Figure ~\ref{fig:qualitative_result}, one for modern dance and another for curtilage dance. Both examples show that our method can obtain 
realism, diverse and music-consistent dance motion.












\subsection{Discussion}\label{sec:discuss}
To demonstrate our method, we need to discuss every part of our method, including: mel spectrum v.s. music features(onset,beat,chroma), musical context-aware encoder, with/without end-effector position, MSE loss v.s. GMM loss. We perform qualitative evaluation in this section, and more quantitative results are shown in the supplementary materials.



\begin{figure}[t] 
  \begin{minipage}[t]{0.49\textwidth} 
  \centering 
  \subfigure[left hand]
  {\includegraphics[width=2.9cm, angle=0]{fig/lhand_height1.pdf}}
  \subfigure[right hand]
    {\includegraphics[width=2.9cm, angle=0]{fig/rhand_height1.pdf}}
  \figcaption{\textbf{The comparison between mel spectrum and music features}. The generated dance has a richer variation of left/right hand motion with music features as input, especially right hand motion.}
 \label{fig:mel_music_hand_height}
  \end{minipage}\begin{minipage}{0.02\textwidth}
  \
  \end{minipage}
\begin{minipage}[t]{0.49\textwidth} 
    \centering
    \subfigure[left toe\_end]
    {\includegraphics[width=2.9cm, angle=0]{fig/lfoot_height1.pdf}}
    \subfigure[right toe\_end]
    {\includegraphics[width=2.9cm, angle=0]{fig/rfoot_height1.pdf}}
    \figcaption{\textbf{The comparison of whether add end-effector position to the 
    motion feature}. Left/right 
    toe\_end motion generated by the model with end-effector position has more variety.
}
    \label{fig:end_position_foot_height}
  \end{minipage}\end{figure}





\textbf{Mel spectrum v.s. Music features.}
We trained the model with mel spectrum and music features respectively, and tested on 
the same music clip. When we use the mel spectrum as input,
the generated dance motion is very stiff and appears jitter problem
(we smooth the motion via Gaussian filter with a large kernel size ). 
We compared the generated motion generated by the same music clip 
and plotted the height of the left/right hand over time (480 frames), as shown in 
Figure ~\ref{fig:mel_music_hand_height}. When the music features are used as input, 
the generated dance is more diverse and music-consistent.
It means that the music features in our method have higher generalization ability. 




\textbf{Musical context-aware encoder.}
The musical context-aware encoder is a very important part. We tried to directly take the music features as 
control signals without the musical context-aware encoder, 
and adopted the same training data and strategies to train the model. 
We found that the realism and diversity of the dance motion generated by the model is poor (only simple dance steps or standing still).
In order to better compare the results, 
we extracted the end-effector position (important motion feature) of the generated motion and 
used PCA to reduce the dimension
to visualize the results,  as shown in Figure ~\ref{fig:pca_musicmap}.
It is obvious that we can get more realistic and diverse motion when we use musical context-aware encoder. One important reason is that music and dance are two modalities and the music features should be encoded. Another explanation is that the dance motion is smooth, the input control signal should be smooth, and the jitter control signals(music features) reduce the realism. Obviously, we visualized the music-context-code and found that it is smooth.






\textbf{With/Without end-effector position.}
In Section ~\ref{sec:motion_representation}, we explain why the end-effector position feature is added to the motion feature.
 In order to verify its advantages, we trained a model without the end-effector position 
 feature. 
We compared the dance motion generated by the same music clip and plotted the height of 
 the toe\_end position over time, as shown in Figure ~\ref{fig:end_position_foot_height}.
It shows that adding the end-effector position can get more diverse dance motions. Eliminating the accumulation errors from root to end-effector can predict more accurate dance motion, thereby increasing the diversity.


\textbf{MSE loss v.s. GMM loss.} 
In the existing motion modeling methods\cite{li2017auto}\cite{lee2018interactive}, MSE loss is usually used. In Sec~\ref{sec:our_result}, we mentioned it is difficult to train the autoregressive model LSTM on our dataset using MSE loss. Similarly, we used MSE loss to train our DanceNet and found that the modeling ability was poor(Table~\ref{table:comparison_result}). Dance is a long sequence of motion. The MSE loss would cause the model to predict a certain motion frame, which could cause error accumulation. In addition, dance is more diverse than simple locomotion, and MSE loss reduces the diversity. The GMM loss allows DanceNet to model a probability distribution that can cover more fileds in the motion graph. We can sample around the predicted mean motion( in Sec~\ref{sec:gmm_loss}) to increase diversity in the generation phase.