\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/benchmark.pdf}
	\caption{\textbf{Benchmarking \muld}. Our proposed approach significantly improves upon ERM (left) as well as sophisticated ensemble construction methods (right) in ZS-MDG, wherein we obtain larger performance gains as the domain discrepancy becomes more severe.}
	\label{fig:benchmark}
\end{figure*} 
\subsection{Dataset Description}
We evaluate \muld~using six standard visual MDG benchmarks (i) PACS~\citep{PACS} dataset comprising  domains, namely photos, art, cartoon and sketches, with images belonging to  different classes; (ii) VLCS~\citep{VLCS} dataset, which is also comprised of  domains corresponding to the four benchmark image datasets (Caltech101, LabelMe, SUN09 and VOC2007) and contains images from  classes; (iii) OfficeHome~\citep{officehome} dataset containing images from  classes, where the images represent  different domains, namely art, clipart, product and real respectively; (iv) Terra Incognita~\citep{terra} comprised of camera trap images of wild animals obtained from four different camera angles (\textit{i.e.}, domains) and  different wildlife categories; and (v) Camelyon-WILDS~\citep{bandi2018detection, koh2021wilds} consisting roughly 400k images of potentially cancer cells taken at different hospitals and scanners.  
\subsection{Experimental Setup}
Following standard practice in ZS-MDG, for every dataset except for Camelyon-WILDS, we run experiments by leaving out one of  domains for testing while using the  domains for training.
For Camelyon-WILDS~\citep{bandi2018detection, koh2021wilds}, we use the standard protocol of using data from first three hospitals as training domains and use data from fourth and fifth hospitals as validation and testing domains. To enable a fair comparison with the state-of-the-art, we use ResNet-50~\citep{he2016deep}, pre-trained on ImageNet~\citep{ILSVRC15} as the backbone feature extractor for all experiments.

For \regmethod, we use the following settings: (i) number of training iterations  is set to  and  to ; (ii) number of groups is fixed at ; (iii)  are set to ,  and  respectively; (iv) batch size of  per domain; and (v) Adam optimizer~\citep{ADAM} with learning rate .

For \muld, we use a random 80-20 split from each of the source domains to obtain the train and validation sets, while the train set itself is further subdivided (80-20) to construct meta-train and meta-validation data. We report the mean and standard deviation of performance, obtained across three trials with different random seeds, for each experiment similar to~\citep{gulrajani2020search}. 
Across all experiments we use the following hyper-parameters: (i) batch size of  per domain; (ii) Adam optimizer~\citep{ADAM} with learning rate of  (for both  and ); (iii) number of training iterations set to  and (iv) ensemble size  is set to . We implement \muld~into the publicly available DomainBed framework.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/behavior.pdf}
	\caption{\textbf{Analysis of \muld~design}. (left) When compared to approaches that utilize advanced data augmentation strategies to improve generalization, \muld~eliminates the need for tailoring the augmentation strategy and is consistently effective for all benchmarks. In contrast, even sophisticated approaches such as RandConv (RC) or its combination with RandAug (RA)  provide varying degrees of improvements over ERM (RA) across different datasets; (b) When compared against different design choices for the MRS function , we find that the proposed gradient-matching performs the best.}
	\label{fig:behavior}
\end{figure*}
In \muld, the training mini-batches are augmented using a composition of the following augmentation choices: random horizontal flip, random color jitter and grayscaling with  probability, which we refer to as RandAug (RA). As described in Section~\ref{sec:meta-test}, we also create additional meta-validation batches by augmenting each batch  using subsets of augmentations used during training. We set  in eq. \eqref{eq:outer loop upate} to  and study the sensitivity of this hyper-parameter as part of our ablation study. We report results for both the proposed model selection strategies and our rigorous empirical study shows that the \textit{Overall Avg.} strategy provides a small margin of improvement over \textit{Overall Ens}. 
\begin{table*}[h]
\centering
\caption{\textbf{Summary performance of popular ZS-MDG baselines obtained using GroupDRO++ and \muld.} While the proposed GroupDRO++ improves over ERM and vanilla GroupDRO methods, \muld~with Overall (Avg.) model selection consistently achieves the best generalization performance.
}
\label{tab:datasets}
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\textwidth}{!}{ 
\begin{tabular}{|c||c||c||c||c|}

\rowcolor[HTML]{404040}

\textcolor{white}{\textbf{Methods}} & \textcolor{white}{\textbf{PACS}} & \textcolor{white}{\textbf{VLCS}} & \textcolor{white}{\textbf{OfficeHome}} & \textcolor{white}{\textbf{TerraIncognita}} \\ 
\hhline{=::=::=::=::=:}

ERM (RA)         & 85.5  0.2     & 77.5  0.4     & 66.5  0.3            & 46.1  1.8\\ \hhline{=::=::=::=::=:}
 
IRM~\citep{arjovsky2019invariant}                & 83.5  0.8     & 78.5  0.5     & 64.3  2.2             & 47.6  0.8 \\        \hhline{=::=::=::=::=:} 
MLDG~\citep{mldg}              & 84.9  1.0     & 77.2  0.4     & 66.8  0.6             & \cellcolor[HTML]{FFDD86}47.7  0.9            \\ \hhline{=::=::=::=::=:}
ARM~\citep{ARM}        & 85.1  0.4      & 77.6  0.3      & 64.8  0.3             & 45.5  0.3               \\ \hhline{=::=::=::=::=:}
RSC~\citep{RSC}              & 85.2  0.9      & 77.1  0.5      & 65.5  0.9             & 46.6  1.0          \\ \hhline{=::=::=::=::=:}
 
CORAL~\citep{CORAL}           & 86.2  0.3     & 78.8  0.6     & \cellcolor[HTML]{FFDD86}68.7  0.3        & 47.6  1.0            \\ \hhline{=::=::=::=::=:}

GroupDRO~\citep{GroupDRO} & 84.4  0.8   & 76.7  0.6      & 66.0  0.7             & 43.2  1.1              \\ 
\hhline{=::=::=::=::=:}
\hline\hline
\regmethod (ours) &  \cellcolor[HTML]{FFDD86}86.66  0.4 & \cellcolor[HTML]{b1e9b0}\textbf{79.81  0.5}& 67.10.3 & 47.45  0.3 \\\hhline{=::=::=::=::=:}

 \muld~(Avg.) (ours) & \cellcolor[HTML]{b1e9b0} \textbf{87.35  0.2} & \cellcolor[HTML]{FFDD86} 79.02  0.3 & \cellcolor[HTML]{b1e9b0}\textbf{69.76  0.2} & \cellcolor[HTML]{b1e9b0}\textbf{48.66  0.2} \\
 \hline 
\end{tabular}
}
\end{table*}



\begin{table}[t]
\centering
\caption{\textbf{Performance of \muld~on the challenging WILDS-Camelyon17 benchmark}. In addition to being significantly superior to ERM (RA), \muld~outperforms best-performing methods such as SagNet~\citep{SagNet} and CORAL~\citep{CORAL}.}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.5\textwidth}{!}{ 
\begin{tabular}{|c|c|}
\hline
\rowcolor[HTML]{404040}
\textcolor{white}{Method} & \textcolor{white}{Accuracy} \\ \hline \hline
ERM                     & 82.31          \\ 
ERM (RA)                     & 85.21                      \\
CORAL~\citep{CORAL} & 92.7\\
SagNet~\citep{SagNet} &  \cellcolor[HTML]{FFDD86}92.9\\
\hline \hline
GroupDRO++ (ours) & 86.7 \\
\muld~(Avg.) (ours)                   & \cellcolor[HTML]{b1e9b0} 94.6                           \\ \hline
\end{tabular}
}
\label{tab:wilds}
\end{table}


\subsection{Key Findings}
We now present a summary of the key findings from our empirical studies. We first find that re-labeling indeed leads to significant improvements over ERM thus providing evidence to our core hypothesis and sheds new light on the competitive behavior of ERM~\citep{gulrajani2020search}. From the results, we notice that \gro, despite leveraging the domain labels, often performs poorly compared to the vanilla ERM. In contrast, \regmethod improves upon both these baselines. Finally, we how that \muld~outperforms other existing ensembling strategies and produces state-of-the-art domain generalization performance.
\begin{table*}[h]
\centering
\caption{\textbf{Re-labeling domains improves generalization}. Here, for each dataset, we show the detailed generalization results for each of the domains using models trained with the remaining three domains. Both \muld~and \regmethod, which re-label samples from the different source domains and perform multi-domain training, lead to significant performance gains over ERM as well as the standard GroupDRO implementation.}
\label{tab:groupdro}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.8\textwidth}{!}{ 
\begin{tabular}{|cccccc|}
\hline 
\rowcolor{gray!20}
\multicolumn{6}{|l|}{\textbf{Dataset: PACS}} \\
\hline 
\rowcolor[HTML]{404040}
\textcolor{white}{ Method} & \textcolor{white}{\textbf{A}} & \textcolor{white}{\textbf{C}} & \textcolor{white}{\textbf{P}} & \textcolor{white}{\textbf{S} }& \textcolor{white}{\textbf{Average}} \\
\hline 
ERM &        &   &        &        &     \\ 
 \gro &        &   &        &        &  \\
 \regmethod (ours) & \cellcolor[HTML]{FFDD86}  & \cellcolor[HTML]{b1e9b0}  &  \cellcolor[HTML]{b1e9b0} & \cellcolor[HTML]{FFDD86} &  \\
 \muld~(Avg.) (ours)& \cellcolor[HTML]{b1e9b0}   &\cellcolor[HTML]{FFDD86}   &   & \cellcolor[HTML]{b1e9b0} & \cellcolor[HTML]{b1e9b0} \\
 
\hline \hline
\rowcolor{gray!20}
\multicolumn{6}{|l|}{\textbf{Dataset: VLCS}} \\
\hline 
\rowcolor[HTML]{404040}

\textcolor{white}{ Method} & \textcolor{white}{\textbf{C}} & \textcolor{white}{\textbf{L}} & \textcolor{white}{\textbf{S}} & \textcolor{white}{\textbf{V}} & \textcolor{white}{\textbf{Average}} \\
\hline 
ERM &     &   & \cellcolor[HTML]{FFDD86}  &   &        \\ 
 \gro &  &  & &  &   \\
 \regmethod (ours) & \cellcolor[HTML]{FFDD86}   &   &  & &    \\
 \muld~(Avg.) (ours)&  &  \cellcolor[HTML]{FFDD86} & &  &\cellcolor[HTML]{FFDD86}\\

\hline \hline
\rowcolor{gray!20}
\multicolumn{6}{|l|}{\textbf{Dataset: OfficeHome}} \\
\hline 
\rowcolor[HTML]{404040}
\textcolor{white}{Method} & \textcolor{white}{\textbf{A}} & \textcolor{white}{\textbf{C}} & \textcolor{white}{\textbf{P}} & \textcolor{white}{\textbf{R}} & \textcolor{white}{\textbf{Average}} \\
\hline 
ERM &  \cellcolor[HTML]{FFDD86}   &   &   & \cellcolor[HTML]{FFDD86}  &     \\ 
 \gro &  &  &  &   &  \\
 \regmethod (ours) &  & \cellcolor[HTML]{FFDD86}  & \cellcolor[HTML]{FFDD86} &  & \cellcolor[HTML]{FFDD86}   \\
 \muld~(Avg.) (ours)&  && & & \\
\hline \hline
\rowcolor{gray!20}
\multicolumn{6}{|l|}{\textbf{Dataset: Terra Incognita}} \\
\hline 
\rowcolor[HTML]{404040}
 \textcolor{white}{Method} & \textcolor{white}{\textbf{L100}} & \textcolor{white}{\textbf{L38}} & \textcolor{white}{\textbf{L43}} &\textcolor{white}{ \textbf{L46}} & \textcolor{white}{\textbf{Average}}
 \\
\hline 


 ERM &  &  & \cellcolor[HTML]{FFDD86} & &     \\ 
 \gro &  &  &  & &  \\
 \regmethod (ours) & \cellcolor[HTML]{FFDD86} &  &  & \cellcolor[HTML]{FFDD86}  & \cellcolor[HTML]{FFDD86}\\
  \muld~(Avg.) (ours) & & \cellcolor[HTML]{FFDD86} &  &  &  \\
\hline 

 \end{tabular}}
\end{table*}

\vspace{0.05in}
\noindent\textbf{Finding 1: \regmethod and \muld~provide significant gains over ERM}.

\noindent As can be seen from Figure~\ref{fig:benchmark} (left), Tables~\ref{tab:datasets}, \ref{tab:groupdro} and \ref{tab:wilds}, using custom domain groups is highly beneficial. Overall, across the benchmarks, both GroupDRO++ and \muld~improve over ERM by significant margins, in terms of average generalization performance, regardless of the level of cross-domain gap inherent to each of the datasets. Furthermore, it is clearly apparent that GroupDRO++ is consistently better than the vanilla GroupDRO implementation. Through the use of a deep ensemble backbone, \muld~produces the best performance among the two proposed methods. For example, datasets such as TerraIncognita, and Camelyon are known to contain much higher cross-domain discrepancies compared to VLCS, thus making zero-shot generalization more challenging. However, we find that \muld~with  achieves large performance gains over ERM and importantly, the gap with ERM widens as the severity of domain shift increases. In the challenging Camelyon-WILDS dataset, \muld~provides  gains over ERM. In order to gain insights into the behavior of \muld, in Figure \ref{fig:groups}, we visualize examples images assigned to the three models () in the ensemble. For this illustration, we trained \muld~using the art (A), cartoon (C) and photo (P) domains from the PACS benchmark. This clearly shows that images from the same input domain can be assigned to different groups, thus enabling improved generalization. Interestingly, we observe that each model evolves to specialize for different styles (in terms of image statistics) and semantic concepts (e.g., images corresponding to the \textit{person} class are strongly representative of group 3). 




\vspace{0.05in}
\noindent\textbf{Finding 2: \muld~outperforms existing ensembling methods with the same complexity}.

\noindent We perform a comparative analysis of \muld~to a state-of-the-art multi-domain ensembling method, DMG~\citep{DMG}, which infers individual masks over the neurons for each of the source domains. Note that both DMG and \muld~are implemented using the DomainBed framework, and hence use the same experiment protocol, \textit{i.e.}, architecture, augmentations etc.
In Figure~\ref{fig:benchmark}(right), the superiority of \muld~over DMG is clearly evident across all benchmarks, with improvements as high as  on the Office-Home benchmark. Through our ERM style update in the meta-train stage and by taking into account the relevance of a member \wrt~to a domain during the meta-test stage, we effectively re-label the samples into optimal domain groups. Detailed results across datasets are reported in Table~\ref{tab:datasets}.


\vspace{0.05in}
\noindent\textbf{Finding 3: \muld~provides non-trivial improvements over SoTA DG methods}. 

\noindent Despite the effectiveness of ERM as a baseline, its inability to leverage domain discrepancies implies that there is a non-trivial performance gap between ERM and state-of-the-art MDG methods. Using rigorous comparisons with benchmarks created by~\citep{gulrajani2020search}, and compiled in Table~\ref{tab:datasets}, we find that \muld~is highly competitive with the SoTA methods, which rely on a variety of strategies to leverage cross-domain discrepancies. Interestingly, our approach produces state-of-the-art results on PACS (+), VLCS(+), Office-Home (+) and matches the performance of the state-of-the-art SagNet on the TerraIncognita dataset. Similarly, as showed in Table~\ref{tab:wilds}, on Camelyon, \muld~achieves an accuracy of , outperforming best-performing approaches such as SagNet and CORAL by a margin of .

\begin{figure*}[h]
    \centering
    \subfloat[]{
    \includegraphics[width=0.9\linewidth]{figures/MULDENS_groups.png}}
    
    \subfloat[]{
    \includegraphics[width=\linewidth]{figures/MULDENS_hist.png}}
    
    \caption{\textbf{Visualizing the groups inferred using \muld}. (a) For this visualization, we trained \muld~using A (art), C (cartoon) and P (photo) domains from the PACS dataset and show randomly selected example images assigned to each of the three models in the ensemble. We notice that the MRS scoring based on gradient-matching assigns different subsets of each input domain to different groups; (b) We plot the distribution of class labels in each of the inferred domain groups. Interestingly, while MRS assigns different subsets of domains to different groups, it does not perform a trivial category split. In fact, our approach is able to effectively exploit the intra- and inter-domain discrepancies in order to evolve meaningful groups that maximally benefit the generalization performance.}
    \label{fig:groups}
\end{figure*}




\subsection{Ablations}
\label{sec:ablation}
In this section, we perform ablation experiments to understand (i) the role of our proposed gradient-matching based MRS; (ii) the impact of meta-validation and train augmentation; (iii) the choice of  and lastly (iv) the impact of ensemble size on generalization performance.

\subsubsection{Choice of MRS strategy}As explained in Section 5, we experimented with four different choices for the scoring function . In Figure~\ref{fig:behavior}(right), we show the effect of these choices on the performance using the VLCS dataset. We find that the proposed gradient-matching based MRS performs the best, considerably outperforming loss-based model assignment.

\subsubsection{Impact of meta-validation and train augmentation} 

In Table~\ref{tab:aug}, we show how the performance of \muld~varies when the augmentation protocol is changed. As discussed earlier, we divide data from each observed domain into three disjoint sets - train, meta-validation and held-out validation. We explore whether adding synthetic augmentations to create additional meta-validation batches can lead to improved implicit grouping. Note that, in this setup, we applied standard augmentation (following DomainBed) to the meta-train batches. It is evident from the table that, removing meta-validation augmentation leads upto  decrease in accuracy in the cases of VLCS and Office Home datasets.   

We performed another ablation by not including any augmentation to the meta-train batches, while still considering synthetically augmented meta-validation batches. We observed that, for benchmarks with larger cross-domain gap such as, TerraIncognita~\citep{terra}, removing augmentation during training has significant effect on performance with upto  drop and unsurprisingly the performance in this case of no train augmentation is lower than ERM with train augmentation. In contrast, with benchmarks such as VLCS, not applying train augmentation does not show any apparent impact.
\begin{table*}[h!]
\caption{\textbf{Impact of augmenting meta-train and meta-validation sets}. Without meta-validation augmentation and only train augmentation, we observed reduced performance across all benchmarks. On the other hand, omitting train augmentation and including only meta-validation leads to bigger performance drops on more challenging datasets such as TerraIncognita. 
}
\label{tab:aug}
\centering
\adjustbox{max width=1.1\textwidth}{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\rowcolor[HTML]{404040}
\multicolumn{2}{|c|}{\textcolor{white}{Method}}                                                                 & \textcolor{white}{PACS}       & \textcolor{white}{VLCS}       & \textcolor{white}{Office Home} & \textcolor{white}{Terra Incognita} \\ \hline \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\muld~\\ (No Meta-Valid Aug)\end{tabular}} & Avg & 86.93  0.73 & 78.50  0.17 & 69.37  0.12  & 48.38  0.73      \\ 
                                                                                       & Ens & 86.58  1.52 & 77.06  0.05 & 68.74  0.28  & 47.99  0.97      \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\muld~\\ (No Train Aug)\end{tabular}}      & Avg & 85.46  1.29 & 78.88  0.45 & 69.05   0.28 & 45.87  1.13      \\ 
                                                                                       & Ens & 85.21  1.26 & 78.52  0.27 & 69.29  0.36  & 46.09  0.52      \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\muld~\end{tabular}}      & Avg & 87.35  0.22 & 79.02  0.20 & 69.76   0.48 & 48.66  0.85      \\ 
                                                                                       & Ens & 87.21  0.94 & 78.40  0.19 & 70.06  0.16  & 48.48  0.96      \\ \hline
\end{tabular}}
\end{table*}
\subsubsection{Choice of }
Next, we studied the sensitivity of the penalty , a hyper-parameter that controls the penalty for the meta-validation loss in eq.~\eqref{eq:outer loop upate} in terms of the performance of \muld~using the OfficeHome~\citep{officehome} dataset. As evidenced in Table~\ref{tab:eta}, for values greater than  the performance of \muld~is stable with respect to changes in .

\begin{table}[t]
    \centering
    \caption{\textbf{Impact of the choice of }. We find that \muld~is stable \wrt~change in  values.}
    \label{tab:eta}
    \renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \rowcolor[HTML]{404040}
    \multicolumn{2}{|c|}{\textcolor{white}{Choice of }}  & \textcolor{white}{A}          & \textcolor{white}{C}          & \textcolor{white}{P}          & \textcolor{white}{R}      & \textcolor{white}{Avg.}  \\
    \hline
    \hline
    \multirow{2}{*}{  } & Avg & 63.08 & 55.52 & 78.60 & 79.80 & 69.25 \\
    & Ens & 64.72 & 55.09 & 78.60 & 81.15 & 69.89 \\\hline
    \multirow{2}{*}{   } & Avg & 63.44 & 56.41 & 77.42 & 79.06 & 69.08 \\
    & Ens & 62.98 & 54.85 & 77.43 & 79.23 & 68.62 \\\hline
    \multirow{2}{*}{  } & Avg & 62.61 & 56.65 & 76.63 & 78.91 & 68.70 \\
    & Ens & 62.52 & 57.18 & 76.54 & 78.60 & 68.71 \\\hline
    \multirow{2}{*}{   } & Avg & 63.54 & 56.41 & 77.06 & 79.35 & 69.09 \\
    & Ens & 62.72 & 56.41 & 76.83 & 79.29 & 68.81 \\\hline
    \multirow{2}{*}{   } & Avg & 63.13 & 56.67 & 76.60 & 79.98 & 69.10 \\
    & Ens & 62.05 & 56.16 & 76.46 & 79.58 & 68.56 \\\hline
    \end{tabular}
\end{table}



\subsubsection{Impact of Ensemble Size.} The ensemble size  is a standard hyper-parameter in any ensembling approach. The complexity (size) of the ensemble is controlled such that a simple averaging strategy can still work for unseen test domains. When  becomes large and models become diverse, one might require a ``manager'' module to select a specific model from the ensemble. However, the ensemble size is not necessarily connected to the number of domains and for simplicity, we fixed  for all cases (identified using parameter search on PACS, VLCS). Table~\ref{tab:change_m} shows the performance of \muld~on VLCS~\citep{VLCS} at different ensemble sizes ().
We observe that while a three-model ensemble outperforms two-model ensemble by almost , there is no significant improvement achieved beyond . We note that at this time the choice of  is empirical, and extending this framework to automatically identify the required number of models, such that simple averaging of predictions from the ensemble can still be effective, is part of our future work.
\begin{table}[h!]
    \centering
    \caption{\textbf{Impact of change in ensemble size}. For the VLCS benchmark, We observed that there was no improvement beyond .}
    \label{tab:change_m}
    \renewcommand{\arraystretch}{1.2}
  
\begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
     \rowcolor[HTML]{404040}
    \multicolumn{2}{|c|}{\textcolor{white}{Choice of M}}  & \textcolor{white}{C}          & \textcolor{white}{L}          & \textcolor{white}{S}          & \textcolor{white}{V}      & \textcolor{white}{Avg.}  \\
    \hline
    \hline
    \multirow{2}{*}{ M = 2 } & Avg & 98.32 & 62.58 & 69.04 & 77.78 & 76.93 \\
    & Ens & 98.05 & 62.40 & 72.88 & 76.78 & 77.53 \\\hline
    \multirow{2}{*}{ M = 3 } & Avg & 98.69  & 65.99  &73.12 &78.27 &79.02  \\
    & Ens & 98.60 & 65.61 &72.26 & 77.11 & 78.4\\\hline
    \multirow{2}{*}{ M = 4 } & Avg & 98.58 & 65.84 & 72.58 & 78.02 & 78.76 \\
    & Ens & 98.58 & 65.43 & 72.76 & 78.41 & 78.80 \\\hline
    \end{tabular}
\end{table}