\section{Experimental Results}
\label{sec:results}
For all experiments we train our model on a dataset composed of the top 50 classes (measured by number of samples in the training set) of the Places365-Challenge dataset \cite{zhou2017places}, producing a training set of just under 2 million images, which we scaled to the spatial dimensions of 257x257 pixels. The use of 50 classes allows us to test how well our model can generalize to multiple categories. We used a held-out set of 500 images from the same set of classes in the Places365 dataset, approximately 10 images per class, to compute quantitative scores and to visualize the image extension results.

\subsection{Image Extension}
\label{subsec:extension}

\begin{table}[tp]
\small
\setlength{\tabcolsep}{2pt}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
  & FID & PSNR & FID & PSNR & FID & PSNR & FID & PSNR\\
& (25\%) & (25\%) & (50\%) & (50\%) & (75\%) & (75\%) & (Inp) & (Inp) \\
\hline
\hline
DF & 1.87 & 7.11 & 11.65 & 6.69 & 31.21 & \textbf{9.74} & 4.96 & \textbf{14.31} \\
PC & 1.40 & \textbf{11.10} & 11.20 & 6.63 & 31.83 & 8.94 & 3.70 & 13.78 \\
NCnd & 0.85 &  8.96 & 5.01 & 7.55 & 19.17 & 9.08 & 2.73 & 14.24 \\
Ours & \textbf{0.79} & 10.17 & \textbf{3.46} & \textbf{8.63} & \textbf{8.79} & 8.07 & \textbf{2.53} & 14.17 \\
\hline
\end{tabular}
\vspace{4pt}
\caption{Quantitative metrics on $500$ test images.The mask types are: 25\% extension (3:1 ratio of context to mask), 50\% extension (1:1 ratio), 75\% (1:3 ratio) and inpainting a central square mask comprising 25\% of image pixels(Inp). We compare DeepFill(DF), PartialConv(PC), ours without conditioning(NCnd), and our model.}
\label{exp:table_all}
\end{table}

\begin{table}[tp]
\small
\setlength{\tabcolsep}{2pt}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
  & FID & PSNR & FID & PSNR & FID & PSNR \\
& (25\%) & (25\%) & (50\%) & (50\%) & (75\%) & (75\%) \\
\hline
\hline
Prcptl & \textbf{0.40} &  9.95 & \textbf{2.32} & 8.31 & 14.15 & \textbf{9.65}  \\
FM & 0.75 &  9.42 & 3.14 & \textbf{8.97} & 14.74 & 8.87  \\
Ours & 0.79 & \textbf{10.17} & 3.46 & 8.63 & \textbf{8.79} & 8.07 \\
\hline
\end{tabular}
\vspace{4pt}
\caption{Comparisons with other methods for stabilizing GAN training. We provide PSNR for reference, but found that FID correlates with perceptual quality best. Based on FID on 25\% and 50\% extensions, feature matching and perceptual losses outperform our conditioning, but the difference is fairly small. On 75\% extensions, our conditioning provides the best results and the difference is large. }
\label{exp:table_fm_prcptl}
\end{table}

\begin{figure*}
	\centering
    \hspace{-10pt}\includegraphics[width=1.02\textwidth]{figs/uncrop_various_ratios.pdf}
    \caption{Extending images from masks of  (a) 25\%,  (b) 50\% and (c) 75\% of the image width using multiple algorithms. From left to right: DeepFill \cite{yu2018generative}, PConv \cite{liu2018partialinpainting}, Photoshop Content Aware Fill, our model with no conditioning, our full model and ground truth.}    
    \label{fig:uncrop_various_masks}
\end{figure*}

We compare our model (which we call \textbf{Ours}) with various baselines both qualitatively and quantitatively on the task of image extension. Specifically, we evaluate each algorithm's ability to fill in masked out image content for three different image extension tasks (where the rightmost 25\%, 50\% and 75\% of pixels in the image are respectively masked) and one inpainting task (a central square mask comprising 25\% of image pixels). For each experiment, our model is retrained with masks of the appropriate position, shape and size. The baselines we compare against are:\\
    \noindent\textbf{No-Cond}: A model that is identical to ``Ours," but without discriminator conditioning.\\
    \noindent\textbf{DeepFill}: Our re-implementation of  DeepFillv2 \cite{yu2018free}, which is a state of the art inpainting model. We confirmed that our reimplementation achieves inpainting results nearly identical to that of the original papers (see Figure \ref{fig:inpainting_ablation}). For each experiment, we retrain the model with the same masks and data as ours. We follow the authors' guidance of training for 5 days on an NVIDIA P100 GPU. We note that this results in the model being trained for many fewer steps than ``Ours" because it trains much more slowly (0.8 steps/sec vs 4.7 steps/sec for ``Ours"). Comparison against this model shows the benefits of our approach compared to simply repurposing an architecture suited for inpainting tasks.\\
    \noindent\textbf{PConv}: The authors of another state of the art inpainting work \cite{liu2018partialinpainting} generated results for us based on provided masks, but the models were not retrained specifically for these tasks. The model was trained on the full Places2 dataset, which is a superset of our training set. They use a database of free-form masks, some of which are very large (up to $50\%$ of the image size), but are often non-contiguous and non-convex, which means that at training time the model may not have needed to generate pixels that were very far from known context. While our comparisons to this paper are not exactly apples-to-apples, we believe that this still provides a strong baseline against which to compare our performance.\\
    \noindent\textbf{CAF}: Results from Adobe Photoshop's content aware fill, which is based on the PatchMatch  algorithm \cite{barnes2009patchmatch}. Content aware fill is a very powerful tool used for image extension, and reprents a strong classical baseline. However, due to the use of only patch level information, it does not provide semantically meaningful extensions.\\   
    
We provide quantitative performance metrics for each mask-type and each algorithm in Table \ref{exp:table_all}. We agree with the authors of \cite{yu2018generative, liu2018partialinpainting} that there are really no good metrics that capture the goals of these experiments, but we nonetheless report the best approximations. Specifically, we report Fr\'echet Inception Distance (FID) \cite{heusel2017gans} on the full output image and PSNR of the masked regions only. For FID we used a diagonal covariance matrix, due to having few samples. Based on our own qualitative evaluations, we feel that FID of the entire output image best correlates with what we perceive as quality image extension. We additionally performed a qualitative analysis. We show results on a few images from our test set in Figures \ref{fig:uncrop_various_masks} and \ref{fig:inpainting_ablation}. We show many more results, including on free-form masks, in the Supplementary Material. Overall we see that all methods, other than ``CAF," perform admirably for inpainting. On the extension tasks, as we move towards larger extensions with smaller context, ``CAF" and ``DeepFill" degrade into just repeating textures, while ``PConv" gets blurrier and more artifact-filled the further away from the context it gets. The ``NoCond" version of our model maintains higher quality for the larger extensions but does show some blurring and semantic drift. Meanwhile, our full model remains semantically consistent, with mostly photorealistic and seamless synthesis.

Furthermore, we experiment with replacing our conditioning with perceptual \cite{johnson2016perceptual} and feature matching \cite{wang2018pix2pixHD} losses, see Table \ref{exp:table_fm_prcptl} and Figure \ref{fig:perceptual}. In the perceptual loss, the generator is optimized to produce images that are close to the ground truth images in the activation space of a pretrained classification network. Similar to our conditioning, perceptual loss uses a pre-trained network to guide the training towards plausible extensions. Unlike our conditioning, the perceptual loss  modifies  only the generator objective to bias it towards semantic coherence, whereas our conditioning figures into both the generator and discriminator objectives and adding semantic information to the whole adversarial optimization game. 
Feature matching is similar in principle to a perceptual loss, but it minimizes the distance between activations in a hidden layer of the discriminator, rather than a pre-trained classification network, and similarly only figures into the generator objective.

In our experiments on 75\% extensions, we found that our conditioning performs significantly better than feature matching and perceptual loss, while on smaller extensions they perform similarly. Our perceptual loss implementation tries to match pre-softmax logits, while our feature matching follows \cite{wang2018pix2pixHD} and tries to match convolutional feature maps in the discriminator at multiple scales. Preliminary experiments indicate that combinations of all the three result in even better results, chiefly with fewer GAN-style artifacts, but we leave a more thorough analysis to future work.

We also qualitatively compare against PixelCNN in Figure \ref{fig:perceptual}. It is clear PixelCNN performs much worse than our method. Furthermore, it takes about 12 minutes to do inference for a single 64x64x3 image, on a Tesla P100. On our higher resolution 256x256 images, this would translate to over 2.5 hours. In contrast, our method takes 0.1 seconds/image.

\begin{figure}[ht!]
\newcommand{\figwidth}{.08\textwidth}
\newcommand{\shiftleft}{\hspace{-20pt}}
\newcommand{\figurefile}[2]{\frame{\includegraphics[width=\figwidth]{figs/perceptual_featurematch_comp/#1/#2}}}
\newcommand{\onerow}[1]{
\figurefile{#1}{input.jpg} & 
\figurefile{#1}{bndls.jpg} & 
\figurefile{#1}{prcptl.jpg} & 
\figurefile{#1}{fm.jpg} &
\figurefile{#1}{pixelcnn.jpg} 
}

\setlength{\tabcolsep}{1.2mm}
\begin{tabular}{ccccc}
 {\small Input}  &  {\small Ours} & {\small Perceptual}  &  {\small FM} & {\small PixelCNN} \\
\onerow{example_6}\\
\onerow{example_7}\\
\onerow{example_1}\\
\end{tabular}
\caption{\small Although FM and Perceptual give slightly better results on short range extensions, our model far outperforms the others in the 75\% case. }
\label{fig:perceptual}
\end{figure}
 



\subsection{Ablations}
\label{subsec:ablation}

In order to validate the contributions of each aspect of our model, we trained models each ablating a single feature of our network. We experimented with removing skip connections in the generator, removing feature conditioning from the discriminator, removing instance norm from the generator and reducing batch size to 64.

We show an example of the results in Figure \ref{fig:inpainting_ablation}. We see that without skip connections, the model has a hard time synthesizing high frequency textures, which often results in a perceptible seam between the real and generated content. Without discriminator conditioning the quality degrades more rapidly as we move further away from the edge with known context. Removing instance norm causes an increase in white and over-saturated artifacts. The smaller batch model generally produces blurrier results.

\begin{figure*}
\newcommand{\figurewidth}{\textwidth}
\small
    \centering
    \begin{minipage}{\textwidth}
	    \centering
        \includegraphics[width=\figurewidth]{figs/inpainting_ablation.pdf}
        \caption{Further analysis: (a) Comparing the different models on inpainting tasks; our conditioned model performs on par with the state of the art models such as PConv and DeepFill for the inpainting problem. (b) Ablation tests: we remove (from second column to fifth column) only one of the following: discriminator conditioning, instance norm, skip connections, and reduce batch size.}    
        \label{fig:inpainting_ablation}
    \end{minipage}\\[5pt]
 
    \begin{minipage}{\textwidth}
    	\centering
        \includegraphics[width=\figurewidth]{figs/panorama.pdf}
        \caption{Our models can also be used generate image panoramas. This can be viewed as a stress test for image extension tasks. We recursively apply the 25\% model to create a very large output image of about 3 times the original width.}
        \label{fig:application:panorama}
    \end{minipage}    
\end{figure*}

\subsection{Panorama Generation}
\label{subsec:panorama}

We evaluate our model on its ability to generate panoramic images from a much narrower seed image. We use the same dataset and models as in Section \ref{subsec:extension}, but whereas in those experiments we ran each model's forward pass only once, this time we apply the model recursively, using the synthesized content from the previous step as the known content for the current step in a sliding window approach. More specifically, we take a 257x192 image, pad it with 65 columns of zeros, and extend it using our our model trained for 25\% extension. We then extend the image again by selecting the right most 192 pixels of the image, padding with zeros, and feeding it to the extension model. This is repeated 5 times, ultimately extending the image 2.7 times out to a width of 582 pixels. We show some results in Figure \ref{fig:application:panorama} and more results in the Supplementary Material. While the results are for the most part plausible and aesthetically pleasing, we do see some degradation and semantic drift as we move away from the original image. 

\subsection{Exploring the Space of Plausible Extensions}
\label{subsec:video}


To visually explore the space of results generated by our extension model, we took sample videos from the YouTube8m dataset \cite{abu2016youtube} and applied our model to extend both the right and left sides horizontally. Videos are a natural domain to test our model since consecutive frames are closely related to each other, and the small variations can generate interesting plausible outcomes. This allows us to verify that our model has not memorized a fixed completion for closely related images or collapses with small natural variations in the input. We scaled the videos to have a height of 257 pixels and respectively selected the right and leftmost 192 columns. We then padded each of them with 65 columns of zeros on the side to be extended. We ran the resulting images, each frame independently, through our model trained with 25\% masks. We then took the model output for the extended region and concatenated it back on the original video frame. Please refer to the supplementary video \url{https://drive.google.com/open?id=1x6FCYPmoqSuCdeLJTD0UpQ_MQhBPv7_e}. 



















