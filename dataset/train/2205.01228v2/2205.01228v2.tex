\appendix

\section*{Appendix}

\section{Datasets}
\label{app:datasets}

We present the complete details for all the datasets used in this paper along with links to download them for reproducibility of results.

\subsection{Pre-training Datasets}
We use the Wikipedia\footnote{\url{https://dumps.wikimedia.org/enwiki/20211101/}}, BookCorpus\footnote{\url{https://huggingface.co/datasets/bookcorpusopen}}, OpenWebText \citep{Gokaslan2019OpenWeb} and CC-News\footnote{\url{https://commoncrawl.org/2016/10/news-dataset-available/}} datasets for performing pre-training of our joint transformer models. We do not use the STORIES dataset as it is no longer available for research use \footnote{\url{https://github.com/tensorflow/models/tree/archive/research/lm\_commonsense\#1-download-data-files}}. After decompression and cleaning we obtained 6GB, 11GB, 38GB and 394GB of raw text respectively from the BookCorpus, Wikipedia, OpenWebText and CC-News.

\subsection{Finetuning Datasets}
We evaluate our joint transformers on three AS2 and one Fact Verification datasets. The latter differs from the former in not selecting the best candidate, but rather explicitly using all candidates to predict the target label.
Here are the details of the finetuning datasets that we use for our experiments along with data statistics for each dataset:


\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{clccc}
    \toprule
        \textbf{Dataset} &  \textbf{Split} & \textbf{\# Questions} & \textbf{\# Candidates} & \textbf{Avg. \# C/Q} \\
        \midrule
        \multirow{3}{*}{\rotatebox[origin=c]{90}{\small ASNQ}}
        & Train   & 57,242 & 20,377,568  & 356.0 \\
        & Dev     & 1,336  & 463,914    & 347.2 \\
        & Test    & 1,336  & 466,148    & 348.9 \\
        \midrule
        \multirow{3}{*}{\rotatebox[origin=c]{90}{\small WikiQA}}
        & Train   & 2,118  & 20,360     & 9.6 \\
        & Dev     & 122   & 1,126      & 9.2 \\
        & Test    & 237   & 2,341      & 9.9 \\
        \midrule
        \multirow{3}{*}{\rotatebox[origin=c]{90}{\small TREC-QA}}
        & Train   & 1,226  & 53,417     & 43.6 \\
        & Dev     & 69    & 1,343      & 19.5 \\
        & Test    & 68    & 1,442      & 21.2 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Statistics for ASNQ, WikiQA and TREC-QA datasets.}
    \label{tab:as2}
\end{table}



\begin{itemize}[wide, labelwidth=!, labelindent=0pt]
    \item \textbf{ASNQ:} A large-scale AS2 dataset~\cite{garg2019tanda}\footnote{\url{https://github.com/alexa/wqa_tanda}} where the candidate answers are from Wikipedia pages and the questions are from search queries of the Google search engine. ASNQ is a modified version of the Natural Questions (NQ)~\cite{kwiatkowski-etal-2019-natural} dataset by converting it from a machine reading to an AS2 dataset. This is done by labelling sentences from the long answers which contain the short answer string as positive correct answer candidates and all other answer candidates as negatives. We use the dev. and test splits released by ~\citeauthor{soldaini-moschitti-2020-cascade}\footnote{\url{https://github.com/alexa/wqa-cascade-transformers}}.

    \item \textbf{WikiQA:} An  AS2 dataset released by \citeauthor{yang2015wikiqa}\footnote{\url{http://aka.ms/WikiQA}} where the questions are derived from query logs of the Bing search engine, and the answer candidate are extracted from Wikipedia. This dataset has a subset of questions having no correct answers (all-) or having only correct answers (all+). We remove both the all- and all+ questions for our experiments (``clean" setting).
    
    \item \textbf{TREC-QA:} A popular AS2 dataset released by \citeauthor{wang-etal-2007-jeopardy}. For our experiments, we trained on the \textit{train-all} split, which contains more noise but also more question-answer pairs. Regarding the dev. and test sets we removed the questions without answers, or those having only correct or only incorrect answer sentence candidates. This setting refers to the ``clean" setting~\cite{shen-etal-2017-inter}, which is a TREC-QA standard.
    
    \item \textbf{FEVER:} A popular benchmark for fact extraction and verification released by \citeauthor{thorne-etal-2018-fever} The aim is to retrieve evidences given a claim, and then identify whether the retrieved evidences support or refute the claim or if there is not enough information to make a choice. For supporting/refuting a claim, at least one of the retrieved evidences must support/retrieve the claim. Note that the performance on FEVER depends crucially on the retrieval system and the candidates retrieved. For our experiments, we are interested only in the fact verification sub-task and thus we exploit the evidences retrieved by \citeauthor{liu2020kernel} using a BERT-based DocIR\footnote{\url{https://github.com/thunlp/KernelGAT/tree/master/data}}.
    
    \begin{table}[h]
    \centering
    \small
    \begin{tabular}{lccc}
    \toprule
        \textbf{Split} & \textbf{\# Claims} & \textbf{\# Evidences} & \textbf{Avg. \# E/C} \\
        \midrule
        Train   & 145,406    & 722,473 & 4.97 \\
        Dev     & 19,998     & 98,915  & 4.95 \\
        Test    & 19,998     & 98,839  & 4.94 \\ \bottomrule
    \end{tabular}
    \caption{Statistics for the FEVER dataset where evidences has been retrieved using \citep{liu2020kernel}.}
    \label{tab:fever}
    \end{table}    
\end{itemize}



\section{Experimental Setup}
\label{app:experiments}

\subsection{Complete Experimental Details}
Following standard practice, the token ids, positional ids and token type ids are embedded using separate embedding layers, and their sum is fed as the input to the transformer layers.
We use  for our experiments (following \citeauthor{zhang-etal-2021-joint, tymoshenko-moschitti-2021-strong}), and perform continuous pre-training starting from the RoBERTa-Base checkpoint using a combination of MLM and our MSPP pre-training objective for 100,000 steps with a batch size of 4096. We use a triangular learning rate with 10,000 warmup steps and a peak value of . We use Adam optimizer with ,  and . We apply a weight decay of  and gradient clipping when values are higher than . We set the dropout ratio to  and we use two different prediction heads for pre-training: IE and AE. We follow the strategy of ~\cite{devlin2019bert,lan2020albert}, and equally weight the the two pre-training loss objectives: MLM and MSPP.

For evaluation, we fine-tune all models on the downstream AS2 and FEVER datasets: using the same IE and AE prediction heads exploited in pre-training for AS2 and using either IE or AE prediction heads for FEVER. We finetune every model with the same maximum sequence length equal to  tokens. For ASNQ we train for up to 6 epochs with a batch size of 512 and a learning rate of  with the same Adam optimizer described above but warming up for only 5000 steps. We do early stopping on the MAP of the development set. For WikiQA and TREC-QA, we created batches of 32 examples and we used a learning equal to  and 1000 warm up steps. We train for up to 40 epochs again with early stopping on the MAP of the development set. On FEVER, we use a batch size of 64, a learning rate of , 1000 warm up steps and we do early stopping checking the Accuracy over the development set. We implemented our code based on HuggingFace's Transformers library~\cite{wolf-etal-2020-transformers}.

\subsection{Baselines}
For AS2, we consider two baselines: (i) pairwise RoBERTa-Base model when used as a cross-encoder for AS2, and (ii) RoBERTa-Base LM when used as a joint model with IE and AE prediction heads independently for AS2 tasks.

For FEVER, we use several recent baselines from \citeauthor{tymoshenko-moschitti-2021-strong}: (i) GEAR~\cite{zhou-etal-2019-gear}, (ii) KGAT~\cite{liu2020kernel}, (iii) Transformer-XH~\cite{zhao2020transformer-xh}, (iv) joint RoBERTa-Base with IE prediction head~\cite{tymoshenko-moschitti-2021-strong}, (v) pairwise RoBERTa-Base when used as a cross-encoder with max-pooling head~\cite{tymoshenko-moschitti-2021-strong}, (vi) pairwise RoBERTa-Base when used as a cross-encoder with weighted-sum head~\cite{tymoshenko-moschitti-2021-strong}.

We used metrics from Torchmetrics~\cite{torchmetrics} to compute MAP, MRR, Precision@1 and Accuracy.


\subsection{Metrics}

The performance of AS2 systems in practical applications is typically~\cite{garg-moschitti-2021-will} measured using the Accuracy in providing correct answers for the questions (the percentage of correct responses provided by the system), also called the Precision-at-1 (P@1). In addition to P@1, we use Mean Average Precision (MAP) and Mean Reciprocal Recall (MRR) to evaluate the ranking produced of the set of candidates by the model.

For FEVER, we measure the performance using Label Accuracy (LA), a standard metric for this dataset, that measures the accuracy of predicting support/refute/neither for a claim using a set of evidences.


\section{Complete Results and Discussion}
\label{app:complete_results}


\begin{table*}[t]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lccccccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{ASNQ}} & &\multicolumn{3}{c}{\textbf{WikiQA}} & & \multicolumn{3}{c}{\textbf{TREC-QA}} \\ \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
                       & \textbf{P@1}    & \textbf{MAP}    & \textbf{MRR}   & &\textbf{ P@1}     & \textbf{MAP}     & \textbf{MRR}   & & \textbf{P@1}   & \textbf{MAP}     & \textbf{MRR}     \\
    \midrule
Pairwise RoBERTa-Base   & 61.8   & 66.9   & 73.1  & & 77.1    & 85.3    & 86.5  & & 87.9    & 89.3    & 93.1    \\
Joint RoBERTa-Base  FT IE   &  25.2    &  44.0      &  45.6     & & 24.6    & 49.3    & 49.7   && 57.6    & 73.7    & 74.6    \\
Joint RoBERTa-Base  FT AE &   25.4     &    44.8    &    46.2    && 26.4    & 50.6    & 51.1   && 60.9   & 74.6    & 76.7    \\
(\textbf{Ours}) Joint MSPP IE  FT IE       & 63.9   & 71.3   & 73.1   && \textbf{82.7}    & \textbf{88.5}    & \textbf{89.0}   && \textbf{92.2}    & \textbf{93.5}    & \textbf{95.4} \\
(\textbf{Ours}) Joint MSPP AE  FT AE   & \textbf{64.3}   & \textbf{71.5}   & \textbf{73.4}   && 82.1    & 87.9    & 88.7   && 91.2    & \textbf{93.5}    & 94.9   \\
\bottomrule
\end{tabular} }
    \caption{Complete results of our joint models for AS2 datasets when \textbf{re-ranking the answer candidates ranked in top-k by Pairwise RoBERTa-Base}. MSPP, FT refer to our pre-training task and finetuning respectively. We indicate the prediction head (IE/AE) used for both pre-training and finetuning.}
    \label{tab:app_results_as2_topk}
\end{table*}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
        \textbf{Model} & \textbf{Dev} & \textbf{Test} \\
        \midrule
        GEAR & 70.69 & 71.60 \\
        KGAT with RoBERTa-Base & 78.29 & 74.07 \\
        Transformer-XH & 78.05 & 72.39 \\
        Pairwise BERT-Base & 73.30 & 69.75 \\
        Pairwise RoBERTa-Base + MaxPool & 79.82 & - \\
        Pairwise RoBERTa-Base + WgtSum & 80.01 & - \\
        Joint BERT-Base & 73.67 & 71.01 \\
        Joint RoBERTa-Base + FT IE & 79.25 & 73.56 \\
        (\textbf{Ours}) Joint Pre IE + FT IE  & \bfunder{81.21} (0.24)    & \textbf{74.39} \\
        (\textbf{Ours}) Joint Pre IE + FT AE  & \underline{81.10} (0.15)  & 74.25 \\
        (\textbf{Ours}) Joint Pre AE + FT IE  & \underline{81.18} (0.14)  & 73.77  \\
        (\textbf{Ours}) Joint Pre AE + FT AE  & \bfunder{81.21} (0.16)    & 74.13 \\ \midrule
        \multicolumn{3}{c}{\textbf{Methods with larger models and/or sophisticated retrieval}}\\
        DOMLIN++ & 77.48 & 76.60 \\
        DREAM & 79.16 & 76.85 \\
        \bottomrule
    \end{tabular}}
    \caption{Complete Results on FEVER dev and test sets. For our method, prediction heads (IE/AE) are only used for finetuning (FT), while for pre-training (Pre) we use the (IE/AE) heads. '-' denotes models that are not publicly released and have no reported results on the test split in their published paper. Statistically significant results (T-Test ) are underlined.}
    \label{tab:app_results_fever}
    \vspace{.5em}
\end{table}


\subsection{Results on AS2 with cascaded pairwise and Joint re-ranker}

Below we present results of evaluating our joint models to re-rank the top- candidates ranked by the pairwise RoBERTa-Base cross-encoder. Our joint models can significantly improve the P@1, MAP and MRR over the baseline for all datasets. The performance gap stems from questions for which the pairwise RoBERTa model was unable to rank the correct answer at the top position, but support from other candidates in the top-k helped the joint model rank it in the top position.


\subsection{Results on FEVER}
Here we present complete results on the FEVER dataset in Table~\ref{tab:app_results_fever}, by also presenting some additional baselines such as: (i) pairwise BERT-Base cross-encoder~\cite{tymoshenko-moschitti-2021-strong}, (ii) joint BERT-Base cross-encoder with IE prediction head, (iii) DOMLIN++~\cite{Stammbach2020eFEVEREA} which uses additional DocIR components and data (MNLI~\cite{williams-etal-2018-broad}) for fine-tuning, (iv) DREAM~\cite{zhong-etal-2020-reasoning} that uses the XL-Net model. Note that comparing our joint models with (iii) and (iv) is unfair since they use additional retrieval components, datasets and larger models. We just include these results here for the sake for completeness. Interestingly, our joint models outperform DREAM and DOMLIN++ on the dev set without using additional retrieval and larger models. 

\subsection{Compute Overhead of Joint Models}
\label{app:number_params}

\mypara{Change in Number of Model Parameters:}
The transformer block of our joint inference model is identical to pre-trained models such as RoBERTa, and contains the exact same number of parameters. Classification heads  and  all operate on the embedding of a single token, and are identical to the classification head of RoBERTa ( operates on the concatenation of two token embeddings, and contains double the number of parameters as the RoBERTa). The maximum sequence length allowed for both the models is the same (512). The exact number of parameters of our joint model with  and the RoBERTa model are  and  respectively.

\mypara{Change in Inference Latency:} While our joint model provides a longer input sequence to the transformer, it also reduces the number of forward passes that need to be done by a pairwise cross-encoder. A \emph{simplified} latency analysis for AS2 (assuming each sentence has a length ): pairwise cross-encoder will need to make  forward passes of the transformer with a sequence of length  ( with each candidate ), while our joint model will only need to make  forward pass of the transformer with input length  ( with  candidates). Transformer self-attention is quadratic in input sequence length, so this should lead to the inference time of out joint model being  times the inference time of the cross-encoder. However, the input embedding layer and the feedforward layers are linear in input sequence length, so this should lead to a reduction in the inference time of our joint model by  times the inference time of the cross-encoder. Empirically, when we fine-tune for WikiQA on one A100-GPU, we only observe latency increasing from  (increase of only ).