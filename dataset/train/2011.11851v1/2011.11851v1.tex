



\documentclass[11pt]{article}

\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}



\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{xspace}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{framed}
\usepackage{tcolorbox}
\usepackage{bm}
\usepackage{subfig}
\usepackage{makecell}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{url}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}







\title{Relation Extraction with Distant Supervision and Human Annotation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}



\mathchardef\mhyphen="2D 

\newcommand{\triple}[3]{\ensuremath{\langle #1,#2,#3\rangle}}
\newcommand{\minisection}[1]{\vspace{0.03in}{\bf \noindent #1.} }
\newcommand{\hanet}{\emph{HA-Net}\xspace}
\newcommand{\dsnet}{\emph{DS-Net}\xspace}
\newcommand{\munet}{-Net\xspace}
\newcommand{\sigmanet}{-Net\xspace}
\newcommand{\normal}[2]{\mathcal{N}(#1, #2)}
\newcommand{\lognormal}[2]{{L}(#1, #2)}
\newcommand{\head}{\xspace}
\newcommand{\tail}{\xspace}
\newcommand{\ehead}{e_{h}}
\newcommand{\etail}{e_{t}}
\newcommand{\norel}{\texttt{NA}\xspace}

\newcommand{\entity}[1]{\textbf{\color{blue!55!green}\textsf{#1}}}
\newcommand{\relation}[1]{\textbf{\color{red!55!yellow}\texttt{#1}}}
\newcommand{\relationt}[1]{\texttt{#1}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\fracsmall}[2]{#1/#2}
\newcommand{\fracpas}[2]{(#1/#2)}
\newcommand{\transpose}[1]{{#1}^\top}
\newcommand{\realset}[1]{\mathbb{R}^{#1}}

\newcommand{\headvp}{\vect{h}'}
\newcommand{\tailvp}{\vect{t}'}

\newcommand{\headv}{\vect{h}}
\newcommand{\tailv}{\vect{t}}

\newcommand{\memo}[1]{{\color{red} \_\_\_#1\_\_\_}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}


\renewcommand{\figurename}{Figure}


\newcommand{\dcnn}{\emph{CNN\textsubscript{D}}\xspace}
\newcommand{\dlstm}{\emph{LSTM\textsubscript{D}}\xspace}
\newcommand{\dca}{\emph{CA\textsubscript{D}}\xspace}
\newcommand{\dbilstm}{\emph{BiLSTM\textsubscript{D}}\xspace}
\newcommand{\dbert}{\emph{BERT\textsubscript{D}}\xspace}


\newcommand{\sbilstm}{\emph{BiLSTM\textsubscript{S}}\xspace}
\newcommand{\scnn}{\emph{CNN\textsubscript{S}}\xspace}
\newcommand{\sbigru}{\emph{BiGRU\textsubscript{S}}\xspace}
\newcommand{\spcnn}{\emph{PCNN\textsubscript{S}}\xspace}
\newcommand{\spalstm}{\emph{PaLSTM\textsubscript{S}}\xspace}
\newcommand{\sbert}{\emph{BERT\textsubscript{S}}\xspace}




\newcommand{\dual}{\emph{DUAL}\xspace}




\newcommand{\bafix}{\emph{BAFix}\xspace}
\newcommand{\baset}{\emph{BASet}\xspace}
\newcommand{\maxth}{\emph{MaxThres}\xspace}
\newcommand{\entth}{\emph{EntThres}\xspace}
\newcommand{\dsonly}{\emph{DS-Only}\xspace}
\newcommand{\haonly}{\emph{HA-Only}\xspace}

\newcommand{\multitask}{\emph{Multitask}\xspace}
\newcommand{\single}{\emph{Single}\xspace}

\newcommand{\inmid}{\!\in\!}
\newcommand{\minusmid}{\!-\!}
\newcommand{\plusmid}{\!+\!}

\newcommand{\firstdef}[1]{\textbf{#1}}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\newtcolorbox[auto counter]{somebox}[1][]{arc=0pt,auto outer arc,left=1pt,boxsep=1pt,boxrule=1pt,width=\columnwidth,right=1pt,#1}


\setlength\FrameSep{0.5em}
\setlength\OuterFrameSep{\partopsep}


\newcolumntype{C}{>{\Centering\arraybackslash}X} %
 
\graphicspath{{figures/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}



\begin{document}
\maketitle
\begin{abstract}





Relation extraction (RE) has been extensively studied due to its importance in real-world applications such as knowledge base construction and question answering.
Most of the existing works train the models on either distantly supervised data or human-annotated data.
To take advantage of the high accuracy of human annotation and the cheap cost of distant supervision, we propose the dual supervision framework which effectively utilizes both types of data.
However, simply combining the two types of data to train a RE model may decrease the prediction accuracy since distant supervision has labeling bias.
We employ two separate prediction networks \hanet and \dsnet to predict the labels by human annotation and distant supervision, respectively, to prevent the degradation of accuracy by the incorrect labeling of distant supervision.
Furthermore, we propose an additional loss term called \emph{disagreement penalty}
to enable \hanet to learn from distantly supervised labels.
In addition, we exploit additional networks to adaptively assess the labeling bias by considering contextual information.
Our performance study on sentence-level and document-level REs confirms the effectiveness of the dual supervision framework.







\end{abstract} 

\section{Introduction}
\label{sec:intro}
Relation extraction (RE) has been widely used in real-world applications such as knowledge base construction \cite{dong2014knowledge,KnowledgeFusion,JungKS19}, question answering \cite{xu2016question} and biomedical data mining \cite{ahmed2019bio}.
Given a pair of entities in a text (e.g., sentence or document), the goal of RE is to discover the relationships between the entities expressed in the text.
More specifically, we aim to extract triples from the text in the form of \triple{\ehead}{r}{\etail} where \head is a head entity, \tail is a tail entity and  is a relationship between the entities.

\blfootnote{This work is licensed under a Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}.}

To train a model for RE, we need a large volume of fully labeled training data in the form of text-triple pairs.
Although human annotation provides high-quality labels to train the relation extraction models, it is difficult to produce a large-scale training data since manual labeling is expensive and time-consuming.
Thus, \newcite{mintz2009distant} proposed distant supervision to automatically produce a large labeled data by using an external knowledge base (KB).
For a text with a head entity \head and a tail entity \tail, when a triple \triple{\ehead}{r}{\etail} exists in the KB for any relation type , distant supervision produces a label \triple{\ehead}{r}{\etail} even though the relationship is not expressed in the text.
Thus, it suffers from the wrong labeling problem.
For instance, if a triple \triple{UK}{\relationt{capital}}{London} is in the KB, distant supervision labels the triple 
even for the sentence `London is the largest city of the UK'.


Although each of the two labeling methods has a certain weakness, most of the existing works for RE utilize either human-annotated (HA) data or distantly supervised (DS) data.
To take advantage of the high accuracy of human annotation and the cheap cost of distant supervision, we propose to effectively utilize a large DS data as well as a small amount of HA data.
Since DS data is likely to have \emph{labeling bias}, simply combining the two types of data to train a RE model may decrease the prediction accuracy.
To take a close look at the labeling bias, let the \emph{inflation} of a relation type be the ratio of the average frequencies of the relation type per text in DS data and HA data, respectively.
We say that a relation type is \emph{unbiased} if the average frequency of the relation type in DS data is the same as that in HA data (i.e., the inflation of the relation is 1).
By examining a document-level RE dataset (DocRED) \cite{yao2019docred} with 96 relation types, we found that the inflations of the relation types are from 0.48 to 85.9.
It indicates that distant supervision tends to generate a large number of false labels for some relation types.




Recently, \newcite{ye2019looking} introduced a domain adaptation approach to tackle the labeling bias problem for RE.
It trains a RE model on DS data and adjusts the bias term of the output layer by using HA data.
Although the bias adjustment achieves a meaningful accuracy improvement, it has a limitation.
An underlying assumption of the method is that the labeling bias is static for every text since it adjusts the bias term only once after training and uses the same bias during the test time.
However, the labeling bias varies depending on contextual information.
For example, in DocRED dataset, most of the \relationt{capital} relation labeled by distant supervision are false positive.
However, if the phrase `is the capital city of' appears in the text, the label is likely to be a true label.
Thus, we need to take account of contextual information to extract relations more accurately by considering the labeling bias.




To effectively utilize DS data and HA data for training RE models, we propose the \emph{dual supervision framework} that can be applied to most existing RE models to achieve additional accuracy gain.
Since the label distributions in HA data and DS data are quite different, we cast the task of training RE models with both data as a multi-task learning problem.
Thus, we employ the two separate output modules \hanet and \dsnet to predict the labels by human annotation and distant supervision, respectively, while previous works utilize a single output module.
This allows the different predictions of the labels for human annotation and distant supervision, and thus it prevents the degradation of accuracy by incorrect labels in DS data.
If we simply separate the prediction networks to apply the multi-task learning, \hanet cannot learn from distantly supervised labels.
To enable \hanet to learn from DS data, we propose an additional loss term called \emph{disagreement penalty}.
It models the ratio of the output probabilities from the prediction networks \hanet and \dsnet by using maximum likelihood estimation with log-normal distributions to generate the calibrated gradient to update \hanet to effectively reflect distantly supervised labels. 
Furthermore, our framework exploits two additional networks \munet and \sigmanet to adaptively estimate the log-normal distribution by considering contextual information.
Moreover, we theoretically show that the disagreement penalty enables \hanet to effectively utilize the labels generated by distant supervision.
Finally, we validate the effectiveness of the dual supervision framework on two types of tasks: sentence-level and document-level REs.
The experimental results confirm that our dual supervision framework significantly improves the prediction accuracy of existing RE models. 
In addition, the dual supervision framework substantially outperforms the state-of-the-art method \cite{ye2019looking} in both sentence-level and document-level REs with the relative F1 score improvement of up to 32\%.





















 \section{Preliminaries}
\label{sec:preliminaries}
We present the problems of sentence-level and document-level relation extractions and next introduce existing works for relation extraction.


\subsection{Problem Statement}
Following the works \cite{yao2019docred,wang2019fine}, we assume that each text is annotated with entity mentions.
For a pair of entities, since a sentence usually describes a single relationship between them, the sentence-level relation extraction is generally regarded as a \emph{multi-class} classification problem.
\begin{definition}[\textbf{Sentence-level relation extraction}] For a pair of the head and tail entities \head and \tail, a relation type set  and a sentence  annotated with entity mentions, we determine the relation  between \head and \tail in the sentence.
Note that  includes a special relation type \norel which indicates that there does not exist any relation between \head and \tail.
\end{definition}


Since multiple relationships between a pair of entities can be expressed in a document,
document-level relation extraction is usually defined as a \emph{multi-label} classification problem.
\vspace{-0.02in}
\begin{definition}[\textbf{Document-level relation extraction}] For a pair of the head and tail entities \head and \tail, a relation type set  and a document  annotated with entity mentions, we find the set of all relations  between \head and \tail appearing in document . Note that  does not include \norel in this case since it can be represented by an empty set of .
\end{definition}

In this paper, we mainly discuss sentence-level RE and extend our framework to document-level RE.









\subsection{Existing Works of Relation Extraction}
\label{sec:existingworks}
A typical RE model consists of a feature encoder and a prediction network, as shown in \figurename~\ref{fig:architecture}\subref{fig:existing_framework}.
The feature encoder converts a text into the hidden representations of the head and tail entities. 
\newcite{cai2016bidirectional} and \newcite{wang2019fine} exploit Bi-LSTM and BERT, respectively, to encode the text.
On the other hand, \newcite{zeng2014relation} and \newcite{zeng2015distant} use CNN for the encoder. 
In addition, \newcite{zeng2014relation} propose the position embedding to consider the relative distance from each word to head and tail entities.  




The prediction network outputs the probability distribution of the relations between the entities.
Since sentence-level RE is a multi-class classification task, sentence-level RE models \cite{cai2016bidirectional,zeng2014relation,zeng2015distant} utilize a \emph{softmax classifier} as the prediction network and use categorical cross entropy as the loss function.
On the other hand, document-level RE models \cite{yao2019docred,wang2019fine} use a \emph{sigmoid classifier} and binary cross entropy as the prediction network and the loss function, respectively.
Since the labels obtained from distant supervision are noisy and biased,
with a single prediction network, it is hard to make accurate predictions for DS data and HA data together.


\begin{figure}[tb]
\subfloat[Existing models]{
		\includegraphics[width=2.3in]{existing_framework}
		\label{fig:existing_framework}
	}
	\hspace{0.03in}
	\subfloat[The dual supervision framework]{
		\includegraphics[width=3.41in]{proposed_framework}
		\label{fig:proposed_framework}
	}
	\vspace{-0.05in}
	\caption{The overall architectures of existing models and our framework}
	\label{fig:architecture}
	\vspace{-0.03in}
\end{figure}

 


\section{Dual Supervision Framework}
We first present an overview of the dual supervision framework which effectively utilizes both human-annotated (HA) data and distantly supervised (DS) data for training RE models.
We next introduce the detailed structure of the output layer in our framework and propose our novel loss function with disagreement penalty that considers the labeling bias of distant supervision.
Then, we describe how to train the proposed model with both types of data as well as how to extract relations from the test data.
Finally, we discuss how the disagreement penalty makes each prediction network learn from the labels for the other prediction network although we use separate prediction networks.

\subsection{An Overview of the Dual Supervision Framework}
As shown in \figurename~\ref{fig:architecture}\subref{fig:proposed_framework}, our framework consists of a feature encoder and an output layer with 4 sub-networks.
It is general enough to accommodate a variety of existing RE models to improve their accuracy.
We can apply our framework to an existing RE model by using the feature encoder of the model and building the four sub-networks which exploit the structure of the original prediction network.
Since our framework uses the feature encoder of the existing models, we briefly describe only the output layer here.








Unlike the previous works, to allow the difference in the predictions for human annotated labels and distantly supervised labels, we exploit multi-task learning by employing two separate prediction networks \hanet and \dsnet to predict the labels in HA data and DS data, respectively.
We also use \hanet to extract relations from the test data.
The separation of the prediction networks prevents the accuracy degradation caused by incorrect labels from distant supervision.
If we simply utilize two prediction networks to apply the multi-task learning, \hanet cannot learn from distantly supervised labels although the prediction networks share the feature encoder.
To enable \hanet to learn from distantly supervised labels, we introduce an additional loss term called \emph{disagreement penalty}.
It models the disagreement between the outputs of \hanet and \dsnet by using maximum likelihood estimation with log-normal distributions.
Furthermore, to adaptively estimate the parameters of the log-normal distribution by considering contextual information, we exploit two parameter networks \munet and \sigmanet.



For a label \triple{\ehead}{r}{\etail}, 
let  be an indicator variable that is 1 if the label is obtained by human annotation and 0 otherwise.
The proposed framework uses the following loss function for a label \triple{\ehead}{r}{\etail}

where  and  denote the prediction loss of \hanet and \dsnet, respectively, and  is the disagreement penalty to capture the distance between the predictions by \hanet and \dsnet.
The hyper parameter  controls the relative importance of the disagreement penalty to the prediction errors.
By using a separate prediction network for each type of data and introducing the disagreement penalty, \hanet learns from distantly supervised labels while reducing overfitting to noisy DS data.














































\subsection{Separate Prediction Networks}
To alleviate the accuracy degradation from the noisy labels in DS data, we utilize two prediction networks.
The network \hanet is used to predict the human-annotated labels from the train data and to predict relations from the test data.
The other prediction network  \dsnet predicts the labels obtained by distant supervision.
We use the prediction network of an existing model for both prediction networks of our framework without sharing the model parameters.
The prediction networks \hanet and \dsnet output the -dimensional vectors   and  , respectively, where  and  are the probabilities that there exists a label \triple{\ehead}{r}{\etail}, in HA data and DS data, respectively. 
We simply denote  and  by  and , respectively.









\subsection{Disagreement Penalty}
Distant supervision labels are biased and the size of the bias varies depending on the type of relation.
Moreover, the bias can vary depending on many other features such as the types of head and tail entities as well as the contents of a text.
Thus, we propose to use an effective disagreement penalty to model the labeling bias depending on the context where the head and tail entities are located.


\begin{wraptable}{R}{0.3\textwidth}
	\caption{The result of K-S test}
	\label{tab:distributions}
	\small
	\begin{tabular}{c|c}
		\toprule
		\textbf{Distribution}  & \textbf{p-value} \\
		\midrule
		Log-normal&{}\\
		Weibull &  \\
		Chi-square &  \\
		Exponential &  \\
		Normal& \\  
		\bottomrule
	\end{tabular}
\end{wraptable}
\noindent\textbf{Distribution of inflations.}
We measure the labeling bias by using the inflations of relations.
Recall that the inflation of a relation type is the ratio of the average frequencies of the relation type per text in DS data and HA data, respectively.
To investigate the distribution of inflations, we computed the inflations of 96 relation types in DocRED data.
Since Kolmogorov-Smirnov (K-S) test \cite{massey1951kolmogorov} is widely used to determine whether an observed data is drawn from a given probability distribution, we used it to find the best-fit distribution of the inflations.
Since the range of the inflation is , we evaluated p-values of the four probability distributions supported on  : Log-normal, Weibull, chi-square and exponential distributions. \
In addition, we include the normal distribution as a baseline.
Table~\ref{tab:distributions} shows the result of K-S test for DocRED data.
Note that a probability distribution has a high p-value if the probability distribution fits the data well.
Since the log-normal distribution has the highest p-value, it is the best-fit distribution among the five probability distributions.
Based on the observation, we model the disagreement penalty between the outputs of the two prediction networks. 



\minisection{Modeling the disagreement penalty}
We develop the disagreement penalty based on the maximum likelihood estimation.
Let  be the random variable which denotes the ratio of  to . 
Since the inflation is the ratio of the number of labels in DS data and HA data,
the ratio  represents the \emph{conditional inflation} of the relation type  conditioned on the text with head and tail entities.
Thus, we assume that  follows a log-normal distribution  whose probability density function is





The disagreement penalty  is defined as the negative log likelihood of the conditional inflation , which is obtained by substituting  into Equation~\eqref{eq:lognormal} as follows:

Since  is constant, we utilize the disagreement penalty in Equation~\eqref{eq:disagreement} without the constant term.


If we set  and  to fixed values, we cannot effectively assess the conditional inflation since it can vary depending on the context.
For example, although the inflation of the relation type \relationt{capital} is high, the conditional inflation should be lower if a particular phrase such as `is the capital city of' appears in the text.
To take account of the contextual information,
we employ two additional networks \munet and \sigmanet to estimate the  and  that are the parameters of log-normal distribution .



\subsection{Parameter Networks}
The parameter networks \munet and \sigmanet output the vectors  and , respectively, which are
the parameters of the log-normal distributions to represent the conditional inflation for .
Both \munet and \sigmanet have the same structure as those of the prediction networks except their output activation functions.
For a log-normal distribution , the parameter  can be positive or negative, and  is always positive.
Thus, we use a hyperbolic tangent function and a softplus function \cite{dugas2001incorporating} as the output activation functions of \munet and \sigmanet, respectively.




For example, if the prediction network of the original RE model consists of a bilinear layer and an output activation function,
the parameter vectors  and  are computed from the head entity vector  and tail entity vector  as

where  and  is a sanity bound preventing extremely small values of  from dominating the loss function, and , ,  and  are learnable parameters.
We set the sanity bound  to 0.0001 in our experiment.



\subsection{Loss Function}
For sentence-level relation extraction, we use the categorical cross entropy loss as the prediction losses  and .
For a label \triple{\ehead}{r}{\etail}, we obtain the following loss function from Equations \eqref{eq:loss_full} and \eqref{eq:disagreement}

where  , and  is 1 if the label is from HA data and 0 otherwise.






























\subsection{Analysis of the Disagreement Penalty}
\label{sec:analysis}
Let  be a learnable parameter of \hanet which predicts relations in the test time.
We investigate the effect of the disagreement penalty by comparing the gradients of loss functions with respect to  for a human annotated label and a distantly supervised label.


For a label \triple{\ehead}{r}{\etail}, let .
If the label is human annotated, we obtain the following
gradient of the loss  with respect to  from Equation \eqref{eq:loss_sent}
\vspace{-0.03in}

 


On the other hand, if the label is annotated by distant supervision,
the gradient becomes
\vspace{-0.03in}



The two gradients in Equations~\eqref{eq:gradha} and \eqref{eq:gradds} have the same direction of .
It implies that a human annotated label and a distantly supervised label have similar effects on training \hanet
except that the magnitudes of gradients are calibrated by  and , respectively.
Thus, \hanet can learn from not only  human annotated labels but also distantly supervised labels by introducing the disagreement penalty.
Recall that the log-normal distribution  describes the conditional inflation for a given sentence with a head entity and a tail entity.
If the median  of  has a high value, the distantly supervised label is likely to be a false label. 
Thus, we decrease the size of  to reduce the effect of a distantly supervised label.
On the other hand, as the median  becomes lower, the size of  increases to aggressively utilize the distantly supervised label.











\subsection{Extension to Document-level Relation Extraction}
For the document-level RE, we use the \emph{binary} cross entropy as the prediction losses  and .
For a pair of entities \head and \tail, let  be the set of relation types between the entities.
In the train time, we use the following loss function for document relation extraction
\vspace{-0.1in}

where  , and  is 1 if the labels are from HA data and 0 otherwise.
We obtain the same property shown in Section~\ref{sec:analysis} for the above loss function.
In the test time, we regard that the model outputs the triple  if  is greater than a threshold which is tuned on the development dataset. 
 

\begin{wrapfigure}{R}{0.45\textwidth}
\includegraphics[width=0.43\textwidth]{entity_encoder}
\caption{Cross-attention entity encoder \label{fig:entity_encoder}}
\end{wrapfigure}


\section{Cross-Attention Entity Encoder for Document-level RE}
The structure of our cross-attention entity encoder is shown in \figurename~\ref{fig:entity_encoder}.
Our entity encoder takes the word vectors s produced by the text encoder as input.
It obtains the intermediate representations  and  of the head and tail entities.
Note that the entity encoders in \cite{yao2019docred,wang2019fine} output these vectors.
Instead, we generate the head entity vector  and tail entity vector  based on the intermediate representation by using two attention networks.
We call both networks the cross-attention entity encoder.



Let  denote the word vector of the -th word in a given document produced by the text encoder.
For each entity mention  ranging from the -th word to the -th word in the document, let us denote its representation  by . Then, the intermediate representation of an entity (i.e.,  and ) is defined as the average of the representations of its entity mentions.
Specifically, we have

where  and  are the mentions of the head and tail entities, respectively.
Note that the intermediate representation of the head entity  is independently computed from the tail entity.
Thus, we always get the same intermediate representation of the head entity regardless of the tail entity although the text encoder has enough model capacity.
Since previous works \cite{yao2019docred,wang2019fine} use this representation in the prediction network, they use the same representation for an entity in a document.
However,each mention of an entity can be involved in a different relationship with a different entity.
Thus, it is desirable to adaptively generate the head entity vector based on the tail entity, and vice versa.





Motivated by the observation, we apply the attention mechanism \cite{bahdanau2015neural} to encode an entity by considering the other entity.
By using the intermediate representation of the tail entity , we first compute a -dimensional attention vector, denoted by , over the words in the head entity mentions where  is the set of word indexes of the head entity mentions.
The attention vector is computed by

where , ,  and  are the learnable parameters of the attention network.
Then, we take the weighted sum of the word embeddings over the words in the head entity mentions to compute the head entity vector .
That is, we have




The tail entity vector   and its  attention vector  are similarly computed as















\iffalse
\subsection{A common structure of the modules}

\begin{figure}[tb]
	\includegraphics[width=2.2in]{common_module}
	\caption{A common structure of the modules of our proposed model \label{fig:common_module}}
\end{figure}
\fi


 \section{Experiments}
We conducted a performance study for sentence-level and document-level REs by following the experimental settings of \cite{ye2019looking} and \cite{yao2019docred,wang2019fine}, respectively.
All models are implemented in PyTorch and trained on a V100 GPU.
We initialized \hanet and \dsnet to have the same initial parameters.
More experimental details including implementations can be found in Appendix~\ref{sec:impl}.










  
 \begin{wraptable}{r}{0.54\textwidth}
\vspace{-0.2in}
\caption{Statistics of datasets}
 	\label{tab:dataset}
 	\footnotesize
 	\begin{tabular}{c|rrrr|r}
 		\toprule
\multirow{2}{*}{Data} & \multicolumn{4}{c|}{Number of instances} &\multirow{2}{*}{\makecell[c]{\# of rel.\\types}}\\
 		& Train-HA & Train-DS & Dev & Test \\
 		\midrule
 		\midrule
KBP  & 378 & 132,369 &14,103 &1,488& 7 \\  
 		NYT & 756 & 323,126 & 34,871 &3,021& 25 \\ 
 		\midrule
DocRED	& 38,269 & 1,508,320 & 12,332 &12,842& 96 \\ 
 		\bottomrule
 	\end{tabular}
\end{wraptable}
\subsection{Experimental Settings}
\textbf{Dataset.}
KBP \cite{ling2012fine,ellis2012linguistic} and NYT \cite{riedel2010modeling,hoffmann2011knowledge} are datasets for sentence-level RE, and DocRED \cite{yao2019docred} is a dataset for document-level RE.
The statistics of the datasets are summarized in Table~\ref{tab:dataset}.
Since KBP and NYT do not have HA train data, 
we use 20\% of the HA test data as the HA train data.
In addition, we randomly split 10\% of train data on KBP and NYT for the development (dev) data.
Note that the ground truth of the test data in DocRED is not publicly available.
However, we can get the F1 score of the result extracted from the test data by submitting the result to the DocRED competition hosted by CodaLab (available at https://competitions.codalab.org/competitions/20717).
We report both the F1 scores computed from the dev data and the test data.



















\minisection{Compared methods}
We compare our dual supervision framework, denoted by \firstdef{\dual}, with the state-of-the-art methods \textbf{\baset} and \textbf{\bafix} in \cite{ye2019looking}.
For sentence-level RE, we compare \dual with two additional baselines \textbf{\maxth} \cite{ren2017cotype} and \textbf{\entth} \cite{liu2017heterogeneous} which are only applicable to multi-class classification and cannot be used in document-level RE.
\maxth outputs \norel if the maximum output probability is less than a threshold. Similarly, \entth outputs \norel if the entropy of the output probability distribution is greater than a threshold. 




\minisection{Used relation extraction models} 
For \emph{sentence-level RE}, we used the six models: \textbf{\sbigru}  \cite{zhang2017position}, \textbf{\spalstm} \cite{zhang2017position}, \textbf{\sbilstm} \cite{zhang2017position}, \textbf{\scnn} \cite{zeng2014relation}, \textbf{\spcnn} \cite{zeng2015distant} and \textbf{\sbert} \cite{wang2019fine}.
On the other hand, for \emph{document-level RE}, we used the five models:
\textbf{\dbert} \cite{wang2019fine}, \textbf{\dcnn} \cite{zeng2014relation}, \textbf{\dlstm} \cite{yao2019docred}, \textbf{\dbilstm} \cite{cai2016bidirectional} and \textbf{\dca} \cite{sorokin2017context}.
Note that \dcnn, \dbilstm, and \dca are originally proposed for sentence-level RE 
and we used the adaptation of them to document-level RE by \newcite{yao2019docred}.
In addition, we adapt \dbert to the sentence-level RE by changing the output activation function from sigmoid to softmax and denote it by \sbert.

 
















\iffalse
\red{
Previous works also reported F1-Ign and AUC-Ign to evaluate the models to ignore the relational facts in train data.\\
Option 1:\\
However, we do not report the scores in detail since the scores are not properly defined to ignore the effect of the train data.
We will discuss the problem in the supplementary materials.\\
Option 2:\\
Due to the lack of space, we will report the result in the supplementary materials.
\\
Instead, we simply report here that our proposed model also increased the F1-Ign and AUC-Ign scores at least 11.7\% and 16.0\%, respectively, compared to the existing models.}
\fi


 









\begin{table}[tb]
	\center
	\caption{Sentence-level RE datasets (KBP and NYT)\label{tab:performance_sentence}}
	\scriptsize
	\begin{tabular}{c|rrrrrr|rrrrrr}
\toprule
Dataset  &  \multicolumn{6}{c|}{KBP}  &  \multicolumn{6}{c}{NYT}\\
RE models & \sbigru & \spalstm & \sbilstm & \spcnn & \scnn & \sbert & \sbigru & \spalstm & \sbilstm & \spcnn & \scnn & \sbert\\
\midrule
\midrule
\haonly & 0.1984 & 0.1153 & 0.1787 & 0.3410 & 0.2586 & 0.1631 & 0.0884 & 0.1259 & 0.1504 & 0.4463 & 0.3978 & 0.1953\\
\dsonly & 0.3909 & 0.3521 & 0.3519 & 0.2705 & 0.2810 & 0.3610 & 0.4532 & 0.4429 & 0.4297 & 0.4177 & 0.4463 & 0.4625\\
\midrule
\baset & 0.3972 & 0.4055 & 0.4053 & 0.2410 & 0.2400 & 0.3858 & 0.4966 & 0.4555 & 0.4561 & 0.3584 & 0.4358 & 0.5081\\
\bafix & 0.4241 & 0.4027 & 0.3581 & 0.2931 & 0.2473 & 0.3383 & 0.4613 & 0.4507 & \textbf{0.4707} & 0.4023 & 0.4532 & 0.5145\\
\maxth & 0.4264 & 0.3630 & 0.4053 & 0.2815 & 0.2645 & 0.3751 & 0.4531 & 0.4462 & 0.4350 & 0.4258 & 0.4655 & 0.4952\\
\entth & 0.4470 & 0.4018 & \textbf{0.4248} & 0.2925 & 0.2826 & 0.3539 & 0.4553 & 0.4472 & 0.4210 & 0.4154 & 0.4427 & 0.4940\\
\midrule
\dual & \textbf{0.4749} & \textbf{0.4420} & 0.4207 & \textbf{0.3872}  & \textbf{0.2969} & \textbf{0.4013}& \textbf{0.5455} & \textbf{0.5210} & 0.4524 & \textbf{0.4986} & \textbf{0.4744}& \textbf{0.5300}\\
\bottomrule
	\end{tabular}
\end{table}





\subsection{Comparison with Existing Methods}
We compare the dual supervision framework with the existing methods. 

\minisection{Sentence-level RE}
\tablename~\ref{tab:performance_sentence} shows F1 scores for relation extraction on KBP and NYT.
Note that \dsonly and \haonly represent the original RE models trained only on distantly supervised and human-annotated labels, respectively.
\dual shows the highest F1 scores with all RE models except \sbilstm.
Since KBP and NYT have a small number of human-annotated labels in train data, \haonly shows worse F1 scores than \dsonly.
Furthermore, \dual achieves improvements of F1 score from 5\% to 40\% over \dsonly by additionally using the small amount of human annotated labels.
On the other hand, the compared methods \bafix, \baset, \maxth and \entth often perform worse than \dsonly and \haonly.





\begin{table}[tb]
	\center
	\caption{Document-level RE dataset (DocRED) \label{tab:performance_doc}}
	\scriptsize
	\begin{tabular}{c|ccccc|ccccc}
\toprule
   &   \multicolumn{5}{c|}{Dev}   &   \multicolumn{5}{c}{Test}  \\
\midrule
RE models  &   \dbert  &   \dbilstm  &   \dca  &   \dlstm  &   \dcnn  &   \dbert  &   \dbilstm  &   \dca  &   \dlstm  &   \dcnn\\
\midrule
\midrule
\haonly&0.5513&0.4992&0.4986&0.4817&0.4788&0.5478&0.4982&0.4992&0.4815&0.4681\\
\dsonly&0.4683&0.4951&0.4890&0.4877&0.4166&0.4587&0.4809&0.4772&0.4713&0.4160\\
\midrule
\baset&0.4807&0.5123&0.5024&0.5012&0.4349&0.4716&0.4949&0.4905&0.4905&0.4320\\
\bafix&0.4802&0.5136&0.5070&0.5166&0.4365&0.4730&0.5061&0.4989&0.4977&0.4354\\
\midrule
\dual&\textbf{0.5880}&\textbf{0.5510}&\textbf{0.5372}&\textbf{0.5392}&\textbf{0.4967}&\textbf{0.5774}&\textbf{0.5379}&\textbf{0.5306}&\textbf{0.5277}&\textbf{0.4909}\\
\bottomrule
	\end{tabular}
\end{table}
\minisection{Document-level RE}
We present F1 scores on DocRED in \tablename~\ref{tab:performance_doc}.
\dual outperforms \baset and \bafix with all RE models.
Especially, the F1 score of dual framework with \dbert shows more than 22\% of improvement over \baset and \bafix.
Since DocRED has a large human-annotated train data, \haonly shows better performance than \dsonly.
For \dbert and \dcnn, the existing methods show lower F1 scores compared to \haonly.
It shows that the accuracy can be degraded although we use additional DA data in addition to HA data due to the labeling bias.
Meanwhile, we achieve a consistent and significant improvement by applying \dual.
In the rest of this paper, we will provide a detailed evaluation of performance on DocRED data which is the largest dataset in this experiment.
For the test data of DocRED, the ground truth is not publicly available and only a F1 score can be obtained from the DocRED competition.
Thus, we provide detailed evaluations of performance on the dev data only.

\begin{figure*}[tb]
	\hspace{-0.14in}
	\subfloat[\dbert]{\includegraphics[width=3.2in]{exp/f1byinf_BERT_line_lgdTrue_g4_dsTrue}\label{fig:acc_by_inf_bert}}
	\hspace{-0.16in}
	\subfloat[\dbilstm]{\includegraphics[width=3.2in]{exp/f1byinf_BiLSTM_line_lgdTrue_g4_dsTrue}\label{fig:acc_by_inf_bilstm}}
	\vspace{-0.03in}
	\caption{F1 scores of different groups \label{fig:acc_varying_inf}}
	\vspace{-0.08in}
\end{figure*}












\minisection{Inflation vs. accuracy}
To investigate the effect of the inflation to the accuracy of relation extraction,
we split the relation types into 4 groups based on the inflation of the relation types.
In \figurename~\ref{fig:acc_varying_inf}, we present the characteristics of each group and plot the F1 scores by groups for \dbert model and \dbilstm model.
All methods have the highest F1 scores when the inflation is close to 1 (at the 2nd group).
Furthermore, the improvement of F1 score by \dual compared to the second best performer increases as the inflation moves away from 1.
Thus, it confirms that our dual supervision framework effectively utilizes both human annotation and distant supervision by modeling the bias of the distant supervision.
Since the other models \dca, \dlstm and \dcnn show similar results with \dbilstm, we omit the result.










\begin{wrapfigure}{R}{0.4\textwidth}
\vspace{-0.16in}
	\includegraphics[width=1\textwidth]{exp/varying_had_BERT_dsFalse_loFalse}
\vspace{-0.13in}
	\caption{Varying the size of HA data \label{fig:acc_by_had}}
	\vspace{-0.13in}
\end{wrapfigure}
\subsection{Ablation Study}
We conducted an ablation study with the existing model \dbert on DocRED to validate the effectiveness of individual components of our framework.
We compared \dual (separate prediction networks + disagreement penalty) and two variations of our framework \multitask (separate prediction networks only) and \single.
\multitask denotes a variation of \dual which does not utilize the disagreement penalty,
while  \dbert without applying the dual supervision framework  is referred to as \single.
Note that \single is also trained on both HA data and DS data together.



To show the effectiveness of the components depending on the size of HA data, we plotted the F1 scores with varying the number of human-annotated document from 152 to 3,053 (i.e., from 5\% to 100\% of the documents with HA).
As we expected, \dual outperforms both variations all the time.
Furthermore, separation of the prediction networks significantly improves the accuracy when we have enough number of human-annotated labels.
However, when we use less than 10\% of the human annotated documents, \multitask suffers from the sparsity problem.
By utilizing the disagreement penalty additionally, \dual outperforms \single even when we use only 5\% of the human-annotated documents for training the model.
It implies that the disagreement penalty enables \hanet to effectively learn from DS data as well as HA data.














\subsection{Quality Comparison}
To give an idea of what false relations are found by existing methods,
we provide two example documents in the dev data of DocRED and the relations extracted by \dual, \bafix and \dsonly with \dbert in \tablename~\ref{tab:casestudy}.
The relation \triple{Sweden}{\relationt{capital}}{Stockholm} is expressed in the document titled `Kungliga Hovkapellet' and
all methods find the relation correctly.
In the document titled `Loopline Bride', the relation \triple{Ireland}{\relationt{capital}}{Dublin} does not exist.
However, \bafix and \dsonly output the incorrect relation.
Since \dual adaptively assess the labeling bias with \munet and \sigmanet, \dual does not output the false relation.
In addition, since the RE models trained with \bafix and \dsonly fail to learn the text pattern corresponding to the relation type due to the labeling bias, they output many false labels such as \triple{Vietnam}{\relationt{capital}}{Taipei} in many documents.
It shows that the dual supervision framework effectively deal with the labeling bias of distant supervision by considering contextual information.

\begin{table}[tb]
	\setlength\extrarowheight{2pt}
\small
	\caption{Examples of documents and extracted relations \label{tab:casestudy}}
	\begin{tabularx}{\textwidth}{c|X|X}
		\toprule
\multirow{1}{*}{Document}  & Title: Kungliga Hovkapellet  & Title: Loopline Bridge\\
& 
		[1] Kungliga Hovkapellet is a Swedish orchestra, originally part of the Royal Court in \entity{[Sweden]}'s capital \entity{[Stockholm]}.
		[2] Its existence ...
		&
		[1] The Loopline Bridge (or the Liffey Viaduct) is a railway bridge spanning the River Liffey and several streets in \entity{[Dublin]}, \entity{[Ireland]}.
		[2] It joins ...\\
\midrule
\multirow{1}{*}{Relations} 
		& True label: \triple{\entity{Sweden}}{\relation{capital}}{\entity{Stockholm}}&  True label:\norel \\
		& \dual: \triple{\entity{Sweden}}{\relation{capital}}{\entity{Stockholm}}&  \dual:\norel \\
		& \bafix: \triple{\entity{Sweden}}{\relation{capital}}{\entity{Stockholm}}& \bafix: \triple{\entity{Ireland}}{\relation{capital}}{\entity{Dublin}} \\
		& \dsonly: \triple{\entity{Sweden}}{\relation{capital}}{\entity{Stockholm}}&  \dsonly:\triple{\entity{Ireland}}{\relation{capital}}{\entity{Dublin}} \\
\bottomrule
	\end{tabularx}
	\vspace{-0.04in}
\end{table}











\begin{wraptable}{r}{0.29\textwidth}
\vspace{-0.2in}
\caption{Topic-aware RE}
	\label{tab:topic_dual}
	\small
\begin{tabular}{c|cc}
		\toprule
		& F1 & AUC	\\	
		\midrule
		\haonly & 0.6569 & 0.6456\\
		\dsonly & 0.6624 & 0.6978\\
		\dual & \textbf{0.6930}& \textbf{0.7125}\\
		\bottomrule
	\end{tabular}
\end{wraptable}


\subsection{Topic-aware RE}
Topic-aware RE is a special case of document-level RE to extract the relations between the topic entity of a document and the other entities.
\newcite{jung2020trex} proposed a topic-aware relation
extraction (T-REX) model which is robust to the omitted mentions
of topic entities in documents.
We apply our dual supervision framework to the T-REX on DocRED dataset and report the result in \tablename~\ref{tab:topic_dual}.
The result shows that our dual supervision framework is also effective in the topic-aware RE task.


 \section{Related Works}
We briefly survey the existing works for RE.
\newcite{mintz2009distant} propose distant supervision to overcome the limitation of the quantity of human-annotated labels.
They utilize lexical, syntactic and named entity tag features obtained by existing NLP tools to extract relations.
Other early works in \cite{riedel2010modeling,hoffmann2011knowledge} also utilized hand-crafted features to find the relations in text.
However, since such RE models take the input features from NLP tools, the errors generated by the NLP tools are propagated to the RE models.
In order to deal with the error propagation, as we discussed in Section~\ref{sec:existingworks}, the works \cite{lin2016neural,zeng2014relation,zeng2015distant,sorokin2017context,wang2019fine} use deep neural networks such as CNN, LSTM and BERT instead of handcrafted features to encode the text for finding the relations.
Since many relational facts are expressed across multiple sentences, the recent works \cite{yao2019docred,wang2019fine} studied document-level RE.
\newcite{yao2019docred} provide a document-level RE dataset (DocRED)  as well as compare the models adapted from the sentence-level RE models \cite{zeng2014relation,hochreiter1997long,cai2016bidirectional,sorokin2017context}.
Moreover, a fine tuned model \cite{wang2019fine} of BERT \cite{devlin2018bert} for document-level RE achieved a higher F1 score than the baselines on DocRED.




The wrong labeling problem in distant supervision has been addressed in many previous works \cite{zeng2015distant,lin2016neural,ye2019distant,beltagy2018combining}.
Among them, \newcite{zeng2015distant}, \newcite{lin2016neural} and \newcite{ye2019distant} 
build a bag-of-sentences for a pair of entities and extract relational facts from the bag-of-sentences with attention over the sentences.
\newcite{beltagy2018combining} propose a bag-of-sentences-level model which utilizes human annotation.
However, they use the human annotated labels only to determine whether there exists a relationship or not since the labels are obtained from a different domain.
The goal of these works is different from ours which is to find the relations \emph{appearing in a given text} (e.g., a document). 
Thus, the bag-of-sentences-level models have a limitation to be used for some applications such as question answering.





The most relevant work to ours is \cite{ye2019looking}.
This paper proposes the bias adjustment methods to utilize a small amount of HA data to improve RE models trained on DS data 
by considering the different distribution of human annotated labels and distantly supervised labels.
However, they do not use HA data to train the models and use the HA data only to obtain a statistic to be used the determine the size of the bias adjustment.
Thus, the bias adjustment methods cannot consider contextual information.





 \section{Conclusion}
We proposed the dual supervision framework to utilize human annotation and distant supervision based on the analysis of labeling bias in distant supervision.
We devised a new structure for the output layer of RE models that consists of 4 sub networks.
The new structure is robust to the noisy labeling of distant supervision since the labels obtained by human annotation and distant supervision are predicted by separate prediction networks \hanet and \dsnet, respectively.
In addition, we introduced an additional loss term called \emph{disagreement penalty} which enables \hanet  to learn from distantly supervised labels.
The parameter networks \munet and \sigmanet adaptively assess the labeling bias by considering contextual information.
Moreover, we theoretically analyzed the effect of the disagreement penalty.
Our experiments showed that the dual supervision framework significantly outperforms the existing methods.
 

\bibliographystyle{coling}
\bibliography{ref}



















\end{document}
