\vspace{-0.05in}
\section{Experiment -- Analysis and Extension}
\label{sec:exp_analysis}
\vspace{-0.05in}

In this section, we first analyze the effectiveness of each component of the proposed method. We also study that whether the proposed method is complimentary to multi-task learning. We further extend SMART to domain adaptation and use both SNLI \citep{snli2015} and SciTail \citep{scitail} to evaluate the effectiveness. Finally, we verified the robustness of the proposed method on ANLI \citep{nie2019adversarial}. 

\subsection{Ablation Study}
\label{subsec:abl}


Note that due to the limitation of time and computational resources, all the experiments reported below are based on the \textbf{BERT\textsubscript{BASE}} model.
In this section, we study the importance of each component of {\model}: smoothness-inducing adversarial regularization and Bregman proximal point optimization. All models in this study used the BERT\textsubscript{BASE} as the encoder for fast training. Furthermore, we also include the BERT\textsubscript{BASE} model as an additional baseline for a fair comparison. {\model} denotes the proposed model. Then we set  to 0, which denotes as -. The model with  is noted as -.

\begin{table}[!htb]
\centering
    \begin{tabular}{l|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt} | c@{\hskip1pt}}
    \hline
   Model                        &MNLI   &RTE    & QNLI  &SST    &MRPC \\ 
                                &Acc    &Acc    &Acc    &Acc    &Acc  \\\hline \hline 
      BERT                      &84.5   &63.5   &91.1   &92.9   &89.0 \\ \hline
     {\model}                   &\textbf{85.6}  &\textbf{71.2}  &\textbf{91.7}   &\textbf{93.0}   &\textbf{91.3} \\ \hline
     -             &84.8   &70.8   &91.3   &92.8   &90.8\\ \hline     
 -                &85.4   &\textbf{71.2}   &91.6   &92.9   &91.2\\ \hline     
    \end{tabular}
\caption{Ablation study of {\model} on 5 GLUE tasks. Note that all models used the BERT\textsubscript{BASE} model as their encoder.}
    \label{tab:smart_abl}
\end{table}

The results are reported in Table~\ref{tab:smart_abl}. It is expected that the removal of either component (smooth regularization or proximal point method) in {\model} would result in a performance drop. For example, on MNLI, removing smooth regularization leads to a 0.8\% (85.6\% vs. 84.8) performance drop, while removing the Breg proximal point optimization, results in a performance drop of 0.2\% (85.6\% vs. 85.4\%). It demonstrates that these two components complement each other. Interestingly, all three proposed models outperform the BERT baseline model demonstrating the effectiveness of each module. Moreover, we obersere that the generalization performance benefits more from {\model}  on small datasets (i.e., RTE and MRPC) by preventing overfitting. 





















\subsection{Error Analysis}
\label{subsec:error}
To understand why {\model} improves the performance, we analyze it on the ambiguous samples of MNLI dev set containing 3 classes, where each sample has 5 annotations. Based on the degree of agreement between these annotations, we divide the samples into 4 categories: 
1) \textbf{5/0/0} all five annotations are the same; 2) \textbf{4/1/0} four annotations are the same; 3) \textbf{3/2/0} three annotations are the same and the other two annotations are the same; 4) \textbf{3/1/1} three annotations are the same and the other two annotations are different.

Figure~\ref{fig:mnli_ambi} summarizes the results in terms of both accuracy and KL-divergence:  For a given sample , the KL-Divergence evaluates the similarity between the model prediction  and the annotation distribution . We observe that {\model}\textsubscript{RoBERTa} outperforms RoBERTa across all the settings. Further, on high degree of ambiguity (low degree of agreement), {\model}\textsubscript{RoBERTa} obtains an even larger improvement showing its robustness to ambiguity.  
 



\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\linewidth]{ambiguity_exp/barplot.pdf}
\caption{Score breakdown by degree of agreement.}
    \label{fig:mnli_ambi}
\end{figure}


\subsection{{\model} with Multi-task Learning}
\label{subsec:smart-mtl}


It has been shown that multi-task learning (MTL, \citet{caruana1997multitask,liu2015mtl,liu2019mt-dnn}) has a regularization effect via alleviating overfitting to a specific task. One question is whether MTL helps {\model} as well. In this section, we are going to answer this question. Following \citet{liu2019mt-dnn}, we first ``pre-trained" shared embeddings using MTL with {\model}, denoted as \textbf{MT-DNN-{\model}} \footnote{Due to limitation of computational resources, we only trained jointly using MTL on MNLI, RTE, QNLI, SST and MRPC, while MT-DNN was trained on the whole GLUE tasks except CoLA. 
}, and then adapted the training data on each task on top of the shared embeddings. We also include a baseline which fine-tuned each task on the publicly released MT-DNN checkpoint \footnote{It is from: https://github.com/namisan/mt-dnn. Note that we did not use the complicated answer module, e.g., SAN \citep{liu2018san4nli}.}, which is indicated as \textbf{MT-DNN-{\model}\textsubscript{v0}}.

\begin{table}[!htb]
\centering
    \begin{tabular}{@{\hskip1pt}l@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt}|@{\hskip1pt}c@{\hskip1pt} |@{\hskip1pt} c@{\hskip1pt}}
    \hline
   Model                        &MNLI   &RTE    & QNLI  &SST    &MRPC \\ 
                                &Acc    &Acc    &Acc    &Acc    &F1  \\\hline  \hline
     BERT                       &84.5   &63.5   &91.1   &92.9   &89.0 \\ \hline
     MT-DNN                     &85.3   &79.1   &91.5   &\textbf{93.6}   &89.2 \\ \hline 
     {\model}                   &85.6   &71.2   &91.6   &93.0   &91.3 \\ \hline
     MT-DNN-{\model}\textsubscript{v0}            &\textbf{85.7}   &80.2   &\textbf{92.0}   &93.3   &91.5 \\ \hline
     MT-DNN-{\model}            &\textbf{85.7}       &\textbf{81.2}   &\textbf{92.0}     &93.5       &\textbf{91.7}  \\ \hline
    \end{tabular}
\caption{Comparison between {\model} and MTL. }
    \label{tab:smart_mtl}
\end{table}

We observe that both MT-DNN and {\model} consistently outperform the BERT model on all five GLUE tasks. Furthermore, {\model} outperforms MT-DNN on MNLI, QNLI, and MRPC, while it obtains worse results on RTE and SST, showing that MT-DNN is a strong counterpart for {\model}. By combining these two models, MT-DNN-{\model}\textsubscript{v0} enjoys advantages of both and thus improved the final results. For example, it achieves 85.7\% (+0.1\%) on MNLI and 80.2\% (+1.1\%) on RTE comparing with the best results of MT-DNN and {\model} demonstrating that these two techniques are orthogonal. Lastly we also trained {\model} jointly and then finetuned on each task like \citet{liu2019mt-dnn}. We observe that MT-DNN-{\model} outperformes MT-DNN-{\model}\textsubscript{v0} and MT-DNN across all 5 tasks (except MT-DNN on SST) showing that {\model} improves the generalization of MTL.

\subsection{Domain Adaptation}
\label{subsec:domain}


In this section, we evaluate our model on the domain adaptation setting. Following \citet{liu2019mt-dnn}, we start with the default training/dev/test set of SNLI and SciTail. Then, we randomly sample 0.1\%, 1\%, 10\% and 100\% of its training data, which is used to train a model. 

\begin{table}[tb!]
\begin{center}
		\begin{tabular}{@{\hskip1pt}l@{\hskip1pt} |@{\hskip1pt} c @{\hskip1pt}|@{\hskip1pt} c @{\hskip1pt}|@{\hskip1pt} c @{\hskip1pt}|@{\hskip1pt} c@{\hskip1pt}}
			\hline \bf Model & 0.1\% & 1\% &10\% & 100\% \\ \hline
            \multicolumn{5}{c}{ SNLI Dataset (Dev Accuracy\%)} \\ \hline
            \#Training Data &549& 5,493& 54,936&549,367 \\ \hline
            BERT &52.5&78.1&86.7 & 91.0 \\ \hline
            MT-DNN &82.1 & 85.2 & 88.4 & 91.5 \\ \hline
            MT-DNN-{\model} &\textbf{82.7} & \textbf{86.0} & \textbf{88.7} &\textbf{91.6}  \\ \hline \hline

\multicolumn{5}{c}{ SciTail Dataset (Dev Accuracy\%)} \\ \hline
            \#Training Data &23& 235& 2,359& 23,596\\ \hline
            BERT &51.2&82.2&90.5 & 94.3 \\ \hline
            MT-DNN &81.9 & 88.3 & 91.1 & 95.8 \\ \hline			
            MT-DNN-{\model} &\textbf{82.3} &\textbf{88.6}  &\textbf{91.3} &\textbf{96.1}  \\ \hline
		\end{tabular}
	\end{center}
\caption{Domain adaptation on SNLI and SciTail. 
	}
	\label{tab:domain}
\end{table}









The results are reported in Table~\ref{tab:domain}. We observe that both MT-DNN and MT-DNN-{\model} significantly outperform the BERT baseline. Comparing with MT-DNN, MT-DNN-{\model} also achieves some improvements indicating the robustness of {\model}. Furthermore, MT-DNN-{\model} outperforms current state-of-the-art on the SNLI/SciTail test. 

\begin{table*}[htb!]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
         \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Dev} & \multicolumn{4}{c}{Test}  \\
         \cline{2-9}
          &  R1 & R2 & R3 & All & R1 & R2 & R3 & All  \\
          \hline 
 		\multicolumn{9}{c}{ MNLI + SNLI + ANLI + FEVER  }  \\ \hline
          BERT\textsubscript{LARGE}  \citep{nie2019adversarial}  &-&-&-& - &57.4&48.3&43.5& 49.3 \\
		\hline
		XLNet\textsubscript{LARGE} \citep{nie2019adversarial}  &-&-&-& -  &67.6&50.7&48.3& 55.1\\
		\hline
		RoBERTa\textsubscript{LARGE} \citep{nie2019adversarial}  &-&-&-& - &73.8&48.9&44.4& 53.7\\
		\hline 
		SMART\textsubscript{RoBERTa-LARGE}  & 74.5&50.9&47.6& \textbf{57.1} &72.4&49.8&50.3& \textbf{57.1} \\ \hline \hline
 		\multicolumn{9}{c}{ ANLI }  \\ 
 		\hline
 		RoBERTa\textsubscript{LARGE} \citep{nie2019adversarial} &-&-&-& -  & 71.3&43.3&43.0& 51.9 \\ 		\hline 
SMART\textsubscript{RoBERTa-LARGE}  &74.2&49.5&49.2& \textbf{57.1} &72.4&50.3&49.5& \textbf{56.9}\\
\hline
    \end{tabular}
    \caption{Experiment Result for Each Round of ANLI.}
    \label{tab:anli_full}
\end{table*}
\begin{table}[!htb]
\begin{center}
		\begin{tabular}{@{\hskip1pt}l @{\hskip1pt}|@{\hskip1pt} c @{\hskip1pt}|@{\hskip1pt} c@{\hskip1pt}}\hline
\bf Model &Dev& Test  \\ \hline 

		\multicolumn{3}{c}{ SNLI Dataset (Accuracy\%)}  \\ \hline 
BERT\textsubscript{BASE}&91.0 & 90.8 \\ \hline		
		BERT\textsubscript{BASE}+SRL\citep{bertdep2019} &- & 90.3 \\ \hline		
		MT-DNN\textsubscript{BASE} &91.4 & 91.1 \\ \hline
		{\model}\textsubscript{BERT-BASE} &91.4& 91.1 \\ \hline
		MT-DNN-{\model}\textsubscript{BASE}\textsubscript{v0} &91.7& 91.4 \\ \hline
		MT-DNN-{\model}\textsubscript{BASE} &91.7&91.5 \\ \hline
		\hline
		BERT\textsubscript{LARGE}+SRL\citep{bertdep2019} & -& 91.3 \\ \hline
		BERT\textsubscript{LARGE} &91.7&91.0\\ \hline
		{MT-DNN\textsubscript{LARGE}} &92.2& 91.6\\ \hline
		{MT-DNN-{\model}\textsubscript{LARGE}}\textsubscript{v0} &\textbf{92.6}&\textbf{91.7}\\ \hline

		\hline
		\multicolumn{3}{c}{ SciTail Dataset (Accuracy\%)}  \\ \hline 	
		GPT \citep{gpt22019} &- &88.3 \\ \hline
		BERT\textsubscript{BASE} &94.3 & 92.0 \\ \hline
		MT-DNN\textsubscript{BASE} &95.8 &94.1 \\ \hline
		{\model}\textsubscript{BERT-BASE} &94.8& 93.2 \\ \hline
		MT-DNN-{\model}\textsubscript{BASE}\textsubscript{v0} &96.0&94.0  \\ \hline  
MT-DNN-{\model}\textsubscript{BASE} &96.1& 94.2 \\ \hline  
		\hline
		BERT\textsubscript{LARGE} &95.7& 94.4\\ \hline
		{MT-DNN}\textsubscript{LARGE} &96.3& 95.0\\ \hline

		\hline
{\model}\textsubscript{BERT-LARGE}& 96.2& 94.7\\
		\hline
MT-DNN-{\model}\textsubscript{LARGE}\textsubscript{v0} &\textbf{96.6}& \textbf{95.2}\\ \hline
		\end{tabular}
	\end{center}
\vspace{-0.125in}
\caption{Results on the SNLI and SciTail dataset.}	
\label{tab:nli}
\end{table}

\subsection{Results on SNLI and SciTail}


In Table~\ref{tab:nli}, we compare our methods, using all in-domain training data, against several state-of-the-art models. We observe that {\model} obtains the same improvement on SNLI in the BERT setting. Combining {\model} with MT-DNN achieves a significant improvement, e.g., our BASE model even outperforms the BERT\textsubscript{LARGE} model. Similar observation is found on SciTail and in the BERT\textsubscript{LARGE} model setting. We see that incorporating {\model} into MT-DNN achieves new state-of-the-art results on both SNLI and SciTail, pushing benchmarks to 91.7\% on SNLI and 95.2\% on SciTail.  











\subsection{Robustness}


One important property of the machine learning model is its robustness to adversarial attack. We test our model on an adversarial natural language inference (ANLI) dataset \cite{nie2019adversarial}. 

We evaluate the performance of SMART on each subset (i.e., R1,R2,R3) of ANLI dev and test set. The results are presented in Table~\ref{tab:anli_full}. Table~\ref{tab:anli_full} shows the results of training on combined NLI data: ANLI \citep{nie2019adversarial} + MNLI \citep{mnli2018} + SNLI \cite{snli2015} + FEVER \citep{thorne2018fever} and training on only ANLI data. In the combined data setting, we obverse that 	{\model}\textsubscript{RoBERTa-LARGE} obtains the best performance compared with all the strong baselines, pushing benchmarks to 57.1\%. In case of the RoBERTa\textsubscript{LARGE} baseline,  {\model}\textsubscript{RoBERTa-LARGE} outperforms 3.4\% absolute improvement on dev and 7.4\% absolute improvement on test, indicating the robustness of {\model}. We obverse that in the ANLI-only setting, {\model}\textsubscript{RoBERTa-LARGE} outperforms the strong RoBERTa\textsubscript{LARGE} baseline with a large margin, +5.2\% (57.1\% vs. 51.9\%)