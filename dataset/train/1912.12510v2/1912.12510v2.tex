\documentclass{article}
\pdfoutput=1


\PassOptionsToPackage{numbers, compress,authoryear}{natbib}




\usepackage[final]{neurips_2019}
\usepackage[font=footnotesize]{caption}
\usepackage{tablefootnote}
\usepackage{pdflscape}
\usepackage{afterpage}

\usepackage{boldline}


\usepackage{subcaption}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}

\usepackage{enumitem}
\usepackage{multirow}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{pifont}
\usepackage{hyperref}       \usepackage{url}            \usepackage{graphicx}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsmath}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{linegoal}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
\newcommand{\todo}[1]{{\bf{\color{blue} TODO: #1}}}


\usepackage[authoryear]{natbib}
\bibliographystyle{abbrvnat}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\newcommand{\train}{\mathpzc{Train}}
\usepackage{makecell}
\newcommand{\layer}[1]{F_{#1}}
\newcommand{\Dc}{D_c}
\newcommand{\ReLU}{\textrm{ReLU}}
\newcommand{\Maxs}{\textrm{Maxs}}
\newcommand{\Mins}{\textrm{Mins}}

\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}

\setlength{\intextsep}{5pt plus 2pt minus 2pt}








\title{Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices}


\author{Chandramouli S Sastry\\
Vector Institute,
Toronto\\
Dalhousie University, Halifax\\
\And
Sageev Oore\\
Vector Institute,
Toronto\\
Dalhousie University, Halifax\\
}

\begin{document}
\maketitle

\begin{abstract}
  When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions. Detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and class predicted. We find that characterizing activity patterns by Gram matrices and identifying anomalies in gram matrix values can yield high OOD detection rates. We identify anomalies in the gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and does not require access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. The method is applicable across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far-from-distribution out-of-distribution examples, it generally performs better than or equal to state-of-the-art OOD detection methods (including those that do assume access to OOD examples).\footnote{The code for this work is available at \url{https://github.com/VectorInstitute/gram-ood-detection}}

\end{abstract}

\section{Introduction}


Even when deep neural networks (DNNs) achieve impressive accuracy on challenging tasks, they do not always visibly falter on misclassified examples: in those cases they can often make predictions that are both very confident and completely incorrect. Yet, predictive uncertainty is essential in real-world contexts tolerating minimal error margins such as autonomous vehicle control and medical, financial and legal fields.

In this work, we focus on flagging test examples that do not contain any of the classes modeled in the train distribution. Such examples are often referred to as being {\it out-of-distribution} (OOD), and while their existence has been well-known for some time, the challenges of identifying them and a baseline method to do so in a variety of tasks such as image classification, text classification, and speech recognition were presented by~\cite{hendrycks2016baseline}. Recently,~\cite{nalisnick2018do} identified a similar problem with generative models: they demonstrate that flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. They report similar findings across several other pairs of popular image datasets.

While we might expect neural networks to respond differently to OOD examples than to in-distribution (ID) examples, exactly where and how to find these differences in activity patterns is not at all clear. \citet{hendrycks2016baseline} and others \citep{old1,old2} showed that looking at the maximal softmax value is insufficient. In Section~\ref{s:related_work} we describe some other recent approaches to this problem. In this work, we find that characterizing activity patterns by feature correlations---computed with an extension of Gram matrices that we introduce---lets us quantify anomalies to allow state-of-the-art (SOTA) detection rates on OOD examples.


\paragraph{Intuition.} We identify out-of-distribution examples by jointly considering the class assigned at the output layer and the activity patterns in the intermediate layers. For example, if an image is predicted to be a dog, yet the intermediate activity patterns are somehow atypical of those seen by the network for other dog images during training, then that is a strong indicator of an OOD example. This effectively allows us to detect incongruence between {\it the prediction made by the network} and {\it the path by which it arrived at that prediction}. To describe the activation path we need to describe the intermediate feature representations, and as a proxy for describing those representations, we use the feature correlations in the Gram matrices at each layer for any given example.

\paragraph{Strengths.} Unlike those previous works that assume access to OOD examples and train an auxiliary classifier for identifying anomalous activity patterns, our method finds differences in activity patterns without requiring access to any OOD examples, and it works across architectures. We hope this will also help further our understanding of how neural networks respond differently to OOD examples {\it in general}, not just how a particular network responds to examples coming from a particular distribution.





\paragraph{Contributions.} This work includes the following contributions:
\begin{enumerate}
\item We extend Gram matrices to compute effective feature correlations.
\item Using the -order Gram matrices, we present a new technique for computing class-conditional anomalies in activity patterns.
\item We evaluate this technique on OOD detection, testing on
     \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
         \item competitive architectures: DenseNet, ResNet;
        \item benchmark OOD datasets including: CIFAR-10, CIFAR-100, SVHN, TinyImageNet, LSUN and iSUN.
         \end{itemize}
        Note that no adversarial or other re-training is required; {\it we can use pre-trained models}.
\item Zero-shot: crucially, {\em our method does not require access to OOD samples} for tuning hyperparameters or for training auxiliary models.
\item We report results which, for the challenging and important cases of far-from-distribution examples, are generally better than or equal to the state-of-the-art method for OOD detection  that does require access to OOD examples.


\end{enumerate}

\section{Related Work}
\label{s:related_work}

Previous work which aims to improve OOD detection can be roughly grouped by several themes:

\paragraph{Bayesian Neural Networks.}
A nice early Bayesian approach~\citep{gal2016dropout} estimates predictive uncertainty by using an ensemble of sub-networks instantiated by applying dropout at test time. As opposed to implicitly learning a distribution over the predictions by learning a distribution over the weights,~\cite{chen2019a} and~\cite{malinin2018predictive} explicitly parameterize a Dirichlet distribution over the output class distributions using DNNs in order to obtain a better estimate of predictive uncertainty; the main differences between these methods is that \cite{chen2019a} use ELBO, which only requires the in-distribution dataset for training whereas \cite{malinin2018predictive} use a contrastive loss which requires access to (optionally synthetic) OOD examples. 

\paragraph{Using any pre-trained softmax deep neural network with OOD examples.} \cite{lee2018simple}---to the best of our knowledge, the current SOTA technique by a significant margin---compute the Mahalanobis distance between the test sample's feature representations and the class-conditional gaussian distribution at each layer; they then represent each sample as a vector of the Mahalanobis distances, and finally train a logistic regression detector on these representations to identify OOD examples. Another technique in this category is ODIN~\citep{liang2017enhancing}: they use a mix of temperature scaling at the softmax layer and input perturbations to achieve better results. In fact, both \cite{lee2018simple} and \cite{liang2017enhancing} add small input perturbations to achieve better results; the former do so to increase the confidence score, while the latter do so to increase the softmax score. \cite{quintanilha2019detecting} achieve results comparable to that of \cite{lee2018simple} by training a logistic regression detector that looks at the means and standard deviations of various channels activations. Unlike the previous two techniques,~\cite{quintanilha2019detecting} achieves comparable results even without the use of input perturbations, which allows it to be applicable to non-continuous domains. Our work, too, does not involve input
perturbations.

Recently, \cite{abdelzad2019detecting} propose to detect OOD examples by training a one-class detector over the representations of an intermediate layer, \textit{chosen} for each OOD detection task.

All of these techniques depend on OOD examples for fine-tuning hyperparameters \citep{liang2017enhancing, abdelzad2019detecting} or for training auxiliary OOD classifiers (\cite{lee2018simple,quintanilha2019detecting}).
Furthermore, these classifiers neither transfer between one non-training distribution and another, nor do they transfer between networks, so separate classifiers must be trained for each (In-Distribution, OOD, Architecture) triplet. In many real-world applications, we may not be able to assume advance access to all possible OOD distributions. Motivated by this observation, our work does not require access to OOD samples.

\paragraph{Alternative Training Strategies.}  \cite{lee2018training} jointly train a classifier, a generator and an adversarial discriminator such that the classifier produces a more uniform distribution on the boundary examples generated by the generator; they use OOD examples to fine-tune hyperparameters. \cite{devries2018learning} train neural networks with a multi-task loss for jointly learning to classify and estimate confidence. \cite{shalev2018out} use multiple semantic dense representations as the target instead of sparse one-hot vectors and use a cosine-similarity based measure for detecting OODs. Building on the idea proposed by \cite{lee2018training},~\cite{hendrycks2018deep} propose an {\it Outlier Exposure} (OE) technique. They regularize a softmax classifier to predict uniform distribution on (any) OOD distribution and show the resulting model can identify examples from unseen OOD distributions; this differs significantly from previous works which used the same OOD distributions for both training and testing. Unlike other methods, they retain the architecture of the classifier and introduce just one additional hyperparameter---the regularization rate---and also demonstrate that their model is quite robust to the choice of OOD examples chosen for the regularization. However, while the OE method is able to generalize across different non-training distributions, it understandably does not achieve the rates of~\cite{lee2018simple}~on most cases. In the same vein, \cite{vernekar2019out} propose a strategy to generate boundary OOD examples to train a classifer with a reject option. Similarly, \cite{unsup} propose to train a two-head CNN on in-distribution data with different decision boundaries by encouraging a higher discrepancy in predictions on unlabeled OOD data.

\cite{geometric} show how self-supervised classifiers trained to predict geometrical transformations in the input image can be used for one-class OOD detection. Recently, ~\cite{hendrycks2019selfsupervised} make significant advances in detecting near-distribution outliers without having any knowledge of the exact out-of-distribution examples by using in-distribution examples in a self-supervised training setting.

\paragraph{Generative Models.}
\cite{ren2019likelihood} hypothesize that stylistic factors might impact the likelihood assignment and propose to detect OOD examples by computing a likelihood ratio which depends on the semantic factors that remain after the dominant stylistic factors are cancelled out. On the other hand, \cite{nalisnick2019detecting} argue that samples generated by a generative model reside in the typical set, which might not necessarily coincide with areas of high density. They demonstrate empirically that OOD examples can be identified by checking if an input resides in the typical set of the generative model. Unlike the standard experimental setting, they aim to identify distributional shift, which predicts if a batch of examples are OOD. Several other recent works~\citep{gen1,gen2,gen3,gen4} also aim to solve these problems.

\section{Extending Gram Matrices for Out-of-Distribution Detection}
\label{s:gram}

\paragraph{Overview} In light of the above considerations, we are interested in proposing a method that does not require access to any OOD examples, that does not introduce hyperparameters that need tuning, and that works across architectures. Gram matrices can be used to compute pairwise feature correlations, and are often used in DNNs to encode stylistic attributes like textures and patterns \citep{style}. We extend these matrices as will be described below, and then use them to compute class-conditional bounds of feature correlations at multiple layers of the network. Starting with a pre-trained network, we compute these bounds over only the training set, and then use them at test time to effectively discriminate between in-distribution samples and out-of-distribution samples. Unlike other SOTA algorithms, we do not need to ``look'' at any out-of-distribution samples to tune any parameters; the only tuning required is that of a normalizing factor, which we compute using a randomly-selected validation partition of the (in-distribution) test set.

\paragraph{Notation} If the considered deep convolutional network has  layers and the  layer has  channels, we consider feature co-occurrences between the  pairs of feature-maps. (Note that by ``layer'' we refer to any set of values obtained immediately after applying convolution or activation functions.)
We use the following notation:

\begin{tabular}{p{1.75cm}p{11cm}}
 & The feature map at the -th layer for input image ; when referring to an arbitrary image , we just write . It can be stored in a matrix of dimensions , where  is the number of channels at the -th layer and , the number of pixels per channel, is the height times the width of the feature map.\\
/ & The predicted class for input image \\
 & The set of all train examples\\
 & The set of all validation examples. 10\% of the examples not used in training are randomly chosen as validation examples.  \\
 & The set of all test examples, disjoint as usual from the training and validation sets. We assume that only the test set may contain out-of-distribution examples.\\
\end{tabular}

\paragraph{Gram Matrices and Higher order Gram Matrices} We compute pairwise feature correlations between channels of the -th layer using the Gram matrix:

where  is an  matrix as defined above.

In order to compute feature correlations with more prominent activations of the feature maps, we define a \textit{higher-order gram matrix}, which we write , to be a matrix computed identically to the regular Gram matrix, but where, instead of using a raw channel activation , we use , the  power of each activation.  is therefore computed using , where the power of  is computed element-wise; in an effort to retain uniform scale across all orders of Gram matrices for a given layer, we compute the (element-wise) -th root. The -th order gram matrix is thus computed as:


We show in Section~\ref{s:discussion} that higher  values help significantly in improving the OOD detectability. In our experiments, we limit the value of  to 10, as exponents beyond 10 are not worth the extra computation that is needed to avoid overflow errors\footnote{The maximum activation values observed in the convolution layers of a ResNet trained on Cifar-10 (open-sourced by \cite{lee2018simple}) are 6.5 and 6.3 on train and test partitions.}.

The flattened upper (or lower) triangular matrix along with the diagonal entries is denoted as . The set of all orders of gram matrices (in our case ) to be considered is denoted by . The schematic diagram of the proposed algorithm is shown in Fig. \ref{fig:schematic} (in Appendix \ref{app:schematic}).



\paragraph{Preprocessing} If we compute  for every layer  and every order , we obtain a total of  correlations for any image D. The preprocessing involves computing the class-specific minimum and maximum values for the correlations: for every class , the minimum and maximum values for each of the  correlations are computed over all training examples D classified as . We keep track of the minimum and maximum values of the  correlations for all the classes in 4-D arrays Mins and Maxs, each of the order . Since each layer has different number of channels, the 4-th dimension must be large enough to accommodate the layer with the highest number of channels.

\begin{algorithm}[H]
\small
\caption{Compute the minimum and maximum values of feature co-occurrences for each class, layer and order}
\label{alg:compute_bounds}
\textbf{Input:} \\
    \phantom{hello} C: Number of output classes\\
    \phantom{hello} L: Number of Layers in entire network\\
    \phantom{hello} P: Set of all orders of Gram Matrix to consider\\
    \phantom{hello} : The train data\\
\textbf{Output:}\\
    \phantom{hello} Mins, Maxs

\begin{algorithmic}[1]
\State{Mins[C][L][P] } \Comment{Stores the Mins for each class, layer and order}
\State{Maxs[C][L][P] } \Comment{Stores the Maxs for each class, layer and order}

\For { in }
\State  \Comment{All the training examples predicted as }
    \For {D }
        \For { in }
    \For { in P}
                \State stat =  \Comment{The flattened upper triangular matrix}
               \For{ in }
                \State Mins = (Mins,stat)
                \State Maxs = (Maxs,stat)
               \EndFor
            \EndFor
        \EndFor
    \EndFor
\EndFor
\State \Return {Mins, Maxs}
\end{algorithmic}
\end{algorithm}


\paragraph{Computing Layerwise Deviations} Given the class-specific minimum and maximum values of the  feature correlations, we can compute the deviation of the test sample from the images seen at train time with respect to each of the layers. In order to account for the scale of values, we compute the deviation as the percentage change with respect to the maximum or minimum values of feature co-occurrences; the deviation of an observed correlation value from the minimum and maximum correlation values observed during train time can be computed as:




The deviation of a test image with respect to a given layer  is the sum total of the deviations with respect to each of the  correlation values:


\textbf{Total Deviation} of a test image D (), is computed by taking the sum total of the layerwise deviations (). However, the scale of layerwise deviations () varies with each layer depending on the number of channels in the layer, number of pixels per channel and semantic information contained in the layer. Therefore, we normalize the deviations by dividing it by , the expected deviation at layer , computed using the validation data. Note that we use the same normalizing factor irrespective of the class assigned.

\paragraph{Threshold} As is standard~\citep{lee2018simple}, a threshold, , for discriminating between out-of-distribution data and in-distribution data is computed as the 95th percentile of the total deviations of test data (). In other words, the threshold is computed so that 95\% of test examples have deviations lesser than the threshold ; the threshold-based discriminator can be formally written as:

\paragraph{Computational Complexity.} In order to reduce computational time, we can in fact compute deviations based on row-wise sums rather than individual elements. This would mean that the variable \textit{stat}, defined in line 8 of Algorithm \ref{alg:compute_bounds}, would now contain row-wise sums of  instead of the flattened upper triangular matrix; the inner loop of Eq. 4 would loop over  elements instead of  elements while also reducing the storage required for  and . In practise, we found that computing the anomalies this way yields differences of less than , and usually imperceptible, so the results described in the next section were computed in this way.

\section{Experiments - Detecting OOD}
In this section, we demonstrate the effectiveness of the proposed metric using competitive deep convolutional neural network architectures such as DenseNet and ResNet on various computer vision benchmark datasets such as: CIFAR-10, CIFAR-100, SVHN, TinyImageNet, LSUN and iSUN.

For fair comparison and to aid reproducibility, we use the pretrained ResNet \citep{resnet} and DenseNet \citep{densenet} models open-sourced by \cite{lee2018simple}, i.e. ResNet34 and DenseNet3 models trained on CIFAR-10, CIFAR-100 and SVHN datasets. For each of these models, we considered the corresponding test partitions as the in-distribution (positive) examples. For CIFAR-10 and CIFAR-100, we considered the out-of-distribution datasets used by \cite{lee2018simple}: TinyImagenet, LSUN and SVHN. Additionally, we also considered the iSUN dataset. For ResNet and DenseNet models trained on SVHN, we used considered CIFAR-10 dataset as the third OOD dataset. Details on these datasets are available in Appendix \ref{appendix:datasets}.

\begin{table}\footnotesize
\centering
\begin{tabular}{|l|l|l|}
\hline
              & \thead{Can work with\\ pre-trained Net?} & \thead{Can work without knowledge \\of OOD test examples?} \\ \hline
DPN  \citep{malinin2018predictive}         & {\xmark}            & {\cmark}                      \\ \cline{1-3} Semantic \citep{shalev2018out}      &  {\xmark}          & {\cmark}                                    \\ \cline{1-3} Variational Dirichlet \citep{chen2019a} &     {\xmark}       & {\cmark}                                       \\ \hlineB{5}
Mahalanobis \citep{lee2018simple}  & {\cmark}           & {\xmark}                       \\ \cline{1-3} ODIN    \citep{liang2017enhancing}      &   {\cmark}            &   {\xmark}   \\ \hlineB{5}
OE \citep{hendrycks2018deep} & {\xmark} &{\cmark}\\                         \hlineB{5}
Baseline    \citep{hendrycks2016baseline}  & {\cmark}           & {\cmark}                      \\ \cline{1-3} Ours   & {\cmark}        &             {\cmark}                              \\ \hlineB{5}
\end{tabular}
\caption{List of closely related methods. Note: OE uses OOD examples during training, but unrelated to test}
\label{tab:list_methods}
\end{table}

We benchmark our algorithm with the works listed in Table \ref{tab:list_methods} using the following metrics:
\begin{enumerate}[noitemsep,topsep=2pt,parsep=5pt,partopsep=0pt]
    \item \textbf{TNR@95TPR} is the probability that an OOD (negative) example is correctly identified when the true positive rate (TPR) is as high as 95\%. TPR can be computed as , where TP and FN denote True Positive and False Negative respectively.
    \item \textbf{Detection Accuracy} measures the maximum possible classification accuracy over all possible thresholds in discriminating between in-distribution and out-of-distribution examples. For those methods which assign a higher-score to the in-distribution examples, it can be calculated as ; for those methods which assign a lower score to in-distribution examples, it can be calculated as .
    \item  \textbf{AUROC} is the measure of the area under the plot of TPR vs FPR. For example, for those methods which assign a higher score to the in-distribution examples, this measures the probability that an OOD example is assigned a lower score than an in-distribution example.
\end{enumerate}

\paragraph{Experimental setup:} We use a pre-trained network to extract class-specific minimum and maximum correlation values for all pairs of features across all orders of gram matrices. Subsequently, the total deviation is computed for each example following Eq. \ref{eq:total_deviation}. Since the total deviation values depend on the randomly selected validation examples, we repeat the experiment 10 times to get a reliable estimate of the performance. The OOD detection performance for several combinations of model architecture, in-distribution dataset and out-of-distribution dataset are shown in Table \ref{tab:ood_results}. The results for Outlier Exposure (OE) are available in Table \ref{tab:OE}; some more results for OE and the results for DPN, Variational Dirichlet and Semantic are available in Appendix \ref{appendix:OE} and Appendix \ref{appendix:dpn+} respectively.

\begin{table}[htbp]
\footnotesize
\centering
\begin{tabular}{cllll}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}In-dist\ResNet)\end{tabular}}    & iSUN                                     & 44.6 / 73.2 / 97.8 / \textbf{99.3}          & 91.0 / 94.0 / 99.5 / \textbf{99.8}          & 85.0 / 86.5 / 96.7 / \textbf{98.1}           \\
                                                                               & LSUN (R)                                 & 49.8 / 82.1 / 98.8 / \textbf{99.6}          & 91.0 / 94.1 / 99.7 / \textbf{99.9}          & 85.3 / 86.7 / 97.7 / \textbf{98.6}           \\

                                                                               & TinyImgNet (R)                           & 41.0 / 67.9 / 97.1 / \textbf{98.7}          & 91.0 / 94.0 / 99.5 / \textbf{99.7}          & 85.1 / 86.5 / 96.3 / \textbf{97.8}           \\

                                                                               & SVHN                                     & 50.5 / 70.3 / 87.8 / \textbf{97.6}          & 89.9 / 96.7 / 99.1 / \textbf{99.5}          & 85.1 / 91.1 / 95.8 / \textbf{96.7}           \\

\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-100\DenseNet)\end{tabular}}  & iSUN                                     & 62.5 / 93.2 / 95.3 / \textbf{99.0}          & 94.7 / 98.7 / 98.9 / \textbf{99.8}          & 89.2 / 94.3 / 95.2 / \textbf{97.9}           \\
                                                                               & LSUN (R)                                 & 66.6 / 96.2 / 97.2 / \textbf{99.5}          & 95.4 / 99.2 / 99.3 / \textbf{99.9}          & 90.3 / 95.7 / 96.3 / \textbf{98.6}           \\
                                                                               & TinyImgNet (R)                           & 58.9 / 92.4 / 95.0 / \textbf{98.8}          & 94.1 / 98.5 / 98.8 / \textbf{99.7}          & 88.5 / 93.9 / 95.0 / \textbf{97.9}           \\
                                                                               & SVHN                                     & 40.2 / 86.2 / 90.8 / \textbf{96.1}          & 89.9 / 95.5 / 98.1 / \textbf{99.1}          & 83.2 / 91.4 / 93.9 / \textbf{95.9}           \\

\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-100\DenseNet)\end{tabular}}      & iSUN                                     & 78.3 / 82.2 / \textbf{99.9} / \textbf{99.4}          & 94.4 / 94.7 / \textbf{99.9} / \textbf{99.8} & 89.6 / 89.7 / \textbf{99.2} / 98.3           \\
                                                                               & LSUN (R)                                 & 77.1 / 81.1 / \textbf{99.9} / \textbf{99.5} & 94.1 / 94.5 / \textbf{99.9} / \textbf{99.8} & 89.1 / 89.2 / \textbf{99.3} / 98.6           \\
                                                                               & TinyImgNet (R)                           & 79.8 / 84.1 / \textbf{99.9} / \textbf{99.1} & 94.8 / 95.1 / \textbf{99.9} / \textbf{99.7} & 90.2 / 90.4 / \textbf{98.9} / 97.9           \\
                                                                               & CIFAR-10                                 & 69.3 / 71.7 / \textbf{96.8} / 80.4          & 91.9 / 91.4 / \textbf{98.9} / 95.5          & 86.6 / 85.8 / \textbf{95.9} / 89.1           \\
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}SVHN\\textbf{Base})\end{tabular}} &
\multicolumn{1}{l|}{{\textbf{Ours}}}  \\
\hline
CIFAR-10 & 65.1 & 90.5 & 51.7625 & \textbf{98.7}                               \\
\hline
CIFAR-100 & 37.3 & 61.5 & 19.15 & \textbf{93.4}                             \\
\hline
SVHN & 93.7 & \textbf{99.9} & 76.65 & 95.2                                \\
\hline
\end{tabular}
\caption{Comparison of results with OE \citep{hendrycks2018deep}. Since OE uses a different model from ours, we also report the corresponding baseline accuracy. We extract the mean TNR @ TPR95 for our technique by considering both ResNet and DenseNet models. Some more results are available in Appendix \ref{appendix:OE}.}
\label{tab:OE}
\end{table}

\paragraph{Near-distribution Outliers} The metric does not perform well for near-distribution outliers (for example, CIFAR10 vs CIFAR100) -- the detailed results can be seen in Table \ref{tab:detailed_ablation}; a recent work which performs very well for near-distribution outliers and yet does not require knowledge of OOD distribution is \cite{hendrycks2019selfsupervised}.



\paragraph{Source of Performance Gain} The above results are obtained when we consider all elements of gram matrix (Algorithm \ref{alg:compute_bounds}), compute deviations from extrema (Eq \ref{eq:cases_delta}) and finally, compute the total deviation with normalized layerwise deviations (Eq \ref{eq:total_deviation}). In order to better understand the source of performance gain, we conduct a detailed ablation study that considers alternative choices for the three steps outlined before; the alternative choices are chosen in order to answer the following questions: Q1) What if strictly diagonal elements or strictly off-diagonal elements are considered instead of complete Gram Matrix?; Q2) What happens if the deviation is computed from the mean instead of the extrema?; Q3) What happens if we do not normalize the layerwise deviations? In all, we conduct 12 experiments: 3 choices for Q1  2 choices for Q2  2 choices for Q3. As a broad summary, we find that, while there is no single rule that is unbroken by an exception, our proposed combination---i.e. using the complete Gram matrix, using the min/max metric, and using normalization as in Eq \ref{eq:total_deviation}---is generally more robust than any of the other combinations that we tried. More details on the ablation tests and discussions are available in Appendix \ref{appendix:ablation_results}.

\section{Discussion and Conclusion}
\label{s:discussion}


Beyond explicit OOD detection, this line of work may ultimately help better interpret neural networks' responses to OOD examples. With this goal in mind, and at the same time to clarify the internal mechanism of our method, we perform tests to address the following two questions:
\begin{enumerate}
    \item \textbf{Which layer representations are most useful?} In order to examine the role of the depth at which we compute  in detecting OODs, we construct detectors which make use of correlations derived from just one residual or dense block at a time; however, all orders of gram matrices are considered. Representative results are shown in Figure \ref{fig:ood_layerwise}. For all combinations of model/in-distribution/out-of-distribution-dataset, we find that the lower level representations are much more informative in discriminating between in-distribution and out-of-distribution datasets. However, the difference in detective power depends on the in-distribution dataset considered: for example, the difference in detective power between higher-level representations and lower-level representations is bigger for Cifar-100 than for Cifar-10. More graphs are available in Appendix \ref{appendix:depth_graphs}.

    \item \textbf{Which orders of gram matrices are most useful?} In order to understand which orders of gram matrices are most helpful in detecting OODs, we construct detectors which make use of only one order of gram matrix at a time; however, correlations are derived from the representations of all residual and dense blocks. Representative results are shown in Figure \ref{fig:ood_orders}. For all combinations of model/in-distribution/out-of-distribution-dataset, we find that the higher order gram matrices are much more informative in discriminating between in-distribution and out-of-distribution datasets. Ignoring the variations at orders greater than 4, we find that the TNR @ 95TPR increases with higher orders and finally saturates. More graphs are available in Appendix \ref{appendix:ablation_results} in Figure \ref{fig:power_start} to \ref{fig:power_end}.
\end{enumerate}

\paragraph{Conclusion.} Out-of-distribution detection is a challenging and important problem. We have proposed and reported on a relatively simple OOD detection method based on pairwise feature correlations that gives new state of the art detection results without requiring access to anything other than the training data itself.

\paragraph{Acknowledgements} We would like to thank Scott Lowe, Jackson Wang, Rich Zemel and the anonymous reviewers for their insightful comments and discussion.




\begin{figure}[H]
\small
\small
    \begin{subfigure}{0.49\linewidth}
      \resizebox {\textwidth} {!}  {
        \begin{tikzpicture}
        \begin{axis}[
            xlabel={Block},
            ylabel={TNR @ TPR95},
grid=none,
            xtick=data,
            xmin=0.8,xmax=4
        ]

        \addplot[
            color=black,
            ]
            coordinates {
        (1,99.699)(2,96.151)(3,70.018)(4,71.364)
            };

        \node[anchor=east] at (axis cs: 4,42) {Baseline};
        \addplot[dashed,mark=none, black] {41};

        \node[anchor=east] at (axis cs: 4,68.9) {ODIN};
        \addplot[dashed,mark=none, black] {67.9};

        \node[anchor=east] at (axis cs: 4,96.1) {Mahalanobis};
        \addplot[dashed,mark=none, black] {97.1};

        \node[anchor=east] at (axis cs: 4,100.5) {Ours};
        \addplot[dashed,mark=none, black] {99.48};

        \end{axis}
        \end{tikzpicture}
    }
  \subcaption{\small ResNet/CIFAR-10 vs Tiny Imagenet}
  \label{fig:sfig1}
\end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
   \resizebox {\textwidth} {!} {
        \begin{tikzpicture}
        \begin{axis}[
            xlabel={Block},
            ylabel={TNR @ TPR95},
grid=none,
            xtick=data,
            xmin=0.8,xmax=3
        ]

        \addplot[
            color=black,
            ]
            coordinates {
            (1,97.432)(2,80.057)(3,56.930)
            };

        \node[anchor=east] at (axis cs: 3,19) {Baseline};
        \addplot[dashed,mark=none, black] {17.6};

        \node[anchor=east] at (axis cs: 3,44) {ODIN};
        \addplot[dashed,mark=none, black] {42.6};

        \node[anchor=east] at (axis cs: 3,88) {Mahalanobis};
        \addplot[dashed,mark=none, black] {86.6};

        \node[anchor=east] at (axis cs: 3,97) {Ours};
        \addplot[dashed,mark=none, black] {96.08};
        \end{axis}
        \end{tikzpicture}
    }

    \subcaption{\small DenseNet/CIFAR-100 vs Tiny ImageNet}
    \label{fig:sfig2}
\end{subfigure}
    \caption{\small Significance of depth: The TNR@TPR95 is computed by constructing detectors which make use of all the gram matrices but consider only one residual or dense block at a time. ResNet32 has 4 residual blocks and DenseNet3 has 3 dense blocks. }
    \label{fig:ood_layerwise}
\end{figure}
\begin{figure}[H]
    \begin{subfigure}{0.49\linewidth}
      \resizebox {\textwidth} {!}  {
        \begin{tikzpicture}
        \begin{axis}[
            xlabel={Order of Gram Matrix},
            ylabel={TNR @ TPR95},
grid=none,
            xmin=0.8,xmax=10.5
        ]

        \addplot[
            color=black,
            ]
            coordinates {
        (1,94.696)(2,97.949)(3,99.403)(4,99.501)(5,99.309)(6,99.602)(7,99.373)(8,99.616)(9,99.322)(10,99.593)
            };

        \node[anchor=east] at (axis cs: 10.5,42) {Baseline};
        \addplot[dashed,mark=none, black] coordinates{(0.8,41)(10.5,41)};

        \node[anchor=east] at (axis cs: 10.5,68.9) {ODIN};
        \addplot[dashed,mark=none, black] coordinates{(0.8,67.9)(10.5,67.9)};

        \node[anchor=east] at (axis cs: 10.5,96.1) {Mahalanobis};
        \addplot[dashed,mark=none, black] coordinates{(0.8,97.1)(10.5,97.1)};

        \node[anchor=east] at (axis cs: 10.5,100.5) {Ours};
        \addplot[dashed,mark=none, black] coordinates{(0.8,99.48)(10.5,99.48)};
        \end{axis}
        \end{tikzpicture}
    }
  \subcaption{\small ResNet/CIFAR-10 vs Tiny Imagenet}
  \label{fig:sfig3}
\end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
   \resizebox {\textwidth} {!} {
        \begin{tikzpicture}
        \begin{axis}[
            xlabel={Order of Gram Matrix},
            ylabel={TNR @ TPR95},
grid=none,
            xmin=0.8,xmax=10.5
        ]

        \addplot[
            color=black,
            ]
            coordinates {
            (1,82.990)(2,89.856)(3,92.643)(4,95.68)(5,94.457)(6,96.7)(7,94.551)(8,96.686)(9,94.294)(10,96.616)
            };

        \node[anchor=east] at (axis cs: 10.5,19) {Baseline};
        \addplot[dashed,mark=none, black] coordinates{(0.8,17.6)(10.5,17.6)};

        \node[anchor=east] at (axis cs: 10.5,44) {ODIN};
        \addplot[dashed,mark=none, black] coordinates{(0.8,42.6)(10.5,42.6)};

        \node[anchor=east] at (axis cs: 10.5,88) {Mahalanobis};
        \addplot[dashed,mark=none, black] coordinates{(0.8,86.6)(10.5,86.6)};

        \node[anchor=east] at (axis cs: 10.5,97) {Ours};
        \addplot[dashed,mark=none, black] coordinates{(0.8,96.08)(10.5,96.08)};
        \end{axis}
        \end{tikzpicture}
    }

    \subcaption{\small DenseNet/CIFAR-100 vs Tiny ImageNet}
    \label{fig:sfig4}
\end{subfigure}
    \caption{ The importance of higher order gram matrices: The TNR@TPR95 is computed by constructing detectors which make use of only one of the gram matrices but consider all layers.}
    \label{fig:ood_orders}
\end{figure}





\bibliography{references}

\newpage

\appendix

\section{Schematic Diagram}
\label{app:schematic}
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.25]{schematic_diagram_1}
    \caption{The Schematic Diagram demonstrating the proposed algorithm}
    \label{fig:schematic}
\end{figure}

\section{Description of OOD Datasets}
\label{appendix:datasets}
The following includes the description of the out-of-distribution datasets:
\begin{enumerate}[noitemsep,topsep=2pt,parsep=5pt,partopsep=0pt]
    \item \textbf{TinyImagenet}, a subset of ImageNet \citep{imagenet} images, contains 10,000 test images from 200 different classes. Each image is downsampled to size 32 x 32 and all 10,000 images are used, as given in the opensourced version by \cite{liang2017enhancing}.
    \item \textbf{LSUN}, the Large-scale Scene UNderstanding dataset \citep{lsun} has 10,000 test images from 10 different scenes. Each image is downsampled to size 32 x 32 and all 10,000 images are used, as given in the opensourced version by \cite{liang2017enhancing}.
    \item \textbf{iSUN}, a subset of SUN images \citep{sun}, consists of 8925 images. Each image is downsampled to size 32 x 32 and is used; the downsampled version of the dataset has been opensourced by \cite{liang2017enhancing}.
    \item \textbf{SVHN}, the Street View House Numbers dataset \citep{svhn}, involves recognizing digits 0-9 in natural scene images. The test partition consisting of 26,032 images is used.
\end{enumerate}

\section{Detailed Ablation Results}
\label{appendix:ablation_results}
The results in the main paper correspond to the performance obtained when considering:
\begin{enumerate}
    \item \textbf{Feature Set}: all gram matrix entries
    \item \textbf{Metric}: layerwise deviations computed with respect to the mins and maxs.
    \item \textbf{Aggregation Scheme}: the total deviation is then computed using Eq \ref{eq:total_deviation}.
\end{enumerate}

In this section, detailed ablation results are reported by considering other options. Specifically:
\begin{enumerate}
    \item \textbf{Alternate Feature Set}: In addition to considering all gram matrix entries, we consider a proper partition of the gram matrix: strictly diagonal elements, and strictly off-diagonal elements. The diagonal elements correspond to the unary features, while the off-diagonal elements correspond to pairwise features. This can be done by appropriately changing the definition of variable \textit{stat} in Line 7 of Algorithm \ref{alg:compute_bounds}. In these experiments, we consider row-wise sums wherever the size of stat is ; in other words, we consider row-wise sums when considering off-diagonal elements and all gram matrix entries.
    \item \textbf{Alternate Metric}: An alternative formulation for computing feature-wise deviations can be to compute the deviation from the means using the one-dimensional Mahalanobis distance. In the preprocessing stage, this would be done by storing the \textit{Means} and \textit{Variances} of \textit{stat} (feature-wise) instead of their \textit{Mins} and \textit{Maxs}. Under this new alternative, the function  defined in Eq \ref{eq:cases_delta} would be redefined as:
    
    Accordingly, the layerwise deviation  can be defined as:
    
    where  would correspond to the statistic chosen in the previous step: diagonal entries only, row-wise sums of off-diagonal entries only or row-wise sums of complete gram matrix.

    We thus consider 2 options for computing the deviations: the Min/Max method presented in the main paper and the Mean/Variance method (Gaussian) described above.
    \item \textbf{Alternate Aggregation Scheme}: In order to compute the total deviation  from the layerwise deviations , we can compute it by following \ref{eq:total_deviation} or taking a simple sum as shown:
    
    We refer to Eq \ref{eq:total_deviation} as the normalized estimate and Eq \ref{eq:unnormalized_sum} as the unnormalized estimate.
\end{enumerate}

In all, the Table \ref{tab:detailed_ablation} reports detection rates in 12 settings: 3 choices for \textit{stat} (only the diagonal entries of gram matrix , only the off-diagonal entries of , or all of )  2 metrics for computing deviation (Min/Max or Mean/Variance)  2 choices for computing total deviation (Normalized sum or Unnormalized sum). All layers and all orders of gram matrix are considered in Table 4.


\begin{table}[H]
\centering
\resizebox{1.25\textwidth}{!}{\hspace{-3.2in}
\begin{tabular}{|cl|rrrr|rrrr|rrrr|rrrr|rrrr|rrrr|rrrr|rrrr|rrrr|}
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\\\ In-dist\Row-wise Sums)\end{tabular}} & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}c@{}}Complete Gram Matrix\Row-wise Sums)\end{tabular}} & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}c@{}}Complete Gram Matrix\Row-wise Sums)\end{tabular}} & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}c@{}}Complete Gram Matrix\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Mean/\\Var\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Min/\\Max\U)\end{tabular}}\\
\hline
\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-10\ResNet)\end{tabular}} & iSUN & 93.8 & 71.8 & 50.0 & 33.8 & 95.4 & 85.9 & 67.2 & 55.0 & 95.1 & 85.3 & 65.4 & 52.8 & 98.7 & 95.5 & 92.1 & 87.8 & 98.9 & 97.4 & 94.5 & 92.7 & 98.8 & 97.3 & 94.2 & 92.3 & 94.5 & 89.3 & 85.4 & 81.1 & 95.3 & 92.2 & 87.9 & 86.1 & 95.1 & 92.0 & 87.6 & 85.6\\ & LSUN & 95.6 & 70.8 & 45.6 & 32.9 & 97.2 & 87.5 & 64.2 & 52.0 & 97.0 & 86.8 & 62.4 & 49.7 & 99.1 & 95.9 & 91.8 & 87.8 & 99.3 & 97.8 & 94.6 & 92.9 & 99.2 & 97.7 & 94.3 & 92.4 & 95.4 & 90.1 & 85.7 & 81.4 & 96.3 & 93.1 & 88.4 & 86.6 & 96.1 & 93.0 & 88.1 & 86.1\\ & TinyImgNet & 94.1 & 68.0 & 51.4 & 34.6 & 95.3 & 84.2 & 68.1 & 52.8 & 95.1 & 83.5 & 66.6 & 50.8 & 98.8 & 95.1 & 92.5 & 87.1 & 99.0 & 97.2 & 94.8 & 92.5 & 98.9 & 97.1 & 94.6 & 92.1 & 94.6 & 88.5 & 85.9 & 80.1 & 95.2 & 91.8 & 88.4 & 85.9 & 95.1 & 91.6 & 88.2 & 85.4\\ & SVHN & 83.1 & 29.8 & 53.2 & 26.0 & 79.1 & 34.4 & 51.8 & 29.6 & 81.4 & 33.9 & 55.6 & 29.2 & 96.5 & 84.7 & 92.3 & 80.8 & 95.7 & 86.8 & 91.6 & 83.5 & 96.1 & 86.7 & 92.2 & 83.1 & 90.2 & 77.5 & 85.1 & 73.6 & 89.2 & 80.3 & 84.3 & 75.3 & 89.7 & 80.3 & 84.8 & 74.9\\ & CIFAR-10 & 12.9 & 18.1 & 19.2 & 18.2 & 11.4 & 17.5 & 17.5 & 17.9 & 12.1 & 17.6 & 18.1 & 18.0 & 69.3 & 71.2 & 76.6 & 74.6 & 67.3 & 70.0 & 75.3 & 71.9 & 67.8 & 70.1 & 75.5 & 72.0 & 64.6 & 66.0 & 71.1 & 69.1 & 63.0 & 64.8 & 69.8 & 66.5 & 63.3 & 65.0 & 70.1 & 66.6\\
\hline
\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-10\DenseNet)\end{tabular}} & iSUN & 96.0 & 84.2 & 95.5 & 91.9 & 96.1 & 88.8 & 95.5 & 95.7 & 95.9 & 88.5 & 95.3 & 95.8 & 99.1 & 97.2 & 98.9 & 98.2 & 99.0 & 97.9 & 98.9 & 99.0 & 99.1 & 97.8 & 98.9 & 99.0 & 95.7 & 91.4 & 95.4 & 93.6 & 95.7 & 92.6 & 95.3 & 95.5 & 95.7 & 92.6 & 95.3 & 95.5\\ & LSUN & 97.4 & 87.5 & 96.9 & 95.5 & 97.5 & 91.4 & 97.0 & 97.8 & 97.3 & 91.2 & 96.8 & 97.8 & 99.4 & 97.8 & 99.3 & 99.0 & 99.4 & 98.3 & 99.3 & 99.4 & 99.4 & 98.3 & 99.3 & 99.4 & 96.4 & 92.7 & 96.1 & 95.3 & 96.5 & 93.7 & 96.2 & 96.7 & 96.4 & 93.7 & 96.2 & 96.7\\ & TinyImgNet & 95.8 & 81.4 & 95.4 & 90.2 & 95.9 & 86.9 & 95.3 & 94.2 & 95.8 & 86.4 & 95.2 & 94.3 & 99.0 & 96.6 & 98.9 & 97.8 & 99.0 & 97.5 & 98.9 & 98.7 & 99.0 & 97.4 & 98.9 & 98.7 & 95.5 & 90.4 & 95.2 & 92.9 & 95.5 & 91.8 & 95.2 & 94.7 & 95.6 & 91.7 & 95.2 & 94.7\\ & SVHN & 89.4 & 59.7 & 88.8 & 64.5 & 87.3 & 63.2 & 86.4 & 67.4 & 89.4 & 62.9 & 87.9 & 67.3 & 97.4 & 92.5 & 97.3 & 92.6 & 97.0 & 92.7 & 96.9 & 93.4 & 97.4 & 92.7 & 97.1 & 93.4 & 92.4 & 85.7 & 92.0 & 86.0 & 91.7 & 86.0 & 91.4 & 87.1 & 92.4 & 86.2 & 91.9 & 87.1\\ & CIFAR-10 & 10.5 & 16.4 & 11.1 & 13.7 & 10.6 & 15.6 & 10.2 & 13.9 & 10.5 & 15.6 & 10.2 & 13.8 & 64.4 & 70.1 & 65.0 & 66.7 & 63.7 & 68.3 & 64.2 & 66.2 & 64.2 & 68.7 & 64.6 & 66.2 & 60.6 & 64.9 & 61.3 & 62.2 & 59.7 & 63.4 & 60.5 & 61.6 & 60.4 & 63.8 & 61.0 & 61.5\\
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}SVHN\DenseNet)\end{tabular}} & iSUN & 99.3 & 99.6 & 99.4 & 99.1 & 99.3 & 99.4 & 99.4 & 97.9 & 99.3 & 99.4 & 99.4 & 98.0 & 99.8 & 99.9 & 99.9 & 99.8 & 99.8 & 99.9 & 99.8 & 99.5 & 99.8 & 99.9 & 99.8 & 99.5 & 98.3 & 98.8 & 98.4 & 98.3 & 98.4 & 98.5 & 98.4 & 97.3 & 98.3 & 98.6 & 98.4 & 97.3\\ & LSUN & 99.5 & 99.7 & 99.7 & 99.4 & 99.5 & 99.4 & 99.6 & 98.2 & 99.5 & 99.4 & 99.6 & 98.3 & 99.9 & 99.9 & 99.9 & 99.9 & 99.8 & 99.9 & 99.9 & 99.6 & 99.8 & 99.9 & 99.9 & 99.6 & 98.6 & 98.9 & 98.7 & 98.5 & 98.6 & 98.6 & 98.7 & 97.6 & 98.5 & 98.7 & 98.7 & 97.7\\ & TinyImgNet & 99.2 & 99.5 & 99.3 & 99.2 & 99.1 & 99.2 & 99.2 & 98.0 & 99.0 & 99.2 & 99.2 & 98.1 & 99.7 & 99.9 & 99.8 & 99.8 & 99.7 & 99.8 & 99.8 & 99.6 & 99.7 & 99.8 & 99.8 & 99.6 & 97.9 & 98.5 & 98.1 & 98.2 & 98.0 & 98.3 & 98.1 & 97.3 & 97.9 & 98.3 & 98.1 & 97.3\\ & CIFAR-10 & 76.6 & 93.5 & 76.9 & 91.8 & 81.2 & 94.3 & 85.6 & 90.1 & 80.2 & 94.2 & 84.7 & 90.1 & 94.5 & 98.5 & 94.9 & 98.1 & 95.6 & 98.6 & 96.5 & 97.6 & 95.5 & 98.6 & 96.4 & 97.7 & 88.1 & 94.3 & 88.6 & 93.5 & 89.2 & 94.7 & 90.7 & 92.6 & 89.0 & 94.7 & 90.6 & 92.6\\
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Summary\end{tabular}} & MEAN & 95.4 & 86.6 & 87.9 & 77.3 & 95.6 & 90.1 & 90.4 & 83.7 & 95.7 & 89.9 & 90.4 & 83.3 & 99.0 & 97.5 & 97.7 & 95.6 & 99.0 & 98.0 & 98.1 & 96.9 & 99.0 & 98.0 & 98.1 & 96.8 & 96.0 & 93.7 & 94.1 & 91.6 & 96.1 & 94.4 & 94.5 & 92.9 & 96.2 & 94.4 & 94.5 & 92.8\\ & STD-DEV & 6.2 & 17.2 & 18.1 & 27.0 & 6.0 & 14.7 & 13.5 & 21.3 & 5.7 & 14.9 & 13.5 & 22.0 & 1.3 & 3.4 & 2.7 & 5.2 & 1.3 & 2.9 & 2.2 & 3.9 & 1.2 & 2.9 & 2.2 & 4.0 & 2.9 & 5.2 & 4.5 & 6.8 & 2.8 & 4.5 & 3.9 & 5.6 & 2.8 & 4.5 & 3.9 & 5.7\\
\hline
\end{tabular}
}
\caption{Detailed Ablation Results demonstrating the detection rates under 12 different settings. The MEAN and STD-DEV are computed by using all elements in the table excepting the CIFAR-10 vs CIFAR-100 and CIFAR-100 vs CIFAR-10 entries.
}
\label{tab:detailed_ablation}
\end{table}

By analysing the ablation results, we attempt to answer the following questions:
\begin{enumerate}
    \item \textbf{Are pairwise features more useful than unary features?} We observe that the Min/Max metric can work equally well with both unary and pairwise features; in some cases, the unary features are marginally better (Ex: ResNet/CIFAR-10 vs SVHN) and in some cases, the pairwise features are marginally better (Ex: ResNet/CIFAR-100 vs iSUN/LSUN/TinyImgNet). Interestingly, the behavior of the Mean/Var metric is different: the performance with pairwise features are significantly higher than with unary features in 19 out of 28 tested cases. For example, the TNR at TPR95 for ResNet/CIFAR-100 vs TinyImgNet is 68.0 with unary features and 84.2 with pairwise features.

    We notice that using the unary features (diagonal entries) sometimes did well when pairwise features (off-diagonal entries) did not do well, and vice versa, so using both gives the kind of effect that we \textit{want} in an ensemble: models that cover and work well over different parts of the space. Therefore, an overall message of our experiments is that it is worthwhile to consider all elements of the gram matrix.

    \item \textbf{Is it neccessary to use Min/Max metric?} Except in 6 cases (ResNet:  CIFAR-10 vs CIFAR-100, ResNet: CIFAR-100 vs CIFAR-10, DenseNet: CIFAR-10 vs CIFAR-100, DenseNet:  CIFAR-100 vs CIFAR-10, ResNet: SVHN vs CIFAR-10 and DenseNet: SVHN vs CIFAR-10), the min/max metric consistently performs better than the mean/var metric. Additionally, it is not clear if the Mean/Var estimate performs better with normalized sums or unnormalized sums: for example, observe that Mean/Var estimate performs very poorly with the unnormalized estimate for ResNet/CIFAR-100, while the performance of Mean/Var for DenseNet/CIFAR-100 is competitive with the performance of Min/Max only when an unnormalized estimate is computed.

    One can observe that computing the one-dimensional Mahalanobis distance for each component of the statistic derived from the Gram Matrix and later computing the total sum is equivalent to representing each input image by a big vector (say, ) derived from the Gram Matrices computed across various layers, constructing class-conditional distributions of  (assuming that each component of  is normally distributed and independent of the other components) and subsequently computing the probability of an unseen . In early research, we noticed the following problems with the Mean/Var estimate:
    \begin{enumerate}
        \item The individual components of gram matrices do not follow normal distribution strictly and Mean/Var assigns lower probabilities to the in-distribution images as well.
        \item The total deviation  -- computed by simply summing across the layerwise deviations,  -- was not able to accurately summarize the information contained in the different s. Specifically, information about the layer where the input example had a higher deviation was lost when a simple sum was taken.
    \end{enumerate}
    The proposed Min/Max idea solves problem (a) by employing a weaker metric: deviation from extrema instead of the mean. It can also be said that the Min/Max metric considers a uniform probability density between the extrema. Problem (b), which exists even for this newer metric, is solved by the normalization scheme described in the main paper for computing the sum total deviation.
\end{enumerate}

\paragraph{Higher Order Gram Matrices} The Min/Max metric is a weak approximation to the true probability density. On conducting a thorough analysis of how the OOD examples were able to fool the metric, it appeared that the intermediate features had several tiny activations that could yield innocuous correlation values. Higher-order gram matrices as described in the main paper provide a natural way to mitigate these effects.

Notable observations from Figures 4 through 9 (all layers are considered but only one order of gram matrix is considered at a time):
\begin{itemize}
    \item \textbf{Ensemble effect}: In 24/28 cases, higher order gram matrices improve detection rates. Higher order gram matrices help both the Min/Max and the Mean/Var metrics. In most cases, the even powers are more helpful than the odd powers; in some cases, the odd powers are more helpful (Ex: DenseNet/CIFAR-100 vs CIFAR-10). Despite these variations, it is possible to get an ensemble effect by considering all possible powers as demonstrated in the main paper.
    \item In ResNet:CIFAR-10 vs CIFAR-100 and DenseNet:CIFAR-10 vs CIFAR-100, the higher order gram matrices yield lower detection rates. We find these exceptions interesting, and would like to understand them better in future.
\end{itemize}

\paragraph{Summary} The unambiguous message from this ablation study is that the Gram matrix contains useful information which can be used for detecting OOD examples. While the standard Mean/Variance metric does not always work well, the proposed Min/Max metric yields consistent performance competitive with state-of-the-art methods. The use of higher-order Gram matrices further boosts the overall performance. Although the Min/Max method can work very well for "far-from-distribution" examples, it does not work well when a fine grained estimate is needed (for example, CIFAR-10 vs CIFAR-100). We hope the strong empirical proof that Gram matrices contain useful information can motivate the development of OOD detectors with powerful density estimators; eventually, such estimators might not need higher-order Gram matrices to boost their performance. Here they allow us to propose a metric based on simple thresholding that, generally, works independently of the particular OOD examples and independently of the architecture, and we believe that the fact that this is the case---the fact that this approach scores relatively well---itself raises several interesting new questions. We hope that answering these questions will ultimately strengthen our understanding of deep neural networks and give rise to more robust neural network models in the future.

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/resnet_cifar10.pdf}
    \caption{ResNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.}
    \label{fig:power_start}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/resnet_cifar100.pdf}
    \caption{ResNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/resnet_svhn.pdf}
    \caption{ResNet/SVHN: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/densenet_cifar10.pdf}
    \caption{DenseNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/densenet_cifar100.pdf}
    \caption{DenseNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/densenet_svhn.pdf}
    \caption{DenseNet/SVHN: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.}
    \label{fig:power_end}
\end{figure}

\subsection{Significance of Depth}
\label{appendix:depth_graphs}
\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/densenet_cifar10_block.pdf}
    \caption{DenseNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/densenet_cifar100_blocks.pdf}
    \caption{DenseNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/resnet_cifar10_blocks.pdf}
    \caption{ResNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/resnet_cifar100_blocks.pdf}
    \caption{ResNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/resnet_svhn_blocks.pdf}
    \caption{DenseNet/SVHN: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.}
\end{figure}

\begin{figure}[H]
    \hspace{-1.5in}
    \includegraphics[scale=0.13]{Graphs/densenet_svhn_blocks.pdf}
    \caption{DenseNet/SVHN: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.}
\end{figure}

\section{Expanded Table}
\begin{table}[htbp]
\footnotesize
\centering
\begin{tabular}{cllll}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}In-dist\ResNet)\end{tabular}}    & iSUN                                     & 44.6 / 73.2 / 97.8 / \textbf{99.3}          & 91.0 / 94.0 / 99.5 / \textbf{99.8}          & 85.0 / 86.5 / 96.7 / \textbf{98.1}           \\
                                                                               & LSUN (R)                                 & 49.8 / 82.1 / 98.8 / \textbf{99.6}          & 91.0 / 94.1 / 99.7 / \textbf{99.9}          & 85.3 / 86.7 / 97.7 / \textbf{98.6}           \\
                                                                               & LSUN (C)                                 & 48.6 / 62.0 / 81.3 / \textbf{89.8}          & 91.9 / 91.2 / 96.7 \textbf{/ 97.8}          & 86.3 / 82.4 / 90.5 / \textbf{92.6}           \\
                                                                               & TinyImgNet (R)                           & 41.0 / 67.9 / 97.1 / \textbf{98.7}          & 91.0 / 94.0 / 99.5 / \textbf{99.7}          & 85.1 / 86.5 / 96.3 / \textbf{97.8}           \\
                                                                               & TinyImgNet (C)                           & 46.4 / 68.7 / 92.0 / \textbf{96.7}          & 91.4 / 93.1 / 98.6 / \textbf{99.2}          & 85.4 / 85.2 / 93.9 / \textbf{96.1}           \\
                                                                               & SVHN                                     & 50.5 / 70.3 / 87.8 / \textbf{97.6}          & 89.9 / 96.7 / 99.1 / \textbf{99.5}          & 85.1 / 91.1 / 95.8 / \textbf{96.7}           \\
                                                                               & CIFAR-100                                & 33.3 / \textbf{42.0} / 41.6 / 32.9          & 86.4 / 85.8 / \textbf{88.2} / 79.0          & 80.4 / 78.6 / \textbf{81.2} / 71.7           \\
\hline
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-100\DenseNet)\end{tabular}}  & iSUN                                     & 62.5 / 93.2 / 95.3 / \textbf{99.0}          & 94.7 / 98.7 / 98.9 / \textbf{99.8}          & 89.2 / 94.3 / 95.2 / \textbf{97.9}           \\
                                                                               & LSUN (R)                                 & 66.6 / 96.2 / 97.2 / \textbf{99.5}          & 95.4 / 99.2 / 99.3 / \textbf{99.9}          & 90.3 / 95.7 / 96.3 / \textbf{98.6}           \\
                                                                               & LSUN (C)                                 & 51.8 / 70.6 / 48.2 / \textbf{88.4}          & 92.9 / 93.6 / 80.2 / \textbf{97.5}          & 86.9 / 86.4 / 75.6 / \textbf{92.0}           \\
                                                                               & TinyImgNet (R)                           & 58.9 / 92.4 / 95.0 / \textbf{98.8}          & 94.1 / 98.5 / 98.8 / \textbf{99.7}          & 88.5 / 93.9 / 95.0 / \textbf{97.9}           \\
                                                                               & TinyImgNet (C)                           & 56.7 / 87.0 / 84.2 / \textbf{96.7}          & 93.8 / 97.6 / 95.3 / \textbf{99.3}          & 88.1 / 92.3 / 89.9 / \textbf{96.1}           \\
                                                                               & SVHN                                     & 40.2 / 86.2 / 90.8 / \textbf{96.1}          & 89.9 / 95.5 / 98.1 / \textbf{99.1}          & 83.2 / 91.4 / 93.9 / \textbf{95.9}           \\
                                                                               & CIFAR-100                                & 40.3 / \textbf{53.1} / 14.5 / 26.7          & 89.3 / \textbf{90.2} / 58.5 / 72.0          & \textbf{82.9} / 82.7 / 57.2 / 67.3           \\
\hline
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-100\DenseNet)\end{tabular}}      & iSUN                                     & 78.3 / 82.2 / \textbf{99.9} / \textbf{99.4}          & 94.4 / 94.7 / \textbf{99.9} / \textbf{99.8} & 89.6 / 89.7 / \textbf{99.2} / 98.3           \\
                                                                               & LSUN (R)                                 & 77.1 / 81.1 / \textbf{99.9} / \textbf{99.5} & 94.1 / 94.5 / \textbf{99.9} / \textbf{99.8} & 89.1 / 89.2 / \textbf{99.3} / 98.6           \\
                                                                               & TinyImgNet (R)                           & 79.8 / 84.1 / \textbf{99.9} / \textbf{99.1} & 94.8 / 95.1 / \textbf{99.9} / \textbf{99.7} & 90.2 / 90.4 / \textbf{98.9} / 97.9           \\
                                                                               & CIFAR-10                                 & 69.3 / 71.7 / \textbf{96.8} / 80.4          & 91.9 / 91.4 / \textbf{98.9} / 95.5          & 86.6 / 85.8 / \textbf{95.9} / 89.1           \\
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}SVHN\WRN 40-2)\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{OOD}} & \multicolumn{3}{c|}{MSP}                                                                                                               & \multicolumn{3}{c|}{Ours}                                                                                                              & \multicolumn{3}{c|}{Ours + MSP}                                                                                                         \\
\cline{3-11}
                                                                             & \multicolumn{1}{c|}{}                     & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TNR at \\TPR 95\%\end{tabular}} & \multicolumn{1}{c}{AUROC} & \multicolumn{1}{c|}{DTACC} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TNR at \\TPR 95\%\end{tabular}} & \multicolumn{1}{c}{AUROC} & \multicolumn{1}{c|}{DTACC} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TNR at \\TPR 95\%\end{tabular}} & \multicolumn{1}{c}{AUROC} & \multicolumn{1}{c|}{DTACC}  \\
\hline
\multirow{7}{*}{CIFAR-10}                                                    & iSUN                                      & 98.3                                                                          & 99.3                      & 96.9                       & 98.9                                                                          & 99.8                      & 97.8                       & \textbf{99.8}                                                                 & 99.9                      & 99.0                        \\
                                                                             & LSUN (R)                                  & 98.5                                                                          & 99.4                      & 97.0                       & 99.4                                                                          & 99.9                      & 98.4                       & \textbf{99.8}                                                                 & 99.9                      & 99.1                        \\
                                                                             & LSUN (C)                                  & 98.0                                                                          & 99.4                      & 96.9                       & 89.5                                                                          & 97.8                      & 92.5                       & \textbf{98.6}                                                                 & 99.6                      & 97.3                        \\
                                                                             & TinyImgNet (R)                            & 93.9                                                                          & 98.5                      & 94.6                       & 98.5                                                                          & 99.7                      & 97.6                       & \textbf{99.5}                                                                 & 99.9                      & 98.5                        \\
                                                                             & TinyImgNet (C)                            & 95.2                                                                          & 98.7                      & 95.2                       & 95.9                                                                          & 99.1                      & 95.7                       & \textbf{99.1}                                                                 & 99.8                      & 97.8                        \\
                                                                             & SVHN                                      & 98.0                                                                          & 99.5                      & 96.9                       & 97.6                                                                          & 99.4                      & 96.8                       & \textbf{99.3}                                                                 & 99.8                      & 98.2                        \\
                                                                             & CIFAR-100                                 & \textbf{73.9}                                                                 & 94.8                      & 87.9                       & 38.9                                                                          & 80.1                      & 73.3                       & 72.9                                                                          & 93.9                      & 87.0                        \\
\hline
\multirow{7}{*}{CIFAR-100}                                                   & iSUN                                      & 50.9                                                                          & 89.8                      & 82.3                       & \textbf{96.3}                                                                 & 99.1                      & 95.9                       & 95.6                                                                          & 98.9                      & 96.0                        \\
                                                                             & LSUN (R)                                  & 58.3                                                                          & 92.0                      & 84.7                       & \textbf{98.4}                                                                 & 99.6                      & 97.3                       & 97.4                                                                          & 99.3                      & 97.4                        \\
                                                                             & LSUN (C)                                  & 69.5                                                                          & 94.0                      & 86.6                       & 69.7                                                                          & 92.6                      & 85.3                       & \textbf{83.1}                                                                 & 96.3                      & 89.7                        \\
                                                                             & TinyImgNet (R)                            & 36.1                                                                          & 85.1                      & 77.5                       & \textbf{96.3}                                                                 & 99.1                      & 95.9                       & 92.8                                                                          & 98.2                      & 94.6                        \\
                                                                             & TinyImgNet (C)                            & 41.6                                                                          & 86.3                      & 78.6                       & \textbf{90.1}                                                                 & 97.7                      & 92.8                       & 87.1                                                                          & 96.9                      & 91.1                        \\
                                                                             & SVHN                                      & 56.2                                                                          & 92.5                      & 85.6                       & 84.8                                                                          & 96.5                      & 90.8                       & \textbf{85.6}                                                                 & 96.8                      & 90.4                        \\
                                                                             & CIFAR-10                                  & \textbf{17.4}                                                                 & 78.4                      & 71.7                       & 7.5                                                                           & 59.3                      & 57.3                       & 16.5                                                                          & 77.7                      & 71.6                        \\
\hline
\end{tabular}}
\caption{Table shows results when our method is combined with OE. The experiment was conducted with pretrained WideResNet open-sourced by \cite{hendrycks2018deep}. MSP uses Maximum Softmax Probability; "Ours" refers to the metric  (Eq. \ref{eq:total_deviation}); "Ours+MSP" is obtained by using .}
\end{table}

\begin{table}[H]
\centering
\resizebox{1.1\textwidth}{!}{

\begin{tabular}{|c|l|lll|lll|lll|lll|lll|lll|} \hline \multicolumn{2}{|c|}{WRN-40-2} & \multicolumn{9}{c|}{Trained with Baseline} & \multicolumn{9}{c|}{Trained with OE} \\ \hline \multirow{2}{*}{In-Distribution} & \multicolumn{1}{c|}{\multirow{2}{*}{OOD}} & \multicolumn{3}{c|}{MSP} & \multicolumn{3}{c|}{Ours} & \multicolumn{3}{c|}{Ours + MSP} & \multicolumn{3}{c|}{MSP} & \multicolumn{3}{c|}{Ours} & \multicolumn{3}{c|}{Ours + MSP} \\ \cline{3-20} & \multicolumn{1}{c|}{} & \begin{tabular}[c]{@{}l@{}}TNR at\\ TPR95\end{tabular} & AUROC & DTACC & \begin{tabular}[c]{@{}l@{}}TNR at\\ TPR95\end{tabular} & AUROC & DTACC & \begin{tabular}[c]{@{}l@{}}TNR at\\ TPR95\end{tabular} & AUROC & DTACC & \begin{tabular}[c]{@{}l@{}}TNR at\\ TPR95\end{tabular} & AUROC & DTACC & \begin{tabular}[c]{@{}l@{}}TNR at\\ TPR95\end{tabular} & AUROC & DTACC & \begin{tabular}[c]{@{}l@{}}TNR at\\ TPR95\end{tabular} & AUROC & DTACC \\ \hline \multirow{5}{*}{CIFAR-10} & iSUN & 43.6 & 89.9 & 83.3 & 98.7 & 99.7 & 97.5 & 98.8 & 99.7 & 97.6 & 98.3 & 99.3 & 96.9 & 98.9 & 99.8 & 97.8 & 99.8 & 100.0 & 99.1 \\ & LSUN & 47.8 & 91.5 & 85.0 & 99.3 & 99.8 & 98.3 & 99.3 & 99.8 & 98.4 & 98.5 & 99.4 & 97.0 & 99.4 & 99.9 & 98.4 & 99.9 & 100.0 & 99.3 \\ & TinyImgNet & 39.1 & 88.2 & 81.6 & 98.1 & 99.6 & 97.3 & 98.3 & 99.6 & 97.4 & 93.9 & 98.5 & 94.5 & 98.5 & 99.7 & 97.6 & 99.5 & 99.9 & 98.8 \\ & SVHN & 51.6 & 91.9 & 85.1 & 97.1 & 99.3 & 96.4 & 97.4 & 99.3 & 96.6 & 98.0 & 99.5 & 96.8 & 97.6 & 99.5 & 96.8 & 99.1 & 99.8 & 98.4 \\ & CIFAR100 & 37.0 & 87.8 & 81.5 & 25.7 & 74.7 & 68.6 & 28.3 & 75.6 & 69.2 & 73.9 & 94.8 & 87.8 & 37.8 & 80.0 & 73.2 & 58.5 & 85.1 & 78.4 \\ \hline \multirow{5}{*}{CIFAR-100} & iSUN & 18.4 & 78.1 & 71.6 & 96.2 & 99.1 & 95.9 & 96.2 & 99.0 & 95.8 & 50.9 & 89.8 & 82.3 & 96.3 & 99.1 & 95.9 & 96.8 & 99.3 & 96.1 \\ & LSUN & 20.2 & 79.2 & 72.5 & 97.8 & 99.4 & 96.9 & 97.8 & 99.4 & 96.8 & 58.3 & 92.0 & 84.6 & 98.4 & 99.6 & 97.2 & 98.3 & 99.6 & 97.2 \\ & TinyImgNet & 20.4 & 78.7 & 71.8 & 96.0 & 99.0 & 95.7 & 96.1 & 99.0 & 95.7 & 36.1 & 85.1 & 77.5 & 96.2 & 99.1 & 95.8 & 95.5 & 99.0 & 95.3 \\ & SVHN & 14.8 & 70.9 & 64.8 & 85.6 & 96.8 & 91.3 & 84.9 & 96.7 & 91.2 & 56.2 & 92.5 & 85.5 & 84.5 & 96.4 & 90.7 & 87.9 & 97.6 & 92.2 \\ & CIFAR10 & 16.4 & 74.0 & 67.8 & 6.0 & 58.3 & 56.3 & 8.9 & 62.8 & 59.6 & 17.4 & 78.4 & 71.7 & 7.4 & 59.1 & 57.1 & 13.8 & 69.4 & 64.9 \\ \hline \end{tabular}}
\caption{Table shows results from preliminary experiments on combining OE with our method. The experiment was conducted with pretrained WideResNet open-sourced by \cite{hendrycks2018deep}. MSP uses Maximum Softmax Probability; "Ours" refers to the metric  (Eq. \ref{eq:total_deviation}); "Ours+MSP" is obtained by dividing  with MSP.}
\end{table}

\section{Few more OOD results}

\subsection{Comparing with OE}
\label{appendix:OE}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|rrrr|}
\hline
In-distribution & OOD & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}OE \Base)\end{tabular}} & \multicolumn{1}{l|}{Ours} \\
\hline
CIFAR-10 & Gaussian & 85.6 & \textbf{99.3} & 43.5 & \textbf{100.} \\
& Rademacher & 52.4 & \textbf{99.5} & 48.3 & \textbf{100.} \\
& Blob & 83.8 & \textbf{99.4} & 52.9 & \textbf{99.8} \\
& Texture & 57.2 & \textbf{87.8} & 37.0 & 85.3 \\
& SVHN & 71.2 & 95.2 & 45.4 & \textbf{96.1} \\
& LSUN & 61.3 & 87.9 & 58.2 & \textbf{99.5} \\
\hline
CIFAR-100 & Gaussian & 45.7 & 87.9 & 18.2 & \textbf{100.} \\
& Rademacher & 61.0 & 82.9 & 15.6 & \textbf{100.} \\
& Blob & 62.0 & 87.9 & 38.4 & \textbf{98.6} \\
& Texture & 28.5 & 45.6 & 19.9 & \textbf{68.5} \\
& SVHN & 30.7 & 57.1 & 23.5 & \textbf{85.4} \\
& LSUN & 26.0 & 42.5 & 18.2 & \textbf{97.2} \\
\hline
SVHN & Gaussian & 94.6 & \textbf{100.} & 87.65 & \textbf{100.} \\
& Bernoulli & 95.6 & \textbf{100.} & 92.25 & \textbf{100.} \\
& Blob & 96.3 & \textbf{100.} & 93.35 & \textbf{100.} \\
& Texture & 92.8 & \textbf{99.8} & 72.6 & 94.9 \\
& Cifar-10 & 94.0 & \textbf{99.9} & 73.8 & 83.0 \\
& LSUN & 93.6 & \textbf{99.9} & 75.7 & \textbf{99.5} \\
\hline
\end{tabular}
\caption{Comparison of Mean TNR@TPR95 values.}
\end{table}

Following \cite{hendrycks2018deep}, we created the gaussian, rademacher, blob and bernoulli synthetic datasets. Their descriptions are as follows: \textit{Gaussian} anomalies have each dimension i.i.d. sampled from an isotropic Gaussian distribution.
\textit{Rademacher}
anomalies are images where each
dimension is -1 or 1 with equal
probability, so each
dimension is sampled from a symmetric Rademacher distribution. \textit{Bernoulli} images have each pixel
sampled from a Bernoulli distribution if the input range is [0, 1]. \textit{Blobs} data consist of algorithmically generated amorphous shapes with definite edges. \textit{Textures} is a dataset
of describable textural images \citep{textures}.

\subsection{Comparing with DPN, VD and Semantic.}
\label{appendix:dpn+}
\begin{table}[H]
\centering
\parbox{.45\linewidth}{
\scriptsize
\begin{tabular}{|llrrr|}
\hline
\multicolumn{1}{|c}{\textbf{OOD}} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{TNR}\\\textbf{@}\\\textbf{TPR95}\end{tabular}} & \multicolumn{1}{l}{\textbf{AUROC}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}\textbf{Detection}
\\\textbf{Accuracy}\end{tabular}}
\\
\hline
\multirow{6}{*}{LSUN} & DPN & 42.60 & 90.20 & 79.50
\\ & VD & 92.30 & 98.30 & 94.10
\\ & Baseline & 49.80 & 91.00 & 85.30
\\ & ODIN & 82.10 & 94.10 & 86.70
\\ & Mahalanobis & 98.80 & 99.70 & 97.70
\\ & \textbf{Ours} & \textbf{99.85} & \textbf{99.89} & \textbf{98.66}
\\
\hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Tiny\\ImgNet\end{tabular}} & DPN & 71.60 & 93.00 & 86.40
\\ & VD & 82.90 & 96.80 & 91.30
\\ & Baseline & 41.00 & 91.00 & 85.10
\\ & ODIN & 67.90 & 94.00 & 86.50
\\ & Mahalanobis & 97.10 & 99.50 & 96.30
\\ & \textbf{Ours} & \textbf{99.48} & \textbf{99.72} & \textbf{97.82}
\\
\hline
\multirow{6}{*}{SVHN} & DPN & 79.90 & 95.90 & 87.30
\\ & VD & 71.30 & 93.20 & 86.40
\\ & Baseline & 50.50 & 89.90 & 85.10
\\ & ODIN & 70.30 & 96.70 & 91.10
\\ & Mahalanobis & 87.80 & 99.10 & 95.80
\\ & \textbf{Ours} & \textbf{98.14} & \textbf{99.50} & \textbf{96.71}
\\
\hline
\end{tabular}
\subcaption{{ResNet/CIFAR-10}}
}
\hfill
\parbox{.45\linewidth}{
\scriptsize
\begin{tabular}{|llrrr|}
\hline
\multicolumn{1}{|c}{\textbf{OOD}} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{TNR}\\\textbf{@}\\\textbf{TPR95}\end{tabular}} & \multicolumn{1}{l}{\textbf{AUROC}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}\textbf{Detection}
\\\textbf{Accuracy}\end{tabular}}
\\
\hline
\multirow{6}{*}{iSUN} & Semantic & 41.60 & 85.20 & 88.40
\\ & VD & 80.20 & 94.20 & 87.80
\\ & Baseline & 16.89 & 75.80 & 70.11
\\ & ODIN & 45.21 & 85.48 & 78.47
\\ & Mahalanobis & 89.91 & 97.91 & 93.05
\\ & \textbf{Ours} & \textbf{95.12} & \textbf{98.9} & \textbf{95.18}
\\
\hline
\multirow{6}{*}{LSUN} & Semantic & 20.50 & 79.00 & 57.80
\\ & VD & 85.50 & 95.90 & 90.40
\\ & Baseline & 18.80 & 75.80 & 69.90
\\ & ODIN & 23.20 & 85.60 & 78.30
\\ & Mahalanobis & 90.89 & 98.2 & 93.5
\\ & \textbf{Ours} & \textbf{97.14} & \textbf{99.28} & \textbf{96.19}
\\
\hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Tiny\\ImgNet\end{tabular}} & Semantic & 37.60 & 83.10 & 75.60
\\ & VD & 83.70 & 95.30 & 89.70
\\ & Baseline & 20.40 & 77.20 & 70.80
\\ & ODIN & 36.1 & 87.6 & 80.1
\\ & Mahalanobis & 90.92 & 98.20 & 93.30
\\ & \textbf{Ours} & \textbf{95.12} & \textbf{98.97} & \textbf{95.13}
\\
\hline
\end{tabular}
\subcaption{{ResNet/CIFAR-100}}
}
\caption{We compare our method with DPN, VD and Semantic by reporting results where available.}
\end{table}

\subsection{Results for Fully-connected Networks}
\label{appendix:mlp_results}
\begin{table}[H]
\centering
\begin{tabular}{|c|clrrr|}
\hline
\textbf{Architecture} & \textbf{OOD} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{TNR @ TPR95}} & \multicolumn{1}{l}{\textbf{AUROC}} & \multicolumn{1}{l|}{\textbf{Detection Accuracy}}
\\
\hline
\multirow{4}{*}{300} & \multirow{2}{*}{KMNIST} & Baseline & 47.66 & 73.96 & 73.91
\\ & & \textbf{Ours} & \textbf{98.57} & \textbf{99.66} & \textbf{97.37}
\\
\cline{2-6} & \multirow{2}{*}{Fashion-MNIST} & Baseline & 44.93 & 66.93 & 71.07
\\ & & \textbf{Ours} & \textbf{93.51} & \textbf{98.64} & \textbf{94.36}
\\
\hline
\multirow{4}{*}{300-150} & \multirow{2}{*}{KMNIST} & Baseline & 59.79 & 75.17 & 79.49
\\ & & \textbf{Ours} & \textbf{97.8} & \textbf{99.4} & \textbf{96.55}
\\
\cline{2-6} & \multirow{2}{*}{Fashion-MNIST} & Baseline & 70.73 & 77.10 & 83.00
\\ & & \textbf{Ours} & \textbf{95.2} & \textbf{99.00} & \textbf{95.17}
\\
\hline
\multirow{4}{*}{300-150-50~} & \multirow{2}{*}{KMNIST} & Baseline & 70.4 & 79.75 & 83.38
\\ & & \textbf{Ours} & \textbf{97.5} & \textbf{99.11} & \textbf{96.4}
\\
\cline{2-6} & \multirow{2}{*}{Fashion-MNIST} & Baseline & 73.92 & 76.54 & 84.67
\\ & & \textbf{Ours} & \textbf{95.7} & \textbf{98.94} & \textbf{95.48}
\\
\hline
\end{tabular}
\caption{The method even works quite well with a fully-connected neural network trained on MNIST. The results are shown for 300-unit single layer MLP, 300-150 two-layer MLP and 300-150-50 MLP.}
\label{fig:mnist_ood}
\end{table}


\end{document}
