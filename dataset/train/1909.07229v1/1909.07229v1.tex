\documentclass{bmvc2k}



\newcommand{\KY}[1]{\textcolor{green}{{[\textbf{KY}: #1]}}}
\newcommand{\LZ}[1]{\textcolor{red}{{[\textbf{LZ}: #1]}}}

\title{Global Aggregation then Local Distribution in Fully Convolutional Networks}

\addauthor{Xiangtai Li}{lxtpku@pku.edu.cn}{1}
\addauthor{Li Zhang}{lz@robots.ox.ac.uk}{2}
\addauthor{Ansheng You}{youansheng@pku.edu.cn}{1}
\addauthor{Maoke Yang}{maokeyang@deepmotion.ai}{3}
\addauthor{Kuiyuan Yang}{kuiyuanyang@deepmotion.ai}{3}
\addauthor{Yunhai Tong}{yhtong@pku.edu.cn}{1}
\usepackage{amssymb}
\usepackage{amsmath}
\addinstitution{
Key Laboratory of Machine Perception, \\
School of EECS,\\
Peking University
}
\addinstitution{
Department of Engineering Science,\\
Torr Vision Group,\\
University of Oxford
}
\addinstitution{
DeepMotion AI Research
}

\runninghead{Li \etal}{Global Aggregation then Local Distribution}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}

\maketitle
\begin{abstract}
\noindent 
It has been widely proven that modelling long-range dependencies in fully convolutional networks (FCNs) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection. However, global aggregation is often dominated by features of large patterns and tends to oversmooth regions that contain small patterns (\eg, boundaries and small objects). 
To resolve this problem, we propose to first use \emph{Global Aggregation} and then \emph{Local Distribution}, which is called GALD, where long-range dependencies are more confidently used inside large pattern regions and vice versa. 
The size of each pattern at each position is estimated in the network as a per-channel mask map. 
GALD is end-to-end trainable and can be easily plugged into existing FCNs with various global aggregation modules for a wide range of vision tasks, and consistently improves the performance of state-of-the-art object detection and instance segmentation approaches. In particular, GALD used in semantic segmentation achieves new state-of-the-art performance on Cityscapes test set with mIoU 83.3\%. 
Code is available at: \url{https://github.com/lxtGH/GALD-Net}

\end{abstract}

\section{Introduction}
Detection and segmentation tasks have made steady progress with more powerful representations learned from Fully Convolutional Networks (FCNs). Since stacking more convolutional layers is not an effective way to achieve large receptive fields for long-range dependency modeling~\cite{zhou2014object,luo2016understanding}, several Global Aggregation (GA) modules have  been proposed to resolve this problem.

In contrast to a standard convolutional layer which aggregates features in a small local window, GA modules use long-range operators such as averaging pooling~\cite{pspnet, deeplabv3} and spatial-wise feature propagation over the whole image~\cite{Nonlocal,ocnet,DAnet}. FCNs coupled with GA modules have consistently improved basic FCNs especially for large objects. 

Unfortunately, the advantage of GA modules for large objects is a disadvantage for small patterns such as object boundaries and small objects, where features from GA modules tends to oversmooth the predictions for these small patterns. Thus, a straightforwards idea is using GA features conditionally on the pattern size of each position. Accordingly, we propose a Local Distribution (LD) module after a GA module (together as GALD for short) to adaptively distribute GA features at each position as illustrated in Fig.~\ref{fig:teaser}. The adaptive process is controlled by a set of mask maps, where each mask map is estimated from a feature map that records activations of some latent pattern over the whole image. 

\begin{figure*}
\centering
\includegraphics[width=0.99\linewidth]{fig/teaser.pdf}
\caption{Our proposed GALD framework for semantic segmentation task. The imbalanced spread of information from small and large patterns in GA module is appropriately handled through LD module.
}
\label{fig:teaser}
\end{figure*}

LD is a simple and universal module, and can be combined with existing GA modules to form different GALD modules for various detection and segmentation tasks. In our experiment, LD is verified on GA modules such as PSP~\cite{pspnet}, ASPP~\cite{deeplabv2}, Non-Local~\cite{Nonlocal} and CGNL~\cite{cgnl}, and achieves consistent performance improvement. We also extensively verify GALD on three vision benchmarks, including Cityscapes for semantic segmentation, Pascal VOC 2007 for object detection, and MS COCO for both object detection and instance segmentation, and all achieve notable improvement. In particular, for semantic segmentation evaluated on Cityscapes test set, GALD achieves mIoU of  with single model and ResNet101 as our backbone network, which surpasses all previously best published single-model results using ResNet101 as backbone network.


 \section{Related Work}
\label{sec:related}
To keep spatial information required by detection and segmentation tasks, convolutional networks designed for image classification are modified to FCNs by removing global information aggregation layers such as global average pooling layer and fully-connected layers~\cite{fcn}. To quickly increase receptive field size while keeping the spatial resolution, filters in top convolutional layers are enlarged by dilation~\cite{deeplabv1, deeplabv2}. 

To further enlarge the receptive field to the whole image, several methods are proposed recently. Global average pooled features are concatenated into existing feature maps in ~\cite{parsenet}. In PSPnet~\cite{pspnet}, average pooled features of multiple window sizes including global average pooling are upsampled to the same size and concatenated together to enrich global information. The DeepLab series of papers \cite{deeplabv1, deeplabv2, deeplabv3} propose atrous or dilated convolutions and atrous spatial pyramid pooling (ASPP) to increase the effective receptive field. DenseASPP~\cite{denseaspp} improves on \cite{deeplabv2} by densely connecting convolutional layers with different dilation rates to further increase the receptive field of network. In addition to concatenating global information into feature maps, multiplying global information into feature maps also shows better performance~\cite{encodingnet, cbam, cgnl, dfn}.In particular, EncNet \cite{encodingnet} and DFN \cite{dfn} use attention along the channel dimension of the convolutional feature map to account for global context such as the co-occurrences of different classes in the scene. CBAM\cite{cbam} explores channel and spatial attention in cascade way to learn task specific representation. 


Recently, advanced global information modeling approaches initiated from non-local network~\cite{Nonlocal} are showing promising results on scene understanding tasks. In contrast to convolutional operator where the information is aggregated locally defined by filters, the non-local operator aggregates information from the whole image based on an affinity matrix calculated among all positions around the image. Using non-local operator, impressive results are achieved in OCNet~\cite{ocnet},CoCurNet~\cite{CoCurrentNet}, DANet~\cite{DAnet}, A2Net~\cite{a2net}, CCnet~\cite{ccnet} and Compact Generalized Non-Local Net~\cite{cgnl}. OCNet~\cite{ocnet} uses non-local bolocks to learn pixel-wise relationship while CoCurNet~\cite{CoCurrentNet} adds extra global average pooling path to learn whole scene statistic. DANet~\cite{DAnet} explores orthogonal relationships in both channel and spatial dimension using non-local operator. CCnet~\cite{ccnet} models the long range dependencies by considering its surrounding pixels on the criss-cross path through a recurrent way to save both computation and memory cost. Compact Generalized non-local Net~\cite{cgnl} considers channel information into affinity matrix. Another similar work to model the pixel-wised relationship is PSANet~\cite{psanet}. It captures pixel-to-pixel relations using an attention module that takes the relative location of each pixel into account. 

Another way to get global representation is using graph convolutional networks, and do reasoning in a non-euclidean space~\cite{zhang2019dynamic,zhangli_dgcn,beyond_grids,graph_reason} where messages are passing between each node before projection back to each position. 
Glore~\cite{graph_reason} projects the feature map into interaction space using learned projection matrix and does graph convolution on projected fully connected graph. BeyondGrids~\cite{beyond_grids} learns to cluster different graph nodes and does graph convolution in parallel. DGCNet~\cite{zhangli_dgcn} proposes to use graph convolution network in both channel and spatial space to harvest different global context information.


All previous work focus on global context modeling, our work also utilizes global information modeling but takes a further step to better distribute the global information to each position, and further improves GA modules on both detection and segmentation tasks. \section{Method}


\subsection{Model Overview}
Our method, Global Aggregation (GA) then Local Distribution (LD), dubbed GALD, exploits the long-range contextual information of the feature   from a fully-convolution network (FCN), and then adaptively distributed the global context to each spatial and channel position of the output feature, .
To be noted, one can choose any one of the methods discussed in Section~\ref{sec:related} as GA.



\begin{figure*}
\centering
\includegraphics[width=0.99\linewidth]{fig/nets.pdf}
\caption{Schematic illustration of GALD, which contains two main components: Global Aggregation (GA) and Local Distribution (LD). 
GALD receives a feature map from the backbone network and outputs a feature map with same size with global information appropriately assigned to each local position.
}
\label{fig:whole}
\end{figure*}

\subsection{GALD}
\noindent{\textbf{Global Aggregation.}} 
To calculate a feature vector for each position, GA module takes feature vectors of  in a large window even the whole feature map depending on different GA designs. Take the Compact Generalized Non-Local (CGNL)~\cite{cgnl} as an example, similar to non-local~\cite{Nonlocal}, it aggregates contextual information from all spatial and channel positions in the same group. 
Specifically,  the global statistics are calculated for each group and multiplied back to the features in the same group, which forms . In our implementation, we downsample  by a factor of  for saving memory and computation cost without observing performance degradation, which also demonstrates the coarse property of global aggregation. 

Since GA modules calculate global statistics of features in large windows, they are easily biased towards features from large patterns as they contain more samples. Then the global information distributed to each position is also biased towards large patterns, which causes over-smoothing results for small patterns. One can refer to Section~\ref{sec:ablation} for more detailed visualization results.

\noindent{\textbf{Local Distribution.}} 
LD is proposed to adaptively use  considering patterns on each position. Without explicit supervision, the required patterns are latently described by  channels in . For each pattern , a spatial operator is learned to recalculate the spatial extent of the pattern in an image based on the activation map   sliced from . Intuitively, spatial operators for large patterns would shrink the spatial extent more while shrinking less even expand for small patterns.

The spatial operators for each pattern/channel is modeled as a set of depth-wise convolutional layers with  as input, i.e.,

where  contains the mask maps for each pattern and describes the recalculated spatial extents of each pattern,  is the sigmoid function,  is the weights of  those depth-wised convolutional filters with  as the downsampling rate by stride convolution. The output mask  is sensitive to both spatial and channel and it is upsampled using bilinear interpolation. With the mask maps ,  is refined into  by

where  the element-wise multiplication, and elements in  are weighted according the estimated spatial extent of each pattern at each position. In summary, LD predicts local weights  for each position of GA features
and avoids issues of coarse feature representation. 

As a common practice~\cite{pspnet}, original feature  and global aggregated feature  are concatenated together for final task-specific head, i.e.,

where  adds point-wise trade-off between global information  and local detailed information . Note that since the lack of details in GA, LD module only changes the proportion and distribution of coarse features in GA and leads to a fine-grained feature representation output .

\subsection{Overall Architecture}
Fig.~\ref{fig:whole} illustrates the overall architecture with GALD. For semantic segmentation, GALD is added right after a FCN, features from Eq.~\ref{eq:final_feature} are used for final prediction. To further boost the performance, Online Hard Example Mining (OHEM) loss~\cite{SegOHEM} is used for training, where only top-K ranked pixels according their losses are used during back-propagation.

For object detection and instance segmentation task, GALD is added at the end of stage4 of a ResNet backbone, FPN~\cite{fpn} is used to build a strong baseline with a feature pyramid for multi-scale object detection.  sits on top of FPN and passes information from the top-down pathway.

 \section{Experiment}
In this section, we verify GALD on three scene understanding tasks including semantic segmentation, object detection and instance segmentation. 



\subsection{Benchmarks} 
\textbf{Cityscapes:} Cityscapes~\cite{Cityscapes} is a benchmark that densely annotated for 19 categories in urban scenes, which contains 5000 fine annotated images in total and is divided into 2975, 500, and 1525 images for training, validation and testing, respectively. In addition, 20,000 coarse labeled images are also provided to enrich the training data. Images of this dataset are all with the same high resolution, i.e., . Following the standard protocol~\cite{Cityscapes}, mean Intersection over Union (mIoU) of all categories on validation set and test set is used for performance comparison. 

\noindent{\textbf{MS COCO:}} MS COCO~\cite{COCO_dataset} is built for detecting and segmenting objects found in everyday life in their natural environment. The dataset for detection consists of three sets for 80 common object categories, i.e., the training set has 118,287 images, validation set has 5,000 images and test-dev set has more than 20,000 images.

\noindent{\textbf{Pascal VOC:}} Pascal VOC~\cite{VOC} is a widely used public benchmark for semantic segmentation and object detection covering  object categories including the background. We use VOC 2007 and VOC 2012 trainval set as training set and report results on VOC 2007 test set. 

\begin{figure*}
\centering
\includegraphics[width=0.99\linewidth]{fig/ablation.pdf}
\caption{Ablation studies on combinations of GA and LD. (a)-(d) shows the different GA modules with LD. (e)-(g) shows the different arrangements of GA and LD. (h)-(f) represents using GA and LD respectively.
}
\label{fig:ablation}
\end{figure*}


\subsection{Implementation Details}

{\bfseries Semantic Segmentation } We employ Fully Convolutional Networks (FCNs) as baseline, where ResNet pretrained on ImageNet is chosen as the backbone following the same setting as PSPNet~\cite{pspnet}, the proposed GALD is appended to the backbone with random initialization. For optimization, we also keep the same setting as PSPNet, where mini-batch SGD with momentum 0.9 and initial learning rate 0.01 is used to train all models with 50K iterations, using mini-batch size of 8 and crop size of 769. During training, ``poly'' learning rate scheduling policy where power = 0.9 is used to adjust the learning rate. Synchronized batch normalization~\cite{encodingnet} is used for better mean / variance estimation across GPUs.

\noindent{{\bfseries Object Detection and Instance Segmentation}} For object detection and instance segmentation, mmdetection~\cite{mmdetection2018} is used as our baseline implementation for fair comparison. GALD is evaluated for object detection on Pascal VOC based on Faster R-CNN, and for both object detection and instance segmentation on MS COCO based on Mask R-CNN. FPN~\cite{fpn} is used as default setting in all these experiments. For fair comparison, we report all the results that we re-implemented in our framework.

\begin{table}[!t]
	\centering			
	\begin{minipage}{\dimexpr.40\linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{ l|c|c }
				\hline
				Method & mIoU(\%) &  \\
				\hline
				FCN (Baseline) & 73.7  & - \\
				\hline \hline
				+ASPP~\cite{deeplabv3} & 77.2 & 3.5  \\ 
				
				+NL~\cite{Nonlocal} & 78.0 & 4.3  \\
+PSP~\cite{pspnet} &  76.2 & 2.5  \\
+CGNL~\cite{cgnl} & \textbf{78.2} & \textbf{4.5}  \\
				\hline
			\end{tabular}
		}
		\par
		{\footnotesize(a) Ablation study on different GA modules using ResNet50 as backbone.}
	\end{minipage}
	\begin{minipage}{\dimexpr.52 \linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{ l|c|c|c }
				\hline
				Method & mIoU(\%) &  & \\
				\hline
				FCN (Baseline) & 73.7  & - & -\\
				\hline
				\hline
				+LD & 77.5 & 3.8  & - \\
				+PSP + LD & 78.9 & 5.2  & \textbf{2.7}  \\ 
				+ASPP + LD & 79.5 & 5.4  & 2.3  \\
				+NL + LD & 79.2 & 5.3  & 1.2  \\
+CGNL + LD & \textbf{79.6} & \textbf{5.9 } & 1.4  \\
\hline
			\end{tabular}
		}\par
		{\footnotesize(b) Ablation study on LD applied on different GA modules using ResNet50 as backbone.}
		
	\end{minipage}
	\vspace{3mm}
	
	\begin{minipage}{\dimexpr.40 \linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{ l|c|c}
				\hline
				Method & mIoU(\%) & \\
				\hline
				FCN (Baseline) & 73.7  & - \\
				\hline
				\hline
				+Parallel & 77.5 & 3.8 \\ 
				+LDGA  & 78.1 & 4.4  \\
				+GALD  & \textbf{79.6} & \textbf{5.9 }\\
				\hline
			\end{tabular}
		}\par
		{\footnotesize(c) Ablation study on different arrangements of GA and LD using ResNet50 as backbone.}
		
	\end{minipage}	
	\begin{minipage}{\dimexpr.50 \linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{ l|c|c|c}
				\hline
				Method & mIoU(\%) & Backbone & \\
				\hline
				FCN (Baseline) & 73.7 & ResNet50  & -\\
				FCN (Baseline) & 75.3 & ResNet101 & - \\
				\hline
				\hline
				+CGNL & \textbf{79.7} & ResNet101 &  4.4   \\
				+CGNL+LD  & \textbf{79.6} & ResNet50 & \textbf{5.9 }\\
				+PSP & 78.6 & ResNet101 & 4.9  \\
				+PSP+LD & 78.9 & ResNet50 & 5.2  \\
				\hline
			\end{tabular}
		}\par
		{\footnotesize(d) Ablation study on different backbones.}
		
	\end{minipage}
	\vspace{3mm}
	
	\begin{minipage}{\dimexpr.50 \linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{ l|c|c}
				\hline
				Method & mIoU(\%)  & \\
				\hline
				FCN (Baseline) & 73.7  & -\\
				FCN + CGNL & 78.2 & - \\
				\hline
				\hline 
				+CGNL+LD(depth-wise convolution)  & \textbf{79.6} & \textbf{1.4 }\\
				+CGNL+LD(bilinear interpolation) & 77.6  & 0.6  \\
				+CGNL+LD(average pooling) & 76.5  & 1.7  \\
				\hline
			\end{tabular}
		}\par
		{\footnotesize(e) Ablation study on downsampling strategies for mask estimation in LD using ResNet50 as backbone, where the downsamping ratio is 8.}
	\end{minipage}
	\vspace{3mm}
	
	\caption{Comparison results on Cityscapes validation set, where  denotes the performance difference comparing with baseline, and  denotes performance difference between using GALD module and the corresponding GA module.
		All methods are evaluated with single-scale crop test.}
	\vspace{-5mm}
	\label{tab:city_ablation}
\end{table}



\subsection{Results on Cityscapes}
Two groups of experiments are conducted on Cityscapes, the first group of experiments verifies the effectiveness of our GALD framework by ablation studies.  The second group of experiments compares GALD to the state-of-the-art methods.




\subsubsection{Ablation Studies }
\label{sec:ablation}
\textbf{Comparison with baseline} 
We explore our LD module with four different GA modules as illustrated in Fig.~\ref{fig:ablation} (a)-(d).
Table~\ref{tab:city_ablation}(a) first reports the performances of adding four GA modules to the baseline FCN, where all methods are using the same backbone ResNet50 for fair comparison.  Obviously, all GA modules significantly improves the baseline FCN on semantic segmentation task, where CGNL performs better than other three GA modules.
Table~\ref{tab:city_ablation}(b) reports the results by adding our proposed LD module. Directly using LD alone improves the baseline FCN by 3.8\%, which demonstrates that features from FCN have the similar problem as features from GA modules. LD together with four different GA modules consistently improves the corresponding GA module. Comparing with baseline, the combination of CGNL+LD achieves the best performance, and we mainly choose CGNL as our GA module in following experiments.



\begin{figure*}
	\centering
	\includegraphics[width=1.0\linewidth]{fig/res_mask.pdf}
	\caption{
	    Comparison of mask maps learned in different arrangements of GA and LD. The mask maps are calculated by the mean of  along channel dimension.
		Best view in color.}
	\label{fig:attenion_mask_2}
\end{figure*}

\noindent{{\bfseries Arrangements of LD and GA}} Considering LD module can also improve the baseline, we further study different arrangements of LD and GA as illustrated in Fig.~\ref{fig:ablation} (e)-(g). (f) and (g) represent LDGA and Parallel in Table.~\ref{tab:city_ablation}(c) respectively. LDGA means first doing LD then doing GA while Parallel concatenates the output of LD and GA.
Table.~\ref{tab:city_ablation}(c) reports the results of the three different arrangements, where all improve the baseline and GALD achieves best result. Fig.~\ref{fig:attenion_mask_2} shows the mask maps learned in LDGA and GALD, where mask maps
learned by GALD are more focused on regions inside large objects then weight global features more in these regions, while mask maps from LDGA have no obvious focus on large objects since the LD module has not accessed to global feature yet.


\noindent{\bfseries Compared with stronger backbone} To further prove the effectiveness of our method, we compare GALD using ResNet50 as backbone with a stronger backbone ResNet101 in Table~\ref{tab:city_ablation}(d). Our method achieves similar performance improvement comparing GA modules with stronger backbone which further prove the effectiveness of LD module.

\noindent{\bfseries Comparison with different downsampling strategies} We also explore three different downsampling strategies for LD, including average pooling, bilinear interpolation and depth-wise stride convolution. Table~\ref{tab:city_ablation}(e) reports the comparison results, depth-wise stride convolution achieves the best result, while average pooling and bilinear interpolation even slightly degrades the performance, which shows that the learnable filters for each channel is important to refine the features from the GA module.

\noindent{\bfseries Visualization of GALD}
To further study the features at different stages, we add another two segmentation heads on features outputted from FCN and GA respectively, the model is fine tuned until converge to analyze segmentation ability of features from different stages. Figure~\ref{fig:attenion_mask} compares the segmentation results, segmentation based on GA resolves the ambiguities in FCN features but also tends to over smoothing regions of small patterns which are shown in red boxes. Segmentation of GALD keeps the global structure of GA while refines back the details. 


\subsubsection{Comparison with state-of-the-art}

\begin{table}
	\centering
	\begin{minipage}{\dimexpr.40\linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{l|c|c}
					\hline
					Method & Backbone & mIoU(\%)  \\
					\hline
					SAC~\cite{sac}\textdagger & ResNet101 &  78.1 \\ 
					AAF~\cite{aaf}\textdagger  & ResNet101 &  79.1 \\ 
					BiSeNet~\cite{bisenet}\textdagger & ResNet101 &  78.9 \\ 
					PSANet~\cite{psanet}\textdagger & ResNet101 &  80.1 \\ 
					DFN~\cite{dfn}\textdagger & ResNet101 &  79.3 \\ 
					DenseASPP~\cite{denseaspp}\textdagger & DenseNet161 & 80.6 \\
					Glore~\cite{graph_reason}\textdagger & ResNe50 & 79.5 \\
					Glore~\cite{graph_reason}\textdagger & ResNet101 & 80.9 \\
					DAnet~\cite{DAnet}\textdagger & ResNet101 & 81.5 \\
					\hline
					GALDNet\textdagger & ResNet50 & \textbf{80.8}  \\ 
					GALDNet\textdagger & ResNet101 & \textbf{81.8} \\
					\hline
				\end{tabular}
		}
		\par
		{\footnotesize(a) Results on Cityscapes test server trained with fine-data.}
	\end{minipage}
	\begin{minipage}{\dimexpr.52 \linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{l|c|c}
				\hline
				Method & Backbone & mIoU(\%)  \\
				\hline
				PSP~\cite{pspnet}\textdaggerdbl & ResNet101 &  81.2 \\ 
				Deeplabv3+~\cite{deeplabv3p}\textdaggerdbl & Xception & 82.1 \\
				DPC~\cite{DPC}\textdaggerdbl & Xception & 82.6 \\
				Auto-Deeplab~\cite{auto-deeplab}\textdaggerdbl & - & 82.1 \\
\hline\hline
				GALDNet\textdaggerdbl & ResNet101 & \textbf{82.9} \\
				\hline
				GALDNet(+Mapillary)\textdaggerdbl & ResNet101 & \textbf{83.3} \\
				\hline
			\end{tabular}
		}\par
		{\footnotesize(b) Results on Cityscapes test server trained with both fine and coarse data}
		
	\end{minipage}
	\vspace{3mm}
	\caption{State-of-the-art comparison experiments on Cityscapes test set. \textdagger means training with only the train-fine dataset. \textdaggerdbl means training with both the train-fine and coarse data}
	\label{tab:cityscapes_results}
\end{table}




We further compare our results with other state-of-the-art methods in this section. We choose dilated ResNet50 and ResNet101 as backbone models. The results are summarized in Table~\ref{tab:cityscapes_results}. For fair comparison, we first compare methods trained with only fine annotation data in Table~\ref{tab:cityscapes_results}(a), and then compare the results with other methods using extra training data in Table~\ref{tab:cityscapes_results}(b). Following~\cite{pspnet}, multi-scale crop test is used for final test submission.
As illustrated, our method surpasses all previous methods. In particular, our model based on a weak backbone ResNet50 can still achieve comparable performance, which is higher than most methods with stronger backbone. By using extra coarse annotation data for training, our method achieves 82.9\% mIoU, which also surpasses the state-of-the-art methods. By further adding Mapillary~\cite{mapillary} as training data, the proposed method achieves 83.3\% mIoU based on ResNet101. To the best of our knowledge, this is the first single model using ResNet101 as backbone that surpasses 83\% mIoU on Cityscapes test server. More detailed per-class results, visualization results and training settings can be referred to the supplementary material. 


\begin{figure*}
	\centering
	\includegraphics[width=1.0\linewidth]{fig/mul_seg_res.png}
	\caption{
		Visualization of different parts output results in one model.(a), input images; (b),results after FCN's outputs; (c), results after GA module's outputs; (d), results after GALD module'ss outputs;
		(e), ground truth. Yellow boxes highlight regions that GA can handle global semantic consistency, while red boxes highlight regions that LD can recover more detailed information.
		Best view in color.}
	\label{fig:attenion_mask}
\end{figure*}


\subsection{Results on Pascal VOC and COCO dataset}
 { \bfseries Pascal VOC:} 
 We perform experiments on the PASCAL VOC 2007 data set to evaluate the effect of GALD for object detection. We train all the models on the union set of VOC 2007 trainval and VOC 2012 trainval (07+12) for 14 epochs with weight decay of 0.0001 and momentum of 0.9. 
 For comparison, experiments of non-local block \cite{Nonlocal} are also summarized and are denoted as NL. 
 As results listed in Table~\ref{tab:voc_coco}(a), GALD consistently improves detection accuracy over the strong baseline Faster-RCNN using both ResNet50 and ResNet101 as backbone, which demonstrates the effectiveness of GALD for object detection. 

\noindent{ {\bfseries COCO:}} 
To further verify the generality of GALD, we conduct the experiments on instance segmentation task on MS COCO based on the state-of-the-art method Mask R-CNN. 
Table~\ref{tab:voc_coco}(b) summarizes the AP of bounding box (AP-box) and AP of mask (AP-mask) evaluated on COCO minival. GALD improves the baseline by about 1\% regardless the used backbone. 
Figure~\ref{fig:seg_results}(b) compares the object detection and instance segmentation results of our method with baseline. With GALD, Mask R-CNN can find objects that are missed in baseline (e.g., the ``light'' in the third column), resolve ambiguity in region classification (e.g., the ``bed'' in the first column) and help to better estimate the spatial contents for objects (e.g., ``bear'' in last column).



\begin{table}
	\centering
	\begin{minipage}{\dimexpr.40\linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{l|c|c}
				\hline
				Backbone& Detector & mAP@.5  \\
				\hline
				ResNet50 & Faster-RCNN & 80.6   \\
ResNet50 & + NL & 81.3  (0.7 ) \\
ResNet50 & +CGNL & 81.1 (0.5 ) \\
				ResNet50 & + GALD  & \textbf{81.5}  (0.9 ) \\
				\hline \hline
				ResNet101 & Faster-RCNN & 80.7  \\
ResNet101 & + NL & 82.3  (1.6 )  \\
ResNet101 & + GALD & \textbf{83.0}  (2.3 ) \\
				\hline
			\end{tabular}
		}
		\par
		{\footnotesize(a) Object detection results on VOC 2007 test set measured by mAP(\%),  Faster-RCNN with FPN serves as the baseline.}
	\end{minipage}
	\begin{minipage}{\dimexpr.50 \linewidth}
		\centering
		\tiny
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{l|c|c|c}
				\hline
				{Backbone} & {Detector} & {AP-box} & {AP-mask} \\
				\hline
				ResNet50 & Mask-RCNN & 38.2  & 34.8 \\
ResNet50 & + NL & 39.0  (0.8 )   & 35.3  (0.8 )  \\
				ResNet50 & + CGNL & 38.9 (0.7 ) & 35.4 (0.6) \\ 
ResNet50 & + GALD  & \textbf{39.2}  (1.0 )  & \textbf{35.6} (1.1 )  \\
				\hline
				\hline
				ResNet101 & Mask-RCNN & 40.2 & 36.3 \\
ResNet101 & + NL & 40.9   (0.7 )  & 37.2   (0.9 ) \\
ResNet101 & + GALD & \textbf{41.1}   (0.9 ) & \textbf{37.8} (1.5 ) \\
				\hline
			\end{tabular}
		}\par
		{\footnotesize(b) Results of object detection and instance segmentation on COCO dataset. Our method can improve Mask-RCNN baseline by around 1\%  across different backbones.}
		
	\end{minipage}
	\vspace{3mm}
\caption{Results on Pascal VOC dataset (a) and MS COCO dataset (b). }
	\label{tab:voc_coco}
\end{table}


\begin{figure}
	\centering
		\includegraphics[width=0.68\textwidth]{fig/coco.pdf}
	\caption{Comparison of object detection and instance segmentation results on MS COCO.}
	\label{fig:seg_results}
\end{figure}

\iffalse
\vspace{8mm}
\begin{figure}
	\centering
	\begin{tabular}{cc}
		\bmvaHangBox{\includegraphics[width=0.48\textwidth]{fig/seg_fcn_vis.png}}&
		\bmvaHangBox{\includegraphics[width=0.48\textwidth]{fig/coco.pdf}}\\
		(a) FCN with ResNet50 as backbone. & (b) Mask-RCNN with ResNet50 as backbone.
	\end{tabular}
	\caption{(a) Visualization of results on Cityscapes. (b) Visualization of results on MS COCO.}
	\label{fig:seg_results}
\end{figure}
\fi \section{Conclusion}
In this paper, we propose GALD to adaptively distribute global information to each position for scene understanding tasks. In contrast to existing methods that assign global information uniformly to each position and cause the problem of blurring, GALD learns a set of mask maps to distribute global information adaptively according pattern distributions over the image. GALD benefits from both the GA module for ambiguity resolving and LD module for detail refinement. Extensive experiments verify the universality of GALD in improving the performance of semantic segmentation, object detection and instance segmentation. 
In the future, we will study the effectiveness of GALD for more vision tasks where both global and local information are important such as depth estimation. 




\section*{Acknowledgments}
We gratefully acknowledge the support of DeepMotion AI Research for providing the computing resources in carrying out this research.
LZ is supported by EPSRC Programme Grant Seebibyte EP/M013774/1.

\section{Appendix}
\subsection{Detailed Results on Cityscapes}
\subsubsection{Training with Coarse labeled Data}
Ctiyscapes provides about 20000 coarse labeled images for training. To verify the 
both capacity and generality, we further fine tune our model on coarse data set.
Different from training on fine data set, we set batch size 16 and fix batch normalization layers in our model for about 50000 iterations using larger crop size. Then we fine tune the model back on the fine data set for 10000 iterations with lower learning rate. When we submit on test server, we use multi scale crop test with flip images input to get more accurate results. Finally we get better results and our single model can achieve 82.9\% mIoU.
\subsubsection{Training with Mapillary Data}
Mapillary~\cite{mapillary} is another city scene data containing 65 different labels. Here we only use 19 classes of 65 labels which are in cityscape category. Again we following the same steps in previous part,
we get more accurate model and our single model with ResNet101 as backbone can achieve 83.3\% mIoU ranked 3-rd in cityscapes leaderboard by the time of paper publication.
\subsubsection{Detailed Results}
Models trained with only fine-data set are shown in Table~\ref{tab:cityscapes_results_detail_fine}.
Models trained with extra data sets including COCO and Mapillary data sets are shown in Table~\ref{tab:cityscapes_results_detail_coarse}.

\begin{table*}[t]
	\scriptsize
	\centering
	\setlength{\tabcolsep}{0.9pt}
	\begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c}
		\hline
		Method & road & swalk & build & wall & fence & pole & tlight & sign & veg. & terrain & sky & person & rider & car & truck & bus & train & mbike & bike & mIoU \\
		\hline
		ResNet38~\cite{resnet38} & 98.5 & 85.7 & 93.0 & 55.5 & 59.1 & 67.1 & 74.8 & 78.7 & 93.7 & 72.6 & 95.5 & 86.6 & 69.2 & 95.7 & 64.5 & 78.8 & 74.1 & 69.0 & 76.7 & 78.4 \\
		PSPNet~\cite{pspnet} & 98.6 & 86.2 & 92.9 & 50.8 & 58.8 & 64.0 & 75.6 & 79.0 & 93.4 & 72.3 & 95.4 & 86.5 & 71.3 & 95.9 & 68.2 & 79.5 & 73.8 & 69.5 & 77.2 & 78.4\\
		AAF~\cite{aaf} & 98.5 & 85.6 & 93.0 & 53.8 & 58.9 & 65.9 & 75.0 & 78.4 & 93.7 &
		72.4 & 95.6 & 86.4 & 70.5 & 95.9 & 73.9 & 82.7 & 76.9 & 68.7 & 76.4 & 79.1 \\
		SegModel~\cite{segmodel} & 98.6 & 86.4 & 92.8 & 52.4 & 59.7 & 59.6 & 72.5 & 78.3 & 93.3 & \textbf{72.8} & 95.5 & 85.4 & 70.1 & 95.6 & 75.4 & 84.1 & 75.1 & 68.7 & 75.0 & 78.5 \\
		DFN~\cite{dfn} & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 79.3 \\
		BiSeNet~\cite{bisenet} & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 78.9 \\
		PSANet~\cite{psanet} & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 80.1 \\
		DenseASPP~\cite{denseaspp}  & 98.7 & 87.1 & 93.4 & 60.7 & 62.7 & 65.6 & 74.6 & 78.5 & 93.6 & 72.5 & 95.4 & 86.2 & 71.9 & 96.0 & \textbf{78.0} & 90.3 & 80.7 & 69.7 & 76.8 & 80.6 \\
\hline
		Ours(ResNet50) & \textbf{98.7} & 86.8 & 93.4 & 57.6 & \textbf{63.1} & 68.7 & 76.1 & 80.3 & 93.6 & 72.3 & 95.4 & 87.0 & 72.2 & 96.1 & 75.4 &88.2 & 77.8 & 68.8 & 76.4 & \textbf{80.8} \\
		\hline
		Ours(ResNet101) & \textbf{98.7} & \textbf{87.2} & \textbf{93.8} & \textbf{59.3} & 61.9 & \textbf{71.4} & \textbf{79.2} & \textbf{82.0} & \textbf{93.9} & \textbf{72.8} & \textbf{95.6} & \textbf{88.4} & \textbf{74.8} & \textbf{96.3} & 74.1 & \textbf{90.6} & \textbf{81.1} & \textbf{73.4} & \textbf{79.8} & \textbf{81.8} \\
		\hline
	\end{tabular}
	\caption{Per-category results on Cityscapes test set. Note that all the models are trained with only fine-data .}
	\label{tab:cityscapes_results_detail_fine}
\end{table*}


\begin{table*}[t]
	\scriptsize
	\centering
	\setlength{\tabcolsep}{1.0pt}
	\begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c}
		\hline
		Method & road & swalk & build & wall & fence & pole & tlight & sign & veg. & terrain & sky & person & rider & car & truck & bus & train & mbike & bike & mIoU \\
		\hline
		PSPNet~\cite{pspnet} & 98.7 & 86.9 & 93.5 & 58.4 & 63.7 & 67.7 & 76.1 & 80.5 & 93.6 & 72.2 & 95.3 & 86.8 & 71.9 & 96.2 & 77.7 & 91.5 & 83.6 & 70.8 & 77.5 & 81.2 \\
		\hline
		ResNet38~\cite{resnet38}& 98.7 & 86.9 & 93.3 & 60.4 & 62.9 & 67.6 & 75.0 & 78.7 & 93.7 & 73.7 & 95.5 & 86.8 & 71.1 & 96.1 & 75.2 & 87.6 & 81.9 & 69.8 & 76.7 & 80.6 \\
		\hline
		InPlaceABN~\cite{inplaceabn}& 98.4 & 85.0 & 93.6 & 61.7 & 63.9 & 67.7 & 77.4 & 80.8 & 93.7 & 71.9 & 95.6 & 86.7 & 72.8 & 95.7 & 79.9 & \textbf{93.1} & \textbf{89.7} & 72.6 & 78.2 & 82.0 \\
		\hline
		DeepLabV3+~\cite{deeplabv3p}& 98.7 & 87.0 & 93.9 & 59.5 & 63.7 & 71.4 & 78.2 & 82.2 & 94.0 & 73.0 & 95.8 & 88.0 & 73.0 & 96.4 & 78.0 & 90.9 & 83.9 & 73.8 & 78.9 & 82.1 \\
		\hline
		Auto-Deeplab~\cite{auto-deeplab}& \textbf{98.8} & \textbf{87.6} & 93.8 & 61.4 & 64.4 & 71.2 & 77.6 & 80.9 & 94.1 & 72.7 & 96.0 & 87.8 & 72.8 & 96.5 & 78.2 & 90.9 & 88.4 & 69.0 & 77.6 & 82.1 \\
		\hline 
		DPC~\cite{DPC}& 98.7 & 87.1 & 93.8 & 57.7 & 63.5 & 71.0 & 78.0 & 82.1 & 94.0 & 73.3 & 95.4 & 88.2 & 74.5 & 96.5 & \textbf{81.2} & 93.3 & 89.0 & \textbf{74.1} & 79.0 & 82.6 \\
		\hline
		DRN-CRL~\cite{DRN} & \textbf{98.8} & 87.7 & 94.0 & 65.1 & 64.2 & 70.1 & 77.4 & 81.6 & 93.9 & \textbf{73.5} & 95.8 & 88.0 & 74.9 & 96.5 & 80.8 & 92.1 & 88.5 & 72.1 & 78.8 & 82.8 \\ 
		\hline
		Ours(ResNet101) & \textbf{98.8} & 87.5 & 94.0 & \textbf{65.3} & 66.4 & 71.0 & 77.6 & 81.0 & 94.0 & 72.6 & 95.9 & 87.6 & 75.0 & 96.3 & 80.2 & 90.3 & 87.9 & 72.9 & 78.9 & \textbf{82.9} \\ 
		\hline
		\hline
		Ours+ Mapillary & \textbf{98.8} & \textbf{87.7} & \textbf{94.2} & 65.0 & \textbf{66.7} & \textbf{73.1} & \textbf{79.3} &\textbf{82.4} & \textbf{94.2} & 72.9 & \textbf{96.0} & \textbf{88.4} & \textbf{76.2} & \textbf{96.5} & 79.8 & 89.6 & 87.7 & 74.0 & \textbf{80.0} & \textbf{83.3} \\
		\hline
	\end{tabular}
	\caption{Per-category results on Cityscapes test set trained with coarse data and Mapillary. Our model achieves the state of art results comparing with other methods using more stronger backbone.ã€€Our method achieves better results than those use stronger backbone~\cite{DRN,inplaceabn}.
	}
	\label{tab:cityscapes_results_detail_coarse}
\end{table*}        

\subsection{Detailed Results on VOC2007}
Here we give the detailed detection results on VOC2007 shown in  and compared the results with previous detection methods, ours model achieves considerable results.

\subsection{More Visible Results on Cityscapes and COCO}
Here we show more results on Cityscapes and COCO dataset. 
COCO results are shown in Figure ~\ref{fig:more_coco}. 
Cityscapes results are shown in Figure ~\ref{fig:more_city}

\begin{table*}[t]
	\tiny
	\centering
	\setlength{\tabcolsep}{1.0pt}
	\begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c c | c}
		\hline
		Method & aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & table & dog & house& mbike & person & plant & sheep & sofa & train & tv & mAP(\%) \\
		\hline
		R-FCN\cite{R-FCN} & 79.9 & 87.2 & 81.5& 72.0& 69.8& 86.8& 88.5& 89.8& 67.0& \textbf{88.1}& 74.5& 89.8& \textbf{90.6}& 79.9& 81.2& 53.7& 81.8& 81.5& 85.9& 79.9& 80.5 \\ 
		\hline
		DSSD\cite{dssd} & 86.6& 86.2& 82.6& \textbf{74.9}& 62.5& \textbf{89.0}& 88.7& 88.8& 65.2& 87.0 & 78.7& 88.2& 89.0& 87.5& 83.7& 51.1& 86.3& \textbf{81.6}& 85.7& \textbf{83.7} & 81.5\\ 
		\hline
		DFPR\cite{dfpr} &  \textbf{92.0}& \textbf{88.2}& 81.1& 71.2& 65.7& 88.2& 87.9& \textbf{92.2}& 65.8& 86.5& \textbf{79.4}& \textbf{90.3}& 90.4& \textbf{89.3}& \textbf{88.6}& 59.4& \textbf{88.4}& 75.3& \textbf{89.2}& 78.5& 82.4\\ 
		\hline
		Faste-RCNN (base)\cite{faster-rcnn} & 86.5& 85.9& 82.9& 70.4& 70.4& 83.3& 88.1& 88.6& 66.0& 82.5& 74.6& 89.1& 87.1& 83.4& 85.8& 58.5& 84.8& 79.2& 85.9& 77.5 & 80.7\\ 
		\hline
		\hline    
		Ours & 86.7 & 87.7 & \textbf{85.3} & 74.5 & \textbf{74.4} & 86.1 & \textbf{89.0} & 89.5 & \textbf{71.2} & 87.5 & 77.5 & 89.0 & 87.9 & 85.4 & 86.5 & \textbf{59.8} & 86.1 & 80.8 & 87.6 & 83.2 & \textbf{83.0} \\ 
		\hline
	\end{tabular}
	\caption{PASCAL VOC 2007 test detection results. All models are trained with 07+12
		(07 trainval + 12 trainval). All the models are using ResNet101 as backbone.
	}
	\label{tab:VOC2007}
\end{table*}  






\begin{figure*}
	\centering
	\includegraphics[width=0.99\linewidth]{fig/coco_more.png}
	\caption{More detection and Segmentation Results on COCO
		First row: Mask-RCNN; Second row: + GALD
	}
	\label{fig:more_coco}
\end{figure*}


\begin{figure*}
	\centering
	\includegraphics[width=0.99\linewidth]{fig/city_res.png}
	\caption{More cityscapes results: (a),input (b),FCN-res50 (c),+LD (d),+GA (e), +GALD (f),ground truth
	}
	\label{fig:more_city}
\end{figure*}
 
\bibliography{egbib}
\end{document}
