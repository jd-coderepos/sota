\section{Experiments}
\label{sec:experiments}

We now use \OURS to evaluate text-to-image diffusion models.
In~\cref{subsec:exp:benchmark}, we introduce our benchmark datasets and models, and present the \OURS results in~\cref{subsec:exp:main_exp}.

\subsection{Benchmark and Evaluation}\label{subsec:exp:benchmark}
In \OURS, we pose the text faithfulness evaluation as an image-text matching problem, where the goal is to pick the right image caption pair among distractors.
\par \noindent \textbf{Tasks.}
We identify and divide the evaluation into six broad reasoning tasks (illustrated in~\cref{exp:fig:data_fig}): 1) \taskStyle{Attribute binding},
 2) \taskStyle{Color}, 3) \taskStyle{Count}, 4) \taskStyle{Shape}, 5) \taskStyle{Spatial relationships}, and 6) \taskStyle{Text corruption}.
Each of these tasks evaluate the model's understanding of a specific aspect of text faithfulness and is similar to the categories of prompts from DrawBench~\cite{Saharia2022PhotorealisticTD}. 
The six tasks are constructed using data from TIFA~\cite{hu2023tifa}, CLEVR~\cite{Johnson2016CLEVRAD} and ARO~\cite{yuksekgonul2023when}.
\par \noindent \textbf{Datasets.}
\noindent \textbf{TIFA}~\cite{hu2023tifa} consists of 4000 text prompts, collected manually and from image captioning datasets, to evaluate the text faithfulness of generative models.
In our evaluation, we use $\sim$2000 of these text-prompts that are constructed from the COCO~\cite{2014-lin} dataset and convert the dataset from question-answering to an image-text matching format as detailed in the supplement.
\noindent \textbf{Attribution, Relation and Order (ARO)}~\cite{yuksekgonul2023when} is a benchmark that uses data from Visual Genome~\cite{vg} for attribute and spatial relations, and COCO for ordering tasks.
\noindent \textbf{CLEVR}~\cite{Johnson2016CLEVRAD} is a benchmark for compositional understanding and visual reasoning that uses synthetic images.
We use the splits proposed by~\cite{Lewis2022DoesCB} for our experiments.

\noindent
We divide the datasets among all the reasoning task as follows. For attribute binding, we combine samples from ARO (attribution) and CLEVR. For colors and counts, we use corresponding samples from TIFA and CLEVR.
For shapes, use samples from CLEVR.
Data for spatial relationships is from TIFA, CLEVR and ARO (relations).
The data for the text corruption task is from the ARO (order sensitivity) dataset.
A sample of each task consists of an image and multiple text prompts and the performance on the task is the classification accuracy of pairing the image with the right caption.

We measure the performance of text-to-image generative models on the benchmark using the following evaluation methods.
We provide full details for each of the methods in the supplement.

\par \noindent \textbf{\OURS (Ours)} is an automatic evaluation method and uses both the images and text from our benchmark introduced in~\cref{subsec:exp:benchmark}.
For each benchmark task, we randomly sample $1000$ examples and evaluate the classification performance on them. We repeat this three times and the report the mean accuracy.
We use 10 trials (\ie$N=10$) and perform diffusion for $100$ steps (\ie$T=100$) for all the models. Refer to the supplement for ablation experiments on $N$, $T$.

\par \noindent \textbf{Human evaluations} are the gold standard for judging the performance of text-to-image models using pairwise comparsions.
We present humans with generations from two models and ask them to vote for one of four choices: ``both'' the generations are faithful, ``none'' of them are faithful, or if only one of the two images (``Image 1'' or ``Image 2'') demonstrates fidelity to the given prompt.
For simplicity, we only report votes where there is a clear preference for a model.
We randomly pick $250$ text prompts from each benchmark task as conditioning for human evaluation and the images are generated using DDIM~\cite{song2021denoising} sampling, with $100$ denoising steps.
Note that unlike \OURS, human evaluations do \emph{not} use the real images from the benchmark tasks and the human evaluators only look at the generated images.

\subsubsection{Models}\label{subsubsec:exp:benchmark:models}
We use models with different image representations: pixel diffusion models which directly use the pixel RGB values, and latent diffusion models where the image is projected into a latent space using an auto-encoder.
We pick models trained with different text encoders within each class.
This enables us to analyze the effect of text encoder on the final performance within each class.

\par \noindent{\textbf{Diffusion models with CLIP text encoder.}} For latent diffusion, we use a model trained with OpenCLIP~\cite{ilharco_gabriel_2021_5143773} text encoder with a ViT-H/14 backbone via an API containing open-sourced model weights. 
This model is trained on a public dataset with 5 billion images, excluding explicit material, and outputs $512\times512$ images.
For pixel diffusion, we adopt the architecture of DALLE-2~\cite{ramesh2022hierarchical} for our experiments and train a model.
We use a CLIP (ViT-L/14) text encoder and produce images of resolution $64\times 64$.
Our model has a total of 4.2B parameters and is trained for 2M steps on an internal image-text dataset (\DATASET).

\noindent{\textbf{Diffusion models with T5 text encoder.}} For latent diffusion, we train a UNet model, similar to~\cite{rombach2022high}, but replace the CLIP text encoder with a T5 XXL~\cite{2020t5} text encoder
that outputs images of resolution $256\times256$.
This model is also trained on \DATASET for 2M steps using a latent space with a $4\times$ downsampling factor and has a total of 5.8B parameters.
We train a 7.5B parameter pixel diffusion model, similar to Imagen~\cite{Saharia2022PhotorealisticTD}, on inputs of resolution $64\times64$ for 2M steps also on \DATASET.
Subsequently, we apply a super resolution model to upsample the output to $512\times 512$.

With the exception of the CLIP-based latent diffusion model~\cite{rombach2022high}, all the other models are trained for the same number of steps on the exact same data to ensure fair comparison.
\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.24\linewidth]{figures/he_d2vi.pdf}\hfill
        \includegraphics[width=0.24\linewidth]{figures/ours_d2vi_cls.pdf}\label{exp:fig:main_res_pdm}
        \centering
        \includegraphics[width=0.24\linewidth]{figures/he_xivsd.pdf}\hfill
        \includegraphics[width=0.24\linewidth]{figures/ours_xivsd_cls.pdf}

        \label{exp:fig:main_res_ldm}
    \end{subfigure}
    \caption{\textbf{Evaluating text-to-image models} using human evaluations and \OURS. We evaluate different types of text-to-image models such as pixel diffusion (first two columns) and latent diffusion model (last two columns), and models that use different text encoders such as T5 XXL and CLIP. We observe that across all 4 diffusion models the relative ordering given by \OURS's accuracy correlates with the pairwise human evaluation results. We also observe that latent diffusion models have a higher \OURS accuracy than pixel diffusion models suggesting better text-faithfulness.
    Using the stronger T5 text encoder leads to better performance across human evaluations and \OURS.
    }\vspace{-2em}
    \label{exp:fig:main_res_all}
\end{figure*}

\subsection{Main results}\label{subsec:exp:main_exp}

We evaluate the four text-to-image models and report results in~\cref{exp:fig:main_res_all}.
For \OURS, we report the accuracy difference with the random chance accuracy, since each of the tasks has a different degree of difficulty.






\par \noindent \textbf{Agreement between \OURS and human evaluation.}
We use both human evaluation and \OURS to evaluate the four different diffusion models in~\cref{exp:fig:main_res_all}.
Human evaluation performance, measured using pairwise comparison, follows the same ranking as given by \OURS when comparing both types of pixel diffusion models and both types of latent diffusion models.
To the best of our knowledge, ours is the first work to establish correlation between the discriminative performance of generative models and human evaluation for text-to-image diffusion models across a wide range of models and tasks.
The high degree of alignment between \OURS and human evaluation suggests that \OURS is a reliable and interpretable way to evaluate and compare the text faithfulness of different diffusion models.

Next, we use \OURS to further analyze the performance of diffusion models.

\par \noindent \textbf{Effect of the text encoder.}
Comparing the different text-encoders used in~\cref{exp:fig:main_res_all}, we observe that diffusion models using the stronger T5 text encoder perform better on most tasks than the ones using the CLIP text encoder.
The stronger performance of T5-based models holds for both human evaluations and \OURS.
The \OURS results also show that diffusion models using the CLIP based encoders have poor performance, worse than random chance, on the \taskStyle{Count} task.
On the \taskStyle{Text Corruption} task that involves identifying a linguistically correct sentence amongst distractors with a shuffled word order, CLIP-based models show a lower performance.
Thus, similar to prior work~\cite{yuksekgonul2023when}, CLIP models show a bag-of-words understanding of the input text and are less sensitive to word order.

\par \noindent \textbf{Pixel \vs latent diffusion.}
We compare the \OURS performance of the pixel diffusion models to that of the latent diffusion models in~\cref{exp:fig:main_res_ldmvpdm}.
Among models that use the same text encoder, \ie PDM-T5 and LDM-T5, we observe that the latent diffusion models outperform the pixel diffusion ones in most cases, especially on the harder tasks of \taskStyle{Attribute Binding}, \taskStyle{Count}, \taskStyle{Spatial Relations} and \taskStyle{Text Corruption}.
We hypothesize that this difference can be explained by the fact that the latent diffusion models operate on the compressed latent space and prioritize the text conditioning while `offloading' the high-frequency image details to the autoencoder.
We further investigate the performance of pixel and latent diffusion models by employing human raters to evaluate their text faithfulness in~\cref{exp:fig:main_res_ldmvpdm}.
The data, for human evaluation, is constructed by randomly picking $500$ examples from the all the tasks ($100$ examples from each task except text corruption), and choosing the right caption as the text prompt.
We convert the accuracy of \OURS, to votes, by counting the number of samples where only one model is right.
From~\cref{exp:fig:main_res_ldmvpdm}, we observe that human raters prefer generations of latent diffusion models to pixel diffusion ones for text faithfulness. \OURS also shows that latent diffusion models have a better text faithfulness and shows an alignment with human evaluations.




\begin{table}[!t]
    \begin{minipage}[t]{0.34\linewidth}
        \centering
        \vspace{0pt}
        \setlength\abovecaptionskip{0pt}
        \includegraphics[width=\textwidth]{figures/he_ldmvpdm.pdf} 
        \captionof{figure}{\textbf{Pixel vs Latent diffusion.} We observe that human raters rank the generation of latent models higher than pixel models in text faithfulness. We notice a similar ordering using \OURS. }\label{exp:fig:main_res_ldmvpdm}     
    \end{minipage}\hfill
    \begin{minipage}[t]{0.65\linewidth}
            \vspace{0pt}
            \centering
            \includegraphics[width=\textwidth]{figures/imagen_vs_dalle2_qual1.pdf}
            \centering
            \includegraphics[width=\textwidth]{figures/imagen_vs_xi_qual2.pdf}    
          \captionof{figure}{\textbf{Qualitative Results.} (Top): Each example compares the generations of pixel diffusion models with CLIP (left) and T5 (right) text encoders. As the difficulty of the prompt increases, models with stronger text encoders maintain higher text fidelity.
          Both the models fail on simple prompts from \taskStyle{Count} and \taskStyle{Spatial relationships}. (Bottom): Comparison between generations of Pixel (left) and Latent (right) diffusion models with a T5 text encoder. Latent diffusion models can get smaller details like ``gray cloth'' and ``white handles'' (second and last example respectively) correctly.}\label{fig:qual1}
        \end{minipage}\vspace{-1.5em}
\end{table}
\par \noindent \textbf{Qualitative results.}
Figure~\ref{fig:qual1} (Top) compares the generations of pixel diffusion models that use T5 and CLIP text encoders. In each example, the image on the left and right are generated using CLIP and T5 text encoder respectively.
We notice that as the difficulty of prompts increases, models with a stronger text encoder performs better. Both the models fail on the much harder task of counting instances and spatial relationships. 
In Figure~\ref{fig:qual1} (Bottom), each example consists two images generated using a pixel diffusion model (left) and a latent diffusion model (right) with a T5 text encoder. We observe that unlike the pixel diffusion model, the latent diffusion model can get small yet important details right (``gray table cloth'' and ``white handles'' in the second and sixth example respectively).
We believe that the latent diffusion model can offload the high frequency appearance details to the autoencoder, allowing it to pay more attention to the conditioning variable. 








        







\subsection{Generative models applied to other reasoning tasks}\label{subsubsec:exp:abl:retrieval}

We now use the challenging Winoground~\cite{Thrush2022WinogroundPV} benchmark to evaluate the vision-language reasoning abilities of diffusion models.
Winoground defines two tasks - (1) `text score' that involves choosing the right text prompt amongst distractors given an input image; and (2) `image score' that involves picking the right image amongst distractor images given a text prompt.

\par \noindent \textbf{\OURS \vs concurrent work}
Concurrent work from~\cite{li2023diffusion} demonstrates that diffusion models perform well on the Winoground text score task and achieve competitive performance with discriminative models.
Using their formulation yields poor results (zero accuracy) on the image score task as shown in Table~\ref{exp:tab:add:retr}.
\cite{li2023diffusion} use the ELBO loss as a proxy for the likelihood $p(\bx | \bc)$ which works well for comparing different text prompts and thus leads to good text score performance.
However, our analysis revealed that the ELBO loss computed for the predictions from two different images are not comparable, and thus leads to zero image score.
\OURS on the other hand, doesn't approximate the likelihood but instead estimates it as described in Sec~\ref{sec:method}.
Using \OURS leads to a non-zero image-score for the same generative model used by~\cite{li2023diffusion}, and yields performance close to that of the discriminative CLIP ViT-L model.

\par \noindent \textbf{\OURS applied to other diffusion models.}
Using \OURS reveals that all the diffusion models introduced in~\cref{subsubsec:exp:benchmark:models} achieve competitive performance on both the image score and text score tasks.
Compared to all the discriminative CLIP models, generative models achieve strong results in both image and text scores using \OURS.
This result reinforces the notion that optimizing the generative objective can provide non-trivial and complementary improvements for several visuo-linguistic reasoning tasks.
 For additional analysis on the effect of various hyperparameters on the Winoground performance, refer to the supplement.

\begin{table}[!t]
    \begin{minipage}[t]{0.52\linewidth}
        \renewcommand{\arraystretch}{1.0}
        \setlength{\tabcolsep}{8pt}
        \centering
        \footnotesize
        \caption{\textbf{Diffusion models evaluated on the Winoground dataset}. We measure the image score (accuracy of picking the correct image given a text prompt) and text score (accuracy of picking the correct text given an image). Using \OURS allows us to use diffusion models for both tasks unlike prior work~\cite{li2023diffusion} which leads to zero image score.}
        \label{exp:tab:add:retr}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & Model & Image Score & Text Score \\
         \midrule
        CLIP (ViT-L/14) & $-$ & 8.00 & 30.25\\
        OCLIP (ViT-H/14) & $-$& 12.75 & 30.75\\
        \midrule
        ~\cite{li2023diffusion} & LDM-CLIP & 0 & 34.00\\
        \OURS & LDM-CLIP & 7.25 & 22.75  \\
        \midrule
        \OURS & PDM-CLIP & 14.00 &  17.00 \\
         \OURS& PDM-T5 & 12.00 & 28.25 \\
         \OURS& LDM-T5 & 13.50 & 29.00 \\
        \bottomrule
        \end{tabular}
        }
    \end{minipage}\hfill
        \begin{minipage}[t]{0.45\linewidth}
            \renewcommand{\arraystretch}{1.2}
            \setlength{\tabcolsep}{4pt}
            \centering
            \footnotesize
            \caption{\textbf{Performance of CLIP on the benchmark.} We evaluate the zero-shot performance of CLIP (ViT-L/14 visual backbone) on the six tasks. ``Random'' denotes the random chance accuracy.
            CLIP achieves impressive performance on the tasks of \taskStyle{Color} and \taskStyle{Shape}. We observe that the performance of CLIP is close to random on \taskStyle{Attribute binding}, \taskStyle{Count}, \taskStyle{Spatial} and \taskStyle{Text corruption}.
            This makes CLIP unsuitable for evaluating text faithfulness of generative models on prompts from these tasks.
            }\label{exp:tab:clip_cls_acc}

            \newcolumntype{L}{>{\hspace*{-\tabcolsep}}c}
            \newcolumntype{R}{c<{\hspace*{-\tabcolsep}}}
            \resizebox{\linewidth}{!}{
            \begin{tabular}{LccccccR}
            \toprule
            Model   &  Attribute & Color & Count & Shape &Spatial & Text \\
            & binding & & & & &corruption\\
            \midrule
            \belowrulesepcolor{Gray}
            \rowcolor{Gray}Random &  50 & 25 & 25 & 33 & 25 & 20 \\
            \aboverulesepcolor{Gray}
            \midrule
            CLIP &  55.40 & 85.20 & 67.80 & 91.10 & 40.50 & 51.00\\
            \bottomrule
            \end{tabular}
            }
        \end{minipage}\vspace{-1em}
            
\end{table}

\subsection{Drawbacks of CLIP score}
In this section, we discuss a few limitations of the CLIP score that \OURS can effectively address.
CLIP score is the most common metric for evaluating text faithfulness of generative models by measuring the cosine similarity between the features of the generated image and the conditioned text caption.

\par \noindent \textbf{Sensitivity to the exact CLIP model.}

We report the CLIP similarity scores of the generations from two versions of the Latent Diffusion Models~\cite{rombach2022high} on prompts from DrawBench~\cite{Saharia2022PhotorealisticTD}, Winoground~\cite{Thrush2022WinogroundPV} and COCO-minival~\cite{2014-lin} datasets in Figure~\ref{fig:clip-drawbacks}.
The first model (LDM-CLIP (ViT-L/14)) uses the text encoder of CLIP with ViT-L/14 backbone and the second model (LDM-CLIP (ViT-H/14)) uses the text encoder with OpenCLIP~\cite{ilharco_gabriel_2021_5143773} ViT-H/14 visual backbone.
Across all the three datasets, we observe that LDM-CLIP (ViT-L/14) ranks higher than LDM-CLIP (ViT-H/14) if a CLIP (ViT-L/14 visual backbone) model is used, but ranks lower with an OpenCLIP (ViT-H/14 visual backbone).
Our hypothesis is that images generated by a model using a particular CLIP text encoder may still contain some residual information, which could cause them to receive higher scores when assessed using the same CLIP model.
This type of bias was identified by~\cite{park2021benchmark} in the context of evaluation of text-to-image models, though not in relation to the CLIP score.
We emphasize the need for caution among researchers who employ this metric, particularly concerning this bias.
\OURS avoids this problem as we do not employ an external model for evaluation.




\par \noindent \textbf{CLIP score is limited by CLIP's performance} and thus using it as a proxy on tasks where CLIP itself has poor performance does not yield meaningful comparsions.
While the CLIP model has demonstrated impressive zero-shot performance on several image-text tasks, it has severe limitations on many complex reasoning tasks.
We compute the performance of CLIP ViT-L/14 model on the six tasks introduced in~\cref{subsec:exp:benchmark} and report the results in~\cref{exp:tab:clip_cls_acc}.
CLIP performs well on \taskStyle{Color} and \taskStyle{Shape} but its performance on all the other tasks is poor.
On the widely used DrawBench prompts, 25\% of the captions evaluate the generations for attribute binding, counting, spatial relationships and text corruption.
Thus, using CLIP to evaluate generations on such prompts in DrawBench is not ideal.
\OURS avoids this problem by directly leveraging the diffusion model itself.













        





