\documentclass{article}

\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}

\usepackage{authblk}
\usepackage{hyperref}
\usepackage{paralist}
\usepackage{placeins}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{multirow,multicol}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{enumerate}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfmath}
\usetikzlibrary{plotmarks,shapes,pgfplots.dateplot}
\usetikzlibrary{math,calc,trees,positioning,arrows,chains,shapes.geometric,decorations.pathreplacing,decorations.pathmorphing,shapes,matrix,shapes.symbols}
\usetikzlibrary{3d,decorations.text,shapes.arrows,fit,backgrounds,quotes,arrows.meta}
\usepackage{ifthen}
\usepackage{numprint}

\newcommand{\todo}[1]{[\textbf{TODO}: #1]}


\begin{document}

\title{STIR: Siamese Transformer for Image Retrieval Postprocessing}
\date{\today}

\author[1]{Aleksei Shabanov}
\author[2]{Aleksei Tarasov}
\author[1]{Sergey Nikolenko}
\affil[ ]{
\{shabanoff.aleksei,  aleksei.v.tarasov\}@gmail.com,  sergey@logic.pdmi.ras.ru
}
\affil[1]{St. Petersburg Department of the Steklov Institute of Mathematics, St. Petersburg, Russia}
\affil[2]{New Yorker  GmbH, Berlin, Germany}





\maketitle

\begin{abstract}
Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the art on standard image retrieval datasets: Stanford Online Products and DeepFashion In-shop. We also release the source code\footnote{
https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/postprocessing/
} and an interactive demo\footnote{https://dapladoc-oml-postprocessing-demo-srcappmain-pfh2g0.streamlit.app/} of our approach.
\end{abstract}

\section{Introduction}

Modern approaches for metric learning and image retrieval usually employ a standard pretrained backbone which is fine-tuned for the task with a metric learning objective such as the triplet loss~\cite{10.5555/1756006.1756042}. Much of the progress in the field has concentrated on ways to improve upon the basic triplet loss. The backbones have usually been standard successful deep architectures, first convolutional ones such as ResNet-50 and later Transformer-based such as the Vision Transformer (ViT)~\cite{DBLP:journals/corr/abs-2010-11929}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{stir6.png}
    \caption{STIR postprocessing on a real example from the In-Shop dataset: the reranking model receives as input concatenated query and gallery images and outputs the probability of them being a negative pair.}\label{fig:sample}
\end{figure}

In 2020, Musgrave at el.~\cite{10.1007/978-3-030-58595-2_41} performed an experimental evaluation of a long line of metric learning results and found little improvement over standard approaches, concluding that (undeniable) progress had been mostly due to steadily improving backbones. Since then, metric learning and information retrieval have become dominated by Transformer-based architectures, with ViT~\cite{DBLP:journals/corr/abs-2010-11929} being especially influential for image retrieval. The standard baseline today is a ViT-like model fine-tuned with the triplet loss to produce a latent space where the dot product of embeddings corresponds to the similarity needed for the retrieval problem. Still, latest works claim significant improvements with a variety of new loss functions~\cite{NEURIPS2021_c622c085,patel2022recall} and even remapping the embeddings into a hyperbolic space~\cite{9880306} (see Section~\ref{sec:related}).

Our first contribution in this work is to go back and re-evaluate the standard triplet loss-based approach with a ViT backbone, which we call \emph{ViT-Triplet} (Fig.~\ref{fig:stir}a). We find that with a brief tuning of hyperparameters and efficient implementation, ViT-Triplet outperforms state of the art results in some settings and reaches similar results in others. Apart from improved results, ViT-Triplet, in our opinion, is a better option in practice since other solutions are either harder to bring to production environments or require more complicated tuning.


Second, we consider postprocessing for ViT-Triplet in the form of reranking the top results. Reranking for image retrieval has a long history~\cite{DBLP:journals/corr/NohASH16,8954470,10.1007/978-3-030-58565-5_43,sarlin20superglue,DBLP:conf/iccv/TanYO21} but it has usually been applied to relatively weak models, where one needed to rerank hundreds of results. We consider reranking for ViT-Triplet output and note that since the results of ViT-Triplet are already quite good we can concentrate on reranking the top few results, which allows us to use much more computationally intensive methods. We present the \emph{Siamese Transformer for Image Retrieval} (STIR) model that uses a ViT architecture to process a concatenation of each query-result pair with a small MLP head on top (Fig.~\ref{fig:stir}b). We show that STIR indeed improves over ViT-Triplet and prior art and thus sets new state of the art for several well-known image retrieval datasets. Fig.~\ref{fig:sample} illustrates sample STIR reranking on the In-Shop dataset~\cite{liuLQWTcvpr16DeepFashion}: distances on top are the result of ViT-Triplet, distances at the bottom are the results of STIR, and the ground truth answer is highlighted in green.

The paper is organized as follows: Section~\ref{sec:related} reviews related work, Section~\ref{sec:method} introduces ViT-Triplet and STIR reranking, Section~\ref{sec:eval} presents our evaluation results, and Section~\ref{sec:concl} concludes the paper.


\section{Related work}\label{sec:related}

We identify two relevant directions of related work. 
First, image retrieval itself, where the best recent work employs Transformer-based backbones. Vision Transformers~\cite{DBLP:journals/corr/abs-2010-11929} were fine-tuned for image retrieval by the IRT model~\cite{DBLP:journals/corr/abs-2102-05644} based on the DeiT distillation approach~\cite{pmlr-v139-touvron21a}. Hyperbolic Vision Transformers (Hyp-ViT)~\cite{9880306} reach state of the art results on several datasets by using pairwise cross-entropy with hyperbolic distances measured on the Poincar{\'e} ball. However, the resulting embeddings are not suitable for most existing vector search engines that rely on algorithms optimized for Euclidean spaces, so Hyp-ViT is hard to bring to production environments. Moreover, Hyp-ViT defines an entire family of models (six variations, two embedding sizes for each) that need to be evaluated in each case, which we view as a kind of hyperparameter tuning.

Another direction of study introduces new loss functions that approximate or provide bounds for non-differentiable retrieval metrics. ROADMAP~\cite{NEURIPS2021_c622c085} presents a decomposable differentiable upper bound for the average precision. Patel et al.~\cite{patel2022recall} proposed a differentiable surrogate loss for recall optimization further augmented with mixup regularization; this, however, also leads to a set of new hyperparameters such as sigmoid temperatures that need to be tuned. We also note the HAPPIER model that proposes a new loss function for hierarchical image retrieval~\cite{ramzi2022hierarchical}. Interestingly, despite the prevalence of Transformers some of the top results are still produced by CNNs: a combination of multiple CNN-based global descriptors was proposed in~\cite{DBLP:journals/corr/abs-1903-10663}, while the standard ResNet-50 backbone has been leveraged with the ProxyNCA++ method (an update on proxy-neighborhood component analysis) in~\cite{10.1007/978-3-030-58586-0_27} and with the Metrix loss function that extends mixup to metric learning objectives (including triplet loss) in~\cite{venkataramanan2022it}.

Second, specifically postprocessing (reranking) approaches are rare in recent works. Classical approaches usually reranked image retrieval results based on local descriptors extracted from the images~\cite{DBLP:journals/corr/NohASH16,8954470,10.1007/978-3-030-58565-5_43}. We note \emph{SuperGlue} that used graph neural networks to link local descriptors~\cite{sarlin20superglue} and the \emph{Reranking Transformer} (RRT) approach that uses a Transformer to process global and local descriptors extracted from an image pair~\cite{DBLP:conf/iccv/TanYO21}. Unlike most approaches that rerank a large set of results (at least several hundred), we aim to correct an already high-performing Transformer-based model so we concentrate on (relatively heavyweight) postprocessing of a few top results.

\section{Method}\label{sec:method}

\subsection{ViT-Triplet}

For the ViT-Triplet model, we follow the approach from~\cite{DBLP:journals/corr/HermansBL17,DBLP:journals/corr/abs-1912-07863} and fine-tune a ViT backbone for image retrieval as shown in Fig.~\ref{fig:stir}a. We form batches by taking  labels (item ids) and  instances (images) for each label. 
To decrease the number of hyperparameters we set  since the median size of a class is 5 in InShop and 4 in SOP (see Section~\ref{sec:data}), so we can avoid severe under- or oversampling. The parameter  is chosen such as to fill the GPU memory ( in our case for an NVIDIA V100 GPU). 




After a batch is sampled, we perform hard triplet mining to form  triplets; namely, we calculate the distance matrix between embeddings of images in the batch and take for each image the hardest positive sample (same label, maximum distance) and the hardest negative sample (different label, minimum distance). 

Then we compute the triplet loss function 

for a query image , positive sample , negative sample , distance in the embedding space , and constant margin ; we set  in all experiments (in previous works, it was usually chosen as ). We name the resulting model \emph{ViT-Triplet}.



\subsection{Siamese Transformer}

Qualitative error analysis has shown that many mistakes may be caused by the fact that the feature extractor has to ``blindly'' represent a given image as a vector in the latent space, without understanding what other images it would be compared against. In a perfect world, all the information needed for this comparison would be already included in the feature vector, but in reality, the model can benefit a lot from a direct side-by-side comparison of the images. 

Another motivation comes from the distribution of results. For instance, the CMC@1 metric for \emph{ViT-Triplet} on the In-Shop dataset is 92.1\% but CMC@5 reaches 97.6\%, i.e., less than a third of the error rate. It means that we usually already have a 
correct answer at the top of the list, and side-by-side comparison may help us push it in the first place. In general, with a good feature extractor CMC@k saturates for relatively small values of , such as , so we do not need to rerank a lot and can afford to use a relatively heavyweight model.

We suggest to use a reranking model that performs pairwise comparisons of the query image and top retrieved results. We want our pairwise postprocessor to have the following properties:
\begin{enumerate}[(i)]
    \item it has to reuse an already trained feature extractor, ideally without new large trainable networks;
    \item it has to have an attention mechanism to compare the regions of a query image and regions of a gallery image pairwise;
    \item it has to be interpretable or at least provide some mechanism that can be used to interpret the results;
    \item it has to be simple and not require additional manual labeling or extra data.
\end{enumerate}

\begin{figure}[!t]
    \centering
    \includegraphics[width=.7\linewidth]{stir7.png}
    \caption{Network architectures: (a) ViT-Triplet; (b) STIR postprocessing.}\label{fig:stir}
\end{figure}

Thus, we propose to use the \emph{Siamese Transformer for Image Retrieval} (STIR) model, which is a ViT feature extractor with an additional MLP on top that takes two concatenated images as input and returns the probability of these images to be a negative pair (Fig.~\ref{fig:stir}b). This output can also be interpreted as a ``distance'': lower probability means more similar images. STIR satisfies the requirements above:
\begin{enumerate}[(i)]
    \item a pretrained \emph{ViT-Triplet} is used for initialization;
    \item the built-in ViT attention mechanism considers the interactions of patches both inside an image and across images;
    \item the resulting attention maps help to achieve interpretability;
    \item the only overhead is a two-layer MLP, and the input can reuse the same image pairs.
\end{enumerate}

To train the postprocessor, similarly to \emph{ViT-Triplet} we form batches by taking  labels and  images for each label, with the same  but  instead of  since STIR has a larger memory footprint due to larger input size. After a batch is sampled, we mine hard pairs (pairs with largest distances and same labels or smallest distances and different labels), concatenate the images, and feed them to STIR, which predicts the
probability of a pair to be negative.
We use the binary cross-entropy as the objective function for STIR.

Another important property of STIR is that it is \emph{asymmetric}, i.e., the results may depend on whether we put the query on the left and a gallery image on the right or vice versa. We have not found significant differences between these two options, but the results improve a little further if we symmetrize STIR by averaging their results. We call this version \emph{STIR-Symmetric} in the tables below and propose it as a slightly improved version of STIR reranking with an additional cost of running the model twice.


\section{Evaluation}\label{sec:eval}

\subsection{Datasets and experimental setup}\label{sec:data}

We concentrate on two standard image retrieval datasets.

The \emph{In-shop Clothes Retrieval Benchmark} (\textbf{In-Shop}) dataset~\cite{liuLQWTcvpr16DeepFashion} is a part of the \emph{DeepFashion} dataset with \numprint{7982} clothing items and \numprint{52712} high quality in-shop images, with the median of  photos per item. There are \numprint{25882} images in the training set and \numprint{26830} images in the test set, which is divided into two non-overlapping parts: query set (\numprint{14128} images) and gallery (the search index, \numprint{12612} images). Each query image corresponds to one or more images of the same clothing item in the gallery. 


\emph{Stanford Online Products} (\textbf{SOP})~\cite{song2016deep} has \numprint{22634} online products with \numprint{120053} related images, with the median of  photos per item. There are \numprint{11318} products (\numprint{59551} images) in the training set and \numprint{11316} products (\numprint{60502} images) in the test set. The test set has no fixed query-gallery split, so we consider each individual photo as a query and evaluate it versus the rest of the images in the test set.

To ensure a fair comparison, we copy (as much as possible) the parameters and backbone models for ViT-Triplet from Hyp-ViT~\cite{9880306}. The model architecture is ViT-S/16 (small version, patch size 16), the optimizer is AdamW with lr=1e-5, the image size is 224, and augmentation transforms are Horizontal Flip and Random Resized Crop with scale randomly chosen from ; we did not use any additional information from the data such as bounding boxes or category labels.
The training setup for STIR is mostly the same as for ViT-Triplet: image size 224, the same augmentations, but with a less agressive Random Resized Crop (sampling the scale parameter from ) so that STIR can compare almost the entire two images side-by-side. We used the AdamW optimizer with learning rate 2e-3 for the first 3 epochs, when we fine-tune the MLP head only, and 1e-5 for the rest of the training. The MLP head consists of two fully connected layers with sizes (384, 192) and (192, 1) respectively, separated by a dropout layer with probability  and sigmoid activation function.
We run all training experiments on two NVIDIA V100 GPUs with half-precision turned on. For the final metrics evaluation, we used only one GPU and turned off half-precision.

In the evaluation tables, all external results are taken from the corresponding papers except for surrogate recall, where the original work~\cite{patel2022recall} reports only the ViT-B version, which is better than the results in Table~\ref{tbl:res} that use the ViT-S backbone. Therefore, we have re-evaluated surrogate recall with ViT-S using the original code~\cite{patel2022recall}.

\subsection{Evaluation metrics}

\def\ntopk{n_{\mathrm{k}}}
\def\ngt{n_{\mathrm{gt}}}

Most works on metric learning and information retrieval report Recall@k for various values of  as the primary evaluation metric. Interestingly, there is a significant discrepancy between the understanding of recall in classical information retrieval and the ``recall'' metric used in many works on metric learning. 

Usually, recall is defined as 
 
where  is the number of ground truth results in the top  retrieved results and  is the total number of ground truth results. However, metric learning works often report, e.g., Recall@1 values close to  even when there exist several ground truth answers to a query, , and Recall@1 should be bounded by . This is because instead of recall they actually report the \emph{cumulative matching characteristics} (CMC) metric: 


For datasets with exactly one ground truth answer for every query, CMC and recall coincide; however, this is not the case for In-Shop and SOP datasets so we keep the CMC terminology in evaluation tables. We also note that since In-Shop and SOP have several correct answers, the \emph{precision} metric, 
 also makes sense for evaluation. Therefore, below we report mean average precision (mAP) values as well, where
 
is the average precision (area under the precision-recall curve), and mAP@k is AP@k averaged over the test set queries.  
Unfortunately, we have nothing to compare with in terms of mAP since its values have not been reported in previous works.

\subsection{Results}

Table~\ref{tbl:res} shows the main results of our comparison. First, note that the ViT-Triplet model, trained with the standard embedding dimension  only with the triplet loss and ViT backbone, shows state of the art results on both SOP and In-Shop datasets, losing to the current state of the art HypViT only in the CMC@1 metric on In-Shop. This supports our conclusion that most of the latest progress in image retrieval has been due to steadily improving Transformer-based backbones, and a well-trained ViT backbone with a straightforward triplet loss is still a very competitive approach to image retrieval.

Second, Table~\ref{tbl:res} shows how STIR postprocessing improves the results of ViT-Triplet, outperforming the best previous results (including ViT-Triplet itself) and Reranking Transformers in the CMC@1 metric (Recall@1). Note that since STIR in Table~\ref{tbl:res} is limited to reranking the top  results, it cannot change the CMC@k and Recall@k 
metrics for , so the rest of the results coincide with ViT-Triplet. STIR results improve monotonically with , but we have chosen  to report in Table~\ref{tbl:res} because in this case, STIR postprocessing has the same running time as reranking Transformers~\cite{DBLP:conf/iccv/TanYO21}.


In Table~\ref{tbl:map}, we report mean average precision scores for our methods; we do not have the results of other approaches here, so we show these numbers to provide a baseline and hope that later works will measure mAP as well.

\begin{table*}[p]\centering
\caption{Image retrieval results on SOP and In-Shop datasets, CMC metric; best results highlighed in bold.}\label{tbl:res}
\setlength{\tabcolsep}{5pt}
\rotatebox{90}{
\begin{tabular}{lc|ccc|ccccc}
\toprule
\textbf{Model} & \textbf{Emb.} & \multicolumn{3}{c|}{\textbf{SOP}, \textbf{CMC metric}} & \multicolumn{5}{c}{\textbf{In-Shop}, \textbf{CMC metric}} \\
& & \textbf{@1} & \textbf{@10} & \textbf{@100} & \textbf{@1} & \textbf{@10} & \textbf{@20} & \textbf{@30} & \textbf{@100} \\\midrule
\multicolumn{10}{c}{\textbf{Embedding-based retrieval}} \\\midrule
HypVit~\cite{9880306} & 128 & 85.5 & 94.9 & \textbf{98.1} & \textbf{92.7} & 98.4 & 98.9 & 99.1 & --- \\
HypVit~\cite{9880306} & 384 & 85.9 & 94.9 & \textbf{98.1} & 92.5 & 98.3 & 98.8 & 99.1 & --- \\
Hyp-DINO~\cite{9880306} & 128 & 84.6 & 94.1 & 97.7 & 92.6 & 98.4 & 99.0 & 99.2 & --- \\
Hyp-DINO~\cite{9880306} & 384 & 85.1 & 94.4 & 97.8 & 92.4 & 98.4 & 98.9 & 99.1 & --- \\
Surrogate recall, ViTs16 backbone~\cite{patel2022recall}  & 384 & 85.6 & 94.8 & 98.0 & --- & --- & --- & --- & --- \\
 based on DeiT-s~\cite{DBLP:journals/corr/abs-2102-05644} & 384 & 84.2 & 93.7 & 97.3 & 91.9 & 98.1 & 98.7 & 98.9 & --- \\
ROADMAP (on DeiT-s)~\cite{NEURIPS2021_c622c085} & 384 & 86.0 & 94.4 & 97.6 & --- & --- & --- & --- & --- \\
ViT-Triplet & 384 & \textbf{86.5} & \textbf{95.2} & \textbf{98.1} & 92.1 & \textbf{98.5} & \textbf{99.1} & \textbf{99.3} & 99.7 \\\midrule
\multicolumn{10}{c}{\textbf{Reranking approaches}} \\\midrule
Reranking Transformers (frozen)~\cite{DBLP:conf/iccv/TanYO21} & & 81.8 & 92.4 & 96.6 & --- & --- & --- & --- & --- \\
Reranking Transformers (finetuned)~\cite{DBLP:conf/iccv/TanYO21} & & 84.5 & 93.2 & 96.6 & --- & --- & --- & --- & --- \\
STIR (ViT-Triplet),  & 384 & 88.1 & \textbf{95.3} & \textbf{98.1} & 94.9 & \textbf{98.5} & \textbf{99.1} & \textbf{99.3} & 99.7 \\
STIR-Symmetric (ViT-Triplet),  & 384 & \textbf{88.3} & \textbf{95.3} & \textbf{98.1} & \textbf{95.0} & \textbf{98.5} & \textbf{99.1} & \textbf{99.3} & 99.7 \\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table}[t]\centering
\caption{Mean average precision on SOP and In-Shop datasets.}\label{tbl:map}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lc|cc|cc}
\toprule
\textbf{Model} & \textbf{Emb.} & \multicolumn{2}{c|}{\textbf{SOP}, \textbf{mAP}} & \multicolumn{2}{c}{\textbf{In-Shop}, \textbf{mAP}} \\
& & \textbf{@5} & \textbf{@10} & \textbf{@5} & \textbf{@10} \\\midrule
ViT-Triplet & 384 & 87.6 & 85.1 & 91.6 & 88.4 \\
STIR,  & 384 & 89.4 & 86.5 & 94.8 & 91.0 \\
STIR-Symmetric,  & 384 & \textbf{89.5} & \textbf{86.6} & \textbf{95.0} & \textbf{91.2} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]\centering
\caption{Ablation study for STIR, In-Shop dataset.}\label{tbl:stir}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \textbf{CMC@1} & \textbf{CMC@10} & \textbf{mAP@5} & \textbf{mAP@10} \\\midrule
STIR,  & 94.4 & \textbf{98.5} & 93.1 & 89.6 \\
STIR,  & \textbf{94.9} & \textbf{98.5} & \textbf{94.8} & 91.0 \\
STIR,  & \textbf{94.9} & \textbf{98.5} & 94.7 & 92.0 \\
STIR,  & \textbf{94.9} & \textbf{98.5} & 94.5 & \textbf{92.7} \\
\bottomrule
\end{tabular}
\end{table}
 
Table~\ref{tbl:stir} presents the results of our ablation study for STIR variations differing by the number  of the results they rerank. Since STIR requires to run a ViT-based model for each query-gallery pair, it is a relatively heavyweight approach to postprocessing so we limit the comparison to small values of . We see that the CMC@1 (Recall@1) metric saturates quickly as we increase . Table~\ref{tbl:stir} also shows the advantages of mAP in this case: it is a holistic metric that improves as positive samples move closer to the top of the list so mAP@5 and mAP@10 can be used to detect improvements, while CMC@10, naturally, remains unchanged when we rerank top results for .

\subsection{Qualitative analysis}

\begin{figure}[!t]\centering
\begin{tabular}{c}
    \includegraphics[width=.9\linewidth]{sample1.png}
    \\ (a) \\
    \includegraphics[width=.9\linewidth]{sample2.png}
    \\ (b) \\
    \includegraphics[width=.9\linewidth]{sample4.png}
    \\ (c) 
\end{tabular}
    \caption{STIR postprocessing examples from the interactive demo.}\label{fig:demo}
\end{figure}

Figure~\ref{fig:demo} shows several reranking examples from the InShop dataset as shown in our interactive demo of STIR\footnote{https://dapladoc-oml-postprocessing-demo-srcappmain-pfh2g0.streamlit.app/}. Fig.~\ref{fig:demo}a and Fig.~\ref{fig:demo}b show two results where the reranking improves the results according to the ground truth labeled in the test set; in particular, in both cases the best (top-1) result has been corrected from wrong to right.

Fig.~\ref{fig:demo}c shows a result where STIR reranking actually makes the output worse according to the ground truth labeling. Note, however, that the ground truth results in this case are problematic themselves: they deal only with the shirt of the model while the query clearly shows both the shirt and jeans that do not match in the second ``correct'' answer. Unfortunately, such ambiguous results are encountered in existing datasets quite often, so we note this as a direction for further improvement that might help the entire field of image retrieval. Note also that the InShop dataset is supposed to care about the cut and fashion of a clothing item rather than color, so the model is supposed to retrieve the same item in different colors as well, which often increases the ambiguity.

\section{Conclusion}\label{sec:concl}

In this work, we have presented a simple \emph{ViT-Triplet} model that uses the ViT backbone and the triplet loss and have shown that it consistently reaches or exceeds state of the art results in image retrieval. Thus, a straightforward solution with the best available backbone and a well-tuned training process still remains at the state of the art level in image retrieval.
Moreover, we have presented a postprocessing approach called STIR that reranks top results by an additional pass of ViT over concatenated query and gallery images; STIR is a heavyweight postprocessing method aimed at improving the top of the list. Our experimental study on SOP and In-Shop datasets has shown that STIR can indeed significantly improve retrieval results. We also release a library that implements our methods and can reproduce all our results\footnote{https://github.com/OML-Team/open-metric-learning}.

We note several directions for further work. First, we only consider direct query-to-gallery interactions, while gallery-to-gallery interactions are left indirect. Second, STIR processes the original concatenated images, which makes it relatively slow, and there may be at least two ways to address the problem. First, our current backbone is ViT, which has quadratic complexity with respect to image size; replacing it with an architecture such as the Swin Transformer~\cite{DBLP:journals/corr/abs-2103-14030} that would reduce this complexity to linear may significantly speed up postprocessing. Second, one can replace original images with descriptors obtained from intermediate layers of the feature extractor during the first stage; much of the semantic information is already contained in these features but this would require a different architecture, which we leave for future work.



\newcommand{\etalchar}[1]{}
\begin{thebibliography}{YCYW19}

\bibitem[CAS20]{10.1007/978-3-030-58565-5_43}
Bingyi Cao, Andr\'{e} Araujo, and Jack Sim.
\newblock Unifying deep local and global features for image search.
\newblock In {\em Computer Vision – ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23–28, 2020, Proceedings, Part XX}, page 726–743,
  Berlin, Heidelberg, 2020. Springer-Verlag.

\bibitem[CSSB10]{10.5555/1756006.1756042}
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio.
\newblock Large scale online learning of image similarity through ranking.
\newblock {\em J. Mach. Learn. Res.}, 11:1109–1135, mar 2010.

\bibitem[DBK{\etalchar{+}}20]{DBLP:journals/corr/abs-2010-11929}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em CoRR}, abs/2010.11929, 2020.

\bibitem[EMK{\etalchar{+}}22]{9880306}
A.~Ermolov, L.~Mirvakhabova, V.~Khrulkov, N.~Sebe, and I.~Oseledets.
\newblock Hyperbolic vision transformers: Combining improvements in metric
  learning.
\newblock In {\em 2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 7399--7409, Los Alamitos, CA, USA, jun 2022. IEEE
  Computer Society.

\bibitem[ENLJ21]{DBLP:journals/corr/abs-2102-05644}
Alaaeldin El{-}Nouby, Natalia Neverova, Ivan Laptev, and Herv{\'{e}}
  J{\'{e}}gou.
\newblock Training vision transformers for image retrieval.
\newblock {\em CoRR}, abs/2102.05644, 2021.

\bibitem[HBL17]{DBLP:journals/corr/HermansBL17}
Alexander Hermans, Lucas Beyer, and Bastian Leibe.
\newblock In defense of the triplet loss for person re-identification.
\newblock {\em CoRR}, abs/1703.07737, 2017.

\bibitem[JKK{\etalchar{+}}19]{DBLP:journals/corr/abs-1903-10663}
HeeJae Jun, ByungSoo Ko, Youngjoon Kim, Insik Kim, and Jongtack Kim.
\newblock Combination of multiple global descriptors for image retrieval.
\newblock {\em CoRR}, abs/1903.10663, 2019.

\bibitem[LLC{\etalchar{+}}21]{DBLP:journals/corr/abs-2103-14030}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em CoRR}, abs/2103.14030, 2021.

\bibitem[LLQ{\etalchar{+}}16]{liuLQWTcvpr16DeepFashion}
Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang.
\newblock Deepfashion: Powering robust clothes recognition and retrieval with
  rich annotations.
\newblock In {\em Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2016.

\bibitem[MBL20]{10.1007/978-3-030-58595-2_41}
Kevin Musgrave, Serge Belongie, and Ser-Nam Lim.
\newblock A metric learning reality check.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
  editors, {\em Computer Vision -- ECCV 2020}, pages 681--699, Cham, 2020.
  Springer International Publishing.

\bibitem[NASH16]{DBLP:journals/corr/NohASH16}
Hyeonwoo Noh, Andre Araujo, Jack Sim, and Bohyung Han.
\newblock Image retrieval with deep local features and attention-based
  keypoints.
\newblock {\em CoRR}, abs/1612.06321, 2016.

\bibitem[PTM22]{patel2022recall}
Yash Patel, Giorgos Tolias, and Ji{\v{r}}{\'\i} Matas.
\newblock Recall@k surrogate loss with large batches and similarity mixup.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7502--7511, 2022.

\bibitem[RAT{\etalchar{+}}22]{ramzi2022hierarchical}
Elias Ramzi, Nicolas Audebert, Nicolas Thome, Cl{\'e}ment Rambour, and Xavier
  Bitot.
\newblock Hierarchical average precision training for pertinent image
  retrieval.
\newblock In {\em European Conference on Computer Vision}, pages 250--266.
  Springer, 2022.

\bibitem[RTR{\etalchar{+}}21]{NEURIPS2021_c622c085}
Elias Ramzi, Nicolas THOME, Cl\'{e}ment Rambour, Nicolas Audebert, and Xavier
  Bitot.
\newblock Robust and decomposable average precision for image retrieval.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 23569--23581. Curran Associates, Inc., 2021.

\bibitem[SAC19]{8954470}
O.~Simeoni, Y.~Avrithis, and O.~Chum.
\newblock Local features and visual words emerge in activations.
\newblock In {\em 2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 11643--11652, Los Alamitos, CA, USA, jun 2019.
  IEEE Computer Society.

\bibitem[SDMR20]{sarlin20superglue}
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.
\newblock {SuperGlue}: Learning feature matching with graph neural networks.
\newblock In {\em CVPR}, 2020.

\bibitem[SXJS16]{song2016deep}
Hyun~Oh Song, Yu~Xiang, Stefanie Jegelka, and Silvio Savarese.
\newblock Deep metric learning via lifted structured feature embedding.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[TCD{\etalchar{+}}21]{pmlr-v139-touvron21a}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herve Jegou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 10347--10357. PMLR, 18--24 Jul 2021.

\bibitem[TDT20]{10.1007/978-3-030-58586-0_27}
Eu~Wern Teh, Terrance DeVries, and Graham~W. Taylor.
\newblock Proxynca++: Revisiting and revitalizing proxy neighborhood component
  analysis.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
  editors, {\em Computer Vision -- ECCV 2020}, pages 448--464, Cham, 2020.
  Springer International Publishing.

\bibitem[TYO21]{DBLP:conf/iccv/TanYO21}
Fuwen Tan, Jiangbo Yuan, and Vicente Ordonez.
\newblock Instance-level image retrieval using reranking transformers.
\newblock In {\em 2021 {IEEE/CVF} International Conference on Computer Vision,
  {ICCV} 2021, Montreal, QC, Canada, October 10-17, 2021}, pages 12085--12095.
  {IEEE}, 2021.

\bibitem[VPK{\etalchar{+}}22]{venkataramanan2022it}
Shashanka Venkataramanan, Bill Psomas, Ewa Kijak, Laurent Amsaleg, Konstantinos
  Karantzalos, and Yannis Avrithis.
\newblock It takes two to tango: Mixup for deep metric learning.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem[YCYW19]{DBLP:journals/corr/abs-1912-07863}
Ye~Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.
\newblock In defense of the triplet loss again: Learning robust person
  re-identification with fast approximated triplet loss and label distillation.
\newblock {\em CoRR}, abs/1912.07863, 2019.

\end{thebibliography}






\end{document} 
