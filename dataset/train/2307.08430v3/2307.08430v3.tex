In this section, we evaluate the benefits of our method against state-of-the-art models on eight heterogeneous datasets. We aim to answer the following questions:  
\textbf{Q1.} How does \model~perform on large-scale and small-scale datasets compared with state-of-the-art baselines? 
\textbf{Q2.} Does \model~perform better on sparser large-scale HINs? 
\textbf{Q3.} Are the search algorithm and searched meta-paths effective? 
\textbf{Q4.} How does \model~perform when the value of maximum hop increases? 
More ablation studies and hyper-parameter studies are provided in the Appendix.

\subsection{Datasets and Baselines}
We evaluate \model~on the large-scale dataset \texttt{ogbn-mag} from OGB challenge~\citep{hu2021ogb}, and three widely-used HINs including \texttt{DBLP}, \texttt{IMDB}, and \texttt{ACM} from HGB benchmark~\citep{lv2021we} on the node classification task. Please see Appendix A.1 for details of all datasets.
For the \texttt{ogbn-mag} dataset, the results are compared to the OGB leaderboard, and all scores are the average of $10$ separate training. For the three datasets from HGB, the results are compared to the baseline scores from the HGB paper and several recent advanced methods. 
Following the convention~\citep{lv2021we,yang2022simple}, all scores are the average of $5$ separate local data partitions. GEMS~\citep{han2020genetic} and RMS-HRec~\citep{ning2022automatic} are ignored because they are designed for recommendation tasks. 
RL-HGNN~\citep{zhong2020reinforcement} is omitted due to a lack of source code. 





\subsection{Experimental Setup}
We set the number of selected meta-paths $M=30$ for all datasets. The maximum hop is $6$ for \texttt{ogbn-mag}, \texttt{DBLP} and $5$ for \texttt{IMDB}, \texttt{ACM}. A two-layer MLP is adopted for each meta-path in the feature projection step and the hidden size is $512$. $\tau$ linearly decays with the number of epochs from $8$ to $4$. All the model parameters are initialized by the Xavier uniform distribution~\citep{DBLP:journals/jmlr/GlorotB10} and are optimized with Adam~\citep{adam} during training.  
The learning rate is $0.001$ and the weight decay is $0$. For searching in the super-net, we train for $200$ epochs with $\lambda=1$ during the first $20$ epochs for warmup and decreasing to $0$ linearly. 
For training of the target-net, we use an early stop mechanism based on the validation performance to promise full training. All the experiments are conducted on a Tesla V100 16GB GPU. 




\begin{table*}
\centering
\begin{minipage}[th!]{\textwidth}
\begin{minipage}[t]{0.415\textwidth}
\caption{Test accuracy ($\%$) on the ogbn-mag compared with other methods on the OGB leaderboard. See main text for details of \textit{label} and \textit{ms}. 
}
\vspace{-7pt}
\vspace{0.5pt}
\label{tab:result_on_large_dataset}
\begin{center}
\footnotesize
\resizebox{\linewidth}{!}{
\begin{tabular}{lc}
\toprule
Method               & Test accuracy       \\ 
\midrule
\textit{Random}   &     35.14±3.78     \\ 
GraphSAGE~\citep{hamilton2017inductive}       & 46.78±0.67          \\
RGCN~\citep{RGCN2018}      & 47.37±0.48          \\

HGT~\citep{HGT}       & 49.27±0.61          \\
NARS~\citep{yu2020scalable}     & 50.88±0.12          \\
SeHGNN~\citep{yang2022simple}   &     51.79±0.24     \\ 
\model    &    \textbf{ 53.56±0.19 }     \\  
\midrule

SAGN~\citep{sun2021scalable}+\textit{label}          & 51.17±0.32          \\
GAMLP~\citep{GAMLP2021}+\textit{label}            & 51.63±0.22          \\ 
SeHGNN+\textit{label}           & 53.99±0.18          \\
\model+\textit{label}          & \textbf{55.54±0.21}     \\ 
\midrule
SeHGNN+\textit{label}+\textit{ms}          & 56.71±0.14 \\
\model+\textit{label}+\textit{ms}     &  \textbf{57.67±0.15}       \\
\bottomrule
\end{tabular}
}
\end{center}
\end{minipage}	
\quad
\begin{minipage}[t]{0.55\textwidth}
\caption{Micro-F1 ($\%$) of \model~and baselines on the three datasets, \ie, \texttt{DBLP}, \texttt{IMDB}, and \texttt{ACM}, from HGB benchmark. All methods do not employ data enhancements like label aggregation.}
\vspace{-7pt}
\label{tab:result_on_midlle_dataset}
\begin{center}
\vspace{-0.5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Method    & DBLP   & IMDB    & ACM        \\
\midrule 
RSHN~\citep{RSHN2019}    &   93.81±0.55   &   64.22±1.03     &     90.32±1.54        \\
HetSANN~\citep{HetSANN} &   80.56±1.50  &     57.68±0.44     &     89.91±0.37       \\
GTN~\citep{yun2019graph}   &    93.97±0.54    &    65.14±0.45     &     91.20±0.71    \\
HGT~\citep{HGT}     &   93.49±0.25    &    67.20±0.57     &     91.00±0.76      \\
Simple-HGN~\citep{lv2021we}     &   94.46±0.22     &   67.36±0.57     &    93.35±0.45   \\ 
HINormer~\citep{mao2023hinormer} &   94.94±0.21     &   67.83±0.34     &    -   \\ 
\midrule

RGCN~\citep{RGCN2018}   &  92.07±0.50  & 62.05±0.15   & 91.41±0.75        \\
HetGNN~\citep{HetGNN2019}     & 92.33±0.41      &   51.16±0.65     &  86.05±0.25   \\
HAN~\citep{wang2019heterogeneous}     &   92.05±0.62     &    64.63±0.58     &    90.79±0.43                   \\
MAGNN~\citep{fu2020magnn}   &   93.76±0.45     &   64.67±1.67       &   90.77±0.65               \\ 
SeHGNN~\citep{yang2022simple}  &   95.24±0.13 &   68.21±0.32 &    93.87±0.50           \\
\midrule

DiffMG~\citep{DBLP:conf/kdd/DingYZZ21} &     94.20±0.36   &  59.75±1.23   &  88.07±3.04  \\
PMMM~\citep{li2022differentiable}   &  95.14±0.22 &     67.58±0.22   &    93.71±0.17            \\  
\midrule
\model  & \textbf{95.59±0.16} &  \textbf{68.82±0.19} &  \ \textbf{94.56±0.17}  \\ 
\bottomrule
\end{tabular}
}
\end{center}	
\end{minipage}
\end{minipage}
\vspace{-3pt}
\end{table*}





 \subsection{Performance Comparison}
\textbf{Results on Ogbn-mag.}
To answer \textbf{Q1}, we report the test accuracy of our proposed \model~and baselines in Table~\ref{tab:result_on_large_dataset}.    
\textit{label} indicates utilizing labels as extra inputs to provide enhancements~\citep{wang2020unifying,shi2020masked, yang2022simple}. \textit{ms} means using multi-stage learning
~\citep{li2018deeper, sun2020multi}. We report results without or with \textit{label} and \textit{ms} for a comprehensive comparison. \textit{Random} means the result of replacing our searched meta-paths with $30$ random meta-paths. We show the average result of 20 random samples.   
It can be seen that \model~achieves state-of-the-art performance under each condition. For example, \model~outperforms the SOTA method SeHGNN by a large margin of $1.77\%$. The advantage of \model~compared to SeHGNN mainly comes from the ability to discover effective long-range meta-paths automatically. SeHGNN shows the results with extra embeddings as an enhancement in the original paper. Though we cannot compare results with this trick due to the untouchability of its embedding file, \model~still outperforms their best result.




\textbf{Performance on HGB Benchmark.} Table~\ref{tab:result_on_midlle_dataset} shows the Micro-F1 scores of \model~and baselines. The 1st, 2nd, and 3rd blocks are metapath-free, metapath-based, and NAS-based methods, respectively. The 4th block is our method. Every block contains a method yielding highly competitive performance, indicating that the HGNN field is very active and each mechanism has its advantage. However, \model~can still consistently achieves the best performance on all the three datasets.







\begin{table}
\centering
\begin{minipage}[th!]{\textwidth}
\centering
\begin{minipage}[t]{0.484\textwidth}
\caption{Results of \model~and SeHGNN on the sparse large-scale HINs. $\bm{\uparrow}$ means the improvements in test accuracy.}
\vspace{-7pt}
\label{tab:ogbn_reduce}
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Dataset & SeHGNN   & \model     &    $\bm{\uparrow}$  \\
\midrule
ogbn-mag-$5$  &  36.04±0.64  & 40.13±0.34 & \textbf{4.09}  \\
ogbn-mag-$10$ &  38.27±0.19  & 41.18±0.18 & \textbf{2.91}  \\
ogbn-mag-$20$ &  39.18±0.09  & 41.40±0.13 & \textbf{2.22}  \\
ogbn-mag-$50$ &  39.50±0.13  & 41.32±0.14 & \textbf{1.82}  \\
\bottomrule
\end{tabular}
}
\end{center}	
\end{minipage}
\quad 
\begin{minipage}[t]{0.485\textwidth}
\caption{Experiments on the generalization of the searched meta paths. * means using the meta-paths searched in \model.
} \vspace{-7pt}
\label{tab:ablation_transferability}
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Method    & DBLP   & IMDB    & ACM        \\\midrule
HAN     &  92.05±0.62     &    64.63±0.58     & 90.79±0.43                   \\
HAN*     & 93.54±0.15          &65.89±0.52      &92.28±0.47                          \\ \midrule
SeHGNN  &   95.24±0.13 &   68.21±0.32 &    93.87±0.50        \\
SeHGNN*  &   95.57±0.23    &   68.59±0.24      &    94.46±0.18    \\
\bottomrule
\end{tabular}
}
\end{center}
\end{minipage}
\end{minipage}
\vspace{-3pt}
\end{table}



 
\subsection{The Necessity of Long-range Dependency}
To answer \textbf{Q2} and explore the necessity of long-range dependency on HINs, we construct four large-scale datasets with high sparsity based on \texttt{ogbn-mag}. To avoid inappropriate preference seed settings of randomly removing, we construct fixed HINs by limiting the maximum in-degree related to edge type. Specifically, we gradually reduce the maximum in-degree related to edge type in \texttt{ogbn-mag} from $50$ to $5$ but leave all nodes unchanged. Details of the four datasets are listed in Appendix A.1.     
The test accuracy of \model~and SOTA method SeHGNN are shown in Table~\ref{tab:ogbn_reduce}. 
\model~outperforms SeHGNN more significantly with the increasing sparsity. In addition, the leading gap of \model~over SeHGNN is more than $4\%$ on the highly sparse dataset \texttt{ogbn-mag-5}. The main difference between SeHGNN and \model~is that the former cannot utilize large hops and only use hop $2$ while the latter has a maximum hop of $6$, demonstrating that long-range dependencies are more effective with increased sparsity and decreased direct neighbors in HINs.




\subsection{Effectiveness of the Searched Meta-paths}
To demonstrate the effectiveness of searched meta-paths, on one hand, the meta-paths should be effective in the proposed model, which the above experiments have verified. On the other hand, the meta-paths should be effective after being generalized to other HGNNs, which is important but has been ignored by previous works. To answer \textbf{Q3}, we verify the generalization of our searched meta-path on the most famous HGNN HAN~\citep{wang2019heterogeneous} and the SOTA method SeHGNN~\citep{yang2022simple}. The Micro-F1 scores on three representative datasets are shown in Table~\ref{tab:ablation_transferability}. After simply replacing the original meta-path set with our searched meta-paths and keeping other settings unchanged, the performance of both methods improves rather than decreases, demonstrating the effectiveness of our searched meta-paths. We also explore the effectiveness of our search algorithm. In our architecture, our meta-paths are replaced by those meta-paths discovered by other methods. DARTS~\citep{DBLP:conf/iclr/LiuSY19} is the first differentiable search algorithm in neural networks. SPOS~\citep{guo2020single} is a classic singe-path differentiable algorithm.  
DiffMG~\citep{DBLP:conf/kdd/DingYZZ21} and PMMM~\citep{li2022differentiable} search for meta-graphs instead of meta-paths. 
The derivation strategies of the four methods are unsuitable for discovering multiple meta-paths. So we changed their derivation strategies to ours to improve their performance. We report the Micro-F1 scores in Table~\ref{tab:effectiveness}. It can be seen that \model~shows the best performance, verifying the effectiveness of our search algorithm.









\begin{table}
\centering
\begin{minipage}[th!]{\textwidth}
\centering
\begin{minipage}[t]{0.442\linewidth}
\centering
\caption{Experiments on the effectiveness of our search algorithm. In our \model, the meta-paths are replaced by those discovered by other methods.
}
\resizebox{\linewidth}{!} {
\begin{tabular}{lccc}
\toprule
Method    & DBLP   & IMDB    & ACM        \\
\midrule
HAN  &  95.44±0.14  & 65.95±0.31 & 90.66±0.30  \\
GTN &  95.33±0.05  & 65.99±0.16 & 90.66±0.30  \\
DARTS &  95.35±0.17  & 66.23±0.14 & 93.45±0.13  \\
SPOS &  95.41±0.43  & 67.10±0.29 & 93.64±0.37  \\
DiffMG &  95.45±0.49  & 66.98±0.37 & 93.61±0.45  \\
PMMM &  95.48±0.27  & 67.49±0.24 & 93.74±0.22  \\
\model &  95.59±0.16  & 68.82±0.19 & 94.56±0.17  \\
\bottomrule
\end{tabular}
}
\label{tab:effectiveness}
\end{minipage}
\quad 
\begin{minipage}[t]{0.522\textwidth}
\caption{Experiments on ogbn-mag to analyze the performance of SeHGNN and \model~under different maximum hops. \#MP is the number of target-node-related meta-paths. "OOM" means out of memory.} 
\label{tab:ogb_max}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\vspace{-1pt}
\multirow{2}{*}{Max hop}& \multirow{2}{*}{\#MP} &   \multicolumn{2}{c}{SeHGNN}  &   \multicolumn{2}{c}{\model}    \\ 
\cmidrule(lr){3-4}\cmidrule(lr){5-6}
\vspace{-1pt}
&  &   Time& Test accuracy &   Time& Test accuracy \\
\midrule
\vspace{1pt}
1& 4 & ~~4.35 & 47.18±0.28  &   ~~3.98 &    46.88±0.10     \\
\vspace{1pt}
2& 10  & ~~6.44 & 51.79±0.24 &  ~~5.63   &   51.91±0.13   \\
\vspace{1pt}
3& 23 & 11.28 & 52.44±0.16 &   10.02  &  52.72±0.24   \\
\vspace{1pt}
4&50 & OOM & OOM&    14.34 &      53.33±0.18   \\
\vspace{1pt}
5&107 & OOM & OOM &  14.77 &     53.42±0.21    \\
6&226 & OOM &  OOM & 14.71  &     53.56±0.19   \\
\bottomrule
\end{tabular}}
\end{minipage}
\end{minipage}
\end{table} 



\begin{figure}[!t]
\footnotesize
\centering
\subfigure[Micro-F1]
{\includegraphics[width=0.326\linewidth]{fig/hop-acc.pdf}}
\vspace{1.2pt}
\hspace{0.3em}
\subfigure[GPU memory cost]
{\includegraphics[width=0.342\linewidth]{fig/hop-memory.pdf}}
\hspace{0.3em}
\subfigure[Time cost]
{\includegraphics[width=0.29\linewidth]{fig/hop-time.pdf}}
\vspace{-3pt}
\caption{ Illustration of 
(a) performance, (b) memory cost, (c) average training time of Simple-HGN, SeHGNN, and \model~relative to the maximum hop or layer on \texttt{DBLP}. The gray dotted line in (a) indicates the number of target-node-related meta-paths under different maximum hops.  
}
\label{fig:largehop}
\vspace{-3pt}
\end{figure} \subsection{Analysis on Large Maximum Hops}





To answer \textbf{Q4}, we compare the performance and training time of \model~with SeHGNN under different maximum hops on large-scale dataset \texttt{ogbn-mag}. When the maximum hop $l=1,2,3$, we utilize the full meta-paths set because the number of target-node-related meta-paths is smaller than $M=30$. 
Following the convention~\citep{lv2021we,yang2022simple}, we measure the average time consumption of one epoch for each model. As shown in Table~\ref{tab:ogb_max}, both the performance of \model~and SeHGNN increases with the growth of maximum hop value.   
Moreover, when the number of target-node-related meta-paths is larger than $30$, the training time of \model~is stable under different maximum hops. 
We also conduct experiments to compare the performance, memory, and efficiency of \model~with the best metapath-free method with source code, Simple-HGN~\citep{lv2021we}, and the best metapath-based method, SeHGNN~\citep{yang2022simple}, on \texttt{DBLP}. Figure~\ref{fig:largehop} (a) shows that \model~has consistent performance with  the increment of maximum hop. 
The failure of Simple-HGN demonstrates the attention mechanism can not eliminate the effects of noise under large hop. 
Figure~\ref{fig:largehop} (b), (c) illustrate each training epoch's average memory and time costs relative to the maximum hop or layer. 
We can observe that the consumption of SeHGNN exponentially grows, and the consumption of Simple-HGN linearly increases, which is consistent with their time complexity as listed in Table~\ref{tab:time_complexity}. At the same time, \model~has almost constant consumption as the maximum hop grows. Figure~\ref{fig:largehop} (c) shows the time cost of \model~in the search stage, which also approximates a constant when the number of meta-paths is larger than $M=30$. 


  
\subsection{Discussion}
The performance of \model~does not always increase with the value of the maximum hop, and the best maximum hop depends on the dataset. For instance, \model~can effectively utilize 12-hop meta-paths on \texttt{DBLP} with high performance and low cost.
However, \texttt{DBLP} is not very sparse, so \model~shows the best performance when the maximum hop is $6$ because longer meta-paths may bring noise. In Table~\ref{tab:ogbn_reduce}, we conduct experiments to demonstrate that longer meta-paths are more useful for sparser HINs. 
In addition, based on Table~\ref{tab:time_complexity}, the time complexity of \model~does not increase with the maximum hop. Consequently, it provides an effective solution for utilizing long-range dependency on HINs for possible applications. In future work, we would consider introducing residual mechanisms into the meta-path to boost the performance of longer meta-paths.





 