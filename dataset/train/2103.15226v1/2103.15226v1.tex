\vspace{-0.8ex}
\section{Experimental Results}
\label{secExp}

We now give the results of the proposed methods on challenging benchmarks.

We denote the proposed methods as (i) \emph{ours (representation)}, when we incorporate the
proposed local geometric representation, and (ii) \emph{ours (representation + graph)} when we
incorporate the proposed geometrically constrained graph construction as well. 

\vspace{-0.6ex}
\subsection{Datasets and Experimental Setup}
We evaluate the proposed method on the following datasets ModelNet40~\cite{wu20153d}, ShapeNet~\cite{yi2016scalable}, ScanNet~\cite{dai2017scannet}, Stanford Large-Scale 3D Indoor Spaces (S3DIS) scans~\cite{armeni20163d}, and Paris-Lille-3D (PL3D)~\cite{roynard2018paris}. We follow standard settings for training and evaluation on respective datasets. We use Dynamic Edge Convolutional network (DGCNN) \cite{wang2019dynamic} as the baseline network. Further, the input to the MLP is the 9D vector, it contains two FC layers with  and  units respectively.

\vspace{-0.6ex}
\subsection{Parametric Study}
Table~\ref{tabNN} gives the performances of the methods with different number of nearest neighbors.
We see that for the baseline method, DGCNN with \knn based graph construction, the performance
initially increases with an increase in  but then the drops a little, \eg mean class accuracy
goes from  to  for , but then drops to  for . The authors in DGCNN
\cite{wang2019dynamic} attributed this to the fact that for densities with large  the Euclidean
distance is not able to approximate the geodesic sufficiently well. However, for the proposed method
the performance improves by a modest amount and does not drop, \ie  for
 respectively. This indicates that with explicit geometric information provided as input,
the geometry is relatively better maintained \cf DGCNN. As the performance increase from  to
 is modest for our method, we use  as a good tradeoff between performance and time.

The values of  and  were set based on validation experiments on a subset of the training and validation data for all experiments reported. 

\begin{table}[t]
\center
\caption{\small{Results on ModelNet40 classification with varying number of nearest neighbors (\#NN) for graph construction}}
\label{tabNN}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc|cc|cc|cc}
\toprule
\# NN &
\multicolumn{2}{c|}{{DGCNN (\knn)}} &
\multicolumn{2}{c|}{{ours (repr.) + \knn}} &
\multicolumn{2}{c|}{{ours (graph)}} &
\multicolumn{2}{c}{{ours (repr. + graph)}} \\ \hline
 & \multicolumn{1}{c}{m-acc(\%)} & \multicolumn{1}{c|}{ov-acc(\%)} & \multicolumn{1}{c}{m-acc(\%)}
& \multicolumn{1}{c|}{ov-acc(\%)} & \multicolumn{1}{c}{m-acc(\%)} & \multicolumn{1}{c|}{ov-acc(\%)} 
& \multicolumn{1}{c}{m-acc(\%)} & \multicolumn{1}{c}{ov-acc(\%)} \\ \hline
\midrule
5  & 88.0 & 90.5 & 88.9 & 91.1 & 88.3 & 90.8 & 89.8 & 91.9 \\ 
10 & 88.9 & 91.4 & 90.1 & 92.4 & 89.2 & 92.6 & 91.6 & 93.9 \\ 
20 & 90.2 & 92.9 & 92.0 & 95.1 & 90.8 & 94.4 & 93.1 & 95.9 \\ 
40 & 89.4 & 92.4 & 91.4 & 94.6 & 91.2 & 95.1 & 93.8 & 96.6 \\ 
\hline
\end{tabular}
}
\vspace{-2ex}

\end{table}

\begin{table}[h]
\begin{center}
\caption{\small{Comparison with state of the art methods on ModelNet40 classification and ShapeNet part segmentation datasets.}}
\label{tabModelNet40Cls}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c c c r}
\hline
  Method & \multicolumn{2}{c}{ModelNet40} & ShapeNet \\ \hline
    & m-acc(\%)  & ov-acc(\%) & mIoU(\%) \\
\hline

PointNet \cite{qi2016pointnet}                   & 86.0 & 89.2 & 83.7\\
PointNet++ \cite{qi2017pointnet++}                    & - & 90.7 & 85.1 \\
Kd-net \cite{klokov2017escape}
    & - & 90.6  & 82.3 \\

DensePoint \cite{zhao20193d} & - & 93.2 & 86.4\\
LP-3DCNN \cite{kumawat2019lp} & - & 92.1 & -\\
DGCNN \cite{wang2019dynamic}                         & 90.2 & 92.9 & 85.2 \\
DGCNN (2048 pts) \cite{wang2019dynamic}                & 90.7 & 93.5 & - \\
LDGCNN \cite{zhang2019linked} & 90.3 & 92.9 & 85.1 \\
KPConv-rigid \cite{thomas2019kpconv} & - & 92.9 & 86.2 \\
KPConv-deform \cite{thomas2019kpconv} & - & 92.7 & 86.4 \\
GSN~\cite{xu2020geometry} & - & 92.9 & - \\
GSN (2048 pts)~\cite{xu2020geometry} & - & 93.3 & - \\
LS~\cite{lyu2020learning} & - & - & 88.8 \\
PointASNL~\cite{yan2020pointasnl} & - & 93.2 & - \\
SPH3D-GCN (2048 pts)~~\cite{lei2020spherical} & 88.5 & 91.4 & 86.8 \\
FKAConv~\cite{boulch2020lightconvpoint} & 89.9 & 92.5 & - \\
FKAConv (2048 pts)~\cite{boulch2020lightconvpoint} & 89.7 & 92.5 & 85.7 \\
ConvPoint~\cite{boulch2020convpoint} & 88.5 & 91.8 & 85.8 \\
ConvPoint (2048 pts)~\cite{boulch2020convpoint} & 89.6 & 92.5 & - \\
\hline
Ours (repr.)                        & \textbf{92.0} & \textbf{95.1} & 87.4\\
Ours (repr.\ + graph)                       & \textbf{93.1} & \textbf{95.9} & \textbf{89.1} \\
Ours (repr.\ + graph) - 2048 pts                       & \textbf{94.1} & \textbf{96.9} & - \\
\hline
\end{tabular}
}
\end{center}
\vspace{-2ex}
\end{table}

\vspace{-3ex}
\subsection{Comparison with state of the art}
\vspace{-1.5ex}
\heading{Classification on ModelNet40.} The classification results on the ModelNet40 datasets are shown in Table \ref{tabModelNet40Cls} with comparison against methods based on 3D input. In
the last block the first two rows of our method are with  points while the third (last) is with  points.  Our full method achieves the best results, \eg  overall class accuracy, compared to many recent
competitive methods such as PointASNL (), RS-CNN () and the baseline DGCNN (). We also see that our method improves more when the point sampling is increased from  to
 with mean accuracy going from  to , \cf  DGCNN  to  for the same settings. 

\heading{ShapeNet Part Segmentation.} Table \ref{tabModelNet40Cls} (Column 4) shows results on ShapeNet part segmentation. Similar to the classification case, we achieve higher results than many recent methods. Our representation only method achieves  mIoU \cf  of DGCNN \cite{wang2019dynamic}. While
our full, representation and graph construction based method achieves  which is higher than recently proposed learning to segment (LS) by . However, LS projects the 3D points to 2D images and uses U-Net segmentation, while the proposed method directly computes the part labels on 3D points.

\heading{Semantic Segmentation.} Table \ref{tab:semanticseg} show results of semantic segmentation on ScanNet, Stanford Indoor Spaces Dataset (S3DIS) and  Paris-Lille-3D datasets (PL3D). On S3DIS dataset, we outperform all the compared methods \wrt mIoU and overall accuracy (ov-acc) on Area-5 protocol and \wrt overall accuracy on k-fold evaluation protocol. Further, we also outperform other methods \wrt overall accuracy on PL3D dataset. We outperform the next best method on PL3D (KPConv-deform, ConvPoint) by  on mIoU while lag behind FKAConv by , however, we outperform FKAConv on S3DIS (k-fold) by  \wrt mIoU. On ScanNet we achieve an mIoU of  lagging behind MinkowskiNet () and Virtual MVFusion (), however, we outperform these methods on S3DIS by .  

Overall, we observe that our method consistently provides results amongst top- methods on all the compared benchmarks, demonstrating the robustness of the proposed modifications to datasets with significant noise \eg ScanNet and well defined CAD models \eg ModelNet40.

\begin{table}[!t]
\caption{\small{Comparison with state of the art methods for 3D scene segmentation on Scannet v2, Stanford 3D Indoor Spaces (S3DIS) and Paris-Lille-3D (PL3D) datasets. 
}}
\vspace{-2ex}
\label{tab:semanticseg}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ l|c c c c c c c }
\hline
Method & Scannet & \multicolumn{2}{c}{S3DIS (k-fold)} & \multicolumn{2}{c}{S3DIS (Area 5)} & \multicolumn{2}{c}{PL3D}\\
\hline
& mIoU & mIoU & ov-acc & mIoU & ov-acc & mIoU & ov-acc \\
\hline
PointNet \cite{qi2016pointnet} & - & 47.6 & 78.6 & 41.1 & - & 40.2 & 93.9 \\
PointNet++ \cite{qi2017pointnet++} & 33.9 & 54.5 & 81.0 & - & - & 36.1 & 88.7 \\
PointCNN \cite{li2018pointcnn} & 45.8 & 65.4 & 88.1 & 57.3 & 85.9 & 65.4 & -\\
KPConv-rigid~\cite{thomas2019kpconv} & 68.6 & 69.6 & - & 65.4 & - & 72.3 & -\\
KPConv-deform~\cite{thomas2019kpconv} & 68.4 & \textbf{70.6} & - & 67.1 & - & 75.9 & -\\
DGCNN~\cite{wang2019dynamic} & - & 56.1 & 84.1 & - & - & 62.5 & 97.0\\
MinkowskiNet~\cite{choy20194d} & 73.6 & - & - & 65.4 & - & - & -\\
HDGCNN~\cite{liang2019hierarchical} & - & 66.9 & - & 59.3 & - & 68.3 & - \\
DPC \cite{engelmann2019dilated} & 59.2 & - & - & 61.3 & 86.8 & - & - \\
Virtual MVFusion \cite{kundu2020virtual} & \textbf{74.6} & - & - & 65.3 & - & - & -\\
JSENet~\cite{hu2020jsenet} & 69.9 & - & - & 67.7 & - & - & -\\
FusionNet~\cite{zhang12356deep} & 68.8 & - & - & 65.38 & - & - & -\\
DCMNet~\cite{schult2020dualconvmesh} & 65.8 & 69.7 & - & 64.0 & - & - & -\\
PointASNL~\cite{yan2020pointasnl} & 63.0 & 68.7 & - & - & - & - & - \\
SPH3D-GCN~\cite{lei2020spherical} & 61.0 & 68.9 & 88.6 & 59.5 & 87.7 & - & - \\
SegGCN~\cite{lei2020seggcn} & 58.9 & 68.5 & 87.8 & 63.6 & 88.2 & - & - \\
ResGCN-28~\cite{li2019deepgcns} & - & 60.0 & 85.9 & - & - & - & - \\
FKAConv~\cite{boulch2020lightconvpoint} & - & 68.4 & - & - & - & \textbf{82.7} & - \\
ConvPoint~\cite{boulch2020convpoint} & - & 69.2 & 88.8 & - & - & 75.9 & - \\

\hline
Ours (repr.+graph)  & 72.4 & 70.1 & \textbf{89.8} & \textbf{69.4} & \textbf{89.4} & 78.5 & \textbf{98.0}\\
\hline
RANK & \textbf{\textcolor{cyan}{3}} & \textbf{\textcolor{blue}{2}} & \textbf{\textcolor{red}{1}} & \textbf{\textcolor{red}{1}} & \textbf{\textcolor{red}{1}} & \textbf{\textcolor{blue}{2}} & \textbf{\textcolor{red}{1}}\\
\hline
\end{tabular}
}
\vspace{-2ex}

\end{table}

\vspace{-1ex}
\subsection{Qualitative Results}
Figure \ref{figQuals3dis} shows semantic segmentation results on S3DIS. In general, we observe that
the proposed technique provides closer to ground-truth segmentation especially at the boundaries. In
first row, we observe that the structures on the wall are better constructed by the proposed method 
as compared to DGCNN especially around the edges. In the box at center, we observe that the proposed
method marks the points on the wall behind the table correctly while DGCNN incorrectly
labels them as wall. Similarly, in second row, the labels in the box at top are closer to the ground
truth with a clear separation from the wall. In the bottom box, we observe that the points
on the table are clearly labelled by our method while DGCNN labels many points as walls. These
results indicate that the proposed method is robust in narrow regions and on fine structures due to
the explicit introduction of geometrical features to GCNN. 

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figs/qual_s3dis.pdf}
\vskip -0.1in
\caption{Qualitative results on S3DIS dataset, notice the fine structures captured by our method and
missed by DGCNN, in the annotated black boxes. Best viewed zoomed in color.}
\label{figQuals3dis}
\vskip -0.2in
\end{figure}

\begin{table}[h]
\caption{\small{Ablation studies on impact of various components of the proposed method.}}
\vspace{-2ex}
    \label{tabAblAll}
\centering
   
\resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|cccccc}
\hline
        & & & \multicolumn{2}{c}{  ModelNet40   } &   ShapeNet   &
        \multicolumn{2}{c}{  S3DIS   }  \\ \hline
         & \; \;  \; \; & graph \quad \; &   m-acc   & ov-acc &
        mIoU &  mIoU  & ov-acc  \\
        \hline
 &  &           & 90.2 & 92.9  & 85.2 & 56.1 & 84.1\\
        \checkmark &  &         & 90.8 & 93.4  & 86.1 & 57.8 & 84.7\\
          & \checkmark &        & 90.7 & 93.8  & 85.9 & 57.8 & 85.1\\
          &   & \checkmark      & 90.8 & 94.4  & 86.2 & 59.4 & 86.6\\
         \checkmark &   & \checkmark    & 92.1 & 95.1  & 87.1 & 61.2 & 87.4\\
            & \checkmark & \checkmark   & 91.8 & 94.9  & 86.8 & 62.1 & 87.4\\
        \checkmark   & \checkmark &     & 92.0 & 94.0  & 86.6 & 65.5 & 88.1 \\
        \checkmark   & \checkmark & \checkmark  & 93.1 & 95.9  & 89.1 & 69.4 & 89.4 \\
        \hline
\end{tabular}
    }
    \vspace{-2ex}
     
\end{table}

 \vspace{-1.5ex}
\subsection{Ablation Experiments}\label{subsec:ablation}
 \vspace{-0.5ex}

We perform ablation experiments to give the complete results with and without including the different
components of the proposed method.

For ablation on absence of various local features, we only set the corresponding elements of the
input vector to the MLP and train the projection MLP as usual. 

\heading{Ablation on Classification.}
In Table \ref{tabAblAll} columns 4 and 5, starting from a baseline of  overall accuracy, we see that the local
geometric features help individually, but their combination is better at . Similarly, the
graph construction method gives  alone but improves with each of the geometric features
added, and performance is best when all the features as well as graph construction are used
together, at . 

\heading{Ablation on Part Segmentation.}
In Table \ref{tabAblAll} column 6, we see similar trends as in the classification case, where each of the geometric features
contribute (), the graph construction helps when used alone (), and the
full combination achieves the best performance at  mIoU, \cf the baseline at  mIoU.

\heading{Ablation on Semantic Segmentation.} In Table \ref{tabAblAll} columns 7 and 8, we observe that
with the graph construction only the mIoU increases from  to  while it increases to
 with only the combination of local features. However, the full combination provides the
best performance at mIoU of  and overall accuracy of .

\begin{table}[h]


\begin{center}
\caption{\small{Ablation study on the training convergence of models}}
\label{tabEpochs}
\resizebox{\linewidth}{!}{
    \begin{tabular}{cc|cccccc}
\hline
    & & \multicolumn{2}{c}{\; ModelNet40 \; } & \multicolumn{2}{c}{\; ShapeNet \; }  & \multicolumn{2}{c}{\; S3DIS \; } \\
    \hline
       & \; \; graph \; \; & \quad tr.\ epochs \quad & frac.& \quad tr.\
     epochs \quad & frac.& \quad tr.\ epochs \quad & frac.\\
     \hline
 &          & 233  & 1.00 & 190 & 1.00 & 258 & 1.00 \\
    \checkmark &        & 205  & 0.87 & 174 & 0.91 & 243 & 0.94 \\
     & \checkmark       & 164  & 0.70 & 151 & 0.79 & 215 & 0.83 \\
     \checkmark & \checkmark    & 139  & 0.60 & 133 & 0.70 & 191 & 0.73 \\
     \hline
\end{tabular}
}
\vspace{-2em}
\end{center}
\end{table}

\heading{Ablation on Model Convergence} 
The number of epochs required for the different models to converge are shown in Table
\ref{tabEpochs}. We observe that using the proposed geometric representation results in reduction by
,  and  epochs for classification, part segmentation and semantic segmentation
respectively. With the proposed geometrically aware graph construction, we observe a reduction of
 for classification,  for part segmentation and  for semantic segmentation. Hence,
providing a geometrically refined local connectivity information to the graph results in significant
reduction in the number of epochs required by the network. Finally, the combination of the proposed
geometric representation and the proposed graph construction results in a reduction of  epochs required for the tasks respectively.

The actual training time required to train our full method on ModelNet40 is  hours as
compared to  hours required by DGCNN, providing a training speed-up of nearly , which is
similar to the epoch reduction of , \ie less epochs translate directly into reduction
in training time. 

\heading{Ablation on MLP.} With only 3D coordinates as input to the MLP, the m-acc on ModelNet40 is  \cf  (DGCNN). With 9D vector as a direct input without MLP to DGCNN, the m-acc is  \cf  with MLP (Table \ref{tabModelNet40Cls}, Ours(repr.)). This shows that MLP is able to extract a robust geometric representation in the feature space.
