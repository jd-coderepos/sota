

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption,subcaption}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\newcommand{\cmark}{\ding{52}}
\newcommand{\xmark}{\ding{56}}


\begin{document}

\title{Learnable Boundary Guided Adversarial Training}
\author{
	Jiequan Cui  \quad 
	Shu Liu  \quad
	Liwei Wang  \quad
	Jiaya Jia  \\
	The Chinese University of Hong Kong \hspace{1cm} SmartMore \hspace{1cm} Tencent \vspace{.7em}\\
	{\tt\small \{jqcui, leojia\}@cse.cuhk.edu.hk, \{liushuhust, wlwsjtu1989\}@gmail.com}
}


\maketitle
\begin{abstract}
   	Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, our target is to reduce natural accuracy degradation. We use the model logits from one clean model  to guide learning of the robust model , taking into consideration that logits from the well trained clean model  embed the most discriminative features of natural data, {\it e.g.}, generalizable classifier boundary. Our solution is to constrain logits from the robust model  that takes adversarial examples as input and make it similar to those from a clean model  fed with corresponding natural data. It lets  inherit the classifier boundary of . Thus, we name our method Boundary Guided Adversarial Training (BGAT).
    Moreover, we generalize BGAT to Learnable Boundary Guided Adversarial Training (LBGAT) by training  and  simultaneously and collaboratively to learn one most robustness-friendly classifier boundary for the strongest robustness. Extensive experiments are conducted on CIFAR-10, CIFAR-100 and challenging Tiny ImageNet datasets. Along with other state-of-the-art adversarial training approaches, {\it e.g.}, Adversarial Logit Pairing (ALP) and TRADES, the performance is further enhanced.
\end{abstract}



\begin{figure}
	\begin{center}
		\includegraphics[width=0.50\textwidth]{Figs/comparison_withmethods.pdf}
		\caption{Model robustness on CIFAR-100 evaluated with 20 iterations PGD under white-box attack. ``Natural Acc'' represents classification accuracy on natural (clean) data. ``Robust Acc'' represents classification accuracy on adversarial data.
			Our method (LBGAT+TRADES with ) improves robustness with the least natural accuracy degradation.}
		\label{fig:comparison_methods}
	\end{center}
    \vspace{-0.2in}
\end{figure}

\section{Introduction}
Deep neural networks have achieved great success in many tasks. With the concern of security of deep models, several methods~\cite{DBLP:conf/cvpr/DongLPS0HL18,Xie_2019_CVPR,DBLP:journals/corr/SzegedyZSBEGF13,Shi_2019_CVPR,DBLP:conf/iclr/TramerKPGBM18,DBLP:conf/aaai/ZhengC019,DBLP:conf/iclr/TramerKPGBM18,DBLP:conf/cvpr/HeZRS16,DBLP:conf/cvpr/HuangLMW17,DBLP:journals/corr/SimonyanZ14a} have shown that deep models could be vulnerable to adversarial attack. Data that is intentionally optimized may easily fool strong classifiers. 

In response to the vulnerability of deep neural networks, adversarial defense has become an essential topic in computer vision. There are now a sizable body of work exploring different ways to get adversarial settings, including defensive distillation \cite{DBLP:conf/sp/PapernotM0JS16}, feature squeezing \cite{DBLP:conf/ndss/Xu0Q18}, randomization based methods \cite{DBLP:conf/iclr/XieWZRY18,DBLP:conf/iclr/DhillonALBKKA18} and augmenting the training with adversarial examples~\cite{zhang2019theoretically,DBLP:journals/corr/abs-1803-06373,DBLP:conf/iclr/MadryMSTV18,DBLP:conf/iclr/TramerKPGBM18}, {\it i.e.}, adversarial training. However, training a robust model is still challenging. Recently, adversarial training with PGD attack~\cite{DBLP:conf/iclr/MadryMSTV18} becomes an effective defense strategy. However, when plotting results of recent work~\cite{zhang2019theoretically,DBLP:journals/corr/abs-1803-06373,DBLP:conf/iclr/MadryMSTV18} in Fig. \ref{fig:comparison_methods}, it is still noticeable that higher robustness is often accompanied with more accuracy degradation on natural data classification. 

Different from previous work that mainly pursues various ways to improve robustness, we meanwhile pursue accuracy preservation on natural data.
In this paper, we propose a novel adversarial training scheme, which significantly improves classification accuracy on natural data. It also achieves high robustness under black- and white-box attack. We take advantage of logits from a clean model, which is trained only on natural data, to guide the learning of a robust model. 

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=1.01\textwidth]{Figs/conceptual_illistration.pdf}
		\caption{Conceptual illustration of our method vs. previous adversarial training approaches. Solid lines denote real classifier boundary of the trained model, while the dotted line is the classifier boundary of the clean model . Different shapes represent logits of images in various classes. Black color marks adversarial examples.} 
		\label{fig:illustration}
	\end{center}
    \vspace{-0.1in}
\end{figure*}

A conceptual illustration is shown in Fig. \ref{fig:illustration} to explain our motivation. As shown in (a), when only trained on natural (clean) data, the learned model  separates natural data (plotted in yellow) well. But it easily fails to classify perturbed data and misclassifies the dark circle into the rectangle category. Previous standard adversarial training methods, {\it e.g.}, Madry et al. \cite{DBLP:conf/iclr/MadryMSTV18} mainly improve the robustness towards adversarial examples. As shown in Fig. \ref{fig:illustration}(c), adversarial examples (plotted in black) can be mostly correctly classified with this strategy. However, some clean data is wrong. Thus, our motivation is to leverage the clean model  to improve the natural data accuracy of .

In order to seek guidance from clean model , we expect the logit output of adversarial example  from  to be similar to logits output of corresponding natural data  that goes through . As plotted in Fig. \ref{fig:illustration}(b), the classifier boundary of our  is constrained by that of the clean model, which helps classify the clean data into correct categories. At the same time, adversarial examples are also correctly labeled, benefiting from the adversarial training scheme.

Instead of constraining  with the classifier boundary from one well trained static ,
we further generalize our BGAT method to Learnable Boundary Guided Adversarial Training (LBGAT) by training  and our required model  at the same time to dynamically adjust classifier boundary of  and learn the robustness-friendly one to further help  enhance robustness. To show the flexibility of our method, we incorporate our model into state-of-the-art methods Adversarial Logit Pairing (ALP) \cite{DBLP:journals/corr/abs-1803-06373} and TRADES \cite{zhang2019theoretically} respectively and accomplish remarkable improvement over the baselines.

We conduct experiments on CIFAR-10, CIFAR-100, and the more challenging Tiny ImageNet to evaluate the performance of our models under both white- and black-box attack. Our models achieve impressive performance on these datasets. For example, our ``LBGAT+TRADES ()'' improves TRADES () by 13.53\% and 3.3\% on natural data of CIFAR-100 and CIFAR-10 respectively. It is noteworthy that the robust accuracy is even slightly better than TRADES () under the strongest auto-attack \cite{croce2020reliable}. On Tiny-ImageNet, our ``LBGAT+TRADES ()'' outperforms TRADES () by 9.29\% on natural data and surpasses it in aspect of model robustness under 20 iterations of PGD attack.
Moreover, under the black-box attack setting, our best model is 12.83\% higher than TRADES () with the natural trained model as the source model on CIFAR-100.


\section{Related Work}
\subsection{Adversarial Attack}
\paragraph{White-box Attack}
Szegedy et al.  \cite{DBLP:journals/corr/SzegedyZSBEGF13} observed that CNNs are vulnerable to adversarial examples computed by the proposed box-constrained L-BFGS attack method. Goodfellow et al. \cite{DBLP:journals/corr/GoodfellowSS14} attributed the existence of adversarial examples to the linear nature of networks, which yields the fast gradient sign method (FGSM) for efficiently generating adversarial examples. 

FGSM was further extended to different versions of iterative attack methods. Kurakin et al. \cite{DBLP:conf/iclr/KurakinGB17a} showed that adversarial examples could exist in the physical world with an I-FGSM attack and iteratively applied FGSM multiple times with a small step size.  Madry et al. \cite{DBLP:conf/iclr/MadryMSTV18} proposed Projected Gradient Descent (PGD) method as a universal “first-order adversary”, {\it i.e.}, the most active attack utilizing the local first-order information about the network. 

Dong et al. \cite{DBLP:conf/cvpr/DongLPS0HL18} integrated the momentum term into an iterative process for attack, called MI-FGSM, to stabilize update of directions and escape from poor local maxima during iterations. This method obtains more transferable adversarial examples. Moreover, boundary-based methods like DeepFool \cite{DBLP:conf/cvpr/Moosavi-Dezfooli16} and optimization-based methods like C\&W \cite{DBLP:conf/sp/Carlini017} were also developed, making adversarial defense more challenging. Recently, the ensemble of diverse attack methods -- auto-attack \cite{croce2020reliable} by Croce et al., consisting of APGD-CE \cite{croce2020reliable}, APGD-DLR \cite{croce2020reliable}, FAB \cite{croce2020minimally}, and Square Attack \cite{ACFH2020square}, became popular benchmark for testing model robustness.  

\paragraph{Black-box Attack}
There are also many ways to explore the transferability of adversarial examples for the black-box attack. Liu et al. \cite{DBLP:conf/iclr/LiuCLS17} was the first to study the transferability of targeted adversarial examples. They observed that a large proportion
of target adversarial examples were able to transfer with their target labels using the proposed ensemble-based attack method. Dong et al. \cite{DBLP:conf/cvpr/DongLPS0HL18} showed that iterative attack methods incorporating the momentum term achieved better transferability. Further, Xie et al. \cite{DBLP:conf/cvpr/XieZZBWRY19} boosted the transferability of adversarial examples by creating diverse input patterns with random resize and random padding.

\subsection{Adversarial Defense}
Recent work focuses generally on developing defense methods to improve model robustness, including input transformation-based methods, randomization based methods \cite{DBLP:conf/iclr/XieWZRY18,DBLP:conf/iclr/DhillonALBKKA18}, and adversarial training~\cite{zhang2019theoretically,DBLP:journals/corr/abs-1803-06373,DBLP:conf/iclr/MadryMSTV18,DBLP:conf/iclr/TramerKPGBM18}. 
Athalye et al. \cite{pmlr-v80-athalye18a} showed that adversarial training with PGD had withstood active attacks. Tram{\`{e}}r et al. \cite{DBLP:conf/iclr/TramerKPGBM18} raised model robustness under black-box attack by the proposed ensemble adversarial training, \emph{i.e.}, producing adversarial examples by static ensemble models. 
Madry et al. \cite{DBLP:conf/iclr/MadryMSTV18} used the universal first-order adversary, {\it i.e.}, PGD attack, to obtain adversarial examples in the course of adversarial training. 
Differently, Kannan et al. \cite{DBLP:journals/corr/abs-1803-06373} enhanced model robustness with adversarial logit pairing, which encourages the logits from natural images and adversarial examples to be similar to each other in the same model. 

Moreover, Zhang et al. \cite{zhang2019theoretically} regularized the output from natural images and adversarial examples with the KL-divergence function, meanwhile using a variant of PGD attack. 
Xie et al. \cite{xie2019intriguing} studied the effect of normalization in adversarial training and proposed the Mixture BN mechanism that uses separate batch normalization layers for natural data and adversarial examples in one model. It still requires the strong assumption of knowing whether an image is natural or adversarial, at inference time, which may not be that practical. 

\subsection{Knowledge Distillation}
Knowledge distillation was first used in \cite{DBLP:journals/corr/HintonVD15} by Hinton et al., which was then widely applied to distill knowledge from a teacher model to a student model. The typical application of knowledge distillation is model compression, transferring from a large network or ensembles to a small network that better suits low-cost computing. Since this work, several methods \cite{Tung_2019_ICCV,DBLP:conf/cvpr/ParkKLC19,DBLP:journals/corr/SauB16,DBLP:conf/iclr/TarvainenV17,DBLP:journals/corr/abs-1710-07535, tian2019crd} were proposed to further improve performance on model compressing and other tasks. 

Goldblum et al. \cite{goldblum2019adversarially} analyzed the application of knowledge distillation in adversarial training and proposed Adversarial Robust Distillation (ARD) to transfer robustness from a large adversarially trained model to a smaller one. In this paper, we propose to use one robustness-friendly boundary learned by one natural model, not necessarily large, to guide the adversarial training without cross-entropy loss. By this way, the robust model can sufficiently inherit the classifier boundary and thus preserves high accuracy on natural data.

\section{Our Method}
\subsection{Boundary Guided Adversarial Training}
As suggested by Madry et al. \cite{DBLP:conf/iclr/MadryMSTV18}, projected gradient descent (PGD) is a universal first-order adversary. Robust methods to defense PGD might be able to resist attack stemming from other first-order methods as well. Similarly, we use adversarial training with
PGD as

where  is the training data distribution,
 is the standard cross-entropy loss function with data point  and its corresponding true label .  represents parameters of the model, and the maximization with respect to  is approximated using noisy BIM~\cite{DBLP:conf/iclr/KurakinGB17a}. We denote the adversarial example  across the paper as . Following previous work~\cite{zhang2019theoretically,DBLP:conf/iclr/MadryMSTV18},  is bounded by .

Our expectation of the robust model is to achieve decent robustness and at the same time keep high accuracy on natural images. 
As illustrated in Fig. \ref{fig:illustration}, we make use of logits from a clean model to help shape the classifier boundary of the robust model. The logits of our required robust model  with  taken as input should be similar to those of  taking  as input. This relation is expressed as

where  is Mean Square Error (MSE) loss function in our experiments and  denotes the logits of model  taking  as input.  is the parameter of . We randomly initialize  and off-line train  on natural data in our experiments.

Our method can be understood from the perspective of \textit{classifier boundary guidance}. Here we give analysis of why our method can yield high performance on natural data.

\paragraph{Classifier Boundary Guidance} 
Since we assume that  is well trained on natural data, logits from  embed more discriminative features for classification, especially the classifier boundary. According to Eq. \eqref{f_BGAT}, when we impose the logits constraints, the system penalizes more on those pairs ( and ) that have more substantial discrepancy in classification. Therefore, this logit guidance makes  inherit decent classifier boundary for adversarial data. Actually, the inherited classifier boundary is still applicable to natural data in following explanation.

It is noteworthy that the adversarial example  is located in the  ball of . According to the min-max mechanism of PGD~\cite{DBLP:conf/iclr/MadryMSTV18}, when the adversarial training converges, the loss value corresponding to  is always larger than the loss value corresponding to  when passing  and  into the same model . Therefore, when we pull  into the correct class with our proposed logits constraints,  is also squeezed into the correct class. Thus the inherited classifier boundary from  separates natural data well and preserves high natural accuracy. 

\begin{figure}
	\begin{center}
		\includegraphics[width=0.4\textwidth]{Figs/LBGAT.pdf}
		\caption{Learnable Boundary Guided Adversarial Training (LBGAT).  and  learn collaboratively in training.  is a batch of natural images and labels.  is the corresponding adversarial examples of .} 
		\label{fig:LBGAT}
	\end{center}
    \vspace{-0.2in}
\end{figure}

\begin{algorithm}[t]
	\caption{Learnable Boundary Guided Adversarial Training (LBGAT)}
	\label{algorithm_LBGAT}
	\begin{algorithmic}[1]
		\State \textbf{Input:} step size  and learning rate , batch size , number of iterations  in inner optimization, model  parameterized by ,  parameterized by .  is one hyper-parameter.
		\State \textbf{Output:} robust model  with .
		\State Initialize  and  randomly or with pre-trained configuration.
		\Repeat
		\State Read mini-batch ,  from training set;
		\State Get adversarial examples  by PGD attack with input , ;
		\State ;
		\State ;
		\State ;
		\State ;
		\State ;
		\State ;
		\Until training converges
	\end{algorithmic}
\end{algorithm}

\subsection{Learnable Boundary Guided Adv. Training}
\label{sec_LBGAT}
For BGAT method,  is constrained by logits of the static . The well trained  has the most desirable classifier boundary for natural data. Thus, inheriting such classifier boundary,  tends to achieve high performance on natural images. 

Nevertheless, the classifier boundary coming from static  might not be the most suitable choice for pursuing robustness.
We generalize the BGAT method to Learnable Boundary Guided Adversarial Training (LBGAT) by training  and  simultaneously and collaboratively. The loss function is therefore changed from Eq. \eqref{f_BGAT} to
\begin{small}
	
\end{small}
where  is the adversarial example corresponding to its natural data , and  is the true label.  is a softmax function. CE represents cross-entropy loss,  and  are parameterized by  and  respectively. We use Mean Square Error (MSE) loss as L function.  is the trade-off parameter. In this paper, we choose . We randomly initialize  and  in our experiments.

As shown in Fig~\ref{fig:LBGAT}, under the regularization of the proposed logits constraints, {\it i.e.}, the  loss item in Eq.~\eqref{f_LBGAT},  adaptively learns one most robustness-friendly classifier boundary during the collaborative training. At the same time, it guarantees least performance degradation on natural data with  loss item in Eq.~\eqref{f_LBGAT}. Note there is no additional cross-entropy loss for optimizing , which makes the classifier boundary be sufficiently inherited from . More details are listed in Algorithm \ref{algorithm_LBGAT}.

\vspace{-0.1in}
\paragraph{Natural Classifier Boundary Improving Robustness} Zhang et al. \cite{zhang2019theoretically} identified trade-off between performance on natural data and robust accuracy. Xie et al. \cite{xie2020adversarial} observed that adversarial examples were helpful to model generalization ability on natural images. However, using models trained only with natural data to enhance model robustness remains unexplored. We instead notice that proper classifier boundary learned by the natural trained model not only helps preserve high natural accuracy but also enhances model robustness (2.44\% improvement on CIFAR-100 dataset under the strongest auto-attack \cite{croce2020reliable} shown in Table~\ref{fig:whitebox}). We attribute the improvement to guidance of the natural classifier boundary in the course of adversarial training.

\vspace{-0.1in}
\paragraph{Comparison with ALP} Adversarial logit pairing (ALP)~\cite{DBLP:journals/corr/abs-1803-06373} pairs logits from  and  output from the same robust model, encouraging similar representation between  and . Ours is totally different by nature. Eqs. \eqref{f_BGAT} \eqref{f_LBGAT} reveals that logits of  from robust model  are guided by logits output of  from model , leading to more generalizable classifier boundary for natural data. The logits come from two different models. This is the most fundamental difference to ALP. We list ALP as one baseline and give extensive comparisons in experiments.

\subsection{Model Flexibility}
Our method provides a new training scheme for adversarial training. It does not conflict or overlap with other adversarial training methods. We show the flexibility of our approach by using it in other state-of-the-art methods, {\it e.g.}, Adversarial Logit Pairing (ALP)~\cite{DBLP:journals/corr/abs-1803-06373} and TRADES method~\cite{zhang2019theoretically}. We validate the improvement over these baselines.

\paragraph{Combined with Adversarial Logit Paring}
Adversarial logit pairing (ALP) requires the logits of natural data  and the corresponding adversarial example  to be the same in one model, which is achieved by adding an extra mean square loss item between two logits output. We combine our BGAT with ALP as the loss of
\begin{small}
	
\end{small}
where  is a trade-off parameter.  is a softmax function and  is the true label.  is the parameter of . We replace the cross-entropy loss item  in the original ALP loss function with our Eq.~(\ref{f_BGAT}).

\paragraph{Combined with TRADES}
The proposed TRADES algorithm~\cite{zhang2019theoretically} explores the trade-off between model robustness and accuracy on natural data by optimizing one regularized surrogate loss. We use our BGAT in the TRADES algorithm as
\begin{small}
	
\end{small}
where  is still a trade-off parameter.   is the parameter of .  is softmax function and  is the true label.  is the boundary error term, pushing classifier boundary away from data point , originally defined in TRADES~\cite{zhang2019theoretically}. We replace the cross-entropy loss item of   in original TRADES loss with our Eq. (\ref{f_BGAT}). 


It is noted that our LBGAT method can also be combined with both ALP and TRADES methods by simply replacing the first loss item in Eqs. (\ref{f_BGAT_ALP}) and (\ref{f_BGAT_TRADES}) with Eq. (\ref{f_LBGAT}).

\section{Experiments}
\label{exp_evaluation}
In this section, we verify the effectiveness of our methods by conducting both white- and black-box attack following the same experimental settings in \cite{zhang2019theoretically}, {\it i.e.}, applying  (white-box or black-box) attack with 20 iterations, perturbation size   with step size 0.003.

\paragraph{Datasets.}
To evaluate the robustness of our models, we conduct extensive experiments on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. 
CIFAR-100 has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. 
Tiny Imagenet \cite{DBLP:conf/cvpr/DengDSLL009}, which is with more complex data, is a miniature of ImageNet dataset. It has 200 classes. Each class has 500 training images, 50 validation images. In our experiments, we resize the image to 32x32 and normalize pixel values to [0,1].
Following \cite{zhang2019theoretically}, we perform standard data augmentation including random crops with 4 pixels of padding and random horizontal flip during training.

\paragraph{Training Details.}
We use the same neural network architecture as \cite{zhang2019theoretically}, {\it i.e.}, the wide residual network WRN-34-10. Following \cite{zhang2019theoretically}, We set perturbation , perturbation step size , number of iterations , learning rate , batch size , and number of training epochs 100 with transition epochs  on the training dataset. Similarly, SGD optimizer with momentum 0.9 and weight decay 2e-4 is adopted.

\subsection{Effectiveness of Our Methods}
\label{exp_effectiveness}
We first verify the effectiveness of our methods compared with vanilla Adversarial Training (AT). Evaluation of model robustness is under the white-box attack using the same setting as described at the beginning of Sec. \ref{exp_evaluation}. Both our BGAT and LBGAT methods significantly outperform vanilla AT shown by results in Table~\ref{tab:effectiveness}. As analyzed in Sec.~\ref{sec_LBGAT}, the BGAT method can achieve higher natural accuracy while the LBGAT method tends to have stronger robustness. Since we aim to achieve the strongest robustness while preserving natural accuracy as high as possible, we treat LBGAT as our core method.   

\begin{table}[t!]
	\footnotesize
	\centering
	\caption{Comparison with vanilla AT method. For BGAT, we use the ensemble of WideResNet and InceptionResNetV2 model as . ResNet18 as  is for LBGAT on CIFAR-10 and CIFAR-100. To rule out randomness, we get numbers by running two independently trained models and taking the average.  represents accuracy on natural images, while  represents the robustness of models.} 
	\begin{tabular}{l|c|c|c}
		\textbf{Methods} & & &\textbf{Datasets} \\
		\hline
		\hline
		vanilla AT &60.90\% &27.46\%  &CIFAR-100 \\
		BGAT        &67.72\% &30.20\% &CIFAR-100 \\
		LBGAT       &66.29\% &34.30\% &CIFAR-100 \\
		\hline
		vanilla AT &86.82\% &52.87\%  &CIFAR-10\\
		BGAT        &89.00\% &55.40\% &CIFAR-10 \\
		LBGAT       &87.08\% &56.60\% &CIFAR-10 \\
		\hline
	\end{tabular}
	\label{tab:effectiveness}
\end{table}

\subsection{Combing with ALP and TRADES}
\label{exp_flexibility}
To verify the flexibility of our method, we show that, combined with our BGAT and LBGAT methods, ALP and TRADES further improve performance. For ALP, BGAT+ALP and LBGAT+ALP methods, we adopt  following the setting in \cite{DBLP:journals/corr/abs-1803-06373}. For the TRADES method, we adopt , with which TRADES achieves the best robustness, as demonstrated in \cite{zhang2019theoretically}.

The evaluation is under the white-box attack following the same setting as described at the beginning of Sec. \ref{exp_evaluation}.
We summarize the results in Table ~\ref{tab:flexibility}. Equipped with regularization items of ALP and TRADES, our method can further enhance model robustness. For CIFAR-100, LBGAT+ALP outperforms ALP by 2.92\% and 6.31\% respectively on natural accuracy and robust accuracy under the white-box attack respectively. Meanwhile, the BGAT+TRADES method also outperforms TRADES in terms of both natural accuracy and robustness under the white-box attack for CIFAR-10, which manifests the great flexibility of our method. To show the advantages of our methods more intuitively, we plot the comparison results on the more challenging CIFAR-100 dataset in Fig. \ref{fig:whitebox}, which indicates that models trained with our method can have stronger robustness while preserving high natural accuracy.

\begin{table}[h]
	\footnotesize
	\centering
	\caption{Our method is supplementary to ALP and TRADES. For BGAT, we use the ensemble of WideResNet and InceptionResNetV2 model as . ResNet18 is adopted as  for LBGAT+TRADES and LBGAT+ALP. To rule out randomness, the numbers are averaged over 2 independently trained models.  represents accuracy on natural images while  represents robustness of models.} 
	\begin{tabular}{l|c|c|c}
		\textbf{Methods} & & &\textbf{Datasets} \\
		\hline
		\hline
		ALP      & 59.75\% & 28.94\% &CIFAR-100\\
		BGAT+ALP  & 63.46\% & 31.27\% &CIFAR-100\\
		LBGAT+ALP & 62.67\% & 35.25\% &CIFAR-100\\
		TRADES ()      &62.37\% &25.31\% &CIFAR-100 \\
		TRADES ()      &56.51\% &30.94\% &CIFAR-100 \\
		BGAT+TRADES ()  &71.27\% &28.70\% &CIFAR-100 \\
		LBGAT+TRADES () &70.03\% &33.01\% &CIFAR-100 \\
		LBGAT+TRADES () &60.43\% &35.50\% &CIFAR-100 \\
		\hline
		ALP       &85.55\% &54.59\% &CIFAR-10\\
		BGAT+ALP  &86.58\% &55.74\% &CIFAR-10\\
		LBGAT+ALP &85.05\% &57.60\% &CIFAR-10\\
		TRADES () &88.64\% &49.14\% &CIFAR-10 \\
		TRADES ()       &84.92\% &56.61\% &CIFAR-10 \\
		BGAT+TRADES ()  &89.06\% &56.75\% &CIFAR-10 \\
		LBGAT+TRADES () &88.22\% &57.55\% &CIFAR-10 \\
		LBGAT+TRADES () &81.98\% &57.78\% &CIFAR-10 \\
		\hline
	\end{tabular}
	\label{tab:flexibility}
\end{table}

\begin{table*}[t!]
	\centering
	\caption{Comparison of our method with previous defense models under white-box attack on CIFAR-10 and CIFAR-100. We use ResNet18 as  for LBGAT method.  represents accuracy on natural images while  represents robustness of models. AA is the strongest attack, {\it i.e.}, auto-attack \cite{croce2020reliable}. * denotes the model is WRN-34-20.
	}
	\begin{tabular}{l|c|c | c | c |c }
		\hline
		\hline
		\multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{\textbf{Attack}} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} \\
		\cline{3-6}
		& & & & & \\
		\hline
		Baseline                      & None  &95.80\% &0\% &78.76\% &0\% \\       
		\hline
		TRADES ()           & &88.64\% & 49.14\% &62.37\% &25.31\% \\
		TRADES ()           & &84.92\% & 56.61\% &56.50\% &30.93\% \\
		LBGAT+ALP                     & &85.05\% &57.60\% &62.67\% &35.25\% \\
		LBGAT+TRADES ()     & &88.22\% &57.55\% &70.03\% &33.01\%  \\
		LBGAT+TRADES ()     & &81.98\% &57.78\% &60.43\% &35.50\%  \\
		\hline
		TRADES ()  &  &88.64\%  &50.93\% &62.37\% &24.53\% \\
		TRADES ()  &  &84.92\%  &54.98\% &56.50\% &28.43\% \\
		LBGAT+ALP                    &    &85.05\% &55.78\% &62.67\% &31.97\% \\
		LBGAT+TRADES ()    &    &88.22\% &56.38\% &70.03\% &31.14\% \\
		LBGAT+TRADES ()    &    &81.98\% &55.53\% &60.64\% &31.50\% \\
		\hline
		TRADES ()          &FGSM  &88.64\% &86.04\% &62.37\% &58.32\%  \\
		TRADES ()          &FGSM  &84.92\% &82.07\% &56.50\% &53.82\%  \\
		LBGAT+ALP                    &FGSM  &85.05\% &83.19\% &62.67\% &59.87\%  \\
		LBGAT+TRADES ()    &FGSM  &88.22\% &86.14\% &70.03\% &66.42\%  \\
		LBGAT+TRADES ()    &FGSM  &81.98\% &80.08\% &60.64\% &58.16\%  \\
		\hline
		TRADES()                       &AA &\textbf{88.64\%} &48.11\% &62.37\% &22.24\% \\
		TRADES()                       &AA &84.92\% &52.64\% &56.50\% &26.87\% \\
		LBGAT+TRADES()                 &AA &\textbf{88.22\%} &\textbf{52.86\%} &\textbf{70.03\%} &\textbf{27.05\%} \\
		LBGAT+TRADES()                 &AA &81.98\% &\textbf{53.14\%} &60.43\% &\textbf{29.34\%} \\
		\hline
		LBGAT+TRADES()*                &AA &\textbf{88.70\%} &\textbf{53.58\%} &\textbf{71.00\%} &\textbf{27.66\%} \\
		LBGAT+TRADES()*                &AA &83.61\% &\textbf{54.45\%} &62.55\% &\textbf{30.20\%} \\
		\hline
		\hline
	\end{tabular}
	\label{tab:white-box}
\end{table*}

\subsection{Robustness on CIFAR-10 and CIFAR-100}
\label{sec:4.1}
\paragraph{White-box Regular Attacks.}
We evaluate the robustness of our models under the white-box attack using the same setting as described at the beginning of Sec. \ref{exp_evaluation}. 
For CIFAR-10, our LBGAT+TRADES () achieves 88.22\% accuracy on natural images, which outperforms TRADES () by 3.3\% at the same time remaining 57.55\% robust accuracy, 0.94\% higher than that of TRADES (). 

For CIFAR-100, our LBGAT+TRADES () achieves 70.03\% accuracy on natural images and 33.01\% robust accuracy, improving TRADES () by 13.53\% and 2.08\% respectively. 
Moreover, our LBGAT+TRADES () further boosts robustness to 57.78\% and 35.50\% on CIFAR-10 and CIFAR-100 respectively. 

We also apply several other regular attack methods, like FGSM and CW, to evaluate our models. Compared with TRADES, our proposed methods consistently achieve better accuracy on natural images and stronger robustness on both CIFAR-10 and CIFAR-100 datasets. The details of our results are presented in Table~\ref{tab:white-box}. Note that the CW attack denotes using CW-loss within the PGD framework here. The evaluation under CW attack is also with 20 iterations, step size 0.003 and perturbation . 

\paragraph{White-box Auto-Attack (AA).}
Auto-Attack \cite{croce2020reliable} is to reliably evaluate model robustness with an ensemble of diverse strong attack methods, including APGD-CE, APGD-DLR, FAB, and Square Attack. We use the open-source code from \cite{croce2020reliable} to test our models with perturbation size 0.031. The results are listed in Table \ref{tab:white-box}. Compared with TRADES(), our LBGAT+TRADES() model improves natural accuracy by 13.53\% and 3.3\% on CIFAR-100 and CIFAR-10 separately, while achieving comparable robustness.
Our LBGAT+TRADES() model further boosts robust accuracy, obtaining 29.34\% and 53.14\% on CIFAR-100 and CIFAR-10, outperforming TRADES() by 2.44\% and 0.5\% respectively.

\begin{table*}[t!]
	\centering
	\caption{Comparison of our method with previous defense models under black-box attack on CIFAR-100 and CIFAR-10. To rule out randomness, the numbers are averaged over 2 independently trained models.  represents accuracy on natural images.  represents robustness under black-box attack.  represents robustness under white-box attack}
	\begin{tabular}{l|c|c | c | c|c }
		\textbf{Target Models} & &  &  &\textbf{Source Models}  &\textbf{Dataset}\\
		\hline
		\hline
		TRADES ()  &61.29\%  &25.31\%  &62.37\%  &Natural &CIFAR-100\\
		TRADES ()  &55.52\%  &30.93\%  &56.51\%  &Natural &CIFAR-100\\
		LBGAT+ALP            &61.38\%  &\textbf{35.25\%}  &62.67\%  &Natural &CIFAR100\\
		LBGAT+TRADES(=0)  &\textbf{68.35\%} &\textbf{33.01\%} &\textbf{70.03\%} &Natural &CIFAR-100\\
		\hline
		TRADES() &42.32\% &25.31\% &62.37\% &LBGAT+TRADES(=0) &CIFAR-100\\
		TRADES() &41.67\% &30.93\% &56.51\% &LBGAT+TRADES(=0) &CIFAR-100\\
		LBGAT+ALP     &45.68\% &\textbf{35.25\%} &62.67\% &TRADES ()  &CIFAR-100\\
		LBGAT+TRADES(=0) &\textbf{50.27\%} &\textbf{33.01\%}  &\textbf{70.03\%} &TRADES () &CIFAR-100\\
		\hline
		TRADES ()  &87.00\% &49.14\%  &\textbf{88.64\%}  &Natural &CIFAR-10\\
		TRADES ()  &83.30\% &56.61\%  &84.92\%  &Natural &CIFAR-10\\
		LBGAT+TRADES ()      &\textbf{87.20\%} &\textbf{57.55\%} &\textbf{88.22\%} &Natural &CIFAR-10\\
		\hline
		TRADES () &66.18\% &49.14\% &\textbf{88.64\%} &LBGAT+TRADES(=0) &CIFAR-10\\
		TRADES () &67.18\% &56.61\% &84.92\% &LBGAT+TRADES(=0) &CIFAR-10\\
		LBGAT+TRADES ()     &\textbf{68.45\%} &\textbf{57.55\%} &\textbf{88.22\%} &TRADES(=6) &CIFAR-10\\
		\hline
		\hline
	\end{tabular}
	\label{tab:black-box_cifar100_cifar10}
\end{table*}


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.42\textwidth]{Figs/whitebox_cropped.pdf}
		\caption{Comparisons with state-of-the-art defense methods on CIFAR-100 with 20 iterations PGD under white-box attack. Black dots represent our methods, while green points stand for other recent methods. More details are included in Tables \ref{tab:effectiveness}, \ref{tab:flexibility} and \ref{tab:white-box} }
		\label{fig:whitebox}
	\end{center}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.48\textwidth]{Figs/blackbox_cropped.pdf}
		\caption{``White-box Robust Acc'' represents classification accuracy under white-box attack. ``Black-box Robust Acc'' represents classification accuracy under black-box attack. Models on the right of the red line are evaluated with the clean model as the source one, while models on the left of the red line models are evaluated with the robust model as the source. More details are included in Table \ref{tab:black-box_cifar100_cifar10}}
		\label{fig:blackbox}
	\end{center}
\end{figure}

\paragraph{Black-box Attacks.}
We verify the robustness of our models under the black-box attack. We first train models without using adversarial training on the CIFAR-10 and CIFAR-100 datasets. The same network architectures that are specified at the beginning of this section, {\it i.e.}, the WRN-34-10 architecture \cite{DBLP:conf/bmvc/ZagoruykoK16}, are adopted. We denote these models by naturally trained models as (Natural). 

The accuracy of teh naturally trained WRN-34-10 model is 95.80\% on the CIFAR-10 dataset and 78.76\% on the CIFAR-100 dataset. We also implement the method proposed in \cite{zhang2019theoretically} on both datasets with their open-source codebase. For both datasets, the  (black-box) method is applied to attack various defense models. We set  and apply  (black-box) attack with 20 iterations with step size set to 0.003. Note that the setup is the same as that specified in the white-box attack.

The results on CIFAR-100 are summarized in Table ~\ref{tab:black-box_cifar100_cifar10}. We use source models to generate adversarial perturbations where the perturbation directions are according to the gradients of the source models on the input images. Our models are more robust against black-box attack transferred from naturally trained models and TRADES \cite{zhang2019theoretically}, while yielding stronger robustness under white-box attack and higher performance on natural images. Specifically, our best model is 12.83\% and 8.60\% higher than TRADES() with the natural trained model and robust model as the source model separately on CIFAR-100. For robustness under black-box attack with one robust source model, our model is tested under TRADES() while TRADES is tested under our LBGAT trained model. A more clear comparison between our method and TRADES method can be seen in Fig. \ref{fig:blackbox}, which exhibits the results on the more challenging dataset CIFAR-100.

\subsection{Robustness on Tiny-ImageNet.}
To further demonstrate the effectiveness of our method on more complex data, we conduct experiments on Tiny ImageNet. Table~\ref{tab:Tiny_ImageNet} shows the experimental results. Our method is better than ALP, and better than TRADES, surpassing baselines with a large margin. Specifically, our LBGAT+TRADES() outperforms the most robust baseline -- TRADES() by 9.29\% on natural data, meanwhile LBGAT+TRADES() is 3\% higher than it on adversarial data, which testifies the effectiveness of our approach again.

\begin{table}[h]
	\footnotesize
	\centering
	\caption{Results on Tiny ImageNet~\cite{DBLP:conf/cvpr/DengDSLL009}. The same evaluation setting with CIFAR is applied under 20 iterations PGD white-box attack. We adopt ResNet18 as  for LBGAT methods.  represents accuracy on natural images while  represents robustness of models.} 
	\vspace{0.1cm}
	\begin{tabular}{l|c|c|c}
		\textbf{Methods} & & &\textbf{Datasets} \\
		\hline
		\hline
		vanilla AT &30.65\% &6.81\% &Tiny ImageNet\\
		LBGAT       &36.50\% &14.00\% &Tiny ImageNet \\
		ALP &30.51\% &8.01\% &Tiny ImageNet \\
		LBGAT+ALP &33.67\% &14.55\% &Tiny ImageNet \\
TRADES () &38.51\% &13.48\% &Tiny ImageNet \\
		LBGAT+TRADES () &\textbf{47.80\%} &14.31\% &Tiny ImageNet \\
		LBGAT+TRADES () &39.26\% &\textbf{16.42\%} &Tiny ImageNet \\
		\hline
		\hline
	\end{tabular}
	\label{tab:Tiny_ImageNet}
\end{table}

\section{Ablation study for  function}
\label{sec:L_function}
To show the importance of boundary guidance from , we conduct ablation experiments with and without cross-entropy loss for  in Eq. \eqref{f_LBGAT}. Experimental results are summarized in Table~\ref{tab:ablation_study}. "w/o" additional cross-entropy loss for  enjoys 2.05\% higher robust accuracy than "w/", which further manifests vast importance of the natural classifier boundary guidance.  

\begin{table}
	\centering
	\caption{Ablation study for  function on CIFAR-10. 20 iterations PGD white-box attack is applied. We adopt ResNet18 as  for LBGAT method.  represents accuracy on natural images while  represents robustness of models.} 
	\vspace{0.1cm}
	\begin{tabular}{l|c|c}
		\textbf{Methods} & & \\
		\hline
		\hline
		vanilla AT          &86.82\% &52.87\% \\
		TRADES () &84.92\% &56.61\% \\
	    LBGAT () w/          &88.35\% &55.50\% \\
	    LBGAT () w/o         &88.22\% &57.55\% \\
		\hline
		\hline
	\end{tabular}
	\label{tab:ablation_study}
\end{table}

\section{Conclusion}
In this paper, we have proposed the Boundary Guided Adversarial Training (BGAT) method, to improve model robustness without losing much accuracy on natural data. Our approach can be understood from the perspective of natural classifier boundary guidance. We also generalized BGAT to Learnable Boundary Guided Adversarial Training (LBGAT) by collaboratively training  and  together to learn one most robustness-friendly classifier boundary and further enhance model robustness. Extensive experiments on CIFAR-10, CIFAR-100, and more challenging Tiny ImageNet datasets proved the effectiveness of our methods.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\onecolumn
\appendix

\title{\centering \Large \textbf{Learnable Boundary Guided Adversarial Training Supplementary File}}
\maketitle


\vspace{1.0in}	
\section{Our Method Creates New SOTA Under the Strongest Auto-Attack on CIFAR-100}
To further show the effectiveness of our method, we compare with more previous works. The experimental results are shown in Table~\ref{tab:more_aa_cifar100}. On the more challlenging CIFAR-100 dataset, our method creates a new state-of-the-art (SOTA) on both robustness and natural accuracy. Specifically, our LBGAT () model with WideResNet-34-10 architecture significantly outperforms previous SOAT method \cite{chen2020efficient} by 7.08\% in the aspect of performance on natural data. Meanwhile, our method surpasses it with respect to model robustness. Further, our strongest model LBGAT () with WideResNet-34-10 architecture enjoys 2.4\% higher robustness than \cite{chen2020efficient}.  

Moreover, It is worthy to note that our LBGAT () model achieves even strong robustness than the model, by Hendrycks \etal \cite{hendrycks2019using}, trained with a large amount of additional unlabeled data. At the same time, we also surpasses it in the aspect of natural accuracy.   

\vspace{0.2in}
\begin{table*}[h]
	\large
	\centering
	\caption{More comparisons under the strongest Auto-Attack on CIFAR-100 dataset. "\dag" denotes numbers are directly copied from \cite{croce2020reliable}. "" denotes that the method has used additional unlabeled data.} 
	\vspace{0.1cm}
	\begin{tabular}{l|c|c|c}
		\textbf{Methods} & Model & & \\
		\hline
		\hline
		LBGAT () Ours                 &WideResNet-34-20 &\textbf{71.00\%} &\textbf{27.66\%} \\
		LBGAT () Ours                 &WideResNet-34-20 &62.55\% &\textbf{30.20\%} \\
		LBGAT () Ours                 &WideResNet-34-10 &\textbf{70.03\%} &\textbf{27.05\%} \\
		LBGAT () Ours                 &WideResNet-34-10 &60.43\% &29.34\% \\
		\hline
		\hline
		
		TRADES() \cite{zhang2019theoretically}      &WideResNet-34-10 &62.37\% &22.24\% \\
		TRADES() \cite{zhang2019theoretically}      &WideResNet-34-10 &56.50\% &26.87\% \\
		Sitawarin \etal \cite{sitawarin2020improving} \dag &WideResNet-34-10 &62.82\%    &24.57\% \\
		Chen \etal \cite{chen2020efficient} \dag           &WideResNet-34-10 &62.15\%	&26.94\% \\ 
		Hendrycks \etal \cite{hendrycks2019using} \dag      &WideResNet-28-10 &59.23\%	&28.42\% \\
		Rice \etal \cite{rice2020overfitting} \dag         &ResNet-18        &53.83\%    &18.95\% \\
		\hline
		\hline
	\end{tabular}
	\label{tab:more_aa_cifar100}
\end{table*}

\newpage
\section{More Comparisons Under the Strongest Auto-Attack on CIFAR-10}
We also compare with more previous methods on CIFAR-10 dataset. The experimental results are summarized in Table~\ref{tab:more_aa_cifar10}. Our LBGAT () model with WideResNet-34-10 architecture can consistently enjoy higher natural performance while keeping the strongest robustness. 



\begin{table*}[h]
	\large
	\centering
	\caption{More comparisons under the strongest Auto-Attack on CIFAR-10 dataset.  "\dag" denotes numbers are directly copied from \cite{croce2020reliable}. "" denotes the methods aiming to accelerate adversarial training.} 
	\vspace{0.1cm}
	\begin{tabular}{l|c|c|c}
		\textbf{Methods} & Model & & \\
		\hline
		\hline
		LBGAT () Ours                 &WideResNet-34-20 &\textbf{88.70\%} &\textbf{53.58\%} \\
		LBGAT () Ours                 &WideResNet-34-20 &83.61\% &\textbf{54.45\%} \\
		LBGAT () Ours                 &WideResNet-34-10 &\textbf{88.22\%} &\textbf{52.86\%} \\
		LBGAT () Ours                 &WideResNet-34-10 &81.98\% &53.14\% \\
		\hline
		\hline
		Rice \etal \cite{rice2020overfitting} \dag    &WideResNet-34-20 &85.34\%	&53.42\% \\
		TRADES()                            &WideResNet-34-10 &\textbf{88.64\%} &48.11\%\\
		TRADES()                            &WideResNet-34-10 &84.92\% &52.64\% \\ 
		Kumari \etal \cite{kumari2019harnessing} \dag &WideResNet-34-10 &87.80\%	&49.12\% \\
		Mao \etal \cite{mao2019metric} \dag           &WideResNet-34-10 &86.21\%	&47.41\% \\
		Zhang \etal \cite{zhang2019you} \dag    &WideResNet-34-10 &87.20\%	&44.83\% \\
		Shafahi \etal \cite{shafahi2019adversarial} \dag  &WideResNet-34-10 &86.11\% &41.47\% \\
		Chan \etal \cite{chan2019jacobian} \dag                 &WideResNet-34-10 &\textbf{93.79\%} &0.26\% \\
		Wang \etal \cite{wang2019bilateral} \dag          &WideResNet-28-10 &\textbf{92.80\%} &29.35\% \\
		Qin \etal \cite{qin2019adversarial} \dag         &WideResNet-40-8  &86.28\% &52.81\% \\
		Chen \etal \cite{chen2020adversarial} \dag       &ResNet-50        &86.04\% &51.56\% \\
		Xiao \etal \cite{xiao2019enhancing} \dag         &DenseNet-121     &79.28\% &18.50\% \\
		Wong \etal \cite{wong2020fast} \dag              &ResNet-18        &83.34\% &43.21\% \\
		\hline
		\hline
	\end{tabular}
	\label{tab:more_aa_cifar10}
\end{table*}
\newpage

\end{document}
