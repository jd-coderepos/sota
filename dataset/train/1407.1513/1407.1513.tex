\documentclass[submission]{eptcs} \usepackage{breakurl}             \usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\usepackage{amsmath,amsfonts,amsthm,latexsym,url}
\usepackage{dsfont}
\usepackage{mathptmx,graphicx,makeidx}
\usepackage[vlined,linesnumbered]{algorithm2e}
\usepackage{xcolor}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newtheorem{property}{Property}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newcommand{\annotcolin}[1]{ \marginpar{\small\itshape\color{blue}#1}}
\newcommand{\annotjames}[1]{ \marginpar{\small\itshape\color{green}#1}}
\newcommand{\annotmarkjan}[1]{ \marginpar{\small\itshape\color{red}#1}}
\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator{\E}{\mathbf{E}}
\DeclareMathOperator{\Pref}{Pref}
\providecommand{\R}{\ensuremath{{\mathds R}}}
\providecommand{\rgives}{\ensuremath{\stackrel{*}{\Longrightarrow}}}
\providecommand{\Grammarclass}{\ensuremath{\mathcal{G}}}
\providecommand{\PCFGclass}{\textup{\textbf{PCFG}(\ensuremath{\Sigma})}}
\providecommand{\Qrat}{\ensuremath{{\mathds Q}}}
\providecommand{\Nrat}{\ensuremath{{\mathds N}}}
\providecommand{\Lone}{\textup{L1}}
\providecommand{\Ltwo}{\textup{L2}}
\providecommand{\Linf}{\textup{L}\ensuremath{_{\infty}}}
\providecommand{\Languageclass}{\ensuremath{\mathbcal{L}}}
\providecommand{\Languageclasstwo}{\ensuremath{\mathbcal{L'}}}
\providecommand{\LanguageclassA}{\ensuremath{\mathbcal{L}_{\mathbf 1}}}
\providecommand{\LanguageclassB}{\ensuremath{\mathbcal{L}_{\mathbf 2}}}
\providecommand{\CFG}{\ensuremath{\textsc{Cfg}}}
\providecommand{\PCFG}{\ensuremath{\textsc{Pcfg}}}
\providecommand{\A}{\ensuremath{{\cal{A}}}}
\providecommand{\M}{\ensuremath{{\cal{M}}}}
\providecommand{\B}{\ensuremath{{\cal{B}}}}
\providecommand{\D}{\ensuremath{{\cal{D}}}}
\providecommand{\AC}{\ensuremath{\textsc{Ac}}}
\providecommand{\TAC}{\ensuremath{\textsc{Tac}}}
\providecommand{\BigO}{\ensuremath{O}}
\providecommand{\primenum}{\ensuremath{\psi}}
\providecommand{\es}{\ensuremath{\lambda}}
\providecommand{\qed}{\ensuremath{\textsc{Dfa}}}
\providecommand{\PCP}{\ensuremath{\textsc{Pcp}}}
\providecommand{\DFA}{\ensuremath{\textsc{Dfa}}}
\providecommand{\NFA}{\ensuremath{\textsc{Nfa}}}
\providecommand{\PFA}{\ensuremath{\textsc{Pfa}}}
\providecommand{\DPFA}{\ensuremath{\textsc{Dpfa}}}
\providecommand{\SDFA}{\ensuremath{\textsc{Dpfa}}}
\providecommand{\HMM}{\ensuremath{\textsc{Hmm}}}
\providecommand{\Naming}{\ensuremath{\mathbb L}}
\providecommand{\Learner}{\ensuremath{{\textbf{A}}}}
\providecommand{\Prob}{\ensuremath{Pr}}
\providecommand{\p}{\ensuremath{p}}
\providecommand{\len}{\textup{len}}
\providecommand{\Dis}{\ensuremath{{\cal{D}}}}
\providecommand{\ie}{\ensuremath{\textit{i.e.}}}
\providecommand{\Sigmastar}{\ensuremath{\mathop{\Sigma^{\star}}}}
\providecommand{\Initprob}{\ensuremath{S}}
\providecommand{\deltaP}{\ensuremath{{\delta}}}
\providecommand{\Fprob}{\ensuremath{F}}
\providecommand{\PFAnot}
  {\ensuremath{\langle \Sigma, Q, \Initprob, \Fprob, \deltaP \rangle}}
\providecommand{\NP}{\ensuremath{{\cal{N}\cal{P}}}}
\providecommand{\RP}{\ensuremath{{\cal{R}\cal{P}}}}
\providecommand{\PP}{\ensuremath{{\cal{P}\cal{P}}}}
\providecommand{\Ppoly}{\ensuremath{{\cal{P}}}}
\providecommand{\Forward}{\ensuremath{{\textsc{Forward}}}}
\providecommand{\TabF}{\textup{F}}
\providecommand{\TabS}{\textup{S}}
\providecommand{\TabAux}{\textup{AuxF}}
\providecommand{\TabAuxS}{\textup{AuxS}}
\providecommand{\PFST}{\ensuremath{\textsc{Pfst}}}
\providecommand{\SFST}{\ensuremath{\textsc{Sfst}}}
\providecommand{\PTT}{\ensuremath{\textsc{Ptt}}}
\providecommand{\transductionend}{\ensuremath{\sigma}}
\providecommand{\Sigmatwostar}{\ensuremath{\mathop{\Gamma^{\star}}}}
\providecommand{\Sigmatwo}{\ensuremath{\Gamma}}
\providecommand{\translatesto}{\ensuremath{::}}
\providecommand{\T}{\ensuremath{{\cal{T}}}}
\providecommand{\funny}{\ensuremath{\bot}}
\providecommand{\sya}{\textup{\texttt{a}}}
\providecommand{\syb}{\textup{\texttt{b}}}
\providecommand{\syc}{\textup{\texttt{c}}}
\providecommand{\syd}{\textup{\texttt{d}}}
\providecommand{\sye}{\textup{\texttt{e}}}
\providecommand{\syf}{\textup{\texttt{f}}}
\providecommand{\syg}{\textup{\texttt{g}}}
\providecommand{\syh}{\textup{\texttt{h}}}
\providecommand{\sys}{\textup{\texttt{s}}}
\providecommand{\syl}{\textup{\texttt{l}}}
\providecommand{\syr}{\textup{\texttt{r}}}
\providecommand{\syzero}{\textup{\texttt{0}}}
\providecommand{\syone}{\textup{\texttt{1}}}
\providecommand{\sytwo}{\textup{\texttt{2}}}
\providecommand{\sythree}{\textup{\texttt{3}}}
\providecommand{\syfour}{\textup{\texttt{4}}}
\providecommand{\syfive}{\textup{\texttt{5}}}
\providecommand{\leng}{\textup{\texttt{len}}}
\providecommand{\styu}{\textup{\textbf{u}}}
\providecommand{\stya}{\textup{\textbf{a}}}
\providecommand{\styb}{\textup{\textbf{b}}}
\providecommand{\styzero}{\textup{\textbf{0}}}
\providecommand{\styone}{\textup{\textbf{1}}}
\providecommand{\w}{\textup{\textbf{w}}}
\providecommand{\good}{\textup{\textbf{strongly sampable}}}
\providecommand{\dedit}{\ensuremath{d_E}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\popt}{p_{\textrm{opt}}}
\newcommand{\pq}{\textbf{Q} }
\providecommand{\TabF}{\textup{F}}
\providecommand{\TabS}{\textup{S}}
\providecommand{\TabAux}{\textup{AuxF}}
\providecommand{\TabAuxS}{\textup{AuxS}}
\providecommand{\PFST}{\ensuremath{\textsc{Pfst}}}
\providecommand{\PTT}{\ensuremath{\textsc{Ptt}}}
\providecommand{\transductionend}{\ensuremath{\sigma}}
\providecommand{\Sigmatwostar}{\ensuremath{\mathop{\Gamma^{\star}}}}
\providecommand{\Sigmatwo}{\ensuremath{\Gamma}}
\providecommand{\translatesto}{\ensuremath{::}}
\providecommand{\T}{\ensuremath{{\cal{T}}}}
\providecommand{\PT}{\ensuremath{\textsc{Pt}}}
\providecommand{\PDT}{\ensuremath{\textsc{Pdt}}}
\providecommand{\PPIDT}{\ensuremath{\textsc{Pipdt}}}
\providecommand{\PSST}{\ensuremath{\textsc{Psst}}}
\providecommand{\PST}{\ensuremath{\textsc{Pst}}}
\providecommand{\COEM}{\ensuremath{\textsc{CoEm}}}
\providecommand{\gives}{\ensuremath{\mathop{\rightarrow}}}
\title{On the Computation of Distances for Probabilistic Context-Free Grammars}
\author{Colin de la Higuera\qquad\qquad James Scicluna
\institute{LINA, UMR 6241\\
University of Nantes\thanks{The first and second author acknowledge partial support by the R\'{e}gion des Pays de la Loire.}\\
France}
\email{cdlh@univ-nantes.fr\qquad james.scicluna@univ-nantes.fr}
\and
Mark-Jan Nederhof
\institute{School of Computer Science \\
University of St Andrews \\ UK}
}
\def\titlerunning{On the Computation of Distances for Probabilistic Context-Free Grammars}
\def\authorrunning{C. de la Higuera, J. Scicluna \& M.-J. Nederhof }
\begin{document}
\maketitle


\begin{abstract}
Probabilistic context-free grammars (s) are used to define distributions over strings, and are powerful modelling tools in a number of areas, including natural language processing, software engineering, model checking, bio-informatics, and pattern recognition. A common important question is that of comparing the distributions generated or modelled by these grammars: this is done through checking language equivalence and computing distances. Two s are language equivalent if every string has identical probability with both grammars. This also means that the distance (whichever norm is used) is null. It is known that the language equivalence problem is interreducible with that  of multiple ambiguity for context-free grammars, a long-standing open question.
In this work, we prove that computing distances corresponds to solving undecidable questions: this is the case for the ,  norm, the variation distance and the Kullback-Leibler divergence. Two more results are less negative: 1. The most probable string can be computed, and, 2. The Chebyshev distance (where the distance between two distributions is the maximum difference of probabilities over all strings) is interreducible with the language equivalence problem.
\end{abstract}
\section{General motivation and introduction}

Probabilistic context-free grammars (s) are powerful modelling tools in a number of areas, including natural language processing, software engineering, model checking, bio-informatics, and pattern recognition. In natural language processing, these grammars are used as language models \cite{jurafsky95,benedi05} or for parsing natural language \cite{johnson98,klein03}. In model checking the crucial questions of program equivalence or meeting specifications will often be solved through tackling the grammar equivalence problem \cite{espa04,fore12}.


In pattern recognition, probabilistic context-free grammars have been proposed and used for 40 years \cite{fubo75}.
In bio-informatics structural dependencies are modelled through context-free rules, whose probabilities can be estimated \cite{saka94,salv02}.

In all these areas, the following questions are important: given two grammars, are they equivalent? Two grammars are equivalent (\emph{strongly}, or \emph{language equivalent}) when every string has identical probability in each distribution. More generally, a distance between distributions expresses how close they are, with a zero distance coinciding with equivalence.

Furthermore, in many areas, these probabilistic models are to be learnt. When learning, comparison between states or nonterminals often determines if a merge or a generalization is to take place.  Key grammatical inference operations \cite{higu10} will depend on the precision with which an equivalence is tested or a distance is measured.

In the case of probabilistic finite automata, these questions have been analysed with care. The initial study by Balasubramanian \cite{bala93} showed that the equivalence problem for hidden Markov models admitted a polynomial algorithm. Later, a number of authors showed that the Euclidian distance could be computed in polynomial time \cite{lyng99}.  This important result made use of the key concept of co-emission. A similar result was obtained for s \cite{murg04}. 

Negative results were also obtained: The  distance, and the variation distance were proved to be intractable \cite{lyng01}.
Relative entropy (or Kullback-Leibler divergence) cannot be computed between general s. But it can be computed for deterministic \cite{carr97} and unambiguous \cite{cort08} s.

A related problem, that of computing the most probable string (also called the \emph{consensus string}) was proved to be an NP-hard problem \cite{casa00a,lyng02};  heuristics and parameterized algorithms have been proposed \cite{higu13b}. Some of these results and techniques were also extended to s \cite{higu13a}.

The co-emission of a  and a hidden Markov model was discussed by \cite{jago01}, who formulated the problem in terms of finding the solution to a system of quadratic equations; for the difficulties in solving such systems, see also \cite{etessami2009}.
By a related construction, a probabilistic finite automaton can be obtained from a given unambiguous (non-probabilistic) finite automaton and a given , in such a way that the Kullback-Leibler divergence between the two probability distributions is minimal \cite{nede04}.

The same problems have been far less studied for s:  the equivalence problem has recently been proved \cite{fore14} to be as hard as the multiple ambiguity problem for context-free grammars: do two context-free grammars have the same number of derivation trees for each string?
Since on the other hand it is known \cite{abne99} that probabilistic pushdown automata are equivalent to  s, it follows that the equivalence problem is also interreducible with the multiple ambiguity problem. Before that, the difficulty of co-emission and of related results was shown in \cite{jago01}.

Since computing distances is at least as hard as checking equivalence (the case where the distance is 0 giving us the answer to the equivalence question) it remains to be seen, for s, just how hard it is to compute a distance, and also to see if all distances are as hard.

This is the subject of this work. We have studied a number of distances over distributions, including the  and  norms, the Hellinger and the variation distances and the Kullback-Leibler divergence. We report that none of these can be computed.

On the other hand, the fact that the consensus string can be computed allows to show that  the Chebyshev distance (or  norm) belongs to the same class as the multiple ambiguity problem for context-free grammars and the equivalence problem for s.

In Section~\ref{sec:def} we remind the reader of the different notations, definitions and key results we will be needing. In Section~\ref{sec:res} we go through the new results we have proved. We conclude in Section~\ref{sec:con}.

\section{Definitions and notations}\label{sec:def}
Let  denote the set  for each . Logarithms will be taken in base 2.
An \emph{alphabet}  is a finite non-empty set of symbols.
A \textit{string}  over  is a finite sequence 
of symbols. Symbols  will be indicated by ,
and  strings by .
Let  denote the length of .
The \emph{empty string} is denoted by .

We denote by  the set of all strings and by 
the set of those of length at most . Similarly, 
,
 and 
.


A \emph{probabilistic language} 
is a probability distribution over .
The probability of a string  under the
distribution  is denoted as
\label{index:prob}
and must satisfy .
If  is a language (thus a set of strings, included in ),
and  a distribution over ,
.


If the distribution is modelled by some grammar ,
the probability of  according to the probability distribution
defined by  is denoted by
. The distribution modelled by a grammar  will
be denoted by .


\subsection{Context-free grammars}

A \emph{context-free grammar} is a tuple  where  is a finite alphabet (of terminal symbols), 
is a finite alphabet (of variables or non-terminals),  is a finite set of production rules, and
  is the axiom or start symbol.


We will write  for rule . If
 and   we have
. This means that string 
\emph{derives} (in one step) into string .


is the reflexive and transitive closure of . If there
exists  such that  we will
write .  is the language generated by : the set of all strings  over  such that .

A sequence  is a derivation of  from . A derivation step  is a \emph{left-most derivation step} if . A derivation is \emph{left-most} if each step is left-most.

A context-free grammar is \emph{proper} if it satisfies the following three properties:
\begin{enumerate}
\item It is cycle-free, i.e. no non-terminal  exists such that .
\item It is -free, i.e. either no rules with  on the RHS exist or exactly one exists with  on the LHS (i.e. ) and  does not appear on the RHS of any other rule.
\item It contains no useless symbols or non-terminals. This means that every symbol and non-terminal should be reachable from  and every non-terminal should derive at least one string from .
\end{enumerate}

A context-free grammar is ambiguous if there exists a string  admitting two different left-most derivations from  to . Given any string , we can define the multiplicity  as the number of different left-most derivations from  to . If ,  is unambiguous. Otherwise it is ambiguous. If ,  is a \emph{finite multiplicity grammar}. If a grammar is proper it has finite multiplicity. Two finite multiplicity grammars  and  are multiplicity equivalent if .

The multiplicity equivalence problem has been studied for many years \cite{kuic86}: the problem has been proved to be decidable only for particular classes of grammars.


Results regarding the decidability of context-free grammars can be found in many textbooks \cite{harr78}:
\begin{enumerate}
\item  Given two context-free grammars  and , the (\emph{equivalence}) question ? is undecidable.
\item  Given two context-free grammars  and , the (\emph{inclusion}) question ? is undecidable.
\item  Given two context-free grammars  and , the (\emph{emptiness of intersection}) question ? is undecidable.
\end{enumerate}

\subsection{Probabilistic context-free grammars}

\begin{definition}
  A \emph{probabilistic context-free grammar ()} 
  is a context-free grammar   with a probability function 
  .
\end{definition}


 is the sum of all the leftmost derivationsâ€™ probabilities of , where the probability of a leftmost derivation is the product of the rule probabilities used in the derivation. A   is said to be consistent if . Unless otherwise specified, any  mentioned from now onwards is assumed to be consistent. 

Parsing with a  is usually done by adapting the Earley or the
\textsc{Cky} algorithms \cite{jelinek92}. By straightforward variants allowing every terminal to match every input position, one can compute , still in polynomial time in . By summing  for  one obtains 
, and  is . Alternatively,  can be computed directly by variants of algorithms computing prefix probabilities \cite{jelinek91,stol95}.

We denote by  the support language of , ie the set of strings of non null probability.
The class of all s over alphabet  will be denoted by .

There exists an effective procedure which, given a proper  , builds a   such that . We call this procedure \textsc{Mp} for Make Probabilistic.

One possible procedure for \textsc{Mp} is to first assign uniform probabilities to the given , thus obtaining a possibly inconsistent  which then can be converted into a consistent  using the procedure explained in \cite{gecs10}.

Let us formally define the (language) equivalence problem:
\begin{definition}
Two probabilistic grammars  and  are \emph{(language) equivalent} if .
We denote by  the decision problem: are two s  and  equivalent?
\end{definition}
The following result holds for  probabilistic pushdown automata, which are shown in \cite{abne99} to be equivalent to s.
\begin{proposition}\cite{fore14}
The  problem is interreducible with the multiplicity equivalence problem for s.
\end{proposition}



\subsection{About co-emissions} 

Co-emission has been identified as a key concept allowing, in the case of hidden Markov models or probabilistic finite-state automata, computation in polynomial time of the distance for the  norm (and more generally any Lp norm, for even values of p): the distance can be computed as a finite sum of co-emissions.
\begin{definition}
The coemission of two probabilistic grammars  and  is the probability that both grammars  and  simultaneously emit the same string:\\

\end{definition}
A particular case of interest is the probability of twice generating the same string when using the same grammar:
Given a  , the \emph{auto-coemission} of , denoted as , is . 
If the grammars are ambiguous, internal factorization is required in the computation of co-emission. In order to detect this we introduce the \emph{tree-auto-coemission} as the probability that the grammar produces exactly the same left-most derivation (which corresponds to a specific tree):
\begin{definition}
Given a  , the \emph{tree-auto-coemission} of  is the probability that  generates the exact same left-most derivation twice. We denote it by .
\end{definition}
Note that the tree-auto-coemission and the auto-coemission coincide if and only if  is unambiguous:
\begin{proposition}\label{ambiguity}
Let  be a .\\
.\\
  is unambiguous.
\end{proposition}


\subsection{Distances between distributions}
A  defines a distribution over . If two grammars can be compared syntactically, they can also be compared semantically: do they define identical distributions, and, if not, how different are these distributions?
\begin{definition}
The  distance (or Manhattan distance) between two probabilistic grammars  and  is:

\end{definition}
\begin{definition}
The  distance (or Euclidian distance) between two probabilistic grammars  and  is:

\end{definition}
 distance can be rewritten in terms of coemission, as:


\begin{definition}
The  distance (or Chebyshev distance) between two probabilistic grammars  and  is:

\end{definition}
Note that the  distance seems closely linked with the consensus string, which is the most probable string in a language.
\begin{definition}
The \emph{variation distance} between two probabilistic grammars  and  is:

\end{definition}
The variation distance looks like , but is actually connected with :


A number of distances have been studied elsewhere (for example \cite{jago01,cort07}):
\begin{definition}
The \emph{Hellinger distance} between two probabilistic grammars  and  is:


The \emph{Jensen-Shannon (JS)} distance between two probabilistic grammars  and  is:


The \emph{chi-squared ()} distance between two probabilistic grammars  and  is:

\end{definition}


The Kullback-Leibler divergence, or relative entropy is not a metric:
\begin{definition}
The KL divergence between two probabilistic grammars  and  is:

\end{definition}
Even if the KL-divergence does not respect the triangular inequality, .
\begin{definition}
Let  be a . The consensus string for  is the most probable string of .
We denote by  the decision problem: is  the most probable string given ?
\end{definition}


\subsection{PCP and the probabilistic grammars}
\begin{definition}
For each distance   the problem  is the decision problem: given two grammars  and  from , and any rational , do we have ?
\end{definition}
We  use \emph{Turing reduction} between decision problems and write:

for problem  reduces to problem : if there exists a terminating algorithm solving  there also is one solving . If simultaneously  and , we will say that  and  are interreducible. The construction can be used for non decision problems: if only  is a decision problem,  is undecidable, and ,  we will say that  is \emph{uncomputable}.


One particular well-known undecidable problem can be used as starting point for the reductions: the Post Correspondence Problem \cite{post46}, which is undecidable:


\noindent\textbf{Name:}   \\
\textbf{Instance:} A finite set  of pairs of strings  over an alphabet .\\
\textbf{Question:} Is there a finite sequence of integers  such that ?
\vspace{0.1in}

We give two standard constructions starting from an instance  of . In both cases we use another alphabet containing one symbol  for each . Let  denote this alphabet.

\vspace{0.1in}
\noindent\textbf{Construction 1:} Two grammars  \\
An instance of  as above is transformed into two s  and  as follows:\\
Rules of :  and  \\
Rules of :  and \\
Each rule has  probability .

\vspace{0.1in}
\noindent\textbf{Construction 2:} One grammar  \\
An instance of  is first transformed into two s  and  as above.
Then a new non-terminal  is introduced and we add the new rules  and , each with probability .

\vspace{0.1in}
The language obtained through ,  and  contains only strings  which can always be decomposed into  with  and . We note that the number of  derivation steps to generate string  is  for  and . For a final string  we denote this number by . For example .

Note that a positive instance of  will lead to  and  with a non empty intersection, and to an ambiguous .

The following properties hold:

\begin{property}~
\begin{itemize}
\item  and  are unambiguous and deterministic.
\item If , 
\item  is a positive instance of  if and only if 
\item  is a positive instance of  if and only if  is ambiguous. In terms of probabilities, if there exists a string  such that 
\end{itemize}
\end{property}

\begin{property}\label{tac_for_pcp}
Let  be an instance of  with  pairs. Then

\end{property}




\section{Some decidability results}\label{sec:res}
We report results concerning the problems related to equivalence and decision computation. 
\subsection{With the Manhattan distance}
\begin{proposition}
One cannot compute, given two s  and ,

\end{proposition}
\begin{proof}
If this were possible, then we could easily solve the \emph{empty intersection problem} by building  \textsc{Mp}() and \textsc{Mp}() and then checking if .
\end{proof}
\begin{corollary}
One cannot compute, given two s  and , the variation distance between  and .
\end{corollary}
The above follows from a straightforward application of Equation~\ref{equation:variation}.

The same construction can be used for the Hellinger and Jenson-Shannon distances: whenever  and  have an empty intersection, it follows that  and .

Summarizing, , ,  and   are undecidable.

\subsection{With the Euclidian distance}
For , positive results were obtained in this case: the distance can be computed, both for  and  in polynomial time\cite{lyng99}.

In \cite{jago01}, Jagota \textit{et al.} gave the essential elements allowing a proof that co-emission, the  and the Hellinger distances are uncomputable. In order to be complete, we reconstruct similar results in this section.

\begin{proposition}
One cannot compute, given two {\PCFG}s  and ,

\end{proposition}

\begin{proof} If this were possible, then we could easily solve the empty intersection problem, by taking  and , building  \textsc{Mp}() and \textsc{Mp}() and then checking if .
\end{proof}
\begin{proposition}\label{coemd2reduction}
Computing the auto-coemission  is at least as difficult as computing the  distance.
\end{proposition}
\begin{proof}
Suppose we have an algorithm to compute the  distance. Then given any grammar , we build a dummy grammar  which only generates, with probability 1, a single string over a different alphabet. It then follows that

and since the intersection between the support languages for  and  is empty, . On the other hand , trivially.
Therefore .
\end{proof}

\begin{corollary}
One cannot compute, given two s  and , the  distance between  and .
\end{corollary}
\begin{proof}
By proposition \ref{coemd2reduction} all we have to prove is that computing the auto-coemission is impossible.
Let  be the probabilistic context-free grammar built from an instance of . Suppose we can compute . Then since (by Property \ref{tac_for_pcp}) we can compute , one could then solve the ambiguity problem via Proposition \ref{ambiguity}. This is impossible.
\end{proof}
Summarizing,  and  are undecidable. Furthermore  and  are interreducible.


\subsection{With the KL divergence}
\begin{proposition}
One cannot compute, given two s  and , .
\end{proposition}
\begin{proof}
Suppose we could compute the KL divergence between two s. We should note that  if and only if . We would therefore be able to check if one context-free language is included in another one, which we know is impossible.
\end{proof}
The same proof can be used for the  distance since  if and only if . Summarizing,  and  are undecidable.


\subsection{About the consensus string}
A first result is that computing the Chebyshev distance is at least as difficult as computing the most probable string :
Any   can be converted into  with the same rules as  but using a disjoint alphabet. Now, it is clear that :
\begin{proposition}\label{linking_CS_Linf}
 is decidable if there exists an algorithm computing . \\
Ie, .
\end{proposition}
Proposition \ref{linking_CS_Linf} does not preclude that  may be decidable if  is not.\

In fact  is decidable:
\begin{lemma}\label{yes we can}
Let . Given any consistent  , there exists  such that 
\end{lemma}

\begin{proof}
Because , 
there exists  such that 
,
hence .
\end{proof}

\begin{proposition}\label{CS_is_decidable}
 is decidable.
\end{proposition}
\begin{proof}
The proof for this is the existence of an algorithm that takes as input a PCFG  and always terminates by returning the consensus string for . Algorithm \ref{algo:enumeration} does exactly this.

\end{proof}

Algorithm \ref{algo:enumeration} goes through all the possible strings in , , , and checks the probability that  assigns to each string. It stores the string with the highest probability value () and the highest probability value itself (). It also subtracts from 1 all the probability values encountered (). So, after the  loop,  is the most probable string in ,  is the probability of  and  is  which is equal to . Using Lemma \ref{yes we can}, we can say that for any , , there exists an  such that after the  iteration,  is smaller than . This means that the algorithm must halt at some point. Moreover, if the most probable string in  has probability higher than , then we can be sure that this is the consensus string. This means that the algorithm always returns the consensus string.  


\begin{algorithm}


  \SetKw{Kwtrue}{true}
  \SetKw{Kwfalse}{false}
  \SetKw{Kwand}{and}
  \KwData{ a  }
  \KwResult{, the most probable string}
\;
\;
\;
\;
\;
\While{}
{  \If{}
   {\;}
   \Else
   {\ForEach{  }
	{ \;
      \;
	  \If{}
	    {\;\;}
    }
    \;
   }	  
}
\Return 
\caption{Finding the consensus string}\label{algo:enumeration}
\end{algorithm}

\subsection{With the Chebyshev distance}
\begin{proposition}
 and  are interreducible.
\end{proposition}
\begin{proof}
\\
Suppose  is decidable. Then if  and  are equivalent, .
If  and  are not equivalent, there exists a smallest string  such that . An enumeration algorithm will find this initial string , whose probability is . Note that if  and , we can be sure that no string in  has a difference of probabilities of more than . This allows us to adapt Algorithm \ref{algo:enumeration} to reach the length  at which we are sure that no string  longer than  can have probability more than . The algorithm will therefore halt.

The converse () is trivial since   and  are equivalent.
\end{proof}



\section{Conclusion}\label{sec:con}
The results presented in this work are threefold:
\begin{itemize}
\item the only positive result concerns the consensus string, which is computable;
\item the multiple ambiguity problem (for s) is equivalent to the Chebyshev distance problem (for s), which in turn is equivalent to the equivalence problem (also for s);
\item all the other results correspond to undecidable problems.
\end{itemize}

Interestingly, if we consider the Chebyshev distance problem as a decision problem, namely:

\vspace{0.1in}
\noindent\textbf{Name:} Chebyshev distance-  \\
\textbf{Instance:} Two s  and , \\
\textbf{Question:} Is ?

\vspace{0.1in}
the problem is actually decidable in all cases but one: when .
Ideally, one would hope to be able to bound the length of the strings over which the search should be done. This is possible in the case of probabilistic finite automata where it is proved that (1) a  can be transformed into an equivalent -free  , and, (2) the length of any string of probability at least  is upper bounded by , with  the size (number of states+1) of the  \cite{higu13b}.

It should be noted that if the question is: Is ?, the problem becomes decidable.

Moreover, it would be important to obtain approximation results, ie,

\vspace{0.1in}
\noindent\textbf{Name:} X-distance-approx  \\
\textbf{Instance:} Two s  and , \\
\textbf{Question:} Compute  such that ?

\vspace{0.1in}

Such results have been studied in the case of probabilistic finite state machines, for example, recently, in \cite{chen14}. In the case of the distances used in this work, the decidability of approximation would be ensured by  Lemma \ref{yes we can}. But the question of finding good approximations in polynomial time is clearly an interesting problem.
\section*{Acknowledgement}
The authors thank James Worrell, Achilles Beros and Uli Fahrenberg for advice and discussions.
\bibliographystyle{eptcs}
\bibliography{PCFG_distances}
\end{document} 