\documentclass[12pt]{article}
\usepackage[letterpaper]{geometry}

\usepackage{graphicx,color}
\usepackage{graphics} 
\usepackage{float}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{rotating}
\usepackage{ctable}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}

\newcolumntype{C}{>{\centering\arraybackslash} m{0.3\textwidth} }
\newcolumntype{R}{>{\centering\arraybackslash} m{0.005\textwidth} }

\newcolumntype{N}{>{\centering\arraybackslash} m{0.2\textwidth} }
\newcolumntype{V}{>{\centering\arraybackslash} m{0.005\textwidth} }



\usepackage{amsmath,amssymb,amsfonts,amstext}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{dsfont}

\newcommand{\ind}{{\rm i_{\rm k}}}

\newcommand{\inv}[1]{\left[{\bf W^{(\rm #1)}}\right]^{\rm -1}}

\newcommand{\Nobs}{\textsc{n}_{{\rm obs}}}

\newcommand{\Nens}{\textsc{n}_{{\rm ens}}}

\newcommand{\Nproc}[1]{{\rm N_{proc}^{\rm #1}}}

\newcommand{\proc}{\rm P}

\newcommand{\nc}[1]{{\rm C_{p}^{\rm #1}}}

\newcommand{\ap}{{{\bf p}^{\rm t}}}

\newcommand{\pa}[1]{{\rm p^{\rm t}_{#1}}}


\newcommand{\ens}{{\rm ens}}

\newcommand{\Nstate}{\textsc{n}_{{\rm state}}}

\newcommand{\BO}{{\mathcal O}}

\newcommand{\XA}{{\bf X}^{\rm A}}

\newcommand{\xa}[1]{{\bf x}^{\rm A}_{#1}}

\newcommand{\XB}{{\bf X}^{\rm B}}

\newcommand{\DXB}{{\bf \Delta X}^{\rm B}}

\newcommand{\SL}{{\bf X^{*}}}

\newcommand{\xb}[1]{{\bf x}^{\rm B}_{{\rm #1}}}

\newcommand{\Xmean}{{\bf \overline{X}}^{\rm B}}

\newcommand{\xmean}{{\bf \overline{x}}^{\rm B}}

\newcommand{\xmeana}{{\bf \overline{x}}^{\rm A}}

\newcommand{\ONES}{\mathbf{1}}

\newcommand{\PB}{{\bf P}^{\rm B}}

\newcommand{\Q}{{\bf Q}}

\newcommand{\PA}{{\bf P}^{\rm A}}

\newcommand{\R}{{\bf R}}


\renewcommand{\S}{{\bf S}}

\newcommand{\Z}{{\bf Z}}

\newcommand{\sZ}[1]{{\bf Z}^{\left(\rm #1 \right)}}

\newcommand{\h}[1]{{\bf h}^{\left( \rm #1\right)}}

\newcommand{\hv}[2]{{h}_{#1}^{\left( #2\right)}}

\newcommand{\z}[1]{{\bf z}_{{\rm #1}}}

\newcommand{\zv}[3]{z_{#1#2}^{(#3)}}

\newcommand{\U}[1]{{\bf U}^{\left ( \rm #1 \right)}}

\renewcommand{\u}[2]{{\bf u}_{\rm #1}^{(\rm #2)}}

\newcommand{\um}[1]{{\bf u}_{\rm #1}}

\newcommand{\uv}[3]{u_{#1#2}^{(#3)}}

\newcommand{\sz}[2]{{\bf z}_{\rm #1}^{(\rm #2)}}

\newcommand{\D}{{\bf D}}

\renewcommand{\d}[1]{{\bf d}_{\rm #1}}

\newcommand{\dv}[2]{d_{#1#2}}

\newcommand{\W}[1]{{{\bf W}^{\left ( {\rm #1} \right )}}}

\newcommand{\V}{{\bf V}}

\renewcommand{\v}[1]{{\bf v}_{{\rm #1}}}

\newcommand{\vv}[2]{v_{#1#2}}

\newcommand{\KAL}{{\bf K}}

\newcommand{\Y}{{\bf Y}}

\newcommand{\y}[1]{{\bf y}_{{\rm #1}}}

\newcommand{\ym}{{\bf y}}

\newcommand{\xt}{{\bf x}^{\rm true}}

\newcommand{\et}{\bf{\eta}}

\newcommand{\Mo}{{\cal M}}

\newcommand{\Ho}{{\cal H}}

\newcommand{\Normal}{\cal N}

\newcommand{\Lo}{{\bf H}}

\renewcommand{\Re}{\mathbbm{R}}

\newcommand{\E}{{\bf \Upsilon}}

\newcommand{\G}[1]{\bf G^{(\rm #1)}}
\newcommand{\gj}[2]{\bf {g_{\rm #1}^{(\rm #2)}}}
\newcommand{\gjj}[1]{\bf {g_{\rm #1}}}

\newcommand{\e}[1]{{\bf \upsilon}_{{\rm #1}}}

\newcommand{\A}{{\bf A}}
\renewcommand{\L}{{\bf L}}
\newcommand{\M}{{\bf M}}
\newcommand{\N}{{\bf N}}

\newcommand{\invM}[1]{\left( #1 \right )^{-1}}

\newcommand{\invS}[1]{ {#1^{-1}}}

\newcommand{\SMF}[4]{\invS{#1}-\invS{#1} \cdot #2  \cdot \invM{\invS{#3} + #4 \cdot #1 \cdot #2} \cdot #4 \cdot \invS{#1}}

\newcommand{\F}[1]{{\bf F}^{\left({\rm #1}\right)}}
\newcommand{\g}[1]{{\bf g}^{\left({\rm #1}\right)}}
\newcommand{\f}[2]{{{\bf f}_{{\rm #2}}^{\left({ \rm #1}\right)}}}

\newcommand{\x}{{\bf x}}

\newcommand{\fun}[1]{{\cal S}\left(#1 \right )}

\newcommand{\ifun}[3]{{\cal S}_{\star}\left(#1 ,#2, #3\right )}

\renewcommand{\k}{{\rm k}}

\newcommand{\level}[3]{
\text{ Level #1}:~~ \left\{ 
\begin{array}{l}
\h{#1} = \left( 1+\v{#1}^{ \bf T } \cdot \u{#1}{#2}\right)^{-1}\, \u{#1}{#2} \\
\sZ{#1} = \sZ{#2}-\h{#1} \cdot \left ( \v{#1}^{\bf T} \cdot \sZ{#2} \right ) \\
\u{i}{#1} = \u{i}{#2}-\h{#1} \cdot \left ( \v{#1}^{\bf T} \cdot \u{i}{#2} \right )\,, ~~ {\rm i}={\rm #3},\dots, \Nens \\
\end{array}
\right.
}

\newcommand{\T}[3]{{\rm T_{\rm #3} \left( #1,#2 \right )}}

\renewcommand{\r}[1]{r_{#1#1}}

\newcommand{\rv}{{\bf r}}

\newcommand{\ir}[1]{\frac{1}{r_{#1,#1}}}

\newcommand{\fl}[1]{fl \left( #1\right )}

\newcommand{\diag}[2]{{\bf diag} \left ( #1_{1}, #1_{2}, \ldots ,  #1_{#2}\right )}

\newcommand{\Lorenz}{{\bf \frac{dx_{\rm i}}{dt}} = \begin{cases}
 \left ({{\bf x_{2}}-{\bf x_{\Nstate-1}}} \right ) \cdot {\bf x_{\Nstate}}-{\bf x_{1}}+{\rm F} & \text{ for } \\
 \left ({{\bf x_{\rm i+1}}-{\bf x_{\rm i-2}}} \right ) \cdot {\bf x_{\rm i-1}}-{\bf x_{\rm i}}+{\rm F} & \text{ for } \\
\left ({{\bf x_{\rm 1}}-{\bf x_{\Nstate-2}}} \right ) \cdot {\bf x_{\Nstate-1}}-{\bf x_{\Nstate}}+{\rm F} & \text{ for }
\end{cases}} 

\newcommand{\B}[1]{{\bf B_{\rm #1}}}
\newcommand{\Bi}[2]{{\bf B_{\rm #1}^{\rm ( #2 )}}}




\begin{document}

\thispagestyle{empty}
\setcounter{page}{0}

\begin{Huge}
\begin{center}
Computational Science Laboratory Technical Report CSL-TR-00-2013\\
\today
\end{center}
\end{Huge}
\vfil
\begin{huge}
\begin{center}
Elias D. Nino, Adrian Sandu and Jeffrey Anderson
\end{center}
\end{huge}

\vfil
\begin{huge}
\begin{it}
\begin{center}
``An Efficient Implementation of the Ensemble Kalman Filter Based on an Iterative Sherman-Morrison Formula''
\end{center}
\end{it}
\end{huge}
\vfil
{\tiny
\textbf{Cite as:} Elias D. Nino-Ruiz, Adrian Sandu, Jeffrey Anderson, ``An efficient implementation of the ensemble Kalman filter based on an iterative Shermanâ€“Morrison formula'',\textit{Statistics and Computing}, ISSN:0960-3174, PP: 1--17, Feb 2014.}

\begin{large}
\begin{center}
Computational Science Laboratory \\
Computer Science Department \\
Virginia Polytechnic Institute and State University \\
Blacksburg, VA 24060 \\
Phone: (540)-231-2193 \\
Fax: (540)-231-6075 \\ 
Email: {\rm enino@vt.edu},{\rm sandu@cs.vt.edu} \\
Web: {\rm http://csl.cs.vt.edu}
\end{center}
\end{large}

\vspace*{1cm}

\begin{center}
\begin{tabular}{c}
\includegraphics[width=1.2cm]{CSL_Logo.jpg}\\ 
.  \\
\includegraphics[width=1.2cm]{VTLogo.jpg} \\
\end{tabular}
\end{center}

\newpage

\title{An Efficient Implementation of the Ensemble Kalman Filter Based on an Iterative Sherman-Morrison Formula}

\author{Elias D. Nino, Adrian Sandu, and Jeffrey Anderson \\
Computational Science Laboratory, Department of Computer Science \\
Virginia Polytechnic Institute and State University \\
Blacksburg, VA 24060, USA \\
enino@vt.edu, sandu@cs.vt.edu \\
Data Assimilation Research Section \\
Institute for Mathematics Applied to Geosciences \\
National Center for Atmospheric Research \\
Boulder, CO 80307-3000 \\
jla@ucar.edu}


\date{}
\maketitle
\tableofcontents


\begin{abstract}
We present a practical implementation of the ensemble Kalman (EnKF) filter based on an iterative Sherman-Morrison formula. 
The new direct method exploits the special structure of the ensemble-estimated error covariance matrices  in order to efficiently solve the linear systems  involved in the analysis step of the EnKF. The computational complexity of the proposed implementation is equivalent to that of the best EnKF implementations available in the literature when the number of observations is much larger than the number of ensemble members. Even when this condition is not fulfilled, the proposed method is expected to perform well since it does not employ matrix decompositions. Moreover, the proposed method provides the best theoretical complexity when compared to generic formulations of matrix inversion based on the Sherman Morrison formula. The stability analysis of the proposed method is carried out and a pivoting strategy is discussed in order to reduce the accumulation of round-off errors without increasing the computational effort. A parallel implementation is discussed as well. Computational experiments carried out using the Lorenz 96 and then oceanic quasi-geostrophic models reveal that the proposed algorithm yields the same accuracy as other EnKF implementations, but is considerably faster.
\end{abstract}

{\bf Keywords:} Ensemble Kalman filter, Matrix Inversion, Sherman-Morrison Formula, Matrix Decomposition


\section{Introduction}
\label{intro}
The ensemble Kalman filter (EnKF) is a well-established, sequential Monte Carlo method to estimate the state and parameters of non-linear, large dynamical models \cite{Even09A} such as those found in atmospheric \cite{Ott04}, oil reservoir \cite{Even09B}, and oceanic \cite{Even08} simulations. The popularity of EnKF owes to its simple conceptual formulation and the relative ease implementation \cite{Evensen2009}. EnKF represents the error statistics by an ensemble of model states, and the evolution of error statistics is obtained implicitly via the time evolution of the ensemble during the forecast step. In the analysis step, information from the model and the measurements is combined in order to obtain an improved estimate of the true vector state. This process is repeated over the observed time period. In typical data assimilation applications, the dimension of state space (number of variables) ranges between  and , and 
the dimension of the observation space between  and . Consequently, the dimension of the linear systems solved
during the analysis step is very large, and the computational cost considerable. In order to address this challenge we propose an efficient implementation of the EnKF analysis step based on an iterative application of the Sherman-Morrison formula.

The paper is structured as follows. Section \ref{sec:enkf} discusses the conceptual formulation of the EnKF and several efficient implementations available in the literature. Section \ref{sec:enkfSherman} presents the novel implementation of the EnKF based on iterative Sherman-Morrison formula, in which the special structure of the measurements error covariance matrix is exploited. Computational cost and stability analyses are carried out for this approach, and pivoting and parallelization ideas are discussed. Section \ref{sec:results} reports numerical results of the proposed algorithm applied to the Lorenz 96 and quasi-geostrophic models. Conclusions are presented in Section \ref{sec:conclusions}. 

\section{Formulation of the EnKF}\label{sec:enkf}

EnKF consists of two steps: the forecast and the analysis.
An EnKF cycle starts with the matrix  whose columns  form an ensemble of model states,
all corresponding to the same model time :

Typically  is an ensemble of model forecasts.
Here  is the size of the model state vector, and  is the number of ensemble members. Each ensemble member  differs from the true state 
of the system , and we denote by  the corresponding error. The statistics of the ensemble of states is consistent with the background probability distribution.

The ensemble mean  and the ensemble covariance matrix  can be written as follows:
 
\label{ensemble-background-mean}
\displaystyle \xmean &=& \frac{1}{\Nens}  \sum_{i=1}^{\Nens} {\xb{i}} = \frac{1}{\Nens} \left ( \XB \cdot \ONES_{\Nstate \times 1} \right ) \in \Re^{\Nstate \times 1}, \\ \label{ensemble-background-mean-matrix}
\displaystyle \Xmean &=& \xmean \otimes {\ONES}_{\Nens \times 1}^{{\bf T}} \in \Re^{\Nstate \times \Nens}, \\ 
\label{ensemble-background-covariance}
{\PB} &=& \frac{1}{{\Nens}-1} \cdot \left ( {\XB} - {\Xmean} \right ) \cdot \left ( {\XB} - {\Xmean} \right )^{{\bf T}} + \Q \in \Re^{\Nstate \times \Nstate}\,.

Here  is a vector whose entries are all equal one.   is the covariance matrix of model errors. In the typical case where  is an ensemble of model forecasts, the explicit addition of the matrix  to the covariance formula is not necessary.
Instead, the effect of model errors can be accounted for by adding random vectors  to model states:
. Prior to any measurement, the forecast step provides the best estimation to the true vector state  \cite{Suarez12}.

The vector of observations  is available at , where   is the number of data points. 
The observations are related to the model state by the relation

where  is the observation operator which maps the model space state into the observed space, and
 is a vector of observation errors, accounting for both instrument and representativeness errors.

In order to account for observation errors one forms the matrix  whose columns  are perturbed measurements \cite{Kova11}:

The vectors  represent errors in the data, and are drawn from a normal distribution . 
We denote

and the ensemble representation of the measurements error covariance matrix is



The EnKF {\em analysis step} produces an ensemble of improved estimates (analyses)  by applying the Kalman filter
to each of the background ensemble members:
 
\label{EnKF:ensemble-analysis}
\displaystyle \XA &=& \XB + \KAL \cdot \left ( {{\Y}-{\Lo} \cdot  {\XB} } \right ) \in \Re^{\Nstate \times \Nens}, \\
\label{EnKF:kalman-gain}
\KAL &=& {\PB}  \cdot {\Lo}^{{\bf T}} \cdot \left ( {\Lo}  \cdot \PB \cdot {\Lo}^{{\bf T}} + \R \right )^{-1} \in \Re^{\Nstate \times \Nobs},

where the matrix  is the Kalman gain and quantifies the contribution of the background--observations difference to the analysis. 

The EnKF {\em forecast step} uses the dynamical model operator  to evolve each member of the ensemble 
 from the current time  to the next time  where observations are available:

The forecast ensemble  is the background for the new EnKF cycle at . The analysis and forecast steps are repeated.

\subsection{Efficient implementations of the analysis step}
\label{enkf-analysis}


From equations \eqref{EnKF:ensemble-analysis}--\eqref{EnKF:kalman-gain} the analysis step can be written as 

where  is the solution of the following linear system:

A direct solution of this linear system can be obtained using the Cholesky decomposition for matrix inversion \cite{Lin99,Sch90,Xia10}. 
While this is a numerically stable and accurate approach  \cite{Gill96,Mei83,Ste97}, its application to \eqref{EnKF:system-to-solve} leads to the following complexity \cite{Jan06} of the analysis step:

This is an acceptable complexity for a large number of degrees of freedom (), but not for a large number of observations (). An alternative is to solve \eqref{EnKF:system-to-solve}, and the overall analysis step, using Singular Value Decomposition (SVD) based methods. Those methods exploit the special structure of the data error covariance matrix , which is often (block) diagonal
and can be easily factorized:

where  is the number of blocks in the matrix  and:

The matrix  is a covariance matrix, and in practice it is always positive definite.

The observation operator  is sparse or can be applied efficiently to a state vector. Then, when  is diagonal, we can express the system matrix \eqref{EnKF:system-to-solve} as follows:

Employ the singular value decomposition

where  and  are orthogonal square matrices, and  is the diagonal matrix holding the singular values of . The linear system \eqref{EnKF:system-to-solve} can be written as follows \cite{Jan06}:

which yields the solution:

The overall complexity of the analysis step

is suitable for large  and , assuming  remains small.
Many algorithms in the literature employ SVD ~\eqref{EnKF:SVD-linear-system-rewritten} for  the solution of the linear system \eqref{EnKF:system-to-solve} \cite{Evensen2009}. The analysis step is written in terms of the solution ~\eqref{SVD:solution-linear-system} in order to minimize the number of matrix computations. Due to this, the solution of the linear system and the improvement of the forecast ensemble are performed as a single step. The ensemble adjustment Kalman filter (EAKF) and the ensemble transform Kalman filter (ETKF) are based on this idea \cite{Tippett2003}. Other efficient implementations of the ensemble Kalman filter make use of SVD decompositions in order to derive pseudo-inverses, furthermore; these algorithms compute the inverse in the -dimensional ensemble space rather than -dimensional measurement space. Thus, in practice, when , those algorithms exhibit a good performance.
All these methods have the overall complexity (number of long operations) given in  \eqref{EnKF:SVD-overall-complexity}.




A different approach is to employ iterative methods for solving the linear system \eqref{EnKF:system-to-solve}, for instance the conjugate gradient method \cite{Reid72,Golub89,Cohen72,Eis81} for  right-hand sides. However, each iteration costs , therefore iterative methods do not seem to be competitive for the solution of \eqref{EnKF:system-to-solve}.

The well-established EnKF implementations presented above employ a Cholesky or SVD decomposition, which require considerable computational effort.
The next section discusses an efficient implementation of the ensemble Kalman filter which does not require any decomposition prior to the solution of the linear system \eqref{EnKF:system-to-solve}. 

\section{Iterative Implementation of the EnKF Analysis Step}
\label{sec:enkfSherman}


We make the assumptions \cite{Tippett2003,Jan06} that, in practice:
\begin{itemize}
 \item The data error covariance matrix  has a simple structure (e.g., is block diagonal). 
 \item The observation operator  is sparse or can be applied efficiently.
 \item The variables  and  are very large.
\end{itemize}
Moreover, we consider the following situations:
\begin{itemize}
 \item In many real applications of the EnKF , and the number of variables ranges between  and .
 \item When many computational resources are available or when the number of components in the model state is relatively small  (, the number of ensemble members can be increased considerably in order to provide more accurate statistics. In this case . 
\end{itemize}

Taking into in account the previous assumptions, we now derive the implementation of the EnKF. We define the matrix of member deviations  as follows:

which allows to write the ensemble covariance matrix as

By replacing equation \eqref{EnKSM:covariance-matrix} in \eqref{EnKF:system-to-solve}, the linear system solved during the analysis step is written as follows:

\label{EnKSM:system-rhs}
&& \D = \Y-\Lo \cdot \XB \in \Re^{\Nobs \times \Nens} \,, \\
\label{EnKSM:member-observations}
&& \V = {\Lo} \cdot \S  \in \Re^{\Nobs \times \Nens}\,,\\
\label{EnKSM:system-to-solve-V}
&& \left( \R + \V \cdot \V^{\bf T} \right) \cdot \Z = \D \in \Re^{\Nobs \times \Nens}\,.

Note that

can be computed recursively via the sequence of matrices :

therefore

By replacing equation \eqref{EnKSM:sequence-to-replace} in \eqref{EnKSM:system-to-solve-V} we obtain:

Theorem \ref{Theo:sequence-non-singular} shows that the matrix \eqref{EnKSM:sequence-to-replace} is non-singular. The linear system \eqref{EnKSM:system-to-solve-W} can be solved by making use again of the Sherman-Morrison formula \cite{Sher89}:

with , ,  and . The solution of \eqref{EnKSM:system-to-solve-W} is computed as follows:

where  and  are given by the solution of the following linear systems:

\label{EnKSM:subsystem-F}
\displaystyle \W{\Nens-1} \cdot \F{\Nens} = \D \in \Re^{\Nobs \times \Nens}\,,

\label{EnKSM:subsystem-g}
\displaystyle \W{\Nens-1} \cdot \g{\Nens} = \v{\Nens} \in \Re^{\Nobs \times 1}\,.

Note that \eqref{EnKSM:subsystem-F} can be written as follows:

where  and  are the i-th columns of the matrices  and , respectively. Following \eqref{EnKSM:solution-sherman-morrison}, the i-th column of the matrix  is given by:


By equation \eqref{EnKMS:z-columns}, the computation of  involves the solution of the linear systems \eqref{EnKSM:subsystem-g} and \eqref{EnKSM:subsystem-f}. We apply the Sherman-Morrison formula  \eqref{SWM} again. The solution of the linear system \eqref{EnKSM:subsystem-f} can be obtained as follows:

where  and  are the solutions of the following linear systems, respectively:


The linear system \eqref{EnKSM:subsystem-g} can be solved similarly. Note that the solution of each linear system involves the computation of two new linear systems, derived from the matrix sequence  \eqref{EnKSM:sequence-to-replace}. each of the new linear systems can be solved by applying recursively the Sherman-Morrison formula. For simplicity we denote by  and  the solutions of the new linear systems in each recursively application of the Sherman-Morrison formula. We have that:
{\allowdisplaybreaks

}
where  can be either, a column of matrix  or . 

We note that:
\begin{itemize}
\item The computation of  involves the solution of the linear systems  and .
\item Since the recursion is based on the sequence of matrices defined in \eqref{EnKSM:sequence-to-replace}, the base case is the linear system  in which the matrix  is (block) diagonal.
\end{itemize}

From the previous analysis we derive a recursive Sherman-Morrison formula as follows. Define

where . the columns of  are computed as follows:


The recursive the computations performed by  can be represented as a tree in which the solution 
 of each node depends on the computations of its left () and right () children (i.e., on the solutions of two linear systems). Figure \ref{Fig:sherman-full} illustrates the derivation of linear systems in order to solve  for  and , .


\begin{figure}[H]
  \centering
	      \includegraphics[width=1\textwidth]{./images/Recursive_SMF/RepeatedComputations.pdf}
  \caption{\label{Fig:sherman-full}The recursive Sherman-Morrison formula () applied to solve the linear system .   Here  is any column of matrix . Dashed nodes represent repeated computations.}
\end{figure}


We see that  solves multiple times identical linear systems. For instance, the repeated computations performed in order to solve  are represented in Figure \ref{Fig:sherman-full} as dashed nodes. There, for instance, the linear system  is solved four times in the last level. The total number of linear systems to solve is , i.e., it increases exponentially with regard to the number of ensemble members if identical computations are not avoided. Next subsection discusses how to achieve this and obtain an efficient implementation of the recursive Sherman-Morrison formula.


\subsection{An iterative Sherman-Morrison formula for matrix inversion}
\label{Sec:IterativeShermanMorrison}


In order to avoid identical computations  in Figure \ref{Fig:sherman-full} we can solve the linear systems from the last level of the tree up to the root level. We denote by  and  the matrices holding partial results of the computations with regard to  and , respectively. 

Level  can be computed as follows without any repeated effort: 

We make use of the Sherman-Morrison formula \eqref{SWM} and compute level 1 as follows:

Note that  has not been updated since it is not needed in the computations of the next levels. Similarly, the computations at level 2 make use of the Sherman-Morrison formula \eqref{SWM}:

The vectors  and  are not required for the computations of level 3, and they are not updated. Making use of the Sherman-Morrison formula
\eqref{SWM} once again, the root level is computed as follows:

The computations performed by this iteration are shown in Figure \ref{Fig:sherman-optimal}. 
\begin{figure}[H]
  \centering
	      \includegraphics[width=1\textwidth]{./images/Recursive_SMF/IterativeComputations.pdf}
  \caption{\label{Fig:sherman-optimal}Necessary computations for the solution of  using the recursive 
  Sherman-Morrison formula. This iterative version avoids all redundant computations.}
\end{figure}

Some key features of the iteration are highlighted next.
\begin{itemize}
\item The number of iterations is .
\item At level 0 matrices  and  are computed as follows:

\item The matrix  is never stored in memory. It can be represented implicitly by matrix . 
This implicit representation realizes considerable memory savings, especially when  .
\item At iteration , only the columns  with  are updated.
\end{itemize}

In summary, the solution of the linear system \eqref{EnKSM:system-to-solve-W} is obtained by the following iteration:

Since the matrix  has a simple structure its inverse is easy to obtain. In the case of  (block) diagonal:

and in general, under the assumptions done ( is easy to decompose), the computations  and  can be performed with no more than  long operations.

Putting it all together, we define the iterative Sherman-Morrison formula  as follows:
\begin{itemize}
\item \textbf{Step 1.} Compute the matrices  and  as follows:
 
\displaystyle 
\sZ{0} &=& \invS{\R} \cdot \D,\\ 
\U{0} &=& \invS{\R} \cdot \V,

where  is computed according to its special structure. 

\item \textbf{Step 2.} For  to  compute:
 
\label{SM-step-2-h}
\h{\k} &=& \left(1+\v{\k}^{ \bf T} \cdot \u{\k}{\k-1}\right)^{-1} \, \u{\k}{\k-1}  \in \Re^{\Nobs \times 1}, \\  
\label{SM-step-2-Z}
\sZ{\k} &=& \sZ{\k-1} - \h{\k} \cdot \left( \v{\k}^{\bf T} \cdot \sZ{\k-1} \right) \in \Re^{\Nobs \times \Nens}, \\ 
\label{SM-step-2-u}
\u{\rm i}{\k} &=& \u{\rm i}{\k-1} - \h{\k} \cdot \left( \v{k}^{\bf T} \cdot \u{\rm i}{\k-1} \right) \in \Re^{\Nobs \times 1}, \text{ }.

\end{itemize}


We now use the iterative Sherman-Morrison formula in the analysis step to obtain
an efficient implementation of the Ensemble Kalman filter (SMEnKF). This filter is as follows.
The background ensemble states  are obtained from the forecast step \eqref{forecast-step},
the ensemble mean  is given by \eqref{ensemble-background-mean}, and the ensemble deviations
form the mean  are given by \eqref{EnSM:Matrix-member-deviations}. 
The analysis is obtained as follows:

where the function  is an efficient implementation of the observation operator
applied to several state vectors, represented by . 

\subsubsection{Inflation aspects}
Inflation increases periodically the ensemble spread, such as to compensate for the small ensemble size, to
simulate the existence of model errors, and to avoid filter divergence \cite{LiHong2009}.	
All the inflation techniques applied in traditional EnKF can be used, virtually without modification, 	
in the context of SMEnKF. For example, after the forecast step, one can increase the spread of the ensemble

such as the ensemble covariance  is increased by a factor   \cite{WuZheng2011}. 

\subsubsection{Localization aspects}
Using \eqref{EnKF:kalman-gain}, the analysis step can be written as follows:

Localization techniques are explained in detail in \cite{Anderson200799}.
Localization replaces the ensemble based  by a matrix  in \eqref{EnKSM:Localization}, where  is a localization matrix and 
  represents the Schur product. 
 
Clearly  localization in the form \eqref{EnKSM:Localization} requires the full covariance matrix, and cannot be applied in the context of the iterative Sherman-Morrison implementation. 
Applying SMEnKF with a single data point  leads to a correction , which can be localized
by multiplication with a {\em diagonal} matrix  that scales down components with the negative exponential of their distance to the observation  location, and sets them to zero if outside the radius of influence:

This can be applied {\em in succession} for all data points to obtain a fully localized solution.

We discuss next a general approach to perform {\em partial localization}.
Let  be an individual component of the state vector and  an individual observation. Define the impact factor 
of the information in  on the state point . For example, one can use a
correlation length, and a radius about the measurement location outside which  the impact factor is zero.
Define the influence matrix , and
replace \eqref{EnKSM:Localization} with the following partial localization formula

The (i, )-th entry contains the i-th component the correction vector for the -th ensemble member and reads

The components of the correction matrix \eqref{localized-correction-component} are independent of one another, and can be evaluated in parallel
after the system solution  has been computed.



\subsection{Computational complexity}
\label{Sec:computational-complexity}


In the complexity analysis of the iterative Sherman-Morrison formula we count only the long operations (multiplications and divisions). Moreover, as discussed before, we make the assumptions presented in \cite{Tippett2003,Jan06}, namely, the data error covariance matrix  is inexpensive to decompose, and the observation operator  can be applied efficiently to any vector. We now analyze each each step of the iterative Sherman-Morrison formula when  is diagonal, the extension to nondiagonal data error covariance matrices is inmediate.

In the first step \eqref{SM-step-1} each row  of matrices  and  is divided by the corresponding component  in order to obtain  and  respectively. This yields to  number of long operation for each matrix, therefore:


In the second step  \eqref{SM-step-2} we compute the vector  \eqref{SM-step-2-h}, and the matrices  \eqref{SM-step-2-Z} and  \eqref{SM-step-2-u}. The number of long operations for each of one are as follows:

Since the second step \eqref{SM-step-2} is performed  times, the number of long operations can be expressed as:

Consequently, from \eqref{EnKSM:operations-count-step-one}--\eqref{EnKSM:step2-long-operations}, we have

which yields a complexity of

Note that when  is not diagonal, under the assumptions done, the computations  \eqref{SM-step-1} of  and  can be efficiently performed in  long operations;  the overall effort becomes . This leads to the same complexity  \eqref{EnKSM:upper-bound} for  diagonal, block diagonal, or in general easy to decompose. 

The overall complexity of the analysis step for the iterative formula

is:

The complexity of the proposed implementation of the EnKF is equivalent to the upper bounds os the methods described in \cite{Tippett2003}, as detailed in the Table  \ref{Tab:Analysis-Complexity-Contrast}. The term  does not appear in the upper-bound of the proposed method even when  is not diagonal. This term can affect the performance of the EnKF when .

\begin{table}[H]
\centering
\begin{tabular}{|l|l|} \hline
\bf Analysis method & \bf Computational cost \\ \hline
Direct \cite{Tippett2003} &  \\
Serial \cite{Anderson07} (for each observation) &  \\
ETKF \cite{Anderson01} &  \\
EAKF \cite{Anderson01} &  \\
{\bf Proposed EnKF Implementation} &  \\ \hline
\end{tabular}
\caption{Summary of computational costs of the analysis steps for several ensemble filters. The costs are functions of the ensemble size , number of observations  and state dimension .}
\label{Tab:Analysis-Complexity-Contrast}
\end{table}

Maponi \cite{Maponi2007276} proposed a general approach based on the Sherman Morrison formula  to solve linear systems. The application of this generic algorithm 
to \eqref{EnKSM:system-to-solve-W} leads to an increased computational cost as the special structure of the system (and special structure of ) are not exploited. The generic algorithm 
applied to EnKF analysis 

uses the decomposition \cite[Remark 1]{Maponi2007276}:

where  is a diagonal matrix holding the diagonal entries of ,  is the i-th column of  and  is the i-th element of the canonical basis in . Thus, according to \cite[Corollary 4]{Maponi2007276}, each linear system \eqref{W-single-representation} can be solved with  long operations, leading to a total of

Therefore the computational cost of the analysis step is:

which is larger than the computational cost of our proposed EnKF implementation when either  or . Moreover, according to \citep[Theorem 3]{Maponi2007276}, when , the solution of linear system ~\eqref{W-single-representation} can be computed with no more than  long operations. The resulting computational cost of the analysis step is:

which is similar to the computational costs of the ETKF and EAKF methods when . In this case, it is unclear how to construct the matrix  according to Maponi's method;  can not be chosen as we propose since  must be diagonal and  differs from our definition in ~\eqref{EnKSM:member-observations}. In addition, Maponi's algorithm requires the explicit representation in memory of the matrix , which, in practice, is  dimensional. In contradistinction,  is not required explicitly in memory by our iterative Sherman Morrison formula. 

Lastly, the stability conditions of Maponi's method are not discussed in \cite{Maponi2007276}. Furthermore, the sequence of matrices  are not proved to be non-singular, which is crucial for the well-performance of that method. On the contrary, the stability analysis of the iterative Sherman Morrison formula is discussed in the next section.

\subsection{Stability Analysis}
\label{EnKSM:stability-iterative-method}

The solution of the linear system \eqref{EnKF:system-to-solve} by the iterative Sherman Morrison formula yields the next sequence of matrices during the computation of :

where 


The following situations may affect the proposed method:
\begin{itemize}
\item If any step produces , then subsequent steps cannot proceed.
\item Round-off errors can be considerably amplified if  (numerical instability). \item If any matrix  in the sequence:

is singular, the algorithm cannot proceed. 
\end{itemize} 

We now show that the positive definiteness of the covariance matrix  is a sufficient in order to guarantee the stability of the iterative Sherman Morrison formula.
 
 
\begin{theorem}
\label{Theo:sequence-positive}
Assume that  is positive definite with  for any .
Then all matrices  are positive definite with  for any .
\end{theorem} 
 
\begin{proof}
First,  is positive definite. Next, we proceed by finite induction and assume that  is positive definite
with .
From \eqref{EnKSM:system-to-solve-W} we have that:

and therefore  is also positive definite:

\end{proof} 
 
\begin{theorem}
\label{Theo:non-zero-gamma-values}
Assume that  is positive definite. The sequence of values  generated by the algorithm are strictly greater than one for all .
\end{theorem}


\begin{proof}
By the iterative Sherman Morrison formula, the common computations () are given by:

Since  is positive definite we have:

consequently  for all .
\end{proof}


We have the following direct corollary of Theorem \ref{Theo:sequence-positive}.

\begin{theorem}
\label{Theo:sequence-non-singular}
Assume that  is positive definite. At iteration , the linear system:

has a unique solution, for .
\end{theorem}




\subsection{Pivoting}
\label{pivoting}


Theorem \ref{Theo:non-zero-gamma-values} shows that  values cannot be near zero. Due to this, we expect that the round-off errors will not increase considerably during an iteration of the iterative Sherman Morrison formula since:


The following pivoting strategy can be (optionally) applied in order to further decrease round-off error accumulation. It consists of interchanging the columns of matrices  and  such that the pair  maximizes . Formally, at iteration , prior the matrix computations \eqref{SM-step-2}, we look for a column index  such that:

and then, the columns  and  are interchanged in matrices  and . 

The iterative Sherman Morrison formula with pivoting gives the next computational cost:

which yields to:

from which we can conclude that seeking the maximum value of  according to \eqref{EnKSM:gamma-value} does not increase the computational cost of the iterative Sherman Morrison formula. Consequently, the overall complexity in the analysis step remains bounded by ~\eqref{EnKSM:overall-complexity-analysis}. 

\subsection{Parallel implementation}
\label{parallelimpl}


In this section we discuss an efficient parallel implementation of the iterative Sherman-Morrison formula. Since the algorithm \eqref{SM-step-1}--\eqref{SM-step-2} can be applied individually to each column of the matrices  and , there are  computations that can be performed in parallel. We define the matrix  holding the columns of  and  as follows:

Let  be the number of available processors at the initial time. The number of operations per processor is 

The matrix \eqref{ParEnKSM:G0}  can be written as 

where the blocks  are 

The parallel, first step \eqref{SM-step-1} of the iterative Sherman-Morrison formula is implemented as an update over the blocks:

which yields


The second step \eqref{SM-step-2} of the iterative Sherman-Morrison formula consists of a sequence of updates applied to the matrices  and . Such matrices are represented by the columns of matrix . Thus, consider the computation of level one, each column of the matrix  can be updated as follows:

Similarly to the first step, the computations can be grouped in blocks

and distributed over the processors:

Note that  () is not updated since it is not required in subsequent computations. 
Thus, for the matrix 

the next common computation is  (), and for the same reasons, this vector is not updated. 

In general, at time step , , the first  columns of the matrix  are not included in the update process:

The parallel computation of \eqref{SM-step-2} at time step  is performed as follows:
\begin{itemize}
\item Compute the number of computation units (columns of matrix ) per processor:

\item Perform the update in parallel over the blocks:

where 

\end{itemize}

This parallel implementation of the iterative Sherman Morrison formula leads to the complexity:


Notice, when the number of processors at time  is  then . Hence, the corresponding computational cost of the analysis step is bounded by:

therefore, when the number of observations is large enough relative to the number of ensemble members, this parallel approach of the iterative Sherman-Morrison formula exhibits a linear behavior, making this implementation attractive. 

\section{Experimental Results}
\label{sec:results}


In this section several computation tests are conducted in order to assess the accuracy and running time of the EnKF based on iterative Sherman Morrison formula. 
\subsection{Experimental setting}
\label{Results:experimental-settings}


The Sherman-Morrison EnKF implementation as well as the EnKF implementations based on Cholesky and SVD are coded in Fortran 90. The Cholesky and SVD decompositions use functions from the LAPACK library \cite{Anderson90} as follows: 
\begin{itemize}
\item The matrix  is built using DSYRK as follows:

\item The functions DPOTRF and DPOTRI are used to compute the Cholesky decomposition of matrix .
\item The SVD decomposition is performed tby the DGESVD function.
\end{itemize}

In order to measure the quality of the solutions we employ the following performance metrics. The Elapsed Time (ET) measures the overall simulation time
for a method . This metric is defined as follows:

Where  and  are the running time for the overall forecast and analysis steps respectively. 

The Root Mean Square Error (RMSE) is defined as follows:

where  is the number of time steps and  is the Root Square Error at time  defined as follows:

where  is the true vector state at time , and  can be either the ensemble mean in the forecast  or analysis  at time . As can be seen the  RMSE measures in average the distance between a reference solution () and the given solution ().

The EnKF implementations are tested on two systems: the Lorenz 96 model \cite{Lorenz98} representing the atmosphere, and a quasi-geostrophic model \cite{Carton94} representing the ocean. They define the model operators () in the EnKF experiments. To compare the performance of different EnKF implementations we measure the elapsed times and the accuracy of analyses for different values of  and . 

\subsection{Lorenz-96 model ()}
\label{Results:Lorenz-Model}


The Lorenz 96 model is described by the following system of ordinary differential equations \cite{Lorenz98}:


which has been heuristically formulated in order to take into in account properties of global atmospheric models such as the advection,  dissipation and forcing. This model exhibits extended chaos with an external forcing value (), when the solution is in the form of moving waves. For this reason, the model is adequate to perform basic studies of predictability.

The test assesses how the efficiency of the EnKF implementations depend on the input parameters  and  when  (the number of observations and ensemble members are relatively close). The experimental setting is described below.

\begin{itemize}
\item One time unit of the Lorenz 96 model corresponds to five days of the atmosphere. The observations are made over 500 days (100 time units).
\item The background error is assumed to be , i.e., the initial ensemble mean's deviation from the reference solution is drawn from a normal distribution
whose standard deviation is  of the reference value.
\item The external forcing is set to . 
\item The dimensions of the model state  are . While the typical dimension for the Lorenz-96 model is ,
we scale the system up to assess the performance of different implementations.
\item The number of observations equals the number of states, . Due to this, the analysis step involves large linear systems of size . 
\item The number of ensemble members  depends on the size of the state vector as shown in Table  \ref{Tab:Experimental-Settings-Nens}.

\item At each time , the synthetic observations are constructed as follows:

since the number of observations and variables of the vector state are the same.  belongs to a normal distribution with zero mean and covariance matrix
 
as is usual in practice. The errors are replicated for each compared, EnKF implementation. Due to this, the same data errors are hold for all tests. 

\item The assimilation window is five model days.

\item The localization \eqref{localized-correction-component} is applied using the influence factors

where  is the minimum distance between the indexes  and  of the vector state; this distance accounts for the
periodic boundary conditions in \eqref{eqn:Lorenz96}.

\end{itemize}

The RMSE results are shown in Table  \ref{Tab:Lorenz-Results-RMS}.
All methods provide virtually identical analyses. As expected, the analysis improves when the size of the ensemble is increased. 

\begin{table}[H]
\centering
{
\begin{tabular}{|c|c|} \hline
 &   \\ \hline
 & \{200,250,300,350,400\} \\ \hline
 & \{400,450,500,550,600\} \\ \hline
 & \{900,950,1000,1050,1100\} \\ \hline
 & \{1500,1550,1600\} \\ \hline
\end{tabular}
}
\caption{Number of ensemble members  with respect to the dimension of the vector state .}
\label{Tab:Experimental-Settings-Nens}
\end{table}

\begin{table}[H]
\centering
{\scriptsize
\begin{tabular}{|c|c|c|c|c|c|} \hline
 & {\bf Step} &  &  &  &   \\ \hline
\multirow{10}{*}{500/500} & \multirow{5}{*}{Forecast}  & 200 &  &  &   \\ 
&  & 250 &  &  &  \\ 
&  & 300 &  &  &   \\ 
&  & 350 &  &  &   \\ 
&  & 400 &  &  &  \\ \cline{2-6}
& \multirow{5}{*}{Analysis} &  200 &  &  &   \\ 
& & 250 &  &  &  \\ 
& & 300 &  &  &   \\ 
& & 350 &  &  &   \\ 
& & 400 &  &  &  \\ \hline
\multirow{10}{*}{1000/1000} & \multirow{5}{*}{Forecast}  & 400 &  &  &   \\ 
&  & 450 &  &  &  \\ 
&  & 500 &  &  &   \\ 
&  & 550 &  &  &   \\ 
&  & 600 &  &  &  \\ \cline{2-6}
& \multirow{5}{*}{Analysis} &  400 &  &  &   \\ 
& & 450 &  &  &  \\ 
& & 500 &  &  &   \\ 
& & 550 &  &  &   \\ 
& & 600 &  &  &  \\ \hline
\multirow{10}{*}{3000/3000} & \multirow{5}{*}{Forecast}  & 900 &  &  &   \\ 
&  & 950 &  &  &  \\ 
&  & 1000 &  &  &   \\ 
&  & 1050 &  &  &   \\ 
&  & 1100 &  &  &  \\ \cline{2-6}
& \multirow{5}{*}{Analysis} &  900 &  &  &   \\ 
& & 950 &  &  &  \\ 
& & 1000 &  &  &   \\ 
& & 1050 &  &  &   \\ 
& & 1100 &  &  &  \\ \hline
\multirow{6}{*}{5000/5000} & \multirow{3}{*}{Forecast}  & 1500 &  &  &   \\ 
&  & 1550 &  &  &  \\ 
&  & 1600 &  &  &  \\ \cline{2-6}
& \multirow{3}{*}{Analysis} &  1500 &  &  &   \\ 
& & 1550 &  &  &  \\ 
& & 1600 &  &  &  \\ \hline
\end{tabular}
}
\caption{RMSE for the Lorenz-96 model with different number of states. When the number of ensemble members is increased the estimation of the true vector state is improved. All EnKF implementations provide virtually identical results.}
\label{Tab:Lorenz-Results-RMS}
\end{table}



\begin{figure}[H]
\centering
\begin{sideways}  \end{sideways} 
\subfigure[Sherman]{
\includegraphics[width=0.25\textwidth]{images/Lorenz_RMSE/Lorenz500/ShermanLorenzA500} 
}
\subfigure[Cholesky]{
\includegraphics[width=0.25\textwidth]{images/Lorenz_RMSE/Lorenz500/CholeskyLorenzA500}
}
\subfigure[SVD]{
\includegraphics[width=0.25\textwidth]{images/Lorenz_RMSE/Lorenz500/SVDLorenzA500} 
}
\caption{Analysis RMSE for the Lorenz model with 500 of variables. Different curves correspond to  different numbers of ensemble members:  and . When the number of ensemble members is increased the analysis is improved. }
\label{Fig:Lorenz500-Results-Simulation}
\end{figure}


Figures  \ref{Fig:Lorenz500-Results-Simulation} and  \ref{Fig:Lorenz1000-Results-Simulation} show the RMSE decrease over the assimilation window for  500 and 1000, respectively. When the number of ensemble members is increased the analysis errors are smaller, as expected. There is no significant difference in results between different implementations of the EnKF.


\begin{figure}[H]
\centering
\begin{sideways}  \end{sideways} 
\subfigure[Sherman]{
\includegraphics[width=0.25\textwidth]{images/Lorenz_RMSE/Lorenz1000/ShermanLorenzA1000} 
}
\subfigure[Cholesky]{
\includegraphics[width=0.25\textwidth]{images/Lorenz_RMSE/Lorenz1000/CholeskyLorenzA1000}
}
\subfigure[SVD]{
\includegraphics[width=0.25\textwidth]{images/Lorenz_RMSE/Lorenz1000/SVDLorenzA1000} 
}
\caption{Analysis RMSE for the Lorenz model with 1000 of variables. Different curves correspond to  different numbers of ensemble members:  and . When the number of ensemble members is increased the analysis is improved. }
\label{Fig:Lorenz1000-Results-Simulation}
\end{figure}

The ET results are shown in Table  \ref{Tab:Lorenz-Results-TIME}.
The Cholesky decomposition is the most efficient for a small number of observations and states. 
When the number of observations is increased, the relative performance of Cholesky deteriorates, as expected from the complexity results presented in Section  \ref{intro}. The Cholesky decomposition solution of the linear system \eqref{EnKF:system-to-solve} is not suitable when the number of observations is large. The SVD implementation exhibits a good performance for a small number of ensemble members and observations. However, the ET of the SVD implementation grows faster than that of the Cholesky implementation when the number of ensemble and/or observations are increased, due to the term  in its complexity formula. Tthe EnKF implementation based on SVD is not suitable  for a large  number of observations or a large number of ensemble members. Finally, the Sherman-Morrison implementation has the best performance for a large number of observations and states. This implementation is suitable for a large number of observations. Since the term  does not appear in the cost upper-bound of the iterative-Sherman implementation, when , the proposed implementation will exhibit a better performance than those implementations presented in ~\cite{Anderson01,Anderson07,Tippett2003} since they are upper-bounded by  \eqref{EnKF:SVD-overall-complexity} (see table  \ref{Tab:Analysis-Complexity-Contrast}). 

\begin{table}[H]
\centering
{
\begin{tabular}{|c|r|r|r|r|} \hline
 &  &  &  &   \\ \hline
\multirow{5}{*}{500/500} & 200 &  s &  s &  s \\ 
&   250 &  s &  s &  s \\ 
&   300 &  s &  s &  s \\ 
&   350 &  s &  s &  s \\ 
&   400 &  s &  s &  s \\ \hline
\multirow{5}{*}{1000/1000} & 400 &  s &  s &  s \\ 
&   450 &  s &  s &  s \\ 
&   500 &  s &  s &   s \\ 
&   550 &  s &  s &   s \\ 
&   600 &  s &  s &  s \\ \hline
\multirow{5}{*}{3000/3000} & 900 &  h &  h &  h  \\ 
&   950 &  h &  h &  h \\ 
&   1000 &  h &  h &  h  \\ 
&   1050 &  h &  h &  h  \\ 
&   1100 &  h &  h &  h \\ \hline
\multirow{3}{*}{5000/5000} & 1500 &  h &  h &  h  \\ 
&   1550 &  h &  h &  h\\ 
&   1600 &  h &  h &  h  \\ \hline
\end{tabular}
}
\caption{Computational times for the Lorenz model assimilation performed with different EnKF implementations. 
The Cholesky decomposition is the most efficient for a small number of observations and states. The
Sherman-Morrison implementation is the bet for a large number of observations and states. 
}
\label{Tab:Lorenz-Results-TIME}
\end{table}

\subsection{Quasi-geostrophic model ()}


The Earth's ocean has a complex flow system influenced by the rotation of the Earth, the density stratification due to temperature and salinity, as well as other factors. The quasi-geostrophic (QG) model is a simple model which mimics the real behavior of the ocean. 
It is defined by the following partial differential equations \cite{Carton94}:

\label{Results:QG-Model}

where 

 is the potential vorticity,  is the stream function,  is the Froud number,  is the relative vorticity,  is a sort of the Rossby number,  is the bottom friction,  is the horizontal friction and  is the biharmonic horizontal friction and  and  represent the horizontal and vertical components of the space.

Moreover,  and  are related to one another through an elliptic operator \cite{Pedl96}:

which yields 

where 


This elliptic property reflects the assumption that the flow is geostrophically balanced in the horizontal direction, and hydrostatically balanced in the vertical direction.

The QG experiment studies the behavior of EnKF implementations when  (the number of observations is much larger than the number of ensemble members) as is usually the case in practice. Moreover, this scenario is more difficult than the previous one (the Lorenz model): large model-errors are considered in the initial ensemble members. Besides, data is available every 10 time units. 

We consider three different grids, denoted QGNM, where the number of horizontal and vertical grid points are  and , respectively. Specifically, we employ in experiments QG33 (small instance),  QG65 (medium instance) and QG129 (large instance). The horizontal and vertical dimensions of the grid are denoted by  and  respectively.  These instances and the corresponding parameter values are summarized in Table  \ref{Tab:QG-Instances}.
\begin{table}[H]
\centering
{ \footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline
Instance &  &  &  &  &  &  &  &  &  \\ \hline
QG33 & 0.4 & 0.4 & 33 & 33 &  &  &  & 1.0 &  \\ \hline
QG65 & 1.0 & 1.0 & 65 & 65 &  &  &  & 1.0 &   \\ \hline
QG129 & 1.0 & 1.0 & 129 & 129 &  &  &  & 1.0 &   \\ \hline
\end{tabular}
}
\caption{Parameter values for the QG model instances considered.  and  represent the horizontal and vertical grid sizes, and  and  are the number of horizontal and vertical grid points, respectively.}
\label{Tab:QG-Instances}
\end{table}

The experimental settings are described below.
\begin{itemize}
\item There are 1200 time steps, each of one representing 1.27 days in the ocean.
\item The vorticity of the ocean at each grid point provides a component of the vector state. 
\item The computation of the stream function is done through the solution of the Helmholtz \cite{Otto99} function according to the elliptic property \eqref{QGModel:inversion-vorticity}.
\item Homogeneous Dirichlet boundary conditions are assumed. Due to this, the boundaries of the grid are not mapped into the state vector, and .
\item The initial ensemble members are constructed as follows:

where  is drawn from a Normal distribution with zero mean and covariance matrix


For testing purposes, three values are assumed for the standard deviation of model errors (): 2.5, 5.0 and 7.5. Notice the large dispersion of the initial ensemble members, which can make difficult the convergence of any filter since the huge spread out of the initial ensemble members with respect to the typical value . 

\item The number of observation per simulation, for each size () of the model state , is defined as follows:

where  is the percentage of components observed from the model state. The values given to  are 50\%, 70\% and 90\%. Those, measurements are taken every 10 time units and they are constructed as shown in equation \eqref{eq:syntethic-observations}. Notice, there are 120 analysis steps out of 1200 time steps (10\% of the total simulation time).

\item For the time evolution of the model, zero boundary conditions are assumed and the boundaries are not included onto the ensemble representation.  Due to this, the dimension of the vector state .

\item For each instance we consider simulations with  ensemble members. The number of ensemble members is one to two orders of magnitude smaller than the total number of observations.

\end{itemize}


The RSME values for analysis errors for the QG33, QG65 and QG129 instances are shown in Tables  \ref{Tab:QG33-Results-RMSE}, \ref{Tab:QG65-Results-RMSE} and  \ref{Tab:QG129-Results-RMSE}, respectively. The results depend on the number of ensemble members (), the number of observations (), and the deviation of the initial ensemble mean (). The RSME is quantifiess errors in the stream function , whose values are computed through the relation \eqref{QGModel:inversion-vorticity}.  In terms of accuracy there is no significant difference between different EnKF implementations. As expected, when the error in the initial ensemble is increased, the accuracy in the analysis decreases. The error does not show an exponential growth, even when the number of components in the model state () is much larger than the number of ensemble members (e.g., for the QG129 instance). When the number of ensemble members is increased, the analysis error is decreased. This is illustrated by the snapshots of the QG33 simulation over 1200 time steps presented in Figure  \ref{Fig:QG33-Snapshots-Simulation}. There, we can clearly see that the ensemble of size 100 provides a better estimation () to the true state of the model () than the ensembles of sizes 20 and 60. Additionally, the number of observations plays an important role in the estimation of the true model state when the size of the vector state is much larger than the number of ensemble members. 

The ET values for the QG33, QG65 and QG129 simulations are shown in Tables  \ref{Tab:QG33-Results-RMSE}, \ref{Tab:QG65-Results-RMSE} and  \ref{Tab:QG129-Results-RMSE}, respectively. The time is expressed in seconds (s)
if it is below 30 minutes, and otherwise is expressed
in minutes (min) and hours (h). The Cholesky implementation shows good performance when the number of observations is small. From Table  \ref{Tab:QG33-Results-ElapsedTime} (the blocks where the number of observations are 480, 672 and 864) we see that the Cholesky implementation performance is more sensitive to the number of observations than to the number of ensemble members. 
This EnKF implementation is not suitable for a large number of observations. For instance, the Cholesky elapsed time for the QG129 instance is not presented since each simulation takes more than 4 days in order to be completed. 

The SVD implementation shows a better relative performance than for the Lorenz 96 test, since the number of ensemble members is small with respect to the number of observations. For example, for the QG33 instance, the SVD implementation shows a better performance than Cholesky when the number of observations and ensemble members are small. In addition, when the size of vector state is increased, the SVD implementation shows a better performance than Cholesky. This agrees with the computational complexity upper bounds presented  in Section  \ref{intro}. As is expected, the performance of the SVD based methods is better than the Cholesky implementations when the number of observations is much larger than the number of the ensemble members. 

The Sherman-Morrison implementation shows the best performance among the compared methods.
This is true even when the number of observations is much larger than the number of ensemble members, as seen in Table  \ref{Tab:QG129-Results-ElapsedTime}. The results of both test cases (the quasigeostrophic and Lorenz models) lead to the conclusion that the performance of the iterative-Sherman implementation is not sensitive to the increase in the number of observations, making it attractive for implementation with large-scale observational systems.


\begin{table}[H]
\centering
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|} \hline
 &  &  &  &  &  \\ \hline


\multirow{9}{*}{20} &  \multirow{3}{*}{480}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{672}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{864}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{1-6}
\multirow{9}{*}{60} &  \multirow{3}{*}{480}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{672}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{864}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{1-6}
\multirow{9}{*}{100} &  \multirow{3}{*}{480}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{672}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{864}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\hline
\end{tabular}
}
\caption{Analysis RMSE for different EnKF implementations applied to the QG33 instance.
All methods give similar results. When the number of ensemble and/or observations is increased, the analysis accuracy is improved.}
\label{Tab:QG33-Results-RMSE}
\end{table}



\begin{figure}[H]
\centering
\begin{tabular}{VNNNN}
&  & ,  & ,  & , \\ 
\rotatebox{90}{} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_True0} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF10} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF20} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF30} \\
\begin{sideways}  \end{sideways} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_True1} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF11} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF21} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF31} \\
\begin{sideways}  \end{sideways} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_True2} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF12} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF22} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF32} \\
\begin{sideways}  \end{sideways} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_True3} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF13} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF23} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF33} \\
\begin{sideways}  \end{sideways} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_True4} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF14} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF24} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF34} \\
\begin{sideways}  \end{sideways} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_True5} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF15} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF25} &\includegraphics[width=0.2\textwidth]{images/QG33_SnapShots/QG33_SF35} 
\end{tabular}
\caption{Snapshots of the QG33 simulation for  = 20,60 and 100 members, at the time steps  and  (out of 1200). As expected, when the number of ensemble members is increased the estimation of the true state () is improved (the RMSE is decreased).
}
\label{Fig:QG33-Snapshots-Simulation}
\end{figure}


\begin{table}[H]
\centering
{\footnotesize
\begin{tabular}{|c|c|c|r|r|r|} \hline
 &  &  &  &  &  \\ \hline
\multirow{9}{*}{20} &  \multirow{3}{*}{480}  & 2.5 & 17.6 s & 33.4 s & 24.7 s\\
& & 5.0 & 17.2 s & 32.8 s & 25.3 s \\ 
& & 7.5 & 17.3 s & 33.1 s & 28.6 s  \\ 
\cline{2-6}
&  \multirow{3}{*}{672}  & 2.5 & 17.9 s & 61.9 s & 39.4 s \\
& & 5.0 & 17.7 s & 62.9 s & 49.3 s \\ 
& & 7.5 & 17.8 s & 62.1 s & 37.6 s \\ 
\cline{2-6}
&  \multirow{3}{*}{864}  & 2.5 & 17.8 s & 113.7 s & 57.9 s \\
& & 5.0 & 18.2 s & 116.3 s & 79.7 s \\ 
& & 7.5 & 18.1 s & 118.4 s & 61.2 s \\ 
\cline{1-6}
\multirow{9}{*}{60} &  \multirow{3}{*}{480}  & 2.5 & 42.9 s & 57.8 s & 63.7 s \\
& & 5.0 & 42.9 s & 57.5 s & 62.9 s \\ 
& & 7.5 & 42.8 s & 57.8 s & 58.3 s \\ 
\cline{2-6}
&  \multirow{3}{*}{672}  & 2.5 & 44.3 s & 90.3 s & 142.3 s\\
& & 5.0 & 44.5 s & 89.8 s & 92.3 s \\ 
& & 7.5 & 44.5 s & 90.3 s & 91.3 s \\ 
\cline{2-6}
&  \multirow{3}{*}{864}  & 2.5 & 46.8 s & 150.6 s & 187.3 s \\
& & 5.0 & 46.6 s & 156.7 s & 144.8 s \\ 
& & 7.5 & 46.5 s & 154.3 s & 200.9 s \\ 
\cline{1-6}
\multirow{9}{*}{100} &  \multirow{3}{*}{480}  & 2.5 & 72.3 s & 83.2 s & 102.0 s \\
& & 5.0 & 72.4 s & 83.2 s & 96.8 s \\ 
& & 7.5 & 72.6 s & 83.4 s & 94.4 s \\ 
\cline{2-6}
&  \multirow{3}{*}{672}  & 2.5 & 77.8 s & 118.9 s & 140.2 s \\
& & 5.0 & 77.8 s & 119.5 s & 166.8 s \\ 
& & 7.5 & 77.1 s & 120.1 s & 216.4 s \\ 
\cline{2-6}
&  \multirow{3}{*}{864}  & 2.5 & 82.8 s & 209.1 s & 412.9 s \\
& & 5.0 & 83.2 s & 212.7 s & 289.7 s \\ 
& & 7.5 & 82.7 s & 202.6 s & 304.6 s \\ 
\hline
\end{tabular}
}
\caption{Computational times for several EnKF implementations applied to the QG33 instance.
Different numbers of ensemble members and numbers of observations are considered.}
\label{Tab:QG33-Results-ElapsedTime}
\end{table}
\begin{table}[H]
\centering
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|} \hline
 &  &  &  &  &  \\ \hline
\multirow{9}{*}{20} &  \multirow{3}{*}{1984}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{2778}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{3572}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{1-6}
\multirow{9}{*}{60} &  \multirow{3}{*}{1984}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{2778}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{3572}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{1-6}
\multirow{9}{*}{100} &  \multirow{3}{*}{1984}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{2778}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\cline{2-6}
&  \multirow{3}{*}{3572}  & 2.5 &  &  &  \\
& & 5.0 &  &  &   \\ 
& & 7.5 &  &  &   \\ 
\hline
\end{tabular}
}
\caption{Analysis RMSE for different EnKF implementations applied to the QG65 instance.
All methods give similar results. }
\label{Tab:QG65-Results-RMSE}
\end{table}




\begin{table}[H]
\centering
{\footnotesize
\begin{tabular}{|c|c|c|r|r|r|} \hline
 &  &  &  &  &  \\ \hline
\multirow{9}{*}{20} &  \multirow{3}{*}{1984}  & 2.5 & 71.5 s &  44.9 min & 454.7 s\\
& & 5.0 & 70.6 s &  33.4 min & 429.3 s \\ 
& & 7.5 & 70.3 s &  45.4 min & 360.6 s\\ 
\cline{2-6}
&  \multirow{3}{*}{2778}  & 2.5 & 71.0 s & 1.8 h & 831.1 s\\
& & 5.0 & 73.2 s &  1.3 h & 735.7 s \\ 
& & 7.5 & 71.9 s &  1.4 h & 795.8 s \\ 
\cline{2-6}
&  \multirow{3}{*}{3572}  & 2.5 & 72.2 s & 3.8 h & 1602.5 s \\
& & 5.0 & 74.3 s &  3.3 h & 1112.3 s \\ 
& & 7.5 & 72.2 s &  3.0 h & 771.5 s \\ 
\cline{1-6}
\multirow{9}{*}{60} &  \multirow{3}{*}{1984}  & 2.5 & 179.0 s &  45.0 min & 1215.7 s \\
& & 5.0 & 179.5 s & 55.3 min & 1235.6 s \\ 
& & 7.5 & 178.4 s & 51.9 min & 1066.7 s \\ 
\cline{2-6}
&  \multirow{3}{*}{2778}  & 2.5 & 190.6 s &  2.4 h & 1463.3 s \\
& & 5.0 & 188.3 s & 1.9 h &   39.4 min\\ 
& & 7.5 & 189.7 s & 2.5 h &  32.4 min \\ 
\cline{2-6}
&  \multirow{3}{*}{3572}  & 2.5 & 202.6 s &  4.1 h & 1.2 h \\
& & 5.0 & 198.9 s & 2.9 h & 1.1 h  \\ 
& & 7.5 & 201.9 s &  4.3 h & 1.0 h  \\ 
\cline{1-6}
\multirow{9}{*}{100} &  \multirow{3}{*}{1984}  & 2.5 & 313.8 s & 52.4 min  & 37.2 min \\
& & 5.0 & 314.3 s & 40.1 min & 33.2 min \\ 
& & 7.5 & 309.6 s &  58.4 min  & 37.9 min  \\ 
\cline{2-6}
&  \multirow{3}{*}{2778}  & 2.5 & 346.9 s & 1.7 h &  1.1 h \\
& & 5.0 & 342.9 s &  1.7 h & 1.0 h  \\ 
& & 7.5 & 340.7 s &  2.9 h &  1.1 h  \\ 
\cline{2-6}
&  \multirow{3}{*}{3572}  & 2.5 & 373.9 s & 4.8 h & 1.6 h \\
& & 5.0 & 378.3 s & 4.0 h & 1.8 h \\ 
& & 7.5 & 383.7 s & 5.1 h  & 1.3 h \\ 
\hline
\end{tabular}
}
\caption{Computational times for several EnKF implementations applied to the QG65 instance.
Different numbers of ensemble members and numbers of observations are considered.}
\label{Tab:QG65-Results-ElapsedTime}
\end{table}



\begin{table}[H]
\centering
{\footnotesize
\begin{tabular}{|c|c|c|c|c|} \hline
 &  &  &  &  \\ \hline
\multirow{9}{*}{20} &  \multirow{3}{*}{8064}  & 2.5 &  &  \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{2-5}
&  \multirow{3}{*}{11290}  & 2.5 &  &   \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{2-5}
&  \multirow{3}{*}{14516}  & 2.5 &  &   \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{1-5}
\multirow{9}{*}{60} &  \multirow{3}{*}{8064}  & 2.5 &  &  \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{2-5}
&  \multirow{3}{*}{11290}  & 2.5 &  &   \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{2-5}
&  \multirow{3}{*}{14516}  & 2.5 &  &   \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{1-5}
\multirow{9}{*}{100} &  \multirow{3}{*}{8064}  & 2.5 &  &  \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{2-5}
&  \multirow{3}{*}{11290}  & 2.5 &  &   \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\cline{2-5}
&  \multirow{3}{*}{14516}  & 2.5 &  &   \\
& & 5.0 &  &    \\ 
& & 7.5 &  &    \\ 
\hline
\end{tabular}
}
\caption{Analysis RMSE for different EnKF implementations applied to the QG129 instance.
All methods give similar results.}
\label{Tab:QG129-Results-RMSE}
\end{table}


\begin{table}[H]
\centering
{\footnotesize
\begin{tabular}{|c|c|c|c|c|} \hline
 &  &  &  &  \\ \hline

\multirow{9}{*}{20} &  \multirow{3}{*}{8064}  & 5 & 289.9 s & 1.8 h \\
& & 5.0 & 293.5 s & 1.4 h \\ 
& & 7.5 & 286.8 s &  1.9 h  \\ 
\cline{2-5}
&  \multirow{3}{*}{11290}  & 5 & 303.2 s & 2.2 h  \\
& & 5.0 & 302.2 s & 2.0 h   \\ 
& & 7.5 & 303.5 s &  2.9  h  \\ 
\cline{2-5}
&  \multirow{3}{*}{14516}  & 5 & 315.7 s & 4.5 h  \\
& & 5.0 & 308.1 s & 3.8 h  \\ 
& & 7.5 & 309.1 s &   3.7 h  \\ 
\cline{1-5}
\multirow{9}{*}{60} &  \multirow{3}{*}{8064}  & 5 & 764.8 s & 4.1 h \\
& & 5.0 & 795.9 s &  3.2 h   \\ 
& & 7.5 & 764.7 s & 3.9 h   \\ 
\cline{2-5}
&  \multirow{3}{*}{11290}  & 5 & 838.0 s &  6.3 h \\
& & 5.0 & 832.6 s & 5.4 h  \\ 
& & 7.5 & 817.7 s & 6.7 h   \\ 
\cline{2-5}
&  \multirow{3}{*}{14516}  & 5 & 910.7 s &  9.9 h  \\
& & 5.0 & 899.9 s &  9.3 h   \\ 
& & 7.5 & 864.8 s &  11.2 h   \\ 
\cline{1-5}
\multirow{9}{*}{100} &  \multirow{3}{*}{8064}  & 5 & 1381.5 s & 5.3 h \\
& & 5.0 & 1360.3 s & 5.2 h  \\ 
& & 7.5 & 1397.9 s & 5.9 h   \\ 
\cline{2-5}
&  \multirow{3}{*}{11290}  & 5 & 1492.4 s & 10.3 h  \\
& & 5.0 & 1494.5 s &  8.9 h   \\ 
& & 7.5 & 1506.6 s & 6.7 h   \\ 
\cline{2-5}
&  \multirow{3}{*}{14516}  & 5 & 1624.8 s & 13.7 h  \\
& & 5.0 & 1634.9 s & 12.9 h   \\ 
& & 7.5 & 1664.2 s & 14.9 h   \\ 
\hline
\end{tabular}
}
\caption{Computational times for several EnKF implementations applied to the QG129 instance.
Different numbers of ensemble members and numbers of observations are considered.}
\label{Tab:QG129-Results-ElapsedTime}
\end{table}
\section{Conclusions and Future Work}\label{sec:conclusions}


We propose a novel implementation of the EnKF based on an iterative application of the Sherman-Morrison formula. The algorithm exploits the special structure of the background error covariance matrix projected onto the observation space. The computational complexity of the new approach is equivalent to that of the best EnKF implementations  available in the literature. Nevertheless, the performance (elapsed time) of most of the existing methods is strongly dependent from the condition  (the number of observations is large compared to the ensemble size); the performance of the new approach is not affected by this condition. In addition, the term  is not presented in the upper-bound of the effort of the proposed method, which leads to better performance when the number of observations and of ensemble members are of similar magnitude (). A sufficient condition for the stability of the proposed method is the non-singularity of the data error covariance matrix, which, in practice, is always the case. In addition, a pivoting strategy is developed in order to reduce round-off error propagation without increasing the computational effort of the proposed method. The computational cost of this algorithm provides a better theoretical performance than other generic formulations of matrix inversion based on the Sherman Morrison formula available in the literature. To assess the accuracy and performance of the proposed implementation two standard test problems have been employed, namely, the Lorenz 96 and the quasi-geostrophic models. All EnKF implementations tested (Cholesky, SVD, Sherman-Morrison) provide virtually identical analyses. However, the proposed Sherman-Morrison approach is much faster than the others even when the number of observations is large with respect to the number of ensemble members (). The parallel version of the new algorithm has a theoretical complexity that grows only linearly with the number of observations, and is therefore well suited for implementation in large scale data assimilation systems.


\section*{Acknowledgment}
This work has been supported in part by NSF through awards NSF OCI-8670904397, NSF CCF-0916493, NSF DMS-0915047, NSF CMMI-1130667, NSF CCF-1218454, AFOSR FA9550-12-1-0293-DEF, AFOSR 12-2640-06, and by the Computational Science Laboratory at Virginia Tech.

\bibliographystyle{abbrv}
\begin{thebibliography}{10}

\bibitem{Anderson90}
E.~Anderson, Z.~Bai, J.~Dongarra, A.~Greenbaum, A.~McKenney, J.~Du~Croz,
  S.~Hammerling, J.~Demmel, C.~Bischof, and D.~Sorensen.
\newblock Lapack: A portable linear algebra library for high-performance
  computers.
\newblock In {\em Proceedings of the 1990 ACM/IEEE conference on
  Supercomputing}, Supercomputing 90, pages 2--11, Los Alamitos, CA, USA, 1990.
  IEEE Computer Society Press.

\bibitem{Anderson01}
J.~L. Anderson.
\newblock {An Ensemble Adjustment Kalman Filter for Data Assimilation}.
\newblock {\em Monthly Weather Review}, 2001.

\bibitem{Anderson200799}
J.~L. Anderson.
\newblock {Exploring the need for localization in ensemble data assimilation
  using a hierarchical ensemble filter}.
\newblock {\em Physica D: Nonlinear Phenomena}, 230(1--2):99 -- 111, 2007.

\bibitem{Anderson07}
J.~L. Anderson and N.~Collins.
\newblock {Scalable Implementations of Ensemble Filter Algorithms for Data
  Assimilation}.
\newblock {\em Journal of Atmospheric \& Oceanic Technology}, 2007.

\bibitem{Carton94}
X.~Carton and R.~Baraille.
\newblock {Data Assimilation in Quasi-geostrophic Ocean Models}.
\newblock In {\em OCEANS 94. Oceans Engineering for Today's Technology and
  Tomorrow's Preservation. Proceedings}, volume~3, pages III/337 --III/346
  vol.3, sep 1994.

\bibitem{Cohen72}
A.~Cohen.
\newblock {Rate of Convergence of Several Conjugate Gradient Algorithms}.
\newblock {\em SIAM Journal on Numerical Analysis}, 9(2):248--259, 1972.

\bibitem{Eis81}
S.~Eisenstat.
\newblock {Efficient Implementation of a Class of Preconditioned Conjugate
  Gradient Methods}.
\newblock {\em SIAM Journal on Scientific and Statistical Computing},
  2(1):1--4, 1981.

\bibitem{Evensen2009}
G.~Evensen.
\newblock {\em {Data Assimilation: The Ensemble Kalman Filter}}, chapter~14,
  pages 210--237.
\newblock Springer, 2009.

\bibitem{Even09B}
G.~Evensen.
\newblock Estimation in an oil reservoir simulator.
\newblock In {\em Data Assimilation}, pages 263--272. Springer Berlin
  Heidelberg, 2009.

\bibitem{Even09A}
G.~Evensen.
\newblock {The Ensemble Kalman Filter for Combined State and Parameter
  Estimation}.
\newblock {\em Control Systems, IEEE}, 29(3):83 --104, june 2009.

\bibitem{Sher89}
A.~M. Fraser.
\newblock {\em {Appendix A: Formulas for Matrices and Gaussians}}, chapter~7,
  pages 117--120.
\newblock SIAM, 2008.

\bibitem{Gill96}
P.~Gill, M.~Saunders, and J.~Shinnerl.
\newblock {On the Stability of Cholesky Factorization for Symmetric
  Quasidefinite Systems}.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 17(1):35--46,
  1996.

\bibitem{Golub89}
G.~Golub and D.~OLeary.
\newblock {Some History of the Conjugate Gradient and Lanczos Algorithms:
  1948-1976}.
\newblock {\em SIAM Review}, 31(1):50--102, 1989.

\bibitem{Even08}
V.~Haugen, G.~Naevdal, L.~J. Natvik, G.~Evensen, A.~Berg, and K.~Flornes.
\newblock {History Matching Using the Ensemble Kalman Filter on a North Sea
  Field Case}.
\newblock {\em SPE Journal}, 13:382--391, 2008.

\bibitem{Kova11}
A.~Kovalenko, T.~Mannseth, and G.~N{\ae}vdal.
\newblock {Error Estimate for the Ensemble Kalman Filter Analysis Step}.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  32(4):1275--1287, 2011.

\bibitem{LiHong2009}
H.~Li, E.~Kalnay, and T.~Miyoshi.
\newblock {Simultaneous Estimation of Covariance Inflation and Observation
  Errors within an Ensemble Kalman Filter}.
\newblock {\em Quarterly Journal of the Royal Meteorological Society},
  135(639):523--533, 2009.

\bibitem{Lin99}
C.~Lin and J.~More.
\newblock {Incomplete Cholesky Factorizations with Limited Memory}.
\newblock {\em SIAM Journal on Scientific Computing}, 21(1):24--45, 1999.

\bibitem{Lorenz98}
E.~N. Lorenz and K.~A. Emanuel.
\newblock Optimal sites for supplementary weather observations: Simulation with
  a small model.
\newblock {\em Journal of the Atmospheric Sciences}, 1998.

\bibitem{Jan06}
J.~Mandel.
\newblock {Efficient Implementation of the Ensemble Kalman Filter }.
\newblock Technical report, University of Colorado at Denver and Health
  Sciences Center, 2006.

\bibitem{Maponi2007276}
P.~Maponi.
\newblock The solution of linear systems by using the sherman--morrison
  formula.
\newblock {\em Linear Algebra and its Applications}, 420(2--3):276 -- 294,
  2007.

\bibitem{Mei83}
J.~Meinguet.
\newblock {Refined Error Analyses of Cholesky Factorization}.
\newblock {\em SIAM Journal on Numerical Analysis}, 20(6):1243--1250, 1983.

\bibitem{Ott04}
E.~Ott, B.~R. Hunt, I.~Szunyogh, A.~V. Zimin, E.~J. Kostelich, M.~Corazza,
  E.~Kalnay, D.~J. Patil, and J.~A. Yorke.
\newblock {A Local Ensemble Kalman Filter for Atmospheric Data Assimilation}.
\newblock {\em Tellus A}, 56(5):415--428, 2004.

\bibitem{Otto99}
K.~Otto and E.~Larsson.
\newblock {Iterative Solution of the Helmholtz Equation by a Second-Order
  Method}.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  21(1):209--229, 1999.

\bibitem{Pedl96}
J.~Pedlosky.
\newblock {\em {Geophysical Fluids Dynamics}}.
\newblock Springer-Verlag, New York Heidelberg, 1996.

\bibitem{Reid72}
J.~Reid.
\newblock {The Use of Conjugate Gradients for Systems of Linear Equations
  Possessing Property A}.
\newblock {\em SIAM Journal on Numerical Analysis}, 9(2):325--332, 1972.

\bibitem{Sch90}
R.~Schnabel and E.~Eskow.
\newblock {A New Modified Cholesky Factorization}.
\newblock {\em SIAM Journal on Scientific and Statistical Computing},
  11(6):1136--1158, 1990.

\bibitem{Ste97}
M.~Stewart and P.~Van~Dooren.
\newblock {Stability Issues in the Factorization of Structured Matrices}.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  18(1):104--118, 1997.

\bibitem{Suarez12}
A.~Suarez, R.~Heather~Dawn, W.~Dustan, and M.~Coniglio.
\newblock {Comparison Of Ensemble Kalman Filter--based Forecasts to Traditional
  Ensemble and Deterministic Forecasts for a Case Study of Banded Snow}.
\newblock {\em Weather and Forecasting}, 27:85--105, 2012.

\bibitem{Tippett2003}
M.~K. Tippett, J.~L. Anderson, C.~H. Bishop, T.~M. Hamill, and J.~S. Whitaker.
\newblock {Ensemble Square Root Filters}.
\newblock {\em Monthly Weather Review}, 2003.

\bibitem{WuZheng2011}
G.~Wu, X.~Zheng, and Y.~Li.
\newblock {Inflation Adjustment on Error Covariance Matrix of Ensemble Kalman
  Filter}.
\newblock In {\em {Multimedia Technology (ICMT), 2011 International Conference
  on}}, pages 2160 --2163, july 2011.

\bibitem{Xia10}
J.~Xia and M.~Gu.
\newblock {Robust Approximate Cholesky Factorization of Rank-Structured
  Symmetric Positive Definite Matrices}.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  31(5):2899--2920, 2010.

\end{thebibliography}


\end{document}
