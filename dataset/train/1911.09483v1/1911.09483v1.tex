
\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{tablefootnote}
\usepackage{color}
\iclrfinaltrue
\usepackage{pgfplotstable} \usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand{\mamark}[1]{\textcolor{orange}{{#1}}}
\newcommand{\macomment}[1]{\textcolor{orange}{{*MA*: #1}}}

\makeatletter
\newcommand{\ssymbol}[1]{}
\makeatother

\title{MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning}



\author{Guangxiang Zhao\ssymbol{2}, Xu Sun\ssymbol{2}\ssymbol{3}\thanks{ Corresponding author} , Jingjing Xu\ssymbol{3}, Zhiyuan Zhang\ssymbol{3}, Liangchen Luo\ssymbol{3} \\
\ssymbol{2}Center for Data Science, Peking University\\
\ssymbol{3}MOE Key Lab of Computational Linguistics, School of EECS, Peking University\\
\texttt{\{zhaoguangxiang,xusun,jingjingxu,zzy1210,luolc\}@pku.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle



\begin{abstract}


In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to over-concentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel \textbf{ MU}lti-\textbf{S}cale att\textbf{E}ntion (\textbf{MUSE}) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention,  and pointwise transformation. MUSE builds on MUSE-simple and explores  combining convolution and self-attention for learning sequence representations from more different scales.
We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism.
Code will be available at \url{https://github.com/lancopku/MUSE}.









\end{abstract}
\section{Introduction}
In recent years, Transformer has been remarkably adept at sequence learning tasks like machine translation~\citep{vaswani2017attention,dehghani2018universal}, 
text classification~\citep{devlin2018bert,yang2019xlnet}, language modeling~\citep{sukhbaatar2019augmenting,Dai_2019}, etc. It is solely based on an attention mechanism that captures global dependencies between input tokens, dispensing with recurrence and convolutions entirely. The key idea of the self-attention mechanism is updating token representations based on a weighted sum of all input representations.  

However, recent research~\citep{DBLP:conf/emnlp/TangMRS18} has shown that the Transformer has surprising shortcomings in long sequence learning, exactly because of its use of self-attention. As shown in Figure 1 (a), in the task of machine translation, the performance of Transformer drops with the increase of the source sentence length, especially for long sequences. The reason is that the attention can be over-concentrated and disperse, as shown in Figure 1 (b), and only a small number of tokens are represented by attention. It may work fine for shorter sequences, but for longer sequences, it  causes insufficient representation of information and brings difficulty for the model to comprehend the source information intactly. In recent work, local attention that constrains the attention to focus on only part of the sequences \citep{child2019generating,Sukhbaatar_2019} is used to address this problem. However, it costs self-attention the ability to capture long-range dependencies and also does not demonstrate effectiveness in sequence to sequence learning tasks.




\begin{figure}[ht]
\centering
\includegraphics[width=0.46\textwidth]{figures/transformer-bleu-length.pdf} \qquad
\includegraphics[width=0.48\textwidth]{figures/self_attention.pdf}
\caption{The left figure shows that the performance drops largely with the increase of sentence length on the De-En dataset. The right figure shows the attention map from the -th encoder layer. As we can see, the attention map is too dispersed to capture sufficient information. For example, ``[EOS]'', contributing little to word alignment, is surprisingly over attended.}
\label{fig:motivation}
\end{figure}

To build a module with both inductive bias of local and global context modelling in sequence to sequence learning, we hybrid self-attention with convolution and present Parallel multi-scale attention called MUSE.  It  encodes inputs into hidden representations and then  applies self-attention  and  depth-separable convolution transformations in parallel. The convolution compensates for the insufficient use of local information while the self-attention focuses on capturing the dependencies. Moreover, this parallel structure is highly extensible, and new transformations can be easily introduced as new parallel branches, and is also favourable to parallel computation.




The main contributions are summarized as follows:
\begin{itemize}

\item We find that the attention mechanism alone suffers from dispersed weights and is not suitable for long sequence representation learning. The proposed method tries to address this problem and achieves much better performance on generating long sequence.

\item We propose a parallel multi-scale attention and explore a simple but efficient method to successfully combine convolution with self-attention all in one module. 


\item MUSE outperforms all previous models with same training data and the comparable model size, with state-of-the-art BLEU scores on three main machine translation tasks. 

\item MUSE-simple introduce parallel representation learning  and brings expansibility and parallelism. Experiments show that the inference speed can be increased by 31\% on GPUs.
\end{itemize}









































\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/model/model.pdf}
\caption{Multi-scale attention hybrids point-wise  transformation, convolution, and self-attention to learn multi-scale sequence representations in parallel. We project convolution and self-attention into the same space to learn contextual representations.}
\label{fig:ffn_conv}
\end{figure}

\section{MUSE: Parallel Multi-Scale Attention}






Like other sequence-to-sequence models, MUSE also adopts an encoder-decoder framework. The encoder takes a sequence of word embeddings  as input where  is the length of input. It transfers word embeddings to a sequence of hidden representation . Given , the decoder is responsible for generating a sequence of text  token by token. 

The encoder is a stack of   MUSE modules. Residual mechanism and layer normalization are used to connect two adjacent layers.  
The decoder is similar to encoder, except that each MUSE module in the decoder not only captures features from the generated text representations  but also performs attention over the output of the encoder stack through additional context attention. Residual mechanism and layer normalization are also used to connect two modules and two adjacent layers. 



The key part in the proposed model is the MUSE module, which contains three main parts: self-attention for capturing global features, depth-wise separable convolution for capturing local features, and a position-wise feed-forward network for capturing token features. The module takes the output of  layer as input and generates the output representation in a fusion way:

where  ``Attention'' refers to self-attention, ``Conv'' refers to dynamic convolution, ``Pointwise'' refers to a position-wise feed-forward network. The followings list the details of each part. 

We also propose MUSE-simple, a simple version of MUSE, which generates the output representation similar to the MUSE model except for that it dose not the include convolution operation:


\subsection{Attention Mechanism for Global Context Representation}


Self-attention is responsible for  learning representations of global context.
 For a given input sequence , it first projects  into three representations, key , query , and value . Then, it uses a self-attention mechanism to get the output representation:

Where , , , and  are projection parameters. The self-attention operation  is the dot-production between key, query, and value pairs:


Note that we conduct a projecting operation over the value in our self-attention mechanism  here.

\subsection{Convolution for Local Context Modeling} 
We introduce convolution operations into MUSE to capture local context. To learn contextual sequence representations in the same hidden space, we choose depth-wise convolution~\citep{Chollet_2017} (we denote it as DepthConv in the experiments) as the convolution operation because it includes two separate transformations, namely, point-wise projecting transformation and contextual transformation. It is because that original convolution operator is not separable, but DepthConv can share the same point-wise projecting transformation with  self-attention mechanism. We choose dynamic convolution~\citep{wu2019pay}, the best variant of DepthConv, as our implementation. 

Each convolution sub-module contains multiple cells with different kernel sizes. They are used for capturing different-range features. The output of the convolution cell with kernel size  is:

where  and  are parameters,  is a point-wise projecting transformation matrix. The  refers to depth convolution in the work of  \citet{wu2019pay}. For an input sequence , the output  is computed as: 

where  is the hidden size.
Note that we conduct the same projecting operation over the input in our convolution mechanism  here with that in self-attention mechanism. 

\textbf{Shared projection}
To learn contextual sequence representations in the same hidden space, the projection in the self-attention mechanism  and that in the convolution mechanism  is shared. Because the shared projection can project the input feature into the same hidden space. If we conduct two independent projection here:  and , where  and  are two parameter matrices, we call it as separate projection. We will analyze the necessity of applying shared projection here instead of separate projection.

\textbf{Dynamically Selected Convolution Kernels}
 We introduce a gating mechanism to automatically select the weight of different convolution cells. 





\subsection{Point-wise Feed-forward Network for Capturing Token Representations} To learn  token level representations, MUSE concatenates an self-attention network  with a position-wise feed-forward network at each layer.  Since the linear transformations are the same across different positions, the position-wise feed-forward network can be seen as a token feature extractor. 
where , , , and  are projection parameters.

















\section{Experiment}
We evaluate MUSE on four machine translation tasks. This section describes the datasets, experimental settings, detailed results, and analysis. \subsection{Datasets}
\textbf{WMT14 En-Fr and En-De datasets} The WMT 2014 English-French translation dataset, consisting of   sentence pairs, is adopted as a big dataset to test our model. We use the standard split of development set and test set. We use \textit{newstest2014} as the test set and use \textit{newstest2012} +\textit{newstest2013} as the development set. Following \citet{gehring2017convolutional}, we also adopt a joint source and target BPE factorization with the vocabulary size of .
For medium dataset, we borrow the setup of \citet{vaswani2017attention} and adopt the WMT 2014 English-German translation dataset which consists of  sentence pairs, the BPE vocabulary size is set to . The test and validation  datasets we used are the same as \citet{vaswani2017attention}.

\textbf{IWSLT De-En and En-Vi datasets} Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of  sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of . The IWSLT 2015 English-Vietnamese translation dataset consists of  training sentence pairs. For the En-Vi task, we build a dictionary including all source and target tokens. The vocabulary size for English is , and the vocabulary size for the Vietnamese is . 



\subsection{Experimental Settings}
\subsubsection{Model}
For fair comparisons, we only compare models reported with the comparable model size and the same training data. We do not compare \citet{Wu_2019} because it is an  ensemble method. We build MUSE-base and MUSE-large with the parameter size  comparable to Transformer-base and Transformer-large. We adopt multi-head attention~\citep{vaswani2017attention} as implementation of self-attention in MUSE module. The number of attention head is set to 4 for MUSE-base and 16 for MUSE-large. We also add the network architecture built by MUSE-simple  in the similar way into the comparison.

MUSE consists of 12 residual blocks for encoder and 12 residual blocks for decoder, the dimension is set to 384 for MUSE-base and 768 for MUSE-large. The hidden dimension of non linear transformation is set to 768 for MUSE-base and 3072 for MUSE-large.



The MUSE-large is trained on 4 Titan RTX GPUs while the MUSE-base is trained on a single NVIDIA RTX 2080Ti GPU. The batch size  is calculated at the token level, which is called dynamic batching~\citep{vaswani2017attention}. 
We adopt dynamic convolution as the variant of depth-wise separable convolution. We tune the kernel size on the validation set. For convolution with a single kernel, we use the kernel size of  for all layers. In case of dynamic selected kernels, the kernel size is  for small kernels and  for large kernels for all layers. 


\subsubsection{Training}
The training hyper-parameters are tuned on the validation set. 

\textbf{MUSE-large}
For training MUSE-large, following \citet{ott2018scaling}, parameters are updated every  steps. We train the model for  updates with a batch size of  for En-Fr, and train the model for  updates with a batch size of  for En-De. The dropout rate is set to  for En-Fr and  for En-De. We borrow the  setup of optimizer from \citet{wu2019pay} and use the cosine learning rate schedule with  warmup steps. The max learning rate is set to  on  En-De translation and  on En-Fr translation. For checkpoint averaging, following \citet{wu2019pay}, we tune the average checkpoints for En-De translation tasks. For En-Fr translation, we do not average checkpoint but use the final single checkpoint.


\textbf{MUSE-base}
We train and test MUSE-base on two small datasets, IWSLT 2014 De-En translation and  IWSLT2015 En-Vi translation.  Following \citet{vaswani2017attention}, we use Adam optimizer with a learning rate of . We use the warmup mechanism and invert the learning rate decay with warmup updates of . 
For the De-En dataset, we train the model for  steps with a batch size of . The parameters are updated every  steps. The dropout rate is set to . For the En-Vi dataset, we train the model for  steps with a batch size of . The parameters are  also updated every  steps. The dropout rate is set to .
We  save checkpoints every epoch and average the last  checkpoints for inference.

\subsubsection{Evaluation}
During inference, we adopt beam search with a beam size of  for De-En, En-Fr and  En-Vi translation tasks. The length penalty is set to 0.8 for En-Fr according to  the validation results, 1 for the two small datasets following the default setting of \cite{ott2019fairseq}. We do not tune beam width and length penalty but use the setting reported in \citet{vaswani2017attention}.
The BLEU\footnote{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl} metric is adopted to evaluate the model performance during evaluation. 

\begin{table*}
\centering
\begin{tabular}{lcr}
\toprule
Model   & En-De & En-Fr \\
\midrule
ConvSeq2seq~\citep{gehring2017convolutional}  & 25.2  & 40.5 \\
SliceNet~\citep{kaiser2017depthwise} & 26.1 &- \\
Transformer~\citep{vaswani2017attention}  &28.4  & 41.0 \\
Weighted Transformer~\citep{ahmed2017weighted}  & 28.9 & 41.4 \\
Layer-wise Coordination~\citep{NIPS2018_8019}  & 29.1 & - \\
Transformer (relative position)~\citep{Shaw_2018}   & 29.2 & 41.5 \\
Transformer~\citep{ott2018scaling} & 29.3 & 43.2 \\
Evolved Transformer~\citep{so2019evolved} &  29.8 & 41.3 \\
DynamicConv~\citep{wu2019pay}  & 29.7 & 43.2 \\

Local Joint Self-attention~\citep{fonollosa2019joint} &29.7  & 43.3 \\
\midrule
\textbf{MUSE-simple} & 29.8 & 43.2 \\
\textbf{MUSE}  & \textbf{29.9} & \textbf{43.5} \\
\bottomrule
\end{tabular}
\caption{MUSE-large outperforms all previous models under the standard training and evaluation setting on WMT14 En-De and WMT14 En-Fr datasets.}
\label{tab:enfr}
\vspace{-0.2cm}
\end{table*}

\begin{table}[t]
\centering
\begin{tabular}{lrrr}
\toprule
Model  & En-Vi & De-En  \\
\midrule
NBMT~\citep{NBMT} & 28.1 & 30.1  \\
SACT~\citep{SACT} & 29.1 &-  \\
NP2MT~\citep{feng2018neural} &30.6 & 31.7 \\ Fixup~\citep{zhang2019fixup}  & -  & 34.5\\
DynamicConv~\citep{wu2019pay}  & - & 35.2 \\
Macaron~\citep{lu2019understanding}  & - &35.4 \\ 
Local Joint Self-attention~\citep{fonollosa2019joint}  &- &35.7 \\
\midrule
\textbf{MUSE-simple} & 30.7 & 35.8 \\
\textbf{MUSE} &\textbf{31.3} &\textbf{36.3}\\
\bottomrule
\end{tabular}
\caption{MUSE-base outperforms previous state-of-the-art models on IWSLT  De-En translation datasets and outperforms previous models without BPE processing on IWSLT  En-Vi. }
\label{tab:mtsmall}
\end{table}


\subsection{Results}


As shown in Table~\ref{tab:enfr}, MUSE outperforms all previously  models on En-De and En-Fr translation, including both  state-of-the-art models of stand alone self-attention~\citep{vaswani2017attention,ott2018scaling}, and convolutional models~\citep{gehring2017convolutional,kaiser2017depthwise,wu2019pay}. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.

Compared to Evolved Transformer \citep{so2019evolved} which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation.

Relative position or local attention constraints bring improvements over origin self-attention model, but parallel multi-scale outperforms them.

MUSE can also scale to small model and small datasets, as depicted in Table~\ref{tab:mtsmall}, MUSE-base pushes the state-of-the-art from 35.7 to 36.3 on IWSLT De-En translation dataset.

It is shown in Table~\ref{tab:enfr} and Table~\ref{tab:mtsmall} that MUSE-simple which contains the  basic idea of parallel multi-scale attention achieves state-of-the-art performance on three major machine translation datasets.









\begin{table*}[t]
\centering
\begin{tabular}{lr}
\toprule
Model & BLEU \\
\midrule
MUSE & 36.3\\
MUSE-simple (without DepthConv) & 35.8 \\ \midrule
substitute DepthConv with Convolution (k=3) & 35.2 \\  
substitute DepthConv with Convolution (k=5) & 35.0 \\  
substitute DepthConv with Convolution (k=7) & 34.5 \\  \midrule
DepthConv without shared projection &34.9 \\
DepthConv single kernel (k=3) & 36.2 \\  
DepthConv single kernel (k=7) & 36.2 \\ 
DepthConv single kernel (k=15) & 36.0 \\  
DepthConv single kernel (k=31) & 35.8 \\
DepthConv single kernel (grow kernels among layers:3,7,15,31) & 35.9 \\
DepthConv dynamically selected kernel (k=3,15) & 36.3 \\
\bottomrule
\end{tabular}
\caption{Comparisons between  MUSE and its variants on the IWSLT 2015 De-En translation task.}
\label{tab:ablaton}
\end{table*}
\subsection{How do we propose effective parallel multi-scale attention?}
In this subsection we compare MUSE and its variants on  IWSLT 2015 De-En translation  to answer the question.
 
\textbf{Does concatenating self-attention with convolution certainly improve the model?}
To bridge the gap between point-wise transformation which learns token level representations and self-attention which learns representations of global context, we introduce convolution to enhance our multi-scale attention. As we can see from the  first experiment group of Table~\ref{tab:ablaton},   convolution is important in the parallel multi-scale attention. 
However, it is not easy to combine convolution and self-attention in one module to build better representations on sequence to sequence tasks. As  shown in the first line of both second and third group of Table~\ref{tab:ablaton}, simply learning local representations by using convolution or depth-wise separable convolution in parallel with self-attention  harms the performance. Furthermore, combining depth-wise separable convolution (in this work we choose its best variant dynamic convolution as implementation) is even worse than  combining convolution.

\textbf{Why do we choose DepthConv and what is the importance of sharing Projection of DepthConv and self-attention?}
We conjecture that convolution and self-attention both learn contextual sequence representations and they should share the point  transformation and perform the contextual transformation in the same hidden space. 
We first project  the input to a hidden representation  and perform a variant of  depth-wise convolution and self-attention transformations in parallel.  
The fist two experiments in third group of Table~\ref{tab:ablaton} show that  validating the utility of sharing Projection in parallel multi-scale attention, shared projection gain 1.4 BLEU scores over separate projection, and bring improvement of 0.5 BLEU scores over MUSE-simple (without DepthConv).

\textbf{How much is the kernel size?}
Comparative experiments show that the too large kernel harms performance  both for DepthConv and convolution. Since there exists self-attention and point-wise transformations, simply applying the growing kernel size schedule proposed in SliceNet~\citep{kaiser2017depthwise}  doesn't  work. Thus, we propose to use dynamically selected kernel size to let the learned network decide the kernel size for each layer.
































\subsection{Further Analysis}

\paragraph{Parallel multi-scale attention brings time efficiency on GPUs}

The underlying parallel structure (compared to the sequential structure in each block of Transformer) allows MUSE to be efficiently computed on GPUs. For example, we can combine small matrices into large matrices, and while it does not reduce the number of actual operations, it can be better paralleled by GPUs to speed up computation.  Concretely, for each MUSE module,  we  first concentrate  of self-attention and  of point feed-forward transformation into a single encoder matrix , and then perform transformation such as self-attention, depth-separable convolution, and nonlinear transformation, in parallel, to learn multi-scale representations in the hidden layer.  can also be combined a single decoder matrix  . The decoder of sequence to sequence architecture can be implemented similarly.

In Table~\ref{tab:speed}, we conduct comparisons to show the speed gains with the aforementioned implementation, and the batch size is set to one sample per batch to simulate online inference environment. Under the settings, where the numbers of parameters are similar for MUSE and Transformer, about 31\% increase in inference speed can be obtained. The experiments use MUSE with  MUSE-simple modules and Transformer with  base blocks. The hidden size is set to . 



\begin{table*}[ht]
\centering
\begin{tabular}{lr}
\toprule
Model  & Inference Speed (tokens/s) \\
\midrule
Transformer  &  132 \\
MUSE  & 173 \\ \midrule
Acceleration & 31\% \\
\bottomrule
\end{tabular}
\caption{The comparison between the inference speed  of  MUSE and Transformer.}
\label{tab:speed}
\end{table*}

\textbf{Parallel multi-scale attention generates  much better long sequence}
As demonstrated in Figure~\ref{fig:bleu-length}, MUSE generates better  sequences of various length than self-attention, but it is remarkably adept at generate long sequence, e.g. for sequence longer than 100, MUSE is two times better.

\textbf{Lower layers prefer local context and higher layers prefer more contextual representations}
MUSE contains multiple dynamic convolution cells, whose streams are fused by a gated mechanism. The weight for each dynamic cell is a scalar. Here we analyze the weight of different dynamic convolution cells in different layers.  
Figure~\ref{fig:layer-k} shows that as the layer depth increases, the weight of dynamic convolution cells with small kernel sizes gradually decreases. It demonstrates that lower layers prefer local features while higher layers prefer global features. It is corresponding to the finding in \citet{ramach2019standalone}.



\begin{figure}[ht]
\centering
\begin{minipage}[]{0.46\linewidth}  
\includegraphics[width=\textwidth]{figures/deen-bleu-length.pdf}
\caption{BLEU scores of models on different groups with different source sentence lengths. The experiments are conducted on the De-En dataset. MUSE performs better than Transformer, especially on long sentences.}
\label{fig:bleu-length}
\end{minipage}
\qquad
\begin{minipage}[]{0.46\linewidth} 
\includegraphics[width=\textwidth]{figures/cnn.pdf}
\caption{Dynamically selected kernels at each layer: The blue bars represent the ratio between the percentage of the  convolution with  smaller kernel sizes and the percentage of the convolution with large kernel sizes.}
\label{fig:layer-k}
\end{minipage}
\end{figure}



\textbf{MUSE not only gains BLEU scores, but also generates more reasonable sentences and increases the translation quality.}
We conduct the case study on the De-En dataset and the cases are shown in Table~\ref{tab:case_study} in Appendix.  In case 1, although the baseline transformer translates many correct words according to the source sentence, the translated sentence is not fluent at all.  It indicates that Transformer does not capture the relationship between some words and their neighbors, such as ``right'' and ``clap''. By contrast, MUSE captures them well by combining local convolution with global self-attention. In case 2, the cause adverbial clause is correctly translated by MUSE while transformer misses the word ``why'' and fails to translate it.











\section{Related Work}
Sequence to sequence learning is an important task in machine learning. It evolves understanding and generating sequence. Machine translation is the touchstone of sequence to sequence learning. Traditional approaches usually adopt long-short term memory networks~\citep{sutskever2014sequence,ma-etal-2018-bag}  to learn the representation of sequences. However, these models either are built upon auto-regressive structures requiring longer encoding time or perform worse on real-world natural language processing tasks.
Recent studies  explore convolutional neural networks (CNN)~\citep{gehring2017convolutional} or self-attention ~\citep{vaswani2017attention} to  support high-parallel sequence modeling and does not require auto-regressive structure during encoding, thus bringing large efficiency improvements. They are strong at capturing local or global dependencies.



There are several studies on combining self-attention and convolution. However, they do not surpass both convectional and self-attention mechanisms.  \citet{sukhbaatar2019augmenting} propose to augment convolution with self attention by directly concentrating them in computer vision tasks.  However, as demonstrated in Table~\ref{tab:ablaton} there method does not work for sequence to sequence learning task. Since state-of-the-art models on question answering tasks still consist on self-attention and do no adopt ideas in QAnet~\citep{yu2018qanet}. Both self-attention~\citep{ott2018scaling} and convolution~\citep{wu2019pay} outperforms Evolved transformer by near 2 BLEU scores on En-Fr translation. It seems that learning global and local context through stacking self-attention and convolution  layers does not beat either self-attention or convolution models. In contrast, the proposed parallel multi-scale attention outperforms  previous convolution or self-attention  based models  on main translation tasks, showing its effectiveness for sequence to sequence learning.









\section{Conclusion and Future work}


Although the self-attention mechanism has been prevalent in sequence modeling, we find that attention suffers from dispersed weights especially for long sequences, resulting from the insufficient local information. 

To address this problem, we present Parallel Multi-scale Attention (MUSE) and MUSE-simple.  MUSE-simple introduces the idea of parallel multi-scale attention into sequence to sequence learning. And MUSE fuses self-attention, convolution, and point-wise transformation together to explicitly learn global, local and token level sequence representations. 
Especially, we find from empirical results that the shared projection plays important part in its success, and is essential for our multi-scale learning.

Beyond the inspiring new state-of-the-art results on three major machine translation datasets, detailed  analysis and model variants also verify the effectiveness of MUSE. 

For future work, the parallel structure is highly extensible and provide many opportunities to improve these models. In addition, given the success of shared projection, we would like to explore its detailed effects on contextual representation learning. Finally, we are exited about future of parallel multi-scale attention and plan to apply this simple but effective idea to other tasks including image and speech.
















































































































\subsubsection*{Acknowledgments}
This work was supported in part by National Natural Science Foundation of China (No. 61673028).


\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\newpage
\appendix
\section{Appendix}
\subsection{Case study}
\begin{table*}[ht]
\centering
\begin{tabular}{p{2cm}p{10cm}}
\toprule
\textbf{Case 1} & \\
\midrule
\textbf{Source} & wenn sie denken, dass die auf der linken seite jazz ist und die, auf der rechten seite swing ist, dann klatschen sie bitte. \\
\midrule
\textbf{Target} & if you think the one on the left is jazz and \textbf{\textcolor{blue}{the one on the right is swing, clap your hands.}} \\
\midrule
 \textbf{Transformer} & if you think it's jazz on the left, and \textbf{\textcolor{red}{those on the right side of the swing are clapping}}, please. \\
\midrule
\textbf{MUSE} & if you think the one on the left is jazz, \textbf{\textcolor{blue}{and the one on the right is swing, please clap.}} \\
\midrule\midrule
\textbf{Case 2} & \\
\midrule
\textbf{Source} & und deswegen haben wir uns entschlossen in berlin eine halle zu bauen, in der wir sozusagen die elektrischen verhältnisse der insel im maßstab eins zu drei ganz genau abbilden können. \\
\midrule
\textbf{Target} & and \textbf{\textcolor{blue}{that's why we decided}} to build a hall in berlin, where we could precisely reconstruct, so to speak, the electrical ratio of the island on a one to three scale. \\
\midrule
 \textbf{Transformer} & and so in berlin, \textbf{\textcolor{red}{we decided}} to build a hall where we could sort of map the electrical proportions of the island at scale one to three very precisely. \\
\midrule
\textbf{MUSE} & and \textbf{\textcolor{blue}{that's why we decided}} to build a hall in berlin, where we can sort of map the electric relationship of the island at the scale one to three very precisely. \\
\bottomrule
\end{tabular}
\caption{Case study on the De-En dataset. The red bolded words denote the \textbf{\textcolor{red}{wrong}} translation and blue bolded words denote the \textbf{\textcolor{blue}{correct}} translation. In case 1, transformer fails to capture the relationship between some words and their neighbors, such as ``right'' and ``clap''. In case 2, the cause adverbial clause is correctly translated by MUSE while transformer misses the word ``why'' and fails to translate it. }
\label{tab:case_study}
\end{table*}

\end{document}
