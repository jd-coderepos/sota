\documentclass[envcountsame]{llncs}


\usepackage{amsmath, amsfonts, amssymb}
\usepackage{xspace}
\usepackage{upgreek}
\usepackage{graphicx}
\usepackage[stable]{footmisc}
\usepackage{braket}














\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\Delta}{\Updelta}

\newcommand{\calA}{\ensuremath{\mathcal{A}}}
\newcommand{\calB}{\ensuremath{\mathcal{B}}}
\newcommand{\calC}{\ensuremath{\mathcal{C}}}
\newcommand{\calP}{\ensuremath{\mathcal{P}}}
\newcommand{\calR}{\ensuremath{\mathcal{R}}}

\newcommand{\TRUE}{\textsc{True}\xspace}
\newcommand{\FALSE}{\textsc{False}\xspace}

\newcommand{\indexing}{\textsc{Indexing}\xspace}
\newcommand{\equality}{\textsc{Equality}\xspace}
\newcommand{\disjointness}{\textsc{Disjointness}\xspace}

\newcommand{\SigmaP}{\ensuremath{\Sigma_{\textup{P}}}}
\newcommand{\SigmaT}{\ensuremath{\Sigma_{\textup{T}}}}
\newcommand{\local}{\ensuremath{\textsc{LocalPM}}}

\newcommand{\wildcard}{\ensuremath{\star}}

\newcommand{\upto}{\ensuremath{\ldots}}

\newcommand{\pregion}{\mbox{p-region}\xspace}
\newcommand{\pregions}{\mbox{p-regions}\xspace}
\newcommand{\prepresentation}{\mbox{p-representation}\xspace}

\newcommand{\LCE}{\ensuremath{\textup{LCE}}}
\newcommand{\LCEP}{\ensuremath{\text{LCE}_\text{p}}\xspace}

\newcommand{\commentbox}[1]{
    \begin{center}
\fbox{
                \begin{minipage}[t]{1.0\textwidth}
                    #1
                \end{minipage}
            }
\end{center}
}

\newcommand{\margin}[1]{\marginpar{\hspace{3mm}\parbox{2.5cm}{\scriptsize #1}}}





\title{Pattern Matching in Multiple Streams\footnote{This work was partially supported by EPSRC.}}
\author{
    Rapha\"{e}l Clifford\inst{1} \and
    Markus Jalsenius\inst{1} \and
    Ely Porat\inst{2} \and
    Benjamin Sach\inst{3}}
\institute{
    University of Bristol, Department of Computer Science, Bristol, UK \and
    Bar-Ilan University, Department of Computer Science, Ramat-Gan, Israel \and
    University of Warwick, Department of Computer Science, Coventry, UK}




\pagestyle{plain}

\begin{document}

\maketitle




\begin{abstract}

    We investigate the problem of deterministic pattern matching in multiple
    streams. In this model, one symbol arrives at a time and is associated with one of 
    streaming texts. The task at each time step is to report if there is a new match between a
    fixed pattern of length  and a newly updated stream.  As is
    usual in the streaming context, the goal is to use as little space
    as possible while still reporting matches quickly.  We
    give almost matching upper and lower space bounds for three distinct
    pattern matching problems.  For exact matching we
    show that the problem can be solved in constant time per arriving symbol and  words of space.  For the
    -mismatch and -difference problems we give  time solutions that require  words of space. In all three cases we also give space lower bounds which show our methods are optimal up to a single logarithmic factor.  Finally we set out a number of open problems related to this new model for pattern matching.
\end{abstract}


\thispagestyle{plain}

\section{Introduction}

We introduce a new set of problems centered on pattern matching in multiple streaming
texts. In this model, one symbol arrives at a time and is added to the tail
of exactly one of  streaming texts.   The task at each time step is to
report if there is a new match between a fixed pattern  of length
 and a newly updated stream.  Our interest is in deterministic
algorithms with guaranteed worst case time complexity.  The goal is to
use as little space as possible while still reporting matches quickly.

The problem of pattern matching in a single stream using limited space had
a major breakthrough in 2009 when it was shown that exact
matching can be performed using only  words of space and
 time~\cite{Porat:09}. This result was subsequently simplified~\cite{EJS:2010}
and then improved~\cite{BG:2011} to run in constant time per new
symbol.   To achieve this small space, these methods are however all
necessarily randomised and allow some small probability of
error. Where neither randomisation nor error is permitted, a
straightforward argument shows us that there is no
hope of using space sublinear in the pattern size.  This follows
directly from the observation that we could use such a matching
algorithm to reproduce the pattern in its entirety and that it
therefore must use at least linear space.

Where there are multiple streams however the situation is not as clear cut
even where no randomisation is allowed.  A naive approach would simply
be to store 
copies of the working space, one for each stream. This will typically then require 
space overall. Where the number of streams  is large, the space
usage of such an approach is therefore likely to be prohibitive.

Our contribution is first to show that for three particularly common pattern matching
problems, pattern matching in multiple streams can be solved in
considerably less than  space without the use of randomisation.
In Section~\ref{sec:exact} we give a constant time algorithm for exact matching that requires
only  words of space.
In Section~\ref{sec:lce} we review a recently introduced data structure which allows us to perform longest common extension (LCE) queries between a streaming text and a fixed pattern in constant time and  space.  This data structure is then used in Sections~\ref{sec:k-mismatch}
and~\ref{sec:k-diff}  where we give  time and  space solutions to the -mismatch and
-difference problems.  In Section~\ref{sec:space} we
give almost matching space lower bounds for our problems as well as lower bounds for some other common pattern matching problems. Finally in
Section~\ref{sec:open} we set out a number of open problems that immediately arise for this new model of pattern matching.






\section{Related work}

Randomised space lower bounds for a wide range of pattern matching
problems in a single stream were given in~\cite{CJPS:2011}.
In~\cite{CEPP:2011,CS:2011} it was also shown that a large set of
pattern matching algorithms could be converted from offline to online
with only at worst a multiplicative logarithmic factor overhead in
their time complexity. This therefore provided an effective
deamortisation of almost the entire field of combinatorial pattern
matching and a ready tool for the construction of fast streaming
pattern matching algorithms.

In the more usual offline setting, a great deal of progress has been made in finding fast  algorithms for
a variety of approximate matching problems. One of the most
studied is the Hamming distance which measures the number of
single character mismatches between two strings.  Given a text of length  and a
pattern of length , the task is to report the Hamming distance at
every possible alignment.   Solutions running in  time which are based
on repeated applications of the FFT were given independently by both
Abrahamson and Kosaraju in 1987~\cite{Abrahamson:1987,Kosaraju:1987}.
Particular interest has been paid to the bounded variant we also
consider called the
-mismatch problem.  Here a bound  is given and we need only
report the Hamming distance if it is less than or equal to .  If  the number of mismatches is greater than the bound, the algorithm need only report that fact and not give the actual Hamming distance. In 1986 Landau and Vishkin gave a  beautiful  time algorithm that is not FFT based and uses constant time
lowest common ancestor (LCA) operations on the suffix tree of the pattern and
text~\cite{LV:1986a}. This was subsequently  improved to  time by a method based on filtering and FFTs again~\cite{ALP:2004}.  A separate line of research considered
the question of how to find approximations within a  multiplicative factor of the Hamming distance~\cite{Indyk:1998,Karloff:1993}. The edit distance measures the minimum number of single character insert, delete and mismatch operations required to transform one string into another. We  consider a bounded problem called -difference which reports the edit distance at every alignment only if it is at most . An  solution to this problem was given in~\cite{LV:1988} using LCE queries to perform jumps within the dynamic programming table.




\subsection{Preliminaries and a new model for multiple streams}

In order to design algorithms for pattern matching in multiple streams
we first define a new computational model which we will use throughout. In this new model there is only one stream, however
we will carefully distinguish between space that any pattern matching
algorithm uses that is associated with the pattern and
space associated with the text.  We call this model, which may be of
independent interest, the \emph{single stream read-only
  preprocessing model} or simply the \emph{read-only preprocessing model} for short.

Any algorithm that operates in this model operates in two phases: a
preprocessing phase followed by an online phase. During the
preprocessing phase, the algorithm processes the pattern without any
knowledge of the text. The output of the preprocessing phase is termed
the \emph{pattern~space}.   The online phase
is provided read-only access to a copy of the pattern space. It is
important to note that the online phase is not provided direct access
to the pattern unless the pattern is explicitly stored in the pattern
space. The online phase continues as in the original single stream model, processing each text character as it arrives. Any space used by the online phase in addition to the read-only pattern space is termed \emph{text~space}. The text space can therefore only store information that is associated with the text and its relation to information already stored in the pattern space.

Algorithms developed in the read-only preprocessing model translate directly to the
multiple stream model that we are interested in. Consider an algorithm
in the read-only preprocessing model. Let  be the
time taken per new arriving symbol,  the pattern space
and  the text space of this algorithm. Since the pattern
space is independent of the text, it can be shared across multiple
texts. Therefore we can directly derive a new pattern matching
algorithm in the multiple stream model which runs in  time per
character using  space.  The space and
time requirements of the preprocessing stage are arguably of less
significance given that they are a function only of the pattern size.
Nevertheless, for all three pattern matching problems we consider,
the preprocessing stage can be implemented in  space and  time.
We assume throughout that all computation is performed in the unit cost RAM model.




\section{Exact matching}\label{sec:exact}

We begin by giving a real-time variant of the KMP algorithm. We will apply it to the read-only preprocessing model ensuring it takes
 time, using  pattern space and  text space,
so that in the multiple stream model it will take  time and use  space.
The more usual real-time variant provided by Galil~\cite{Galil:1981} will not suffice for our purposes as it buffers the text and therefore would use  space in the multiple stream model.


First recall that at each stage of the standard KMP algorithm we keep track of the longest prefix of the pattern that matches a suffix of the text seen so far. When a new character  arrives, we extend the matching prefix by  if possible, otherwise we shift the pattern according to a pre-computed prefix table, also known as the failure function. More precisely, suppose that  matches  when  arrives. If , we extend the match, otherwise we look at position  of the prefix table, which gives us the largest value  such that  matches  and . Then we compare  with  to see if we can extend the new match by . If not, we shift the pattern again using the prefix table, and so on.

While it is well-known that the time complexity per
character is  amortised, our motivations call for an unamortised
solution. Instead of using a prefix table, for each position  of the pattern we store a dictionary  that contains every pair  where  is the largest index such that  matches  and .
The pairs  in  are indexed by the symbol  and  denotes the alphabet.
In other words, whenever a match  cannot be extended to  (i.e.,~), instead of repeatedly shifting the pattern according to the prefix table until  is matched, we look up symbol  in  and immediately get the length  of the prefix of  that the KMP algorithm would eventually align with  in order to be able to extend the match to . The dictionaries  can be pre-computed as all shifts are based on self-matches with the pattern itself. By using a static perfect dictionary we can do lookups in constant time. Thus, a KMP algorithm equipped with
these dictionaries instead of a prefix table will run in unamortised  time per character.
An interesting fact is that the total space usage to store these dictionaries is only . The following lemma was proved in~\cite{Simon:1993}, where it was stated in the language of finite automata. For completeness we include a proof.

\begin{lemma}[Theorem 1 of~\cite{Simon:1993}]
    \label{lem:shifts}
    The sum of the sizes of all dictionaries, 
\end{lemma}
\begin{proof}
    A lookup in the dictionary  results in a shift of the pattern. We show that for every length  there is at most one element over all dictionaries that moves the pattern along by  positions. For contradiction, suppose that some  and , where , both shift the pattern by . By definition, we have . If the same shift  is applied when the -length prefix of  is moved along, we must have , which leads to a contradiction.
    \qed
\end{proof}

By using static perfect hashing, we can store all dictionaries  in  pattern space. Although preprocessing time is not our focus, we briefly discuss how to construct the dictionaries in  time.

We begin by constructing the standard KMP prefix table in  time. We then construct each dictionary  by considering  in increasing order, starting with . For any , the dictionary  is constructed as follows. Let  be the index given by the original KMP prefix function for . The elements in
 are added to . These are precisely the elements that belong to  according to its definition. Over all~, gathering the elements to be added to the dictionaries takes  time. The running time is therefore dominated by the time it takes to insert the elements into the dictionaries. By using the the static dictionary of Ru\v{z}i\'{c}~\cite{Ruzic:08}, construction takes

time, where Lemma~\ref{lem:shifts} has been applied.

The next lemma summarises the result, which together with the properties of the read-only preprocessing model gives us Theorem~\ref{thm:exact}.

\begin{lemma}
    Exact matching can be solved in the read-only preprocessing model in  time per character and using  pattern space and  text space.
\end{lemma}

\begin{theorem}
    \label{thm:exact}
    Exact matching in the multiple stream model with  texts can be solved in  time per character and  space.
\end{theorem}










\section{LCE queries in a stream}\label{sec:lce}

In preparation for the algorithms we give for the -mismatch and
-difference problems, we will be required to maintain a data
structure that allows us to
compute LCE queries in a streaming text in  time  and  space.  This method was first introduced
in~\cite{CS:2010}, although the idea of representing the text in terms of substrings of the pattern was first used in a different
setting in~\cite{ALLS:2007}. We provide a brief recap here.

We will split the streaming text into contiguous substrings which are encoded as triplets, , each representing an -length text substring  that equals a pattern substring . We refer to such a triple as \emph{\pregion} and a disjoint ordered sequence of triplets that encode the entire text as a \emph{\prepresentation}. For example, with  and , a \prepresentation of  is  The \prepresentation is not necessarily unique. We say that it is of minimal length if it contains a minimal number of triplets.

In~\cite{CS:2010} it was shown that we can extend a minimal length \prepresentation of  to a minimal length \prepresentation of  in  time when symbol  arrives. More precisely, the extended \prepresentation is obtained greedily by updating the last \pregion if possible, otherwise adding a new \pregion , where  is some position such that .
To accomplish this task in  time, a suffix tree of the pattern
can be used~\cite{CS:2010}. For our purposes in terms of pattern space
and text space, we may construct the suffix tree during the pattern
preprocessing phase and store it together with the pattern itself in
the pattern space. Deciding whether to update the last \pregion or add
a new one when a new symbol  arrives can then be done in
constant text space. For simplicity of explanation we will assume that
all symbols in  occur at least once~in~.  For further details of
the method and how to handle symbols which occur in the text but not
the pattern, we refer the interested reader to~\cite{CS:2010}.

As we will see shortly, a benefit of using a minimal length \prepresentation of  is that we can answer longest common extension (LCE) queries between the pattern and  in  time. We write  to denote the length of the longest prefix of  that is also a prefix of .
The following lemma was stated as Lemma~1 in~\cite{CS:2010}.

\begin{lemma}
    \label{lem:three-regions}
    For a minimal length \prepresentation of , at most three \pregions overlap , where  is the length returned by any  query.
\end{lemma}

It is well known that we can precompute a static data structure using
 space, denoted \LCEP, to support LCE queries between the
pattern and itself in  time. This is traditionally achieved with
a suffix tree on which lowest common ancestor queries are answered in
constant time. It now follows from Lemma~\ref{lem:three-regions} that
any  query on the streaming text can be answered in  time by performing at most three pattern-pattern LCE queries in the \LCEP structure.


\section{-mismatch in multiple streams}\label{sec:k-mismatch}

We described in Section~\ref{sec:lce} how a \prepresentation of the
text can be maintained in  time and  pattern space. The
actual \prepresentation of  will be stored in the text
space of size . Instead of storing every \pregion, we could
store only the most recent \pregions of the text. This representation
will of course only give us access to some suffix of the text seen so
far. Our algorithm for the -mismatch problem, which we present in the read-only preprocessing
model in the first instance, will store the most
recent  \pregions, requiring  text space.

In order to determine whether  has at most  mismatches with  when  arrives, we apply the kangaroo technique~\cite{LV:1985} consisting of up to  LCE queries between the text and the pattern. We will perform these LCE queries in the \emph{reverse} of  and , starting from the rightmost character . It should not be too hard to see that the data structures described in Section~\ref{sec:lce} can be modified to support reverse LCE queries between the pattern and the text with no effect on the asymptotic time and space complexities. Also, Lemma~\ref{lem:three-regions} holds for reverse LCE queries. In the next lemma we prove that all LCE queries performed by the algorithm fall within the  most recent \pregions.

\begin{lemma}
    \label{lem:k-mismatch}
    The -mismatch problem can be solved in the read-only
    preprocessing model in   time per character and using  pattern space and  text space.
\end{lemma}
\begin{proof}
    The time complexity follows immediately from the algorithm description, similarly with the pattern and text space complexities. For the correctness of the algorithm we need to show that none of the at most  reverse LCE queries performed to determine the number of mismatches between  and  fall outside the text substring represented by the  most recent \pregions.

    From Lemma~\ref{lem:three-regions} it follows that one LCE query could span three \pregions. As part of the kangaroo technique, we skip over a mismatch position between each LCE query. The mismatch could fall inside a new \pregion, hence no more than a total of  \pregions are involved in a series of up to  LCE queries.
    \qed
\end{proof}

From Lemma~\ref{lem:k-mismatch} and the properties of the read-only
preprocessing model, we immediately have the following theorem.

\begin{theorem}
    The -mismatch problem can be solved in the multiple stream model with  texts in  time per character and  space.
\end{theorem}



\section{-difference in multiple streams}\label{sec:k-diff}

Let  denote the minimum of all -bounded edit distances between the pattern prefix  and all suffixes of . For the -difference problem we want to output  as soon as  arrives. We have the standard dynamic programming recurrence,

where  if  and  otherwise. For the base cases we have  and  for all .

We now present a solution for the -difference problem, first in
the read-only preprocessing model and then give the final result in
the multiple stream model as required.  Whenever a text character  arrives such that  is a multiple of , we start a \emph{child process} which will be responsible for outputting  for all  as each such  arrives (Interval~2 in Fig.~\ref{fig:dynamic}). Therefore, there is a child process responsible for each and every output. The  text arrivals between  and the first output for a child process will, as explained below, give us enough time to prepare for the outputs. Observe that at most two child processes are running at any one time, hence we only need to show that one child process can be implemented in  time per character,   pattern space and  text space.
\begin{figure}[ht]
    \hskip 45pt \includegraphics[scale=1.0]{dynamic-diagram}
    \caption{\label{fig:dynamic} The dynamic programming table for -difference.}
\end{figure}

The operation of a child process is divided into three stages, each responsible for computing cells of the blocks denoted ,  and , respectively, of the dynamic programming table in~Fig.~\ref{fig:dynamic}. In the first stage, which runs during the -length text interval starting with the arrival of  (first half of Interval~1 in Fig.~\ref{fig:dynamic}), the child process will compute the cells marked  in Fig.~\ref{fig:dynamic}, that is the cells . To do this we use the technique of Landau-Vishkin~\cite{LV:1988} which is an offline algorithm that we run on block . Their algorithm takes  time and uses  space by operating along the diagonals of the dynamic programming table (shaded in dark gray in the figure). Their algorithm is based around LCE queries of the text, which we can take advantage of in a similar fashion to the -mismatch algorithm from the previous section. We will see below that we only need to store  \pregions of the text to perform the LCE queries, hence we can run their algorithm in  text space. The running time of  is spread evenly over  text arrivals, and therefore takes  time per character.

Once the first stage is completed we have obtained the values of the cells marked  in the figure. Now the second stage takes over. Here we compute the cells of block  of the dynamic programming table by direct use of the recurrence above. The work is spread evenly over the next  text arrivals (second half of Interval~1) by computing two columns of the block  per arrival. The final column, marked  in the figure, is therefore done by the arrival of . Thus, the second stage takes  time per character and uses only  space. We should mention that the values of cells on the boundary of  (excluding ) that are used in the recurrence when computing block  are all set to . This however does not affect the correctness of the computed values of .

In the final and third stage we compute all cell values of block~ by direct use of the recurrence, like we did for block  during the second stage. We use the values of  and set the other boundary cells to  (which does not affect the correctness of the final output). For each arriving character during Interval~2 in Fig.~\ref{fig:dynamic}, we compute the corresponding column of  block . Therefore the running time is  per character, and space usage is .

All steps of the child process described above, except for the real-time LCE processing, require only  space. During the pattern preprocessing phase, as for the -mismatch algorithm above, we will construct the required  structure and the suffix tree of the pattern, and store both these structures as well as a copy of the pattern, using a total of  pattern space.
We modify the LCE processing to store only the most recent  \pregions.
We will show that whenever the edit distance is  or less, storing the  most recent \pregions is sufficient to support every LCE query. Thus, if any LCE query stretches beyond these \pregions, the edit distance must be more than . The following lemma summarises the result. The result for the multiple stream model follows immediately from the above observation about the relationship between the models.



\begin{lemma}
    \label{lem:k-diff}
    The -difference problem can be solved in the read-only
    preprocessing model in   time per character using  pattern space and  text space.
\end{lemma}

\begin{proof}[sketch]
    The time and space complexities follow from inspection of the algorithm description.
For correctness, suppose there exists an  such that . Therefore, by the problem definition, there exists an  such that  can be transformed into  in at most  insert, delete and mismatch operations. We will first show that this immediately implies the existence of a \prepresentation of  containing at most  \pregions.

    Consider any transformation of  into  which contains at most  operations. We denote by  the -length array which states the `origin' of each character in :  if the transformation aligns  with  and , otherwise , which means that  is the result of an insert operation or is aligned with a symbol different from .

    We can construct a \prepresentation  of  by a single pass of  as follows.
    If , we begin by creating the \pregion . Otherwise, we create the region  where  is any index such that .
    For each , we consider three disjoint cases:

    \begin{enumerate}
        \item . Increase the length of the most recent region by one.
        \item . Create a new \pregion .
        \item . Create a new \pregion , where  is any index such that .
    \end{enumerate}







    We now consider the number of \pregions in the \prepresentation . An additional \pregion is only created when either case~2 or case~3 occurs in the construction. Case~3 occurs only when . However, by the definition of , each  corresponds  to being a result of a mismatch or insert operation of which there are at most  in total.
    Therefore case~3 occurs at most  times. Case~2 occurs when . By the definition of , this implies that either some character  with  was deleted or  which in turn implies that either a mismatch or insert operation occurred at . Hence case~2 can occur at most  times. The total number of \pregions is therefore upper bounded by  as a mismatch or insert can cause two new p-regions to be created, one at some  and another at .


    To see that  \pregions are enough, first observe that every LCE query performed ends in the substring .
    As  can be transformed into  in at most  moves, we have that . We also have that  and hence  is a substring of . We can then then convert the \prepresentation  of   into a \prepresentation of  by adding at most  \pregions of length one each. Thus,  \pregions suffice to support any LCE query.    \qed
\end{proof}




\begin{theorem}
    The -difference problem can be solved in the multiple stream model with  texts in  time per character and  space.
\end{theorem}


\section{Space lower bounds}\label{sec:space}

In this section we show that our space upper bounds for -mismatch
and -difference are optimal (up to a log factor). We also show
that for several other common distance measures, pattern matching in  streams requires  bits of space, implying that we may not do better than treating each stream independently.

The log sized gap between our lower bounds and upper bounds comes from
the fact that we state the lower bounds in bits whereas the upper
bounds are given in words. A smaller gap could be obtained by
considering large alphabets (see e.g.\@~\cite{CJPS:2011}), however for
simplicity we give our lower bounds assuming binary alphabets.

Our results are based on reductions from two one-way communication complexity problems with known lower bounds.
In a one-way communication model, only Alice can send messages to Bob and then Bob must output the correct answer.
In the \equality problem, Alice has a string  and Bob has a string . Bob must determine whether . The communication complexity is  bits~\cite{Kushilevitz:97}.
In the \indexing problem, Alice has a string  and Bob has an index . Bob must find . The problem is known to have an  bit lower bound~\cite{Kushilevitz:97}.


\begin{theorem}
    \label{thm:k-lower}
    The -mismatch and -difference problems in  streams both require  bits of space.
\end{theorem}
\begin{proof}
    First consider the case where . We reduce from the \equality problem, where Alice has a string  and Bob has a bit string .
    Let the pattern  be the string . Let  be any algorithm that solves either -mismatch or -difference on the pattern . Alice sends the internal state of  to Bob, who feeds the algorithm with the string  in one of the streams. The output is~0 if and only if , hence  bits of space is required.

    Now consider the case where . We reduce from the \indexing problem, where Alice has a string  and Bob has an index . Let  be any algorithm that solves either -mismatch or -difference on the pattern . Alice feeds each of the  streams with  bits from her string  such that the first stream is fed the first  bits of , the second stream is fed the next  bits of , and so on. Alice then sends the internal state of  to Bob who now wants to determine  which was fed into stream . Bob feeds the stream  with  0s, which ensures that  is aligned with the first position of . Let  be the output by  at this alignment and observe that for both -mismatch and -difference,  equals the number of 1s in the last  symbols of the stream . Bob now feeds another 0 into the stream . Let  be the new output by . It follows that  if and only if , hence  bits of space is required.    The space lower bound of  bits is obtained by combining the two cases.
    \qed
\end{proof}

\begin{theorem}
    Exact matching in  streams requires  bits of space.
\end{theorem}
\begin{proof}
    Following the proof of Theorem~\ref{thm:k-lower}, we reduce from the \equality problem if , otherwise we reduce from \indexing where Alice feeds each stream with either one 0 or one 1. By feeding  0s into any stream, Bob can determine any of the  bits.
    \qed
\end{proof}

We now turn our attention to the ,  and Hamming distance problems. For any constant , the  distance between two equal length strings  and  is given by .

\begin{theorem}
    Computing the ,  and Hamming distances, as well as the cross-correlation/convolution in  streams, requires  bits of space.
\end{theorem}
\begin{proof}
    We reduce from the \indexing problem, where Alice has a string  and Bob has an index . Let  be any algorithm that solves either of the problems in the statement of the lemma on the pattern , where instead of computing the
     distance, the square of the distance is computed. Observe that a lower bound for computing the square of the  distance is also a lower bound for the  distance. Each of the problems is now \emph{local} in the sense that the output is the sum of  position-wise values. Following the idea in the proof of Theorem~\ref{thm:k-lower}, Alice feeds each stream with  bits of her string  before sending the internal state to Bob. In order to determine , Bob feeds the appropriate stream  with enough 1s to align  with the first position of . By feeding another 1 into the stream  and comparing the two outputs, Bob can determine the value of .
    \qed
\end{proof}


\section{Open problems}\label{sec:open}

The space complexity of the results we give are tight to within a log factor when no randomisation is permitted. Further, the time complexity of both the exact matching and -difference algorithms we give match that of the fastest known offline algorithms per arriving symbol.  However, for -mismatch there remains a gap of approximately  between the fastest single stream algorithm~\cite{CS:2010} and the time complexity we give for multiple streams. There is an even more pronounced gap for the special case of constant sized alphabets when the bound  is relatively large. Here the -mismatch problem can be solved in a single stream in  time per symbol and  space~\cite{CEPP:2008}, independent of the value of . It would be interesting to consider whether there is a  time, multiple stream algorithm using only  space in this case.

We have seen that the read-only preprocessing model is a useful
conceptual tool for developing algorithms in the multiple stream
model. In particular we have used the fact that any efficient algorithm in the former model immediately gives an efficient algorithm in the latter. It is natural to wonder whether these models are in fact equivalent. We conjecture that for any  space algorithm for a pattern matching problem in the multiple stream model (where  do not depend on ), there is an  pattern space,  text space algorithm in the read-only preprocessing model with the same time complexity per character.


If we are concerned with
randomised computation where each output has to be correct with some
(arbitrarily large) constant probability, we can derive new space
lower bounds for all three problems with some modification. In particular, -mismatch and
-difference will require at least  bits of
space and exact matching   bits of space. These
lower bounds follow from the randomised counterpart of \equality and
\indexing. The randomised one-way communication complexity with private
randomness for \equality is  bits~\cite{Yao:79}, and
for \indexing it remains  bits (see~\cite{JKS:08} for an
elementary proof).  It is not yet clear whether these randomised lower bounds for
the multiple streams problem can be met by matching algorithmic upper bounds.



\bibliographystyle{plain}
\bibliography{multbib}





\end{document}
