\twocolumn[
\icmltitle{\titletext}







\begin{icmlauthorlist}
\icmlauthor{Joan Serr\`a}{tef}
\icmlauthor{D\'idac Sur\'is}{tef,upc}
\icmlauthor{Marius Miron}{tef,upf}
\icmlauthor{Alexandros Karatzoglou}{tef}
\end{icmlauthorlist}

\icmlaffiliation{tef}{Telef\'onica Research, Barcelona, Spain}
\icmlaffiliation{upc}{Universitat Polit\`ecnica de Catalunya, Barcelona, Spain}
\icmlaffiliation{upf}{Universitat Pompeu Fabra, Barcelona, Spain}

\icmlcorrespondingauthor{Joan Serr\`a}{joan.serra@telefonica.com}


\icmlkeywords{catastrophic forgetting, lifelong learning, hard attention, multitask}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  



\begin{abstract} 
Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45~to~80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.
\end{abstract} 



\section{Introduction}
\label{sec:Intro}

With the renewed interest in neural networks, old problems re-emerge, specially if the solution is still open. That is the case with the so-called catastrophic forgetting or catastrophic interference problem~\cite{McCloskey89PLM,Ratcliff90PR}. In essence, catastrophic forgetting corresponds to the tendency of a neural network to forget what it learned upon learning from new or different information. For instance, when a network is first trained to convergence on one task, and then trained on a second task, it forgets how to perform the first task. 


Overcoming catastrophic forgetting is an important step in the advancement towards more general artificial intelligence systems~\cite{Legg07MM}. Such systems should be able to seamlessly remember different tasks, and to learn them sequentially, following a lifelong learning paradigm~\cite{Thrun95RAS}. Apart from being more biologically plausible~\cite{Clegg98TCS}, there are many practical situations which require a sequential learning system~\citep[cf.][]{Thrun95RAS}. For instance, it may be unattainable for a robot to retrain from scratch its underlying model upon encountering a new object/task. After accumulating a large number of objects/tasks and their corresponding information, performing concurrent or multitask learning at scale may be too costly. 

Storing previous information and using it to retrain the model was among the earliest attempts to overcome catastrophic forgetting; a strategy named ``rehearsal''~\cite{Robins95CS}. The use of memory modules in this context has been a subject of research until today~\cite{Rebuffi2017CVPR,LopezPaz17NIPS}. However, due to efficiency and capacity constrains, memory-free approaches were also introduced, starting with what was termed as ``pseudo-rehearsal''~\cite{Robins95CS}. This approach has found some success in transfer learning situations where one needs to maintain a certain accuracy on the source task after learning the target task~\cite{Jung16ARXIV,Li17PAMI}. Within the pseudo-rehearsal category, we could also consider recent approaches that substitute the memory module by a generative network~\cite{Venkatesan17ARXIV,Shin17NIPS,Nguyen17ARXIV}. Besides the difficulty of training a generative network for a sequence of tasks or certain types of data, both rehearsal and pseudo-rehearsal approaches imply some form of concurrent learning, that is, having to re-process `old' instances for learning a new task.

The other popular strategy to overcome catastrophic forgetting is to reduce representational overlap~\cite{French91CCS}. This can be done at the output, intermediate, and also input levels~\cite{Gutsein15IJCNN,He18ICLR}. A clean way of doing that in a soft manner is through so-called ``structural regularization''~\cite{Zenke17ICML}, either present in the loss function~\cite{Kirkpatrick17PNAS,Zenke17ICML} or at a separate merging step~\cite{Lee17NIPS}. With these strategies, one seeks to prevent major changes in the weights that were important for previous tasks. Dedicating specific sub-parts of the network for each task is another way of reducing representational overlap~\cite{Rusu16ARXIV,Fernando17ARXIV,Yoon18ICLR}. The main trade-off in representational overlap is to effectively distribute the capacity of the network across tasks while maintaining important weights and reusing previous knowledge. 



In this paper, we propose a task-based hard attention mechanism that maintains the information from previous tasks without affecting the learning of a new task. Concurrently to learning a task, we also learn almost-binary attention vectors through gated task embeddings, using backpropagation and minibatch stochastic gradient descent (SGD). The attention vectors of previous tasks are used to define a mask and constrain the updates of the network's weights on current tasks. Since masks are almost binary, a portion of the weights remains static while the rest adapt to the new task. We call our approach hard attention to the task (HAT).
We evaluate HAT in the context of image classification, using what we believe is a high-standard evaluation protocol: we consider random sequences of 8~publicly-available data sets representing different tasks, and compare with a dozen of recent competitive approaches. We show favorable results in 4~different experimental setups, cutting current rates by 45 to 80\%. We also show robustness with respect to hyperparameters and illustrate a number of monitoring capabilities. We make our code publicly-available\footnote{\url{https://github.com/joansj/hat}}.





\section{Putting Hard Attention to the Task}
\label{sec:Method}

\subsection{Motivation}
\label{sec:Method_Motiv}

The primary observation that drives the proposed approach is that the task definition or, more pragmatically, its identifier, is crucial for the operation of the network. Consider the task of discriminating between bird and dog images. When training the network to do so, it may learn some set of intermediate features. If the second task is to discriminate between brown and black animals using the same data (assuming it only contained birds and dogs that were either brown or black), the network may learn a new set of features, some of them with not much overlap with the first ones. Thus, if training data is the same in both tasks, one important difference should be the task description or identifier. Our intention is to learn to use the task identifier to condition every layer, and to later exploit this learned conditioning to prevent forgetting previous tasks. 

\subsection{Architecture}
\label{sec:Method_Arch}

To condition to the current task , we employ a layer-wise attention mechanism (Fig.~\ref{fig:blockdiag}). Given the output of the units\footnote{In the remaining of the paper, we will use `units' to refer to both linear units (or fully-connected neurons) and convolutional filters. HAT can be extended to other parametric layers.} of layer , , we element-wise multiply . 
However, an important difference with common attention mechanisms is that, instead of forming a probability distribution,  is a gated version of a single-layer task embedding ,

where  is a gate function and  is a positive scaling parameter. We use a sigmoid gate in our experiments, but note that other gating mechanisms could be used. All layers  operate equally except the last one, layer , where  is binary hard-coded. The operation of layer  is equivalent to a multi-head output~\cite{Bakker02JMLR}, which is routinely employed in the context of catastrophic forgetting~\citep[for example][]{Rusu16ARXIV,Li17PAMI,Nguyen17ARXIV}.

\begin{figure}[t]
\begin{center}
	\includegraphics[width=1\linewidth]{fig_blockdiag}
    \vskip -0.1in
\caption{Schematic diagram of the proposed approach: forward (top) and backward (bottom) passes.}
	\label{fig:blockdiag}
	\end{center}
	\vskip -0.1in
\end{figure} 

The idea behind the gating mechanism of Eq.~\ref{eq:gate} is to form hard, possibly binary attention masks which, act as ``inhibitory synapses''~\cite{McCulloch43TBMB}, and can thus activate or deactivate the output of the units of every layer. In this way, and similar to PathNet~\cite{Fernando17ARXIV}, we dynamically create and destroy paths across layers that can be later preserved when learning a new task. However, unlike PathNet, the paths in HAT are not based on modules, but on single units. Therefore, we do not need to pre-assign a module size nor to set a maximum number of modules per task. Given some network architecture, HAT learns and automatically dimensions individual-unit paths, which ultimately affect individual layer weights. Furthermore, instead of learning paths in a separate stage using genetic algorithms, HAT learns them together with the rest of the network, using backpropagation and SGD.

\subsection{Network Training}
\label{sec:Method_NetTrain}

To preserve the information learned in previous tasks upon learning a new task, we condition the gradients according to the cumulative attention from all the previous tasks. To obtain a cumulative attention vector, after learning task  and obtaining , we recursively compute

using element-wise maximum and the all-zero vector for . 
This preserves the attention values for units that were important for previous tasks, allowing them to condition the training of future tasks.

To condition the training of task , we modify the gradient  at layer  with the reverse of the minimum of the cumulative attention in the current and previous layers:

where the unit indices  and  correspond to the output () and input () layers, respectively. In other words, we expand the vectors  and  to match the dimensions of the gradient tensor of layer , and then perform a element-wise minimum, subtraction, and multiplication (Fig.~\ref{fig:blockdiag}). We do not compute any attention over the input data if this consists of complex signals like images or audio. However, in the case such data consisted of separate or independent features, one could also consider them as the output of some layer and apply the same methodology.

Note that, with Eq.~\ref{eq:gradmask}, we create masks to prevent large updates to the weights that were important for previous tasks. This is similar to the approach of PackNet~\cite{Mallya17ARXIV}, which was made public during the development of HAT. In PackNet, after an heuristic selection and retraining, a binary mask is found and later applied to freeze the corresponding network weights. In this regard, HAT differs from PackNet in three important aspects. 
Firstly, our mask is unit-based, with weight-based masks automatically derived from those. Therefore, HAT also stores and maintains a lightweight structure.
Secondly, our mask is learned, instead of heuristically- or rule-driven. Therefore, HAT does not need to pre-assign compression ratios nor to determine parameter importance through a post-training step. Thirdly, our mask is not necessarily binary, allowing intermediate values between 0 and 1. This can be useful if we want to reuse weights for learning other tasks, at the expense of some forgetting, or we want to work in a more online mode, forgetting the oldest tasks to remember new ones. 

\subsection{Hard Attention Training}
\label{sec:Method_AttTrain}

To obtain a totally binary attention vector , one could use a unit step function as gate. However, since we want to train the embeddings  with backpropagation (Fig.~\ref{fig:blockdiag}), we prefer a differentiable function. To construct a pseudo-step function that allows the gradient to flow, we use a sigmoid with a positive scaling parameter  (Eq.~\ref{eq:gate}). This scaling is introduced to control the polarization, or `hardness', of the pseudo-step function and the resulting output . Our strategy is to anneal  during training, inducing a gradient flow, and set  during testing, using  such that Eq.~\ref{eq:gate} approximates a unit step function. Notice that when  we get , and that when  we get . We will use the latter to start a training epoch with all network units being equally active, and progressively polarize them within the epoch.

During a training epoch, we incrementally linearly anneal the value of  by

where  is the batch index and  is the total number of batches in an epoch. The hyperparameter  controls the stability of the learned tasks or, in other words the plasticity of the network's units. If  is close to 1, the gating mechanism operates like a regular sigmoid function, without particularly enforcing the binarization of . This provides plasticity to the units, with the model being able to forget previous tasks at the backpropagation stage (Sec.~\ref{sec:Method_NetTrain}). If, alternatively,  is a larger number, the gating mechanism starts operating as a unit step function. This provides stability with regard to previously learned tasks, preventing changes in the corresponding weights at the backpropagation stage.

\subsection{Embedding Gradient Compensation}
\label{sec:Method_Compens}

In preliminary analysis, we empirically observed that embeddings  were not changing much, and that the magnitude of the gradient was weak on those weights. After some investigation, we realized that the major part of the problem was due to the introduced annealing scheme (Eq.~\ref{eq:anneal}). To illustrate the effect of the annealing scheme on the gradients of , consider a uniformly distributed embedding  across the active range of a standard sigmoid, . If we do not perform any annealing and set , we obtain a cumulative gradient after one epoch that has a bell-like shape and spans the whole sigmoid range (Fig.~\ref{fig:compensation}). Contrastingly, if we set , we obtain a much larger magnitude, but in a much lower range ( in Fig.~\ref{fig:compensation}). The annealed version of  yields a distribution in-between, with a lower range than  and a lower magnitude than . A desirable situation would be to have a wide range, ideally spanning the range of , and a large cumulative magnitude, ideally proportional to the one in the active region when . To achieve that, we apply a gradient compensation before updating .

\begin{figure}[t]
\begin{center}
	\includegraphics{fig_compensation}
\vskip -0.15in
	\caption{Illustration of the effect that annealing  has on the gradients  of .}
	\label{fig:compensation}
	\end{center}
	\vskip -0.1in
\end{figure} 

In essence, the idea of the embedding gradient compensation is to remove the effects of the annealed sigmoid and to artificially impose the desired range and magnitude motivated in the previous paragraph. To do so, we divide the gradient  by the derivative of the annealed sigmoid, and multiply by the desired compensation,

which, after operating, yields

For numerical stability, we clamp  and constrain  to remain within the active range of the standard sigmoid, . In any case, however,  when we hit those limits. That is, we are in the constant regions of the pseudo-step function. Notice also that, by Eq.~\ref{eq:anneal}, the minimum  is never equal to 0.

\subsection{Promoting Low Capacity Usage}
\label{sec:Method_Compress}

It is important to realize that the hard attention values  that are `active', that is, , directly determine the units that will be dedicated to task . Therefore, in order to have some model capacity reserved for future tasks, we promote sparsity on the set of attention vectors . To do so, we add a regularization term to the loss function  that takes into account the set of cumulative attention vectors up to task , :

where  is the regularization constant, 

is the regularization term, and  corresponds to the number of units in layer . Notice that Eq.~\ref{eq:reg} corresponds to a weighted and normalized L1~regularization over . Cumulative attentions over the past tasks  define a weight for the current task, such that if  then  receives a weight close to 0 and vice versa. This excludes the units that were attended in previous tasks from regularization, unconstraining their reuse in the current task. 
The hyperparameter  controls the capacity spent on each task (Eq.~\ref{eq:loss}). In a sense, it can be thought of as a compressibility constant, affecting the compactness of the learned models: the higher the , the lower the number of active attention values  and the more sparse the resulting network is. We set  globally for all tasks and let HAT adapt to the best compression for each individual task. 

The use of L1~regularization to promote network sparsity in the context of catastrophic forgetting has also been considered by~\citet{Yoon18ICLR} with dynamically expandable networks (DEN), which were introduced while developing HAT. In DEN, plain L1~regularization is combined with a considerable set of heuristics such as L2-transfer, thresholding, and a measure of ``semantic drift'', and is applied to all network weights in the so-called ``selective retraining'' phase. In HAT, we use an attention-weighted L1~regularization over attention values, which is an independent part of the single training phase of the approach. Instead of considering network weights, HAT focuses on unit attention.



\section{Related Work}
\label{sec:SOTA}

We compare the proposed approach with the conceptually closest works, some of which appeared concurrently to the development of HAT. A more general overview of related work has been done in Sec.~\ref{sec:Intro}. A qualitative comparison with three of the most related strategies has been done along Sec.~\ref{sec:Method}. A quantitative comparison with these and other approaches is done in Sec.~\ref{sec:Exper} and \apx{apx:Res}. 

Both elastic weight consolidation~\citep[EWC;][]{Kirkpatrick17PNAS} and synaptic intelligence~\citep[SI;][]{Zenke17ICML} approaches add a `soft' structural regularization term to the loss function in order to discourage changes to weights that are important for previous tasks. HAT uses a `hard' structural regularization, and does it both at the loss function and gradient magnitudes explicitly. EWC measures weights' importance after network training, while SI and HAT compute weights' importance concurrently to network training. EWC and SI use specific formulation while HAT learns attention masks. Incremental moment matching~\citep[IMM;][]{Lee17NIPS} is an evolution of EWC, performing a separate model-merging step after learning a new task.

Progressive neural networks~\citep[PNNs;][]{Rusu16ARXIV} distribute the network weights in a column-wise fashion, pre-assigning a column width per task. They employ so-called adapters to reuse knowledge from previous columns/tasks, leading to a progressive increase of the number of weights assigned to future tasks. Instead of blindly pre-assigning column widths, HAT learns such `widths' per layer, together with the network weights, and adapts them to the difficulty of the current task.
PathNet~\cite{Fernando17ARXIV} also pre-assigns some amount of network capacity per task but, in contrast to PNNs, avoids network columns and adapters. It uses an evolutionary approach to learn paths between a constant number of so-called modules (layer subsets) that interconnect between themselves. HAT does not maintain a population of solutions, entirely trains with backpropagation and SGD, and does not rely on a constant set of modules. 

Together with PNNs and PathNet, PackNet~\cite{Mallya17ARXIV} also employs a binary mask to constrain the network. However, such constrain is not based on columns nor layer modules, but on network weights. Therefore, it allows for a potentially better use of the network's capacity. PackNet is based on heuristic weight pruning, with pre-assigned pruning ratios. HAT also focuses on network weights, but uses unit-based masks to constrain those, which also results in a lightweight structure. It avoids any absolute or pre-assigned pruning ratio, although it uses the compressibility parameter  to influence the compactness of the learned models. Another difference between HAT and the previous three approaches is that it does not use purely binary masks. Instead, the stability parameter  controls the degree of binarization.


Dynamically expandable networks~\citep[DEN;][]{Yoon18ICLR} also assign network capacity depending on the task at hand. However, they do so in a separate stage called ``selective retraining''. A complex mixture of heuristics and hyperparameters is used to identify ``drifting'' units, which are duplicated and retrained in another stage. L1~regularization and L2-transfer are employed to condition learning, together with the corresponding regularization constants and an additional set of thresholds. HAT strives for simplicity, restricting the number of hyperparameters to two that have a straightforward conceptual interpretation. Instead of plain L1~regularization over network weights, HAT employs an attention-weighted L1~regularization over attention masks. Attention masks are a lightweight structure that can be plugged in without the need of introducing important changes to a pre-existing network.



\section{Experiments}
\label{sec:Exper}

\textbf{Setups ---} Common setups to evaluate catastrophic forgetting in a classification context are based on permutations of the MNIST data~\cite{Srivastava13NIPS}, label splits of the MNIST data~\cite{Lee17NIPS}, incrementally learning classes of the CIFAR data sets~\cite{LopezPaz17NIPS}, or two-task transfer learning setups where accuracy is measured on both source and target tasks~\cite{Li17PAMI}. However, there are some limitations with these setups. Firstly, performing permutations of the MNIST data has been suggested to favor certain approaches, yielding misleading results\footnote{Essentially, the MNIST data contains many values close to 0 that allow for an easier identification of the important units or weights which, if permuted, can then be easily frozen without overlapping with the ones of the other tasks~\citep[see][]{Lee17NIPS}.} in the context of catastrophic forgetting~\cite{Lee17NIPS}. Secondly, using only the MNIST data may not be very representative of modern computer vision tasks, nor particularly challenging~\cite{Xiao17ARXIV}. Thirdly, incrementally adding classes or groups of classes implies the assumption that all data comes from the same joint distribution, which is unrealistic for a real-world setting. Finally, evaluating catastrophic forgetting with only two tasks biases the conclusions towards transfer learning setups, and prevents the analysis of truly sequential learning with more than two tasks. In this paper, we consider the aforementioned MNIST and CIFAR setups (Sec.~\ref{sec:Exper_Addition}). Nonetheless, we primarily evaluate on a sequence of multiple tasks formed by different classification data sets (Sec.~\ref{sec:Exper_Mixture}). 

To obtain a generic estimate, we weigh a number of tasks and uniformly randomize their order. After training task , we compute the accuracies on all testing sets of tasks . We repeat 10~times this sequential train/test procedure with 10~different seed numbers, which are also used in the rest of randomizations and initializations (see below). To compare between different task accuracies, and in order to obtain a general measurement of the amount of forgetting, we introduce the forgetting ratio

where  is the accuracy measured on task  after sequentially learning task ,  is the accuracy of a random stratified classifier using the class information of task , and  is the accuracy measured on task  after jointly learning  tasks in a multitask fashion. Note that  and  correspond to performances close to the ones of the random and multitask classifiers, respectively. To report a single number after learning  tasks, we take the average


\textbf{Data ---} We consider 8~common image classification data sets and adapt them, if necessary, to an input size of  pixels. The number of classes goes from 10 to~100, training set sizes from 16,853 to~73,257, and test set sizes from 1,873 to~26,032. For each task, we randomly split 15\% of the training set and keep it for validation purposes. The considered data sets are: CIFAR10 and CIFAR100~\cite{krizhevsky2009learning}, FaceScrub~\cite{ng2014data}, FashionMNIST~\cite{Xiao17ARXIV}, NotMNIST~\cite{Bulatov11TR}, MNIST~\cite{LeCun98PIEEE}, SVHN~\cite{Netzer2011}, and TrafficSigns~\cite{Stallkamp2011IJCNN}. For further details on data we refer to \apx{apx:Setup_Data}. 

\textbf{Baselines ---} We consider 2~reference approaches plus 9~recent and competitive ones: standard SGD with dropout~\cite{Goodfellow14ICLR}, SGD freezing all layers except the last one (SGD-F), EWC, IMM (Mean and Mode variants), learning without forgetting~\citep[LWF;][]{Li17PAMI}, less-forgetting learning~\citep[LFL;][]{Jung16ARXIV}, PathNet, and PNNs. To find the best hyperparameter combination for each approach, we perform a grid search using a task sequence determined by a single seed. To compute the forgetting ratio  (Eq.~\ref{eq:accuracy}), we also run the aforementioned random and multitask classifiers.

\textbf{Network ---} Unless stated otherwise, we employ an AlexNet-like architecture~\cite{krizhevsky2012imagenet} with 3~convolutional layers of 64, 128, and 256 filters with , , and  kernel sizes, respectively, plus two fully-connected layers of 2048 units each. We use rectified linear units as activations, and  max-pooling after the convolutional layers. We also use a dropout of 0.2 for the first two layers and of 0.5 for the rest. A fully-connected layer with a softmax output is used as a final layer, together with categorical cross entropy loss. All layers are randomly initialized with Xavier uniform initialization~\cite{glorot2010understanding} except the embedding layers, for which we use a Gaussian distribution . Unless stated otherwise, our code uses PyTorch's defaults for version 0.2.0~\cite{Paszke17NIPSW}. We adapt the same base architecture to all baseline approaches and match their number of parameters to 7.1\,M. 

\textbf{Training ---} We train all models with backpropagation and plain SGD, using a learning rate of 0.05, and decaying it by a factor of 3 if there is no improvement in the validation loss for 5~consecutive epochs. We stop training when we reach a learning rate lower than  or we have iterated over 200~epochs (we made sure that all considered approaches reached a stable solution before 200~epochs). Batch size is set to 64. All methods use the same task sequence, data split, batch shuffle, and weight initialization for a given seed. 

\subsection{Results}
\label{sec:Exper_Mixture}

We first look at the average forgetting ratio  after learning task  (Fig.~\ref{fig:mixture}). A first thing to note is that not all the considered baselines perform better than the SGD references. That is the case of LWF and LFL. For LWF, we observe it is still competitive in the two-task setup for which it was designed, . However, its performance rapidly degrades for , indicating that the approach has difficulties in extending beyond a transfer learning setup. We find LFL extremely sensitive to the configuration of its hyperparameter, to the point that what is a good value for one seed, turns out to be a bad choice for another seed. Hence the poor average performance for 10~seeds. The highest standard deviations are obtained by LFL and PathNet (Table~\ref{tab:mixture0}), which suggests a high sensitivity with respect to hyperparameters, initializations, or data sets. Another thing to note is that the IMM approaches only perform similarly or slightly better than the SGD-F reference. We believe this is due to both the different nature of the tasks' data and the consideration of more than two tasks, which complicates the choice of the mixing hyperparameter.

\begin{figure}[t]
\begin{center}
	\includegraphics{fig_mixture}
\vskip -0.15in
	\caption{Average forgetting ratio  for the considered approaches (10~runs).}
	\label{fig:mixture}
	\end{center}
	\vskip -0.1in
\end{figure} 

\begin{table}[t]
    \caption{Average forgetting ratio after the second () and the last () task for the considered approaches (10~runs, standard deviation into parenthesis).}
    \label{tab:mixture0}
\begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{lcc}
    \toprule
    Approach &  &  \\
    \midrule
LFL          & -0.73 (0.29) & -0.92 (0.08) \\
LWF          & -0.14 (0.13) & -0.80 (0.06) \\
SGD          & -0.20 (0.08) & -0.66 (0.03) \\
IMM-Mode     & -0.11 (0.08) & -0.49 (0.05) \\
SGD-F        & -0.20 (0.15) & -0.44 (0.06) \\
IMM-Mean     & -0.12 (0.10) & -0.42 (0.04) \\
EWC          & -0.08 (0.06) & -0.25 (0.03) \\
PathNet      & -0.09 (0.16) & -0.17 (0.23) \\
PNN          & -0.11 (0.10) & -0.11 (0.01) \\
\textbf{HAT}         & \textbf{-0.02 (0.03)} & \textbf{-0.06 (0.01)} \\
	\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

The best performing baselines are EWC, PathNet, and PNN. PathNet and PNN present contrasting behaviors. Both, by construction, never forget; therefore, the important difference is in their learning capability. PathNet starts by correctly learning the first task and progressively exhibits difficulties to do so for . Contrastingly, PNNs exhibits difficulty in the first tasks and becomes better as  increases. These contrasting behaviors are due to the way the two approaches allocate the network capacity. As mentioned, they cannot do it dynamically, and therefore need to pre-assign a number of network weights per task. When having more tasks but the same network capacity, this pre-assignment increasingly harms the performance of these baselines, lowering the corresponding curves in Fig.~\ref{fig:mixture}.

We now move to the HAT results. First of all, we observe that HAT consistently performs better than all considered baselines for all  (Fig.~\ref{fig:mixture}). For the case of , it obtains an average forgetting ratio , while the best baseline is EWC with  (Table~\ref{tab:mixture0}). For the case of , HAT obtains , while the best baseline is PNN with . This implies a reduction in forgetting of 75\% for  and 45\% for . Notice that the standard deviation of HAT is lower than the ones obtained by the big majority of the baselines (Table~\ref{tab:mixture0}). This denotes a certain stability of HAT with respect to different task sequences, data sets, data splits, and network initializations. 

Given the slightly increasing tendency of PNN with  (Fig.~\ref{fig:mixture}), one could speculate that PNN would score above HAT for . However, our empirical analyzes suggest that that is not the case (presumably due to the capacity pre-assignment and parameter increase problems underlined in Sec.~\ref{sec:SOTA} and above). In particular, we observe a gradual lowering of PathNet and PNN curves with increasing sequences from ~to~8. In addition, we observe PathNet and PNN obtaining worse performances than EWC in the case of  for the incremental class setup (see below and \apx{apx:Res_IncCIFAR}). In general, none of the baseline methods consistently outperforms the rest across setups and for all , a situation that we do observe with HAT. 

\subsection{Additional Results}
\label{sec:Exper_Addition}

To broaden the strength of our results, we additionally experiment with three common alternative setups. First, we consider an incremental class learning scenario, similar to~\citet{LopezPaz17NIPS}, using class subsets of both CIFAR10 and CIFAR100 data. In this setup, the best baseline after  is EWC, with . HAT scores  (55\% forgetting reduction). Next, we consider the permuted MNIST sequence of tasks~\cite{Srivastava13NIPS}. In this setup, the best result we could find in the literature was from SI, with . HAT scores  (52\% error rate reduction). Finally, we also consider the split MNIST task of~\citet{Lee17NIPS}. In this setup, the best result from the literature corresponds to the conceptor-aided backpropagation approach~\cite{He18ICLR}, with . HAT scores  (80\% error rate reduction). The detail for all these setups and results can be found in \apx{apx:Res}.

\subsection{Hyperparameters}
\label{sec:Exper_Param}

In any machine learning algorithm, it is important to assess the sensitivity with respect to the hyperparameters. HAT has two: the stability parameter  and the compressibility parameter  (Secs.~\ref{sec:Method_AttTrain} and~\ref{sec:Method_Compress}). A low  provides plasticity to the units and capacity of adaptation, but the network may easily forget what it learned. A high  prevents forgetting, but the network may have difficulties in adapting to new tasks. A low  allows to use almost all of the network's capacity for a given task, potentially spending too much in the current task. A high  forces it to learn a very compact model, at the expense of not reaching the accuracy that the original network could have reached. We empirically found good operation ranges  and . As we can see, any variation within these ranges results in reasonable performance (Fig.~\ref{fig:params}). Unless stated otherwise, we use  and .

\begin{figure}[t]
\begin{center}
	\includegraphics{fig_params}
\vskip -0.15in
	\caption{Effect of hyperparameters  and  on average forgetting ratio . Results for seed 0.}
	\label{fig:params}
	\end{center}
	\vskip -0.1in
\end{figure} 

\subsection{Monitoring and Network Pruning}
\label{sec:Exper_Monitor}

It is interesting to note that the hard attention mechanism introduced in Sec.~\ref{sec:Method} offers a number of possibilities to monitor the behavior of our models. For instance, by computing the conditioning mask in Eq.~\ref{eq:gradmask} from the hard attention vectors , we can assess which weights obtain a high attention value, binarize it, and compute an estimate of the instantaneous network capacity usage (Fig.~\ref{fig:capacity}). We may also inform ourselves of the amount of active weights per layer and task (\apx{apx:layeruse}). Another facet we can monitor is the weight reuse across tasks. By a similar procedure, comparing the conditioning masks between tasks  and , , we can asses the percentage of weights of task  that are later reused in task  (Fig.~\ref{fig:neuron_overlap}).

\begin{figure}[t]
\begin{center}
	\includegraphics{fig_capacity}
\vskip -0.15in
	\caption{Network capacity usage with sequential task learning (seed 0). Dashed vertical lines correspond to a task switch.}
	\label{fig:capacity}
	\end{center}
	\vskip -0.1in
\end{figure} 

\begin{figure}[t]
	\vskip 0.2in
	\begin{center}
	\includegraphics{fig_neuron_overlap}
\vskip -0.1in
\caption{Percentage of weight reuse across tasks. Seed 0 sequence: (1) FaceScrub, (2) MNIST, (3) CIFAR100, (4) NotMNIST, (5) SVHN, (6) CIFAR10, (7) TrafficSigns, and (8) FashionMNIST.}
	\label{fig:neuron_overlap}
	\end{center}
	\vskip -0.1in
\end{figure} 

Another by-product of hard attention masks is that we can use them to assess which of the network's weights are important, and then prune the most irrelevant ones~\cite{LeCun90NIPS}. This way, we can compress the network for further deployment in low-resource devices or time-constrained environments~\citep[cf.][]{Han16ICLR}. If we want to focus on such compression task, we can set  to a higher value than the one used for catastrophic forgetting and start with a positive random initialization of the embeddings . The former will promote more compression while the latter will ensure we start learning the model by putting attention to all weights in the first epochs (full capacity). We empirically found that using  and  yields a reasonable trade-off between accuracy and compression for a single task (Fig.~\ref{fig:compression}). With that, we can compress the network to sizes between 1 and 21\% of its original size, depending on the task (\apx{apx:Raw_Compress}). Comparing these numbers with the compression rates used by PackNet (25 or 50\%), we see that HAT generally uses a much more compact model. Comparing with DEN on the specific MNIST and CIFAR100 tasks (18 and 52\%), we observe that HAT compresses to 1 and 21\%, respectively. Interestingly, and in contrast to these and the majority of network pruning approaches, HAT learns to prune network weights through backpropagation and SGD, and at the same time as the network weights themselves.

\begin{figure}[t]
\begin{center}
	\includegraphics{fig_compression}
\vskip -0.15in
	\caption{Validation accuracy  as a function of compression percentage. Every dot corresponds to an epoch and triangles match the accuracy of the SGD approach (no compression).}
	\label{fig:compression}
	\end{center}
	\vskip -0.1in
\end{figure} 



\section{Conclusion}
\label{sec:Conclusion}

We introduce HAT, a hard attention mechanism that, by focusing on a task embedding, is able to protect the information of previous tasks while learning new tasks. This hard attention mechanism is lightweight, in the sense that it adds a small fraction of weights to the base network, and is trained together with the main model, with negligible overhead using backpropagation and vanilla SGD. We demonstrate the effectiveness of the approach to control catastrophic forgetting in the image classification context by running a series of experiments with multiple data sets and state-of-the-art approaches. HAT has only two hyperparameters, which intuitively refer to the stability and compactness of the learned knowledge, and whose tuning we demonstrate is not crucial for obtaining good performance. In addition, HAT offers the possibility to monitor the used network capacity across tasks and layers, the unit reuse across tasks, and the compressibility of a model trained for a given task. We hope that our approach may be also useful in online learning or network compression contexts, and that the hard attention mechanism presented here may also find some applicability beyond the catastrophic forgetting problem.









\balance
\bibliography{biblio}
\bibliographystyle{icml2018}
