\documentclass{article}




\PassOptionsToPackage{numbers}{natbib}
\usepackage[final]{neurips_2022}











\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{multirow}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}        
\usepackage{textcomp, gensymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{caption}
\captionsetup[table]{skip=5pt}
\usepackage{enumitem, multirow, xspace}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\algname}{TCP\xspace}
\newcommand{\yan}[1]{{\textcolor[rgb]{0,0,1}{#1}}}
\newcommand{\jxs}[1]{{\textcolor[rgb]{0,0,1}{#1}}}
\newcommand{\rebuttal}[1]{{\textcolor[rgb]{0,0,1}{#1}}}

\newcommand\blankfootnote[1]{\let\thefootnote\relax\footnotetext{#1}\let\thefootnote\svthefootnote }

\newcommand{\bb}{\hspace{-1mm} }
\title{
\algname: A New Autonomous Driving Paradigm of
Learning Control from Trajectory Planning
}

\title{
On Synergizing Trajectory and Control:\\ a Heterogeneous Multi-task Learning approach for End-to-end Autonomous Driving Planning
}
\title{
When Trajectory Planning Meets Direct Control: a Heterogeneous Mutual Learning Approach for End-to-end Autonomous Driving
}
\title{
Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline
}



\author{Penghao~Wu{} \\
  Shanghai AI Laboratory\\
  Shanghai Jiao Tong University \\
  \texttt{wupenghaocraig@sjtu.edu.cn} \\
\And
  Xiaosong~Jia{} \\
  Shanghai Jiao Tong University \\
   Shanghai AI Laboratory\\
  \texttt{jiaxiaosong@sjtu.edu.cn}
  \And 
  Li~Chen{} \\
  Shanghai AI Laboratory \\
  \texttt{lichen@pjlab.org.cn} \\
  \And 
  Junchi~Yan{} \\
  Shanghai Jiao Tong University \\
  Shanghai AI Laboratory \\
  \texttt{yanjunchi@sjtu.edu.cn} \\
  \And 
  Hongyang~Li \\
  Shanghai AI Laboratory \\
  Shanghai Jiao Tong University \\
  \texttt{lihongyang@pjlab.org.cn}
  \And 
  Yu~Qiao \\
  Shanghai AI Laboratory \\
  \texttt{qiaoyu@pjlab.org.cn}



}


\begin{document}


\maketitle

\blankfootnote{ Equal Contribution. Work done when PW and XJ were interns at Shanghai AI Laboratory.} 
\blankfootnote{ Correspondence author.}


\begin{abstract}
  Current end-to-end autonomous driving methods either run a controller based on a planned trajectory or perform control prediction directly, which have spanned two separately studied lines of research. Seeing their potential mutual benefits to each other, this paper takes the initiative to explore the combination of these two well-developed worlds. Specifically, our integrated approach has two branches for trajectory planning and direct control, respectively. The trajectory branch predicts the future trajectory, while the control branch involves a novel multi-step prediction scheme such that the relationship between current actions and future states can be reasoned. 
  The two branches are connected so that the control branch receives corresponding guidance from the trajectory branch at each time step. The outputs from two branches are then fused to achieve complementary advantages. Our results are evaluated in the closed-loop urban driving setting with challenging scenarios using the CARLA simulator. Even with a monocular camera input, the proposed approach ranks \textit{first} 
  on the official CARLA Leaderboard, outperforming other complex candidates with multiple sensors or fusion mechanisms by a large margin. 
The source code is publicly available at \url{https://github.com/OpenPerceptionX/TCP}.
\end{abstract}
\section{Introduction} \label{sec:intro}
End-to-end autonomous driving methods, which directly map raw sensor data to a planned trajectory or low-level control actions, show the virtue of simplicity, conceptually avoiding the cascading error of complex modular design and heavy hand-crafted rules.
The output prediction of the model for end-to-end autonomous driving generally falls into two forms: trajectory/waypoints \citep{rhinehart2019deep, pilotnet2020nvidia, chen2020lbc, prakash2021transfuser, chitta2021neat, jaeger2021transfuser+, chen2022lav} and direct control actions \citep{codevilla2018cil, liang2018cirl, codevilla2019cilrs, ohn2020lsd, chen2021wor, zhang2021roach, chekroun2021gri}. However, there is still no clear conclusion as to which of these two forms is better for all circumstances or certain scenarios.

Different from control predictions that could be directly applied to the vehicle, for methods that \textbf{plan trajectory}, additional controllers such as PID controllers are usually needed as a subsequent step to convert the planned trajectory into control signals.
One attractive and potential supremacy of trajectory-based prediction is that it actually considers a relatively longer time horizon into the future and could be further combined with other modules (\textit{e.g.}, multi-agent trajectory prediction \citep{zhang2021lbw, chen2022lav}, semantic or occupancy prediction modules \citep{chitta2021neat, casas2021mp3, hu2022stp3}) to reduce possible collisions.
However, turning the trajectory into control actions so that the vehicle could follow the planned trajectory is not trivial \citep{zablocki2021xai}. The industry usually adopts sophisticated control algorithms such as model predictive control to achieve reliable trajectory-following performance \citep{camacho2013mpc, guo2019mpcfollowing}. Simple PID controllers may perform worse in situations such as taking a big turn or starting at the red light due to the inertial problem of end-to-end models \citep{jaeger2021transfuser+}.
For \textbf{control-based} methods, the control signals are directly optimized. Nevertheless, their focus on the current step may cause deferred reactions to avoid potential collisions with other moving agents. The independence between the control predictions of different steps also makes the actions of the vehicle more unstable or discontinuous.
Fig.~\ref{fig:motivation} shows two typical cases where two paradigms fail respectively.
How to combine these two forms of prediction model as well as their outputs is an interesting yet relatively rarely studied area, which motivates this work. 

One straightforward (but in fact rarely studied in literature) idea is to train a control prediction model and a trajectory planning model separately, and combine their ultimate outputs directly. It can be viewed as an ensemble of two different models.
However, such a naive approach not only doubles the size of the model, but also ignores possible useful correlations between these two forms. To this end, we introduce the \textbf{\algname} (Trajectory-guided Control Prediction) framework, packing these two branches into a unified framework.
It can be viewed as a multi-task learning (MTL)~\cite{caruana1997mtl, argyriou2006mtfl} framework where a shared backbone extracts common features with decreased computational complexity 
as well as the increased ability of generalization due to the close relationship between the two tasks \citep{liang2019multi3d, kumar2021omnidet, chen2022persformer}.
Furthermore, to address the drawbacks of current control prediction methods, we delicately devise a novel multi-step control branch and a trajectory-guided control prediction scheme.




\begin{figure}[t!]
      \centering
      \includegraphics[width=\textwidth]{figures/motivation_v1.png}
\caption{Typical failure cases of two prediction paradigms. \textcolor{red}{Red} dots indicate the trajectory prediction, \textcolor{blue}{blue} dots are the actual path following the \textcolor{red}{red} trajectory with PID controllers, and \textcolor{green}{green} dots denote the actual path from control-based method. (a)  trajectory-based methods may struggle for big turns. (b) control-based methods may have a reaction latency and suffer from abrupt obstacles due to focusing on current time step only. 
These observations motivate us to propose a unified framework to combine these two worlds for mutual benefits.
}
\label{fig:motivation}
  \end{figure}



While trajectory planning considers several steps into the future, directly learning the control in a behavior cloning fashion \citep{pomerleau1988alvinn, muller2005e2eavoidance, codevilla2018cil, codevilla2019cilrs, chen2020lbc} often focuses on the current time step only, given prior on each state-action pair as independent and identical distributed (IID).
This assumption is not accurate and may hamper the long-term performance since the driving task is a sequential decision-making problem. To alleviate the problem, we propose  to predict multi-step control actions into the future.
However, the multi-step control process needs interactions with the environment. Thus we formulate a temporal module to learn the forward process and interactions between the ego agent and the environment.
A temporal module implemented with GRU \citep{GRU} progressively deals with the feature representation for each time step,  implicitly taking into account the dynamic motion of agents, interaction among them and dynamic environment information such as the changing of traffic lights.


Additionally, to generate accurate control signals in the multi-step prediction scheme, the model should retrieve proper location information from current sensor input for different future time steps. For example, an agent may pay more attention to nearby regions for a few early future time steps and far away regions for the remote ones. Considering that the knowledge has already been partly encoded in the trajectory branch, we adopt the attention mechanism to locate those critical and helpful areas in the long-term trajectory prediction branch, and guide the control prediction branch to pay attention to them at each future step in a corresponding way.
As a result, our model is capable of reasoning about how to optimize current control prediction so that the future states are similar to those from the expert when the predicted control actions are applied.

With the predicted trajectory and control signals from two branches, we propose a situation based fusion scheme to adaptively combine these two forms in a self-ensemble way to form the ultimate output according to the experiments results and prior knowledge.
It combines the best of these two forms, which further boosts the performance under different scenarios.



\algname has shown superior performance when being validated in the CARLA driving simulator \citep{Dosovitskiy17carla}. Our method, which only uses a \textbf{monocular camera}, achieves a \textbf{75.137} driving score and ranks \textbf{1st} on the public CARLA Leaderboard \citep{carlaleaderboard}, even surpassing prior state-of-the-art methods using multiple cameras and a LiDAR by 13.291 points. The main \textbf{contributions} of this paper include:
\begin{itemize}
\item We examine two dominant paradigms for end-to-end autonomous driving: trajectory planning and direct control, and propose to combine them in an integrated learning pipeline. To our knowledge, this is the first time that such two branches are jointly learned and fused for prediction.



\item 
A multi-step control prediction branch with a temporal module and trajectory-guided attention is devised to enable temporal reasoning. To combine the best of two branches, we design a situation based scheme to fuse the two outputs. 



\item As a simple yet strong baseline, our method with only a monocular camera as input achieves new state-of-the-art on the CARLA Leaderboard with many competitors using multiple sensors. We conduct thorough ablation studies to verify the effectiveness of our approach.
\end{itemize}


\section{Related Work} \label{sec:related_work}
\subsection{End-to-end Autonomous Driving}




Learning-based end-to-end autonomous driving has emerged as an active research topic in recent years. Studies usually fall into two categories: reinforcement learning (RL) and imitation learning.
RL is a promising way to address the problem of being more robust to the distribution shifts of datasets. Liang \textit{et al.} \citep{liang2018cirl} use DDPG to train a policy which is pre-trained in a supervised way. Kendall \textit{et al.} \citep{kendall2019driveaday} train their deep RL algorithm onboard to efficiently learn to drive a real-world vehicle.
The perception task is decoupled out of the online RL process in \citep{toromanoff2020modelfreerl, chekroun2021gri, zhao2022cadre}. The model-based method WoR \citep{chen2021wor} assumes world on rails and uses policy distillation to realize powerful performance.

Imitation learning (IL), especially behavior cloning, collects recorded data for models to mimic with high data efficiency. The expert data typically has two forms, trajectories and control actions.
Zeng \textit{et al.} \citep{zeng2019nmp} train a cost volume to generate the planning route, while \citep{sadat2020p3, casas2021mp3, hu2022stp3} explicitly design safety and comfort costs based on semantic occupancy maps to select the best one in the expert trajectory sets.
Zhang \textit{et al.} \citep{zhang2021lbw} predict trajectories of surrounding vehicles with labeled BEV map. LBC \citep{chen2020lbc} and NEAT \citep{chitta2021neat} decode waypoints from a dense heatmap or offset map. 
These approaches aforementioned all utilize a relatively dense representation to obtain results which increases model complexity. Transfuser and its variants \citep{prakash2021transfuser, jaeger2021transfuser+} adopt a simple GRU to auto-regress waypoints. 
LAV \citep{chen2022lav} adopts a temporal GRU module to further refine the trajectory. They unanimously achieve impressive performance on the CARLA leaderboard, motivating us to adapt the auto-regression scheme as well in our design. On the other hand, 
all trajectory-based methods use PID controllers to get the ultimate actions, which may cause inferior effects in complicated scenarios. 

Another genre to  predict control actions directly is 
proposed in \citep{pomerleau1988alvinn, muller2005e2eavoid, bojarski2016nvend, xu2017lstme2e}. CIL \citep{codevilla2018cil} adds a measurement encoder and multiple branches for different high level commands with the image encoder.
CILRS \citep{codevilla2019cilrs} is proposed afterwards and further introduces a speed prediction head. They stand as classic baselines for IL in the CARLA driving simulator. Diverse optimized approaches are presented based on them, such as multi-modal inputs \citep{hawke2020urbancil, xiao2020multimodal}, multi-task learning \citep{yang2018e2emmmt, li2018rethinking, hou2019fmnet, ishihara2021multie2eattention, kim2020fasnet, huch2021e2ecav, zhu2022mtcil}, dataset aggregation \citep{prakash2020darb} and knowledge distillation \citep{zhao2021sam, zhang2021roach}.
However, the compact control-based methods often have higher vehicles collision rates, remaining an interesting domain to explore.
Similar work exists in other related domains such as robotic navigation as well. \citep{pokle2019robot} learns a controller after a local trajectory planner to improve the overall navigation behavior.


\subsection{Multi-task and Ensemble Learning for Autonomous Driving}






Multi-task learning is a popular approach to train several related tasks simultaneously to help each other and improve generalization \citep{caruana1997mtl, argyriou2006mtfl}. Combinations of various autonomous driving tasks such as object detection, lane detection, semantic segmentation, depth estimation, \textit{etc}. have been proved to be capable of achieving incredible performance \citep{liang2019multi3d, chennupati2019multinet++, rajamanoharan2019mtmlreid, kumar2021omnidet, wu2021yolop, chen2022persformer}.
MTL is also suitable in the end-to-end problem since it is observed the performance of a direct mapping from an image to control signals is limited.
\citep{yang2018e2emmmt} adds a speed prediction task similar to CIL \citep{codevilla2018cil} and \citep{zhu2022mtcil} separates the lateral and longitudinal controls as two tasks. LAV \citep{chen2022lav} trains an extra scene mapping network, and \citep{hou2019fmnet, ishihara2021multie2eattention, jaeger2021transfuser+} additionally predict optical flow or dense depth.
Our idea of training trajectory and control simultaneously is closely related to FASNet \citep{kim2020fasnet}. FASNet predicts future positions of the ego agent as an auxiliary task and adds a kinematic loss considering the relation between control and locations. However, the constrain is based on a constant velocity model which neglects the important throttle and brake, and it does not work at the inference time. On the other hand, our \algname framework has feature interactions at an earlier stage to fully explore their potential mutual benefits.



Ensembles of models have long been utilized to improve the performance in computer vision \citep{dietterich2000ensemble, krahenbuhl2015lpo, lakshminarayanan2017deepensemble, vyas2018oodensemble, xu2020mmensemble, prakash2020darb}.
Besides the normal combination of models, two classic ensemble learning methods are particularly preferred in the autonomous driving regime. One is the Test-Time Augmentation (TTA), which is of great help to the 3D object detection task with LiDAR \citep{carranza2021odadensemble, li2022deepfusion}.
Another one is the fusion of experts \citep{jacobs1991expertmixture} where experts are trained on a subset of the input space and a gating network is trained to provide the fusion weights. LSD \citep{ohn2020lsd} and MoDE \citep{kim2022mode} divide a dataset into sub-scenarios to get different sub-policies for end-to-end autonomous driving.
These traditional ensemble approaches combine models of the same structure while our approach tries to combine two different representations.
Also, the multiple experts design increases the complexity of the training strategy and we seek to have a simpler situation based fusion scheme to boost the performance.




\section{Trajectory-guided Control Prediction} \label{sec:method}
\subsection{Problem Setting} \label{sec:prob_set}



\textbf{Problem formulation.} Given the state  comprised of the sensor signal , the speed of the vehicle , and the high level navigation information  including a discrete navigation command and the coordinates of navigation target provided by the global planner, the end-to-end model needs to output control signals  comprised of longitudinal control signals \textit{throttle}  and \textit{brake} , and the lateral control signal \textit{steer} .


Conventional methods tackle this problem with either a trajectory-output or a control-output only model. However, \algname combines both of them as two branches: a trajectory branch which predicts the planned trajectory and a control branch which is guided by the trajectory one and outputs both current and multi-step control signals into the future. Both branches are trained in a supervised manner.
Consider an expert which directly outputs the control signals at each step, supervising the predicted trajectory with the ground truth trajectory makes it not strictly satisfy the setting of behavior cloning in imitation learning. The ground truth trajectory indeed involves future expert actions and future states about the environment, so we formulate it as a trajectory planning task with ground truth trajectory as supervision for our trajectory branch. As for the control branch, training a control model which 
makes current control prediction supervised by the expert control is just behavior cloning in imitation learning, and it can be formulated as:

where  is a dataset comprised of state-action pairs collected from the expert.  denotes the policy of the control branch, and  is the loss  measuring how close the action from the expert and the action from our model is. The expert collects the dataset by controlling the vehicle and interacting with the world. Each collected route is a trajectory  as a sequence of state action pairs , which is then added into the whole dataset . 

\textbf{Expert demonstration.}
Here we choose Roach \citep{zhang2021roach} as the expert. Roach is a simple model trained by RL with privileged information, including roads, lanes, routes, vehicles, pedestrians, traffic lights, and stops, all being rendered into a 2D BEV image. Such a learning-based expert can transfer more information besides the direct supervision signals compared with an expert made by hand-crafted rules.
Specifically, we have a feature loss which forces the latent features before the final output head from the student model to be similar to that of the expert. A value loss is also added as an auxiliary task for the student model to predict an expected return. 



\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/teaser_.png}
    \caption{Overview of Trajectory-guided Control Prediction (\algname).
The encoded features are shared by the trajectory and multi-step control branch. 
The trajectory branch provides per-step guidance for multi-step control prediction. Outputs from two branches are combined according to our situation based fusion scheme to generate the ultimate control actions. 
}
    \label{fig:pipeline}
\end{figure}

\subsection{Architecture Design} \label{sec:arch}
\textbf{Overview.} As illustrated in Fig.~\ref{fig:pipeline}, the whole architecture is comprised of an input encoding stage and two subsequent branches.
The input image  goes through a CNN based image encoder, such as ResNet \citep{he2016resnet}, to generate a feature map .
In the meantime, the navigation information  is concatenated with the current speed  to form the measurement input , then an MLP based measurement encoder takes  as its input and outputs the measurement feature .
The encoded features are then shared by two branches for subsequent trajectory and control predictions. Specifically, the control branch is a novel multi-step prediction design with guidance from the trajectory one, which will be illustrated in detail in the following sections.
Finally, a situation based fusion scheme is adopted to combine the best of the two output paradigms.
We will go over each part in detail below.


\subsubsection{Trajectory planning branch} \label{sec:traj_branch}
Different from control prediction which directly predicts control actions, the trajectory planning branch first generates a planned trajectory comprised of waypoints at  steps for the agent to follow, and then the trajectory is processed by low-level controllers to get the final control actions.
With the shared feature from the input encoder, the image feature map  is average pooled and concatenated with the measurement feature  to form . Inspired by \citep{prakash2021transfuser}, we feed  into a GRU \citep{GRU} to auto-regressively obtain future waypoints one by one to form the planned trajectory altogether. 

We have two PID controllers for longitudinal and lateral control respectively. With the planned trajectory, we first calculate the vectors between consecutive waypoints. The magnitudes of these vectors represent the desired speed and are sent to the longitudinal controller to generate  and  control actions, and the orientations are sent to the lateral controller to get the  action.





\subsubsection{Multi-step control prediction branch} \label{sec:ctl_branch}
As discussed in Sec.~\ref{sec:prob_set}, for a control model predicting current control actions based on current input only, the supervised training is just behavior cloning, which relies on the independent and identically distributed (IID) assumption.
This assumption apparently does not hold because of the distribution shifts in test cases, since the closed-loop tests require sequential decision making where the historical actions will affect the future states and actions.
Instead of modeling it as a Markov Decision Process (MDP) and resorting to reinforcement learning, here we devise a simple way to mitigate the problem by predicting multi-step control into the future. 

Given the current state , now our multi-step control prediction branch outputs multiple actions: .
However, it is difficult to predict future control actions since we only have sensor inputs at the current time step. Towards this problem, we devise a temporal module to implicitly carry out the changing and interaction process of the environment and our agent. It is supposed to provide mainly dynamic information about the environment and the status of the agent itself, such as the motion of other objects, the changing of traffic lights, and the status of the ego agent.
Meanwhile, to improve the ability of incorporating critical static information (\textit{e.g.}, curbs and lanes) and boost the spatial consistency of two branches,
we propose to use the trajectory branch to guide the control counterpart to attend to proper regions of the input image at each future time step.  


\textbf{Temporal module.} 
Our temporal module is implemented with a GRU for better consistency with the trajectory branch.
At step , the input  for the temporal module is the concatenation of the current feature  (more construction details in the next section) and current predicted action , which is a compact representation about the current states of the environment and the agent itself.
The temporal module is supposed to reason about the dynamic changing process based on current feature vector and the predicted action.
Then the updated hidden state  will contain dynamic information about the environment and the updated status of the agent at time step .
To some extent, the temporal module acts as a coarse simulator with the whole environment and the agent being abstracted as a feature vector. It then simulates the interaction between the environment and the agent based on current prediction of actions.




\textbf{Trajectory-guided attention.} 
With the sensor input at current step only, it is hard to pick out desirable regions where the model should focus on at future steps. However, the location of the ego agent contains important cues about how to find those regions containing critical static information for control prediction at each step.

\setlength{\intextsep}{3pt}
\captionsetup[wrapfigure]{font=footnotesize}
\begin{wrapfigure}{r}{0pt}
\centering
\includegraphics[width=0.54\textwidth]{figures/trajectory_guide_att_v1.png}
\caption{Detailed trajectory guiding process. For predictions at time step , the hidden states from the waypoint GRU and the temporal module are combined to learn an attention weight map to re-aggregate the 2D image feature map for  control prediction.}
    \label{fig:traj_att}
\end{wrapfigure}

Therefore, we seek help from the trajectory planning branch to get information about the possible location of our agent at that corresponding step. As shown in Fig.~\ref{fig:traj_att}, \algname implements this by learning an attention map to extract important information from the encoded feature map. 
The interaction between two branches enhances the consistency of these two strongly related output paradigms and further elaborates the multi-task spirit.
Specifically, with the 2D feature map extracted by the image encoder  at time step , we calculate an attention map  using the corresponding hidden states from the control branch and the trajectory branch:

The attention map  is adopted to aggregate the feature map  for this step.
We then combine the attended feature map with  to form the informative representation feature  containing both static and dynamic information about the environment and the ego agent. The process can be described as follows:

The informative representation feature  is fed into a policy head which is shared among all time steps to predict the corresponding control action . 
Note that for the initial step, we only use the measurement feature to calculate the initial attention map and combine the attended image feature with the measurement feature to form the initial feature vector . To guarantee the feature  does describe the state at that step and contain the important information for control prediction, we add a feature loss at each step to make  close to the feature of the expert as well. 

To this end, our \algname framework endows the model with the reasoning ability along a short time horizon. It emphasizes how to make current control prediction close to the one from the expert. Furthermore, it takes into account what current control prediction can make the environment states and status of ego agent in future time steps similar to the ones from the expert.


\subsection{Loss Design} \label{sec:loss}
Our loss contains trajectory planning loss , control prediction , and auxiliary loss .

For the trajectory planning branch, the loss  can be expressed as:

where  are the predicted and ground truth waypoint at the  step respectively.
 indicates the feature loss measuring the  distance between  and the feature  from the expert at the current step as an additional supervision signal~\citep{zhang2021roach}.  is a tunable loss weight.

For the control prediction branch, we model the action as a beta distribution. The loss  is:

where  denotes the beta distribution represented by the corresponding  predicted distribution parameters and KL-divergence is used to measure the similarity between the predicted control distribution and the one from expert, \textit{i.e.}, . Feature loss is applied here as well. 
Note that all losses for future time steps () are averaged and then added to the loss for the current time step (), since the action executed immediately should be our key target to optimize.


To help the agent better estimate its current state, we add a speed prediction head to predict current speed  from the image feature and a value prediction head to predict the expected return estimated by the expert, similarly as in \cite{zhang2021roach}. We take the  loss for the speed prediction and  loss for the value prediction, denoting their weighted sum as .

The overall loss is as follows, as weighted by :

    
\subsection{Output Fusion} \label{sec:ensemble}
\begin{wrapfigure}{R}{0.52\textwidth}

\footnotesize
\raisebox{0pt}[\dimexpr\height-2\baselineskip\relax]{
\begin{algorithm}[H]
\SetKwInOut{Parameter}{Hyper parameters}
\SetAlgoLined
\KwIn {sensory input , speed of the ego vehicle , high level navigation information .}
\Parameter{combination weight }
\KwOut {final control signals }
\BlankLine

,   TCP(, , )

  Low-level Controller ()

Get current 

\eIf{ {\rm is} trajectory specialized}
{  }
{  }

 \caption{\footnotesize Situation based fusion scheme to combine the two output paradigms}
 \label{alg:ensemble}
\end{algorithm}
}
\end{wrapfigure}


We have two forms of output representations from our \algname framework: the planned trajectory and the predicted control. To further combine their advantages, we devise a situation-based fusion strategy as depicted in Algorithm~\ref{alg:ensemble}. Specifically, denote  as a combination weight whose value is between 0 to 0.5, in a certain situation where one representation is more suitable according to our prior belief, we combine the results from trajectory and control predictions by taking average with weight  so that the more suitable one takes up more weight (). Note that the combination weight  indeed does not need to be a constant or symmetric, which means we can set it to different values under different situations or different for specific control signals. In our experiment, we choose the  according to whether the ego vehicle is turning, implying that if it is turning, the  is  otherwise .






\section{Experiments} \label{sec:exp}
\subsection{Experimental Setup} \label{sec:exp_setup}
\textbf{Task \& Evaluation metrics.}
Our method is validated and tested in the CARLA driving simulator \citep{Dosovitskiy17carla}. Given a route defined by a sequence of sparse navigation points together with high level commands (straight, turn left/right, lane changing, and lane following), the closed-loop driving task requires the autonomous agent to drive towards the destination point. It is designed to simulate realistic traffic situations and includes different challenging scenarios such as obstacle avoidance, crossing an unsignalized intersection, and sudden control loss.
There are three major metrics: Driving Score, Route Completion, and Infraction Score. Route Completion is the percentage of the route completed by the autonomous agent. Infraction Score measures the number of infractions made along the route, with pedestrians, vehicles, road layouts, red lights, and \textit{etc}. Driving Score is the main metric which is the product of Route Completion and Infraction Score.

\textbf{Dataset.}
We use randomly generated routes under random weather conditions to collect 420K data in the 8 public towns offered by the CARLA simulator. Similar to \citep{chen2022lav}, we train \algname on 189K of data in 4 out of 8 towns (Town01, Town03, Town04, and Town06) for ablations and train with all 420K data for our online leaderboard submission.



\subsection{State-of-the-art Comparison} \label{sec:exp_comparison}


\begin{table}[t!]
\centering
\caption{Evaluation on the public CARLA Leaderboard \citep{carlaleaderboard} (accessed in May 2022). Our method \algname and \algname-Ens achieve a driving score of 69.714 and 75.137 respectively with only a monocular camera. More detailed infraction statistics can be found in the Supplementary.}
\label{table:comparesota}
\scalebox{0.85}{
\begin{tabular}{@{}llccccc@{}}
\toprule
\multirow{2}{*}{Rank} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Sensor Inputs}  & \multicolumn{3}{c}{Key Metrics }  \\
\cmidrule(r){3-4} \cmidrule(r){5-7}
 & &  \#Cameras & LiDAR & \begin{tabular}[c]{@{}c@{}}Driving \\ Score\end{tabular} & \begin{tabular}[c]{@{}c@{}}Route \\ Completion\end{tabular} & \begin{tabular}[c]{@{}c@{}}Infraction \\ Score\end{tabular} \\ \midrule
1    & \textbf{\algname-Ens} (ours)  & 1  &\xmark &\textbf{75.137} & 85.629 & \textbf{0.873} \\
1    & \textbf{\algname} (ours)      & 1  &\xmark & \textbf{69.714} & 82.962 & \textbf{0.851} \\
1    & \textbf{\algname-SB} (ours)      & 1  &\xmark & \textbf{68.695} & 82.957 & \textbf{0.833} \\ \midrule
2    & LAV \citep{chen2022lav}  & 4  &\cmark & 61.846 & \textbf{94.459} & 0.640 \\
3    & Transfuser        & 3  &\cmark & 61.181 & 86.694          & 0.714 \\
4    & Latent Transfuser & 3  &\xmark & 45.029 & 75.366          & 0.618 \\
5    & GRIAD \citep{chekroun2021gri} & 3  &\xmark & 36.787 & 61.855          & 0.597 \\
6    & Transfuser+ \citep{jaeger2021transfuser+}      & 4  &\cmark & 34.577 & 69.841          & 0.562 \\
7    & WoR \citep{chen2021wor}               & 4  &\xmark & 31.370 & 57.647          & 0.557 \\
8    & MaRLn \citep{toromanoff2020modelfreerl}             & 1  &\xmark & 24.980 & 46.968          & 0.518 \\ 
9    & NEAT \citep{chitta2021neat}   & 3 & \xmark & 21.832 & 41.707 & 0.650 \\ \bottomrule
\end{tabular}
}
\end{table}

Table~\ref{table:comparesota} shows the result of the comparison between our method and the top 8 entries on the public CARLA Leaderboard \citep{carlaleaderboard}. We report the results of \algname and two variants. \textbf{\algname-SB} replaces shared encoders of \algname with two separate ones for two branches, and \textbf{\algname-Ens} is the ensemble of \algname and \algname-SB.
Our method \algname-Ens ranks first on the leaderboard with a 75.137 driving score and highest infraction score, and \algname alone also surpasses prior methods.
Note that our method only uses a monocular camera while the top 2-4 methods all use multiple cameras and a LiDAR. Our driving score is 50.157 higher than the second-best monocular camera method, MaRLn \citep{toromanoff2020modelfreerl}.
Our route completion is slightly inferior to the LiDAR candidates - one reason is that methods using LiDAR may have a better object detection ability. Based on the detection results, they usually adopt a crawling strategy, indicating that the vehicle would move slowly when it has stopped for a long time and there are no obstacles ahead. As described in \citep{jaeger2021transfuser+}, this could alleviate ego vehicle's blocking problems to boost the route completion performance.

\subsection{Control vs. Trajectory} \label{sec:ctl_vs_traj}
\begin{table}[t!]
\centering
\caption{Comparison between the control and trajectory only model in terms of infractions frequency. TurnRatio means the corresponding ratio of happening during turning.}
\label{table:trajvscontrol}
\scalebox{0.8}{
\begin{tabular}{@{}lccccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Driving \\ Score\end{tabular}}
& \multicolumn{2}{c}{Collisions vehicles}                                                      & \multicolumn{2}{c}{Collisions layout}                                                        & \multicolumn{2}{c}{Off-road infractions}                                                     & \multicolumn{2}{c}{Agent blocked}                                                            \\ \cmidrule(l){3-4} \cmidrule(l){5-6} \cmidrule(l){7-8} \cmidrule(l){9-10} 
& & \#/km   & TurnRatio & \#/km   & TurnRatio& \#/km   & TurnRatio& \#/km   & TurnRatio \\ \midrule
Control-Only   & 32.452.23 & 1.25                       & 50.90\%                                                        & \textbf{0.23}                       & 10.00\%                                                           & \textbf{0.59}                       & 46.15\%                                                       & \textbf{0.41}                       & 50.00\%                                                           \\
Trajectory-Only & 28.293.03 & \textbf{0.85}                      & 38.70\%                                                        & 0.77                       & 64.20\%                                                        & 0.74                       & 62.90\%                                                        & 0.77                        & 64.20\%                                                        \\ \bottomrule
\end{tabular}
}
\end{table}

In this section, we conduct quantitative experiments to compare the \textbf{Control-Only} model and the \textbf{Trajectory-Only} model to demonstrate their advantages and disadvantages.
For both models, we use the same setting except for the output head and its corresponding loss. We use a ResNet-34 to encode visual inputs and a measurement module to encode the navigation information. Similar to~\citep{zhang2021roach}, we add speed and value heads as auxiliary tasks to help the model better encode the environment.
For \textbf{Control-Only}, we predict the control distribution based on the concatenated latent feature from the two encoders.
As for \textbf{Trajectory-Only}, we feed the feature to a GRU decoder to generate waypoints. 
As shown in Table~\ref{table:trajvscontrol}, though Trajectory-Only collides with vehicles less frequently than  Control-Only, it has more layout collisions, off-road infractions, and agent blocks.
We also count the ratio of each kind of infraction that occurs during turning. It can be observed that for Trajectory-Only, a large portion of such infractions happen when the ego agent is turning compared to Control-Only.
This has verified that Trajectory-Only performs worse when the agent is turning, which is probably caused by the unsatisfactory trajectory following performance of simple PID controllers as discussed in Sec.~\ref{sec:intro}.
As for the fact that Control-Only has a higher vehicle-collision rate, it is because the model focuses on the current time step and the reaction to potential collisions tends to be late, as depicted in Sec.~\ref{sec:intro} as well. The results above further validate the necessity of combining the two output paradigms.



\subsection{Ablative Study and Visualization} \label{sec:ablation}


\textbf{Component analysis.} We first validate the effectiveness of the trajectory-guided multi-step control prediction design, as shown in Table~\ref{table:ablation}.
We only employ the control branch output except for the last complete one when fusion is applied for these ablations. Adding a trajectory branch as an auxiliary task improves the performance by 2.5 points. The multi-step predictions with our temporal module greatly help with 7.9 points gain, and adding the trajectory-guided attention further acquires an improvement of 3.2 points. Finally, applying our situation based fusion scheme ( is set to 0.3) significantly boosts the infraction score, leading the overall driving score to 57.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/att_vis.pdf}
\caption{The trajectory-guided attention maps in two cases. In each case (row), from the left to right we show that the input image with the predicted trajectory (the first waypoint is projected out of the image), the predicted trajectory in the top-down view, the attention map , the attention map .}
    \label{fig:att}
\end{figure}
\begin{table}[t!]
\parbox[b]{.45\linewidth}{
\centering
\caption{Ablative study on the effectiveness of different components design of our model.}
\label{table:ablation}
\scalebox{0.85}{
\begin{tabular}[b]{@{}lccc@{}}
\toprule
Exp. & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Driving \\ Score\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Route \\ Completion\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Infraction \\ Score\end{tabular}} \\ \midrule
Control   & 32.452.23 & 76.543.22 & 0.450.03  \\
 traj-task & 34.981.96 & 81.325.50 & 0.490.05 \\
 temporal & 42.874.77  & \textbf{87.51}3.63 & 0.490.07 \\
 traj-attn & 46.083.47 & 84.951.84 & 0.560.03  \\
 fusion  & \textbf{57.01}1.88 & 85.271.20  & \textbf{0.67}0.01    \\ 
\bottomrule
\end{tabular}}
}
\hspace{15pt}
\parbox[b]{.45\linewidth}{
\centering
\caption{Comparison between MTL and ensemble methods ( is 0.3 for all experiments).}
\label{table:mtlvsensemble}
\scalebox{0.85}{
\begin{tabular}[b]{@{}lcccc@{}}
\toprule
Exp.                                                                             & \begin{tabular}[c]{@{}c@{}}Driving \\ Score\end{tabular}  & \#Param. & FLOPs & FPS \\ \midrule
Ensemble  &45.031.28 & 46.81M & 17.07G & 69.47\\
MTL & 48.270.58 & 23.58M & 8.54G & 133.30        \\ \algname-SB  &52.464.66 & 47.26M & 17.07G & 69.35  \\
\algname & 57.011.88& 25.77M & 8.54G & 125.71 \\ 
\algname-Ens &59.093.66 & 73.03M & 25.61G & 44.70  \\
\bottomrule
\end{tabular}}}
\end{table}

\textbf{Multi-task vs. Ensemble.} 
The comparison regarding their performances and computational complexity is given in Table~\ref{table:mtlvsensemble}.
\textbf{Ensemble} denotes directly combining the outputs of Control-Only and Trajectory-Only with our situation based fusion scheme.
\textbf{MTL} represents the model with a shared CNN backbone and measurement encoders followed by a trajectory branch and a control branch, but the control branch predicts current step prediction only and there are no interactions between the two branches.
We conclude that directly combining two models with our fusion scheme greatly improves the performance, and using an MTL approach works better than ensemble but with a much smaller model size and GFLOPs. A conventional ensemble approach to combine results from \algname and \algname-SB as \algname-Ens brings further performance gain at the cost of computational complexity.


\captionsetup[wrapfigure]{font=footnotesize}
\begin{wrapfigure}{r}{0pt}
\includegraphics[width=0.38\textwidth]{figures/mixture_weight_boxplot.png}
\caption{Box plot of the driving score with different  values (3 trials for each ).}
    \label{fig:mixture_weight}
\end{wrapfigure}
\textbf{Situation based fusion weight.}
We investigate the choice of the combination weight  in the situation based fusion scheme and show the box plot of the driving scores in the figure to the right. Besides , we additionally test 0.7 and 1, meaning that two results are conversely mixed with our  definition.
We see that only using the control from the specialized branch  performs poorly while directly taking the average or fusing conversely still has comparable results. One reason is that the  criterion used here is whether the vehicle is turning, making most cases , and the stronger control branch is not utilized enough if  is small.
Note that the situation based fusion scheme is general and flexible, and the criterion or  value used here is relatively coarse. 



\textbf{Visualization.} Fig.~\ref{fig:att} visualizes the trajectory-guided attention maps. The trajectory branch provides location-related information to guide the control branch to focus on important regions which are useful for future control prediction. See more qualitative results in the Supplementary.


\section{Conclusion}
In this work, we study two learning and prediction paradigms based on trajectory and direct control, respectively, for end-to-end autonomous driving. We propose a unified framework comprised of a trajectory branch and a novel multi-step control branch with interactions in between. We design a situation based fusion scheme to combine the results from two branches. Our method with only a monocular camera has achieved state-of-the-art performance on the CARLA Leaderboard.


\section*{Acknowledgments}
This work was partly supported by National Key Research and Development Program of China (2020AAA0107600), NSFC (62206172, 61972250), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and Shanghai Committee of Science and Technology (21DZ1100100, 22511105100).

\bibliographystyle{plainnat}
{
\small
\bibliography{egbib}
}
































\appendix









\end{document}