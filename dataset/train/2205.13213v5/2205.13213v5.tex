\section{Experiment} \label{sec:experiment}
In this section we conduct experiments to validate the effectiveness of the proposed LITv2.
Following common practice~\cite{pvt,swin,chu2021Twins,yang2021focal}, we experiment LITv2 on three tasks, including image classification on ImageNet-1K~\cite{imagenet}, object detection and instance segmentation on COCO~\cite{coco} and semantic segmentation on ADE20K~\cite{ade20k}. 

\begin{table}[!htb]
\centering
\vspace{-3mm}
\caption{Image classification results on ImageNet-1K. By default, the FLOPs, throughput and memory consumption are measured based on the resolution . We report the throughput and training/test time memory consumption with a batch size of 64. Throughput is tested on one NVIDIA RTX 3090 GPU and averaged over 30 runs. ResNet results are from "ResNet Stikes Back"~\cite{resnet_back}. ``'' means a model is finetuned at the resolution . ``OOM'' means ``out-of-memory''.}
\label{tab:imagenet_1k}
\renewcommand\arraystretch{1.1}
\scalebox{1.0}{
\begin{tabular}{l|ccccccc}
Model &
  \begin{tabular}[c]{@{}c@{}}Param \\ (M)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}FLOPs \\ (G)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Throughput \\ (imgs/s)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Train Mem \\ (GB)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Test Mem \\ (GB)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Top-1 \\ (\%)\end{tabular} \\ \shline
ResNet-50~\cite{resnet_back}     & 26 & 4.1  & 1,279 & 7.9  & 2.8 & 80.4 \\
ConvNext-Ti~\cite{convnext}     & 28 & 4.5  & 1,079 & 8.3  & 1.7 & 82.1 \\
PVT-S~\cite{pvt}         & 25 & 3.8  & 1,007 & 6.8  & 1.3 & 79.8 \\
Swin-Ti~\cite{swin}       & 28 & 4.5  & 961  & 6.1  & 1.5 & 81.3 \\
CvT-13~\cite{cvt}        & 20 & 4.5  & 947  & 6.1  & 1.5 & 81.6 \\
Focal-Tiny~\cite{yang2021focal}    & 29 & 4.9  & 384  & 12.2 & 3.3 & \textbf{82.2} \\
Twins-PCPVT-S~\cite{chu2021Twins} & 24 & 3.8  & 998  & 6.8  & 1.2 & 81.2 \\
LITv1-S~\cite{lit}         & 27 & 4.1  & 1,298 & 5.8  & 1.2 & 81.5 \\
\textbf{LITv2-S}      & 28 & 3.7  & \textbf{1,471} & \textbf{5.1}  & \textbf{1.2} & \textcolor{mblue}{82.0} \\ \hline
ResNet-101~\cite{resnet_back}    & 45 & 7.9  & 722  & 10.5 & 3.0 & 81.5 \\
ConvNext-S~\cite{convnext}    & 50 & 8.7  & 639  & 12.3 & 1.8 & 83.1 \\
PVT-M~\cite{pvt}         & 44 & 6.7  & 680  & 9.3  & 1.5 & 81.2 \\
Twins-SVT-B~\cite{chu2021Twins}   & 56 & 8.3  & 621  & 9.8  & 1.9 & 83.2 \\
Swin-S~\cite{swin}        & 50 & 8.7  & 582  & 9.7  & 1.7 & 83.0 \\
LITv1-M~\cite{lit}         & 48 & 8.6  & 638  & 12.0 & 1.4 & 83.0 \\
\textbf{LITv2-M}      & 49 & 7.5  & \textbf{812}  & \textbf{8.8}  & \textbf{1.4} & \textbf{83.3} \\ \hline
ResNet-152~\cite{resnet_back}    & 60 & 11.6  & 512  & 13.4 & 2.9 & 82.0 \\
ConvNext-B~\cite{convnext}    & 89 & 15.4  & 469  & 16.9 & 2.9 & \textbf{83.8} \\
Twins-SVT-L~\cite{chu2021Twins}   & 99 & 14.8 & 440  & 13.7 & 3.1 & 83.7 \\
Swin-B~\cite{swin}        & 88 & 15.4 & 386  & 13.4 & 2.4 & 83.3 \\
LITv1-B~\cite{lit}         & 86 & 15.0 & 444  & 16.4 & 2.1 & 83.4 \\
\textbf{LITv2-B}      & 87 & 13.2 & \textbf{602}  & \textbf{12.2} & \textbf{2.1} & \textcolor{mblue}{83.6} \\ \hline
DeiT-B~\cite{deit}        & 86 & 55.4 & 159  & 39.9 & \textbf{2.5} & 83.1 \\
Swin-B~\cite{swin}        & 88 & 47.1 & 142  & OOM & 6.1 & 84.5 \\
\textbf{LITv2-B}      & 87 & 39.7 & \textbf{198}  & \textbf{35.8} & 4.6 & \textbf{84.7}
\end{tabular}
}
\vspace{-3mm}
\end{table}


\subsection{Image Classification on ImageNet-1K} \label{sec:imagenet}
We conduct image classification experiments on ImageNet-1K~\cite{imagenet}, a large-scale image dataset which contains 1.2M training images and 50K validation images from 1K categories. We measure the model performance by Top-1 accuracy. Furthermore, we report the FLOPs, throughput, as well as training/test memory consumption on GPUs. We compare with two CNN-based models~\cite{resnet_back,convnext} and several representative SoTA ViTs~\cite{pvt,swin,cvt,yang2021focal,chu2021Twins}. Note that this paper does not consider mobile-level architectures~\cite{mobileformer,mobilevit}. Instead, we focus on models with the similar model size. Besides, we are also not directly comparable with NAS-based methods~\cite{glit,autoformer} as LITv2 is manually designed.

\textbf{Implementation details.}
All models are trained for 300 epochs from scratch on 8 V100 GPUs. At training time, we set the total batch size as 1,024. The input images are resized and randomly cropped into . The initial learning rate is set to  and the weight decay is set to . We use AdamW optimizer with a cosine decay learning rate scheduler. All training strategies including the data augmentation are same as in LITv1. For HiLo, the window size  is set to 2. The split ratio  is set to 0.9, which is chosen from a simple grid search on ImageNet-1K. The depthwise convolutional layers in FFNs are set with a kernel size of , stride of  and zero padding size of .

\textbf{Results.}
In Table~\ref{tab:imagenet_1k}, we report the experiment results on ImageNet-1K. First, compared to LITv1 baselines, LITv2 achieves consistent improvement on Top-1 accuracy while using less FLOPs. Moreover, benefit from HiLo, LITv2 achieves faster throughput and significant training time memory reduction (\eg, , ,  inference speedup for the small, medium and base settings, respectively) compared to LITv1. Second, compared to CNNs, LITv2 models outperform all counterparts of ResNet and ConvNext in terms of FLOPs, throughput and memory consumption while achieving comparable performance. Last, compared to SoTA ViTs, LITv2 surpasses many models in terms of throughput and memory consumption with competitive performance. For example, under the similar amount of FLOPs, LITv2-S achieves faster inference speed than PVT-S and Twins-PCPVT-S with better performance. Although Focal-Tiny achieves better Top-1 accuracy than LITv2-S, it runs much slower (\ie, 384 vs. 1,471 images/s) and requires a large amount of memory to train. Besides, when finetuning on a higher resolution, LITv2-B outperforms both DeiT-B and Swin-B with a faster throughput and lower complexity.

\begin{table}[]
\centering
\renewcommand\arraystretch{1.1}
\caption{Object detection and instance segmentation performance on the COCO \texttt{val2017} split using the RetinaNet~\cite{retinanet} and Mask R-CNN~\cite{mask_rcnn} framework.  and  denote the bounding box AP and mask AP, respectively. ``*'' indicates the model adopts a local window size of 4 in HiLo.}
\label{tab:coco_exp}
\scalebox{0.85}{
\begin{tabular}{l|cccc|ccccc}
\multirow{2}{*}{Backbone} & \multicolumn{4}{c|}{RetinaNet}             & \multicolumn{5}{c}{Mask R-CNN}                             \\ \cline{2-10} 
                          & Params & FLOPs (G) & FPS  &          & Params & FLOPs (G) & FPS  &           &         \\ \shline
ResNet-50~\cite{resnet}                 & 38M         & 239       & 18.5 & 36.3          & 44M         & 260       & 27.1 & 38.0          & 34.4          \\
PVT-S~\cite{pvt}                     & 34M         & 273       & 13.0 & 40.4          & 44M         & 292       & 16.2 & 40.4          & 37.8          \\
Swin-T~\cite{swin}                    & 38M         & 251       & 17.0 & 41.5          & 48M         & 270       & 21.1 & 42.2          & 39.1          \\
Twins-SVT-S~\cite{chu2021Twins}               & 34M         & 225       & 15.5 & 43.0          & 44M         & 244       & 20.4 & 43.4          & 40.3          \\
LITv1-S~\cite{lit}                     & 39M         & 305       & 3.3  & 41.6          & 48M         & 324       & 3.2  & 42.9          & 39.6          \\
\textbf{LITv2-S}                   & 38M         & 242       & 18.7 & \textbf{44.0} & 47M         & 261       & 18.7 & \textbf{44.9} & \textbf{40.8} \\
\textbf{LITv2-S*}                  & 38M         & 230       & \textcolor{mblue}{20.4} & \textbf{43.7} & 47M         & 249       & \textcolor{mblue}{21.9} & \textbf{44.7} & \textbf{40.7} \\ \hline
ResNet-101~\cite{resnet}                & 57M         & 315       & 15.2 & 38.5          & 63M         & 336       & 20.9 & 40.4          & 36.4          \\
PVT-M~\cite{pvt}                     & 54M         & 348       & 10.5 & 41.9          & 64M         & 367       & 10.8 & 42.0          & 39.0          \\
Swin-S~\cite{swin}                    & 60M         & 343       & 13.3 & 44.5          & 69M         & 362       & 15.8 & 44.8          & 40.9          \\
Twins-SVT-B~\cite{chu2021Twins}               & 67M         & 358       & 10.8 & 45.3          & 76M         & 377       & 12.7 & 45.2          & 41.5          \\
\textbf{LITv2-M}                  & 59M         & 348       & 12.2 & \textbf{46.0} & 68M         & 367       & 12.6 & \textbf{46.8} & \textbf{42.3} \\
\textbf{LITv2-M*}                  & 59M         & 312       & \textcolor{mblue}{14.8} & \textbf{45.8} & 68M         & 315       & \textcolor{mblue}{16.0} & \textbf{46.5} & \textbf{42.0} \\ \hline
ResNeXt101-64x4d~\cite{resnext}          & 96M         & 473       &  10.3    & 41.0          & 102M        & 493       &  12.4    & 42.8          & 38.4          \\
PVT-L~\cite{pvt}                    & 71M         & 439       & 9.5     & 42.6          & 81M         & 457       & 8.3     & 42.9          & 39.5          \\
Swin-B~\cite{swin}                 & 98M         & 488       & 11.0 & 44.7          & 107M        & 507       & 11.3 & 45.5          & 41.3          \\
Twins-SVT-L~\cite{chu2021Twins}              & 111M        & 504       & 9.9  & 45.7          & 120M        & 524       & 10.1 & 45.9          & 41.6          \\
\textbf{LITv2-B}          & 97M         & 481       & 9.5     & \textbf{46.7} & 106M        & 500       &  9.3    & \textbf{47.3} & \textbf{42.6} \\
\textbf{LITv2-B*}        & 97M         & 430       & \textcolor{mblue}{11.8}     & \textbf{46.3} & 106M        & 449       & \textcolor{mblue}{11.5}     & \textbf{46.8} & \textbf{42.3}
\end{tabular}
}
\vspace{-8mm}
\end{table}

\vspace{-0.5em}
\subsection{Object Detection and Instance Segmentation on COCO} \label{sec:coco}
In this section, we conduct experiments on COCO 2017, a common benchmark for object detection and instance segmentation which contains 118K images for the training set and 5K images for the validation set. Following common practice~\cite{chu2021Twins,pvt}, we experiment with two detection frameworks: RetinaNet~\cite{retinanet} and Mask R-CNN~\cite{mask_rcnn}. We measure model performance by Average Precision (AP). 

\textbf{Implementation details.}
All backbones are initialized with pretrained weights on ImageNet-1K. We train each model on 8 GPUs with 1 schedule (12 epochs) and a total batch size of 16. For a fair comparison, we adopt the same training strategy and hyperparameter settings as in LITv1~\cite{lit}. Note that we pretrain LITv2 with a local window size of 2 and  on ImageNet-1K. Under the same , a larger window size helps to achieve lower complexity and thus improves the speed at high resolution, as explained in Section~\ref{sec:hilo}. In this case, we also train models with a slightly larger window size of  for better efficiency, which we denote with ``*''. By default, FLOPs is evaluated based on the input resolution of . FPS is measured on one RTX 3090 GPU based on the mmdetection~\cite{mmdetection} framework.

\textbf{Results.}
In Table~\ref{tab:coco_exp}, we report the experimental results on COCO. In general, LITv2 outperforms LITv1 by a large margin in almost all metrics. Besides, our LITv2 significantly surpasses ResNet in terms of AP, though it runs slightly slower in some cases. More importantly, 
our LITv2 beats all the compared SoTA ViTs, achieving the best AP with compelling fast inference speed.
Furthermore, by adopting a larger window size (\ie, ), LITv2 achieves better efficiency with a slightly performance drop.


\begin{wraptable}{R}{0.5\textwidth} 
\begin{minipage}{0.5\textwidth} 
\vspace{-4mm}
\centering
\caption{Semantic segmentation performance of different backbones on the ADE20K validation set. FLOPs is evaluated based on the image resolution of . }
\label{tab:ade20k}
\renewcommand\arraystretch{1.1}
\scalebox{0.85}{
\begin{tabular}{l|cccc}
Backbone    & \begin{tabular}[c]{@{}c@{}}Params \\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}FLOPs \\ (G)\end{tabular} & FPS  & \begin{tabular}[c]{@{}c@{}}mIoU \\ (\%)\end{tabular} \\ \shline
ResNet-50~\cite{resnet}   & 29  & 45  & 45.4 & 36.7          \\
PVT-S~\cite{pvt}       & 28  & 40  & 38.7 & 39.8          \\
Swin-Ti~\cite{swin}     & 32  & 46  & 39.6 & 41.5          \\
Twins-SVT-S~\cite{chu2021Twins} & 28  & 37  & 34.5 & 43.2          \\
LITv1-S~\cite{lit}       & 32  & 46  & 18.1 & 41.7          \\
\textbf{LITv2-S}    & 31  & 41  &  \textcolor{mblue}{42.6} & \textbf{44.3} \\ \hline
ResNet-101~\cite{resnet}  & 48  & 66  & 36.7 & 38.8          \\
PVT-M~\cite{pvt}       & 48  & 55  & 29.7 & 41.6          \\
Swin-S~\cite{swin}      & 53  & 70  & 24.4 & 45.2          \\
Twins-SVT-B~\cite{chu2021Twins} & 60  & 67  & 28.0 & 45.3          \\
\textbf{LITv2-M}    & 52  & 63  &  \textcolor{mblue}{28.5} & \textbf{45.7} \\ \hline
PVT-L~\cite{pvt}  & 65  & 71  & 20.5 & 42.1 \\
Swin-B~\cite{swin}      & 107 & 107 & 25.5 & 46.0          \\
Twins-SVT-L~\cite{chu2021Twins} & 104 & 102 & 25.9 & 46.7          \\
\textbf{LITv2-B}   & 90  & 93  & \textcolor{mblue}{27.5} & \textbf{47.2}
\end{tabular}
}
\end{minipage}
\end{wraptable}

\subsection{Semantic Segmentation on ADE20K}
In this section, we evaluate LITv2 on the semantic segmentation task. We conduct experiments on ADE20K~\cite{ade20k}, a widely adopted dataset for semantic segmentation which has 20K training images, 2K validation images and 3K test images. Following prior works, we adopt the framework of Semantic FPN~\cite{sem_fpn} and measure the model performance by mIoU.
We train each model on 8 GPUs with a total batch size of 16 with 80K iterations. All backbones are initialized with pretrained weights on ImageNet-1K. The stochastic depth for the small, medium and base models of LITv2 are 0.2, 0.2 and 0.3, respectively. All other training strategies are the same as in LITv1~\cite{lit}.

\textbf{Results.} In Table~\ref{tab:ade20k}, we compare LITv2 with ResNet and representative ViTs on ADE20K. In general, LITv2 achieves fast speed while outperforming many SoTA models. 
For example, our LITv2-S, LITv2-M and LITv2-B surpass Swin-Ti, Swin-S and Swin-B by 2.8\%, 0.5\% and 1.2\% in mIoU with higher FPS, respectively.

\subsection{Ablation Study} \label{sec:ablation}
In this section, we provide ablation studies for LITv2, including the comparison with other efficient attention variants, the effect of  in HiLo, as well as the effect of architecture modifications. By default, the throughput and memory consumption are measured on one RTX 3090 GPU with a batch size of 64 under the resolution of . 

\begin{table}[]
\centering
\caption{Performance comparisons with other efficient attention mechanisms in ViTs based on LITv2-S. We report the Top-1 accuracy on ImageNet-1K and mIoU on ADE20K.}
\renewcommand\arraystretch{1.1}
\label{tab:comparison_attentions}
\scalebox{0.82}{
\begin{tabular}{l|cccccc|ccc}
\multirow{2}{*}{Method} & \multicolumn{6}{c|}{ImageNet-1K} & \multicolumn{3}{c}{ADE20K} \\ \cline{2-10} 
 & \begin{tabular}[c]{@{}c@{}}Params \\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}FLOPs\\  (G)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Throughput \\ (images/s)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Train Mem\\  (GB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Test Mem\\  (GB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Top-1 \\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}} Params \\ (M) \end{tabular} & \begin{tabular}[c]{@{}c@{}} FLOPs \\ (G) \end{tabular} & \begin{tabular}[c]{@{}c@{}} mIoU \\ (\%) \end{tabular} \\ \shline
MSA & 28 & 4.1 & 1,293 & 6.5 & 1.2 & 82.3 & 32 & 46.5 & 43.7 \\
SRA~\cite{pvt} & 32 & 4.0 & 1,425 & 5.1 & 1.3 & 81.7 & 35 & \textbf{42.4} & 42.8 \\
W-MSA~\cite{swin} & 28 & 4.0 & 1,394 & 5.3 & 1.2 & 81.9 & 32 & 42.7 & 41.9 \\
T-MSA~\cite{chu2021Twins} & 30 & 4.0 & 1,462 & \textbf{5.0} & 1.3 & 81.8 & 33 & 42.5 & 44.0 \\
HiLo & \textbf{28} & \textbf{3.7} & \textbf{1,471} & 5.1 & \textbf{1.2} & \textbf{82.0} & \textbf{31} & 42.6 & \textbf{44.3}
\end{tabular}
}
\end{table}

\begin{figure}[]
	\centering
	\vspace{-2mm}
	\includegraphics[width=0.9\linewidth]{figures/comparison.pdf}
	\caption{Comparison with other attention mechanisms based on LITv2-S. We report the FLOPs, throughput, and training/test time memory consumption. Evaluations are based on a batch size of 64 on one RTX 3090 GPU. The black cross symbol means ``out-of-memory''.}
	\label{fig:attn_comparison}
	\vspace{-5mm}
\end{figure}


\textbf{Comparing HiLo with other attention mechanisms.}
Based on LITv2-S, we compare the performance of HiLo with other efficient attention mechanisms on ImageNet-1K, including spatial reduction attention (SRA) in PVT~\cite{pvt}, shifted-window based attention (W-MSA) in Swin~\cite{swin} and alternated local and global attention (T-MSA) in Twins~\cite{chu2021Twins}. In our implementation, we directly replace HiLo with each compared method. The results are reported in Table~\ref{tab:comparison_attentions}. In general, HiLo reduces more FLOPs while achieving better performance and faster speed than the compared methods. Furthermore, in Figure~\ref{fig:attn_comparison}, we provide comprehensive benchmarks for more attention mechanisms based on different image resolutions, including Focal~\cite{yang2021focal}, QuadTree~\cite{tang2022quadtree} and Performer~\cite{performer}. Suffering from weak parallelizability, they are even slower than that of using standard MSAs on GPUs. Compared to them, HiLo achieves competitive results in terms of the FLOPs, throughput and memory consumption. Moreover, we conduct experiments based on ADE20K and Semantic FPN and show that HiLo achieves more performance gain than other attention mechanisms on the downstream dense prediction task.


\begin{table}[t]
\begin{minipage}{0.43\linewidth}
\centering
\footnotesize
  \includegraphics[width=1.0\textwidth]{figures/alpha_window.pdf}
  \captionof{figure}{Effect of  based on LITv2-S.}
  \label{fig:effect_of_alpha}
\end{minipage}
\hfill
\begin{minipage}{0.52\linewidth}
\setlength{\tabcolsep}{1.8pt}
\centering
\footnotesize
\vspace{-5mm}
  \caption{Effect of architecture modifications based on LITv1-S. ``ConvFNN'' means we add one layer of  depthwise convolutional layer into each FFN. ``RPE'' refers to relative positional encoding~\cite{swin}.}
  \renewcommand\arraystretch{1.2}
  \scalebox{0.95}{
\begin{tabular}{l|ccc|ccc}
\multirow{2}{*}{Name} & \multicolumn{3}{c|}{ImageNet-1K} & \multicolumn{3}{c}{COCO (RetinaNet)} \\ \cline{2-7} 
 &
  \begin{tabular}[c]{@{}c@{}}FLOPs \\ (G)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Mem \\ (GB)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Top-1 \\ (\%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}FLOPs \\ (G)\end{tabular} &
  FPS &
  AP \\ \shline
LITv1-S~\cite{lit}                 & 4.1       & 5.8      & 81.5      & 305        & 3.3        & 41.6       \\
+ ConvFFN               & 4.1       & 6.5      & 82.5      & 306        & 3.1        & 45.1          \\
+ Remove RPE            & 4.1       & 6.5      & 82.3      & 306        & 13.3       & 44.7          \\
+ HiLo              & 3.7       & 5.1      & 82.0      & 224        & 18.7       & 44.0      
\end{tabular}
  }
  \label{tab:effect_modification}
\end{minipage}
\vspace{-5mm}
\end{table}




\textbf{Effect of .}
As shown in Figure~\ref{fig:effect_of_alpha}, since the complexity of Lo-Fi is lower than Hi-Fi under the resolution of  and the window size of 2, a larger  helps to reduce more FLOPs as we allocate more heads to Lo-Fi. Moreover, we found HiLo performs badly with , in which case only the Hi-Fi is left and HiLo only focuses on high frequencies. We speculate that low frequencies play an important role in self-attention. For other values of , we find the performance difference is around 0.2\%, where  achieves the best performance. However, it is worth noting that although the pure Lo-Fi branch () can achieve competitive results on ImageNet-1K, high-frequency signals play an important role in capturing fine object details, which is particularly important for  dense prediction tasks such as semantic segmentation. For example, with , LITv2-S based Semantic FPN achieves more performance gain (+0.6\%) than that of using  (43.7\%).

\textbf{Effect of architecture modifications.} Based on LITv2-S, we explore the effect of architecture modifications. As shown in Table~\ref{tab:effect_modification}, benefit from the enlarged receptive field in the early stages, the adoption of depthwise convolutions improves the performance on both ImageNet and COCO. Next, by removing the relative positional encoding, we significantly improve FPS on dense prediction tasks with a slightly performance drop on both datasets. Also note that since depthwise convolutions have encoded positional information by zero paddings~\cite{cnn_pos_encoding}, the elimination of RPE does not result in a significant performance drop compared to prior works~\cite{swin}. Finally, benefit from HiLo, we achieve more gains in model efficiency on both ImageNet and COCO.

\begin{figure}[]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/freq_vis_3.pdf}
	\vspace{-3mm}
	\caption{Frequency magnitude () from 8 output channels of Hi-Fi and Lo-Fi in LITv2-B. The magnitude is averaged over 100 samples. The lighter the color, the larger the magnitude. A pixel that is closer to the centre means a lower frequency. Visualization code can be found in the supplementary material.
 }
	\label{fig:vis_freq}
	\vspace{-7mm}
\end{figure}

\textbf{Spectrum analysis of HiLo.} 
In Figure~\ref{fig:vis_freq}, we visualize the magnitude of frequency component~\cite{rao2021global} by applying Fast Fourier Transform (FFT) to the output feature maps from Hi-Fi and Lo-Fi attentions, respectively. The visualisation indicates that Hi-Fi captures more high frequencies and Lo-Fi mainly focuses on low frequencies. This strongly aligns with our aim of disentangling high and low frequencies in feature maps at a single attention layer. 

\begin{table}[]
\centering
\renewcommand\arraystretch{1.1}
\caption{Speed and performance comparisons between LITv2-S and other recent ViTs on different GPUs. All throughput results are averaged over 30 runs with a total batch size of 64 and image resolution of  on one GPU card. We also report the Top-1 accuracy on ImageNet-1K.}
\label{tab:more_vit_compare}
\scalebox{0.85}{
\begin{tabular}{l|ccccccc}
Model & Params (M) & FLOPs (G) & A100 & V100 & RTX 6000 & RTX 3090 & Top-1 (\%) \\ \shline
ResNet-50~\cite{resnet_back} & 26 & 4.1 & 1,424 & 1,123 & 877 & 1,279 & 80.4 \\
PVT-S~\cite{pvt} & 25 & 3.8 & 1,460 & 798 & 548 & 1,007 & 79.8 \\
Twins-PCPVT-S~\cite{chu2021Twins} & 24 & 3.8 & 1,455 & 792 & 529 & 998 & 81.2 \\
Swin-Ti~\cite{swin} & 28 & 4.5 & 1,564 & 1,039 & 710 & 961 & 81.3 \\
TNT-S~\cite{tnt} & 24 & 5.2 & 802 & 431 & 298 & 534 & 81.3 \\
CvT-13~\cite{cvt} & 20 & 4.5 & 1,595 & 716 & 379 & 947 & 81.6 \\
CoAtNet-0~\cite{coatnet} & 25 & 4.2 & 1,538 & 962 & 643 & 1,151 & 81.6 \\
CaiT-XS24~\cite{cait} & 27 & 5.4 & 991 & 484 & 299 & 623 & 81.8 \\
PVTv2-B2~\cite{pvtv2} & 25 & 4.0 & 1,175 & 670 & 451 & 854 & 82.0 \\
XCiT-S12~\cite{xcit} & 26 & 4.8 & 1,727 & 761 & 504 & 1,068 & 82.0 \\
ConvNext-Ti~\cite{convnext} & 28 & 4.5 & 1,654 & 762 & 571 & 1,079 & 82.1 \\
Focal-Tiny~\cite{yang2021focal} & 29 & 4.9 & 471 & 372 & 261 & 384 & \textbf{82.2} \\
\textbf{LITv2-S} & 28 & \textbf{3.7} & \textbf{1,874} & \textbf{1,304} & \textbf{928} & \textbf{1,471} & \textcolor{mblue}{82.0}
\end{tabular}
}
\vspace{-3mm}
\end{table}

\textbf{Speed and performance comparisons with more ViTs on different GPUs.} \label{sec:more_speed_test}
We compare the inference speed with more models and on more types of GPUs. Table~\ref{tab:more_vit_compare} reports the results. It shows that LITv2-S still achieves consistent faster throughput (images/s) than many ViTs on NVIDIA A100, Tesla V100, RTX 6000, and RTX 3090. It is also worth noting that under similar performance (82.0\%), LITv2-S is 2.1 faster than PVTv2-B2~\cite{pvtv2}, 1.7 faster than XCiT-S12~\cite{xcit} and ConvNext-Ti~\cite{convnext}, and 3.5 faster than Focal-Tiny~\cite{yang2021focal} on V100, which is another common GPU version for speed test in previous works~\cite{swin,deit,convnext,zhang2022vsa}.

\textbf{Throughput comparisons with more attention mechanisms on CPUs and GPUs.} In Figure~\ref{fig:hilo_speed_cpu_gpu}, we show that HiLo is consistently faster than many attention mechanisms~\cite{pvt,swin,xcit,linformer,performer,dong2021cswin,dat,yang2021focal,rao2022hornet,tang2022quadtree,van,vit} on both CPUs and GPUs. In particular, under CPU testing, HiLo is 1.4 faster than SRA~\cite{pvt}, 1.6 faster than local window attention~\cite{swin} and 17.4 faster than VAN~\cite{van}.
Detailed benchmark configurations can be found in the supplementary material.


\begin{figure}[]
	\centering
\includegraphics[width=1.0\linewidth]{figures/hilo_cpu_gpu.pdf}
\caption{Throughput comparisons with more attention mechanisms on CPUs and GPUs based on a single attention layer and 1414 feature maps.}
	\label{fig:hilo_speed_cpu_gpu}
	\vspace{-7mm}
\end{figure}

