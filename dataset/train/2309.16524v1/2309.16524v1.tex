\documentclass{article}

\usepackage[preprint]{corl_2023} 

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}



\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage{pgfplots, pgfplotstable}
\usepgfplotslibrary{groupplots}
\usepackage{multirow}

\usepackage{xcolor}
\definecolor{darkcyan}{RGB}{15,144,144}
\definecolor{darkorange}{RGB}{202,112,5}
\definecolor{darkpurple}{RGB}{117,9,111}
\definecolor{lightred}{RGB}{230,29,72}
\definecolor{middlegreen}{RGB}{64,175,54}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\usepackage{booktabs}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}


\pgfplotstableread[col sep=comma]{data/hoffner/h_idle.csv}\hidle
\pgfplotstableread[col sep=comma]{data/hoffner/r_idle.csv}\ridle
\pgfplotstableread[col sep=comma]{data/hoffner/c_act.csv}\cact
\pgfplotstableread[col sep=comma]{data/hoffner/f_det.csv}\fdet
\pgfplotstableread[col sep=comma]{data/revised/h03_n03.csv}\revisedservethree
\pgfplotstableread[col sep=comma]{data/revised/h03_n05.csv}\revisedservefive
\pgfplotstableread[col sep=comma]{data/revised/h03_n07.csv}\revisedserveseven



\title{HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs}



\author{
Esteve Valls Mascaro, Daniel Sliwowski, Dongheui Lee\\
 Technische Universität Wien (TU Wien), Autonomous Systems Lab\\
 Institute of Robotics and Mechatronics (DLR), German Aerospace Center\\
\texttt{\{esteve.valls.mascaro, daniel.sliwowski, dongheui.lee\}@tuwien.ac.at}\\
\href{https://evm7.github.io/HOI4ABOT_page}{evm7.github.io/HOI4ABOT\_page}
}

\pgfplotsset{compat=newest}
\pgfplotsset{/pgfplots/error bars/error bar style={very thick}}
\pgfplotsset{
  every axis plot/.append style={very thick, black},
}

\begin{document}
\maketitle



\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/Overviewv2.png}
    \caption{\textbf{Overview of our HOI4ABOT framework.} A robot leverages RGB data to detect and anticipate the human-object interactions in its surroundings and assist the human in a timely manner. The robot anticipates the human intention of holding the cup, so it prepares itself for pouring by grabbing the bottle. The robot reacts to the human holding the cup by pouring water. }
    \label{fig:Overview}
\end{figure}

\begin{abstract}
Robots are becoming increasingly integrated into our lives, assisting us in various tasks. To ensure effective collaboration between humans and robots, it is essential that they understand our intentions and anticipate our actions. In this paper, we propose a Human-Object Interaction (HOI) anticipation framework for collaborative robots. We propose an efficient and robust transformer-based model to detect and anticipate HOIs from videos.  This enhanced anticipation empowers robots to proactively assist humans, resulting in more efficient and intuitive collaborations. Our model outperforms state-of-the-art results in HOI detection and anticipation in VidHOI dataset with an increase of 1.76\% and 1.04\% in mAP respectively while being 15.4 times faster. We showcase the effectiveness of our approach through experimental results in a real robot, demonstrating that the robot's ability to anticipate HOIs is key for better Human-Robot Interaction.
\end{abstract}

\keywords{Human-Object Interaction, Collaborative Robots, Human Intention}




\section{Introduction} In recent years, the field of robotics has witnessed significant interest in human-robot interaction (HRI), with a focus on enhancing the ability of robots to assist humans in various tasks ~\citep{hri1, hri2, hri3, Koppula2016}. To facilitate effective human-robot collaboration (HRC), it is crucial for the robot to possess an understanding of both the surrounding environment and the individuals within it, including their intentions.  For example, consider the scenario visualized in Fig. \ref{fig:Overview} where a robot assists a person in the kitchen. By recognizing the person's intention to prepare a drink and understanding their actions such as reaching for the cup, the robot can proactively provide the necessary support in a timely manner, such as picking up a bottle and pouring water. Therefore, by recognizing and anticipating human-object interactions (HOIs), the robot gets a solid understanding of the person's intention and better caters to their needs ~\citep{hri1}.

While HOI is a long-standing challenge in the computer vision community, most approaches only consider the detection of these interactions from single frames ~\citep{Xu2018, Ulutan2020, JunYI2023, Xiaoqian2022, Liao2022, park2023viplo}. However, to minimize the latency when a person is assisted by a robot, the detection is not enough, but the anticipation is needed ~\citep{hri_delay, hoffman5, GARCIA2020315}. Therefore, we consider the task of HOI detection and anticipation, and we propose to leverage temporal cues from videos to better understand human intention. HOI recognition in videos has been explored recently ~\citep{Chiou2021, Cong2021, Tu2022, NI2023103741}. In this paper, we propose a real-time deep learning architecture that combines pre-trained models with spatio-temporal consistency to successfully detect and anticipate HOIs. Our model outperforms the state-of-the-art in VidHOI dataset ~\citep{Chiou2021} in terms of accuracy and speed. Moreover, we ensemble our framework with behavior trees ~\citep{BehaviorTrees} to adapt in real-time the robot actions for better interaction with the human.  We implement our framework in a real robot and demonstrate the effectiveness of our approach in the pouring task, showcasing the robot's ability to anticipate HOIs and proactively assist the human while reducing latency in the execution.


The contributions of our paper are summarized next: 

\begin{itemize}
    \item A real-time transformer-based model for HOI detection and anticipation.
    \item A novel patch merging strategy to align image features to pre-extracted bounding boxes.
    \item To the best of our knowledge, we are the first to assess HOI anticipation in a real robot experiment for a collaborative task.
\end{itemize}

\section{Related Works} \subsection{Human Intention in Robotics}

Recognizing and predicting human intention is crucial to ensure seamless human-robot collaboration (HRC) ~\citep{hoffman5, GARCIA2020315,hri_intention, hri_intention2}. \citep{hoffman5} observed significant differences in the robot's contribution and commitment in an experiment of a human carrying car parts to a shared workspace with an anticipatory robot to assemble them.  Recent works in computer vision have highlighted the potential of harnessing human intention to better anticipate future human actions ~\citep{Mascaro_2023_WACV, IntentionBasura, IntentionRobots}.  In particular, ~\citep{IntentionRobots} leverages the detection of human-object interactions (HOIs) within a scene to understand this high-level intention of the individuals. Despite the benefits of using HOIs, their application in robotics from vision data has not been extensively explored ~\citep{hri1}. ~\citep{Koppula2016} proposes a conditional random field (CRF) to assess the feasibility of a robot executing a given task based on the anticipated human actions. The CRF predicts the next human actions by considering object affordances and positions in the future. However, ~\citep{Koppula2016} is not scalable to new tasks as the CRF relies on hand-crafted features. Instead, we train our model in the largest HOI video dataset available to learn robust features that enhance the robot's ability to anticipate human intention. Recently, ~\citep{scene_graph_hoi} proposed a spatial-attention network to extract scene graphs from images in an industrial scenario. However, ~\citep{scene_graph_hoi} neglects the time dependency in the task and does not anticipate the human intention to enhance HRC.  \citep{scengraph_robotplan1, scengraph_robotplan2, scengraph_robotplan3} also adopted scene graphs but focused on task planning.

\subsection{HOI Detection and Anticipation}
HOI focuses on localizing the humans and objects in a scene and classifying their interactions using a ⟨human, interaction, object⟩ triplet (e.g. ⟨person1, hold, cup⟩). HOI task has recently gained attention in the computer vision community due to its promising applications in downstream tasks, such as scene understanding~\citep{hoi_scene_understanding} or action recognition~\citep{hoi_har}. The primary focus is the detection of HOI from images~\citep{Xu2018, Ulutan2020, JunYI2023, Xiaoqian2022, Liao2022, park2023viplo}. Some~\citep{JunYI2023, Xiaoqian2022, Liao2022} adopt a one-stage approach, directly operating on the images to predict the HOI triplet. However, these methods require higher training resources and do not benefit from pre-trained object detections. On the contrary, ~\citep{Xu2018, Ulutan2020, park2023viplo} employ a two-stage method to first locate the objects and humans in the image using pre-trained models and then classify each interaction using multi-stream classifiers. In particular,~\citep{park2023viplo} uses a ViT transformer \citep{dosovitskiy2020vit} to extract the patched features and proposes Masking with Overlapped Area (MOA) to extract features per object or human through a self-attention layer. Our work shows that weighting the patched features is sufficient to outperform MOA while not requiring any additional parameters.

While processing individual frames may be adequate for HOI detection, we argue that HOI anticipation benefits from leveraging the temporal aspects inherent in these interactions. Several studies in HOI detection address this temporal dimension by focusing on videos ~\citep{Chiou2021, Cong2021, Tu2022, NI2023103741}. ~\citep{Tu2022} fuses patched features at multiple levels to generate instance representations utilizing a deformable tokenizer. ~\citep{Chiou2021} employs a two-stage model that uses 3D convolutions to merge features across the temporal dimension. ~\citep{Cong2021} also adopts a two-stage approach but relies on a spatio-temporal transformer \citep{transformer} to detect the interactions in videos. Finally, ~\citep{NI2023103741} extends the architecture from ~\citep{Cong2021} by concatenating the human and object temporal features and fusing them with the human gaze information using cross-attention. ~\citep{NI2023103741} is the first work to propose both HOI detection and anticipation in videos. Similarly to ~\citep{park2023viplo}, ~\citep{NI2023103741} also adopts focal loss ~\citep{focalloss} to tackle the HOI imbalance in training. We adopt the findings from ~\citep{NI2023103741} but observe their model to not be feasible to work in real-time. Moreover, ~\citep{NI2023103741} trains a unique model for each anticipation horizon in the future. Instead, we propose a novel real-time multi-head model that can detect and anticipate HOIs in a single step.


\subsection{Task and Motion Planning}
For a robot to effectively assist and collaborate with a human in a particular task, it needs to understand the structure and order of actions involved, enabling the robot to achieve desired goals ~\citep{10.1145/3583136}. Finite State Machines (FSM) have been the standard choice for representing the task structure for a long time ~\citep{fsm1, fsm2}. However, scaling FSM poses a challenge due to their lack of modularity and flexibility ~\citep{BehaviorTrees}. Recently, Behavior Trees (BT) ~\citep{BehaviorTrees} have gained popularity as they can facilitate task planning in HRC tasks ~\citep{BT_robot1, BT_robot2}, where the environment is dynamic. Our work adopts BT and defines its behavior based on the anticipated human intention and its uncertainty. Once a suitable chain of actions has been found by the task planner, motion planning is responsible for determining the low-level movements of the robot. Motion planning is a core problem in robotics ~\citep{MP1_sampl, MP2_sampl, MP3_opt, MP4_opt, DMPs_survey}. ~\citep{MP1_sampl, MP2_sampl} proposed to randomly sample points in the state space towards the goal. However, they consider humans as obstacles or constraints, not collaborators. Some approaches ~\citep{MP3_opt, MP4_opt} formulate motion planning as an optimization problem, but their applications in HRC are limited as determining the cost function related to humans is not trivial. Alternatively, motion generators can be learned from human demonstrations to obtain more natural movement ~\citep{DMPs_survey, MP_learning1}. Dynamic Movement Primitives (DMPs)~\citep{DMPs_survey} have been successfully employed in HRC, by dynamically adapting their parameters ~\citep{MP_learning2, MP_learning3, MP_learning4}.

\section{Methodology} In this section, we present our \textbf{H}uman-\textbf{O}bject \textbf{I}nteraction Anticipation for Coll\textbf{A}borative ro\textbf{BOT}s (\textbf{HOI4ABOT}) framework. First, we formulate the HOI detection and anticipation task. Then, we describe the integration of the deep learning architecture into the robot framework.


\subsection{Human-Object Interaction}

Let  be a frame sequence of duration . The goal is to predict the interaction class  in the subsequent time  between any human  and object  pair  observed during the video , where  .  A visual illustration of our HOI4ABOT architecture is depicted in Fig. \ref{fig:model_architecture}.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/ModelArchitecture_v3.jpg}
    \caption{\textbf{HOI4ABOT architecture overview.} We consider a video of  frames with the pre-extracted object and human bounding boxes . Our module initially extracts relevant features per frame (left) to later on detect and anticipate HOIs (right) later. First, a ViT backbone ~\citep{oquab2023dinov2} extracts patch-based local  and global  features per each frame . Then, we obtain features per human  and object  by aligning  to their bounding boxes, as shown in light blue. We also project each  to  using a box embedder ~\citep{posembed_fourier}, and the object category to  using CLIP \citep{clip}. Our Dual Transformer, shown in purple, leverages the human and object-constructed windows (sequences in red and blue respectively) through two cross-attention transformers, where ey, uery, and alue are used in the attention mechanism.  is a learnable parameter to learn the evolution of the location in time. Finally, we project the enhanced last feature from the Human Blender to detect and anticipate HOIs at several time horizons  in the future through our \textit{Hydra} head (shown in light green).}
    \vspace{-5mm}
    \label{fig:model_architecture}
\end{figure}


\textbf{Detection and tracking}. HOI4ABOT is a two-stage method. First, we leverage off-the-shelf state-of-the-art object detection and tracking methods to identify the bounding boxes , label , and track identifier  for any object  in the video .  represents a list of  pixel coordinates of the top-left corner and right-bottom corner of the bounding box that locates a given object  at each frame  of . We obtain the same information for each human .   In the second stage, we exploit each individual pair  to predict its interaction class  in a given time horizon  using various data modalities. This requires understanding the visual features of the pair, how their spatial relationship evolves through time  and also the intrinsic semantics of the object .

\textbf{Visual features}. We use Dinov2 \citep{oquab2023dinov2} as a pre-trained Visual Transformer (ViT) \citep{dosovitskiy2020vit} backbone to divide each frame  into  patches and project each patch  to a visual token  that encodes the image information of that patch . In total, the image encoder obtains  that captures the local visual features, plus the global context vector  of a frame . 

We develop a simple but efficient technique, called Patch Merger, to extract individual features per human and object from a frame through a single step. Let  be an object  with its box  at frame . First, we create a binary mask for , where  denotes a pixel laying within . We convert the binary mask in a sequence of patches following \cite{dosovitskiy2020vit}. Then, we obtain a weighting vector  by computing the percentage that  overlaps each patch using 2D Average Pooling and normalization. Finally, we compute the weighted sum of local visual features , obtaining the individual representation of . Compared to \citep{park2023viplo}, which normalizes along the patch dimension and uses a quantized sequence as the attention mask for a self-attention layer, our algorithm is parameter-free, more efficient, and shows better performance in our experiments.

We propose to capture the context within a frame using , contrary to the spatial transformer proposed in \citep{NI2023103741}. We claim that this context (e.g. a kitchen, an office) should be invariant in short time periods and be the dominant component among all  tokens. Consequently,  we use Average Pooling to reduce the N  features to a single representation , which is the context of the scene. 

\textbf{Spatial features}. For each bounding box , we extract the  normalized pixel coordinates for the top-left corner and right-bottom corner. Then, we adopt a positional encoding using random spatial frequencies ~\citep{posembed_fourier} to embed the location of each point and merge these two corner representations into one box representation  using a fully connected layer.  This process is also applied to humans, thus obtaining  to encode each human  position in the scene.

\textbf{Object semantics}. Leveraging the object semantics is essential to understanding the possible interactions in a given pair. While `holding a cup' or `holding a bottle' are both feasible, `holding a car' becomes more unrealistic. Thus, we extract object semantic information   per object  to facilitate the model predicts the intention class . For that, we use the CLIP text encoder \citep{clip}.


\textbf{Pair Interaction. }
We construct a temporal architecture that leverages the evolution of the interactions between a human  and an object  in time. We process each pair independently, and therefore we focus on a single pair in the formulation. We stack both the visual tokens  and the spatial features  in time and construct a human temporal window . Similarly, we also construct an object's temporal window . We add a sinusoidal positional encoding to   and , Later, we prepend the global visual feature and a learnable spatial parameter  to .  learns the evolution of the location of the human in time through the attention mechanism. We also extend   by prepending the semantic token  that encodes the object type.  Therefore, we obtain a temporal feature  and  per pair. 

To extract the HOI relationships between  and , we train a dual transformer with cross-attention layers. First, an Object Blender transformer enhances the object window  based on the human knowledge . Then, the blended object features  are used to extend the human representation  in the Human Blender transformer to . Finally, we extract the last token from , which encodes the most current status of the scene, and classify the interaction pair  using a fully connected layer. As a given human-object pair can have multiple interactions simultaneously, we use a sigmoid function and define a threshold to classify the current interactions.

\textbf{Multi-head classification for multiple future horizons. }
The goal is to predict the interaction class  in the subsequent time  between any human  and object  pair . We considered  the problem of HOI detection () and also the anticipation in multiple future horizons (). Contrary to \cite{NI2023103741} that proposes one trained model for each , we developed a single model that can predict multiple time horizon interactions. For that, we froze the HOI4ABOT trained in the detection task, and train an additional linear layer that projects the last token from  to the interaction for the particular . We call this shared backbone the \textit{Hydra} variant, which allows us to simultaneously predict interactions across multiple , making our model faster and more efficient. We consider our \textit{Hydra} variant with  number of heads.

\subsection{Motion generation and task planning}
\textbf{Motion Generation.}
The proposed framework segments the complex movements into simpler movement primitives, which are learned with DMPs. To collect demonstrations of each movement primitive, we employ kinesthetic teaching, where an operator guides the robot's end effector by physically manipulating it ~\citep{KinestheticTeaching}. Generating the motion requires estimating the goal position, which we obtain through the use of a calibrated vision system that relies on a pre-trained object detector (i.e. YOLOv8 \citep{Jocher_YOLO_by_Ultralytics_2023}) and a depth camera. The position of the goals with respect to the robot base is computed using the intrinsic and extrinsic camera matrices.




\textbf{Task planning.}
Properly scheduling the acquired movement primitives is crucial to reach a desired goal. We implement Behavior Trees (BT) ~\citep{BehaviorTrees} as a ROS node that subscribes to the predicted HOIs and their confidence. The reactiveness of BTs allows adapting the robot's behavior by considering the anticipated human intention and changing to the appropriate sub-tree if needed. This is motivated by how humans interact with each other. For example, if a bartender observes a client approaching the bar, they can prepare for the interaction by grabbing a glass, thus reducing the serving time. 


\textbf{Robot control.}
The generated poses from the motion generator are passed to the controller. In our system, we employ a Cartesian impedance controller~\citep{4788393, 10.1115/1.3140713} to achieve the compliant behavior of the manipulator. This controller enhances the safety of human-robot collaboration by allowing the robot to respond in a compliant manner to external forces and disturbances.

\section{Experiments} 

\subsection{Dataset and Metrics}
We train and evaluate our model on the VidHOI dataset~\citep{Chiou2021}, the largest dataset available for human-object interactions in videos. This dataset encompasses 7.3 million frames with 755,000 annotated interactions of one frame per second. To assess the performance of our approach, we adopted the same evaluation metrics as those presented in~\citep{NI2023103741}. We computed the mean average precision (mAP) using the method presented in~\citep{tamura_cvpr2021}. The mAP@50 incorporates the precision-recall curves for all interaction classes. To determine a correct HOI triplet, three conditions need to be met: (i) the detected bounding boxes for human and object must overlap with their corresponding ground truths with an Intersection over Union (IoU) of 50 \%, (ii) the predicted object category is correct, (iii) the predicted interaction is correct. Following standard evaluation in VidHOI, we report mAP across three different HOI sets: (i) Full: all interaction categories, (ii) Non-Rare: frequent interactions in the validation set (more than 25 appearances), (iii) Rare: non-frequent interactions (less than 25). Additionally, we evaluated our approach in  \textit{Oracle mode}, where we use the human and object detections from ground truth, and in \textit{Detection mode}, where those are predicted using YOLOv5~\citep{YOLOv5} as in ~\citep{NI2023103741}. Finally, we computed the Person-wise top-k metrics~\citep{NI2023103741} where the anticipation was considered correct if one of the top-k predicted interactions matched the ground truth. 


\subsection{Quantitative evaluation}
\begin{table}[]
    \begin{minipage}[t]{.4\linewidth}
        \centering
        \caption{Detection mAP.}
        \resizebox{.95\linewidth}{!}{\begin{tabular}{lccc}
            \toprule
Method& Full & Non-Rare & Rare \\ 
             \midrule\midrule
             \multicolumn{4}{c}{\textbf{Oracle Mode}}\\

             \midrule
            ST-HOI~\citep{Chiou2021}         & 17.6           & 27.2           & 17.3           \\
            QPIC~\citep{tamura_cvpr2021}          & 21.4           & 32.9           & 20.56          \\
            TUTOR~\citep{Tu2022}          & 26.92          & 37.12          & 23.49          \\
            STTran~\citep{Cong2021}        & 28.32          & 42.08          & 17.74          \\
            ST-Gaze~\citep{NI2023103741}         & 38.61          & 52.44          & 27.99          \\
            \midrule
            Ours (\textit{Dual})    & {\ul 40.37}    & \textbf{54.52}    & {\ul 29.5}     \\
            Ours (\textit{Stacked}) & \textbf{40.55} & \uline{53.94} & \textbf{30.26} \\
            \midrule\midrule
            \multicolumn{4}{c}{\textbf{Detection Mode}}\\
\midrule
            STTran~\citep{Cong2021} & 7.61  & 13.18 & 3.33 \\
            ST-Gaze~\citep{NI2023103741} & 10.4  & 16.83 & 5.46 \\
            \midrule
            Ours (\textit{Dual})         & \textbf{11.12} & \textbf{18.48} & \textbf{5.61} \\
            Ours (\textit{Stacked}) & {\ul 10.79}    & {\ul 17.79}    & {\ul 5.42}    \\
            \bottomrule
        \end{tabular}}
        \label{tab:det_detection}
    \end{minipage}
     \hspace{0.15cm}
    \begin{minipage}[t]{.59\linewidth}
        \centering
        \caption{Anticipation mAP in Oracle mode.}
        \resizebox{0.95\textwidth}{!}{\begin{tabular}{lcccccc}
        \toprule
        \multirow{2}{*}{Method}                 & \multirow{2}{*}{} & \multirow{2}{*}{mAP} & \multicolumn{4}{c}{Preson-wise top-5} \\
        \cmidrule{4-7}
                                                &                    &                           & Rec     & Prec    & Acc     & F1      \\
        \midrule
        \multirow{3}{*}{STTran~\citep{Cong2021}}          & 1 & 29.09          & \textbf{74.76} & 41.36          & 36.61          & 50.48          \\
                                         & 3 & 27.59          & \textbf{74.79} & 40.86          & 36.42          & 50.16          \\
                                         & 5 & 27.32          & \textbf{75.65} & 41.18          & 36.92          & 50.66          \\
        \midrule
        \multirow{3}{*}{ST-Gaze~\citep{NI2023103741}}          & 1 & 37.59          & 72.17          & 59.98          & 51.65          & 62.78          \\
                                         & 3 & 33.14          & 71.88          & 60.44          & 52.08          & 62.87          \\
                                         & 5 & 32.75          & 71.25          & 59.09          & 51.14          & 61.92          \\
        \midrule
        \multirow{3}{*}{\shortstack[l]{Ours \\ (\textit{Dual, Scratch})}} & 1 & \textbf{38.46} & 73.32          & {\ul 63.78}    & {\ul 55.37}    & {\ul 65.59}    \\
                                         & 3 & {\ul 34.58}    & 73.61          & {\ul 61.7}     & {\ul 54}       & {\ul 64.48}    \\
                                         & 5 & {\ul 33.79}    & 72.33          & {\ul 63.96}    & {\ul 55.28}    & {\ul 65.21}    \\
        \midrule
        \multirow{3}{*}{\shortstack[l]{Ours \\ (\textit{Dual, Hydra})}}    & 1 & {\ul 37.77}    & {\ul 74.07}    & \textbf{64.9}  & \textbf{56.38} & \textbf{66.53} \\
                                         & 3 & \textbf{34.75} & {\ul 74.37}    & \textbf{64.52} & \textbf{56.22} & \textbf{66.4}  \\
                                         & 5 & \textbf{34.07} & {\ul 73.67}    & \textbf{65.1}  & \textbf{56.31} & \textbf{66.4}   \\
        \bottomrule
        \end{tabular}}
        \label{tab:hoi_anticipation}
    \end{minipage}
\end{table} 

HOI4ABOT outperforms state-of-the-art models \citep{Chiou2021, tamura_cvpr2021, Tu2022,  Cong2021, NI2023103741} in terms of accuracy and speed across all different tasks and scenarios, as shown in Table \ref{tab:det_detection} and Table \ref{tab:hoi_anticipation}. Moreover, Table \ref{tab:hoi_anticipation} shows how our \textit{Hydra} variant outperforms all models in the anticipation task, even training from scratch a separate model for each anticipation horizon. We consider that the detections provide a great deal of information regarding what a human is doing now, and what they might be interested in doing next. By using the \textit{Hydra} variant we ground the anticipation to what is happening at the present time.

\subsection{Ablation study}
This section analyses our proposed approaches and their impact on the performance of the HOI task. All results are depicted in Table \ref{tab:ablation}. For simplification, we only consider the HOI detection task. 

\begin{wraptable}{r}{0.45\linewidth}
\centering
    \caption{Albation study in HOI detection.}
        \begin{tabular}{lc}
        \toprule
        Variant                    & mAP           \\
        \midrule
        Feature blender = \textit{MOA}         & 40                              \\
        Interaction token = \textit{Learnable} & 40.29                           \\
        Main branch = \textit{Object}          & 39.85                           \\
        \midrule
        Transformer type = \textit{Single}     & 40.26                           \\
        Transformer type = \textit{Stacked}    & \textbf{40.55}                  \\
        \textit{Dual}                          & \uline{40.37}                  \\
        \bottomrule
        \end{tabular}\vspace{-0.5cm}
    \label{tab:ablation}
\end{wraptable}
 
Firstly we explore different variations in the extraction and arrangement of features to compose the human and object windows. We compare our Patch Merger strategy to the MOA strategy from \citep{park2023viplo}. Using MOA requires an additional self-attention block, which increases the model's parameters while underperforming. Moreover, we explore different feature aggregation strategies to classify an interaction. Instead of using the last observed token in  for classification, we prepend an additional learnable token to  which aggregates the interaction relationships, inspired by the ViT class token \citep{dosovitskiy2020vit}. However, Table \ref{tab:ablation} shows that classifying from the last observed features is better while not requiring additional parameters. Last, we consider varying the order of the cross-attention branches, first the Human Blender and second the Object Blender. We claim that the decrease in performance is due to the different behavior between humans and objects: objects are static and therefore less informative than humans, which are dynamic and lead the interaction.



Secondly, we assess our dual transformer by comparing it with other variants. We consider the \textit{Single} variant when only using the Human Blender transformer, which is not able to effectively capture the HOIs. We also consider stacking both  and  to a single feature window pair, . 
We observe slight improvements in this variant in terms of mAP when detecting in the \textit{Oracle mode}, but it underperforms in the \textit{Detection mode} and for the anticipation tasks, as shown in Appendix~\ref{apx:comparison}.

Finally, we compare the inference time of our model to \citep{NI2023103741}  to assess the efficiency in real-world applications in robots. Our \textit{Dual} variant is  times faster than \citep{NI2023103741} for the detection task. \citep{NI2023103741} requires extracting gaze maps, which drastically slows down the inference speed of their model. When using our \textit{Hydra} model, we obtain interactions for the time horizons 0, 1, 3, and 5 using one forward pass, with nearly the same inference speed and parameters as using one head. More information can be found in Appendix~\ref{apx:inference}.


\subsection{Real World Experiments}

HOI detection and anticipation are essential for robots to comprehend the surrounding humans and better predict their needs, so the robot can assist in a timely manner.  We conduct real experiments with a Franka Emika Panda robot to showcase the benefit of our approach in collaborative robots beyond the offline VidHOI dataset.  The VidHOI dataset contains user-collected videos of humans, mostly performing outdoor activities that can not be easily related to robotic collaboration tasks. We consider the `pouring task' in a kitchen scenario where the robot assumes the role of a bartender with the goal of pouring a beverage for the human. The scenario is shown in Fig. \ref{fig:Overview}. To assess the performance of our model in unseen scenarios, we collected 20 videos of 5 people in our kitchen lab. The human is instructed to grab the cup and informed that the robot will assist them in the task. We manually annotate the time the person grabs the cup to use as ground truth. Our \textit{Hydra} variant detects and anticipates the HOI between a person and a cup in real-time. When the robot anticipates that the human will be near the cup, it proceeds to grab the bottle. However, if the human moves away the robot releases the bottle and returns to the initial pose. The robot proceeds to pour the liquid into the cup after detecting that the human is holding it. 

We assess our real-world experiments by considering well-established metrics in HRC \citep{GARCIA2020315}. \citep{GARCIA2020315} proposes to evaluate human-robot fluency in the joint task by considering four objective metrics. \textit{Human Idle Time} (H-IDLE) and \textit{Robot Idle Time} (R-IDLE) are proposed to evaluate the percentage of the total task time that the respective agent is not active, which reflects the team coordination and the inefficiency of the agent in the task.  \textit{Concurrent Activity} (C-ACT) measures the percentage of total task time in which both agents are active concurrently (the action overlap between different members). A higher  C-ACT indicates a better-synchronized team. \textit{Functional Delay} (F-DEL) measures the delay experienced by the agents immediately after completing an activity: the percentage of total task time between the completion of one agent's action and the beginning of the other agent's action. A negative F-DEL indicates that actions are overlapping and implies an efficient use of team members' time. Figure \ref{fig:main_hoffner} summarizes the average objective fluency metrics across our pouring experiments. The results indicate that HOI anticipation allows for better human-robot coordination and efficiency of each other's time, thus making the task more fluent. We observe a substantial improvement in Figure \ref{fig:main_hoffner} when using anticipation () compared to detection (). Additional quantitative and qualitative results are provided in Appendix~\ref{apx:experimets}.

\begin{figure}[!t]
    \begin{minipage}[]{\linewidth}
\begin{tikzpicture}
        \begin{groupplot}[group style={group size=4 by 1, horizontal sep=1cm},height=4cm,width=4cm]
            \nextgroupplot[title=Human Idle Time, xtick={0, 1, 3, 5},ymajorgrids, xlabel={}, ylabel={Percentage [\%]}]
                \addplot[mark=square,
                     red]
                table [col sep=comma, x=future, y=0.3] {\hidle};\addplot[mark=square,
                     darkcyan]
                table [col sep=comma, x=future, y=0.5] {\hidle};\addplot[mark=square,
                     darkpurple]
                table [col sep=comma, x=future, y=0.7] {\hidle};

            \nextgroupplot[title=Robot Idle Time, xtick={0, 1, 3, 5}, ymajorgrids, xlabel={}]
                \addplot[mark=square,
                     red]
                table [col sep=comma, x=future, y=0.3] {\ridle};\addplot[mark=square,
                     darkcyan]
                table [col sep=comma, x=future, y=0.5] {\ridle};\addplot[mark=square,
                     darkpurple]
                table [col sep=comma, x=future, y=0.7] {\ridle};

            \nextgroupplot[title=Concurent Activity, xtick={0, 1, 3, 5}, ymajorgrids, xlabel={}, legend style={at={(0.72,.285)},anchor=north west}]
                \addplot[mark=square,
                     red]
                table [col sep=comma, x=future, y=0.3] {\cact};\addplot[mark=square,
                     darkcyan]
                table [col sep=comma, x=future, y=0.5] {\cact};\addplot[mark=square,
                     darkpurple]
                table [col sep=comma, x=future, y=0.7] {\cact};

            \nextgroupplot[title=Functional Delay, xtick={0, 1, 3, 5}, ymajorgrids, xlabel={}]
                \addplot[mark=square,
                     red]
                table [col sep=comma, x=future, y=0.3] {\fdet};\addplot[mark=square,
                     darkcyan]
                table [col sep=comma, x=future, y=0.5] {\fdet};\addplot[mark=square,
                     darkpurple]
                table [col sep=comma, x=future, y=0.7] {\fdet};\end{groupplot}
\end{tikzpicture}
    \vspace{-0.5cm}
    \captionsetup{type=figure}
    \centering
    \captionof{figure}{Mean objective fluency metrics for pouring experiments for different confidence thresholds \{\textcolor{red}{0.3}, \textcolor{darkcyan}{0.5}, \textcolor{darkpurple}{0.7}\} in the HOIs prediction. }
    \label{fig:main_hoffner}
\end{minipage}
 \end{figure}

\section{Limitations} Despite outperforming state-of-the-art models in HOI from videos, we observe from qualitative experiments the challenge of the implementation in the real world. First, there is a domain gap between the VidHOI dataset, mainly representing humans in daily scenes and our robotic scenario. For instance, anticipating that `a human is holding a cup' is challenging, despite being correctly detected. We explore the VidHOI dataset and observe that most people already appear with the cup in their hand. To overcome this issue, we sample more frequently clips where the interaction changes in the anticipation horizon. Still, this is insufficient to ensure correct anticipation in `holding a cup' with higher confidence. Other datasets are not better suited for our problem as they mainly are image-based \citep{Xu2018, gupta2015visual} or do not track the humans and objects in videos \citep{ji2020action}. Future research directions consider training with a dataset more coupled to our robotics scenario to improve the model predictions. This would allow us to extend our experiments to more complex daily scenarios. Second, in our real experiments, we assume that the objects present in the scene are sufficiently visible so that object detection can recognize them. Finally, the employed DMPs could be expanded or replaced by visual servoing to consider goal-following behaviors.

\section{Conclusions} In this paper, we proposed a \textbf{H}uman-\textbf{O}bject \textbf{I}nteraction Anticipation for Coll\textbf{A}borative ro\textbf{BOT}s framework (\textbf{HOI4ABOT}). We consider the task of detecting and anticipating human-object interactions (HOI) in videos through a transformer architecture. We train and evaluate HOI4ABOT in the VidHOI dataset and outperform current state-of-the-art across all tasks and metrics while being  faster. Moreover, our model runs in real-time thanks to our efficient design. Additionally, we extend our HOI4ABOT model with a multi-head architecture, which can detect and anticipate HOIs across different future horizons in a single step. We demonstrate the effectiveness of our approach by implementing our model in a Franka Emika Panda robot. We show that anticipating HOIs in real-time is essential for a robot to better assist a human in a timely manner and we support our findings with real experiments. In conclusion, our approach demonstrates its effectiveness and defines a new road to explore, where intention reading plays a crucial role for robots in collaboration scenarios. 




\clearpage

\acknowledgments{This work is funded by Marie Sklodowska-Curie Action Horizon 2020 (Grant agreement No. 955778) for the project ’Personalized Robotics as Service Oriented Applications’ (PERSEO).}



\bibliography{main}  

\clearpage
\appendix
\section*{Appendix}
\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}

\section{Implementation Details} \label{apx:implementation}
In this section, we offer a comprehensive summary of the implementation details to aid in the reproduction of the experiments and the replication of the results. All experiments were conducted using a single NVIDIA RTX A4000 graphics card with 16GB of memory and an Intel i7-12000K CPU. 

\textbf{Hyperparameters.} All trained models are conducted using the same strategy as \citep{NI2023103741}. We use the official code from \href{https://github.com/nizhf/hoi-prediction-gaze-transformer}{https://github.com/nizhf/hoi-prediction-gaze-transformer} and implement our HOI4ABOT model into their framework. All training settings are summarized in Table \ref{tab:training_settings}. We adopt Cross Binary Focal Loss \citep{focalloss} with   and , which improves training in extremely imbalanced datasets, such as VidHOI \citep{Chiou2021}. We train our models using the AdamW optimizer \cite{adamw}. We define a scheduler for the learning rate, with an initial value of  that increases to a peak value of  in 3 warm-up epochs. The learning rate then decreases with an exponential decay with a factor . We run the training for 40 epochs.

\textbf{Model configuration.} All trained models use a similar configuration, but some variants such as \textit{Stacked} or \textit{Single} are adapted to ensure having a similar number of trainable parameters in the architecture (57.04M).  All models reported in our paper use the DINOv2 \citep{oquab2023dinov2} as the image feature extractor, using the smallest variant available ViT-B/14 that only contains 22.06M parameters; and CLIP \citep{clip} for the semantic extractor, with the largest available variant ViT-L/14 that contains 85.05M parameters. However, due to the fact that the number of objects in the dataset is limited, we pre-extracted the features for all possible objects.  For our baseline HOI4ABOT model, we consider two transformer models with cross-attention layers, each of them with depth 4 and MLP expansions of ratio 4.0. Each transformer uses the multi-head attention variant with 8 heads to better extract the relationships within a sequence of features. Moreover, we consider sinusoidal positional embedding to facilitate learning the temporal information of a sequence.  Finally, we consider the embedding size of each extracted feature, bounding box, or image feature, as 384. The embedding size for the prepended class token is also 384, as this is the embedding dimensions of the features extracted using DINOv2. For the semantics, CLIP obtains a feature of dimensionality 764.

\begin{table}[htbp]
    \begin{minipage}[t]{.5\linewidth}
        \raggedright
        \caption{Training settings.}
        \begin{tabular}{l|c}
        \toprule
        Optimizer         & AdamW               \\
        Weight Decay      & 1.0e-2               \\
        Scheduler         & ExponentialDecay \\
        Warmup Epochs     & 3        \\
        Initial LR            & 1e-8          \\
        Peak LR            & 1e-4               \\
        Exponential Decay  & 0.1            \\
        Epochs            & 40            \\
        Random Seed       & 1551                     \\
        Augmentation      & Horizontal Flip           \\
        Flip Ratio & 0.5               \\
        Batch Size        & 16                     \\
        Dropout           & 0.1              \\  \bottomrule      
        \end{tabular}\label{tab:training_settings}
    \end{minipage}\begin{minipage}[t]{.5\linewidth}
            \caption{Model settings.}
            \raggedleft
            \begin{tabular}{l|c}
            \toprule
            Transformer Depth         & 4               \\
            Number of Heads      & 8             \\
            Feature Extractor         & DINOv2: ViT-B/14 \citep{oquab2023dinov2} \\
            Semantic Extractor     & CLIP: ViT-L/14    \citep{clip}    \\
            Embedding Dimension            & 384          \\
            Positional Embedding            & Sinusoidal               \\
            Exponential Decay  & 0.1            \\
            Mainbranch            & humans            \\
            MLP ratio       & 4.0                     \\  \bottomrule      
            \end{tabular}\label{tab:modelsettings}
    \end{minipage}
\end{table}
 
\section{Experimental Scenario}
\label{apx:experimets}
\textbf{Task description}. Our HOI4ABOT framework enhances human intention reading through HOI anticipation. We conduct a real-world experiment with a Franka Emika Panda robot to support our proposed approach. Fig. \ref{fig:experiment_overview} provides a step-by-step overview of the considered bartender scenario. First, the robot detects a human in the scene and anticipates the human intention to approach a kitchen island.  When the robot anticipates with confidence that the human will be close to the cup, it executes a movement to grab the bottle, thus preparing for pouring. If the intention of the human changes, the robot adapts its behavior and moves back to the initial position after placing the bottle on the table. on the other hand, if the human proceeds to grab the cup, the robot pours the drink and goes back to its initial position. This preparatory behavior reduces the serving time while improving the overall experience for the human.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{images/Experiments.png}
    \caption{Real-world experiments scenario.}
    \label{fig:experiment_overview}
\end{figure}

\textbf{Additional Qualitative Results}
On the project website (\url{https://evm7.github.io/HOI4ABOT_page/}) we present additional qualitative results that showcase the ability of our model to operate in more ambiguous scenarios (with multiple objects and people instances, and cluttered scenes) and execute different motions depending on the predicted interactions. Our model predicts the interaction associated with specific human and object instances, which are associated with the identifiers obtained from the tracker. Therefore, we are able to execute different movements depending on the interaction (like pouring, pushing, or turning off the lights) and the object category (grabbing from the side in case of a cup or a bottle or grabbing from the top in case of a bowl) or instance (cup-1 and cup-2). For instance, we consider the case of multiple cups in the scene, where the robot conditions its pouring behavior based on the cup the human holds. Additionally, we also show the ability to operate in an ambiguous situation with multiple objects and people instances. 

\textbf{Evaluation of the use case scenario of HOI4ABOT}. 
To validate our hypothesis, we extend our evaluation of the framework in real-world experiments with additional quantitative metrics.  First, we assess the human waiting time until the robot proceeds to serve. Fig. \ref{fig:app_servetime} shows the quantitative benefit of our approach by considering the absolute time a human waits to be served (serving is considered until the robot starts pouring). The results indicate that our robot behaves proactively when anticipating HOIs and therefore reduces the time to wait until a drink is poured, compared to the reactive behavior observed if the robot is only detecting HOIs.  Fig. \ref{fig:app_servetime} shows a slight reduction in the waiting time when reducing the confidence threshold in the prediction: to be more confident in the human intention the robot waits more. In addition, we observe only a slight decrease in the waiting time for different anticipation horizons (). This subtle variation might be caused because of the dataset limitation pointed out in the main manuscript. Secondly, we measure the effectiveness of our robot pouring a drink in our real-world trials by considering the success rate of the pouring task in  new real-world experiments. Four lab members were instructed to approach the robot and grab the cup. Each person did 5 repetitions. Our framework correctly executes the pouring task in  out of  executions, resulting in a success rate of . 


\begin{figure}[t]
    \centering
        \begin{minipage}[t]{0.45\textwidth}
\pgfplotsset{width=7cm, height=6cm, compat=1.9}
    \begin{tikzpicture}
        \begin{axis}[
            grid=both,
            xlabel={},
            ylabel={Serving time [s]},
            xtick=data,
            xticklabels from table={\revisedservethree}{future},
            xlabel style = {xshift=0cm,yshift=0mm},
            legend cell align=center,
            legend style={at={(axis cs:2,6.5)},anchor=north west, nodes={scale=0.7, transform shape}},
            ymin=1,
            ymax=7,
        ]
        \addlegendimage{empty legend}
        \addplot[mark=x,
                 only marks,
                 red,
                 x filter/.code=\pgfmathparse{\pgfmathresult-0.2}]
            plot [error bars/.cd, y dir=both, y explicit]
            table [col sep=comma, x expr=\coordindex, y=M, y error plus=Sp, y error minus=Sp] {\revisedservethree};

        \addplot[mark=x,
             only marks,
             darkcyan,
             ]
        plot [error bars/.cd, y dir=both, y explicit]
        table [col sep=comma, x expr=\coordindex, y=M, y error plus=Sp, y error minus=Sp] {\revisedservefive};
        
        \addplot[mark=x,
                 only marks,
                 darkpurple,
                 x filter/.code=\pgfmathparse{\pgfmathresult+0.2}]
            plot [error bars/.cd, y dir=both, y explicit]
            table [col sep=comma, x expr=\coordindex, y=M, y error plus=Sp, y error minus=Sp] {\revisedserveseven};
        \legend{\hspace{-.3cm}\textbf{Confidence},0.3,0.5,0.7}
        \end{axis}
    \end{tikzpicture}
    \captionsetup{type=figure}
    \centering
    \captionof{figure}{Human waiting time to be served the drink for different confidence thresholds (   and anticipation heads .}
    \label{fig:app_servetime}
%
         \end{minipage}\hspace{1cm}
        \begin{minipage}[t]{0.45\textwidth}
            \centering
            \includegraphics[width=0.9\linewidth]{images/lfd.png}
            \vspace{0.4cm}
            \caption{\textbf{Quantitative evaluation of the pouring task.} We overlay on the image of the workspace of the robot the position of the bottle and the cup. Green signifies successful task execution and red failed cases.}
            \label{fig:app1}
        \end{minipage}\end{figure}




\section{Motion Generation and Task Planning}
\label{apx:motion}
\textbf{Motion Generation. }Our framework decomposes the complex movements into simpler movement primitives, which are learned with DMPs. For instance, the pouring task consists of multiple steps, like grabbing the bottle, moving to the cup, tilting the bottle, and placing the bottle back. Learning the entire movement as a single primitive is possible, but this might oversimplify the motion, particularly for sharp movements, compromising accuracy. In our experiments, each motion segment was learned from a single demonstration. We verified the success rate in the pouring scenario with 60 different arrangements of the objects `Bottle' and `Cup'. Figure \ref{fig:app1} in the attached document shows that our robot successfully pours  out of  executions, resulting in a success rate of . We can observe that failure cases occur mainly when the objects are arranged close to the non-reachable areas for the robot. Working close to the non-reachable zone is more problematic as the robot is operating near its kinematic constraints. However, this issue can be solved by rearranging the robot base position adequately to the user's need. 
    
\textbf{Task Planning: Behavior Tree. } In this section, we describe the structure of the Behavior Tree ~\citep{BehaviorTrees} used in our real-world experiments, which is shown in Fig.~\ref{fig:BT}. The primary focus of this work is to enhance human-robot collaboration through human intention reading using HOI anticipation. We conduct a simple real-world experiment with a Franka Emika Panda robot to showcase the benefits of our approach. This paper does not intend to provide a general development of BT for HOI tasks. However, the same methodology employed can be extended to more complex scenarios thanks to the modularity of BT.

\begin{figure}[]
    
\end{figure}

The entire tree is built from three sub-trees: the \textit{Pour branch}, the \textit{Approach branch}, and the \textit{Move Away branch}. First, the \textit{Pour branch} is responsible for pouring the liquid into the cup. It is executed once the bottle is grabbed, and the `hold' interaction between the human and the cup is detected. To achieve this conditional execution we add the \textit{Execute check} behavior at the beginning of the branch. Then, we reset the \textit{Grabbed flag} and set the \textit{Poured flag} to prevent any potential duplication of pouring into the cup. Secondly, the goal of the \textit{Approach branch} is to grab the bottle. This sub-tree is executed when the bottle is not currently grabbed and the robot anticipates the `next to' interaction with a confidence greater than a pre-defined threshold. Once the bottle is grabbed, the \textit{Grabbed flag} is set. Thirdly, the \textit{Move Away branch} is responsible for releasing the bottle and moving it back to its initial position. This branch is executed when the bottle is grasped by the robot and the robot anticipates the interaction `next to'  with a confidence lower than a predefined threshold. After executing the movements the \textit{Grabbed flag} is reset.

The appropriate sub-branch is selected by using the \textit{Main Selector} composite node. This node attempts to execute each sub-tree starting from left to right. The selector node executes the next branch in the sequence when the check in the preceding branch is not satisfied. Finally, the last behavior in the sequence is an \textit{Idle} behavior where the robot waits for a short period of time.

The root of the tree is a sequential node, which first collects all messages from the appropriate ROS topics, next checks if the beverage has been already poured, and finally executes the \textit{Main Selector}. To achieve continuous operation, the \textit{Root} node is decorated by a \textit{Repeat} modifier, which executes the root node indefinitely.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{images/BT_structure.png}
    \caption{Schematic of the Behaviour Tree for our HOI4ABOT framework. }
    \label{fig:BT}
\end{figure}

\section{Inference time} \label{apx:inference}
Our model is able to run in real-time thanks to the efficient design and reduced dimensionality. 

\textbf{Inference time versus the number of human-object pairs.} Due to the nature of HOIs, each interaction needs to be computed for each human-object pair existing in the scene at a given time step. Therefore, to speed up the results and parallelize the forward pass for a given video, we stack all found human-object pairs in the batch dimension. Still, we consider it necessary to observe how different models' inference speed is affected by the number of pairs in a given video. Therefore, we run  executions of our model processing a given video with  interactions. We implement all models reported in Fig. \ref{fig:inferencespeed_types} and \ref{fig:inferencespeed_variants} in the same batch strategy and observe a similar tendency in the increase of the inference time for a higher number of interactions.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            grid=both,
            name=plot1,
            xlabel={Number of interactions},
            ylabel={Inference time [ms]},
            ylabel shift = -2 pt,
            ymode=log,
            domain=0:150,
            legend cell align=left,
            legend style={at={(axis cs:100,800)},anchor=north west, line width=2 pt,draw=none},
            width=0.8\textwidth,
            height=6cm
        ]
            \addplot[mauve]{7.813264617*x+224.0524192};
            \addplot[darkpurple]{0.421150677*x+17.44697146};
            \addplot[middlegreen]{0.411178733*x+16.66521792};

            
            \addplot[
                only marks,
                mauve,
                mark=x,
                opacity=0.5]
            table[x=num_interaction, y=mean inference ms, col sep=comma]
            {data/results_Zhifan.csv};

            
            \addplot[
                only marks,
                darkpurple,
                mark=x,
                opacity=0.5]
            table[x=num_interaction, y=mean inference ms, col sep=comma]
            {data/Ours_dual.csv};

            
            \addplot[
                only marks,
                middlegreen,
                mark=x,
                opacity=0.5]
            table[x=num_interaction, y=mean inference ms, col sep=comma]
            {data/Ours_stacked.csv};
            \legend{ST-GAZE~\citep{NI2023103741}, Ours (Dual), Ours (Stacked)}
        \end{axis}

    \end{tikzpicture}
    \caption{Model performance depends on the number of interactions for different architectures. Our variants (`Dual' and `Stacked') have similar inference times (curves overlap) while outperforming by large margins the ST-GAZE model \citep{NI2023103741} }
    \label{fig:inferencespeed_types}



%
     \vspace{10 pt}
    \centering
    \begin{tikzpicture}    
        \begin{axis}[
            name=plot2,
            grid=both,
xlabel={Number of interactions},
            ylabel={Inference time [ms]},
            ylabel shift = -2 pt,
domain=0:150,
            legend cell align=left,
            legend style={at={(axis cs:-15,150)},anchor=north west, line width=2 pt,draw=none},
            width=0.8\textwidth,
            height=6cm
            ]

            \addplot[darkcyan]{0.850369641*x+20.13450812};
            \addplot[darkorange]{0.430100167*x+17.35292587};
            \addplot[darkpurple]{0.421150677*x+17.44697146};
            
            
            \addplot[
                only marks,
                darkcyan,
                mark=x,
                opacity=0.5]
            table[x=num_interaction, y=mean inference ms, col sep=comma]
            {data/Dual_both.csv};

            
            \addplot[
                only marks,
                darkorange,
                mark=x,
                opacity=0.5]
            table[x=num_interaction, y=mean inference ms, col sep=comma]
            {data/results_HYDRA_FIX.csv};


            
            \addplot[
                only marks,
                darkpurple,
                mark=x,
                opacity=0.5]
            table[x=num_interaction, y=mean inference ms, col sep=comma]
            {data/Ours_dual.csv};
            \legend{Dual Detection + Anticipation, Dual Hydra, Dual Detection}
        \end{axis}
    \end{tikzpicture}
    \caption{Model performance depends on the number of interactions for different model variants. The proposed multi-head approach allows us to detect and anticipate HOIs at multiple time horizons while maintaining a similar inference speed as the `Dual' version (purple and dark orange curves overlap). We observe the benefit of the Hydra compared to running a specific `Dual' transformer per detection and per anticipation.}
    \label{fig:inferencespeed_variants}



%
 \end{figure}

\textbf{Efficiency comparison with current state-of-the-art \cite{NI2023103741}.} Both HOI4ABOT and \cite{NI2023103741} adopt a transformer-based architecture to comprehend the temporal relationships between the humans and objects in the scene. However, our model is designed to be efficient and to run in real-time despite having a large number of interactions, contrary to \cite{NI2023103741}. The comparison of the efficiency of both models is depicted in Fig. \ref{fig:inferencespeed_types}, which shows that our HOI4ABOT outperforms ~\citep{NI2023103741} by large margins in terms of speed. Next, we list the major differences in the model design that cause our improvement. First, we do not use any additional modality to predict HOIs, compared to ~\citep{NI2023103741} that leverages pre-extracted gaze features to capture the human's attention. Predicting these gaze features is costly as it requires detecting and tracking each human's head in the scene, predicting the corresponding gaze per human, and matching it to the corresponding body. Thus the speed decreases considerably depending on the number of humans in the scene. Moreover, ~\citep{NI2023103741} also considers an initial spatial transformer that leverages all humans and objects per frame, thus ~\citep{NI2023103741} speed is more affected by the number of frames considered.

\textbf{Efficiency comparison of the \textit{Hydra} HOI4ABOT.} Human intention reading requires understanding both current and future HOIs. Therefore, we develop a multi-head HOI4ABOT, called \textit{Hydra}, that allows us to predict HOIs at different time horizons in the future through a single forward step. While Table \ref{tab:anticipationAll} shows the benefit of our \textit{Hydra} variant compared to training from scratch, in this subsection we focus on the benefit of efficiency. Fig. \ref{fig:inferencespeed_variants} shows the inference time in milliseconds depending on the number of human-object pairs across different variants. We consider the \textit{Dual Detection} as the baseline of our HOI4ABOT model when only predicting the HOI in the present. \textit{Dual Detection + Anticipation} is an optimized model that uses two dual transformer blocks that benefit from the same image backbone, one for HOI detection and the other for HOI anticipation in a single future . Finally, our \textit{Dual Hydra} performs HOI detection and anticipation for  in a single step by using our multi-head strategy. We observe the benefit of our \textit{Hydra} variant compared to the model ensemble, as it has a comparable speed to the single head while anticipating HOIs in three additional future horizons.

\section{Extensive comparison with variants}
\label{apx:comparison}
\begin{table}[t]
\centering
\caption{Anticipation mAP in Oracle mode.}
\label{tab:anticipationAll}
\resizebox{0.65\textwidth}{!}{\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method}             & \multirow{2}{*}{t} & \multirow{2}{*}{mAP} & \multicolumn{4}{c}{Preson-wise top-5}                             \\
    \cmidrule{4-7}
                                    &                    &                           & Rec            & Prec           & Acc            & F1             \\
\midrule
\multirow{3}{*}{STTran~\cite{Cong2021}}             & 1                  & 29.09                     & \textbf{74.76} & 41.36          & 36.61          & 50.48          \\
                                    & 3                  & 27.59                     & \textbf{74.79} & 40.86          & 36.42          & 50.16          \\
                                    & 5                  & 27.32                     & \textbf{75.65} & 41.18          & 36.92          & 50.66          \\
\midrule
\multirow{3}{*}{ST-Gaze~\cite{NI2023103741}}             & 1                  & 37.59                     & 72.17          & 59.98          & 51.65          & 62.78          \\
                                    & 3                  & 33.14                     & 71.88          & 60.44          & 52.08          & 62.87          \\
                                    & 5                  & 32.75                     & 71.25          & 59.09          & 51.14          & 61.92          \\
\midrule
\multirow{3}{*}{Ours (\textit{Dual, scratch})}    & 1                  & \textbf{38.46}            & 73.32          & 63.78          & 55.37          & 65.59          \\
                                    & 3                  & 34.58                     & 73.61          & 61.7           & 54             & 64.48          \\
                                    & 5                  & 33.79                     & 72.33          & 63.96          & 55.28          & 65.21          \\
\midrule
\multirow{3}{*}{Ours (\textit{Dual, Hydra})}       & 1                  & 37.77                     & {\ul 74.07}    & {\ul 64.9}     & \textbf{56.38} & \textbf{66.53} \\
                                    & 3                  & {\ul 34.75}               & {\ul 74.37}    & {\ul 64.52}    & {\ul 56.22}    & \textbf{66.4}  \\
                                    & 5                  & 34.07                     & {\ul 73.67}    & {\ul 65.1}     & {\ul 56.31}    & \textbf{66.4}  \\
\midrule
\multirow{3}{*}{Ours \textit{(Stacked, Scratch)}} & 1                  & 36.14                     & 70.03          & 64.61          & 53.99          & 64.34          \\
                                    & 3                  & 34.65                     & 73.85          & 62.13          & 54.15          & 64.77          \\
                                    & 5                  & {\ul 34.27}               & 72.29          & 61.81          & 53.65          & 64.03          \\
\midrule
\multirow{3}{*}{Ours \textit{(Stacked, Hydra)}}    & 1                  & {\ul 37.8}                & 72.05          & \textbf{65.58} & {\ul 56.23}    & {\ul 66.09}    \\
                                    & 3                  & \textbf{34.9}             & 72.96          & \textbf{65.05} & \textbf{56.3}  & {\ul 66.2}     \\
                                    & 5                  & \textbf{35}               & 72.86          & \textbf{65.18} & \textbf{56.36} & {\ul 66.2}    \\
\bottomrule
\end{tabular}}
\label{tab:anticipation_all}
\end{table} 
Our HOI4ABOT model outperforms the current state-of-the-art across all tasks and metrics in the VidHOI dataset, as shown in Tabel~\ref{tab:anticipation_all}. In this section, we extend the comparison from the manuscript for the HOI anticipation for our \textit{Dual} and \textit{Stacked} variants, both when being trained by scratch or through the multi-head \textit{Hydra} mode. Our results show that the \textit{Stacked} variant obtains slightly better performance in the mAP for longer futures. We consider this marginal improvement to be motivated because of the width difference in the transformer blocks, as well as the bigger representation space from which we project when classifying the HOIs. The \textit{Stacked} variant is based on a single self-attention block that operates on the human windows and object windows stacked in time. Therefore, the \textit{Stacked} transformer has double the width compared to the \textit{Dual} variant. Given that the output of a transformer model has the same shape as its input, the obtained tokens are also wider in the \textit{Stacked} variant. Having a bigger embedding dimension in the projected token allows the encoding of more information, which could result in better performance. However, Table \ref{tab:anticipationAll} shows that the \textit{Stacked} variant has a lower recall and therefore lower F1-Score. These findings might indicate that the \textit{Stacked} variant struggles when anticipating HOIs in the videos where the interaction changes in the anticipation horizon, being more conservative in its predictions. Therefore, we consider the \textit{Dual} variant to be optimal as it balances both precision and recall metrics across all tasks, as shown by outperforming all other models in the F1-score for the \textit{Hydra} version.


\end{document}
