\section{Introduction}


In the last several decades there has been impressive progress in the
development and application of automated software
testing~\cite{king1976sea, dart, CadarGPDE2006, SenMA2005} and formal
verification techniques~\cite{floyd, floyd71,
  DBLP:journals/cacm/Hoare69, clarke}. Despite these gains,
establishing trustworthiness of software systems remains a notoriously
difficult and expensive problem. Scalably guaranteeing 100\%
compliance of large software systems with respect to (w.r.t) rich
specifications remains an open challenge. In fact, users simply assume
that software systems will never be completely free of security
errors, despite our best efforts.

In response to this seeming inevitability of exploitable errors in
software, security researchers have proposed ingenious defense
mechanisms (e.g., address space layout randomization abbreviated as
ASLR~\cite{aslr}, randomization for a variety of memory protection
schemes~\cite{diehard}, obfuscations~\cite{variaphd}, program
shepherding~\cite{kiriansky}, use of cyrptographic devices, and
encrypted program execution) that, under suitable assumptions, make it
``practically difficult'' for resource-bounded attackers to exploit
certain class of errors even though the errors are present in the
system-under-attack. Many of these mechanisms are widely used in
practice because of how effective they are. These defense mechanisms
are designed to compliment traditional approaches such as testing and
formal verification, and provide an additional crucial layer of
security. 

Defense mechanisms distinguish themselves from formal methods and
testing in the following ways: First, They give an ``intuitive'' yet
effective {\it partial guarantee} of compliance of a software system
w.r.t a non-exploitability specification, even though the system may
have errors in it. By {\it partial} we mean that a resource-bounded
attacker can still launch an attack but the probability is vanishingly
small, under suitable assumptions. By contrast, the traditional aim of
formal verification is a complete guarantee or establish the absence
of errors. Testing techniques, on the other hand, are typically
incomplete and provide {\it no guarantees} of compliance of a large
software system w.r.t a specification. Second, defense mechanisms
offer different tradeoffs between completeness and scalability, when
compared to testing or formal verification. For example, appropriate
use of defense mechanisms may lower the burden of establishing
security guarantees on software developers by not requiring them to
establish full compliance while giving some measure of security, at
the cost of possibly allowing certain hard-to-find errors.

While the inevitability of errors in software and the need for
effective defense mechanisms has been forcefully made by many
researchers in the past, and eloquently captured in a recent
paper~\cite{berger}, what seems largely missing is a formal
characterization of the security provided by these defenses. Such a
formal and precise characterization of security, in terms of a partial
compliance guarantee, can be a foundational way of reasoning about and
comparing defense mechanisms of all kinds using the same standard
approach, just as in the field of cryptography, notions such as
``semantic security'' are standard ways of comparing encryption
schemes.

In this paper, building on paradigms from cryptography and program
analysis, we propose a general and formal definition of partial
compliance, we call \resistance, of a software system w.r.t a
(non-exploitability) specification. The key insight of our formal
program analysis approach is that a program P together with its
defense mechanisms D (written P+D) is \resistant against a
well-defined probabilistic polynomial time attacker (or attack model)
A if the code and runtime behavior of P+D is resistant to analysis and
reverse engineering by A, and this can be expressed using the idea of
{\it computational indistinguishability}. Informally, we say that P+D
is \resistant to attacker A if the a priori ``exploit knowledge''
(pre-analysis) of the attacker A is {\it computationally
  indistinguishable}, asymptotically in some suitable security
parameter, to its a posteriori ``exploit knowledge''
(post-analysis). Based on this definition, we propose a formal
analysis framework and a research program whose goal is to analyze the
efficacy of defense mechanisms from a variety of contexts. We make the
following contributions in this ``Vision and Challenges'' paper:

\subsection*{Contributions}

\begin{enumerate}

\item We provide a formal definition of \resistance. We show the
  utility of this definition by analyzing the defense mechanism
  described below which (at least abstractly) can be shown to be
  effective against the vast majority of buffer overflow based
  control-hijack attacks.

\item The defense mechanism we propose in this paper is briefly as
  follows: All pointers in the program-under-protection of the defense
  mechanism are encrypted (a la PointGuard and variants), and detailed
  micro-tagging of secret bits enables detection of leakage of secret
  keys. We informally argue that programs P protected by this defense
  mechanism which we simply refer to as , are \resistant. That is
  the program obtained by appropriately combining P and D, denoted as
  , is \resistant. (A more formal argument would require detailed
  analysis of the implementation of the defense mechanism, which is
  beyond the limits imposed by the format of the paper.)

\item Finally, we suggest that unlike formal verification of arbitrary
  programs to establish their non-exploitability (e.g., absence of
  overflow errors), the value of formally establishing \resistance is
  that we only have to pay the cost of formally verifying the defense
  mechanism exactly once, irrespective of the program being defended.

\end{enumerate}

\section{Attacker Model}

We first carefully define the attacker model. The attacker can be
defined as probabilistic polynomial time (PPT) Turing machine that has
access to the source code of the program-under-attack P (e.g., a Web
server). The assumption that the attacker has the source code of P is
realistic, given that binary code and obfuscations can be easily
reverse-engineered by attackers using sophisticated program analysis
tools. The attacker is allowed to run any type of program analysis
(static, dynamic, machine learning,...)  within a time bounded by a
polynomial over the length of the program-under-attack. The attacker's
goal is to learn the vulnerabilities in the program P, and construct
appropriate exploits.

The vulnerable machine M (e.g., a machine running the Apache Web
Server) is remote and the attacker is allowed network access to the
program P running on it. The attacker is not allowed side-channel
access to the runtime of P or physical access to M. The attacker A's
modus operandi is to send bit strings to the program P running on M,
cause a buffer overflow, upload its payload into a suitable part of
memory that can be controlled by A, cause control flow of P to jump to
the payload causing it to execute, thus resulting in a control-hijack
attack. (For further reading, we refer to the rich literature already
out there on buffer overflow attacks in C/C++
programs~\cite{erickson2008hacking, rop}, and defense
mechanisms~\cite{aslr}.)

  \section{Buffer Overflow Attacks, Pointer Encryption, and
    Attack-Resistance}

  An example of a defense mechanism that can be construed as
  \resistance is the variation of PointGuard \cite{tuck2004hardware}
  technique that we consider here. For the purposes of this discussion
  we assume that attacker is only interested in function pointers and
  data pointers on the heap in the program-under-attack, and that P is
  suitably protected against other forms of buffer-overflow attacks
  (e.g., stack-smashing is not possible because the Not Execute bit is
  properly set or that ret-to-libc attacks are not possible because
  libc is properly ASLRed~\cite{aslr}.) We also allow for program P to
  leak secret information. (Note that in our analysis below, we will
  conveniently ignore the overhead of the proposed defense mechanism,
  many of the minitua regarding the PointGuard implementation, and
  leave that as future work.)

  The defense mechanism that we propose has two parts to it and it
  works as follows: First, just like PointGuard and its variants, our
  technique protects against pointer corruption by encrypting code and
  data pointer values while they are in memory. These pointers are
  appropriately decrypted immediately before dereferencing, i.e., just
  before they loaded into registers. The encryption/decryption key is
  is created on the machine M right before (or at the very begining
  of) P is loaded for execution.

  Attackers attempting to overwrite pointers in memory, with the goal
  of controlling the pointed-to part of memory, can indeed destroy the
  encrypted pointer value through a buffer overflow. However, unless
  they have the appropriate decryption key, whatever they overwrite
  pointer values with will be gibberish. Put differently, the attacker
  is prevented from constructing meaningful payloads unless he has
  knowledge of certain ``runtime secret bits'', namely, the decryption
  keys. (Observe that in the context of address-space layout
  randomization (ASLR), the corresponding ``runtime secret bits'' are
  the base address of libc or the call stack.) PointGuard implements
  this mechanism by modifying a C compiler (GCC) to emit code that
  encrypts pointers for storage in memory and decrypts them for
  dereferencing. While PointGuard is very interesting mechanism, it is
  effective only under the assumption that the decryption key is not
  somehow leaked by the program P under attack.

  This brings us to the second part of the defense mechanism, namely,
  a fine-grained taint tracking mechanism that dynamically tracks the
  flow of bits marked as secret in P to sinks observable by
  attackers. (There is considerable work on such runtime or dynamic
  information flow tracking for all kinds of security applications,
  e.g., TaintDroid~\cite{enck2014taintdroid} or
  Dytan~\cite{clause2007dytan}. Also refer these papers on
  quantification~\cite{schwartz2010all,kang2011dta++} of dynamic
  taint.) This mechanism ensures that if for some reason P attempts ot
  leak ``runtime secret bits'' onto observable sinks at runtime, then
  P is shutdown. This mechanism can be implemented through the use of
  OS-level taint-tracking as in TaintDroid or program-level as in
  Dytan.

  Two important issues arise in the dynamic information flow context
  are: First, how to deal with implicit dynamic information flow, and
  second, what to do when there is legitimate reason for flow of taint
  from secret bits to an observable sink without any accompanying loss
  of secret bits. With regard to tracking implicit information flow
  many techniques have been proposed that can be used in our
  context~\cite{schwartz2010all}. However, dealing with false
  positives, where there is a legit reason for taint flow from
  computation over secret bits to observable sinks, is not
  straightforward. One potential approach is information-flow
  obfuscation, i.e., removal of any meaningful relationship between
  leaked information on observable channels and the secret bits. Put
  differently, the idea is to make the leaked information uncorrelated
  to the secret data, perhaps through some form of randomization of
  encrypted data (and derivatives) that can be undone after
  decryption. One such technique used widely in cryptography is called
  blinding~\cite{rsa-blinding}.

  We will now informally argue that the proposed defense mechanism is
  \resistant. Observe that before the attacker A analyzed P his only
  options in launching an attack were limited to guessing the location
  of buffer overflow vulnerabilities and the decryption key K. (The
  security parameter in this analysis is length of K in number of
  bits.) As K grows the effort of guessing is dominated by the length
  of K and the probability of success of the attacker falls
  exponentially. We call this success probability as the ``a priori
  probability of success''. 

  Now assume that through appropriate formal analysis we can establish
  that the encryption used in our defense mechanism is semantically
  secure, and further that the information leakage technique can
  detect all possible leaks of the decryption key. Note that such an
  analysis is independent of the program P under the protection of the
  defense mechanism, and needs to be done only once. Under this
  assumption, it follows that even if the attacker were to employ very
  sophisticated analysis on the source code of his copy of P to learn
  vulnerabilities his post-analysis attack probability is dminishes
  exponentially with the length of K. 

  Thus, the a priori probability of success of the attacker to launch
  an attack on P (defended by D) is asymptotically the same as the a
  posteriori probability of success. We capture this \resistance
  property using the idea of computational indistinguishability from
  cryptography, described below.

  \section{Computational Indistinguishability and Entropy}

  We start with some informal definitions. We say that two probability
  distribution ensembles  and  are computationally
  indistinguishable if for all probabilistic polynomial time (PPT)
  attackers A, the ``difference'' between the samples  and 
  is asymptotically negligible in n.

  Now consider the probability distributions over an attacker A's
  knowledge of certain {\it crucially important secret} bits (e.g.,
  cryptographic keys) of the program P under-attack. We informally say
  that the defense mechanism D is effective, if forall A, the a priori
  (before analyzing P and D) distribution of A's knowledge of these
  secret bits is asymptotically the same as the a posteriori (after
  analyzing P and D) distribution.

\section{The Definition of Attack Resistance}

In this Section we define the notion of \resistance more formally. To
begin with we assume that we are given a family of programs P paired
with defense mechanisms D, that are parameterized in the length K of
the crucially important secret bits.  We say that the family of
program P paired with an appropriate family of defense mechanism D
(written P+D), \resistant in K (or simply P+D is \resistant) against a
well-defined probabilistic polynomial time (PPT in K) attacker A if
the following formal conditions hold:

\begin{enumerate}

\item {\bf Runtime Secret Bits:} A necessary condition for the
  efficacy of defense mechanisms is the existence of some finite set
  of bit sequences called {\it Runtime Secret Bits} or RSBs in the
  pair P+D of program and defense mechanism.

\item {\bf Attacker's a priori and a posteriori Knowledge (Entropy):}
  The attacker's knowledge is measured in terms of entropy of these
  {\it runtime secret bits}. The second necessary condition for the
  efficacy of these defense mechanism is that the attacker's a priori
  knowledge, prior to code analysis of P+D, of these RSBs must be low,
  i.e., the a priori entropy of RSB bits is high.

  We similarly define the attacker's a posteriori knowledge to be A's
  entropy of RSBs after code and runtime analysis. The idea of using
  entropy is standard in capturing the hardness of guessing in
  cryptographic analysis.

\item {\bf Resistance to Code Analysis:} The third necessary condition
  for the efficacy of defense mechanism D is that the attacker A's a
  priori knowledge of RSBs is computationally indistinguishable in K
  from A's a posteriori knowledge of RSBs after {\it code analysis} of
  P+D. By code analysis we mean that the attacker A can use any
  program analysis technique at her disposable, including but not
  limited to, static, dynamic and machine learning.
 
\item {\bf Resistance to Runtime Analysis:} The fourth necessary
  condition for the efficacy of a defense mechanism D is that the
  attacker A cannot learn the RSBs through dynamic information flow
  analysis of P+D at runtime, given only blackbox (or remote box)
  access to P+D. More precisely, attacker A's a priori knowledge of
  RSBs is computationally indistinguishable in k from A's a posteriori
  knowledge of RSBs after {\it runtime analysis} of P+D. In the
  example above, the information flow tagging detects all such
  potential leaks and stops the program before it can send such
  information out.

  By runtime analysis we mean that attacker A can send queries to P+D
  and observe the output. Given that the code analysis may have
  revealed information leakage channels (e.g., format string errors),
  the attacker could now send appropriate queries to the P+D at
  runtime, and learn the RSBs by analyzing the replies to those
  queries. At this point in time we only allow the attacker only
  input/output access to P+D through the network, and plan to consider
  timing and other side-channels at a later point in time.
\end{enumerate}

\section {Threats to Validity of the Research Program}

\noindent{\bf Practical Value of The Proposed Formal Analysis
  Framework, and its difference with respect to traditional formal
  verification:}
The proposed analysis framework essentially establishes that
cryptographic primitives used by the defense mechanism D are properly
implemented, and that the runtime information-flow leakage technique
used by D would detect and prevent all potential runtime leaks of
secret bits. This is a one time cost of analyzing D, irrespective of
the programs protected. This can be accomplished by using appropriate
formal verification techniques. By contrast, in traditional formal
verification approaches, we will have to prove that every program P is
completely free of buffer overflow errors, if our goal is 100\%
guarantee that P adheres to a non-exploitability specification.

\noindent{\bf Error-finding is PSPACE-Hard:} Some might argue that the
error-finding problem given a program and specification is already
known to be PSACE-Hard, and so shouldn't exploit-construction be
equally hard (in general undecidable) from a complexity-theoretic
point of view. In what way are the guarantees being proposed here any
stronger? Observe that we are not saying anything about the general
exploit-construction problem, and instead focus on analysis of a
parametric class of programs protected by a defense mechanism. The
PSPACE-Hardness result is general, and doesn't say anything about the
hardness of exploiting a given program. We are simply proposing that
given a parametric class of programs, certain defense mechanisms can
make it harder to exploit and this guarantee of hardness can be
captured precisely using the notion of \resistance.

\noindent{\bf Don't Use Unsafe Languages:} Some might argue that
buffer overflow attacks can be completely solved through the use of
safe languages. While true, the complex software ecosystem of today
will be subject to all manner of attacks and it pays to formally
analyze the defense mechanism used. The idea of \resistance can be
used to analyze any defense mechanism that relies on proper
implementation and use of cryptography and guarantees against leakage
of secrets.

\noindent{\bf Some Defense Mechanisms Shift Attacks on Integrity and/or 
Confidentiality to Attacks on Availability:} We have assumed that 
detects and prevents all attacks against . However, in practice it 
is often the case that an attack is ``detected'' by a crash of , which 
prevents an attacker from taking over the control-flow of . This is
the case for defense mechanisms such as ASLR, where an incorrect 
assumption of the base address of \emph{libc} would lead to reading 
from unallocated memory. The threat to validity is that \resistance 
refers to integrity and/or confidentiality attacks, but not availability.


\section {Related Work}

There also extensive use of formal methods in the security context,
esp. for protocol analysis~\cite{datta2007protocol}. There is also
considerable prior work on dynamic information flow
leakage~\cite{schwartz2010all}, and on analysis of implementation of
cryptographic primitives~\cite{appelverification}. Our work is
different in that we propose the combination of such analyses to
formally establish the effectiveness of defense mechanisms.

\section{Conclusion}

In conclusion, we have proposed a new notion of partial compliance
that we call \resistance and defined using the notion of computational
indistinguishability. Informally, if a well-defined PPT attacker's a
priori and a posteriori knowledge of crucial secrets remain
asymptotically computationally indistinguishable for a given pair P+D,
then we can say that P+D is \resistant, and that the defense mechanism
D is effective. We argue that this notion can be very helpful in
analyzing, and helping design new defense mechanisms. Furthermore,
such analysis can be useful in recognizing design and implementation
weaknesses of defense mechanisms that are not \resistant.

