\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}




\title{Focus Your Attention (with Adaptive IIR Filters)}



\author{Shahar Lutati \quad Itamar Zimerman \quad Lior Wolf \\
  The School of Computer Science\\
  Tel Aviv University\\
   \texttt{shahar761@gmail.com} \\  \texttt{zimerman1@mail.tau.ac.il} \\ \texttt{wolf@cs.tau.ac.il } \\
}
\begin{document}
\maketitle


\begin{abstract}
We present a new layer in which dynamic (i.e., input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control theory, and is shown to generalize diagonal state-space layers. The layer performs on-par with state-of-the-art networks, with a fraction of their parameters and with time complexity that is sub-quadratic with input size. The obtained layer is favorable to layers such as Heyna, GPT2, and Mega, both with respect to the number of parameters and the obtained level of performance on multiple long-range sequence problems.
\end{abstract}

\section{Introduction}




Designing sequence models that capture short- and long-term dependencies is a central goal of sequence modeling. Besides performance, computational complexity also plays a part when dealing with long sequences. Although transformers~\cite{attention_is_all_u_need} excel at tasks that involve short-range dependencies, their performance on data with long-range dependencies can be poor. For example, regardless of the high time complexity, on the Long Range Arena benchmark (LRA)~\cite{tay2020long} transformers perform poorly compared to other sequence models.

Another approach that has emerged to address long-range data processing is the utilization of regularized implicit global (long) convolutions. In this technique, convolutions are employed along the sequence dimension, enabling convolutions with a global receptive field. Initially, this approach was implemented through state-space layers~\cite{gu2021combining,gu2021efficiently}, which introduced a recurrent layer that could be efficiently computed via a global convolution. Recent research has explored alternative variations of global convolutions, including implicit~\cite{poli2023hyena} and regularized parameterization~\cite{fu2023simple,li2022makes}. These methods have demonstrated improved performance on tasks involving long-range dependencies and with sub-quantitative complexity. They have also shown effectiveness in enhancing long-range transformer capabilities~\cite{ma2022mega,zuo2022efficient,saon2023diagonal}. However, it remains uncertain whether these models can scale up or function similarly to transformers across a diverse range of tasks~\cite{Vardasbi2023StateSA}.

This work strives to efficiently integrate convolution-based sequence models and transformers, to provide a model that is capable of handling both short and long dependencies. The attempt to combine these components was first presented by \citet{ma2022mega}, who used a simple global convolution before each transformer block. This convolution is parameterized by the Exponential Moving Average (EMA) recurrent rule and can be seen as an IIR filter. In this work, instead of using first-order IIR filters, we introduce learnable adaptive IIR filters, which allow us to propose Focus, a layer that combines local attention and a novel type of regularized global convolution grounded on a hypernetwork that produces adaptive IIR filters.

\textbf{Our main contribution} is the focus layer, which has several unique properties: (i) We are the first to use data-dependent global filters, which are implemented by a global hyper-network mechanism that focuses local attention. (ii) In contrast to other works in the domain that employ FIR filters, it relies on IIR filters. (iii). We present an efficient and stable computation of those IIR filters. (iv) Theoretically, our layer is grounded in the theory of control systems, similar to state-space layers, which are built on the state-space model (SSM) of control theory. {Furthermore, in Sec. \ref{seq:analysis} we show that IIR filters are a generalization of SSMs and diagonal-linear RNNs, which have recently been recognized as remarkable long-range learning architectures~\cite{gupta2022diagonal,gu2022parameterization,orvieto2023resurrecting,gupta2022simplifying,saon2023diagonal,david2023decision}. By drawing upon the extensive research conducted on IIR filters, our findings can provide additional insights into the effectiveness, stability, expressiveness, and initialization of those layers.}


\section{Background and related work}
IIR filters, known as infinite impulse response filters, are digital filters that utilize feedback to generate an output signal. Their primary applications involve signal smoothing, filtering, and signal modification. These filters are extensively employed in various fields, such as audio processing, speech processing, and image processing. One notable advantage of IIR filters is their ability to achieve a significantly sharper roll-off in the transition region compared to an FIR filter of the same order. This is made possible by the presence of complex poles in the IIR filters, which enable them to attenuate frequencies more rapidly.

The state-space representation of an IIR filter is a convenient way to represent the filter's dynamics and to implement it in software. It consists of three parts: the state vector, the state transition matrix, and the output matrix. The state vector contains the filter's internal state variables, the state transition matrix describes how the state vector changes over time, and the output matrix describes how the output signal is computed from the state vector. Such representation are described in \cite{zhang2023effectively}

\subsection{Learnable IIR Filters}
Since IIR filters are computationally efficient, yet expressive, it is natural to design IIR filters with Deep Learning.
 \cite{kuznetsov2020differentiable} proposes an approach to using traditional digital IIR filter structures inside deep-learning networks trained using backpropagation. The authors establish the link between such structures and recurrent neural networks and present three different differentiable IIR filter topologies. They compare the proposed topologies against each other and an established baseline and show that the proposed topologies can achieve better performance in some cases. Additionally, the authors present a simple Wiener-Hammerstein model, using differentiable IIRs as its filtering component, and train it on a guitar signal. 

\subsection{Global Convolutions}
The global convolution, also known as a long convolution, is a layer that applies scalar convolutions along the sequence dimension, enabling the handling of unrestricted 1-D sequences. Empirically, these layers have shown strong performance in tasks involving long-range dependencies, particularly in domains such as NLP~\cite{dao2022hungry,mehta2022long,wang2022pretraining}, audio~\cite{goel2022s}, speech~\cite{saon2023diagonal}, video~\cite{islam2022efficient,wang2023selective}, time-series analysis~\cite{zhang2023effectively} and more. Moreover, they exhibit computational efficiency as their cost is sub-quadratic. However, to achieve SOTA results, appropriate regularization is necessary. The approach of \cite{gu2021combining,gu2021efficiently,ma2022mega,li2022makes} incorporates a parameterization that inherently regularizes the kernel and decoupling sequence length from parameter count. \cite{romero2021ckconv,poli2023hyena} utilizes an implicit parameterization learned by FFNs operating on positional encodings, while ~\cite{fu2023simple} explicitly regularizes the convolution kernels using squash or smooth operators. 



\subsection{Long Range Transformers}
Transformers~\cite{attention_is_all_u_need}  have emerged as highly effective models for various tasks, but their widespread adoption has been limited by the quadratic cost of the self-attention mechanism and poor performance on long-range tasks. Researchers have pursued diverse approaches to overcome this challenge and to create efficient transformer architectures~\cite{fournier2021practical,tay2022efficient}. From the perspective of efficiency,
techniques such as sparse attention~\cite{child2019generating}, low-rank attention~\cite{wang2020linformer,winata2020lightweight}, kernel-based attention~\cite{choromanski2020rethinking}, recurrent mechanisms~\cite{hutchins2022block,dai2019transformer}, and efficient IO-awareness-based implementation~\cite{dao2022flashattention} proved efficient. From the perspective of effectiveness, ~\cite{yu2023megabyte, ivgi2023efficient} combines local and global attention models hierarchically, enhancing the model's ability to handle extensive context
 Other techniques employ global memory-based Attention~\cite{gupta2020gmat,al2022global,burtsevmemory}, and~\cite{zhou2022fedformer} applies attention in the frequency domain to expand long-range capabilities. 









\subsection{Hyper Networks}
A hypernetwork \cite{ha2016hypernetworks} is a function that maps a set of inputs to a set of weights, which are used as the parameters of a ``primary network''. Hypernetworks have been shown to be effective for a variety of tasks, including, for example, image classification~\citep{lutati2022ocd}, natural language processing~\cite{he2022hyperprompt}, and speech recognition~\cite{szatkowski2022hypersound}. They have also been shown to be able to improve the performance of neural networks on meta-learning tasks, such as few-shot learning~\cite{bertinetto2016learning}, continual learning~\cite{von2019continual}, and neural architecture search~\cite{zhang2019graph}.

\subsection{Adaptive Filtering}
Adaptive filtering is a technique used to improve the quality of a signal by removing noise or interference. Adaptive filters are able to adapt to changes in the signal or the environment, making them well-suited for a variety of applications. 

One common technique used in adaptive filtering is the short-time Fourier transform (STFT), which provides a time-frequency representation of a signal. It enables the analysis of time-varying properties of a signal by dividing it into short-time windows and applying the Fourier transform to each window. The STFT reveals the distribution of frequency content over time, which allows the adaptive filter to track the frequency content of the signal and adapt its coefficients accordingly. 
However, STFT introduce non-causal implementation due to overlapping time-bins. To mitigate it, we introduce chunked-FFT, a degenerate form of the STFT.

Recent research in AI has focused on using deep learning to improve the performance of adaptive filters. For example, deep learning has been used to improve the performance of adaptive filters for noise cancellation \cite{ZHANG20211}, echo cancellation \cite{haubner2022deep}, and equalization \cite{Zhou_2020}. Deep learning has also been used to develop new adaptive filter architectures that are more robust to noise and interference \cite{s22165935}.
\citet{Revach_2022} demonstrate how deep learning can be used to improve the performance of Kalman filtering \cite{kalman1960new}, a classical control algorithm. 



\section{Method}

\subsection{Overview}
We start by discussing the main design choices of our architecture.

\noindent{\bf Chunking and the combination of local and global models\quad} 
Given the quadratic complexity of transformers, chunking is a common practice for computing short-range attention efficiently. However, despite excelling in short-range tasks, full-length transformers often struggle to handle long-range dependencies and often perform comparably to local-attention-based transformers~\cite{xiong2021simple}. Recent studies have demonstrated that a combination of local and global transformers can achieve state-of-the-art performance on long-range tasks~\cite{ivgi2023efficient,yu2023megabyte,hutchins2022block}. Inspired by these findings, we introduce local attention as the local model, which is combined with a novel type of global convolution as the global model. Furthermore, in contrast to \cite{hutchins2022block, bulatov2023scaling}, our global model does not use recurrent computations, since it severely restricts parallelization.

\noindent{\bf Adaptive IIR Filters\quad} In MEGA ~\cite{ma2022mega}, it was demonstrated that incorporating an EMA at the beginning of each transformer block improves transformer performance in long-range tasks. EMA can be viewed as a convolution operation using simple first-order IIR filters. Motivated by this finding, we adopt a more versatile and expressive convolution approach that utilizes adaptive filters generated by a hypernetwork. Since the hypernetwork is an integral part of our global model, it employs global convolutions. Specifically, the regularized global convolution of~\cite{fu2023simple} is used, as the most straightforward option. A common challenge with hypernetworks is ensuring relatively small output sizes. In this regard, leveraging IIR filters, which have only a few parameters, is a reasonable choice.

\subsection{The Focus Layer}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/focus_full_arch.png}
    \caption{Focus Architecture: (a) The architecture of a single head. (b) The obtained layer. (c) The entire model. The architecture of the model and layer are defined similarly to MEGA~\cite{ma2022mega}. Blocks in blue are not learned, while blocks in red are learned parameters.  (serial to parallel) and  (parallel to serial) are the chunking and the de-chunking operations, respectively.}
    \label{fig:architecture}
\end{figure*}

In this section, we describe the focus attention head, our primary contribution. This head is integrated into the MEGA backbone, as visualized in Fig~\ref{fig:architecture}.
Let  be the input for the focus layer, where ,  is the sequence length, and  is the input's dimension.
Our method, termed Focus, utilizes the foundations of adaptive filtering theory to cope with very long stochastic sequences.
Given the seasonality of the sequence, the resolution of the FFT is determined by its size in each time-bin. Denote the size of the FFT in a single time-bin as .

The first component of the Focus layer is the hypernetwork, . The output of  is , which is the set of IIR kernels used for the forward processing of the sequence.  has a dimension of , denoting  kernels, each with a kernel size of two for  feature channels. The kernel is unique per time-bin, , which makes the filter adaptive to changes over time. 

 has two main components. The first is a shallow global convolution \cite{fu2023simple} based sub-model that is followed by adaptive max pooling (over each feature channel) \cite{Pytorch} with a size of , where  is the oversampling factor.

where  is the embedding from processing  using the global convolution layer. 
This computation can be shared across multiple Focus layers and can be split into local () and global () sub-components, as in most of our experiments, thus reducing substantially the computational cost. Furthermore, the embedding is permuted such that the feature space has the size  while  is added to the batch dimension for parallel computing.

The second component of  is a 2-layer MLP with sigmoid activations that maps the embedding  to a tensor with size .
With mapping of latent dimension  to 

where  is the IIR kernel, with size .  is the forward MLP mapping, as described above. Since  is a hypernetwork, the initialization of the last layer of the MLPs follows \cite{Chang2020Principled}. The rest of the layers follow the Xavier initialization \cite{pmlr-v9-glorot10a}.
The input  is split into non-overlapping time bins, where each time bin is passed through the FFT of the size . Denote the input in the r-th time bin as .

where  is the normalized frequency variable, sampled evenly on  range, and  is the index over the different time bins.

\noindent{\bf A note about causality\quad} To maintain a fully causal model that is applicable to auto-regressive tasks, each time bin is processed on its own and is not overlapped with other time bins.
In addition,   is shifted right by one time bin, such that the sequence at time bin  is processed by the kernel computed from time bin .

For each time-bin index, the corresponding IIR filter is applied. 
The IIR  filter of order 2 has the following frequency response, denote it as  


Since a Sigmoid activation is used for the last layer of , it is guaranteed that 's elements are positive real numbers smaller than 1. 
Further analysis and reasoning behind the specific selections made are presented in Sec.~\ref{seq:analysis}.

Recall that in the frequency domain, the equivalent of filtering is multiplying with the conjugated impulse response,

where the conjugation is denoted by a star and  is the elementwise (Hadamard) multiplication.
The hyper-dimension  defined earlier as the filter-bank size is collapsed via regular sum operation, denote the collapsed tensor as 

where  is an all-ones vector with size .
The collapsed tensor is the short-time Fourier representation of the original sequence filtered with adaptive filter kernels. The frequency representation goes through the inverse chunked Fourier transform (IFFT), to obtain the time-domain sequence.

The time-domain sequence has the same dimensions as the original sequence, yet, by applying an adaptive filter to it, we furnish it with an induction bias that helps smaller context attention head to cope with complicated tasks.

The sequence is split into  separate non-overlapping chunks. Denote the chunk length as , such that .
We denote the chunk of signal with uppercase , and use square brackets for indexing, starting with index 0, as follows



\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/iir_filters.png}
    \caption{Filter responses for three random filters with the specific denominator structure of Eq.~\ref{eq:iir_filter_s_plane}.}
    \label{fig:random_iir}
\end{figure}

All chunks are processed in parallel with the same small attention head,

where  are the query, key, and value matrices that map each chunk to their latent corresponding space.
Note that the attention head is causal, as it uses lower triangle masking.
The chunks, , are rearranged to form a complete signal, with sequence length , denote it as .
Following \cite{ma2022mega}, the output of the Focus layer, , passes through reset gate , the update gate . Specifically,



where ,  and  are learned matrices with size . , , and  are learned biases with size . SiLU stands for the 
sigmoid linear unit \cite{elfwing2018sigmoid}.

The final output is the gated summation of the gated attention and the input sequence,


\section{Analysis}
\label{seq:analysis}
{\noindent{\bf IIR and FIR Filters\enspace}}
IIR (Infinite Impulse Response) and FIR (Finite Impulse Response) filters are two commonly used types of digital filters with distinct characteristics. The main difference between them lies in their impulse response and filtering properties. FIR filters have a finite impulse response, meaning that the filter output is based solely on a finite number of past input samples. In contrast, IIR filters have an infinite impulse response, allowing the filter output to depend on both past and future input samples.

One advantage of IIR filters is their ability to achieve a desired frequency response with fewer coefficients compared to FIR filters. This makes IIR filters more computationally efficient, requiring fewer calculations and lower memory requirements. Consequently, in control feedback systems where real-time operation and computational efficiency are crucial, IIR filters are often preferred.

Additionally, IIR filters can exhibit higher selectivity and sharper roll-off in the frequency domain compared to FIR filters. This characteristic can be advantageous in control feedback systems, where precise control over specific frequency components is necessary.

{\noindent{\bf Stability\enspace}}
IIR filters can be more sensitive to quantization errors and can be prone to instability if not properly designed. The presence of feedback loops in control systems can further impact stability considerations. Therefore, careful attention must be given to stability analysis and appropriate filter design techniques to ensure reliable performance.

The filter can be described in the frequency domain as a rational polynomial function of the complex exponent . Denote the complex exponent as . 

A second-order system can be described as follows,

Specifically solving for general  gives,

where  and  are normalizing factors. Denote the term under the square root as discriminant, . For any  and  the following holds:

In order to guarantee stability, both exponents should decay, which leads to the requirement that the real part must be negative.

This can be achieved if  is positive. 
In the scenario where , the exponents have an imaginary part, causing it to oscillate.  This is called an under-damped response.
This response is stable, yet more expressive than Exponential Moving Average (EMA) filters, as found in MEGA~\cite{ma2022mega}.
The frequency of the sine, , in this case is
 
and the time domain impulse response reads,

where  is the normalizing factor, and  is the phase from aggregating both sine and cosine functions with the same frequency. 
Note that for orders above 2, there is no simple condition that guarantees that the real part will be negative and the response stable.

To demonstrate the oscillating behavior of the generated IIR filters, the time-domain impulse responses of some random kernels are drawn in Fig.~\ref{fig:random_iir}. In this plot, the purple kernel acts as EMA, while other kernels have a more complicated response. However, all filters decay as time increases, which leads to a stable response and has been identified by~\citet{li2022makes} as an essential property for capturing long-range dependencies.

{\noindent{\bf Time Complexity\enspace}}
The global conv time complexity is  
This is due to the FFT and IFFT that the signal is passed through.
This computation is done only once and is shared through multiple layers of Focus. The MLP is mapping between  and .

The chunked-FFT complexity depends on the size of the FFT used (). Denote the size of a single time bin as ,

The time complexity of a single time bin is .
The total time complexity of the chunked FFT reads,

Next, the time complexity of the attention head depends on the size of the context length ,


The total time complexity of the Focus layer is, therefore, 

where we neglected smaller terms when the sequence length is large (greater than dimensions). Recalling that , and rearranging terms, we have,

obtaining sub-quadratic time complexity with respect to input sequence length.
A visual comparison of overall complexity versus the standard attention head is depicted in Fig.~\ref{fig:time_complexity}.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Focus_complexity.png}
\caption{Time Complexity of the Focus layer and of Attention, increasing sequence length}
    \label{fig:time_complexity}
\end{figure}

\noindent{\bf Expressiveness \enspace}
An emerging class of diagonal linear RNNs~\cite{orvieto2023resurrecting,gupta2022simplifying} recently achieved near SOTA results in several long-range tasks. They include complex and real variants, as well as diagonal state-space layers~\cite{gupta2022diagonal,gu2022parameterization}. The following recurrent rule describes each channel of those layers:

where  is the recurrent state at time . By isolating , we can rewrite Eq. \ref{eq:ssm_reccurent_rule} as follows:




Recall that the differential equation of an IIR filter of order 2 can be represented as follows:



By substituting the values of , , , , it becomes evident that the IIR filter can be constrained to a linear SSM. In machine learning,  is often omitted in SSMs or diagonal RNNs, since it can be seen as a parameter-based skip-connection. In this case, the SSM can be represented by an IIR filter of order 1.

As mentioned earlier, higher-order filters can introduce stability issues. Therefore, our decision to utilize IIR filters of order 2 is justified, as we opt for the most expressive IIR filters that still maintain stability during training.

\section{Experiments} 

Below we present experimental results for the proposed Focus layer. In addition to our full method, we introduce an ablation to evaluate the importance of adaptive filtering, in which instead of the hypernetwork , the IIR filters are conventional learned parameters. This ablation is denoted by ``Focus-H''.
\subsection{In-context learning}
 In order to evaluate our method relative to other state-of-the-art long-range architectures, such as \cite{poli2023hyena}, \cite{dai2019transformer}, the associative recall synthetic task is evaluated. The associative recall task was first introduced in \cite{elhage2021mathematical} and is part of a number of simple yet informative tasks that test the capabilities of the model in processing long-range sequences.

In the associative recall task, each string is formed by concatenating key-value pairs sampled randomly from a dictionary. The model should output the correct value given a singular key, regardless of whether the key is in the long sequence.

Similarly to \citet{poli2023hyena}, we employ the associative recall task in order to explore the memory capabilities of our model. 

In all synthetic data experiments the same shared hyperparameters are used, with the exception of the sequence length. The hyperparameters are depicted in {in Appendix \ref{sec:appendix}}. The AdamW optimizer \citep{loshchilov2017fixing} is used.



\begin{table*}[t]
\centering
\caption{Test accuracy (\%) for associative recall on long sequences of length  and a vocabulary size of 30. NF - not feasible to test. NR = not reported. }\begin{tabular}{lccccc}\toprule
L & Focus & Focus-H&Hyena &FlashTransformer &Transformer \\
\midrule
30 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0\\
1K & 100.0 & 98.0 & 100.0& 95.0&100.0\\
8K & 100.0 & 85.3& 100.0 &NR& NF\\
32K & 100.0& 34.6&100.0 & 32.4& NF\\
64K & 100.0& 28.0&100.0 & 26.7 & NF\\
\bottomrule
\label{tab:synth}
\end{tabular}
\end{table*}

As can be seen in Tab.~\ref{tab:synth}, our model is able to obtain an accuracy of 100\% for all sequence lengths, without overfitting, despite the low number of examples (2000), and with no memory explosion thanks to linear scaling with input size. These results show that the Focus mechanism is able to improve the performance of regular transformers to be on par with Heyna \cite{poli2023hyena}, with smaller footprint.
In addition, the ablation experiment shows the importance of adaptive filtering, i.e. estimating the filter kernels online to focus the attention mechanism on important sub-sequences where the results degrade in the ablation due to the greater sequence length.
\begin{figure}[t]
    \centering
\includegraphics[width=1.0\linewidth]{figures/iir_filters_2.png}
    \caption{Frequency Response of IIR kernels, for 1K sequence split into 18 time bins. The important key is found in the 12th time bin.}
    \label{fig:Focusing_Effect}
\end{figure}

The associative recall task is used not only for benchmarking, but also to gain insights into the adaptive filtering mechanism.
As can be seen in Fig.~\ref{fig:Focusing_Effect}, the frequency response of the adaptive filtering is plotted for this task. In this specific run, the important key is found in the 12th time bin. The frequency response of the IIR filters is almost 5 orders of magnitude higher than for nearby time bins. This effect demonstrates the ``Focus'' mechanism. Note that using only 2 parameters for the IIR kernel, the filters are able to differentiate between important and unimportant time bins with 5 orders of magnitude. This supports our design choice of IIR filter, seeing that with a kernel size as low as 2 the filter is still sharp enough.


\subsection{Language Modeling}
The enwiki8 dataset is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. It is a commonly used dataset for benchmarking character-level language models. 

The Text8 dataset is a corpus of text used for training and evaluating language models. It is a subset of the Wikipedia dump from March 2006 and consists of 90 million characters. The text is tokenized and lowercased, and each token is assigned a unique id.




The metric used to evaluate language models on enwiki8 and Text8 is bits per character (BPC). The lower the BPC, the better the language model.
To compute the BPC the average cross-entropy is computed in the log2 basis

where  is the target distribution, and  is the output distribution.  is the sequence length.
To maintain the same capabilities such as Mega \cite{ma2022mega}, we used 8 layers of the Focus layer, with a hidden dimension of 1024 and an input dimension of 512. 

\begin{table}[t]
\centering
\caption{BPC for enwiki8 dataset.}
\begin{tabular}{@{}l@{~}c@{~}c@{}}
\toprule
Model & \#params &  BPC\\
\midrule
Transformer XL \cite{dai2019transformer} & 277M & 0.99\\
Mega \cite{ma2022mega} & 39M & 1.02\\
GPT2 \cite{Radford2019LanguageMA} & 1542M & 0.94\\
Focus-H (ablation)  & 21M & 1.06\\
\textbf{Focus}  & 22M & \textbf{0.94}\\
\bottomrule
\end{tabular}
\label{tab:enwiki8}
\smallskip
\smallskip
\smallskip


\centering
\caption{BPC for text8 dataset.}
\begin{tabular}{@{}l@{~}c@{~}c@{}}
\toprule
Model & \#params &  BPC\\
\midrule
Transformer XL \cite{dai2019transformer} & 277M & 1.08\\
GPT2 \cite{Radford2019LanguageMA} & 1542M & 0.98\\
Focus-H (ablation)  & 21M & 1.10\\
\textbf{Focus}  & 22M & \textbf{0.98}\\
\bottomrule
\end{tabular}
\label{tab:text8}
\smallskip
\smallskip
\smallskip


\caption{Accuracy for sMNIST, pMNIST}
\begin{tabular}{@{}l@{~}c@{~}c@{}}
\toprule
Model & sMNIST &  pMNIST\\
\midrule
Transformer& 98.9 & 97.9\\
S4 \cite{gu2021efficiently} & 99.6 & 98.7\\
Focus-H (ablation)  & 98.9 & 98.0\\
\textbf{Focus}  & \textbf{99.7} & \textbf{98.8}\\
\bottomrule
\end{tabular}
\label{tab:snpMNIST}
\end{table}


The enwiki8 results are reported in Tab.~\ref{tab:enwiki8}. Evidently, Focus outperforms both Mega~\cite{ma2022mega} and Transformer XL~\cite{dai2019transformer}, despite a much lower number of parameters, performing on-par with GPT2\cite{Radford2019LanguageMA} (zero-shot) but with a fraction of its parameters and substantially less FLOPS. 
The same occurs for the Text8 dataset, reported in Tab.~\ref{tab:text8}. While the ablation is inferior to Transformer-XL, with the full method Focus is on par with GPT2.

\subsection{1-D image classification}
We evaluated our model on the sequential MNIST task, a challenging problem that requires models to capture long-range dependencies. Permuted MNIST is a variant of MNIST where the order of pixels in each image is scrambled, making the task more challenging. Following S4, we use a hidden dimension of 512 but to save resources, we use 6 layers and not more. The results are listed in Tab.~\ref{tab:snpMNIST}. Focus has an accuracy of 99.7\% (98.8\%) on unpermuted (permuted) MNIST, outperforming the transformer and S4. 




\subsection{Efficiency Comparison}
To assess the efficiency of the Focus layer, we measure peak memory usage and inference speed. We compare several related models on the association recall task. This task involves processing a sequence of 1K tokens, which represents the maximal fit in memory for transformers. The results are presented in Tab.~\ref{tab:comparisonEff}. As can be seen, Focus exhibits the lowest peak memory consumption, using only 38\% of the memory consumed by the Transformer model. However, its inference speed is slightly slower than the other methods. 
\begin{table}[t]
\centering
\begin{tabular}{@{}l@{}c@{~}c@{}}
\toprule
\textbf{Model} & Inference time & Memory \\
\midrule
Transformer & x1 & x1 \\
S4 & x1.58 & x0.43 \\
MEGA & x1.49 & x0.57 \\
\textbf{Focus} & x1.75 & x0.38 \\
\bottomrule
\end{tabular}
\caption{Comparison of inference speed and peak memory consumption for related models.}
\label{tab:comparisonEff}
\end{table}



\section{Conclusions}



Attention models are extremely powerful for modeling sequences, as demonstrated by the seminal work of \citet{attention_is_all_u_need}. Indeed, transformers have revolutionized the way deep learning is practiced, leading to unprecedented performance across almost all studied AI domains. 

However, transformers have quadratic complexity in the sequence length, which can impact their efficiency, and often struggle to perform optimally in tasks that involve long-range dependencies~\cite{tay2020long}. In this work, we present a dynamic filtering approach that enables us to subsequently employ attention within much shallower architectures. As our ablation shows, the dynamic nature of these filters is crucial to the success of the layer. Similarly crucial is the use of IIR filters, and we analyze the regime in which these are stable. 

\section{Limitations}

While this paper presents promising results, there are a few limitations to consider. Firstly, although we are the first to utilize IIR filters for long-range tasks, we have not examined sequence models solely based on IIR filters. Additionally, we have not investigated the impact of different types of hyper-global convolution on performance. Furthermore, IIR filters can be computed using recurrent rules or convolution. While the convolutional view is more natural for training, the recurrent view presented in Eq.~\ref{eq:iir_order2_diff_eq}, can be leveraged for efficient auto-regressive generation. This can lead to a significant reduction in the time and space complexity of the layer during inference, which is beneficial for real-time applications.

\section{Acknowledgments}
This work was supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD). It is part of a PhD research conducted by the first author.

\newpage
\bibliography{main}
\bibliographystyle{acl_natbib}
\newpage
\appendix
\section{Hyperparameters}
The hyperparameters for the associative recall task are provided in Tab.~\ref{tab:hyperparameters}. Hyperparameters that differ in other experiments are reported in the respective sections. For example, in Sec.~5.2, eight layers of the Focus layer are used, with a hidden dimension of 1024 and an input dimension of 512, in order to compare with Mega on similar terms. 
\label{sec:appendix}
\begin{table}[h]
\caption{Hyperparameter settings for the synthetic associative recall task}
\begin{tabular}{l l}
\toprule
Optimizer & AdamW\\
Optimizer momentum & \\
Vocabulary Size & 30\\
NFFT & \\
F & 1\\
C & 32 \\
Learning Rate & 1E-4\\
Batch Size & 32\\
Num Samples& 2000\\
Warmup epochs& 10\\
Number of Layers& 2\\
Width& 64\\
\bottomrule
\label{tab:hyperparameters}
\end{tabular}
\end{table}
\smallskip
\end{document}
