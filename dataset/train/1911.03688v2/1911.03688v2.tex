

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{color, colortbl}
\usepackage{amsmath}
\usepackage{booktabs}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\definecolor{Gray}{gray}{0.95}

  
\newcommand\ivan[1]{\textcolor{red}{\textbd{Ivan}: #1}}
\newcommand\matt[1]{\textcolor{blue}{\textbf{Matt}: #1}}
\title{
ConveRT: Efficient and Accurate \\ Conversational Representations from Transformers
\\
{
        \ttfamily \small \href{https://github.com/PolyAI-LDN/polyai-models}{github.com/PolyAI-LDN/polyai-models}
}
}
\author{
 Matthew Henderson,
 I{\~{n}}igo Casanueva,
 Nikola Mrk{\v{s}}i\'c, \\
 {\bf 
 Pei-Hao Su, Tsung-Hsien Wen \normalfont{and}
 \textbf{Ivan Vuli\'{c}}
 } \\
 \texttt{\small matt@poly-ai.com} \\
 PolyAI Limited,
 London, UK
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose \textbf{ConveRT} (\textbf{Conve}rsational \textbf{R}epresentations from \textbf{T}ransformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight memory- and energy-efficient model. We show that ConveRT achieves state-of-the-art performance across widely established response selection tasks. We also demonstrate that the use of extended dialog history as context yields further performance gains. Finally, we show that pretrained representations from the proposed encoder can be transferred to the intent classification task, yielding strong results across three diverse data sets. {ConveRT} trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders. With its reduced size and superior performance, we believe this model promises wider portability and scalability for Conversational AI applications. \end{abstract}

\section{Introduction}
\label{s:intro}
Dialog systems, also referred to as conversational systems or conversational agents, have found use in a wide range of applications. They assist users in accomplishing well-defined tasks such as finding and booking restaurants, hotels, and flights \cite{Hemphill:1990,Williams:2012b,ElAsri:2017sigdial}, with further use in tourist information \cite{Budzianowski:2018emnlp}, language learning \cite{Raux:2003,Chen:2017survey}, entertainment \cite{Fraser:2018iva}, and  healthcare \cite{Laranjo:2018,Fadhil:2019arxiv}. They are also key components of intelligent virtual assistants such as Siri, Alexa, Cortana, and Google Assistant.

Data-driven task-oriented dialog systems require domain-specific labelled data: annotations for intents, explicit dialog states, and mentioned entities \cite{Williams:2014sigdial,Wen:17,Wen:2017icml,Ramadan:2018acl,Liu:2018naacl,Zhao:2019naacl}. This makes the scaling and maintenance of such systems very challenging. Transfer learning on top of pretrained models \cite[\textit{inter alia}]{Devlin:2018arxiv,Liu:2019roberta} provides one avenue for reducing the amount of annotated data required to train models capable of generalization.

Pretrained models making use of language-model (LM) based learning objectives have become prevalent across the NLP research community. When it comes to dialog systems, \emph{response selection} provides a more suitable pretraining task for learning representations that can encapsulate conversational cues. Such models can be pretrained using large corpora of natural unlabelled \emph{conversational} data \cite{Henderson:2019acl,Mehri:2019acl}. Response selection is also directly applicable to retrieval-based dialog systems, a popular and elegant approach to framing dialog  \cite{Wu:2017acl,Weston:2018ws,Mazare:2018emnlp,Gunasekara:2019dstc7,Henderson:2019acl}.\footnote{Retrieval-based dialog is popular because posing dialog as response selection \cite{Gunasekara:2019dstc7} simplifies system design  \cite{Boussaha:2019arxiv}. Unlike modular or end-to-end task-oriented systems, retrieval-based ones do not rely on dedicated modules for language understanding, dialog management, and generation. They mitigate the requirements for explicit task-specific semantics hand-crafted by domain experts \cite{Henderson:14a,Mrksic:15,Mrksic:17}.
}







\vspace{1.4mm}
\noindent \textbf{Response Selection} is a task of selecting the most appropriate \textit{response} given the dialog history \cite{Wang:2013emnlp,AlRfou:2016arxiv,Yang:2018repl,Du:2018scai,Chaudhuri:2018conll}. This task is central to retrieval-based dialog systems, which typically encode the \textit{context} and a large collection of responses in a joint semantic space, and then retrieve the most relevant response by matching the query representation against the encodings of each candidate response. The key idea is to: \textbf{1)} make use of large unlabelled conversational datasets (such as Reddit conversational threads) to \textit{pretrain} a neural model on the general-purpose response selection task; and then \textbf{2)} \textit{fine-tune} this model, potentially with additional network layers, using much smaller amounts of task-specific data. 

Dual-encoder architectures pretrained on response selection have become increasingly popular in the dialog community \cite{Cer:2018arxiv,Humeau:2019arxiv,Henderson:2019acl}. In recent work, \newcite{Henderson:2019arxiv} show that standard pretraining LM-based architectures cannot match the performance of dual encoders when applied to dialog tasks such as response retrieval.

\vspace{1.4mm}
\noindent \textbf{Scalability and Portability.} 
A fundamental problem with pretrained models is their large number of parameters (see Table~\ref{tab:comparison} later): they are typically highly computationally expensive to both train and run \cite{Liu:2019roberta}. 
 Such high memory footprints and computational requirements hinder quick deployment as well as their wide portability, scalability, and research-oriented exploration. The need to make pretrained models more compact has been recognized recently, with a line of work focused on building more efficient pretraining and fine-tuning protocols \cite{Tang:2019arxiv,Sanh:2019arxiv}. The desired reductions have been achieved through techniques such as distillation \cite{Sanh:2019arxiv}, quantization-aware training \cite{Zafrir:2019arxiv}, weight pruning \cite{Michel:2019nips} or weight tying \cite{Lan:2019albert}. However, the primary focus so far has been on optimizing the LM-based models, such as BERT.
 
 \vspace{1.4mm}
\noindent \textbf{ConveRT.} 
This work introduces a {\em more compact pretrained response selection model} for dialog. ConveRT is only 59MB in size, making it significantly smaller than the previous state-of-the-art dual encoder (444MB). It is also more compact than other popular sentence encoders, as illustrated in Table~\ref{tab:comparison}. This notable reduction in size and training acceleration are achieved through combining 8-bit embedding quantization and quantization-aware training, subword-level parameterization, and pruned self-attention. Furthermore, the lightweight design allows us to reserve additional parameters to improve the expressiveness of the dual-encoder architecture; this leads to \textit{improved learning of conversational representations} that can be transferred to other dialog tasks \cite{Casanueva:2020ws,Bunk:2020arxiv}. 



\vspace{1.4mm}
\noindent \textbf{Multi-Context Modeling.} 
ConveRT moves beyond the limiting single-context assumption made by \newcite{Henderson:2019acl}, where only the immediate preceding context was used to look for a relevant response. We propose a multi-context dual-encoder model which combines the immediate context with previous dialog history in the response selection task. The multi-context ConveRT variant remains compact (73MB in total), while offering improved performance on a range of established response selection tasks. We report significant gains over the previous state-of-the-art on benchmarks such as Ubuntu DSTC7 \cite{Gunasekara:2019dstc7}, AmazonQA \cite{Wan:2016icdm} and Reddit response selection \cite{Henderson:2019arxiv}, both in single-context and multi-context scenarios. Moreover, we show that sentence encodings learned by the model can be transferred to other dialog tasks, reaching strong intent classification performance over three evaluation sets. Pretrained dual-encoder models, both single-context and multi-context ones, are shared as TensorFlow Hub modules at \href{https://github.com/PolyAI-LDN/polyai-models}{\ttfamily \small github.com/PolyAI-LDN/polyai-models}.\footnote{Finally, our more compact neural response selection architecture is well aligned with the recent socially-aware initiatives on reducing costs and improving fairness and inclusion in NLP research and practice \cite{Strubell:2019acl,Mirzadeh:2019arxiv,Schwartz:2019green}. Cheaper training (pretraining the proposed dual-encoder model on the entire Reddit costs only 85 USD) and quicker development cycles offer new opportunities for more researchers and practitioners to tap into the construction of neural task-based dialog systems. }



%
 
\section{Methodology}
\label{s:methodology}
\noindent \textbf{Pretraining on Reddit Data.} 
We assume working with English throughout the paper. Simplifying the conversational learning task to response selection, we can relate target dialog tasks to general-domain conversational data such as Reddit~\cite{AlRfou:2016arxiv}. This allows us to fine-tune the parameters of the task-specific response selection model, starting from the general-domain response selection model pretrained on Reddit. Similar to \newcite{Henderson:2019acl}, we choose Reddit  for pretraining due to: \textbf{1)} its organic conversational structure; and \textbf{2)} its unmatched size, as the public repository of Reddit data comprises 727M \textit{(input, response)} pairs.\footnote{\textcolor{darkblue}{github.com/PolyAI-LDN/conversational-datasets}}




\subsection{More Compact Response Selection Model}
\label{ss:main}
We propose \textbf{ConveRT} -- \textbf{C}onversational \textbf{R}epresentations from \textbf{T}ransformers -- 
a \textit{compact dual-encoder pretraining architecture}, leveraging subword representations, transformer-style blocks, and quantization, as illustrated in Figure~\ref{fig:main-enc}. ConveRT satisfies all the following requirements: it is effective, affordable, and quick to train.



\begin{figure}[!t]
\centering
\includegraphics[width=0.87\linewidth]{main-encoder.pdf}
\vspace{-2mm}
\caption{\textit{Single-context ConveRT} dual-encoder model architecture. Its multi-context extension is illustrated in Figure~\ref{fig:multi-enc}. It is possible to \textit{transfer} learned encodings at different network layers (e.g.,  or the final ) to other tasks such as intent detection or value extraction (see \S\ref{s:results}).
Note that the model uses two different feed-forward network (FFN) layers: 1) \textit{feed-forward 1} is the standard FFN layer also used by \newcite{Vaswani:2017nips}, and 2) \textit{feed-forward 2} contains 3 fully-connected non-linear feed-forward layers followed by a linear layer which maps to the final encodings  and  (note that the two \textit{feed-forward 2} networks do not share parameters, while the \textit{feed-forward 1} parameters are shared).
}
\vspace{-2.5mm}
\label{fig:main-enc}
\end{figure}



\vspace{1.3mm}
\noindent \textbf{Input and Response Representation.} 
Prior to training, we obtain a vocabulary of subwords  shared by the input side and the response side: we randomly sample and lowercase 10M sentences from Reddit, and then iteratively run any subword tokenization algorithm.\footnote{In the actual implementation, we use the same subword tokenization as \newcite{vaswani2018}. We run it for 4 iterations and retain only subwords occurring at least 250 times, containing no more than 20 UTF8 characters, also disallowing more than 4 consecutive digits.} The final vocabulary  contains 31,476 subword tokens. During training and inference, if we encounter an OOV character it is treated as a subword token, where its ID is computed using a hash function, and it gets assigned to one of 1,000 additional ``buckets'' reserved for the OOVs. We therefore reserve parameters (i.e., embeddings) for the 31,476 subwords from  and for the additional 1,000 OOV-related buckets. At training and inference, after the initial word-level tokenization on UTF8 punctuation and word boundaries, input text  is split into subwords following a simple left-to-right greedy prefix matching \cite{vaswani2018}. We tokenize all responses  during training in exactly the same manner.

\vspace{1.3mm}
\noindent \textbf{Input and Response Encoder Networks.}
The subword embeddings then go through a series of transformations on both the input and the response side. The transformations are based on the standard Transformer architecture \cite{Vaswani:2017nips}. Before going through the self-attention blocks, we add positional encodings to the subword embedding inputs. Previous work (e.g., BERT and related models) \cite[\textit{inter alia}]{Devlin:2018arxiv,Lan:2019albert} learns a fixed number of positional encodings, one for each position in the sequence, allowing the model to represent a fixed number of positions. Instead, we learn two positional encoding matrices of different sizes-  of dimensionality [47, 512] and  of dimensionality [11, 512]. An embedding at position  is added to: .\footnote{Note that since 47 and 11 are coprime, this gives  different possible positional encodings. Similar to the original (non-learned) positional encodings from \newcite{Vaswani:2017nips}, the rationale behind this choice of positional encoding is to allow the model to generalize to unseen sequence lengths.}



The next layers closely follow the original Transformer architecture with some notable differences. First, we set maximum relative attention \cite{Shaw:2018naacl} in the six layers to the following respective values: [3, 5, 48, 48, 48, 48].\footnote{We zero out in training and inference the attention scores for pairs of words if they are further apart than the set maximum relative attention values.} This also helps the architecture to generalize to long sequences and distant dependencies: earlier layers are forced to group together meanings at the phrase level before later layers model larger patterns. We use single-headed attention throughout the network.\footnote{Multi-headed attention requires running computations on 4-tensors: [batch, time, head, embedding], while for single-headed attention, this reduces to 3-tensors, and effectively speeds up training without hurting performance.}

Before going into a softmax, we add a bias to the attention scores that depends only on the relative positions:  where  is a learned bias vector. This helps the model understand relative positions, but is much more computationally efficient than computing full relative positional encodings \cite{Shaw:2018naacl}. Again, it also helps the model generalize to longer sequences.

Six Transformer blocks use a 64-dim projection for computing attention weights, a 2,048-dim kernel (\textit{feed-forward 1} in Figure~\ref{fig:main-enc}), and 512-dim embeddings. Note that all Transformer layers use parameters that are fully shared between the input side and the response side. As in the Universal Sentence Encoder (\textsc{use}) \cite{Cer:2018arxiv}, we use square-root-of-N reduction to convert the embedding sequences to fixed-dimensional vectors. Two self-attention heads each compute weights for a weighted sum, which is scaled by the square root of the sequence length; the length is computed as the number of constituent subwords.\footnote{In fact, rather than computing the self-attended sequence, then reducing it, we reduce the attention weights accordingly, and then directly apply them via matrix multiplication to the input sequence to get the final reduced representation, that is, we \emph{fuse} these two operations. This is more computationally efficient, avoiding another 3-tensor multiplication.} The outputs of the reduction layer, labelled  and  in Figure~\ref{fig:main-enc}, are 1,024-dimensional vectors that are fed to the two ``side-specific'' (i.e., they do not share parameters) feed-forward networks.

In other words, the vectors  and  go through a series of  -dim feed-forward hidden layers (; ) with skip connections, layer normalization, and orthogonal initialization. The activation function used in these networks and throughout the architecture is the fast GeLU approximation \cite{hendrycks2016gelu}: . The final layer is linear and maps the text into the final L2-normalized 512-dim representation:  for the input text, and  for the corresponding response text (Figure~\ref{fig:main-enc}).

\vspace{1.4mm}
\noindent \textbf{Input-Response Interaction.}
The relevance of each response to the given input is then quantified by the score , computed as cosine similarity with annealing between the encodings  and . It starts at 1 and ends at , linearly increasing over the first 10K training batches. Training proceeds in batches of  \textit{(input, response)} pairs . The aim of the objective is to distinguish between the true relevant response () and irrelevant responses (i.e., negative samples)  for each input sentence . The training objective for a single batch of  pairs is as follows: . The goal is to maximize the score of positive training pairs  and minimize the score of pairing each input  with  negative examples, which are responses that are not associated with the input : for simplicity, all other  from the current batch are used as negative examples.



\vspace{1.3mm}
\noindent \textbf{Quantization.}
Very recent work has shown that large models of language can be made more compact by applying quantization techniques \cite{Han:2016iclr}: e.g., quantized versions of Transformer-based machine translation systems \cite{Bhandare:2019arxiv} and BERT \cite{Shen:2019arxiv,Zhao:2019arxiv,Zafrir:2019arxiv} are now available. In this work, we focus on enabling quantization-aware conversational pretraining on the response selection task. We show that the dual-encoder ConveRT model from Figure~\ref{fig:main-enc} can be also be trained in a quantization-aware manner. Rather than the standard 32-bits per parameter, all embedding parameters are represented using only 8 bits, and other network parameters with just 16 bits; they are trained in a quantization-aware manner by adapting the mixed precision training scheme from \newcite{Micikevicius:2018iclr}. It keeps shadow copies of each variable with 32bit Floating Point (FP32) precision, but uses FP16-cast versions in the computations and inference models. Some operations in the graph, however, require FP32 precision to be numerically stable: layer normalization, L2-normalization, and softmax in attention layers.

Again, following \newcite{Micikevicius:2018iclr}, the final loss is scaled by 128, and the updates to the shadow FP32 variables are scaled back by 1/128: this allows the gradient computations to stay well represented by FP16 (e.g., they will not get rounded to zero). The subword embeddings are stored using 8-bits per parameter, and the quantization range is adjusted dynamically through training. It is updated periodically to contain all of the embedding values that have so-far been learned, with room for growth above and below - 10\% of the range, or 0.01 - whichever is larger. Finally, quantization also allows doubling the batch size, which also has a favourable effect of increasing the number of negative examples in training. 




\begin{figure}[!t]
\centering
\includegraphics[width=0.79\linewidth]{contextual-encoder.pdf}
\vspace{-1.5mm}
\caption{\textit{Multi-context ConveRT}. It models 1) the interaction between the immediate context and its accompanying response, 2) the interaction of the response with up to 10 earlier contexts from the conversation history, as well as 3) the interaction of the full context with the response. \textit{Transformer layers} refer to the standard Transformer architecture also used in the single-context encoder model in Figure~\ref{fig:main-enc}; the \textit{feed-forward 2} blocks are the same as with the single-context encoder architecture, see Figure~\ref{fig:main-enc}. The block \textit{mean} refers to simple averaging of two context encodings  and .}
\vspace{-1.5mm}
\label{fig:multi-enc}
\end{figure}



\vspace{1.4mm}
\noindent \textbf{Multi-Context ConveRT.}
\label{ss:contextual}
 Figure~\ref{fig:main-enc} depicts a single-context dual encoder architecture. Intuitively, the single-context assumption is limiting for modeling multi-turn conversations, where strong conversational cues can be found in earlier dialog history, and there has been a body of work on leveraging richer dialog history for response selection \cite{Chaudhuri:2018conll,Zhou:2018acl,Humeau:2019arxiv}. Taking a simple illustrative example: 



{\footnotesize
\vspace{1.4mm}
\indent \textbf{Student:} I'm very interested in representation learning. \\
\indent \textbf{Teacher:} Do you have any experience in PyTorch? \\
\indent \textbf{Student:} Not really. \\
\indent \textbf{Teacher:} And what about TensorFlow?}\vspace{1.4mm}

\noindent Selecting the last Teacher's response would be very difficult given only the immediate preceding context. However, the task becomes easier when taking into account the entire context of the conversation. We thus construct a \textit{multi-context dual-encoder model} by using up to 10 more previous messages in a Reddit thread. The extra 10 contexts are concatenated from most recent to oldest, and treated as an extra feature in the network, as shown in Figure~\ref{fig:multi-enc}. Note that all context representations are still independent from the representation of a candidate response, so we can still do efficient response retrieval and training. The full training objective is a linear combination of three sub-objectives: 1) ranking responses given the immediate context (i.e., this is equal to the single-context model from \S\ref{ss:main}), 2) ranking responses given only the extra (non-immediate) contexts, and 3) ranking responses given the averaged representation of the immediate context and additional contexts.\footnote{Combining multiple objectives in a dual-encoder framework has also been done by \newcite{AlRfou:2016arxiv} and \newcite{Henderson:2017arxiv}. Note that more sophisticated solutions to fusing dialog  history are possible such as using attention over older contexts as done by \newcite{Vlasov:2019arxiv} on the much smaller MultiWOZ 2.1 dataset \cite{Eric:2019arxiv}, but we have opted for simple concatenation as an efficient solution for training on the large Reddit data. The multiple objectives result in quicker learning, and also give useful diagnostic probes into the performance of each feature throughout training.}

%
 
\section{Experimental Setup}
\label{s:exp}


\noindent \textbf{Training Data and Setup.} 
We base all our (pre)training on the large Reddit conversational corpus \cite{Henderson:2019arxiv} derived from 3.7B Reddit comments: it comprises 727M \textit{(input, response)} pairs for single-context modeling -- 654M pairs are reserved for training, the rest is used for testing. We truncate sequences to 60 subwords, embedding size is set to 512 for all subword embeddings and bucket embeddings, and the final encodings , , , and  are all 512-dimensional. The hidden layer size of \textit{feed forward 2} networks is set to 1,024 (with  hidden layers used).

We train using ADADELTA with  \cite{Zeiler:2012ada}, batch size of 512, and a learning rate of 1.0 annealed to 0.001 with cosine decay over training. L2-regularization of  is used, subword embedding gradients are clipped to 1.0, and label smoothing of 0.2 is applied.\footnote{The label smoothing technique \cite{Szegedy:2016cvpr} reduces overfitting by preventing a network to assign full probability to the correct training example \cite{Pereyra:2017arxiv}. It means that each positive example in each batch is assigned the probability of 0.8, while the remaining probability mass is evenly redistributed across in-batch negative examples.} 



We pretrain the model on Reddit on 12 GPU nodes with one Tesla K80 each for 18 hours; this is typically sufficient to reach convergence. The total pretraining cost is roughly \Nk\mathbf{R}_{N}@kN=100; k=1\mathbf{R}_{100}@1r_xh_xr_xr_x\approx 40\%\mathbf{R}_{100}@1 \times 100\%\mathbf{R}_{100}@1\mathbf{R}_{100}@1h_{x}^{T} h_{y}z\times\timesr_x$ transferred to another dialog task. They outperform \textsc{use-large} in all three tasks and \textsc{bert-large} in 2/3 tasks. Note that, besides quicker pretraining, intent classifiers based on ConveRT encodings train 40 times faster than \textsc{bert-large}-based ones, as only the classification layers are trained for ConveRT. In sum, these preliminary results suggest that ConveRT as a sentence encoder can be useful beyond the core response selection task. The usefulness of ConveRT-based sentence representations have been recently confirmed on other intent classification datasets \cite{Casanueva:2020ws}, with different intent classifiers \cite{Bunk:2020arxiv}, and in another dialog task: turn-based value extraction \cite{Coope:2020acl,Bunk:2020arxiv}. In future work, we plan to investigate other possible applications of transfer, especially for low-data setups.  
\section{Conclusion}
\label{s:conclusion}
We have introduced ConveRT, a new light-weight model of neural response selection for dialog, based on Transformer-backed dual-encoder networks, and have demonstrated its state-of-the-art performance on an array of response selection tasks and in transfer learning for intent classification tasks. In addition to offering \textit{more accurate} conversational pretraining models this work has also resulted in \textit{more compact} conversational pretraining. The quantized versions of ConveRT and multi-context ConveRT take up only 59 MB and 73 MB, respectively, and train for 18 hours with a training cost estimate of only 85 USD. In the hope that this work will motivate and guide further developments in the area of retrieval-based task-oriented dialog, we publicly release pretrained ConveRT models.



%
 

\clearpage
\bibliographystyle{acl_natbib}
\bibliography{refs}

\end{document}
