

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage[accsupp]{axessibility}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{s}
\crefname{}{Tab.}{Tabs.}


\def\cvprPaperID{6492} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Parametric Scattering Networks}



\author{\parbox{\textwidth}{\centering
    Shanel Gauthier\thanks{Equal contributions. Equal senior author contribution. This research was partially funded by NSERC CGS-M~[\emph{S.G.,L.A.}] and URA~[\emph{B.T.}] scholarhips; NSERC Discovery Grant RGPIN-2021-04104.~[\emph{E.B.}]; IVADO PRF Grant [\emph{I.R.,E.B.,G.W.}]; and CIFAR AI Chairs [\emph{I.R.,G.W.}]. We acknowledge resources provided by Compute Canada and Calcul Quebec. The content is solely the responsibility of the authors and does not necessarily represent the official views of funding agencies. Correspondence to: \texttt{eugene.belilovsky@concordia.ca, meickenberg@flatironinstitute.org, guy.wolf@umontreal.ca}} \hspace{-10pt}
    \qquad Benjamin Th\'{e}rien \hspace{-10pt}
    \qquad Laurent Als\`{e}ne-Racicot \hspace{-10pt}
    \qquad Muawiz Chaudhary \\
    Irina Rish
    \qquad Eugene Belilovsky
    \qquad Michael Eickenberg 
    \qquad Guy Wolf} \vspace{5pt}\\
 Universit\'{e} de Montr\'{e}al;  Mila -- Quebec AI Institute;  Concordia University, Montreal, QC, Canada \\
Waterloo University, Waterloo, ON, Canada; Flatiron Institute, New York, NY, USA\\ 
}



\maketitle
\begin{abstract}\vspace{-4pt}
   The wavelet scattering transform creates geometric invariants and deformation stability. In multiple signal domains, it has been shown to yield more discriminative representations compared to other non-learned representations and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. In this work, we investigate whether this standard wavelet filterbank construction is optimal. Focusing on Morlet wavelets, we propose to learn the scales, orientations, and aspect ratios of the filters to produce problem-specific parameterizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains in small-sample classification settings over the standard scattering transform. Moreover, our empirical results suggest that traditional filterbank constructions may not always be necessary for scattering transforms to extract effective representations.
\end{abstract}
\vspace{-12pt}
\section{Introduction}
\label{sec:introduction}



The scattering transform, proposed in~\cite{mallat2012group}, is a cascade of wavelets and complex modulus nonlinearities, which can be seen as a convolutional neural network (CNN) with fixed, predetermined filters. This construction can be used to build representations with geometric invariants and is shown to be stable to deformations. It has been demonstrated to yield impressive results on problems involving highly structured signals \cite{bruna2013invariant,oyallon2013generic, anden2014deep, sifre2014rigid, hirn2015quantum, hirn2017wavelet, eickenberg2018solid, anden2019joint, sinz2020wavelet, perlmutter2020geometric}, outperforming a number of other classic signal processing techniques. Since scattering transforms are instantiations of CNNs, they have been studied as mathematical models for understanding the impressive success of CNNs in image classification \cite{bruna2013invariant,mallat2016understanding}. As discussed in~\cite{bruna2013invariant}, first-order scattering coefficients are similar to SIFT descriptors \cite{SIFT}, and higher-order scattering can provide insight into the information added with depth \cite{mallat2016understanding}. Moreover, theoretical and empirical study of information encoded in scattering networks indicates that they often promote linear separability, which leads to effective representations for downstream classification tasks \cite{bruna2013invariant,oyallon2017scaling,anden2015joint,eickenberg2018solid}.

Scattering-based models have been shown to be useful in several applications involving scarcely annotated or limited labeled data~\cite{bruna2013invariant,sifre2013rotation,oyallon2018replearning,eickenberg2018solid}. Indeed, most breakthroughs in deep learning in general, and CNNs in particular, involve significant effort in collecting massive amounts of well-annotated data to be used when training deep overparameterized networks. While big data is becoming increasingly prevalent, there are numerous applications where the task of annotating more than a small number of samples is infeasible, giving rise to increasing interest in small-sample learning tasks and deep-learning approaches towards them~\cite{ barz2020deep,learnfewsamples, Worrall_2017_CVPR}. Recent work has shown that, in image classification, state-of-the-art results can be achieved by hybrid networks that harness the scattering transform as their early layers followed by learned layers based on a wide residual network architecture~\cite{oyallon2018replearning}. Here, we further advance this research avenue by proposing to use the scattering paradigm not only as fixed preprocessing layers in a concatenated architecture, but also as a parametric prior to learn filters in a CNN. This allows us to also shed light on whether the  standard wavelet construction \cite{mallat1999wavelet} is an optimal approach for building filterbanks from a mother-wavelet for discriminative tasks.

Recall that the scattering construction is based on complex wavelets, generated from a mother wavelet via dilations and rotations, aimed to cover the frequency plane while having the capacity to encode informative variability in input signals~\cite{bruna2013invariant}. Further, discrete parameterization and indexing of these operations (i.e., by dilation scaling or rotation angle) have traditionally been carefully constructed to ensure the resulting filter bank forms an efficient tight frame \cite{mallat1999wavelet,mallat2012group} with well-established energy preservation properties. On the other hand, it has been observed that the first layers of convolutional networks resemble wavelets but may not necessarily form a tight frame \cite{krizhevsky2012imagenet}. The question then arises: is it necessary to use the standard wavelet filterbank construction? Here, we relax the standard construction by considering another alternative where a small number of wavelet parameters used to create the wavelet filterbanks are optimized for the task at hand.




To our knowledge, this is the first work that aims to learn the wavelet filters of scattering networks in 2D signals.
Related work and the empirical protocol are summarized in Sec.~\ref{sec:parametric_scattering_networks}. and  Sec.~\ref{sec:exp-setup} respectively.  In Sec.~\ref{section:converging}, we compare scattering parameterizations obtained from optimizing over different datasets. In Sec.~\ref{section:deformation}, we evaluate the robustness of our parametric scattering networks to deformation. In Sec.~\ref{sec:smalldata}, we demonstrate the advantages of our approach in limited labeled data settings and study the adaptation of the wavelet parameters toward a supervised task. In Sec.~\ref{sec:unsup}, we investigate the adaptation of the parametrized scattering using an unsupervised objective. Finally, in Sec.~\ref{sec:computation} we evaluate the computational and memory complexity of our hybrid networks. Further technical details appear in appendices, and code accompanying the work is available at \url{https://github.com/bentherien/parametricScatteringNetworks}.




\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{img/figureEB.pdf}
    \caption{\textbf{Initialized wavelet filters pre and post-training.} Real part of Morlet wavelet filters initialized with \textit{tight-frame} (left) and  \textit{random} (right) schemes before (top) and after (bottom) training. The filters were optimized on the entire CIFAR-10 training set with linear model. We use the Morlet canonical wavelet parameterization. For the tight-frame filters, we observe substantial changes in both scale and aspect ratio. On the other hand, all random filters undergo major changes in orientation and scale. }
    \label{fig:filters-cifar-real-before-kymatio}
    \vspace{-10pt}
\end{figure*}
\vspace{-5pt}

\section{Related work}
\label{sec:relatedwork}
Learning useful representations from little training data \cite{learnfewsamples} is arduous and a reality in a variety of domains such as in biomedicine and healthcare. 
Recent works have tried to tackle this problem. Lezama et al.~\cite{lezama2018ole} replace the categorical cross-entropy loss with a geometric loss called Orthogonal Low-rank Embedding (OLÃ‰) to reduce the intra-class variance and enforce inter-class margins. Barz and Denzler~\cite{barz2020deep} also propose to replace the categorical cross-entropy loss, but this time with the cosine loss function in order to decrease overfitting in the small-sample classification settings. The cosine loss function, as opposed to the softmax function used with cross-entropy, does not push the logits of the true class to infinity.
Other methods show promise by incorporating prior knowledge into the model~\cite{Gens_2014_NIPS, Jacobsen_2016_CVPR, bruintjes2021vipriors, Kayhan_2020_CVPR, brigato2021tune}. Oyallon et al.~\cite{oyallon2018replearning} introduce hybrid networks where the scattering transform with fixed wavelets was shown to be an effective replacement for early layers of learned convolutional networks on a wide residual network architecture. 
Cotter and Kingsbury~\cite{cotter2019learnable} also propose a hybrid network called a learnable ScatterNet, where learning layers are intermixed between the scattering orders, unlike our work where only a few parameters governing the wavelet construction are modified.  Ulicny et al.~\cite{8902831} propose Harmonic Networks (HN), a hybrid network consisting of fixed Discrete Cosine Transform filters combined with learnable weights in CNNs. 


Related to our work, adding learnable components to existing wavelet-based representations has been considered in a number of recent works in the context of time-series~\cite{balestriero2018spline, seydoux2020clustering, cosentino2020learnable, balestriero2020interpretable}. Balestriero et al.~\cite{balestriero2018spline} and Seydoux et al.~\cite{seydoux2020clustering} learn a spline-parametrized mother wavelet for 1D problems. Similarly, Cosentino and Aazhang~\cite{cosentino2020learnable} parametrized the group transform in the context of time-series data. Our work, alternatively, focuses on 2D problems and maintains the canonical Morlet wavelet parameterization, but allows deviation from a tight-frame filter bank.




\section{Parameterization of Scattering Networks}
\label{sec:parametric_scattering_networks}
We first revisit the formulation of traditional scattering convolution networks in Sec.~\ref{sec:scatteringnetworks} and introduce our parametric scattering transform in Sec.~\ref{sec:parametricoptim} and ~\ref{sec:eq}. Finally, Sec.~\ref{sec:init} discusses scattering parameter initialization.

\subsection{Scattering Networks}
\label{sec:scatteringnetworks}

For simplicity, we focus here on 2D scattering networks up to their 2nd order. Subsequent orders can be computed by following the same iterative scheme, but have been shown to yield negligible energy \cite{bruna2013invariant}. Given a signal , where  is the spatial position index, we compute the scattering coefficients , of order 0, 1, and 2 respectively. For an integer , corresponding to the spatial scale of the scattering transform, and assuming an  signal input with one channel, the resulting feature maps are of size , with channel sizes varying with the scattering coefficient order (i.e., 1 channel at order 0,  channels at order 1 and  channels at order 2).   

To calculate 0th-order coefficients, we consider a low-pass filter  with a spatial window of scale , such as a Gaussian smoothing function. We then convolve this filter with the signal and downsample by a factor of  to obtain 
Due to the low-pass filtering, high-frequency information is discarded here and is recovered in higher-order coefficients via wavelets introduced as in a filter bank.

Morlet wavelets are a typical example of filters used in conjunction with the scattering transform, and are defined as
\vspace{-15pt}

where  is a normalization constant to ensure wavelets integrate to 0 over the spatial domain, ,  is the rotation matrix of angle  and 

The four parameters can be adjusted and are presented in Table~\ref{tab:params}. 
From one wavelet , the traditional wavelet filterbank is obtained by dilating it by factors , , and rotating by  angles  equally spaced over the circle, to get , which is then completed with the lowpass .  This can be written in terms of the parameters in Table~\ref{tab:params} as . By slight abuse of notations, we use  here, , to denote such wavelets indexed by  and . The resulting set of filters is visualized in the frequency domain in Figure \ref{fig:graphlwp}. 

\begin{table}[tb]
\centering
\small
\caption{Canonical Parameters of Morlet wavelet\vspace{-5pt}}
    \begin{tabular}{|c|c||c|c|}
     \hline Param  &  Role &  Param  &  Role\\\hline
   & Gaussian window scale & 
         & Global orientation \\
      \hline    & Frequency scale & 
         & Aspect Ratio \\
      \hline \end{tabular}
    \label{tab:params}
\vspace{-15pt}
\end{table}


First-order scattering coefficients are calculated by first convolving the input signal with one of the generated complex wavelets (i.e., indexed by the parameters in Table~\ref{tab:params}) and downsampling the resulting filtered signal by the scale factor  of the wavelet chosen. Then, a pointwise complex modulus is used to add nonlinearity, and the resulting real signal is smoothed via a low-pass filter. Finally, another downsampling step is applied, this time by a factor of , to obtain an optimally compressed output size. Mathematically, we have 

The resulting feature map has  channels, based on the number of wavelets in the generated family. Second-order coefficients are generated similarly, with the addition of another cascade of wavelet transform and modulus operator before the low-pass smoothing, i.e.,

Due to the interaction between the bandwidths and frequency supports of first and second order, only coefficients with  have significant energy. Hence, the second-order output yields a feature map with 
 channels. 

\subsection{Morlet Canonical Parameterization}
\label{sec:parametricoptim}
While the wavelet filters are traditionally fixed, we let the network learn the optimal parameters of each wavelet. In other words, we constrain our filters to always be Morlet wavelets by only optimizing the parameters in Table~\ref{tab:params}. We call this approach the Morlet canonical parameterization of the wavelet. To provide such data-driven optimization of scattering parameters, we show, in Appendix \ref{sec:derivative}, that it is possible to backpropagate through this construction. We adapted the Kymatio software package \cite{andreux2020kymatio} to create the learnable scattering network.



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{img/graph_lwp/graph_lwp_w7_2-3.pdf} 
    \vspace{-7pt}
    \caption{\textbf{Parametric scattering network learns dataset specific filters.} The graph (top right) shows the \textit{filterbank distance} over epochs as the filters are trained on different datasets. We visualize dataset specific parameterizations of scattering filterbanks (border colors from the legend) in Fourier space.  The x and y axis are the frequency axis. Scattering filters optimized for natural (CIFAR-10) and medical image (COVIDx CRX2) become more orientation-selective, i.e., thinner in the Fourier domain. On the other hand, filters optimized for texture discrimination (KTH-TIPS2) become less orientation-selective and deviate most from a tight-frame setup.\vspace{-10pt}}
    \label{fig:graphlwp}
\end{figure*}

\subsection{Initialization}
\label{sec:init}
To evaluate the importance of the standard wavelet construction, we consider two initializations and study their impact on resulting performance in both learned and nonlearned settings. First, the standard wavelet construction follows common implementations of the scattering transform by setting , , and  for , , while for each , we set  to be equally spaced on . The construction ensures the resulting filter bank forms an efficient tight frame. Thus, we call this construction the tight-frame initialization. Second, as an alternative, we consider a random initialization where these parameters are sampled as , , , and . That is, orientations are selected uniformly at random on the circle, the filter width  is selected using an exponential distribution across available scales and the spatial frequency  is chosen to be in the interval , which lies in the center of the feasible range between aliasing () and the fundamental frequency of the signal size ( where  is the number of pixels). Finally, we select the \textit{aspect ratio} variable to vary around the spherical setting of , with a bias towards stronger orientation selectivity () compared to lesser orientation selectivity ().
\vspace{-3pt}

\subsection{Morlet Equivariant Parameterization }
\label{sec:eq}
In the Morlet canonical parameterization approach, the canonical parameters of each filter are learned. As an alternative method, we consider the Morlet equivariant parameterization in which the number of learnable parameters is reduced by a factor L compared to the Morlet canonical parameterization. Each filter per scale is constructed using the same four parameters in Table \ref{tab:params}: , ,  and . 
However, the global orientation of the  filters for each scale are set to be [, , , \ldots ,  ]. 







\section{Experiments}
\label{sec:exp-setup}
\label{sec:results}
Our empirical evaluations are based on three image datasets: CIFAR-10,  COVIDx CRX-2, and KTH-TIPS2. CIFAR-10 and KTH-TIPS2 are natural image and texture recognition datasets, correspondingly. They are often used as general-purpose benchmarks in similar image analysis settings \cite{glico,sifre2013rotation}. COVIDx CRX-2 is a dataset of X-ray scans for COVID-19 diagnosis; its use here demonstrates the viability of our parametric scattering approach in practice, e.g., in medical imaging applications.

We evaluate the use of the parametrized scattering with two common models. In the first case, we consider the scattering as feeding into a simple linear model (denoted LL). The LL configurations are used to evaluate the linear separability of the obtained scattering representations and have the added benefit of providing a more interpretable model. In the second case, we take the approach of \cite{oyallon2018replearning} and  consider the scattering as the first stage of a deeper CNN, specifically a Wide Residual Network (WRN)~\cite{wideresnet}. The architecture of the WRN hybrid is described in more detail in Appendix~\ref{appendix-wrn}.

For both models (LL and WRN), we compare learned parametric scattering networks (LS) to fixed ones (S). For learned scattering (LS), we consider two scattering parameterization approaches: \textit{Morlet canonical}, described in Sec.~\ref{sec:parametricoptim} and \textit{Morlet equivariant},  described in Sec.~\ref{sec:eq}. To show the importance of the parametric approach, we also ablate the naive parameterization where all pixels of the wavelets are adapted, which we refer to as a pixel-wise parameterization.
For each scattering architecture, we consider both random and tight-frame (TF) initialization.  The fixed scattering models determined by the TF construction are equivalent to traditional scattering transforms. Finally, we also compare our approach to a fully learned WRN (with no scattering priors) and ResNet-50~\cite{he2016deep} applied directly to input data. We note that the latter is unmodified form its ImageNet architecture and that we do not initialize it with pre-trained weights. 

Across all scattering configurations, a batch-normalization layer with learnable affine parameters is added after all scattering layers. Classification is performed via a softmax layer yielding the final output. All models are trained using cross-entropy loss, minimized by stochastic gradient descent with momentum of 0.9. Weight decay is applied to the linear model and to the WRN. The learning rate is scheduled according to the one cycle policy~\cite{smith2019super}. Implementation details specific to each dataset are described in Appendix \ref{appendix-details}. 
We replicate some of the experiments with learnable scattering networks followed by WRN on CIFAR-10, COVIDx-CRX2, and KTH-TIPS2 using the cosine loss function \cite{barz2020deep}. The results are reported in Appendix~\ref{appendix-cosine-loss}.



\subsection{Exploring Dataset-specific Parameterizations}
\label{section:converging}


\begin{figure*}
    \centering
    \includegraphics[width=0.27\linewidth]{img/Deformation/Rotation.pdf}
    \includegraphics[width=0.27\linewidth]{img/Deformation/Shear.pdf}
    \includegraphics[width=0.27\linewidth]{img/Deformation/Scale.pdf}
    \vspace{-15pt}
    \caption{\textbf{Normalized distances between scattering representations of an image and its deformation.} Our parametric scattering transform shares similar stability to deformations as the scattering transform.   }\label{fig:defo}
\vspace{-12pt}
\end{figure*}
We first compare dataset-specific Morlet wavelet parameterizations and evaluate their similarities to a tight frame. Specifically, we train our parametric scattering networks using the canonical Morlet wavelet formulation with a linear classification layer and qualitatively compare the similarities of the learned filter bank to the tight-frame initialization. To facilitate quantitative comparison, we use a distance metric for comparing the sets of Morlet wavelet filters and Morlet wavelet filterbanks (i.e., scattering network instantiations), allowing us to measure deviations from the tight-frame initialization. 

We evaluate distances between two individual Morlet wavelets as  where  denotes the parameterization of the Morlet wavelet. We use the arc distance on the unit circle to compare values of theta. Since the set of learned scattering filters does not have a canonical order, to compare a learned scattering network to the tight frame scattering network, we use a matching algorithm to match one set of filters to another. Specifically, we first compute  between all combinations of filter pairs from both networks, then use a minimum cost bipartite matching algorithm \cite{kuhn1955} to find the minimal distance match between the two sets of filters. The final distance we use as a notion of similarity between two scattering networks is the sum of  for all matched pairs in the bipartite graph. Henceforth, we will refer to this distance as the \textit{filterbank distance}. 







The graph in Figure~\ref{fig:graphlwp} leverages the \textit{filterbank distance} to show the evolution of scattering networks initialized from a tight frame and trained on different datasets. Each network is trained on 1188 samples of its respective dataset (the standard size for KTH-TIPS2). All filters deviate quickly from a tight frame, but KTH-TIPS2's keep changing the longest and ultimately deviate the most. We also observe that filters initialized with the random initialization of Sec.~\ref{sec:parametric_scattering_networks} become more similar to our tight-frame initialization during the course of training (see Figure~\ref{fig:graphlwp_random} in Appendix~\ref{appendix:random-init}). 

On the left-hand side of Figure \ref{fig:graphlwp}, we visualize the dataset-specific scattering network parameterizations in Fourier space. White contours are drawn around each Morlet wavelet for clarity. The top black border corresponds to tight-frame initialization at J=2, shown for comparison to CIFAR-10 in blue (also J=2). The bottom black border corresponds to tight-frame initialization at J=4, shown for comparison to COVIDX-CRX2 red and KTH-TIPS2 yellow (both J=4). 


The filters optimized on the KTH-TIPS2 texture dataset (yellow) become less orientation-selective (wider in Fourier space) than the tight-frame initialization, with filters at J=0 becoming the least orientation-selective of the whole filter bank. We note that the filters at spatial scales J= 2 and 3 seem to change the most from a tight frame as illustrated in the appendix (Fig. \ref{fig:kthsorted}). In contrast, the filters optimized on COVIDx-CRX2 become more orientation-selective in general i.e., thinner in Fourier space, while changing the most at spatial scale J=0 as shown in the appendix (Fig. \ref{fig:covidsorted}). The filters optimized on CIFAR-10 mirror those optimized on COVIDx-CRX2, also becoming more orientation-selective than their tight-frame counterparts. We suspect that this is due to a reliance on edges for object classification datasets, which seem to require more orientation-selective filters. On the other hand, the morlet wavelets optimized for texture classification seem to discard some edge information in favor of less orientation-specific filters. Each dataset-specific parameterization seems to discard unneeded information from the tight-frame initialization in favor of accentuating problem-specific attributes. In Sec.~\ref{sec:smalldata}, we demonstrate these learned filters are not only interpretable but improve task performance, suggesting the tight frame is not optimal for many problems of interest. Nonetheless, a tight-frame does constitute a good starting point for learning. Indeed, the dataset-specific parameterizations for COVIDX-CRX2 and KTH-TIPS2 are, visually, very different, yet they move similar filterbank distances from the tight-frame initialization (see fig.\ref{fig:graphlwp}), which are small relative to the distances observed for randomly initialized and trained models. 
\begin{table*}[t] 
    \centering
    \caption{CIFAR-10 mean accuracy and std.\ error over 10 seeds, with  and multiple training sample sizes. Learnable scattering with TF initialization improves performance for all architectures, while randomly initialized scattering requires more training data to reach similar performance.} 
    \label{table:cifarresults}
    \fontsize{8}{8.5}\selectfont 
    \begin{tabularx}{0.75\linewidth}{lllllll} 
          \hline
        Arch. &Init. &Parameterization & 100 samples &500 samples & 1000 samples & All \\
        \hline
        \-2mm]
         LS+WRN&TF &Canonical& &  && \\ 
         LS+WRN&TF &Equivariant& &  &&  \\  
         LS+WRN&TF &Pixel-Wise& &  & & \\
        S +WRN&TF &-& &  & & \\ 
       LS+WRN& Rand &Canonical&  &  && \\
        LS+WRN&Rand &Equivariant& &  &&  \\
        LS+WRN& Rand &Pixel-Wise& &  &&  \\
        S +WRN&Rand &-& &  && \\
        WRN-16&-&-&  &  &&   \\
       ResNet-50&-&-&  &  &&   \\ \hline
    \end{tabularx}
\begin{flushleft}
\scriptsize
\vspace{-5pt}
\hspace{65pt} params : 156k for S+LL; 155k for LS+LL; 22.6M for S+WRN;  22.6M for LS+WRN; 22.3M for WRN; and 22.5M for ResNet\\
\hspace{65pt}: ours;\hspace{5pt}  TF: Tight-Frame;\hspace{5pt}LS: Learnable Scattering;\hspace{5pt}S: Scattering;\hspace{5pt} Rand: Random\\
\end{flushleft}
\vspace{-20pt}
\end{table*} 


\subsection{Robustness to Deformation}
\label{section:deformation}
In \cite{mallat2012group}, it is shown that the scattering transform is stable to small deformations of the form  where  is a signal and  a diffeomorphism. Given the substantial changes to the filter composition in the learning process, we ask now whether these seem to significantly deviate from the stability result obtained from the carefully handcrafted construction proposed in~\cite{mallat2012group} and extensively used in previous work e.g.,~\cite{bruna2013invariant,eickenberg2018solid}. To evaluate the robustness of our parametric scattering networks to different geometric distortions, we apply several tractable deformations to a chest X-ray image  with varying deformation strength and encode all images with different (learned and fixed) scattering networks. The learned ones are trained using the Morlet canonical wavelet formulation with a linear classification layer. The transformed image is denoted by . For each of the different deformation strengths, we plot the Euclidean distance between the scattering feature constructed from the original image  and the scattering feature constructed from the transformed image . We then normalize the obtained distance by  to measure the relative deviation in scattering coefficients (handcrafted or learned). Figure \ref{fig:defo} demonstrates representative results for a small rotation, shear and scale on images from the COVIDx datasets, while additional deformations are shown in Appendix \ref{appendix:deformations}. We observe that the substantial change in the filter construction retains the scattering robustness properties for these simple deformations, thus indicating that the use of learned filters (instead of designed ones) does not necessarily detract from the stability of the resulting transform.

\begin{table*}[t] 
    \centering
    \vspace{-10pt}
    \caption{COVIDx CRX-2 and KTH-TIPS2 mean accuracy \& std.\ error with  over 10 seeds and 16 seeds respectively. (COVIDx CRX-2) TF-initialized learnable scattering network performs better than models that do not incorporate scattering priors. (KTH-TIPS2) Similarly, the WRN-16-8 and ResNet-50 perform extremely poorly relative to hybrid models trained on KTH-TIPS2.}
    \label{table:covidresults}
      \fontsize{8}{8,5}\selectfont 
\begin{tabularx}{0.8\linewidth}{llllll|ll} 
             \hline
Arch.&Init. & Parameterization&C-100 samples & C-500 samples &C-1000 samples & KTH-1188 samples  \\
            \hline
            LS+LL&TF &Canonical&  & &  &  \\ 
            LS+LL&TF &Equivariant& &  & & \\
            S +LL&TF&-& & &   &\\
            LS+LL&Rand  &Canonical& & & &  \\
            LS+LL&Rand  &Equivariant& &  & &  \\
            S +LL&Rand &-& & &   &}	& \parbox{10pt}{Infer.\GB)} & \parbox{25pt}{\#Params\
        \varphi(u) = &\exp(-\frac{1}{2\sigma^2}(u_1^2(\cos^2(\theta)+\sin^2(\theta) \gamma^2)\\&+u_2^2(\cos^2(\theta) \gamma^2 + \sin^2(\theta)) \\& + 2\cos(\theta) \sin(\theta) u_1 u_2 (1-\gamma^2)) \\& + i \xi (\cos(\theta) u_1 + \sin(\theta) u_2)). \tag{4}

    \frac{\partial \varphi}{\partial \theta} (u) =& \; \frac{1}{\sigma^2}(u_2 \cos\theta - u_1 \sin\theta)(i\xi \sigma^2 + u_1 (\gamma^2-1)\cos\theta \\& + u_2(\gamma^2-1)\sin\theta)\varphi(u);\tag{5}\\
    \frac{\partial \varphi}{\partial \sigma} (u) =& \; \frac{1}{\sigma^3} (u_1^2(\cos^2\theta+ \gamma^2 \sin^2\theta )+u_2^2(\gamma^2 \cos^2\theta +\sin^2\theta)  \\&+ 2 u_1 u_2 \cos\theta \sin\theta (1-\gamma^2)) \varphi(u); \tag{6} \\
    \frac{\partial \varphi}{\partial \xi} (u) =& \; i(u_1 \cos\theta + u_2 \sin\theta) \varphi(u); \tag{7} \text{ and} \\
    \frac{\partial \varphi}{\partial \gamma} (u) =& -\frac{1}{\sigma^2}(u_1^2 \gamma \sin^2\theta  + u_2^2 \gamma \cos^2\theta  \\&- 2 u_1 u_2 \gamma \cos\theta \sin\theta ) \varphi(u). \tag{8}
-2mm]
        TF &LS+LL&Yes& &  && \\ 
        TF &LS+LL&No& &  && \\ 
            \hline
        \-2mm]
        Rand &LS+LL&Yes&  &  && \\ 
        Rand &LS+LL&No&  &  && \\  \hline
        \-2mm]
         TF &LS+WRN&Yes& &  && \\
        TF &LS+WRN&No& &  && \\ 
            \hline
        \-2mm]
        Rand &LS+WRN&Yes&  &  && \\
        Rand &LS+WRN&No&  &  && \\ 
         \hline
        \-2mm]
        TF&LS+WRN&CIFAR-10 & CE& & && -\\ 
        TF &LS+WRN& CIFAR-10 & Cosine & &  & & -\\ \hline
        \-2mm]
        TF &LS+WRN & KTH-TIPS2 & CE &  -&-&-& \\
        TF &LS+WRN & KTH-TIPS2 & Cosine &  -&-&-& \\
        \hline
    \end{tabularx}
\begin{flushleft}
\scriptsize
\vspace{-5pt}
\hspace{45pt} TF: tight-frame\hspace{5pt}LS: Learnable Scattering\hspace{5pt}S: Scattering\hspace{5pt} CE: Cross-Entropy Loss \hspace{20pt} \\
\end{flushleft}
\vspace{-18pt}
\end{table}

\section{Robustness to Deformations}

\label{appendix:deformations}
In Section~\ref{section:deformation}, we investigated if the parametric scattering transform is stable to small deformations (i.e., rotation, scale and shear). Here, we explore additional deformations. Models used were pre-trained and evaluated using the COVIDxCRX-2 setup mentioned in Section \ref{section:deformation} with a linear classifer head. The additional deformations explored are translation and several diffeomorphisms (denoted Custom 1 and Custom 2). The strengths for the all the deformations used ranges from 0 to the maximum value given in Table \ref{table:deformations}, except for scale which goes from 1 to its maximum value. Additional results are depicted in Figure \ref{fig:defo-appendix}.

Custom 1, , and Custom 2, , are defined as such:


\begin{table}[H]
    \caption{Deformations and their maximum value\vspace{-10pt}}
    \centering
    \small
    
    \label{table:deformations}
      \fontsize{10}{10}\selectfont 
    \begin{tabular}{ll}
             \hline
          Deformation & Maximum Value\\
            \hline
           Custom1 & 1 \\
           Custom2 & 1 \\
           Rotation & 10 \\
           Scale & 1.4 \\
           Shear & 5 \\
           Translation & 22 \\
\end{tabular}
\end{table}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.21]{img/Deformation/Translation.pdf}
    \includegraphics[scale=0.21]{img/Deformation/Custom_1.pdf}
    \includegraphics[scale=0.21]{img/Deformation/Custom_2.pdf}
    \vspace{-10pt}
    \caption{\textbf{Normalized distances between scattering representations of an image and its deformation.} (Left) Translation. (Middle) Custom 1 Transformation. (Right) Custom 2 Transformation. }\label{fig:defo-appendix}
\end{figure}







\section{Details of Training Unsupervised Scattering with SimCLR Objective}
\label{appendix:unsup}
The learnable scattering networks with tight-frame and random initializations were pretrained on CIFAR-10 via the SimCLR method using a temperature of 0.5 and batch size of 128 for 500 epochs. The following basic augmentations were used as part of the SimCLR augmentation pipeline: random crop and resize, random flip, and color distortion. The optimizer used was Adam, with a learning rate of 1e-3, similar to settings from \cite{chen2020simple}. Scattering weights were then frozen and used as the backbone for a linear evaluation. For linear evaluation, we used SGD with the same optimization settings as our experiments on CIFAR-10.

\section{Dataset Specific parameterizations}
\label{appendix-converging}

The following figures~(\ref{fig:covidj},\ref{fig:covidsorted},\ref{fig:kthj},\ref{fig:kthsorted},\ref{fig:cifarj}) show individual filter assignments obtained by hungarian matching as described in in \ref{section:converging}. They also show the individual parameter values of each filter by adopting a common naming scheme for each title. The first letter of the title is either F (fixed filters) or O (optimized filters). The next character is always a number and corresponds to the ID of the match. For all Oixxxxxxx titles, there will be a corresponding Fixxxxxxxx title; these filters correspond to the ith match. The next character is always D (distance). It is superseded by a numerical value, the Morlet wavelet distance (see \ref{section:converging}) between the filter and its match. The next character is the Gaussian window scale , followed by a number corresponding to the magnitude of the distances between the  parameters of both filters. The next character is the aspect ratio , followed by a number corresponding to the magnitude of the distances between the  parameters of both filters. The next character is the frequency scale , followed by a number corresponding to the magnitude of the distances between the  parameters of both filters. We omit the filter orientation  as it is easy enough to perceive visually.
\clearpage
\subsection{COVIDX-CRX2}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/fd_appendix/param_match_j_xray.pdf}
    \vspace{-50pt}
    \caption{Filters trained on 1188 samples of COVIDx-CRX2 for 500 epochs, the first, third, fifth, and seventh rows correspond to filters optimized from a tight-frame, while the second, fourth, sixth, and eighth rows correspond to tight-frame initialized filters. The filters are displayed in pairs correspond to the `closest' (by our distance metric defined above) filters of both types. For instance, the first filter of row one matches the first filter of row 2.}
    \label{fig:covidj}
\end{figure}
\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/fd_appendix/param_match_sorted_xray.pdf}
    \vspace{-50pt}
    \caption{Filters trained on 1188 samples of COVIDx-CRX2 for 500 epochs, the first, third, fifth, and seventh rows correspond to filters optimized from a tight-frame, while the second, fourth, sixth, and eighth rows correspond to tight-frame initialized filters. The filters are displayed in pairs correspond to the `closest' (by our distance metric defined above) filters of both types. For instance, the first filter of row one matches the first filter of row 2. The filters are displayed in increasing order of their distances. The top left corner corresponds to the filters that changed the least from their initialization, while the filters in the bottom right corner changed the most.}
    \label{fig:covidsorted}
\end{figure}

\clearpage
\subsection{KTH-TIPS2}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/fd_appendix/param_match_j_kth.pdf}
    \vspace{-50pt}
    \caption{Filters trained on 1188 samples of KTH-TIPS2 for 500 epochs, the first, third, fifth, and seventh rows correspond to filters optimized from a tight-frame, while the second, fourth, sixth, and eighth rows correspond to tight-frame initialized filters. The filters are displayed in pairs correspond to the `closest' (by our distance metric defined above) filters of both types. For instance, the first filter of row one matches the first filter of row two.}
    \label{fig:kthj}
\end{figure}
\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/fd_appendix/param_match_sorted_kth.pdf}
    \vspace{-50pt}
    \caption{Filters trained on 1188 samples of KTH-TIPS2 for 500 epochs, the first, third, fifth, and seventh rows correspond to filters optimized from a tight-frame, while the second, fourth, sixth, and eighth rows correspond to tight-frame initialized filters. The filters are displayed in pairs correspond to the 'closest' (by our distance metric defined above) filters of both types. For instance, the first filter of row one matches the first filter of row two. The filters are displayed in increasing order of their distances. The top left corner corresponds to the filters that changed the least from their initialization, while the filters in the bottom right corner changed the most.}
    \label{fig:kthsorted}
\end{figure}

\subsection{CIFAR-10}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/fd_appendix/param_match_j_cifar.pdf}
    \vspace{-30pt}
    \caption{Filters trained on 1190 samples of CIFAR-10 for 500 epochs, the first and third,  rows correspond to filters optimized from a tight-frame, while the second and fourth rows correspond to tight-frame initialized filters. The filters are displayed in pairs correspond to the `closest' (by our distance metric defined above) filters of both types. For instance, the first filter of row one matches the first filter of row two.}
    \label{fig:cifarj}
\end{figure}

\subsection{Dataset Specific initializations with a Random Initialization}\label{appendix:random-init}
In Figure~\ref{fig:graphlwp_random}, we show how the filters adapt when initialization begins from a random setting. We note the deviation to tight frame is much greater than in the case where we initialize in a tight frame. However, as per our filterbank distance, we observe the filters do move closer to the tight frame than their initialization.
\begin{figure}[H]
    \centering
\includegraphics[width=0.7\textwidth]{img/graph_lwp/graph_lwp_w7_random_2-3.pdf}
    \vspace{-7pt}
    \caption{The graph shows the \textit{filterbank distance} over epochs as the filters, initialized from random initialization, are trained on different datasets. To the left, we visualize dataset specific parameterizations of scattering filterbanks in Fourier space. The graph on the right shows that the randomly initialized filterbanks become more similar to a tight frame during training.}
    \label{fig:graphlwp_random}
\end{figure}

\section{Number of Filters Ablation}
\label{appendix:num_filter}
In this section, we investigate the effect of modifying the number of filters () per spatial scale on CIFAR-10. We use the same settings and hyperparameters as described in Appendix~\ref{appendix-details}. So far, in all experiments, we have set the number of filters per scale at 8 for fair comparison with parameters.

For this ablation, we train a parametric scattering network followed by a linear layer where the wavelets are initialized using the tight frame construction and the canonical parameterization. The spatial scale is set to 2. We do not use autoaugment on the training set since, as shown in Appendix~\ref{appendic:noautoaugment}, autoaugment can be harmful when the scattering network is followed by a linear layer. Table~\ref{table:L} shows the accuracy of the full training set for different values of . We observe that the performance increases when the number of filters per scale also increases. Around 14-16 filters per spatial scale, the performance seems to have stopped increasing. 

Table~\ref{table:Lcompare} demonstrates the mean accuracy over different sizes of training samples where the number of filters per scale is set to 8 or 16. We also consider the fixed version of the scattering transform. Over all training sample sizes, we observe that the highest performance is obtained with learnable scattering using 16 filters per spatial scale. When the scattering network is fixed, we observe that performances are lower with 16 filters instead of 8. It seems that increasing the number of filters per scale is only beneficial in the learned version of the scattering network. 


\begin{table}[H] 
    \centering
    \caption{CIFAR-10 accuracy of learnable scattering followed by a linear layer (LS + LL) and multiple numbers of filters per scale () trained on all the training set. The wavelet filters are initialized using the tight frame construction and the canonical parameterization. The spatial scale is set to 2. No autoaugment is used for this experiment. We observe that the performance increases when the number of filters per scale () also increases. Around 14 filters per spatial scale, the performance seems to have stopped increasing. } 
    \label{table:L}
    \fontsize{9}{9}\selectfont 
    \begin{tabularx}{0.15\linewidth}{ll} 
          \hline
         L & All Data\\
        \hline
        \-2mm]
        LS+LL&Canonical& 8& &  
        &&\\ 
        LS+LL&Canonical& 16& & 
         && \\ 
        S +LL&-&8 & &  && \\
        S +LL&-&16 & 	 &   &  & \\
        \hline
    \end{tabularx}
\end{table} 


\section{Perturbing a Converged Parameterization Ablation}

The training accuracies and losses over epochs are shown in Figure~\ref{fig:learningcurve} for LS+L and S+L. We observe that both networks are following similar patterns. Local minima are expected here (see results with different init Figure~\ref{fig:filters-cifar-real-before-kymatio}), as in any non-convex problem. To showcase learnable scattering's stability to stochastic optimization, we perform an ablation below (Figures~\ref{fig:pert1} \& \ref{fig:pert2}) for trained LS+L, where each parameter is perturbed and then individually optimized. We find that they all return rapidly to the (local) optimum.




\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{img/learning_curves/accuracy_LSLLvsSLL.pdf}\qquad
\includegraphics[width=0.45\textwidth]{img/learning_curves/loss_LSLLvsSLL.pdf} \caption{Plots comparing the training accuracies and losses over epochs of LS+LL and S+LL. Note that scattering parameters are only optimized for LS+LL. The networks were trained for 500 epochs on 1000 samples of CIFAR-10.} \label{fig:learningcurve}
\end{figure*}

\begin{figure}
\noindent\adjustbox{width=\columnwidth}{
         \begin{tabular}{ccc|ccc}
         \textbf{Initial} & \textbf{Perturbed} & \textbf{Final} & \textbf{Initial} & \textbf{Perturbed} & \textbf{Final}\\
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation00.pdf}}& 
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation01.pdf}} & 
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation02.pdf}} &
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation10.pdf}}& 
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation11.pdf}} & \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation12.pdf}} \\  
\end{tabular}}
    \caption{(left)   parameter of a learned parameterization is perturbed and returns to an almost identical orientation after gradient based optimization. (right)  parameter of a learned parameterization is perturbed and returns to an almost identical frequency scale after gradient based optimization. }
    \label{fig:pert1}
\end{figure}

\begin{figure}
\centering
\small
\noindent\adjustbox{width=\columnwidth}{
         \begin{tabular}{ccc|ccc}
         \textbf{Initial} & \textbf{Perturbed} & \textbf{Final} & \textbf{Initial} & \textbf{Perturbed} & \textbf{Final}\\
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation20.pdf}}& 
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation21.pdf}} & 
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation22.pdf}} &
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation30.pdf}}& 
         \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation31.pdf}} & \adjustbox{width=1.8cm}{\includegraphics[trim=2.3in 1.3in 2.1in 1.4in, clip]{img/perturbations/perturbation32.pdf}} \\ 
\end{tabular}}
    \caption{(left)   parameter of a learned parameterization is perturbed and returns to an almost identical gaussian window scale after gradient based optimization. (right)  parameter of a learned parameterization is perturbed and returns to an almost identical aspect ratio after gradient based optimization.}
    \label{fig:pert2}
\end{figure}



\clearpage
\section{Visualizing Parameter Values over Time}

 Figure~\ref{fig:params_over_time} shows, on the left, a Morlet wavelet filter before training and, in the middle, the same wavelet filter after training. The parametric scattering network was trained for 1K epochs on 1000 samples of CIFAR-10. We use the Morlet canonical parameterization. We observe that the global orientation has changed during training. Figure~\ref{fig:params_over_time} (right) shows the values of the parameters from Table~\ref{tab:params} over the training steps. We observe that the value of all parameters changed during training. However, the aspect ratio seems to have returned to its initial value. In Figure~\ref{fig:params_over_time} (right), we also show the value of , which is a proxy for the wavelet scale factor, over the steps. The value of  is smaller after training. This can be observed in Figure~\ref{fig:params_over_time} (left-middle), where the wavelet after training seems to be more compressed than before training.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/parameters_over_time/parameters_over_time.pdf}
    \caption{Parameter values over time. Real part of Morlet wavelet filters initialized with \textit{tight-frame}  schemes before (left) and after (middle) training. The network was trained for 1K epochs on 1000 samples of CIFAR-10. We use the Morlet canonical wavelet parameterization. The plot (right) shows the parameter values over epochs. }
    \label{fig:params_over_time}
\end{figure}



\end{document}
