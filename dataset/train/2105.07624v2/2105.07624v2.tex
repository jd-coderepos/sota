
\subsection{Baselines}
\label{sec:baselines}
\noindent \textbf{Textual QA Models}
We adopt two reading comprehension (RC) models as baselines over textual data: BERT-RC~\citep{Devlin2018Bert}, which is a SQuAD-style RC model;
and NumNet+ V2~\footnote{https://github.com/llamazing/numnet\_plus}~\citep{ran2019numnet}, which achieves promising performance on DROP that requires numerical reasoning over textual data.
We adapt them to our \finqa~as follows.
We convert the table to a sequence by row, also as input to the models, followed by tokens from the paragraphs. 
Besides, we add a multi-class classifier, exactly as in our \tagop, to enable the two models to predict the scale based on Eq.~\eqref{eq_scale}.

\noindent \textbf{Tabular QA Model}
We employ \tapas~for WikiTableQuestion (WTQ)~\citep{herzig2020tapas} as a baseline over tabular data.
\tapas~is pretrained over large-scale tables and associated text from Wikipedia jointly for table parsing.
To train it, we heuristically locate the evidence in the table with the annotated answer or derivation, which is the first matched one if a same value appears multiple times.
In addition, we remove the ``numerical rank id'' feature in its embedding layer, which ranks all values per numerical column in the table but does not make sense in \finqa. 
Similar to above textual QA setting, we add an additional multi-class classifier to predict the scale as in Eq.~\eqref{eq_scale}.

\noindent \textbf{Hybrid QA Model}
We adopt HyBrider~\citep{chen2020hybridqa} as our baseline over hybrid data, which tackles tabular and textual data from Wikipedia.
We use the code released in the original paper\footnote{https://github.com/wenhuchen/HybridQA}, but adapt it to \finqa. 
Concretely, each cell in the table of \finqa~is regarded as ``linked'' with associated paragraphs of this table, like hyperlinks in the original paper, and we only use its cell matching mechanism to link the question with the table cells in its linking stage.
The selected cells and paragraphs are fed into the RC model in the last stage to infer the answer.
For ease of training on \finqa, 
we also omit the prediction of the scale, i.e. 
we regard the predicted scale by this model as always correct.

\subsection{Evaluation Metrics}
We adopt the popular Exact Match (EM) and numeracy-focused \fone{} score \citep{Dua2019DROP} to measure model performance on \finqa. 
However, the original implementation of both metrics is insensitive to whether a value is positive or negative in the answer as the minus is omitted in evaluation.  
Since this issue is crucial for correctly interpreting numerical values, especially in the finance domain, we keep the plus-minus of a value when calculating them.
In addition, the numeracy-focused \fone{} score is set to $0$ unless the predicted number multiplied by predicted scale equals exactly the ground truth.


\subsection{Results and Analysis}

In the following, we report our experimental results on dev and test sets of \finqa.

\noindent \textbf{Comparison with Baselines}
We first compare our \tagop~with three types of previous QA models as described in Section~\ref{sec:baselines}.
The results are summarized in \tabref{overall-metric}.
It can be seen that our model is always superior to other baselines in terms of both metrics, with very large margins over the second best, namely  50.1/58.0 vs. 37.0/46.9 in EM/\fone{} on test set of \finqa~respectively.
This well reveals the effectiveness of our method that reasons over both tabular and textual data involving lots of numerical contents. 
For two textual QA baselines, NumNet+ V2 performs better than BERT-RC, which is possibly attributed to the stronger capability of numerical reasoning of the latter, but it is still worse than our method.
The tabular QA baseline Tapas for WTQ is trained with only tabular data in \finqa, showing very limited capability to process hybrid data, as can be seen from its performance.
The HyBrider is the worst among all baseline models, because it is designed for HybridQA~\citep{chen2020hybridqa} which does not focus on the comprehensive interdependence of table and paragraphs, nor  numerical reasoning.


However, all the models perform significantly worse than human performance\footnote{The human performance is evaluated by asking annotators to answer 50 randomly sampled hybrid contexts (containing 301 questions) from our test set. 
Note the human performance is still not 100\% correct because our questions require relatively heavy cognitive load like tedious numerical calculations. Comparing human performance of \fone{} in SQUAD~\citep{Rajpurkar2016SQuAD} ($86.8$\%) and DROP~\citep{Dua2019DROP}) ($96.4$\%), the score ($90.8$\%) in our dataset already indicates a good quality and annotation consistency in our dataset.}, indicating \finqa~is challenging to current QA models and more efforts on hybrid QA are demanded.

\noindent \textbf{Answer Type and Source Analysis}
Furthermore, we analyze detailed performance of \tagop~w.r.t answer type and source ~in \tabref{detail-metric}.
It can be seen that \tagop~performs better on the questions whose answers rely on the tables compared to those from the text.
This is probably because table cells have clearer boundaries than text spans to the model, thus it is relatively easy for the model to extract supporting evidences from the tables leveraging sequence tagging techniques.
In addition, \tagop~performs relatively worse on arithmetic questions compared with other types.
This may be because the calculations for arithmetic questions are diverse and harder than other types, indicating the challenge of \finqa, especially for the requirement of numerical reasoning.


\begin{table}
    \small
    \centering
    \begin{tabular}{lrrrr}
    \toprule
     \multirow{2}{*}{\bf Method}    & \multicolumn{2}{c}{\bf Dev} & \multicolumn{2}{c}{\bf Test} \\
     \cmidrule(lr){2-3}
     \cmidrule(lr){4-5}
         & EM & \fone{} & EM&  \fone{}\\
    \midrule
    
\bf Human  &   -   &   -   &    84.1   &   90.8   \\
\addlinespace


    \multicolumn{5}{l}{\bf Textual QA}\\
    BERT-RC   &      9.5 & 17.9 & 9.1 & 18.7  \\
    NumNet+ V2   &  38.1 & 48.3 & 37.0 & 46.9 \\
    \addlinespace
    
    \multicolumn{5}{l}{\bf Tabular QA}\\
    \tapas~for WTQ    &  18.9 & 26.5 & 16.6 & 22.8 \\
    \addlinespace

    \multicolumn{5}{l}{\bf Hybrid QA}\\
     HyBrider    &  6.6 & 8.3 & 6.3 & 7.5 \\

    \addlinespace

     \bf \tagop & \bf 55.2 & \bf 62.7 & \bf 50.1 & \bf 58.0 \\
    \bottomrule
    \end{tabular}
    \caption{Performance of different models on dev and test set of \finqa. Best results are marked in bold.}
   
    \label{tab:overall-metric}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{L{1.2cm}C{1.0cm}C{1.0cm}C{1.3cm}C{1.0cm}}
\toprule
& Table & Text & Table-text & Total \\
\cmidrule{2-5}
& EM/\fone{} & EM/\fone{} & EM/\fone{} & EM/\fone{}
\\
\midrule
Span &  56.5/57.8 & 45.2/70.6 & 68.2/71.7 & 54.1/67.9 \\
Spans & 66.3/77.0 & 19.0/59.1 & 63.2/76.9 & 60.0/75.1 \\
Counting & 63.6/63.6 & -/- & 62.1/62.1 & 62.5/62.5 \\
Arithmetic & 41.1/41.1 & 27.3/27.3 & 46.5/46.5 & 42.5/42.5 \\
Total &  47.8/49.3 & 43.3/68.7 & 58.3/62.2 & 50.1/58.0 \\
\bottomrule
\end{tabular}
\caption{Detailed experimental results of \tagop~w.r.t. answer types and sources on test set.
}

    \label{tab:detail-metric}    
\end{table}

\noindent \textbf{Results of \tagop~with Different Operators}
We here investigate the contributions of the ten aggregation operators to the final performance of \tagop.
As shown in \tabref{op-ablation}, we devise nine variants of the full model of \tagop; based on the variant of \tagop~with only one operator (e.g. Span-in-text), for each of other variants, we add one more operator back.
As can be seen from the table, all added operators can benefit the model performance.
Furthermore, we find that some operators like \emph{Span-in-text}, \emph{Cell-in-table}, \emph{Difference} and \emph{Average} make more contributions than others.
In comparison, \emph{Sum} and \emph{Multiplication} bring little gain or even decline.
After analysis, we find this is because the instances of \emph{Sum} or \emph{Multiplication} are minor in our test set, which are easily influenced by randomness.

\begin{table}
    \small
    \centering
    \begin{tabular}{lrrrr}
    \toprule
     \multirow{2}{*}{\bf Model}    & \multicolumn{2}{c}{\bf Dev} & \multicolumn{2}{c}{\bf Test} \\
     \cmidrule(lr){2-3}
     \cmidrule(lr){4-5}
         & EM & \fone{} & EM&  \fone{}\\
    \midrule
    + Span-in-text & 13.4 & 20.5 & 14.1 & 21.8 \\
    + Cell-in-table & 25.4 & 36.0 & 24.1 & 35.3 \\
    + Spans & 33.6 & 41.3 & 31.3 & 39.4 \\
    + Sum & 33.8 & 41.3 & 31.2 & 39.1 \\
    + Count & 35.9 & 43.5 & 32.7 & 40.6 \\
    + Average & 43.3 & 50.6 & 38.2 & 45.9 \\
    + Multiplication & 44.2 & 51.4 & 37.9 & 46.0 \\
    + Division & 45.0 & 52.5 & 39.2 & 47.5 \\
    + Difference & 51.4 & 58.7 & 45.1 & 53.3 \\
    + Change ratio (Full) & \bf 55.2 & \bf 62.7 & \bf 50.1 & \bf 58.0 \\
    \bottomrule
    \end{tabular}
    \caption{Performance with different aggregation operators of \tagop~model.}
    \label{tab:op-ablation}
\end{table}


\noindent \textbf{Error Analysis}
We further investigate our \tagop~by analysing error cases. 
We randomly sample $100$ error instances from the test set, and classify them into five categories as shown in \tabref{error-case}, each with an example:
(1) \emph{Wrong Evidence} ($55$\%), meaning the model obtained wrong supporting evidence from the hybrid context;
(2) \emph{Missing Evidence} ($29$\%), meaning the model failed to extract the supporting evidence for the answer;
(3) \emph{Wrong Calculation} ($9$\%), meaning the model failed to compute the answer with the correct supporting evidence;
(4) \emph{Unsupported Calculation} ($4$\%), meaning the ten operators defined cannot support this calculation;
(5) \emph{Scale Error} ($3$\%), meaning the model failed to predict the scale of the numerical value in an answer.

We can then observe about $84$\% error is caused by the failure to extract the supporting evidence from the table and paragraphs given a question.
This demonstrates more efforts are needed to strengthen the model's capability of precisely aggregating information from hybrid contexts.


After instance-level analysis, we find another interesting error resource is the dependence on domain knowledge. 
While we encourage annotators to create questions answerable by humans without much finance knowledge, we still find domain knowledge is required for some questions.
For example, given the question \emph{``What is the gross profit margin of the company in 2015?''},  the model needs to extract the gross profit and revenue from the hybrid context and compute the answer according to the finance formula \emph{(``gross profit margin = gross profit / revenue'')}. 
How to integrate such finance knowledge into QA models to answer questions in \finqa~still needs further exploration.

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{L{1.6cm}|L{5.2cm}}
\toprule
\multirow{3}{*}{ \makecell[l]{Wrong\\ Evidence\\(55\%)}} & Q: How much did the level 2 OFA change by from 2018 year end to 2019 year end? \\
& G: \textcolor{blue}{375} - 2,032 \\
& P: \textcolor{red}{1,941} - 2,032 \\
\midrule
\multirow{3}{*}{ \makecell[l]{Missing\\ Evidence\\(29\%) }} &  Q: How many years did adjusted EBITDA exceed \$4,000 million? \\
& G: \emph{count}(2017, 2018, \textcolor{blue}{2019}) \\
& P: \emph{count}(2017, 2018) \\
\midrule
\multirow{3}{*}{ \makecell[l]{Wrong\\Calculation \\
(9\%)} }
& Q: What is the change in the \% of pre-tax loss from 2018 to 2019? \\
& G: \textcolor{blue}{39\% - 20\%} \\
& P: \textcolor{red}{20\% - 39\%} \\
\midrule
\multirow{3}{*}{ \makecell[l]{Unsupported\\Calculation \\
(4\%)} } & Q: What is the proportion of investor relations and consultants over the total operating expense in 2019? \\
& G: \textcolor{blue}{(105,639 + 245,386) /19,133,139} \\
& P: \textcolor{red}{245,386 / 19,133,139} \\
\midrule

\multirow{3}{*}{\makecell[l]{ Scale\\Error\\(3\%)} } & Q: What is the closing price in March, 2020? \\
& G: 0.22 \\
& P: 0.22 \textcolor{red}{million} \\
\bottomrule
\end{tabular} 
\caption{Examples of error and corresponding percentage. 
Q, G, P denote question, ground truth, prediction.}
\label{tab:error-case} 
\end{table}


