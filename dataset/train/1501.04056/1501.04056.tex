\documentclass{article}                     

\textwidth 17cm
\textheight 24cm
\hoffset -2.5cm \voffset -2.5cm 


\usepackage{amsmath,amssymb,graphicx,color,relsize,here}
\newcommand{\pmat}[1]{\begin{pmatrix} #1\end{pmatrix}}
\newtheorem{definition}{\bf Definition}
\newtheorem{propos}{\bf Proposition}[section]
\newtheorem{corollary}{\bf Corollary}[section]
\newcommand{\bfu}{{\mathbf{u}}}
\newtheorem{hyp}{\bf Assumption}[section]
\newtheorem{theorem}{\bf Theorem}[section]
\newcommand{\bfx}{{\mathbf{x}}}
\newcommand{\hbfu}{\hat{\mathbf{u}}}
\usepackage{tikz}
\usetikzlibrary{arrows,calc}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{mindmap,trees,backgrounds}
\usepackage{bm}
\usetikzlibrary{scopes}
\usepackage{lineno,hyperref,amsmath,amssymb,pgfplots}
\usepackage{pgf,tikz}
\usetikzlibrary{arrows}



\newtheorem{property}{\bf Property}
\newtheorem{assumption}{\bf Assumption}
\newtheorem{proposition}{\bf Proposition}
\newtheorem{remark}{\bf Remark}


\usepackage{graphicx}
\begin{document}

\title{NLP Solutions as Asymptotic Values of ODE Trajectories}
\author{Mazen Alamir \ \\ \ \\  CNRS/Gipsa-lab Control systems department.\\ Email : mazen.alamir@grenoble-inp.fr}




\date{}



\maketitle

\begin{abstract}
\noindent In this paper, it is shown that the solutions of general differentiable constrained optimization problems can be viewed as asymptotic solutions to  sets of Ordinary Differential Equations (ODEs). The construction of the ODE associated to the optimization problem is based on an exact penalty formulation in which the weighting parameter dynamics is coordinated with that of the decision variable so that there is no need to solve a sequence of optimization problems, instead, a single ODE has to be solved using available efficient methods. Examples are given in order to illustrate the results. This includes a novel systematic approach to solve combinatoric optimization problems as well as fast computation of a class of optimization problems using analogic circuits leading to fast, parallel and highly scalable solutions.
\end{abstract}

\section{Introduction}
\label{secintro}
Consider the following optimization problem with inequality constraints:
 
where  and 's are scalar functions of the decision variable . Let  be the exact penalty induced function defined by:
 
where  is given by:
 
A wide class of algorithms intends to solve (\ref{defdeP}) by solving a sequence of unconstrained optimization problems of the form
 
for a varying (generally increasing) values of the weighting coefficient . For each problem in the sequence, only  is searched for while  is kept constant \cite{Byrd:99,Birgin:2012}. The series of unconstrained problems are sometimes replaced a series of problems with by box constraints as descent method with projection are easy to perform. \ \\ \ \\ 
The increase of  is generally defined by  with . The sequence of precision parameter  (to which the intermediate problems have to be solved) is also made such that  in order to avoid solving with a uselessly high precision intermediate problems. It is then obvious that the efficiency of the resulting algorithms is tightly related to the choice of  and the intermediate precision sequence  since small values of  leads to unnecessarily high number of intermediate problems while a too high values of  leads to stiff problems that may lead to slow convergence (because this makes the solution of the box-constraint subproblem harder \cite{Birgin:2012}) beside the fact that it breaks the {\em continuation} argument that underlies the whole scheme. The choice of  corresponds to similar trade-offs that need to be carefully handled. Such issues are extensively studied in \cite{Birgin:2012} leading to non necessarily monotonic behavior of  when solving the sequence of intermediate box constrained modified Lagrangian problems in order to avoid high number of iterations that result when  is unnecessarily high. This recent study \cite{Birgin:2012} shows at least that monitoring the dynamic evolution of  is not a trivial issue.\ \\ \ \\ 
In this paper, it is shown that simultaneous dynamics of  and  can be defined through a differential equation of the form:
 
such that solving (\ref{eqdiff}) gives trajectories that asymptotically converge towards the set of solutions of (\ref{defdeP}). \ \\ \ \\ 
Note that by doing so, the present paper does not propose a specific alternative algorithm to solve the NLP problem (\ref{defdeP}). Rather, it enables all the efficient algorithms that are available through the huge literature on ODE integration to become candidate algorithms for (\ref{defdeP}). Moreover all the computational background regarding many issues (such as parametric sensitivity \cite{Leis:1988}, parallel computing \cite{Voss199765},  precision monitoring to cite but few items) become  available for the constrained optimization problem paradigm. As such, the result of the present paper can be viewed as a starting point for future investigation. More interestingly, it is shown briefly in this paper that expressing the fact that solving (\ref{defdeP}) can be done by integrating ODEs enables (for some specific problems) to built analogic circuits that can achieve the task in fast, parallel and massively scalable way. \ \\ \ \\ 
This paper is organized as follows: first section \ref{secdefnot} gives the definitions and notation used throughout the paper. Section \ref{secworkingassumptions} states the working assumptions that are needed to derive the main results of the paper. These results are stated and proved in section \ref{secmainresult} while section \ref{secexamples} gives some examples of application of the paper results. Finally the paper ends with section \ref{secconclusion} that summarizes the contribution and gives hints for further investigations.    
\subsection{Definition \& Notation} \label{secdefnot} 
\noindent Throughout the paper, the following notation is used. The -dimensional vectors ,  and  denote the gradients of ,  and  w.r.t . The scalar function  denotes the euclidian norm of  at , namely:
 
For a given weighting coefficient , the set of stationary points of  is denoted by , namely:
 
For any , the notation  refer to the distance between  and the set , namely:
 
The set of admissible values of  is denoted by:
 
\section{Working Assumptions} \label{secworkingassumptions} 
\noindent The first assumption states that the optimization problem is well posed in the sense that either the original cost  is already lower bounded or the constraints are such that the weighted cost  is lower bounded:\\
\begin{assumption}{\bf [Well posedness]}\label{ass1} 
For any , there is a lower bound  such that  for all .
\end{assumption}
\ \\
Note that in the framework of the present paper, it is not assumed that the functions involved are convex. This means that the set  may not be a singleton , it is assumed that the norm of the gradient of the weighted cost  {\bf away} from  can be bounded below by the distance to the set  through some coefficient . This leads to the following generalization of the strong convexity assumption:\\
\begin{assumption}{\bf [)-Strong Convexity]} \label{ass2} 
There is a constant  such that the following inequality holds:
 
for all .
\end{assumption}
\ \\
Note that contrary to the classical strong convexity assumption that involves two arbitrary points  and , the inequality (\ref{defdekc}) involves the distance from an arbitrary  to those points lying inside the set of stationary points .\ \\ \ \\ 
The Next assumption describes a generalized Lypschitz-like assumption on the constraints and the way they are used to construct the exact penalty term . \\
\begin{assumption}{\bf [Growth rate of ]} \label{ass3}  There is a polynomial  of degree  with   that satisfies the  following inequality
 
for all . 
\end{assumption}
\ \\
Note that the use of the polynomial  of the form:
 
accounts for the possibility to use different penalty exponents  in the definition of the constraint penalty term in (\ref{defdepsi}) and the fact that the bounding function may involve lower powers for small distances  and higher powers far from the set . \ \\ \ \\ 
The following assumption is needed to guarantee the existence of solutions to the ODE built up with the functions  and :\\
\begin{assumption}{\bf [Locally-Lypschitz maps]} \label{ass4} 
For all finite  the maps ,  and  are {\bf locally} Lypschitz.
\end{assumption}
\ \\
Note that this last assumption expresses {\em local} requirement while (\ref{defdekc}) and (\ref{defdegrowthpsi}) are required to hold for any .\ \\ \ \\ 
The last assumption concerns the relevance of the use of the penalty method to solve (\ref{defdeP}). It states that when the penalty coefficient  goes to infinity, the  possible stationary points for the weighted cost converge towards the admissible set :\\
\begin{assumption}{\bf [Relevance of the penalty approach]}\label{ass5} 
 
\end{assumption}
\ \\ 
This assumption is almost implicitly required in any penalty-based approach to solve the constrained optimization problem (\ref{defdeP}). It can obviously be replaced by some more apparently trivial assumptions that can be used to prove (\ref{eqass}). The short form is preferred here for the sake of clarity.  
\section{Main Results} \label{secmainresult}
The main result of the paper can be stated in the following proposition:\\
\begin{proposition}{\bf [Main Result]} \label{prop1} 
Assume that some  is chosen. Consider the following system of differential equations:
 
{\bf if} the following conditions hold:
\begin{enumerate}
\item Assumptions \ref{ass1}-\ref{ass5} are satisfied
\item  [see (\ref{defdePol})]
\end{enumerate}
{\bf then} for any , there is a sufficiently small  such that any asymptotic solution of (\ref{ode1})-(\ref{ode2}) satisfies the KKT necessary conditions of optimality for the constrained optimization problem (\ref{defdeP}). 
\end{proposition}
\ \\
{\sc Proof}. Let us compute the derivative of the weighted cost :
 
Let  be the closest point to  that lies inside the stationary set . According to (\ref{defdegrowthpsi}) of Assumption \ref{ass3}, one can write:
  
Now by virtue of Assumption \ref{ass5},  satisfies the following asymptotic property:
 
Therefore, (\ref{tfr5}) becomes [using (\ref{defdePol})] :
 
On the other hand, the sum in the r.h.s of (\ref{iju8}) satisfies [because of (\ref{defdekc})] the following inequality:
 
Now using (\ref{pkhg58})  and (\ref{tgFFFF}) in (\ref{iju8}) enables to write [dropping all the terms with indices higher than  in the summing term of (\ref{iju8})] and using the identity :
 
where  while  and  are given by:
 
and taking  sufficiently small so as to satisfy the following inequality:
 
the following inequalities hold for  and :
 
With these inequalities, inequality (\ref{POh6}) implies:
 
Let us now show that inequality  (\ref{POh687}) implies that . Indeed, if this was not the case, then by the very definition of the dynamic on  [see (\ref{ode2})] it comes that  goes to infinity. This together with (\ref{POh687}) and the lower boundedness of  [Assumption \ref{ass1}] implies that =0 ( converges to the set ). But this implies by (\ref{eqass}) of assumption \ref{ass5} that  converges to  which  contradicts the assumption. Now since  converges to , the inequality (\ref{iju8}) together with the lower boundedness of  implies also that  converges to .\ \\ \ \\ 
By now it has  been shown that provided that  is sufficiently small to satisfy (\ref{suffsmall}), the trajectory of  converges to the following set
 
It remains to prove that if  belongs to the set defined by (\ref{theset}), then  satisfies the KKT necessary conditions of optimality. Remember that these conditions require the existence of a vector  such that the following conditions hold:
 
But  can be explicitly written as follows:
 
which obviously shows that by taking  such that:
 
the first KKT condition is satisfied by construction. The second condition () results from . The third and the fifth conditions ( and ) result from (\ref{tgt543210}). This ends the proof.  
\ \\ \ \\ 
Note that if the summation in (\ref{ode1}) is performed with an infinite number of terms, the following corollary can be obtained:\\
\begin{corollary} \label{corollary1} 
Assume that some  is chosen. Consider the following system of differential equation:
 
{\bf If} Assumptions \ref{ass1}-\ref{ass5} hold then for sufficiently small , any asymptotic solution of (\ref{odee1})-(\ref{odee2}) satisfies the necessary conditions of optimality for the constrained optimization problem (\ref{defdeP}).      
\end{corollary}
\ \\
Note that the result of Proposition \ref{prop1} holds for any initial condition that can be used to initialize the trajectory of (\ref{ode1})-(\ref{ode2}). The price to obtain such a global result lies in the use of the -term summation  that premultiplies the gradient term  in (\ref{ode1}). The next proposition gives a weaker result that can nevertheless be preferable in some circumstances. In this weaker result, the convenient sufficiently small  would depend on the initial values of  and . \ \\ 
\begin{proposition}{\bf [A Simpler Weaker Result]} \label{prop2} 
Assume that some  is chosen. Consider the following system of differential equations:
 
{\bf If} the following conditions hold:
\begin{enumerate}
\item Assumptions \ref{ass1}-\ref{ass5} are satisfied,
\item  is proper (that is )
\end{enumerate}  
then for any initialization , there is sufficiently small  such that the resulting asymptotic solution of (\ref{ode1bis})-(\ref{ode2bis}) satisfies the KKT necessary conditions of optimality for the optimization problem (\ref{defdeP}).  
\end{proposition}
\ \\
{\sc Proof}. Note that the result  can be obtained if one can show that everything behaves as if  holds in (\ref{defdegrowthpsi}). Indeed, in this case  can be used and (\ref{ode1}) is equivalent to (\ref{ode1bis}). This can be done using classical arguments that are typically used to derive semi-global results. More precisely, given the initial state , define the following level set in :

to which the initial value  obviously belongs [because  for all ]. Note that since  is proper by assumption, the set  is a compact set. Consequently, there is some sufficiently high  such that the following inequality holds for all :
 
this means that as far as the trajectory remains in , the result of Proposition \ref{prop1} can be used with  therefore, there exists sufficiently small  such that the dynamics defined by (\ref{ode1}) with  [which is the same as (\ref{ode1bis})] decreases the value of . But this guarantees that the trajectory of  remains in . This implies that the inequality (\ref{ed6g5}) remains true and the result obviously follows.      \ \\ \ \\ 
Note that such finite  exists as long as the last inequality in (\ref{ed6g5}) is required only on the compact set . The latter is defined in terms of the initial paire . This is why the value of  does depend on the initialization and may not exit globally. 
\subsection{General Comments}
\noindent Before getting to the examples section, it is worth mentioning that the results of the present section build a theoretical bridge between NLP and ODE algorithms in a rather systematic way and for a large class of problems. However, it must be underlined that although the following examples show rather efficient computational results, the integration of the resulting ODE may not be the more efficient way to solve the underlying optimization problems. This is because integration schemes try to reproduce high precision solution over the whole trajectories while from the NLP solution point of view, only the asymptotic trajectory matters. \ \\ \ \\ 
To this respect, the results of the present section can be used to derive gradient-based algorithms (fast gradient for instance \cite{Nesterov1983,Nesterov:04}) using the r.h.s of the ODE as extended gradient in the extended space of  with  as cost function. By doing so, even certification results similar to the one proposed in \cite{Richter:2012} can be extended from the case where only saturations on the control input is used to the more general case of affine constraints on the state. This being said, no such efficiency-oriented development is done here focusing on the main theoretical contribution of the paper. \ \\ \ \\ 
On the other hand, another consequence of the theoretical result of the present section is the possibility to built electronic circuits that realize analogic ultra-fast integration of the ODE for a class of NLPs. This is briefly discussed in section \ref{circuits}. 
\section{Illustrative Examples} \label{secexamples} 
\subsection{Example 1: QP problems}
\noindent As a first examples let us consider the use of the ODE framework described in Proposition \ref{prop1} to solve Quadratic Programming (QP) problem with inequality constraints. This leads to the following instantiation of the cost function  and the constraints : 
  
\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{validation_qp.jpg}
\end{center}
\caption{{\bf Example 1}. Trajectories of  and  for  different randomly generated problems. Note that the constraints are asymptotically satisfied () and that the resulting costs converge towards the exact optimal value  (since ). For all trials, the initial conditions  is used. Initial values of  shows initial strong violation of the constraints.} \label{validation_qp} 
\end{figure}
where , ,  and . Now using  to define the constraints-related weighting term:
 
gives  [see (\ref{defdegrowthpsi}) and (\ref{defdePol})]. Consequently, following Proposition \ref{prop1}, the following ODE is defined (taking ):
  
where 
 
This ODE is then integrated using the Matlab ODE15s stiff solver to get the solution of the original QP problem defined by (\ref{defdeqp1})-(\ref{defdeqp2}). Fifty randomly generated sets of matrices  are generated leading to  feasible QPs with  unknown and  constraints. The resulting ODEs (\ref{odeqp1})-(\ref{odeqp2}) are defined with the parameters  and . Figure \ref{validation_qp} shows the resulting trajectories of  and the cost function normalized by the optimal cost value (computed using the standard Matlab QuadProg solver). All the trajectories are started from  and . The Figure clearly shows that the trajectories converge to the solutions of the problems as the constraints are satisfied and the cost function values converge toward the optimal values for all the generated problems. 
Figure \ref{evolution_xi_qp} shows a typical behavior of the system's trajectory starting from  and converging towards the optimal values. The computation times shows a mean of  with a variance of  (Using Matlab on Mac PowerBook OSX, 2.8 GHz Intel Core i7 processor).
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{evolution_xi_qp}
\end{center} 
\caption{{\bf Example 1}. Typical behavior of of the components of  on the trajectories of the ODE. Initial value  is used.}\label{evolution_xi_qp}
\end{figure}

\subsection{Example 2: Analogic MPC Solvers} \label{circuits} 
\noindent Recall that Model Predictive Control (MPC) for linear time invariant systems of the form:
 
is based on the repetitive solution of a quadratic programming problem of the form:
 
under the constraint:
 
where  is the parameter vector that defines the control trajectory over the prediction horizon, namely:
  
for some appropriately chosen parametrization matrix  where  is the dimension of the control input . Note that the only difference between (\ref{defdeqp1})-(\ref{defdeqp2}) and (\ref{qpana1})-(\ref{qpana2}) is that the affine term in (\ref{qpana1}) and the r.h.s of the inequalities (\ref{qpana2}) depends on the state of the controlled system . For more details on MPC design, the reader can refer to \cite{Mayne:2000} \ \\ \ \\ 
Now applying Corollary \ref{corollary1} to the QP defined by  (\ref{qpana1})-(\ref{qpana2}) with a sufficiently small  for all initial conditions of interest, it comes that the QP solution (for a given ) can be obtained by integrating the following set of ODEs:
 
The idea is then to perform the integration through analogic circuits. The literature is very rich regarding the way transfer functions and more generally nonlinear differential relationships can be realized by analogic circuits (see \cite{Papazoglou:1997,Gunes:1997} and the references therein). Let us concentrate on the operations involved in (\ref{tgfr4})-(\ref{tgfr5}) to check that analogic realizations can be derived considering that  is represented by a vector of currents (voltage options is also possible \cite{Yuanmao:2012}  although it is not discussed here):
\begin{itemize}
\item {\em Constant current sources} Note first of all that the constant terms  and   corresponds to tunable source of currents.
\item {\em State dependent current sources} The state dependent terms  and  are computed numerically and the results is assigned to another vector of current sources that remain constant during the integration step. This is the only numerical operation which determines the sampling rate of the resulting MPC controller.
\item {\em Linear combination of currents}. This concerns the terms  and  and can be realized for instance using unity gain cells as shown for instance in \cite{Gunes:1997}.
\item {\em Current summation and substraction}. This concerns the realization of the sums  and   and can be viewed as a particular instantiation of the previous item and can therefore be realized using unity gain cells. 
\item {\em Squaring signals}. This is necessary to compute the summated terms in (\ref{tgfr5}) and can be achieved for instance using the circuits proposed in \cite{Filanovski:1992,Hidayat:2008} or any later work containing more recent devices and architectures.
\item {\em Multiplication by }. This operation can be realized through tunable gain or by using standard signal multipliers as the on proposed in \cite{Hidayat:2008}.   
\end{itemize} 
\ \\
Note that the time needed to analogically integrate (\ref{tgfr4})-(\ref{tgfr5}) is the time necessary to fill the corresponding circuit's capacitors. This time can be made extremely short (nano or even pico-seconds) if the problem is appropriately normalized so as to have its normalized solution components  scaled down so that they correspond to tiny capacitor voltages. \ \\ \ \\ 
Note that in the above presentation, the linear character of the controlled system plays no determinant role. Indeed, thanks to the possibility of signal multiplication, squaring and even the possibility to implement the square rooting of signals \cite{Filanovski:1992}, a wide class of ODE's that would be associated to the solution of a wide class of non quadratic constrained NLP can be analogically solved in extremely fast way. Moreover, the potential use of massively integrated circuit makes it possible to solve large scale problems in this way.\ \\ \ \\ 
Note finally that many of the above mentioned circuits can be realized using on-line assignable gains which makes it potentially possible to use the same circuits for many different problems. It remains however necessary to analyse the cost of such circuit design and realization which is beyond the scope of the present paper that studied the conceptual opportunities that are made possible by the ODE-related formulation of constrained optimization problems. 
\subsection{Example 3: Solving Nonlinear Mixed-Integer Optimization Problems}
\noindent In this section, presentation is done for the special case where all the decision variables are binary. The case where some decision variable can be continuous can be obtained easily with extra notational complexity. Consider the optimization problem given by:
 
for all . \ \\ \ \\ 
It is well known that this problem can be put in the standard form (\ref{defdeP}) by transforming the binary  constraints  into standard constraints of the form
  
which yields a number of inequality constraints . Moreover, the integer  that characterizes the growth of  [see (\ref{defdegrowthpsi}) and (\ref{defdePol}) is given by  where  is the exponent used in the definition (\ref{defdepsi}) of .\ \\ \ \\ 
Using  leads to the following definition of :
  
Now applying the result of Proposition \ref{prop1} suggests the solution of the combinatoric optimization problem can be done by integrating the following ODE:
  
Now obviously the admissible set is not a convex set and the presence of local minima is very likely. The following algorithm can be used to visit such local minima successively. In this algorithm a successively modified cost function  where  is initialized to the original cost and where  denotes the number of already visited local minima. The local minimum  is found by integrating the ODE defined by the weighted function  and its corresponding norm of the gradient , namely:
  
This is done starting from the initial condition  and integrating the ODE until some stopping conditions on both  and  are satisfied. Then a term is added to the cost function which makes the current solution  inappropriate. This can be done by first defining a neighbor vector  to  such that:
 
The new cost function  is now defined by:
 
Now for sufficiently high , this new cost function is such that  is no more a local minimum since 

therefore, incrementing  and firing the integration of the new resulting ODE (\ref{odehghg1})-(\ref{odehghg2}) starting from the initial condition  leads to a necessarily different minimum and so on. \ \\ \ \\ 
The only assumption that is implicitly assumed is that there always exists a neighbor vector  that is admissible in the sense of (\ref{yhg672}). If this is not satisfied, less close  can be searched provided that a deterministic generation process is defined.
\section{Conclusion and Future Work} \label{secconclusion} 
\noindent In this paper, it is shown that the solution of optimization problems with inequality constraints can be obtained by solving appropriately defined ODEs. In these ODEs, simultaneous dynamics are given to the decision variable as well as to the weight associated to the exact penalty term on the constraint violation. One of the major impacts of this result lies in the possibility to design analogic circuits that can quickly and physically integrate the corresponding ODEs. Pushing this latter idea towards a concrete realization is the obvious follow up of the present work. Another direction is to use the result to derive a fast gradient algorithm together with its associated certification bounds regarding the number of iterations that would be necessary to achieve a prescribed level of precision following the steps of \cite{Richter:2012} while including affine constraints that are not considered in \cite{Richter:2012}. This was not possible precisely because when standard fast gradient is used, only projection on the box-like set can be done while guaranteeing the decrease of the cost function. The formulation proposed in the present paper provide generalization of this property to an extended monotonically decreasing cost function provided that the r.h.s of the ODE is used as an extended gradient.  

\bibliographystyle{plain}
\bibliography{mybibfile}
\end{document}
