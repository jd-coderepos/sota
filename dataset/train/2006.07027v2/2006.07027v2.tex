
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage[inline]{enumitem}
\usepackage[utf8]{inputenc} \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsmath}
\usepackage{graphicx,wrapfig,lipsum}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}  
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{tikz-layers}
\usepackage{todonotes}
\usepackage{bm}
\usepackage[inline]{enumitem}
\usepackage{mathtools}

\usepackage{rotating}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{bibunits}

\tikzset{rect_sharp/.style={rectangle, draw=black, minimum width=30, minimum height=20, outer sep=2.5pt, inner sep=2.0pt},
    rect/.style={rectangle, rounded corners=2.5pt, draw=black, minimum width=30, minimum height=20, outer sep=2.5pt, inner sep=2.0pt},
	rect2/.style={rectangle, rounded corners=2.5pt, draw=black, minimum width=40, minimum height=20, outer sep=2.5pt, inner sep=2.0pt},
	every picture/.style={line width=0.75pt},
    ncbar angle/.initial=90,
    ncbar/.style={
        to path=(\tikztostart)
        -- ()
        -- ((\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget))
        -- (\tikztotarget)
    },
    ncbar/.default=0.5cm,
}

\tikzset{square left brace/.style={ncbar=0.15cm}}
\tikzset{square right brace/.style={ncbar=-0.2cm}}

\tikzset{
    seq/.style={rectangle, text centered, minimum height=0.5cm, minimum width=3.5cm, rotate=90, outer sep=2.5pt, inner sep=0pt},
    wideseq/.style={rectangle, text centered, minimum height=0.75cm, minimum width=3.5cm, rotate=90, outer sep=2.5pt, inner sep=0pt},
    seq2/.style={rectangle, text centered, minimum height=0.5cm, minimum width=3.5cm, outer sep=1.0pt, inner sep=0pt},
    wideseq2/.style={rectangle, text centered, minimum height=0.75cm, minimum width=3.5cm, outer sep=1.0pt, inner sep=0pt}
}

\tikzset{
    sqr/.style={rectangle, text centered, minimum height=1.0cm, minimum width=1.0cm, outer sep=0.25cm, inner sep=0pt},
}

\tikzset{
    circ/.style={circle, draw=black, minimum size=25, outer sep=2.5pt, inner sep=0.0pt},
    circ2/.style={circle, minimum width=0.4cm, minimum height=0.4cm, outer sep=2.5pt, inner sep=0pt}
}

\tikzset{
    cross/.style={cross out, draw=black, minimum size=12.5pt, inner sep=0pt, outer sep=0pt, rotate=45},
    cross/.default={1pt}
}

\definecolor{grey}{HTML}{F5F5F5}
\definecolor{darkgrey}{HTML}{888888}
\definecolor{orange}{HTML}{FFF2CC}
\definecolor{brown}{HTML}{D6B656}
\definecolor{lightyellow}{HTML}{fff0d2}
\definecolor{yellow}{HTML}{d0ab49}
\definecolor{lightblue}{HTML}{e6f1f8}
\definecolor{blue}{HTML}{69a7d0}
\definecolor{lightgreen}{HTML}{e9f5e9}
\definecolor{green}{HTML}{7cb959}
\definecolor{lightpurple}{HTML}{f4ebf9}
\definecolor{purple}{HTML}{b793c8}
\definecolor{darkerpurple}{HTML}{9673A6}

%
 \usetikzlibrary{calc,positioning,decorations.pathreplacing,shapes.misc,arrows.meta,3d}

\pgfrealjobname{camera_ready}


\newcommand{\cN}{\mathcal{N}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\EE}{\mathbb{E}}



\newcommand{\cX}{\mathcal{X}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cD}{\mathcal{D}}


\renewcommand\given[1][]{\:#1\vert\:}


\newcommand{\T}[1]{\mathrm{T}({#1})}
\newcommand{\Tra}[2]{\mathrm{T}^{{#1}}({#2})}
\newcommand{\TAf}[1]{\mathrm{T}({#1})} 
\newcommand{\ba}{\mathbf{a}}\newcommand{\bb}{\mathbf{b}}\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\Seq}[1]{\operatorname{Seq}(#1)}
\newcommand{\bphi}{\bm\Phi}

\newcommand{\FCN}[1]{\ensuremath{\text{FCN}_{#1}}}

\newcommand{\LStwoT}[1]{\ensuremath{\text{LS2T}^{#1}}}
\newcommand{\LStwoTwidth}[2]{\ensuremath{\text{LS2T}_{#1}^{#2}}}
\newcommand{\FCNLStwoT}[1]{\ensuremath{\text{FCN-LS2T}^{#1}}}
\newcommand{\FCNLStwoTwidth}[3]{\ensuremath{\text{FCN}_{#1}\text{-}\text{LS2T}_{#2}^{#3}}}

\DeclareMathOperator{\spn}{span}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{example}[thm]{Example}


\title{Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections}



\author{Csaba Toth \And
  Patric Bonnier \And 
  Harald Oberhauser \And \
  \Seq{\cX}=\{\bx=(\bx_i)_{i=1,\ldots,L}: \bx_i \in \cX,\, L \ge 1\} 
  \phi: \cX \to V

 \Phi: \Seq{\cX} \to \T{V},\quad \Phi(\bx)=\prod_{i=1}^L \varphi(\bx_i) \label{eq:mult_varphi}

  \T{V} :=\{\bt=(\bt_m)_{m\ge 0}\,\vert\, \bt_m \in V^{\otimes m}\}
\bs+\bt:=(\bs_m+\bt_m)_{m\geq 0}, \quad c\cdot \bt=(c \bt_m)_{m \ge 0}
 \label{eq:tensorprod}
\bs \cdot \bt := \big( \sum_{i=0}^m \bs_i\otimes \bt_{m-i} \big)_{m\geq 0} = \big( 1, \bs_1 + \bt_1, \bs_2 + \bs_1\otimes \bt_1 + \bt_2,\ldots \big) \in \T{V}
 \label{eq:oneplus}
  \varphi(\bx) = (1, \phi(\bx), 0, 0 \ldots ) \in \T{V}.

	\Phi(\bz) = \Phi(\bx) \cdot \Phi(\by). \label{eq:conc_product}
\label{eq:phi formula}
	\Phi_m(\bx) = \sum_{1 \leq i_1 < \dots < i_m \leq L} \bx_{i_1} \otimes \cdots \otimes \bx_{i_m} \in V^{\otimes m}, \hspace{5pt} \Phi(\bx) = (\Phi_m(\bx))_{m \geq 0},
\label{eq: coordinates functional}
  \langle \ell, \bt \rangle:= \sum_{m=0}^M \langle \ell_m, \bt_m \rangle = \sum_{m = 0}^M \sum_{i_1,\ldots,i_m \in \{1,\ldots,d\}} \ell^{i_1,\ldots,i_m}_m \bt_m^{i_1,\ldots,i_m}.

\Phi:	\mathrm{Seq}(\cX) \to \T{V}, \quad \bx \mapsto \Phi(\bx).
	
	\bt_m = \sum_{i=0}^r \bv_i^1 \otimes \cdots \otimes \bv_i^m, \quad \bv_i^1, \ldots, \bv_i^m \in V.
	
    \langle \ell, \Phi(\bx)\rangle &=
\sum_{m=0}^M \sum_{1 \le i_1 < \cdots < i_m \leq L} \prod_{k=1}^m \langle \bv_k^m, \phi(\bx_{i_k})\rangle \label{eq:sum}
  
	\tilde\Phi_{\tilde\theta}(\bx_1, \dots, \bx_L) := \cL\circ \Phi(\bx_1,\ldots,\bx_L) = (\langle \ell^j, \Phi(\bx_1,\ldots,\bx_L))_{j=1}^N \in \R^{N}.

	\tilde\cH = \big\{\langle \sum_{j=1}^N \alpha_j \ell^j, \Phi(\bx_1, \dots, \bx_L) \rangle \,\vert\, \alpha_j \in \R \big\} \subsetneq \cH = \big\{\langle \ell, \Phi(\bx_1, \dots, \bx_L) \,\vert\, \ell \in \T{V}\big\}	
 \label{eq:seq2seq_lls2t}
	\Seq{\cX} \to \Seq{\R^N},\quad \bx \mapsto \big(\tilde\Phi_{\tilde\theta}(\bx_1), \tilde\Phi_{\tilde\theta}(\bx_1, \bx_2), \dots, \tilde\Phi_{\tilde\theta}(\bx_1, \dots, \bx_L)\big).
\label{eq:stacked seq2seq}
  \Seq{\cX} \rightarrow \Seq{\R^{N_1}} \rightarrow \Seq{\R^{N_2}} \rightarrow \cdots \rightarrow \Seq{\R^{N_D}}. \label{eq:SeqFeatures}
  \tilde\Phi_{\tilde \theta_1,\ldots,\tilde \theta_{D}}:\Seq{\cX} \rightarrow \R^{n_D}.

    \tilde\Phi^{\operatorname{b}}_{(\tilde\theta_1, \tilde\theta_2)}(\mathbf{x}): \Seq{\R^d} \rightarrow \Seq{\R^{N + N^{\prime}}}, \quad \mathbf{x} \mapsto (\tilde\Phi_{\tilde\theta_1}(\bx_1, \ldots, \bx_i), \tilde\Phi_{\tilde\theta_2}(\bx_i, \ldots, \bx_L))_{i=1}^L.

   \T{V} := \prod_{m\geq 0} V^{\otimes m} =   \{\bt=(\bt_m)_{m\ge 0}\,\vert\, \bt \in V^{\otimes m}\}
 \bs + \bt = (\bs_m+\bt_m)_{m \ge 0} \in \T{V}\text{ and } c \cdot \bt = (c\bt_m)_{m \ge 0} \in \T{V}
 \bt_m:=\bv^{\otimes m}:= \underbrace{\bv \otimes \cdots \otimes \bv}_{m \text{ many tensor products }\otimes} \in (\R^d)^{\otimes m} \text{ and by convention we set }\bv^{\otimes 0 }:=1 \in  (\R^d)^{\otimes 0}.\label{eq:ncp}
    \bs \cdot \bt := \big( \sum_{i=0}^m \bs_i\otimes \bt_{m-i} \big)_{m\geq 0} = \big( 1, \bs_1 + \bt_1, \bs_2 + \bs_1\otimes \bt_1 + \bt_2,\ldots \big).
	 \phi: \cX \to V \Phi:\Seq{\cX} \rightarrow \T{V},\quad \bx \to \prod_{i=1}^T  \varphi(\bx_i).
		\{ x \mapsto \langle \ell, f(x) \rangle \,:\, \ell \in W' \} \subseteq C_b(\cX)
		\label{eq: phi}
		\Phi:\mathrm{Seq}(\cX) \to \T{V}, \quad (\bx_1,\ldots,\bx_L) \mapsto \prod_{i=1}^L \varphi(\bx_i)
		
  \langle \ell_1,\Phi(\bx) \rangle \langle \ell_2,\Phi(\bx) \rangle =  \langle \ell,\Phi(\bx) \rangle. 
 \label{eq:phi}
	\Phi:\Seq{V}\to \T{V}, \quad \Phi(\bx_1, \ldots, \bx_L) := \prod_{i=1}^L \varphi(\bx_i)
	
	\Phi(\bx_1, \ldots, \bx_L) = (1, \sum_{i=1}^L \underbrace{\bx_i}_{V}, \sum_{1 \leq i_1 < i_2 \leq L} \underbrace{\bx_{i_1}\otimes \bx_{i_2}}_{V^{\otimes 2}}, \sum_{1 \leq i_1 < i_2 < i_3 \leq L} \underbrace{\bx_{i_1}\otimes \bx_{i_2}\otimes \bx_{i_3}}_{V^{\otimes 3}}, \cdots) 
	 \label{eq:phim}
	\Phi_m(\bx) = \sum_{1 \le i_1 < \cdots < i_m \le L} \bx_{i_1} \otimes \cdots \otimes \bx_{i_m}.
	
	&\langle \ell, \Phi_m(\bx) \rangle 
	= \langle \bv_1 \otimes \cdots \otimes \bv_m, \sum_{1 \le i_1 < \cdots < i_m \le L} \bx_{i_1} \otimes \cdots \otimes \bx_{i_m} \rangle \\
	&= \sum_{1 \le i_1 < \cdots < i_m \le L} \langle \bv_1 \otimes \cdots \otimes \bv_m, \bx_{i_1} \otimes \cdots \otimes \bx_{i_m} \rangle
	= \sum_{1 \le i_1 < \cdots < i_m \le L} \langle \bv_1 , \bx_{i_1} \rangle \cdots \langle \bv_m, \bx_{i_m} \rangle.
	
	\Tra{2}{V} = \prod_{n_1, \ldots, n_k \geq 0} V^{\otimes n_1} \big\vert \cdots \big\vert V^{\otimes n_k}
	
		x^\star := (x^{\otimes m})_{m\geq 0} = (1, x, x^{\otimes 2}, x^{\otimes 3}, \ldots )
		
		\bx^\star := (\bx_1^\star, \ldots, \bx_L^\star) \in \Seq{\T{V}}
		
		\star : \Tra{2}{V} \times \Tra{2}{V} \to \Tra{2}{V}
		
		(\ell_1 \vert e_i)\star(\ell_2 \vert e_j) = (\ell_1 \vert e_i\star \ell_2)\vert e_j + (\ell_1\star \ell_2 \vert e_j)\vert e_i + (\ell_1\star \ell_2)\vert(e_i\otimes e_j).
		
		\langle \ell_1, \Phi(\bx) \rangle \langle \ell_2, \Phi(\bx) \rangle = \langle \ell_1\star \ell_2, \Phi(\bx^\star) \rangle.
		
		\langle e_{i_1} \vert \cdots \vert e_{i_m}, \Phi(\bx) \rangle = \sum_{1 \leq k_1 < \cdots < k_m \leq L} \langle e_{i_1}, \bx_{k_1} \rangle \cdots \langle e_{i_m}, \bx_{k_m} \rangle.
		
		\mathrm{Seq}^1(V) \to \T{V}, \quad (\bx_1, \ldots, \bx_L) \to \prod_{i=1}^L (1 + \bx_i)
		
		\langle e_i, \Phi(\bx)\rangle = x_i.
		
		&\langle \ell_1\otimes e_0 + \gamma e_0\otimes \ell_2, \Phi(\bx)-\Phi(\by)\rangle \\
		&= \langle \ell_1, \Phi(\bx_1, \ldots, \bx_L)-\Phi(\by_1, \ldots, \by_L)\rangle + \gamma\langle \ell_2, \Phi(\bx_2, \ldots, \bx_{L+1})-\Phi(\by_2, \ldots, \by_{L+1})\rangle.
		
		\langle \ell_1, \Phi(\bx) \rangle \langle \ell_2, \Phi(\bx) \rangle = \langle \ell_1\star \ell_2, \prod_{i=1}^L \phi(\bx_i)^\star \rangle
		
		\langle e_i, \phi(\bx_k)\rangle \langle e_j,\phi(\bx_k)\rangle = \langle h, \phi(\bx_k) \rangle + \varepsilon(\bx_k)
		
		&\langle e_i\otimes e_j, \prod_{i=1}^L \phi(\bx_i)^\star) \rangle = \sum_{k=1}^L \langle e_i, \phi(\bx_k)\rangle \langle e_j,\phi(\bx_k)\rangle = \sum_{k=1}^L \langle h, \phi(\bx_k)\rangle + \varepsilon(\bx_k) \\
		&= \langle e_{h}, \Phi(\bx) \rangle + n\max_{1\leq k\leq n}\varepsilon(\bx_k).
		
 \text{The string "aabc" has the 2-mers }  \{  aa, ab, ac, ab, ac, bc\}.

    \Phi(\bx) =& \varphi(a)\varphi(a)\varphi(b)\varphi(c)\\
              =& (1,e_1,0,0,\ldots,) \cdot (1,e_1,0,0,\ldots) \cdot (1,e_2,0,0,\ldots)\cdot (1,e_3,0,0,\ldots)\\
              =& (1, \underbrace{2e_1 + e_2 + e_3}_{\in V},\underbrace{ e_1\otimes e_1 + 2 e_1 \otimes e_2+ 2e_1 \otimes e_3 + e_2 \otimes e_3}_{\in V^{\otimes 2}},\\
                & \underbrace{e_1\otimes e_1 \otimes e_2+e_1\otimes e_1 \otimes e_3, e_1 \otimes e_1 \otimes e_3 \otimes e_4}_{\in V^{\otimes 3}},\underbrace{e_1 \otimes e_1 \otimes e_2 \otimes e_3}_{\in V^{\otimes 4}},0 , \ldots ).
  
  \bx^L:=(x(t_0^L), x(t_1^L) - x(t_0^L), \ldots, x(t_L^L)-x(t_{L-1}^L))
  
    \Phi_m(\bx^L) \to \int_{0 \le t_1 \le \cdots \le t_m \le 1} \frac{dx}{dt}(t_1) \otimes \cdots \otimes \frac{dx}{dt}(t_m) dt_1 \cdots d t_m  \text{ as } L \to \infty. 
  
\Phi_m(\bx^L) &= \sum_{i=1}^L \Phi_{m-1}(\bx_i^L) \otimes \big(x(t_{i+1}) - x(t_i)\big) 

\Phi_m(\bx^L) &= \sum_{i=1}^L \Phi_{m-1}(\bx_i^L) \otimes \frac{dx}{dt}(t_i) (t_{i+1} - t_i) + O(1/L),

\Phi_m(\bx^L) &= \sum_{1 \leq i_1 < \ldots < i_m \leq L} \frac{dx}{dt}(t_{i_1}) \otimes \cdots \otimes \frac{dx}{dt}(t_{i_m}) (t_{{i_1}+1} - t_{i_1})\cdots(t_{{i_m}+1} - t_{i_m}) + O(1/L),
\langle e_1, \int_{t = 0}^1 \frac{dx}{dt}(t) dt \rangle =  \langle e_1, x(1)-x(0) \rangle
  \langle e_1 \otimes e_2, \int_{0 \le t_1 \le \cdots \le t_m \le 1} \frac{x(t_1)}{dt_1} \otimes \frac{x(t_2)}{dt_2} dt_1 dt_2= \int_{0 \le t_1 \le t_2 \le 1} \langle e_1, \frac{x(t_1)}{dt_1} \rangle \langle e_2, \frac{x(t_2)}{dt_2}\rangle dt_1 dt_2.   
 
	\Seq{V} &\rightarrow \Seq{\T{V}}\\
	(\bx_1,\bx_2,\bx_3,\ldots, \bx_L) &\mapsto \Big(\Phi(\bx_1),\Phi(\bx_1, \bx_2), \Phi(\bx_1, \bx_2, \bx_3), \ldots, \Phi(\bx_1,\ldots,\bx_L) \Big).
	\label{eq:seq2seq}
	\Seq{V}=\Seq{\T{V}} \rightarrow \Seq{\T{\T{V}}} \rightarrow \cdots \rightarrow \Seq{\underbrace{T(\cdots T}_{D \text{ times }}(V)\cdots )}.
	
		\Tra{0}{V} &= V, \quad 
		\Tra{D}{V} = \prod_{m\geq 0} \big( \Tra{D-1}{V}\big)^{\otimes m}. 
		
	\Phi^D : \Seq{V} \to \Tra{D}{V}.
	
		\ell_1\prec (\ell_2\otimes_{(2)} e_i) = (\ell_1\star \ell_2)\otimes_{(2)} e_i 
		
		\Seq{V} \to \Seq{\T{V}}, \quad (\bx_1, \ldots, \bx_L) \mapsto (\Phi(\bx_1), \ldots, \Phi(\bx, \ldots, \bx_L))
		
		\langle e_{\ell_1}\otimes_{(3)} e_{\ell_2}, \Phi(\Delta\bphi(\bx)) \rangle = \langle \ell_1 \prec \ell_2, \Phi(\bx)\rangle
		
		&\langle e_{\ell_1}\otimes_{(3)} e_{\ell_2\otimes_{(2)} e_i}, \Phi(\Delta\bphi(\bx)) \rangle \\
		&= \sum_{1 \leq k_1 < k_2 \leq L} \big(\langle \ell_1, \Phi(\bx)_{k_1}-\langle \ell_1, \Phi(\bx)_{k_1-1}\big) \rangle \big(\langle \ell_2\otimes_{(2)} e_i, \Phi(\bx)_{k_2}\rangle-\langle \ell_2\otimes_{(2)} e_i, \Phi(\bx)_{k_2-1}\rangle\big) \\
		&= \sum_{k=1}^{L-1} \langle \ell_1, \Phi(\bx)_{k} \rangle \big(\langle \ell_2\otimes_{(2)} e_i, \Phi(x)_{k+1}\rangle-\langle \ell_2\otimes_{(2)} e_i, \Phi(\bx)_{k}\rangle\big) \\
		&= \sum_{k=1}^{L-1} \langle \ell_1, \Phi(\bx)_{k} \rangle \big( \sum_{1\leq l\leq k} \langle \ell_2, \Phi(\bx)_{l}\rangle \bx^i_{l+1}-\langle \ell_2, \Phi(\bx)_{l-1}\rangle \bx^i_{l}\big) \\
		&= \sum_{k=1}^{L-1} \langle \ell_1, \Phi(\bx)_{k} \rangle \langle \ell_2, \Phi(\bx)_{k}\rangle \bx^i_{k+1} 
		= \sum_{k=1}^{L-1} \langle \ell_1\star \ell_2, \Phi(\bx)_{k} \rangle \bx^i_{k+1} \\
		&= \langle (\ell_1\star \ell_2)\otimes_{(2)} e_i, \Phi(\bx)_{L} \rangle.
		
    \Phi(\bx) = (\Phi_m(\bx))_{m \geq 0}, \quad \Phi_m(\bx) = \sum_{1 \leq i_1 < \dots < i_m \leq L} \phi(\bx_{i_1}) \otimes \cdots \otimes \phi(\bx_{i_m}),

\langle \ell^k, \Phi(\bx) \rangle = \sum_{m=0} \langle \ell^k_m, \Phi_m(\bx) \rangle,

    \tilde\Phi_{m, \tilde\theta}(\bx) = (\langle \ell^1_m, \Phi_m(\bx), \dots, \langle \ell^n_m, \Phi_m(\bx) \rangle) \quad\text{and}\quad \tilde\Phi_{\tilde\theta}(\bx) = (\tilde\Phi_{m, \tilde\theta}(\bx))_{m \geq 0}, 

    \Phi_m(\by) &= \sum_{1 \leq i_1 < \cdots < i_M \leq L} \by_{i_1} \otimes \cdots \otimes \by_{i_m} 
    = \sum_{1 \leq i_1 < \cdots < i_M \leq L} (\alpha \bx_{i_1}) \otimes \cdots \otimes (\alpha \bx_{i_m}) \\
    &=  \alpha^m \sum_{1 \leq i_1 < \cdots < i_M \leq L} \bx_{i_1} \otimes \cdots \otimes \bx_{i_m},

    \tilde\Phi_{m, \tilde\theta} (\by) &= (\langle \ell^1_m, \Phi_m(\by) \rangle, \dots, \langle \ell^n_m, \Phi_m(\by) \rangle) 
    = (\langle \ell^1_m, \alpha^m \Phi_m(\bx) \rangle, \dots, \langle \ell^n_m, \alpha^m \Phi_m(\bx) \rangle) \\
    &= \alpha^m (\langle \ell^1_m, \Phi_m(\bx) \rangle, \dots, \langle \ell^n_m, \Phi_m(\bx) \rangle).

    \Delta \bx := (\bx_1, \bx_2 - \bx_1, \dots, \bx_L - \bx_{L-1}) \in \Seq{\R^d},

    \Phi_1(\bx) = \sum_{i=1}^L \Delta \bx_i = \bx_L,

    \tilde\Phi_{1,\theta}(\bx) = \sum_{i=1}^L (\langle \ell^1_1, \Delta \bx_i \rangle, \dots, \langle \ell^n_1, \Delta \bx_i \rangle) = (\langle \ell^1_1, \bx_L \rangle, \dots, \langle \ell^n_1, \bx_L \rangle),    
 \label{eq:diff_phi}
    \Phi_m(\bx) &= \sum_{1 \leq i_1 < \dots i_m \leq L} \Delta \bx_{i_1} \otimes \cdots \otimes \Delta\bx_{i_m},
 \label{eq:diff_tildephi}
    \tilde\Phi_{m, \tilde\theta} &= \sum_{1 \leq i_1 < \cdots < i_m \leq L} (\langle \bz^1_{m, 1}, \Delta \bx_{i_1} \rangle \cdots \langle \bz^1_{m, m}, \Delta \bx_{i_m} \rangle, \dots, \langle \bz^n_{m, 1}, \Delta \bx_{i_1} \rangle \cdots \langle \bz^n_{m, m}, \Delta \bx_{i_m} \rangle)
 \label{eq:recursive_Seq2Tens}
    \Phi_m(\bx_1, \dots \bx_l) = \Phi_m(\bx_1, \dots \bx_{l-1}) + \Phi_{m-1}(\bx_1, \dots, \bx_{l-1}) \otimes \bx_{l},
 \label{eq:ls2t_independent}
    \langle \ell_m, \Phi_m(\bx_1, \dots, \bx_l) \rangle =& \langle \bz_{m, 1} \otimes \cdots \otimes \bz_{m, m}, \Phi_m(\bx_1, \dots, \bx_l) \rangle \\ =& \langle \bz_{m, 1} \otimes \cdots \otimes \bz_{m, m}, \Phi_m(\bx_1, \dots, \bx_{l-1}) \rangle \\
    &+ \langle \bz_{m, 1} \otimes \cdots \otimes \bz_{m, m-1}, \Phi_{m-1}(\bx_1, \dots, \bx_{l-1}) \rangle \langle \mathbf{z}_{m,m}, \bx_{l} \rangle \\
    =& \langle \ell_m, \Phi_m(\bx_1, \dots \bx_{l-1}) \rangle \label{eqline:lm_prev} \\
    &+ \langle \bz_{m, 1} \otimes \cdots \otimes \bz_{m, m-1}, \Phi_{m-1}(\bx_1, \dots, \bx_{l-1}) \rangle \langle \mathbf{z}_{m,m}, \bx_{l} \label{eqline:not_lm_1_prev} \rangle
 \label{eq:ls2t_recursive}
    \langle \ell_m, \Phi_m(\bx_1, \dots, \bx_l) \rangle = \langle \ell_m, \Phi_m(\bx_1, \dots, \bx_{l-1} \rangle + \langle \ell_{m-1}, \Phi_{m-1}(\bx_1, \dots, \bx_{l-1})\rangle \langle \bz_m, \bx_{l} \rangle,

    \bh_{1, i} &=     \bh_{1, i-1} + \bZ_1 \bx_i, \label{eq:ls2t_recursive_vectorized_lv1} \\
    \bh_{m, i} &= \bh_{m, i-1} + \bh_{m-1, i-1} \odot \bZ_m \bx_i \quad \text{for } m \geq 2,

  		A[\dots, :, \boxplus, :, \dots][\dots, i_{j-1}, i_j,, i_{j+1} \dots] := \sum_{\kappa=1}^{i_j} A[\dots, i_{j-1}, \kappa, i_{j+1}, \dots].
  	
	  	A[\dots, :, \Sigma, :, \dots][\dots, i_{j-1}, i_{j+1}, \dots] := \sum_{\kappa=1}^{n_j} A[\dots, i_{j-1}, \kappa, i_{j+1}, \dots].
  	
	  	A[\dots, :, +m, :, \dots][\dots, i_{j-1}, i_j, i_{j+1}, \dots] := \left\lbrace\begin{array}{ll} A[\dots, i_{j-1}, i_j-m, i_{j+1}, \dots], & \text{ if } i_j > m, \\ 0, & \text{ if } i_j \leq m.  \end{array}\right.
  	(A \odot B) [i_1, \dots, i_k] := A[i_1, \dots, i_k] \cdot B[i_1, \dots, i_k]. 
    \ell^k_m = \bz_{m, 1}^k \otimes \cdots \otimes \bz_{m, m}^k \in (\R^d)^{\otimes m} \quad\text{for}\quad k = 1, \dots, n_\ell\quad\text{and}\quad m \geq 0.

    \EE[z^k_{m, j, p}] = 0 \quad\text{and}\quad \EE[z^k_{m, j, p}]^2 = \sigma_m^2 \quad\text{for}\quad j=1, \dots, m \quad\text{and}\quad p=1, \dots, d.

    \ell^k_{m, \mathbf{i}} = z^k_{m, 1, i_1} \cdots z^k_{m, m, i_m}    

    \EE[\ell^k_{m, \mathbf{i}}] = 0\quad\text{and}\quad\EE[\ell^k_{m, \mathbf{i}}]^2 = \sigma_m^{2m}

    \sigma_m^2 = \sqrt[m]{\frac{2}{d^m + n_\ell}}.    

    \ell^k_m = \bz^k_1 \otimes \cdots \otimes \bz^k_m \quad\text{for}\quad k=1,\dots,n_\ell \quad\text{and}\quad m \geq 0.

    \ell^k_{m, \mathbf{i}} = z^k_{1, i_1} \cdots z^k_{m, i_m},

    \EE[\ell^k_{m, \mathbf{i}}] = 0 \quad\text{and}\quad \EE[\ell^k_{m, \mathbf{i}}]^2 = \sigma_1^2 \cdots \sigma_m^2,

    \sigma_1^2 = \frac{2}{d + n_\ell} \quad\text{and}\quad \sigma_{m+1}^2 = \frac{d^m + n_\ell}{d^{m+1} + n_\ell} \quad\text{for}\quad {m \geq 1}. 

p_\theta(\bx_i \vert \bz_i) = \cN(\bx_i \given g_\theta(\bz_i), \sigma^2 \mathbf{I}_d),

    k(\tau, \tau^\prime) = \tilde\sigma^2 \left(1 + \frac{(\tau - \tau^\prime)^2}{l^2}\right)^{-1},

    q_\psi(\bz_1, \dots, \bz_L \given \bx_1, \dots \bx_L) &= q_\psi(z_1^1, \dots z_L^1 \given \bx_1, \dots, \bx_L) \cdots, q_\psi(z_1^{d^\prime}, \dots z_L^{d^\prime} \given \bx_1, \dots \bx_L) \\
    &= \cN(z_1^1, \dots, z_L^1 \given \mathbf{m}_1, \mathbf{A}_1) \cdots \cN(z_1^{d^\prime}, \dots, z_L^{d^\prime} \given \mathbf{m}_{d^\prime}, \mathbf{A}_{d^\prime}), 

    \frac{1}{n_\bX} \sum_{i=1}^{n_\bX} \log p(\bx_i) \geq \frac{1}{n_\bX}\sum_{i=1}^{n_\bX} \Big( \sum_{j=1}^{L_i} &\EE_{q_\psi(\bz_{i, j} \given \bx_i)}[\log p_\theta(\bx_{i, j} \given \bz_{i, j})] \\
    &- \beta \KL{q_\psi(\bz_i \given \bx_i)}{p(\bz_{i})}) \Big), 

where the log-likelihood term is only computed across observed features as was done in \citet{nazabal2018handling}. Similarly to -VAEs \citep{higgins2017beta},  is used to rebalance the KL term, now to account for the missingness rate. 

\paragraph{Baselines.} Additionally to the baseline GP-VAE, the reported baseline results are the same ones as in \citet{fortuin2019gpvae}, which are mean imputation, forward imputation, VAE \citep{kingma2013auto}, HI-VAE \citep{nazabal2018handling} and BRITS \citep{Cao2018brits}. Among these, the VAE based models are Bayesian and provide a probability measure on possible imputations, while the mean/forward imputation methods and the RNN based BRITS only provide a single imputation.

\paragraph{Datasets.}

\begin{wraptable}{r}{9.25cm}
    \vspace{-15pt}
	\caption{Specification of datasets used for imputation}
	\label{table:dataset_spec2}
	\vspace{-10pt}
\begin{center}
    \begin{small}
	\begin{sc}
    \begin{tabular}{lcccccc}
    \toprule
    Dataset  &  &  &  &  &  &  \\
    \midrule
        HMNIST &  &  &  &  &  &  \\
        Sprites & - &  &  &  &  &  \\
        Physionet &  &  &  &  &  & - \\
    \bottomrule
    \end{tabular}
	\end{sc}
    \end{small}
    \end{center}
\vspace{-15pt}
\end{wraptable}

Table \ref{table:dataset_spec2} details the datasets used, which are the same ones as considered in \citet{fortuin2019gpvae}. The columns are defined as:  denotes the number of classes if the dataset is labelled,  denotes ratio of missing data,  denotes the state space dimension of sequences,  denotes the sequence length,  denote the number of examples in the respective training and testing sets. For Sprites no labels are available, while for Physionet all examples are in the training set and no ground truth values are available. For HMNIST, the MNAR version was used, the most difficult missingness mechanism \citep{fortuin2019gpvae}.


\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
	\vspace{-5pt}
\vspace{-10pt}
		\hspace{-10pt}
		\centering
\beginpgfgraphicnamed{gpvae-ls2t-encoder-plot}
		\begin{tikzpicture}[scale=0.6, every node/.style={scale=0.6}, shorten >=1pt,draw=black!50, node distance=\layersep]
			
		\node[seq, draw=purple, fill=lightpurple] at (0.0, 0.0) (inp) {\sc Input};
\node at (-0.5, 0.0) (l) {};
\node at (0.0, 2.0) (l) {};
		
		\node[seq, draw=blue, fill=lightblue] at (1.0, 0.0) (preprocess) {\sc Preprocessor};
		\draw[->, color=black] (inp) -- (preprocess);
		\node at (1.0, 2.0) (l) {};
		
		\node[seq, draw=blue, fill=lightblue] at (2.0, 0.0) (conv) {\sc Convolution};
		\draw[->, color=black] (preprocess) -- (conv);
		\node at (2.0, 2.0) (l) {};
		
		\node[seq, draw=green, fill=lightgreen] at (3.0, 0.0) (time) {\sc Time + Diff};
		\draw[->, color=black] (conv) -- (time);
		\node at (3.0, 2.0) (l) {};
		
		\node[seq, draw=green, fill=lightgreen] at (3.7, 0.0) (ls2t_behind3) {};
		\node[seq, draw=green, fill=lightgreen] at (3.8, 0.0) (ls2t_behind3) {};
		\node[seq, draw=green, fill=lightgreen] at (3.9, 0.0) (ls2t_behind3) {};
		\node[seq, draw=green, fill=lightgreen] at (4.0, 0.0) (bls2t) {\sc B-LS2T};
		\draw[->, color=black] (time) -- ();
		\node at (4.0, 2.0) (l) {};
		
		\node[wideseq, draw=green, fill=lightgreen] at (5.125, 0.0) (wide) {\sc LN + Reshape};
		\draw[->, color=black] (bls2t) -- (wide);
		\node at (5.125, 2.0) (l) {};
		
		\node[seq, draw=blue, fill=lightblue] at (6.25, 0.0) (dense1) {\sc Dense};
		\draw[->, color=black] (wide) -- (dense1);
		\node at (6.125, 2.0) (l) {};
		
		\node[outer sep=0.5pt, inner sep=0pt] at (7.25, 0.0) (dots) {};
		\draw[->, color=black] (dense1) -- (dots);
		
		\node[seq, draw=blue, fill=lightblue] at (8.25, 0.0) (dense2) {\sc Dense};
\node at (8.25, 2.0) (l) {};
		\draw[->, color=black] (dots) -- (dense2);
		
		\node[seq, draw=yellow, fill=lightyellow] at (9.25, 0.0) (latent) {\sc Latent};
		\draw[->, color=black] (dense2) -- (latent);
		\node at (9.25, 2.0) (l) {};
		
		\end{tikzpicture}
		\endpgfgraphicnamed
\vspace{-5pt}
	\caption{Encoder in GP-VAE (B-LS2T).}
	\label{fig:gpvae_ls2t_encoder}
	\vspace{-10pt}
	\end{wrapfigure}

\paragraph{Experiment details.} As depicted in Figure \ref{fig:gpvae_ls2t_encoder}, the difference between the original GP-VAE model and ours is that is that we additionally employ a single bidirectional Seq2Tens block (B-LS2T) in the encoder network following the convolutional layer, but preceding the time-distributed dense layers. The motivation for this is that the original encoder only takes local sequential structure into account using the convolutional layer. Hence, it does not exploit global sequential information, which might limit the expressiveness of the encoder network. This limitation can lead to suboptimal inference, due to the fact that the encoder is not able to represent a rich enough subset of the variational family of distributions. This is called the amortization gap in the literature \citep{Cremer2018inference}.

We have thus hypothesized that by incorporating a bidirectional LS2T layer into the model that takes sequential structure into account not only locally, but globally, we can improve the expressiveness of the encoder network, that can in turn improve on the variational approximation. However, it should be noted that according to the findings of \citet{Cremer2018inference}, a larger encoder network can potentially result in the variational parameters being overfitted to the training data, and can degrade the generalization on unseen data examples. Therefore, the main question is whether increasing the expressiveness of the encoder will improve the quality of the variational approximation on both seen and unseen examples, or will it lead to overfitting to the seen examples?  

Another interesting question that we have not considered experimentally, but could lead to improvements is the following. The time-point-wise decoder function assumes that  is large enough, so that  is able to fully represent  in a time-point-wise manner including its dynamics. Although in theory the GP prior should be able to learn the temporal dynamics in the latent space, this might again only be possible for a large enough latent state size . In practice, it could turn out to be more efficient to use some of the contextual information in the decoder network as well, either locally, using e.g. a CNN, or globally, using e.g. LS2T layers or RNNs/LSTMs.

\paragraph{Implementation.}
For the implementation of the GP-VAE, we used the same one as in \citet{fortuin2019gpvae}, which implements it using Keras and Tensorflow. The bidirectional LS2T layer used our own implementation based on the same frameworks. The hyperparameters of the models, which are depicted in Appendix A in \citet{fortuin2019gpvae}, were left unchanged. The only change we concocted is the B-LS2T layer in the encoder network as depicted in Figure \ref{fig:gpvae_ls2t_encoder}. The width of the B-LS2T layer was set to be the same as the convolutional layer, and  tensor levels were used. The parametrization of the low-rank LS2T layer used the independent formulation as detailed in Appendix \ref{app:recursive}.

We also made a simple change to how the data is fed into the encoder. In the original model, the missing values were imputed with , while we instead used the forward imputed values. This was necessary due to the difference operation preceding the B-LS2T layer in Figure \ref{fig:gpvae_ls2t_encoder}. With the zero imputation, the coordinates with missing values exhibited higher oscillations after differencing, while with forward imputation the missing values were more well-behaved. A simple way to see this is that, if there were no preprocessing and convolutional layers in Figure \ref{fig:gpvae_ls2t_encoder} preceding the difference block, then this step would be equivalent to imputing the missing values with zero after differencing.

\paragraph{Result details.} Table \ref{table:gp_vae_comparison} shows the achieved performance on the datasets with our upgraded model, GP-VAE (B-LS2T), compared against the original GP-VAE \citep{fortuin2019gpvae} and the baselines. The reported results are negative log-likelihood (NLL), mean squared error (MSE) and AUROC on HMNIST, while on Sprites the MSE is reported and on Physionet the AUROC score. As Sprites is unlabeled, downstream classification performance (AUROC) is undefined on this dataset, while Physionet does not have ground truth values for the missing entries, and reconstruction error (MSE,NLL) is not defined. The only missing entry is Sprites NLL, which was omitted to preserve space. We observe that increasing the expressiveness of the encoder did manage to improve the results on HMNIST and Physionet. The only case where no improvement was observable is Sprites, where the GP-VAE already achieved a very low MSE score of .

To gain some intuition whether the lack of improvement on Sprites was due to the GP-VAE's performance already being maxed out, or there was some other pathology in the model, we further investigated the Sprites dataset and found a bottleneck in both the original and enhanced GP-VAE models. Due to the high dimensionality of the state space of input sequences, , the width of the first convolutional layer in the encoder network was set to  in order to keep the number of parameters in the layer manageable and be able to train the model with a batch size of , while all subsequent layers had a width of . Thus, to see if this was indeed an information bottleneck, we increased the convolution width to  and decreased the batch size to , with all other hyperparameters unchanged. Then, we trained using this modification both the baseline GP-VAE and our GP-VAE (B-LS2T) five times on the Sprites datasets. The achieved MSE scores were \begin{enumerate*}[label=(\roman*)] \item GP-VAE (base): , \item GP-VAE (B-LS2T): \end{enumerate*}. Therefore, the smaller convolutional layer was indeed causing an information bottleneck, and by increasing its width to be on par with the other layers in the encoder, we managed to improve the performance of both models. The improvement on the GP-VAE (B-LS2T) was larger, which can be explained by the observation that lifting the information bottleneck additionally allowed the benefits of the B-LS2T layer to kick in, as detailed previously.

To sum up, we have empirically validated the hypothesis that capturing global information in the encoder was indeed beneficial, and managed to improve on the results even on unseen examples in all cases. The experiment as a whole supports that our introduced LS2T layers can serve as useful building blocks in a wide range of models, not only discriminative, but also generative ones.

\begin{figure}[t]
	\centering
\includegraphics[width=5in]{imgs/sprites_reconstruction.pdf}
\caption{Reconstructions from the Sprites dataset with the images with missingness (top), reconstructed (middle) and original (bottom).}
\end{figure}

\newpage

\subsubsection*{Acknowledgements}
CT is supported by the ``Mathematical Institute Award'' from the Mathematical Institute at the University of Oxford. PB is supported by the Engineering and Physical Sciences Research Council [EP/R513295/1].
HO is supported by the EPSRC grant ``Datasig'' [EP/S026347/1], the Alan Turing Institute, and the Oxford-Man Institute.
	




















































		


















		




















		
































































































        






































        
























	






		






































		


















	
\end{document}
