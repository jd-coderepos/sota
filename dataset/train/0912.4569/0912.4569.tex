\documentclass[proceedings]{stacs}
\stacsheading{2010}{179-190}{Nancy, France}
\firstpageno{179}



\usepackage{cite,amsthm,amsmath,amssymb,latexsym}






\theoremstyle{definition}\newtheorem{fact}{Fact}

\newcommand{\mnote}[1]{\marginpar{\scriptsize\it #1}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ri}{\mbox{\it Recent-items}}
\newcommand{\ei}{\mbox{\it Expire}}
\newcommand{\eif}{\mbox{\it F-Expire}}

\newcommand{\epb}{\varepsilon_\textrm{\mbox{\rm b}}}
\newcommand{\epf}{\varepsilon_\textrm{\mbox{\rm f}}}


\newcommand{\ns}{c}
\newcommand{\hns}{\hat c}
\newcommand{\kns}{\tilde c}

\newcommand{\cs}{c}
\newcommand{\hcs}{\hat c}
\newcommand{\kcs}{\tilde c}

\newcommand{\rs}{r_\sigma}
\newcommand{\cjs}{c_{j}}
\newcommand{\cji}[1]{c_{j_{#1}}}
\newcommand{\hcjs}{\hat c_{j}}
\newcommand{\rjs}{r_{j,\sigma}}
\newcommand{\ljs}{\ell_{j}}

\newcommand{\csm}{c_\sigma}
\newcommand{\hcsm}{\hat c_\sigma}
\newcommand{\nsm}{c_\sigma}
\newcommand{\hnsm}{\hat c_\sigma}
\newcommand{\cjsm}{c_{j,\sigma}}
\newcommand{\hcjsm}{\hat c_{j,\sigma}}

\newcommand{\off}{\mbox{\it off}_{j}}
\newcommand{\true}{\mbox{\it true}}
\newcommand{\false}{\mbox{\it false}}
\newcommand{\comment}[1]{}

\begin{document}

\title[Continuous Monitoring of Distributed Data Streams over Sliding Window]
{Continuous Monitoring of Distributed Data Streams over a Time-based Sliding Window}

\author[lab1]{H.L. Chan}{Ho-Leung Chan}
\address[lab1]{Department of Computer Science, University of Hong Kong, Hong Kong}  \email{{hlchan, twlam, hfting}@cs.hku.hk}
\author[lab1]{T.W. Lam}{Tak-Wah Lam}
\author[lab2]{L.K. Lee}{Lap-Kei Lee}
\address[lab2]{Max-Planck-Institut f\"ur Informatik, 66123 Saarbr\"ucken, Germany}	
\email{lklee@mpi-inf.mpg.de}
\author[lab1]{H.F. Ting}{Hing-Fung Ting}
\thanks{T.W. Lam is partially supported by the GRF Grant HKU-713909E;
H.F. Ting is partially supported by the GRF Grant HKU-716307E}

\keywords{Algorithms, distributed data streams, communication efficiency, frequent items}
\subjclass{F.2.2 [Analysis of algorithms and problem complexity]: Nonnumerical algorithms and problems}




\begin{abstract}
\noindent
The past decade has witnessed many interesting algorithms for maintaining
statistics over a data stream.
This paper initiates a theoretical study of algorithms for
monitoring distributed data streams over a time-based sliding window
(which contains a variable number of items
and possibly out-of-order items).  The concern is how to
minimize the communication between individual streams and the root,
while allowing the root, at any time, to be able to report the
global statistics of all streams within a given error bound.  This
paper presents communication-efficient algorithms for three
classical statistics, namely, basic counting,
frequent items and quantiles.
The worst-case communication cost over a window is

bits for basic counting and
 words for the remainings, where
 is the number of distributed data streams,  is
the  {total}
 number of items in the streams that arrive or expire in the
window, and
 is the desired error bound. Matching and
nearly matching lower bounds are also obtained.
\end{abstract}

\maketitle

\section{Introduction}
The problems studied in this paper are best illustrated by the following
puzzle. John and Mary work in different laboratories and communicate by
telephone only.  In a forever-running experiment, John records which
devices have an exceptional signal in every 10 seconds.
To adjust her devices, Mary at any time needs to keep track of the number of
exceptional signals generated by each device of John in the last one hour.
John can call Mary every 10 seconds to report the exceptional signals,
yet this requires too many calls in an hour and the total
message size per hour is linear to
the total number  of exceptional signals in an hour.
Mary's devices actually allow some small error.  Can the number of
calls and message size be
reduced to , or even poly- if a small error (say, 0.1\%)
is allowed?
It is important to note that the input is given online
and Mary needs to know the answers continuously;
this makes our problem different from those in other similar classical
models, such as the
Simultaneous Communication
Complexity model \cite{BabaiGKL04}, in which all inputs
are given in advance and the parties need to
compute an answer only once.

\vspace{0.5ex}
{\bf Motivation.}
The above problem appears in data stream applications, e.g.,
network monitoring or stock analysis.
In the last decade, algorithms for continuous monitoring of a
single  massive data stream gained
a lot of attention (see \cite{Aggarwal06,Muthukrishnan05} for a survey), and
the main challenge has been how to represent the massive data
using limited space, while allowing
certain statistics (e.g., item counts, quantiles)
to be computed with sufficient accuracy.

The space-accuracy tradeoff for representing a single stream
has gradually been
understood over the years (e.g., \cite{AlonMS02,Indyk00,GuhaKS01,DemaineLM02}).
Recently, motivated by large scale networks,
the database community is enthusiastic about
communication-efficient algorithms for
continuous monitoring of
multiple, distributed data streams.
In such applications, we have  remote sites
each monitoring a data stream, and there is a
root (or coordinator) responsible for computing some global
statistics.  A remote site needs to
maintain certain statistics itself, and has to
communicate with the root often enough so that
the root can compute, at any time,
the statistics of the union of all
data streams within a certain error.
The objective is to minimize the communication.
The communication aspects of data streams introduce several challenging
theoretical questions such as
what is the optimal communication-accuracy tradeoff for maintaining a
particular statistic, and whether two-way communication is inherently
more efficient than one-way communication.

\vspace{0.5ex}
{\bf Data stream models and {\boldmath }-approximate queries.}
The data stream at
each remote site is a sequence of items from a totally ordered set .
Each item is associated with
an integral time-stamp recording its  {arrival} time.
Each remote site has limited space and hence it
can only maintain the required statistics approximately.
The statistics can be
based on the whole data stream
\cite{AlonMS02,Indyk00,GuhaKS01,DemaineLM02} or
only the recent items
\cite{DatarM02,ArasuM04,LeeT06b}.
Recent items can be modeled by two types of sliding windows
\cite{BabcockDM02,DatarGIM02}.
Let  be the window size, which is a positive integer.
The \emph{count-based sliding window}
includes the last  items in the data stream, while the
\emph{time-based sliding window}
includes items whose time-stamps are within the last  time units.
The latter assumes that zero or more items can arrive at a time.
Items in a sliding window will expire and are more
difficult to handle than in the whole data stream.
For example, counting the frequency of a
certain item in the whole stream
can be done easily by maintaining a single counter, yet the same
problem requires space
 bits
for a count-based sliding window
even if we allow a relative error of at most 
\cite{DatarGIM02, GibbonsT02}.  In fact, the whole data stream model
can be viewed as a special case of the sliding window model with
window size being infinite.  Also,
a count-based window is a special case of a time-based
  window in which exactly one item arrives at a time.
This paper focuses on time-based window, and
the algorithms are applicable to the other two models.

We study algorithms that enable the root to answer
three types of classical
-approximate queries, defined as follows.
Let . For any stream ,
let 
and  be the count of
item  and all items
  {whose timestamps are} in the current window,
respectively.
Denote  and 
as the total count of item  and all items in all the data streams,
respectively.

\begin{itemize}
\item{\it Basic Counting.}
Return an estimate  on the total count 
such that .  (Note that this query can be generalized to count
data items of a fixed subset ;
the literature often refers to the special case with
 and .)
\item{\it Frequent Items.}
Given any ,
return
a set  which includes all items  with 
and possibly some items  with .


\item{\it Quantiles}.
Given any , return an
item whose rank is in 
among the  items in the current sliding window.
\end{itemize}
  {As in most previous works,
we need to answer the
  following type of -approximate
queries in order to answer queries on frequent items.}
\begin{itemize}
\item{\it Approximate Counting.}
Given any item ,
return an estimate  such that .
(Note that this query gives estimate for any item, not just the
frequent items.   {Also, the error bound is in term
of , which may be much larger than .})
\end{itemize}


We need an algorithm to determine when and how
the remote sites communicate with the root so that the
root can answer the queries at any time.
The objective is to minimize the worst-case communication cost
within a window of  time units.



\vspace{.5ex}
{\bf Previous works.}
Recently, the database literature has a flurry of results on continuous
monitoring of distributed data streams, e.g.
\cite{OlstonJW03,GreenwaldK04,DasGGR04,SharfmanSK06,
ManjhiSDO05,CormodeGMR05,BabcockO03,JainYDZ05,MouratidisBP06,CormodeG05}.
The algorithms studied can be classified into two
types: {\em one-way} algorithms only allow messages
sent from each remote site to the root, and
{\em two-way} algorithms allow bi-directional
communication between the root and each site.
One-way algorithms are often very simple
as a remote site has little
information and all it can do is to update the root
when its local statistics deviate significantly from those previously sent.
On the other hand,
most two-way algorithms are complicated and
often involve non-trivial heuristics.
It is commonly believed in the database community
that two-way algorithms
are more efficient; however, for most
existing two-way algorithms, their worst-case communication costs
are still waiting for rigorous mathematical analysis, and
existing works often rely on experimental results when evaluating
the communication cost.



The literature contains several results on the
mathematical analysis of the worst-case
performance of one-way algorithms.
They are all for the whole data stream setting.
Keralapura et al.\ \cite{KeralapuraCR06}
studied the thresholded-count problem, which leads to
an algorithm for basic counting with communication cost
 words, where  and
 are the number of streams and the
number of items in these streams, respectively.
Cormode~et~al.\ \cite{CormodeGMR05} gave an algorithm for
quantiles with communication cost  words per stream.
They also showed how to handle frequent items via
a reduction to quantiles,
so the communication cost remains the same.
More recently,  Yi and Zhang \cite{YiZ08} have reduced
the communication cost for frequent items
to  words,
and quantile to  words, using some two-way algorithms; these are the
only analyses for two-way algorithms so far.

There have been attempts to devise heuristics
to extend some whole-data-stream algorithms to
sliding windows, yet not much has been known about
their worst-case performance.
For example, Cormode~et al.\ \cite{CormodeGMR05} have
extended their algorithms for quantiles and frequent items
to sliding windows.
They believed that the communication cost would only have
a mild increase,
but no supporting analysis has been given.  The
analysis of sliding-window algorithms is more difficult because
the expiry of items destroys some monotonic
property that is important to the analysis for
whole data stream.
In fact,
finding sliding-window algorithms with efficient worst-case
communication has been posed as an open problem
in the latest work of Yi and Zhang \cite{YiZ08}.

\vspace{0.5ex}
{\bf Our results.}
This paper gives the first mathematical analysis of
the communication cost in the sliding window model.
We derive
lower bounds on the worst-case communication cost
of any two-way
algorithm (and hence any one-way algorithm) for answering the four types
of -approximate queries.
These lower bounds hold even when each remote site
has unlimited space to maintain the local statistics exactly.
More interestingly, we analyze
some common-sense algorithms that use one-way communication only
and prove that their communication costs
match or nearly match the corresponding lower bounds.
In our algorithms, each remote site only needs to
maintain some -approximate statistics for its local data,
which actually adds more complication
to the problem.
These results demonstrate optimal or near optimal
communication-accuracy tradeoffs for supporting these queries
over the sliding window.
Our work reveals that
two-way algorithms could not
be much better than one-way algorithms in the worst case.


\renewcommand{\baselinestretch}{0.9}
\begin{table}\small
\center{
\begin{tabular}{| c || c | c | c |}
\hline
& {Basic Counting} \rule[-2.5mm]{0mm}{7mm}  & Approximate Counting/ & Quantiles  \\
& \raisebox{1ex}[0mm][0mm]{({\small\bf bits}) }  &
  \raisebox{1ex}[0mm][0mm]{Frequent items (\small\bf words)} &
  \raisebox{1ex}[0mm][0mm]{({\small\bf words})} \\
\hline \hline
\raisebox{-1ex}[0mm][0mm]{Whole data} &
 \raisebox{-0.2ex}[0mm][0mm]{ words  \cite{KeralapuraCR06}}
 &  \cite{YiZ08}
 & \rule[-2.5mm]{0mm}{8mm}
\raisebox{0.4ex}[0mm][0mm]{ \cite{YiZ08}}

\\ \cline{3-4}
\raisebox{1ex}[0mm][0mm]{stream}&
  \raisebox{0.5ex}[0cm][0cm]{
  bits
  }
&  \multicolumn{2}{c|}{\rule[-2.5mm]{0mm}{7mm} \cite{YiZ08,YiZ09}} \\ \hline
\raisebox{-1.9ex}[0mm][0mm]{Sliding window}  &
\rule[-2.5mm]{0mm}{7mm}
\raisebox{-1.9ex}[0cm][0cm]{}
&
 &
 \\ \cline{3-4}
\raisebox{0ex}[0mm][0mm]{}  & & \multicolumn{2}{c|}{
\rule[-2.5mm]{0mm}{7mm}
}
\\ \hline
\raisebox{-2ex}[0mm][0mm]{Sliding window} \rule[-4mm]{0mm}{9mm} &

&
 &
 \\
\cline{2-4}
\raisebox{1ex}[0mm][0mm]{\& out-of-order } &

&
\multicolumn{2}{c|}{\rule[-3.3mm]{0mm}{8.5mm}}
\\ \hline
\end{tabular}
\caption{Bounds on the communication costs.
Note that the bounds are stated in bits for basic counting,
and in words for the other problems.}
\label{table:summary}
}
\end{table}
\renewcommand{\baselinestretch}{1}

Below we state the lower and upper bounds precisely.
  {Recall that there are  remote sites and
the sliding window contains  time units.
We prove that}
 {within any window,}
  {the root and the remote sites need to communicate,
in the worst case,

bits  for basic counting
and 
words for the other three queries, where  is the total number of items
arriving or expiring within that
window.}\footnote{  {Note
that the number of items arriving or expiring within window
 is no greater than the number of items arriving within
.}}
For upper bounds, our analysis shows that
basic counting requires  bits within any window,
and approximate counting  words.
The estimates given by approximate counting are
sufficient to find frequent items, hence the latter problem
has the same communication cost.
For quantiles, it takes  words.
See the second row (sliding window)
of Table~\ref{table:summary} for a summary.


As mentioned before, sliding-window algorithms can be
applied to handle the special case of whole data streams
  {in which the window size  is infinite and 
  is the total number of arrived items.}
The first row of Table~\ref{table:summary} shows
the results on whole data streams.
Our work has improved the communication cost for
basic counting from 
words~\cite{KeralapuraCR06}
to  bits.
For approximate counting and frequent items, our work implies
a one-way algorithm with communication cost of

words; this matches the performance of
the two-way algorithm by Yi and Zhang~\cite{YiZ08}.
In their algorithm, the root
regularly updates every remote site about the global count of all items.
In contrast,
we use the idea that items with small count could
be ``turned off'' for further updating.
As a remark, our upper bound on quantiles is
 words
which is weaker than that of~\cite{YiZ08}.



Our algorithms can be readily applied
to out-of-order streams \cite{BuschT07,CormodeKT08}.
 {In an out-of-order stream, each item is associated
  with an integral time-stamp recording its creation time, which may
  be different from its arrival time.
We say that the stream has \emph{tardiness}  if
any item with time-stamp  must arrive within  time
units from , i.e., at any time in .
Without loss of generality, we assume that  (if an item time-stamped at  arrives after ,
it has already expired and can be ignored).
Note that for any data stream with tardiness greater than zero, the
items may not be arriving in non-decreasing order of their time-stamps.
Our previous discussion of
data streams assumes tardiness equal to , and
such data streams are called \emph{in-order} data streams.}
The previous lower bounds for in-order streams are all valid
in the out-of-order setting. In addition,
we obtain lower bounds related to~,
namely,  bits for basic counting and
 words for the other three problems.
Regarding upper bounds,
our algorithms when applied to
out-of-order streams with tardiness  will just
increase the communication
cost by a factor of .
The results are summarized in the last row of Table~\ref{table:summary}.






\vspace{0.5ex}
The idea for basic counting is relatively simple.
As the root does not require an exact total count, each data
stream can communicate to the root only when its local count
increases or decreases by a certain ratio ; we call such a
communication step an \emph{up} or \emph{down} event, respectively.
To answer the total count of all streams, the root simply sums up
all the individual counts it has received. It is easy to prove that
this answer is within some desired error bound.
If each count is over the whole stream (i.e., window size = 
and   { is the total number of arrived items}),
the count is increasing and there is no down event.
A stream would have at most  up events
and the communication cost is at most that many words.
However, the analysis becomes non-trivial in a sliding time window.
Now items can expire and down events can occur.
An up event may be followed by some
down events and the count is no longer increasing.
The tricky part is to find a new measure of progress.
We identify a ``characteristic set''
of each up event such that
each up event must increase the size of
this set by a factor of at least ,
hence bounding the number of up events to be .
Down events are bounded using another characteristic set.
Due to space limitation, the details can only be given in the full paper.










Approximate counting of all possible items is much more complicated,
which will be covered in details in the rest of this paper.
Assuming in-order streams, we derive and analyze two algorithms
 for approximate counting in Section~\ref{sec:fi}.
In Section~\ref{sec:extension}, we discuss frequent items, quantiles, and
finally out-of-order streams.
The lower bound results are relatively simple and omitted due to space limitation.












\section{Approximate Counting of all items}
\label{sec:fi}
This section presents algorithms for the streams to communicate to the root
so that the root at any time can approximate
the count of each item.
As a warm-up, we first consider the simple algorithm
in which a stream will inform the root
whenever its count of an item increases or decreases
by a certain fraction of its total item count.
  {We show
in Section~\ref{sect:simpleAlg} that within any window of  time units,
each data stream  () needs to send at most
 words to the
root, where  is the number of distinct items and 
is the number of items of  that arrive or expire within the
window.
Then, the total communication cost within this window is , which, by
Jensen's inequality, is no greater than 
where .}
We then modify the algorithm so that
a stream can ``turn off'' items whose counts are too small,
and we give a more complicated analysis
to deal with the case when many such items increase their counts rapidly
(Section~\ref{sect:fullAlg}).
The communication cost is reduced to  words,  {independent of .}



\subsection{A simple algorithm}
\label{sect:simpleAlg}
Consider any stream .  At any time ,
let  and  be the number of all items and item 
arriving at  in , respectively.
Let  be a positive constant
(which will be set to ).
We maintain two -approximate data structures \cite{DatarGIM02,LeeT06}
at  locally,
which can report estimates  and  for
 and , respectively,
such that
\footnote{
The constant 6 in the inequality is arbitrary.
It can be replaced with any number provided that
other constants in the algorithm and analysis
(e.g., the constant 9 in definition of up events)
are adjusted accordingly.
}



\vspace{.5ex}
\begin{center}
\begin{minipage}{.9\textwidth}
\hrulefill

{\bf Simple algorithm.}
At any time , for any item ,
let  be the last time  is sent
to the root. The stream sends the estimate 
to the root if the following event occurs.
\begin{itemize}
\item
{\it Up:}\hspace*{3.1ex}  .

\item
{\it Down:}  .
\end{itemize}
\vspace*{-.1in}
\hrulefill
\end{minipage}
\end{center}


\vspace{1ex}
\noindent{\bf Root's perspective.}
At any time ,
let  be the last estimate received from a stream
 for item  (at or before ).  The root can estimate
the total count of item  over all streams by
summing all  received.
More precisely, for any ,
we set  and let each stream
use the simple algorithm.
Then for each stream ,
the approximate data structures for  and
 together with the simple algorithm guarantee that

Summing  over all streams
would give the root an estimate of the total count of item 
within an error of  of the total count of all items.

\vspace{.5ex}
\noindent{\bf Communication Complexity.}
At any time , we denote the reference window as
, where .
 {Let  be the number of items of  that
arrive or expire in .}
Assume that there are at most  distinct items.
We first show that a stream  encounters
 up events
and sends  words
within .  The analysis of down events is similar and will
be detailed later. For any time ,
it is useful to define 
(resp.\ )
as the multi-set of all items (resp.\ item  only)
arriving at  within , and 
as the size of this multi-set.


Consider an up event  of some item 
that occurs at time .
Define the {\em previous event} of  to be
the latest event (up or down) of item 
that occurs at time .
We call  the \emph{previous-event time} of .
The number of up events
with previous-event time before 
is at most~.
To upper bound the number of
up events with previous-event time 
is, however, non-trivial;
below we call such an up event a \emph{follow-up} (event).
Intuitively, a follow-up can be triggered by
frequent arrivals of an item, or mainly
the relative decrease of the total count.
This motivates us to classify follow-ups
into two types and analyze them differently.
A follow-up  is said to be {\em absolute} if
, and {\em relative} otherwise.
Define  to be
the multi-set of item 's that arrive after the
previous event of , i.e., .

\vspace{.5ex}
{\bf Absolute follow-ups.}
To obtain a tight bound of absolute follow-ups, we need
a characteristic-set argument that can
consider the growth of different items together.
Let  be the times in 
when some absolute follow-ups (of one or more items) occur.
Let  be the number of items having an absolute follow-up at .
Note that for all , ,\footnote{
  If an up event of an item  occurs at time ,
  then .
  Thus the number of up events at time  is at most
  .
}  and  is the number of absolute follow-ups in .
We define the characteristic set  at each  as follows:







 {Recall that  is the number of items of 
  that arrive or expire in .}

\begin{lemma} \label{lem:fi-step-up-s-simple}
{\bf (i)} For any , .
{\bf (ii)} There are 
absolute follow-ups within .
\end{lemma}

\begin{proof}
For (i), consider an absolute follow-up  of an item ,
occurring at time  with previous-event time .
Note that the increase in the count of item  from  to  must be due to the recent
items. We have

There are  absolute follow-ups at ,
so .
Since ,
.
Therefore, we have .


For (ii), we note that ,
and .
Thus, ,
or equivalently, .
The latter is at least
.
The last inequality follows from that  for all .
Thus, .
\end{proof}

{\bf Relative follow-ups.}
A relative follow-up occurs only when a lot of items expire,
and relative follow-ups of the same item cannot occur
too frequently. Below we define  time intervals
and argue that no item can have two relative follow-ups within
an interval.
For an item with time-stamp ,
we define the \emph{first expiry time} to be .
At any time  in , define 
to be the set of all items whose
first expiry time is within ,
i.e., .
 is non-increasing as  increases.
Consider the times 
such that for ,  is the first time
such that .
For convenience, let .
Note that  and .


\begin{lemma}\label{lem:fi-type2-up}
{\bf (i)} Every item  has at most one relative follow-up 
within each interval .
{\bf (ii)} There are at most 
relative follow-ups within .
\end{lemma}
\begin{proof}
For (i), assume  occurs at time  in , and its previous event
occurs at time .
By definition, . Thus,

and .
Since   and ,
we have  and .
For (ii), there are  distinct items, so there
are at most  relative follow-ups
within each interval ,
and  at most  relative follow-ups within .
\end{proof}

{\bf Down events.} The analysis is symmetric to that
of up events. The only non-trivial thing is the definition of the
characteristic set for
bounding the absolute follow-downs , which
is defined in an opposite sense:
Assume  occurs at time  and its  previous event occurs at .
 is said to be {\em absolute}
if .
Let  be
the multi-set of item 's whose first expiry time is within
. I.e., .


It is perhaps a bit tricky that
instead of defining the characteristic set of absolute follow-downs
at the time they occur,
we consider the times of the corresponding
\emph{previous events} of these follow-downs.
Let  be the times in  such that
there is at least one event  (up or down) at  which is the
previous event of an absolute follow-down  occurring after .
Let  be the number of such previous events at , and
let  be the set of corresponding absolute follow-downs.
Note that  (unlike ) only admits
a trivial upper bound of .
We define
the characteristic set  for each  as follows:

Similar to Lemma~\ref{lem:fi-step-up-s-simple}, we can show
that . Owing to a weaker bound of individual , the number
of absolute follow-downs, which equals
, is shown to be
.

\vspace{.5ex}
Combining the analyses on up and down events,
and let ,  we have the following.
\begin{theorem}\label{thm:simple}
  {
The simple algorithm
sends at most  words
to the root during window .}
\end{theorem}

\vspace*{-1ex}
\subsection{The full algorithm}
\label{sect:fullAlg}
\vspace*{-1ex}
In this section, we extend the previous algorithm and
give a new characteristic-set analysis that is based on
future events (instead of the past events) to show that
  {each stream's communication cost per window can be
  reduced to
 words.  Then,
by Jensen's inequality again, we conclude that the total
communication cost
per window is .}
Intuitively, when the estimate  of an item  is too small,
say, less than ,
the algorithm treats this estimate as 0 and
set the   flag of  to be true.
This restricts the number of items with a positive estimate
to .
Initially, the  flag is true for all items .
Given    {},
 the stream communicates with the root as follows.

\vspace{.5ex}
\begin{center}
\begin{minipage}{.9\textwidth}
\hrulefill

{\bf Algorithm AC.}
At any time , for any item ,  {let
  {} be the time the
last estimate of , i.e., , is
sent to the root.}
The stream sends the estimate of  to the root
if the following event occurs.
 \begin{itemize}
 \item
 {\it Up:} \hspace{3ex}If ,
 send  and set ~.
 \item
 {\it Off:} \hspace{2.7ex}If
   and ,
{reset  to 0,
  send }\\
  \mbox{}\hspace{.5in}{and set .}
 \item
 {\it Down:} If  and ,
 send .
 \end{itemize}
\vspace*{-.1in}
\hrulefill
 \end{minipage}
\end{center}
\vspace*{.05in}
\vspace{.5ex}










It is straightforward to check that the root can
answer the approximate counting query for any item.
We analyze the communication complexity of different events as follows.

\begin{fact} \label{lem:rj}
At any time , the number of items  with  is at most
.\footnote{For any item , if ,
  then  and
  .  Thus
  the number of items  with  is at most
  .
} \end{fact}

\noindent{\bf Off events.}
  {
Recall that we are considering the window , and
 is the number of items arriving or expiring within .}
By Fact~\ref{lem:rj},
just before ,
there are at most  items with .
Within , only an up event can set the {\em off}\/ flag
to false.
Thus the number of off events within  
is bounded by  plus the number of up events.



\vspace{.5ex}
\noindent{\bf Up and Down events.}
The assumption of  gives a trivial bound on
 those events involving  items with very small counts and in
particular, those up events immediately following the off events.
Such up events are called {\em poor-up} events or simply {\em poor-ups}.
Using the {\em off}\/ flag, we can easily adapt
the analysis of the simple algorithm to bound
all the down and up events of the full algorithm,
but except the poor-ups. The following simple
observations, derived from Fact 1, allow us to replace
 with  in the previous analysis
to obtain a tighter upper bound of
.  Let  be any time in .

\vspace{.5ex}
\begin{itemize}
\item
There are at most  items
whose first event after  is a down event.
\vspace{.5ex}
\item
There are at most  non-poor-up
events after  whose previous event is before .
\end{itemize}
\vspace{.5ex}

It remains to analyze the poor-ups.
Consider a poor-up  at time  in .
By definition,  at time .
The trick of analyzing 's
is to consider when the corresponding
items will be ``off'' again instead
of what items constitute the up events.
Then a characteristic
set argument can be formulated easily.
Specifically, we first observe that,
by Fact~1, there are at most  poor-ups
whose {\em off}\/ flags remain false up to time .
Then it remains to consider those 
whose {\em off}\/ flags will be set to true at some time .
Below we refer to  as the \emph{first off time} of .



\vspace{.5ex}
\noindent{\bf Poor-up with early off.}
Consider a poor-up  that occurs at time  in 
and has its first off time at  in .
Let  be all the item  whose first expiry time is
within .  I.e.,  .
As an early off can be due to the expiry of many copies of item 
or the arrival of a lot of items, it is natural to divide
the poor-ups into two types:  with an \emph{absolute} off if
, and \emph{relative} off otherwise.
For the case with absolute off, we consider
the distinct times  in ]
when such poor-ups occur.
Let  be the number of such poor-ups at time .
Note that .
For each time ,
we define the characteristic set


\begin{lemma} \label{lem:fi-poor-up-s}
{\bf (i)}
For any , .
{\bf (ii)}
Within , there are 
poor-ups each with an absolute off.
\end{lemma}

\begin{proof}
For (i),
consider an item  and a poor-up  with an absolute off that occurs at time 
and has its first off at time .
The decrease in  must be due to expiry of item~.

Thus,
.
Since ,
. Therefore,
.
By (i), we can prove (ii) similarly to Lemma~\ref{lem:fi-step-up-s-simple} (ii).
\end{proof}

Analyzing poor-ups with a relative off is again based
on an isolating argument.
We divide  into  intervals
according to how fast the total item count starting from  grow;
specifically, we want two consecutive time boundaries  and 
to satisfy .
Then we  show that for any poor-up  within ,
its relative off, if exists, occurs
at or after .  Thus
there are at most  such poor-ups within each interval
and a total of  within .

\begin{lemma}\label{lem:fi-type2}
{\bf (i)} Consider a poor-up  with a relative off.  Suppose it occurs at time 
in , and its first off time is at  in .
Then .
{\bf (ii)} Within , there are at most  poor-ups each with a relative off.
\end{lemma}

\begin{proof}
For (i), by the definition of a relative off,
.
Thus,
.
This implies
.

For (ii), consider the times 
such that for ,
 is the first time such that
.
For convenience, let .
Note that  and .
Furthermore, for any time ,
.
Therefore, by (i),
for any poor-up of an item  within ,
its relative off, if exists, occurs at or after ,
which implies
at time , .
Then within each interval ,
the number of such  as well as the number of poor-ups with a relative off
are at most .
Within , there are  intervals
and hence 
poor-ups each with a relative off.
\end{proof}








\begin{theorem} \label{thm:fi-comm}
For approximate counting,
each individual stream can use the algorithm AC
with  and
it sends at most  words to the root
within a window.
\end{theorem}

{\bf Memory usage of each remote site.} Recall that we use two -approximate data structures
\cite{DatarGIM02,LeeT06}
for the total item count and individual item counts,
which respectively require
 bits
and  words.
Note that  bits
is equivalent to  words.
Furthermore, at any time,
we only need to keep track of
the last estimate sent to the root of
all item  with ,
which by Fact~\ref{lem:rj}, requires  words.
By setting  (see Theorem~\ref{thm:fi-comm}),
the total memory usage of a remote site
is  words.

\section{Extensions}
\label{sec:extension}
We extend the previous techniques to solve the problems of frequent items
and quantiles and  handle out-of-order streams.
Below BC refers to our algorithm for basic counting.


\vspace{.5ex}
{\bf Frequent items.}
Using the algorithms BC and AC, the root can
answer the -approximate frequent items as follows.
Each stream  communicates with the root using BC
with error parameter 
and AC with error parameter .
At any time ,
let  and  be the latest estimates of
the numbers of all items and item , respectively,
received by the root from .  To answer
a query of frequent items with threshold
 at time ,
the root can return all items  with  as the set of frequent
items.

To see the correctness, let  and 
be the number of all items and item  in  at time , respectively.
Algorithm BC guarantees ,
and algorithm AC guarantees .
Therefore, if an item  is returned by the root, then

where the second inequality comes from the definition of the algorithm.
The last term above is at least ,
so  is a frequent item.
If an item  is not returned by the root, then  and we can
show similarly that .


\vspace{.5ex}
{\bf Quantiles.}
We give an algorithm for
-approximate quantiles queries.
Let .
For each stream, we keep track of the -approximate -quantiles
for .
We update the root for all these -quantiles when one of the following two events occurs:
(i) for any , the value of the -quantile is
larger than the value of the -quantile last reported to the root, or
(ii) for any , the value of the -quantile is
smaller than the value of the -quantile last reported to the root.
The stream also communicates with the root using BC with
error parameter . In the root's perspective,
at any query time , let  be the
query given and let  be the last estimate sent by
 for the number of all items.
The root sorts the quantiles last reported by all streams
and for each stream , gives a weight of  to each quantile of .
Then the root returns the smallest item  in the sorted sequence such that
the sum of weights for all items no greater than  is at least .
Careful counting can show that  is
an -approximate -quantile.
  {To bound the communication cost, let  be
the number of items of  arriving or expiring during the window
.}
We observe that
when an event occurs, many items have either arrived or expired
after the previous event.
Using similar analysis as before,
we can show that within a window, there are at most  such events
and thus each stream sends 
words.
  {By Jensen's inequality again, our algorithm's
 total communication cost per window is 
where  is the number of items of the  streams
that arrive or expire within the window.}
Note that the lower bound of  words
for approximate frequent items carries to approximate quantiles,
as we can answer approximate frequent items using approximate
quantiles as follows. The root poses -approximate
-quantile queries
for .
Given the threshold  for frequent items,
the root returns all items that repeatedly occur as  (or more) consecutive quantiles,
and these items are -approximate frequent items.

\vspace{.5ex}
{\bf Out-of-order streams.}
All our algorithms
can be extended to out-of-order stream with a communication cost increased
by a factor of , as follows.
Each stream uses the data structures for out-of-order streams
(e.g., \cite{BuschT07,CormodeKT08}) to maintain the local estimates.
Then each stream uses our communication algorithms for in-order streams.
It is obvious the root
can answer the corresponding queries.
For the communication cost,
consider any time interval  of size .
Items arriving in  must have time-stamps in .
Using the same arguments as before, we can show
the same communication cost of each algorithm,
but only for a window of size  instead of .
Equivalently, in any window of size ,
the communication cost is increased by a factor of .




\bibliographystyle{plain}
\begin{thebibliography}{10}
\small


\bibitem{Aggarwal06}
C.~Aggarwal.
\newblock {\em Data streams: models and algorithms}.
\newblock Springer, 2006.

\bibitem{AlonMS02}
N.~Alon, Y.~Matias, and M.~Szegedy.
\newblock The space complexity of approximating the frequency moments.
\newblock {\em Journal of Computer and System Sciences}, 58(1):137--147, 1999.

\bibitem{ArasuM04}
A.~Arasu and G.~Manku.
\newblock Approximate counts and quantiles over sliding windows.
\newblock In {\em Proc. PODS}, pages 286--296, 2004.

\bibitem{BabaiGKL04}
L.~Babai, A.~Gal, P.~Kimmel, and S.~Lokam.
\newblock Communication compleixty of simultaneous messages.
\newblock {\em SIAM Journal on Computing}, 33(1):137--166, 2004.

\bibitem{BabcockDM02}
B.~Babcock, M.~Datar, and R.~Motwani.
\newblock Sampling from a moving window over streaming data.
\newblock In {\em Proc. SODA}, pages 633--634, 2002.

\bibitem{BabcockO03}
B.~Babcock and C.~Olston.
\newblock Distributed top- monitoring.
\newblock In {\em Proc. SIGMOD}, pages 28--39, 2003.

\bibitem{BuschT07}
C.~Busch and S.~Tirthapua.
\newblock A deterministic algorithm for summarizing asynchronous streams over a
  sliding window.
\newblock In {\em STACS}, 2007.

\bibitem{CormodeG05}
G.~Cormode and M.~Garofalakis.
\newblock Sketching streams through the net: distributed approximate query
  tracking.
\newblock In {\em Proc. VLDB}, pages 13--24, 2005.

\bibitem{CormodeGMR05}
G.~Cormode, M.~Garofalakis, S.~Muthukrishnan, and R.~Rastogi.
\newblock Holistic aggregates in a networked world: distributed tracking of
  approximate quantiles.
\newblock In {\em Proc. SIGMOD}, 25--36, 2005.

\bibitem{CormodeKT08}
G.~Cormode, F.~Korn, and S.~Tirthapura.
\newblock Time-decaying aggregates in out-of-order streams.
\newblock In {\em Proc. PODS}, pages 89--98, 2008.

\bibitem{CormodeMY08}
G.~Cormode, S.~Muthukrishnan, and K.~Yi.
\newblock Algorithms for distributed functional monitoring.
\newblock In {\em Proc. SODA}, pages 1076--1085, 2008.

\bibitem{DasGGR04}
A.~Das, S.~Ganguly, M.~Garofalakis, and R.~Rastogi.
\newblock Distributed set-expression cardinality estimation.
\newblock In {\em Proc. VLDB}, pages 312--323, 2004.

\bibitem{DatarGIM02}
M.~Datar, A.~Gionis, P.~Indyk, and R.~Motwani.
\newblock Maintaining stream statistics over sliding windows.
\newblock {\em SIAM Journal on Computing}, 31(6):1794--1813, 2002.

\bibitem{DatarM02}
M.~Datar and S.~Muthukrishnan.
\newblock Estimating rarity and similarity over data stream windows.
\newblock In {\em Proc. ESA}, pages 323--334, 2002.

\bibitem{DemaineLM02}
E.~Demaine, A.~Lopez-Ortiz, and J.~Munro.
\newblock Frequency estimation of internet packet streams with limited space.
\newblock In {\em Proc. ESA}, pages 348--360, 2002.





\bibitem{GibbonsT02}
P.~Gibbons and S.~Tirthapura.
\newblock Distributed streams algorithms for sliding windows.
\newblock In {\em Proc. SPAA}, pages 63--72, 2002.

\bibitem{GreenwaldK04}
M.~Greenwald and S.~Khanna.
\newblock Power-conserving computation of order-statistics over sensor
  networks.
\newblock In {\em Proc. PODS}, pages 275--285, 2004.

\bibitem{GuhaKS01}
S.~Guha, N.~Koudas, and K.~Shim.
\newblock Data-streams and histograms.
\newblock In {\em Proc. STOC}, pages 471--475, 2001.



\bibitem{Indyk00}
P.~Indyk.
\newblock Stable distributions, pseudorandom generators, embeddings and data
  stream computation.
\newblock In {\em Proc. FOCS}, pages 148--155, 2000.

\bibitem{JainYDZ05}
N.~Jain, P.~Yalagandula, M.~Dahlin, and Y.~Zhang.
\newblock Insight: A distributed monitoring system for tracking continuous
  queries.
\newblock In {\em Proc. SOSP}, pages 1--7, 2005.

\bibitem{KeralapuraCR06}
R.~Keralapura, G.~Cormode, and J.~Ramamirtham.
\newblock Communication-efficient distributed monitoring of thresholded counts.
\newblock In {\em Proc. SIGMOD}, pages 289--300, 2006.

\bibitem{LeeT06b}
L.~K. Lee and H.~F. Ting.
\newblock Maintaining significant stream statistics over sliding windows.
\newblock In {\em Proc. SODA}, pages 724--732, 2006.

\bibitem{LeeT06}
L.~K. Lee and H.~F. Ting.
\newblock A simpler and more efficient deterministic scheme for finding
  frequent items over sliding windows.
\newblock In {\em Proc. PODS}, pages 290--297, 2006.

\bibitem{ManjhiSDO05}
A.~Manjhi, V.~Shkapenyuk, K.~Dhamdhere, and C.~Olston.
\newblock Finding (recently) frequent items in distributed data streams.
\newblock In {\em Proc. ICDE}, pages 767--778, 2005.

\bibitem{MouratidisBP06}
K.~Mouratidis, S.~Bakiras, and D.~Papadias.
\newblock Continuous monitoring of top-k queries over sliding windows.
\newblock In {\em Proc. SIGMOD}, pages 635--646, 2006.

\bibitem{Muthukrishnan05}
S.~Muthukrishnan.
\newblock {\em Data streams: algorithms and applications}.
\newblock Now Publisher Inc., 2005.

\bibitem{OlstonJW03}
C.~Olston, J.~Jiang, and J.~Widom.
\newblock Adaptive filters for continuous queries over distributed data
  streams.
\newblock In {\em Proc. SIGMOD}, pages 563--574, 2003.

\bibitem{SharfmanSK06}
I.~Sharfman, A.~Schuster, and D.~Keren.
\newblock A geometric approach to monitoring threshold functions over
  distributed data streams.
\newblock {\em ACM TODS}, 32(4), 2007.

\bibitem{YiZ08}
K.~Yi and Q.~Zhang.
\newblock Optimal tracking of distributed heavy hitters and quantiles.
\newblock In {\em Proc. PODS}, pages 167--174, 2009.

\bibitem{YiZ09}
K.~Yi and Q.~Zhang.
\newblock Private communication.
\end{thebibliography}


\end{document}
