\section{Experiments} \label{sec:5}

\subsection{Experiment Setup} \label{sec:5.1}

We evaluate the proposed SAIS method on the following three document-level RE benchmarks.
DocRED \cite{yao2019docred} is a large-scale crowd-sourced dataset based on Wikipedia articles.
It consists of  relation types, seven entity types, and  documents in total, where each document has  entities on average. 
CDR \cite{li2016biocreative} and GDA \cite{wu2019renet} are two biomedical datasets where CDR studies the binary interactions between disease and chemical concepts with  documents and GDA studies the binary relationships between gene and disease with  documents.
We follow \citet{christopoulou2019connecting} for splitting the train and develop sets.

We run our experiments on one Tesla A6000 GPU and carry out five trials with different seeds to report the mean and one standard error.
Based on Huggingface \cite{wolf2019huggingface}, we apply cased BERT-base \cite{devlin2018bert} and RoBERTa-large \cite{liu2019roberta} for DocRED and cased SciBERT \cite{beltagy2019scibert} for CDR and GDA.
The embedding dimension  of BERT or SciBERT is , and that of RoBERTa is .
The number of groups  in all group bilinear layers is .

For the general hyperparameters of language models, we follow the setting in \cite{zhou2021document}.
The learning rate for fine-tuning BERT is , that for fine-tuning RoBERTa or SciBERT is , and that for training the other parameters is .
All the trials are optimized by AdamW \cite{loshchilov2017decoupled} for  epochs with early stopping and a linearly decaying scheduler \cite{goyal2017accurate} whose warm-up ratio .
Each batch contains  documents and the gradients of model parameters are clipped to a maximum norm of .


For the unique hyperparameters of our method, we choose  from  for the focal hyperparameters  and  based on the develop set.
We also follow \citet{xie2021eider} for setting the FER prediction threshold  as  and all the relative task weights  for  as .

\subsection{Quantitative Evaluation} \label{sec:5.2}

Besides RE, DocRED also suggests to predict the supporting evidence for each relation instance.
Therefore, we apply  to both RE and ER.
We report the results of  as well as existing graph-based and transformer-based baselines in Table~\ref{tab:docred_result}\footnote{For a fair comparison, we report the scores of SSAN \cite{xu2021entity} without being pretrained on an extra dataset.} (full details in Appendix~\ref{ap:c}).
Generally, thanks to PLMs' strength in modeling long-range dependencies, transformer-based methods perform better on RE than graph-based methods.
Moreover, most earlier approaches are not capable of ER despite the interpretability ER adds to the predictions.
In contrast, our  method not only establishes a new state-of-the-art result on RE, but also outperforms the runner-up significantly on ER. 

Since neither CDR nor GDA annotates evidence sentences, we apply  here.
It is trained with RE, CR, and ET and infers without data augmentation.
As shown in Table~\ref{tab:bio_result} (full details in Appendix~\ref{ap:c}), our method improves the prior best RE F1 scores by  and  absolutely on CDR and GDA, respectively.
It indicates that our proposed method can still improve upon the baselines even if only part of the four complementary tasks are annotated and operational.


\subsection{Ablation Study} \label{sec:5.3}

To investigate the effectiveness of each of the four complementary tasks proposed in Section~\ref{sec:3}, we carry out an extensive ablation study on the DocRED develop set by training SAIS with all possible combinations of those tasks.
As shown in Table~\ref{tab:ablation_task}, without any complementary tasks, the RE performance of SAIS is comparable to ATLOP \cite{zhou2021document} due to similar neural architectures.
When only one complementary task is allowed, PER is the most effective single task, followed by ET.
Although FER is functionally analogous to PER, since FER only involves the small subset of entity pairs with valid relations, the performance gain brought by FER alone is limited.
When two tasks are used jointly, the pair of PER and ET, which combines textual contexts and entity type information, delivers the most significant improvement.
The pair of PER and FER also performs well, which reflects the finding in \cite{peng2020learning} that context is the most important source of information.
The version with all tasks except CR sees the least drop in F1, indicating that CR's supervision signals on capturing contexts can be covered in part by PER and FER.
Last but not least, the SAIS pipeline with all four complementary tasks achieves the highest F1 score.
Similar trends are also recognized on CDR and GDA in Table~\ref{tab:bio_result}, where SAIS trained with both CR and ET (besides RE) scores higher than its single-task counterpart.

\begin{table}[t!]
\setlength\tabcolsep{2pt}
  \centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{lcc}
    \toprule
        \textbf{Model} & \textbf{CDR} & \textbf{GDA} \\
    \midrule
    \midrule
 \cite{nan2020reasoning} & 64.8 & 82.2 \\
         \cite{beltagy2019scibert} & 65.1 & 82.5 \\
         \cite{zhang2020document} & 65.9 & 83.1 \\
 \cite{xu2021entity} & 68.7 & 83.7 \\
         \cite{zhou2021document} & 69.4 & 83.9 \\
         \cite{zeng2021sire} & 70.8 & 84.7 \\
         \cite{zhang2021document} & 76.3 & 85.3 \\
    \midrule
         (Ours) & \textbf{79.0}  \textbf{0.8} & \textbf{87.1}  \textbf{0.3} \\
    \midrule
         &  &  \\
         &  &  \\
         &  &  \\
    \bottomrule
  \end{tabular}
}
  \caption{RE F1 results () on the CDR and GDA test sets.
  The baseline scores are from the corresponding papers.
   scores the highest on both datasets.
Full details in Appendix~\ref{ap:c}.}
  \label{tab:bio_result}
\end{table}

Moreover, as compared to the original pipeline , pseudo document-based data augmentation  acts as a hard filter by directly removing predicted non-evidence sentences, while attention mask-based data augmentation  distills the context more softly.
Therefore, we observe in Table~\ref{tab:ablation_ensemble} that  earns a higher precision, whereas  attains a higher recall.
By ensembling , , and , we improve the RE F1 score by  absolutely on the DocRED develop set.





\begin{table}[t!]
  \centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{ccccc|c} 
    \toprule
        \textbf{CR} & \textbf{ET} & \textbf{PER} & \textbf{FER} & \textbf{RE} & \textbf{F1}\\
    \midrule
    \midrule
              &        &        &        & \cmark &  \\
    \midrule
        \cmark &        &        &        & \cmark &  \\
              & \cmark &        &        & \cmark &  \\
              &        & \cmark &        & \cmark &  \\
              &        &        & \cmark & \cmark &  \\
  \midrule
        \cmark & \cmark &        &        & \cmark &  \\
        \cmark &        & \cmark &        & \cmark &  \\
        \cmark &        &        & \cmark & \cmark &  \\
              & \cmark & \cmark &        & \cmark &  \\
              & \cmark &        & \cmark & \cmark &  \\
              &        & \cmark & \cmark & \cmark &  \\
    \midrule
              & \cmark & \cmark & \cmark & \cmark &  \\
        \cmark &        & \cmark & \cmark & \cmark &  \\
        \cmark & \cmark &        & \cmark & \cmark &  \\
        \cmark & \cmark & \cmark &        & \cmark &  \\
    \midrule
        \cmark & \cmark & \cmark & \cmark & \cmark &  \\
    \bottomrule
  \end{tabular}
}
  \caption{Ablation study () using  to assess the effectiveness of the four complementary tasks (i.e., CR, ET, PER, and FER) for RE based on the DocRED develop set.}
  \label{tab:ablation_task}
\end{table}



\begin{table}[t!]
  \setlength\tabcolsep{4pt}
  \centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{ccc|ccc} 
    \toprule
         &  &  & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
    \midrule
    \midrule
        \cmark &        &        & 66.58 & 58.70 & 62.39 \\
               & \cmark &        & 73.21 & 45.59 & 56.19 \\
               &        & \cmark & 53.14 & 67.49 & 59.46 \\
    \midrule
        \cmark & \cmark &        & 71.14 & 54.35 & 61.62 \\
        \cmark &        & \cmark & 61.61 & 62.90 & 62.25 \\
    \midrule
        \cmark & \cmark & \cmark & 67.76 & 58.79 & 62.96 \\
    \bottomrule
  \end{tabular}
}
  \caption{Ablation study () using  to assess the effectiveness of data augmentation (i.e., original (), pseudo document-based (), and attention mask-based ()) for RE based on the DocRED develop set.}
  \label{tab:ablation_ensemble}
\end{table}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figure/figure_3.pdf}
    \caption{
    \textbf{(a)} 
    Case study on the effectiveness of textual contexts and entity type information based on models' extracted relations from the DocRED develop set.
    By capturing contexts across sentences and regularizing them with entity type information,  extracts relations of better quality.
    \textbf{(b)} 
    Case study on the difference between FER and PER based on retrieved evidence from the DocRED develop set. 
    FER considers evidence unique to each relation for better interpretability.
    Irrelevant sentences are omitted here.
    } 
    \label{fig:3}
\end{figure*}


\subsection{Qualitative Analysis} \label{sec:5.4}

To obtain a more insightful understanding of how textual contexts and entity type information help with RE, we present a case study in Figure~\ref{fig:3} (a).
Here,  is trained with the task (i.e., ET) related to entity type information while  is trained with the tasks (i.e., CR, PER, and FER) related to textual contexts.
Compared to , which is trained with all four complementary tasks, they both exhibit drawbacks qualitatively.
In particular,  can easily infer the relation ``\textit{country}" between Entities E and C based on their respective types ``\textit{ORG}" and ``\textit{LOC}", whereas  may misinterpret Entity E as of type ``\textit{PER}" and infer the relation ``\textit{citizenship}" wrongly.
On the other hand,  can directly predict the relation ``\textit{place\_of\_birth}" between Entities A and B by pattern matching, while overemphasizing the type ``\textit{LOC}" of Entity B may cause  to deliver the wrong relation prediction ``\textit{location}".
Last but not least,  effectively models contexts spanning multiple sentences and regularizes them with entity type information.
As a result, it is the only SAIS variant that correctly predicts the relation ``\textit{country\_of\_origin}" between Entities D and C.

Furthermore, to examine why SAIS (which uses FER for retrieving evidence) outperforms Eider \cite{xie2021eider} (which uses PER) significantly on ER in Table~\ref{tab:docred_result}, we compare the performance of FER and PER based on a case study in Figure~\ref{fig:3} (b).
More specifically, PER identifies the same set of evidence for both relations between Entities A and B, among which Sentence 2 describes ``\textit{place\_of\_birth}" while Sentence 6 discusses ``\textit{place\_of\_death}".
In contrast, FER considers an evidence set unique to each relation and outputs more interpretable results.