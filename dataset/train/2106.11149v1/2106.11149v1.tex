\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{multirow}
\usepackage{soul}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{booktabs}

\usepackage{verbatim}
\usepackage{multirow, makecell}
\usepackage{lineno}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{gensymb}
\usepackage{bm}
\usepackage{diagbox}
\usepackage{tabularx}
\usepackage{rotating}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{xmpmulti}
\usepackage{arydshln}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{OadTR: Online Action Detection with Transformers}
\author{
    Xiang Wang
    \hspace{0.2cm} Shiwei Zhang
    \hspace{0.2cm} Zhiwu Qing
    \hspace{0.2cm} Yuanjie Shao
    \hspace{0.2cm} Zhengrong Zuo\\
    \hspace{0.2cm} Changxin Gao 
    \hspace{0.2cm} Nong Sang\thanks{Corresponding Author}\
X_{0} =\tilde{F}+ E_{pos}
\label{eq1}\\

{X}'& = {\rm Norm}(X_{0}) \\
{\rm Attention}(Q_{i}; K_{i}; V_{i}) &= {\rm softmax}\left (\frac{Q_{i}K_{i}^{T}}{\sqrt{d_{k}}}\right )V_{i}
\label{eq2}\\
H_{i} &= {\rm Attention}(Q_{i}; K_{i}; V_{i})
\label{eq3}

\hat{H} = Stack\left ( H_{1},H_{2},...,H_{N_{head}} \right )\mathbf{W}_{d} \in \mathbb{R}^{\left (T+2\right ) \times D}
\label{eq4}

\hat{H} &={\rm MSA}(Norm(X_{0}))\\
{m'}_{1} &=\hat{H} + X_{0} \label{eq5}\\ 
{{m}_{n}} &={\rm FFN}\left ({\rm Norm}({m'}_{n-1})\right ) + {m'}_{n-1}
\\
{m'}_{n} &= {\rm MSA}({\rm Norm}({{m}_{n-1}})) + {{m}_{n-1}}
\label{eq6}

\widetilde{Q} &= Avg\text{-}pool(\widetilde{Q}_{1},\widetilde{Q}_{2},...,\widetilde{Q}_{\ell_d})
\\
p_{0} &= {\rm softmax}(Concat [{m}_{N}^{token},\widetilde{Q} ]\mathbf{W}_{c})
\label{eq10}

\widetilde{p}^{i} = {\rm softmax}(\widetilde{Q}_{i} \mathbf{W}_{c}^{'}), i=1,2,...,\ell_d
\label{eq16}

Loss = {\rm CE}(p_{0},y_{0}) + \lambda \sum_{i=1}^{\ell_d}{\rm CE}(\widetilde{p}_{i}, \widetilde{y}_{i})
\label{eq14}

cPrec &= \frac{TP}{TP+\frac{FP}{w}}
\\ 
cAP &= \frac{\sum_{k}cPrec(k) \times {I}(k) }{\sum TP}

where  is equal to 1 if frame  is a TP. The coefficient  is the ratio between negative and positive frames. 

\begin{table*}[t!]\centering
\hspace{1mm}
\subfloat[Ablation study of the effectiveness of our proposed components on the HDD dataset.  \label{tab:HDD_compare_1}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
\footnotesize
\# Method & \small mAP (\%) \\
\shline
 LSTM~\cite{HDD} & 23.8 \\
 Encoder-only (Baseline) & 28.7 \\
 \hline
 Baseline + TT & 29.0 \\
 Baseline + DE & 29.2 \\
 Baseline + TT + DE (OadTR) &  \bf{29.8}\\
\end{tabular}}
\hspace{4mm}
\subfloat[Ablation study of the effectiveness of our proposed components on the TVSeries dataset. \label{tab:TVSeries_compare_2}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
\footnotesize
\# Method & \small mcAP (\%) \\
\shline
 LSTM~\cite{IDN} & 80.9 \\
 Encoder-only (Baseline) & 84.8 \\
 \hline
 Baseline + TT & 85.0 \\
 Baseline + DE & 85.1 \\
 Baseline + TT + DE (OadTR) &  \bf{85.4}\\
\end{tabular}}
\hspace{4mm}
\subfloat[Ablation study of the effectiveness of our proposed components on the THUMOS14 dataset. \label{tab:THUMOS14_compare_3}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
\footnotesize
\# Method & \small mAP (\%) \\
\shline
 LSTM~\cite{IDN} & 46.3 \\
 Encoder-only (Baseline) & 55.8 \\
 \hline
 Baseline + TT & 56.9 \\
 Baseline + DE & 56.7 \\
 Baseline + TT + DE (OadTR) &  \bf{58.3}\\
\end{tabular}}
\hspace{1mm}


\vspace{2mm}
\subfloat[Ablation study of different position encoding manners on the HDD dataset.  \label{tab:HDD_compare_1}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
\footnotesize
\# Position encoding & \small mAP (\%) \\
\shline
 No Position & 28.8 \\
 Fixed Position & 29.3 \\
 Learned Position &  \bf{29.8}\\
\multicolumn{2}{c}{~}\\
\multicolumn{2}{c}{~}\\
\multicolumn{2}{c}{~}\\
\end{tabular}}
\hspace{1mm}
\subfloat[Ablation study of the head number on the THUMOS14 dataset. \label{tab:THUMOS14_compare_2}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
 \footnotesize
\# Head & \small mAP (\%) \\
\shline
 Head = 1 & 57.4 \\
 Head = 2 & 57.7 \\
 Head = 4 &  \bf{58.3}\\
Head = 8  & 57.7 \\
 Head = 16  & 57.8 \\
Head = 32  & 57.4 \\
\end{tabular}}
\hspace{1mm}
\subfloat[Ablation study of the query dimensions on the THUMOS14 dataset. \label{tab:THUMOS14_compare_3}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
 \footnotesize
\# Dim & \small mAP (\%) \\
\shline
 Dim = 128 & 56.4 \\
 Dim = 256 & 57.1 \\
 Dim = 512 & 57.3 \\
 Dim = 1024 & \bf{58.3} \\
 Dim = 1536 & 57.7 \\
 Dim = 2048 & 57.6 \\
\end{tabular}}
\hspace{1mm}
\subfloat[Generalization evalation. The results indicate the generalization and superiority of our model design. \label{tab:sparse_compare_3}]{
\tablestyle{4pt}{1.05}
\footnotesize
\begin{tabular}{l|c}
 \footnotesize
\# Generalization & \small m(c)AP (\%) \\ \shline
 Vanilla Transformer (HDD) & 29.8\\
 Sparse Transformer (HDD) & 29.6 \\
 Vanilla Transformer (TVSeries) & 85.4 \\
 Sparse Transformer (TVSeries) & 85.0 \\
 Vanilla Transformer (THUMOS14) & 58.3 \\
 Sparse Transformer (THUMOS14) & 58.1 \\
\end{tabular}}
\\
\caption{Ablation studies.
\label{table:Ablation_study}}
\vspace{-4mm}
\end{table*}\subsection{Compared with state-of-the-art methods}
To evaluate the performance, we compare our proposed OadTR and other state-of-the-art methods~\cite{RED,TRN,IDN} on HDD, TVSeries, and THUMOS14 datasets.
Noted that we use the same network parameter settings (\eg, , ) when comparing different datasets. 
As illustrated in Table~\ref{table:HDD_detection}, our OadTR achieves state-of-the-art performance and improves mAP from 29.2 to 29.8 on the HDD dataset, demonstrating that our OadTR can achieve an overall performance promotion of online action detection. It could be attributed to that our OadTR introduces Transformers to obtain global historical information and the future context efficiently.

Table~\ref{table:TVSeries_detection} compares recent online action detection methods on the TVSeries dataset. To ensure a fair comparison, we adopt the same video features. The results signify that our OadTR can achieve good performance in the case of different video feature inputs. The reason for the better results of TSN-Kinetics may be that the categories of Kinetics are more diverse and contains much common generalizable presentations. 

As shown in Table~\ref{table:THUMOS14_detection}, we also conduct a comprehensive comparison on the THUMOS14 dataset. Specifically, performance is improved by 8.3 (50.0  58.3 ) under TSN-Anet feature input and 4.9 (60.3  65.2 ) under TSN-Kinetics feature input. The above results indicate the effectiveness of our proposed OadTR. 


When only a fraction of each action is considered, we compared OadTR with previous methods. Table~\ref{tab6} shows that our OadTR significantly outperforms state-of-the-art methods~\cite{TRN,IDN} at most time stages. Specifically, this indicates the superiority of OadTR in recognizing actions at early stages as well as all stages. It could be attributed to the ability of our OadTR to efficiently model temporal dependencies.

\subsection{Ablation studies}
\label{ablation}
To facilitate our analysis of the model, we take the encoder without task token as our baseline.
We further conduct detailed ablation studies to evaluate different components of the proposed framework, include the following:\\
\textbf{\textit{Encoder-only (Baseline)}:} We adopt the encoder in the original Transformer~\cite{Transformer} and apply it directly to the online action detection task. Note that compared with our OadTR's encoder, the task token is missing, and the classifier is applied to the last output representation (\ie, corresponding to ) of the Transformer's encoder.
\\
\textbf{\textit{Baseline + TT}:} We incorporate \textit{Baseline} and the task token (TT) together, which is our OadTR's encoder. This method adds a task-related token, and we use ablation experiments to illustrate the necessity of this token. \\
\textbf{\textit{Baseline + DE}:} In this method, we add the decoder (DE) of the prediction task in OadTR to the baseline method to test and verify the function of the decoder. \\
\textbf{\textit{Baseline + TT + DE (OadTR)}:} This is the method we propose in this paper, adding task token and decoder to the baseline method together. 

In Table~\ref{table:Ablation_study} (a-c), we report the performance comparison experiments of the above methods on HDD, TVSeries, and THUMOS14 datasets. The results of \textit{Baseline + TT} demonstrate that using the additional task token is helpful for action classification. The results of \textit{Baseline + DE} explain the remarkable power of the auxiliary prediction task. When combined with the above two improvements, our OadTR (\ie, \textit{Baseline + TT + DE}) achieves the best results on three datasets. Specifically, when compared with the baseline method, our OadTR approach has improved by 1.1, 0.6, and 2.5 on HDD, TVSeries, and THUMOS14 datasets, respectively. 
\vspace{+3pt} 

\begin{table}[t]
    \centering
    \footnotesize
\tablestyle{4pt}{1.05}
\begin{tabular}
        {@{\;\;}l@{\;\;}@{\;\;}|l@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}}
& & \multicolumn{4}{c}{Decoder steps ()}\\
\cline{3-6}
        Dataset & Task & 2 & 4 & 8 & 16 \\
\shline
\multirow{2}{*}{HDD}
        & {Online Action Detection}   & 28.3 & 28.4 & \bf{29.8} & 28.8 \\
        & {Action Anticipation}       & 27.2 & 26.2 & 23.0 & 16.9 \\ 
\hline
        \multirow{2}{*}{TVSeries}
        & {Online Action Detection}   & 85.3 & 85.2 & \bf{85.4} & 85.2 \\
        & {Action Anticipation}       & 81.8 & 80.4 & 77.8 & 74.3 \\ 
\hline
        \multirow{2}{*}{THUMOS14}
        & {Online Action Detection}   & 57.4 & 57.8 & \bf{58.3} & 58.0 \\
        & {Action Anticipation}       & 53.5 & 51.0 & 45.9 & 40.7 \\


    \end{tabular}
    \vspace{-5pt}
    \caption{
        Online action detection and action anticipation results of our proposed OadTR
        with decoder steps .
    }
    \vspace{-8pt}
    \label{table:decoder}
\end{table}
\begin{table}[t]
    \centering
    \footnotesize
    \begin{subtable}[t]{\linewidth}
\centering
\tablestyle{4pt}{1.05}
    \begin{tabular}
        {@{\;\;}l@{\;\;}@{\;\;}|l@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c}
& & \multicolumn{6}{c}{Encoding layers ()}\\
\cline{3-8}
        Method & Dataset & 1 & 2 & 3 & 4 & 5 & 6 \\
\shline
        \multirow{3}{*}{OadTR}
        & {HDD}   & 28.8 & 29.0 & \bf{29.8} & 29.5 & 28.4 & 28.4 \\ & {TVSeries}   & 85.4 & \bf{85.5} & 85.4 & 85.3 & 84.8 & 85.0 \\ & {THUMOS14}   & 57.5 & 56.9 & \bf{58.3} & 57.4 & 56.9 & 56.6 \\


    \end{tabular}
    \vspace{-3pt}
    \caption{
        Online action detection results of different encoding layers. Note that we fixed  for simplicity. 
    }
    \vspace{+3pt}
\end{subtable}
\begin{subtable}[t]{\linewidth}
        \centering
\tablestyle{4pt}{1.05}
\begin{tabular}
        {@{\;\;}l@{\;\;}@{\;\;}|l@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c}
& & \multicolumn{6}{c}{Decoding layers ()}\\
\cline{3-8}
        Method & Dataset & 1 & 2 & 3 & 4 & 5 & 6 \\
\shline
        \multirow{3}{*}{OadTR}
        & {HDD}   & 27.7 & 28.0 & 29.5 & 29.5 & \bf{29.8} & 28.6 \\ & {TVSeries}   & 84.8 & 84.8 & 85.2 & 85.4 & 85.4 & \bf{85.5} \\ & {THUMOS14}   & 57.4 & 57.9 & 56.9 & 57.2 & \bf{58.3} & 56.6 \\


    \end{tabular}
    \vspace{-3pt}
    \caption{
        Online action detection results of different decoding layers. Note that we fixed  for simplicity. 
    }
    \vspace{-5pt}
\end{subtable}
\caption{
        Ablation study of encoding layers  and decoding layers  using TSN-Anet features.
    }
    \label{table:encoder_decoder}
\vspace{-8pt}
\end{table}
\begin{table*}[t!]
    \small \centering
\tablestyle{4pt}{1.05}
    \begin{subtable}[t]{\textwidth} \centering
        \begin{tabular}
            {@{\;}@{\;}l@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}|c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\,}@{\;}r@{\;\;\;}@{\;}@{\;}}
& \multicolumn{8}{c}{Time predicted into the future (seconds)} & \\
\cline{2-9}    
Method & 0.25s & 0.5s & 0.75s & 1.0s & 1.25s & 1.5s & 1.75s & 2.0s &\;\;\;\; \;\;\;\;Avg \\
\shline
            {ED~\cite{RED}} & 78.5 & 78.0 & 76.3 & 74.6 & 73.7 & 72.7 & 71.7 & 71.0 & 74.5 \\
            {RED~\cite{RED}} & 79.2 & 78.7 & 77.1 & 75.5 & 74.2 & 73.0 & 72.0 & 71.2 & 75.1 \\
            {TRN~\cite{TRN}} & 79.9 & 78.4 & 77.1 & 75.9 & 74.9 & 73.9 & 73.0 & 72.3 & 75.7 \\
            {\bf{OadTR}} & 81.9 & 80.6 & 79.4 & 78.2 & 77.1 & 76.0 & 75.2 & 74.3 & \textbf{77.8} \\ \cdashline{1-10}
            {\bf{OadTR (Kinetics)}}& 84.1 & 82.6 & 81.3 & 80.1 & 78.9 & 77.7 & 76.7 & 75.7 & \textbf{79.1} \\
\end{tabular}
        \vspace{-3pt}
        \caption{
            Results on the TVSeries dataset in terms of mcAP (\%).
        }
        \vspace{3pt}
    \end{subtable}
\tablestyle{4pt}{1.05}
    \begin{subtable}[t]{\textwidth}
        \centering
        \begin{tabular}
            {@{\;}@{\;}l@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}|c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}@{\;}c@{\;}@{\,}@{\;}r@{\;\;\;}@{\;}@{\;}}
& \multicolumn{8}{c}{Time predicted into the future (seconds)} & \\
\cline{2-9}        
Method & 0.25s & 0.5s & 0.75s & 1.0s & 1.25s & 1.5s & 1.75s & 2.0s &\;\;\;\; \;\;\;\;Avg \\
\shline
            {ED~\cite{RED}} & 43.8 & 40.9 & 38.7 & 36.8 & 34.6 & 33.9 & 32.5 & 31.6 & 36.6 \\
            {RED~\cite{RED}} & 45.3 & 42.1 & 39.6 & 37.5 & 35.8 & 34.4 & 33.2 & 32.1 & 37.5 \\
            {TRN~\cite{TRN}} & 45.1 & 42.4 & 40.7 & 39.1 & 37.7 & 36.4 & 35.3 & 34.3 & 38.9 \\
            {\bf{OadTR}} & 50.2 & 49.3 & 48.1 & 46.8 & 45.3 & 43.9 & 42.4 & 41.1 & \textbf{45.9} \\ \cdashline{1-10}
            {\bf{OadTR (Kinetics)}} & 59.8 & 58.5 & 56.6 & 54.6 & 52.6 & 50.5 & 48.6 & 46.8 & \textbf{53.5} \\
\end{tabular}
        \vspace{-3pt}
        \caption{
            Results on the THUMOS14 dataset in terms of mAP (\%).
        }        
        \vspace{3pt}
    \end{subtable}
    \vspace{-10pt}
    \caption{
        Action anticipation results of our OadTR compared to state-of-the-art
        methods using the same two-stream features.
    }
    \vspace{-2pt} 
    \label{table:anticipation}
\end{table*}
\begin{table}[t]
    \centering
    \footnotesize
\tablestyle{4pt}{1.05}
\begin{tabular}
        {@{\;\;}l@{\;\;}@{\;\;}|l@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}}
& & \multicolumn{3}{c}{Settings}\\
\cline{3-5}
        Method & Dataset & Max-pool & Avg-pool & w/o encoder  \\
\shline
\multirow{3}{*}{OadTR}
        & {HDD}   & 29.2 & \bf{29.8} & 26.1 \\
& {TVSeries}   & 85.1 & \bf{85.4} & 80.8 \\
& {THUMOS14}   & 57.9 & \bf{58.3} & 53.5 


    \end{tabular}
    \vspace{-5pt}
    \caption{
        Comparison of different fusion methods and the necessity of the encoder feature encoding.
    }
    \vspace{-3pt}
    \label{table:aggregation_type}
\end{table}
\noindent\textbf{Importance of position encoding.} To demonstrate the importance of using position encoding, we organized some comparative experiments. As shown in Table~\ref{table:Ablation_study} (d), position encoding is necessary, and the learnable position encoding achieves the best result.
\vspace{+3pt}
\\
\textbf{Effect of head number.} Multi-head self-attention is a critical component. Here we study the impact of different head numbers of the decoder on performance. We can find that a optimal result is achieved when the number of heads is 4 (Table~\ref{table:Ablation_study} (e)). 
\vspace{+3pt}
\\
\textbf{Effect of query dimensions.} We conduct experiments to examine how different query dimensions affect online action detection performance. As shown in Tabel~\ref{table:Ablation_study} (f), when the number of feature dimensions is relatively small (\eg, 128), the model capacity is limited and the performance is relatively poor. As the number of feature dimensions gradually increases, the model capacity increases, and performance improves. However, when it exceeds a specific value (\eg, 1024), over-fitting may occur. 
\vspace{+3pt}
\\
\textbf{Generalizability.} To further investigate the generalization of our OadTR for many Transformer variants, we replace the standard Transformer~\cite{Transformer} with Sparse Transformer~\cite{sparse_Transformer}. As illustrated in Table~\ref{table:Ablation_study} (g), we can find that the performance is still good after replacement. Generally, Sparse Transformer can reduce computational consumption but leads to a little performance degradation.
\vspace{+3pt}
\\
\textbf{Effect of decoder step count.} The step size used to predict the future also has an impact on performance. In Table~\ref{table:decoder}, we compare four step sizes (\ie, 2, 4, 8, and 16), and the results denote that  achieves the best results on the three datasets. \vspace{+3pt}
\\
\textbf{Effect of encoding layers  and decoding layers .} To further study the impact of different encoding and decoding layers on performance, additional experiments are conducted, and results are shown in Table~\ref{table:encoder_decoder}. In most cases, the best results are achieved when . However, there will also be fluctuations, such as on the TVSeries dataset. 
\vspace{+3pt}
\\
\textbf{Feature aggregation type.} We also conduct experiments to explore different types that aggregate future and current features. We can notice that Avg-pool is better than Max-pool (Table~\ref{table:aggregation_type}). The reason may be that the predicted deep semantic representations of different time steps all have a specific promotion effect on the current classification. Simultaneously, the results of \textit{w/o encoder} also indicate the necessity of the encoder to learn discriminative features. 

\begin{figure}[t!]
\centering
{\centering{\includegraphics[width=.48\linewidth]{pictures/thumos_idu-select-0.pdf}}}
\hspace{1mm}
\centering{\includegraphics[width=.48\linewidth]{pictures/thumos_OadTR-select-0.pdf}} 
\vspace{-6mm}
\caption{\label{figure-visual-1}The t-SNE visualization of the classification embedding logits. Different colors correspond to different action categories from the THUMOS14 dataset. The mutual color correspondences include: \textcolor[RGB]{241,125,31}{CliffDiving}, \textcolor[RGB]{148,181,81}{HighJump}, \textcolor[RGB]{104,50,6}{PoleVault} and \textcolor[RGB]{211,222,252}{Shotput}. Better view in colored PDF.
}
\vspace{-4mm}
\end{figure}
\begin{figure}[t!]
\centering
\centering{\includegraphics[width=.99\linewidth]{pictures/visual.pdf}} 
\vspace{-1mm}
\caption{\label{figure-visual-2}Attention visualization maps. They indicate how much attention is paid to parts of the input streaming video. 
}
\vspace{-4mm}
\end{figure}
\subsection{Action anticipation}
In our proposed OadTR, we introduce predicted future information to identify current actions. To evince the accuracy of our predictions, we also conduct experiments to compare with other methods. Table~\ref{table:anticipation} demonstrates that OadTR outperforms the current state-of-the-art method~\cite{TRN} by a large margin.
In particular, the performance of OadTR is 2.1 higher than TRN~\cite{TRN} on the TVSeries dataset and 7.0 higher on THUMOS14. Furthermore, the performance of OadTR can also be further improved when pre-training on Kinetics. \subsection{Qualitative evaluation}
For better analysis, we visualize the classification results in Figure~\ref{figure-visual-1}. Obviously, by visualizing all the test samples of the four action categories, we can observe that our OadTR has better separability compared with the current state-of-the-art IDN~\cite{IDN}.
Further, we show OadTR's multi-head attention visualization results in Figure~\ref{figure-visual-2}, which indicates the importance of the multi-head design to learn more complex and comprehensive relations between different neighboring frame chunks. 


\section{Conclusion}
In this paper, we have proposed a new online action detection framework built upon Transformers, termed OadTR. 
In contrast to existing RNN based methods that process the sequence one by one recursively and hard to be optimized, we aim to design a direct end-to-end parallel network.
OadTR can recognize current actions by encoding historical information and predicting future context simultaneously.
Extensive experiments are conducted and verify the effectiveness of OadTR. Particularly, OadTR achieves higher training and inference speeds than current RNN based approaches, and obtains significantly better performance compared to the state-of-the-art methods.
In the future, we will extend our OadTR model to more tasks such as action recognition, spatio-temporal action detection, \etc.




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
