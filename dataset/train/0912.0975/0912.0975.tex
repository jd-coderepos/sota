\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}



\usepackage[dvips]{geometry}
\geometry{papersize={1.06\textwidth,1.1\textheight},total={\textwidth,\textheight}}


\usepackage{amsthm,amsmath,amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}

\title{An expected-case sub-cubic solution to the all-pairs shortest path problem in }
\author{Julian~J.~McAuley\thanks{The authors are with the Statistical Machine Learning Program at NICTA, and the Research School of Information Sciences and Engineering, Australian National University. Queries should be addressed to \texttt{julian.mcauley@nicta.com.au}.} and Tib\'erio~S.~Caetano}

\newcommand{\nbr}[1]{\left\|#1\right\|}
\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}


\usepackage{cite}
\usepackage{graphicx}
\usepackage{url}


\newcommand{\eq}[1]{(eq.~\ref{#1})}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicreturn}{\textbf{Return:}}

\begin{document}

\maketitle

\begin{abstract}
It has been shown by Alon et al.~that the so-called `all-pairs shortest-path' problem can be solved in  for graphs with  vertices, with integer distances bounded by . We solve the more general problem for graphs in  (assuming no negative cycles), with expected-case running time . While our result appears to violate the  requirement of ``Funny Matrix Multiplication'' (due to Kerr), we find that it has a sub-cubic \emph{expected time} solution subject to reasonable conditions on the data distribution. The expected time solution arises when certain sub-problems are uncorrelated, though we can do better/worse than the expected-case under positive/negative correlation (respectively). Whether we observe positive/negative correlation depends on the statistics of the graph in question.
In practice, our algorithm is significantly faster than Floyd-Warshall, even for dense graphs.
\end{abstract}

\section{Problem Definition}

The all-pairs shortest path problem \cite{Dijkstra59} consists of solving

for all vertices , where  is the space of all paths connecting  to  in , and  is the path length, i.e.,  where  is the weight of the edge connecting  to , or  if no such edge exists.

A simple divide-and-conquer solution to \eq{eq:problem} can be obtained by defining  to be the shortest path between  and  containing at most  edges. This solution exploits the fact that

This allows us to solve the all-pairs shortest path problem via Algorithm \ref{alg:apsp}, which we requires  time (this is by no means the optimal solution, though it is this version to which our improvements apply).

Algorithm \ref{alg:apsp}, Line \ref{line:min} requires that we solve a problem of the form

Although this appears to be a \emph{linear-time} operation (in ), we note that it can be reduced to  (in the expected-case) if we know the permutations that sort  and . The sorted values of  will be reused for every value of , and likewise the sorted values of  will be reused for every value of .

Lines \ref{line:funnystart}--\ref{line:min} of Algorithm \ref{alg:apsp} are sometimes referred to as the ``Funny Matrix Multiplication'' problem: replacing  with  yields the traditional version of matrix multiplication. Kerr \cite{Kerr70} showed that it is  if only the operations  and  are allowed. We find that under reasonable conditions on  and , an expected-case sub-cubic solution exists, requiring only  and .

\begin{algorithm}
 \caption{All-pairs shortest-path problem}
 \label{alg:apsp}
\begin{algorithmic}[1]
 \REQUIRE a graph 
 \FOR{}
   \FOR{}
     \STATE 
   \ENDFOR
 \ENDFOR
 \FOR{ \COMMENT{}}
   \FOR{} \label{line:funnystart}
     \FOR{}
       \STATE  \COMMENT{} \label{line:min}
     \ENDFOR
   \ENDFOR\ \COMMENT{}
 \ENDFOR\ \COMMENT{}
\end{algorithmic}
\end{algorithm}

\section{Our Approach}

The following elementary lemma is the key observation required in order to solve \eq{eq:max3} efficiently:

\begin{lemma}
If the  smallest element of  has the same index as the  smallest element of , then we only need to search through the  smallest values of , and the  smallest values of ; any values `behind' these cannot possibly contain the smallest solution.
\label{main_lemma}
\end{lemma}

This observation is used to construct Algorithm \ref{alg1}. Here we iterate through the indices starting from the smallest values of  and , stopping once both indices are `behind' the minimum value found so far (which we then know is the minimum). This algorithm is demonstrated pictorially in Figure \ref{fig:alg1}.

\begin{algorithm}
  \caption{Find  such that  is minimised}
  \label{alg1}
\begin{algorithmic}[1]
\REQUIRE two vectors  and , and permutation functions  and  that sort them in increasing order (so that  is the smallest element in )
\STATE \textbf{Initialize:} ,
, 
\COMMENT{if , then the smallest element in  has the same index as the  smallest element in }\\
\STATE , 
\IF {}
\STATE , 
\ENDIF
\WHILE{
}\label{line2}
\STATE 
\IF {} \label{if1}
\STATE 
\STATE 
\ENDIF
\IF {}
\STATE 
\ENDIF \label{endif1}
\STATE \COMMENT{repeat Lines \ref{if1}--\ref{endif1}, interchanging  and } \label{line:repeat}
\ENDWHILE\ \COMMENT{this takes \emph{expected time} }
\RETURN 
\end{algorithmic}
\end{algorithm}

\begin{figure*}[ht]
\footnotesize
\begin{center}
\parbox[c]{31.58pt}{\centering\begin{tabular}{c}
                                     \ \\ \includegraphics[scale=0.3]{figures_fast/correspond_min}
                                    \end{tabular}
}
\parbox[c]{309.38pt}
{\centering\begin{tabular}{cccc}
 &  &  & \\
\includegraphics[scale=0.3]{figures_fast/block_step1_min} & \includegraphics[scale=0.3]{figures_fast/block_step2_min} & \includegraphics[scale=0.3]{figures_fast/block_step3_min} & \includegraphics[scale=0.3]{figures_fast/block_step4_min}
\end{tabular}}
\end{center}
\vspace{-3mm}
 \caption{Left: The lists  and  before sorting. Right: Black squares show corresponding elements in the sorted lists ( and ); red squares indicate the elements currently being read ( and ). We can imagine expanding a gray box of size  until it contains an entry; note that the minimum is found during the first step.}
\label{fig:alg1}\end{figure*}

An upper-bound on the expected-case running time of Algorithm \ref{alg1} is given by the following theorem:
\begin{theorem}
 The \emph{expected} running time of Algorithm \ref{alg1} is .
\label{the:alg1}
\end{theorem}
The expected-case running time arises under the assumption that  and  are uncorrelated. The running time approaches  as  and  become increasingly correlated, and it approaches  as  and  become increasingly anti-correlated. Algorithm \ref{alg1} shall be analysed in detail in Section \ref{sec:analysis}.



\begin{algorithm}
 \caption{All-pairs shortest-path problem in expected-case }
 \label{alg:fast}
\begin{algorithmic}[1]
 \REQUIRE a graph 
 \FOR{}
   \FOR{}
     \STATE 
   \ENDFOR
 \ENDFOR
 \FOR{ \COMMENT{}}
   \FOR{}
     \STATE{ permutation that sorts }
     \STATE{ permutation that sorts  \COMMENT{}}
   \ENDFOR\ \COMMENT{}
   \FOR{}
     \FOR{}
       \STATE  \COMMENT{}
       \STATE 
     \ENDFOR
   \ENDFOR\ \COMMENT{}
 \ENDFOR\ \COMMENT{}
\end{algorithmic}
\end{algorithm}


Using Algorithm \ref{alg1}, we can solve the all-pairs shortest path problem in  in the expected-case, for graphs with edge-weights in  with no negative cycles. This is shown in Algorithm \ref{alg:fast}. For dense graphs, our method has worst-case performance , and best-case performance . Our Algorithm requires  memory. Also note that Algoritm \ref{alg1} can exploit sparsity in the graph structure: the algorithm may terminate as soon as it reaches entries with infinite weight -- thus if only  edges are viable, our algorithm has worst-case performance  (meaning that it does not surpass Johnson's Algorithm on sparse graphs \cite{johnson}).

\subsection{Comparison to Existing Approaches}

To our knowledge, the only existing sub-cubic approach is due to \cite{allpsp} (for edge weights taking small integer values); our algorithm shall not surpass this \emph{per se}, as it is not \emph{deterministic} -- it depends on the distribution of the edge weights, and it is certainly possible to adversarially generate graphs yielding worst-case performance. Our algorithm has best-case and worst-case performance of  and  respectively; thus it does not surpass Floyd-Warshall on dense graphs in the worst-case. Unlike Floyd-Warshall it is able to exploit graph sparsity, though it does not have better worst-case performance than Johnson's Algorithm. In short, our algorithm does not improve upon existing solutions in the worst-case, though under reasonable conditions, it has lower complexity than existing algorithms. We shall see in Section \ref{sec:experiments} that our algorithm is significatly faster than Floyd-Warshall in practice, making it a viable solution to real-world all-pairs shortest path problems, despite its lack of worst-case guarantees.

\section{Asymptotic Performance of Algorithm \ref{alg1}}
\label{sec:analysis}

In this section we shall determine the expected-case running time of Algorithm \ref{alg1}. Algorithm \ref{alg1} traverses  and  until it reaches the smallest value of  for which there is some  for which . If  is a random variable representing this smallest value of , then we wish to find .

By representing a permutation of the digits  to  as shown in Figure \ref{fig:perms}, we observe that  is simply the width of the smallest square (expanding from the top left) that includes an element of the permutation (i.e., it includes  and ). 



\begin{figure}
\footnotesize
 \begin{center}
 \begin{tabular}{cccc}
 \includegraphics[scale=0.3]{figures_fast/perm1} & \includegraphics[scale=0.3]{figures_fast/perm2_V} & \includegraphics[scale=0.3]{figures_fast/perm4_V}\\
(a) & (b) & (c)
 \end{tabular}
 \end{center}
\caption{(a) A permutation can be represented as an array, where there is exactly one non-zero entry in each row and column; (b) We want to find the smallest value of  such that the grey box includes a non-zero entry; (c) For the sake of establishing an upper-bound, we consider a shaded region of width  and height .}
\label{fig:perms}
\end{figure}

Simple analysis reveals that the probability of choosing a permutation that does not contain a value inside a square of size  is

This is precisely , where  is the cumulative density function of . It is immediately clear that , which defines the best and worst-case performance of Algorithm \ref{alg1}.

Using the identity , we can write down a formula for the expected value of :

Thus the expected-case running time of our all-pairs shortest path solver (assuming uncorrelated sub-problems) is . We show in the following section that .

\subsection{An Upper Bound on }

Although \eq{eq:runtimek1} precisely defines the running time of Algorithm \ref{alg1}, it is not easy to ascertain the speed improvement it achieves, as the values to which the summations converge for large  are not obvious. Here, we shall try to obtain an upper-bound on their performance, which we shall assess experimentally in Section \ref{sec:experiments}. In doing so we shall prove Theorem \ref{the:alg1}.





\begin{proof}[Proof of Theorem \ref{the:alg1}]
Consider the shaded region in Figure \ref{fig:perms} (c). This region has a width of , and its height  is chosen such that it contains precisely one non-zero value. Let  be a random variable representing the height of the grey region needed in order to include a non-zero entry. We note that

our aim is to find the smallest  such that . The probability that none of the first  samples appear in the shaded region is

Next we observe that if the entries in our  grid do not define a permutation, but we instead choose a \emph{random} entry in each row, then the probability (now for ) becomes

(for simplicity we allow  to take arbitrarily large values). We certainly have that , meaning that  is an upper bound on , and therefore on . Thus we compute the expected value

This is just a geometric progression, which sums to . Thus we need to find  such that

Clearly  will do. Thus we conclude that

\end{proof}

We will show that this upper bound is empirically tight in the following section.

\section{Experiments}
\label{sec:experiments}

\subsection{Performance of Algorithm \ref{alg1}}

For our first experiment, we compare the performance of Algorithm \ref{alg1} to the na\"ive linear time solution. We generate  uniform samples from  to obtain the lists  and .  corresponds to the size of the graph in question. The performance of Algorithm \ref{alg1} is shown in Figure \ref{fig:exp1}; the value reported is simply the value of  upon termination of the algorithm; this is compared to  itself, which is the number of elements read by the na\"ive solution. The upper-bounds we obtained in the previous section are also reported, while the true expected performance (i.e., \eq{eq:runtimek1}). Visually, we find that our upper-bound is empirically very close to the true performance, suggesting that the bound is reasonably tight.

\begin{figure*}
 \begin{center}
  \includegraphics[width=0.5\textwidth]{figures_fast/plots/plot2_start}\end{center}
\caption{Performance of our algorithm and bounds. For , the exact expectation is shown, which appears to precisely match the average performance (over 100 trials). The dotted lines show the upper-bound, which appears to be extremely close to the average performance, indicating that the bound is reasonably tight.}
\label{fig:exp1}
\end{figure*}



\subsection{Performance for Correlated Variables}

The expected-case running time of our algorithm was obtained under the assumption that the variables were uncorrelated, as was the case for the previous experiment. We suggested that we will obtain worse performance in the case of negatively correlated variables, and better performance in the case of positively correlated variables; we will assess these claims in this experiment.

We report the performance for two lists (i.e., for Algorithm \ref{alg1}), whose values are sampled from a 2-dimensional Gaussian, with covariance matrix

meaning that the two lists are correlated with correlation coefficient . Performance is shown in Figure \ref{fig:correlated} for different values of  (, is not shown, as this is the case observed in the previous experiment).

In real graphs,  shall be the correlation coefficient between  and  (which is free over ). Unless  is equal to precisely  for all , , and , we obtain a sub-cubic solution. Whether we observe positive, negative, or zero correlation will depend on the statistics of the graphs in question.

\begin{figure*}
 \begin{center}
  \includegraphics[width=0.5\textwidth]{figures_fast/plots/plot_cc_0-2_start}\includegraphics[width=0.5\textwidth]{figures_fast/plots/plot_cc_0-5_start}\\\includegraphics[width=0.5\textwidth]{figures_fast/plots/plot_cc_-0-2_start}\includegraphics[width=0.5\textwidth]{figures_fast/plots/plot_cc_-0-5_start}\end{center}
\caption{Performance of our algorithm for different correlation coefficients. The top three plots show positive correlation, the bottom three show negative correlation. Correlation coefficients of  and  capture precisely the best and worst-case performance (respectively) of our algorithm.}
\label{fig:correlated}
\end{figure*}

\subsection{Performance of Algorithm \ref{alg:fast}}

Finally, we compare our algorithm to the divide-and-conquer solution of Algorithm \ref{alg:apsp}, and to the popular Floyd-Warshall Algorithm \cite{floyd} on dense graphs in .

We generate dense graphs of size  with edge weights sampled uniformly in . The performance of our algorithm, compared to Algorithm \ref{alg:apsp} and the Floyd-Warshall Algorithm is shown in Figure \ref{fig:compare}. We note that our algorithm is faster than Algorithm \ref{alg:apsp} after only , meaning that its computational overhead is negligible. It is faster than Floyd-Warshall after .

\begin{figure*}
 \begin{center}
  \includegraphics[width=0.5\textwidth]{figures_fast/plots/plot_apsp}\end{center}
\caption{The running time of our algorithm compared to the divide-and-conquer solution of Algorithm \ref{alg:apsp}, and the Floyd-Warshall Algorithm. The average of 10 trials is shown. All algorithms were implemented in Python.}
\label{fig:compare}
\end{figure*}

\subsection{Conclusion}

We have presented an expected-case subcubic solution to the problem of Funny Matrix Multiplication, resulting in an expected-case  solution to the all-pairs shortest path problem. The running time of our method depends on the distribution of edge weights for the graph in question, though we achieve performance at least as good as the expectation under reasonable conditions. Our algorithm is significantly faster than Floyd-Warshall in practice, making it a viable solution to real-world all-pairs shortest path problems.

\subsection*{Acknowledgements}

We would like to thank Pedro Felzenszwalb for alerting us to the link between inference in graphical models and the all-pairs shortest path problem. NICTA is funded by the Australian Government's \emph{Backing Australia's Ability} initiative, and the Australian Research Council's \emph{ICT Centre of Excellence} program.

\begin{thebibliography}{}

\bibitem[Alon et~al., 1997]{allpsp}
Alon, N., Galil, Z., and Margalit, O. (1997).
\newblock On the exponent of the all pairs shortest path problem.
\newblock {\em Journal of Computer and System Sciences}, 54(2):255--262.

\bibitem[Dijkstra, 1959]{Dijkstra59}
Dijkstra, E.~W. (1959).
\newblock A note on two problems in connexion with graphs.
\newblock {\em Numerische Mathematik}, 1(1):269--271.

\bibitem[Floyd, 1962]{floyd}
Floyd, R.~W. (1962).
\newblock Algorithm 97: Shortest path.
\newblock {\em Commun. ACM}, 5(6):345.

\bibitem[Johnson, 1977]{johnson}
Johnson, D.~B. (1977).
\newblock Efficient algorithms for shortest paths in sparse networks.
\newblock {\em J. ACM}, 24(1):1--13.

\bibitem[Kerr, 1970]{Kerr70}
Kerr, L.~R. (1970).
\newblock {\em PhD Thesis}.

\end{thebibliography}


\end{document}
