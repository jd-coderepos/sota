\section{Experimental Evaluation}\label{sec:exp}
\subsection{Implementation Details}\label{subsec:implementdetails}
\PAR{Inference with \method.} Once the method is trained, we detect objects by selecting the maximum responses from the output center heatmap . Since the datasets are annotated with bounding boxes, we need to convert our estimates into this representation. In detail, we apply (after max pooling) a threshold  (e.g.~0.3 for MOT17 and 0.4 for MOT20 in \method) to the center heatmap, thus producing a list of center positions . We extract the object size  associated to each position  in . The set of detections produced by \method\ is denoted as . In parallel, for associating objects through frames (tracking), given the position of an object  at , we can estimate the object position in the current frame by extracting the corresponding displacement estimate  from . Therefore, we can construct a set of \textit{tracked positions} . Finally, we use the Hungarian algorithm~\cite{Kuhn55thehungarian} to match the tracked positions --  and the detection at  -- . The matched \addnote[R2Q4-2]{2}{detections} are used to update the tracked object positions at . The birth and death processes are naturally integrated in \method: detections not associated with any tracked object give birth to new tracks, while unmatched tracks are put to sleep for at most  frames before being discarded. An external Re-ID network is often used in MOT methods~\cite{bergmann2019tracking} to recover tracks in sleep, which is proven unnecessary in our experiment in Sec.~\ref{subsec:ablation}. We also assess inference speed in fps in the testset results either obtained from~\cite{zhang2021bytetrack} or tested under the same GPU setting.
\PAR{Network and Training Parameters.} The input images are resized to  with padding in \method\ and \method-Dual while it is set to  in \methodlite. In \method, the PVT-Encoder has  layers ( in PVT-Lite) for each image feature scale and the corresponding hidden dimension  ( in PVT-Lite).  for the \method\ Decoder with eight attention heads and six layers (four layers in \methodlite). All \method\ models are trained with loss weights ,  and  by the AdamW optimizer~\cite{loshchilov2017decoupled} with learning rate . The training converges at around 50 epochs, applying a learning rate decay of  at the 40th epoch. The entire network is pre-trained on the pedestrian class of COCO~\cite{lin2014microsoft} and then fine-tuned on the respective MOT dataset~\cite{MOT16, MOTChallenge20}.
We also present the results finetuned with extra data like CrowdHuman dataset~\cite{shao2018crowdhuman} (see Sec.~\ref{sec:sota} for details). \newcommand{\gb}{\cellcolor{green!12}}
\newcommand{\ob}{\cellcolor{orange!12}}
\newcommand{\rb}{\cellcolor{red!12}}
\newcommand{\bb}{\cellcolor{black!12}}

\begin{table*}[ht]
    \center
    \caption{Results on MOT17 testset: the left and right halves of the table correspond to public and private detections respectively. The cell background color encodes the amount of extra-training data: green for none, orange for one extra dataset, red for (more than) five extra datasets.  Methods with * are not associated to a publication. The best result within the same training conditions (background color) is \underline{underlined}. The best result among published methods is in \textbf{bold}. Best seen in color.} \label{tab:mot17merged}
    \tabcolsep=0.11cm
    \resizebox{\linewidth}{!}{
    
            \begin{tabular}{ l | c c c c c c c c c c|| c c c c c c c c c c}
            \toprule
            & \multicolumn{10}{c||}{Public Detections} & \multicolumn{10}{c}{Private Detections}\\\midrule
              Method & Data &  MOTA  & MOTP  & IDF1  & MT  & ML  & FP  & FN  & IDS  & FPS & Data &  MOTA  & MOTP  & IDF1  & MT  & ML  & FP  & FN  & IDS  & FPS\\ [0.5ex] 
             \midrule
                MOTDT17 \cite{chen2018real} & \ob\dc{re1}& \ob50.9 & \ob76.6 & \ob52.7 & \ob17.5 & \ob35.7 & \ob24,069 & \ob250,768 & \ob2,474  
                 & \ob \underline{\textbf{18.3}}& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb \\
                 
                *UnsupTrack \cite{karthik2020simple} &  \ob\dc{pt}& \ob61.7  & \ob78.3 & \ob58.1 & \ob27.2 & \ob32.4 & \ob 16,872 & \ob197,632 & \ob 1,864 
                 & \ob 17.5  & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb \\ 

                GMT\_CT \cite{he2021learnable}& \ob\dc{re2}& \ob 61.5 & \bb & \ob {\textbf{66.9}} & \ob 26.3 & \ob 32.1 & \ob \underline{\textbf{14,059}} & \ob 200,655 & \ob \textbf{2,415}  
                 & \bb& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb\\

                
                 
                TrackFormer \cite{meinhardt2021trackformer} &  \ob\dc{ch} & \ob62.5 & \bb & \ob60.7 & \ob 29.8 & \ob{26.9} & \ob14,966 & \ob206,619 & \ob\underline{1,189}  & 
                 \ob 6.8 & \bb & \bb & \bb &  \bb & \bb& \ob \bb & \bb  & \ob \bb& \bb & \bb \\ 

                SiamMOT \cite{shuai2021siammot}& \ob\dc{ch} & \ob 65.9 & \bb & \ob 63.5 & \ob 34.6 & \ob 23.9 & \ob 18,098 & \ob 170,955 & \ob 3,040 
                  & \ob 12.8 & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb \\

                *MOTR \cite{zeng2021motr}& \ob\dc{ch} & \ob 67.4 & \bb & \ob \underline{67.0} & \ob 34.6 & \ob 24.5 & \ob 32,355 & \ob 149,400 & \ob 1,992 
                  & \ob 7.5 & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb \\

                TrackFormer \cite{meinhardt2021trackformer} &  \bb & \bb & \bb &\bb & \bb & \bb & \bb & \bb & \bb  & 
                 \bb& \ob\dc{ch} & \ob 65.0 & \bb &  \ob 63.9 & \ob 45.6 & \ob 13.8 & \ob70,443  & \ob 123,552& \ob 3,528 & \ob6.8 \\ 

                 
                 CenterTrack \cite{zhou2020tracking} & \bb& \bb & \bb & \bb & \bb  & \bb  & \bb & \bb & \bb
                 &\bb  & \ob\dc{ch}& \ob67.8 &  \ob78.4 & \ob{{64.7}}& \ob34.6&  \ob24.6&  \ob\underline{\textbf{18,489}} & \ob160,332 & \ob\underline{\textbf{3,039}} 
                 & \ob \underline{\textbf{17.5}} \\

                TraDeS \cite{wu2021track} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb &  \ob\dc{ch}& \ob69.1 & \bb & \ob63.9 & \ob36.4 & \ob21.5 & \ob20,892 & \ob150,060 & \ob 3,555  
                 & \ob \underline{\textbf{17.5}}\\ 

                PermaTrack \cite{tokmakov2021learning} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb& \ob\dc{ch} & \ob 73.8 & \bb & \ob 68.9 & \ob 43.8  & \ob 17.2  & \ob 28,998 & \ob 114,104  & \ob 3,699
                  & \ob 11.9 \\

                *TransTrack \cite{sun2020transtrack} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb&  \ob\dc{ch}& \ob74.5 & \ob 80.6 & \ob 63.9 & \ob46.8& \ob11.3& \ob28,323 & \ob112,137 & \ob3,663  
                 & \ob 10.0\\
                  \midrule
                
                \method\   &  \ob\dc{ch}  & \ob \underline{\textbf{75.9}}  & \ob \underline{\textbf{81.2}} & \ob 65.9  & \ob \underline{\textbf{49.8}}  & \ob \underline{\textbf{12.1}}  & \ob30,190  & \ob \underline{\textbf{100,999}} & \ob  4,626 
                & \ob 11.7 & \ob \dc{ch} &\ob \underline{\textbf{76.2}} & \ob \underline{\textbf{81.1}} &\ob \underline{\textbf{65.5}} &\ob \underline{\textbf{53.5}}  & \ob \underline{\textbf{7.9}} & 40,101\ob   & \ob \underline{\textbf{88,827}} & \ob 5,394  
                & \ob11.8  \\

               
                 \midrule
                 
                 GSDT \cite{Wang2021_GSDT} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb &  \rb \dc{5d1} & \rb 66.2 & \rb 79.9 & \rb 68.7 & \rb 40.8 & \rb 18.3 & \rb 43,368 & \rb 144,261 & \rb 3,318 
                 & \rb 4.9\\ 

                SOTMOT \cite{zheng2021improving} & \rb \dc{5d1} & \rb {{62.8}} & \bb & \rb \underline{\textbf{ 67.4}} & \rb {{24.4}} & \rb {{33.0}} & \rb \underline{\textbf{6,556}} & \rb {{201,319}} & \rb \underline{\textbf{2,017}}  
                 & \rb \underline{\textbf{16.0}} & \rb \dc{5d1} & \rb 71.0 & \bb & \rb 71.9 &  \rb 42.7  & \rb 15.3 & \rb 39,537 & \rb 118,983 & \rb 5,184   & \rb 16.0\\

                GSDT\_V2 \cite{Wang2021_GSDT} &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb & \bb&\rb\dc{5d1}& \rb73.2 &  \bb &  \rb66.5 & \rb41.7 & \rb17.5 & \rb \textbf{26,397} &	\rb120,666	 & \rb3,891 
                & \rb 4.9 \\ 

                CorrTracker \cite{wang2021multiple}& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb & \rb \dc{5d1} & \rb \underline{\textbf{76.5}} & \bb & \rb 73.6 & \rb 47.6 & \rb {{12.7}} & \rb 29,808 & \rb{ 99,510} & \rb 3,369 
                 & \rb 15.6\\ 

                FairMOT \cite{zhang2020fairmot} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb& \rb \dc{5d1+CH} & \rb 73.7 & \rb 81.3 & \rb 72.3& \rb 43.2 & \rb 17.3 & \rb 27,507 & \rb 117,477 & \rb 3,303 &  \rb \underline{\textbf{25.9}}\\ 

 
                *RelationTrack \cite{yu2021relationtrack} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb& \rb \dc{5d1+CH} & \rb 73.8 & \rb 81.0 & \rb 74.7 & \rb 41.7& \rb 23.2 & \rb 27,999 & \rb 118,623 & \rb \underline{1,374} &  \rb 7.4\\ 

                CSTrack \cite{liang2020rethinking} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb & \rb\dc{5d1+CH}& \rb74.9 &  \rb80.9 & \rb 72.6 & \rb 41.5 & \rb 17.5 & \rb \underline{23,847}	& \rb 114,303 & \rb 3,567 &\rb 15.8\\ 

                MLT \cite{zhang2020multiplex} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb & \rb(\dc{5d1+CH}) & \rb 75.3  & \rb\underline{\textbf{81.7}} & \rb\underline{\textbf{75.5}} & \rb 49.3 & \rb 19.5 & \rb {27,879} & \rb 109,836 & \rb\textbf{1,719}  
                 & \rb 5.9\\ 

                *FUFET \cite{shan2020tracklets} &\bb& \bb & \bb & \bb &\bb&\bb& \bb &\bb & \bb 
                 & \bb & \rb(\dc{5d1+CH})& \rb 76.2 &  \rb81.1 &  \rb68.0 & \rb{{51.1}} &\rb 13.6 &\rb 32,796 &	\rb{98,475} & \rb3,237 
                 & \rb 6.8 \\
                
                  \midrule
                \method\   &  \rb\dc{5d1+CH} & \rb \underline{\textbf{76.0}} & \rb \underline{\textbf{81.4}} & \rb 65.6 & \rb \underline{\textbf{47.3}} & \rb \underline{\textbf{15.3}}  & \rb  28,369  & \rb \underline{\textbf{101,988}} & \rb 4,972   & \rb11.7 & \rb \dc{5d1+CH} & \rb 76.4 & \rb 81.2 &\rb 65.4 &\rb \underline{\textbf{51.7}} & \rb \underline{\textbf{11.6}} & \rb 37,005  & \rb \underline{\textbf{89,712}} & \rb 6,402 
                & \rb10.9
                 \\
                 \midrule
                 
                 TrctrD17 \cite{xu2020train} & \gb \dc{no} & \gb 53.7 & \gb 77.2 & \gb 53.8 & \gb 19.4 & \gb 36.6 & \gb 11,731 & \gb 247,447 & \gb 1,947  
                 & \gb 2.0& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb\\
                 Tracktor \cite{bergmann2019tracking} & \gb \dc{no} & \gb 53.5  & \gb 78.0 & \gb 52.3 &  \gb 19.5 & \gb 36.6 & \gb 12,201 & \gb 248,047 &  \gb 2,072  
                 & \gb 2.0& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb\\
                 
                Tracktor++ \cite{bergmann2019tracking} &\gb\dc{no}&\gb 56.3  & \gb78.8 &\gb 55.1 &\gb 21.1 & \gb 35.3 & \gb {{8,866}} & \gb 235,449 & \gb 1,987 & \gb  2.0 & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb\\
                
                 GSM\_Tracktor \cite{ijcai2020-0074} &\gb\dc{no}& \gb56.4  & \gb77.9 & \gb57.8 & \gb22.2 &\gb 34.5 & \gb14,379 & \gb230,174 & \gb{1,485}  
                 & \gb 8.7 & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb\\
                 
                 TADAM \cite{guo2021online} & \gb\dc{no}& \gb 59.7 & \bb & \gb58.7 & \bb  & \bb  &  \gb \underline{\textbf{9,676}} & \gb 216,029 & \gb 1,930  
                 &\bb &\bb & \bb & \bb & \bb & \bb & \bb  &  \bb & \bb & \bb  & \bb\\

                CenterTrack \cite{zhou2020tracking} & \gb\dc{no}& \gb61.5 & \gb78.9 & \gb59.6 & \gb26.4  & \gb31.9  &  \gb14,076 & \gb200,672 & \gb 2,583 
                 & \gb 17.5  & \bb& \bb &  \bb & \bb& \bb&  \bb&  \bb & \bb & \bb
                 & \bb \\

                 *FUFET \cite{shan2020tracklets} &\gb\dc{no}& \gb 62.0 & \bb & \gb 59.5 & \gb27.8 &\gb 31.5 & \gb15,114 &	\gb196,672 & \gb2,621  
                 & \gb 6.8 &\bb& \bb &  \bb &  \bb & \bb &\bb&\bb &	\bb & \bb
                 & \bb \\

                 ArTIST-C \cite{saleh2021probabilistic}& \gb\dc{no}& \gb62.3 & \bb & \gb 59.7 & \gb29.1 & \gb34.0 & \gb19,611 & \gb191,207 & \gb2,062  
                 & \gb 17.5 & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb 
                 &\bb \\

                 MAT \cite{han2020mat} & \gb\dc{no}& \gb67.1 & \gb\underline{\textbf{80.8}} & \gb\underline{\textbf{69.2}} & \gb{38.9} & \gb26.4 & \gb22,756 & \gb161,547 & \gb\underline{\textbf{1,279}}  
                 &\gb 9.0 & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb\\
                
                MTP \cite{kim2021discriminative} & \gb\dc{no}& \gb 51.5 & \bb & \gb54.9 & \gb20.5  & \gb35.5  &  \gb29,623 & \gb241,618 & \gb 2,563 
                & \gb 20.1 &  \gb\dc{no}& \gb 55.9 & \bb & \gb60.4 & \gb20.5 & \gb36.7  &  \gb\underline{\textbf{8,653}} & \gb238,853 & \gb \underline{\textbf{1,188}}
                & \gb 20.1 \\

                 ChainedTracker \cite{peng2020chained} &\bb &\bb &\bb & \bb & \bb&\bb &\bb &\bb & \bb  & \bb &\gb\dc{no}&  \gb 66.6& \gb 78.2 &  \gb57.4& \gb32.2 & \gb24.2& \gb22,284 &\gb 160,491 & \gb5,529
                 & \gb 6.8\\

                 QDTrack \cite{pang2021quasi}&  \gb\dc{no}& \gb64.6 & \gb79.6 & \gb{65.1} &  \gb32.3 & \gb28.3 & \gb14,103 & \gb18,2998 & \gb2,652  
                 & \gb \underline{\textbf{20.3}} &  \gb\dc{no}& \gb68.7 & \gb79.0 & \gb\underline{\textbf{66.3}} & \gb{{40.6}}  & \gb21.9&  \gb26,589 & \gb14,6643 & \gb 3,378 
                 & \gb \underline{\textbf{20.3}} \\\midrule 

                \method\  & \gb\dc{no} & \gb \underline{\textbf{71.9}} & \gb 80.5 & \gb 64.1 & \gb \underline{\textbf{44.4}}  & \gb \underline{\textbf{18.6}}  & \gb 27,356  & \gb \underline{\textbf{126,860}} & \gb 4,118 
                & \gb 11.9  &  \gb\dc{no} & \gb \underline{\textbf{72.7}} & \gb \underline{\textbf{80.3}} & \gb 64.0  & \gb \underline{\textbf{48.7}}   &\gb \underline{\textbf{14.0}}  & \gb 33,807  & \gb \underline{\textbf{115,542}} & \gb  4,719
                & \gb 11.8\\
                
                
                
            \bottomrule
            \end{tabular}
            }

\end{table*} \subsection{Protocol}
\PAR{Datasets and Detections.} We use the standard split of the MOT17~\cite{MOT16} and MOT20~\cite{MOTChallenge20} datasets and the testset evaluation is obtained by submitting the results to the MOTChallenge website. The MOT17 testset contains 2,355 trajectories distributed in 17,757 frames. MOT20 testset contains 1,501 trajectories within only 4,479 frames, which leads to a much more challenging crowded-scene setting. We evaluate \method\ both under public and private detections. When using public detections, we limit the maximum number of birth candidates at each frame to the number of public detections per frame, as in~\cite{zhou2020tracking,meinhardt2021trackformer}. The selected birth candidates are those closest to the public detections with IOU larger than 0. When using private detections, there are no constraints, and the detections depend only on the network's detection capacity, the use of external detectors, and more importantly, the use of extra training data. For this reason, we regroup the results by the use of extra training datasets as detailed in the following. In addition, we evaluate our \method\ on the KITTI dataset under the autonomous driving setting. KITTI dataset contains annotations of cars and pedestrians in 21 and 29 video sequences in the training and test sets, respectively. For the results of KITTI dataset, we use also \cite{thomas_pandikow_kim_stanley_grieve_2021} as extra data.\begin{table*}[ht]
    \center
    \tabcolsep=0.11cm
    \caption{Results on MOT20 testset: the table is structured following the same principle as Tab.~\ref{tab:mot17merged}. Methods with * are not associated to a publication. The best result within the same training conditions (background color) is \underline{underlined}. The best result among published methods is in \textbf{bold}. Best seen in color.} \label{tab:mot20merge}
    \resizebox{\linewidth}{!}{
            \begin{tabular}{ l | c c c c c c c c c c || c c c c c c c c c c}
            \toprule
            & \multicolumn{10}{c||}{Public Detections} & \multicolumn{10}{c}{Private Detections}\\\midrule
              Method & Data &  MOTA  & MOTP  & IDF1  & MT  & ML  & FP  & FN  & IDS  & FPS  & Data &  MOTA  & MOTP  & IDF1  & MT  & ML  & FP  & FN  & IDS  & FPS  \\\midrule
             
            *UnsupTrack \cite{karthik2020simple} & \ob\dc{pt}& \ob53.6 & \ob{80.1} & \ob{50.6}  & \ob30.3  & \ob25.0  & \ob\underline{6,439} & \ob231,298& \ob\underline{2,178}  & \ob \underline{17.5}&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb &\bb& \bb\\
            
            *TransTrack \cite{sun2020transtrack} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb  & \bb&\bb  & \ob\dc{ch}& \ob64.5 & \ob 80.0 & \ob \underline{59.2} & \ob49.1& \ob13.6& \ob \underline{28,566} & \ob151,377 & \ob3,565
            & \ob 7.2\\ \midrule

             \method   &  \ob\dc{ch} & \ob \underline{\textbf{72.8}} & \ob \underline{\textbf{81.0}}  &\ob \underline{\textbf{57.6}} &\ob \underline{\textbf{65.5}}  & \ob \underline{\textbf{12.1}} & \ob \textbf{28,026}  & \ob \underline{\textbf{110,312}} & \ob \textbf{2,621}
                & \ob \textbf{8.4} &  \ob \dc{ch} & \ob \underline{\textbf{72.9}} & \ob \underline{\textbf{81.0}} & \ob  \textbf{57.7} & \ob \underline{\textbf{66.5}} & \ob \underline{\textbf{11.8}} & \ob \textbf{28,596} & \ob \underline{\textbf{108,982}} & \ob \underline{\textbf{2,625}}  
                & \ob \underline{\textbf{8.7}}  \\
 
            
            \midrule


            CorrTracker \cite{wang2021multiple}& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb& \rb \dc{5d1} & \rb 65.2 & \bb & \rb 69.1 & \rb {66.4} & \rb {8.9} & \rb 79,429 & \rb \underline{\textbf{95,855}} & \rb 5,183
            & \rb 8.5\\ 

            
            GSDT\_V2 \cite{Wang2021_GSDT} &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb& \rb\dc{5d1}& \rb67.1 &  \bb &  \rb67.5 & \rb53.1 & \rb13.2 & \rb{31,507} &\rb135,395	 & \rb3,230
            & \rb 0.9\\ 

            GSDT \cite{Wang2021_GSDT} &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb& \rb\dc{5d1}& \rb67.1 &  \rb{79.1} &  \rb67.5 & \rb53.1 & \rb13.2 & \rb31,913 &	\rb135,409	 & \rb{{3,131}}
            & \rb 0.9\\ 

            SOTMOT \cite{zheng2021improving} &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb & \rb \dc{5d1} & \rb {{68.6}} & \bb & \rb \underline{\textbf{71.4}} & \rb \underline{\textbf{64.9}} & \rb \underline{\textbf{9.7}} & \rb 57,064 & \rb 101,154 & \rb 4,209
            & \rb 8.5\\



            FairMOT \cite{zhang2020fairmot}&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb& \bb& \rb\dc{5d1+CH}& \rb61.8 & \rb78.6 &  \rb\textbf{67.3}  & \rb\underline{\textbf{68.8}}  & \rb\underline{\textbf{7.6}}  & \rb103,440& \rb \underline{\textbf{88,901}} & \rb5,243      & \rb \underline{\textbf{13.2}}\\  CSTrack \cite{liang2020rethinking}&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&  \rb\dc{5d1+CH}& \rb66.6 &  \rb78.8 &  \rb68.6 & \rb50.4 & \rb15.5 &\rb\underline{25,404}&\rb144,358 & \rb3,196
             & \rb 4.5\\ 

            *RelationTrack \cite{yu2021relationtrack} & \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb  & \bb& \rb \dc{5d1+CH} & \rb 67.2 & \rb 79.2 & \rb \underline{70.5} & \rb 62.2& \rb 8.9 & \rb 61,134 & \rb 104,597 & \rb 4,243 &  \rb 2.7\\ \midrule
            
            \method   &  \rb\dc{5d1+CH} & \rb \underline{\textbf{72.4}} &\rb \underline{\textbf{81.2}} & \rb \underline{\textbf{57.9}} &\rb \underline{\textbf{64.2}} & \rb \underline{\textbf{12.3}} & \rb \underline{\textbf{25,121}}   & \rb \underline{\textbf{115,421}} & \rb  \underline{\textbf{2,290}}
                & \rb \underline{\textbf{8.6}} &  \rb \dc{5d1+CH} & \rb \underline{\textbf{72.5}} & \rb \underline{\textbf{81.1}} & \rb 58.1  & \rb 64.7 & \rb 12.2  & \rb {\textbf{25,722}} & \rb  114,310 & \rb \underline{\textbf{2,332}}
                & \rb 8.8  \\
                 \midrule
            




             
            SORT \cite{bewley2016simple}&\gb\dc{no}& \gb42.7  & \gb78.5 & \gb45.1  & \gb16.7  & \gb26.2  &\gb27,521 &	\gb264,694	 & \gb4,470  
            \gb& \gb \underline{\textbf{ 27.7}} &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb& \bb\\

            Tracktor++ \cite{bergmann2019tracking} &\gb\dc{no}& \gb52.6 & \gb\underline{\textbf{79.9}} & \gb{{52.7}}  & \gb29.4  & \gb26.7  & \gb \underline{{\textbf{6,930}}} &\gb236,680 & \gb1,648  
            & \gb 1.2 &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb& \bb\\
            
            ArTIST-T \cite{saleh2021probabilistic}& \gb\dc{no}& \gb53.6 & \bb & \gb 51.0 & \gb31.6 & \gb28.1 & \gb7,765 & \gb230,576 & \gb \underline{\textbf{1,531}} 
            & \gb 1.2& \bb & \bb & \bb &  \bb & \bb & \bb & \bb & \bb & \bb & \bb\\ 

            GNNMatch \cite{papakis2020gcnnmatch} &\gb\dc{no}& \gb54.5& \gb79.4 & \gb49.0 & \gb32.8 & \gb25.5 & \gb9,522 & \gb223,611& \gb2,038 
            & \gb 0.1 &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb & \bb\\
             
             TADAM \cite{guo2021online} & \gb\dc{no}& \gb 56.6 & \bb & \gb51.6 & \bb  & \bb  &  \gb39,407 & \gb 18,2520 & \gb 2,690 
             &\bb &\bb & \bb & \bb & \bb & \bb & \bb  &  \bb & \bb & \bb& \bb\\

             
            MLT \cite{zhang2020multiplex} &\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb&\bb& \bb&\gb\dc{no}& \gb48.9 & \gb78.0 & \gb{{54.6}}  & \gb30.9  & \gb22.1  & \gb\underline{\textbf{45,660}} & \gb216,803 & \gb\underline{\textbf{2,187}}& \gb 3.7 \\\midrule
             
            \method  & \gb\dc{no} & \gb \underline{\textbf{ 67.7}}  & \gb 79.8  & \gb \underline{\textbf{58.9}}   & \gb \underline{\textbf{65.6}}  & \gb \underline{\textbf{11.3}}   & \gb 54,967   & \gb \underline{\textbf{108,376}} & \gb 3,707
                & \gb 8.4  &  \gb\dc{no} & \gb \underline{\textbf{ 67.7}}  & \gb \underline{\textbf{79.8}}   & \gb \underline{\textbf{58.7}} & \gb \underline{\textbf{66.3}} &\gb \underline{\textbf{11.1}}  & \gb  56,435  & \gb \underline{\textbf{107,163}}  & \gb 3,759 
                & \gb \underline{\textbf{8.4}} \\
            
            \bottomrule
            \end{tabular}
            }
\end{table*} \PAR{Extra Training Data.} To fairly compare with the \addnote[R3Q5-1]{1}{state-of-the-art} methods, we denote the extra data used
to train each method, including several pre-prints listed in the MOTChallenge leaderboard, which are marked with * in our result tables\footnote{COCO~\cite{lin2014microsoft} and ImageNet~\cite{imagenet_cvpr09} are not considered as extra data according to the MOTchallenge~\cite{MOT16,MOTChallenge20}.}: \dc{ch} for {CrowdHuman}~\cite{shao2018crowdhuman}, \dc{pt} for {PathTrack}~\cite{8237302}, \dc{re1} for the combination of {Market1501}~\cite{Zheng_2015_ICCV}, {CUHK01} and {CUHK03}~\cite{li2014deepreid} person re-identification datasets, \dc{re2} replaces {CUHK01}~\cite{li2014deepreid} with DukeMTMC~\cite{ristani2016performance}, \dc{5d1} for the use of five extra datasets (ETH~\cite{eth_biwi_00534}, {Caltech Pedestrian}~\cite{Dollar2012PAMI,dollarCVPR09peds}, {CityPersons}~\cite{zhang2017citypersons}, {CUHK-SYS~\cite{xiao2016end}}, and {PRW}~\cite{zheng2017person}), \dc{5d1+CH} is the same as \dc{5d1} plus CroudHuman.  (\dc{5d1+CH}) uses the tracking/detection results of FairMOT~\cite{zhang2020fairmot} trained within the \dc{5d1+CH} setting, and \dc{no} stands for using no extra dataset.
\PAR{Metrics.} Standard MOT metrics such as MOTA (Multiple Object Tracking Accuracy) and MOTP (Multiple Object Tracking Precision)~\cite{bernardin2008evaluating} are used: MOTA is mostly used since it reflects the average tracking performance including the number of FP (False positives, predicted bounding boxes not enclosing any object), FN (False negatives, missing ground-truth objects) and IDS~\cite{li2009learning} (Identities of predicted trajectories switch through time). MOTP evaluates the quality of bounding boxes from successfully tracked objects. Moreover, we also evaluate IDF1~\cite{ristani2016performance} (the ratio of correctly identified detections over the average number of ground-truth objects and predicted tracks), MT (the ratio of ground-truth trajectories that are covered by a track hypothesis more than 80\% of their life span), and ML (less than 20\% of their life span).
\subsection{Testset Results and Discussion}\label{sec:sota}
\PAR{MOT17.} Tab.~\ref{tab:mot17merged} presents the results obtained in the MOT17 testset. The first global remark is that most state-of-the-art methods do not evaluate under both public and private detections, and under different extra-training data settings, while we do. Secondly, \method~sets new state-of-the-art performance compared to other methods, in terms of MOTA, under \dc{CH} and no-extra training data conditions, both for public and private detections. Precisely, the increase of MOTA w.r.t. \addnote[R3Q5-2]{1}{the state-of-the-art} methods is of  and  (both including unpublished methods by now) for the public detection setting under \dc{CH} and no-extra training data, and of  and  for the private detection setting, respectively. The superiority of \method\ is remarkable in most of the metrics. We can also observe that TransCenter trained with no extra-training data outperforms, not only the methods trained with no extra data but also some methods trained with one extra dataset. Similarly, TransCenter trained on \dc{ch} performs better than seven methods trained with five or more extra datasets in the private setting, comparable to the best result in \dc{5d1+ch} (-0.3\% MOTA), showing that \method\ is less data-hungry. Moreover, trained with \dc{5d1+ch}, the performance is further improved while running at around 11 fps. Overall, these results confirm our hypothesis that \method\ with dense detection representations and sparse tracking representations produced by global relevant queries in transformers is a better choice.
\PAR{MOT20.} Tab.~\ref{tab:mot20merge} reports the results obtained in MOT20  testset. \emph{In all settings}, similar to the case in MOT17, \method\ \emph{leads the competition by a large margin compared to all the other methods}. Concretely, \method\ outperforms current methods by +19.2\%/+8.4\% in MOTA with the public/private setting trained with \dc{ch} and +11.1\%/18.8\% without extra data. From the results, another remarkable achievement of \method\ is the significant decrease of FN while keeping a relatively low FP number. This indicates that the dense representation of the detection queries can help effectively detect objects sufficiently and accurately. As for tracking, \method\ maintains low IDS numbers in MOT20 running at around 8 fps in such crowded scenes, thanks to our careful choices of QLN and the \method\ Decoder. Very importantly, to the best of our knowledge, our study is the first to report the results of \emph{all settings} on MOT20, demonstrating the tracking capacity of \method\ even in a densely crowded scenario. The outstanding results of \method\ in MOT20 further show the effectiveness of our design.


\begin{table}[ht]
\centering

\tabcolsep=0.11cm
    \caption{KITTI testset results in MOTA, MOTP, FP, FN, IDS and FPS. Best results are \underline{underlined}.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c l| c c c c c c}
    \toprule
    \midrule
     &Method & MOTA  & MOTP  & FP  & FN  & IDS   & FPS  \\ [0.5ex] 
     \midrule
     \parbox[t]{1mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{Car}}} &   MASS~\cite{karunasekera2019multiple} & 84.6 & 85.4 & 4,145 & 786 & 353 & 100.0  \\
     &  IMMDP~\cite{xiang2015learning}       & 82.8 & 82.8 & 5,300 & 422 & 211 & 5.3  \\
     &  AB3D~\cite{weng20203d}               &83.5  & 85.2 &4,492  &1,060  & \underline{126} & \underline{214.7}  \\
     &  SMAT~\cite{gonzalez2020smat}         & 83.6 & 85.9 & 5,254 & \underline{175} & 198 &10.0 \\
     &  TrackMPNN~\cite{rangesh2021trackmpnn}     & 87.3 & 84.5 & 2,577 & 1,298 & 481 & 20.0\\
     &  CenterTrack~\cite{zhou2020tracking}   & 88.8 & 85.0 & 2,703 & 886 & 254  & 22.2\\
       \cmidrule(){2-8}
     & \method   & 87.3 & 83.8  & 3,189 &  847 & 340  & 18.5 \\
    
    \midrule
     \parbox[t]{1mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Person}}} &  AB3D~\cite{weng20203d}  & 38.9 & 64.6 &  11,744 & 2,135   & 259 &  \underline{214.7}  \\
     &  TrackMPNN~\cite{rangesh2021trackmpnn}  & 52.1 &73.4  &7,705   &  2,758  & 626  &  20.0 \\
     &  CenterTrack~\cite{zhou2020tracking}   & 53.8  & 73,7 &  8,061 &  2,201  & 425 & 22.2   \\
       \cmidrule(){2-8}
     & \method  & 59.1 &  73.2 & 6,889 & 2,142  & 436   &  18.5\\
    \bottomrule
    \end{tabular}
    }
  \label{tab:kitti}
\end{table} \PAR{KITTI.}Additionally, we show the results of \method\ evaluated on the KITTI dataset. \method\ significantly outperforms CenterTrack~\cite{zhou2020tracking} in pedestrian tracking (+5.3\% MOTA) while keeping a close performance in car tracking. However, the KITTI dataset is constructed in an autonomous driving scenario with only up to 15 cars and 30 pedestrians per image but some of the sequences contain no pedestrians. The sparse object locations cannot fully show the capacity of \method~to detect and track densely crowded objects.

\subsection{Efficiency-Accuracy Tradeoff Discussion} \label{sec:speed_acc_tradeoff}
    


\begin{table}[ht]
\centering
\tabcolsep=0.11cm
    \caption{Testset comparisons on the MOT17 and MOT20 among CenterTrack~\cite{zhou2020tracking}, FairMOT~\cite{zhang2020fairmot},
    Trackformer~\cite{meinhardt2021trackformer},
    TransTrack~\cite{sun2020transtrack}, ByteTrack~\cite{zhang2021bytetrack}, MO3TR-PIQ~\cite{zhu2021looking} and our proposed models in number of model parameters (\#params), Inference Memory (IM), Frame Per Second (FPS) and MOTA. Best results are \underline{underlined}. They are grouped according to their encoders or used detections, indicated as Enc/Det. Without extra notation, private detections are used by default.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c c c c c c  c}
    \toprule

     & Model & Enc/Det & \#params (M)  & IM (MB)   &  FPS  & MOTA \0.5ex] 
     \midrule
      1& DETR &Dual& \bb S-S\protect \footnotemark & \bb QLN  & 49.4  & 49.6 &  4,909 & 8,202 & 602 & \underline{2.5} & 42.5 & 26.8 &19,243  &   153,429  &  4,375 & \underline{1.5} \\ 

      2&DETR &Dual& \bb D-D\protect \footnotemark & \bb QLN & 56.6 & 50.1  &3,510 & 7,577 & 678 & 1.7 &67.3  &32.1  & 45,547 & \underline{46,962} & 8,115 &0.9 \\ \vspace{+1mm}3&DETR & Dual & \bb D-D& \bb QLN & \underline{69.2} & \underline{71.9} & \underline{1,202}  & \underline{6,951} & \underline{203} &1.9& \underline{79.4}\protect \footnotemark & \underline{66.9} & \underline{7,697} & 53,987 & \underline{1,680}&1.2 \\ \midrule  
       4& DETR&Dual &D-D& \bb QLN  & 66.8  & 71.2  & 1,074  & 7,757 &  \underline{167}&1.9 &  78.3 & 64.7  & 5517 & 59,447 & 1,832 & 1.2 \\ 

        
        5&DETR& Dual & D-D & \bb QLN  & 65.8  & 70.3  & \underline{1,122} & 7,987  & 176 &1.9 &  78.4& 64.2 & \underline{5,340} &59,288 & 1,798& 1.1 \\ 6&DETR & Dual & D-D& \bb QLN & \underline{69.2} & \underline{71.9} & 1,202  & \underline{6,951} & 203 &1.9& \underline{79.4} & \underline{66.9} & 7,697 & 53,987 & \underline{1,680}&1.2 \\ 

         \midrule
     7&\bb DETR& Single & D-D &QLN & 68.1  & \underline{72.0} &  \underline{580} &  7,922 & \underline{141} & 2.1& 79.8 & 66.9 & \underline{6,955}  & 53,445 & 1,657& 1.2\\ 

     8&\bb PVT& Single & D-D& QLN & \underline{72.3} & 71.4  & 1,116  & \underline{6,156} & 249 & \underline{8.1} & \underline{83.6}  & \underline{75.1} &  14,358 & \underline{34,782}& \underline{1,348} & \underline{6.1}\\ 

        \midrule
      9&\bb PVT-Lite& Single &D-S& QLN  & 69.8 & 71.6  & 2,008  & 5,923  & \underline{252}&\underline{19.4}& 82.9 & 75.5  & \underline{14,857} & 36,510 & 1,181&\underline{12.4} \\ 

      10&\bb PVT & TQSA-Single & D-S & QLN  & \underline{73.8} & \underline{74.1} & \underline{1,302}  & \underline{5,540}  & 258& 12.4&  \underline{83.6}& \underline{75.7} & 15,459 & \underline{34,054} & \underline{1,085} & 8.9\\ 

       \midrule
       11&PVT &\bb TQSA-Dual & D-S & QLN  & \underline{74.6}  & \underline{76.5} &  \underline{892} & 5,879  & \underline{128} &5.6& \underline{84.6} & \underline{78.0} & \underline{13,415} & \underline{33,202} & \underline{921} &4.8\\ 

       12&PVT & \bb TQSA-Single & D-S & QLN  & 73.8 & 74.1 & 1,302  & \underline{5,540}  & 258& \underline{12.4}&  83.6& 75.7 & 15,459 & 34,054 & 1,085 & \underline{8.9}\\ 

    \midrule
    13&PVT & \bb Single & D-S & QLN & 69.4 & 72.5  & 1,756  & 6,335 & \underline{197} & \underline{13.3} & 83.0  &74.7 &  \underline{14,584} & 36,441& 1,321 &\underline{9.8} \\ 14&PVT & \bb TQSA-Single & D-S& QLN & \underline{73.8}  & \underline{74.1} & \underline{1,302} & \underline{5,540} & 258 & 12.4 & \underline{83.6} & \underline{75.7} & 15,459 & \underline{34,054} & \underline{1,085}  & 8.9 \\ 

      \midrule
    15&PVT &  Single & \bb D-D & QLN & 71.1 & 71.7  & \underline{1,274}  & 6,274 & 278 & 9.0 & 82.5  & 75.0 &  \underline{12,686} & 40,003 & 1,216 & 6.3 \\ 16&PVT & TQSA-Single & \bb D-S& QLN & \underline{73.8}  & \underline{74.1} & 1,302 & \underline{5,540} & \underline{258} & \underline{12.4} & \underline{83.6} & \underline{75.7} & 15,459 & \underline{34,054} & \underline{1,085}  & \underline{8.9} \\ 

    \midrule
   17&PVT & TQSA-Dual & D-S & \bb QLN  & \underline{74.6}  & \underline{76.5} &  \underline{892} & 5,879  & \underline{128} &\underline{5.6}& \underline{84.6} & \underline{78.0} & \underline{13,415} & 33,202 & \underline{921} &\underline{4.8}\\ 

   18&PVT & TQSA-Dual & D-S & \bb QLN  &  72.7 & 73.7  &  2,012  & \underline{5,213}  & 181 & 5.5 & 83.9 & 77.4 & 16,252 & \underline{32,402} & 976 & \underline{4.8}\\ 
   
   
  
    \bottomrule
    \end{tabular}
    }
  \label{tab:ablateMOT1720naive}
\end{table*} \begin{table*}[ht]
\centering
\tabcolsep=0.11cm
    \caption{Ablation study of external inference overheads: the results are expressed in MOTA, MOTP, FP, FN, IDS as well as FPS. They are evaluated on both MOT17 and MOT20 validation sets using \method, \methodlite\ and \method-Dual, respectively.}
    \resizebox{.8\linewidth}{!}{
    \begin{tabular}{ c| c c c c c c | c c c c c c c}
    \toprule
    
       &   &  \multicolumn{4}{c}{MOT17}  &  & &  & MOT20  &    & &   \\
    \midrule

     Setting & MOTA  & IDF1  & FP  & FN  & IDS   & FPS   & MOTA  & IDF1  & FP  & FN  & IDS   & FPS  \0.5ex] 
    \midrule
   

    
      + ex-ReID  & 73.7  & 73.5 &  \underline{1,047} & 5,800  & 286 & 4.5 & \underline{83.6} & 75.5 & 15,468 & 34,057 & \underline{1,064} & 2.9 \\
    
     + NMS & \underline{73.8}  & \underline{74.1} &  1,300 & 5,548 & 261& 10.8 &\underline{83.6} & \underline{75.8} & \underline{15,458} & \underline{34,053} & 1,084 & 5.1 \\
      \method\ & \underline{73.8}  & \underline{74.1} & 1,302 & \underline{5,540} & \underline{258} & \underline{12.4} & \underline{83.6} & 75.7& 15,459 & 34,054 & 1,085  & \underline{8.9} \\
     
     \midrule
    
     + ex-ReID  &  \underline{69.8}  & \underline{72.0}  & \underline{2,008} &  \underline{5,922} & \underline{248} & 5.2 & \underline{82.9} & 74.9  & 14,894 & \underline{36,498} & \underline{1,147} & 3.1\\
      + NMS &\underline{69.8}  & 71.6 & 2,009 & 5,923 & 253 & 15.9  &\underline{82.9} & 75.4 & 14,872 & 36,507   &  1,181 & 6.2\\
      \methodlite\ & \underline{69.8} & 71.6 & \underline{2,008} & 5,923 & 252 & \underline{19.4} & \underline{82.9} & \underline{75.5}& \underline{14,857}  & 36,510& 1,181  & \underline{12.4}\\

    \midrule
     

    
     + ex-ReID    & \underline{74.6} & 75.8  & \underline{840}  & 5,882 & 162  & 3.0  & 84.5 & 77.7 & 13,478 & 33,209& \underline{913} & 2.2 \\
    
     + NMS  & \underline{74.6}  & \underline{76.5}  & 891 & \underline{5,879} &  \underline{127}  & 5.1 & \underline{84.6} & 77.9  & \underline{13,413} & 33,203 & 921 & 3.4 \\
      \method-Dual  & \underline{74.6}  & \underline{76.5} &  892 & \underline{5,879} &  128 & \underline{5.6} & \underline{84.6} & \underline{78.0} & 13,415 & \underline{33,202} & 921 & \underline{4.8} \\
     
    \bottomrule
    \end{tabular}
    }
  \label{tab:ablateMOT1720}
\end{table*} \subsection{Ablation Study}\label{subsec:ablation}
In this section, we first experimentally demonstrate the importance of our proposed image-related dense queries with naive DETR to MOT approaches. Then, we justify the effectiveness of our choices of QLN and \method\ Decoder (see illustrations in Fig.~\ref{fig:qln} and Fig.~\ref{fig:transformerdecoder}, respectively), considering the computational efficiency and accuracy. All results are shown in Tab.~\ref{tab:ablateMOT1720naive}. Furthermore, in  Tab.~\ref{tab:ablateMOT1720}, we ablate the impacts of removing the external Re-ID network and the NMS (Non-Maximum Suppression) during inference. Finally, we show an additional ablation of the number of decoder layers in Sec. A of the Supplementary Material. \addnote[R4Q8]{2}{For the ablation, we divide the training sets into a train-validation split, we take the first 50\% of frames as training data and test on the last 25\%. The rest 25\% of frames in the middle of the sequences are thrown to prevent over-fitting.} All the models are pre-trained on CrowdHuman~\cite{shao2018crowdhuman} and tested under the private detection setting.
\PAR{Dense Representations Are Beneficial.} We implemented a naive DETR MOT tracker with its original 100 sparse queries (from learnable embeddings initialized from noise) with the DETR-Encoder and Dual \method\ Decoder (Line 1 in Tab.~\ref{tab:ablateMOT1720naive}). To compare, the same tracker but having dense representations (43,520, i.e.~) is shown in Line 2. From the results shown in Line 1-2 of Tab.~\ref{tab:ablateMOT1720naive}, we see that the limited number of queries (100, by default in~\cite{zhu2020deformable}) is problematic because it is insufficient for detecting and tracking objects, especially in very crowded scenes MOT20  (+106,467 FN, -24.8\% MOTA, compared to Line 2). This indicates that having dense representations is beneficial, especially for handling crowded scenes in MOT. Some visualization examples are shown in Sec.~\ref{subsec:qvcs}.\PAR{Naive Dense, Noisy Dense~v.s.~Image-Related Dense.} One naive way to alleviate the insufficient queries is to greatly increase the number of queries like in Line 2 of Tab.~\ref{tab:ablateMOT1720naive}: we drastically increase the number of queries from 100 to 43,520 (i.e.~), same as the dense output of~TransCenter. Meanwhile, we compare this naive dense queries implementation to a similar one in Line 3 but with image-related dense queries like in \method. Concretely, we obtain the dense queries from QLN  (as described in Fig.~\ref{fig:qln_b_old_trctr_mt_dq}) with the memories output from DETR-Encoder. We note that the image-related implementation~has indeed 14,450 queries (i.e.~the sum of the 1/8, 1/16, 1/32, and 1/64 of the image size), and the up-scale and merge operation (see Fig.\ref{fig:dla})
\footnotetext[8]{100 noise-initialized learnable queries.} \footnotetext[9]{43,520 noise-initialized learnable queries.}\footnotetext[10]{Like other MOT methods, we observe that clipping the box size within the image size for tracking results in MOT20 improves slightly the MOTA performance. To have a fair comparison, all the results in MOT20 are updated with this technique.}forms a dense output having the same number of pixels of 43,520. Therefore, we ensure that the supervisions for the losses are equivalent\footnote{The Gaussian supervision in~TransCenter~for negative examples has values very close to 0, thus similar to the classification loss in Line 2.} for both Line 2 and 3.

The main difference between Line 2 and 3 is the queries: the proposed multi-scale dense detection queries are related to the input image where one query represents one pixel. The benefit of image-related pixel-level queries is well-discussed in Sec.~\ref{sec:methodology}. From the experimental aspect in Tab.~\ref{tab:ablateMOT1720naive}, for the noise-initialized queries in Line 2, despite the manual one-to-one matching during training, increasing the queries naively and drastically tends to predict noisier detections causing much higher FP (compared to Line 3, +2,308 and +37,850 in MOT17 and MOT20, respectively) and thus much under-performed tracking results (-12.6\% and -12.1\% MOTA for MOT17 and MOT20, respectively). Although more sophisticated implementations using sparse noise-initialized queries and Hungarian matching loss like in~\cite{sun2020transtrack} and~\cite{meinhardt2021trackformer} exhibit improved results, \method\ shows both better accuracy and efficiency, as discussed in Sec.~\ref{sec:speed_acc_tradeoff}.

\addnote[R2Q2]{2}{Furthermore, we combine \method(-Dual) with QLN (see Fig.~\ref{fig:qln_a_proposed} in red) where the detection queries are of image size as in QLN but noise-initialized, denoted as \textit{Noise Dense}. We note that since they are noise-initialized (i.e.~unrelated to the image), it makes no sense to discard the detection attention -- DDCA (see Sec.\ref{subsec:dualdecoders}) from \method\ Decoder. The \textit{Noisy Dense} detection queries thus need to correlate with  to extract information from the image  in \method\ Decoder and cannot speed up by removing DDCA like in \method. The result with such design in Line 18 is compared with \textit{\method-Dual} in line 17 having the same structure but with the chosen QLN. We observe that the \textit{Noise Dense} queries produce a decent result thanks to the rest of \method\ designs like image-related sparse queries, \method\ Decoder, and the use of PVT encoder but they tend to have more FP caused by the noise initialization for detection queries, leading to the overall worse performance compared to \method-Dual in MOTA, IDF1, and IDS.}
\PAR{QLN Inputs.}A QLN generates tracking queries  and memories ; detection queries  and memories . Based on the nature of tracking, we argue that  and  should be obtained from information at different time steps since tracking means associating object positions in the adjacent frames. This creates two variants of QLN, namely QLN with  from  and  are from  (Line 4) and inversely, QLN, where  are from  and  are from  (Line 5). From their results, we do not observe a significant difference in terms of performance, indicating both implementations are feasible. Further, we experimentally compare QLN (Line 4) and QLN (Line 6), where the only difference is the number of FFN for outputting . With an extra FFN for , the performance is slightly improved (+2.4\% for MOT17 and +1.1\% for MOT20).
\PAR{Efficient~\method\ Decoder.} As we discuss in Sec.~\ref{subsec:dualdecoders}, the design of~\method\ Decoder can have an important impact on the computational efficiency and accuracy with different variants (Line 11-16). Precisely, comparing Line 11 and 12, we observe indeed that, with dual-decoder handling cross-attention for both detection and tracking, the performance is superior (+0.8\% MOTA for MOT17 and +1.0\% for MOT20) but the inference is slowed down by around 50\%. Balancing the efficiency and accuracy, we argue that \method\ (Line 12) is a better choice. Moreover, by removing the TQSA module (Line 13, 14), we obtain a slight inference speed up (+0.9 fps for MOT17 and MOT20) but at the cost of accuracy (-4.4\% MOTA in MOT17 and -0.6\% in MOT20). Finally, we also study the effect of sparse and dense tracking (Line 15-16), surprisingly, we find that using sparse tracking can help better associate object positions between frames  (-20 IDS in MOT17 and -131 IDS in MOT20) and in a more efficient way (+3.4 fps in MOT17 and +2.6 in MOT20).
\PAR{Efficient PVT-Encoder.} Passing from DETR-Encoder (Line 7) to PVT-Encoder (Line 8) helps get rid of the ResNet-50 feature extractor, which partially speeds up the inference from 2.1 fps to 8.1 in MOT17, and 1.2 to 6.1 fps in MOT20. Moreover, the PVT-Encoder exhibits better results which may be due to the lighter structure that eases the training (+4.2\% MOTA in MOT17 and +3.8\% in MOT20). Similarly, with PVT-Lite, we can speed up +7 fps for MOT17 and +3.5 fps for MOT20, comparing Line 9 and 10 while keeping a competitive performance.

\PAR{External Inference Overheads.} MOT methods like~\cite{bergmann2019tracking} use an external Re-ID network to extract identity features so that we can recover the objects which are temporally suspended by the tracker through appearance similarities. The Re-ID network (paired with a light-weight optical flow estimation network LiteFlowNet~\cite{hui2018liteflownet} pre-trained on KITTI~\cite{Geiger2012CVPR}) is often a ResNet-50~\cite{bergmann2019tracking}, pre-trained on object identities in MOT17 trainset. From Tab.~\ref{tab:ablateMOT1720}, we observe that this external Re-ID does help reduce IDS, especially in crowded scenes but it slows down significantly the inference. To speed up, we replace the external Re-ID features extractor simply by sampled features from memories , with a feature sampler like in QLN (Fig.~\ref{fig:qln_a_proposed}) using positions from detections or tracks at , which is almost costless in terms of calculation and achieves comparable results to external Re-ID.

\addnote[R3Q3-1]{1}{\noindent One of the benefits of using our image-related dense representations is the discard of the one-to-one assignment process during training. Intuitively, no NMS is needed during inference. However, recall from Eq.~\ref{eq:gaussian} that the heatmaps are Gaussian distributions centered at the object centers, a 3x3 max-pooling operation is somehow needed to select the maximum response from each distribution (i.e.~object center). In practice, an NMS is only performed as in~\cite{bergmann2019tracking} within tracked objects. However, from the results shown in Tab.~\ref{tab:ablateMOT1720}, the NMS operation between tracks has little impact on the accuracy but imports important overheads. For this reason, such NMS is discarded from \method, \methodlite, and \method-Dual.}

In summary, \method\ can efficiently perform MOT with sparse tracking queries and dense detection queries operating on the proposed QLN and \method\ Decoder structures, leveraging the PVT-Encoder. The accuracy is further enhanced with the finer multi-scale features from the PVT-Encoder, the sparsity of the tracking queries as well as the chosen designs of the QLN and~\method\ Decoder. Therefore, \method\ shows a better efficiency-accuracy tradeoff compared to naive approaches and existing works.

\begin{figure*}[t!]
\centering
\begin{subfigure}{0.38\textwidth}
  \includegraphics[width=\textwidth]{figs/revision/quali_visu_new/MOT20-04.png}
  \caption{}
  \label{fig:MOT20-04-1-supp}
\end{subfigure}
\centering
\begin{subfigure}{0.47\textwidth}
  \includegraphics[width=\textwidth]{figs/revision/quali_visu_new/MOT20-07.png}
  \caption{}
  \label{fig:MOT20-07-1-supp}
\end{subfigure}\\
\centering
\begin{subfigure}{0.86\textwidth}
  \includegraphics[width=\textwidth]{figs/revision/quali_visu_new/MOT20-06.png}
  \caption{}
  \label{fig:MOT20-06-1-supp}
\end{subfigure}
\centering
\caption{\method\ tracking trajectories visualization of some very crowded scenes in MOT20 under the private detection setting.}\label{fig:quali-supp}
\end{figure*}
\subsection{Qualitative Visualizations in Crowded Scenes}\label{subsec:qvcs}
We report in Fig.~\ref{fig:quali-supp} the qualitative results from some crowded MOT20 sequences, to demonstrate the detection and tracking abilities of~\method\~in the context of crowded scenes. Concretely, we show in Fig.~\ref{fig:quali-supp} the predicted center trajectories and the corresponding object sizes. Fig.~\ref{fig:MOT20-04-1-supp} is extracted from MOT20-04,  Fig.~\ref{fig:MOT20-07-1-supp} from MOT20-07 and Fig.~\ref{fig:MOT20-06-1-supp} from MOT20-06. We observe that \method~manages to keep high recall, even in the context of drastic mutual occlusions, and reliably associate detections across time. To summarize,~\method~exhibits outstanding results on both MOT17  and MOT20  datasets for both public and private detections, and for both with or without extra training data, which indicates that multiple-object center tracking using transformers equipped with dense image-related queries is a promising research direction.
