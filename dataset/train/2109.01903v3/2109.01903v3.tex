\clearpage


\appendix


\section{Pseudocode for WiSE-FT}
\label{sec:pseudo-code}

\begin{algorithm}[H]
\caption{Pytorch pseudocode for WiSE-FT}
\label{alg:code}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codeblue2}{rgb}{0,0,1}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue2},
  stringstyle=\color{mauve},
}
\begin{lstlisting}[language=python]
def wse(model, zeroshot_checkpoint, finetuned_checkpoint, alpha):
    # load state dicts from checkpoints
    theta_0 = torch.load(zeroshot_checkpoint)["state_dict"]
    theta_1 = torch.load(finetuned_checkpoint)["state_dict"]

    # make sure checkpoints are compatible
    assert set(theta_0.keys()) == set(theta_1.keys())

    # interpolate between all weights in the checkpoints
    theta = {
        key: (1-alpha) * theta_0[key] + alpha * theta_1[key]
        for key in theta_0.keys()
    }

    # update the model (in-place) according to the new weights
    model.load_state_dict(theta)

def wise_ft(model, dataset, zeroshot_checkpoint, alpha, hparams):
    # load the zero-shot weights
    theta_0 = torch.load(zeroshot_checkpoint)["state_dict"]
    model.load_state_dict(theta_0)

    # standard fine-tuning
    finetuned_checkpoint = finetune(model, dataset, hparams)

    # perform weight-space ensembling (in-place)
    wse(model, zeroshot_checkpoint, finetuned_checkpoint, alpha)
        
\end{lstlisting}
\end{algorithm}


\section{Mixing coefficient}
\label{sec:appendix_alpha}

Table \ref{tab:alpha} compares the performance of WiSE-FT using a fixed mixing coefficient  with the fixed optimal mixing coefficient. On ImageNet and the five derived distribution shifts, the average performance of the optimal  is  to  percentage points better than that of . 
Due to its simplicity and effectiveness, we recommend using  when no domain knowledge is available. Finding the optimal value of the mixing coefficient for any distribution is an interesting question for future work. Unlike other hyperparameters, no re-training is required to test different , so tuning is relatively cheap.


\section{Additional experiments}
\label{sec:appendix_additional_exps}




This section supplements the results of Section~\ref{sec:results}. First, in Section~\ref{sec:fig1breakdown} we provide a breakdown of Figure~\ref{fig:fig1} for each distribution shift.
Next, in Section~\ref{sec:moreshift} we provide effective robustness scatter plots for six additional distribution shifts, finding WiSE-FT to provide consistent improvements  under distribution shift without any loss in performance on the reference distribution.
Section~\ref{sec:baselines-appendix} compares WiSE-FT with additional alternatives including distillation and CoOp \cite{coop}.
Beyond robustness, Section~\ref{sec:low-data} demonstrates that WiSE-FT can provide accuracy improvements on reference data, with a focus on the low-data regime. Section~\ref{sec:scaling} showcases that the accuracy improvements under distribution shift are not isolated to large models, finding similar trends across scales of pre-training computes. Section \ref{sec:more-models} explores the application of WiSE-FT for additional models such as ALIGN \cite{jia2021scaling}, a ViT-H/14 model  pre-trained on JFT \cite{dosovitskiy2021an} and BASIC \cite{pham2021scaling}. Finally, Section~\ref{sec:beyond} ensembles zero-shot CLIP with an independently trained classifier.

\subsection{Breakdown of CLIP experiments on ImageNet}
\label{sec:fig1breakdown}

\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
\texttt{ViT-B/16}, end-to-end & 0.9 & 0.4 & 1.4 & 0.2 & 0.4 & 2.4 & 0.5 & 0.0 \\
\texttt{ViT-B/16}, linear classifier & 1.8 & 0.6 & 1.2 & 0.1 & 0.2 & 0.6 & 0.1 & 0.2 \\
\texttt{ViT-L/14@336}, end-to-end & 0.3 & 0.0 & 0.9 & 0.3 & 1.0 & 1.1 & 0.5 & 0.1 \\
\texttt{ViT-L/14@336}, linear classifier & 1.6 & 0.6 & 0.2 & 0.0 & 0.0 & 0.0 & 0.0 & 0.4 \\
\bottomrule
\end{tabular}
\caption{\label{tab:alpha}
Difference in performance (percentage points) between WiSE-FT using the optimal mixing coefficient and a fixed value of  for CLIP \texttt{ViT-B/16} and \texttt{ViT-L/14@336}. For each cell in the table, the optimal mixing coefficient  is chosen individually such that the corresponding metric is maximized.
Results for all mixing coefficients are available in Tables \ref{tab:breakdown1} and \ref{tab:breakdown2}. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}

In contrast to Figures \ref{fig:fig1} and \ref{fig:beyondclip}, where our key experimental results for ImageNet and five derived distribution shifts are averaged, we now display the results separately for each distribution shift. Results are provided in Figures~\ref{fig:main_breakdown}, \ref{fig:main_zoomout}. 

To assist in contextualizing the results, the scatter plots we display also show a wide range of machine learning models from a comprehensive testbed of evaluations \cite{taori2020measuring, miller21b}, including:
models trained on  (\textit{standard training}); models trained on additional data and fine-tuned using   
(\textit{trained with more data}); and models trained using various \textit{existing robustness interventions}, e.g. special data augmentation 
\cite{devries2017improved,engstrom2019exploring,geirhos2018imagenet,hendrycks2019augmix} or adversarially robust models \cite{madry2017towards, cohen2019certified,salman2019provably,shafahi2019adversarial}.

Additionally, Tables \ref{tab:breakdown1} and \ref{tab:breakdown2} show the performance of WiSE-FT for various values of the mixing coefficient  on ImageNet and five derived distribution shifts, for CLIP \texttt{ViT-L/14@336} and the \texttt{ViT-B/16} model.

\FloatBarrier



\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{figures/clip_l_breakdown.pdf}
    \caption{A per-dataset breakdown of the key experimental results (Figure~\ref{fig:fig1}). WiSE-FT improves accuracy on ImageNet and five derived distribution shifts.  Standard ImageNet models, models trained with more data, and existing robustness interventions are from the testbed of \citet{taori2020measuring}.}
    \label{fig:main_breakdown}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{figures/clip_l_breakdown_zoomout.pdf}
    \caption{A zoomed-out version of Figure~\ref{fig:main_breakdown}. WiSE-FT improves accuracy on ImageNet and five derived distribution shifts.  Standard ImageNet models, models trained with more data, and existing robustness interventions are from the testbed of \citet{taori2020measuring}.}
    \label{fig:main_zoomout}
\end{figure}
\FloatBarrier

\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
WiSE-FT, end-to-end & & & & & & & &\\
\quad & 76.6 & 70.5 & 89.0 & 60.9 & 68.5 & 77.6 & 73.3 & 74.9 \\
\quad & 78.7 & 72.6 & 89.6 & 62.2 & 69.5 & 79.0 & 74.6 & 76.7 \\
\quad & 80.4 & 74.2 & 89.9 & 63.1 & 70.4 & 79.8 & 75.5 & 78.0 \\
\quad & 81.9 & 75.4 & 90.1 & 63.8 & 71.1 & 80.4 & 76.2 & 79.1 \\
\quad & 83.2 & 76.5 &  \dunderline{1pt}{90.3} & 64.3 & 71.6 & 80.8 & 76.7 & 80.0 \\
\quad & 84.2 & 77.5 &  \dunderline{1pt}{90.3} & 64.6 &  \dunderline{1pt}{72.1} &  \dunderline{1pt}{81.0} & 77.1 & 80.7 \\
\quad & 85.1 & 78.3 &  \dunderline{1pt}{90.3} & 64.9 &  \dunderline{1pt}{72.1} &  \dunderline{1pt}{81.0} & 77.3 & 81.2 \\
\quad & 85.7 & 78.7 & 90.1 &  \dunderline{1pt}{65.0} & 72.0 &  \dunderline{1pt}{81.0} &  \dunderline{1pt}{77.4} & 81.6 \\
\quad & 86.2 & 79.2 & 89.9 &  \dunderline{1pt}{65.0} & 71.9 & 80.7 & 77.3 & 81.8 \\
\quad & 86.6 & 79.4 & 89.6 & 64.9 & 71.6 & 80.6 & 77.2 &  \dunderline{1pt}{81.9} \\
\quad & 86.8 &  \dunderline{1pt}{79.5} & 89.4 & 64.7 & 71.1 & 79.9 & 76.9 & 81.8 \\
\quad & 87.0 & 79.3 & 88.9 & 64.5 & 70.7 & 79.1 & 76.5 & 81.8 \\
\quad &  \dunderline{1pt}{87.1} & 79.2 & 88.5 & 64.1 & 70.1 & 78.2 & 76.0 & 81.5 \\
\quad &  \dunderline{1pt}{87.1} & 79.3 & 87.8 & 63.6 & 69.6 & 77.4 & 75.5 & 81.3 \\
\quad &  \dunderline{1pt}{87.1} & 79.1 & 87.0 & 63.1 & 68.9 & 76.5 & 74.9 & 81.0 \\
\quad & 87.0 & 78.8 & 86.1 & 62.5 & 68.1 & 75.2 & 74.1 & 80.5 \\
\quad & 86.9 & 78.4 & 85.1 & 61.7 & 67.4 & 73.8 & 73.3 & 80.1 \\
\quad & 86.8 & 78.0 & 84.0 & 61.0 & 66.4 & 72.0 & 72.3 & 79.5 \\
\quad & 86.7 & 77.6 & 82.8 & 60.0 & 65.5 & 69.9 & 71.2 & 79.0 \\
\quad & 86.5 & 77.2 & 81.3 & 59.0 & 64.3 & 67.7 & 69.9 & 78.2 \\
\quad & 86.2 & 76.8 & 79.8 & 57.9 & 63.3 & 65.4 &  68.6 & 77.4 \\
\midrule
WiSE-FT, linear classifier & & & & & & & &\\
\quad     & 76.6 & 70.5 & 89.0 & 60.9 & 69.1 & 77.7 & 73.4 & 75.0 \\
\quad     & 77.6 & 71.3 & 89.2 & 61.3 & 69.3 & 78.3 & 73.9 & 75.8 \\
\quad     & 78.4 & 72.1 & 89.4 & 61.7 & 69.6 & 78.8 & 74.3 & 76.3 \\
\quad     & 79.3 & 72.8 & 89.5 & 62.1 & 70.0 & 79.0 & 74.7 & 77.0 \\
\quad     & 80.0 & 73.5 & 89.6 & 62.4 & 70.3 & 79.3 & 75.0 & 77.5 \\
\quad     & 80.8 & 74.1 & 89.7 & 62.6 & 70.5 & 79.5 & 75.3 & 78.0 \\
\quad     & 81.5 & 74.8 & 89.7 & 62.8 &  \dunderline{1pt}{70.7} & 79.5 & 75.5 & 78.5 \\
\quad     & 82.1 & 75.4 &  \dunderline{1pt}{89.8} & 62.9 &  \dunderline{1pt}{70.7} & 79.6 & 75.7 & 78.9 \\
\quad     & 82.7 & 75.8 & 89.7 &  \dunderline{1pt}{63.0} &  \dunderline{1pt}{70.7} & 79.6 & 75.8 & 79.2 \\
\quad     & 83.2 & 76.1 & 89.7 &  \dunderline{1pt}{63.0} &  \dunderline{1pt}{70.7} & 79.6 & 75.8 & 79.5 \\
\quad     & 83.7 & 76.3 & 89.6 &  \dunderline{1pt}{63.0} &  \dunderline{1pt}{70.7} &  \dunderline{1pt}{79.7} &  \dunderline{1pt}{75.9} & 79.8 \\
\quad     & 84.1 & 76.5 & 89.5 & 62.9 & 70.5 & 79.6 & 75.8 & 79.9 \\
\quad     & 84.4 & 76.7 & 89.3 & 62.7 & 70.3 & 79.5 & 75.7 & 80.1 \\
\quad     & 84.7 & 76.8 & 89.1 & 62.6 & 70.1 & 79.4 & 75.6 &  \dunderline{1pt}{80.2} \\
\quad     & 85.0 &  \dunderline{1pt}{76.9} & 88.9 & 62.3 & 69.9 & 79.1 & 75.4 &  \dunderline{1pt}{80.2} \\
\quad     & 85.1 & 76.8 & 88.4 & 61.9 & 69.7 & 78.9 & 75.1 & 80.1 \\
\quad     &  \dunderline{1pt}{85.3} &  \dunderline{1pt}{76.9} & 87.9 & 61.4 & 69.3 & 78.5 & 74.8 & 80.0 \\
\quad     &  \dunderline{1pt}{85.3} & 76.7 & 87.4 & 60.9 & 68.8 & 78.1 & 74.4 & 79.8 \\
\quad     &  \dunderline{1pt}{85.3} & 76.4 & 86.8 & 60.3 & 68.4 & 77.3 & 73.8 & 79.5 \\
\quad     &  \dunderline{1pt}{85.3} & 76.2 & 86.1 & 59.5 & 67.7 & 76.8 & 73.3 & 79.3 \\
\quad     & 85.2 & 75.8 & 85.3 & 58.7 & 67.2 & 76.1 & 72.6 & 78.9 \\

\bottomrule
\end{tabular}

\caption{\label{tab:breakdown1}
WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient . Results shown for CLIP \texttt{ViT-L/14@336}. Note that  corresponds to the zero-shot model, while  corresponds to standard fine-tuning. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}


\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
WiSE-FT, end-to-end & & & & & & & &\\
\quad & 68.3 & 61.9 & 77.6 & 48.2 & 53.0 & 49.8 &              
     58.1 & 63.2 \\
\quad & 70.7 & 64.0 & 78.6 & 49.6 & 54.5 & 51.5 &              
     59.6 & 65.2 \\
\quad & 72.9 & 65.7 & 79.4 & 50.8 & 55.7 & 52.5 &              
     60.8 & 66.8 \\
\quad & 74.8 & 67.2 & 79.9 & 51.7 & 56.6 & 53.5 &              
     61.8 & 68.3 \\
\quad & 76.4 & 68.7 &  \dunderline{1pt}{80.1} & 52.5 & 57.1 & 54.2 &              
     62.5 & 69.5 \\
\quad & 77.8 & 69.9 &  \dunderline{1pt}{80.1} & 53.1 & 57.4 &  \dunderline{1pt}{54.6} &              
     63.0 & 70.4 \\
\quad & 78.9 & 70.6 &  \dunderline{1pt}{80.1} & 53.6 & 57.5 &  \dunderline{1pt}{54.6} &              
     63.3 & 71.1 \\
\quad & 79.7 & 71.5 & 79.9 & 53.9 & 57.6 & 54.3 &              
     63.4 & 71.5 \\
\quad & 80.5 & 72.1 & 79.6 &  \dunderline{1pt}{54.1} &  \dunderline{1pt}{57.7} & 53.8 &  \dunderline{1pt}{63.5} & 72.0 \\
\quad & 81.2 & 72.4 & 79.3 & 54.0 & 57.5 & 53.2 &              
     63.3 & 72.2 \\
\quad & 81.7 & 72.8 & 78.7 & 53.9 & 57.3 & 52.2 &              
     63.0 &  \dunderline{1pt}{72.3} \\
\quad & 82.1 & 73.0 & 78.0 & 53.8 & 56.6 & 51.4 &              
     62.6 &  \dunderline{1pt}{72.3} \\
\quad & 82.4 & 72.9 & 77.2 & 53.4 & 56.2 & 50.0 &              
     61.9 & 72.2 \\
\quad &  \dunderline{1pt}{82.6} & 73.1 & 76.3 & 53.0 & 55.5 & 48.9 &              
     61.4 & 72.0 \\
\quad &  \dunderline{1pt}{82.6} &  \dunderline{1pt}{73.2} & 75.2 & 52.4 & 55.0 & 47.4 &              
     60.6 & 71.6 \\
\quad &  \dunderline{1pt}{82.6} & 73.1 & 73.9 & 51.8 & 54.3 & 46.0 &              
     59.8 & 71.2 \\
\quad & 82.5 & 72.8 & 72.7 & 51.0 & 53.5 & 44.6 &              
     58.9 & 70.7 \\
\quad & 82.3 & 72.4 & 71.1 & 50.0 & 52.7 & 42.9 &              
     57.8 & 70.0 \\
\quad & 82.1 & 72.0 & 69.5 & 48.9 & 51.7 & 40.9 &              
     56.6 & 69.3 \\
\quad & 81.7 & 71.5 & 67.7 & 47.6 & 50.7 & 38.8 &              
     55.3 & 68.5 \\
\quad & 81.3 & 70.9 & 65.6 & 46.3 & 49.6 & 36.7 &              
     53.8 & 67.5 \\
\midrule
WiSE-FT, linear classifier & & & & & & & &\\
\quad     & 68.4 & 62.6 & 77.6 & 48.2 & 53.8 & 50.0 & 58.4 & 63.4 \\
\quad     & 69.9 & 63.7 & 77.9 & 48.9 & 54.2 & 50.6 & 59.1 & 64.5 \\
\quad     & 71.3 & 64.8 & 78.2 & 49.5 & 54.7 & 51.0 & 59.6 & 65.5 \\
\quad     & 72.5 & 65.8 &  \dunderline{1pt}{78.4} & 50.0 & 55.1 & 51.1 & 60.1 & 66.3 \\
\quad     & 73.6 & 66.6 &  \dunderline{1pt}{78.4} & 50.5 & 55.3 & 51.5 & 60.5 & 67.0 \\
\quad     & 74.7 & 67.4 &  \dunderline{1pt}{78.4} & 50.8 & 55.3 &  \dunderline{1pt}{51.8} & 60.7 & 67.7 \\
\quad     & 75.6 & 68.0 & 78.3 & 51.1 & 55.4 & 51.7 & 60.9 & 68.2 \\
\quad     & 76.4 & 68.8 & 78.2 &  \dunderline{1pt}{51.3} &  \dunderline{1pt}{55.5} & 51.6 &  \dunderline{1pt}{61.1} & 68.8 \\
\quad     & 77.1 & 69.0 & 77.8 &  \dunderline{1pt}{51.3} &  \dunderline{1pt}{55.5} & 51.4 & 61.0 & 69.0 \\
\quad     & 77.7 & 69.4 & 77.6 &  \dunderline{1pt}{51.3} & 55.4 & 51.3 & 61.0 & 69.3 \\
\quad     & 78.2 & 69.9 & 77.2 & 51.2 & 55.3 & 51.2 & 61.0 & 69.6 \\
\quad     & 78.6 & 70.1 & 76.7 & 51.0 & 55.0 & 50.9 & 60.7 & 69.7 \\
\quad     & 79.0 & 70.2 & 76.1 & 50.8 & 54.7 & 50.5 & 60.5 &  \dunderline{1pt}{69.8} \\
\quad     & 79.3 & 70.4 & 75.7 & 50.4 & 54.5 & 50.1 & 60.2 &  \dunderline{1pt}{69.8} \\
\quad     & 79.6 & 70.4 & 75.2 & 50.1 & 54.2 & 49.9 & 60.0 &  \dunderline{1pt}{69.8} \\
\quad     & 79.7 & 70.4 & 74.6 & 49.7 & 53.9 & 49.5 & 59.6 & 69.7 \\
\quad     & 79.8 &  \dunderline{1pt}{70.5} & 73.9 & 49.3 & 53.6 & 49.0 & 59.3 & 69.5 \\
\quad     & 79.9 & 70.4 & 73.2 & 48.7 & 53.3 & 48.6 & 58.8 & 69.3 \\
\quad     &  \dunderline{1pt}{80.0} & 70.3 & 72.4 & 48.1 & 52.8 & 47.8 & 58.3 & 69.2 \\
\quad     & 79.9 & 70.1 & 71.7 & 47.5 & 52.6 & 46.9 & 57.8 & 68.8 \\
\quad     & 79.9 & 69.8 & 70.8 & 46.9 & 52.1 & 46.4 & 57.2 & 68.6 \\
\bottomrule
\end{tabular}
\caption{\label{tab:breakdown2}
WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient . Results shown for CLIP \texttt{ViT-B/16}. Note that  corresponds to the zero-shot model, while  corresponds to standard fine-tuning. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}



\FloatBarrier
\clearpage


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig_more_shifts.pdf}
    \caption{WiSE-FT improves accuracy under distribution shift relative to standard fine-tuning on ImageNet-Vid-Robust, YTBB-Robust \cite{vidrobust}, CIFAR-10.1 \cite{pmlr-v97-recht19a}, CIFAR-10.2 \cite{lu2020harder}, WILDS-FMoW \cite{wilds2021, christie2018functional}, and WILDS-iWildCam \cite{wilds2021, beery2021iwildcam}.} \label{fig:fig_more_shifts}
\end{figure*}


\subsection{Robustness on additional distribution shifts}
\label{sec:moreshift}

Figure~\ref{fig:fig_more_shifts} displays the effective robustness scatter plots for the six additional distribution shifts discussed in Section~\ref{sec:results} (analogous results provided in Table~\ref{tab:moreshifts}).

Concretely, we consider: (i) ImageNet-Vid-Robust and YTBB-Robust, datasets with distribution shift induced by temporal perturbations in videos \cite{vidrobust};
(ii) CIFAR-10.1 \cite{pmlr-v97-recht19a} and CIFAR-10.2 \cite{lu2020harder}, reproductions of the popular image classification dataset CIFAR-10 \cite{krizhevsky2009learning} with a distribution shift;
(iii) WILDS-FMoW, a satellite image recognition task where the test set has a geographic and temporal distribution shift \cite{wilds2021, christie2018functional};
(iv) WILDS-iWildCam, a wildlife recognition task where the test set has a geographic distribution shift \cite{wilds2021, beery2021iwildcam}.






\begin{table*}
\setlength\tabcolsep{5pt}
\small
\begin{center}

\begin{tabular}{lrrrrr}
\toprule

{} &  Zero-shot &  Fine-tuned &  WiSE-FT,  &  WiSE-FT, optimal \\
\midrule
ImageNet-Vid-Robust  (pm-0)           &       95.9 &        86.5 &                  95.5 &               96.5 \\
YTBBRobust (pm-0)                   &       95.8 &        66.5 &                  89.7 &  96.0 \\
CIFAR-10.1 (top-1) &       92.5 &        95.9 &                  97.6 &               98.0 \\
CIFAR-10.2 (top-1) &       88.8 &        91.3 &                  93.4 &                   94.4 \\
WILDS-FMoW: ID test (accuracy)                &       28.0 &        73.3 &                  73.0 &           74.8 \\
WILDS-FMoW: OOD worst region accuracy       &       23.8 &        46.0 &                  49.5 &                49.7 \\
WILDS-iWildCam: ID test macro F1       &       15.1 &        52.1 &                  55.8 & 55.8 \\
WILDS-iWildCam: OOD test macro F1     &       15.5 &        39.9 &                  46.1 &                 46.4 \\
\bottomrule
\end{tabular}
\caption{\label{tab:moreshifts}
WiSE-FT improves results on ImageNet-Vid-Robust, YTBB-Robust \cite{vidrobust}, CIFAR-10.1 \cite{pmlr-v97-recht19a}, CIFAR-10.2 \cite{lu2020harder}, WILDS-FMoW \cite{wilds2021, christie2018functional}, and WILDS-iWildCam \cite{wilds2021, beery2021iwildcam}. Reported numbers are percentages. This is the corresponding table for Figure~\ref{fig:fig_more_shifts}. This table displays results for fine-tuning only a linear classifier for ImageNet-Vid-Robust and YTBBRobust and end-to-end fine-tuning for the remainder. }
\end{center}
\end{table*}

\subsection{Comparison with alternative methods}
\label{sec:baselines-appendix}

We now extend Section~\ref{sec:results} and compare WiSE-FT to additional methods of fine-tuning. We begin with contrasting the weight-space and output-space ensemble.
Next, we show the that varying the decay parameter of an exponential moving average also moves along the curve produced by WiSE-FT.
Finally, we compare with additional methods when fine-tuning only a linear classifier including distillation and various forms
of regularization.

\subsubsection{Output-space ensembles}

 Figure~\ref{fig:ose} compares the weight-space ensemble
   with the output-space ensemble . 
  Both exhibit a favorable trend, though the output-space ensemble requires twice as much compute. Section~\ref{sec:ntk} further explores the relation between the weight-space and output-space ensemble.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ose.pdf}
    \caption{Comparing the weight-space ensemble  with the output-space ensemble  when fine-tuning end-to-end with learning rate . Note that the output-space ensemble requires 2x compute.} \label{fig:ose}
\end{figure*}

\FloatBarrier
\clearpage

\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/newema.pdf}
    \caption{Results for the debiased variant of EMA described in Appendix~\ref{sec:ema}. EMA improves accuracy on both ImageNet and on the distribution shifts, and further applying WiSE-FT to EMA solutions can improve robustness.
    The solutions with no EMA, decay 0.99, and decay 0.999 are overlapping in the plot, as are the solutions with decay 0.99999 and 0.999999.
    }\label{fig:ema0}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/rebuttal.pdf}
    \caption{Results for the variant of EMA biased towards the initialization, described in Appendix~\ref{sec:ema}.
    Varying the EMA decay  moves
    along the curve produced by WiSE-FT. Applying WiSE-FT to EMA solutions moves further along the curve
    produced by WiSE-FT.
    }\label{fig:ema1}
\end{figure*}

\subsubsection{Comparison to exponential moving averages}\label{sec:ema}

Weight-averaging along the trajectory can improve the performance of models.
For instance, \citet{szegedy2016rethinking} use a running average of the model parameters for their Inception-v2 model.
The exponential moving average (EMA) is a standard technique for keeping a running average of model parameters and
is implemented in libraries such as Optax~\cite{optax2020github} and Pytorch ImageNet Models~\cite{rw2019timm}.

This section explores two variants of EMA for model parameters . The first variant is a debiased EMA, where debiasing is done as in \citet{kingma2014adam} (Algorithm 1).
For each iteration  let  be the model parameters at step  and let  be the EMA at step . For , , otherwise  where  is a decay hyperparameter. 
The final debiased EMA is given by . Results for various decay hyperparameters are illustrated by Figure~\ref{fig:ema0}.

Next, we explore a variant of EMA that is biased towards the initialization . As before, . However  is now initialized to be , instead of zeros. Moreover, at the end of fine-tuning we use the biased estimate . Results for this variant are illustrated by Figure~\ref{fig:ema1}.

Section~\ref{sec:main_results} (Figure~\ref{fig:hparams}) showed that decreasing learning rate, training epochs, or early stopping leads to solutions that lie below the curve produced by WiSE-FT.
On the other hand, using an exponential moving average (EMA) and varying the EMA decay  can move along or slightly outside or along the curve produced
by WiSE-FT.
For instance, solutions using the second EMA variant follow the WiSE-FT curve.
Indeed, applying WiSE-FT with mixing coefficient  to the debiased EMA variant exactly recovers the second EMA variant described above.
Moreover, further applying WiSE-FT to EMA solutions (i.e., interpolating the weights of the zero-shot model with the EMA solution)
can lead to additional robustness. We also evaluate EMA along the fine-tuning trajectory,
finding improved performance under distribution shift for the variant biased towards the initialization.
For the debiased EMA, each model along the trajectory is debiased by .
As shown in Figures~\ref{fig:ema0},\ref{fig:ema1}, evaluations along the trajectory underperform solutions generated by applying WiSE-FT.


\subsubsection{Additional comparisons when fine-tuning a linear classifier}
\label{sec:subsec_additional}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/linear_baselines.pdf}
    \caption{Accuracy on the reference and shifted distributions of WiSE-FT and the alternatives described in Section~\ref{sec:subsec_additional}.}
    \label{fig:baselines}
\end{figure}

We compare against several additional alternatives when fine-tuning only a linear classifier. As this setting is computationally cheaper compared to end-to-end, it allows for comprehensive experimentation. Many of the examined approaches exhibit a concave trend in effective robustness plots, although WiSE-FT matches methods requiring more compute or offers better performance (Figure~\ref{fig:baselines}).

\paragraph{Random interpolation.} This method uses either the zero-shot or fine-tuned linear classifier depending on a (biased) coin flip.
For hyperparameter  outputs are computed as  where  is a Bernoulli random variable.
For this method and all others with a hyperparameter  we evaluate models for .

\paragraph{Ensembling softmax outputs.} Instead of ensembling in weight space, this method combines softmax probabilities assigned by the zero-shot and fine-tuned linear classifier.
Concretely, for hyperparameter  outputs are computed as .
This method performs comparably to weight-space ensembling but requires slightly more compute.

\paragraph{Linear classifier with various regularizers.} We explore fine-tuning linear classifiers with four regularization strategies: no regularization, weight decay, L1 regularization, and label smoothing \cite{muller2019does}. Linear classifiers are trained with mini-batch optimization, using the AdamW optimizer \cite{loshchilov2018decoupled, paszke2019pytorch} with a cosine-annealing learning rate schedule \cite{loshchilov2016sgdr}.
This method is significantly faster and less memory-intensive than the L-BFGS implementation used by \citet{radford2021learning} at ImageNet scale with similar accuracy.
Additional details on hyperparameters and more analyses are provided in Appendix~\ref{sec:moreclf}. 

Two variants of this method are shown in Figure~\ref{fig:baselines}, one for which the the linear classifier is initialized randomly and another for which the linear classifier is initialized with the zero-shot weights (denoted \emph{warmstart}). If the convex problem is solved then the initialization does not play a role. However we are using mini-batch optimization and, in certain cases, terminating training before an optimum is reached. 


\FloatBarrier

\paragraph{Distillation.} Network distillation \cite{hinton2015distilling} trains one network to match the outputs of another.
We use this technique to fine-tune while matching the outputs of the zero-shot model with weights .
For a hyperparameter  and cross-entropy loss  we fine-tune  according to the minimization  objective


\paragraph{Regularization towards zero-shot.} We train a linear classifier with an additional regularization term which penalizes movement from the zero-shot classifier's weights.
For a hyperparameter  we add the regularization term  where  is the linear classifier being fine-tuned.
In most cases this method performs slightly worse than distillation. 






Finally, Figure~\ref{fig:coop} and Table \ref{tab:coop} demonstrate that WiSE-FT achieves better accuracy than the recently proposed CoOp method \cite{coop} on ImageNet and four derived distribution shifts.
Instead of fine-tuning network parameters, CoOp instead learns continuous embedding for the language prompts. We note that CoOp and WiSE-FT could be used in conjunction in future work. We compare with the \texttt{ViT-B/16} section in Table 7 of \citet{coop}. For comparison we use the same CLIP model as CoOp and also train only on 16 images per class. When end-to-end fine-tuning we use 10 epochs and learning rate . 

\begin{figure*}
    \centering
    \includegraphics[width=.9\textwidth]{figures/coop.pdf}
    \caption{Comparing WiSE-FT with CoOp \cite{coop}. Both methods fine-tune the \texttt{ViT-B/16} CLIP model on 16 examples per class of ImageNet.}
    \label{fig:coop}
\end{figure*}

\begin{table*}[]
    \centering
\begin{tabular}{lccccc}
\toprule
{} &  ImageNet (IN) &  INV2 &  IN-R &  IN-A &  IN Sketch \\
\midrule
CoOp \cite{coop} &     71.73 &       64.56 &       75.28 &       49.93 &            47.89 \\
WiSE-FT (linear classifiere, )   &     73.02 &       65.19 &       77.63 &       49.81 &            49.09 \\
WiSE-FT (end-to-end, )  &     72.38 &       65.29 &       78.47 &       51.07 &            49.72 \\
\bottomrule
\end{tabular}
    \caption{Comparing WiSE-FT with CoOp \cite{coop}. Both methods fine-tune the \texttt{ViT-B/16} CLIP model on 16 examples per class of ImageNet. Also see Figure~\ref{fig:coop}.}
    \label{tab:coop}
\end{table*}

\subsection{Changes in data augmentation}\label{sec:dataaug}

In the majority of our experiments we follow \citet{radford2021learning} in using minimal data augmentation. However,
Figure~\hyperlink{fig:hparamsaug}{14} recreates Figure~\ref{fig:hparams} with the default ImageNet train augmentation used in PyTorch ImageNet Models \cite{rw2019timm}, which includes random cropping, horizontal flipping and color jitter. As shown in Figure~\hyperlink{fig:hparamsaug}{14}, we find similar trends with this stronger data augmentation. Further investigating the effect of data augmentation remains an interesting direction for future work.

\begin{figure*}
\begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=\textwidth]{figures/more_hparams_ta_timm.pdf}};
    \draw (3., 0) node[right,scale=0.9,inner sep=0pt,outer sep=0pt,text width = 6.3cm] {Figure 14.
    The robustness of fine-tuned models varies substantially under even small changes in hyperparameters.
    Applying WiSE-FT addresses this brittleness and can remove the trade-off between accuracy on the reference and shifted distributions.
    Results shown for CLIP \texttt{ViT-B/16} fine-tuned with cosine-annealing learning rate schedule and ImageNet data augmentation from Pytorch ImageNet Models \cite{rw2019timm}.
    };
\end{tikzpicture}
\captionlistentry{}
\hypertarget{fig:hparamsaug}{}
\end{figure*}

\subsection{Accuracy improvements on reference datasets}
\label{sec:low-data}
Beyond robustness, Figure~\ref{fig:1d} demonstrates that WiSE-FT can provide accuracy improvements on ImageNet and a number of datasets considered by \citet{kornblith2019better}: CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, Describable Textures \cite{dtd}, Food-101 \cite{food101}, SUN397 \cite{sun397}, and Stanford Cars \cite{cars}.
This is surprising as standard fine-tuning optimizes for low error on the reference distribution.
Figure~\ref{fig:1d} supplements Table~\ref{tab:id_gains} by providing accuracy information for all mixing coefficients .

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/1d.pdf}
    \caption{The accuracy of WiSE-FT (end-to-end) with mixing coefficient  on ImageNet and a number of datasets considered by \citet{kornblith2019better}: CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, Describable Textures \cite{dtd}, Food-101 \cite{food101}, SUN397 \cite{sun397}, and Stanford Cars \cite{cars}.}
    \label{fig:1d}
\end{figure*}

In many application-specific scenarios, only a small amount of data is available for fine-tuning.
Accordingly, we examine the performance of WiSE-FT when only  examples per class are used for fine-tuning on the seven aforementioned datasets (). In contrast with Figure~\ref{fig:1d}, we now fine-tune only the linear classifier allowing for comprehensive experiments. 
Average results are shown in Figure~\ref{fig:figntrain_avg}, while Figures~\ref{fig:figntrain} and \ref{fig:figntrainv2} provide a breakdown for all datasets.


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/fig_ntrain_average.pdf}
    \caption{WiSE-FT can improve accuracy over the linear classifier and zero-shot model in the low data regime. On the -axis we consider  examples per class for fine-tuning. On the -axis we display accuracy improvements of WiSE-FT averaged over seven datasets \cite{deng2009imagenet,krizhevsky2009learning,dtd,food101,sun397,cars}. For , the zero-shot model outperforms the fine-tuned linear classifier, and ensembles closer to the zero-shot model (small ) yield high performance. When more data is available, the reverse is true, and higher values of  improve performance. Figures \ref{fig:figntrain} and \ref{fig:figntrainv2} display a breakdown for all datasets.}
    \label{fig:figntrain_avg}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/fig_ntrain_4cols_v4.pdf}
    \caption{WiSE-FT improves accuracy over the linear classifier and zero-shot model in the low data regime. On the -axis we consider  examples per class and the full training set. On the -axis we consider the accuracy improvement of WiSE-FT over the \textbf{(top)} zero-shot model, \textbf{(middle)} fine-tuned linear classifier, and \textbf{(bottom)} best of the zero-shot and fine-tuned linear classifier.}
    \label{fig:figntrain}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig_ntrain_4cols_v5.pdf}
    \caption{WiSE-FT improves accuracy over the linear classifier and zero-shot model in the low data regime. On the -axis we consider  examples per class and the full training set. On the -axis we consider the accuracy improvement of WiSE-FT over the \textbf{(top)} zero-shot model, \textbf{(middle)} fine-tuned linear classifier, and \textbf{(bottom)} best of the zero-shot and fine-tuned linear classifier.}
    \label{fig:figntrainv2}
\end{figure*}

\FloatBarrier
\clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_scale_avg.pdf}
    \caption{WiSE-FT provides benefits for all CLIP models. Accuracy under distribution shift can be improved relative to the linear classifier with less than  percentage points (pp) loss in accuracy on the reference distribution, across orders of magnitude of training compute. The CLIP model \texttt{RN50x64} requires the most GPU hours to train.}
    \label{fig:fig_scale}
    
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=.85\textwidth]{figures/clip_m_breakdown.pdf}
    \caption{WiSE-FT improves accuracy on the reference and shifted distributions for numerous distribution shifts with a smaller CLIP \texttt{ViT-B/16} model.}
    \label{fig:b16}
\end{figure}


\subsection{Robustness across scales of pre-training compute}
\label{sec:scaling}

The strong correlation between standard test accuracy and accuracy under distribution shift holds from low to high performing models.
This offers the opportunity to explore robustness for smaller, easy to run models.
Our exploration began with the lowest accuracy CLIP models and similar trends held at scale.
Figure~\ref{fig:fig_scale} shows improved accuracy under distribution shift with minimal loss on reference performance across orders of magnitude of pre-training compute with WiSE-FT when fine-tuning a linear classifier.
Moreover, in Figure~\ref{fig:b16} we recreate the experimental results for ImageNet and five associated distribution shifts with a smaller CLIP \texttt{ViT-B/16} model, finding similar trends. Recall that unless otherwise mentioned our experiments use the larger CLIP model (\texttt{ViT-L/14@336px}).

\FloatBarrier

\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
CLIP \texttt{ViT-B/16} \cite{radford2021learning} &  &  &  &  &  &  &  &  \\
\quad Zero-shot & 68.3 & 61.9 & 77.6 & 48.2 & 53.0 & 49.8 & 58.1 & 63.2 \\
\quad Standard fine-tuning & 81.3 & 70.9 & 65.6 & 46.3 & 49.6 & 36.7 & 53.8 & 67.5 \\
\quad WiSE-FT () & 81.7 & 72.8 & 78.7 & 53.9 & 57.3 & 52.2 & 63.0 &  72.3 \\
\quad WiSE-FT (opt. ) & 82.6 & 73.2 & 80.1 & 54.1 & 57.7 & 54.6 & 63.5 &  72.3 \\
CLIP \texttt{ViT-L/14@336px} \cite{radford2021learning} &  &  &  &  &  &  &  &  \\
\quad Zero-shot & 76.6 & 70.5 & 89.0 & 60.9 & 68.5 & 77.6 & 73.3 & 74.9 \\
\quad Standard fine-tuning & 86.2 & 76.8 & 79.8 & 57.9 & 63.3 & 65.4 &  68.6 & 77.4 \\
\quad WiSE-FT () & 86.8 &  79.5 & 89.4 & 64.7 & 71.1 & 79.9 & 76.9 & 81.8 \\
\quad WiSE-FT (opt. ) & 87.1 & 79.5 & 90.3 & 65.0 & 72.1 & 81.0 & 77.4 &  81.9 \\
ALIGN \cite{jia2021scaling} & & & & & & & & \\
\quad Zero-shot & 76.4 & 70.1 & 92.1 & 67.9 & 67.2 & 75.9 & 74.6 & 75.5 \\
\quad Standard fine-tuning & 88.2 & 80.1 & 88.5 & 69.1 & 61.0 & 76.3 & 75.0 & 81.6 \\
\quad WiSE-FT () & 86.3 & 79.2 & 93.0 & 71.1 & 67.8 & 81.0 & 78.4 & 82.3 \\
\quad WiSE-FT (opt. ) & 88.3 & 80.4 & 93.3 & 71.1 & 68.6 & 81.0 & 78.4 &  82.8 \\
JFT pre-trained \texttt{ViT-H} \cite{dosovitskiy2021an} &  &  &  &  &  &  &  &  \\
\quad Zero-shot & 72.9 & 66.1 & 85.9 & 57.0 & 59.2 & 58.4 & 65.3 & 69.1 \\
\quad Standard fine-tuning & 85.4 & 77.6 & 84.9 & 62.8 & 63.1 & 60.8 & 69.8 & 77.6 \\
\quad WiSE-FT () & 82.9 & 75.4 &  89.3 & 63.8 & 65.8 & 66.2 & 72.1 & 77.5 \\
\quad WiSE-FT (opt. ) & 85.4 & 77.8 & 89.3 & 64.5 & 66.0 & 66.6 & 72.5 &  78.6 \\
BASIC-M \cite{pham2021scaling} &  &  &  &  &  &  &  &  \\
\quad Zero-shot & 81.4 & 74.1 & 90.6 & 67.4 & 73.5 & 66.7 & 74.5 & 78.0 \\
\quad Standard fine-tuning & 86.2 & 77.8 & 84.9 & 64.3 & 75.3 & 63.7 & 73.2 & 79.7 \\
\quad WiSE-FT () & 85.6 & 78.5 & 90.2 & 68.6 & 78.0 & 71.1 & 77.3 & 81.4 \\
\quad WiSE-FT (opt. ) & 86.2 & 78.6 & 91.1 & 68.8 & 78.0 & 71.4 & 77.4 &  81.4 \\
BASIC-L \cite{pham2021scaling} &  &  &  &  &  &  &  &  \\
\quad Zero-shot & 85.6 & 80.5 & 95.7 & 76.2 & 82.3 & 85.7 & 84.1 & 84.8 \\
\quad Standard fine-tuning & 87.5 & 79.8 & 84.3 & 68.0 & 77.4 & 72.1 & 76.3 & 81.9 \\
\quad WiSE-FT () & 87.9 & 81.6 & 94.5 & 73.6 & 84.1 & 83.2 & 83.4 & 85.7 \\
\quad WiSE-FT (opt. ) & \textbf{87.9} & \textbf{82.1} & \textbf{96.0} & \textbf{76.5} & \textbf{84.9} & \textbf{86.5} & \textbf{85.0} &  \textbf{86.2} \\
\bottomrule
\end{tabular}
\caption{\label{tab:merged}
WiSE-FT accuracy on ImageNet and derived distribution shifts for various models fine-tuned end-to-end. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts. For optimal , we choose the single mixing coefficient that maximizes the column.
}
\end{center}
\end{table*}


\begin{figure*}
    \centering
    \includegraphics[width=.9\textwidth]{figures/align_paper_setup.pdf}
    \caption{WiSE-FT applied to ALIGN \cite{jia2021scaling}. We also show the effect of varying the L2 regularization strength for linear classifier fine-tuning.}
    \label{fig:align}
\end{figure*}

\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (reference) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     reference, shifts\\
\midrule
WiSE-FT, end-to-end & & & & & & & &\\
\quad  & 76.4 & 70.1 & 92.1 & 67.9 & 67.2 & 75.9 & 74.6 & 75.5 \\
\quad  & 77.9 & 71.6 & 92.5 & 68.5 & 67.8 & 76.9 & 75.5 & 76.7 \\
\quad  & 79.2 & 73.0 & 92.7 & 69.0 & 68.2 & 77.9 & 76.2 & 77.7 \\
\quad  & 80.5 & 74.3 & 92.9 & 69.5 & 68.5 & 78.6 & 76.8 & 78.7 \\
\quad  & 81.6 & 75.4 & 93.0 & 70.0 &  \dunderline{1pt}{68.6} & 79.2 & 77.2 & 79.4 \\
\quad  & 82.7 & 76.3 & 93.2 & 70.3 &  \dunderline{1pt}{68.6} & 79.8 & 77.6 & 80.2 \\
\quad  & 83.5 & 77.1 & 93.2 & 70.5 &  \dunderline{1pt}{68.6} & 80.1 & 77.9 & 80.7 \\
\quad  & 84.4 & 77.8 &  \dunderline{1pt}{93.3} & 70.7 &  \dunderline{1pt}{68.6} & 80.3 & 78.1 & 81.2 \\
\quad  & 85.2 & 78.3 &  \dunderline{1pt}{93.3} & 70.8 & 68.3 & 80.6 & 78.3 & 81.8 \\
\quad  & 85.8 & 78.8 & 93.2 & 71.0 & 68.1 & 80.8 &  \dunderline{1pt}{78.4} & 82.1 \\
\quad  & 86.3 & 79.2 & 93.0 &  \dunderline{1pt}{71.1} & 67.8 &  \dunderline{1pt}{81.0} &  \dunderline{1pt}{78.4} & 82.3 \\
\quad  & 86.7 & 79.6 & 92.8 &  \dunderline{1pt}{71.1} & 67.3 &  \dunderline{1pt}{81.0} &  \dunderline{1pt}{78.4} & 82.6 \\
\quad  & 87.1 & 79.7 & 92.6 &  \dunderline{1pt}{71.1} & 66.8 & 80.8 & 78.2 & 82.7 \\
\quad  & 87.5 & 80.0 & 92.3 & 71.0 & 66.3 & 80.6 & 78.0 &  \dunderline{1pt}{82.8} \\
\quad  & 87.7 & 80.2 & 92.0 & 70.9 & 65.8 & 80.4 & 77.9 &  \dunderline{1pt}{82.8} \\
\quad  & 87.9 &  \dunderline{1pt}{80.4} & 91.5 & 70.7 & 65.1 & 79.9 & 77.5 & 82.7 \\
\quad  & 88.0 & 80.3 & 91.1 & 70.5 & 64.3 & 79.4 & 77.1 & 82.5 \\
\quad  & 88.2 &  \dunderline{1pt}{80.4} & 90.5 & 70.2 & 63.5 & 78.9 & 76.7 & 82.5 \\
\quad  &  \dunderline{1pt}{88.3} &  \dunderline{1pt}{80.4} & 89.9 & 69.9 & 62.8 & 78.2 & 76.2 & 82.2 \\
\quad  &  \dunderline{1pt}{88.3} & 80.3 & 89.2 & 69.5 & 61.8 & 77.3 & 75.6 & 81.9 \\
\quad  & 88.2 & 80.1 & 88.5 & 69.1 & 61.0 & 76.3 & 75.0 & 81.6 \\\midrule
WiSE-FT, linear classifier & & & & & & & &\\
\quad  & 76.4 & 70.1 & 92.1 & 68.0 & 67.2 & 75.8 & 74.6 & 75.5 \\
\quad  & 77.5 & 71.1 & 92.3 & 68.3 & 67.4 & 76.3 & 75.1 & 76.3 \\
\quad  & 78.6 & 72.0 & 92.3 & 68.6 & 67.6 & 76.5 & 75.4 & 77.0 \\
\quad  & 79.5 & 73.0 &  \dunderline{1pt}{92.4} & 69.0 & 67.7 & 76.9 & 75.8 & 77.7 \\
\quad  & 80.3 & 73.5 &  \dunderline{1pt}{92.4} & 69.1 &  \dunderline{1pt}{67.8} & 77.3 & 76.0 & 78.2 \\
\quad  & 81.1 & 74.2 &  \dunderline{1pt}{92.4} &  \dunderline{1pt}{69.2} &  \dunderline{1pt}{67.8} & 77.3 & 76.2 & 78.7 \\
\quad  & 81.8 & 74.6 &  \dunderline{1pt}{92.4} &  \dunderline{1pt}{69.2} &  \dunderline{1pt}{67.8} & 77.5 & 76.3 & 79.0 \\
\quad  & 82.4 & 75.1 &  \dunderline{1pt}{92.4} & 69.1 &  \dunderline{1pt}{67.8} & 77.6 &  \dunderline{1pt}{76.4} & 79.4 \\
\quad  & 82.9 & 75.5 & 92.2 & 69.0 & 67.7 &  \dunderline{1pt}{77.8} &  \dunderline{1pt}{76.4} & 79.7 \\
\quad  & 83.4 & 75.8 & 92.2 & 68.9 & 67.4 & 77.7 &  \dunderline{1pt}{76.4} & 79.9 \\
\quad  & 83.7 & 76.1 & 91.9 & 68.8 & 67.3 & 77.6 & 76.3 & 80.0 \\
\quad  & 84.1 & 76.0 & 91.8 & 68.6 & 67.1 & 77.4 & 76.2 &  \dunderline{1pt}{80.2} \\
\quad  & 84.5 & 76.3 & 91.6 & 68.5 & 66.8 & 77.0 & 76.0 &  \dunderline{1pt}{80.2} \\
\quad  & 84.7 & 76.4 & 91.3 & 68.2 & 66.4 & 76.9 & 75.8 &  \dunderline{1pt}{80.2} \\
\quad  & 84.9 & 76.4 & 91.0 & 68.0 & 66.2 & 76.5 & 75.6 &  \dunderline{1pt}{80.2} \\
\quad  & 85.1 & 76.4 & 90.6 & 67.6 & 65.9 & 76.2 & 75.3 &  \dunderline{1pt}{80.2} \\
\quad  &  \dunderline{1pt}{85.2} & 76.4 & 90.2 & 67.3 & 65.5 & 75.9 & 75.1 &  \dunderline{1pt}{80.2} \\
\quad  &  \dunderline{1pt}{85.2} &  \dunderline{1pt}{76.5} & 89.7 & 66.8 & 65.0 & 75.3 & 74.7 & 80.0 \\
\quad  &  \dunderline{1pt}{85.2} & 76.3 & 89.2 & 66.3 & 64.4 & 74.9 & 74.2 & 79.7 \\
\quad  &  \dunderline{1pt}{85.2} & 76.0 & 88.6 & 65.7 & 63.8 & 74.4 & 73.7 & 79.5 \\
\quad  & 85.1 & 75.7 & 87.8 & 65.1 & 63.2 & 73.7 & 73.1 & 79.1 \\
\bottomrule

\end{tabular}
\caption{\label{tab:align}
 WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient . Results shown for ALIGN, fine-tuned end-to-end (top) and with a linear classifier (bottom). Note that  corresponds to the zero-shot model, while  corresponds to standard fine-tuning. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}


\subsection{WiSE-FT and additional models}
\label{sec:more-models}

Table \ref{tab:merged} summarizes the results for the main models we study, CLIP, ALIGN, BASIC and a ViT model pre-trained on JFT. Details are provided in the subsequent sections.

\subsubsection{ALIGN}
\label{sec:align}

In addition to CLIP, we show WiSE-FT to be effective for an additional zero-shot model, ALIGN \cite{jia2021scaling}.  Results are shown in Figure \ref{fig:align} and Table \ref{tab:align}. End-to-end fine-tuning is performed using AdamW, which we found to perform slightly better than SGD + momentum. The model is fine-tuned for 40,000 steps with a batch size of 512, a maximum learning rate of , and weight decay of 0.1. The learning rate schedule consisted of 500 steps of linear warmup followed by cosine decay. The linear classifier is trained using L-BFGS and no label smoothing. All models are evaluated on  pixel crops obtained by taking the central 87.5\% square region of the test set images. For end-to-end fine-tuning, we take  pixel Inception-style random crops from the original ImageNet images during training; for linear classifier training, we use the same preprocessing as at evaluation time. The weights of the zero-shot model are calibrated using temperature scaling on the ImageNet training set before performing WiSE-FT.


\FloatBarrier
\clearpage

\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\textwidth]{figures/vit_h_jft.pdf}
    \caption{WiSE-FT applied to \texttt{ViT-H/14} \cite{dosovitskiy2021an} pre-trained on JFT. We also show the effect of varying the L2 regularization strength for linear classifier fine-tuning.}
    \label{fig:jft}
\end{figure*}


\subsubsection{JFT pre-training}
\label{sec:JFT}


\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
WiSE-FT, edn-to-end & & & & & & & &\\
\quad  & 72.9 & 66.1 & 85.9 & 57.0 & 59.2 & 58.4 & 65.3 & 69.1 \\
\quad  & 74.1 & 67.3 & 86.4 & 57.9 & 60.4 & 59.9 & 66.4 & 70.2 \\
\quad  & 75.3 & 68.3 & 86.9 & 58.8 & 61.2 & 60.8 & 67.2 & 71.2 \\
\quad  & 76.5 & 69.5 & 87.4 & 59.7 & 62.2 & 61.7 & 68.1 & 72.3 \\
\quad  & 77.5 & 70.8 & 87.9 & 60.5 & 63.0 & 62.8 & 69.0 & 73.2 \\
\quad  & 78.5 & 71.6 & 88.3 & 61.2 & 63.8 & 63.5 & 69.7 & 74.1 \\
\quad  & 79.6 & 72.5 & 88.6 & 61.8 & 64.4 & 64.3 & 70.3 & 74.9 \\
\quad  & 80.6 & 73.5 & 88.9 & 62.3 & 64.9 & 64.8 & 70.9 & 75.8 \\
\quad  & 81.5 & 74.1 & 89.1 & 62.8 & 65.3 & 65.4 & 71.3 & 76.4 \\
\quad  & 82.2 & 74.8 & 89.2 & 63.3 & 65.6 & 65.8 & 71.7 & 77.0 \\
\quad  & 82.9 & 75.4 &  \dunderline{1pt}{89.3} & 63.8 & 65.8 & 66.2 & 72.1 & 77.5 \\
\quad  & 83.4 & 75.9 &  \dunderline{1pt}{89.3} & 64.0 &  \dunderline{1pt}{66.0} & 66.3 & 72.3 & 77.8 \\
\quad  & 83.9 & 76.4 &  \dunderline{1pt}{89.3} & 64.3 &  \dunderline{1pt}{66.0} &  \dunderline{1pt}{66.6} &  \dunderline{1pt}{72.5} & 78.2 \\
\quad  & 84.3 & 76.8 & 89.1 &  \dunderline{1pt}{64.5} & 65.9 & 66.4 &  \dunderline{1pt}{72.5} & 78.4 \\
\quad  & 84.7 & 77.1 & 88.9 &  \dunderline{1pt}{64.5} & 65.8 & 66.0 &  \dunderline{1pt}{72.5} &  \dunderline{1pt}{78.6} \\
\quad  & 84.9 & 77.4 & 88.5 &  \dunderline{1pt}{64.5} & 65.6 & 65.3 & 72.3 &  \dunderline{1pt}{78.6} \\
\quad  & 85.2 & 77.6 & 88.1 & 64.4 & 65.2 & 64.8 & 72.0 &  \dunderline{1pt}{78.6} \\
\quad  & 85.3 &  \dunderline{1pt}{77.8} & 87.5 & 64.1 & 64.7 & 63.8 & 71.6 & 78.4 \\
\quad  &  \dunderline{1pt}{85.4} &  \dunderline{1pt}{77.8} & 86.8 & 63.7 & 64.4 & 63.2 & 71.2 & 78.3 \\
\quad  &  \dunderline{1pt}{85.4} &  \dunderline{1pt}{77.8} & 85.9 & 63.3 & 63.9 & 62.2 & 70.6 & 78.0 \\
\quad  &  \dunderline{1pt}{85.4} & 77.6 & 84.9 & 62.8 & 63.1 & 60.8 & 69.8 & 77.6 \\\midrule
WiSE-FT, linear classifier & & & & & & & &\\
\quad  & 72.9 & 66.1 & 85.9 & 57.0 & 59.2 & 58.4 & 65.3 & 69.1 \\
\quad  & 74.0 & 67.3 & 86.3 & 57.5 & 60.3 & 59.2 & 66.1 & 70.0 \\
\quad  & 75.1 & 68.3 & 86.7 & 58.1 & 61.2 & 60.1 & 66.9 & 71.0 \\
\quad  & 76.1 & 69.1 & 87.0 & 58.5 & 61.8 & 60.8 & 67.4 & 71.8 \\
\quad  & 77.1 & 70.0 & 87.3 & 59.0 & 62.4 & 61.1 & 68.0 & 72.5 \\
\quad  & 78.0 & 71.0 & 87.5 & 59.5 & 63.0 & 61.6 & 68.5 & 73.2 \\
\quad  & 78.8 & 71.7 & 87.7 & 59.8 & 63.3 & 61.9 & 68.9 & 73.8 \\
\quad  & 79.6 & 72.2 & 87.8 & 60.1 & 63.6 & 62.2 & 69.2 & 74.4 \\
\quad  & 80.3 & 72.9 & 87.9 & 60.4 & 63.6 & 62.3 & 69.4 & 74.8 \\
\quad  & 80.9 & 73.4 &  \dunderline{1pt}{88.0} & 60.5 & 63.8 &  \dunderline{1pt}{62.5} & 69.6 & 75.2 \\
\quad  & 81.5 & 73.8 &  \dunderline{1pt}{88.0} & 60.7 &  \dunderline{1pt}{63.9} &  \dunderline{1pt}{62.5} &  \dunderline{1pt}{69.8} & 75.7 \\
\quad  & 81.9 & 74.1 &  \dunderline{1pt}{88.0} &  \dunderline{1pt}{60.8} & 63.7 &  \dunderline{1pt}{62.5} &  \dunderline{1pt}{69.8} & 75.8 \\
\quad  & 82.4 & 74.4 & 87.9 &  \dunderline{1pt}{60.8} & 63.5 & 62.4 &  \dunderline{1pt}{69.8} & 76.1 \\
\quad  & 82.8 & 74.7 & 87.8 & 60.7 & 63.2 & 62.3 & 69.7 & 76.2 \\
\quad  & 83.1 & 75.0 & 87.6 & 60.7 & 63.0 & 62.0 & 69.7 & 76.4 \\
\quad  & 83.4 & 75.2 & 87.4 & 60.5 & 62.7 & 61.8 & 69.5 &  \dunderline{1pt}{76.5} \\
\quad  & 83.6 &  \dunderline{1pt}{75.4} & 87.1 & 60.2 & 62.4 & 61.4 & 69.3 & 76.4 \\
\quad  & 83.7 &  \dunderline{1pt}{75.4} & 86.7 & 59.8 & 61.9 & 60.7 & 68.9 & 76.3 \\
\quad  & 83.9 &  \dunderline{1pt}{75.4} & 86.3 & 59.4 & 61.4 & 60.3 & 68.6 & 76.2 \\
\quad  &  \dunderline{1pt}{84.0} & 75.3 & 85.7 & 58.9 & 61.0 & 59.4 & 68.1 & 76.0 \\
\quad  &  \dunderline{1pt}{84.0} & 75.1 & 85.1 & 58.3 & 60.4 & 58.8 & 67.5 & 75.8 \\
\bottomrule
\end{tabular}
\caption{\label{tab:jft}
WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient . Results shown for \texttt{ViT-H/14} pre-trained on JFT-300M, fine-tuned end-to-end (top) and with a linear classifier (bottom). Note that  corresponds to the zero-shot model, while  corresponds to standard fine-tuning. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}


We also investigate whether WiSE-FT can provide gains for models trained using a standard image classification objective on the JFT-300M dataset \cite{sun2017revisiting}. Results are shown in Figure \ref{fig:jft} and Table \ref{tab:jft}. For 973/1000 ImageNet classes, we were able to manually identify a corresponding class from the 18K classes in JFT. We use this mapping between ImageNet and JFT classes to obtain zero-shot ImageNet weights from the final layer weights of the pre-trained \texttt{ViT-H/14} model from Dosovitskiy et al.~\cite{dosovitskiy2021an}. We also train a linear classifier on the fixed penultimate layer of the same \texttt{ViT-H/14} model using L-BFGS without label smoothing with softmax cross-entropy loss, and fine-tune end-to-end using AdamW with maximum learning rate  and weight decay 0.1 for 20k iterations at batch size 512 with sigmoid cross-entropy loss. As for CLIP models, our learning rate schedule consists of 500 steps of linear warmup followed by cosine decay. All ViT-H/14 models are trained and evaluated on  pixel images. For fair evaluation, we prevent fine-tuned solutions from predicting the 27 classes with no plausible corresponding JFT class at all points on the WiSE-FT curve but still include these points in the denominator when computing accuracy. 

\FloatBarrier

\subsubsection{BASIC}
\label{sec:basic}

We apply WiSE-FT to BASIC \cite{pham2021scaling}, fine-tuning both the image and text encoder with a contrastive loss on half of the ImageNet training data, as in \citet{pham2021scaling}. Results are shown in Figure \ref{fig:basicbreakdown} and Tables \ref{tab:basic_m} and \ref{tab:basic_l}.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/basic_breakdown.pdf}
    \caption{WiSE-FT improves accuracy relative to the fine-tuned model on ImageNet and five derived distribution shifts for BASIC-L \cite{pham2021scaling} using ImageNet class names to construct the zero-shot classifier.}
    \label{fig:basicbreakdown}
\end{figure}


\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
 & 81.4 & 74.1 & 90.6 & 67.4 & 73.5 & 66.7 & 74.5 &    
                78.0 \\         
 & 82.2 & 75.0 & 90.8 & 67.9 & 74.6 & 67.8 & 75.2 &    
                78.7 \\         
 & 82.8 & 75.9 & 90.9 & 68.2 & 75.4 & 68.5 & 75.8 &    
                79.3 \\         
 & 83.3 & 76.4 & 91.0 & 68.4 & 76.2 & 69.3 & 76.3 &    
                79.8 \\         
 & 83.8 & 76.8 & 91.0 & 68.6 & 76.9 & 70.0 & 76.7 &    
                80.2 \\         
 & 84.1 & 77.1 &  \dunderline{1pt}{91.1} & 68.7 & 77.4 & 70.5 & 77.0 &    
                80.5 \\         
 & 84.5 & 77.4 & 91.0 &  \dunderline{1pt}{68.8} & 77.7 & 70.8 & 77.1 &    
                80.8 \\         
 & 84.9 & 77.9 & 90.8 &  \dunderline{1pt}{68.8} & 77.8 & 71.3 & 77.3 &    
                81.1 \\         
 & 85.2 & 78.1 & 90.7 & 68.7 & 77.9 & 71.3 & 77.3 &    
                81.2 \\         
 & 85.4 & 78.3 & 90.5 & 68.7 &  \dunderline{1pt}{78.0} &  \dunderline{1pt}{71.4} &  \dunderline{1pt}{77.4} &  \dunderline{1pt}{81.4} \\         
 & 85.6 & 78.5 & 90.2 & 68.6 &  \dunderline{1pt}{78.0} & 71.1 & 77.3 &  \dunderline{1pt}{81.4} \\         
 & 85.8 & 78.5 & 89.9 & 68.4 &  \dunderline{1pt}{78.0} & 70.6 & 77.1 &  \dunderline{1pt}{81.4} \\         
 & 85.9 & 78.4 & 89.5 & 68.1 &  \dunderline{1pt}{78.0} & 70.5 & 76.9 &  \dunderline{1pt}{81.4} \\         
 & 86.0 & 78.5 & 89.1 & 67.7 & 77.8 & 70.3 & 76.7 &    
                81.3 \\         
 & 86.1 & 78.5 & 88.8 & 67.3 & 77.6 & 69.7 & 76.4 &    
                81.2 \\         
 &  \dunderline{1pt}{86.2} &  \dunderline{1pt}{78.6} & 88.4 & 67.0 & 77.3 & 69.2 & 76.1 &    
                81.2 \\         
 &  \dunderline{1pt}{86.2} & 78.5 & 87.8 & 66.6 & 77.1 & 68.3 & 75.7 &    
                81.0 \\         
 &  \dunderline{1pt}{86.2} & 78.5 & 87.2 & 66.0 & 76.7 & 67.5 & 75.2 &    
                80.7 \\         
 &  \dunderline{1pt}{86.2} & 78.4 & 86.5 & 65.5 & 76.2 & 66.4 & 74.6 &    
                80.4 \\         
 &  \dunderline{1pt}{86.2} & 78.2 & 85.7 & 65.0 & 75.8 & 65.3 & 74.0 &    
                80.1 \\         
 &  \dunderline{1pt}{86.2} & 77.8 & 84.9 & 64.3 & 75.3 & 63.7 & 73.2 &    
                79.7 \\                  
\bottomrule
\end{tabular}
\caption{\label{tab:basic_m}
WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient . Results shown for BASIC-M using ImageNet class names. Note that  corresponds to the zero-shot model, while  corresponds to standard fine-tuning. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}


\begin{table*}
\setlength\tabcolsep{5.1pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (ref.) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     ref., shifts\\
\midrule
 & 85.6 & 80.5 & 95.7 & 76.2 & 82.3 & 85.7 & 84.1 & 84.8 \\
 & 86.4 & 81.2 & 95.8 &  \dunderline{1pt}{76.5} & 83.6 & 86.0 & 84.6 & 85.5 \\
 & 86.9 & 81.7 &  \dunderline{1pt}{96.0} &  \dunderline{1pt}{76.5} & 84.3 &  \dunderline{1pt}{86.5} &  \dunderline{1pt}{85.0} & 86.0 \\
 & 87.3 & 81.9 &  \dunderline{1pt}{96.0} & 76.4 & 84.6 & 86.3 &  \dunderline{1pt}{85.0} &  \dunderline{1pt}{86.2} \\
 & 87.5 &  \dunderline{1pt}{82.1} & 95.9 & 76.1 & 84.8 & 86.1 &  \dunderline{1pt}{85.0} &  \dunderline{1pt}{86.2} \\
 & 87.6 &  \dunderline{1pt}{82.1} & 95.7 & 75.8 &  \dunderline{1pt}{84.9} & 86.0 & 84.9 &  \dunderline{1pt}{86.2} \\
 & 87.7 &  \dunderline{1pt}{82.1} & 95.6 & 75.4 &  \dunderline{1pt}{84.9} & 85.7 & 84.7 &  \dunderline{1pt}{86.2} \\
 & 87.8 & 82.0 & 95.4 & 75.0 &  \dunderline{1pt}{84.9} & 84.9 & 84.4 & 86.1 \\
 & 87.8 & 81.8 & 95.1 & 74.5 & 84.7 & 84.5 & 84.1 & 85.9 \\
 & 87.8 & 81.6 & 94.9 & 74.0 & 84.5 & 83.8 & 83.8 & 85.8 \\
 &  \dunderline{1pt}{87.9} & 81.6 & 94.5 & 73.6 & 84.1 & 83.2 & 83.4 & 85.7 \\
 & 87.8 & 81.4 & 94.1 & 73.1 & 83.9 & 82.6 & 83.0 & 85.4 \\
 &  \dunderline{1pt}{87.9} & 81.3 & 93.6 & 72.7 & 83.6 & 82.0 & 82.6 & 85.2 \\
 &  \dunderline{1pt}{87.9} & 81.3 & 93.0 & 72.3 & 83.2 & 81.3 & 82.2 & 85.1 \\
 & 87.8 & 81.2 & 92.3 & 71.8 & 82.7 & 80.5 & 81.7 & 84.8 \\
 & 87.8 & 81.0 & 91.5 & 71.4 & 82.0 & 79.6 & 81.1 & 84.4 \\
 &  \dunderline{1pt}{87.9} & 81.0 & 90.4 & 70.7 & 81.3 & 78.5 & 80.4 & 84.2 \\
 & 87.8 & 80.8 & 89.1 & 70.1 & 80.6 & 77.5 & 79.6 & 83.7 \\
 & 87.7 & 80.6 & 87.7 & 69.5 & 79.6 & 76.1 & 78.7 & 83.2 \\
 & 87.5 & 80.3 & 86.1 & 68.8 & 78.5 & 74.5 & 77.6 & 82.5 \\
 & 87.5 & 79.8 & 84.3 & 68.0 & 77.4 & 72.1 & 76.3 & 81.9 \\
\bottomrule
\end{tabular}
\caption{\label{tab:basic_l}
WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient . Results shown for BASIC-L using ImageNet class names. Note that  corresponds to the zero-shot model, while  corresponds to standard fine-tuning. \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}

\FloatBarrier
\clearpage






\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/beyond_probe.pdf}
    \caption{\textbf{Ensembling with a zero-shot model improves accuracy under distribution shift of an independently trained model}. \textbf{(Left)} Output-space ensembling with an independently trained model (NoisyStudent EfficientNet-B6) with comparable performance to the end-to-end fine-tuned model on the reference distribution. \textbf{(Right)} Output-space ensembling with an independently trained model with strong performance on the reference distribution (NoisyStudent EfficientNet-L2). Results averaged over the five distribution shifts as in Figure~\ref{fig:fig1}.}
    \label{fig:beyond}
\end{figure*}

\begin{table*}
\setlength\tabcolsep{3.2pt}
\small
\begin{center}
\begin{tabular}{lc|ccccc|cc}
\toprule
{} &            &             \multicolumn{5}{c|}{Distribution shifts}             & Avg &     Avg\\
{} &           IN (reference) &             IN-V2 &              IN-R &                 IN-Sketch &                 ObjectNet &              IN-A & shifts &     reference, shifts\\
\midrule
CLIP &            &  & & & &  &  & \\
\quad End-to-end fine-tuned                 &           86.2 &           76.8 &           79.8 &           57.9 &           63.3 &           65.4 &           68.6 &           77.4 \\  
\quad WSE ()     &           87.0 &           78.8 &           86.1 &           62.5 &           68.1 &           75.2 &           74.1 &           80.5 \\             
\quad WSE ()      &           86.8 &           79.5 &           89.4 &           64.7 &           71.1 &           79.9 &           76.9 &           81.8 \\             
\quad WSE ()      &           86.2 &           79.2 &           89.9 &  \textbf{65.0} &           71.9 &           80.7 &           77.3 &           81.8 \\             
\quad WSE (optimal )    &           87.1 &           79.5 &           90.3 &  \textbf{65.0} &           72.1 &           81.0 &           77.6 &           82.3 \\               
NS EfficientNet-B6  &            &  & & & &  &  & \\
\quad No ensemble      &           86.5 &           77.7 &           65.6 &           47.8 &           58.3 &           62.3 &           62.3 &           74.4 \\  
\quad OSE ()  &           87.0 &           78.8 &           86.4 &           56.7 &           66.5 &           75.9 &           72.9 &           80.0 \\                
\quad OSE ()   &           86.2 &           78.7 &           89.2 &           63.8 &           69.3 &           78.6 &           75.9 &           81.1 \\                
\quad OSE ()   &           84.3 &           77.2 &           89.5 &           63.8 &           69.7 &           79.0 &           75.8 &           80.0 \\                
\quad OSE (optimal ) &           87.1 &           79.3 &           89.7 &           63.8 &           69.7 &           79.3 &           76.4 &           81.8 \\                 

NS EfficientNet-L2   &            &  & & & &  &  & \\
\quad No ensemble      &           88.3 &           80.8 &           74.6 &           47.6 &           69.8 &           84.7 &           71.5 &           79.9 \\  
\quad OSE ()  &  \textbf{88.6} &           81.6 &           88.0 &           53.4 &           72.2 &  \textbf{87.1} &           76.5 &           82.5 \\                
\quad OSE ()   &           87.4 &           80.6 &           90.2 &           63.4 &  \textbf{73.1} &           86.5 &           78.8 &           83.1 \\                
\quad OSE ()   &           85.2 &           78.5 &  \textbf{90.5} &           63.9 &           72.6 &           86.0 &           78.3 &           81.8 \\                
\quad OSE (optimal ) &  \textbf{88.6} &  \textbf{81.7} &  \textbf{90.5} &           63.9 &  \textbf{73.1} &  \textbf{87.1} &  \textbf{79.3} &  \textbf{83.9} \\ 


\bottomrule
\end{tabular}
\caption{\label{tab:beyond}
Accuracy of various independently trained models ensembled with CLIP on ImageNet and derived distribution shifts. OSE denotes output-space ensembling.  \textit{Avg shifts} displays the mean performance among the five distribution shifts, while \textit{Avg reference, shifts} shows the average of ImageNet (reference) and Avg shifts.
}
\end{center}
\end{table*}


\subsection{Ensembling zero-shot CLIP with independently trained models}
\label{sec:beyond}


So far we have shown that a zero-shot model can be used to improve performance under distribution shift of the derived fine-tuned model. 
Here, we investigate whether this improvement is specific to fine-tuned models. On the contrary, we find that the performance under distribution shift of \textit{independently trained models} improves when ensembling with robust models. Note that in the general case where the models being ensembled have different architectures, we are unable to perform weight-space ensembling;
instead, we ensemble the outputs of each model. This increases the computational cost of inference, in contrast to the results shown in Section \ref{sec:results}.

Concretely, we ensemble zero-shot CLIP with two Noisy Student EfficientNet models \cite{xie2020self,tan2019efficientnet}: (i) EfficientNet-B6 (Figure \ref{fig:beyond}, left), with performance on the reference distribution comparable to the end-to-end fine-tuned CLIP model;
 and (ii) EfficientNet-L2 (Figure \ref{fig:beyond}, right), the strongest model available on PyTorch ImageNet Models \cite{rw2019timm}.
 In both cases, we observe substantial improvements from ensembling---13.6 pp and 6.9 pp in average accuracy under distribution shift without reducing performance on the reference dataset. Further results are shown in Table \ref{tab:beyond}.


\section{Experimental details}
\label{sec:appendix_hparam}

\subsection{CLIP zero-shot}\label{sec:morezs}

This section extends Section \ref{sec:expsetup} with more details on inference with the CLIP zero-shot model. First, in all settings we use the CLIP model \texttt{ViT-L/14@336px}, except when explicitly mentioned otherwise.
Second, CLIP learns a temperature parameter which is factored into the learned weight matrix  described in Section~\ref{sec:expsetup}. Finally, to construct  we ensemble the 80 prompts provided by CLIP at \url{https://github.com/openai/CLIP}. However, we manually engineer prompts for five datasets: WILDS-FMoW, WILDS-iWildCam, Stanford Cars, Describable Textures and Food-101, which are found in the code.

\subsection{End-to-end fine-tuning}
\label{sec:e2e-ft}

Two important experimental details for end-to-end fine-tuning are as follows:
\begin{itemize}
    \item We initialize the final classification layer with the zero-shot classifier used by CLIP. We scale the zero-shot classifier weights by the temperature parameter of the pre-trained CLIP model at initialization, and do not include a temperature parameter during fine-tuning.
    \item As the zero-shot classifier expects the outputs of the image-encoder  to be normalized, we continue to normalize the outputs of  during fine-tuning.
\end{itemize}

When fine-tuning end-to-end, unless otherwise mentioned, we use the AdamW optimizer \cite{loshchilov2018decoupled, paszke2019pytorch} and choose the largest batch size such that the model fits into 8 GPUs (512 for \texttt{ViT-B/16}). Unless otherwise mentioned, we use the default PyTorch AdamW hyperparameters , , , weight decay of 0.1 and a cosine-annealing learning rate schedule \cite{loshchilov2016sgdr} with 500 warm-up steps. Unless otherwise mentioned we use a learning rate of , gradient clipping at global norm 1 and fine-tune for a total of 10 epochs. Additionally, unless otherwise mentioned we use the same data augmentations as \cite{radford2021learning}, randomly cropping a square from resized images with the largest dimension being 336 pixels for \texttt{ViT-L/14@336px} and 224 for the remaining models.

\subsection{Fine-tuning a linear classifier}\label{sec:moreclf}

This section extends the description of linear classifier training from Appendix~\ref{sec:baselines-appendix} with details on hyperparameters and additional analyses. In each of the four regularization strategies---no regularization, weight decay, L1 regularization, and label smoothing---we run 64 hyperparameter configurations. For each trial, mini-batch size is drawn uniformly from  and learning rate is set to  with  chosen uniformly at random from the range .
Hyperparameters for each regularization strategy are as follows: (i) The weight decay coefficient is set to  where  is chosen uniformly at random from  for each trial; (ii) The L1 regularization coefficient is set to  where  is chosen uniformly at random from  for each trial; (iii) The label smoothing \cite{muller2019does} coefficient  is chosen uniformly at random from  for each trial. The linear classifier used for ensembling attains the best performance in-distribution. The hyperparameters from this trial are then used in the distillation and regularization experiments described in Appendix~\ref{sec:baselines-appendix}. In the low-data regime (Section~\ref{sec:low-data}), this process is repeated for each  and dataset.





When training linear classifiers with  images per class as in Section \ref{sec:low-data} the maximum number of epochs  is scaled approximately inversely proportional to the amount of data removed (e.g., with half the data we train for twice as many epochs so the number of iterations is consistent). To choose the  we use default PyTorch AdamW hyperparameters (learning rate 0.001, weight decay 0.01) and double the number of epochs until performance saturates. For each random hyperparameter run we choose the epochs uniformly from .

\subsection{ObjectNet}\label{sec:objectnet}

The zero-shot models in Table \ref{tab:main} use the ImageNet class names instead of the ObjectNet class names. However, this \textit{adaptation to class shift} improves performance by 2.3\% \cite{radford2021learning}. Out of the five datasets used for the majority of the experiments in Section~\ref{sec:main}, ObjectNet is the only dataset for which this is possible. In Figure \ref{fig:fig_objectnet} we compare weight-space ensembles with and without adaptation to class shift.

\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig_objectnet.pdf}
    \caption{Effective robustness scatter plots for ObjectNet, with and without adapting to class shift. \textbf{Left:} Using ImageNet class names to construct the zero-shot classifier. \textbf{Right:} Using ObjectNet class names to construct the zero-shot classifier.}
    \label{fig:fig_objectnet}
\end{figure*}




\section{Diversity measures}
\label{sec:diversity_defs}

Let  be a classification set with input data  and labels , where  is the number of classes. A classifier  is a function that maps inputs  to logits , yielding predictions . We consider measures of diversity  between two classifiers  and  and the dataset . For simplicity,  is used to denote the predictions from classifier  given inputs  (and similarly for ).

\paragraph{Prediction Diversity (PD).} One of the most intuitive ways to measure diversity between pairs of classifiers is to compute the fraction of samples where they disagree while one is correct \cite{ho1998random,skalak1996sources}. Formally, the prediction diversity  is defined as:

where




\paragraph{Cohen's Kappa Complement (CC).} Cohen's kappa coefficient is a measure of agreement between two annotators \cite{mchugh2012interrater}. Here, we use it's complement as a diversity measure between two classifiers:

where  is the expected agreement between the classifiers and  is the empirical probability of agreement. Formally, if  is the number of samples where classifier  predicted label  (i.e. ), then:


\paragraph{KL Divergence (KL).} The Kullback-Leibler divergence measures how different a probability distribution is from another. Let  for a classifier , and let  be the probability assigned to class . We consider the average KL-divergence over all samples as a diversity measure:



\paragraph{Centered Kernel Alignment Complement (CKAC).} CKA is a similarity measure that compares two different sets of high-dimensional representations \cite{kornblith2019similarity}. It is commonly used for comparing representations of two neural networks, or determining correspondences between two hidden layers of the same network. CKA measures the agreement between two matrices containing the pair-wise similarities of all samples in a dataset, where each matrix is constructed according to the representations of a model. More formally, let  denote the -dimensional features for all samples in a dataset , pre-processed to center the columns. For two models  and  yielding similarity matrices  and , CKA is defined as:


where  denotes the Frobenius norm of the matrix . Larger CKA values indicate larger similarities between the representations of the two models, and thus, smaller diversity. We define the diversity measure CKAC as:

Note that CKAC is computationally expensive to compute for large datasets. For this reason, in our experiments with distributions larger than 10,000 samples, we randomly sample 10,000 to compute this measure. 


\paragraph{Diversity across different architectures} We extend Figure \ref{fig:diversity_and_confidence} to show results for all combinations of diversity measures, datasets, and CLIP models. Similarly to before, the baselines compares models with the same encoder, with two linear classifiers trained on different subsets of ImageNet with half of the data. Results are shown in Figures \ref{fig:diversity_breakdown_start}-\ref{fig:diversity_breakdown_end}.

\FloatBarrier

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/PD_diversity_breakdown.pdf}
    \caption{\textbf{Prediction Diversity (PD)} for multiple datasets and CLIP models (Equation \ref {eq:pd}).}
    \label{fig:diversity_breakdown_start}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/CC_diversity_breakdown.pdf}
    \caption{\textbf{Cohen's Kappa Complement (CC)} for multiple datasets and CLIP models (Equation \ref {eq:cc}).}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/KL_diversity_breakdown.pdf}
    \caption{\textbf{Average KL Divergence (KL)} for multiple datasets and CLIP models (Equation \ref {eq:kl}).}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/CKAC_diversity_breakdown.pdf}
    \caption{\textbf{Central Kernel Alignment Complement (CKAC)} for multiple datasets and CLIP models (Equation \ref {eq:ckac}).}
    \label{fig:diversity_breakdown_end}
\end{figure*}

\FloatBarrier


\section{When do weight-space ensembles approximate output-space ensembles?}\label{sec:ntk}

In practice we observe a difference between weight-space and output-space ensembling. 
However, it is worth noting that these two methods of ensembling are not as different as they initially appear. In certain regimes a weight-space ensemble approximates the corresponding output-space ensemble---for instance, when training is well approximated by a linear expansion, referred to as the 
NTK regime \cite{jacot2018neural}. \citet{fort2020deep} find that a linear expansion becomes more accurate in the later phase of neural network training, a phase which closely resembles fine-tuning. 


Consider the set  consisting of all  which lie on the linear path between  and .

\textbf{Proposition 1.} When  for all , the weight- and output-space ensemble of  and  are equivalent.

\textit{Proof.} We may begin with the weight-space ensemble and retrieve the output-space ensemble

where the first and final line follow by the linearity assumption. \qed



 











