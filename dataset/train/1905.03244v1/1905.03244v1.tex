In this Section we present our proposed approach for predicting 3D human shape from a single image.  First, in Subsection~\ref{sec:image_cnn} we briefly describe the image-based architecture that we use as a generic feature extractor. In Subsection~\ref{sec:graph_cnn} we focus on the core of our approach, the Graph CNN architecture that is responsible to regress the 3D vertex coordinates of the mesh that deforms to reconstruct the human body. Then, Subsection~\ref{sec:s2s} describes a way to combine our non-parametric regression with the prediction of SMPL model parameters. Finally, Subsection~\ref{sec:details} focuses on important implementation details.

\subsection{Image-based CNN}\label{sec:image_cnn}
The first part of our pipeline consists of a typical image-based CNN following the ResNet-50 architecture~\cite{he2016resnet}. From the original design we ignore the final fully connected layer, keeping only the 2048-D feature vector after the average pooling layer. This CNN is used as a generic feature extractor from the input representation. To demonstrate the flexibility of our approach, we experiment with a variety of inputs, i.e., RGB images, part segmentation and DensePose input~\cite{guler2018densepose}. For RGB images we simply use raw pixels as input, while for the other representations, we assume that another network~\cite{guler2018densepose}, provides us with the predicted part segmentation or DensePose. Although we present experiments with a variety of inputs, our goal is not to investigate the effect of the input representation, but rather we focus our attention on the graph-based processing that follows.

\subsection{Graph CNN}\label{sec:graph_cnn}
At the heart of our approach, we propose to employ a Graph CNN to regress the 3D coordinates of the mesh vertices. For our network architecture we draw inspiration from the work of Litany~\etal~\cite{litany2017deformable}. We start from a template human mesh with  vertices as depicted in \figurename~\ref{fig:main_figure}. Given the 2048-D feature vector extracted by the generic image-based network, we attach these features to the 3D coordinates of each vertex in the template mesh. From a high-level perspective, the Graph CNN uses as input the 3D coordinates of each vertex along with the input features and has the goal of estimating the 3D coordinates for each vertex in the output, deformed mesh. This processing is performed by a series of Graph Convolution layers.

For the graph convolutions we use the formulation from Kipf~\etal~\cite{kipf2017semi} which is defined as:

where  is the input feature vector,  the weight matrix and  is the row-normalized adjacency matrix of the graph. Essentially, this is equivalent to performing per-vertex fully connected operations followed by a neighborhood averaging operation. The neighborhood averaging is essential for producing a high quality shape because it enforces neighboring vertices to have similar features, and thus the output shape is smooth. With this design choice we observed that there is no need of a smoothness loss on the shape, as for example in~\cite{kato2018renderer}. We also experimented with the more powerful graph convolutions proposed in~\cite{verma2018feastnet} but we did not observe quantitative improvement in the results, so we decided to keep our original and simpler design choice.

For the graph convolution layers, we make use of residual connections as they help in speeding up significantly the training and also lead in higher quality output shapes. Our basic building block is similar to the Bottleneck residual block~\cite{he2016resnet} where  convolutions are replaced by per-vertex fully connected layers and Batch Normalization~\cite{ioffe2015icml} is replaced by Group Normalization~\cite{wu2018eccv}. We noticed that Batch Normalization leads to unstable training and poor test performance, whereas with no normalization the training is very slow and the network can get stuck at local minima and collapse early during training. 

Besides the 3D coordinates for each vertex, our Graph CNN also regresses the camera parameters for a weak-perspective camera model. Following Kanazawa~\etal~\cite{kanazawa2018end}, we predict a scaling factor  and a 2D translation vector . Since the prediction of the network is already on the camera frame, we do not need to regress an additional global camera rotation. The camera parameters are regressed from the graph embedding and not from the image features directly. This way we get a much more reliable estimate that is consistent with the output shape. 

Regarding training, let  be the predicted 3D shape,  the ground truth shape and  the ground truth 2D keypoint locations of the joints. From our 3D shape we can also regress the location for the predicted 3D joints  employing the same regressor that the SMPL model is using to recover joints from vertices. Given these 3D joints, we can simply project them on the image plane, . Now, we train the network using two forms of supervision. First, we apply a per-vertex  loss between the predicted and ground truth shape, i.e.,

Empirically we found that using  loss leads to more stable training and better performance than  loss. Additionally, to enforce image-model alignment, we also apply an  loss between the projected joint locations and the ground truth keypoints, i.e.,

Finally, our complete training objective is:


This form of supervised training requires us to have access to images with full 3D ground truth shape. However, based on our empirical observation, it is not necessary for all the training examples to come with ground truth shape. In fact, following the observation of Omran~\etal~\cite{omran2018neural}, we can leverage additional images that provide only 2D keypoint ground truth. In these cases, we simply ignore the first term of the previous equation and train only with the keypoint loss. We have included evaluation under this setting of weaker supervision in the Sup.~Mat.

\subsection{SMPL from regressed shape}\label{sec:s2s}
\begin{figure}[!t]
    \centering
    \includegraphics[scale=.7]{figures/shape_to_smpl.pdf}
    \caption{\textbf{Predicting SMPL parameters from regressed shape}. Given a regressed 3D shape from the network of \figurename~\ref{fig:main_figure}, we can use a Multi-Layer Perceptron (MLP) to regress the SMPL parameters and produce a shape that is consistent with the original non-parametric shape}
    \label{fig:shape_to_smpl}
	\vspace*{-1.0em}
\end{figure}
Although we demonstrate that non-parametric regression is an easier task for the network, there are still many applications where a parametric representation of the human body can be very useful (e.g., motion prediction). In this Subsection, we present a straightforward way to combine our non-parametric prediction with a particular parametric model, i.e., SMPL. To achieve this goal, we train another network that regresses pose () and shape () parameters of the SMPL parametric model given the regressed 3D shape as input. The architecture of this network can be very simple, i.e., a Multi-Layer Perceptron (MLP)~\cite{rumelhart86} for our implementation. This network is presented in \figurename~\ref{fig:shape_to_smpl} and the loss function for training is:

Here,  and  are the losses on the 3D shape and 2D joint reprojection as before, while  and  are  losses on the SMPL pose and shape parameters respectively.

As observed by previous works, e.g.,~\cite{pavlakos2018learning, martinez2017simple}, it is challenging to regress the pose parameters , which represent 3D rotations in the axis-angle representation. To avoid this, we followed the strategy employed by Omran~\etal~\cite{omran2018neural}. More specifically, we convert the parameters from axis-angle representation to a rotation matrix representation using the Rodrigues formula, and we set the output of our network to regress the elements of the rotation matrices. To ensure that the output is a valid rotation matrix we project it to the manifold of rotation matrices using the differentiable SVD operation. Although this representation does not explicitly improve our quantitative results, we observed faster convergence during training, so we selected it as a more practical option.

\subsection{Implementation details}\label{sec:details}

An important detail regarding our Graph CNN is that we do not operate directly on the original SMPL mesh, but we first subsample it by a factor of 4 and then upsample it again to the original scale using the technique described in \cite{ranjan2018generating}. This is essentially performed by precomputing downsampling and upsampling matrices  and  and left-multiply them with the graph every time we need to do resampling. This downsampling step helps to avoid the high redundancy in the original mesh due to the spatial locality of the vertices, and decrease memory requirements during training.

Regarding the training of the MLP, we employ a 2-step training procedure. First we train the network that regresses the non-parametric shape and then with this network fixed we train the MLP that predicts the SMPL parameters. We also experimented with training them end-to-end but we observed a decrease in the performance of the network for both the parametric and non-parametric shape. 