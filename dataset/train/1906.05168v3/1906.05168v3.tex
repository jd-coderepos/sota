\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\title{Attention-based Multi-Input Deep Learning Architecture for Biological Activity Prediction: \\An Application in EGFR Inhibitors
}
 
\author{\IEEEauthorblockN{Huy Ngoc Pham}
\IEEEauthorblockA{\textit{Research \& Development} \\
\textit{OPC Pharmaceutical Company}\\
Ho Chi Minh City, Vietnam \\
ngochuy.yds@gmail.com}
\and
\IEEEauthorblockN{Trung Hoang Le}
\IEEEauthorblockA{\textit{Research Engineer} \\
\textit{Trusting Social}\\
Ho Chi Minh City, Vietnam \\
le.hg.trung@gmail.com}}



\IEEEoverridecommandlockouts
\IEEEpubid{\makebox[\columnwidth]{978-1-7281-3003-3/19/\yx_1,x_2,...x_n\theta\epsilon\hat{y}yx_iw_i1w_0\sigma\mathbf{x}y\left( f(x) = \frac{1}{1+e^{-x}}\right) \left( f(x) = max(0, x) \right)1l^{(k)}\hat{y}kF_k \in \mathbb{R}^{w \times w \times c}w35ccX \in R^{d_1 \times d_2 \times d_3}F_kXO^k \in \mathbb{R}^{((d1-w+)/s+1) \times (d2-w)/s+1)}[X]_{i,j} \in \mathbb{R}^{w\times w\times d_3}X(i,j)s2 \times 2F(n, m)\max \{F_{i,j}, F_{i+1,j}, F_{i,j+1}, F_{i+1,j+1} \}i, jF(m/2, n/2)\theta\hat{\theta}n\mathcal{L}\theta\gammapp=0.50.8px_i\mathcal{B} = \{x_1, x_2, ..., x_k \}\hat{x}_ix_i\mu_{\mathcal{B}} = \frac{1}{k}\sum_{i=1}^{k}x_i\sigma^2_{\mathcal{B}} = \frac{1}{k} \sum_{i=1}^{k}(x_i - \mu_{\mathcal{B}})^2\epsilon\gamma\beta\alphaIC50nM506:2986 \approx 1:6s, sp, sp^2, sp^3, sp^3 d, sp^3 d^2,1^{st}2^{nd}1^{st}2^{nd}3^{rd}\Vec{m}R\Vec{m}i\Vec{R}_i\Vec{R}_i\Vec{a}_i\Vec{a}\Vec{f}\Vec{t}\Vec{t}\Vec{f}\Vec{m}\Vec{t}\frac{\text{TP}}{\text{TP} + \text{FN}}\frac{\text{TN}}{\text{TN} + \text{FP}}\frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}\frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP}+\text{FP})(\text{TP}+\text{FN})(\text{TN}+\text{FP})(\text{TN}+\text{FN})}}0.10 - 0.160.22 -0.28$ for Validation set at the end of each fold when cross-validating the model CNN + MD + ATT 



The training batch size is 128 which gave the best utility on the GPU Tesla K80. When comparing the effect of two types of the optimizer, we observed that Adaptive Moment Estimation (ADAM) showed a better result than Stochastic gradient descent (SGD) in both running time and model performance. 

The optimal collection of hyper-parameters was listed in the third column of Table \ref{hpt}. These values were used for evaluating the performance of three considered models.

\subsection{Performance}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Loss_value.png}
    \caption{The Loss value of each model on the Validation steps}
    \label{fig:arch}
\end{figure*}

Our architecture was trained on the EGFR dataset and evaluated by mentioned cross-validation method. The final result represents in Table \ref{tab:perform}. When using structure information in the CNN branch only, the performance was slightly better than H. Singh \textit{et al.} model in all metrics excepting AUC. However, the CNN + MD model and CNN + MD + ATT model was outperforming to the reference model. By comparing two important indicators, it can be seen that there was an improvement in MCC and AUC. Additionally, when training with more branch (CNN + MD and CNN + MD + ATT), the Running time was also reduced significantly to around a half.

\begin{table}[htbp]    
    \centering    
    \begin{threeparttable}
        \caption{Performance Comparison }
        \label{tab:perform}
        \centering    
        \begin{tabular}{lcccc}
        \hline
        \textbf{Metrics} \tnote{\textdagger}
                        & H. Singh  & CNN   & CNN + MD & CNN + MD \\
                        &\textit{et al.}    &   &   & + ATT \tnote{\textdaggerdbl}  \\
        \hline
        \textbf{LOSS}   & N.A                      & 0.2727  & 0.2434   & 0.2399   \\
        \textbf{SENS}   & 69.89\%                  & 74.31\% & 75.29\%  & 74.11\%   \\
        \textbf{SPEC}   & 86.03\%                  & 85.77\% & 89.75\%  & 90.15\%   \\
        \textbf{ACC}    & 83.66\%                  & 84.10\% & 87.66\%  & 87.83\%   \\
\textbf{MCC}    & 0.49                  & 0.50  & 0.58  & 0.57  \\
        \textbf{AUC}    & 89.00\%                  & 87.52\%  & 90.32\%  & 90.84\%  \\
\hline      
        \end{tabular}
        \begin{tablenotes}
            \footnotesize
            \item[\textdagger] LOSS: Average Best Loss value in 5-fold Cross validation, SENS: Sensitivity, SPEC: Specificity, ACC: Accuracy, MCC: The Matthews correlation coefficient, AUC: The Area under the ROC curve, RT: Running time.
            \item[\textdaggerdbl] CNN: using CNN branch only, CNN + MD: using both CNN and MD branch, CNN + MD + ATT: using both CNN and MD branch with Attention mechanism.
        \end{tablenotes}
    \end{threeparttable}
\end{table}





\subsection{Attention mechanism}\label{subsec:res_att}
The Attention mechanism was successfully implemented in our architecture. From the attention weights vector, the weights representing to each atom in the molecules were extracted and used to indicate their contribution to the compound's activity. By using visualization on the package \texttt{rdkit}, these attention weights could be used to visualize the distribution of contribution over the molecular structure. Figure \ref{fig:weight_vis} illustrates some example from the model.

Additionally, the integration of attention mechanism to the model did not give much more improvement in performance. We believed that this problem could be improved by optimizing the hyperparameters. However, we did not focus on hyperparameters optimization for this model because the aim of Attention vector was to make the model more interpretable.

The visualization of Attention vectors revealed some key findings in chemical structure, such as the Nitrogens in the hetero-cyclic structure are usually highlighted by the model and the halobenzyl substitution on heterocyclic structure contributed positively to the bioactivity.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{weight_vis.png}
    \caption{Chemical structure interpretation using Attention weight}
    \label{fig:weight_vis}
\end{figure}


 
\section{Discussion}
There are two major advantages to our architecture. The first strong point is the combination of both structure information and chemical attributes in a single learning model. As a result, this advancement made a significant improvement in both performance and automation. Another worthy innovation was the integration of attention mechanism which facilitated the interpretation of the model. In fact, attention weight generated by the model would help explain the contribution of each atom on the overall biological activity.

Comparing to another effort to make deep learning model more interpretable, our architecture has an advantage in computation because it is easier to generate the SMILES Feature matrix than other algorithms. For example, Sanjoy Dey \textit{et al.} \cite{Dey2018} used ECFP fingerprint algorithms to transform the chemical structure into the matrix feature. This method does not treat the molecule as a whole structure but calculate on each fragment of a chemical with a particular radius. Additionally, there are required calculation to generate the features including tuning the hyper-parameters of the algorithms (e.g the radius of calculation).

Regarding to our implementation of Attention mechanism, we observed that each atom in a substance was treated separately; as a result, the connection between atom was not highlighted in our model, as well as the contribution of some functional groups which contain many atoms (e.g, carbonyl, carboxylic, etc) was not clearly illustrated. We proposed a solution for this limitation that is to add another branch to the architecture which embeds the substructure patterns (e.g, Extended-Connectivity Fingerprints \cite{Rogers2010}, Chemical Hashed Fingerprint \cite{Al-Lazikani2004}).

Additionally, the lower performance of the CNN model comparing to the baseline model was another interesting finding. This could be due to the sparsity of SMILES feature matrix. In fact, the CNN as well as other deep learning algorithms require much data to accumulate the information in the training step. In case of SMILES feature data, because of zero paddings to justify the length of encoding vectors, the feature matrix became sparse. This led to the fact that the model required more data for training but the dataset was quite small for deep learning. However, in the case of CNN + MD or CNN + MD + ATT model, because of the addition of another input data, the models acquired information more easily. As a result, the performance was improved in terms of all metrics.

When considering the running time between different models, it is clear that the longest running time was that of model with only CNN branch (59 min) while the more complicated model with more data like CNN + MD and CNN + MD + ATT took just a half of running time with 31 min and 37 min, respectively. This could be because the SMILES Feature matrix in the CNN model was sparse so the model should train longer to achieve the convergence of loss function. In the CNN + MD and CNN + MD + ATT model, there could be a complement between different input branches and we supposed that there was an information flow transferring between two branches, which facilitated the training stage and performance improvement. In other studies which also used several types of data \cite{Koch2013, Koch2013a}, the model trained model separately and did not use this information connection. This phenomenon might represent an advantage of our architecture.

In conclusion, the combination of different source of features is definitely useful for bioactivity prediction, especially when using deep learning model. The attention-based multi-input architecture we proposed achieved a superior score comparing to referring model. Additionally, the attention mechanism would help to interpret the interaction between each element of chemical structures and their activity. 
\bibliographystyle{ieeetr}
\bibliography{paper}

\end{document}
