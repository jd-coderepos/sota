
\documentclass[11pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,matrix,patterns,positioning}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[norelsize,boxed,linesnumbered,vlined]{algorithm2e}
\usepackage{float}
\usepackage{amsmath,amsthm,bm,color,epsfig,enumerate,caption}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{todonotes}
\usepackage{xspace}
\usepackage{subfig}

\usepackage{ulem}

\newcommand{\BEA}{}
\newcommand{\comment}[1]{}
\usepackage{amssymb,amsmath,txfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\newtheorem{property}[theorem]{Property}

\def\eg{\emph{e.g}}
\def\Eg{\emph{E.g}}
\def\ie{\emph{i.e}}
\def\Ie{\emph{I.e}}
\def\cf{\emph{c.f}}
\def\Cf{\emph{C.f}}
\def\etc{\emph{etc}}
\def\vs{\emph{vs}}
\def\wrt{w.r.t}
\def\dof{d.o.f}
\def\etal{\emph{et al}}

\makeatletter
\newcommand*{\extendadd}{
  \mathbin{
    \mathpalette\extend@add{}
  }
}
\newcommand*{\extend@add}[2]{
  \ooalign{
    \vphantom{}
    \cr
    \hfil\hfil
  }
}
\makeatother

\begin{document}

\title{Drop-Activation: Implicit Parameter Reduction and Harmonious Regularization}
\author{Senwei Liang \\ Department of Mathematics\\ Purdue University, IN 47907, USA\\ \href{mailto:liang339@purdue.edu}{liang339@purdue.edu}
        \and Yuehaw Khoo\\ Department of Statistics and the College\\ The University of Chicago, Chicago, IL 60637\\ \href{mailto:ykhoo@galton.uchicago.edu}{ykhoo@galton.uchicago.edu}
         \and Haizhao Yang \\ Department of Mathematics\\ Purdue University, IN 47907, USA\footnote{Current institute.}\\Department of Mathematics\\ National University of Singapore, Singapore\footnote{Institute when the project was started.}\\
          \href{mailto:haizhao@purdue.edu}{haizhao@purdue.edu} }

\maketitle

\begin{abstract}
Overfitting frequently occurs in deep learning. In this paper, we propose a novel regularization method called Drop-Activation to reduce overfitting and improve generalization. The key idea is to drop nonlinear activation functions by setting them to be identity functions randomly during training time. During testing, we use a deterministic network with a new activation function to encode the average effect of dropping activations randomly. Our theoretical analyses support the regularization effect of Drop-Activation as implicit parameter reduction and verify its capability to be used together with Batch Normalization~\cite{bn}. The experimental results on CIFAR-10, CIFAR-100, SVHN, EMNIST, and ImageNet show that Drop-Activation generally improves the performance of popular neural network architectures for the image classification task. Furthermore, as a regularizer Drop-Activation can be used in harmony with standard training and regularization techniques such as Batch Normalization and Auto Augment~\cite{autoaugment}. The code is available at \url{https://github.com/LeungSamWai/Drop-Activation}.
\end{abstract}

{\bf Keywords.} Deep Learning, Image Classification, Overfitting, Regularization

\section{Introduction}
Convolution neural network (CNN) is a powerful tool for computer vision tasks. With the help of gradually increasing depth and width, CNNs \cite{resnet, preresnet, densenet, wrn, resnext} gain a significant improvement in image classification problems by capturing multiscale features \cite{visual}. However, when the number of trainable parameters is far more than that of training data, deep networks may suffer from overfitting. This leads to the routine usage of regularization methods such as data augmentation \cite{autoaugment}, weight decay \cite{cifar}, Dropout \cite{dropout} and Batch Normalization~(BN) \cite{bn} to prevent overfitting and improve generalization.

Although regularization has been an essential part of deep learning, deciding which regularization methods to use remains an art. Even if each of the regularization methods works well on its own, combining them does not always give improved performance. For instance, the network trained with both Dropout and BN may not produce a better result \cite{bn, BNandDropout}. Dropout may change the statistical variance of layers output when we switch from training to testing, while BN requires the variance to be the same during both training and testing~\cite{BNandDropout}.

\textbf{Our contributions:} To deal with the aforementioned challenges, we propose a novel regularization method, Drop-Activation, inspired by the works in \cite{dropout, cutout, stochasticdepth, shakedrop, dropconnect, swapout, zoneout}, where some structures of networks are dropped to achieve better generalization. The advantages are as follows:
\begin{itemize}
    \item Drop-Activation provides an easy-to-implement yet effective method for regularization via implicit parameter reduction.
    \item Drop-Activation can be used in synergy with the most popular architectures and regularization methods, leading to improved performance in various datasets for image classification.
\end{itemize}
The basic idea of Drop-Activation is that the nonlinearities in the network will be randomly activated or deactivated during training. More precisely, the nonlinear activations are turned into identity mappings with a certain probability, as shown in Figure~\ref{fig:DropActMethod}. At testing time, we propose using a deterministic neural network with a new activation function which is a combination of identity mapping and the dropped nonlinearity, to represent the ensemble average of the random networks generated by Drop-Activation. Rectified linear unit~(ReLU) has the advantage of reducing the saturation of gradient and accelerating the training compared with sigmoid or tanh activation function~\cite{verydeep}. It is frequently adopted in modern deep neural networks~\cite{resnet, preresnet, densenet, wrn, resnext}. In this paper, we focus on studying the random replacement of the ReLU activation function with the identity function.

\begin{figure}\centering
    \subfloat[Standard neural network with nonlinearity.]{{\includegraphics[width=0.3\textwidth]{std-act.pdf} }\label{fig:std-act}}\qquad
    \qquad
    \subfloat[After applying Drop-Activation during training.]{{\includegraphics[width=0.3\textwidth]{drop-act.pdf}}\label{fig:drop-act}}\caption{Illustration of Drop-Activation. \textbf{Left:} A standard 2-hidden-layer network with nonlinear activation (Blue). \textbf{Right:} A new network generated by applying Drop-Activation to the network on the left. Nonlinear activation functions are randomly selected and replaced with identity maps (Red).}\label{fig:DropActMethod}\vspace{-0.5cm}
\end{figure}

The starting point of Drop-Activation is to randomly draw an ensemble of neural networks with either an identity mapping or a ReLU activation function. The training process of Drop-Activation is to identify a set of parameters such that various neural networks in this ensemble work well when being assigned with these parameters. By ``fitting'' to many neural-networks instead of a fixed one, overfitting can potentially be prevented. Indeed, our theoretical analysis shows that Drop-Activation implicitly adds a penalty term to the loss function, aiming at network parameters such that the corresponding deep neural network can be approximated by a linear network, \ie, implicit parameter reduction.

\textbf{Organizations:} The remainder of this paper is structured as follows. In Section \ref{sec:2}, we review some of the regularization methods and discuss their relations with our work. In Section \ref{sec:3}, we formally introduce Drop-Activation. In Section \ref{sec:5}, the theoretical analysis demonstrates the regularization of Drop-Activation and its synergy with BN. In Section \ref{sec:4}, these advantages of Drop-Activation are further supported by our numerical experiments carried on different datasets and networks.

\section{Related Work}
\label{sec:2}
Various regularization methods have been proposed to reduce the risk of overfitting. Data augmentation achieves regularization by directly enlarging the original training dataset via randomly transforming the input images \cite{verydeep, simonyan2014very, cutout, autoaugment} or output labels \cite{mixup, disturblabel}. Another class of methods regularize the network by adding randomness into various neural network structures such as nodes \cite{dropout}, connections \cite{dropconnect}, pooling layers \cite{pool}, activations \cite{rrelu} and residual blocks \cite{shake, stochasticdepth, shakedrop}. In particular \cite{dropout, cutout, stochasticdepth, shakedrop, dropconnect, swapout, zoneout} add randomness by dropping some structures of neural networks at random in training.

Dropout \cite{dropout} drops nodes along with its connection with some fixed probability during training. DropConnect \cite{dropconnect} has a similar idea but masks out some weights randomly. \cite{stochasticdepth} improves the performance of ResNet \cite{resnet} by dropping the entire residual block at random during training and passing through skip connections (identity mapping). This idea is also used in \cite{shakedrop} when training ResNeXt \cite{resnext} type 2-residual-branch network. The idea of dropping also arises in data augmentation. Cutout \cite{cutout} randomly cuts out a square region of training images to prevent the neural network from putting too much emphasis on the specific region of features.

A related idea of dropping ``functions'' in neural networks were proposed in~\cite{swapout, zoneout}, where subnetwork structures are discarded randomly, instead of replacing an activation function with the identify function. \cite{swapout} proposes a framework of Swapout , where  is the input feature map,  is a sub-network,  and  are i.i.d Bernoulli distribution, and  is the element-wise product. \cite{swapout} verified the effectiveness of the proposed Swapout framework on a large structure, i.e.,  is a residual in ResNet~\cite{resnet} which consists of layers of BN, ReLU, Convolution. Similarly, Zoneout~\cite{zoneout} discussed the case when  is a set of layers. Dropping a subnetwork structure can lead to instability of training and it requires more careful hyperparameter tuning. On the contrary, Drop-Activation focuses on the nonlinear activation functions, a smaller and more basic structure of networks.We will show the effectiveness of Drop-Activation on a wider range of networks and datasets than those in~\cite{swapout, zoneout}.

In the next section, inspired by the above methods, we propose the Drop-Activation method for regularization. We want to emphasize that the improvement by Drop-Activation is universal to most neural-network architectures, and it can be readily used in conjunction with many regularizers without conflicts.

\section{Formulation of Drop-Activation}
\label{sec:3}
This section describes the Drop-Activation method. Suppose  is an input vector of an -layer feed forward network. Let  be the output of -th layer.  is the element-wise nonlinear activation operator that maps an input vector to an output vector by applying a nonlinearity on each of the entries of the input. Without the loss of generality, we assume , \eg,

where  could be a ReLU, a sigmoid or a tanh function but we only consider that  is a ReLU function in our paper. For a standard fully connected or convolution network, the -dimensional output can be written as

where  is the weight matrix of the -th layer. Biases are neglected for the convenience of presentation.

In what follows, we modify the way of applying the nonlinear activation operator  to achieve regularization.
In the training phase, we remove the pointwise nonlinearities in  randomly. In the testing phase, the function  is replaced with a new deterministic nonlinearity.

\textbf{Training Phase:}  During training, the  nonlinearities  in the operator  are kept with probability  (or dropping them with probability ). The output of the -th layer is thus

where ,  are independent and identical random variables following a Bernoulli distribution  that takes value  with probability  and  with probability . We use  to denote the identity matrix. Intuitively, when , then , meaing all the nonlinearities in this layer are kept. When  , then , meaning all the nonlinearities are dropped. The general case lies somewhere between these two limits where the nonlinearities are kept or dropped partially. At each iteration, a different realization of  is sampled from the Bernoulli distribution again. 

When the nonlinear activation function in Eqn.~\eqref{eq:output-L-da-train} is ReLU, the -th component of  can be written as



\textbf{Testing Phase:} During testing, we use a deterministic nonlinear function resulting from averaging the realizations of . More precisely, we take the expectation of the Eqn.~\eqref{eq:output-L-da-train} with respect to the random variable :

and the new activation function  is the convex combination of an  identity operator  and an activation operator . Eqn.~\eqref{eq:droprelu} is the deteministic nonlinearity used to generate a deterministic neural network for testing. In particular, when ReLU is used, then the new activation  is the leaky ReLU with slope  in its negative part~\cite{rrelu}.

\section{Theoretical Analysis}
\label{sec:5}
In Section \ref{subsec:regularizer}, we show that in a ReLU neural-network with one-hidden-layer, Drop-Activation provides a regularization via penalizing the difference between nonlinear activation network and linear network, which can be understood as implicit parameter reduction, \ie, the intrinsic dimension of the parameter space is reduced. In Section \ref{subsec:harmony}, we further show that the use of Drop-Activation does not impact some other techniques such as BN, which ensures the practicality of using Drop-Activation in deep networks.

\subsection{Drop-Activation as a regularizer}
\label{subsec:regularizer}
We use similar ideas in \cite{dropout} and \cite{dropanalysis} to show that having Drop-Activation in a standard one-hidden layer fully connected neural network with ReLU activation gives rise to an explicit regularizer.

Let  be the input vector,  be the output. The output of the one-hidden layer neural ReLU network is , where ,  are weights of the network,  is the function for applying ReLU elementwise to the input vector. Let  denotes the leaky ReLU with slope  in the negative part. As in Eqn.~\eqref{eq:output-L-da-train} and \eqref{eq:output-L-da-test}, applying Drop-Activation to this network gives

during training, and

during testing. Suppose we have  training samples . To reveal the effect of Drop-Activation, we average the training loss function over :

where the expectation is taken with respect to the feature noise . The use of Drop-Activation can be seen as applying a stochastic minimization to such an average loss. The result after averaging the loss function over  is summarized as follows.
\begin{property}
	The optimization problem (\ref{opt:train}) is equivalent to
	
	\label{prop1}
	\vspace{-0.5cm}
\end{property}

\begin{proof}
Suppose that  is the input vector. Let , where  is a 0-1 vector, and the j-th component of  is equal to 1 if the j-th component of  is positive or is equal to 0 else. Then, the ReLU mapping of  can be written as . For simplification, we denote


On one hand, . We expand it and obtain

where function  is the trace operator computing the sum of matrix diagonal. We denote  as a function converting the diagonal matrix into a column vector. Rewrite the first term of Eqn.~\eqref{eq:avg} and get


On the other hand, we have

where the expectation is taken with respect to the feature noise  Similar to Eqn.~\eqref{eq:Sp}, we combine the matrices containing random variables and obtain


Since  has property of linearity, taking the expectation of Eqn. (\ref{eq:S}) with respect to  obtains


Denote , and then


Using Eqn.~\eqref{eq:S_simp}, Eqn.~\eqref{eq:Sp} and Eqn.~\eqref{eq:S}, we can get the difference between Eqn.~\eqref{eq:avg} and Eqn.~\eqref{eq:not_avg},


Note that , so we have


Finally, we attain the difference between Eqn.~\eqref{eq:avg} and Eqn.~\eqref{eq:not_avg},

\end{proof}

We refer to the objective function of the optimization~(\ref{opt:3}). The first term is nothing but the  loss during prediction time , where 's are defined via \eqref{eq:output-test}. Therefore, Property \ref{prop1} shows that Drop-Activation incurs a penalty

on top of the prediction loss. In Eqn. (\ref{eq:penalty}), the coefficient  influences the magnitude of the penalty. In our experiments,  is selected to be a large number close to  (typically ). The magnitude of the penalty will not be large in our numerical experiments.







The penalty (\ref{eq:penalty}) consists of the terms  and .  has no nonlinearity, so it is a linear network. In contrast, since  has the nonlinearity ,  it can be considered as a deep network. The two networks share the same parameters  and . Therefore the penalty (\ref{eq:penalty}) encourages weights  such that the prediction of the relatively deep network  should be somewhat close to that of a linear network. In this way, the penalty incurs by Drop-Activation may help in reducing overfitting by implicit parameter reduction. 

To illustrate this point, we perform a simple regression task for two functions. To generate the training dataset, we sample 20  pairs from the ground truth function and add gaussian noise on the outputs. Then we train a fully connected network with three hidden layers of width 1000, 800, 200, respectively. Figure \ref{fig:r1} and \ref{fig:r2} show that the network with ReLU has a low prediction error on training data points, but is generally erroneous in other regions. Although the network with Drop-Activation does not fit as well to the training data (comparing with using normal ReLU), overall it achieves a lower prediction error. With the effect of incurred penalty \eqref{eq:penalty}, the network with Drop-Activation reduces the influence of data noise and yields a smooth curve.

\begin{figure*}\centering
    \subfloat[The ground true function: .]{{\includegraphics[width=0.37\textwidth]{regression-eps-converted-to.pdf} }\label{fig:r1}}\qquad
    \subfloat[The ground true function: A piecewise constant function.]{{\includegraphics[width=0.37\textwidth]{regression_2-eps-converted-to.pdf}}\label{fig:r2}}\caption{Comparison between the networks equipped with Drop-Activation and normal ReLU. (a) Regression of . (b) Regression of a piecewise constant function. Blue:  Ground truth functions. Orange: Regression results using ReLU. Green: Regression results using Drop-Activation. ``'': Training data perturbed by Gaussian noise.}\label{fig:reg}\vspace{-0.5cm}
\end{figure*}

Figure~\ref{fig:resnet-164} shows the training of ResNet164 on CIFAR100, the training error with Drop-Activation is slightly larger than that without Drop-Activation. However, in terms of the generalization error, Drop-Activation gives improved performance. This verifies that the original network has been over-parametired and Drop-Activation can regularize the network by implicit parameter reduction.

\subsection{Compatibility of Drop-Activation with BN}
\label{subsec:harmony}
In this section, we show theoretically that Drop-Activation essentially keeps the statistical property of the output of each network layer when going from training to testing phase and hence it can be used together with BN. \cite{BNandDropout} argues that BN assumes the output of each layer has the same variance during training and testing. However, Dropout~\cite{dropout} will shift the variance of the output during the testing time leading to disharmony when used in conjunction with BN. Using a similar analysis as \cite{BNandDropout}, we show that unlike Dropout, Drop-Activation can be used together with BN since it maintains the output variance.

\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.4\textwidth}
\vspace{0pt}
\includegraphics[width=0.95\textwidth]{BasicBlock2.pdf}
\caption{\textbf{Left}: A basic block in ResNet. \textbf{Right}: A basic block of a network with Drop-Activation. }
\label{fig:basicblock}
\end{minipage}
\hspace{.3in}
\begin{minipage}[t]{0.4\textwidth}
\vspace{0pt}
\includegraphics[width=1\textwidth]{shift.pdf}
\caption{The shift ratio of the output of the second stage for ResNet-164.
   and  denote the average of the variance for the output of the second stage during training and testing respectively.}
\label{fig:var_shift}
\end{minipage}
\vspace{-0.4cm}
\end{figure}

To this end, we analyze the mappings in ResNet \cite{resnet}. Figure \ref{fig:basicblock} (Left) shows a basic block of ResNet while Figure \ref{fig:basicblock} (Right) shows a basic block with Drop-Activation. We focus on the rectangular box with dashed line. Suppose the output from the  shown in Figure \ref{fig:basicblock} is . \cite{lee2019wide} shows the hidden features converge in distribution to the Gaussian when  is large, so for simplification, we assume that  are i.i.d. random variables. When  is passed to the Drop-Activation layer followed by a linear transformation  with weights , we obtain

where  and . Similarly, during testing, taking the expectation over 's gives

The output of the rectangular box  (and  during testing) is then used as the input to  in Figure~\ref{fig:basicblock}. Since for BN we only need to understand the entry-wise statistics of its input, without loss of generality, we assume the linear transformation  maps a vector from  to ,  and  are scalars.

We want to show  and  have similar statistics. By design, . Notice that the expectation here is taken with respect to both the random variables  and the input  of the box in Figure~\ref{fig:basicblock}. Thus the main question is whether the variances of  and  are the same. To this end, we introduce the Shift Ratio \cite{BNandDropout}:

 as a metric for evaluating the variance shift. The shift ratio is expected to be close to , since the BN layer  requires its input having similar variance in both training and testing time.
\begin{property}
The shift ratio of  and  is

\label{prop2}
\vspace{-0.5cm}
\end{property}

\begin{proof}
Since  it is easy to get
, , , and ,
where the expectation is taken with respect to random variable 
We have

 where expectation is taken with respect to feature noise  and inputs . In what follows, we compute  and .

Expand the square of  to get


Then we obtain its expectation,


Using the fact that , we get


So far, we have finished . Now we are going to compute . Expand  to get


We take expectation with respect to the input ,


Using the fact that , we can obtain that


With Eqn.~\ref{eq:xtrain_var} and Eqn.~\ref{eq:xtest_var}, we have

\end{proof}

In Eqn.~\eqref{formula}, the range of the shift ratio lies on the interval . In particular, when , , therefore  is close to . This shows that in Drop-Activation, the difference in the variance of inputs to a BN layer between the training and testing phase is rather minor.

We further demonstrate numerically that Drop-Activation does not generate an enormous shift in the variance of the internal covariates when going from the training time to the testing time. We train ResNet164 with CIFAR100. ResNet164 consists of a stack of three stages. Each stage contains 54 convolution layers with the same spatial size.  We observe the statistics of the output of the second stage by evaluating its shift ratio. We compute the variances of the output for each channel and then average the channels' variance. As shown in Figure~\ref{fig:var_shift}, the shift ratio stabilizes close to  at the end of the training, which is consistent with our analysis.

In summary, by maintaining the statistical property of the internal output of hidden layers in testing time, Drop-Activation can be combined with BN to improve performance.


\section{Experiments}
\label{sec:4}
In this section, we empirically evaluate the performance of Drop-Activation and demonstrate its effectiveness. We apply Drop-Activation to modern deep neural architectures on various datasets. This section is organized as followed. Section \ref{subsec:design} contains basic experiment settings. In Section \ref{subsec:dataset}, we introduce the datasets and implementation details. In section \ref{subsec:result}, we present the numerical results.

\subsection{Experiment design}
\label{sec:exp design}
Our experiments are to demonstrate the following points:
\textbf{(1) Comparison with RReLU:} Due to the similarity between the activation function used in our proposed method when having  as ReLU in Eqn. \eqref{eq:output-L-da-test} and the randomized leaky rectified linear units (RReLU), one may speculate that the use of RReLU gives similar performance. We show that this is indeed not the case by comparing Drop-Activation with the use of RReLU.
\textbf{(2) Improvement upon modern neural network architectures:} We show the improvement that Drop-Activation brings is rather universal by applying it to different modern network architectures on a variety of datasets.
\textbf{(3) Compatibility with other approaches:} We show that Drop-Activation is compatible with other popular regularization methods by combining them in different network architectures.



\label{subsec:design}
\textbf{Comparison with RReLU:} RReLU is proposed in \cite{rrelu} with the following training scheme for an input vector ,

where  is a random variable with a uniform distribution  with . In the case of ReLU in Drop-Activation, a comparison between Eqn.~\eqref{eq:droprelu} with Eqn.~\eqref{eq:rrelu} shows that the main difference between our approach and RReLU is the random variable used on the negative axis. It can be seen from Eqn. \eqref{eq:rrelu} that RReLU passes the negative data with a random shrinking rate, while Drop-Activation randomly lets the complete information pass. The parameters  and  in RReLU are set at 1/8 and 1/3 respectively, as suggested in \cite{rrelu}.

\textbf{Improvement upon modern neural network architectures:} The residual-type neural network structures greatly facilitate the optimization for deep neural network \cite{resnet} and are employed by ResNet \cite{resnet}, PreResNet \cite{preresnet}, DenseNet \cite{densenet}, ResNeXt \cite{resnext}, WideResNet (WRN)\cite{wrn} and SENet \cite{se}. We demonstrate that Drop-Activation works well with these modern architectures. Moreover, since these networks use BN to accelerate training and may contain Dropout to improve generalization. e.g., WRN, these experiments also show the ability of Drop-Activation to work in synergy with the prevalent training techniques.

\textbf{Compatibility with other regularization approaches:} To further show that Drop-Activation can cooperate well with other training techniques, we combine Drop-Activation with two other popular data augmentation approaches: Cutout \cite{cutout} and AutoAugment \cite{autoaugment}. Cutout randomly masks a square region of training data and AutoAugment uses reinforcement learning to obtain an improved data augmentation scheme.

\subsection{Datasets and implementation details}
\label{subsec:dataset}
\textbf{Choosing the probability of retaining activation:}
In our method, the only parameter that needs to be tuned is the probability  of retaining activation. To get a rough estimate of what  is, we train a simple network on CIFAR10 without data augmentation and perform a grid search for  on the interval , with a step size equal to . The simple network consists of three convolution layers and two fully connected layers, and details are in the Appendix. We split the train set of CIFAR10 into two parts, 10\% for validation and 90\% for training. The Figure~\ref{fig:parameters} shows the validation error on CIFAR10 versus , which is minimal at . Each data point is averaged over the outcomes of  trained neural-networks. Based on this observation, we choose  for all experiments.

\begin{figure}
\centering
\begin{minipage}[t]{0.4\textwidth}
\vspace{0pt}
\includegraphics[width=1\textwidth]{fig_2.pdf}
\caption{Validation error on CIFAR10 with 95\% confidence intervals with respect to the probability  of retaining activation (average of  runs).}
\label{fig:parameters}
\end{minipage}
\hspace{.3in}
\begin{minipage}[t]{0.4\textwidth}
\vspace{0pt}
\includegraphics[width=1\textwidth]{resnet164.pdf}
\caption{Training curves on CIFAR100 with ResNet164.}
\label{fig:resnet-164}
\end{minipage}

\vspace{-0.4cm}
\end{figure}

\noindent\textbf{Datasets and implementation:} We train the models with Drop-Activation on CIFAR10, CIFAR100 \cite{cifar}, SVHN \cite{svhn}, EMNIST (``Balanced'') \cite{emnist} and ImageNet 2012 \cite{ImageNet} (random cropping size 224224). When applying Drop-Activation to these models, we directly substitute all the original ReLU function with Drop-Activation except for the case of ImageNet. In particular, due to the relatively underfitting of training on ImageNet, only ReLUs in the last two stages of networks are modified by Drop-Activation. All the models are optimized using SGD with a momentum of  \cite{Sutskever:2013}. The other implementation details are given in the Appendix.

\subsection{Experiment results}
\label{subsec:result}
Table~\ref{tab:c100}, \ref{tab:svhnandemnist} and \ref{tab:imagenet} show the testing error on different datasets. The baseline results are from original networks without Drop-Activation. Table~\ref{tab:trainingtime} shows the training time of different models. In what follows, we discuss how our results support the points raised in Section \ref{sec:exp design} and analyse the training time of applying Drop-Activation.

\textbf{Comparison with RReLU: } As shown in Table~\ref{tab:c100}, RReLU may have worse performance than the baseline method. However, Drop-Activation consistently results in superior performance over RReLU and almost all baseline networks. Although Drop-Activation can not reduce the testing error of ResNeXt-864d on CIFAR10, Drop-Activation with DenseNet190-40 has the best testing error smaller than that of the original ResNeXt29-864d.

\begin{table}[htbp]
\small
  \centering
  
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \toprule
          & \multicolumn{3}{c|}{CIFAR10} & \multicolumn{3}{c|}{CIFAR100} \\
\cmidrule{2-7}          & Baseline & RReLU & Drop-Act & Baseline & RReLU & Drop-Act \\
    \midrule
    VGG19(BN) & 6.560.13  & 6.600.22  & \textbf{6.380.07}  & 28.670.30     & 28.620.15     & \textbf{28.550.28} \\
    ResNet110 & 6.770.27  & 7.370.22  & \textbf{6.250.06}  & 28.240.13     & 29.640.06     & \textbf{27.910.18} \\
    ResNet164 & 5.940.27     & 6.080.03     & \textbf{5.620.08}     & 25.860.42  & 24.780.43  & \textbf{24.180.22}  \\
    PreResNet164 & 5.010.03  & 5.170.12  & \textbf{4.870.16}  & 23.490.17  & 23.210.05  & \textbf{22.790.16}  \\
    WideResNet28-10 & 3.850.13  & 4.320.05  & \textbf{3.740.05}  & 18.840.26  & 19.530.13  & \textbf{18.140.22}  \\
    DenseNet100-12 & 4.730.10  & 5.060.03  & \textbf{4.380.09}  & 22.660.25  & 22.590.20  & \textbf{21.800.21}  \\
    DenseNet190-40 & 3.910.15  &   3.840.08    & \textbf{3.510.06}  & 17.280.45   &   18.580.12    & \textbf{16.800.12}  \\
    ResNeXt29-864 & 3.950.05  & 4.560.16  & 3.950.18  & 18.560.38  & 18.650.08  & \textbf{17.650.16}  \\
    \bottomrule
    \end{tabular}

  \caption{Test error (\%) on CIFAR10 an CIFAR100. The test accuracy is averaged over three repeated experiments. We use Baseline to indicate the usage of the original architecture without modifications.}
  \label{tab:c100}
  \vspace{-0.5cm}
\end{table}

\textbf{Application to modern models:} As shown in Table~\ref{tab:c100}, Drop-Activation in almost all cases improves the testing accuracy consistently comparing to Baseline for CIFAR10 and CIFAR100. To further demonstrate this, we apply Drop-Activation to various neural-network architectures and demonstrate the successes on the datasets SVHN, EMNIST, and ImageNet. Again, in Table~\ref{tab:svhn_minist} and \ref{tab:imagenet} we see a consistent improvement when Drop-Activation is used. 

Therefore, Drop-Activation can work with most modern networks for different datasets. Besides, our results implicitly show that Drop-Activation is compatible with regularization techniques such as BN or Dropout used in training these networks.

\begin{table}[htbp]
\begin{minipage}[t]{0.55\linewidth}
\small
  \centering
    \begin{tabular}{|c|c|c|c|c|}
    \toprule
    Models & \multicolumn{2}{c|}{SVHN} & \multicolumn{2}{c|}{EMNIST} \\
\cmidrule{2-5}          & Base & Drop-Act & Base & Drop-Act \\
    \midrule
    ResNet164 & -     & -     & 8.85  & \textbf{8.82}  \\
    PreResNet164 & -     & -     & 8.88  & \textbf{8.72}  \\
    WRN16-8 & 1.54  & \textbf{1.46}  & -     & - \\
    WRN28-10 & -     & -     & 8.97  & \textbf{8.72}  \\
    DenseNet100-12 & 1.76  & \textbf{1.71}  & \textbf{8.81}  & 8.90  \\
    ResNeXt29,8*64 & 1.79  & \textbf{1.69}  & 9.07  & \textbf{8.91}  \\
    \bottomrule
    \end{tabular}

\caption{Test error (\%) on SVHN, EMNIST (Balanced). The Baseline results of WRN and DenseNet for SVHN are obtained from the original papers.}\label{tab:svhn_minist}
  \label{tab:svhnandemnist}\end{minipage}
\begin{minipage}[t]{0.45\linewidth}

\small
  \centering

    \begin{tabular}{|c|c|c|}
    \toprule
          & \multicolumn{2}{c|}{ImageNet 2012} \\
\cmidrule{2-3}      Models    & Baseline & Drop-Act \\
    \midrule
    ResNet34 & 26.07  & \textbf{25.85}  \\
    SENet50 & 23.39  & \textbf{23.18}  \\
    \bottomrule
    \end{tabular}

  \caption{Validation error (\%) on ImageNet.}
 \label{tab:imagenet}

\end{minipage}
\end{table}



\textbf{Compatibility with other regularization approaches:} We apply Drop-Activation to network models that use Cutout or AutoAugment. As shown in Table~\ref{tab:combination}, Drop-Activation can further improve with Cutout or AutoAugment by decreasing the test error on CIFAR100 and CIFAR10.
\begin{table}[htbp]
\small
  \centering
  
   \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \toprule
    \multirow{2}[4]{*}{Model} & \multirow{2}[4]{*}{Dataset} & \multirow{2}[4]{*}{Baseline} & \multirow{2}[4]{*}{DA} & \multicolumn{2}{c|}{with Cutout (CO)} & \multicolumn{2}{c|}{with AutoAug (AA)} \\
\cmidrule{5-8}          &       &       &       & CO    & CO+DA & AA    & AA+DA \\
    \midrule
    ResNet18 & CIFAR100 & 22.46  &   21.61    & 21.96  & 20.99  & -     & - \\
    ResNet164 & CIFAR100 & 25.86  & 24.18  & -     & -     & 21.12  & 20.39  \\
    WideResNet28-10 & CIFAR100 & 18.84  & 18.14  & 18.41  & 17.86  & 17.09  & 16.20  \\
    \midrule
    DenseNet190-40 & CIFAR10 & 3.91  & 3.51  & 3.15  & 2.79  & 2.54  & 2.36  \\
    \bottomrule
    \end{tabular}

    \caption{Test error~(\%) for CIFAR100 or CIFAR10 with combination of Drop-Activation (DA) and Cutout (CO) or AutoAugement (AA). The results of Cutout are quoted from \cite{cutout}. The WideResNet result of AutoAug is quoted from \cite{autoaugment}.}
  \label{tab:combination}\vspace{-0.5cm}
\end{table}

\textbf{Training time:}
The increment of the computational cost of the Drop-Activation network compared with the ReLU network comes from the different realizations of Bernoulli random variables for each activation function. This results in an unavoidable increment of training time. Table~\ref{tab:trainingtime} shows the training time of each batch for different models. In particular, RReLU that we use is Pytorch official function. We train ResNet164 and WideResNet28 with batch size 128 on CIFAR10 using the workstation with CPU AMD Ryzen Threadripper 1920X and 2 GPUs 2080Ti. From Table~\ref{tab:trainingtime}, we can see that both Drop-Activation and officially implemented RReLU suffer from the training time increment.
\begin{table}[htbp]
\small
  \centering
  
    \begin{tabular}{|c|c|c|c|}
    \toprule
    Model & Baseline & RReLU & Drop-Activation \\
    \midrule
    ResNet164 & 0.151 & 0.175 & 0.223 \\
    WideResNet28-10 & 0.128 & 0.212 & 0.179 \\
    \bottomrule
    \end{tabular}\caption{The training time (sec) for each batch of different models.}
   \label{tab:trainingtime}\end{table}

\section{Conclusion}
In this paper, we propose Drop-Activation, a regularization method that introduces randomness on the activation function. Drop-Activation works by randomly dropping the nonlinear activations in the network during training and uses a deterministic network with modified nonlinearities for prediction.

The advantage of the proposed method is two-fold. Firstly, Drop-Activation provides a simple yet effective method for regularization, as demonstrated by the numerical experiments. Furthermore, this is supported by our analysis in the case of one hidden-layer. We show that Drop-Activation gives rise to a regularizer that penalizes the difference between nonlinear and linear networks. Future direction includes the analysis of Drop-Activation with more than one hidden layer. Secondly, experiments verify that Drop-Activation improves the generalization in most modern neural networks and cooperates well with some other popular training techniques. Moreover, we show theoretically and numerically that Drop-Activation maintains the variance during both training and testing time, and thus Drop-Activation can work well with Batch Normalization. These two properties should allow the wide applications of Drop-Activation in many network architectures.

\section{Conflict of Interest}
On behalf of all authors, the corresponding author states that there is no conflict of interest.

\vspace{1cm}
{\bf Acknowledgments.} S. Liang and H. Yang gratefully acknowledge the support of National Supercomputing Center (NSCC) Singapore \cite{nscc} and High-Performance Computing (HPC) of the National University of Singapore for providing computational resources, and the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. H. Yang was partially supported by National Science Foundation under the grant award 1945029.


\begin{thebibliography}{10}

\bibitem{nscc}
The computational work for this article was partially performed on resources of
  the national supercomputing centre, singapore (https://www.nscc.sg).

\bibitem{emnist}
G.~Cohen, S.~Afshar, J.~Tapson, and A.~van Schaik.
\newblock Emnist: an extension of mnist to handwritten letters.
\newblock {\em arXiv preprint arXiv:1702.05373}, 2017.

\bibitem{autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 113--123, 2019.

\bibitem{cutout}
T.~DeVries and G.~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{shake}
X.~Gastaldi.
\newblock Shake-shake regularization.
\newblock {\em arXiv preprint arXiv:1705.07485}, 2017.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{preresnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem{se}
J.~Hu, L.~Shen, and G.~Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141, 2018.

\bibitem{densenet}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{stochasticdepth}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem{bn}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{cifar}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{verydeep}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{zoneout}
D.~Krueger, T.~Maharaj, J.~Kram{\'a}r, M.~Pezeshki, N.~Ballas, N.~R. Ke,
  A.~Goyal, Y.~Bengio, A.~Courville, and C.~Pal.
\newblock Zoneout: Regularizing rnns by randomly preserving hidden activations.
\newblock {\em arXiv preprint arXiv:1606.01305}, 2016.

\bibitem{lee2019wide}
J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  8570--8581, 2019.

\bibitem{BNandDropout}
X.~Li, S.~Chen, X.~Hu, and J.~Yang.
\newblock Understanding the disharmony between dropout and batch normalization
  by variance shift.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2682--2690, 2019.

\bibitem{svhn}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{ImageNet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{swapout}
S.~Singh, D.~Hoiem, and D.~Forsyth.
\newblock Swapout: Learning an ensemble of deep architectures.
\newblock In {\em Advances in neural information processing systems}, pages
  28--36, 2016.

\bibitem{dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{Sutskever:2013}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem{dropanalysis}
S.~Wager, S.~Wang, and P.~S. Liang.
\newblock Dropout training as adaptive regularization.
\newblock In {\em Advances in neural information processing systems}, pages
  351--359, 2013.

\bibitem{dropconnect}
L.~Wan, M.~Zeiler, S.~Zhang, Y.~Le~Cun, and R.~Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In {\em International conference on machine learning}, pages
  1058--1066, 2013.

\bibitem{disturblabel}
L.~Xie, J.~Wang, Z.~Wei, M.~Wang, and Q.~Tian.
\newblock Disturblabel: Regularizing cnn on the loss layer.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4753--4762, 2016.

\bibitem{resnext}
S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{rrelu}
B.~Xu, N.~Wang, T.~Chen, and M.~Li.
\newblock Empirical evaluation of rectified activations in convolutional
  network.
\newblock {\em arXiv preprint arXiv:1505.00853}, 2015.

\bibitem{shakedrop}
Y.~Yamada, M.~Iwamura, T.~Akiba, and K.~Kise.
\newblock Shakedrop regularization for deep residual learning.
\newblock {\em arXiv preprint arXiv:1802.02375}, 2018.

\bibitem{wrn}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{pool}
M.~D. Zeiler and R.~Fergus.
\newblock Stochastic pooling for regularization of deep convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:1301.3557}, 2013.

\bibitem{visual}
M.~D. Zeiler and R.~Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em European conference on computer vision}, pages 818--833.
  Springer, 2014.

\bibitem{mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\end{thebibliography}


\section{Appendix}
\subsection{The simple model for finding the best parameter }
To find the best parameter for Drop-Activation, we perform a grid search on a simple model. The simple network consists of the following layers: We first stack three blocks, and each block contains convolution with  filter, BN, ReLU, and average pooling, as shown in Figure~\ref{fig:simplemodel}. The number of  filters for , ,  is 32, 64, 128 respectively. The widths for fully connected layers are 1000 and 10 respectively.
\begin{figure}[ht]
  \centering
\includegraphics[width=0.4\textwidth]{CNN_1.pdf}\\
  \caption{The model for finding the best parameter for Drop-Activation.}\label{fig:simplemodel}
\end{figure}

\subsection{Introdcution of datasets}
We use the following datasets in our numerical experiments,

\textbf{CIFAR:} Both CIFAR10 and CIFAR100 contain 60k color nature images of size 32 by 32. There are 50k images for training and 10k images for testing. CIFAR-10 has ten classes of objects and 6k for each class. CIFAR100 is similar to CIFAR10, except that it includes 100 classes and 600 images for each class. Normalization and standard data augmentation (random cropping and horizontal flipping) are applied to the training data as \cite{resnet}.


\textbf{SVHN:} The dataset of Street View House Numbers (SVHN) contains ten classes of color digit images of size 32 by 32. There are about 73k training images, 26k testing images, and additional 531k images. The training and additional images are used together for training, so there are totally over 600k images for training. An image in SVHN may contain more than one digit, and the recognition task is to identify the digit in the center of the image. We preprocess the images following \cite{wrn}. The pixel values of the images are rescaled to , and no data augmentation is applied.

\textbf{EMNIST:} EMNIST is a set of  grayscale images containing handwritten English characters and digits. There are six different splits in this dataset and we use the split ``Balanced''. In the ``Balanced'' split, there are 131,600 images in total, including 112,800 for training and 18,800 for testing.


\textbf{ImageNet 2012:} The ImageNet 2012 dataset consists of 1.28 million training images and 50K validation images from 1,000 classes. The models are evaluated on the validation set. We train the models for 120 epochs with an initial learning rate 0.1.
\subsection{Implementation detail}
The hyper-parameters for different networks are shown in Table~\ref{tab:cifar-params}, \ref{tab:imgnet} and \ref{tab:svhn}, and we offer the explanation of hyper-parameter names in Table~\ref{tab:explain}.
\begin{table}[htbp]
\scriptsize
  \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \toprule
          & ResNet & PreResNet & WRN-28 & ResNext29-8*64 & VGG19(BN) & DenseNet190 & DenseNet100 \\
    \midrule
    Batch size & 128   & 128   & 128   & 128   & 128   & 32    & 64 \\
    Epoch & 164   & 164   & 200   & 300   & 200   & 300   & 300 \\
    Optimizer & SGD(0.9) & SGD(0.9) & SGD(0.9) & SGD(0.9) & SGD(0.9) & SGD(0.9) & SGD(0.9) \\
    Depth & -     & -     & 28    & 29    & 19    & 190   & 100 \\
    Schedule & 81/122 & 81/122 & 80/120/160 & 150/225 & 80/140 & 150/225 & 150/225 \\
    Weight-decay & 1.00E-04 & 1.00E-04 & 5.00E-04 & 5.00E-04 & 1.00E-04 & 1.00E-04 & 1.00E-04 \\
    Gamma & 0.1   & 0.1   & 0.2   & 0.1   & 0.1   & 0.1   & 0.1 \\
    Grow-rate & -     & -     & -     & -     & -     & 40    & 12 \\
    Widen-factor & -     & -     & 10    & 4     & -     & -     & - \\
    Cardinality & -     & -     & -     & 8     & -     & -     & - \\
    LR    & 0.1   & 0.1   & 0.1   & 0.1   & 0.1   & 0.1   & 0.1 \\
    Dropout & -     & -     & 0.3   & -     & -     & -     & - \\
    \bottomrule
    \end{tabular}\caption{Hyper-parameter setting for training models on CIFAR10/100 and EMNIST.}
  \label{tab:cifar-params}\end{table}

\begin{table}[htbp]
\scriptsize
\begin{minipage}[b]{0.4\linewidth}

  \centering
     \begin{tabular}{|c|c|c|}
    \toprule
          & ResNet34 & SENet50 \\
    \midrule
    Batch size & 256   & 256 \\
    Epoch & 120   & 120 \\
    Optimizer & SGD(0.9) & SGD(0.9) \\
    Depth & 34    & 50 \\
    Schedule & 30/60/90 & 30/60/90 \\
    Weight-decay    & 1.00E-04 & 1.00E-04 \\
    Gamma & 0.1   & 0.1 \\
    LR    & 0.1   & 0.1 \\
    \bottomrule
    \end{tabular}\caption{Hyper-parameter setting for training models on ImageNet.}
  \label{tab:imgnet}\end{minipage}
\hspace{.5in}
\begin{minipage}[b]{0.5\linewidth}


  \centering
    \begin{tabular}{|c|c|c|c|}
    \toprule
          & WRN-16 & ResNext29-8*64 & DenseNet100 \\
    \midrule
    Batch size & 128   & 128   & 64 \\
    Epoch & 160   & 100   & 40 \\
    Optimizer & SGD(0.9) & SGD(0.9) & SGD(0.9) \\
    Depth & 16    & 29    & 100 \\
    Schedule & 80/120 & 40/70 & 20/30 \\
    Weight-decay & 5.00E-04 & 5.00E-04 & 1.00E-04 \\
    Gamma & 0.2   & 0.1   & 0.1 \\
    Grow-rate & -     & -     & 12 \\
    Widen-factor & 8     & 4     & - \\
    Cardinality & -     & 8     & - \\
    LR    & 0.01  & 0.1   & 0.1 \\
    Dropout & 0.4   & -     & - \\
    \bottomrule
    \end{tabular}\caption{Hyper-parameter setting for training models on SVHN.}
 \label{tab:svhn}\end{minipage}
\end{table}

\begin{table}[htbp]
		\small
		\centering
		\begin{tabular}{|l|l|}
			\toprule
			Batch size & Number of samples for training at each iteration \\
			Epoch & Number of total epochs to train \\
			Depth & The depth of network \\
			Schedule & Decrease learning rate at these epochs \\
			Weight-decay    & The coefficient of l2 loss \\
			Gamma & Learning rate is multiplied by Gamma on schedule \\
			Widen-factor & Widen factor \\
			Cardinality & Model cardinality (group) \\
			LR    & initial learning rate \\
			Dropout  & Dropout ratio \\
			\bottomrule
		\end{tabular}\caption{The explanation of hyper-parameter names.}
        \label{tab:explain}\vspace{-0.2cm}
	\end{table}\end{document}
