\documentclass[11pt,a4paper]{article}

\usepackage{amsfonts,enumerate,subfigure,amsthm,latexsym,amssymb,amsmath,amsfonts} 
\usepackage[final]{graphicx}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage[table]{xcolor} 
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{dcolumn}   \usepackage{bm}        \usepackage{multicol}

\newtheorem{theorem}{\noindent Theorem}\newtheorem{corollary}[theorem]{\noindent Corollary} 
\newtheorem{lemma}[theorem]{\noindent Lemma}
\newtheorem{proposition}[theorem]{\noindent Proposition} 

\usepackage[algoruled,vlined]{algorithm2e}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output} 
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{multirow,multicol}

\usepackage{color}
\newcommand\coltwo[2]{{\left[\begin{array}{c} #1\\ #2\end{array}\right] }} 
\long\def\symbolfootnote[#1]#2{\begingroup \def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}
\def\B{{\mathcal B}} \def\E{{\mathcal E}}
\def\NN{{\mathbb N}} \def\RR{{\mathbb R}} \def\ZZ{{\mathbb Z}} 
\def\qh{{\hat q}} \def\th{{\hat t}}
\def\vol{\mbox{vol}}
\def\Line{{\tt Line}}
\def\Walk{{\tt Walk}}
\def\vol{\mbox{vol}}
\def\Min{{\it min}} 
\def\Max{{\it max}}



\title{Efficient Random-Walk Methods for \\ Approximating Polytope Volume}
\author{Ioannis\,Z.~Emiris\thanks{
Department of Informatics and Telecommunications,
National and Kapodistrian University of Athens, Greece. 
\{emiris,vfisikop\}@di.uoa.gr.}
\and Vissarion Fisikopoulos\footnotemark[1]}
\date{}



\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract} 
We experimentally study the fundamental problem of computing the volume of a convex polytope given as an intersection of linear inequalities. 
We implement and evaluate practical randomized algorithms for accurately
approximating the polytope's volume in high dimensions (e.g.\ one hundred).  
To carry out this efficiently we experimentally correlate the effect
of parameters, such as random walk length and number of sample points, on accuracy and runtime.
Moreover, we exploit the problem's geometry by implementing an iterative rounding procedure, computing partial generations of random points and designing fast polytope boundary oracles. 
Our publicly available code is significantly faster than exact computation and more accurate
than existing approximation methods. We provide volume approximations for the Birkhoff polytopes , whereas exact methods have only computed that of . 

\medskip

{\bf Keywords:} volume approximation, general dimension, random walk,
polytope oracle, algorithm engineering, ray shooting 
\end{abstract}

\newpage
\setcounter{page}{1}
\section{Introduction}\label{sec:intro}

A fundamental problem in discrete and computational geometry is
to compute the volume of a convex body in general dimension or,
more particularly, of a polytope.
In the past~15 years, randomized algorithms for this problem have
witnessed a remarkable progress.
Starting with the breakthrough poly-time algorithm
of~\cite{DyerFrKa91}, subsequent results brought down
the exponent on the dimension from~27 to~4~\cite{LovaszVemp06vol}.
However, the question of an efficient implementation had remained open.  

\textbf{Notation.}
Convex bodies are typically given by a membership oracle.
A polytope  can also be represented as the convex hull
of vertices (V-polytope) or, as is the case here, as the (bounded) intersection
 of  halfspaces given by
,  (H-polytope);
 is its boundary, and
 hides polylog factors in the argument.
The input includes approximation factor ;
 denotes the most important runtime parameter, namely random walk length.

\textbf{Previous work.} 
Volume computation is -P hard for V- and for H-polytopes~\cite{DyerFr88}.
Several exact algorithms are surveyed in
\cite{FukVolume00} and implemented in {\tt VINCI}~\cite{vinci},
which however cannot handle general polytopes for dimension .
An interesting challenge is the volume of the -Birkhoff polytope,
computed only for  using highly specialized software
(Sect.~\ref{sec:exper}).
Regarding deterministic approximation,
no poly-time algorithm can compute the volume with less than
exponential relative error~\cite{Elekes86}.
The algorithm of~\cite{Henk93} has error . 

The landmark randomized poly-time algorithm in~\cite{DyerFrKa91} 
approximates the volume of a convex body with high probability 
and arbitrarily small relative error.
The best complexity, as a function of , given a membership oracle, is  oracle calls~\cite{LovaszVemp06vol}. 
All approaches except~\cite{LovaszVemp06vol}
define a sequence of co-centric balls,
and produce uniform point samples in their intersections with 
to approximate the volume of .

Concerning existing software (cf Sect.~\ref{sec:further}),
\cite{CousinsV13_matlab} presented recently {\tt Matlab} code 
based on~\cite{LovaszVemp06vol} and~\cite{CousinsV13}. The latter offers a
randomized algorithm for Gaussian volume
(which has no direct reduction to or from volume)
in , as a function of .
In \cite{LovaszD12} they implement~\cite{LovaszVemp06vol}, focusing on
variance-decreasing techniques, and
an empirical estimation of mixing time.
In~\cite{LZZ07DirectMC}, they use a straightforward acceptance-rejection
method, which is not expected to work in high dimension;
it was tested only for .
An approach using thermodynamic integration~\cite{Jaekel11} offers only
experimental guarantees on runtime and accuracy.

The key ingredient of all approaches is random walks that produce an
almost uniform point sample.  Such samples is a fundamental problem
of independent interest with important applications in, e.g.,
global optimization, statistics, machine learning,
Monte Carlo (MC) integration, and non-redundant constraint identification.
Several questions of sampling combinatorial structures such as
contingency tables 
and more generally lattice points in polytopes 
may be reduced to sampling a polytope.

No simple sampling method exists unless the body
has standard shape, e.g., simplex, cube or ellipsoid.
Acceptance-rejection techniques are inefficient in high dimensions.
E.g., the number of uniform points one needs to generate in a 
bounding box before finding one in  is exponential in .
A Markov chain is the only known method, and it
may use geometric random walks such as the grid walk,
the ball walk (or variants such as the Dikin walk),
and Hit-and-run \cite{Simonovits03}. 
The Markov chain has to make a (large) number of steps, before the
generated point becomes distributed approximately uniformly
(which is the stationary limit distribution of the chain).
We focus on Hit-and-run which yields the fastest algorithms today.  

In contrast to other walks, Hit-and-run is implemented by 
computing the intersection of a line with .  
In general, this reduces to binary search on the line,
calling membership at every step. 
For H-polytopes, the intersection is obtained by a {\em boundary oracle};
for this, we employ ray-shooting with respect to
the  facet hyperplanes (Sect.~\ref{sec:OraclSampl}).
In exact form, it is possible to avoid linear-time queries by using space in ,
achieving queries in ~\cite{Ramos99}.
Duality reduces oracles to (approximate) -nearest neighbor
queries, which take 

using  space by locality sensitive hashing~\cite{Andoni08}.
Moreover, space-time tradeoffs from  time
and  space to  time and 
 space are available by~\cite{AryaFonsMoun12socg}.
Approximate oracles are also connected to polytope approximation.
Classic results, such as Dudley's, show that  facets suffice to approximate a convex body of unit diameter
within a Hausdorff distance of .
This is optimized to  \cite{AryaFonsMoun12socg}.
The boundary oracle is dual to finding the extreme point in
a given direction among a known pointset.
This is -approximated through -coresets
for measuring extent, in particular (directional) width, but requires
a subset of  points
\cite{AgarwalHPVa05}.  
The exponential dependence on  or the linear dependence on 
make all aforementioned methods of little practical use.
Ray shooting has been studied in practice only in low dimensions,
e.g., in 6-dimensional V-polytopes~\cite{ZhenYaman13}. 

\textbf{Contribution.}
We implement and experimentally study efficient algorithms for approximating
the volume of polytopes. Point sampling, which is the bottleneck of these algorithms,
is key in achieving poly-time complexity and high accuracy.
To this end, we study variants of Hit-and-run. 
It is widely believed that the theoretical bound on  is quite loose,
and this is confirmed by our experiments, where we set 
and obtain a  error in up to  dimensions (Sect.~\ref{sec:exper}).  

Our emphasis is to exploit the underlying geometry.
Our algorithm uses the recursive technique of co-centric balls (cf.\ Sect.~\ref{sec:vol}) introduced in~\cite{DyerFrKa91} and used in a series of papers, with the most recent to be~\cite{KannanLS97}.  
This technique forms a sequence of {\em diminishing} radii
which, unlike previous papers, allowing us to only sample {\em partial generations} of points in each intersection with , instead of sampling  points for each.
In fact, the algorithm starts with computing the largest interior ball by an LP.
Unlike most theoretical approaches, that use an involved rounding procedure,
we sample a set of points in  and compute the minimum enclosing ellipsoid of this set, which is then linearly transformed to a ball. This procedure  is repeated until the ratio of the minimum over the maximum ellipsoid axes reaches some user-defined threshold. This {\em iterative rounding} allows us to handle skinny polytopes efficiently.

We study various oracles (Sect.~\ref{sec:OraclSampl}). 
Line search using membership requires
 arithmetic operations.
This is improved to a boundary oracle in  by avoiding membership. 
Using Coordinate Direction Hit-and-run, we further improve 
the oracle to  amortized complexity.
We also exploit duality to reduce the oracle to
-nearest neighbor search:
although the asymptotic complexity is not improved,
for certain instances such as cross-polytopes in ,
kd-trees achieve a 40x speed-up. 

Our C++ code is
open-source (sourceforge) and uses the {\tt CGAL} library. A series of experiments establishes that
it handles dimensions substantially larger than existing exact approaches,
e.g., cubes and products of simplices
within an error of  for , in about 20~min.
Compared to approximate approaches, it computes significantly
more accurate results.
It computes in few hours volume estimations within an error of  for Birkhoff polytopes ;  has been exactly computed by specialized parallel software in a sequential time of years.   
More interestingly, it provides volume estimations for \vol,\dots,\vol, whose exact values are unknown, within 9 hours.  
In conclusion, we claim that the volume of general H-polytopes
in high dimensions (e.g.\ one hundred) can be efficiently and accurately
approximated on standard computers.  

\textbf{Paper organization.}
The next section discusses walks and oracles.
Sect.~\ref{sec:vol} presents the overall volume algorithm.  
Sect.~\ref{sec:exper} discusses our experiments, and we conclude
with open questions in Sect.~\ref{sec:further}.

\section{Random walks and Oracles} \label{sec:OraclSampl}
This section introduces the paradigm of Hit-and-run walks and focuses on
their implementation, with particular emphasis on exploiting the geometry
of H-polytopes. The methods presented here are analysed experimentally in Sect.~\ref{sec:exper}.

\textbf{Hit-and-run random walks.} 
The main method to randomly sample a polytope is
by (geometric) random walks.
We shall focus on variants of Hit-and-run, which generate a uniform
distribution of points~\cite{smith1984montecarlo}. Assume we possess procedure \Line(), which returns line  through
point ;  will be specified below.
The main procedure of Hit-and-run is \Walk(), which reads in point
 and repeats  times:
(i) run \Line(), 
(ii) move  to a random point uniformly distributed on .
We shall consider two variants of Hit-and-run.

In {\it Random Directions Hit-and-run} (RDHR),
\Line() returns  defined by a random vector
uniformly distributed on the unit sphere centered at .
The vector coordinates are drawn from the standard normal distribution.
RDHR generates a uniformly distributed point in  

starting at an arbitrary, or at
a uniformly distributed point (also known as warm start), respectively, 
where  is the ratio of the radius of the smallest enclosing 
ball over that of the largest enclosed ball in ~\cite{LovaszV06corner}.

In {\it Coordinate Directions Hit-and-run} (CDHR),
\Line() returns  defined by a random vector
uniformly distributed on the set ,
where 
This is a continuous variant of the Grid walk.  
As far as the authors know, the mixing time has not been analyzed.
We offer experimental evidence that CDHR is faster than RDHR and
sufficiently accurate.
An intermediate variant is Artificially Centering
Hit-and-run~\cite{Smith_AHNR98}, where first a set  of sample points
is generated as with RDHR, then \Line() returns  through  and a randomly
selected point from .
This however is not a Markov chain, unlike CDHR and RDHR.

Procedure \Walk(,\ ,\ ) requires at every step an access to a {\it boundary oracle} which computes the intersection of line  with .
In the sequel we discuss various implementations of this oracle. 

\vspace{.5em}
\textbf{Boundary oracle by membership.}
For general convex bodies, a boundary oracle can be implemented using a {\em membership oracle} which, given vector , decides whether .
The intersection of  with  is computed
by binary search on the segment defined by any point on  lying in
the body and the intersection of  with a bounding ball.
Each step calls membership to test whether the current point is internal,
and stops when some accuracy  is certified.
Checking the point against a hyperplane takes  operations,
thus obtaining the intersection of  with the hyperplane.
We store this intersection so that subsequent tests against
this hyperplane take .
The total complexity is  arithmetic operations,
where  is the ball radius. 

\textbf{Boundary oracle by facet intersection.}
Given an H-polytope  the direct method to compute the intersection of line  with  is to examine all  hyperplanes.
Let us consider \Walk() and
line ,
where  lies on , and  is the 
direction of .
We compute the intersection of  with
the -th hyperplane , , namely

We seek points  at which  intersects ,
namely   and 

This is computed in  arithmetic operations.  
In practice, only the  are computed, where
.

In the context of the volume algorithm (Sect.\ref{sec:vol}), 
the intersection points of  with  are compared to the intersections of  with the current sphere.
Assuming the sphere is centered at the origin with radius ,
its intersections with 
are  such that 
. 
If  give a negative sign when substituted to
the aforementioned equation 
then  are the
endpoints of the segment of  lying in the intersection of
 and the current ball.
Otherwise, we have to compute one or two roots of
the aforementioned equation 
since the segment has one or two endpoints on the sphere.

However, in CDHR, where  and  are vertical,  
after the computation of the first pair , all
other pairs can be computed in  arithmetic operations.
This is because two sequential points produced by the walk
differ only in one coordinate.
Let  be the walk coordinate of the previous and the current
step respectively.
Then, assuming , where ,

This becomes ,
where  is the -th column of .
The two maximizations are solved in  ops.
Let vector .
At the next step, given point , where 
is the -th standard basis vector, we perform two maximizations
 in .

\textbf{Boundary oracle by duality.}\label{subsec:duality}
Duality reduces the problem to nearest neighbor (NN) search and its variants. 
Given a pointset  and query point , NN search returns a point  
s.t.\  for all , where 
is the Euclidean distance between points .
Let us consider, w.l.o.g., boundary intersection for line 
parallel to the -axis:

It reduces to two ray-shooting questions; it suffices to describe one,
namely with the upward vertical ray, defined by .  We seek the first facet hyperplane hit which, equivalently, has the
maximum negative signed vertical distance from  to any hyperplane 
of the upper hull, for fixed .
This distance is denoted by sv.
Let us consider the standard (aka functional) duality transform
between points  and non-vertical hyperplanes :
\begin{small}

\end{small}
This transformation is self-dual, preserves point-hyperplane incidences,
and negates vertical
distance, hence sv, where sv
is the signed vertical distance from hyperplane  to point 
in dual space.
Hence, our problem is equivalent to
minimizing sv.
Equivalently, we seek point  minimizing absolute vertical distance 
to hyperplane  on its side of positive distances.
In dual space, consider 


where the latter operation is inner product in Euclidean space 
of ``lifted" datapoint  with
``lifted" query point .
Let 

following an idea of \cite{BasriHZM11}. By the cosine rule, 

where dist stands for Euclidean distance in .
Since the  lie on hyperplane , optimizing
 over a set of points  is equivalent to optimizing
,  
Hence, point  
minimizing sv corresponds to 
minimizing dist. 
Thus the problem is reduced to (exact) nearest neighbor in .
Ray shooting to the lower hull with same  reduces to 
farthest neighbor. 
Unfortunately, an approximate solution to these problems incurs
an additive error to the corresponding original problem.  

Alternatively, we shall consider hyperplane queries.
Let us concentrate on hyperplanes supporting facets on the lower 
hull of .  Their dual points lie in convex position.
Given that point  is interior in , the dual points
of the lower hull facets lie on the upper halfspace of .
In dual space, consider point  and hyperplane  as in expression~(\ref{Eduals}).
Let sd be the signed Euclidean distance from  to , i.e.\
the minimum Euclidean distance of any point on  to .
Then

where the normal is .
Our question, therefore, becomes equivalent to 
minimizing sd over all datapoints  for which
sd; i.e., we seek the NN above .
Starting with facets on the upper hull, the problem becomes that of
maximizing sd, i.e.\ finding the NN below .

The above approaches motivate us to use NN software for exact 
point and hyperplane queries (Sect.~\ref{sec:exper}).
 

\section{The volume algorithm}\label{sec:vol}

This section details our poly-time methods for approximating the volume of .
Algorithms in this family are the current state-of-the-art with
respect to asymptotic complexity bounds.
Moreover, they can achieve any approximation ratio given by the user,
i.e., they form a fully polynomial randomized approximation scheme (FPRAS).
Given polytope , they execute 
{\it sandwiching} and {\it Multiphase Monte Carlo} (MMC)~\cite{Simonovits03}. 

We consider that  is a full-dimensional -polytope. However, we can also consider  to be lower dimensional and be given in form 
, where , ,  , ,  . Using {\it Gauss-Jordan elimination} the linear system  can be transformed to its unique {\it reduced row echelon form} , where  is the identity matrix.  Then  can be written as , i.e.\ a full-dimensional -polytope in . 

\textbf{Rounding and sandwiching.}
This stage involves first rounding  to reach a
near isotropic position, second sandwiching, i.e.\ to compute ball 
and scalar  such that .
There is an abundance of methods in literature for rounding and sandwiching (cf.~\cite{Simonovits03} and references therein). However, here we develop a simple, efficient method that succeed significantly accurate results in practice (cf.\ Sect.~\ref{sec:exper} and Table~\ref{table:rounding}). The method doesn't compute a ball that covers  but a ball  such that  contains almost all the volume of .

For rounding, we sample a set  of  random points in .
Then we approximate the minimum volume ellipsoid  that covers ,
and satisfies the inclusions
,
in time  \cite{Khachiyan96}.
Let us write

where  is a positive semi-definite (p.s.d.) matrix and
 its Cholesky decomposition.
By substituting  we map the ellipsoid to the ball 
. Applying this transformation to  we have  which is the rounded polytope,
where \vol. 
We iterate this procedure until the ratio of the minimum over the maximum ellipsoid axes reaches some user defined threshold.

For sandwiching  we first compute the
{\it Chebychev ball}  of , i.e.\ the largest inscribed ball in .
It suffices to solve the LP:

where  is the -th row of , and
the optimal values of  and  yield, respectively,
the radius  and the center  of the Chebychev ball.

Then we may compute a uniform random point in  and use it as a start
to perform a random walk in , eventually generating  random points.
Now, compute the largest distance between each of the  points and ;
this defines a (approximate) bounding ball.
Finally, define the sequence of balls  

where  and . 

\textbf{Multiphase Monte Carlo (MMC).}
MMC constructs a sequence of bodies 

where  and 
(almost) contains .
Then it approximates vol by the telescopic product

This reduces to estimating the ratios
, which is achieved by 
generating  uniformly distributed points in 
and by counting how many of them fall in .

For point generation we use random walks as in Sect.~\ref{sec:OraclSampl}. 
We set the walk length , which is of the same
order as in \cite{LovaszD12} but significantly lower than theoretical bounds.
This choice is corroborated experimentally (Sect.~\ref{sec:exper}). 

Unlike typical approaches, which generate points in 
for , here we proceed inversely.
First, let us describe initialization.
We generate an (almost) uniformly distributed random point ,
which is easy since .
Then we use  to start a random walk in 
and so on,
until we obtain a uniformly distributed point in . 
We perform  random walks starting from this point to generate 
(almost) uniformly distributed points in  and then
count how many of them fall into .
This yields an estimate of .
Next we keep the points that lie in ,
and use them to start walks so as to gather a total of  (almost) 
uniformly distributed points in .
We repeat until we compute the last ratio . 

The implementation is based on
a data structure  that stores the random points.
In step , we wish to compute
 and  contains  random points in  from the previous step.
The computation in this step consists in removing from  the points not in
, then sampling  new points in 
and, finally, counting how many lie in .
Testing whether such a point lies in some 
reduces to testing whether  because .

One main advantage of our method is that it creates partial generations
of random points for every new body , as opposed to having
always to generate  points.
This has a significant effect on runtime since it reduces it by a
constant raised to .
Partial generations of points have been used
in convex optimization~\cite{BertsimasVempala04}.
 
\def\Plarge{P_{\mbox{\footnotesize large}}}{}
\def\Psmall{P_{\mbox{\footnotesize small}}}{} 
\begin{algorithm}[t!] \caption{\label{Alg:vol} VolEsti ()}
 \BlankLine
  \Input{H-polytope , objective approximation , rounding threshold } 
  \Output{approximation of \vol}
  \BlankLine
  ;\quad 
  \;  
  \BlankLine\tcp{rounding and sandwiching}  
  compute the Chebychev ball \; 
  generate a random point  in \;
  \Repeat{}{    
    \;  
    \For{ to }{
       \Walk()\;
      add  in \;
    }
    compute min encl.\ ellipsoid  of , with p.s.d.\ \;
    set as  the min and max   axes\;
    compute the Cholesky decomposition  of \;
    transform  and  w.r.t.\ \;
  }  
  set  the largest distance from  to any point in \;
  \BlankLine  
  \tcp{MMC}  
  set ;\quad
  \;
 for \;
  \;    
  \; 
  \While{}{
    ;\quad
    ;\quad
    \;    
    ;\quad 
    remove from  the points not in ;\quad
    \; 
    Set  to be an arbitrary point from \;   
    \For{ to }{
       \Walk()\;
      \If{}{
        \;
        add  in \;
      }
    }
    \;
  }  
  \Return  \;
  \BlankLine
\end{algorithm} 
 
We use {\it threads}, also in~\cite{LovaszD12}, to ensure independence
of the points. A thread is a sequence of points each generated from
the previous point in the sequence by a random walk.
The first point in the sequence is uniformly distributed in the ball
inscribed in .
Alg.~\ref{Alg:vol} describes our algorithm using a single thread. 

\textbf{Complexity.}
The first  algorithm was in~\cite{KannanLS97}, using
a sequence of subsets defined as the intersection of
the given body with a ball. 
It uses isotropic sandwiching to bound the number of balls by ,
it samples  points per ball, 
and follows a ball walk to generate each point in  oracle calls. 
Interestingly, both sandwiching and MMC each require  oracle calls.
Later the same complexity was obtained by
Hit-and-run under the assumption the convex body is well sandwiched.

\begin{proposition}{\em\cite{KannanLS97}}\label{prop:volume}
Assuming , the volume algorithm of~\cite{KannanLS97} 
returns an estimation of \vol, which lies between 
 and ,
with probability , by 
 
oracle calls with probability , where we have assumed
 is fixed.
Sandwiching yields
, implying a total of  calls.
\end{proposition}

In~\cite{LovaszVemp06vol}, they construct 
a sequence of log-concave functions and estimate ratios of integrals, instead of ratios of balls, using simulated annealing.
The complexity reduces to 
by decreasing both number of phases and number of samples per phase to . Using Hit-and-run,  still bounds the time to sample each point.
Moreover, they improve isoperimetric sandwiching to .
 
The following Lemma states the runtime of Alg.~\ref{Alg:vol}, which is in fact a variant of the algorithm analysed in~\cite{KannanLS97} (see also Prop.~\ref{prop:volume}). Although there is no theoretical bound on the approximation error of Alg.~\ref{Alg:vol}, our experimental analysis in Sect.~\ref{sec:exper} shows that in practice the achieved error is always  better than the one proved in~Prop.~\ref{prop:volume}.

\begin{lemma}
Given -polytope , Alg.~\ref{Alg:vol}
performs  phases of rounding in , and approximates 
in  arithmetic operations,
assuming  is fixed,
where  and  denote the radii of the largest inscribed ball and
of the co-centric ball covering .
\end{lemma}
\begin{proof}
Our approach generates  balls and uses Hit-and-run.
Assuming  contains the unit ball, an upper bound on 
is diameter . 
In each ball intersected with , we generate
 random points.
Each point is computed after  steps of CDHR.

The boundary oracle of CDHR is implemented in Sect.~\ref{sec:OraclSampl}.
In particular,  CDHR steps require  arithmetic operations. 
It holds  and .
Thus, the amortized complexity of a CDHR step is .
Overall, the algorithm needs
 operations.

Each rounding iteration decreases  and runs in
, where  stands for the number
of sampled points, and  is the approximation of
the minimum volume ellipsoid of Eq.~(\ref{eq:ellipsoid}).
We generate  points, each in  arithmetic operations.
Hence, rounding runs in , where  is fixed.
Moreover,  is typically constant since  is enough to handle,
e.g., polytopes with  in dimension up to . 
\end{proof}

Let us check this bound with the
experimental data for cubes, products of simplices, and Birkhoff polytopes,
with  and , where
,  and , respectively, for the 3 classes, 
and for cubes .  
Fig.~\ref{fig:runtime_dim} shows that the~3 classes behave similarly.
Performing a fit of , runtime follows 
which shows a smaller dependence on  than our bounds,
at this range of experiments.


\section{Experiments}\label{sec:exper}



\begin{table}[t!]\centering\tiny
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}r@{\quad}r@{\quad}r@{\quad}r
@{\quad}r@{\quad}r@{\quad}r@{\quad}r@{~}r@{\quad}r@{\quad}r}
 &  &   & \vol &  &  & [min, max]  & std-dev &  
\multirow{2}{*}{} & {\tt VolEsti} & {\tt Exact}\\
& & & & & & & & & (sec) & (sec)\\\hline
cube-10 & 10 & 20 & 1.024E+003 & 9210 & 1.027E+003 & [0.950E+003,1.107E+003] & 3.16E+001 & 0.0030 & 0.42 & 0.01\\
cube-15 & 15 & 30 & 3.277E+004 & 16248 & 3.24E+004 & [3.037E+004,3.436E+004] & 9.41E+002 & 0.0088 & 1.44 & 0.40\\
cube-20 & 20 & 40 & 1.048E+006 & 23965 & 1.046E+006 & [0.974E+006,1.116E+006] & 3.15E+004 & 0.0028 & 4.62 & swap\\
cube-50 & 50 & 100 & 1.126E+015 & 78240 & 1.125E+015 & [1.003E+015,1.253E+015] & 4.39E+013 & 0.0007 & 117.51 & swap\\
cube-100 & 100 & 200 & 1.268E+030 & 184206 & 1.278E+030 & [1.165E+030,1.402E+030] & 4.82E+028 & 0.0081 & 1285.08 & swap\\
-10 & 10 & 11 & 2.756E-007 & 9210 & 2.76E-007 & [2.50E-007,3.08E-007] & 1.08E-008 & 0.0021 & 0.56 & 0.01\\
-50 & 60 & 61 & 1.202E-082 & 98264 & 1.21E-082 & [1.07E-082,1.38E-082] & 6.44E-084 & 0.0068 & 183.12 & 0.01\\
-100 & 100 & 101 & 1.072E-158 & 184206 & 1.07E-158 & [9.95E-159,1.21E-158] & 4.24E-160 & 0.0032 & 907.52 & 0.02\\
-20-20 & 40 & 42 & 1.689E-037 & 59022 & 1.70E-037 & [1.54E-037,1.87E-037] & 7.33E-039 & 0.0088 & 53.13 & 0.01\\
-40-40 & 80 & 82 & 1.502E-096 & 140224 & 1.50E-096 & [1.32E-096,1.70E-096] & 7.70E-098 & 0.0015 & 452.05 & 0.01\\
-50-50 & 100 & 102 & 1.081E-129 & 184206 & 1.10E-129 & [1.01E-129,1.19E-129] & 4.65E-131 & 0.0154 & 919.01 & 0.02\\
cross-10 & 10 & 1024 & 2.822E-004 & 9210 & 2.821E-004 & [2.693E+004,2.944E+004] & 5.15E-006 & 0.0003 & 1.58 & 388.50\\
cross-11 & 11 & 2048 & 5.131E-005 & 10550 & 5.126E-005 & [4.888E-005,5.437E-005] & 1.15E-006 & 0.0010 & 5.19 & 6141.40\\
cross-12 & 12 & 4096 & 8.551E-006 & 11927 & 8.557E-006 & [8.130E-006,9.020E-006] & 1.69E-007 & 0.0007 & 12.21 & ---\\
cross-15 & 15 & 32768 & 2.506E-008	& 16248	& 2.505E-008 & [2.332E-008,2.622E-008] & 5.15E-010 & 0.0004 & 541.22 & ---\\
cross-18 & 18 & 262144 & 4.09E-011 & 20810 & 4.027E-011	&[3.97E-011,4.08E-011] & 5.58E-013	& 0.0165 & 5791.06 &  ---\\
rh-8-25 & 8 & 25 & 7.859E+002 & 6654 & 7.826E+002 & [7.47E+002,8.15E+002] & 1.93E+001 & 0.0042 & 0.30 & 1.14\\
rh-8-30 & 8 & 30 & 2.473E+002 & 6654 & 2.449E+002 & [2.28E+002,2.68E+002] & 1.06E+001 & 0.0099 & 0.27 & 5.56\\
rh-10-25 & 10 & 25 & 5.729E+003 & 9210 & 5.806E+003 & [5.55E+003,6.06E+003] & 1.85E+002 & 0.0134 & 0.66 & 6.88\\
rh-10-30 & 10 & 30 & 2.015E+003 & 9210 & 2.042E+003 & [1.96E+003,2.21E+003] & 7.06E+001 & 0.0132 & 0.67 & swap\\
rv-8-10 & 8 & 24 & 1.409E+019 & 6654 & 1.418E+019 & [1.339E+019,1.497E+019] & 5.24E+017 & 0.0107 & 0.37 & 0.01\\
rv-8-11 & 8 & 54 & 3.047E+018 & 6654 & 3.056E+018 & [2.562E+018,3.741E+018] & 3.98E+017 & 0.0028 & 0.76 & 0.54\\
rv-8-12 & 8 & 94 & 4.385E+019 & 6654 & 4.426E+019 & [4.105E+019,4.632E+019] & 2.07E+018 & 0.0093 & 0.59 & 261.37\\
rv-8-20 & 8 & 1191 & 2.691E+021 & 6654 & 2.724E+021 & [2.517E+021,2.871E+021] & 1.05E+020 & 0.0123 & 3.69 & swap\\
rv-8-30 & 8 & 4482 & 7.350E+021 & 6654 & 7.402E+021 & [7.126E+021,7.997E+021] & 2.19E+020 & 0.0072 & 12.73 & swap\\
rv-10-12 & 10 & 35 & 2.136E+022 & 9210 & 2.155E+022 & [1.952E+022,2.430E+022] & 1.53E+021 & 0.0093 & 1.00 & 0.01\\
rv-10-13 & 10 & 89 & 1.632E+023 & 9210 & 1.618E+023 & [1.514E+023,1.714E+023] & 6.23E+021 & 0.0088 & 1.24 & 59.50\\
rv-10-14 & 10 & 177 & 2.931E+023 & 9210 & 2.962E+023 & [2.729E+023,3.195E+023] & 1.71E+022 & 0.0135 & 2.08 & swap\\
cc-8-10 & 8 & 70 & 1.568E+005 & 26616 & 1.589E+005 & [1.52E+005,1.64E+005] & 3.50E+003 & 0.0138 & 1.95 & 0.05\\
cc-8-11 & 8 & 88 & 1.391E+006 & 26616 & 1.387E+006 & [1.35E+006,1.43E+006] & 2.65E+004 & 0.0034 & 2.10 & 0.08\\
Fm-4 & 6 & 7 & 8.640E+001 & 4300 & 8.593E+001 & [7.13E+001,1.12E+002] & 8.38E+000 & 0.0055 & 0.19 & 0.01\\
Fm-5 & 10 & 25 & 7.110E+003 & 9210 & 7.116E+003 & [6.35E+003,8.10E+003] & 3.01E+002 & 0.0009 & 0.69 & 0.02\\
Fm-6 & 15 & 59 & 2.861E+005 & 16248 & 2.850E+005 & [2.42E+005,3.22E+005] & 1.55E+004 & 0.0038 & 3.24 & swap\\
ccp-5 & 10 & 56 & 2.312E+000 & 9210 & 2.326E+000 & [2.16E+000,2.52E+000] & 7.43E-002 & 0.0064 & 0.49 & 38.00\\
ccp-6 & 15 & 368 & 1.346E+000 & 16248 & 1.346E+000 & [1.26E+000,1.45E+000] & 3.81E-002 & 0.0002 & 6.14 & swap\\
 & 49 & 64 & 4.42E-023 & 76279 & 4.46E-023 & [4.05E-023,\,7.32E-024] & 1.93E+004 & 0.0092 & 192.97 & 1920.00\\ 
 & 64 & 81 &  2.60E-033 & 106467 & 2.58E-033 & [2.23E-033,\,3.07E-033] & 2.13E-034 & 0.0069 & 499.56 & 8 days\\
 & 81 & 100 & 8.78E-046 & 142380 & 8.92E-046 & [7.97E-046,\,9.96E-046] & 4.99E-047 & 0.0152 & 1034.74 & 6160 days\\
 & 100 & 121 & ??? & 184206 & 1.40E-060 & [1.06E-060,\,1.67E-060] & 1.10E-061 & ??? & 2398.17 & ---\\
 & 121 & 144 & ??? & 232116 &  7.85E-078 & [6.50E-078,\,9.31E-078] & 5.69E-079 & ??? & 4946.42 & ---\\
 & 144 & 169 & ??? & 286261 &  1.33E-097 & [1.13E-097,\,1.62E-097] & 1.09E-098 & ??? & 9802.73 & ---\\
 & 169 & 196 & ??? & 346781 &  5.96E-120 & [5.30E-120,\,6.96E-120] & 3.82E-121 & ??? & 17257.61 & ---\\
 & 196 & 225 & ??? & 413804 &  5.70E-145 & [5.07E-145,\,6.52E-145] & 1.55E-145 & ??? & 31812.67 & ---\\
\end{tabular*}
\caption{ Overall results; 
, ``swap" indicates it ran out of memory and started
swapping. ``???" indicates that the exact volume is unknown; ``---" indicates it didn't terminate after at least 10h. {\tt VINCI} is used for exact volume computation except Birkhoff polytopes where {\tt birkhoff} is used instead.\label{table:vol_results}}
\end{table}



\begin{table}[t]\centering\tiny
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}rr@{~~}r@{~~}rrrr@{}rrr@{~}r@{~}r}
& & & \multicolumn{4}{c}{RDHR} & & \multicolumn{4}{c}{CDHR}\\\cline{4-7}\cline{9-12}\-.5em]
 & 16 & 1 & 2.27E-07 & [1.66E-07,2.85E-07] &  0.0072 & 22.90 & &  2.25E-07 & [1.87E-07,2.80E-07] & 0.0003 & 4.06\\
 & 25 & 1 & 8.53E-13 & [3.72E-13,1.22E-12] &  0.0982 & 105.96 & &  9.53E-13 & [7.30E-13,1.15E-12] & 0.0083 & 17.26\\
 & 36 & 1 & 2.75E-20 & [1.78E-21,6.71E-20] &  0.4259 & 479.40 & &  4.82E-20 & [3.86E-20,6.18E-20] & 0.0056 & 56.64\\
cube-10 & 10 & 1 & 1022.8 & [944.3951,1103.968] & 0.0012 & 2.03 & &  1026.83 & [970.3117,1096.469] & 0.0027 & 0.34\\
cube-10 & 10 & 0.4 & --  & -- & --  & -- & &  1022.88 & [993.0782,1060.409] & 0.0011 & 2.02\\
cube-20 & 20 & 1 & 1.04E+6 & [9.38E+5,1.14E+6] & 0.0033 & 25.44 & &  1.04E+6 & [9.74E+5,1.12E+6] & 0.0028 & 4.62\\
\end{tabular*} 
\caption{ Experiments with CDHR vs RDHR; . 
\label{table:hnr_compare}}
\end{table}

\begin{table}[t]\centering\tiny
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}r@{\quad}r@{\quad}rrrrr@{\quad}r@{\quad}|r@{\quad}r@{\quad}|r@{\quad}r@{\quad}r@{\quad}r}
 &  &  \vol & N &  & [min, max] & std-dev &  & VolEsti  & mem. & VolEsti* & mem. \\
 &  & & & & &  &  & (sec)  & MB & (sec) & MB \\
\hline
10 & 1024 &  2.82E-04 &  9210 &  2.82E-04 & [2.67E-04,3.00E-04] & 5.74E-06 & 0.0001 & 1.58 & 35 & 0.51 & 42\\
12 & 4096 &  8.55E-06 &  11927 & 8.54E-06 & [8.04E-06,8.89E-06] & 1.72E-07 & 0.0010 &  12.21 & 35 & 1.62 & 72\\
14 & 16384 & 1.88E-07 &   14778 &  1.88E-07 & [1.80E-07,1.99E-07] & 4.09E-09 & 0.0006 &  237.22 & 36 & 6.49 & 230\\
16 & 65536 &  3.13E-09 &   17744 &  3.13E-09 & [2.97E-09,3.33E-09] & 6.44E-11 & 0.0004 &   1430.93 & 37 & 32.87 & 992\\
18 & 262144 & 4.09E-11 &  20810 &  4.09E-11 & [3.99E-11,4.29E-11] & 7.19E-13 & 0.0013 & 5791.06& 38 & 188.43 &  4781\\
\end{tabular*} 
\caption{ Experiments with NN for boundary oracle on cross-polytopes;
VolEsti uses {\tt flann}; . \label{table:flann}}
\end{table}

We implement and experimentally test the above algorithms and methods in the 
software package {\tt VolEsti}.
The code currently consists of around 2.5K lines in C++ and is
open-source\footnote{\tt http://sourceforge.net/projects/randgeom}.
It relies on the {\tt CGAL} library~\cite{CGAL} for its -dimensional kernel
to represent objects such as points and vectors,
for its LP solver~\cite{CGAL_LPSolver},
for the approximate minimum ellipsoid~\cite{CgalBoundingVolumes},
and for generating random points in balls.
We use {\tt Eigen}~\cite{eigenlib} for linear algebra.
The memory consumption is dominated by the list of random points
which needs  space during the entire execution
of the algorithm (Sect.~\ref{sec:vol}). 
Arithmetic uses the {\tt double} data type of C++,
except from the LP solver, which uses the
\emph{GNU Multiple Precision arithmetic library} to avoid double exponent
overflow. We experimented with several pseudo-random number generators in
Boost~\cite{Boostrandom} and chose the fastest, namely mersenne twister generator {\tt mt19937}.
All timings are on an Intel Core i5-2400 GHz, MB L2 cache,
GB RAM, 64-bit Debian GNU/Linux.

\vspace{.5em}
\textbf{Data.}
The following polytopes are tested (the first 7 are from the {\tt VINCI} 
webpage):\vspace{-.2cm}
\begin{itemize}\itemsep-.3em
\item cube-:  ,
\item cross-: cross polytope, the dual of cube, i.e.\
,
\item rh--: polytopes constructed by randomly choosing  hyperplanes tangent to the sphere, \item rv--: dual to rh--, i.e.\ polytopes with  vertices randomly distributed on the sphere,
\item cc-8-: the -dimensional product of two -dimensional cyclic polyhedra with  vertices, \item ccp-: complete cut polytopes on  vertices,
\item Fm-:\ one facet of the metric polytope in dimension ,
\item -: the -dimensional simplex ,
\item --: product of two simplices, i.e\ ,
\item skinny-cube-:  , 
rotated by  in the plane defined by the first two coordinate axes,
\item : the -Birkhoff polytope (defined below).
\end{itemize}

Each experiment is repeated 100 times with  unless otherwise stated. 
The reported timing for each experiment is the mean of 100 timings. 
We keep track of and report the \Min\ and the \Max\ computed values,
the mean , and the standard deviation.  We measure the accuracy of our method by (\vol)/\vol and (\Max\Min)/; unless otherwise stated mean error of approximation refers to the first quantity. 
The reader should not confuse these quantities which refer to the approximation error that computed {\it in practice} with  which refers to the {\it objective} approximation error.
Comparing the practical and objective approximation error, our method is in practice more accurate than indicated by the theoretical bounds.
In particular, in all experiments all computed values are contained in the
interval ,
while theoretical results in~\cite{KannanLS97} guarantee only  of them.
Actually, the above interval is larger than [\Min, \Max].
In general our experimental results show that our software can approximate
the volume of general polytopes up to dimension 100 in less than 2~hours
with mean approximation error at most 2\% (cf.\ Table~\ref{table:vol_results}).

\vspace{.5em}
\textbf{Random walks and oracles.}
First, we compare the implementations of boundary oracles using membership oracles versus using facet intersection.  
By performing experiments with RDHR our algorithm
approximate the volume of a 10-cube in 42.58~sec using the former,
whereas it runs in~2.03~sec using the latter. 

We compare RDHR to CDHR. The latter take advantage of more efficient boundary oracle implementations as described in Sect.~\ref{sec:OraclSampl}.
Table~\ref{table:hnr_compare} shows that our algorithm using CDHR
becomes faster and more accurate than using RDHR by means of smaller
[\Min,\Max] interval.
Additionally, since CDHR is faster we can increase the accuracy
(decrease ) and obtain even more accurate results than RDHR,
including smaller error . 

Finally, we evaluate our implementations of boundary oracles using {\it duality} and {\it NN search} (Sect.~\ref{subsec:duality}). The motivation comes from the fact that the boundary oracle becomes slow when the number of facets is large, e.g., for cross-, . 
We consider state-of-the-art NN software:
{\tt CGAL}'s dD Spatial Searching implements kd-trees \cite{CGALKDtree},
{\tt ANN}~\cite{ANN97} implements kd- and BBD-trees,
{\tt LSH} implements Locality Sensitive Hashing~\cite{Andoni08},
and {\tt FLANN}~\cite{Muj11} implements randomized kd-trees.   
We compare them against our oracle running in , on
cross-,  and cpp-.
We build two kd-trees per coordinate, i.e.\ one per direction, each
tree storing the dual of the corresponding lower and upper hulls.

Consider point queries.  
{\tt FLANN}, is very fast in high dimensions
(typically ), but lacks theoretical guarantees.
It turns out that {\tt KDTreeSingleIndexParams} on cross- returns
exact results for all  and  tested, since the tree stores vertices of a cube.
Compared to the  oracle, for  it is 10x slower,
for  it is competitive, and for  it lets us approximate
vol(cross-18) with a 40x speed-up, but with extra memory usage
(Table~\ref{table:flann}).   
On other datasets, {\tt FLANN} does not always compute the exact NN 
even for .
{\tt ANN}, is very fast up to dimension  and
offers theoretical guarantees. For , it guarantees the exact NN,
but is x slower than our  oracle, though it becomes
significantly faster for . In~\cite{Muja09}, {\tt LSH} is reported to be 10x slower than {\tt FLANN}
and competitive with {\tt ANN}, thus we do test it here.

{\tt CGAL} for point queries is slower than {\tt ANN}, but can be
parametrized to handle hyperplane queries with theoretical guarantees.
Given hyperplane , we set as query point the projection of the origin
on  and as distance-function the inner product between points. 
With the {\tt Sliding\_midpoint} rule and , this is a bit
(while {\tt ANN} is x) slower than our boundary oracle for cross-17.
It is important to design methods for which
 accelerates computation so as to use them with
approximate boundary oracles. 

The above study provides motivation for the design of algorithms that can use approximate boundary queries and hence take advantage of NN software to handle more general polytopes with large number of facets. 
Of particular relevance is the development of efficient methods and
data-structures for approximate hyperplane queries.



\begin{figure*}[t]
	\centering
	\includegraphics[width=.6\textwidth]{NW.pdf}
	\caption{Runtime of {\tt VolEsti}
w.r.t.\ dimension; , y-axis in logscale; 
fitting on cube- results.}\label{fig:wN}
\end{figure*}

\begin{figure}[t]
	\centering
	\includegraphics[width=.6\textwidth]{runtime_dim.pdf}
	\caption{Experiments with  on the effect of  and  (or ) on
accuracy, measured by (min, max)/ (crosses), and runtime,
measured by levels of , for .}\label{fig:runtime_dim}	
\end{figure}

\begin{table}[t!]\centering\scriptsize
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}rrrrrrrrrrr}
 &  &  &  &  & [\Min,\Max] & std-dev & (\vol-) & 
(min, max) \\
&  &  & &  & &  & /\vol & / \\
\hline
(*) cube-10 & 10 & 20 & {\bf 10} & 1026.953 & [925.296,1147.101] & 33.91331 & 0.0029 & 0.2160\\
cube-10 & 10 & 20 & 15 & 1024.157 & [928.667,1131.928] & 31.34121 & 0.0002 & 0.1985\\
cube-10 & 10 & 20 & 20 & 1026.910 & [932.118,1144.601] & 30.97023 & 0.0028 & 0.2069\\
 &  &  &  &  &  &  &  & \\
cube-50 & 50 & 100 & 10 & 1.123E+15 & [1.019E+15,1.257E+15] & 4.135E+13 & 0.0022 & 0.2125\\
(*) cube-50 & 50 & 100 & {\bf 15} & 1.131E+15 & [1.039E+15,1.237E+15] & 3.882E+13 & 0.0044 & 0.1744\\
cube-50 & 50 & 100 & 20 & 1.127E+15 & [1.033E+15,1.216E+15] & 3.893E+13 & 0.0007 & 0.1629\\
 &  &  &  &  &  &  &  & \\
cube-100 & 100 & 200 & 10 & 1.278E+30 & [1.165E+30,1.402E+30] & 4.819E+28 & 0.0081 & 0.1856\\
cube-100 & 100 & 200 & 15 & 1.250E+30 & [1.243E+30,1.253E+30] & 4.075E+27 & 0.0140 & 0.0083\\
(*) cube-100 & 100 & 200 & {\bf 20} & 1.263E+30 & [1.190E+30,1.321E+30] & 3.987E+28 & 0.0038 & 0.1038\\
 &  &  &  &  &  &  &  & \\
-20-20 & 40 & 42 & 10 & 1.699E-37 & [1.527E-37,1.881E-37] & 7.670E-39 & 0.0056 & 0.2083\\
(*) -20-20 & 40 & 42 & {\bf 14} & 1.694E-37 & [1.526E-37,1.892E-37] & 7.096E-39 & 0.0025 & 0.2166\\
-20-20 & 40 & 42 & 20 & 1.694E-37 & [1.433E-37,1.836E-37] & 7.006E-39 & 0.0024 & 0.2382\\
 &  &  &  &  &  &  &  & \\
-50-50 & 100 & 102 & 10 & 1.098E-129 & [1.012E-129,1.189E-129] & 4.652E-131 & 0.0154 & 0.1612\\
-50-50 & 100 & 102 & 15 & 1.111E-129 & [1.090E-129,1.139E-129] & 1.610E-131 & 0.0281 & 0.0437\\
(*) -50-50 & 100 & 102 & {\bf 20} & 1.079E-129 & [1.011E-129,1.148E-129] & 3.685E-131 & 0.0015 & 0.1266\\
 &  &  &  &  &  &  &  & \\
 & 81 & 100 & 10 & 7.951E-55 & [6.291E-55,9.077E-55] & 8.533E-56 & 0.0946 & 0.3504\\
 & 81 & 100 & 15 & 8.124E-55 & [7.451E-55,8.774E-55] & 5.015E-56 & 0.0750 & 0.1629\\
(*)  & 81 & 100 & {\bf 20} & 7.489E-55 & [7.398E-55,7.552E-55] & 6.615E-57 & 0.1472 & 0.0106\\
\end{tabular*} 
\caption{ Experiments with varying ; .  (*) 
indicate minimum  where either (\vol-)/\vol or
(min, max)/ is .  \label{table:walk_len}}
\end{table}

\begin{table}[t]\centering\scriptsize
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}rrrrrrr}
 & \vol &  &  & [\Min,\Max] &  & {\tt VolEsti}(sec) 
\\\hline
rv-8-11 & 3.047E+18 & 6654 & 1.595E+18 & [6.038E+17,3.467E+18] & 0.4766 & 1.48\\
rv-8-11 & 3.047E+18 & 665421 & 3.134E+18 & [3.134E+18,3.134E+18] & 0.0283 & 157.46\\
(*) rv-8-11 & 3.047E+18 & 6654 & 3.052E+18 & [2.755E+18,3.383E+18] & 0.0013 & 1.34\\
skinny-cube-10 & 1.024E+05 & 9210 & 5.175E+04 & [2.147E+04,1.228E+05] & 0.4946 & 0.69\\
(*) skinny-cube-10 & 1.024E+05 & 9210 & 1.029E+05 & [8.445E+04,1.149E+05] & 0.0050 & 0.71\\
skinny-cube-20 & 1.049E+08 & 23965 & 4.193E+07 & [2.497E+07,7.259E+07] & 0.6001 & 5.59\\
(*) skinny-cube-20 & 1.049E+08 & 23965 & 1.040E+08 & [8.458E+07,1.163E+08] & 0.0084 & 6.70\\
\end{tabular*} 
\caption{ Experiments with rounding; (*): means that we use rounding.
\label{table:rounding}}
\end{table} 

\textbf{Choice of parameters and rounding.}
We consider two crucial parameters,  
the length of a random walk, denoted by , and approximation
, which determines the number  of random points. 
We set .
Our experiments indicate that, with this choice, either
(\vol-)/\vol or  is 
up to ~(Table~\ref{table:walk_len}). 
Moreover, for higher  the improvement in accuracy is not significant,
which supports the claim that asymptotic bounds are unrealistically high.
Fig.~\ref{fig:wN} correlates runtime (expressed by ) and
accuracy (expressed by  which actually measures 
some ``deviation") to  and  (expressed by ).
A positive observation is that accuracy tightly correlates with runtime:
e.g., accuracy values close to or beyond~1 lie under the curve ,
and those rounded to  lie roughly above .
It also shows that,
increasing  converges faster than increasing  to a value beyond which the improvement in accuracy is not significant.

To experimentally test the effect of {\it rounding} we construct skinny
hypercubes skinny-cube-d.  
We rotate them to avoid CDHR taking unfair advantage of the degenerate
situation where the long edge is parallel to an axis. 
Table~\ref{table:rounding} on these and other polytopes shows that
rounding reduces approximation error by~2 orders of magnitude.
Without rounding, for polytope rv-8-11
one needs to multiply  (thus runtime) by 100 in order to achieve 
approximation error same as with rounding.  



\begin{table}[t]\centering\footnotesize
\begin{tabular}{r|rrrr|rrrr|rrrr}
\multirow{2}{.5cm}{:} & \multicolumn{4}{c}{rv-15-} & \multicolumn{4}{c}{rv-10-} & \multicolumn{4}{c}{cube-} \\
& 30 & 40 & 50 & 60 & 100 & 150 & 200 & 250 & 7 & 8 & 9 & 10 \\\hline time (sec) & 7.7 & 82.8 & 473.3 & swap & 37.3 & 107.8 & 282.5 & 449.0 & 0.1 & 2.2 & 119.5 & h\\
\end{tabular} 
\caption{ Experiments with {\tt qhull}; ``swap" indicates it
ran out of memory and started swapping; ``5h" indicates it did not
terminate after 5 hours.  \label{table:qhull_vol}}
\end{table}

\begin{table}[t!]\centering\tiny
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}r@{\,}|@{\,}r@{\,}
r@{\,}r@{\,}r@{\,}r@{\,}|@{\,}r@{\,}r@{\,}r@{\,}r@{\,}r}
& \multicolumn{5}{c}{software of~\cite{CousinsV13}} & \multicolumn{5}{c}{{\tt Volesti}}\\
   & [min, max]  & std-dev & {\tiny \multirow{2}{*}{}} & \# total & time(sec)
  & [min, max]  & std-dev & \multirow{2}{*}{} & \# total & time(sec)\\
 &&&&steps&&&&&steps& \\\hline
cube-20  & [5.11E+05,\,1.55E+06] & 1.67E+05 & 0.0198 & 7.96E+04 & 21.48 & [9.74E+05,\,1.12E+06] & 3.15E+04 & 0.0028 & 3.61E+06 &  4.62\\
cube-30  & [6.75E+08,\,1.45E+09] & 1.72E+08 & 0.0440 & 2.22E+05 &  49.24 & [9.91E+08,\,1.16E+09] & 3.89E+07 & 0.0039 & 1.21E+07 &  17.96\\
cube-40  & [7.90E+11,\,1.38E+12] & 1.67E+11 & 0.0731 & 4.30E+05 &  88.09 & [1.01E+12,\,1.23E+12] & 4.46E+10 & 0.0039 & 2.84E+07 &  50.72\\
cube-50  & [8.75E+14,\,1.45E+15] & 1.43E+14 & 0.0327 & 7.16E+05 &  148.06 & [1.00E+15,\,1.25E+15] & 4.39E+13 & 0.0007 & 5.49E+07 &  117.51\\
cube-60  & [8.89E+17,\,1.43E+18] & 1.64E+17 & 0.0473 & 1.15E+06 &  229.33 & [1.06E+18,\,1.27E+18] & 4.00E+16 & 0.0051 & 9.42E+07 &  222.10\\
cube-70  & [9.01E+20,\,1.36E+21] & 1.49E+20 & 0.0707 & 1.66E+06 &  427.82 & [1.02E+21,\,1.32E+21] & 5.42E+19 & 0.0013 & 1.49E+08 &  358.93\\
cube-80  & [9.30E+23,\,1.36E+24] & 1.46E+23 & 0.1145 & 2.30E+06 &  531.46 & [1.13E+24,\,1.30E+24] & 4.42E+22 & 0.0009 & 2.21E+08 &  582.19\\
cube-90  & [1.07E+27,\,1.88E+27] & 2.20E+26 & 0.0394 & 3.30E+06 &  701.54 & [1.09E+27,\,1.44E+27] & 5.18E+25 & 0.0019 & 3.15E+08 &  875.69\\
cube-100 & [9.53E+29,\,1.64E+30] & 1.93E+29 & 0.0357 & 4.19E+06 &  884.43 & [1.17E+30,\,1.40E+30] & 4.82E+28 & 0.0081 & 4.33E+08 &  1285.08\\
   & [2.12E-23,\,2.45E-22] & 6.25E-23 & 0.3970 & 9.31E+05 &  221.30 & [4.05E-23,\,7.32E-24] & 1.93E+04 & 0.0092 & 1.01E+08 & 192.97\\
   & [1.54E-33,\,2.77E-33] & 3.71E-34 & 0.1830 & 2.05E+06 &  420.07 & [2.23E-33,\,3.07E-33] & 2.13E-34 & 0.0069 & 2.27E+08 & 499.56\\
  & [3.39E-46,\,1.92E-45] & 4.75E-46 & 0.1207 & 3.69E+06 &  691.97 & [7.97E-46,\,9.96E-46] & 4.99E-47 & 0.0152 & 4.62E+08 & 1034.74\\
\end{tabular*} 
\caption{ Comparison of the software~\cite{CousinsV13} vs {\tt VolEsti}; each experiment is run 10 times, total steps refer to the mean of the total number of Hit-and-run steps in each execution.\label{table:matlab}}
\end{table} 

\vspace{.5em}
\textbf{Other software.}
Exact volume computation concerns software computing the exact value of
the volume, up to round-off errors in case it uses floating point arithmetic. 
We mainly test against {\tt VINCI}\,1.0.5~\cite{vinci}, which implements
state-of-the art algorithms, cf.\ Table~\ref{table:vol_results}.
For H-polytopes, the method based on Lawrence's general formula 
is numerically unstable resulting in wrong results in many examples~\cite{FukVolume00},
and thus was excluded.
Therefore, we focused on Lasserre's method. 
For all polytopes there is a threshold dimension for which
{\tt VINCI} cannot compute the volume:
it takes a lot of time (e.g.\  4 hrs for cube-20) and 
consumes all system memory, thus starts swapping.

{\tt LRS} is not useful for H-polytopes as stated on its webpage:
``If the volume option
is applied to an H-representation, the results are not predictable."
{\tt Latte} implements the same decomposition methods
as {\tt VINCI}; it is less prone to round-off error but slower~\cite{Latte}.
{\tt Normaliz} applies triangulation:
it handles cubes for , in ~min,
but for , it did not terminate after 5~hours.
{\tt Qhull} handles V-polytopes
but does not terminate for cube-10 nor random polytope rv-15-60
(Table~\ref{table:qhull_vol}). 
This should be juxtaposed to the duals, namely
our software approximates the volume of cross-10 in 2 sec with  error and 
rh-15-60 in 3.44 sec.
A general conclusion for exact software is that it cannot handle
.

We compare with the most relevant approximation method, namely the
{\tt Matlab} implementation of~\cite{CousinsV13_matlab} for bodies
represented as the intersection of an H-polytope and an ellipsoid.
They report that the code is optimized to achieve about  success
rate for bodies of dimension  and  (not to be confused with the  of our method).
Testing~\cite{CousinsV13_matlab} with default options and
, our implementation with  runs faster for , performs  roughly  times more total Hit-and-run steps and returns significantly
more accurate results, e.g.\ from  to  times smaller error
on cube- when , and from  to  times on Birkhoff polytopes (Table~\ref{table:matlab}).

\begin{table}[t!]\centering\small
\begin{tabular}{rr|rrrrrrrr}
& n & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\\hline
\multirow{2}{1cm}{} & \cite{GMcKay09} & 1.25408 & 1.22556 & 1.19608 & 1.17258 & 1.15403 & 1.13910 & 1.12684 & 1.11627\\
& VolEsti & 0.99485 & 1.09315 & 1.00029 & 1.00830 & 1.00564 & 0.99440 & 0.99313 & 1.01525\\  
\end{tabular} 
\caption{ Comparison between asymptotic and experimental approximation of the volume of .
\label{table:birk_asymptotic}}
\end{table}

\vspace{.5em}
\textbf{Birkhoff polytopes}
are well studied in combinatorial geometry and offer an important benchmark.
The -th Birkhoff polytope 
 
also described as the polytope of the 
perfect matchings of the complete bipartite graph , 
the polytope of the  doubly stochastic matrices, 
and the Newton polytope of the determinant.
In~\cite{BeckPixton03}, they present a complex-analytic method for 
this volume, implemented in package {\tt birkhoff}, which has managed
to compute \vol in parallel execution, 
which corresponds to a single processor running at 1~GHz for almost 17~years.

First, : we project  to
a subspace of this dimension.
Our software, with ,  computes the volume of polytopes up to 
in ~hour with mean error of  (Table~\ref{table:vol_results}).
The computed approximation values improves upon the best known upper bounds on , obtained through the asymptotic formula of~\cite{GMcKay09}, cf.\ Table~\ref{table:birk_asymptotic}.
By setting  we obtain an error of  for \vol,
in ~hours. The computed approximation of the volume has two correct digits, i.e.\ its first two digits equal to the ones of the exact volume.
More interestingly, using  we compute, in  hours, an approximation as well as an interval of values for \vol,\dots, \vol,
whose exact values are unknown (Table~\ref{table:vol_results}).

\section{Further work}\label{sec:further}

NN search seems promising and could accelerate our
code, especially if it were performed approximately with
hyperplane queries.
Producing (almost) uniform point samples
is of independent interest in machine learning, including
sampling contingency tables and learning the p-value. We plan
to exploit such applications of our software. 
We may also study sampling for special polytopes such as Birkhoff.
It is straightforward to parallelize certain aspects of the algorithm, 
such as random walks assigning each thread to a processor,
though other aspects, such as the algorithm's phases,
require more sophisticated parallelization.
Our original motivation and ultimate goal is to extend these methods to
V-polytopes represented by an optimization oracle.

\section{Acknowledgments}
This work is  
co-financed by the European Union (European Social Fund - ESF) and Greek national funds through the Operational Program ``Education and Lifelong Learning" of the National Strategic Reference Framework (NSRF) - Research Funding Program: THALIS - UOA (MIS 375891).
The authors acknowledge discussions with Matthias Beck on Birkhoff polytopes,
and Andreas Enge on {\tt VINCI}, and help with experiments
by Ioannis Psarros and Georgios Samaras, students at UoA.

\bibliographystyle{plain} \bibliography{../bibliography} 

\end{document}
