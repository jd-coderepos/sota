\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\usepackage[switch]{lineno}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{subcaption}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand{\round}[1]{\ensuremath{\lfloor#1\rceil}}
\DeclareMathOperator{\E}{\mathbb{E}}
\usepackage{adjustbox}
\usepackage{nicefrac}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{pdfpages}

\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}



\setcounter{secnumdepth}{2} 





\title{Towards Light-weight and Real-time Line Segment Detection}
\author {


        Geonmo Gu\thanks{Authors contributed equally.},
        Byungsoo Ko\footnotemark[1],
        SeoungHyun Go,
        Sung-Hyun Lee,
        Jingeun Lee,
        Minchul Shin\\
}
\affiliations {
NAVER/LINE Vision\\
    \{korgm403, kobiso62, powflash, shlee.mars, jglee0206, min.stellastra\}@gmail.com
}



\begin{document}

\maketitle

\begin{abstract}
Previous deep learning-based line segment detection (LSD) suffers from the immense model size and high computational cost for line prediction.
This constrains them from real-time inference on computationally restricted environments.
In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments named Mobile LSD (M-LSD).
We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction found in previous methods.
To maintain competitive performance with a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation, matching and geometric loss.
SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process.
Moreover, the matching and geometric loss allow a model to capture additional geometric cues.
Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSD-tiny) achieves competitive performance with 2.5\% of model size and an increase of 130.5\% in inference speed on GPU.
Furthermore, our model runs at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively.
To the best of our knowledge, this is the first real-time deep LSD available on mobile devices.
Our code is available~\footnote{https://github.com/navervision/mlsd}.
\end{abstract}

\section{Introduction}

Line segments and junctions are crucial visual features in low-level vision, which provide fundamental information to the higher level vision tasks, such as pose estimation~\cite{pvribyl2017absolute,xu2016pose}, structure from motion~\cite{bartoli2005structure,micusik2017structure}, 3D reconstruction~\cite{denis2008efficient, faugeras1992depth}, image matching~\cite{xue2017anisotropic}, wireframe to image translation~\cite{xue2019neural} and image rectification~\cite{xue2019rectification}.
Moreover, the growing demand for performing such vision tasks on resource constraint platforms, like mobile or embedded devices, has made real-time line segment detection (LSD) an essential but challenging task. 
The difficulty arises from the limited computational power and model size when finding the best accuracy and resource-efficiency trade-offs to achieve real-time inference.

\begin{figure}[t!]
\centering
\includegraphics[width=0.8\columnwidth]{figures/teaser.png}
\caption{Comparison of M-LSD and existing LSD methods on Wireframe dataset. Inference speed (FPS) is computed on Tesla V100 GPU. Size and value of circles indicate the number of model parameters (Millions). M-LSD achieves competitive performance with the lightest model size and the fastest inference speed. Details are in Table~\ref{table:sota}.}
\label{fig:teaser}
\end{figure}


\begin{figure*}[t!]
\includegraphics[width=0.9\textwidth]{figures/strategies_speed.png}
\caption{(a) Previous LSD methods exploit multi-module processing for line segment prediction. In contrast, our method directly predicts line segments from feature maps with a single module. (b) Our method shows superior speed on backbone and line prediction by employing a light-weight network with a single module of line prediction.}\label{fig:strategy}
\end{figure*}

\begin{comment}
\begin{table*}[t!]
\begin{adjustbox}{width=1.0\columnwidth,center}
\begin{tabular}{clcccc}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Stra-\\ tegy\end{tabular}}    & \multirow{2}{*}{Method} & \multirow{2}{*}{Input} & \multicolumn{3}{c}{Inference speed (FPS)} \\ \cline{4-6} 
                      &                          &                        & Backbone      & Prediction      & Total      \\
\midrule
TD                    & AFM                      & 320                    & 77.1          & 17.3         & 14.1       \\ \hline
\multirow{3}{*}{BU}   & L-CNN                    & 512                    & 55.2          & 23.8         & 16.6       \\
                      & L-CNN-P                  & 512                    & 55.2          & 0.4          & 0.4        \\
                      & HAWP                     & 512                    & 55.0          & 82.2         & 32.9       \\ \hline
\multirow{4}{*}{TP}   & TP-LSD-Lite              & 320                    & 138.4         & 234.6        & 87.1       \\
                      & TP-LSD-Res34             & 320                    & 129.0         & 71.0         & 45.8       \\
                      & TP-LSD-Res34             & 512                    & 128.8         & 23.7         & 20.0       \\ 
                     & TP-LSD-HG                & 512                    & 64.7         & 200.5         & 48.9       \\ \hline
\multirow{4}{*}{Ours} & M-LSD-tiny               & 320                    & \textbf{241.1}& \textbf{1202.8}& \textbf{200.8}      \\
                      & M-LSD-tiny               & 512                    & 201.6         & 881.9       & 164.1      \\
                      & M-LSD                    & 320                    & 156.3         & 1194.7       & 138.2      \\
                      & M-LSD                    & 512                    & 132.8         & 883.4        & 115.4 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Inference speed of backbone, prediction modules, and total on GPU. Strategies are from Figure~\ref{fig:strategy}. Our method shows superior speed on backbone and line prediction by employing a light-weight network with a single module of line prediction.}
\label{table:strategy_speed}
\end{table*}
\end{comment}

With the advent of deep neural networks, deep learning-based LSD architectures have adopted models to learn various geometric cues of line segments and have proved to show improvements in performance.
As described in Figure~\ref{fig:strategy}, we have summarized multiple strategies that use deep learning models for LSD.
The top-down strategy~\cite{xue2019learning} first detects regions of line segment with attraction field maps and then squeezes these regions into line segments to make predictions.
In contrast, the bottom-up strategy first detects junctions, then arranges them into line segments, and lastly verifies the line segments by using an extra classifier~\cite{zhou2019end,xue2020holistically,zhang2019ppgnet} or a merging algorithm~\cite{huang2019wireframe,huang2018learning}.
Recently, \cite{huang2020tp} proposes Tri-Points (TP) representation for a simpler process of line prediction without the time-consuming steps of line proposal and verification.

Although previous efforts of using deep networks have made remarkable achievements, real-time inference for LSD on resource-constraint platforms still remains limited.
There have been attempts to present real-time LSD~\cite{huang2020tp,meng2020lgnn,xue2020holistically}, but they still depend on server-class GPUs.
This is mainly because the models that are used exploit heavy backbone networks, such as dilated ResNet50-based FPN~\cite{zhang2019ppgnet}, stacked hourglass network~\cite{meng2020lgnn,huang2020tp}, and atrous residual U-net~\cite{xue2019learning}, which require large memory and high computational power.
In addition, as shown in Figure~\ref{fig:strategy}, the line prediction process consists of multiple modules, which include line proposal~\cite{xue2019learning,zhang2019ppgnet,zhou2019end,xue2020holistically}, line verification networks~\cite{zhang2019ppgnet,zhou2019end,xue2020holistically} and mixture of convolution module~\cite{huang2020tp,huang2018learning}.
As the size of the model and the number of modules for line prediction increase, the overall inference speed of LSD can become slower, as shown in Figure~\ref{fig:strategy}b, while demanding higher computation.
Thus, increases in computational cost make it difficult to deploy LSD on resource-constraint platforms.

In this paper, we propose a real-time and light-weight LSD for resource-constrained environments, named Mobile LSD (M-LSD).
For the network, we design a significantly efficient architecture with a single module to predict line segments.
By minimizing the network size and removing the multi-module process from previous methods, M-LSD is extremely light and fast.
To maintain competitive performance even with a light-weight network, we present novel training schemes: SoL augmentation, matching and geometric loss.
SoL augmentation divides a line segment into subparts, which are further used to provide augmented line data during the training phase.
Matching and geometric loss train a model with additional geometric information, including relation between line segments, junction and line segmentation, length and degree regression.
As a result, our model is able to capture extra geometric information during training to make more accurate line predictions.
Moreover, the proposed training schemes can be used with existing methods to further improve performance in a plug-and-play manner.

As shown in Figure~\ref{fig:teaser}, our methods achieve competitive performance and faster inference speed with a much smaller model size.
M-LSD outperforms previously the real-time method, TP-LSD-Lite~\cite{huang2020tp}, with only 6.3\% of the model size but gaining an increase of 32.5\% in inference speed.
Moreover, M-LSD-tiny runs in real-time at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively.
To the best of our knowledge, this is the first real-time LSD method available on mobile devices.

\section{Related Works}

\begin{comment}
\begin{figure}[t!]
\centering
\includegraphics[width=0.8\columnwidth]{figures/framework_vertical.png}
\caption{An overview of M-LSD network architecture. The final feature maps are extracted from the feature extractor, which consists of TP, SoL, and segmentation maps. The predicted line segments are generated by merging center points and displacement vectors from the TP maps.}
\label{fig:framework}
\end{figure}
\end{comment}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.78\textwidth]{figures/framework.png}
\caption{The overall architecture of M-LSD. In the feature extractor, block 1 $\sim$ 14 are parts of MobileNetV2, and block 15 $\sim$ 23 are designed as a top-down architecture. The predicted line segments are generated with center and displacement maps.}
\label{fig:framework}
\end{figure*}


\textbf{Deep Line Segment Detection. }
There have been active studies on deep learning-based LSD.
In junction-based methods, DWP~\cite{huang2018learning} includes two parallel branches to predict line and junction heatmaps, followed by a merging process.
PPGNet~\cite{zhang2019ppgnet} and L-CNN~\cite{zhou2019end} utilize junction-based line segment representations with an extra classifier to verify whether a pair of points belongs to the same line segment.
Another approach uses dense prediction.
AFM~\cite{xue2019learning} predicts attraction field maps that contain 2-D projection vectors representing associated line segments, followed by a squeeze module to recover line segments.
HAWP~\cite{xue2020holistically} is presented as a hybrid model of AFM and L-CNN.
Recently, \cite{huang2020tp} devises the TP line representation to remove the use of extra classifiers or heuristic post-processing found in previous methods and proposes TP-LSD network with two branches: TP extraction and line segmentation branches.
Other approaches include the use of transformers~\cite{xu2021line} or Hough transform with deep networks~\cite{lin2020deep}.
However, it is commonly observed that the aforementioned multi-module processes restrict existing LSD to run on resource-constrained environments.


\textbf{Real-time Object Detectors. }
Real-time object detection has been an important task for deep learning-based object detection.
Object detectors proposed in earlier days, such as RCNN-series~\cite{girshick2014rcnn,girshick2015fast,ren2015faster}, consist of two-stage architecture: generating proposals in the first stage, then classifying the proposals in the second stage.
These two-stage detectors typically suffer from slow inference speed and difficulty in optimization.
To handle this problem, one-stage detectors, such as YOLO-series~\cite{redmon2016you,redmon2017yolo9000,redmon2018yolov3} and SSD~\cite{liu2016ssd}, are proposed to achieve GPU real-time inference by reducing backbone size and simplifying the two-stage process into one.
This one-stage architecture has been further studied and improved to run in real-time on mobile devices~\cite{howard2017mobilenets,sandler2018mobilenetv2,wang2018pelee,li2018tiny}.
Motivated by the transition from two-stage to one-stage architecture in object detection, we argue that the complicated multi-module processing in previous LSD can be disregarded.
We simplify the line prediction process with a single module for faster inference speed and enhance the performance by the efficient training strategies; SoL augmentation, matching and geometric loss.





\section{M-LSD for Line Segment Detection}
In this section, we present the details of M-LSD.
Our design mainly focuses on efficiency while retaining competitive performance.
Firstly, we exploit a light-weight backbone and reduce the modules involved in processing line predictions for better efficiency.
Next, we apply additional training schemes, including SoL augmentation, matching and geometric loss, to capture extra geometric cues.
As a result, M-LSD is able to balance the trade-off between accuracy and efficiency to be well suited for mobile devices.

\subsection{Network Architecture}

We design light (M-LSD) and lighter (M-LSD-tiny) models as popular encoder-decoder architectures.
In efforts to build a light-weight LSD model, our encoder networks are based on MobileNetV2~\cite{sandler2018mobilenetv2} which is well-known to run in real-time on mobile environments.
The encoder network uses parts of MobileNetV2 to make it even lighter.
As illustrated in Figure~\ref{fig:framework}, the encoder of M-LSD includes an input to 96-channel of bottleneck blocks.
The number of parameters in the encoder network is 0.56M (16.5\% of MobileNetV2), while the total parameters of MobileNetV2 are 3.4M.
For M-LSD-tiny, a slightly smaller yet faster model, the encoder network also uses parts of MobileNetV2, including an input to 64-channel of bottleneck blocks which results in a number of 0.25M (7.4\% of MobileNetV2).
The decoder network is designed using a combination of block types A, B, and C.
The expansive path consists of concatenation of feature maps from the skip connection and upscale from block type A, followed by two 3 $\times$ 3 convolutions with a residual connection in-between from block type B.
Similarly, block type C performs two 3 $\times$ 3 convolutions, the first being a dilated convolution, followed by a 1 $\times$ 1 convolution.
Please refer to the supplementary material for further details on the network architectures.

As shown in Figure~\ref{fig:strategy}b, we observe that one of the most critical bottlenecks in inference speed has been the prediction process, which contains multi-module processing from previous methods.
In this paper, we argue that the complicated multi-module can be disregarded.
As illustrated in Figure~\ref{fig:framework}, we generate line segments directly from the final feature maps in a single module process.
In the final feature maps, each feature map channel serves its own purpose:
1) TP maps have seven feature maps, including one length map, one degree map, one center map, and four displacement maps.
2) SoL maps have seven feature maps with the same configuration as TP maps.
3) Segmentation maps have two feature maps, including junction and line maps.


\begin{comment}
We use parts of MobileNetV2~\cite{sandler2018mobilenetv2} to encoder network, where M-LSD-tiny and M-LSD include input to 64-channel and 96-channel of bottleneck blocks of MobileNetV2, respectively.
As shown in Figure~\ref{fig:framework}, our proposed M-LSD includes a feature extractor as popular encoder-decoder architectures and final feature maps, which consists of TP, SoL, and segmentation maps.
In efforts to build a light-weight LSD model, we use parts of MobileNetV2~\cite{sandler2018mobilenetv2} to encoder network, where M-LSD-tiny and M-LSD include input to 64-channel and 96-channel of bottleneck blocks of MobileNetV2, respectively.
For the decoder network, we design the architecture similar to~\cite{ronneberger2015u} and remove the typical multi-module process for line prediction in previous methods.
To be more specific, M-LSD performs line prediction directly from the final feature maps for the best efficiency, without additional modules, such as squeeze module, LoI pooling, a mixture of convolution and etc.
In the final feature maps, each feature map channel serves its own purpose:
1) TP maps have seven feature maps, including one length map, one degree map, one center map, and four displacement maps.
2) SoL maps have seven feature maps with the same configuration as TP maps.
3) Segmentation maps have two feature maps, including junction and line maps.
Please refer to the supplementary material for further details on the architectures of M-LSD-tiny and M-LSD.
\end{comment}

\begin{comment}
We design light (M-LSD) and lighter (M-LSD-tiny) models as popular encoder-decoder architectures.
In efforts to build a light-weight LSD model, our encoder networks are based on MobileNetV2~\cite{sandler2018mobilenetv2} which is well-known to run in real-time on mobile environments.
The encoder network uses parts of MobileNetV2 (block $1 \sim 11$ of the feature extractor in Figure~\ref{fig:framework}) to make it even lighter, which includes an input to 64-channel of bottleneck blocks.
The number of parameters in the encoder network is 0.25M (7.4\% of MobileNetV2) while the total parameters of MobileNetV2 are 3.4M.
For M-LSD, a slightly bigger yet more performant model, the encoder network also uses parts of MobileNetV2 including an input to 96-channel of bottleneck blocks which results to a number of 0.56M parameters (16.5\% of MobileNetV2).
The decoder network is designed using a combination of block types A, B, and C.
Block type A concatenates feature maps from skip connection and upscale.
Block type B performs two 3 $\times$ 3 convolutions with a residual connection in-between.
Similarly, block type C performs two 3 $\times$ 3 convolutions followed by a 1 $\times$ 1 convolution, where the first is a dilated convolution.
The final feature maps in M-LSD-tiny are generated by upscaling with $H/2 \times W/2 \times 16$ tensors when the input image is $H \times W \times 3$.
On the other hand, M-LSD uses the feature map from block type C as a final feature map with the same size of $H/2 \times W/2 \times 16$.
\end{comment}

\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.8\columnwidth]{figures/TP.png}
         \caption{TP representation}
         \label{fig:tp}
     \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.8\columnwidth]{figures/SoL.png}
         \caption{SoL augmentation}
         \label{fig:sol}
     \end{subfigure}
\caption{Tri-Points (TP) representation and Segments of Line segment (SoL) augmentation. $l_s$, $l_c$, and $l_e$ denote start, center, and end points, respectively. $d_s$ and $d_e$ are displacement vectors to start and end points. $l_0 \sim l_2$ indicates internally dividing points of the line segment $\overline{l_s l_e}$.}
\label{fig:line_representation}
\end{figure}

\subsection{Line Segment Representation}
Line segment representation determines how line segment predictions are generated and ultimately affects the efficiency of LSD.
Hence, we employ the TP representation~\cite{huang2020tp} which has been introduced to have a simple line generation process and shown to perform real-time LSD using GPUs.
TP representation uses three key-points to depict a line segment: start, center, and end points.
As illustrated in Figure~\ref{fig:tp}, the start $l_s$ and end $l_e$ points are represented by using two displacement vectors ($d_s$, $d_e$) with respect to the center $l_c$ point.
The line generation process, which is to convert center point and displacement vectors to a vectorized line segment, is performed as:
\begin{eqnarray}
(x_{l_s}, y_{l_s}) = (x_{l_c}, y_{l_c}) + d_s(x_{l_c}, y_{l_c}), \nonumber \\
(x_{l_e}, y_{l_e}) = (x_{l_c}, y_{l_c}) + d_e(x_{l_c}, y_{l_c}),
\label{eq:line_gen}
\end{eqnarray}
where ($x_\alpha$, $y_\alpha$) denotes coordinates of an arbitrary $\alpha$ point.
$d_s(x_{l_c}, y_{l_c})$ and $d_e(x_{l_c}, y_{l_c})$ indicate 2D displacements from the center point $l_c$ to the corresponding start $l_s$ and end $l_e$ points.
The center point and displacement vectors are trained with one center map and four displacement maps (one for each $x$ and $y$ value of the displacement vectors $d_s$ and $d_e$).
In the line generation process, we extract the exact center point position by applying non-maximum suppression on the center map.
Next, we generate line segments with the extracted center points and the corresponding displacement vectors using a simple arithmetic operation as expressed in Equation~\ref{eq:line_gen}; thus, making inference efficient and fast.

\subsection{Matching Loss}
Following~\cite{huang2020tp}, we use the weighted binary cross-entropy (WBCE) loss and smooth L1 loss as center loss $\mathcal L_{center}$ and displacement loss $\mathcal L_{disp}$, which are for training the center and displacement map, respectively.
The line segments under the TP representation are decoupled into center points and displacement vectors, which are optimized separately.
However, the coupled information of the line segment is under-utilized in the objective functions.

To resolve this problem, we present a matching loss, which leverages the coupled information w.r.t. the ground truth.
As illustrated in Figure~\ref{fig:matching}, matching loss considers relation between line segments by guiding the generated line segments to be similar to the matched GT.
We first take the endpoints of each prediction, which can be calculated via the line generation process, and measure the Euclidean distance $d(\cdot)$ to the endpoints of the GT.
Next, these distances are used to match predicted line segments $\hat l$ with GT line segments $l$ that are under a threshold $\gamma$:
\begin{eqnarray}
d(l_s, \hat{l}_s) < \gamma \text{ and } d(l_e, \hat{l}_e) < \gamma,
\end{eqnarray}
where $l_s$ and $l_e$ are the start and end points of the line $l$, and $\gamma$ is set to 5 pixels.
Then, we obtain a set $\mathbb{M}$ of matched line segments ($l$, $\hat l$) that satisfies this condition.
Finally, the L1 loss is used for the matching loss, which aims to minimize the geometric distance of the matched line segments w.r.t the start, end, and center points as follows:
\begin{eqnarray}
\mathcal L_{match}&=&\frac{1}{\mid\mathbb{M}\mid}\sum_{(l,\hat{l}) \in \mathbb{M}} \parallel l_s-\hat{l}_s \parallel_1 + \parallel l_e-\hat{l}_e \parallel_1 \nonumber \\
&&+\parallel \tilde C(\hat{l})-(l_s + l_e)/2 \parallel_1,
\end{eqnarray}
where $\tilde C(\hat{l})$ is the center point of line $\hat{l}$ from the center map.
The total loss function for the TP map can be formulated as $\mathcal L_{TP} =  \mathcal L_{center}  + \mathcal L_{disp} + \mathcal L_{match}$.

\begin{comment}
For a loss function to train the center map, the weighted binary cross-entropy (WBCE) loss is used in \cite{huang2020tp}.
However, we observe that the number of positive (foreground) pixels is much less than that of negative (background) pixels, and such foreground-background class imbalance degrades the performance of the WBCE loss.
This is because the majority of pixels are easy negatives that contribute no useful learning signals.
Thus, we separate positive and negative terms of the binary cross-entropy loss to have the same scale, and reformulate a separated binary classification loss as follows:
\begin{eqnarray}
\label{eq:c_separate}
\resizebox{.7\hsize}{!}{$\ell_{pos}(F)=\frac{-1}{\sum_{p}I(p)}\sum_{p}W(p)\cdot log\sigma(F(p))$}, \\
\resizebox{.9\hsize}{!}{$\ell_{neg}(F)=\frac{-1}{\sum_{p}1-I(p)}\sum_{p}(1-I(p))\cdot log(1 - \sigma(F(p)))$}, \\
\ell_{cls}(F) =\lambda_{pos}\cdot \ell_{pos}(F) + \lambda_{neg}\cdot \ell_{neg}(F),
\label{eq:c_loss}
\end{eqnarray}
where $I(p)$ outputs 1 if the pixel $p$ of the GT map is non-zero, otherwise 0, $\sigma$ denotes a sigmoid function, and $W(p)$ and $F(p)$ are pixel values of the GT and feature map, respectively.
We use the center loss as $\mathcal{L}_{center}=\ell_{cls}(C)$, where $C$ denotes center map and set the weights ($\lambda_{pos}$, $\lambda_{neg}$) as (1,30).
For the displacement loss $\mathcal{L}_{disp}$, we use smooth L1 loss for regression learning as~\cite{huang2020tp}.
\end{comment}
\begin{comment}
\begin{eqnarray}
\mathcal L_{disp} =\frac{1}{\sum_{p}I(p)}\sum_{p}I(p)\cdot L_1^{smooth}(D(p), \hat{D}(p)),
\label{eq:d_loss}
\end{eqnarray}
where $D(p)$ is a pixel value in the displacement map.
\end{comment}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=1.0\columnwidth]{figures/matching.png}
         \caption{Matching loss}
         \label{fig:matching}
     \end{subfigure}
     \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.8\columnwidth]{figures/geometric.png}
         \caption{Geometric loss}
         \label{fig:geometric}
     \end{subfigure}
\caption{Matching and geometric loss. (a) Given a matched pair of a predicted line $\hat l$ and a GT line $l$, matching loss ($\mathcal L_{match}$) optimizes the predicted start, end, and center points. (b) Given a line segment, M-LSD learns various geometric cues: junction ($\mathcal L_{junc}$) and line ($\mathcal L_{line}$) segmentation, length ($\mathcal L_{length}$) and degree ($\mathcal L_{degree}$) regression.}
\label{fig:losses}
\end{figure}

\subsection{SoL Augmentation}
We propose Segments of Line segment (SoL) augmentation that increases the number of line segments with wider varieties of length for training.
Learning line segments with center points and displacement vectors can be insufficient in certain circumstances where a line segment may be too long to manage within the receptive field size or the center points of two distinct line segments may be too close to each other.
To address these issues and provide auxiliary information to the TP representation, SoL explicitly splits line segments into multiple subparts with overlapping portions of each other.
An overlap between each split is enforced to preserve connectivity among the subparts.

As described in Figure~\ref{fig:sol}, we compute $k$ internally dividing points ($l_0, l_1, \cdots, l_k$) and separate the line segment $\overline{l_sl_e}$ into $k$ subparts ($\overline{l_sl_1}$, $\overline{l_0l_2}$, $\cdots$, $\overline{l_{k-1}l_e}$).
Expressed in TP representation, each subpart is trained as if it is a typical line segment.
The number of internally dividing points $k$ is determined by the length of the line segment as $k = \round{r(l)/(\mu/2)} - 1$, where $r(l)$ denotes the length of line segment $l$, and $\mu$ is the base length of subparts.
Note that when $k \leq 1$, we do not split the line segment.
The resulting length of each subpart can be similar to $\mu$ with small margins of error due to the rounding function \round{\cdot}, and we empirically set $\mu= input\_size \times 0.125$.
The loss function of $\mathcal L_{SoL}$ follows the same configuration as $\mathcal L_{TP}$, while each subpart is treated as an individual line segment.
Note that the line generation process is only done in TP maps, not in SoL maps.


\begin{table}[t!]
\centering
\begin{adjustbox}{width=1.0\columnwidth,center}
\begin{tabular}{clccc}
\toprule
M & Schemes                & $F^H$ & $sAP^{10}$ & $LAP$ \\
\midrule
1 & Baseline    &   74.3   & 48.9  &  48.1  \\
2 & + Matching loss  &   75.4 \textcolor{Green}{(+1.1)}  & 52.2 \textcolor{Green}{(+3.3)}  &  52.5 \textcolor{Green}{(+4.4)} \\
3 & + Geometric loss     &   76.2 \textcolor{Green}{(+0.8)}  & 55.1 \textcolor{Green}{(+2.9)} &   55.3 \textcolor{Green}{(+2.8)}  \\
4 & + SoL augmentation      &   \textbf{77.2} \textcolor{Green}{(+1.0)}  & \textbf{58.0} \textcolor{Green}{(+2.9)} &   \textbf{57.9} \textcolor{Green}{(+2.6)}  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation study of M-LSD-tiny on Wireframe. The baseline is M-LSD-tiny trained with only TP representation. M denotes model number.}
\label{table:ablation}
\end{table}

\subsection{Learning with Geometric Information}
To boost the quality of predictions, we incorporate various geometric information about line segments which helps the overall learning process.
In this section, we present learning LSD with junction and line segmentation, and length and degree regression for additional geometric information.


\subsubsection{Junction and Line Segmentation}
Center point and displacement vectors are highly related to pixel-wise junctions and line segments in the segmentation maps of Figure~\ref{fig:framework}.
For example, end points, derived from the center point and displacement vectors, should be the junction points.
Also, center points must be localized on the pixel-wise line segment.
Thus, learning the segmentation maps of junctions and line segments works as a spatial attention cue for LSD.
As illustrated in Figure~\ref{fig:framework}, M-LSD contains segmentation maps, including a junction map and a line map.
We construct the junction GT map by scaling with Gaussian kernel as the center map, while using a binary map for line GT map.
The total segmentation loss is defined as $\mathcal L_{seg} =  \mathcal L_{junc}  + \mathcal L_{line}$, where we use WBCE loss for both $\mathcal L_{junc}$ and $\mathcal L_{line}$.


\subsubsection{Length and Degree Regression}
As displacement vectors can be derived from the length and degree of line segments, they can be additional geometric cues to support the displacement maps.
We compute the length and degree from the ground truth and mark the values on the center of line segments in each GT map.
Next, these values are extrapolated to a $3 \times 3$ window so that all neighboring pixels of a given pixel contain the same value.
As shown in Figure~\ref{fig:framework}, we maintain predicted length and degree maps for both TP and SoL maps, where TP uses the original line segment and SoL uses augmented subparts.
As the ranges of length and degree are wide, we divide each length by the diagonal length of the input image for normalization.
For degree, we divide each degree by $2\pi$ and add 0.5.
The total regression loss can be formulated as $\mathcal L_{reg} = \mathcal L_{length} + \mathcal L_{degree}$, where we use smooth L1 loss for both $\mathcal L_{length}$ and $\mathcal L_{degree}$.

\begin{comment}
\begin{eqnarray}
L_{length} =\frac{1}{\sum_{p}I(p)}\sum_{p}I(p)\cdot L_1(M(p), \hat{M}(p))
\end{eqnarray}
\begin{eqnarray}
L_{degree} =\frac{1}{\sum_{p}I(p)}\sum_{p}I(p)\cdot L_1(D(p), \hat{D}(p))
\end{eqnarray}
\end{comment}

\subsection{Final Loss Functions}
The geometric loss function is defined as the sum of segmentation and regression loss:
\begin{eqnarray}
\label{eq:geo}
\mathcal L_{Geo} = \mathcal L_{seg} + \mathcal L_{reg}.
\end{eqnarray}
The loss function for SoL maps $\mathcal L_{SoL}$ follows the same formulation as $\mathcal L_{TP}$ but with SoL augmented GT.
Finally, we obtain the final loss function to train M-LSD as follows:
\begin{eqnarray}
\label{eq:total}
\mathcal L_{total} &=& \mathcal L_{TP} + \mathcal L_{SoL} + \mathcal L_{Geo}.
\end{eqnarray}
Please refer to the supplementary material for further details on the feature maps and losses.

\begin{comment}
The loss function for TP maps $\mathcal L_{TP}$ is defined as the sum of center and displacement loss, length and degree regression loss, and a matching loss:
\begin{eqnarray}
\label{eq:tp}
\mathcal L_{TP} = \mathcal L_{center} + \mathcal L_{disp} + \mathcal L_{length} + \mathcal L_{degree} + \mathcal L_{match}.
\end{eqnarray}
The loss function for SoL maps $\mathcal L_{SoL}$ follows the same formulation as Equation~\ref{eq:tp} but with SoL augmented GT.
Finally, we obtain the final loss function $\mathcal L_{total}$ as follows:
\begin{eqnarray}
\label{eq:total}
\mathcal L_{total} &=& \mathcal L_{TP} + \mathcal L_{SoL} + \mathcal L_{seg}.
\end{eqnarray}
As illustrated in Figure~\ref{fig:framework}, $\mathcal L_{TP}$ and $\mathcal L_{SoL}$ each optimizes the line representation maps and $\mathcal L_{seg}$ optimizes the segmentation maps.
\end{comment}



\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.55\columnwidth]{figures/match_x.png}
         \caption{Baseline (M1)}
         \label{fig:match_x}
     \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.55\columnwidth]{figures/match_o.png}
         \caption{w/ matching loss (M2)}
         \label{fig:match_o}
     \end{subfigure}
     \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.55\columnwidth]{figures/sol_x.png}
         \caption{w/ geometric loss (M3)}
         \label{fig:sol_x}
     \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering\includegraphics[width=0.55\columnwidth]{figures/sol_o.png}
         \caption{w/ SoL augmentation (M4)}
         \label{fig:sol_o}
     \end{subfigure}
\caption{Saliency maps generated from TP center map. Model numbers (M1$\sim$4) are from Table~\ref{table:ablation}.}
\label{fig:comparison_cam}
\end{figure}

\section{Experiments}
In this section, we conduct extensive ablation studies, quantitative and qualitative analysis of the proposed method.
For better understanding, we add extended experiments in the supplementary material, including ablation study of architecture, SoL augmentation, application example and so on.

\begin{table*}[t!]
\centering
\begin{adjustbox}{width=0.8\textwidth,center}
\begin{tabular}{lcccccccccccc}
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Input} & \multicolumn{4}{c}{Wireframe} & \multicolumn{4}{c}{YorkUrban} & \multirow{2}{*}{Params(M)} & \multirow{2}{*}{FPS} \\ \cline{3-10}
                         &                        & F$^H$   & sAP$^5$ & sAP$^{10}$ & LAP  & F$^H$   & sAP$^5$ & sAP$^{10}$ & LAP  &                            &                      \\
\midrule
LSD~\cite{von2008lsd}    & 320                    & 64.1  & 6.7   & 8.8    & 18.7 & 60.6  & 7.5   & 9.2    & 16.1 & -                           & 100.0$^{\dagger}$                \\
DWP~\cite{huang2018learning}& 512                    & 72.7  & 3.7     & 5.1    & 6.6  & 65.2  & 2.8     & 2.6    & 3.1  & 33.0                       & 2.2                  \\
AFM~\cite{xue2019learning}& 320                    & 77.3  & 18.3  & 23.9   & 36.7 & 66.3  & 7.0   & 9.1    & 17.5 & 43.0                       & 14.1                 \\
LGNN~\cite{meng2020lgnn} & 512                    & -     & -     & 62.3   & -    & -     & -     & -      & -    & -                           & 15.8$^{\ddagger}$                 \\
LGNN-lite~\cite{meng2020lgnn}& 512                    & -     & -     & 57.6   & -    & -     & -     & -      & -    & -                          & 34.0$^{\ddagger}$                 \\
TP-LSD-Lite~\cite{huang2020tp}& 320                    & 80.4  & 56.4  & 59.7   & 59.7 & \textcolor{blue}{\textbf{68.1}}  & 24.8  & 26.8   & 31.2 & 23.9                       & \textcolor{blue}{87.1}                 \\
TP-LSD-Res34~\cite{huang2020tp}& 320                    & 81.6  & 57.5  & 60.6   & 60.6 & 67.4  & 25.3  & 27.4   & 31.1 & 23.9                       & 45.8                 \\
TP-LSD-Res34~\cite{huang2020tp}& 512                    & 80.6  & 57.6  & 57.2   & 61.3 & 67.2  & \textcolor{blue}{\textbf{27.6}}  & 27.7   & \textcolor{blue}{\textbf{34.3}} & 23.9                       & 20.0                 \\
TP-LSD-HG~\cite{huang2020tp}& 512                    & 82.0  & 50.9  & 57.0   & 55.1 & 67.3  & 18.9  & 22.0   & 24.6 & \textcolor{blue}{7.4}                        & 48.9                 \\
LETR~\cite{xu2021line}& 1100$^{\ast}$                    & \textcolor{blue}{\textbf{82.6}}  & 59.2  & 65.6   & \textcolor{blue}{\textbf{65.1}} & 66.6  & 24.0  & 27.6   & 32.5 & 121.2                        & 5.4                 \\
L-CNN~\cite{zhou2019end} & 512                    & 77.5  & 58.9  & 62.8   & 59.8 & 64.6  & 25.9  & 28.2   & 32.0 & 9.8                        & 16.6                 \\
HAWP~\cite{xue2020holistically}& 512                    & 80.3  & 62.5  & 66.5   & 62.9 & 64.8  & 26.1  & \textcolor{blue}{\textbf{28.5}}   & 30.4 & 10.4                       & 32.9                 \\
HT-L-CNN~\cite{lin2020deep}& 512                    & -  & 60.3  & 64.2   & -  & -  & 25.7   & 28.0 & - & 9.3                       & 7.5$^{\ddagger}$                 \\
HT-HAWP~\cite{lin2020deep}& 512                    & -  & \textcolor{blue}{62.9}  & \textcolor{blue}{66.6}   & - & -  & 25.0  & 27.4   & - & 10.5                       & 12.2$^{\ddagger}$                 \\
\midrule
L-CNN + \textit{M-LSD-s} & 512                    & 80.7  & 59.4  & 63.7   & 63.8 & 66.5  & \textcolor{Red}{27.5}  & 28.1   & 31.7 & 9.8                        & 16.6                 \\
HAWP + \textit{M-LSD-s} & 512                    & \textcolor{Red}{82.5}  & \textcolor{Red}{\textbf{63.3}}  & \textcolor{Red}{\textbf{67.1}}   & \textcolor{Red}{64.2} & \textcolor{Red}{66.7}  & \textcolor{Red}{27.5}  & \textcolor{Red}{\textbf{28.5}}   & \textcolor{Red}{32.4} & 10.4                       & 32.9                 \\
\textit{M-LSD-tiny}              & 320                    & 76.8  & 43.0  & 51.3   & 50.1 & 61.9  & 17.4  & 21.3   & 23.7 & \textcolor{Red}{\textbf{0.6}}                        & \textcolor{Red}{\textbf{200.8}}                \\
\textit{M-LSD-tiny}              & 512                    & 77.2  & 52.3  & 58.0   & 57.9 & 62.4  & 22.1  & 25.0   & 28.3 & \textcolor{Red}{\textbf{0.6}}                        & 164.1                \\
\textit{M-LSD}                   & 320                    & 78.7  & 48.2  & 55.5   & 55.7 & 63.4  & 20.2  & 23.9   & 27.7 & 1.5                        & 138.2                \\
\textit{M-LSD}                   & 512                    & \textcolor{Black}{80.0}  & \textcolor{Black}{56.4}  & \textcolor{Black}{62.1}   & \textcolor{Black}{61.5} & \textcolor{Black}{64.2}  & \textcolor{Black}{24.6}  & \textcolor{Black}{27.3}   & \textcolor{Black}{30.7} & 1.5                        & 115.4               \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Quantitative comparisons with existing LSD methods. FPS is evaluated in Tesla V100 GPU, where $^{\dagger}$ denotes CPU FPS and $^{\ddagger}$ denotes the values from the corresponding paper due to no published or incomplete implementation. $\ast$ denotes resizing the image with the shortest side at least 1100 pixels. M-LSD-s indicates the proposed training schemes. The best scores among previous methods, our models, and all together are marked in \textcolor{blue}{blue}, \textcolor{red}{red}, and \textbf{bold}, respectively.}
\label{table:sota}
\end{table*}

\subsection{Experimental Setting}
\textbf{Dataset and Evaluation Metrics. }
We evaluate our model with two famous LSD datasets: \textit{Wireframe}~\cite{huang2018learning} and \textit{YorkUrban}~\cite{denis2008efficient}.
The Wireframe dataset consists of 5,000 training and 462 test images of man-made environments, while the YorkUrban dataset has 102 test images.
Following the typical training and test protocol~\cite{huang2020tp,zhou2019end}, we train our model with the training set from the Wireframe dataset and test with both Wireframe and YorkUrban datasets.
We evaluate our models using prevalent metrics for LSD~\cite{huang2020tp,zhang2019ppgnet,meng2020lgnn,xue2019learning,zhou2019end} that include: heatmap-based metric $F^H$, structural average precision (sAP), and line matching average precision (LAP).

\textbf{Optimization. }
We train our model on Tesla V100 GPU.
We use the TensorFlow~\cite{abadi2016tensorflow} framework for model training and TFLite~\footnote{www.tensorflow.org/lite} for porting models to mobile devices.
Input images are resized to $320 \times 320$ or $512 \times 512$ in both training and testing, which are specified in each experiment.
The input augmentation consists of horizontal and vertical flips, shearing, rotation, and scaling.
We use ImageNet~\cite{deng2009imagenet} pre-trained weights on the parts of MobileNetV2~\cite{sandler2018mobilenetv2} in M-LSD and M-LSD-tiny.
Our model is trained using the Adam optimizer~\cite{kingma2014adam} with a learning rate of 0.01.
We use linear learning rate warm-up for 5 epochs and cosine learning rate decay~\cite{loshchilov2016sgdr} from 70 epoch to 150 epoch.
We train the model for a total of 150 epochs with a batch size of 64.


\subsection{Ablation Study and Interpretability}
\label{sec:ablation}
We conduct a series of ablation experiments to analyze our proposed method.
M-LSD-tiny is trained and tested on the Wireframe dataset with an input size of $512 \times 512$.
As shown in Table~\ref{table:ablation}, all the proposed schemes contribute to a significant performance improvement.
In addition, we include saliency map visualizations generated from each feature map to analyze networks learned from each training scheme in Figure~\ref{fig:comparison_cam} using GradCam~\cite{selvaraju2017grad}.
The saliency map interprets important regions and importance levels on the input image by computing the gradients from each feature map.

\begin{comment}
\textbf{Baseline and Augmentation. }
The baseline model is trained with M-LSD-tiny backbone, including only the TP representation and no other proposed schemes.
We observe that adding horizontal and vertical flips, shearing, rotation, and scaling input augmentation on the baseline model shows small performance improvement.
\end{comment}

\textbf{Matching Loss. }
Integrating matching loss shows performance boosts on both pixel localization accuracy and line prediction quality.
We observe weak attention on center points from the baseline saliency maps in Figure~\ref{fig:match_x}, while w/ matching loss amplifies the attention on center points in Figure~\ref{fig:match_o}.
This demonstrates that training with coupled information of center points and displacement vectors allows the model to learn with more line-awareness features.

\textbf{Geometric Loss. }
Adding geometric loss gives performance boosts in every metric.
Moreover, the saliency map of Figure~\ref{fig:sol_x} shows more distinct and stronger attention on center points and line segments as compared to that of saliency maps w/ matching loss in Figure~\ref{fig:match_o}.
It shows that geometric information work as spatial attention cues for training.

\textbf{SoL Augmentation. }
Integrating SoL augmentation shows significant performance boost.
In the saliency maps of Figure~\ref{fig:sol_x}, w/ geometric loss shows strong but vague attention on center points with disconnected line attention for long line segments.
This can be a problem because the entire line information is essential to compute the center point.
In contrast, w/ SoL augmentation in Figure~\ref{fig:sol_o} shows more precise center point attention as well as clearly connected line attention.
This demonstrates that augmenting line segments by the number and length guides the model to be more robust in pixel-based and line matching-based qualities.

\subsection{Comparison with Other Methods}
\label{sec:sota}
As shown in Table~\ref{table:sota}, we conduct experiments that combine the proposed training schemes (SoL augmentation, matching and geometric loss) with existing methods.
Finally, we compare our proposed M-LSD and M-LSD-tiny with the previous state-of-the-art methods.

\textbf{Existing methods with M-LSD Training Schemes. }
As our proposed training schemes can be used with existing LSD methods, we demonstrate this using L-CNN and HAWP following Deep Hough Transform (HT)~\cite{lin2020deep}, a recently proposed combinable method.
L-CNN + HT (HT-L-CNN) shows a performance boost of 1.4\% while L-CNN + M-LSD-s shows a boost of 0.9\% in $sAP^{10}$.
HAWP + HT (HT-HAWP) shows 0.1\% of performance boost, while HAWP + M-LSD-s shows 0.6\% of performance boost in $sAP^{10}$, which makes the combination one of the state-of-the-art performance.
Thus, it demonstrates that the proposed training schemes are flexible and powerful to use with existing LSD methods.

\textbf{M-LSD and M-LSD-tiny. }
Our proposed models achieve competitive performance and the fastest inference speed even with a limited model size.
In comparison with the previous fastest model, TP-LSD-Lite, M-LSD with input size of 512 shows higher performance and an increase of 32.5\% in inference speed with only 6.3\% of the model size.
Our fastest model, M-LSD-tiny with 320 input size, has a slightly lower performance than that of TP-LSD-Lite, but achieves an increase of 130.5\% in inference speed with only 2.5\% of the model size.
Compared to the previous lightest model TP-LSD-HG, M-LSD with 512 input size outperforms on $sAP^{5}$, $sAP^{10}$ and $LAP$ with an increase of 136.0\% in inference speed with 20.3\% of the model size.
Our lightest model, M-LSD-tiny with 320 input size, shows an increase of 310.6\% in the inference speed with 8.1\% of the model size compared to TP-LSD-HG.
Previous methods can be deployed as real-time line segment detectors on server-class GPUs, but not on resource-constrained environments either because the model size is too large or the inference speed is too slow.
Although M-LSD does not achieve state-of-the-art performance, it shows competitive performance and the fastest inference speed with the smallest model size, offering the potential to be used in real-time applications on resource-constrained environments, such as mobile devices.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{figures/vis.png}
\caption{Qualitative evaluation of M-LSD-tiny and M-LSD on WireFrame dataset.}
\label{fig:vis}
\end{figure}


\subsection{Visualization}
We visualize outputs of M-LSD and M-LSD-tiny in Figure~\ref{fig:vis}.
Junctions and line segments are colored with cyan blue and orange, respectively.
Compared to the GT, both models are capable of identifying junctions and line segments with high precision even in complicated low contrast environments such as (a) and (c).
Although the results of M-LSD-tiny may have a few small line segments missing and junctions incorrectly connected, the fundamental line segments to identify the environmental structure are accurate.

The goal of our model is to detect the structural line segments as~\cite{huang2018learning} while avoiding texture and photometric line segments.
However, we observe that some are included in our results, such as texture on the floor in (b) and shadow on the wall in (d).
We acknowledge this to be a common problem for existing methods, and considering texture and photometric features for training would be great future work.
We include more visualizations with a comparison of existing methods in the supplementary material.


\subsection{Deployment on Mobile Devices}
We deploy M-LSD on mobile devices and evaluate the memory usage and inference speed.
We use iPhone 12 Pro with A14 bionic chipset and Galaxy S20 Ultra with Snapdragon 865 ARM chipset.
As shown in Table~\ref{table:mobile}, M-LSD-tiny and M-LSD are small enough to be deployed on mobile devices where memory requirements range between 78MB and 508MB.
The inference speed of M-LSD-tiny is fast enough to be real-time on mobile devices where it ranges from a minimum of 17.9 FPS to a maximum of 56.8 FPS.
M-LSD still can be real-time with 320 input size, however, with 512 input size, FP16 may be required for a faster FPS over 10.
Overall, as all our models have small memory requirements and fast inference speed on mobile devices, the exceptional efficiency allows M-LSD variants to be used in real-world applications.
To the best of our knowledge, this is the first and the fastest real-time line segment detector on mobile devices ever reported.


\begin{table}[t!]
\centering
\begin{adjustbox}{width=1.0\columnwidth,center}
\begin{tabular}{lcccccc}
\toprule
Model                       & Input                & Device                   & FP & Latency (ms) & FPS  & Memory (MB) \\
\midrule
\multirow{8}{*}{M-LSD-tiny} & \multirow{4}{*}{320} & \multirow{2}{*}{iPhone}  & 32 & 30.6         & 32.7 & 169         \\
                            &                      &                          & 16 & 20.6         & 48.6 & 111         \\ \cline{3-7} 
                            &                      & \multirow{2}{*}{Android} & 32 & 31.0         & 32.3 & 103         \\
                            &                      &                          & 16 & \textbf{17.6}         & \textbf{56.8} & \textbf{78}          \\ \cline{2-7} 
                            & \multirow{4}{*}{512} & \multirow{2}{*}{iPhone}  & 32 & 51.6         & 19.4 & 203         \\
                            &                      &                          & 16 & 36.8         & 27.1 & 176         \\ \cline{3-7} 
                            &                      & \multirow{2}{*}{Android} & 32 & 55.8         & 17.9 & 195         \\
                            &                      &                          & 16 & \textbf{25.4}         & \textbf{39.4} & \textbf{129}         \\
\midrule
\multirow{8}{*}{M-LSD}      & \multirow{4}{*}{320} & \multirow{2}{*}{iPhone}  & 32 & 74.5         & 13.4 & 241         \\
                            &                      &                          & 16 & 46.4         & 21.6 & 188         \\ \cline{3-7} 
                            &                      & \multirow{2}{*}{Android} & 32 & 82.4         & 12.1 & 236         \\
                            &                      &                          & 16 & \textbf{38.4}         & \textbf{26.0} & \textbf{152}         \\ \cline{2-7} 
                            & \multirow{4}{*}{512} & \multirow{2}{*}{iPhone}  & 32 & 121.6        & 8.2  & 327         \\
                            &                      &                          & 16 & 90.7         & 11.0 & \textbf{261}         \\ \cline{3-7} 
                            &                      & \multirow{2}{*}{Android} & 32 & 177.3        & 5.6  & 508         \\
                            &                      &                          & 16 & \textbf{79.0}         & \textbf{12.7} & 289         \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Inference speed and memory usage on iPhone (A14 Bionic chipset) and Android phone (Snapdragon 865 chipset). FP denotes floating point.}
\label{table:mobile}
\end{table}

\section{Conclusion}
We introduce M-LSD, a light-weight and real-time line segment detector for resource-constrained environments.
Our model is designed with a significantly efficient network architecture and a single module process to predict line segments.
To maintain competitive performance even with a light-weight network, we present novel training schemes: SoL augmentation, matching and geometric loss.
As a result, our proposed method achieves competitive performance and the fastest inference speed with the lightest model size.
Moreover, we show that M-LSD is deployable on mobile devices in real-time, which demonstrates the potential to be used in real-time mobile applications.
     
\bibliography{aaai22}

\clearpage

\includepdf[pages={1-}]{supplementary.pdf}

\end{document}
