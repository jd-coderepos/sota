\section{Experiments}

\subsection{Datasets}
We evaluate the performance of our proposed model extensively on three challenging action segmentation datasets (50Salads \cite{stein2013combining}, GTEA \cite{fathi2011learning}, and Breakfast \cite{kuehne2014language}).  
We follow previous work~\cite{asrf,asformer,li2020ms,farha2019ms,wang2020boundary,chen2020action} and perform 4-fold cross-validation on Breakfast and GTEA and 5-fold cross-validation on 50Salads.

\subsection{Evaluation Metrics}
For evaluation,  following previous works, we report the  frame-wise accuracy (Acc), segmental edit score (Edit), and the segmental F1 score at overlapping thresholds $10\%$, $25\%$, and $50\%$, denoted by $\text{F1@}\{10, 25, 50\}$~\cite{lea2017temporal}. The overlapping threshold is determined based on the intersection over union (IoU) ratio. 
Although frame-wise accuracy is the most commonly used metric for action segmentation, it does not portray a thorough picture of the performance of action segmentation models. 
A major disadvantage of frame-wise accuracy is that long action classes have a higher impact than short action classes and dominate the results. Furthermore, over-segmentation errors have a relatively low impact on Acc, which is particularly problematic for applications such as video summarization.  
On the other hand, Edit and F1 scores establish more comprehensive measures of the quality of the segmentations~\cite{lea2017temporal}; Edit measures the quality of the predicted transcript of the segmentation, while F1 scores penalize over-segmentation and are also insensitive to the duration of the action classes. Our proposed method performs particularly well on the Edit, and F1 scores on all datasets and in fully and timestamp supervised setups, achieving state-of-the-art results in most cases.

\subsection{Implementation Details and Training}
We follow the standard training strategy from existing  algorithms \cite{farha2019ms,li2020ms,asrf,asformer,singhania2021coarse} and train our main network (Section~\ref{sec:model1}) end-to-end with batch size of $1$. We train our
model for at most $800$ epochs using Adam optimizer with learning rate $0.0005$ and the loss (\ref{eq6}). 
In the cross-attention loss, Eq.~\eqref{eq:ca2}, we set $\tau=1$ during training to ensure training stability, and $\tau=0.0001$ during inference.
As input for our model, we use the same I3D \cite{carreira2017quo} features that
were used in many previous works.
For the encoder, we used a modified version of the encoder proposed in~\cite{asformer}\footnote{Please see the supplementary material for more details, hyper-parameters, and ablations.\label{footnote1}}.
For the decoder, we use a standard decoder architecture~\cite{Vaswani:attention:2017}, with two layers and single head attention.
Due to a strong imbalance in the segment durations, we propose a \textit{split-segment} approach for improved training: longer action segments are split up into several shorter ones so that segment durations are more uniformly distributed; for details and ablations, see supplemental material. During the inference, we do not use any split-segment and use the entire video.

For the alignment decoder (Section~\ref{sec:alignment_decoder}), we use a single layer, single head decoder. To train this model, we use similar hyper-parameters and optimizers while freezing the encoder-decoder model from Section~\ref{sec:model1} and only train the alignment decoder with our cross-attention loss. For positional encoding, we use the standard sinusoidal positional encoding~\cite{Vaswani:attention:2017}.
Furthermore, we use random dropping of the features as an augmentation method, where we randomly drop $\sim1\%$ of the features in the sequence.

\subsection{Performance Evaluation}
Here, we provide the overall performance comparison of our proposed method, \textit{\textbf{UVAST}}, on three challenging action segmentation datasets with different levels of supervision.
We demonstrate the effectiveness of our proposed method for both the fully supervised and timestamp supervised setup and achieve competitive results on both settings.
We provide the results of our proposed model for four scenarios: Transcript prediction of our encoder-decoder architecture (referred to as ``w/o duration") and three different approaches to obtain durations for the segments, namely alignment decoder from Section~\ref{sec:alignment_decoder} (``+ alignment decoder"), Viterbi (``+ Viterbi"), and FIFA~\cite{fifa2021} (``+ FIFA").
We only report the Edit score for ``w/o duration", as it does not provide segment durations. 
A significant advantage of our method is that a predicted transcript is readily available and can be used in these inference algorithms instead of the previous methods, which need to iterate over the training transcripts. Furthermore, we can optionally use the predicted duration of the alignment decoder to initialize the segment lengths in FIFA. 

\subsubsection{Fully Supervised Comparison.}
Table~\ref{tab:fully_supervised} shows the performance of our method in the fully supervised setting compared with state-of-the-art methods.
At the bottom of Table~\ref{tab:fully_supervised} we provide the results of our proposed model for the four scenarios explained above. 
\textit{\textbf{UVAST}} achieves significantly better Edit score on transcript prediction (``w/o duration") than all other existing methods on all three datasets, which demonstrates the effectiveness of our model to capture and summarize the actions occurring in the video. 
\begin{table}[t]
\centering
\caption{\textbf{Fully supervised results on all three datasets.} Best and second best results are shown in bold and underlined, respectively. With the assistance of Viterbi/FIFA our method outperforms state-of-the-art in terms of Edit and F1 scores on all datasets.}
\resizebox{\textwidth}{!} {
\begin{tabular}{ll|ccc|c|c||ccc|c|c||ccc|c|c}
\hline 
\hline 
\multicolumn{2}{l|}{} & \multicolumn{5}{c||}{{Breakfast}} & \multicolumn{5}{c||}{{50Salads}} & \multicolumn{5}{c}{{GTEA}}\\
\cline{3-17} 
 &  & \multicolumn{3}{c|}{{F1@\{10,25,50\}}} & Edit & Acc & \multicolumn{3}{c|}{{F1@\{10,25,50\}}} & Edit & Acc & \multicolumn{3}{c|}{{F1@\{10,25,50\}}} & Edit & Acc \\
\hline 
\multicolumn{2}{l|}{TDRN \cite{lei2018temporal}} & - & - & - & - & - & $72.9$ & $68.5$ & $57.2$ & $66.0$ & $68.1$ & $79.2$ & $74.4$ & $62.7$ & $74.1$ & $70.1$ \\
\multicolumn{2}{l|}{SSA-GAN \cite{gammulle2020fine}} & - & - & - & - & - & $74.9$ & $71.7$ & $67.0$ & $69.8$ & $73.3$ & $80.6$ & $79.1$ & $74.2$ & $76.0$ & $74.4$\\
\multicolumn{2}{l|}{MuCon \cite{Souri:2021:MuCon}} & $73.2$ & $66.1$ & $48.4$ & $76.3$ & $62.8$ & - & - & - & - & - & - & - & - & - & -\\
\multicolumn{2}{l|}{{DTGRM \cite{wang2020temporal}}} & $68.7$ & $61.9$ & $46.6$ & $68.9$ & $68.3$ & $79.1$ & $75.9$ & $66.1$ & $72.0$ & $80.0$ & $87.3$ & $85.5$ & $72.3$ & $80.7$ & $77.5$\\
\multicolumn{2}{l|}{Gao  \textit{et al.} \cite{gao2021global2local}} & $74.9$ & $69.0$ & $55.2$ & $73.3$ & $70.7$ & $80.3$ & $78.0$ & $69.8$ & $73.4$ & $82.2$ & $89.9$ & $87.3$ & $75.8$ & $84.6$ & $78.5$\\
\multicolumn{2}{l|}{MS-TCN++ \cite{li2020ms}} & $64.1$ & $58.6$ & $45.9$ & $65.6$ & $67.6$ & $80.7$ & $78.5$ & $70.1$ & $74.3$ & $83.7$ & $88.8$ & $85.7$ & $76.0$ & $83.5$ & $80.1$\\
\multicolumn{2}{l|}{BCN \cite{wang2020boundary}} & $68.7$ & $65.5$ & $55.0$ & $66.2$ & $70.4$ & $82.3$ & $81.3$ & $74.0$ & $74.3$ & $84.4$ & $88.5$ & $87.1$ & $77.3$ & $84.4$ & $79.8$\\
\multicolumn{2}{l|}{SSTDA \cite{chen2020action}} & $75.0$ & $69.1$ & $55.2$ & $73.7$ & $70.2$ & $83.0$ & $81.5$ & $73.8$ & $75.8$ & $83.2$ & $90.0$ & $89.1$ & $78.0$ & $86.2$ & $79.8$\\
\multicolumn{2}{l|}{Singhania \textit{et al.} \cite{singhania2021coarse}} & $70.1$ & $66.6$ & $56.2$ & $68.2$ & \uline{$73.5$} & $76.6$ & $73.0$ & $62.5$ & $69.2$ & $80.1$ & $90.5$ & $88.5$ & $77.1$ & $87.3$ & $\mathbf{80.3}$\\
\multicolumn{2}{l|}{ASRF \cite{asrf}} & $74.3$ & $68.9$ & $56.1$ & $72.4$ & $67.6$ & $84.9$ & $83.5$ & $77.3$ & $79.3$ & $84.5$ & $89.4$ & $87.8$ & \uline{$79.8$} & $83.7$ & $77.3$\\
\multicolumn{2}{l|}{ASFormer \cite{asformer}} & $76.0$ & $70.6$ & $57.4$ & $75.0$ & \uline{$73.5$} & $85.1$ & $83.4$ & $76.0$ & \uline{$79.6$} & \uline{$85.6$} & $90.1$ &  $88.8$  & $79.2$ & $84.6$ & $79.7$\\
\multicolumn{2}{l|}{ASFormer \cite{asformer} + Viterbi} & $76.1$ & $70.5$ & $57.1$ & $74.5$ & $70.2$ & $84.1$ & $82.3$ & $74.9$ & $76.1$ & $84.7$ & \underline{$91.1$} & \underline{$90.0$} & $79.5$ & $86.5$ & $80.0$ \\
\multicolumn{2}{l|}{ASFormer \cite{asformer} + FIFA} & \underline{$76.8$} & \underline{$71.4$} & $\mathbf{58.9}$ & $75.6$ & $\mathbf{73.7}$ & $84.5$ & $83.2$ & $75.4$ & $78.5$ & $85.4$ & $90.4$ & $88.6$ & $78.1$ & $86.2$ & $78.9$ \\ 
\hline 
\multirow{4}{*}{UVAST (Ours)} & w/o duration & - & - & - & $76.9$ & - & - & - & - & $\mathbf{83.9}$ & - & - & - & - & $\mathbf{92.1}$ & - \\
  & + alignment decoder & $76.7$ & $70.0$ & $56.6$ & $\mathbf{77.2}$ & $68.2$ & $86.2$ & $81.2$ & $70.4$ & $\mathbf{83.9}$ & $79.5$ & $77.1$ & $69.7$ & $54.2$ & \uline{$90.5$} & $62.2$ \\
  & + Viterbi & $75.9$ & $70.0$ & $57.2$ & $76.5$ & $66.0$ & $\mathbf{89.1}$ & $\mathbf{87.6}$ & $\mathbf{81.7}$ & $\mathbf{83.9}$ & $\mathbf{87.4}$ & $\mathbf{92.7}$ & $\mathbf{91.3}$ & $\mathbf{81.0}$ & $\mathbf{92.1}$ & \uline{$80.2$} \\
  & + FIFA & $\mathbf{76.9}$ & $\mathbf{71.5}$ & \underline{$58.0$} & \uline{$77.1$} & $69.7$ & \uline{$88.9$} & \uline{$87.0$} & \uline{$78.5$} & $\mathbf{83.9}$ & $84.5$ & $82.9$ & $79.4$ & $64.7$ & \uline{$90.5$} & $69.8$ \\
\hline 
\hline
\end{tabular}
}
\label{tab:fully_supervised}
\end{table}
In the last three rows of Table~\ref{tab:fully_supervised}, we use three different approaches to compute the duration of the segments.
Combining \textit{\textbf{UVAST}} with the alignment decoder from Section~\ref{sec:alignment_decoder} achieves competitive results. However, it is important to note that Transformers are very data-hungry and training them on small datasets can be challenging.
We observe that \textit{\textbf{UVAST}} with alignment decoder outperforms other methods in terms of Edit score. While the F1 scores are comparable to the state-of-the-art on the Breakfast dataset, the small size of the GTEA dataset hinders the training of the alignment decoder. 

Moreover, with frame-wise predictions and transcript prediction available, our method conveniently allows applying inference algorithms at test time, such as FIFA and Viterbi, without the need to expensively iterate over the training transcripts.
Combining our method with Viterbi outperforms the existing methods on GTEA and 50Salads in terms of Edit and F1 scores, and achieves competitive results on Breakfast. We also provide the results of \textit{\textbf{UVAST}} with FIFA, where we initialize the duration with the predicted duration. It achieves strong performance on Breakfast and 50Salads. Note that although FIFA is a fast approximation of Viterbi, it achieves better results on the Breakfast dataset. This is due to the fact that the objective function that is minimized by FIFA/Viterbi does not optimize the evaluation metrics directly, \textit{i.e.,} the global optimum of the Viterbi objective function does not guarantee the global optimum of the evaluation metrics. This observation is consistent with the results reported in~\cite{fifa2021}.     

The comparison to ASFormer~\cite{asformer} is also interesting. While ASFormer performs like most other approaches frame-level prediction, Fig.~\ref{fig:model_step_by_step}~(a), \textit{\textbf{UVAST}} predicts the action segments in an autoregressive manner, Fig.~\ref{fig:model_step_by_step}~(d,e). As expected, ASFormer achieves in general a better frame-wise accuracy while \textit{\textbf{UVAST}} achieves a better Edit score. Since ASFormer uses a smoothing loss and multiple refinement stages to address over-segmentation similar to MS-TCN~\cite{farha2019ms,li2020ms}, it has $\sim1.3$M learnable parameters, whereas our proposed model has $\sim1.1$M parameters. Our approach with Viterbi achieves similar F1 scores on the Breakfast dataset, but higher F1 scores on the other datasets.    
For a more thorough and fair comparison with ASFormer, we additionally provide the results when combined with Viterbi or FIFA during inference. To that end, we extract the transcript to be used in Viterbi/FIFA from the frame-wise predictions of the model.

Overall, we find that our method achieves strong performance in terms of Edit and F1 scores, while Acc is compared to the state-of-the-art lower on Breakfast. Note that Acc is dominated by long segments and less sensitive to over-segmentation errors. Lower Acc and higher Edit/F1 scores indicate that \textit{\textbf{UVAST}} localizes action boundaries, which are difficult to annotate precisely, less accurately. It is therefore an interesting research direction to improve the segment boundaries, \textit{e.g.,} by using an additional refinement like ASFormer.      

\begin{table}[t!]
\begin{center}
\caption{
\textbf{Timestamp supervision results on all three datasets.}
UVAST, ASFormer~\cite{asformer}, and MSTCN~\cite{farha2019ms} are trained via our constrained k-medoids pseudo-labels. Best result shown in bold. \textbf{\textit{UVAST}} outperforms SOTA on all datasets and metrics except for Acc on Breakfast. The performance in terms of Edit distance is significant, and is comparable to the fully supervised setup.
}
\label{tab:timestamp}
\resizebox{\linewidth}{!} {
\begin{tabular}{ll|ccc|c|c||ccc|c|c||ccc|c|c}
\hline 
\hline 
\multicolumn{2}{l|}{} & \multicolumn{5}{c||}{{Breakfast}} & \multicolumn{5}{c||}{{50Salads}} & \multicolumn{5}{c}{{GTEA}}\\
\cline{3-17} 
 &  & \multicolumn{3}{c|}{{F1@\{10,25,50\}}} & Edit & Acc & \multicolumn{3}{c|}{{F1@\{10,25,50\}}} & Edit & Acc & \multicolumn{3}{c|}{{F1@\{10,25,50\}}} & Edit & Acc \\
\hline 
\multicolumn{2}{l|}{Li et al. \cite{timestamp2021}} & $70.5$ & $63.6$ & $47.4$ & $69.9$ & $\mathbf{64.1}$ & $73.9$ & $70.9$ & $60.1$ & $66.8$ & $75.6$ & $78.9$ & $73.0$ & $55.4$ & $72.3$ & $66.4$\\
\multicolumn{2}{l|}{MS-TCN~\cite{farha2019ms}} & $56.1$ & $50.0$ & $36.8$ & $61.7$ & $62.5$ & $74.4$ & $70.4$ & $57.7$ & $66.8$ & $72.8$ & $82.8$ & $80.3$ & $63.5$ & $79.5$ & $67.7$\\
\multicolumn{2}{l|}{MS-TCN~\cite{farha2019ms} + Viterbi} & $43.3$ & $37.2$ & $25.6$ & $43.5$ & $35.9$ & $74.0$ & $70.0$ & $55.5$ & $68.2$ & $72.8$ & $82.6$ & $79.7$ & $61.6$ & $81.0$ & $68.1$\\
\multicolumn{2}{l|}{MS-TCN~\cite{farha2019ms} + FIFA} & $36.1$ & $30.5$ & $21.9$ & $42.6$ & $35.8$ & $73.7$ & $69.5$ & $54.9$ & $69.2$ & $72.8$ & $81.7$ & $77.7$ & $57.7$ & $81.0$ & $67.3$ \\
\multicolumn{2}{l|}{ASFormer~\cite{asformer}} & $70.9$ & $62.9$ & $44.0$ & $71.6$ & $61.3$ & $76.6$ & $72.1$ & $59.6$ & $70.0$ & $76.9$ & $\mathbf{87.2}$ & $83.1$ & ${67.5}$ & $83.0$ & $68.8$ \\
\multicolumn{2}{l|}{ASFormer~\cite{asformer} + Viterbi} & $71.3$ & $63.1$ & $44.3$ & $71.1$ & $60.7$ & $76.3$ & $72.1$ & $59.4$ & $68.8$ & $\mathbf{77.0}$ & $87.1$ & $83.1$ & $\mathbf{68.2}$ & $83.0$ & $69.1$ \\
\multicolumn{2}{l|}{ASFormer~\cite{asformer} + FIFA} & $71.1$ & $62.7$ & $44.3$ & $71.8$ & $61.8$ & $76.7$ & $72.0$ & $58.8$ & $70.0$ & $76.9$ & $86.8$ & $81.9$ & $65.4$ & $83.0$ & $68.4$ \\
\hline 
\multirow{3}{*}{{UVAST (Ours)}}
 & + alignment decoder & $\mathbf{72.0}$ & $64.1$ & $\mathbf{48.6}$ & $\mathbf{74.3}$ & $60.2$ & $75.7$ & $70.6$ & $58.2$ & $78.4$ & $67.8$ & $70.8$ & $63.5$ & $49.2$ & $88.2$ & $55.3$\\
 & + Viterbi & $71.3$ & $63.3$ & $48.3$ & $74.1$ & $60.7$ & $\mathbf{83.0}$ & $\mathbf{79.6}$ & $\mathbf{65.9}$ & $78.2$ & $\mathbf{77.0}$ & $\mathbf{87.2}$ & $\mathbf{83.7}$ & $66.0$ & $\mathbf{89.3}$ & $\mathbf{70.5}$\\
 & + FIFA & $\mathbf{72.0}$ & $\mathbf{64.2}$ & $47.6$ & $74.1$ & $60.3$ & $80.2$ & $74.9$ & $61.6$ & $\mathbf{78.6}$ & $72.5$ & $80.7$ & $75.2$ & $57.4$ & $88.7$ & $66.0$\\
\hline 
\hline 
\end{tabular}    
}
\end{center}
\end{table}


\subsubsection{Timestamp Supervision Comparison.}
We use our proposed constrained k-medoids to generate pseudo-segmentation using the frame-wise input features and ground truth timestamps. The output consists of continuous segments, which can be identified with the transcript to yield a pseudo-segmentation. While this approach can be applied both to the input features and encoder features in principle, we find that using the input features already gives a surprisingly good performance; we report Acc and F1 scores in Table~\ref{table:kmethoids} averaged over all splits. Note that this is not a temporal segmentation method as it requires timestamp supervision as input. We use the resulting pseudo-segmentation as the auxiliary signal to our encoder during the training where we have access to the timestamp supervision. 

In Table~\ref{tab:timestamp}, we compare our proposed timestamp model with the recently proposed method~\cite{timestamp2021} on the three action segmentation datasets.
To the best of our knowledge, \cite{timestamp2021} is the first work that proposed and applied timestamp supervision for the temporal action segmentation task. 
Although other weakly supervised methods exist, they are based on \textit{transcript} supervision, a weaker form of supervision; 
therefore, we additionally train MS-TCN~\cite{farha2019ms} and ASFormer~\cite{asformer} with our constrained k-medoids. To get more thorough and fair comparisons, we further show their performance when combined with Viterbi decoding or FIFA during inference.  

Table~\ref{tab:timestamp} shows that: I) our  method largely outperforms the other methods by achieving the best performance on $13$ out of $15$ metrics.
Analogously to the fully supervised case, we observe the strong performance of our alignment decoder in terms of Edit and F1 scores on Breakfast; with FIFA and Viterbi, we outperform the method of \cite{timestamp2021} on 50Salads and GTEA. Notably, \textbf{\textit{UVAST}} achieves significantly higher performance in terms of Edit distance, which is comparable to the fully supervised setup. II) ASFormer and MSTCN perform reasonably well in the timestamp supervision setup when trained on the pseudo-labels of our constrained k-medoids algorithm, which demonstrates one more time the effectiveness of our proposed constrained k-medoids algorithm. III) ASFormer and MSTCN do not benefit from the Viterbi algorithm in this case. This is due to the relatively lower Edit distance of these methods. Namely, Viterbi hurts MSTCN on Breakfast as it achieves significantly lower Edit distance compared to ours.


\subsection{Qualitative Evaluation}
We show qualitative results of two videos from the Breakfast dataset in the fully supervised and timestamp supervised setting in  Fig.~\ref{fig:qualitative}. We visualize the ground truth segmentations (first row) as well as the predicted segmentations of our encoder (second row) and decoder with alignment decoder, FIFA or Viterbi for duration prediction (last three rows).
The encoder predictions demonstrate well the common problem of over-segmentation with frame-level predictions; the segment-level predictions of our decoder on the other hand yield coherent action segments.

\begin{figure}
\begin{minipage}{\linewidth}
    \centering
    \includegraphics[width=0.92\linewidth]{figures/UVAST_segmentation.png}
    \caption{\textbf{Qualitative results.} We show ground truth and predicted segmentation of fully supervised (left) and timestamp supervised (right) \textit{\textbf{UVAST}} of two videos from the Breakfast dataset.}
    \label{fig:qualitative}
\end{minipage}
\end{figure}

\subsection{Ablations Studies}
\paragraph{Duration Prediction.}
As discussed in Section~\ref{sec:intro}, the vanilla Transformer model, Fig.~\ref{fig:model_step_by_step}~(b),  does not generalize to the action segmentation task, see Table~\ref{tab:expl_dur}. We train this model using $\mathcal{L}_{\text{segment}}$ and MSE between predicted and ground truth durations, which are scaled to $[0, 1]$ by dividing by the total number of frames $T$.
Our first modification involves applying a frame-wise loss to the encoder features, which drastically improves the results. However, this explicit duration prediction still struggles to accurately localize the segments. 
Predicting duration implicitly via our alignment decoder instead, Fig.~\ref{fig:model_step_by_step}~(d)+(e), on the other hand improves the localization, increasing Acc and F1.

\begin{table}[h]
\centering
\caption{\textbf{Constrained K-medoids results.} We evaluate the pseudo-segmentations of our constrained k-medoids algorithm, Alg.~\ref{alg:kmethoids}, given the frame-wise input features and ground truth timestamps.}
\label{table:kmethoids}
\resizebox{1.6 in}{!} {
\begin{tabular}{l|ccc|c}
\hline 
\hline 
Dataset & \multicolumn{3}{c|}{F1@\{10,25,50\}} & Acc \\
\hline 
Breakfast & $95.5$ & $87.5$ & $70.0$ & $76.9$\\
50Salads & $97.5$ & $90.4$ & $75.6$ & $81.3$\\
GTEA & $99.8$ & $97.7$ & $83.0$ & $75.3$\\
\hline 
\hline 
\end{tabular}
}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Explicit duration prediction on Breakfast split 1.}
We show the results of different steps described in Section~\ref{sec:intro} from explicit duration prediction via the vanilla Transformer to implicit duration prediction with our alignment decoder and the impact of the full loss function, Eq.~\eqref{eq6}.}
\label{tab:expl_dur}
\resizebox{4.0 in}{!} {
\begin{tabular}{ll|ccc|c|c}
\hline 
\hline 
Method & Loss & \multicolumn{3}{c|}{F1@\{10,25,50\}} & Edit & Acc \\
\hline 
Vanilla Transformer, Fig.~\ref{fig:model_step_by_step}~(b) & Eq.~\eqref{eq:ce_segment} + MSE & $48.1$ & $42.3$ & $26.7$ & $52.9$ & $35.0$ \\
\hline
\multirow{2}{*}{+ Frame-wise Loss, Fig.~\ref{fig:model_step_by_step}~(c) } & Eq.~\eqref{eq:ce_segment} + Eq.~\eqref{eq:ce_frame} + MSE 
& $70.7$ & $63.5$ & $44.4$ & $73.9$ & $59.1$ \\
& Eq.~\eqref{eq6} + MSE & $72.1$ & $65.1$ & $48.7$ & $76.5$ & $59.0$ \\
\hline
\multirow{2}{*}{+ Alignment Decoder, Fig.~\ref{fig:model_step_by_step}~(d)+(e)} & Eq.~\eqref{eq:ce_segment} + Eq.~\eqref{eq:ce_frame} + AD & $73.5$ & $68.3$ & $54.3$ & $75.2$ & $67.7$ \\
& Eq.~\eqref{eq6} + AD & $77.1$ & $72.0$ & $60.4$ & $78.2$ & $71.7$ \\
\hline 
\hline 
\end{tabular}
}
\end{table}


\paragraph{Impact of the Loss Terms.}
In Table~\ref{tab:ablation_losses} we investigate the impact of the different loss terms (Section~\ref{sec:losses}) on split 1 of Breakfast and 50Salads.
In the first row of Table~\ref{tab:ablation_losses}, we evaluate the encoder when trained only using the frame-wise loss, \ie following the frame-wise prediction design as previous works. As expected, solely relying on the frame-wise loss leads to over-segmentation and poor performance. The rest of Table~\ref{tab:ablation_losses} shows the performance of our proposed model when using both encoder and decoder as explained in Sections~\ref{sec:model1} and \ref{sec:losses}, and reflect the key idea of our method to directly predict the segments.
While the most basic version using the segment-wise loss~\eqref{eq:ce_segment} improves over frame-wise predictions, we observe that using both the frame-wise~\eqref{eq:ce_frame} and segment-wise~\eqref{eq:ce_segment} loss term increases the performance drastically.
Moreover, we observe that adding the cross-attention loss~\eqref{eq:ca} further improves the results, demonstrating its effectiveness for longer sequences with many action segments, such as 50Salads.
While adding the group-wise loss terms \eqref{eq:g_frame} and \eqref{eq:g_segment} individually improves the performance moderately, the real benefit is revealed when combining them all together.

\begin{table}[h]
\centering
\caption{\textbf{Loss terms.} Contribution of different loss terms on Breakfast and 50Salads (split 1).}
\resizebox{4.0 in}{!} {
\begin{tabular}{l||ccc|c||ccc|c}
\hline 
\hline 
\multicolumn{1}{c||}{} & \multicolumn{4}{c||}{Breakfast} & \multicolumn{4}{c}{50Salads}\\
\cline{2-9} 
 & \multicolumn{3}{c|}{F1@\{10,25,50\}} & Edit & \multicolumn{3}{c|}{F1@\{10,25,50\}} & Edit \\
\hline 
$\mathcal{L}_{\text{frame}}$ & $8.9$ & $7.7$ & $5.9$ & $14.1$ & $13.5$ & $12.8$ & $10.8$ & $11.4$\\
$\mathcal{L}_{\text{segment}}$ & $49.5$ & $39.7$ & $22.9$ & $55.6$ & $20.1$ & $16.3$ & $8.6$ & $29.2$\\
$\mathcal{L}_{\text{frame}}$+$\mathcal{L}_{\text{segment}}$ & $71.8$ & $66.3$ & $52.6$ & $73.4$ & $55.0$ & $52.4$ & $37.5$ & $45.3$\\
$\mathcal{L}_{\text{frame}}$+$\mathcal{L}_{\text{segment}}$+$\mathcal{L}_{\text{CA}}$ & $73.8$ & $67.0$ & $54.8$ & $74.5$ & $74.2$ & $71.0$ & $58.4$ & $65.5$\\
$\mathcal{L}_{\text{frame}}$+$\mathcal{L}_{\text{segment}}$+$\mathcal{L}_{\text{g-frame}}$ & $73.3$ & $65.8$ & $52.8$ & $73.6$ & $56.6$ & $53.4$ & $40.2$ & $44.0$\\
$\mathcal{L}_{\text{frame}}$+$\mathcal{L}_{\text{segment}}$+$\mathcal{L}_{\text{g-segment}}$ & $72.8$ & $64.3$ & $53.7$ & $73.2$ & $59.1$ & $56.1$ & $42.8$ & $51.6$\\
$\mathcal{L}_{\text{frame}}$+$\mathcal{L}_{\text{segment}}$+$\mathcal{L}_{\text{g-frame}}$+$\mathcal{L}_{\text{g-segment}}$ & $73.5$ & $67.9$ & $55.0$ & $73.1$ & $57.0$ & $54.5$ & $40.4$ & $42.4$\\
$\mathcal{L}_{\text{frame}}$+$\mathcal{L}_{\text{segment}}$+$\mathcal{L}_{\text{g-frame}}$+$\mathcal{L}_{\text{g-segment}}$+$\mathcal{L}_{\text{CA}}$ & $75.1$ & $68.9$ & $54.9$ & $76.1$ & $73.6$ & $71.5$ & $55.3$ & $78.4$\\
\hline 
\hline 
\end{tabular}
\label{tab:ablation_losses}
}
\end{table}


To shed more light on the contribution of our cross-attention loss we visualize its impact in Fig.~\ref{fig:ca_ablation}.  
Given the ground truth segmentation, Fig.~\ref{fig:ca_ablation}~(a), of a video, Fig.~\ref{fig:ca_ablation}~(b) shows our expected target activations (output of softmax) of the decoder's cross-attention map; we hypothesize that activations should be higher in areas that belong to the corresponding segment.
Fig.~\ref{fig:ca_ablation}~(c) shows the output of the cross-attention when using our cross-attention loss. 
We observe that this loss indeed guides the cross-attention to have higher activations in the regions that belong to the related segment for an action.
Fig.~\ref{fig:ca_ablation}~(d) shows that lack of our cross-attention loss causes the attention map to be noisy; it's unclear which region is used for the segment classification. 


\begin{figure}[h]
    \centering
    \includegraphics [scale=.4]{figures/Cross_Attention_2.png}
\caption{\textbf{Impact of the Cross-Attention Loss for the Transcript Decoder.} (a) A ground truth example of  a video with $13150$ frames and $12$ segments from the 50Salads dataset. (b) The target cross-attention map after softmax with dimension $12\times 13150$. (c) and (d) show the zoomed-in segments of the cross-attention map of the decoder when using the cross-attention loss (top) or not using it (bottom). 
    In (b-d) brighter color means higher values of the activations.}
    \label{fig:ca_ablation}
\end{figure}
