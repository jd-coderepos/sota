\documentclass{article}







\PassOptionsToPackage{numbers,compress}{natbib}



\usepackage[workshop]{nips_2018}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{nameref}


\usepackage{float}
\usepackage{graphicx}
\usepackage{tocstyle}


\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[mathscr]{eucal}\usepackage{bm}\usepackage{bbm}
\usepackage{relsize}
\usepackage{graphics}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{enumitem} 

\usepackage{array}


\usepackage{pgf}
\usepackage{xinttools}\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usepackage{textcomp}
\usetikzlibrary{calc,shapes,arrows,positioning,automata,trees}
\usepackage{placeins} \usepackage{tabularx}
\usepackage{graphicx}
\usepackage{subcaption} \usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fixed,
  fontadjust=true,
  basewidth=0.5em
}

\usepackage{xargs,lipsum,caption,changepage,ifthen}

\usepackage[toc,page]{appendix}
\renewcommand{\appendixname}{Appendix}
\renewcommand{\appendixtocname}{Appendix}
\renewcommand{\appendixpagename}{Appendix}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}\newtheorem{proposition}{Proposition}\newtheorem{lemma}{Lemma}\newtheorem{conjecture}{Conjecture}\newtheorem*{theorem*}{Theorem}
\newtheorem*{definition*}{Definition}

\newtheorem{theoremA}{Theorem}
\newtheorem{definitionA}{Definition}
\newtheorem{corollaryA}{Corollary}\newtheorem{propositionA}{Proposition}\newtheorem{lemmaA}{Lemma}\newtheorem{conjectureA}{Conjecture}


\renewcommand{\thetheoremA}{A\arabic{theoremA}}
\renewcommand{\thedefinitionA}{A\arabic{definitionA}}
\renewcommand{\thecorollaryA}{A\arabic{corollaryA}}
\renewcommand{\thepropositionA}{A\arabic{propositionA}}
\renewcommand{\thelemmaA}{A\arabic{lemmaA}}


\newcommand\Ba{\bm{a}}
\newcommand\Bb{\bm{b}}
\newcommand\Bc{\bm{c}}
\newcommand\Bd{\bm{d}}
\newcommand\Be{\bm{e}}
\newcommand\Bf{\bm{f}}
\newcommand\Bg{\bm{g}}
\newcommand\Bh{\bm{h}}
\newcommand\Bi{\bm{i}}
\newcommand\Bj{\bm{j}}
\newcommand\Bk{\bm{k}}
\newcommand\Bl{\bm{l}}
\newcommand\Bm{\bm{m}}
\newcommand\Bn{\bm{n}}
\newcommand\Bo{\bm{o}}
\newcommand\Bp{\bm{p}}
\newcommand\Bq{\bm{q}}
\newcommand\Br{\bm{r}}
\newcommand\Bs{\bm{s}}
\newcommand\Bt{\bm{t}}
\newcommand\Bu{\bm{u}}
\newcommand\Bv{\bm{v}}
\newcommand\Bw{\bm{w}}
\newcommand\Bx{\bm{x}}
\newcommand\By{\bm{y}}
\newcommand\Bz{\bm{z}}
\newcommand\BA{\bm{A}}
\newcommand\BB{\bm{B}}
\newcommand\BC{\bm{C}}
\newcommand\BD{\bm{D}}
\newcommand\BE{\bm{E}}
\newcommand\BF{\bm{F}}
\newcommand\BG{\bm{G}}
\newcommand\BH{\bm{H}}
\newcommand\BI{\bm{I}}
\newcommand\BJ{\bm{J}}
\newcommand\BK{\bm{K}}
\newcommand\BL{\bm{L}}
\newcommand\BM{\bm{M}}
\newcommand\BN{\bm{N}}
\newcommand\BO{\bm{O}}
\newcommand\BP{\bm{P}}
\newcommand\BQ{\bm{Q}}
\newcommand\BR{\bm{R}}
\newcommand\BS{\bm{S}}
\newcommand\BT{\bm{T}}
\newcommand\BU{\bm{U}}
\newcommand\BV{\bm{V}}
\newcommand\BW{\bm{W}}
\newcommand\BX{\bm{X}}
\newcommand\BY{\bm{Y}}
\newcommand\BZ{\bm{Z}}
\newcommand\Bal{\bm{\alpha}}
\newcommand\Bbe{\bm{\beta}}
\newcommand\Bla{\bm{\lambda}}
\newcommand\Bep{\bm{\epsilon}}
\newcommand\Bga{\bm{\gamma}}
\newcommand\Bmu{\bm{\mu}}
\newcommand\Bnu{\bm{\nu}}
\newcommand\Brh{\bm{\rho}}
\newcommand\Bth{\bm{\theta}}
\newcommand\Bxi{\bm{\xi}}
\newcommand\Bka{\bm{\kappa}}
\newcommand\Bsi{\bm{\sigma}}
\newcommand\Bta{\bm{\tau}}
\newcommand\Bph{\bm{\phi}}
\newcommand\Bom{\bm{\omega}}
\newcommand\BDe{\bm{\Delta}}
\newcommand\BLa{\bm{\Lambda}}
\newcommand\BPh{\bm{\Phi}}
\newcommand\BPs{\bm{\Psi}}
\newcommand\BSi{\bm{\Sigma}}
\newcommand\BUp{\bm{\Upsilon}}
\newcommand\BXi{\bm{\Xi}}
\newcommand\BGa{\bm{\Gamma}}
\newcommand\BTh{\bm{\Theta}}
\newcommand\BOm{\bm{\Omega}}
\newcommand\BOn{\bm{1}}
\newcommand\BZe{\bm{0}}
\newcommand\BLOn{\mathlarger{\mathlarger{\bm{1}}}}
\newcommand\BLZe{\mathlarger{\mathlarger{\bm{0}}}}
\newcommand{\dA}{\mathbb{A}} \newcommand{\dB}{\mathbb{B}} 
\newcommand{\dC}{\mathbb{C}} \newcommand{\dD}{\mathbb{D}} 
\newcommand{\dE}{\mathbb{E}} \newcommand{\dF}{\mathbb{F}}
\newcommand{\dG}{\mathbb{G}} \newcommand{\dH}{\mathbb{H}}
\newcommand{\dI}{\mathbb{I}} \newcommand{\dJ}{\mathbb{J}} 
\newcommand{\dK}{\mathbb{K}} \newcommand{\dL}{\mathbb{L}}
\newcommand{\dM}{\mathbb{M}} \newcommand{\dN}{\mathbb{N}}
\newcommand{\dO}{\mathbb{O}} \newcommand{\dP}{\mathbb{P}} 
\newcommand{\dQ}{\mathbb{Q}} \newcommand{\dR}{\mathbb{R}}
\newcommand{\dS}{\mathbb{S}} \newcommand{\dT}{\mathbb{T}} 
\newcommand{\dU}{\mathbb{U}} \newcommand{\dV}{\mathbb{V}} 
\newcommand{\dW}{\mathbb{W}} \newcommand{\dX}{\mathbb{X}}
\newcommand{\dY}{\mathbb{Y}} \newcommand{\dZ}{\mathbb{Z}}

\newcommand{\rA}{\mathrm{A}} \newcommand{\rB}{\mathrm{B}} 
\newcommand{\rC}{\mathrm{C}} \newcommand{\rD}{\mathrm{D}} 
\newcommand{\rE}{\mathrm{E}} \newcommand{\rF}{\mathrm{F}}
\newcommand{\rG}{\mathrm{G}} \newcommand{\rH}{\mathrm{H}}
\newcommand{\rI}{\mathrm{I}} \newcommand{\rJ}{\mathrm{J}} 
\newcommand{\rK}{\mathrm{K}} \newcommand{\rL}{\mathrm{L}}
\newcommand{\rM}{\mathrm{M}} \newcommand{\rN}{\mathrm{N}}
\newcommand{\rO}{\mathrm{O}} \newcommand{\rP}{\mathrm{P}} 
\newcommand{\rQ}{\mathrm{Q}} \newcommand{\rR}{\mathrm{R}}
\newcommand{\rS}{\mathrm{S}} \newcommand{\rT}{\mathrm{T}}
\newcommand{\rU}{\mathrm{U}} \newcommand{\rV}{\mathrm{V}} 
\newcommand{\rW}{\mathrm{W}} \newcommand{\rX}{\mathrm{X}}
\newcommand{\rY}{\mathrm{Y}} \newcommand{\rZ}{\mathrm{Z}}

\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}


\newcommand{\sA}{\mathscr{A}} \newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}} \newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}} \newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}} \newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}} \newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}} \newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}} \newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}} \newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}} \newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}} \newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}} \newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}} \newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}} \newcommand{\sZ}{\mathscr{Z}}


\newcommand{\Ra}{\mathrm{a}} \newcommand{\Rb}{\mathrm{b}} 
\newcommand{\Rc}{\mathrm{c}} \newcommand{\Rd}{\mathrm{d}} 
\newcommand{\Rre}{\mathrm{e}} \newcommand{\Rf}{\mathrm{f}}
\newcommand{\Rg}{\mathrm{g}} \newcommand{\Rh}{\mathrm{h}}
\newcommand{\Ri}{\mathrm{i}} \newcommand{\Rj}{\mathrm{j}} 
\newcommand{\Rk}{\mathrm{k}} \newcommand{\Rl}{\mathrm{l}}
\newcommand{\Rm}{\mathrm{m}} \newcommand{\Rn}{\mathrm{n}}
\newcommand{\Ro}{\mathrm{o}} \newcommand{\Rp}{\mathrm{p}} 
\newcommand{\Rq}{\mathrm{q}} \newcommand{\Rr}{\mathrm{r}}
\newcommand{\Rs}{\mathrm{s}} \newcommand{\Rt}{\mathrm{t}}
\newcommand{\Ru}{\mathrm{u}} \newcommand{\Rv}{\mathrm{v}} 
\newcommand{\Rw}{\mathrm{w}} \newcommand{\Rx}{\mathrm{x}}
\newcommand{\Ry}{\mathrm{y}} \newcommand{\Rz}{\mathrm{z}}



\newcommand\xs{x}
\newcommand\zs{z}
\newcommand\xsp{\left(x_{\phi}\right)}
\newcommand\xp{\bm{x}_{\phi}}
\newcommand\zp{\bm{z}_{\omega}}
\newcommand\zps{\left(z_{\omega}\right)}
\newcommand\sign{\mbox{sign}}
\newcommand\EXP{\mathbf{\mathrm{E}}}
\newcommand\PR{\mathbf{\mathrm{Pr}}}
\newcommand\VAR{\mathbf{\mathrm{Var}}}
\newcommand\COV{\mathbf{\mathrm{Cov}}}
\newcommand\TR{\mathbf{\mathrm{Tr}}}
\newcommand\ALs{{\mbox{\boldmath }}}
\newcommand\XIs{{\mbox{\boldmath }}}
\newcommand\sgn{\mathop{\mathrm{sgn}\,}}
\newcommand\argmax{\mathop{\mathrm{argmax}\,}}
\newcommand\oo{\mathrm{old}}
\newcommand\nn{\mathrm{new}}
\newcommand\rank{\mathrm{rank}}
\newcommand\net{\mathrm{net}}
\newcommand\emp{\mathrm{emp}}
\newcommand{\mse}{\mathop{\bf mse}}
\newcommand{\bias}{\mathop{\bf bias}}
\newcommand{\var}{\mathop{\bf var}}

\newcommand{\p}[4]{{#3}\!\left#1{#4}\right#2} 

\newcommand{\ABS}[1]{{{\left| #1 \right|}}} \newcommand{\BRA}[1]{{{\left\{#1\right\}}}} \newcommand{\NRM}[1]{{{\left\| #1\right\|}}} \newcommand{\PAR}[1]{{{\left(#1\right)}}} \newcommand{\SBRA}[1]{{{\left[#1\right]}}} 


\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\newcommand{\R}{\textsf{R~}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\texttt{#1}}}

\newcommand{\figpath}{figures/}
\newcommand{\tabpath}{tables/}
\newcommand{\RR}{RR}
\newcommand{\subgrapwidth}{.5}

\newcommand{\returnrealization}{\mathbf{g}}


\usepackage{pdflscape} 

\newcolumntype{R}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedleft\arraybackslash}p{#1}}


\usepackage{bigdelim}
\usepackage{colortbl} \definecolor{mColor1}{rgb}{0.95,0.95,0.95}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}


\newcommandx{\mycaptionminipage}[3][3=c,usedefault]{\begin{minipage}[#3]{#1}\ifthenelse{\equal{#3}{b}}{\captionsetup{aboveskip=0pt}}{}
        \ifthenelse{\equal{#3}{t}}{\captionsetup{belowskip=0pt}}{}
        \vspace{0pt}\centering\captionsetup{width=\textwidth} #2\end{minipage}}\newcommandx{\mysidecaption}[4][4=c,usedefault]{\checkoddpage \ifoddpage \mycaptionminipage{\dimexpr\linewidth-#1\linewidth-\intextsep\relax}{#3}[#4]\hfill \mycaptionminipage{#1\linewidth}{#2}[#4]\else \mycaptionminipage{#1\linewidth}{#2}[#4]\hfill \mycaptionminipage{\dimexpr\linewidth-#1\linewidth-\intextsep\relax}{#3}[#4]\fi }



\hypersetup{
   colorlinks=true,
   linkcolor=blue,
   citecolor=green,
   urlcolor=magenta,
pdfborder=0 0 0,
   pdftitle=RUDDER: Return Decomposition for Delayed Rewards,
   pdfsubject=reinforcement learning; delayed reward; reward redistribution; return decomposition; bias-variance; credit assignment; LSTM ,
   pdfkeywords={reinforcement learning, delayed reward, reward redistribution, return decomposition, bias-variance, credit assignment, LSTM },
   pdfauthor=anonymous,pdfstartview=FitH
}

\title{RUDDER: Return Decomposition for Delayed Rewards}

\author{\vspace{0.1cm} Jose A. Arjona-Medina\thanks{authors contributed equally} \quad Michael Gillhofer\footnotemark[1] \quad Michael Widrich\footnotemark[1] \\ {\vspace{0.1cm}\bf Thomas Unterthiner \quad Johannes Brandstetter \quad Sepp Hochreiter\footnotemark[2]} \\
  LIT AI Lab \\ Institute for Machine Learning\\
  Johannes Kepler University Linz, Austria \\
  \footnotemark[2]~~also at Institute of Advanced Research in Artificial Intelligence (IARAI)
}



\DeclareUnicodeCharacter{2212}{-}


\begin{document}





\maketitle


\begin{abstract}
We propose RUDDER,
a novel reinforcement learning approach
for delayed rewards in finite Markov decision processes (MDPs).
In MDPs the -values are equal to the
expected immediate reward plus the expected future rewards.
The latter are related to bias problems 
in temporal difference (TD) learning and to
high variance problems in Monte Carlo (MC) learning.
Both problems are even more severe when rewards are delayed.
RUDDER aims at making the expected future rewards
zero, which simplifies -value estimation to
computing the mean of the immediate reward.
We propose the following two new concepts to push the
expected future rewards toward zero.
(i) Reward redistribution that leads to return-equivalent
decision processes with the same optimal policies and, when optimal,
zero expected future rewards.
(ii) Return decomposition via contribution analysis which
transforms the reinforcement learning task 
into a regression task at which deep learning excels. 
On artificial tasks with delayed rewards,
RUDDER is significantly faster than 
MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD(),
and reward shaping approaches.
At Atari games, 
RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, 
which is most prominent at games with delayed rewards.
Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.
\end{abstract}

\section{Introduction}

Assigning credit for a received reward to past actions
is central to reinforcement learning \cite{Sutton:18book}.
A great challenge is to learn long-term credit assignment for delayed 
rewards \cite{Ke:18,Hung:18,Hernandez-Leal:18,Sahni:18}.
Delayed rewards are often episodic or sparse
and common in real-world problems \cite{Rahmandad:09,Luoma:17}.
For Markov decision processes (MDPs), 
the -value is equal to the expected immediate reward plus 
the expected future reward.
For -value estimation,
the expected future reward leads to biases in temporal difference (TD) and
high variance in Monte Carlo (MC) learning.
For delayed rewards, 
TD requires exponentially many updates to correct the bias, 
where the number of updates is exponential in the number of delay steps.
For MC learning the number of states affected by a delayed reward can 
grow exponentially with the number of delay steps. 
(Both statements are proved after 
theorems \ref{th:AexponDecay} and \ref{th:Aaffect} in the appendix.)
An MC estimate of the expected future reward 
has to average over all possible future trajectories,
if rewards, state transitions, or policies are probabilistic.
Delayed rewards make an MC estimate much harder.
    
The main goal of our approach is 
to construct an MDP that has {\bf expected future rewards equal to zero}. 
If this goal is achieved, 
-value estimation simplifies to computing the mean of the immediate rewards.
To push the expected future rewards to zero, we require two new concepts.
The first new concept is {\bf reward redistribution} to create {\bf return-equivalent MDPs}, 
which are characterized by having the same optimal policies. 
An optimal reward redistribution should transform a delayed reward MDP into  
a return-equivalent MDP with zero expected future rewards.
However, expected future rewards equal to zero are in general not possible for MDPs.
Therefore, we introduce sequence-Markov decision processes (SDPs),
for which reward distributions need not to be Markov.
We construct a reward redistribution that leads to a return-equivalent
SDP with a second-order Markov reward distribution and 
expected future rewards that are equal to zero.
For these return-equivalent 
SDPs, -value estimation simplifies to computing the mean. 
Nevertheless, the -values or advantage functions 
can be used for learning optimal policies.
The second new concept is
{\bf return decomposition} and its realization via {\bf contribution analysis}.
This concept serves to efficiently construct a proper reward redistribution,
as described in the next section.
Return decomposition transforms a reinforcement learning task 
into a regression task, where the sequence-wide 
return must be predicted from the whole state-action sequence.
The regression task identifies which state-action pairs
contribute to the return prediction and, therefore, receive a redistributed reward. 
Learning the regression model uses only completed episodes as training set,
therefore avoids problems with unknown future state-action trajectories.
Even for sub-optimal reward redistributions, we obtain an enormous speed-up
of -value learning if relevant reward-causing state-action pairs are identified.
We propose RUDDER (RetUrn Decomposition for DElayed Rewards) for learning with 
reward redistributions that are obtained via return decompositions.

{\bf To get an intuition} for our approach,
assume you repair pocket watches and then sell them.
For a particular brand of watch you have to decide
whether repairing pays off.
The sales price is known,
but you have unknown costs, 
i.e.\ negative rewards, caused by repair and delivery.
The advantage function is
the sales price
minus the expected immediate repair costs
minus the expected future delivery costs.
Therefore, you want to know whether the advantage function is positive.
--- Why is zeroing the expected future costs beneficial? ---
If the average delivery costs are known,
then they can be added to the repair costs
resulting in zero future costs.
Using your repairing experiences,
you just have to average over the repair costs
to know whether repairing pays off.
--- Why is return decomposition so efficient? ---
Because of pattern recognition.
For zero future costs, you have to estimate
the expected brand-related delivery costs,
which are e.g.\ packing costs.
These brand-related costs are superimposed by 
brand-independent general delivery costs 
for shipment (e.g.\ time spent for delivery).
Assume that general delivery costs
are indicated by patterns, e.g.\ weather conditions, which delay delivery.
Using a training set of completed deliveries,
supervised learning can identify these patterns
and attribute costs to them.
This is return decomposition.
In this way, only brand-related delivery costs remain 
and, therefore, can be estimated more efficiently than by MC.

{\bf Related Work.}
Our new learning algorithm is gradually changing 
the reward redistribution during learning, 
which is known as shaping \cite{Skinner:58,Sutton:18book}.
In contrast to RUDDER, potential-based shaping like 
reward shaping \cite{Ng:99}, look-ahead advice, and
look-back advice \cite{Wiewiora:03icml} use a fixed reward redistribution. 
Moreover, since these methods keep the original reward, 
the resulting reward redistribution is not optimal,
as described in the next section,
and learning can still be exponentially slow. 
A monotonic positive reward transformation \cite{Peters:07}
also changes the reward distribution but is neither assured 
to keep optimal policies nor to have expected future rewards of zero.
Disentangled rewards keep optimal policies but are neither environment 
nor policy specific, therefore can in general not achieve 
expected future rewards being zero \cite{Fu:18}.
Successor features decouple environment and policy 
from rewards, but changing the reward changes the  
optimal policies \cite{Barreto:17,Barreto:18}. 
Temporal Value Transport (TVT) uses 
an attentional memory mechanism to learn
a value function that serves as
fictitious reward \cite{Hung:18}. 
However, expected future rewards are not close to zero
and optimal policies are not guaranteed to be kept.
Reinforcement learning tasks have been changed
into supervised tasks \cite{Schaal:99,Barto:04,Schmidhuber:15}. 
For example, a model that predicts the return can supply update 
signals for a policy by sensitivity analysis. 
This is known as ``backpropagation through a model''  \cite{Munro:87,Robinson:89,RobinsonFallside:89,Werbos:90,Schmidhuber:91nips,Bakker:02,Bakker:07}.
In contrast to these approaches, 
(i) we use contribution analysis instead of 
sensitivity analysis,
and (ii) we use the whole state-action sequence to predict its
associated return.


\section{Reward Redistribution and Novel Learning Algorithms}


Reward redistribution is the main new concept
to achieve expected future rewards equal to zero.
We start by introducing MDPs,  
return-equivalent sequence-Markov decision processes (SDPs),
and reward redistributions.
Furthermore, optimal reward redistribution is defined and
novel learning algorithms based on reward redistributions 
are introduced.

\paragraph{MDP Definitions and 
           Return-Equivalent Sequence-Markov Decision Processes (SDPs).}
\label{c:def}
A finite Markov decision process (MDP) 
is 5-tuple  of finite sets
 of states  (random variable  at time ),
 of actions  (random variable ),
and  of rewards  (random variable ).
Furthermore,  has
transition-reward distributions

conditioned on state-actions, and a discount factor .
The marginals are  and 
.
The expected reward is .
The return  is ,
while for finite horizon MDPs with sequence
length  and  it is .
A Markov policy is given as
action distribution  conditioned on
states.
We often equip an MDP  with a policy  
without explicitly mentioning it. 
The action-value function  for policy  is 
.
The goal of learning is to maximize the expected return at time ,
that is .
The optimal policy  is .
A {\em sequence-Markov decision process} (SDP) is defined 
as a decision process which is equipped with a Markov policy
and has Markov transition probabilities
but a reward that is not required to be Markov.
Two SDPs  and 
are {\em return-equivalent} if
(i) they differ only in their reward distribution  
and (ii) they
have the same expected return at  for each policy :
. 
They are {\em strictly return-equivalent} if
they have the same expected return for every episode and
for each policy .
Strictly return-equivalent SDPs are return-equivalent.
Return-equivalent SDPs have the same optimal policies.
For more details see Section~\ref{sec:AReturnEquivalent} in the appendix.



\paragraph{Reward Redistribution.}
Strictly return-equivalent SDPs  and 
can be constructed by reward redistributions.
A {\em reward redistribution} given an SDP  
is a procedure that redistributes 
for each sequence  the realization of 
the sequence-associated return variable 
 
or its expectation along the sequence. 
Later we will introduce a reward redistribution that 
depends on the SDP .
The reward redistribution creates a new SDP  with
the redistributed reward  at time  and the return variable
. 
A reward redistribution is second order Markov if
the redistributed reward  depends only on . 
If the SDP  is obtained from the 
SDP  by reward redistribution, 
then  and  are strictly return-equivalent.
The next theorem states that the optimal policies are still the same 
for  and  (proof after Section Theorem~S2). 
\begin{theorem}
\label{th:EquivT}
Both the SDP  
with delayed reward 
and the SDP  with redistributed reward  
have the same optimal policies.
\end{theorem}


\paragraph{Optimal Reward Redistribution with 
           Expected Future Rewards Equal to Zero.}
We move on to the main goal of this paper: 
to derive an SDP via reward redistribution 
that has expected future rewards equal to zero and, therefore,
no delayed rewards.
At time  the immediate reward is  with expectation
.
We define the expected future rewards  at time 
as the expected sum of future rewards from  to .
\begin{definition}
For  and ,  
the expected sum of delayed rewards at time  
in the interval  is defined as
.
\end{definition}
For every time point , the expected future rewards  
given 
is the expected sum of future rewards until sequence end, 
that is, in the interval . 
For MDPs, the Bellman equation for -values becomes
.
We aim to derive an MDP with ,
which gives .
In this case, learning the -values 
simplifies to estimating the expected immediate reward
.
Hence, the reinforcement learning task reduces to computing 
the mean, e.g.\ the arithmetic mean, for each
state-action pair .
A reward redistribution is defined to be {\em optimal},  
if  for .
In general, an optimal reward redistribution violates
the Markov assumptions and the Bellman equation does
not hold (proof after Theorem~\ref{th:Aviolate} in the appendix).
Therefore, we will consider SDPs in the following.
The next theorem states that 
a delayed reward MDP  
with a particular policy 
can be transformed into a return-equivalent SDP 
with an optimal reward redistribution.
\begin{theorem}
\label{th:zeroExp}
We assume a delayed reward MDP , 
where the accumulated reward is given at sequence end.
A new SDP  is obtained by a 
second order Markov reward redistribution,
which ensures that  is return-equivalent to .
For a specific , the following two
statements are equivalent:
(I) ~~, i.e.\ the reward redistribution is optimal, 

An optimal reward redistribution
fulfills for  and :
  . 
\end{theorem}
The proof can be found after Theorem~\ref{th:AzeroExp} in the appendix.
Equation  implies that the new SDP 
has no delayed rewards, that is, 
,
for  
(Corollary~\ref{th:ApropDelay} in the appendix).
The SDP  has no delayed rewards since no state-action pair
can increase or decrease the expectation of a future reward.
Equation~\eqref{eq:diffQ} shows that for an optimal reward redistribution
the expected reward has to be the difference of
consecutive -values of the original delayed reward.
The optimal reward redistribution is
second order Markov since the expectation of  at time 
 depends on .



The next theorem states the major advantage of an
optimal reward redistribution:
 can be estimated with an offset that 
depends only on  
by estimating the expected immediate redistributed reward.
Thus, -value estimation becomes trivial and the
the advantage function of the MDP  can be readily computed.
\begin{theorem}
\label{th:OptReturnDecomp}
If the reward redistribution is 
optimal, then the -values 
of the SDP  are given by 
 \vspace{-0.7cm}

The SDP  and the original MDP  
have the same advantage function.
Using a behavior policy 
 the expected immediate reward is

\end{theorem}
The proof can be found after Theorem~\ref{th:AOptReturnDecomp} in the appendix. 
If the reward redistribution is not optimal, then 
 measures the deviation of the -value 
from . 
This theorem justifies several learning methods based on
reward redistribution presented in the next paragraph.


\paragraph{Novel Learning Algorithms Based on Reward Redistributions.}
\label{c:novel}
We assume  and a finite horizon or an absorbing state
original MDP  with delayed rewards.
For this setting we introduce new reinforcement learning algorithms.
They are gradually changing 
the reward redistribution during learning and are
based on the estimations in Theorem~\ref{th:OptReturnDecomp}.
These algorithms are also valid for non-optimal reward redistributions,
since the optimal policies are kept (Theorem~\ref{th:EquivT}).
Convergence of RUDDER learning
can under standard assumptions be proven by the stochastic 
approximation for 
two time-scale update rules \cite{Borkar:97,Karmakar:17}.
Learning consists of an LSTM and a -value update.
Convergence proofs to an optimal policy are 
difficult, since locally stable attractors 
may not correspond to optimal policies.



According to Theorem~\ref{th:EquivT}, reward redistributions
keep the optimal policies. 
Therefore, even non-optimal reward redistributions ensure correct learning. 
However, an optimal reward redistribution speeds up learning considerably.
Reward redistributions can be combined 
with methods that use -value ranks or advantage functions.
We consider 
(A) -value estimation, 
(B) policy gradients, and 
(C) -learning.
Type (A) methods estimate -values and are divided 
into variants (i), (ii), and (iii).
Variant (i) assumes an optimal reward redistribution
and estimates  with an offset
depending only on .
The estimates are based on Theorem~\ref{th:OptReturnDecomp}
either by on-policy direct -value estimation according to Eq.~\eqref{eq:qvalue}
or by off-policy immediate reward estimation according to Eq.~\eqref{eq:behavior}.
Variant (ii) methods assume a non-optimal reward redistribution and 
correct Eq.~\eqref{eq:qvalue} by estimating .
Variant (iii) methods use eligibility traces for the redistributed reward.
RUDDER learning can be based on policies like
``greedy in the limit with infinite exploration'' (GLIE) or
``restricted rank-based randomized'' (RRR) \cite{Singh:00}. 
GLIE policies change toward greediness with respect to the -values
during learning.
For more details on these learning approaches 
see Section~\ref{sec:QestimateA} in the apendix.

Type (B) methods replace in the expected updates  
 of policy gradients the value  
by an estimate of  or by
a sample of the redistributed reward. 
The offset  in Eq.~\eqref{eq:qvalue}
or   in Eq.~\eqref{eq:behavior}
reduces the variance as baseline normalization does. 
These methods can be extended to Trust Region Policy
Optimization (TRPO) \cite{Schulman:15icml} as used in 
Proximal Policy Optimization (PPO) \cite{Schulman:17}.
The type (C) method is -learning with the redistributed reward. 
Here, -learning is justified if
immediate and future reward are drawn together,
as typically done.


\section{Constructing Reward Redistributions by Return Decomposition}


We now propose methods to construct reward redistributions. 
Learning with non-optimal reward redistributions {\em does work} since the 
optimal policies do not change according to Theorem~\ref{th:EquivT}.
However, reward redistributions that are optimal considerably speed up learning,
since future expected rewards introduce 
biases in TD methods and high variances in MC methods.
The expected optimal redistributed reward is 
the difference of -values according to Eq.~\eqref{eq:diffQ}. 
The more a reward redistribution deviates from these differences,
the larger are the absolute -values and, in turn, the less optimal
the reward redistribution gets.
Consequently, to construct a reward redistribution which is close to optimal
we aim at identifying the largest -value differences.


\paragraph{Reinforcement Learning as Pattern Recognition.}
We want to transform the reinforcement learning problem into
a pattern recognition task to employ deep learning approaches.
The sum of the -value differences gives the 
difference between expected return at sequence begin and
the expected return at sequence end (telescope sum).
Thus, -value differences allow to predict the 
expected return of the whole state-action sequence.
Identifying the largest -value differences 
reduces the prediction error most.
-value differences are assumed to be associated with
patterns in state-action transitions.
The largest -value differences 
are expected to be found more frequently in sequences
with very large or very low return.
The resulting task is to predict the expected return
from the whole sequence and identify which 
state-action transitions have contributed the most to the prediction.
This pattern recognition task serves to
construct a reward redistribution, where the redistributed reward
corresponds to the different contributions.
The next paragraph shows how the return is decomposed and redistributed
along the state-action sequence.


\paragraph{Return Decomposition.}
\label{para:returnDecomposition}
The {\em return decomposition} idea is 
that a function  predicts the expectation
of the return
for a given state-action sequence (return for the whole sequence).
The function  is neither a value nor an action-value function
since it predicts the expected return when the whole sequence is given.
With the help of  either the predicted value or
the realization of the return is redistributed over
the sequence. 
A state-action pair receives as redistributed reward
its contribution to the prediction, which
is determined by contribution analysis.
We use contribution analysis since sensitivity analysis has serious drawbacks:
local minima, instabilities, exploding or vanishing
gradients, and proper exploration
\cite{Hochreiter:90,Schmidhuber:90diff}.
The major drawback is that
the relevance of actions is missed
since sensitivity analysis does not consider the contribution of actions to 
the output,
but only their effect on the output when slightly perturbing them.
Contribution analysis
determines how much a state-action pair contributes to the final prediction.
We can use any contribution analysis method, but
we specifically consider three methods:
(A) differences of return predictions,
(B) integrated gradients (IG) \cite{Sundararajan:17}, and
(C) layer-wise relevance propagation (LRP) \cite{Bach:15}.
For (A),  must try to predict the  
sequence-wide return at every time step.
The redistributed reward is given by 
the difference of consecutive predictions. 
The function  can be decomposed into
past, immediate, and future contributions to the return.
Consecutive predictions share the same past and the same
future contributions except for two immediate state-action pairs.
Thus, in the difference of consecutive predictions 
contributions cancel except for the two immediate state-action pairs.
Even for imprecise predictions of future contributions to the return, 
contribution analysis is more precise, 
since prediction errors cancel out.
Methods (B) and (C) rely on information later in the sequence for
determining the contribution and thereby may introduce a non-Markov reward.
The reward can be viewed to be probabilistic
but is prone to have high variance. 
Therefore, we prefer method (A).

\paragraph{Explaining Away Problem.}
\label{para:explainingAway}
We still have to tackle the problem that reward causing actions do not receive redistributed rewards
since they are explained away by later states.
To describe the problem, assume an MDP  with the only 
reward at sequence end.
To ensure the Markov property, states in  have to store 
the reward contributions of previous state-actions;
e.g.\  has to store all previous contributions such that the expectation 
is Markov.
The explaining away problem is that later states
are used for return prediction, while reward causing
earlier actions are missed.
To avoid explaining away,
we define a difference function 
between a state-action pair  and 
its predecessor .
That  is a function of 
is justified by Eq.~\eqref{eq:diffQ}, which ensures that such
s allow an optimal reward redistribution.
The sequence of differences is 
. 
The components  are assumed 
to be statistically independent from each other, therefore
 cannot store reward contributions of previous .
The function  should predict the return by 
 and
can be decomposed into . 
The contributions are

for .
For the redistributed rewards , we ensure 
.
The reward  of 
is probabilistic and 
the function  might not be perfect,
therefore neither  for the return
realization  nor
 for the expected return
holds.
Therefore, we need to introduce the compensation

as an extra reward  at time 
to ensure strictly return-equivalent SDPs.
If  was perfect, then it would predict the expected return which
could be redistributed.
The new redistributed rewards
 are based on the return decomposition, since they 
must have the contributions  as mean:\newline
 , 
  , 
,
where the realization  is replaced by its
random variable .
If the prediction of  is perfect, then we can
redistribute the expected return via the prediction.
Theorem~\ref{th:zeroExp} holds also for
the correction  (see Theorem~\ref{th:AzeroExpCorr} in the appendix). 
A  with zero prediction errors 
results in an optimal reward redistribution.
Small prediction errors lead to reward redistributions 
close to an optimal one.

\paragraph{RUDDER: Return Decomposition using LSTM.}
RUDDER uses a Long Short-Term Memory (LSTM) network for 
return decomposition and the resulting reward redistribution.
RUDDER consists of three phases. 
{\bf (I) Safe exploration.}
Exploration sequences should generate LSTM training samples 
with delayed rewards by avoiding 
low -values during a particular time interval. 
Low -values hint at states where the agent gets stuck. 
Parameters comprise starting time, length, and -value threshold.
{\bf (II) Lessons replay buffer for training the LSTM.}
If RUDDER's safe exploration discovers 
an episode with unseen delayed rewards,
it is secured in a lessons replay buffer \cite{Lin:93}. 
Unexpected rewards are indicated by a large prediction error of the LSTM.
For LSTM training, episodes with larger errors are sampled more often 
from the buffer, similar to prioritized
experience replay \cite{Schaul:15}.
{\bf (III) LSTM and return decomposition.}
An LSTM learns to predict sequence-wide return at 
every time step and, thereafter, 
return decomposition uses differences of return predictions
(contribution analysis method (A)) to construct a reward redistribution. 
For more details see Section~\ref{sec:ALSTMadjust} in the appendix.


\paragraph{Feedforward Neural Networks (FFNs) vs.\ LSTMs.}
In contrast to LSTMs, 
FNNs are not suited for processing sequences. 
Nevertheless, FNNs can learn a action-value function, which 
enables contribution analysis by 
differences of predictions. 
However, this leads to serious problems by spurious contributions
that hinder learning.
For example, any contributions would be incorrect
if the true expectation of the return did not change. 
Therefore, prediction errors might falsely cause contributions 
leading to spurious rewards. 
FNNs are prone to such prediction errors since they
have to predict the expected return
again and again from each different state-action pair and 
cannot use stored information.
In contrast, the LSTM is less prone to produce spurious 
rewards:
(i) The LSTM will only learn to store information 
if a state-action pair has a strong evidence 
for a change in the expected return. 
If information is stored, then internal states and,
therefore, also the predictions change, otherwise the predictions
stay unchanged.
Hence, storing events receives a contribution and
a corresponding reward, while by default nothing is stored and 
no contribution is given.
(ii) The LSTM tends to have smaller prediction errors since it can 
reuse past information for predicting the expected return. 
For example, key events can be stored.
(iii) Prediction errors of LSTMs are much more likely to cancel 
via prediction differences than those of FNNs. 
Since consecutive predictions of LSTMs rely on the same
internal states, they usually have highly correlated errors.

\paragraph{Human Expert Episodes.} They are an alternative to 
exploration and can 
serve to fill the lessons replay buffer.
Learning can be sped up considerably when LSTM identifies
human key actions.
Return decomposition will reward human key actions even for episodes
with low return since other actions that thwart high
returns receive negative reward.
Using human demonstrations in reinforcement learning
led to a huge improvement on some Atari
games like Montezuma's Revenge \cite{Pohlen:18,Aytar:18}.


\paragraph{Limitations.}
In all of the experiments reported in this manuscript, we show that RUDDER significantly outperforms other methods for delayed reward problems. However, RUDDER might not be effective when the reward is not delayed since LSTM learning takes extra time and has problems with very long sequences. Furthermore, reward redistribution may introduce disturbing spurious reward signals.

\section{Experiments}
\label{sec:Mexperiments}
RUDDER is evaluated on three artificial tasks with delayed rewards.
These tasks are designed to show problems of TD, MC, 
and potential-based reward shaping.
RUDDER overcomes these problems.
Next, we demonstrate that RUDDER also works for more complex tasks
with delayed rewards.
Therefore, we compare RUDDER with 
a Proximal Policy Optimization (PPO) baseline
on 52 Atari games. 
All experiments use finite time horizon or 
absorbing states MDPs with  and reward at episode end.
For more information see Section~\ref{sec:Aexp} in the appendix.

{\bf Artificial Tasks (I)--(III).} 
Task (I) shows that TD methods have problems with vanishing information 
for delayed rewards.
Goal is to learn that a delayed reward is
larger than a distracting immediate reward.
Therefore, the correct expected future reward must be  
assigned to many state-action pairs. 
Task (II) is a variation of the introductory pocket watch example
with delayed rewards.
It shows that MC methods have problems with the 
high variance of future unrelated rewards.
The expected future reward that is caused by the first action
has to be estimated.
Large future rewards that are not associated 
with the first action impede MC estimations. 
Task (III) shows that potential-based reward shaping methods have
problems with delayed rewards.
For this task, only the first two actions are relevant, to which
the delayed reward has to be propagated back.

The tasks have different delays, 
are tabular (-table),
and use an -greedy policy with .
We compare RUDDER, MC, and TD() 
on all tasks, and Monte Carlo Tree Search (MCTS) on task (I).
Additionally, on task (III), SARSA() and reward shaping are compared.
We use  as suggested \cite{Sutton:18book}.
Reward shaping methods are the original method, look-forward advice,
and look-back advice with three different potential functions.
RUDDER uses an LSTM without output and forget gates,
no lessons buffer, and no safe exploration. 
For all tasks contribution analysis is performed 
with difference of return predictions.  
A -table is learned 
by an exponential moving average of 
the redistributed reward (RUDDER's -value estimation) 
or by -learning. 
Performance is measured by 
the learning time to achieve 90\% of the maximal expected return.
A Wilcoxon signed-rank test determines 
the significance of performance differences between RUDDER
and other methods.



{\bf(I) Grid World} 
shows problems of TD methods with delayed rewards.
The task illustrates a time bomb that
explodes at episode end.
The agent has to defuse the bomb
and then run away as far as possible since
defusing fails with a certain probability.
Alternatively, the agent can immediately run away, 
which, however, leads to less reward on average.
The Grid World is a  grid with
{\em bomb} at coordinate  and
{\em start} at , where  is the delay of the task.
The agent can move 
{\em up}, {\em down}, {\em left}, and {\em right} as long as
it stays on the grid.
At the end of the episode, after  steps, 
the agent receives a reward of 1000
with probability of 0.5, 
if it has visited {\em bomb}.
At each time step, the agent receives 
an immediate reward of , 
where  depends on the chosen action, 
 is the current time step, and  is 
the Hamming distance to {\em bomb}.
Each move toward the {\em bomb}, 
is immediately penalized with . 
Each move away from the {\em bomb}, 
is immediately rewarded with . 
The agent must learn the -values precisely to
recognize that directly running away is not optimal.
Figure~\ref{fig:test}(I) shows the 
learning times 
to solve the task vs.\ the delay of the reward averaged over 100 trials.
For all delays, RUDDER is significantly faster than all other methods
with -values .
Speed-ups vs.\ MC and MCTS,
suggest to be exponential with delay time.
RUDDER is exponentially faster with increasing 
delay than (), 
supporting Theorem~\ref{th:AexponDecay} in the appendix. 
{\bf RUDDER significantly outperforms all other methods.}


\begin{figure}[!t]\centering \resizebox{\linewidth}{!}{\input{\figpath plot3_td09_v12.pgf}}
\caption{Comparison of RUDDER and other methods on artificial tasks 
with respect to the learning time in episodes (median of 100 trials)
vs.\ the delay of the reward.
The shadow bands indicate the  and  quantiles.
In (II), the y-axis of the inlet is scaled by .
In (III), reward shaping (RS), 
look-ahead advice (look-ahead), 
and look-back advice (look-back) use three different potential functions.
In (III), the dashed blue line
represents RUDDER with (), 
in contrast to RUDDER with -estimation. 
In all tasks, RUDDER significantly outperforms all other methods.\label{fig:test}}\end{figure}

{\bf (II) The Choice}
shows problems of MC methods with delayed rewards.
This task has probabilistic state transitions, which
can be represented as a tree with states as nodes.
The agent traverses the tree from the root (initial state) 
to the leafs (final states).
At the root, the agent has to choose between 
the left and the right subtree,
where one subtree has a higher expected reward.
Thereafter, it traverses the tree randomly 
according to the transition probabilities. 
Each visited node adds its fixed share to the final reward. 
The delayed reward is given as accumulated shares at a leaf.
The task is solved when 
the agent always chooses the subtree with higher expected reward.
Figure~\ref{fig:test}(II) shows the 
learning times 
to solve the task vs.\ the delay of the reward averaged over 100 trials.
For all delays, RUDDER is significantly faster than all other methods
with -values .
The speed-up vs.\ MC, 
suggests to be exponential with delay time.
RUDDER is exponentially faster with increasing delay 
than (), 
supporting Theorem~\ref{th:AexponDecay} in the appendix. 
{\bf RUDDER significantly outperforms all other methods.}

{\bf (III) Trace-Back} 
shows problems of potential-based reward shaping methods
with delayed rewards.
We investigate how fast information about delayed rewards 
is propagated back by
RUDDER, (), SARSA(), 
and potential-based reward shaping.
MC is skipped since it does not 
transfer back information.
The agent can move in a 1515 grid
to the 4 adjacent positions as long as it remains on the grid.
Starting at , the number of moves per episode is . 
The optimal policy moves the agent up in  and  
right in , 
which gives immediate reward of  at , 
and a delayed reward of 150 at the end . 
Therefore, the optimal return is 100.
For any other policy, 
the agent receives only an immediate reward of 50 at .
For , state transitions are deterministic, while
for  they are uniformly distributed and 
independent of the actions.
Thus, the return does not depend on actions at .
We compare RUDDER, original reward shaping, 
look-ahead advice, and look-back advice. 
As suggested by the authors, we use SARSA instead of -learning 
for look-back advice. 
We use three different potential functions for reward shaping,
which are all based on the reward redistribution 
(see appendix). 
At , there is a distraction since the immediate 
reward is  for the optimal and 50 for other actions. 
RUDDER is significantly faster than all other methods
with -values .
Figure~\ref{fig:test}(III) shows the 
learning times averaged over 100 trials.
{\bf RUDDER is exponentially faster than 
all other methods and significantly outperforms them.}

\paragraph{Atari Games.}\label{para:Atari}
\label{c:Atari}
RUDDER is evaluated with respect to its learning time and 
achieves scores on Atari games of the
Arcade Learning Environment (ALE) \cite{Bellemare:13}
and OpenAI Gym \cite{Brockman:16}.
RUDDER is used on top of the TRPO-based \cite{Schulman:15icml} 
policy gradient method PPO that uses GAE \cite{Schulman:15}.
Our PPO baseline differs from the original 
PPO baseline \cite{Schulman:17} in two aspects.
(i) Instead of using the sign function of the rewards, 
rewards are scaled by their current maximum.
In this way, the ratio between different rewards
remains unchanged and the advantage of large delayed rewards
can be recognized. 
(ii) The safe exploration strategy of RUDDER is used.
The entropy coefficient is replaced by 
Proportional Control \cite{Bolton:15,Berthelot:17}. 
A coarse hyperparameter optimization is performed for the PPO baseline.
For all 52 Atari games, RUDDER uses 
the same architectures, losses, and hyperparameters, 
which were optimized for the baseline.
The only difference to the PPO baseline is that
the policy network predicts the value function 
of the redistributed reward to integrate reward redistribution
 into the PPO framework.
Contribution analysis uses an LSTM with
differences of return predictions.
Here  is the pixel-wise
difference of two consecutive frames augmented 
with the current frame.
LSTM training and reward redistribution are restricted to
sequence chunks of 500 frames.
Source code is provided upon publication.










\begin{table}
\begin{center}
\begin{tabular}{lcccr}
\toprule
& RUDDER & baseline & delay & delay-event\\
Bowling  & 192 & 56 & 200 & strike pins\\
Solaris  & 1,827 & 616 & 122 & navigate map\\
Venture  & 1,350 & 820 & 150 & find treasure\\
Seaquest  & 4,770 & 1,616 & 272 & collect divers\\
\bottomrule
\\ 
\end{tabular}
\caption{Average scores over 3 random seeds with 10 trials each for 
delayed reward Atari games.
"delay": frames between reward and first related action.  
RUDDER considerably improves the PPO baseline on delayed reward games.\label{tab:atarires}}\end{center}
\end{table}


Policies are trained with no-op starting condition
for 200M game frames using every 4th frame.
Training episodes end with losing a life 
or at maximal 108K frames.
All scores are averaged over 3 different random seeds 
for network and ALE initialization.
We asses the performance by the learning time and
the achieved scores. 
First, we compare RUDDER to the baseline by
average scores per game throughout training, 
to assess learning speed \cite{Schulman:17}.
For 32 (20) games RUDDER (baseline) learns on average faster.
Next, we compare the average scores of the last 10 training games.
For 29 (23) games RUDDER (baseline) has higher average scores.
In the majority of games RUDDER, improves the scores of the PPO baseline.
To compare RUDDER and the baseline on Atari games that are 
characterize by delayed rewards, 
we selected the games Bowling, Solaris, Venture, and Seaquest.
In these games, high scores are achieved by learning the delayed reward, 
while learning the immediate reward and 
extensive exploration (like for Montezuma's revenge)
is less important.
The results are presented in Table~\ref{tab:atarires}.
For more details and further results see Section~\ref{sec:Aatari} in the appendix.
Figure~\ref{fig:bowlingExample}
displays how RUDDER redistributes rewards to key events 
in Bowling.
{\bf At delayed reward Atari games,
RUDDER considerably increases the scores compared to the PPO baseline.}

\begin{figure}[htp]
\begin{center}
\includegraphics[angle=0,width=0.7\textwidth]{\figpath rr_examples/bowling_v6.pdf} 
\caption{RUDDER redistributes rewards to key events in the Atari game Bowling.
Originally, rewards are delayed and only given at episode end.
The first 120 out of 200 frames of the episode are shown.
RUDDER identifies key actions that steer the ball to hit all pins.
\label{fig:bowlingExample}}
\end{center}
\end{figure}









{\bf Conclusion.} 
We have introduced RUDDER, a novel reinforcement learning 
algorithm based on the new concepts of reward redistribution
and return decomposition.
On artificial tasks,
RUDDER significantly outperforms TD(), MC, MCTS and reward shaping methods,
while on Atari games it improves a PPO baseline on average but 
most prominently on long delayed rewards games.



\subsubsection*{Acknowledgments}
This work was supported by 
NVIDIA Corporation,
Merck KGaA, 
Audi.JKU Deep Learning Center, 
Audi Electronic Venture GmbH, 
Janssen Pharmaceutica (madeSMART), 
TGW Logistics Group,
ZF Friedrichshafen AG, UCB S.A.,
FFG grant 871302,
LIT grant DeepToxGen and AI-SNN,
and
FWF grant P 28660-N31.

\subsubsection*{References}
References are provided in Section~\ref{sec:Areferences} in the appendix.









\newpage

\begin{appendices}



\newlength{\auxparskip}
\setlength{\auxparskip}{\parskip} 
\setlength{\parskip}{0pt}
\tableofcontents

\newpage 



\renewcommand{\thesection}{A\arabic{section}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\theequation}{A\arabic{equation}}

\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}

\setcounter{theorem}{0}
\setcounter{definition}{0}
\setcounter{corollary}{0}
\setcounter{proposition}{0}
\setcounter{lemma}{0}

\maketitle
\section{Definition of Finite Markov Decision Processes}
\label{sec:AMDP}

We consider a finite Markov decision process (MDP) ,
which is a 5-tuple :
\begin{itemize}
\item  is a finite set of states;
   is the random variable
  for states at time  with value
  .  has a discrete probability distribution.
\item      is a finite set of actions (sometimes state-dependent
  );   is the random variable
  for actions at time  with value
  .  has a discrete probability distribution.
\item   is a finite set of rewards;  is the random variable
  for rewards at time  with value
  .  has a discrete probability distribution.
\item  are the
  transition and reward distributions over states and rewards, respectively, conditioned on
  state-actions,
\item   is a discount factor for the reward.
\end{itemize}
The Markov policy  is a distribution over
actions given the state: .
We often equip an MDP  with a policy  
without explicitly mentioning it. 
At time , the random variables give the states, actions, and
rewards of the MDP, while low-case letters give possible values.
At each time , the environment is in some state . The policy
 takes an action , which causes a transition of
the environment to state  and a reward  for the
policy.
Therefore, the MDP creates a sequence

The marginal probabilities for 

are:



We use a sum convention:  goes over all possible values of
 and , that is, all combinations which fulfill the constraints
on  and . If  is a function of  (fully determined by ),
then .


We denote expectations:
\begin{itemize}
\item  is the expectation where
 the random variable is an MDP sequence of states, actions, and rewards
 generated with policy .
\item  is the expectation where
 the random variable is  with values .
\item  is the expectation where
 the random variable is  with values .
\item  is the expectation where
 the random variable is  with values .
\item  is the expectation where
  the random variables are  with values ,
   with values ,  with values , 
   with values ,  and  with values .
  If more or fewer random variables are used, the notation is
  consistently adapted. 
\end{itemize}


The return  is the accumulated reward starting from :

The discount factor  determines how much immediate rewards are
favored over more delayed rewards.
For  the return (the objective) is determined
as the largest expected immediate reward, while
for  the return is determined by the
expected sum of future rewards if the sum exists.


\paragraph{State-Value and Action-Value Function.}
The state-value function  for policy  and state 
is defined as

Starting at :

the optimal state-value function  and policy  are



The action-value function  for policy  is the
expected return when starting from , taking action ,
and following policy :

The optimal action-value function  and policy  are

The optimal action-value function  can be expressed via the optimal value
function  :

The optimal state-value function 
can be expressed via the optimal action-value
function   using the optimal policy : 


\paragraph{Finite time horizon and no discount.}
We consider a {\bf finite} time horizon, that is, we consider only
episodes of length , but may receive reward  at episode end at time .
The finite time horizon MDP creates a sequence

Furthermore, we do not discount future rewards, that is, we set .
The return  from time  to  is the sum of rewards:


The state-value function  for policy  is

and the action-value function  for policy  is


From the Bellman equation Eq.~\eqref{eq:bellmanA}, we obtain:
 

The expected return at time  for policy  is

The agent may start in a particular starting state  which is
a random variable. Often  has only one value .

\paragraph{Learning.}
The {\bf goal} of learning is to find the policy  that
maximizes the expected future discounted reward (the return)
if starting at . Thus, the optimal policy  is

We consider two learning approaches for -values: Monte Carlo and
temporal difference. 

\paragraph{Monte Carlo (MC).}
To estimate , MC computes the arithmetic mean of all observed
returns  in the data.
When using Monte Carlo for learning a policy we use an 
exponentially weighted arithmetic mean since the policy steadily changes.

For the th update Monte Carlo tries to minimize  with the residual 

such that the update of the action-value  at state-action  is
 
This update is called {\em constant- MC} \cite{Sutton:18book}.
 
\paragraph{Temporal difference (TD) methods.}
TD updates are based on the Bellman equation.
If  and
 have been estimated,
the -values can be updated according to the Bellman equation:

The update is applying the Bellman operator with estimates
 and
 to  to obtain
.
The new estimate  is closer to
the fixed point  of the Bellman operator,
since the Bellman operator is a contraction
(see Section~\ref{sec:ApropPolyCon2}
and Section~\ref{sec:ApropPolyCon}).

Since the estimates  and  are not known,
TD methods try to minimize  with the Bellman residual :

TD methods use an estimate  of  and a learning
rate  to make an update

For all TD methods  is estimated by  and  by
, while
 does not change with the
current sample, that is, it is fixed for the estimate.
However, the sample determines which  is chosen.
The TD methods differ in how they select .
{\bf SARSA}~\cite{Rummery:94} selects  by sampling from the policy:

and {\bf expected SARSA}~\cite{John:94} averages over selections

It is possible to estimate  separately via an unbiased 
minimal variance estimator
like the arithmetic mean and then 
perform TD updates with the Bellman error
using the estimated  \cite{Romoff:18}.
{\bf -learning} \cite{Watkins:89} is  an
off-policy TD algorithm which is proved
to converge \cite{Watkins:92,Dayan:92}. The proofs were later generalized 
\cite{Jaakkola:94,Tsitsiklis:94}.
-learning uses

The action-value function , which is learned by -learning, approximates 
independently of the policy that is followed. More precisely,
with -learning  converges with probability 1 to the optimal .
However, the policy still
determines which state-action pairs are encountered during learning.
The convergence only requires that all action-state pairs are visited and
updated infinitely often.




\section{Reward Redistribution, Return-Equivalent SDPs, Novel Learning Algorithms, 
         and Return Decomposition}
\label{c:RR}

\subsection{State Enriched MDPs}
\label{sec:AStateEnriched}

For MDPs with a delayed reward the
states have to code the reward. 
However, for an immediate reward the
states can be made more compact by removing the reward information.
For example, states with memory of a delayed reward can be mapped
to states without memory.
Therefore, in order to compare MDPs, we introduce the concept of homomorphic MDPs.
We first need to define a partition of a set induced by a function.
Let  be a partition of a set . For any ,
we denote  the block of  to which  belongs. 
Any function  from a set  to a set  
induces a partition (or equivalence relation) on , 
with  if and only if .
We now can define homomorphic MDPs.
\begin{definitionA}[Ravindran and Barto \cite{Ravindran:01,Ravindran:03}]
An MDP homomorphism  from
an MDP 
to an MDP

is a a tuple of surjections  ( is number of
states), with ,
where  and  for  ( are the admissible actions in state 
and  are the admissible actions in state ).
Furthermore, for all :

We use  if and only if .
\end{definitionA}

We call
 the {\em homomorphic image} of  under .
For homomorphic images the optimal -values and the optimal 
policies are the same. 
\begin{lemmaA}[Ravindran and Barto \cite{Ravindran:01}]
\label{th:Arav}
If  is a homomorphic image of , then
the optimal -values are the same and
a policy that is optimal in  can be transformed to
an optimal policy in  by normalizing the number of actions 
that are mapped to the same action .
\end{lemmaA}
Consequently, the original MDP
can be solved by solving a homomorphic image.

Similar results have been obtained by
Givan~et~al.\ using stochastically bisimilar MDPs:
``Any stochastic bisimulation used for aggregation preserves the
optimal value and action sequence properties as well as the optimal
policies of the model'' \cite{Givan:03}.
Theorem 7 and Corollary 9.1 in Givan~et~al.\ show the facts of
Lemma~\ref{th:Arav}. 
Li~et~al.\ give an overview over state abstraction and state aggregation for
Markov decision processes, which covers homomorphic MDPs \cite{Li:06}.


A Markov decision process  is state-enriched compared to
an MDP  if  has the same states, actions, transition
probabilities, and reward probabilities as
 but with additional information in its states.
We define state-enrichment as follows:
\begin{definitionA}
  A Markov decision process  is {\em state-enriched} compared to
  a Markov decision process  if  is a homomorphic image of
  , where  is the identity and 
   is not bijective. 
\end{definitionA}
Being not bijective means that there exist 
and  with , that is, 
 has more elements than .
In particular, state-enrichment does not change the optimal policies nor
the -values in the sense of Lemma~\ref{th:Arav}.
\begin{propositionA}
\label{th:Aenrich}
If an MDP  is {\em state-enriched} compared to
an MDP , then both MDPs have the same 
optimal -values and the same optimal policies. 
\end{propositionA}
\begin{proof}
According to the definition  is a homomorphic image of
. The statements of Proposition~\ref{th:Aenrich} 
follow directly from Lemma~\ref{th:Arav}.
\end{proof}
Optimal policies of the 
state-enriched MDP 
can be transformed to optimal policies of the original MDP  
and, vice versa, 
each optimal policy of the original MDP  
corresponds to at least one optimal
policy of the state-enriched MDP .

\subsection{Return-Equivalent Sequence-Markov Decision Processes (SDPs)}
\label{sec:AReturnEquivalent}

Our goal is to compare Markov decision processes (MDPs)
with delayed rewards to decision processes (DPs) without delayed rewards.
The DPs without delayed rewards can but need not to be Markov in the rewards.
Toward this end, we consider two DPs  and  which
differ only in their (non-Markov) reward distributions. 
However for each policy  the DPs  and 
have the same expected return at , 
that is, , 
or they have the same expected return for every episode.

\subsubsection{Sequence-Markov Decision Processes (SDPs)}
\label{sec:Asdps}

We first define decision processes that
are Markov except for the reward, which is not required to be Markov.
\begin{definitionA}
  \label{def:Asdp}
A sequence-Markov decision process (SDP) is defined as a finite decision process which is equipped with 
a Markov policy and has Markov transition probabilities 
but a reward distribution that is not required to be Markov.
\end{definitionA}

\begin{propositionA}
\label{th:AsdpMDP}
  Markov decision processes are sequence-Markov decision processes.
\end{propositionA}
\begin{proof}
MDPs have Markov transition probabilities
and are equipped with Markov policies.
\end{proof}



\begin{definitionA}
  \label{def:AseqEqi}
We call two sequence-Markov decision processes  and   
that have the same Markov transition probabilities
and are equipped with the same Markov policy {\em sequence-equivalent}.
\end{definitionA}



\begin{lemmaA}
\label{th:AsdpSeq}
Two sequence-Markov decision processes that are 
sequence-equivalent have the same probability to generate 
state-action sequences , . 
\end{lemmaA}
\begin{proof}
Sequence generation only depends on transition probabilities
and policy. Therefore the probability of generating a particular
sequences is the same for both SDPs.
\end{proof}

\subsubsection{Return-Equivalent SDPs}
\label{sec:AEquiSDPs}

We define return-equivalent SDPs which can be shown to
have the same optimal policies.
\begin{definitionA}
  \label{def:AreturnEqSDP}
  Two sequence-Markov decision processes  and 
  are {\em return-equivalent} if
  they differ only in their reward but
  for each policy  have the same expected return
  .
   and  
  are {\em strictly return-equivalent} if
  they have the same expected return for every episode and
  for each policy :
  
\end{definitionA}
The definition of return-equivalence can be generalized to 
strictly monotonic functions 
for which . Since strictly monotonic functions
do not change the ordering of the returns, maximal returns stay
maximal after applying the function .

Strictly return-equivalent SDPs are return-equivalent as the
next proposition states.
\begin{propositionA}
\label{th:AstrictToEqsdp}
  Strictly return-equivalent sequence-Markov decision processes
  are return-equivalent.
\end{propositionA}
\begin{proof}
 The expected return at  given a policy
  is the sum of
  the probability of generating a sequence times the expected reward
  for this sequence. Both expectations are the same for 
  two strictly return-equivalent sequence-Markov decision processes.
 Therefore the expected return at time  is the same.
\end{proof}

The next proposition states that return-equivalent SDPs  
have the same optimal policies.
\begin{propositionA}
\label{th:AsdpPol}
  Return-equivalent sequence-Markov decision processes 
  have the same optimal policies.
\end{propositionA}
\begin{proof}
  The optimal policy is defined as maximizing the expected return at
  time . 
  For each policy the 
  expected return at
  time  is the same for return-equivalent decision processes.
  Consequently, the optimal policies are the same.
\end{proof}

Two strictly return-equivalent SDPs have the same 
expected return for each state-action sub-sequence
, .
\begin{lemmaA}
\label{th:AreturnSub}
Two strictly return-equivalent SDPs  and 
have the same expected return for each
state-action sub-sequence
, :

\end{lemmaA}
\begin{proof}
Since the SDPs are strictly return-equivalent, we have

We used the marginalization of the full probability and
the Markov property of the state-action sequence.
\end{proof}

We now give the analog definitions and
results for MDPs which are SDPs.
\begin{definitionA}
  \label{def:AreturnEq}
  Two Markov decision processes  and 
  are {\em return-equivalent} if
  they differ only in  and  but
  have the same expected return
   for each policy .
   and  
  are {\em strictly return-equivalent} if
  they have the same expected return for every episode and
  for each policy :
  
\end{definitionA}

Strictly return-equivalent MDPs are return-equivalent as the
next proposition states.
\begin{propositionA}
\label{th:AstrictToEq}
  Strictly return-equivalent decision processes are return-equivalent.
\end{propositionA}
\begin{proof}
Since MDPs are SDPs, the proposition follows from Proposition~\ref{th:AstrictToEqsdp}.
\end{proof}

\begin{propositionA}
\label{th:AthRE}
  Return-equivalent Markov decision processes have the same optimal policies.
\end{propositionA}
\begin{proof}
Since MDPs are SDPs, the proposition follows from Proposition~\ref{th:AsdpPol}.
\end{proof}



For strictly return-equivalent MDPs the expected return is the same if 
a state-action sub-sequence is given. 
\begin{propositionA}
\label{th:AstrictSub}
Strictly return-equivalent MDPs  and 
have the same expected return for a given state-action sub-sequence
, :

\end{propositionA}
\begin{proof}
Since MDPs are SDPs, the proposition follows from Lemma~\ref{th:AreturnSub}.
\end{proof}


\subsection{Reward Redistribution for Strictly Return-Equivalent SDPs}
\label{sec:rewardRedist}

Strictly return-equivalent SDPs  and 
can be constructed by a reward redistribution.

\subsubsection{Reward Redistribution}

We define reward redistributions for SDPs.
\begin{definitionA}
A {\em reward redistribution} given an SDP  
is a fixed procedure that redistributes 
for each state-action sequence  the realization of 
the associated return variable 
 
or its expectation 
along the sequence. 
The redistribution creates a new SDP  with
redistributed reward  at time  and return variable
.
The redistribution procedure 
ensures for each sequence either 
or 

\end{definitionA}


Reward redistributions can be very general.
A special case is if the return can be deduced from the past sequence, 
which makes the return causal. 
\begin{definitionA}
A reward redistribution is {\em causal} if for the redistributed
reward  the following holds:

\end{definitionA}

For our approach we only need reward redistributions that are
second order Markov.
\begin{definitionA}
A causal reward redistribution is {\em second order Markov} if

\end{definitionA}



\subsection{Reward Redistribution Constructs Strictly Return-Equivalent SDPs}

\begin{theoremA}
\label{th:AsdpEquiv}
If the SDP  is obtained by reward redistribution from the 
SDP , then  and  are strictly return-equivalent.
\end{theoremA}

\begin{proof}
For redistributing the reward we have for 
each state-action sequence  the same return
, therefore  

For redistributing the expected return the last equation holds 
by definition.
The last equation is the definition of 
strictly return-equivalent SDPs.
\end{proof}

The next theorem states that the optimal policies are still the same 
when redistributing the reward.
\begin{theoremA}
\label{th:AEquivT}
If the SDP  is obtained by reward redistribution from the 
SDP , then both SDPs have the same optimal policies.
\end{theoremA}


\begin{proof}
\label{c:t1p}
According to Theorem~\ref{th:AsdpEquiv}, 
the SDP  is strictly return-equivalent to the SDP .
According to Proposition~\ref{th:AstrictToEqsdp} and Proposition~\ref{th:AsdpPol}
the SDP  and the SDP  have  
the same optimal policies. 
\end{proof}





\subsubsection{Special Cases of Strictly Return-Equivalent Decision Processes: 
  Reward Shaping, Look-Ahead Advice, and Look-Back Advice}
\label{sec:AshapingEquiv}

Redistributing the reward via
reward shaping \cite{Ng:99,Wiewiora:03}, look-ahead advice, and
look-back advice \cite{Wiewiora:03icml} is 
a special case of reward redistribution that leads to MDPs 
which are strictly return-equivalent to the original MDP.
We show that reward shaping is a special case of
reward redistributions that lead to MDPs 
which are strictly return-equivalent to the original MDP. 
First, we subtract from the potential the constant 
,
which is the potential of the initial state minus the 
discounted potential in the last state divided by a fixed divisor. 
Consequently, the sum of additional rewards in reward shaping, 
look-ahead advice, or look-back advice from  to  is zero.
The original sum of additional rewards is

If we assume  and ,
then reward shaping does not change the return and 
the shaping reward is a reward 
redistribution leading to an MDP that is 
strictly return-equivalent to the original MDP.
For  only
 is required.
The assumptions can always be fulfilled by adding a single new initial
state and a single new final state to the original MDP.

Without the assumptions  and , 
we subtract 
from all
potentials , and obtain

Therefore, the potential-based shaping function (the additional reward)
added to the original reward does not change the return, which means
that the shaping reward is a reward 
redistribution that leads to an MDP that is 
strictly return-equivalent to the original MDP.
Obviously, reward shaping is a special case of reward
redistribution that leads to a strictly 
return-equivalent MDP.
Reward shaping does not change the general learning behavior if a constant 
is subtracted from the potential function . 
The -function of the original reward shaping and the -function of
the reward shaping, which has a constant 
subtracted from the potential function , differ by 
for every -value \cite{Ng:99,Wiewiora:03}.
For infinite horizon MDPs with , 
the terms  and  
vanish, therefore it is sufficient to subtract  from the
potential function.  


Since TD based reward shaping methods keep the original reward, 
they can still be exponentially slow for delayed rewards.
Reward shaping methods like reward shaping, look-ahead advice, and
look-back advice rely on the Markov property of the original reward, 
while an optimal reward redistribution is not Markov.
In general, reward shaping does not lead to 
an optimal reward redistribution according to 
Section~\ref{sec:Aopt_rew_red}.


As discussed in Paragraph~\ref{sec:Aremark},
the optimal reward redistribution does not 
comply to the Bellman equation.
Also look-ahead advice does not comply to the 
Bellman equation.
The return for the look-ahead advice reward  is

with expectations for the reward 

The expected reward  depends on 
future states  and, more importantly, on future actions .
It is a non-causal reward redistribution.
Therefore look-ahead advice cannot be directly used for selecting the 
optimal action at time .
For look-back advice we have

Therefore look-back advice introduces a 
second-order Markov reward like the optimal reward redistribution.


\subsection{Transforming an Immediate Reward MDP to a Delayed Reward MDP}
\label{sec:Aequiv}

We assume to have a Markov decision process  with immediate reward. 
The MDP  is transformed into an MDP  with delayed
reward, where the reward is given at sequence end.
The reward-equivalent MDP  with delayed reward 
is state-enriched,
which ensures that it is an MDP. 

The state-enriched MDP  has
\begin{itemize}
\item reward:

 \item state:
 
\end{itemize}
Here we assume that  can only take a finite number of values to assure
that the enriched states  are finite. 
If the original reward was continuous, then  can represent the accumulated reward 
with any desired precision if the sequence length is  and the original reward was
bounded. We assume that  is sufficiently precise to distinguish the 
optimal policies, which are deterministic, from sub-optimal deterministic policies. 
The random variable  is distributed according to
.
We assume that the time  is coded in  in order to know when the
episode ends and reward is no longer received, otherwise we introduce
an additional state variable  that codes the time.


\begin{propositionA}
\label{th:AreturnEqu}
If a Markov decision process  with immediate reward is
transformed by above defined  and  
to a Markov decision process  with delayed
reward, where the reward is given at sequence end, then:
(I) the optimal policies do not change, and (II)
for 

for , , and .
\end{propositionA}


\begin{proof}
For (I) we first perform an state-enrichment of  by
 with  for  leading to an intermediate MDP.
We assume that the finite-valued  is 
sufficiently precise to distinguish the 
optimal policies, which are deterministic, 
from sub-optimal deterministic policies. 
Proposition~\ref{th:Aenrich} ensures that 
neither the optimal -values 
nor the optimal policies change between the original MDP 
and the intermediate MDP.
Next, we redistribute the original reward  
according to the redistributed reward . 
The new MDP  with state enrichment and reward redistribution
is strictly return-equivalent to 
the intermediate MDP with state enrichment but the original reward.
The new MDP  is Markov since the enriched state ensures that
 is Markov.
Proposition~\ref{th:AstrictToEq} and Proposition~\ref{th:AthRE} ensure that
the optimal policies are the same.

For (II) we show a proof without Bellman equation and a proof using
the Bellman equation.\\
{\bf Equivalence without Bellman equation.}
We have .
The Markov property ensures that the future reward is independent of
the already received reward:

We assume .

We obtain
 
We used , which is ensured
since reward probabilities, transition probabilities, and
the probability of choosing an action by the policy correspond to each
other in both settings.  


Since the optimal policies do not change for
reward-equivalent and state-enriched processes,
we have
 



{\bf Equivalence with Bellman equation.}
With  as optimal action-value function for the
original Markov decision process, we define a new Markov
decision process with action-state function .
For , , and  we have
 

Since , , and  is
constant, the values  and

can be computed from
, , and . Therefore, we have 
 


For , we have  and , where we set
:
 

For  we have  and
 as well as .
Both  and  must be zero for  since after time 
there is no more reward.
We obtain for  and :
 


Since  fulfills the Bellman 
equation, it is the action-value function for .

\end{proof}



\subsection{Transforming an Delayed Reward MDP to an Immediate Reward SDP}
\label{sec:AdelayIm}

Next we consider the opposite direction, where the delayed reward
MDP  is given and we want to find an immediate reward
SDP  that is return-equivalent to .
We assume an episodic reward for , that is, 
reward is only given at sequence end.
The realization of final reward, that is the realization of the return,
 is redistributed 
to previous time steps.
Instead of redistributing the realization  
of the random variable , also its expectation
 
can be redistributed since -value estimation 
considers only the mean.  
We used the Markov property 

Redistributing the expectation reduces the variance of estimators since
the variance of the random variable is already factored out.

We assume a delayed reward MDP  with reward

where  means that the random variable  is always zero.
The expected reward at the last time step is

which is also the expected return.
Given a state-action sequence ,
we want to redistribute either the realization  of the
random variable  or its expectation , 



\subsubsection{Optimal Reward Redistribution}
\label{sec:Aopt_rew_red}

The main goal in this paper is to derive 
an SDP via reward redistribution 
that has zero expected future rewards.
Consequently the SDP has no delayed rewards.
To measure the amount of delayed rewards,
we define the expected sum of delayed rewards .
\begin{definitionA}
For  and ,  
the expected sum of delayed rewards at time  
in the interval  is defined as

\end{definitionA}
The Bellman equation for -values becomes
 
where  
is the expected sum of future rewards until sequence end given , 
that is, in the interval . 
We aim to derive an MDP with ,
which gives .
In this case, 
learning the -values reduces to estimating the average immediate reward
.
Hence, the reinforcement learning task reduces to computing 
the mean, e.g.\ the arithmetic mean, for each
state-action pair .
Next, we define an optimal reward redistribution.
\begin{definitionA}
A reward redistribution is optimal,  
if   for .
\end{definitionA}

Next theorem states that in general an MDP with optimal reward
redistribution does not exist, which
is the reason why we will consider SDPs in the following.
\begin{theoremA}
\label{th:Aviolate}
In general, an optimal reward redistribution violates
the assumption that the reward distribution is Markov, 
therefore the Bellman equation does not hold.
\end{theoremA}

\begin{proof}
We assume an MDP  with
 and which has policies
that lead to different expected returns at time .
If all reward is given at time , 
all policies have the same expected return at time .
This violates our assumption, therefore not all reward can 
be given at .
In vector and matrix notation the Bellman equation is

where  is the row-stochastic matrix with
 
at positions .
An optimal reward redistribution requires the 
expected future rewards to be zero:

and, since optimality requires ,
we have

where  is the vector 
with components .
Since (i) the MDPs are return-equivalent, 
(ii) , 
and (iii) not all reward is given at ,
an  exists with .
We can construct an MDP  which has
(a) at least as many state-action pairs  
as pairs  and (b) the transition matrix
 has full rank.
 
is now a contradiction to
 and 
 has full rank.
Consequently, simultaneously ensuring Markov properties 
and ensuring zero future return
is in general not possible.
\end{proof}


For a particular ,
the next theorem states that an optimal reward redistribution,
that is , is equivalent to 
a redistributed reward which expectation is the difference of
consecutive -values of the original delayed reward.
The theorem states that an optimal reward redistribution exists but
we have to assume an SDP  that has a
second order Markov reward redistribution.
\begin{theoremA}
\label{th:AzeroExp}
We assume a delayed reward MDP , 
where the accumulated reward is given at sequence end.
An new SDP  is obtained by a 
second order Markov reward redistribution,
which ensures that  is return-equivalent to .
For a specific , the following two
statements are equivalent:
(I) ~~, i.e.\ the reward redistribution is optimal, 

 
Furthermore, an optimal reward redistribution
fulfills for  and :
 
\end{theoremA}


\begin{proof}
\label{c:t2p}
PART (I): we assume that the reward redistribution is optimal, that is,
 
The redistributed reward  is second order Markov.
We abbreviate the expected  by :
  
  
The assumptions of Lemma~\ref{th:AreturnSub} hold for 
for the delayed reward MDP  and
the redistributed reward SDP . 
Therefore for a given state-action sub-sequence
, :

with
 and .
The Markov property of the MDP 
ensures that the future reward  from  on is independent of
the past sub-sequence :

The second order Markov property of the SDP 
ensures that the future reward from  on is independent of
the past sub-sequence :


Using these properties we obtain

We used


It follows that 
 

~~\newline
~~\newline



PART (II): we assume that
 

The expectations

like

are expectations over all episodes starting in 
and ending in some .

First, we consider  and  , therefore
.
Since   for , we have
 
Using this equation we obtain for :
 

Next, we consider the expectation of 
for  and  (for )

We used that  for .

For  and  we have

which characterizes an optimal reward redistribution.

\end{proof}
Thus, an SDP with an optimal reward redistribution 
has a expected future rewards that are zero.
Equation  means that the new SDP 
has no delayed rewards as shown in next corollary.
\begin{corollaryA}
\label{th:ApropDelay}
An SDP with an optimal reward redistribution
fulfills for  

The SDP has no delayed rewards since no state-action pair
can increase or decrease the expectation of a future reward.
\end{corollaryA}

\begin{proof}
For  we use  from Theorem~\ref{th:AzeroExp}
with :


For ,
we also use  from Theorem~\ref{th:AzeroExp}:


\end{proof}


A related approach is to ensure zero return by 
reward shaping if the exact value function is known \cite{Schulman:15}.


The next theorem states the major advantage of an
optimal reward redistribution:
 can be estimated with an offset that 
depends only on  
by estimating the expected immediate redistributed reward.
Thus, -value estimation becomes trivial and the
the advantage function of the MDP  can be readily computed.
\begin{theoremA}
\label{th:AOptReturnDecomp}
If the reward redistribution is 
optimal, then the -values 
of the SDP  are given by 
 
The SDP  and the original MDP  
have the same advantage function.
Using a behavior policy 
 the expected immediate reward is

\end{theoremA}


\begin{proof}
The expected reward  is computed for , 
where  are states and actions, which are introduced 
for formal reasons at the beginning of an episode. 
The expected reward  
is with :
 

The expectations

like

are expectations over all episodes starting in 
and ending in some .

The -values for the SDP 
are defined for  as:
 
The second equality uses
 

The posterior   is
 
where we used  and  .
The posterior does no longer contain .
We can express the mean of previous -values
by the posterior  :
 
with 
 


The SDP  and the MDP
 have the same advantage function, 
since the value functions are the expected -values across the actions
and follow the equation .
Therefore  cancels in the advantage function of the SDP . 


~~\newline

Using a behavior policy 
 the expected immediate reward is
 
The posterior   is
 
where we used  and  .
The posterior does no longer contain .
We can express the mean of previous -values
by the posterior  :
 
with 
 
Therefore we have



\end{proof}




\subsection{Novel Learning Algorithms based on Reward Redistributions}
\label{sec:Alearning}
We assume  and a finite horizon or absorbing state
original MDP  with delayed reward.
According to Theorem~\ref{th:AOptReturnDecomp}, 
 can be estimated with an offset that 
depends only on  
by estimating the expected immediate redistributed reward.
Thus, -value estimation becomes trivial and the
the advantage function of the MDP  can be readily computed.
All reinforcement learning methods like policy gradients that use 
 or 
the advantage function 

of the original MDP 
can be used. These methods either rely on Theorem~\ref{th:AOptReturnDecomp}
and either estimate  according to Eq.~\eqref{eq:Aqvalue} 
or the expected immediate reward 
according to Eq.~\eqref{eq:Abehavior}. 
Both approaches estimate 
 with an offset 
that depends only on  (either  
or ).
Behavior policies like
``greedy in the limit with infinite exploration'' (GLIE) or
``restricted rank-based randomized'' (RRR) allow to prove
convergence of SARSA \cite{Singh:00}.
These policies can be used with reward redistribution.
GLIE policies can be realized by a softmax with exploration coefficient 
on the -values, 
therefore  or  cancels.
RRR policies select actions probabilistically according to 
the ranks of their -values, where the 
greedy action has highest probability. Therefore 
or 
is not required.
For function approximation, 
convergence of the -value estimation together 
with reward redistribution and GLIE or RRR policies 
can under standard assumptions be proven by the stochastic 
approximation theory for 
two time-scale update rules \cite{Borkar:97,Karmakar:17}.
Proofs for convergence to an optimal policy are in general 
difficult, since locally stable attractors 
may not correspond to optimal policies.


Reward redistribution can be used for
\begin{itemize}
\item
(A) -value estimation, 
\item
(B) policy gradients, and 
\item
(C) -learning.
\end{itemize}


\subsubsection{Q-Value Estimation}
\label{sec:QestimateA}
Like SARSA, RUDDER learning continually predicts 
-values to improve the policy. 
Type (A) methods estimate -values and are divided 
into variants (i), (ii), and (iii).
Variant (i) assumes an optimal reward redistribution
and estimates  with an offset
depending only on .
The estimates are based on Theorem~\ref{th:AOptReturnDecomp}
either by on-policy direct -value estimation according to Eq.~\eqref{eq:Aqvalue}
or by off-policy immediate reward estimation according to Eq.~\eqref{eq:Abehavior}.
Variant (ii) methods assume a non-optimal reward redistribution and 
correct Eq.~\eqref{eq:Aqvalue} by estimating .
Variant (iii) methods use eligibility traces for the redistributed reward.






\paragraph{Variant (i): Estimation of  
with an offset assuming optimality.}
Theorem~\ref{th:AOptReturnDecomp} justifies the estimation
of  with an offset
by on-policy direct -value estimation via Eq.~\eqref{eq:Aqvalue} or 
by off-policy immediate reward estimation via Eq.~\eqref{eq:Abehavior}.
RUDDER learning can be based on policies like
``greedy in the limit with infinite exploration'' (GLIE) or
``restricted rank-based randomized'' (RRR) \cite{Singh:00}. 
GLIE policies change toward greediness with respect to the -values
during learning.



\paragraph{Variant (ii): TD-learning of  and correction of the redistributed reward.}
For non-optimal reward redistributions  can be estimated 
to correct the -values.
{\bf TD-learning of .}
The expected sum of delayed rewards  can be formulated as
  
Therefore,  can be estimated by  and , 
if the last two are drawn together, 
i.e.\ considered as pairs. 
Otherwise the expectations of  and  given
 must be estimated.
We can use TD-learning if the immediate reward and 
the sum of delayed rewards are drawn as pairs, that is, simultaneously.  
The TD-error  becomes
  

We now define eligibility traces for .
Let the -step return samples of  for  be
  
The -return for  is
  
We obtain
  
We can reformulate this as
  
The  error  is
  
The derivative of 
  
with respect to  is
  

The full gradient of the sum of  errors is
 
We set , so that  becomes  and  becomes 
.
The recursion
  
can be written as
  

Therefore, we can use following update rule for minimizing 
 with respect to  with
:
  


{\bf Correction of the reward redistribution.}
For correcting the redistributed reward, we apply 
a method similar to reward shaping or look-back advice.
This method ensures that the corrected redistributed reward 
leads to an SDP that is has the same return per sequence as the
SDP .
The reward correction is 

we define the corrected redistributed reward as

We assume that , therefore
 
Consequently, the corrected redistributed reward  does not
change the expected return for a sequence, therefore, the resulting SDP has the
same optimal policies as the SDP without correction. 


For a predictive reward of  at time , which
can be predicted from time  to time , we have:

The reward correction is


{\bf Using  as auxiliary task 
in predicting the return for return decomposition.}
A  prediction can serve as additional output of 
the function  that predicts the return and
is the basis of the return decomposition. 
Even a partly prediction of  means that 
the reward can be distributed further back. 
If  can partly predict , then 
has all information to predict the return earlier in the 
sequence. If the return is predicted 
earlier, then the reward will be distributed further back.
Consequently, the reward redistribution 
comes closer to an optimal reward redistribution.
However, at the same time,  can no longer be predicted.  
The function  must find another  that can be predicted.
If no such  is found, then optimal reward redistribution is
indicated.



\paragraph{Variant (iii): Eligibility traces assuming optimality.}
We can use eligibility traces to further distribute the reward back.
For an optimal reward redistribution, we have
. 
The new returns  are given by the recursion

The expected policy gradient updates with the new returns  are
.
To avoid an estimation of the value function , 
we assume optimality,
which might not be valid. However, the error should be small if the 
return decomposition works well.
Instead of estimating a value function, we can use a correction 
as it is shown in
next paragraph.



\subsubsection{Policy Gradients}
\label{sec:PolicyA}
Type (B) methods are policy gradients.
 In the expected updates  
 of policy gradients, the value  
is replaced by an estimate of  or by
samples of the redistributed reward. 
Convergence to optimal policies is guaranteed even with the
offset  in Eq.~\eqref{eq:Aqvalue} 
similar to baseline normalization for policy gradients.
With baseline normalization,
the baseline  
is subtracted from , which gives the policy gradient  
. 
With eligibility traces using  for  \cite{Sutton:18book}, we have 
the new returns  with .
The expected updates with the new returns  are
.


\subsubsection{Q-Learning}
\label{sec:qlearningA}
The type (C) method is -learning with the redistributed reward. 
Here, -learning is justified if
immediate and future reward are drawn together,
as typically done.
Also other temporal difference methods are justified when
immediate and future reward are drawn together.

\subsection{Return Decomposition to construct a Reward Redistribution}


We now propose methods to construct reward redistributions which 
ideally would be optimal. 
Learning with non-optimal reward redistributions {\em does work} since the 
optimal policies do not change according to Theorem~\ref{th:AEquivT}.
However reward redistributions that are optimal considerably speed up learning,
since future expected rewards introduce 
biases in TD-methods and the high variance in MC-methods.
The expected optimal redistributed reward is according to Eq.~\eqref{eq:AdiffQ} 
the difference of -values. 
The more a reward redistribution deviates from these differences,
the larger are the absolute -values and, in turn, the less optimal
is the reward redistribution.
Consequently we aim at identifying the largest -value differences to
construct a reward redistribution which is close to optimal.
Assume a grid world where you have to take a key to later open a door
to a treasure room. Taking the key increases the chances to receive the
treasure and, therefore, is associated with a large positive -value difference.
Smaller positive -value difference are steps toward the key location.

\paragraph{Reinforcement Learning as Pattern Recognition.}
We want to transform the reinforcement learning problem into
a pattern recognition problem to employ deep learning approaches.
The sum of the -value differences gives the 
difference between expected return at sequence begin and
the expected return at sequence end (telescope sum).
Thus, -value differences allow to predict the 
expected return of the whole state-action sequence.
Identifying the largest -value differences 
reduce the prediction error most.
-value differences are assumed to be associated with
patterns in state-action transitions 
like taking the key in our example. 
The largest -value differences 
are expected to be found more frequently in sequences
with very large or very low return.
The resulting task is to predict the expected return
from the whole sequence and identify which 
state-action transitions contributed most to the prediction.
This pattern recognition task is utilized to
construct a reward redistribution, where redistributed reward
corresponds to the contribution.


\subsubsection{Return Decomposition Idea}

The {\em return decomposition idea} is 
to predict the realization of the return or its
expectation by a function  from the state-action sequence 

The return is the accumulated reward along the whole sequence
.
The function  depends on the policy  that is used to 
generate the state-action sequences.
Subsequently, the prediction or the realization of the return
is distributed over
the sequence with the help of .
One important advantage of a deterministic function  is 
that it predicts with proper loss functions and if being perfect 
the expected return. Therefore, it 
removes the sampling variance of returns. 
In particular the variance of probabilistic rewards is averaged out. 
Even an imperfect function  removes the variance as it is deterministic.
As described later, the sampling variance may be 
reintroduced when strictly return-equivalent SDPs are ensured.
We want to determine for each sequence element 
its contribution to the prediction of the function . 
Contribution analysis computes the contribution of each 
state-action pair to the prediction, that is, the information of each
state-action pair about the prediction. 
In principle, we can use any contribution analysis method.
However, we prefer three methods:
(A) Differences in predictions.
If we can ensure that  predicts the sequence-wide return
at every time step. 
The difference of two consecutive predictions is a measure of
the contribution of the current state-action pair to the return prediction.
The difference of consecutive predictions is the redistributed reward.
(B) Integrated gradients (IG) \cite{Sundararajan:17}.
(C) Layer-wise relevance propagation (LRP) \cite{Bach:15}.
The methods (B) and (C) use information later in the sequence for
determining the contribution of the current state-action pair. Therefore,
they introduce a non-Markov reward. 
However, the non-Markov reward can be viewed as probabilistic reward.
Since probabilistic reward increases the variance, we prefer method (A).

\paragraph{Explaining Away Problem.}
\label{para:AexplainingAway}
We still have to tackle the problem that reward causing actions do not receive redistributed rewards
since they are explained away by later states.
To describe the problem, assume an MDP  with the only 
reward at sequence end.
To ensure the Markov property, states in  have to store 
the reward contributions of previous state-actions;
e.g.\  has to store all previous contributions such that the expectation 
is Markov.
The explaining away problem is that later states
are used for return prediction, while reward causing
earlier actions are missed.
To avoid explaining away,
between the state-action pair  and its predecessor , where
 are introduced for starting an episode.
The sequence of differences is defined as

We assume that the differences  
are mutually independent \cite{Hyvarinen:01}:

The function  predicts the realization of the sequence-wide
return or its expectation from the sequence :
 
{\bf Return decomposition} deconstructs  into contributions
 at time :
 
If we can assume that  can predict the return at every time step:
 
then we use the contribution analysis method "differences of return predictions",
where the contributions are defined as:
 

We assume that the sequence-wide return cannot be predicted from the
last state. The reason is that either immediate rewards are given only 
at sequence end without storing them in the states 
or information is removed from the states.
Therefore, a relevant event for predicting the final reward must be identified by
the function . 
The prediction errors at the end of the episode become, in general, smaller since the
future is less random. Therefore, prediction errors later 
in the episode are up-weighted while early predictions ensure that information is
captured in  for being used later.
The prediction at time  has the largest weight and 
relies on information from the past.

If  does predict the return at every time step,
contribution analysis decomposes .
For decomposing a linear  one can use the Taylor decomposition 
(a linear approximation) of  with respect to the 
\cite{Bach:15,Montavon:17taylor}.
A non-linear  can be decomposed by 
layerwise relevance propagation (LRP)
\cite{Bach:15,Montavon:17} or integrated gradients (IG)
\cite{Sundararajan:17}.


\subsubsection{Reward Redistribution based on Return Decomposition}

We assume a return decomposition
 
with
 
We use these contributions for redistributing the reward. 
The reward redistribution is given by 
the random variable  for the reward at time .
These new redistributed rewards
 must have the contributions  as mean:
 

The reward  of 
is probabilistic and 
the function  might not be perfect,
therefore neither  for the return
realization  nor
 for the expected return
holds.
To assure strictly return-equivalent SDPs,
we have to compensate for both a probabilistic reward  
and an imperfect function .
The compensation is given by
 
We compensate with an extra reward  at time  
which is immediately given
after  at time  after the state-action pair .
The new redistributed reward  is 

where the realization  is replaced by its
random variable .
If the the prediction of  is perfect, then we can set
 and redistribute the expected return which is
the predicted return.
 compensates for both a probabilistic reward  
and an imperfect function .
Consequently all variance of sampling the return is moved to  .
Only the imperfect function  must be corrected while the variance
does not matter. 
However, we cannot distinguish, e.g.\ in early learning phases, 
between errors of  and random reward. 
{\bf A perfect  results in an optimal reward redistribution.}

Next theorem
shows that Theorem~\ref{th:AzeroExp} holds also for
the correction . 
\begin{theoremA}
\label{th:AzeroExpCorr}
The optimality conditions
hold also for reward redistributions with corrections:
 
\end{theoremA}

\begin{proof}
The expectation of 
, 
that is  with .

If we substitute  by  ( one step further and  one step smaller)
it follows 
 

Next, we consider the case , that is , 
which is the expected correction.
We will use following equality for the 
expected delayed reward at sequence end:
 
since  .
For  we obtain
 

\end{proof}

In the experiments we also use a uniform compensation 
where each reward has the same
contribution to the compensation:

Consequently all variance of sampling the return is 
uniformly distributed across
the sequence. Also the error of  is uniformly distributed 
across the sequence.

An optimal reward redistribution implies
 
since the expected reward is
 
according to Eq.~\eqref{eq:AdiffQ} in Theorem~\ref{th:AzeroExp}
and 
 




\subsection{Remarks on Return Decomposition}
\label{sec:Aremark}

\subsubsection{Return Decomposition for Binary Reward}
A special case is a reward that indicates success or failure by giving 
a reward of 1 or 0, respectively. 
The return is equal to the final reward , which is a Bernoulli variable.
For each state  or each
state-action pair  the expected return can be 
considered as a Bernoulli variable with success probability
 or . The value function is 
and  the action-value is  which is in 
both cases the expectation of success. 
In this case, the optimal reward redistribution tracks the success probability
 
The redistributed reward is the change in the success probability. A good action
increases the success probability and obtains a positive reward while a bad action
reduces the success probability and obtains a negative reward.


\subsubsection{Optimal Reward Redistribution reduces the MDP 
to a Stochastic Contextual Bandit Problem}

The new SDP  has a redistributed reward
with random variable  at time  distributed according to .
Theorem~\ref{th:AOptReturnDecomp} states
 
This equation looks like a contextual bandit problem, where
 is an estimate of the mean reward for action 
for state or context .
Contextual bandits \cite[p. 208]{Lattimore:18} are characterized by 
a conditionally -subgaussian noise (Def.~5.1 \cite[p. 68]{Lattimore:18}).
We define the zero mean noise variable  by
 
where we assume that  is a conditionally -subgaussian noise variable. 
Therefore,  is distributed 
according to  and fulfills
 
Subgaussian random variables have tails that decay almost as fast as a Gaussian.
If the reward  is bounded by , then  is bounded by  
and, therefore, a -subgaussian. 
For binary rewards it is of interest that 
a Bernoulli variable is 0.5-subgaussian \cite[p. 71]{Lattimore:18}.
In summary, an optimal reward redistribution reduces the MDP 
to a stochastic contextual bandit problem.






\subsubsection{Relation to ''Backpropagation through a Model}
The relation of reward redistribution if applied to policy gradients
and ''Backpropagation through a Model is discussed here.
For a delayed reward that is only received at the end of an episode, 
we decompose the return  into

The policy gradient for an optimal reward redistribution is
 
Summing up the gradient for one episode, the gradient becomes 
 
where   
and  are the sequences of actions, 
 and  
are the sequences of states,
 is the Jacobian of the -probability of the 
state sequence with respect to the parameter vector ,
and  is the vector with
entries . 

An alternative approach via sensitivity analysis is ''Backpropagation through a Model, 
where  is maximized, that is, the return is maximized.
Continuous actions are directly fed into  while probabilistic actions are 
sampled before entering . Analog to gradients used for Restricted Boltzmann Machines,
for probabilistic actions the -likelihood of the actions is used to construct a gradient.
The likelihood can also be formulated as the cross-entropy
between the sampled actions and the action probability.
The gradient for ''Backpropagation through a Model is
 
where  is the gradient of  with respect to the action sequence . 

If for ''Backpropagation through a Model 
the model gradient with respect to actions
is replaced by the vector of contributions of actions in the model, 
then we obtain 
redistribution applied to policy gradients.

\section{Bias-Variance Analysis of MDP Q-Value Estimators}
\label{sec:AbiasVariance}

Bias-variance investigations have been done for -learning.
Gr{\"{u}}new{\"{a}}lder \& Obermayer \cite{Grunewalder:11}
investigated the bias of temporal
difference learning (TD), Monte Carlo estimators (MC), and least-squares temporal
difference learning (LSTD).
Mannor et al.\ \cite{Mannor:07} and O'Donoghue et al.\ \cite{ODonoghue:17}
derived bias and variance expressions for updating
-values.

The true, but unknown, action-value function 
is the expected future return.
We assume to have the data ,
which is a set of state-action sequences with return,
that is a set of episodes with return.
Using data ,  is 
estimated by , which
is an estimate with bias and variance.
For bias and variance we have to compute
the expectation  over the data .
The mean squared error (MSE) of an estimator  is

The bias of an estimator  is

The variance of an estimator  is

The bias-variance decomposition of the MSE of an estimator  is


The bias-variance decomposition of the MSE of an estimator
 as a vector is





\subsection{Bias-Variance for MC and TD Estimates of the Expected Return}
\label{sec:Abias_variance_estimator}

{\bf Monte Carlo (MC)} computes the arithmetic mean
 of  for  over the episodes
given by the data. 


For {\bf temporal difference (TD)} methods,
like SARSA, with learning rate  the updated estimate of  is:

Similar updates are used for expected SARSA and -learning, where
only  is chosen differently. 
Therefore,
for the estimation of , SARSA and -learning perform 
an exponentially weighted arithmetic mean of  .
If for the updates  is fixed on some data, then
SARSA and -learning perform an exponentially weighted arithmetic mean of the immediate
reward  plus averaging over which  (which ) is chosen.
In summary, TD methods like SARSA and -learning are biased via
 and perform an exponentially weighted arithmetic mean of the
immediate reward  and the next (fixed) .

{\bf Bias-Variance for Estimators of the Mean.}
Both Monte Carlo and TD methods, like SARSA and -learning, respectively, estimate
, which is the expected
future return. The expectations are estimated
by either an arithmetic mean over samples
with Monte Carlo or an exponentially weighted arithmetic mean over samples with TD methods.
Therefore, we are interested in computing the bias and variance of
these estimators of the expectation.
In particular, we consider the arithmetic mean and the
exponentially weighted arithmetic mean.

We assume  samples for a state-action pair . However, the expected
number of samples depends on the probabilistic number of visits of
 per episode. 

{\bf Arithmetic mean.}
For  samples  from a distribution with mean
 and variance , the arithmetic mean, its bias and and its variance are:

The estimation variance of the arithmetic mean is determined by ,
the variance of the distribution the samples are drawn from.

{\bf Exponentially weighted arithmetic mean.}
For  samples  from a distribution with mean
 and variance , the variance of the exponential mean with initial value  is

which gives


This is a weighted arithmetic mean with 
exponentially decreasing weights, 
since the coefficients sum up to one:




The estimator  is biased, since:

Asymptotically () the estimate is unbiased.
The variance is

Also the estimation variance of the exponentially weighted arithmetic mean is proportional to ,
which is the variance of the distribution the samples are drawn from.

The deviation of random variable  from its
mean  can be analyzed with Chebyshev's inequality.
Chebyshev's inequality \cite{Bienayme:53,Chebyshev:67} states that for 
a random variable  with expected value 
and variance  and for any real number :

or, equivalently,


For  samples  from a distribution with expectation
 and variance  we compute the arithmetic mean
.
If  is the arithmetic mean, then
 and we obtain




Following Gr{\"{u}}new{\"{a}}lder and Obermayer \cite{Grunewalder:11},
Bernstein's inequality can be used to describe the deviation of
the arithmetic mean (unbiased estimator of )
from the expectation 
(see Theorem~6 of G{\'{a}}bor Lugosi's
lecture notes \cite{Lugosi:03}):

where .


\subsection{Mean and Variance of an MDP Sample of the Return}
\label{sec:Abias_variance_sample}

Since the variance of the estimators of the expectations (arithmetic mean and 
exponentially weighted arithmetic mean) is governed by the variance of the samples, 
we compute mean and variance of the return estimate . 
We follow
\cite{Sobel:82,Tamar:12,Tamar:16} for deriving the mean and variance.

We consider an MDP with finite horizon , 
that is, each episode has length .
The finite horizon MDP can be generalized to an MDP 
with absorbing (terminal) state .
We only consider proper policies, that is there exists an integer 
such that from any initial state the probability of achieving
the terminal state  after  steps is strictly positive.
 is the time to the
first visit of the terminal state: .
The return  is:


The action-value function, the -function, is the expected return

if starting in state  and action :


The second moment of the return is:


The variance of the return is:



Using  , and analogously  and , the
next Theorem~\ref{th:Avar} gives mean and variance

of sampling returns from an MDP.


\begin{theoremA}
  \label{th:Avar}
  The mean   and variance  
  of sampled returns from an MDP are

\end{theoremA}

\begin{proof}
\label{proof:Avar}
The Bellman equation for -values is

This equation gives the mean if drawing one sample.
We use

For the second moment, we obtain \cite{Tamar:12}:



For the variance, we obtain:

\end{proof}


For deterministic reward, that is,
,
the corresponding result is given as
Equation~(4) in Sobel 1982 \cite{Sobel:82}
and as Proposition~3.1 (c) in Tamar et al.\ 2012 \cite{Tamar:12}.

For temporal difference (TD) learning,
the next -values are fixed to  when drawing a sample.
Therefore, TD is biased, that is, both SARSA and -learning are biased.
During learning with according updates of -values, 
approaches , and the bias is reduced.
However, this reduction of the bias is exponentially small in the number of time
steps between reward and updated -values, as we will see later.
The reduction of the bias is exponentially small for eligibility
traces, too.



The variance recursion  Eq.~\eqref{eq:Avarq} of sampled returns consists of three parts:
\begin{itemize}
\item (1) the immediate variance
 of the
immediate reward stemming from the
probabilistic reward ,
\item (2) the local
variance  
from state transitions  and new actions ,
\item (3) the expected
variance 
of the next -values.
\end{itemize}
For different settings the following parts may be zero:
\begin{itemize}
\item (1) the immediate variance
 is zero for deterministic
immediate reward,
\item (2) the local
variance  
is zero for (i) deterministic state transitions and deterministic policy
and for (ii)  (only immediate reward),
\item (3) the expected
variance 
of the next -values is zero for (i) temporal difference (TD) learning,
since the next -values are fixed and set to their current estimates
(if just one sample is drawn)
and for (ii)  (only immediate reward).
\end{itemize}


The local variance 
is the variance of a linear combination of -values
weighted by a multinomial distribution
.
The local variance is

This result is Equation~(6) in Sobel 1982 \cite{Sobel:82}.
Sobel derived these formulas also for finite horizons and
an analog formula if the reward depends
also on the next state, that is, for .


Monte Carlo uses the accumulated future rewards for updates, therefore
its variance is given by the recursion in Eq.~\eqref{eq:Avarq}.
TD, however, fixes  to the current
estimates , which do not change in the current
episode. 
Therefore, TD has  and
only the local variance

is present.
For -step TD, the recursion in Eq.~\eqref{eq:Avarq} must be applied
 times. Then, the expected next variances are zero since
the future reward is estimated by .

{\bf Delayed rewards}.
For TD and delayed rewards, information on new data is only captured by the
last step of an episode that receives a reward. This reward is used
to update the estimates of the -values of the
last state .
Subsequently, the reward information is propagated
one step back via the estimates  for each sample.
The drawn samples (state action sequences)
determine where information is propagated back.
Therefore, delayed reward introduces a large bias for TD over a long
period of time,
since the estimates  need a long time to reach their true -values.

For Monte Carlo and delayed rewards,
the immediate variance  except for the
last step of the episode. 
The delayed reward
increases the variance of -values according to Eq.~\eqref{eq:Avarq}.




{\bf Sample Distribution Used by Temporal Difference and
  Monte Carlo.}
  \label{sec:ASDTDMC}
Monte Carlo (MC) sampling uses the true mean and true variance, where the true mean is

and the true variance is


Temporal difference (TD) methods replace
 by  which does not depend on
the drawn sample.
The mean which is used by temporal difference is

This mean is biased by

The variance used by temporal difference is

since  if  is used instead of
the future reward of the sample.
The variance of TD is smaller than for MC, since variances are not
propagated back.




\subsection{TD corrects Bias exponentially slowly with Respect to
  Reward Delay}
\label{sec:ATDslow}



\paragraph{Temporal Difference.}
We show that TD updates for delayed rewards are exponentially small,
fading exponentially with the number of delay steps.
-learning with learning rates  at the th update
leads to an arithmetic mean as estimate, which was shown to be
exponentially slow \cite{Beleznay:99}. 
If for a fixed learning rate the agent always travels along 
the same sequence of states, then TD is superquadratic \cite{Beleznay:99}.
We, however, consider the general case where the agent travels 
along random sequences due to a random environment or due to exploration.
For a fixed learning rate,
the information of the delayed reward has to be propagated back
either through the Bellman error or via eligibility traces.
We first consider backpropagation of reward information via the
Bellman error.
For each episode
the reward information is propagated back one step at visited
state-action pairs via the TD update rule.
We denote the -values of episode  as  and assume that the
state action pairs  are the most visited ones.
We consider the update of  of
a state-action pair  that is visited at time  in
the th episode: 
 

\paragraph{Temporal Difference with Eligibility Traces.}
Eligibility traces have been introduced to propagate
back reward information of an episode and are now standard for
TD() \cite{Singh:96}.
However, the eligibility traces are exponentially decaying when
propagated back.
The accumulated trace is defined as \cite{Singh:96}:
 
while the replacing trace is defined as \cite{Singh:96}:
 

With eligibility traces using , the -return  is \cite{Sutton:18book}
 
We obtain
 




We use the naive , where eligibility traces are not set to
zero.
In contrast, Watkins'  \cite{Watkins:89}
zeros out eligibility traces
after non-greedy actions, that is, if not the  is chosen.
Therefore, the decay is even stronger for  Watkin's .
Another eligibility trace method is Peng's  \cite{Peng:96}
which also does not zero out eligibility traces.


The next Theorem~\ref{th:AexponDecay} 
states that the decay of TD is exponential for
-value updates in an MDP with delayed reward, even for eligibility traces.
Thus, for delayed rewards TD requires exponentially many updates to correct the bias, 
where the number of updates is exponential in the delay steps.  
\begin{theoremA}
\label{th:AexponDecay}
For initialization   and
delayed reward with  for , 
 receives its 
first update not earlier than at episode  via
,
where  is the reward of episode 1.
Eligibility traces with  
lead to an exponential decay of  when the reward
is propagated  steps back.
\end{theoremA}


\begin{proof}
\label{proof:ATD}

If we assume that -values are initialized with zero, then
 for all .
For delayed rewards we have  for .
The -value   at time  can receive an
update for the first time at episode . Since all -values have
been initialized with zero, the update is
 
where  is the reward at time  for episode 1.

We move on to eligibility traces, where the
update for a state  is
 

If states are not revisited,
the eligiblity trace at time  for a visit of state  at time  is:
 
If all  are zero except for ,
then the update of  is
 
\end{proof}

A learning rate of  does not work since it would imply to
forget all previous learned estimates, and therefore no averaging over
episodes would exist.
Since , we observe exponential decay backwards in time for online updates. 


\subsection{MC affects the Variance of Exponentially Many Estimates
  with Delayed Reward }
  \label{sec:AMCvariance}


The variance for Monte Carlo is

This is a Bellman equation of the variance. For undiscounted reward , we obtain


If we define the ``on-site'' variance  as

we get

This is the solution of the
general formulation of the
Bellman operator. The Bellman operator
is defined component-wise for any variance  as

According to the results in Section~\ref{sec:ApropPoly},
for proper policies  a unique fixed point  exists:

where  is any initial variance.
In Section~\ref{sec:ApropPoly} it was shown that
the operator  is
continuous, monotonically increasing (component-wise larger or smaller),
and a contraction mapping for
a weighted sup-norm. If we define the operator  as depending on the on-site
variance , that is
 , then it is monotonically in . 
We obtain component-wise for :


It follows for the fixed points  of  and
 of : 

Therefore if 

then 




\begin{theoremA}
Starting from the sequence end at ,
as long as  holds also
the following holds:

If for  the strict inequality
 
holds, then we have the strict inequality

If  for some   then

Therefore, the strict inequality 
 
is propagated back as a strict inequality of variances.
\end{theoremA}

\begin{proof}
Proof by induction:
Induction base:   and
.

Induction step ():
The induction hypothesis is that for all  we have

and  .
It follows that

We obtain

If for  the strict inequality 
 
holds, then we have the strict inequality
.
If  for some   then

Therefore, the strict inequality 
 
is propagated back as a strict inequality of variances
as long as   for some  .

The induction goes through as long as 
.
\end{proof}


In Stephen Patek's PhD thesis, \cite{Patek:97} Lemma~5.1 on page 88-89 and proof
thereafter state that if  , then
the solution  is continuous and decreasing in .
From the inequality above it follows that



\paragraph{Time-Agnostic States.}


We defined a Bellman operator as

where  is the vector with value  at position 
and  is the vector with value
 at position . The fixed point equation is known as
the {\em Bellman equation}. In vector and matrix notation
the Bellman equation reads

where  is the row-stochastic matrix with
 at position .
We assume that the set of state-actions  is equal to the set
of next state-actions , therefore  is a square
row-stochastic matrix.
This Bellman operator has the same characteristics as the Bellman
operator for the action-value function .

Since  is a row-stochastic matrix, the
Perron-Frobenius theorem says
that (1)  has as largest eigenvalue 1 for which
the eigenvector corresponds to the steady state and
(2) the absolute value of each (complex) eigenvalue is smaller equal 1.
Only the eigenvector to eigenvalue 1 has purely positive real components.
Equation~7 of Bertsekas and Tsitsiklis, 1991, \cite{Bertsekas:91}
states that


Applying the operator  recursively
 times can be written as \cite{Bertsekas:91}:

In particular for , we obtain

For finite horizon MDPs,
the values  are correct for time step  since
no reward for  exists. Therefore, the 
``backward induction algorithm'' \cite{Puterman:90,Puterman:05} gives
the correct solution:

The product of square stochastic matrices is a stochastic matrix,
therefore  is a stochastic matrix.
Perron-Frobenius theorem states that the spectral radius
 of the stochastic matrix  is: .
Furthermore, the largest eigenvalue is 1 and all
eigenvalues have absolute values smaller or equal one.
Therefore,  can have large influence on  at every time step. 




\paragraph{Time-Aware States.}

Next we consider time-aware MDPs, where transitions occur only from
states  to . The transition matrix from
states  to  is denoted by . We assume that
 are row-stochastic matrices which are rectangular, that is
.
\begin{definitionA}
A row-stochastic matrix  has
non-negative entries and the entries of each row sum up to one.
\end{definitionA}
It is known that the product of
square stochastic matrices   is a
stochastic matrix. We show in next theorem
that this holds also for rectangular matrices.
\begin{lemmaA}
\label{th:ArowStochastic}
  The product  with 
  of a row-stochastic matrix 
and a row-stochastic matrix  is row-stochastic.
\end{lemmaA}

\begin{proof}
  All entries of  are non-negative since they are sums and
  products of non-negative entries of  and
  .
  The row-entries of  sum up to one:
   
\end{proof}


We will use the -norm and the
1-norm of a matrix, which are defined based on the
-norm  and 1-norm 
of a vector .
\begin{definitionA}
The -norm of a matrix is the maximum absolute row sum: 
 
The 1-norm of a matrix is the maximum absolute column sum: 
 
\end{definitionA}


The statements of next theorem are known as Perron-Frobenius theorem
for square stochastic matrices , e.g.\ that
the spectral radius  is .
We extend the theorem to a ``-norm equals one'' property for
rectangular stochastic matrices . 
\begin{lemmaA}[Perron-Frobenius]
\label{th:Aperron}
If  is a row-stochastic matrix, then
 
\end{lemmaA}

\begin{proof}
 is a row-stochastic matrix, therefore
. Furthermore, the rows of  sum up
to one. Thus,  .
Since the column sums of  are the row sums of , it follows
that  .

For square stochastic matrices, that is ,
Gelfand's Formula (1941) says that
for any matrix norm , for the spectral norm
 of a matrix  we obtain:
 


Since the product of row-stochastic matrices is a row-stochastic
matrix,  is a row-stochastic matrix.
Consequently  and . 
Therefore, the spectral norm
 of a row-stochastic matrix 
is
 

The last statement follows from
Perron-Frobenius theorem, which says that the spectral radius of  is 1. 
\end{proof}

Using random matrix theory, we can guess how much the spectral radius
of a rectangular matrix deviates from that of a square matrix.
Let  be a matrix whose entries are independent copies of some random
variable with zero mean, unit variance, and finite fourth moment.
The Marchenko-Pastur quarter circular law for rectangular
matrices says that for  the maximal singular value
is  \cite{Marchenko:67}.
Asymptotically we have for the maximal
singular value  \cite{Rudelson:10}. 
A bound on the largest singular value is given by \cite{Soshnikov:02}:

Therefore, a rectangular matrix modifies the largest singular value by a factor of 
 compared to a  square matrix.
In the case that tstates are time aware, transitions only occur from
states  to . The transition matrix from
states  to  is denoted by .

{\bf States affected by the on-site variance  (reachable states).}
Typically,   states in  have only few predecessor states in
 compared to , the number of possible states in .
Only for those states in  the transition probability to the
state in  is larger than zero.
That is, each  has only few  for which
.
We now want to know how many states have increased variance due to
, that is how many states are affected by .
In a general setting, we assume random connections.

Let  be the number of all states  that are reachable after  time steps
of an episode.
 is the arithmetic mean of .
Let  be the
average connectivity of a state in  to states in  and
 the geometric mean of
the .
Let  be the number of states in  that are affected by the
on-site variance  at time  
for .
The number of states affected by  is .
We assume that  only has one component larger than zero, that
is, only one state at time  is affected: .
The number of affected edges from  to  is .
However, states in  may be affected multiple times by different
affected states in .
Figure~\ref{fig:affectedStates} shows examples of how affected states affect states in a
previous time step. The left panel shows no overlap
since affected states in  connect only to
one affected state in . The right panel shows some overlap
since affected states in  connect to multiple
affected states in .

\begin{figure}[htp]
\centering
\includegraphics[angle=0,width=0.49\textwidth]{net5treeac}
\includegraphics[angle=0,width=0.49\textwidth]{net5treebc}
\caption{Examples of how affected states (cyan) affect states in a
  previous time step (indicated by cyan edges) starting with 
  (one affected state).  The left panel shows no overlap
  since affected states in  connect only to
  one affected state in . The right panel shows some overlap
  since affected states in  connect to multiple
  affected states in .
\label{fig:affectedStates}}
\end{figure}


\begin{figure}[htb]
\centering
\includegraphics[angle=0,width=0.8\textwidth]{scaleFunction}
\caption{The function  which scales  in
    Theorem~\ref {th:Aaffect}. This function determines the growth of
    , which is exponentially at the beginning, and then linearly
    when the function approaches 1.
\label{fig:growth}}
\end{figure}



The next theorem states that the on-site
variance  can have large effect on the variance of
each previous state-action pair. 
Furthermore, for small  the number of affected
states grows exponentially, while for large  it grows only 
linearly after some time . Figure~\ref{fig:growth} shows the
function which determines how much  grows with . 

\begin{theoremA}
\label{th:Aaffect}
  For ,  contributes to 
  by the term , where .

The number  of states affected by the on-site variance
 is

\end{theoremA}



\begin{proof}
The ``backward induction algorithm'' \cite{Puterman:90,Puterman:05}
gives with  and on-site variance :

where we define  and .

Since the product of two row-stochastic matrices is a row-stochastic
matrix according to Lemma~\ref{th:ArowStochastic},
 is a row-stochastic matrix.
Since  according to Lemma~\ref{th:Aperron},
each on-site variance  with  can have large effects on
.
Using the row-stochastic matrices , we can
reformulate the variance:

with .
The on-site variance  at step 
increases all variances  with .

Next we proof the second part of the theorem, which considers the
growth of .
To compute  we first have
to know . For computing  from , we want to know how
many states are affected in  if 
states are affected in .
The answer to this question is the expected
coverage when searching a document collection
using a set of independent computers \cite{Cox:09}.  
We follow the approach of Cox~et~al.\ \cite{Cox:09}.
The minimal number of affected states in  is , where
each of the  affected states in  
connects to each of the  states in  (maximal overlap).
The maximal number of affected states in  is
, where each affected state in 
connects to only one affected state in  (no overlap).
We consider a single state in .
The probability of a state in  being connected to this single
state in  is  and being not connected to this state in
 is .
The probability of a state in 
being not connected to any of the  affected states in  is

The probability of a state in 
being at least connected to one of the  affected states in  is

Thus, the expected number of distinct states in  being
connected to one of the  affected states in  is

The number  of affected states by  is

\end{proof}

\begin{corollaryA}
\label{th:Acoro}
 For small , the number  of states affected by
the on-site variance  at step 
grows exponentially with  by a factor of : 

For large  and after some time ,
the number  of states affected by
 grows linearly with  with a factor of : 

\end{corollaryA}

\begin{proof}
For small  with , we have

thus


For large  compared to the number of connections  of a
single state in  to states in ,
we have the approximation

We obtain

For small , we again have

Therefore, for small , we obtain

Thus, for small  the number  of states affected by  is

Consequently, for small  the number  of states affected by  grows exponentially with  by a factor of .  
For large , at a certain time ,  has grown such that
, yielding
, and thus

Therefore 

Consequently, for large  the number  of states affected by
 grows linearly with  by a factor of .  
\end{proof}

Therefore, we aim for decreasing the on-site variance  for large , in order to
reduce the variance.
In particular, we want to avoid delayed rewards and provide the reward
as soon as possible in each episode.
Our goal is to give the reward as early as possible in each episode to reduce the
variance of action-values that are affected by late rewards and
their associated immediate and local variances.



\pagebreak
\section{Experiments}

\subsection{Artificial Tasks}
\label{sec:Aexp}
This section provides more details for the artificial tasks (I), (II) and (III) in the main paper. Additionally, we include artificial task (IV) characterized by deterministic reward and state transitions, and artificial task (V) which is solved using policy gradient methods.


\subsubsection{Task (I): Grid World}
\label{sec:AGW}
This environment is characterized by probabilistic delayed rewards.
It illustrates a situation, where a time bomb
explodes at episode end.
The agent has to defuse the bomb
and then run away as far as possible since
defusing fails with a certain probability.
Alternatively, the agent can immediately run away, 
which, however, leads to less reward on average 
since the bomb always explodes.
The Grid World is a quadratic  grid with
{\em bomb} at coordinate  and
{\em start} at , where  is the delay of the task.
The agent can move in four different directions 
({\em up}, {\em right}, {\em left}, and {\em down}).
Only moves are allowed that keep the agent on the grid. 
The episode finishes after  steps.
At the end of the episode, 
with a given probability of 0.5, the agent receives a reward of 1000
if it has visited {\em bomb}.
At each time step the agent receives 
an immediate reward of , 
where the factor  depends on the chosen action, 
 is the current time step, and  is the Hamming distance to {\em bomb}.
Each move of the agent, which reduces the Hamming distance to {\em bomb}, 
is penalized by the immediate reward via . 
Each move of the agent, which increases the Hamming distance to {\em bomb}, 
is rewarded by the immediate reward via .
The agent is forced to learn the -values precisely,
since the immediate reward of directly running away hints at a sub-optimal policy.

For non-deterministic reward, the agent receives the delayed reward for
having visited {\em bomb} with probability . 
For non-deterministic transitions, the probability of transiting to next state  
is . 
For the deterministic environment these probabilities were either 1 or zero. 


\paragraph{Policy evaluation: learning the action-value estimator for a fixed policy.} 
First, the theoretical statements on bias and variance of 
estimating the action-values by TD in Theorem~\ref{th:AexponDecay} and 
by MC in Theorem~\ref{th:Aaffect} are experimentally verified for a fixed policy.
Secondly, we consider the bias and variance of TD and MC estimators 
of the transformed MDP with optimal reward redistribution according to Theorem~\ref{th:AOptReturnDecomp}.

The new MDP with an optimal reward redistribution has advantages over
the original MDP both for TD and MC. For TD, the new MDP corrects the bias exponentially faster and 
for MC it has fewer number of action-values with high variance. 
Consequently, estimators for the new MDP learn faster 
than the same estimators in the original MDP.

Since the bias-variance analysis is defined for a particular number of samples 
drawn from a fixed distribution, we need to fix the policy for sampling. 
We use an -greedy version of the optimal policy, where
 is chosen such that on average in 10\% of the episodes the agent 
visits {\em bomb}. For the analysis, the delay ranges from 5 to 30 in steps of 5.
The true -table for each delay is computed 
by backward induction and we use 10 different action-value estimators 
for computing bias and variance.

For the TD update rule we use the exponentially weighted arithmetic mean that is sample-updates,
with initial value .
We only monitor the mean and the variance for action-value 
estimators at the first time step, 
since we are interested in the time required for correcting the bias.  
10 different estimators are run for 10,000 episodes. Figure~\ref{fig:A_TDbias} shows the bias correction for different delays, normalized by the first error.


For the MC update rule we use the arithmetic mean for policy evaluation
(later we will use constant- MC for learning the optimal policy).
For each delay, a test set of state-actions for each delay is generated 
by drawing 5,000 episodes with the -greedy optimal policy. 
For each action-value estimator the mean and the variance 
is monitored every 10 visits. If every action-value has 500 updates (visits), 
learning is stopped. Bias and variance are computed based on 10 different 
action-value estimators.
As expected from Section~\ref{sec:Abias_variance_estimator}, in Figure~\ref{fig:A_MCvar}
the variance decreases by , where  is the number of samples.
Figure~\ref{fig:A_MCvar} shows that the number of state-actions with a variance larger than a threshold 
increases exponentially with the delay.
This confirms the statements of Theorem~\ref{th:Aaffect}. 

\begin{figure}[htp]
 \centering
 \begin{subfigure}{.49\textwidth}
  \centering \resizebox{1\textwidth}{!}{\includegraphics[]{\figpath ATDbias.pdf} }\caption{\label{fig:A_TDbias}}
 \end{subfigure}
 \begin{subfigure}{.49\textwidth}
  \centering \resizebox{1\textwidth}{!}{\includegraphics[]{\figpath AMCvariance.pdf}}\caption{\label{fig:A_MCvar}}
 \end{subfigure}\caption{(a) Experimental evaluation of bias and variance of
          different -value estimators on the Grid World.
          (b) Normalized bias reduction for different delays.
          Right: Average variance reduction for the 10th highest values.
          }
          
\end{figure}



\paragraph{Learning the optimal policy.} 
For finding the optimal policy for the Grid World task, 
we apply Monte Carlo Tree Search (MCTS), 
-learning, and Monte Carlo (MC). 
We train until the greedy policy reaches 90\% of the return of the optimal policy. The learning time is measured by the number of episodes.
We use {\em sample updates} for -learning and MC \cite{Sutton:18book}. 
For MCTS the greedy policy uses  for the exploration constant in UCB1 \cite{Kocsis:06}.
The greedy policy is evaluated in 100 episodes intervals. 
The MCTS selection step begins in the start state, 
which is the root of the game tree 
that is traversed using UCB1 \cite{Kocsis:06} as the tree policy. 
If a tree-node gets visited the first time, 
it is expanded with an initial value obtained by  simulated trajectories that start at this node. 
These simulations use a uniform random policy whose average Return is calculated. 
The backpropagation step uses the MCTS(1) update rule \cite{Khandelwal:16}. 
The tree policies exploration constant is .
-learning and MC use a learning rate of  and an 
-greedy policy with .
For RUDDER the optimal reward redistribution using 
a return decomposition as stated 
in Section~\ref{sec:Aopt_rew_red}
is used. For each {\em delay} and each method, 300 runs with different seeds are performed to obtain statistically relevant results.

\paragraph{Estimation of the median learning time and quantiles.}
\label{sec:Alr_estimation}
The performance of different methods is measured by 
the median learning time in terms of episodes.
We stop training at  million episodes. 
Some runs, especially for long delays, have taken too long 
and have thus been stopped. To resolve this bias the quantiles of the 
learning time are estimated by fitting a distribution using right censored data~\cite{gijbels2010censored} .The median is still robustly estimated 
if more than 50\% of runs have finished, 
which is the case for all plotted datapoints.
We find that for delays where all runs have finished 
the learning time follows a Log-normal distribution. 
Therefore, we fit a Log-normal distribution on the right censored data. 
We estimate the median from the existing data, 
and use maximum likelihood estimation to obtain 
the second distribution parameter . 
The start value of the  estimation is calculated by 
the measured variance of the existing data 
which is algebraically transformed to get the  parameter.

\subsubsection{Task (II): The Choice}
\label{c:thechoice}
In this experiment we compare RUDDER, temporal difference (TD) and Monte Carlo (MC) in 
an environment with delayed deterministic reward 
and probabilistic state transitions to investigate how reward information
is transferred back to early states.
This environment is a variation of our introductory
pocket watch example and reveals problems of TD and MC,
while contribution analysis excels. 
In this environment, only the first action at the very beginning 
determines the reward at the end of the episode.

\paragraph{ The environment} is an MDP consisting of two actions ,
an initial state , two {\em charged} states , , 
two {\em neutral} states , , and a final state .
After the first action  in state ,
the agent transits to state  for action  and to  for 
action . 
Subsequent state transitions are probabilistic and independent on actions. 
With probability  the agent stays in the {\em charged} states  or ,
and with probability  it transits from  or 
to the {\em neutral} states  or , respectively. 
The probability to go from {\em neutral} states 
to {\em charged} states is , 
and the probability to stay in {\em neutral} states is . 
Probabilities to transit from 
 or  to  or  or vice versa are zero.
Thus, the first action determines whether that agent stays in ""-states or 
""-states.
The reward is determined by how many times the agent visits {\em charged} states 
plus a bonus reward depending on the agent's first action.
The accumulative reward is given at sequence end and is deterministic.
After  time steps, the agent is in the final state , 
in which the reward  is provided.
  is the sum of 3 deterministic terms: 
 \begin{enumerate}
     \item , the baseline reward associated to the first action; 
     \item , the collected reward across states, 
     which depends on the number of visits  to the {\em charged} states;
     \item , a bonus if the first action .
 \end{enumerate}
The expectations of the accumulative rewards for  and  
have the same absolute value but opposite signs, 
therefore they cancel in expectation over episodes.
Thus, the expected return of an episode is the expected reward : . 
The rewards are defined as follows:

where  is the baseline reward for {\em charged} states, 
and  the probability of staying in or transiting to {\em charged} states.
The expected visits of charged states is  and
.

\begin{figure}
\centering
\begin{tikzpicture}[node distance=1.6cm]
  \tikzstyle{every state}=[thick, fill=gray!10]
  \tikzset{edge/.style = {->,> = latex'}}
  \tikzstyle{initial text}=[" "]
  \node[state] (s100)                    {};
  \node[state]         (s201) [above right of=s100] {};
  \node[state]         (s202) [below right of=s100] {};
  \node[state]         (s301) [right of=s201] {};
  \node[state]         (s302) [right of=s202] {};
 \node[state]         (s400) [below right of=s301] {};
 
\path   (s201) edge [-Latex,bend right]      node[left]  {} (s301) ;
\path   (s301) edge [-Latex,bend right]      node[left]  {} (s201) ;
\path   (s202) edge [-Latex,bend right]      node[left]  {} (s302) ;
\path   (s302) edge [-Latex,bend right]      node[left]  {} (s202) ;
\path   (s100) edge [-Latex,bend left]      node[ left] {}  (s201);
\path   (s100) edge [-Latex,bend right]      node[left]  {} (s202);
\path   (s302) edge [-Latex,bend right]      node[left]  {} (s400) ; 
\path   (s301) edge [-Latex,bend left]      node[left]  {} (s400) ;
\end{tikzpicture}
\caption{State transition diagram for The Choice task. 
The diagram is a simplification of the actual MDP.}
\end{figure}


\paragraph{Methods compared: }
The following methods are compared:
\begin{enumerate}
    \item -learning with eligibility traces according to Watkins \cite{Watkins:89}, 
   \item Monte Carlo,
    \item RUDDER with reward redistribution.
\end{enumerate}


For RUDDER, we use an LSTM without lessons buffer 
and without safe exploration. 
Contribution analysis is realized by differences of return predictions.
For MC, -values are the exponential moving average of the episode return.
For RUDDER, the -values are estimated by an exponential moving average of the reward redistribution.


\paragraph{Performance evaluation and results.}

The task is considered as solved 
when the exponential moving average of the selection of the desired action 
at time   is equal to , where  is the exploration rate.
The performances of the compared methods are measured by the average learning time
in the number of episodes required to solve the task.
A Wilcoxon signed-rank test is performed between the learning time of RUDDER 
and those of the other methods.
Statistical significance p-values are obtained by Wilcoxon signed-rank test.
RUDDER with reward redistribution is significantly faster than all other methods with p-values . 
Table~\ref{tab:res1} reports the number of episodes required by different
methods to solve the task.  
RUDDER with reward redistribution clearly outperforms all other methods. 




\newpage
\begin{landscape}
\begin{table}[htp]
\begin{flushleft}
\caption{Number of episodes required by different 
 methods to solve the grid world task with delayed reward. Numbers give the  
 mean and the standard deviation over 100 trials.
 RUDDER with reward redistribution clearly outperforms all other TD methods.}
\label{tab:res1}\begin{tabular}{*{1}{>{\raggedright}p{6em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 10} &\multicolumn{3}{c}{\bf Delay 15} &\multicolumn{3}{c}{\bf Delay 20} &\\
\toprule[1pt]
RUDDER & 3520.06 & {\small  2343.79} & {\small p = 5.00E-01} & 3062.07 & {\small  1278.92} & {\small p = 5.00E-01} & 3813.96 & {\small  2738.18} & {\small p = 5.00E-01} &  \\
MC & 10920.64 & {\small  7550.04} & {\small p = 5.03E-24} & 17102.89 & {\small  12640.09} & {\small p = 1.98E-30} & 22910.85 & {\small  19149.02} & {\small p = 1.25E-28} &  \\
Q & 66140.76 & {\small  1455.33} & {\small p = 1.28E-34} & 115352.25 & {\small  1962.20} & {\small p = 1.28E-34} & 171571.94 & {\small  2436.25} & {\small p = 1.28E-34} &  \\
\addlinespace[1pt]

\end{tabular}
\end{flushleft}

\begin{flushleft}
\begin{tabular}{*{1}{>{\raggedright}p{6em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 25} &\multicolumn{3}{c}{\bf Delay 30} &\multicolumn{3}{c}{\bf Delay 35} &\\
\toprule[1pt]
MC & 39772 & {\small  47460} & {\small p < 1E-29} & 41922 & {\small  36618} & {\small p < 1E-30} & 50464 & {\small  60318} & {\small p < 1E-30} &  \\
Q & 234912 & {\small  2673} & {\small p < 1E-33} & 305894 & {\small  2928} & {\small p < 1E-33} & 383422 & {\small  4346} & {\small p < 1E-22} &  \\
RUDDER & 4112 & {\small  3769} &  & 3667 & {\small  1776} &  & 3850 & {\small  2875} & &  \\
\addlinespace[1pt]
\end{tabular}
\end{flushleft}

\begin{flushleft}
\begin{tabular}{*{1}{>{\raggedright}p{6em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 40} &\multicolumn{3}{c}{\bf Delay 45} &\multicolumn{3}{c}{\bf Delay 50} &\\
\toprule[1pt]
MC & 56945 & {\small  54150} & {\small p < 1E-30} & 69845 & {\small  79705} & {\small p < 1E-31} & 73243 & {\small  70399} & {\small p = 1E-31} &  \\
Q & 466531 & {\small  3515} & {\small p = 1E-22} &  &  &  &  &  &  &  \\
RUDDER & 3739 & {\small  2139} &  & 4151 & {\small  2583} &  & 3884 & {\small  2188} &  &  \\
\addlinespace[1pt]
\end{tabular}
\end{flushleft}

\begin{flushleft}
\begin{tabular}{*{1}{>{\raggedright}p{14em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 100} &\multicolumn{3}{c}{\bf Delay 500} &\\
\toprule[1pt]
MC & 119568 & {\small  110049} & {\small p < 1E-11} & 345533 & {\small  320232} & {\small p < 1E-16} &  \\
RUDDER & 4147 & {\small  2392} &  & 5769 & {\small  4309} &  &  \\
\addlinespace[1pt]
\end{tabular}
\end{flushleft}
\end{table}

\end{landscape}


\newpage








\subsubsection{Task(III): Trace-Back}
This section supports the artificial task (III) -- {\bf Trace-Back} -- in the main paper.
RUDDER is compared to potential-based reward shaping methods.
In this experiment, we compare reinforcement learning methods that
have to transfer back information about a delayed reward.
These methods comprise RUDDER, TD() and potential-based reward shaping approaches.
For potential-based reward shaping we compare the original 
{\em reward shaping} \cite{Ng:99}, {\em look-forward advice}, 
and {\em look-back advice}~\cite{Wiewiora:03}
with three different potential functions.
Methods that transfer back reward information
are characterized by low variance estimates
of the value function or the action-value function, 
since they use an estimate of the future return instead of the
future return itself.
To update the estimates of the future returns, 
reward information has to be transferred back.
The task in this experiment can be solved by Monte Carlo
estimates very fast, which do not transfer back information but use samples of 
the future return for the estimation instead. 
However, Monte Carlo methods have high variance, which is
not considered in this experiment. 
\paragraph{ The environment } is a 1515 grid, where 
actions move the agent from its current position 
in 4 adjacent positions ({\em up, down, left, right}), 
except the agent would be moved outside the grid. 
The number of steps (moves) per episode is . 
The starting position is  in the middle of the grid. 
The maximal return is a combination of negative immediate reward 
and positive delayed reward.
To obtain the maximum return, 
the policy must move the agent {\em up} in the time step  and  
{\em right} in the following time step . In this case, the 
agent receives an immediate reward of -50 at  and 
a delayed reward of 150 at the end of the episode at , 
that is, a return of 100.
Any other combination of actions gives the agent immediate reward of 50 at  without
any delayed reward, that is, a return of 50.
To ensure Markov properties the position of the agent, the time, as well as the delayed reward
are coded in the state. The future reward discount rate  is set to 1.
The state transition probabilities are deterministic for the first two moves.
For  and for each action, state transition probabilities 
are equal for each possible next state (uniform distribution), 
meaning that 
actions after  do not influence the return.
For comparisons of long delays, 
both the size of the grid and the length of the episode are increased. 
For a delay of , a  grid is used
with an episode length of , 
and starting position .

\paragraph{Compared methods.}
We compare different TD() and potential-based reward shaping methods.
For TD(), the baseline is , 
with eligibility traces  and  and Watkins' implementation \cite{Watkins:89}. 
The potential-based reward shaping methods are the original reward shaping, 
look-ahead advice as well as look-back advice. 
For look-back advice, we use SARSA()~\cite{Rummery:94} instead of  
as suggested by the authors~\cite{Wiewiora:03}. 
-values are represented by a state-action table, that is, we consider only
tabular methods.
In all experiments an -greedy policy with  is used.
All three reward shaping methods 
require a potential function ,
which is based on the reward redistribution () 
in three different ways:


(I) The Potential function  is the difference of LSTM predictions, 
which is the redistributed reward :

(II) The potential function  is the sum of 
future redistributed rewards, i.e.\ 
the q-value of the redistributed rewards. 
In the optimal case, this coincides with implementation (I):

(III) The potential function  corresponds to the LSTM predictions. In the optimal case this corresponds
to the accumulated reward up to  plus the q-value of the delayed MDP:





The following methods are compared:
\begin{enumerate}
    \item -learning with eligibility traces according to Watkins (), 
    \item SARSA with eligibility traces (SARSA()),
    \item Reward Shaping with potential functions (I), (II), or (III) 
    according to -learning and eligibility traces according to Watkins,
    \item Look-ahead advise with potential functions (I), (II), or (III) 
    with ,
    \item Look-back advise with potential functions (I), (II), or (III) 
    with SARSA(),
    \item RUDDER with reward redistribution for -value estimation and RUDDER applied on top of -learning.
\end{enumerate}

RUDDER is implemented with an LSTM architecture 
without output gate nor forget gate. For this experiments, 
RUDDER does not use lessons buffer nor safe exploration. 
For contribution analysis we use differences of return predictions.
For RUDDER, the -values are estimated by an exponential moving average 
(RUDDER -value estimation) or alternatively by -learning. 

\paragraph{ Performance evaluation: }
The task is considered solved when the exponential moving average of the return 
is above 90, which is 90\% of the maximum return. 
Learning time is the number of episodes required to solve the task. 
The first evaluation criterion is the average learning time.
The -value differences at time step  are monitored. 
The -values at  are the most important ones, since they have to predict 
whether the maximal return will be received or not.
At  the immediate reward acts as a distraction since it is -50 for the action leading  
to the maximal return () and 50 for all other actions ().  
At the beginning of learning, the -value difference 
between  and  is about -100, since the immediate reward is -50 and 50, respectively.
Once the -values converge to the optimal policy, the difference approaches 50. 
However, the task will already be correctly solved as soon as this difference is positive.
The second evaluation criterion is the -value differences at time step ,
since it directly shows to what extend the task is solved.

\paragraph{ Results: }
Table~\ref{tab:res1} reports the number of episodes required by different
methods to solve the task. 
The mean and the standard deviation over 100 trials are given.
A Wilcoxon signed-rank test is performed between the learning time of RUDDER 
and those of the other methods.
Statistical significance p-values are obtained by Wilcoxon signed-rank test.
RUDDER with reward redistribution is significantly faster than all other methods with p-values . 
Tables~\ref{tab:Ares1},\ref{tab:Ares2} report the results for all methods.


\newpage
\begin{landscape}
\begin{table}[htp]
\begin{center}
\caption{Number of episodes required by different 
 methods to solve the Trace-Back task with delayed reward. The numbers represent the  
 mean and the standard deviation over 100 trials.
 RUDDER with reward redistribution significantly outperforms all other methods.
}
\label{tab:Ares1}\begin{tabular}{*{1}{>{\raggedright}p{14em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 6} &\multicolumn{3}{c}{\bf Delay 8} &\multicolumn{3}{c}{\bf Delay 10} &\\
\toprule[1pt]
Look-back I & 6074 & {\small  952} & {\small p = 1E-22} & 13112 & {\small  2024} & {\small p = 1E-22} & 21715 & {\small  4323} & {\small p = 1E-06} &  \\
Look-back II & 4584 & {\small  917} & {\small p = 1E-22} & 9897 & {\small  2083} & {\small p = 1E-22} & 15973 & {\small  4354} & {\small p = 1E-06} &  \\
Look-back III & 4036.48 & {\small  1424.99} & {\small p = 5.28E-17} & 7812.72 & {\small  2279.26} & {\small p = 1.09E-23} & 10982.40 & {\small  2971.65} & {\small p = 1.03E-07} &  \\
Look-ahead I & 14469.10 & {\small  1520.81} & {\small p = 1.09E-23} & 28559.32 & {\small  2104.91} & {\small p = 1.09E-23} & 46650.20 & {\small  3035.78} & {\small p = 1.03E-07} &  \\
Look-ahead II & 12623.42 & {\small  1075.25} & {\small p = 1.09E-23} & 24811.62 & {\small  1986.30} & {\small p = 1.09E-23} & 43089.00 & {\small  2511.18} & {\small p = 1.03E-07} &  \\
Look-ahead III & 16050.30 & {\small  1339.69} & {\small p = 1.09E-23} & 30732.00 & {\small  1871.07} & {\small p = 1.09E-23} & 50340.00 & {\small  2102.78} & {\small p = 1.03E-07} &  \\
Reward Shaping I & 14686.12 & {\small  1645.02} & {\small p = 1.09E-23} & 28223.94 & {\small  3012.81} & {\small p = 1.09E-23} & 46706.50 & {\small  3649.57} & {\small p = 1.03E-07} &  \\
Reward Shaping II & 11397.10 & {\small  905.59} & {\small p = 1.09E-23} & 21520.98 & {\small  2209.63} & {\small p = 1.09E-23} & 37033.40 & {\small  1632.24} & {\small p = 1.03E-07} &  \\
Reward Shaping III & 12125.48 & {\small  1209.59} & {\small p = 1.09E-23} & 23680.98 & {\small  1994.07} & {\small p = 1.09E-23} & 40828.70 & {\small  2748.82} & {\small p = 1.03E-07} & \\
 & 14719.58 & {\small  1728.19} & {\small p = 1.09E-23} & 28518.70 & {\small  2148.01} & {\small p = 1.09E-23} & 44017.20 & {\small  3170.08} & {\small p = 1.03E-07} &  \\
SARSA() & 8681.94 & {\small  704.02} & {\small p = 1.09E-23} & 23790.40 & {\small  836.13} & {\small p = 1.09E-23} & 48157.50 & {\small  1378.38} & {\small p = 1.03E-07} &  \\
RUDDER  & 726.72 & {\small  399.58} & {\small p = 3.49E-04} & 809.86 & {\small  472.27} & {\small p = 3.49E-04} & 906.13 & {\small  514.55} & {\small p = 3.36E-02} &  \\
RUDDER & 995.59 & {\small  670.31} & {\small p = 5.00E-01} & 1128.82 & {\small  741.29} & {\small p = 5.00E-01} & 1186.34 & {\small  870.02} & {\small p = 5.00E-01} &  \\
\end{tabular}


\begin{tabular}{*{1}{>{\raggedright}p{14em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 12} &\multicolumn{3}{c}{\bf Delay 15} &\multicolumn{3}{c}{\bf Delay 17} &\\
\toprule[1pt]
Look-back I & 33082.56 & {\small  7641.57} & {\small p = 1.09E-23} & 49658.86 & {\small  8297.85} & {\small p = 1.28E-34} & 72115.16 & {\small  21221.78} & {\small p = 1.09E-23} &  \\
Look-back II & 23240.16 & {\small  9060.15} & {\small p = 1.09E-23} & 29293.94 & {\small  7468.94} & {\small p = 1.28E-34} & 42639.38 & {\small  17178.81} & {\small p = 1.09E-23} &  \\
Look-back III & 15647.40 & {\small  4123.20} & {\small p = 1.09E-23} & 20478.06 & {\small  5114.44} & {\small p = 1.28E-34} & 26946.92 & {\small  10360.21} & {\small p = 1.09E-23} &  \\
Look-ahead I & 66769.02 & {\small  4333.47} & {\small p = 1.09E-23} & 105336.74 & {\small  4977.84} & {\small p = 1.28E-34} & 136660.12 & {\small  5688.32} & {\small p = 1.09E-23} &  \\
Look-ahead II & 62220.56 & {\small  3139.87} & {\small p = 1.09E-23} & 100505.05 & {\small  4987.16} & {\small p = 1.28E-34} & 130271.88 & {\small  5397.61} & {\small p = 1.09E-23} &  \\
Look-ahead III & 72804.44 & {\small  4232.40} & {\small p = 1.09E-23} & 115616.59 & {\small  5648.99} & {\small p = 1.28E-34} & 149064.68 & {\small  7895.48} & {\small p = 1.09E-23} &  \\
Reward Shaping I & 68428.04 & {\small  3416.12} & {\small p = 1.09E-23} & 107399.17 & {\small  5242.88} & {\small p = 1.28E-34} & 137032.14 & {\small  6663.12} & {\small p = 1.09E-23} &  \\
Reward Shaping II & 56225.24 & {\small  3778.86} & {\small p = 1.09E-23} & 93091.44 & {\small  5233.02} & {\small p = 1.28E-34} & 122224.20 & {\small  5545.63} & {\small p = 1.09E-23} &  \\
Reward Shaping III & 60071.52 & {\small  3809.29} & {\small p = 1.09E-23} & 99476.40 & {\small  5607.08} & {\small p = 1.28E-34} & 130103.50 & {\small  6005.61} & {\small p = 1.09E-23} &  \\
 & 66952.16 & {\small  4137.67} & {\small p = 1.09E-23} & 107438.36 & {\small  5327.95} & {\small p = 1.28E-34} & 135601.26 & {\small  6385.76} & {\small p = 1.09E-23} &  \\
SARSA() & 78306.28 & {\small  1813.31} & {\small p = 1.09E-23} & 137561.92 & {\small  2350.84} & {\small p = 1.28E-34} & 186679.12 & {\small  3146.78} & {\small p = 1.09E-23} &  \\
RUDDER  & 1065.16 & {\small  661.71} & {\small p = 3.19E-01} & 972.73 & {\small  702.92} & {\small p = 1.13E-04} & 1101.24 & {\small  765.76} & {\small p = 1.54E-01} &  \\
RUDDER & 1121.70 & {\small  884.35} & {\small p = 5.00E-01} & 1503.08 & {\small  1157.04} & {\small p = 5.00E-01} & 1242.88 & {\small  1045.15} & {\small p = 5.00E-01} &  \\
\end{tabular}
\end{center}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}[htp]
\begin{center}
\caption{Cont. Number of episodes required by different 
 methods to solve the Trace-Back task with delayed reward. The numbers represent the  
 mean and the standard deviation over 100 trials.
 RUDDER with reward redistribution significantly outperforms all other methods.
}
\label{tab:Ares2}\begin{tabular}{*{1}{>{\raggedright}p{14em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{4em}}*{1}{>{\columncolor{mColor1}\raggedleft}p{5em}}*{1}{>{\columncolor{mColor1}\raggedright}p{5em}}*{1}{>{\raggedleft}p{4em}}*{1}{>{\raggedleft}p{5em}}*{1}{>{\raggedright}p{5em}}*{1}{>{\raggedright}p{0.01em}}}
\toprule[1pt]
\addlinespace[2pt]
{\bf Method} &\multicolumn{3}{c}{\bf Delay 20} &\multicolumn{3}{c}{\bf Delay 25} &\\
\toprule[1pt]
Look-back I & 113873.30 & {\small  31879.20} & {\small p = 1.03E-07} &  &  &  &  \\
Look-back II & 56830.30 & {\small  19240.04} & {\small p = 1.03E-07} & 111693.34 & {\small  73891.21} & {\small p = 1.09E-23} &  \\
Look-back III & 35852.10 & {\small  11193.80} & {\small p = 1.03E-07} &  &  &  &  \\
Look-ahead I & 187486.50 & {\small  5142.87} & {\small p = 1.03E-07} &  &  &  &  \\
Look-ahead II & 181974.30 & {\small  5655.07} & {\small p = 1.03E-07} & 289782.08 & {\small  11984.94} & {\small p = 1.09E-23} &  \\
Look-ahead III & 210029.90 & {\small  6589.12} & {\small p = 1.03E-07} &  &  &  &  \\
Reward Shaping I & 189870.30 & {\small  7635.62} & {\small p = 1.03E-07} & 297993.28 & {\small  9592.30} & {\small p = 1.09E-23} &  \\
Reward Shaping II & 170455.30 & {\small  6004.24} & {\small p = 1.03E-07} & 274312.10 & {\small  8736.80} & {\small p = 1.09E-23} &  \\
Reward Shaping III & 183592.60 & {\small  6882.93} & {\small p = 1.03E-07} & 291810.28 & {\small  10114.97} & {\small p = 1.09E-23} &  \\
 & 186874.40 & {\small  7961.62} & {\small p = 1.03E-07} &  &  &  &  \\
SARSA() & 273060.70 & {\small  5458.42} & {\small p = 1.03E-07} & 454031.36 & {\small  5258.87} & {\small p = 1.09E-23} &  \\
RUDDER I & 1048.97 & {\small  838.26} & {\small p = 5.00E-01} & 1236.57 & {\small  1370.40} & {\small p = 5.00E-01} &  \\
RUDDER II & 1159.30 & {\small  731.46} & {\small p = 8.60E-02} & 1195.75 & {\small  859.34} & {\small p = 4.48E-01} &  \\
\end{tabular}


\end{center}
\end{table}
\end{landscape}












\subsubsection{Task (IV): Charge-Discharge}
\label{sec:Acharge_discharge}
The Charge-Discharge task depicted in Figure~\ref{fig:charge}
is characterized by
deterministic reward and state transitions.
The environment consists of two states: 
{\em charged}  / {\em discharged}  
and two actions {\em charge}  / {\em discharge}
.
The deterministic reward is ,
and . 
The reward  is accumulated 
for the whole episode and given only at time ,
where  corresponds to the maximal delay of the reward.
The optimal policy alternates between charging and discharging
to accumulate a reward of 10 every other time step. 
The smaller immediate reward of  distracts the agent from 
the larger delayed reward. The distraction forces the agent to learn 
the value function well enough to distinguish between the contribution of the 
immediate and the delayed reward to the final return.  
\begin{figure}[htp]\centering \resizebox{0.5\textwidth}{!}{\begin{tikzpicture}[node distance=1.6cm]
  \tikzstyle{every state}=[thick, fill=gray!10]
\tikzset{edge/.style = {->,> = latex'}}
\tikzstyle{initial text}=[" "]



  \node[state] (s100)                    {};
  \node[state]         (s101) [below of=s100] {};
  \node[state]         (s201) [right of=s101] {};
  \node[state]         (s102) [below of=s101] {};
  \node[state]         (s202) [right of=s102] {};
  \node[state]         (s112) [right of=s202] {};
\node[state]         (s103) [below of=s102] {};
  \node[state]         (s203) [right of=s103]       {};
  \node[state]         (s113) [right of=s203] {};
  \node[state]         (s213) [right of=s113] {};
  \node[state,accepting]          (s104) [below of=s103] {};
  \node[state,accepting]          (s204) [right of=s104]       {};
  \node[state,accepting]          (s114) [right of=s204] {};
  \node[state,accepting]          (s214) [right of=s114] {};
  \node[state,accepting]         (s124) [right of=s214] {};





\path 
    (s100.south) -- coordinate (t1) (s100.south|-s101.north);
\path
    (s101.south) -- coordinate (t2) (s101.south|-s102.north);
\path
    (s102.south) -- coordinate (t3) (s102.south|-s103.north);
\path
    (s103.south) -- coordinate (t4) (s103.south|-s104.north);
\path
    (s203.west) -- coordinate (c1) (s203.west-|s113.east);
    \draw[dotted,opacity=0.8] (s101.north-|c1) -- ([yshift=-1cm]s104.south-|c1);   
\path
    (s213.west) -- coordinate (c2) (s213.west-|s124.east);
    \draw[dotted,opacity=0.8] (s103.north-|c2) -- ([yshift=-1cm]s104.south-|c2);   
    
\path   (s100) edge [-Latex,bend left]      node[ left] {}  (s201)
        (s201) edge  [-Latex,bend left]      node[left]  {} (s112)
        (s112) edge  [-Latex,bend left]      node[left]  {} (s213)
        (s213) edge  [-Latex,bend left]      node[left]  {} (s124)
        
        (s201) edge [-Latex]      node[right]  {} (s202)
        (s202) edge [-Latex]      node[right]  {} (s203)
        (s203) edge [-Latex]      node[right]  {} (s204)

        (s101) edge  [-Latex, bend left]      node[left] {} (s202)
        (s202) edge  [-Latex, bend left]     node[left] {} (s113)
        (s113) edge  [-Latex, bend left]      node[left] {} (s214)
        
        (s102) edge  [-Latex, bend left]      node[left] {} (s203)
        (s203) edge  [-Latex, bend left]      node[left] {} (s114)
        
        (s103) edge  [-Latex, bend left]      node[left] {} (s204)
        
        (s100) edge [-Latex]      node[right]  {} (s101)
        (s101) edge [-Latex]      node[right]  {} (s102)
        (s102) edge [-Latex]      node[right]  {} (s103)
        (s103) edge [-Latex]      node[right]  {} (s104)
        
        
        (s112) edge [-Latex]      node[right]  {} (s113)
        (s113) edge [-Latex]      node[right]  {} (s114)
        
        (s213) edge [-Latex]      node[right]  {} (s214)
;

\node[text width=1cm] at ([xshift=-1cm]s101.west) {};
\node[text width=1cm] at ([xshift=-1cm]s102.west) {};
\node[text width=1cm] at ([xshift=-1cm]s103.west) {};
\node[text width=1cm] at ([xshift=-1cm]s104.west) {};


\path
    (s103.west) -- coordinate (cd1) (s103.west-|s203.east);
\path
    (s114.west) -- coordinate (cd2) (s114.west-|s214.east);
\path
    (s124.west) -- coordinate (cd3) (s124.west-|s124.east);

\node[text width=2cm,align=center] at ([yshift=-1cm]s104-|cd1) { };
\node[text width=2cm, align=center] at ([yshift=-1cm]s104-|cd2) {};
\node[text width=2cm, align=center] at ([yshift=-1cm]s104-|cd3) {};

\end{tikzpicture} }
\caption{The Charge-Discharge task with two basic states: {\em charged}  and 
 {\em discharged} .
 In each state the actions charge  leading to 
 the charged state  
 and discharge  leading to discharged state  are possible. 
 Action  in the discharged state  leads to a small immediate reward of  and in the charged state  
 to a delayed reward of . 
 After sequence end , the accumulated delayed reward  is given. \label{fig:charge}}
\end{figure}


For this task, 
the RUDDER backward analysis is based on 
monotonic LSTMs
and on layer-wise relevance propagation (LRP). 
The reward redistribution provided by RUDDER uses an 
LSTM which consists of  memory cells 
and is trained with Adam and a learning rate of . 
The reward redistribution is used to learn an optimal policy by -learning and by MC
with a learning rate of  and an exploration rate of . 
Again, we use {\em sample updates} for -learning and MC \cite{Sutton:18book}.
The learning is stopped either if the agent achieves \% of the reward of the optimal policy or after a maximum number of  million episodes. 
For each  and each method, 100 runs with different seeds are performed to obtain statistically relevant results. For delays with runs which did not finish within 100m episodes we estimate parameters like described in Paragraph~\ref{sec:Alr_estimation}. 


\subsubsection{Task (V): Solving Trace-Back using policy gradient methods}
In this experiment, we compare policy gradient methods instead of -learning based methods. 
These methods comprise RUDDER on top of PPO with and without GAE, and a baseline PPO using GAE.
The environment and performance evaluation are the same as reported in Task III.
Again, RUDDER is exponentially faster than PPO. RUDDER on top of PPO is slightly better with GAE than without.

\begin{figure}[h]
 \centering \resizebox{0.7\linewidth}{!}{\input{\figpath PG_TaskV.pgf}}
\caption{Comparison of performance of RUDDER with GAE (RUDDER+GAE) and without GAE (RUDDER) and PPO with GAE (PPO) on artificial task V
with respect to the learning time in episodes (median of 100 trials) in log scale
vs.\ the delay of the reward.
The shadow bands indicate the  and  quantiles.
Again, RUDDER significantly outperforms all other methods.\label{fig:ppo}}\end{figure}



















\subsection{Atari Games}
\label{sec:Aatari}
In this section we describe the implementation of RUDDER for Atari games.
The implementation is largely based on the {\em OpenAI baselines}
package \cite{Dhariwal:17} for the RL components and our package
for the LSTM reward redistribution model, which will be announced upon publication.
If not specified otherwise, standard input processing, such as skipping  frames and stacking  frames, 
is performed by the {\em OpenAI baselines} package.

We consider the 52 Atari games that were compatible with OpenAI baselines,
Arcade Learning Environment (ALE) \cite{Bellemare:13}, and OpenAI Gym \cite{Brockman:16}.
Games are divided into episodes,
i.e.\ the loss of a life or the exceeding of 108k frames
trigger the start of a new episode without resetting the environment.
Source code will be made available at upon publication.

\subsubsection{Architecture}
\label{sec:Aatari-arch}
We use a modified PPO architecture and a separate reward redistribution model. While parts of the two could be combined, this separation allows for better comparison between the PPO baseline with and without RUDDER.

\paragraph{PPO architecture.}
The design of the policy and the value network relies 
on the {\em ppo2} implementation \cite{Dhariwal:17}, which  
is depicted in Figure~\ref{fig:atari-arch} and 
summarized in Table~\ref{tab:atari-arch}.
The network input, 4 stacked Atari game frames \cite{Mnih:15},
is processed by 3 convolution layers with ReLU activation functions,
followed by a fully connected layer with ReLU activation functions.
For PPO with RUDDER,
2 output units, for the original and redistributed reward value function,
and another set of output units for the policy prediction are applied.
For the PPO baseline without RUDDER,
the output unit for the redistributed reward value function is omitted.


\paragraph{Reward redistribution model.}
Core of the reward redistribution model
is an LSTM layer containing 64 memory cells
with sigmoid gate activations, tanh input nonlinearities, 
and identity output activation functions, 
as illustrated in Figure~\ref{fig:atari-arch} 
and summarized in Table~\ref{tab:atari-arch}.
This LSTM implementation omits output gate and forget gate to simplify the network dynamics.
Identity output activation functions were chosen to support 
the development of linear counting dynamics within the LSTM layer, 
as is required to count the reward pieces during an episode chunk.
Furthermore, the input gate is only connected recurrently to other LSTM blocks and the
cell input is only connected to forward connections from the lower layer.
For the vision system the same architecture was used as with the PPO network,
with the first convolution layer being doubled to process
 frames and full frames separately in the first layer.
Additionally, the memory cell layer receives the vision feature activations
of the PPO network, the current action, and the approximate in-game time as inputs.
No gradients from the reward redistribution network are propagated over the connections to the PPO network.
After the LSTM layer, the reward redistribution model
has one output node for the prediction 
of the return realization  of the return variable .
The reward redistribution model has 4 additional
output nodes for the auxiliary tasks as described in Section~\ref{sec:Aatari-taupdate}.

\begin{figure}[htp]\centering \resizebox{0.5\textwidth}{0.5\textwidth}{\input{\figpath a2c_rr_architecture_colors_nips2019.pgf}}\caption{RUDDER architecture for Atari games as described in Section~\ref{sec:Aatari-arch}. 
 Left: The {\em ppo2} implementation \cite{Dhariwal:17}.
 Right: LSTM reward redistribution architecture.
 The reward redistribution network has access to the PPO vision features (dashed lines)
 but no gradient is propagated between the networks.
 The LSTM layer receives the current action and an approximate 
 in-game-time as additional input.
 The PPO outputs  for value function prediction and  for policy prediction each represent multiple output nodes: the original and redistributed reward value function prediction for  and the outputs for all of the available actions for .
 Likewise, the reward redistribution network output  represents multiple outputs, 
 as described in Section~\ref{sec:Aatari-taupdate}
 Details on layer configuration are given in Table~\ref{tab:atari-arch}.\label{fig:atari-arch}}\end{figure}

\begin{table}[htp]\resizebox{\linewidth}{!}{\begin{tabular}[t]{lllllll}
    \toprule
Layer&Specifications&&Layer&Specifications&\\
    \midrule
Conv.Layer 0&features&32&Conv.Layer 4&features&32\\
&kernelsize&8x8&&kernelsize&8x8\\
&striding&4x4&&striding&4x4\\
&act&ReLU&&act&ReLU\\
&initialization&orthogonal, \lstinline{gain}=&&initialization&orthogonal, \lstinline{gain}=\\
Conv.Layer 1&features&64&Conv.Layer 5&features&64\\
&kernelsize&4x4&&kernelsize&4x4\\
&striding&2x2&&striding&2x2\\
&act&ReLU&&act&ReLU\\
&initialization&orthogonal, \lstinline{gain}=&&initialization&orthogonal, \lstinline{gain}=\\
Conv.Layer 2&features&64&Conv.Layer 6&features&64\\
&kernelsize&3x3&&kernelsize&3x3\\
&striding&1x1&&striding&1x1\\
&act&ReLU&&act&ReLU\\
&initialization&orthogonal, \lstinline{gain}=&&initialization&orthogonal, \lstinline{gain}=\\
Dense Layer&features&512&LSTM Layer&cells&64\\
&act&ReLU&&gate act.&sigmoid\\
&initialization&orthogonal, \lstinline{gain}=&&ci act.&tanh\\
Conv.Layer 3&features&32&&output act.&linear\\
&kernelsize&8x8&&bias ig&trunc.norm., \lstinline{mean}\\
&striding&4x4&&bias ci&trunc.norm., \lstinline{mean}\\
&act&ReLU&&fwd.w. ci&trunc.norm., \lstinline{scale}\\
&initialization&orthogonal, \lstinline{gain}=&&fwd.w. ig&omitted\\
&&&&rec.w. ci&omitted\\
&&&&rec.w. ig&trunc.norm., \lstinline{scale}\\
&&&&og&omitted\\
&&&&fg&omitted\\
    \bottomrule
  \end{tabular}}\caption{Specifications of PPO and RUDDER architectures 
as shown in Figure~\ref{fig:atari-arch}.
Truncated normal initialization has the default values 
\lstinline{mean}, \lstinline{stddev} and is optionally multiplied by a factor \lstinline{scale}.
\label{tab:atari-arch}}\end{table}

\subsubsection{Lessons Replay Buffer}
\label{sec:Alessonbuffer}
The lessons replay buffer is realized as
a priority-based buffer
containing up to  samples.
New samples are added to the buffer if (i) the buffer is not filled or
if (ii) the new sample is considered more important than the least important sample in the buffer,
in which case the new sample replaces the least important sample.

Importance of samples for the buffer is determined based on a combined ranking of
(i) the reward redistribution model error and
(ii) the difference of the sample return to the mean return of all samples in the lessons buffer.
Each of these two rankings contributes equally to the final ranking of the sample. Samples with higher loss and greater difference to the mean return achieve a higher ranking.

Sampling from the lessons buffer is performed as a sampling from a softmax function on the sample-losses in the buffer. Each sample is a sequence of  consecutive transitions, as described in the last paragraph of Section~\ref{sec:Aatari-taupdate}.


\subsubsection{Game Processing, Update Design, and Target Design}\label{sec:Aatari-taupdate}
Reward redistribution is performed in an online fashion as new transitions are sampled from the environment. This allows to keep the original update schema of the PPO baseline, while still using the redistributed reward for the PPO updates. Training of the reward redistribution model is done separately on the lessons buffer samples from Section~\ref{sec:Alessonbuffer}. These processes are described in more detail in the following paragraphs.


\paragraph{Reward Scaling.}
As described in the main paper,
rewards for the PPO baseline and RUDDER are scaled based on the maximum return per sample encountered during training so far.
With  samples sampled from the environment
and a maximum return of

encountered, the scaled reward  is


Goal of this scaling is to normalize the reward  to range  
with a linear scaling, suitable for training the PPO and reward redistribution model.
Since the scaling is linear, the original proportions between rewards are kept.
Downside to this approach is that if a new
maximum return is encountered,
the scaling factor is updated, and the models have to readjust.

\paragraph{Reward redistribution.}
Reward redistribution is performed using differences of return predictions of the LSTM network.
That is, the differences of the reward redistribution model prediction 
at time step  and  serve as contribution analysis and thereby give the redistributed reward .
This allows for online reward redistribution on the sampled transitions
before they are used to train the PPO network,
without waiting for the game sequences to be completed.

To assess the current quality of the reward redistribution model,
a quality measure based on the relative absolute error of 
the prediction  at the last time step  is introduced:

with  as quality threshold of  and the maximum
possible error  as  due to the reward scaling applied.
{\tt quality} is furthermore clipped to be within range .

\paragraph{PPO model.}
\label{c:ppomodel}
The {\em ppo2} implementation \cite{Dhariwal:17}
samples from the environment using multiple agents in parallel.
These agents play individual environments but share all weights,
i.e.\ they are distinguished by random effects in the environment
or by exploration.
The value function and policy network is trained online on a batch of transitions
sampled from the environment.
Originally, the policy/value function network updates are adjusted using a policy loss,
a value function loss, and an entropy term, each with dedicated scaling factors \cite{Schulman:17}.
To decrease the number of hyperparameters,
the entropy term scaling factor is adjusted automatically using Proportional Control
to keep the policy entropy in a predefined range.

We use two value function output units to predict the value functions of 
the original and the redistributed reward.
For the PPO baseline without RUDDER, the output unit for the redistributed reward is omitted.
Analogous to the {\em ppo2} implementation, these two value function predictions 
serve to compute the advantages used to scale the policy gradient updates.
For this, the advantages for original reward  and redistributed reward  are combined
as a weighted sum .
The PPO value function loss term  is replaced by the sum of the value function 
loss  for the original reward  and
the scaled value function  loss  for the redistributed reward,
such that .
Parameter values were taken from
the original paper \cite{Schulman:17} and implementation \cite{Dhariwal:17}.
Additionally, a coarse hyperparameter search was performed with value function coefficients 

and replacing the static entropy coefficient by a Proportional Control scaling of 
the entropy coefficient.
The Proportional Control target entropy was linearly decreased from  to  over 
the course of training.
PPO baseline hyperparamters were used for PPO with RUDDER without changes.

Parameter values are listed in Table~\ref{tab:atari_params}.

\paragraph{Reward redistribution model.}
The loss of the reward redistribution model for a sample is composed of four parts.
(i) The main loss ,
which is the squared prediction loss of  at the last time step  of the episode

(ii) the continuous prediction loss  of  at each time step

(iii) the loss  of the prediction of the output at  at each time step 

as well as (iv) the loss on 3 auxiliary tasks.
At every time step ,
these auxiliary tasks are
(1) the prediction of the action-value function ,
(2) the prediction of the accumulated original reward  in the next 10 frames 
,
and (3) the prediction of the accumulated reward in the next 50 frames
,
resulting in the final auxiliary loss  as


The final loss for the reward redistribution model is then computed as


The continuous prediction and earlier prediction losses  and 
push the reward redistribution model toward performing an optimal reward redistribution.
This is because important events that are redundantly encoded 
in later states are stored as early as possible.
Furthermore, the auxiliary loss  speeds up learning 
by adding more information about the original immediate rewards to the updates.

The reward redistribution model is only trained on the lessons buffer.
Training epochs on the lessons buffer are performed every  PPO updates or if a new sample was added to the lessons buffer.
For each such training epoch, 8 samples are sampled from the lessons buffer.
Training epochs are repeated until the reward redistribution quality 
is sufficient () for all replayed samples in the last 5 training epochs.

The reward redistribution model is not trained or used until
the lessons buffer contains at least 32 samples
and samples with different return have been encountered.

Parameter values are listed in Table~\ref{tab:atari_params}.


\begin{table}[htp]
\begin{center}
\begin{tabular}{lrclr}
\toprule
\multicolumn{2}{c}{PPO} & & \multicolumn{2}{c}{RUDDER}\\
learning rate &  & & learning rate &  \\
policy coefficient &  & &  weight decay  &  \\
initial entropy coefficient &  & & gradient clipping  &  \\
value function coefficient &  & & optimization & ADAM \\
\bottomrule
\end{tabular}
\end{center}
\caption{Left: Update parameters for PPO model.
Entropy coefficient is scaled via Proportional Control 
with the target entropy linearly annealed from  to  over the course of learning.
Unless stated otherwise,
default parameters of {\em ppo2} implementation \cite{Dhariwal:17} are used.
Right: Update parameters for reward redistribution model of RUDDER.
\label{tab:atari_params}}\end{table}


\paragraph{Sequence chunking and Truncated Backpropagation Through Time (TBPTT).}
Ideally, RUDDER would be trained on completed game sequences,
to consequently redistribute the reward within a completed game.
To shorten computational time for learning the reward redistribution model,
the model is not trained on completed game sequences
but on sequence chunks consisting of  time steps.
The beginning of such a chunk is treated as beginning of a new episode for the model
and ends of episodes within this chunk reset the state of the LSTM,
so as to not redistribute rewards between episodes.
To allow for updates on sequence chunks even if the game sequence is not completed,
the PPO value function prediction is used to estimate the expected future reward at the end of the chunk.

Utilizing TBPTT to further speed up LSTM learning,
gradients for the reward redistribution LSTM are cut after every 128 time steps.




\subsubsection{Exploration}
\label{sec:Aatari-exploration}
Safe exploration to increase the likelihood of observing 
delayed rewards is an important feature of RUDDER.
We use a safe exploration strategy, which is realized by normalizing the output 
of the policy network to range  and randomly picking one of the actions 
that is above a threshold . 
Safe exploration is activated once per sequence at a random sequence position 
for a random duration between 0 and the average game length . 
Thereby we encourage long but safe off-policy trajectories within parts of the game sequences.
Only 2 of the 8 parallel actors use safe exploration with  and , respectively.
All actors sample from the softmax policy output.

To avoid policy lag during safe exploration transitions,
we use those transitions only to update
the reward redistribution model but not the PPO model.


\subsubsection{Results}
Training curves for 3 random seeds for PPO baseline and PPO with RUDDER are shown in Figure~\ref{fig:atari_training} and scores are listed in Table~\ref{tab:full_atari_scores} for all 52 Atari games. Training was conducted over 200M game frames (including skipped frames), as described in the experiments section of the main paper. 

We investigated failures and successes of RUDDER in different Atari games.
RUDDER failures were observed to be mostly due to LSTM failures and comprise e.g. slow learning in Breakout, explaining away in Double Dunk, spurious redistributed rewards in Hero, overfitting to the first levels in Qbert, and exploration problems in MontezumaRevenge.
RUDDER successes were observed to be mostly due to redistributing rewards to important key actions that would otherwise not receive reward, such as moving towards the built igloo in Frostbite, diving up for refilling oxygen in Seaquest, moving towards the treasure chest in Venture, and shooting at the shield of the enemy boss UFO, thereby removing its shield.

\begin{figure}[htp]\centering \resizebox{\textwidth}{!}{\input{\figpath atari_training_curves.pgf}}\caption{Training curves for PPO baseline and PPO with RUDDER over 200M game frames, 3 runs with different random seeds each. Curves show scores during training of a single agent that does not use safe exploration, smoothed using Locally Weighted Scatterplot Smoothing (y-value estimate using 20\% of data with 10 residual-based re-weightings).\label{fig:atari_training}}\end{figure}




\begin{table}[htp]
\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{3}{c}{{\em average}} & \multicolumn{3}{c}{{\em final}}\\
 & baseline & RUDDER & \% & baseline & RUDDER & \%\\
Alien & 1,878 & 3,087 & 64.4 & 3,218 & 5,703 & 77.3 \\
Amidar & 787 & 724 & -8.0 & 1,242 & 1,054 & -15.1 \\
Assault & 5,788 & 4,242 & -26.7 & 10,373 & 11,305 & 9.0 \\
Asterix & 10,554 & 18,054 & 71.1 & 29,513 & 102,930 & 249 \\
Asteroids & 22,065 & 4,905 & -77.8 & 310,505 & 154,479 & -50.2 \\
Atlantis & 1,399,753 & 1,655,464 & 18.3 & 3,568,513 & 3,641,583 & 2.0 \\
BankHeist & 936 & 1,194 & 27.5 & 1,078 & 1,335 & 23.8 \\
BattleZone & 12,870 & 17,023 & 32.3 & 24,667 & 28,067 & 13.8 \\
BeamRider & 2,372 & 4,506 & 89.9 & 3,994 & 6,742 & 68.8 \\
Berzerk & 1,261 & 1,341 & 6.4 & 1,930 & 2,092 & 8.4 \\
Bowling & 61.5 & 179 & 191 & 56.3 & 192 & 241 \\
Boxing & 98.0 & 94.7 & -3.4 & 100 & 99.5 & -0.5 \\
Breakout & 217 & 153 & -29.5 & 430 & 352 & -18.1 \\
Centipede & 25,162 & 23,029 & -8.5 & 53,000 & 36,383 & -31.4 \\
ChopperCommand & 6,183 & 5,244 & -15.2 & 10,817 & 9,573 & -11.5 \\
CrazyClimber & 125,249 & 106,076 & -15.3 & 140,080 & 132,480 & -5.4 \\
DemonAttack & 28,684 & 46,119 & 60.8 & 464,151 & 400,370 & -13.7 \\
DoubleDunk & -9.2 & -13.1 & -41.7 & -0.3 & -5.1 & -1,825 \\
Enduro & 759 & 777 & 2.5 & 2,201 & 1,339 & -39.2 \\
FishingDerby & 19.5 & 11.7 & -39.9 & 52.0 & 36.3 & -30.3 \\
Freeway & 26.7 & 25.4 & -4.8 & 32.0 & 31.4 & -1.9 \\
Frostbite & 3,172 & 4,770 & 50.4 & 5,092 & 7,439 & 46.1 \\
Gopher & 8,126 & 4,090 & -49.7 & 102,916 & 23,367 & -77.3 \\
Gravitar & 1,204 & 1,415 & 17.5 & 1,838 & 2,233 & 21.5 \\
Hero & 22,746 & 12,162 & -46.5 & 32,383 & 15,068 & -53.5 \\
IceHockey & -3.1 & -1.9 & 39.4 & -1.4 & 1.0 & 171 \\
Kangaroo & 2,755 & 9,764 & 254 & 5,360 & 13,500 & 152 \\
Krull & 9,029 & 8,027 & -11.1 & 10,368 & 8,202 & -20.9 \\
KungFuMaster & 49,377 & 51,984 & 5.3 & 66,883 & 78,460 & 17.3 \\
MontezumaRevenge & 0.0 & 0.0 & 38.4 & 0.0 & 0.0 & 0.0 \\
MsPacman & 4,096 & 5,005 & 22.2 & 6,446 & 6,984 & 8.3 \\
NameThisGame & 8,390 & 10,545 & 25.7 & 10,962 & 17,242 & 57.3 \\
Phoenix & 15,013 & 39,247 & 161 & 46,758 & 190,123 & 307 \\
Pitfall & -8.4 & -5.5 & 34.0 & -75.0 & 0.0 & 100 \\
Pong & 19.2 & 18.5 & -3.9 & 21.0 & 21.0 & 0.0 \\
PrivateEye & 102 & 34.1 & -66.4 & 100 & 33.3 & -66.7 \\
Qbert & 12,522 & 8,290 & -33.8 & 28,763 & 16,631 & -42.2 \\
RoadRunner & 20,314 & 27,992 & 37.8 & 35,353 & 36,717 & 3.9 \\
Robotank & 24.9 & 32.7 & 31.3 & 32.2 & 47.3 & 46.9 \\
Seaquest & 1,105 & 2,462 & 123 & 1,616 & 4,770 & 195 \\
Skiing & -29,501 & -29,911 & -1.4 & -29,977 & -29,978 & 0.0 \\
Solaris & 1,393 & 1,918 & 37.7 & 616 & 1,827 & 197 \\
SpaceInvaders & 778 & 1,106 & 42.1 & 1,281 & 1,860 & 45.2 \\
StarGunner & 6,346 & 29,016 & 357 & 18,380 & 62,593 & 241 \\
Tennis & -13.5 & -13.5 & 0.2 & -4.0 & -5.3 & -32.8 \\
TimePilot & 3,790 & 4,208 & 11.0 & 4,533 & 5,563 & 22.7 \\
Tutankham & 123 & 151 & 22.7 & 140 & 163 & 16.3 \\
Venture & 738 & 885 & 20.1 & 820 & 1,350 & 64.6 \\
VideoPinball & 19,738 & 19,196 & -2.7 & 15,248 & 16,836 & 10.4 \\
WizardOfWor & 3,861 & 3,024 & -21.7 & 6,480 & 5,950 & -8.2 \\
YarsRevenge & 46,707 & 60,577 & 29.7 & 109,083 & 178,438 & 63.6 \\
Zaxxon & 6,900 & 7,498 & 8.7 & 12,120 & 10,613 & -12.4 \\

 \bottomrule
\end{tabular}
}\caption{Scores on all 52 considered Atari games for the PPO baseline and PPO with RUDDER and the improvement by using RUDDER in percent (\%).
Agents are trained for 200M game frames (including skipped frames) with {\em no-op starting condition},
i.e.\ a random number of up to 30 no-operation actions at the start of
each game.
Episodes are prematurely terminated if a maximum of 108K frames is reached.  
Scoring metrics are (a) {\em average},
the average reward per completed game throughout training, which favors fast learning \cite{Schulman:17} 
and (b) {\em final}, 
the average over the last 10 consecutive games at the end of training, 
which favors consistency in learning.
Scores are shown for one agent without safe exploration.}\label{tab:full_atari_scores}\end{center}
\end{table}

\clearpage
{\bf Visual Confirmation of Detecting Relevant Events by Reward Redistribution.} 
We visually confirm a meaningful and helpful redistribution
of reward in both Bowling and Venture during training.
As illustrated in Figure~\ref{fig:ventureexample},
RUDDER is capable of redistributing a reward to key events in a game,
drastically shortening the delay of the reward and quickly steering
the agent toward good policies.
Furthermore, it enriches sequences that were sparse in reward
with a dense reward signal. Video demonstrations are available at \url{https://goo.gl/EQerZV}.
\begin{figure}[htp]
\includegraphics[angle=0,width=0.49\textwidth]{\figpath rr_examples/bowling_rr.pdf} \hfill
\includegraphics[angle=0,width=0.49\textwidth]{\figpath rr_examples/venture_rr.pdf}
\caption{Observed return decomposition by RUDDER in two Atari games with long delayed rewards.
{\bf Left:} In the game Bowling, reward is only given after a turn which consist of multiple rolls.
RUDDER identifies the actions that 
guide the ball in the right direction to hit all pins. 
Once the ball hit the pins, RUDDER detects the delayed reward 
associated with striking the pins down. 
In the figure only 100 frames are represented 
but the whole turn spans more than 200 frames. 
In the original game, the reward is given only at the end of the turn.
{\bf Right:} In the game Venture, reward is only obtained after picking the treasure.
RUDDER guides the agent (red) towards the treasure (golden) via reward redistribution.
Reward is redistributed to entering a room with treasure.
Furthermore, the redistributed reward gradually increases 
as the agent approaches the treasure.
For illustration purposes, the green curve shows the
return redistribution before applying lambda. 
The environment only gives reward at the event of 
collecting treasure (blue). \label{fig:ventureexample}}
\end{figure}




\clearpage
\pagebreak
\section{Discussion and Frequent Questions}

\paragraph{RUDDER and reward rescaling.}
RUDDER works with no rescaling, various rescalings, and sign function 
as we have confirmed in additional experiments.
Rescaling ensures similar reward magnitudes across different Atari games, 
therefore the same hyperparameters can be used for all games. 
For LSTM and PPO, we only scale the original return by a constant factor, 
therefore do not change the problem and do not simplify it. 
The sign function, in contrast, may simplify the problem 
but may change the optimal policy.

\paragraph{RUDDER for infinite horizon: Continual Learning.}
RUDDER assumes a finite horizon problem.
For games and for most tasks in real world these assumptions apply: 
did you solve the task? (make tax declaration, convince a customer to buy, 
design a drug, drive a car to a location, 
assemble a car, build a building, clean the room, cook a meal, pass the Turing test). 
In general our approach can be extended to continual learning with discounted reward. 
Only the transformation of an immediate reward MDP 
to an MDP with episodic reward is no longer possible. 
However the delayed reward problem becomes more obvious and also 
more serious when not discounting the reward.

\paragraph{Is the LSTM in RUDDER a state-action value function?}
For reward redistribution we assume an MDP with one reward (=return) 
at sequence end which can be predicted from the last state-action pair. 
When introducing the -states, 
the reward cannot be predicted from the last  
and the task is no longer Markov. 
However the return can be predicted from the sequence of s. 
Since the s are mutually independent, 
the contribution of each  to the return must be stored 
in the hidden states of the LSTM to predict the final reward. 
The  can be generic as states and actions can be numbered 
and then the difference of this numbers can be used for .

In the applications like Atari with immediate rewards 
we give the accumulated reward at the end of the episode without enriching the states. 
This has a similar effect as using . 
We force the LSTM to build up an internal state 
which tracks the already accumulated reward. 

True, the LSTM is the value function at time  based on the 
 sub-sequence up to t. 
The LSTM prediction can be decomposed into two sub-predictions. 
The first sub-prediction is the contribution of the already known 
 sub-sequence up to t to the return (backward view). 
The second sub-prediction is the expected contribution 
of the unknown future sequence from t+1 onwards 
to the return (forward view). 
However, we are not interested in the second sub-prediction 
but only in the contribution of  to the prediction of the expected return. 
The second sub-prediction is irrelevant for our approach. 
We cancel the second sub-prediction via the differences of predictions. 
The difference at time t gives the contribution of  to the expected return.

Empirical confirmation:
Four years ago, 
we started this research project with using LSTM as a value function, but we failed. 
This was the starting point for RUDDER. 
In the submission, we used LSTM predictions in artificial task (IV) 
as potential function for reward shaping, look-ahead advice, and look-back advice. 
Furthermore, we investigated LSTM as a value function for artificial task (II) 
but these results have not been included.
At the time where RUDDER already solved the task, 
the LSTM error was too large to allow learning via a value function. 
Problem is the large variance of the returns at the beginning of the sequence 
which hinders LSTM learning (forward view). 
RUDDER LSTM learning was initiated by propagating back prediction errors at the sequence end, 
where the variance of the return is lower (backward view). 
These late predictions initiated the storing of key events at the sequence beginning 
even with high prediction errors. 
The redistributed reward at the key events led RUDDER solve the task. 
Concluding: at the time RUDDER solved the task, 
the early predictions are not learned due to the high variance of the returns. 
Therefore using the predictions as value function does not help (forward view).

Example: The agent has to take a key to open the door. 
Since it is an MDP, 
the agent is always aware to have the key indicated by a key bit to be on. 
The reward can be predicted in the last step. 
Using differences  the key bit is zero, 
except for the step where the agent takes the key. 
Thus, the LSTM has to store this event and will transfer reward to it.

\paragraph{Compensation reward.} The compensation corrects for prediction errors of  ( is the sum of ). 
The prediction error of  can have two sources: 
(1) the probabilistic nature of the reward, 
(2) an approximation error of g for the expected reward. 
We aim to make (2) small and then the correction is only for the probabilistic nature of the reward.
The compensation error depends on , which, in turn, depends on the whole sequence. 
The dependency on state-action pairs from  to  is viewed as random effect, 
therefore the compensation reward only depends on the last state-action pair. 

That  and  depends only on  
is important to prove Theorem 3. Then  cancels and the advantage function remains the same.


\paragraph{Connection theory and algorithms.}
Theorem 1 and Theorem 2 ensure that the algorithms are correct since 
the optimal policies do not change even for non-optimal return decompositions. 
In contrast to TD methods which are biased, 
Theorem 3 shows that the update rule -value estimation 
is unbiased when assuming optimal decomposition. 
Theorem 4 explicitly derives optimality conditions 
for the expected sum of delayed rewards kappa 
and measures the distance to optimality. 
This kappa is used for learning and 
is explicitly estimated to correct learning 
if an optimal decomposition cannot be assured. 
The theorems are used to justify following learning methods (A) and (B):

(A) Q-value estimation: 
(i) Direct Q-value estimation (not Q-learning) according to Theorem 3 is given in Eq. (9) when an optimal decomposition is assumed.
(ii) Q-value estimation with correction by kappa according to Theorem 4, when optimal decomposition is not assumed. 
Here kappa is learned by TD as given in Eq. (10). (iii) Q-value estimation using eligibility traces.
(B) Policy gradient: Theorems are used as for Q-value estimation as in (A) 
but now the Q-values serve for policy gradient.
(C) Q-learning: Here the properties in Theorem 3 and Theorem 4 are ignored.

We also shows variants (not in the main paper) on page 31 and 32 of using kappa Correction of the reward redistribution 
by reward shaping with kappa and 
Using kappa as auxiliary task in predicting the return for return decomposition.

\paragraph{Optimal Return Decomposition, contributions and policy.} The Q-value  depends on a particular policy . 
The function h depends on policy  since 
h predicts the expected return () 
which depends on . 
Thus, both return decomposition and optimal return decomposition 
are defined for a particular policy . 
A reward redistribution from a return decomposition 
leads to a return equivalent MDP. 
Return equivalent MDPs are defined via all policies 
even if the reward redistribution was derived from a particular policy. 
A reward redistribution depends only on the state-action sequence 
but not on the policy that generated this sequence. 
Also  does not depend on a policy.


\paragraph{Optimal policies are preserve for every state.}
We assume all states are reachable via at least one non-zero transition probability 
to each state and policies that have a non-zero probability 
for each action due to exploration. 
For an MDP being optimal in the initial state 
is the same as being optimal in every reachable state. 
This follows from recursively applying the Bellman optimality equation 
to the initial value function. 
The values of the following states must be optimal 
otherwise the initial value function is smaller. 
Only states to which the transition probability is zero 
the Bellman optimality equation does not determine the optimality.

All RL algorithms are suitable. 
For example we applied TD, Monte Carlo, Policy Gradient, 
which all work faster with the new MDP.

\paragraph{Limitations.}
In all of the experiments reported in this manuscript, we show that RUDDER significantly outperforms other methods for delayed reward problems. However, RUDDER might not be effective when the reward is not delayed since LSTM learning takes extra time and has problems with very long sequences. Furthermore, reward redistribution may introduce disturbing spurious reward signals.


\clearpage
\pagebreak
\section{Additional Related Work}

\paragraph{Delayed Reward.}

To learn delayed rewards there are three phases to consider:
(i) discovering the delayed reward,
(ii) keeping information about the delayed reward,
(iii) learning to receive the delayed reward to secure it for the future.
Recent successful reinforcement learning methods provide solutions to one
or more of these phases.
Most prominent are
Deep -Networks (DQNs) \cite{Mnih:13,Mnih:15}, which
combine -learning with convolutional neural networks for 
visual reinforcement learning \cite{Koutnik:13}.
The success of DQNs is attributed to
{\em experience replay} \cite{Lin:93}, which stores
observed state-reward transitions and then samples from them.
Prioritized experience replay \cite{Schaul:15,Horgan:18} advanced the
sampling from the replay memory.
Different policies perform exploration in parallel for the Ape-X DQN 
and share a prioritized experience replay memory \cite{Horgan:18}.
DQN was extended to double DQN (DDQN) \cite{Hasselt:10,Hasselt:16}
which helps exploration as the overestimation bias is reduced.
Noisy DQNs \cite{Fortunato:18} explore by
a stochastic layer in the policy network (see \cite{Hochreiter:90,Schmidhuber:90diff}).
Distributional -learning \cite{Bellemare:17} profits from noise since means that
have high variance are more likely selected.
The dueling network architecture \cite{Wang:15,Wang:16} separately
estimates state values and action advantages,
which helps exploration in unknown states.
Policy gradient approaches \cite{Williams:92} explore via parallel
policies, too. 
A2C has been improved by IMPALA through parallel actors and
correction for policy-lags between actors and learners \cite{Espeholt:18}.
A3C with asynchronous gradient descent \cite{Mnih:16}
and  Ape-X DPG \cite{Horgan:18} also rely on parallel policies.
Proximal policy optimization (PPO) extends A3C by a surrogate
objective and a trust region optimization that is realized by clipping or
a Kullback-Leibler penalty \cite{Schulman:17}.

Recent approaches aim to solve learning problems caused
by delayed rewards.
Function approximations of value functions or critics \cite{Mnih:15,Mnih:16}
bridge time intervals if states associated
with rewards are similar to states that were encountered many steps earlier.
For example, assume a function that has learned to predict a large reward
at the end of an episode
if a state has a particular feature. 
The function can generalize this
correlation to the beginning of an episode and predict already
high reward for states possessing the same feature.
Multi-step temporal difference (TD) learning \cite{Sutton:88td,Sutton:18book} 
improved both DQNs
and policy gradients \cite{Hessel:17,Mnih:16}.
AlphaGo and AlphaZero learned to play Go and Chess better than
human professionals using Monte Carlo Tree Search (MCTS) \cite{Silver:16,Silver:17}.
MCTS simulates games from a time point until the end of 
the game or an evaluation point and therefore captures long delayed rewards.
Recently, world models using an evolution strategy were successful \cite{Ha:18}.
These forward view approaches are not
feasible in probabilistic environments with a high branching factor of state transition.

\paragraph{Backward View.}

We propose learning from a backward view, which 
either learns a separate model or analyzes a forward model. 
Examples of learning a separate model are 
to trace back from known goal states
\cite{Edwards:18} or from high reward states \cite{Goyal:18}.
However, learning a backward model is very challenging.
When analyzing a forward model that predicts the return then either
sensitivity analysis or contribution analysis may be utilized.
The best known backward view approach
is sensitivity analysis (computing the gradient)
like ''Backpropagation through a Model
\cite{Munro:87,Robinson:89,RobinsonFallside:89,Werbos:90,Bakker:07}. 
Sensitivity analysis has several drawbacks:
local minima, instabilities, exploding or vanishing
gradients, and proper exploration
\cite{Hochreiter:90,Schmidhuber:90diff}.
The major drawback is that
the relevance of actions is missed
since sensitivity analysis does not consider their contribution to 
the output but
only their effect on the output when slightly perturbing them.

We use contribution analysis
since sensitivity analysis has serious drawbacks.
Contribution analysis determines how much a state-action pair
contributes to the final prediction.
To focus on state-actions which are most relevant for learning 
is known from prioritized sweeping 
for model-based reinforcement learning \cite{Moore:93}. 
Contribution analysis can be done 
by computing differences of return predictions when adding another input,
by zeroing out an input and then compute the change in the prediction,
by contribution-propagation \cite{Landecker:13},
by a contribution approach \cite{Poulin:06},
by excitation backprop \cite{Zhang:16},
by layer-wise relevance propagation (LRP) \cite{Bach:15},
by Taylor decomposition \cite{Bach:15,Montavon:17taylor}, or 
by integrated gradients (IG) \cite{Sundararajan:17}.


\paragraph{LSTM.}

LSTM was already used in reinforcement learning \cite{Schmidhuber:15}
for advantage learning \cite{Bakker:02}, for constructing a potential 
function for reward shaping by representing the return by a sum of 
LSTM outputs across an episode \cite{Su:15}, and
learning policies \cite{Hausknecht:15,Mnih:16,Heess:16}.


\paragraph{Reward Shaping, Look-Ahead Advice, Look-Back Advice.}

Redistributing the reward is fundamentally different from
reward shaping \cite{Ng:99,Wiewiora:03}, look-ahead advice and
look-back advice \cite{Wiewiora:03icml}. 
However, these methods can be viewed as a special case of 
reward redistribution that result in an MDP that is return-equivalent 
to the original MDP as is shown in Section~\ref{sec:AReturnEquivalent}. 
On the other hand every reward function can be expressed as
look-ahead advice \cite{Harutyunyan:15}.
In contrast to these methods, 
reward redistribution is not limited to potential functions, where
the additional reward is the potential difference, therefore it is a more
general concept than shaping reward or look-ahead/look-back advice.
The major difference of reward redistribution to reward shaping,
look-ahead advice, and look-back advice 
is that the last three keep the original rewards. 
Both look-ahead advice and look-back advice 
have not been designed for replacing for the original rewards.
Since the original reward is kept, the reward redistribution is
not optimal according to Section~\ref{sec:Aopt_rew_red}.
The original rewards 
may have long delays that cause an exponential slow-down of learning. 
The added reward improves sampling but a delayed original reward must
still be transferred to the -values of early states that caused the reward.
The concept of return-equivalence of SDPs resulting from 
reward redistributions allows to 
eliminate the original reward completely.
Reward shaping can replace the original reward.
However, it only depends on states but not on actions, 
and therefore, it cannot identify
relevant actions without the original reward.





\clearpage
\pagebreak


\section{Markov Decision Processes with Undiscounted Rewards}

We focus on Markov Decision Processes (MDPs)
with undiscounted rewards, since
the relevance but also the problems
of a delayed reward can be considerably decreased by discounting it.
Using discounted rewards both the bias correction in TD as well as the variance 
of MC are greatly reduced. The correction amount decreases exponentially with 
the delay steps, and also the variance contribution to one state 
decreases exponentially 
with the delay of the reward.

MDPs with undiscounted rewards are either finite horizon or process absorbing 
states without reward. The former can always be described by the latter.


\subsection{Properties of the Bellman Operator in MDPs with Undiscounted Rewards}
\label{sec:ApropPoly}

At each time  the environment is in some state . The agent
takes an action  according to policy , which causes a transition of
the environment to state 
and a reward  for the agent
with probability .


The Bellman operator maps a action-value function  to another
action-value function. We do not require that  are -values and
that  is the actual reward. 
We define the Bellman operator  for policy  as:
 

We often rewrite the operator as
 
where
 
We did not explicitly express the dependency on the policy  and
the state-action pair  in the
expectation . A more precise way would be to write
.


More generally, we have
 
In the following we show properties for this general formulation.

\subsubsection{Monotonically Increasing and Continuous}
\label{sec:ApropPolyFP}

We assume the general formulation Eq.~\eqref{eq:bellGen} of the
Bellman operator.
Proposition 2.1 on pages 22-23 in
Bertsekas and Tsitsiklis, 1996, \cite{Bertsekas:96} shows that
a fixed point  of the Bellman operator exists and that for every
:
 
The fixed point equation 

is called {\em Bellman equation} or {\em Poisson equation}.
For the Poisson equation see
Equation~33 to Equation~37 for the undiscounted case and Equation~34 and Equation~43 for the
discounted case in Alexander Veretennikov, 2016,
\cite{Veretennikov:16}.
This form of the Poisson equation describes the Dirichlet boundary
value problem. The Poisson equation is
 
where  is the long term average reward or the expected 
value of the reward for the stationary distribution:
 
We assume  since after some time the agent does no longer
receive reward in MDPs with finite time horizon or in MDPs with
absorbing states that have zero reward.


 is {\em monotonically increasing} in its arguments \cite{Bertsekas:96}.
For  and  with the component-wise condition , we have

where ``'' is component-wise. The last inequality follows from
the component-wise condition .

We define the norm , which gives the maximal difference of
the -values:


 is a {\em non-expansion mapping} for  and :
 

The first inequality is valid since
the absolute value is moved into the sum.
The second inequality is valid since
the expectation depending on  is replaced by a maximum that
does not depend on .
Consequently, the operator  is continuous.



\subsubsection{Contraction for Undiscounted Finite Horizon}
\label{sec:ApropPolyCon}

For time-aware states, we can define another norm with
 which
allows for a contraction mapping:



 is a {\em contraction mapping} for  and  \cite{Bertsekas:96}:
 
The equality in the last but one line stems from the fact that
all -values at  are zero and that all -values at  have
the same constant value. 


Furthermore, all  values are equal to
zero for additionally introduced
states at  since for  all rewards are zero.
We have
 
which is correct for additionally introduced states at time  since they are zero.
Then, in the next iteration -values of states at time  are correct.
After iteration , -values of states at time  are correct.
This iteration is called the ``backward induction algorithm'' \cite{Puterman:90,Puterman:05}.
If we perform this iteration for a policy  instead of
the optimal policy, then this procedure is called ``policy evaluation
algorithm'' \cite{Puterman:90,Puterman:05}.





\subsubsection{Contraction for Undiscounted Infinite Horizon With Absorbing States}
\label{sec:ApropPolyCon2}

A stationary policy is {\em proper} if there exists an integer 
such that from any initial state  the probability of achieving
the terminal state after  steps is strictly positive.

If all terminal states are absorbing and cost/reward free and
if all stationary policies are proper the Bellman operator is a contraction
mapping with respect to a weighted sup-norm.

The fact that the Bellman operator is a contraction
mapping with respect to a weighted sup-norm
has been proved in Tseng, 1990, in Lemma 3 with equation (13) and
text thereafter \cite{Tseng:90Journal}. Also Proposition 1
in Bertsekas and Tsitsiklis, 1991, \cite{Bertsekas:91},
Theorems 3 and 4(b) \& 4(c) in Tsitsiklis, 1994, \cite{Tsitsiklis:94},
and Proposition 2.2 on pages 23-24 in
Bertsekas and Tsitsiklis, 1996, \cite{Bertsekas:96} have proved
the same fact.


\subsubsection{Fixed Point of Contraction is Continuous wrt Parameters}
\label{sec:ApropPolyFPcon}


The mean   and variance  are continuous with respect to , that is
, with
respect to the reward distribution  and with respect to
the transition probabilities . 

A complete metric space or a Cauchy space is a space where every
Cauchy sequence of points has a limit in the space, that is,
every Cauchy sequence converges in the space.
The Euclidean space   with the usual distance metric is complete.
Lemma 2.5 in Jachymski, 1996, is \cite{Jachymski:96}:
\begin{theoremA}[Jachymski: complete metric space]
Let  be a complete metric space, and let  be a
metric space. Let 
be continuous in the first variable and
contractive in the second variable with the same Lipschitz constant
. For
, let 
be the unique fixed point of the map . Then the
mapping  is continuous.
\end{theoremA}
This theorem is Theorem 2.3 in Frigon, 2007,  \cite{Frigon:07}.
Corollary 4.2 in Feinstein, 2016, generalized the theorem
to set valued operators, that is, these operators may have
more than one fixed point \cite{Feinstein:16}
(see also \cite{Kirr:97}).
All mappings  must have the same Lipschitz constant
.


A locally compact space is a space where  every point has a compact neighborhood.
 is locally compact as a consequence of the Heine-Borel theorem.
Proposition 3.2 in Jachymski, 1996, is \cite{Jachymski:96}:
\begin{theoremA}[Jachymski: locally compact complete metric space]
Let  be a locally compact complete metric space,
and let  be a
metric space. Let 
be continuous in the first variable and
contractive in the second variable with not necessarily the
same Lipschitz constant.
For , let 
be the unique fixed point of the map . Then the
mapping  is continuous.
\end{theoremA}
This theorem is Theorem 2.5 in Frigon, 2007,  \cite{Frigon:07}
and Theorem 2 in Kwiecinski, 1992, \cite{Kwiecinski:92}.
The mappings  can have different Lipschitz constants.

\subsubsection{t-fold Composition of the Operator}
\label{sec:At-fold}

We define the Bellman operator as

where  is the vector with value  at position 
and  is the vector with value
 at position .

In vector notation we obtain
the {\em Bellman equation} or {\em Poisson equation}.
For the Poisson equation see
Equation~33 to Equation~37 for the undiscounted case and Equation~34 and Equation~43 for the
discounted case in Alexander Veretennikov, 2016,
\cite{Veretennikov:16}.
This form of the Poisson equation describes the Dirichlet boundary
value problem.
The {\em Bellman equation} or {\em Poisson equation} is

where  is the row-stochastic matrix with
 at position .


The Poisson equation is
 
where  is the vector of ones and
 is the long term average reward or the expected 
value of the reward for the stationary distribution:
 
We assume  since after some time the agent does no longer
receive reward for MDPs with finite time horizon or MDPs with
absorbing states that have zero reward.




Since  is a row-stochastic matrix, the
Perron-Frobenius theorem says that (1)  has as largest eigenvalue 1 for which
the eigenvector corresponds to the steady state and
(2) the absolute value of each (complex) eigenvalue is smaller or equal 1.
Only the eigenvector to the eigenvalue 1 has purely positive real components.





Equation~7 of Bertsekas and Tsitsiklis, 1991, \cite{Bertsekas:91}
states


If  is the stationary distribution vector for , that is,

then




\subsection{Q-value Transformations: Shaping Reward, Baseline, and Normalization}
\label{sec:qtransform}

The Bellman equation for the action-value function  is


The expected return at time  is:

As introduced for the REINFORCE algorithm,
we can subtract a baseline  from the return.
We subtract the baseline  from the last reward.
Therefore, for the new reward 
we have  for  and
.
Consequently,  for .


The TD update rules are:

The -errors are

and for the last step


If we set 

then the -errors and the updates remain the same for  and .
We are equally far away from the optimal solution in both cases.





Removing the offset  at the end by ,
can also be derived via reward shaping.
However, the offset has to be added at the beginning: .
Reward shaping requires for the shaping reward  and a potential
function  \cite{Ng:99,Wiewiora:03}:


For introducing a reward of  at time  and removing
it from time  we set:

then the shaping reward is


For , , and  we obtain above situation but with
 and ,
that is,  is removed at the end and
added at the beginning. All -values except  are
decreased by .
In the general case,
all -values  with
 are increased by .


{\bf -value normalization}:
We apply reward shaping \cite{Ng:99,Wiewiora:03} for normalization of
the -values.
The potential  defines the shaping reward
.
The optimal policies do not change and the -values become

We change the -values for all , but not
for  and . The first and the last -values are not normalized.
All the shaped reward is added/subtracted to/from the initial and the
last reward.

\begin{itemize}
\item The maximal -values are zero and the non-optimal -values are negative
for all :

  
\item The minimal -values are zero and all others -values are positive
for all :

\end{itemize}




\subsection{Alternative Definition of State Enrichment}
\label{sec:AStateEnrich}
Next, we define state-enriched processes  compared to .
The state  of  is enriched with a
deterministic information compared to a state  of .
The enriched information
in  can be computed from the state-action pair 
and the reward .
Enrichments may be the accumulated reward, count of the time step,
a count how often a certain action
has been performed, a count how often a certain state has been
visited, etc.
Givan et~al.\ have already shown that state-enriched Markov decision
processes (MDPs) preserve the optimal action-value and action sequence properties
as well as the optimal policies of the model \cite{Givan:03}.
Theorem 7 and Corollary 9.1 in Givan et~al.\ proved
these properties \cite{Givan:03} by bisimulations
(stochastically bisimilar MDPs).
A homomorphism between MDPs maps a MDP 
to another one with corresponding reward and transitions probabilities.
Ravindran and Barto have shown that solving the original MDP can be
done by solving a homomorphic image \cite{Ravindran:03}.
Therefore, Ravindran and Barto have also shown that 
state-enriched MDPs preserve the optimal action-value and action sequence properties.
Li et al.\ give an overview over state abstraction or state aggregation for
MDPs, which covers state-enriched MDPs \cite{Li:06}.


\begin{definitionA}
  A decision process  is {\em state-enriched} compared to
  a decision process  if following conditions hold.
  If  is the state of ,
  then there exists a function  with ,
  where   is the state of .
  There exists a function , where 
  gives the additional information of state  compared to .
  There exists a function  with
  , that is, the state  can be
  constructed from the original state and the additional information.
  There exists a function  with , where 
  is the next state and  the reward.
   ensures that   of the next state 
  can be computed from
  reward , actual state , and the actual action
  . Consequently,  can be computed from .
  For all  and  following holds: 
 
 where  and  are the probabilities of the initial states
 of  and , respectively.
\end{definitionA}
If the reward is deterministic, then
 and
.




We proof the following theorem, even if it has been proved several
times as mention above.
\begin{theoremA}
  If decision process  is state-enriched compared to
  , then for each optimal policy  of  there
  exists an equivalent optimal policy  of , and vice
  versa, with . The
  optimal return is the same for  and .
\end{theoremA}




\begin{proof}
  We proof by induction that
   if
  .

  {\bf Basis}: The end of the sequence.
  For  we have
  , since
  no policy receives reward for .
  
{\bf Inductive step ()}: Assume  for the next state  and next action .

For the induction step  we use

instead of .

It follows that
, and therefore



Using Bellman's optimality equation would give the same result,
where in above equation
both  and  are
replaced by .
\end{proof}


\begin{theoremA}
  If a Markov decision process  is state-enriched compared to
  the MDP , then for each optimal policy  of  there
  exists an equivalent optimal policy  of , and vice
  versa, with . The
  optimal return is the same for  and .
\end{theoremA}
\begin{proof}
  The MDP  is a homomorphic image of .
  For state-enrichment, the mapping  is bijective, therefore the
  optimal policies in  and  are equal according to
  Lemma~\ref{th:Arav}. The optimal return is also equal since it does
  not change via state-enrichment.
\end{proof}


\subsection{Variance of the Weighted Sum of a Multinomial Distribution}

State transitions are multinomial distributions and the future
expected reward is a weighted sum of multinomial distributions.
Therefore, we are interested in the variance of the weighted sum
of a multinomial distribution.
Since we have

the variance of 
is determined by the variance of the multinomial distribution
. In the following we derive
the variance of the estimation of a
linear combination of variables of a multinomial distribution like
.


A multinomial distribution with parameters  as
event probabilities satisfying 
and support ,  for 
trials, that is , has

where  is the random variable and  the actual count.

A linear combination of random variables has variance



The variance of estimating the mean  of independent random variables
 that all have variance  is:



When estimating the mean  over  samples
of a linear combination of variables of a
multinomial distribution , where each  has
 trials, we obtain:


\section{Long Short-Term Memory (LSTM)}

\subsection{LSTM Introduction}


Recently, {\em Long Short-Term Memory} (LSTM; \cite{Hochreiter:91,Hochreiter:95,Hochreiter:97})
networks have emerged as the best-performing technique in speech and language processing.
LSTM networks have been overwhelming successful in different speech and language applications,
including handwriting recognition \cite{Graves:09}, generation of writings
\cite{Graves:13}, language modeling and identification \cite{Gonzalez-Dominguez:14,Zaremba:14arxiva},
automatic language translation \cite{Sutskever:14nips}, 
speech recognition \cite{Sak:14,Geiger:14}
analysis of audio data \cite{Marchi:14}, analysis, annotation, and
description of video data \cite{Donahue:14,Venugopalan:14,Srivastava:15}. 
LSTM has facilitated recent benchmark records in TIMIT phoneme recognition (Google),
optical character recognition, text-to-speech synthesis (Microsoft),
language identification (Google), large vocabulary speech recognition (Google),
English-to-French translation (Google), audio onset detection, social signal classification,
image caption generation (Google), video-to-text description, end-to-end speech recognition (Baidu),
and semantic representations. In the proceedings of the flagship conference {\em ICASSP 2015}
(40\textsuperscript{th} IEEE International Conference on Acoustics, Speech and Signal
Processing, Brisbane, Australia, April 19--24, 2015), 13 papers had ``LSTM'' in their
title, yet many more contributions described computational approaches that make use of LSTM.

The key idea of LSTM is the use of memory cells that allow for constant error flow
during training. Thereby, LSTM avoids the {\em vanishing gradient problem}, that is,
the phenomenon that training errors are decaying when they are back-propagated through time
\cite{Hochreiter:91,Hochreiter:00}.
The vanishing gradient problem severely impedes {\em credit assignment} in recurrent neural
networks, i.e.\ the correct identification of relevant events whose effects are not
immediate, but observed with possibly long delays.
LSTM, by its constant error flow, avoids vanishing gradients and, hence, allows for
{\em uniform credit assignment}, i.e.\ all input signals obtain a similar error signal.
Other recurrent neural networks are not able to assign the same credit to all input signals,
therefore they are very limited concerning the solutions they will
find. Uniform credit assignment enabled LSTM networks to excel in speech and
language tasks: if a sentence is analyzed, then the first word can be as important as
the last word. Via uniform credit assignment, LSTM networks regard all words of a sentence equally.
Uniform credit assignment enables to consider all input information
at each phase of learning, no matter where it is located in the input
sequence. Therefore, uniform credit assignment reveals many more
solutions to the learning algorithm which would otherwise remain hidden. 

\begin{figure}[htb]
\centering
\includegraphics[angle=0,width=1.0\textwidth]{lstm_fig}
\caption{LSTM memory cell without peepholes. 
 is the vector of cell input
activations,  is the vector of input gate
activations,   is the vector of forget gate
activations,   is the vector of memory cell states,
 is the vector of output gate
activations, and  is the vector of cell output 
activations. The activation functions are  for the cell input,  for the cell
state, and  for the gates. Data flow is either ``feed-forward''
without delay or ``recurrent'' with an one-step delay.
``Input'' connections are from the
external input to the LSTM network, while ``recurrent'' connections take inputs
from other memory cells and hidden units of the LSTM network with a delay of one time step.  
\label{fig:cellFB}}
\end{figure}

\subsection{LSTM in a Nutshell}

The central processing and storage unit for LSTM recurrent networks is
the {\em memory cell}. As already mentioned, it avoids vanishing gradients and allows for
uniform credit assignment.
The most commonly used LSTM memory cell architecture in the 
literature \cite{Graves:05,Schmidhuber:15} 
contains forget gates \cite{Gers:99a,Gers:00}
and peephole connections \cite{Gers:00a}. 
In our previous work \cite{Hochreiter:01,Hochreiter:07}, 
we found that peephole connections are 
only useful for modeling time series, but not for 
language, meta-learning, or biological sequences. 
That peephole connections can be removed without performance decrease, 
was recently confirmed in a large assessment, where 
different LSTM architectures have been tested \cite{Greff:15}.
While LSTM networks are highly successful in various applications, 
the central memory cell architecture was not modified since 2000 \cite{Schmidhuber:15}.
A memory cell architecture without peepholes is depicted in
Figure~\ref{fig:cellFB}. 

In our definition of a LSTM network, all units of one kind are
pooled to a vector:  is the vector of cell input
activations,  is the vector of input gate
activations,   is the vector of forget gate
activations,   is the vector of memory cell states,
 is the vector of output gate
activations, and  is the vector of cell output 
activations.
We assume to have an input sequence, where the input vector at 
time  is . The matrices , ,
, and  correspond to the
weights of the connections between inputs and cell input, input gate, forget gate, and
output gate, respectively.
The vectors  , ,
, and  are the bias vectors of cell input, input gate, forget gate, and
output gate, respectively.
The activation functions are  for the cell input,  for the cell
state, and  for the gates, where these functions are evaluated in a
component-wise manner if they are applied to vectors.
Typically, either the sigmoid  or
 are used as activation functions.
 denotes the point-wise multiplication
of two vectors. Without peepholes, the LSTM memory cell forward pass rules
are (see Figure~\ref{fig:cellFB}):


\subsection{Long-Term Dependencies vs.\ Uniform Credit Assignment}

The LSTM network has been proposed with the aim
to learn {\em long-term dependencies} in sequences
which span over long intervals
\cite{Hochreiter:97,Hochreiter:97e,Hochreiter:97f,Hochreiter:98}. 
However, besides extracting long-term dependencies, 
LSTM memory cells have another, even
more important, advantage in sequence learning:
as already described in the early 1990s,
LSTM memory cells allow for {\em uniform credit assignment}, that is,
the propagation of errors back to inputs without 
scaling them \cite{Hochreiter:91}. 
For uniform credit assignment of current LSTM architectures,
the forget gate  must be one or close to one.  
A memory cell without an input gate  just sums up all the squashed inputs it
receives during scanning the input sequence.
Thus, such a memory cell is equivalent to a unit that sees all sequence
elements at the same time, as has been shown via 
the ``Ersatzschaltbild'' \cite{Hochreiter:91}.
If an output error occurs only at the end of the sequence,
such a memory cell, via backpropagation, supplies
the same delta error at the cell input unit  at every time
step.
Thus, all inputs obtain the same credit for producing the correct
output and are treated on an equal level and, consequently, the incoming weights to a memory cell 
are adjusted by using the same delta error at the input unit .

In contrast to LSTM memory cells, standard recurrent networks scale
the delta error and assign different credit to different inputs.
The more recent the input, the more credit it obtains.
The first inputs of the sequence are hidden from the final states of
the recurrent network.
In many learning tasks, however, important information is distributed over
the entire length of the sequence and can even occur at the very beginning. For
example, in language- and text-related tasks, 
the first words are often important for the meaning of a sentence. 
If the credit assignment is not uniform along the input sequence, then
learning is very limited. Learning would start by trying to improve
the prediction solely by using the most recent inputs.
Therefore, the solutions that can be found are restricted to those
that can be constructed if the last inputs are considered first.
Thus, only those solutions are found that are accessible by gradient
descent from regions in the parameter space that only use the most recent input information.
In general, these limitations lead to sub-optimal solutions, since 
learning gets trapped in local optima. 
Typically, these local optima correspond to solutions 
which efficiently exploit the most recent information in the input
sequence, while information way back in the past is neglected.

\subsection{Special LSTM Architectures for contribution Analysis}
\label{sec:ALSTMadjust}


\subsubsection{LSTM for Integrated Gradients}

For Integrated Gradients contribution analysis with LSTM, 
we make following assumptions:
\begin{enumerate}[label=\textbf{(A\arabic*)}]
\item  for all . That is the forget gate is always 1 and
  nothing is forgotten. We assume uniform credit assignment, which
  is ensured by the forget gate set to one.

\item  for all . That is the output gate is always 1 and
  nothing is forgotten. 

\item We set  with .

\item We set  with .

\item The cell input gate  is only connected to the input but not
  to other memory cells.   has only connections to the
  input. 

\item The input gate  is not connected to the input, that is,
   has only connections to other memory cells. This ensures
  that LRP assigns relevance only via  to the input.


\item The input gate  has a negative bias, that is,
  . The negative bias
  reduces the drift effect, that is, the memory 
  content  either increases or decreases over time.
  Typical values are .

\item The memory cell content is initialized with zero at time ,
  that is, .
 
\end{enumerate}

The resulting LSTM forward pass rules for Integrated Gradients are:

See Figure~\ref{fig:cellMarkovA} which depicts these  
forward pass rules for Integrated Gradients. 


\begin{figure}[htb]
\centering
\includegraphics[angle=0,width=1.0\textwidth]{lstm_figMarkovA}
\caption{LSTM memory cell used for Integrated Gradients (IG). 
  Forget gates and output gates are set to 1 since they
  can modify all cell inputs at times after they have been observed,
  which can make the dynamics highly nonlinear.
\label{fig:cellMarkovA}}
\end{figure}


\subsubsection{LSTM for LRP}

LRP has already been used for LSTM in order to
identify important terms in sentiment analysis \cite{Arras:17}.
In texts, positive and negative terms with respect to the topic
could be identified.

For LRP contribution analysis with LSTM, we make following assumptions:
\begin{enumerate}[label=\textbf{(A\arabic*)}]
\item  for all . That is the forget gate is always 1 and
  nothing is forgotten. We assume uniform credit assignment, which
  is ensured by the forget gate set to one.

\item , that is,  is positive. For example we can use a sigmoid
  : , with
  .
  Methods like LRP have problems with negative contributions
  which cancel with positive contributions \cite{Montavon:17}.
  With a positive  all
  contributions are positive.
  The cell input  (the function ) has a negative bias, that is,
  . This is important to avoid the drift effect.
  The drift effect is that the memory content only gets positive
  contributions which lead to an increase of  over time.
  Typical values are .

\item We want to ensure that . If the memory content is zero
  then nothing is transferred to the next layer.
  Therefore we set  with .

\item The cell input gate  is only connected to the input but not
  to other memory cells.   has only connections to the
  input. This ensures
  that LRP assigns relevance  to the input and  is not
  disturbed by redistributing relevance to the network.

\item The input gate  is not connected to the input, that is,
   has only connections to other memory cells. This ensures
  that LRP assigns relevance only via  to the input.

\item The output gate  is not connected to the input, that is,
   has only connections to other memory cells. This ensures
  that LRP assigns relevance only via  to the input.

\item The input gate  has a negative bias, that is,
  . Like with the cell input the negative bias
  avoids the drift effect.
  Typical values are .

\item The output gate  may also have a negative bias, that is,
  . This allows to bring in different memory cells at
  different time points. It is related to resource allocation.
  
\item The memory cell content is initialized with zero at time ,
  that is, . The memory cell content  
  is non-negative  since 
   and .
 
\end{enumerate}

The resulting LSTM forward pass rules for LRP are:

See Figure~\ref{fig:cellLRP} which depicts these  
forward pass rules for LRP. However, gates may be used while no
relevance is given to them which may lead to inconsistencies.


\begin{figure}[htb]
\centering
\includegraphics[angle=0,width=1.0\textwidth]{lstm_figLPR}
\caption{LSTM memory cell used for Layer-Wise Relevance Propagation (LRP). 
 is the vector of cell input
activations,  is the vector of input gate
activations,   is the vector of memory cell states,
 is the vector of output gate
activations, and  is the vector of cell output 
activations. The activation functions are
the sigmoid 
and the cell state activation . 
Data flow is either ``feed-forward''
without delay or ``recurrent'' with an one-step delay.
External input reaches the LSTM network 
only via the cell input . All gates only receive
recurrent input, that is, from other memory cells.
\label{fig:cellLRP}}
\end{figure}



\paragraph{LRP and Contribution Propagation for LSTM.}

We analyze Layer-wise Relevance Propagation (LRP) and Contribution Propagation
for LSTM networks.
A single memory cell can be described by:

Here we treat  like a weight for  and  has weight 1.


For positive values of ,  , and ,
both LRP and contribution propagation leads to

Since we predict only at the last step , we have
 for . For  we obtain , since
.

We obtain for :

which gives

and consequently as  we obtain

Since we assume , we have

and therefore


Therefore the relevance  is distributed across the inputs
 for , where input  obtains relevance .


\subsubsection{LSTM for Nondecreasing Memory Cells}

contribution analysis is made simpler if memory cells are nondecreasing since
the contribution of each input to each memory cells
is well defined. The problem that a 
negative and a positive input cancels each other is avoided. 
For nondecreasing memory cells 
and contribution analysis with LSTM, 
we make following assumptions:
\begin{enumerate}[label=\textbf{(A\arabic*)}]
\item  for all . That is the forget gate is always 1 and
  nothing is forgotten. We assume uniform credit assignment, which
  is ensured by the forget gate set to one.

\item , that is,  is positive. For example we can use a sigmoid
  : , with
  .
  With a positive  all
  contributions are positive.
  The cell input  (the function ) has a negative bias, that is,
  . This is important to avoid the drift effect.
  The drift effect is that the memory content only gets positive
  contributions which lead to an increase of  over time.
  Typical values are .

\item We want to ensure that . If the memory content is zero
  then nothing is transferred to the next layer.
  Therefore we set  with .

\item The cell input gate  is only connected to the input but not
  to other memory cells.   has only connections to the
  input. 

\item The input gate  is not connected to the input, that is,
   has only connections to other memory cells. 

\item The output gate  is not connected to the input, that is,
   has only connections to other memory cells. 

\item The input gate  has a negative bias, that is,
  . Like with the cell input the negative bias
  avoids the drift effect.
  Typical values are .

\item The output gate  may also have a negative bias, that is,
  . This allows to bring in different memory cells at
  different time points. It is related to resource allocation.
  
\item The memory cell content is initialized with zero at time ,
  that is, . We ensured via the architecture that 
  and , that is, 
  the memory cells are positive and nondecreasing.
 
\end{enumerate}

The resulting LSTM forward pass rules for nondecreasing memory cells are:

See Figure~\ref{fig:cellLRPMarkov} for a LSTM memory cell that is
nondecreasing.

\begin{figure}[htb]
\centering
\includegraphics[angle=0,width=1.0\textwidth]{lstm_figMarkov}
\caption{A nondecreasing LSTM memory cell. 
\label{fig:cellLRPMarkov}}
\end{figure}

\subsubsection{LSTM without Gates}

The most simple LSTM architecture for contribution analysis does not
use any gates. Therefore complex dynamics that have to be treated 
in the contribution analysis are avoided.
For LSTM without gates, 
we make following assumptions:
\begin{enumerate}[label=\textbf{(A\arabic*)}]
\item  for all . That is the forget gate is always 1 and
  nothing is forgotten. 

\item  for all . That is the output gate is always 1.

\item  for all . That is the input gate is always 1.

\item , that is,  is positive. For example we can use a sigmoid
  : , with
  .
  With a positive  all
  contributions are positive.
  The cell input  (the function ) has a negative bias, that is,
  . This is important to avoid the drift effect.
  The drift effect is that the memory content only gets positive
  contributions which lead to an increase of  over time.
  Typical values are .

\item We want to ensure that . If the memory content is zero
  then nothing is transferred to the next layer.
  Therefore we set  with .

\item The memory cell content is initialized with zero at time ,
  that is, . 
 
\end{enumerate}

The resulting LSTM forward pass rules are:

See Figure~\ref{fig:cellLRPNoGate} for a LSTM memory cell without
gates which perfectly distributes the relevance across the input.


\begin{figure}[htb]
\centering
\includegraphics[angle=0,width=1.0\textwidth]{lstm_figNoGate}
\caption{LSTM memory cell without gates. 
\label{fig:cellLRPNoGate}}
\end{figure}

\clearpage


\pagebreak

\section{Contribution Analysis}
\label{sec:Aback}

\subsection{Difference of Consecutive Predictions for Sequences}

\paragraph{General Approach.}

The idea is to assess the information gain that is induced by an input at a particular time step. This information gain is used for predicting the target at sequence end
by determining the change in prediction.
The input to a recurrent neural network is the sequence
 with target , which is only given at
sequence end.
The prefix sequence  of length  is .
 predicts the target  at every time step :
 
We can define the decomposition of  through contributions at different time steps
 
where  is a predefined constant.
We have
 

We assume a loss function for  that is minimal if  predicts the expected

 
Then 
 
In this case, the contributions are the change in the expectation of 
the target that will be observed at sequence end.
The contribution can be viewed as the information gain in time step 
for predicting the target.
If we cannot ensure that  predicts the target at every time step,
then other contribution analysis methods must be employed.
For attributing the prediction of a deep
network to its input features several contribution analysis
methods have been proposed.
We consider Input Zeroing, 
Integrated Gradients (IG), and Layer-Wise Relevance Propagation (LRP).


\paragraph{Linear Models and Coefficient of Determination.}

We consider linear models and the average gain of information about
the reward at sequence end if we go one time step further in the 
input sequence.
By adding a variable, that is, another sequence element, the mean squared error (MSE)
decreases, which is the amount by which the expectation improves due to new information.
But by what amount does the MSE decrease in average? 
Here, we consider linear models.
For linear models we are interested in how much the coefficient of 
determination increases if we add another variable, that is, if we
see another input.

We consider the feature vector  from which
the target  (the reward at sequence end) has to be predicted. 
We assume to have  pairs ,
as training set. 
The prediction or estimation of  from  is  with 
. The vector of all training labels is  and
the training feature matrix is .
We define the mean squared error (MSE) as



The {\em coefficient of determination} 
is equal to the correlation between the target  and
its prediction .
 is given by:

Therefore,  is one minus the ratio of the mean squared
error divided by the mean total sum of squares.
 is a strict monotonically decreasing function of the
mean squared error.


We will give a breakdown of the factors that determine how much
each variable adds to  \cite[chapter 10.6, p.~263]{Rencher:08}.
The feature vector  is expanded by one additional feature :
.
We want to know the increase in  due to adding .
Therefore, we decompose  into  and .  
The difference in coefficients of determination is the difference 
of the according MSEs divided by the empirical variance of :


We further need definitions: 
\begin{itemize}
\item .
\item .
\item The sample covariance between  and 
  is , where
   and  are the sample means. The variance of  is  often
  written as , the standard deviation squared: .
\item The correlation between  and  is .
\item The covariance matrix  of a vector  is the matrix
  with entries .
\item The covariance matrix  of a vector  is the matrix
  with entries .
\item The diagonal matrix 
  has a th diagonal entry
   and is the diagonal matrix of standard deviations of the components
  of .
\item  is the squared multiple correlation between  and .
\item  is the squared multiple correlation between  and .
\item   is
  the squared multiple correlation between  and .
\item  is the simple correlation between  and : .
\item  is
  the vector of correlations between  and .
\item  is
  the vector of correlations between  and .
\item  is
  the vector of standardized regression coefficients (beta weights)
of  regressed on .
\item The parameter vector is partitioned into the constant 
  and  via
  .
  We have for the maximum likelihood estimate
 
 The offset  guarantees ,
 therefore, ,
 since :
 

 
\item The vector of {\em standardized coefficients}  are
 
\end{itemize}


The next theorem is Theorem~10.6 in Rencher and Schaalje \cite{Rencher:08}
and gives a breakdown of the factors that determine how much
each variable adds to  \cite[Chapter 10.6, p.~263]{Rencher:08}.
\begin{theorem}[Rencher Theorem 10.6]
The increase in  due to  can be expressed as

where 
is a ``predicted'' value of  based on the relationship of  to
the 's.
\end{theorem}

The following equality shows
that  is indeed a
prediction of :


If  is orthogonal to 
(i.e., if ), then , which implies
that  and . In this case,
Eq.~\eqref{eq:increase} can be written as

Consequently, if all  are independent from each other, then

The contribution of  to  can either be less than or
greater than .
If the correlation  can be predicted from , then
 is close to  and, therefore,  has
contributes less to  than .

Next, we compute the contribution of  to  explicitly. 
The correlation between  and  is 

We assume that .
We want to express the information gain using the mean squared error (MSE)
.
We define the error  at sample  
with . Therefore,
the MSE is equal to the empirical variance
.
The correlation  between the target  and the
error  is

Using Eq.~\eqref{eq:empCorr} and Eq.~\eqref{eq:corrYZ},
we can express the difference between the estimate  and
the true correlation  by:


The information gain can now be expressed by
the correlation  between the target  and the
error :

The information gain is the squared
correlation  between the target  and the
error .
{\bf The information gain is the information in  about , which is
not contained in .}


                           



\subsection{Input Zeroing}

The simplest contribution analysis method is Input Zeroing, where just
an input is set to zero to determine its contribution to the output.
Input Zeroing sets a particular input  to zero and then computes
the network's output. For the original input  and the input with
, i.e.\ ,
we compute  to obtain the
contribution of . 
We obtain for the difference of  to the baseline
of average zeroing :

The problem is that the  have to be computed -times, that
is, for each input component zeroed out.

Input Zeroing does not recognize redundant inputs, i.e.\ each one of the 
inputs is sufficient 
to produce the output but if all inputs are missing at the same time 
then the output changes. 
In contrast, Integrated Gradients (IG) and Layer-Wise Relevance Propagation (LRP)
detect the relevance of an input even if it is redundant.



\subsection{Integrated Gradients}

Integrated gradients is a recently introduced method
\cite{Sundararajan:17}.
Integrated gradients decomposes the difference 
between the network output  and a baseline :

In contrast to previous approaches, we have  and its derivative to
evaluate only -times, where .
 

The equality can be seen if we define  and

Consequently, we have


For the final reward decomposition, we obtain



\subsection{Layer-Wise Relevance Propagation}
\label{sec:ALRP}

Layer-Wise Relevance Propagation (LRP) \cite{Bach:15}
has been introduced to interpret machine learning models.
LRP is an extension of
the contribution-propagation algorithm \cite{Landecker:13} based on
the contribution approach \cite{Poulin:06}.
Recently ``excitation backprop'' was proposed \cite{Zhang:16},
which is like LPR but uses only positive weights and shifts the
activation function to have non-negative values.
Both algorithms assign a relevance or importance value
to each node of a neural network which describes how much
it contributed to generating the network output.
The relevance or importance is recursively propagated back:
A neuron is important to the network output if it has been important to its
parents, and its parents have been important to the network output.
LRP moves through a neural network like backpropagation:
it starts at the output, redistributes the relevance scores of one
layer to the previous layer until the input layer is reached.
The redistribution procedure satisfies a local relevance
conservation principle. All relevance values that a node obtains from its parents will be
redistributed to its children.
This is analog to Kirchhoff's first law for the conservation of electric charge
or the continuity equation in physics for transportation in general form.
LRP has been used for deep neural networks (DNN) \cite{Montavon:17} and
for recurrent neural networks like LSTM \cite{Arras:17}.

We consider a neural network with activation  for neuron .
The weight from neuron  to neuron  is denoted by .
The activation function is  and  is the netinput to
neuron  with bias .
We have following forward propagation rules:
 


Let  be the relevance for neuron  and 
the share of relevance  that flows from neuron  in the higher
layer to neuron  in the lower layer.
The parameter  is a weighting for the share of  of neuron
 that flows to neuron . We define  as

The relative contributions  are
previously defined as \cite{Bach:15,Montavon:17,Arras:17}:

Here,  is the contribution of  to the netinput value .
If neuron  is removed from the network,
then   will be the difference to the original .



The relevance  of neuron  is the sum of relevances it obtains
from its parents  from a layer above: 

Furthermore, a unit  passes on all its relevance  to its children,
which are units  of the layer below:

It follows the {\em conservation of relevance}.
The sum of relevances 
of units  in a layer is equal to the sum of
relevances  of units 
of a layer below:
 


The scalar output  
of a neural network with input 
is considered as relevance  which is
decomposed into contributions  of the inputs :
 

The decomposition is valid for recurrent neural networks,
where the relevance at the output
is distributed across the sequence elements of the
input sequence.


\subsubsection{New Variants of LRP}
\label{sec:ALRPvariants}


An alternative definition of  is
 
where  is the mean of  across samples.
Therefore,  is the contribution of the
actual sample to the variance of . This in turn is related to
the information carried by .
Here,  is the contribution of  to the variance of
. However, we can have negative values of
 which may lead to negative contributions even
if the weights are positive.

Another alternative definition of  is
 
Here,  is the contribution of  to the activation
value .
If neuron  is removed from the network,
then   will be the difference to the original .
If  is strict monotone increasing and , then
positive weights  will lead to positive values and negative weights
 to negative values.


\noindent Preferred Solution:\newline
A definition of  is
 
where  is the minimum of  either across samples
(mini-batch) or across time steps.
The difference  is always positive.
Using this definition,
activation functions with negative values are possible, like for
excitation backprop \cite{Zhang:16}.
The minimal value is considered as default off-set, which can be
included into the bias. 


\subsubsection{LRP for Products}
\label{sec:ALRPproduct}


Here we define relevance propagation for products of two units.
We assume that  with  and . 
We view  and  as units of a layer below the layer in
which  is located. Consequently,  has to be divided between  and ,
which gives the conservation rule



Alternative 1:

The relevance is equally distributed.





\noindent Preferred Solution:\newline
Alternative 2:
The contributions according to the deep Taylor decomposition around
 are

We compute the relative contributions:

For  we obtain  and 
as contributions.


We use this idea but scale  and  to the range :

The relevance is distributed according to how close the maximal value
is achieved and how far away it is from the minimal value. 




Alternative 3:

All -values are negative, therefore the fraction in front of
 is positive.  leads to a zero relevance for .
The ratio of the relevance for 
increases to 1 when  approaches . 
The relevance is distributed according to how close the maximal value
is achieved. We assume that the maximal value is a saturating value,
therefore we use , the natural logarithm.



\subsection{Variance Considerations for contribution Analysis}

We are interested how the redistributed reward affects the variance 
of the estimators. 
We consider
(A) the difference of consecutive predictions is the redistributed reward,
(B) integrated gradients (IG), and
(C) layer-wise relevance propagation (LRP).

For (A) the difference of consecutive predictions is the redistributed reward,
all variance is moved to the final correction. However imperfect  and variance
cannot be distinguished.

For (B) integrated gradients (IG) the redistributed rewards depend on future values.
Therefore the variance can even be larger than in the original MDP.

For (C) layer-wise relevance propagation (LRP) the variance is propagated back without
decreasing or increasing if the actual return is used as relevance. If the prediction is
used as relevance and a final correction is used then the variance is moved to the final
prediction but new variance is injected since rewards depend on the future path.



\clearpage
\pagebreak
\section{Reproducibility Checklist}
We followed the reproducibility checklist \cite{Pineau:18} and point to relevant sections. 
\paragraph{For all models and algorithms presented, check if you include:}
\begin{itemize}
    \item \textbf{A clear description of the mathematical setting, algorithm, and/or model.}
    
    Description of mathematical settings starts at paragraph ~\nameref{c:def}.
    
    Description of novel learning algorithms starts at paragraph \nameref{c:novel}.
    \item \textbf{An analysis of the complexity (time, space, sample size) of any algorithm.}
    
    Plots in Figure~\ref{fig:test} show the number of episodes, i.e.\ the sample size, which are needed for convergence to the optimal policies. They are evaluated for different algorithms and delays in all artificial tasks.
    For Atari games, the number of samples corresponds to the number of game frames. See paragraph \nameref{c:Atari}.
    We further present a bias-variance analysis of TD and MC learning in Section~\ref{sec:Abias_variance_estimator} and Section~\ref{sec:Abias_variance_sample} in the appendix.
\item \textbf{A link to a downloadable source code, with specification of all dependencies, including external libraries.}
    
    \href{https://github.com/ml-jku/baselines-rudder}{https://github.com/ml-jku/baselines-rudder}
\end{itemize}

\paragraph{For any theoretical claim, check if you include:}
\begin{itemize}
    \item \textbf{A statement of the result.}
    
    The main theorems:
    \begin{itemize}
        \item Theorem \ref{th:EquivT}
        \item Theorem \ref{th:zeroExp}
        \item Theorem \ref{th:OptReturnDecomp}
    \end{itemize}
    
    Additional supporting theorems can be found in the proof section of the appendix \ref{c:RR}.
    \item \textbf{A clear explanation of any assumptions.}
    
The proof section \ref{c:RR} in the appendix covers all the assumptions for the main theorems.
    \item \textbf{A complete proof of the claim.}
    
Proof of the main theorems are moved to the appendix.
\begin{itemize}
    \item Proof of Theorem 1 can be found after Theorem \ref{th:AEquivT} in the appendix. 
    \item Proof of Theorem 2 can be found after Theorem \ref{th:AzeroExp} in the appendix. 
    \item Proof of Theorem 3 can be found after Theorem \ref{th:AOptReturnDecomp} in the appendix. 
\end{itemize}
Proofs for additional theorems can also be found in this appendix.
\end{itemize}

\paragraph{For all figures and tables that present empirical results, check if you include:}
\begin{itemize}
    \item \textbf{A complete description of the data collection process, including sample size.}
    
    For artificial tasks the environment descriptions can be found in section \nameref{sec:Aexp} in the main paper. For Atari games, we use the standard sampling procedures as in OpenAI Gym \cite{Brockman:16} (description can be found in paragraph \nameref{para:Atari}).
\item \textbf{A link to a downloadable version of the dataset or simulation environment.}
    
    Link to our repository:
    \href{https://github.com/ml-jku/baselines-rudder}{https://github.com/ml-jku/rudder}
    
    \item \textbf{An explanation of any data that were excluded, description of any pre-processing step}
    
    For Atari games, we use the standard pre-processing described in \cite{Mnih:16}.
    \item \textbf{An explanation of how samples were allocated for training / validation / testing.}
    
    For artificial tasks, description of training and evaluation are included in section \ref{sec:Aexp} . For Atari games, description of training and evaluation are included Section~\ref{sec:Aexp}.
    \item \textbf{The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results.}
    
    A description can be found at paragraph \nameref{c:ppomodel} in the appendix. 
    \item \textbf{The exact number of evaluation runs.}
    
    For artificial tasks evaluation was performed during training runs. See Figure~\ref{fig:test}.
    For Atari games see paragraph \nameref{para:Atari}.
    We also provide a more detailed description in Section~\ref{sec:Aexp} and Section~\ref{sec:Aatari} in the appendix.
    \item \textbf{A description of how experiments were run}.
    For artificial task, description can be found at \ref{sec:Mexperiments}.
    
    For Atari games, description starts at paragraph \nameref{para:Atari}.
    We also provide a more detailed description in Section~\ref{sec:Aexp} and Section~\ref{sec:Aatari} in the appendix.
    \item \textbf{A clear definition of the specific measure or statistics used to report results.}
    
For artificial tasks, see section \ref{sec:Mexperiments}.
For Atari games, see section \ref{sec:Aatari} and the caption of Table 1. 
We also provide a more detailed description in Section~\ref{sec:Aexp} and Section~\ref{sec:Aatari} in the appendix.
    \item \textbf{Clearly defined error bars.}
    
For artificial tasks, see caption of Figure~\ref{fig:test}, second line.
For Atari games we show all runs in Figure~\ref{fig:atari_training} in the appendix.
    \item \textbf{A description of results with central tendency (e.g.\ mean) \& variation (e.g.\ stddev).}
    
An exhaustive description of the results including mean, variance and significant test, is included in Table~\ref{tab:res1}, Table~\ref{tab:Ares1} and Table~\ref{tab:Ares2} in Section~\ref{sec:Aexp} in the appendix.
    \item \textbf{A description of the computing infrastructure used.}
    
We distributed all runs across 2 CPUs per run and 1 GPU per 4 runs for Atari experiments. We used various GPUs including GTX 1080 Ti, TITAN X, and TITAN V.
Our algorithm takes approximately 10 days.
\end{itemize}

\vfill
\pagebreak
\section{References}
\label{sec:Areferences}
\renewcommand{\section}[2]{}\bibliographystyle{plain}
\bibliography{lrp}





\end{appendices}





\end{document}
