\section{Experiments}\label{sxn:exps}

The goal of our experimental section is to establish that our approximation
to  (as computed by Algorithm~\ref{alg1a}) is both accurate and fast for both
dense and sparse matrices.
The accuracy of Algorithm~\ref{alg1a} is measured by comparing its result
against the exact  computed via the Cholesky factorization.
The rest of this section is organized as follows:
in Section~\ref{subsec:software}, we describe our software for approximating
; in Section~\ref{subsec:environment} we describe the computational environment that we used; and
in Sections~\ref{subsec:dense_matrices} and~\ref{subsec:sparse_matrices} we
discuss experimental results for dense and sparse SPD matrices, respectively.

\subsection{Software}
\label{subsec:software}
We developed high-quality, shared- and distributed-memory parallel \Cpp{}
code for the algorithms listed in this paper.
All of the code that was developed for this paper is hosted at \url{https://github.com/pkambadu/ApproxLogDet}.
In it's current state, our software supports:
(1) ingesting dense (binary and text format) and sparse (binary,
text, and matrix market format) matrices,
(2) generating large random SPD matrices,
(3) computing both approximate and exact spectral norms of matrices,
(4) computing both approximate and exact traces of matrices, and
(5) computing both approximate and exact log determinants of matrices.
Currently, we support both Eigen~\cite{eigenweb} and
Elemental~\cite{poulson2013elemental} matrices.
The Eigen software package supports both dense and sparse matrices, while the Elemental software package mostly
supports dense matrices and only recently added support for sparse matrices
(pre-release).
As we wanted the random SPD generation to be fast, we have used parallel
random number generators from Random123~\cite{salmon2011parallel} in
conjunction with Boost.Random.


\subsection{Environment}
\label{subsec:environment}
All our experiments were run on ``Nadal'', a 60-core machine, where each
core is an Intel\textregistered{} Xeon\textregistered{} E7-4890 machine running
at 2.8 Ghz.
Nadal has 1 TB of RAM and runs Linux kernel version 2.6-32.
For compilation, we used GCC 4.9.2.
We used Eigen 3.2.4, OpenMPI 1.8.4, Boost 1.55.7, and the latest version of
Elemental at {\small\url{https://github.com/elemental}}.
For experiments with Elemental, we used OpenBlas, which is an extension of
GotoBlas~\cite{goto2008high}, for its parallel prowess; Eigen has built-in the
BLAS and LAPACK packages.


\subsection{Dense Matrices}
\label{subsec:dense_matrices}
\vspace{0.01in}\noindent \textbf{Data Generation.}
In our experiments, we used two types of synthetic SPD matrices. The first
type were diagonally dominant SPD matrices and were generated as follows.
First, we created  by drawing  entries from a
uniform sphere with center 0.5 and radius 0.25.
Then, we generated a symmetric matrix  by setting

Finally, we ensured that the desired matrix  is positive definite by adding the value  to
each diagonal entry~\cite{curran2009variation} of :

We call this method~\code{randSPDDenseDD}.

The second approach generates SPD matrices that are not diagonally dominant.
We created  by drawing  and 
entries, respectively, from a uniform sphere with center 0.5 and radius 0.25;
 is a diagonal matrix with small entries.
Next, we generated an orthogonal random matrix . Thus,
 is an orthonormal basis for .
Finally, we generated

We call this method \code{randSPDDense}.
\code{randSPDDense} is more expensive than \code{randSPDDenseDD}, as it requires
an additional  computations for the QR factorization and the
matrix-matrix product.

\vspace{0.02in}\noindent \textbf{Evaluation.}
To evaluate the runtime of Algorithm~\ref{alg1a} against a baseline, we used the Cholesky decomposition
to compute the . More specifically, we computed  and
returned .
Since Elemental provides distributed and shared memory parallelism, we restricted
ourselves to experiments with Elemental matrices throughout this section.
Note that we measured the accuracy of the approximate algorithm in terms of the
relative error to ensure that we have numbers of the same scale for matrices
with vastly different values for ;
we defined the relative error  as , where  is the true value and
 is the approximation.
Similarly, we defined the speedup  as ,
where  is the time needed to compute  and  is the time needed to
compute the approximation .

\vspace{0.02in}\noindent \textbf{Results.}
For dense matrices, we first used synthetic matrices generated using~\code{randSPDDense}; these are relatively ill-conditioned matrices. We experimented with values of  (number of rows and columns of ) in the set .
The three key points pertaining to these matrices are shown in
Figure~\ref{fig:dense}.
First, we discuss the effect of , the number of terms in the Taylor series
used to approximate ; Figure~\ref{fig:dense-accuracy} depicts
our results for the sequential case.
On the -axis, we see the relative error, which is measured against the exact
 as computed via the Cholesky factorization.
We observe that for these ill-conditioned matrices, for small values of  (less than four)
the relative error is high.
However, for all values of , we observe that the error drops
significantly and stabilizes.
We note that in each iteration, all random processes were re-seeded with new
values; we have plotted the error bars throughout Figure~\ref{fig:dense}.
The standard deviation for both accuracy and time was consistently
small; indeed, it is not visible to the naked eye at scale.
To see the benefit of approximation, we look at
Figure~\ref{fig:dense-speedup-m} together with Figure~\ref{fig:dense-accuracy}.
For example, at , for all matrices, we get at least a factor of two speedup.
As  gets larger, the speedups of the
approximation also increase.
For example, for , the speedup at  is nearly six-fold. In terms of accuracy,
Figure~\ref{fig:dense-accuracy} shows that at , the relative error is approximately
.
This speedup is expected as the Cholesky factorization requires 
operations; Algorithm~\ref{alg1a} only relies on matrix-matrix products where one
of the matrices has a small number of columns (equal to ), which is
independent of .
\begin{figure*}[t]
\begin{center}
\subfigure[Accuracy vs. ]{\includegraphics[width=0.32\textwidth]{results/dense-accuracy-m-p=0.png}
\label{fig:dense-accuracy}
}
\subfigure[Speedup vs. ]{\includegraphics[width=0.32\textwidth]{results/dense-speedup-m-p=0.png}
\label{fig:dense-speedup-m}
}
\subfigure[Parallel Speedup]{\includegraphics[width=0.32\textwidth]{results/dense-speedup-p-m=4.png}
\label{fig:dense-speedup-np}
}
\end{center}
\caption{
Panels~\ref{fig:dense-accuracy} and~\ref{fig:dense-speedup-m} depict the effect
of  (see Algorithm~\ref{alg1a}) on the accuracy of the approximation and the
time to completion, respectively, for dense matrices generated by
\code{randSPDDense}.
For all the panels,  and .
The baseline for all experiments was the Cholesky factorization, which was used
to compute the exact value of .
For panels~\ref{fig:dense-accuracy} and~\ref{fig:dense-speedup-m}, the number
of cores, , was set to one.
The last panel~\ref{fig:dense-speedup-np} depicts the relative speedup of
the approximate algorithm when compared to the baseline solver (at ).
Elemental was used as the backend for these experiments.
For the approximate algorithm, we report the mean and standard deviation
of ten iterations.
}
\label{fig:dense}
\end{figure*}
\begin{figure*}[t]
\begin{center}
\subfigure[Accuracy vs. ]{\includegraphics[width=0.32\textwidth]{results/dense-dd-accuracy-m-p=0.png}
\label{fig:dense-dd-accuracy}
}
\subfigure[Speedup vs. ]{\includegraphics[width=0.32\textwidth]{results/dense-dd-speedup-m-p=0.png}
\label{fig:dense-dd-speedup-m}
}
\subfigure[Parallel Speedup]{\includegraphics[width=0.32\textwidth]{results/dense-dd-speedup-p-m=2.png}
\label{fig:dense-dd-speedup-np}
}
\end{center}
\caption{
Panels~\ref{fig:dense-dd-accuracy} and~\ref{fig:dense-dd-speedup-m} depict the
effect of  (see Algorithm~\ref{alg1a}) on the accuracy of the approximation
and the time to completion, respectively, for diagonally dominant dense random
matrices generated by \code{randSPDDenseDD}.
For all the panels,  and .
The baseline for all experiments was the Cholesky factorization, which was used
to compute the exact value of .
For panels~\ref{fig:dense-dd-accuracy} and~\ref{fig:dense-dd-speedup-m}, the
number of cores, , was set to one.
The last panel~\ref{fig:dense-dd-speedup-np} depicts the relative speedup of
the approximate algorithm when compared to the baseline solver (at ).
Elemental was used as the backend for these experiments.
For the approximate algorithm, we report the mean and standard deviation
over ten iterations.
}
\label{fig:dense-dd}
\end{figure*}
\begin{table}
\center
\scriptsize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{}  &
\multicolumn{3}{|c|}{} &
\multicolumn{3}{|c|}{time (secs)} \\
\cline{2-7}
      &   exact    & mean       & std   &   exact  & mean  &  std   \\\hline
5000  & -3717.89  & -3546.920  &  8.10  &   2.56   &  1.15 & 0.0005 \\\hline
7500  & -5474.49  & -5225.152  &  8.73  &   7.98   &  2.53 & 0.0015 \\\hline
10000 & -7347.33  & -7003.086  &  7.79  &  18.07   &  4.47 & 0.0006 \\\hline
12500 & -9167.47  & -8734.956  & 17.43  &  34.39   &  7.00 & 0.0030 \\\hline
15000 & -11100.9  & -10575.16  & 15.09  &  58.28   & 10.39 & 0.0102 \\\hline
\end{tabular}
\caption{
Accuracy and sequential running times (at ,  and ) for dense
random matrices generated using \code{randSPDDense}.
Baselines were computed using the Cholesky factorization; mean and standard
deviation are reported over ten iterations.
}
\label{tbl:dense-abs}
\normalsize
\end{table}
\begin{table}
\center
\scriptsize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{}  &
\multicolumn{3}{|c|}{} &
\multicolumn{3}{|c|}{time (secs)} \\
\cline{2-7}
      &   exact   & mean     & std    &   exact  & mean  &  std   \\\hline
10000 &  92103.1  &  92269.5 &  5.51  &   18.09  &  2.87 &  0.01  \\\hline
20000 & 198069.0  & 198397.4 &  9.60  &  135.92  & 12.41 &  0.02  \\\hline
30000 & 309268.0  & 309763.8 & 20.04  &  448.02  & 30.00 &  0.12  \\\hline
40000 & 423865.0  & 424522.4 & 14.80  & 1043.74  & 58.05 &  0.05  \\\hline
\end{tabular}
\caption{
Accuracy and sequential running times (at , , and ) for
diagonally dominant dense random matrices generated using
\code{randSPDDenseDD}.
Baselines were computed using the Cholesky factorization; mean and standard
deviation are reported over ten iterations.
}
\label{tbl:dense-dd-abs}
\normalsize
\end{table}
Finally, we discuss the parallel speedup in Figure~\ref{fig:dense-speedup-np},
which shows the relative speedup of the approximate algorithm with respect to
the baseline Cholesky algorithm.
For this evaluation, we set  and varied the number of processes, denoted by ,
from  to .
The main take away from Figure~\ref{fig:dense-speedup-np} is that the
approximate algorithm provides nearly the same or increasingly better speedups
relative to a parallelized version of the exact (Cholesky) algorithm.
For example, for , the speedups for using the approximate algorithm
are consistently better that .
The absolute values for  and timing along with the baseline
numbers for this experiment are given in Table~\ref{tbl:dense-abs}.
We report the numbers in Table~\ref{tbl:dense-abs} at  at which point, we
have low relative error.

For the second set of dense experiments, we generated diagonally dominant
matrices using \code{randSPDDenseDD}; we were able to quickly generate and
run benchmarks on matrices of sizes  with  in the set  due to the
relatively simpler procedure involved in matrix generation.
In this set of of experiments, due to the diagonal dominance, all matrices
were well-conditioned.
The results of our experiments on these well-conditioned matrices are
presented in Figure~\ref{fig:dense-dd} and show a marked improved over the
results presented in Figure~\ref{fig:dense}.
First, notice that very few terms of the Taylor series (i.e., small ) are
sufficient to get high accuracy approximations; this is apparent in Figure~\ref{fig:dense-dd-accuracy}.
In fact, we see that even at , we are near convergence and at , for
most of the matrices, we have near-zero relative error.
This experimental result, combined with Figure~\ref{fig:dense-dd-speedup-m} is particularly encouraging; at , we seem to not only have a nearly lossless approximation of , but also have at least a five-fold speedup.
Similarly to Figure~\ref{fig:dense}, the speedups are better for larger matrices.
For example, for , the speedup at  is nearly twenty-fold.
We conclude our analysis by presenting Figure~\ref{fig:dense-dd-speedup-np},
which similarly to Figure~\ref{fig:dense-speedup-np}, points out that at any
level of parallelism, Algorithm~\ref{alg1a} maintains its relative performance
over the exact (Cholesky) factorization.
The absolute values for  and the corresponding running times, along with the baseline
for this experiment are presented in Table~\ref{tbl:dense-dd-abs}.
We report the numbers in Table~\ref{tbl:dense-abs} at , at which point we
have a low relative error.


\subsection{Sparse Matrices}

\label{subsec:sparse_matrices}
\vspace{0.02in}\noindent \textbf{Data Synthesis.}
To generate a sparse, synthetic matrix , with
 non-zeros, we use a Bernoulli distribution to determine the location of
the non-zero entries and a uniform distribution to generate the values.
First, we completely fill the  principle diagonal entries.
Next, we generate  index positions in the upper triangle for
the non-zero entries by sampling from a Bernoulli distribution with
probability .
We reflect each entry across the principle diagonal to ensure that  is symmetric and we add  to each diagonal entry to ensure that  is SPD (actually,  is also diagonally dominant).

\vspace{0.02in}\noindent \textbf{Real Data.}
\noindent
To demonstrate the prowess of Algorithm~\ref{alg1a} on real-world data, we
used SPD matrices from the University of Florida's sparse matrix
collection~\cite{davis2011university}.
The complete list of matrices from this collection used in our experiments, as well as a brief description of each matrix, is given in columns 1--4 of Table~\ref{tbl:ufl}.
\begin{table*}
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{name} &
\multirow{3}{*}{} &
\multirow{3}{*}{} &
\multirow{3}{*}{area of origin} &
\multicolumn{3}{|c|}{\logdet{\matA}} &
\multicolumn{2}{|c|}{time (sec)} &
\multirow{3}{*}{} \\ \cline{5-9}

 & & & & \multirow{2}{*}{exact}  &
         \multicolumn{2}{|c|}{approx} &
         \multirow{2}{*}{exact} &
         approx & \\ \cline{6-7} \cline{9-9}

 & & & & & mean & std & & mean & \\\hline
thermal2        & 1228045  & 8580313   & Thermal         & 1.3869e6   & 1.3928e6  & 964.79 & 31.28  & 31.24 & 149 \\ \hline
ecology2        & 999999   & 4995991   & 2D/3D           & 3.3943e6   & 3.403e6   & 1212.8 & 18.5   & 10.47 & 125 \\ \hline
ldoor           & 952203   & 42493817  & Structural      & 1.4429e7   & 1.4445e7  & 1683.5 & 117.91 & 17.60 &  33 \\ \hline
thermomech\_TC  & 102158   & 711558    & Thermal         & -546787    & -546829.4 & 553.12 & 57.84  &  2.58 &  77 \\ \hline
boneS01         & 127224   & 5516602   & Model reduction & 1.1093e6   & 1.106e6   & 247.14 & 130.4  &  8.48 & 125 \\ \hline
\end{tabular}
\caption{
Description of the SPD matrices from the University of Florida sparse
matrix collection~\cite{davis2011university} that were used in our experiments.
All experiments were run sequentially () using Eigen.
Accuracy results for Algorithm~\ref{alg1a} are reported using both the mean and the standard
deviation over ten iterations at (with  and ); we only report the mean for the running times, since the standard deviation is negligible.
The exact  was computed using the Cholesky factorization.
}
\label{tbl:ufl}
\end{table*}

\vspace{0.02in}\noindent \textbf{Evaluation.}
\noindent
It is tricky to pick any single method as the ``exact method'' to compute the
 for a sparse SPD matrix .
One approach would be to use direct methods such as Cholesky decomposition of
~\cite{davis2006direct,gupta2000wsmp}.
For direct methods, it is difficult to derive an analytical solution
for the number of operations required for the factorization as a function of the
number of non-zero entries of the matrix, as this is highly dependent on the structure of
the matrix~\cite{gupta1997highly}.
In the distributed setting, one also needs to consider the volume of
communication involved, which is often the bottleneck.
Alternately, we can use iterative methods to compute the eigenvalues of
~\cite{davidson1975iterative} and use the eigenvalues to compute
.
It is clear that the worst case performance of both the direct and iterative
methods is .
However, iterative methods are typically used to compute a few eigenvalues and
eigenvectors: therefore, we chose to use the Cholesky factorization based on
matrix reordering to compute the exact value of .
It is important to note that both the direct and iterative methods are
notoriously hard to implement, which comes to stark contrast with the almost trivial implementation of Algorithm~\ref{alg1a}, which is also readily parallelizable.
\begin{figure}[t]
\begin{center}
\subfigure[Convergence with ]{\includegraphics[height=0.25\textwidth,width=0.45\textwidth]{results/sparse-convergence-m-p=0.png}
\label{fig:sparse-convergence-m}
}
\subfigure[Cost as a function of ]{\includegraphics[height=0.25\textwidth,width=0.45\textwidth]{results/cost-of-sparse-m-p=0.png}
\label{fig:sparse-cost-m}
}
\end{center}
\caption{
Panels~\ref{fig:sparse-convergence-m} and~\ref{fig:sparse-cost-m} depict the
effect of the number of terms in the Taylor expansion, , (see
Algorithm~\ref{alg1a}) on the convergence to the final solution and the time to
completion of the approximation.
The matrix size was fixed at  and sparsity was varied as .
Experiments were run sequentially () and we set , .
For panel~\ref{fig:sparse-convergence-m}, the baseline is the final value of
 at .
For panel~\ref{fig:sparse-cost-m}, the baseline is the time to completion of the approximation algorithm with .
Eigen was used as the backend for these experiments.
}
\label{fig:sparse}
\end{figure}

\vspace{0.02in}\noindent \textbf{Results.}
The true power of Algorithm~\ref{alg1a} lies in its ability to approximate
 for sparse .
The Cholesky factorization can introduce  non-zeros during
factorization due to fill-in; for many problems, there is insufficient
memory to factorize a large, sparse matrix.
In our first set of experiments, we wanted to show the effect of  on: (1)
convergence of , and (2) cost of the solution.
To this end, we generated sparse, diagonally dominant SPD matrices of size
 and varied the sparsity from  to  in increments of
.
We did not attempt to compute the exact  for these synthetic
matrices --- our aim was to merely study the speedup with  for different
sparsities, while  and  were held constant at  and 
respectively.
The results are shown in Figure~\ref{fig:sparse}.
Figure~\ref{fig:sparse-convergence-m} depicts the convergence of
 measured as a relative error of the current estimate over the
final estimate.
As can be seen --- for well conditioned matrices --- convergence is quick.
Figure~\ref{fig:sparse-cost-m} shows the relative cost of increasing ; here
the baseline is .
Therefore, the additional cost incurred by increasing  is linear when
all other parameters are held constant.

The results of running Algorithm~\ref{alg1a} on the UFL matrices are shown in
Table~\ref{tbl:ufl}.
The numbers reported for the approximation are the mean and standard deviation
over ten iterations, , and ~\footnote{We experimented with different
 and settled on the smallest values that did not result in loss in
accuracy.}.
The value of  was varied between one and 150 in increments of five to select the best
average accuracy.
The matrices shown in Table~\ref{tbl:ufl} have a nice structure, which lends
itself to nice reorderings and therefore an efficient computation of the Cholesky factorization.
We see that even in such cases, the performance of Algorithm~\ref{alg1a} is
commendable due to its lower
algorithmic complexity; \texttt{ldoor} is the only exception as the
approximation takes longer to compute than the Cholesky factorization.
In the case of \texttt{thermomech\_TC}, we achieve good accuracy while achieving
a 22x speedup.
%
