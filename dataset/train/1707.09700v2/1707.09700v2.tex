


\section{Experiment}
Scene graph generation can be viewed as an intermediate task connecting the object detection and region captioning, which aims at capturing the structural information of an image with a set of pair-wise relationships. Compared to object detection, the scene graph generation measures the feature learning from more aspects. And different from the region captioning, the performance of the scene graph generation model is easier to measure quantitatively and it excludes the influence brought by the different language model implementations. Therefore, the experiment part mainly focuses on this task.

Some explanatory experiments are also done on the object detection and region captioning tasks to show mutual improvements brought by the joint inference across semantic levels.

\subsection{Dataset}
\label{sec:dataset}
All the experiments are done on the Visual Genome~\cite{visual_genome} dataset. The objects and relationships are from the Relationship subset, and the region caption annotations are based on the Region Description subset. The two subsets share the image but target on different tasks.

First, we do some preprocessing on the relationship annotations. We normalize the words in different tenses and then select the top-150 frequent object categories and top-50 predicate categories. Moreover, the object boxes whose shorter edges are smaller than 16 pixels are removed. After preprocessing, there are 95998 images left.

For the remaining 95998 images, we further pre-process the region caption annotations. All the words are changed to lower case. Top-10000 frequent words~(including punctuations) are used to build up the dictionary and all the other words are changed to  token. In addition, all the small regions with shorter edges smaller than 32 are removed. NLTK~\cite{nltk} is used to tokenize the sentence. 

After the two preprocessing steps above, a cleansed dataset containing the annotations of localized objects, phrases and region descriptions are built for our experiments. From the 95998 images in the dataset, 25000 images are sampled as the testing set and the remaining 70998 images are used as the training set. 


\begin{table*}[t]
	\renewcommand{\arraystretch}{1.1}
	\setlength{\tabcolsep}{4pt}
	\small
	\begin{center}
		\begin{tabularx}{1.0\linewidth}{c| cccc | cccccc}
			\hline
			\multirow{2}{*}{ID} & \multirow{2}{*}{Message Passing} & \multirow{2}{*}{Cap. branch} & \multirow{2}{*}{Cap. Supervision} & \multirow{2}{*}{FR-iters} & \multicolumn{2}{c}{PredCls} & \multicolumn{2}{c}{PhrCls} & \multicolumn{2}{c}{SGGen} \\
			& & & & & Rec@50& Rec@100 & Rec@50 & Rec@100 & Rec@50 & Rec@100 \\
			\hline
			1 & -  & - & - & 0 & 49.28 & 52.69 & 7.31& 10.48 & 2.39 & 3.82 \\
			2 & \checkmark & - & - & 1 & 63.12 & 66.41 & 19.30 & 21.82 & 7.73 & 10.51\\
			3 &\checkmark & \checkmark & - & 1 & 63.82 &  67.23& 20.91 & 23.09 & 8.20 & 11.35 \\
			4 &\checkmark & \checkmark & \checkmark & 1 & 66.70 & \textbf{71.02} & 23.42 & 25.68 & 10.23 & 13.89\\
			5 &\checkmark & \checkmark & \checkmark & 2 & \textbf{67.03} & 71.01& \textbf{24.22} & \textbf{26.50} & \textbf{10.72} & \textbf{14.22} \\
			6 &\checkmark & \checkmark & \checkmark & 3 & 66.23 & 70.43& 23.16&  25.28& 10.01& 13.62\\
			\hline
		\end{tabularx}
	\end{center}
	\caption{Ablation studies of the proposed model. \textbf{PredCls} denotes predicate recognition task. \textbf{PhrCls} denotes phrase recognition task. \textbf{SGGen} denotes the scene graph generation task. \textbf{Message passing} denotes whether to add feature refining structure to pass message.  \textbf{Cap. branch} denotes whether to use the caption branch as an extra connection source. \textbf{Cap. Supervision} indicates whether to use region caption annotation as the supervision to guide the learning of the caption branch. \textbf{FR-iters} denotes the number of feature refining iterations.}
	\label{tab:component}
\end{table*}

\subsection{Implementation Details}

\textbf{Model training details} 
Our model is initialized on the ImageNet pretrained VGG-16 network~\cite{VGG}. To reduce the number of parameters, we only use 1024 neurons of the fully-connected layers from the original 4096 ones and then scale up the weights accordingly as initialization. The newly introduced parameters are randomly initialized. We first train RPNs and then jointly train the entire model from the base learning rate 0.01 using SGD with gradients clipping. The parameters of VGG convolutional layers are fixed at first, and then trained with 0.1 times the learning rates of other layers after the first decay of the base learning rate. In addition, there is no weight decay for the language model and the parameters are updated using Adam. 

\textbf{Loss Functions}
For the object branch, we use the cross-entropy loss for the object classification and the smooth L1 loss for the box regression. For the phrase branch, the cross-entropy loss is used for predicting the labels of predicates. For the caption branch, the cross-entropy loss is used for generating the every word of free-form sentences and the smooth L1 loss is used for regressing the corresponding proposals. Three losses are summed up equally. Since every step at feature refining parts is differentiable, BP can be applied for the feature refining part.

\textbf{Mini-batch preparation for training} 
A mini-batch contains one image. After generating proposals with RPN layers, we use 0.7 and 0.75 as the NMS threshold for object proposals and caption proposals respectively and keep at most 2000 boxes after NMS. Then we sample 256 object proposals and 128 caption proposals from each image. As the number of phrase proposals is too large and the positive samples are sparse, we sample 512 with 25\% positive instances. In addition, we assign  to the negative phrase samples,  to the negative objects, and the  to the negative caption proposals.

\textbf{Details for inference.} In testing, we set the NMS threshold to 0.35 and 0.45 for object and caption region proposals. After the graph for the image is constructed,  features from all the sampled proposals are used for refining their features.

\subsection{Evaluation on Scene Graph Generation}
\subsubsection{Experiment settings}
\label{sec:exp_setting}

\textbf{Performance Metric.} Following \cite{visual_relationship}, the \emph{Top-K recall}~(denoted as \emph{Rec@K}) is used as the main performance metric, which is the fraction of the ground truth instances hit in the top- predictions. The reason of using \emph{recall} instead of \emph{mean average precision(mAP)} is that the annotations of the relationships are incomplete. \emph{mAP} will falsely penalize the positive but unlabeled predictions. In our experiment,  \emph{Rec@50} and \emph{Rec@100} will be reported. 

\textbf{Task Settings.} Since scene graph generation involves the classification of the \emph{subject-predicate-object}  triplet and localization of objects. We evaluate our proposed model on three sub-tasks of scene graph generationz proposed in \cite{xu2017scene}:
\begin{itemize}
	\item \textbf{Predicate Recognition} (PredCls): To recognize the relationship between the objects given the ground truth location of object bounding boxes. This task is aimed at examining the model performance on the classification of the predicates alone.

	\item \textbf{Phrase Recognition} (PhrCls): To predict the predicate categories as well as the object categories given the ground-truth location of objects. This task evaluates the model performance on the recognition of both predicates and objects.

	\item \textbf{Scene Graph Generation} (SGGen): To detect objects and recognize their pair-wise relationships. The object is correctly detected if it is correctly classified and its overlap with the ground truth bounding box is larger than 0.5. A relationship is correctly detected if both the  and  are correctly detected and the  is correctly predicted. The location of objects is not provided.
\end{itemize} 




\subsubsection{Comparison with existing works}

We compare our proposed MSDN with the following methods under the three task settings:  (1) The model using Language Prior (\textbf{LP})~\cite{visual_relationship}, which detects objects first and then estimate the categories of predicate using visual features and word embeddings. (2) Iterative Scene Graph Generation~(\textbf{ISGG})~\cite{xu2017scene}, which uses the iterative message passing along the scene graph with a GRU-based feature refining scheme. We have reimplemented their model. The model is trained and tested on the cleansed dataset mentioned in Section \ref{sec:dataset}.  All the methods are based on the VGG-16 model.

\begin{table}[h]
	\small
\begin{center}
		\begin{tabular}{l  l ||  c c c  }
			\multicolumn{2}{c}{Task} & LP~\cite{visual_relationship}  & ISGG~\cite{xu2017scene}  & Ours  \\ \hline\hline
			\multirow{2}{*}{PredCls}     &R@50  & 26.67  & 58.17       &\textbf{67.03} \\
			&R@100  & 33.32 & 62.74        &  \textbf{71.01}      \\\hline
			\multirow{2}{*}{PhrCls}      &R@50& 10.11   & 18.77       &  \textbf{24.34}      \\
			&R@100 &  12.64  & 20.23      &  \textbf{26.50}\\\hline
			\multirow{2}{*}{SGGen}      &R@50& 0.08    & 7.09        &  \textbf{10.72}     \\
			&R@100 & 0.14    & 9.91      &  \textbf{14.22}      \\
			\hline
		\end{tabular}
\end{center}
	\caption{Evaluation on the Visual Genome
		dataset~\cite{visual_genome}. We compare our proposed model with existing works on the three tasks illustrated in Sec.~\ref{sec:exp_setting}. The result of LP is reported in~\cite{xu2017scene}. ISGG is reimplemented by ourselves and evaluated on our cleansed dataset. }
	\label{table:vg_eval}
\end{table}


From the results in Table~\ref{table:vg_eval}, we can see that our proposed model performs better than the existing works. Compared to the ISGG model~\cite{xu2017scene}, our model introduces the caption branch to provide more context information for phrase recognition. In addition, our model passes message as residual, which makes the model easier to train. 


\subsubsection{Component Analysis}

There are many components that influence the performance of MSDN. Table~\ref{tab:component} shows our investigation on the performance of different settings of MSDN on the Visual Genome dataset~\cite{visual_genome}.

\textbf{Message passing.} Model 1 in Table~\ref{tab:component} is the baseline that does not use message passing to refine features and does not have the branch for caption. Model 2 is based on Model 1 and passes message between related object and phrase nodes. By comparing Model 1 and 2 in Table~\ref{tab:component}, we can see that passing message with the feature refining structure  proposed in Sec.~\ref{sec:feature_refining} can help to leverage the connection between the objects and phrases, which significantly improves the model performance by  on SGGen task. 

\textbf{Caption region branch.} Based on Model 2, Model 3 only has an extra caption branch without the caption supervision. We remove the LSTM language model in Fig.~\ref{fig:hdn} and only use the caption branch as extra context information source. Model 3 has  gain when compared with Model 2.  This improvement is more likely to come from the more parameters introduced by the caption branch.

\textbf{Region Caption Supervision.} Model 4 further uses additional supervision of region caption sentences for the region caption branch. It outperforms Model 3 by . The improvement mainly comes from the complementary features learned with additional information.  Supervision helps the caption branch learn it own specialized features, which can provided extra information for other branches. Compared to the object and predicate categories, region captions provide another way to understand the image. 

\textbf{The number of feature refining iterations.} Model 46 are different in the number of iterations in message passing. By comparing Model 46, the results show that two iterations may be the optimal settings for the scene graph generation. Compared to Model 4 with one iteration, Model 5 with two iterations constructs the connection between captions and objects indirectly, which brings  gain. However, more iterations make the model harder to train. Therefore, when we refine the features for three iterations, the training issue suppress the gain brought by the better feature refining. Therefore, the performance of Model 6 will deteriorate by .    


\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=\linewidth]{qualitative.pdf}
	\end{center}
	\caption{Qualitative results for region captioning. The most salient regions with captions are shown~(yellow boxes). We also show several relationships that are connected to the captions. The connection is built by our proposed dynamic graph generation in Sec.~\ref{Sec:GraphLayer}. }
	\label{fig:result}
\end{figure*}





\subsection{Evaluation on Object Detection}\label{sec:object_detection}

We further evaluate our proposed MSDN on object detection task. 

\textbf{Setup.} We directly use the objects within the dataset prepared in \ref{sec:exp_setting}. All the objects have at least one relationship with other objects. We adopt the mean Average Precision~(mAP) metric as one evaluation metric. In addition, as most of the objects are small, poor localization of the objects highly influences the mAP metrics, we also report the accuracy of the object classification with the ground truth bounding boxes given.

We compare our proposed MSDN with Faster R-CNN~\cite{faster_rcnn}~(\textbf{FRCNN}) trained on the same dataset. In addition, to check whether the additional supervision can benefit the feature learning of convolutional layers, we also show the results for the model with the feature refining structure removed~(\textbf{Baseline-3-bran.}) and use the object branch for object detection~(like the model 1 in \ref{tab:component}). 

\begin{table}[t]
	\small
	\vspace*{-5pt}
	\begin{center}
		\begin{tabular}{l || c c c  }
			\hline 
			\textbf{Object Det.} & FRCNN~\cite{faster_rcnn} &  Baseline-3-bran. & Ours \\\hline
			mean AP(\%)    & 6.72  & 6.70  &     \textbf{7.43}   \\
			Acc. Top-1(\%)   &  53.57  &  53.14 &    \textbf{61.12}     \\
			Acc. Top-5(\%)   &  83.50 &  83.25 &  \textbf{89.86}       \\\hline\hline
			\textbf{Region Caption} & Baseline &  Baseline-3-bran. & Ours \\\hline
			AP~\cite{densecap}(\%)   & 4.41 & 4.28 &  \textbf{5.39}       \\
			\hline
		\end{tabular}
		\vspace*{-3pt}
	\end{center}
	\caption{Object detection and region captioning results evaluated on Visual Genome
		dataset~\cite{visual_genome}. \emph{Baseline-3-bran.} has 3 separate branches without message passing.}
	\label{table:other_task}
\end{table}


\textbf{Results.}  Since the Visual Genome Dataset has many object classes that are small and hard to detect, the mAP is small for all approaches. Nevertheless, our model outperforms Faster R-CNN and baseline model with three separated branches on the Visual Genome Dataset, because our model introduces more context information from phrases and captions~( when trained with more than two iterations) to the objects as complementary source, which serves as visual cues to help recognize objects. 



\subsection{Evaluation on Region Captioning}
We further evaluate our model on the region caption task. 

\textbf{Setup.}  We adopt the evaluation metric proposed by Johnson~\etal in \cite{densecap} for region captioning. It measures the mean Average Precision across a range of thresholds for both localization and language accuracy. The Meteor scores~\cite{banerjee2005meteor} are used as the language metric, because it is highly correlated with human judgments. During the evaluation, the ground truth bounding regions are merged as one region with several reference annotations if they are heavily overlapped with each other~(based on IOU with threshold of 0.7).

To make the model comparable, we re-implement the main part of Densecap~\cite{densecap} using Faster R-CNN~\cite{faster_rcnn} pipeline based on VGG-Net~(\textbf{Baseline}) and use the same language model as our proposed model. Our implementation performs comparably with the original Densecap under same settings~(4.41\% vs 4.62\% evaluated on our cleansed dataset). In addition, similar to \ref{sec:object_detection}, we also include another baseline model with three separated branch without message passing~(\textbf{Baseline-3-bran.}).  All the models are evaluated on our cleansed dataset. 

\textbf{Quantitative Results.}  
From Table.~\ref{table:other_task}, we can see that, our proposed model outperforms the other two baseline models. Because we have excluded the influence brought by the number of parameters and utilized the same language model for them, the gain is obtained by the extra information introduced through the message passing. And the messages passed to the region come from the scene graph composed by the objects and their relationships. Such structural information can help the region branch infer the content of the region. In addition, by comparing the two baseline models, simply introducing extra supervision will not improve the accuracy.  


\textbf{Qualitative Results.} 
Region captioning results with the highest score are shown in Figure~\ref{fig:result}. We also show the objects and their relationships that are connected to the captions through the dynamic graph. We can see that the region captioning result is highly correlated to the scene graph. We also observe failure case (bottom right in Figure~\ref{fig:result}, where the misclassification of objects and relationships would mislead the caption branch to recognize the region as \emph{a large pile of luggage}. 




