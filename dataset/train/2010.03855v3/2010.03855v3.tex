

In this section, we provide the experimental setups, competing methods and performance evaluation of relational captioning with both quantitative and qualitative results, so that we empirically show the benefit and potential of the proposed relational captioning task and the proposed method.


\begin{table}[t]
    \centering
    \vspace{-0mm}
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{l  cccc } 
        \toprule
			Model & Output of RPN & Input of LSTM & LSTM & POS prediction\\
			\midrule
			\texttt{Direct Union} 	& Union region 	& U		& 	Single	& 	$\times$	\\
			\texttt{Union} 			& Object 		& U		& 	Single	& 	$\times$	\\
			\texttt{Union+Coord.} 	& Object 		& U + C		& 	Single	& 	$\times$	\\
			\texttt{Subj+Obj}  		& Object 		& S + O		& 	Single	& 	$\times$	\\
			\texttt{Subj+Obj+Coord.}& Object 		& S + O + C		& 	Single	& 	$\times$	\\
			\texttt{Subj+Obj+Union} & Object 		& S + O + U		& 	Single	& 	$\times$	\\
			\texttt{Union (w/MTL)} 	& Object 		& U		& 	Single	& 	$\bigcirc$	\\
			\texttt{Subj+Obj+Coord.(w/MTL)}& Object 		& S + O + C		& 	Single	& 	$\bigcirc$	\\
			\texttt{Subj+Obj+Union (w/MTL)} & Object 		& S + O + U		& 	Single	& 	$\bigcirc$	\\
			\texttt{Union+Union+Union (w/MTL)} & Object 		& U + U + U		& 	Triple	& 	$\bigcirc$	\\
			\texttt{TSNet} 		& Object 		& S $|$ O $|$ U	+ C	& 	Triple	& 	$\times$	\\
			\texttt{MTTSNet} 		& Object 		& S $|$ O $|$ U + C		& 	Triple	& 	$\bigcirc$	\\
            \bottomrule
		\end{tabular}
    \hspace*{0mm}}
    \vspace{0mm}
    \caption{Comparison of model configurations. 
    `$|$' and `+' indicate separation and concatenation of input respectively.
    }
	\vspace{-8mm}
	\label{table:baselineconf}	
\end{table}

\subsection{Experimental Setups}
\label{sec:exp_setup}
\noindent\textbf{Implementation details.}
We use Torch7~\cite{collobert2011torch7} to implement our model.
For the {backbone} visual feature extraction, we use VGG-16~\cite{simonyan2014very} and initialize with the weights pre-trained on ImageNet~\cite{russakovsky2015imagenet}.
We pre-train the RPN on the Visual Genome (VG) dense captioning data~\cite{krishna2017visual}. 
For sequence modeling, we set the {dimension} of 
all the LSTM hidden layers
to be 512.
A training batch contains an image that is resized to have a longer side of 720 pixels. 
We use Adam optimizer~\cite{ba2015adam} for training (learning rate $lr {=} 10^{-6}$, $b1{=}0.9$, $b2{=}0.999$).
For the RPN, we use 12 anchor boxes for generating the anchor positions in each cell of the feature map, and 128 boxes are sampled in each forward pass of training.
We use Titan X GPU, and it takes about four days for a model to convergence when training on our relational captioning dataset.



We use the setting for the region proposals similar to that of \cite{johnson2016densecap} for fairness.
For training, a region is positive if it has 
{at least 0.7 IoU ratio with a corresponding ground truth region,}
and 
{a region is negative if its IoUs are less than 0.3 with all ground truth regions.}
For evaluation, after non-maximum suppression (NMS) based on the predicted proposal confidences, 50 confident bounding boxes are selected.  
{We can additionally reduce box pair predictions by discarding the pairs that produce captions with low 
confidence scores.
Caption confidence scores can be computed by sequentially multiplying all of the generated word probabilities.}






\begin{table}[t]
\vspace{0mm}
    \centering
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{l ccc}\toprule
										&	mAP (\%) 	&Img-Lv. Recall	&METEOR\\\midrule
            \texttt{Direct Union}		&		--	&	17.32	&11.02\\\midrule
            \texttt{Union}				&	0.57	&	25.61	&12.28\\
            \texttt{Union+Coord.}		&	0.56 	&	27.14 	&13.71\\
            \texttt{Subj+Obj}			&	0.51 	& 	28.53	&13.32\\
           \texttt{Subj+Obj+Coord.}		&	0.57 	&	30.53 	&14.85\\
           \texttt{Subj+Obj+Union}		& 0.59		& 	30.48	& 15.21\\
           \texttt{\textbf{TSNet (Ours)} }	&	\textbf{0.61}	& 	\textbf{32.36}		&\textbf{16.09}\\
           \midrule
           \texttt{Union (w/MTL)}		&	0.61	&	26.97 	&12.75\\
           \texttt{Subj+Obj+Coord (w/MTL)}		&	0.63 	&	31.15 	&15.31\\
           \texttt{Subj+Obj+Union (w/MTL)}		& 0.64	& 	31.63		& 16.63\\
\texttt{Union+Union+Union (w/MTL)}		& 0.58		& 	34.11	& 14.69\\
            \texttt{\textbf{MTTSNet (Ours)} } &{0.88}&{34.27}	&\textbf{18.73}\\
\texttt{\textbf{MTTSNet (Ours) + REM}~\cite{wang2018non}}&\textbf{1.12}&\textbf{45.96}	&{18.44}\\
\midrule
            \texttt{\textbf{MTTSNet (Ours) + REM (R)}}&{1.48}&{48.56}	&{19.48}\\
            \midrule
            \texttt{Neural Motifs}~\cite{zellers2018neural} &0.25 & 29.90&15.34\\
\bottomrule
		\end{tabular}


        }
        \vspace{-0mm}
	\caption{Ablation study for {the} relational dense captioning task on {the} relational captioning dataset. 
	{The second and third row sections (2-7 and 8-12th rows) show the comparison of the baselines with and without {the} POS classification (\texttt{w/MTL})}.
	In the last row, we show the performance of the state-of-the-art scene graph generator, \texttt{Neural Motifs}~\cite{zellers2018neural}.
	{Union+Union+Union denotes the results of using three LSTMs with only union features 
as LSTM inputs, (R) indicates ResNet-50~\cite{he2016deep} as a backbone network instead of VGG-16.} 
    }
    \vspace{-4mm}
	\label{table:captioning}	
\end{table}

\noindent\textbf{Relational captioning dataset.}
{Since there is no existing dataset for the relational captioning task, we construct a dataset by utilizing VG relationship dataset version 1.2~\cite{krishna2017visual} which consists of 85,200 images with 75,456/4,871/4,873 splits for train/validation/test sets respectively.
We tokenize the relational expressions {into word level tokens}, 
and we assign the POS class from the triplet association for each word. }



However, the VG relationship 
{dataset has a}
limited diversity {of} the words used.
{Therefore, na\"ively converting such VRD dataset to a captioning dataset is not desirable, in that the captions generated from a trained model on the dataset tends to be too simple (\eg, ``building-has-window'').}
This limited data restricts the expressiveness of the model.
To examine the diverse expressions of our relational captioner, we construct our relational captioning dataset to have more natural sentences with richer expressions.

Through observation, {we noticed that labels in the VG relationship dataset lack \emph{attributes} describing the subject and object, which are perhaps what enriches the expressiveness of sentences the most}.
{We enrich the dataset by leveraging the {VG \emph{attribute} dataset}~\cite{krishna2017visual}}.
The specific procedure of {this attribute enrichment} is described in Appendix.
After this enrichment, we obtain 15,595 {different} vocabularies for our relational captioning dataset, which was 11,447 {different} vocabularies before this process. 





{We train our model with this dataset, and report its result in this section.}
In {the following subsections}, we {evaluate in multiple views including}
a holistic image captioning performance and various analysis such as comparison with scene graph generation.














\subsection{Relational Dense Captioning: Ablation Study}
\label{sec:exp_relation}


\noindent\textbf{Baselines.} Since no direct {related} work for relational captioning exists, we implement several baselines by modifying the most relevant methods, which facilitate our ablation study.
All the configurations are summarized in \Tref{table:baselineconf} and described as follows.




{\setdefaultleftmargin{3mm}{}{}{}{}{}
\begin{itemize}
\item \texttt{Direct Union} has the same architecture with \texttt{DenseCap}~\cite{johnson2016densecap}, but of which RPN is trained to directly predict union regions.
{A union region is converted to a {512-dimensional} region code, followed by a single LSTM to generate a relational caption.}\vspace{0mm}

\item \texttt{Union} also resembles \texttt{DenseCap}~\cite{johnson2016densecap} and \texttt{Direct union}, but its RPN predicts individual object regions. 
The object regions are paired as (subject, object), and then {only} a union region from each pair is fed to a single LSTM for captioning.
Also, we implement two additional variants: \texttt{Union (w/MTL)} additionally predicts the POS classification task, and 
\texttt{Union+Coord.} appends the geometric feature to the region code of the union.
{Lastly, to match the number of parameters with our \texttt{MTTSNet}, we additionally introduce the \texttt{Union+Union+Union} baseline with the triple-stream architecture, which only takes the union region as input.}
\vspace{0mm}

\item \texttt{Subj+Obj} and \texttt{Subj+Obj+Union} models use the concatenated region code of (subject, object) and (subject, object, union) respectively and pass them through a single LSTM (an early fusion approach).
Also, \texttt{Subj+Obj+Coord.} uses the geometric feature instead of the region code of the union.
Moreover, we {evaluate the baselines}, \texttt{Subj+Obj+\{Union,Coord\}} {again by adding} the POS classification (\ie, MTL loss).
\vspace{0mm}

\item \texttt{TSNet} denotes the proposed triple-stream LSTM model without a branch for the POS classifier.
Each stream takes the region codes of (subject, object, union{+}coord.) separately. 
\texttt{MTTSNet} (\ie, \texttt{TSNet}{+}POS) denotes the {multi-task triple-stream network with the POS classifier, and} \texttt{MTTSNet+REM}  denotes the model {combined with} the REM.


\end{itemize}
}
 


\begin{table}[t]
\centering
    \resizebox{1\linewidth}{!}{\begin{tabular}{l cc cc}\toprule
													&		Recall	&	METEOR		&\#Caption	&Caption/Box	\\\midrule
			Image Cap. (\texttt{Show\&Tell})~\cite{vinyals2015show} 	            &		23.55	&	8.66		&	1		&	N/A		\\
			Image Cap. (\texttt{Show\&Tell})~\cite{vinyals2015show}$^\dagger$ 	&		23.81	&	9.46		&	10		&	N/A		\\
            Image Cap. (\texttt{SCST})~\cite{rennie2017self} 	                &		24.04	&	14.00		&	1		&	N/A		\\
            Image Cap. (\texttt{SCST})~\cite{rennie2017self}$^\dagger$ 	        &		24.17	&	13.87		&	10		&	N/A		\\
            Image Cap. (\texttt{RFNet})~\cite{jiang2018recurrent} 	            &		24.91	&	17.78		&	1		&	N/A		\\
            Image Cap. (\texttt{RFNet})~\cite{jiang2018recurrent}$^\dagger$ 	    &		25.26	&	17.83		&	10		&	N/A		\\
            \midrule
			Dense Cap. (\texttt{DenseCap})~\cite{johnson2016densecap}    &		42.63	&	19.57	&		9.16	&	1		\\
			Dense Cap. (\texttt{TLSTM})~\cite{Yang_2017_CVPR}            &		43.15	&	20.48	&	9.24	&	1		\\
			\midrule
			Relational Cap. (\texttt{Union})	&		38.88	&	18.22	&	85.84	&	9.18	\\
			Relational Cap. (\textbf{\texttt{MTTSNet}})		&{46.78} &{21.87}	&{89.32}&{9.36}\\
			Relational Cap. (\textbf{\texttt{MTTSNet+REM}})		&\textbf{56.52} &\textbf{22.03}	&{80.95}&{9.24}\\
			\midrule
			Relational Cap. (\textbf{\texttt{MTTSNet+REM} (R)})		&{59.71} &{23.27}	&{85.37}&{9.26}\\
			\midrule
			Relational Cap. (\texttt{Union})$^{(GT)}$	&		41.64	&	18.90	&	\multirow{3}{*}{83.44}&\multirow{3}{*}{9.30}	\\
			Relational Cap. ({\texttt{MTTSNet}})$^{(GT)}$		&{48.50} &{21.63}	\\
			Relational Cap. ({\texttt{MTTSNet+REM}})$^{(GT)}$		&{56.62} &{22.50}	\\\bottomrule
		\end{tabular}
    }
    \vspace{0mm}
	\caption{Comparisons of the holistic level image captioning. We compare the results of the relational captioners with those of three image captioners~\cite{jiang2018recurrent,rennie2017self,vinyals2015show} and two dense captioners~\cite{johnson2016densecap,Yang_2017_CVPR}.
	{To compare with stronger baselines, we modify the image captioners 
by deploying a stochastic sampling.
	We annotate the modified versions with stochastic sampling with $\dagger$.
	We annotate $(GT)$ for the methods that replace RPN with ground truth bounding boxes; thus, those represent proxy upper bounds of performance.
	{(R) indicates ResNet-50~\cite{he2016deep} as a backbone network instead of VGG-16.}
}
\vspace{-4mm}
	}
	\label{table:recall}	
\end{table}



\begin{figure*}[t]
\vspace{-2mm}
\centering
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative3_2}}
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative8}}\\
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative2}}{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative7}}\\
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative1_2}}
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative5}}\\
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative4}}
{\includegraphics[height=0.24\linewidth,keepaspectratio]{Figures/qualitative6}}\\
\vspace{-2mm}
\caption{Example captions and region generated by the proposed model on Visual Genome test images. 
The region detection and caption results are obtained by the proposed model from Visual Genome test images. 
   We compare our result with the image captioner~\cite{vinyals2015show} and the dense captioner~\cite{johnson2016densecap} in order to contrast the amount of information and diversity.
   \vspace{-2mm}}
\label{fig:qualitative}
\end{figure*}


\noindent\textbf{Evaluation metrics.}
Motivated by the evaluation metric suggested for the dense captioning task by Johnson~\etal\cite{johnson2016densecap},
we suggest a modified evaluation metric for the relational dense captioning.
{
Firstly, to assess the caption quality, we measure the average METEOR score~\cite{denkowski2014meteor} for predicted captions (noted as METEOR). 
Also, we use a mean Average Precision (mAP) similar to  Johnson~\etal which measures both localization and language accuracy.
For language accuracy,
we measure METEOR score with thresholds $\{ 0, 0.05, 0.1 0.15, 0.2, 0.25\}$, 
and we use IOU thresholds $\{0.2, 0.3, 0.4, 0.5, 0.6\}$ for localization accuracy.}
The AP values, obtained by all the pairwise combinations of language and localization thresholds, are averaged to get the final mAP score.
The major difference of our metric from that of Johnson~\etal is that, for the localization AP, we measure for both the subject and object bounding boxes with respective ground truths. 
In particular, we only consider the samples with IOUs of both the subject and object bounding boxes greater than the localization threshold, which yields a \emph{more challenging metric}.
For all cases, we use percentage as the unit of metric.



In addition, we suggest another metric, called ``image-level (Img-Lv.) recall.'' 
This measures the caption quality at the holistic image level by considering the bag of all captions generated from an image as a single prediction.
This metric evaluates the diversity of the produced representations by the model for a given image.
Specifically, with the aforementioned language thresholds of METEOR, we measure the recall of the predicted captions over about 20 ground truth captions.







\begin{figure*}[t]
\vspace{-2mm}
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}[c]{c}
    \subfigure[]{\includegraphics[width=0.35\linewidth,keepaspectratio]{Figures/scene-graph1_1}}				
    \subfigure[]{\includegraphics[width=0.35\linewidth,keepaspectratio]{Figures/scene-graph2_1}}\\
    \subfigure[]{\includegraphics[width=0.35\linewidth,keepaspectratio]{Figures/scene-graph3_1}}
    \subfigure[]{\includegraphics[width=0.35\linewidth,keepaspectratio]{Figures/scene-graph4_1}}\\
    \end{tabular}
}
	\vspace{-4mm}
	\caption{ Results of generating ``caption graph'' from our relational captioniner. In order to compare the diversity of the outputs, we also show the result of the scene graph generator, \texttt{Neural Motifs}~\cite{zellers2018neural}.\vspace{-2mm}} 
	\label{fig:scene-graph}
\end{figure*}




\begin{figure}[b]
\vspace{-4mm}
\centering
 \resizebox{1.0\linewidth}{!}{{
  \subfigure[]{\includegraphics[width=0.3\linewidth]{Figures/multibox1_POS}}
  \subfigure[]{\includegraphics[width=0.3\linewidth]{Figures/multibox2_POS}}
   \subfigure[]{\includegraphics[width=0.3\linewidth]{Figures/multibox3_POS}}
   }
  }
  \vspace{-4mm}
   \caption{Examples of different captions predicted from relational captioning by (a) changing objects, (b) changing subjects, and (c) switching the subject and object. Our model shows different predictions from different subject and object pairs.\vspace{-0mm}}
\label{fig:multibox}
\end{figure}


\noindent\textbf{Results.}
\Tref{table:captioning} compares the performance of {various methods for} the relational dense captioning task on {the} relational captioning dataset. 
{To compare with a different representation of relationship,} we additionally compare with the state-of-the-art scene graph generator, \texttt{Neural Motifs}~\cite{zellers2018neural}.
Due to the different output structure, we compare with \texttt{Neural Motifs} trained with the supervision for relationship detection.
Similar to the setup in \cite{johnson2016densecap}, we fix the number of region proposals after NMS to 50 for all methods for a fair comparison.

Within the second row section (2-7th rows) of \Tref{table:captioning}, our \texttt{TSNet} shows the best result suggesting that the triple-stream component alone is a sufficiently strong baseline over the others.
On top of \texttt{TSNet}, applying the MTL loss (\ie, \texttt{MTTSNet}) improves overall performance, and especially improves mAP, where the detection accuracy is
dominantly improved compared to 
the other metrics.
This shows that \emph{triple-stream LSTM} is the key module that most leverages the MTL loss across other early fusion approaches (see the third {row section} of the table).
{Also, compared to \texttt{Union+Union+Union~(w/MTL)}, our \texttt{MTTSNet} shows much higher performance, which validates that the performance improvement by our method is not simply due to the increased number of the model parameters.}
Moreover, by adding REM to our late fusion method, \texttt{MTTSNet}, we have achieved further improvements in both mAP and Img-Lv. 
Recall scores (more strongly on Img-Lv. Recall).
As another factor, {we can see from \Tref{table:captioning} that the relative spatial information (\texttt{Coord.}) and union feature information (\texttt{Union}) improves the results.} 
This is because 
the union feature itself preserves the spatial information to some extent from the $7\times 7$ grid form of its activation. 
Also, the relational captioner baselines including our \texttt{TSNet} and \texttt{MTTSNet} perform favorably against \texttt{Neural Motifs} in all metrics.
Note that handling free-form language generation which we aim to achieve is more challenging than the simple triplet prediction of scene graph generation.




















\begin{table}[b]
	\centering 
    \resizebox{0.9\linewidth}{!}{\begin{tabular}{l cccc}\toprule
    &		R@1		&	R@5	&	R@10		& 	Med	\\\midrule
    Image Cap. (\texttt{Full Image RNN})~\cite{karpathy2015deep}	    &		0.09	&	0.27	&	0.36	&	14	\\
    Dense Cap. (\texttt{Region RNN})~\cite{girshick2015fast}          &		0.19	&	0.47	&	0.64	&	6	\\
    Dense Cap. (\texttt{DenseCap})~\cite{johnson2016densecap}		    &		0.25	&	0.48	&	0.61	&	6	\\
    Dense Cap. (\texttt{TLSTM})~\cite{Yang_2017_CVPR}		&		0.27	&	0.52	&	0.67    &	5	\\
    \midrule
    Relational Cap. (\textbf{\texttt{MTTSNet}})		&{0.29} &{0.60}	&{0.73}&{4}\\
    Relational Cap. (\textbf{\texttt{MTTSNet+REM}})		&\textbf{0.32} &\textbf{0.64}	&\textbf{0.79}&\textbf{3}\\\midrule
    Random chance & 0.001 & 0.005	& 0.01 & - \\\bottomrule
    \end{tabular}    }
    \vspace{1mm}
	\caption{Sentence based image retrieval performance {comparison across different representations.}
We evaluate ranking using recall at $k$ ($R@K$, higher is better) and the median rank of the target image (Med, lower is better). 
	{The random chance performance 
is  provided for reference.
	{We compare with \texttt{TLSTM} in addition to the baselines (\texttt{Full Image RNN}, \texttt{Region RNN}, \texttt{DenseCap}) suggested in Johnson~\etal\cite{johnson2016densecap}.}
	}
	\vspace{-0mm}}
    \label{table:retrieval}	
\end{table}





\begin{figure*}[t]
	\vspace{-2mm}
	\centering 
\includegraphics[width=1\linewidth,keepaspectratio]{Figures/retrieval2}
    \vspace{-6mm}
	\caption{Sentence based image and region-pair retrieval results on Visual Genome test images. The retrieved results are shown in the {ranked} order.
    \vspace{-0mm}}
	\label{fig:retrieval}
\end{figure*}






\subsection{{Comparison with} Holistic Image Captioning}
We also compare our approach with other image captioning frameworks, \emph{Image Captioner} {(\texttt{Show\&Tell}~\cite{vinyals2015show}, \texttt{SCST}~\cite{rennie2017self}, and \texttt{RFNet}~\cite{jiang2018recurrent}}), and \emph{Dense Captioner}~(\texttt{DenseCap}~\cite{johnson2016densecap} {and \texttt{TLSTM}~\cite{Yang_2017_CVPR}}) in a holistic image description perspective.
To measure the performance of \emph{holistic image-level} captioning for dense captioning methods, we use Img-Lv. Recall metric {defined in the previous section}.
We compare them with two relational dense captioning methods, \texttt{Union} and \texttt{MTTSNet} ({as well as \texttt{+REM}}), denoted as \emph{Relational Captioner}.
For a fair comparison, for \emph{Dense} and \emph{Relational Captioner}, we adjust the number of region proposals after NMS to be similar, which is different from the setting in the previous section which fixes the number of proposals before NMS.
{For fair comparison with the \emph{Image Captioner}, in addition to  {the typical selection of words according to maximum probabilities in} 
caption generation,
we introduce another baselines using} a stochastic sampling (probabilistically selecting a word 
{proportional to}
{the probabilities of words} from a model) to 
{allow diverse caption generation from the LSTM.}
We generate 10 captions from {the stochastic variant image captioners} {in order to match the number of captions between \emph{Image Captioner} and \emph{Dense Captioner}}.
{Finally, in order to isolate the performance of the caption generation and the box localization modules, we measure the captioning performance by setting the bounding boxes as the ground truth boxes. 
We annotate such variant of relational captioners with $(GT)$.}









\Tref{table:recall} {compares} the image-level recall, METEOR, and additional quantities. 
\emph{\#Caption} denotes the average number of captions generated from an input image and \emph{Caption/Box} denotes the average ratio of the number of captions generated and the number of boxes remaining after NMS.
Therefore, \emph{Caption/Box} demonstrates how many captions can be generated given the same number of boxes generated after NMS.
By virtue of multiple captions per image from multiple boxes, the \emph{Dense Captioner} is able to achieve higher performance than {all  the} \emph{Image Captioner}s. 
{While {the} stochastic sampling {methods slightly improve} 
image captioning performance in terms of recall, 
the performance is still far lower than \emph{Dense Captioner}s or \emph{Relational Captioner}s by a large margin,
{as the diversity of an image captioner's output is still very limited by its inherent design.}
}
Compared with the \emph{Dense Captioner}s, \texttt{MTTSNet} as a \emph{Relational Captioner} can generate an even larger number of captions, given the same number of boxes.
Hence, as a result of learning to generate diverse captions, the \texttt{MTTSNet} achieves higher recall and METEOR.
{\texttt{TLSTM}~\cite{Yang_2017_CVPR} 
improves the performance 
of
\texttt{DenseCap}~\cite{johnson2016densecap} 
due to a better representational power, but the performance is still lower than that of \texttt{MTTSNet}.}
Comparing to
\texttt{Union}, we can see that it is difficult to obtain better captions than \emph{Dense Captioner} by only learning to use the union of subject and object boxes, despite having a larger number of captions.
{Adding \texttt{REM} {to our \texttt{MTTSNet},} further 
improves
the performance in both the Recall and the METEOR score.}
{In addition, even when setting the bounding boxes as the ground truth bounding boxes, by virtue of the more powerful language module, \texttt{MTTSNet} (especially \texttt{MTTSNet+REM}) shows favorable performance compared to \texttt{Union}.}



We show prediction examples of our relational captioning model in \Fref{fig:qualitative} {along with the comparisons against the traditional frameworks, image captioner~\cite{vinyals2015show} and dense captioner~\cite{johnson2016densecap}.}
Our model is able to generate rich and diverse captions for an image, {compared to other paradigms.}
While the dense captioner is able to generate diverse descriptions than an image captioner by virtue of {localized} regions, our model can generate an even more number of captions from the combination of the bounding boxes.





\Fref{fig:multibox} shows caption prediction examples for multiple box pair combinations. 
{Based on the output of {the} POS predictor, we color the words of the caption as (red, green, blue) for (\texttt{subj}-\texttt{pred}-\texttt{obj}) respectively.}
We note that, while the traditional dense captioning simply takes a single region as input and predicts one dominant description, in 
our framework, different captions can be obtained from different subject and object pairs. 
In addition, one can see that the predicted POS is correctly aligned with the words in the generated captions. 
Although the POS classification is not our target task, for completeness, we measure the accuracy of the \texttt{MTTSNet} POS estimation by comparing it with 
the ground truth POS, which 
is 89.7\%.
{The detailed accuracies for subject, predicate, and object are 91.6\%, 86.5\%, and 90.9\%, respectively.}






\begin{figure*}[t] 
	\vspace{-2mm}
	\centering
	\includegraphics[width=1.0\linewidth,keepaspectratio]{Figures/VRD_comparison}
	\caption{Qualitative comparison with visual relationship detection model~\cite{lu2016visual}. The proposed relational captioning model is able to provide more detailed information than the traditional relationship detection model.\vspace{-0mm}} 
	\label{fig:VRD_comparison}
\end{figure*}





 








\begin{table}[t]
\resizebox{1.0\linewidth}{!}{\begin{tabular}{l ccc}\toprule
&	mAP (\%)	&	Img-Lv. Recall 	& METEOR\\\midrule

\texttt{Direct Union}&	--	&	54.51	&25.53\\\midrule
\texttt{Union} &	1.66	& 	54.30	&24.82\\	
\texttt{Union+Coord.}				&{1.90}	& 	64.11		&30.81\\	
\texttt{Subj+Obj}					&1.90		& 	55.06	& 25.09\\
\texttt{Subj+Obj+Coord.}				&1.68	& 	68.33	& 33.45\\
\texttt{Subj+Obj+Union}			&{1.94}	& 	{68.32}	& {33.77}\\
\texttt{\textbf{TSNet (Ours)} }		&\textbf{1.99}	&	\textbf{68.44} 		&\textbf{34.49}\\
\midrule
\texttt{Union (w/MTL)}					&1.70		& 	66.39	&31.62\\
\texttt{Subj+Obj+Coord (w/MTL)}		&	1.93 	&	68.80 	&33.49\\
\texttt{Subj+Obj+Union (w/MTL)}		& {2.17}	& 	65.04	& 32.25\\
\texttt{\textbf{MTTSNet (Ours)}}			&{2.18}& 	{71.44}	&{35.47}\\
\texttt{\textbf{MTTSNet (Ours)+REM}}			&\textbf{2.21}& 	\textbf{73.36}	&\textbf{35.65}\\
\midrule
\texttt{\textbf{MTTSNet (Ours)+REM (R)}}			&{2.33}& 	{77.44}	&{37.63}\\
\midrule
\texttt{Language Prior}~\cite{lu2016visual} &2.13 & 46.60&28.12 \\
\texttt{Shuffle-Then-Assemble}~\cite{yang2018shuffle}& 2.20 & 69.98&29.50 \\
\bottomrule
\end{tabular}
}
\vspace{0mm}
\caption{{Ablation study on the relational dense captioning task with the VRD dataset. {Our \texttt{TSNet} and \texttt{MTTSNet} ({both with and without \texttt{+REM}}) show top
{performance} among the relational captioning models.
{(R) indicates ResNet-50~\cite{he2016deep} as a backbone network instead of VGG-16.}
In addition, \texttt{MTTSNet} ({both with and without \texttt{+REM}}) shows favorable performance against {the} VRD models~\cite{lu2016visual,yang2018shuffle} with a noticeable
margin.}} \vspace{-6mm} }
\label{table:ablation_VRD}
\end{table}





\subsection{Comparison with Scene Graph}
{
Motivated by scene graph, which is derived from the VRD task,
we extend to a new type of a scene graph, which we call ``caption graph.'' 
\Fref{fig:scene-graph} shows the caption graphs generated from our \texttt{MTTSNet} as well as the scene graphs from \texttt{Neural Motifs}~\cite{zellers2018neural}.
{For caption graph, we follow the same procedure with \texttt{Neural Motifs}, but replace the relationship detection network {with} our \texttt{MTTSNet}.}
In both methods, we use ground truth bounding boxes 
for fair comparison.








By virtue of being free form, our caption graph can have richer expression and information including attributes, whereas the traditional scene graph is limited to a closed set of the \texttt{subj-pred-obj} triplet.
For example, in \Fref{fig:scene-graph}-(b,d), given the same object ``person,'' our model is able to distinguish the fine-grained category (\ie, man vs boy and man vs woman).
In addition, our model can provide more status information about the object (\eg, standing, black), by virtue of the attribute contained in our relational captioning data.
Most importantly, the scene graph can contain unnatural relationships (\eg, tree-on-tree in \Fref{fig:scene-graph}-(c)), because the back-end relationship detection methods, \eg, \cite{zellers2018neural}, predict object classes independently.
In contrast, by predicting the full sentence for every object pair, {the} relational captioner can assign a more appropriate word with attributes for an object by considering the relations, \eg, ``Green leaf on a tree.''



Lastly, our model is able to assign different words for the same object by considering the context (the man vs baseball player in \Fref{fig:scene-graph}-(d)), whereas the scene graph generator can only assign one most likely class (man).
Thus, our relational captioning framework enables more diverse interpretation of the objects compared to the traditional scene graph generation models, which would be more favorable representation to scene context understanding.
}




\begin{figure*}[t]
	\vspace{-2mm}
	\centering
	\includegraphics[width=.8\linewidth,keepaspectratio]{Figures/weight_transfer}
    \vspace{-0mm}
	\caption{{Visualization of POS importance transition}. 
		Y-axis represents respective representative hidden values of \texttt{Subject}-\texttt{Predicate}-\texttt{Object} LSTMs, and X-axis represents {words of each caption in order.}
		{\texttt{subj}-\texttt{pred}-\texttt{obj} are color-coded by red, green, and blue colors according to the output of the POS predictor, respectively. {Each word in the captions comes from the corresponding LSTM.}}
		\vspace{-2mm}
}
	\label{fig:weighttransfer}
\end{figure*}




\begin{table}[t]{\centering
    \resizebox{.9\linewidth}{!}{\setlength{\tabcolsep}{1mm}
		\begin{tabular}{l ccc ccc}\toprule
			&\multicolumn{3}{c }{Phrase detection}&\multicolumn{3}{c}{Relationship detection}\\&mAP&		R@100	&	R@50		&mAP&		R@100	&	R@50 	\\\midrule
\texttt{Language Prior}~\cite{lu2016visual} 	&{2.07}&		17.03&16.17&{1.52}&14.70&13.86		\\


             
             \texttt{VTransE}~\cite{zhang2017visual} &-&22.42&19.42&-&15.20&14.07		\\
             \texttt{VRL}~\cite{liang2017deep}       &-&22.60&21.37&-&20.79&18.19\\
             \texttt{ViP-CNN}~\cite{li2017vip}	    &-&27.91&22.78&-&20.01&17.32\\
             \texttt{DR-Net}~\cite{dai2017detecting} &-&23.45&19.93&-&20.88&17.73	\\
             
             \texttt{CAI}~\cite{zhuang2017towards}   &-&19.24&17.60&-&17.39&15.63\\
             \texttt{PPR-FCN}~\cite{zhang2017ppr}    &-&23.15&19.62&-&15.72&14.41	\\
             Yu~\etal~\cite{yu2017visual}   &-&24.03&23.14&-&21.34&19.17		\\
             \midrule
\textbf{\texttt{MTTSNet (Ours)}}		&{2.88}  & 20.98& {20.64} & {1.59} & 20.05 & {17.49} \\
            \textbf{\texttt{MTTSNet (Ours)+REM}}		&{2.91}  & 21.54& 21.39 & 1.64 & 20.70 & 17.74 \\
\textbf{\texttt{MTTSNet (Ours)+REM (R)}}		&3.09  & 28.40& 24.18 & 1.73 & 21.87 & 19.36 \\
        \bottomrule
		\end{tabular}
    }
    \vspace{1mm}
 	\caption{Comparison of our \texttt{MTTSNet} with VRD models {on the} VRD metrics on the VRD dataset. 
 	{(R) indicates ResNet-50~\cite{he2016deep} as a backbone network instead of VGG-16.}
 	{Despite the disadvantages for predicting complex captions compared to simple triplets, our \texttt{MTTSNet} and \texttt{MTTSNet+REM} show favorable {or comparable} performance against {the} VRD models.
 	{Also, note that most of the VRD models have the benefit of strong backbones such as ResNet, but our \texttt{MTTSNet+REM (R)} with ResNet-50 surpasses all the other competing methods even with the VRD metrics unfavorable to ours.}
}\vspace{-6mm}
 	}
 	\label{table:VRDmetric}	
}
\end{table}






\subsection{Sentence-based Image and Region-pair Retrieval}
Since our relational captioning framework produces richer image representations than other frameworks, it may have benefits on image {and}
region-pair retrieval by sentence.
{Our method can directly deal with free-form natural language queries, whereas scene graph or VRD models require additional processing to handle the free-form queries.} {In this section,} we evaluate our method on the retrieval task.
Following the same procedure 
from \cite{johnson2016densecap} but with our relational captioning dataset,
we randomly choose 1,000 images from the test set, and from these chosen images, we collect 100 query sentences by sampling four random captions from 25 randomly chosen images.
The task is to retrieve the correct image for each query by matching it with the generated captions.

{Our relational captioning based retrieval is done as follows.}
For every test image, we generate 100 region proposals from the RPN followed by NMS.
{To measure the degree of association, \ie, matching score,}
between a query and a region pair in the image, we compute the probability that the query text may occur from the region pair by multiplying the probability of words over recursive steps.
Among all the scores {of} the region pairs from the image, we take the maximum matching score value as a representative score of 
{matching between the query text and} the image.
The retrieved images are sorted {according to} these computed matching scores.











{
We compare the retrieval performance with several baselines in \Tref{table:retrieval}.
We measure \emph{recall at top K}, \emph{R@K}, which is the success ratio across all the queries that, by each given query,
its ground-truth image is retrieved within top $K$ ranks.
We report $K \in \{1,5,10\}$ cases.
}
We also report the median rank of the correctly retrieved images across all 1000 test images.
{We follow the same procedure by Johnson~\etal of running through random test sets 3 times to report the average results.
{We add an additional retrieval result with a more competitive dense captioning model, \texttt{TLSTM}~\cite{Yang_2017_CVPR}.}
From the result, our proposed relational captioners show favorable performance against the baselines.
{This is meaningful because a region pair based method deals with a more difficult input form than that of the single region based approaches.}
}
{Moreover, \texttt{MTTSNet+REM} {consistently} shows better retrieval performance compared to \texttt{MTTSNet}.}





\begin{table}[b]
\vspace{-2mm}
\centering
    	\resizebox{.8\linewidth}{!}{\begin{tabular}{l cc}\toprule
													&			words/img		&	words/box	\\\midrule
			Image Cap.~\cite{vinyals2015show}	&				4.16		&	-	\\
            Scene Graph~\cite{zellers2018neural}				&		7.66		&	3.29	\\
            Dense Cap.~\cite{johnson2016densecap}		&				18.41		&	4.59	\\
            Relational Cap. (\textbf{\texttt{MTTSNet}})	 			&{20.45}&{15.31}\\
            Relational Cap. (\textbf{\texttt{MTTSNet+REM}})	 			&\textbf{25.57}&\textbf{18.02}\\
            \bottomrule
		\end{tabular}
        }\vspace{1mm}
	\caption{Diversity comparison between image captioning, scene graph generation, dense captioning, and relational captioning {frameworks}.
    We measure the number of different words per image (words/img) and the number of words per bounding box (words/box).\vspace{-0mm}}
	\label{table:diversity}	
\end{table}



\Fref{fig:retrieval} shows the qualitative results on the sentence based {image and region-pair} retrieval.
Given a sentence query, we show the retrieved images and their region pairs
with
the maximum matching score.
Image retrieval based on our approach has a distinct advantage in that it retrieves images containing similar contextual relationships despite significant visual differences.
More specifically, in the 3rd row of \Fref{fig:retrieval}, our method can retrieve images with an abstract contextual relationship of ``White sign near paved road.'' 
{The retrieved images} are visually diverse but share the same contextual information.
{Also, the natural language based retrieval from our framework is distinctive compared to traditional relationship detection methods (classification) which cannot handle natural language queries with variable length due to {their}
fixed form {input} (\ie\texttt{subj-pred-obj}).}
For example, in the 1st row, given a query that specifies the color ``red,'' our model is able to retrieve images of a plane with red wings \emph{which VRD models are not capable of}.










\begin{figure}[b]
\vspace{-2mm}
\centering
	\includegraphics[width=1.0\linewidth,keepaspectratio]{Figures/failure_POS}
   \vspace{-4mm}
   \caption{Failure cases of our model. The reasons for failure cases are often due to
   visual ambiguity and illumination.
   {\texttt{subj}-\texttt{pred}-\texttt{obj} are color-coded by red, green, and blue colors according to the output of the POS predictor, respectively.}}
\label{fig:failure}
\end{figure}





\subsection{Comparison with VRD Model}
In order to demonstrate the flexibility of our model's output,
\ie, natural language based sentences, we qualitatively compare our model with one of the benchmark models of visual relationship detection (VRD) task. 
We test the VRD benchmark model~\cite{lu2016visual} and our \texttt{MTTSNet} (and with \texttt{+REM}).
The comparison is shown in \Fref{fig:VRD_comparison}. 
While the output of the VRD model is limited to the \texttt{subj-pred-obj} triplet with a smaller number of classes in a closed set, the output of our model has
more flexibility and can contain more contextual information {by virtue of being free form}.
For example, given the same object ``person,'' our model is able to distinguish the fine-grained category, \ie, man and woman.
In addition, our model can provide rich information about the object (\eg, smiling, gray) by virtue of leveraging attribute information of our relational captioning data.
Thus, our relational captioning framework enables higher level interpretation of the objects compared to the relationship detection framework.










{Since the output of the VRD task has {a} relatively simple form (\ie, \texttt{subj-pred-obj} triplet) compared to that {of} our captioning framework (caption with free-form and variable length), a VRD model is easier to train given the same relationship detection dataset.
Thus, a direct comparison with a VRD model on the VRD dataset \cite{lu2016visual} is unfair for our method.}
Despite this, we perform quantitative comparisons with VRD models by restricting {the output vocabulary of our model such that  the words {appeared} in the VRD dataset without attributes are only used.}
We use the VRD dataset that contains in total 5000 images with 4000/1000 splits for train/test sets respectively.
Similar to the construction process of our relational captioning dataset, we tokenize the form of triplet expression, \ie, \texttt{subj-pred-obj}, to form natural language expressions, and for each word, we assign the POS class from the triplet association.
By tokenizing, we obtain 160 vocabularies for the VRD dataset.


















{We evaluate on this regime in Tables~\ref{table:ablation_VRD} and \ref{table:VRDmetric} with the relational captioning metrics and VRD metrics, respectively. Firstly,}
\Tref{table:ablation_VRD} shows {the comparisons with VRD models~\cite{lu2016visual,yang2018shuffle} on the VRD dataset along with the ablation study.}
Overall, the ablation study shows similar {trends} to that of using our relational captioning dataset {(\cf, \Tref{table:captioning})}. 
{Our \texttt{TSNet} and \texttt{MTTSNet} ({both with and without \texttt{+REM}}) show top performance among the relational captioning models, {of which difference is with and without POS prediction (w/MTL), respectively}. 
This suggests that, even on the VRD dataset, the triplet-stream component is still a strong baseline over others.}
Moreover, interestingly, while the POS classification appears to be an easy and basic task, adding the POS classification in the form of multi-task learning consistently helps the caption generation performance by a noticeable margin in our context, as shown in Tables \ref{table:captioning} and \ref{table:ablation_VRD}.




In the last row, we show the performance of the VRD models by Lu \etal~\cite{lu2016visual} and Yang \etal~\cite{yang2018shuffle} with the relational captioning metrics.
Note that {these VRD models are}
designed specifically for triplet classification on the VRD dataset.
Thus, in terms of mAP, 
it has an advantage compared to the results of the other relational captioning baselines. 
{Nonetheless, compared to the VRD model, {our} relational captioners (especially our \texttt{MTTSNet+REM}) show favorable performance on Img-Lv Recall and METEOR with a notable margin.
This suggests that the proposed relational captioning framework is advantageous in generating \emph{diverse} and \emph{semantically natural} expressions.
On the other hand,} VRD models are disadvantageous in these aspects because they use {a} closed vocabulary set and predict object classes individually without considering the context.



\Tref{table:VRDmetric} shows the comparison between our \texttt{MTTSNet} ({both with and without \texttt{+REM}}) and other VRD models 
measured on {the} VRD metrics.
Due to the difference of our output type to that of VRD, we use METEOR score thresholds proposed by \cite{johnson2016densecap} as the matching criteria {between model outputs and ground truth labels}. 
{Among the three VRD tasks (\emph{predicate classification}, \emph{phrase detection} and \emph{relationship detection}) defined in~\cite{lu2016visual}, we do not measure \emph{predicate classification} because a simple classification is out of scope for our model,} {but context understanding.}
As shown in the table, our model shows favorable {or comparable} performance to {the VRD} models despite {the fact} that they are specifically designed for the VRD task.
{Also, note that most of the VRD methods take an advantage of strong backbone networks such as ResNet over our \texttt{MTTSNet+REM} that uses VGG-16.
According to the table, our method with the ResNet-50 backbone performs better than all the other competing VRD methods.
}
This is worth noting in that, as opposed to VRD, our output label space is more complex than that of VRD due to variable caption length {and {a much larger number} 
of vocabulary}.




\subsection{Additional Analysis}



\noindent\textbf{Vocabulary statistics.}
In addition, we measure the vocabulary statistics and compare those of
the frameworks in \Tref{table:diversity}.
The types of statistics measured are:
1) an average number of unique words that have been used to describe an image, and
2) an average number of words to describe each box.
Specifically, we count the number of unique words in all the predicted sentences and present the average number per image or box.
Thus, the metric {is proportional to} the amount of information we can obtain given an image or a fixed number of boxes.
These statistics increase in the order of \emph{Image Cap.}, \emph{Scene Graph}, \emph{Dense Cap.}, and \emph{Relational Cap} ({both with and without \texttt{+REM}}).
{In conclusion, the proposed relational captioning is favorable
in diversity and amount of information ({especially when {the} REM module is added}), compared to both of the traditional object-centric scene understanding frameworks, \ie, \emph{Dense Cap.} and \emph{Scene Graph}.}









\noindent\textbf{Importance transition along the triple-LSTMs.}
{Since we have the three state LSTMs to predict a single word, it might be questionable whether each LSTM learns their own semantic roles properly.
To see the behavior of each LSTMs,}
we visualize the weight transition from each LSTM for each time step. 
For this, given a set of features fed to the triple-stream LSTMs, we compute the L2 norm of the LSTM hidden state vector for each time step {as a measure of importance value}. 
These {values} from the three LSTMs are normalized across time through mean value subtraction.
These {values} can be regarded as information or importance quantities.
\Fref{fig:weighttransfer} shows the transitions of the representative values across time. 
As the {POS phase} changes through subject-predicate-object, the weight of the subject LSTM consistently decreases {while that of the object LSTM increases.}
{The} predicate LSTM has {a relatively consistent} intensity between subject and object LSTMs {as the POS changes}.
Thus, LSTMs plausibly disentangle their own roles according to POS.
{In other words, each word in the captions comes from the corresponding LSTM, \eg,~a subject word is generated from the subj-LSTM.}







\noindent\textbf{Discussion of the failure cases.}
\Fref{fig:failure} shows failure cases of {our} relational captioning.
The captions generated from our method can be inaccurate for several reasons.
One of the important factors is visual ambiguity. 
Ambiguity may come from visually similar but different objects (first column) or by geometric ambiguity (second column). 
{Lastly}, due to illumination, the model may describe the object {with} 
a different color (\eg, ``blue'') (third column).
{Each of cases requires challenging capabilities, such as geometric reasoning, high resolution spatial representation learning, illumination invariance, \etc, which are all fundamental computer vision challenges.}
we postulate that these problems may be resolved by improving visual feature representation; we leave these failure cases as a future direction.
Note that 
the predicted POS is still correctly aligned with the words in the generated captions.










