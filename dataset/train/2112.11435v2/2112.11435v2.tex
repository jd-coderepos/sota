\section{Experiments}

\subsection{Image Recognition \& ImageNet-1K Results}
\begin{table}
\centering
\resizebox{0.9\columnwidth}{!}{\begin{tabular}{@{}>{\columncolor{white}[0pt][\tabcolsep]}l|ccc|>{\columncolor{white}[\tabcolsep][0pt]}c@{}}
\addlinespace
\toprule[1.5pt]
\multicolumn{1}{r|}{Method}  & Params & GFLOPS & Throughput & Top-1 Acc. \\ \midrule
ResNet50~\cite{ResNet, ResNetStrikeBack} & 26M & 4.1 & \textbf{1287} & 80.4 \\
ResNet101~\cite{ResNet, ResNetStrikeBack} & 45M & 7.9 &  770 & 81.5 \\
ResNet152~\cite{ResNet, ResNetStrikeBack} & 60M & 11.6 &  539 & 82.0 \\ \midrule
DeiT-S~\cite{DEiT}           & 22M    & 4.6    & 940 & 79.8           \\
DeiT-B~\cite{DEiT}           & 86M    & 17.5   & 292 & 81.8           \\ \midrule
Swin-Tiny~\cite{Swin}                & 29M    & 4.5    & 723 & 81.3           \\
Swin-Small~\cite{Swin}               & 50M    & 8.7    & 425 & 83.0           \\
Swin-Base~\cite{Swin}                & 88M    & 15.4   & 277 & 83.5           \\
Swin-Base~\cite{Swin}  & 88M    & 47.0   & 85 & 84.5           \\
\midrule
NestT-Tiny~\cite{NestT}        & 17M    & 5.8    & 568 & 81.5           \\
NestT-Small~\cite{NestT}       & 38M    & 10.4    & 352 & 83.3           \\
NestT-Base~\cite{NestT}        & 68M    & 17.9   & 233 & 83.8           \\ \midrule
Focal-Tiny~\cite{Swin}            & 29M    & 4.9    & 546 & 82.2           \\
Focal-Small~\cite{Swin}           & 51M    & 9.1    & 282 & 83.5           \\
Focal-Base~\cite{Swin}            & 90M    & 16.0   & 207 & 83.8           \\ \midrule
\rowcolor{lightgreen} QnA-Tiny              & \textbf{16M}    & 2.5    & 1060 & 81.7           \\
\rowcolor{lightgreen} QnA-Tiny              & 16M    & 2.6    & 895 & 82.0           \\
\rowcolor{lightgreen} QnA-Small             & 25M    & 4.4    & 596 & 83.2           \\
\rowcolor{lightgreen} QnA-Base              & 56M    & 9.7    & 372 & 83.7           \\
\rowcolor{lightgreen} QnA-Base    & 56M    & 30.6  & 177 & \textbf{84.8}           \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{\textbf{ImageNet-1K\cite{ImageNet} pre-training results.} All models were pre-trained and tested on input size . Models marked with  are later also fine-tuned and tested on  resolution, following~\cite{FixResNet}. The Accuracy, parameter count, and floating point operations are as reported in the corresponding publication. Throughput was calculated using the timm~\cite{timm} library, on a single NVIDIA V100 GPU with 16GB memory. For QnA, a  window size was used instead of . Our model achieves comparable results to state-of-the-art models, with fewer parameters and better computation complexity.}
\label{tbl:main_classification_results}
\end{table} \paragraph{Setting:} we evaluate our method using the ImageNet-1K~\cite{ImageNet} benchmark, and follow the training recipe of DEiT~\cite{DEiT}, except we omit EMA~\cite{EMA} and repeated augmentations~\cite{RepeatedAug}. For full-training details please refer to~\cref{sup:training_details}. 

\paragraph{Results:} A summary comparison between different models appears in~\Cref{tbl:main_classification_results}. As shown from the table, most transformer-based vision models outperform CNN-based ones in terms of the top-1 accuracy, even when the CNN models are trained using a strong training procedure. For example, ResNet50~\cite{ResNet} with standard ImageNet training achieves 76.6\% top-1 accuracy. However, as argued in~\cite{ResNetStrikeBack}, with better training, its accuracy sky-rockets to 80.4\%. Indeed, this is a very impressive improvement, yet it falls short behind transformer models. In particular, our model (the tiny version) improves upon ResNet by 1.3\% with ~40\% fewer parameters and FLOPs. 

In terms of speed, CNNs are very fast and have a smaller memory footprint (see~\Cref{fig:complexity_comparison}). The throughput gap can be evident by investigating the vision transformers reported in Table~\ref{tbl:main_classification_results}. A particular strong ViT is the Focal-ViT~\cite{FocalViT}; in its tiny version, it improves upon ResNet101 by 0.7\% while the latter enjoys x1.4-times better throughput. Nonetheless, our model stands out in terms of the speed-accuracy trade-off. Comparing QnA-Tiny with Focal-Tiny, we achieve only 0.5\% less accuracy while having x2-times better throughput, parameter-count, and flops. We can even reduce this gap by training the QnA with a larger receptive field. For example, setting the receptive field of the QnA to be 7x7, instead of 3x3, achieve 82.0\% accuracy, with negligible effect on the model speed and size. 

Finally, we notice that most Vision Transformers achieve similar Top-1 accuracy. More specifically, tiny models (in terms of parameters and number of FLOPs) achieve roughly the same Top-1 accuracy of 81.2-82.0\%. The accuracy difference is even less significant in larger models (e.g., base variants accuracy differs by only 0.1\%), and this accuracy difference can be easily tipped to either side by many factors, even by choosing a different seed~\cite{Seed}. Nonetheless, our model is faster, all while using fewer resources. 

\paragraph{The reason behind better accuracy-efficiency trade-off:} QnA-ViT achieves a better accuracy-efficiency trade-off for several reasons. First, QnA is fast, which is crucial for better throughput. Further, most of the vision transformer's parameter count is due to the linear projection matrices. Our method reduces the number of linear projections by omitting the query projections (i.e., the  matrix is replaced with 2-learned queries). Furthermore, the feed-forward network requires  more parameters than the self-attention. Our model uses smaller embedding dimensions than existing models without sacrificing accuracy. Namely, NesT-Tiny~\cite{NestT} uses an embedding dimension of 192, while Swin-Tiny~\cite{Swin} and Focal-Tiny~\cite{FocalViT} use 96 embedding dimensions. On the other hand, our method achieves a similar feature representation capacity, with a lower dimension of 64.

Finally, other parameter efficient methods achieve low parameter count by training on larger input images~\cite{HaloNet, EfficientNet}. This is shown to improve image-classification accuracy~\cite{FixResNet}. However, it comes at the cost of lower-throughput and more FLOPs. For example, EfficientNet-B5~\cite{EfficientNet}, which was trained and tested on images of  resolution, achieves 83.6\% accuracy while using only 30M parameters. Nonetheless, the network's throughput is 170 images/sec, and it uses 9.9 GFLOPs. Compared to our base model, QnA achieves similar accuracy with twice the throughput. Also, it is important to note that these models were optimized via Neural Architecture search, an automated method for better architecture design. We believe employing methods with similar purpose~\cite{NViT} would even further optimize our models' parameter count.

\begin{table}
\centering
\resizebox{0.7\linewidth}{!}{\begin{tabular}{@{}>{\columncolor{white}[0pt][\tabcolsep]}lc
>{\columncolor{lightgreen}}c 
>{\columncolor{lightgreen}}c 
>{\columncolor{lightgreen}}c 
>{\columncolor{lightgreen}}c >{\columncolor{white}[0pt][\tabcolsep]}c@{}}
\toprule[1.5pt]
                        &       & \multicolumn{4}{c}{\cellcolor{lightgreen}QnA}     \\
                        & SASA  &           &     &        &        \\ \midrule[1.5pt]
Top-1 Acc.              & \textbf{80.86} & 80.3         & 80.7       & 80.76     & 80.81     \\ \midrule
Params (M).             & 16.440& \textbf{16.182}       & 16.188     & 16.192   & 16.200.   \\ \midrule
FLOPS (G)               & 2.620 & \textbf{2.378}        & 2.400      & 2.420     & 2.442     \\ \bottomrule[1.5pt]
\end{tabular}}
\caption{\textbf{Multiple queries effect.} We compare the performance of SASA~\cite{SASA} to QnA with a varying amount of queries. As can be seen,  using multiple queries improves QnA, reaching comparable performance, using an order of magnitude less memory.}
\label{tbl:sasa_multiple_queries}
\end{table} 
\subsection{Ablation \& Design Choices}
\label{sec:ablation_and_design_choices}

\paragraph{Number of Queries:} Using multiple queries allows us to capture different feature subspaces. We consider SASA~\cite{SASA} as our baseline, which extracts the self-attention queries from the window elements. Due to its heavy memory footprint, we cannot consider SASA variants similar to QnA-ViT. Instead, we consider a lightweight variant that combines local self-attention with SASA. All SASA layers use a 3x3 window size. Downsampling is performed similar to QnA-ViT, except that we replace QnA with SASA. Finally, the local-self attention layers use a 7x7 window size without overlapping (see~\Cref{sup:num_of_queries}). The results are summarized in Table~\ref{tbl:sasa_multiple_queries}. As can be seen, we achieved comparable results to SASA. In addition, two queries outperform one, but this improvement saturates quickly. We hence recommend using two queries, as it enjoys efficiency and expressiveness. 

\paragraph{Number of heads:} Most vision transformers use large head dimension (e.g., )\cite{CaiT}. However, we found that the QnA layer enjoys more heads. We trained various models based on QnA and self-attention layers with different training setups to verify this. Our experiments found that a head dimension  works best for QnA layers. Similar to previous work~\cite{CaiT}, in hybrid models, where both self-attention and QnA layers are used, we found that self-attention layers still require a large head dimension (i.e., ). Moreover, we found that using more heads for QnA is considerably better (up to  improvement) for small networks. Moreover, this performance gap is more apparent when training the models for fewer epochs without strong augmentations (see~\Cref{sup:num_of_heads} for further details). Intuitively, since the QnA layer is local, it benefits more from local pattern identifications, unlike global context, which requires expressive representation. 

\begin{table}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}>{\columncolor{white}[0pt][\tabcolsep]}ccccc|>{\columncolor{white}[\tabcolsep][0pt]}c@{}}
\toprule[1.5pt]
Global Attention              & QnA              & Downsampling & Params & FLOPs & Top1-Acc. \\ \midrule[1.1pt]
\multicolumn{6}{c}{Different downsampling choices}                              \\ \midrule[1.1pt]
{[}3,3,6,2{]}  & {[}0,0,0,0{]}  & Nest~\cite{NestT}         & 16.8M  & 3.7   & 81.2          \\
{[}3,3,6,2{]}  & {[}0,0,0,0{]}  & Swin~\cite{Swin}         & \textbf{16.0M}  & \textbf{3.1}   & 81.2          \\
{[}3,3,6,2{]}  & {[}1,1,1,0{]}  & QnA          & \textbf{16.0M} & 3.2   & \textbf{81.9}          \\ \midrule[1.1pt]
\multicolumn{6}{c}{Number of QnA blocks vs Transformer blocks}                                                \\ \midrule[1.1pt]
{[}0,0,0,0{]}  & {[}4,4,7,2{]}  & QnA          & 14.9M  & 2.4   & 80.9          \\
{[}3,3,6,2{]}  & {[}1,1,1,0{]}  & QnA          & 16.0M  & 3.2   & \textbf{81.9}          \\
\rowcolor{lightgreen} 
{[}0,0,4,2{]}  & {[}4,4,3,0{]}  & QnA          & \textbf{15.8M}  & \textbf{2.6}   & \textbf{81.9}          \\ \midrule[1.1pt]
\multicolumn{6}{c}{Deeper Models}                                               \\ \midrule[1.1pt]
{[}0,0,8,2{]}  & {[}3,4,11,0{]} & QnA          & \textbf{24.7M}      & \textbf{4.2}     & 82.7          \\
{[}0,0,10,2{]} & {[}3,4,9,0{]}  & QnA          & 24.8M      & 4.3     & 83.0          \\
\rowcolor{lightgreen} 
{[}0,0,12,2{]} & {[}3,4,7,0{]}  & QnA          & 25.0M      & 4.4     & \textbf{83.2}          \\
{[}0,0,16,2{]} & {[}3,4,3,0{]}  & QnA          & 25.3M      & 4.6     & 83.1          \\ \bottomrule[1.5pt]
\end{tabular}}
\caption{\textbf{Ablation studies and design choices.} In the first two columns we specify the number of global-attention and QnA layers used in each stage. See~\Cref{sec:ablation_and_design_choices} for further details, and the supp. materials for more configurations.}
\label{tbl:design_choices}
\end{table} \paragraph{How many QnA layers do you need?} 
In order to verify the expressive power of QnA, we consider a dozen different models. Each model consists of four stages. In each stage, we consider using self-attention and QnA layers. A summary report can be found in Table~\ref{tbl:design_choices} (for the full report, please see~\Cref{sup:how_much_qna}). In our experiments, we conclude that the QnA layer is effective in the early stages and can replace global attention without affecting the model's performance. QnA is fast and improves the model's efficiency. Finally, the QnA layer is a very effective down-sampling layer. For example, we considered two baseline architectures which are mostly composed of transformer blocks, (1) one model uses simple 2x2 strided-convolution to reduce the feature maps (adopted in~\cite{Swin}), and the (2) other is based on the down-sampling used in NesT~\cite{NestT}, which is a 3x3 convolution, followed by a layer-norm and max-pooling layer. These two models achieve similar accuracy, which is 81.2\%. On the other hand, when merely replacing the downsampling layers with the QnA layer, we witness a ~0.7\% improvement without increasing the parameter count and FLOPs. Note, global self-attention is still needed to achieve good performance. However, it can be diminished by local operations, e.g., QnA.

\paragraph{Deep models:} 
To scale-up our model, we chose to increase the number of layers in the network's third stage (as typical in previous works~\cite{ResNet}). This design choice is adapted mainly for efficiency reasons, where the spatial and feature dimension are manageable in the third stage. In particular, we increase the total number of layers in the third stage from 7 to 19 and consider four configurations where each configuration varies by the number of QnA layers used. The models' accuracies are reported in Table~\ref{tbl:design_choices}. As seen from the table, the model's accuracy can be maintained by reducing the number of global attention. This indicates that while self-attention can capture global information, it is beneficial to a certain degree, and local attention could be imposed by the architecture design for efficiency consideration. 

\subsection{Object Detection}
\begin{table}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}>{\columncolor{white}[0pt][\tabcolsep]}llccccc>{\columncolor{white}[\tabcolsep][0pt]}c@{}}
\toprule
Model    & Backbone &  &  &   &   &   & AP   \\ \midrule
         & R50      & 55.4 & 36.6 & 53.2 & 38.0 & 15.1 & 35.3 \\ \cmidrule(l){2-8} 
DETR &
  \cellcolor{lightgreen}QnA-Ti &
  \cellcolor{lightgreen}58.9 &
  \cellcolor{lightgreen}38.6 &
  \cellcolor{lightgreen}56.8 &
  \cellcolor{lightgreen}40.6 &
  \cellcolor{lightgreen}16.0 &
  \cellcolor{lightgreen}37.5 \\ \cmidrule(l){2-8} 
 &
  \cellcolor{lightgreen}QnA-Ti7 &
  \cellcolor{lightgreen}\textbf{59.6} &
  \cellcolor{lightgreen}39.3 &
  \cellcolor{lightgreen}\textbf{57.6} &
  \cellcolor{lightgreen}41.2 &
  \cellcolor{lightgreen}16.0 &
  \cellcolor{lightgreen}37.9 \\ \midrule
\rowcolor{lightgreen} 
DETR-QnA & QnA-Ti   & \textbf{59.6} & \textbf{39.7} & 57.4 & \textbf{41.8} & \textbf{18.2} & \textbf{38.5} \\ \bottomrule
\end{tabular}}
\caption{\textbf{DETR~\cite{DETR} Based Object detection on the COCO Dataset~\cite{COCO}}. Incorporating QnA-ViT-Tiny with DETR substantially improves upon the ResNet50 backbone (by up to 3.2). QnA with receptive field 7x7 improves the average precision on large objects (), and incorporating QnA into the DETR network improves performance on smaller objects, indicating locality. }
\label{tbl:detr}
\end{table} 
\paragraph{Setting:} To evaluate the representation quality of our pre-trained networks, we use the DETR~\cite{DETR} framework, which is a transformer-based end-to-end object detection framework. We use three backbones for our evaluations; ResNet50~\cite{ResNet}, and two variants of QnA-ViT, namely, QnA-ViT-Tiny, and QnA-ViT-Tiny-7x7, which uses a 7x7 receptive for all QnA layers (instead of 3x3). Complete training details are provided in the supplemental material.


\paragraph{Revisiting DETR transformer design: } 
DETR achieves comparable results to CNN-based frameworks~\cite{FastRCNN}. However, it achieves less favorable average precision when tested on smaller objects. The DETR model uses a vanilla transformer encoder to process the input features extracted from the backbone network. As argued earlier, global attention suffers from locality issues. To showcase the potential of incorporating QnA in existing transformer-based networks, we propose DETR-QnA architecture, in which two transformer blocks are replaced with four QnA blocks.

\paragraph{Results: } We report the results in Table~\ref{tbl:detr}. As can be seen, DETR trained with QnA-Tiny achieves +2.2 better AP compared to the ResNet50 backbone. Using a larger receptive field () further improves the AP by 0.4. However, much improvement is due to better performance on large objects (+0.7). Finally, when incorporating QnA into the DETR encoder, we gain an additional +0.6AP (and +1.0AP relative to using the DETR model). More particularly, incorporating QnA with DETR achieves an impressive +2.2 AP improvement on small objects, indicating the benefits of QnA's locality.

\begin{figure}
    \centering
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includegraphics[height=4.cm]{figures/qna_autoencoder_qualitative/bilinear.pdf}
        \caption{Bilinear}
    \end{subfigure}
        \begin{subfigure}{0.3\linewidth}
        \centering
        \includegraphics[height=4.cm]{figures/qna_autoencoder_qualitative/transposed.pdf}
        \caption{ConvTransposed}
    \end{subfigure}
        \begin{subfigure}{0.3\linewidth}
        \centering
        \includegraphics[height=4.cm]{figures/qna_autoencoder_qualitative/qna.pdf}
        \caption{QnA}
    \end{subfigure}
    \caption{\textbf{Qualitative Auto-Encoder Results.} We train a simple Autoencoder using convolution layers (a-b), and (c) QnA layers. We show reconstructed images from the CelebA test set~\cite{CelebA}. QnA shows preferable reconstructions. See~\Cref{sec:qna_upsample} for more details.}
    \label{fig:upsample_qualitative}
\end{figure} \subsection{QnA as an upsampling layer}
\label{sec:qna_upsample}
We suggest that QnA can be adapted to other tasks besides classification and detection. To demonstrate this, we train an autoencoder network on the CelebA~\cite{CelebA} dataset, using the  reconstruction loss. We consider two simple baselines that are convolution-based. In particular, one baseline uses bilinear up-sampling to upscale the feature maps, and another baseline uses the transposed convolution layer~\cite{DeConv}. Qualitative and quantitative results appear in Figure~\ref{fig:upsample_qualitative} and Figure~\ref{fig:upsample_quantitative}, respectively. The figures show that the QnA-based auto-encoder achieves better qualitative and quantitative results and introduces fewer artifacts (see~\Cref{sup:auto-encoders} for further details). 
\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
\includegraphics[trim={0 1.4cm 0 0 }, clip, height=2.6cm]{figures/qna_autoencoder_quantitive/Autoencoder_test_l1_loss_v3.pdf}
        \includegraphics[trim={0 1.4cm 0 0 }, clip, height=2.6cm]{figures/qna_autoencoder_quantitive/Autoencoder_test_pSNR_loss.pdf}
\end{subfigure}
        \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[trim={0 1.4cm 0 0 }, clip, height=2.6cm]{figures/qna_autoencoder_quantitive/Autoencoder_test_SSIM_loss.pdf}
        \includegraphics[trim={0 1.4cm 0 0 }, clip, height=2.6cm]{figures/qna_autoencoder_quantitive/Autoencoder_test_l2_loss.pdf}
\end{subfigure}
    \caption{\textbf{Quantitative Auto-Encoder Results} are reported, compared to the same convolutional baselines as in Figure~\ref{fig:upsample_qualitative}. We compare our method (gray) to bilinear upsampling (green) and transposed covolution-based upsampling (pink). We show consistent improvement across epochs (horizontal axes) in the L1 loss (top left), the pSNR (top right), the SSIM (bottom left), and MSE (bottom right) metrics. 
}
    \label{fig:upsample_quantitative}
\end{figure} 