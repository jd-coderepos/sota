\documentclass[sigconf]{acmart}
\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}








\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{float}
\newcommand{\model}{GIPA\xspace}

\begin{document}

\title{GIPA: General Information Propagation Algorithm for Graph Learning}



\author{Qinkai Zheng, Houyi Li, Peng Zhang,\\Zhixiong Yang, Guowei Zhang, Xintan Zeng, Yongchao Liu}
\affiliation{
 \institution{Ant Group}
 \institution{Alibaba Group}
 \city{Hangzhou}
 \country{China}
}
\email{{qinkai.zheng1028, zhangpeng04}@gmail.com, {houyi.lhy, zhixiong.yangzx, bale.zgw, xintan.zxt, yongchao.ly}@antgroup.com}
\thanks{Equal contributions to this research.}
















\renewcommand{\shortauthors}{Zheng and Li, et al.}


\begin{abstract}
Graph neural networks (GNNs) have been popularly used in analyzing graph-structured data,  showing promising results in various applications such as node classification, link prediction and network recommendation. 
In this paper, we present a new graph attention neural network, namely \model, for attributed graph data learning.
\model consists of three key components: attention, feature propagation and aggregation.
Specifically, the attention component introduces a new  multi-layer perceptron based multi-head to generate better  non-linear feature mapping and representation than conventional implementations such as dot-product. 
The propagation component considers not only node features but also edge features, which differs from existing GNNs that merely consider node features.
The aggregation component uses a residual connection to generate the final embedding.
We evaluate the performance of \model using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset.
The experimental results reveal that \model can beat the  state-of-the-art models in terms of prediction accuracy, e.g., \model achieves an average test ROC-AUC of  and outperforms all the previous methods listed in the ogbn-proteins leaderboard.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

\keywords{Graph neural networks, Graph learning, Deep learning, Attention, Node property prediction}

\maketitle
\pagestyle{plain}

\section{Introduction}
\label{sec:intro}
Graph representation learning typically aims to learn an   informative embedding for each graph node based on the graph topology (link) information.
Generally, the embedding of a node is represented as a low-dimensional feature vector, which can be used to facilitate downstream applications.
This research starts from homogeneous graphs that have only one type of nodes and one type of edges. The purpose is to learn node representations from the graph topology~\cite{grover2016node2vec, perozzi2014deepwalk, dai2016discriminative}.
Specifically, given a node ,  either breadth-first search, depth-first search or random walks is used to identify a set of neighboring nodes. Then, the 's embedding is learnt by maximizing the co-occurrence probability of  and its neighbors.
In reality, nodes and edges can carry a rich set of information such as attributes, texts, images or videos, beyond graph structure.
These information can be used to generate node or edge feature by means of a feature transformation function, which  further improves the performance of network embedding~\cite{liao2018attributed}.

These pioneer studies on graph embedding have limited capability to capturing neighboring information from a graph because they are based on shallow learning models such as SkipGram~\cite{mikolov2013distributed}.
Moreover, transductive learning is used in these graph embedding methods which cannot generalize to new nodes that are absent in the training graph.

On the other hand, graph neural networks~\cite{kipf2016semi, hamilton2017inductive, velivckovic2017graph} are proposed to overcome the limitations of graph embedding models.
GNNs employ deep neural networks to aggregate feature information from neighboring nodes and thereby have the potential to gain better aggregated embedding.
GNNs can support inductive learning and infer the class labels of unseen nodes during prediction~\cite{hamilton2017inductive, velivckovic2017graph}.
The success of GNNs is mainly based on the neighborhood information aggregation.
Two critical challenges to GNNs are \textit{which neighboring nodes of a target node are involved in message passing, and how much contribution each neighboring node makes to the aggregated embedding}.
For the former question, neighborhood sampling~\cite{hamilton2017inductive, ying2018graph, chen2018fastgcn, huang2018adaptive, zou2019layer, ji2020accelerating} is proposed for large dense or power-law graphs.
For the latter, neighbor importance estimation is used to add different weights to different neighboring nodes during feature propagation.
Importance sampling~\cite{chen2018fastgcn, zou2019layer, ji2020accelerating} and attention~\cite{velivckovic2017graph,liu2019geniepath, wang2019heterogeneous, yun2019graph, hu2020heterogeneous} are two popular techniques.

Importance sampling is a specialization of neighborhood sampling, where the importance weight of a neighboring node is drawn from a distribution over nodes.
This distribution can be derived from normalized Laplacian matrices~\cite{chen2018fastgcn, zou2019layer} or jointly learned with GNNs~\cite{ji2020accelerating}.
With this distribution, each step samples a subset of neighbors and aggregates their information with importance weights.
Similar to importance sampling, attention also adds importance weights to neighbors.
Nevertheless, attention differs from importance sampling.
Attention is represented as a neural network and always learned as a part of a GNN model.
In contrast, importance sampling algorithms use statistical models without trainable parameters.
Existing attention ranks nodes but does not drop off any of them. On the contrary, importance sampling reduces the number of neighbors.
Basically, we would expect an (almost equal to) zero importance score for noise neighbors and higher scores for strongly correlated neighbors.

In this paper, we present a new graph attention neural network, namely GIPA (\underline{G}eneral \underline{I}nformation \underline{P}ropagation \underline{A}lgorithm), for attributed graphs.
In general, GIPA consists of three components, i.e.,  attention, propagation and aggregation.
The attention component uses a multi-head method which  is implemented with a multi-layer perceptron (MLP). 
The propagation component incorporates both node and edge features, unlike GAT~\cite{velivckovic2017graph} that uses only node features.
The aggregation component reduces messages from the neighbors of a given node  and concatenates the resulted message with a linear projection of node 's features by means of a residual connection.
Experiments on the Open Graph Benchmark (OGB)~\cite{hu2020open} proteins dataset (ogbn-proteins)  demonstrate that GIPA reaches the best accuracy with an average test ROC-AUC of  compared with the  state-of-the-art methods listed in the ogbs-proteins leaderboard~\footnote{https://ogb.stanford.edu/docs/leader\_nodeprop/\#ogbn-proteins}.
Up to date, this performance has been exhibited to the public in the leaderboard with \model in the first place, as shown in~\figurename~\ref{fig:ogbn_proteins}.
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/ogbn_proteins.png}
    \caption{A snapshot of the leaderboard of ogbn-proteins with \model as the champion.}
    \label{fig:ogbn_proteins}
\end{figure*} 
\section{Related Work}
\label{sec:related}
In this part, we survey existing attention primitive implementations in brief.
\cite{bahdanau2014neural} proposed an additive attention that calculates the attention alignment score using a simple feed-forward neural network with only one hidden layer.
The alignment score  between two vectors  and  is defined as

where  is an attention vector and the attention weight  is computed by normalizing  over all  values with the softmax function.
The core of the additive attention lies in the use of attention vector .
This idea has been widely adopted by several algorithms~\cite{yang2016hierarchical, pavlopoulos2017deeper} for neural language processing.
\cite{luong2015effective} introduces a global attention and a local attention.
In global attention, the alignment score can be computed by three alternatives: dot-product (), general () and concat ().
In contrast, local attention computes the alignment score solely from a vector ().
Likewise, both global and local attention normalize the alignment scores with the softmax function.
\cite{vaswani2017attention} proposed a self-attention mechanism based on scaled dot-products.
This self-attention computes the alignment scores between any  and  as follows.

This attention differs from the dot-product attention~\cite{luong2015effective} by only a scaling factor of .
The scaling factor is used because the authors of~\cite{vaswani2017attention} suspected that for large values of , the dot-products grow large in magnitude and thereby push the softmax function into regions where it has extremely small gradients.
Furthermore, a multi-head mechanism is proposed in order to stabilize the self-attention values computed.

In this paper, we introduce an MLP based multi-head implementation to compute attention scores.
In addition, these aforementioned attention primitives have been extended to use in heterogeneous graph models.
HAN~\cite{wang2019heterogeneous} uses a two-level hierarchical attention made out of a node-level attention and a semantic-level attention.
In HAN, the node-level attention learns the importance of a node to any other node in a meta-path, while the semantic-level one weighs all meta-paths.
HGT~\cite{zhang2019heterogeneous} weakens the dependency on meta-paths and instead uses meta-relation triplets as basic units.
In HGT, it uses node-type-aware feature transform functions and edge-type-aware multi-head attention to compute the importance of each edge to a target node.
It needs to be addressed that no evidence shows that heterogeneous models are always superior to homogeneous ones, and vice versa. 
\section{Methodology}
\label{sec:method}
\subsection{Preliminaries}

\paragraph{Graph Neural Networks.} Consider an attributed graph , where  is the set of nodes and  is the set of edges. GNNs use the same model framework as follows~\cite{hamilton2017inductive,xu2018powerful}:


\noindent where  represents an aggregation function and  represents an update function. The objective of GNNs is to update the embedding of each node by aggregating the information from its neighbor nodes and the connections between them. 

\subsection{GIPA Architecture}

In this part, we present the architecture of \model, which  extracts information from node features as well as edge features in a more general way. The process of \model is shown in~\figurename~\ref{fig:gipa_process}. Consider a node  with feature  and its neighbor nodes  with feature .  represents the edge feature between node  and . The problem is how to generate an expected embedding for node  from its own node feature , its neighbors' node features  and the related edge features . 

The \model process mainly consists of three parts: \texttt{attention}, \texttt{propagation} and \texttt{aggregation}. Firstly, \model computes the embedding , , and  by means of linear projection from the features ,  and , respectively. Secondly, the \texttt{attention} process calculates the multi-head attention weights by using fully-connected layers on , , and , respectively. Thirdly, the \texttt{propagation} process focuses on propagating information of each neighbor node  by combining 's node embedding  with the associated edge embedding . Finally, the \texttt{aggregation} process aggregates all messages from neighbors to update the embedding of . The following subsections introduce the details of each process.

\begin{figure*}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/gipa_process.png}
    \caption{Process of \model, which consists of \textit{attention}, \textit{propagation}, and \textit{aggregation} process.}
    \label{fig:gipa_process}
\end{figure*}

\subsubsection{Attention Process}

Different from the existing attention mechanisms like self-attention or scaled dot-product attention, we use MLP to realize a multi-head attention mechanism.
The \texttt{attention} process of \model can be formulated as follows:



\noindent where the attention function  is realized by an \texttt{MLP} with learnable weights  (without bias). Its input is the concatenation of the node embedding  and  as well as the edge embedding . As the scale of parameters of \texttt{MLP} is adjustable, this attention mechanism could be more representative than previous ones simply based on dot-product. The final attention weight is calculated by an edge-wise \texttt{softmax} activation function:



\subsubsection{Propagation Process}

Unlike GAT~\cite{velivckovic2017graph} that only considers the node feature of neighbors, \model incorporates both node and edge features during the \textit{propagation} process:

\noindent where the propagation function  is also realized by an \text{MLP} with learnable weights . Its input is the concatenation of a neighbor node embedding  and the related edge embedding . Thus, the  is done edge-wise rather than node-wise. 

Combining the results by \texttt{attention} and \texttt{propagation} by element-wise multiplication, \model gets the message  of node  from :


\subsubsection{Aggregation Process}

For each node , \model repeats previous processes to get messages from its neighbors. The \texttt{aggregation} process first gathers all these messages by a reduce function, summation for example:



\noindent Then, a residual connection between the linear projection  and the message of  is added through concatenation: 





\noindent where an \texttt{MLP} with learnable weights  is applied to get the final output . 
Finally, we would like to emphasize that the process of \model can be easily extended to multi-layer variants by stacking the process multiple times. 
\section{Experiments}
\label{sec:exp}
\subsection{Dataset and Settings}

\textbf{Dataset.} In our experiments, the \textit{ogbn-proteins} dataset from OGB~\cite{hu2020open} is used. The dataset is an undirected and weighted graph, containing 132,534 nodes of 8 different species and 79,122,504 edges with 8-dimensional features. The task is a multi-label binary classification problem with 112 classes representing different protein functions. 

\noindent \textbf{Evaluation metric.} The performance is measured by the average ROC-AUC scores. We follow the dataset splitting settings as recommended in OGB and test the performance of 10 different trained models with different random seeds. 

\noindent \textbf{Hyperparameters.} The hyperparamters used in \model and its training process are concluded in \tablename~\ref{tab:hyper}.

\noindent \textbf{Environment.} \model is implemented in Deep Graph Library (DGL)~\cite{wang2019deep} with Pytorch~\cite{paszke2019pytorch} as the backend. The experiments are done in a platform with GeForce TITAN RTX GPU (24GB RAM) and AMD Ryzen Threadripper 3960X CPU (24 Cores).

\begin{table}[t]
\caption{Hyperparameters of \model model.}
\begin{tabular}{lc}
\hline
\textbf{Hyperparameter}      & \textbf{Value} \\ \hline
Node embedding length        & 80             \\ 
Edge embedding length        & 16             \\ 
Number of attention layers   & 2              \\ 
Number of attention heads    & 8              \\ 
Number of propagation layers & 2              \\ 
Number of hidden units       & 80             \\ 
Number of GIPA layers        & 6              \\ 
Edge drop rate               & 0.1            \\ 
Aggregation                  & SUM            \\ 
Activation                   & ReLU           \\
Dropout of node embedding    & 0.1            \\
Dropout of attention         & 0.1            \\ 
Dropout of propagation       & 0.25           \\ 
Dropout of aggregation       & 0.25           \\ 
Dropout of the last FC layer & 0.5            \\ 
Learning rate & 0.01            \\ 
Optimizer & AdamW~\cite{loshchilov2017decoupled}  \\ \hline
\end{tabular}
\label{tab:hyper}
\end{table}
\begin{table}[!h]
\caption{Test and validation performance (ROC-AUC) on \textit{ogbn-proteins} dataset.}
\begin{tabular}{lcc}
\hline
Method              & Test ROC-AUC  & Validation ROC-AUC \\ \hline
MLP                 &  &       \\
GCN                 &  &       \\
GraphSAGE           &  &       \\
DeeperGCN           &  &       \\
GAT                 &  &       \\
UniMP               &  &       \\
UniMP+CrossEdgeFeat &  &       \\
GIPA (ours)         &  &       \\ \hline
\label{tab:performance}
\end{tabular}
\end{table}
\subsection{Performance}
\tablename~\ref{tab:performance} shows the average ROC-AUC and the standard deviation for the test set and the validation set, respectively. The results of the baselines are retrieved from the ogbn-proteins leaderboard\footnotemark[1]. Our \model outperforms all previous methods in the leaderboard and reaches an average test ROC-AUC higher than 0.8700 for the first time.
% 
\section{Conclusion}
\label{sec:conclusion}
We have presented \model, a new graph attention network architecture for attributed graph data learning.
\model has investigated three technical novelties: an MLP based multi-head attention, a propagation function involving both node feature and edge feature, and a residual connection  aggregation method.
The performance evaluations on the ogbn-proteins dataset have demonstrated that \model yields an average test ROC-AUC of , showing the best results compared with the state-of-the-art methods listed in the ogbn-proteins leaderboard. In the future, we will extend \model to heterogeneous GNNs and conduct testing on the open graph datasets.  
\begin{acks}
This work is primarily presented by the GeaLearn team of Ant Group, China.
Qinkai Zheng was a research intern in the GeaLearn team.
We thank Dr. Changhua He and Dr. Wenguang Chen for their support and guidance. \end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}





\end{document}
