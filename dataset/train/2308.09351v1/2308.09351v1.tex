\section{Experiments} \label{sec:experiments}


\begin{table}[t]
\setlength{\tabcolsep}{3pt}
  \centering
    \begin{tabular}{cc|ccc|cc}
    \toprule
     &  & \textbf{Rare} & \textbf{Non-Rare} & \textbf{Full} & \textbf{\#Params} & \textbf{FPS} \\
    \midrule
    \midrule
    1     & 6     & 10.92  & 13.99  & 13.28  & 246.8M & 18.93 \\
    2     & 3     & \textbf{12.12} & \textbf{14.07} & \textbf{13.62} & 206.6M & 21.49 \\
    3     & 2     & 10.57  & 14.05  & 13.25  & 193.2M & 22.15 \\
    6     & 1     & 11.26  & 13.90  & 13.30  & 179.8M & 23.17 \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{The effect of the sparsification of language encoding layers in \textit{Cross-Modal Fusion}}. Results are evaluated on HICO-DET under zero-shot (NF) setting. FPS (frames per second) is tested on a single NVIDIA A100 with a minibatch size 1.}
    \vspace{-.2cm}
  \label{tab:sparsification_language_encoder}
\end{table}

\textbf{Pre-training datasets.}
To pre-train RLIPv2, we utilize VG~\cite{krishna2017visualgenome}, COCO~\cite{lin2014MSCOCO} and Objects365~\cite{shao2019Objects365}.
VG has 108 images annotated with free-form relation and object annotations.
COCO has 117 images with only object annotations in 80 classes, and Objects365 has 1,742 images with only object annotations in 365 classes.
Thus, we use relational pseudo-labelling to tag relation labels for COCO and Objects365, enabling them to support relational pre-training.




\textbf{Downstream datasets.} 
For \textbf{HOI detection}, we follow~\cite{tamura2021qpic,Yuan2022RLIP,Yuan2022OCN,zhang2021CDN} to evaluate on HICO-DET~\cite{chao2015hico} and V-COCO~\cite{gupta2015VisualSemanticRole}.
For HICO-DET that contains 117 verbs and 80 objects, we evaluate under the \textbf{Default} setting on \textit{Full}, \textit{Rare} and \textit{Non-Rare} sets.
For V-COCO that contains 24 interactions and 80 objects, we evaluate following the official evaluation code~\cite{gupta2015VisualSemanticRole} under two scenarios:   and .
For \textbf{SGG}, we assess RLIPv2 on the widely-used Open Images v6~\cite{kuznetsova2020open_image} dataset, which is annotated with 288 objects and 30 relations.
We evaluate RLIPv2 using standard evaluation metrics~\cite{kuznetsova2020open_image,zhang2019RelDN}: 
Recall@50 (), weighted mean Average Precision for relation detection () and phrase detection ().
The final score is calculated as:
.







\textbf{Implementation details.}
To assess the effectiveness of RLIPv2, we choose to adopt DDETR~\cite{zhu2020deformableDETR} to compose \textbf{RLIPv2-ParSeD} and adopt DAB-DDETR~\cite{liu2022DABDETR} to compose \textbf{RLIPv2-ParSeDA}.
To peform a fair comparison with previous works, we ensure that RLIPv2 has  DDETR/DAB-DDETR encoding layers.
For \textit{Parallel Entity Detection} and \textit{Sequential Relation Inference}, 3 layers are adopted following~\cite{Yuan2022RLIP,Yuan2022OCN,tamura2021qpic,zhang2021CDN}.
 is set to 100 during pre-training and fine-tuning, except when we fine-tune on HICO-DET where  is set to 64 following~\cite{zhang2021CDN}.
Regarding model initialization, we use COCO detection parameters as initialisation when using VG or VG and COCO for pre-training; when using VG, COCO and Objects365 for pre-training, we use COCO and Object365 detection parameters as initialisation.
Regarding the configuration of minibatch sizes and learning rate (LR), we set the minibatch size to 64, LR for the text encoder to 1.41e-5 and LR for other modules to 1.41e-4 when using ResNet-50~\cite{he2016resnet} and Swin-T~\cite{liu2021swin};
We set the minibatch size to 32, LR for the text encoder to 1e-5 and LR for other modules to 1e-4 when using Swin-L~\cite{liu2021swin}.
RLIPv2 and R-Tagger are pre-trained and fine-tuned for 20 epochs unless otherwise stated, with LR dropping by a factor of 10 at the 15 epoch.
For the BLIP captioner, we adopt the ViT-L/16~\cite{dosovitskiy2020ViT_16_16} model fine-tuned on COCO Caption~\cite{chen2015COCO_Caption}.




{\renewcommand{\arraystretch}{0.9}
\begin{table}[t]
  \small
  \setlength{\tabcolsep}{11pt}
  \centering
    \begin{tabular}{cc|ccc}
    \toprule
    \textbf{Gating} & {\rm tanh()} & \textbf{Rare} & \textbf{Non-Rare} & \textbf{Full}  \\
    \midrule    
    \midrule
         & \multirow{3}[2]{*}{\Checkmark} & 10.47  & 13.83  & 13.05  \\
     &       & 9.91  & 13.54  & 12.70  \\
    SE    &       & 11.07  & 13.89  & 13.24  \\
    \midrule
         & \multirow{3}[2]{*}{\xmark} & \textbf{12.12} & \textbf{14.07} & \textbf{13.62} \\
     &       & 10.98  & \textbf{14.07} & 13.36  \\
    SE    &       & 11.07  & 14.00  & 13.32  \\
    \bottomrule
    \end{tabular}\vspace{-.1cm}
    \caption{\small \textbf{Comparisons of different gating functions} introduced in~\cref{sec:ALIF}. We report zero-shot (NF) results on HICO-DET.}
    \vspace{-.2cm}
  \label{tab:gating_function}\end{table}}

\begin{figure*}[t]
{\renewcommand{\arraystretch}{0.9}
\begin{minipage}[l]{0.63\textwidth}
  \small
\setlength{\tabcolsep}{2pt}
  \centering
    \begin{tabular}{c|ccc|cc|c}
    \toprule
    \multirow{2}[2]{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Pre-training}} & \multicolumn{2}{c|}{\textbf{Fully-finetuning}} & \multirow{2}[2]{*}{\textbf{FPS}} \\
& \textbf{Ep.} & \textbf{Time} & \textbf{Zero-shot (NF)} & \textbf{Ep.} & \textbf{Result} &  \\
    \midrule
    RLIPv1-ParSeD & 50    & 25.9h  & 11.20 / \textbf{14.73} / \textbf{13.92} & 60    & 24.67 / 32.50 / 30.70 & 21.89 \\
    \rowcolor{mygray}  RLIPv2-ParSeD & 20    & 10.9h  & \textbf{12.12} / 14.07 / 13.62 & 20    & \textbf{26.47} / \textbf{33.51} / \textbf{31.89} & 21.49 \\
    \midrule
    RLIPv1-ParSeDA & 50    & 27.2h  & 11.34 / 14.56 / 13.82 & 60    & 22.85 / 30.87 / 29.03 & 19.41 \\
    \rowcolor{mygray}  RLIPv2-ParSeDA & 20    & 11.3h  & \textbf{13.03} / \textbf{14.98} / \textbf{14.53} & 20    & \textbf{27.01} / \textbf{35.21} / \textbf{33.32} & 18.94 \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \makeatletter\def\@captype{table}\makeatother\caption{\small \textbf{Comparisons of RLIPv1 and RLIPv2} under zero-shot (NF) and fully-finetuning settings on HICO-DET pre-trained on VG. Results are reported on \textit{Rare}/\textit{Non-Rare}/\textit{Full} sets. FPS is tested on a single NVIDIA A100 with minibatch size 1. Pre-training time is tested on 8 NVIDIA A100. ``Ep.'' denotes number of epochs. }
    \vspace{-.2cm}
  \label{tab:RLIPv1_and_RLIPv2}\end{minipage}}
\hspace{2pt}
{\renewcommand{\arraystretch}{0.9}
\begin{minipage}[r]{0.35\textwidth}
\setlength{\tabcolsep}{3pt}
  \centering
    \begin{tabular}{c|ccc}
    \toprule
    \textbf{Initialization} & \textbf{Rare} & \textbf{Non-Rare} & \textbf{Full} \\
    \midrule
    \midrule
    COCO (default) & 12.12  & 14.07  & 13.62  \\
    \midrule
    R-Tagger & 12.55 & 13.64 & 13.39 \\
w/o noise & 9.94  & 12.74 & 12.09 \\
    w/o attn masks & 8.55  & 11.22 & 10.61 \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \makeatletter\def\@captype{table}\makeatother\caption{\small \textbf{The quality of R-Tagger parameters.} Results are evaluated using RLIPv2-ParSeD with ResNet-50 under zero-shot (NF) setting. }
    \vspace{-.2cm}
  \label{tab:R-Tagger_params}
\end{minipage}}
\end{figure*}



\subsection{Ablation Study}
We perform the ablation study using RLIPv2-ParSeD with ResNet-50 as the backbone, and VG as the pre-training dataset unless otherwise stated.
We pre-train for 20 epochs and evaluate performance on HICO-DET under the zero-shot with no fine-tuning (NF) setting.

\vspace{-.2cm}
\subsubsection{Asymmetric Language-Image Fusion}
\textbf{Sparsification of the language layers.} 
First of all, we ablate on the number of language layers to investigate the effect of sparsifying language encoding layers in \textit{Cross-Modal Fusion}.
As introduced in~\cref{sec:ALIF}, dense language layers cause difficult training.
To avoid training collapse, we compute classification losses ( and ) using all intermediate language features rather than using only the last layer of language features in \textit{Cross-Modal Fusion}.
Results are shown in~\cref{tab:sparsification_language_encoder}.
Our findings suggest that sparsifying the language encoder does not compromise zero-shot performance and improves parameter efficiency.
We attribute this to overfitting effects on upstream datasets impairing models' generalization.
We choose  and  as the default hyper-parameters for the following experiments.









\textbf{The choice of the gating function .}
To investigate the effect of the gating function on cross-modal fusion, we experiment with variants of the gating function as described in~\cref{sec:ALIF}.
The results are shown in~\cref{tab:gating_function}.
We note that  consistently degrades performance for the three gating methods.
We conjecture that the output range of  can constrain the magnitude of the features to be fused, thus limiting the performance.
Based on the results, we choose the simplest one: the learnable scalar gating method parameterised by .




\textbf{Comparisons of RLIPv2 with RLIPv1.}
To compare the architectural benefit of RLIPv2 with RLIPv1, we adopt DDETR~\cite{zhu2020deformableDETR} and DAB-DDETR~\cite{liu2022DABDETR} to follow two designs, and evaluate performance under zero-shot (NF) and fully-finetuning setting. Results are shown in~\cref{tab:RLIPv1_and_RLIPv2}.
From this table, we can observe that by modifying the architecture without compromising much inference speed, RLIPv2 generally outperforms its RLIPv1 counterpart.
Specifically, RLIPv2 obtains comparable zero-shot results to RLIPv1, while costing about 0.4 pre-training time.
In terms of fine-tuning results, RLIPv2 surpasses RLIPv1 with 0.33 fine-tuning time due to earlier and deeper fusion.






\vspace{-.2cm}
\subsubsection{Relational Pseudo-labelling} \label{sec:exp_relational_pseudo_labelling}
By default, we use RLIPv2-ParSeD with ResNet-50 backbone as the basic structure of R-Tagger.


\textbf{The necessity of denoising pre-training and attention masks for R-Tagger.}
If noise is not added during R-Tagger's pre-training, the loss will fluctuate instead of steadily decreasing as the optimization proceeds.
If attention masks are not utilized to prevent information leakage, the model will tend to learn an identity mapping, thus degrading its ability to infer relations.
R-Tagger is pre-trained for 20 epochs.
To assess the quality of R-Tagger's parameters, thanks to R-Tagger's identical structure to RLIPv2-ParSeD, we initialize RLIPv2-ParSeD with R-Tagger's parameters and pre-train for 10 epochs.
We evaluate the zero-shot (NF) performance on HICO-DET as shown in~\cref{tab:R-Tagger_params}.
We observe that removing additional noise or attention masks both impair performance, highlighting their importance.





{\renewcommand{\arraystretch}{0.9}
\begin{table}[t]
  \small
  \setlength{\tabcolsep}{4.5pt}
  \centering
    \begin{tabular}{cc|ccc}
    \toprule
    \textbf{Tagging Method} & \textbf{Overlap} & \textbf{Rare} & \textbf{Non-Rare} & \textbf{Full} \\
    \midrule
    \midrule
    \multirow{2}[2]{*}{Greedy~\cite{zhong2021scene_graph_language}} & \xmark & 11.15  & 11.65  & 11.55  \\
          & \Checkmark & 13.16  & 14.70  & 14.35  \\
    \midrule
    \multirow{2}[2]{*}{CLIP (ViT-L/14)~\cite{radford2021CLIP}} & \xmark & 12.66  & 12.76  & 12.74  \\
          & \Checkmark     & 14.63  & 14.94  & 14.87  \\
    \midrule
    \multirow{2}[2]{*}{R-Tagger (ResNet-50)} & \xmark & 15.33  & \textbf{15.54}  & \textbf{15.49}  \\
          & \Checkmark & \textbf{15.36}  & 15.37  & 15.36  \\
    \bottomrule
    \end{tabular}\vspace{-.1cm}
    \caption{\small \textbf{Comparisons of relation tagging methods.} ``Overlap'' denotes the ``overlap'' prior for SO pairs introduced in~\cref{sec:relation_tagger}. We report zero-shot (NF) results pre-trained on VG and COCO. We use oracle captions from \textbf{COCO Caption}~\cite{chen2015COCO_Caption} ().}
    \vspace{-.2cm}
  \label{tab:tagging_strategy}
\end{table}}

{\renewcommand{\arraystretch}{0.9}
\begin{table}[t]
  \small
  \setlength{\tabcolsep}{7.5pt}
  \centering
    \begin{tabular}{cc|ccc}
    \toprule
    \textbf{Caption type} &  & \textbf{Rare} & \textbf{Non-Rare} & \textbf{Full} \\
    \midrule
    \midrule
    Oracle & 5     & 15.33  & 15.54  & 15.49  \\
    \midrule
    BLIP (beam) & 1     & 9.86  & 12.02  & 11.52  \\
    BLIP (nucleus) & 5     & 14.67  & 14.76  & 14.74  \\
    BLIP (nucleus) & 10    & \textbf{15.08} & \textbf{15.10} & \textbf{15.09} \\
    BLIP (nucleus) & 20    & 14.24  & 14.91  & 14.75  \\
    \midrule
    BLIP (nucleus)\textsuperscript{*} & 5     & 12.31  & 14.37  & 13.89  \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{Comparisons of different caption types.} ``beam'' and ``nucleus'' denote beam search and nucleus sampling. ``Oracle'' denotes captions from COCO Caption. By default, we adopt COCO Caption fine-tuned BLIP model. \textsuperscript{*} denotes that we adopt the pre-trained BLIP model.}
    \vspace{-.2cm}
  \label{tab:caption_type}
\end{table}}

{\renewcommand{\arraystretch}{0.9}
\begin{table}[t]
  \small
  \setlength{\tabcolsep}{3.8pt}
  \centering
    \begin{tabular}{cc|ccc}
    \toprule
    \textbf{Dataset} & \textbf{Relation candidate} & \textbf{Rare} & \textbf{Non-Rare} & \textbf{Full} \\
    \midrule
    \midrule
    VG    & -     & 12.12  & 14.07  & 13.62  \\
    \midrule
    VG+COCO & BLIP  & \textbf{15.08}  & \textbf{15.10}  & \textbf{15.09}  \\
    VG+COCO & Selection from VG & 10.34  & 11.33  & 11.11  \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{Comparisons of methods to generate relation candidate sets.} We report zero-shot (NF) results on HICO-DET.}
    \vspace{-.2cm}
  \label{tab:necessity_of_BLIP}
\end{table}}

\begin{table}[t]
\scriptsize
  \setlength{\tabcolsep}{4pt}
  \centering
    \begin{tabular}{c|ccc}\toprule
          & \multicolumn{1}{c}{\textbf{VG}} & \multicolumn{1}{c}{\textbf{VG+COCO}} & \textbf{VG+COCO+O365} \\
    \midrule
    \midrule
    \textbf{ResNet-50} & 13.03 / 14.98 / 14.53 & 15.00 / 16.60 / 16.23 & 19.64 / 17.24 / 17.79 \\
    \textbf{Swin-T} & 13.01 / 16.06 / 15.35 & 17.13 / 18.74 / 18.37 & 21.24 / 19.47 / 19.87 \\
    \textbf{Swin-L} & 19.93 / 18.74 / 19.02 & 22.59 / 21.09 / 21.44 & 27.97 / 21.90 / 23.29 \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small 
        \textbf{Model and dataset scaling experiments using RLIPv2-ParSeDA.}
        Results are evaluated on HICO-DET \textit{Rare}/\textit{Non-Rare}/\textit{Full} sets under zero-shot (NF) setting.
        }
    \vspace{-.2cm}
  \label{tab:model_dataset_scaling}
\end{table}

\begin{table}[t]
  \scriptsize
\setlength{\tabcolsep}{2pt}
  \centering
    \begin{tabular}{c|c|c|cc|cc|c}
    \toprule
    \multirow{2}[2]{*}{\textbf{Model}} & \multirow{2}[2]{*}{\textbf{Backbone}} & \multirow{2}[2]{*}{\textbf{Extra}} & \multirow{2}[2]{*}{} & \multirow{2}[2]{*}{} & \multicolumn{2}{c|}{\textbf{wmAP}} & \multirow{2}[2]{*}{} \\
          &       &       &       &       & \textbf{rel} & \multicolumn{1}{c|}{\textbf{phr}} &  \\
    \midrule
    \midrule
    VCTree~\cite{tang2019vctree} & X101-F & -     & 33.91  & 74.08  & 34.16  & 33.11  & 40.21  \\
    G-RCNN~\cite{yang2018graph_rcnn} & X101-F & -     & 34.04  & 74.51  & 33.51  & 34.21  & 41.84  \\
    Motifs~\cite{zellers2018neuralmotif} & X101-F & -     & 32.68  & 71.63  & 29.91  & 31.59  & 38.93  \\
    Unbiased~\cite{tang2020unbiasedSG} & X101-F & -     & 35.47  & 69.30  & 30.74  & 32.80  & 39.27  \\
    GPS-Net~\cite{lin2020gps} & X101-F & -     & 35.26  & 74.81  & 32.85  & 33.98  & 41.69  \\
    RelDN~\cite{zhang2019RelDN} & R101  & -     & 36.80  & 72.75  & 29.87  & 30.42  & 38.67  \\
    BGNN~\cite{li2021BGNN}  & R101  & -     & 39.41  & 74.93  & 31.15  & 31.37  & 40.00  \\
    SGTR~\cite{Li2021SGTR}  & R101  & -     & 42.61  & 59.91  & 36.98  & 38.73  & 42.28  \\
    RelTR~\cite{cong2022RelTR} & R50   & -     & -     & 64.47  & 34.17  & 37.44  & 41.54  \\
    \midrule
    \rowcolor{mygray} RLIPv2-ParSeD & R50\textsuperscript{*}   & -  & 44.58  & 58.04  & 43.30  & 43.12  & 46.18  \\
    \rowcolor{mygray}       & R50\textsuperscript{*}   & -  & 44.88  & 60.20  & 44.73  & 43.36  & 47.28  \\
    \rowcolor{mygray}       & R50\textsuperscript{\dag}   & - & 45.59  & 61.15  & 45.71  & 43.73  & 48.01  \\
    \rowcolor{mygray}       & R50   &  \textbf{(i)} & 50.42  & 63.35  & 47.65  & 45.23  & 49.82  \\
    \rowcolor{mygray}       & R50   &  \textbf{(ii)} & \textbf{52.07}  & 64.53  & 49.14  & \textbf{46.14}  & 51.01  \\
    \rowcolor{mygray}       & R50   &  \textbf{(iii)} & 51.31  & \textbf{65.99}  & \textbf{49.54}  & 45.71  & \textbf{51.30} \\
    \rowcolor{mygray}       & Swin-T &  \textbf{(iii)} & \textbf{59.61} & \textbf{68.81}  & \textbf{52.70}  & \textbf{48.01}  & \textbf{54.05} \\
\rowcolor{mygray}  \multirow{-7}[1]{*}{RLIPv2-ParSeDA}     & Swin-L   &  \textbf{(iii)} & \textbf{64.72}  & \textbf{72.49}  & \textbf{56.38}  & \textbf{50.70}  & \textbf{57.34} \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{Comparisons with previous methods on Open Images v6 SGG benchmark.} X101-F denote ResNeXt-101 FPN~\cite{xie2017resnext}. \textsuperscript{*} and \textsuperscript{\dag} denote ImageNet pretrained and COCO object detection pre-trained. ``\textbf{Extra}'' denotes extra relations adopted from \textbf{(i)} VG, \textbf{(ii)} VG+COCO and \textbf{(iii)} VG+COCO+O365.}
    \vspace{-.2cm}
  \label{tab:SOTA_SGG}
\end{table}




\textbf{Comparisons of different relation tagging strategies.} 
We compare R-Tagger with other two methods:
\textbf{(i)} the greedy matching algorithm~\cite{zhong2021scene_graph_language};
\textbf{(ii)} the CLIP~\cite{radford2021CLIP} tagging method.
Specifically, to tag relations for a given SO region pair with a candidate relation text, the CLIP tagging method clips the minimum bounding box of the SO region pair, creates two prompts (``a photo of \{subject\} \{relation\} \{object\}'' and ``a photo of \{subject\} not interacting with \{object\}'' are adopted as positive and negative prompts.), and performs zero-shot prediction.
If the {\rm softmax} probability of the positive prompt is greater than a pre-defined threshold, we tag this relation text to this SO region pair.
(We traverse the threshold and find the optimal one for the CLIP tagging method, as detailed in the Appendix.)
We also ablate on the ``overlap'' prior~\cite{zhong2021scene_graph_language} to observe whether a given tagging method relies on strong prior knowledge to filter out false positive relations.
The results are shown in~\cref{tab:tagging_strategy}.
From this table, we conclude that greedy matching and CLIP tagging method generate a significant number of low-quality non-overlapped triplets.
Thus, the ``overlap'' prior is essential for them.
R-Tagger, however, suffers a slight performance drop when using the ``overlap'' prior, indicating that R-Tagger generates more reliable non-overlapped triplets.
Besides, although CLIP is pre-trained on a massive quantity of language-image pairs, it struggles in recognizing relations, which R-Tagger is expert in.



\textbf{Comparisons of different caption types.}
Previous experiments demonstrate that oracle COCO captions can benefit pre-training.
However, most datasets lack such high-quality captions.
Thus, BLIP provides an alternative for caption generation.
We conduct experiments on various caption types, as shown in~\cref{tab:caption_type}.
From this table, we can observe that:
\textbf{(i)} BLIP-generated captions achieve a decent performance compared to oracle captions, indicating the practicality of adopting generated captions for pseudo-labelling.
\textbf{(ii)} If we compare BLIP model with different parameters (COCO Caption fine-tuned or only pre-trained), we can see that the fine-tuned model generates better captions with more boost on the \textit{Rare} set (). 
We conjecture that the caption quality of the curated style dataset COCO Caption is better than pre-training captions harvested from the web. 
\textbf{(iii)} By adopting generated captions, we can increase the number of captions per image, thus diversifying the relations contained in captions and boost RLIPv2 ().
Besides, although datasets like Conceptual Caption~\cite{sharma2018CC3M} (CC3M) also provide captions, the quantity of captions (average one caption per image) is not enough to describe many relations in an image.
In comparison, our pipeline can work without caption annotation while performing better.



{\renewcommand{\arraystretch}{0.9}
\begin{table}[t]
\scriptsize
  \setlength{\tabcolsep}{4pt}
  \centering
    \begin{tabular}{c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{Backbone} & \textbf{UC-RF} & \textbf{UC-NF} \\
    \midrule
    \midrule
    VCL~\cite{hou2020VCL} & ResNet-50 & 10.06 / 24.28 / 21.43 & 16.22 / 18.52 / 18.06 \\
    ATL~\cite{hou2021ATL} & ResNet-50 & 9.18 / 24.67 / 21.57 & 18.25 / 18.78 / 18.67 \\
    FCL~\cite{hou2021FCL} & ResNet-50 & 13.16 / 24.23 / 22.01 & 18.66 / 19.55 / 19.37 \\
    GEN-VLKT~\cite{GEN_VLKT} & ResNet-50 & 21,36 / 32.91 / 30.56 & \textbf{25.05} / 23.38 / 23.71 \\
    RLIPv1-ParSeD~\cite{Yuan2022RLIP} &  ResNet-50 & 16.43 / 30.59 / 27.76 &  16.99 / 24.71 / 22.93 \\
    RLIPv1-ParSe~\cite{Yuan2022RLIP} & ResNet-50 & 19.19 / 33.35 / 30.52 & 20.27 / 27.67 / 26.19 \\
    \midrule
RLIPv2-ParSeDA & ResNet-50 & \textbf{21.45} / \textbf{35.85} / \textbf{32.97} & 22.81 / \textbf{29.52} / \textbf{28.18} \\
    RLIPv2-ParSeDA & Swin-T & \textbf{26.95} / \textbf{39.92} / \textbf{37.32} & 21.07 / \textbf{35.07} / \textbf{32.27} \\
    RLIPv2-ParSeDA & Swin-L & \textbf{31.23} / \textbf{45.01} / \textbf{42.26} & 22.65 / \textbf{40.51} / \textbf{36.94} \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{Comparisons with methods on HICO-DET under UC-RF and UC-NF settings.} We adopt ResNet-50 as the backbone. Results are reported on \textit{Unseen}/\textit{Seen}/\textit{Full} sets.}
    \vspace{-.2cm}
  \label{tab:zero-shot_UC-NF_UC-RF}
\end{table}}

{\renewcommand{\arraystretch}{0.9}
\begin{table}[t]
\scriptsize
  \setlength{\tabcolsep}{4pt}
  \centering
    \begin{tabular}{c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{Backbone} & \textbf{1\% Data} & \textbf{10\% Data} \\
    \midrule
    \midrule
    RLIPv1-ParSeD~\cite{Yuan2022RLIP} & ResNet-50 & 16.22 / 18.92 / 18.30 & 15.89 / 23.94 / 22.09 \\
    RLIPv1-ParSe~\cite{Yuan2022RLIP} & ResNet-50 & 17.47 / 18.76 / 18.46 & 20.16 / 23.32 / 22.59 \\
    \midrule
    RLIPv2-ParSeDA & ResNet-50 & \textbf{22.13} / \textbf{24.51} / \textbf{23.96} & \textbf{23.28} / \textbf{30.02} / \textbf{28.46} \\
    RLIPv2-ParSeDA & Swin-T & \textbf{24.26} / \textbf{28.92} / \textbf{27.85} & \textbf{28.31} / \textbf{32.93} / \textbf{31.87} \\
    RLIPv2-ParSeDA & Swin-L & \textbf{31.89} / \textbf{32.32} / \textbf{32.22} & \textbf{34.75} / \textbf{38.27} / \textbf{37.46} \\
\bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{Comparisons on HICO-DET under few-shot settings.} Results are reported on \textit{Rare}/\textit{Non-Rare}/\textit{Full} sets. 
}
    \vspace{-.2cm}
  \label{tab:few-shot_transfer}
\end{table}}

{\renewcommand{\arraystretch}{0.9}
\begin{table*}[t]
  \centering
\small
  \setlength{\tabcolsep}{4pt}
    \begin{tabular}{c|c|c|cc|cc}
    \toprule
    \multirow{2}[2]{*}{\textbf{Model}} & \multirow{2}[2]{*}{\textbf{Backbone}} & \multirow{2}[2]{*}{\textbf{Extra Relations}} & \multicolumn{2}{c}{\textbf{HICO-DET}} & \multicolumn{2}{|c}{\textbf{V-COCO}} \\
          &       &       & \textbf{Zero-shot (NF)} & \textbf{Fully-finetuning} &  &  \\
    \midrule
    \midrule
    InteractNet~\cite{gkioxari2018InteractNet} & R50-FPN & -     & -     & 7.16 / 10.77 / 9.94 & 40.0  & - \\
    UnionDet~\cite{kim2020uniondet} & R50-FPN & -     & -     & 11.72 / 19.33 / 17.58 & 47.5  & 56.2  \\
    PPDM~\cite{liao2020ppdm}  & HG104 & -     & -     & 13.97 / 24.32 / 21.94 & -     & - \\
    HOTR~\cite{kim2021hotr}  & R50   & -     & -     & 17.34 / 27.42 / 25.10 & 55.2  & 64.4  \\
    QPIC~\cite{tamura2021qpic}  & R50   & -     & -     & 21.85 / 31.23 / 29.07 & 58.8  & 61.0  \\
    OCN~\cite{Yuan2022OCN}   & R50   & -     & -     & 25.56 / 32.51 / 30.91 & 64.2  & 66.3  \\
    CDN~\cite{zhang2021CDN}   & R50   & -     & -     & 27.39 / 32.64 / 31.44 & 61.7  & 63.8  \\
GEN-VLKT~\cite{GEN_VLKT} & R50   & -     & -     & 29.25 / 35.10 / 33.75 & 62.4  & 64.5  \\
    QAHOI~\cite{chen2021qahoi} & Swin-L\textsuperscript{*} & -     & -     & 29.80 / 37.56 / 35.78 & -     & - \\
    UniVRD~\cite{zhao2023UniVRD} & ViT-H/14\textsuperscript{\dag} & - & - & 31.65 / 39.99 / 38.07 & 65.8  &  66.9 \\
    RLIPv1-ParSeD~\cite{Yuan2022RLIP} & R50   & VG    & 11.20 / 14.73 / 13.92 & 24.67 / 32.50 / 30.70 & 61.7  & 63.8  \\
    RLIPv1-ParSe~\cite{Yuan2022RLIP} & R50   & VG    & 15.08 / 15.50 / 15.40 & 26.85 / 34.63 / 32.84 & 61.9  & 64.2  \\
    \midrule
    \rowcolor{mygray} RLIPv2-ParSeDA & R50   & VG    & 13.03 / 14.98 / 14.53 & 27.01 / 35.21 / 33.32 &   63.0    &  65.1 \\
    \rowcolor{mygray} RLIPv2-ParSeDA & R50   & VG+COCO & 15.00 / 16.60 / 16.23 & 27.89 / 35.27 / 33.57 &    64.5   &  66.7\\
    \rowcolor{mygray} RLIPv2-ParSeDA & R50   & VG+COCO+O365 & \textbf{19.64} / \textbf{17.24} / \textbf{17.79} & \textbf{29.61} / \textbf{37.10} / \textbf{35.38} &   \textbf{65.9}    & \textbf{68.0} \\
    \rowcolor{mygray} RLIPv2-ParSeDA & Swin-T & VG+COCO+O365 & \textbf{21.24} / \textbf{19.47} / \textbf{19.87} &  \textbf{33.66} / \textbf{40.07} / \textbf{38.60} &  \textbf{68.8}  &  \textbf{70.8} \\
    \rowcolor{mygray} RLIPv2-ParSeDA & Swin-L & VG+COCO+O365 &   \textbf{27.97} / \textbf{21.90} / \textbf{23.29}    &  \textbf{43.23} / \textbf{45.64} / \textbf{45.09}  &  \textbf{72.1}   & \textbf{74.1} \\
    \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\small \textbf{Comparisons with previous methods on HICO-DET and V-COCO.} Results on HICO-DET are reported on \textit{Rare}/\textit{Non-Rare}/\textit{Full} sets. R50 and HG denote ResNet-50~\cite{he2016resnet} and Hourglass~\cite{newell2016hourglass}. 
    * denotes the backbone is pre-trained with  resolution, while others use .
    \dag \ indicates the backbone is pre-trained using LiT~\cite{zhai2022LiT}, then fine-tuned on Objects365, COCO and HICO with the objective of object detection.}
    \vspace{-.2cm}
  \label{tab:SOTA_HOI}
\end{table*}}





\textbf{The necessity of using captioners.}
By default, we use BLIP to generate relation texts. Another option is to query all possible SO pairs and select all possible relation texts from VG as it contains an enormous quantity of relations (36,515 kinds).
Then, we run R-Tagger with selected relation texts as  and all region pairs as .
The results are shown in~\cref{tab:necessity_of_BLIP}.
We can observe that selecting from VG generates low-quality candidates, harming performance.





\textbf{Model scaling and dataset scaling.}
Equipped with the labelling pipeline introduced above, we can scale RLIPv2 to larger models and datasets.
In~\cref{tab:model_dataset_scaling}, we adopt RLIPv2-ParSeDA as the base architecture and observe the benefit of scaling by zero-shot (NF) performance on HICO-DET.
In terms of data, adding COCO and Objects365 can both boost performance, and the benefit of adding data exhibits a  scaling trend~\cite{cherti2022reproducible_scaling_law}.
Models pre-trained with Objects365 consistently have better \textit{Rare} result, which we attribute to the distribution misalignment of Objects365 and HICO-DET~\cite{entezari2023role_of_pre-training_data}.
In terms of models, switching to stronger backbone models can improve the data efficiency at the cost of larger amounts of computation.
\hangjie{Regarding scaling experiments using RLIPv2-ParSeD, we present it in the Appendix.}







\subsection{Comparisons with State-of-the-Arts}


\textbf{Scene graph generation.}
We compare RLIPv2 series models with previous methods on Open Images v6 in~\cref{tab:SOTA_SGG}.
We also report mean Recall@50 () to better show the usefulness of RLIPv2.
Our findings suggest that
\textbf{(i)} with the assistance of DDETR family models, RLIPv2 can serve as a strong baseline without any pre-training;
\textbf{(ii)} naive object detection pre-training can boost the performance to some extent (), while pre-training on VG can further boost the performance especially on  ();
\textbf{(iii)} adding pseudo-labelled relation annotations in pre-training or switching to stronger backbones both contribute to better performance.
However, the boost of adding Objects365 is negligible.
We attribute this to the distribution discrepancy of Objects365 and Open Images v6.



\textbf{HOI Detection under UC-NF and UC-RF settings.}
We report results in~\cref{tab:zero-shot_UC-NF_UC-RF} on unseen combinations (UC) under UC-RF and UC-NF settings following~\cite{hou2020VCL,hou2021ATL}.
We only fine-tune for 10 epochs under UC-NF setting.
Our method outperforms previous methods except on one metric.
We attribute this to the strong transferability of CLIP features that GEN-VLKT adopts.
\hangjie{Regarding experiments using RLIPv2-ParSeD, we present it in the Appendix.}

\textbf{Few-shot HOI Detection.} 
We follow~\cite{Yuan2022RLIP} to only fine-tune 10 epochs on partial data (1\% and 10\%), results of which are shown in~\cref{tab:few-shot_transfer}.
We can observe significant improvements upon all metrics by scaling up pre-training compared with previous methods.
This improves the practicality of RLIPv2 in low-data scenarios.
\hangjie{Regarding experiments using RLIPv2-ParSeD, we present it in the Appendix.}

\begin{figure}[t]
\centering
\vspace{-0.4cm}
\includegraphics[width=0.44\textwidth]{ICCV2023/figures/pseudo-labels_visualization.pdf}
\vspace{-0.2cm}
\caption{\small \textbf{(a) Visualization of pseudo-labelled relations on COCO}. Left column: R-Tagger; right column: CLIP tagging method. \textbf{(b) Visualization of failure cases of R-Tagger.}}
\vspace{-0.4cm}
\label{fig:pseudo-labels_visualization}
\end{figure}


\textbf{HOI detection under fully-finetuning and zero-shot (NF) settings.}
We compare the performance of RLIPv2 series models with previous methods on HICO-DET and V-COCO in~\cref{tab:SOTA_HOI}.
We can observe from the table that
\textbf{(i)} dataset and model scaling can both boost the final performance on two datasets;
\textbf{(ii)} on HICO-DET, the benefit of pre-training is more prominent on zero-shot than fully-finetuning, especially on the \textit{Rare} set.
\hangjie{Regarding experiments using RLIPv2-ParSeD, we present it in the Appendix.}















\subsection{Qualitative Analysis}

\textbf{Comparisons of relation tagging methods.}
We visualize three examples to compare the quality of pseudo-labelled relations by R-Tagger and CLIP tagging method in~\cref{fig:pseudo-labels_visualization}(a).
Generally, CLIP is more object-centric and position-agnostic, and thus struggles in discriminating relations.
It tends to tag relations as long as the subject and object have strong co-occurrence priors.
However, R-Tagger tags more reasonable relations.

\textbf{Failure cases of R-Tagger.}
Recognizing relations is challenging, especially in complex scenes.
In~\cref{fig:pseudo-labels_visualization}(b), we present three examples of R-Tagger's failure cases.
In particular, we observe failure cases when the scene contains multiple similar subjects or objects.









%
