\documentclass[fleqn,10pt]{SelfArx} 

\usepackage[english]{babel} 

\usepackage{lipsum} 

\usepackage{afterpage}

\usepackage{amsmath}

\usepackage{graphicx}

\makeatletter
\newcommand{\subalign}[1]{\vcenter{\Let@ \restore@math@cr \default@tag
    \baselineskip\fontdimen10 \scriptfont\tw@
    \advance\baselineskip\fontdimen12 \scriptfont\tw@
    \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
    \lineskiplimit\lineskip
    \ialign{\hfil&\hfil\crcr
      #1\crcr
    }}}
\makeatother


\setlength{\columnsep}{0.55cm} \setlength{\fboxrule}{0.75pt} 



\definecolor{color1}{RGB}{0,0,90} \definecolor{color2}{RGB}{0,20,20} 



\usepackage{hyperref} \hypersetup{hidelinks, colorlinks=true, breaklinks=true, urlcolor=blue, citecolor=blue, linkcolor=blue, bookmarksopen=false, pdftitle={Title},pdfauthor={Author}}



\JournalInfo{Rev 9 - 11/18/2021} \Archive{First Rev 10/15/2021} 

\PaperTitle{Local Multi-Head Channel Self-Attention for Facial Expression Recognition} 

\Authors{Roberto Pecoraro\textsuperscript{1}, Valerio Basile\textsuperscript{2}, Viviana Bono\textsuperscript{3}, Sara Gallo\textsuperscript{4}} \affiliation{\textsuperscript{1}\textit{Department of Computer Science, University of Turin, Italy 10124} - \texttt{roberto.pecoraro@unito.it}; \texttt{robertopecoraro@live.com}} \affiliation{\textsuperscript{2}\textit{Department of Computer Science, University of Turin, Italy 10124} - \texttt{valerio.basile@unito.it}} \affiliation{\textsuperscript{3}\textit{Department of Computer Science, University of Turin, Italy 10124} - \texttt{bono@di.unito.it}} \affiliation{\textsuperscript{4}\textit{Department of Computer Science, University of Turin, Italy 10124} - \texttt{sara.gallo@unito.it}} 

\Keywords{} \newcommand{\keywordname}{Keywords} 



\Abstract{Since the Transformer architecture was introduced in 2017 there has been many attempts to bring the self-attention paradigm in the field of computer vision. In this paper we propose a novel self-attention module that can be easily integrated in virtually every convolutional neural network and that is specifically designed for computer vision, the \textit{LHC}: Local (multi) Head Channel (self-attention). \textit{LHC} is based on two main ideas: first, we think that in computer vision the best way to leverage the self-attention paradigm is the channel-wise application instead of the more explored spatial attention and that convolution will not be replaced by attention modules like recurrent networks were in NLP; second, a local approach has the potential to better overcome the limitations of convolution than global attention. With \textit{LHC-Net} we managed to achieve a new state of the art in the famous FER2013 dataset with a significantly lower complexity and impact on the ``host'' architecture in terms of computational cost when compared with the previous SOTA.}



\begin{document}

\flushbottom 

\maketitle 

\tableofcontents 

\thispagestyle{empty} 



\section*{Introduction} 

\addcontentsline{toc}{section}{Introduction} 

The aim of this work is to explore the capabilities of the self-attention paradigm in the context of computer vision, more in particular, in the facial expression recognition. In order to do that we designed a new channel self-attention module, the \textit{LHC}, which is thought as a processing block to be integrated into a pre-existing convolutional architecture. 

It inherits the basic skeleton of the self-attention module from the very well known \textit{Transformer} architecture by Vaswani et al. \cite{Transformer} with a new design thought to improve it and adapt it as an element of a computer vision pipeline. We call the final architecture \textit{LHC-Net}: Local (multi-)Head Channel (self-attention) Network.
In the context of a wider research focused on the recognition of human emotions we tested \textit{LHC-Net} on the FER2013 dataset, a dataset for facial emotion recognition \cite{FER2013}. FER2013 was the object of a 2013 Kaggle competition. It is a dataset composed of  grey-scale  images of faces classified in  categories: anger, disgust, fear, happiness, sadness, surprise, neutral. The dataset is divided in a training set ( images), a public test set ( images), which we used as validation set, and a private test set ( images), usually considered the test set for final evaluations. FER2013 is known as a challenging dataset because of its noisy data with a relatively large number of non-face images and misclassifications. It is also strongly unbalanced, with only  samples in the less populated category, ``Disgust'', and  samples in the more populated category, ``Happiness'':

\noindent\includegraphics[]{FER2013DIST3}

\begin{figure*}[ht]\centering \includegraphics[width=\linewidth]{LHC_Net}
\caption{Five \textit{LHC} modules integrated into a \textit{ResNet34v2} architecture. Every module features a residual connection to obtain an easier integration, especially when pre-training is used for the backbone architecture.}
\end{figure*}

\textit{LHC-Net} should be generally considered as a family of neural network architectures having a backbone represented by a convolutional neural network in which one or more \textit{LHC} modules are integrated. More specifically, in this paper, we will refer to \textit{LHC-Net} as a \textit{ResNet34} \cite{resnet} having integrated  \textit{LHC} modules as shown in Figure 1.

For this reason \textit{LHC-Net} is a general purpose computer vision architecture since it doesn't feature any specific solution designed for facial expression recognition. 

In our experiments, \textit{LHC-Net} achieved a classification accuracy on the private test set of FER2013 which is, to the best of our knowledge (and accordingly with the paperswithcode's \href{https://paperswithcode.com/sota/facial-expression-recognition-on-fer2013}{\texttt{leaderboard}} at the time this paper is being written), the current single deep learning model state of the art both with and without test time augmentation, with a computational cost which is only a fraction of the previous \textit{SOTA} architecture.\textit{}



\section{Related work}
\subsection{Attention}
The attention paradigm became very popular in the last few years with a large variety of mechanics and implementations in both NLP and computer vision scenarios. There are two main attention paradigms: either we pay attention to the data with the idea of enhancing the most meaningful aspects or we can try to exploit the inner relationships within these aspects in order to produce a more meaningful representation of the data. The latter approach is usually called \textit{self-attention} because in some way the data pays attention to itself. 

The first approach was introduced in  by Bahdanau et al. \cite{bahdanau2014neural} an updated by Luong et al. in 2015 \cite{luong2015effective}. The proposed solutions were used in neural machine translation and integrated in a classic ``seq to seq'' encoder/decoder architecture in which the decoder learns what are the outputs of the encoder where to pay more attention dinamically. 
Self-attention was introduced in  by Vaswani et al. \cite{Transformer} (again for neural machine translation) and it is considered by many the greatest breakthrough technology in AI since the backpropagation was introduced in  \cite{rumelhart1986learning}. It fully replaced the previous state of the art technologies, the recurrent and convolutional networks, in NLP.

Since then there has been many attempts to bring self-attention in computer vision but, as of now, with only partial success. As opposite as the NLP case, in computer vision self-attention struggles to outperform the SOTA computer vision architectures like the classical \textit{Inception} \cite{inception},  \textit{ResNet} \cite{resnet}, \textit{VGG}, \cite{vgg} etc.

In computer vision there are several type of attentions paradigms, for clarity from now on we will use the following nomenclature:

\begin{itemize}
  \item \textbf{Global Attention}: usually it is only a module used before another model with the idea to enhance the important parts of an image and to ignore the rest of the image.
  \item \textbf{Spatial Attention}: the attention modules focus on single pixels or areas of the feature maps.
  \item \textbf{Channel Attention}: the attention modules focus on entire feature maps.
  \item \textbf{Self-Attention}: the attention tries to find relationships between different aspects of the data.
  \item \textbf{Stand-Alone Attention}: the architecture is aimed at fully replacing the convolutional blocks and defining a new processing block for computer vision based on some attention mechanism (mostly self-attention).
\end{itemize}

Xu et al. proposed a global attention module for medical image classification \cite{xu2020novel}, this module pre-processes images enhancing important areas pixel by pixel before feeding them into a standard convolutional neural network. This kind of pre-processing is thought to make more robust the following convolution processing. It could be associated to the one proposed by Jaderberg et al. \cite{jaderberg2015spatial} which attempts to compensate for the lack of rotation/scaling invariance of the convolution paradigm. The proposed module learns a sample-dependant affine transformation to be applied to images in order to make them centered and properly scaled/rotated. 

The channel approach we propose in this paper, despite being relatively unexplored in our self-attention mode, is instead very popular when associated with vanilla attention. Hu et al. proposed the \textit{SE-Net} (Squeeze and Excitation) \cite{hu2018squeeze}, a simple and effective module which enhances the more important features of a convolutional block. Squeeze and excitation lately became a key module in the very popular \textit{Efficient-Net} by Tan et al. \cite{tan2019efficientnet}  which set a new SOTA on several benchamrk datasets. 
Similarly Woo et al. proposed the \textit{CBAM} (Convolutional Block Attention Module), a sequential module composed of a spatial and a channel attention sub-modules \cite{woo2018cbam}.
There are other examples of channel and spatial vanilla attention: \textit{ECA-Net} (Efficient Channel Attention) by Wang et al. \cite{wang2020ecanet} is a new version of Squeeze and Excitation; \textit{SCA-CNN} (Spatial and Channel-wise attention) proposed by Chen et al. \cite{chen2017sca} combines both spatial and channel vanilla attention for image captioning. \textit{URCA-GAN} by Nie et al. \cite{nie2021urca} is a GAN (Generative Adversarial Network) featuring a residual channel attention mechanism thought for image-to-image translation.

Channel attention wasn't used only in vanilla approaches; similarly to our architecture Fu et al., Liu et al. and Tian et al. proposed self-attention architectures \cite{fu2019dual}, \cite{liu2021scsa}, \cite{tian2020triple} respectively for scene segmentation, feature matching between pairwise images and video segmentation. The main differences between these modules and ours are the following:

\begin{itemize}
  \item in all of them channel attention always has a secondary role and there's always a spatial attention sub-module with a primary role
  \item in all of them the crucial multi-head structure is lacking
  \item all of them implement channel attention as a ``passive'' non-learning module
  \item none of them integrates our local spatial behavior for channel attention
  \item none of them integrates our dynamic scaling which is very specific of our architecture.
\end{itemize}

As opposite as channel self-attention, spatial self-attention is widely explored, in most cases with the ambitious goal of totally replacing the convolution in computer vision, just like Vaswani's Transformer made \textit{LSTM} obsolete. 
Bello et al. proposed an attention-augmented convolutional network \cite{bello2019attention} in which Vaswani's self-attention is straightforwardly applied to pixels representations and integrated in a convolutional neural network.

Similarly Wu et al. proposed the Visual Transformer \cite{wu2020visual}, an architecture in which many ``tokens'' (i.e., image sections derived from a spatial attention module) are feeded into a transformer. The entire block is integrated in a convolutional network.
The Visual Transformer is inspired by \textit{ViT}, the Vision Transformer by Dosovitskiy et al. \cite{dosovitskiy2020image}, \textit{ViT} is a stand-alone spatial self-attention architecture in which the transformer's inputs are patches extracted from the tensor image. Previous attempts to implement stand-alone spatial attention were done by Ramachandran et al. \cite{ramachandran2019stand} and Parmar et al. \cite{parmar2018image}. Spatial self-attention was also used in GANs by Zhang et al. with their \textit{SAGAN} (Self-Attention Generative Adversarial Network) \cite{zhang2019self}.

More recently Liu et al. and Dai et al. proposed other two spatial stand-alone self-attention architectures, respectively the Swin Transformer \cite{liu2021swin} and the \textit{CoAtNet} \cite{dai2021coatnet} (depthwise Convolution and self-Attention). We can think at stand-alone architectures as attempts of rethinking convolution and replace it in a way able to address its limitations. Many improvements of convolution were proposed, mainly to make them invariant for more general transformations than translations, such as the Deep Simmetry Network proposed by Gens et al. \cite{gens2014deep} or the Deformable Convolutional Network by Dai et al. \cite{dai2017deformable}.

Both \textit{ViT} and \textit{CoAtNet} can be considered the current state of the art on Imagenet but they outperform Efficient Net by only a very small margin \cite{pham2021meta} and at the price of a complexity up to  and of a pre-training on the prohibitive JFT-3B dataset containing 3 billions of images.

These are good reasons for considering convolution not yet fully replaceable by self-attention in computer vision. But the main reason we didn't pursue the goal of a stand-alone architecture is that we don't believe in the main assumption spatial self-attention is based on in computer vision. Self-attention had a great success in NLP because it eventually exploited the inner relationships between the words in a phrase which sequential approaches were not able to model effectively. Every word in a sentence has a strong well defined relationship with any other word in that phrase, and they finally form a complex structure composed of these relationships. But, for instance, if we take a picture of a landscape we see no reason to believe that such a relationship could exist between a rock on the ground and a cloud in the sky or, even more extremely, between two random pixels, at least not in the same way the subject of a phrase is related to its verb.
On the other hand this observation does not hold for the features extracted from a picture, and the best way we know, so far, to extract features from a picture is convolution. These are the main reasons we decided to further explore channel self-attention in synergy with convolution, not as a stand-alone solution.

 






\subsection{FER2013}
As mentioned before FER2013 is a challenging dataset for facial expressions recognition. 
As reported by Goodfellow et al. even human accuracy on FER2013 is limited to  \cite{FER2013}. Tang et al. \cite{FER2013} successfully used linear support vector machines reaching  accuracy.
Minaee et al. achieved  accuracy using a convolutional neural network augmented with a global spatial attention module \cite{minaee2021deep}. Pramerdorfer et al. experimented several architectures on FER2013 reaching  accuracy with \textit{Inception},  with \textit{ResNet} and  with \textit{VGG} \cite{pramerdorfer2016facial}. Khanzada et al. managed to achieve  accuracy with \textit{SE-ResNet50} and  with \textit{ResNet50} \cite{khanzada2020facial}. Khaireddin et al. reached  accuracy using \textit{VGG} with a specific hyper-parameters fine tuning \cite{khaireddin2021facial}.
Pham et al. designed the \textit{ResMaskingNet} which is a \textit{ResNet} backbone augmented with a spatial attention module based on the popular \textit{U-Net}, a segmentation network mostly used in medical image processing. \textit{ResMaskingNet} achieves the remarkable accuracy of . Pham et al. also reported that an ensemble of  convolutional neural networks, including \textit{ResMaskingNet}, reaches  accuracy \cite{resmaskingnet}.

\section{\textit{LHC-Net}}
As already mentioned and shown in Figure  our \textit{LHC} module can be integrated in virtually any existing convolutional architecture, including of course \textit{AlexNet} \cite{krizhevsky2012imagenet}, \textit{VGG} \cite{vgg}, \textit{Inception} \cite{inception} and \textit{ResNet} \cite{resnet}. 

In this section we will give a detailed mathematical definition of \textit{LHC} as shown in Figure , starting from a generic tensor and forward propagating it through the entire architecture.
\subsection{Architecture}
We first need to define the model hyper-parameters: let  be the number of local heads,  the kernel size of the convolution we will use to process the value tensor,  the pool size used in average pooling and max pooling blocks,  the embedding dimension of every head and  a constant we will need in the dynamic scaling module.

Let  be a generic input tensor, where ,  and  are respectively the height, width and number of channels, with the constraint that  must be divisible by .

We define ,  and  as follows:



\begin{figure*}[ht]\centering \includegraphics[width=\linewidth]{LHC_Module2}
\caption{The \textit{LHC} module in its more general multi-head form. Image tensors of shape  are in pale blue, when reshaped/processed they are in dark blue. The processing units are in violet.}
\end{figure*}

where the pooling operators subscripts are respectively the pool size and the stride and the convolution operator subscripts are respectively the kernel size and the stride.

Now we want to split the tensors ,  and  into  horizontal slices and reshape the resulting tensors as follows:


Every head is deputed to process a triplet  then we have  separate fully connected layers with linear output and weights/biases: 
, .

Queries and keys will share the same dense blocks resulting in  embeddings as follows:



Or, more shortly (from now on we will omit the head logic quantifier):


Now we can compute the attention scores through usual transposition and matrix product:



Dynamic scaling produces a channel-wise learned scaling (not dependent from heads) through averaging the scores and passing them through another fully connected layer with sigmoid activation and weights/biases , :


12pt]
             & T_{h}^{i} = \operatorname{Sig}\left(\sum_{t=1}^{C}\tilde{S}_{h}^{t}w_{2}^{t,i} + b_{2}^{i}\right) \in \mathbb{R} \, \, \, \, \, \,  \forall i =1, ..., C\12pt]
             & \mathbf{W}_{h} = \operatorname{Softmax}_{dim=2}(\mathbf{N}_{h}) \in \mathbb{R}^{C,C}

             & \hspace{2cm} \mathbf{A}_{h} = \mathbf{W}_{h} \cdot  \mathbf{v}_{h} \in \mathbb{R}^{C,(HxW)/n}

             & \mathbf{y} = \operatorname{SplitHeads}^{-1}([\mathbf{A}_{1}, \mathbf{A}_{2}, ..., \mathbf{A}_{n}]) \in \mathbb{R}^{H,W,C}
\mathbf{N} = \frac{\mathbf{S}}{\sqrt{d}}
		   & \lim_{\alpha \to +\infty}\frac{\displaystyle \mathrm{e}^{\alpha x_{i}}}{\displaystyle \sum_{j=1}^{n}\mathrm{e}^{\alpha x_{j}}} = \begin{cases} 1, & \mathrm{if} \hspace{0.1cm} x_{i} = \operatorname{max}(\mathbf{x}) \\ 0, & \mathrm{otherwise} \end{cases} \\
             & \lim_{\alpha \to 0^{+}}\frac{\displaystyle \mathrm{e}^{\alpha x_{i}}}{\displaystyle \sum_{j=1}^{n}\mathrm{e}^{\alpha x_{j}}} = \frac{\displaystyle 1}{\displaystyle \sum_{j=1}^{n}1} = \frac{1}{n}\\
             & \frac{\displaystyle \mathrm{e}^{x_{i_1}}}{\displaystyle \sum_{j=1}^{n}\mathrm{e}^{x_{j}}} < \frac{\displaystyle \mathrm{e}^{x_{i_2}}}{\displaystyle \sum_{j=1}^{n}\mathrm{e}^{x_{j}}} \Leftrightarrow 
                \frac{\displaystyle \mathrm{e}^{\alpha x_{i_1}}}{\displaystyle \sum_{j=1}^{n}\mathrm{e}^{\alpha x_{j}}} < \frac{\displaystyle \mathrm{e}^{\alpha x_{i_2}}}{\displaystyle \sum_{j=1}^{n}\mathrm{e}^{\alpha x_{j}}}

             & \hspace{1.55cm} \tilde{\mathbf{q}} = \mathbf{q} \cdot \mathbf{w}_{1} \in \mathbb{R}^{C,d} \nonumber \\
             & \hspace{1.55cm} \tilde{\mathbf{k}} = \mathbf{k} \cdot \mathbf{w}_{1} \in \mathbb{R}^{C,d} \nonumber

             & \hspace{1.55cm} G1 = \frac{HWd}{3(HW + d)2} \\
             & \hspace{1.55cm} G2 = \frac{HWd}{3(HWd)2} = \frac{1}{6}
 A + B + C = n 
             L1 &= \left(A\frac{\frac{HW}{n}d}{2(\frac{HW}{n} + d)} +  B\frac{\frac{HW}{n}d}{3(\frac{HW}{n} + d)2}\right)/(A + B)\\
             L2 &= \left(A\frac{\frac{HW}{n}d}{2(\frac{HW}{n}d)} +  B\frac{\frac{HW}{n}d}{3(\frac{HW}{n}d)2}\right)/(A + B) \nonumber \\
                 &= \left(\frac{A}{2} +  \frac{B}{6}\right)/(A + B)

             & L2 > G2 \Leftrightarrow \nonumber\\
             & \left(\frac{A}{2} +  \frac{B}{6}\right)/(A + B) > \frac{1}{6} \Leftrightarrow A > 0 \nonumber                 

             & \hspace{1.55cm} L2 \geq G2\\
             & \hspace{1.55cm} L2 = G2 \Leftrightarrow A = 0                

             &L1 > G1 \Leftrightarrow \nonumber \\
             &\frac{\left(A\frac{\frac{HW}{n}d}{2(\frac{HW}{n} + d)} +  B\frac{\frac{HW}{n}d}{2(\frac{HW}{n} + d)3}\right)}{(A + B)}  > \frac{HWd}{3(HW + d)2} \Leftrightarrow \nonumber\\
             &\left(A\frac{\frac{1}{n}}{2\frac{HW}{n}(1 + \frac{1}{2})} +  B\frac{\frac{1}{n}}{6\frac{HW}{n}(1 + \frac{1}{2})}\right)  > \frac{A+B}{6HW(1 + \frac{1}{2n})} \Leftrightarrow \nonumber \\
             &\left(A\frac{1}{2(1 + \frac{1}{2})} +  B\frac{1}{6(1 + \frac{1}{2})}\right)  > \frac{A+B}{6(1 + \frac{1}{16})} \Leftrightarrow \nonumber \\
             &\left(\frac{A}{3} + \frac{B}{9}\right)  > \frac{16A+16B}{102} \Leftrightarrow \nonumber \\
             &\left(3A + B\right)  > \frac{144A+144B}{102} \Leftrightarrow A > 0.26B\nonumber


In this case the combinations ,  and ,  give an advantage to global single head. Every other possible combination do the opposite as shown in this figure:

\noindent\includegraphics[]{grid1}

It appears clear that local heads have an advantage over global heads in any real-world application. For example in FER2013 it is unlikely that a feature extracted from a face could appear anywhere in the picture. For example eyebrows will be almost always in the upper section of the picture.

This, of course, has not the ambition to be a rigorous proof of the goodness of local heads over global head, it is only a qualitative analysis giving an encouraging view.

\section{Experiments}

As mentioned we mainly focused on using \textit{LHC} in conjunction with a pre-trained backbone, the \textit{ResNet34v2}. The training process consisted in training a \textit{ResNet34v2} (with Imagenet pre-training initialization) on FER2013, then adding  \textit{LHC} modules as shown in Fig.1 and further training the entire architecture. The Idea was designing modules with a small impact on the ``host'' network similarly at the approach of the Squeeze and Excitation modules \cite{hu2018squeeze}. In other words our main goal was to test the ability of \textit{LHC} to give an extra performance boost to an existing good performing model. Secondarily we also tested \textit{LHC-Net} as a stand-alone model trained (only Imagenet pre-training of the \textit{ResNet} part) from scratch and we obtained limited but very good results. In this section we will discuss the details of our training protocol and the experimental results.
\

\paragraph{Setup}

\

We rescaled the FER2013 images to  and converted them to RGB in order to match the resolution of Imagenet and make them compatible with the pre-trained \textit{ResNet}. For rescaling we used bilinear interpolation.
Then, in order to save RAM memory, we stored the entire training set as jpeg images accepting some neglectable quality loss and used TensorFlow Image Data Generator to feed the model during training. Saving images in jpeg format implies two different quality losses: the jpeg compression itself and the need to approximate the tensor images to be uint8 (bilinear interpolation in rescaling generate non integer values). To do that the tensors could be rounded or truncated. Considering that truncation is only a rounding with the input shifted of  and that this shifting makes the training set in FER2013 better matching the validation and test set average pixel value we proceeded with raw truncation.

The implementation details of \textit{ResNet} are reported in Fig.1 and the model parameters of the  \textit{LHC} blocks are the following:
\begin{center}
\begin{tabular}{||c | c | c | c | c | c||}  
 \hline
 \textbf{Block}  & \textbf{Heads} & \textbf{Dim} & \textbf{Pool} & \textbf{Scale} & \textbf{Ker}\\ 
 \hline
 \hline
\textbf{\textit{LHC}} &  &  &  & & \\
 \hline
 \hline
\textbf{\textit{LHC}} &  &  &  & & \\
 \hline
 \hline
\textbf{\textit{LHC}} &  &  &  & & \\
 \hline
 \hline
\textbf{\textit{LHC}} &  &  &  & & \\
 \hline
 \hline
\textbf{\textit{LHC}} &  &  &  & & \\
 \hline
\end{tabular}
\end{center}

We trained the model in a sequential mode with  training stages, using standard crossentropy loss, varying the data augmentation protocol, the batch size and the optimzer at every stage. Early stopping is performed on validation set.

\

\textbf{Stage1}:
\begin{center}
\begin{tabular}{||c c||}  
 \hline
 \textbf{Optimizer} & Adam, lr =  \\ 
 \hline
 \textbf{Batch Size} &  \\
 \hline
 \textbf{Patience} &  epochs \\
 \hline
 \textbf{Augmentation} &  deg. rot. \\
 \hline
\end{tabular}
\end{center}

\textbf{Stage2}:
\begin{center}
\begin{tabular}{||c c||}  
 \hline
 \textbf{Optimizer} & SGD, lr =  \\ 
 \hline
 \textbf{Batch Size} &  \\
 \hline
 \textbf{Patience} &  epochs \\
 \hline
                                 &  deg. rot. \\
  \textbf{Augmentation} &  h/v shift \\
                                 &  zoom \\
 \hline
\end{tabular}
\end{center}

\textbf{Stage3}:

\begin{center}
\begin{tabular}{||c c||}  
 \hline
 \textbf{Optimizer} & SGD, lr =  \\ 
 \hline
 \textbf{Batch Size} &  \\
 \hline
 \textbf{Patience} &  epochs \\
 \hline
 \textbf{Augmentation} & - \\
 \hline
\end{tabular}
\end{center}

\

At this point we have our \textit{ResNet} ready to be augmented and further trained. We used a very simple training protocol.

\

\textbf{Stage 4 (\textit{LHC} training)}:

\begin{center}
\begin{tabular}{||c c||}  
 \hline
 \textbf{Optimizer} & SGD, lr =  \\ 
 \hline
 \textbf{Batch Size} &  \\
 \hline
 \textbf{Patience} &  epochs \\
 \hline
 \textbf{Augmentation} & - \\
 \hline
\end{tabular}
\end{center}
We observed in some cases, depending on the \textit{LHC} initialization, that the added modules are somehow ``rejected'' by the host network and the training struggles to converge, in one case it totally diverged.
It happened in a minority of the total attempts but to perform the following evaluations we kept only the models whose training loss was less than the starting \textit{ResNet} training loss plus an extra  to take into account the augmented complexity of the model.

To evaluate \textit{LHC} we first applied stage  to the single best \textit{ResNet34} model we managed to achieve (with stages ,  and ), varying the data generator seed, without \textit{LHC} modules (set ). Then, starting from the same base network we augmented it with \textit{LCH} modules and trained it using the same protocol. We tried a small number of trainings with a variety of model parameters (keeping the data generator seed fixed) and clearly detected a neighbourhood of settings appearing to work well (set ). At this point we trained several other models with the best promising parameters setting varying the generator seed (set ). We then compared the set  with the set .

We also considered a minor variation of \textit{LHC-Net}. We tried to exploit the analysis on the  modules we discussed in the previous section showing the last modules playing a minor role and trained  weights, limited by hyperbolic tangent, for every residual sum shown in Fig.1. We manually initialized this  weights by setting them as follows: , , , ,  with the idea of limiting the impact of the last  modules. We call it \textit{LHC-NetC}.
Accordingly with the original Kaggle rules and with almost all evaluation protocols in literature only the private test set was used for final evaluations (public test set performance also appeared to be not well correlated with neither training nor private test performances).
For comparison with \textit{ResNet} we didn't use test time augmentation (TTA). We used TTA only for final evaluation and comparison with other models in literature.
Our TTA protocol is totally deterministic; we first used a sequence of transformations involving horizontal flipping,  pixels horizontal/vertical shifts and finally  radians rotations, in this order. We use rotation after shifting to combine their effect. Rotating first puts the images in only 9 spots, which becomes  if we shift first. At this point we used a second batch of transformations involving horizontal flipping,  zoom and again  radians rotations. Finally we weighted the no-transformation inference  times the weight of others inferences.

\paragraph{Results}
\begin{center}
\begin{tabular}{||p{1.58cm} | p{0.85cm} | p{0.85cm} | p{0.85cm} | p{0.85cm} | p{0.85cm}||}  
 \hline
                      &                                                & \hspace{0.15cm}\textbf{Top}         &                                                    &\hspace{0.08cm} \textbf{Top}                             &  \\ 
                      &\hspace{0.1cm} \textbf{Top}        & \hspace{0.15cm} & \hspace{0.15cm}\textbf{Top}           &\hspace{0.08cm}                     &\\
\hspace{0.35cm}\textbf{Model}                      &\hspace{0.1cm}  & \hspace{0.15cm}\textbf{w/o}         & \hspace{0.15cm}   &\hspace{0.08cm} \textbf{w/o}                            & \hspace{0.03cm} \textbf{Best}\\
                      &                                                & \hspace{0.15cm}\textbf{best}         &                                                    &\hspace{0.14cm}\textbf{best}                            &\\
 \hline
 \hline
\textit{ResNet34v2} &  &  &  &  & \\
 \hline
 \hline
\textbf{\textit{LHC-Net}} &  &  &  &  & \\
 \hline
 \hline
\textbf{\textit{LHC-NetC}} &  &  &  &  & \\
 \hline
\end{tabular}
\end{center}

\textit{LHC-Net} was able to consistently outperform our best performing \textit{ResNet34v2},  both on average and on peak result. Note that the average is not dramatically affected by peak result. Removing peak results does not alter the average qualitative evaluation. \begin{center}
\begin{tabular}{||p{2.2cm} | p{1.3cm} | p{0.7cm} | p{1.2cm} | p{0.8cm}||}  
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{TTA} & \textbf{Params} & \hspace{0.17cm}\textbf{Att}\\ 
\hline
\hline
\textit{BoW Repr.}\cite{FER2013}          & \hspace{0.15cm}               & \hspace{0.17cm}no  & \hspace{0.5cm}-           & \hspace{0.4cm}-\\
\hline
\hline
Human \cite{FER2013}                         & \hspace{0.4cm}70\%                      & \hspace{0.15cm}no  & \hspace{0.5cm}-           & \hspace{0.4cm}- \\
\hline
\hline
\textit{CNN}\cite{minaee2021deep}       & \hspace{0.15cm}               & \hspace{0.15cm}no   & \hspace{0.5cm}-          & \hspace{0.4cm}-\\
\hline
\hline
\textit{VGG19}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{EffNet}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.12cm}M &\hspace{0.4cm}-\\
\hline
\hline
\textit{SVM}\cite{FER2013}                  & \hspace{0.15cm}               & \hspace{0.15cm}no   & \hspace{0.5cm}- & \hspace{0.4cm}-\\
\hline
\hline
\textit{Inception}\cite{pramerdorfer2016facial}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{Incep.v1}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.32cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{ResNet34}\cite{pramerdorfer2016facial}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.12cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{ResNet34}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.12cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{VGG}\cite{pramerdorfer2016facial}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{SE-Net50}\cite{khanzada2020facial}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.27cm}M & \\
\hline
\hline
\textit{Incep.v3}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{ResNet34v2}       & \hspace{0.15cm}               & \hspace{0.15cm}no   & \hspace{0.12cm}27.6M & \hspace{0.4cm}-\\
\hline
\hline
\textit{BAMRN50}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \\
\hline
\hline
\textit{Dense121}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.12cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{ResNet50}\cite{khanzada2020facial}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.12cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{ResNet152}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{VGG}\cite{khaireddin2021facial}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{CBAMRN50}\cite{resmaskingnet}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.05cm}M & \hspace{0.2cm}\\
\hline
\hline
\textbf{\textit{LHC-Net}}                      & \hspace{0.15cm} & \hspace{0.15cm}no  & \hspace{0.12cm}M & \\
\hline
\hline
\textbf{\textit{LHC-NetC}}                    & \hspace{0.15cm} & \hspace{0.15cm}no  & \hspace{0.12cm}M & \\
\hline
\hline
\textit{ResNet34v2}       & \hspace{0.15cm}               & \hspace{0.12cm}yes   & \hspace{0.12cm}M & \hspace{0.4cm}-\\
\hline
\hline
\textit{RM-Net} \cite{resmaskingnet}      & \hspace{0.15cm}              & \hspace{0.12cm}yes & \hspace{0.05cm}142.9M &  \\
\hline
\hline
\textbf{\textit{LHC-NetC}}                    & \hspace{0.15cm} & \hspace{0.12cm}yes & \hspace{0.12cm}M & \\
\hline
\hline
\textbf{\textit{LHC-Net}}                      & \hspace{0.15cm} & \hspace{0.12cm}yes & \hspace{0.12cm}M & \\
\hline
\end{tabular}
\end{center}
 these models are reported in the GitHub repository associated with the referenced paper, not directly into the paper.
\

There are some key points emerging from this analysis:
\begin{itemize}
\item \textit{ResNet34} is confirmed to be the most effective architecture on FER2013, especially its v version. In our experiments raw \textit{ResNet34} trained with the multi-stage protocol and inferenced with TTA reaches an accuracy not distant from the previous SOTA (\textit{ResMaskingNet})
\item heavy architectures seem not able to outperform more simple models on FER2013
\item \textit{LHC-Net} has the top accuracy both with and without TTA
\item \textit{LHC-NetC} outperforms \textit{LHC-Net} but is outperformed when TTA is used
\item more importantly, \textit{LHC-Net} outperforms the previous SOTA with less than one fourth of its free parameters and the impact of the \text{LHC} modules on the base architecture is much lower (less than  VS over ) and it is close to other attention modules like \textit{CBAM}/\textit{BAM}/\textit{SE}
\end{itemize}

As mentioned we limitedly experimented stand-alone training as well with very good results. We trained in parallel, using the same data generator seeds and the same multi-stage protocol, both \textit{LHC-Net} and \textit{ResNet34v2}. In both models the \textit{ResNet34v2} was initialized with Imagenet pre-trained weights. It resulted that \textit{LHC-Net} consistently outperformed \textit{ResNet34v2} at the end of every training stage. It is a limited but very encouraging result.

\section{Conclusions and Future Developments}
Attention, in its every form and shape, is a powerful idea and, despite its impact on computer vision might be not as revolutionary as on NLP, it is still proven to be an important, sometimes decisive, tool.

\

In particular we designed a novel local multi-head channel self-attention module, the \textit{LHC},  and it contributed proving that channel self-attention, in synergy with convolution, could be a functioning paradigm by setting a new state of the art on the well known FER2013 dataset. We also proved that self-attention works well as a small attention module intended as a booster for pre-existing architectures, like other famous vanilla attention modules as \textit{CBAM} or Squeeze and Excitation.

The future research on this architectures will include many aspects:

\begin{itemize}
\item testing \textit{LHC} on other, more computational intensive, scenarios like the Imagenet dataset
\item testing \textit{LHC} with other backbone architectures and with a larger range of starting performances (not only peak performances)
\item we did not optimize the general topology of \textit{LHC-Net} and the model hyper-parameters of the attention blocks are hand-selected with only a few attempts. There's evidence that both the  blocks topology and hyper-parameters might be sub-optimal
\item further research on the stand-alone training mode will be necessary
\item normalization blocks before and after the the \textit{LHC} blocks should be better evaluated in order to mitigate the divergence issue mentioned in the previous section
\item a second convolution before the residual connection should be considered to mimic the general structure of the original \textit{Transformer}
\item a better head splitting technique could be key in the future research. The horizontal splitting we used was only the most obvious way to do it but not necessarily the most effective. Other approaches should be evaluated. For example learning the optimal areas through spatial attention
\end{itemize}

The main results of this paper are replicable by cloning the repository and following the instructions available at:
\
\href{https://github.com/Bodhis4ttva/LHC_Net}{\texttt{https://github.com/Bodhis4ttva/LHC\textunderscore Net}}


\section{Acknowledgment}
We would like to express our deepest appreciation to Dr. Carmen Frasca for her crucial support to our research. We would also like to extend our sincere thanks to Dr. Luan Pham and Valerio Coderoni for their helpfulness and kindness.

\newpage

\

\newpage



\phantomsection
\bibliographystyle{unsrt}
\bibliography{references}



\end{document}